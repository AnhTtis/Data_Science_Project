\subsection{Graph Classification}
\label{gclass}
% This section covers the basics and reasoning behind graph classification using GDL and a GNN discussed in the previous section.
% This method was chosen because, in this paper's case study, the desired outcome is classifying each graph based on a particular metric.

\begin{figure}[t]
\centering
\includegraphics[scale=0.95]{input/figures/graphclass.pdf}
\caption{Illustration of graph classification based on performance values.}
\label{fig:graphclass}
\end{figure}

\textit{Graph classification} refers to classifying graphs based on selected labels.
Here, we consider \textit{supervised} graph classification, where we have a collection of graphs $\mathcal{G}$, each with its label $J(G_i)$, used to train a model using additional graph properties, such as the structure, embeddings, and node data.
These features help the model discriminate between the graphs, improving the prediction of the labels.
An illustration of graph classification is shown in \fref{fig:graphclass}.
% Considering , we can visually show what graph classification can do in its simplest form.
Here, the median performance value might be used as the determination point to divide our graphs into binary classes representing ``good'' graphs in the lower half of the observed performance values and ``bad'' graphs in the upper half. 

The rationale for this approach over regression was to better reflect the designer's intention in early-stage conceptual design.
Often the goal is not to narrow the potential graphs down to \textit{one} particular graph, but rather a \textit{group} of ``good'' or promising graphs that would be analyzed further (often at a higher fidelity due to assumptions made in modeling and other areas during graph design). 
Furthermore, the performance values $J(G_i)$ in the case study have significant variations and many coarse values.
Therefore, the intent is to show the ``belongingness'' of the graphs to a specific group versus assigning specific values to graphs.

% , so there is the possibility of the model not seeing a large majority of the ``good'' or ``best'' graphs.

%  because the graph with the best performance value is not necessarily the best solution to the problem.

% \xrev{The reason that this method was chosen over another method, such as regression, was because we are not interested in narrowing the graphs down to one particular graph, but rather a group of ``good'' graphs to analyze further because the graph with the best performance value is not necessarily the best solution to the problem.
% }

% Here, we show a predefined metric, the median performance value, based on the graphs we already know.
% We then use that value as the determination point to divide our graphs into their various classes; in this case, one represents the "good" graphs in the green zone, and zero represents the rest.
% For example, suppose we have 1,000 variants of one architecture represented as graphs.
% Five hundred of those graphs have an associated binary label where one states that the architecture is "good," and zero states that it is "bad," based on a predefined set of constraints.
% We would train on the 500 graphs with labels and then use that model to predict or classify the remaining 500 graphs as either a one or a zero.
% There are various methods of graph classification, and each one attempts to perform the classification process differently, hoping to achieve a higher level of accuracy.

There are various methods of graph classification, including Deep Graph Convolutional Neural Networks (DGCNN) \cite{b2}, hidden layer representation that encodes the graph structure and node features \cite{b8}, EigenPooling \cite{b12}, and differentiable pooling \cite{b81}.
It has been used in many different studies, such as learning molecular fingerprints \cite{b80}, text categorization \cite{b98}, encrypted traffic analysis \cite{b99}, and cancer research \cite{b100}.

\subsection{Datasets}
\label{sec:datasets}

Based on the Type 1 problem classification considered here from \sref{enumeration}, we will consider the case when only some of the performance values $J(G_i)$ for $G_i \subset \mathcal{G}$ are known.
This will divide the graphs into two sets as follows:
\begin{align}
\mathcal{G} \equiv \mathcal{G}_{all} = \mathcal{G}_{known} \cup \mathcal{G}_{unknown}
\end{align}

\noindent where $\mathcal{G}_{known}$  is the set of graphs with known values for $J(G_i)$ and $\mathcal{G}_{unknown}$ represents graphs with unknown $J(G_i)$ values (and this is what the GDL model is for).
We will denote the sizes of the sets as $|\mathcal{G}_{all}| = N_{all}$, $|\mathcal{G}_{known}| = N_{known}$, and $|\mathcal{G}_{unknown}| = N_{unknown}$. They also satisfy:
\begin{align}\label{eq:N}
N_{all} = N_{known} + N_{unknown}
\end{align}

\noindent We also note that this is a bit different than other types of datasets used in machine learning as there is a known, finite amount of potential inputs.
The goal here is to develop accurate models where $N_{known} \ll N_{unknown}$.

As is typical in machine learning, we will create two subsets from $\mathcal{G}_{known}$ :
\begin{align}
\mathcal{G}_{known} = \mathcal{G}_{training} \cup \mathcal{G}_{validation}
\end{align}

\noindent where $\mathcal{G}_{training}$ is the training dataset and $\mathcal{G}_{validation}$ is the validation dataset.
The model fits using $\mathcal{G}_{training}$, and the fitted model is used to predict the responses for the observations in $\mathcal{G}_{validation}$ \cite{b46}. 
For example, after each iteration, the model will adjust its weights accordingly and test them on the validation set, which can help understand model performance and adjust options as necessary. 
The unknown set is used when the model is finalized, and you want to now test it on data that the model has not seen.


\begin{figure}[t]
\centering
\includegraphics[scale=0.95]{input/figures/set_distinction.pdf}
\caption{Illustration of $\mathcal{G}_{all}$ and $\mathcal{G}_{known}$ for 100 graphs, the performance-based classification of $\mathcal{G}_{known}$, and the potential difference between the medians.}
\label{perc}
\end{figure}

As shown in \fref{fig:graphclass}, the binary classification of graphs in $\mathcal{G}_{known}$  is based on the median performance value (or potentially some other dividing line using the known data).
Since we only know $J(G_i)$ in $\mathcal{G}_{known}$  rather than $\mathcal{G}_{all}$, the median performance value used for the initial labeling might be different than the true value if we had all the performance values for $\mathcal{G}_{all}$.
However, as the relative size of $\mathcal{G}_{known}$  increases, this error will become small.
% This issue is explored in the case study as we will have both the true and experimental classifications available.
The concepts in this section are illustrated in \fref{perc}.
Here we have 100 graphs in $\mathcal{G}_{all}$ and 20 randomly selected graphs for $\mathcal{G}_{known}$.
The true median and $\mathcal{G}_{known}$-based median do differ, and two graphs are between these lines. 

