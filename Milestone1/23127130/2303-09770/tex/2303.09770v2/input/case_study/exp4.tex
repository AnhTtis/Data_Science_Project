% \subsection{Experiment 4: }
% \label{exp4}

% In this experiment, the goal is to see if the GDL model can narrow the dataset down to the actual top 10\% of the dataset through multiple iterations.
% Here, we take the original dataset and split it into the KNOWN and UNKNOWN sets, and once again, use the performance median of the KNOWN set as the threshold for the class splitting of ones and zeros.

% We then take the KNOWN dataset and using an 80/20 split, divide the set into training and validation sets to train the first model, which is then used to predict the classes of the UNKNOWN dataset.
% Once the predictions are completed, we determine which circuits were predicted as zero and remove them under the assumption that those circuits were predicted to be "bad."
% We also remove the classified zeros from the known set since we already have classified them as "bad."
% Now, we are left with our original KNOWN set of "good" graphs, and the newly predicted ones from the UNKNOWN set, and we concatenate these two sets to create a new KNOWN2 set.
% Next, we train a new model on the KNOWN2 dataset using the KNOWN2's performance median as the new class threshold.

% The expected outcome of this experiment is that after each iteration of training on the newly formed KNOWN dataset, the class threshold will continue to reduce to reveal the top candidates for the best realizable performing solution.

% In the three previous experiments, the number of ones and zeros was split 50/50.
% In this experiment, after the select number of graphs was synthesized, the result was an imbalance between the two classes, reflecting closer to a real-world scenario where the user does not know the outcome of the performance values.
% The intended purpose of this is to show that GDL still has a high level of accuracy, even when performed in a case of this nature.

% There are two approaches to this experiment, both of which the user pre-selects some graphs to synthesize; in this case, we chose 50\% of the data and 10\% of the data to synthesize.

% \subsubsection*{The first approach: Create a 50/50 Split Training Set}
% After the data is synthesized, you end up with your ones ($n_1$) and zeros ($n_0$) where $n_0 \gg n_1$.
% It is now time to split the data into the training and validation sets, and in this approach, we take a percentage of the ones for training ($p_{training}$) where $n_{1,training} = n_1 p_{training}$, and then use the same number of zeros where $n_{0,training} \subset n_0(n_{1,training})$.
% This would create a training set $N_{training} = n_{1,training} \cup n_{0,training}$ that has a 50/50 split between classes, and the remainder of the classes would be placed into the validation set $N_{validation} = (n_1 - n_{1,training}) \cup (n_0 - n_{0,training})$.
% The purpose of this approach is to mimic the previous experiments, where there is an equal number in each class.
% The downside to this approach is that the remaining graphs that the model would later predict may have a much more significant class imbalance

% \subsubsection*{The second approach: Shuffle the Data} 
% In a similar situation, after synthesizing the data, you end up with an imbalance of classes where $n_0 \gg n_1$, except this time, we shuffle the data and split the shuffled result into training and validation sets.
% The hypothesis behind this method is: having fewer ones within the training set can help those "top performing" graphs stand out more among the zeros, making them more distinguishable.
% The downside to using this approach, one could end up with $N_{training}$ having little to no ones depending on the size of the synthesized dataset.

