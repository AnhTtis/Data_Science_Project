% \subsection{Experiment 2: Addition of Graph-Based Features}
% The goal of this experiment is not only to increase the models' performance but also to maintain efficiency.
% What do we mean by efficiency?
% One way of increasing model accuracy is to train the model for more epochs, which takes more time while also running the risk of overfitting.
% Increasing efficiency means training for the same amount of epochs or less.
% To do that, we added graph-based features to each node: eigenvector centrality and betweenness centrality. 
% This is known as feature engineering and feature selection, where the features give a higher ability to explain the variance in the training data.

% \subsubsection{Eigenvector Centrality.}
% Eigenvector centrality computes a node's centrality based on its neighbors' centrality. 
% The eigenvector centrality for node $v$ is the $i$-th element of the vector $x$ defined by \eref{evc}, where $A$ is the adjacency matrix with eigenvalue $\lambda$ \cite{b60}.
% %
% \begin{equation}
%     Ax=\lambda x
%     \label{evc}
% \end{equation}

% \subsubsection{Betweenness Centrality.}
% Betweenness centrality of a node is the sum of the fraction of all-pairs shortest paths that pass through that node as shown in Eq.~\eqref{bc} where $V$ is the set of nodes, $\sigma(s,t)$ is the number of shortest paths, and $\sigma(s,t|v)$ is the number of those paths passing through some node $v$ other than $s,t$ \cite{b61}.
% %
% \begin{equation}
%     c_B(v) = \sum_{s,t\in V} \frac{\sigma(s,t|v)}{\sigma(s,t)}
%     \label{bc}
% \end{equation}

% With the additional features, the new features matrix $\textbf{X}$ for each individual graph has gone from $\textbf{X} \in \mathbb{R}^{n \times 1}$ to $\textbf{X} \in \mathbb{R}^{n \times 3}$.
% The expected outcome for this experiment is that we will be able to achieve higher scores from the metrics described in \sref{metrics}.