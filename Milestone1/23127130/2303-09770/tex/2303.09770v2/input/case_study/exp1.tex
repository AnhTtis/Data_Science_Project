% \subsection{Experiment 1: Establish a Baseline}
% \label{exp1}

% In the first experiment, we start by creating a baseline model for future comparisons.
% Here we consider a 80\% (34,599 graphs) in the training set $\mathcal{G}_{training}$, 2\% (865) in the validation set $\mathcal{G}_{validation}$, and the remaining 18\% (7,785) in  $\mathcal{G}_{unknown}$.
% These are by no means recommended distributions, but rather a nearly best case scenario that will be used to explore what is possible and what might be done to increase accuracy while maintaining efficiency.


% with the raw data to establish a baseline.
% Why would we need to establish a baseline?
% This creates a frame of reference so that we know early on how well the model trains on the data "as is" without further manipulation.
% We did an 80/20 split for the batch sizes: the model trained on 80\% of the data points, and we tested the model on the remaining 20\%.
% Since we were also using a validation dataset, 865 graphs were deducted from the training set and designated for validation.
% The remaining graphs were used as the "unseen" dataset to test the model's accuracy.
% See table \ref{exp1tab} for an exact breakdown of the datasets.

% \begin{table}[t]
% \centering
% \caption{The initial breakdown of the datasets to establish a baseline.}
% \label{exp1tab}
% \begin{tabular}{c|r}
%     \hline \hline
%     \textbf{Dataset} & \textbf{\# of Graphs}\\
%     \hline
%     Training & 34,599 \\
%     Validation & 865 \\
%     Test & 7,785 \\
%     \hline \hline
% \end{tabular}
% \end{table}

% Once the baseline, or frame of reference, is established, we can now determine what we need to do in order to increase accuracy while maintaining efficiency.
% For the results of this experiment, please see \sref{exp1r}.
