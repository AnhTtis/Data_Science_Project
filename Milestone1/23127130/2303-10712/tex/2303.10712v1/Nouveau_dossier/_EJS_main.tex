\documentclass[ejs]{imsart}

\RequirePackage{amsthm,amsmath,amsfonts,amssymb}
\RequirePackage[authoryear]{natbib}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\RequirePackage{graphicx}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{multirow}
\definecolor{ForestGreen}{RGB}{34,139,34}
\startlocaldefs
\newcommand{\VB}[1]{[VB: \textcolor{purple}{#1}]}
\newcommand{\ED}[1]{[ED: \textcolor{red}{#1}]}
\newcommand{\CL}[1]{[CL: \textcolor{ForestGreen}{#1}]}

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}
\usepackage{dsfont}
\usepackage{xcolor}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Uncomment next line to change            %%
%% the type of equation numbering           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Axiom, Claim, Corollary, Hypothezis, %%
%% Lemma, Theorem, Proposition              %%
%% use \theoremstyle{plain}                 %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theo}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}

\newtheorem{cor}{Corollary}
\newtheorem{defi}{Definition}
\newtheorem{hyp}{Hypothesis}
\newtheorem{ass}{Assumption}


\newtheorem{assumpCons}{Assumption}
\newtheorem{assumpId}{Assumption}
\renewcommand\theassumpCons{C.\arabic{assumpCons}}
\renewcommand\theassumpId{ID.\arabic{assumpId}.s}
\setcounter{assumpId}{2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Assumption, Definition, Example,     %%
%% Notation, Property, Remark, Fact         %%
%% use \theoremstyle{remark}                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{remark}
\newtheorem{rem}{Remark}
%\newtheorem*{???}{???}
%\newtheorem{???}{???}[???]
%\newtheorem{???}[???]{???}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please put your definitions here:        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Commande2.tex}
\endlocaldefs

\begin{document}

\begin{frontmatter}

\title{Mixture of segmentation for heterogeneous functional data }
%\title{A sample article title with some additional note\thanksref{T1}}
\runtitle{Mixture of segmentation}

\begin{aug}

\author[A]{\fnms{Vincent} \snm{Brault}\ead[label=e1]{vincent.brault@univ-grenoble-alpes.fr}},
\author[B]{\fnms{\'{E}milie} \snm{Devijver}\ead[label=e2]{emilie.devivjer@univ-grenoble-alpes.fr}}
\and
\author[C]{\fnms{Charlotte} \snm{Laclau}\ead[label=e3]{charlotte.laclau@telecom-paris.fr}}

%%% Les affiliations buguent donc si ça ne compile pas, mettre en commentaire, compiler, décommenter et recompiler
%%% Je les enlève pour l'instant
\address[A]{Univ. Grenoble Alpes, CNRS, Grenoble INP\footnote[1]{Institute of Engineering Univ. Grenoble Alpes}, LJK, 38000 Grenoble, France, \printead{e1}}
\address[B]{CNRS, Univ. Grenoble Alpes, Grenoble INP\footnotemark[1], LIG, 38000 Grenoble, France, \printead{e2}}
\address[C]{Télécom Paris, Institut Polytechnique de Paris, \printead{e3}}
\end{aug}

\begin{abstract}
In this paper we consider functional data with heterogeneity in time and in population. 
We propose a mixture model with segmentation of time to  represent this heterogeneity while keeping the functional structure. 
Maximum likelihood estimator is considered, proved to be identifiable and consistent. 
In practice, an EM algorithm is used, combined with dynamic programming for the maximization step, to approximate the maximum likelihood estimator. 
The method is illustrated on a simulated dataset, and used on a real dataset of electricity consumption. 
\end{abstract}

\begin{keyword}
\kwd{Mixture model}
\kwd{Segmentation}
\kwd{Functional Data}
\kwd{Consistency}
\end{keyword}

\end{frontmatter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please use \tableofcontents for articles %%
%% with 50 pages and more                   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents

\section{Introduction}

%\paragraph*{Functional data analysis (FDA)}
Functional Data Analysis (FDA) deals with the theory and the exploration of data observed over a finite discrete grid and expressed as curves (or mathematical functions) varying over some continuum such as time. This type of data is commonly encountered in many fields, including economy (\cite{bugni2009}), computational biology (\cite{giacofci2013}) or environmental sciences (\cite{bouveyron:21}), to name a few.
For an in-depth review of techniques and applications, we refer the interested readers to the books of \cite{FerratyView2006} and \cite{ramsay2002,ramsay2005}. 
%Functional data are encountered in many fields, illustrated on many example in the books 
In many of these applications, such as electricity load, used for illustration here, we observe multiple curves corresponding to several individuals over a given time interval. As a result, one can expect a high heterogeneity of the data, both at the level of the studied individuals, that may correspond to different behavior or consumer profiles, but also on the time dimension where changes of power consumption regimes are likely to occur over the course of one year for instance. 
%are studied over time.
To consider a parametric model, homogeneous data is required, both at population and time levels. In this paper, we propose to split the considered heterogeneous data into homogeneous clusters of individual curves, each of them being segmented over time into homogeneous regimes. To this end, we consider a mixture of segmentation over the projection of the curves onto some functional basis. Figure \ref{fig:motivation} serves to illustrate this objective. The top-row represents the initial functional data consisting of 100 individuals (curves) observed over a period of 50 days. Following rows allow to visualize on the one hand the decomposition of the population into clusters (here 3 clusters - red yellow, purple) and on the other hand, within each cluster the segmentation obtained on the time dimension. Note that, in our case we allow each cluster to have a different segmentation, leading to a more flexible model. In this example, we visualize the segmentation on the three dimensions, denoted by $r$, of the projection on a wavelet basis. 

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=\linewidth]{Fig/Illustration_small.png}
    \caption{Motivation example. The top-row represents the initial functional data consisting of 100 individuals (curves) observed every half-hour over a period of 50 days. Following rows allow to visualize 1/ the decomposition of the population into clusters (here 3 clusters - red yellow, purple), 2/ within each cluster the segmentation obtained on the time dimension, 3/ the projection on a wavelet basis at several level $r$.}
    \label{fig:motivation}
\end{figure}

Model-based clustering approaches for functional data have been extensively studied in the literature (\cite{james2003_sparsefunc,Liu2009,bouveyron2011,jjacques2013,jjacques2014, devijver2017}). For the particular case of heterogeneous data that interests us in this article, one can broadly differentiate between methods that perform simultaneously clustering and segmentation (\cite{alon2003,Hebrail_2010,same2011,Same_2012,chamroukhi_2016}) and co-clustering based methods (\cite{bouveyron:2017, bouveyron:21, bouveyron:2021_2, GALVANI2021107219}). We provide further details on these two families of approaches hereafter and position our contributions with respect to the existing state of the art.
%Biblio : segmentation rapide, 2020, \url{https://arxiv.org/abs/2002.06633}
%Model-based clustering of functional data have been extensively studied over the past decade \citep{jjacques2014}. 

%\paragraph*{Model-based clustering for functional data}
%\CL{Peut etre introduire ici les différentes méthodes de projections. Pas la peine de beaucoup détailler le contenu des ref.}\ED{Je ne sios pas sure que lister les différentes projections ce soit la peine - et en fait je serai bien embetée de le faire pour être honnête. Je vais mettre de la motivation pour les ondelettes dans la partie modele}
%To add : \cite{gaffney99}; \cite{james2003_sparsefunc};\cite{Liu2009};  \cite{jjacques2013}; 
%\cite{devijver2017}; \cite{bouveyron2011};
%\cite{jjacques2014} (survey on clustering for functional data).

%\subsection{Related works}
\paragraph*{Clustering and segmentation}

%\CL{Je ne sais pas ou le mettre Same2011 pour le moment - je dirais ici parce que le coté changement de regime c'est de la segmentation quand meme}

%Mixture of hidden logistic process regressions: 
\cite{same2011} proposed to deal with heterogeneous time series by integrating the notion of change of regimes within a mixture of hidden logistic process regressions. The model is considering two latent variables, one for the mixture component and one for the segmentation. 
Model selection is done through an adapted BIC criterion. However, while attempting to consider changes of regime, this approach fails to account for the ordering of observations, a key feature when dealing with functional data.  
%\ED{mais je ne comprneds pas comment ils assurent que les points voisins sont dans le même segment... }\CL{J'ai regardé, en effet il n'y a aucune contrainte de ce type... Abracadabra}
\cite{Same_2012} extended this model for online segmentation of time series.
In an effort to account for these potential changes of regimes, another family of mixture models, namely the mixture of piecewise regression, has been proposed. 
\cite{Hebrail_2010} first define this notion of piecewise regression to analyze temporal data, by proposing a distance-based model that simultaneously performs clustering on the set of functional observations (through a Kmeans-like algorithm) and segmentation (in the form of piecewise constant function summarizing) within each of the obtained cluster. This work was further generalized to a more flexible probabilistic framework by \cite{chamroukhi_2016}, who designed a model based on a mixture of piecewise regression densities. The piecewise regression is modeled by a segmentation of polynomial functions, as a generalization of spline basis where knots have to be fixed. However, this sets a particular form within each segment. %The clustering is done through an hard-assignment, within a CEM algorithm. 


%Mixture of hidden Markov regression models.
%To add \cite{alon2003}: 

%\CL{Les deux paragraphes précédents c'est que de l'univarié - donc le côté multivarié peut etre la transition avec les versions co-clustering} 


\paragraph*{Model-based co-clustering for FDA}
%In \cite{chamroukhi_2016} il continue un peu le même travail, avec le code dans son package \verb?flamingos?
\cite{bouveyron:2017} proposed a co-clustering model to analyze multivariate functional data. They apply this model to analyze electricity consumption curves, and found that due to the nature of the temporal data, the clustering over timepoints is in fact close to a segmentation over time.
\cite{bouveyron:21} extend this method to multivariate time series (with several time series for each observation and each timepoint), using a sparse representation over principal components. In \cite{bouveyron:2021_2}, authors extend this co-clustering approach using a shape invariant model, allowing for translation in time, and translation and scaling in mean.
\cite{GALVANI2021107219} propose another bi-clustering algorithm for functional data while considering a potential misalignment through translation. While co-clustering based approach have proven efficient in this context, the clustering obtained on the time dimension do not account for the ordering of the observation. 


%Machné, R., Murray, D.B. \& Stadler, P.F. Similarity-Based Segmentation of Multi-Dimensional Signals. Sci Rep 7, 12355 (2017). https://doi.org/10.1038/s41598-017-12401-8


%\CL{Selling points for us : multivariate + strong theoretical guarantees (completely absent from all aforementioned papers) + application on real (for real) world data.}
%\CL{Je n'ai pas vu d'autre papier utilisé la version Lasso pour la maximisation. Tous les papiers clustering + segmentation font de la prog. dynamique}


\paragraph{Contributions and organization of the paper}
Our contribution is threefold, we propose (1) a method to study multivariate functional data, decomposing the population into homogeneous clusters and the time into homogeneous segments, where we ensure coherence on the time order; (2) we then focus on the theoretical study of the model (identifiability) and the estimation of the parameters (consistency), which is completely missing from all aforementioned related articles; (3) finally, we study a real-world electricity consumption data set to illustrate the benefits of our method. 

The paper is organized as follows. 
In Section \ref{sec:framework} the modeling framework is introduced together with the necessary notations. The identifiability of the model is obtained. Details about the estimation procedure are provided in Section \ref{sec:estimation}. The maximum likelihood estimator is proposed, approximated by an EM algorithm. The maximization step is solved by a dynamical programming. The consistency of the estimator is provided. The finite-sample performance of the proposed estimation method is investigated in Section \ref{sec:simu}. The methodology is finally used to analyse electricity consumption in Section \ref{sec:real_data}.  The paper concludes by some discussion in Section \ref{sec:conc}. The code is publicly available at \url{https://github.com/laclauc/MixtSegmentation}. All proofs are given in the Appendix. 

\section{The model and its identifiability}\label{sec:framework}
Suppose one observes multivariate individual  curves ${X}_1(t), \ldots, {X}_n(t)$ on discrete timepoints $t_1,\ldots, t_d$. 
First, we introduce the various elements of the modeling framework, and provide the identifiability of the model. The proof of identifiability can be found in Appendix \ref{App:identifiability}.

\subsection{A multivariate functional model with segments in time and clusters in  population}
%Up to a scaling in time, without loss of generality, we assume that every individual  curve is observed on the interval $[0,1]$. 
We observe multivariate individual curves $({X}_{ih}(t_j))_{1\leq i \leq n, 1\leq j \leq d, 1\leq h \leq H}$ of dimension $H$ over $d$ timepoints  and within a population of size $n$.
The heterogeneous population is studied through a mixture model of $K$ clusters, encoded indifferently in its binary form, $z_{ik}=1$ if and only if the curve $i$ belongs to the cluster $k$, and its vector form, $z_{i}=k$ if and only if the curve $i$ belongs to the cluster $k$, for $1 \leq k \leq K$ and $1\leq i \leq n$. Each observation belongs to the cluster $k\in\{1,\ldots, K\}$ with probability $\pi_k \in [0,1]$.
The heterogeneity in time is represented through $L_k +1$ segments $(I_{k \ell})_{ 0\leq \ell \leq L_K}$: if $z_{ik}=1$ and $j \in I_{k \ell}$, encoded by $w_{j\ell} = 1$,
\begin{align}
    \label{model_fda}
    {X}_{ih}(t_j) = f_{k \ell h}(t_j) + \eta_{ijh}
\end{align}
with $\eta_{ijh}$ corresponds to some random noise, more details being given in Section \ref{sec:proj}.
Usually in segmentation, we assume that the signal is constant. Here, we would like to emphasize some coherence in time, but not necessarily through a strong assumption as constant. Then, we propose to decompose our signal into several time periods that are meaningful in practice (in hours, in days, in weeks depending on the application), and to have the same function $f_{k \ell h}$ within the considered interval, through the same segment. 


The modeling assumption is equivalent to a main function $f_{k \ell h}$ for the $h$th component, for individuals belonging to the cluster $k$, and for a timepoint in the $\ell$th segment. This means that within a segment and a cluster, there is a random variation (seen as a noise) independent and identically distributed over each component of the multivariate curve. 
%In the next section, we propose to project each curve observed on discrete timepoints onto a functional basis to keep the time aspect while doing statistics. 

\subsection{Projection onto a functional basis and matrix-variate model}\label{sec:proj}

We denote $(\boldsymbol{y}_{ij}) \in \mathbb{R}^{p}$ the  coefficient decomposition vectors of the  component $j \in \{1,\ldots, d\}$ onto the functional basis, and the individual $i \in \{1,\ldots, n\}$, and the orthonormal characterization leads to, for the level $M$,
$$(\mathbf{X}_{i.}(t_j))_{1\leq j \leq d}= \Pi \boldsymbol{y}_{ij} ;$$
where $\Pi$ is a matrix defined by  the functional basis of size $M$. 
%
We consider the wavelet coefficient dataset $(\mathbf{Y}_{i})_{1\leq i \leq n} = (\mathbf y_{i.})_{1\leq i \leq n} \in (\mathbb{R}^{d \times p})^n$, which defines $n$ observations whose probability distribution is modeled by the following finite matrix-variate Gaussian mixture of segmentation model. 
%
As mentioned previously, the heterogeneous population is represented by $K$ clusters. For the cluster $k \in \{1,\ldots, K\}$, the heterogeneity in time is described by $L_k+1$ segments, defined by $\Lk$ break-points $T_{k0}<T_{k1}<\cdots<T_{k\Lk}<T_{k,\Lk+1}$. 
Then, for an observation $i\in \{1,\ldots,n\}$  in the cluster $k\in \{1,\ldots,K\}$, 
%for  $\Lk+1$ means $\left(\bmu_{k\ell}\right)_{1\leq \ell\leq \Lk+1}\in\setR^{\left(\Lk+1\right)\times p}$ and $\Lk+1$ variances $\left(\bsigma_{k\ell}\right)_{1\leq \ell\leq \Lk+1}\in\left(\setR_{\star}^+\right)^{\left(\Lk+1\right)\times p}$ such that for all $k\in\left\{1,\ldots,K\right\}$, for all $i$ in the cluster $k$, 
 for  $\ell\in\left\{0,\ldots,\Lk\right\}$ such that  $j\in\left\{T_{k, \ell}+1,T_{k, \ell}+2,\ldots,T_{k,\ell+1}\right\}$, we have:
\begin{align}
[\mathbf{Y}_{i}]_{j.} | (z_{ik} = 1, W_{j\ell} = 1) = \bmu_{k \ell} + \varepsilon_{ij}
\label{model_multiv}
\end{align}
with $\varepsilon_{ij} \sim \mathcal{N}_p\left(0, \Sigma_{k\ell}\right)$ where $\Sigma_{k\ell}$ is diagonal with the values $\left(\sigma_{k \ell r}\right)_{1\leq r \leq p}$. 
%The equivalence between segments in \eqref{model_fda} and \eqref{model_multiv} is explained by the choice of wavelet coefficients, that keep the sorting of time. 
%\ED{discuter les hypotheses d'independence}
\subsection{Identifiability of the  model}
In this section, we first establish the identifiability of the multivariate model \eqref{model_multiv}. 

\begin{theo}[Identifiability of \eqref{model_multiv}]\label{Th:Identifiability} Assume that:
\begin{enumerate}[label=(ID.\arabic*)]
    \item \label{ID:1} For every $k\in\{1,\ldots,K\}$ and $\ell\in\{0,\ldots,\Lk\}$, there exists at least one $r\in\{1,\ldots,p\}$ such that $\sigma_{k \ell r}\neq\sigma_{k,\ell+1,r}$ or $\mu_{k \ell r}\neq\mu_{k,\ell+1,r}$.
    \item \label{ID:2} We have:
    \[p \geq \underset{k\in\{1,\ldots,K\}}{\max}\Lk+1.\]
     \item \label{ID:3} If there exists $k\neq k'$ such that $\Lk=L_{k'}$ then:
    \begin{itemize}
        \item there exists $\ell\in\{0,\ldots,\Lk\}$ such that $T_{k \ell}\neq T_{k',\ell}$,
        \item or there exists $\ell\in\{0,\ldots,\Lk\}$ and $r\in\{1,\ldots,p\}$ such that:
    \[\sigma_{k \ell r}\neq\sigma_{k',\ell,r} \text{ or }\mu_{k \ell r}\neq\mu_{k',\ell,r}.\]
    \end{itemize}
    \item \label{ID.4} For every $k\in\{1,\ldots,K\}$, $\pi_k>0$.
\end{enumerate}
Under these assumptions, the model \eqref{model_multiv} is identifiable.
\end{theo}
The breakpoints models of each cluster are identifiable by the\linebreak assumption~(ID.1) and~(ID.2). The  assumption~(ID.3) allows to differentiate the clusters. 

Mixture models are known to be identifiable up to a label switching: two partitions can be the same while the cluster labels being reversed. In this model, a natural order is to choose the labeling of each cluster such that
\[k\leq k'\Leftrightarrow L_k\leq L_{k'}.\]
This alleviates the problem of label switching; and it can be completely removed when the $(L_k)_{1\leq k\leq K}$ are all different.



\section{Estimation}\label{sec:estimation}
In this paper, we assume that $K$ the number of clusters is known, as well as the number of segment within each cluster $(L_k)_{1\leq  k \leq K}$.
\subsection{Maximum Likelihood Estimation}
Using the model \eqref{model_multiv}, under identifiability, by noting $\bT$ the set of the break points and $\btheta = ((\mu_{k\ell r},\sigma_{k\ell r})_{1\leq k \leq K,  0\leq \ell \leq L_K, 1\leq r \leq p}, (\pi_k)_{1\leq k \leq K})$ the set of parameters, we obtain the following likelihood: 
\[\text{lik}\left(\bY;K,\bT,\btheta\right)=\prod_{i=1}^n\sum_{k=1}^K\pi_k\prod_{\ell=0}^{\Lk}\prod_{j=T_{k \ell}+1}^{T_{k,\ell+1}}\prod_{r=1}^p\left[\frac{1}{\sqrt{2\pi\sigma_{k \ell r}}}e^{-\frac{1}{2\sigma_{k \ell r}}\left(Y_{ij r}-\mu_{k \ell r}\right)^2}\right].\]
The mixture model leads to  the product over individuals $i \in \{1,\ldots,n\}$ and the sum over the clusters $k \in \{1,\ldots,K\}$ while  the segmentation is related to the product over each segment $\ell \in \{0,\ldots, L_k\}$ and timepoints indexed by $j \in \{T_{k\ell}+1,\ldots, T_{k,\ell+1}\}$, for the cluster $k \in \{1,\ldots, K\}$. In addition to the parameters $K$, $\bT$ and $\btheta$, we search to estimate the partition $\bz$.  

We denote $\hat{\btheta}$ the maximum likelihood estimator. 
\subsection{EM algorithm}
Considering a mixture model, we use the \textit{Expectation Maximisation} (EM) algorithm~(\cite{dempster1977maximum}) to estimate the parameters. The principle is, for the step $(c)$, to fix a parameter $\bthetac$ and break points $\bT$, and to maximize the following function:
\[(\btheta,\bT)\mapsto Q\left(\btheta,\bT\left|\bthetac,\bTc\right.\right)=\Espc{\bz\left|\bY;\bthetac,\bTc \right. }{\log p\left(\bY,\bz;\btheta,\bT\right)}\]
where $p(\bY,\bz;\btheta,\bT)$ is the joint distribution of $\bY$ and $\bz$ for fixed parameters $\btheta$ and break points $\bT$. To do so, we alternate between  two steps:
\begin{itemize}
    \item[E-step:] For $i\in\{1,\ldots,n\}, k\in\{1,\ldots,K\}$ compute  the values $\sikc$
    defined by:
\[\sikc=\Prob{\zik=1\left|\bY;\bthetac,\bTc\right.};\]
    \item[M-step:] Maximization with respect to $\btheta$ and $\bT$ the function \linebreak$(\btheta,\bT)\mapsto Q\left(\btheta,\bT\left|\bthetac,\bTc\right.\right)$.
\end{itemize}

For $i\in\{1,\ldots,n\}, k\in\{1,\ldots,K\}$, the computation of $\sikc$ is explicit using the following proposition. 



\begin{prop}[E-Step]\label{prop:stepE}
For every $i\in\{1,\ldots,n\}$ and $k\in\{1,\ldots,K\}$ we have:
\begin{align*}
    \sikc=\frac{\pic_{k}\prod_{\ell=0}^{\Lc_{k}}\prod_{j=\Tc_{k \ell}+1}^{\Tc_{k,\ell+1}}\prod_{r=1}^p\left[\frac{1}{\sqrt{2\pi\sigmac_{k \ell r}}}e^{-\frac{1}{2\sigmac_{k \ell r}}\left(Y_{i\ell r}-\muc_{k \ell r}\right)^2}\right]}{\sum_{k'=1}^K\pic_{k'}\prod_{\ell=0}^{\Lc_{k'}}\prod_{j=\Tc_{k'\ell}+1}^{\Tc_{k',\ell+1}}\prod_{r=1}^p\left[\frac{1}{\sqrt{2\pi\sigmac_{k'\ell r}}}e^{-\frac{1}{2\sigmac_{k'\ell r}}\left(Y_{i\ell r}-\muc_{k'\ell r}\right)^2}\right]}.
\end{align*}
\end{prop}

For the maximization, the problem is more difficult due to the unknown segmentation over time. The next proposition explicit the formulae for the proportions $(\pi_k)_{1\leq k \leq K}$.
\begin{prop}[Proportion in the M-Step]\label{prop:stepM}
For every $k\in\{1,\ldots,K\}$ we have:
\[\pikc=\frac{\spluskc}{n}=\frac{\sum_{i=1}^n\sikc}{n}.\]
\end{prop}

The other parameters $(\mu_{k\ell r},\sigma_{k\ell r})_{1\leq k \leq K, 1\leq \ell \leq L_K, 1\leq r \leq p}$ are given using the dynamic programming  (\cite[see for example][]{bellman1957dynamic,kay1993fundamentals}).
We start by observing that
\begin{align}
\underset{\left(\btheta,\bT\right)}{\max}\;Q\left(\btheta,\bT\left|\bthetac,\bTc\right.\right)=&-\frac{1}{2}\underset{k=1}{\overset{K}{\sum}}\left[\underset{ T_{k.} \in \mathcal{T}}{\min}\underset{\ell=0}{\overset{\Lk}{\sum}}\Deltakc\left(T_{k \ell};T_{k,\ell+1}\right)\right]\nonumber\\
&-\frac{ndp}{2}\log\left(2\pi\right)+\underset{k=1}{\overset{K}{\sum}}\spluskc\log\pik,
\label{Eq:maxQ}
\end{align}
where $\mathcal T = \{0=T_{k0}<T_{k1}<\cdots<T_{k,\Lk+1}=d+1\}$
with for every $k\in\{1,\ldots,K\}$, $0\leq t_1<t_2\leq d$,
\begin{eqnarray}
&&\!\!\!\!\!\!\!\!\!\!\!\!\!\!\Deltakc\left(t_1;t_2\right)\nonumber\\
&=&\underset{\overset{\bmu \in \mathbb{R}^p,}{\underset{\bsigma\in (\mathbb{R}_+^*)^p}{}}}{\min}\underset{r=1}{\overset{p}{\sum}}\left[\spluskc(t_2-t_1)\log\left(\sigma_r\right)+\frac{1}{\sigma_r}\underset{j=t_1+1}{\overset{t_2}{\sum}}\underset{i=1}{\overset{n}{\sum}}\sikc\left(Y_{ijr}-\mu_r\right)^2\right].\label{Eq:Deltakc}
\end{eqnarray}
This optimization problem is explicitly solved in the following proposition, for each cluster independently. 
%As the decomposition~\eqref{Eq:maxQ} exists, we can use dynamic programming for each cluster independently. For this, we observe that the equation~\eqref{Eq:Deltakc} can be rewritten as:
\begin{prop}[Form of $\Deltakc$]For all $k\in\{1,\ldots,K\}$ and $0\leq t_1<t_2\leq d$, we have:
\begin{eqnarray*}
&&\!\!\!\!\!\!\!\!\!\!\!\!\!\!\Deltakc\left(t_1;t_2\right)\\
&=&\underset{r=1}{\overset{p}{\sum}}\left[\spluskc(t_2-t_1)\log\left(\sigmah_{k t_1 r}\right)+\frac{1}{\sigmah_{k t_1 r}}\underset{j=t_1+1}{\overset{t_2}{\sum}}\underset{i=1}{\overset{n}{\sum}}\sikc\left(Y_{ijr}-\muh_{k t_1 r}\right)^2\right]
\end{eqnarray*}
with
\begin{eqnarray*}
\overline{\mathbf{Y}}_{i,t_1:t_2,r} &=& \frac{1}{t_2-t_1}\sum_{j=t_1+1}^{t_2}Y_{ijr}\\
\muh_{k t_1 r}&=&\sum_{i=1}^n\frac{\sikc}{\spluskc} \overline{\mathbf{Y}}_{i,t_1:t_2,r}\\
\sigmah_{k t_1 r}&=&\sum_{i=1}^n\frac{\sikc}{\spluskc}. \frac{1}{t_2-t_1}\sum_{j=t_1+1}^{t_2}\left(Y_{ijr}-\overline{\mathbf{Y}}_{i,t_1:t_2,r}\right)^2.
\end{eqnarray*}
\end{prop}

%\begin{rem}
%In the case where $\bsigma$ does not depend on groups $k$, segments $\ell$ and/or dimensions $r$, we propose the formula in Appendix~\ref{sec:app:deltakx}.
To optimize the computation time, we suggest to first compute all the means $\Yibarrc{t_1}{t_2}$ for $t_2>t_1$. In particular, in the case of $\bsigma$ depends only on the cluster $k$, we improve the complexity.
%\end{rem}



\begin{prop}[Complexity]
The complexity of the \textit{EM} algorithm with $N_{\text{algo}}$ iterations is $\mathcal{O}\left[pnd^2\max\left(d,KN_{\text{algo}}\right)\right]$.
\end{prop}

\begin{proof}
 The values $\overline{\mathbf{Y}}_{i,t_1:t_2,r}$ are computed for each $i$, each $r$ and each pair $t_1<t_2$: as the computation is a mean, the final complexity is $\mathcal{O}\left(pnd^3\right)$. For each iteration in the algorithm, the complexity of E-step is $\mathcal{O}\left(Kpnd\right)$ and, thanks the storage of the $\overline{\mathbf{Y}}_{i,t_1:t_2,r}$, the complexity of the computation of each matrix $\Deltakc$ is only $\mathcal{O}\left(npd^2\right)$. Finally, the update of the estimation of $\bmu$ and $\bsigma$ is $\mathcal{O}\left(Knpd\right)$. The combination of all these complexities gives the result.
\end{proof}

Remark that the two most time-consuming steps are the computation of the means $\overline{\mathbf{Y}}_{i,t_1:t_2,r}$ and the computations of $\Deltakc$ at each iteration; these two steps can be easily parallelized. Moreover, the table of averages can be stored for later use.


\subsection{Consistency}
In this section, we prove that the model introduced in Equation \eqref{model_multiv} is consistent. To simplify notations, we consider univariate functional data or  projection of the observed functions onto a 1-dimensional basis, such that $p=1$ in this section, but the conclusion would be the same. To simplify the notations, we set $\sigma_{k\ell}=1$, but the results can be extended as well to any variance. 



First, we assume that the parameter space is bounded. 


\begin{assumpCons}\label{ass:C.1}
There exists $M>0$ such that for all $k\in\{1,\ldots,K\}$ and $\ell\in\{0,\ldots,\Lk\}$,
\[\mu_{k\ell}\in[-M;M].\]%\text{ and }\sigma_{k\ell}\in\left[\frac{1}{M};M\right].\]
\end{assumpCons}

We  assume that there are enough observations in each segment:
\begin{assumpCons}\label{ass:C.2} There exists $\tau_{\min}>0$ such that for all $k\in\{1,\ldots,K\}$ and $\ell\in\{0,\ldots,\Lk\}$, 
\[T_{k,\ell+1}-T_{k\ell}>\tau_{\min}d.\]
\end{assumpCons}
Let $\mathcal{T}_{\tau_{\min}}$  the set of breakpoints satisfying Assumption \eqref{ass:C.2}. 

We need an assumption about the rate of convergence. 

\begin{assumpCons} \label{ass:C.3} We assume that $\log(n) / d \underset{n,d\to+\infty}{\longrightarrow}0$. 
\end{assumpCons}


We also want to distinguish between clusters. To do so, we introduce the notion of equivalent clusters and the related symmetry. 

\begin{defi}[Equivalent clusters]
Two partitions $\bzs$ and  $\bz$ with $K$ clusters are  \emph{equivalent}\index{partition@Partition!équivalente}, denoted $\bzs \sim \bz$, if there exists a permutation 
 $\sigma\in\mathfrak{S}(\left\{1,\ldots,K\right\})$ such that for all $i\in\{1,\ldots,n\}$ and $k\in\{1,\ldots,K\}$, $z_{i\sigma(k)}=\zs_{ik}$.

 By similarity, we denote $\btheta', \bT' \sim \btheta, \bT$ if there exists a permutation that leads to the same parameters. 
%
%
%\ED{associer la notions d'éqivalence : si ils sont dans le meme Sym}
\end{defi}


We can thus define a distance between two partitions. 
\begin{defi}[Distance for partitions]
We define the distance $d_{0,\sim}$ between two partitions $\bz, \bzs$ with $K$ clusters by 
\begin{align}\label{d0sim}
d_{0,\sim}(\bzs,\bz)&=\underset{\sigma\in\mathfrak{S}(\left\{1,\ldots,K\right\})}{\min}\sum_{k=1}^K\sum_{i=1}^n \zs_{ik} z_{i\sigma(k)}.% =n -\underset{\sigma\in\mathfrak{S}(\left\{1,\ldots,K\right\})}{\max}\sum_{k=1}^K\RK_{k,\sigma(k)},
\end{align}



For a partition $\bzs$ and a radius $r>0$, and $\mathcal{Z}$ is the set of all potential partitions,
let $\mathcal{B}\left(\bzs;r\right)$ the ball
\[\mathcal{B}\left(\bzs;r\right)=\left\{\bz\in\mathcal{Z}\left|d_{0,\sim}(\bz,\bzs)\leq rn\right.\right\}.\]
\end{defi}

For the consistency of the estimator, we need to distinguish between close clusters, assuming something stronger than Assumption \ref{ID:3}.
\begin{assumpId} \label{ID:3:s} If there exists $k\neq k'$ such that $\Lk=L_{k'}$ then we assume that there exists at least $\tau_{\min}d$ coordinates $j$ such that the distribution of $Y_{ij}|\zs_{ik}=1$ is different from the distribution of $ Y_{ij}|\zs_{ik'}=1$.
\end{assumpId}
This is needed when the models within each segment are the same, and only the segments are different. 

We also need an assumption stronger than \ref{ID.4} about the number of curves in each cluster:
\begin{assumpId} \label{ID:4:s}   There exists a constant $c>0$ such that for every $k\in\{1,\ldots,K\}$, $\pi_k>c$.
\end{assumpId}


%\begin{rem}
%Eventuellement, (C.3) c'est plutôt (ID.2.s) puisque c'est une contrainte plus forte que (ID.2).
%\end{rem}

Variances are supposed to be equal whatever the cluster, the segment and the dimension, and to have the value $\sigma_{kjr}=1$. Extension to any variance is straightforward but derivations of formula are more technical. 

\begin{theo}\label{th:consistence}
Let $\mathbf{Y}$ be a matrix of a $n\times T$ observations of the model \eqref{model_multiv} with true parameter $\bthetas, \bTs$ where the number of clusters $K$ and the number of segments $(L_k)_{1\leq k \leq,K}$ are known. 
We assume \ref{ID:1}, \eqref{ID:3:s}, \eqref{ID:4:s}, \eqref{ass:C.1}, \eqref{ass:C.2} and \eqref{ass:C.3}. Then, 
for every parameter $\btheta\in\bTheta$ and $\bT\in\mathcal{T}_{\tau_{\min}}$,
\[\frac{p(\bY;\btheta,\bT)}{p(\bY;\bthetas,\bTs)}=\max_{(\btheta',\bT')\sim(\btheta,\bT)}\frac{p\left(\bY,\bzs;\btheta',\bT'\right)}{p\left(\bY,\bzs;\bthetas,\bTs\right)}\left[1+o_P(1)\right]+o_P(1)\]
where $o_P$ are uniform over $\bTheta$ and $\mathcal{T}_{\tau_{\min}}$, and $(\btheta',\bT')\sim(\btheta,\bT)$ means for any parameter up to the label switching. 
\end{theo}

\textbf{Sketch of proof.}
% Pour cette preuve, je suppose que la variance est égale à quelque soit le groupe. La différence se fait sur les calculs qui sont plus moches sinon. De même, on se place dans le cas $p=1$ pour gagner du temps.
We will prove that the complete likelihood with a bad clustering becomes small asymptotically with respect to the complete likelihood associated to the true partition.
To do so, we decompose the probability with respect to potential partitions.
\begin{align*}
p(\bY;\btheta,\bT)&=\sum_{\bz \in \mathcal{Z}}p(\bY,\bz;\btheta,\bT)\\
%&= \sum_{\bz \in \mathcal{Z}}p(\bz;\btheta,\bT) p(\bY|\bzs;\bthetas,\bTs) \exp(\Fntz)
&= \sum_{\bz \sim \bzs}p(\bY,\bz;\btheta,\bT) + \!\!\!\!\!\!
\sum_{\underset{\bz\nsim\bzs}{\bz\in\mathcal{B}\left(\bzs;{c}\right)}}p(\bY,\bz;\btheta,\bT) +\!\!\!\!\!\!
\sum_{\bz\notin\mathcal{B}\left(\bzs;{c}\right)}p(\bY,\bz;\btheta,\bT).
\end{align*}
%where
%\[\Fntz=\log\frac{p(\bY|\bz;\btheta,\bT)}{p(\bY|\bzs;\bthetas,\bTs)}.\]
Each term is controlled by the following propositions, that are proved in Appendix \ref{app:Cons}.


\begin{prop}[Equivalent partitions]\label{prop:part_equi}
Under Assumptions (ID.1) and (ID.3), we have for all $\btheta\in\bTheta$ and $\bT\in\mathcal{T}_{\tau_{\min}}$:
\[\sum_{\bz\sim\bzs}\frac{p(\bY,\bz;\btheta,\bT)}{p\left(\bY,\bzs;\bthetas,\bTs\right)}=\max_{(\btheta',\bT')\sim(\btheta,\bT)}\frac{p\left(\bY,\bzs;\btheta',\bT'\right)}{p\left(\bY,\bzs;\bthetas,\bTs\right)}\left[1+o_P(1)\right]\]
where $o_P$ is uniform on $\bTheta$.
\end{prop}



    \begin{prop}[Partitions close to the true one]\label{prop:contrib_local}
Under Assumptions (ID.3.s), (ID.4.s), (C.1), (C.2) and (C.3), we have that for all  $\tilde{c}<c/4$:
\[\sup_{(\btheta,\bT)\in\bTheta\times\mathcal{T}_{\tau_{\min}} }\sum_{\underset{\bz\nsim\bzs}{\bz\in\mathcal{B}\left(\bzs;\tilde{c}\right)}}p\left(\bY,\bz;\btheta,\bT\right)=o_P\left[p\left(\bY,\bzs;\bthetas,\bTs\right)\right].\]
\end{prop}



    \begin{prop}[Partitions far from the true one]
Under Assumptions  (C.1) and (ID.4.s), asymptotically in  $n$ and $d$, if there exists a sequence of radius  $R_{nd}$ converging to 0 such that $R_{nd}>\max\left(\sqrt{\log K/d},\frac{4\text{Diam}(\bTheta)K^2}{\sqrt{nd}\delta(\bthetas)},\frac{4\log \left(\frac{1}{c}\right)}{d\delta(\bthetas)}\right)$, then for all  $\btheta\in\bTheta$ and all $\bT\in\mathcal{T}_{\tau_{\min}}$:
\begin{equation}
\sup_{(\btheta,\bT)\in\bTheta\times\mathcal{T}_{\tau_{\min}}} \sum_{\bz\notin\mathcal{B}\left(\bzs;R_{nd}\right)}p(\bY,\bz;\btheta,\bT)=p(\bY,\bzs;\bthetas,\bTs)o_P(1)
\end{equation}
with probability $1-\exp\left(-\varepsilon_{nd}^2nd\right)$ where $\varepsilon_{nd}=\min\left(\frac{\delta(\bthetas)R_{nd}}{16},1/\sqrt{2}\right)$.
\label{prop:part_eloign}
\end{prop}

Then, we get that 
\begin{align*}
p(\bY;\btheta,\bT)
&= \sum_{\bz \sim \bzs}p(\bY,\bz;\btheta,\bT) + 
p(\bY|\bzs;\bthetas,\bTs) o_P(1)
\end{align*}
which gives the result. 


%En particulier, nous observons que pour tout partition $\bz$ et pour tout ensemble de paramètres et ruptures $(\btheta,\bT)$, nous avons~
%\begin{eqnarray*}
%p(\bY,\bz;\btheta,\bT)&=&p(\bY|\bz;\btheta,\bT)p(\bz;\btheta,\bT)\\
%&=&p(\bz;\btheta,\bT)p(\bY|\bz;\btheta,\bT)\frac{p(\bY|\bzs;\bthetas,\bTs)}{p(\bY|\bzs;\bthetas,\bTs)}\\
%&=&p(\bz;\btheta,\bT)p(\bY|\bzs;\bthetas,\bTs)\exp\underbrace{\left[\log\frac{p(\bY|\bz;\btheta,\bT)}{p(\bY|\bzs;\bthetas,\bTs)}\right]}_{=:\Fntz}\\
%\end{eqnarray*}

\begin{cor}\label{cor:consistency}

Let $\mathbf{Y}$ be a matrix of a $n\times T$ observations of the model \eqref{model_multiv} with true parameter $\bthetas, \bTs$ where the number of clusters $K$ and the number of segments $(L_k)_{1\leq k \leq K}$ are known. 
We assume \ref{ID:1}, \eqref{ID:3:s}, \eqref{ID:4:s}, \eqref{ass:C.1}, \eqref{ass:C.2} and \eqref{ass:C.3}. Then, 

\begin{align*}
\left(\widehat{\btheta},\widehat{\bT}\right)\underset{n, d \rightarrow +\infty}{\overset{\mathbb{P}}{\rightarrow}} \left(\btheta^\star,\bT^\star\right).
\end{align*}
\end{cor}

From Theorem~\ref{th:consistence}, we have that the likelihood focuses on the true partition of the data, hence the maximum likelihood estimator is asymptotically close to the complete maximum likelihood, given the true partitions. In this particular case, since the partitions are known, our problem boils down to a standard segmentation problem.
%\VB{Dire que, comme la vraisemblance se concentre sur la bonne partition, l'estimateur du maximum de vraisemblance est asymptotiquement proche du maximum de vraisemblance complète avec la vraie partition. Dans ce cas, on se retrouve dans les conditions \textit{classiques} d'un problème de segmentation et on peut utiliser tous les résultats de la bibliographie}

\section{Simulation study}
\label{sec:simu}

We first provide empirical evaluation of our model on univariate generated data.
\subsection{Experimental Protocol}
\paragraph{Data generation process} We simulate data based on the following generative process. For a given number of multivariate observations $n\in\{100, 1000\}$ with $H=32$ and a number of days $d\in\{50, 100\}$, we start by generating a mixture model with $K=3$ clusters with equal proportions ($\pi_k=\frac{1}{3}$ for $k \in \{1,2,3\}$). For each $k\in \{1,\ldots, K\}$, we take:
$$f_{k\ell}(t_j) = (-1)^{k}. \alpha. \cos\left(\frac{2\pi t_j}{1+\ell}\right), $$
where $\alpha$ guides the amplitude of the generated curves, $\ell \in \{0, \ldots, L_k\}$ is the index of the current segment, and $j \in \{1, \ldots, T\}$ is the index of the timepoint. As a result $\alpha$ plays a dual role: the smaller $\alpha$ is, the harder it is to both differentiate between the clusters and to detect the break-points. In the sequel, we consider different settings by varying the values of $\alpha \in \{0.1, 0.2, 1\}$. Finally, we fix the variance $\sigma_{k\ell}=1$ for all cluster $k\in\{1,\ldots,3\}$ and segment $\ell \in\{1,\ldots,L_k\}$. 
Figure \ref{fig:dgp} illustrates all settings for $K=3$ and fixed $n$ and $d$.

\paragraph{Projection onto a wavelet basis}
%Any functional basis might be used to project the observed functions, but we propose here to deal with wavelets in practice so we describe the model in that case. 
The discretization of each component of the $p$-dimensionial curve is projected onto a wavelet basis\footnote{Note that any functional basis might be used to project the observed functions.}, that represents localized features of functions in a sparse way (\cite{Mallat}).  In our paper, the Discrete Wavelet Transform (DWT) is performed using a computationally fast pyramid algorithm (\cite{Mallat2, Poggi}).
%Let $\psi$ be a real wavelet function, satisfying for $t\in \mathbb{R}$,
%$$ \psi \in L^1 \cap L^2, t\mapsto t \psi(t) \in L^1, \text{ and } \int_{\mathbb{R}} \psi(t) dt =0.$$
%We denote for $(m, h)\in \mathbb{Z}^2$ by $\psi_{m h}$ the function defined from $\psi$ by dyadic dilation and translation:
%$$\psi_{m h}(t)=2^{m/2} \psi(2^m t-h) \text{ for } (m, h) \in \mathbb{Z}^2, \text{ for } t \in \mathbb{R}.$$
%We  define the wavelet coefficients of a signal $f$ by
%$$d_{m h}(f) =\int_{\mathbb{R}} f(t) \psi_{m h}(t)dt %\text{ for } (m, h) \in \mathbb{Z}^2.$$
%Let $\varphi$ be a scaling function, related to $\psi$, and let $\varphi_{m h}$ be the dilatation and translation of $\varphi$ for $(m, h) \in \mathbb{Z}^2$. We also define, for $(m, h) \in \mathbb{Z}^2$,
%$\mathbf{B}_{m, h}(f) = \int_{\mathbb{R}} f(t) \varphi_{m h}(t)dt$.
%
We use both scaling functions to construct approximations of the function of interest, and the wavelet functions serve to provide the details not captured by successive approximations.
%
%We denote  $V_m = \text{Vect} (\{\varphi_{m h}\}_{h \in \mathbb{Z}})$ and  $W_m = \text{Vect}(\{ \psi_{m h}\}_{h \in \mathbb{Z}})$ for all $m \in \mathbb{Z}$.
%Remark that
%$V_{m-1} = V_m \oplus W_m \text{ for all } m \in \mathbb{Z}$ and 
%$ \text{L}^2 = \oplus_{m \in \mathbb{Z}} W_m$.


%Let $M \in \mathbb{N}^*$. % For a signal $f$, we  define the approximation at the level $L$ by
%A signal $f$ is decomposed  by the approximation at the level $M$, $C_M(f)$, and by the details $(d_{m h})_{m <M}$, where 
%$$C_M(f) = \sum_{m >M} \sum_{h \in \mathbb{Z}} d_{m h} (f) \psi_{m h} = \sum_{h \in \mathbb{Z}} \boldsymbol{B}_{m h}(f) \varphi_{m h}.$$
%The collection $\{\mathbf{B}_{m h}({f}),d_{m h}({f}) \}_{m \leq M, h \in \mathbb{Z}}$ is the Discrete Wavelet Transform (DWT) of $f$.
%For a level $M$, we focus on scaling coefficients $(\mathbf{B}_{m h}(f))_{1\leq m \leq M}$ which are sorted with time and on which we can do a segmentation. 

\paragraph{Evaluation Metrics}

In order to evaluate the quality of our output, we consider different metrics of evaluation. For the clustering part, we compute the Adjusted Rand Index (ARI) (\cite{hubert1985comparing}) and the Normalized Classification Error (NCE) (\cite{robert2021}). 

The ARI is a measure of agreement between two partitions defined by
\[
\text{ARI}(\mathbf{z}^{*}, \hat{\mathbf{z}}) = \frac{\sum_{k,k'}\binom{n_{kk'}{2} }-\left[\sum_k\binom{n_k}{2}\sum_{k'}\binom{\hat{n}_{k'}}{2}/\binom{n}{2} \right]}{\frac{1}{2}\left[\sum_k\binom{n_k}{2}+\sum_{k'}\binom{\hat{n}_{k'}}{2}\right] - \left[\sum_k\binom{n_k}{2}\sum_{k'}\binom{\hat{n}_{k'}}{2}\right]/\binom{n}{2} }, 
\]
where $n_k$ denotes the number of observations contained in the $k$-th cluster described by $\bzs$, $\hat{n}_k'$ is the number of observations in the estimated $k$-th cluster described by $\hat{\bz}$ and $n_{kk'}$ denotes the number of observations that are in the intersection between ground-truth cluster $k$ and the estimated cluster $k'$. The ARI lies between $0$ and $1$, with $1$ indicating a perfect agreement between the two partitions and $0$ that the two partitions are random. 

The NCE is defined by 
$$
\text{NCE}(\bzs,\hat{\bz}) = 1 - \frac{d_{0,\sim}(\bzs,\hat{\bz})}{n/K},
$$
where the distance $d_{0,\sim}$ between two partitions $\bz, \bzs$ is given by Eq. \eqref{d0sim}. 
% \begin{align*}
% d_{0,\sim}(\bzs,\bz)&=\underset{\sigma\in\mathfrak{S}(\left\{1,\ldots,K\right\})}{\min}\sum_{k=1}^K\sum_{i=1}^n \zs_{ik} z_{i\sigma(k)}.
%\end{align*}
The NCE lies in $[0, 1]$, with $0$ indicating a perfect estimation of $\bzs$.

Regarding the quality of the segmentation part, we compute the Hausdorff distance (\cite{BraultOSL18}), defined by
\begin{align*}
d_{\text{Haus}}(\mathbf{T}^*;\hat{\mathbf{T}}) = \max_{k\in \{1,2,3\}} \max_{1 \leq \ell \leq L_k} \left\{ \frac{|T_{k\ell}^* - \hat{T}_{k\ell}|}{d}\right\}.
\end{align*}
Note that $d_{\text{Haus}} \in [0,1]$, where $0$ indicates a perfect matching between the ground-truth and the estimated break-points. 

Finally, we propose to evaluate the quality of the parameter estimation with respect to the real values, by taking the difference between the ground-truth parameters (obtained by using the projection of $f_{k\ell}$, without additional noise) and the estimated ones. 
%We study the univariate case $p=1$ to get easier analysis. 
%We vary the sample size $n\in\{100, 1000\}$ to see the asymptotic effect. We vary the dimension $d\in\{50, 100\}$. 
%We generate a mixture with $3$ components with equal size ($\pi_k=\frac13$) pour tout $k\in\{1,2,3\}$, and each cluster has a specific structure into segment: the first cluster has one break-point (two segments of size $d/2$), the second cluster has two break-points (three segments of size $d/3$) and the third cluster has three break-points (four segments of size $d/4$). 
%To see the effect of the signal-to-noise ratio, we fix the variance $\sigma_{k\ell}=1$ for all cluster $k\in\{1,\ldots,3\}$ and segment $\ell \in\{1,\ldots,L_k\}$, and we define several settings by varying the mean in the set $\{1,2,3,4,5\}$ for every cluster $k\in\{1,\ldots,3\}$ and segment $\ell \in\{1,\ldots,L_k\}$. 
%At each break-point, we change the sign of the mean (not its value).
%For example, for the setting with mean $1$, $\mu_{11}= \mu_{22} = \mu_{31} = \mu_{33} = 1$ and $\mu_{12}= \mu_{21} = \mu_{23} = \mu_{32} = \mu_{34} = -1$.
 

\begin{figure}[!h]
\includegraphics[width=\textwidth]{Fig/Exemple_simu9.png}
\caption{Illustration of the different settings for $K=3$ with $(\alpha =0.1$ (left column), $0.2$ (middle column) and $1$ (right column). The number of breakpoints is $L_1=1, L_2=2, L_3=3$.}
\label{fig:dgp}
\end{figure} 

%For each setting, we first check the clustering.
%We suppose $K$ and $(L_k)_{1\leq k \leq K}$ are known. 

\subsection{Results}
%To do so, we represent for each setting the ARI  which measure how two partitions are close, closer to 1 is the ARI, better is the clustering. As we generate the data, we know the true partition. 
ARI and NCE results are summarized in Table \ref{tab:ARI}. We observe that our method behave as expected. As $\alpha$ increases (i.e. the task becomes easier), ARI and NCE increases and decreases, respectively. We obtain a perfect clustering when $\alpha=1$. In addition, we recover the results of theorem~\ref{th:consistence}: one can see that as the number of observations ($d$) and the number of individuals ($n$) increases, the classification gets better (contrary to the one dimensional mixture model where the proportion of errors tends toward a limit value even if the number of individuals keeps increasing). 

%\VB{On voit que plus le nombre d'observations et d'individus augmentent, meilleure est la classification. On retrouve ainsi les résultats du théorème~\ref{th:consistence} (contrairement à un modèle de mélange à une dimension où les erreurs vont avoir tendances à se stabiliser quand on augmente le nombre d'individus).}
\begin{table}[t]
    \centering
    \caption{ARI (on the top) and error of classification (below) for different settings: $(n,d)$ in row and $\alpha$ in columns with mean and standard deviation.}
    \label{tab:ARI}
    \begin{tabular}{c}
    \begin{minipage}{0.95\textwidth}
    \begin{center}
    \begin{tabular}{c|ccc}
    \multicolumn{4}{c}{ARI}\\
         \hline
         Setting&\input{Tab/ARI.tex}
    \end{tabular}
    \end{center}
    \end{minipage}
         \\\\
    \begin{minipage}{\textwidth}
    \begin{center}
    \begin{tabular}{c|ccc}
    \multicolumn{4}{c}{Error of classification}\\
         \hline
         Setting&\input{Tab/CE.tex}
    \end{tabular}
    \end{center}
    \end{minipage}
    \end{tabular}
\end{table}

The same trend can is observed for the segmentation results presented in Figure \ref{fig:Haus}. We observe that the quality of the segmentation part (and hence the breakpoints localization) is correlated with the clustering performance aforementioned. 

\begin{figure}[!ht]
\begin{center}
\begin{tabular}{c|c|c}
&$d=50$&$d=100$\\
\hline
&&\\
\rotatebox{90}{\!\!\!\!\!$n=100$}&\begin{minipage}{0.4\textwidth}
\includegraphics[width=\linewidth]{Fig/TL_n_100_d_50.pdf}
\end{minipage}&
\begin{minipage}{0.4\textwidth}
\includegraphics[width=\linewidth]{Fig/TL_n_100_d_100.pdf}
\end{minipage}\\
%&\\
\hline
&&\\
%&\\
\rotatebox{90}{\!\!\!\!\!$n=1000$}&\begin{minipage}{0.4\textwidth}
\includegraphics[width=\linewidth]{Fig/TL_n_1000_d_50.pdf}
\end{minipage}&
\begin{minipage}{0.4\textwidth}
\includegraphics[width=\linewidth]{Fig/TL_n_1000_d_100.pdf}
\end{minipage}\\
\end{tabular}
\end{center}
\caption{Boxplots of the Hausdorff distance following the number of curves (rows), observations (columns) and difficulties (colors). }
\label{fig:Haus}
\end{figure} 

\begin{figure}[!ht]
\begin{center}
\begin{tabular}{c|c|c}
&$d=50$&$d=100$\\
\hline
&&\\
\rotatebox{90}{\!\!\!\!\!$n=100$}&\begin{minipage}{0.4\textwidth}
\includegraphics[width=\linewidth]{Fig/Mu_n_100_d_50.pdf}
\end{minipage}&
\begin{minipage}{0.4\textwidth}
\includegraphics[width=\linewidth]{Fig/Mu_n_100_d_100.pdf}
\end{minipage}\\
%&\\
\hline
&&\\
%&\\
\rotatebox{90}{\!\!\!\!\!$n=1000$}&\begin{minipage}{0.4\textwidth}
\includegraphics[width=\linewidth]{Fig/Mu_n_1000_d_50.pdf}
\end{minipage}&
\begin{minipage}{0.4\textwidth}
\includegraphics[width=\linewidth]{Fig/Mu_n_1000_d_100.pdf}
\end{minipage}\\
\end{tabular}
\end{center}
\caption{Boxplots of the difference $\widehat{\mu}_{k\ell}$ and $\mu^\star_{k\ell}$ following the number of curves (rows), observations (columns), difficulties (x-axis of each graphic) and cluster (colors).}
\label{fig:TCL:Mu}
\end{figure} 

Finally, Figure \ref{fig:TCL:Mu} shows the performance of our approach on the estimation of the model parameters. First, we recover the consistency results stated in corollary~\ref{cor:consistency}. We also observe that the parameters from cluster 1 are better estimated than the ones from cluster 3. This comes as no surprise as the number of breakpoints is less for cluster 1, resulting in more observations to perform the parameters estimation.
%\VB{Pour cela, nous prenons un nombre de courbes $n\in\{100,1000\}$ et un nombre de jours $d\in\{50,100\}$ de $p=32$ heures (vérifier si $p$ sera toujours la notation car on l'utilise pour deux choses différentes). Nous supposons qu'il existe $K=3$ groupes respectivement avec $L_1=1$, $L_2=2$ et $L_3=3$ ruptures. Pour chaque groupe $k$ et chaque individu $i$ de ce groupe, nous suppons que pour tout $\ell\in\{0,\ldots,\Lk\}$, nous avons~:
% \[\]
% }

%\CL{Our model is based on the expansion of the curves in the Haar wavelet basis. Pour les histoires de niveaux (le fait qu'on ai prit 3) on peut en parler au moment des xp?}


\section{Real Data Analysis}
\label{sec:real_data}
We propose to apply our model to analyze electricity consumption using the Enedis Open Data Set\footnote{available at \href{https://data.enedis.fr/pages/accueil/?id=init}{https://data.enedis.fr/pages/accueil/?id=init}}. We focus on the year 2020 (52 weeks), corresponding to the outburst of the COVID-19 pandemic.
We built 984 observations by cross-referencing information on the type of contract subscribed to, the customer profile and the region of France. Out of these 984 observations, we removed those with missing values to obtain a final population of $889$ individuals. The curves are observed in kW every 30 minutes. We have chosen to analyze the curves by considering the week as a time unit of interest, hence we project those curves onto the Haar basis with $p=42$ ($d=52$ weeks).

\paragraph{Remark} We first ran our approach on such data. The result of the model selection (see next paragraph) resulted in 2 clusters. After having analyzed these results, we found that the model isolated all profiles associated with public lighting contracts, which are not subject to a notion of energy consumption behavior in the sense that interests us.
For this reason, we chose to discard these observations in the following, which give us a final population of $791$ observations.

\subsection{Model Selection}
When dealing with real data, we have no access to the \textit{true} number of clusters or the number of breakpoints. We therefore propose to adapt a model selection strategy proposed in \cite{zhang2007modified} relying on the Bayesian information criterion (BIC) (\cite{Schwarz_1978}). We obtain the following criterion for our model:
\begin{eqnarray*}
\text{BIC}(K, \mathbf{L})&=&\max_{\btheta,\bT}\text{lik}\left(\bY;K,\bT,\btheta\right)-\underbrace{\frac{K-1}{2}\log (n)}_{\text{for }\bpi}\\
&&\underbrace{-\frac{1}{2}\sum_{k=1}^K\left[3p(\Lk+1)\log(ndp)+\sum_{\ell=0}^{\Lk}\log\left(\frac{\widehat{T}_{k,\ell+1}-\widehat{T}_{k\ell}}{d}np\right)\right]}_{\text{for }\bmu\text{ and }\bT}\\
&&\underbrace{-\frac{K}{2}\log(ndp)}_{\text{for }\bsigma}.\\
\end{eqnarray*}
The general form of the penalty in this criterion allow us to account for the specificities of all parameters.

Since an exhaustive exploration of the number of clusters and breakpoints is not possible, we adapt the \texttt{bikm1} strategy proposed by \cite{robert2021bikm1}. Given a reference configuration (the current state of the model), we proceed as follows: 
\begin{itemize}
    \item \textbf{Backward Search}: we remove a cluster (K possible options).  For each of these options, we make 10 random initializations as well as a random distribution of the observations of the deleted cluster in the remaining ones. 
    \item \textbf{Forward Search} : we add a cluster with 1 breakpoint and make 10 random initializations. 
    \item \textbf{Number of breakpoints}: we proceed with the same principles (backward and forward searches) for the number of breakpoints (with $K$ fixed). 
\end{itemize}

In the end, we obtain $K=3$ clusters and the number of breakpoints within each cluster is given by $L_1=1$, $L_2=2$ and $L_3=6$. Figure \ref{fig:rep-cluster} shows the distribution of our observations within clusters and the locations of the different breakpoints.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Fig/Frise_modif_BIC5.png}
    \caption{Distribution of the observations across clusters (horizontal dashed lines) and localization of the break points within each cluster (vertical dashed lines). Vertical dot points areas correspond to the start and end weeks of the lockdown periods. }
    \label{fig:rep-cluster} %%%% proportion des clusters : log(1+table(res_ref@z))
\end{figure}
We observe that cluster 3 only contains one observation and hence overfit on the number of breakpoints. On the basis of the information at our disposal (type of contract, profile and geographic area), we have not been able to draw interesting conclusions for this particular observation\footnote{We are currently investigating this point with the help of Enedis}. Aside from that particular point, the obtained clusters are essentially shaped to distinguish the profiles built by Enedis (see Table \ref{tab:cluster-profil}). We believe that choosing a less penalizing BIC criterion could reveal less discriminating effects than the profile, such as a regional effect for instance.
%Remarques: très déséquilibté. 1 cluster pour 1 seule observation. (décrire l'observation)/ Overfitting nombre de segment pour ce cluster en particulier. 
%RES2 (+RES5) Auvergne Rhone Alphes (108).

\begin{table}
    \centering
    \caption{Contingency table between clusters and Enedis profiles.}
    \label{tab:cluster-profil}
    \begin{tabular}{c|c||c|c|c}
            \multicolumn{2}{c||}{}&ENT/PRO&RES1, 3 or 4 & Other RES\\\hline\hline
  \multirow{3}{*}{\rotatebox{90}{$\!$\small{Cluster}}}&1&     341&       36&       182\\\cline{2-5}
  &2&       0&       0&        231\\\cline{2-5}
  &3&       0&         0&        1\\
    \end{tabular}
\end{table}

\subsection{Results Discussion}
Let us begin by recalling the two periods of lockdown observed in France in 2020: the first one lasted from March $17^{th}$ until May $11^{th}$, while the second one started on October $30^{th}$ and ended on December $15^{th}$. For cluster 1, which is mainly composed of enterprises and professionals, we observe a unique breakpoint that matches with the beginning of the first lockdown. We note that for this cluster no other breakpoints are observed. We can make two assumptions regarding this point: (1) it indicates that the regime change operated by these consumers did not return to \textit{normal} \footnote{here normal refers to prior to the crisis} after the beginning of the crisis; (2) the return to this so-called \textit{normal} regime did not occur at the same moment for all individuals. For the second cluster, corresponding mainly to the Residential profiles, we observe two breakpoints, each appearing during the lockdown period (mid April and end of November). Once again, two hypothesis are in order: (1) a delayed effect of each lockdown or (2) the impact of outdoor temperatures, particularly high in France between mid-April and the end of November. Finally, for the last cluster, the fact that it only contains one observation does not allow us to draw significant conclusions. We can only note that most of the breakpoints matches breakpoints from both cluster 1 and 2. Additional results are provided in Figures \ref{SI:fig:cluster1}, \ref{SI:fig:cluster2} and \ref{SI:fig:cluster3} in the Supplementary.

\section{Conclusion}
\label{sec:conc}
In this paper, we define a novel model to analyze multivariate functional data by performing clustering and segmentation simultaneously. We derive an EM algorithm where the maximization step is carried out by dynamic programming.   From a theoretical point of view, we establish the identifiability and the consistency of the proposed model. We apply this model on synthetic data to control the behavior and validate our theoretical statements. We also demonstrate the usefulness of our model on real electricity consumption data by focusing on the year 2020 corresponding to the outbreak of the COVID pandemic.

This work can be further extended in different directions. On the estimation part, three developments can be considered. To speed up the parameter estimation, we could adapt the pruned dynamic programming algorithm proposed by \cite{rigaill2015pruned} and recently improved by~\cite{maidstone2017optimal}, who proposed to prune the set of candidate change-points rather than looking at all possible cases. In order to circumvent this speed problem, a second alternative could be to replace the current maximization step with a group-lasso procedure for the parameter estimation. For instance \cite{brault2017efficient} proposed to transform the problem into an equivalent estimate of a linear regression whose parameter of interest would be sparse and thus to use the LASSO (Least Absolute Shrinkage and Selection Operator) procedures. The main challenge of these methods being the regularization parameter of the LASSO method and the multiplication of the number of estimated breakpoints. Moreover, in relation to the real observed data, a non parametric extension based on rank statistics (see \cite{brault2018nonparametric}) can be studied.%fthe values coming from the summer would therefore probably be put in separate segments.
Finally, regarding the model selection criterion proposed in the experimental part, a theoretical study of the latter would allow us to define the most appropriate form for the penalty. Indeed, the first results appear to indicate that the proposed version is too penalizing, not allowing to highlight fine-grained information. 
%- pour la partie estimation vitesse - cite rigail / alternative prendre le point de vue du lasso (ref, ref). défi sur l'aspect optimisation (ref). \\
%- amélioration du critère de sélection modèle. Etudier théoriquement afin de trouve la pénalité la plus adéquate possible. 


\appendix

\section{Identifiability}
\label{App:identifiability}
To prove Theorem~\ref{Th:Identifiability}, we need the following lemma:
\begin{lem}[Identifiability for the breakpoints model]\label{lem:Identifiability} We define a $L$-breakpoints model of $p$-dimensional spherical Gaussian of length $d$ with the parameters $0=T_0<T_1<\cdots<T_L<T_{L+1}=d$, $\bmu\in\mathcal{M}_{(L+1)\times p}\left(\setR\right)$ and $\bsigma\in\left(\setR_{\star}^+\right)^{L+1}$ with the following likelihood for every $\bx\in\setR^{d\times p}$:
\[p(\bx;\bT,\bmu,\bsigma)=\prod_{\ell=0}^L\prod_{j=T_{\ell}+1}^{T_{\ell+1}}\prod_{r=1}^pf\left(x_{jr};\mu_{lr},\sigma_{\ell r}\right)\]
where $f$ is the density of an univariate Gaussian distribution. We assume that:
\begin{enumerate}[label=(ID.\alph*)]
    \item For every $\ell\in\{0,\ldots,L\}$,  there exists $r\in\{1,\ldots,p\}$ such that:
    \[\sigma_{\ell r}\neq\sigma_{\ell+1,r} \text{ or }\mu_{\ell r}\neq\mu_{\ell+1,r}.\]
    \item We have $d\geq L+1$.
\end{enumerate}
Under the assumptions (ID.a) and (ID.b), the model is identifiable.
\end{lem}
\begin{proof}
Let two parameters $\left(L,\bT,\bmu,\bsigma\right)$ and $\left(L',\bT',\bmu',\bsigma'\right)$ satisfying the assumptions~(ID.a) and~(ID.b), and $\bX$ and $\bX'$ the random matrices depending of each parameter. We assume that $\bX$ and $\bX'$ have the same distribution. To prove that the parameters are equal, we use the characteristic function defined for every $\bxi\in\setR^{d\times p}$ by:
\[\Phi_{\bX}\left(\bxi\right)=\Esp{e^{\Scal{i\bxi}{\bX}}}\]
where $\Scal{\cdot}{\cdot}$ is the scalar product and $i$ is the imaginary number, satisfying $i^2=-1$. In our case, we have, for every $\bxi\in\setR^{d\times p}$:
\begin{align*}
\Phi_{\bX}\left(\bxi\right)&=\Esp{e^{\Scal{i\bxi}{\bX}}} =\prod_{j=1}^d\Esp{e^{\Scal{i\bxi_j}{\bX_j}}}\\
&=\prod_{\ell=0}^L\prod_{j=T_{\ell}+1}^{T_{\ell+1}}\prod_{r=1}^p\Esp{e^{\Scal{i\xi_{jr}}{X_{jr}}}} =\prod_{\ell=0}^L\prod_{j=T_{\ell}+1}^{T_{\ell+1}}\prod_{r=1}^p\left(e^{i\xi_{jr}\mu_{\ell r}-\frac{\sigma_{\ell r}^2\xi_{jr}^2}{2}}\right)\\
&=\exp\left[\sum_{\ell=0}^L\sum_{j=T_{\ell}+1}^{T_{\ell+1}}\sum_{r=1}^p\left(i\xi_{jr}\mu_{\ell r}-\frac{\sigma_{\ell r}^2\xi_{jr}^2}{2}\right)\right]\\
&=\exp\left[i\sum_{\ell=0}^L\sum_{r=1}^p\mu_{\ell r}\sum_{j=T_{\ell}+1}^{T_{\ell+1}}\xi_{jr}-\sum_{\ell=0}^L\sum_{r=1}^p\frac{\sigma_{\ell r}^2}{2}\sum_{j=T_{\ell}+1}^{T_{\ell+1}}\xi_{jr}^2\right].
\end{align*}
Let $\bw$ be the vector of $\{0,\ldots,L\}^{d}$ with $w_{j}=\ell$ if and only if the $T_{\ell}+1\leq j\leq T_{\ell+1}$. Then, 
\begin{align*}
\Phi_{\bX}\left(\bxi\right)&=\exp\left[i\sum_{j=1}^d\sum_{r=1}^p\xi_{jr}\mu_{w_{j}r}-\frac{1}{2}\sum_{j=1}^d\sum_{r=1}^p\xi_{jr}^2\sigma_{w_jr}^2\right].
\end{align*}
The distribution of $\bX$ and $\bX'$ are equal if and only if the characteristic functions are equal: for all $\bxi\in\setR^{d\times p}$, we have
\begin{eqnarray*}
&&\!\!\!\!\!\!\!\!\!\Phi_{\bX}\left(\bxi\right)=\Phi_{\bX'}\left(\bxi\right)\\
%&\Leftrightarrow&\exp\left[i\sum_{j=1}^d\sum_{r=1}^p\xi_{jr}\mu_{w_{j}r}-\frac{1}{2}\sum_{j=1}^d\sum_{r=1}^p\xi_{jr}^2\sigma_{w_jr}^2\right]=\exp\left[i\sum_{j=1}^d\sum_{r=1}^p\xi_{j r}\mu'_{w'_{j}r}-\frac{1}{2}\sum_{j=1}^d\sum_{r=1}^p\xi_{j r}^2{\sigma'_{w'_j,r}}^2\right]\\
&\Leftrightarrow&i\sum_{j=1}^d\sum_{r=1}^p\xi_{j r}\mu_{w_{j}r}-\frac{1}{2}\sum_{j=1}^d\sum_{r=1}^p\xi_{j r}^2\sigma_{w_j,r}^2=\\
&&\quad\quad\quad\quad\quad i\sum_{j=1}^d\sum_{r=1}^p\xi_{j r}\mu'_{w'_{j}r}-\frac{1}{2}\sum_{j=1}^d\sum_{r=1}^p\xi_{j r}^2{\sigma'_{w'_j,r}}^2\\
&\Leftrightarrow&i\sum_{j=1}^d\sum_{r=1}^p\xi_{j r}\left(\mu_{w_{j}r}-\mu'_{w'_{j}r}\right)-\frac{1}{2}\sum_{j=1}^d\sum_{r=1}^p\xi_{j r}^2\left(\sigma_{w_jr}^2-{\sigma'}_{w'_jr}^2\right)=0.
\end{eqnarray*}
A polynomial is null if and only all coefficients are null:
\[\text{for every $j\in\{1,\ldots,d\}$, }\left\{\begin{array}{l}
 \text{for every $r\in\{1,\ldots,p\}$, }\mu_{w_{j}r}=\mu'_{w'_{j}r}\\
 \text{ and }\sigma_{w_jr}^2={\sigma'}_{w'_j}^2. 
\end{array}\right.\]
By the assumption (ID.b), we know that there is at least one observation by segment (by definition of $\bT$ and $\bT'$) then, by definition also, $w_1=0=w'_1$ which implies that
\[\text{for every $r\in\{1,\ldots,p\}$, }\mu_{0,r}=\mu'_{0,r}
 \text{ and }\sigma_{0,r}^2={\sigma'}_{0,r}^2.\]
Then, if we assume that $T_1\neq T'_1$ (for example, $T'_1>T_1$), we observe that $w_{T_1+1}=0$ and $w'_{T_1+1}=1$ but we know that:
\begin{eqnarray*}
&&\!\!\!\!\!\!\!\!\!
 \text{for every $r\in\{1,\ldots,p\}$, }\mu_{w_{T_1+1},r}=\mu'_{w'_{T_1+1},r}
 \text{ and }\sigma_{w_{T_1+1}}^2={\sigma'}_{w'_{T_1+1}}^2\\
 &\Leftrightarrow& \text{for every $r\in\{1,\ldots,p\}$, }\mu_{0,r}=\mu'_{1r}
 \text{ and }\sigma_{0,r}^2={\sigma'}_{1r}^2\\
 &\Leftrightarrow& \text{for every $r\in\{1,\ldots,p\}$, }\mu'_{0,r}=\mu'_{1r}
 \text{ and }{\sigma'}_{0,r}^2={\sigma'}_{1r}^2\\
\end{eqnarray*}
by the previous results. This affirmation contradicts the assumption (ID.b), therefore $T_1= T'_1$.
We then continue with $w_{T_1+1}=1=w'_{T_1+1}$ and by an identical reasoning, we show that
\[\text{for every $r\in\{1,\ldots,p\}$, }\mu_{1r}=\mu'_{1r}
 \text{ and }\sigma_{1r}^2={\sigma'}_{1r}^2\]
and so on until the segment $\left[T_{\min{L,L'}}+1;T_{\min{L,L'}+1}\right]$. If $L=L'$, the proof is finished since we prove that all the parameters are identical.
If $L\neq L'$, for example $L>L'$, we observe that $w_{T_{L'+1}+1}=L+1$ (since by the previous reasoning $T_L=T'_{L}$ and $T_{L'}<T_{L'+1}<d$) and $w'_{T_{L'+1}+1}=L'=L$ then, by the same reasoning, this implies:
\begin{eqnarray*}
&&\!\!\!\!\!\!\!\!\!
 \text{for every $r\in\{1,\ldots,p\}$, }\mu_{w_{T_{L'+1}+1},r}=\mu'_{w'_{T_{L'+1}+1},r}\\
 && 
 \text{ and }\sigma_{w_{T_{L'+1}+1},r}^2={\sigma'}_{w'_{T_{L'+1}+1},r}^2\\
 &\Leftrightarrow& \text{for every $r\in\{1,\ldots,p\}$, }\mu_{L'+1,r}=\mu'_{L',r}
 \text{ and }\sigma_{L'+1,r}^2={\sigma'}_{L',r}^2\\
 &\Leftrightarrow& \text{for every $r\in\{1,\ldots,p\}$, }\mu_{L'+1,r}=\mu_{L',r}
 \text{ and }{\sigma}_{L'+1,r}^2={\sigma}_{L',r}^2
\end{eqnarray*}
which again contradicts the assumption~(ID.b).
Then, $L=L'$ and, the parameters being identical, the model is identifiable. 
\end{proof}

To prove Theorem~\ref{Th:Identifiability}, we start by observing that the assumptions~(ID.1) and~(ID.3) imply the assumptions~(ID.a) and~(ID.b) of Lemma~\ref{lem:Identifiability} and, with the assumptions~(ID.2), we have that the distribution of each cluster is unique.
As the image of the distribution functions by any isomorphism defined on the vector subspace generated by the set of distribution functions is a free family in the arrival space, then model is identifiable (see~\cite{droesbeke2013modeles}).

% \section{$\Deltakc$ calculation formula}\label{sec:app:deltakx}

% In this section, we present the different values for the estimation of $\Deltakc$ depending on the dependencies of $\sigmaklr$:
% \begin{enumerate}
% \item If $\bsigma$ depends on the cluster $k$ and the segment $\ell$ then     \[\Deltakc(s;t)=\spluskc(t-s)r\log\left(\sigmah\right)+\frac{1}{\sigmah}\underset{j=s+1}{\overset{t}{\sum}}\underset{i=1}{\overset{n}{\sum}}\sikc\underset{r=1}{\overset{p}{\sum}}\left(\Yijr-\muh_r\right)^2\]
%     with
%     \[\sigmah=\underset{i=1}{\overset{n}{\sum}}\frac{\sikc}{\spluskc(t-s)r}\underset{j=s+1}{\overset{t}{\sum}}\underset{r=1}{\overset{p}{\sum}}\left(\Yijr-\Yibarrc{s}{t}\right)^2.\]
% \item If $\bsigma$ depends on the cluster $k$ then:
% \begin{eqnarray*}
% &&\!\!\!\!\!\!\!\!\!\underset{\left(\btheta,\bT\right)}{\max}\;Q\left(\btheta,\bT\left|\bthetac,\bTc\right.\right)\\
% &=&-\frac{1}{2}\underset{k=1}{\overset{K}{\sum}}\left\{\spluskc dr\log\left(\sigmah_k\right)+\frac{1}{\sigmah_k}\left[\underset{\small0=T_{k0}<T_{k1}<\cdots<T_{k,\Lk+1}=d+1}{\min}\underset{\ell=0}{\overset{\Lk}{\sum}}\Deltatildekc\left(T_{k \ell};T_{k,\ell+1}\right)\right]\right\}\\
% &&-\frac{ndp}{2}\log\left(2\pi\right)+\underset{k=1}{\overset{K}{\sum}}\spluskc\log\pik\\
% \end{eqnarray*}
% with
% \[\Deltatildekc(s;t)=\spluskc(t-s)\underset{j=s+1}{\overset{t}{\sum}}\underset{i=1}{\overset{n}{\sum}}\sikc\underset{r=1}{\overset{p}{\sum}}\left(\Yijr-\muh_r\right)^2\]
% and
% \[\sigmah_k=\frac{1}{\spluskc dr}\left[\underset{\small0=T_{k0}<T_{k1}<\cdots<T_{k,\Lk+1}=d+1}{\min}\underset{\ell=0}{\overset{\Lk}{\sum}}\Deltatildekc\left(T_{k \ell};T_{k,\ell+1}\right)\right]\]
% \end{enumerate}

\section{Consistency}
\label{app:Cons}
%\subsection{Régularité de la vraie matrice}
%In the first step, we prove that the true partition given by $\bzs$ is regular enough such that clusters are not too small. 
%La première étape consiste à remarquer que la vraie partition représentée par $\bzs$ est suffisamment régulière pour ne pas avoir de groupes trop petits. Pour cela, nous avons la proposition suivante:


\subsection{Notations and first results}%Espérance et variables d'intérêts}
Let $\bzs$ be the true partition, and  $\bz$ any partition.

We denote $\RK$ the matrix of size $K$ about contingency of intersection of the two clusterings: for all $(k,k')\in\{1\ldots,K\}^2$,
\begin{align}
\RK_{k,k'}=\sum_{i=1}^n\zs_{ik}z_{i,k'}.
\label{RK}
\end{align}
Particularly, remark that marginals give the contingency table: 
\begin{align}
&\forall k\in\{1,\ldots,K\}, \RK_{k,+}=\zs_{+,k} \label{Rk+}\\
&\forall k'\in\{1,\ldots,K\}, \RK_{+,k'}=\zs_{k',+} \label{R+k'}.
\end{align}

Also, if the two partitions are equal up to label switching, then $\RK$ is diagonal up to a permutation of rows and columns. 

Similarly, we denote $\NLk$ the same matrix for the segments: for all 
$(k,k')\in\{1\ldots,K\}^2$, for all $(\ell,\ell')\in\{1\ldots,L_k\}\times\{1\ldots,L_{k'}\}$,
\begin{align}
\NLk_{\ell,\ell'} %&=\left|\left\{j\in\{1,\ldots,d\}\left|\Ts_{k\ell}+1\leq j\leq\Ts_{k,\ell+1}\text{ et }T_{k'\ell'}+1\leq j\leq T_{k',\ell'+1}\right.\right\}\right|\\
&=\left|{D^\star}_{\ell}^{k}\bigcap{D}_{\ell'}^{k'}\right|
\label{NLk}
\end{align}
where
\begin{align*}
{D}_{\ell'}^{k'}&=\left\{j\in\{1,\ldots,d\}\left|T_{k'\ell'}+1\leq j\leq T_{k',\ell'+1}\right.\right\},\\
{D^\star}_{\ell'}^{k'}&=\left\{j\in\{1,\ldots,d\}\left|T^\star_{k'\ell'}+1\leq j\leq T^\star_{k',\ell'+1}\right.\right\}. 
\end{align*}
Remark that 
\begin{align}
\sum_{\ell_1=1}^{L_{k_1}}\NL{k_1}{k}_{\ell_1,\ell}
=\left|{D^\star}_{\ell}^{k}\right|.
\label{sumNk=Dk}
\end{align}

For every $(\btheta,\bT,\bz)$, we denote 
\[\Fntz=\log\frac{p(\bY|\bz;\btheta,\bT)}{p(\bY|\bzs;\bthetas,\bTs)},\]
and its expectation 
\[\Gntz=\Espc{\bY|\bzs;\bthetas,\bTs}{\Fntz}.\]

In the following, we study the Kullback-Leibler divergence between two Gaussian distributions with variance 1. As the distributions only depend on the respective means, we denote $\KL{\mu}{\mu'}$ this divergence. Remark that 
\begin{align}
\label{KL}
KL(\mu,\mu') = \frac12 (\mu-\mu')^2.
\end{align}
\begin{prop}[Distribution of $\Fntz$]\label{prop:Fn}
For all $\btheta$, $\bT\in\mathcal{T}_{\tau_{\min}}$ and $\bz$, we get
\begin{eqnarray*}
\Fntz  %\sum_{k=1}^K\sum_{k'=1}^K\sum_{\ell=1}^{\Lk}\sum_{\ell'=1}^{L_{k'}}\left(\mus_{k\ell}-\mu_{k'\ell'}\right)\sum_{i=1}^n\zs_{ik}z_{i,k'}\sum_{j\in{D^\star}_{\ell}^{k}\bigcap{D}_{\ell'}^{k'}} \left(\mus_{k\ell}-Y_{ij}\right)\\
&\sim&\mathcal{N}\left(\Gntz,\Gntz\right).
\end{eqnarray*}
with 
\begin{eqnarray*}
&&\!\!\!\!\!\!\Gntz\\
&=&\!\!\!\!-\!\sum_{k=1}^K\!\sum_{k'=1}^K\!\sum_{\ell=1}^{\Lk}\!\sum_{\ell'=1}^{L_{k'}}\KL{\mus_{k\ell}}{\mu_{k'\ell'}}\RK_{k,k'}\NLk_{\ell,\ell'}.%;\\
%v_n(\btheta, \bT, \bz) &= \sum_{k=1}^K\sum_{k'=1}^K\sum_{\ell=1}^{\Lk}\sum_{\ell'=1}^{L_{k'}}\left(\mus_{k\ell}-\mu_{k'\ell'}\right)^2\RK_{k,k'}\NLk_{\ell,\ell'}.
\end{eqnarray*}
\end{prop}

The proof stands in Section \ref{sec:tools}.




We are also particularly interested in the maximum of $g_n$ %Enfin, nous notons les maximums de ces deux fonctions:
\begin{eqnarray*}
\Lambdat(\bz,\bT)&=&\max_{\btheta\in\Theta}\Gntz\\
\end{eqnarray*}
%\end{defi}

\begin{prop}\label{prop:lambdat}
For all $\bz, \bT$, 
\begin{eqnarray*}
&&\!\!\!\!\!\!\!\!\!\!\!\!\Lambdat(\bz,\bT)\\
&=&\!\!\!\!-\frac{1}{2}\sum_{k=1}^K\sum_{\ell=1}^{L_{k}}\frac{1}{\left|{D^\star}_{\ell}^{k}\right|\bzs_{+,k}}\sum_{k_1=1}^K\sum_{\ell_1=1}^{L_{k_1}}\RK_{k_1,k}\NL{k_1}{k}_{\ell_1,\ell}\\
&&\times\sum_{k_2=1}^K\sum_{\ell_2=1}^{L_{k_2}}\RK_{k_2,k}\NL{k_2}{k}_{\ell_2,\ell}\KL{\mus_{k_1,\ell_1}}{\mus_{k_2,\ell_2}}\\
\end{eqnarray*}
\end{prop}

The proof stands in Section \ref{sec:tools}.

Then, we need a measure of the minimal difference between two different parameters, computed through the Kullback-Leibler divergence. 
%Nous avons besoin d'une notation pour expliquer la différence minimale entre deux paramètres différents:
\begin{defi}[Minimal Kullback-Leibler divergence]
Let $\delta(\bthetas)$ be the minimal nonzero Kullback-Leibler divergence: 
\[\delta(\bthetas)=\min_{\underset{\mus_{k_1,\ell_1}\neq \mus_{k_2,\ell_2}}{k_1,\ell_1,k_2,\ell_2}}\KL{\mus_{k_1,\ell_1}}{\mus_{k_2,\ell_2}}>0.\]
\end{defi}

\subsection{Proof of Proposition \ref{prop:part_equi}: equivalent partitions}
Let $\sigma\in\mathfrak{S}(\left\{1,\ldots,K\right\})$ be a permutation. We say that $(\bpi, \bmu,\bT)$ has a \emph{symmetry} for   $\sigma$ if we have, for all  $k\in\{1,\ldots,K\}$:
\[\pi_{\sigma(k)}=\pik,\,\forall\ell\in\{1,\ldots,\Lk\},\mu_{\sigma(k),\ell}=\mu_{k\ell}\text{ and }T_{\sigma(k),\ell}=T_{k\ell}.\]
%If we denote $\left(\btheta^{(\sigma)},\bT^{(\sigma)}\right) := (\pi_{\sigma(k)},\theta_{\sigma(k),\ell},T_{\sigma(k),\ell})$,it means  $\left(\btheta^{(\sigma)},\bT^{(\sigma)}\right)=\left(\btheta,\bT\right)$.
We denote  $\text{Sym}(\btheta, \bT)$ the set of permutations such that  $\left(\btheta,\bT\right)$ has a symmetry. % and  $\#\text{Sym}\left(\btheta,\bT\right)$ its cardinal. 

Remark that under Assumption (ID.3), we have
\[\#\text{Sym}\left(\btheta,\bT\right)=1,\]
which makes the next computations easier than in \cite{brault2020consistency}, where we directly get, in our particular case,  $\{\sigma\in\mathfrak{S}_{\bz,\bzs}\} = \{(\btheta',\bT')\sim(\btheta,\bT)\}$.

Let $\sigma$  be a permutation such that for all  $i\in\{1,\ldots,n\}$ and $k\in\{1,\ldots,K\}$, we have
\[z_{i\sigma(k)}=\zsik,\]
$\mathfrak{S}_{\bz,\bzs}$ the set of all possible permutations and we denote $\bz^{(\sigma)} :=\left(z_{i\sigma(k)}\right)_{ik}$. We have: 
\[p(\bY,\bz;\btheta,\bT)=p\left(\bY,{\bzs}^{(\sigma)};\btheta,\bT\right)=p\left(\bY,\bzs;\btheta^{(\sigma)},\bT^{(\sigma)}\right).\]
If   $\sigma\in\text{Sym}(\btheta)$, it leads to 
$p(\bY,\bz;\btheta,\bT)=p\left(\bY,\bzs;\btheta,\bT\right)$. By summing, we get
\begin{eqnarray*}
\sum_{\bz\sim\bzs}p(\bY,\bz;\btheta,\bT)&=&\sum_{\sigma\in\mathfrak{S}_{\bz,\bzs}}p\left(\bY,{\bzs}^{(\sigma)};\btheta,\bT\right)\\
&=&\sum_{\sigma\in\mathfrak{S}_{\bz,\bzs}}p\left(\bY,\bzs;\btheta^{(\sigma)},\bT^{(\sigma)}\right)\\
%&=&\sum_{(\btheta',\bT')\sim(\btheta,\bT)}\#\text{Sym}\left(\btheta',\bT'\right)p\left(\bY,\bzs;\btheta',\bT'\right)\\
&=&\sum_{(\btheta',\bT')\sim(\btheta,\bT)}p\left(\bY,\bzs;\btheta',\bT'\right).\\
%&=&\left(\btheta,\bT\right)\sum_{(\btheta',\bT')\sim(\btheta,\bT)}p\left(\bY,\bzs;\btheta',\bT'\right).
\end{eqnarray*}
However, the function $\btheta\mapsto p\left(\bY,\bzs;\btheta,\bT\right)$ is unimodal and maximal for the maximum of the complete likelihood. As the estimator is consistent as soon as we have the true partition, under Assumption (ID.1), we have $p\left(\bY,\bzs;\btheta,\bT\right)=\mathcal{O}_P\left[p\left(\bY,\bzs;\bthetas,\bTs\right)\right]$ when $\theta$ is in a neighborhood of $\thetas$ and $p\left(\bY,\bzs;\btheta,\bT\right)=o_P\left[p\left(\bY,\bzs;\bthetas,\bTs\right)\right]$ elsewhere. If  $\btheta$ is close to  $\bthetas$, the set of equivalent $\btheta'$ but not symmetric are far and we get:
\[p\left(\bY,\bzs;\btheta',\bT'\right)=o_P\left[p\left(\bY,\bzs;\bthetas,\bTs\right)\right].\]
Then,
\[\sum_{(\btheta',\bT')\sim(\btheta,\bT)}\frac{p\left(\bY,\bzs;\btheta',\bT'\right)}{p\left(\bY,\bzs;\bthetas,\bTs\right)}=\max_{(\btheta',\bT')\sim(\btheta,\bT)}\frac{p\left(\bY,\bzs;\btheta',\bT'\right)}{p\left(\bY,\bzs;\bthetas,\bTs\right)}\left[1+o_P(1)\right].\]




\subsection{Proof of Proposition \ref{prop:contrib_local}: partitions that are close}


\begin{lem}
Assume Ass. (ID.4.s) with parameter $c>0$.
Let the event 
$$\Omega_1(c) = \left\{\bzs\in\mathcal{Z}\left|\forall k\in\{1,\ldots,K\},\,\zs_{+,k}\geq nc/2\right.\right\},$$
 where $\mathcal{Z}$ is the set of all possible partitions. 
Then, 
\[\mathbb{P}_{\thetas}\left(\overline{\Omega_1(c)}\right)\leq K e^{-\frac{nc^2}{2}}.\]
\end{lem}

\begin{proof}
First, remark that for all $k\in\{1,\ldots,K\}$, $\Zs_{+,k} \sim \mathcal{B}(\pis_k)$ with $\pis_k>c$ according to Assumption (ID.4.s). Then, using Hoeffding inequality,

\begin{eqnarray*}
\mathbb{P}_{\thetas}\left(\overline{\Omega_1(c)}\right)&=&\mathbb{P}_{\thetas}\left(\bigcup_{k=1}^K\{\Zs_{+,k}< nc/2\}\right)
\leq \sum_{k=1}^K\mathbb{P}_{\thetas}\left(\Zs_{+,k}< nc/2\right)\\
&\leq&\sum_{k=1}^K\mathbb{P}_{\thetas}\left(\Zs_{+,k}< n\pis_k/2\right)
\leq \sum_{k=1}^K\mathbb{P}_{\thetas}\left(\Zs_{+,k}-n\pis_k< -n\pis_k/2\right)\\
&\leq&\sum_{k=1}^K\exp\left[-\frac{2\left(-n\pis_k/2\right)^2}{\sum_{i=1}^n(1-0)^2}\right]
\leq \sum_{k=1}^K\exp\left[-\frac{2n^2{\pis_k}^2}{4n}\right]\\
&\leq&\sum_{k=1}^K\exp\left[-\frac{n{\pis_k}^2}{2}\right]
\leq \sum_{k=1}^K\exp\left[-\frac{nc^2}{2}\right]
\leq K\exp\left[-\frac{nc^2}{2}\right].
\end{eqnarray*}
\end{proof}

In the next lemma, we split the balls into slices. 
\begin{lem}[Upper bounding of  $\Fntz$]
Under Assumptions (ID.3.s), (C.1) and (C.2), for all $r\geq 1/n$, for all $\btheta\in\bTheta$ and $\bz\in\mathcal{Z}$ such that\linebreak$d_{0,\sim}(\bz,\bzs)=rn$, we have:
\begin{equation}\label{eq:cor:majFn}
    \Fntz\leq-\frac{d\tau_{\min}\delta\left(\bthetas\right)}{2}rn\left[1+o_P(1)\right].
\end{equation}
\end{lem}


\begin{proof}
First, we remark that for all $\btheta$, $\bT\in\mathcal{T}_{\tau_{\min}}$ and $\bz$, we have:
\begin{eqnarray*}
\Fntz&\leq&\Fntz-\Gntz+\Lambdat\left(\bz\right)\\
&\leq&\Fntz-\Gntz-\frac{d\tau_{\min}\delta(\bthetas)}{2}d_{0,\sim}(\bz,\bzs)\\
&\leq&\Fntz-\Gntz-\frac{d\tau_{\min}\delta(\bthetas)}{2}rn.
\end{eqnarray*}
By Lemma~\ref{prop:Fn},  $\Fntz-\Gntz$ is a centered Gaussian.  
We also know that 
\begin{align*}
(\mu_{k\ell} - \mu_{k'\ell'})^2 &\leq (\text{Diam} \Theta)^2\\
\sum_{\ell=1}^{\Lk}\sum_{\ell'=1}^{L_{k'}}\NLk_{\ell,\ell'} &\leq d\\
\sum_{k=1}^K\sum_{k'=1}^K\RK_{k,k'}&\leq n,
\end{align*}
then using Lemma~\ref{lem:inegalitemesouviensplus}, we get for all $t>0$:
\begin{eqnarray*}
\!\!\!\!\!\!\!\!\!\!\!\!\Prob{\Fntz-\Gntz\geq t} \leq\exp\left\{-\frac{t^2}{2\text{Diam}(\bTheta)^2nd}\right\},
\end{eqnarray*}
then for all partition such that  $d_{0,\sim}(\bz,\bzs)=rn$, we have:
\begin{eqnarray*}
&&\Prob{\Fntz-\Gntz\geq \frac{dd_{0,\sim}(\bz,\bzs)\tau_{\min}\delta\left(\bthetas\right)}{2}}\\
&\leq&\exp\left\{-\frac{\frac{d^2\tau_{\min}^2d_{0,\sim}(\bz,\bzs)^2\delta\left(\bthetas\right)^2}{2^2}}{2\text{Diam}(\bTheta)^2nd}\right\}\\
%&\leq&\exp\left\{-\frac{\delta\left(\bthetas\right)^2d\tau_{\min}d_{0,\sim}(\bz,\bzs)rn}{8\text{Diam}(\bTheta)^2n}\right\}\\
&\leq&\exp\left\{-\frac{\delta\left(\bthetas\right)^2d\tau_{\min}d_{0,\sim}(\bz,\bzs)r}{8\text{Diam}(\bTheta)^2}\right\} \underset{n, d \rightarrow +\infty}\longrightarrow 0, 
\end{eqnarray*}
where $d_{0,\sim}$ depends on $n$. Then,
\begin{eqnarray*}
\Fntz&\leq& o_P\left[\frac{d\tau_{\min}d_{0,\sim}(\bz,\bzs)\delta\left(\bthetas\right)}{2}\right]-\frac{d\tau_{\min}\delta\left(\bthetas\right)}{2}rn\\
&\leq&-\frac{d\tau_{\min}\delta\left(\bthetas\right)}{2}rn\left[1+o_P(1)\right]
\end{eqnarray*}
\end{proof}

\begin{lem}[Having different partitions]
For all $c>0$, considering the event $\Omega_1(c)$, we have for all $\tilde{c}\leq c/4$ and $\bz\in\mathcal{B}\left(\bzs,\tilde{c}\right)$:
\begin{equation}\label{eq:demo:borneProbaParition}
    \frac{p\left(\bz;\btheta,\bT\right)}{p\left(\bzs;\bthetas,\bTs\right)}\leq \mathcal{O}_P\left(1\right)e^{M_{c/4}\left\|\bz-\bzs\right\|_0}.
\end{equation}
\end{lem}

\begin{proof}
Considering $\Omega_1(c)$: for all  $k\in\{1,\ldots,K\}$, we have $\zs_{+,k}\geq nc/2$. As $\bz\in\mathcal{B}\left(\bzs,\tilde{c}\right)$ with $\tilde{c}\leq c/4$ then for all $k\in\{1,\ldots,K\}$, we have  $z_{+,k}\geq nc/4$.

For a partition $\bz$, let $\pih{\bz}$ be the maximum of $\bpi\mapsto p\left(\bz;\bpi\right)$:  for all $k\in\{1,\ldots,K\}$,
\[\pih{\bz}_k=\frac{z_{+,k}}{n}.\]

Then, we have
\begin{eqnarray*}
\frac{p\left(\bz;\btheta,\bT\right)}{p\left(\bzs;\bthetas,\bTs\right)}&=&\frac{p\left(\bz;\bpi\right)}{p\left(\bzs;\bpis\right)}
\leq\frac{p\left(\bz;\pih{\bz}\right)}{p\left(\bzs;\pih{\bzs}\right)}\times\frac{p\left(\bz;\pih{\bzs}\right)}{p\left(\bzs;\bpis\right)},
\end{eqnarray*}
by definition of $\pih{\bz}$. Using \cite{brault2020consistency}[Lemma D.2] we get:
\begin{eqnarray*}
\log\left[\frac{p\left(\bz;\pih{\bz}\right)}{p\left(\bzs;\pih{\bzs}\right)}\right]
&=&\log p\left(\bz;\pih{\bz}\right)-\log p\left(\bzs;\pih{\bzs}\right)\\
&=&\sum_{i=1}^n\sum_{k=1}^K\zik\log\pih{\bz}_k-\sum_{i=1}^n\sum_{k=1}^K\zsik\log\pih{\bzs}_k\\
%&=&\sum_{k=1}^K\log\pih{\bz}_k\sum_{i=1}^n\zik-\sum_{k=1}^K\log\pih{\bzs}_k\sum_{i=1}^n\zsik\\
%&=&\sum_{k=1}^Kn\pih{\bz}_k\log\pih{\bz}_k-\sum_{k=1}^Kn\pih{\bzs}_k\log\pih{\bzs}_k\\
&=&n\sum_{k=1}^K\left[\pih{\bz}_k\log\pih{\bz}_k-\pih{\bzs}_k\log\pih{\bzs}_k\right].
\end{eqnarray*}
Let $H(\bpi)=\sum_{k=1}^K\pik\log\pik$. This function is differentiable and we can use the mean value theorem to the function $x\mapsto -x\log x$ with derivative $x\mapsto \log x+1$. Then, for all $k\in\{1,\ldots,K\}$, there exists $\kappa_k\in]\pisk;\pik[$ such that 
\begin{eqnarray*}
&&\frac{\pik\log\pik-\pisk\log\pisk}{\pik-\pisk}=-\log \kappa_k-1\\
\Rightarrow&&\left|\frac{\pik\log\pik-\pisk\log\pisk}{\pik-\pisk}\right|=\left|-\log \kappa_k-1\right|.
%&\Rightarrow&\left|\frac{\pik\log\pik-\pisk\log\pisk}{\pik-\pisk}\right|=\max\left(-\log \kappa_k-1,\log \kappa_k+1\right).
\end{eqnarray*}
However, as $\bpi$ and $\bpis$ are regular enough, we know that $\kappa_k\in]c/4;1-c/4[$ then there exists a constant $M_{c/4}$ such that
\begin{eqnarray*}
\left|\frac{\pik\log\pik-\pisk\log\pisk}{\pik-\pisk}\right| \leq \frac{M_{c/4}}{2}.
\end{eqnarray*}
By summing, we get
\begin{eqnarray*}
\left|H(\bpi)-H(\bpis)\right|&=&\left|\sum_{k=1}^K\pik\log\pik-\sum_{k=1}^K\pisk\log\pisk\right|\\
&\leq&\sum_{k=1}^K\frac{M_{c/4}}{2}\left|\pik-\pisk\right|
\leq\frac{M_{c/4}}{2}\left\|\bpi-\bpis\right\|_1.
\end{eqnarray*}
Finally, we have
\begin{eqnarray*}
&&\!\!\!\!\!\!\!\!\left|\log\left[\frac{p\left(\bz;\pih{\bz}\right)}{p\left(\bzs;\pih{\bzs}\right)}\right]\right|\\
&=&n\left|H(\pih{\bz})-H(\pih{\bzs})\right|
\leq\frac{nM_{c/4}}{2}\left\|\pih{\bz}-\pih{\bzs}\right\|_1\\
&\leq&\frac{nM_{c/4}}{2}\sum_{k=1}^K\left|\frac{z_{+,k}}{n}-\frac{\zs_{+,k}}{n}\right|\\
%\leq\frac{M_{c/4}}{2}\sum_{k=1}^K\left|\sum_{i=1}^nz_{ik}-\sum_{i=1}^n\zs_{ik}\right|\\
&\leq&\frac{M_{c/4}}{2}\sum_{i=1}^n\sum_{k=1}^K\left|z_{ik}-\zs_{ik}\right|
\leq \frac{M_{c/4}}{2}\sum_{i=1}^n\sum_{k=1}^K\mathds{1}_{\{z_{ik}\neq\zs_{ik}\}}\\
&\leq&  M_{c/4}\left\|\bz-\bzs\right\|_0.
\end{eqnarray*}
Indeed, if $i$ does not belong to the true partition, there are two nonzero terms. 

On the other side, by the law of large numbers, we know that
$\pih{\bzs}=\mathcal{O}_P\left(\bpis\right)$ and as $\bpis$ is regular, we have:
\begin{equation}\label{eq:demo:bornefrac2}
    \frac{p\left(\bz;\pih{\bzs}\right)}{p\left(\bzs;\bpis\right)}=\mathcal{O}_P\left(1\right).
\end{equation}
\end{proof}



\begin{proof}[Proof of Proposition \ref{prop:contrib_local}]
Considering $\tilde{c}<c/4$, $\btheta\in\bTheta$ and $\bT\in\mathcal{T}_{\tau_{\min}}$, considering the event $\Omega_1(c)$:
\begin{eqnarray*}
&&\!\!\!\!\!\!\!\!\!\!\!\!\sum_{\underset{\bz\nsim\bzs}{\bz\in\mathcal{B}\left(\bzs;\tilde{c}\right)}}p\left(\bY,\bz;\btheta,\bT\right)\\
&=&\sum_{\underset{\bz\nsim\bzs}{\bz\in\mathcal{B}\left(\bzs;\tilde{c}\right)}}p\left(\bY|\bz;\btheta,\bT\right)p\left(\bz;\btheta,\bT\right)\frac{p\left(\bY|\bzs;\bthetas,\bTs\right)}{p\left(\bY|\bzs;\bthetas,\bTs\right)}\\
&=&\sum_{\underset{\bz\nsim\bzs}{\bz\in\mathcal{B}\left(\bzs;\tilde{c}\right)}}\frac{p\left(\bY,\bzs;\bthetas,\bTs\right)}{p\left(\bzs;\bthetas,\bTs\right)}p\left(\bz;\btheta,\bT\right)\frac{p\left(\bY|\bz;\btheta,\bT\right)}{p\left(\bY|\bzs;\bthetas,\bTs\right)}\\
&=&p\left(\bY,\bzs;\bthetas,\bTs\right)\sum_{\underset{\bz\nsim\bzs}{\bz\in\mathcal{B}\left(\bzs;\tilde{c}\right)}}\frac{p\left(\bz;\btheta,\bT\right)}{p\left(\bzs;\bthetas,\bTs\right)}e^{\Fntz}\\
&=&p\left(\bY,\bzs;\bthetas,\bTs\right)\sum_{\underset{\bz\nsim\bzs}{\bz\in\mathcal{B}\left(\bzs;\tilde{c}\right)}}\mathcal{O}_P\left(1\right)e^{M_{c/4}\left\|\bz-\bzs\right\|_0}e^{-\frac{d\tau_{\min}\delta\left(\bthetas\right)}{2}\left\|\bz-\bzs\right\|_0\left[1+o_P(1)\right]}
\end{eqnarray*}
using Eqs. ~\eqref{eq:cor:majFn} and ~\eqref{eq:demo:borneProbaParition}. Then, 
\begin{eqnarray*}
&&\!\!\!\!\!\!\!\!\!\!\!\!\sum_{\underset{\bz\nsim\bzs}{\bz\in\mathcal{B}\left(\bzs;\tilde{c}\right)}}p\left(\bY,\bz;\btheta,\bT\right)\\
&\leq&p\left(\bY,\bzs;\bthetas,\bTs\right)\mathcal{O}_P\left(1\right)\sum_{R=1}^{\left[\tilde{c}n\right]}\binom{n}{R}K^Re^{M_{c/4}R-\frac{d\tau_{\min}\delta\left(\bthetas\right)}{2}R\left[1+o_P(1)\right]}
\end{eqnarray*}
because there are at most  $\binom{n}{R}K^R$ possible partitions at distance  $R$, and then
\begin{eqnarray*}
&&\!\!\!\!\!\!\!\!\!\!\!\!\sum_{\underset{\bz\nsim\bzs}{\bz\in\mathcal{B}\left(\bzs;\tilde{c}\right)}}p\left(\bY,\bz;\btheta,\bT\right)\\
&\leq&p\left(\bY,\bzs;\bthetas,\bTs\right)\mathcal{O}_P\left(1\right)\sum_{R=1}^{\left[\tilde{c}n\right]}\binom{n}{R}\left(e^{\log K+M_{c/4}-\frac{d\tau_{\min}\delta\left(\bthetas\right)}{2}\left[1+o_P(1)\right]}\right)^R\\
%&\leq&p\left(\bY,\bzs;\bthetas,\bTs\right)\mathcal{O}_P\left(1\right)\sum_{R=0}^{n}\binom{n}{R}\left(e^{\log K+M_{c/4}-\frac{d\tau_{\min}\delta\left(\bthetas\right)}{2}\left[1+o_P(1)\right]}\right)^R1^{n-R}\\
&\leq&p\left(\bY,\bzs;\bthetas,\bTs\right)\mathcal{O}_P\left(1\right)\left(1+e^{\log K+M_{c/4}-\frac{d\tau_{\min}\delta\left(\bthetas\right)}{2}\left[1+o_P(1)\right]}\right)^n\\
%&\leq&p\left(\bY,\bzs;\bthetas,\bTs\right)\mathcal{O}_P\left(1\right)\left(1+e^{\log K+M_{c/4}-\frac{d\tau_{\min}\delta\left(\bthetas\right)}{2}\left[1+o_P(1)\right]}\right)^n\\
&\leq&p\left(\bY,\bzs;\bthetas,\bTs\right)\mathcal{O}_P\left(1\right)\exp\left\{n\log\left[1+e^{\log K+M_{c/4}-\frac{d\tau_{\min}\delta\left(\bthetas\right)}{2}\left[1+o_P(1)\right]}\right]\right\}\\
&\leq&p\left(\bY,\bzs;\bthetas,\bTs\right)\mathcal{O}_P\left(1\right)\exp\left\{ne^{\log K+M_{c/4}-\frac{d\tau_{\min}\delta\left(\bthetas\right)}{2}\left[1+o_P(1)\right]}[1+o\left(1\right)]\right\}\\
&\leq&p\left(\bY,\bzs;\bthetas,\bTs\right)\mathcal{O}_P\left(1\right)\\
&&\quad\times\exp\left\{e^{-d\left\{\frac{\log(n)}{d}+\frac{\log K+M_{c/4}}{d}-\frac{\delta\left(\bthetas\right)}{2}\left[1+o_P(1)\right]\right\}}[1+o\left(1\right)]\right\}\\
&\leq&p\left(\bY,\bzs;\bthetas,\bTs\right)o_P(1)\text{ using Assumption (C.3).}
\end{eqnarray*}
As this is true for all  $\theta\in\bTheta$ and all $\bT\in\mathcal{T}_{\tau_{\min}}$, this is also true for the maximum. 
\end{proof}




\subsection{Proof of Proposition \ref{prop:part_eloign}: partitions that are far}

\begin{prop}[Separability]\label{prop:separability}
Assume (ID.3.s) and (C.2). There exists  $R>0$ and a constant $B(R)$  such that 
\begin{align*}
\max_{\bT} \Lambdat(\bz, \bT)&\leq -\frac{d\tau_{\min}\delta(\bthetas) \max_k L_k}{2}d_{0,\sim}(\bz,\bzs) \hspace{1cm} \text{ for } \bz\in \mathcal{B}\left(\bzs;R\right);\\
 \max_{\bT} \Lambdat(\bz,\bT)&\leq -B(R)dn \hspace{4.4cm} \text{ for }\bz\notin\mathcal{B}\left(\bzs;R\right).
\end{align*}
\end{prop}
\begin{proof}

If $\bz\notin\mathcal{B}\left(\bzs;R\right)$, because the two partitions $\bz$ and $\bzs$ are far from each other (at least a radius $R>0$), there exists a constant $B(R)$ such that the second inequality holds. 

Else, let assume that $\bz\in\mathcal{B}\left(\bzs;R\right)$.
From Assumptions (ID.3.s) and (C.2), for all  $k\neq k'$, there exists at least  $\tau_{\min}d$ columns such that the Kullback-Leibler divergence is strictly positive, then
$$ \sum_{\ell_2=1}^{L_{k_2}}\NL{k_2}{k}_{\ell_2,\ell}\KL{\mus_{k_1,\ell_1}}{\mus_{k_2,\ell_2}} \geq d\tau_{\min}\delta(\bthetas).$$

 
Then, according to Proposition~\ref{prop:lambdat}, we get
\begin{eqnarray*}
&&\!\!\!\!\!\!\!\!\!\!\!\!\Lambdat(\bz, \bT)\\
&\leq&-\frac{d\tau_{\min}\delta(\bthetas)}{2}\sum_{k=1}^K\sum_{\ell=1}^{L_{k}}\frac{1}{\left|{D^\star}_{\ell}^{k}\right|\bzs_{+,k}}\sum_{k_1=1}^K\RK_{k_1,k} \\
&&\quad\times\sum_{\ell_1=1}^{L_{k_1}}\NL{k_1}{k}_{\ell_1,\ell}\\
&&\quad\quad\times\left[\sum_{k_2=1}^K\RK_{k_2,k}-\RK_{k_1,k}\right]\\
&\leq&-\frac{d\tau_{\min}\delta(\bthetas)}{2}\sum_{k=1}^K\sum_{\ell=1}^{L_{k}}\frac{1}{\bzs_{+,k}}\sum_{k_1=1}^K\RK_{k_1,k}\\
&&\quad\quad\times\left[\sum_{k_2=1}^K\RK_{k_2,k}-\RK_{k_1,k}\right], 
\end{eqnarray*}
using Eq. \eqref{sumNk=Dk} in the last inequality. 
Then, using Eq. \eqref{R+k'}, and the fact that 
$\RK_{k_1,k} \leq \bzs_{+,k}$,

\begin{eqnarray*}
&&\!\!\!\!\!\!\!\!\!\!\!\!\Lambdat(\bz,\bT)\\
&\leq&-\frac{d\tau_{\min}\delta(\bthetas)\max_kL_k}{2}\sum_{k=1}^K\frac{1}{\bzs_{+,k}}\sum_{k_1=1}^K\RK_{k_1,k}\\
&&\quad\times\left[\sum_{k_2=1}^K\RK_{k_2,k}-\RK_{k_1,k}\right]\\
&\leq&-\frac{d\tau_{\min}\delta(\bthetas)\max_kL_k}{2}\left[n-\sum_{k=1}^K\sum_{k_1=1}^K\frac{\RK_{k_1,k}^2}{\bzs_{+,k}}\right]\\
%&\leq&-\frac{d\tau_{\min}\delta(\bthetas)}{2}\left[n-\sum_{k=1}^K\sum_{k_1=1}^K\frac{\RK_{k_1,k}\bzs_{+,k}}{\bzs_{+,k}}\right]\\
&\leq&-\frac{d\tau_{\min}\delta(\bthetas)\max_kL_k}{2}\left[n-\sum_{k=1}^K\sum_{k_1=1}^K\RK_{k_1,k}\right]\\
%&\leq&-\frac{d\tau_{\min}\delta(\bthetas)}{2}\left[n-\sum_{k=1}^K\bzs_{+,k}\right]\\
%&\leq&-\frac{d\tau_{\min}\delta(\bthetas)}{2}\left[n-\underset{\sigma\in\mathfrak{S}(\left\{1,\ldots,K\right\})}{\max}\sum_{k=1}^K\RK_{k,\sigma(k)}\right]\\
&\leq&-\frac{d\tau_{\min}\delta(\bthetas)\max_kL_k}{2}d_{0,\sim}(\bz,\bzs).
\end{eqnarray*}

\end{proof}


\begin{lem}[Large deviation]\label{propo:largeDeviation}
Under assumption (C.1), and for all $\varepsilon_{nd}<1/\sqrt{2}$, we get:
\begin{align*}
&\Prob{\sup_{\btheta,\bT,\bz}\left[\Fntz-\Lambdat(\bz,\bT)\right]\geq \text{Diam}(\bTheta)\sqrt{nd}K^2+4\varepsilon_{nd}\text{Diam}(\bTheta)nd}\\
\leq& K^n\exp\left(-\varepsilon_{nd}^2nd\right).
\end{align*}
\end{lem}
\begin{proof}
By definition of $\Fntz$ and $\Gntz$:
\begin{eqnarray*}
&&\!\!\!\!\!\!\!\!\!\!\!\!\!\!\Fntz-\Lambdat(\bz,\bT)\leq\Fntz-\Gntz\\
%&\leq&-\sum_{i=1}^n\sum_{k=1}^K\sum_{k'=1}^K\zs_{ik}z_{i,k'}\sum_{\ell=1}^{\Lk}\sum_{\ell'=1}^{L_{k'}}\sum_{j\in{D^\star}_{\ell}^{k}\bigcap{D}_{\ell'}^{k'}}\log\left[\frac{\varphi\left(Y_{ij};\mus_{k\ell}\right)}{\varphi\left(Y_{ij};\mu_{k'\ell'}\right)}\right]\\
%&&-\sum_{i=1}^n\sum_{k=1}^K\sum_{k'=1}^K\zs_{ik}z_{i,k'}\sum_{\ell=1}^{\Lk}\sum_{\ell'=1}^{L_{k'}}\sum_{j\in{D^\star}_{\ell}^{k}\bigcap{D}_{\ell'}^{k'}}\Espc{\bY|\bzs;\bthetas,\bTs}{\log\left[\frac{\varphi\left(Y_{ij};\mus_{k\ell}\right)}{\varphi\left(Y_{ij};\mu_{k'\ell'}\right)}\right]}\\
%&\leq&-\sum_{i=1}^n\sum_{k=1}^K\sum_{k'=1}^K\zs_{ik}z_{i,k'}\sum_{\ell=1}^{\Lk}\sum_{\ell'=1}^{L_{k'}}\sum_{j\in{D^\star}_{\ell}^{k}\bigcap{D}_{\ell'}^{k'}}\\
%&&\quad\times\left\{Y_{ij}\left(\mus_{k\ell}-\mu_{k'\ell'}\right)-\Espc{\bY|\bzs;\bthetas,\bTs}{Y_{ij}}\left(\mus_{k\ell}-\mu_{k'\ell'}\right)\right\}\\
&\leq&-\sum_{i=1}^n\sum_{k=1}^K\sum_{k'=1}^K\zs_{ik}z_{i,k'}\sum_{\ell=1}^{\Lk}\sum_{\ell'=1}^{L_{k'}}\\
&&\quad\times\sum_{j\in{D^\star}_{\ell}^{k}\bigcap{D}_{\ell'}^{k'}}\left(Y_{ij}-\Espc{\bY|\bzs;\bthetas,\bTs}{Y_{ij}}\right)\left(\mus_{k\ell}-\mu_{k'\ell'}\right)\\
%&\leq&\sum_{k=1}^K\sum_{k'=1}^K\sum_{\ell=1}^{\Lk}\sum_{\ell'=1}^{L_{k'}}\left(\mus_{k\ell}-\mu_{k'\ell'}\right)\sum_{i=1}^n\zs_{ik}z_{i,k'}\sum_{j\in{D^\star}_{\ell}^{k}\bigcap{D}_{\ell'}^{k'}}\left(\mus_{k\ell}-Y_{ij}\right)\\
&\leq&\sup_{\underset{\|\bGamma\|_\infty\leq \text{Diam}(\bTheta)}{\bGamma\in\mathbb{R}^{K\times K}}}\sum_{k=1}^K\sum_{k'=1}^K\sum_{\ell=1}^{\Lk}\sum_{\ell'=1}^{L_{k'}}\Gamma_{k,k'}\sum_{i=1}^n\zs_{ik}z_{i,k'}\sum_{j\in{D^\star}_{\ell}^{k}\bigcap{D}_{\ell'}^{k'}}\left(\mus_{k\ell}-Y_{ij}\right)\\
%&\leq&\sup_{\underset{\|\bGamma\|_\infty\leq \text{Diam}(\bTheta)}{\bGamma\in\mathbb{R}^{K\times K}}}\sum_{k=1}^K\sum_{k'=1}^K\Gamma_{k,k'}\sum_{i=1}^n\zs_{ik}z_{i,k'}\sum_{\ell=1}^{\Lk}\sum_{\ell'=1}^{L_{k'}}\sum_{j\in{D^\star}_{\ell}^{k}\bigcap{D}_{\ell'}^{k'}}\left(\mus_{k\ell}-Y_{ij}\right)\\
&\leq&\sup_{\underset{\|\bGamma\|_\infty\leq \text{Diam}(\bTheta)}{\bGamma\in\mathbb{R}^{K\times K}}}\sum_{k=1}^K\sum_{k'=1}^K\Gamma_{k,k'}\sum_{i=1}^n\zs_{ik}z_{i,k'}\sum_{\ell=1}^{\Lk}\sum_{j\in{D^\star}_{\ell}^{k}}\left(\mus_{k\ell}-Y_{ij}\right).
\end{eqnarray*}
Let 
$$ W_{k,k'}= \sum_{i=1}^n\zs_{ik}z_{i,k'}\sum_{\ell=1}^{\Lk}\sum_{j\in{D^\star}_{\ell}^{k}}\left(\mus_{k\ell}-Y_{ij}\right) \sim \mathcal{N}\left(0,d\RK_{k',k}\right). $$

Then, 
using \cite{brault2020consistency}[Proposition C.4], for $\bGamma \in \mathbb{R}^{K\times K}$ such that $\|\bGamma\|_\infty\leq \text{Diam}(\bTheta)$,
%Or, chaque variable gaussienne centrée réduite est sous-exponentielle de paramètre $(1,1)$ donc la somme est sous-gaussienne de paramètres $(d\RK_{k',k},1)$. Ainsi, par la proposition C.4 de l'article (complètement complet) de~\cite{brault2020consistency}, $W$ est sous-exponentielle de paramètres $(8\text{Diam}(\bTheta)^2nd,2\sqrt{2}\text{Diam}(\bTheta))$ et son espérance vérifie:
\[\Espc{\bthetas}{\sum_{k=1}^K\sum_{k'=1}^K\Gamma_{k,k'} W_{k,k'}}\leq \text{Diam}(\bTheta)\sqrt{nd}K^2.\]

Then, for all nonnegative sequence $(\varepsilon_{nd})_{(n,d)\in\left(\mathbb{N}^\star\right)^2}$ satisfying\linebreak $4\varepsilon_{nd}\text{Diam}(\bTheta)nd\leq\frac{8\text{Diam}(\bTheta)^2nd}{2\sqrt{2}\text{Diam}(\bTheta)}=2\sqrt{2}\text{Diam}(\bTheta)nd$, using Lemma \ref{lem:inegalitemesouviensplus}:
%$\sum_{k=1}^K\sum_{k'=1}^K\Gamma_{k,k'} W_{k,k'}

\begin{eqnarray*}
&&\!\!\!\!\!\!\!\!\!\!\!\!\Prob{\sum_{k=1}^K\sum_{k'=1}^K\Gamma_{k,k'} W_{k,k'} - \Espc{\bthetas}{\sum_{k=1}^K\sum_{k'=1}^K\Gamma_{k,k'} W_{k,k'}}  \geq 4\varepsilon_{nd}\text{Diam}(\bTheta)nd}\\
&\leq&\exp\left[-\frac{\left(4\varepsilon_{nd}\text{Diam}(\bTheta)nd\right)^2}{16\text{Diam}(\bTheta)^2nd}\right]
\leq\exp\left(-\varepsilon_{nd}^2nd\right).
\end{eqnarray*}
Then, 
\begin{eqnarray*}
&&\!\!\!\!\!\!\!\!\!\!\!\!\Prob{\Fntz-\Lambdat(\bz,\bT)\geq \text{Diam}(\bTheta)\sqrt{nd}K^2+4\varepsilon_{nd}\text{Diam}(\bTheta)nd}\\
&\leq&\exp\left(-\varepsilon_{nd}^2nd\right).
\end{eqnarray*}
This inequality holds for all $(\btheta,\bT)$ and $\bz$, then we get:
\begin{eqnarray*}
&&\mathbb{P}\left(\sup_{\btheta,\bT,\bz}\left[\Fntz-\Lambdat(\bz,\bT)\right]\geq\right.\\
&&\quad\quad\left. \text{Diam}(\bTheta)\sqrt{nd}K^2+4\varepsilon_{nd}\text{Diam}(\bTheta)nd\right)\\
%\Delta(\varepsilon_{nd})%&=&\Prob{\sup_{\btheta,\bT,\bz}\left[\Fntz-\Lambdat(\bz,\bT)\right]\geq \text{Diam}(\bTheta)\sqrt{nd}K^2+4\varepsilon_{nd}\text{Diam}(\bTheta)nd}\\
&\leq&\sum_{\bz\in\mathcal{Z}}\mathbb{P}\left(\sup_{\btheta,\bT}\left[\Fntz-\Lambdat(\bz,\bT)\right]\geq\right.\\
&&\quad\quad\left. \text{Diam}(\bTheta)\sqrt{nd}K^2+4\varepsilon_{nd}\text{Diam}(\bTheta)nd\right)\\
%&\leq&\sum_{\bz\in\mathcal{Z}}\Prob{W\geq \Espc{\bthetas}{W}+4\varepsilon_{nd}\text{Diam}(\bTheta)nd}\\
&\leq&\sum_{\bz\in\mathcal{Z}}\exp\left(-\varepsilon_{nd}^2nd\right) \leq K^n\exp\left(-\varepsilon_{nd}^2nd\right).
\end{eqnarray*}
\end{proof}



Let $R>0$. Then, according to Proposition \ref{prop:separability},
\begin{align*}
\Lambdat(\bz,\bT)&\leq -B(C)nd\text{ if }\bz\notin\mathcal{B}\left(\bzs;R\right)\\
\Lambdat(\bz,\bT)&\leq-\frac{d\tau_{\min}\delta(\bthetas)}{2}d_{0,\sim}(\bz,\bzs)\\
&\leq-\frac{d\tau_{\min}\delta(\bthetas)}{2}nR_{nd}\text{ if }\bz\in\mathcal{B}\left(\bzs;R\right)\backslash\mathcal{B}\left(\bzs;R_{nd}\right).
\end{align*}

Using Lemma~\ref{propo:largeDeviation}, with $\varepsilon_{nd}=\min\left(\frac{\delta(\bthetas)\tau_{\min}R_{nd}}{16},1/\sqrt{2}\right)$, we get with probability   $1-K^n\exp\left(-\varepsilon_{nd}^2nd\right)$,
\begin{eqnarray*}
&&\Fntz-\Lambdat(\bz,\bT)+\Lambdat(\bz,\bT)\\
&\leq&\Fntz-\Lambdat(\bz,\bT)-\frac{d\tau_{\min}\delta(\bthetas)}{2}nR_{nd}\\
&\leq&\text{Diam}(\bTheta)\sqrt{nd}K^2+4\varepsilon_{nd}\text{Diam}(\bTheta)nd-\frac{d\tau_{\min}\delta(\bthetas)}{2}nR_{nd}\\
&\leq&\text{Diam}(\bTheta)\sqrt{nd}K^2+4\frac{\delta(\bthetas)\tau_{\min}R_{nd}}{16}\text{Diam}(\bTheta)nd-\frac{d\tau_{\min}\delta(\bthetas)}{2}nR_{nd}\\
&\leq& \text{Diam}(\bTheta)\sqrt{nd}K^2+\frac{d\tau_{\min}\delta(\bthetas)}{4}nR_{nd}-\frac{d\tau_{\min}\delta(\bthetas)}{2}nR_{nd}\\
&\leq&\text{Diam}(\bTheta)\sqrt{nd}K^2-\frac{d\tau_{\min}\delta(\bthetas)}{4}nR_{nd}\\
&\leq&-dnR_{nd}\frac{\delta(\bthetas)\tau_{\min}}{4}\left[1-\frac{4\text{Diam}(\bTheta)K^2}{\sqrt{nd}R_{nd}\tau_{\min}\delta(\bthetas)}\right]\\
&\leq&-dnR_{nd}\frac{\tau_{\min}\delta(\bthetas)}{4}
\end{eqnarray*}
 for $n$ and $d$ large enough. 

 We also know that
 \begin{eqnarray*}
{p(\bzs;\bthetas,\bTs)}
&\leq& e^{-\log \prod_{i=1}^n\prod_{k=1}^K\pik^{\zs_{ik}}} 
\leq e^{-\sum_{i=1}^n\sum_{k=1}^K\zs_{ik}\log \pik} \\
&\leq& e^{-\sum_{i=1}^n\sum_{k=1}^K\zs_{ik}\log c} \text{ by Assumption (C.1),}\\
&\leq& e^{-\sum_{i=1}^n\log c}
\leq e^{n \log \frac{1}{c}}.
\end{eqnarray*}
Then, it leads to

\begin{eqnarray*}
\!\!\!\!\!\!\!\!\!\!\sum_{z\notin\mathcal{B}\left(\bzs;R_{nd}\right)}p(\bY,\bz;\btheta,\bT)&=&\sum_{z\notin\mathcal{B}\left(\bzs;R_{nd}\right)}p(\bY|\bz;\btheta,\bT)p(\bz;\btheta,\bT)\\
&=&p(\bY|\bzs;\bthetas,\bTs)\sum_{z\notin\mathcal{B}\left(\bzs;R_{nd}\right)}p(\bz;\btheta,\bT)\frac{p(\bY|\bz;\btheta,\bT)}{p(\bY|\bzs;\bthetas,\bTs)}\\
&=&p(\bY|\bzs;\bthetas,\bTs)\sum_{z\notin\mathcal{B}\left(\bzs;R_{nd}\right)}p(\bz;\btheta,\bT)e^{\Fntz}\\
&\leq&p(\bY,\bzs;\bthetas,\bTs)e^{-n\left[dR_{nd}\frac{\delta(\bthetas)\tau_{\min}}{4}-\log \frac{1}{c}\right]}.
\end{eqnarray*}
Then, with probability  $1-K^n\exp\left(-\varepsilon_{nd}^2nd\right)$, under Assumption  (C.1), we get:
\[\sum_{z\notin\mathcal{B}\left(\bzs;R_{nd}\right)}p(\bY,\bz;\btheta,\bT)=p(\bY,\bzs;\bthetas,\bTs)o_P(1)\]
for all $\btheta \in \bTheta$ and $\bT\in\mathcal{T}_{\tau_{\min}}$.



\subsection{Tools and details}
\label{sec:tools}
\begin{lem}[Chernoff's lemma]
\label{lem:inegalitemesouviensplus}
Let $Z\sim\mathcal{N}\left(0,\sigma^2\right)$. Then, for all $t>0$:
\[\Prob{Z\geq t}\leq e^{-\frac{t^2}{2\sigma^2}}.\]
\end{lem}


\begin{proof}[Proof of Proposition \ref{prop:Fn}]
By definition, 
\begin{eqnarray*}
\Fntz&=&-\sum_{i=1}^n\sum_{k=1}^K\sum_{k'=1}^K\zs_{ik}z_{i,k'}\sum_{\ell=1}^{\Lk}\sum_{\ell'=1}^{L_{k'}}\sum_{j\in{D^\star}_{\ell}^{k}\bigcap{D}_{\ell'}^{k'}}\log\left[\frac{\varphi\left(Y_{ij};\mus_{k\ell}\right)}{\varphi\left(Y_{ij};\mu_{k'\ell'}\right)}\right].
\end{eqnarray*}
The computation gives:
\begin{eqnarray}
\log\left[\frac{\varphi\left(Y_{ij};\mus_{k\ell}\right)}{\varphi\left(Y_{ij};\mu_{k'\ell'}\right)}\right]&=&\log\frac{\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}\left(Y_{ij}-\mus_{k\ell}\right)^2}}{\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}\left(Y_{ij}-\mu_{k'\ell'}\right)^2}}\nonumber\\
%&=&\log\frac{\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}\left(Y_{ij}-\mus_{k\ell}\right)^2}}{\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}\left(Y_{ij}-\mu_{k'\ell'}\right)^2}}\\
&=&-\frac{1}{2}\left(Y_{ij}-\mus_{k\ell}\right)^2+\frac{1}{2}\left(Y_{ij}-\mu_{k'\ell'}\right)^2\nonumber\\
%&=&-\frac{1}{2}\left[-2Y_{ij}\left(\mus_{k\ell}-\mu_{k'\ell'}\right)+{\mus_{k\ell}}^2-\mu_{k'\ell'}^2\right]\nonumber\\
&=&Y_{ij}\left(\mus_{k\ell}-\mu_{k'\ell'}\right)-\frac{1}{2}\left({\mus_{k\ell}}^2-\mu_{k'\ell'}^2\right) \label{ratioLogLik}
\end{eqnarray}
which leads to 
\begin{eqnarray*}
&&\Fntz=-\sum_{i=1}^n\sum_{k=1}^K\sum_{k'=1}^K\zs_{ik}z_{i,k'}\sum_{\ell=1}^{\Lk}\sum_{\ell'=1}^{L_{k'}}\sum_{j\in{D^\star}_{\ell}^{k}\bigcap{D}_{\ell'}^{k'}}Y_{ij}\left(\mus_{k\ell}-\mu_{k'\ell'}\right)\\
&&-\frac{1}{2}\sum_{k=1}^K\sum_{k'=1}^K\sum_{\ell=1}^{\Lk}\sum_{\ell'=1}^{L_{k'}}\RK_{k,k'}\NLk_{\ell,\ell'}\left({\mus_{k\ell}}^2-\mu_{k'\ell'}^2\right).
\end{eqnarray*}
We recognize the linear combination of independent Gaussian variable, then the distribution of $\Fntz$ is also Gaussian. The computation of the expectation and the variance are straightforward.  
\end{proof}

\begin{proof}[Proof of Proposition \ref{prop:lambdat}]
The function $\btheta\mapsto\Gntz$ is maximal for
\[\widehat{\mu}_{k'\ell'}=\frac{\sum_{k=1}^K\sum_{\ell=1}^{\Lk}\RK_{k,k'}\NLk_{\ell,\ell'}\mus_{k\ell}}{\sum_{k=1}^K\sum_{\ell=1}^{\Lk}\RK_{k,k'}\NLk_{\ell,\ell'}}.\]
Indeed, the Kullback-Leibler divergence is equal to %La divergence de Kullback en $\mu$ et $\mu'$ vaut:
\[\KL{\mu}{\mu'}=\frac{1}{2}\left(\mu-\mu'\right)^2,\]
and the maximum is get by differentiating in each value. Then,
\begin{align*}
\mus_{k_1,\ell_1}-\widehat{\mu}_{k\ell}
&= \mus_{k_1,\ell_1}-\frac{\sum_{k_2=1}^K\sum_{\ell_2=1}^{L_{k_2}}\RK_{k_2,k}\NL{k_2}{k}_{\ell_2,\ell}\mus_{k_2,\ell_2}}{\sum_{k_2=1}^K\sum_{\ell_2=1}^{L_{k_2}}\RK_{k_2,k}\NL{k_2}{k}_{\ell_2,\ell}}\\
%&= \frac{\sum_{k_2=1}^K\sum_{\ell_2=1}^{L_{k_2}}\RK_{k_2,k}\NL{k_2}{k}_{\ell_2,\ell}\mus_{k_1,\ell_1}}{\sum_{k_2=1}^K\sum_{\ell_2=1}^{L_{k_2}}\RK_{k_2,k}\NL{k_2}{k}_{\ell_2,\ell}}-\frac{\sum_{k_2=1}^K\sum_{\ell_2=1}^{L_{k_2}}\RK_{k_2,k}\NL{k_2}{k}_{\ell_2,\ell}\mus_{k_2,\ell_2}}{\sum_{k_2=1}^K\sum_{\ell_2=1}^{L_{k_2}}\RK_{k_2,k}\NL{k_2}{k}_{\ell_2,\ell}}\\
&= \frac{\sum_{k_2=1}^K\sum_{\ell_2=1}^{L_{k_2}}\RK_{k_2,k}\NL{k_2}{k}_{\ell_2,\ell}\left(\mus_{k_1,\ell_1}-\mus_{k_2,\ell_2}\right)}{\sum_{k_2=1}^K\sum_{\ell_2=1}^{L_{k_2}}\RK_{k_2,k}\NL{k_2}{k}_{\ell_2,\ell}}.
\end{align*}
%and 
%\begin{align*}
%(\mus_{k_1,\ell_1}-\widehat{\mu}_{k\ell})^2
%&= \frac{\sum_{k_2=1}^K\sum_{\ell_2=1}^{L_{k_2}} \sum_{k'_2=1}^K\sum_{\ell'_2=1}^{L_{k'_2}} \RK_{k_2,k} \NL{k_2}{k}_{\ell_2,\ell}}{(\sum_{k_2=1}^K\sum_{\ell_2=1}^{L_{k_2}}\RK_{k_2,k}\NL{k_2}{k}_{\ell_2,\ell})^2}\\
%&\times \left[\right]\\
%&\quad\times \RK_{k'_2,k}\NL{k'_2}{k}_{\ell'_2,\ell} \left(\mus_{k_1,\ell_1}-\mus_{k_2,\ell_2}\right) \left(\mus_{k_1,\ell_1}-\mus_{k'_2,\ell'_2}\right).
%\end{align*}
However, if ones want to take the square of the previous formulae, we get 
\begin{align*}
\left(\mus_{k_1,\ell_1}-\mus_{k_2,\ell_2}\right) \left(\mus_{k_1,\ell_1}-\mus_{k'_2,\ell'_2}\right) 
&= {\mus_{k_1,\ell_1}}^2-\mus_{k_1,\ell_1}\mus_{k_2,\ell_2}-\mus_{k'_2,\ell'_2}\left(\mus_{k_1,\ell_1}-\mus_{k_2,\ell_2}\right),
\end{align*}
then it leads to
\begin{align*}
(\mus_{k_1,\ell_1}-\widehat{\mu}_{k\ell})^2
&= \frac{\sum_{k_2=1}^K\sum_{\ell_2=1}^{L_{k_2}} \sum_{k'_2=1}^K\sum_{\ell'_2=1}^{L_{k'_2}} \RK_{k_2,k} \NL{k_2}{k}_{\ell_2,\ell}}{(\sum_{k_2=1}^K\sum_{\ell_2=1}^{L_{k_2}}\RK_{k_2,k}\NL{k_2}{k}_{\ell_2,\ell})^2}\\
&\quad\times \RK_{k'_2,k}\NL{k'_2}{k}_{\ell'_2,\ell} \left({\mus_{k_1,\ell_1}}^2-\mus_{k_1,\ell_1}\mus_{k_2,\ell_2} \right)\\
& -  \frac{\sum_{k_2=1}^K\sum_{\ell_2=1}^{L_{k_2}} \sum_{k'_2=1}^K\sum_{\ell'_2=1}^{L_{k'_2}} \RK_{k_2,k} \NL{k_2}{k}_{\ell_2,\ell}}{(\sum_{k_2=1}^K\sum_{\ell_2=1}^{L_{k_2}}\RK_{k_2,k}\NL{k_2}{k}_{\ell_2,\ell})^2}\\
&\quad\times \RK_{k'_2,k}\NL{k'_2}{k}_{\ell'_2,\ell} \mus_{k'_2,\ell'_2}\left(\mus_{k_1,\ell_1}-\mus_{k_2,\ell_2}\right)\\
&= \frac{\sum_{k_2=1}^K\sum_{\ell_2=1}^{L_{k_2}}  \RK_{k_2,k} \NL{k_2}{k}_{\ell_2,\ell}  \left({\mus_{k_1,\ell_1}}^2-\mus_{k_1,\ell_1}\mus_{k_2,\ell_2} \right)}{\sum_{k_2=1}^K\sum_{\ell_2=1}^{L_{k_2}}\RK_{k_2,k}\NL{k_2}{k}_{\ell_2,\ell}}\\
%&\quad\times \\
& -  \frac{\sum_{k_2=1}^K\sum_{\ell_2=1}^{L_{k_2}} \sum_{k'_2=1}^K\sum_{\ell'_2=1}^{L_{k'_2}} \RK_{k_2,k} \NL{k_2}{k}_{\ell_2,\ell}}{(\sum_{k_2=1}^K\sum_{\ell_2=1}^{L_{k_2}}\RK_{k_2,k}\NL{k_2}{k}_{\ell_2,\ell})^2}\\
&\quad\times \RK_{k'_2,k}\NL{k'_2}{k}_{\ell'_2,\ell} \mus_{k'_2,\ell'_2}\left(\mus_{k_1,\ell_1}-\mus_{k_2,\ell_2}\right).
\end{align*}
When considering $\Lambdat(\bz,\bT)$, we are summing with respect to $k_1,\ell_1$ as well, and the second term becomes $0$. Then, using the explicit form of $F_n$ given in the proof of Proposition \ref{prop:Fn}, it leads to
\begin{eqnarray*}
&&\!\!\!\!\!\!\!\!\!\!\!\!\Lambdat(\bz,\bT)\\
&=&-\frac{1}{2}\sum_{k=1}^K\sum_{\ell=1}^{L_{k}}
\frac{\sum_{k_1=1}^K\sum_{\ell_1=1}^{L_{k_1}} \sum_{k_2=1}^K\sum_{\ell_2=1}^{L_{k_2}} \RK_{k_1,k}\RK_{k_2,k}}{\sum_{k_2=1}^K\sum_{\ell_2=1}^{L_{k_2}}\RK_{k_2,k}\NL{k_2}{k}_{\ell_2,\ell}}\\
%&&\quad \times \\
&&\quad\times\NL{k_1}{k}_{\ell_1,\ell}\NL{k_2}{k}_{\ell_2,\ell}\frac{1}{2}\left({\mus_{k_1,\ell_1}}-\mus_{k_2,\ell_2}\right)^2.
\end{eqnarray*}
Then, using \ref{Rk+}, \ref{sumNk=Dk} and \ref{KL}, 
we finally get the desired formulae. 
\end{proof}

\begin{acks}[Acknowledgments]
This work has been partially supported by MIAI@Grenoble Alpes (ANR-19-P3IA-0003). All the computations presented in this paper were performed using the GRICAD infrastructure (\url{https://gricad.univ-grenoble-alpes.fr}), which is supported by Grenoble research communities.
\end{acks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Supplementary Material, if any, should %%
%% be provided in {supplement} environment %%
%% with title and short description. %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{supplement}
\stitle{Additional figures for Enedis data analysis}
\sdescription{In this part, three additional figures (\ref{SI:fig:cluster1}, \ref{SI:fig:cluster2} and~\ref{SI:fig:cluster3}) for the section~\ref{sec:real_data} are presented.}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{Fig/Courbe_BIC5_projection_k_1.png}
    \caption{Evolution of the coefficients grouped per day (rows) and time slots (columns) for the cluster~1. For each graphics, the evolution week per week of each curve is represented in color, the vertical black line corresponds at the estimated break-point, the horizontal black dashed line at the mean and the horizontal black dotted lines at the confident intervals. The vertical blue dashed lines correspond at the begin and the end of the both lock-down.}
    \label{SI:fig:cluster1}
\end{figure}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{Fig/Courbe_BIC5_projection_k_2.png}
    \caption{Evolution of the coefficients grouped per day (rows) and time slots (columns) for the cluster~2. For each graphics, the evolution week per week of each curve is represented in color, the vertical black lines correspond at the estimated break-points, the horizontal black dashed line at the mean and the horizontal black dotted lines at the confident intervals. The vertical blue dashed lines correspond at the begin and the end of the both lock-down.}
    \label{SI:fig:cluster2}
\end{figure}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{Fig/Courbe_BIC5_projection_k_3.png}
    \caption{Evolution of the coefficients grouped per day (rows) and time slots (columns) for the cluster~3. For each graphics, the evolution week per week of each curve is represented in color, the vertical black lines correspond at the estimated break-points, the horizontal black dashed line at the mean and the horizontal black dotted lines at the confident intervals. The vertical blue dashed lines correspond at the begin and the end of the both lock-down.}
    \label{SI:fig:cluster3}
\end{figure}

\end{supplement}


\bibliographystyle{imsart-nameyear}
\bibliography{_references}

\end{document}