% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother
\usepackage{tikz}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[accsupp]{axessibility}



% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%\usepackage{multicol}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{adjustbox}

\usepackage{color}

\usepackage[normalem]{ulem}

\usepackage{todonotes}

\definecolor{ben}{rgb}{0.9,0.,0.5}
\newcommand{\ben}[1]{\textcolor{ben}{\emph{Ben:~{#1}}}}

\definecolor{pat}{rgb}{0.6,0.2,0.1}
\newcommand{\pat}[1]{\textcolor{pat}{\emph{Pat:~{#1}}}}

\definecolor{GY}{rgb}{0.4,0.1,0.9}
\newcommand{\gy}[1]{\textcolor{GY}{\emph{GY:~{#1}}}}

\newcommand{\mycomment}[1]{}

% custom commands
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{9363} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks \\ -- \\ Supplementary Material}

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }
\author{
\hspace{-20pt}
HyunJun Jung$^{\ast 1}$,
Patrick Ruhkamp$^{\ast 1,2}$,
Guangyao Zhai$^{1}$,
Nikolas Brasch$^{1}$,
Yitong Li$^{1}$,\\
Yannick Verdie$^{1,3}$,
Jifei Song$^{3}$,
Yiren Zhou$^{3}$,
Anil Armagan$^{3}$,
Slobodan Ilic$^{1,4}$,\\
Ales Leonardis$^{3}$,
Nassir Navab$^{1}$,
Benjamin Busam$^{1,2}$
% HyunJun Jung$^{\ast 1}$\and
% Patrick Ruhkamp$^{\ast 1,2}$\and
% Guangyao Zhai$^{1}$\and
% Nikolas Brasch$^{1}$\and
% Yitong Li$^{1}$\and
% Yannick Verdie$^{1,3}$\and
% Jifei Song$^{3}$\and
% Yiren Zhou$^{3}$\and
% Anil Armagan$^{3}$\and
% Slobodan Ilic$^{1,4}$\and
% Ales Leonardis$^{3}$\and
% Nassir Navab$^{1}$\and
% Benjamin Busam$^{1,2}$
\\
\\
\small
$^1$ Technical University of Munich,
$^2$ 3Dwe.ai,
$^3$  Huawei Noah's Ark Lab,
$^4$  Siemens AG,
$^*$ Equal Contribution
\\ \footnotesize{\fontfamily{qcr}\selectfont
hyunjun.jung@tum.de, p.ruhkamp@tum.de, guangyao.zhai@tum.de, b.busam@tum.de
}
}
\maketitle

% \twocolumn[{%
% \renewcommand\twocolumn[1][]{#1}%
% \maketitle
% \begin{center}
%     \centering
%     \captionsetup{type=figure}
%     \includegraphics[width=\textwidth]{figures/hammer_teaser_quali56.pdf}
%     \captionof{figure}{\textbf{Sensor Influence on Dense 3D Vision Tasks.} Monocular depth estimation, 3D reconstruction, and novel view synthesis are all influenced by inherent sensor artefacts. Time-of-Flight (ToF) sensors suffer from Multi-Path-Inference (MPI) and fail to measure correctly reflective or translucent objects. Active Stereo (AS) recovers such material but struggles on diffuse texture-less parts and at depth discontinuities. Our novel multi-modal dataset allows for the first time to analyse systematically such sensor characteristics quantitatively and qualitatively and fosters the way for novel learning-based dense 3D computer vision methods in photometrically challenging environments.}
%     \label{fig:teaser}
% \end{center}%
% }]



%%%%%%%%% ABSTRACT
% \begin{abstract}
% ...
% \end{abstract}

% Learning-based methods to solve dense 3D vision problems typically train on 3D sensor data. The respectively used principle of measuring distances provides advantages and drawbacks. These are typically not compared nor discussed in the literature due to a lack of multi-modal datasets. Texture-less regions are problematic for structure from motion and stereo, reflective material poses issues for active sensing, and distances for translucent objects are intricate to measure with existing hardware. Training on inaccurate or corrupt data induces model bias and hampers generalisation capabilities. These effects remain unnoticed if the sensor measurement is considered as ground truth during the evaluation. This paper investigates the effect of sensor errors for the dense 3D vision tasks of depth estimation and reconstruction. We rigorously show the significant impact of sensor characteristics on the learned predictions and notice generalisation issues arising from various technologies in everyday household environments. For evaluation, we introduce a carefully designed dataset\footnote{Our dataset will be made publicly available upon acceptance.} comprising measurements from commodity sensors, namely D-ToF, I-ToF, passive/active stereo, and monocular RGB+P. Our study quantifies the considerable sensor noise impact and paves the way to improved dense vision estimates and targeted data fusion.



% %Geometry estimation is a core element in 3D computer vision pipelines.
% %It is an inherent task to predict reliable depth and to reconstruct scenes.
% %Learning based methods to solve dense 3D vision problems are typically provided with depth data from various sources during training.

% %, and sensor capabilities are typically not discussed in the literature.
% %Everyday objects in indoor environments, however, pose severe challenges for some devices.

% %with challenging but everyday scene content.


% % ECCV
% %Depth estimation is a core task in 3D computer vision. Recent methods investigate the task of monocular depth trained with various depth sensor modalities. Every sensor has its advantages and drawbacks caused by the nature of estimates. In the literature, mostly mean average error of the depth is investigated and sensor capabilities are typically not discussed. Especially indoor environments, however, pose challenges for some devices. Textureless regions pose challenges for structure from motion, reflective materials are problematic for active sensing, and distances for translucent material are intricate to measure with existing sensors. This paper proposes HAMMER, a dataset comprising depth estimates from multiple commonly used sensors for indoor depth estimation, namely ToF, stereo, active stereo together with monocular RGB+P data. We construct highly reliable ground truth depth maps with the help of 3D scanners and aligned renderings. A popular depth estimators is trained on this data and typical depth sensors. The estimates are extensively analyze on different scene structures. We notice generalization issues arising from various sensor technologies in household environments with challenging but everyday scene content. HAMMER, which we make publicly available (https://github.com/Junggy/HAMMER-dataset), provides a reliable base to pave the way to targeted depth improvements and sensor fusion approaches.
% %\keywords{Depth estimation, indoor scenes, monocular depth, ToF, stereo, active stereo, sensor fusion}
% \end{abstract}

%%%%%%%%% BODY TEXT

% \input{sections/1_introduction}
% \input{sections/2_relatedwork}
% \input{sections/3_dataset}
% \input{sections/4_methods}
% \input{sections/5_experiments}
% \input{sections/6_discussion.tex}

\input{sections/8_suppmat.tex}

\clearpage
\newpage
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{PaperForReview_supp}
}

\end{document}
