\section{Sensor Impact for Dense 3D Vision Tasks}
\label{sec:experiments}

\begin{table}[!b]
\centering
\caption{\textbf{Depth Prediction Results for Different Training Signals.} Top: Dense supervision from different depth modalities. Bottom: Evaluation of semi-supervised (pose GT) and self-supervised (mono and mono+stereo) training. The entire scene (Full), background (BG), and objects (Obj) are evaluated separately. Objects material is further split into textured, reflective and transparent. \textbf{Best} and \underline{2nd best} RMSE in mm are indicated.
}
\footnotesize
\resizebox{\columnwidth}{!}{
\begin{tabular}{ll|c|cc|ccc}
\toprule
& Training Signal &\multicolumn{1}{c|}{Full} & BG & \multicolumn{1}{c|}{Obj}  & Text. & Refl. & Transp. \\
\midrule
\multirow{3}{*}{\rotatebox[origin=c]{90}{Sup.}}     & I-ToF & 113.29 & 111.13 & 119.72 & 54.45 & 87.84 & 207.89 \\
                                & D-ToF & \underline{77.97} & \textbf{69.87}& \underline{112.83}& \textbf{37.88} & \underline{71.59} & \underline{207.85} \\
                                & Active Stereo & \textbf{72.20} & \underline{71.94}& \textbf{61.13}& \underline{50.90} & \textbf{52.43} & \textbf{87.24} \\
\midrule\\
\midrule
\multirow{3}{*}{\rotatebox[origin=c]{90}{Sel/Sem}}     & Pose & \textbf{154.87} & \textbf{158.67} & \textbf{65.42}& \textbf{57.22} & \textbf{37.78} & \underline{61.86} \\
& M & 180.34 & 183.65& 85.51 & 84.26 & \underline{48.80} & \textbf{49.62} \\
                                & M+S & \underline{159.80} & \underline{161.65} & \underline{82.16} &\underline{71.24} & 63.92 & 66.48\\                        
\bottomrule
\end{tabular}
}
\label{tab:depth_supervision_results}
\end{table}


We train a series of networks for the task of monocular depth estimation and implicit scene reconstruction.

\subsection{Depth Estimation}
Results for monocular depth estimation with varying training signal are summarized in Table~\ref{tab:depth_supervision_results} and Fig.~\ref{fig:depth_qualitative}. 
We report average results for the scenes and separate performances for background, objects, and materials of different photometric complexity. 
The error varies from background to objects. Their varying photometric complexity can explain this. Not surprisingly, the ToF training is heavily influenced by reflective and transparent object material, where the active stereo camera can project some patterns onto diffusely reflective surfaces. 
Interestingly, the self- and semi-supervised setups help to recover information in these challenging setups to some extent, such that these cases even outperform the ToF supervision for photometrically challenging objects. In contrast, simpler structures (such as the background) benefit from the ToF supervision. 
This indicates that sensor-specific noise is learnt and reveals that systematic errors of learning approaches cannot be evaluated if such 3D devices are used for ground truth evaluation without critical analysis. 
This might ultimately lead to incorrect result interpretations, particularly if self-supervised approaches are evaluated against co-modality sensor data.
The table also discloses that the mutual prediction of inter-frame poses in self-supervision indoor setups is challenging, and accurate pose labels can have an immediate and significant impact on the depth results (Pose vs. M).

\begin{figure}[!t]
 \centering
    \includegraphics[width=\linewidth]{figures/qualitative_new_ori2.png}
    \vspace{-6mm}
    \caption{\textbf{Fully Supervised Monocular Depth.} Monocular depth tends to overfit on the specific noise of the sensor the network is trained on. Prediction from Active Stereo GT is robust on the material while depth map is blurry, while both I-ToF and D-ToF has strong material dependent artifact but sharp on the edges.}
    \label{fig:depth_qualitative}
\end{figure}


\begin{figure}[!t]
 \centering
    \includegraphics[width=\linewidth]{figures/fusion_new.png}
    \vspace{-6mm}
    \caption{\textbf{Dense SfM.} A scene with our GT (left), Kinect~\cite{kinectfusion} (top) and TSDF~\cite{Zhou2018} (bottom) fusion approaches. Inherent sensor noise due to MPI (white), transparent objects (red), and diffuse texture-less material (yellow) persists.}
    \label{fig:fusion}
\end{figure}

\begin{figure}[!b]
 \centering
    \includegraphics[width=\linewidth]{figures/wild-tofu.PNG}
    \vspace{-6mm}
    \caption{\textbf{Sensor Fusion.} Scene (left) with I-ToF depth (centre) and ToF+RGB Fusion~\cite{jung2021wild} (right). Fusion can help to resolve some material induced artefacts (yellow) as well as MPI (blue).}
    \label{fig:wildtofu}
\end{figure}

\begin{figure*}[!t]
 \centering
    \includegraphics[width=1.0\linewidth]{figures/nerf_new.png}
    \vspace{-7mm}
    \caption{\textbf{Reconstruction Results.} The results of an implicit scene reconstruction with a Neural Radiance Field (NeRF) are shown. Images are synthesised for depth, surface normals and RGB for an unseen view, which is shown together with the prediction errors. The columns allow us to compare different methods where a NeRF~\cite{mildenhall2021nerf} is trained solely on RGB (first column) and various depth maps for regularisation as proposed in~\cite{roessle2022dense}. The last column illustrates synthesised results from training with GT depth for comparison. Differences are visible, especially for the partly reflective table edges, the translucent bottle and around depth discontinuities.}
    \label{fig:nerf_comparison}
\end{figure*}


Fig.~\ref{fig:fusion} shows that multi-view data aggregation in the form of dense SfM fails to reproduce highly reliable 3D reconstructions. In particular transparent and diffuse texture-less objects pose challenges to both Active Stereo and D-ToF. These can neither be recovered by the Kinect Fusion pipeline~\cite{kinectfusion} nor by the TSDF Fusion implementation of Open3D~\cite{Zhou2018} for which we use the GT camera poses. Inherent sensor artefacts are present even if depth maps from different viewpoints are combined. This quality advantage justifies our expensive annotation setup.
We further analysed the results of training runs with DPT~\cite{ranftl2021vision} and MIDAS~\cite{Ranftl2022}, which we train from scratch. While these more complex architectures with higher capacity show the same trend and also learn sensor noise, the training time is significantly longer. More details are provided in the supplementary material.
From the previous results, we have seen that ToF depth is problematic for translucent and reflective material. Fig~\ref{fig:wildtofu} illustrates that an additional co-modal input signal at test time can cure these effects partly. It can be observed that the use of additional RGB data in~\cite{jung2021wild} reduces the influence of MPI and resolves some material-induced depth artefacts. Our unique dataset also inspires cross-modal fusion pipelines' development and objective analysis.



\subsection{Implicit 3D Reconstruction {\&} View Synthesis}
Our implicit 3D reconstruction generates novel views for depth, normals and RGB with varying quality. If trained with only colour information, the NeRF produces convincing RGB views with the highest PSNR (cf. Fig.~\ref{fig:nerf_comparison} and Table~\ref{tab:nerf_results}).
However, the 3D scene geometry is not well reconstructed.
In line with the literature~\cite{deng2022depth,roessle2022dense}, depth regularization improves this (e.g. on texture-less regions).
Regularising with different depth modalities makes the sensor noise of I-ToF, AS, and D-ToF clearly visible. 
While the RMSE behaves similarly to the monocular depth prediction results with AS as best, followed by D-ToF and I-ToF. The cosine similarity for surface normal estimates confirms this trend.
The overall depth and normal reconstruction for AS are very noisy, but depth error metrics are more sensitive for significant erroneous estimates for reflective and translucent objects. 
Prior artefacts of the respective sensor influence the NeRF and translate into incorrect reconstructions (e.g. errors from D-ToF and I-ToF for translucent material or noisy background and inaccurate depth discontinuities at edges for AS).
Interestingly, the D-ToF prior can improve the overall reconstruction for most of the scene but fails for the bottle, where the AS can give better depth priors. This is also visible in the synthesised depth.
Leveraging synthetic depth GT (last row) mitigates these issues and positively affects the view synthesis with higher SSIM.

\begin{table}[!b]
\centering
\caption{\textbf{Novel View Synthesis from Implicit 3D Reconstruction.} Evaluation against GT for RGB, depth and surface normal estimates for different optimisation strategies (RGB-only for supervision and $+$ respective sensor depth). We indicate \textbf{best}, \underline{2nd best} and \dashuline{3rd best}. Depth metrics in mm.
}
\vspace{-1mm}
\footnotesize
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cc|cccc|c}
\toprule
& \multicolumn{2}{c|}{RGB}
& \multicolumn{4}{c|}{Depth}
& Normal
\\
Modality & PSNR $\uparrow$ & SSIM $\uparrow$ & Abs.Rel.$\downarrow$ &   Sq.Rel.$\downarrow$ &     RMSE $\downarrow$ & 
$\sigma < 1.25$ $\uparrow$
& Cos.Sim.$\downarrow$

\\

\midrule
RGB Only&
\textbf{32.406}&
\underline{0.889}
&   0.328  &   111.229  & 226.187  &
0.631  
& 0.084
\\
\midrule
$+$ AS&
17.570&
0.656
&   \dashuline{0.113}  &   \underline{16.050}  &  \underline{94.520}  &    
\dashuline{0.853}     
& \dashuline{0.071}
\\

$+$ I-ToF&
18.042&
0.653
&   0.296  &   91.426  & 217.334  &
0.520     
& 0.102
\\
$+$ D-ToF&
\dashuline{31.812} &
\dashuline{0.888}
&   \underline{0.112}  &  \dashuline{24.988}  & \dashuline{119.455}  &
\underline{0.882}
& \underline{0.031}
\\
$+$ Syn. &
\underline{32.082}&
\textbf{0.894}
&   \textbf{0.001}  &   \textbf{0.049}  &   \textbf{3.520}  &   
\textbf{1.000}     
& \textbf{0.001}
\\

\bottomrule
\end{tabular}
}
\label{tab:nerf_results}
\end{table}