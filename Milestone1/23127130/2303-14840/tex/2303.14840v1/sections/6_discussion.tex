\section{Discussion \& Conclusion}
\label{sec:discussion}
This paper shows that questioning and investigating commonly used 3D sensors helps to understand their impact on dense 3D vision tasks.
For the first time, we make it possible to study how sensor characteristics influence learning in these areas objectively. We quantify the effect of various photometric challenges, such as translucency and reflectivity for depth estimation, reconstruction and novel view synthesis and provide a unique dataset to stimulate research in this direction.
While obvious sensor noise is not "surprising", our dataset quantifies this impact for the first time.
For instance, interestingly, D-ToF supervision is significantly better suited (13.02~mm) for textured objects than AS, which in return surpasses I-ToF by 3.55~mm RMSE (cf.~\ref{tab:depth_supervision_results}). Same trend holds true on mostly texture-less backgrounds where D-ToF is 37\% more accurate than I-ToF.
For targeted analysis and research of dense methods for reflective and transparent objects, a quantitative evaluation is of utmost interest
- while our quantifiable error maps allow specifying the detailed deviations.
Although our dataset tries to provide scenes with varying backgrounds, the possible location of the scene is restricted due to the limited working range of the robot manipulator.
Aside from our investigations and the evaluation of sensor signals for standard 3D vision tasks, we firmly believe that our dataset can also pave the way for further investigation of cross-modal fusion pipelines.

