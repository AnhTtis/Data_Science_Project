\noindent
\section{Dense 3D Vision Tasks}
\subsection{Monocular Depth Estimation}
\label{sec:extra_evaluation}
Following the results on monocular depth estimation in the main paper, we describe the implementation details of the training, show additional results on different scenes and provide additional metrics on different test scenes. 

\paragraph{Implementation Details}
For all our depth estimation experiments, we use PyTorch~\cite{paszke2017automatic} and train for 20 epochs for comparability using Adam~\cite{kingma2014adam}.
Monocular approaches are trained with a batch size of 12 on one NVIDIA RTX-3090 GPU.
We chose $\lambda_{\text{s}}=10^{-3}$ and sample $S$ with $T=10$ frames offset due to small relative camera movement between frames and the high frame rate.
The RGB inputs are scaled to $480 \times 320$ for supervised training and to $320 \times 160$ for self-supervised training, respectively. 
The depth network regresses dense depth predictions on four pyramid levels, each with half the resolution of the previous.
Pose network and augmentations follow~\cite{monodepth2}.
We choose an initial learning rate of $1 \times 10^{-4}$ for 15 epochs, which we decrease to $1 \times 10^{-5}$ after 15 epochs in the self-supervised setting.
For the supervised case, we start with a learning rate of $1 \times 10^{-3}$, which we decrease every five epochs by a factor of ten.


\begin{table*}[!ht]
\centering
\caption{\textbf{Depth prediction comparison when training with different modalities and tested on different unseen scenes and seen scenes. }(Top) Evaluation against GT of depth predictions on the test set with dense supervision from different depth modalities. (Bottom) Predictions evaluated on respective modality. 
Error is reported as Sq.Rel. and RMSE in mm.
}
\begin{adjustbox}{}
\footnotesize
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{ll|cc|cccc|cccccc}
\shline
& Mask&\multicolumn{2}{c|}{Full Scene} & \multicolumn{2}{c}{Background} & \multicolumn{2}{c|}{All Objects}  & \multicolumn{2}{c}{Textured} & \multicolumn{2}{c}{Reflective} & \multicolumn{2}{c}{Transparent} \\
\hline
&Metric&Sq.Rel. & RMSE&Sq.Rel. & RMSE&Sq.Rel. & RMSE&Sq.Rel. & RMSE&Sq.Rel. & RMSE&Sq.Rel. & RMSE \\
\shline
\multirow{3}{*}{\rotatebox[origin=c]{-90}{Test 1}}     & I-ToF & 
24.78  &  148.09
 & 
22.25  &  151.07
 & 
29.62  &  123.19
 & 
16.47  &  99.08
 & 
102.79  &  214.60
 & 
44.29  &  134.44
\\
                                & D-ToF & 
24.23  &  151.72
 & 
23.74  &  159.28
 & 
22.85  &  110.88
 & 
16.22  &  101.12
 & 
57.14  &  148.61
 & 
30.23  &  107.23
\\
                                & Active Stereo & 
32.15  &  173.72
 & 
33.84  &  184.16
 & 
22.23  &  116.57
 & 
19.55 &  114.07
 & 
64.27  &  167.71
 & 
12.92  &  69.49
\\
\shline\\
\shline

\multirow{3}{*}{\rotatebox[origin=c]{-90}{Test 2}}     & I-ToF & 
27.42  &  123.79
 & 
22.66  &  116.86
 & 
39.85  &  139.67
 & 
48.66  &  144.92
 & 
16.15  &  99.44
 & 
25.15  &  122.25
\\
                                & D-ToF & 
23.00  &  115.40
 & 
21.18  &  113.27
 & 
27.89  &  119.59
 & 
30.00  &  112.92
 & 
15.81  &  90.89
 & 
23.73  &  117.72
\\
                                & Active Stereo & 
25.94  &  124.17
 & 
25.50  &  126.28
 & 
27.18  &  117.04
 & 
32.81  &  121.24
 & 
16.40  &  101.86
 & 
15.73  &  95.27
\\              
\shline\\
\shline

\multirow{3}{*}{\rotatebox[origin=c]{-90}{Test 3}}     & I-ToF & 
36.82  &  152.51
 & 
35.92  &  153.26
 & 
38.75  &  147.14
 & 
34.09  &  127.51
 & 
20.21  &  110.85
 & 
55.09  &  183.14
\\
                                & D-ToF & 

32.99  &  145.50
 & 
35.64  &  153.07
 & 
25.90  &  120.35
 & 
19.92  &  96.01
 & 
21.59  &  105.41
 & 
37.26  &  149.66
\\
                                & Active Stereo & 
31.63  &  141.77
 & 
35.24  &  151.37
 & 
22.44  &  110.42
 & 
23.47  &  106.63
 & 
14.49  &  94.51
 & 
21.21  &  109.53
\\                              
\shline\\
\shline

\multirow{3}{*}{\rotatebox[origin=c]{-90}{T. Seen}}     & I-ToF & 
9.87  &  77.99
 & 
4.62  &  57.10
 & 
33.91  &  133.46
 & 
6.18  &  60.48
 & 
35.65  &  119.76
 & 
91.30  &  224.27
\\
                                & D-ToF & 

15.43  &  93.31
 & 
11.62  &  79.89
 & 
31.12  &  123.97
 & 
4.40  &  51.91
 & 
17.42  &  82.29
 & 
89.19  &  212.55
\\
                                & Active Stereo & 
9.43  &  88.30
 & 
9.28  &  88.24
 & 
9.11  &  75.21
 & 
6.32  &  65.54
 & 
12.98  &  65.73
 & 
16.62  &  98.75
\\
\shline\\
\\
\\
\multicolumn{6}{l}{Tested on Modality:}
\\
\shline
\multirow{4}{*}{\rotatebox[origin=c]{-90}{Test Seen}}     & I-ToF & 
8.34  &  52.29
 & 
8.57  &  50.00
 & 

7.01  &  58.85
&
3.80  &  43.44
 & 
23.28  &  95.38
 & 
13.69  &  65.41
\\
                                & D-ToF & 

8.05  &  50.43
 & 
6.82  &  45.50
 & 
 
13.52  &  66.34
&
9.00  &  54.15
 & 
30.91  &  87.71
 & 
27.92  &  87.32
\\
                                & Active Stereo & 
39.25  &  101.76
 & 
40.87  &  102.29
 & 
 
30.32  &  90.00
&
32.24  &  90.49
 & 
23.36  &  72.21
 & 
37.25  &  101.23
\\
                                & GT & 
1.12  &  28.81
 & 
0.71  &  24.41
 & 
 
2.65  &  40.41
&
1.83  &  34.89
 & 
2.16  &  29.55
 & 
5.02  &  52.43
\\
\shline

\end{tabular}
}
\end{adjustbox}
\label{tab:depth_supervision_results_additional}
\end{table*}


\subsubsection{Quantitative evaluation}
\paragraph{Test scenes. }
Table~\ref{tab:depth_supervision_results_additional} summarizes the extensive quantitative evaluation of the supervised training with different depth modalities as supervision signal for different test scenes. Test scene 1 has a similar background compared to the training scenes and includes additional unseen objects. The scene is also observed from viewing angles that differ significantly from the training data. The background in test scene 2 is only partly observed in the training data and it includes mostly unseen objects. Test scene 3 is similar to test scene 2, but with a modified object layout and difficult lighting in the background from an additional bright light source above the scene. The additional test set with (partly) seen scenes is an additional test split which includes the first 10 frames of each training sequence. Please note that these frames have not been used during training. Here, we first test all predictions against the rendered ground truth (Top) and additionally on each individual respective modality (Bottom) to highlight the overfitting issue of invalid ground truth from each modality.
The results suggest that overall the supervision with accurate rendered ground truth achieves to generalize best for (mostly) unknown scenes. It is noticeable, that the active stereo achieves to produce good predictions for transparent objects and also performs well for reflective ones. The I-ToF and D-ToF predictions suffer from incorrect ground truth values for such objects.


\paragraph{Overfitting on (partly) seen scenes. }
The (partly) seen scene shows generally lower overall errors for all modalities as compared to the (mostly) unseen test scenes 1,2, and 3. Again, the active stereo can provide decent depth supervision for reflective and transparent objects, where the ToF sensors cannot provide valid depth. The prediction of the background of the scene performs worst for the active stereo, as the textureless wall is still problematic for the sensor.

When testing on the respective modality itself, the overfitting issue due to incorrect depth values of the sensor becomes apparent. It can be noticed, that for objects where the respective sensor cannot yield accurate depth values (e.g. transparent objects for I-ToF or reflective objects for D-ToF), the errors are significantly lower, indicating overfitting to the specific sensor modality.


\subsubsection{Qualitative predictions}
Figures~\ref{fig:qual_scene12_1},~\ref{fig:qual_scene13_1} and~\ref{fig:qual_scene14_1} show predictions on exemplary frames of the test scenes 1, 2 and 3, together with the different sensor modalities and the error plot of the prediction compared against the ground truth. The training with rendered ground truth generally performs best. Both ToF sensors show incorrect depth values for reflective or transparent objects which also translates to incorrect predictions in these areas (compare Fig.~\ref{fig:qual_scene12_1}. The predictions when training with active stereo as supervision are more blurry and show less distinct edges at depth boundaries when compared to other modalities, which may arise from many depth pixels being invalidated by the sensors around such boundaries (compare Fig.~\ref{fig:qual_scene13_1}). The very challenging test scene 3 with bright lighting and many unseen objects is difficult to predict for all training setups (compare Fig.~\ref{fig:qual_scene14_1}. We can see similar artifacts as described above. Additionally, the unseen trophy object with partly reflective and partly transparent material shows large errors for the sensor inputs as well as for its predictions. The desk surface is also incorrectly captured by the D-ToF sensors due to large reflections and MPI from the background.




\begin{figure*}[!p]
 \centering
    \includegraphics[width=\linewidth]{figures/qual_scene12_1.png}
    \caption{Qualitative evaluation on test scene 1. Each depth modality, the network prediction when trained with supervision of each modality, and the error, are shown as qualitative evaluation.}
    \label{fig:qual_scene12_1}
\end{figure*}

\begin{figure*}[!p]
 \centering
    \includegraphics[width=\linewidth]{figures/qual_scene13_1.png}
    \caption{Qualitative evaluation on test scene 2. Each depth modality, the network prediction when trained with supervision of each modality, and the error, are shown as qualitative evaluation.}
    \label{fig:qual_scene13_1}
\end{figure*}

\begin{figure*}[!p]
 \centering
    \includegraphics[width=\linewidth]{figures/qual_scene14_1.png}
    \caption{Qualitative evaluation on test scene 3. Each depth modality, the network prediction when trained with supervision of each modality, and the error, are shown as qualitative evaluation.}
    \label{fig:qual_scene14_1}
\end{figure*}

\subsection{Implicit Reconstruction}
\paragraph{Implementation Details}
As mentioned in the main paper, we follow NeRF~\cite{mildenhall2021nerf} and build upon the work of~\cite{roessle2022dense} without a depth completion network, but leverage the respective sensor depth with a scale-invariant depth loss $\mathcal{L}_{\text{D}}$. We use images with a resolution of $640 \times 480$ and process batches of 1024 rays. We set $\lambda_{\text{D}}$ to 0.1 and the learning rate to 0.0005 and optimize for 100k iterations with Adam optimizer~\cite{kingma2014adam}. 

\subsection{Camera Pose Estimation}
The analysis above focuses on dense monocular depth estimation and novel view synthesis as recent and important approaches - for which pixelwise prediction and evaluation are crucial. We add results for direct SLAM (DSO)~\cite{engel2017direct}, KinectFusion~\cite{kinectfusion} with different depth modalities, and COLMAP SfM~\cite{schoenberger2016sfm} in Fig.~\ref{fig:recon_comp}.
\begin{figure*}[h!]
 \centering
    \includegraphics[width=1.0\textwidth]{figures/recon_comp.png}
    \caption{Qualitative reconstruction results from SLAM and SfM.}
    \label{fig:recon_comp}
\end{figure*}

Tab.~\ref{tab:pose} summarizes the relative pose error for different approaches (cf. Fig~\ref{fig:recon_comp}). Note the pose accuracy results for KinectFusion~\cite{kinectfusion} align with the depth results from Tab.2 in the main paper.

\begin{table}[!t]
\centering
\caption{Relative Pose Error of SLAM and SfM.}
\resizebox{1.0\columnwidth}{!}{
    \centering 
    \begin{tabular}{l|c|ccc|c} 
    \shline
    Error & Direct (DSO) & Dense dToF & Dense iToF & Dense AS & SfM \\ 
    \shline
    rot [deg] & 0.22 & 0.18 & 0.51 & 0.56 & 10.76 \\
    trans [cm] & 0.27 & 0.31 & 0.68 & 0.62 & 2.86 \\
    \shline
    \end{tabular}}
    \label{tab:pose}
\end{table}

\clearpage

\section{Dataset}
\subsection{Detailed Dataset Description}
\label{sec:detailed_dataset_description}

Sec.~3 of the main paper mentioned that our dataset uses multiple images/depth sensors to collect the dataset with highly accurate annotations of the scene using the robotic arm in a synchronized manner. This section shows the detailed description of data we include in our dataset. 


\subsubsection{Polarization Camera}
\label{subsec:polarization_camera}

Fig.~\ref{fig:polarization_included} shows examples of images included for the polarization camera. As mentioned in Sec.~2 of the main paper, a polarization camera provides images with different polarization angles, which can extract cues like the surface normal by using the physical property of object material in the scene. The polarization camera we used in our dataset (See Sec.~3 in the main paper) provides polarized images at 4 different angles (0, 90, 180 270 degrees) which are saved in a single 2x2 image (Fig.~\ref{fig:polarization_included}, (a)). A regular RGB image is obtained by averaging the 4 images (Fig.~\ref{fig:polarization_included}, (b)). To showcase the results of the depth map trained with different depth cameras, we include warped depth images from each depth camera into the polarization camera coordinates using the extrinsic between the two cameras and its depth image (Fig.~\ref{fig:polarization_included}, (d-g)). These can be additionally used for RGBD-based depth completion research. On top of that, we include extra information, such as instance map (Fig.~\ref{fig:polarization_included}, (c)) to help train or validate pipelines for categorical level tasks, accurate 6d pose of the camera as the 4x4 matrix obtained from the robotic arm, extrinsic transformation between cameras as 4x4 matrices and camera intrinsics as 3x3 matrix.

\begin{figure*}[!htbp]
 \centering
    \includegraphics[width=\linewidth]{figures/polarization_included.PNG}
    \caption{Example of the images included for the polarization camera input (top) together with instance label map and depth estimates warped onto the same coordinate reference frame.}
    \label{fig:polarization_included}
\end{figure*}

\subsubsection{D-ToF Camera}
\label{subsec:dtof_camera}


Fig.~\ref{fig:dtof_included} shows an example of images included for the D-ToF camera. Direct ToF (D-ToF) camera senses the depth information of its surrounding by emitting an infrared signal and measuring the difference in time between the emitted and received signal. The quality of this modality highly depends on the reflection of the signal. It often suffers from specific physical noise such as Multi-Path-Interference (MPI) or strong material dependent artefacts (Fig.~\ref{fig:dtof_mpi}). For the D-ToF camera, we provide the depth map from the camera (Fig.~\ref{fig:dtof_included}, (a)) as well as its rendered ground truth depth map (Fig.~\ref{fig:dtof_included}, (b))  such that one can also research on D-ToF refinement pipelines to reduce such errors. As in the polarization camera, we include extra information such as instance label map (Fig.~\ref{fig:dtof_included}, (c)), camera pose, intrinsic and extrinsics of the camera as well.

\begin{figure*}[!htbp]
 \centering
    \includegraphics[width=\linewidth]{figures/dtof_included.PNG}
    \caption{Example of the images included for the D-ToF camera: its depth map (left), ground truth depth (centre) and an object instance label map (right).}
    \label{fig:dtof_included}
\end{figure*}


\subsubsection{I-ToF Camera}
\label{subsec:itof_camera}


Fig.~\ref{fig:itof_included} shows image examples for the I-ToF camera. Indirect ToF (I-ToF) cameras sense the depth information of their surrounding by emitting a frequency modulated signal and measuring the return signal. Unlike Direct ToF (D-ToF), I-ToF cameras do not calculate the time difference to infer the depth. Instead, the camera correlates the returning signal with phase-shifted emitting signals to generate 4 different measurements, called correlation images. These are measured as sinus functions of distance ($\left( \sin(d),\cos(d),-\sin(d),-\cos(d) \right) = \left( c_{1}, c_{2}, c_{3}, c_{4} \right)$ in Fig.~\ref{fig:itof_included}, (a)). Either arc-tangent formula or convolutional neural networks can be used to extract depth information from the correlation images. As I-ToF modality also relies on the reflection of the signal like in D-ToF, it suffers from similar artefacts, such as MPI and material dependent artefacts (compare qualitative results of the test scenes in Figs.~\ref{fig:qual_scene12_1}, ~\ref{fig:qual_scene13_1} and ~\ref{fig:qual_scene14_1}). Here, we provide raw correlation images and depth map from the camera (see Fig.~\ref{fig:itof_included}, (a,b)) as well as its rendered ground truth depth (Fig.~\ref{fig:itof_included}, (c)) such that one can train I-ToF depth improvement pipelines either from raw signal or from I-ToF depth itself. As the other cameras, extras such as instance map (Fig.~\ref{fig:itof_included}, (d)), camera pose, intrinsic and extrinsics are included.

\begin{figure*}[!htbp]
 \centering
    \includegraphics[width=\linewidth]{figures/itof_included.PNG}
    \caption{Example of the images included for the I-ToF camera.}
    \label{fig:itof_included}
\end{figure*}

\subsubsection{Active Stereo Camera}
\label{subsec:active_stereo_camera}

Fig.~\ref{fig:d435_included} shows the examples of images included for the Active Stereo camera. Stereo depth estimation infers depth using and photometric consistency and geometrical constraints from epipolar geometry and triangulates the depth map from the disparity between left and right cameras. As the disparity is calculated via matching on the image itself, the stereo based depth estimation methods suffers less from the specific material, but they suffer from other aspects such as stereo occlusion and large texture-less regions. Active projection (Active Stereo) is used to overcome this issue. We provide both, active and passive stereo left / right images (Fig.~\ref{fig:d435_included}, (a),(b)) and raw depth from the camera (active,  Fig.~\ref{fig:d435_included}, (c)) as well as the rendered ground truth (Fig.~\ref{fig:d435_included}, (d)). This allows to use our dataset to improve stereo methods from either passive or active stereo and also depth refinement pipelines. Similar to the other cameras, extras such as instance map (Fig.~\ref{fig:d435_included}, (e)), camera pose, intrinsic and extrinsics are included.

\begin{figure*}[!htbp]
 \centering
    \includegraphics[width=\linewidth]{figures/d435_included.PNG}
    \caption{Example of the images included for the Active Stereo camera.}
    \label{fig:d435_included}
\end{figure*}

\subsection{Error Analysis on Different Modality}
\label{sec:Error_analysis}
In this section, we show specific errors on each depth modality to illustrate the implication of the depth quality when the given modality is used as the ground truth, as well as advantage of using our rendered depth as the ground truth.

\subsubsection{D-ToF Camera}
\label{subsec:dtof_camera_error}
As mentioned in Subsec.~\ref{subsec:dtof_camera}, D-ToF modality suffers from its own reflection-based nature, such as MPI and material dependent artefacts. When the angle of the surface normal of the scene is close to the incident angle of the infrared signal, the strength of the reflected signal becomes weak due to scattering effects (Fig.~\ref{fig:dtof_mpi}, (a) blue arrow) while multiple scattered signals from the other surfaces which has more traveling distance are received and with stronger strength (Fig.~\ref{fig:dtof_mpi}, (a) red arrow) and interfere with the original signal (MPI), producing a wrong measurement of the depth on the area with further distance which looks like a reflection or shadow of the object to the surface (Fig.~\ref{fig:dtof_mpi}, (b) red marker). This effect can be intensified when the surface material is reflective, which gives even stronger artefact as its reflective surface bounces even weaker and noisier signal with less attenuation (Fig.~\ref{fig:dtof_mpi}, (a,b) yellow arrow\&marker). On the other hands, when the surface material is transparent, the emitted infrared signal rather goes through the object in the both ways  (Fig.~\ref{fig:dtof_mpi}, (a) green arrow) which at the end ignores the object and the sensor produce the depth value as similar level as its background (Fig.~\ref{fig:dtof_mpi}, (b) green marker - material dependent artefact). Quality of the depth map degrades slightly around some boundaries after warping into the RGB frame (Fig.~\ref{fig:dtof_aligned}, (b), red), while the invalid regions actually helps to invalidate more area on wrong depth especially on the reflective objects  (Fig.~\ref{fig:dtof_aligned}, (b), green) , which might become beneficial when it is used in the training.

\begin{figure*}[!htbp]
 \centering
    \includegraphics[width=\linewidth]{figures/dtof_not_aligned.PNG}
    \caption{Detailed ray paths with MPI and surface material induced error on D-ToF modality. While D-ToF produces dense and sharp depth, its quality is highly dependent on the surface material and the incident angle.}
    \label{fig:dtof_mpi}
\end{figure*}

\begin{figure*}[!htbp]
 \centering
    \includegraphics[width=\linewidth]{figures/dtof_aligned.PNG}
    \caption{Error after warping D-ToF into RGB view. Slight errors are introduced on some edges (red) while expansion of the invalid area helps to invalidate on the reflective objects (green).}
    \label{fig:dtof_aligned}
\end{figure*}


\subsubsection{I-ToF Camera}
\label{subsec:itof_camera_error}
As mentioned in Subsec.~\ref{subsec:itof_camera}, I-ToF modality suffers by its own reflection based nature as well similar to D-ToF, such as MPI and material dependent artefact (Fig.~\ref{fig:itof_mpi}. Although the quality of depth itself seems better as the depth itself is more dense (with less invalid region) and amount of the artefacts are less, it is hard to say I-ToF modality is better than D-ToF as these two camera are in different price range and power level. Also less invalid area but rather with wrong depth didn't help invalidating depth (Fig.~\ref{fig:itof_aligned}) not like in D-ToF case, which could result in artefact in the prediction when it is used as GT during the training.

\begin{figure*}[!tbp]
 \centering
    \includegraphics[width=\linewidth]{figures/itof_not_aligned.PNG}
    \caption{Depth quality from I-ToF camera. I-ToF modality suffers from same type of artefect as D-ToF. While depth map itself is more sense and suffers less from MPI artefact on the table.}
    \label{fig:itof_mpi}
\end{figure*}

\begin{figure*}[!tbp]
 \centering
    \includegraphics[width=\linewidth]{figures/itof_aligned.PNG}
    \caption{Error after warping I-ToF into RGB view. Not like D-ToF, most of depth error exists without being invalidated, which might introduce more error when it used as GT during the training.}
    \label{fig:itof_aligned}
\end{figure*}


\subsubsection{Active Stereo Camera}
\label{subsec:active_stereo_camera_error}
As the stereo camera uses left and right matching with photoelectric cue, depth map suffers less on the challenging material as the projection can be visible on the surface as well as left-right check can be performed to invalidate region with the wrong depth. For this reason, depth on glass or the reflective object is significantly more accurate compared to either of ToF modality (Fig.~\ref{fig:d435_not_aligned}, green arrow). On the other hands, due to its nature of pattern projection far distance that depth quality gets worsen as the scene gets further (Fig.~\ref{fig:d435_not_aligned}, red arrow) the projection pattern gets attenuated and spread in the far distance. Moreover, the depth map in general is more blurry, jittery, sparse and has wrong values on some regions without being invalidated (Fig.~\ref{fig:d435_not_aligned}, orange arrow) which can introduce negative influence when it is used as GT, such as blurriness and depth jittering. Error introduced by warping is trivial (Fig.~\ref{fig:d435_aligned}) as the original depth map is already blurry and sparse.

\begin{figure*}[!htbp]
 \centering
    \includegraphics[width=\linewidth]{figures/d435_not_aligned.PNG}
    \caption{Depth quality from Active Stereo camera. While depth map suffers less on the challenging material, quality of depth itself is far behind either of ToF modality in multiple aspects, such as sharpness, variance, sparsity.}
    \label{fig:d435_not_aligned}
\end{figure*}

\begin{figure*}[!htbp]
 \centering
    \includegraphics[width=\linewidth]{figures/d435_aligned.PNG}
    \caption{Error after warping Active Stereo into RGB view. Note that there isn't significant change in the depth quality after the warping.}
    \label{fig:d435_aligned}
\end{figure*}




\clearpage
\newpage

\subsection{Detailed Background and Objects Description}
\label{sec:bckgr_obj_description}


\begin{figure*}[!b]
 \centering
    \includegraphics[width=\linewidth]{figures/Chair.PNG}
    \caption{Chairs used in the dataset. Chairs in group (a) are used for the training set and the chair in (b) is used for the test set.}
    \label{fig:chair}
\end{figure*} 

\begin{figure*}[!p]
 \centering
    \includegraphics[width=\linewidth]{figures/background_full_new.PNG}
    \caption{Backgrounds used in the dataset. Note that one of the background in the group (b) is also included in the training set, but we varied the lighting condition to provide different various factors for evaluation.}
    \label{fig:wall}
\end{figure*}

As described in Sec.~4 in the main paper, our dataset comprises a total of 13 scenes divided into 10 scenes for training and 3 testing scenes composed of a mixture of 4 different chairs, 6 different tables, 64 household objects from 8 plus 4 different categories (i.e. cup, teapot, bottle, remote, boxes, can, glass, cutlery and tube, shoe, plastic kitchenware, trophy) and and 7 different indoor areas. Test sets have 1 unseen background and 2 seen backgrounds with and without different lighting and contain a mixture of seen/unseen objects from seen/unseen categories. In this section, we show detailed images of backgrounds, chairs, tables, and other objects. Fig.~\ref{fig:chair} and~\ref{fig:table} respectively show images of 3 chairs and 6 tables used in the dataset and their corresponding meshes. Fig.~\ref{fig:obj_train} and~\ref{fig:obj_test} show a collection of household objects used in training and test set. Fig.~\ref{fig:wall} shows 9 backgrounds used in the dataset and their corresponding meshes.


\begin{figure*}[!t]
 \centering
    \includegraphics[width=\linewidth]{figures/table.PNG}
    \caption{Tables used in the dataset. Tables in group (a) are used for the training set and the table in (b) is used for the test set. Note that, unlike small objects or chairs, we decide not to scan some parts of the large tables (e.g. end of their legs) as the cameras cannot see the part in their trajectories.}
    \label{fig:table}
\end{figure*}

\begin{figure*}[!b]
 \centering
    \includegraphics[width=\linewidth]{figures/household_train.PNG}
    \caption{Collection of small household objects used in the training set. Objects from 8 household categories are used in the training set, 3 of which have photometrically challenging surface material - partially reflective (can), transparent (glass/plastic), reflective (cutlery).}
    \label{fig:obj_train}
\end{figure*}

\begin{figure*}[!t]
 \centering
    \includegraphics[width=\linewidth]{figures/household_test.PNG}
    \caption{Collection of small household objects used in the test set. The test set comprises a mixture of seen (left column) and unseen (mid column) objects from 8 seen categories and a few objects from unseen categories (right column - tube, slipper, plastic kitchenware, trophy) are used.}
    \label{fig:obj_test}
\end{figure*}




\subsubsection{Detailed Scene Description}
\label{sec:scene_description}

As described, our training set is composed of 10 scenes, and the test set is composed of 3 scenes. For each scene, we include 2 different trajectories. Each trajectory covers 2 setups with and without objects (naked scene). This sums up to 800-1200 frames per scene and a total of ca.~10k frames. In this section, we show several sample images of the scenes in Fig.~\ref{fig:wall1},~\ref{fig:wall2}, and~\ref{fig:wall3}, ~\ref{fig:wall4}. Each of them consists of an annotated mesh and RGB images with different types of rendering, which show the diversity and quality of our dataset.

\begin{figure*}[!htbp]
 \centering
    \includegraphics[width=\linewidth]{figures/scene1_caption.PNG}
    \caption{Example images from Training Scene 1. The annotated mesh is shown on the left together with an RGB view from the scene (second from left) with and without objects. The overlayed masks (second from right) and the rendered depth (right) illustrate the annotation quality of our data.}
    \label{fig:wall1}
\end{figure*}

\begin{figure*}[!htbp]
 \centering
    \includegraphics[width=\linewidth]{figures/scene2-5_caption.PNG}
    \caption{Example images from Training Scene 2-5. The annotated mesh for 4 different scenes is shown on the left together with an RGB view from the scene (second from left) with and without objects. The overlayed masks (second from right) and the rendered depth (right) illustrate the annotation quality of our data.}
    \label{fig:wall2}
\end{figure*}

\begin{figure*}[!htbp]
 \centering
    \includegraphics[width=\linewidth]{figures/scene6-9.PNG}
    \caption{Example images from Training Scene 6-9. The annotated mesh for four different scenes is shown on the left together with an RGB view from the scene (second from left) with and without objects. The overlayed masks (second from right) and the rendered depth (right) illustrate the annotation quality of our data.}
    \label{fig:wall3}
\end{figure*}

\begin{figure*}[!htbp]
 \centering
    \includegraphics[width=\linewidth]{figures/scene10-13.PNG}
    \caption{Example images from Training Scene 10 and Test scene 1-3. The annotated mesh is shown on the left together with an RGB view from the scene (second from left) with and without objects. The overlayed masks (second from right) and the rendered depth (right) illustrate the annotation quality of our data. Note that the test scene 2,3 are recorded in the exactly same pose and trajectory but with the different lighting.}
    \label{fig:wall4}
\end{figure*}


\subsubsection{Partial Scanning of the Scene and Mesh Fitting}
\label{sec:partial_scanning}

As mentioned in Sec.~3 in the main paper, we use partial scanning and mesh fitting to annotate background, large objects, and objects outside the robotic workspace. This section shows images of partial scanning and the mesh fitting from one of the scenes as an example. The green box in Fig.~\ref{fig:annotated_objects}, (a) shows annotated meshes of the objects by the robotic arm. Once the objects are annotated, the scene is partially scanned with multiple viewpoints to make the scanning dense and cover multiple facets of the background. Note that the center of the scanning is not yet in the robot base coordinates (Fig.~\ref{fig:annotated_objects}, (a) blue box). Once the partial scanning is done, the scanned mesh is then fit onto the annotated objects, such that the partially scanned mesh origin concides with the robot base (Fig.~\ref{fig:annotated_objects}, (b)). Once the scanned mesh is put to robot base coordinates, we fit background, large objects, and distant objects meshes also in robot base coordinates to annotate them (Fig.~\ref{fig:fitting_final}, (a)). Fig.~\ref{fig:fitting_final}, (b-c) shows the result of the annotated mesh. %As this method may contain errors propagated from noise and error of the mesh, we manually refined the pose of the fitted mesh in few mm in translation and sub degree in rotation such that the objects can be well globally well aligned into all of the cameras' viewpoints. 
For the mesh fitting, we used Artec Studio 10 Professional (Artec 3D, Luxembourg) which runs a point correspondence and ICP-based method to fit the meshes.

\begin{figure*}[!htbp]
 \centering
    \includegraphics[width=\linewidth]{figures/scanning_before_after_fitting.PNG}
    \caption{Example of partial scanning of the scene before and after the fitting on scene 13. Note that the center of the partial scanned mesh is aligned to robot base (xyz coordinate marker) after fitting it onto the mesh of the annotated objects.}
    \label{fig:annotated_objects}
\end{figure*}

\begin{figure*}[!htbp]
 \centering
    \includegraphics[width=\linewidth]{figures/object_fitting_cut.PNG}
    \caption{Example of far objects and background fitting onto partially scanned mesh. Left: Background and objects are fit to partial scans. Centre: All annotated meshes are shown without partial scans. Right: Corresponding scene from the camera viewpoint with augmented object masks. Note that the annotation quality of meshes with partial scans and robot arm is similar. The annotated meshes via partial scanning are marked with red arrows.}
    \label{fig:fitting_final}
\end{figure*}




