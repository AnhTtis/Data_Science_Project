\section{Related Work}
\label{sec:relatedwork}
\subsection{Geometry from X}
A variety of sensor modalities have been used to obtain depth maps.
Typical datasets comprise one ground truth sensor used for all acquisitions, which is assumed to give accurate enough data to validate the models:

\smallskip
\noindent
\textit{Stereo Vision.}
In the stereo literature, early approaches~\cite{scharstein2002taxonomy} use a pair of passive cameras and restrict scenes to piecewise planar objects for triangulation.
Complex setups with an industrial robot and structured light can yield ground truth depth for stereo images~\cite{aanaes2016IJCV}. Robots have also been used to annotate keypoints on transparent household objects~\cite{liu2020keypose}.
As these methods are incapable of retrieving reliable depth in textureless areas where stereo matching fails, active sensors are used to project patterns onto the scenes to artificially create structures.
The availability of active stereo sensors makes it also possible to acquire real indoor environments~\cite{silberman2012indoor} where depth data at missing pixels is inpainted.
Structure from motion (SfM) is used to generate the depth maps of Sun3D~\cite{xiao2013sun3d} where a moving camera acquires the scenes and data is fused ex post. 
A temporally tracked handheld active sensor is further used for depth mapping for SLAM evaluation in the pioneering dataset of Sturm et al.~\cite{sturm2012benchmark}.
While advancing the field, its depth maps are limited to the active IR-pattern used by its RGB-D sensor.

\begin{figure*}[!t]
 \centering
    \includegraphics[width=\linewidth]{figures/annotation_pipeline_new_2.png}
    \vspace{-5mm}
    \caption{\textbf{Scanning Process Overview.} To extract highly accurate geometry, we design a multi-stage acquisition process. At first, 3D models are extracted with structured light 3D scanners (a). Scene objects (b) and mounted sensor rig (b) are calibrated towards a robot for accurate camera pose retrieval~\cite{PhoCal}. A motion trajectory is recorded in gravity compensation mode (d) and repeated to record synchronized images of all involved sensors (e). A partial digital twin of the 3D scene (f) is aligned to small (g) and larger (h) objects to retrieve an entire in silico replica of the scene which can be rendered from the camera views of each sensor used (i) which results in highly accurate dense depth maps that enable investigations of individual sensor components.}
    \label{fig:annotation_overview}
\end{figure*}


\smallskip
\noindent
\textit{Time-of-Flight Sensors.}
Further advances in active depth sensing emphasize ToF more. 
Initial investigations focus on simulated data~\cite{Guo_2018_ECCV} and controlled environments with little ambient noise~\cite{son2016learning}. 
The broader availability of ToF sensors in commercial products (e.g. Microsoft Kinect series) and modern smartphones (e.g. I-ToF of Huawei P30 Pro, D-ToF in Apple iPhone 12) creates a line of research around curing the most common sensor errors. These are multi-path interference (MPI), motion artefacts and a high level of sparsity and shot noise~\cite{jung2021wild}. Aside of classical active and passive stereo, we therefore also include D-ToF and I-ToF modalities in all our experiments.



\smallskip
\noindent
\textit{Polarimetric Cues.}
Other properties of light are used to indirectly retrieve scene surface properties in the form of normals for which the amount of linearly polarized light and its polarization direction provide information, especially for highly reflective and transparent objects~\cite{kalra2020deep,gao2021polarimetric}. 
Initial investigations for shape from polarization mainly analyse controlled setups~\cite{garcia2015surface,atkinson2006recovery,yu2017shape,smith2018height}. More recent approaches investigate also sensor fusion methods~\cite{kadambi2017depth} even in challenging scenes with strong ambient light~\cite{CroMo}. 
We consequently also acquire RGB+P data for all scenes.

\smallskip
\noindent
\textit{Synthetic Renderings.}
In order to produce pixel-perfect ground truth, some scholars render synthetic scenes~\cite{mayer2016large}. 
While this produces the best possible depth maps, the scenes are artificially created and lack realism, causing pipelines trained on Sintel~\cite{Butler:ECCV:2012} or SceneFlow~\cite{mayer2016large} to suffer from a synthetic-to-real domain gap. 
In contrast, we follow a hybrid approach and leverage pixel-perfect synthetic data from modern 3D engines to adjust highly accurate 3D models to real captures.


\begin{figure*}[!t]
 \centering
    \includegraphics[width=\linewidth]{figures/image_quality_wide_2.png}
    \vspace{-7mm}
    \caption{\textbf{Data Quality.} A full 3D reconstruction of the RGB scene (left) allows to render highly accurate depth maps from arbitrary views. These serve as GT to study sensor errors of various depth sensors for different scene structures (right). E.g., due to the measurement principle, the translucent glass becomes invisible for the ToF sensors.}
    \label{fig:dataset_quality}
\end{figure*}


\subsection{Monocular Depth Estimation}
Depth estimation from a single image is inherently ill-posed. 
Deep learning has enabled this task for real scenes. 

\smallskip
\noindent
\textit{Supervised Training.}
Networks can learn to predict depth with supervised training. Eigen et al.~\cite{eigen2014depth} designed the first monocular depth estimation network by learning to predict coarse depth maps, which are then refined by a second network. Laina et al.~\cite{laina2016deeper} improved the latter model by using only convolutional layers in a single CNN. 
The required ground truth often limits these methods to outdoor scenarios~\cite{Geiger_2013}. 
A way of bypassing this is to use synthetic data~\cite{mayer2018makes}. 
Narrowing down the resulting domain gap can be realized~\cite{Guo_2018_ECCV}.  MiDaS~\cite{Ranftl2022} generalizes better to unknown scenes by mixing data from 3D movies. 
To predict high-resolution depth, most methods use multi-scale features or post processing~\cite{Miangoleh2021Boosting,Wei2021CVPR} which complicates learning. 
If not trained on a massive set of data, these methods show limited generalization capabilities.

\smallskip
\noindent
\textit{Self-Supervision.}
Self-supervised monocular methods try to circumvent this issue. 
The first such methods~\cite{xie2016deep3d,garg2016unsupervised} propose to use stereo images to train a network for depth prediction. With it, the left image is warped into the right where photometric consistency serves as training signal. 
Monodepth~\cite{Godard_2017} added a left-right consistency loss to mutually leverage warping from one image into the other. 
Even though depth quality improves, it requires synchronized image pairs. 
Monocular training methods are developed that use only one camera where frames in a video are leveraged for the warping with simultaneously estimated poses between them. This task is more intricate, however, Monodepth2~\cite{monodepth2} reduces the accuracy gap between the stereo and monocular training by automasking and with a minimum reprojection loss. 
A large body of work further improves the task~\cite{yang2018lego,chen2019towards,spencer2020defeat,lee2021patch,ranftl2021vision,Ranftl2022} and investigates temporal consistency~\cite{luo2020consistent,watson2021temporal,ruhkamp2021attention}. 
To compare the effect of various supervision signals for monocular depth estimation, we utilized the ResNet backbone of the popular Monodepth2~\cite{monodepth2} together with its various training strategies. 



\subsection{Reconstruction and Novel View Synthesis}
The 3D geometry of a scene can be reconstructed from 2D images and optionally their depth maps~\cite{kinectfusion}. Scenes are stored explicitly or implicitly. Typical explicit representation include point clouds or meshes~\cite{curless1996volumetric} while popular implicit representation are distance fields~\cite{zeng20163dmatch} which provide the scene as a level set of a given function, or neural fields where the scene is stored in the weights of a network~\cite{xie2022}.

\smallskip
\noindent
\textit{NeRFs.}
Due to their photorealism in novel view synthesis, recent advances around neural radiance fields (\textit{NeRF})~\cite{mildenhall2021nerf} experience severe attention. 
In this setup, one network is trained on a posed set of images to represent a scene. The method optimizes for the prediction of volume density and view-dependent emitted radiance within a volume. Integration along query rays allows to synthesize novel views of static and deformable~\cite{park2021nerfies} scenes.
Most noticeable recent advances extend the initial idea to unbounded scenes of higher quality with Mip-NeRF 360~\cite{barron2022mip} or factor the representation into low-rank components with TensoRF~\cite{chen2022tensorf} for faster and more efficient usage. Also robustness to pose estimates and calibration are proposed~\cite{lin2021barf,wang2021nerf}. 
While the initial training was computationally expensive, methods have been developed to improve inference and training. 
With spherical harmonics spaced in a voxel grid structure, Plenoxels~\cite{fridovich2022plenoxels} speed up processes even without a neural network and interpolation techniques~\cite{sun2022direct} accelerate training. 
Geometric priors such as sparse and dense depth maps can regularize convergence, improve quality and training time~\cite{deng2022depth,roessle2022dense}.
Besides recent works on methods themselves, \cite{Reizenstein_2021_ICCV} propose to leverage real world objects from crowd-sourced videos on a category level to construct a dataset to evaluate novel view synthesis and category-centric 3D reconstruction methods.

We make use of most recent NeRF advances and analyse the impact of sensor-specific depth priors in~\cite{roessle2022dense} for the task of implicit scene reconstruction.
To neglect the influence of pose estimates and produce highly accurate data, we leverage the robotic pose GT of our dataset. 