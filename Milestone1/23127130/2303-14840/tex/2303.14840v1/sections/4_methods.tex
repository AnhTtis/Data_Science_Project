\section{Methodology}
\label{sec:methods}
The dataset described above allows for the first time for rigorous, in-depth analysis of different depth sensor modalities and a detailed quantitative evaluation of learning-based dense scene regression methods when trained with varying supervision signals.
We focus on the popular tasks of monocular depth estimation and implicit 3D reconstruction with the application of novel view synthesis.


\subsection{Depth Estimation}
To train the depth estimation from a single image, we leverage the widely adopted architecture from~\cite{monodepth2}. 
%This allows to learn with full supervision, perform semi-supervised training with ground-truth camera poses, and train with self-supervised configurations.
We train an encoder-decoder network with a ResNet18 encoder and skip connections to regress dense depth. Using different supervision signals from varying depth modalities allows to study the influence and the characteristics of the 3D sensors. 
Additionally, we analyze whether complementary semi-supervision via information of the relative pose between monocular acquisitions and consecutive image information of the moving camera can overcome sensor issues. 

We further investigate the network design influence on the prediction quality for the supervised case. For this, we train two high-capacity networks with transformer backbones on our data, namely DPT~\cite{ranftl2021vision} and MIDAS~\cite{Ranftl2022}. 

\paragraph{Dense Supervision}
In the fully supervised setup, depth modalities from the dataset are used to supervise the prediction of the four pyramid level outputs after upsampling to the original input resolution with:
%\begin{align}
$\mathcal{L}_{\text{supervised}}=\sum_{i=1}^{i=4}\left\Vert{\widetilde{D}}_i-{D}\right\Vert_{1}$,
  \label{equ:soft-argmin}
%\end{align}
where $D$ is the supervision signal for valid pixels of the depth map and $\widetilde{D}_i$ the predicted depth at pyramid scale $i$.

\paragraph{Self-Supervision}
Depth and relative pose prediction between consecutive frames of a moving camera can be formulated as coupled optimization problem. We follow established methods to formulate a dense image reconstruction loss through projective geometric warping~\cite{monodepth2}.
In this process, a temporal image $I_{t^\prime}$ at time $t^\prime$ is projectively transformed to the frame at time $t$ via:\\
% \begin{align}
$I_{t^\prime \to t} = I_{t^\prime}\Big\langle \text{proj}(D_t, T_{t \to t^\prime}, K) \Big\rangle$,
\label{eqn:warp}
% \end{align}
where $D_t$ is the predicted depth for frame $t$, $T_{t \to t^\prime}$ the relative camera pose, and $K$ the camera intrinsics.
The photometric reconstruction error~\cite{monodepth2,watson2021temporal,ruhkamp2021attention} between image $I_x$ and $I_y$, given by:
%~\cite{Godard_2017}
%\begin{align}
${E_{\text{pe}}}(I_x,I_y) = \alpha\tfrac{1-\text{SSIM}(I_x, I_y)}{2} + (1-\alpha) \left \Vert I_x-I_y \right \Vert_{1}$
\label{eqn:photo}
%\end{align}
is computed between target frame $I_t$ and each source frame $I_s$ with $s \in S$. The pixel-wise minimum error is retrieved to finally define $\mathcal{L}_{\text{photo}}$ over $S = [t-F, t+F]$ as 
% \begin{align}
$\mathcal{L}_{\text{photo}} = \min_{s \in S}  {E_{\text{pe}}}(I_t,I_{s \rightarrow t})$.
% \end{align}
The edge-aware smoothness $\mathcal{L}_{\text{s}}$ is applied~\cite{monodepth2} to encourage locally smooth depth estimations with the mean-normalized inverse depth $\overline{d_{t}}$ as
% \begin{align}
$\mathcal{L}_{\text{s}} = \left|\partial_{x}\overline{d_{t}}\right|e^{-\left|\partial_{x}I_{t}\right|} + \left|\partial_{y}\overline{d_{t}}\right|e^{-\left|\partial_{y}I_{t}\right|}$.
% \end{align}
The final training loss for the self-supervised setup is:
%\begin{align}
$\mathcal{L}_{\text{self-supervised}} = \mathcal{L}_{\text{photo}} +  \lambda_{\text{s}} \cdot \mathcal{L}_{\text{s}}$.
%\end{align}

\paragraph{Semi-Supervision}
For the semi-supervised training, the ground truth relative camera pose is leveraged. 
%to perform differentiable backwards warping with bilinear interpolation. 
The predicted depth estimate is used to formulate the photometric image reconstruction. We also enforce the smoothness loss as detailed above.

\paragraph{Data Fusion}
Despite providing high accuracy ground truth, our annotation pipeline is time-consuming. 
One may ask whether this cannot be done with multi-view data aggregation. 
We therefore compare the quality against the dense structure from motion method Kinect Fusion~\cite{kinectfusion} and an approach for TSDF Fusion~\cite{Zhou2018}. 
The synchronized sensor availability allows also to investigate and improve sensor fusion pipelines. To illustrate the impact of high quality GT for this task, we also train the recent raw ToF+RGB fusion network Wild-ToFu~\cite{jung2021wild} on our dataset. 

\subsection{Implicit 3D Reconstruction}
%\pat{describe problem, NeRF stuff + some details on DS-NeRF regularization}
Recent work on implicit 3D scene reconstruction leverages neural radiance fields (NeRF)~\cite{mildenhall2021nerf}. 
The technique works particularly well for novel view synthesis and allows to render scene geometry or RGB views from unobserved viewpoints.
%with a set of input images with leverages known camera intrinsics and extrinsics through differentiable rendering techniques. 
%While the focus of applications is to render RGB images from the implicit scene representation, a large capacity of the network weights encode the 3D scene geometry, as this is vital for accurate novel view synthesis. 
Providing additional depth supervision regularizes the problem such that fewer views are required and training efficiency is increased~\cite{deng2022depth,roessle2022dense}.
%By providing an additional supervision signal for the synthesized depth representation, e.g. from sparse structure-from-motion (SfM) estimates, the optimisation can be improved and less views are required~\cite{deng2022depth}.
%When providing dense depth priors for the neural radiance field~\cite{roessle2022dense}, novel view synthesis and geometric scene estimates can be improved even more. 
%In~\cite{roessle2022dense} dense depth priors are learned through depth completion of sparse SfM 3D points by supervising with RGB-D data for later use in the NeRF.
%This does not take into account the inherent artefacts of such sensors as discussed here.
We follow the motivation of~\cite{roessle2022dense} and leverage different depth modalities to serve as additional depth supervision for novel view synthesis. 
Following NeRF literature~\cite{mildenhall2021nerf,roessle2022dense}, we encode the radiance field for a scene in an MLP $F_{\theta}$ to predict colour $\mathbf{C} = [r, g, b]$ and volume density $\sigma$ for some 3D position $\mathbf{x} \in \mathbb{R}^3$ and viewing direction $\mathbf{d} \in \mathbb{S}^2$. We use the positional encoding from~\cite{roessle2022dense}.
For each pixel, a ray $\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}$ from the camera origin $\mathbf{o}$ is sampled through the volume at location $t_k \in [t_n, t_f]$ between near and far planes by querying $F_{\theta}$ to obtain colour and density:\\
% \begin{align}
$\hat{\mathbf{C}}(\mathbf{r}) = \sum_{k=1}^{K} w_k\mathbf{c}_k \hspace{0.25em} \text{with} \hspace{0.25em}  w_k=T_k\left(1 - \exp(-\sigma_k\delta_k)\right)$, \\
%\, , 
% \end{align}
% \begin{align}
$T_k= \exp \left( -\sum_{k'=1}^{k} \sigma_{k'}\delta_{k'}\right)\, \hspace{0.25em} \text{and}  \hspace{0.25em}
\delta_k = t_{k+1} - t_k$. \\
%\, .
% \end{align}
The NeRF depth $\hat{z}(\mathbf{r})$ is computed by:
%\begin{align}
$\hat{z}(\mathbf{r}) = \sum_{k=1}^{K}w_kt_k$
%\end{align}
% NeRF parameters $\theta$ are optimized via the standard mean squared error (MSE)loss for color $\mathcal{L}_{\mathrm{color}}$ 
and the depth regularization for an image with rays $\mathcal{R}$ is:
% $\mathcal{L}_{\mathrm{depth}}$: 
%\begin{align}
%\mathcal{L}_{\text{col}} = \displaystyle\sum_{\mathbf{r}}
%\begin{Vmatrix} \hat{\mathbf{C}}(\mathbf{r}) - \mathbf{C}(\mathbf{r}) \end{Vmatrix}_2^2, \ 
 %    + 
%     \lambda \frac{\vert\hat{z}(\mathbf{r}) - {z}(\mathbf{r})\vert}{\hat{z}(\mathbf{r}) + {z}(\mathbf{r})}\Big) , 
    % \mathcal{L}_{\mathrm{color}}(\mathbf{r}) + \lambda \mathcal{L}_{\mathrm{depth}}(\mathbf{r})
    % \\
    % \mathcal{L}_{\mathrm{color}}(\mathbf{r}) &= \begin{Vmatrix} \hat{\mathbf{C}}(\mathbf{r}) - \mathbf{C}(\mathbf{r}) \end{Vmatrix}_2^2, \\
    % \mathcal{L}_{\mathrm{depth}}(\mathbf{r}) &= \frac{\vert\hat{z}(\mathbf{r}) - {z}(\mathbf{r})\vert}{\hat{z}(\mathbf{r}) + {z}(\mathbf{r})}, 
$\mathcal{L}_{\text{D}} = \displaystyle\sum_{\mathbf{r} \in \mathcal{R}}
%\begin{Vmatrix} \hat{\mathbf{C}}(\mathbf{r}) - \mathbf{C}(\mathbf{r}) \end{Vmatrix}_2^2
 %    + 
     \frac{\vert\hat{z}(\mathbf{r}) - {z}(\mathbf{r})\vert}{\hat{z}(\mathbf{r}) + {z}(\mathbf{r})}$ , 
%\end{align}
where $z(\mathbf{r})$ is the depth of the sensor.
%For more details on ray sampling, depth-guided sampling and optimisation, we refer to~\cite{roessle2022dense}.
Using the mean squared error (MSE) loss 
$\mathcal{L}_{\text{colour}} = \text{MSE}(\hat{\mathbf{C}}, {\mathbf{C}})$ 
% $\mathcal{L}_{\text{colour}} = \begin{Vmatrix}\hat{\mathbf{C}}, {\mathbf{C}}\end{Vmatrix}_2^2$ 
for synthesized colours, the final training loss is:
%\begin{align}
$\mathcal{L}_{\text{NeRF}} = \mathcal{L}_{\text{colour}} +  \lambda_{\text{D}} \cdot \mathcal{L}_{\text{D}}$.
%\end{align}