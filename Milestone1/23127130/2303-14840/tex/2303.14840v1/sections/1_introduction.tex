\section{Introduction}
\label{sec:introduction}
Our world is 3D. Distance measurements are essential for machines to understand and interact with our environment spatially. 
Autonomous vehicles~\cite{geiger2012we,ruhkamp2021attention,kong2020semantic,su2023opa} need this information to drive safely, robot vision requires distance information to manipulate objects~\cite{fang2020graspnet,wang2021demograsp,da2dataset,zhai2022monograspnet}, and AR realism benefits from spatial understanding~\cite{kopf2021robust,busam2019sterefo}.

\begin{figure}[t]
\begin{center}
\includegraphics[width=1.00\linewidth]{figures/new_teaser.png}
\vspace{-0.9cm}
\end{center}
  \caption{Other datasets for dense 3D vision tasks reconstruct the scene as a whole in one pass~\cite{replica19arxiv,dai2017scannet,Matterport3D}, resulting in low quality and accuracy (cf. red boxes). On the contrary, our dataset scans the background and every object in the scene separately a priori and annotates them as dense and high-quality 3D meshes. Together with precise camera extrinsics from robotic forward-kinematics, this enables a fully dense rendered depth as accurate pixel-wise ground truth with multimodal sensor data, such as RGB with polarization, D-ToF, I-ToF and Active Stereo. Hence, it allows quantifying different downstream 3D vision tasks such as monocular depth estimation, novel view synthesis, or 6D object pose estimation.}
\label{fig:teaser}
\vspace{-0.2cm}
\end{figure}



A variety of sensor modalities and depth prediction pipelines exist.
The computer vision community thereby benefits from a wide diversity of publicly available datasets~\cite{scharstein2002taxonomy,geiger2012we,silberman2012indoor,sturm2012benchmark,xiao2013sun3d,PhoCal,CroMo}, which allow for evaluation of depth estimation pipelines.
Depending on the setup, different sensors are chosen to provide ground truth (GT) depth maps, all of which have their respective advantages and drawbacks determined by their individual principle of distance reasoning.
Pipelines are usually trained on the data without questioning the nature of the depth sensor used for supervision and do not reflect areas of high or low confidence of the GT.

Popular \textbf{passive sensor} setups include multi-view stereo cameras where the known or calibrated spatial relationship between them is used for depth reasoning~\cite{scharstein2002taxonomy}. 
Corresponding image parts or patches are photometrically or structurally associated, and geometry allows to triangulate points within an overlapping field of view.
Such photometric cues are not reliable in low-textured areas and with little ambient light where \textbf{active sensing} can be beneficial~\cite{silberman2012indoor,sturm2012benchmark}.
Active stereo can be used to artificially create texture cues in low-textured areas and photon-pulses with a given sampling rate are used in Time-of-Flight (ToF) setups either directly (D-ToF) or indirectly (I-ToF)~\cite{Guo_2018_ECCV}.
With the speed of light, one can measure the distance of objects from the return time of the light pulse, but unwanted multi-reflection artifacts also arise.
Reflective and translucent materials are measured at incorrect far distances, and multiple light bounces distort measurements in corners and edges. 
While ToF signals can still be aggregated for dense depth maps, a similar setup is used with LiDAR sensors which sparsely measure the distance using coordinated rays that bounce from objects in the surrounding.
The latter provides ground truth, for instance, for the popular outdoor driving benchmark KITTI~\cite{geiger2012we}.
While LiDAR sensing can be costly, radar~\cite{gasperini2021r4dyn}  provides an even sparser but more affordable alternative.
\textbf{Multiple modalities} can also be fused to enhance distance estimates.
A common issue, however, is the inherent problem of warping onto a common reference frame which requires the information about depth itself~\cite{lopez2020project,jung2021wild}.
While multi-modal setups have been used to enhance further monocular depth estimation using self-supervision from stereo and temporal cues~\cite{monodepth2,CroMo}, its performance analysis is mainly limited to average errors and restricted by the individual sensor used.
An unconstrained analysis of depth in terms of RMSE compared against a GT sensor only shows part of the picture as different sensing modalities may suffer from drawbacks.

Where are the drawbacks of current depth-sensing modalities - and how does this impact pipelines trained with this (potentially partly erroneous) data?
Can self- or semi-supervision overcome some of the limitations posed currently?
To objectively investigate these questions, we provide multi modal sensor data as well as highly accurate annotated depth so that one can analyse the deterioration of popular monocular depth estimation and 3D reconstruction methods (see Fig.~\ref{fig:teaser}) on areas of different photometric complexity and with varying structural and material properties while changing the sensor modality used for training.
To quantify the impact of sensor characteristics, we build a unique camera rig comprising a set of the most popular indoor depth sensors and acquire synchronised captures with highly accurate ground truth data using 3D scanners and aligned renderings.
To this end, our main contributions can be summarized as follows:
\begin{enumerate}
    \item We question the measurement quality from commodity \textbf{depth sensor} modalities and analyse their \textbf{impact} as supervision signals for the dense 3D vision tasks of depth estimation and reconstruction.
    \item We investigate performance on texture-varying material as well as \textbf{photometrically challenging} reflective, translucent and transparent \textbf{areas} where \textbf{learning methods} systematically \textbf{reproduce sensor errors}.
    \item To objectively assess and quantify different data sources, we contribute an \textbf{indoor dataset} comprising an unprecedented combination of \textbf{multi-modal sensors}, namely I-ToF, D-ToF, monocular RGB+P, monochrome stereo, and active light stereo together with highly accurate ground truth. 
\end{enumerate}