\section{Data Acquisition \& Sensor Modalities}
\label{sec:dataset}


% \begin{figure}[!b]
%  \centering
%     \includegraphics[width=\linewidth]{figures/hammer_other_datasets_comp.pdf}
%     \caption{\textbf{Issues of Other Datasets.} Multiple sources of error, e.g. missing glass surfaces (table in left and in right scenes), noisy background, missing data, corrupted thin objects, holes, etc. exist in Replica~\cite{replica19arxiv}, ScanNet~\cite{dai2017scannet}, or MatterPort3D~\cite{Matterport3D}.}
%     \label{fig:comparison_datasets}
% \end{figure}


We set up scenes composed of multiple objects with different shapes and materials to analyse sensor characteristics. 3D models of photometrically challenging objects with reflective or transparent surfaces are recorded with high quality a priori and aligned to the scenes. Images are captured from a synchronised multi-modal custom sensor mounted at a robot end-effector to allow for precise pose camera measurements~\cite{PhoCal}. High-quality rendered depth can be extracted a posteriori from the fully annotated scenes for the viewpoint of each sensor. The acquisition pipeline is depicted in Fig.~\ref{fig:annotation_overview}.
 
Previous 3D and depth acquisition setups~\cite{replica19arxiv,Matterport3D,dai2017scannet} scan the scene as a whole which limits the quality by the used sensor. We instead separately scan every single object, including chairs and background, as well as small household objects a priori with two high-quality structured light object scanners. 
This process significantly pushes the annotation quality for the scenes as the robotic 3D labelling process only has a point RMSE error of $0.80$~mm~\cite{PhoCal}. 
For comparison, a Kinect Azure camera induces a standard deviation of $17$~mm in its working range~\cite{liu2021stereobj}. 
The accuracy allows us to investigate depth errors arising from sensor noise objectively, as shown in Fig.~\ref{fig:dataset_quality}, while resolving common issues of imperfect meshes in available datasets (cf. Fig.~\ref{fig:teaser}, left).


\subsection{Sensor Setup \& Hardware Description}
The table-top scanner (EinScan-SP, SHINING 3D Tech. Co., Ltd., Hangzhou, China) uses a rotating table and is designed for small objects. The other is a hand-held scanner (Artec Eva, Artec 3D, Luxembourg) which we use for larger objects and the background. 
For objects and areas with challenging material, self-vanishing 3D scanning spray (AESUB Blue) is used. For larger texture-less areas such as tables and walls we temporarily attach small markers~\cite{garrido2014automatic} to the surface to allow for relocalization of the 3D scanner. 
The robotic manipulator is a KUKA LBR iiwa 7 R800 (KUKA Roboter GmbH, Germany) with a position accuracy of $\pm0.1$~mm. 
We validated this during our pivot calibration stage (Fig.~\ref{fig:annotation_overview} b) by calculating the 3D location of the tool tip (using forward kinematics and hand-tip calibration) while varying robot poses. The position varied in $\left[-0.158,0.125\right]$~mm in line with this.
Our dataset features a unique multi-modal setup with four different cameras, which provide four types of input images (RGB, polarization, stereo, Indirect ToF (I-ToF) correlation) and three different depth images modalities (Direct ToF (D-ToF), I-ToF, Active Stereo). RGB and polarization images are acquired with a Phoenix 5.0 MP Polarization camera (PHX050S1-QC, LUCID Vision Labs, Canada) equipped with a Sony Polarsens sensor (IMX264MYR CMOS, Sony, Japan). 
To acquire stereo images, we use an Intel RealSense D435 (Intel, USA) with switched off infrared projector. Depth is acquired from an Intel RealSense L515 D-ToF sensor, an Intel Realsense D435 active stereo sensor with infrared pattern projection, and a Lucid Helios (HLS003S-001, LUCID Vision Labs, Canada) I-ToF sensor. 
A Raspberry Pi triggers each camera separately to remove interference effects between infrared signals of depth sensors. 
The hardware is rigidly mounted at the robot end-effector (see Fig.~\ref{fig:hardware}) which allows to stop frame-by-frame for the synchronized acquisition of a pre-recorded trajectory.


\begin{figure}[!t]
 \centering
    \includegraphics[width=0.9\linewidth]{figures/sensor_rig_one_column.png}
    \vspace{-2mm}
    \caption{\textbf{Camera Rig and 3D Sensor Data.} The custom multi-modal sensor rig comprises depth sensors for I-ToF (top left), Stereo (lower left), D-ToF (lower right), and RGB-P (Polarization, top right). It is fixed to a robot end-effector (top) and a Raspberry Pi (right) triggers acquisition.}
    \label{fig:hardware}
\end{figure}

\begin{table*}[!ht]
\setlength{\tabcolsep}{7pt}
\centering
\caption{\textbf{Comparison of Datasets}. Shown are differences between our dataset and previous multi-modal depth datasets for indoor environments. Our dataset is the only one that provides highly accurate GT (Depth, Surface Normals, 6D Object Poses, Instance Masks, Camera Poses, Dense Scene Mesh) together with varying sensor data for real scenes.}
\label{tab:dataset_comparison}
\vspace{-2mm}
\begin{tabular}{r | c c c c c c c c c c c c } 
\toprule
  \small Dataset &
  {\small Acc.GT}          &
  {\small RGB}        &
  {\small D-ToF}          &
  {\small I-ToF}        &
  {\small Stereo} &
  {\small Act.Stereo} &
  {\small Polar.} &
  {\small Indoor}          &
  {\small Real}         &
  {\small Video}        &
  {\small Frames}       \\ 
\midrule
Agresti~\cite{Agresti_2019_CVPR} & -          &  - & - & \checkmark & -          & -& -          & \checkmark & \checkmark & -          & $113$      \\
CroMo~\cite{CroMo} & - &  -  & - & \checkmark & \checkmark & \checkmark & \checkmark & (\checkmark) & \checkmark & \checkmark & ${>}10$k    \\
Zhu~\cite{zhu2019depth}      & -            & (\checkmark) & -          & - & - & - & \checkmark & \checkmark & \checkmark & -        & $1$   \\
Sturm~\cite{sturm2012benchmark}         & - & \checkmark & - & - & - & -          & -          & \checkmark & \checkmark & \checkmark & ${>}10$k   \\
\cite{kadambi2017depth}/\cite{qiu:2019a}/\cite{ba2020deep}            & - & \checkmark   & -          & - & - & - & \checkmark & \checkmark & \checkmark & -          & $1/40/300$ \\
Guo~\cite{Guo_2018_ECCV}       & \checkmark   & -          & - & \checkmark & -          & - & -          & \checkmark & - & - & $2000$\\
\textbf{Ours} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & ${>}10$k\\
\bottomrule
\end{tabular}
\end{table*}


\subsection{Scene Statistics \& Data Comparison}
We scanned 7 indoor areas, 6 tables, and 4 chairs, with the handheld scanner as background and large objects. 64 household objects from 9 categories (bottle, can, cup, cutlery, glass, remote, teapot, tube, shoe) are scanned with the tabletop structured light scanner. The data comprises 13 scenes split into 10 scenes for training and 3 scenes for testing. Each scene is recorded with 2 trajectories of 200-300 frames with and without the objects. This sums up to 800-1200 frames per scene, with a total of 10k frames for training and 3k frames for our test set. The 3 test scenes have different background setups: 1) Seen background, 2) Seen background with different lighting conditions and 3) Unseen background and table, with three different object setups: 1) Seen objects 2) Unseen objects from the seen category 3) Unseen objects from unseen categories (shoe and tube).
Table~\ref{tab:dataset_comparison} compares our dataset with various existing setups. To the best of our knowledge, our dataset is the only multi-modal dataset comprising RGB, ToF, Stereo, Active Stereo, and Polarisation modalities simultaneously with reliable ground truth depth maps. 