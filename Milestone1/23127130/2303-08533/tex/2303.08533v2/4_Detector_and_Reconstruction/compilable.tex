% don't remove the folling lines, and edit the defintion of \main if needed
%\documentclass[../report.tex]{subfiles}
\documentclass[../ReviewEPJC.tex]{subfiles}
\providecommand{\main}{..}
%\IfEq{\jobname}{\currfilebase}{\AtEndDocument{\bibliographystyle{report}\bibliography{references}}}{}
\IfEq{\jobname}{\currfilebase}{\AtEndDocument{\bibliographystyle{report}\bibliography{ReportBIBOnINSPIRE,ReportBIBNOTOnINSPIRE}}}{}
% until here

% don't remove the folling lines, and edit the defintion of \main if needed
%\documentclass[../ReviewEPJC.tex]{subfiles}
%\providecommand{\main}{..}
%\IfEq{\jobname}{\currfilebase}{\AtEndDocument{\bibliographystyle{report}\bibliography{ReportBIBOnINSPIRE,ReportBIBNOTOnINSPIRE}}}{}
% until here

%\input symbols.tex

\begin{document}

%\linenumbers

\section{Particle detectors and event reconstruction}
\label{sec:detectorandreconstruction}

The unstable nature of muons makes the beam-induced background (BIB) a much more challenging issue at a muon collider than it is at facilities that use stable-particle beams. For instance, with $2.2 \cdot 10^{12}$ muons per bunch a \qty{1.5}{\TeV} muon beam leads to about $2 \cdot 10^5$ muon decays per meter in a 3~TeV MuC with parameters as in Table~\ref{MC:t:param}. The interactions of the decay products with the accelerator lattice produce even larger amounts of particles that eventually reach the detector, making the reconstruction of clean $\mu^+ \mu^-$ collision events nearly impossible without a dedicated BIB mitigation. 

Muon collider detectors and event reconstruction techniques therefore need to be designed specifically to cope with the presence of the continuous flux of secondary and tertiary particles from the BIB. This section reviews the state-of-the-art design studies, and it is organised as follows. In Section~\ref{sec:environment} we describe the muon collision environment based on simulations of the BIB fluxes and composition reaching the detector. A tentative detector model is employed. The software setup used for the detector response simulation is described in Section~\ref{sec:software}. Section~\ref{sec:technology} presents promising technologies that could be employed in the tracking detector, the calorimeter systems, and dedicated muon spectrometers. General considerations regarding trigger systems and data acquisition are also discussed. Section~\ref{sec:performance} describes the status of development of the reconstruction algorithms and their expected performance for the basic objects needed to carry out a comprehensive physics programme. The reconstruction of other objects, as $\tau$ leptons or missing momentum, is still in progress, but it is expected to pose challenges similar to the ones that have been already solved. A discussion of the special challenges and opportunities for progress that are posed by the forward region of the detector is reported in Section~\ref{sec:fwdandlumi}. The summary and conclusions are presented in Section~\ref{sec:outlook}.

\subsection{Collision environment}
\label{sec:environment}

The BIB creates a large particle flux that interacts with the detector elements. On top of a detector model, its detailed simulation would require the design of the machine interaction region and of the Machine–Detector Interface (MDI). In fact, the BIB emerges from a chain of interactions with the material that composes these elements, entailing a strong dependence on their configuration of the BIB composition, flux, and energy spectra~\cite{Mokhov:2011zzd,Mokhov:2014hza,Bartosik:2019dzq,Lucchesi:2020dku}. The interaction region and MDI design also offers opportunities for the mitigation of the level of BIB that reaches the detector. 

Since no final design of these elements nor of the detector is available, and even the conceptual design of the collider facility is ongoing, current studies are based on tentative configurations, described below.

\begin{figure}[t]
    \begin{center}
        \centering
        \includegraphics[trim={0.8cm 0.8cm 0.8cm 0.8cm},clip,width=0.5\textwidth]{figures/detector-v1.pdf}
    \end{center}
    \caption{Illustration of the full detector, from the \textsc{Geant4} model. Different colours represent different sub-detector systems: the innermost region, highlighted in the yellow shade, represents the tracking detectors. The green and red elements represent the calorimeter system, while the blue outermost shell represents the magnet return yoke instrumented with muon chambers. The space between the calorimeters and the return yoke is occupied by a 3.57~T solenoid magnet.}
    \label{fig:detector}
\end{figure}

\subsubsection*{Detector model}
%\label{subsec:layout}

The design of a dedicated experiment is still in its preliminary phase, but some general conclusions can be already drawn. Given the breadth of the expected physics programme, a hermetic detector with angular coverage as close as possible to $4\pi$ is required. The detector will feature a cylindrical layout and will include: an inner tracking detector immersed in a magnetic field; a set of calorimeter systems designed to fully contain the products of the muon collisions; and an external muon spectrometer. 


\begin{figure*}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/bib/geometryok_color.png}
    \includegraphics[width=.7\textwidth, trim={-10 0 0 -10}]{figures/bib/geometry_zoom2_color.png}
    \caption{Cross-sectional view of the MDI as designed by the MAP collaboration for a $\sqrt{s} = $1.5~TeV MuC and visualised with FLUKA. Distinct colours represent different materials of the MDI: tungsten (green), borated polyethylene (dark magenta), iron (dark yellow), and concrete (gray). The black box in the center encloses the detector volume, which is excluded from the standalone BIB simulation process. Dimensions are reported in centimeters.}
    \label{fig:bib-mdi}
\end{figure*}


The tentative detector model we consider, referred to as Muon Collider Detector (MCD), is based on the CLICdet concept~\cite{CLICdp:2017vju,CLICdp:2018vnx,ILDConceptGroup:2020sfq,ILC:2007vrf}. The innermost system consists of a full-silicon tracking detector divided in three sub-detectors: the Vertex Detector, the Inner and the Outer Tracker. The tracking detector is surrounded by a calorimeter system that consists of an electromagnetic calorimeter (ECAL) and a hadronic calorimeter (HCAL), and is immersed in a magnetic field of 3.57~T provided by a solenoid with an inner bore of 3.5~m. Finally, the outermost part of the detector features a magnet iron yoke designed to contain the return flux of the magnetic field and is instrumented with muon chambers. The full detector is shown in Figure~\ref{fig:detector}. 
The most relevant modifications to the CLICdp detector are in the tracker topology. They are introduced for the installation of two double–cone shielding absorbers made of tungsten with a borated polyethylene (BCH2) coating and having an opening angle of $10^{\circ}$, referred to as ``nozzles''. The nozzles are located inside the detector in the forward regions\footnote{A right-handed coordinate system with its origin at the nominal interaction point in the centre of the detector is used. Cylindrical coordinates $(r,\phi)$ are used in the transverse plane, $\phi$ being the azimuthal angle around the z-axis and $\theta$ the polar angle with respect to the z-axis.} along the beam axis in the region between 6 and 600 cm away from the Interaction Point (IP), as displayed in Figure~\ref{fig:bib-mdi}.

The installation of the nozzles was proposed by the MAP collaboration~\cite{Mokhov:2011zzd} in order to mitigate the BIB effects. These nozzles, assisted by the magnetic field induced by a solenoidal magnet encasing the innermost detector region, could trap most of the electrons arising from muon decays close to the IP, as well as most of incoherent $e^{+}e^{-}$ pairs generated at the IP. With this sophisticated shielding in the MDI region, a total BIB reduction of more than three orders of magnitude was obtained~\cite{Mokhov:2011zzd}. The exact shape and positioning of the nozzles, including the \qty{10}{\degree} opening angle and \qty{12}{\centi\metre} distance between the tips, was optimised specifically for the MAP design of a MuC with $\sqrt{s} =$~\qty{1.5}{\TeV} energy in the centre of mass. Their re-optimisation will be an important component of future work on the design of the MDI for the 3 and 10~TeV colliders.

\subsubsection*{Characterisation of BIB}
%\label{sec:bib}

\begin{table*}[!ht]
    \centering
        \caption{Multiplicities of different types of particles after the shielding structure, therefore arriving on the detector surface. A single bunch crossing with $2\cdot 10^{12}$ muons is considered. In all cases, the MAP 1.5~TeV collider design and optimised MDI is assumed.}
    \label{tab:BIB_com_energy}
    \vspace{2mm}
    \begin{tabular}{|l|c|c|c|c|c|} \hline
         Monte Carlo simulator                          & MARS15             & MARS15            & FLUKA             & FLUKA             & FLUKA \\ \hline
         Beam energy [GeV]                              & 62.5               & 750               & 750               & 1500              & 5000  \\
         $\mu$ decay length [m]                         & $3.9\cdot 10^5$    & $46.7\cdot 10^5$  & $46.7\cdot 10^5$  & $93.5\cdot 10^5$  & $311.7\cdot 10^5$\\
         $\mu$ decay/m/bunch                            & $51.3\cdot 10^5$   & $4.3\cdot 10^5$   & $4.3\cdot 10^5$   & $2.1\cdot 10^5$   & $0.64\cdot 10^5$ \\
         Photons ($E_{\gamma}>0.1$~MeV)                 & $170\cdot 10^6$    & $86\cdot 10^6$    & $51\cdot 10^6$    & $70\cdot 10^6$    & $107\cdot 10^6$  \\
         Neutrons ($E_{n}> 1$~MeV)                      & $65\cdot 10^6$     & $76\cdot 10^6$    & $110\cdot 10^6$   & $91\cdot 10^6$    & $101\cdot 10^6$ \\ 
         Electrons \& positrons ($E_{e^{\pm}}>0.1$~MeV) & $1.3\cdot 10^6$    & $0.75\cdot 10^6$  & $0.86\cdot 10^6$  & $1.1\cdot 10^6$   & $0.92\cdot 10^6$\\
         Charged hadrons ($E_{h^{\pm}}>0.1$~MeV)        & $0.011\cdot 10^6$  & $0.032\cdot 10^6$ & $0.017\cdot 10^6$ & $0.020\cdot 10^6$ & $0.044\cdot 10^6$ \\
         Muons ($E_{\mu^{\pm}}>0.1$~MeV)                & $0.0012\cdot 10^6$ & $0.0015\cdot 10^6$& $0.0031\cdot 10^6$& $0.0033\cdot 10^6$& $0.0048\cdot 10^6$\\
\hline
    \end{tabular}
\end{table*}


Detailed BIB simulations were first performed in the context of the MAP studies, employing the MARS15 \cite{Mokhov:2017klc} Monte Carlo software. These are based on the accelerator lattice and interaction regions designed by MAP for a \qty{1.5}{\TeV} MuC. The previously-mentioned optimised MDI design was based on these simulations. A Higgs-pole muon collider with \qty{62.5}{\GeV} energy beams was also considered by MAP. 

The BIB simulation for the \qty{1.5}{\TeV} collider have been repeated in~\cite{Collamati:2021sbv}, using the Monte Carlo multi-particle transport code FLUKA~\cite{Ahdida:2022gjl,Battistoni:2015epi}. The complex FLUKA geometry was assembled by means of the LineBuilder program~\cite{Mereghetti:2012zz} using the optics file provided by the MAP collaboration. The accelerator elements have been defined in the FLUKA Elements Database following the information contained in this file and in MAP publications~\cite{Alexahin:2011zz, DiBenedetto:2018cpy}. The particles induced by the muon decays are collected at the outer surface of the MDI and before entering the detector volume, which is represented by a black box on Figure~\ref{fig:bib-mdi}. This will allow to later simulate their interaction with the detector together with particles from the $\mu^+\mu^-$ collision. The ``BIB sample'' that we describe here thus refers to the collection of particles originating from the muon decays before any interaction with the detector material.

The results obtained by FLUKA are in good agreement with the ones from MAP as shown in Table~\ref{tab:BIB_com_energy} and discussed in Ref.~\cite{Collamati:2021sbv} in more detail. The FLUKA simulation is then repeated with the same setup for the higher energy beams of the 3 and 10~TeV MuC~\cite{Calzolari:2022lgu}. This corresponds to assuming that the interaction region and the MDI are the same at all energies, which is not fully realistic but sufficient for a first assessment of the BIB levels dependence on the collider energy. Furthermore, the findings of~\cite{Collamati:2021sbv} confirm the major role that is played by the nozzles in determining the particles fluxes that arrive on the detector surface. Their optimisation for the 3 and 10~TeV MuC could thus reduce the estimated BIB levels strongly. Studies for the 10~TeV collider option showed that lattice design choices such as combined function magnets in the final focus region or a larger $L^{*}$ provide instead only a limited potential for reducing the BIB~\cite{Calzolari:2022lgu}.

Table~\ref{tab:BIB_com_energy} (see also~\cite{Collamati:2021sbv}) displays a moderate dependence of the BIB multiplicities on the collider energy. In what follows we will thus employ \qty{1.5}{\TeV} BIB simulation results, being confident that no dramatic changes are expected at higher energies. The results below are obtained for a single beam travelling counterclockwise starting \qty{200}{\m} away from the IP. The other beam will have the mirrored effect owing to the symmetric nature of the BIB due to $\mu^+$ and $\mu^-$ decays.

The most important BIB property is that it is composed of a large number of particles with low energy, thanks to the MDI mitigation action, and it is characterised by a broad arrival time in the detector. More specifically, around $4 \cdot 10^{8}$ low-momentum particles exit the MDI in a single bunch crossing depositing energy to the detector in a diffused manner. %The separation between the tips of the tungsten nozzles leads to most of the BIB particles exiting at a significant distance from the interaction point. 
There is a substantial spread in the arrival time of the BIB particles with respect to the bunch crossing, ranging from a few nanoseconds for electrons and photons to microseconds for neutrons, due to their smaller velocity.

Each of these aspects has different implications for the BIB signatures in different parts of the detector, which depend on the position, spatial granularity and timing capabilities of the corresponding sensitive elements. Thus, a careful choice of detector technologies and reconstruction techniques allows to mitigate the negative BIB effects, as demonstrated in later sections.

The time at which the BIB particles exit the machine in the interaction region is spread over several tens of ns, but the major  concentration is around the beam crossing time ($t = 0$), as shown by the left panel of Figure~\ref{fig:bibtime}. This distribution suggests that the use of time-sensitive detectors would allow to suppress a large fraction of the background. The right panel of Figure~\ref{fig:bibtime} reports the longitudinal distribution of primary $\mu^-$ decays generating the most relevant BIB components: neutrons, photons and electrons/positrons. Simulations show that to correctly account for the secondary $\mu^\pm$, it is necessary to consider primary decays up $\sim\qty{100}{\m}$ from the IP.

\begin{figure*}[t]
\centering
\includegraphics[width=0.5\textwidth]{figures/bib/BIB_time.pdf}\includegraphics[width=0.5\textwidth]{figures/bib/BIB_zmu.pdf}
\caption{\label{fig:bibtime} Time distribution of BIB particles exiting the machine (left) and longitudinal distribution of primary $\mu^-$ decay generating BIB particles exiting the machine (right). The results are based on the  FLUKA simulation, considering the primary $\mu^-$ decays within \qty{100}{\m} from the IP. }
\end{figure*}

The kinetic energy distribution of most relevant BIB particle types is reported in Figure~\ref{fig:bibenergy}. Energy cutoffs have been applied in the simulation at 100 keV for $\gamma$, $e^\pm$, $\mu^\pm$, charged hadrons and at 10$^{-14}$ GeV for neutrons. The shielding nozzles strongly suppress the high energy BIB component, making the fraction of particles entering the detector volume with kinetic energy above few GeVs negligible. Only charged hadrons and secondary muons can reach higher energies, but their rate is of the order of 10$^{4}$ and 10$^{3}$, with respect to 10$^{7}$ photons, neutrons and 10$^{5}$ electrons, positrons. The longitudinal exit coordinate distribution displayed in the figure shows that most BIB particles enter the detector with a large longitudinal displacement from the collision region. This suggest that detectors with excellent pointing capabilities would allow to strongly suppress these background contributions.

\begin{figure*}[t]
\centering
\includegraphics[width=0.5\textwidth]{figures/bib/BIB_Ekin.pdf}\includegraphics[width=0.5\textwidth]{figures/bib/BIB_z.pdf}
\caption{\label{fig:bibenergy} Lethargy plot (left) and longitudinal exit coordinate distribution (right) of BIB particles, by particle type. No time cut is applied to distributions represented in dotted lines while in solid lines only particles exiting the machine between -1 and \qty{15}{\ns} are considered. The results are based on the FLUKA simulation, considering primary $\mu^-$ within \qty{100}{\m} from the IP.}
\end{figure*}

\subsubsection*{Radiation levels}
The BIB distributions and rates are crucial to quantify the radiation levels and in turn the requirements on the detector components. The FLUKA BIB sample and the detector model previously described are employed.
%, assuming the detector model described in Section~\ref{subsec:layout}. 
The simulation~\cite{Collamati:2021sbv} used in fact a simplified detector geometry. The calorimeters, magnetic coils, and the return yoke were approximated with cylindrical elements with densities and material composition based on the averages from the full geometry. The magnetic field was assumed to be uniform. The silicon layers composing the inner tracker were instead included with exact dimensions.

Figures~\ref{fig:fluence} and~\ref{fig:tid} display respectively the expected 1~MeV neutron equivalent fluence (1-MeV-neq) and the total ionising dose (TID) in the detector region, shown as a function of the beam axis z and the radial distance $r$ from the beam axis.
The normalisation for the dose maps is computed considering that the muon collisions are expected to happen at the maximum rate of 100~kHz, corresponding to the minimum time between crossings of 10~$\mu$s. With a single bunch collider operation scheme, this in turn corresponds to a minimal collider ring length of 2.5~km.
%
%beam crossing frequency is determined by the size of the collider ring.
%a 2.5~km circumference ring and a beam injection frequency of 5~Hz. {\bf{why circumference matters?}} 
Assuming 200 days of operation during a year, the 1-MeV-neq fluence is expected to be $\sim 10^{14-15}$~cm$^{-2}$y$^{-1}$ in the region of the tracking detector and of $\sim 10^{14}$~cm$^{-2}$y$^{-1}$ in the electromagnetic calorimeter, with a steeply decreasing radial dependence beyond it. The total ionising dose is $\sim10^{-3}$~Grad/y on the tracking system and $\sim10^{-4}$~Grad/y on the electromagnetic calorimeter.

\begin{figure*}[h]
    \begin{center}
        \centering
\includegraphics[width=0.8\textwidth]{figures/1mevneq_nodump.png}
    \end{center}
    \caption{Map of the 1-MeV-neq fluence in the detector region, shown as a function of the position along the beam axis and the radius. The map is normalised to one year of operation (200 days/year) and a collision rate of 100~kHz.}
    \label{fig:fluence}
\end{figure*}

\begin{figure*}[h]
    \begin{center}
        \centering
        \includegraphics[width=.8\textwidth]{figures/tid_nodump.png}
    \end{center}
    \caption{Map of the TID in the detector region, shown as a function of the position along the beam axis and the radius. The map is normalised to one year of operation (200 days/year) and a collision rate of 100~kHz.}
    \label{fig:tid}
\end{figure*}

\subsection{Detector simulation software}
\label{sec:software}

The full simulation of a $\mu^+ \mu^-$ collision event involves several stages going from the generation of input particles, the simulation of their interactions with the detector material and of the detector response.

The first stage corresponds to the generation of all particles entering the detector. This stage is handled by standalone software, such as FLUKA or MARS15 for the BIB particles as previously described and Monte Carlo event generators for the $\mu^+ \mu^-$ scattering process. 

The input particles are then propagated through the detector material and their interactions with the passive and sensitive material of the detector are simulated with the 
\textsc{Geant4}~\cite{GEANT4:2002zbu} software.
The iLCSoft framework~\cite{ilcsoft}, previously used by CLIC~\cite{Linssen:2012hp} and now forked for developments of muon collider studies~\cite{mucolsoft}, is used for this and all further processing stages.

The detector response and event reconstruction are handled inside the modular Marlin framework~\cite{marlin}.
The detector geometry is defined using the DD4hep detector description toolkit~\cite{dd4hep}, which provides a consistent interface with both the \textsc{Geant4} and Marlin environments. The response of each sensitive detector element to the corresponding energy deposits returned by \textsc{Geant4} is simulated by dedicated digitisation modules implemented as individual Marlin processors.

The tracking detectors use Gaussian smearing functions to account for the spatial and time resolutions of the hits registered on the sensor surface. Acceptance time intervals, individually configured for each detector, are used for replicating the finite readout time windows in the electronics of a real detector and to reject hits from from out-of-time BIB particles.

The result of this simplified approach is a one-to-one correspondence between the \textsc{Geant4} hits and digitised hits, which ignores the effect of charge distribution across larger area due to the Lorentz drift and shallow crossing angles with respect to the sensor surface.
These effects are taken into account in the more realistic tracker digitisation software that is currently under development and will allow stronger BIB suppression based on cluster-shape analysis.

The ECAL and HCAL detectors are digitised using realistic segmentation of sensitive layers into cells by summing all energy deposits in a single cell over the configured integration time of \qty{\pm 250}{\pico\second}.
The time of the earliest energy deposit is consequently assigned to the whole digitised hit. The same digitisation approach is used also for the Muon Detector.

More details about the software structure and computational optimisation methods used for simulating the very large number of BIB particles are given in Ref.~\cite{Bartosik:2021bjh}. For the interested user, pointers to the documentation of the software stack, tutorials and other tools are available at the MuC software project page~\cite{mucolsoft}.

\subsection{Detector technologies}
\label{sec:technology}

The simulation workflow described in the previous sections enables a first assessment of the challenges for the various detector systems and of the required technologies, which are described in the present section.

\subsubsection*{Tracking systems}
%\label{sec:tracker}

The ability to reconstruct trajectories of charged particles in the tracking system and to measure their parameters with high precision is essential at the muon collider experiments. 
%The detectors must operate in a sea of BIB, consisting primarily of low energy neutrons and photons. Much of this background emerges from the walls of the detector and the cavern as well as the shielding cones that taper to the interaction point. 
In the expected operating conditions, high performance tracking is necessary to achieve good efficiency and resolution for reconstructing charged leptons, jets, energy sums, displaced vertices originating from the heavy flavour hadron decays, as well as potential new phenomena. 

The BIB represents a significant hurdle for tracking, both in terms of the data volumes generated by the tracker as well as by introducing a combinatorial challenge in the track reconstruction. In each bunch crossing, BIB particles on average generate 500\,000 hits in the most inner layer of the tracker, located just few centimetres away from the interaction point. This corresponds to hit density of up to 1000 hits/cm$^2$. However, the density does decrease rapidly as a function of the radial distance from the beam-line, as shown in Figure~\ref{fig:HitMultTracker}. 

\begin{figure}[!ht]
\center
\includegraphics[width=0.5\textwidth, trim= {35 55 0 0 } , clip]{figures/HitDensity.png}
\caption{Average hit density per bunch crossing in the tracker as a function of the detector layer.}
\label{fig:HitMultTracker}
\end{figure}

It is clear from these numbers that high granularity of silicon pixels is necessary in order to achieve hit occupancy level of a few \%. In addition, various handles to reduce the BIB should be explored for both on- and off-detector filtering.
Possible filtering schemes include:
\begin{itemize}
\item \textbf{Timing}: Removing hits incompatible with the main bunch crossing time could reduce the data load by about a factor of 3. Timing information will eventually be used in the event reconstruction, but an initial on-detector filtering could be implemented as well.
\item \textbf{Clustering}: Pixel clustering to reduce the number of single pixels to be read out. This requires more on-detector processing and results in more bits per cluster and a higher power budget, but can reduce the number of hits read out. Selection requirements can also be applied to the cluster shape. The effectiveness needs to be assessed for each BIB cluster type.
\item \textbf{Energy Deposition}: Each of the backgrounds has a characteristic energy deposition signature. For example neutrons have low, localised energy deposits. On-detector filters could efficiently exploit this quantity.
\item  \textbf{Correlation Between Layers}: This is a powerful handle for background rejection. However, an implementation may be complex and costly, doubling the number of channels. For on-detector filtering, it also requires transfer of data between layers in a very busy environment.
\item \textbf{Local Track Angle}: Track angle measurement can be made in a single detector if the thickness/pitch ratio distributes the signal over several pixels. This avoids the complexity of inter-detector connections and could provide a monolithic solution~\cite{Lipton:2022njd,Lipton:2019drv}.
\item \textbf{Pulse Shape}: Signals from BIB can come with a variety of angles and may not give the deposit profile and pulse shape of a typical minimum ionising particle (MIP).  Appropriate pulse processing, such as multiple sampling, RC-CR filters, zero crossing, or delay line clipping can be used to reduce the data load.
\end{itemize}

The basic trade-offs are between the complexity, power, and mass needed to implement a on-detector filter, and the benefit of reduced data rate. Particular caution should be taken when it comes to on-detector filtering: overly aggressive front-end filtering schemes can introduce irrecoverable inefficiencies and biases in track reconstruction and can limit acceptance for some beyond the Standard Model signatures, such as those of long-lived particles. 

A study was conducted in simulation to determine the granularity and timing requirements for the tracker sensors in order to reduce the hit occupancy to under the 1\% target level. In this study, the pixel size and per hit timing resolution were independently varied in each layer of the detector. The detector hits were integrated in the time period of 1~ns following the bunch crossing. It was found that for the vertex detector granularity of $25 \times 25$~$\mu$m$^{2}$ and time resolution of 30~ps are needed to achieve the desired occupancy goal. The inner tracker was relying on asymmetric macropixels with $50 \times 100$~$\mu$m$^{2}$ size and 60~ps timing resolution were sufficient to satisfy the requirement, while the outer tracker assumed either macropixels or microstrips with the size of 50~$\mu$m$ \times 10$~mm and a 60~ps time resolution. It is thus evident that R\&D efforts towards 4D tracking are necessary to achieve the required spacial granularity and timing resolution. 

Silicon-based sensors have come to dominate the technology for collider detector tracking systems. This is likely to continue into the muon collider era. In the past decade there have been a number of technological developments that promise to achieve many of the capabilities discussed in the previous section. They address different aspects of the needs for space and time resolution, pattern recognition, electronics integration, radiation hardness, and low cost.
A description of promising technologies for achieving such goals is provided below. 

\subsubsection*{Monolithic Devices (CMOS MAPS)}
CMOS Monolithic Active Pixel Sensors (MAPS) are based on standard CMOS process flows with thick (20--50~$\mu$m) epitaxial layers. Charge is collected from electron-hole pairs generated in the epitaxy. There is a small area n-type collection electrode with the CMOS circuitry embedded in a deep $p$-well to avoid parasitic charge collection by the CMOS transistors. The geometry of the electrodes and associated circuitry means that the epitaxy is difficult to deplete evenly. The first such devices used diffusion rather than drift to collect charge.  Recent prototypes, shown in Figure~\ref{CMOS_LGAD}a, have added a deep, lightly doped, n-layer below the p-well to provide a more uniform drift field in the epitaxy~\cite{Cardella:2019ksc}. 
The proximity of the CMOS transistors to the epitaxy means these devices have significant sensitivity to analog-digital cross-talk. It is likely that the CMOS analog section would have to be 3D stacked with digital TDC, ADC and I/O tiers to achieve adequate isolation and overall functionality.  Radiation hardness needs to be studied and improved.  In particular the effects of doping evolution in the epitaxy will affect fields and operation as the device ages. This is a particular problem with the large ratio of collection node to collection area which forces a difficult geometry for the collection field. The size of CMOS sensors is limited by the typically 2x3 cm CMOS reticule. To achieve a large area device the reticules must be ``stitched'' using an additional metal layer or tiled while minimising dead area at the edges.

\begin{figure*}[h]
\centering
\includegraphics[width=0.95\textwidth]{figures/CMOS_and_LGAD.jpg}
\caption{a) Cross section of the MALTA~\cite{Cardella:2019ksc} CMOS sensor showing the implant structure.  The low dose n-implant provides an electrode that improves the uniformity of the p-epitaxial drift field.  b) Cross section of an AC coupled LGAD~\cite{Giacomini:2019kqz}.  The Junction Termination Edge (JTE in the figure), which would normally separate each pixel, is only needed at the edge of the device, providing near 100\% fill factor.}
\label{CMOS_LGAD}
\end{figure*}

\subsubsection*{Devices with Intrinsic Gain}
In sensors based on the Low Gain Avalanche Diode (LGAD) design, the initial charge created by an impinging particle is amplified in a ``gain layer'' by a factor of about 10--30. The resulting current signal is large and fast, enabling a 20--30~ps time resolution. An interesting feature of LGAD-based sensors is that the associated front-end requires less power since the sensor provides the first amplification stage. 
The LGAD  design is relatively new and is undergoing rapid development.
The design of the current generation of LGADs being used for endcap timing layers in the CMS and ATLAS HL-LHC upgrades has now been superseded. This first generation of devices suffers from limited fill-factor due to edge field limiting structures and moderate radiation hardness. Recent works have combined internal gain with the novel resistive read-out design, reaching a design (the so-called RSD, Resistive Silicon Detectors) with a 100\% fill factor and excellent spatial precision even with pixels with a large pitch (a precision of less than 5\% of the pitch)~\cite{Tornago:2020otn}. Parallel to this development, new studies suggest possible improvements in the radiation resistance of the LGAD design. The two most promising are: (i) the insertion of an additional layer of carbon to reduce acceptor removal in the gain implant and (ii) the design of the gain implant using both acceptor and donor dopings so that acceptor removal is compensated by donor removal, extending the radiation hardness of the gain implant. 
The resistive charge-sharing design also allows for sparser read-out geometries, limiting the density of analog channels. 

For the muon collider, being a high-rate environment, the novel DC-coupled design of the resistive read-out technique might be very beneficial, as it allows for a faster recovery time. 
Detailed studies of AC-RSD and DC-RSD will be needed, optimising the electrode density and geometry in conjunction with charge deposition characteristics of the photon, electron/positron, and neutron backgrounds. Dopant removal is the primary limitation to radiation hardness, and continued study is needed to understand the practical restrictions imposed by this effect. The total current going into detector bias will be larger in devices with gain and may represent a significant fraction of the total power.

\subsubsection*{Hybrid Small Pixel devices}
These are standard ``hybrid'' pixel diode sensors without gain. However fast timing and good position resolution can be achieved with fine pixel pitch and low input capacitance. Bump bonding using solder, indium, or copper, is limited to $\approx$15-50 micron interconnect pitch. Three-dimensional hybrid bonding~\footnote{The electronics industry has adopted ``hybrid bonding'' as the generic term for oxide bonded stacks of wafers or chips with embedded metal to achieve top to bottom electrical connection. This technology is commonly used in cell phone image sensors.}, illustrated in Figure~\ref{fig:Bump_Hybrid}, can achieve both $<5~\mu$m pitch and low enough interconnect capacitance to meet noise and power limitations. The sensors have the advantage of being intrinsically radiation hard with signal/noise large enough to provide 20--30~ps time resolution. It will likely be necessary a design where ADCs and TDC service multiple small pixels to reduce the density of ADCs and TDCs, thus reducing both the in-pixel density and power. 

\begin{figure*}[h]
\centering
\includegraphics[width=0.95\textwidth]{figures/Bump_and_hybrid.pdf}
\caption{a) A typical bump bonded sensor/readout chip geometry. The spacing is determined by the size of the bump and under-bump metalization pad.  b) A hybrid bonded sensor/readout stack with the pitch limited by the micron-level hybrid metalization imbedded in the top oxide layer.  c) An example of a three-tier hybrid bonded stack with separate analog and digital readout layers\cite{Lipton:2015vca}. The readout pitch is 24 microns and the readout stack thickness is 35$\mu$.}
\label{fig:Bump_Hybrid}
\end{figure*}

\subsubsection*{Intelligent Sensors}
The different characteristic signals generated by electromagnetic, neutron, and charged hadron backgrounds and signal MIPs prompts consideration of more ``intelligent'' sensors that can separate the BIB from the signal. An example is the current 2-layer track trigger design for CMS at the LHC where low $p_{\textrm{T}}$ tracks are filtered out by comparing hits on separated sensor layers. Such multi-layer designs are limited by the complex interconnection and data transmission paths needed to communicate between sensor layers. However, for a device where the thickness/pixel pitch ratio is large enough, the pixel pulse shapes and cluster patterns will be very different for MIPs and BIB hits. This information can be used for a prompt local filter to reject BIB. Radiation-induced traps will cause the pulse shapes and induced current patterns to change during the lifetime of the detector, possibly necessitating changes in algorithms.

Appropriate information density can be achieved in small pixel devices or double-sided LGADs~\cite{Lipton:2022njd}. In the double sided LGAD fast timing signals are read on a top, larger pitch layer coupled to the gain layer. Charge deposition patterns and timing are reconstructed on a bottom, pixelated layer. Other concepts can be explored where the very different pulse shapes and patterns can be used to separate signal from BIB, perhaps incorporating on-chip machine-learning techniques.

\subsubsection*{Power Considerations}
This section describes a tentative estimate of the power constraints on the tracker based on extrapolations of the existing technologies. The study focuses on the vertex detector and assumes a design with $25\times25$ $\mu$m$^2$ pixels with four barrel layers and four endcap disks on each side, as previously described. Conventional scaled CMOS electronics~\cite{OConnor:1999isd} and possible extrapolations of optical-based data transmission are also assumed. New technologies might change the picture completely. 

For conventional CMOS-based amplifiers and a conventional silicon detector operating with no internal gain the front-end power will be determined by the capacitive load on the front-end and the desired signal/noise and rise time. For example, a simple SPICE model of a preamplifier loaded with 100~fF capacitance provides 4~ps time jitter for 1 $\mu$A bias current and  45~ps for a bias current of 100~nA. A time jitter of $< 30$~ps can be achieved in a conventional sensor with 50~$\mu$m thickness and front-end current of less than 250~pA if the detector capacitance is carefully controlled using 3D interconnections. If the sensor is based on LGAD-like internal gain the signal presented to the preamplifier can be $10-20$ times larger. For the same signal over noise, this could reduce the front-end transductance and associated drain current by roughly the square root of the gain. A conventional CMOS amplifier is expected to draw about 450~W of power into the vertex detector for the analog bias.

In addition to front-end power there is the power necessary to bias the detector. This can become significant for heavily irradiated detectors. If a HL-LHC-like operating scenario is considered, the final depletion voltage can be as high as  500~V (depending on the technology chosen). Under these conditions the vertex sensor bias power is about 100~W.

It is also useful to estimate the detector data load. Using the simulated layer occupancies in the vertex detector, a total rate of hits of $15 \times 10^{13}$ bits per second (b/s) for the vertex detector is obtained. More detail about the estimate is provided at the end of the section.
%in Section~\ref{sec:tdaq}. 
The power needed per bit for the Low Power Gigabit Transceiver (lpGBT)~\cite{Guettouche:2022wzn} is about 41~pJoule/bit. Power efficient optical transmission is the subject of intense study by the semiconductor industry and 10~pJoule/bit is assumed as a conservative estimate for the future power consumption. This gives us a data transmission power of about 1.5~kW. Attojoule/bit levels, albeit before radiation damage considerations, appear feasible in the near future~\cite{7805240}, further reducing the penalty for reading the full event. Each link must be capable of transmitting at a rate of about $20$~Gb/s to limit the number of physical optical connections to one per module. Higher speeds (if available) will lead to a reduction of the overall number of optical links in the system. 

\FloatBarrier
\subsubsection*{Calorimeter systems}
%\label{sec:calorimeter}

The measurement of physics processes at the energy frontier requires excellent energy and spatial resolutions to resolve the structure of collimated high-energy jets. Jet reconstruction, and the improvement of the reconstructed jet energy resolution, is the driving theme of ongoing R\&D activities in high energy physics, and a muon collider will not diverge from this general theme. Future lepton colliders aim at separating W and Z bosons in the dijet channel, which requires a 3-4\% jet energy resolution for jets above 100~GeV.

In a multi-TeV muon collider, the calorimeter system has to operate in the intense flux of low energy particles arising from the BIB. The BIB in the calorimeter region is mainly formed by photons (96\%) and neutrons (4\%). In the current detector layout, a flux of about 300 particles per cm$^2$ is present at the ECAL barrel surface, with an average photon energy of about 1.7~MeV. Given the high flux, several particles may overlap in a single cell, resulting in a hit where their energy is summed up. The occupancy, defined as the number of hits per mm$^2$ in a calorimeter layer, is shown as a function of the calorimeter depth in Figure~\ref{fig:calo_occupancy_barrel} and of the z coordinate in Figure~\ref{fig:calo_occupancy_endcap}. The simulation shows how the ECAL system absorbs most of the BIB radiation, resulting in a significantly lower occupancy for the HCAL system.
The ECAL is expected to receive approximately 100~krad/y of total ionising dose and a $10^{13-14}$~cm$^{-2}$~1-MeV-neq fluence. The spatial distributions of the energy deposited in ECAL and HCAL in a single bunch-crossing are shown respectively in Figure~\ref{fig:ecal_deposit} and Figure~\ref{fig:hcal_deposit}.

\begin{figure}[!ht]
\center
\includegraphics[width=0.425\textwidth, trim = {65 55 0 10} ,clip]{figures/occupancy_calo_barrel.png}
\caption{BIB hit occupancy in the calorimenter barrel region in a single bunch-crossing.}
\label{fig:calo_occupancy_barrel}
\end{figure}
   
\begin{figure}[!ht]
\center
\includegraphics[width=0.4226\textwidth,trim = {65 45 0 0} ,clip]{figures/occupancy_calo_endcap.png}
\caption{BIB hit occupancy in the calorimeter endcap region in a single bunch-crossing.}
\label{fig:calo_occupancy_endcap}
\end{figure}
   
Similarly to what can be done in the tracker, the time of arrival of particles in a calorimeter cell can be exploited to discriminate the BIB contributions from the primary interactions. At the same time, the BIB particles are expected to deposit most of their energy in the innermost layers of the calorimeter,  with particles from the primary interaction propagating deeper in the detector.

The technology and the design of the calorimeters should be chosen to reduce the effect of the BIB, while keeping good physics performance. Several requirements can be inferred:
\begin{itemize}
    \item \textbf{High granularity} to reduce the overlap of BIB particles in the same calorimeter cell. The overlap can produce hits with an energy similar to the signal, making harder to distinguish it from the BIB;
    \item \textbf{Good timing} to reduce the out-of-time component of the BIB. An acquisition time window of about $\Delta t = 300$~ps could be applied to remove most of the BIB, while preserving most of the signal. This means that a time resolution in the order of $\sigma_t = 100$~ps (from $\Delta t \approx 3 \sigma_t$) should be achieved;
    \item \textbf{Longitudinal segmentation} to discriminate between the different energy profiles of signal processes and the BIB. A fine segmentation of the calorimeter can help in distinguishing the signal showers from the fake showers produces by the BIB;
    \item \textbf{Good energy resolution} of ${10\%}/{\sqrt{E}}$ in the ECAL system is expected to be enough to obtain good physics performance, as has been already demonstrated for conceptual particle flow calorimeters.
\end{itemize}

\begin{figure}[!t]
\center
\includegraphics[width=0.48\textwidth,trim = {14 9 0 0} ,clip]{figures/edeposit_ECAL.pdf}   \caption{Energy deposited by the BIB in a single bunch-crossing in the ECAL.}
\label{fig:ecal_deposit}
\end{figure}
   
\begin{figure}[!t]
\center
\includegraphics[width=0.48\textwidth,trim = {12 9 0 0} ,clip]{figures/edeposit_HCAL.pdf}
\caption{Energy deposited by the BIB in a single bunch-crossing in the HCAL.}
\label{fig:hcal_deposit}
\end{figure}
   
The requirements imposed by the need to house the calorimeter systems within a large magnetic coil tend to disfavour designs fully based on homogeneous calorimetry. Sampling calorimeters based on alternating dense passive materials, such as copper, steel, or tungsten, and active readout materials, such as plastic scintillators, silicon, or gaseous detectors are likely to be employed, at least in the HCAL.
Two major approaches are being pursued to exploit sampling calorimeters and improve upon the current generation of collider experiments: multi-readout (dual or triple)~\cite{Lee:2017shn,INFNRD-FA:2020fiu,Ferrari:2018noa} and particle flow~\cite{Thomson:2009rp} calorimetry.
The first approach focuses on reducing the fluctuations in the hadronic shower reconstruction, which are the main responsible for the deterioration in the determination of the jet energy. This goal is achieved by measuring independently the electromagnetic and the non-electromagnetic components of a hadronic shower, thus allowing to correct event-by-event for the different response of the calorimeter to various particle species. 
The second approach focuses on the reconstruction of the four-momenta of every particle recorded by the detector. This method exploits tracking information and requires a detector with extreme granularity, combined with powerful reconstruction algorithms aimed at resolving each particle's trajectory through the whole detector.

\subsubsection*{Dual-readout calorimetry}
The energy resolution of a calorimeter system is affected by fluctuations in the energy deposited in its active elements. When measuring hadronic showers, the fluctuations in the electromagnetic component of a shower represent the dominant contribution to the total resolution.
To minimise these fluctuations, Dual-Readout calorimetry aims at measuring both the scintillation light component (sensitive to both hadrons and e.m. particles) and the Cherenkov light component (sensitive only to relativistic e.m. particles). Notable examples of implementations of this concept can be found in the work of the DREAM/RD52 collaboration~\cite{Wigmans:2007es} and the proposed IDEA~\cite{FCC:2018evy,CEPCStudyGroup:2018rmc} detector for FCCee/CepC.
In these calorimeters, signals are generated in scintillating fibers, which measure the deposited energy, and in clear plastic PMMA or quartz fibers, which are sensitive to the Cherenkov light. A large number of such fibers are embedded in a fully projective lead or copper absorber structure. This detector is not longitudinally segmented: e.m. and hadronic showers can be distinguished using short and long fibers in the calorimeter, and longitudinal information could be extracted by reading out the fibers on both ends. Past results~\cite{Lee:2017xss,Lee:2017shn} have demonstrated to reach an energy resolution for charged pions of 34\%/$\sqrt{\textrm{GeV}}$ and the final goal of 30\%/$\sqrt{\textrm{GeV}}$ seems well within reach. The main challenges appear in the handling of the high number of fibers and SiPMs which are of the order of few 10$^8$ and constitute an important fraction of the technology cost. 
The implementation of a third fiber material, or alternatively the time readout of the scintillation fibers, can be used to measure the MeV-scale neutrons produced in a hadronic shower and suppress the fluctuations arising from binding energy loss~\cite{Lee:2017oye}. This latter method is referred to as triple readout.
The absence of longitudinal segmentation is likely the limiting factor for deploying such design in a muon collider detector. However, hybrid designs composed of an ECAL made of crystals (such as LYSO or PbWO$_{4}$) and the hadronic section based on the DREAM fiber prototype have been also proposed~\cite{Lucchini:2020bac,Gaudio:2011zzb}. These designs need to demonstrate the feasibility of dual readout concepts in the crystal matrix.

\subsubsection*{Particle flow calorimetry}
In recent years the concept of high granularity particle flow calorimetry~\cite{Brient:2001fow} has been developed in the context of the proposed International Linear Collider (ILC). The CALICE collaboration is the major developer of calorimeter concepts and technologies for highly granular detectors for particle flow. The goal of particle flow calorimeters is to build an image of the showers induced by the various jet-fragments to allow the correct matching of these showers with the charged particles measured in the tracker. This in turn enables to correctly identify and measure the energy of the showers induced by neutral hadrons.

\subsubsection*{Silicon-based sampling}
With a radiation length ($X_0$) of 0.35~cm and an interaction length ($\lambda_I$) of 9.95~cm, tungsten is an ideal absorber for an electromagnetic sampling calorimeter. Silicon sensors can be used as active elements to achieve a high channel granularity and longitudinal segmentation. Moreover, state-of-the-art silicon sensors can sustain the high radiation dose of the expected BIB. Analogous technologies are adopted by LHC experiments upgrades~\cite{CMS:2017jpq}, and considered by the CLIC collaboration. The CLIC ECAL barrel, on which the current muon collider detector design is based, is composed of 64M sensors sampling 40 layers. Future developments should implement a precise timing measurement in these sensors ($<$100 ps) in order to make them usable at a muon collider. Although the high granularity is a clear advantage, the associated number of electronic read-out channels is a non-trivial technological problem. Moreover the cost for such a system exceeds the cost of other solutions. 

\subsubsection*{Scintillator-based sampling}
Plastic scintillators can be used for a high-granularity detector. Small tiles or strips of scintillating material can be produced with a typical size of 1-5~cm$^{2}$ for e.m. applications and about 10~cm$^{2}$ for hadronic applications. The typical thickness of the active layer is 0.3-0.5~cm, which makes it possible to design detectors with a longitudinal segmentation of several tens of layers. Each calorimeter cell can be read out via a silicon-based photo-detector mounted directly on the scintillating element~\cite{Sefkow:2018rhp}. Passive absorbers, such as steel, can be intertwined with the sensitive layers.
The high granularity that can be achieved allows to implement compensation in hadronic showers in the reconstruction software using energy density techniques.

\subsubsection*{Micro-pattern gaseous detector-based sampling}
Calorimeters with gaseous detectors as active element can reach a higher granularity with respect to more traditional scintillator-based calorimeters. Furthermore, a 1x1~cm$^2$ pad is economically affordable with respect to 3x3~cm$^2$ scintillator tiles. The CALICE collaboration has been studying the performance of digital~\cite{Adams:2016por} and semi-digital~\cite{Baulieu:2015pfa} hadronic calorimeters. Besides having a high granularity, gaseous detectors have the advantage to be radiation hard, leading to simple calibration procedures. Resistive-Plate Chambers (RPCs) have been chosen historically because of their low cost to instrument large area, because of their intrinsic digital nature and because of the rather low rate expected at future electron-positron colliders, which were the main target of these designs. Recently, Micro-Pattern Gaseous Detectors (MPGDs) have been proposed since they will likely outperform RPCs for this task because of their higher rate capability, their operation with environmental-friendly gases, their good energy resolution (of about 20\%), high detector stability and low pad multiplicity. In the last decade resistive Micromegas were developed and tested for calorimetry at ILC~\cite{Chefdeville:2021ava} and for tracking in high-rate environments~\cite{Alviggi:2020hoy} at LHC, while new resistive detectors like the $\mu$RWELL~\cite{Bencivenni:2014exa} and RP-WELL~\cite{Rubin:2013jna} were developed, the latter being actively pursued for SDHCAL calorimeter of CepC detector~\cite{ShakedRenous:2020xeu}. 
It is expected that a good time resolution will play an important role in helping to match reconstructed charged particle trajectories and calorimeter energy deposits. Single-layer RPCs can reach time-resolutions of few hundred ps, but rely on gases with high global warming potential that will be phased out in the near future. Replacement gases are currently under investigation. Micro-Pattern Gaseous Detectors typically have a time resolution of the order of few ns, with R\&D ongoing to bring this to sub-ns levels.

\subsubsection*{Other promising calorimeter technologies}

Calorimeters are generally divided in two categories, homogeneous and sampling. The best compromise between the two technologies is sought in order to optimise the experimental requirements and minimise the drawbacks associated with the limitations of standard solutions. The most recent technological developments allow this rigid distinction to be abandoned in favour of novel architectures: the Crilin calorimeter~\cite{Ceravolo:2022rag} is a semi-homogeneous calorimeter based on Lead Fluoride (PbF$_2$) crystals read out by surface mounted UV extended Silicon Photomultipliers (SiPMs).

\subsubsection*{Crilin: a CRystal calorImeter with Longitudinal INformation}

The Crilin calorimeter can be segmented longitudinally as a function of the energy of the particles and of the background level, thanks to its modular design which enables a high degree of reconfigurability. The Crilin R\&D proposal embeds a modular architecture based on stackable submodules composed of matrices of  crystals, in which each crystal is individually read out by two series of two UV-extended surface mount SiPMs.  Crystal dimensions are $10\times10\times40$~mm$^3$ and the surface area of each SiPM is 4$\times$4~mm$^2$, so as to closely match the crystal surface.

In the current design, the prototype consists of two submodules, each composed of a 3-by-3 crystals matrix. The submodules are arranged in a series and assembled together by screws, resulting in a compact and small calorimeter, shown in Figure~\ref{fig:x1}.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/CADprototype.pdf}
    \caption{CAD model of  Crilin Prototype.}
    \label{fig:x1}
\end{figure}

Each crystal matrix is housed in a light-tight case which also embeds the front-end electronic boards and the cooling system. The on-detector electronics and the SiPMs must be cooled during operation, so as to improve and stabilise the performance of SiPMs against irradiation. The Crilin design is capable of removing the heat load due to the increased photosensor currents after exposure to the expected $2-5 \cdot 10^{13}$~1-MeV-neq~cm$^{-2}$/year fluence, along with the power dissipated by the amplification circuitry. The total heat load was estimated as 350~mW per channel. The Crilin cooling system, which is based on conduction and forced convection of nitrogen, will provide the optimum operating temperature for the electronics and SiPMs at around $0^{\circ}$ C. Gas fluxing will also prevent any condensation on SiPM or crystal surfaces.

\FloatBarrier
\subsubsection*{Muon systems}
%\label{sec:muon}
A compact detector can be obtained using an iron yoke to concentrate the magnetic flux return from the solenoid, instrumented with several layers of muon detectors.

The BIB hits in the muon system are concentrated around the beam axis in the endcaps, as shown in Figure~\ref{fig:BIB2fig}.
The BIB in the muon system is mainly composed of high energy neutrons and photons. The neutron energy, shown in Figure~\ref{fig:BIB_15TeV_flux_neutron}, ranges from \SI{10}{MeV} up to \SI{2.5}{GeV}, with the majority of the flux in the region $E < \SI{100}{MeV}$. 

\begin{figure}[!t]
    \center
    \includegraphics[width=0.48\textwidth, trim = {10 6 0 32} , clip]{figures/cell.pdf}
    \caption{BIB muon hit spatial distribution in the first layer of the muon endcap. The detector hits not associated to a cluster are shown by the red markers. The blue circle corresponds to region $\theta<8^{\circ}$, while the purple to $\theta<10^{\circ}$.}
    \label{fig:BIB2fig}
\end{figure}


\begin{figure}[!t]
    \center
    \includegraphics[trim={0cm 0cm 0cm 2cm},clip,width=0.5\textwidth]{figures/BIBEnergyDistribution_NeutronsvsTheta.png}
    \caption{Energy distribution of neutrons from BIB. Colours represent different geometrical regions of the muon system.}
    \label{fig:BIB_15TeV_flux_neutron}
\end{figure}

The photon energy, shown in Figure~\ref{fig:BIB_15TeV_flux_photon}, instead ranges between \SI{100}{keV} and \SI{200}{MeV}, with the majority of the flux in the region $E < $\SI{10}{MeV}.

\begin{figure}[!h]
    \center
    \includegraphics[trim={0cm 0cm 0cm 2cm},clip,width=0.5\textwidth]{figures/BIBEnergyDistribution_PhotonsvsTheta.png}
    \caption{Energy distribution of photons from BIB. Colours represent different geometrical regions of the muon system.}
    \label{fig:BIB_15TeV_flux_photon}
\end{figure}

The colours represent different geometrical regions of the detector, based on the polar coordinate $\theta$: the fluxes are higher in the inner part of the endcap, at lower $\theta$ and closer to the beam line, and then lower in the outer regions. 

While at high energies it is expected for the muon momentum resolution to be dominated by the measurements performed by the inner tracking detectors, with the muon detectors providing the additional muon identification, sensitivity studies pointed out that some technologies, such as RPCs, are already at the limit of their current rate capability in the most forward regions. These results, together with preliminary requirements on the spatial ($\approx$\SI{100}{\micro \meter}) and time resolution (below \SI{1}{ns}), suggest the need for gaseous detectors R\&D. Classical well-known MPGDs, such as GEMs or Micromegas, are characterised by an excellent spatial resolution, but do not match the demanding request on the timing resolution. R\&D on new generation MPGDs are still at the initial phase, but are obtaining promising results for a future implementation. A possible muon system design can be a heterogeneous detector, composed of layers of different technologies to optimise the timing and tracking performance. Moreover, an excellent spatial resolution would give the possibility to use the standalone muon objects to seed the global muon track reconstruction.

\subsubsection*{Detectors with high spatial resolution}
\label{detec_space}
Classical gaseous detectors, Multi-Wire Proportional Chambers (MWPCs), reach spatial resolutions of the order of few mm (dominated by the mechanical limitation in the wire spacing). Resolutions of \SI{50}{\micro m} to \SI{100}{\micro m} are obtained measuring precisely the drift time (Drift Tubes, DT) or by patterning the cathode combined with precise charge measurement (Cathode Strip Chambers, CSCs), however all wire-based detectors have intrinsic rate limitations due to the slow evacuation of ions. This limitation has been overcome in Micro-pattern Gaseous Detectors, where the electrodes are created using photo-lithographic techniques, which allows the reduction of the electrode spacing of at least one order of magnitude, resulting in fast ion evacuation combined with high spatial resolution.

A Gas Electron Multiplier (GEM) consists of a thin polymer foil (often \SI{50}{\micro m} polyimide), cladded on both sides with a thin layer of copper (\SI{5}{\micro m}), chemically perforated with a high density of holes, typically of 100/mm$^2$~\cite{Sauli:1997qp}. Applying a potential difference between the top and bottom electrodes, a high electric field is formed in the holes where electron multiplication can take place. Several GEM-foils can be stacked on top of each other leading to detectors with high gain ($>10^5$) and low discharge probability ($<10^{-10}$). GEM detectors are used in different collider experiments~\cite{Ketzer:2001dt}, mainly for tracking and triggering purposes. Spatial resolution down to \SI{50}{\micro \meter} is possible, with a time resolution that depends on the gas mixture used: 7--10~ns in Ar:CO2~\cite{CMSMuon:2019pzw} down to 3.5~ns when CF$_4$ is used in the gas mixture~\cite{Alfonsi:2004gh}. Rate capabilities up to 100 kHz/cm$^2$ have been assessed.

Micromegas are parallel-plate chambers where the amplification takes  place in a thin gap, separated from the conversion region by a fine metallic mesh~\cite{Giomataris:1995fq}. Micromegas are used in collider experiments~\cite{Thers:2001qs} and a spark-protected evolution with resistive strips will be used mainly for tracking in the upgraded forward muon system of the ATLAS experiment~\cite{Kawamoto:2013udg}. A spatial resolution of \SI{80}{\micro \meter}, a time resolution of 7-\SI{10}{ns}, and a rate capability up to \SI{100}{kHz/cm^2} have been achieved.

The Micro-resistive Well ($\mu$-RWELL) is a single amplification stage resistive MPGD~\cite{Bencivenni:2014exa}. Such technology is reliable, since the presence of the resistive layer assures a very low discharge rate quenching the spark amplitude. It can achieve a rate capability up to 10~MHz/cm$^2$ with a detection efficiency of the order of 97-98\%~\cite{Bencivenni:2019wxr}. Typical spatial resolution is $<$\SI{60}{\micro \meter}, time resolution measured with CF$_4$ gas mixture was measured to be below \SI{6}{ns}, time resolution in Ar:CO$_2$ mixtures is expected to be similar to the triple-GEM detectors (7-10\,ns).

\subsubsection*{Detectors with sub-ns timing resolution}
\label{detec_timing}

MRPC~\cite{CerronZeballos:1995iy} have acquired solidity and importance in the High Energy Physics domain where both high efficiency and  good time resolution are demanding. A time resolution of about \SI{60}{ps} and $95\%$ efficiency has been obtained~\cite{ALICE:2008ngc,Llope:2005yw} for a detector composed of 10 gas gaps $250~\mu$m size arranged in a double stack design using floating soda-lime glass (bulk resistivity $ 5 \times 10^{12}\, \Omega\, {\rm{cm}}$).  A recirculating gas mixture (C$_2$H$_2$F$_4$:SF$_6$ 97:3) allows operation in avalanche mode with electric field of approximately $100$~kV/cm. The technology was operated~\cite{Friese:2006dj} in high particle fluxes (though for small-size detector units) in a series of tests for the endcap upgrade of the STAR ToF and for the mini-CBM  experiment at the GSI/SIS8 Synchrotron.  Data with thinner standard float glass show rate capability of some~kHz/cm$^2$ at time resolutions of 70--80~ps.

The rate capability is limited by the current flowing through the resistive plates and hence a step forward to increase this value, at a constant front end electronics threshold, would be to use low resistivity glass or to decrease their thickness. Lower resistivity glass ($ 10^{10}\,\Omega\,$cm) have been used in tests beam~\cite{Wang:2019jjz} and showed rate capability of 35~kHz/cm$^2$ with efficiency above $90\%$ and time resolutions below \SI{80}{ps}. A still better behaviour can be seen by lowering the glass resistivity by one order of magnitude, currently R\&D is concentrated on establishing \SI{20}{ps} time resolution at rates of 100~kHz/cm$^2$.

Although the MRPC have shown excellent timing resolution over large area and R\&D for larger rate capability and even better timing is underway, this detector technology can not be considered for future collider experiments with the current gas mixture which has a high Global Warming Potential (GWP). The use of freons will be gradually phased out by 2030. While for standard (High Pressure Laminate, HPL) RPCs encouraging results have been obtained to replace the freons with alternative gases as Hydrofluoroolefine (HFO-1234ze), MRPC performances have yet to be proven with those new gas mixtures. Moreover MRPC performance relies on a non-negligible fraction of SF$_6$ which has even higher GWP with respect to the freons. R\&D should start urgently in order to propose these detectors for future use.

The time resolution of a classical MPGD is dominated by the fluctuations on the position on the first ionisation cluster in the drift gap.
The contribution to the time resolution of the drift velocity ($v_d$) is then given by $\sigma_t = (\lambda v_d)^{-1}$ where $\lambda$ is the average number of primary clusters generated by an ionising particle inside the gas per unit length~\cite{DeOliveira:2015bda}. Therefore a better time resolution is expected with a faster mixture. However, even with a fast gas mixture, as for example Ar:CO$_2$:CF$_4$, classical MPGDs usually cannot reach time resolution better than few ns.

A possible approach aimed at improving the time resolution is the one followed by the PicoSec Collaboration~\cite{Bortfeldt:2019ecw}: the proposed detector is a standard Micromegas (MM) with a drift gap reduced to \SI{200}{\micro \meter} in order to minimise the possibility of primary ionisation. Particles pass instead through a Cherenkov radiator placed on top of the MM, where they produce Cherenkov photons, which are then converted by a photocathode and enter in the drift region, removing the fluctuations on the position of the first ionisation cluster.
Preliminary results of the first prototypes of few~cm$^2$ proved the reliability of the principle with a time resolution of \SI{25}{ps} measured with a Ne/C$_2$H$_6$/CF$_4$ gas mixture. Further studies of this new technology will be focused on: proper stability of the detectors, choice of the materials and geometry, radiation hardness and gas mixture (avoid use of CF$_4$ and flammable gases).

An alternative approach is represented by the Fast Timing Micropattern (FTM) gas detector~\cite{DeOliveira:2015bda}. Here the drift gap is segmented in $N$ thinner fully resistive drift and amplification stages, which are in competition between each other when the detector is fully efficient (sum of drift gaps $\geq$ \SI{1.5}{mm}). The fastest stage is the one that determines the timing of the signal, thus reducing the time resolution of the detector of a factor $N$ with respect to a standard MPGD. The first prototype, made of just two stages, obtained a time resolution of \SI{2}{ns} with pion beam~\cite{Abbaneo:2017tat}. The current R\&D is focused on improving the quality of the resistive layers used for the amplification stage and establishing the detection principle with multiple layers on a small scale prototype~\cite{Colaleo:2019tmy}, and assessing the technology on larger scale prototypes. The main limitations come from the quality of the detector elements with resistive layers, which is a technology currently developed, and the single-stage reachable gain; interesting results have been recently obtained with a Ne/iC$_4$H$_{10}$\,95/5 gas mixture~\cite{Pellecchia:2021sip}.

\subsubsection*{Technology comparison}
In order to understand the response of the muon detectors to the BIB particles, the detector sensitivities have been studied with a standalone \textsc{Geant4} simulation.

The sensitivity is defined as the probability for a BIB particle to generate a visible signal in the detector. It is computed as the ratio $s = N/M$, where $N$ is the number of events in which at least one charged particle reaches a sensitive gas gap, while $M$ is the number of incident particles. 

The hit rate (R) is then obtained from the flux ($\Phi$) as $R = s \times \Phi$ for each energy value and particle type. The estimated rate is shown in Figure~\ref{fig:BIB_15TeV_hitrates_neutron} for neutron and in Figure~\ref{fig:BIB_15TeV_hitrates_photon} photons as a function of the angular coordinate $\theta$ for the different detector technologies considered.

\begin{figure}[!ht]
    \center
    \includegraphics[trim={1cm 1cm 0cm 2cm},clip,width=0.5\textwidth]{figures/Comparison_NeutronHitRate_BF.png}
    \caption{Estimated hit rate from neutrons at a \SI{3}{TeV} muon collider. Different colours represent different gaseous detector technologies considered: triple GEM, standard HPL RPC, glass RPCs (GRPC) and PicoSec. The shaded bands represent the statistical uncertainty from the simulated events.}
    \label{fig:BIB_15TeV_hitrates_neutron}
\end{figure}

\begin{figure}[!ht]
    \center
    \includegraphics[trim={1cm 1cm 0cm 2cm},clip,width=0.5\textwidth]{figures/Comparison_GammaHitRate_BF.png}
    \caption{Estimated hit rate from photons at a \SI{3}{TeV} muon collider. Different colours represent different gaseous detector technologies considered: triple GEM, standard HPL RPC, glass RPCs (GRPC) and PicoSec. The shaded bands represent the statistical uncertainty from the simulated events.}
    \label{fig:BIB_15TeV_hitrates_photon}
\end{figure}

The difference between the technologies is mainly due to a different material composition of the detectors: in general Micro-Pattern Gaseous Detectors, i.e. Triple-GEM and PicoSec, result in having a lower hit rates when compared to RPC.

\subsubsection*{Trigger and data acquisition}
%\label{sec:tdaq}

Experiments at a high energy muon collider are expected to operate at instantaneous luminosity levels of $10^{34}-10^{35}$ cm$^{-2}$ s$^{-1}$.

The Trigger and Data Acquisition (TDAQ) systems of the future muon collider experiments will be required to perform partial or full reconstruction of every collision event in order to identify and store events of interest to the physics programme. Given that realising the muon collider will take time, it is way too early to define the TDAQ strategy. Future advancements in the data transmission and processing technologies can substantially alter the vision of what a TDAQ system at the muon collider would look like. However, an initial estimate of the data rates and processing needs helps to outline possible options and strategies, in particular when put in the context of today’s technologies. 

Trigger and DAQ strategy taken by different collider experiments varies a lot and depends on the luminosity and complexity of their collision events. Experiments such as ATLAS and CMS, utilise hardware triggers~\cite{ATLAS:2016wtr,CMS:2016ngn} that rely on a subset of the detector subsystems for initial filtering of the events. This is followed by a High Level Trigger (HLT) farm where further processing and filtering takes place using more complete event information. The LHCb experiment, operating at lower luminosity and with smaller event size, recently opted for a so-called ``triggerless'' or ``streaming'' approach~\cite{LHCbCollaboration:2014vzo,Colombo:2018upq}, which eliminates need for a hardware trigger and where all collision data is streamed at 40~MHz directly to a HLT farm for event reconstruction. Similarly, electron-positron collider experiments~\cite{Behnke:2013lya,CLICdp:2018vnx} typically adopt a triggerless readout scheme due to the relative cleanliness of events when compared to hadron colliders. A streaming approach offers a number of advantages: the availability of the full event data typically translates into a better trigger decision, it is easier to support and upgrade software triggers, simplified design of the detector front-end, etc. However, the presence of large BIB at the muon collider may be prohibitive for a full triggerless TDAQ scheme. In the following, an initial estimate of the data rates is provided to show that from the data rates/volumes consideration a streaming DAQ implementation is feasible. Early estimates of the event processing time are also provided and compared to those anticipated at the HL-LHC experiments.

The amount of data acquired by the muon collider experiments is expected to be dominated by the tracker and calorimeter systems. For the silicon tracker, the event size and data rates are estimated by acquiring an average number of hits per event from simulation and multiplying it by the 100 kHz event rate. The average hit multiplicity as a function of the tracker layer can be found in Figure~\ref{fig:HitMultTracker}. In this estimation, it is assumed that each hit consists of 32 bits to encode charge, position, and time information and that zero-suppression is applied in the detector front-end. The hits are integrated in the time period of 1~ns following the bunch crossing, which allows to preserve good efficiency for hits from particles originating in the hard scattering but rejects a significant fraction of the BIB. No filtering based on the hit direction information is applied to avoid possible biases in the online selection. An additional safety factor of 2 is embedded in the calculation. With these assumptions, the tracker event size is estimated to be 40 Megabytes (MB) and the data rate from the tracker to be 30 Terabits per second (Tb/s). It should be noted that the numbers are dominated by the BIB hits. A similar approach is applied for calculating event data rates originating in the calorimeter. Here, the ECAL dominates with approximately 90 million channels and average occupancy of about 10$^{-3}$ hits per mm$^{2}$. A minimum energy threshold of 0.2~MeV is required in order for the hit to be read out and hits are assumed to be 20 bits wide. The HCAL contribution is small, less than 10\% of the ECAL.  After applying a safety factor of 2, calorimeter event size is estimated to be 40 MB and similar to that for the tracker. The full data rate corresponding to the sum of the tracker and calorimeter rates is therefore about 60 Tb/s, which is a factor of few larger than HLT input of LHCb experiment in Run-3~\cite{LHCbCollaboration:2014vzo} and comparable to the HLT input of CMS experiment in HL-LHC~\cite{Collaboration:2759072}. Therefore, from the data volumes point of view, a streaming operation at 100~kHz appears to be feasible. It should be emphasised that the rates are directly proportional to the bunch crossing frequency and can be much larger with a smaller collider ring or a multi-bunch operation scheme, in which case the strategy may have to be re-evaluated.

Another important parameter to consider is the HLT output rate to offline storage. Here again different approaches can be taken. One approach is to eliminate most of the events using filtering done in the HLT, but preserve full raw event information for the ones that pass the filter.  HL-LHC experiments assume HLT output bandwidth of approximately 60 GB/s. This would translate into 750 Hz of full event content sent to the storage. For comparison, single Higgs production at $\sqrt{s}=10$~TeV is expected to have a much lower rate of less than 0.1~Hz, and WW production via vector boson fusion will be at 1~Hz level. Storing full event content allows for later reprocessing of the data in order to improve the performance or reconstruct novel signatures. However, this approach would require about 4~PB of storage per day of running. It will produce a total dataset similar in size to that of ATLAS and CMS in HL-LHC, but dominated by BIB hits that are not interesting from the physics point of view. An alternative approach based on reducing event content by filtering out hits and clusters clearly unassociated with the hard scattering may be more prudent. Here one may choose to preserve the output rate of 750~kHz and reduce the total dataset size. Alternatively, one can aim for the same total dataset size but increase the rate of events, for example if on average 99\% of BIB hits in each event can be filtered out, every produced event at 100~kHz can be sent to the storage. 

In addition to data rates, it is also important to take into account time needed to reconstruct each event at the HLT.  Long processing times lead to unmanageable large farm requirements, which in turn is difficult to procure, maintain, and operate. HL-LHC experiments project processing times of about one second per event with tails extending as far as a minute. At the muon collider, the offline event reconstruction is currently dominated by charged particle tracking and takes up to few minutes per event, which is significantly slower than a good HLT target of few seconds. To estimate the amount of time needed at the HLT, a preliminary reconstruction of charged tracks was attempted in the outer layers where the BIB density is less severe. An initial estimation was performed by taking hits from the outer six barrel layers and applying an algorithm based on a three-dimensional Hough Transform in the ($\phi$, $\theta$, $p_{\rm T}$) parameter space. This approach assumed that all tracks would come from the origin. The preliminary analytical calculations suggest that, even with a three-dimensional array finely subdivided into 240 million cells, with the currently estimated BIB rates, tens of purely combinatorial track candidates with $p_{\rm T} > 2.5$~GeV would be found, precluding any possibility to use that information as an effective event filter. However, if an independent filter based on timing (or pointing, etc ...) is applied to lower the BIB hit rate by a factor four, requiring a single candidate track with $p_{\rm T} > 2.5$~GeV would give us the needed event rejection factor of about 50. In this case, 44 million cells in the Hough transform array would have to be filled for each beam crossing. Doing this in few seconds requires a powerful CPU, but is not out of reach. Future CPUs and accelerators (GPU, FPGA, etc), as well as algorithm improvements can further improve timing. The bottom line is that this is a crucial area of development and careful attention needs to be paid to this area in order to keep the processing time under control.

\begin{figure}[!t]
\center
\includegraphics[width=0.42\textwidth]{figures/HLT_swEB.png}
\includegraphics[width=0.42\textwidth]{figures/HLT_hwEB.png}
\caption{Possible TDAQ system architectures using (top) an LHCb-like approach with a software Event Builder or (bottom) hardware boards to structure event data and pass them to a high-level trigger farm.}
\label{fig:TDAQArch_LHCb_HW}
\end{figure}

Finally, it may be useful to sketch how a potential TDAQ system could look and estimate its size. The number of input links is an important parameter for this purpose. The lpGBT developed for HL-LHC are expected to provide bandwidth of up to 10~Gb/s per link. It is reasonable to assume that links with at least twice higher speeds (20~Gb/s) will be available on the timescale of 20 years. Under the assumption that data from multiple detector modules can be combined into a single 20~Gb/s link, close to 10,000 of such links will be needed to bring the data from the detector to the electronics area. The back-end system will consist of few hundred readout boards used to receive and format the data. A full-mesh hardware or a software Event Builder network with aggregate bandwidth of about 60~Tb/s will be required, for which both custom and industry provided solutions can be considered. Note that this number can be much smaller if a more aggressive filtering scheme than the one described above is utilised in the front-end. For example, preliminary studies indicate that filtering tracker hits based on their pointing information can yield up to a factor of eight reduction in the data rates. The required output bandwidth to storage will depend on the chosen filtering strategy at the HLT, but should not exceed 100~Gb/s even in the most aggressive scenarios. A schematic illustration of two alternative architectures is in Figure~\ref{fig:TDAQArch_LHCb_HW}. 

To summarise, despite the large presence of BIB, preliminary estimates based on simulation indicate that a streaming DAQ architecture can provide an attractive solution for the future muon collider experiments. With improvements to the tracking speed, such a solution can likely be realised with technologies available today. Future advancements (e.g. higher speed optical links, fast processors, etc) are likely to result in a smaller and/or more performing DAQ system. Work should be invested in improving  HLT reconstruction algorithms and exploiting GPU/FPGA/ASIC acceleration schemes with the aim to bring per-event processing time down to a few second level. 

\FloatBarrier
\subsection{Reconstruction performance studies}
\label{sec:performance}

This section describes the current status of the event reconstruction performance of a multi-purpose MCD with the layout described in Section~\ref{sec:environment}. The study focuses on the reconstruction algorithms and performance of high-p$_{\rm T}$ objects that will be needed for a successful physics programme. The results are built upon and significantly extend the ones reported in Ref.~\cite{Bartosik:2020xwr}. The current work aims at showing that satisfactory performance can be achieved in a muon collider environment. Further optimisation of the detector design and reconstruction algorithms are expected to significantly improve the presented performance and will be subject of future work.

The results are obtained using detailed simulations of the MCD, using the software framework described in Section~\ref{sec:software}. All simulations, unless otherwise stated, include overlaying on top of the desired $\mu^+\mu^-$ collision the model of BIB described in Section~\ref{sec:environment}.
The BIB sample used for detector-performance studies is the one of the $\sqrt{s} =$~\qty{1.5}{\TeV} muon collider produced with the MARS15 software, because the FLUKA-based workflow was not fully validated when most of the results were obtained.

\subsubsection*{Charged particle reconstruction}

In the magnetic field of the MCD, a charged particle will follow a helix trajectory aligned with the $z$ axis. The radius of curvature in the transverse plane is proportional to the transverse momentum of the particle and inversely proportional to the magnetic field.  Deviations from a perfect helix can occur due to multiple-scattering, ionising  energy losses and bremsstrahlung. The first two are a direct function of the detector material. The amount of radiation lengths and hadronic interaction lengths that a particle traverses through the tracking detector when starting from the nominal collision point are shown respectively in Figures~\ref{fig:tracking:x0-tracker} and~\ref{fig:tracking:l0-tracker}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/detector/x0.pdf}
    \caption{Radiation length of the tracking detectors, as seen along a line defined by the nominal interaction point and the polar angle $\theta$.}
    \label{fig:tracking:x0-tracker}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/detector/l0.pdf}
    \caption{Hadronic interaction length of the tracking detectors, as seen along a line defined by the nominal interaction point and the polar angle $\theta$.}
    \label{fig:tracking:l0-tracker}
\end{figure}

The charged particle trajectory is reconstructed from the spacepoints corresponding to the hit positions in the silicon tracking detectors. The reconstructed object is called a track. A track consists of a set of hits (one per layer) and five fitted parameters describing the helix. A track reconstruction algorithm can roughly be broken up into two steps: pattern recognition to identify the hits belonging to a single track and fitting the hit coordinates by a track model to deduce the relevant track parameters.

Track reconstruction in the MCD is complicated by the presence of a huge number of hits in the silicon sensor originating from the beam-induced background (BIB hits). The density of BIB hits is ten times larger than the expected contribution from pile-up events at a High Luminosity LHC detector. Table~\ref{tab:tracking:density} compares the hit density between the MCD, the ATLAS Inner TracKer (ITk)~\cite{ATLAS:2017svb, ATLAS:2017azf} and the ALICE ITS3~\cite{ALICE-PUBLIC-2018-013} upgrades for HL-LHC operation. The increase in possible hit combinations creates a challenge for the hit pattern recognition step. It is crucial to reduce the amount of hits given as input to a track reconstruction algorithm through alternate means, such as precision timing information. The BIB hits are out-of-time with hard collision hits after the time-of-flight correction has been applied. By applying a $-3\sigma/+5\sigma$ time window, the hit density can be reduced by a factor of two as seen in Figure~\ref{fig:HitMultTracker}.

\begin{table}[htb]
    \centering
    \caption{Comparison of the hit density in the tracking detector between a MuC with full BIB overlay, the ATLAS ITk and ALICE ITS3 upgrades for HL-LHC. The hit densities for the first and second layers of the vertex detectors are shown. The MCD hit densities are reported after timing cuts.}
    \label{tab:tracking:density}
    \vspace{0.3cm}
    \begin{tabular}{l|c|c|c}
        \textbf{Detector} & \multicolumn{3}{c}{\textbf{Hit Density [\unit{\mm^{-2}}]}} \\
        \textbf{Reference} & \textbf{MCD} & \textbf{ATLAS ITk}  & \textbf{ALICE ITS3} \\
        \hline
        \hline
         Pixel Layer 0 & 3.68 & 0.643  & 0.85 \\
         Pixel Layer 1 & 0.51 & 0.022  & 0.51 \\
%         Strips Layer 1 & 0.03 & 0.003  & \\
    \end{tabular}
\end{table}

The spatial distribution of BIB-hits is also unique. They are different from hits created by pile-up collisions. Pile-up hits come from real charged particle tracks originating from multiple vertices in the collision region. On the other hand, BIB-hits come from a diffuse shower of soft particles originating from the nozzles. The compatibility of a track with a trajectory originating from the luminous region provides an important handle for differentiating ``real'' tracks of charged particles produced in the primary collision and ``fake'' tracks generated from random combinations of BIB-hits.

The remainder of this section describes three approaches that were studied for track reconstruction at the MCD. The first two use the Conformal Tracking (CT) algorithm developed for the clean environment of the electron-positron colliders~\cite{Brondolin:2019awm}. However in the presence of BIB, the CT algorithm takes weeks to reconstruct a single event and is impractical for large-scale production of simulated data. To ease the computational effort, the input hits are first reduced by either defining a Region of Interest or by exploiting the double-layered Vertex Detector to select only hit pairs pointing to the collision region. A third approach uses the Combinatorial Kalman Filter (CKF)~\cite{Billoir:1989mh, Billoir:1990we, Mankel:1997dy} algorithm developed for the busy environment of hadron colliders. It can perform track reconstruction in a reasonable time without requiring any additional filtering of input hits.

It should be noted that the CT and CKF algorithms have very different software implementations that are responsible for much of the difference in their performance. The CKF algorithm is implemented using the A Common Tracking Software (ACTS)~\cite{Ai:2021ghi} library that is heavily optimised for efficient computing. The same is not true for the CT algorithm implemented directly in iLCSoft with less emphasis on computational efficiency. It is possible that part of the computational improvements come from the code optimisation alone. For example, the ACTS Kalman Filter implementation is a factor 200 faster than the default iLCSoft implementation given the same inputs.  This demonstrates the advantage of an experiment-independent track reconstruction library developed by a dedicated team with strong computing expertise.

The expected performance is assessed using a sample of single muons originating from the interaction point. Two set of samples are used; one set is generated with the muon having a fixed momentum $(p)$ of $1$, $10$ and $100$~GeV and a uniform distribution in $\theta$. The second set is generated at discrete values of $\theta=13^\circ,30^\circ,89^\circ$ and uniform transverse momentum distribution in the $1-100$~GeV range. The chosen $\theta$ values correspond to particles expected to leave hits entirely in the endcap system, some in the barrel and some in the endcap, and entirely in the barrel region, respectively.

\subsubsection*{Conformal tracking in regions of interest}
\label{sec:trk-roi}
The Conformal Tracking algorithm is based on Ref.~\cite{Brondolin:2019awm} and its implementation in iLCSoft. The algorithm is designed and optimised to find charged particles in very clean environments, as the ones in $e^+e^-$ colliders. The conformal mapping technique~\cite{Hansroul:1988wa} is combined with cellular automaton approach~\cite{Glazov:1993ur} to increase the acceptance of non-prompt particles.  

Due to the very large hit multiplicity from BIB-hits, running such an algorithm for the full event is prohibitive in terms of CPU and memory resources. Instead, a region-of-interest approach is used, where hits to be considered are pre-selected based on existing objects reconstructed in either the calorimeters or the muon system. To assess tracking performance only hits within a cone of $\Delta R < 0.5$ around the signal muon were selected as input to the CT algorithm, where $\Delta R = \sqrt{\Delta \phi^2 + \Delta \eta^2}$.

Figure~\ref{fig:tracking:perf-roi_pteff} shows the reconstruction efficiency as a function of $p_{\rm T}$; a muon is considered reconstructed if at least half of the hits associated to the track have been originated by the muon. Optimal reconstruction efficiency is achieved throughout the $p_{\rm T}$ spectrum, with a somewhat smaller efficiency for very forward particles due to their proximity to the nozzle and much larger expected occupancy in the region. The latter is expected to be recovered by a more dedicated tuning of the algorithm or by using one of the algorithms described in the next sections. 

\begin{figure}[t]
\centering
\includegraphics[width=0.49\textwidth, trim = {0 4 0 18 } , clip]{figures/tracking/RoI/eff_vs_pt_purity50.pdf}
\caption{Track reconstruction efficiency as a function of $p_{\rm T}$ for single-muon events overlaid with BIB.}
\label{fig:tracking:perf-roi_pteff}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.49\textwidth, trim = {0 8 0 18 } , clip]{figures/tracking/RoI/res_vs_theta_purity50_chi2ndf10.pdf}
\caption{Momentum resolution $\Delta p_{\rm T}/p^2_{\rm T}$ as a function of $\theta$ for single-muon events overlaid with BIB.}
\label{fig:tracking:perf-roi_resopt}
\end{figure}

Figure~\ref{fig:tracking:perf-roi_resopt} shows transverse momentum resolution as a function of polar angle $\theta$. The resolution is computed by comparing reconstructed and generated $p_{\rm T}$ and shown divided by $p^2_{\rm T}$. A localised degradation of the resolution can be seen around $\theta\approx 35^\circ$, corresponding to the barrel-endcap transition; in addition, the feature is enhanced by the non-physical lack of spread of the muon originating point. It is expected that a more realistic simulation of the luminous region as well as future optimisations of the tracker layout will mitigate such localised degradation to negligible levels.

Figure~\ref{fig:tracking:perf-roi_d0} shows the resolution on the transverse impact parameter $D_0$, while Figure~\ref{fig:tracking:perf-roi_z0} the resolution on the longitudinal impact parameter $Z_0$ as a function of the polar angle $\theta$. Similarly to the case of the resolution on $p_{\rm T}$, the resolution on $D_0$ and $Z_0$ slightly degrades in the barrel-endcap transition region, around $\theta \approx 35^\circ$. 

\begin{figure}[t]
\centering
\includegraphics[width=0.49\textwidth, trim = {0 10 0 18 } , clip]{figures/tracking/RoI/trk_res_D0.pdf}
\caption{Transverse impact parameter resolution as a function of $\theta$ for single muon events overlaid with BIB.}
\label{fig:tracking:perf-roi_d0}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.49\textwidth, trim = {0 10 0 18 } , clip]{4_Detector_and_Reconstruction/figures/tracking/RoI/trk_res_Z0.pdf}
\caption{Longitudinal impact parameter resolution as a function of $\theta$ for single muon events overlaid with BIB.}
\label{fig:tracking:perf-roi_z0}
\end{figure}

\subsubsection*{Conformal tracking with double layers}
\label{sec:trk-dl}
The Vertex Detector is constructed using double-layers (DL). A double-layer consists of two silicon detector layers separated by a small distance (\qty{2}{mm} for the MCD). This concept will also be used by the CMS Phase-II tracking detector\cite{CMS:2017lum} to reduce the hit combinatorics for a fast track reconstruction in their trigger system. It works by selecting only those hits that can form a pair with a hit from the neighbouring layer that is aligned with the IP. If there is no second hit in the double-layer to form a consistent doublet the hit is discarded. This approach is particularly effective for rejecting BIB hits, because BIB electrons are very likely to either stop in the first layer due to their very low momentum, or to cross the double-layer at shallow angles, creating doublets that are not aligned with the IP.
The DL filtering implemented in the simulation software is based on the angular distance between the two hits of a doublet when measured from the interaction point, as shown in Figure~\ref{fig:tracking:dl-howto}. For simplicity the two variables used for filtering are the polar ($\Delta\theta$) and azimuthal ($\Delta\phi$) angle differences.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{4_Detector_and_Reconstruction/figures/tracking/DL/double_layer.pdf}
    \caption{Illustration of the doublet-layer filtering used for the rejection of BIB-induced hits in the Vertex Detector. The horizontal black lines represent double layers of pixel sensors that are crossed by signal (green) and BIB (grey) particle tracks. Hit doublets created by BIB particles are characterised by larger angular difference than those created by signal particles, due to their shallow crossing angle and more displaced origin.}
    \label{fig:tracking:dl-howto}
\end{figure}

In practice there are several limitations to the precision of alignment that can be imposed by the DL filtering while maintaining high efficiency for signal tracks.
The first is driven by the finite spatial resolution of the pixel sensors, which limits the minimum resolvable displacement between the two hits of a doublet. The sensor positions needs to be known beforehand and any uncertainty will result in an inefficiency. The latter point is also important for particles with non-negligible lifetime, such as $b$-meson decay products, that do not originate from the IP.


%
\begin{figure}[t]
\centering
\includegraphics[width=0.47\textwidth]{figures/tracking/DL/dPhi_pt_vxdb.pdf}
\caption{Distribution of $\Delta\phi$ in hit doublets in the innermost double-layer of the Vertex Detector in the barrel region for muon tracks of different transverse momenta. Two separate peaks become visible for low-$p_{\rm T}$ tracks corresponding to $\mu^+$ and $\mu^-$.}
\label{fig:tracking:perf-dl-dphi}
\end{figure}
%
Figure~\ref{fig:tracking:perf-dl-dphi} shows the distributions of $\Delta\phi$ in the first layer of the vertex detector for single-muon events with a realistic beamspot spatial distribution. 
The bi-modal nature of the $\Delta\phi$ distribution for low energy muons is the result of the circular path that charged particle take in the transverse plane. This biases the DL filtering toward charge particles with higher $p_{\rm T}$ unless very loose selection criteria are used.

The distribution of $\Delta\theta$ is shown instead in Figure~\ref{fig:tracking:perf-dl-dtheta}, comparing scenarios with different longitudinal displacements of the interaction point. 
%
\begin{figure}[t]
\centering
\includegraphics[width=0.47\textwidth]{figures/tracking/DL/dTheta_pt_bs_vxdb.pdf}
\caption{Distribution of $\Delta\theta$ in hit doublets in the innermost double-layer of the Vertex Detector in the barrel region for muon tracks of $p_{\rm T} = \qty{1}{\GeV}$ varying the longitudinal displacement of the interaction point by \qty{10}{\milli\metre}.}
\label{fig:tracking:perf-dl-dtheta}
\end{figure}
%
Table~\ref{tab:tracking:dl-rejection} lists the two operating points ({\em loose} and {\em tight}) used to filter hits for the CT algorithm in two stages.
%
\begin{table*}[h]
    \centering

    \begin{tabular}{c|l||c|c|c|c||c|c|c|c}
    \multicolumn{2}{r||}{}       & \multicolumn{4}{c||}{\textbf{Barrel}} & \multicolumn{4}{c}{\textbf{Endcap}}\\
    \multicolumn{1}{c}{} & Layer IDs & 0,1 & 2,3 & 4,5 & 6,7 & 0,1 & 2,3 & 4,5 & 6,7\\
    \hline\hline
    \multirow{3}{.75in}{\centering \textbf{Loose DL} selections} 
    &Max. $\Delta\phi$ (mrad)    & 2.8 & 2.0 & 1.7 & 1.5 & 2.1 & 1.7 & 1.6 & 1.5 \\
    &Max. $\Delta\theta$ (mrad)  & 35 & 18 & 10 & 6.5 & 3.5 & 1.5 & 0.7 & 0.5  \\
    &Hit surival fraction        &\multicolumn{4}{c||}{55\%} &\multicolumn{4}{c}{18\%}\\
    \hline
    \multirow{3}{.75in}{\centering \textbf{Tight DL} selections} 
    &Max. $\Delta\phi$ (mrad)    & 3.0 & 2.0 & 1.6 & 1.5 & 2.2 & 1.8 & 1.7 & 1.6\\
    &Max. $\Delta\theta$ (mrad)  & 0.5 & 0.4 & 0.3 & 0.25 & 0.2 & 0.18 & 0.12 & 0.1 \\
    &Hit survival fraction       &\multicolumn{4}{c||}{2\%} & \multicolumn{4}{c}{2\%}\\
    \end{tabular}
    
    \caption{Angular selection on double-layers for hit suppression. A loose (left) and tight (right) selection is shown. The hit survival rate represents the fraction of hits (mostly from BIB) surviving the selections.}
    \label{tab:tracking:dl-rejection}
\end{table*}
%
Figure~\ref{fig:tracking:perf-dl-hits-reduction} shows the hit multiplicity (mostly BIB-hits) as a function of VXD layer. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/tracking/hit_mult_dl.pdf}
    \caption{Expected reduction of hit multiplicity in the Vertex Detector achieved by applying the loose or tight double-layer filtering.}
    \label{fig:tracking:perf-dl-hits-reduction}
\end{figure}

The {\em loose} working point targets high efficiency reconstruction of $p>\qty{1}{\GeV}$ muons with a realistic beam-spot size. It reduces the number of hits in the innermost double-layer by a factor of two. This reduces the CT reconstruction time from $\approx\qty{1}{week/event}$ to $\approx\qty{2}{days/event}$. While significant, this is still not practical for sample production. Instead the {\em loose} working point is used with a special CT configuration as a first stage of a two-stage reconstruction process. The first stage reconstructs high-$p_{\rm T}$ tracks to precisely determine the IP position. 

The {\em tight} working point is optimised for the scenario when the interaction point is precisely known. It has a hit survival rate of $\approx{}2\%$ in the inner-most layer. This reduces the hit multiplicity enough for the CT algorithm to complete in $\approx\qty{2}{min/event}$. It is used as the second stage of the track reconstruction algorithm, with the IP determined from the first-stage.

The two-stage doublet-layer filtering provides a computationally viable method to track reconstruction in the presence of the BIB. However its efficiency is very limited for reconstructing non-prompt particles, including those arising from $b$-meson decays.

\subsubsection*{Combinatorial Kalman filter}
\label{sec:trk-ckf}
The CKF algorithm seeded using hit triplets was implemented using the ACTS library v13.0.0. The implementation is a Marlin Processor that serves as a drop-in replacement for the existing tracking Processors. In addition to providing an alternate algorithm designed for large hit multiplicity, ACTS also provides a heavily optimised code for fast computation. The reconstruction strategy used in this section reconstructs tracks at the rate of \qty{4}{\min/event}. It provides the first practical and comprehensive tracking solution for the MCD.

The seeds for the CKF algorithm are formed from hit triplets in the four layers of the Vertex Detector. Only hits in the outer half of doublet layer are considered. Several heuristics are use to determine if each triplet is compatible with a track and can be used as a seed. The seeding algorithm is configured using the ACTS default values, with the exception of:
\begin{itemize}
    \item the radial distance between hits is between \qty{5}{\mm} and \qty{80}{\mm},
    \item the minimum estimated $p_{\rm T}$~is \qty{500}{\MeV},
    \item the maximum forward angle of \qty{80}{\degree},
    \item the extrapolated collision region is within \qty{1}{mm} of detector centre,
    \item the average radiation length per seed is 0.1,
    \item the allowed amount of scattering is \qty{50}{\sigma},
    \item the middle hits in each seed are unique.
\end{itemize} 

This configuration has not been fully optimised. For example, the size of the collision region is smaller than the expected beam-spot size. An initial implementation of track seeding from the Outer Tracker, followed by a track extrapolation towards the centre of the detector, has recently become available but has not yet studied in details. The lower BIB-hit multiplicity in this detector region could be exploited to loosen some requirements and improve the reconstruction efficiency for tracks originating far from the IP.

Around 150\,000 seeds are found per event. The efficiency of the seeding algorithm is found to be fully efficient for muons with $p_{\rm T}>2$~GeV, dropping to about 90\% in the region within \qty{10}{\degree} from the beam axis. Loosening the collision region definition increases the amount of fake seeds and reduces the seed finding efficiency due to the seed overlap removal. The latter can be addressed, at a cost in run time, by allowing multiple seeds to share the same middle hit.

The CKF is run inside-out, meaning that the track extrapolation starts from the inner-most seed hit and continues outward in the radial direction. The initial track parameters are estimated from the seed. The CKF algorithm has only two tunable parameters: the number of hits added for each layer, which is set to one, and the width of the hit search window at each layer, which is set to \qty{10}{\chi^2}. As with the seeding algorithm, these values were not yet optimised. The consideration of only a single hit at each layer means that the CKF algorithm will not branch to consider multiple track candidates for a single seed. A good track reconstruction efficiency is observed irrespective of this tight requirement, as demonstrated in Figures~\ref{fig:trk-ckf-pteff} and~\ref{fig:trk-ckf-thetaeff}, which show the track reconstruction efficiency as a function of particle $p_{\rm T}$ and $\theta$ for single muon events with $p=\qty{10}{\GeV}$. This underlines the difference between BIB and pile-up; the BIB-hits are a ``random'' background that is not compatible with the trajectory of a track.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.486\textwidth, trim = {0 0 0 15 } , clip]{figures/tracking/ckf/efficiency_muonGun_truth_pt.pdf}
    \caption{Track reconstruction efficiency for events containing a single muon with (red) and without (blue) BIB overlay as a function of the true muon $p_{\rm T}$.}
    \label{fig:trk-ckf-pteff}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.475\textwidth, trim = {0 0 0 25 } , clip]{figures/tracking/ckf/efficiency_muonGun_truth_theta.pdf}
    \caption{Track reconstruction efficiency for events containing a single muon with (red) and without (blue) BIB overlay as a function of the true muon $\theta$.}
    \label{fig:trk-ckf-thetaeff}
\end{figure}

Muons with $p_{\rm T}>\qty{2}{\GeV}$ are reconstructed with 90\% efficiency or greater even in the presence of BIB. There is a considerable amount of fake tracks ($\approx100,000$ per event). Figure~\ref{fig:trk-ckf-pt} compares the $p_{\rm T}$ distribution for real and fake tracks. Fake tracks are mostly at low $p_{\rm T}$.
Similarly, when considering the number of hits associated to a track, shown in Figure~\ref{fig:trk-ckf-nhits}, the fake tracks arising from the BIB are associated with a significantly smaller number of hits. The latter further underlines the randomness of the BIB-hits and provides a handle for reducing the fake rate.

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth, trim = {0 0 0 10 } , clip]{figures/tracking/ckf/fakevsreal_muonGun_pt.pdf}
\caption{Track $p_{\rm T}$ distributions for tracks with (blue) and without (red) a match to the true simulated tracks, for single muon events with BIB overlay.}
\label{fig:trk-ckf-pt}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.475\textwidth]{figures/tracking/ckf/fakevsreal_muonGun_nhit.pdf}
\caption{Hit multiplicity $N_{\text{hit}}$ distribution for tracks with (blue) and without (red) a match to the true simulated tracks, for single muon events with BIB overlay.}
\label{fig:trk-ckf-nhits}
\end{figure}

%\FloatBarrier
\subsubsection*{Jets}

Jet reconstruction is one of the most difficult reconstruction tasks at a muon collider, since almost all sub-systems are involved, and the impact of the BIB is significant in all of them, with different features in different sub-detectors.
The jet reconstruction algorithm employed is described in this section, and its performance is discussed. The algorithm has been designed to reconstruct jets in the presence of the BIB, but it is far from being fully optimised, and further studies are needed in the future.
However, even at this very early stage, the jet reconstruction can achieve a decent performance.

The algorithm follows the principles of particle flow reconstruction as implemented in the PandoraPFA package, and comprises of the following steps:
\begin{enumerate}
    \item tracks are reconstructed using the CKF algorithm described in Section~\ref{sec:trk-ckf} and are required to have at least three hits in the Vertex Detector and at least two hits in the Inner Tracker;
    \item calorimeter hits are selected by requiring a hit time window and an energy threshold;
    \item tracks and calorimeter hits are used as inputs in the PandoraPFA~\cite{Marshall:2013bda} algorithm to obtain reconstructed particles;
    \item the reconstructed particles are clustered into jets with the $k_t$ algorithm.
\end{enumerate}

The reconstructed jets are then required so satisfy quality selections to reject fake jets arising from the spurious combination of BIB energy deposits. Finally, the energy of the jets passing the BIB-removal selection are calibrated.

The jet performance has been evaluated on simulated samples of $b\bar{b}$, $c\bar{c}$ and $q\bar{q}$ dijets, where $q$ stands for a light quark ($u$,$d$ or $s$). These samples have been generated with an almost uniform dijet $p_{\rm T}$ distribution from 20 to \qty{200}{\GeV}. Samples of $\mu^+ \mu^- \to H ( \to b\bar{b}) + X $ and $\mu^+ \mu^- \to Z ( \to b\bar{b}) + X $ at $\sqrt{s}=3$ TeV are also used to study the dijet invariant mass resolution. 

\subsubsection*{Calorimeter hit selection}

Calorimeter hits are filtered depending on the normalised hit time, defined as $t_N = t - t_0 - cD$, where $t$ is the absolute hit time, $t_0$ is the collision time, $c$ is the speed of light, and $D$ is the hit distance from the origin of the reference system.
A time window of $\pm 250$~ps is applied to remove most of the BIB hits but preserving the signal, as can be seen in Figure~\ref{fig:calo_time}. 

\begin{figure}[b]
\centering
\includegraphics[width=0.48\textwidth]{figures/jets/norm_time_calo.pdf}
\caption{Normalised hit time in ECAL barrel, for $b$-jets and BIB. Both distributions are normalised to the same area. The time is not smeared for the detector time resolution. The time window of \qty{\pm 250}{\pico\second} applied in the jet reconstruction is shown.}
\label{fig:calo_time}
\end{figure}

A time window of width $\Delta$ is generally applicable if the Full Width at Half Maximum (FWHM) of the time resolution of the calorimeter cell is below $\Delta/3$. Therefore, in this particular case, a FWHM of at least 167~ps is assumed, which should be achievable by state-of-the-art calorimeter technologies.
    
Several calorimeter hit energy thresholds have been tested, in order to reduce hits produced by BIB. The computing time of the jet algorithm exponentially grows with the number of calorimeter hits: therefore, with the current resources it is not possible to reduce the thresholds far below 2~MeV. A threshold of 2~MeV is hence applied to both ECAL and HCAL. 
This requirement reduces the average number of ECAL Barrel hits from 1.5 million to less than $10^4$.

\subsubsection*{Particle flow, jet clustering and fake jet removal} 
    
Calorimeter hits and tracks are given as input to the PandoraPFA algorithm, that produces as output reconstructed particles known as particle flow objects. The PandoraPFA algorithm is described in detail in Ref.\cite{Thomson:2009rp}.
The particle flow objects are then clustered into jets by the $k_T$ algorithm. A cone radius of $R=0.5$ is used.

An average of 13 fake jets from BIB energy deposits per event is reconstructed, which need to be removed by applying additional quality criteria. The number of tracks in the jet has been found to be the most discriminating feature between real and fake jets, as shown in Figure~\ref{fig:jet_ntracks}.

\begin{figure}[b]
\centering
\includegraphics[width=0.47\textwidth]{figures/jets/jet_ntracks.pdf}
\caption{Number of tracks associated to real $b$-jets and to fake jets from BIB.}
\label{fig:jet_ntracks}
\end{figure}

Most of the fake jets from BIB have no tracks associated to them. Therefore, requiring at least one track allows to reduce the rate of fake jets by more than two orders of magnitude, with a moderate cost, at the level of 5--10\%, in terms of real jet selection efficiency. Figure~\ref{fig:jet_pT_eff} shows the jet selection efficiency as a function of the true jet $p_{\rm T}$ in the region $|\eta|<1.5$, for different jet flavours. The overall efficiency varies between 82$\%$ at low $p_{\rm T}$ to 95$\%$ at higher $p_{\rm T}$. 

\begin{figure}[b]
\centering
\includegraphics[width=0.5\textwidth]{figures/jets/jet_eff_flavour.pdf}
\caption{Jet selection efficiency as a function jet $p_{\rm T}$ for $b$-jets, $c$-jets and light jets in the central region $|\eta|<1.5$. The differences between the jet flavours are mainly due to different jet $\eta$ distributions in the three samples.}
\label{fig:jet_pT_eff}
\end{figure} 

The dependency of the selection efficiency on the jet polar angle $\theta$ is shown in Figure~\ref{fig:bjet_eta_eff} for a sample of $b$-jets. The efficiency is around 90$\%$ in the central region. A significant drop is observed for $|\theta|<0.5$, where the efficiency is below 30$\%$. This effect is mainly due to the track requirement, since many jets without reconstructed tracks are found in the forward region. 

\begin{figure}[b]
\centering
\includegraphics[width=0.48\textwidth]{figures/jets/jet_eff_theta.pdf}
\caption{Efficiency of jet selection as a function of truth-level jet $\theta$.}
\label{fig:bjet_eta_eff}
\end{figure}

Figure~\ref{fig:jet_fakes} shows the fake jet rate as a function of jet $p_{\rm T}$ for a sample of $b$-jets. The fake rate is defined as the average number of reconstructed fake jets that are not matched with a true particle originating from the IP per event, which is well below 1\%.

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{figures/jets/jet_fake_rate.pdf}
\caption{Jet fake rate as a function of the jet $p_{\rm T}$, obtained after requiring at least one track associated to the jet.}
\label{fig:jet_fakes}
\end{figure}

\subsubsection*{Jet momentum correction}

The jet 4-momentum is defined by the sum of 4-momenta of particles that belong to the jet. The jet axis is identified by the jet momentum direction. A fiducial region for jet reconstruction is defined by selecting jets with $|\eta|<2.5$.

In order to recover the energy lost by reconstruction inefficiencies, non sensitive material, as well as to take into account BIB contamination, a correction to the jet 4-momentum is applied. This correction has been determined by comparing the reconstructed jet $p_{\rm T}$ with the corresponding truth-level jet $p_{\rm T}$. Truth-level jets are defined as jet clustered by applying the $k_t$ algorithm to visible Monte Carlo particles. Reconstructed and truth-level jets are matched if their $\Delta R = \sqrt{(\Delta \eta)^2 + (\Delta \phi)^2} < 0.5$, where $\Delta \eta$ and $\Delta \phi$ are respectively the pseudo-rapidity and the azimuthal angle differences between the reconstructed-level jet axis and truth-level jet axis. If more than one reconstructed jet is matched to the same truth-level jet, then the one with lowest $\Delta R$ is chosen.

The correction is evaluated in five equal-width intervals of reconstructed $|\eta|$ between 0 and 2.5. Each pseudo-rapidity interval is further divided in 19 equal-width intervals of reconstructed $p_{\rm T}$ between 10 and 200~GeV. For each interval the average and standard deviation of the truth-level jet $p_{\rm T}$ distribution is calculated. Transfer functions are then obtained in each $\eta$ interval by fitting the average truth-level jet $p_{\rm T}$ as a function of reconstructed jet $p_{\rm T}$. Examples of transfer functions are shown in Figures~\ref{fig:jec1} and~\ref{fig:jec2}. These functions are then used to obtain the scale factor that is applied to each component of the reconstructed 4-momentum.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/jets/jec_0.pdf}
    \caption{Transfer function used for jet momentum corrections. The average truth-level jet $p_{\rm T}$ as a function of the reconstructed jet $p_{\rm T}$ is shown, for $1.09<\theta<1.57$. The error bars represent the standard deviation of the truth-level jet $p_{\rm T}$ in each interval.}
    \label{fig:jec1}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/jets/jec_3.pdf}
    \caption{Transfer function used for jet momentum corrections. The average truth-level jet $p_{\rm T}$ as a function of the reconstructed jet $p_{\rm T}$ is shown, for $0.27<\theta<0.44$. The error bars represent the standard deviation of the truth-level jet $p_{\rm T}$ in each interval.}
    \label{fig:jec2}
\end{figure}

\subsubsection*{Jet reconstruction performance}

The jet reconstruction performance is evaluated with a simulated sample of dijets events.
The relative difference between reconstructed and true jet $\theta$ is shown in Figure~\ref{fig:theta_reso}: the standard deviation of this distribution, directly related to the jet-axis angular resolution, is 14$\%$.

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{figures/jets/delta_theta.pdf}
\caption{Relative difference between reconstructed and true jet pseudo-rapidity.}
\label{fig:theta_reso}
\end{figure}  

The jet $p_{\rm T}$ resolution as a function of the true jet $p_{\rm T}$ is shown in Figure~\ref{fig:jet_flavour} for different jet flavours. The resolution goes from 35$\%$ for jet $p_{\rm T}$ around 20~GeV to 20$\%$ for high jet $p_{\rm T}$.

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/jets/jet_reso_flavour.pdf}
\caption{Jet $p_{\rm T}$ resolution as a function jet $p_{\rm T}$ for $b$-jets, $c$-jets and light jets in the central region $0.44<\theta<2.70$. The differences between the jet flavours are mainly due to different jet $\theta$ distributions in the three samples.}
\label{fig:jet_flavour}
\end{figure} 

Simulated event samples of $H \rightarrow b \bar{b}$ and $Z \rightarrow b \bar{b}$ are used to evaluate the dijet invariant mass reconstruction.
The invariant mass separation between these two processes is of paramount importance for physics measurements at the muon collider. In this study both jets are required to have $p_{\rm T}>40$~GeV and $0.44<\theta<2.70$. The distributions for the two processes are fitted with double Gaussian functions, and the shapes are compared in Figure~\ref{fig:hbb}. A relative width, defined as the standard deviation divided by the average value of the mass distribution, of 27$\%$(29$\%$) for $H \rightarrow b \bar{b}$($Z \rightarrow b \bar{b}$) is found. 

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/jets/higgs_vs_z.pdf}
\caption{Fitted dijet invariant mass distributions for $H \rightarrow b \bar{b}$ and $Z \rightarrow b \bar{b}$. The distributions are normalised to the same area.}
\label{fig:hbb}
\end{figure}

\subsubsection*{Future prospects on jet reconstruction}
\label{sec:future_jets}
Several ongoing studies are aimed at improving the jet reconstruction performance targeting several aspects, such as:
\begin{itemize}
    \item \textbf{track filter}: the track filter has a different impact in the central and the forward region, in particular the efficiency in the forward region is lower. An optimised selection will be defined to mitigate the efficiency loss in the forward region;
    \item \textbf{cell energy threshold}: the hit energy threshold has been set to the relatively high value of 2~MeV, as a compromise between computing time and jet reconstruction performance. This is a major limitation in the jet performance as can be seen in Figure~\ref{fig:h_2_vs_1}, where the $H \rightarrow b \bar{b}$ dijet invariant mass, reconstructed without the BIB overlay, is compared between 2~MeV and 200~keV thresholds. Reducing this threshold is not an easy task, given the large number of calorimeter hits selected from the BIB that contaminate the jet reconstruction. 
    %Figure~\ref{fig:h_2_vs_1} (bottom) demonstrates this by comparing the performance with a threshold of 1~MeV with the current setting of 2~MeV. 
    To tackle this problem an optimised algorithm should be developed: as an example thresholds that depend on the sensor depth could by applied, since the longitudinal energy distribution released by the BIB is different from the signal jets as shown in Figure~\ref{fig:calo_hit}. A generalisation of this idea could be the application of a multivariate-algorithm trained to select signal hits and reject BIB hits;
    \item \textbf{fake jet removal}: the fake jet removal applied in this study has an impact in reducing the jet efficiency in the forward region. Moreover this issue is highly dependent from the calorimeter thresholds. A fake removal tool based on machine learning and with jet sub-structure observables as input could be developed to solve this task.
\end{itemize}
    
\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/jets/higgs_noBIB.pdf}
    \caption{$H \rightarrow b \bar{b}$ dijet invariant mass, reconstructed without the presence of the BIB and with 2~MeV and 200~keV calorimeter hit energy thresholds. 
    }
    \label{fig:h_2_vs_1}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/jets/calo_longitudinal.pdf}
    \caption{Distribution of the ECAL barrel hits distance from the interaction point (weighted for the hit energy), for both $b$-jets and BIB. Both distributions are normalised to the same area.}
    \label{fig:calo_hit}
\end{figure}

\FloatBarrier
\subsubsection*{Jet flavour identification}

The $b$-jet identification algorithm described in this section relies on the reconstruction of the secondary vertices that are compatible with the decay of the heavy-flavour hadron in the geometrical proximity of a reconstructed jet.
 
For the secondary vertices reconstruction, tracks are reconstructed with the Conformal Tracking with Double Layers algorithm described in Section~\ref{sec:trk-roi}, within regions of interest defined by cones with $\Delta R=0.7$ around the jet axes. 
%The effect of the double layer filter on the secondary vertex reconstruction is then corrected to obtain the efficiency prior to its application.

\subsubsection*{Secondary vertex tagging}
\label{sv_tag_algo}

The vertexing algorithm employed for the primary and secondary vertex reconstruction is described in detail in Ref.~\cite{Suehara:2015ura}. In order to reduce the amount of combinatorial tracks due to BIB, cuts are applied to the reconstructed tracks used as input to the algorithm. 
The algorithm proceeds as follows:
\begin{enumerate}
\item \textbf{primary vertex finding}: tracks with $|d_{0}| \leq \qty{0.1}{\mm}$ and $|z_{0}| \leq \qty{0.1}{\mm}$ are selected and used as inputs to the primary vertex fitter. Furthermore, each track is required to have at least four hits in the vertex detector in order to reduce the number of BIB tracks;
\item \textbf{tracks selection for secondary vertex finder}: tracks not used to reconstruct the primary vertex are used as input to the secondary vertex fitter. Figure~\ref{fig:b_c_vs_bib} shows the distributions of the total number of hits in the vertex detector for BIB tracks and for tracks matched at Monte Carlo level with particles generated by $b$ or $c$ hadrons decays. A minimum number of 4 hits in the vertex detector is required in order to keep most of the the tracks from $b$ and $c$ hadrons decay, while rejecting most of BIB tracks. 

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/heavy_flavour/nvtx_hits_def.pdf}
\caption{Number of hits in the vertex detector of BIB combinatorial tracks (red) and of tracks matched with Monte Carlo truth particles coming from $b$ or $c$ hadrons (blue and black respectively) decay. The distributions are normalised to unit area.}
\label{fig:b_c_vs_bib}
\end{figure}

Further selections on the track $p_{\rm T}$, the maximum track $z_0$ and $d_0$, and the $d_0$ and $z_0$ errors are applied in order to further reduce the amount of BIB tracks. As an example, Figure~\ref{fig:b_c_vs_bib_pt} shows the $p_{\rm T}$ distributions for tracks from $b$- or $c$-hadrons decays and BIB. By requiring $p_{\rm T} > 0.8$~GeV about 80\% of the BIB tracks are rejected, while retaining an efficiency of approximately 85--90\% for the tracks from $b$- or $c$-hadrons.
The $z_0$ distributions for tracks coming from $b$- and $c$-hadrons and BIB are shown in Figure~\ref{fig:b_c_vs_bib_z0}. A requirement of $|z_0| \leq \qty{5}{mm}$ is applied to reject BIB tracks at large $z_0$;
\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/heavy_flavour/p_t_tracks_def_log.pdf}
 \caption{Transverse momentum distribution for tracks coming from $b$ (blue) or $c$ (black) hadrons decay and combinatorial BIB (red) tracks. The distributions are normalised to the number of tracks.}
\label{fig:b_c_vs_bib_pt}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/heavy_flavour/z0.pdf}
 \caption{Longitudinal impact parameter distribution for tracks coming from $b$ (blue) or $c$ (black) hadrons decay and combinatorial BIB (red) tracks. The distributions are normalised to the number of tracks.}
\label{fig:b_c_vs_bib_z0}
\end{figure}

\item \textbf{secondary vertex finding}: the tracks passing the requirements are used to build two-tracks vertex candidates, that must satisfy the following requirements: the invariant mass must be below 10 GeV and must be smaller than the energy of each track; the position with respect to the primary vertex must lie in the same side of the sum of the tracks momenta; and the $\chi^2$ of the tracks with respect the secondary vertex position must be below 5. The track pairs are also required not to be compatible with coming from the decay of neutral long lived particles.
Additional tracks are iteratively added to the two-tracks vertices if they satisfy the above requirements. Tracks associated to more than one secondary vertex are assigned to the vertex with the lowest $\chi^2$ and removed from other vertices.
\end{enumerate}

\subsubsection*{Secondary vertex tagging performance}
\label{sv_tagging_eff}

The performance of the identification of jets arising from the hadronisation of $b$ quarks,  $b$-tagging, is evaluated using the secondary vertex tagging as only discriminator.
In order to proceed, a truth-level flavour is associated to the reconstructed jets with the following steps: first, the truth-level jets are matched with the quarks from Monte Carlo to determine its flavour, requiring a distance $\Delta R<0.5$ between the truth-level jet axis and quark momentum. If more than one truth-level jet is found to match with the same quark, the one with the lowest $\Delta R$ is chosen. Then, the flavour of the reconstructed jets is determined by matching them with the truth-level jets, by requiring a $\Delta R<0.5$. If the reconstructed jet does not match any truth-level jet, it is labelled as fake.

The characteristics of secondary vertices inside reconstructed jets have been studied in order to reduce the mis-identification of $c$, light and fake jets.
Figure~\ref{fig:tau} shows the distribution of the secondary vertices proper lifetime ($\tau$) for $b$-jets, $c$-jets and light+fake jets. A cut on $\tau>0.2$~ns rejects $\sim 30 \%$ of both $c$- and light+fake jets, while retaining 90\% of $b$-jets. 
A reconstructed jet is tagged as $b$-jet, if at least one secondary vertex with $\tau>$ 0.2~ns is found inside its cone ($\Delta R <$ 0.5).

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/heavy_flavour/tau.pdf}
\caption{Distribution of the secondary vertex proper lifetime for $b$, $c$ and light-tagged jets. Distributions are normalised to the unit area.}
\label{fig:tau}
\end{figure}

The $b$-tagging efficiency is defined as the ratio between the number of tagged truth-matched $b$-jets and the total number of $b$-jets from the MC truth.
Analogously, the mistag of $c$- and light+fake jets is calculated as the ratio between the number of tagged truth-matched $c$ (light+fake)-jets and the total number of $c$ (light+fake)-jets from the MC truth.

The effect of the Double Layer filter on the secondary vertex finding efficiency has been evaluated reconstructing $b\bar{b}$, $c\bar{c}$ and $q\bar{q}$ dijet samples without BIB, with and without the Double Layer filter. A correction for the impact of the Double Layer filter has been computed as a function of the jet $p_{\rm T}$ and $\theta$ as the ratio of the number of tagged jets without any double layer filtering and the number of tagged jets passing the double layer filter. The final tagging efficiencies are then corrected for this ratio, assuming that its value does not change in the presence of the BIB.
The $b$-tagging efficiency is shown as a function of the jet $p_{\rm T}$ in Figure~\ref{fig:b_eff_pt} and and as a function of the jet $\theta$ in Figure~\ref{fig:b_eff_theta}.

\begin{figure}[b]
\centering
\includegraphics[width=0.5\textwidth]{figures/heavy_flavour/b_tagging.pdf}
\caption{Efficiency of the $b$-tagging algorithm as a function of the jet $p_{\rm T}$. The efficiency was evaluated in $b \bar{b}$ dijet events in $\mu^+\mu^-$ collisions at $\sqrt{s}=3$~TeV.}
\label{fig:b_eff_pt}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{figures/heavy_flavour/b_tagging_theta.pdf}
\caption{Efficiency of the $b$-tagging algorithm as a function of the jet $\theta$. The efficiency was evaluated in $b \bar{b}$ dijet events in $\mu^+\mu^-$ collisions at $\sqrt{s}=3$~TeV.}
\label{fig:b_eff_theta}
\end{figure}

The efficiency is around 50\% at low $p_{\rm T}$ and increases up to 70\% at high $p_{\rm T}$.

The mis-tagging rate for $c$-jets is shown in Figures~\ref{fig:c_eff_pt} and~\ref{fig:c_eff_theta} and is found to be around 20\%. As for $b$-jet efficiency, the $c$ mistag increases in the central region of the detector.

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{figures/heavy_flavour/c_misidentification.pdf}
\caption{Misidentification rate for $c$-jets as a function of the jet $p_{\rm T}$. The rate was evaluated in $c \bar{c}$ dijet events in $\mu^+\mu^-$ collisions at $\sqrt{s}=3$~TeV.}
\label{fig:c_eff_pt}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{figures/heavy_flavour/c_misidentification_theta.pdf}
\caption{Misidentification rate for $c$-jets as a function of the jet $\theta$. The rate was evaluated in $c \bar{c}$ dijet events in $\mu^+\mu^-$ collisions at $\sqrt{s}=3$~TeV.}
\label{fig:c_eff_theta}
\end{figure}

Figures~\ref{fig:light_eff_pt} and~\ref{fig:light_eff_theta} show the mis-tagging rates for the light and fake jets as a function of the jet $p_{\rm T}$ and $\theta$. This rate is found to be lower than $1\%$ for a jet $p_{\rm T}$ below 50 GeV and increase to $5\%$ at higher jet $p_{\rm T}$.

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{figures/heavy_flavour/light_misidentification.pdf}
\caption{Misidentification rate for light jets as a function of the jet $p_{\rm T}$. The rate was evaluated in $q \bar{q}$ dijet events in $\mu^+\mu^-$ collisions at $\sqrt{s}=3$~TeV.}
\label{fig:light_eff_pt}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{figures/heavy_flavour/light_misidentification_theta.pdf}
\caption{Misidentification rate for light jets as a function of the jet $\theta$. The rate was evaluated in $q \bar{q}$ dijet events in $\mu^+\mu^-$ collisions at $\sqrt{s}=3$~TeV.}
\label{fig:light_eff_theta}
\end{figure}

An initial $b$-jet identification algorithm based exclusively on the identification of secondary vertices has been put in place, demonstrating the effectiveness of one of the basic components used in flavour tagging techniques in the complex muon collider environment. 
Further work is ongoing to take advantage of improved tracking methods, such as the CKF algorithm described in Section~\ref{sec:trk-ckf} to improve the inputs to secondary vertex finding, or on the exploitation of additional handles, such as the presence of charged leptons within the jet, or the impact parameter of the track associated to the jet.

\subsubsection*{Photons and electrons}

The photon reconstruction and identification performance of the muon collider detector is assessed in a sample of single photon events. The photons were generated in the nominal collision vertex at the centre of the detector, uniformly distributed in energy between 1 and 1500~GeV, in polar angle between $10^\circ$ and $170^\circ$, and in the full azimuthal angle range. The sample was then processed with the detector simulation and reconstruction software.

Prior to track reconstruction, the tracker hits were processed with the Double Layer filter. Moreover, to get rid of most of the fake tracks due to the spurious hits from the background, a track quality selection is applied before the track refitting step, which requires at least three hits in the vertex detector and at least two hits in the inner tracker.
To reject part of the background hits in the calorimeters, an energy threshold of 2~MeV is applied to both the ECAL and HCAL hits. Photons are reconstructed and identified with the Pandora Particle Flow algorithm~\cite{Thomson:2009rp}.

The energy threshold of the calorimeter hits and the presence of the beam-induced background affect the energy scale of the reconstructed photons.
A correction factor is applied to the reconstructed photon energy to make the detector response uniform as a function of the photon energy and the photon polar angle.
The correction was calculated from the ratio of the reconstructed photon energy with the photon energy at generator level in an independent set of events.

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{figures/photons/photon_eff_ene.pdf}
\caption{Photon reconstruction efficiency as a function of the photon energy.
\label{fig:ph_efficiency}}
\end{figure}

Figure~\ref{fig:ph_efficiency} shows a comparison of the photon reconstruction efficiency as a function of the generated photon energy, with and without the BIB overlay. The dependency on the  polar angle $\theta$ is shown instead in Figure~\ref{fig:ph_efficiency_theta}.

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{figures/photons/photon_eff_theta.pdf}
\caption{Photon reconstruction efficiency as a function of the photon  polar angle $\theta$.
\label{fig:ph_efficiency_theta}}
\end{figure}

The efficiencies are defined as the fraction of generated photons in the range $10^\circ$ and $170^\circ$ that are matched to a reconstructed photon within $\Delta R<0.05$.
A decrease of about 10\% in the reconstruction efficiency in the presence of BIB is observed in the angular region corresponding to the transition between the barrel and endcap calorimeters, and is reflected in the efficiency below \qty{400}{\GeV}. 

The effect of BIB on the photon energy resolution has also been evaluated, and is shown in Figures~\ref{fig:ph_eneres} and~\ref{fig:ph_eneres_theta} as a function of the energy and polar angle of the photon. The BIB was found to affect more significantly the forward region, where the energy resolution is degraded by a factor of two, and the transition region between the barrel and the endcap calorimeters.

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{figures/photons/photon_res_ene.pdf}
\caption{Energy resolution of the reconstructed photon as a function of the photon energy.
\label{fig:ph_eneres}}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{figures/photons/photon_res_theta.pdf}
\caption{Energy resolution of the reconstructed photon as a function of the photon polar angle $\theta$.
\label{fig:ph_eneres_theta}}
\end{figure}

The development of a dedicated algorithm to recover the effects on both the reconstruction efficiency and energy resolution is ongoing. However, the results demonstrate an excellent level of expected performance across the full investigated energy spectrum.

Figure~\ref{fig:ph_elefake} reports the fraction of photons that are reconstructed and identified as electrons. The resulting inefficiency from misidentifications was found to be at the level of 0.3\% and relatively unaffected by the presence of BIB.

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{figures/photons/photon_misID_ene.pdf}
\caption{Fraction of photons misidentified as electrons as a function of the photon energy. 
\label{fig:ph_elefake}}
\end{figure}    

The performance of electron reconstruction and identification was studied in single electron events, with the electrons produced at the nominal collision point. The generated electrons are uniformly distributed in energy between 1 and 1500~GeV, in polar angle between $10^\circ$ and $170^\circ$, and in azimuthal angle over the whole range. The sample was then processed with the detector simulation and reconstruction software.

Electrons are identified by means of an angular matching of the electromagnetic clusters with tracks reconstructed with the CKF algorithm, as described in Section~\ref{sec:trk-ckf} in a $R=0.1$ cone.
A Double Layer filter was used to reject BIB hits upstream of the track reconstruction and tracks are required to have $\chi^2/\textrm{ndof} < 10$.
In the presence of the beam-induced background, the energy thresholds of the calorimeter hits play a dominant role for an efficient and precise cluster reconstruction. In this study, a threshold of 5~MeV was set. 

The electron reconstruction and identification efficiencies as a function of the electron generated energy and polar angle are shown in Figures~\ref{fig:ele_efficiency} and~\ref{fig:ele_efficiency_theta}.

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/electrons/electron_eff_ene.pdf}
\caption{Comparison of the electron reconstruction efficiency as a function of the electron energy in the cases of no beam-induced background and with the BIB added to the event.
\label{fig:ele_efficiency}}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/electrons/electron_eff_theta.pdf}
\caption{Comparison of the electron reconstruction efficiency as a function of the electron polar angle in the cases of no beam-induced background and with the BIB added to the event.
\label{fig:ele_efficiency_theta}}
\end{figure}

The efficiency drops at $\theta < 20^\circ$ and $\sim40^\circ$ are caused by tracking inefficiencies related to the application of the Double-Layer filter in the forward region and in the transition region between the VXD barrel and endcap. 

An excellent performance is observed across the investigated energy range. New developments and more sophisticated algorithms will be required to further improve the cluster reconstruction and cluster-to-track association, particularly in the lower energy regime.

\FloatBarrier
\subsubsection*{Muons}

The performance of muon reconstruction and identification was studied in single muon events. The muons were produced at the nominal collision point. The generated muons are uniformly distributed in energy in the range \qty{100}{\MeV}--\qty{50}{\GeV}, in polar angle between $8^\circ$ and $172^\circ$, and in azimuthal angle over the whole range.

The muons are reconstructed and identified with the Pandora Particle Flow algorithm~\cite{Thomson:2009rp}, by matching track in the inner detector reconstructed from the Combinatorial Kalman Filter approach with clusters of hits in the muon system. A cluster is defined as a combination of hits (one hit per layer) inside a cone extending to the neighbouring layers. A detailed description of the muon reconstruction algorithm is reported in~\cite{Linssen:2012hp}.

The cluster finding efficiency, defined as the ratio between generated particles associated with a cluster and total generated particles, was found to be higher than 99\% for $p_{\rm T}>\qty{10}{\GeV}$ and higher than 98\% for $\qty{8}{\degree} < \theta < \qty{172}{\degree}$.

Figures~\ref{fig:muon_efficiency} and~\ref{fig:muon_thetaefficiency} show the muon reconstruction efficiency respectively as a function of the muon $p_{\rm T}$ polar angle $\theta$.

\begin{figure}[t]
\center
\includegraphics[width=0.47\textwidth]{figures/muons/efficiency_pt.pdf}
\caption{Muon reconstruction efficiency as a function of transverse momentum in a sample of single muon events.}
\label{fig:muon_efficiency}
\end{figure}

\begin{figure}[t]
\center
\includegraphics[width=0.47\textwidth]{figures/muons/efficiency_theta.pdf}
\caption{Muon reconstruction efficiency as a function of the polar angle $\theta$ in a sample of single muon events.}
\label{fig:muon_thetaefficiency}
\end{figure}

The muon $p_{\rm T}$ resolution is shown in Figure~\ref{fig:muon_ptresolution} where $\Delta p_{\rm T}$ is the difference between the generated muon $p_{\rm T}$ and the $p_{\rm T}$ of the corresponding reconstructed particle. It results to be less than $10^{-4}$\,GeV$^{-1}$ for $p_{\rm T}>30$\,GeV and around a factor of 7 better in the barrel region compare to the endcap. 

\begin{figure}[t]
\center
\includegraphics[width=0.5\textwidth]{figures/muons/resolution.pdf}
 \caption{Muon track transverse momentum resolution as a function of the muon transverse momentum in a sample of single muon events.}
\label{fig:muon_ptresolution}
\end{figure}

The BIB was found not to strongly affect the muon reconstruction performance: the efficiency is lower only in the endcaps where all the BIB hits are concentrated, and the $p_{\rm T}$ resolution is just a few percent worse. Additional work is planned to improve the reconstruction efficiency for low transverse momenta. 

\FloatBarrier
\subsection{Forward detectors}
\label{sec:fwdandlumi}

Dedicated detectors in the forward region are required to fully harvest the physics potential of a muon collider. 

The ability to tag, or reconstruct muons down to $\theta \sim 0.1$ would provide a unique handle to distinguish events with a collinear emission of virtual $Z$ boson from the muon. This is a unique feature that can only be exploited at muon colliders, which would enable VBS and VBF measurements, as well as searches for new physics phenomena. Consequently, the design of a forward muon tagging system is considered to be of high-priority.
The low-interacting nature of muons, which can travel through hundreds of meters of material without being stopped, would allow them to traverse the shielding nozzles making this detector concept possible.
A detailed study of the expected muon trajectories as they propagate through the shielding material, final focusing magnets and other collider elements, as well as of the expected BIB rates in this region, still needs to be performed.
However, the outgoing muons are expected to have very large momenta and to be confined in a region relatively close to the beam axis.
The detector design and technology change depending on whether a simple muon tagging is required, a measurement of the muon momenta is also needed. Such measurement could be obtained with the measurement of the outgoing muon position, and scattering angle at the IP, in a high-precision forward tracking station. Alternative designs exploiting a dipole-based muon spectrometer, such as those considered for the FCC-hh detectors~\cite{FCC:2018vvp}, will be investigated, but would require a significantly larger detector volume.

A precise measurement of the collider luminosity could also be obtained using forward muons. The determination of the luminosity is of crucial or high importance for any absolute cross section measurement since it directly translates to its error. LHC experiments have dedicated detectors, so-called luminometers that are used in combination with the van der Meer scan method~\cite{vanderMeer:296752,ALICE:2022xir,ATLAS:2022hro,CMS:2021xjt,LHCb:2014vhh} to precisely measure the instantaneous luminosity. The $e^+e^-$ experiments like Belle-II~\cite{Belle-II:2019usr} and BESIII~\cite{BESIII:2015qfd} measure the integrated luminosity by counting the number of events of a process whose cross section is theoretically known with high precision. The most used one is the Bhabha scattering ($e^+e^- \rightarrow e^+e^-$) where, for example, the theoretical uncertainty on $\sigma$ at  $\sqrt{s}=1-10$ GeV is 0.1\% at large angle~\cite{CarloniCalame:2011aa}.
Due to the reduced acceptance in the forward region because of the nozzles shielding structure at muon collider, only the large angle muon Bhabha ($\mu^+$ $\mu^-$ $\rightarrow$ $\mu^+$ $\mu^-$) has been considered so far as a method for the luminosity~\cite{Giraldin:2021gxz}. 
By assuming an instantaneous luminosity $\mathcal{L} = 1.25 \times 10^{34} $ cm$^{-2}$s$^{-1}$ and considering a year of data taking ($10^7$ s) the statistical uncertainty obtained at $\sqrt{s}=1.5$~TeV is 0.2\%. The expected total uncertainty on the luminosity measurement using this method strongly depends on the accuracy on the theoretical cross section of $\mu$-Bhabha scattering at large angles, and at several TeV centre of mass energy. The addition of a forward muon detector could significantly improve these constraints.

The second major opportunity in the forward region arises from the decay of the high-energy muons, which produces a collimated beam of high-energy neutrinos very close to the muon beam axis that could enable precise measurements of neutrino cross-sections and dark sector studies. 
Such programme represents the ideal extension of the investigations carried out by dedicated LHC detectors such as FASER~\cite{FASER:2019dxq} and SND@LHC~\cite{SNDLHC:2022ihg}. The forward neutrinos at the LHC come from the decay of hadrons and they are complex to simulate, while the neutrino flux at the muon collider will be precisely known. 
The expected muon decay rates reported in Table~\ref{tab:BIB_com_energy} can be used to estimate the rate of neutrino interactions in a dedicated detector placed one kilometer away from the IP. At such distance, the neutrino beam would have a transverse size of the order of ten centimeters. Depending on the detector size, technology and material, up to several $10^{19}$ neutrino interactions per year can be observed.
The design and optimisation of such a detector, as well as a full characterisation of its physics potential, still needs to be performed.

\FloatBarrier
\subsection{Conclusions}
\label{sec:outlook}

Muon colliders can combine excellent discovery potential with high precision capabilities. For the purpose of event detection and reconstruction, the challenge that separates $\mu^{+}\mu^{-}$ with the $e^{+}e^{-}$ counterparts is the beam induced background. Because muons are unstable, they decay in flight, producing electrons that further interact with the accelerator and detector components. This creates very large multiplicities of mostly soft secondary particles, some of which end up in the detector. 
The hits produced by the secondary particles in the detectors lead to significant challenges for the particle detection and reconstruction. In this section, preliminary design and specifications for a muon collider detector were described. An assessment of the expected radiation levels was presented, showing that the radiation levels are similar to those at the HL-LHC experiments.
Possible methods to mitigate effects of the BIB were outlined.  The BIB imposes stringent requirements on the granularity, resolution, and timing properties of the muon collider detectors, and presented a number of emerging detector technologies that have a potential to address the challenge. Several R\&D efforts to further develop these technologies are needed to get them to the maturity level necessary for the detector construction, and a few of these were highlighted throughout the section.

The current performance of a detector design was also presented, including appropriate shielding near the interaction point. The expected performance for reconstructing charged particles, jets, electrons, muons and photons has been presented, in addition to preliminary results on tagging heavy-flavour jets and measuring the delivered integrated luminosity. The results demonstrate that it is possible to successfully cope with the expected BIB and reconstruct with high accuracy the main physics observables needed for carrying out the expected physics programme.

\end{document}
