@inproceedings{amaruEPFLCombinationalBenchmark2015a,
  title = {The {{EPFL Combinational Benchmark Suite}}},
  booktitle = {Proceedings of the 24th {{International Workshop}} on {{Logic}} \& {{Synthesis}}},
  author = {Amar{\`u}, Luca and Gaillardon, Pierre-Emmanuel and De Micheli, Giovanni},
  year = {2015},
  @address = {Mountain View, CA},
  abstract = {In this paper, we present the EPFL combinational benchmark suite. We aim at completing existing benchmark suites by focusing only on \&lt;i\&gt;natively\&lt;/i\&gt; combinational benchmarks. The EPFL combinational benchmark suite consists of 23 combinational circuits designed to challenge modern logic optimization tools. It is further divided into three parts. The first part includes 10 arithmetic benchmarks, e.g., square-root, hypotenuse, divisor, multiplier etc.. The second part consists of 10 random/control benchmarks, e.g., round-robin arbiter, lookahead XY router, alu control unit, memory controller etc.. The third part contains 3 very large circuits, featuring more than ten million gates each. All benchmarks have a moderate number of inputs/outputs ranging from few tens to about one thousand. The EPFL benchmark suite is available to the public and distributed in all Verilog, VHDL, BLIF and AIGER formats. In addition to providing the benchmarks, we keep track of the best optimization results, mapped into LUT-6, for size and depth metrics. Better logic implementations can be submitted online. After combinational equivalence checking tests, the best LUT-6 realizations will be included in the benchmark suite together with the author's name and affiliation.}
}

@inproceedings{azizExistsSATProjected2015,
  title = {\#{$\exists$}{{SAT}}: {{Projected Model Counting}}},  shorttitle = {\$\${\textbackslash}\#{\textbackslash}exists \$\${{SAT}}},
  booktitle = {Theory and {{Applications}} of {{Satisfiability Testing}} -- {{SAT}} 2015},
  author = {Aziz, Rehan Abdul and Chu, Geoffrey and Muise, Christian and Stuckey, Peter},
  @editor = {Heule, Marijn and Weaver, Sean},
  year = {2015},
  pages = {121--137},
  publisher = {Springer International Publishing},
  @address = {Cham},
  doi = {10.1007/978-3-319-24318-4_10},
  abstract = {Model counting is the task of computing the number of assignments to variables \$\${\textbackslash}mathcal\{V\}\$\$that satisfy a given propositional theory F. The model counting problem is denoted as \#SAT. Model counting is an essential tool in probabilistic reasoning. In this paper, we introduce the problem of model counting projected on a subset of original variables that we call priority variables \$\${\textbackslash}mathcal\{P\}{\textbackslash}subseteq {\textbackslash}mathcal\{V\}\$\$. The task is to compute the number of assignments to \$\${\textbackslash}mathcal\{P\}\$\$such that there exists an extension to non-priority variables \$\${\textbackslash}mathcal\{V\}{\textbackslash}setminus {\textbackslash}mathcal\{P\}\$\$that satisfies F. We denote this as \$\${\textbackslash}\#{\textbackslash}exists \$\$SAT. Projected model counting arises when some parts of the model are irrelevant to the counts, in particular when we require additional variables to model the problem we are counting in SAT. We discuss three different approaches to \$\${\textbackslash}\#{\textbackslash}exists \$\$SAT (two of which are novel), and compare their performance on different benchmark problems.},
  isbn = {978-3-319-24318-4},
  langid = {english}
}

@book{baralKnowledgeRepresentationReasoning2003,
  title = {Knowledge {{Representation}}, {{Reasoning}} and {{Declarative Problem Solving}}},
  author = {Baral, Chitta},
  year = {2003},
  month = jan,
  publisher = {Cambridge University Press},
  abstract = {Knowledge management and knowledge-based intelligence are areas of importance in the economy and society, and to exploit them fully and efficiently it is necessary both to represent and reason about knowledge via a declarative interface whose input language is based on logic. In this book, originally published in 2003, Chitta Baral shows exactly how to go about doing that: how to write programs that behave intelligently by giving them the ability to express knowledge and reason about it. He presents a language, AnsProlog, for both knowledge representation and reasoning, and declarative problem solving. The results have been organised here into a form that will appeal to practising and would-be knowledge engineers wishing to learn more about the subject, either in courses or through self-teaching. A comprehensive bibliography rounds off the book.},
  googlebooks = {iTS4ZdEpGZQC},
  isbn = {978-1-139-43644-1},
  langid = {english}
}

@incollection{barrettSatisfiabilityModuloTheories2021,
  title = {Satisfiability {{Modulo Theories}}},
  booktitle = {Handbook of {{Satisfiability}}},
  author = {Barrett, Clark and Sebastiani, Roberto and Seshia, Sanjit A. and Tinelli, Cesare},
  @editor = {Biere, Armin and Heule, Marijn and van Maaren, Hans and Walsh, Toby},
  year = {2021},
  series = {Frontiers in {{Artificial Intelligence}} and {{Applications}}},
  edition = {2},
  volume = {336},
  pages = {1267--1329},
  publisher = {IOS Press},
  doi = {10.3233/FAIA201017},
  isbn = {978-1-64368-160-3 978-1-64368-161-0}
}

@inproceedings{bayardoUsingCSPLookback1997,
  title = {Using {{CSP}} Look-Back Techniques to Solve Real-World {{SAT}} Instances},
  booktitle = {Proceedings of the 14th {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Bayardo, Roberto J. and Schrag, Robert C.},
  year = {1997},
  month = jul,
  series = {{{AAAI}}'97},
  pages = {203--208},
  publisher = {AAAI Press},
  @address = {Providence, Rhode Island},
  urldate = {2024-01-10},
  abstract = {We report on the performance of an enhanced version of the "Davis-Putnam" (DP) proof procedure for propositional satisfiability (SAT) on large instances derived from real-world problems in planning, scheduling, and circuit diagnosis and synthesis. Our results show that incorporating CSP look-back techniques -- especially the relatively new technique of relevance-bounded learning -- renders easy many problems which otherwise are beyond DP's reach. Frequently they make DP, a systematic algorithm, perform as well or better than stochastic SAT algorithms such as GSAT or WSAT. We recommend that such techniques be included as options in implementations of DP, Just as they are in systematic algorithms for the more general constraint satisfaction problem.},
  isbn = {978-0-262-51095-0}
}

@inproceedings{belleProbabilisticInferenceHybrid2015,
  title = {Probabilistic {{Inference}} in {{Hybrid Domains}} by {{Weighted Model Integration}}},
  booktitle = {24th {{International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Belle, Vaishak and Passerini, Andrea and den Broeck, Guy Van},
  year = {2015},
  month = jun,
  pages = {2770--2776},
  publisher = {AAAI Press},
  @address = {Buenos Aires, Argentina},
  urldate = {2022-12-19},
  abstract = {Weighted model counting (WMC) on a propositional knowledge base is an effective and general approach to probabilistic inference in a variety of formalisms, including Bayesian and Markov Networks. However, an inherent limitation of WMC is that it only admits the inference of discrete probability distributions. In this paper, we introduce a strict generalization of WMC called weighted model integration that is based on annotating Boolean and arithmetic constraints, and combinations thereof. This methodology is shown to capture discrete, continuous and hybrid Markov networks. We then consider the task of parameter learning for a fragment of the language. An empirical evaluation demonstrates the applicability and promise of the proposal.},
  isbn = {978-1-57735-738-4},
  langid = {english},
  keywords = {conference,selected,strong}
}

@article{bernasconiCompactDSOPPartial2013,
  title = {Compact {{DSOP}} and {{Partial DSOP Forms}}},
  author = {Bernasconi, Anna and Ciriani, Valentina and Luccio, Fabrizio and Pagli, Linda},
  year = {2013},
  month = nov,
  journal = {Theory of Computing Systems},
  volume = {53},
  number = {4},
  pages = {583--608},
  issn = {1433-0490},
  doi = {10.1007/s00224-013-9447-2},
  urldate = {2023-07-19},
  abstract = {Given a Boolean function f on n variables, a Disjoint Sum-of-Products (DSOP) of f is a set of products (ANDs) of subsets of literals whose sum (OR) equals~f, such that no two products cover the same minterm of~f. DSOP forms are a special instance of partial DSOPs, i.e. the general case where a subset of minterms must be covered exactly once and the other minterms (typically corresponding to don't care conditions of~f) can be covered any number of times. We discuss finding DSOPs and partial DSOPs with a minimal number of products, a problem theoretically connected with various properties of Boolean functions and practically relevant in the synthesis of digital circuits. Finding an absolute minimum is hard, in fact we prove that the problem of absolute minimization of partial DSOPs is NP-hard. Therefore it is crucial to devise a polynomial time heuristic that compares favorably with the known minimization tools. To this end we develop a further piece of theory starting from the definition of the weight of a cube~c as a functions of the number of fragments induced on other cubes by the selection of~c, and show how cube weights can be exploited for building a class of minimization heuristics for DSOP and partial DSOP synthesis. A~set of experiments conducted on major benchmark functions show that our method, with a family of variants, always generates better results than the ones of previous heuristics, including the method based on a BDD representation of~f.},
  langid = {english}
}

@article{biereAIGERAndInverterGraph2007a,
  title = {The {{AIGER And-Inverter Graph}} ({{AIG}}) {{Format Version}} 20071012},
  author = {Biere, Armin},
  year = {2007},
  abstract = {This report describes the AIG file format as used by the AIGER library. The purpose of this report is not only to motivate and document the format, but also to allow independent implementations of writers and readers by giving precise and unambiguous definitions.},
  langid = {english},
  note = {Publisher: Institut for Formal Models and Verification, Johannes Kepler University}
}

@book{biereHandbookSatisfiability2021,
  title = {Handbook of {{Satisfiability}}},
  author = {Biere, A. and Heule, M. and van Maaren, H.},
  year = {2021},
  series = {Frontiers in {{Artificial Intelligence}} and {{Applications}}},
  edition = {2},
  volume = {336},
  publisher = {IOS Press},
  doi = {10.3233/FAIA336},
  abstract = {``Satisfiability (SAT) related topics have attracted researchers from various disciplines: logic, applied areas such as planning, scheduling, operations research and combinatorial optimization, but also theoretical issues on the theme of complexity and much more, they all are connected through SAT. My personal interest in SAT stems from actual solving: The increase in power of modern SAT solvers over the past 15 years has been phenomenal. It has become the key enabling technology in automated verification of both computer hardware and software. Bounded Model Checking (BMC) of computer hardware is now probably the most widely used model checking technique. The counterexamples that it finds are just satisfying instances of a Boolean formula obtained by unwinding to some fixed depth a sequential circuit and its specification in linear temporal logic. Extending model checking to software verification is a much more difficult problem on the frontier of current research. One promising approach for languages like C with finite word-length integers is to use the same idea as in BMC but with a decision procedure for the theory of bit-vectors instead of SAT. All decision procedures for bit-vectors that I am familiar with ultimately make use of a fast SAT solver to handle complex formulas. Decision procedures for more complicated theories, like linear real and integer arithmetic, are also used in program verification. Most of them use powerful SAT solvers in an essential way. Clearly, efficient SAT solving is a key technology for 21st century computer science. I expect this collection of papers on all theoretical and practical aspects of SAT solving will be extremely useful to both students and researchers and will lead to many further advances in the field.''--Edmund Clarke (FORE Systems University Professor of Computer Science and Professor of Electrical and Computer Engineering at Carnegie Mellon University, winner of the 2007 A.M. Turing Award)},
  isbn = {978-1-60750-376-7},
  langid = {english},
  keywords = {Computers / Computer Science}
}

@article{bierePicoSATEssentials2008,
  title = {{{PicoSAT Essentials}}},
  author = {Biere, Armin},
  @editor = {Speckenmeyer, Ewald and Li, Chu Min and Manquinho, Vasco and Tacchella, Armando},
  year = {2008},
  month = may,
  journal = {Journal on Satisfiability, Boolean Modeling and Computation},
  volume = {4},
  number = {2-4},
  pages = {75--97},
  issn = {15740617},
  doi = {10.3233/SAT190039},
  urldate = {2023-07-13},
  abstract = {In this article we describe and evaluate optimized compact data structures for watching literals. Experiments with our SAT solver PicoSAT show that this low-level optimization not only saves memory, but also turns out to speed up the SAT solver considerably. We also discuss how to store proof traces compactly in memory and further unique features of PicoSAT including an aggressive restart schedule.},
  langid = {english}
}

@article{birnbaumGoodOldDavisPutnam1999,
  title = {The {{Good Old Davis-Putnam Procedure Helps Counting Models}}},
  author = {Birnbaum, E. and Lozinskii, E. L.},
  year = {1999},
  month = jun,
  journal = {Journal of Artificial Intelligence Research},
  volume = {10},
  pages = {457--477},
  issn = {1076-9757},
  doi = {10.1613/jair.601},
  urldate = {2024-07-22},
  abstract = {As was shown recently, many important AI problems require    counting the number of models of propositional formulas. The problem    of counting models of such formulas is, according to present    knowledge, computationally intractable in a worst case. Based on the    Davis-Putnam procedure, we present an algorithm, CDP, that computes    the exact number of models of a propositional CNF or DNF formula    F. Let m and n be the number of clauses and variables of F,    respectively, and let p denote the probability that a literal l of F    occurs in a clause C of F, then the average running time of CDP is    shown to be O(nm{\textasciicircum}d), where d=-1/log(1-p).  The practical    performance of CDP has been estimated in a series of experiments on a    wide variety of CNF formulas.},
  copyright = {Copyright (c)},
  langid = {english}
}

@article{bjorkSuccessfulSATEncoding2009,
  title = {Successful {{SAT Encoding Techniques}}},
  author = {Bj{\"o}rk, Magnus},
  year = {2009},
  month = jul,
  journal = {Journal on Satisfiability, Boolean Modeling and Computation},
  volume = {7},
  number = {4},
  pages = {189--201},
  issn = {15740617},
  doi = {10.3233/SAT190085},
  urldate = {2023-05-03},
  abstract = {This article identifies good practices for SAT encodings by analysing interviews with a number of well known SAT experts. The purpose is both to determine the confidence in different encoding strategies, by analysing whether there is consensus among the experts or not, as well as bringing out hidden knowledge to SAT users.},
  langid = {english}
}

@inproceedings{boudaneSATbasedApproachMining2016,
  title = {A {{SAT-based Approach}} for {{Mining Association Rules}}},
  booktitle = {25th {{International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Boudane, Abdelhamid and Jabbour, Said and Sais, Lakhdar and Salhi, Yakoub},
  year = {2016},
  month = jul,
  pages = {2472--2478},
  publisher = {AAAI Press},
  @address = {New York, New York, USA},
  urldate = {2023-07-18},
  abstract = {Discovering association rules from transaction databases is one of the most studied data mining task. Many effective techniques have been proposed over the years. All these algorithms share the same two steps methodology: frequent itemsets enumeration followed by effective association rules generation step. In this paper, we propose a new propositional satisfiability based approach to mine association rules in a single step. The task is modeled as a Boolean formula whose models correspond to the rules to be mined. To highlight the flexibility of our proposed framework, we also address two other variants, namely the closed and indirect association rules mining tasks. Experiments on many datasets show that on both closed and indirect association rules mining tasks, our declarative approach achieves better performance than the state-of-the-art specialized techniques.},
  isbn = {978-1-57735-770-4}
}

@article{boydelatourOptimalityResultClause1992,
  title = {An {{Optimality Result}} for {{Clause Form Translation}}},
  author = {{Boy de la Tour}, Thierry},
  year = {1992},
  month = oct,
  journal = {Journal of Symbolic Computation},
  volume = {14},
  number = {4},
  pages = {283--301},
  issn = {0747-7171},
  doi = {10.1016/0747-7171(92)90009-S},
  urldate = {2023-07-21},
  abstract = {The exponential complexity in size of the standard clause form translation is often considered as a serious drawback of the resolution method. Fortunately, a polynomial translation is possible by first introducing definitions, one for each subformula of the conjecture. This exhaustiveness can however be proved inefficient when the length of proofs is considered. In order to improve this interesting technique, we first generalize it to renamings, which consist in introducing definitions only for a subset of subformulas, resulting in a wide set of possible clause forms from a single conjecture. We show how a simple and efficient algorithm yields a renaming which, on equivalence-free conjectures, minimizes the number of clauses among these clause forms. This translation has been tested on the famous challenge problem by P. Andrews, yielding a spectacular reduction in search space and time, and therefore is one of the more simple and general technique to efficiently produce a resolution proof for this problem.},
  langid = {english}
}

@inproceedings{brglezNeutralNetlist101985,
  title = {A {{Neutral Netlist}} of 10 {{Combinational Benchmark Circuits}} and a {{Target Translator}} in {{Fortran}}},
  booktitle = {Proceedings of {{IEEE International Symposium Circuits}} and {{Systems}} ({{ISCAS}} 85)},
  author = {Brglez, F. and Fujiwara, H.},
  year = {1985},
  pages = {677--692},
  publisher = {IEEE Press}
}

@inproceedings{bryantCOSMOSCompiledSimulator1987,
  title = {{{COSMOS}}: A Compiled Simulator for {{MOS}} Circuits},
  shorttitle = {{{COSMOS}}},
  booktitle = {Proceedings of the 24th {{ACM}}/{{IEEE Design Automation Conference}}},
  author = {Bryant, Randal E. and Beatty, D. and Brace, K. and Cho, K. and Sheffler, T.},
  year = {1987},
  month = oct,
  series = {{{DAC}} '87},
  pages = {9--16},
  publisher = {Association for Computing Machinery},
  @address = {New York, NY, USA},
  doi = {10.1145/37888.37890},
  urldate = {2023-07-26},
  abstract = {The COSMOS simulator provides fast and accurate switch-level modeling of MOS digital circuits. It attains high performance by preprocessing the transistor network into a functionally equivalent Boolean representation. This description, produced by the symbolic analyzer ANAMOS, captures all aspects of switch-level networks including bidirectional transistors, stored charge, different signal strengths, and indeterminate (X) logic values. The LGCC program translates the Boolean representation into a set of machine language evaluation procedures and initialized data structures. These procedures and data structures are compiled along with code implementing the simulation kernel and user interface to produce the simulation program. The simulation program runs an order of magnitude faster than our previous simulator MOSSIM II.},
  isbn = {978-0-8186-0781-3}
}

@article{chistikovApproximateCountingSMT2017,
  title = {Approximate {{Counting}} in {{SMT}} and {{Value Estimation}} for {{Probabilistic Programs}}},
  author = {Chistikov, Dmitry and Dimitrova, Rayna and Majumdar, Rupak},
  year = {2017},
  month = dec,
  journal = {Acta Informatica},
  volume = {54},
  number = {8},
  pages = {729--764},
  issn = {1432-0525},
  doi = {10.1007/s00236-017-0297-2},
  urldate = {2023-08-24},
  abstract = {\#SMT, or model counting for logical theories, is a well-known hard problem that generalizes such tasks as counting the number of satisfying assignments to a Boolean formula and computing the volume of a polytope. In the realm of satisfiability modulo theories (SMT) there is a growing need for model counting solvers, coming from several application domains (quantitative information flow, static analysis of probabilistic programs). In this paper, we show a reduction from an approximate version of \#SMT~ to SMT. We focus on the theories of integer arithmetic and linear real arithmetic. We propose model counting algorithms that provide approximate solutions with formal bounds on the approximation error. They run in polynomial time and make a polynomial number of queries to the SMT solver for the underlying theory, exploiting ``for free'' the sophisticated heuristics implemented within modern SMT solvers. We have implemented the algorithms and used them to solve the value problem for a model of loop-free probabilistic programs with nondeterminism.},
  langid = {english}
}

@article{clarkePredicateAbstractionANSIC2004,
  title = {Predicate {{Abstraction}} of {{ANSI-C Programs Using SAT}}},
  author = {Clarke, Edmund and Kroening, Daniel and Sharygina, Natasha and Yorav, Karen},
  year = {2004},
  month = sep,
  journal = {Formal Methods in System Design},
  volume = {25},
  number = {2},
  pages = {105--127},
  issn = {1572-8102},
  doi = {10.1023/B:FORM.0000040025.89719.f3},
  urldate = {2023-07-18},
  abstract = {Predicate abstraction is a major method for verification of software. However, the generation of the abstract Boolean program from the set of predicates and the original program suffers from an exponential number of theorem prover calls as well as from soundness issues. This paper presents a novel technique that uses an efficient SAT solver for generating the abstract transition relations of ANSI-C programs. The SAT-based approach computes a more precise and safe abstraction compared to existing predicate abstraction techniques.},
  langid = {english}
}

@incollection{clarkNegationFailure1978,
  title = {Negation as {{Failure}}},
  booktitle = {Logic and {{Data Bases}}},
  author = {Clark, Keith L.},
  editor = {Gallaire, Herv{\'e} and Minker, Jack},
  year = {1978},
  pages = {293--322},
  publisher = {Springer US},
  @address = {Boston, MA},
  doi = {10.1007/978-1-4684-3384-5_11},
  urldate = {2024-07-25},
  abstract = {A query evaluation process for a logic data base comprising a set of clauses is described. It is essentially a Horn clause theorem prover augmented with a special inference rule for dealing with negation. This is the negation as failure inference rule whereby {\textasciitilde} P can be inferred if every possible proof of P fails. The chief advantage of the query evaluator described is the effeciency with which it can be implemented. Moreover, we show that the negation as failure rule only allows us to conclude negated facts that could be inferred from the axioms of the completed data base, a data base of relation definitions and equality schemas that we consider is implicitly given by the data base of clauses. We also show that when the clause data base and the queries satisfy certain constraints, which still leaves us with a data base more general than a conventional relational data base, the query evaluation process will find every answer that is a logical consequence of the completed data base.},
  isbn = {978-1-4684-3384-5},
  langid = {english}
}

@inproceedings{darwicheCompilerDeterministicDecomposable2002,
  title = {A {{Compiler}} for {{Deterministic}}, {{Decomposable Negation Normal Form}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Darwiche, Adnan},
  year = {2002},
  month = jul,
  pages = {627--634},
  publisher = {American Association for Artificial Intelligence},
  @address = {USA},
  urldate = {2024-07-24},
  abstract = {We present a compiler for converting CNF formulas into deterministic, decomposable negation normal form (d-DNNF). This is a logical form that has been identified recently and shown to support a number of operations in polynomial time, including clausal entailment; model counting, minimization and enumeration; and probabilistic equivalence testing, d-DNNFs are also known to be a superset of, and more succinct than, OBDDs. The polytime logical operations supported by d-DNNFs are a subset of those supported by OBDDs, yet are sufficient for model-based diagnosis and planning applications. We present experimental results on compiling a variety of CNF formulas, some generated randomly and others corresponding to digital circuits. A number of the formulas we were able to compile efficiently could not be similarly handled by some state-of-the-art model counters, nor by some state-of-the-art OBDD compilers.},
  isbn = {978-0-262-51129-2}
}

@article{darwicheKnowledgeCompilationMap2002,
  title = {A Knowledge Compilation Map},
  author = {Darwiche, Adnan and Marquis, Pierre},
  year = {2002},
  month = sep,
  journal = {Journal of Artificial Intelligence Research},
  volume = {17},
  number = {1},
  pages = {229--264},
  issn = {1076-9757},
  abstract = {We propose a perspective on knowledge compilation which calls for analyzing different compilation approaches according to two key dimensions: the succinctness of the target compilation language, and the class of queries and transformations that the language supports in polytime. We then provide a knowledge compilation map, which analyzes a large number of existing target compilation languages according to their succinctness and their polytime transformations and queries. We argue that such analysis is necessary for placing new compilation approaches within the context of existing ones. We also go beyond classical, flat target compilation languages based on CNF and DNF, and consider a richer, nested class based on directed acyclic graphs (such as OBDDs), which we show to include a relatively large number of target compilation languages.}
}

@inproceedings{dlalaComparativeStudySATBased2016,
  title = {A {{Comparative Study}} of {{SAT-Based Itemsets Mining}}},
  booktitle = {Research and {{Development}} in {{Intelligent Systems XXXIII}}},
  author = {Dlala, Imen Ouled and Jabbour, Said and Sais, Lakhdar and Yaghlane, Boutheina Ben},
  @editor = {Bramer, Max and Petridis, Miltos},
  year = {2016},
  pages = {37--52},
  publisher = {Springer International Publishing},
  @address = {Cham},
  doi = {10.1007/978-3-319-47175-4_3},
  abstract = {Mining frequent itemsets from transactional datasets is a well known problem. Thus, various methods have been studied to deal with this issue. Recently, original proposals have emerged from the cross-fertilization between data mining and artificial intelligence. In these declarative approaches, the itemset mining problem is modeled either as a constraint network or a propositional formula whose models correspond to the patterns of interest. In this paper, we focus on the propositional satisfiability based itemset mining framework. Our main goal is to enhance the efficiency of SAT model enumeration algorithms. This issue is particularly crucial for the scalability and competitiveness of such declarative itemset mining approaches. In this context, we deeply analyse the effect of the different SAT solver components on the efficiency of the model enumeration problem. Our analysis includes the main components of modern SAT solvers such as restarts, activity based variable ordering heuristics and clauses learning mechanism. Through extensive experiments, we show that these classical components play an essential role in such procedure to improve the performance by pushing forward the efficiency of SAT solvers. More precisely, our experimental evaluation includes a comparative study in enumerating all the models corresponding to the closed frequent itemsets. Additionally, our experimental analysis is extended to include the Top-k itemset mining problem.},
  isbn = {978-3-319-47175-4},
  langid = {english}
}

@inproceedings{friedEntailingGeneralizationBoosts2024,
  title = {Entailing {{Generalization Boosts Enumeration}}},
  booktitle = {27th {{International Conference}} on {{Theory}} and {{Applications}} of {{Satisfiability Testing}}},
  author = {Fried, Dror and Nadel, Alexander and Sebastiani, Roberto and Shalmon, Yogev},
  year = {2024},
  series = {Leibniz International Proceedings in Informatics ({{LIPIcs}})},
  volume = {305},
  pages = {13:1-13:14},
  publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
  doi = {10.4230/LIPIcs.SAT.2024.13},
  urldate = {2024-08-27},
  abstract = {Given a combinational circuit {$\Gamma$} with a single output o, AllSAT-CT is the problem of enumerating all solutions of {$\Gamma$}. Recently, we introduced several state-of-the-art AllSAT-CT algorithms based on satisfying generalization, which generalizes a given total Boolean solution to a smaller ternary solution that still satisfies the circuit. We implemented them in our open-source tool HALL. In this work we draw upon recent theoretical works suggesting that utilizing generalization algorithms, which can produce solutions that entail the circuit without satisfying it, may enhance enumeration. After considering the theory and adapting it to our needs, we enrich HALL's AllSAT-CT algorithms by incorporating several newly implemented generalization schemes and additional SAT solvers. By conducting extensive experiments we show that entailing generalization substantially boosts HALL's performance and quality (where quality corresponds to the number of reported generalized solutions per instance), with the best results achieved by combining satisfying and entailing generalization.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english}
}

@inproceedings{friedAllSATCombinationalCircuits2023,
  title = {{{AllSAT}} for {{Combinational Circuits}}},
  booktitle = {26th International {{Conference}} on {{Theory}} and {{Applications}} of {{Satisfiability Testing}}},
  author = {Fried, Dror and Nadel, Alexander and Shalmon, Yogev},
  @editor = {Mahajan, Meena and Slivovsky, Friedrich},
  year = {2023},
  series = {Leibniz International Proceedings in Informatics ({{LIPIcs}})},
  volume = {271},
  pages = {9:1--9:18},
  publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
  @address = {Dagstuhl, Germany},
  doi = {10.4230/LIPIcs.SAT.2023.9},
  isbn = {978-3-95977-286-0},
  keywords = {AllSAT,Circuits,SAT}
}

@inproceedings{garioPySMTSolveragnosticLibrary2015,
  title = {{{PySMT}}: A Solver-Agnostic Library for Fast Prototyping of {{SMT-based}} Algorithms},
  booktitle = {{{SMT Workshop}} 2015},
  author = {Gario, Marco and Micheli, Andrea},
  year = {2015}
}

@inproceedings{gebserConflictDrivenAnswerSet2007,
  title = {Conflict-{{Driven Answer Set Enumeration}}},
  booktitle = {Logic {{Programming}} and {{Nonmonotonic Reasoning}}},
  author = {Gebser, Martin and Kaufmann, Benjamin and Neumann, Andr{\'e} and Schaub, Torsten},
  @editor = {Baral, Chitta and Brewka, Gerhard and Schlipf, John},
  year = {2007},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {4483},
  pages = {136--148},
  publisher = {Springer Berlin Heidelberg},
  @address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-72200-7_13},
  urldate = {2023-07-13},
  abstract = {We elaborate upon a recently proposed approach to finding an answer set of a logic program based on concepts from constraint processing and satisfiability checking. We extend this approach and propose a new algorithm for enumerating answer sets. The algorithm, which to our knowledge is novel even in the context of satisfiability checking, is implemented in the clasp answer set solver. We contrast our new approach to alternative systems and different options of clasp, and provide an empirical evaluation.},
  isbn = {978-3-540-72200-7},
  langid = {english}
}

@inproceedings{gebserSolutionEnumerationProjected2009,
  title = {Solution {{Enumeration}} for {{Projected Boolean Search Problems}}},
  booktitle = {Integration of {{AI}} and {{OR Techniques}} in {{Constraint Programming}} for {{Combinatorial Optimization Problems}}},
  author = {Gebser, Martin and Kaufmann, Benjamin and Schaub, Torsten},
  @editor = {{van Hoeve}, Willem-Jan and Hooker, John N.},
  year = {2009},
  pages = {71--86},
  publisher = {Springer},
  @address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-01929-6_7},
  abstract = {Many real-world problems require the enumeration of all solutions of combinatorial search problems, even though this is often infeasible in practice. However, not always all parts of a solution are needed. We are thus interested in projecting solutions to a restricted vocabulary. Yet, the adaption of Boolean constraint solving algorithms turns out to be non-obvious provided one wants a repetition-free enumeration in polynomial space. We address this problem and propose a new algorithm computing projective solutions. Although we have implemented our approach in the context of Answer Set Programming, it is readily applicable to any solver based on modern Boolean constraint technology.},
  isbn = {978-3-642-01929-6},
  langid = {english}
}

@article{geComputingEstimatingVolume2018,
  title = {Computing and Estimating the Volume of the Solution Space of {{SMT}}({{LA}}) Constraints},
  author = {Ge, Cunjing and Ma, Feifei and Zhang, Peng and Zhang, Jian},
  year = {2018},
  month = sep,
  journal = {Theoretical Computer Science},
  volume = {743},
  pages = {110--129},
  issn = {03043975},
  doi = {10.1016/j.tcs.2016.10.019},
  urldate = {2023-08-24},
  langid = {english}
}

@article{giunchigliaAnswerSetProgramming2006,
  title = {Answer {{Set Programming Based}} on {{Propositional Satisfiability}}},
  author = {Giunchiglia, Enrico and Lierler, Yuliya and Maratea, Marco},
  year = {2006},
  month = apr,
  journal = {Journal of Automated Reasoning},
  volume = {36},
  number = {4},
  pages = {345--377},
  issn = {1573-0670},
  doi = {10.1007/s10817-006-9033-2},
  urldate = {2024-07-25},
  abstract = {Answer set programming (ASP) emerged in the late 1990s as a new logic programming paradigm that has been successfully applied in various application domains. Also motivated by the availability of efficient solvers for propositional satisfiability (SAT), various reductions from logic programs to SAT were introduced. All these reductions, however, are limited to a subclass of logic programs or introduce new variables or may produce exponentially bigger propositional formulas. In this paper, we present a SAT-based procedure, called ASPSAT, that (1) deals with any (nondisjunctive) logic program, (2) works on a propositional formula without additional variables (except for those possibly introduced by the clause form transformation), and (3) is guaranteed to work in polynomial space. From a theoretical perspective, we prove soundness and completeness of ASPSAT. From a practical perspective, we have (1) implemented ASPSAT in Cmodels, (2) extended the basic procedures in order to incorporate the most popular SAT reasoning strategies, and (3) conducted an extensive comparative analysis involving other state-of-the-art answer set solvers. The experimental analysis shows that our solver is competitive with the other solvers we considered and that the reasoning strategies that work best on `small but hard' problems are ineffective on `big but easy' problems and vice versa.},
  langid = {english}
}

@incollection{gomesModelCounting2021,
  title = {Model {{Counting}}},
  booktitle = {Handbook of {{Satisfiability}}},
  author = {Gomes, Carla P. and Sabharwal, Ashish and Selman, Bart},
  year = {2021},
  pages = {993--1014},
  publisher = {IOS Press},
  doi = {10.3233/FAIA201009},
  urldate = {2024-07-26}
}

@incollection{grumbergMemoryEfficientAllSolutions2004,
  title = {Memory {{Efficient All-Solutions SAT Solver}} and {{Its Application}} for {{Reachability Analysis}}},
  booktitle = {Formal {{Methods}} in {{Computer Aided Design}}},
  author = {Grumberg, Orna and Schuster, Assaf and Yadgar, Avi},
  @editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Hu, Alan J. and Martin, Andrew K.},
  year = {2004},
  volume = {3312},
  pages = {275--289},
  publisher = {Springer Berlin Heidelberg},
  @address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-30494-4_20},
  urldate = {2023-07-13},
  abstract = {This work presents a memory-efficient All-SAT engine which, given a propositional formula over sets of important and non-important variables, returns the set of all the assignments to the important variables, which can be extended to solutions (satisfying assignments) to the formula. The engine is built using elements of modern SAT solvers, including a scheme for learning conflict clauses and nonchronological backtracking. Re-discovering solutions that were already found is avoided by the search algorithm itself, rather than by adding blocking clauses. As a result, the space requirements of a solved instance do not increase when solutions are found. Finding the next solution is as efficient as finding the first one, making it possible to solve instances for which the number of solutions is larger than the size of the main memory.},
  isbn = {978-3-540-23738-9 978-3-540-30494-4},
  langid = {english}
}

@article{hansenUnveilingISCAS85Benchmarks1999,
  title = {Unveiling the {{ISCAS-85}} Benchmarks: A Case Study in Reverse Engineering},
  shorttitle = {Unveiling the {{ISCAS-85}} Benchmarks},
  author = {Hansen, M.C. and Yalcin, H. and Hayes, J.P.},
  year = {1999},
  month = jul,
  journal = {IEEE Design \& Test of Computers},
  volume = {16},
  number = {3},
  pages = {72--80},
  issn = {1558-1918},
  doi = {10.1109/54.785838},
  abstract = {Designing at higher levels of abstraction is key to managing the complexity of today's VLSI chips. The authors show how they reverse-engineered the ISCAS-85 benchmarks to add a useful, new high-level tool to the designer's arsenal.},
  keywords = {Adders,Benchmark testing,Circuit synthesis,Circuit testing,Data mining,Engineering management,Hardware design languages,Logic circuits,Logic testing,Reverse engineering}
}

@inproceedings{huangUsingDPLLEfficient2004,
  title = {Using {{DPLL}} for {{Efficient OBDD Construction}}},
  booktitle = {7th {{International Conference}} on {{Theory}} and {{Applications}} of {{Satisfiability Testing}}},
  author = {Huang, Jinbo and Darwiche, Adnan},
  year = {2004},
  month = may,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {157--172},
  publisher = {Springer-Verlag},
  @address = {Berlin, Heidelberg},
  doi = {10.1007/11527695_13},
  urldate = {2023-11-17},
  abstract = {The DPLL procedure has found great success in SAT, where search terminates on the first solution discovered. We show that this procedure is equally promising in a problem where exhaustive search is used, given that it is augmented with appropriate caching. Specifically, we propose two DPLL-based algorithms that construct OBDDs for CNF formulas. These algorithms have a worst-case complexity that is linear in the number of variables and size of the CNF, and exponential only in the cutwidth or pathwidth of the variable ordering. We show how modern SAT techniques can be harnessed by implementing the algorithms on top of an existing SAT solver. We discuss the advantage of this new construction method over the traditional approach, where OBDDs for subsets of the CNF formula are built and conjoined. Our experiments indicate that on many CNF benchmarks, the new method runs orders of magnitude faster than a comparable implementation of the traditional method.},
  isbn = {978-3-540-27829-0},
  langid = {english}
}

@inproceedings{iserMinimizingModelsTseitinEncoded2013a,
  title = {Minimizing {{Models}} for {{Tseitin-Encoded SAT Instances}}},
  booktitle = {16th {{International Conference}} on {{Theory}} and {{Applications}} of {{Satisfiability Testing}}},
  author = {Iser, Markus and Sinz, Carsten and Taghdiri, Mana},
  year = {2013},
  month = jul,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {224--232},
  publisher = {Springer-Verlag},
  @address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-39071-5_17},
  urldate = {2023-11-17},
  abstract = {Many applications of SAT solving can profit from minimal models---a partial variable assignment that is still a witness for satisfiability. Examples include software verification, model checking, and counterexample-guided abstraction refinement. In this paper, we examine how a given model can be minimized for SAT instances that have been obtained by Tseitin encoding of a full propositional logic formula. Our approach uses a SAT solver to efficiently minimize a given model, focusing on only the input variables. Experiments show that some models can be reduced by over 50 percent.},
  isbn = {978-3-642-39070-8},
  langid = {english}
}

@inproceedings{jabbourEnumeratingPrimeImplicants2014,
  title = {Enumerating {{Prime Implicants}} of {{Propositional Formulae}} in {{Conjunctive Normal Form}}},
  booktitle = {Logics in {{Artificial Intelligence}}},
  author = {Jabbour, Said and {Marques-Silva}, Joao and Sais, Lakhdar and Salhi, Yakoub},
  @editor = {Ferm{\'e}, Eduardo and Leite, Jo{\~a}o},
  year = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {152--165},
  publisher = {Springer International Publishing},
  @address = {Cham},
  doi = {10.1007/978-3-319-11558-0_11},
  abstract = {In this paper, a new approach for enumerating the set prime implicants (PI) of a Boolean formula in conjunctive normal form (CNF) is proposed. It is based on an encoding of the input formula as a new one whose models correspond to the set of prime implicants of the original theory. This first PI enumeration approach is then enhanced by an original use of the boolean functions or gates usually involved in many CNF instances encoding real-world problems. Experimental evaluation on several classes of CNF instances shows the feasibility of our proposed framework.},
  isbn = {978-3-319-11558-0},
  langid = {english}
}

@inproceedings{jacksonClauseFormConversions2005,
  title = {Clause {{Form Conversions}} for {{Boolean Circuits}}},
  booktitle = {7th {{International Conference}} on {{Theory}} and {{Applications}} of {{Satisfiability Testing}}},
  author = {Jackson, Paul and Sheridan, Daniel},
  @editor = {Hoos, Holger H. and Mitchell, David G.},
  year = {2005},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {183--198},
  publisher = {Springer},
  @address = {Berlin, Heidelberg},
  doi = {10.1007/11527695_15},
  abstract = {The Boolean circuits is well established as a data structure for building propositional encodings of problems in preparation for satisfiability solving. The standard method for converting Boolean circuits to clause form (naming every vertex) has a number of shortcomings.},
  isbn = {978-3-540-31580-3},
  langid = {english}
}

@article{jarvisaloSimulatingCircuitLevelSimplifications2012,
  title = {Simulating {{Circuit-Level Simplifications}} on {{CNF}}},
  author = {J{\"a}rvisalo, Matti and Biere, Armin and Heule, Marijn J. H.},
  year = {2012},
  month = dec,
  journal = {Journal of Automated Reasoning},
  volume = {49},
  number = {4},
  pages = {583--619},
  issn = {0168-7433, 1573-0670},
  doi = {10.1007/s10817-011-9239-9},
  urldate = {2024-07-20},
  copyright = {http://www.springer.com/tdm},
  langid = {english}
}

@techreport{jayaraman2014automated,
  title = {Automated Analysis and Debugging of Network Connectivity Policies},
  author = {Jayaraman, Karthick and Bj{\o}rner, Nikolaj and Outhred, Geoff and Kaufman, Charlie},
  year = {2014},
  month = jul,
  number = {MSR-TR-2014-102},
  institution = {Microsoft},
  abstract = {Network connectivity policies are crucial for assuring the security and availability of large-scale datacenter. Managing these policies is fraught with complexity and operator errors. The difficulties are exacerbated when deploying large scale offerings of public cloud services where multiple tenants are hosted within customized isolation boundaries. In these large-scale settings it is impractical to depend on human effort or trial and error to maintain the correctness and consistency of policies. We describe an approach for automatically validating network connectivity policies and its implementation in a tool called SecGuru. SecGuru can check selected properties of policies, e.g., is some traffic permitted or denied, and it can compare two policies yielding a semantic diff to summarize drifts. We use bit-vector logic to encode policies and semantic diffs; and the theorem prover Z3 as the underlying solver. A key contribution is a new algorithm for compactly enumerating symbolic diffs. We finally describe the experience of using SecGuru in Azure, a public cloud provider. Azure uses SecGuru for continuously monitoring policy configurations and alerting on errors, and also as a regression test suite to check policies before deployment. As a result of using SecGuru, today Azure proactively detects and avoids policy misconfigurations that lead to security and availability issues.}
}

@article{jayaramanAutomatedAnalysisDebugging,
  title = {Automated {{Analysis}} and {{Debugging}} of {{Network Connectivity Policies}}},
  author = {Jayaraman, Karthick and Bj{\o}rner, Nikolaj and Kaufman, Charlie and Outhred, Geoff},
  abstract = {Network connectivity policies are crucial for assuring the security and availability of large-scale datacenter. Managing these policies is fraught with complexity and operator errors. The difficulties are exacerbated when deploying large scale offerings of public cloud services where multiple tenants are hosted within customized isolation boundaries. In these large-scale settings it is impractical to depend on human effort or trial and error to maintain the correctness and consistency of policies.},
  langid = {english}
}

@inproceedings{jin2005prime,
  title = {Prime {{Clauses}} for {{Fast Enumeration}} of {{Satisfying Assignments}} to {{Boolean Circuits}}},
  booktitle = {Proceedings of the 42nd Annual Design Automation Conference},
  author = {Jin, HoonSang and Somenzi, Fabio},
  year = {2005},
  pages = {750--753}
}

@incollection{jinEfficientConflictAnalysis2005,
  title = {Efficient {{Conflict Analysis}} for {{Finding All Satisfying Assignments}} of a {{Boolean Circuit}}},
  booktitle = {Tools and {{Algorithms}} for the {{Construction}} and {{Analysis}} of {{Systems}}},
  author = {Jin, HoonSang and Han, HyoJung and Somenzi, Fabio},
  @editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Halbwachs, Nicolas and Zuck, Lenore D.},
  year = {2005},
  volume = {3440},
  pages = {287--300},
  publisher = {Springer Berlin Heidelberg},
  @address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-31980-1_19},
  urldate = {2023-07-12},
  abstract = {Finding all satisfying assignments of a propositional formula has many applications to the synthesis and verification of hardware and software. An approach to this problem that has recently emerged augments a clause-recording propositional satisfiability solver with the ability to add ``blocking clauses.'' One generates a blocking clause from a satisfying assignment by taking its complement. The resulting clause prevents the solver from visiting the same solution again. Every time a blocking clause is added the search is resumed until the instance becomes unsatisfiable. Various optimization techniques are applied to get smaller blocking clauses, since enumerating each satisfying assignment would be very inefficient.},
  isbn = {978-3-540-25333-4 978-3-540-31980-1},
  langid = {english}
}

@inproceedings{khurshidCaseEfficientSolution2004,
  title = {A {{Case}} for {{Efficient Solution Enumeration}}},
  booktitle = {7th {{International Conference}} on {{Theory}} and {{Applications}} of {{Satisfiability Testing}}},
  author = {Khurshid, Sarfraz and Marinov, Darko and Shlyakhter, Ilya and Jackson, Daniel},
  @editor = {Giunchiglia, Enrico and Tacchella, Armando},
  year = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {272--286},
  publisher = {Springer},
  @address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-24605-3_21},
  abstract = {SAT solvers have been ranked primarily by the time they take to find a solution or show that none exists. And indeed, for many problems that are reduced to SAT, finding a single solution is what matters. As a result, much less attention has been paid to the problem of efficiently generating all solutions.},
  isbn = {978-3-540-24605-3},
  langid = {english}
}

@inproceedings{kuiterTseitinNotTseitin2022,
  title = {Tseitin or Not {{Tseitin}}? {{The Impact}} of {{CNF Transformations}} on {{Feature-Model Analyses}}},
  shorttitle = {Tseitin or Not {{Tseitin}}?},
  booktitle = {37th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Kuiter, Elias and Krieter, Sebastian and Sundermann, Chico and Th{\"u}m, Thomas and Saake, Gunter},
  year = {2022},
  month = oct,
  pages = {1--13},
  publisher = {ACM},
  @address = {Rochester MI USA},
  doi = {10.1145/3551349.3556938},
  urldate = {2023-02-01},
  abstract = {Feature modeling is widely used to systematically model features of variant-rich software systems and their dependencies. By translating feature models into propositional formulas and analyzing them with solvers, a wide range of automated analyses across all phases of the software development process become possible. Most solvers only accept formulas in conjunctive normal form (CNF), so an additional transformation of feature models is often necessary. However, it is unclear whether this transformation has a noticeable impact on analyses. In this paper, we compare three transformations (i.e., distributive, Tseitin, and Plaisted-Greenbaum) for bringing featuremodel formulas into CNF. We analyze which transformation can be used to correctly perform feature-model analyses and evaluate three CNF transformation tools (i.e., FeatureIDE, KConfigReader, and Z3) on a corpus of 22 real-world feature models. Our empirical evaluation illustrates that some CNF transformations do not scale to complex feature models or even lead to wrong results for modelcounting analyses. Further, the choice of the CNF transformation can substantially influence the performance of subsequent analyses.},
  isbn = {978-1-4503-9475-8},
  langid = {english}
}

@inproceedings{lagniezImprovedDecisionDNNFCompiler2017,
  title = {An {{Improved Decision-DNNF Compiler}}},
  booktitle = {26th {{International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Lagniez, Jean-Marie and Marquis, Pierre},
  year = {2017},
  month = aug,
  pages = {667--673},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  @address = {Melbourne, Australia},
  doi = {10.24963/ijcai.2017/93},
  urldate = {2024-07-24},
  abstract = {We present and evaluate a new compiler, called D4, targeting the Decision-DNNF language. As the state-of-the-art compilers C2D and Dsharp targeting the same language, D4 is a top-down treesearch algorithm exploring the space of propositional interpretations. D4 is based on the same ingredients as those considered in C2D and Dsharp (mainly, disjoint component analysis, conflict analysis and non-chronological backtracking, component caching). D4 takes advantage of a dynamic decomposition approach based on hypergraph partitioning, used sparingly. Some simplification rules are also used to minimize the time spent in the partitioning steps and to promote the quality of the decompositions. Experiments show that the compilation times and the sizes of the Decision-DNNF representations computed by D4 are in many cases significantly lower than the ones obtained by C2D and Dsharp.},
  isbn = {978-0-9992411-0-3},
  langid = {english}
}

@inproceedings{lagniezLeveragingDecisionDNNFCompilation2024,
  title = {Leveraging {{Decision-DNNF Compilation}} for {{Enumerating Disjoint Partial Models}}},
  booktitle = {21st {{International Conference}} on {{Principles}} of {{Knowledge Representation}} and {{Reasoning}}},
  author = {Lagniez, Jean-Marie and Lonca, Emmanuel},
  year = {2024},
  month = nov,
  urldate = {2024-07-24},
  abstract = {The All-Solution Satisfiability Problem (AllSAT) extends SAT by requiring the identification of all possible solutions for a propositional formula. In practice, enumerating all complete models is often infeasible, making the identification of partial models essential for generating a concise representation of the solution set. Deterministic Decomposable Negation Normal Form (d-DNNF) serves as a language for representation known to offer polynomial-time algorithms for model enumeration. Specifically, when a propositional formula is encoded in d-DNNF, it enables iterative model enumeration with polynomial delay between models. However, despite the existence of theoretical algorithms for this purpose, no available implementations are currently accessible. Furthermore, these theoretical approaches are nearly impractical as they solely yield complete models. We introduce a novel algorithm that maintains a polynomial delay between partial models while significantly enhancing efficiency compared to baseline approaches. Furthermore, through experimental validation, we demonstrate the superiority of compiling a CNF formula {$\Sigma$} into a d-DNNF formula {$\Sigma\prime$} and subsequently enumerating models of {$\Sigma\prime$} over existing state-of-the-art methodologies for CNF partial model enumeration.},
  langid = {english}
}

@inproceedings{lagniezRecursiveAlgorithmProjected2019,
  title = {A {{Recursive Algorithm}} for {{Projected Model Counting}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Lagniez, Jean-Marie and Marquis, Pierre},
  year = {2019},
  month = jul,
  volume = {33},
  pages = {1536--1543},
  doi = {10.1609/aaai.v33i01.33011536},
  urldate = {2024-07-20},
  abstract = {We present a recursive algorithm for projected model counting, i.e., the problem consisting in determining the number of models k{$\exists$}X.{$\Sigma$}k of a propositional formula {$\Sigma$} after eliminating from it a given set X of variables. Based on a ''standard'' model counter, our algorithm projMC takes advantage of a disjunctive decomposition scheme of {$\exists$}X.{$\Sigma$} for computing k{$\exists$}X.{$\Sigma$}k. It also looks for disjoint components in its input for improving the computation. Our experiments show that in many cases projMC is significantly more efficient than the previous algorithms for projected model counting from the literature.},
  copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
  langid = {english}
}

@inproceedings{lahiriSMTTechniquesFast2006,
  title = {{{SMT Techniques}} for {{Fast Predicate Abstraction}}},
  booktitle = {Computer {{Aided Verification}}},
  author = {Lahiri, Shuvendu K. and Nieuwenhuis, Robert and Oliveras, Albert},
  year = {2006},
  month = aug,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {424--437},
  publisher = {Springer-Verlag},
  @address = {Berlin, Heidelberg},
  doi = {10.1007/11817963_39},
  urldate = {2023-11-17},
  abstract = {Predicate abstraction is a technique for automatically extracting finite-state abstractions for systems with potentially infinite state space. The fundamental operation in predicate abstraction is to compute the best approximation of a Boolean formula {$\phi$} over a set of predicates P . In this work, we demonstrate the use for this operation of a decision procedure based on the DPLL(T) framework for SAT Modulo Theories (SMT). The new algorithm is based on a careful generation of the set of all satisfying assignments over a set of predicates. It consistently outperforms previous methods by a factor of at least 20, on a diverse set of hardware and software verification benchmarks. We report detailed analysis of the results and the impact of a number of variations of the techniques. We also propose and evaluate a scheme for incremental refinement of approximations for predicate abstraction in the above framework.},
  isbn = {978-3-540-37406-0},
  langid = {english}
}

@inproceedings{lahiriSymbolicApproachPredicate2003,
  title = {A {{Symbolic Approach}} to {{Predicate Abstraction}}},
  booktitle = {Computer {{Aided Verification}}},
  author = {Lahiri, Shuvendu K. and Bryant, Randal E. and Cook, Byron},
  @editor = {Hunt, Warren A. and Somenzi, Fabio},
  year = {2003},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {141--153},
  publisher = {Springer},
  @address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-45069-6_15},
  abstract = {Predicate abstraction is a useful form of abstraction for the verification of transition systems with large or infinite state spaces. One of the main bottlenecks of this approach is the extremely large number of decision procedures calls that are required to construct the abstract state space. In this paper we propose the use of a symbolic decision procedure and its application for predicate abstraction. The advantage of the approach is that it reduces the number of calls to the decision procedure exponentially and also provides for reducing the re-computations inherent in the current approaches. We provide two implementations of the symbolic decision procedure: one based on BDDs which leverages the current advances in early quantification algorithms, and the other based on SAT-solvers. We also demonstrate our approach with quantified predicates for verifying parameterized systems. We illustrate the effectiveness of this approach on benchmarks from the verification of microprocessors, communication protocols, parameterized systems, and Microsoft Windows device drivers.},
  isbn = {978-3-540-45069-6},
  langid = {english}
}

@inproceedings{liangAllSATCCBoostingAllSAT2022,
  title = {{{AllSATCC}}: {{Boosting AllSAT Solving}} with {{Efficient Component Analysis}}},
  shorttitle = {{{AllSATCC}}},
  booktitle = {31st {{International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Liang, Jiaxin and Ma, Feifei and Zhou, Junping and Yin, Minghao},
  year = {2022},
  month = jul,
  pages = {1866--1872},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  @address = {Vienna, Austria},
  doi = {10.24963/ijcai.2022/259},
  urldate = {2023-07-12},
  abstract = {All Solution SAT (AllSAT) is a variant of Propositional Satisfiability, which aims to find all satisfying assignments for a given formula. AllSAT has significant applications in different domains, such as software testing, data mining, and network verification. In this paper, observing that the lack of component analysis may result in more work for algorithms with non-chronological backtracking, we propose a DPLL-based algorithm for solving AllSAT problem, named AllSATCC, which takes advantage of component analysis to reduce work repetition caused by non-chronological backtracking. The experimental results show that our algorithm outperforms the state-of-the-art algorithms on most instances.},
  isbn = {978-1-956792-00-3},
  langid = {english}
}

@inproceedings{liNovelSATAllsolutions2004,
  title = {A Novel {{SAT}} All-Solutions Solver for Efficient Preimage Computation},
  booktitle = {Automation and {{Test}} in {{Europe Conference}} and {{Exhibition Proceedings Design}}},
  author = {Li, Bin and Hsiao, M.S. and Sheng, Shuo},
  year = {2004},
  month = feb,
  volume = {1},
  pages = {272--277},
  issn = {1530-1591},
  doi = {10.1109/DATE.2004.1268860},
  abstract = {In this paper, we present a novel all-solutions preimage SAT solver, SOLALL, with the following features: (1) a new success-driven learning algorithm employing smaller cut sets; (2) a marked CNF database non-trivially combining success/conflict-driven learning; (3) quantified-jump-back dynamically quantifying primary input variables from the preimage; (4) improved free BDD built on the fly, saving memory and avoiding inclusion of PI variables; finally, (5) a practical method of storing all solutions into a canonical OBDD format. Experimental results demonstrated the efficiency of the proposed approach for very large sequential circuits.}
}

@article{linASSATComputingAnswer2004,
  title = {{{ASSAT}}: Computing Answer Sets of a Logic Program by {{SAT}} Solvers},
  shorttitle = {{{ASSAT}}},
  author = {Lin, Fangzhen and Zhao, Yuting},
  year = {2004},
  month = aug,
  journal = {Artificial Intelligence},
  series = {Nonmonotonic {{Reasoning}}},
  volume = {157},
  number = {1},
  pages = {115--137},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2004.04.004},
  urldate = {2024-10-28},
  abstract = {We propose a new translation from normal logic programs with constraints under the answer set semantics to propositional logic. Given a normal logic program, we show that by adding, for each loop in the program, a corresponding loop formula to the program's completion, we obtain a one-to-one correspondence between the answer sets of the program and the models of the resulting propositional theory. In the worst case, there may be an exponential number of loops in a logic program. To address this problem, we propose an approach that adds loop formulas a few at a time, selectively. Based on these results, we implement a system called ASSAT(X), depending on the SAT solver~X used, for computing one answer set of a normal logic program with constraints. We test the system on a variety of benchmarks including the graph coloring, the blocks world planning, and Hamiltonian Circuit domains. Our experimental results show that in these domains, for the task of generating one answer set of a normal logic program, our system has a clear edge over the state-of-art answer set programming systems Smodels and DLV.},
  keywords = {Answer set programming,Answer set semantics,Logic programming,SAT,SAT solvers,Stable model semantics}
}

@inproceedings{liuProgramAnalysisQualitative2011,
  title = {Program {{Analysis}}: {{From Qualitative Analysis}} to {{Quantitative Analysis}} ({{NIER Track}})},
  shorttitle = {Program Analysis},
  booktitle = {33rd {{International Conference}} on {{Software Engineering}}},
  author = {Liu, Sheng and Zhang, Jian},
  year = {2011},
  month = may,
  pages = {956--959},
  issn = {1558-1225},
  doi = {10.1145/1985793.1985957},
  abstract = {We propose to combine symbolic execution with volume computation to compute the exact execution frequency of program paths and branches. Given a path, we use symbolic execution to obtain the path condition which is a set of constraints; then we use volume computation to obtain the size of the solution space for the constraints. With such a methodology and supporting tools, we can decide which paths in a program are executed more often than the others. We can also generate certain test cases that are related to the execution frequency, e.g., those covering cold paths.}
}

@article{lopes2013network,
  title = {Network {{Verification}} in the {{Light}} of {{Program Verification}}},
  author = {Lopes, Nuno P. and Bj{\o}rner, Nikolaj and Godefroid, Patrice and Varghese, George},
  year = {2013},
  month = sep,
  journal = {MSR},
  volume = {Rep},
  abstract = {The fastest tools for network reachability queries use ad-hoc algorithms to compute all packets from a source S that can reach a destination D. This paper examines whether network reachability can be solved efficiently using existing verification tools. While most verification tools only compute reachability (``Can S reach D?''), we efficiently generalize them to compute all reachable packets. Using new and old benchmarks, we compare model checkers, SAT solvers and various Datalog implementations. The only existing verification method that worked competitively on all benchmarks in seconds was Datalog with a new composite Filter-Project operator and a Difference of Cubes representation. While Datalog is slightly slower than the Hassel C tool, it is far more flexible. We also present new results that more precisely characterize the computational complexity of network verification. This paper also provides a gentle introduction to program verification for the networking community. We would like to thank Jim Larus for inspiring this work.}
}

@inproceedings{lopesCheckingBeliefsDynamic2015,
  title = {Checking {{Beliefs}} in {{Dynamic Networks}}},
  booktitle = {12th {{USENIX Symposium}} on {{Networked Systems Design}} and {{Implementation}}},
  author = {Lopes, Nuno P. and Bj{\o}rner, Nikolaj and Godefroid, Patrice and Jayaraman, Karthick and Varghese, George},
  year = {2015},
  pages = {499--512},
  urldate = {2023-07-18},
  isbn = {978-1-931971-21-8},
  langid = {english}
}

@inproceedings{luoEfficientTwophaseMethod2021,
  title = {An {{Efficient Two-phase Method}} for {{Prime Compilation}} of {{Non-clausal Boolean Formulae}}},
  booktitle = {2021 {{IEEE}}/{{ACM International Conference On Computer Aided Design}}},
  author = {Luo, Weilin and Want, Hai and Zhong, Hongzhen and Wei, Ou and Fang, Biqing and Song, Xiaotong},
  year = {2021},
  month = nov,
  pages = {1--9},
  issn = {1558-2434},
  doi = {10.1109/ICCAD51958.2021.9643520},
  abstract = {Prime compilation aims to generate all prime implicates/implicants of a Boolean formula. Recently, prime compilation of non-clausal formulae has received great attention. Since it is hard for {\textbackslash}Sigma\_2{\textasciicircum}P, existing methods have performance issues. We argue that the main performance bottleneck stems from enlarging the search space using dual rail (DR) encoding, and computing a minimal clausal formula as a by-product. To deal with the issue, we propose a two-phase approach, namely CoAPI, for prime compilation of non-clausal formulae. Thanks to the two-phase framework, we construct a clausal formula without using DR encoding. In addition, to improve performance, the key in our work is a novel bounded prime extraction (BPE) method that, interleaving extracting prime implicates with extracting small implicates, enables constructing a succinct clausal formula rather than a minimal one. Following the assessment way of the state-of-the-art (SOTA) work, we show that CoAPI achieves SOTA performance. Particularly, for generating all prime implicates, CoAPI is up to about one order of magnitude faster. Moreover, we evaluate CoAPI on a benchmark sourcing from real-world industries. The results also confirm the outperformance of CoAPI11Our code and benchmarks are publicly available at https://github.com/LuoWeiLinWillam/CoAPI.}
}

@misc{masina_2024_cnf_code,
  title = {On {{CNF Conversion}} for {{SAT}} and {{SMT Enumeration}}: {{Source Code}}},
  author = {Masina, Gabriele},
  year = {2024},
  month = nov,
  doi = {10.5281/zenodo.14033422},
  howpublished = {Zenodo},
  note = {\url{https://doi.org/10.5281/zenodo.14033422}}
}

@misc{masina_2024_cnf_results,
  title = {On {{CNF Conversion}} for {{SAT}} and {{SMT Enumeration}}: {{Benchmarks}}, {{Results}} and {{Plots}}},
  author = {Masina, Gabriele},
  year = {2024},
  month = nov,
  howpublished = {Zenodo},
  note = {\url{https://doi.org/10.5281/zenodo.14035595}}
}

@inproceedings{masinaCNFConversionDisjoint2023,
  title = {On {{CNF Conversion}} for {{Disjoint SAT Enumeration}}},
  booktitle = {26th {{International Conference}} on {{Theory}} and {{Applications}} of {{Satisfiability Testing}}},
  author = {Masina, Gabriele and Spallitta, Giuseppe and Sebastiani, Roberto},
  @editor = {Mahajan, Meena and Slivovsky, Friedrich},
  year = {2023},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  volume = {271},
  pages = {15:1--15:16},
  publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
  @address = {Dagstuhl, Germany},
  doi = {10.4230/LIPIcs.SAT.2023.15},
  urldate = {2023-10-13},
  isbn = {978-3-95977-286-0}
}

@inproceedings{mathsat5_tacas13,
  title = {The {{MathSAT5 SMT Solver}}},
  booktitle = {Tools and {{Algorithms}} for the {{Construction}} and {{Analysis}} of {{Systems}}},
  author = {Cimatti, Alessandro and Griggio, Alberto and Schaafsma, Bastiaan Joost and Sebastiani, Roberto},
  @editor = {Piterman, Nir and Smolka, Scott A.},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {93--107},
  publisher = {Springer},
  @address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-36742-7_7},
  abstract = {MathSAT is a long-term project, which has been jointly carried on by FBK-IRST and University of Trento, with the aim of developing and maintaining a state-of-the-art SMT tool for formal verification (and other applications). MathSAT5 is the latest version of the tool. It supports most of the SMT-LIB theories and their combinations, and provides many functionalities (like e.g. unsat cores, interpolation, AllSMT). MathSAT5 improves its predecessor MathSAT4 in many ways, also providing novel features: first, a much improved incrementality support, which is vital in SMT applications; second, a full support for the theories of arrays and floating point; third, sound SAT-style Boolean formula preprocessing for SMT formulae; finally, a framework allowing users for plugging their custom tuned SAT solvers. MathSAT5 is freely available, and it is used in numerous internal projects, as well as by a number of industrial partners.},
  isbn = {978-3-642-36742-7},
  langid = {english},
  keywords = {Bound Model Check,Model Check,Predicate Abstraction,Theory Solver,Variable Elimination}
}

@inproceedings{maVolumeComputationBoolean2009,
  title = {Volume {{Computation}} for {{Boolean Combination}} of {{Linear Arithmetic Constraints}}},
  booktitle = {Automated {{Deduction}}},
  author = {Ma, Feifei and Liu, Sheng and Zhang, Jian},
  @editor = {Schmidt, Renate A.},
  year = {2009},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {453--468},
  publisher = {Springer},
  @address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-02959-2_33},
  abstract = {There are many works on the satisfiability problem for various logics and constraint languages, such as SAT and Satisfiability Modulo Theories (SMT). On the other hand, the counting version of decision problems is also quite important in automated reasoning. In this paper, we study a counting version of SMT, i.e., how to compute the volume of the solution space, given a set of Boolean combinations of linear constraints. The problem generalizes the model counting problem and the volume computation problem for convex polytopes. It has potential applications to program analysis and verification, as well as approximate reasoning, yet it has received little attention. We first give a straightforward method, and then propose an improved algorithm. We also describe two ways of incorporating theory-level lemma learning technique into the algorithm. They have been implemented, and some experimental results are given. Through an example program, we show that our tool can be used to compute how often a given program path is executed.},
  isbn = {978-3-642-02959-2},
  langid = {english}
}

@inproceedings{mcmillanApplyingSATMethods2002,
  title = {Applying {{SAT Methods}} in {{Unbounded Symbolic Model Checking}}},
  booktitle = {Computer {{Aided Verification}}},
  author = {McMillan, Kenneth L.},
  year = {2002},
  month = jul,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {250--264},
  publisher = {Springer-Verlag},
  @address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-45657-0_19},
  urldate = {2023-11-17},
  abstract = {A method of symbolic model checking is introduced that uses conjunctive normal form (CNF) rather than binary decision diagrams (BDD's) and uses a SAT-based approach to quantifier elimination. This method is compared to a traditional BDD-based model checking approach using a set of benchmark problems derived from the compositional verification of a commercial microprocessor design.},
  isbn = {978-3-540-43997-4 978-3-540-45657-5},
  langid = {english}
}

@article{miltersenConvertingCNFDNF2005,
  title = {On Converting {{CNF}} to {{DNF}}},
  author = {Miltersen, Peter Bro and Radhakrishnan, Jaikumar and Wegener, Ingo},
  year = {2005},
  month = nov,
  journal = {Theoretical Computer Science},
  volume = {347},
  number = {1-2},
  pages = {325--335},
  issn = {03043975},
  doi = {10.1016/j.tcs.2005.07.029},
  urldate = {2023-07-19},
  langid = {english}
}

@inproceedings{minatoFindingAllSimple1998,
  title = {Finding {{All Simple Disjunctive Decompositions Using Irredundant Sum-of-Products Forms}}},
  booktitle = {1998 {{IEEE}}/{{ACM International Conference}} on {{Computer-Aided Design}}},
  author = {Minato, S. and De Micheli, G.},
  year = {1998},
  month = nov,
  pages = {111--117},
  doi = {10.1145/288548.288586},
  abstract = {Finding disjunctive decompositions is an important technique to realize compact logic networks. Simple disjunctive decomposition is a basic and useful concept, that extracts a single output subblock function whose input variable set is disjunctive from the other part. The paper presents a method for finding simple disjunctive decompositions by generating irredundant sum-of-products forms and applying factorization. We prove that all simple disjunctive decompositions can be extracted in our method, namely all possible decompositions are included in the factored logic networks. Experimental results show that our method can efficiently extract all the simple disjunctive decompositions of the large scale functions. Our result clarifies the relationship between the functional decomposition method and the two-level logic factorization method.}
}

@inproceedings{mohleDualizingProjectedModel2018,
  title = {Dualizing {{Projected Model Counting}}},
  booktitle = {30th {{IEEE International Conference}} on {{Tools}} with {{Artificial Intelligence}}},
  author = {M{\"o}hle, Sibylle and Biere, Armin},
  year = {2018},
  month = nov,
  pages = {702--709},
  issn = {2375-0197},
  doi = {10.1109/ICTAI.2018.00111},
  abstract = {In many recent applications of model counting not all variables are relevant for a specific problem. For instance redundant variables are added during formula transformation. In projected model counting these redundant variables are ignored by projecting models onto relevant variables. Inspired by dual propagation which has its origin in solving quantified Boolean formulae and jointly works on both the original formula and its negation, we present a novel calculus for dual projected model counting. It allows to capture existing techniques such as blocking clauses, chronological as well as non-chronological backtracking, but also introduces new concepts including discounting and dual conflict analysis to obtain partial models. Experiments demonstrate the benefit of our approach.}
}

@article{mohleEnumeratingShortProjected2025,
  title = {On {{Enumerating Short Projected Models}}},
  author = {M{\"o}hle, Sibylle and Sebastiani, Roberto and Biere, Armin},
  year = {2025},
  month = jan,
  journal = {Discrete Applied Mathematics},
  volume = {361},
  pages = {412--439},
  issn = {0166-218X},
  doi = {10.1016/j.dam.2024.10.021},
  urldate = {2024-11-18},
  abstract = {Propositional model enumeration, or All-SAT, is the task to record all models of a propositional formula. It is a key task in software and hardware verification, system engineering, and predicate abstraction, to mention a few. It also provides a means to convert a CNF formula into DNF, which is relevant in circuit design. While in some applications enumerating models multiple times causes no harm, in others avoiding repetitions is crucial. We therefore present two model enumeration algorithms which adopt dual reasoning in order to shorten the found models. The first method enumerates pairwise contradicting models. Repetitions are avoided by the use of so-called blocking clauses for which we provide a dual encoding. In our second approach we relax the uniqueness constraint. We present an adaptation of the standard conflict-driven clause learning procedure to support model enumeration without blocking clauses. Our procedures are expressed by means of a calculus and proofs of correctness are provided.},
  langid = {english},
  keywords = {All-SAT,Computer Science - Logic in Computer Science,Model enumeration,Projection,Propositional logic}
}

@inproceedings{mohleFourFlavorsEntailment2020,
  title = {Four {{Flavors}} of {{Entailment}}},
  booktitle = {23rd {{International Conference}} on {{Theory}} and {{Applications}} of {{Satisfiability Testing}}},
  author = {M{\"o}hle, Sibylle and Sebastiani, Roberto and Biere, Armin},
  @editor = {Pulina, Luca and Seidl, Martina},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {62--71},
  publisher = {Springer International Publishing},
  @address = {Cham},
  doi = {10.1007/978-3-030-51825-7_5},
  abstract = {We present a novel approach for enumerating partial models of a propositional formula, inspired by how theory solvers and the SAT solver interact in lazy SMT. Using various forms of dual reasoning allows our CDCL-based algorithm to enumerate partial models with no need for exploring and shrinking full models. Our focus is on model enumeration without repetition, with potential applications in weighted model counting and weighted model integration for probabilistic inference over Boolean and hybrid domains. Chronological backtracking renders the use of blocking clauses obsolete. We provide a formalization and examples. We further discuss important design choices for a future implementation related to the strength of dual reasoning, including unit propagation, using SAT or QBF oracles.},
  isbn = {978-3-030-51825-7},
  langid = {english}
}

@article{morettin-wmi-aij19,
  title = {Advanced {{SMT}} Techniques for {{Weighted Model Integration}}},
  author = {Morettin, Paolo and Passerini, Andrea and Sebastiani, Roberto},
  year = {2019},
  journal = {Artificial Intelligence},
  volume = {275},
  number = {C},
  pages = {1--27},
  issn = {00043702},
  doi = {10.1016/j.artint.2019.04.003},
  abstract = {Weighted model integration (WMI) is a recent formalism generalizing weighted model counting (WMC) to run probabilistic inference over hybrid domains, characterized by both discrete and continuous variables and relationships between them. WMI is computationally very demanding as it requires to explicitly enumerate all possible truth assignments to be integrated over. Component caching strategies which proved extremely effective for WMC are difficult to apply in this formalism because of the tight coupling induced by the arithmetic constraints. In this paper we present a novel formulation of WMI, which allows to exploit the power of SMT-based predicate abstraction techniques in designing efficient inference procedures. A novel algorithm combines a strong reduction in the number of models to be integrated over with their efficient enumeration. Experimental results on synthetic and real-world data show drastic computational improvements over the original WMI formulation as well as existing alternatives for hybrid inference.},
  langid = {english},
  keywords = {Probabilistic inference,Satisfiability modulo theories,Weighted model counting,Weighted model integration}
}

@inproceedings{morettin-wmi-ijcar17,
  title = {Efficient {{Weighted Model Integration}} via {{SMT-Based Predicate Abstraction}}},
  booktitle = {26th {{International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Morettin, Paolo and Passerini, Andrea and Sebastiani, Roberto},
  year = {2017},
  month = aug,
  pages = {720--728},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  @address = {Melbourne, Australia},
  doi = {10.24963/ijcai.2017/100},
  urldate = {2022-12-19},
  abstract = {Weighted model integration (WMI) is a recent formalism generalizing weighted model counting (WMC) to run probabilistic inference over hybrid domains, characterized by both discrete and continuous variables and relationships between them. Albeit powerful, the original formulation of WMI suffers from some theoretical limitations, and it is computationally very demanding as it requires to explicitly enumerate all possible models to be integrated over. In this paper we present a novel general notion of WMI, which fixes the theoretical limitations and allows for exploiting the power of SMTbased predicate abstraction techniques. A novel algorithm combines a strong reduction in the number of models to be integrated over with their efficient enumeration. Experimental results on synthetic and real-world data show drastic computational improvements over the original WMI formulation as well as existing alternatives for hybrid inference.},
  isbn = {978-0-9992411-0-3},
  langid = {english}
}

@inproceedings{morgadoGoodLearningImplicit2005,
  title = {Good {{Learning}} and {{Implicit Model Enumeration}}},
  booktitle = {17th {{IEEE International Conference}} on {{Tools}} with {{Artificial Intelligence}}},
  author = {Morgado, Antonio and {Marques-Silva}, Joao},
  year = {2005},
  month = nov,
  pages = {131--136},
  publisher = {IEEE Computer Society},
  @address = {USA},
  urldate = {2023-10-13},
  abstract = {A large number of practical applications rely on effective algorithms for propositional model enumeration and counting. Examples include knowledge compilation, model checking and hybrid solvers. Besides practical applications, the problem of counting propositional models is of key relevancy in computational complexity. In recent years a number of algorithms have been proposed for propositional model enumeration. This paper surveys algorithms for model enumeration, and proposes optimizations to existing algorithms, namely through the learning and simplificationof goods. Moreover, the paper also addresses open topics in model counting related with good learning. Experimental results indicate that the proposed techniques are effective for model enumeration.},
  isbn = {978-0-7695-2488-7}
}

@inproceedings{muiseDsharpFastDDNNF2012,
  title = {Dsharp: {{Fast}} d-{{DNNF Compilation}} with {{sharpSAT}}},
  shorttitle = {Dsharp},
  booktitle = {Advances in {{Artificial Intelligence}}},
  author = {Muise, Christian and McIlraith, Sheila A. and Beck, J. Christopher and Hsu, Eric I.},
  @editor = {Kosseim, Leila and Inkpen, Diana},
  year = {2012},
  pages = {356--361},
  publisher = {Springer},
  @address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-30353-1_36},
  abstract = {Knowledge compilation is a compelling technique for dealing with the intractability of propositional reasoning. One particularly effective target language is Deterministic Decomposable Negation Normal Form (d-DNNF). We exploit recent advances in \#SAT solving in order to produce a new state-of-the-art CNF {$\rightarrow$} d-DNNF compiler: Dsharp. Empirical results demonstrate that Dsharp is generally an order of magnitude faster than c2d, the de facto standard for compiling to d-DNNF, while yielding a representation of comparable size.},
  isbn = {978-3-642-30353-1},
  langid = {english}
}

@article{palopoliAlgorithmsSelectiveEnumeration1999,
  title = {Algorithms for Selective Enumeration of Prime Implicants},
  author = {Palopoli, Luigi and Pirri, Fiora and Pizzuti, Clara},
  year = {1999},
  month = jul,
  journal = {Artificial Intelligence},
  volume = {111},
  number = {1-2},
  pages = {41--72},
  issn = {00043702},
  doi = {10.1016/S0004-3702(99)00035-1},
  urldate = {2023-07-26},
  abstract = {We present a new approach for selective enumeration of prime implicants of CNF formulae. The method uses a 0--1 programming schema, having feasible solutions corresponding to prime implicants. Prime implicants are generated one at a time, so that as many of them can be computed as needed by the specific application considered. Selective generation is also supported, whereby preferences on the structure of generated prime implicants can be specified. We present two algorithms for selective enumeration of prime implicants and discuss their properties. The former amounts to solving the basic 0--1 programming schema first, to obtain an implicant {$\psi$} (not necessarily a prime one), and then generating a prime implicant implied by {$\psi$} . The latter is based on adding a suitable minimization function to the basic 0--1 programming schema so that finding optimal solutions corresponds one-to-one to generating prime implicants of the original theory. We show that the latter algorithm has wider applicability but is less efficient than the former one. Finally we present experimental results, which confirm the effectiveness of our approach in computing prime implicants of CNF formulae. {\copyright} 1999 Elsevier Science B.V. All rights reserved.},
  langid = {english}
}

@inproceedings{phanAllSolutionSatisfiabilityModulo2015,
  title = {All-{{Solution Satisfiability Modulo Theories}}: {{Applications}}, {{Algorithms}} and {{Benchmarks}}},
  shorttitle = {All-{{Solution Satisfiability Modulo Theories}}},
  booktitle = {10th {{International Conference}} on {{Availability}}, {{Reliability}} and {{Security}}},
  author = {Phan, Q. and Malacaria, P.},
  year = {2015},
  month = aug,
  pages = {100--109},
  publisher = {IEEE},
  @address = {Toulouse, France},
  doi = {10.1109/ARES.2015.14},
  abstract = {Satisfiability Modulo Theories (SMT) is a decision problem for logical formulas over one or more first-order theories. In this paper, we study the problem of finding all solutions of an SMT problem with respect to a set of Boolean variables, henceforth All-SMT. First, we show how an All-SMT solver can benefit various domains of application: Bounded Model Checking, Automated Test Generation, Reliability analysis, and Quantitative Information Flow. Secondly, we then propose algorithms to design an All-SMT solver on top of an existing SMT solver, and implement it into a prototype tool, called aZ3. Thirdly, we create a set of benchmarks for All-SMT in the theory of linear integer arithmetic QF LIA and the theory of bit vectors with arrays and uninterpreted functions QF AUFBV. We compare aZ3 against MathSAT, the only existing All-SMT solver, on our benchmarks. Experimental results show that aZ3 is more precise than MathSAT.},
  isbn = {978-1-4673-6590-1},
  langid = {english}
}

@article{plaistedStructurepreservingClauseForm1986,
  title = {A {{Structure-preserving Clause Form Translation}}},
  author = {Plaisted, David A. and Greenbaum, Steven},
  year = {1986},
  journal = {Journal of Symbolic Computation},
  volume = {2},
  number = {3},
  pages = {293--304},
  publisher = {Elsevier},
  issn = {07477171},
  doi = {10.1016/S0747-7171(86)80028-1},
  abstract = {Most resolution theorem provers convert a theorem into clause form before attempting to find a proof. The conventional translation of a first-order formula into clause form often obscures the structure of the formula, and may increase the length of the formula by an exponential amount in the worst case. We present a non-standard clause form translation that preserves more of the structure of the formula than the conventional translation. This new translation also avoids the exponential increase in size which may occur with the standard translation. We show how this idea may be combined with the idea of replacing predicates by their definitions before converting to clause form. We give a method of lock resolution which is appropriate for the non-standard clause form translation, and which has yielded a spectacular reduction in search space and time for one example. These techniques should increase the attractiveness of resolution theorem provers for program verification applications, since the theorems that arise in program verification are often simple but tedious for humans to prove.},
  langid = {english}
}

@incollection{prestwichCNFEncodings2021,
  title = {{{CNF Encodings}}},
  booktitle = {Handbook of {{Satisfiability}}},
  author = {Prestwich, Steven},
  @editor = {Biere, Armin and Heule, Marijn and van Maaren, Hans and Walsh, Toby},
  year = {2021},
  series = {Frontiers in {{Artificial Intelligence}} and {{Applications}}},
  edition = {2},
  volume = {336},
  pages = {75--100},
  publisher = {IOS Press},
  doi = {10.3233/FAIA200985},
  urldate = {2024-01-10},
  isbn = {978-1-64368-160-3 978-1-64368-161-0}
}

@inproceedings{previtiPrimeCompilationNonClausal,
  title = {Prime {{Compilation}} of {{Non-Clausal Formulae}}},
  booktitle = {24th {{International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Previti, Alessandro and Ignatiev, Alexey and Morgado, Antonio and {Marques-Silva}, Joao},
  year = {2015},
  langid = {english}
}

@incollection{raviMinimalAssignmentsBounded2004,
  title = {Minimal {{Assignments}} for {{Bounded Model Checking}}},
  booktitle = {Tools and {{Algorithms}} for the {{Construction}} and {{Analysis}} of {{Systems}}},
  author = {Ravi, Kavita and Somenzi, Fabio},
  @editor = {Goos, Gerhard and Hartmanis, Juris and Van Leeuwen, Jan and Jensen, Kurt and Podelski, Andreas},
  year = {2004},
  volume = {2988},
  pages = {31--45},
  publisher = {Springer Berlin Heidelberg},
  @address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-24730-2_3},
  urldate = {2023-07-17},
  abstract = {A traditional counterexample to a linear-time safety property shows the values of all signals at all times prior to the error. However, some signals may not be critical to causing the failure. A succinct explanation may help human understanding as well as speed up algorithms that have to analyze many such traces. In Bounded Model Checking (BMC), a counterexample is constructed from a satisfying assignment to a Boolean formula, typically in CNF. Modern SAT solvers usually assign values to all variables when the input formula is satisfiable. Deriving minimal satisfying assignments from such complete assignments does not lead to concise explanations of counterexamples because of how CNF formulae are derived from the models. Hence, we formulate the extraction of a succinct counterexample as the problem of finding a minimal assignment that, together with the Boolean formula describing the model, implies an objective. We present a two-stage algorithm for this problem, such that the result of each stage contributes to identify the ``interesting'' events that cause the failure. We demonstrate the effectiveness of our approach with an example and with experimental results.},
  isbn = {978-3-540-24730-2},
  langid = {english}
}

@misc{sebastianiAreYouSatisfied2020,
  title = {Are {{You Satisfied}} by {{This Partial Assignment}}?},
  author = {Sebastiani, Roberto},
  year = {2020},
  month = feb,
  number = {arXiv:2003.04225},
  eprint = {2003.04225},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2003.04225},
  howpublished = {arXiv preprint arXiv:2003.04225},
  urldate = {2022-12-19},
  abstract = {Many procedures for SAT and SAT-related problems -- in particular for those requiring the complete enumeration of satisfying truth assignments -- rely their efficiency on the detection of partial assignments satisfying an input formula. In this paper we analyze the notion of partial-assignment satisfiability -- in particular when dealing with non-CNF and existentially-quantified formulas -- raising a flag about the ambiguities and subtleties of this concept, and investigating their practical consequences. This may drive the development of more effective assignment-enumeration algorithms.},
  archiveprefix = {arXiv},
  langid = {english}
}

@inproceedings{sharmaGANAKScalableProbabilistic2019,
  title = {{{GANAK}}: {{A Scalable Probabilistic Exact Model Counter}}},
  shorttitle = {{{GANAK}}},
  booktitle = {Proceedings of the 28th {{International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Sharma, Shubham and Roy, Subhajit and Soos, Mate and Meel, Kuldeep S.},
  year = {2019},
  month = aug,
  pages = {1169--1176},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  @address = {Macao, China},
  doi = {10.24963/ijcai.2019/163},
  urldate = {2024-07-22},
  abstract = {Given a Boolean formula F , the problem of model counting, also referred to as \#SAT, seeks to compute the number of solutions of F . Model counting is a fundamental problem with a wide variety of applications ranging from planning, quantified information flow to probabilistic reasoning and the like. The modern \#SAT solvers tend to be either based on static decomposition, dynamic decomposition, or a hybrid of the two. Despite dynamic decomposition based \#SAT solvers sharing much of their architecture with SAT solvers, the core design and heuristics of dynamic decomposition-based \#SAT solvers has remained constant for over a decade. In this paper, we revisit the architecture of the state-ofthe-art dynamic decomposition-based \#SAT tool, sharpSAT, and demonstrate that by introducing a new notion of probabilistic component caching and the usage of universal hashing for exact model counting along with the development of several new heuristics can lead to significant performance improvement over state-of-the-art model-counters. In particular, we develop GANAK, a new scalable probabilistic exact model counter that outperforms state-of-the-art exact and approximate model counters sharpSAT and ApproxMC3 respectively, both in terms of PAR-2 score and the number of instances solved. Furthermore, in our experiments, the model count returned by GANAK was equal to the exact model count for all the benchmarks. Finally, we observe that recently proposed preprocessing techniques for model counting benefit exact model counters while hurting the performance of approximate model counters.},
  isbn = {978-0-9992411-4-1},
  langid = {english}
}

@article{slagleNewAlgorithmGenerating1970,
  title = {A {{New Algorithm}} for {{Generating Prime Implicants}}},
  author = {Slagle, J.R. and Chang, Chin-Liang and Lee, R.C.T.},
  year = {1970},
  month = apr,
  journal = {IEEE Transactions on Computers},
  volume = {C-19},
  number = {4},
  pages = {304--310},
  issn = {1557-9956},
  doi = {10.1109/T-C.1970.222917},
  abstract = {This paper describes an algorithm which will generate all the prime implicants of a Boolean function. The algorithm is different from those previously given in the literature, and in many cases it is more efficient. It is proved that the algorithm will find all the prime implicants. The algorithm may possibly generate some nonprime implicants. However, using frequency orderings on literals, the experiments with the algorithm show that it usually generates very few ( possibly none) nonprime implicants. Furthermore, the algorithm may be used to find the minimal sums of a Boolean function. The algorithm is implemented by a computer program in the LISP language.}
}

@inproceedings{spallittaDisjointPartialEnumeration2024,
  title = {Disjoint {{Partial Enumeration}} without {{Blocking Clauses}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Spallitta, Giuseppe and Sebastiani, Roberto and Biere, Armin},
  year = {2024},
  month = mar,
  volume = {38},
  pages = {8126--8135},
  doi = {10.1609/aaai.v38i8.28652},
  urldate = {2024-06-18},
  abstract = {A basic algorithm for enumerating disjoint propositional models (disjoint AllSAT) is based on adding blocking clauses incrementally, ruling out previously found models. On the one hand, blocking clauses have the potential to reduce the number of generated models exponentially, as they can handle partial models. On the other hand, the introduction of a large number of blocking clauses affects memory consumption and drastically slows down unit propagation.   We propose a new approach that allows for enumerating disjoint partial models with no need for blocking clauses by integrating: Conflict-Driven Clause-Learning (CDCL), Chronological Backtracking (CB), and methods for shrinking models (Implicant Shrinking). Experiments clearly show the benefits of our novel approach.},
  copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
  langid = {english}
}

@misc{spallittaDisjointProjectedEnumeration2024,
  title = {Disjoint {{Projected Enumeration}} for {{SAT}} and {{SMT}} without {{Blocking Clauses}}},
  author = {Spallitta, Giuseppe and Sebastiani, Roberto and Biere, Armin},
  year = {2024},
  month = oct,
  number = {arXiv:2410.18707},
  eprint = {2410.18707},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.18707},
  howpublished = {arXiv preprint arXiv:2410.18707},
  urldate = {2024-10-28},
  abstract = {All-Solution Satisfiability (AllSAT) and its extension, All-Satisfiability Modulo Theories (AllSMT), have become more relevant in recent years, mainly in formal verification and artificial intelligence applications. The goal of these problems is the enumeration of all satisfying assignments to a formula (for SAT and SMT problems, respectively), making them useful for test generation, model checking, and probabilistic inference. Nevertheless, traditional AllSAT algorithms face significant computational challenges due to the exponential growth of the search space and inefficiencies caused by blocking clauses, which cause memory blowups and degrade unit propagation performances in the long term. This paper presents two novel solvers: {\textbackslash}solverPlus\{\}, a projected AllSAT solver, and {\textbackslash}solverSMT\{\}, a projected AllSMT solver. Both solvers combine Conflict-Driven Clause Learning (CDCL) with chronological backtracking to improve efficiency while ensuring disjoint enumeration. To retrieve compact partial assignments we propose a novel aggressive implicant shrinking algorithm to minimize the number of partial assignments, reducing overall search complexity, and compatible with chronological backtracking. Furthermore, we extend the solver framework to handle projected enumeration and SMT formulae effectively and efficiently, adapting the baseline framework to integrate theory reasoning and the distinction between important and non-important variables. An extensive experimental evaluation demonstrates the superiority of our approach compared to state-of-the-art solvers, particularly in scenarios requiring projection and SMT-based reasoning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Logic in Computer Science}
}

@article{spallittaEnhancingSMTbasedWeighted2024a,
  title = {Enhancing {{SMT-based Weighted Model Integration}} by {{Structure Awareness}}},
  author = {Spallitta, Giuseppe and Masina, Gabriele and Morettin, Paolo and Passerini, Andrea and Sebastiani, Roberto},
  year = {2024},
  month = mar,
  journal = {Artificial Intelligence},
  volume = {328},
  pages = {104067},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2024.104067},
  urldate = {2024-02-05},
  abstract = {The development of efficient exact and approximate algorithms for probabilistic inference is a long-standing goal of artificial intelligence research. Whereas substantial progress has been made in dealing with purely discrete or purely continuous domains, adapting the developed solutions to tackle hybrid domains, characterized by discrete and continuous variables and their relationships, is highly non-trivial. Weighted Model Integration (WMI) recently emerged as a unifying formalism for probabilistic inference in hybrid domains. Despite a considerable amount of recent work, allowing WMI algorithms to scale with the complexity of the hybrid problem is still a challenge. In this paper we highlight some substantial limitations of existing state-of-the-art solutions, and develop an algorithm that combines SMT-based enumeration, an efficient technique in formal verification, with an effective encoding of the problem structure. This allows our algorithm to avoid generating redundant models, resulting in drastic computational savings. Additionally, we show how SMT-based approaches can seamlessly deal with different integration techniques, both exact and approximate, significantly expanding the set of problems that can be tackled by WMI technology. An extensive experimental evaluation on both synthetic and real-world datasets confirms the substantial advantage of the proposed solution over existing alternatives. The application potential of this technology is further showcased on a prototypical task aimed at verifying the fairness of probabilistic programs.}
}

@inproceedings{spallittaSMTbasedWeightedModel2022,
  title = {{{SMT-based Weighted Model Integration}} with {{Structure Awareness}}},
  booktitle = {38th {{Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Spallitta, Giuseppe and Masina, Gabriele and Morettin, Paolo and Passerini, Andrea and Sebastiani, Roberto},
  @editor = {Cussens, James and Zhang, Kun},
  year = {2022},
  volume = {180},
  pages = {1876--1885},
  publisher = {PMLR},
  abstract = {Weighted Model Integration (WMI) is a popular formalism aimed at unifying approaches for probabilistic inference in hybrid domains, involving logical and algebraic constraints. Despite a considerable amount of recent work, allowing WMI algorithms to scale with the complexity of the hybrid problem is still a challenge. In this paper we highlight some substantial limitations of existing state-of-the-art solutions, and develop an algorithm that combines SMT-based enumeration, an efficient technique in formal verification, with an effective encoding of the problem structure. This allows our algorithm to avoid generating redundant models, resulting in substantial computational savings. An extensive experimental evaluation on both synthetic and real-world datasets confirms the advantage of the proposed solution over existing alternatives.},
  langid = {english}
}

@inproceedings{tibebuAugmentingAllSolution2018,
  title = {Augmenting {{All Solution SAT Solving}} for {{Circuits}} with {{Structural Information}}},
  booktitle = {{{IEEE}} 21st {{International Symposium}} on {{Design}} and {{Diagnostics}} of {{Electronic Circuits}} \& {{Systems}} ({{DDECS}})},
  author = {Tibebu, Abraham Temesgen and Fey, Goerschwin},
  year = {2018},
  pages = {117--122},
  issn = {2473-2117},
  doi = {10.1109/DDECS.2018.00028},
  abstract = {All solutions SAT (All-SAT) is important in applications where we require enumerating all satisfying assignments of a propositional formula, e.g., when reasoning over many or all possible test patterns in Automatic Test Pattern Generation (ATPG). We applied structural analysis starting from primary inputs or primary outputs to generalize a current total assignment to a partial assignment. This speeds up the determination of all satisfying assignments. The experiments were conducted using a large number of random instances and different available All-SAT solvers. We show that structural analysis techniques can significantly speed up enumeration of all satisfying assignments of combinational circuits and yield the the second largest number of total satisfying assignments from all compared All-SAT solvers.},
  keywords = {All-SAT,Automatic test pattern generation,Binary decision diagrams,blocking clause,Boolean functions,Combinational circuits,Data preprocessing,Logic gates,partial assignment,Periodic structures,primary inputs,primary outputs,structural analysis}
}

@inproceedings{todaBDDConstructionAll2015,
  title = {{{BDD}} Construction for All Solutions {{SAT}} and Efficient Caching Mechanism},
  booktitle = {Proceedings of the 30th {{Annual ACM Symposium}} on {{Applied Computing}}},
  author = {Toda, Takahisa and Tsuda, Koji},
  year = {2015},
  month = apr,
  pages = {1880--1886},
  publisher = {ACM},
  @address = {Salamanca Spain},
  doi = {10.1145/2695664.2695941},
  urldate = {2023-07-17},
  abstract = {We improve an existing OBDD-based method of computing all total satisfying assignments of a Boolean formula, where an OBDD means an ordered binary decision diagram that is not necessarily reduced. To do this, we introduce lazy caching and finer caching by effectively using unit propagation. We implement our methods on top of a modern SAT solver, and show by experiments that lazy caching significantly accelerates the original method and finer caching in turn reduces an OBDD size.},
  isbn = {978-1-4503-3196-8},
  langid = {english}
}

@article{todaExploitingFunctionalDependencies2017,
  title = {Exploiting {{Functional Dependencies}} of {{Variables}} in {{All Solutions SAT Solvers}}},
  author = {Toda, Takahisa and Inoue, Takeru},
  year = {2017},
  journal = {Journal of Information Processing},
  volume = {25},
  number = {0},
  pages = {459--468},
  issn = {1882-6652},
  doi = {10.2197/ipsjjip.25.459},
  urldate = {2023-07-17},
  abstract = {All solutions SAT (AllSAT) is the problem of generating satisfying assignments to a given conjunctive normal form (CNF) and has been a key issue commonly found in several applications of formal verification including model checking. CNF encoding, which translates original problems for AllSAT solvers, spawns many auxiliary variables and, what is worse, obscures functional dependencies over variables. AllSAT solvers consequently have to deal with unnecessarily larger CNFs, although the original problems might be much more tractable in essence. This paper proposes a novel AllSAT solver along with a CNF encoding technique; our solver extracts functional dependencies through the encoding process, and the dependence is effectively utilized to solve the CNF. Our solver is designed based on the OBDD compilation technique, which allows us to efficiently handle intractable CNFs with a number of solutions in dynamic programming manner. Our proposal is very simple but powerful; experiments with real network instances showed that our solver exhibits a great improvement.},
  langid = {english}
}

@article{todaImplementingEfficientAll2016,
  title = {Implementing {{Efficient All Solutions SAT Solvers}}},
  author = {Toda, Takahisa and Soh, Takehide},
  year = {2016},
  month = nov,
  journal = {ACM Journal of Experimental Algorithmics},
  volume = {21},
  pages = {1--44},
  issn = {1084-6654, 1084-6654},
  doi = {10.1145/2975585},
  urldate = {2023-07-12},
  abstract = {All solutions SAT (AllSAT for short) is a variant of the propositional satisfiability problem. AllSAT has been relatively unexplored compared to other variants despite its significance. We thus survey and discuss major techniques of AllSAT solvers. We accurately implemented them and conducted comprehensive experiments using a large number of instances and various types of solvers including a few publicly available software. The experiments revealed the solvers' characteristics. We made our implemented solvers publicly available so that other researchers can easily develop their solvers by modifying our code and comparing it with existing methods.},
  langid = {english}
}

@incollection{tseitinComplexityDerivationPropositional1983,
  title = {On the {{Complexity}} of {{Derivation}} in {{Propositional Calculus}}},
  booktitle = {Automation of {{Reasoning}}: 2: {{Classical Papers}} on {{Computational Logic}} 1967--1970},
  author = {Tseitin, Grigori S.},
  @editor = {Siekmann, J{\"o}rg H. and Wrightson, Graham},
  year = {1983},
  series = {Symbolic {{Computation}}},
  pages = {466--483},
  publisher = {Springer},
  @address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-81955-1_28},
  urldate = {2022-12-20},
  abstract = {The question of the minimum complexity of derivation of a given formula in classical propositional calculus is considered in this article and it is proved that estimates of complexity may vary considerably among the various forms of propositional calculus. The forms of propositional calculus used in the present article are somewhat unusual, {\dag} but the results obtained for them can, in principle, be extended to the usual forms of propositional calculus.},
  isbn = {978-3-642-81955-1},
  langid = {english}
}

@inproceedings{yuAllSATUsingMinimal2014,
  title = {All-{{SAT Using Minimal Blocking Clauses}}},
  booktitle = {2014 27th {{International Conference}} on {{VLSI Design}} and 2014 13th {{International Conference}} on {{Embedded Systems}}},
  author = {Yu, Yinlei and Subramanyan, Pramod and Tsiskaridze, Nestan and Malik, Sharad},
  year = {2014},
  month = jan,
  pages = {86--91},
  issn = {2380-6923},
  doi = {10.1109/VLSID.2014.22},
  abstract = {The All-SAT problem deals with determining all the satisfying assignments that exist for a given propositional logic formula. This problem occurs in verification applications including predicate abstraction and unbounded model checking. A typical All-SAT solver is based on iteratively computing satisfying assignments using a traditional Boolean satisfiability (SAT) solver and adding blocking clauses which are the complement of the total/partial assignments. We argue that such an algorithm is doing more work than needed and introduce new algorithms that are more efficient. Experiments show that these algorithms generate solutions with up to 14X fewer partial assignments and are up to three orders of magnitude faster.}
}

@inproceedings{zhangAcceleratingAllSATComputation2020,
  title = {Accelerating All-{{SAT}} Computation with Short Blocking Clauses},
  booktitle = {Proceedings of the 35th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Zhang, Yueling and Pu, Geguang and Sun, Jun},
  year = {2020},
  month = dec,
  pages = {6--17},
  publisher = {ACM},
  @address = {Virtual Event Australia},
  doi = {10.1145/3324884.3416569},
  urldate = {2023-07-12},
  abstract = {The All-SAT (All-SATisfiable) problem focuses on finding all satisfiable assignments of a given propositional formula, whose applications include model checking, automata construction, and logic minimization. A typical ALL-SAT solver is normally based on iteratively computing satisfiable assignments of the given formula. In this work, we introduce BASolver, a backbone-based All-SAT solver for propositional formulas. Compared to the existing approaches, BASolver generates shorter blocking clauses by removing backbone variables from the partial assignments and the blocking clauses. We compare BASolver with 4 existing ALL-SAT solvers, namely MBlocking, BC, BDD, and NBC. Experimental results indicate that although finding all the backbone variables consumes additional computing time, BASolver is still more efficient than the existing solvers because of the shorter blocking clauses and the backbone variables used in it. With the 608 formulas, BASolver solves the largest amount of formulas (86), which is 22\%, 36\%, 68\%, 86\% more formulas than MBlocking, BC, NBC, and BDD respectively. For the formulas that are both solved by BASolver and the other solvers, BASolver uses 88.4\% less computing time on average than the other solvers. For the 215 formulas which first 1000 satisfiable assignments are found by at least one of the solvers, BASolver uses 180\% less computing time on average than the other solvers.},
  isbn = {978-1-4503-6768-4},
  langid = {english}
}

@article{zhouEstimatingVolumeSolution2015,
  title = {Estimating the {{Volume}} of {{Solution Space}} for {{Satisfiability Modulo Linear Real Arithmetic}}},
  author = {Zhou, Min and He, Fei and Song, Xiaoyu and He, Shi and Chen, Gangyi and Gu, Ming},
  year = {2015},
  month = feb,
  journal = {Theory of Computing Systems},
  volume = {56},
  number = {2},
  pages = {347--371},
  issn = {1433-0490},
  doi = {10.1007/s00224-014-9553-9},
  urldate = {2023-08-24},
  abstract = {Satisfiability Modulo Theories techniques can check if a formula is satisfiable. In many cases, not only the qualitative judgment (satisfiable or not) but also the quantitative judgment (the dimension and size of the solution space) are of practical interest. For instance, the volume of path condition formula reflects the probability of the corresponding program path being taken. However, existing algorithms are not practical because they only work for small instances. Given a formula with Boolean structures, its volume is typically obtained by first decomposing it to a series of conjunctions (of linear constraints) with disjoint solution spaces and then accumulating the volume of each one. For the former step, we propose a BDD-based search algorithm which sharply reduces the number of conjunctions. For the latter one, we propose a Monte-Carlo integration with a ray-based sampling strategy, which approximates the volume efficiently and accurately. Furthermore, degenerate solution spaces, which are not considered by other algorithms, could be handled properly by ours. Experimental results show that our method can handle formulas with up to 20 variables, which will cover many practical cases in software engineering},
  langid = {english}
}

@article{LIN2004115,
title = {ASSAT: computing answer sets of a logic program by SAT solvers},
journal = {Artificial Intelligence},
volume = {157},
number = {1},
pages = {115-137},
year = {2004},
note = {Nonmonotonic Reasoning},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2004.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370204000578},
author = {Fangzhen Lin and Yuting Zhao},
keywords = {Answer set programming, Answer set semantics, Stable model semantics, Logic programming, SAT, SAT solvers},
abstract = {We propose a new translation from normal logic programs with constraints under the answer set semantics to propositional logic. Given a normal logic program, we show that by adding, for each loop in the program, a corresponding loop formula to the program's completion, we obtain a one-to-one correspondence between the answer sets of the program and the models of the resulting propositional theory. In the worst case, there may be an exponential number of loops in a logic program. To address this problem, we propose an approach that adds loop formulas a few at a time, selectively. Based on these results, we implement a system called ASSAT(X), depending on the SAT solverX used, for computing one answer set of a normal logic program with constraints. We test the system on a variety of benchmarks including the graph coloring, the blocks world planning, and Hamiltonian Circuit domains. Our experimental results show that in these domains, for the task of generating one answer set of a normal logic program, our system has a clear edge over the state-of-art answer set programming systems Smodels and DLV.}
}
