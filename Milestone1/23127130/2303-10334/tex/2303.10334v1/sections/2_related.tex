\section{Related Works}
\label{sec_related}

Image classification models are optimized to capture only the discriminative local regions (features) of objects, leading to the poor coverage of its CAM on the objects. While the ideal pseudo labels (extended from CAM) to train WSSS models should cover all parts of the object regardless of dicriminativeness. To this end, many efforts have been made in the field of WSSS. Below, we introduce only the variants for seed generation and mask refinement.

\noindent
\textbf{Seed Generation}
One direction is the erasing-based methods. AE-PSL~\cite{adv_erasing} is an adversarial erasing strategy running in an iterative manner. It masks out the discriminative regions in the current iteration, to explicitly force the model to discover new regions in the next iteration. ACoL~\cite{adv_erasing2} is an improved method. It is based on an end-to-end learning framework composed of two branches. One branch applies a feature-level masking on the other. However, these two methods have an over-erasing problem, especially for small objects. CSE~\cite{cse} is a class-specific erasing method. It masks out a random object class based on CAM and then explicitly penalizes the prediction of the erased class. In this way, it gradually approaches to the boundary of the object on the image. It can not only discover more non-discriminative regions but also alleviate the over-erasing problem (of the above two methods) since it also penalizes over-erased regions. However, erasing-based methods have the low-efficiency problem as they have to feed forward an image multiple times.

Besides of erasing, there are other advanced methods recently.
RIB~\cite{rib} interpreted the poor coverage problem of CAM in an information bottleneck principle. It re-trains the multi-label classifier by omitting the activation function in the last layer to encourage the information transmission of information non-discriminative regions to the classification. Jiang et al.~\cite{l2g} empirically observed that classification models can discover more discriminative regions when taking local image patches rather than whole input image as input. They proposed a local-to-global attention transfer method contains a local network that produces local attentions with rich object details for local views as well as a global network that receives the global view as input and aims to distill the discriminative attention knowledge from the local network. Some other researchers explore to utilizing contrastive learning~\cite{rca,ppc}, graph neural network~\cite{group}, and self-supervised learning~\cite{sub_category,seam} to discover more non-discriminative regions.

Compared to aforementioned methods, our method have the following advantages:
1) it does not require additional training on the basis of CAM;
and 2) it can be taken as a generic substitute of the conventional CAM and plugged into many CAM-based WSSS frameworks.

\noindent
\textbf{Mask Refinement}
One category of refinement methods~\cite{psa,irn,auxiliary,bes} propagate the object regions in the seed to semantically similar pixels in the neighborhood. It is achieved by the random walk~\cite{randomwalk} on a transition matrix where each element is an affinity score. The related methods have different designs of this matrix. PSA~\cite{psa} is an AffinityNet to predict semantic affinities between adjacent pixels. IRN~\cite{irn} is an inter-pixel relation network to estimate class boundary maps based on which it computes affinities. Another method is BES~\cite{bes} that learns to predict boundary maps by using CAM as pseudo ground truth. All these methods introduced additional network modules to vanilla CAM. Another category of refinement methods~\cite{dsrg,ooa,ficklenet,edam,eps} utilize saliency maps~\cite{saliency1,saliency2} to extract background cues and combine them with object cues. EPS~\cite{eps} proposed a joint training strategy to combine CAM and saliency maps. EDAM~\cite{edam} introduced a post-processing method to integrate the confident areas in the saliency map into CAM. Our LPCAM is orthogonal to them and can be plugged into those methods.