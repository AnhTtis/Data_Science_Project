\section{Experiments}
\label{sec_exper}

\subsection{Datasets and Implementation Details}
\label{sec_datasets}

\noindent
\textbf{Datasets} are the commonly used WSSS datasets: PASCAL VOC 2012~\cite{voc} and MS~COCO 2014~\cite{mscoco}. PASCAL VOC 2012 contains $20$ foreground object categories and $1$ background category with $1,464$ \texttt{train} images, $1,449$ \texttt{val} images, and $1,456$ \texttt{test} images. Following related works~\cite{irn,recam,advcam,amn,ppc,rca}, we use the enlarged training set with $10,582$ training images provided by SBD~\cite{voc_aug}. MS~COCO 2014 dataset consists of $80$ foreground categories and $1$ background category, with $82,783$ and $40,504$ images in \texttt{train} and \texttt{val} sets, respectively. 

\noindent
\textbf{Evaluation Metrics.}
To evaluate the quality of seed mask and pseudo mask, we first generate them for every image in the \texttt{train} set and then use the ground truth masks to compute mIoU. For semantic segmentation, we train the segmentation model, use it to predict masks for the images in \texttt{val} and \texttt{test} sets, and compute mIoU based on ground truth masks.

\noindent
\textbf{Implementation Details.}
We follow \cite{irn,conta,recam,advcam,amn} to use ResNet-50~\cite{resnet} pre-trained on ImageNet~\cite{imagenet} as the backbone of multi-label classification model. For fair comparison with related works, we also follow \cite{seam,ppc,rca} to use WideResNet-38~\cite{wresnet} as backbone and follow EDAM~\cite{edam} to use saliency maps~\cite{saliency} to refine CAM. 

\noindent
\emph{Extra Hyperparameters.}
For K-Means clustering, we set $K$ as $12$ and $20$ for VOC and MS~COCO, respectively. The threshold $\tau$ in Eq.~\ref{eq:cam_fg_bg} is set to $0.1$ for VOC and $0.25$ for MS~COCO. For the selection of prototypes, $\mu_f$ is set to $0.9$ on both datasets, and $\mu_b$ is $0.9$ and $0.5$ on VOC and MS~COCO, respectively. We conduct the sensitivity analysis on these four hyperparameters to show that LPCAM is not sensitive to any of them.

\noindent
\emph{Common Hyperparameter (in CAM methods).}
The hard threshold used to generate 0-1 seed mask is $0.3$ for LPCAM on both datasets. Please note that we follow previous works~\cite{conta,advcam,rib,recam,amn,ppc} to select this threshold by using the ground truth masks in the training set as ``validation''.

\noindent
\emph{Time Costs.}
In K-Means clustering, we use all \texttt{train} set on VOC and sample $100$ images per class on MS~COCO (to control time costs). If taking the time cost of training a multi-label classification model as unit $1$, our extra time cost (for clustering) is about $0.9$ and $1.1$ on VOC and MS~COCO, respectively. 

\noindent
\emph{For Semantic Segmentation.}
When using DeepLabV2~\cite{deeplabv2} for semantic segmentation, we follow the common settings \cite{irn,recam,advcam,rib,amn,ppc} as follows.
The backbone of DeepLabV2 model is ResNet-101~\cite{resnet} and is pre-trained on ImageNet\cite{imagenet}. We crop each training image to the size of $321\times 321$ and use horizontal flipping and random crop for data augmentation. We train the model for $20k$ and $100k$ iterations on VOC and MS COCO, respectively, with the respective batch size of $5$ and $10$. The weight decay is set to $5$e-$4$ on both datasets and the learning rate is $2.5$e-$4$ and $2$e-$4$ on VOC and MS~COCO, respectively. 

When using UperNet, we follow ReCAM~\cite{recam}. We resize the input images to $2,048\times 512$ with a ratio range from $0.5$ to $2.0$, and then crop them to $512\times 512$ randomly. Data augmentation includes horizontal flipping and color jitter. We train the models for $40k$ and $80k$ iterations on VOC and MS~COCO datasets, respectively, with a batch size of 16. We deploy AdamW~\cite{adamw} solver with an initial learning rate $6e^{-5}$ and weight decay as $0.01$. The learning rate is decayed by a power of $1.0$ according to polynomial decay schedule.

\subsection{Results and Analyses}

\noindent
\textbf{Ablation Study.}
We conduct an ablation study on the VOC dataset to evaluate the two terms of LPCAM in Eq.~\ref{eq:LPCAM}: foreground term $\bm{FG}_n$ and background term $\bm{BG}_n$ that accord to class and context prototypes, respectively. In Table~\ref{table_ablation}, we show the mIoU results (of seed masks), false positive (FP), false negative (FN), precision, and recall. We can see that our methods of using class prototypes (LPCAM-F and LPCAM) greatly improve the recalls---$11.4\%$ and $12.0\%$ higher than CAM, reducing the rates of FN a lot. This validates the ability of our methods to capture non-discriminative regions of the image. We also notice that LPCAM-F increases the rate of FP over CAM. The reason is that confusing context features (e.g., ``railroad'' for ``train'') may be wrongly taken as class features. Fortunately, when we explicitly resolve this issue by applying the negative context term $-\bm{BG}_n$ in LPCAM, this rate can be reduced (by $3.3\%$ for VOC), and the overall performance (mIoU) can be improved (by $2.8\%$ for VOC). We are thus confident to take LPCAM as a generic substitute of CAM in WSSS methods (see empirical validations below). 

\input{tables/ablation}
\input{tables/plugin}
\input{figures/fig_vis}
\input{tables/seg}

\noindent
\textbf{Generality of LPCAM.}
We validate the generality of LPCAM based on multiple WSSS methods, the popular IRN~\cite{irn}, the top-performing AMN~\cite{amn}, the saliency-map-based EDAM~\cite{edam}, and the transformer-arch-based MCTformer~\cite{mctformer}), by simply replacing CAM with LPCAM. Table~\ref{table_plugin} and Table~\ref{table_seg} show the consistent superiority of LPCAM. For example, on the first row of Table~\ref{table_plugin} (plugging LPCAM in IRN), LPCAM outperforms CAM by $6.1\%$ on seed masks and $4.7\%$ on pseudo masks. These margins are almost maintained when using pseudo masks to train semantic segmentation models in Table~\ref{table_seg}. The improvements on the large-scale dataset MS~COCO are also obvious and consistent, e.g., $2.7\%$ and $2.2\%$ for generating seed masks in IRN and AMN, respectively.

\input{figures/fig_sensitivity}
\input{tables/related_works}

\noindent
\textbf{Sensitivity Analysis for Hyperparameters.}
In Figure~\ref{fig_sensitivity}, we show the quality (mIoU) of generated seed masks when plugging LPCAM in AMN on VOC dataset. We perform hyperparameter sensitivity analyses by changing the values of (a) the threshold $\tau$ for dividing foreground and background local features, (b) the threshold $\mu_f$ for selecting class prototypes and the threshold $\mu_b$ for selecting context prototypes, (c) the number of clusters $K$ in K-Means, and (d) the threshold used to generate 0-1 seed mask (a common hyperparameter in all CAM-based methods). Figure~\ref{fig_sensitivity}(a) shows that the optimal value of $\tau$ is $0.1$. Adding a small change does not make any significant effect on the results, e.g., the drop is less than $1\%$ if increasing $\tau$ to $0.2$. Figure~\ref{fig_sensitivity}(b) shows that the optimal values of $\mu_f$ and $\mu_b$ are both $0.9$. The gentle curves show that LPCAM is little sensitive to $\mu_f$ and $\mu_b$. This is because classification models (trained in the first step of WSSS) often produce overconfident (sharp) predictions~\cite{confident}, i.e., output probabilities are often close to $0$ or $1$. It is easy to set thresholds ($\mu_f$ and $\mu_b$) on such sharp values. In Figure~\ref{fig_sensitivity}(c), the best mIoU of seed mask is $65.3\%$ when $K$=$12$, and it drops by only $0.7$ percentage points when $K$ goes up to $20$. In Figure~\ref{fig_sensitivity}(d), LPCAM shows much gentler slopes than CAM around their respective optimal points, indicating its lower sensitivity to the changes of this threshold.

\noindent
\textbf{Qualitative Results.}
Figure~\ref{fig_vis} shows qualitative examples where LPCAM leverages both discriminative and non-discriminative local features to generate heatmaps and 0-1 masks.
In the leftmost two examples, CAM focuses on only discriminative features, e.g., the ``head'' regions of ``teddy bear'' and ``dog'', while our LPCAM has better coverage on the non-discriminative feature, e.g., the ``leg'',  ``body'' and ``tail'' regions. In the ``surfboard'' example, the context prototype term $-\bm{BG}_n$ in Eq.~\ref{eq:LPCAM} helps to remove the context ``waves''. In the rightmost example, we show a failure case: LPCAM succeeds in capturing more object parts of ``train'' but unnecessarily covers more on the context ``railroad''. We think the reason is the strong co-occurrence of  ``train'' and ``railroad'' in the images of ``train''. 

\noindent
\textbf{Comparing to Related Works.}
We compare LPCAM with state-of-the-art methods in WSSS. As shown in Table~\ref{table_related}, on the common setting (ResNet-50 based classification model, and ResNet-101 based DeepLabV2 segmentation model pre-trained on ImageNet), our AMN+LPCAM achieves the state-of-the-art results on VOC ($70.1\%$ mIoU on \texttt{val} and $70.4\%$ on \texttt{test}). On the more challenging MS~COCO dataset, our AMN+LPCAM (ResNet-50 as backbone) outperforms the state-of-the-art result AMN and all related works based on WRN-38.