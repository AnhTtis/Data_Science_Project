\section{Method}
\label{sec_method}
\input{figures/fig_framework}
In Section~\ref{sec_pipeline}, we introduce the pipeline of generating LPCAM using a collection of class-wise prototypes including class prototypes and context prototypes, without any re-training on the classification model. The step-by-step illustration is shown in Figure~\ref{fig_framework}, demonstrating the steps of generating local prototypes from all images of a class and using these prototypes to extract LPCAM for each single image. In Section~\ref{sec_justfication}, we justify 1) the advantages of using clustered local prototypes in LPCAM; and 2) the effectiveness of LPCAM from the perspective of map normalization. 

\subsection{LPCAM Pipeline}
\label{sec_pipeline}

\noindent
\textbf{Backbone and Features.} 
We use a standard ResNet-50~\cite{resnet} as the network backbone (i.e., feature encoder) of the multi-label classification model to extract features, following related works~\cite{irn,conta,advcam,rib,recam,amn}. Given an input image $\bm{x}$, and its multi-hot class label $\bm{y}\in\{0,1\}^{N}$,  we denote the output of the trained feature encoder as $f(\bm{x}) \in \mathbb{R}^{W \times H \times C}$. $C$ denotes the number of channels, $H$ and $W$ are the height and width, respectively, and $N$ is the total number of foreground classes in the dataset.

\noindent
\textbf{Extracting CAM.}
Before clustering local prototypes of class as well as context, we need the rough location information of foreground and background. We use the conventional CAM to achieve this. We extract it for each individual class $n$ given the feature $f(\bm{x})$ and the corresponding classifier weights $\mathbf{w}_n$ in the FC layer, as follows,
\begin{equation} \label{eq:cam}
    \operatorname{CAM}_n(\bm{x})=
    \frac{\operatorname{ReLU}\left(\bm{A}_n\right)}{\max \left(\operatorname{ReLU}\left(\bm{A}_n\right)\right)},
    \quad
    \bm{A}_n=\mathbf{w}_{n}^{\top}f(\bm{x}).
\end{equation}

\noindent
\textbf{Clustering.}
We perform clustering for every individual class. Here we discuss the details for class $n$. Given an image sample $\bm{x}$ of class $n$, we divide the feature block $f(\bm{x})$ spatially into two sets, $\mathcal{F}$ and $\mathcal{B}$, based on CAM:
\begin{equation}\label{eq:cam_fg_bg}
    f(\bm{x})^{i,j} \in 
        \begin{cases}
            \mathcal{F}, & \text { if } \operatorname{CAM}_n^{i,j}(\bm{x}) \geq \tau \\ 
            \mathcal{B}, & \text { otherwise }
        \end{cases}
\end{equation}
where $f(\bm{x})^{i,j} \in \mathbb{R}^{C}$ denotes the local feature at spatial location $(i,j)$. $\tau$ is the threshold to generate a 0-1 mask from $\operatorname{CAM}_n(\bm{x})$. $\mathcal{F}$ denotes the set of foreground local features, and $\mathcal{B}$ for the set of background (context) local features.

Similarly, we can collect $\mathcal{F}$ to contain the foreground features of all samples\footnote{We use a random subset of samples for each class in the real implementation, to reduce the computation costs of clustering.} in class $n$, and $\mathcal{B}$ for all background features, where the subscript $n$ is omitted for brevity. After that, we perform K-Means clustering, respectively, for $\mathcal{F}$ and $\mathcal{B}$, to obtain $K$ class centers in each of them, where $K$ is a hyperparameter. We denote the foreground cluster centers as $\mathbf{F}=\{\mathbf{F}_{1}, \cdots, \mathbf{F}_{K}\}$ and the background cluster centers as $\mathbf{B}=\{\mathbf{B}_{1}, \cdots, \mathbf{B}_{K}\}$.

\noindent
\textbf{Selecting Prototypes.}
The masks of conventional CAM are not accurate or complete, e.g., background features could be grouped into $\mathcal{F}$. To solve this issue, we need an ``evaluator'' to check the eligibility of cluster centers to be used as prototypes. The intuitive way is to use the classifier $\mathbf{w}_n$ as an auto ``evaluator'': using it to compute the prediction score of each cluster center $\mathbf{F}_i$ in $\mathbf{F}$ by:
\begin{equation}
    \bm{z}_i = \frac{\exp(\mathbf{F}_i \cdot \mathbf{w}_n)}{\sum_{j} \exp (\mathbf{F}_i \cdot \mathbf{w}_j)}.
\end{equation}
Then, we select those centers with high confidence: $\bm{z}_i>\mu_f$, where $\mu_f$ is a threshold---usually a very high value like $0.9$. 
We denote selected ones as $\mathbf{F'}=\{\mathbf{F'}_{1}, \cdots, \mathbf{F'}_{K'_1}\}$. Intuitively, confident predictions indicate strong local features, i.e., prototypes, of the class.


Before using these local prototypes to generate LPCAM, we highlight that in our implementation of LPCAM, we not only \emph{preserve} the non-discriminative features but also \emph{suppress} strong context features (i.e., false positive),
as the extraction and application of context prototypes are convenient---similar to class prototypes but in a reversed manner. We elaborate these in the following.
For each $\mathbf{B}_i$ in the context cluster center set $\mathbf{B}$, we apply the same method (as for $\mathbf{F}_i$) to compute a prediction score:
\begin{equation}
    \bm{z}_i = \frac{\exp(\mathbf{B}_i \cdot \mathbf{w}_n)}{\sum_{j} \exp (\mathbf{B}_i \cdot \mathbf{w}_j)}.
\end{equation}
Intuitively, if the model is well-trained on class labels, its prediction on context features should be extremely low.
Therefore, we select the centers with $\bm{z}_i<\mu_b$ (where $\mu_b$ is usually a value like $0.5$), and denote them as $\mathbf{B'}=\{\mathbf{B'}_{1}, \cdots, \mathbf{B'}_{K'_2}\}$. It is worth noting that our method is not sensitive to the values of the hyperparameters $\mu_f$ and $\mu_b$, given reasonable ranges, e.g., $\mu_f$ should have a large value around $0.9$. We show an empirical validation for this in Section~\ref{sec_exper}.


\noindent
\textbf{Generating LPCAM.}
Each of the prototypes represents a local visual pattern in the class: $\mathbf{F'}_{i}$ for class-related pattern (e.g., the ``leg'' of ``sheep'' class) and $\mathbf{B'}_{i}$ for context-related pattern (e.g., the ``grassland'' in ``sheep'' images) where the context often correlates with the class. Here we introduce how to apply these prototypes on the feature map block to generate LPCAM. LPCAM can be taken as a substitute of CAM. In Subsection~\ref{sec_justfication}, we will justify why LPCAM is superior to CAM from two perspectives: Global Average Pooling (GAP) and Normalization.

For each prototype, we slide it over all spatial positions on the feature map block, and compute its similarity to the local feature at each position. We adopt cosine similarity as we used it for K-Means. In the end, we get a cosine similarity map between prototype and feature. After computing all similarity maps (by sliding all local prototypes), we aggregate them as follows,
\begin{equation}
    \begin{aligned}
         &\bm{FG}_n=\frac{1}{K'_1}\sum_{\mathbf{F'_i}\in\mathbf{F'}}sim(f(\bm{x}),\mathbf{F'_i}), \\
        & \bm{BG}_n=\frac{1}{K'_2}\sum_{\mathbf{B'_i}\in\mathbf{B'}}sim(f(\bm{x}),\mathbf{B'_i}),
    \label{eq:fg_bg}
    \end{aligned}
\end{equation}
where $sim()$ denotes cosine similarity. As $sim()$ value is always within the range of $[-1, 1]$, each pixel on the maps of $\bm{FG}_n$ and $\bm{BG}_n$ has a normalized value, i.e., $\bm{FG}_n$ and $\bm{BG}_n$ are normalized. Intuitively, $\bm{FG}_n$ highlights the class regions in the input image correlated to the $n$-th prototype, while $\bm{BG}_n$ highlights the context regions. The former needs to be preserved and the latter (e.g., pixels highly correlated to backgrounds) should to be removed. Therefore, we can formulate LPCAM as follows:
\begin{equation}
    \begin{aligned}
        &\operatorname{LPCAM}_n(\bm{x})=
        \frac{\operatorname{ReLU}\left(\bm{A}_n\right)}{\max \left(\operatorname{ReLU}\left(\bm{A}_n\right)\right)}, \\
        &\bm{A}_n= \bm{FG}_n - \bm{BG}_n,
    \label{eq:LPCAM}
    \end{aligned}
\end{equation}
where the first formula is the application of the maximum-value based normalization (the same as in CAM).

\input{sections/3_2_justify}