\section{Introduction}
\label{sec_intro}
Extracting CAM~\cite{cam} from classification models is the essential step for training semantic segmentation models when only image-level labels are available, i.e., in the WSSS tasks~\cite{conta,advcam,rib,recam,amn}. More specifically, the general pipeline of WSSS consists of three steps:
1)~training a multi-label classification model with the image-level labels; 2)~extracting CAM of each class to generate a 0-1 mask (usually called seed mask), often with a further step of refinement to generate pseudo mask~\cite{sec,irn}; and 3)~taking all-class pseudo masks as pseudo labels to train a semantic segmentation model in a fully-supervised fashion~\cite{deeplabv2,deeplabv3+,upernet}. It is clear that the CAM in the first step determines the performance of the final semantic segmentation model. However, the conventional CAM and its variants often suffer from the poor coverage of foreground objects in the image, i.e., a large amount of object pixels are mistakenly recognized as background, as demonstrated in Figure~\ref{fig_cam_examples}(a) where only few pixels are activated in warm colors.

We point out that this locality is due to the fact that CAM is extracted from a discriminative model. The training of such model naturally discards the non-discriminative regions which confuse the model between similar as well as highly co-occurring object classes. This is a general problem of discriminative models, and is particularly obvious when the number of training classes is small~\cite{conta,adv_erasing,adv_erasing2}. To visualize the evidence, we use the classifier weights of confusing classes, e.g., ``car'', ``train'' and ``person'', to compute CAMs for the ``bus'' image, and show them in Figure~\ref{fig_cam_examples}(b). We find from (a) and (b) that the heating regions in ground truth class and confusing classes are complementary. E.g., the upper and frontal regions (on the ``bus'' image) respectively heated in the CAMs of ``car'' and ``train'' (confusing classes) are missing in the CAM of ``bus'' (ground truth), which means the classifier de-activates those regions for ``bus'' as it is likely to recognize them as ``car'' or ``train''.

\input{figures/fig_cam}


Technically, for each class, we use two factors to compute CAM: 1) the feature map block after the last conv layer, and 2) the weight of the classifier for that class.  As aforementioned, the second factor is often biased to discriminative features. Our intuition is to replace it with a non-biased one. The question becomes how to derive a \emph{non-biased} classifier from a biased classification model, where \emph{non-biased} means representing all local semantics of the class. We find the biased classifier is due to incomplete features, i.e., only discriminative local features fed into the classifier, and the non-discriminative ones are kicked out by the global average pooling (GAP) after the last conv layer. To let the model pay attention to non-discriminative features as well, we propose to omit GAP, derive a prototype-based classifier by clustering all local features (collected across all spatial locations on the feature map blocks of all training samples in the class) into $K$ local prototypes each representing a local semantic of the class. In Section~\ref{sec_method}, we give a detailed justification that this prototype-based classifier is able to capture both discriminative and non-discriminative features.



Then, the question is how to use local prototypes on the feature map block (i.e., the 1st factor) to generate CAM. We propose to apply them one-by-one on the feature map block to generate $K$ similarity maps (e.g., by using cosine distance), each aiming to capture the local regions that contain similar semantics to one of the prototypes. We highlight that this ``one-by-one'' is important to preserve non-discriminative regions as the normalization on each similarity map is independent. We provide a detailed justification from the perspective of normalization in Section~\ref{sec_justfication}. Then, we average across all normalized similarity maps to get a single map---we call Local Prototype CAM (LPCAM). In addition, we extend LPCAM by using the local prototypes of contexts as well. We subtract the context similarity maps (computed between the feature map block and the clustered context prototypes) from LPCAM. The idea is to remove the false positive pixels (e.g., the ``rail'' of ``train''~\cite{ood}). Therefore, our LPCAM not only captures the missing local features of the object but also mitigates the spurious features caused by confusing contexts.


LPCAM is a new operation to compute class activation maps based on clustered local prototypes. In principle, it can be taken as a generic substitute of the conventional CAM in CAM-based WSSS methods. To evaluate LPCAM on different WSSS methods (as they use different backbones, pre-training strategies or extra data), we conduct extensive experiments by plugging it in multiple methods: the popular refinement method IRN~\cite{irn}, the top-performing AMN~\cite{amn}, the saliency-map-based EDAM~\cite{edam}, and the transformer-arch-based MCTformer~\cite{mctformer},
on two popular benchmarks of semantic segmentation, PASCAL VOC 2012~\cite{voc} and MS COCO 2014~\cite{mscoco}. 


\noindent
\textbf{Our Contributions} in this paper are thus two-fold. 
1)~A novel method LPCAM that leverages non-discriminative local features and context features (in addition to discriminative ones) to generate class activation maps with better coverage on the complete object.
2) Extensive evaluations of LPCAM by plugging it in multiple WSSS methods, on two popular WSSS benchmarks.