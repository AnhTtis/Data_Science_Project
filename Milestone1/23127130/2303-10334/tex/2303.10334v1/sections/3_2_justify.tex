
\subsection{Justifications}
\label{sec_justfication}
\input{figures/fig_norm}
We justify the effectiveness of LPCAM from two perspectives: clustering and normalization. We use the ``bird'' example shown in Figure~\ref{fig_justification}. In (a), we consider only three local regions ($\bm{x_1}$, $\bm{x_2}$ and $\bm{x_3}$), for simplicity. Their semantics are respectively: $\bm{x_1}$ as ``head'' (a discriminative object region), $\bm{x_2}$ as ``tail'' (a non-discriminative region), and $\bm{x_3}$ as ``sky'' (a context region). We suppose $f(\bm{x_1})$, $f(\bm{x_2})$, and $f(\bm{x_3})$ are 3-dimensional local features (2048-dimensional features in our real implementation) respectively extracted from the three regions, where the three dimensions represent the attributes of ``head'', ``tail'' and ``sky'', respectively. The discriminativeness is reflected as follows. First, $f(\bm{x_1})$ extracted on the head region $\bm{x_1}$ has a significantly higher value of the first dimension than $f(\bm{x_2})$ and $f(\bm{x_3})$. Second, $f(\bm{x_2})$ has the highest value on the second dimension but this value is lower than the first dimension of $f(\bm{x_1})$, because ``tail'' ($\bm{x_2}$) is less discriminative than ``head'' ($\bm{x_1}$) for recognizing ``bird''. In (b), we assume three local class prototypes (``head'', ``tail'', and ``sky'') are selected.

\noindent
\textbf{Clustering.} 
As shown in Figure~\ref{fig_justification}(a), 
\textbf{in LPCAM}, $\bm{x_1}$ and $\bm{x_2}$ go to different clusters. This is determined by their dominant feature dimensions, i.e., the first dimension in $f(\bm{x_1})$ and the second dimension in $f(\bm{x_2})$. Given all samples of ``bird'', their features clustered into the ``head'' cluster all have high values in the first dimension, and features in the ``tail'' have high values in the second dimension. The centers of these clusters are taken as local prototypes and equally used for generating LPCAM. Sliding each of prototypes over the feature map block (of an input image) can highlight the corresponding local region. The intuition is each prototype works like a spatial-wise filter that amplifies similar regions and suppresses dissimilar regions.

However, \textbf{in CAM}, the heatmap computation uses the classifier weights biased on discriminative dimensions\footnote{We empirically validate this in the supplementary materials.}. It is because the classifier is learned from the global average pooling features, e.g., $f_{GAP}=\frac{1}{3}(f(\bm{x_1})+f(\bm{x_2})+f(\bm{x_3}))=[12,5,4]$ biased to the ``head'' dimension. As a result, only discriminative regions (like ``head'' for the class of ``bird'') are highlighted on the heatmap of CAM.


\noindent
\textbf{Normalization.} 
We justify the effectiveness of \textbf{LPCAM} by presenting the normalization details in Figure~\ref{fig_justification}(b). We denote the two class prototypes (``head'' and ``tail'') as $\mathbf{F'_1}, \mathbf{F'_2}$ and the context prototype (``sky'') as $\mathbf{B'_1}$. Based on Eq.~\ref{eq:fg_bg} and Eq.~\ref{eq:LPCAM}, we have $\bm{A}(\bm{x})=\frac{1}{2}\sum_{i=1}^{2}sim(f(\bm{x}),\mathbf{F'_i}) - sim(f(\bm{x}),\mathbf{B'_1})$, where $\bm{x}$ denotes any local region. For simplicity, we use the first term for explanation: the prototype ``head'' $\mathbf{F'_1}$ has the highest similarity ($1.0$) to region $\bm{x_1}$ and the prototype ``tail'' $\mathbf{F'_2}$ has the highest similarity ($0.9$) to $\bm{x_2}$. $0.9$ and $1.0$ are very close. After the final maximum-value based normalization (as in the first formula of Eq.~\ref{eq:LPCAM}), they become $1.00$ and $0.83$, i.e., only a small gap between discriminative ($\bm{x_1}$) and non-discriminative ($\bm{x_2}$) regions. However, \textbf{CAM} (Eq.~\ref{eq:cam}) uses $\mathbf{w}^{\top}f(\bm{x})$, resulting a much higher activation value of $\bm{x_1}$ than $\bm{x_2}$, as $\mathbf{w}$ is obviously biased to ``head'', i.e., $130$ vs. $35$ (``tail''). After the final maximum-value based normalization, there is no change to this bias: $1.00$ and $0.27$---a large gap. In other words, the non-discriminative feature is closer to background $\bm{x_3}$, making the boundary between foreground and background blurry, and hard to find a threshold to separate them.

One may argue that ``separate $\bm{x_2}$ and $\bm{x_3}$'' can be achieved in either CAM or LPCAM if the threshold is carefully selected in each method. However, it is not realistic to do such ``careful selection'' for every input image. The general way in WSSS is to use a common threshold for all images. Our LPCAM makes it easier to find such a threshold, since its heatmap has a much clearer boundary between foreground and background. We conduct a threshold sensitivity analysis in experiments to validate this.
