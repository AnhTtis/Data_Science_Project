\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[accsupp]{axessibility}
%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{11304} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

%%MY PACKAGES
\usepackage{pgfplots}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{hyperref}
%\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{gensymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{tabu}
\usepackage{soul}
\usepackage{floatrow}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]

\newdimen\figrasterwd
\figrasterwd\columnwidth
\newcommand{\revised}[1]{{\color{black}#1}}
\newcommand{\ns}[1]{{\color{blue}#1}}
\newcommand{\mlv}[1]{{\color{purple}#1}}

\title{Data-efficient Large Scale Place Recognition with Graded Similarity Supervision}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{  Mar\'ia Leyva-Vallina\\
  University of Groningen\\
{\tt\small m.leyva.vallina@rug.nl}
\and
 Nicola Strisciuglio \\
University of Twente \\
{\tt\small n.strisciuglio@utwente.nl}
\and
   Nicolai Petkov \\
   University of Groningen \\
{\tt\small n.petkov@rug.nl}
}


% \pgfplotsset{compat=1.18}
\begin{document}


\maketitle


\begin{abstract}
Visual place recognition (VPR) is a fundamental task of computer vision for visual localization. Existing methods are trained using image pairs that either depict the same place or not. Such a binary indication does not consider continuous relations of similarity between images of the same place taken from different positions, determined by the continuous nature of camera pose. The binary similarity induces a noisy supervision signal into the training of VPR methods, which stall in local minima and require expensive hard mining algorithms to guarantee convergence. Motivated by the fact that two images of the same place only partially share visual cues due to camera pose differences, we deploy an automatic re-annotation strategy to re-label VPR datasets. We compute graded similarity labels for image pairs based on available localization metadata. Furthermore, we propose a new Generalized Contrastive Loss (GCL) that uses graded similarity labels for training contrastive networks. We demonstrate that the use of the new labels and GCL allow to dispense from hard-pair mining, and to train image descriptors that perform better in VPR by nearest neighbor search, obtaining superior or comparable results than methods that require expensive hard-pair mining and re-ranking techniques. 


%This affects the training of VPR methods, that usually stall in local minima and are thus coupled with expensive pair-mining strategies to select hard training pairs properly. Motivated by this, we propose automatic methods to re-label VPR datasets and exploit camera pose metadata or 3D information as proxy to estimate a graded similarity of image pair. 
%achieves

%We demonstrate that graded similarity labels can be used to compose training batches instead of using hard-pair mining, with a speed-up of the training up to $\sim$200x faster that existing methods. 

%\st{Visual place recognition is a challenging task of computer vision and a key component of visual-based localization and navigation systems.  State-of-the-art models are trained using image pairs or triplets labeled as \emph{similar} or \emph{dissimilar}, in a binary fashion. In practice, however, the similarity between two images is continuous rather than binary. Furthermore, training these models is computationally complex and involves costly hard-negative pair and triplet mining.  We propose a Generalized Contrastive loss (GCL) function for metric learning that exploits continuous image similarity to train siamese networks. We use camera position and orientation to re-annotate the MSLS dataset with the degree of similarity of image pairs. Siamese networks trained with the GCL function consistently outperform their counterparts trained using the traditional  contrastive loss. They achieve superior performance than more complex state-of-the-art approaches that use triplet loss, pair mining and re-ranking, and generalize well to unseen datasets. We do not use pair mining, considerably reducing memory and computation requirements compared to existing methods (5 hours to train a VGG backbone with GLC compared to 45 days to train a VGG-NetVLAD model, on an NVidia V100 gpu). The source code and MSLS re-annotation will be released.}
\end{abstract}



\section{Introduction}
\input{sections/1_intro}
%\section{Related works}

\input{sections/2_previouswork}

%\section{Methodology}
\input{sections/3_methodology}
%\section{Training data and automatic labelling}
%\input{sections/4_data}

%\section{Experimental framework}
\input{sections/5_experiments}
%\section{Results and discussion}
\input{sections/6_results}
%\section{Conclusions}

\input{sections/8_conclusions}


\begin{comment}
    
\section*{Acknowledgements}

We thank the Center for Information Technology of the University of Groningen for their support and for providing access to the Peregrine high performance computing cluster.
\end{comment}
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\newpage
\appendix

\input{sections/appendix.tex}
%\bibliographystyle{spmpsci}
%\bibliography{egbib}


\end{document}