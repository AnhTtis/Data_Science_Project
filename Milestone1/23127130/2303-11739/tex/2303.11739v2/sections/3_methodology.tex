\section{Generalized Contrastive Learning}
\label{sec:methodology}
\noindent\textbf{Preliminaries.} 
Contrastive approaches for metric learning in visual place recognition consider training a (convolutional) neural network $\hat{f}(x)$ so that the distance of the vector representation of similar (or dissimilar) images in a latent space is minimized (or maximized). In this work, we consider siamese networks optimized using a Contrastive Loss function~\cite{hadsell2006dimensionality}.

Let $x_i$ and $x_j$ be two input images, with $\hat{f}(x_i)$ and $\hat{f}(x_j)$ their descriptors. The distance of the descriptors in the latent space is the $L_2$-distance $d(x_i, x_j) = \left \| \hat{f}(x_i)-\hat{f}(x_j) \right \|_2$. The Contrastive Loss $\mathcal{L}_{CL}$ used to train the networks is defined as:
\begin{equation}
 \mathcal{L}_{CL}(x_i,x_j)=\begin{cases}
     \frac{1}{2}d(x_i, x_j)^2 ,& \text{if } y= 1\\
    \frac{1}{2}\max(\tau-d(x_i, x_j),0)^2,& \text{if } y=0
\end{cases}
\label{eq:contrastive}
\end{equation}
where $\tau$ is the margin, an hyper-parameter that defines a boundary between similar and dissimilar pairs. The ground truth label $y$ is binary: $1$ indicates a pair of similar images, and $0$ a not-similar pair of images.  In practice, however, a binary ground truth for similarity may cause the trained models to provide unreliable predictions. 


\noindent\textbf{Generalized Contrastive Loss. }
We reformulate the Contrastive Loss, using a generalized definition of pair similarity as a continuous value $\psi_{i,j} \in \left[0, 1 \right]$. We define the Generalized Contrastive Loss function $ \mathcal{L}_{GCL}$ as:

\begin{align}
  \begin{split} 
 \mathcal{L}_{GCL}(x_i,x_j)= \psi_{i,j}\cdot \frac{1}{2}d(x_i, x_j)^2 + \\ (1-\psi_{i,j}) \cdot \frac{1}{2}\max(\tau-d(x_i, x_j),0)^2
\end{split}
\label{eq:generalized_contrastive_loss}
\end{align}

In contrast to Eq.~\ref{eq:contrastive}, here the similarity $\psi_{i,j}$ is a continuous value ranging from 0 (completely dissimilar) to 1 (identical). 
By minimising the Generalized Contrastive Loss, the distance of image pairs in the latent space is optimized proportionally to the corresponding degree of similarity. 
%\begin{figure}[t!]
%    \centering
    %\includegraphics[width=\columnwidth]{figures/3_methodology/Siamese.pdf}
%    {\fontsize{10pt}{12pt}
%    \def\svgwidth{\textwidth}
%      \input{figures/3_methodology/architecture.tex}}
%    \caption{Sketch of a siamese architecture where $x_i$ and $x_j$ are the input images, $\hat{f}$ indicates the backbone with a pooling layer and $\hat{f}(x_i)$ and $\hat{f}(x_j)$ are the representations of the input images. They are used as input for the Generalized Contrastive Loss function $\mathcal{L}_{GCL}(x_i, x_j)$.}
%    \label{fig:siamese}
%\end{figure}

\noindent\textbf{Gradient of the GCL.}
% 
In the training phase, the loss function is minimized by gradient descent optimization and the weights of the network are updated by backpropagation. In the case of the Constrastive Loss function, the gradient is:
\begin{equation}
     \nabla  \mathcal{L}_{CL}(x_i,x_j)=\begin{cases}
     d(x_i, x_j) ,& \text{if } y= 1\\
    \min(d(x_i, x_j)-\tau,0),& \text{if } y=0
\end{cases}
\label{eq:contrastive_loss_grad}
\end{equation}
The gradient is computed for all positive pairs, and corresponds to a direct minimization of their descriptor distance in the latent space. For negative pairs, the update of the network weights takes place only in the case the distance of the descriptors is within the margin $\tau$. If the latent vectors are already at a distance higher than $\tau$, no update is done. %A partial similarity between pairs is therefore not considered, and the training process may incur in learning inconsistent representations. 



%In contrast, the proposed Generalized Constrastive Loss function takes into account the degree of similarity between the input images during the training process. Its gradient is:
The Generalized Contrastive Loss, instead, explicitly accounts for graded similarity $\psi_{i,j}$ of input pairs $(x_i,\!x_j)$ to weight the learning steps, and this reflects into the gradient:

\begin{equation}
     \nabla  \mathcal{L}_{GCL}(x_i,\!x_j)\!=\!\begin{cases}d(x_i,\!x_j)\!+\!\tau(\psi_{i,j}\!-\!1),& \text{if } d(x_i,\!x_j)\!<\!\tau\\
     d(x_i, x_j) \cdot \psi_{i,j},& \text{if } d(x_i,\!x_j)\!\geq\!\tau\\
     \end{cases}
     \label{eq:generalized_contrastive_loss_grad}
\end{equation} 

\noindent The gradient of $\mathcal{L}_{GCL}$ is modulated by the degree of similarity of the input image pairs, $\psi_{i,j}$. This results in an implicit regularization of learned latent space. In the supplementary material, we provide and compare plots of the latent space learned with the $\mathcal{L}_{CL}$ and $\mathcal{L}_{GCL}$ functions.  %\ns{ADD HERE A NICE OBSERVATION.}
At the extremes of the similarity range, for $\psi_{i,j}=0$ (completely dissimilar input images) and $\psi_{i,j}=1$ (same exact input images), the gradient is the same as in Eq.~\ref{eq:contrastive_loss_grad}.



%In the case the distance $d(x_i,x_j)$ is larger than the margin $\tau$, the weights of the network are updated proportionally to $\psi_{i,j}$. More interestingly, when the distance $d(x_i,x_j) < \tau$, the Generalized Contrastive Loss function has an intrinsic regularization effect of the learned latent space.
%The network weights are indeed updated so that the vector representations of the training images $x_i$ and $x_j$ are moved closer in the latent space if their ground truth degree of similarity is $\psi_{i,j} > 1-\frac{d(x_i,x_j)}{\tau}$, otherwise they are pushed away. % in the case $\psi_{i,j} < 1-\frac{d(x_i,x_j)}{\tau}$. 

