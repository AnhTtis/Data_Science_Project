\section{Experimental evaluation}
\subsection{Data}

\noindent\textbf{Mapillary Street Level Sequences. } The Mapillary Street Level Sequences (MSLS) dataset is designed for life-long large-scale visual place recognition. It contains about 1.6M images taken in $30$ cities across the world~\cite{msls}. Images are divided into a training (22 cities, 1.4M images), validation (2 cities, 30K images) and test (6 cities, 66k images) set. The dataset presents strong challenges related to images taken at different times of the day, in different seasons and with strong variations of camera viewpoint. The images are provided with GPS data in UTM format and compass angle. According to the original paper~\cite{msls}, two images are considered similar if they are taken by cameras located within $25m$ of distance, and with less than $40^{\circ}$ of viewpoint variation. We created (and will release) new ground truth labels for the training set of MSLS, with specification of the graded similarity of image pairs (see next Section for details).
We use the MSLS dataset to train our large-scale VPR models, which we test on the validation set, and also the private test set using the available evaluation server. 


%It includes challenging variations of camera viewpoint, season, time and illumination. Moreover, the images have been taken with different cameras. The training set contains over 500k query images and 900k map images, taken in 22 cities. The validation set consists of 19k map images and 11k query images, taken in two cities, and the test set has 39k map images and 27k query images, taken in six different cities.
 
%We evaluate train our models on the MSLS training set and evaluate them on the validation and test sets. For the former, we use the evaluation script published by the authors of the dataset. For the latter, since the ground truth is not publicly available, we submit our predictions to the official evaluation server. Following the protocol established by the authors of the dataset

\noindent\textbf{OOD test data for generalization. }  We use out-of-distribution (OOD) test sets, to evaluate the generalization abilities of our models and compare them with existing methods. We thus use the test and validation sets of several other benchmark datasets, namely the Pittsburgh30k~\cite{Arandjelovic2017}, Tokyo24/7~\cite{Torii-PAMI2015}, RobotCar Seasons v2~\cite{Maddern2017,sattler2018benchmarking} and  Extended CMU Seasons~\cite{Badino2011,sattler2018benchmarking} datasets. In the supplementary material, we also report results on Pittsburgh250k and TokyoTM~\cite{Torii-PAMI2015}. %To compare with the results of several other metric loss functions, we follow the evaluation protocol used in~\cite{thoma2020soft} on the CMU Seasons dataset~\cite{Badino2011}.

\noindent\textbf{TB-Places and 7Scenes.} We carried out experiments also using the TB-Places~\cite{leyvavallina2019access} and 7Scenes~\cite{Shotton2013} datasets, which were recorded in small-scale environments. We train models on them and report the results in the supplementary material.
TB-Places was recorded in an outdoor garden over two years and contains challenges related to drastic viewpoint variations, as well as illumination changes, and scenes mostly filled with repetitive texture of green color. Each image has 6DOF pose metadata. The 7Scenes dataset is recorded in seven indoor environments. It contains 6DOF pose metadata for each image and  a 3D pointcloud of each scene.

The different format and type of metadata, namely 6DOF camera pose and 3D pointclouds of the scenes, are of interest to investigate different ways to estimate the ground truth graded similarity of image pairs. In the following, we present techniques to automatically re-label VPR datasets, when 6DOF pose or 3D pointclouds metadata are available.
%Apart from being an extra test bed, with these two datasets we show that we 

%This dataset was designed for place recognition in garden environments~\cite{leyvavallina2019access,leyvavallina2019caip}. It contains images taken by a rover robot in a garden at the University of Wageningen, the Netherlands, over the course of three years. The dataset was collected for the TrimBot2020 project~\cite{strisciuglio2018trimbot2020}. It includes drastic viewpoint variations, as well as illumination changes. The garden is a very challenging small environment with repetitive textures. 

%The dataset consists of three subsets, i.e. W16, that contains 41k images taken in 2016; W17, that includes 11k images taken in 2017; and W18, that has 23k images taken in 2018. As in~\cite{leyvavallina2019caip} we use the W17 subset to train our models.
%
%We design two experiments to evaluate our models. For the first one we establish W17 as map set (11k images) and W18 as query (23k images). With this configuration we aim to test the robustness of our models w.r.t. changes between the map and the query sets. For the second experiment, we divide W18 into query (17k images) and map (6k images) to test the generalization capabilities of our models in the case both map and query sets were not used for training. In Fig.~\ref{fig:tb_places_test_sets}, we show a sketch of the trajectory that the robot covered in the TrimBot2020 garden for the recording of the reference map (blue trajectory) and query (orange trajectory) images. It is worth pointing out that the query images were taken from locations not covered by map images, thus including substantial viewpoint variations.

%\noindent\textbf{7Scenes.} It is a benchmark dataset for indoor camera localization algorithms~\cite{Shotton2013}. It includes 26k training images and 17k test images, taken in seven different environments. Each image has an associated ground truth 6DOF camera pose. Additionally, a 3D reconstruction of each scene is available. We use it to test our models for visual place recognition in indoor environments. For evaluation purposes, we define an image pair as a positive match if their annotated degree of similarity is higher than $50\%$. We use the training set as map, and the test set as query.


\subsection{Graded similarity labels}


%Existing datasets provide labels for image pairs in a binary fashion, while the optimization of the Generalized Contrastive Loss function relies on a ground truth for image pairs defined in the range $[0,1]$. 
%
%We propose to compute the graded similarity between two images, by estimating the overlap of the field-of-view (FoV) of the cameras taking the pictures. 




%We estimate the similarity of two images by approximating a measure of the overlap of their two-dimensional Field-of-View (FoV) in the horizontal plane.
%Let us consider a camera with a FoV defined by the angle $\theta$ and radius $r$, positioned in an environment according to a 2D translation vector $\mathbf{t}$ with respect to the origin of the reference system. The camera is oriented at an angle $\alpha$ with respect to the north direction of the reference system. We define the 2D FoV as the sector of the circle denoted by the center $(t_0,t_1)$ and radius $r$ enclosed in the angle range delimited by $[\alpha - \frac{\theta}{2}, \alpha +\frac{\theta}{2}]$ (see Fig.~\ref{fig:fov_graph}).







Images of the same place can be taken from different positions, i.e. with different camera pose, and share only part of the visual cues. On the basis of the amount of shared characteristics among images, we indeed tend to perceive images more or less similar~\cite{Desolneux2008}. 
%We tend to perceive two images as more or less related, according  to the amount of shared characteristics~\cite{}. 
Actual labeling of VPR datasets do not consider this and instead mark two images either similar (depicting the same place) or not (depicting different places). This does not take into account continuous relations between images, which are induced by the continuous nature of camera pose. 
%The continuous nature of camera pose is not taken into account in actual labeling of VPR datasets, which . In reality, we 

We combine the concept of perceived visual similarity with the continuous nature of camera pose, and design a method to automatically relabel VPR dataset by annotating the graded similarity of image pairs\footnote{We release the labels at \url{https://github.com/marialeyvallina/generalized_contrastive_loss}.}. We approximate the similarity between two images via a proxy, namely measuring the overlap of the field of view of the cameras or 3D information associated to the images. % (see Fig.~\ref{fig:examples-gt}).

%The principle of similarity states that items which share a visual characteristic are perceived as more related than items that are dissimilar.
\begin{figure}[t!]
  \centering
  %\parbox{\figrasterwd}{
   % \parbox{.3\figrasterwd}{%
      \begin{subfigure}{.2\textwidth}%\includegraphics[ width=.6\textwidth]{figures/twod_fov_vert.eps}
      \def\svgwidth{\textwidth}
      \input{figures/4_data/twod_fov_tex.pdf_tex}
      \caption{}
      \label{fig:fov_graph}
      \end{subfigure}
      \hspace{.3cm}
      \begin{subfigure}{.3\textwidth}
      \includegraphics[width=\textwidth, trim=2.5cm 2cm 2.5cm 3.25cm, clip]{figures/4_data/msls_0_40_55_63.pdf}
      \vspace{.8mm}
                \caption{}
            \label{fig:msls_0m_40deg}
          \end{subfigure}
          \hspace{.3cm}
     % \vskip1em
      \begin{subfigure}{0.3\textwidth}
      \includegraphics[width=\textwidth, trim=2.5cm 2cm 2.5cm 3.5cm, clip]{figures/4_data/msls_25_0_45_01.pdf}
      \vspace{.8mm}
        \caption{}
        \label{fig:msls_25m_0deg}
        \end{subfigure}
  %  }
    \label{fig:fov}
    \caption{(a) FoV with angle $\theta$ and radius $r$. The point $\mathbf{t}$ is the camera location in the environment, and $\alpha$ is the camera orientation in the form of a compass angle with respect to the north $N$. (b) An example of FoV overal for two cameras %(with  $\theta=90^{\circ}$ and $r=50m$)
    in the same position and with orientations $40 ^{\circ}$ apart. (c) An example of FoV overlap for two cameras %(with  $\theta=90^{\circ}$ and $r=50m$) 
    located 25m apart but with the same orientation. }
 % }
\end{figure}

\begin{figure}[t!]
  \centering
    \begin{subfigure}{0.31\textwidth}
        \includegraphics[height=1.8cm,width=\textwidth]{figures/4_data/london_query_75_5.jpg}
                  \caption{}
\end{subfigure}
    %
    \begin{subfigure}{0.31\textwidth}
        \includegraphics[height=1.8cm,width=\textwidth]{figures/4_data/london_db_75_5.jpg}
                      \caption{}

    \end{subfigure}
    %
    \begin{subfigure}{0.31\textwidth}
        \includegraphics[width=\textwidth]{figures/4_data/london_fov_75_5.png}
\caption{FoV overlap 75.5\%}
\end{subfigure}
    
    \begin{subfigure}{0.31\textwidth}

      \includegraphics[width=\textwidth,height=1.8cm]{figures/4_data/W18_positive_q.png}
                    \caption{}
\end{subfigure}
      \begin{subfigure}{0.31\textwidth}

\includegraphics[width=\textwidth,height=1.8cm]{figures/4_data/W18_positive_db.png}
                    \caption{}
\end{subfigure}
      \begin{subfigure}{0.31\textwidth}

      \includegraphics[width=\columnwidth, trim=1cm 2cm 1cm 2cm, clip]{figures/4_data/w18_positive_74_21.png}
        \caption{FoV overlap 75\%}
\end{subfigure}
      
      \begin{subfigure}{0.31\textwidth}

      \includegraphics[width=\textwidth,height=1.8cm]{figures/4_data/example_fov_pumpkin_query.png}
                  \caption{}
\end{subfigure}
      \begin{subfigure}{0.31\textwidth}

\includegraphics[width=\textwidth,height=1.8cm]{figures/4_data/example_fov_pumpkin_map.png}
              \caption{}
\end{subfigure}
      \begin{subfigure}{0.31\textwidth}

\includegraphics[width=\textwidth]{figures/4_data/example_fov_pumpkin_noaxis.png}
        \caption{FoV overlap 50\%}

      \end{subfigure}

    \caption{Examples of graded similarity estimated for MSLS (first row) and TB-Places (second row) with the FoV overlap, and 7Scenes (third row) with 3D overlap (magenta color points). }
    \label{fig:examples-gt}
 % }
\end{figure}

\noindent\textbf{Graded similarity for MSLS and TB-Places: field of view overlap. } For MSLS and TB-Places dataset, images are provided with camera pose metadata
in the form of a vector $\mathbf{t}$ and orientation $\alpha$. The MSLS has UTM data and compass angle information associated to the images, while in TB-Places the images are provided with precise camera pose recorded with a laser tracker and IMU.

For an image, we build the field of view (FoV) of the camera as the sector of the circle centered at $\mathbf{t}$ with radius $r$, delimited by the angle range $[\alpha - \frac{\theta}{2}, \alpha +\frac{\theta}{2}]$ (see Fig.~\ref{fig:fov_graph}), where $\theta$ is the nominal size of the FoV of the camera concerned. The FoV overlap is the intersection-over-union (IoU) of the FoV of the cameras. In the first and second row of Figure~\ref{fig:examples-gt}, we show examples of the graded similarity estimated for pairs in the MSLS and TB-Places datasets. This approach differs from the camera frusta overlap~\cite{balntas2018relocnet} that needs 3D overlap measures for precise camera pose estimation. %, and 

%Let us consider a camera into an environment has pose indicated by a vector $\mathbf{t}$ and orientation $\alpha$. One can think of the FoV of the camera as the sector of the circle centered at $\mathbf{t}$ with radius $r$, delimited by the angle range $[\alpha - \frac{\theta}{2}, \alpha +\frac{\theta}{2}]$ (see Fig.~\ref{fig:fov_graph}), where $\theta$ is the nominal size of the FoV of the camera concerned. The FoV overlap corresponds to the intersection-over-union (IoU) of their FoVs. Differently from~\cite{balntas2018relocnet,ding2019camnet} that compute the overlap of  3D camera frusta for camera localization, we relax this concept by only considering the overlap of the camera FoVs in the horizontal plane.

%We consider the UTM data and the compass angle associated with the images as the translation vector $t$ and the orientation of the cameras $\alpha$, respectively, and use them to estimate the FoV overlap. Camera intrinsic parameters are not provided with the MSLS dataset. We estimate the nominal FoV of the cameras used to take the pictures in the MSLS dataset to be $90^\circ$. In the supplementary materials, we provide details of the labeling process and show several examples of image similarity, defined using prior camera geometry information associated to the images.


\noindent\textbf{Graded similarity for 7Scenes: 3D overlap. } The 7Scenes dataset has a 3D pointcloud for each scene, and 6DOF camera pose associated to the images. In this case, we can estimate the similarity overlap differently from the cases above. 
We project a pair of images onto the 3D pointcloud, so that we select the points associated to the two images and measure their intersection-over-union (IoU) as a measure of the image pair similarity. A similar strategy based on maximum inliers was used for hard-pair mining in~\cite{radenovic2018fine}.
In the third row of Figure~\ref{fig:examples-gt}, we show an example of graded similarity estimation for two images in the 7Scenes dataset.
% of the sets of 3D points associated with the two images.

%image with an associated 6DOF camera pose onto the reconstructed pointcloud of the environment. 

%When a 3D reconstruction of a concerned environment is available, we propose to estimate the degree of similarity of image pairs by computing the \emph{3D Field-of-View overlap}. We project a given image with an associated 6DOF camera pose onto the reconstructed pointcloud of the environment. 
%We select the subset of 3D points that falls within the boundaries of the image as the image 3D FoV. For an image pair, we compute their 3D FoV overlap as the intersection-over-union (IoU) of the sets of 3D points associated with the two images. \revised{This computation is similar to the \emph{maximum inliers} measure proposed in~\cite{radenovic2018fine}, where it was used as part of a pair-mining strategy that also involved the computation of a distance in the latent-space.} We consider, instead, the computed 3D FoV overlap as a measure of the degree of similarity of a pair of images\revised{, and use it to compute the graded ground truth.} 



% For re-annotating the MSLS dataset, we consider the UTM data as the translation vector $(t_0,t_1)$ and the compass angle as the orientation $\alpha$ necessary to estimate the 2D FoV of the cameras. The FoV angle of the cameras and the intrinsics are not provided. We thus estimate the value of the FoV angle $\theta$ according to the following reasoning. 


%We consider as positives the image pairs with determined ground truth FoV overlap higher than $50\%$, and the rest as negatives. More specifically, the negative pairs with a similarity higher than $0\%$ are soft negatives, while the pairs with a similarity degree equal to $0\%$ are hard negatives.







\subsection{Place recognition pipeline }
\noindent\textbf{Embeddings.}  We use a fully convolutional backbone (ResNet~\cite{he2016deep}, VGG16~\cite{simonyan2015very} and ResNeXt~\cite{xie2017aggregated}) with a GeM pooling layer~\cite{radenovic2018fine}, which receives as input an image  $x \in R^{w_n \times h_n \times 3}$, and outputs a  representation $\hat{f}(x)  \in R^{d_m}$, where $d_m$ is the number of kernels of the last convolutional layer. We train $\hat{f}(x)$ using a contrastive learning framework. 

\noindent\textbf{Training batch composition.} Batch composition is an important part of model training. %in any classification problem, especially for those with unbalanced training sets. In the case of siamese
For contrastive architectures, the selection of meaningful image tuples is crucial for the correct optimization of the model. If the selected tuples are too challenging, the training might become unstable~\cite{shi2016embedding}. If they are too easy, the learning might stall. This, coupled with binary pairwise labels, makes necessary to use complex descriptor-based mining strategies to ensure model convergence. The \emph{hard-negative mining} strategy needed to train contrastive networks~\cite{Arandjelovic2017,hausler2021patch,liu2019stochastic} periodically computes the descriptor of all training images and their pairwise distance to select certain pairs (tuples) of images to be used for the subsequent training steps. This is a memory- and computation-expensive procedure. %For each query image, they select a set of potential positive map images and a set with its 10 hardest negative map images. The best combination of positive and negative samples is selected by means of their contribution to a Triplet Ranking Loss function. This implies that all the images need to be forwarded to the network to compute the value of the loss function, although only three contribute to the learning.

We do not perform hard-pair mining. We instead compose the training batches taking into account the graded similarity labels that we computed. %using the proposed proxies to estimate image similarity. 
We balance the pairs in the training batches on the basis of their annotated degree of similarity.
%We argue that a more informative ground truth can dispense with the need of these complex pair selection procedures, and instead approach the task as an unbalanced classification problem, and ensure that all classes are represented equally, without the need of unnecessary forward passes. 
%We consider as positives the image pairs with determined ground truth FoV overlap higher than $50\%$, and the rest as negatives. More specifically, the negative pairs with a similarity higher than $0\%$ are soft negatives, while the pairs with a similarity degree equal to $0\%$ are hard negatives. 
For each batch, we make sure to select $50\%$ of positive pairs (similarity higher than $50\%$), $25\%$ of soft negative samples (similarity higher than $0\%$ and lower than $50\%$) and $25\%$ of hard negatives ($0\%$ similarity) -- see Section~\ref{sec:discussion} for results. %We elaborate on the effects of different batch composition in Section~\ref{sec:discussion}.
 
\begin{comment}

%\revised{A continuous similarity ground truth can lead to improved learning~\cite{thoma2020soft}, but it does not dispense with the need for hard mining strategies by itself. When relying only on the GPS without taking in consideration the orientation, for instance, the ground truth annotations are likely to contain visual inaccuracies, i.e. two images taken 20cm apart facing in opposite directions will not be visually similar, despite having a high annotated similarity. These contradictions make necessary the use of further curation of the selected pairs or triplets by means of a costly hard mining strategy.}

%We argue that a better curated and more informative ground truth can replace these complicate mining techniques. We use our graded similarity annotations, presented in Sections~\ref{sec:2Dfov} and~\ref{sec:3Dfov} to select the image pairs that compose a training batch. For our training process, we ensure that each training batch contains a balanced amount of positive and negative pairs. In the case of the negative pairs, we also ensure that half of them are soft negatives (i.e. their annotated similarity is higher than 0). \revised{We explore several batch composition strategies and discuss them in Section~\ref{sec:discussion}.} This means that each selected image pair consist of a query and a match image, and we do not require complicated mining strategies or to compute any latent representations to form the pairs. Hence, we are able to train our models with a batch size of 64.
\end{comment}




\noindent\textbf{Image retrieval.} Let us consider a set $X$ of reference images with a known camera location, and a set $Y$ of query images taken from unknown positions. In order to localize the camera that took the query images, similar images to the query are to be retrieved from the reference set. We compute the descriptors of the reference images $\hat{f}(x)  \,\forall x \in X$, and of the query images $\hat{f}(y)\, \forall y \in Y$. For a given query descriptor $\hat{f}(y)$, image retrieval is performed by nearest neighbor search within the reference descriptors $\hat{f}(x)\, \forall x \in X$, retrieving $k$ images ranked by the closest descriptor distance.



\subsection{Performance measures}
We apply widely used place recognition evaluation protocols and consider a query as correctly identified if any of the top-k retrieved images are annotated as a positive match~\cite{Sattler2012,Arandjelovic2017,msls}. We computed the following metrics.
For the MSLS, Pittsburgh30k, Tokyo24/7 (Pittsburgh250k, TokyoTM, TrimBot2020 and 7Scences in the supplementary material) we compute the \textbf{Top-k recall (R@k)}. It measures the percentage of queries for which at least a correct map image is present among their $k$ nearest neighbors retrieved. For the RobotCar Seasons v2 and the Extended CMU datasets, we compute the \textbf{percentage of correctly localized queries}. It measures the amount of images that are correctly retrieved for a given translation and rotation threshold. %\textbf{Average Precision (AP),} an approximation of the area under the precision-recall curve for the classification of all the possible pairs of images. We use this measure for the  7Scenes dataset~\cite{Shotton2013}.

%\newpage
