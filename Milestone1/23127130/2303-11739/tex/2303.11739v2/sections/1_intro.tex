%Page and a half approx
%Intro to the problem


Visual place recognition (VPR) is  an important task of computer vision, and a fundamental building block of navigation systems for autonomous vehicles~\cite{Lowry2016,Zaffar2021}. It is approached either with structure-based methods, namely Structure-from-Motion~\cite{colmap} and SLAM~\cite{Milford2012}, or with image retrieval~\cite{Arandjelovic2017,hausler2021patch,radenovic2018fine,radenovic2018revisiting,leyvavallina2019caip,weyand2020google}. The former focus on precise relative camera pose estimation~\cite{Sattler2012,Sattler2019}. 
The latter aim at learning image descriptors for effective retrieval of similar images to a given query in a nearest search approach~\cite{benchmarkingir3DV2020}. The goal of descriptor learning is to ensure images of the same place to be projected onto close-by points in a latent space, and images of different places to be projected onto distant points~\cite{milford15metric,Liu2019metric,Chen2022surveyretrieval}. Contrastive~\cite{radenovic2018fine,leyvavallina2019access} and triplet~\cite{Arandjelovic2017, Lopez-Antequera2017,liu2019stochastic,Peng2021} loss were used for this goal and resulted in state-of-the-art performance on several VPR benchmarks. 
%The metric learning task is carried out considering

\begin{figure}[t!]
    \centering
    %\includegraphics[width=\columnwidth]{figures/3_methodology/Siamese.pdf}
    {\fontsize{10pt}{12pt}
    \def\svgwidth{\textwidth}
      \input{figures/1_intro/example-intro}}
      \vspace{-6mm}
    \caption{(a) A place in the city of Amman. (b) An image taken 6m away is labeled as positive (same place), while (c) an image taken 25.6m away is labeled as negative (not the same place) despite sharing a lot of visual cues.}
    \label{fig:similarity-example}
\end{figure}

VPR methods are normally trained using image pairs labelled to indicate they either depict the same place or not, in a binary fashion. In practice, images of a certain place can be taken from different positions, i.e. with a different camera pose, and thus share only a part of their visual cues (or surface in 3D). In existing datasets, two images are usually labeled to be of the same place (positive) if they are taken within a predefined range (usually $25m$) computed using e.g. GPS metadata. This creates ambiguous cases. For instance, Figure~\ref{fig:similarity-example} shows a reference image (a) of a place and two other pictures taken $6m$ (b) and $25.6m$ (c) away from its position. The images are respectively labeled as positive and negative match, although they share many visual cues (e.g. the building on the right).
Binary labels are thus noisy and interfere with the training of VPR networks, that usually stall in local minima. To address this, resource- and time-costly hard pair mining strategies are used to compose the training batches. For example, training NetVLAD~\cite{Arandjelovic2017} on the Mapillary Street Level Sequences (MSLS) dataset~\cite{msls} can take more than 20 days on an Nvidia v100 gpu due to the complexity of pair mining.
We instead build on the observation that two images depict the same place only to a certain degree of shared cues, namely a degree of similarity, and propose to embed this information in new continuous labels for existing datasets that can be used to reduce the effect of noise in the training of effective VPR methods. % 
%f.i. by computing a measure of camera pose distance or overlap ratio of shared points on a 3D pointcloud. 

%In reality, two images have a certain  $\eta\%$ of appearance overlap. %being $100\%$ similar only in the case of same exact images. 
%This is not taken into account to train place recognition methods.
%The ill-posed use of binary similarity for image pairs %to perform (deep) metric learning 
%has drawbacks for the training process, stalling in local minima and requiring extra ad-hoc regularization or design efforts. The optimization of siamese networks heavily relies on expensive hard-pair mining strategies, which are resource-hungry and time-costly. For example, training NetVLAD~\cite{Arandjelovic2017} on the Mapillary Street Level Sequences (MSLS) data set~\cite{msls} takes about 45 days on an NVIDIA V100 GPU due to the pair mining and the complexity of the VLAD layer. Optimizing a ranking loss function, e.g. AP-GeM~\cite{revaud2019learning}, is also a very expensive process, difficult to extrapolate to other problems and large data sets. 

In this paper we exploit camera pose metadata or 3D information associated to image pairs as a proxy to estimate an approximate degree of similarity (hereinafter, graded similarity) between images of the same place, and use it to relabel popular VPR datasets. Graded similarity labels can be used to pick easy- and hard-pairs and compose training batches without complex pair-mining, thus speeding-up the training of VPR networks and enabling an efficient use of data.
Furthermore, we embed the graded similarity into a Generalized Contrastive Loss (GCL) function that we use to train a VPR pipeline. The intuition behind this choice is that the update of network weights should not be equal for all training pairs, but rather be influenced by their similarity. The representations of image pairs with larger graded similarity should be pushed together in the latent space more strongly than those of images with a lower graded similarity. The distance in the latent space is thus expected to be a better measure of ranking images according to their similarity, avoiding the use of expensive re-ranking to improve retrieval results. 
We validate the proposed approaches on several VPR benchmark datasets. % for indoor and outdoor scenarios.
To the best of our knowledge, this work is the first to use graded similarity for large-scale place recognition, and paying attention to data-efficient training. 

%We address the metric learning problem taking into account explicitly the graded similarity of image pairs. We define a Generalized Contrastive Loss function that weights the minimization (or maximization) of the descriptor distance of similar (or dissimilar) images proportionally to their similarity degree, which we compute as  overlap ratio of their camera field-of-view. We exploit extra information available with images, such as GPS coordinates and compass angle, as priors to estimate the camera field-of-view overlap as an approximate measure of the common visual cues. 
%The use of graded ground truth similarity provides an intrinsic regularization effect to the contrastive learning, which then does not require costly hard-negative pair mining and results in much faster convergence. 

%A siamese architecture with a VGG-16 backbone converges after 5 hours of training on the MSLS data set (using 500k image pairs only, corresponding to one epoch), and achieves better results than NetVLAD with the same backbone that trains for 45 days (and converges after 22 epochs). The data-efficiency and less computation requirements of our method allow to train more complex backbones, e.g. ResNeXt~\cite{xie2017aggregated}, on a single NVIDIA V100 GPU. We obtain superior results than existing methods on the challenging MSLS data set, with no need of hard-pair mining or re-ranking of retrieved images. These results couple with robustness to domain shifts, namely strong generalization capabilities to other data sets.
We summarize the contributions of this work as: 
\begin{itemize}
    \item new labels for VPR datasets indicating the graded similarity of image pairs. We computed the labels with automatic methods that use camera pose metadata included with the images or 3D surface information;
    \item a generalized contrastive loss (GCL) that exploits graded similarity of image pairs to learn effective descriptors for VPR;
    \item an efficient VPR pipeline trained without hard-pair mining, and that does not require re-ranking. Training our pipeline with a VGG-16 backbone converges $\sim100x$ faster than NetVLAD with the same backbone, achieving higher VPR results on several benchmarks. The efficiency of our scheme enables training larger backbones in a short time.
    %\item a training strategy that does not require hard-pair mining, reducing training time and computation requirements, and achieving higher results than existing methods
    % \item the use of camera position priors to estimate a ground truth similarity for image pairs based on their camera field-of-view overlap (to be released together with the trained models)
\end{itemize}

%\noindent Furthermore, we extend the computation of graded image similarity labels to the 7Scenes and TB-Places dataset, for which we report results in the Appendix. 

 %and 2) the use of world- and geometry-aware priors to compute an approximated ground truth similarity for image pairs based on their camera field-of-view overlap, which we will release together with the trained models. %\ns{TO HIGHLIGHT BETTER}

% In the rest of the paper, we discuss existing literature in Section~\ref{sec:related} and present the Generalized Contrastive Loss function in Section~\ref{sec:methodology}. and the results that we achieved and a comparison with state-of-the-art methods in Section~\ref{sec:results}. We discuss the implications and benefits of reformulating the metric learning problem using graded similarity in Section~\ref{sec:discussion}, and finally draw conclusions in Section~\ref{sec:conclusions}.
 
 
 






















