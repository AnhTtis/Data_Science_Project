\section{Conclusions}
\label{sec:conclusions}
We extended the learning of image descriptors for visual place recognition by using measures of camera pose similarity and 3D surface overlap as proxies for graded image pair similarity to re-annotate existing VPR datasets (i.e. MSLS, 7Scenes and TB-Places). We demonstrated that the new labels can be used to effectively compose training batches without the need of hard-pair mining, decisively speeding-up training time while reducing memory requirements.
Furthermore, we reformulated the Contrastive Loss function, proposing a Generalized Contrastive Loss (GCL). The GCL exploits the graded similarity of image pairs, and contributes to learning way better performing image descriptors for VPR than those of other losses that are not designed to use graded image similarity labels (i.e. triplet, quadruplet and their variants) and that require hard-pair mining during training.  
Models trained with the GCL and new graded similarity labels obtain comparable or higher results that several existing VPR methods, including those that apply re-ranking of the retrieved images, while keeping a more efficient use of the data, training time and memory. We achieved good generalization to unseen environments, showing robustness to domain shifts on the Pittsburgh30k, Tokyo 24/7, RobotCar Seasons v2 and Extended CMU Seasons datasets.
%
The combination of graded similarity annotations and a loss function that can embed them in the training
paves a way to learn more effective descriptors for VPR in a data- and resource-efficient manner.


%\st{We have proposed a Generalized Contrastive Loss function for metric learning, and showed how to effectively train siamese networks for visual place recognition by relying on a graded measure of similarity between image pairs. We have demonstrated that networks consisting of a fully convolutional backbone and a GeM pooling, when trained using the proposed GCL function consistently outperform their counterparts trained with a classical binary contrastive loss, and other existing loss functions, i.e. triplet, quadruplet and their variants, while not requiring hard-negative pair mining, thus substantially saving memory and computational resources. Furthermore, our models outperform existing visual place recognition approaches, including those performing heavy post-processing as re-ranking of the retrieved images. 
%As opposed to previous works, the training procedure that we implement does not require a complex pair mining process:  we only ensure that training batches are composed by positive and negative pairs with fairly-distributed degrees of similarity. Our models generalize well to unseen environments, showing robustness to domain shifts as we demonstrated on the Pittsburgh\revised{30k, Tokyo 24/7, RobotCar Seasons v2 and Extended CMU Seasons} datasets. We showed that a reformulation of the contrastive learning framework, explicitly considering image similarity as a graded property, paves a way to learn more effective metric functions in a data- and resource-efficient way.}