\label{sec:data}

%1 page text, LOTS of figures
%2-D FoV example, gen√©rico, MSLS, TB-Places
%3-D pointcloud IOU example
\subsection{The Mapillary Street Level Sequences dataset}
This is a large scale place recognition dataset that contains images taken in 30 different cities across six continents~\cite{msls}. It includes challenging variations of camera viewpoint, season, time and illumination. Moreover, the images have been taken with different cameras. The training set contains over 500k query images and 900k map images, taken in 22 cities. The validation set consists of 19k map images and 11k query images, taken in two cities, and the test set has 39k map images and 27k query images, taken in six different cities.
 
We evaluate train our models on the MSLS training set and evaluate them on the validation and test sets. For the former, we use the evaluation script published by the authors of the dataset. For the latter, since the ground truth is not publicly available, we submit our predictions to the official evaluation server. Following the protocol established by the authors of the dataset~\cite{msls}, two images are considered as similar if they are taken by cameras located within $25m$ of distance, and with less than $40^{\circ}$ of viewpoint variation.

\subsection{Re-annotating with the 2D FoV overlap}
The optimization of the Generalized Contrastive Loss function relies on the ground truth similarity of image pairs defined in the range $[0,1]$.

\begin{comment}
\begin{figure}[t!]
  \centering
  %\parbox{\figrasterwd}{
   % \parbox{.3\figrasterwd}{%
      \subfloat[]{%\includegraphics[ width=.6\textwidth]{figures/twod_fov_vert.eps}
      \def\svgwidth{.24\textwidth}
      \input{figures/4_data/twod_fov_tex.pdf_tex}
      \label{fig:fov_graph}
      }\hspace{-.15cm}
      \subfloat[]{\includegraphics[width=.35\textwidth, trim=2.5cm 2cm 2.5cm 3.25cm, clip]{figures/4_data/msls_0_40_55_63.pdf}
          \label{fig:msls_0m_40deg}
          }
     % \vskip1em
      \subfloat[]{\includegraphics[width=.35\textwidth, trim=2.5cm 2cm 2.5cm 3.5cm, clip]{figures/4_data/msls_25_0_45_01.pdf}
          \label{fig:msls_25m_0deg}}
  %  }
    \label{fig:fov}
    \caption{(a) 2D Field-of-View representation with angle $\theta$ and radius $r$. The point $(t_0,t_1)$ is the camera location in the environment, and $\alpha$ is the camera orientation in the form of a compass angle with respect to the north $N$. A (b) soft positive match: two cameras (with  $\theta=90^{\circ}$ and $r=50m$) in the same position but with orientations $40 ^{\circ}$  apart. A (c) soft negative example: two cameras (with  $\theta=90^{\circ}$ and $r=50m$) located 25m apart but with the same orientation. }
 % }

\end{figure}

\end{comment}
\begin{figure}[t!]
  \centering
  %\parbox{\figrasterwd}{
   % \parbox{.3\figrasterwd}{%
      \begin{subfigure}%\includegraphics[ width=.6\textwidth]{figures/twod_fov_vert.eps}
      \def\svgwidth{.24\textwidth}
      \input{figures/4_data/twod_fov_tex.pdf_tex}
      \label{fig:fov_graph}
      \end{subfigure}
      \hspace{-.15cm}
      \begin{subfigure}
      \includegraphics[width=.35\textwidth, trim=2.5cm 2cm 2.5cm 3.25cm, clip]{figures/4_data/msls_0_40_55_63.pdf}
          \label{fig:msls_0m_40deg}
          \end{subfigure}
     % \vskip1em
      \begin{subfigure}\includegraphics[width=.35\textwidth, trim=2.5cm 2cm 2.5cm 3.5cm, clip]{figures/4_data/msls_25_0_45_01.pdf}
          \label{fig:msls_25m_0deg}\end{subfigure}
  %  }
    \label{fig:fov}
    \caption{(a) 2D Field-of-View representation with angle $\theta$ and radius $r$. The point $(t_0,t_1)$ is the camera location in the environment, and $\alpha$ is the camera orientation in the form of a compass angle with respect to the north $N$. A (b) soft positive match: two cameras (with  $\theta=90^{\circ}$ and $r=50m$) in the same position but with orientations $40 ^{\circ}$  apart. A (c) soft negative example: two cameras (with  $\theta=90^{\circ}$ and $r=50m$) located 25m apart but with the same orientation. }
 % }

\end{figure}

\begin{figure*}[t!]
    \centering
\begin{subfigure}%[Query]{
    \includegraphics[width=.22\textwidth]{figures/4_data/london_query_75_5.jpg}
    \label{fig:msls_london_query}
    \end{subfigure}
     \begin{subfigure}
    \includegraphics[width=.22\textwidth]{figures/4_data/tokyo_query_50_31.jpg}
    \label{fig:msls_tokyo_query}
    \end{subfigure}
    \begin{subfigure}
    \includegraphics[width=.22\textwidth]{figures/4_data/sf_query_16_78.jpg}
    \label{fig:msls_sf_query}
    \end{subfigure}
    \begin{subfigure}
    \includegraphics[width=.22\textwidth]{figures/4_data/trondheim_q_0_0.jpg}
    \label{fig:msls_trondheim_query}
    \end{subfigure}
    
   \begin{subfigure}
    \includegraphics[width=.22\textwidth]{figures/4_data/london_db_75_5.jpg}
    \label{fig:msls_london_db}
    \end{subfigure}
    \begin{subfigure}
    \includegraphics[width=.22\textwidth]{figures/4_data/tokyo_db_50_31.jpg}
    \label{fig:msls_tokyo_db}
    \end{subfigure}
     \begin{subfigure}
    \includegraphics[width=.22\textwidth]{figures/4_data/sf_db_16_78.jpg}
    \label{fig:msls_sf_db}
    \end{subfigure}
    \begin{subfigure}
    \includegraphics[width=.22\textwidth]{figures/4_data/trondheim_db_0_0.jpg}
    \label{fig:msls_trondheim_db}
    \end{subfigure}
    
    \begin{subfigure}
    \includegraphics[width=.22\textwidth]{figures/4_data/london_fov_75_5.png}
    \label{fig:msls_london_fov}
    \end{subfigure}
    \begin{subfigure}
    \includegraphics[width=.22\textwidth]{figures/4_data/tokyo_fov_50_31.png}
    \label{fig:msls_tokyo_fov}
    \end{subfigure}
    \begin{subfigure}
    \includegraphics[width=.22\textwidth]{figures/4_data/sf_fov_16_78.png}
    \label{fig:msls_sf_fov}
   \end{subfigure}
    \begin{subfigure}
    \includegraphics[width=.22\textwidth, trim=1cm 1cm 1cm 1cm, clip]{figures/4_data/trondheim_fov_0.png}
    \label{fig:msls_trondheim_fov}
    \end{subfigure}
   
    \caption{Example image pairs from the MSLS dataset. The first row shows the query images, the second row shows the corresponding matches from the map set, and the third row shows the estimated 2D FoV overlap. The query image is associated with the red camera, while the map image with the blue camera. The first column shows a positive match with 75.5\% FoV overlap, and many visual features in common. The second column shows a borderline pair: the two images have 50.31\% FoV overlap and some features in common. The third column shows a soft negative match, where the two images have FoV overlap of 16.78\%. The fourth column shows a hard negative match, where the two images are taken by cameras looking in opposite directions, and the FoV overlap is 0\%.}
    \label{fig:msls_fov}
\end{figure*}

We estimate the similarity of two images by approximating a measure of the overlap of their two-dimensional Field-of-View (FoV) in the horizontal plane.
Let us consider a camera with a FoV defined by the angle $\theta$ and radius $r$, positioned in an environment according to a 2D translation vector $(t_0,t_1)$ with respect to the origin of the reference system. The camera is oriented at an angle $\alpha$ with respect to the north direction of the reference system. We define the 2D FoV as the sector of the circle denoted by the center $(t_0,t_1)$ and radius $r$ enclosed in the angle range delimited by $[\alpha - \frac{\theta}{2}, \alpha +\frac{\theta}{2}]$ (see Fig.~\ref{fig:fov_graph}).
The 2D Field-of-View overlap between two images corresponds to the intersection-over-union (IoU) of their FoVs. \revised{Differently from~\cite{balntas2018relocnet,ding2019camnet} that compute the overlap of  3D camera frusta for camera localization, we relax this concept by only considering the overlap of the camera field-of-views in the horizontal plane.} We consider as positives the image pairs with determined ground truth FoV overlap higher than $50\%$, and the rest as negatives. More specifically, the negative pairs with a similarity higher than $0\%$ are soft negatives, while the pairs with a similarity degree equal to $0\%$ are hard negatives.

For re-annotating the MSLS dataset, wee consider the UTM data as the translation vector $(t_0,t_1)$ and the compass angle as the orientation $\alpha$ necessary to estimate the 2D FoV of the cameras. The FoV angle of the cameras and the intrinsics are not provided. We thus estimate the value of the FoV angle $\theta$ according to the following reasoning. The authors of MSLS define a positive match when the retrieved map image falls within 25m and $40^{\circ}$ from the query. We define a similarity measure that satisfies those constraints. Image pairs taken at locations that are closer than 25m and with orientation differences lower than $40^{\circ}$ are expected to have a similarity higher than $50\%$. Moreover, the borderline cases with distances close to 25m and/or orientation difference near to $40^{\circ}$ should have a similarity close to $50\%$. Hence, we define $r=25m\times2=50m$ and estimate a $\theta$ that gives approximately a $50\%$ FoV overlap for the borderline cases,  i.e. 0m@$40^{\circ}$, and 25m@$0^{\circ}$. For the former, the optimal $\theta$ corresponds to $80^{\circ}$, and for the second latter to $102^{\circ}$. We settle for a value in the middle and define $\theta=90^{\circ}$, which gives $55.63\%$ and $45.01\%$ FoV overlaps, as shown in Fig.~\ref{fig:msls_0m_40deg} and~\ref{fig:msls_25m_0deg}. We display  examples of the similarity ground truth for MSLS in Fig.~\ref{fig:msls_fov}. \revised{We compute the similarity ground truth for each possible query-map pair, per city.}



\subsection{Selection of training pairs}
\label{sec:training_pairs}
Training a model for visual place recognition is usually formulated as a binary classification problem. The aim is to determine whether two images depict a similar (class 1) or dissimilar (class 2) place. 
To train binary classifiers, it is generally desirable to have balanced datasets, to ensure that the two classes are equally weighted. This is often not the case for visual place recognition, where the dissimilar image pairs significantly outnumber the positive pairs. Moreover, when training a siamese architecture, it is necessary to form batches with meaningful image pairs or triplets. For instance, if the training batch consists of pairs that are too easy, the learning process might stall, and the weights of the network not be updated. If the pairs are too difficult, the training dynamics can become unstable~\cite{shi2016embedding}. Hence, the selection of image pairs is a crucial element of the training. 

Existing approaches use complex image pair mining strategies. In the case of  NetVLAD~\cite{Arandjelovic2017} \revised{and NetVLAD-based methods like Patch-NetVLAD~\cite{hausler2021patch} or NetVLAD-SARE~\cite{liu2019stochastic}}, the authors use a Hard Negative Mining strategy. For each query image, they select a set of potential positive map images and a set with its 10 hardest negative map images. The best combination of positive and negative samples is selected by means of their contribution to a Triplet Ranking Loss function. This implies that all the images need to be forwarded to the network to compute the value of the loss function, although only three contribute to the learning. In~\cite{msls}, the authors deploy a similar strategy, selecting only the top-5 hardest negatives. For each triplet, many image latent representations are computed. Even with a caching strategy, bigger backbones like VGG cannot be trained in their entirety~\cite{Arandjelovic2017} and large training batches do not fit in the memory of a regular GPU. In~\cite{msls}, indeed, the authors use a batch size of 4 triplets. 

\revised{A continuous similarity ground truth can lead to improved learning~\cite{thoma2020soft}, but it does not dispense with the need for hard mining strategies by itself. When relying only on the GPS without taking in consideration the orientation, for instance, the ground truth annotations are likely to contain visual inaccuracies, i.e. two images taken 20cm apart facing in opposite directions will not be visually similar, despite having a high annotated similarity. These contradictions make necessary the use of further curation of the selected pairs or triplets by means of a costly hard mining strategy.}

We argue that a better curated and more informative ground truth can replace these complicate mining techniques. We use our graded similarity annotations, presented in Sections~\ref{sec:2Dfov} and~\ref{sec:3Dfov} to select the image pairs that compose a training batch. For our training process, we ensure that each training batch contains a balanced amount of positive and negative pairs. In the case of the negative pairs, we also ensure that half of them are soft negatives (i.e. their annotated similarity is higher than 0). \revised{We explore several batch composition strategies and discuss them in Section~\ref{sec:discussion}.} This means that each selected image pair consist of a query and a match image, and we do not require complicated mining strategies or to compute any latent representations to form the pairs. Hence, we are able to train our models with a batch size of 64.

