Response to Reviewer vsaP:
We thank the reviewer for reviewing our work and their insightful comments. 

Concerning practicality of computation, we considered a restricted case where it is at least possible to accurately estimate the quantities in the bounds. The practicality of training a large number of models for this purpose depends on the size of the networks and the nature of the tasks. We have added a brief discussion of this in the limitation section of the Discussion.

We concur that the use of the PAC-Bayes bounds for model selection is an interesting topic. However, we found that investigating this in a more complete manner would have increased the amount of work substantially while not being directly related to the computation and comparison of the bounds, which is our primary focus. Therefore we chose to leave such an investigation to future work.

We agree with the reviewers points that assuming overlapping support is likely too strong of an assumption to be empirically viable and the assumption required for the MMD bound to hold is fairly opaque. The MMD assumption does not rely on overlap, but whether it is satisfied depends on the behavior of loss functions outside of the support of the labeled data. As such, it is not verifiable without overlap. If there is reason to believe that the function under estimation is smooth, the assumption is more plausible but should nevertheless be used with care. More work is needed to intuit for which tasks and models this is a reasonable assumption and when other ones are needed. We expanded the discussion on the intuition of overlap and the loss assumption in the discussion section. 

Response to Reviewer TMKi:
We thank the reviewer for their comments and helpful references to related work. The comments below are regarding the requested changes.

Regarding the importance weighting bound we have stated explicitly in section 3 that we assume that they are uniformly bounded and that we can calculate them exactly, based on knowledge of the data set construction, for simplicity.

We have added standard deviations to the bounds in figure 6 and similar figures in the appendix.

We thank the reviewer for reminding us of the works using localized discrepancies. We have added a discussion on these to the second section and a short note of the intractability of computing the discrepancy for a subset of promising classifiers when these are deep neural networks. This disqualifies them from consideration in this work as we study practically computable bounds.

Further on the point of lacking literature coverage. Paper [3] is primarily focused on achieving a bound which might be successfully minimised (in part) by methods like DANN. It thus uses a very specific construction which is unique in the sense that it requires a lot of assumptions on the structure of the domains under a representation mapping and how they are related. We have added a note at the end of section 2 exemplifying this as a direction for future work. However, we do not consider this work along with other representation learning approaches for reasons stated at the end of section 2. 

The last paper, [4], also focuses on constructions that can be effectively controlled by adversarial learning. We have added it to the overview section. However, the resulting bounds also contain the joint optimal error parameter $\lambda$ which disqualifies it similar to many other works considered in section 2.

Regarding the boundedness of the kernel. While we have not explicitly mentioned it in the paper, $K$ is bounded by 1 for our choice of kernel, the Gaussian RBF-kernel, irrespective of model. Hence, we can compute all parts of the bound.  However, making any concrete statement about the variation in tigher bounds on $K$ for neural networks is very hard. Generally speaking, the value might be high or low depending on the type of data considered, what kernel one chooses to work with etc. The kernel value itself is not directly influenced by the model choice, other than through the assumption that the loss can be pointwise bounded by a function in the RKHS given by $k$. 
 

Response to Reviewer iFWv:
We thank the reviewer for taking their time to review our work. We are glad to hear that the reviewer finds our extensions of previous work interesting. We share the sentiment. 

We have added a short introduction and definition of the MMD.

Concerning the effect of sample size. We have investigated the effect of simply increasing the amount of datapoints available when calculating which of course tightens the bounds. However, the question is what exactly increasing the sample means. If we constrain the total amount of datapoints, it is more difficult to disentangle effects as both the posterior learned and the bound calculation are affected. 

If the reviewer refers to just constraining the amount used for the bound it is simply a exercise of increasing $m$ in the bounds. This results in the IW and Add bounds becoming increasingly tight as they converge to the target risk. The Mult. and MMD bounds will still have the $\beta_\infty$ and $MMD$ terms respectively which will not allow them to become fully tight unless they are 1 and 0 respectively.


