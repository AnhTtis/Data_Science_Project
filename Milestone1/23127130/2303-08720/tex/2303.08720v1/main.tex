
\documentclass[10pt]{article} % For LaTeX2e
\usepackage[preprint]{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
%\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

%%%% OWN ADDED PACKAGES
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{newfloat}
\usepackage{subcaption}
\usepackage{dsfont}
\usepackage{graphicx}
\newcommand{\hltodo}[1]{{\bf \hl{[#1]}}}
\newcommand{\edit}{\textcolor{blue}}
\newcommand{\blockcmnt}{\color{blue}}
\usepackage{color,colortbl}
\usepackage{wrapfig}
\definecolor{Green}{rgb}{0,1,0.5}
\usepackage[group-separator={,}]{siunitx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

\def\supp{\mathrm{supp}}
\def\add{\textbf{Add}}
\def\mult{\textbf{Mult}}
\def\ipm{\textbf{IPM}}
\def\mmd{\textbf{MMD}}
\def\iw{\textbf{IW}}

\title{Practicality of generalization guarantees for unsupervised domain adaptation with neural networks}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Adam Breitholtz \email adambre@chalmers.se \\
      \addr Department of Computer Science\\
      Chalmers University of Technology
      \AND
      \name Fredrik D. Johansson \email fredrik.johansson@chalmers.se \\
      \addr Department of Computer Science\\
      Chalmers University of Technology
    %   \AND
    %   \name Devdatt Dubhashi \email dubhashi@chalmers.se\\
    %   \addr Department of Computer Science\\
    %  Chalmers University of Technology
}
% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{10}  % Insert correct month for camera-ready version
\def\year{2022} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=vUuHPRrWs2}} % Insert correct link to OpenReview for camera-ready version

\begin{document}
\maketitle
\begin{abstract}
Understanding generalization is crucial to confidently engineer and deploy machine learning models, especially when deployment implies a shift in the data domain. 
For such domain adaptation problems, we seek generalization bounds which are tractably computable and tight. If these desiderata can be reached, the bounds can serve as guarantees for adequate performance in deployment.
However, in applications where deep neural networks are the models of choice, deriving results which fulfill these remains an unresolved challenge; most existing bounds are either vacuous or has non-estimable terms, even in favorable conditions.
In this work, we evaluate existing bounds from the literature with potential to satisfy our desiderata on domain adaptation image classification tasks, where deep neural networks are preferred. We find that all bounds are vacuous and that sample generalization terms account for much of the observed looseness, especially when these terms interact with measures of domain shift. To overcome this and arrive at the tightest possible results, we combine each bound with recent data-dependent PAC-Bayes analysis, greatly improving the guarantees. We find that, when domain overlap can be assumed, a simple importance weighting extension of previous work provides the tightest estimable bound. Finally, we study which terms dominate the bounds and identify possible directions for further improvement.   %This paper cannot be longer than 12 pages of main content(said so on Openreview)
\end{abstract}

\section{Introduction}
%% DA
Successful deployment of machine learning systems relies on generalization to inputs never seen in training.
In many cases,
training and in-deployment inputs differ systematically; these are domain adaptation (DA) problems. %% healthcare
An example of a setting where these problems arise is healthcare. Learning a classifier from data from one hospital and applying to samples from another is an example of tasks that machine learning often fail at~\citep{albadawy_deep_2018,perone_unsupervised_2019,castro_causality_2020}. 
%% guarantees, what should they do and why do we want them
In high-stakes settings like healthcare, guarantees on model performance would be required before meaningful deployment is accepted. Modern machine learning models, especially neural networks, perform well on diverse and challenging tasks on which conventional models have had only modest success. However, due to the high flexibility and opaque nature of neural networks, it is often hard to quantify how well we can expect them to perform in practice. 

Performance guarantees for machine learning models are typically expressed as generalization bounds. Bounds for unsupervised domain adaptation (UDA), where no labels are available from the target domain, have been explored in a litany of papers using both the PAC~\citep{ben-david_analysis,mansour_domain_2009} and PAC-Bayes~\citep{germain_pac-bayes_2020} frameworks; see \citet{redko_survey_2020} for an extensive survey.
%%% few actually report the bounds, most cannot be calculated
Despite great interest in this problem, very few works actually compute or report the \emph{value} of the proposed bounds. Instead, the results are used only to guide optimization or algorithm development. Moreover, the bounds presented often contain terms which are non-estimable without labeled data from the target domain even under favourable conditions~\citep{johansson_support_2019,zhao_learning_2019}.

%%% What the goal is, 
For deployment in sensitive settings we wish to find bounds which are: a) Amenable to calculation; they do not contain non-estimable terms and are tractable to compute. b) Tight; they are close to the error in deployment (or at least non-vacuous).
%\begin{enumerate}
%    \item Amenable to calculation. I.e. they do not contain non-estimable terms and are tractable to compute.
%\item Tight. I.e. they are close to the error in deployment (or at least non-vacuous).
%\end{enumerate}
How do existing bounds fare in solving this problem? As we will see, for realistic problems under favorable conditions, most, if not all, bounds in the literature struggle to satisfy one or both of these goals to varying degrees.

%Using bounds as generalisation guarantees which are tractably computable, tight or even estimable for realistic problems can thus prove quite challenging.

%%%%%% Do both an explanation of what exactly we do in the work and after that we do a short summary of the contributions
%%%%  what do we search for, what do we find, what do we evaluate and what contributions do we do?


%%% bounds representerar klasser av bounds, 
In this work, we examine the practical usefulness of current UDA bounds as performance guarantees in the context of learning with neural networks.
Examining the literature with respect to our desiderata, we identify bounds which show promise in being estimable and tractably computable (Section 2.1). We find that terms related to sample generalization dominate existing bounds for neural networks, prohibiting tight guarantees. To remedy this, we apply PAC-Bayes analysis~\citep{mcallester1999} with data-dependent priors~\citep{dziugaite_data-dependent_2019} in four diverse bounds (Sections 2.3--2.4). Two are existing PAC-Bayes bounds from the UDA literature and two are PAC-Bayes adaptations of bounds based on importance weighting (IW) and integral probability metrics (IPM). 
%
We evaluate the bounds empirically under favorable conditions in two tasks which fulfill the covariate shift and domain overlap assumptions; one task concerns digit image classification and the second X-ray classification (Section 3). Our results show that all four bounds are vacuous on both tasks without data-dependent priors, but some can be made tight with them (Section 4). Furthermore, we find that the simple extension of applying importance weights to previous work outperforms the best fully observable bound from the literature in tightness. This result highlights amplification of bound looseness due to interactions between domain adaptation and sample generalization terms. We conclude by offering insights into achieving the tightest bounds possible given the current state of the literature (Section 5).

\section{Background}
In this section, we introduce the unsupervised domain adaptation (UDA) problem and give a survey of existing generalization bounds through the lens of practicality: do the bounds contain non-estimable terms and are they tractably computable? We go on to select a handful of promising bounds and combine them with data-dependent PAC-Bayes analysis to arrive at the tightest guarantees available. %These are later evaluated empirically on image classification tasks. 
%%%%the next sentence is bad and should be removed
%We seek to find actually useful guarantees for our models and as such need to be able to compute the generalisation bounds. Thereafter, we will from this select four bounds representative of the literature that we will calculate. Two of these from the literature and two by applying small extensions to a bound by \citet{mcallester_pac-bayesian_2013}.

%
% UDA and problem setup
%
%\subsection{Preliminaries}
We study UDA for binary classification, in the context of an input space $\mathcal{X} \subseteq \mathbb{R}^d$ and a label space $\mathcal{Y}=\{-1,1\}$. While our arguments are general, we use as running example the case where $\mathcal{X}$ is a set of black-and-white images. Let $\mathcal{S}$ and $\mathcal{T}$, where $\mathcal{S}\neq \mathcal{T}$, be two distributions, or \emph{domains}, over the product space $\mathcal{X} \times \mathcal{Y}$, called the source domain and target domain respectively.
%
The source domain is observed through a labeled sample $S=\{x_i,y_i\}_{i=1}^n\sim (\mathcal{S})^n$ and the target domain through a sample $S'_x = \{x'_i\}_{i=1}^m\sim (\mathcal{T}_x)^m$ which lacks labels, where $\mathcal{T}_x$ is the marginal distribution on $\mathcal{X}$ under $\mathcal{T}$. Throughout, $(\mathcal{D})^N$ denotes the distribution of a sample of $N$ datapoints drawn i.i.d. from the domain  $\mathcal{D}$. 

The UDA problem is to learn hypotheses $h$ from a hypothesis class $\mathcal{H}$, by training on $S$ and $S'_x$, such that the hypotheses perform well on unseen data drawn from $\mathcal{T}_x$. In the Bayesian setting, we learn posterior distributions $\rho$ over $\mathcal{H}$ from which sampled hypotheses perform well on average. We measure performance using the expected \emph{target risk} $R_\mathcal{T}$ of a single hypothesis $h$ or posterior $\rho$,
\begin{equation}\label{eq:risk}
\underbrace{R_\mathcal{T}(h)=\underset{(x,y)\sim\mathcal{T}}{\mathbb{E}}[\ell(h(x),y)]}_{\text{Risk for single hypothesis} ~h} \;\; \mbox{ or }\underbrace{\underset{h \sim \rho}{\mathbb{E}} R_\mathcal{T}(h)}_{\text{Gibbs risk of posterior $\rho$}},
\end{equation}
for a loss function $ \ell : \mathcal{Y}\times\mathcal{Y}\to \mathbb{R_+}$. In this work, we study the zero-one loss, $\ell(y,y') = \mathds{1}[y \neq y']$. The Gibbs risk is used in the PAC-Bayes guarantees~\citep{ShaweWilliamson97,mcallester_pac-bayesian_1998}, a generalization of the PAC framework~\citep{valiant1984, vladimirvapnik1998}.
%

When learning from samples, the empirical risk $\hat{R}_\mathcal{D}$ can be used as an observable measure of performance, 
\begin{equation}\label{eq:emp_risk}
\hat{R}_\mathcal{D}(h)= \frac{1}{m} \sum_{i=1}^m \ell(h(x_i),y_i),
\end{equation}
for a sample $\{(x_i,y_i)\}_{i=1}^n\sim(\mathcal{D})^n $, with the empirical risk of the Gibbs classifier defined analogously.
%
However, since no labeled sample from $\mathcal{T}$ is available, the risk of interest is not directly observable. Hence, the most common way to approximate this quantity is to derive an upper bound on the target risk. We refer to these as \emph{UDA bounds}. Crucially, \emph{any practical performance guarantee must be made using only observed data and assumptions on how $\mathcal{S}$ and $\mathcal{T}$ relate}.
% 
Throughout, we make the following common assumptions.
\begin{assumption}[Covariate shift \& overlap]\label{asmp:covshift_overlap}
The source domain $\mathcal{S}$ and target domain $\mathcal{T}$ satisfy for all $x, y$
\begin{align*}%
\def\arraystretch{1.2}%
\begin{array}{ll}
\text{{\bf Covariate shift: }}& \mathcal{T}_y(Y\mid X=x) = \mathcal{S}_y(Y\mid X=x) \;\;\mbox{ and }\;\; \mathcal{T}_x(X=x) \neq  \mathcal{S}_x(X=x)\\
\text{{\bf Overlap: }} & \mathcal{T}_x(X=x)>0 \Rightarrow \mathcal{S}_x(X=x)>0 ~.
\end{array}%
\end{align*}%
\end{assumption}
These are strong assumptions and covariate shift cannot be verified statistically. They are not required by every bound in the literature but together they are sufficient to guarantee identification and consistent estimation of the target risk~\citep{shimodaira_improving_2000}. More importantly, the generalization guarantees we study more closely are not fully observable without them unless target labels are available. Finally, this setting is among the most simple and favorable ones for UDA which should make for an interesting benchmark---if existing bounds are vacuous also here, significant challenges remain.

%
% Overview of bounds
%
\subsection{Overview of existing UDA bounds}
\label{sec:da_bounds}
%
% Overview of bounds and how they fare in estimability and if they are tractable
%
Most existing UDA bounds on the target risk share a common structure due to their derivation. The typical process starts by bounding the \emph{expected} target risk using the \emph{expected} source risk and measures of domain shift. 
Thereafter, terms are added which bound the sample generalization error, the difference between the expected source risk and its empirical estimate. The results can be summarized conceptually, with $f$ and arguments variously defined, as expressions on the form
$$
\begin{aligned}
R_\mathcal{T}\leq f(&\text{Empirical source risk, }
\text{Measures of domain shift, }
\text{Sample generalization error})~.
\end{aligned}
$$
%%% give example 
There are two main forms taken by this function; one in which sample generalization terms are related to domain shift terms through addition and one where they are multiplied. We call these additive and multiplicative bounds respectively.
One example is the classical result due to \citet{ben-david_analysis} which uses the so-called $\mathcal{A}$-distance to bound the target risk of $h\in \mathcal{H}$ with probability $\geq 1-\delta$, % \hltodo{explain what $\mathcal{A}$ is here}
\begin{equation}\label{eq:bendavid}
R_\mathcal{T}(h)\leq \underbrace{\hat{R}_\mathcal{S}(h) \vphantom{\sqrt{\frac{\frac{1}{2}}{2}}} }_{\text{Emp. risk} } +\underbrace{\sqrt{\frac{4(d\log\frac{2em}{d}+\log\frac{4}{\delta})}{m}}}_{\text{Sample generalization}}+\underbrace{d_{\mathcal{H}}(\mathcal{S}, \mathcal{T})+\lambda  \vphantom{\sqrt{\frac{\frac{1}{2}}{2}}}}_{\text{Domain shift}},
\end{equation}
where $d$ is the VC dimension of the $\mathcal{H}$, $\lambda$ is the sum of the errors on both domains of the best performing classifier $h^*=\argmin_{h\in \mathcal{H}}(R_\mathcal{S}(h)+R_\mathcal{T}(h))$, and $d_{\mathcal{H}}(\mathcal{S}, \mathcal{T})=2\sup_{A\in \{\{x : h(x)=1\} : h\in \mathcal{H}\}} |\mbox{Pr}_{\mathcal{S}}[A] - \mbox{Pr}_{\mathcal{T}}[A]|$ is the $\mathcal{A}$-distance for the characteristic sets of hypotheses in $\mathcal{H}$.


Three challenges limits the practicality of this bound: i) $\lambda$ is not directly estimable without target labels and must be assumed small for an informative bound. This is a pattern in UDA theory which illustrates a fundamental link between estimability and assumption. ii) The VC dimension can easily lead to a vacuous result for modern neural networks. For example, the VC dimension of piecewise polynomial networks is $\Omega(pl\log \frac{p}{l})$ where $p$ is the number of parameters and $l$ is the number of layers~\citep{bartlett_vcdim}. iii) The $\mathcal{A}$-distance can be tractably computed only for restricted hypothesis classes. These issues are not unique the bound above, they are exhibited to varying degrees by any UDA bound.

In response to concerns for practical generalization bounds in deep learning, \citet{valle2020generalization} put forth seven desiderata for predictive bounds. While these are of interest also for UDA, we concern ourselves primarily with the fifth (non-vacuity of the bound) and sixth (efficient computability) desiderata as they are of paramount importance to achieving practically useful guarantees. 
%
In this work, we study UDA bounds with emphasis on how each term influences the following properties when learning with deep neural networks.
\begin{enumerate}
\item {\bf Tightness.} Is the term a poor approximation? Is it likely to lead to a loose bound?
\item {\bf Estimability.} Is the term something which we can estimate from observed data? 
\item {\bf Computability.} Can we tractably compute it for real-world data sets and hypothesis classes? 
\end{enumerate}

\begin{table}[t!] %% have sample complexity term or similar here?
    \centering
    \caption{Overview of existing UDA bounds with respect to a) measures of domain divergence, b) whether domain and sample generalization terms add or multiply, c) non-estimable terms, d) whether the bounds were computed empirically, e) computational tractability. The highlighted rows represent a selection of bounds which are possible to estimate under the assumptions we make and which hold promise in fulfilling our other desiderata. $\text{\sffamily X}^*$ denotes that under the assumptions made in this work we do not have non-estimable terms.}%\checkmark$^\dagger$ denotes that only parts of the bound was calculated.
    \begin{tabular}{c|c|c|c|c|c}
         Paper/reference & Divergence & Add/Mult &  \shortstack{Non-est. \\ terms } & \shortstack{Evaluated \\ bound} & Tractable \\
         \hline
         \citet{ben-david_analysis}& $\mathcal{A}$-distance & Add & \checkmark& \text{\sffamily X}& \text{\sffamily X} \\
         \citet{blitzer_learning}& $H\Delta H$ &Add & \checkmark & \text{\sffamily X} & \text{\sffamily X} \\
         \citet{Ben-David2010a}&$H\Delta H$&Add & \checkmark & \text{\sffamily X}& \text{\sffamily X} \\
          \citet{morvant_parsimonious_2012} & $H\Delta H$ & Add & \checkmark& \text{\sffamily X}& \text{\sffamily X}\\
        \citet{mansour_domain_2009}& Discrepancy dist. &Add &\checkmark & \text{\sffamily X}& \text{\sffamily X}\\
         \citet{redko_2019} & Discrepancy dist. &Add & \checkmark & \text{\sffamily X}& \text{\sffamily X}\\
       \citet{kuroki_unsupervised_2019}  & S-discrepancy&Add &\checkmark & \text{\sffamily X}& \text{\sffamily X}\\
        
         \citet{cortes_domain_2014}& \shortstack{Gen. discrepancy} & Add &\checkmark & \text{\sffamily X}& \text{\sffamily X}\\
         \citet{cortes_adaptation_2015}& \shortstack{Gen. discrepancy} &Add & \checkmark & \text{\sffamily X}& \text{\sffamily X} \\
       \rowcolor{Green} \citet{zhang_generalization_2012} & IPM &Add &\text{\sffamily X} & \text{\sffamily X}& \checkmark\\
       %%% long
        \citet{redkothesis} & IPM, MMD &Add & \checkmark & \text{\sffamily X}& \checkmark \\
      \citet{long_learning_2015} & MMD & Add & \checkmark & \text{\sffamily X}& \checkmark \\
        \citet{redko_theoretical_2017} & IPM & Add & \checkmark & \text{\sffamily X}& \checkmark \\
        \citet{johansson_support_2019} & IPM & Add & \checkmark & \text{\sffamily X}& \checkmark \\
        \citet{zhang_bridging_2019} & \shortstack{ Margin disparity} & Add & \checkmark & \text{\sffamily X} & \text{\sffamily X}\\
        \citet{dhouib_2020} & Wasserstein &Add & \checkmark & \text{\sffamily X}& \checkmark \\
        \citet{shen_wasserstein_2018} & Wasserstein&Add & \checkmark & \text{\sffamily X}& \checkmark \\
         \citet{courty_joint_2017}&  Wasserstein&Add & \checkmark & \text{\sffamily X}& \checkmark \\
        \citet{germain_pac-bayesian_2013} & \shortstack{Domain disagreement} &Add & \checkmark & \text{\sffamily X}& \text{\sffamily X}\\
 \citet{zhang_localized_2020} & Localized discrepancy & Add&\checkmark &\text{\sffamily X}& \text{\sffamily X}\\
         \citet{cortes2019} & Localized discrepancy & Add&\checkmark &\text{\sffamily X}& \text{\sffamily X}\\
           \citet{acuna_fdiv_2021} & f-divergences & Add&\checkmark &\text{\sffamily X}& \text{\sffamily X}\\
        \rowcolor{Green} \citet{germain16} & $\beta$-divergence &Mult &~\text{\sffamily X}$^*$ &\text{\sffamily X}&\checkmark\\
        \rowcolor{Green} \citet{cortes_learning_2010} & \shortstack{R\'enyi} & Mult & ~\text{\sffamily X}$^*$ & \text{\sffamily X}& \text{\sffamily X}\\
% \citet{mansour_2009b} & R\'enyi & Mult &  \text{\sffamily X} & \text{\sffamily X}& \checkmark\\
     %  \citet{hoffman_2018} & R\'enyi& Mult& \text{\sffamily X} & \text{\sffamily X}& \checkmark\\
        \citet{dhouib_revisiting_2018} & $L^1, \chi^2$& Mult & \checkmark & \text{\sffamily X}& \text{\sffamily X}\\
         % \edit{\citet{wu_asymmetric_2019}} & - & Add&\checkmark &\text{\sffamily X}& \text{\sffamily X}\\
       
%   \citet{mansour_robust_2014} & $\lambda$-shift & Add&\text{\sffamily X} &\text{\sffamily X}& \text{\sffamily X}\\
%   \citet{mcnamara_risk_2017}& \text{\sffamily X}& Add&\text{\sffamily X} &\checkmark& \checkmark\\
%  \citet{mansour_domain_2008}& \text{\sffamily X} &\text{\sffamily X} & \text{\sffamily X} & \text{\sffamily X}& \text{\sffamily X} \\
       \end{tabular}
    \label{tab:boundtable}
\end{table}

Next, we give a short summary of existing UDA bounds, with the excellent survey by~\citet{redko_2019} as starting point, while evaluating whether they contain non-estimable terms, if the bound was computed in the paper\footnote{By this we mean the whole bound. Some of the works listed have computed one or several parts of their bound. However, the computation is generally done for simpler model classes than neural networks.} and if the bound is computationally tractable for neural networks. We have listed considered bounds in Table~\ref{tab:boundtable}.  

%%%% bounds which are about a different problem/setting than the one we want to consider here

% First, we note that some bounds, though related, were constructed for different settings or for different problems than UDA. The works by \citet{mansour_domain_2008}, \citet{hoffman_2018} and \citet{mansour_2009b} are all predicated upon the assumption that the target is a mixture of different sources, each of which are observed through labeled samples. While interesting, this setting is distinct from that considered here. Moreover, the work by \citep{mansour_robust_2014} is based on a notion of algorithmic robustness and assumes that the input/output space can be partitioned in a specific way and that such a partition is given. Further, they assume bounded variation of the loss in these partitions. The $\lambda$-shift divergence also assumes a constrained label shift which does not follow from the assumptions made here. Finally, the work by \citet{mcnamara_risk_2017} assumes access to target labels. We will not consider the works mentioned above further.

% Bounds which depend on terms which are infeasible to calculate due to scale/complexity etc.
We will now reason about the bounds' potential to reach our stated desiderata, starting with tractability. We begin by noting that several divergence measures (e.g., $\mathcal{A}$-distance, $H \Delta H$-distance, Discrepancy distance) are defined as suprema over the hypothesis class $\mathcal{H}$. Typically, these are intractable to compute for neural nets due to the richness of the class, and approximations would yield lower bounds rather than upper bounds. Several works fail to yield practically computable bounds for neural networks for this reason~\citep{ben-david_analysis,blitzer_learning,Ben-David2010a,morvant_parsimonious_2012,mansour_domain_2009,redko_2019,cortes_domain_2014,cortes_adaptation_2015}. There are also some works which deal with so called localized discrepancy which depends on finding a subset of promising classifiers and bounding their performance instead.~\citep{zhang_localized_2020,cortes2019} However, this subset is not easy to find in general and as such we view these approaches as intractable also.

% Bounds which depend on non-estimable terms etc.
Continuing with estimability, we may remove from consideration also those whose non-estimable term cannot be dealt with without assuming that they are small---an untestable assumption which does not follow from overlap and covariate shift. This immediately disqualifies a large swathe of bounds which all include the joint error of the optimal hypothesis on both domains, or some version thereof, a very common non-estimable term in DA bounds~\citep{kuroki_unsupervised_2019,redkothesis,long_learning_2015,redko_theoretical_2017,johansson_support_2019,zhang_bridging_2019,dhouib_2020,shen_wasserstein_2018,courty_joint_2017,germain_pac-bayesian_2013,dhouib_revisiting_2018,acuna_fdiv_2021}. %\hltodo{Need to argue that it is non-estimable also under our assumptions. In principle, we could use importance sampling under overlap, but it would require solving a different optimization problem. We could get an upper bound on it that way however. }
In principle, we might be able to approximate this quantity under overlap, e.g. by using importance sampling. However, this would entail solving a new optimization problem to find a hypothesis which has a low joint error (see discussion after \eqref{eq:bendavid}). If we instead wish to upper bound the term, we must solve an equivalent problem to the one we are trying to solve in the first place.
%\hltodo{The structure could be improved here. The first examples here are disqualified for being about a different problem. The remainder here has to do with the scale and computation of sample generalisation terms. I think these should be separated and that we should connect the sample generalisation discussion to the fact that we actually consider variations of these types (Cortes etc) but with PAC-Bayes. }

%%% figure out how to make this look nice
\begin{wrapfigure}{r}{0.5\textwidth}
%\begin{center}
\centering
\includegraphics[width=0.48\textwidth]{Images/task2_vacuous_lenet.pdf}
%\end{center}
\caption{\label{fig:nodatadep}Best bounds achieved without data-dependent priors on the MNIST/MNIST-M task using LeNet-5 as well as the target error for the same posterior hypothesis. Note that all the bounds are vacuous, i.e. they are above one.}
\end{wrapfigure} 
%
%\hltodo{Conclude that the three Zhang, Cortes Germain seem most promising. Neither have tight generalisation guarantees (cite Zhang2021). Neither does PAC-Bayes (see right figure). Move on to data-dep. }
Three bounds remain: \citet{zhang_generalization_2012} use integral probability metrics (IPM) between source and target domains to account for covariate shift in their bound, which are tractable to compute under assumptions on the hypothesis class; \citet{cortes_learning_2010} use importance weights (IW) and Renyi divergences which are well-defined and easily computed under overlap; \citet{germain16} use a related metric based on the norm of the density ratio between domains with similar properties. Respectively, the first two results bound sample generalization error using the uniform entropy number and the covering number. These measures are intractable to compute for neural networks, and while they may be upper bounded by the VC dimension~\citep{wainwright_book}, this is  typically large enough to yield uninformative guarantees~\citep{zhang2021understanding}. In contrast, \citet{germain16} use a PAC-Bayes analysis which we can apply to neural networks by specifying prior and posterior distributions over network weights. Using Gaussian distributions for both priors and posteriors, the PAC-Bayes bound can be readily computed. Thus, to enable closer comparison and tractable computation, in the coming sections, we unify each bounds' dependence on sample generalization by adapting IW and IPM bounds to the PAC-Bayes framework. For completeness, we include an additional PAC-Bayes bound from the UDA literature, which has a non-estimable term, namely \citet{germain_pac-bayesian_2013}.

%% what do we do for PAC-Bayes? data dep priors
The selected bounds are given in Sections~\ref{sec:germain_bounds}--\ref{sec:adap_bounds}. \citet{germain16} will be referred to as the multiplicative bound (\mult{}), \citet{germain_pac-bayesian_2013} as the additive bound (\add{}), the adaptation of importance weighting to PAC-Bayes as \iw{}; and the adaptation of IPM bounds as \mmd{}. 
%
Unfortunately, also the resulting PAC-Bayes bounds are uninformative for even simple image classification UDA tasks; see Figure~\ref{fig:nodatadep}. Consistent with our understanding of standard learning with neural networks, \emph{we find both classical PAC and PAC-Bayes bounds vacuous in the tasks most frequently used as empirical benchmarks in papers deriving UDA bounds}.
%
Next, we detail how to make use of data-dependent priors to get the tightest possible bounds.

%
% DATA-DEPENDENT PRIORS
%
\subsection{Tighter sample generalization guarantees using PAC-Bayes with data-dependent priors}
\label{sec:datadep}
%
PAC-Bayes theory studies generalization of a posterior distribution $\rho$ over hypotheses in $\mathcal{H}$, learned from data, in the context of a prior distribution over hypotheses, $\pi$. The generalization error in $\rho$ may be bounded using the divergence between $\rho$ and $\pi$ as seen in the following classical result due to McAllester. 
\begin{theorem}[Adapted from Thm. 2 in \citet{mcallester_pac-bayesian_2013}]
\label{thm:mcallester}
For a prior $\pi$ and posterior $\rho$ on $\mathcal{H}$, a bounded loss function $\ell : \mathcal{Y} \times \mathcal{Y} \rightarrow [0,1]$ and any fixed $\gamma, \delta \in (0,1)$,  we have w.p. at least $1-\delta$ over the draw of $m$ samples from  $\mathcal{D}$, with $\KL(p \Vert q)$ the Kullback-Liebler (KL) divergence between $p$ and $q$, 
$$
\underset{h \sim \rho}{\mathbb{E}} R_\mathcal{D}(h) \leq \frac{1}{\gamma} \underset{h \sim \rho}{\mathbb{E}}\hat{R}_\mathcal{D}(h)+\frac{\KL(\rho \Vert \pi)+ \ln(\frac{1}{\delta})}{2\gamma(1-\gamma)m}~.
$$
\end{theorem}
% 

The bound in Theorem~\ref{thm:mcallester} grows loose when prior $\pi$ and posterior $\rho$ diverge---when the posterior is sensitive to the training data. When learning with neural networks, $\pi$ and $\rho$ are typically taken to be distributions on the weights of the network before and after training. However, the weights of a trained  deep neural network will be far away from any uninformed prior after only a few epochs. 
For this reason, \citet{dziugaite_role_2020} developed a methodology, based on work by \citet{ambroladze2007} and \citet{parrado-hernandez_pac-bayes}, for learning \emph{data-dependent} neural network priors by a clever use of sample splitting.
To ensure that the bound remains valid, any data which is used to fit the prior must be independent of the data used to evaluate the bound. In this work, we learn $\pi$ and $\rho$ following \citet{dziugaite_role_2020}, as described below.
\begin{enumerate}
    \item A fraction $\alpha\in[0,1)$ is chosen and the available training data, $S$, is split randomly into two parts, $S_\alpha$ and $S\setminus S_\alpha$ of size $\alpha m $ and $(1-\alpha)m$, respectively.
    \item A neural network is randomly initialized and trained using stochastic gradient descent on $S_\alpha$ for one epoch. From this we get the weights, $w_\alpha$.
    \item The same network is trained, starting from $w_\alpha$, on all of $S$ until a stopping condition is satisfied. In this work, we terminate training after 5 epochs. We save the weights, $w_\rho$.
    \item From $w_\alpha$ and $w_\rho$ we create our prior and posterior from Normal distributions centered on the learned weights,  $\pi=\mathcal{N}(w_\alpha,\sigma I)$ and $\rho=\mathcal{N}(w_\rho,\sigma I)$ respectively. $\sigma$ is a hyperparameter governing the specificity (variance) of the prior which may be chosen when evaluating.
    \item Finally, we use the learned prior and posterior to evaluate the bounds on $S\setminus S_\alpha$.
\end{enumerate}


%
% The bounds from Germain
%
\subsection{PAC-Bayes bounds from the domain adaptation literature}
\label{sec:germain_bounds}
Both the additive and multiplicative PAC-Bayes UDA bounds described in Section~\ref{sec:da_bounds} are defined in~\citet{germain_pac-bayes_2020} and make use of a decomposition of the zero-one risk into the \emph{expected joint error} 
$$
e_\mathcal{D}(\rho)=\underset{h,h'\sim \rho\times\rho}{\mathbb{E}}~\underset{x,y\sim \mathcal{D}}{\mathbb{E}}\ell(h(x),y)\ell(h'(x),y),
$$ 
which measures how often two classifiers drawn from $\rho$ make the same errors, and the \emph{expected disagreement}
$$
d_{\mathcal{D}_x}(\rho)=\underset{h,h'\sim \rho\times\rho}{\mathbb{E}}~\underset{x\sim \mathcal{D}_x}{\mathbb{E}}\ell(h(x),h'(x)),
$$
which measures how often two classifier disagree on the labeling of the same point. Empirical variants $\hat{e}_\mathcal{D}(\rho)$ and $\hat{d}_{\mathcal{D}_x}(\rho)$ replace expectations with sample averages analogous to~\eqref{eq:emp_risk}.

In the bound of \citet{germain16}, the sample generalization component (KL-divergence between prior and posterior) is multiplied with a domain shift component (supremum density ratio). % We will henceforth refer to this as the \emph{multiplicative bound}.
%
\begin{theorem}[Multiplicative bound, \citet{germain16}]
\label{thm:mult}
For any real numbers $a,b>0$ and $\delta\in (0,1)$, it holds, under Assumption~\ref{asmp:covshift_overlap}, with probability at least $1-\delta$ over labeled source samples $S\sim (\mathcal{S})^m$ and unlabeled target samples $T_x\sim (\mathcal{T}_x)^n$, with constants $a'=\frac{a}{1-e^{-a}}$,  $b'=\frac{b}{1-e^{-b}}$, for every posterior $\rho$ on $\mathcal{H}$ that
$$
\begin{aligned}
 \underset{h\sim\rho}{\mathbb{E}}R_\mathcal{T}(h)&\leq a'\frac{1}{2}\hat{d}_{\mathcal{T}_x}+b'\beta_\infty(\mathcal{T}\|\mathcal{S})\hat{e}_\mathcal{S} 
 %+\eta_{\mathcal{T}\setminus\mathcal{S}} \\ Removed due to covariate shift
 +\Big(\frac{a'}{na}+\frac{b'\beta_\infty(\mathcal{T}\|\mathcal{S})}{mb}\Big)\Big(2\KL(\rho \Vert \pi)+\ln{\frac{2}{\delta}}\Big) + \eta_{\mathcal{T}\setminus \mathcal{S}}~,
\end{aligned}
$$
with 
$
\beta_\infty(\mathcal{T}\|\mathcal{S})=\underset{x \in \supp(\mathcal{S}_x)}{\sup}{\mathcal{T}_x(x)}/{\mathcal{S}_x(x)} % Removed $y$ due to Assumption of cov shift
$,\footnote{Terms $\beta_q$ based on the $q$:th moment of the density ratio for $q< \infty$ are considered in \citep{germain_pac-bayes_2020} but not here.} and 
$\eta_{\mathcal{T}\setminus \mathcal{S}}=\underset{(x,y)\sim \mathcal{T}}{\mathbb{E}}[\mathds{1}[(x,y)\notin \supp(\mathcal{S})]]\sup_{h\in \mathcal{H}}R_{\mathcal{T}\setminus \mathcal{S}}(h)$.
\end{theorem}

The bound is simplified slightly in our setting due to Assumption~\ref{asmp:covshift_overlap} as $\eta_{\mathcal{T}\setminus \mathcal{S}}$ will be 0. By Bayes rule, $\beta_\infty$ can be computed as the maximum ratio between conditional probabilities of an input being sampled from $\mathcal{T}$ or $\mathcal{S}$. The second result due to Germain et al. is additive in its interaction between domain and sample generalization terms.
\begin{theorem}[Additive bound, \citet{germain_pac-bayesian_2013}] \label{thm:add}
For any real numbers $\omega,\gamma>0$ and $\delta \in (0,1)$, with probability at least $1-\delta$ over labeled source samples $S\sim (\mathcal{S})^m$ and unlabeled target samples $T_x\sim (\mathcal{T}_x)^m$; for every posterior $\rho$ on $\mathcal{H}$, it holds with constants $\omega'=\frac{\omega}{1-e^{-\omega}}$ and $\gamma'=\frac{2\gamma}{1-e^{-2\gamma}}$ that 
    $$
   \begin{aligned}
    &\underset{h\sim\rho}{\mathbb{E}}R_\mathcal{T} (h)\leq \underset{h\sim\rho}{\mathbb{E}} \omega'\hat{R}_\mathcal{S} (h)+\gamma'\frac{1}{2}\hat{\text{Dis}}_\rho(S,T_x) 
    +\Big(\frac{\omega'}{\omega}+\frac{\gamma'}{\gamma}\Big)\frac{\KL(\rho \Vert \pi)+\log\frac{3}{\delta}}{m}+\lambda_\rho+\frac{1}{2}(\gamma'-1),
   \end{aligned}
    $$
    where $\hat{\text{Dis}}_\rho(S,T_x)=|\hat{d}_{\mathcal{T}_x}-\hat{d}_{\mathcal{S}_x}|
    $ is the empirical domain disagreement, $\lambda_\rho=|e_\mathcal{T}(\rho)-e_\mathcal{S}(\rho)|$.
\end{theorem}
Next we will combine the main techniques used to account for domain shift in \citet{cortes_learning_2010} and \citet{zhang_generalization_2012} with the PAC-Bayes analysis of Theorem~\ref{thm:mcallester}, producing two corollaries for the UDA setting.

%
% Extensions of McAllester bound
%
\subsection{Adapting the classical PAC-Bayes bound to unsupervised domain adaptation}
\label{sec:adap_bounds}
%
First, we adapt the bound in Theorem~\ref{thm:mcallester} to UDA by incorporating importance weighting~\citep{shimodaira_improving_2000,cortes_learning_2010}.%% maybe cite something else here
%
%We take this theorem and simply extend it by considering a weighted loss which will ensure that we get a bound which does not always suffer from multiplication with the worst case weight on our loss (which happens in the multiplicative bound). 
We define a weighted loss $\ell^w(h(x),y)=w(x)\ell(h(x),y)$ where $w(x)=\frac{T(x)}{S(x)}$ and $\ell$ is the zero-one loss. The risk of a hypothesis using this loss is denoted by $R^w$.
\begin{corollary}{(IW bound)}
Consider the conditions in Theorem~\ref{thm:mcallester} and let $\beta_\infty=\sup_{x\sim \mathcal{X}}w(x)$. We have, for any choice of $\gamma, \delta \in (0,1)$ and any pick of prior $\pi$ and posterior $\rho$ on $\mathcal{H}$,  
$$
\underset{h \sim \rho}{\mathbb{E}} R_\mathcal{T}(h) \leq  \frac{1}{\gamma} \underset{h \sim \rho}{\mathbb{E}} \hat{R^w_\mathcal{S}}(h) + \beta_\infty\frac{\KL(\rho \Vert \pi)+ \ln(\frac{1}{\delta})}{2\gamma(1-\gamma)m}~.
$$
\end{corollary}
\begin{proof}
Since Theorem~\ref{thm:mcallester} holds for loss functions mapping onto $[0,1]$, we divide the weighted loss $\ell^w$ by the maximum weight, $\beta_\infty$. The argument then follows naturally when we apply Theorem~\ref{thm:mcallester} with the loss function $\frac{\ell^w}{w_{max}}$.
$$
\underset{h \sim \rho, (x,y)\sim \mathcal{T}}{\E}\Big[\frac{\ell(h(x),y)}{\beta_\infty}\Big]
= \underset{h \sim \rho, (x,y)\sim \mathcal{S}}{\E}\Big[\frac{\ell^w(h(x),y)}{\beta_\infty}\Big]
\leq \frac{1}{\gamma \beta_\infty}\hat{R^w_\mathcal{S}}+\frac{\KL(\rho \Vert \pi)+ \ln(\frac{1}{\delta})}{2\gamma(1-\gamma)m}
$$
The first equality holds due to Assumption~\ref{asmp:covshift_overlap} and the definitions of $w$ and $\ell^w$.
\end{proof}

%%%
%argue why Germain with $q\neq\infty$ in $\beta^q$ is not sufficient. What happens in expectation etc.
Now we continue with applying an argument based on integral probability metrics (IPM) drawn from  \citet{zhang_generalization_2012} and similar works.
IPMs, such as the kernel maximum mean discrepancy (MMD)~\citep{gretton2012kernel} and the Wasserstein distance have been used to give tractable and even differentiable bounds on target error in UDA to guide algorithm development~\citep{courty2017joint,long_learning_2015}. 
The kernel MMD is a IPM, defined as follows, in terms of its square, given a reproducing kernel $k(\cdot,\cdot):\mathcal{X}\times\mathcal{X}\to \R$
$$
\mmd{}_k(P,Q)^2=\underset{X\sim P,X'\sim P}{\mathbb{E}}[k(X,X')]-2\underset{X\sim p,Y\sim Q}{\mathbb{E}}[k(X,Y)]+\underset{Y\sim Q,Y'\sim Q}{\mathbb{E}}[k(Y,Y')].
$$
Here, $X$ and $Y$ are random variables, and $X'$ is an independent copy of $X$ with the same distribution and $Y'$ is an independent copy of $Y$.
This measures a notion of discrepancy between the distributions $P$ and $Q$ based on their samples.

We combine the MMD with the bound from \citet{mcallester_pac-bayesian_2013} to arrive at what we will call the \mmd{} bound.
\begin{corollary}{(MMD bound)}\label{thm:ipm}
Let $\overline{\ell}_h(x) = \E[\ell(h(x), Y) \mid X=x]$ be the expected pointwise loss at $x\in \mathcal{X}$ and assume that, for any $h \in \mathcal{H}$, $\overline{\ell}_h$ can be uniformly bounded by a function in reproducing-kernel Hilbert $\mathcal{L}$ space with kernel $k$ such that $\forall x,x'\in \mathcal{X} : 0 \leq k(x,x') \leq K$. Then, under Assumption~\ref{asmp:covshift_overlap}, with $\gamma, \delta \in (0,1)$ and probability $\geq 1-\delta$ over labeled source samples $S\sim (\mathcal{S})^m$ and unlabeled target samples $S_x' \sim \mathcal{T}_x^m$, 
\begin{align}\label{eq:IPM_bound}
\underset{h\sim\rho}{\mathbb{E}} R_{\mathcal{T}}(h) & \leq \frac{1}{\gamma} \underset{h\sim\rho}{\mathbb{E}}\hat{R}_{\mathcal{S}}(h) + \frac{\KL(\rho \| \pi) + \log\frac{2}{\delta}}{2\gamma(1-\gamma)m}
 + \hat{\mathrm{MMD}}_{k}(\mathcal{S},\mathcal{T}) +  2\sqrt{\frac{K}{m}}\Big(2+\sqrt{\log{\frac{4}{\delta}}}\Big)~,
\end{align}
where $\hat{\mathrm{MMD}}_k(\mathcal{S}, \mathcal{T})$ is the biased empirical estimate of the maximum mean discrepancy between $\mathcal{S}$ and $\mathcal{T}$ computed from $S_x$ and $S_x'$, see eq. (2) in~\citet{gretton2012kernel}.
\end{corollary}
\begin{proof}
By assumption, for any hypothesis $h \in \mathcal{H}$, 
\begin{align*}\label{eq:IPM_bound_proof} 
R_{\mathcal{T}}(h) & = R_{\mathcal{S}}(h) + \mathbb{E}_{\mathcal{T}}[\ell(h(X),Y)] - \mathbb{E}_{\mathcal{S}}[\ell(h(X),Y)]
%& = R_{\mathcal{S}}(h) + |\mathbb{E}_{\mathcal{T}_x}[\ell_h(X)] - \mathbb{E}_{\mathcal{S}_x}[\ell_h(X)]| \nonumber \\
\leq R_{\mathcal{S}}(h) + \sup_{l \in \mathcal{L}} \left| \mathbb{E}_{\mathcal{T}_x}[l_h(X)] - \mathbb{E}_{\mathcal{S}_x}[l_h(X)] \right|  ~.
%= R_{\mathcal{S}}(h) + \mathrm{MMD}_{\mathcal{L}}(\mathcal{S}, \mathcal{T})
%& \leq R_{\mathcal{S}}(h) + \mathrm{MMD}_{\mathcal{L}}(\mathcal{S}, \mathcal{T}), 
\end{align*}
The inequality holds because $\E_{\mathcal{S}}[\ell(h(x), Y) \mid X=x] = \E_{\mathcal{T}}[\ell(h(x), Y) \mid X=x]$ due to Assumption~\ref{asmp:covshift_overlap} (covariate shift) and the assumption that $\ell_h$ is uniformly bounded by a function in $\mathcal{L}$. The RHS of the inequality is precisely $\mathrm{MMD}_{\mathcal{L}}$ and the full result follows by linearity of expectation (over $\rho$), since the MMD term is independent of $h$, and application of Theorem~\ref{thm:mcallester}. The right-most term in \eqref{eq:IPM_bound} follows from a finite-sample bound on MMD, Theorem 7 in \citet{gretton2012kernel}, and a union bound w.r.t. $\delta$. 
\end{proof}
%%% kernel k talk here
%To compute a bound fully from data, the $\mathrm{MMD}$ must also be estimated from finite samples. This can be done using standard measure concentration inequalities~\citep{sriperumbudur_integral_2009,gretton2012kernel}. Since the MMD term in \eqref{eq:mmdtheorem} is independent of $h$, this does not affect comparison of different hypotheses. 
\paragraph{Representation learning.} When no function family $\mathcal{L}$ satisfying the conditions of Corollary~\ref{thm:ipm} is known, an additional unobservable error term must be added to the bound, to account for excess error. 
Bounds based on the MMD and other IPMs have been used heuristically in representation learning to find representations which minimize the induced distance between domains and achieve better domain adaptation~\citep{long_learning_2015}. However, even if covariate shift holds in the input space, these are not guaranteed to hold in the learned representation. For this reason, we do not explore such approaches even though they might hold some promise. An example of this is recent work by \citet{wu_asymmetric_2019} which provided some interesting ideas about assumptions which constrains the structure of the source and target distributions under a specific representation mapping. Exploring such ideas further is an interesting direction for future work.
%
% EXPERIMENTS
%
\section{Experimental setup}
We describe the experimental setup briefly, leaving more details in Appendix~\ref{app:exp}. We examine the dynamics of the chosen bounds (\add{}, \mult{}, \mmd{}, \iw{}), which parts of the bounds dominate their value, the effect of varying the amount of data used to inform the prior and if the bounds have utility for early stopping indication or model selection.
In addition to these we also want to answer if these bounds practically useful as guarantees, and if not, what is lacking and what future directions should be explored to reach our desiderata? Since the term $\lambda_\rho$ in \add{} (Theorem~\ref{thm:add}) depends on target labels, we give access to these for purposes of analysis and illustration, keeping in mind that the bound is not fully estimable in practice. 

We perform experiments on two image classification tasks, described further below, using three different neural network architectures. The architectures used are: a modified version of LeNet-5 due to~\citet{zhou2019nonvacuous}, a fully connected network and a version of ResNet50~\citep{He2016DeepRL}. These specific architectures were picked as examples due to their varying parameter size and complexity. The experiments are repeated for 5 distinct random seeds, with the exception of the varying of image size where we only conduct the experiment for one seed.

We learn prior and posterior network weights as described in Section~\ref{sec:datadep}. When both sets of weights have been trained, we use these as the means for prior and posterior distributions $\pi$ and $\rho$, chosen to be isotropic Gaussians with equal variance, $\sigma$. Each bound is calculated for each pair of prior and posterior. To estimate the expectation over the posterior we sample 5 pairs of classifiers from the posterior and average their bound components (e.g., source risk) to get an estimate of the bound parts. When this has been calculated, we perform a small optimization step to choose the free parameters of the different bound through a simple grid search over a small number of combinations. We use the combination that produces the lowest minimum bound and account for testing all $k$ combinations by applying a union bound argument, modifying the certainty parameter $\delta$ to $\delta/k$. 
%
When calculating the MMD, we use the linear statistic for the MMD as detailed in \citet{gretton2012kernel} with the kernel $k(x,y)=\exp(\frac{-\|x-y\|^2}{2\kappa^2})$.
This calculation is averaged over 10 random shuffles of the data for a chosen bandwidth, $\kappa>0$. The process is repeated for different choices of bandwidth (see Appendix for details) and the maximum of the results is taken as the value of the MMD. Note that we calculate the MMD in the input space and have not adjusted it for sample variance. When calculating the importance weights we assume here that the maximum weight, $\beta_\infty$, is uniformly bounded as the bound would potentially be vacuous otherwise. In addition, we will consider the importance weights to be perfectly computed for simplicity.

We construct two tasks from standard data sets which both fulfill  Assumption~\ref{asmp:covshift_overlap} by design, one based on digit classification and one real-world task involving classification of X-ray images. These are meant to represent realistic UDA tasks where neural networks are the model class of choice. 

%
% MNIST
%
\subsection{Task 1: MNIST mixture}
%
\begin{figure}[t!]
    \centering
    \includegraphics[width=.49\columnwidth]{Images/labelshift_mnist.png}
    \caption{Example of source label densities in the task with the mix of MNIST and MNIST-M samples. An example from each of the two data sets can be seen on the right, the upper one being from MNIST and lower one from MNIST-M. The target domain is  the complement of source samples.}
    \label{fig:task}
\end{figure}
%
MNIST~\citep{lecun_gradient-based_1998} is a digit classification data set containing 70000 images widely used as a benchmark for image classifiers. MNIST-M was introduced by \citet{ganin_domain-adversarial_2016} to study domain adaptation and is a variation of MNIST where the digits have been blended with patches taken from images from the BSDS500 data set~\citep{bsds500}. 
%
We use MNIST and MNIST-M to construct source and target domains, both of which contain samples from each data set, but with different label density for images from MNIST and MNIST-M. To create the source data set, we start with images labeled ``0'' by adding 1/12th of the samples from MNIST-M and 11/12th of the samples from MNIST, we increase the proportion from MNIST-M by 1/12 for each subsequent label, ``1'', ``2'', and so on.
The complement of the source samples is then used as the target data, see Figure~\ref{fig:task} for an illustration. We make this into a binary classification problem by relabeling digits 0-4 to  ``0'' and the rest to  ``1''. The supremum density ratio $\beta_\infty \approx 11$ (see Theorem~\ref{thm:mult}) is known and the mixture guarantees overlap in the support of the domains (Assumption~\ref{asmp:covshift_overlap}).
%
% X-RAY
%
\subsection{Task 2: X-ray mixture}
ChestX-ray14~\citep{wang2017chestxray} and CheXpert~\citep{Irvin2019CheXpertAL} are data sets of chest X-rays and labeled according to the presence of common thorax diseases. The data sets contain \num{112120} and \num{224316} labeled images, respectively. Since the two data sets do not have full overlap in labels, we use the subset of labels for which there is overlap. The labels which occur in both data sets are: No finding, Cardiomegaly, Edema, Consolidation, Atelectasis, and Pleural Effusion. In addition, there is an uncertainty parameter present in the CheXpert data set which indicates how certain a label is. As we consider binary classification, we set all labels that are uncertain to positive. Therefore, a single image in the CheXpert data set might have multiple associated labels. For most experiments we resize all the images in the data sets to 32x32 to be able to use the same architectures for both tasks. However, we also conduct a small experiment with ResNet50 where larger image sizes are considered.

We take 20\% of chestX-ray14 and add it to CheXpert to create our source data set. The target is the remaining part of ChestX-ray14 which is then \num{89696} images compared to the \num{246072} of the source. With this, we know from Appendix~\ref{app:chest} that $\beta_\infty \approx 11$ and we have overlap. We turn this into a binary classification problem in a one-vs-rest fashion by picking one specific label to identify, relabeling images with ``1'' if it is present, and setting the labels of images with any other finding to ``0''. In this work, we consider only the task of classifying ``No Finding''.


%
% RESULTS
%
\section{Results}
%%%%% - data-dependent priors work and significantly tightens the bounds
%%%%% - talk about different terms and why they behave the way they do. 
%%%%% - IW and Add. are the only tight bounds
%%%%% - different architectures does not give us very much, their training time before convergence likely differs a lot and as such it is hard to say what would be the best experiment to perform here
%%%%%
%
\begin{figure*}[t!]
    \centering
      \begin{subfigure}{0.49\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/task2_nonvacuous_lenet.pdf}
        \caption{MNIST/MNIST-M task.}
        \label{fig:2nonvaclenet}
    \end{subfigure}%
        \begin{subfigure}{0.49\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/task6_nonvacuous_lenet.pdf}
        \caption{CheXpert+ChestX-ray14 task}
        \label{fig:6nonvaclenet}
    \end{subfigure}%
    \caption{The tightest bounds achieved on the LeNet-5 architecture. This illustrates the tightening effect of using data-dependent priors. Non-vacuous bounds are obtained when using data to inform the prior. The shaded area between 0.5 and 1 is where a random classifier would perform on average, and the shaded area above 1 signifies vacuity.}
    \label{fig:nonvacdatadep}
\end{figure*}
%
As expected, when we compute bounds with data-dependent priors, we achieve bounds which are substantially tighter than without them, as seen clearly by comparing Figure~\ref{fig:nodatadep} to Figure~\ref{fig:nonvacdatadep}. We also observe that the additive bound (\add{}) due to \citet{germain_pac-bayesian_2013} is the tightest overall for both tasks, followed closely by the \iw{} bound. The latter is not so surprising as when we apply data-dependent priors, there is effectively a point in training where the $\KL$-divergence between prior and posterior networks is very small. Moreover, due to overlap, the weighted source error is equal to the target error in expectation. Thus the only sources of looseness left is the error in the approximation of the expectation over the posterior and the $\log \frac{1}{\delta}$ term which is very small here. We can also see in Figure~\ref{fig:2bestbound} and \ref{fig:6bestbound} that the minimum of the \iw{} bound is often very close to the minimum of the additive bound. However, unlike \iw{}, the \add{} bound relies on access to target labels in to compute the term $\lambda_\rho$ (see further discussion below). 

\begin{figure*}[t!]
     \centering
      \begin{subfigure}{0.32\textwidth}
    \centering
        \includegraphics[width=1\textwidth]{Images/2_fc_beta_portion_0_0.03.pdf}
        \caption{\mult{} bound, $\alpha=0$}
        \label{fig:2fcbetaboundparts0}
    \end{subfigure}%
        \begin{subfigure}{0.32\textwidth}
    \centering
        \includegraphics[width=1\textwidth]{Images/2_fc_disrho_portion_0.3_0.03.pdf}
        \caption{\add{} bound, $\alpha=0.3$}
        \label{fig:2fcdisboundparts03}
    \end{subfigure}%
    %      \begin{subfigure}{0.24\textwidth}
    % \centering
    %     \includegraphics[width=.92\textwidth]{Images/2_fc_iw_portion_0.3.pdf}
    %     \caption{IW bound}
    %     \label{fig:2fciwboundparts03}
    % \end{subfigure}
        \begin{subfigure}{0.32\textwidth}
    \centering
        \includegraphics[width=1\textwidth]{Images/2_fc_mmd_portion_0.3_0.03.pdf}
        \caption{\mmd{} bound, $\alpha=0.3$}
        \label{fig:2fcmmdboundparts03}
    \end{subfigure}%
    \caption{An illustration of constituent parts for three of the bounds with the fully connected architecture on the MNIST mixture task. $\sigma=0.03$ }
    \label{fig:2boundparts}
\end{figure*}

The evolution of the different bounds during training is shown for both tasks in Figure~\ref{fig:datadeptrain}. Of course, all bounds will increase at some point as training progresses and the prior and posterior diverges further from each other and $\KL$ increases. While \add{} is consistently very tight, we note that the $\lambda_\rho$ term which we cannot observe might be a significant part of the bound when the $\KL$-term is low as we can see in Figure~\ref{fig:2fcdisboundparts03}. 
This is an issue for the additive bound since if we have sufficiently small variance of the posterior then the disagreement will be low, using informed priors will make the $\KL$ small while using neural networks often lead to having a low source error. This will leave only the constant term, $\log \frac{1}{\delta}$ and the unobservable $\lambda_\rho$ terms and in those situations the bound might even be dominated by the unobservable term.

\begin{figure*}[t!]
    \centering
      \begin{subfigure}{0.49\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/task2_bestboundvserr.pdf}
        \caption{MNIST/MNIST-M}
        \label{fig:2bestbound}
    \end{subfigure}%
        \begin{subfigure}{0.49\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/task6_bestboundvserr.pdf}
        \caption{CheXpert+ChestX-ray14}
        \label{fig:6bestbound}
    \end{subfigure}%
    \caption{An illustration of the minimum bound value achieved by each of the three architectures on both tasks. The lowest target error achieved is indicated by a  black marker with a vertical dotted line through it.}
    \label{fig:bestbound}
\end{figure*}

The multiplicative bound of \citep{germain16} (\mult{}) suffers from the amplification of the source error $e_S$  and $\KL$ term by the factor $\beta_\infty$, and is generally larger than the \add{} and \iw{} bounds. Conceptually, the \mult{} and \iw{} bounds are similar, but in the former, the loss is multiplied uniformly by the largest weight. For tasks where certain inputs with high loss are more uncommon in the target domain than in the source domain, this is especially detrimental. The \mmd{} bound is initially dominated by the MMD distance between inputs from the source and target domains, as shown in \ref{fig:2fcmmdboundparts03}, which is large and independent of the learned hypothesis. As such, this term cannot be reduced by optimization, without, for example, computing it in representation space~\citep{long_learning_2015}. With this approach, unobservable errors due to non-invertible representations must be accounted for~\citep{johansson_support_2019}. 

Experiments on using bounds for early stopping and model selection with different architectures yield the results seen in Figure~\ref{fig:bestbound}. We can see that the errors achieved by  terminating training at the smallest bound value (colored markers) do not coincide with the best-achieved target performance during training (denoted by the vertical dotted lines). Clearly, the bounds are not tight enough to do early stopping. This is a result of the sample generalization term $\KL$ increasing during training. For other analyses, this need not be the case. For larger architectures, the early-stopped models are closer to the best target models. If we instead look at the same figure again, but this time focus on utility for model selection we find something interesting. It seems that the bounds might be useful in this regard as they consistently have lower values for architectures/models which perform well. However, to be able to say this conclusively a more thorough study with different learning setups must be done. Both of the two previous observations should be contextualised with the fact that the domain shift terms are not dependent on the model as such, but amplify looseness in the case of the \mult{} and \iw{} bounds. For the \mmd{} and \add{} bounds, increased looseness during training is an artifact only of sample generalization. 
\begin{figure*}[t!]
    \centering
      \begin{subfigure}{0.49\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/2_lenettraining_0.3_0.03.pdf}
        \caption{MNIST/MNIST-M task}
        \label{fig:2lenettrain03}
    \end{subfigure}%
        \begin{subfigure}{0.49\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_lenettraining_0.3_0.03.pdf}
        \caption{CheXpert+ChestX-ray14 task}
        \label{fig:6lenettrain03}
    \end{subfigure}%
    \caption{Bounds evaluated at different points during training of the LeNet-5 architecture. $\alpha=0.3$, $\sigma=0.03$. The shaded areas represent one standard deviation.}
    \label{fig:datadeptrain}
\end{figure*}

\begin{figure} %%% make this figure wrap too?
    \centering
     \begin{subfigure}{0.46\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/imagesizeplot.pdf}
        \caption{Minimum bound value and target error on the X-ray task with different sizes of input images. Architecture used is ResNet50, $\alpha=0.3$, $\sigma=0.003$}
        \label{fig:imagesize}
    \end{subfigure}%
    \;
        \begin{subfigure}{0.46\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/2_lenet_alphaplot.pdf}
        \caption{The minimum value of the bounds on the MNIST/MNIST-M task for different values of $\alpha$. The architecture used is LeNet-5.}
        \label{fig:2lenetalpha}
    \end{subfigure}
    \caption{a) show results of varying image sizes when training the ResNet50 network on the X-ray task. b) shows how the minimum value of the bound varies with $\alpha$. }
    \label{fig:oddexp}
\end{figure}
As we can see in Figure~\ref{fig:imagesize}, when we vary the size of the images we give to the ResNet50 architecture we observe that the error seems to decrease for the larger image sizes. Although, the minimum bound value achieved does not seem to follow the same trend consistently. This is likely the result of the amount of epochs trained for both prior and posterior. In Figure~\ref{fig:2lenetalpha}, we see that the choice of prior sample proportion $\alpha$ makes some change to the smallest bound achieved. We also see the minimum bound values grow for large values of $\alpha$, indicating that the remaining data is better spent calculating the bound than informing the prior in this case. We can also infer from Figure~\ref{fig:nodatadep} that using no data to inform the prior is worse than using some. The overall shape is consistent with the results reported in \citet[Figure 1]{dziugaite_role_2020}.
%% different image sizes
%
% DISCUSSION
%
\section{Discussion}
From our survey of the literature, it is clear that only a small handful of analyses of UDA generalization can be informative as practical bounds on target domain performance. The main obstacle for computing existing bounds is that they are vacuous or intractable to compute for the kinds of models which perform the best on common UDA benchmarks---deep neural networks. A potential remedy is the use of PAC-Bayes bounds, which perform well once they are applied with data-dependent priors; without this they are vacuous. In our experiments, the \add{} bound with the unobservable term is the tightest which is unsurprising given its dependence on target labels. Furthermore, we note that the application of importance weights also performs very well as the setting is sufficiently benign. As such we can say that in this setting we can achieve the desiderata of a tractably computable, tight bound using the \iw{} bound. However, recall that the guarantee we get is on a distribution over classifiers and not on one specific classifier. It should be noted, however, that the \iw{} bound can become vacuous in certain situations where the worst-case density ratio, $\beta_\infty$, is large and either the $\KL$ term or errors on underrepresented classes is large enough.

We found that the lowest value of the bounds achieved during training does not in general correspond to the best performing model on target. This tells us that these bounds are not useful metrics for early stopping. Further, the findings for using bound values for model selection are inconclusive, more experiments have to be conducted to answer this question satisfactorily. During training, we see that the dynamics are dominated by the KL-divergence term, inherent to PAC-Bayes analysis, as training progresses. This reinforces our view that these bounds might be useful in getting performance estimates of methods at one particular point and not over several points during training if we do not have access to a large sample. This issue might be ameliorated by regularizing towards the prior during training, although this introduces yet another optimization as we now have to find the optimal regularization strength. In addition, it is not certain whether this will have any adverse effects on the final performance of the learned classifier.

%%%% Movde from results

A limitation of this work is that the bounds are cumbersome to compute and it is possible to do several optimizations in the process of producing the bounds. We have tried to do as few as possible in the name of practicality. We list some of the possible further optimizations in Appendix~\ref{app:exp} for the reader's consideration. The impracticality of computing PAC-Bayes bounds is a known issue that has had some work done by \citet{viallard_general_2021} where they introduce an approach which would remove the computation of expectation over the posterior. In addition, in this work the computation of test errors have dominated the computation time. To produce the results one has to compute the predictions at least 50 times(5 pairs of sampled models from the posterior and 5 random seeds) for each datapoint in the bound for a single choice of prior. This will naturally consume increasing amounts of time with larger data sets.

Furthermore, the overlap assumption will not hold for all real-world applications. In fact, many of the benchmarks for algorithm development, such as the SVHN$\rightarrow$MNIST task~\citep{ganin_domain-adversarial_2016} blatantly violate overlap, since images of house numbers and handwritten digits differ vastly in pixel space. Examples where overlap holds by definition include when the target domain represents a subpopulation of a larger population given by the source domain, e.g., women (target) among all patients (source) with a medical condition. Although an easy learning problem on its face, the optimal model in the full population may not be optimal for the subpopulation. Even when overlap is violated, many share the intuition that overlap may hold in a transformed space~\citep{wu_asymmetric_2019}, representative of the core aspects of the problem---a digit is a digit, whether on a house or a postcard. 

The strictness of the overlap assumption has been studied by \citet{DAMOUR21} where it was found that even for Gaussian distributions with insubstantial differences in mean parameters, overlap vanishes in high dimensions. Motivated by this fact we might wish to adopt relaxed versions of our assumptions or completely novel ones which still guarantee consistent estimation. A first step could be to require overlap only in a transformed space, not in the input space, like in \citet{wu_asymmetric_2019} or  only requiring overlap in specific regions and leveraging assumptions on  ``closeness'' in the other regions, as in \citet{johansson_support_2019}. Further, task-specific assumptions are likely needed for a more complete description of out-of-distribution generalization. We mean task-specific in the sense that the assumptions will depend on the structure on the problem and the data-generating process~\citep{hansen2008} or other approaches. Overcoming this gap is a important direction of future study. 

Another limitation of this work is that the hypotheses do not optimize for adaptation to the target domain, which might be achieved through representation learning as in \citet{ganin_domain-adversarial_2016} or minimization of a weighted loss~\citep{shimodaira_improving_2000}. Our setting is representative for tasks where the target domain is unknown during training, but known when computing the bounds. Further, in this work we have assumed that we are able to estimate the importance weights exactly which may not be feasible in high-dimensional settings. In addition, there is no guarantee that the estimation error of the weights is small and thus even a small misestimation may have quite large implications for the resulting bound.

%%% everyone should probably think about the utility of the theory they are making. What are the bounds good for and what value do they take if computable. 
Future work regarding generalization bounds should preferably comment upon usefulness of their bound as a practical guarantee for performance, which is something that is often lacking. Ideally this would extend to explicit calculation if the bound is possible to compute. New bounds are often used as inspiration towards new algorithms which are hoped to result in more generalizable models. However, this is seldom guaranteed by theory and verified only in limited settings empirically. 

%%% how does one achieve a tight bound using what is presently available in the literature? Should they use bounds as performance guarantees? Probability of not holding etc.
Our results offer indications for how to obtain tractable and tight bounds for neural networks used in UDA tasks with available tools. If overlap can be assumed to hold, then use the \iw{} bound, estimate importance weights using density estimation~\citep{sugiyama_density_2012} or probabilistic classifiers and apply data-dependent priors. The amount of data to use and how long to train your prior etc. are all task dependent and thus some engineering is necessary to pick optimal values. %
If this cannot be assumed, the most promising approach to get bounds which fulfill our desiderata in this case would be to use the \mmd{} bound as this does not technically rely on overlap and is tractable to compute for neural networks. This relies on the added assumptions of the pointwise loss being bounded by a function in the associated reproducing-kernel Hilbert space, which may or may not hold. The nature of this assumption makes it less useful since no test for this is available absent overlap and is similar in nature to assuming that joint optimal error is small. However, if the function under estimation is believed to be smooth, the assumption is more plausible.
In conclusion, it is clear that the general case demands new research, and alternative, task-specific assumptions, to allow tight performance guarantees for realistic problems. In either setting, we conjecture that the tightest bounds will be coupled to the training procedure.
\section*{Acknowledgements}
This work was supported in part by the Wallenberg AI, Autonomous Systems and
Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.

The computations and data handling were enabled by resources provided by the Swedish National Infrastructure for Computing (SNIC) at Chalmers Centre for Computational Science and Engineering (C3SE) partially funded by the Swedish Research Council through grant agreement no. 2018-05973.
Mikael hman at C3SE is acknowledged for his assistance concerning technical and implementation aspects in making the code run on the C3SE resources.
\bibliography{tmlr}
\bibliographystyle{tmlr}

\appendix

\section{Experimental details}
\label{app:exp}
%%% Describe the setup in detail with the whole pipeline. What epochs, learning rates, regularisation strength have been used etc.
%The bounds are evaluated using the 0-1 loss but the training is done with the binary cross entropy loss so as to be differentiable.
The experiments were carried out with a modified version of LeNet-5 due to \citet{zhou2019nonvacuous}, a 1024-600-600-2 fully connected network similar to the one used in \citet{rivasplata_pac-bayes_2020} and a ResNet50 architecture~\citep{He2016DeepRL} as mentioned before. Since the 0-1 loss is not differentiable we substitute it with the binary cross entropy loss which is and provides a tight upper bound on the 0-1 loss. We train with SGD with momentum 0.95 as the optimizer with a batch size of 128. The learning rate was chosen to be $3\times 10^{-3}$ for the LeNet-5 and fully connected architectures while for ResNet50 $3\times 10^{-4}$ was used. The images from MNIST and MNIST-M were padded with zeros to 32$\times$32 images. This was done to be able to use them with the ResNet50V2 implementation in Tensorflow which does not support smaller image sizes.


The training procedure went as follows. We load the data and construct our source and target. The source data is then split into an $\alpha$-fraction $S_\alpha$ which is used to train the prior network on for $|S_\alpha|/b$ iterations, where $b$ is the batch size, to get an informed prior. This is then used as the starting point when training the posterior which is done on all the available data. During training of the posterior network we save 10 network weights during the first epoch and then at the end of every subsequent epoch until termination. We terminate the training of the posterior network when we have trained 5 epochs. %for the MNIST+MNIST-M task and $\epsilon\in \{0.15,0.1,0.05\}$ for the CheXpert + ChestX-ray14 task. 

When training the posterior is terminated we save the weights and proceed with the computation of the bound. In contrast with \citet{dziugaite_role_2020} we not only consider the bound at the point of termination but also at previous points during training. This is done with the goal of gaining an understanding of the bounds' behaviour during training. 

We assume that our prior can be modeled by an isotropic gaussian akin to earlier work, this is done to get an easily computable closed form expression of the KL divergence. To pick a good value of the $\sigma$ parameter one could sweep over some range of values and then use a union bound argument to be able to pick the best result with only a small penalty to the bound. We do not do such optimization and simply pick a value.

We perform a small optimization step when determining the free parameters of the bound. For the bounds from \citet{germain_pac-bayes_2020}, i.e. $a,b,\gamma$ and $\omega$, we iterate over values in the range $\{1\times 10^{-3}, 5\times 10^{-3}, 1\times 10^{-2},\dots, 5 \times 10^{4}, 1\times 10^{5}\}$ for both free parameters and pick the combination which yields the lowest bound. For the \mmd{} and \iw{} bounds we pick $\gamma$ from the range $\{1\times 10^{-3}, 5\times 10^{-3}, 1\times 10^{-2}, 5\times 10^{-2}, 1\times 10^{-1}, 5\times 10^{-1}, 9.9\times 10^{-1}\}$, choosing the one which yields the lowest bound.

\subsection{Possible optimization when producing the bounds}

When computing these bounds there are a lot of different parameter and hyperparameter choices to make, many of which can be optimized. We first have to train at least one model(depending on how many values of $\alpha$ to consider) with all parameter choices that entails. Then we sample models according to whatever the posterior distribution is; the amount depending on how well we want to estimate the expectation over the posterior. All PAC-Bayes bounds contain some sort of parameter which is free to choose and we must do a at least a rudimentary parameter search to arrive at a good bound. In addition to all these choices of parameters we can of course optimise these bounds even further. Some that we did not perform for this work are: Optimise the representation for smaller MMD, L2 regularisation towards the prior for each specific parameter set (also entails finding the optimal regularization strength) and perform even finer grid searches for the optimal bound parameters to name just a few.
\section{Importance weights and how to derive them}
\label{app:chest}
So assume that we do a mixing of two data sets (let's call them 0 and 1) to form two domains.
We want to derive the way we should calculate and subsequently use the importance weights for this situation. We will do this first for the CXR task. In this task the underlying label set is multi-label and as such we need to make it into categorical variables before calculating and applying weights. We achieve this by making the problem into a binary classification problem where we try to predict if there is a finding or not. From this point we may calculate the importance weights as follows:
$$
w=\frac{T(x,y)}{S(x,y)}=\frac{T(x|y)T(y)}{S(x|y)S(y)}=\frac{T(x|y)T(y)}{(S(x|y,D=1)S(D=1|y)+S(x|y,D=0)S(D=0|y))S(y)}
$$
If we now assume that we have only no examples from data set 0 in the target as in the CXR task then we have the following
$$
w=\frac{T(x|y)T(y)}{(S(x|y,D=1)S(D=1|y)+\underbrace{S(x|y,D=0)S(D=0|y)}_{=0,~ \text{as T(y)=0 when this is non-zero}})S(y)}=\frac{T(x|y)T(y)}{(S(x|y,D=1)S(D=1|y)S(y)}
$$
Now we note that $T(x|y)$ and $S(x|y,D=1)$ cancel as the conditional distribution of these should be the same as we mixed uniformly over the initial label and T(x|y,D=1)=T(x|y). We are thus left with
$$
w(y)=\frac{T(y)}{S(y)S(D=1|y)}=\frac{\#\text{examples with label y in T}}{\# T}/\frac{\#\text{examples with label y in S which come from data set 1}}{\# S}
$$
Through this argument we can see that the final importance weight is in the case where we use 20\% of the images from data set 1, which will become the target, to mix with data set 0 to become the source. Assume that the initial amount from data set 1 is $m_1$.
$$
w=\frac{\# S}{\# T}\cdot \frac{\#\text{examples w/ label y in T}}{\#\text{examples w/ label y in S which come from data set 1}} =\frac{\#S}{\# T}\frac{0.8m_1}{0.2m_1}=4\frac{\#S}{\# T}
$$
We can do the same type of argument for the MNIST/MNIST-M mix. There we have a more balanced data set where the classes are evenly distributed in amount across source and target.
%%%%% argument about the subsequent binarization f_bin(y)= 0/1

$$
w=\frac{T(x,y)}{S(x,y)}=\frac{T(x|y,D=1)T(D=1|y)+T(x|y,D=0)T(D=0|y))T(y)}{(S(x|y,D=1)S(D=1|y)+S(x|y,D=0)S(D=0|y))S(y)}
$$
Since the labels are balanced between the data sets $\frac{T(y)}{S(y)}=1$. Since we have mixed the datapoints for each label in a uniform fashion we know what $\frac{T(x|y,D=0)}{S(x|y,D=0)}=1$ for every label. As such we can calculate the weight as 
$$
w(x|y,D=0)=\frac{\#\text{examples w/ label y in T which come from data set 0}}{\#\text{examples w/ label y in S which come from data set 0}} 
$$
and similar for datapoints from the other data set.
\section{Additional results}
\subsection{Constituent parts of bounds}
\begin{figure*}[t!]
    \centering
      \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/2_fc_beta_portion_0.pdf}
        \caption{Multiplicative bound}
        \label{fig:2fcbetaboundparts}
    \end{subfigure}%
        \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/2_fc_disrho_portion_0.pdf}
        \caption{Additive bound}
        \label{fig:2fcdisboundparts}
    \end{subfigure}%
         \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/2_fc_iw_portion_0.pdf}
        \caption{IW bound}
        \label{fig:2fciwboundparts}
    \end{subfigure}%
        \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/2_fc_mmd_portion_0.pdf}
        \caption{MMD bound}
        \label{fig:2fcmmdboundparts}
    \end{subfigure}%
    \caption{An illustration of constituent parts of each of the four bounds with the fully connected architecture on the MNIST mixture task. $\alpha=0$}
    \label{fig:2boundparts0}
\end{figure*}

\begin{figure*}[t!]
     \centering
      \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_fc_beta_portion_0.pdf}
        \caption{Multiplicative bound}
        \label{fig:6fcbetaboundparts0}
    \end{subfigure}%
        \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_fc_disrho_portion_0.pdf}
        \caption{Additive bound}
        \label{fig:6fcdisboundparts0}
    \end{subfigure}%
         \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_fc_iw_portion_0.pdf}
        \caption{IW bound}
        \label{fig:6fciwboundparts0}
    \end{subfigure}%
        \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_fc_mmd_portion_0_0.003.pdf}
        \caption{MMD bound}
        \label{fig:6fcmmdboundparts0}
    \end{subfigure}%
    \caption{An illustration of constituent parts of each of the four bounds with the fully connected architecture on the X-ray task. $\alpha=0$}
    \label{fig:6boundparts0}
\end{figure*}

\begin{figure*}[t!]
     \centering
      \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_lenet_beta_portion_0.pdf}
        \caption{Multiplicative bound}
        \label{fig:6lenetbetaboundparts0}
    \end{subfigure}%
        \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_lenet_disrho_portion_0.pdf}
        \caption{Additive bound}
        \label{fig:6lenetdisboundparts0}
    \end{subfigure}%
         \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_lenet_iw_portion_0.pdf}
        \caption{IW bound}
        \label{fig:6lenetiwboundparts0}
    \end{subfigure}%
        \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_lenet_mmd_portion_0_0.003.pdf}
        \caption{MMD bound}
        \label{fig:6lenetmmdboundparts0}
    \end{subfigure}%
    \caption{An illustration of constituent parts of each of the four bounds with the LeNet-5 architecture on the X-ray task. $\alpha=0$}
    \label{fig:6lenetboundparts0}
\end{figure*}
\begin{figure*}[t!]
     \centering
      \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_resnet_beta_portion_0.pdf}
        \caption{Multiplicative bound}
        \label{fig:6resnetbetaboundparts0}
    \end{subfigure}%
        \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_resnet_disrho_portion_0.pdf}
        \caption{Additive bound}
        \label{fig:6resnetdisboundparts0}
    \end{subfigure}%
         \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_resnet_iw_portion_0.pdf}
        \caption{IW bound}
        \label{fig:6resnetiwboundparts0}
    \end{subfigure}%
        \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_resnet_mmd_portion_0_0.003.pdf}
        \caption{MMD bound}
        \label{fig:6resnetmmdboundparts0}
    \end{subfigure}%
    \caption{An illustration of constituent parts of each of the four bounds with the ResNet50 architecture on the X-ray task. $\alpha=0$}
    \label{fig:6resnetboundparts0}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Alpha = 0.3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{figure*}[t!]
    \centering
      \begin{subfigure}{0.46\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/2_fc_beta_portion_0.3_0.03.pdf}
        \caption{Multiplicative bound, $\sigma=0.03$}
        \label{fig:2fcbetaboundparts03}
    \end{subfigure}%
         \begin{subfigure}{0.46\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/2_fc_iw_portion_0.3.pdf}
        \caption{IW bound, $\sigma=0.03$}
        \label{fig:2fciwboundparts03}
    \end{subfigure}%
    \caption{An illustration of constituent parts of each of the four bounds with the fully connected architecture on the MNIST mixture task. $\alpha=0.3$}
    \label{fig:2boundparts03}
\end{figure*}

\begin{figure*}[t!]
     \centering
      \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_fc_beta_portion_0.3_0.03.pdf}
        \caption{Multiplicative bound}
        \label{fig:6fcbetaboundparts03}
    \end{subfigure}%
        \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_fc_disrho_portion_0.3_0.03.pdf}
        \caption{Additive bound}
        \label{fig:6fcdisboundparts03}
    \end{subfigure}%
         \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_fc_iw_portion_0.3_0.03.pdf}
        \caption{IW bound}
        \label{fig:6fciwboundparts03}
    \end{subfigure}%
        \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_fc_mmd_portion_0.3_0.03.pdf}
        \caption{MMD bound}
        \label{fig:6fcmmdboundparts03}
    \end{subfigure}%
    \caption{An illustration of constituent parts of each of the four bounds with the fully connected architecture on the X-ray task. $\alpha=0.3$, $\sigma=0.03$}
    \label{fig:6boundparts03}
\end{figure*}

\begin{figure*}[t!]
     \centering
      \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_lenet_beta_portion_0.3_0.03.pdf}
        \caption{Multiplicative bound}
        \label{fig:6lenetbetaboundparts03}
    \end{subfigure}%
        \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_lenet_disrho_portion_0.3_0.03.pdf}
        \caption{Additive bound}
        \label{fig:6lenetdisboundparts03}
    \end{subfigure}%
         \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_lenet_iw_portion_0.3_0.03.pdf}
        \caption{IW bound}
        \label{fig:6lenetiwboundparts03}
    \end{subfigure}%
        \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_lenet_mmd_portion_0.3_0.03.pdf}
        \caption{MMD bound}
        \label{fig:6lenetmmdboundparts03}
    \end{subfigure}%
    \caption{An illustration of constituent parts of each of the four bounds with the LeNet-5 architecture on the X-ray task. $\alpha=0.3$, $\sigma=0.03$}
    \label{fig:6lenetboundparts03}
\end{figure*}
\begin{figure*}[t!]
     \centering
      \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_resnet_beta_portion_0.3_0.03.pdf}
        \caption{Multiplicative bound}
        \label{fig:6resnetbetaboundparts03}
    \end{subfigure}%
        \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_resnet_disrho_portion_0.3_0.03.pdf}
        \caption{Additive bound}
        \label{fig:6resnetdisboundparts03}
    \end{subfigure}%
         \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_resnet_iw_portion_0.3_0.03.pdf}
        \caption{IW bound}
        \label{fig:6resnetiwboundparts03}
    \end{subfigure}%
        \begin{subfigure}{0.24\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_resnet_mmd_portion_0.3_0.03.pdf}
        \caption{MMD bound}
        \label{fig:6resnetmmdboundparts03}
    \end{subfigure}%
    \caption{An illustration of constituent parts of each of the four bounds with the ResNet50 architecture on the X-ray task. $\alpha=0.3$, $\sigma=0.03$}
    \label{fig:6resnetboundparts03}
\end{figure*}

%
% VARYING ALPHA
%
\subsection{Best bounds achieved for different prior sample proportions}
\begin{figure*}[t!]
    \centering
      \begin{subfigure}{0.49\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/2_fc_alphaplot.pdf}
        \caption{Fully connected}
        \label{fig:2fcalpha}
    \end{subfigure}%
        \begin{subfigure}{0.49\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/2_resnet_alphaplot.pdf}
        \caption{ResNet50}
        \label{fig:2resnetalpha}
    \end{subfigure}%
    \caption{The minimum value of the bound on the MNIST mixture task for different values of $\alpha$.}
    \label{fig:2alpha}
\end{figure*}

\begin{figure*}[t!]
    \centering
     \begin{subfigure}{0.32\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_lenet_alphaplot.pdf}
        \caption{LeNet-5}
        \label{fig:6lenetalpha}
    \end{subfigure}%
      \begin{subfigure}{0.32\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_fc_alphaplot.pdf}
        \caption{Fully connected}
        \label{fig:6fcalpha}
    \end{subfigure}%
        \begin{subfigure}{0.32\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_resnet_alphaplot.pdf}
        \caption{ResNet50}
        \label{fig:6resnetalpha}
    \end{subfigure}%
    \caption{The minimum value of the bounds on the X-ray task for different values of $\alpha$.}
    \label{fig:6alpha}
\end{figure*}

\subsection{Bound during training}
\begin{figure*}[t!]
    \centering
      \begin{subfigure}{0.32\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/2_lenettraining_0.3_0.03.pdf}
        \caption{LeNet-5, $\alpha=0.3$, $\sigma=0.03$}
        %\label{fig:betabestbound}
    \end{subfigure}%
        \begin{subfigure}{0.33\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/2_fctraining_0.3_0.03.pdf}
        \caption{Fully connected, $\alpha=0.3$, $\sigma=0.03$}
        %\label{fig:disbestbound}
    \end{subfigure}%
         \begin{subfigure}{0.32\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/2_resnettraining_0.3_0.03.pdf}
        \caption{ResNet50, $\alpha=0.3$, $\sigma=0.03$}
       % \label{fig:iwbestbound}
    \end{subfigure}%
    \caption{The evolution of the bounds during training on the MNIST mixture task when we use $30\%$ of our sample to inform the prior.}
    \label{fig:2boundstraining}
\end{figure*}

\begin{figure*}[t!]
    \centering
      \begin{subfigure}{0.32\textwidth}
    \centering
       \includegraphics[width=.92\textwidth]{Images/6_lenettraining_0.3_0.03.pdf}
        \caption{LeNet-5, $\alpha=0.3$, $\sigma=0.03$}
        \label{fig:6lenettraining03}
    \end{subfigure}%
        \begin{subfigure}{0.33\textwidth}
    \centering
       \includegraphics[width=.92\textwidth]{Images/6_fctraining_0.3_0.03.pdf}
        \caption{Fully connected, $\alpha=0.3$, $\sigma=0.03$}
       \label{fig:6fctraining03}
    \end{subfigure}%
         \begin{subfigure}{0.32\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_resnettraining_0.3_0.03.pdf}
        \caption{ResNet50, $\alpha=0.3$, $\sigma=0.03$}
       \label{fig:6resnettraining03}
    \end{subfigure}%
    \caption{The evolution of the bounds during training on the X-ray task when we use $30\%$ of our sample to inform the prior.}
    \label{fig:6boundstraining}
\end{figure*}

\begin{figure*}[t!]
    \centering
      \begin{subfigure}{0.32\textwidth}
    \centering
       \includegraphics[width=.92\textwidth]{Images/6_lenettraining_0.3_0.003.pdf}
        \caption{LeNet-5, $\alpha=0.3$, $\sigma=0.003$}
        \label{fig:6lenettraining03sigma}
    \end{subfigure}%
        \begin{subfigure}{0.33\textwidth}
    \centering
       \includegraphics[width=.92\textwidth]{Images/6_fctraining_0.3_0.003.pdf}
        \caption{Fully connected, $\alpha=0.3$, $\sigma=0.003$}
       \label{fig:6fctraining03sigma}
    \end{subfigure}%
         \begin{subfigure}{0.46\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{Images/6_resnettraining_0.3_0.003.pdf}
        \caption{ResNet50, $\alpha=0.3$, $\sigma=0.003$}
       \label{fig:6resnettraining03sigma}
    \end{subfigure}%
    \caption{The evolution of the bounds during training on the X-ray task when we use $30\%$ of our sample to inform the prior.}
    \label{fig:6boundstrainingothersigma}
\end{figure*}
\end{document}
