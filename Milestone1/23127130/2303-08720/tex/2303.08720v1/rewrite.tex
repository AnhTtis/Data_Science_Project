Main points to change
R1-----
- including the lacking literature aspects, i.e. bounds based on asymmetrically relaxed divergences, localization, and f-divergences. In particular, there should be a theoretical discussion on the tightness of such bounds. 

-since the experiments concern several seeds, including the deviation from the mean can be more informative on the behavior of the bounds. ### DONE, maybe comment on it somewhere, e.g. captions

-Either taking into account the estimation of importance weights, or clearly stating that it is assumed that they are known. ### DONE

-Stating that the importance weights are assumed to be uniformly bounded. ### DONE

-Adding a discussion on how K behaves for a neural network.
-------

Weaknesses:

R1-----
Some literature aspects of DA bounds are lacking, namely:
Asymmetrically relaxed discrepancies [3], where the importance weight is only required to be bounded in order to guarantee good adaptation, instead of requiring equality of distributions.
Localized discrepancies [1,2], where the restriction of the supremum defining the divergence to a proper subset of the hypothesis space yields significantly lower divergence measures as illustrated in [1].
Other important DA contributions with bounds leading to algorithms such as [4].
On importance weighting and the IW bound (Corollary 1):

- The IW bound proposed in the paper can become vacuous if . This can happen even if the source domain's support completely includes the target's. ### RESOLVED BY ADDITION BELOW
-In the IW bound, the estimation error of the weights is not taken into account, i.e. it assumes perfect computation of the importance weights . This goes against the spirit of the paper concerned with the tightness of generalization bounds. ### ADDED LINE IN SECTION 3 CLARIFYING THIS
-The MMD bound contains , a parameter of the kernel that controls the estimation error of the MMD. However, no discussion is provided on its behavior for neural networks, whereas a main motivation of the paper is the vacuity of classic generalization bounds in the case of neural networks, notably those relying on the VC dimension.
-------
Small edits:
R1-----
-In figure 5, a logarithmic scale figure for values close to the target error, and adding a grid, might facilitate reading. ### ADDED GRID

-In the discussion section, it is mentioned "The main obstacle for using existing bounds is that they ... ". I would say "computing" rather than "using", are such bounds are used to propose DA algorithms in several cases. ### CHANGED WORDING, although we maybe should change the whole sentence..
-------