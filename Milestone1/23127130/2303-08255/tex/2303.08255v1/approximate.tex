\section{Cross-Layer Approximation for Printed ML Bespoke Circuits}
\orange{

\begin{figure}[t]
\centering
\includegraphics[]{graphs/framework3-1.pdf}
\caption{Overview of our proposed framework flow diagram.
}
\label{fig:framework}
\end{figure}


In this section, we present our automated framework (Fig.~\ref{fig:framework}) for approximate printed ML circuits.
Briefly, our framework receives as input a trained model (e.g., dumped from scikit-learn) and performs a hardware-driven coefficient approximation.
Next, our framework prunes the generated synthesized netlist through a full search DSE by systematically prunning gates based on their significance and their switching activity.
Finally, to further boost power efficiency, a VOS exploration is performed in the derived Pareto space, where Pareto optimal approximate circuits are obtained.

\subsection{Hardware-Driven Coefficient Approximation}\label{sec:coa}


A weighted sum (as for example in the case of MLPs and SVMs) is expressed as:
\begin{equation}
    S=\sum_{1\leq i \leq N}{x_i\cdot w_i},
\end{equation}
where $w_i$ are the predefined coefficients (or weights) obtained after training, $x_i$ are the inputs, and $N$ is the number of coefficients.
In the case of bespoke ML architectures, these coefficients are hardwired within the circuit~\cite{Mubarik:MICRO:2020:printedml}.
As a result, the area (and power) of each bespoke multiplier $\mathrm{BM}_w$ required to compute the product $x\cdot w$, $\forall x$, is determined by the value of the coefficient $w$ and the width of the input $x$.
For example, Fig.~\ref{fig:mult8area} presents the area of $\mathrm{BM}_w$, $\forall w \in [-128,127]$ (i.e., 8-bit coefficients), for 4-bit and 8-bit input values.
For comparison, in the caption of Fig.~\ref{fig:mult8area} we also report the area of the conventional $4\times 8$ and $8 \times 8$ multipliers.
In both cases, the bespoke multipliers $\mathrm{BM}_w$ offer significantly lower area than the conventional multiplier for all the $w$ values.
Moreover, it is evident that the area of $\mathrm{BM}_w$ highly depends on $w$ and the input bitwidth.
However, similar trend is observed in Fig~\ref{fig:mult8area}a and~\ref{fig:mult8area}b, i.e., neighbouring $w$ values may offer significantly different area.
Importantly, in many cases the area may be nullified, e.g., when $w$ is a power of two.
%Note that we obtained identical results for different for $w$ and $x$ bitwidths.
This motivates us to investigate and propose a hardware-driven coefficient approximation, tailored for bespoke architectures, that replaces a coefficient value $w$ with a neighbouring value $\tilde{w}$ so that \texttt{AREA}($\mathrm{BM}_{\tilde{w}}$) $<$ \texttt{AREA}($\mathrm{BM}_w$).


Fig.~\ref{fig:multareasav} presents the area reduction that is achieved by our coefficient approximation with respect to several bespoke multipliers sizes (a-d).
To generate each boxplot in Fig.~\ref{fig:multareasav}, for all $w$, we select $\tilde{w}$ so that $\tilde{w}$ offers the lowest \texttt{AREA}($\mathrm{BM}_{\tilde{w}}$) and $\tilde{w} \in [w-e,w+e]$, where $e$ is a given threshold (x-axis).
Clipping is applied at the borders.
As shown in Fig.~\ref{fig:multareasav}, the obtained $\mathrm{BM}_{\tilde{w}}$ offer significantly lower area than the $\mathrm{BM}_w$.
Our coefficient approximation delivers a median area reduction of more than $19\%$ when $e=1$ while this value increases to $53\%$ when $e=4$.
Nevertheless, in most cases, for $e\geq4$ the area reduction becomes less significant.
For example, in Fig.~\ref{fig:multareasav}b, the median area reduction is $44\%$ for $e=4$ and increases to only $61\%$ for $e=10$.
In many cases, in Fig.~\ref{fig:multareasav}, the area reduction goes up to $100$\% or it is $0$\%.
The former is explained by the fact that $w$ was replaced by $\tilde{w}$ that was a power of two and thus the area reduction is $100$\% since $\tilde{w}$ features zero area.
On the other hand, in the cases that $w$ features the lowest area in the segment $[w-e,w+e]$, $w$ is not replaced and the area reduction is zero.



\begin{figure}[t!]
\centering
\includegraphics[]{graphs/multarea.pdf}
\caption{The area of the bespoke multiplier w.r.t. the coefficient value $w$. Two architectures are considered: a) 4-bit inputs and 8-bit coefficients and b) 8-bit inputs and 8-bit coefficients.
For reference the area of the conventional $4\times 8$ and $8\times 8$ multipliers is 83.61$mm^{2}$ and 207.43$mm^{2}$, respectively. Figure obtained from~\cite{DATE22:Armen}.
}

\label{fig:mult8area}
\end{figure}



\begin{figure}[t]
\centering
\includegraphics[width=0.45\columnwidth]{graphs/Boxplot_4x6_s.PDF}
\includegraphics[width=0.45\columnwidth]{graphs/Boxplot_4x8_s.PDF}\\
\includegraphics[width=0.45\columnwidth]{graphs/Boxplot_8x8_s.PDF}
\includegraphics[width=0.45\columnwidth]{graphs/Boxplot_12x8_s.PDF}
\caption{The area reduction delivered by our coefficient approximation when $(w-\tilde{w})\leq e$. Several bespoke multipliers are considered (a-d). Figure obtained from~\cite{DATE22:Armen}.}
\label{fig:multareasav}
\end{figure}

When replacing $w$ by $\tilde{w}$, the multiplication error is equal to $x\cdot(w-\tilde{w})$.
Thus, the error $\epsilon_S$ of the weighted sum is:
\begin{equation}\label{eq:wsumerror}
    \epsilon_S =\sum_{1\leq i \leq N}{x_i\cdot (w_i-\tilde{w}_i). }
\end{equation}
Considering positive inputs (see Section \ref{sec:bespoke}), by systematically selecting $\tilde{w}_i$ to balance the positive and negative errors (i.e., $w_i-\tilde{w}_i$), we can minimize~\eqref{eq:wsumerror}.

Given an MLP or SVM, we implement our proposed hardware-driven coefficient approximation as follows:
\begin{enumerate}[leftmargin=*]
\item Given the coefficients $w_i$ and the bitwidth of the inputs, we evaluate the area of all the bespoke multipliers (\texttt{AREA}($\mathrm{BM}_{\tilde{w}}$)), $\forall i$ and $\forall \tilde{w} \in [w_i-e, w_i+e]$.
This step uses Synopsys Design Compiler and the EGT PDK~\cite{Bleier:ISCA:2020:printedmicro} for circuit synthesis and area analysis.~\label{item:synthbm}
\item For all the coefficients $w_i$, create a set $R_i=\{\tilde{w}_i^-,\tilde{w}_i^+\}$ s.t. $\tilde{w}_i^- \in [w, w+e]$ and  $\tilde{w}_i^-$ features the smallest area in that segment, i.e., \texttt{AREA}($\mathrm{BM}_{\tilde{w}_i^-}$) = min(\texttt{AREA}($\mathrm{BM}_{\tilde{w}_i}$)), $\forall \tilde{w} \in [w, w+e]$.
Similarly, we select $\tilde{w}_i^+ \in [w-e, w]$. By definition replacing $w$ with $\tilde{w}_i^-$ generates a negative error while replacing $w$ with $\tilde{w}_i^+$ generates a positive error.~\label{item:minarea}
\item We perform a brute-force search to select the configuration $\{\tilde{w}_i: \tilde{w}_i \in R_i, \forall i \}$ so that $\sum_{\forall i}{(w_i-\tilde{w}_i)}$ is minimized. In case of a tie, we select the one that minimizes $\sum_{\forall i}{\texttt{AREA}(\mathrm{BM}_{\tilde{w}_i})}$.
Note that given the small search space size, brute force approach is feasible.~\label{item:miner}
\end{enumerate}
Steps~\ref{item:synthbm}-\ref{item:miner} are executed for each weighted sum, i.e., neuron in MLPs and 1-vs-1 classifier in SVMs.
In addition, we set $e$=$4$ in our analysis since for $e>4$ the area gains quite saturate (see Fig.~\ref{fig:multareasav}).
% At the worst case, step~\ref{item:synthbm} required less than $6$s using $12$ threads (i.e., limit of available licenses).
In step~\ref{item:miner} we implement an exhaustive search to extract the final configuration.
Unlike conventional silicon VLSI, in printed electronics the examined ML models are rather small in size (in terms of number of parameters).
Hence, each weighted sum (neuron or classifier) features only a limited number of coefficients, i.e., the size of the design space is well constrained.
It is noteworthy, that in the worst case, step~\ref{item:miner} required only 3s using $80$ threads.
The aforementioned execution times refer to a dual-CPU Intel Xeon Gold 6138 server.

In our optimization (steps~\ref{item:synthbm}-\ref{item:miner}) the sum $\sum_{\forall i}{\texttt{AREA}(\mathrm{BM}_{\tilde{w}_i})}$ is used as a proxy of the area of the weighted sum.
In other words, by minimizing the area (through our coefficient approximation) of the required bespoke multipliers, we aim in minimizing the area of the weighted sum.
We evaluate our area proxy against $1000$ randomly generated weighted sum circuits (i.e., random coefficients and input sizes).
The Pearson correlation coefficient between the area of the weighted sum obtained by Design Compiler and the area estimation using $\sum_{\forall i}{\texttt{AREA}(\mathrm{BM}_{\tilde{w}_i})}$ is $0.91$, i.e., perfect linear correlation.
Hence, our proxy precisely captures the area trend and minimizing $\sum_{\forall i}{\texttt{AREA}(\mathrm{BM}_{\tilde{w}_i})}$ in our optimization, will result in a weighted sum circuit with minimal area.
Finally, since our technique replaces the coefficient values with approximate more hardware-friendly ones, it does not require any specific/custom hardware implementation (e.g., as usually done in logic approximation).
Hence, it can be seamlessly integrated in any design framework and exploits all the optimization and IPs (e.g., multipliers) of synthesis tools.




\subsection{Netlist Pruning}\label{subsec:prune}


\begin{figure}[t]
\centering
\includegraphics[]{graphs/netlist.pdf}
\caption{Overview of our netlist pruning approach
}
\label{fig:netprun}
\end{figure}


To further increase the area efficiency, in addition to our coefficient approximation, we apply netlist pruning.
Netlist pruning is based on the observation that the output of several gates in a netlist remains constant (`0' or `1') for the majority of the execution time.
Hence, removing such a gate from the netlist and replacing its output with a constant value, results in low error rate.
Netlist pruning has been widely studied to enable design-agnostic approximation~\cite{GatePrun2017,Scarabottolo:DAC:2019:prune}. 
In this section we provide a brief description of how we implemented and tailored netlist pruning for bespoke printed ML architectures.

First we define two pruning parameters for a gate: $\tau$ is the maximum percentage of time that the gate's output is `0' or `1' and $\phi$ the most significant output bit (starting from 0) that the gate is connected to (through any path).
Using $\tau$ and $\phi$ \yellow{we constraint} the error frequency and the error magnitude, respectively. 
For example, assume that gate U1 is `1' the $\tau$=$90$\% of the time and that $\phi$=$3$.
Replacing U1 by `1' will result in an error rate of $10$\% and the maximum error will be less than $2^4$.
Netlist pruning is mainly implemented using heuristics~\cite{GatePrun2017} and thus optimality cannot be guaranteed.
In our work, leveraging that i) bespoke architectures feature significantly fewer area/gates than conventional architectures and ii) that ML models for printed electronics are rather limited in size, we use $\tau$ and $\phi$ to constraint the pruning design space and we implement an exhaustive search to obtain Pareto-optimal solutions.
Aiming for high area-efficiency, we prune all gates that feature $\tau$ and $\phi$ less or equal to given constraints $\tau_c$ and $\phi_c$.
Since all the pruned gates feature $\phi \leq \phi_c$, the maximum output error is less than $2^{\phi_c+1}$ irrespective of the number of pruned gates.
Overall, our coarse-grained approach ensures maximum area reduction while satisfying a maximum error threshold and enables fast design space exploration since only a gate's $\tau$ and $\phi$ need to be calculated.

Leveraging $\phi$ we filter all the gates that feature high $\tau$ and prune those that satisfy a given worst-case error.
In the case of regressors (MLP-R and SVM-R) this works well and many gates are pruned for low $\phi_c$ values.
However, classifiers require special consideration.
MLP-C and SVM-C use an argmax function at the end to translate the numerical predictions (e.g., values of output neurons in MLP-C) to a class.
As a result, the paths passing from all the gates are eventually congested in a few output bits, limiting the pruning granularity (possible $\phi_c$). 
Moreover, argmax breaks the correlation between the introduced numerical error in predictions and the final output.
For example, argmax([$0.9$, $0.1$])=argmax([$0.4$, $0.1$])=$0$.
%Hence, argmax breaks the efficiency of $\phi_c$ in controlling the gates that will be pruned.
For this reason, for the classifiers, we calculate $\phi$ for each gate with respect to the inputs of the argmax function.
For example, assume an MLP-C with $k$ output neurons $O_1$,..., $O_k$.
We define the value $\phi$ of a gate as $\max\limits_{\forall i}\phi(O_i)$, where $\phi(O_i)$ is the most significant output bit of the neuron $O_i$ that the gate is connected to.
If such a path doesn't exist, we set $\phi(O_i)=-1$.

Given a netlist (either exact or coefficient approximated), our netlist pruning operates as follows (also depicted in Fig.~\ref{fig:netprun}):
\begin{enumerate}[leftmargin=*]
\item Run RTL simulation of the synthesized netlist using the training dataset and Questasim to obtain the switching activity interchange format (SAIF) file.\label{item:sim}
\item Parse SAIF to calculate $\tau$ and the respective constant value (`0' or `1' ) for each gate.
For example, if the output of a gate is the $85$\% of the time `1' and the $15$\% it is `0', then $\tau$=$85$\% and the constant value is `1'. \label{item:tau}
\item Extract all the gates with $\tau \leq \tau_c$ and calculate their $\phi$. $\phi$ is easily calculated with the synthesis tool by reporting paths from a gate to the outputs.
\label{item:tauphi}
\item Prune all gates with $\tau \leq \tau_c$ and $\phi \leq \phi_c$, i.e., replace their output with the constant value extracted in step~\ref{item:tau}.\label{item:prune}
\item Synthesize the pruned netlist and evaluate its area and power as well as its accuracy on the test dataset.\label{item:synth}
\end{enumerate}
%If the synthesized netlist is not available, we use Design Compiler to synthesize the RTL description.
The pruned netlist is synthesized to exploit all optimizations of the synthesis tool, e.g., constant propagation.
Steps~\ref{item:sim} and~\ref{item:tau} are executed only once.
Step~\ref{item:tauphi} is executed $\forall \tau_c \in [80\%, 99\%]$.}
Then, for a given $\tau_c$, steps~\ref{item:prune}-\ref{item:synth} are executed $\forall \phi_c \in \Phi_\tau$.
$\Phi_\tau$ is equal to the set of the unique $\phi$ values obtained in step~\ref{item:tauphi}.
$\Phi_\tau$ enables us to explore only the relevant $\phi_c$ values.
%, accelerating our full search exploration.
\orange{
For example, if all the gates that feature $\tau\geq99$\% affect only the zero and first output bits, then $\phi_c>1$ is meaningless since it will return the same solution as $\phi_c=1$.
%At the worst case, using $10$ threads (i.e., limit of available licenses) our exhaustive design space exploration required only $28$ min.
Unlike the pruning state-of-the-art that examines only very simple circuits~\cite{GatePrun2017,Scarabottolo:DAC:2019:prune}, our implementation is evaluated on complex ML circuits.
Moreover, as explained above for the classifiers, conventional pruning~\cite{GatePrun2017,Scarabottolo:DAC:2019:prune} cannot be used.}



\begin{figure}[t!]
\centering
\includegraphics[]{graphs/cpd_se_mlp_c.pdf}
\includegraphics[]{graphs/cpd_se_mlp_r.pdf}
\includegraphics[]{graphs/cpd_se_svm_c.pdf}
\includegraphics[]{graphs/cpd_se_svm_r.pdf}
\caption{Normalized critical path delay delivered by coefficient \& pruning approximation against exact bespoke design when $80\%\leq\tau\leq100\%$ for the same $\phi$ and for Seeds dataset. MLP-C (a), MLP-R (b), SVM-C (c) and SVM-R (d) are considered.
}
\label{fig:cpd}
\end{figure}


\subsection{Voltage Over-Scaling}\label{sec:vos}

%Exploiting the potential for approximations in all three distinct layers (i.e., algorithmic, logic and circuit level), we also apply VOS to construct our final circuits.
%VOS can deliver even lower power consumption, since power dissipation of a circuit and supply voltage value depend quadratically, as shown in the relationship given by:
Finally, at the circuit level we apply VOS to maximize the power gains of our framework since power consumption of a circuit depends quadratically on the voltage supply value:
\begin{equation}
    P_{total} = P_{static} + a \times C \times f_s \times V_{dd}^2,
\label{eq:vdd}
\end{equation} 
where $a$ is the switching activity, $C$ is the circuit's capacitance, $f_s$ is the operating clock frequency and $V_{dd}$ the voltage value.
It is evident, thus, that power consumption significantly decreases when voltage supply ($V_{dd}$) is decreased.
%From Eq.~\ref{eq:vdd} it can be derived that the total power consumption decreases with the supply voltage $V{dd}$.

The main drawback of VOS is that circuits become slower and paths delay are increased. 
Therefore, when supply voltage is scaled below its nominal value~\cite{vader:zerv} for a given frequency, timing violations occur.
As the voltage continues to drop, an increasing number of internal paths cannot complete within the clock timing and the timing violation rate increases~\cite{vader:zerv}.
% However, in error-tolerant ML applications, such violations are not equalized with accuracy and consequently quality degradation.
% This mainly happens due to the fact that i) even if some gates suffer from a violation, the whole path can still produce a correct result, since some errors can be logically masked~\cite{masked:vos}, ii) not all neuron computations have the same latency, but VOS is performed based on the worst case, in which the corresponding neuron could be rarely activated and as previously mentioned, iii) the argmax function of classifiers breaks the correlation between the computation inaccuracy of predictions and the final outputs.

Applying coefficient approximation and netlist pruning not only reduces the circuit's area (and power) but also results in delay gain that can be therefore leveraged to apply aggressive VOS.   
%On the other hand, approximate techniques from coefficient replacement and netlist pruning produce simpler circuits with reduced delay and critical path, paving the way for voltage decreases, with fewer violated paths than in exact circuits, as well.
An example of the obtained delay gain is illustrated in Fig.~\ref{fig:cpd}.
%The trend of such delay reduction in four different models of one of our examined datasets is depicted in Fig.~\ref{fig:cpd}.
To generate Fig.~\ref{fig:cpd} we calculated for each model the delay when applying more aggressive approximation ($\tau<100\%$), normalized against the delay of their exact designs ($\tau=100\%$).
Note that $\phi$ was arbitrary selected equal to zero, since it does not affect circuit's logic, but only the error magnitude.
It is observed that, on average, $40\%$ lower delay can be achieved when $\tau=80\%$, while almost $9\%$ when $\tau$ is as high as $95\%$.
% , keeping the same $\phi$ value
Hence, assuming iso-frequency operation for the approximate and baseline circuits, the approximate circuits will feature fewer timing violations (if any) and the impact of VOS on the delivered accuracy is expected to be significantly diminished.
%Hence, with appropriate circuit design methodology, efficient voltage over-scaled approximate circuits that satisfy quality requirements, while also boosting VOS to its limit, can be generated. 
% Hence, decreasing voltage below its nominal value~\cite{zerv:axmult} can generate efficient approximate circuits  paves the way for printed circuits with higher performance and very low power consumption, since printed technology with appropriate circuit design methodology can 
% provide ultra-low voltage operation down to even $0.6V$~\cite{PrintVoltage:Tahoori}.
As a result, considering that EGT printed circuits can be operated even down to $0.6V$~\cite{PrintVoltage:Tahoori},
% at the cost of a small accuracy degradation, 
combining VOS with coefficient approximation and netlist pruning, enables ultra-low power operation without any performance loss due to the voltage decrease.
%In this section, we present our VOS methodology and how ``precise'' VOS simulations at gate level are performed in order to eventually identify the appropriate $V_{dd}$ for each approximate circuit.

In order to explore, in a timely manner\footnote{Significantly faster than SPICE simulations that would be infeasible to run as part of our optimization solution.}, the impact of VOS and evaluate the accuracy and power of voltage over-scaled approximate circuits, we perform VOS-aware gate-level post-synthesis timing simulations following the methodology presented in~\cite{vosim:zervakis}.
% Questasim simulates the gate netlist produced after synthesis at the previously relaxed clock, using as timing information an SDF file.
% This SDF file is produced for every examined voltage value using Synopsys command \emph{define\_scaling\_lib\_group}, which defines and groups some given libraries at different voltage values and interpolate between them.
%Moreover, printed technology can provide circuits with ultra-low voltage operation down to $0.6V$~\cite{PrintVoltage:Tahoori}.
\yellow{Realistic voltage values of $1.0V$ down to $0.6V$~\cite{PrintVoltage:Tahoori} with a $20mV$ step are considered for the VOS application.
Each circuit is set to one optimized voltage, which is found using the presented offline analysis, and so no run-time voltage regulators or controllers are needed. 
Note also that printed batteries can be fully customized in terms of voltage, shape, polarity, etc.,~\cite{PrintedBatteries2018}.
}\label{commentR1C3b}
Finally, since VOS-errors are timing errors and thus input dependent~\cite{vader:zerv,vosim:zervakis}, large input datasets are required to efficiently capture the accuracy behavior of circuits under VOS.
For this reason, we create large input stimuli by replicating and shuffling each test dataset as many times as required in order to obtain 1M randomly sequenced inputs.
\yellow{In our evaluations we use the aforementioned simulation-based tool-flow built upon industrial-strength EDA tools (i.e., Synopsys Design Compiler, Prime Time, Mentor Questasim), using printed PDK and standard cell libraries~\cite{Bleier:ISCA:2020:printedmicro} calibrated based on fabrication and measurements from low voltage printing technologies.}\label{commentR1C2}
% Finally, since computation errors derived from timing violations are also input dependent and require many and different input sequences, we perform
% VOS-aware simulations by shuffling and merging each dataset as many times as needed to obtain 1M input sequence samples.
% The aforementioned tool-flow is used in our whole VOS methodology.

% Since  timing  errors  are  essentially  silent  errors,  they  do not manifest themselves until the corrupted data has led to an unusual application behavior, which may be detected long after the error has occurred, wasting the entire computation done so far

% circuits  can  be pushed beyond their limits to compute incorrectly to achieve higher energy efficiency.


