\section{Existing Benchmarking Approaches}
Over the past decade, numerous vulnerability detection techniques and empirical studies have been proposed. Correspondingly, many vulnerability detection benchmarking approaches have been used to evaluate these techniques. In this section, we review the existing benchmarking approaches used to evaluate these techniques.
We first examine the benchmarking approaches used in the papers that propose vulnerability detection techniques. Next, we review the approaches used in empirical studies that compare and evaluate existing vulnerability detection techniques to understand their strengths and weaknesses.


%Over the past decade, many vulnerability detection techniques and relevant empirical studies are proposed. At the same time, many vulnerability detection benchmarking approaches are also used to evaluate these techniques. In this section, we review the existing benchmarking approaches that are used to evaluate the techniques. We first review the approaches used in the papers that propose vulnerability detection techniques. Then, we review the approaches used in empirical studies that compare and evaluate the existing vulnerability detection techniques to understand the strengths and weaknesses of them.
\subsection{Technique Proposing Papers}
It is common for technical papers to use benchmarking approaches to evaluate the techniques they introduce. However, during our literature study, we noticed that the benchmarking approaches used before and after the trend of DL-based vulnerability detection are significantly different.

Specifically, traditional vulnerability detection techniques (such as static analysis, dynamic analysis, and penetration analysis) before the emergence of DL-based techniques were typically evaluated through technical reviews and case studies. In contrast, DL-based techniques are often evaluated on large-scale vulnerability datasets due to the need for numerous data to train and test them. Therefore, we separately discuss the benchmarking approaches used in papers on traditional vulnerability detection techniques and those on DL-based techniques.

%It is natural that a technical paper uses some benchmarking approaches to evaluate the technique it introduces. However, during our literature study, we notice that the benchmarking approaches before and after the trend of DL-based vulnerability detection are significantly different. The traditional vulnerability detection techniques (e.g., static analysis, dynamic analysis, penetration analysis) before the trend of DL-based techniques are usually evaluated by technical reviews and case studies, while the DL-based techniques are usually evaluated on large-scale vulnerability datasets, because of the nature that DL-based techniques require enumerous data to train and test. Thus, we separately discuss the benchmarking approaches in the traditional vulnerability detection technique papers and the DL-based technique papers.

\subsubsection{Traditional Vulnerability Detection Techniques}
Since traditional vulnerability detection techniques are mostly deterministic, their technical strengths and weaknesses can be relatively easy to analyze by humans. Thus, some traditional vulnerability detection techniques evaluate themselves by reviewing their technical strengths and weaknesses only:

%As traditional vulnerability detection techniques are mostly deterministic, the technical strengths and weaknesses can be relatively easy to analyze by human.  Thus, some traditional vulnerability detection techniques evaluate themselves by only reviewing the technical strengths and weaknesses of them. 

UNO~\cite{holzmann2002static} is a static analyzer that detects software vulnerabilities by checking user-defined properties. It evaluates itself by reviewing its features and side effects, such as in which scenarios UNO can detect vulnerabilities. CBMC~\cite{kroening2014cbmc} is a static bounded model checker for vulnerability detection, which evaluates itself by discussing its strengths and weaknesses. Frama-C~\cite{kirchner2015frama, cuoq2012frama} is a C program verification tool for vulnerability detection, which evaluates itself by discussing its technical features. ITS4~\cite{viega2000its4} is a vulnerability scanner for C and C++ code, which also evaluates itself by reviewing its features, strengths, and weaknesses, while comparing it with other techniques and describing practical experiences of using it. Zhu's model~\cite{zhu2022model} is evaluated based on which vulnerability-relevant properties it can check. ESC/Java~\cite{flanagan2002extended} is an extended vulnerability checker for Java, which evaluates itself by discussing the user experience in different scenarios. These techniques' benchmarking approaches are based on feature review and user experience, without applying them to vulnerability detection test cases, which makes it difficult to compare them with each other.

%UNO~\cite{holzmann2002static} is a static analyzer that detect software vulnerabilities by checking user-defined properties. It evaluates itself by reviewing the features and the side effects of the technique (i.e., in what scenarios UNO can detect vulnerabilities). CBMC~\cite{kroening2014cbmc} is a static bounded model checker for vulnerability detection. It simply evaluates itself by discussing the strengths and weaknesses. Frama-C~\cite{kirchner2015frama, cuoq2012frama} is a C program verification tool for vulnerability detection. It evaluates itself by discussing the technical features of it. ITS4~\cite{viega2000its4} is a vulnerability scanner for C and C++ code. It also evaluates itself by reviewing the features, strengths and weaknesses, while comparing it with other techniques and describing the practical experience of using it. Zhu's model~\cite{zhu2022model} is evaluated on which vulnerability relevant properties can be checked. ESC/Java~\cite{flanagan2002extended} is an extended vulnerability checker for Java. It also evaluates itself by discussing the user experience in different scenarios.  These techniques' benchmarking approaches are based on feature review and user experience, without applying them to vulnerability detection test cases, making them hard to compare with each other.  

In comparison, some other techniques evaluate themselves by conducting case studies to better convince users. Undangle~\cite{caballero2012undangle} uses case studies on Firefox to demonstrate its ability to detect dangling pointers early. Valgrind MemCheck~\cite{nethercote2003valgrind, nethercote2007valgrind, seward2005using} applies itself to Open Office~\cite{ven2006introduction} to showcase its capability in finding real-world vulnerabilities. FindBugs~\cite{hovemeyer2004finding} collects user experiences from applying it to a geographic information system and a financial application to demonstrate its effectiveness. FlowDist~\cite{fu2021flowdist} evaluates recall and precision through manual evaluation on selected cases. Astree~\cite{blanchet2003static} applies itself to a project with 132,000 lines of code to demonstrate its effectiveness and efficiency in vulnerability detection. CodeSonar~\cite{jetley2008static} is evaluated on medical device software to discuss its strengths and weaknesses. AddressSanitizer~\cite{serebryany2012addresssanitizer} is applied to Google Chrome to demonstrate its ability to find real-world vulnerabilities. While these case studies apply the techniques to real-world vulnerability detection scenarios, the cases used are not large-scale, and different techniques use different cases, making it difficult to benchmark and compare techniques comprehensively and fairly.

%In comparison, to better convince the user, some other techniques evaluate themselves by doing case studies. Undangle~\cite{caballero2012undangle} evaluates itself by doing case studies on Firefox to demostrate its capability on detecting dangling pointers early. Valgrind MemCheck~\cite{nethercote2003valgrind, nethercote2007valgrind, seward2005using} applies itself to Open Office~\cite{ven2006introduction} to demostrate its capability on finding real-world vulnerabilities. FindBugs~\cite{hovemeyer2004finding} collects the user experience of applying it to a geographic information system and a financial application to show the effectiveness. FlowDist~\cite{fu2021flowdist} is evaluated recall and precision based on manual evaluation on some cases. Astree~\cite{blanchet2003static} applies it to a project with 132,000 lines of code to show the vulnerability detection effectiveness and efficiency. CodeSonar~\cite{jetley2008static} is evaluated on medical device software to discuss the strengths and weaknesses of it. AddressSanitizer~\cite{serebryany2012addresssanitizer} is also applied on Google Chrome to show that it can find real-world vulnerabilities. Such case studies apply the techniques to real-world vulnerability detection scenarios, but since the case studies are not large-scale and the cases used by different techniques are different, it is still difficult to benchmarking and comparing different techniques comprehensively and fairly.

Other papers have used large-scale standard benchmarking approaches to evaluate the effectiveness of vulnerability detection techniques. For instance, Facebook Infer~\cite{harmim2019scalable} used the benchmarking dataset introduced in CPROVER~\cite{kroening2016sound} to evaluate the performance of detecting deadlocks. MemSafe~\cite{simpson2013memsafe} applied itself to BugBench suite~\cite{lu2005bugbench} to evaluate its ability to detect memory safety errors. KLEE~\cite{cadar2008klee} was evaluated on multiple real-world test suites to test its error coverage. PCA~\cite{li2020pca}, Valgrind~\cite{nethercote2003valgrind}, DrMemory~\cite{bruening2011practical}, MemorySanitizer~\cite{stepanov2015memorysanitizer}, and AddressSanitizer~\cite{serebryany2012addresssanitizer} were evaluated on SPEC CPU2000~\cite{henning2000spec} and SPEC CPU2006~\cite{henning2006spec} to test the running time and memory usage of the vulnerability detection on real-world projects. However, these papers were published before the trend of DL-based vulnerability detection. Additionally, some benchmarking datasets, such as SPEC CPU2006, were not originally designed for vulnerability detection benchmarking but for other proposes (e.g., testing running time and memory usage). Thus, they can only be used to evaluate the efficiency of the techniques or check the number of vulnerabilities that can be found. Because the benchmarking datasets lack vulnerability ground truths, important evaluation metrics such as recall, precision, and F1 score cannot be evaluated.

%Other papers using large-scale standard benchmarking approaches to evaluate the techniques exist. Facebook Infer~\cite{harmim2019scalable} uses the benchmarking dataset introduced in CPROVER~\cite{kroening2016sound} to evaluate the performance on deadlock. MemSafe~\cite{simpson2013memsafe} applies itself to BugBench suite~\cite{lu2005bugbench} to evaluate the performance on detecting memory safety errors. KLEE~\cite{cadar2008klee} is evaluated on multiple real-world test suites to test error coverage. PCA~\cite{li2020pca}, Valgrind~\cite{nethercote2003valgrind}, DrMemory~\cite{bruening2011practical}, MemorySanitizer~\cite{stepanov2015memorysanitizer}, and AddressSanitizer~\cite{serebryany2012addresssanitizer} are evaluated on SPEC CPU2000~\cite{henning2000spec} and SPEC CPU2006~\cite{henning2006spec} to test the running time and memory usage of the vulnerability detection on real-world projects. However, these papers are usually published before the trend of DL-based vulnerability detection. Some of the benchmarking datasets such as SPEC CPU2006 are not originally designed for benchmarking vulnerability detection but for other purposes (e.g., testing running time and memory usage). Thus, they can be only used to test the efficiency of the techniques or check the number of vulnerabilities that can be found, but since the benchmarking datasets have no vulnerability ground truths, some important evaluation metrics such as recall, precision, and F1 score cannot be evaluated.

A few papers have used vulnerability benchmarking datasets with ground truths to evaluate their techniques comprehensively. PolyCruise~\cite{li2022polycruise} is a dynamic flow analysis technique for multilingual software vulnerability detection. To enable a comprehensive evaluation, the authors of PolyCruise manually built a benchmarking dataset called PyCBench~\cite{li2022polycruise}, which contains ground truths to evaluate the technique. BOFSanitizer~\cite{wang2021bofsanitizer} is evaluated for detecting buffer overflow vulnerabilities using the Juliet Test Suite~\cite{black2018juliet}. VulSlicer~\cite{salimi2022vulslicer}, MVP~\cite{xiao2020mvp}, and LEOPARD~\cite{du2019leopard} are evaluated on the real-world vulnerability database CVE/NVD~\cite{booth2013national} for vulnerability detection. These evaluations against vulnerability benchmarking datasets enable crucial metrics such as recall, precision, and F1, which are essential for benchmarking the techniques fairly and comprehensively.

%A few papers use vulnerability benchmarking datasets with ground truths to evaluate their techniques. PolyCruise~\cite{li2022polycruise} is a technique that dynamic flow analysis technique for multilingual software vulnerability detection. To enable the comprehensive evaluation, the authors of PolyCruise manually build a benchmarking dataset PyCBench~\cite{li2022polycruise} which contain ground truths to evaluate the technique. BOFSanitizer~\cite{wang2021bofsanitizer} is evaluated on Juliet Test Suite~\cite{black2018juliet} for detecting buffer overflow vulnerabilities.  VulSlicer~\cite{salimi2022vulslicer}, MVP~\cite{xiao2020mvp}, and LEOPARD~\cite{du2019leopard} are evaluated on real-world vulnerability database CVE/NVD~\cite{booth2013national} for vulnerability detection. These evaluations against vulnerability benchmarking datasets enable important metrics such as recall, precision, and F1, which are crucial for benchmarking the techniques fairly and comprehensively. 

\subsubsection{DL-based Vulnerability Detection Techniques}
In recent years, there has been an increasing use of deep learning (DL) for software vulnerability detection. An empirical study on DL-based vulnerability detection techniques reveals that more than 55 techniques were proposed between 2016 and 2020~\cite{nong2022open}. Because DL-based models require a large amount of data to train and test, DL-based software vulnerability detection techniques use various benchmarking approaches to evaluate them. These benchmarking approaches cover different dimensions: training/testing data, granularity, and real-world case studies. We will discuss each of these dimensions separately below.

%In recent years, there are increasing use of deep learning (DL) in software vulnerability detection. According to an empirical study on DL-based vulnerability detection techniques, there are more than 55 techniques proposed between 2016 to 2020~\cite{nong2022open}. Since DL-based models need a large-amount of data to train and test, the DL-based software vulnerability detection techniques use very different benchmarking approaches to evaluate them. The benchmarking approaches also cover different dimensions: \textbf{Training/testing data, granularity, real-world case studies}. We will separately discuss these dimensions below:


\textbf{Training/Testing Data: }
In 2018, Li et al.~\cite{li2018vuldeepecker} proposed VulDeePecker, which was the first DL-based vulnerability detector that achieved great success. The authors used 10,691 vulnerability samples to train and test the model. These samples had ground truths that indicated whether the given sample was vulnerable or non-vulnerable. This enabled the evaluation metrics such as accuracy, recall, precision, and F1 score. Among them, 9,851 samples were from SARD~\cite{black2017sard}, which were synthetic samples (i.e., artificially generated without considering the realism of the samples), and 840 samples were from CVE/NVD~\cite{nvd}, which were from real-world software projects. Subsequently, many papers used the same benchmarking approach and evaluation dataset (although the number of samples may differ slightly, they were from SARD and CVE/NVD) to evaluate their proposed new techniques~\cite{li2021sysevr, zou2019mu, zagane2020deep, wu2022vulcnn, tang2022sevuldet, thapa2022transformer,cao2022mvd,pinconschitenet}.
%In 2018, Li et al.~\cite{li2018vuldeepecker} proposed VulDeePecker, which is the first DL-based vulnerability detector that achieves great success. In this paper, 10691 vulnerability samples are used to train and test the model. These samples have ground truths that tell whether the given sample is vulnerable or non-vulnerable. This enables the metrics such as accuracy, recall, precision, and F1. Among them, 9851 samples are from SARD~\cite{black2017sard}, which are synthetic samples (i.e., artificially generated without considering the realism of the samples), and 840 samples from CVE/NVD~\cite{nvd}, which are from real-world software projects. After that, many of the papers use the same benchmarking approach and the same evaluation dataset (although the number of samples may be a little bit different, they are from SARD and CVE/NVD) to evaluate their proposed new techniques~\cite{li2021sysevr, zou2019mu, zagane2020deep, wu2022vulcnn, tang2022sevuldet, thapa2022transformer,cao2022mvd,pinconschitenet}.

However, most vulnerability samples used for DL-based techniques are synthetic, making them too simple and easy for DL models to learn, leading to inflated results~\cite{chakraborty2021deep}. Therefore, using real-world vulnerability samples is more promising. Some techniques, like Draper~\cite{russell2018automated}, collected 1.27 million samples and used static analyzers to label vulnerable code to build their benchmarking dataset. However, the static analyzers' low accuracy, precision, recall, and F1 on real-world samples~\cite{nong2020preliminary, nong2021evaluating} make training and testing datasets very noisy. To address this, Devign~\cite{zhou2019devign} manually collected 22,361 vulnerability samples from 2 real-world projects, taking 600 man-hours. The benchmarking dataset has relatively balanced vulnerable and non-vulnerable samples, but in real-world projects, vulnerable code is the minority, making a balanced dataset unrealistic~\cite{nong2022open}.

%However, since most of the vulnerability samples are synthetic, these samples are mostly very simple and the vulnerability patterns are very easy for the DL models to learn. Thus, the results reported are usually inflated~\cite{chakraborty2021deep}. Therefore, using real-world vulnerability samples are much more promising. Some other DL-based vulnerability detection techniques use samples from real-world projects. Draper~\cite{russell2018automated} collects 1.27 million samples and uses static analyzers to label whether they are vulnerable or not, to build their benchmarking dataset. However, while the number of samples is large, the static analyzers have low accuracy, precision, recall, and F1 on the real-world samples~\cite{nong2020preliminary, nong2021evaluating}, making the training and testing datasets very noisy.  To solve this issue, Devign~\cite{zhou2019devign} manually collects 22361 vulnerability samples using 600 man-hours. The benchmarking dataset covers 2 different real-world projects and the number of vulnerable samples and non-vulnerable samples are relatively balanced. However, in the real-world projects, vulnerable code is minority. The balanced dataset may not reveal the realistic vulnerability detection scenarios~\cite{nong2022open}. Thus, other DL-based vulnerability detection techniques use real-world samples without balancing to build the benchmarking datasets for evaluation. 

Several DL-based vulnerability detection techniques use real-world samples without balancing to build benchmarking datasets for evaluation. For instance, TRL~\cite{lin2018cross} and RLMD~\cite{lin2019software} use 39,942 vulnerability samples from six real-world projects as their benchmarking datasets, where only 1.46\% of the samples are vulnerable. ReVeal~\cite{chakraborty2021deep} manually collects 18,169 vulnerability samples, of which only 9.16\% are vulnerable. On the other hand, IVDetect~\cite{li2021vulnerability}, LineVD~\cite{hin2022linevd}, muVulPreter~\cite{zou2022mvulpreter}, LineVul~\cite{fu2022linevul}, and Cheng~\cite{cheng2022path} use Big-Vul~\cite{fan2020ac}, which contains more than 168,000 non-vulnerable samples and 10,000 vulnerable samples. These benchmarking datasets can better represent real-world vulnerability detection scenarios.

%TRL~\cite{lin2018cross} and RLMD~\cite{lin2019software} use 39,942 vulnerability samples from six real-world projects as their benchmarking datasets, where the proportion of vulnerable samples is only 1.46\%. ReVeal~\cite{chakraborty2021deep} manually collects 18169 vulnerability samples and only 9.16\% of them are vulnerable. 
%IVDetect~\cite{li2021vulnerability}, LineVD~\cite{hin2022linevd},  muVulPreter~\cite{zou2022mvulpreter},  LineVul~\cite{fu2022linevul}, Cheng~\cite{cheng2022path} use Big-Vul~\cite{fan2020ac} which have more than 168 thousand non-vulnerable samples and 10 thousand vulnerable samples. 
%Such benchmarking datasets can better represent the real-world vulnerability detection scenarios. 

\textbf{Granularity:} 
In addition to the datasets used for training and testing DL models, granularity in vulnerability detection is also crucial. In real-world vulnerability detection scenarios, developers not only need to know if a file or function is vulnerable, but they also need to identify the trigger point of the vulnerability (e.g., one or more statements that can be exploited) and the type of vulnerabilities. Traditional vulnerability detection techniques can easily provide detailed information because they analyze the code by examining whether statements contain potential trigger points for attacks. However, DL-based vulnerability detection models are black boxes, and we can only train them based on the task formulation and the training data.

%Besides the datasets used for training and testing the DL models, vulnerability detection granularity is also important. Since in real-world vulnerability detection scenarios, the developers not only need to know whether a file or a function is vulnerable or not, they also need to know the trigger point of the vulnerability (e.g., one or several statements that can be attacked) and the type of vulnerabilities. In traditional vulnerability detection techniques, they can easily provide detailed information because that is the way they analyze the code (i.e., analyze whether statements have the potential trigger points for attacking). However, in the DL-based vulnerability detection, the models are black boxes. We can only train them based on the task formulation and the training data. 

Based on our literature study, most of the DL-based vulnerability detection techniques use function-level granularity (i.e., the models decide whether a given function is vulnerable or not), without considering the vulnerability types~\cite{zagane2020deep, wu2022vulcnn, tang2022sevuldet, thapa2022transformer,cao2022mvd, chakraborty2021deep,zhou2019devign,lin2018cross,lin2019software}. VulDeePecker~\cite{li2018vuldeepecker}, SySeVR~\cite{li2021sysevr}, uVulDeePecker~\cite{zou2019mu}, and VulDeeLocator~\cite{li2020vuldeelocator} consider code slice-level vulnerability detection, which is more fine-grained and easier for developers to understand and fix the vulnerabilities. uVulDeePecker~\cite{zou2019mu} not only achieves code slice-level detection, but also is able to classify the vulnerability types. IVDetect~\cite{li2021vulnerability}, LineVD~\cite{hin2022linevd}, mVulPreter~\cite{zou2022mvulpreter}, and LineVul~\cite{fu2022linevul} even consider statement-level vulnerability detection~\cite{li2021sysevr}. Based on the chronological order of these papers, detecting vulnerabilities in fine-grain granularities (e.g., statement-level) and providing the vulnerability types should be the future trend, because that is the need of real-world developers.

\textbf{Real-world Case Studies:} Ideally, the DL-based vulnerability detection techniques should not only show the high accuracy on the used benchmarking datasets, but also should be usable for real-world vulnerability detection. Thus, to show the usability of the techniques, some of the papers apply their techniques to real-world software projects and successfully find unknown vulnerabilities. Devign~\cite{zhou2019devign} selects the latest 112 vulnerability samples to check whether it has the capability to detect zero-day vulnerabilities. VulDeePecker~\cite{li2018vuldeepecker}, SySeVR~\cite{li2021sysevr}, VulCNN~\cite{wu2022vulcnn}, VulDeeLocator~\cite{li2020vuldeelocator}, and VulHunter~\cite{guo2019vulhunter} apply their techniques to real-world software projects and successfully find unknown vulnerabilities. The case studies on real-world scenarios significantly show the practicability of the techniques and should be an important benchmarking approach for vulnerability detection techniques. 


