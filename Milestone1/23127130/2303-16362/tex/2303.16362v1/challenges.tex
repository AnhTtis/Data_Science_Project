\section{Challenges and Solutions}
Based on the survey above, we notice that there are several challenges for benchmarking vulnerability detection:

\begin{itemize}
    \item First, while many of the papers propose vulnerability detection techniques or build their evaluation datasets, their source code and datasets are not fully released to other researchers, making the benchmarking from other researchers difficult.
    \item Second, there are many different kinds of evaluation metrics. Some evaluations use the numbers of vulnerabilities the techniques can find to evaluate. Some other evaluations use samples with ground truths to compute recall, precision, and F1 score. Besides, there are many different factors that can impact the evaluation, such as the realism of the evaluation datasets, the data balance of the datasets, the granularity of the samples (e.g., line, function, file or project), training dataset size for deep learning models, and the complexity of the samples (e.g., code structure and control flow complexity). These should be also comprehensively considered for the benchmarking.
    \item Third, there is a lack of large-scale and real-world vulnerability datasets for benchmarking vulnerability detection. Many of them use small (only several hundred samples) datasets to evaluate the vulnerability detection techniques. This may be enough for traditional vulnerability detection techniques, but not enough for deep learning-based vulnerability detection because they need many samples (usually >10,000) to train the models well.


\end{itemize}

To solve the three challenges above, some solutions have been tried for solving the challenges. Nong et al.~\cite{nong2022open} evaluate the availability, excitability, reproducibility, and replicability of existing vulnerability detection techniques and provide actionable suggestions to solve the first challenge. Chakraborty et al.~\cite{chakraborty2021deep} build their own dataset and evaluate the impact of data realism and data balance on the effectiveness, which help solve the second challenge. Croft et al.~\cite{croft2023data} and liu et al.~\cite{liu2022investigating} evaluate the data quality of the vulnerability datasets, which help solve the third challenge. Indeed, there are many independent works trying to build high-quality vulnerability datasets. Some of them collect the respective source code from the publicly available CVE/NVD databases~\cite{bhandari2021cvefixes, fan2020ac, reis2021ground}. Some of them propose techniques that automatically collect vulnerability data in the wild~\cite{xu2021tracer, sawadogo2021early, sawadogo2022sspcatcher, nguyen2022hermes, nguyen2022vulcurator, guo2022hyvuldect, zheng2021d2a, woo2021v0finder,li2022polyfax}. 

However, these datasets are still not large-scale for training effective deep learning-based vulnerability detection techniques. Recently, there are new approaches proposed which automatically generate high-quality and large-scale vulnerability dataset. Nong et al.~\cite{nong2022generating} try to use existing deep learning-based code repair/edit tools to inject vulnerability into existing real-world normal programs and show the feasibility of doing so. He et al.~\cite{he2023controlling} develop a tool which generates both secure and vulnerable code by controlling large language models.
Nong et al. did similar work~\cite{nong2022vulgen} which combines the advantages of traditional pattern application and deep learning-based vulnerability injection localization to generate more high-quality vulnerability datasets. This indicates that generating vulnerability data may be a good way to build high-quality and large-scale datasets.

