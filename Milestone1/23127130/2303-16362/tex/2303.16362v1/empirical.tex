\subsection{Empirical Study Papers}
Besides the papers that introduce vulnerability detection techniques, there are also many empirical studies that aim to fairly and comprehensively evaluate the existing vulnerability detection techniques, so that they can find the weaknesses and strengths of the techniques and provide practical insights and suggestions for future technique development. To do that, these empirical studies also need to find effective benchmarking approaches to evaluate the techniques. Since the empirical studies need to cover many different vulnerability detection techniques, the benchmarking approaches need to be general, fair, and standalone, which have higher standards than the benchmarking approaches in the technique proposing papers. 

In this section, we review the existing empirical studies on vulnerability detection techniques, and summarize the better benchmarking approaches for traditional and DL-based vulnerability detection techniques.

\subsubsection{Traditional Vulnerability Detection Techniques}
Many empirical studies for benchmarking vulnerability detection techniques exist and they use different approaches to evaluate and compare them. Some studies compare and evaluate different static code analysis approaches against SQL injection and XSS attack vulnerabilities in web services`\cite{antunes2009comparing,5552783,fonseca2007testing}. Some other studies evaluate buffer overflow vulnerability detectors. However, they only target specific vulnerability types and domains which limit the comprehensiveness of the studies. Also, these studies benchmark the evaluated techniques using a relatively small number of samples, thus many cases are missed by the techniques that are not actually evaluated.

Some other studies use a relatively large number of samples to benchmark vulnerability detection techniques~\cite{kang2022detecting}. Austin et al.~\cite{austin2011one, austin2013comparison} uses three electronic health record systems to benchmark automated penetration testing and static analysis techniques. In some other studies~\cite{pozza2006comparing, antunes2009comparing}, the authors evaluate the vulnerability detection techniques by counting the number of vulnerabilities found in synthetic or real-world software projects. However, these evaluations do not refer to any ground truth. Thus, they cannot compute precision, recall, and F1 score which are important metrics in their studies. 

From the cases above, we can see that the benchmarking metrics are important for the empirical studies on vulnerability detection techniques. Thus, in ~\cite{5552783}, the authors discuss and provide the metrics for benchmarking vulnerability detection techniques, but the actual empirical experiments on the state-of-the-art techniques are not performed. Another technical report~\cite{shirey2000internet} simply discusses the capabilities of the evaluated vulnerability detection techniques without any analytical experiments, thus no empirical comparisons are conducted, making the actual detection performance not able to be assessed. 

A few empirical studies~\cite{antunes2010benchmarking, fonseca2007testing} evaluate and compare vulnerability detection techniques using samples with ground truths, thus they can compute precision, recall, and F1. However, these studies only evaluate commercial tools, thus the reasons behind the successes and failures are difficult to assess.

In comparison, Yu et al. conduct empirical studies on five open-source vulnerability detection techniques against datasets with ground truths~\cite{nong2020preliminary, nong2021evaluating}. The datasets cover 20 categories of memory related vulnerabilities. Thus, the studies benchmark general vulnerability detection techniques and the precision, recall, F1 score, as well as the efficiency are comprehensively evaluated. They also do several in-depth case studies to analyze the reasons behind the successes and failures. However, in the datasets they used to evaluate, most of the samples are synthetic, making the evaluation not very referable to real-world vulnerability detection scenarios.

\subsubsection{DL-based Vulnerability Detection Techniques}
Because of the trend of DL-based vulnerability detection techniques, many empirical studies are also proposed on these DL-based vulnerability detection techniques. Li et al.~\cite{li2019comparative} did a comparative study of deep learning-based vulnerability detection tools. They quantitatively evaluate different factors' impact on the vulnerability detection performance. However, they only simply use several different deep learning models without vulnerability detection specification to do the evaluation.
Fernandez et al.~\cite{fernandez2019case} did a case study for network intruction detection using deep learning models. Jabeen et al.~\cite{jabeen2022machine} did a comparative study on machine learning techniques for software prediction. However, the above techniques only evaluate the DL models without using large-scale real-world vulnerability datasets, making their evaluation not comprehensive. Zhang~\cite{zhang2023survey} did a survey on the learning-based vulnerability repair techniques.

In comparison, Mazuera-Rozo et al.~\cite{mazuera2021shallow} uses different source code representations to compare the effectiveness of deep learning models. They use both real-world, highly imbalanced dataset and a balanced dataset for comparison. Steenhoek et al.~\cite{steenhoek2022empirical} did an empirical study of deep learning models for vulnerability detection. They investigate six research questions on nine state-of-the-art deep learning models against two widely used vulnerability detection datasets. Siow~\cite{siow2022learning} did an empirical study on program semantics learning with different code representations. They evaluate different mainstream code representations including feature-based, sequence-based, tree-based, and graph-based. 

However, most of the above empirical studies only evaluate several different deep learning models or source code representation for deep learning models. It is difficult for them to find available and replicable tools to do their comparative studies. Thus, Nong et al.~\cite{nong2022open} conduct a comprehensive empirical study of the open science status on deep learning-based vulnerability detection. They evaluate the availability, excitability, reproducibility, and replicability of 55 existing vulnerability detection techniques. Their in-depth study investigates the source code links in the papers, tool documentation and completeness, tool reproduction which uses the original datasets, and tool replication which uses third-party real-world datasets. 
