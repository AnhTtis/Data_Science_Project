@article{batsheva2023protes,
  title={{PROTES}: probabilistic optimization with tensor sampling},
  author={Batsheva, Anastasia and Chertkov, Andrei and Ryzhakov, Gleb and Oseledets, Ivan},
  journal={arXiv preprint arXiv:2301.12162},
  year={2023}
}
@article{chertkov2022optimization,
  title         = {Optimization of functions given in the tensor train format},
  author        = {Chertkov, Andrei and Ryzhakov, Gleb and Novikov, Georgii and Oseledets, Ivan},
  journal       = {arXiv preprint arXiv:2209.14808 (submitted to IEEE Computing in Science and Engineering)},
  year          = {2022}
}
@article{cichocki2016tensor,
  title={Tensor networks for dimensionality reduction and large-scale optimization: {Part} 1 low-rank tensor decompositions},
  author={Cichocki, Andrzej and Lee, Namgil and Oseledets, Ivan and Phan, Anh-Huy and Zhao, Qibin and Mandic, Danilo},
  journal={Foundations and Trends in Machine Learning},
  volume={9},
  number={4-5},
  pages={249--429},
  year={2016},
  publisher={Now Publishers Inc. Hanover, MA, USA}
}
@article{cichocki2017tensor,
  title = {Tensor Networks for Dimensionality Reduction and Large-scale Optimization: {Part} 2 Applications and Future Perspectives},
  year = {2017},
  volume = {9},
  journal = {Foundations and Trends in Machine Learning},
  number = {6},
  pages = {431-673},
  author = {Andrzej Cichocki and Anh Phan and Qibin Zhao and Namgil Lee and Ivan Oseledets and Masashi Sugiyama and Danilo Mandic}
}
@article{ebrahimi2018adversarial,
  title={On adversarial examples for character-level neural machine translation},
  author={Ebrahimi, Javid and Lowd, Daniel and Dou, Dejing},
  journal={arXiv preprint arXiv:1806.09030},
  year={2018},
   ilink={https://arxiv.org/pdf/1806.09030.pdf}
}
@article{kosinski2023theory,
  title={Theory of mind may have spontaneously emerged in large language models},
  author={Kosinski, Michal},
  journal={arXiv preprint arXiv:2302.02083},
  year={2023},
  ilink={https://arxiv.org/pdf/2302.02083.pdf}
}
@article{oseledets2011tensor,
  title={Tensor-train decomposition},
  author={Oseledets, Ivan},
  journal={SIAM Journal on Scientific Computing},
  volume={33},
  number={5},
  pages={2295--2317},
  year={2011},
  publisher={SIAM}
}
@article{sadrizadeh2023transfool,
  title={{TransFool}: an adversarial attack against neural machine translation models},
  author={Sadrizadeh, Sahar and Dolamic, Ljiljana and Frossard, Pascal},
  journal={arXiv preprint arXiv:2302.00944},
  year={2023},
  ilink={https://arxiv.org/pdf/2302.00944.pdf},
}
@article{selvanayagam2022global,
  title={Global optimization of surface warpage for inverse design of ultra-thin electronic packages using tensor train decomposition},
  author={Selvanayagam, Cheryl and Duong, Pham Luu Trung and Wilkerson, Brett and Raghavan, Nagarajan},
  journal={IEEE Access},
  volume={10},
  pages={48589--48602},
  year={2022},
  publisher={IEEE}
}
@article{shetty2022tensor,
  title={Tensor train for global optimization problems in robotics},
  author={Shetty, Suhan and Lembono, Teguh and Loew, Tobias and Calinon, Sylvain},
  journal={arXiv preprint arXiv:2206.05077},
  year={2022}
}
@inproceedings{sozykin2022ttopt,
  title         = {{TTOpt}: a maximum volume quantized tensor train-based optimization and its application to reinforcement learning},
  author        = {Sozykin, Konstantin and Chertkov, Andrei and Schutski, Roman and Phan, Anh-Huy and Cichocki, Andrzej and Oseledets, Ivan},
  booktitle     = {Advances in Neural Information Processing Systems},
  year          = {2022}
}
@inproceedings{zhang2021crafting,
  title={Crafting adversarial examples for neural machine translation},
  author={Zhang, Xinze and Zhang, Junzhe and Chen, Zhenhua and He, Kun},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={1967--1977},
  year={2021},
  ilink={https://github.com/JHL-HUST/AdvNMT-WSLS},
}

@inproceedings{DBLP:journals/corr/SzegedyZSBEGF13,
  author    = {Christian Szegedy and
               Wojciech Zaremba and
               Ilya Sutskever and
               Joan Bruna and
               Dumitru Erhan and
               Ian J. Goodfellow and
               Rob Fergus},
  title     = {Intriguing properties of neural networks},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year      = {2014},
}

@inproceedings{DBLP:journals/corr/GoodfellowSS14,
  author    = {Ian J. Goodfellow and
               Jonathon Shlens and
               Christian Szegedy},
  title     = {Explaining and Harnessing Adversarial Examples},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
}

@article{guerreiro2022looking,
  title={Looking for a needle in a haystack: a comprehensive study of hallucinations in neural machine translation},
  author={Guerreiro, Nuno M and Voita, Elena and Martins, Andr{\'e} FT},
  journal={arXiv preprint arXiv:2208.05309},
  year={2022}
}

@inproceedings{DBLP:conf/coling/EbrahimiLD18,
  author    = {Javid Ebrahimi and
               Daniel Lowd and
               Dejing Dou},
  title     = {On Adversarial Examples for Character-Level Neural Machine Translation},
  booktitle = {Proceedings of the 27th International Conference on Computational
               Linguistics, {COLING} 2018, Santa Fe, New Mexico, USA, August 20-26,
               2018},
  pages     = {653--663},
  year      = {2018},
}

@inproceedings{DBLP:conf/acl/EbrahimiRLD18,
  author    = {Javid Ebrahimi and
               Anyi Rao and
               Daniel Lowd and
               Dejing Dou},
  title     = {HotFlip: White-Box Adversarial Examples for Text Classification},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2018, Melbourne, Australia, July 15-20, 2018, Volume
               2: Short Papers},
  pages     = {31--36},
  year      = {2018},
}

@inproceedings{DBLP:conf/conll/BlohmJSYV18,
  author    = {Matthias Blohm and
               Glorianna Jagfeld and
               Ekta Sood and
               Xiang Yu and
               Ngoc Thang Vu},
  title     = {Comparing Attention-Based Convolutional and Recurrent Neural Networks:
               Success and Limitations in Machine Reading Comprehension},
  booktitle = {Proceedings of the 22nd Conference on Computational Natural Language
               Learning, CoNLL 2018, Brussels, Belgium, October 31 - November 1,
               2018},
  pages     = {108--118},
  year      = {2018},
}

@inproceedings{DBLP:conf/aaai/ChengYCZH20,
  author    = {Minhao Cheng and
               Jinfeng Yi and
               Pin{-}Yu Chen and
               Huan Zhang and
               Cho{-}Jui Hsieh},
  title     = {Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models
               with Adversarial Examples},
  booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
               2020, The Thirty-Second Innovative Applications of Artificial Intelligence
               Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
               Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
               February 7-12, 2020},
  pages     = {3601--3608},
  year      = {2020},
}

@article{chen2022should,
  title={Why should adversarial perturbations be imperceptible? rethink the research paradigm in adversarial nlp},
  author={Chen, Yangyi and Gao, Hongcheng and Cui, Ganqu and Qi, Fanchao and Huang, Longtao and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2210.10683},
  year={2022}
}

@article{berger2017israel,
  title={Israel arrests Palestinian because Facebook translated’good morning’to’attack them’},
  author={Berger, Yotam},
  journal={Ha’aretz},
  volume={22},
  year={2017}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}


@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{wallace,
    title = "Universal Adversarial Triggers for Attacking and Analyzing {NLP}",
    author = "Wallace, Eric  and
      Feng, Shi  and
      Kandpal, Nikhil  and
      Gardner, Matt  and
      Singh, Sameer",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    urltmp = "https://aclanthology.org/D19-1221",
    doitmp = "10.18653/v1/D19-1221",
    pages = "2153--2162",
    abstract = "Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94{\%} to 0.55{\%}, 72{\%} of {``}why{''} questions in SQuAD to be answered {``}to kill american people{''}, and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.",
}
@inproceedings{ebrahimi-etal-2018-hotflip,
    title = "{H}ot{F}lip: White-Box Adversarial Examples for Text Classification",
    author = "Ebrahimi, Javid  and
      Rao, Anyi  and
      Lowd, Daniel  and
      Dou, Dejing",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    urltmp = "https://aclanthology.org/P18-2006",
    doitmp = "10.18653/v1/P18-2006",
    pages = "31--36",
    abstract = "We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier. We find that only a few manipulations are needed to greatly decrease the accuracy. Our method relies on an atomic flip operation, which swaps one token for another, based on the gradients of the one-hot input vectors. Due to efficiency of our method, we can perform adversarial training which makes the model more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.",
}
@article{Seq2Sick,
    author = {Cheng, Minhao and Yi, Jinfeng and Zhang, Huan and Chen, Pin-Yu and Hsieh, Cho-Jui},
    year = {2018},
    month = {03},
    pages = {},
    title = {Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples},
    volume = {34},
    journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
    doitmp = {10.1609/aaai.v34i04.5767}
}

@article{deng2022rlprompt,
  title={Rlprompt: Optimizing discrete text prompts with reinforcement learning},
  author={Deng, Mingkai and Wang, Jianyu and Hsieh, Cheng-Ping and Wang, Yihan and Guo, Han and Shu, Tianmin and Song, Meng and Xing, Eric P and Hu, Zhiting},
  journal={arXiv preprint arXiv:2205.12548},
  year={2022}
}

@article{Sadrizadeh2023TargetedAA,
  title={Targeted Adversarial Attacks against Neural Machine Translation},
  author={Sahar Sadrizadeh and AmirHossein Dabiri Aghdam and Ljiljana Dolamic and Pascal Frossard},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.01068}
}

@inproceedings{li-etal-2020-bert-attack,
    title = "{BERT}-{ATTACK}: Adversarial Attack Against {BERT} Using {BERT}",
    author = "Li, Linyang  and
      Ma, Ruotian  and
      Guo, Qipeng  and
      Xue, Xiangyang  and
      Qiu, Xipeng",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    urltmp = "https://aclanthology.org/2020.emnlp-main.500",
    doitmp = "10.18653/v1/2020.emnlp-main.500",
    pages = "6193--6202",
    abstract = "Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose \textbf{BERT-Attack}, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at \url{https://github.com/LinyangLee/BERT-Attack}.",
}

@article{Li2018TextBuggerGA,
  title={TextBugger: Generating Adversarial Text Against Real-world Applications},
  author={Jinfeng Li and Shouling Ji and Tianyu Du and Bo Li and Ting Wang},
  journal={ArXiv},
  year={2018},
  volume={abs/1812.05271}
}

@inproceedings{wan2022paeg,
  title={PAEG: Phrase-level Adversarial Example Generation for Neural Machine Translation},
  author={Wan, Juncheng and Yang, Jian and Ma, Shuming and Zhang, Dongdong and Zhang, Weinan and Yu, Yong and Li, Zhoujun},
  booktitle={Proceedings of the 29th International Conference on Computational Linguistics},
  pages={5085--5097},
  year={2022}
}

@inproceedings{guo-etal-2021-gradient,
    title = "Gradient-based Adversarial Attacks against Text Transformers",
    author = "Guo, Chuan  and
      Sablayrolles, Alexandre  and
      J{\'e}gou, Herv{\'e}  and
      Kiela, Douwe",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    urltmp = "https://aclanthology.org/2021.emnlp-main.464",
    doitmp = "10.18653/v1/2021.emnlp-main.464",
    pages = "5747--5757",
    abstract = "We propose the first general-purpose gradient-based adversarial attack against transformer models. Instead of searching for a single adversarial example, we search for a distribution of adversarial examples parameterized by a continuous-valued matrix, hence enabling gradient-based optimization. We empirically demonstrate that our white-box attack attains state-of-the-art attack performance on a variety of natural language tasks, outperforming prior work in terms of adversarial success rate with matching imperceptibility as per automated and human evaluation. Furthermore, we show that a powerful black-box transfer attack, enabled by sampling from the adversarial distribution, matches or exceeds existing methods, while only requiring hard-label outputs.",
}

@INPROCEEDINGS{9747475,
  author={Sadrizadeh, Sahar and Dolamic, Ljiljana and Frossard, Pascal},
  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Block-Sparse Adversarial Attack to Fool Transformer-Based Text Classifiers}, 
  year={2022},
  volume={},
  number={},
  pages={7837-7841},
  doitmp={10.1109/ICASSP43922.2022.9747475}}


@inproceedings{wang2022super,
  title={Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Naik, Atharva and Ashok, Arjun and Dhanasekaran, Arut Selvan and Arunkumar, Anjana and Stap, David and others},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={5085--5109},
  year={2022}
}

@article{wang2021putting,
  title={Putting humans in the natural language processing loop: A survey},
  author={Wang, Zijie J and Choi, Dongjin and Xu, Shenyu and Yang, Diyi},
  journal={arXiv preprint arXiv:2103.04044},
  year={2021}
}