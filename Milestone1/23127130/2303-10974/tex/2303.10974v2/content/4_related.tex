\section{Related work}

In recent years, large language models have improved significantly in various NLP areas, especially in generative tasks.
A lot of new concepts were introduced, starting from attention mechanism~\cite{bahdanau2014neural}, transformers~\cite{vaswani2017attention} to multitask, learning from instructions~\cite{wang2022super} and human feedback~\cite{wang2021putting}.
The last becomes extremely popular in the generative context including machine translation. 
% new architectures were proposed~\cite{radford2019language,brown2020language}, and, 
Consequently, the usage of machine translation tools has become a necessary compound for understanding a foreign language. 
Unfortunately, like other neural network-based algorithms, these tools are vulnerable to adversarial examples~\cite{DBLP:journals/corr/GoodfellowSS14}. 
Starting from text classification \cite{li-etal-2020-bert-attack,DBLP:conf/acl/EbrahimiRLD18,Li2018TextBuggerGA}, vulnerability and robustness received a lot of attention in the NLP community. 
For MT systems one of the pioneering works was~\cite{ebrahimi2018adversarial}, where a character-level approach to generate adversarial examples was proposed.
Inheriting HotFlip~\cite{ebrahimi-etal-2018-hotflip} there were considered
% white- and black-box
settings, where only a few symbols in an input query are subject to change imitating typos.

While white-box optimization may yield stronger adversarial perturbations it implies access to the model's architecture and weights which is impractical in the case of online MT tools. 
In~\cite{wallace} there was considered a white-box universal approach to a targeted attack on conditional text generation. 
The authors modeled perturbation as an insertion of a trigger, a token sequence of small length, that results in a generated sequence similar to the target set of sentences. 
While during experiments certain triggers cause a model to produce sensitive racist output, they are generally meaningless and similarly to character-level attacks are easy to detect. 
Authors of~\cite{guo-etal-2021-gradient,9747475} reported high attack transferability making this approach promising for black-box setup, however,  the research is limited only to the GPT-2 model for generation task.
The above papers use greedy techniques to walk through the searching space during the optimization, on the other hand, attacks on NLP models could be found via projection onto embeddings~\cite{wallace}, and for MT task this was discovered in~\cite{Seq2Sick,Sadrizadeh2023TargetedAA,sadrizadeh2023transfool}. 
In~\cite{zhang2021crafting}, it was shown that black-box optimization may yield transferable word-level attack that fools online translation tools, e.g., Baidu and Bing. 
This work proposed to use the word saliency as the measure of uncertainty. 
Masking candidates the saliency was estimated via additional BERT model~\cite{devlin2018bert} which lead to strong readable and imperceptible adversaries, however, neither human evaluation was performed nor quantities results for online tools were given. In~\cite{wan2022paeg}, a gradient-based approach to generate phrase-level adversarial examples for neural MT systems was proposed. Similarly to~\cite{zhang2021crafting}, it is proposed to estimate the vulnerable word positions are estimated in an input phrase with the use of gradient information and replace corresponding words by the candidates computed with an auxiliary model.

We also note the recent work~\cite{guerreiro2022looking}, in which the hallucination problem of MT systems is discussed and the method for detecting and alleviating such hallucinations is presented.
Authors identified a set of hallucinations in a large number of translations by various hallucination detection methods (anomalous encoder-decoder attention, simple model uncertainty measures, etc.), and gathered for them human annotations.
This allowed them to conduct a comparative analysis of detection methods and to suggest a new approach for detection.