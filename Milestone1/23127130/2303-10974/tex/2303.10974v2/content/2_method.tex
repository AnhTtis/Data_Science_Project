\section{Method}

\begin{figure}[t!]
\includegraphics[width=\textwidth]
    {figures/TranFighterPro.png}
\caption{
    Proposed approach for the search of the ``hallucinogens''.
}
\label{fig:method}
\end{figure}

Our approach is presented in Figure~\ref{fig:method} and is based on the idea of searching for $d$-letter combinations $w = (w_1, w_2, \ldots, w_d)$ in the source language that are the least similar to the existing words (gibberish or “hallucinogens”), however correctly translatable into the target language as  $\tran{w}$. 
Without loss of generality, we have chosen Russian as the source language (it has $n = 33$ letters of the alphabet), English language as the target language (it has $n_t = 26$ letters of the alphabet), and $d = 7$.

To assess the quality (score) of a word or phrase, we use perplexity~\cite{sadrizadeh2023transfool}:
\begin{equation}\label{eq:score}
\func{s}(w) =
    \func{exp}\left[
        -\frac{1}{d}
        \sum_{i=1}^{d}
            \log p_{\theta}(w_i | w_{<i})
    \right],
\end{equation}
where $p_{\theta}(w_i | w_{<i})$ is the log-likelihood of the i-th token conditioned on the preceding tokens according to the pre-trained GPT-2 model.
It can be thought of as an evaluation of the model’s ability to predict among the set of specified tokens in a corpus.
The value $\func{s}(w)$ is non-negative, for the most common words it is close to zero, and for the gibberish, it is expected to be a large positive number.

To maximize the difference between the perplexity of the translation $\tran{w}$ and the source text $w$ we introduce the following loss function:
\begin{equation}\label{eq:optimizer_loss_level1}
P(w) =
    \func{s}(\tran{w}) -
    \func{s}(w) +
    \func{penalty}(\tran{w}),
\end{equation}
where $\func{penalty}(\tran{w})$ is a penalty term, which is equal to a large positive number for the case when the translation is too short (less than $5$ characters) or contains stop characters (various non-letter characters); otherwise it is zero.

We search for minimum of~\eqref{eq:optimizer_loss_level1} in terms of the discrete optimization problem for an implicitly given $d$-dimensional array $\tens{P} \in \set{R}^{n \times n \times \ldots \times n}$:
\begin{equation}\label{eq:tensor}
\tens{P}[i_1, i_2, \ldots, i_d] =
    P(w),
\quad
w = (
    A[i_1], A[i_2], \ldots, A[i_d]
),
\end{equation}
where $[i_1, i_2, \ldots, i_d]$ is a multi-index, $A$ is the alphabet, and $A[i_k]$ if the $i_k$-th symbol of the alphabet.
For example, as shown in Figure~\ref{fig:method}, for the multi-index $[32,\,2,\,1,\,2,\,1,\,2,\,33]$ we get the word $w$ ``\foreignlanguage{russian}{юбабабя}'' in Russian.

To find the ``hallucinogen'' $\hat{w}$ which minimizes the loss function~\eqref{eq:optimizer_loss_level1}, we use the global optimization method PROTES.
It is based on the low-rank tensor train (TT) decomposition~\cite{oseledets2011tensor,cichocki2016tensor,cichocki2017tensor,sozykin2022ttopt,chertkov2022optimization}, which allows bypassing the curse of dimensionality problem\footnote{
    The complexity of algorithms in the TT-format (e.\,g., element-wise addition, multiplication, solution of linear systems, convolution, integration, etc.) turns out to be polynomial in dimension and mode size, and it makes TT-decomposition extremely popular in a wide range of applications, including computational mathematics and machine learning.
}.
% In the last few years, several new discrete optimization algorithms based on the TT-format have been proposed~\cite{sozykin2022ttopt,selvanayagam2022global,shetty2022tensor,chertkov2022optimization}.
% Due to the use of a low-rank format, they are particularly effective for highly multidimensional optimization problems.
% However, the most suitable in our case is the PROTES method, which 
The method operates with a multidimensional discrete probability distribution in the TT-format, followed by efficient sampling from it and updating its parameters by stochastic gradient ascent to approximate the minimum or maximum in a better way.
We save the request history of the optimization method and, at the end of its run, we form a set of hallucinogens $\hat{w}^{(1)}, \hat{w}^{(2)}, \ldots, \hat{w}^{(m)}$ ($m$ here is a number of requests for a translator, i.e. computational budget), ordered by the value of the loss function.

It is worth mentioning that the described method does not generate adversarial examples per se (i.e., it does not force mistranslation) but produces examples (hallucinogens) that are translatable when they should not be.
However, it turns out to be an interesting empirical fact that combinations of hallucinogens also lead to the emergence of translation artifacts, while, as we will show below, these artifacts can turn out to be long meaningful phrases in the target language.

Accordingly, in the second stage, we repeat the described optimization process, composing phrases of $d^{(2)}$ hallucinogens.
As the possible candidates, we select $n^{(2)}$ ($n^{(2)} \leq m$) top hallucinogens $\hat{w}^{(1)}, \hat{w}^{(2)}, \ldots, \hat{w}^{(n^{(2)})}$ from the result of the first stage.
Without loss of generality, we have chosen $d^{(2)} = 7$ and $n^{(2)} = 33$, i.e., the same values as in the first stage.
In this case, we use the loss function~\eqref{eq:optimizer_loss_level1} without the second term, i.e., we do not maximize the perplexity of the input text, since it is already composed of the hallucinogens.
Note that we can repeat this process an arbitrary number of times, getting longer and longer ``phrases'' from the hallucinogens.