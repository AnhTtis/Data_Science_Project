\section{Related work}

In recent years, large language models have improved significantly in various NLP areas, especially in generative tasks.
A lot of new concepts were introduced, starting from attention mechanism~\cite{bahdanau2014neural}, transformers~\cite{vaswani2017attention} to multitask, learning from instructions~\cite{wang2022super} and human feedback~\cite{wang2021putting}.
The last becomes extremely popular in the generative context including machine translation. 
% new architectures were proposed~\cite{radford2019language,brown2020language}, and, 
Consequently, the usage of machine translation tools has become a necessary compound for understanding a foreign language. 
Unfortunately, like other neural network-based algorithms, these tools are vulnerable to adversarial examples~\cite{DBLP:journals/corr/GoodfellowSS14}. 
Starting from text classification \cite{li-etal-2020-bert-attack,DBLP:conf/acl/EbrahimiRLD18,Li2018TextBuggerGA}, vulnerability and robustness received a lot of attention in the NLP community. 
For MT systems one of the pioneering works was~\cite{ebrahimi2018adversarial}, where authors proposed a character-level approach to generate adversarial examples.
% that neural MT systems are vulnerable to character-level perturbations, where only a few symbols in an input query are subject to change. 
Inheriting HotFlip~\cite{ebrahimi-etal-2018-hotflip} there were considered white-box and black-box settings, where only a few symbols in an input query are subject to change imitating typos.

While white-box optimization may yield stronger adversarial perturbations it implies access to the model's architecture and weights which is impractical in the case of online MT tools. 
In~\cite{wallace} there was considered a white-box universal approach to a targeted attack on conditional text generation. 
The authors modeled perturbation as an insertion of a trigger, a token sequence of small length, that results in a generated sequence similar to the target set of sentences. 
While during experiments certain triggers cause a model to produce sensitive racist output, they are generally meaningless and similarly to character-level attacks are easy to detect. 
Authors of~\cite{guo-etal-2021-gradient,9747475} reported high attack transferability making this approach promising for black-box setup, however,  the research is limited only to the GPT-2 model for generation task. 
The above papers use greedy techniques to walk through the searching space during the optimization, on the other hand, attacks on NLP models could be found via projection onto embeddings~\cite{wallace}, and for MT task this was discovered in~\cite{Seq2Sick,Sadrizadeh2023TargetedAA,sadrizadeh2023transfool}. 
In~\cite{zhang2021crafting}, it was shown that black-box optimization may yield transferable word-level attack that fools online translation tools, for example Baidu and Bing translators. 
This work proposed to use the word saliency as the measure of uncertainty. 
Masking candidates the saliency was estimated via additional BERT model~\cite{devlin2018bert}  which lead to strong readable and imperceptible adversaries, however, neither human evaluation was performed nor quantities results for online tools were given. In~\cite{wan2022paeg}, a gradient-based approach to generate phrase-level adversarial examples for neural MT systems was proposed. Similarly to~\cite{zhang2021crafting}, it is proposed to estimate the vulnerable word positions are estimated in an input phrase with the use of gradient information and replace corresponding words by the candidates computed with an auxiliary model.

% \mynote{actually we may underline that we do not generate adversarial examples per se (we arent aimed at misclassification), but rather generate inputs that are been translated though they should not}

% \mynote{TODO: Maybe add more criticism of zhang2021crafting and point out the differences in our approach.}

% \todopa{}{}{
% https://www.semanticscholar.org/paper/AdvAug\%3A-Robust-Adversarial-Augmentation-for-Neural-Cheng-Jiang/1e7d3a9846da556bc7b84ae1410d257b89448c30
% }

%\todopa{}{}{
%https://www.semanticscholar.org/paper/A-Targeted-Attack-on-Black-Box-Neural-Machine-with-Xu-Wang/2a46eb47e8742be29b16a5b83dc1a38616b24ce6
%}

%\todopa{}{}{https://www.semanticscholar.org/paper/PAEG\%3A-Phrase-level-Adversarial-Example-Generation-Wan-Yang/a6dd2a8debb5d5324c4f2be7fb7bb52ce109cbaf}

% \todopa{}{}{
% https://download.huan-zhang.com/events/srml2022/accepted/bhandari22lost.pdf
% }

%\todopa{}{}{http://fan-yao.com/paper/2021_SEED_nmtstroke.pdf}

% \todopa{}{}{https://arxiv.org/pdf/2303.01068v1.pdf}

%\todopa{kosinski2023theory}
%    {Theory of mind may have spontaneously emerged in large language models}
%    {https://arxiv.org/pdf/2302.02083.pdf}
%    {We can say that large language models are very clever now, etc...}

% \todopa{ebrahimi2018adversarial}
%     {On adversarial examples for character-level neural machine translation}
%     {https://arxiv.org/pdf/1806.09030.pdf}
%     {Very related work (see beamsearch in the text also)...}

% \todopa{zhang2021crafting}
%     {Crafting adversarial examples for neural machine translation}
%     {https://github.com/JHL-HUST/AdvNMT-WSLS}
%     {Very related work. See: ``Besides, WSLS exhibits strong transferability on attacking Baidu and Bing online translators.''}

% \todopa{sadrizadeh2023transfool}
%     {TransFool: An Adversarial Attack against Neural Machine Translation Models}
%     {https://arxiv.org/pdf/2302.00944.pdf}
%     {Very related work!}