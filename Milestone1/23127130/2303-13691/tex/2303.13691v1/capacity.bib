@article{Charles2014,
abstract = {With our ability to record more neurons simultaneously, making sense of these data is a challenge. Functional connectivity is one popular way to study the relationship between multiple neural signals. Correlation-based methods are a set of currently well-used techniques for functional connectivity estimation. However, due to explaining away and unobserved common inputs (Stevenson et al., 2008), they produce spurious connections. The general linear model (GLM), which models spikes trains as Poisson processes (Okatan et al., 2005; Truccolo et al., 2005; Pillow et al., 2008), avoids these confounds. We develop here a new class of methods by using differential signals based on simulated intracellular voltage recordings. It is equivalent to a regularized AR(2) model. We also expand the method to simulated local field potential (LFP) recordings and calcium imaging. In all of our simulated data, the differential covariance-based methods achieved better or similar performance to the GLM method and required fewer data samples. This new class of methods provides alternative ways to analyze neural signals.},
archivePrefix = {arXiv},
arxivId = {1706.02451},
author = {Charles, Adam S. and Yap, Han Lun and Rozell, Christopher J.},
doi = {10.1162/NECO_a_00590},
eprint = {1706.02451},
file = {:home/epaxon/Research/Redwood/Kanerva/References/charles2014.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
month = {jun},
number = {6},
pages = {1198--1235},
pmid = {25602775},
title = {{Short-Term Memory Capacity in Networks via the Restricted Isometry Property}},
url = {http://arxiv.org/abs/1706.02451 http://www.mitpressjournals.org/doi/10.1162/NECO{\_}a{\_}00590},
volume = {26},
year = {2014}
}
@book{Hulse2007,
author = {H{\"{u}}lse, Martin and Wischmann, Steffen and Manoonpong, Poramate and von Twickel, Arndt and Pasemann, Frank},
file = {:home/epaxon/Research/Redwood/Kanerva/References/[Max{\_}Lungarella,{\_}{\_}Fumiya{\_}Iida,{\_}{\_}Josh{\_}C.{\_}Bongard,{\_}{\_}(b-ok.org).pdf:pdf},
isbn = {3-540-77295-2, 978-3-540-77295-8},
pages = {186--195},
title = {{50 Years of Artificial Intelligence}},
year = {2007}
}
@article{Jain1976,
abstract = {A device is proposed which is the optical analog of the transistor wherein a weak incoming optical signal directly controls a strong outgoing optical signal. The basic principle utilized in the proposed device is the strong dependence of second harmonic generation on the phase‐matching condition. A weak incoming signal can perturb the indices of refraction in a birefringent material so that the second harmonic power derived from a pump beam is significantly affected. Large incremental power gain is shown to be possible. Tellurium is considered as an example.},
author = {Jain, K. and Pratt, G. W.},
doi = {10.1063/1.88627},
file = {:home/epaxon/Research/Redwood/Kanerva/References/jain1976.pdf:pdf},
issn = {00036951},
journal = {Applied Physics Letters},
number = {12},
pages = {719--721},
title = {{Optical transistor}},
volume = {28},
year = {1976}
}
@article{Lim2011,
abstract = {In short-term memory networks, transient stimuli are represented by patterns of neural activity that persist long after stimulus offset. Here, we compare the performance of two prominent classes of memory networks, feedback-based attractor networks and feedforward networks, in conveying information about the amplitude of a briefly presented stimulus in the presence of gaussian noise. Using Fisher information as a metric of memory performance, we find that the optimal form of network architecture depends strongly on assumptions about the forms of nonlinearities in the network. For purely linear networks, we find that feedforward networks outperform attractor networks because noise is continually removed from feedforward networks when signals exit the network; as a result, feedforward networks can amplify signals they receive faster than noise accumulates over time. By contrast, attractor networks must operate in a signal-attenuating regime to avoid the buildup of noise. However, if the amplification of signals is limited by a finite dynamic range of neuronal responses or if noise is reset at the time of signal arrival, as suggested by recent experiments, we find that attractor networks can outperform feedforward ones. Under a simple model in which neurons have a finite dynamic range, we find that the optimal attractor networks are forgetful if there is no mechanism for noise reduction with signal arrival but nonforgetful (perfect integrators) in the presence of a strong reset mechanism. Furthermore, we find that the maximal Fisher information for the feedforward and attractor networks exhibits power law decay as a function of time and scales linearly with the number of neurons. These results highlight prominent factors that lead to trade-offs in the memory performance of networks with different architectures and constraints, and suggest conditions under which attractor or feedforward networks may be best suited to storing information about previous stimuli.},
author = {Lim, Sukbin and Goldman, Mark S.},
doi = {10.1162/NECO_a_00234},
file = {:home/epaxon/Research/Redwood/Kanerva/References/LimGoldman{\_}NoiseToleranceAttractorFF{\_}published.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
number = {2},
pages = {332--390},
pmid = {22091664},
title = {{Noise Tolerance of Attractor and Feedforward Memory Models}},
url = {http://dx.doi.org/10.1162/NECO{\_}a{\_}00234{\%}5Cnhttp://www.mitpressjournals.org/action/cookieAbsent{\%}5Cnhttp://www.mitpressjournals.org/doi/abs/10.1162/NECO{\_}a{\_}00234?prevSearch=allfield{\%}3A{\%}28goldman+lim{\%}29{\&}searchHistoryKey={\%}5Cnhttp://www.mitpressjournals.org/doi/},
volume = {24},
year = {2011}
}
@article{Wallace2013,
abstract = {The brain is easily able to process and categorize complex time-varying signals. For example, the two sentences, "It is cold in London this time of year" and "It is hot in London this time of year," have different meanings, even though the words hot and cold appear several seconds before the ends of the two sentences. Any network that can tell these sentences apart must therefore have a long temporal memory. In other words, the current state of the network must depend on events that happened several seconds ago. This is a difficult task, as neurons are dominated by relatively short time constants--tens to hundreds of milliseconds. Nevertheless, it was recently proposed that randomly connected networks could exhibit the long memories necessary for complex temporal processing. This is an attractive idea, both for its simplicity and because little tuning of recurrent synaptic weights is required. However, we show that when connectivity is high, as it is in the mammalian brain, randomly connected networks cannot exhibit temporal memory much longer than the time constants of their constituent neurons.},
author = {Wallace, Edward and Maei, Hamid Reza and Latham, Peter E.},
doi = {10.1162/NECO_a_00449},
file = {:home/epaxon/Research/Redwood/Kanerva/References/wallace{\_}maei{\_}latham.pdf:pdf},
isbn = {10.1162/NECO{\_}a{\_}00449},
issn = {0899-7667},
journal = {Neural Computation},
number = {6},
pages = {1408--1439},
pmid = {23517097},
title = {{Randomly Connected Networks Have Short Temporal Memory}},
url = {http://www.mitpressjournals.org/doi/10.1162/NECO{\_}a{\_}00449},
volume = {25},
year = {2013}
}
@article{Rahimi2017,
abstract = {We outline a model of computing with high- dimensional (HD) vectors - where the dimensionality is in the thousands. It is built on ideas from traditional (symbolic) computing and artificial neural nets/deep learning, and complements them with ideas from probability theory, statistics, and abstract algebra. Key properties of HD computing include a well-defined set of arithmetic operations on vectors, generality, scalability, robustness, fast learning, and ubiquitous parallel operation, making it possible to develop efficient algorithms for large-scale real-world tasks.We present a 2-D architecture and demonstrate its functionality with examples from text analysis, pattern recognition, and biosignal processing, while achieving high levels of classification accuracy (close to or above conventional machine- learning methods), energy efficiency, and robustness with simple algorithms that learn fast. HD computing is ideally suited for 3-D nanometer circuit technology, vastly increasing circuit density and energy efficiency, and paving a way to systems capable of advanced cognitive tasks. Index},
author = {Rahimi, Abbas and Datta, Sohum and Kleyko, Denis and Frady, Edward Paxon and Olshausen, Bruno and Kanerva, Pentti and Rabaey, Jan M.},
doi = {10.1109/TCSI.2017.2705051},
file = {:home/epaxon/Research/Redwood/Kanerva/References/TCAS17.pdf:pdf},
issn = {1549-8328},
journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
month = {sep},
number = {9},
pages = {2508--2521},
title = {{High-Dimensional Computing as a Nanoscalable Paradigm}},
url = {http://ieeexplore.ieee.org/document/7942066/},
volume = {64},
year = {2017}
}
@article{Charles2017,
abstract = {Recurrent neural networks (RNNs) have drawn interest from machine learning researchers because of their effectiveness at preserving past inputs for time-varying data processing tasks. To understand the success and limitations of RNNs, it is critical that we advance our analysis of their fundamental memory properties. We focus on echo state networks (ESNs), which are RNNs with simple memoryless nodes and random connectivity. In most existing analyses, the short-term memory (STM) capacity results conclude that the ESN network size must scale linearly with the input size for unstructured inputs. The main contribution of this paper is to provide general results characterizing the STM capacity for linear ESNs with multidimensional input streams when the inputs have common low-dimensional structure: sparsity in a basis or significant statistical dependence between inputs. In both cases, we show that the number of nodes in the network must scale linearly with the information rate and poly-logarithmically with the ambient input dimension. The analysis relies on advanced applications of random matrix theory and results in explicit non-asymptotic bounds on the recovery error. Taken together, this analysis provides a significant step forward in our understanding of the STM properties in RNNs.},
archivePrefix = {arXiv},
arxivId = {1605.08346},
author = {Charles, Adam and Yin, Dong and Rozell, Christopher},
eprint = {1605.08346},
file = {:home/epaxon/Research/Redwood/Kanerva/References/16-270.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {0,4,and christopher rozell,at,attribution requirements are provided,by,c 2017 adam charles,cc-by 4,covery,creativecommons,dong yin,license,licenses,low-rank re-,org,recurrent neural networks,restricted isometry property,see https,short-term memory,sparse signal recovery},
pages = {1--37},
title = {{Distributed Sequence Memory of Multidimensional Inputs in Recurrent Networks}},
url = {http://arxiv.org/abs/1605.08346},
volume = {18},
year = {2017}
}
@article{Charles2014a,
abstract = {{\textcopyright} 2014 IEEE. The short term memory of randomly connected networks has been recently studied in order to better understand the computational and predictive power of such networks. In particular, random, linear, orthogonal networks have been explored extensively in the context a single input stream driving the network. The most recent results state that a stream of length N can be recovered from a network of size 0(S log 6  (N)) assuming that the input is S-sparse in some basis. Little work, however, addresses more complex networks where multiple input streams feed into the same network. In this paper we extend the results for recovering sparse input streams the multiple input streams feeding into the same network. We find that we can recover L input streams of length N with a network that has O (S log 5  (LN)) nodes.},
author = {Charles, Adam S. and Yin, Dong and Rozell, Christopher J.},
doi = {10.1109/GlobalSIP.2014.7032143},
file = {:home/epaxon/Research/Redwood/Kanerva/References/07032143.pdf:pdf},
isbn = {9781479970889},
journal = {2014 IEEE Global Conference on Signal and Information Processing, GlobalSIP 2014},
keywords = {Linear neural network,Restricted isometry constant,Short-term memory,Sparse signals},
pages = {379--383},
title = {{Can random linear networks store multiple long input streams?}},
year = {2014}
}
@article{Golomb1964,
author = {Golomb, S. W.},
journal = {Bulletin of the American Mathematical Society},
number = {747},
title = {{Random Permutations}},
volume = {70},
year = {1964}
}
@article{Jaeger2004,
abstract = {We present a method for learning nonlinear systems, echo state networks (ESNs). ESNs employ artificial recurrent neural networks in a way that has recently been proposed independently as a learning mechanism in biological brains. The learning method is computationally efficient and easy to use. On a benchmark task of predicting a chaotic time series, accuracy is improved by a factor of 2400 over previous techniques. The potential for engineering ap- plications is illustrated by equalizing a communication channel, where the signal error rate is improved by two orders of magnitude.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jaeger, H. and Haas, Harald},
doi = {10.1126/science.1091277},
eprint = {arXiv:1011.1669v3},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaeger - 2004 - Harnessing Nonlinearity Predicting Chaotic Systems and Saving Energy in Wireless Communication.pdf:pdf},
isbn = {1095-9203 (Electronic)$\backslash$n0036-8075 (Linking)},
issn = {0036-8075},
journal = {Science},
number = {5667},
pages = {78--80},
pmid = {15064413},
title = {{Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1091277},
volume = {304},
year = {2004}
}
@article{Peterson1954,
author = {Peterson, W.W. and Birdsall, T.G. and Fox, W.C.},
journal = {Proceedings of the IRE Professional Group on Information Theory},
pages = {171--212},
title = {{The theory of signal detectability.}},
volume = {4},
year = {1954}
}
@article{kullback1951,
author = {Kullback, S and Leibler, R A},
doi = {10.1214/aoms/1177729694},
journal = {Ann. Math. Statist.},
number = {1},
pages = {79--86},
publisher = {The Institute of Mathematical Statistics},
title = {{On Information and Sufficiency}},
url = {https://doi.org/10.1214/aoms/1177729694},
volume = {22},
year = {1951}
}
@article{Kleyko2017,
abstract = {We propose an integer approximation of Echo State Networks (ESN) based on the mathematics of hyperdimensional computing. The reservoir of the proposed Integer Echo State Network (intESN) contains only n-bits integers and replaces the recurrent matrix multiply with an efficient cyclic shift operation. Such an architecture results in dramatic improvements in memory footprint and computational efficiency, with minimal performance loss. Our architecture naturally supports the usage of the trained reservoir in symbolic processing tasks of analogy making and logical inference.},
archivePrefix = {arXiv},
arxivId = {1706.00280},
author = {Kleyko, Denis and Frady, Edward Paxon and Osipov, Evgeny},
eprint = {1706.00280},
file = {:home/epaxon/Research/Redwood/Kanerva/References/1706.00280.pdf:pdf},
journal = {ArXiv},
month = {jun},
pages = {1--10},
title = {{Integer Echo State Networks: Hyperdimensional Reservoir Computing}},
url = {http://arxiv.org/abs/1706.00280},
year = {2017}
}
@article{Graves2009,
abstract = {Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance.},
author = {Graves, Alex and Liwicki, Marcus and Fern{\'{a}}ndez, Santiago and Bertolami, Roman and Bunke, Horst and Schmidhuber, J{\"{u}}rgen},
doi = {10.1109/TPAMI.2008.137},
file = {:home/epaxon/Research/Redwood/Kanerva/References/tpami{\_}2008.pdf:pdf},
isbn = {0162-8828 (Print)$\backslash$r0098-5589 (Linking)},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Bidirectional long short-term memory,Connectionist temporal classification,Handwriting recognition,Hidden Markov model,Offline handwriting,Online handwriting,Recurrent neural networks},
number = {5},
pages = {855--868},
pmid = {19299860},
title = {{A novel connectionist system for unconstrained handwriting recognition}},
volume = {31},
year = {2009}
}
@article{Jain1976,
abstract = {A device is proposed which is the optical analog of the transistor wherein a weak incoming optical signal directly controls a strong outgoing optical signal. The basic principle utilized in the proposed device is the strong dependence of second harmonic generation on the phase‐matching condition. A weak incoming signal can perturb the indices of refraction in a birefringent material so that the second harmonic power derived from a pump beam is significantly affected. Large incremental power gain is shown to be possible. Tellurium is considered as an example.},
author = {Jain, K. and Pratt, G. W.},
doi = {10.1063/1.88627},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jain, Pratt - 1976 - Optical transistor.pdf:pdf},
issn = {00036951},
journal = {Applied Physics Letters},
number = {12},
pages = {719--721},
title = {{Optical transistor}},
volume = {28},
year = {1976}
}
@article{Kullback1951,
author = {Kullback, S and Leibler, R A},
doi = {10.1214/aoms/1177729694},
journal = {Ann. Math. Statist.},
number = {1},
pages = {79--86},
publisher = {The Institute of Mathematical Statistics},
title = {{On Information and Sufficiency}},
url = {https://doi.org/10.1214/aoms/1177729694},
volume = {22},
year = {1951}
}
@article{Kleyko2017,
abstract = {We propose an integer approximation of Echo State Networks (ESN) based on the mathematics of hyperdimensional computing. The reservoir of the proposed Integer Echo State Network (intESN) contains only n-bits integers and replaces the recurrent matrix multiply with an efficient cyclic shift operation. Such an architecture results in dramatic improvements in memory footprint and computational efficiency, with minimal performance loss. Our architecture naturally supports the usage of the trained reservoir in symbolic processing tasks of analogy making and logical inference.},
archivePrefix = {arXiv},
arxivId = {1706.00280},
author = {Kleyko, Denis and Frady, Edward Paxon and Osipov, Evgeny},
eprint = {1706.00280},
file = {:home/epaxon/Research/Redwood/Kanerva/References/1706.00280.pdf:pdf},
journal = {ArXiv},
pages = {1--10},
title = {{Integer Echo State Networks: Hyperdimensional Reservoir Computing}},
url = {http://arxiv.org/abs/1706.00280},
year = {2017}
}
@article{Trusts2008,
author = {Trusts, Killam},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Trusts - 2008 - The Kanerva Machine A Generative Distributed Memory.pdf:pdf},
pages = {1--14},
title = {{The Kanerva Machine: A Generative Distributed Memory}},
year = {2008}
}
@article{Graves2009,
abstract = {Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance.},
author = {Graves, Alex and Liwicki, Marcus and Fern{\'{a}}ndez, Santiago and Bertolami, Roman and Bunke, Horst and Schmidhuber, J{\"{u}}rgen},
doi = {10.1109/TPAMI.2008.137},
file = {:home/epaxon/Research/Redwood/Kanerva/References/tpami{\_}2008.pdf:pdf},
isbn = {0162-8828 (Print)$\backslash$r0098-5589 (Linking)},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Bidirectional long short-term memory,Connectionist temporal classification,Handwriting recognition,Hidden Markov model,Offline handwriting,Online handwriting,Recurrent neural networks},
number = {5},
pages = {855--868},
pmid = {19299860},
title = {{A novel connectionist system for unconstrained handwriting recognition}},
volume = {31},
year = {2009}
}
@inproceedings{Abadi2016,
author = {Abadi, Martin and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
pages = {265--283},
title = {{TensorFlow: A system for large-scale machine learning}},
url = {https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf},
year = {2016}
}
@article{Buonomano1995,
author = {Buonomano, Dean V and Merzenich, Michael M},
file = {:home/epaxon/Research/Redwood/Kanerva/References/2886297.pdf:pdf},
journal = {Science},
number = {5200},
pages = {1028--1030},
title = {{Temporal Information Transformed into a Spatial Code by a Neural Network with Realistic Properties}},
volume = {267},
year = {1995}
}
@article{Tsodyks1988,
abstract = {The modified Hopfield model defined in terms of "V-variables" (V = 0; 1), which is appropriate for storage of correlated patterns, is considered. The learning algorithm is proposed to enhance significantly the storage capacity in comparison with previous estimates. At low levels of neural activity, p {\textless}{\textless} 1, we obtain $\alpha$c(p) {\~{}} (p|ln p|)-1 which resembles Gardner's estimate for the maximum storage capacity.},
author = {Tsodyks, Misha V and Feigel'man, M V},
doi = {10.1209/0295-5075/6/2/002},
file = {:home/epaxon/Research/Redwood/Kanerva/References/The{\_}Enhanced{\_}Storage{\_}Capacity{\_}in{\_}Neural{\_}Networks{\_}w.pdf:pdf},
issn = {0295-5075},
journal = {Europhysics Letters},
number = {2},
pages = {101--105},
title = {{The enhanced storage capacity in neural networks with low activity level}},
volume = {6},
year = {1988}
}
@article{McCulloch1943,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {McCulloch, Warren S. and Pitts, Walter},
doi = {10.1007/BF02478259},
eprint = {arXiv:1011.1669v3},
file = {:home/epaxon/Research/Redwood/Kanerva/References/art{\%}3A10.1007{\%}2FBF02478259.pdf:pdf},
isbn = {0007-4985},
issn = {00074985},
journal = {The Bulletin of Mathematical Biophysics},
number = {4},
pages = {115--133},
pmid = {2185863},
title = {{A logical calculus of the ideas immanent in nervous activity}},
volume = {5},
year = {1943}
}
@article{Gao2015,
abstract = {Estimating mutual information (MI) from samples is a fundamental problem in statistics, machine learning, and data analysis. Recently it was shown that a popular class of non-parametric MI estimators perform very poorly for strongly dependent variables and have sample complexity that scales exponentially with the true MI. This undesired behavior was attributed to the reliance of those estimators on local uniformity of the underlying (and unknown) probability density function. Here we present a novel semi-parametric estimator of mutual information, where at each sample point, densities are {\{}$\backslash$em locally{\}} approximated by a Gaussians distribution. We demonstrate that the estimator is asymptotically unbiased. We also show that the proposed estimator has a superior performance compared to several baselines, and is able to accurately measure relationship strengths over many orders of magnitude.},
archivePrefix = {arXiv},
arxivId = {1508.00536},
author = {Gao, Shuyang and Steeg, Greg Ver and Galstyan, Aram},
eprint = {1508.00536},
file = {:home/epaxon/Research/Redwood/Kanerva/References/224.pdf:pdf},
isbn = {9780000000002},
title = {{Estimating Mutual Information by Local Gaussian Approximation}},
url = {http://arxiv.org/abs/1508.00536},
year = {2015}
}
@article{Rumelhart1985,
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
file = {:home/epaxon/Research/Redwood/Kanerva/References/a164453.pdf:pdf},
journal = {ICS Report 8506},
number = {V},
title = {{Learning internal representations by error propagation}},
year = {1985}
}
@article{Gelfand1957,
author = {Gel'fand, I.M. and Yaglom, A.M.},
doi = {10.18287/0134-2452-2015-39-4-459-461.},
file = {:home/epaxon/Research/Redwood/Kanerva/References/rm7522.pdf:pdf},
journal = {Uspekhi Mat. Nauk},
number = {1},
pages = {3--52},
title = {{Computation of the amount of information about a stochastic function contained in another such function}},
volume = {12},
year = {1957}
}
@article{Zhang2013a,
author = {Zhang, Ruiming},
doi = {10.1016/j.jmaa.2012.10.022},
file = {:home/epaxon/Research/Redwood/Kanerva/References/zhang13pochhammer.pdf:pdf},
issn = {0022247X},
journal = {Journal of Mathematical Analysis and Applications},
keywords = {q,q -exponential e q,q -gamma function $\gamma$,q -shifted factorial,z},
number = {1},
pages = {285--292},
publisher = {Elsevier Ltd},
title = {{Asymptotics for some fundamental q-functions}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0022247X12008347},
volume = {400},
year = {2013}
}
@article{Hermans2010,
abstract = {Reservoir Computing is a novel technique which employs recurrent neural networks while circumventing difficult training algorithms. A very recent trend in Reservoir Computing is the use of real physical dynamical systems as implementation platforms, rather than the customary digital emulations. Physical systems operate in continuous time, creating a fundamental difference with the classic discrete time definitions of Reservoir Computing. The specific goal of this paper is to study the memory properties of such systems, where we will limit ourselves to linear dynamics. We develop an analytical model which allows the calculation of the memory function for continuous time linear dynamical systems, which can be considered as networks of linear leaky integrator neurons. We then use this model to research memory properties for different types of reservoir. We start with random connection matrices with a shifted eigenvalue spectrum, which perform very poorly. Next, we transform two specific reservoir types, which are known to give good performance in discrete time, to the continuous time domain. Reservoirs based on uniform spreading of connection matrix eigenvalues on the unit disk in discrete time give much better memory properties than reservoirs with random connection matrices, where reservoirs based on orthogonal connection matrices in discrete time are very robust against noise and their memory properties can be tuned. The overall results found in this work yield important insights into how to design networks for continuous time. ?? 2009 Elsevier Ltd. All rights reserved.},
author = {Hermans, Michiel and Schrauwen, Benjamin},
doi = {10.1016/j.neunet.2009.08.008},
file = {:home/epaxon/Research/Redwood/Kanerva/References/hermans2010.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Continuous time,Linear dynamics,Memory function,Recurrent neural networks,Reservoir computing},
number = {3},
pages = {341--355},
pmid = {19748225},
publisher = {Elsevier Ltd},
title = {{Memory in linear recurrent neural networks in continuous time}},
url = {http://dx.doi.org/10.1016/j.neunet.2009.08.008},
volume = {23},
year = {2010}
}
@article{Chechik2005,
abstract = {The problem of extracting the relevant aspects of data was previously addressed through the information bottleneck (IB) method, through (soft) clustering one variable while preserving information about another-relevance-variable. The current work extends these ideas to obtain continuous representations that preserve relevant information, rather than discrete clusters, for the special case of multivariate Gaussian variables. While the general continuous IB problem is difficult to solve, we provide an analytic solution for the optimal representation and tradeoff between compression and relevance for the this important case. The obtained optimal representation is a noisy linear projection to eigenvectors of the normalized regression matrix Sigma(x vertical bar y)Sigma(-1)(x), which is also the basis obtained in canonical correlation analysis. However, in Gaussian IB, the compression tradeoff parameter uniquely determines the dimension, as well as the scale of each eigenvector, through a cascade of structural phase transitions. This introduces a novel interpretation where solutions of different ranks lie on a continuum parametrized by the compression level. Our analysis also provides a complete analytic expression of the preserved information as a function of the compression (the "information-curve"), in terms of the eigenvalue spectrum of the data. As in the discrete case, the information curve is concave and smooth, though it is made of different analytic segments for each optimal dimension. Finally, we show how the algorithmic theory developed in the IB framework provides an iterative algorithm for obtaining the optimal Gaussian projections.},
author = {Chechik, Gal and Globerson, a and Tishby, N and Weiss, Yair},
file = {:home/epaxon/Research/Redwood/Kanerva/References/Chechik{\_}JMLR2004.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {J Mach Learn Res},
keywords = {Canonical Correlation Analysis,Decoder,Dimensionality Reduction,Gaussian Processes,Information Bottleneck,Network,Side Information},
number = {1},
pages = {165--188},
pmid = {236328800006},
title = {{Information bottleneck for Gaussian variables}},
volume = {6},
year = {2005}
}
@book{James1890,
abstract = {This book provides a foundation to the principles of psychology. It draws upon the natural sciences, avoiding metaphysics, for the basis of its information. According to James, this book, assuming that thoughts and feelings exist and are vehicles of knowledge, thereupon contends that psychology, when it has ascertained the empirical correlation of the various sorts of thought or feeling with definite conditions of the brain, can go no farther as a natural science.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {James, William},
doi = {10.1353/hph.1983.0040},
eprint = {arXiv:1011.1669v3},
file = {:home/epaxon/Research/Redwood/Kanerva/References/The principles of psychology.pdf:pdf},
isbn = {0674705599},
issn = {0022-3018},
pmid = {870},
title = {{The Principles of Psychology}},
year = {1890}
}
@book{Fuster1995,
author = {Fuster, J M},
file = {:home/epaxon/Research/Redwood/Kanerva/References/35965304.pdf:pdf},
title = {{Memory in the Cerebral Cortex: An Empirical Approach to Neural Networks in the Human and Nonhuman Primate.}},
year = {1995}
}
@article{Caianiello1961,
abstract = {Thought-processes and certain typical mental phenomena are schematized into exact mathematical definitions, in terms of a theory which, with the assumption that learning is a relatively slow process, reduces to two sets of equations: “neuronic equations”, with fixed coefficients, which determine the instantaneous behavior, “mnemonic equations”, which determine the long-term behavior of a “model of the brain” or “thinking machine”. A qualitative but rigorous discussion shows that this machine exhibits, as a necessary consequence of the theory, many properties that are typical of the living brain: including need to “sleep”, ability spontaneously to form new ideas (patterns) which associate old ones, self-organization towards more reliable operation, and many others. Future works will deal with the quantitative solution of these equations and with concrete problems of construction—things that appear reasonably feasible. With a transposition of names, this theory could be applied to many sorts of social or, more generally, “collective” problems.},
author = {Caianiello, E.R.},
file = {:home/epaxon/Research/Redwood/Kanerva/References/1-s2.0-0022519361900467-main.pdf:pdf},
journal = {Journal of Theoretical Biology},
pages = {204--235},
title = {{Outline of a Theory of Thought-Processes Machines and Thinking}},
volume = {2},
year = {1961}
}
@article{Little1975,
abstract = {We present a theory of short, intermediate and long term memory of a neural network incorporating the known statistical nature of chemical transmission at the synapses. Correlated pre- and post-synaptic facilitation (related to Hebb's Hypothesis) on three time scales are crucial to the model. Considerable facilitation is needed on a short time scale both for establishing short term memory (active persistent firing pattern for the order of a sec) and the recall of intermediate and long term memory (latent capability for a pattern to be re-excited). Longer lasting residual facilitation and plastic changes (of the same nature as the short term changes) provide the mechanism for imprinting of the intermediate and long term memory. We discuss several interesting features of our theory: nonlocal memory storage, large storage capacity, access of memory, single memory mechanism, robustness of the network and statistical reliability, and usefulness of statistical fluctuations.},
author = {Little, W.A. and Shaw, Gordon L.},
doi = {10.1016/S0091-6773(75)90122-4},
file = {:home/epaxon/Research/Redwood/Kanerva/References/1-s2.0-S0091677375901224-main.pdf:pdf},
isbn = {0091-6773},
issn = {00916773},
journal = {Behavioral Biology},
number = {2},
pages = {115--133},
title = {{A statistical theory of short and long term memory}},
url = {http://www.sciencedirect.com/science/article/pii/S0091677375901224},
volume = {14},
year = {1975}
}
@book{Phaf1994,
author = {Phaf, R. Hans},
file = {:home/epaxon/Research/Redwood/Kanerva/References/10.1007{\_}978-94-011-0840-9.pdf:pdf},
isbn = {9789401043625},
title = {{Learning in Natural and Connectionist Systems}},
year = {1994}
}
@article{Zipser1993,
abstract = {Studies of cortical neurons in monkeys performing short-term memory tasks have shown that information about a stimulus can be maintained by persistent neuron firing for periods of many seconds after removal of the stimulus. The mechanism by which this sustained activity is initiated and maintained is unknown. In this article we present a spiking neural network model of short-term memory and use it to investigate the hypothesis that recurrent, or "re-entrant," networks with constant connection strengths are sufficient to store graded information temporarily. The synaptic weights that enable the network to mimic the input-output characteristics of an active memory module are computed using an optimization procedure for recurrent networks with non-spiking neurons. This network is then transformed into one with spiking neurons by interpreting the continuous output values of the nonspiking model neurons as spiking probabilities. The behavior of the model neurons in this spiking network is compared with that of 179 single units previously recorded in monkey inferotemporal (IT) cortex during the performance of a short-term memory task. The spiking patterns of almost every model neuron are found to resemble closely those of IT neurons. About 40{\%} of the IT neuron firing patterns are also found to be of the same types as those of model neurons. A property of the spiking model is that the neurons cannot maintain precise graded activity levels indefinitely, but eventually relax to one of a few constant activities called fixed-point attractors. The noise introduced into the model by the randomness of spiking causes the network to jump between these attractors. This switching between attractor states generates spike trains with a characteristic statistical temporal structure. We found evidence for the same kind of structure in the spike trains from about half of the IT neurons in our test set. These results show that the behavior of many real cortical memory neurons is consistent with an active storage mechanism based on recurrent activity in networks with fixed synaptic strengths.},
author = {Zipser, D and Kehoe, B and Littlewort, G and Fuster, J},
doi = {10.1162/JOCN_A_00061},
file = {:home/epaxon/Research/Redwood/Kanerva/References/3406.full.pdf:pdf},
isbn = {0270-6474 (Print)},
issn = {0270-6474},
journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
keywords = {memory,neural network,short-term},
number = {8},
pages = {3406--3420},
pmid = {8340815},
title = {{A spiking network model of short-term active memory.}},
volume = {13},
year = {1993}
}
@article{Schwenker1996,
abstract = {We investigate the pattern completion performance of neural auto-associative memories composed of binary threshold neurons for sparsely coded binary memory patterns. By focussing on iterative retrieval, we are able to introduce effective threshold control strategies. These are investigated by means of computer simulation experiments and analytical treatment. To evaluate the systems performance we consider the completion capacity C and the mean retrieval errors. The asymptotic completion capacity values for the recall of sparsely coded binary patterns in one-step retrieval is known to be In 2/4 {\~{}} 17.32{\%} for binary Hebbian learning, and 1/(8 ln 2) {\~{}} 18{\%} for additive Hebbian learning. These values are accomplished with vanishing error probability and yet are higher than those obtained in other known neural memory models. Recent investigations on binary Hebbian learning have proved that iterative retrieval as a more refined retrieval method does not improve the asymptotic completion capacity of one step retrieval. In a finite size auto-associative memory we show that iterative retrieval achieves higher capacity and better error correction than one-step retrieval. One-step retrieval produces high retrieval errors at optimal memory load. Iterative retrieval reduces the retrieval errors within a few iteration steps (t ???5). Experiments with additive Hebbian [earning show that in the finite model, binary Hebbian learning exhibits much better performance. Thus the main concern of this paper is binary Hebbian learning. We examine iterative retrieval in experiments with up to n = 20,000 threshold neurons. With this system size one-step retrieval yield; a completion capacity of about 16{\%}, the second retrieval step increases this value to 17.9{\%} and with iterative retrieval we obtain 19{\%}. The first two retrieval steps in the finite system have also been treated analytically. For one-step retrieval the asymptotic capacity value is approximated from below with growing system size. In the second retrieval step (and as the experiments suggest also for iterative retrieval) the finite size behaviour is different. The capacity exceeds the asymptotic value, reaches an optimum for finite system size, and decreases to the asymptotic limit.},
author = {Schwenker, Friedhelm and Sommer, F. T. and Palm, G.},
doi = {10.1016/0893-6080(95)00112-3},
file = {:home/epaxon/Research/Redwood/Kanerva/References/1-s2.0-0893608095001123-main.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {Hebbian learning,Iterative retrieval,Neural auto-associative memory,Sparse coding,Threshold control},
number = {3},
pages = {445--455},
title = {{Iterative retrieval of sparsely coded associative memory patterns}},
volume = {9},
year = {1996}
}
@article{Sommer1998,
abstract = {It is well known that for finite-sized networks, onestep retrieval in the autoassociative Willshaw net is a suboptimal way to extract the information stored in the synapses. Iterative retrieval strategies are much better, but have hitherto only had heuristic justification. We show how they emerge naturally from considerations of probabilistic inference under conditions of noisy and partial input and a corrupted weight matrix. We start from the conditional probability distribution over possible patterns for retrieval. This contains all possible information that is available to an observer of the network and the initial input. Since this distribution is over exponentially many patterns, we use it to develop two approximate, but tractable, iterative retrieval methods. One performs maximum likelihood inference to find the single most likely pattern, using the (negative log of the) conditional probability as a Lyapunov function for retrieval. In physics terms, if storage errors are present, then the modified iterative update equations contain an additional antiferromagnetic interaction term and site dependent threshold values. The second method makes a mean field assumption to optimize a tractable estimate of the full conditional probability distribution. This leads to iterative mean field equations which can be interpreted in terms of a network of neurons with sigmoidal responses but with the same interactions and thresholds as in the maximum likelihood update equations. In the absence of storage errors, both models become very similiar to the Willshaw model, where standard retrieval is iterated using a particular form of linear threshold strategy.},
author = {Sommer, Friedrich T. and Dayan, Peter},
doi = {10.1109/72.701183},
file = {:home/epaxon/Research/Redwood/Kanerva/References/00701183.pdf:pdf},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Bayesian reasoning,Correlation associative memory,Graded response neurons,Iterative retrieval,Maximum likelihood retrieval,Mean field methods,Storage errors,Threshold strategies,Willshaw model},
number = {4},
pages = {705--713},
pmid = {18252493},
title = {{Bayesian retrieval in associative memories with storage errors}},
volume = {9},
year = {1998}
}
@misc{Hecht-Nielsen1995,
abstract = {Replicator neural networks self-organize by using their inputs as desired outputs; they internally form a compressed representation for the input data. A theorem shows that a class of replicator networks can, through the minimization of mean squared reconstruction error (for instance, by training on raw data examples), carry out optimal data compression for arbitrary data vector sources. Data manifolds, a new general model of data sources, are then introduced and a second theorem shows that, in a practically important limiting case, optimal-compression replicator networks operate by creating an essentially unique natural coordinate system for the manifold.},
author = {Hecht-Nielsen, R},
booktitle = {Science (New York, N.Y.)},
doi = {10.1126/science.269.5232.1860},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hecht-Nielsen - 1995 - Replicator neural networks for universal optimal source coding.pdf:pdf},
isbn = {0036-8075 (Print)$\backslash$r0036-8075 (Linking)},
issn = {0036-8075},
number = {5232},
pages = {1860--1863},
pmid = {17820241},
title = {{Replicator neural networks for universal optimal source coding.}},
volume = {269},
year = {1995}
}
@article{Lin2003,
author = {Lin, Jessica and Gunopulos, D.},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin, Gunopulos - 2003 - Dimensionality reduction by random projection and latent semantic indexing.pdf:pdf},
journal = {proceedings of the Text Mining Workshop, at the 3rd SIAM International Conference on Data Mining},
title = {{Dimensionality reduction by random projection and latent semantic indexing}},
url = {http://www.cs.gmu.edu/{~}jessica/publications/lsi{\_}sdm{\_}workshop03.pdf},
year = {2003}
}
@article{Dasgupta2000,
abstract = {Recent theoretical work has identified random projection as a promising dimensionality reduc- tion technique for learning mixtures of Gaus- sians. Here we summarize these results and il- lustrate them by a wide variety of experiments on synthetic and real data.},
author = {Dasgupta, Sanjoy},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dasgupta - 2000 - Experiments with random projection.pdf:pdf},
isbn = {1-55860-709-9},
journal = {Proceedings of the Sixteenth conference on {\ldots}},
pages = {143--151},
title = {{Experiments with random projection}},
url = {http://yaroslavvb.com/papers/dasgupta-experiments.pdf{\%}5Cnhttp://dl.acm.org/citation.cfm?id=2073964},
year = {2000}
}
@article{Gayler2009,
abstract = {We are concerned with the practical fea- sibility of the neural basis of analogical map- ping. All existing connectionist models of ana- logical mapping rely to some degree on local- ist representation (each concept or relation is represented by a dedicated unit/neuron). These localist solutions are implausible because they need too many units for human-level compe- tence or require the dynamic re-wiring of net- works on a sub-second time-scale. $\backslash$n$\backslash$nAnalogical mapping can be formalised as finding an approximate isomorphism between graphs representing the source and target con- ceptual structures. Connectionist models of analogical mapping implement continuous heuristic processes for finding graph isomor- phisms. We present a novel connectionist mechanism for finding graph isomorphisms that relies on distributed, high-dimensional representations of structure and mappings. Consequently, it does not suffer from the prob- lems of the number of units scaling combinato- rially with the number of concepts or requiring dynamic network re-wiring.},
author = {Gayler, Ross W. and Levy, Simon D.},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gayler, Levy - 2009 - A distributed basis for analogical mapping.pdf:pdf},
journal = {New Frontiers in Analogy Research, Proceedings of the Second International Conference on Analogy, ANALOGY-2009},
pages = {165--174},
title = {{A distributed basis for analogical mapping}},
url = {http://www.nbu.bg/cogs/analogy09/proceedings/18-T15.pdf{\%}5Cnhttp://sites.google.com/site/rgayler/18-T15.pdf?attredirects=0},
year = {2009}
}
@article{HechtNielsen94ContextVectors,
author = {R., Hecht-Nielsen},
journal = {Computational Intelligence: Imitating Life, IEEE Press},
keywords = {imported},
pages = {43--56},
title = {{Context vectors: general purpose approximate meaning representations self-organized from raw data}},
year = {1994}
}
@article{Kaski1998,
abstract = {When the data vectors are high-dimensional it is computationally$\backslash$ninfeasible to use data analysis or pattern recognition algorithms which$\backslash$nrepeatedly compute similarities or distances in the original data space.$\backslash$nIt is therefore necessary to reduce the dimensionality before, for$\backslash$nexample, clustering the data. If the dimensionality is very high, like$\backslash$nin the WEBSOM method which organizes textual document collections on a$\backslash$nself-organizing map, then even the commonly used dimensionality$\backslash$nreduction methods like the principal component analysis may be too$\backslash$ncostly. It is demonstrated that the document classification accuracy$\backslash$nobtained after the dimensionality has been reduced using a random$\backslash$nmapping method will be almost as good as the original accuracy if the$\backslash$nfinal dimensionality is sufficiently large (about 100 out of 6000). In$\backslash$nfact, it can be shown that the inner product (similarity) between the$\backslash$nmapped vectors follows closely the inner product of the original vectors$\backslash$n},
author = {Kaski, S.},
doi = {10.1109/IJCNN.1998.682302},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaski - 1998 - Dimensionality reduction by random mapping fast similarity computation for clustering.pdf:pdf},
isbn = {0-7803-4859-1},
issn = {1098-7576},
journal = {1998 IEEE International Joint Conference on Neural Networks Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98CH36227)},
number = {1},
pages = {413--418},
title = {{Dimensionality reduction by random mapping: fast similarity computation for clustering}},
url = {http://ieeexplore.ieee.org/document/682302/},
volume = {1},
year = {1998}
}
@incollection{Jordan1986a,
author = {Jordan, Michael I.},
booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations},
chapter = {9},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jordan - 1986 - An Introduction to Linear Algebra in Parallel Distributed Processing.pdf:pdf;:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jordan - 1986 - An Introduction to Linear Algebra in Parallel Distributed Processing(2).pdf:pdf},
isbn = {0262631105},
pages = {365--422},
title = {{An Introduction to Linear Algebra in Parallel Distributed Processing}},
year = {1986}
}
@article{Graves2014,
abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
archivePrefix = {arXiv},
arxivId = {1410.5401},
author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
doi = {10.3389/neuro.12.006.2007},
eprint = {1410.5401},
isbn = {0028-0836},
issn = {2041-1723},
pages = {1--26},
pmid = {18958277},
title = {{Neural Turing Machines}},
year = {2014}
}
@article{Salton1975,
abstract = {In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.},
author = {Salton, Gerard and Wong, Anita and Yang, Chung-Shu},
doi = {10.1145/361219.361220},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Salton, Wong, Yang - 1975 - A vector space model for automatic indexing.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {and phrases,automatic indexing,automatic information,content analysis,document,retrieval},
number = {11},
pages = {613--620},
pmid = {15142973},
title = {{A vector space model for automatic indexing}},
volume = {18},
year = {1975}
}
@article{Lee1997,
abstract = {Efficient and effective text retrieval techniques are critical in managing the increasing amount of textual information available in electronic form. Yet text retrieval is a daunting task because it is difficult to extract the semantics of natural language texts. Many problems must be resolved before natural language processing techniques can be effectively applied to a large collection of texts. Most existing text retrieval techniques rely on indexing keywords. Unfortunately, keywords or index terms alone cannot adequately capture the document contents, resulting in poor retrieval performance. Yet keyword indexing is widely used in commercial systems because it is still the most viable way by far to process large amounts of text. Using several simplifications of the vector-space model for text retrieval queries, the authors seek the optimal balance between processing efficiency and retrieval effectiveness as expressed in relevant document rankings},
author = {Lee, Dik L. and Chuang, Huei and Seamons, Kent},
doi = {10.1109/52.582976},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Chuang, Seamons - 1997 - Document ranking and the vector-space model.pdf:pdf},
isbn = {0740-7459},
issn = {07407459},
journal = {IEEE Software},
number = {2},
pages = {67--75},
title = {{Document ranking and the vector-space model}},
volume = {14},
year = {1997}
}
@misc{Robertson1976,
abstract = {Abstract 10.1002/asi.4630270302.abs This paper examines statistical techniques for exploiting relevance information to weight search terms. These techniques are presented as a natural extension of weighting methods using information about the distribution of index terms in documents in general. A series of relevance weighting functions is derived and is justified by theoretical considerations. In particular, it is shown that specific weighted search methods are implied by a general probabilistic theory of retrieval. Different applications of relevance weighting are illustrated by experimental results for test collections.},
author = {Robertson, SE and Jones, KS},
booktitle = {Journal of American Society of Information Science},
doi = {10.1002/asi.4630270302},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Robertson, Jones - 1976 - Relevance weighting of search terms.pdf:pdf},
isbn = {0-947568-21-2},
issn = {1097-4571},
number = {3},
pages = {129--146},
pmid = {3048012021851954321},
title = {{Relevance weighting of search terms}},
volume = {27},
year = {1976}
}
@article{QasemiZadeh2015,
abstract = {Random indexing (RI) is an incremental method for constructing a vector space model (VSM) with a reduced dimensionality. Previously, the method has been justified using the mathematical framework of Kanerva's sparse distributed memory. This justification, although intuitively plausible, fails to provide the information that is required to set the parameters of the method. In order to suggest criteria for the method's parameters, the RI method is revisited and described using the principles of linear algebra and sparse random projections in Euclidean spaces. These simple mathematics are then employed to suggest criteria for setting the method's parameters and to explain their influence on the estimated distances in the RI-constructed VSMs. The empirical results observed in an evaluation are reported to support the suggested guidelines in the paper.},
author = {QasemiZadeh, Behrang and Handschuh, Siegfried},
doi = {10.1007/978-3-319-24033-6_47},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/QasemiZadeh, Handschuh - 2015 - Random indexing explained with high probability.pdf:pdf},
isbn = {9783319240329},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Dimensionality reduction,Random indexing,Text analytics},
pages = {414--423},
publisher = {Springer International Publishing Switzerland},
title = {{Random indexing explained with high probability}},
volume = {9302},
year = {2015}
}
@article{Graves2016,
abstract = {Machine reading using differentiable reasoning models has recently shown remarkable progress. In this context, End-to-End trainable Memory Networks, MemN2N, have demonstrated promising performance on simple natural language based reasoning tasks such as factual reasoning and basic deduction. However, other tasks, namely multi-fact question-answering, positional reasoning or dialog related tasks, remain challenging particularly due to the necessity of more complex interactions between the memory and controller modules composing this family of models. In this paper, we introduce a novel end-to-end memory access regulation mechanism inspired by the current progress on the connection short-cutting principle in the field of computer vision. Concretely, we develop a Gated End-to-End trainable Memory Network architecture, GMemN2N. From the machine learning perspective, this new capability is learned in an end-to-end fashion without the use of any additional supervision signal which is, as far as our knowledge goes, the first of its kind. Our experiments show significant improvements on the most challenging tasks in the 20 bAbI dataset, without the use of any domain knowledge. Then, we show improvements on the dialog bAbI tasks including the real human-bot conversion-based Dialog State Tracking Challenge (DSTC-2) dataset. On these two datasets, our model sets the new state of the art.},
archivePrefix = {arXiv},
arxivId = {1610.04211},
author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'{n}}ska, Agnieszka and Colmenarejo, Sergio G{\'{o}}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
doi = {10.1038/nature20101},
eprint = {1610.04211},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Graves et al. - 2016 - Hybrid computing using a neural network with dynamic external memory.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
month = {oct},
number = {7626},
pages = {471--476},
publisher = {Nature Publishing Group},
title = {{Hybrid computing using a neural network with dynamic external memory}},
volume = {538},
year = {2016}
}
@article{Salton1987,
author = {Salton, Gerard and Buckley, Chris},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Salton, Buckley - 1987 - Term Weighting Approaches in Automatic Text Retrieval.pdf:pdf},
title = {{Term Weighting Approaches in Automatic Text Retrieval}},
year = {1987}
}
@article{Dubin2004,
abstract = {Gerard Salton is often credited with developing the vector space model (VSM) for information retrieval (IR). Citations to Salton give the impression that the VSM must have been articulated as an IR model sometime between 1970 and 1975. However, the VSM as it is understood today evolved over a longer time period than is usually acknowledged, and an articulation of the model and its assumptions did not appear in print until several years after those assumptions had been criticized and alternative models proposed. An often cited overview paper titled “A Vector Space Model for Information Retrieval” (alleged to have been published in 1975) does not exist, and citations to it represent a confusion of two 1975 articles, neither of which were overviews of the VSM as a model of information retrieval. Until the late 1970s, Salton did not present vector spaces as models of IR generally but rather as models of specifi c computations. Citations to the phantom paper refl ect an apparently widely held misconception that the operational features and explanatory devices now associated with the VSM must have been introduced at the same time it was fi rst proposed as an IR model.},
author = {Dubin, David},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dubin - 2004 - The Most Influential Paper Gerard Salton Never Wrote.pdf:pdf},
isbn = {0024-2594},
issn = {0024-2594},
journal = {Library Trends},
number = {4},
pages = {748--764},
title = {{The Most Influential Paper Gerard Salton Never Wrote}},
volume = {52},
year = {2004}
}
@incollection{Rumelhart1986b,
abstract = {What makes people smarter than computers? The work described in these two volumes suggests that the answer lies in the massively parallel architecture of the human mind. It is some of the most exciting work in cognitive science, unifying neural and cognitive processes in a highly computational framework, with links to artificial intelligence. Although thought and problem solving have a sequential character when viewed over a time frame of minutes or hours, the authors argue that each step in the sequence is the result of the simultaneous activity of a large number of simple computational elements, each influencing others and being influenced by them. "Parallel Distributed Processing" describes their work in developing a theoretical framework for describing this parallel distributed processing activity and in applying the framework to the development of models of aspects of perception, memory, language, and thought. Volume 1 lays the theoretical foundations of parallel distributed processing. It introduces the approach and the reasons why the authors feel it is a fruitful one, describes several models of basic mechanisms with wide applicability to different problems, and presents a number of specific technical analyses of different aspects of parallel distributed models.},
author = {Rumelhart, D. E. and Hinton, G. E. and Mcclelland, James L},
booktitle = {Parallel distributed processing: explorations in the microstructure of cognition},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rumelhart, Hinton, Mcclelland - 1986 - A General framework for Parallel Distributed Processing.pdf:pdf},
isbn = {026268053x},
pages = {45 -- 76},
title = {{A General framework for Parallel Distributed Processing}},
year = {1986}
}
@article{Smolensky1986a,
abstract = {Smolensky, P. (1986). Information processing in dynamical systems: Foundations of harmony theory. In Rumelhart, D. E. and McClelland, J. L., editors, Parallel Distributed Processing: Volume 1: Foundations, pages 194-281. MIT Press, Cambridge, MA.},
author = {Smolensky, Paul},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Smolensky - 1986 - Information processing in dynamical systems Foundations of harmony theory.pdf:pdf;:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Smolensky - 1986 - Information processing in dynamical systems Foundations of harmony theory(2).pdf:pdf},
isbn = {026268053X},
journal = {Parallel Distributed Processing Explorations in the Microstructure of Cognition},
number = {1},
pages = {194--281},
title = {{Information processing in dynamical systems: Foundations of harmony theory}},
volume = {1},
year = {1986}
}
@article{Gregoire2006,
abstract = {This paper reports on a new algorithm to detect the presence of a known acoustic signal in an unknown source. The algorithm, map seeking circuits, has been successfully used in the visual domain. The algorithm seeks to find an appropriate transform that will match a stored template to an unknown signal. The algorithm uses superposition to significantly reduce the computational complexity of searching for a given feature in a signal. This results in a linear computational increase rather than an exponential increase as the complexity of the signal increases. The algorithm was tested with a corpus of six instruments. Results varied from 66{\{}{\%}{\}} for the piano to 94{\{}{\%}{\}} for the horn},
author = {Gregoire, B. Jerry G and Maher, Robert C.},
doi = {10.1109/DSPWS.2006.265476},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gregoire, Maher - 2006 - Map seeking circuits A novel method of detecting auditory events using iterative template mapping.pdf:pdf},
isbn = {1-4244-3534-3},
journal = {2006 IEEE 12th Digital Signal Processing Workshop and 4th IEEE Signal Processing Education Workshop},
keywords = {Acoustic,Detection,Map seeking circuits,Template matching},
pages = {511--515},
title = {{Map seeking circuits: A novel method of detecting auditory events using iterative template mapping}},
year = {2006}
}
@inproceedings{Kanerva2015,
abstract = {Today's computers compute with numbers and memory pointers that hardly ever exceed 64 bits. With nan- otechnology we will soon be able to build computers with 10,000-bit words. How would such a computer work and what would it be good for? The paper describes a 10,000- bit architecture that resembles von Neumann's. It has a random-access memory (RAM) for 10,000-bit words, and an arithmetic–logic unit (ALU) for “adding” and “multiplying” 10,000-bit words, in abstract algebra sense. Sets, sequences, lists, and other data structures are encoded “holographically” from their elements using addition and multiplication, and they end up as vectors of the same 10,000-dimensional space, which makes recursive composition possible. The theory of computing with high-dimensional vectors (e.g. with 10,000-bit words) has grown out of attempts to understand the brain's powers of perception and learning in computing terms and is based on the geometry and algebra of high-dimensional spaces, dynamical systems, and the statistical law of large numbers. The architecture is suited for statistical learning from data and is used in cognitive modeling and natural-language processing where it is referred to by names such as Holographic Reduce Representation, Vector Symbolic Architecture, Random Indexing, Semantic Indexing, Semantic Pointer Architecture, and Hyperdimensional Computing.},
author = {Kanerva, Pentti},
booktitle = {2014 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
doi = {10.1109/ALLERTON.2014.7028470},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kanerva - 2014 - Computing with 10,000-bit words.pdf:pdf},
isbn = {978-1-4799-8009-3},
month = {sep},
pages = {304--310},
publisher = {IEEE},
title = {{Computing with 10,000-bit words}},
year = {2014}
}
@article{Williams1989,
abstract = {The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Williams, Ronald J. and Zipser, David},
doi = {10.1162/neco.1989.1.2.270},
eprint = {arXiv:1011.1669v3},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
month = {jun},
number = {2},
pages = {270--280},
pmid = {20505160},
title = {{A Learning Algorithm for Continually Running Fully Recurrent Neural Networks}},
volume = {1},
year = {1989}
}
@article{Shannon1959,
abstract = {A study is made of coding and decoding systems for a continuous channel with an additive gaussian noise and subject to an average power limitation at the transmitter. Upper and lower bounds are found for the error probability in decoding with optimal codes and decoding systems. These bounds are close together for signaling rates near channel capacity and also for signalling rates near zero, but diverge between. Curves exhibiting these bounds are given.},
author = {Shannon, Claude E.},
doi = {10.1002/j.1538-7305.1959.tb03905.x},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shannon - 1959 - Probability of Error for Optimal Codes in a Gaussian Channel.pdf:pdf;:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shannon - 1959 - Probability of Error for Optimal Codes in a Gaussian Channel(2).pdf:pdf},
issn = {15387305},
journal = {Bell System Technical Journal},
number = {3},
pages = {611--656},
title = {{Probability of Error for Optimal Codes in a Gaussian Channel}},
volume = {38},
year = {1959}
}
@article{Arjovsky2015,
abstract = {Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very long-term dependencies.},
archivePrefix = {arXiv},
arxivId = {1511.06464},
author = {Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
eprint = {1511.06464},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arjovsky, Shah, Bengio - 2015 - Unitary Evolution Recurrent Neural Networks.pdf:pdf},
journal = {arXiv},
pages = {1--11},
title = {{Unitary Evolution Recurrent Neural Networks}},
year = {2015}
}
@article{Plate1995,
abstract = {Associative memories are conventionally used to represent data with very simple structure: sets of pairs of vectors. This paper describes a method for representing more complex compositional structure in distributed representations. The method uses circular convolution to associate items, which are represented by vectors. Arbitrary variable bindings, short sequences of various lengths, simple frame-like structures, and reduced representations can be represented in a fixed width vector. These representations are items in their own right and can be used in constructing compositional structures. The noisy reconstructions extracted from convolution memories can be cleaned up by using a separate associative memory that has good reconstructive properties.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Plate, Tony A T.A.},
doi = {10.1109/72.377968},
eprint = {arXiv:1011.1669v3},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Plate - 1995 - Holographic reduced representations.pdf:pdf},
isbn = {9788578110796},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {icle},
month = {may},
number = {3},
pages = {623--641},
pmid = {18263348},
title = {{Holographic reduced representations}},
volume = {6},
year = {1995}
}
@article{Plate1997,
abstract = {Over the last few years a number of schemes for encoding compositional structure in distributed representations have been proposed, e.g., Smolensky's tensor products, Pollack's RAAMs, Plate's HRRs, Halford et al's STAR model, and Kanerva's binary spatter codes. All of these schemes can placed in a general framework involving superposition and binding of patterns. Viewed in this way, it is often simple to decide whether what can be achieved within one scheme will be able to be achieved in...},
author = {Plate, Tony A.},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Plate - 1997 - A common framework for distributed representation schemes for compositional structure.pdf:pdf},
journal = {Connectionist Systems for Knowledge Representations and Deduction},
number = {c},
pages = {15--34},
title = {{A common framework for distributed representation schemes for compositional structure}},
year = {1997}
}
@article{Plate1993,
abstract = {Holograpic Recurrent Networks (HRNs) are recurrent networks which incorporate associative memory techniques for storing sequential structure. HRNs can be easily and quickly trained using gradient descent techniques to generate sequences of discrete outputs and trajectories throught continuous space. The performance of HRNs is found to be superior to that of ordinary recurrent networks on these sequence generation tasks.},
author = {Plate, Tony A.},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Plate - 1993 - Holographic Recurrent Networks.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
number = {1},
pages = {34--41},
title = {{Holographic Recurrent Networks}},
volume = {5},
year = {1993}
}
@article{Siegelmann1995c,
abstract = {This paper deals with finite size networks which consist of inter-connections of synchronously evolving processors. Each processor updates its state by applying a "sigmoidal" function to a linear combination of the previous states of all units. We proves that one may simulate all Turing machines by such nets. In particular, one can simulate any mlti-stack Turing machine in real time, and there is a net made up of 886 processors which computes a universal partial-recursive function.},
author = {Seigelmann, Hava T. and Sontag, Eduardo D.},
doi = {10.1006/jcss.1995.1013},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Seigelmann, Sontag - 1995 - On the Computational Power of Neural Nets.pdf:pdf},
isbn = {089791497X},
issn = {00220000},
journal = {Journal of Computer and System Sciences},
keywords = {nn},
month = {feb},
number = {1},
pages = {132--150},
pmid = {23773339},
title = {{On the Computational Power of Neural Nets}},
volume = {50},
year = {1995}
}
@article{Plate1991,
author = {Plate, Tony A.},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Plate - 1991 - Holographic Reduced Representations Convolution Algebra for Compositional Distributed Representations.pdf:pdf},
journal = {Proceedings of the 12th international joint conference on Artificial intelligence},
pages = {30--35},
pmid = {1000185211},
title = {{Holographic Reduced Representations : Convolution Algebra for Compositional Distributed Representations}},
year = {1991}
}
@article{Saxe2013,
abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
archivePrefix = {arXiv},
arxivId = {1312.6120},
author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
eprint = {1312.6120},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Saxe, McClelland, Ganguli - 2013 - Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.pdf:pdf},
isbn = {1312.6120},
journal = {Advances in Neural Information Processing Systems},
pages = {1--9},
title = {{Exact solutions to the nonlinear dynamics of learning in deep linear neural networks}},
year = {2013}
}
@incollection{Kanerva1993,
abstract = {This chapter describes one basic model of associative memory, called the sparse distributed memory, and relates it to other models and circuits: to ordinary computer memory, to correlation-matrix memories, to feed-forward artificial neural nets, to neural circuits in the brain, and to associative-memory models of the cerebellum. Presenting the various designs within one framework will hopefully help the reader see the similarities and the differences in designs that are often described in different ways.},
author = {Kanerva, Pentti},
booktitle = {Associative Neural Memories: Theory and Implementation},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kanerva - 1993 - Sparse distributed memory and related models.pdf:pdf},
isbn = {0-19-507682-6},
pages = {50--76},
pmid = {2913},
title = {{Sparse distributed memory and related models}},
year = {1993}
}
@inproceedings{Kanerva97,
abstract = {A fully distributed representation based on the binary spatter code is described. It is shown how the information of a conventional record with fields is encoded into a long random bit string, or a holistic record, that has no fields, and how the fields are extracted from the holistic record. It is argued that holistic representation should be used in modeling high-level mental functions.},
author = {Kanerva, Pentti},
booktitle = {Proceedings of 1997 Real World Computing Symposium, RWC'97},
doi = {10.1.1.2.9479},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kanerva - 1997 - Fully Distributed Representation.pdf:pdf},
isbn = {0789731533},
number = {c},
pages = {358--365},
title = {{Fully Distributed Representation}},
year = {1997}
}
@article{Hammer2000,
abstract = {The capability of recurrent neural networks of approximating functions from lists of real vectors to a real vector space is examined. Any measurable function can be approximated in probability. Additionally, bounds on the resources sufficient for an approximation can be derived in interesting cases. On the contrary, there exist computable mappings on symbolic data which cannot be approximated in the maximum norm. For restricted input length, some continuous functions on real-valued sequences need a number of neurons increasing at least linearly in the input length. On unary sequences, any mapping with bounded range can be approximated in the maximum norm. Consequently, standard sigmoidal networks can compute any mapping on offline inputs as a computational model. (C) 2000 Elsevier Science B.V.},
author = {Hammer, Barbara},
doi = {10.1016/S0925-2312(99)00174-5},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hammer - 2000 - On the approximation capability of recurrent neural networks.pdf:pdf},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Computational capability,Recurrent neural networks,Sigmoidal networks,Universal approximation},
month = {mar},
number = {1-4},
pages = {107--123},
title = {{On the approximation capability of recurrent neural networks}},
volume = {31},
year = {2000}
}
@article{Sussillo2009,
abstract = {Neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output. How are these two forms of activity related? We develop a procedure called FORCE learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns. FORCE learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning. Using this approach, we construct networks that produce a wide variety of complex output patterns, input-output transformations that require memory, multiple outputs that can be switched by control inputs, and motor patterns matching human motion capture data. Our results reproduce data on premovement activity in motor and premotor cortex, and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than generally appreciated. {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
author = {Sussillo, David and Abbott, L. F.},
doi = {10.1016/j.neuron.2009.07.018},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sussillo, Abbott - 2009 - Generating coherent patterns of activity from chaotic neural networks.pdf:pdf},
isbn = {0896-6273},
issn = {08966273},
journal = {Neuron},
keywords = {Action Potentials,Action Potentials: physiology,Feedback,Humans,Neural Networks (Computer),Neuronal Plasticity,Neuronal Plasticity: physiology,Nonlinear Dynamics,Physiological,Physiological: physiology,SYSNEURO},
month = {aug},
number = {4},
pages = {544--557},
pmid = {19709635},
publisher = {Elsevier Ltd},
title = {{Generating coherent patterns of activity from chaotic neural networks.}},
volume = {63},
year = {2009}
}
@article{Siegelmann1993,
author = {Siegelmann, Hava T. and Sontag, Eduardo D.},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Siegelmann, Sontag - 1993 - Analog Computation via Neural Networks.pdf:pdf},
journal = {The Second Israel Symposium on Theory of Computing and Systems},
pages = {331--360},
title = {{Analog Computation via Neural Networks}},
volume = {131},
year = {1993}
}
@article{Hopfield1982,
abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
author = {Hopfield, John J.},
doi = {10.1073/pnas.79.8.2554},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hopfield - 1982 - Neural networks and physical systems with emergent collective computational abilities.pdf:pdf},
isbn = {0027-8424},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Animals,Computers,Mathematics,Memory,Models,Neurological,Neurons,Neurons: physiology},
month = {apr},
number = {8},
pages = {2554--2558},
pmid = {6953413},
title = {{Neural networks and physical systems with emergent collective computational abilities.}},
volume = {79},
year = {1982}
}
@article{Joshi2016,
abstract = {Random Indexing is a simple implementation of Random Projections with a wide range of applications. It can solve a variety of problems with good accuracy without introducing much complexity. Here we use it for identifying the language of text samples. We present a novel method of generating language representation vectors using letter blocks. Further, we show that the method is easily implemented and requires little computational power and space. Experiments on a number of model parameters illustrate certain properties about high dimensional sparse vector representations of data. Proof of statistically relevant language vectors are shown through the extremely high success of various language recognition tasks. On a difficult data set of 21,000 short sentences from 21 different languages, our model performs a language recognition task and achieves 97.8{\{}{\%}{\}} accuracy, comparable to state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {1412.7026},
author = {Joshi, Aditya and Halseth, JT Johan JT and Kanerva, Pentti},
eprint = {1412.7026},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Joshi, Halseth, Kanerva - 2016 - Language Geometry using Random Indexing.pdf:pdf},
journal = {submitted 2015},
month = {dec},
pages = {1--7},
title = {{Language Geometry using Random Indexing.}},
year = {2016}
}
@article{Verstraeten2007,
abstract = {Three different uses of a recurrent neural network (RNN) as a reservoir that is not trained but instead read out by a simple external classification layer have been described in the literature: Liquid State Machines (LSMs), Echo State Networks (ESNs) and the Backpropagation Decorrelation (BPDC) learning rule. Individual descriptions of these techniques exist, but a overview is still lacking. Here, we present a series of experimental results that compares all three implementations, and draw conclusions about the relation between a broad range of reservoir parameters and network dynamics, memory, node complexity and performance on a variety of benchmark tests with different characteristics. Next, we introduce a new measure for the reservoir dynamics based on Lyapunov exponents. Unlike previous measures in the literature, this measure is dependent on the dynamics of the reservoir in response to the inputs, and in the cases we tried, it indicates an optimal value for the global scaling of the weight matrix, irrespective of the standard measures. We also describe the Reservoir Computing Toolbox that was used for these experiments, which implements all the types of Reservoir Computing and allows the easy simulation of a wide range of reservoir topologies for a number of benchmarks. ?? 2007 Elsevier Ltd. All rights reserved.},
author = {Verstraeten, D. and Schrauwen, B. and D'Haene, M. and Stroobandt, D.},
doi = {10.1016/j.neunet.2007.04.003},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Verstraeten et al. - 2007 - An experimental unification of reservoir computing methods.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Chaos,Lyapunov exponent,Memory capability,Reservoir computing},
number = {3},
pages = {391--403},
pmid = {17517492},
title = {{An experimental unification of reservoir computing methods}},
volume = {20},
year = {2007}
}
@article{McLaughlin1993,
abstract = {No connectionist model comes close to explaining systematicity, including the models of Elman, Chalmers, and Smolensky. Chalmers' (1990) model exhibits systematic behavior, but that misses the point: it has never been an issue whether connectionist models can exhibit systematic behavior; the problem is that the representations adverted to in connectionist explanation do not themselves {\_}have{\_} syntactic structure. The reason is that psychological explanation requires functional analysis (cf. Cummins 1975, 1983), so explaining behavioral systematicity requires explaining what it consists in.},
author = {McLaughlin, Brian P.},
doi = {10.1007/BF00989855},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McLaughlin - 1993 - The connectionismclassicism battle to win souls.pdf:pdf},
issn = {00318116},
journal = {Philosophical Studies},
number = {2},
pages = {163--190},
title = {{The connectionism/classicism battle to win souls}},
volume = {71},
year = {1993}
}
@book{Papoulis1984,
address = {New York},
author = {Papoulis},
isbn = {0073660116},
publisher = {McGraw-Hill},
title = {{Probability, Random Variables, and Stochastic Processes}},
year = {1984}
}
@article{Rachkovskij2001,
abstract = {The schemes for compositional distributed representations include those allowing on-the-fly construction of fixed dimensionality codevectors to encode structures of various complexity. Similarity of such codevectors takes into account both structural and semantic similarity of represented structures. We provide a comparative description of sparse binary distributed representation developed in the framework of the associative-projective neural network architecture and the more well known holographic reduced representations of T.A. Plate (1995) and binary spatter codes of P. Kanerva (1996). The key procedure in associative-projective neural networks is context-dependent thinning which binds codevectors and maintains their sparseness. The codevectors are stored in structured memory array which can be realized as distributed auto-associative memory. Examples of distributed representation of structured data are given. Fast estimation of the similarity of analogical episodes by the overlap of their codevectors is used in the modeling of analogical reasoning both for retrieval of analogs from memory and for analogical mapping.},
author = {Rachkovskij, D. A.},
doi = {10.1109/69.917565},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rachkovskij - 2001 - Representation and Processing of Structures with Binary Sparse Distributed Codes.pdf:pdf},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Analogical mapping,Analogical retrieval,Analogy,Binary coding,Binding,Compositional distributed representations,Connectionist symbol processing,Hierarchical representation,Long-term memory,Nested representation,Representation of structure,Sparse coding},
number = {2},
pages = {261--276},
title = {{Representation and processing of structures with binary sparse distributed codes}},
volume = {13},
year = {2001}
}
@article{Jaeger2007,
abstract = {Standard echo state networks (ESNs) are built from simple additive units with a sigmoid activation function. Here we investigate ESNs whose reservoir units are leaky integrator units. Units of this type have individual state dynamics, which can be exploited in various ways to accommodate the network to the temporal characteristics of a learning task. We present stability conditions, introduce and investigate a stochastic gradient descent method for the optimization of the global learning parameters (input and output feedback scalings, leaking rate, spectral radius) and demonstrate the usefulness of leaky-integrator ESNs for (i) learning very slow dynamic systems and replaying the learnt system at different speeds, (ii) classifying relatively slow and noisy time series (the Japanese Vowel dataset - here we obtain a zero test error rate), and (iii) recognizing strongly time-warped dynamic patterns. ?? 2007 Elsevier Ltd. All rights reserved.},
author = {Jaeger, Herbert and Luko{\v{s}}evi{\v{c}}ius, Mantas and Popovici, Dan and Siewert, Udo},
doi = {10.1016/j.neunet.2007.04.016},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaeger et al. - 2007 - Optimization and applications of echo state networks with leaky- integrator neurons.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {Pattern generation,Recurrent neural networks,Speaker classification},
number = {3},
pages = {335--352},
pmid = {17517495},
title = {{Optimization and applications of echo state networks with leaky- integrator neurons}},
volume = {20},
year = {2007}
}
@article{Rachkovskij2001a,
abstract = {Distributed representations were often criticized as inappropriate for encoding of data with a complex structure. However Plate's Holographic Reduced Representations and Kanerva's Binary Spatter Codes are recent schemes that allow on-the-fly encoding of nested compositional structures by real-valued or dense binary vectors of fixed dimensionality. In this paper we consider procedures of the Context-Dependent Thinning which were developed for representation of complex hierarchical items in the architecture of Associative-Projective Neural Networks. These procedures provide binding of items represented by sparse binary codevectors (with low probability of 1s). Such an encoding is biologically plausible and allows a high storage capacity of distributed associative memory where the codevectors may be stored. In contrast to known binding procedures, Context-Dependent Thinning preserves the same low density (or sparseness) of the bound codevector for varied number of component codevectors. Besides, a bound codevector is not only similar to another one with similar component codevectors (as in other schemes), but it is also similar to the component codevectors themselves. This allows the similarity of structures to be estimated just by the overlap of their codevectors, without retrieval of the component codevectors. This also allows an easy retrieval of the component codevectors. Examples of algorithmic and neural-network implementations of the thinning procedures are considered. We also present representation examples for various types of nested structured data (propositions using role-filler and predicate-arguments representation schemes, trees, directed acyclic graphs) using sparse codevectors of fixed dimension. Such representations may provide a fruitful alternative to the symbolic representations of traditional AI, as well as to the localist and microfeature-based connectionist representations.},
author = {Rachkovskij, Dmitri A. and Kussul, Ernst M.},
doi = {10.1162/089976601300014592},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rachkovskij, Kussul - 2001 - Binding and Normalization of Binary Sparse Distributed Representations by Context-Dependent Thinning.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
keywords = {binary coding,binding,compositional distributed representations,connectionist symbol processing,distributed representation,nested representation,recursive representation,representation of structure,sparse coding,structured representation,variable binding},
number = {2},
pages = {411--452},
title = {{Binding and Normalization of Binary Sparse Distributed Representations by Context-Dependent Thinning}},
volume = {13},
year = {2001}
}
@article{Verstraeten2007,
abstract = {Three different uses of a recurrent neural network (RNN) as a reservoir that is not trained but instead read out by a simple external classification layer have been described in the literature: Liquid State Machines (LSMs), Echo State Networks (ESNs) and the Backpropagation Decorrelation (BPDC) learning rule. Individual descriptions of these techniques exist, but a overview is still lacking. Here, we present a series of experimental results that compares all three implementations, and draw conclusions about the relation between a broad range of reservoir parameters and network dynamics, memory, node complexity and performance on a variety of benchmark tests with different characteristics. Next, we introduce a new measure for the reservoir dynamics based on Lyapunov exponents. Unlike previous measures in the literature, this measure is dependent on the dynamics of the reservoir in response to the inputs, and in the cases we tried, it indicates an optimal value for the global scaling of the weight matrix, irrespective of the standard measures. We also describe the Reservoir Computing Toolbox that was used for these experiments, which implements all the types of Reservoir Computing and allows the easy simulation of a wide range of reservoir topologies for a number of benchmarks. ?? 2007 Elsevier Ltd. All rights reserved.},
author = {Verstraeten, D. and Schrauwen, B. and D'Haene, M. and Stroobandt, D.},
doi = {10.1016/j.neunet.2007.04.003},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Chaos,Lyapunov exponent,Memory capability,Reservoir computing},
month = {apr},
number = {3},
pages = {391--403},
pmid = {17517492},
title = {{An experimental unification of reservoir computing methods}},
volume = {20},
year = {2007}
}
@article{Kleyko2014,
abstract = {The contribution of this article is twofold. First, it presents an encoding approach for seamless bidirectional transitions between localist and distributed representation domains. Second, the approach is demonstrated on the example of using Vector Symbolic Architecture for solving a problem of finding common substrings. The proposed algorithm uses elementary operations on long binary vectors. For the case of two patterns with respective lengths L1 and L2 it requires ??(L1 + L2 - 1) operations on binary vectors, which is equal to the suffix trees approach - the fastest algorithm for this problem. The simulation results show that in order to be robustly detected by the proposed approach the length of a common substring should be more than 4{\%} of the longest pattern.},
author = {Kleyko, Denis and Osipov, Evgeny},
doi = {10.1016/j.procs.2014.11.091},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kleyko, Osipov - 2014 - On Bidirectional Transitions between Localist and Distributed Representations The Case of Common Substrings Sear.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Distributed data representation,The longest common substring problem,VSA},
number = {C},
pages = {104--113},
publisher = {Elsevier Masson SAS},
title = {{On Bidirectional Transitions between Localist and Distributed Representations: The Case of Common Substrings Search Using Vector Symbolic Architecture}},
volume = {41},
year = {2014}
}
@article{Rinkus2012,
abstract = {Quantum superposition states that any physical system simultaneously exists in all of its possible states, the number of which is exponential in the number of entities composing the system. The strength of presence of each possible state in the superposition—i.e., the probability with which it would be observed if measured—is represented by its probability amplitude coefficient. The assumption that these coefficients must be represented physically disjointly from each other, i.e., localistically, is nearly universal in the quantum theory/computing literature. Alternatively, these coefficients can be represented using sparse distributed representations (SDR), wherein each coefficient is represented by a small subset of an overall population of representational units and the subsets can overlap. Specifically, I consider an SDR model in which the overall population consists of Q clusters, each having K binary units, so that each coefficient is represented by a set of Q units, one per cluster. Thus, KQ coefficients can be represented with KQ units. We can then consider the particular world state, X, whose coefficient's representation, R(X), is the set of Q units active at time t to have the maximal probability and the probabilities of all other states, Y, to correspond to the size of the intersection of R(Y) and R(X). Thus, R(X) simultaneously serves both as the representation of the particular state, X, and as a probability distribution over all states. Thus, set intersection may be used to classically implement quantum superposition. If algorithms exist for which the time it takes to store (learn) new representations and to find the closest-matching stored representation (probabilistic inference) remains constant as additional representations are stored, this would meet the criterion of quantum computing. Such algorithms, based on SDR, have already been described. They achieve this "quantum speed-up" with no new esoteric technology, and in fact, on a single-processor, classical (Von Neumann) computer.},
author = {Rinkus, Gerard J.},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rinkus - 2012 - Quantum computation via sparse distributed representation.pdf:pdf},
isbn = {1303-5150},
issn = {13035150},
journal = {NeuroQuantology},
keywords = {Localist,Probability amplitude,Quantum computing,Sparse distributed representations,Superposition},
number = {2},
pages = {311--315},
title = {{Quantum computation via sparse distributed representation}},
volume = {10},
year = {2012}
}
@article{Lukosevicius2009,
abstract = {Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current "brand-names" of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed "map" of it. ?? 2009 Elsevier Inc. All rights reserved.},
author = {Luko{\v{s}}evi{\v{c}}ius, Mantas and Jaeger, Herbert},
doi = {10.1016/j.cosrev.2009.03.005},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luko{\v{s}}evi{\v{c}}ius, Jaeger - 2009 - Reservoir computing approaches to recurrent neural network training.pdf:pdf},
isbn = {1574-0137},
issn = {15740137},
journal = {Computer Science Review},
month = {aug},
number = {3},
pages = {127--149},
pmid = {16939104},
title = {{Reservoir computing approaches to recurrent neural network training}},
volume = {3},
year = {2009}
}
@article{Williams1989cs,
abstract = {Abstract The real-time recurrent learning algorithm is a gradient-following learning algorithm for completely recurrent networks running in continually sampled time. Here we use a series of simulation experiments to investigate the power and properties of this algorithm. In the recurrent networks studied here, any unit can be connected to any other, and any unit can receive external input. These networks run continually in the sense that they sample their inputs on every update cycle, and any unit can have a training target on any cycle. The storage required and computation time on each step are independent of time and are completely determined by the size of the network, so no prior knowledge of the temporal structure of the task being learned is required. The algorithm is nonlocal in the sense that each unit must have knowledge of the complete recurrent weight matrix and error vector. The algorithm is computationally intensive in sequential computers, requiring a storage capacity of the order of the third power of the number of units and a computation time on each cycle of the order of the fourth power of the number of units. The simulations include examples in which networks are taught tasks not possible with tapped delay lines?that is, tasks that require the preservation of state over potentially unbounded periods of time. The most complex example of this kind is learning to emulate a Turing machine that does a parenthesis balancing problem. Examples are also given of networks that do feedforward computations with unknown delays, requiring them to organize into networks with the correct number of layers. Finally, examples are given in which networks are trained to oscillate in various ways, including sinusoidal oscillation. The real-time recurrent learning algorithm is a gradient-following learning algorithm for completely recurrent networks running in continually sampled time. Here we use a series of simulation experiments to investigate the power and properties of this algorithm. In the recurrent networks studied here, any unit can be connected to any other, and any unit can receive external input. These networks run continually in the sense that they sample their inputs on every update cycle, and any unit can have a training target on any cycle. The storage required and computation time on each step are independent of time and are completely determined by the size of the network, so no prior knowledge of the temporal structure of the task being learned is required. The algorithm is nonlocal in the sense that each unit must have knowledge of the complete recurrent weight matrix and error vector. The algorithm is computationally intensive in sequential computers, requiring a storage capacity of the order of the third power of the number of units and a computation time on each cycle of the order of the fourth power of the number of units. The simulations include examples in which networks are taught tasks not possible with tapped delay lines?that is, tasks that require the preservation of state over potentially unbounded periods of time. The most complex example of this kind is learning to emulate a Turing machine that does a parenthesis balancing problem. Examples are also given of networks that do feedforward computations with unknown delays, requiring them to organize into networks with the correct number of layers. Finally, examples are given in which networks are trained to oscillate in various ways, including sinusoidal oscillation.},
author = {Williams, Ronald J. and Zipser, David},
doi = {10.1080/09540098908915631},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Williams, Zipser - 1989 - Experimental Analysis of the Real-time Recurrent Learning Algorithm.pdf:pdf},
isbn = {0954-0091},
issn = {0954-0091},
journal = {Connection Science},
month = {jan},
number = {1},
pages = {87--111},
title = {{Experimental Analysis of the Real-time Recurrent Learning Algorithm}},
volume = {1},
year = {1989}
}
@article{Amit1995,
abstract = {The neurophysiological evidence from the Miyashita group s experiments on monkeys as well as cognitive experience common to us all suggests that local neuronal spike rate distributions might persist in the absence of their eliciting stimulus. In Hebb's cell-assembly theory, learning dynamics stabilize such self-maintaining reverberations. Quasi-quantitive modeling of the experimental data on internal representations in association-cortex modules identifies the reverberations (delay spike activity) as the internal code (representation). This leads to cognitive and neurophysiological predictions, many following directly from the language used to describe the activity in the experimental delay period, others from the details of how the model captures the properties of the internal representations. entao concluiremos que uma palavra, quando dita, dura mais que o som e os sons que a formaram, fica por ai, invisivel e inaudivel para poder guardar o seu proprio segredo . . . Jos{\pounds} Saraniago, A Jangada de Pedra 1988 We may conclude that a word, once said, persists longer than the vibration or the sound that formed it, it stays there, invisible and inaudible so as to preserve its very own se-cret . . . The Stone Raft 1. Introspective attractors Neural network models of brain function can be roughly classified as feed-forward and feed-back or attractor net-works. Very clear and informative evidence of the presence of attractors in temporal cortex of primates is presented in the recent studies of Miyashita and colleagues (Miyashita 1988; Miyashita {\&} Chang 1988; Sakai {\&} Miyashita 1991; see also Amit 1993). Here I will suggest that most of the results observed in these seminal experiments could be predicted on the basis of simple observations concerning common cognitive phenomena, without recourse to any specific model. One can then argue that such considerations proba-bly underlay Hebb's (1949) postulation of synaptic dy-namics as a means of stabilizing reverberations in neural assemblies. Such a reverbatory mechanism provides a po-tentially useful way of dissecting the complexity of the connection between sensory input and motor reaction. Consider the familiar every-day situation in which one is given the task of translating a word from one language to another. The word is known in both languages, that is, both words are in memory. Suppose also that the task is commu-nicated verbally (spoken). Both the task and the word in the first language are well understood. The acoustic stimulus disappears as soon as it has been presented. The response, the word in the second language, (1) might be produced very rapidly and correctly, or (equally commonly) (2) the first word might be recognized, together with a strong sense that the translated word is known, but the retrieval of the corresponding word might not be possible. One knows that one knows, yet one does not know. One can persist with the effort to retrieve the translated word for quite a while despite the absence of the eliciting stimulus. And not uncommon is a sequel in which the entire episode (word + task) fades away from our consciousness, only to surface resolved hours or days later. During this long search period, conscious or uncon-scious, the originally presented word must have been avail-able in a sense that goes beyond the fact that it was contained in our memory, that is, we knew it. After all, there are very many words we know in the language of the word presented for translation. What is special about this particu-lar word, of course, is that it has been tagged by the stimulus -the sound of the spoken word. This naive resolution of the problem posed by the famil-iar cognitive situation has quite significant implications. Somewhere in the skull, between the locus of the fully preprocessed stimulus and before the beginning of the generation of a response, there must be loci storing, pas-sively, many memories. Those are the things we know and remember. They are, most likely, stored in the synaptic structure of each locus. Each of these loci must be able to maintain one memory out of the passive stock (the one tagged by the stimulus) in a special status, for relatively long times, and in a status which will make it available for future attempts to perform the task.},
author = {Amit, Daniel J.},
doi = {10.1017/S0140525X0004022X},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Amit - 1995 - The Hebbian paradigm reintegrated Local reverberations as internal representations.pdf:pdf},
isbn = {1469-1825},
issn = {0140-525X},
journal = {Behavioral and Brain Sciences},
keywords = {active memory,associative cortex,attractor dynamics,content sensitivity,internal representations,learning,modeling},
month = {dec},
number = {04},
pages = {617},
title = {{The Hebbian paradigm reintegrated: Local reverberations as internal representations}},
volume = {18},
year = {1995}
}
@article{Lehky2014,
abstract = {We have calculated the intrinsic dimensionality of visual object representations in anterior inferotemporal (AIT) cortex, based on responses of a large sample of cells stimulated with photographs of diverse objects. As dimensionality was dependent on data set size, we determined asymptotic dimensionality as both the number of neurons and number of stimulus image approached infinity. Our final dimensionality estimate was 93 (SD: ± 11), indicating that there is basis set of approximately a hundred independent features that characterize the dimensions of neural object space. We believe this is the first estimate of the dimensionality of neural visual representations based on single-cell neurophysiological data. The dimensionality of AIT object representations was much lower than the dimensionality of the stimuli. We suggest that there may be a gradual reduction in the dimensionality of object representations in neural populations going from retina to inferotemporal cortex, as receptive fields become increasingly complex. Introduction},
archivePrefix = {arXiv},
arxivId = {1309.2848v1},
author = {Lehky, Sidney R. and Kiani, Roozbeh and Esteky, Hossein and Tanaka, Keiji},
doi = {10.1162/NECO},
eprint = {1309.2848v1},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lehky et al. - 2014 - Dimensionality of object representations in monkey inferotemporal cortex.pdf:pdf},
isbn = {0899-7667},
issn = {1530888X},
journal = {Neural computation},
number = {10},
pages = {1840--1872},
pmid = {25602775},
title = {{Dimensionality of object representations in monkey inferotemporal cortex}},
volume = {1872},
year = {2014}
}
@article{Amit1994,
abstract = {We discuss the long term maintenance of acquired memory in synaptic connections of a perpetually learning electronic device. This is affected by ascribing each synapse a finite number of stable states in which it can maintain for indefinitely long periods. Learning uncorrelated stimuli is expressed as a stochastic process produced by the neural activities on the synapses. In several interesting cases the stochastic process can be analyzed in detail, leading to a clarification of the performance of the network, as an associative memory, during the process of uninterrupted learning. The stochastic nature of the process and the existence of an asymptotic distribution for the synaptic values in the network imply generically that the memory is a palimpsest but capacity is as low as log N for a network of N neurons. The only way we find for avoiding this tight constraint is to allow the parameters governing the learning process (the coding level of the stimuli; the transition probabilities for potentiation and depression and the number of stable synaptic levels) to depend on the number of neurons. It is shown that a network with synapses that have two stable states can dynamically learn with optimal storage efficiency, be a palimpsest, and maintain its (associative) memory for an indefinitely long time provided the coding level is low and depression is equilibrated against potentiation. We suggest that an option so easily implementable in material devices would not have been overlooked by biology. Finally we discuss the stochastic learning on synapses with variable number of stable synaptic states.},
author = {Amit, Daniel J. and Fusi, Stefano},
doi = {10.1162/neco.1994.6.5.957},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Amit, Fusi - 1994 - Learning in Neural Networks with Material Synapses.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
number = {5},
pages = {957--982},
title = {{Learning in Neural Networks with Material Synapses}},
volume = {6},
year = {1994}
}
@article{Gardner1987,
abstract = {The upper storage capacity of a neural network for patterns of fixed magnetization m is calculated. The optimal capacity increases with the correlation m{\^{}}2 between the patterns.},
author = {Gardner, E.},
doi = {10.1209/0295-5075/4/4/016},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gardner - 1987 - Maximum Storage Capacity in Neural Networks.pdf:pdf},
issn = {0295-5075},
journal = {Europhys. Lett.},
number = {4},
pages = {481--485},
pmid = {1000231972},
title = {{Maximum Storage Capacity in Neural Networks}},
volume = {4},
year = {1987}
}
@article{Ganguli2010a,
abstract = {Compressed sensing (CS) is an important recent advance that shows how to reconstruct sparse high dimensional signals from surprisingly small numbers of random measurements. The nonlinear nature of the reconstruction process poses a challenge to understanding the performance of CS. We employ techniques from the statistical physics of disordered systems to compute the typical behavior of CS as a function of the signal sparsity and measurement density. We find surprising and useful regularities in the nature of errors made by CS, a new phase transition which reveals the possibility of CS for nonnegative signals without optimization, and a new null model for sparse regression.},
author = {Ganguli, Surya and Sompolinsky, Haim},
doi = {10.1103/PhysRevLett.104.188701},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ganguli, Sompolinsky - 2010 - Statistical mechanics of compressed sensing.pdf:pdf},
isbn = {1079-7114 (Electronic) 0031-9007 (Linking)},
issn = {00319007},
journal = {Physical Review Letters},
number = {18},
pages = {1--4},
pmid = {20482215},
title = {{Statistical mechanics of compressed sensing}},
volume = {104},
year = {2010}
}
@article{Hornik1989,
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. ?? 1989.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
doi = {10.1016/0893-6080(89)90020-8},
eprint = {arXiv:1011.1669v3},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hornik, Stinchcombe, White - 1989 - Multilayer feedforward networks are universal approximators.pdf:pdf},
isbn = {08936080 (ISSN)},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
month = {jan},
number = {5},
pages = {359--366},
pmid = {74},
title = {{Multilayer feedforward networks are universal approximators}},
volume = {2},
year = {1989}
}
@article{Debao1993,
abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
author = {Cybenko, G.},
doi = {10.1007/BF02551274},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cybenko - 1989 - Approximation by superpositions of a sigmoidal function.pdf:pdf},
isbn = {0780300564},
issn = {0932-4194},
journal = {Mathematics of Control, Signals, and Systems},
keywords = {approximation,completeness,neural networks},
month = {dec},
number = {4},
pages = {303--314},
title = {{Approximation by superpositions of a sigmoidal function}},
volume = {2},
year = {1989}
}
@article{Danihelka2016,
abstract = {We investigate a new method to augment recurrent neural networks with extra memory without increasing the number of network parameters. The system has an associative memory based on complex-valued vectors and is closely related to Holographic Reduced Representations and Long Short-Term Memory networks. Holographic Reduced Representations have limited capacity: as they store more information, each retrieval becomes noisier due to interference. Our system in contrast creates redundant copies of stored information, which enables retrieval with reduced noise. Experiments demonstrate faster learning on multiple memorization tasks.},
archivePrefix = {arXiv},
arxivId = {1602.03032},
author = {Danihelka, Ivo and Wayne, Greg and Uria, Benigno and Kalchbrenner, Nal and Graves, Alex},
eprint = {1602.03032},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Danihelka et al. - 2016 - Associative Long Short-Term Memory.pdf:pdf},
journal = {arXiv},
title = {{Associative Long Short-Term Memory}},
year = {2016}
}
@article{D.Keeler1987,
abstract = {The information capacity is investigated for two classes of neural network models using the outer-product (hebbian) storage rule. A generalization of Hopfield-type models using higher-order interactions is analyzed, as well as a similar generalization of a three-layer network that uses hebbian learning among the second and third layer. It is shown that the total information stored in these systems is a constant times the number connections in the network, independent of the particular model, the order of the model, or whether clipped weights are used or not. ?? 1987.},
author = {{D. Keeler}, James},
doi = {10.1016/0375-9601(87)90371-9},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/D. Keeler - 1987 - Information capacity of outer-product neural networks.pdf:pdf},
issn = {03759601},
journal = {Physics Letters A},
number = {1-2},
pages = {53--58},
title = {{Information capacity of outer-product neural networks}},
volume = {124},
year = {1987}
}
@article{Denning1986,
author = {Denning, Peter J.},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Denning - 1986 - A view of Kanerva's Sparse Distributed Memory.pdf:pdf},
journal = {RIACS Technical Report},
number = {14},
title = {{A view of Kanerva's Sparse Distributed Memory}},
volume = {86},
year = {1986}
}
@article{Balc??zar1997,
author = {Balcazar, J.L. L and Gavalda, R. and Siegelmann, H.T. T},
doi = {10.1109/18.605580},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Balcazar, Gavalda, Siegelmann - 1997 - Computational power of neural networks a characterization in terms of Kolmogorov complexity.pdf:pdf},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Kolmogorov complexity,Neural networks,Turing machines},
month = {jul},
number = {4},
pages = {1175--1183},
title = {{Computational power of neural networks: a characterization in terms of Kolmogorov complexity}},
volume = {43},
year = {1997}
}
@article{Knoblauch2010,
abstract = {Neural associative networks with plastic synapses have been proposed as computational models of brain functions and also for applications such as pattern recognition and information retrieval. To guide biological models and optimize technical applications, several definitions of memory capacity have been used to measure the efficiency of associative memory. Here we explain why the currently used performance measures bias the comparison between models and cannot serve as a theoretical benchmark. We introduce fair measures for information-theoretic capacity in associative memory that also provide a theoretical benchmark. In neural networks, two types of manipulating synapses can be discerned: synaptic plasticity, the change in strength of existing synapses, and structural plasticity, the creation and pruning of synapses. One of the new types of memory capacity we introduce permits quantifying how structural plasticity can increase the network efficiency by compressing the network structure, for example, by pruning unused synapses. Specifically, we analyze operating regimes in the Willshaw model in which structural plasticity can compress the network structure and push performance to the theoretical benchmark. The amount C of information stored in each synapse can scale with the logarithm of the network size rather than being constant, as in classical Willshaw and Hopfield nets ({\textless} or = ln 2 approximately 0.7). Further, the review contains novel technical material: a capacity analysis of the Willshaw model that rigorously controls for the level of retrieval quality, an analysis for memories with a nonconstant number of active units (where C {\textless} or = 1/e ln 2 approximately 0.53), and the analysis of the computational complexity of associative memories with and without network compression.},
author = {Knoblauch, Andreas and Palm, G{\"{u}}nther and Sommer, Friedrich T},
doi = {10.1162/neco.2009.08-07-588},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Knoblauch, Palm, Sommer - 2010 - Memory capacities for synaptic and structural plasticity.pdf:pdf},
isbn = {1530-888X (Electronic){\$}\backslash{\$}r0899-7667 (Linking)},
issn = {0899-7667},
journal = {Neural computation},
number = {2},
pages = {289--341},
pmid = {19925281},
title = {{Memory capacities for synaptic and structural plasticity.}},
volume = {22},
year = {2010}
}
@article{Jaeger2002a,
abstract = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.},
author = {Jaeger, H},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaeger - 2002 - Short term memory in echo state networks.pdf:pdf},
journal = {GMD Report 152},
title = {{Short term memory in echo state networks}},
year = {2002}
}
@article{Beaufays2014,
abstract = {Long Short-Term Memory (LSTM) is a specific recurrent neu- ral network (RNN) architecture thatwas designed to model tem- poral sequences and their long-range dependencies more accu- rately than conventional RNNs. In this paper, we exploreLSTM RNN architectures for large scale acoustic modeling in speech recognition. We recently showed that LSTM RNNs are more effective than DNNs and conventional RNNs for acoustic mod- eling, considering moderately-sized models trained on a single machine. Here, we introduce the first distributed training of LSTM RNNs using asynchronous stochastic gradient descent optimization on a large cluster of machines. We show that a two-layer deep LSTM RNN where each LSTM layer has a lin- ear recurrent projection layer can exceed state-of-the-art speech recognition performance. This architecture makes more effec- tive use of model parameters than the others considered, con- verges quickly, and outperforms a deep feed forward neural net- work having an order of magnitude more parameters. Index Terms: Long Short-Term Memory, LSTM, recurrent neural network, RNN, speech recognition, acoustic modeling},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.1128v1},
author = {Beaufays, Francoise and Sak, Hasim and Senior, Andrew},
eprint = {arXiv:1402.1128v1},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Beaufays, Sak, Senior - 2014 - Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling Has.pdf:pdf},
journal = {Interspeech},
number = {September},
pages = {338--342},
title = {{Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling Has}},
year = {2014}
}
@article{Le2015,
abstract = {Learning long term dependencies in recurrent networks is difficult due to van-ishing and exploding gradients. To overcome this difficulty, researchers have de-veloped sophisticated optimization techniques and network architectures. In this paper, we propose a simpler solution that use recurrent neural networks composed of rectified linear units. Key to our solution is the use of the identity matrix or its scaled version to initialize the recurrent weight matrix. We find that our solution is comparable to a standard implementation of LSTMs on our four benchmarks: two toy problems involving long-range temporal structures, a large language modeling problem and a benchmark speech recognition problem.},
archivePrefix = {arXiv},
arxivId = {arXiv:1504.00941v1},
author = {Le, Quoc V and Jaitly, Navdeep and Hinton, Geoffrey E},
doi = {10.1109/72.279181},
eprint = {arXiv:1504.00941v1},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Jaitly, Hinton - 2015 - A Simple Way to Initialize Recurrent Networks of Rectified Linear Units.pdf:pdf},
isbn = {9781461268758},
issn = {1045-9227},
journal = {arXiv},
pages = {1--9},
pmid = {17756722},
title = {{A Simple Way to Initialize Recurrent Networks of Rectified Linear Units}},
year = {2015}
}
@incollection{Plate2003a,
author = {Plate, Tony},
booktitle = {Encyclopedia of Cognitive Science},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Plate - 2003 - Distributed representations.pdf:pdf},
number = {59},
pages = {1--15},
title = {{Distributed representations}},
year = {2003}
}
@inproceedings{Kanerva1986,
author = {Kanerva, Pentti},
booktitle = {AIP Conference Proceedings},
doi = {10.1063/1.36276},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kanerva - 1986 - Parallel structures in human and computer memory.pdf:pdf},
issn = {0094243X},
keywords = {NASA},
number = {December},
pages = {247--258},
publisher = {AIP},
title = {{Parallel structures in human and computer memory}},
volume = {151},
year = {1986}
}
@article{Murdock1991,
abstract = {Sparse Distributed Memory was proposed by Pentti Kanerva as a model of human long term memory. He presented it as an architecture that could store large patterns and retrieve them based on partial matches with current sensory inputs. The architecture can be realized as a neural net or as an associative memory. SDM exhibits behaviors, both in theory and in experiment, that resemble those previously unapproachable by machines -- e.g., rapid recognition of faces or odors, discovery of new connections between seemingly unrelated ideas, continuation of a sequence of events when given a cue from the middle, knowing that one doesn't know, or getting stuck with an answer on the tip of one's tongue. These behaviors are now within reach of machines that can be incorporated into the computing systems of robots capable of seeing, talking, and manipulating. Kanerva's theory is a new interpretation of learning and cognition that respects biology and the mysteries of individual human beings},
author = {Murdock, Bennet},
doi = {10.1016/0001-6918(91)90056-6},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Murdock - 1991 - Sparse distributed memory.pdf:pdf},
isbn = {0262111322},
issn = {00016918},
journal = {Acta Psychologica},
month = {feb},
number = {1},
pages = {92--94},
pmid = {17526333},
title = {{Sparse distributed memory}},
volume = {76},
year = {1991}
}
@article{Pascanu2012,
abstract = {There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and the exploding gradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pascanu, Mikolov, Bengio - 2012 - On the difficulty of training recurrent neural networks.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {1045-9227},
journal = {Proceedings of The 30th International Conference on Machine Learning},
number = {2},
pages = {1310--1318},
pmid = {18267787},
title = {{On the difficulty of training recurrent neural networks}},
year = {2012}
}
@article{Siegelmann1996,
abstract = {This paper reasons about the need to seek for particular kinds of models of computation that imply stronger computability than the classical models. A possible such model, constituting a chaotic dynamical system, is presented. This system, which we term as the analog shift map, when viewed as a computational model has super-Turing power and is equivalent to neural networks and the class of analog machines. This map may be appropriate to describe idealized physical phenomena.},
author = {Siegelmann, Hava T.},
doi = {10.1016/S0304-3975(96)00087-4},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Siegelmann - 1996 - The simple dynamics of super Turing theories.pdf:pdf},
issn = {03043975},
journal = {Theoretical Computer Science},
month = {nov},
number = {2},
pages = {461--472},
title = {{The simple dynamics of super Turing theories}},
volume = {168},
year = {1996}
}
@article{Siegelmann1995b,
abstract = {Extensive efforts have been made to prove the Church-Turing thesis, which suggests that all realizable dynamical and physical systems cannot be more powerful than classical models of computation. A simply described but highly chaotic dynamical system called the analog shift map is presented here, which has computational power beyond the Turing limit (super-Turing); it computes exactly like neural networks and analog machines. This dynamical system is conjectured to describe natural physical phenomena.},
author = {Siegelmann, Hava T.},
doi = {10.1126/science.268.5210.545},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Siegelmann - 1995 - Computation Beyond the Turing Limit.pdf:pdf},
isbn = {9781461268758},
issn = {0036-8075},
journal = {Science},
month = {apr},
number = {5210},
pages = {545--548},
pmid = {17756722},
title = {{Computation Beyond the Turing Limit}},
volume = {268},
year = {1995}
}
@inproceedings{Jaeger2002b,
abstract = {Echo state networks ESN are a novel approach to recurrent neural network training. An ESN consists of a large, fixed, recurrent "reservoir" network, from which the desired output is obtained by training suitable output connection weights. Determination of optimal output weights becomes a linear, uniquely solvable task of MSE minimization. This article reviews the basic ideas and describes an online adaptation scheme based on the RLS algorithm known from adaptive linear systems. As an example, a 10-th order NARMA system is adaptively identified. The known benefits of the RLS algorithms carry over from linear systems to nonlinear ones; specifically, the convergence rate and misadjustment can be determined at design time.},
author = {Jaeger, Herbert},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaeger - 2002 - Adaptive Nonlinear System Identification with Echo State Networks.pdf:pdf},
isbn = {0262025507},
issn = {10495258},
pages = {593--600},
title = {{Adaptive Nonlinear System Identification with Echo State Networks}},
year = {2002}
}
@article{Keeler1988a,
author = {Keeler, J.D. D},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Keeler - 1988 - Capacity for patterns and sequences in Kanerva's SDM as compared to other associative memory models.pdf:pdf},
isbn = {0-88318-569-5},
journal = {Neural information processing systems: Denver, CO, 1987},
pages = {412},
title = {{Capacity for patterns and sequences in Kanerva's SDM as compared to other associative memory models}},
year = {1988}
}
@article{Hochreiter1997,
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hochreiter, Schmidhuber - 1997 - Long Short-Term Memory.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
month = {nov},
number = {8},
pages = {1735--1780},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@article{Ganguli2008,
abstract = {E-mail: {\{}{\{}{\}}at{\{}{\}}{\}}phy.ucsf.edu; References. ↵: Lowenstein Y,; H. (2003) Temporal integration by calcium dynamics in a model neuron.},
author = {Ganguli, Surya and Huh, B. D. and Sompolinsky, Haim},
doi = {10.1073/pnas.0804451105},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ganguli, Huh, Sompolinsky - 2008 - Memory traces in dynamical systems.pdf:pdf},
isbn = {1091-6490},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences},
number = {48},
pages = {18970--18975},
pmid = {19020074},
title = {{Memory traces in dynamical systems}},
volume = {105},
year = {2008}
}
@article{Lundqvist2016,
author = {Lundqvist, Mikael and Rose, Jonas and Herman, Pawel and Brincat, Scott L. L and Buschman, Timothy J. J and Miller, Earl K. K},
doi = {10.1016/j.neuron.2016.02.028},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lundqvist et al. - 2016 - Gamma and Beta Bursts Underlie Working Memory.pdf:pdf},
issn = {08966273},
journal = {Neuron},
pages = {1--13},
pmid = {26996084},
publisher = {Elsevier Inc.},
title = {{Gamma and Beta Bursts Underlie Working Memory}},
year = {2016}
}
@article{Bengio1994,
abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio, Simard, Frasconi - 1994 - Learning Long-Term Dependencies with Gradient Descent is Difficult.pdf:pdf},
isbn = {1045-9227 VO - 5},
issn = {19410093},
journal = {IEEE Transactions on Neural Networks},
number = {2},
pages = {157--166},
pmid = {18267787},
title = {{Learning Long-Term Dependencies with Gradient Descent is Difficult}},
volume = {5},
year = {1994}
}
@article{Kanerva1996,
abstract = {Information with structure is traditionally organized into records with fields. For example, a medical record consisting of name, sex, age, and weight might look like (Joe, male, 66, 77). What 77 stands for is determined by its location in the record, so that this is an example of local representation. The brain's wiring, and robustness under local damage, speak for the importance of distributed representations. The Holographic Reduced Representation (HRR) of Plate is a prime example based on real or complex vectors. This paper describes how spatter coding leads to binary HRRs, and how the fields of a record are encoded into a long binary word without fields and how they are extracted from such a word.},
author = {Kanerva, Pentti},
doi = {10.1007/3-540-61510-5_146},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kanerva - 1996 - Binary spatter-coding of ordered K-tuples.pdf:pdf},
isbn = {3540615105},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {869--873},
title = {{Binary spatter-coding of ordered K-tuples}},
volume = {1112 LNCS},
year = {1996}
}
@article{Keeler1988,
abstract = {comparison between Hopfield network and sparse distributed memory.{\$}\backslash{\$}ncapacity of Hopfield (and SDM) explained.{\$}\backslash{\$}n{\$}\backslash{\$}nThe Sparse, Distributed Memory (SDM) model (Kanerva. 1984) is compared{\$}\backslash{\$}nto Hopfield-type, neural-network models. A mathematical framework{\$}\backslash{\$}nfor cornporing the two models is developed, and the capacity of each{\$}\backslash{\$}nmodel is investigated. The capacity of the SDM can be increased independent{\$}\backslash{\$}nof the dimension of the stored vectors, whereas the Hopfield capacity{\$}\backslash{\$}nis limited to a fraction of this dimension. The stored information{\$}\backslash{\$}nis proportional to the number of connections, and it is shown that{\$}\backslash{\$}nthis proportionality constant is the same for the SDM. the Hopfield{\$}\backslash{\$}nmodel, and higher-order models. The models are also compared in their{\$}\backslash{\$}nobility to store and recall temporal sequences of patterns. The SDM{\$}\backslash{\$}nalso includes time delays so that contextual information con be used{\$}\backslash{\$}nto recover sequences. A gen- eralization of the SDM allows storage{\$}\backslash{\$}nof correlated patterns.},
author = {Keeler, J.D.},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Keeler - 1988 - Comparison Between Kanerva Sdm and Hopfield-Type Neural Networks.pdf:pdf},
journal = {Cogn. Sci.},
keywords = {pdf},
number = {3},
pages = {299--329},
title = {{Comparison Between Kanerva Sdm and Hopfield-Type Neural Networks}},
volume = {12},
year = {1988}
}
@article{Rasmussen2011,
abstract = {Abstract Inductive reasoning is a fundamental and complex aspect of human intelligence. In particular, how do subjects, given a set of particular examples, generate general descriptions of the rules governing that set? We present a biologically plausible method ... {\$}\backslash{\$}n},
author = {Rasmussen, Daniel and Eliasmith, Chris},
doi = {10.1111/j.1756-8765.2010.01127.x},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rasmussen, Eliasmith - 2011 - A neural model of rule generation in inductive reasoning.pdf:pdf},
isbn = {17568757},
issn = {17568757},
journal = {Topics in Cognitive Science},
keywords = {Cognitive modeling,Fluid intelligence,Inductive reasoning,Neural engineering framework,Raven's progressive matrices,Realistic neural modeling,Rule generation,Vector symbolic architectures},
number = {1},
pages = {140--153},
pmid = {25164178},
title = {{A neural model of rule generation in inductive reasoning}},
volume = {3},
year = {2011}
}
@article{Anwar2003a,
abstract = {In this work we are reporting a case study on the use of SDM as the associative memory for a software agent, CMattie, whose architecture is modeled on human cognition. Sparse distributed memory (SDM) is a content-addressable memory technique that relies on close memory items tending to be clustered together. In this work, we used an enhanced version of SDM augmented with the use of genetic algorithms as an associative memory in our 'conscious' software agent, CMattie, who is responsible for emailing seminar announcements in an academic department. Interacting with seminar organizers via email in natural language, CMattie can replace the secretary who normally handles such announcements. SDM is a key ingredient in a complex agent architecture that implements global workspace theory, a psychological theory of consciousness and cognition. In this architecture, SDM, as the primary memory for the agent, provides associations with incoming percepts. These include disambiguation of the percept by removing noise, correcting misspellings, and adding missing pieces of information. It also retrieves behaviors and emotions associated with the percept. These associations are based on previous similar percepts, and their consequences, that have been recorded earlier. SDM also possesses several key psychological features. Some enhancements to SDM including multiple writes of important items, use of error detection and correction, and the use of hashing to map the original information into fixed size keys were used. Test results indicate that SDM can be used successfully as an associative memory in such complex agent architectures. The results show that SDM is capable of recovering a percept based on a part of that percept, and finding defaults for empty perception registers. The evaluation of suggested actions and emotional states is satisfactory. We think that this work opens the door to more scientific and empirical uses for SDM. ?? 2003 Elsevier B.V. All rights reserved.},
author = {Anwar, Ashraf and Franklin, Stan},
doi = {10.1016/S1389-0417(03)00015-9},
issn = {13890417},
journal = {Cognitive Systems Research},
keywords = {Artificial intelligence,Cognition,Consciousness,Genetic algorithms,Software agents,Sparse distributed memory},
month = {dec},
number = {4},
pages = {339--354},
title = {{Sparse distributed memory for ‘conscious' software agents}},
volume = {4},
year = {2003}
}
@article{Ganguli2010,
abstract = {Recent proposals suggest that large, generic neuronal networks could store mem- ory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neu- rons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant sce- nario is that of sparse input sequences. In this scenario, we showhowlinear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical sys- tems, and provides a theoretical analysis of their asymptotic performance. 1},
author = {Ganguli, Surya and Sompolinsky, Haim},
doi = {10.1163/187633309X12563839996540},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ganguli, Sompolinsky - 2010 - Short-term memory in neuronal networks through dynamical compressed sensing.pdf:pdf},
isbn = {9781617823800},
issn = {00944467},
journal = {Advances in Neural Information Processing Systems},
pages = {1--9},
title = {{Short-term memory in neuronal networks through dynamical compressed sensing.}},
year = {2010}
}
@article{Hinton1990,
abstract = {(From the chapter) first section of this chapter stresses some of the virtues of distributed representations / second section considers the efficiency of distributed representations, and shows clearly why distributed representations can be better than local ones for certain classes of problems / final section discusses some difficult issues which are often avoided by advocates of distributed representations, such as the representation of constituent structure and the sequential focusing of processing effort on different aspects of a structure object (PsycINFO Database Record (c) 2009 APA, all rights reserved)},
author = {Hinton, Geoffrey E and McClelland, James L and Rumelhart, David E},
doi = {10.1146/annurev-psych-120710-100344},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton, McClelland, Rumelhart - 1990 - Distributed representations.pdf:pdf},
isbn = {0-262-68053-X},
issn = {1534-7362},
journal = {The philosophy of artificial intelligence},
keywords = {artificial intelligence,problem solving},
pages = {248--280},
pmid = {21943171},
title = {{Distributed representations}},
year = {1990}
}
@article{Redlich1993,
abstract = {A redundancy reduction strategy, which can be applied in stages, is proposed as a way to learn as efficiently as possible the statistical properties of an ensemble of sensory messages. The method works best for inputs consisting of strongly correlated groups, that is features, with weaker statistical dependence between different features. This is the case for localized objects in an image or for words in a text. A local feature measure determining how much a single feature reduces the total redundancy is derived which turns out to depend only on the probability of the feature and of its components, but not on the statistical properties of any other features. The locality of this measure makes it ideal as the basis for a "neural" implementation of redundancy reduction, and an example of a very simple non-Hebbian algorithm is given. The effect of noise on learning redundancy is also discussed.},
author = {Redlich, A. Norman},
doi = {10.1162/neco.1993.5.2.289},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Redlich - 1993 - Redundancy Reduction as a Strategy for Unsupervised Learning.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
pages = {289--304},
title = {{Redundancy Reduction as a Strategy for Unsupervised Learning}},
volume = {5},
year = {1993}
}
@incollection{Rumelhart1986,
abstract = {What makes people smarter than computers? The work described in these two volumes suggests that the answer lies in the massively parallel architecture of the human mind. It is some of the most exciting work in cognitive science, unifying neural and cognitive processes in a highly computational framework, with links to artificial intelligence. Although thought and problem solving have a sequential character when viewed over a time frame of minutes or hours, the authors argue that each step in the sequence is the result of the simultaneous activity of a large number of simple computational elements, each influencing others and being influenced by them. "Parallel Distributed Processing" describes their work in developing a theoretical framework for describing this parallel distributed processing activity and in applying the framework to the development of models of aspects of perception, memory, language, and thought. Volume 1 lays the theoretical foundations of parallel distributed processing. It introduces the approach and the reasons why the authors feel it is a fruitful one, describes several models of basic mechanisms with wide applicability to different problems, and presents a number of specific technical analyses of different aspects of parallel distributed models.},
author = {Rumelhart, D. E. and Hinton, G. E. and Mcclelland, James L},
booktitle = {Parallel distributed processing: explorations in the microstructure of cognition},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rumelhart, Hinton, Mcclelland - 1986 - A General framework for Parallel Distributed Processing.pdf:pdf},
isbn = {026268053x},
pages = {45 -- 76},
title = {{A General framework for Parallel Distributed Processing}},
year = {1986}
}
@misc{Rumelhart2013,
abstract = {(From the chapter) parallel distributed processing models as constraint satisfaction networks constraint satisfaction and schemata parallel distributed processing models and thinking contents of consciousness the problem of control mental models mental simulations and mental practice conversations: actual and imagined external representations and formal reasoning goal direction in thinking (PsycINFO Database Record (c) 2009 APA, all rights reserved)},
author = {Rumelhart, D. E. and Smolensky, P. and McClelland, J. L. and Hinton, G. E.},
booktitle = {Readings in Cognitive Science: A Perspective from Psychology and Artificial Intelligence},
doi = {10.1016/B978-1-4832-1446-7.50020-0},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rumelhart et al. - 2013 - Schemata and Sequential Thought Processes in PDP Models.pdf:pdf},
isbn = {1558600132},
pages = {224--249},
pmid = {69},
title = {{Schemata and Sequential Thought Processes in PDP Models}},
year = {2013}
}
@article{Shannon1950,
abstract = {This thesis develops and evaluates predictive theories$\backslash$nof music. Good theories should model a particular class of music and$\backslash$npredict new pieces in the class with high probability. Attention is$\backslash$nrestricted to melody alone -- harmony and polyphony are not$\backslash$nconsidered. Theories are constructed using an empirical learning$\backslash$napproach, and to construct and evaluate them, one hundred Bach$\backslash$nchorale melodies are analyzed. Theories are evaluated by a data$\backslash$ncompression measure, which is a strong indicator of their predictive$\backslash$npower. The entropy of the chorales is estimated by averaging the$\backslash$namount of compression given to a test set using a theory learned$\backslash$nfrom a training set. The central hypothesis is that the chorales are$\backslash$nquite redundant in the information theoretic sense. A novel approach$\backslash$nto the induction of sequence generating rules, called multiple$\backslash$nviewpoints, is created. This method is based on the variable--order$\backslash$nMarkov model, with extensions to incorporate timescales and parallel$\backslash$nstreams of description. A multiple viewpoint system comprises two$\backslash$nparts: a long--term theory which adapts to a class of sequences, and$\backslash$na short--term theory which adapts to a particular instance of the$\backslash$nclass. Predictions from both are combined into an overall$\backslash$nprediction. The performance of several different multiple viewpoint$\backslash$nsystems is assessed on the chorale data. The redundancy of the$\backslash$nchorales is thereby estimated to be 55{\%}. This thesis concludes that$\backslash$nthe estimate must be compared with human performance at the same$\backslash$npredictive task. The theory should also be evaluated in terms of the$\backslash$nquality of the new chorales it generates, and by its ability to$\backslash$ndiscriminate chorales from non--chorales.},
author = {Shannon, Claude E.},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shannon - 1950 - Prediction and Entropy of Printed English.pdf:pdf},
journal = {Bell system technical journal},
number = {1},
pages = {50--64},
title = {{Prediction and Entropy of Printed English}},
volume = {30},
year = {1950}
}
@article{Wickelgren1969,
abstract = {The problem of serial order in noncreative behavior is defined in much the same manner as Lashley (1951), and several theories of serial order are examined. Lashley's rejection of associative-chain theories of serial order is shown to apply to one particular theory, and to be invalid as applied to other associative theories. Indeed, the most plausible theory is the "context-sensitive associ- ative theory," which assumes that serial order is encoded by means of associ- ations between context-sensitive elementary motor responses. In speech, this means that a word such as "stop" is assumed to be coded "allophonically" as /{\#}st, »t0, tOp, op{\#}/, rather than being coded phonetnically as /s, t, o, p/. This theory handles the pronunciation of single words and even phrases in a certain sense.},
author = {Wickelgren, Wayne A},
doi = {10.1037/h0026823},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wickelgren - 1969 - Context-sensitive coding, associative memory, and serial order in (speech) behavior.pdf:pdf},
isbn = {1939-1471},
issn = {0033-295X},
journal = {Psychological Review},
number = {1},
pages = {1--15},
pmid = {328219},
title = {{Context-sensitive coding, associative memory, and serial order in (speech) behavior}},
volume = {76},
year = {1969}
}
@article{Arathorn2005,
author = {Arathorn, David W.},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arathorn - 2005 - A Cortically-Pausible Inverse Problem Solving Mthod Applied to Reconginizing Static and Kinematic 3D Objects.pdf:pdf},
title = {{A Cortically-Pausible Inverse Problem Solving Mthod Applied to Reconginizing Static and Kinematic 3D Objects}},
year = {2005}
}
@article{Gayler2003jackendoff,
abstract = {Jackendoff (2002) posed four challenges that linguistic combinatoriality and rules of language present to theories of brain function. The essence of these problems is the question of how to neurally instantiate the rapid construction and transformation of the compositional structures that are typically taken to be the domain of symbolic processing. He contended that typical connectionist approaches fail to meet these challenges and that the dialogue between linguistic theory and cognitive neuroscience will be relatively unproductive until the importance of these problems is widely recognised and the challenges answered by some technical innovation in connectionist modelling. This paper claims that a little-known family of connectionist models (Vector Symbolic Architectures) are able to meet Jackendoff's challenges.},
archivePrefix = {arXiv},
arxivId = {cs/0412059},
author = {Gayler, Ross W.},
doi = {10.1017/S0140525X06309028},
eprint = {0412059},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gayler - 2003 - Vector Symbolic Architectures answer Jackendoff's challenges for cognitive neuroscience.pdf:pdf},
issn = {0140-525X},
journal = {Proceedings of the ICCS/ASCS International Conference on Cognitive Science},
number = {2002},
pages = {6},
primaryClass = {cs},
title = {{Vector Symbolic Architectures answer Jackendoff's challenges for cognitive neuroscience}},
year = {2003}
}
@article{Hornik1991,
abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(??) performance criteria, for arbitrary finite input environment measures ??, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives. ?? 1991.},
author = {Hornik, Kurt},
doi = {10.1016/0893-6080(91)90009-T},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hornik - 1991 - Approximation capabilities of multilayer feedforward networks.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Activation function,Input environment measure,Lp(??) approximation,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities},
month = {jan},
number = {2},
pages = {251--257},
title = {{Approximation capabilities of multilayer feedforward networks}},
volume = {4},
year = {1991}
}
@article{Kanerva2009hyper,
abstract = {The 1990s saw the emergence of cognitive models that depend on very high dimensionality and randomness. They include Holographic Reduced Representations, Spatter Code, Semantic Vectors, Latent Semantic Analysis, Context-Dependent Thinning, and Vector-Symbolic Architecture. They represent things in high-dimensional vectors that are manipulated by operations that produce new high-dimensional vectors in the style of traditional computing, in what is called here hyperdimensional computing on account of the very high dimensionality. The paper presents the main ideas behind these models, written as a tutorial essay in hopes of making the ideas accessible and even provocative. A sketch of how we have arrived at these models, with references and pointers to further reading, is given at the end. The thesis of the paper is that hyperdimensional representation has much to offer to students of cognitive science, theoretical neuroscience, computer science and engineering, and mathematics.},
author = {Kanerva, Pentti},
doi = {10.1007/s12559-009-9009-8},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kanerva - 2009 - Hyperdimensional computing An introduction to computing in distributed representation with high-dimensional random vect.pdf:pdf},
isbn = {1866-9956},
issn = {18669956},
journal = {Cognitive Computation},
keywords = {Cognitive code,Holistic mapping,Holistic record,Holographic reduced representation,Random indexing,von Neumann architecture},
pages = {139--159},
title = {{Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors}},
volume = {1},
year = {2009}
}
@article{Laiho2015,
author = {Laiho, Mika and Poikonen, Jussi H. and Kanerva, Pentti and Lehtonen, Eero},
doi = {10.1109/BioCAS.2015.7348414},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laiho et al. - 2015 - High-dimensional computing with sparse vectors.pdf:pdf},
isbn = {978-1-4799-7234-0},
journal = {2015 IEEE Biomedical Circuits and Systems Conference (BioCAS)},
pages = {1--4},
title = {{High-dimensional computing with sparse vectors}},
year = {2015}
}
@article{Smolensky1986,
abstract = {Smolensky, P. (1986). Information processing in dynamical systems: Foundations of harmony theory. In Rumelhart, D. E. and McClelland, J. L., editors, Parallel Distributed Processing: Volume 1: Foundations, pages 194-281. MIT Press, Cambridge, MA.},
author = {Smolensky, Paul},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Smolensky - 1986 - Information processing in dynamical systems Foundations of harmony theory.pdf:pdf;:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Smolensky - 1986 - Information processing in dynamical systems Foundations of harmony theory(2).pdf:pdf},
isbn = {026268053X},
journal = {Parallel Distributed Processing Explorations in the Microstructure of Cognition},
number = {1},
pages = {194--281},
title = {{Information processing in dynamical systems: Foundations of harmony theory}},
volume = {1},
year = {1986}
}
@inproceedings{Gayler1998,
address = {New Bulgarian University, Sofia, Bulgaria},
author = {Gayler, Ross W.},
booktitle = {Gentner, D., Holyoak, K. J., Kokinov, B. N. (Eds.), Advances in analogy research: Integration of theory and data from the cognitive, computational, and neural sciences},
pages = {1--4},
title = {{Multiplicative binding, representation operators {\&} analogy}},
year = {1998}
}
@inproceedings{Rahimi2016,
author = {Rahimi, Abbas and Kanerva, Pentti and Rabaey, Jan M},
booktitle = {Low Power Electronics and Design (ISLPED), 2016 IEEE/ACM International Symposium on},
month = {aug},
title = {{A Robust and Energy Efficient Classifier Using Brain-Inspired Hyperdimensional Computing}},
year = {2016}
}
@article{Shannon_TC01,
address = {New York, NY, USA},
author = {Shannon, Claude E.},
doi = {10.1145/584091.584093},
issn = {1559-1662},
journal = {SIGMOBILE Mob. Comput. Commun. Rev.},
month = {jan},
number = {1},
pages = {3--55},
publisher = {ACM},
title = {{A Mathematical Theory of Communication}},
volume = {5},
year = {2001}
}
@article{holoGN16,
author = {Kleyko, D and Osipov, E and Senior, A and Khan, A I and Şekercioǧlu, Y A},
doi = {10.1109/TNNLS.2016.2535338},
file = {:home/epaxon/Research/Redwood/Kanerva/References/2016{\_}IEEE{\_}TNNLS{\_}HoloGN.pdf:pdf},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Biological system modeling;Computational modeling;},
number = {99},
pages = {1--13},
title = {{Holographic Graph Neuron: A Bioinspired Architecture for Pattern Processing}},
volume = {PP},
year = {2016}
}
@inproceedings{Dollar-Mexico,
author = {Kanerva, Pentti},
booktitle = {AAAI Fall Symposium: Quantum Informatics for Cognitive, Social, and Semantic Processes},
file = {:home/epaxon/Research/Redwood/Kanerva/References/232c0a0835dcbc4fc6b6283db484695482f9.pdf:pdf},
pages = {2--6},
title = {{What We Mean When We Say ``What's the Dollar of Mexico?'': Prototypes and Mapping in Concept Space}},
year = {2010}
}
@article{White2004,
abstract = {We study the ability of linear recurrent networks obeying discrete time dynamics to store long temporal sequences that are retrievable from the instantaneous state of the network. We calculate this temporal memory capacity for both distributed shift register and random orthogonal connectivity matrices. We show that the memory capacity of these networks scales with system size.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0402452},
author = {White, Olivia L. and Lee, Daniel D. and Sompolinsky, Haim},
doi = {10.1103/PhysRevLett.92.148102},
eprint = {0402452},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/White, Lee, Sompolinsky - 2004 - Short-term memory in orthogonal neural networks.pdf:pdf},
isbn = {0031-9007},
issn = {00319007},
journal = {Physical Review Letters},
number = {14},
pages = {148102--1},
pmid = {15089576},
primaryClass = {cond-mat},
title = {{Short-term memory in orthogonal neural networks}},
volume = {92},
year = {2004}
}
@misc{Jordan1986,
author = {Jordan, Michael},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jordan - 1986 - An Introduction to Linear Algebra in Parallel Distributed Processing.pdf:pdf},
title = {{An Introduction to Linear Algebra in Parallel Distributed Processing}},
year = {1986}
}
@article{McClelland1986,
abstract = {What makes people smarter than machines? They certainly are not quicker or more precise. Yet people are far better at perceiving objects in natural scenes and noting their relations , at understanding language and retrieving contextually appropriate information from memory, at making plans and carrying out contextually appropriate actions, and at a wide range of other natural cognitive .tasks. People are also far better at learning to do these things more accurately and fluently through pro-cessing experience. What is the basis for these differences? One answer, perhaps the classic one we might expect from artificial intelligence, is " software." If we only had the right computer program, the argument goes , we might be able to capture the fluidity and adaptability of human information proceSSIng. Certainly this answer is partially correct. There have been great breakthroughs in our understanding of cognition as a result of the development of expressive high-level computer languages and powerful algorithms. No doubt there will be more such breakthroughs in the future. However , we do not think that software is the whole story. In our view, people are smarter than today s computers because the brain employs a basic computational architecture that is more suited to deal with a central aspect of the natural information processing tasks that people are so good at. In this chapter, we will show through exam-ples that these tasks generally require the simultaneous consideration of many pieces of information or constraints. Each constraint may be imperfectly specified and ambiguous, yet each can playa potentially},
author = {McClelland, Jl and Rumelhart, De and Hinton, Ge},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McClelland, Rumelhart, Hinton - 1986 - The Appeal of Parallel Distributed Processing.pdf:pdf},
isbn = {9780262181204},
journal = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations},
pages = {3--44},
title = {{The Appeal of Parallel Distributed Processing}},
year = {1986}
}
@article{Pascanu2012,
abstract = {There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and the exploding gradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pascanu, Mikolov, Bengio - 2012 - On the difficulty of training recurrent neural networks.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {1045-9227},
journal = {Proceedings of The 30th International Conference on Machine Learning},
number = {2},
pages = {1310--1318},
pmid = {18267787},
title = {{On the difficulty of training recurrent neural networks}},
url = {http://jmlr.org/proceedings/papers/v28/pascanu13.pdf},
year = {2012}
}
@article{chiani2003new,
author = {Chiani, Marco and Dardari, Davide and Simon, Marvin K},
journal = {IEEE Transactions on Wireless Communications},
number = {4},
pages = {840--845},
publisher = {IEEE},
title = {{New exponential bounds and approximations for the computation of error probability in fading channels}},
volume = {2},
year = {2003}
}
@article{chernoff1952measure,
author = {Chernoff, Herman},
journal = {The Annals of Mathematical Statistics},
pages = {493--507},
publisher = {JSTOR},
title = {{A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations}},
year = {1952}
}
@article{chang2011chernoff,
author = {Chang, Seok-Ho and Cosman, Pamela C and Milstein, Laurence B},
journal = {IEEE Transactions on Communications},
number = {11},
pages = {2939--2944},
publisher = {IEEE},
title = {{Chernoff-type bounds for the Gaussian error function}},
volume = {59},
year = {2011}
}
@article{owen1980table,
author = {Owen, D. B.},
journal = {Communications in Statistics-Simulation and Computation},
number = {4},
pages = {389--419},
publisher = {Taylor {\&} Francis},
title = {{A table of normal integrals: A table}},
volume = {9},
year = {1980}
}
@article{bound1970probability,
author = {Hellman, M E and Raviv, J},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hellman, Raviv - 1970 - Probability of error, equivocation, and the Chernoff bound.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
number = {4},
title = {{Probability of error, equivocation, and the Chernoff bound}},
volume = {16},
year = {1970}
}
@article{jacobs1966probability,
author = {Jacobs, Irwin},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jacobs - 1966 - Probability-of-error bounds for binary transmission on the slowly fading Rician channel.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
number = {4},
pages = {431--441},
publisher = {IEEE},
title = {{Probability-of-error bounds for binary transmission on the slowly fading Rician channel}},
volume = {12},
year = {1966}
}
@article{Vishwanathan2010,
author = {Vishwanathan, S V N and Smola, Alexander J},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vishwanathan, Smola - 2010 - Binet-Cauchy Kernels on Dynamical Systems and its Application to the Analysis of Dynamic Scenes ∗.pdf:pdf},
keywords = {arma models and dynamical,binet-cauchy theorem,sylvester,systems},
pages = {1--43},
title = {{Binet-Cauchy Kernels on Dynamical Systems and its Application to the Analysis of Dynamic Scenes ∗}},
year = {2010}
}
@article{Berlekamp1967,
abstract = {Lower bounds to minimum error probability using block coding on noisy discrete memoryless communication channels},
author = {Berlekamp, E R and Gallager, R G and Shannon, C E},
doi = {10.1016/S0019-9958(67)90052-6},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Berlekamp, Gallager, Shannon - 1967 - Lower bounds to error probability for coding on discrete memoryless channels. II.pdf:pdf},
issn = {00199958},
journal = {Information and Control},
number = {5},
pages = {522--552},
title = {{Lower bounds to error probability for coding on discrete memoryless channels. II.}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0019995867912004},
volume = {10},
year = {1967}
}
@article{Lee2016,
abstract = {Question answering tasks have shown remarkable progress with distributed vector representations. In this paper, we look into the recently proposed Facebook 20 tasks (FB20). Finding the answers for questions in FB20 requires complex reasoning. Because the previous work on FB20 consists of end-to-end models, it is unclear whether errors come from imperfect understanding of semantics or in certain steps of the reasoning. To address this issue, we propose two vector space models inspired by tensor product representation (TPR) to perform analysis, knowledge representation, and reasoning based on common-sense inference. We achieve near-perfect accuracy on all categories, including positional reasoning and pathfinding that have proved difficult for all previous approaches due to the special two-dimensional relationships identified from this study. The exploration reported in this paper and our subsequent work on generalizing the current model to the TPR formalism suggest the feasibility of developing further reasoning models in tensor space with learning capabilities.},
archivePrefix = {arXiv},
arxivId = {1511.06426},
author = {Lee, Moontae and He, Xiaodong and Yih, Wen-tau and Gao, Jianfeng and Deng, Li and Smolensky, Paul},
eprint = {1511.06426},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee et al. - 2016 - Reasoning in Vector Space An Exploratory Study of Question Answering.pdf:pdf},
journal = {Iclr},
number = {2015},
pages = {1--11},
title = {{Reasoning in Vector Space: An Exploratory Study of Question Answering}},
year = {2016}
}
@article{Rasanen2015,
author = {R{\"{a}}s{\"{a}}nen, Okko J and Saarinen, Jukka P},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/R{\"{a}}s{\"{a}}nen, Saarinen - 2015 - Hyperdimensional Coding Applied to the Analysis of Mobile Phone Use Patterns.pdf:pdf},
journal = {Transactions on Neural Networks and Learning Systems},
pages = {1--12},
title = {{Hyperdimensional Coding Applied to the Analysis of Mobile Phone Use Patterns}},
year = {2015}
}
@article{Root1968,
abstract = {Gaussian channels capacity, studying supremum of information transmission rates with small error probability},
author = {Root, W L and Varaiya, P P},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Root, Varaiya - 1968 - Capacity of classes of Gaussian channels.pdf:pdf},
journal = {SIAM Journal on Applied Mathematics},
number = {6},
pages = {1350--1393},
title = {{Capacity of classes of Gaussian channels.}},
volume = {16},
year = {1968}
}
@article{Shannon1949,
author = {Shannon, Claude E.},
doi = {10.1109/JRPROC.1949.232969},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shannon - 1949 - Communication in the Presence of Noise.pdf:pdf},
isbn = {0252725484},
issn = {0096-8390},
journal = {Proceedings of the IRE},
month = {jan},
number = {1},
pages = {10--21},
pmid = {9230594},
title = {{Communication in the Presence of Noise}},
volume = {37},
year = {1949}
}
@article{Smolensky1990,
abstract = {A general method, the tensor product representation, is defined for the connectionist representation of value/variable bindings. The technique is a formalization of the idea that a set of value/variable pairs can be represented by accumulating activity in a collection of units each of which computes the product of a feature of a variable and a feature of its value. The method allows the fully distributed representation of bindings and symbolic structures. Fully and partially localized special cases of the tensor product representation reduce to existing cases of connectionist representations of structured data. The representation rests on a principled analysis of structure; it saturates gracefully as larger structures are represented; it permits recursive construction of complex representations from simpler ones; it respects the independence of the capacities to generate and maintain multiple bindings in parallel; it extends naturally to continuous structures and continuous representational patterns; it permits values to also serve as variables; and it enables analysis of the interference of symbolic structures stored in associative memories. It has also served as the basis for working connectionist models of high-level cognitive tasks. ?? 1990.},
author = {Smolensky, Paul},
doi = {10.1016/0004-3702(90)90007-M},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Smolensky - 1990 - Tensor product variable binding and the representation of symbolic structures in connectionist systems.pdf:pdf},
isbn = {9780262256360},
issn = {00043702},
journal = {Artificial Intelligence},
number = {1-2},
pages = {159--216},
title = {{Tensor product variable binding and the representation of symbolic structures in connectionist systems}},
volume = {46},
year = {1990}
}
@inproceedings{Buckingham2008,
abstract = {The performance of random error control codes approaches the Shannon capacity limit as the code length goes to infinity. When the code length is finite, then the code will be unable to achieve arbitrarily low error probability and a nonzero codeword error rate is inevitable. Information-theoretic bounds on codeword error rate may be found as a function of length through traditional methods such as sphere packing. Alternatively, the behavior of finite-length codes can be characterized in terms of an information-outage probability. The information- outage probability is the probability that the mutual information, which is a random variable, is less than the rate. In this paper, a Gaussian approximation is proposed that accurately models the information-outage probability for moderately small codes. The information-outage probability is related to several previously derived bounds, including Shannon's sphere-packing and random coding bounds, as well as a bound on maximal error probability known as Feinstein's lemma. It is shown that the information- outage probability is a useful predictor of achievable error rate.},
author = {Buckingham, David and Valenti, Matthew C},
booktitle = {CISS 2008, The 42nd Annual Conference on Information Sciences and Systems},
doi = {10.1109/CISS.2008.4558558},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Buckingham, Valenti - 2008 - The information-outage probability of finite-length codes over AWGN channels.pdf:pdf},
isbn = {9781424422470},
pages = {390--395},
title = {{The information-outage probability of finite-length codes over AWGN channels}},
year = {2008}
}
@book{Cover2005,
abstract = {Following a brief introduction and overview, early chapters cover the basic algebraic relationships of entropy, relative entropy and mutual information, AEP, entropy rates of stochastics processes and data compression, duality of data compression and the growth rate of wealth. Later chapters explore Kolmogorov complexity, channel capacity, differential entropy, the capacity of the fundamental Gaussian channel, the relationship between information theory and statistics, rate distortion and network information theories. The final two chapters examine the stock market and inequalities in information theory. In many cases the authors actually describe the properties of the solutions before the presented problems.},
archivePrefix = {arXiv},
arxivId = {ISBN 0-471-06259-6},
author = {Cover, Thomas M. and Thomas, Joy A.},
booktitle = {Elements of Information Theory},
doi = {10.1002/047174882X},
eprint = {ISBN 0-471-06259-6},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cover, Thomas - 2005 - Elements of Information Theory.pdf:pdf},
isbn = {9780471241959},
issn = {15579654},
pages = {1--748},
pmid = {20660925},
title = {{Elements of Information Theory}},
year = {2005}
}
@inproceedings{Dolinar1998,
abstract = {We show that a single family of turbo codes is {\&}ldquo;nearly perfect{\&}rdquo; with respect to Shannon's (1959) sphere-packing bound, over a wide range of code rates and block sizes. We also assess the {\&}ldquo;imperfectness{\&}rdquo; of various non-turbo codes for comparison,},
archivePrefix = {arXiv},
arxivId = {TMO PR 42-133},
author = {Dolinar, S and Divsalar, D and Pollara, F},
booktitle = {IEEE International Symposium on Information Theory - Proceedings},
doi = {10.1109/ISIT.1998.708612},
eprint = {TMO PR 42-133},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dolinar, Divsalar, Pollara - 1998 - Turbo code performance as a function of code block size.pdf:pdf},
isbn = {0780350006},
issn = {21578095},
pages = {32},
title = {{Turbo code performance as a function of code block size}},
year = {1998}
}
@article{Gallager1965,
abstract = {Upper bounds are derived on the probability of error that can be achieved$\backslash$nby using block codes on general time-discrete memoryless channels.$\backslash$nBoth amplitude-discrete and amplitude-continuous channels are treated,$\backslash$nboth with and without input constraints. The major advantages of$\backslash$nthe present approach are the simplicity of the derivations and the$\backslash$nrelative simplicity of the results; on the other hand, the exponential$\backslash$nbehavior of the bounds with block length is the best known for all$\backslash$ntransmission rates between 0 and capacity. The results are applied$\backslash$nto a number of special channels, including the binary symmetric channel$\backslash$nand the additive Gaussian noise channel.},
author = {Gallager, R},
doi = {10.1109/TIT.1965.1053730},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gallager - 1965 - A Simple Derivation of the Coding Theorem and Some Applications.pdf:pdf},
isbn = {0018-9448},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
keywords = {Block codes,Memoryless channels},
number = {1},
pages = {3--18},
title = {{A Simple Derivation of the Coding Theorem and Some Applications}},
volume = {11},
year = {1965}
}
@article{Goblick1965,
author = {Goblick, T},
doi = {10.1109/TIT.1965.1053821},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goblick - 1965 - Theoretical limitations on the transmission of data from analog sources.pdf:pdf},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
number = {4},
pages = {558--567},
title = {{Theoretical limitations on the transmission of data from analog sources}},
volume = {11},
year = {1965}
}
@article{Little1978,
abstract = {Previously, we developed a model of short and long term memory which was based on an analogy to the Ising spin system in a neural network. We assumed that the modification of the synaptic strengths was dependent upon the correlation of pre- and post-synaptic neuronal firing. This assumption we denote as the Hebb hypothesis. In this paper, we solve exactly a linearized version of the model and explicitly show that the capacity of the memory is related to the number of synapses rather than the much smaller number of neurons. In addition, we show that in order to utilize this large capacity, the network must store the major part of the information in the capability to generate patterns which evolve with time. We are also led to a modified Hebb hypothesis. ?? 1978.},
author = {Little, W. A. and Shaw, Gordon L.},
doi = {10.1016/0025-5564(78)90058-5},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Little, Shaw - 1978 - Analytic study of the memory storage capacity of a neural network.pdf:pdf},
issn = {00255564},
journal = {Mathematical Biosciences},
number = {3-4},
pages = {281--290},
title = {{Analytic study of the memory storage capacity of a neural network}},
volume = {39},
year = {1978}
}
@article{Palm1992,
abstract = {A new access to the asymptotic analysis of autoassociation properties in recurrent McCulloch-Pitts networks in the range of low activity is proposed. Using information theory, this method examines the static structure of stable states imprinted by a Hebbian storing process. In addition to the definition of critical pattern capacity usually considered in the analysis of the Hopfield model, we introduce the definition of information capacity which guarantees content adressability and is a stricter upper bound of the information really accessible in an autoassociation process. We calculate these two types of capacities for two types of local learning rules which are very effective for sparsely coded patterns: the Hebb rule and the clipped Hebb rule. It turns out that for both rules the information capacity is exactly half the pattern capacity.},
author = {Palm, G and Sommer, F},
doi = {10.1088/0954-898X/3/2/006},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Palm, Sommer - 1992 - Information capacity in recurrent McCulloch–Pitts networks with sparsely coded memory states.pdf:pdf},
issn = {0954-898X},
journal = {Network: Computation in Neural Systems},
number = {2},
pages = {177--186},
title = {{Information capacity in recurrent McCulloch–Pitts networks with sparsely coded memory states}},
volume = {3},
year = {1992}
}
@article{Polyanskiy2010,
abstract = {This paper investigates the maximal channel coding rate achievable at a given blocklength and error probability. For general classes of channels new achievability and converse bounds are given, which are tighter than existing bounds for wide ranges of parameters of interest, and lead to tight approximations of the maximal achievable rate for blocklengths n as short as 100. It is also shown analytically that the maximal rate achievable with error probability ¿ isclosely approximated by C - ¿(V/n) Q-1(¿) where C is the capacity, V is a characteristic of the channel referred to as channel dispersion , and Q is the complementary Gaussian cumulative distribution function.},
author = {Polyanskiy, Yury and Poor, H Vincent and Verd{\'{u}}, Sergio},
doi = {10.1109/TIT.2010.2043769},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Polyanskiy, Poor, Verd{\'{u}} - 2010 - Channel coding rate in the finite blocklength regime.pdf:pdf},
isbn = {0018-9448 VO - 56},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Achievability,Channel capacity,Coding for noisy channels,Converse,Finite blocklength regime,Shannon theory},
number = {5},
pages = {2307--2359},
title = {{Channel coding rate in the finite blocklength regime}},
volume = {56},
year = {2010}
}
@article{Shi2007,
author = {Shi, Jun and Wesel, Richard D.},
doi = {10.1109/TIT.2007.903156},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shi, Wesel - 2007 - A study on universal codes with finite block lengths.pdf:pdf},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Compound channel,Random coding bound,Sphere-packing bound (SPB),Universal code},
number = {9},
pages = {3066--3074},
title = {{A study on universal codes with finite block lengths}},
volume = {53},
year = {2007}
}
@article{Slepian1963,
author = {Slepian, David},
doi = {10.1002/j.1538-7305.1963.tb04013.x},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Slepian - 1963 - Bounds on Communication.pdf:pdf},
issn = {15387305},
journal = {Bell System Technical Journal},
number = {3},
pages = {681--707},
title = {{Bounds on Communication}},
volume = {42},
year = {1963}
}
@article{Sommer1999,
abstract = {The Willshaw model is asymptotically the most efficient neural associative memory (NAM), but its finite version is hampered by high retrieval errors. Iterative retrieval has been proposed in a large number of different models to improve performance in auto-association tasks. In this paper, bidirectional retrieval for the hetero-associative memory task is considered: we define information efficiency as a general performance measure for bidirectional associative memory (BAM) and determine its asymptotic bound for the bidirectional Willshaw model. For the finite Willshaw model, an efficient new bidirectional retrieval strategy is proposed, the appropriate combinatorial model analysis is derived, and implications of the proposed sparse BAM for applications and brain theory are discussed. The distribution of the dendritic sum in the finite Willshaw model given by Buckingham and Willshaw [Buckingham, J., and Willshaw, D. (1992). Performance characteristics of associative nets. Network, 3, 407-414] allows no fast numerical evaluation. We derive a combinatorial formula with a highly reduced evaluation time that is used in the improved error analysis of the basic model and for estimation of the retrieval error in the naive model extension, where bidirectional retrieval is employed in the hetero-associative Willshaw model. The analysis rules out the naive BAM extension as a promising improvement. A new bidirectional retrieval algorithm - called crosswise bidirectional (CB) retrieval - is presented. The cross talk error is significantly reduced without employing more complex learning procedures or dummy augmentation in the pattern coding, as proposed in other refined BAM models [Wang, Y. F., Cruz, J. B., and Mulligan, J. H. (1990). Two coding strategies for bidirectional associative memory. IEEE Trans. Neural Networks, 1(1), 81-92; Leung, C.-S., Chan, L.-W., and Lai, E. (1995). Stability, capacity and statistical dynamics of second-order bidirectional associative memory. [EEE Trans. Syst. Man Cybern., 25(10), 1414-1424]. The improved performance of CB retrieval is shown by a combinatorial analysis of the first step and by simulation experiments: it allows very efficient hetero- associative mapping, as well as auto-associative completion for sparse patterns the experimentally achieved information efficiency is close to the asymptotic bound. The different retrieval methods in the hetero-associative Willshaw matrix are discussed as Boolean linear optimization problems. The improved BAM model opens interesting new perspectives, for instance, in information retrieval it allows efficient data access providing segmentation of ambiguous user input, relevance feedback and relevance ranking. Finally, we discuss BAM models as functional models for reciprocal cortico-cortical pathways, and the implication of this for a more flexible version of Hebbian cell-assemblies.},
author = {Sommer, Friedrich T and Palm, G{\"{u}}nther},
doi = {10.1016/S0893-6080(98)00125-7},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sommer, Palm - 1999 - Improved bidirectional retrieval of sparse patterns stored by Hebbian learning.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {Bidirectional associative memory,Cell-assemblies,Combinatorial analysis,Hebbian learning,Iterative retrieval,Neural information retrieval},
number = {2},
pages = {281--297},
pmid = {12662704},
title = {{Improved bidirectional retrieval of sparse patterns stored by Hebbian learning}},
volume = {12},
year = {1999}
}
@article{Valembois2004,
abstract = {The main reference of this paper is the sphere-packing bound of 1967 (SP67) derived by Shannon, Gallager, and Berlekamp. It offers a lower bound on the decoding error probability over a very large variety of channels. If it has failed so far to provide any usable material in practical implementation of telecommunication systems, it is due to an original focus on asymptotic results (making it inapplicable for moderate code lengths) and to the difficulty of the involved methods (which makes the derivation of SP67 quite hermetic and uninspiring for further research). The purpose of this paper is two-fold: 1) to stir up some renewed interest in the topic on which Shannon concluded his career in information theory thanks to a qualitative (rather than technical) review of the derivation of SP67, introduced by a review of the simpler sphere-packing bound derived by Shannon in 1959; 2) to prove the practical interest of SP67 by extending its field of application to continuous output channels and particularly the additive white Gaussian noise (AWGN) channel used with any particular modulation scheme, and by improving its lower bound for the moderate code length case so that it becomes the best lower bound for most iteratively decodable codes (turbo codes, low-density parity-check (LDPC) codes, repeat-accumulate (RA) codes, etc.) of usual lengths. ER -},
author = {Valembois, Antoine and Fossorier, Marc P C},
doi = {10.1109/TIT.2004.838090},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Valembois, Fossorier - 2004 - Sphere-packing bounds revisited for moderate block lengths.pdf:pdf},
isbn = {0018-9448},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Additive white Gaussian noise (AWGN),Block codes,Lower bound on error probability},
number = {12},
pages = {2998--3014},
title = {{Sphere-packing bounds revisited for moderate block lengths}},
volume = {50},
year = {2004}
}
@article{Wyner1998,
author = {Wyner, Aaron D and Shamai, S},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wyner, Shamai - 1998 - Introduction to “ Communication in the Presence of Noise ” by C . E . Shannon.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
number = {2},
pages = {442--446},
title = {{Introduction to “ Communication in the Presence of Noise ” by C . E . Shannon}},
volume = {86},
year = {1998}
}
@misc{Abramowitz1965,
abstract = {Numerical tables of mathematical functions are in continual demand by scientists and engineers for preliminary surveys of problems before programming for computing machines. This handbook was designed to provide scientific investigators with a comprehensive and self-contained summary of the mathematical functions that arise in physical and engineering problems. The chapters contain numerical tables, graphs, polynomial or rational approximations for automatic computers, and statements of the principal mathematical properties of the tabulated functions. Many numerical examples are given to illustrate the use of the tables and also the computation of function values which lie outside their range. At the end of each chapter is a short list of references in which proofs of the properties may be found and the more important numerical tables.},
author = {Abramowitz, Milton and Stegun, Irene a. and Miller, David},
booktitle = {Journal of Applied Mechanics},
doi = {10.1115/1.3625776},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abramowitz, Stegun, Miller - 1965 - Handbook of Mathematical Functions With Formulas, Graphs and Mathematical Tables (National Bureau of.pdf:pdf},
isbn = {0486612724},
issn = {00218936},
number = {1},
pages = {239},
pmid = {20699170},
title = {{Handbook of Mathematical Functions With Formulas, Graphs and Mathematical Tables (National Bureau of Standards Applied Mathematics Series No. 55)}},
volume = {32},
year = {1965}
}
@article{Ben-haim2008,
author = {Ben-haim, Yael and Litsyn, Simon and Member, Senior},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ben-haim, Litsyn, Member - 2008 - Improved Upper Bounds on the Reliability Function of the Gaussian Channel.pdf:pdf},
number = {1},
pages = {5--12},
title = {{Improved Upper Bounds on the Reliability Function of the Gaussian Channel}},
volume = {54},
year = {2008}
}
@article{Cover1975,
abstract = {Letp(y{\_}1, y{\_}2 mid x)denote a discrete memoryless channel with a single sourceXand two independent receiversY{\_}1andY{\_}2. We exhibit an achievable region of rates(R{\_}{\{}11{\}},R{\_}{\{}12{\}},R{\_}{\{}22{\}})at which independent information can be sent, respectively, to receiver 1, to both receivers 1 and 2, and to receiver 2. The achievability of the region is shown by using a version of the asymptotic equipartition property involving many simultaneous "typicality" constraints. These results immediately generalize to yield an achievable rate region for them-sendern-receiver channel in terms of standard mutual information quantities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1501.07417v1},
author = {Cover, Thomas M},
doi = {10.1109/TIT.1975.1055418},
eprint = {arXiv:1501.07417v1},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cover - 1975 - An Achievable Rate Region for the Broadcast Channel.pdf:pdf},
isbn = {9781467377041},
issn = {15579654},
journal = {IEEE Transactions on Information Theory},
number = {4},
pages = {399--404},
title = {{An Achievable Rate Region for the Broadcast Channel}},
volume = {21},
year = {1975}
}
@article{Feinstein1954,
author = {Feinstein, A.},
doi = {10.1109/TIT.1954.1057459},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Feinstein - 1954 - A new basic theorem of information theory.pdf:pdf},
issn = {2168-2690},
journal = {Transactions of the IRE Professional Group on Information Theory},
keywords = {communication theory,entropy,foundations of statistical mechanics,information theory,mathematics,probability theory,quantitative measure of uncertainty},
number = {4},
pages = {2--22},
title = {{A new basic theorem of information theory}},
volume = {4},
year = {1954}
}
@article{Isely,
abstract = {A new algorithm is proposed for a) unsupervised learning of sparse representations from subsampled measurements and b) estimating the parameters required for linearly reconstructing signals from the sparse codes. We verify that the new algorithm performs efficient data compression on par with the recent method of compressive sampling. Further, we demonstrate that the algorithm performs robustly when stacked in several stages or when applied in undercomplete or overcomplete situations. The new algorithm can explain how neural populations in the brain that receive subsampled input through fiber bottlenecks are able to form coherent response properties.},
archivePrefix = {arXiv},
arxivId = {1011.0241},
author = {Isely, Guy and Hillar, Christopher J and Sommer, Friedrich T},
eprint = {1011.0241},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Isely, Hillar, Sommer - 2011 - Deciphering subsampled data adaptive compressive sampling as a principle of brain communication.pdf:pdf},
isbn = {9781617823800},
journal = {Advances in Neural Information Processing Systems 23},
keywords = {Neurons and Cognition,Quantitative Methods},
pages = {910--918},
pmid = {2457268},
title = {{Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication}},
url = {http://arxiv.org/abs/1011.0241{\%}5Cnhttp://books.nips.cc/papers/files/nips23/NIPS2010{\_}1099.pdf},
year = {2011}
}
@article{Parisien,
abstract = {In cortical neural networks, connections from a given neuron are either inhibitory or excitatory but not both. This constraint is often ignored by theoreticians who build models of these systems. There is currently no general solution to the problem of converting such unrealistic network models into biologically plausible models that respect this constraint. We demonstrate a constructive transformation of models that solves this problem for both feedforward and dynamic recurrent networks. The resulting models give a close approximation to the original network functions and temporal dynamics of the system, and they are biologically plausible. More precisely, we identify a general form for the solution to this problem. As a result, we also describe how the precise solution for a given cortical network can be determined empirically.},
author = {Parisien, Christopher and Anderson, Charles H and Eliasmith, Chris},
doi = {10.1162/neco.2008.07-06-295},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Parisien, Anderson, Eliasmith - 2008 - Solving the Problem of Negative Synaptic Weights in Cortical Models.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
number = {6},
pages = {1473--1494},
pmid = {18254696},
title = {{Solving the Problem of Negative Synaptic Weights in Cortical Models}},
volume = {20},
year = {2008}
}
@article{Sciences,
abstract = {This paper shows how sparse, high-dimensional probability distributions could be represented by neurons with exponential compression. The representation is a novel application of compressive sensing to sparse probability distributions rather than to the usual sparse signals. The compressive measurements correspond to expected values of nonlinear functions of the probabilistically distributed vari- ables. When these expected values are estimated by sampling, the quality of the compressed representation is limited only by the quality of sampling. Since the compression preserves the geometric structure of the space of sparse probability distributions, probabilistic computation can be performed in the compressed do- main. Interestingly, functions satisfying the requirements of compressive sensing can be implemented as simple perceptrons. If we use perceptrons as a simple model of feedforward computation by neurons, these results show that the mean activity of a relatively small number of neurons can accurately represent a high- dimensional joint distribution implicitly, even without accounting for any noise correlations. This comprises a novel hypothesis for how neurons could encode probabilities in the brain.},
archivePrefix = {arXiv},
arxivId = {1206.1800},
author = {Pitkow, Xaq},
eprint = {1206.1800},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pitkow - 2012 - Compressive neural representation of sparse, high-dimensional probabilities.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1--9},
title = {{Compressive neural representation of sparse, high-dimensional probabilities}},
year = {2012}
}
@phdthesis{Plate1994,
abstract = {Distributed representations are attractive for a number of reasons. They offer the pos- sibility of representing concepts in a continuous space, they degrade gracefully with noise, and they can be processed in a parallel network of simple processing elements. However, the problem of representing nested structure in distributed representations has been for some time aprominent concern of both proponents and critics of connectionism [Fodor and Pylyshyn 1988; Smolensky 1990; Hinton 1990]. The lack of connectionist representations for complex structure has held back progress in tackling higher-level cognitive tasks such as language understanding and reasoning. In this thesis I review connectionist representations and propose a method for the distributed representation of nested structure, which I call “Holographic Reduced Rep- resentations” (HRRs). HRRs provide an implementation of Hinton's [1990] “reduced descriptions”. HRRs use circular convolution to associate atomic items, which are repre- sented by vectors. Arbitrary variable bindings, short sequences of various lengths, and predicates can be represented in a fixed-width vector. These representations are items in their own right, and can be used in constructing compositional structures. The noisy re- constructions extracted from convolution memories can be cleaned up by using a separate associative memory that has good reconstructive properties. Circular convolution, which is the basic associative operator for HRRs, can be built into a recurrent neural network. The network can store and produce sequences. I show that neural network learning techniques can be used with circular convolution in order to learn representations for items and sequences. Oneof the attractions of connectionist representations of compositional structures is the possibility of computing without decomposing structures. I show that it is possible to use dot-product comparisons ofHRRsfor nested structures to estimate the analogical similarity of the structures. This demonstrates how the surface form of connectionist representations can reflect underlying structural similarity and alignment.},
author = {Plate, Tony A.},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Plate - 1994 - Distributed Representations and Nested Compositional Structure.pdf:pdf},
isbn = {0-315-97247-5},
pages = {216},
title = {{Distributed Representations and Nested Compositional Structure}},
year = {1994}
}
@inproceedings{Rahimi2016,
abstract = {The mathematical properties of high-dimensional (HD) spaces showremarkable agreement with behaviors controlled by the brain. Computing with HD vectors, referred to as “hypervectors,” is a brain-inspired alternative to computing with numbers. Hypervec- tors are high-dimensional, holographic, and (pseudo)random with independent and identically distributed (i.i.d.) components. They provide for energy-efficient computing while tolerating hardware variation typical of nanoscale fabrics. We describe a hardware ar- chitecture for a hypervector-based classifier and demonstrate it with language identification from letter trigrams. The HD classifier is 96.7{\%} accurate, 1.2{\%} lower than a conventional machine learning method, operating with half the energy. Moreover, the HD classi- fier is able to tolerate 8.8-fold probability of failure of memory cells while maintaining 94{\%} accuracy. This robust behavior with erro- neous memory cells can significantly improve energy efficiency},
author = {Rahimi, Abbas and Kanerva, Pentti and Rabaey, Jan M},
booktitle = {Proceedings of the 2016 International Symposium on Low Power Electronics and Design - ISLPED '16},
doi = {10.1145/2934583.2934624},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rahimi, Kanerva, Rabaey - 2016 - A Robust and Energy-Efficient Classifier Using Brain-Inspired Hyperdimensional Computing.pdf:pdf},
isbn = {9781450341851},
pages = {64--69},
title = {{A Robust and Energy-Efficient Classifier Using Brain-Inspired Hyperdimensional Computing}},
year = {2016}
}
@article{Saul2012,
abstract = {Many networks of scientific interest naturally decompose into clusters or communities with comparatively fewer external than internal links; however, current Bayesian models of network communities do not exert this intuitive notion of communities. We formulate a nonparametric Bayesian model for community detection consistent with an intuitive definition of communities and present a Markov chain Monte Carlo procedure for inferring the community structure. A Matlab toolbox with the proposed inference procedure is available for download. On synthetic and real networks, our model detects communities consistent with ground truth, and on real networks, it outperforms existing approaches in predicting missing links. This suggests that community structure is an important structural property of networks that should be explicitly modeled.},
author = {Saul, Lawrence K},
doi = {10.1162/NECO},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Saul - 2012 - Un c orr ect Un c ect ed Pro of.pdf:pdf},
issn = {1530-888X},
journal = {Neural Computation},
pages = {1--23},
pmid = {23517101},
title = {{Un c orr ect Un c ect ed Pro of}},
volume = {31},
year = {2012}
}
@article{Shannon1956,
abstract = {The zero error capacity{\textless}tex{\textgreater}C{\_}o{\textless}/tex{\textgreater}of a noisy channel is defined as the least upper bound of rates at which it is possible to transmit information with zero probability of error. Various properties of{\textless}tex{\textgreater}C{\_}o{\textless}/tex{\textgreater}are studied; upper and lower bounds and methods of evaluation of{\textless}tex{\textgreater}C{\_}o{\textless}/tex{\textgreater}are given. Inequalities are obtained for the{\textless}tex{\textgreater}C{\_}o{\textless}/tex{\textgreater}relating to the "sum" and "product" of two given channels. The analogous problem of zero error capacity{\textless}tex{\textgreater}C{\_}oF{\textless}/tex{\textgreater}for a channel with a feedback link is considered. It is shown that while the ordinary capacity of a memoryless channel with feedback is equal to that of the same channel without feedback, the zero error capacity may be greater. A solution is given to the problem of evaluating{\textless}tex{\textgreater}C{\_}oF{\textless}/tex{\textgreater}.},
author = {Shannon, Claude E.},
doi = {10.1109/TIT.1956.1056798},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shannon - 1956 - The zero error capacity of a noisy channel.pdf:pdf},
isbn = {0096-1000},
issn = {00961000},
journal = {IRE Trans. Inf. Theory},
keywords = {Block codes,Capacity planning,Coding,Decoding,Feedback,Feedback communication,Filling,Information analysis,Laboratories,Memoryless channels,Memoryless systems,Reactive power,Telephony,Upper bound},
number = {3},
pages = {8--19},
title = {{The zero error capacity of a noisy channel}},
volume = {2},
year = {1956}
}
@article{Gallant2013,
author = {Gallant, Stephen I. and Okaywe, T. Wendy},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gallant, Okaywe - 2013 - Representing Objects, Relations, and Sequences.pdf:pdf},
journal = {Neural Computation},
month = {aug},
number = {8},
pages = {2038--2078},
title = {{Representing Objects, Relations, and Sequences}},
volume = {25},
year = {2013}
}
@article{Jonas2016,
author = {Jonas, Eric and Kording, Konrad Paul},
doi = {10.1101/055624},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jonas, Kording - 2016 - Could a neuroscientist understand a microprocessor.pdf:pdf},
keywords = {berkeley,chicago,computation,computer science,illinois,institute of chicago,neuroscience,northwestern university and rehabilitation,of electrical engineering and,of physical medicine and,rehabilitation,reverse-engineering,university of california},
number = {1},
pages = {1--5},
title = {{Could a neuroscientist understand a microprocessor ?}},
volume = {XXI},
year = {2016}
}
@misc{Kleyko2016,
abstract = {This article proposes the use of Vector Symbolic Architectures for implementing Hierarchical Graph Neuron, an architecture for memorizing patterns of generic sensor stimuli. The adoption of a Vector Symbolic representation ensures a one-layered design for the approach, while maintaining the previously reported properties and performance characteristics of Hierarchical Graph Neuron, and also improving the noise resistance of the architecture. The proposed architecture enables a linear (with respect to the number of stored entries) time search for an arbitrary sub-pattern.},
archivePrefix = {arXiv},
arxivId = {1501.03784},
author = {Kleyko, Denis and Osipov, Evgeny and Senior, Alexander and Khan, Asad I and Sekercioglu, Yasar Ahmet},
booktitle = {IEEE Transactions on Neural Networks and Learning Systems},
doi = {10.1109/TNNLS.2016.2535338},
eprint = {1501.03784},
file = {:home/epaxon/Research/Redwood/Kanerva/References/2016{\_}IEEE{\_}TNNLS{\_}HoloGN.pdf:pdf},
issn = {21622388},
pages = {1--13},
title = {{Holographic Graph Neuron: A Bioinspired Architecture for Pattern Processing}},
year = {2016}
}
@inproceedings{Arathorn,
author = {Arathorn, D.},
booktitle = {33rd Applied Imagery Pattern Recognition Workshop (AIPR'04)},
doi = {10.1109/AIPR.2004.20},
isbn = {0-7695-2250-5},
pages = {73--78},
publisher = {IEEE},
title = {{Computation in the Higher Visual Cortices: Map-Seeking Circuit Theory and Application to Machine Vision}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1409678}
}
@book{Arathorn2002map,
address = {Stanford},
author = {Arathorn, David W.},
publisher = {Stanford Press},
title = {{Map Seeking Circuits: A Computational Mechanism for Biological and Machine Vision.}},
year = {2002}
}
@article{Arathorn2014,
author = {Arathorn, David W},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arathorn - 2014 - Map-Seeking Circuit ( MSC ).pdf:pdf},
title = {{Map-Seeking Circuit ( MSC )}},
year = {2014}
}
@article{Maass2002,
author = {Maass, Wolfgang and Natschl{\"{a}}ger, Thomas and Markram, Henry},
doi = {10.1162/089976602760407955},
file = {:home/epaxon/Research/Redwood/Kanerva/References/maas2002liquidStateMachines.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
month = {nov},
number = {11},
pages = {2531--2560},
title = {{Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations}},
volume = {14},
year = {2002}
}
@article{Schafer2007b,
abstract = {Recurrent Neural Networks (RNN) have been developed for a better understanding and analysis of open dynamical systems. Still the question often arises if RNN are able to map every open dynamical system, which would be desirable for a broad spectrum of applications. In this article we give a proof for the universal approximation ability of RNN in state space model form and even extend it to Error Correction and Normalized Recurrent Neural Networks.},
author = {Sch{\"{a}}fer, Anton Maximilian and Zimmermann, Hans-Georg},
doi = {10.1142/S0129065707001111},
issn = {0129-0657},
journal = {International journal of neural systems},
month = {aug},
number = {4},
pages = {253--63},
pmid = {17696290},
title = {{Recurrent Neural Networks are universal approximators.}},
volume = {17},
year = {2007}
}
@book{Plate2003hrr,
author = {Plate, Tony A.},
isbn = {978-1575864303},
publisher = {Stanford: CSLI Publications},
title = {{Holographic Reduced Representation: Distributed Representation for Cognitive Structures}},
year = {2003}
}
@article{Snaider2012,
abstract = {Sparse distributed memory is an auto associative memory system that stores high dimensional Boolean vectors. Here we present an extension of the original SDM, the Integer SDM that uses modular arithmetic integer vectors rather than binary vectors. This extension preserves many of the desirable properties of the original SDM: auto associativity, content addressability, distributed storage, and robustness over noisy inputs. In addition, it improves the representation capabilities of the memory and is more robust over normali zation. It can also be extended to support forgetting and re liable sequence storage.},
author = {Snaider, Javier and Franklin, Stan},
doi = {10.3233/978-1-60750-959-2-351},
isbn = {9781607509585},
issn = {18669956},
journal = {Proceedings of the Twenty-Fifth International Florida Artificial Intelligence Research Society Conference},
keywords = {General Conference Short Papers},
pages = {136--139},
title = {{Integer Sparse Distributed Memory}},
year = {2012}
}
@article{,
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Introduction Compositional representations Binding operation Defining properties.pdf:pdf},
pages = {1--8},
title = {{Introduction Compositional representations Binding operation Defining properties}}
}
@article{Seung1993a,
abstract = {In many neural systems, sensory information is distributed throughout a population of neurons. We study simple neural network models for extracting this information. The inputs to the networks are the stochastic responses of a population of sensory neurons tuned to directional stimuli. The performance of each network model in psychophysical tasks is compared with that of the optimal maximum likelihood procedure. As a model of direction estimation in two dimensions, we consider a linear network that computes a population vector. Its performance depends on the width of the population tuning curves and is maximal for width, which increases with the level of background activity. Although for narrowly tuned neurons the performance of the population vector is significantly inferior to that of maximum likelihood estimation, the difference between the two is small when the tuning is broad. For direction discrimination, we consider two models: a perceptron with fully adaptive weights and a network made by adding an adaptive second layer to the population vector network. We calculate the error rates of these networks after exhaustive training to a particular direction. By testing on the full range of possible directions, the extent of transfer of training to novel stimuli can be calculated. It is found that for threshold linear networks the transfer of perceptual learning is nonmonotonic. Although performance deteriorates away from the training stimulus, it peaks again at an intermediate angle. This nonmonotonicity provides an important psychophysical test of these models.},
author = {Seung, H. Sabastian and Sompolinsky, Haim},
isbn = {1074910753},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Afferent,Afferent: physiology,Animals,Humans,Likelihood Functions,Models,Nerve Net,Neurons,Orientation,Orientation: physiology,Perception,Perception: physiology,Stochastic Processes,Theoretical},
month = {nov},
number = {22},
pages = {10749--53},
pmid = {8248166},
title = {{Simple models for reading neuronal population codes.}},
volume = {90},
year = {1993}
}
@article{Sussillo2009,
abstract = {Neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output. How are these two forms of activity related? We develop a procedure called FORCE learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns. FORCE learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning. Using this approach, we construct networks that produce a wide variety of complex output patterns, input-output transformations that require memory, multiple outputs that can be switched by control inputs, and motor patterns matching human motion capture data. Our results reproduce data on premovement activity in motor and premotor cortex, and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than generally appreciated.},
author = {Sussillo, David and Abbott, L F},
doi = {10.1016/j.neuron.2009.07.018},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sussillo, Abbott - 2009 - Generating coherent patterns of activity from chaotic neural networks.pdf:pdf},
issn = {1097-4199},
journal = {Neuron},
keywords = {Action Potentials,Action Potentials: physiology,Feedback,Humans,Neural Networks (Computer),Neuronal Plasticity,Neuronal Plasticity: physiology,Nonlinear Dynamics,Physiological,Physiological: physiology},
month = {aug},
number = {4},
pages = {544--57},
pmid = {19709635},
publisher = {Elsevier Ltd},
title = {{Generating coherent patterns of activity from chaotic neural networks.}},
volume = {63},
year = {2009}
}
@article{Olshausen1996,
abstract = {Natural images contain characteristic statistical regularities that set them apart from purely random images. Understanding what these regularities are can enable natural images to be coded more efficiently. In this paper, we describe some of the forms of structure that are contained in natural images, and we show how these are related to the response properties of neurons at early stages of the visual system. Many of the important forms of structure require higher-order (i.e. more than linear, pairwise) statistics to characterize, which makes models based on linear Hebbian learning, or principal components analysis, inappropriate for finding efficient codes for natural images. We suggest that a good objective for an efficient coding of natural scenes is to maximize the sparseness of the representation, and we show that a network that learns sparse codes of natural scenes succeeds in developing localized, oriented, bandpass receptive fields similar to those in the mammalian striate cortex.},
author = {Olshausen, Bruno A. and Field, David J.},
doi = {10.1088/0954-898X/7/2/014},
file = {:home/epaxon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Olshausen, Field - 1996 - Natural image statistics and efficient coding.pdf:pdf},
issn = {0954-898X},
journal = {Network (Bristol, England)},
month = {may},
number = {2},
pages = {333--9},
pmid = {16754394},
title = {{Natural image statistics and efficient coding.}},
volume = {7},
year = {1996}
}
@article{Eliasmith2012spaun,
abstract = {A central challenge for cognitive and systems neuroscience is to relate the incredibly complex behavior of animals to the equally complex activity of their brains. Recently described, large-scale neural models have not bridged this gap between neural activity and biological function. In this work, we present a 2.5-million-neuron model of the brain (called "Spaun") that bridges this gap by exhibiting many different behaviors. The model is presented only with visual image sequences, and it draws all of its responses with a physically modeled arm. Although simplified, the model captures many aspects of neuroanatomy, neurophysiology, and psychological behavior, which we demonstrate via eight diverse tasks.},
author = {Eliasmith, Chris and Stewart, Terrence C and Choo, Xuan and Bekolay, Trevor and DeWolf, Travis and Tang, Yichuan and Tang, Charlie and Rasmussen, Daniel},
doi = {10.1126/science.1225266},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
keywords = {Behavior,Brain,Brain: anatomy {\&} histology,Brain: physiology,Humans,Models,Neural Networks (Computer),Neurological,Software},
month = {nov},
number = {6111},
pages = {1202--5},
pmid = {23197532},
title = {{A large-scale model of the functioning brain.}},
volume = {338},
year = {2012}
}
