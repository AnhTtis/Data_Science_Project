
@article{chau2020,
  author    = {Ho Yin Chau and
               Frank Qiu and
               Yubei Chen and
               Bruno A. Olshausen},
  title     = {Disentangling images with Lie group transformations and sparse coding},
  journal   = {CoRR},
  volume    = {abs/2012.12071},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.12071},
  eprinttype = {arXiv},
  eprint    = {2012.12071},
  timestamp = {Tue, 05 Jan 2021 16:02:31 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-12071.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{renner2022scene,
  doi = {10.48550/ARXIV.2208.12880},
  url = {https://arxiv.org/abs/2208.12880},
  author = {Renner, Alpha and Supic, Lazar and Danielescu, Andreea and Indiveri, Giacomo and Olshausen, Bruno A. and Sandamirskaya, Yulia and Sommer, Friedrich T. and Frady, E. Paxon},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Neural and Evolutionary Computing (cs.NE), Image and Video Processing (eess.IV), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, I.4.8},
  title = {Neuromorphic Visual Scene Understanding with Resonator Networks},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{renner2022vo,
  doi = {10.48550/ARXIV.2209.02000},
  url = {https://arxiv.org/abs/2209.02000},
  author = {Renner, Alpha and Supic, Lazar and Danielescu, Andreea and Indiveri, Giacomo and Frady, E. Paxon and Sommer, Friedrich T. and Sandamirskaya, Yulia},
  keywords = {Robotics (cs.RO), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences, I.4.9},
  title = {Neuromorphic Visual Odometry with Resonator Networks},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@book{mitchell1980need,
  title={The need for biases in learning generalizations},
  author={Mitchell, Tom M},
  year={1980},
  publisher={Citeseer}
}
@article{shorten2019survey,
  title={A survey on image data augmentation for deep learning},
  author={Shorten, Connor and Khoshgoftaar, Taghi M},
  journal={Journal of big data},
  volume={6},
  number={1},
  pages={1--48},
  year={2019},
  publisher={SpringerOpen}
}
@article{Abbott1990,
abstract = {A model of neuronal behaviour capable of accounting for the oscill{\&} tory, plateau and rebound properties of biological neurons is derived, discussed and analysed. The model is based on a piecewise linear form of the FitzHugh-Nagumo equations, but reduces t.o a set of maps very similar to those of the Hopfield model. In particular, the binary descript.ion of individual neurons and the well studied form of the synaptic current J;, S, are preserved, although the model is capable of re-producing behaviours on the slow timescales characteristic of plateau and oscillation. By coupling two model cells together a mutually inhibitory or half-centred oscillator and an oscillator, fixed-phase follower pairs are constructed. The behaviour of a net-work of oscillatory cells is analysed with particular attention to phase-locking. The response of a single cell to a square wave input provides a mean-field approximation for large networks. This approach is compared with the results of a phase-coupling description of the oscillators. The network of oscillators discussed can be used to con-struct associative memories in which the signal for memory recall is not fixed-point behaviour but phase locking. The performance and capacity of such phase-locking memories is analysed.},
author = {Abbott, L F},
doi = {10.1088/0305-4470/23/16/028},
file = {:home/epaxon/Documents/Mendeley Desktop/Abbott - 1990 - A network of oscillators.pdf:pdf},
issn = {0305-4470},
journal = {Journal of Physics A: Mathematical and General},
number = {16},
pages = {3835--3859},
title = {{A network of oscillators}},
url = {http://iopscience.iop.org/0305-4470/23/16/028{\%}5Cnhttp://stacks.iop.org/0305-4470/23/i=16/a=028?key=crossref.aec50c4e47c9a65f1eef01d2d962ba58},
volume = {23},
year = {1990}
}
@article{Abbott1993,
author = {Abbott, L. F. and van Vreeswijk, Carl},
file = {:home/epaxon/Documents/Mendeley Desktop/Abbott, van Vreeswijk - 1993 - Asynchronous states in networks of pulse-coupled oscillators.pdf:pdf},
journal = {Physical Review E},
number = {2},
title = {{Asynchronous states in networks of pulse-coupled oscillators}},
volume = {48},
year = {1993}
}
@article{Abu-Mostafa1985,
abstract = {The information capacity of general forms of memory is formalized. The number of bits of information that can be stored in the Hopfield model of associative memory is estimated. It is found that the asymptotic information capacity of a Hopfield network ofNneurons is of the orderN{\^{}}{\{}3{\}}b. The number of arbitrary state vectors that can be made stable in a Hopfield network ofNneurons is proved to be bounded above byN.},
author = {Abu-Mostafa, Yaser S. and {St. Jacques}, Jeannine Marie},
doi = {10.1109/TIT.1985.1057069},
file = {:home/epaxon/Documents/Mendeley Desktop/Abu-Mostafa, St. Jacques - 1985 - Information Capacity of the Hopfield Model.pdf:pdf},
issn = {15579654},
journal = {IEEE Transactions on Information Theory},
number = {4},
pages = {461--464},
title = {{Information Capacity of the Hopfield Model}},
volume = {31},
year = {1985}
}
@incollection{Adelson1999,
address = {Cambridge, MA},
author = {Adelson, E. H.},
booktitle = {The Cognitive Neurosciences1},
editor = {Gazzaniga, M. S.},
publisher = {MIT Press},
title = {{Lightness perception and lightness illusions.}},
year = {1999}
}
@article{Ahissar1997,
abstract = {The temporally encoded information obtained by vibrissal touch could be decoded "passively," involving only input-driven elements, or "actively," utilizing intrinsically driven oscillators. A previous study suggested that the trigeminal somatosensory system of rats does not obey the bottom-up order of activation predicted by passive decoding. Thus, we have tested whether this system obeys the predictions of active decoding. We have studied cortical single units in the somatosensory cortices of anesthetized rats and guinea pigs and found that about a quarter of them exhibit clear spontaneous oscillations, many of them around whisking frequencies ( approximately 10 Hz). The frequencies of these oscillations could be controlled locally by glutamate. These oscillations could be forced to track the frequency of induced rhythmic whisker movements at a stable, frequency-dependent, phase difference. During these stimulations, the response intensities of multiunits at the thalamic recipient layers of the cortex decreased, and their latencies increased, with increasing input frequency. These observations are consistent with thalamocortical loops implementing phase-locked loops, circuits that are most efficient in decoding temporally encoded information like that obtained by active vibrissal touch. According to this model, and consistent with our results, populations of thalamic "relay" neurons function as phase "comparators" that compare cortical timing expectations with the actual input timing and represent the difference by their population output rate.},
author = {Ahissar, E and Haidarliu, S and Zacksenhouse, M},
doi = {10.1073/pnas.94.21.11633},
file = {:home/epaxon/Documents/Mendeley Desktop/Ahissar, Haidarliu, Zacksenhouse - 1997 - Decoding temporally encoded sensory input by cortical oscillations and thalamic phase comparat.pdf:pdf},
isbn = {0027-8424 (Print)$\backslash$r0027-8424 (Linking)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {21},
pages = {11633--11638},
pmid = {9326662},
title = {{Decoding temporally encoded sensory input by cortical oscillations and thalamic phase comparators}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.94.21.11633},
volume = {94},
year = {1997}
}
@book{Aizenberg2011,
abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Aizenberg, Igor},
booktitle = {Studies in Computational Intelligence},
doi = {10.1007/978-3-642-20353-4_1},
eprint = {arXiv:1011.1669v3},
file = {:home/epaxon/Documents/Mendeley Desktop/Aizenberg - 2011 - Complex-valued neural networks with multi-valued neurons.pdf:pdf},
isbn = {9783642203527},
issn = {1860949X},
pages = {1--277},
pmid = {15991970},
title = {{Complex-valued neural networks with multi-valued neurons}},
volume = {353},
year = {2011}
}
@article{Amari1977,
abstract = {The dynamics of pattern formation is studied for lateral-inhibition type homogeneous neural fields with general connections. Neural fields consisting of single layer are first treated, and it is proved that there are five types of pattern dynamics. The type of the dynamics of a field depends not only on the mutual connections within the field but on the level of homogeneous stimulus given to the field. An example of the dynamics is as follows: A fixed size of localized excitation, once evoked by stimulation, can be retained in the field persistently even after the stimulation vanishes. It moves until it finds the position of the maximum of the input stimulus. Fields consisting of an excitatory and an inhibitory layer are next analyzed. In addition to stationary localized excitation, fields have such pattern dynamics as production of oscillatory waves, travelling waves, active and dual active transients, etc.},
author = {ichi Amari, Shun},
doi = {10.1007/BF00337259},
file = {:home/epaxon/Documents/Mendeley Desktop/Amari - 1977 - Dynamics of pattern formation in lateral-inhibition type neural fields.pdf:pdf},
isbn = {0340-1200 (Print)$\backslash$r0340-1200 (Linking)},
issn = {03401200},
journal = {Biological Cybernetics},
number = {2},
pages = {77--87},
pmid = {911931},
title = {{Dynamics of pattern formation in lateral-inhibition type neural fields}},
volume = {27},
year = {1977}
}
@article{Amari1978,
abstract = {The nerve cells are believed to have such ability of self-organization that, given a number of input patterns, each cell tunes itself to become re- sponsive to only one of the patterns, or to one subset of patterns having some features in common. The de- tectors of patterns or pattern subsets are formed in this manner. A simple but plausible mechanism of self- organization is proposed based on the two hypotheses : 1) Synaptic modification process is non-linear, acti- vated when the output of a cell is positive. 2) Not only excitatory but also inhibitory synapses are modifiable. A rigorous mathematical analysis is given to elucidate the characteristics of modifiable synapses to form these detectors. The present model fits well most of the experiments on the developmental plasticity of the visual cortex such as the formation of orientation detecting cells, monocular and alternate monocular deprivation in normal and abnormal environments.},
author = {ichi Amari, Shun and Takeuchi, Akikazu},
doi = {10.1007/BF00337348},
file = {:home/epaxon/Documents/Mendeley Desktop/Amari, Takeuchi - 1978 - Mathematical theory on formation of category detecting nerve cells.pdf:pdf},
isbn = {0340-1200},
issn = {03401200},
journal = {Biological Cybernetics},
number = {3},
pages = {127--136},
title = {{Mathematical theory on formation of category detecting nerve cells}},
volume = {29},
year = {1978}
}
@article{Amari1972,
abstract = {Various information-processing capabilities of self-organizing nets of threshold elements are studied. A self-organizing net, learning from patterns or pattern sequences given from outside as stimuli, "remembers" some of them as stable equilibrium states or state-transition sequences of the net. A condition where many patterns and pattern sequences are remembered in a net at the same time is shown. The stability degree of their remembrance and recalling under noise disturbances is investigated theoretically. For this purpose, the stability of state transition in an autonomous logical net of threshold elements is studied by the use of characteristics of threshold elements.},
author = {Amari, Shun-Ichi},
doi = {10.1109/T-C.1972.223477},
file = {:home/epaxon/Documents/Mendeley Desktop/Amari - 1972 - Learning patterns and pattern sequences by self-organizing nets of threshold elements.pdf:pdf},
isbn = {0018-9340},
issn = {00189340},
journal = {IEEE Transactions on Computers},
keywords = {Associative memory,brain model,concept formation,logic nets of threshold elements,self-organization,sequential recalling,stability of state transition},
number = {11},
pages = {1197--1206},
title = {{Learning patterns and pattern sequences by self-organizing nets of threshold elements}},
volume = {C-21},
year = {1972}
}
@article{Amit1988,
abstract = {It is shown that the ideas that led to neural networks capable of recalling associatively and asynchronously temporal sequences of patterns can be extended to produce a neural network that automatically counts the cardinal number in a sequence of identical external stimuli. The network is explicitly constructed, analyzed, and simulated. Such a network may account for the cognitive effect of the automatic counting of chimes to tell the hour. A more general implication is that different electrophysiological responses to identical stimuli, at certain stages of cortical processing, do not necessarily imply synaptic modification, a la Hebb. Such differences may arise from the fact that consecutive identical inputs find the network in different stages of an active temporal sequence of cognitive states. These types of networks are then situated within a program for the study of cognition, which assigns the detection of meaning as the primary role of attractor neural networks rather than computation, in contrast to the parallel distributed processing attitude to the connectionist project. This interpretation is free of homunculus, as well as from the criticism raised against the cognitive model of symbol manipulation. Computation is then identified as the syntax of temporal sequences of quasi-attractors.},
author = {Amit, D J},
doi = {10.1073/pnas.85.7.2141},
file = {:home/epaxon/Documents/Mendeley Desktop/Amit - 1988 - Neural networks counting chimes.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {7},
pages = {2141--2145},
pmid = {3353371},
title = {{Neural networks counting chimes.}},
volume = {85},
year = {1988}
}
@article{Aoki2000,
author = {Aoki, Hiroyuki and Kosugi, Yukio},
file = {:home/epaxon/Documents/Mendeley Desktop/Aoki, Kosugi - 2000 - An Image Storage System Using Complex-Valued Associative Memories.pdf:pdf},
isbn = {0769507506},
issn = {10514651},
journal = {IEEE},
pages = {626--629},
title = {{An Image Storage System Using Complex-Valued Associative Memories}},
year = {2000}
}
@article{Axmacher2006,
abstract = {Cognitive functions not only depend on the localization of neural activity, but also on the precise temporal pattern of activity in neural assemblies. Synchronization of action potential discharges provides a link between large-scale EEG recordings and cellular plasticity mechanisms. Here, we focus on the role of neuronal synchronization in different frequency domains for the subsequent stages of memory formation. Recent EEG studies suggest that synchronized neural activity in the gamma frequency range (around 30-100 Hz) plays a functional role for the formation of declarative long-term memories in humans. On the cellular level, gamma synchronization between hippocampal and parahippocampal regions may induce LTP in the CA3 region of the hippocampus. In order to encode spatial locations or sequences of multiple items and to guarantee a defined temporal order of memory processing, synchronization in the gamma frequency range has to be accompanied by a stimulus-locked phase reset of ongoing theta oscillations. Simultaneous gamma- and theta-dependent plasticity leads to complex learning rules required for realistic declarative memory formation. Subsequently, consolidation of declarative memories may occur via replay of newly acquired patterns in so-called sharp wave-ripple complexes, predominantly during slow-wave sleep. These irregular bursts induce longer lasting forms of synaptic plasticity in output regions of the hippocampus and in the neocortex. In summary, synchronization of neural assemblies in different frequency ranges induces specific forms of cellular plasticity during subsequent stages of memory formation. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
author = {Axmacher, Nikolai and Mormann, Florian and Fern{\'{a}}ndez, Guillen and Elger, Christian E. and Fell, Juergen},
doi = {10.1016/j.brainresrev.2006.01.007},
file = {:home/epaxon/Documents/Mendeley Desktop/Axmacher et al. - 2006 - Memory formation by neuronal synchronization.pdf:pdf},
isbn = {0165-0173 (Print)},
issn = {01650173},
journal = {Brain Research Reviews},
keywords = {Declarative memory,Gamma oscillation,Hippocampus,Neuronal synchronization,Phase synchronization,Synaptic plasticity},
number = {1},
pages = {170--182},
pmid = {16545463},
title = {{Memory formation by neuronal synchronization}},
volume = {52},
year = {2006}
}
@article{Bair1996,
author = {Bair, Wyeth and Koch, Christof},
doi = {10.1162/neco.1996.8.6.1185},
file = {:home/epaxon/Documents/Mendeley Desktop/Bair, Koch - 1996 - Temporal Precision of Spike Trains in Extrastriate Cortex of the Behaving Macaque Monkey.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
keywords = {Sequence Learning},
language = {English},
mendeley-tags = {Sequence Learning},
month = {aug},
number = {6},
pages = {1185--1202},
publisher = {MIT Press},
title = {{Temporal Precision of Spike Trains in Extrastriate Cortex of the Behaving Macaque Monkey}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6795892},
volume = {8},
year = {1996}
}
@article{Baldi1990,
abstract = {Recent experimental findings (Gray et al. 1989; Eckhorn et al. 1988) seem to indicate that rapid oscillations and phase-lockings of different populations of cortical neurons play an important role in neural computations. In particular, global stimulus properties could be reflected in the correlated firing of spatially distant cells. Here we describe how simple coupled oscillator networks can be used to model the data and to investigate whether useful tasks can be performed by oscillator architectures. A specific demonstration is given for the problem of preattentive texture discrimination. Texture images are convolved with different sets of Gabor filters feeding into several corresponding arrays of coupled oscillators. After a brief transient, the dynamic evolution in the arrays leads to a separation of the textures by a phase labeling mechanism. The importance of noise and of long range connections is briefly discussed.},
author = {Baldi, Pierre and Meir, Ronny},
doi = {10.1162/neco.1990.2.4.458},
file = {:home/epaxon/Documents/Mendeley Desktop/Baldi, Meir - 1990 - Computing with Arrays of Coupled Oscillators An Application to Preattentive Texture Discrimination.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
pages = {458--471},
title = {{Computing with Arrays of Coupled Oscillators: An Application to Preattentive Texture Discrimination}},
volume = {2},
year = {1990}
}
@article{Baram1994,
abstract = {We present a neural network employing Hebbian storage and sparse internal coding, which is capable of memorizing and correcting sequences of binary vectors by association. A ternary version of the Kanerva memory, folded into a feedback configuration, is shown to perform the basic sequence memorization and regeneration function. The inclusion of lateral connections between the internal cells increases the network capacity considerably and facilitates the correction of individual input patterns and the detection of large errors. The introduction of higher delays in the transmission lines between the external input-output layer and the internal memory layer is shown to further improve the network's error correction capability.},
author = {Baram, Yoram},
doi = {10.1109/72.329695},
file = {:home/epaxon/Documents/Mendeley Desktop/Baram - 1994 - Memorizing Binary Vector Sequences by a Sparsely Encoded Network.pdf:pdf},
issn = {19410093},
journal = {IEEE Transactions on Neural Networks},
number = {6},
pages = {974--981},
pmid = {18267872},
title = {{Memorizing Binary Vector Sequences by a Sparsely Encoded Network}},
volume = {5},
year = {1994}
}
@article{Belatreche2007,
abstract = {This paper presents new findings in the design and application of biologically plausible neural networks based on spiking neuron models, which represent a more plausible model of real biological neurons where time is considered as an important feature for information encoding and processing in the brain. The design approach consists of an evolutionary strategy based supervised training algorithm, newly developed by the authors, and the use of different biologically plausible neuronal models. A dynamic synapse (DS) based neuron model, a biologically more detailed model, and the spike response model (SRM) are investigated in order to demonstrate the efficacy of the proposed approach and to further our understanding of the computing capabilities of the nervous system. Unlike the conventional synapse, represented as a static entity with a fixed weight, employed in conventional and SRM-based neural networks, a DS is weightless and its strength changes upon the arrival of incoming input spikes. Therefore its efficacy depends on the temporal structure of the impinging spike trains. In the proposed approach, the training of the network free parameters is achieved using an evolutionary strategy where, instead of binary encoding, real values are used to encode the static and DS parameters which underlie the learning process. The results show that spiking neural networks based on both types of synapse are capable of learning non-linearly separable data by means of spatio-temporal encoding. Furthermore, a comparison of the obtained performance with classical neural networks (multi-layer perceptrons) is presented.},
author = {Belatreche, Ammar and Maguire, Liam P. and McGinnity, Martin},
doi = {10.1007/s00500-006-0065-7},
file = {:home/epaxon/Documents/Mendeley Desktop/Belatreche, Maguire, McGinnity - 2007 - Advances in design and application of spiking neural networks.pdf:pdf},
isbn = {4428713754},
issn = {14327643},
journal = {Soft Computing},
keywords = {Dynamic synapse,Evolutionary strategy,Integrate-and-fire model,Spike response model,Spiking neurons,Supervised learning,Temporal coding},
number = {3},
pages = {239--248},
title = {{Advances in design and application of spiking neural networks}},
volume = {11},
year = {2007}
}
@article{Benchabane2011,
author = {Benchabane, A. and Bennia, A. and Charif, F.},
file = {:home/epaxon/Documents/Mendeley Desktop/Benchabane, Bennia, Charif - 2011 - Complex-valued hopfield neural network for amplitude estimation of sinusoidal signals.pdf:pdf},
journal = {GESJ: Computer Science and Telecommunications},
keywords = {amplitude estimation,hopfield neural,matched-filterbank,spectral analysis},
number = {30},
pages = {87--93},
title = {{Complex-valued hopfield neural network for amplitude estimation of sinusoidal signals}},
volume = {1},
year = {2011}
}
@article{Berry1997,
abstract = {Assessing the reliability of neuronal spike trains is fundamental to an understanding of the neural code. We measured the reproducibility of retinal responses to repeated visual stimuli. In both tiger salamander and rabbit, the retinal ganglion cells responded to random f licker with discrete, brief periods of firing. For any given cell, these firing events covered only a small fraction of the total stimulus time, often less than 5{\%}. Firing events were very reproducible from trial to trial: the timing jitter of individual spikes was as low as 1 msec, and the standard deviation in spike count was often less than 0.5 spikes. Comparing the precision of spike timing to that of the spike count showed that the timing of a firing event conveyed several times more visual information than its spike count. This sparseness and precision were general characteristics of ganglion cell responses, maintained over the broad ensemble of stimulus waveforms produced by random f licker, and over a range of contrasts. Thus, the responses of retinal ganglion cells are not properly described by a firing probability that varies continuously with the stimulus. In-stead, these neurons elicit discrete firing events that may be the fundamental coding symbols in retinal spike trains. All of our visual experience derives from sequences of action potentials traveling down the optic nerve. Many theories have been proposed to explain how these spike trains from retinal ganglion cells encode the visual world (1–4). Fundamental to such an understanding is the reproducibility of these neural symbols to repeated presentations of the same stimulus. If retinal spike trains are highly deterministic, then individual visual messages can be attached to each spike; whereas, if they are highly stochastic, then the brain must average over many spikes to obtain an equally informative visual message. As far back as 1928, Adrian (5) proposed that information about the sensory environment is conveyed in the time-varying firing rate of spiking sensory neurons—a view that has been influential to neuroscience ever since (6–8). As a result, many researchers have concentrated on estimates of the firing rate derived from averages over long time windows or multiple stimulus presentations (9, 10). Measurements of response reliability have often focused on the trial-to-trial variance in this spike count: in the visual cortex, this variance is found to be greater than the mean (11, 12), whereas similar experiments in the thalamus and retina have found variance-to-mean ratios both above and below one (13–15). The picture emerging from this work is that spike trains in the visual system are intrinsi-cally stochastic; that, at best, one can determine the instanta-neous probability that the neuron will fire, and that this firing rate depends in some smooth fashion on the sensory stimulus. However, poor reproducibility can also arise from confound-ing factors (16), such as anesthesia (13), uncontrolled eye movements (17, 18), or ongoing brain activity (19). Further-more, the response precision may depend on the stimulus. For example, a sudden step change in illumination can reproduc-ibly elicit precisely timed action potentials from retinal gan-glion cells (20–22). The importance of precise spike timing has long been appreciated in the auditory system, where it is known to convey information essential for sound localization (23). If high-precision spike trains were common also among visual neurons, their information capacity could be significantly higher than previously estimated (24–26). To assess the reproducibility of retinal responses, the retina was repeatedly presented with the same visual input and spike trains were recorded simultaneously from many ganglion cells. The isolated retina preparation eliminated any effects of anesthesia or eye movements. The stimulus consisted of spa-tially uniform illumination that flickered randomly with a Gaussian intensity distribution. This stimulus ensemble pre-sents the retina with a wide variety of temporal waveforms, which are essential for investigating the generality of retinal precision. Some experiments also provided spatial modulation, by flickering different parts of the field independently. By comparing responses to many repeats of the same stimulus, we found that individual neurons responded with brief, highly precise periods of firing, separated by intervals of complete silence. The measured spike trains were found to be qualita-tively and quantitatively inconsistent with the conventional model, in which a ganglion cell's firing rate depends smoothly on the preceding visual stimulus. These observations have important consequences for our understanding of both the neural code of the retina and visual responses in subsequent regions of the brain.},
author = {Berry, M. J. and Warland, D. K. and Meister, M.},
doi = {10.1073/pnas.94.10.5411},
file = {:home/epaxon/Documents/Mendeley Desktop/Berry, Warland, Meister - 1997 - The structure and precision of retinal spike trains.pdf:pdf},
isbn = {0027-8424},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {10},
pages = {5411--5416},
pmid = {9144251},
title = {{The structure and precision of retinal spike trains}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.94.10.5411},
volume = {94},
year = {1997}
}
@article{Bialek1992,
abstract = {Spiking neurons encode continuous, time-varying signals in sequences of identical action potentials. Relatively simple algorithms allow one to 'decode' this neural representation of sensory data to estimate the input signals. Decoding experiments provide a quantitative characterization of information transmission and computational reliability under real-time conditions. The results of these studies show that neural coding and computation in several systems approach fundamental physical and informational theoretic limits to performance. ?? 1992.},
author = {Bialek, William and Rieke, Fred},
doi = {10.1016/0166-2236(92)90005-S},
file = {:home/epaxon/Documents/Mendeley Desktop/Bialek, Rieke - 1992 - Reliability and information transmission in spiking neurons.pdf:pdf},
isbn = {0166-2236},
issn = {01662236},
journal = {Trends in Neurosciences},
number = {11},
pages = {428--434},
pmid = {1281349},
title = {{Reliability and information transmission in spiking neurons}},
volume = {15},
year = {1992}
}
@article{Bialek1991,
author = {Bialek, William and Rieke, Fred and {de Ruyter van Steveninck}, Rob R. and Warland, David},
file = {:home/epaxon/Documents/Mendeley Desktop/Bialek et al. - 1991 - Reading a Neural Code.pdf:pdf},
journal = {Science},
title = {{Reading a Neural Code}},
url = {http://science.sciencemag.org/content/252/5014/1854.full-text.pdf+html},
volume = {252},
year = {1991}
}
@article{Blair2008,
abstract = {As a rat navigates through a familiar environment, its position in space is encoded by firing rates of place cells and grid cells. Oscillatory interference models propose that this positional firing rate code is derived from a phase code, which stores the rat's position as a pattern of phase angles between velocity-modulated theta oscillations. Here we describe a three-stage network model, which formalizes the computational steps that are necessary for converting phase-coded position signals (represented by theta oscillations) into rate-coded position signals (represented by grid cells and place cells). The first stage of the model proposes that the phase-coded position signal is stored and updated by a bank of ring attractors, like those that have previously been hypothesized to perform angular path integration in the head-direction cell system. We show analytically how ring attractors can serve as central pattern generators for producing velocity-modulated theta oscillations, and we propose that such ring attractors may reside in subcortical areas where hippocampal theta rhythm is known to originate. In the second stage of the model, grid fields are formed by oscillatory interference between theta cells residing in different (but not the same) ring attractors. The model's third stage assumes that hippocampal neurons generate Gaussian place fields by computing weighted sums of inputs from a basis set of many grid fields. Here we show that under this assumption, the spatial frequency spectrum of the Gaussian place field defines the vertex spacings of grid cells that must provide input to the place cell. This analysis generates a testable prediction that grid cells with large vertex spacings should send projections to the entire hippocampus, whereas grid cells with smaller vertex spacings may project more selectively to the dorsal hippocampus, where place fields are smallest.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Blair, Hugh T. and Gupta, Kishan and Zhang, Kechen},
doi = {10.1002/hipo.20509},
eprint = {NIHMS150003},
file = {:home/epaxon/Documents/Mendeley Desktop/Blair, Gupta, Zhang - 2008 - Conversion of a phase- to a rate-coded position signal by a three-stage model of theta cells, grid cells, a.pdf:pdf},
isbn = {1098-1063 (Electronic)$\backslash$r1050-9631 (Linking)},
issn = {10509631},
journal = {Hippocampus},
keywords = {Entorhinal cortex,Head-direction cells,Hippocampus,Oscillatory interference,Ring attractor},
number = {12},
pages = {1239--1255},
pmid = {19021259},
title = {{Conversion of a phase- to a rate-coded position signal by a three-stage model of theta cells, grid cells, and place cells}},
volume = {18},
year = {2008}
}
@article{Bohte2002,
abstract = {For a network of spiking neurons that encodes information in the timing of individual spike times, we derive a supervised learning rule, SpikeProp, akin to traditional error-backpropagation. With this algorithm, we demonstrate how networks of spiking neurons with biologically reasonable action potentials can perform complex non-linear classification in fast temporal coding just as well as rate-coded networks. We perform experiments for the classical XOR problem, when posed in a temporal setting, as well as for a number of other benchmark datasets. Comparing the (implicit) number of spiking neurons required for the encoding of the interpolated XOR problem, the trained networks demonstrate that temporal coding is a viable code for fast neural information processing, and as such requires less neurons than instantaneous rate-coding. Furthermore, we find that reliable temporal computation in the spiking networks was only accomplished when using spike response functions with a time constant longer than the coding interval, as has been predicted by theoretical considerations. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
author = {Bohte, Sander M. and Kok, Joost N. and {La Poutr{\'{e}}}, Han},
doi = {10.1016/S0925-2312(01)00658-0},
file = {:home/epaxon/Documents/Mendeley Desktop/Bohte, Kok, La Poutr{\'{e}} - 2002 - Error-backpropagation in temporally encoded networks of spiking neurons.pdf:pdf},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Error-backpropagation,Spiking neurons,Temporal coding},
pages = {17--37},
pmid = {1000253838},
title = {{Error-backpropagation in temporally encoded networks of spiking neurons}},
volume = {48},
year = {2002}
}
@article{Bolding2017,
abstract = {{\textless}p{\textgreater}The ability to represent both stimulus identity and intensity is fundamental for perception. Using large-scale population recordings in awake mice, we find distinct coding strategies facilitate non-interfering representations of odor identity and intensity in piriform cortex. Simply knowing which neurons were activated is sufficient to accurately represent odor identity, with no additional information about identity provided by spike time or spike count. Decoding analyses indicate that cortical odor representations are not sparse. Odorant concentration had no systematic effect on spike counts, indicating that rate cannot encode intensity. Instead, odor intensity can be encoded by temporal features of the population response. We found a subpopulation of rapid, largely concentration-invariant responses was followed by another population of responses whose latencies systematically decreased at higher concentrations. Cortical inhibition transforms olfactory bulb output to sharpen these dynamics. Our data therefore reveal complementary coding strategies that can selectively represent distinct features of a stimulus.{\textless}/p{\textgreater}},
author = {Bolding, Kevin A and Franks, Kevin M},
doi = {10.7554/eLife.22630},
file = {:home/epaxon/Documents/Mendeley Desktop/Bolding, Franks - 2017 - Complementary codes for odor identity and intensity in olfactory cortex.pdf:pdf},
issn = {2050-084X},
journal = {eLife},
keywords = {Mouse},
pages = {e22630},
title = {{Complementary codes for odor identity and intensity in olfactory cortex}},
url = {http://elifesciences.org/lookup/doi/10.7554/eLife.22630},
volume = {6},
year = {2017}
}
@article{Borst1999,
abstract = {Information theory quantifies how much information a neural response carries about the stimulus. This can be compared to the information transferred in particular models of the stimulus-response function and to maximum possible information transfer. Such comparisons are crucial because they validate assumptions present in any neurophysiological analysis. Here we review information-theory basics before demonstrating its use in neural coding. We show how to use information theory to validate simple stimulus-response models of neural coding of dynamic stimuli. Because these models require specification of spike timing precision, they can reveal which time scales contain information in neural coding. This approach shows that dynamic stimuli can be encoded efficiently by single neurons and that each spike contributes to information transmission. We argue, however, that the data obtained so far do not suggest a temporal code, in which the placement of spikes relative to each other yields additional information.},
author = {Borst, a and Theunissen, F E},
doi = {10.1038/14731},
file = {:home/epaxon/Documents/Mendeley Desktop/Borst, Theunissen - 1999 - Information theory and neural coding.pdf:pdf},
issn = {1097-6256},
journal = {Nature neuroscience},
keywords = {Algorithms,Animals,Entropy,Humans,Information Theory,Linear Models,Nerve Net,Nerve Net: physiology,Neurons,Neurons: physiology,Probability,Reproducibility of Results},
month = {nov},
number = {11},
pages = {947--57},
pmid = {10526332},
title = {{Information theory and neural coding.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10526332},
volume = {2},
year = {1999}
}
@article{Brader2007a,
abstract = {We present a model of spike-driven synaptic plasticity inspired by exper-imental observations and motivated by the desire to build an electronic hardware device that can learn to classify complex stimuli in a semisu-pervised fashion. During training, patterns of activity are sequentially imposed on the input neurons, and an additional instructor signal drives the output neurons toward the desired activity. The network is made of integrate-and-fire neurons with constant leak and a floor. The synapses are bistable, and they are modified by the arrival of presynaptic spikes. The sign of the change is determined by both the depolarization and the state of a variable that integrates the postsynaptic action potentials. Fol-lowing the training phase, the instructor signal is removed, and the output neurons are driven purely by the activity of the input neurons weighted by the plastic synapses. In the absence of stimulation, the synapses pre-serve their internal state indefinitely. Memories are also very robust to the disruptive action of spontaneous activity. A network of 2000 input neu-rons is shown to be able to classify correctly a large number (thousands) of highly overlapping patterns (300 classes of preprocessed Latex charac-ters, 30 patterns per class, and a subset of the NIST characters data set) and to generalize with performances that are better than or comparable to those of artificial neural networks. Finally we show that the synaptic dynamics is compatible with many of the experimental observations on the induction of long-term modifications (spike-timing-dependent plas-ticity and its dependence on both the postsynaptic depolarization and the frequency of pre-and postsynaptic neurons).},
author = {Brader, Joseph M. and Senn, Walter and Fusi, Stefano},
doi = {10.1162/neco.2007.19.11.2881},
file = {:home/epaxon/Documents/Mendeley Desktop/Brader, Senn, Fusi - 2007 - Learning Real-World Stimuli in a Neural Network with Spike-Driven Synaptic Dynamics.pdf:pdf},
isbn = {neco.2007.19.11.2881},
issn = {0899-7667},
journal = {Neural Computation},
number = {11},
pages = {2881--2912},
pmid = {17883345},
title = {{Learning Real-World Stimuli in a Neural Network with Spike-Driven Synaptic Dynamics}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.2007.19.11.2881},
volume = {19},
year = {2007}
}
@incollection{Braitenberg1977,
author = {Braitenberg, V.},
booktitle = {Theoretical Approaches to Complex Systems},
doi = {10.10071978-3-642-93287-8},
editor = {Heim, R. and Palm, G{\"{u}}nther},
file = {:home/epaxon/Documents/Mendeley Desktop/Braitenberg - 1977 - Cell assemblies in the cerebral cortex.pdf:pdf},
isbn = {9783540090922},
publisher = {Springer-Verlag},
title = {{Cell assemblies in the cerebral cortex}},
year = {1977}
}
@article{Brea2013,
abstract = {Storing and recalling spiking sequences is a general problem the brain needs to solve. It is, however, unclear what type of biologically plausible learning rule is suited to learn a wide class of spatiotemporal activity patterns in a robust way. Here we consider a recurrent network of stochastic spiking neurons composed of both visible and hidden neurons. We derive a generic learning rule that is matched to the neural dynamics by minimizing an upper bound on the Kullback-Leibler divergence from the target distribution to the model distribution. The derived learning rule is consistent with spike-timing dependent plasticity in that a presynaptic spike preceding a postsynaptic spike elicits potentiation while otherwise depression emerges. Furthermore, the learning rule for synapses that target visible neurons can be matched to the recently proposed voltage-triplet rule. The learning rule for synapses that target hidden neurons is modulated by a global factor, which shares properties with astrocytes and gives rise to testable predictions.},
author = {Brea, Johanni and Senn, Walter and Pfister, Jean-Pascal},
file = {:home/epaxon/Documents/Mendeley Desktop/Brea, Senn, Pfister - 2013 - Matching recall and storage in sequence learning with spiking neural networks.pdf:pdf},
issn = {1529-2401},
journal = {The Journal of Neuroscience},
keywords = {Action Potentials,Action Potentials: physiology,Learning,Learning: physiology,Mental Recall,Mental Recall: physiology,Models,Neural Networks (Computer),Neurological,Sequence Learning,Synapses,Synapses: physiology},
mendeley-tags = {Sequence Learning},
month = {jun},
number = {23},
pages = {9565--75},
title = {{Matching recall and storage in sequence learning with spiking neural networks.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23739954},
volume = {33},
year = {2013}
}
@article{Brown2013,
abstract = {“Emergence” is an idea that has received much attention in consciousness literature, but it is difficult to find characterizations of that concept which are both specific and useful. I will precisely define and characterize a type of epistemic (“weak”) emergence and show that it is a property of some neural circuits throughout the CNS, on micro-, meso- and macroscopic levels. I will argue that possession of this property can result in profoundly altered neural dynamics on multiple levels in cortex and other systems. I will first describe emergent neural entities (ENEs) abstractly. I will then show how ENEs function specifically and concretely, and demonstrate some implications of this type of emergence for the CNS.},
author = {Brown, Steven Ravett},
doi = {10.1007/s11571-012-9229-6},
file = {:home/epaxon/Documents/Mendeley Desktop/Brown - 2013 - Emergence in the central nervous system.pdf:pdf},
isbn = {1871-4080},
issn = {18714080},
journal = {Cognitive Neurodynamics},
keywords = {Emergence,Gamma,Internal viewpoint,Mesoscopic,Microcircuit,Mushroom body,Recurrence,Recursive,Theta},
number = {3},
pages = {173--195},
pmid = {24427200},
title = {{Emergence in the central nervous system}},
volume = {7},
year = {2013}
}
@article{Buonomano1999,
abstract = {Numerous studies have suggested that the brain may encode information in the temporal firing pattern of neurons. However, little is known regarding how information may come to be temporally encoded and about the potential computational advantages of temporal coding. Here, it is shown that local inhibition may underlie the temporal encoding of spatial images. As a result of inhibition, the response of a given cell can be significantly modulated by stimulus features outside its own receptive field. Feedforward and lateral inhibition can modulate both the firing rate and temporal features, such as latency. In this article, it is shown that a simple neural network model can use local inhibition to generate temporal codes of handwritten numbers. The temporal encoding of a spatial pattern has the interesting and computationally beneficial feature of exhibiting position invariance. This work demonstrates a manner by which the nervous system may generate temporal codes and shows that temporal encoding can be used to create position-invariant codes.},
author = {Buonomano, Dean V. and Merzenich, Michael},
doi = {papers://FAFC0638-5DD4-4A81-A69F-F8A54DFE70C3/Paper/p8406},
file = {:home/epaxon/Documents/Mendeley Desktop/Buonomano, Merzenich - 1999 - A neural network model of temporal code generation and position-invariant pattern recognition.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
number = {1},
pages = {103--16},
pmid = {9950725},
title = {{A neural network model of temporal code generation and position-invariant pattern recognition.}},
url = {http://www.ncbi.nlm.nih.gov/htbin-post/Entrez/query?db=m{\&}form=6{\&}dopt=r{\&}uid=9950725{\%}5Cnhttp://mitpress.mit.edu/journal-issue-abstracts.tcl?issn=08997667{\&}volume=11{\&}issue=1{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/9950725},
volume = {11},
year = {1999}
}
@article{Burgess2007a,
abstract = {Anatomical connectivity and recent neurophysiological results imply that grid cells in the medial entorhinal cortex are the prin- cipal cortical inputs to place cells in the hippocampus. The authors pro- pose a model in which place fields of hippocampal pyramidal cells are formed by linear summation of appropriately weighted inputs from entorhinal grid cells. Single confined place fields could be formed by summing input from a modest number (10–50) of grid cells with rela- tively similar grid phases, diverse grid orientations, and a biologically plausible range of grid spacings. When the spatial phase variation in the grid-cell input was higher, multiple, and irregularly spaced firing fields were formed. These observations point to a number of possible con- straints in the organization of functional connections between grid cells and place cells},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Burgess, Neil and Barry, Caswell and O'Keefe, John},
doi = {10.1002/hipo.20327},
eprint = {NIHMS150003},
file = {:home/epaxon/Documents/Mendeley Desktop/Trygve Solstad, Moser, Gaute T. Einevoll - 2007 - From Grid Cells to Place Cells A Mathematical Model.pdf:pdf},
isbn = {1050-9631 (Print)$\backslash$r1050-9631 (Linking)},
issn = {10509631},
journal = {Hippocampus},
keywords = {campus,dendrites,entorhinal cortex,hippo-,place cells,stellate cells,theta rhythm},
month = {sep},
number = {9},
pages = {801--812},
pmid = {17598147},
title = {{An oscillatory interference model of grid cell firing}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve{\&}db=PubMed{\&}dopt=Citation{\&}list{\_}uids=17598147 http://doi.wiley.com/10.1002/hipo.20327},
volume = {17},
year = {2007}
}
@article{Burgess2007,
abstract = {Anatomical connectivity and recent neurophysiological results imply that grid cells in the medial entorhinal cortex are the prin- cipal cortical inputs to place cells in the hippocampus. The authors pro- pose a model in which place fields of hippocampal pyramidal cells are formed by linear summation of appropriately weighted inputs from entorhinal grid cells. Single confined place fields could be formed by summing input from a modest number (10–50) of grid cells with rela- tively similar grid phases, diverse grid orientations, and a biologically plausible range of grid spacings. When the spatial phase variation in the grid-cell input was higher, multiple, and irregularly spaced firing fields were formed. These observations point to a number of possible con- straints in the organization of functional connections between grid cells and place cells},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Burgess, Neil and Barry, Caswell and O'Keefe, John},
doi = {10.1002/hipo.20327},
eprint = {NIHMS150003},
file = {:home/epaxon/Documents/Mendeley Desktop/Burgess, Barry, O'Keefe - 2007 - An oscillatory interference model of grid cell firing.pdf:pdf},
isbn = {1050-9631 (Print)$\backslash$r1050-9631 (Linking)},
issn = {10509631},
journal = {Hippocampus},
keywords = {campus,dendrites,entorhinal cortex,hippo-,place cells,stellate cells,theta rhythm},
month = {sep},
number = {9},
pages = {801--812},
pmid = {17598147},
title = {{An oscillatory interference model of grid cell firing}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve{\&}db=PubMed{\&}dopt=Citation{\&}list{\_}uids=17598147 http://doi.wiley.com/10.1002/hipo.20327},
volume = {17},
year = {2007}
}
@article{Burns2011,
abstract = {Gamma-band (25-90 Hz) peaks in local field potential (LFP) power spectra are present throughout the cerebral cortex and have been related to perception, attention, memory, and disorders (e.g., schizophrenia and autism). It has been theorized that gamma oscillations provide a "clock" for precise temporal encoding and "binding" of signals about stimulus features across brain regions. For gamma to function as a clock, it must be autocoherent: phase and frequency conserved over a period of time. We computed phase and frequency trajectories of gamma-band bursts, using time-frequency analysis of LFPs recorded in macaque primary visual cortex (V1) during visual stimulation. The data were compared with simulations of random networks and clock signals in noise. Gamma-band bursts in LFP data were statistically indistinguishable from those found in filtered broadband noise. Therefore, V1 LFP data did not contain clock-like gamma-band signals. We consider possible functions for stochastic gamma-band activity, such as a synchronizing pulse signal.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Burns, S. P. and Xing, D. and Shapley, R. M.},
doi = {10.1523/JNEUROSCI.0660-11.2011},
eprint = {NIHMS150003},
file = {:home/epaxon/Documents/Mendeley Desktop/Burns, Xing, Shapley - 2011 - Is Gamma-Band Activity in the Local Field Potential of V1 Cortex a Clock or Filtered Noise.pdf:pdf},
isbn = {1529-2401 (Electronic)$\backslash$r0270-6474 (Linking)},
issn = {0270-6474},
journal = {Journal of Neuroscience},
number = {26},
pages = {9658--9664},
pmid = {21715631},
title = {{Is Gamma-Band Activity in the Local Field Potential of V1 Cortex a "Clock" or Filtered Noise?}},
url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.0660-11.2011},
volume = {31},
year = {2011}
}
@article{Butts2007,
abstract = {The timing of action potentials relative to sensory stimuli can be precise down to milliseconds in the visual system, even though the relevant timescales of natural vision are much slower. The existence of such precision contributes to a fundamental debate over the basis of the neural code and, specifically, what timescales are important for neural computation. Using recordings in the lateral geniculate nucleus, here we demonstrate that the relevant timescale of neuronal spike trains depends on the frequency content of the visual stimulus, and that 'relative', not absolute, precision is maintained both during spatially uniform white-noise visual stimuli and naturalistic movies. Using information-theoretic techniques, we demonstrate a clear role of relative precision, and show that the experimentally observed temporal structure in the neuronal response is necessary to represent accurately the more slowly changing visual world. By establishing a functional role of precision, we link visual neuron function on slow timescales to temporal structure in the response at faster timescales, and uncover a straightforward purpose of fine-timescale features of neuronal spike trains.},
author = {Butts, Daniel a and Weng, Chong and Jin, Jianzhong and Yeh, Chun-I and Lesica, Nicholas a and Alonso, Jose-Manuel and Stanley, Garrett B},
doi = {10.1038/nature06105},
file = {:home/epaxon/Documents/Mendeley Desktop/Butts et al. - 2007 - Temporal precision in the neural code and the timescales of natural vision.pdf:pdf},
isbn = {1476-4687 (Electronic)},
issn = {1476-4687},
journal = {Nature},
keywords = {Action Potentials,Action Potentials: physiology,Animals,Cats,Geniculate Bodies,Geniculate Bodies: cytology,Geniculate Bodies: physiology,Models,Neurological,Neurons,Neurons: physiology,Photic Stimulation,Time Factors,Visual Perception,Visual Perception: physiology},
number = {7158},
pages = {92--96},
pmid = {17805296},
title = {{Temporal precision in the neural code and the timescales of natural vision.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17805296},
volume = {449},
year = {2007}
}
@article{Buzsaki1995a,
abstract = {Network oscillations are postulated to be instrumental for synchronizing the activity of anatomically distributed populations of neurons. Results from recent studies on the physiology of cortical interneurons suggest that through their interconnectivity, they can maintain large-scale oscillations at various frequencies (4-12 Hz, 40-100 Hz and 200 Hz). We suggest that networks of inhibitory interneurons within the forebrain impose co-ordinated oscillatory 'contexts' for the 'content' carried by networks of principal cells. These oscillating inhibitory networks may provide the precise temporal structure necessary for ensembles of neurons to perform specific functions, including sensory binding and memory formation. ?? 1995.},
author = {Buzs{\'{a}}ki, Gy{\"{o}}rgy and Chrobak, James J.},
doi = {10.1016/0959-4388(95)80012-3},
file = {:home/epaxon/Documents/Mendeley Desktop/Buzs{\'{a}}ki, Chrobak - 1995 - Temporal structure in spatially organized neuronal ensembles a role for interneuronal networks.pdf:pdf},
isbn = {0959-4388 (Print)$\backslash$n0959-4388 (Linking)},
issn = {09594388},
journal = {Current Opinion in Neurobiology},
number = {4},
pages = {504--510},
pmid = {7488853},
title = {{Temporal structure in spatially organized neuronal ensembles: a role for interneuronal networks}},
volume = {5},
year = {1995}
}
@article{Buzsaki2004a,
abstract = {Clocks tick, bridges and skyscrapers vibrate, neuronal networks oscillate. Are neuronal oscillations an inevitable by-product, similar to bridge vibrations, or an essential part of the brain's design? Mammalian cortical neurons form behavior-dependent oscillating networks of various sizes, which span five orders of magnitude in frequency. These oscillations are phylogenetically preserved, suggesting that they are functionally relevant. Recent findings indicate that network oscillations bias input selection, temporally link neurons into assemblies, and facilitate synaptic plasticity, mechanisms that cooperatively support temporal representation and long-term consolidation of information.},
author = {Buzs{\'{a}}ki, Gyorgy and Draguhn, Andreas},
doi = {10.1126/science.1099745},
file = {:home/epaxon/Documents/Mendeley Desktop/Buzs{\'{a}}ki, Draguhn - 2004 - Neuronal oscillations in cortical networks.pdf:pdf},
isbn = {1095-9203 (Electronic)$\backslash$n0036-8075 (Linking)},
issn = {1095-9203},
journal = {Science},
keywords = {Animals,Biological Clocks,Biological Clocks: physiology,Brain,Brain: physiology,Cerebral Cortex,Cerebral Cortex: physiology,Electroencephalography,Humans,Learning,Membrane Potentials,Membrane Potentials: physiology,Nerve Net,Nerve Net: physiology,Neuronal Plasticity,Neurons,Neurons: physiology,Synapses,Synapses: physiology,Synaptic Transmission},
number = {June},
pages = {1926--1929},
pmid = {15218136},
title = {{Neuronal oscillations in cortical networks}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15218136},
volume = {304},
year = {2004}
}
@article{Buzsaki2004,
abstract = {Clocks tick, bridges and skyscrapers vibrate, neuronal networks oscillate. Are neuronal oscillations an inevitable by-product, similar to bridge vibrations, or an essential part of the brain's design? Mammalian cortical neurons form behavior-dependent oscillating networks of various sizes, which span five orders of magnitude in frequency. These oscillations are phylogenetically preserved, suggesting that they are functionally relevant. Recent findings indicate that network oscillations bias input selection, temporally link neurons into assemblies, and facilitate synaptic plasticity, mechanisms that cooperatively support temporal representation and long-term consolidation of information.},
author = {Buzs{\'{a}}ki, Gyorgy and Draguhn, Andreas},
doi = {10.1126/science.1099745},
file = {:home/epaxon/Documents/Mendeley Desktop/Buzs{\'{a}}ki, Draguhn - 2004 - Neuronal oscillations in cortical networks(2).pdf:pdf},
isbn = {1095-9203 (Electronic)$\backslash$n0036-8075 (Linking)},
issn = {1095-9203},
journal = {Science},
keywords = {Animals,Biological Clocks,Biological Clocks: physiology,Brain,Brain: physiology,Cerebral Cortex,Cerebral Cortex: physiology,Electroencephalography,Humans,Learning,Membrane Potentials,Membrane Potentials: physiology,Nerve Net,Nerve Net: physiology,Neuronal Plasticity,Neurons,Neurons: physiology,Synapses,Synapses: physiology,Synaptic Transmission},
number = {June},
pages = {1926--1929},
pmid = {15218136},
title = {{Neuronal oscillations in cortical networks}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15218136},
volume = {304},
year = {2004}
}
@article{Buzsaki2004b,
abstract = {Clocks tick, bridges and skyscrapers vibrate, neuronal networks oscillate. Are neuronal oscillations an inevitable by-product, similar to bridge vibrations, or an essential part of the brain's design? Mammalian cortical neurons form behavior-dependent oscillating networks of various sizes, which span five orders of magnitude in frequency. These oscillations are phylogenetically preserved, suggesting that they are functionally relevant. Recent findings indicate that network oscillations bias input selection, temporally link neurons into assemblies, and facilitate synaptic plasticity, mechanisms that cooperatively support temporal representation and long-term consolidation of information.},
author = {Buzs{\'{a}}ki, Gyorgy and Draguhn, Andreas},
doi = {10.1126/science.1099745},
file = {:home/epaxon/Documents/Mendeley Desktop/Buzs{\'{a}}ki, Draguhn - 2004 - Neuronal oscillations in cortical networks.pdf:pdf},
isbn = {1095-9203 (Electronic)$\backslash$n0036-8075 (Linking)},
issn = {1095-9203},
journal = {Science},
keywords = {Animals,Biological Clocks,Biological Clocks: physiology,Brain,Brain: physiology,Cerebral Cortex,Cerebral Cortex: physiology,Electroencephalography,Humans,Learning,Membrane Potentials,Membrane Potentials: physiology,Nerve Net,Nerve Net: physiology,Neuronal Plasticity,Neurons,Neurons: physiology,Synapses,Synapses: physiology,Synaptic Transmission},
number = {June},
pages = {1926--1929},
pmid = {15218136},
title = {{Neuronal oscillations in cortical networks}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15218136},
volume = {304},
year = {2004}
}
@article{Canolty2009,
abstract = {We observed robust coupling between the high- and low-frequency bands of ongoing electrical activity in the human brain. In particular, the phase of the low-frequency theta (4 to 8 hertz) rhythm modulates power in the high gamma (80 to 150 hertz) band of the electrocorticogram, with stronger modulation occurring at higher theta amplitudes. Furthermore, different behavioral tasks evoke distinct patterns of theta/high gamma coupling across the cortex. The results indicate that transient coupling between low- and high-frequency brain rhythms coordinates activity in distributed cortical areas, providing a mechanism for effective communication during cognitive processing in humans},
author = {Canolty, R. T and Edwards, E. and Dalal, S. S and Soltani, M. and Nagarajan, S. S and Berger, M. S and Barbaro, N. M and Knight, R. T and Kirsch, H. E and Berger, M. S and Barbaro, N. M and Knight, R. T},
doi = {10.1126/science.1128115.High},
file = {:home/epaxon/Documents/Mendeley Desktop/Canolty et al. - 2009 - High Gamma Power is Phase-Locked to Theta Oscillations in Human Neocortex.pdf:pdf},
isbn = {1095-9203 (Electronic)$\backslash$n0036-8075 (Linking)},
issn = {0036-8075},
journal = {Science (New York, N.Y.)},
number = {5793},
pages = {1626--1628},
pmid = {16973878},
title = {{High Gamma Power is Phase-Locked to Theta Oscillations in Human Neocortex}},
volume = {313},
year = {2009}
}
@article{Capaday2006,
abstract = {The firing rate gain of neurons, defined as the slope of the relation between input to a neuron and its firing rate, has received considerable attention in the past few years. This has been largely motivated by the many experimental demonstrations of behavior related gain changes in a variety of neural circuits of the CNS. A surprising result was that a prime candidate, shunting inhibition, apparently does not change the firing rate gain of neurons. However, in this paper, we show a physiologically plausible mechanism by which shunting inhibition in the dendritic tree does, in a simple and direct manner, modulate the firing gain of neurons. The effect is due to a strong attenuation of the dendritic current arriving at the soma by shunting dendritic inhibition. Increasing the dendritic inhibitory conductance enhances the attenuation of current flowing from the dendritic to the somatic compartment and thus reduces firing gain. This mechanism relies on known physiological and anatomical properties of CNS neurons and does not require special features such as tunable neural noise inputs. Gain control by the proposed mechanism may prove to be a ubiquitous feature of neural circuit operations and it is readily verifiable experimentally.},
author = {Capaday, Charles and van Vreeswijk, Carl},
issn = {0219-6352},
journal = {Journal of integrative neuroscience},
keywords = {Action Potentials,Action Potentials: physiology,Action Potentials: radiation effects,Animals,Dendrites,Dendrites: physiology,Dendrites: radiation effects,Dose-Response Relationship, Radiation,Electric Conductivity,Electric Stimulation,Electric Stimulation: methods,Models, Neurological,Neural Inhibition,Neural Inhibition: physiology,Neural Inhibition: radiation effects,Neural Networks (Computer),Neurons,Neurons: classification,Neurons: cytology,Neurons: physiology,Neurons: radiation effects},
month = {jun},
number = {2},
pages = {199--222},
pmid = {16783869},
title = {{Direct control of firing rate gain by dendritic shunting inhibition.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16783869},
volume = {5},
year = {2006}
}
@article{Carpenter1989,
abstract = {This review outlines some fundamental neural network modules for associative memory, pattern recognition, and category learning. Included are discussions of the McCulloch-Pitts neuron, perceptrons, adaline and madaline, back propagation, the learning matrix, linear associative memory, embedding fields, instars and outstars, the avalanche, shunting competitive networks, competitive learning, computational mapping by instarl outstar families, adaptive resonance theory, the cognitron and neocognitron, and simulated annealing. Adaptive filter formalism provides a unified notation. Activation laws include additive and shunting equations. Learning laws include back-coupled error correction, Hebbian learning, and gated instar and outstar equations. Also included are discussions of real-time and off-line modeling, stable and unstable coding, supervised and unsupervised learning, and self-organization. ?? 1989.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Carpenter, Gail A.},
doi = {10.1016/0893-6080(89)90035-X},
eprint = {NIHMS150003},
file = {:home/epaxon/Documents/Mendeley Desktop/Carpenter - 1989 - Neural network models for pattern recognition and associative memory.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Adaptive filter,Associative memory,Category formation,Competition,Neural network,Pattern recognition,Perceptron,Self-organization},
number = {4},
pages = {243--257},
pmid = {4786750},
title = {{Neural network models for pattern recognition and associative memory}},
volume = {2},
year = {1989}
}
@article{Cessac2010,
abstract = {In the present overview, our wish is to demystify some aspects of coding with spike-timing, through a simple review of well-understood technical facts regarding spike coding. Our goal is a better understanding of the extent to which computing and modeling with spiking neuron networks might be biologically plausible and computationally efficient. We intentionally restrict ourselves to a deterministic implementation of spiking neuron networks and we consider that the dynamics of a network is defined by a non-stochastic mapping. By staying in this rather simple framework, we are able to propose results, formula and concrete numerical values, on several topics: (i) general time constraints, (ii) links between continuous signals and spike trains, (iii) spiking neuron networks parameter adjustment. Beside an argued review of several facts and issues about neural coding by spikes, we propose new results, such as a numerical evaluation of the most critical temporal variables that schedule the progress of realistic spike trains. When implementing spiking neuron networks, for biological simulation or computational purpose, it is important to take into account the indisputable facts here unfolded. This precaution could prevent one from implementing mechanisms that would be meaningless relative to obvious time constraints, or from artificially introducing spikes when continuous calculations would be sufficient and more simple. It is also pointed out that implementing a large-scale spiking neuron network is finally a simple task. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Cessac, Bruno and Paugam-Moisy, H{\'{e}}l{\`{e}}ne and Vi{\'{e}}ville, Thierry},
doi = {10.1016/j.jphysparis.2009.11.002},
file = {:home/epaxon/Documents/Mendeley Desktop/Cessac, Paugam-Moisy, Vi{\'{e}}ville - 2010 - Overview of facts and issues about neural coding by spikes.pdf:pdf},
isbn = {0928-4257},
issn = {09284257},
journal = {Journal of Physiology Paris},
keywords = {Neural code,Spike train metrics,Spiking neuron networks,Time constraints},
number = {1-2},
pages = {5--18},
pmid = {19925865},
publisher = {Elsevier Ltd},
title = {{Overview of facts and issues about neural coding by spikes}},
url = {http://dx.doi.org/10.1016/j.jphysparis.2009.11.002},
volume = {104},
year = {2010}
}
@article{Chakravarthy2009,
author = {Chakravarthy, V. Srinivasa},
doi = {10.4018/978-1-60566-214-5.ch004},
file = {:home/epaxon/Documents/Mendeley Desktop/Chakravarthy - 2009 - A Complex-Valued Hopfield Neural Network Dynamics and Applications.pdf:pdf},
isbn = {1605662143},
journal = {Complex-Valued Neural Networks: Utilizing High-Dimensional Parameters},
pages = {79},
title = {{A Complex-Valued Hopfield Neural Network: Dynamics and Applications}},
url = {http://www.igi-global.com/Bookstore/Chapter.aspx?TitleId=6765},
year = {2009}
}
@book{Cnrs2012,
abstract = {Clinoptilolite ((Na, K, Ca){\textless}inf{\textgreater}2{\textless}/inf{\textgreater} -{\textless}inf{\textgreater}3{\textless}/inf{\textgreater}Al{\textless}inf{\textgreater}3{\textless}/inf{\textgreater}(Al, Si){\textless}inf{\textgreater}2{\textless}/inf{\textgreater}Si{\textless}inf{\textgreater}13{\textless}/inf{\textgreater}O{\textless}inf{\textgreater}36{\textless}/inf{\textgreater}-12H{\textless}inf{\textgreater}2{\textless}/inf{\textgreater}O, Hydrated Sodium Potassium Calcium Aluminum Silicate) may be not the most well known, but it is one of the most useful natural zeolites. Clinoptilolite is used in many applications such as a chemical sieve, a gas absorber, a feed additive, or food additive, an odor control agent and as a water filter for municipal and residential drinking water and aquariums. Clinoptilolite is well suited for these applications due to its large amount of pore space, high resistance to extreme temperatures and chemically neutral basic structure. What ight strike many as odd is the food and feed additives. Clinoptilolite has been used for several years now as an additive to feed for cows, pigs, horses and chickens. It absorbs oxins in the feed that are created by molds and microscopic parasites and has enhanced food absorption by these animals. A similar use in actual people food is being tested. Zeolites can easily absorb ammonia and other toxic gases from air and water and thus can be used in filters, both for health reasons and for odor removal. Can zeolite preserve food? The answer is YES. This chapter describes an up-to-date review for the implantations of zeolites in several food industrial activities. {\textcopyright} 2012 Bentham Science Publishers. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Cnrs, Laboratoire Liris},
doi = {10.1007/978-3-540-92910-9},
eprint = {arXiv:1011.1669v3},
file = {:home/epaxon/Documents/Mendeley Desktop/Cnrs - 2012 - Handbook of Natural Computing.pdf:pdf},
isbn = {978-3-540-92909-3},
issn = {1098-6596},
pmid = {25246403},
title = {{Handbook of Natural Computing}},
url = {http://link.springer.com/10.1007/978-3-540-92910-9},
year = {2012}
}
@article{Coolen1988,
author = {Coolen, A C C and Gielen, C.C.A.M.},
doi = {10.1209/0295-5075/7/3/016},
file = {:home/epaxon/Documents/Mendeley Desktop/Coolen, Gielen - 1988 - Delays in Neural Networks.pdf:pdf},
issn = {12864854},
journal = {Europhysics Letters},
pages = {281--285},
title = {{Delays in Neural Networks}},
volume = {7},
year = {1988}
}
@article{Crook1998a,
abstract = {Oscillations in many regions of the cortex have common temporal characteristics with dominant frequencies centered around the 40 Hz (gamma) frequency range and the 5-10 Hz (theta) frequency range. Experimental results also reveal spatially synchronous oscillations, which are stimulus dependent (Gray {\&} Singer,1987; Gray, Konig, Engel, {\&} Singer, 1989; Engel, Konig, Kreiter, Schillen, {\&} Singer, 1992). This rhythmic activity suggests that the coherence of neural populations is a crucial feature of cortical dynamics (Gray, 1994). Using both simulations and a theoretical coupled oscillator approach, we demonstrate that the spike frequency adaptation seen in many pyramidal cells plays a subtle but important role in the dynamics of cortical networks. Without adaptation, excitatory connections among model pyramidal cells are desynchronizing. However, the slow processes associated with adaptation encourage stable synchronous behavior.},
author = {Crook, S M and Ermentrout, G B and Bower, J M},
doi = {10.1162/089976698300017511},
file = {:home/epaxon/Documents/Mendeley Desktop/Crook, Ermentrout, Bower - 1998 - Spike frequency adaptation affects the synchronization properties of networks of cortical oscillators.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
keywords = {activation,cat visual-cortex,computer-simulation,coupled neural oscillators,hippocampal pyramidal cells,modulation,neurons invitro,olfactory-bulb,population events,rat piriform cortex},
number = {4},
pages = {837--854},
pmid = {9573408},
title = {{Spike frequency adaptation affects the synchronization properties of networks of cortical oscillators}},
volume = {10},
year = {1998}
}
@article{Crook1997a,
abstract = {Coupled oscillator models use a single phase variable to approximate the voltage oscillation of each neuron during repetitive firing where the behavior of the model depends on the connectivity and the interaction function chosen to describe the coupling. We introduce a network model consisting of a continuum of these oscillators that includes the effects of spatially decaying coupling and axonal delay. We derive equations for determining the stability of solutions and analyze the network behavior for two different interaction functions. The first is a sine function, and the second is derived from a compartmental model of a pyramidal cell. In both cases, the system of coupled neural oscillators can undergo a bifurcation from synchronous oscillations to waves. The change in qualitative behavior is due to the axonal delay, which causes distant connections to encourage a phase shift between cells. We suggest that this mechanism could contribute to the behavior observed in several neurobiological systems.},
author = {Crook, S. M. and Ermentrout, G. B. and Vanier, M. C. and Bower, J. M.},
doi = {10.1023/A:1008843412952},
file = {:home/epaxon/Documents/Mendeley Desktop/Crook et al. - 1997 - The role of axonal delay in the synchronization of networks of coupled cortical oscillators.pdf:pdf},
isbn = {0929-5313 (Print)$\backslash$n0929-5313 (Linking)},
issn = {09295313},
journal = {Journal of Computational Neuroscience},
keywords = {Axonal delay,Cortical oscillators,Coupled oscillators,Phase,Synchrony},
number = {2},
pages = {161--172},
pmid = {9154522},
title = {{The role of axonal delay in the synchronization of networks of coupled cortical oscillators}},
volume = {4},
year = {1997}
}
@article{DeRuytervanSteveninck1997,
author = {{de Ruyter van Steveninck}, R. R.},
doi = {10.1126/science.275.5307.1805},
file = {:home/epaxon/Documents/Mendeley Desktop/de Ruyter van Steveninck - 1997 - Reproducibility and Variability in Neural Spike Trains.pdf:pdf},
issn = {00368075},
journal = {Science},
number = {5307},
pages = {1805--1808},
title = {{Reproducibility and Variability in Neural Spike Trains}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.275.5307.1805},
volume = {275},
year = {1997}
}
@article{Deco1998,
abstract = {We analyse analytically the coding of information by a spiking neuron. The emphasis is on the question of how many spikes are necessary for the reliable discrimination of two different input signals. The discrimination ability is measured by the second-order R{\'{e}}nyi mutual information between the random variable describing the name of the signal and a sequence of n output spikes. Analysing this measure as a function of n, we study the coding strategy of a single spiking neuron, with the following main results. A small number of output spikes is required for efficient discrimination of input signals, i.e. for encoding them, if the separation is easy; a large number of output spikes is required in the difficult case of separation of very similar input signals. Three different versions of the spike response model of a single neuron are studied. The approach presented can be regarded as a non-parametric version of the reconstruction method of Bialek.},
author = {Deco, G and Sch{\"{u}}rmann, B},
file = {:home/epaxon/Documents/Mendeley Desktop/Deco, Sch{\"{u}}rmann - 1998 - The coding of information by spiking neurons an analytical study.pdf:pdf},
isbn = {0954-898X (Print)$\backslash$r0954-898X (Linking)},
issn = {0954-898X},
journal = {Network: Computation in Neural Systems},
keywords = {Action Potentials,Action Potentials: physiology,Discrimination (Psychology),Discrimination (Psychology): physiology,Information Theory,Neural Networks (Computer),Neurons,Neurons: physiology},
number = {3},
pages = {303--17},
pmid = {9861992},
title = {{The coding of information by spiking neurons: an analytical study.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/9861992},
volume = {9},
year = {1998}
}
@article{Deco2011,
abstract = {Rhythmic neural synchronization is found throughout the brain during many different tasks and even at rest. Beyond their underlying mechanisms, the question of their role is still controversial. Modeling can bring insight on this difficult question. We review here our recent modeling results concerning this issue in different situations. During rest, we show how local rhythmic synchrony can induce a spatiotemporally organized spontaneous activity at the brain level. Then, we show how rhythmic synchrony decreases reaction time in attention and enhances the strength and speed of information transfer between different groups of neurons. Finally, we show that when rhythmic synchrony creates firing phases, the learning with spike timing-dependent plasticity of repeatedly presented input patterns is greatly enhanced.},
author = {Deco, Gustavo and Buehlmann, Andres and Masquelier, Timoth{\'{e}}e and Hugues, Etienne},
doi = {10.3389/fnhum.2011.00004},
file = {:home/epaxon/Documents/Mendeley Desktop/Deco et al. - 2011 - The Role of Rhythmic Neural Synchronization in Rest and Task Conditions.pdf:pdf},
isbn = {1662-5161 (Electronic)$\backslash$r1662-5161 (Linking)},
issn = {1662-5161},
journal = {Frontiers in Human Neuroscience},
keywords = {attention,communication,learning,oscillations,resting state},
number = {February},
pages = {1--6},
pmid = {21326617},
title = {{The Role of Rhythmic Neural Synchronization in Rest and Task Conditions}},
url = {http://journal.frontiersin.org/article/10.3389/fnhum.2011.00004/abstract},
volume = {5},
year = {2011}
}
@article{Dehaene1987,
abstract = {A model for formal neural networks that learn temporal sequences by selection is proposed on the basis of observations on the acquisition of song by birds, on sequence-detecting neurons, and on allosteric receptors. The model relies on hypothetical elementary devices made up of three neurons, the synaptic triads, which yield short-term modification of synaptic efficacy through heterosynaptic interactions, and on a local Hebbian learning rule. The functional units postulated are mutually inhibiting clusters of synergic neurons and bundles of synapses. Networks formalized on this basis display capacities for passive recognition and for production of temporal sequences that may include repetitions. Introduction of the learning rule leads to the differentiation of sequence-detecting neurons and to the stabilization of ongoing temporal sequences. A network architecture composed of three layers of neuronal clusters is shown to exhibit active recognition and learning of time sequences by selection: the network spontaneously produces prerepresentations that are selected according to their resonance with the input percepts. Predictions of the model are discussed.},
author = {Dehaene, S. and Changeux, J. P. and Nadal, J. P.},
doi = {10.1073/pnas.84.9.2727},
file = {:home/epaxon/Documents/Mendeley Desktop/Dehaene, Changeux, Nadal - 1987 - Neural networks that learn temporal sequences by selection.pdf:pdf},
isbn = {0027-8424 (Print)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {9},
pages = {2727--2731},
pmid = {3472233},
title = {{Neural networks that learn temporal sequences by selection.}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.84.9.2727},
volume = {84},
year = {1987}
}
@article{Derrida1987,
author = {Derrida, B. and Gardner, E. and Zippelius, A.},
file = {:home/epaxon/Documents/Mendeley Desktop/Derrida, Gardner, Zippelius - 1987 - An exactly solvable asymmetric neural network model.pdf:pdf},
journal = {Europhysics Letters},
number = {2},
pages = {167--173},
title = {{An exactly solvable asymmetric neural network model}},
volume = {4},
year = {1987}
}
@article{Destexhe1994a,
abstract = {Markov kinetic models were used to synthesize a complete description of synaptic transmission, including opening of voltage-dependent channels in the presynaptic terminal, release of neurotransmitter, gating of postsynaptic receptors, and activation of second-messenger systems. These kinetic schemes provide a more general framework for modeling ion channels than the Hodgkin-Huxley formalism, supporting a continuous spectrum of descriptions ranging from the very simple and computationally efficient to the highly complex and biophysically precise. Examples are given of simple kinetic schemes based on fits to experimental data that capture the essential properties of voltage-gated, synaptic and neuromodulatory currents. The Markov formalism allows the dynamics of ionic currents to be considered naturally in the larger context of biochemical signal transduction. This framework can facilitate the integration of a wide range of experimental data and promote consistent theoretical analysis of neural mechanisms from molecular interactions to network computations.},
author = {Destexhe, Alain and Mainen, Zachary F and Sejnowski, Terrence J},
file = {:home/epaxon/Documents/Mendeley Desktop/Destexhe, Mainen, Sejnowski - 1994 - Synthesis of models for excitable membranes, synaptic transmission and neuromodulation using a comm.pdf:pdf},
issn = {0929-5313},
journal = {Journal of computational neuroscience},
keywords = {Animals,Cell Membrane,Cell Membrane: metabolism,Kinetics,Models,Neurological,Neurotransmitter Agents,Neurotransmitter Agents: metabolism,Synaptic Transmission,Synaptic Transmission: physiology},
month = {aug},
number = {3},
pages = {195--230},
pmid = {8792231},
shorttitle = {J Comput Neurosci},
title = {{Synthesis of models for excitable membranes, synaptic transmission and neuromodulation using a common kinetic formalism.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/8792231},
volume = {1},
year = {1994}
}
@article{Diesmann1999,
abstract = {The classical view of neural coding has emphasized the importance of information carried by the rate at which neurons discharge action potentials. More recent proposals that information may be carried by precise spike timing have been challenged by the assumption that these neurons operate in a noisy fashion--presumably reflecting fluctuations in synaptic input and, thus, incapable of transmitting signals with millisecond fidelity. Here we show that precisely synchronized action potentials can propagate within a model of cortical network activity that recapitulates many of the features of biological systems. An attractor, yielding a stable spiking precision in the (sub)millisecond range, governs the dynamics of synchronization. Our results indicate that a combinatorial neural code, based on rapid associations of groups of neurons co-ordinating their activity at the single spike level, is possible within a cortical-like network.},
author = {Diesmann, M and Gewaltig, M O and Aertsen, a},
doi = {10.1038/990101},
file = {:home/epaxon/Documents/Mendeley Desktop/Diesmann, Gewaltig, Aertsen - 1999 - Stable propagation of synchronous spiking in cortical neural networks.pdf:pdf},
isbn = {0028-0836 (Print)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
number = {6761},
pages = {529--533},
pmid = {10591212},
title = {{Stable propagation of synchronous spiking in cortical neural networks.}},
volume = {402},
year = {1999}
}
@article{Dockendorf2013,
abstract = {Spike patterns in vivo are often incomplete or corrupted with noise that makes inputs to neuronal networks appear to vary although they may, in fact, be samples of a single underlying pattern or repeated presentation. Here we present a recurrent spiking neural network (SNN) model that learns noisy pattern sequences through the use of homeostasis and spike-timing dependent plasticity (STDP). We find that the changes in the synaptic weight vector during learning of patterns of random ensembles are approximately orthogonal in a reduced dimension space when the patterns are constructed to minimize overlap in representations. Using this model, representations of sparse patterns maybe associated through co-activated firing and integrated into ensemble representations. While the model is tolerant to noise, prospective activity, and pattern completion differ in their ability to adapt in the presence of noise. One version of the model is able to demonstrate the recently discovered phenomena of preplay and replay reminiscent of hippocampal-like behaviors.},
author = {Dockendorf, Karl and Srinivasa, Narayan},
doi = {10.3389/fncom.2013.00080},
file = {:home/epaxon/Documents/Mendeley Desktop/Dockendorf, Srinivasa - 2013 - Learning and prospective recall of noisy spike pattern episodes.pdf:pdf},
isbn = {1662-5188 (Electronic)$\backslash$r1662-5188 (Linking)},
issn = {1662-5188},
journal = {Frontiers in Computational Neuroscience},
keywords = {learning,memory,preplay,prospection,replay,sequences,spiking,spiking, STDP, learning, sequences, prospection, p,stdp},
number = {June},
pages = {1--11},
pmid = {23801961},
title = {{Learning and prospective recall of noisy spike pattern episodes}},
url = {http://journal.frontiersin.org/article/10.3389/fncom.2013.00080/abstract},
volume = {7},
year = {2013}
}
@article{Dodson2011,
abstract = {The membrane potential dynamics of stellate neurons in layer II of the medial entorhinal cortex are important for neural encoding of location. Previous studies suggest that these neurons generate intrinsic theta-frequency membrane potential oscillations, with a period that depends on neuronal location on the dorsal–ventral axis of themedial entorhinal cortex, and which in behaving animals could support generation of grid-like spatial firing fields. To address the nature and organization of this theta-like activity, we adopt the Lombmethod of least-squares spectral analysis. We demonstrate that peaks in frequency spectra that differ significantly from Gaussian noise do not necessarily imply the existence of a periodic oscillator, but can instead arise from filtered stochastic noise or a stochastic random walk. We show that theta-like membrane potential activity recorded fromstellate neurons in mature brain slices is consistentwith stochastic mechanisms, but not with generation by a periodic oscillator. The dorsal–ventral organization of intrinsic theta-likemembrane potential activity, and themodification of this activity during block of HCN channels, both reflect altered frequency distributions of stochastic spectral peaks, rather than tuning of a periodic oscillator. Our results demonstrate the importance of distinguishing periodic oscillations from stochastic processes.We suggest that dorsal–ventral tuning of theta-like membrane potential activity is due to differences in stochastic current fluctuations resulting from organization of ion channels that also control synaptic integration.},
author = {Dodson, Paul D. and Pastoll, Hugh and Nolan, Matthew F.},
doi = {10.1113/jphysiol.2011.205021},
file = {:home/epaxon/Documents/Mendeley Desktop/Dodson, Pastoll, Nolan - 2011 - Dorsal-ventral organization of theta-like activity intrinsic to entorhinal stellate neurons is mediated.pdf:pdf},
issn = {00223751},
journal = {The Journal of Physiology},
number = {12},
pages = {2993--3008},
pmid = {21502290},
title = {{Dorsal-ventral organization of theta-like activity intrinsic to entorhinal stellate neurons is mediated by differences in stochastic current fluctuations}},
url = {http://doi.wiley.com/10.1113/jphysiol.2011.205021},
volume = {589},
year = {2011}
}
@article{Donq-Liang1999,
author = {Donq-Liang, Lee},
file = {:home/epaxon/Documents/Mendeley Desktop/Donq-Liang - 1999 - New Stability Conditions for Hopfield Networks in Partial Simultaneous Update Mode.pdf:pdf},
journal = {IEEE Transactions on Neural Networks},
number = {4},
pages = {975--978},
title = {{New Stability Conditions for Hopfield Networks in Partial Simultaneous Update Mode}},
volume = {10},
year = {1999}
}
@article{Donq-Liang1998,
abstract = {A bidirectional associative memory (BAM) with complex states and connection weights is investigated in this paper. The states are represented by quantization values defined on the unit circle of the complex plane. A given Lyapunov function indicates that the proposed complex domain BAM (CDBAM) is bidirectionally stable no matter which operation (synchronous or asynchronous) is used. We also prove that all the equilibrium (fixed) points of CDBAM correspond to local energy minima so that the design problem can be solved by a gradient descent algorithm. Finally, a gradient descent algorithm in the complex domain is derived to design the weight matrix of CDBAM. Several computer simulations are performed to illustrate the validity, capacity, attractivity and the applications of the CDBAM.},
author = {Donq-Liang, Lee and Wen-June, Wang},
doi = {10.1016/S0893-6080(98)00078-1},
file = {:home/epaxon/Documents/Mendeley Desktop/Donq-Liang, Wen-June - 1998 - A multivalued bidirectional associative memory operating on a complex domain.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {Bidirectional associative memory,Bidirectional stability,Complex domain,Gradient descend algorithm,Lyapunov function,Multivalued associative memory},
number = {9},
pages = {1623--1635},
title = {{A multivalued bidirectional associative memory operating on a complex domain}},
volume = {11},
year = {1998}
}
@article{During1998,
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/9805073v1},
author = {During, A and Coolen, ACC C C and Sherrington, D and D{\"{u}}ring, A. and Coolen, ACC C C and Sherrington, D},
doi = {10.1088/0305-4470/31/43/005},
eprint = {9805073v1},
file = {:home/epaxon/Documents/Mendeley Desktop/During et al. - 1998 - Phase diagram and storage capacity of sequence processing neural networks.pdf:pdf},
issn = {0305-4470},
journal = {Journal of Physics A: Mathematical and General},
pages = {8607},
primaryClass = {arXiv:cond-mat},
title = {{Phase diagram and storage capacity of sequence processing neural networks}},
url = {http://iopscience.iop.org/0305-4470/31/43/005},
volume = {31},
year = {1998}
}
@article{Eccles1954,
abstract = {NONE},
author = {Eccles, J. C. and Fatt, P. and Koketsu, K.},
doi = {10.1113/jphysiol.1954.sp005226},
file = {:home/epaxon/Documents/Mendeley Desktop/Eccles, Fatt, Koketsu - 1954 - Cholinergic and inhibitory synapses in a pathway from motor-axon collaterals to motoneurones.pdf:pdf},
isbn = {0022-3751 (Print) 0022-3751 (Linking)},
issn = {0022-3751},
journal = {The Journal of physiology},
keywords = {SYNAPSES},
number = {3},
pages = {524--562},
pmid = {13222354},
title = {{Cholinergic and inhibitory synapses in a pathway from motor-axon collaterals to motoneurones.}},
url = {http://dx.doi.org/10.1113/jphysiol.1954.sp005226},
volume = {126},
year = {1954}
}
@article{Ecker2010,
abstract = {Correlated trial-to-trial variability in the activity of cortical neurons is thought to reflect the functional connectivity of the circuit. Many cortical areas are organized into functional columns, in which neurons are believed to be densely connected and to share common input. Numerous studies report a high degree of correlated variability between nearby cells. We developed chronically implanted multitetrode arrays offering unprecedented recording quality to reexamine this question in the primary visual cortex of awake macaques. We found that even nearby neurons with similar orientation tuning show virtually no correlated variability. Our findings suggest a refinement of current models of cortical microcircuit architecture and function: Either adjacent neurons share only a few percent of their inputs or, alternatively, their activity is actively decorrelated.},
author = {Ecker, Alexander S and Berens, Philipp and Keliris, Georgios a and Bethge, Matthias and Logothetis, Nikos K and Tolias, Andreas S},
doi = {10.1126/science.1179867},
file = {:home/epaxon/Documents/Mendeley Desktop/Ecker et al. - 2010 - Decorrelated Neuronal Firing in Cortical Microcircuits.pdf:pdf},
isbn = {1095-9203 (Electronic)$\backslash$r0036-8075 (Linking)},
issn = {10959203},
journal = {Science},
number = {5965},
pages = {584--587},
pmid = {20110506},
title = {{Decorrelated Neuronal Firing in Cortical Microcircuits}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20110506},
volume = {327},
year = {2010}
}
@book{Eirola2015,
author = {Eirola, Emil and Gritsenko, Andrey and Akusok, Anton and Bj{\"{o}}rk, Kaj-Mikael and Miche, Yoan and Sovilj, Du{\v{s}}an and Nian, Rui and He, Bo and Lendasse, Amaury},
booktitle = {Advances in Computational Intelligence: 13th International Work-Conference on Artificial Neural Networks},
doi = {10.1007/978-3-319-19222-2_13},
file = {:home/epaxon/Documents/Mendeley Desktop/Eirola et al. - 2015 - Extreme Learning Machines for Multiclass Classification Refining Predictions with Gaussian Mixture Models.pdf:pdf},
isbn = {978-3-319-19222-2},
issn = {16113349 03029743},
pages = {153--164},
title = {{Extreme Learning Machines for Multiclass Classification: Refining Predictions with Gaussian Mixture Models}},
url = {http://dx.doi.org/10.1007/978-3-319-19222-2{\_}13},
year = {2015}
}
@article{El-bakry2007,
author = {El-bakry, Hazem M and Mastorakis, Nikos},
file = {:home/epaxon/Documents/Mendeley Desktop/El-bakry, Mastorakis - 2008 - A Novel Hopfield Neural Network for Perfect Calculation of Magnetic Resonance Spectroscopy.pdf:pdf},
isbn = {9789606766930},
journal = {BEBI},
keywords = {cross correlation,fast fourier transform,hopfield neural networks,magnetic resonance spectroscopy,nuclear magnetic resonance},
pages = {372--384},
title = {{A Novel Hopfield Neural Network for Perfect Calculation of Magnetic Resonance Spectroscopy}},
year = {2008}
}
@article{Eliasmith2005,
abstract = {Extending work in Eliasmith and Anderson (2003), we employ a general framework to construct biologically plausible simulations of the three classes of attractor networks relevant for biological systems: static (point, line, ring, and plane) attractors, cyclic attractors, and chaotic attractors. We discuss these attractors in the context of the neural systems that they have been posited to help explain: eye control, working memory, and head direction; locomotion (specifically swimming); and olfaction, respectively. We then demonstrate how to introduce control into these models. The addition of control shows how attractor networks can be used as subsystems in larger neural systems, demonstrates how a much larger class of networks can be related to attractor networks, and makes it clear how attractor networks can be exploited for various information processing tasks in neurobiological systems.},
author = {Eliasmith, Chris},
doi = {10.1162/0899766053630332},
file = {:home/epaxon/Documents/Mendeley Desktop/Eliasmith - 2005 - A Unified Approach to Building and Controlling Spiking Attractor Networks.pdf:pdf},
isbn = {0899-7667 (Print)$\backslash$r0899-7667 (Linking)},
issn = {0899-7667},
journal = {Neural Computation},
number = {6},
pages = {1276--1314},
pmid = {15901399},
title = {{A Unified Approach to Building and Controlling Spiking Attractor Networks}},
url = {http://www.mitpressjournals.org/doi/10.1162/0899766053630332},
volume = {17},
year = {2005}
}
@article{Eliasmith2012,
abstract = {A central challenge for cognitive and systems neuroscience is to relate the incredibly complex behavior of animals to the equally complex activity of their brains. Recently described, large-scale neural models have not bridged this gap between neural activity and biological function. In this work, we present a 2.5-million-neuron model of the brain (called "Spaun") that bridges this gap by exhibiting many different behaviors. The model is presented only with visual image sequences, and it draws all of its responses with a physically modeled arm. Although simplified, the model captures many aspects of neuroanatomy, neurophysiology, and psychological behavior, which we demonstrate via eight diverse tasks.},
author = {Eliasmith, Chris and Stewart, Terrence C and Choo, Xuan and Bekolay, Trevor and DeWolf, Travis and Tang, Yichuan and Tang, Charlie and Rasmussen, Daniel},
doi = {10.1126/science.1225266},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
keywords = {Behavior,Brain,Brain: anatomy {\&} histology,Brain: physiology,Humans,Models, Neurological,Neural Networks (Computer),Neurological,Software},
month = {nov},
number = {6111},
pages = {1202--5},
pmid = {23197532},
title = {{A large-scale model of the functioning brain.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23197532},
volume = {338},
year = {2012}
}
@article{Engel1991,
author = {Engel, Andreas K and Konig, Peter and Kreiter, Andreas K and Singer, Wolf},
file = {:home/epaxon/Documents/Mendeley Desktop/Engel et al. - 1991 - Interhemispheric Synchronization of Oscillatory Neuronal Responses in Cat Visual Cortex.pdf:pdf},
journal = {Science},
number = {May},
pages = {24--26},
title = {{Interhemispheric Synchronization of Oscillatory Neuronal Responses in Cat Visual Cortex}},
year = {1991}
}
@article{Engelhard2014,
abstract = {Gamma oscillations in cortex have been extensively studied with relation to behavior in both humans and animal models; however, their computational role in the processing of behaviorally relevant signals is still not clear. One oft-overlooked characteristic of gamma oscillations is their spatial distribution over the cortical space and the computational consequences of such an organization. Here, we advance the proposal that the spatial organization of gamma oscillations is of major importance for their function. The interaction of specific spatial distributions of oscillations with the functional topography of cortex enables select amplification of neuronal signals, which supports perceptual and cognitive processing.},
author = {Engelhard, Ben and Vaadia, Eilon},
doi = {10.3389/fnsys.2014.00165},
file = {:home/epaxon/Documents/Mendeley Desktop/Engelhard, Vaadia - 2014 - Spatial computation with gamma oscillations.pdf:pdf},
issn = {1662-5137},
journal = {Frontiers in Systems Neuroscience},
keywords = {cortical computation,functional topography,gamma oscillations,gamma oscillations, functional topography, tempora,phase coding,temporal synchrony},
number = {September},
pages = {1--7},
pmid = {25249950},
title = {{Spatial computation with gamma oscillations}},
url = {http://journal.frontiersin.org/article/10.3389/fnsys.2014.00165/abstract},
volume = {8},
year = {2014}
}
@article{Ermentrout1981,
author = {Ermentrout, George Bard},
file = {:home/epaxon/Documents/Mendeley Desktop/Ermentrout - 1981 - n m Phase-locking of weakly coupled oscillators.pdf:pdf},
journal = {Journal of Mathematical Biology},
keywords = {integrate and fire -,oscillations - phase-locking -,perturbation},
pages = {327--342},
title = {{n: m Phase-locking of weakly coupled oscillators}},
url = {http://link.springer.com/article/10.1007/BF00276920},
volume = {5101099},
year = {1981}
}
@article{Ermentrout1984,
abstract = {A chain of {\$}n + 1{\$} weakly coupled oscillators with a linear gradient in natural frequencies is shown to exhibit “frequency plateaus,” or sequences of oscillators having the same frequency, with a jump in frequency from one plateau to another. We first show that the equations for the coupled oscillators admit an invariant {\$}(n + 1){\$}-torus on which the equations have a special form, one in which an n-dimensional subsystem is approximately invariant. We then show that when the linear gradient becomes too steep to allow phaselocking, there emerges a large-scale invariant circle in this n-dimensional system which corresponds to the existence of a pair of plateaus, and whose homotopy class within the n-torus corresponds to the position of the frequency jump. Also discussed are the effects of anisotropic and nonuniform coupling.},
author = {Ermentrout, George Bard and Kopell, Nancy},
doi = {10.1137/0515019},
file = {:home/epaxon/Documents/Mendeley Desktop/Ermentrout, Kopell - 1984 - Frequency Plateaus in a Chain of Weakly Coupled Oscillators, I.pdf:pdf},
issn = {0036-1410},
journal = {SIAM Journal on Mathematical Analysis},
number = {2},
pages = {215--237},
title = {{Frequency Plateaus in a Chain of Weakly Coupled Oscillators, I.}},
volume = {15},
year = {1984}
}
@article{Fries2001,
abstract = {In crowded visual scenes, attention is needed to select relevant stimuli. To study the underlying mechanisms, we recorded neurons in cortical area V4 while macaque monkeys attended to behaviorally relevant stimuli and ignored distracters. Neurons activated by the attended stimulus showed increased gamma-frequency (35 to 90 hertz) synchronization but reduced low-frequency ({\textless}17 hertz) synchronization compared with neurons at nearby V4 sites activated by distracters. Because postsynaptic integration times are short, these localized changes in synchronization may serve to amplify behaviorally relevant signals in the cortex.},
author = {Fries, P. and Reynolds, J H and Rorie, a E and Desimone, R},
doi = {10.1126/science.291.5508.1560},
file = {:home/epaxon/Documents/Mendeley Desktop/Fries et al. - 2001 - Modulation of Oscillatory Neuronal Synchronization by Selective Visual Attention.pdf:pdf},
isbn = {0036-8075 (Print) 0036-8075 (Linking)},
issn = {00368075},
journal = {Science},
keywords = {Action Potentials,Animals,Attention,Attention: physiology,Cues,Electrophysiology,Fixation,Macaca,Neurons,Neurons: physiology,Ocular,Photic Stimulation,Visual Cortex,Visual Cortex: physiology,Visual Perception,Visual Perception: physiology},
number = {5508},
pages = {1560--1563},
pmid = {11222864},
title = {{Modulation of Oscillatory Neuronal Synchronization by Selective Visual Attention}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1055465{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/11222864},
volume = {291},
year = {2001}
}
@article{Fries2005,
abstract = {At any one moment, many neuronal groups in our brain are active. Microelectrode recordings have characterized the activation of single neurons and fMRI has unveiled brain-wide activation patterns. Now it is time to understand how the many active neuronal groups interact with each other and how their communication is flexibly modulated to bring about our cognitive dynamics. I hypothesize that neuronal communication is mechanistically subserved by neuronal coherence. Activated neuronal groups oscillate and thereby undergo rhythmic excitability fluctuations that produce temporal windows for communication. Only coherently oscillating neuronal groups can interact effectively, because their communication windows for input and for output are open at the same times. Thus, a flexible pattern of coherence defines a flexible communication structure, which subserves our cognitive flexibility. {\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1111.7219},
author = {Fries, Pascal},
doi = {10.1016/j.tics.2005.08.011},
eprint = {1111.7219},
file = {:home/epaxon/Documents/Mendeley Desktop/Fries - 2005 - A mechanism for cognitive dynamics Neuronal communication through neuronal coherence.pdf:pdf},
isbn = {1364-6613 (Print) 1364-6613 (Linking)},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {10},
pages = {474--480},
pmid = {16150631},
title = {{A mechanism for cognitive dynamics: Neuronal communication through neuronal coherence}},
volume = {9},
year = {2005}
}
@article{Fries2007,
abstract = {Activated neuronal groups typically engage in rhythmic synchronization in the gamma-frequency range (30-100 Hz). Experimental and modeling studies demonstrate that each gamma cycle is framed by synchronized spiking of inhibitory interneurons. Here, we review evidence suggesting that the resulting rhythmic network inhibition interacts with excitatory input to pyramidal cells such that the more excited cells fire earlier in the gamma cycle. Thus, the amplitude of excitatory drive is recoded into phase values of discharges relative to the gamma cycle. This recoding enables transmission and read out of amplitude information within a single gamma cycle without requiring rate integration. Furthermore, variation of phase relations can be exploited to facilitate or inhibit exchange of information between oscillating cell assemblies. The gamma cycle could thus serve as a fundamental computational mechanism for the implementation of a temporal coding scheme that enables fast processing and flexible routing of activity, supporting fast selection and binding of distributed responses. This review is part of the INMED/TINS special issue Physiogenic and pathogenic oscillations: the beauty and the beast, based on presentations at the annual INMED/TINS symposium (http://inmednet.com). {\textcopyright} 2007 Elsevier Ltd. All rights reserved.},
author = {Fries, Pascal and Nikoli{\'{c}}, Danko and Singer, Wolf},
doi = {10.1016/j.tins.2007.05.005},
file = {:home/epaxon/Documents/Mendeley Desktop/Fries, Nikoli{\'{c}}, Singer - 2007 - The gamma cycle.pdf:pdf},
isbn = {0166-2236 (Print)},
issn = {01662236},
journal = {Trends in Neurosciences},
number = {7},
pages = {309--316},
pmid = {17555828},
title = {{The gamma cycle}},
volume = {30},
year = {2007}
}
@article{Geisler2007,
abstract = {The phase of spikes of hippocampal pyramidal cells relative to the local field theta oscillation shifts forward ("phase precession") over a full theta cycle as the animal crosses the cell's receptive field ("place field"). The linear relationship between the phase of the spikes and the travel distance within the place field is independent of the animal's running speed. This invariance of the phase-distance relationship is likely to be important for coordinated activity of hippocampal cells and space coding, yet the mechanism responsible for it is not known. Here we show that at faster running speeds place cells are active for fewer theta cycles but oscillate at a higher frequency and emit more spikes per cycle. As a result, the phase shift of spikes from cycle to cycle (i.e., temporal precession slope) is faster, yet spatial-phase precession stays unchanged. Interneurons can also show transient-phase precession and contribute to the formation of coherently precessing assemblies. We hypothesize that the speed-correlated acceleration of place cell assembly oscillation is responsible for the phase-distance invariance of hippocampal place cells.},
author = {Geisler, C and Robbe, D and Zugaro, M and Sirota, A and Buzsaki, G},
doi = {0610121104 [pii]\r10.1073/pnas.0610121104},
file = {:home/epaxon/Documents/Mendeley Desktop/Geisler et al. - 2007 - Hippocampal place cell assemblies are speed-controlled oscillators.pdf:pdf},
isbn = {0027-8424 (Print)},
issn = {0027-8424 {\{}(Print){\}} 0027-8424},
journal = {Proc Natl Acad Sci U S A},
keywords = {*Theta Rhythm,Animals,Hippocampus/cytology/*physiology,Interneurons/physiology,Male,Neurons/*physiology,Pyramidal Cells/physiology,Rats,Rats, Long-Evans,Running},
number = {19},
pages = {8149--8154},
pmid = {17470808},
title = {{Hippocampal place cell assemblies are speed-controlled oscillators}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve{\&}db=PubMed{\&}dopt=Citation{\&}list{\_}uids=17470808},
volume = {104},
year = {2007}
}
@article{Gerwinn2009,
abstract = {The timing of action potentials in spiking neurons depends on the temporal dynamics of their inputs and contains information about temporal fluctuations in the stimulus. Leaky integrate-and-fire neurons constitute a popular class of encoding models, in which spike times depend directly on the temporal structure of the inputs. However, optimal decoding rules for these models have only been studied explicitly in the noiseless case. Here, we study decoding rules for probabilistic inference of a continuous stimulus from the spike times of a population of leaky integrate-and-fire neurons with threshold noise. We derive three algorithms for approximating the posterior distribution over stimuli as a function of the observed spike trains. In addition to a reconstruction of the stimulus we thus obtain an estimate of the uncertainty as well. Furthermore, we derive a 'spike-by-spike' online decoding scheme that recursively updates the posterior with the arrival of each new spike. We use these decoding rules to reconstruct time-varying stimuli represented by a Gaussian process from spike trains of single neurons as well as neural populations.},
author = {Gerwinn, Sebastian and Macke, Jakob and Bethge, Matthias},
doi = {10.3389/neuro.10.021.2009},
file = {:home/epaxon/Documents/Mendeley Desktop/Gerwinn, Macke, Bethge - 2009 - Bayesian population decoding of spiking neurons.pdf:pdf},
isbn = {1662-5188 (Electronic)},
issn = {16625188},
journal = {Front Comput Neurosci},
keywords = {approximate inference,bayesian decoding,population coding,spiking neurons},
number = {October},
pages = {21},
pmid = {20011217},
title = {{Bayesian population decoding of spiking neurons}},
url = {http://www.frontiersin.org/neuroscience/computationalneuroscience/paper/10.3389/neuro.10/021.2009/{\%}5Cnpapers3://publication/doi/10.3389/neuro.10.021.2009{\%}5Cnhttp://journal.frontiersin.org/article/10.3389/neuro.10.021.2009/abstract},
volume = {3},
year = {2009}
}
@article{Ghosh-dastidar2009,
author = {Ghosh-dastidar, Samanwoy and Lichtenstein, Abba G},
file = {:home/epaxon/Documents/Mendeley Desktop/Ghosh-dastidar, Lichtenstein - 2009 - Spiking Neural Networks.pdf:pdf},
journal = {International Journal of Neural Systems},
keywords = {information encoding,learning algorithm,spiking neural network,spiking neuron,supervised learning,unsuper-,vised learning},
number = {4},
pages = {295--308},
title = {{Spiking Neural Networks}},
volume = {19},
year = {2009}
}
@article{Goodman2009,
abstract = {"Brian" is a simulator for spiking neural networks (http://www.briansimulator.org). The focus is on making the writing of simulation code as quick and easy as possible for the user, and on flexibility: new and non-standard models are no more difficult to define than standard ones. This allows scientists to spend more time on the details of their models, and less on their implementation. Neuron models are defined by writing differential equations in standard mathematical notation, facilitating scientific communication. Brian is written in the Python programming language, and uses vector-based computation to allow for efficient simulations. It is particularly useful for neuroscientific modelling at the systems level, and for teaching computational neuroscience.},
author = {Goodman, Dan F M and Brette, Romain},
doi = {10.3389/neuro.01.026.2009},
issn = {1662-453X},
journal = {Frontiers in neuroscience},
keywords = {python,simulation,spiking neural networks,systems neuroscience,teaching},
month = {sep},
number = {2},
pages = {192--7},
pmid = {20011141},
title = {{The brian simulator.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2751620{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {3},
year = {2009}
}
@article{Gray1989,
abstract = {In areas 17 and 18 of the cat visual cortex the firing probability of neurons, in response to the presentation of optimally aligned light bars within their receptive field, oscillates with a peak frequency near 40 Hz. The neuronal firing pattern is tightly correlated with the phase and amplitude of an oscillatory local field potential recorded through the same electrode. The amplitude of the local field-potential oscillations are maximal in response to stimuli that match the orientation and direction preference of the local cluster of neurons. Single and multiunit recordings from the dorsal lateral geniculate nucleus of the thalamus showed no evidence of oscillations of the neuronal firing probability in the range of 20-70 Hz. The results demonstrate that local neuronal populations in the visual cortex engage in stimulus-specific synchronous oscillations resulting from an intracortical mechanism. The oscillatory responses may provide a general mechanism by which activity patterns in spatially separate regions of the cortex are temporally coordinated.},
archivePrefix = {arXiv},
arxivId = {156869},
author = {Gray, C M and Singer, W},
doi = {10.1073/pnas.86.5.1698},
eprint = {156869},
file = {:home/epaxon/Documents/Mendeley Desktop/Gray, Singer - 1989 - Stimulus-specific neuronal oscillations in orientation columns of cat visual cortex.pdf:pdf},
isbn = {0027-8424 (Print)$\backslash$r0027-8424 (Linking)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {5},
pages = {1698--1702},
pmid = {2922407},
title = {{Stimulus-specific neuronal oscillations in orientation columns of cat visual cortex.}},
volume = {86},
year = {1989}
}
@misc{Gray1989a,
abstract = {A fundamental step in visual pattern recognition is the establishment of relations between spatially separate features. Recently, we have shown that neurons in the cat visual cortex have oscillatory responses in the range 40-60 Hz (refs 1, 2) which occur in synchrony for cells in a functional column and are tightly correlated with a local oscillatory field potential. This led us to hypothesize that the synchronization of oscillatory responses of spatially distributed, feature selective cells might be a way to establish relations between features in different parts of the visual field. In support of this hypothesis, we demonstrate here that neurons in spatially separate columns can synchronize their oscillatory responses. The synchronization has, on average, no phase difference, depends on the spatial separation and the orientation preference of the cells and is influenced by global stimulus properties.},
author = {Gray, Charles M and K{\"{o}}nig, Peter and Engel, Andreas K and Singer, Wolf},
booktitle = {Nature},
doi = {10.1038/338334a0},
file = {:home/epaxon/Documents/Mendeley Desktop/Gray et al. - 1989 - Oscillatory responses in cat visual cortex exhibit inter-columnar synchronization which reflects global stimulus pr.pdf:pdf},
isbn = {0028-0836 (Print)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
number = {6213},
pages = {334--337},
pmid = {2922061},
title = {{Oscillatory responses in cat visual cortex exhibit inter-columnar synchronization which reflects global stimulus properties}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/2922061{\%}5Cnhttp://www.nature.com/doifinder/10.1038/338334a0},
volume = {338},
year = {1989}
}
@book{Greibach1975,
author = {Greibach, SA},
booktitle = {Lecture Notes in Computer Science},
file = {:home/epaxon/Documents/Mendeley Desktop/Greibach - 1975 - Lecture Notes in Computer Science.pdf:pdf},
isbn = {3540404082},
title = {{Lecture Notes in Computer Science}},
url = {http://www.ulb.tu-darmstadt.de/tocs/59142804.pdf},
year = {1975}
}
@book{Greibach1975,
author = {Greibach, SA},
booktitle = {Lecture Notes in Computer Science},
file = {:home/epaxon/Documents/Mendeley Desktop/Greibach - 1975 - Lecture Notes in Computer Science(2).pdf:pdf},
isbn = {9783540287520},
title = {{Lecture Notes in Computer Science}},
url = {http://www.ulb.tu-darmstadt.de/tocs/59142804.pdf},
year = {1975}
}
@article{Grossberg1991,
abstract = {A neural network model of synchronized oscillator activity in visual cortex is presented in order to account for recent neurophysiological findings that such synchronization may reflect global properties of the stimulus. In these recent experiments, it was reported that synchronization of oscillatory firing responses to moving bar stimuli occurred not only for nearby neurons, but also occurred between neurons separated by several cortical columns (several mm of cortex) when these neurons shared some receptive field preferences specific to the stimuli. These results were obtained not only for single bar stimuli but also across two disconnected, but colinear, bars moving in the same direction. Our model and computer simulations obtain these synchrony results across both single and double bar stimuli. For the double bar case, synchronous oscillations are induced in the region between the bars, but no oscillations are induced in the regions beyond the stimuli. These results were achieved with cellular units that exhibit limit cycle oscillations for a robust range of input values, but which approach an equilibrium state when undriven. Single and double bar synchronization of these oscillators was achieved by different, but formally related, models of preattentive visual boundary segmentation and attentive visual object recognition, as well as nearest-neighbor and randomly coupled models. In preattentive visual segmentation, synchronous oscillations may reflect the binding of local feature detectors into a globally coherent grouping. In object recognition, synchronous oscillations may occur during an attentive resonant state that triggers new learning. These modelling results support earlier theoretical predictions of synchronous visual cortical oscillations and demonstrate the robustness of the mechanisms capable of generating synchrony. ?? 1991.},
author = {Grossberg, Stephen and Somers, David},
doi = {10.1016/0893-6080(91)90041-3},
file = {:home/epaxon/Documents/Mendeley Desktop/Grossberg, Somers - 1991 - Synchronized oscillations during cooperative feature linking in a cortical model of visual perception.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Cooperative feature linking,Neural networks,Oscillations,Vision,Visual cortex},
number = {4},
pages = {453--466},
title = {{Synchronized oscillations during cooperative feature linking in a cortical model of visual perception}},
volume = {4},
year = {1991}
}
@article{Gupta2007,
abstract = {A spiking neural network model is used to identify characters in a character set. The network is a two layered structure consisting of integrate-and-fire and active dendrite neurons. There are both excitatory and inhibitory connections in the network. Spike time dependent plasticity (STDP) is used for training. The winner take all mechanism is enforced by the lateral inhibitory connections. It is found that most of the characters are recognized in a character set consisting of 48 characters. The network is trained successfully with increased resolution of the characters. Also, addition of uniform random noise does not decrease its recognition capability.},
author = {Gupta, a. and Long, L.N.},
doi = {10.1109/IJCNN.2007.4370930},
file = {:home/epaxon/Documents/Mendeley Desktop/Gupta, Long - 2007 - Character Recognition using Spiking Neural Networks.pdf:pdf},
isbn = {978-1-4244-1379-9},
issn = {1098-7576},
journal = {2007 International Joint Conference on Neural Networks},
number = {1},
title = {{Character Recognition using Spiking Neural Networks}},
volume = {16802},
year = {2007}
}
@article{Gupta2014,
abstract = {Background Sparse codes are found in nearly every sensory system, but the role of spike timing in sparse sensory coding is unclear. Here, we use the olfactory system of awake locusts to test whether the timing of spikes in Kenyon cells, a population of neurons that responds sparsely to odors, carries sensory information to and influences the responses of follower neurons.
Results We characterized two major classes of direct followers of Kenyon cells. With paired intracellular and field potential recordings made during odor presentations, we found that these followers contain information about odor identity in the temporal patterns of their spikes rather than in the spike rate, the spike phase, or the identities of the responsive neurons. Subtly manipulating the relative timing of Kenyon cell spikes with temporally and spatially structured microstimulation reliably altered the response patterns of the followers.
Conclusions Our results show that even remarkably sparse spiking responses can provide information through stimulus-specific variations in timing on the order of tens to hundreds of milliseconds and that these variations can determine the responses of downstream neurons. These results establish the importance of spike timing in a sparse sensory code.},
author = {Gupta, Nitin and Stopfer, Mark},
doi = {10.1016/j.cub.2014.08.021},
file = {:home/epaxon/Documents/Mendeley Desktop/Gupta, Stopfer - 2014 - A temporal channel for information in sparse sensory coding.pdf:pdf},
isbn = {1879-0445 (Electronic)$\backslash$r0960-9822 (Linking)},
issn = {09609822},
journal = {Current Biology},
number = {19},
pages = {2247--2256},
pmid = {25264257},
publisher = {Elsevier Ltd},
title = {{A temporal channel for information in sparse sensory coding}},
url = {http://dx.doi.org/10.1016/j.cub.2014.08.021},
volume = {24},
year = {2014}
}
@article{Haddad2013,
abstract = {Odor stimulation evokes complex spatiotemporal activity in the olfactory bulb, suggesting that the identity of activated neurons as well as the timing of their activity convey information about odors. However, whether and how downstream neurons decipher these temporal patterns remains debated. We addressed this question by measuring the spiking activity of downstream neurons while optogenetically stimulating two foci in the olfactory bulb with varying relative timing in mice. We found that the overall spike rates of piriform cortex neurons were sensitive to the relative timing of activation. Posterior piriform cortex neurons showed higher sensitivity to relative input times than neurons in the anterior piriform cortex. In contrast, olfactory bulb neurons rarely showed such sensitivity. Thus, the brain can transform a relative time code in the periphery into a firing-rate-based representation in central brain areas, providing evidence for the relevance of relative time-based code in the olfactory bulb.},
author = {Haddad, Rafi and Lanjuin, Anne and Madisen, Linda and Zeng, Hongkui and Murthy, Venkatesh N and Uchida, Naoshige},
doi = {10.1038/nn.3407},
file = {:home/epaxon/Documents/Mendeley Desktop/Haddad et al. - 2013 - Olfactory cortical neurons read out a relative time code in the olfactory bulb.pdf:pdf},
isbn = {1546-1726 (Electronic)$\backslash$r1097-6256 (Linking)},
issn = {1097-6256},
journal = {Nature Neuroscience},
number = {7},
pages = {949--957},
pmid = {23685720},
publisher = {Nature Publishing Group},
title = {{Olfactory cortical neurons read out a relative time code in the olfactory bulb}},
url = {http://www.nature.com/doifinder/10.1038/nn.3407},
volume = {16},
year = {2013}
}
@article{Hasenstaub2005,
abstract = {Temporal precision in spike timing is important in cortical function, interactions, and plasticity. We found that, during periods of recurrent network activity (UP states), cortical pyramidal cells in vivo and in vitro receive strong barrages of both excitatory and inhibitory postsynaptic potentials, with the inhibitory potentials showing much higher power at all frequencies above approximately 10 Hz and more synchrony between nearby neurons. Fast-spiking inhibitory interneurons discharged strongly in relation to higher-frequency oscillations in the field potential in vivo and possess membrane, synaptic, and action potential properties that are advantageous for transmission of higher-frequency activity. Intracellular injection of synaptic conductances having the characteristics of the recorded EPSPs and IPSPs reveal that IPSPs are important in controlling the timing and probability of action potential generation in pyramidal cells. Our results support the hypothesis that inhibitory networks are largely responsible for the dissemination of higher-frequency activity in cortex. Copyright {\textcopyright}2005 by Elsevier Inc.},
author = {Hasenstaub, Andrea and Shu, Yousheng and Haider, Bilal and Kraushaar, Udo and Duque, Alvaro and McCormick, David A.},
doi = {10.1016/j.neuron.2005.06.016},
file = {:home/epaxon/Documents/Mendeley Desktop/Hasenstaub et al. - 2005 - Inhibitory postsynaptic potentials carry synchronized frequency information in active cortical networks.pdf:pdf},
isbn = {0896-6273 (Print)},
issn = {08966273},
journal = {Neuron},
number = {3},
pages = {423--435},
pmid = {16055065},
title = {{Inhibitory postsynaptic potentials carry synchronized frequency information in active cortical networks}},
volume = {47},
year = {2005}
}
@article{Hasselmo2007,
abstract = {Anatomical connectivity and recent neurophysiological results imply that grid cells in the medial entorhinal cortex are the prin- cipal cortical inputs to place cells in the hippocampus. The authors pro- pose a model in which place fields of hippocampal pyramidal cells are formed by linear summation of appropriately weighted inputs from entorhinal grid cells. Single confined place fields could be formed by summing input from a modest number (10–50) of grid cells with rela- tively similar grid phases, diverse grid orientations, and a biologically plausible range of grid spacings. When the spatial phase variation in the grid-cell input was higher, multiple, and irregularly spaced firing fields were formed. These observations point to a number of possible con- straints in the organization of functional connections between grid cells and place cells},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Hasselmo, Michael E. and Giocomo, Lisa M. and Zilli, Eric A.},
doi = {10.1002/hipo.20374},
eprint = {NIHMS150003},
file = {:home/epaxon/Documents/Mendeley Desktop/Trygve Solstad, Moser, Gaute T. Einevoll - 2007 - From Grid Cells to Place Cells A Mathematical Model.pdf:pdf},
isbn = {1050-9631 (Print)$\backslash$r1050-9631 (Linking)},
issn = {10509631},
journal = {Hippocampus},
keywords = {campus,dendrites,entorhinal cortex,hippo-,place cells,stellate cells,theta rhythm},
month = {dec},
number = {12},
pages = {1252--1271},
pmid = {17598147},
title = {{Grid cell firing may arise from interference of theta frequency membrane potential oscillations in single neurons}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve{\&}db=PubMed{\&}dopt=Citation{\&}list{\_}uids=17598147 http://doi.wiley.com/10.1002/hipo.20374},
volume = {17},
year = {2007}
}
@book{Hebb1949,
abstract = {The organization of behaviour: A neuropsychological theory. New York: Wiley.},
author = {Hebb, D. O.},
doi = {citeulike-article-id:1282862},
file = {:home/epaxon/Documents/Mendeley Desktop/Hebb - 1949 - The Organization of Behavior A Neuropsychological theory.pdf:pdf},
isbn = {0471367273},
issn = {00029556},
pages = {62},
pmid = {1605},
publisher = {John Wiley {\&} Sons},
title = {{The Organization of Behavior: A Neuropsychological theory}},
year = {1949}
}
@article{Henze2003,
abstract = {The information processing capabilities of neuronal populations are a function of the connectivity and single cell properties of the neurons that comprise the network. An important area of investigation is how information is encoded and processed by neurons in the network. There is a growing body of evidence demonstrating how the specific biophysical properties of individual neurons contribute to network processing as a whole. We illustrate through several examples in the hippocampal formation how cellular biophysics can directly effect the timing of neuronal spiking in the context of on-going network oscillations and how network patterns can influence the intrinsic properties of single cells. We discuss the consequences of the cooperation and competition between single cell and network properties.},
author = {Henze, Darrell a and Buzs{\'{a}}ki, Gy{\"{o}}rgy},
doi = {10.1016/S0531-5131(03)01049-5},
file = {:home/epaxon/Documents/Mendeley Desktop/Henze, Buzs{\'{a}}ki - 2003 - Single cell contributions to network activity in the hippocampus.pdf:pdf},
issn = {05315131},
journal = {International Congress Series},
keywords = {hippocampus,network activity,single cell properties},
pages = {161--181},
title = {{Single cell contributions to network activity in the hippocampus}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0531513103010495},
volume = {1250},
year = {2003}
}
@article{Hirose1992,
abstract = {A novel neural network that processes input vectors and attractors fully in complex space using complex weights is proposed. Real and imaginary data are treated consistently with an equivalent significance in nondegenerate complex space. This network can be applied for ill-posed problems concerning realistic physical fields and continuous motion controls. The dynamics are presented and demonstrated.},
author = {Hirose, A},
doi = {10.1049/el:19920948},
file = {:home/epaxon/Documents/Mendeley Desktop/Hirose - 1992 - Dynamics of Fully Complex-Valued Neural Networks.pdf:pdf},
isbn = {0013-5194},
issn = {00135194},
journal = {Electronics Letters},
keywords = {ASSOCIATIVE MEMORY,Engineering, Electrical {\&} Electronic,NEURAL NETWORKS},
number = {16},
pages = {1492--1494},
title = {{Dynamics of Fully Complex-Valued Neural Networks}},
volume = {28},
year = {1992}
}
@article{Hirose1992a,
author = {Hirose, Akira},
file = {:home/epaxon/Documents/Mendeley Desktop/Hirose - 1992 - Proposal of fully complex-valued neural networks.pdf:pdf},
journal = {IEEE},
pages = {152--157},
title = {{Proposal of fully complex-valued neural networks}},
year = {1992}
}
@incollection{Ligne1973,
author = {Hirose, Akira},
booktitle = {Artificial Intelligence and Soft Computing},
doi = {10.1007/978-3-642-15314-3},
file = {:home/epaxon/Documents/Mendeley Desktop/Hirose - 2010 - Recent progress in applications of complex-valued neural networks.pdf:pdf},
isbn = {9783642153136},
title = {{Recent progress in applications of complex-valued neural networks}},
year = {2010}
}
@article{Hopfield1982,
abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
author = {Hopfield, J J},
doi = {10.1073/pnas.79.8.2554},
file = {:home/epaxon/Documents/Mendeley Desktop/Hopfield - 1982 - Neural networks and physical systems with emergent collective computational abilities(2).pdf:pdf},
isbn = {0027-8424},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {8},
pages = {2554--2558},
pmid = {6953413},
title = {{Neural networks and physical systems with emergent collective computational abilities.}},
volume = {79},
year = {1982}
}
@misc{Hopfield1995,
abstract = {A computational model is described in which the sizes of variables are represented by the explicit times at which action potentials occur, rather than by the more usual 'firing rate' of neurons. The comparison of patterns over sets of analogue variables is done by a network using different delays for different information paths. This mode of computation explains how one scheme of neuroarchitecture can be used for very different sensory modalities and seemingly different computations. The oscillations and anatomy of the mammalian olfactory systems have a simple interpretation in terms of this representation, and relate to processing in the auditory system. Single-electrode recording would plot detect such neural computing. Recognition 'units' in this style respond more like radial basis function units than elementary sigmoid un its.},
author = {Hopfield, J J},
booktitle = {Nature},
doi = {10.1038/376033a0},
file = {:home/epaxon/Documents/Mendeley Desktop/Hopfield - 1995 - Pattern-Recognition Computation Using Action-Potential Timing for Stimulus Representation.pdf:pdf},
isbn = {0028-0836 (Print)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
keywords = {bat,brain-stem,cells,circuit,delay-lines,inferior colliculus,information,interaural time,oscillations,visual-cortex},
number = {6535},
pages = {33--36},
pmid = {7596429},
title = {{Pattern-Recognition Computation Using Action-Potential Timing for Stimulus Representation}},
url = {papers://dfceb4b3-c740-44e2-b0ae-49a26135fc12/Paper/p2324{\%}5Cnpapers://dfceb4b3-c740-44e2-b0ae-49a26135fc12/Paper/p1313},
volume = {376},
year = {1995}
}
@article{Hopfield2004,
abstract = {Many stimuli have meaning only as patterns over time. Most auditory and many visual stimuli are of this nature and can be described as multidimensional, time-dependent vectors. A simple neuron can encode a single component of the vector in a firing rate. The addition of a small subthreshold oscillatory current perturbs the action-potential timing, encoding the signal also in a timing relationship, with little effect on the coexisting firing rate representation. When the subthreshold signal is common to a group of neurons, the timing-based information is significant to neurons receiving inputs from the group. This information encoding allows simple implementation of computations not readily done with rate coding. These ideas are examined by using speech to provide a realistic input signal to a biologically inspired model network of spiking neurons. The output neurons of the two-layer system are shown to specifically encode short linguistic elements of speech.},
author = {Hopfield, J. J.},
doi = {10.1073/pnas.0401125101},
file = {:home/epaxon/Documents/Mendeley Desktop/Hopfield - 2004 - Encoding for computation Recognizing brief dynamical patterns by exploiting effects of weak rhythms on action-potentia.pdf:pdf},
isbn = {0027-8424},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {16},
pages = {6255--6260},
pmid = {15075391},
title = {{Encoding for computation: Recognizing brief dynamical patterns by exploiting effects of weak rhythms on action-potential timing}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0401125101},
volume = {101},
year = {2004}
}
@article{Hopfield1996,
abstract = {Motifs of neural circuitry seem surprisingly conserved over different areas of neocortex or of paleocortex, while performing quite different sensory processing tasks. This apparent paradox may be resolved by the fact that seemingly different problems in sensory information processing are related by transformations (changes of variables) that convert one problem into another. The same basic algorithm that is appropriate to the recognition of a known odor quality, independent of the strength of the odor, can be used to recognize a vocalization (e.g., a spoken syllable), independent of whether it is spoken quickly or slowly. To convert one problem into the other, a new representation of time sequences is needed. The time that has elapsed since a recent event must be represented in neural activity. The electrophysiological hallmarks of cells that are involved in generating such a representation of time are discussed. The anatomical relationships between olfactory and auditory pathways suggest relevant experiments. The neurophysiological mechanism for the psychophysical logarithmic encoding of time duration would be of direct use for interconverting olfactory and auditory processing problems. Such reuse of old algorithms in new settings and representations is related to the way that evolution develops new biochemistry.},
author = {Hopfield, J. J.},
doi = {10.1073/pnas.93.26.15440},
file = {:home/epaxon/Documents/Mendeley Desktop/Hopfield - 1996 - Transforming neural computations and representing time.pdf:pdf},
isbn = {0027-8424 (Print)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {26},
pages = {15440--15444},
pmid = {8986830},
title = {{Transforming neural computations and representing time}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.93.26.15440},
volume = {93},
year = {1996}
}
@article{Hopfield1984,
abstract = {A model for a large network of "neurons" with a graded response (or sigmoid input-output relation) is studied. This deterministic system has collective properties in very close correspondence with the earlier stochastic model based on McCulloch - Pitts neurons. The content- addressable memory and other emergent collective properties of the original model also are present in the graded response model. The idea that such collective properties are used in biological systems is given added credence by the continued presence of such properties for more nearly biological "neurons." Collective analog electrical circuits of the kind described will certainly function. The collective states of the two models have a simple correspondence. The original model will continue to be useful for simulations, because its connection to graded response systems is established. Equations that include the effect of action potentials in the graded response system are also developed.},
author = {Hopfield, J. J.},
doi = {10.1073/pnas.81.10.3088},
file = {:home/epaxon/Documents/Mendeley Desktop/Hopfield - 1984 - Neurons with graded response have collective computational properties like those of two-state neurons.pdf:pdf},
isbn = {0027-8424 (Print)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {10},
pages = {3088--3092},
pmid = {6587342},
title = {{Neurons with graded response have collective computational properties like those of two-state neurons.}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.81.10.3088},
volume = {81},
year = {1984}
}
@article{Hopfield1985,
abstract = {Highly-interconnected networks of nonlinear analog neurons are shown to be extremely effective in computing. The networks can rapidly provide a collectively-computed solution (a digital output) to a problem on the basis of analog input information. The problems to be solved must be formulated in terms of desired optima, often subject to constraints. The general principles involved in constructing networks to solve specific problems are discussed. Results of computer simulations of a network designed to solve a difficult but well-defined optimization problem--the Traveling-Salesman Problem--are presented and used to illustrate the computational power of the networks. Good solutions to this problem are collectively computed within an elapsed time of only a few neural time constants. The effectiveness of the computation involves both the nonlinear analog response of the neurons and the large connectivity among them. Dedicated networks of biological or microelectronic neurons could provide the computational capabilities described for a wide class of problems having combinatorial complexity. The power and speed naturally displayed by such collective networks may contribute to the effectiveness of biological information processing.},
author = {Hopfield, J. J. and Tank, D. W.},
doi = {10.1007/BF00339943},
file = {:home/epaxon/Documents/Mendeley Desktop/Hopfield, Tank - 1985 - Neural computation of decisions in optimization problems.pdf:pdf},
isbn = {0340-1200 (Print)},
issn = {03401200},
journal = {Biological Cybernetics},
number = {3},
pages = {141--152},
pmid = {4027280},
title = {{"Neural" computation of decisions in optimization problems}},
volume = {52},
year = {1985}
}
@article{Hoppensteadt1996a,
abstract = {This is the second of two articles devoted to analyzing the relationship between synaptic organizations (anatomy) and dynamical properties (function) of networks of neural oscillators near multiple supercritical Andronov-Hopf bifurcation points. Here we analyze learning processes in such networks. Regarding learning dynamics, we assume (1) learning is local (i.e. synaptic modification depends on pre- and postsynaptic neurons but not on others), (2) synapses modify slowly relative to characteristic neuron response times, (3) in the absence of either pre- or postsynaptic activity, the synapse weakens (forgets). Our major goal is to analyze all synaptic organizations of oscillatory neural networks that can memorize and retrieve phase information or time delays. We show that such networks have the following attributes: (1) the rate of synaptic plasticity connected with learning is determined locally by the presynaptic neurons, (2) the excitatory neurons must be long-axon relay neurons capable of forming distant connections with other excitatory and inhibitory neurons, (3) if inhibitory neurons have long axons, then the network can learn, passively forget and actively unlearn information by adjusting synaptic plasticity rates.},
author = {Hoppensteadt, F C and Izhikevich, E M},
doi = {10.1007/s004220050280},
file = {:home/epaxon/Documents/Mendeley Desktop/Hoppensteadt, Izhikevich - 1996 - Synaptic organizations and dynamical properties of weakly connected neural oscillators II. Learning ph.pdf:pdf},
isbn = {0340-1200 (Print)$\backslash$r0340-1200 (Linking)},
issn = {0340-1200},
journal = {Biological Cybernetics},
number = {2},
pages = {129--135},
pmid = {8855351},
title = {{Synaptic organizations and dynamical properties of weakly connected neural oscillators: II. Learning phase information}},
url = {http://links.isiglobalnet2.com/gateway/Gateway.cgi?GWVersion=2{\&}SrcAuth=mekentosj{\&}SrcApp=Papers{\&}DestLinkType=FullRecord{\&}DestApp=WOS{\&}KeyUT=A1996VH41800004{\%}5Cnpapers3://publication/uuid/9EC6B3C2-2BB3-4BFA-BAD3-128FCF35AF81},
volume = {75},
year = {1996}
}
@article{Hoppensteadt1996,
abstract = {We study weakly connected networks of neural oscillators near multiple Andronov-Hopf bifurcation points. We analyze relationships between synaptic organizations (anatomy) of the networks and their dynamical properties (function). Our principal assumptions are: (1) Each neural oscillator comprises two populations of neurons; excitatory and inhibitory ones; (2) activity of each population of neurons is described by a scalar (one-dimensional) variable; (3) each neural oscillator is near a nondegenerate supercritical Andronov-Hopf bifurcation point; (4) the synaptic connections between the neural oscillators are weak. All neural networks satisfying these hypotheses are governed by the same dynamical system, which we call the canonical model. Studying the canonical model shows that: (1) A neural oscillator can communicate only with those oscillators which have roughly the same natural frequency. That is, synaptic connections between a pair of oscillators having different natural frequencies are functionally insignificant. (2) Two neural oscillators having the same natural frequencies might not communicate if the connections between them are from among a class of pathological synaptic configurations. In both cases the anatomical presence of synaptic connections between neural oscillators does not necessarily guarantee that the connections are functionally significant. (3) There can be substantial phase differences (time delays) between the neural oscillators, which result from the synaptic organization of the network, not from the transmission delays. Using the canonical model we can illustrate self-ignition and autonomous quiescence (oscillator death) phenomena. That is, a network of passive elements can exhibit active properties and vice versa. We also study how Dale's principle affects dynamics of the networks, in particular, the phase differences that the network can reproduce. We present a complete classification of all possible synaptic organizations from this point of view. The theory developed here casts some light on relations between synaptic organization and functional properties of oscillatory networks. The major advantage of our approach is that we obtain results about all networks of neural oscillators, including the real brain. The major drawback is that our findings are valid only when the brain operates near a critical regime, viz. for a multiple Andronov-Hopf bifurcation.},
author = {Hoppensteadt, Frank C and Izhikevich, Eugene M},
file = {:home/epaxon/Documents/Mendeley Desktop/Hoppensteadt, Izhikevich - 1996 - Synaptic organizations and dynamical properties of weakly connected neural oscillators I. Analysis of.pdf:pdf},
issn = {0340-1200},
journal = {Biological Cybernetics},
keywords = {Cell Death,Cell Death: physiology,Models,Neural Pathways,Neural Pathways: physiology,Neurological,Neurons,Neurons: cytology,Neurons: physiology,Olfactory Bulb,Olfactory Bulb: physiology,Periodicity,Synapses,Synapses: physiology},
number = {2},
pages = {117--127},
pmid = {8855350},
title = {{Synaptic organizations and dynamical properties of weakly connected neural oscillators: I. Analysis of a canonical model}},
volume = {75},
year = {1996}
}
@article{Horn1989,
abstract = {We incorporate local threshold functions into the dynamics of the Hopfield model. These functions depend on the history of the individual spin (= neuron). They reach a maximal height if the spin remains constant. The resulting one-pattern model has ferromagnetic, paramagnetic, and periodic phases. This model is solved by a master equation and approximated by simplified systems of equations that are substantiated by numerical simulations. When several patterns are included as memories in the model, it exhibits transitions—as well as oscillations—between them. The latter can be excluded by known methods. By introducing threshold functions which affect only spins which remain positive, thus mimicking fatigue of the individual neurons, one can obtain open-ended movement in pattern space. Using couplings which form pointers from one pattern to another, our system leads to self-driven temporal sequences of patterns, resembling the process of associative thinking.},
author = {Horn, D. and Usher, M.},
doi = {10.1103/PhysRevA.40.1036},
file = {:home/epaxon/Documents/Mendeley Desktop/Horn, Usher - 1989 - Neural networks with dynamical thresholds.pdf:pdf},
issn = {10502947},
journal = {Physical Review A},
number = {2},
pages = {1036--1044},
pmid = {9902229},
title = {{Neural networks with dynamical thresholds}},
volume = {40},
year = {1989}
}
@article{Huang2005,
abstract = { The ordinary Takagi-Sugeno (TS) fuzzy models have provided an approach to represent complex nonlinear systems to a set of linear sub-models by using fuzzy sets and fuzzy reasoning. In this paper, stochastic fuzzy Hopfield neural networks with time-varying delays (SFVDHNNs) are studied. The model of SFVDHNN is first established as a modified TS fuzzy model in which the consequent parts are composed of a set of stochastic Hopfield neural networks with time-varying delays. Secondly, the global exponential stability in the mean square for SFVDHNN is studied by using the Lyapunov-Krasovskii approach. Stability criterion is derived in terms of linear matrix inequalities (LMIs), which can be effectively solved by some standard numerical packages.},
author = {Huang, He Huang He and Ho, D.W.C. and Lam, J.},
doi = {10.1109/TCSII.2005.846305},
file = {:home/epaxon/Documents/Mendeley Desktop/Huang, Ho, Lam - 2005 - Stochastic stability analysis of fuzzy hopfield neural networks with time-varying delays.pdf:pdf},
issn = {1549-7747},
journal = {IEEE Transactions on Circuits and Systems II: Express Briefs},
keywords = {Fuzzy systems,Hopfield neural networks,stability,stochastic systems,time-varying delay systems},
number = {5},
pages = {251--255},
title = {{Stochastic stability analysis of fuzzy hopfield neural networks with time-varying delays}},
volume = {52},
year = {2005}
}
@book{Hutchison2014,
author = {Hutchison, David},
file = {:home/epaxon/Documents/Mendeley Desktop/Hutchison - 2014 - LNCS 8834 - Neural Information Processing.pdf:pdf},
isbn = {9783319126364},
title = {{LNCS 8834 - Neural Information Processing}},
year = {2014}
}
@article{Huxter2003,
abstract = {In the brain, hippocampal pyramidal cells use temporal as well as rate coding to signal spatial aspects of the animal's environment or behaviour. The temporal code takes the form of a phase relationship to the concurrent cycle of the hippocampal electroencephalogram theta rhythm. These two codes could each represent a different variable. However, this requires the rate and phase to vary independently, in contrast to recent suggestions that they are tightly coupled, both reflecting the amplitude of the cell's input. Here we show that the time of firing and firing rate are dissociable, and can represent two independent variables: respectively the animal's location within the place field, and its speed of movement through the field. Independent encoding of location together with actions and stimuli occurring there may help to explain the dual roles of the hippocampus in spatial and episodic memory, or may indicate a more general role of the hippocampus in relational/declarative memory.},
author = {Huxter, John and Burgess, Neil and O'Keefe, John},
doi = {10.1038/nature02058},
file = {:home/epaxon/Documents/Mendeley Desktop/Huxter, Burgess, O'Keefe - 2003 - Independent rate and temporal coding in hippocampal pyramidal cells.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {1476-4687},
journal = {Nature},
keywords = {Action Potentials,Animals,Male,Memory,Memory: physiology,Models,Neurological,Pyramidal Cells,Pyramidal Cells: physiology,Rats,Reward,Running,Space Perception,Space Perception: physiology,Theta Rhythm},
number = {6960},
pages = {828--32},
pmid = {14574410},
title = {{Independent rate and temporal coding in hippocampal pyramidal cells.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2677642{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {425},
year = {2003}
}
@article{Ito2011,
abstract = {During natural vision, primates perform frequent saccadic eye movements, allowing only a narrow time window for processing the visual information at each location. Individual neurons may contribute only with a few spikes to the visual processing during each fixation, suggesting precise spike timing as a relevant mechanism for information processing. We recently found in V1 of monkeys freely viewing natural images, that fixation-related spike synchronization occurs at the early phase of the rate response after fixation-onset, suggesting a specific role of the first response spikes in V1. Here, we show that there are strong local field potential (LFP) modulations locked to the onset of saccades, which continue into the successive fixation periods. Visually induced spikes, in particular the first spikes after the onset of a fixation, are locked to a specific epoch of the LFP modulation. We suggest that the modulation of neural excitability, which is reflected by the saccade-related LFP changes, serves as a corollary signal enabling precise timing of spikes in V1 and thereby providing a mechanism for spike synchronization.},
author = {Ito, Junji and Maldonado, Pedro and Singer, Wolf and Gr{\"{u}}n, Sonja},
doi = {10.1093/cercor/bhr020},
file = {:home/epaxon/Documents/Mendeley Desktop/Ito et al. - 2011 - Saccade-related modulations of neuronal excitability support synchrony of visually elicited spikes.pdf:pdf},
isbn = {1460-2199 (Electronic)$\backslash$r1047-3211 (Linking)},
issn = {10473211},
journal = {Cerebral Cortex},
keywords = {free viewing,local field potential,phase locking,primary visual cortex,spike synchrony},
number = {11},
pages = {2482--2497},
pmid = {21459839},
title = {{Saccade-related modulations of neuronal excitability support synchrony of visually elicited spikes}},
volume = {21},
year = {2011}
}
@article{Izhikevich2006,
abstract = {We present a minimal spiking network that can polychronize, that is, exhibit reproducible time-locked but not synchronous firing patterns with millisecond precision, as in synfire braids. The network consists of cortical spiking neurons with axonal conduction delays and spike-timing-dependent plasticity (STDP); a ready-to-use MATLAB code is included. It exhibits sleeplike oscillations, gamma (40 Hz) rhythms, conversion of firing rates to spike timings, and other interesting regimes. Due to the interplay between the delays and STDP, the spiking neurons spontaneously self-organize into groups and generate patterns of stereotypical polychronous activity. To our surprise, the number of coexisting polychronous groups far exceeds the number of neurons in the network, resulting in an unprecedented memory capacity of the system. We speculate on the significance of polychrony to the theory of neuronal group selection (TNGS, neural Darwinism), cognitive neural computations, binding and gamma rhythm, mechanisms of attention, and consciousness as "attention to memories."},
author = {Izhikevich, Eugene M},
doi = {10.1162/089976606775093882},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Attention,Attention: physiology,Brain,Brain: physiology,Memory,Memory: physiology,Models,Neural Networks (Computer),Neurological,Neuronal Plasticity,Neuronal Plasticity: physiology,Neurons,Neurons: physiology},
month = {feb},
number = {2},
pages = {245--82},
pmid = {16378515},
shorttitle = {Neural Comput},
title = {{Polychronization: computation with spikes.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16378515},
volume = {18},
year = {2006}
}
@article{Izhikevich2000,
author = {Izhikevich, Eugene M},
file = {:home/epaxon/Documents/Mendeley Desktop/Izhikevich - 2000 - Computing With Oscillators.pdf:pdf},
journal = {Neural Networks},
pages = {1--14},
title = {{Computing With Oscillators}},
year = {2000}
}
@article{Izhikevich2003a,
abstract = {A model is presented that reproduces spiking and bursting behavior of known types of cortical neurons. The model combines the biologically plausibility of Hodgkin-Huxley-type dynamics and the computational efficiency of integrate-and-fire neurons. Using this model, one can simulate tens of thousands of spiking cortical neurons in real time (1 ms resolution) using a desktop PC.},
author = {Izhikevich, Eugene M},
doi = {10.1109/TNN.2003.820440},
file = {:home/epaxon/Documents/Mendeley Desktop/Izhikevich - 2003 - Simple model of spiking neurons.pdf:pdf},
issn = {1045-9227},
journal = {IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council},
month = {jan},
number = {6},
pages = {1569--72},
pmid = {18244602},
shorttitle = {IEEE Trans Neural Netw},
title = {{Simple model of spiking neurons.}},
volume = {14},
year = {2003}
}
@article{Jacobs2007,
author = {Jacobs, J. and Kahana, M. J. and Ekstrom, A. D. and Fried, I.},
doi = {10.1523/JNEUROSCI.4636-06.2007},
file = {:home/epaxon/Documents/Mendeley Desktop/Jacobs et al. - 2007 - Brain Oscillations Control Timing of Single-Neuron Activity in Humans.pdf:pdf},
issn = {0270-6474},
journal = {Journal of Neuroscience},
keywords = {gamma,intracranial eeg,local field potential,navigation,phase locking,theta},
number = {14},
pages = {3839--3844},
title = {{Brain Oscillations Control Timing of Single-Neuron Activity in Humans}},
url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.4636-06.2007},
volume = {27},
year = {2007}
}
@article{Jain2011,
abstract = {Next-generation sequencing (NGS) technologies provide a revolutionary tool with numerous applications in transcriptome studies. The power of NGS technologies to address diverse biological questions has already been proved in many studies. One of the most important applications of NGS is the sequencing and characterization of transcriptome of a non-model species using RNA-seq. This application of NGS technologies can be used to dissect the complete expressed gene content of an organism. In this article, I illustrate the use of NGS technologies in transcriptome characterization of a non-model species taking example of chickpea from our recent studies.},
archivePrefix = {arXiv},
arxivId = {1203.2655},
author = {Jain, Mukesh},
doi = {10.1371/Citation},
eprint = {1203.2655},
file = {:home/epaxon/Documents/Mendeley Desktop/Jain - 2011 - A next-generation approach to the characterization of a non-model plant transcriptome.pdf:pdf},
isbn = {0011-3891},
issn = {00113891},
journal = {Current Science},
keywords = {Expressed sequence tags,Next-generation sequencing technologies,Non-model species,Transcriptome},
number = {11},
pages = {1435--1439},
pmid = {1000090690},
title = {{A next-generation approach to the characterization of a non-model plant transcriptome}},
volume = {101},
year = {2011}
}
@article{Jankowski1996,
abstract = {A model of a multivalued associative memory is presented. This memory has the form of a fully connected attractor neural network composed of multistate complex-valued neurons. Such a network is able to perform the task of storing and recalling gray-scale images. It is also shown that the complex-valued fully connected neural network may be considered as a generalization of a Hopfield network containing real-valued neurons. A computational energy function is introduced and evaluated in order to prove network stability for asynchronous dynamics. Storage capacity as related to the number of accessible neuron states is also estimated.},
author = {Jankowski, Stanislaw and Lozowski, Andrzej and Zurada, Jacek M.},
doi = {10.1109/72.548176},
file = {:home/epaxon/Documents/Mendeley Desktop/Jankowski, Lozowski, Zurada - 1996 - Complex-valued multistate neural associative memory.pdf:pdf},
isbn = {1045-9227},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
number = {6},
pages = {1491--1496},
pmid = {18263542},
title = {{Complex-valued multistate neural associative memory}},
volume = {7},
year = {1996}
}
@article{Jehee2009,
abstract = {Biphasic neural response properties, where the optimal stimulus for driving a neural response changes from one stimulus pattern to the opposite stimulus pattern over short periods of time, have been described in several visual areas, including lateral geniculate nucleus (LGN), primary visual cortex (V1), and middle temporal area (MT). We describe a hierarchical model of predictive coding and simulations that capture these temporal variations in neuronal response properties. We focus on the LGN-V1 circuit and find that after training on natural images the model exhibits the brain's LGN-V1 connectivity structure, in which the structure of V1 receptive fields is linked to the spatial alignment and properties of center-surround cells in the LGN. In addition, the spatio-temporal response profile of LGN model neurons is biphasic in structure, resembling the biphasic response structure of neurons in cat LGN. Moreover, the model displays a specific pattern of influence of feedback, where LGN receptive fields that are aligned over a simple cell receptive field zone of the same polarity decrease their responses while neurons of opposite polarity increase their responses with feedback. This phase-reversed pattern of influence was recently observed in neurophysiology. These results corroborate the idea that predictive feedback is a general coding strategy in the brain.},
author = {Jehee, Janneke F M and Ballard, Dana H.},
doi = {10.1371/journal.pcbi.1000373},
file = {:home/epaxon/Documents/Mendeley Desktop/Jehee, Ballard - 2009 - Predictive feedback can account for biphasic responses in the lateral geniculate nucleus.pdf:pdf},
isbn = {1553-7358 (Electronic)},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {5},
pmid = {19412529},
title = {{Predictive feedback can account for biphasic responses in the lateral geniculate nucleus}},
volume = {5},
year = {2009}
}
@article{Kamondi1998a,
abstract = {Theta frequency field oscillation reflects synchronized synaptic potentials that entrain the discharge of neuronal populations within the D100–200 ms range. The cellular-synaptic generation of theta activity in the hippocampus was investigated by intracellular recordings from the somata and dendrites of CA1 pyramidal cells in urethane-anesthetized rats. The recorded neurons were verified by intracellular injection of biocytin. Transition from non-theta to theta state was charac-terized by a large decrease in the input resistance of the neuron (39{\%} in the soma), tonic somatic hyperpolarization and dendritic depolarization. The probability of pyramidal cell discharge, as measured in single cells and from a population of extracellularly recorded units, was highest at or slightly after the negative peak of the field theta recorded from the pyramidal layer. In contrast, cyclic depolarizations in dendrites corre-sponded to the positive phase of the pyramidal layer field theta (i.e. the hyperpolarizing phase of somatic theta). Current-induced depolarization of the dendrite triggered large amplitude slow spikes (putative Ca 2؉ spikes) which were phase-locked to the positive phase of field theta. In the absence of background theta, strong dendritic depolarization by current injection led to large amplitude, self-sustained oscillation in the theta frequency range. Depolarization of the neuron resulted in a voltage-dependent phase precession of the action potentials. The voltage-dependent phase-precession was replicated by a two-compartment conduc-tance model. Using an active (bursting) dendritic compartment spike phase advancement of action potentials, relative to the somatic theta rhythm, occurred up to 360 degrees. These data indicate that distal dendritic depolarization of the pyramidal cell by the entorhinal input during theta overlaps in time with somatic hyperpolarization. As a result, most pyramidal cells are either silent or discharge with single spikes on the negative portion of local field theta (i.e., when the somatic region is least polarized). However, strong dendritic excitation may overcome periso-matic inhibition and the large depolarizing theta rhythm in the dendrites may induce spike bursts at an earlier phase of the extracellular theta cycle. The magnitude of dendritic depolarization is reflected by the timing of action potentials within the theta cycle. We hypothesize that the competition between the out-of-phase theta oscilla-tion in the soma and dendrite is responsible for the advancement of spike discharges observed in the behav-ing animal. Hippocampus 1998;8:244–261.},
author = {Kamondi, Anita and Acsady, Laszlo and Wang, Xiao-Jing and Buzsaki, Gyorgy},
file = {:home/epaxon/Documents/Mendeley Desktop/Kamondi et al. - 1998 - Theta Oscillations in Somata and Dendrites of Hippocampal Pyramidal Cells In Vivo Activity-Dependent Phase-Prece.pdf:pdf},
journal = {Hippocampus},
keywords = {dendrites,ing,inhibition,input resistance,network,resonance,spike tim-,temporal coding},
number = {March},
pages = {244--261},
title = {{Theta Oscillations in Somata and Dendrites of Hippocampal Pyramidal Cells In Vivo: Activity-Dependent Phase-Precession of Action Potentials}},
volume = {8},
year = {1998}
}
@article{Kaplan2014,
abstract = {Olfactory sensory information passes through several processing stages before an odor percept emerges. The question how the olfactory system learns to create odor representations linking those different levels and how it learns to connect and discriminate between them is largely unresolved. We present a large-scale network model with single and multi-compartmental Hodgkin-Huxley type model neurons representing olfactory receptor neurons (ORNs) in the epithelium, periglomerular cells, mitral/tufted cells and granule cells in the olfactory bulb (OB), and three types of cortical cells in the piriform cortex (PC). Odor patterns are calculated based on affinities between ORNs and odor stimuli derived from physico-chemical descriptors of behaviorally relevant real-world odorants. The properties of ORNs were tuned to show saturated response curves with increasing concentration as seen in experiments. On the level of the OB we explored the possibility of using a fuzzy concentration interval code, which was implemented through dendro-dendritic inhibition leading to winner-take-all like dynamics between mitral/tufted cells belonging to the same glomerulus. The connectivity from mitral/tufted cells to PC neurons was self-organized from a mutual information measure and by using a competitive Hebbian-Bayesian learning algorithm based on the response patterns of mitral/tufted cells to different odors yielding a distributed feed-forward projection to the PC. The PC was implemented as a modular attractor network with a recurrent connectivity that was likewise organized through Hebbian-Bayesian learning. We demonstrate the functionality of the model in a one-sniff-learning and recognition task on a set of 50 odorants. Furthermore, we study its robustness against noise on the receptor level and its ability to perform concentration invariant odor recognition. Moreover, we investigate the pattern completion capabilities of the system and rivalry dynamics for odor mixtures.},
author = {Kaplan, Bernhard A. and Lansner, Anders},
doi = {10.3389/fncir.2014.00005},
file = {:home/epaxon/Documents/Mendeley Desktop/Kaplan, Lansner - 2014 - A spiking neural network model of self-organized pattern recognition in the early mammalian olfactory system.pdf:pdf},
isbn = {1662-5110},
issn = {1662-5110},
journal = {Frontiers in Neural Circuits},
keywords = {bcpnn,concentration invariance,large-scale neuromorphic systems,network,olfactory bulb,pattern recognition,pattern recognition, olfactory bulb, piriform cort,pattern rivalry,piriform cortex,spiking neural},
number = {February},
pages = {1--20},
pmid = {24570657},
title = {{A spiking neural network model of self-organized pattern recognition in the early mammalian olfactory system}},
url = {http://journal.frontiersin.org/article/10.3389/fncir.2014.00005/abstract},
volume = {8},
year = {2014}
}
@article{Kasabov2013,
abstract = {On-line learning and recognition of spatio- and spectro-temporal data (SSTD) is a very challenging task and an important one for the future development of autonomous machine learning systems with broad applications. Models based on spiking neural networks (SNN) have already proved their potential in capturing spatial and temporal data. One class of them, the evolving SNN (eSNN), uses a one-pass rank-order learning mechanism and a strategy to evolve a new spiking neuron and new connections to learn new patterns from incoming data. So far these networks have been mainly used for fast image and speech frame-based recognition. Alternative spike-time learning methods, such as Spike-Timing Dependent Plasticity (STDP) and its variant Spike Driven Synaptic Plasticity (SDSP), can also be used to learn spatio-temporal representations, but they usually require many iterations in an unsupervised or semi-supervised mode of learning. This paper introduces a new class of eSNN, dynamic eSNN, that utilise both rank-order learning and dynamic synapses to learn SSTD in a fast, on-line mode. The paper also introduces a new model called deSNN, that utilises rank-order learning and SDSP spike-time learning in unsupervised, supervised, or semi-supervised modes. The SDSP learning is used to evolve dynamically the network changing connection weights that capture spatio-temporal spike data clusters both during training and during recall. The new deSNN model is first illustrated on simple examples and then applied on two case study applications: (1) moving object recognition using address-event representation (AER) with data collected using a silicon retina device; (2) EEG SSTD recognition for brain-computer interfaces. The deSNN models resulted in a superior performance in terms of accuracy and speed when compared with other SNN models that use either rank-order or STDP learning. The reason is that the deSNN makes use of both the information contained in the order of the first input spikes (which information is explicitly present in input data streams and would be crucial to consider in some tasks) and of the information contained in the timing of the following spikes that is learned by the dynamic synapses as a whole spatio-temporal pattern. ?? 2012 Elsevier Ltd.},
author = {Kasabov, Nikola and Dhoble, Kshitij and Nuntalid, Nuttapod and Indiveri, Giacomo},
doi = {10.1016/j.neunet.2012.11.014},
file = {:home/epaxon/Documents/Mendeley Desktop/Kasabov et al. - 2013 - Dynamic evolving spiking neural networks for on-line spatio- and spectro-temporal pattern recognition.pdf:pdf},
isbn = {9781467322782},
issn = {08936080},
journal = {Neural Networks},
keywords = {Dynamic synapses,EEG pattern recognition,Evolving connectionist systems,Moving object recognition,Rank-order coding,Spatio-temporal pattern recognition,Spike time based learning,Spiking neural networks},
number = {1995},
pages = {188--201},
pmid = {23340243},
publisher = {Elsevier Ltd},
title = {{Dynamic evolving spiking neural networks for on-line spatio- and spectro-temporal pattern recognition}},
url = {http://dx.doi.org/10.1016/j.neunet.2012.11.014},
volume = {41},
year = {2013}
}
@article{Kaslik2009,
abstract = {This paper is devoted to the analysis of a discrete-time-delayed Hopfield-type neural network of p neurons with ring architecture. The stability domain of the null solution is found, the values of the characteristic parameter for which bifurcations occur at the origin are identified and the existence of Fold/Cusp, Neimark-Sacker and Flip bifurcations is proved. These bifurcations are analyzed by applying the center manifold theorem and the normal form theory. It is proved that resonant 1:3 and 1:4 bifurcations may also be present. It is shown that the dynamics in a neighborhood of the null solution become more and more complex as the characteristic parameter grows in magnitude and passes through the bifurcation values. A theoretical proof is given for the occurrence of Marotto's chaotic behavior, if the magnitudes of the interconnection coefficients are large enough and at least one of the activation functions has two simple real roots. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Kaslik, Eva and Balint, Stefan},
doi = {10.1016/j.neunet.2009.03.009},
file = {:home/epaxon/Documents/Mendeley Desktop/Kaslik, Balint - 2009 - Complex and chaotic dynamics in a discrete-time-delayed Hopfield neural network with ring architecture.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Bifurcation,Chaos,Hopfield neural network,Ring architecture,Stability},
number = {10},
pages = {1411--1418},
pmid = {19386470},
publisher = {Elsevier Ltd},
title = {{Complex and chaotic dynamics in a discrete-time-delayed Hopfield neural network with ring architecture}},
url = {http://dx.doi.org/10.1016/j.neunet.2009.03.009},
volume = {22},
year = {2009}
}
@article{Kleinfeld1988,
abstract = {Cyclic patterns of motor neuron activity are involved in the production of many rhythmic movements, such as walking, swimming, and scratching. These movements are controlled by neural circuits referred to as central pattern generators (CPGs). Some of these circuits function in the absence of both internal pacemakers and external feedback. We describe an associative neural network model whose dynamic behavior is similar to that of CPGs. The theory predicts the strength of all possible connections between pairs of neurons on the basis of the outputs of the CPG. It also allows the mean operating levels of the neurons to be deduced from the measured synaptic strengths between the pairs of neurons. We apply our theory to the CPG controlling escape swimming in the mollusk Tritonia diomedea. The basic rhythmic behavior is shown to be consistent with a simplified model that approximates neurons as threshold units and slow synaptic responses as elementary time delays. The model we describe may have relevance to other fixed action behaviors, as well as to the learning, recall, and recognition of temporally ordered information.},
author = {Kleinfeld, D. and Sompolinsky, H.},
doi = {10.1016/S0006-3495(88)83041-8},
file = {:home/epaxon/Documents/Mendeley Desktop/Kleinfeld, Sompolinsky - 1988 - Associative neural network model for the generation of temporal patterns. Theory and application to cent.pdf:pdf},
issn = {00063495},
journal = {Biophysical Journal},
number = {6},
pages = {1039--1051},
pmid = {3233265},
title = {{Associative neural network model for the generation of temporal patterns. Theory and application to central pattern generators}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0006349588830418},
volume = {54},
year = {1988}
}
@article{Kleinfeld1986,
abstract = {Sequential patterns of neural output activity form the basis of many biological processes, such as the cyclic pattern of outputs that control locomotion. I show how such sequences can be generated by a class of model neural net-works that make defined sets of transitions between selected memory states. Sequence-generating networks depend upon the interplay between two sets of synaptic connections. One set acts to stabilize the network in its current memory state, while the second set, whose action is delayed in time, causes the net-work to make specified transitions between the memories. The dynamic properties of these networks are described in terms of motion along an energy surface. The performance of the net-works, both with intact connections and with noisy or missing connections, is illustrated by numerical examples. In addition, I present a scheme for the recognition of externally generated sequences by these networks. Cyclic patterns of motor neuron activity are involved in the production of many rhythmic movements, such as locomo-tion. The neural circuits that control the muscles involved in executing the movements are typically referred to as central pattern generators (CPGs) (for reviews see refs. 1-3). A common feature of CPGs is that sequence generation can occur in the absence of both sensory feedback and feedback from other neural centers. A second feature of some CPGs is that they do not contain intrinsic pacemakers. Thus the cy-clic activity, and the timing of this activity, is a collective property of the CPG. In this paper I present a formal model of sequence genera-tion in the context of networks of model neurons, such as those discussed by Hopfield (4, 5) (see also refs. 6-13). Hop-field networks use highly interconnected model neurons to perform complex computational tasks. These tasks, such as recalling information by association (4, 9, 13) or solving opti-mization problems (14), involve networks that seek a single, stationary state as their output. The sequential state genera-tors that I describe do not come to rest in a single state. Rather they make transitions between selected states and thus generate sequential patterns of bit sequences. Se-quence-generating networks provide a formal setting for un-derstanding some CPGs. They also provide a basis for the sequential recall of information in associative memories.},
author = {Kleinfeld, David},
doi = {10.1073/pnas.83.24.9469},
file = {:home/epaxon/Documents/Mendeley Desktop/Kleinfeld - 1986 - Sequential state generation by model neural networks.pdf:pdf},
isbn = {0027-8424 (Print)$\backslash$n0027-8424 (Linking)},
issn = {0027-8424},
journal = {Biophysics},
number = {December},
pages = {9469--9473},
pmid = {3467316},
title = {{Sequential state generation by model neural networks}},
volume = {83},
year = {1986}
}
@article{Kobayashi2017,
author = {Kobayashi, Masaki},
file = {:home/epaxon/Documents/Mendeley Desktop/Kobayashi - 2017 - Fast Recall for Complex-Valued Hopfield Neural Networks with Projection Rules.pdf:pdf},
journal = {Comoputational Intelligence and Neuroscience},
pages = {1--6},
title = {{Fast Recall for Complex-Valued Hopfield Neural Networks with Projection Rules}},
volume = {2017},
year = {2017}
}
@article{Kwag2011,
author = {Kwag, Jeehyun and McLelland, Douglas and Paulsen, Ole},
doi = {10.3389/fnhum.2011.00003},
file = {:home/epaxon/Documents/Mendeley Desktop/Kwag, McLelland, Paulsen - 2011 - Phase of Firing as a Local Window for Efficient Neuronal Computation Tonic and Phasic Mechanisms in th.pdf:pdf},
isbn = {1662-5161 (Electronic)$\backslash$r1662-5161 (Linking)},
issn = {1662-5161},
journal = {Frontiers in Human Neuroscience},
number = {February},
pages = {1--8},
pmid = {21344003},
title = {{Phase of Firing as a Local Window for Efficient Neuronal Computation: Tonic and Phasic Mechanisms in the Control of Theta Spike Phase}},
url = {http://journal.frontiersin.org/article/10.3389/fnhum.2011.00003/abstract},
volume = {5},
year = {2011}
}
@article{Lee2002,
abstract = {This paper presents a novel continuous-time Hopfield-type network which is effective for temporal sequence recognition. The fundamental problem of recalling pattern sequences by neural networks is first reviewed. Since it is difficult to implement a desired flow vector field distribution by using conventional matrix encoding scheme, a time-varying Hopfield model (TVHM) is proposed. The weight matrix of the TVHM is constructed in such a way that its auto-correlation and cross-correlation parts are encoded from two different sets of patterns. With this mechanism, flow vectors between any two adjacent stored patterns are of the same directions. Moreover, the flow vector field distribution around a stored pattern can be modulated by the time variable. Then, theoretical results regarding the radii of attraction and the recalling dynamics of the TVHM are presented. The proposed approach is different from the existing methods because neither synchronous dynamics nor interpolated training patterns are required. A way of increasing the storage capacity of the TVHM is proposed. Finally, experimental results are presented to illustrate the validity, capacity, recall capability, and the applications of the proposed model.},
author = {Lee, Donq Liang},
doi = {10.1109/72.991419},
file = {:home/epaxon/Documents/Mendeley Desktop/Lee - 2002 - Pattern sequence recognition using a time-varying hopfield network.pdf:pdf},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Hopfield networks,Pattern sequence recognition,Time-varying systems},
number = {2},
pages = {330--342},
pmid = {18244435},
title = {{Pattern sequence recognition using a time-varying hopfield network}},
volume = {13},
year = {2002}
}
@article{Lee2005,
abstract = {Working memory has been linked to elevated single neuron discharge in monkeys and to oscillatory changes in the human EEG, but the relation between these effects has remained largely unexplored. We addressed this question by measuring local field potentials and single unit activity simultaneously from multiple electrodes placed in extrastriate visual cortex while monkeys were performing a working memory task. We describe a significant enhancement in theta band energy during the delay period. Theta oscillations had a systematic effect on single neuron activity, with neurons emitting more action potentials near their preferred angle of each theta cycle. Sample-selective delay activity was enhanced if only action potentials emitted near the preferred theta angle were considered. Our results suggest that extrastriate visual cortex is involved in short-term maintenance of information and that theta oscillations provide a mechanism for structuring the recurrent interaction between neurons in different brain regions that underlie working memory.},
author = {Lee, Han and Simpson, Gregory V. and Logothetis, Nikos K. and Rainer, Gregor},
doi = {10.1016/j.neuron.2004.12.025},
file = {:home/epaxon/Documents/Mendeley Desktop/Lee et al. - 2005 - Phase locking of single neuron activity to theta oscillations during working memory in monkey extrastriate visual co.pdf:pdf},
isbn = {0896-6273},
issn = {08966273},
journal = {Neuron},
number = {1},
pages = {147--156},
pmid = {15629709},
title = {{Phase locking of single neuron activity to theta oscillations during working memory in monkey extrastriate visual cortex}},
volume = {45},
year = {2005}
}
@article{Lestienne2001,
abstract = {To what extent is the variability of the neuronal responses compatible with the use of spike timing for sensory information processing by the central nervous system? In reviewing the state of the art of this question, I first analyze the characteristics of this variability with its three elements: synaptic noise, impact of ongoing activity and possible fluctuations in evoked responses. I then review the recent literature on the various sensory modalities: somato-sensory, olfactory, gustatory and visual and auditory processing. I emphasize that the conditions in which precise timing, at the millisecond level, is usually obtained, are conditions that usually require dynamic stimulation or sharp changes in the stimuli. By contrast, situations in which stimulation not belonging to the temporal domain is temporally encoded lead to much coarser temporal coding; although in both cases, neural networks transmit the signals with similarly high precision. Synchronization among neurons is an important tool in information processing in both cases but again seems to act either at millisecond or tens of millisecond levels. Information theory applied to both situations confirms that the average rate of information transmission is much higher in dynamic than in static situations. These facts suggest that channels of precise temporal encoding may exist in the brain but imply populations of neurons working in a yet to be discovered way. {\textcopyright} 2002 Published by Elsevier Science Ltd.},
author = {Lestienne, R{\'{e}}my},
doi = {10.1016/S0301-0082(01)00019-3},
file = {:home/epaxon/Documents/Mendeley Desktop/Lestienne - 2001 - Spike timing, synchronization and information processing on the sensory side of the central nervous system.pdf:pdf},
isbn = {0301-0082 (Print)$\backslash$r0301-0082 (Linking)},
issn = {03010082},
journal = {Progress in Neurobiology},
number = {6},
pages = {545--591},
pmid = {11728644},
title = {{Spike timing, synchronization and information processing on the sensory side of the central nervous system}},
volume = {65},
year = {2001}
}
@article{Li2016,
author = {Li, Chaojie and Yu, Xinghuo and Huang, Tingwen and Chen, Guo and He, Xing},
doi = {10.1109/TNNLS.2015.2496658},
file = {:home/epaxon/Documents/Mendeley Desktop/Li et al. - 2016 - A Generalized Hopfield Network for Nonsmooth Constrained Convex Optimization Lie Derivative Approach.pdf:pdf},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Constraints,Filippov solutions,Hopfield network,Lie derivative.,enhanced Fritz John conditions,global optimization},
number = {2},
pages = {308--321},
title = {{A Generalized Hopfield Network for Nonsmooth Constrained Convex Optimization: Lie Derivative Approach}},
volume = {27},
year = {2016}
}
@article{Liao2001,
author = {Liao, Xiaofeng and Wong, Kwok-wo and Wu, Zhongfu and Chen, Guanrong},
doi = {10.1109/81.964428},
file = {:home/epaxon/Documents/Mendeley Desktop/Liao et al. - 2001 - Novel robust stability criteria for interval-delayed Hopfield neural networks.pdf:pdf},
issn = {10577122},
journal = {IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications},
number = {11},
pages = {1355--1359},
title = {{Novel robust stability criteria for interval-delayed Hopfield neural networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=964428},
volume = {48},
year = {2001}
}
@article{Lipton2015,
abstract = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research.},
archivePrefix = {arXiv},
arxivId = {1506.00019},
author = {Lipton, Zachary C. and Berkowitz, John and Elkan, Charles},
doi = {10.1145/2647868.2654889},
eprint = {1506.00019},
file = {:home/epaxon/Documents/Mendeley Desktop/Lipton, Berkowitz, Elkan - 2015 - A Critical Review of Recurrent Neural Networks for Sequence Learning.pdf:pdf},
isbn = {9781450330633},
issn = {9781450330633},
journal = {arXiv},
pages = {1--38},
pmid = {18267787},
title = {{A Critical Review of Recurrent Neural Networks for Sequence Learning}},
url = {http://arxiv.org/abs/1506.00019},
year = {2015}
}
@article{Lisman2008,
abstract = {Brain oscillations are important in controlling the timing of neuronal firing. This process has been extensively analyzed in connection with gamma frequency oscillations and more recently with respect to theta frequency oscillations. Here we review evidence that theta and gamma oscillations work together to form a neural code. This coding scheme provides a way for multiple neural ensembles to represent an ordered sequence of items. In the hippocampus, this coding scheme is utilized during the phase precession, a phenomenon that can be interpreted as the recall of sequences of items (places) from long-term memory. The same coding scheme may be used in certain cortical regions to encode multi-item short-term memory. The possibility that abnormalities in theta/gamma could underlie symptoms of schizophrenia is discussed.},
author = {Lisman, John and Buzs{\'{a}}ki, Gy{\"{o}}rgy},
doi = {10.1093/schbul/sbn060},
file = {:home/epaxon/Documents/Mendeley Desktop/Lisman, Buzs{\'{a}}ki - 2008 - A neural coding scheme formed by the combined function of gamma and theta oscillations.pdf:pdf},
isbn = {0586-7614 (Print)$\backslash$n0586-7614 (Linking)},
issn = {05867614},
journal = {Schizophrenia Bulletin},
keywords = {Hippocampus,Memory,Phase precession,Synchronization},
number = {5},
pages = {974--980},
pmid = {18559405},
title = {{A neural coding scheme formed by the combined function of gamma and theta oscillations}},
volume = {34},
year = {2008}
}
@article{Liu2009,
author = {Liu, Xiaoyu and Fang, Kangling and Liu, Bin},
file = {:home/epaxon/Documents/Mendeley Desktop/Liu, Fang, Liu - 2009 - A synthesis method based on stability analysis for complex-valued Hopfield neural network.pdf:pdf},
isbn = {9788995605691},
journal = {Proceedings of the 7th Asian Control Conference},
keywords = {1975,algorithm in,associative memory,attracted,complex-valued hopfield neural network,complex-valued neural network has,ince widrow proposed complex-valued,stability analysis,synthesis method},
title = {{A synthesis method based on stability analysis for complex-valued Hopfield neural network}},
year = {2009}
}
@article{Losonczy2010,
abstract = {Although hippocampal theta oscillations represent a prime example of temporal coding in the mammalian brain, little is known about the specific biophysical mechanisms. Intracellular recordings support a particular abstract oscillatory interference model of hippocampal theta activity, the soma-dendrite interference model. To gain insight into the cellular and circuit level mechanisms of theta activity, we implemented a similar form of interference using the actual hippocampal network in mice in vitro. We found that pairing increasing levels of phasic dendritic excitation with phasic stimulation of perisomatic projecting inhibitory interneurons induced a somatic polarization and action potential timing profile that reproduced most common features. Alterations in the temporal profile of inhibition were required to fully capture all features. These data suggest that theta-related place cell activity is generated through an interaction between a phasic dendritic excitation and a phasic perisomatic shunting inhibition delivered by interneurons, a subset of which undergo activity-dependent presynaptic modulation.},
author = {Losonczy, Attila and Zemelman, Boris V and Vaziri, Alipasha and Magee, Jeffrey C},
doi = {10.1038/nn.2597},
file = {:home/epaxon/Documents/Mendeley Desktop/Losonczy et al. - 2010 - Network mechanisms of theta related neuronal activity in hippocampal CA1 pyramidal neurons.pdf:pdf},
isbn = {1546-1726},
issn = {1097-6256},
journal = {Nature Neuroscience},
number = {8},
pages = {967--972},
pmid = {20639875},
publisher = {Nature Publishing Group},
title = {{Network mechanisms of theta related neuronal activity in hippocampal CA1 pyramidal neurons}},
url = {http://www.nature.com/doifinder/10.1038/nn.2597},
volume = {13},
year = {2010}
}
@article{Lu2001,
abstract = {Because auditory cortical neurons have limited stimulus-synchronized responses, cortical representations of more rapidly occurring but still perceivable stimuli remain unclear. Here we show that there are two largely distinct populations of neurons in the auditory cortex of awake primates: one with stimulus-synchronized discharges that, with a temporal code, explicitly represented slowly occurring sound sequences and the other with non-stimulus-synchronized discharges that, with a rate code, implicitly represented rapidly occurring events. Furthermore, neurons of both populations displayed selectivity in their discharge rates to temporal features within a short time-window. Our results suggest that the combination of temporal and rate codes in the auditory cortex provides a possible neural basis for the wide perceptual range of temporal information.},
author = {Lu, T and Liang, L and Wang, X},
doi = {10.1038/nn737},
file = {:home/epaxon/Documents/Mendeley Desktop/Lu, Liang, Wang - 2001 - Temporal and rate representations of time-varying signals in the auditory cortex of awake primates.pdf:pdf},
isbn = {10976256},
issn = {1097-6256},
journal = {Nature neuroscience},
number = {11},
pages = {1131--1138},
pmid = {11593234},
title = {{Temporal and rate representations of time-varying signals in the auditory cortex of awake primates.}},
volume = {4},
year = {2001}
}
@article{Maass1997,
abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.},
author = {Maass, W},
doi = {10.1016/S0893-6080(97)00011-7},
file = {:home/epaxon/Documents/Mendeley Desktop/Maass - 1997 - Networks of spiking neurons the third generation of neural network models.pdf:pdf},
isbn = {08936080},
issn = {08936080},
journal = {Neural networks},
keywords = {Computational Neuroscience,Computational complexity,Integrate-and-fire neuron,Lower bounds,Sigmoidal neural nets,Spiking neuron},
mendeley-tags = {Computational Neuroscience},
number = {9},
pages = {1659--1671},
title = {{Networks of spiking neurons: the third generation of neural network models}},
url = {http://www.sciencedirect.com/science/article/pii/S0893608097000117},
volume = {10},
year = {1997}
}
@article{Maass1997c,
abstract = {We show that networks of relatively realistic mathematical models for biological neurons in principle can simulate arbitrary feedforward sigmoidal neural nets in a way that has previously not been considered. This new approach is based on temporal coding by single spikes (respectively by the timing of synchronous firing in pools of neurons) rather than on the traditional interpretation of analog variables in terms of firing rates. The resulting new simulation is substantially faster and hence more consistent with experimental results about the maximal speed of information processing in cortical neural systems.As a consequence we can show that networks of noisy spiking neurons are ''universal approximators'' in the sense that they can approximate with regard to temporal coding any given continuous function of several variables. This result holds for a fairly large class of schemes for coding analog variables by firing times of spiking neurons.This new proposal for the possible organization of computations in networks of spiking neurons systems has some interesting consequences for the type of learning rules that would be needed to explain the self-organization of such networks.Finally, the fast and noise-robust implementation of sigmoidal neural nets by temporal coding points to possible new ways of implementing feedforward and recurrent sigmoidal neural nets with pulse stream VLSI.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Maass, W},
doi = {10.1162/neco.1997.9.2.279},
eprint = {arXiv:1011.1669v3},
file = {:home/epaxon/Documents/Mendeley Desktop/Maass - 1997 - Fast sigmoidal networks via spiking neurons.pdf:pdf},
isbn = {9788578110796},
issn = {0899-7667},
journal = {Neural Computation},
number = {2},
pages = {279--304},
pmid = {25246403},
title = {{Fast sigmoidal networks via spiking neurons}},
url = {papers3://publication/uuid/EC0FA46A-410F-4DB6-A6D3-B8634EB794CA},
volume = {9},
year = {1997}
}
@article{Maass1997b,
abstract = {Abstract We exhibit a novel way of simulating sigmoidal neural nets by networks of noisy spiking neurons in temporal coding. Furthermore it is shown that networks of noisy spiking neurons with temporal coding have a strictly larger computational power than sigmoidal ...},
author = {Maass, W.},
doi = {10.1.1.49.2055},
file = {:home/epaxon/Documents/Mendeley Desktop/Maass - 1997 - Noisy spiking neurons with temporal coding have more computational power than sigmoidal neurons.pdf:pdf},
issn = {1049-5258},
journal = {Advances in Neural Information Processing Systems},
pages = {211--217},
title = {{Noisy spiking neurons with temporal coding have more computational power than sigmoidal neurons}},
volume = {9},
year = {1997}
}
@article{Maass1996,
author = {Maass, Wolfgang},
doi = {10.1162/neco.1996.8.1.1},
file = {:home/epaxon/Documents/Mendeley Desktop/Maass - 1996 - Lower Bounds for the Computational Power of Networks of Spiking Neurons.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
month = {jan},
number = {1},
pages = {1--40},
title = {{Lower Bounds for the Computational Power of Networks of Spiking Neurons}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1996.8.1.1},
volume = {8},
year = {1996}
}
@article{Maass1997a,
abstract = {A theoretical model for analogue computation in networks of spiking neurons with temporal coding is introduced and tested through simulations in GENESIS. It turns out that the use of multiple synapses yields very noise robust mechanisms for analogue computations via the timing of single spikes in networks of detailed compartmental neuron models.In this way, one arrives at a method for emulating arbitrary Hopfield nets with spiking neurons in temporal coding, yielding new models fbr associative recall of spatio-temporal firing patterns. We also show that it suffices to store these patterns in the efficacies of excitatory synapses.A corresponding layered architecture yields a refinement of the synfire-chain model that can assume a fairly large set of different stable firing patterns for different inputs.},
author = {Maass, Wolfgang and Natschl{\"{a}}ger, Thomas},
doi = {10.1088/0954-898X/8/4/002},
file = {:home/epaxon/Documents/Mendeley Desktop/Maass, Natschl{\"{a}}ger - 1997 - Networks of spiking neurons can emulate arbitrary Hopfield nets in temporal coding.pdf:pdf},
isbn = {0954-898X},
issn = {0954-898X},
journal = {Network: Computation in Neural Systems},
number = {4},
pages = {355--371},
title = {{Networks of spiking neurons can emulate arbitrary Hopfield nets in temporal coding}},
url = {http://www.informaworld.com/openurl?genre=article{\&}doi=10.1088/0954-898X/8/4/002{\&}magic=crossref{\%}7C{\%}7CD404A21C5BB053405B1A640AFFD44AE3},
volume = {8},
year = {1997}
}
@article{Mainen1995,
author = {Mainen, Zachary F. and Sejnowski, Terrence J.},
file = {:home/epaxon/Documents/Mendeley Desktop/Mainen, Sejnowski - 1995 - Reliability of Spike Timing in Neocortical Neurons.pdf:pdf},
journal = {Science},
title = {{Reliability of Spike Timing in Neocortical Neurons}},
volume = {268},
year = {1995}
}
@article{Makay1990,
author = {Makay, Primary Examiner-albert J and Kocher, Erich J},
file = {:home/epaxon/Documents/Mendeley Desktop/Makay, Kocher - 1990 - US . Patent.pdf:pdf},
pages = {2--6},
title = {{US . Patent}},
year = {1990}
}
@article{Masuda2007,
abstract = {Selective attention is an important filter for complex environments where distractions compete with signals. Attention increases both the gamma-band power of cortical local field potentials and the spike-field coherence within the receptive field of an attended object. However, the mechanisms by which gamma-band activity enhances, if at all, the encoding of input signals are not well understood. We propose that gamma oscillations induce binomial-like spike-count statistics across noisy neural populations. Using simplified models of spiking neurons, we show how the discrimination of static signals based on the population spike-count response is improved with gamma induced binomial statistics. These results give an important mechanistic link between the neural correlates of attention and the discrimination tasks where attention is known to enhance performance. Further, they show how a rhythmicity of spike responses can enhance coding schemes that are not temporally sensitive.},
author = {Masuda, Naoki and Doiron, Brent},
doi = {10.1371/journal.pcbi.0030236},
file = {:home/epaxon/Documents/Mendeley Desktop/Masuda, Doiron - 2007 - Gamma oscillations of spiking neural populations enhance signal discrimination.pdf:pdf},
isbn = {1553-734X},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {11},
pages = {2348--2355},
pmid = {18052541},
title = {{Gamma oscillations of spiking neural populations enhance signal discrimination}},
volume = {3},
year = {2007}
}
@article{Mceliece1987,
abstract = {The capacity of the Hopfield associative memory (HAM) is analyzed$\backslash$nby using a statistical approach. By assuming that the memory network in$\backslash$nasynchronous update mode evolves in accordance with a stationary Markov$\backslash$nprocess, the capacity and the recall probability of the asymmetric$\backslash$nnetwork are numerically calculated. A convergence theorem is contrived$\backslash$nwhich, in contrast with that proposed by Hopfield, ensures not only the$\backslash$nconvergence behavior of a symmetric connection matrix but also any$\backslash$nasymmetric connection matrix. If {\textless}e1{\textgreater}M{\textless}/e1{\textgreater} prototype memories, each of$\backslash$nlength {\textless}e1{\textgreater}N{\textless}/e1{\textgreater}, are chosen at random and independently from the set$\backslash$n{\{}-1.1{\}}, the storage capacity for small {\textless}e1{\textgreater}N{\textless}/e1{\textgreater} is shown to be$\backslash$napproximately 0.12{\textless}e1{\textgreater}N{\textless}/e1{\textgreater} (with an acceptance level of 0.985). As$\backslash$n{\textless}e1{\textgreater}N{\textless}/e1{\textgreater} approaches infinity, the asymptotic capacity of the network$\backslash$nis found to be no more than {\textless}e1{\textgreater}N{\textless}/e1{\textgreater}/4 log {\textless}e1{\textgreater}N{\textless}/e1{\textgreater}},
author = {Mceliece, Robert J. and Posner, Edward C. and Rodemich, Eugene R. and Venkatesh, Santosh S.},
doi = {10.1109/TIT.1987.1057328},
file = {:home/epaxon/Documents/Mendeley Desktop/Mceliece et al. - 1987 - The Capacity of the Hopfield Associative Memory.pdf:pdf},
isbn = {0-7803-0559-0},
issn = {15579654},
journal = {IEEE Transactions on Information Theory},
number = {4},
pages = {461--482},
title = {{The Capacity of the Hopfield Associative Memory}},
volume = {33},
year = {1987}
}
@article{McLelland2009,
abstract = {Theoretical and experimental studies suggest that oscillatory modes of processing play an important role in neuronal computations. One well supported idea is that the net excitatory input during oscillations will be reported in the phase of firing, a 'rate-to-phase transform', and that this transform might enable a temporal code. Here, we investigate the efficiency of this code at the level of fundamental single cell computations. We first develop a general framework for the understanding of the rate-to-phase transform as implemented by single neurons. Using whole cell patch-clamp recordings of rat hippocampal pyramidal neurons in vitro, we investigated the relationship between tonic excitation and phase of firing during simulated theta frequency (5 Hz) and gamma frequency (40 Hz) oscillations, over a range of physiological firing rates. During theta frequency oscillations, the phase of the first spike per cycle was a near-linear function of tonic excitation, advancing through a full 180 deg, from the peak to the trough of the oscillation cycle as excitation increased. In contrast, this relationship was not apparent for gamma oscillations, during which the phase of firing was virtually independent of the level of tonic excitatory input within the range of physiological firing rates. We show that a simple analytical model can substantially capture this behaviour, enabling generalization to other oscillatory states and cell types. The capacity of such a transform to encode information is limited by the temporal precision of neuronal activity. Using the data from our whole cell recordings, we calculated the information about the input available in the rate or phase of firing, and found the phase code to be significantly more efficient. Thus, temporal modes of processing can enable neuronal coding to be inherently more efficient, thereby allowing a reduction in processing time or in the number of neurons required.},
author = {McLelland, Douglas and Paulsen, Ole},
doi = {10.1113/jphysiol.2008.164111},
file = {:home/epaxon/Documents/Mendeley Desktop/McLelland, Paulsen - 2009 - Neuronal oscillations and the rate-to-phase transform mechanism, model and mutual information.pdf:pdf},
isbn = {1469-7793 (Electronic)$\backslash$n1469-7793 (Linking)},
issn = {00223751},
journal = {The Journal of Physiology},
number = {4},
pages = {769--785},
pmid = {19103680},
title = {{Neuronal oscillations and the rate-to-phase transform: mechanism, model and mutual information}},
url = {http://doi.wiley.com/10.1113/jphysiol.2008.164111},
volume = {587},
year = {2009}
}
@article{Mehta2002,
abstract = {In the vast majority of brain areas, the firing rates of neurons, averaged over several hundred milliseconds to several seconds, can be strongly modulated by, and provide accurate information about, properties of their inputs. This is referred to as the rate code. However, the biophysical laws of synaptic plasticity require precise timing of spikes over short timescales ({\textless}10 ms). Hence it is critical to understand the physiological mechanisms that can generate precise spike timing in vivo, and the relationship between such a temporal code and a rate code. Here we propose a mechanism by which a temporal code can be generated through an interaction between an asymmetric rate code and oscillatory inhibition. Consistent with the predictions of our model, the rate and temporal codes of hippocampal pyramidal neurons are highly correlated. Furthermore, the temporal code becomes more robust with experience. The resulting spike timing satisfies the temporal order constraints of hebbian learning. Thus, oscillations and receptive field asymmetry may have a critical role in temporal sequence learning.},
author = {Mehta, M R and Lee, a K and Wilson, M a},
doi = {10.1038/nature00807},
file = {:home/epaxon/Documents/Mendeley Desktop/Mehta, Lee, Wilson - 2002 - Role of experience and oscillations in transforming a rate code into a temporal code.pdf:pdf},
isbn = {0028-0836 (Print)$\backslash$n0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
keywords = {and,central cycle,circular anova,estimated as the slope,firing rate were,its intercept in the,neighbouring cycles on each,of the fit line,side,ters to nature,the derivative and instantaneous},
number = {6890},
pages = {741--6},
pmid = {12066185},
title = {{Role of experience and oscillations in transforming a rate code into a temporal code.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12066185},
volume = {417},
year = {2002}
}
@misc{Morita1996,
abstract = {Conventional neural network models for temporal association generally do not work well in the absence of synchronizing neurons. This is because their dynamical properties are fundamentally not suitable for storing sequential patterns, no matter what storage or learning algorithm is used. The present article describes a nonmonotone neural network (NNN) model in which sequential patterns are stored by being embedded in a trajectory attractor of the dynamical system, and recalled stably and smoothly without synchronization, recall is done in such a way that the network state successively moves along the trajectory. A simple and natural learning algorithm for the NNN is also presented, where one only has to vary the input pattern gradually and modify the synaptic weights according to a kind of covariance rule, then the network state follows slightly behind the input pattern, and its trajectory grows to be an attractor with a small number of repetitions.},
author = {Morita, Masahiko},
booktitle = {Neural Networks},
doi = {10.1016/S0893-6080(96)00021-4},
file = {:home/epaxon/Documents/Mendeley Desktop/Morita - 1996 - Memory and learning of sequential patterns by nonmonotone neural networks.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
keywords = {covariance rule,nonmonotone dynamics,nonmonotone neural networks,sequential pattern memory,spatiotemporal pattern learning,temporal association,trajectory attractors},
number = {8},
pages = {1477--1489},
pmid = {12662546},
title = {{Memory and learning of sequential patterns by nonmonotone neural networks}},
volume = {9},
year = {1996}
}
@article{Mou2008,
abstract = {In this brief, the problem of global asymptotic stability for delayed Hopfield neural networks (HNNs) is investigated. A new criterion of asymptotic stability is derived by introducing a new kind of Lyapunov-Krasovskii functional and is formulated in terms of a linear matrix inequality (LMI), which can be readily solved via standard software. This new criterion based on a delay fractioning approach proves to be much less conservative and the conservatism could be notably reduced by thinning the delay fractioning. An example is provided to show the effectiveness and the advantage of the proposed result.},
author = {Mou, Shaoshuai and Gao, Huijun and Lam, James and Qiang, Wenyi},
doi = {10.1109/TNN.2007.912593},
file = {:home/epaxon/Documents/Mendeley Desktop/Mou et al. - 2008 - A new criterion of delay-dependent asymptotic stability for Hopfield neural networks with time delay.pdf:pdf},
issn = {1045-9227},
journal = {IEEE Transaction of Neural Networks},
keywords = {Algorithms,Computer-Assisted,Humans,Neural Networks (Computer),Signal Processing,Time Factors},
number = {3},
pages = {532--5},
pmid = {18334372},
title = {{A new criterion of delay-dependent asymptotic stability for Hopfield neural networks with time delay}},
volume = {19},
year = {2008}
}
@article{Muezzinoglu2003,
abstract = {A method to store each element of an integral memory set M subset of {\{}1,2,...,K{\}}(n) as a fixed point into a complex-valued multistate Hopfield network is introduced. The method employs a set of inequalities to render each memory pattern as a strict local minimum of a quadratic energy landscape. Based on the solution of this system,. it gives a recurrent network of n multistate neurons with complex and. symmetric synaptic weights, which operates on the finite state space {\{}1, 2,...,K{\}}(n) to minimize this quadratic functional. Maximum number of integral vectors that can be embedded into the energy landscape of the network. by this method is investigated by computer experiments. This paper also enlightens the performance of the proposed method in reconstructing noisy gray-scale images.},
author = {Muezzinoglu, Mehmet Kerem and Guzelis, Cuneyt and Zurada, Jacek M.},
doi = {10.1109/TNN.2003.813844},
file = {:home/epaxon/Documents/Mendeley Desktop/Muezzinoglu, Guzelis, Zurada - 2003 - A new design method for the complex-valued multistate hopfield associative memory.pdf:pdf},
isbn = {1045-9227},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Complex-valued Hopfield network,Gray-scale image retrieval,Linear inequalities,Multistate associative memory},
number = {4},
pages = {891--899},
pmid = {18238068},
title = {{A new design method for the complex-valued multistate hopfield associative memory}},
volume = {14},
year = {2003}
}
@article{Nadasdy2009,
abstract = {Fundamental questions in neural coding are how neurons encode, transfer, and reconstruct information from the pattern of action potentials (APs) exchanged between different brain structures. We propose a general model of neural coding where neurons encode information by the phase of their APs relative to their subthreshold membrane oscillations. We demonstrate by means of simulations that AP phase retains the spatial and temporal content of the input under the assumption that the membrane potential oscillations are coherent across neurons and between structures and have a constant spatial phase gradient. The model explains many unresolved physiological observations and makes a number of concrete, testable predictions about the relationship between APs, local field potentials, and subthreshold membrane oscillations, and provides an estimate of the spatio-temporal precision of neuronal information processing.},
author = {Nadasdy, Zoltan},
doi = {10.3389/neuro.06.006.2009},
file = {:home/epaxon/Documents/Mendeley Desktop/Nadasdy - 2009 - Information encoding and reconstruction from the phase of action potentials.pdf:pdf},
isbn = {1662-5137 (Electronic)$\backslash$r1662-5137 (Linking)},
issn = {16625137},
journal = {Frontiers in Systems Neuroscience},
keywords = {action potential,local field potential,neural coding,subthreshold membrane potential oscillations},
number = {July},
pages = {1--20},
pmid = {19668700},
title = {{Information encoding and reconstruction from the phase of action potentials}},
url = {http://journal.frontiersin.org/article/10.3389/neuro.06.006.2009/abstract},
volume = {3},
year = {2009}
}
@article{Nara1993,
abstract = {Complex dynamics is expected to enable us to avoid combinatorial explosion and diverging program complexity that has been one of the essential difficulties in solving complex problems (for instance, an ill-posed problem) by serial processing algorithms. In this paper, it is shown by numerical investigations that a complicated memory search task is executable using complex dynamics in a recurrent neural network model with asymmetric synaptic connection. In spite of the simplicity of the proposed search algorithm, the complexity of chaotic wandering orbit in the state space offiring pattern leads us to a certain number of successful cases of search tasks with better efficiencies than random search. It is shown that the dynamical structure of chaotic orbit has a strong influence on the efficiency of search performance. A learning rule is proposed in order to realize “constrained chaos” that has, in the state space, a wandering structure suited to a given search task.},
author = {Nara, Shigetoshi and Davis, Peter and Totsuji, Hiroo},
doi = {10.1016/S0893-6080(09)80006-3},
file = {:home/epaxon/Documents/Mendeley Desktop/Nara, Davis, Totsuji - 1993 - Memory search using complex dynamics in a recurrent neural network model.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Adaptive bifurcation,Chaos,Constrained chaos,Learning,Memory search,Neural networks},
number = {7},
pages = {963--973},
publisher = {Pergamon Press Ltd.},
title = {{Memory search using complex dynamics in a recurrent neural network model}},
url = {http://www.sciencedirect.com/science/article/pii/S0893608009800063{\%}5Cnfiles/1196/Nara et al. - 1993 - Memory search using complex dynamics in a recurren.pdf{\%}5Cnfiles/1197/S0893608009800063.html{\%}5Cnhttp://linkinghub.elsevier.com/retrieve/pii/S0893608009800},
volume = {6},
year = {1993}
}
@article{Narendra1990,
abstract = {It is demonstrated that neural networks can be used effectively for the identification and control of nonlinear dynamical systems. The emphasis is on models for both identification and control. Static and dynamic backpropagation methods for the adjustment of parameters are discussed. In the models that are introduced, multilayer and recurrent networks are interconnected in novel configurations, and hence there is a real need to study them in a unified fashion. Simulation results reveal that the identification and adaptive control schemes suggested are practically feasible. Basic concepts and definitions are introduced throughout, and theoretical questions that have to be addressed are also described.},
author = {Narendra, K S and Parthasarathy, K},
doi = {10.1109/72.80202},
file = {:home/epaxon/Documents/Mendeley Desktop/Narendra, Parthasarathy - 1990 - Identification and control of dynamical systems using neural networks.pdf:pdf},
isbn = {1045-9227},
issn = {1045-9227},
journal = {IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council},
number = {1},
pages = {4--27},
pmid = {18282820},
title = {{Identification and control of dynamical systems using neural networks.}},
volume = {1},
year = {1990}
}
@article{Nasrabadi1992,
abstract = {An optimization approach is used to solve the correspondence problem for a set of features extracted from a pair of stereo images. A cost function is defined to represent the constraints on the solution, which is then mapped onto a two-dimensional Hopfield neural network for minimization. Each neuron in the network represents a possible match between a feature in the left image and one in the right image. Correspondence is achieved by initializing (exciting) each neuron that represents a possible match and then allowing the network to settle down into a stable state. The network uses the initial inputs and the compatibility measures between the matched points to find a stable state.},
author = {Nasrabadi, Nasser M. and Choo, Chang Y.},
doi = {10.1109/72.105413},
file = {:home/epaxon/Documents/Mendeley Desktop/Nasrabadi, Choo - 1992 - Hopfield Network for Stereo Vision Correspondence.pdf:pdf},
isbn = {1045-9227},
issn = {19410093},
journal = {IEEE Transactions on Neural Networks},
number = {1},
pages = {5--13},
pmid = {18276401},
title = {{Hopfield Network for Stereo Vision Correspondence}},
volume = {3},
year = {1992}
}
@article{Natarajan2008,
abstract = {Naturally occurring sensory stimuli are dynamic. In this letter, we consider how spiking neural populations might transmit information about continuous dynamic stimulus variables. The combination of simple encoders and temporal stimulus correlations leads to a code in which information is not readily available to downstream neurons. Here, we explore a complex encoder that is paired with a simple decoder that allows representation and manipulation of the dynamic information in neural systems. The encoder we present takes the form of a biologically plausible recurrent spiking neural network where the output population recodes its inputs to produce spikes that are independently decodeable. We show that this network can be learned in a supervised manner by a simple local learning rule.},
author = {Natarajan, Rama and Huys, Quentin J M and Dayan, Peter and Zemel, Richard S},
doi = {10.1162/neco.2008.01-07-436},
file = {:home/epaxon/Documents/Mendeley Desktop/Natarajan et al. - 2008 - Encoding and Decoding Spikes for Dynamic Stimuli.pdf:pdf},
isbn = {0899-7667 (Print)},
issn = {0899-7667},
journal = {Neural Comput},
number = {9},
pages = {2325--2360},
pmid = {18386986},
title = {{Encoding and Decoding Spikes for Dynamic Stimuli}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/neco.2008.01-07-436},
volume = {20},
year = {2008}
}
@article{OKeefe2005,
abstract = {We review the ideas and data behind the hypothesis that hippocampal pyramidal cells encode information by their phase of firing relative to the theta rhythm of the EEG. Particular focus is given to the further hypothesis that variations in firing rate can encode information independently from that encoded by firing phase. We discuss possible explanation of the phase-precession effect in terms of interference between two independent oscillatory influences on the pyramidal cell membrane potential, and the extent to which firing phase reflects internal dynamics or external (environmental) variables. Finally, we propose a model of the firing of the recently discovered "grid cells" in entorhinal cortex as part of a path-integration system, in combination with place cells and head-direction cells.},
author = {O'Keefe, John and Burgess, Neil},
doi = {10.1002/hipo.20115},
file = {:home/epaxon/Documents/Mendeley Desktop/O'Keefe, Burgess - 2005 - Dual phase and rate coding in hippocampal place cells Theoretical significance and relationship to entorhinal.pdf:pdf},
isbn = {1050-9631 (Print)},
issn = {10509631},
journal = {Hippocampus},
keywords = {Cognitive map,Computational model,Hippocampus,Oscillation,Theta},
number = {7},
pages = {853--866},
pmid = {16145693},
title = {{Dual phase and rate coding in hippocampal place cells: Theoretical significance and relationship to entorhinal grid cells}},
volume = {15},
year = {2005}
}
@article{OKeefe1993,
abstract = {Many complex spike cells in the hippocampus of the freely moving rat have as their primary correlate the animal's location in an environment (place cells). In contrast, the hippocampal electroencephalograph theta pattern of rhythmical waves (7-12 Hz) is better correlated with a class of movements that change the rat's location in an environment. During movement through the place field, the complex spike cells often fire in a bursting pattern with an interburst frequency in the same range as the concurrent electroencephalograph theta. The present study examined the phase of the theta wave at which the place cells fired. It was found that firing consistently began at a particular phase as the rat entered the field but then shifted in a systematic way during traversal of the field, moving progressively forward on each theta cycle. This precession of the phase ranged from 100 degrees to 355 degrees in different cells. The effect appeared to be due to the fact that individual cells had a higher interburst rate than the theta frequency. The phase was highly correlated with spatial location and less well correlated with temporal aspects of behavior, such as the time after place field entry. These results have implications for several aspects of hippocampal function. First, by using the phase relationship as well as the firing rate, place cells can improve the accuracy of place coding. Second, the characteristics of the phase shift constrain the models that define the construction of place fields. Third, the results restrict the temporal and spatial circumstances under which synapses in the hippocampus could be modified.},
author = {O'Keefe, John and Recce, Michael L.},
doi = {10.1002/hipo.450030307},
file = {:home/epaxon/Documents/Mendeley Desktop/O'Keefe, Recce - 1993 - Phase relationship between hippocampal place units and the hippocampal theta rhythm.pdf:pdf},
isbn = {1050-9631 (Print) 1050-9631 (Linking)},
issn = {1050-9631},
journal = {Hippocampus},
keywords = {1973,1975,Action Potentials,Animals,Electroencephalography,Hippocampus,Hippocampus: cytology,Hippocampus: physiology,Inbred Strains,Motor Activity,Motor Activity: physiology,Neurons,Neurons: physiology,Rats,Theta Rhythm,and theta cells,complex spike cells,fox and ranck,from the pyra-,midal cell layers of,phase correlates,place units,ranck,the pri-,the rat hippocampus,theta,two principal types of,unit can be recorded},
number = {3},
pages = {317--330},
pmid = {8353611},
title = {{Phase relationship between hippocampal place units and the hippocampal theta rhythm}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/8353611},
volume = {3},
year = {1993}
}
@article{Pajares2006,
abstract = {This paper outlines an optimization relaxation approach based on the analog Hopfield neural network (HNN) for solving the image change detection problem between two images. A difference image is obtained by subtracting pixel by pixel both images. The network topology is built so that each pixel in the difference image is a node in the network. Each node is characterized by its state, which determines if a pixel has changed. An energy function is derived, so that the network converges to stable states. The analog Hopfield's model allows each node to take on analog state values. Unlike most widely used approaches, where binary labels (changed/unchanged) are assigned to each pixel, the analog property provides the strength of the change. The main contribution of this paper is reflected in the customization of the analog Hopfield neural network to derive an automatic image change detection approach. When a pixel is being processed, some existing image change detection procedures consider only interpixel relations on its neighborhood. The main drawback of such approaches is the labeling of this pixel as changed or unchanged according to the information supplied by its neighbors, where its own information is ignored. The Hopfield model overcomes this drawback and for each pixel allows a tradeoff between the influence of its neighborhood and its own criterion. This is mapped under the energy function to be minimized. The performance of the proposed method is illustrated by comparative analysis against some existing image change detection methods.},
author = {Pajares, Gonzalo},
doi = {10.1109/TNN.2006.875978},
file = {:home/epaxon/Documents/Mendeley Desktop/Pajares - 2006 - A Hopfield neural network for image change detection.pdf:pdf},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Cluster Analysis,Computer-Assisted,Computer-Assisted: methods,Computing Methodologies,Image Enhancement,Image Enhancement: methods,Image Interpretation,Neural Networks (Computer),Pattern Recognition,Signal Processing,Subtraction Technique},
number = {5},
pages = {1250--64},
pmid = {17001985},
title = {{A Hopfield neural network for image change detection.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17001985},
volume = {17},
year = {2006}
}
@article{Palm2014,
abstract = {Donald Hebb's concept of cell assemblies is a physiology-based idea for a distributed neural representation of behaviorally relevant objects, concepts, or constellations. In the late 70s Valentino Braitenberg started the endeavor to spell out the hypothesis that the cerebral cortex is the structure where cell assemblies are formed, maintained and used, in terms of neuroanatomy (which was his main concern) and also neurophysiology. This endeavor has been carried on over the last 30 years corroborating most of his findings and interpretations. This paper summarizes the present state of cell assembly theory, realized in a network of associative memories, and of the anatomical evidence for its location in the cerebral cortex.},
author = {Palm, G??nther and Knoblauch, Andreas and Hauser, Florian and Sch??z, Almut},
doi = {10.1007/s00422-014-0596-4},
file = {:home/epaxon/Documents/Mendeley Desktop/Palm et al. - 2014 - Cell assemblies in the cerebral cortex.pdf:pdf},
issn = {14320770},
journal = {Biological Cybernetics},
keywords = {Associative memory,Brain theory,Cell assemblies,Cerebral cortex,Synaptic plasticity},
number = {5},
pages = {559--572},
pmid = {24692024},
title = {{Cell assemblies in the cerebral cortex}},
volume = {108},
year = {2014}
}
@article{Paninski2004a,
abstract = {Recent work has examined the estimation of models of stimulus-driven neural activity in which some linear filtering process is followed by a nonlinear, probabilistic spiking stage. We analyze the estimation of one such model for which this nonlinear step is implemented by a known parametric function; the assumption that this function is known speeds the estimation process considerably. We investigate the shape of the likelihood function for this type of model, give a simple condition on the nonlinearity ensuring that no non-global local maxima exist in the likelihood-leading, in turn, to efficient algorithms for the computation of the maximum likelihood estimator-and discuss the implications for the form of the allowed nonlinearities. Finally, we note some interesting connections between the likelihood-based estimators and the classical spike-triggered average estimator, discuss some useful extensions of the basic model structure, and provide two novel applications to physiological data.},
author = {Paninski, L},
doi = {10.1088/0954-898X/15/4/002},
file = {:home/epaxon/Documents/Mendeley Desktop/Paninski - 2004 - Maximum likelihood estimation of cascade point-process neural encoding models.pdf:pdf},
isbn = {0954-898X (Print)},
issn = {0954-898X},
journal = {Network: Computation in Neural Systems},
number = {4},
pages = {243--262},
pmid = {15600233},
title = {{Maximum likelihood estimation of cascade point-process neural encoding models}},
url = {http://www.informaworld.com/openurl?genre=article{\&}doi=10.1088/0954-898X/15/4/002{\&}magic=crossref{\%}7C{\%}7CD404A21C5BB053405B1A640AFFD44AE3},
volume = {15},
year = {2004}
}
@article{Paninski2004,
abstract = {We examine a cascade encoding model for neural response in which a linear filtering stage is followed by a noisy, leaky, integrate-and-fire spike generation mechanism. This model provides a biophysically more realistic alternative to models based on Poisson (memoryless) spike generation, and can effectively reproduce a variety of spiking behaviors seen in vivo. We describe the maximum likelihood estimator for the model parameters, given only extracellular spike train responses (not intracellular voltage data). Specifically, we prove that the log-likelihood function is concave and thus has an essentially unique global maximum that can be found using gradient ascent techniques. We develop an efficient algorithm for computing the maximum likelihood solution, demonstrate the effectiveness of the resulting estimator with numerical simulations, and discuss a method of testing the model's validity using time-rescaling and density evolution techniques.},
author = {Paninski, L and Pillow, J W and Simoncelli, E P},
doi = {10.1162/0899766042321797},
file = {:home/epaxon/Documents/Mendeley Desktop/Paninski, Pillow, Simoncelli - 2004 - Maximum likelihood estimation of a stochastic integrate-and-fire neural encoding model.pdf:pdf},
isbn = {0899-7667 (Print)},
issn = {0899-7667},
journal = {Neural Comput},
keywords = {Algorithms,Biophysics,Electrophysiology,Likelihood Functions,Membrane Potentials,Models, Neurological,Models, Statistical,Neurons/ physiology,Nonlinear Dynamics,Poisson Distribution},
number = {12},
pages = {2533--2561},
pmid = {15516273},
title = {{Maximum likelihood estimation of a stochastic integrate-and-fire neural encoding model}},
volume = {16},
year = {2004}
}
@article{Paninski2007,
abstract = {There are two basic problems in the statistical analysis of neural data. The "encoding" problem concerns how information is encoded in neural spike trains: can we predict the spike trains of a neuron (or population of neurons), given an arbitrary stimulus or observed motor response? Conversely, the "decoding" problem concerns how much information is in a spike train, in particular, how well can we estimate the stimulus that gave rise to the spike train? This chapter describes statistical model-based techniques that in some cases provide a unified solution to these two coding problems. These models can capture stimulus dependencies as well as spike history and interneuronal interaction effects in population spike trains, and are intimately related to biophysically based models of integrate-and-fire type. We describe flexible, powerful likelihood-based methods for fitting these encoding models and then for using the models to perform optimal decoding. Each of these (apparently quite difficult) tasks turn out to be highly computationally tractable, due to a key concavity property of the model likelihood. Finally, we return to the encoding problem to describe how to use these models to adaptively optimize the stimuli presented to the cell on a trial-by-trial basis, in order that we may infer the optimal model parameters as efficiently as possible. ?? 2007 Elsevier B.V. All rights reserved.},
author = {Paninski, Liam and Pillow, Jonathan and Lewi, Jeremy},
doi = {10.1016/S0079-6123(06)65031-0},
file = {:home/epaxon/Documents/Mendeley Desktop/Paninski, Pillow, Lewi - 2007 - Statistical models for neural encoding, decoding, and optimal stimulus design.pdf:pdf},
isbn = {0444528237},
issn = {00796123},
journal = {Progress in Brain Research},
keywords = {decoding,neural coding,optimal experimental design},
pages = {493--507},
pmid = {17925266},
title = {{Statistical models for neural encoding, decoding, and optimal stimulus design}},
volume = {165},
year = {2007}
}
@article{Panzeri2010,
abstract = {Computational analyses have revealed that precisely timed spikes emitted by somatosensory cortical neuronal populations encode basic stimulus features in the rat's whisker sensory system. Efficient spike time based decoding schemes both for the spatial location of a stimulus and for the kinetic features of complex whisker movements have been defined. To date, these decoding schemes have been based upon spike times referenced to an external temporal frame - the time of the stimulus itself. Such schemes are limited by the requirement of precise knowledge of the stimulus time signal, and it is not clear whether stimulus times are known to rats making sensory judgments. Here, we first review studies of the information obtained from spike timing referenced to the stimulus time. Then we explore new methods for extracting spike train information independently of any external temporal reference frame. These proposed methods are based on the detection of stimulus-dependent differences in the firing time within a neuronal population. We apply them to a data set using single-whisker stimulation in anesthetized rats and find that stimulus site can be decoded based on the millisecond-range relative differences in spike times even without knowledge of stimulus time. If spike counts alone are measured over tens or hundreds of milliseconds rather than milliseconds, such decoders are much less effective. These results suggest that decoding schemes based on millisecond-precise spike times are likely to subserve robust and information-rich transmission of information in the somatosensory system.},
author = {Panzeri, Stefano and Diamond, Mathew E.},
doi = {10.3389/fnsyn.2010.00017},
file = {:home/epaxon/Documents/Mendeley Desktop/Panzeri, Diamond - 2010 - Information carried by population spike times in the whisker sensory cortex can be decoded without knowledge o.pdf:pdf},
isbn = {1663-3563 (Electronic) 1663-3563 (Linking)},
issn = {16633563},
journal = {Frontiers in Synaptic Neuroscience},
keywords = {Decoding,Information theory,Neural coding,Population coding,Somatosensation,Spike patterns},
number = {JUN},
pages = {1--14},
pmid = {21423503},
title = {{Information carried by population spike times in the whisker sensory cortex can be decoded without knowledge of stimulus time}},
volume = {2},
year = {2010}
}
@article{Paulin2004,
author = {Paulin, Mike},
file = {:home/epaxon/Documents/Mendeley Desktop/Paulin - 2004 - Spikes are the Operands of Neural Computation.pdf:pdf},
journal = {ENNS-INNS-JNNS Neural Networks Society Newsletters},
pages = {1--4},
title = {{Spikes are the Operands of Neural Computation}},
volume = {2},
year = {2004}
}
@article{Pillow2005,
author = {Pillow, J. W. and Paninski, L. and Uzzell, V. J. and Simoncelli, Eero P and Chichilnisky, E J},
doi = {10.1523/JNEUROSCI.3305-05.2005},
file = {:home/epaxon/Documents/Mendeley Desktop/Pillow et al. - 2005 - Prediction and Decoding of Retinal Ganglion Cell Responses with a Probabilistic Spiking Model.pdf:pdf},
issn = {0270-6474},
journal = {Journal of Neuroscience},
keywords = {computational model,decoding,integrate,neural coding,precision,retinal ganglion cell,spike timing,spike trains,variability},
number = {47},
pages = {11003--11013},
title = {{Prediction and Decoding of Retinal Ganglion Cell Responses with a Probabilistic Spiking Model}},
url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.3305-05.2005},
volume = {25},
year = {2005}
}
@article{Pillow2007,
abstract = {This chapter discusses likelihood-based approaches to building mathematical models of the neural code. It introduces probabilistic neural models such as the linear-non-linear-Poisson (LNP) model (models of neural response), the generalized linear model (GLM), and the generalized integrate-and-fire (GIF) model. The chapter also examines the methods of evaluating the validity of probabilistic models, which includes cross-validation, time rescaling, and model-based decoding.},
author = {Pillow, Jonathan W},
doi = {10.7551/mitpress/9780262042383.003.0003},
file = {:home/epaxon/Documents/Mendeley Desktop/Pillow - 2007 - Likelihood-Based Approaches to Modeling the Neural Code.pdf:pdf},
isbn = {9780262042383},
journal = {Bayesian Brain: Probabilistic Approaches to Neural Coding},
keywords = {GIF,GLM,LNP,Poisson model,cross-validation,generalized linear model,integrate and fire,likelihood-based approaches,neural code,time rescaling},
pages = {53--70},
title = {{Likelihood-Based Approaches to Modeling the Neural Code}},
volume = {70},
year = {2007}
}
@article{Pillow2008,
abstract = {Statistical dependencies in the responses of sensory neurons govern both the amount of stimulus information conveyed and the means by which downstream neurons can extract it. Although a variety of measurements indicate the existence of such dependencies, their origin and importance for neural coding are poorly understood. Here we analyse the functional significance of correlated firing in a complete population of macaque parasol retinal ganglion cells using a model of multi-neuron spike responses. The model, with parameters fit directly to physiological data, simultaneously captures both the stimulus dependence and detailed spatio-temporal correlations in population responses, and provides two insights into the structure of the neural code. First, neural encoding at the population level is less noisy than one would expect from the variability of individual neurons: spike times are more precise, and can be predicted more accurately when the spiking of neighbouring neurons is taken into account. Second, correlations provide additional sensory information: optimal, model-based decoding that exploits the response correlation structure extracts 20{\%} more information about the visual scene than decoding under the assumption of independence, and preserves 40{\%} more visual information than optimal linear decoding. This model-based approach reveals the role of correlated activity in the retinal coding of visual stimuli, and provides a general framework for understanding the importance of correlated activity in populations of neurons.},
author = {Pillow, Jonathan W and Shlens, Jonathon and Paninski, Liam and Sher, Alexander and Litke, Alan M and Chichilnisky, E J and Simoncelli, Eero P},
doi = {10.1038/nature07140},
file = {:home/epaxon/Documents/Mendeley Desktop/Pillow et al. - 2008 - Spatio-temporal correlations and visual signalling in a complete neuronal population.pdf:pdf},
issn = {1476-4687},
journal = {Nature},
keywords = {Action Potentials,Animals,Macaca mulatta,Macaca mulatta: physiology,Models,Neurological,Ocular,Ocular: physiology,Photic Stimulation,Retinal Ganglion Cells,Retinal Ganglion Cells: physiology,Time Factors,Vision},
month = {aug},
number = {7207},
pages = {995--9},
pmid = {18650810},
shorttitle = {Nature},
title = {{Spatio-temporal correlations and visual signalling in a complete neuronal population.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18650810},
volume = {454},
year = {2008}
}
@book{Plate2003,
author = {Plate, Tony A.},
isbn = {978-1575864303},
publisher = {Stanford: CSLI Publications},
title = {{Holographic Reduced Representation: Distributed Representation for Cognitive Structures}},
year = {2003}
}
@article{Prechtl2000,
abstract = {Visual stimuli induce oscillations in the membrane potential of neurons in cortices of several species. In turtle, these oscillations take the form of linear and circular traveling waves. Such waves may be a consequence of a pacemaker that emits periodic pulses of excitation that propagate across a network of excitable neuronal tissue or may result from continuous and possibly reconfigurable phase shifts along a network with multiple weakly coupled neuronal oscillators. As a means to resolve the origin of wave propagation in turtle visual cortex, we performed simultaneous measurements of the local field potential at a series of depths throughout this cortex. Measurements along a single radial penetration revealed the presence of broadband current sources, with a center frequency near 20 Hz (gamma band), that were activated by visual stimulation. The spectral coherence between sources at two well-separated loci along a rostral-caudal axis revealed the presence of systematic timing differences between localized cortical oscillators. These multiple oscillating current sources and their timing differences in a tangential plane are interpreted as the neuronal activity that underlies the wave motion revealed in previous imaging studies. The present data provide direct evidence for the inference from imaging of bidirectional wave motion that the stimulus-induced electrical waves in turtle visual cortex correspond to phase shifts in a network of coupled neuronal oscillators.},
author = {Prechtl, J C and Bullock, T H and Kleinfeld, D},
doi = {10.1073/pnas.97.2.877},
file = {:home/epaxon/Documents/Mendeley Desktop/Prechtl, Bullock, Kleinfeld - 2000 - Direct evidence for local oscillatory current sources and intracortical phase gradients in turtle v.pdf:pdf},
isbn = {0027-8424 (Print)$\backslash$n0027-8424 (Linking)},
issn = {00278424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {2},
pages = {877--882},
pmid = {10639173},
title = {{Direct evidence for local oscillatory current sources and intracortical phase gradients in turtle visual cortex.}},
volume = {97},
year = {2000}
}
@article{Prechtl1997,
author = {Prechtl, JC and Cohen, LB and Pesaran, B. and Mitra, PP and Kleinfeld, D.},
file = {:home/epaxon/Documents/Mendeley Desktop/Prechtl et al. - 1997 - Visual stimuli induce waves of electrical activity in turtle cortex.pdf:pdf},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {14},
pages = {7621},
publisher = {National Acad Sciences},
title = {{Visual stimuli induce waves of electrical activity in turtle cortex}},
url = {http://www.pnas.org/content/94/14/7621.full},
volume = {94},
year = {1997}
}
@article{Rabinovich2014a,
abstract = {Psychiatric disorders are often caused by partial heterogeneous disinhibition in cognitive networks, controlling sequential and spatial working memory (SWM). Such dynamic connectivity changes suggest that the normal relationship between the neuronal components within the network deteriorates. As a result, competitive network dynamics is qualitatively altered. This dynamics defines the robust recall of the sequential information from memory and, thus, the SWM capacity. To understand pathological and non-pathological bifurcations of the sequential memory dynamics, here we investigate the model of recurrent inhibitory-excitatory networks with heterogeneous inhibition. We consider the ensemble of units with all-to-all inhibitory connections, in which the connection strengths are monotonically distributed at some interval. Based on computer experiments and studying the Lyapunov exponents, we observed and analyzed the new phenomenon-clustered sequential dynamics. The results are interpreted in the context of the winnerless competition principle. Accordingly, clustered sequential dynamics is represented in the phase space of the model by two weakly interacting quasi-attractors. One of them is similar to the sequential heteroclinic chain-the regular image of SWM, while the other is a quasi-chaotic attractor. Coexistence of these quasi-attractors means that the recall of the normal information sequence is intermittently interrupted by episodes with chaotic dynamics. We indicate potential dynamic ways for augmenting damaged working memory and other cognitive functions.},
author = {Rabinovich, Mikhail I. and Sokolov, Yury and Kozma, Robert},
doi = {10.3389/fnsys.2014.00220},
file = {:home/epaxon/Documents/Mendeley Desktop/Rabinovich, Sokolov, Kozma - 2014 - Robust sequential working memory recall in heterogeneous cognitive networks.pdf:pdf},
isbn = {1662-5137 (Electronic)$\backslash$r1662-5137 (Linking)},
issn = {1662-5137},
journal = {Frontiers in Systems Neuroscience},
keywords = {cognitive dynamics,cognitive dynamics, memory disorders, inhibition,,complex networks,heteroclinic chimeras,inhibition,memory disorders,sequential intermittency},
number = {November},
pages = {1--11},
pmid = {25452717},
title = {{Robust sequential working memory recall in heterogeneous cognitive networks}},
url = {http://journal.frontiersin.org/article/10.3389/fnsys.2014.00220/abstract},
volume = {8},
year = {2014}
}
@article{Rabinovich2014,
abstract = {Recent results of imaging technologies and nonlinear dynamics make possible to relate the structure and dynamics of functional brain networks to different mental tasks and to build theoretical models for the description and prediction of cognitive activity. Such models are nonlinear dynamical descriptions of the interaction of the core components –brain modes– participating in a specific mental function. The dynamical images of different mental processes depend on their temporal features. The dynamics of many cognitive functions are transient. They are often observed as a chain of sequentially changing metastable states. A stable heteroclinic channel consisting of chain of saddles -metastable states- connected by unstable separatrices is a mathematical image for robust transients. In this paper we focus on hierarchical chunking dynamics that can represent several forms of transient cognitive activity. Chunking is a dynamical phenomenon that nature uses to perform information processing of long sequences by dividing them in shorter information items. Chunking, for example, makes more efficient the use of short-term memory by breaking up long strings of information (like in language where one can see the separation of a novel on chapters, paragraphs, sentences and finally words). Chunking is important in many processes of perception, learning and cognition in humans and animals. Based on anatomical information about the hierarchical organization of functional brain networks, we proposed here a cognitive network architecture that hierarchically chunks and super-chunks switching sequences of metastable states produced by winnerless competitive heteroclinic dynamics.},
author = {Rabinovich, Mikhail I. and Varona, Pablo and Tristan, Irma and Afraimovich, Valentin S.},
doi = {10.3389/fncom.2014.00022},
file = {:home/epaxon/Documents/Mendeley Desktop/Rabinovich et al. - 2014 - Chunking dynamics heteroclinics in mind.pdf:pdf},
isbn = {1662-5188},
issn = {1662-5188},
journal = {Frontiers in Computational Neuroscience},
keywords = {activity,chunking and superchunking,cognition modeling principles,cognitive dynamics,cognitive dynamics, stable heteroclinic channel, t,hierarchical sequences,low dimensionality of brain,stable heteroclinic channel,transient dynamics},
number = {March},
pages = {1--10},
pmid = {24672469},
title = {{Chunking dynamics: heteroclinics in mind}},
url = {http://journal.frontiersin.org/article/10.3389/fncom.2014.00022/abstract},
volume = {8},
year = {2014}
}
@article{Ranhel2011,
abstract = {Information can be encoded in spiking neural network (SNN) by precise spike-time relations. This hypothesis can explain cell assembly formation, such as polychronous group (PNG), a notion created to explain how groups of neurons fire time-locked to each other, not necessarily synchronously. In this paper we present a set of PNGs capable of retaining triggering events in bistable states. Triggering events may be data or computational controls. Both, data and control signals are memorized as a result of intrinsic operational PNG attributes, and no neural plasticity mechanisms are involved. This behavior can be fundamental for several computational operations in SNNs. It is shown how bistable neural pools can perform tasks such as binary and stack-like counting, and how they can realize hierarchical organization in parallel computing.},
author = {Ranhel, Jo{\~{a}}o and Lima, Cacilda V. and Monteiro, J{\'{u}}lio L R and Kogler, Jo{\~{a}}o E. and Netto, Marcio L.},
doi = {10.1109/FOCI.2011.5949465},
file = {:home/epaxon/Documents/Mendeley Desktop/Ranhel et al. - 2011 - Bistable memory and binary counters in spiking neural network.pdf:pdf},
isbn = {9781424499823},
journal = {IEEE SSCI 2011 - Symposium Series on Computational Intelligence - FOCI 2011: 2011 IEEE Symposium on Foundations of Computational Intelligence},
keywords = {bistable neural memory,neural counters,neural hierarchical organization,neural stack counter,polychronization,spiking neural networks},
pages = {66--73},
title = {{Bistable memory and binary counters in spiking neural network}},
year = {2011}
}
@article{Reinagel2000,
abstract = {The amount of information a sensory neuron carries about a stimulus is directly related to response reliability. We recorded from individual neurons in the cat lateral geniculate nucleus (LGN) while presenting randomly modulated visual stimuli. The responses to repeated stimuli were reproducible, whereas the responses evoked by nonrepeated stimuli drawn from the same ensemble were variable. Stimulus-dependent information was quantified directly from the difference in entropy of these neural responses. We show that a single LGN cell can encode much more visual information than had been demonstrated previously, ranging from 15 to 102 bits/sec across our sample of cells. Information rate was correlated with the firing rate of the cell, for a consistent rate of 3.6 +/- 0.6 bits/spike (mean +/- SD). This information can primarily be attributed to the high temporal precision with which firing probability is modulated; many individual spikes were timed with better than 1 msec precision. We introduce a way to estimate the amount of information encoded in temporal patterns of firing, as distinct from the information in the time varying firing rate at any temporal resolution. Using this method, we find that temporal patterns sometimes introduce redundancy but often encode visual information. The contribution of temporal patterns ranged from -3.4 to +25.5 bits/sec or from -9.4 to +24.9{\%} of the total information content of the responses.},
author = {Reinagel, P and Reid, R C},
doi = {20/14/5392 [pii]},
file = {:home/epaxon/Documents/Mendeley Desktop/Reinagel, Reid - 2000 - Temporal coding of visual information in the thalamus.pdf:pdf},
isbn = {0270-6474 (Print)},
issn = {0270-6474},
journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
keywords = {a neural response,any assumptions about what,entropy,how that,in the sense that,information theory,lgn,neural coding,of,reliability,the method is independent,the neuron represents or,variability,white noise},
number = {14},
pages = {5392--400},
pmid = {10884324},
title = {{Temporal coding of visual information in the thalamus.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10884324},
volume = {20},
year = {2000}
}
@article{Reynolds1999,
abstract = {For purposes of this review, we will define the binding problem as the problem of how the visual system cor- rectly links up all the different features of complex objects. For example, when viewing a person seated in a blue car, one effortlessly sees that the person{\"{i}}s nose belongs to his face and not to the car, and that the car, but not the nose, is blue. To fully understand the solution to this problem requires a good neurobiological theory of object recognition, which does not exist. We will therefore follow the lead of the computer engineer, who, when asked to describe how he would write a computer program to recognize a chicken, replied, {\"{i}}first, assume spherical chicken.{\"{i}} Thus, in this review we will make some assumptions that simplify the binding problem in order to appreciate how neural mechanisms of attention provide a partial solution.},
author = {Reynolds, J. H. and Desimone, R.},
doi = {10.1016/S0896-6273(00)80819-3},
file = {:home/epaxon/Documents/Mendeley Desktop/Reynolds, Desimone - 1999 - The role of neural mechanisms of attention in solving the binding problem.pdf:pdf},
isbn = {0896-6273 (Print)$\backslash$r0896-6273 (Linking)},
issn = {08966273},
journal = {Neuron},
number = {1},
pages = {19--29},
pmid = {10677024},
title = {{The role of neural mechanisms of attention in solving the binding problem}},
volume = {24},
year = {1999}
}
@article{Riehle1997,
abstract = {It is now commonly accepted that planning and execution of movements are based on distributed processing by neuronal populations in motor cortical areas. It is less clear, though, how these populations organize dynamically to cope with the momentary com- putational demands. Simultaneously recorded activities of neurons in the primary motor cortex of monkeys during performance of a delayed-pointing task exhibited context- dependent, rapid changes in the patterns of coincident action potentials. Accurate spike synchronization occurred in relation to external events (stimuli, movements) and was commonly accompanied by discharge rate modulations but without precise time locking of the spikes to these external events. Spike synchronization also occurred in relation to purely internal events (stimulus expectancy), where firing rate modulations were distinctly absent. These findings indicate that internally generated synchronization of individual spike discharges may subserve the cortical organization of cognitive motor processes.},
author = {Riehle, Alexa and Grun, Sonja and Diesmann, Markus and Aertsen, Ad and Gr{\"{u}}n, S and Diesmann, Markus and Aertsen, Ad and Grun, Sonja and Diesmann, Markus and Aertsen, Ad},
doi = {10.1126/science.278.5345.1950},
file = {:home/epaxon/Documents/Mendeley Desktop/Riehle et al. - 1997 - Spike Synchronization and Rate Modulation Differentially Involved in Motor Cortical Function.pdf:pdf},
isbn = {0036-8075 (Print)$\backslash$r0036-8075 (Linking)},
issn = {00368075},
journal = {Science},
number = {5345},
pages = {1950--1953},
pmid = {9395398},
title = {{Spike Synchronization and Rate Modulation Differentially Involved in Motor Cortical Function}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.278.5345.1950},
volume = {278},
year = {1997}
}
@article{Ruf1998,
abstract = {We propose a mechanism for unsupervised learning in networks of spiking neurons which is based on the timing of single firing events. Our results show that a topology preserving behavior quite similar to that of Kohonen's self-organizing map can be achieved using temporal coding. In contrast to previous approaches, which use rate coding, the winner among competing neurons can be determined fast and locally. Our model is a further step toward a more realistic description of unsupervised learning in biological neural systems. Furthermore, it may provide a basis for fast implementations in pulsed VLSI (very large scale integration).},
author = {Ruf, B and Schmitt, M},
doi = {10.1109/72.668899},
file = {:home/epaxon/Documents/Mendeley Desktop/Ruf, Schmitt - 1998 - Self-organization of spiking neurons using action potential timing.pdf:pdf},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
number = {3},
pages = {575--578},
pmid = {18252481},
title = {{Self-organization of spiking neurons using action potential timing.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18252481},
volume = {9},
year = {1998}
}
@article{Scaglione2014,
abstract = {Objective. Sensory processing of peripheral information is not stationary but is, in general, a dynamic process related to the behavioral state of the animal. Yet the link between the state of the behavior and the encoding properties of neurons is unclear. This report investigates the impact of the behavioral state on the encoding mechanisms used by cortical neurons for both detection and discrimination of somatosensory stimuli in awake, freely moving, rats. Approach. Neuronal activity was recorded from the primary somatosensory cortex of five rats under two different behavioral states (quiet versus whisking) while electrical stimulation of increasing stimulus strength was delivered to the mystacial pad. Information theoretical measures were then used to measure the contribution of different encoding mechanisms to the information carried by neurons in response to the whisker stimulation. Main results. We found that the behavioral state of the animal modulated the total amount of information conveyed by neurons and that the timing of individual spikes increased the information compared to the total count of spikes alone. However, the temporal information, i.e. information exclusively related to when the spikes occur, was not modulated by behavioral state. Significance. We conclude that information about somatosensory stimuli is modulated by the behavior of the animal and this modulation is mainly expressed in the spike count while the temporal information is more robust to changes in behavioral state.},
author = {Scaglione, Alessandro and Foffani, Guglielmo and Moxon, Karen A},
doi = {10.1088/1741-2560/11/4/046022},
file = {:home/epaxon/Documents/Mendeley Desktop/Scaglione, Foffani, Moxon - 2014 - Spike count, spike timing and temporal information in the cortex of awake, freely moving rats.pdf:pdf},
issn = {1741-2560},
journal = {Journal of Neural Engineering},
number = {4},
pages = {046022},
pmid = {25024291},
publisher = {IOP Publishing},
title = {{Spike count, spike timing and temporal information in the cortex of awake, freely moving rats}},
url = {http://stacks.iop.org/1741-2552/11/i=4/a=046022?key=crossref.9385e9860e93cd339b79dc73dcce8629},
volume = {11},
year = {2014}
}
@article{Scarpetta2013,
abstract = {We study the collective dynamics of a Leaky Integrate and Fire network in which precise relative phase relationship of spikes among neurons are stored, as attractors of the dynamics, and selectively replayed at different time scales. Using an STDP-based learning process, we store in the connectivity several phase-coded spike patterns, and we find that, depending on the excitability of the network, different working regimes are possible, with transient or persistent replay activity induced by a brief signal. We introduce an order parameter to evaluate the similarity between stored and recalled phase-coded pattern, and measure the storage capacity. Modulation of spiking thresholds during replay changes the frequency of the collective oscillation or the number of spikes per cycle, keeping preserved the phases relationship. This allows a coding scheme in which phase, rate and frequency are dissociable. Robustness with respect to noise and heterogeneity of neurons parameters is studied, showing that, since dynamics is a retrieval process, neurons preserve stable precise phase relationship among units, keeping a unique frequency of oscillation, even in noisy conditions and with heterogeneity of internal parameters of the units.},
archivePrefix = {arXiv},
arxivId = {http://arxiv.org/pdf/1210.6789v1.pdf},
author = {Scarpetta, Silvia and Giacco, Ferdinando},
doi = {10.1007/s10827-012-0423-7},
eprint = {/arxiv.org/pdf/1210.6789v1.pdf},
file = {:home/epaxon/Documents/Mendeley Desktop/Scarpetta, Giacco - 2013 - Associative memory of phase-coded spatiotemporal patterns in leaky Integrate and Fire networks.pdf:pdf},
issn = {09295313},
journal = {Journal of Computational Neuroscience},
keywords = {Associative memory,Learning and memory,Noise robustness,Phase-of-spikes coding,Replay,STDP,Storage capacity},
number = {2},
pages = {319--336},
pmid = {23053861},
primaryClass = {http:},
title = {{Associative memory of phase-coded spatiotemporal patterns in leaky Integrate and Fire networks}},
volume = {34},
year = {2013}
}
@article{Schaefer2006,
abstract = {Although oscillations in membrane potential are a prominent feature of sensory, motor, and cognitive function, their precise role in signal processing remains elusive. Here we show, using a combination of in vivo, in vitro, and theoretical approaches, that both synaptically and intrinsically generated membrane potential oscillations dramatically improve action potential (AP) precision by removing the membrane potential variance associated with jitter-accumulating trains of APs. This increased AP precision occurred irrespective of cell type and--at oscillation frequencies ranging from 3 to 65 Hz--permitted accurate discernment of up to 1,000 different stimuli. At low oscillation frequencies, stimulus discrimination showed a clear phase dependence whereby inputs arriving during the trough and the early rising phase of an oscillation cycle were most robustly discriminated. Thus, by ensuring AP precision, membrane potential oscillations dramatically enhance the discriminatory capabilities of individual neurons and networks of cells and provide one attractive explanation for their abundance in neurophysiological systems.},
author = {Schaefer, Andreas T. and Angelo, Kamilla and Spors, Hartwig and Margrie, Troy W.},
doi = {10.1371/journal.pbio.0040163},
file = {:home/epaxon/Documents/Mendeley Desktop/Schaefer et al. - 2006 - Neuronal oscillations enhance stimulus discrimination by ensuring action potential precision.pdf:pdf},
isbn = {1545-7885},
issn = {15457885},
journal = {PLoS Biology},
number = {6},
pages = {1010--1024},
pmid = {16689623},
title = {{Neuronal oscillations enhance stimulus discrimination by ensuring action potential precision}},
volume = {4},
year = {2006}
}
@article{Schuck2010,
author = {Schuck, Nicolas W and Burgess, Neil},
doi = {10.1186/1471-2202-11-S1-P173},
file = {:home/epaxon/Documents/Mendeley Desktop/Schuck, Burgess - 2010 - Oscillatory interference in parietal cortex a mechanism to represent order in working memory.pdf:pdf},
issn = {1471-2202},
journal = {BMC Neuroscience},
number = {Suppl 1},
pages = {P173},
title = {{Oscillatory interference in parietal cortex: a mechanism to represent order in working memory}},
url = {http://www.biomedcentral.com/1471-2202/11/S1/P173},
volume = {11},
year = {2010}
}
@article{Schuck2010a,
author = {Schuck, Nicolas W and Burgess, Neil},
doi = {10.1186/1471-2202-11-S1-P173},
file = {:home/epaxon/Documents/Mendeley Desktop/Schuck, Burgess - 2010 - Oscillatory interference in parietal cortex a mechanism to represent order in working memory.pdf:pdf},
issn = {1471-2202},
journal = {BMC Neuroscience},
number = {Suppl 1},
pages = {P173},
title = {{Oscillatory interference in parietal cortex: a mechanism to represent order in working memory}},
url = {http://www.biomedcentral.com/1471-2202/11/S1/P173},
volume = {11},
year = {2010}
}
@article{Sejnowski2006,
abstract = {Despite extensive work on the behavioral and physiological correlates of brain rhythms, it is still unresolved whether they have any important function in the mammalian cerebral cortex. In particular, there is no consensus on whether there are general computational roles for network oscillations. Three main possibilities will be discussed here. One possibility is that network oscillations contribute to representation of information. A second idea is that, rather than representing information as such, oscillations and synchrony regulate the flow of information in neural circuits. A third possibility is that oscillations assist in the storage and retrieval of information in neural circuits. These three possibilities are not mutually exclusive.},
author = {Sejnowski, T. J. and Paulsen, Ole},
doi = {10.1523/JNEUROSCI.3737-05d.2006},
file = {:home/epaxon/Documents/Mendeley Desktop/Sejnowski, Paulsen - 2006 - Network Oscillations Emerging Computational Principles.pdf:pdf},
isbn = {ISSN{\~{}}{\~{}}2661673p},
issn = {0270-6474},
journal = {Journal of Neuroscience},
number = {6},
pages = {1673--1676},
pmid = {16467514},
title = {{Network Oscillations: Emerging Computational Principles}},
url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.3737-05d.2006},
volume = {26},
year = {2006}
}
@article{Seung2003a,
abstract = {It is well-known that chemical synaptic transmission is an unreliable process, but the function of such unreliability remains unclear. Here I consider the hypothesis that the randomness of synaptic transmission is harnessed by the brain for learning, in analogy to the way that genetic mutation is utilized by Darwinian evolution. This is possible if synapses are "hedonistic," responding to a global reward signal by increasing their probabilities of vesicle release or failure, depending on which action immediately preceded reward. Hedonistic synapses learn by computing a stochastic approximation to the gradient of the average reward. They are compatible with synaptic dynamics such as short-term facilitation and depression and with the intricacies of dendritic integration and action potential generation. A network of hedonistic synapses can be trained to perform a desired computation by administering reward appropriately, as illustrated here through numerical simulations of integrate-and-fire model neurons.},
author = {Seung, H Sebastian},
file = {:home/epaxon/Documents/Mendeley Desktop/Seung - 2003 - Learning in spiking neural networks by reinforcement of stochastic synaptic transmission.pdf:pdf},
issn = {0896-6273},
journal = {Neuron},
keywords = {Action Potentials,Action Potentials: physiology,Learning,Learning: physiology,Neural Networks (Computer),Reinforcement (Psychology),Stochastic Processes,Synaptic Transmission,Synaptic Transmission: physiology},
month = {dec},
number = {6},
pages = {1063--73},
pmid = {14687542},
title = {{Learning in spiking neural networks by reinforcement of stochastic synaptic transmission.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/14687542},
volume = {40},
year = {2003}
}
@article{Seung1993a,
abstract = {In many neural systems, sensory information is distributed throughout a population of neurons. We study simple neural network models for extracting this information. The inputs to the networks are the stochastic responses of a population of sensory neurons tuned to directional stimuli. The performance of each network model in psychophysical tasks is compared with that of the optimal maximum likelihood procedure. As a model of direction estimation in two dimensions, we consider a linear network that computes a population vector. Its performance depends on the width of the population tuning curves and is maximal for width, which increases with the level of background activity. Although for narrowly tuned neurons the performance of the population vector is significantly inferior to that of maximum likelihood estimation, the difference between the two is small when the tuning is broad. For direction discrimination, we consider two models: a perceptron with fully adaptive weights and a network made by adding an adaptive second layer to the population vector network. We calculate the error rates of these networks after exhaustive training to a particular direction. By testing on the full range of possible directions, the extent of transfer of training to novel stimuli can be calculated. It is found that for threshold linear networks the transfer of perceptual learning is nonmonotonic. Although performance deteriorates away from the training stimulus, it peaks again at an intermediate angle. This nonmonotonicity provides an important psychophysical test of these models.},
author = {Seung, H. Sabastian and Sompolinsky, Haim},
isbn = {1074910753},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Afferent,Afferent: physiology,Animals,Humans,Likelihood Functions,Models,Nerve Net,Neurons,Orientation,Orientation: physiology,Perception,Perception: physiology,Stochastic Processes,Theoretical},
month = {nov},
number = {22},
pages = {10749--53},
pmid = {8248166},
title = {{Simple models for reading neuronal population codes.}},
volume = {90},
year = {1993}
}
@article{Shadlen1999,
abstract = {Singer, 1994). The basis for this argument is that it is necessary to " tag " each visual neuron to signify the object to which its activity relates. Each neuron therefore has to carry two distinct signals, one that indicates how effective a stimulus is falling on its receptive field, and Howard Hughes Medical Institute and a second that tags it as a member of a particular cell assembly. To make these signals distinct, von der Mals-Center for Neural Science New York University burg (1981) proposed that the " effectiveness " signal would be carried by a conventional rate code, while the New York, New York 10003 " tag " signal would be created by synchronizing the spike activity of the neuron with spikes from other neurons in Introduction the same assembly. This novel idea has led to a great deal of experimental work and to several further elabora-In the early stages of visual processing, objects and scenes are represented by neurons with small visual tions of the original theory. In this paper, we will articulate our doubts and con-receptive fields. Each neuron provides information about local features of a scene, but to describe a scene in cerns about this theory and its experimental support. We consider first whether the theory is an a priori reasonable terms of objects requires that these features be com-bined. Objects can cover wide areas of visual space and approach to solving the binding problem, and conclude that it is at best incomplete. We then ask whether spike be partially occluded by other objects, so the problem of binding the separate representations of parts into synchrony can plausibly be used as an informational code, and conclude that there are significant practical coherent wholes is not a simple one. This " binding problem " has received considerable attention. Gestalt and theoretical obstacles both to encoding and to de-coding information in this way. We then examine the psychologists articulated a number of principles for grouping and organizing scene elements (K{\"{o}} hler, 1930; experimental evidence usually adduced to support the synchrony hypothesis, and conclude that the evidence Koffka, 1935; Kanisza, 1979), and more recently the re-lated problem of image segmentation has received a is largely indirect and has no proven relevance to the issue of binding per se. We will finish by asking whether good deal of attention in computer vision (see, for exam-ple, Pal and Pal, 1993, for a review). The binding problem the binding problem is truly of unique difficulty and re-quires a unique solution, and by considering some strat-is really best considered as a series of related problems, all of which require the combination of information from egies for solving the binding problem that do not require the creation of a special neural code. multiple sources. Information must be integrated across the visual field and combined according to specific attri-butes. Some of those attributes are pictorial features Addressing the Binding Problem with a Temporal Code like line orientation, texture, color, simultaneity of ap-The Temporal Binding Hypothesis pearance, and common motion, but others require more Von der Malsburg (1981) proposed temporal correlation complex information about such things as 3D shape, to escape a combinatorial problem in neural coding. lighting, and object surface properties (Gregory, 1970; Theories that propose the creation of " cardinal " cells to Marr, 1982; Shimojo et al., 1989; Adelson, 1993; Ullman, represent particular combinations of signals from lower-1996; Adelson, 1999; Kersten, 1999). order neurons are implausible because the number of All of these levels of representation must be combined combinations to be coded exceeds the number of neu-to solve the binding problem. A simple example is cari-rons available. In Von der Malsburg's theory, the activity catured in Figure 1A. The four arrows are effortlessly of low-order neurons would be combined only when perceived as separate overlaid objects, but the compu-their spike activity was synchronized to within a few tations that generate this percept must be informed by milliseconds to create a synchronously active cell as-notions of occlusion and object continuity and must sembly (Hebb, 1949; Braitenberg, 1978; Abeles, 1991). remain unconfused by serendipitously shared features Synchronization would be dynamically modulated, so like orientation and surface color. that a particular cell could belong to one cell assembly Higher-level vision poses many problems; some, like at one moment and to a second at another; in this way, visual object recognition, seem at least as difficult to the combinatorial bullet could be dodged and arbitrarily solve as the binding problem. But, in recent years, von large numbers of states coded with a reasonable num-der Malsburg and others have advanced the view that ber of neurons. binding is a special problem and requires a special solu-Although he conceived the " temporal correlation the-tion (von der Malsburg, 1981, 1985, 1995; von der Mals-ory " to have broad applications to neural computation, burg and Schneider, 1986; Reitboeck et al., 1987; Wang Von der Malsburg offered the specific problem of figure– et al., 1990; Grossberg and Somers, 1991; Sporns et al., ground discrimination as a sample case, suggesting that 1991; Neven and Aertsen, 1992; Tononi et al., 1992; spike synchronization would group together the ele-ments that make up figure and ground (see Lamme and Spekreijse, 1998, for a test of this particular idea).},
author = {Shadlen, M. N. and Movshon, J. A.},
doi = {10.1016/S0896-6273(00)80822-3},
file = {:home/epaxon/Documents/Mendeley Desktop/Shadlen, Movshon - 1999 - Synchrony unbound A critical evaluation of the temporal binding hypothesis.pdf:pdf},
isbn = {0896-6273 (Print)$\backslash$n0896-6273 (Linking)},
issn = {08966273},
journal = {Neuron},
number = {1},
pages = {67--77},
pmid = {10677027},
title = {{Synchrony unbound: A critical evaluation of the temporal binding hypothesis}},
volume = {24},
year = {1999}
}
@article{Shrivastava1992,
author = {Shrivastava, Yash and Dasgupta, Soura and Reddy, Sudhakar M.},
doi = {10.1109/72.165596},
file = {:home/epaxon/Documents/Mendeley Desktop/Shrivastava, Dasgupta, Reddy - 1992 - Guaranteed Convergence in a Class of Hopfield Networks.pdf:pdf},
issn = {19410093},
journal = {IEEE Transactions on Neural Networks},
number = {6},
pages = {951--961},
title = {{Guaranteed Convergence in a Class of Hopfield Networks}},
volume = {3},
year = {1992}
}
@article{Shukla2016,
abstract = {Harnessing the computational capabilities of dynamical systems has attracted the attention of scientists and engineers form varied technical disciplines over decades. The time evolution of coupled, non-linear synchronous oscillatory systems has led to active research in understanding their dynamical properties and exploring their applications in brain-inspired, neuromorphic computational models. In this paper we present the realization of coupled and scalable relaxation-oscillators utilizing the metal-insulator-metal transition of vanadium-dioxide (VO2) thin films. We demonstrate the potential use of such a system in pattern recognition, as one possible computational model using such a system.},
author = {Shukla, N. and Datta, S. and Parihar, A. and Raychowdhury, A.},
doi = {10.1002/9781119069225.ch2-3},
file = {:home/epaxon/Documents/Mendeley Desktop/Shukla et al. - 2016 - Computing with Coupled Relaxation Oscillators.pdf:pdf},
isbn = {9781119069225},
issn = {0738100X},
journal = {Future Trends in Microelectronics: Journey into the Unknown},
keywords = {Boolean computational framework,Complementary metal-oxide-semiconductor transistor,Insulator-metal transition,Phase synchronization dynamics,Power-intensive multiply-accumulate operations,Tunable coupled relaxation oscillators,Vanadium dioxide},
pages = {147--156},
title = {{Computing with Coupled Relaxation Oscillators}},
year = {2016}
}
@article{Siapas2005,
abstract = {The interactions between cortical and hippocampal circuits are critical for memory formation, yet their basic organization at the neuronal network level is not well understood. Here, we demonstrate that a significant portion of neurons in the medial prefrontal cortex of freely behaving rats are phase locked to the hippocampal theta rhythm. In addition, we show that prefrontal neurons phase lock best to theta oscillations delayed by approximately 50 ms and confirm this hippocampo-prefrontal directionality and timing at the level of correlations between single cells. Finally, we find that phase locking of prefrontal cells is predicted by the presence of significant correlations with hippocampal cells at positive delays up to 150 ms. The theta-entrained activity across cortico-hippocampal circuits described here may be important for gating information flow and guiding the plastic changes that are believed to underlie the storage of information across these networks. Copyright ?? 2005 by Elsevier Inc.},
author = {Siapas, Athanassios G. and Lubenov, Evgueniy V. and Wilson, Matthew A.},
doi = {10.1016/j.neuron.2005.02.028},
file = {:home/epaxon/Documents/Mendeley Desktop/Siapas, Lubenov, Wilson - 2005 - Prefrontal phase locking to hippocampal theta oscillations.pdf:pdf},
isbn = {0896-6273 (Print)},
issn = {08966273},
journal = {Neuron},
number = {1},
pages = {141--151},
pmid = {15820700},
title = {{Prefrontal phase locking to hippocampal theta oscillations}},
volume = {46},
year = {2005}
}
@article{Siegel2009a,
abstract = {The ability to hold multiple objects in memory is fundamental to intelligent behavior, but its neural basis remains poorly understood. It has been suggested that multiple items may be held in memory by oscillatory activity across neuronal populations, but yet there is little direct evidence. Here, we show that neuronal information about two objects held in short-term memory is enhanced at specific phases of underlying oscillatory population activity. We recorded neuronal activity from the prefrontal cortices of monkeys remembering two visual objects over a brief interval. We found that during this memory interval prefrontal population activity was rhythmically synchronized at frequencies around 32 and 3 Hz and that spikes carried the most information about the memorized objects at specific phases. Further, according to their order of presentation, optimal encoding of the first presented object was significantly earlier in the 32 Hz cycle than that for the second object. Our results suggest that oscillatory neuronal synchronization mediates a phase-dependent coding of memorized objects in the prefrontal cortex. Encoding at distinct phases may play a role for disambiguating information about multiple objects in short-term memory.},
archivePrefix = {arXiv},
arxivId = {arXiv:1408.1149},
author = {Siegel, Markus and Warden, Melissa R and Miller, Earl K},
doi = {10.1073/pnas.0908193106},
eprint = {arXiv:1408.1149},
file = {:home/epaxon/Documents/Mendeley Desktop/Siegel, Warden, Miller - 2009 - Phase-dependent neuronal coding of objects in short-term memory(2).pdf:pdf},
isbn = {0908193106},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {50},
pages = {21341--21346},
pmid = {19926847},
title = {{Phase-dependent neuronal coding of objects in short-term memory.}},
volume = {106},
year = {2009}
}
@article{Siegel2009,
abstract = {The ability to hold multiple objects in memory is fundamental to intelligent behavior, but its neural basis remains poorly understood. It has been suggested that multiple items may be held in memory by oscillatory activity across neuronal populations, but yet there is little direct evidence. Here, we show that neuronal information about two objects held in short-term memory is enhanced at specific phases of underlying oscillatory population activity. We recorded neuronal activity from the prefrontal cortices of monkeys remembering two visual objects over a brief interval. We found that during this memory interval prefrontal population activity was rhythmically synchronized at frequencies around 32 and 3 Hz and that spikes carried the most information about the memorized objects at specific phases. Further, according to their order of presentation, optimal encoding of the first presented object was significantly earlier in the 32 Hz cycle than that for the second object. Our results suggest that oscillatory neuronal synchronization mediates a phase-dependent coding of memorized objects in the prefrontal cortex. Encoding at distinct phases may play a role for disambiguating information about multiple objects in short-term memory.},
archivePrefix = {arXiv},
arxivId = {arXiv:1408.1149},
author = {Siegel, Markus and Warden, Melissa R. and Miller, Earl K.},
doi = {10.1073/pnas.0908193106},
eprint = {arXiv:1408.1149},
file = {:home/epaxon/Documents/Mendeley Desktop/Siegel, Warden, Miller - 2009 - Phase-dependent neuronal coding of objects in short-term memory.pdf:pdf},
isbn = {0908193106},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {50},
pages = {21341--21346},
pmid = {19926847},
title = {{Phase-dependent neuronal coding of objects in short-term memory}},
url = {http://www.pnas.org/lookup/doi/10.1073/pnas.0908193106},
volume = {106},
year = {2009}
}
@article{Singer2015,
abstract = {Visual recognition takes a small fraction of a second and relies on the cascade of signals along the ventral visual stream. Given the rapid path through multiple processing steps between photoreceptors and higher visual areas, information must progress from stage to stage very quickly. This rapid progression of information suggests that fine temporal details of the neural response may be important to the brain's encoding of visual signals. We investigated how changes in the relative timing of incoming visual stimulation affect the representation of object information by recording intracranial field potentials along the human ventral visual stream while subjects recognized objects whose parts were presented with varying asynchrony. Visual responses along the ventral stream were sensitive to timing differences as small as 17 ms between parts. In particular, there was a strong dependency on the temporal order of stimulus presentation, even at short asynchronies. From these observations we infer that the neural representation of complex information in visual cortex can be modulated by rapid dynamics on scales of tens of milliseconds.},
author = {Singer, Jedediah M. and Madsen, Joseph R. and Anderson, William S. and Kreiman, Gabriel},
doi = {10.1152/jn.00556.2014},
file = {:home/epaxon/Documents/Mendeley Desktop/Singer et al. - 2015 - Sensitivity to timing and order in human visual cortex.pdf:pdf},
issn = {0022-3077},
journal = {Journal of Neurophysiology},
number = {5},
pages = {1656--1669},
pmid = {25429116},
title = {{Sensitivity to timing and order in human visual cortex}},
url = {http://jn.physiology.org/lookup/doi/10.1152/jn.00556.2014},
volume = {113},
year = {2015}
}
@article{Singer2010,
abstract = {In the cerebral cortex two complementary strategies are applied for the representation of perceptual objects: the generation of highly specific neurons that respond to complex conjunctions of elementary features and the dynamic formation of coherent assemblies of cells, whereby each cell codes only for particular aspects of the perceptual object. The second strategy requires that neurons convey two messages in parallel: first, whether the feature constellation to which they are tuned is present and, second, with which of the other neurons they cooperate to form an assembly. Evidence suggests that the presence of the respective feature constellation is signaled by an increase in discharge rate while the relations with other neurons are established by precise synchronization of action potentials. This requires context-dependent dynamic adjustment of spike timing. The oscillatory patterning of neuronal responses that occurs in different frequency bands and in most structures in the brain is proposed to serve this adjustment and to provide the basis for the generation and read-out of temporal codes. ?? 2008 Elsevier Inc. All rights reserved.},
author = {Singer, W.},
doi = {10.1016/B978-012370880-9.00287-5},
file = {:home/epaxon/Documents/Mendeley Desktop/Singer - 2010 - Temporal Coherence A Versatile Code for the Definition of Relations.pdf:pdf},
isbn = {9780123708809},
issn = {08966273},
journal = {The Senses: A Comprehensive Reference},
keywords = {Cerebral,Cortex,Feature binding,Gamma frequency,Neuronal coding,Oscillations,Synchrony,Temporal codes},
pages = {1--9},
pmid = {10677026},
title = {{Temporal Coherence: A Versatile Code for the Definition of Relations}},
volume = {2},
year = {2010}
}
@article{Sirotin2015,
abstract = {Stimulus intensity is a fundamental perceptual feature in all sensory systems. In olfaction, perceived odor intensity depends on at least two variables: odor concentration; and duration of the odor exposure or adaptation. To examine how neural activity at early stages of the olfactory system represents features relevant to intensity perception, we studied the responses of mitral/tufted cells (MTCs) while manipulating odor concentration and exposure duration. Temporal profiles of MTC responses to odors changed both as a function of concentration and with adaptation. However, despite the complexity of these responses, adaptation and concentration dependencies behaved similarly. These similarities were visualized by principal component analysis of average population responses and were quantified by discriminant analysis in a trial-by-trial manner. The qualitative functional dependencies of neuronal responses paralleled psychophysics results in humans. We suggest that temporal patterns of MTC responses in the olfactory bulb contribute to an internal perceptual variable: odor intensity.},
author = {Sirotin, Yevgeniy B and Shusterman, Roman and Rinberg, Dmitry},
doi = {10.1523/ENEURO.0083-15.2015},
file = {:home/epaxon/Documents/Mendeley Desktop/Sirotin, Shusterman, Rinberg - 2015 - Neural coding of perceived odor intensity.pdf:pdf},
isbn = {2373-2822 (Electronic) 2373-2822 (Linking)},
issn = {2373-2822},
journal = {eNeuro},
keywords = {animal models,concentration versus adaptation,ence,establishing a link between,extracellular electrophysiology,human psychophysics,is one of the,major goals of systems,neural recording,neurosci-,olfactory bulb,perception and neural activity,remains,significance statement,tracking perceptual variables in,where one can perform,yet},
number = {December},
pages = {1--16},
pmid = {26665162},
title = {{Neural coding of perceived odor intensity}},
volume = {2},
year = {2015}
}
@article{Skaggs1996,
abstract = {O'Keefe and Recce [1993] Hippocampus 3:317-330 described an interaction between the hippocampal theta rhythm and the spatial firing of pyramidal cells in the CA1 region of the rat hippocampus: they found that a cell's spike activity advances to earlier phases of the theta cycle as the rat passes through the cell's place field. The present study makes use of large-scale parallel recordings to clarify and extend this finding in several ways: 1) Most CA1 pyramidal cells show maximal activity at the same phase of the theta cycle. Although individual units exhibit deeper modulation, the depth of modulation of CA1 population activity is about 50{\%}. The peak firing of inhibitory interneurons in CA1 occurs about 60 degrees in advance of the peak firing of pyramidal cells, but different interneurons vary widely in their peak phases. 2) The first spikes, as the rat enters a pyramidal cell's place field, come 90 degrees-120 degrees after the phase of maximal pyramidal cell population activity, near the phase where inhibition is least. 3) The phase advance is typically an accelerating, rather than linear, function of position within the place field. 4) These phenomena occur both on linear tracks and in two-dimensional environments where locomotion is not constrained to specific paths. 5) In two-dimensional environments, place-related firing is more spatially specific during the early part of the theta cycle than during the late part. This is also true, to a lesser extent, on a linear track. Thus, spatial selectivity waxes and wanes over the theta cycle. 6) Granule cells of the fascia dentata are also modulated by theta. The depth of modulation for the granule cell population approaches 100{\%}, and the peak activity of the granule cell population comes about 90 degrees earlier in the theta cycle than the peak firing of CA1 pyramidal cells. 7) Granule cells, like pyramidal cells, show robust phase precession. 8) Cross-correlation analysis shows that portions of the temporal sequence of CA1 pyramidal cell place fields are replicated repeatedly within individual theta cycles, in highly compressed form. The compression ratio can be as much as 10:1. These findings indicate that phase precession is a very robust effect, distributed across the entire hippocampal population, and that it is likely to be inherited from the fascia dentata or an earlier stage in the hippocampal circuit, rather than generated intrinsically within CA1. It is hypothesized that the compression of temporal sequences of place fields within individual theta cycles permits the use of long-term potentiation for learning of sequential structure, thereby giving a temporal dimension to hippocampal memory traces.},
author = {Skaggs, William E. and McNaughton, Bruce L. and Wilson, Matthew A. and Barnes, Carol A.},
doi = {10.1002/(SICI)1098-1063(1996)6:2<149::AID-HIPO6>3.0.CO;2-K},
file = {:home/epaxon/Documents/Mendeley Desktop/Skaggs et al. - 1996 - Theta phase precession in hippocampal neuronal populations and the compression of temporal sequences.pdf:pdf},
isbn = {1050-9631},
issn = {10509631},
journal = {Hippocampus},
keywords = {CA1,Fascia dentata,Long-term potentiation,Phase coding,Place cells},
number = {2},
pages = {149--172},
pmid = {8797016},
title = {{Theta phase precession in hippocampal neuronal populations and the compression of temporal sequences}},
volume = {6},
year = {1996}
}
@article{Smith1999,
author = {Smith, Kate A.},
doi = {10.1287/ijoc.11.1.15},
file = {:home/epaxon/Documents/Mendeley Desktop/Smith - 1999 - Neural Networks for Combinatorial Optimization A Review of More Than a Decade of Research.pdf:pdf},
isbn = {1091-9856},
issn = {10919856},
journal = {INFORMS Journal on Computing},
number = {1},
pages = {15--35},
title = {{Neural Networks for Combinatorial Optimization: A Review of More Than a Decade of Research.}},
volume = {11},
year = {1999}
}
@article{Sommer2005,
abstract = {Synfire chain models store and retrieve hetero-associative sequences of firing patterns, thereby explaining basic aspects of the neuronal processing of temporal information. Existing models were based on McCulloch-Pitts or integrate {\&} fire neurons and therefore neglect most physiological properties of real neurons. Here, we study a model with conductance-based neurons and both, hetero- and auto-associative couplings which support synfire vs. attractor activity, respectively. We show that the speed of synfire recall is influenced by slow neuronal variables and is sensitive to the ratio between auto- and hetero-associative synapses while quite insensitive to background activity. We then propose a bidirectional synfire model where the duration of states in a synfire chain is variable and can be coordinated by a timed but otherwise unspecific external signal. ?? 2004 Elsevier B.V. All rights reserved.},
author = {Sommer, Friedrich T. and Wennekers, Thomas},
doi = {10.1016/j.neucom.2004.10.015},
file = {:home/epaxon/Documents/Mendeley Desktop/Sommer, Wennekers - 2005 - Synfire chains with conductance-based neurons Internal timing and coordination with timed input.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Associative memory,Sequence memory,Synfire chains,Temporal neural processing,Timing of sequences},
number = {SPEC. ISS.},
pages = {449--454},
title = {{Synfire chains with conductance-based neurons: Internal timing and coordination with timed input}},
volume = {65-66},
year = {2005}
}
@article{Sompolinsky1986,
abstract = {A neural network model which is capable of recalling time sequences and cycles of patterns is intro-duced. In this model, some of the synaptic connections, J{\~{}}, between pairs of neurons are asymmetric (JttseJN) and have slow dynamic response. The effects of thermal noise on the generated sequences are discussed. Simulation results demonstrating the performance of the network are presented. The model may be also useful in understanding the generation of rhythmic patterns in biological motor systems. PACS numbers: 87.30.6y Studies' of neural-network models of associative memory focussed mostly on systems with symmetric cou-plings, i.e. , the connections between pairs of neurons satisfy JJ JJ;, iwj. In this case, the dynamics of the network is relatively simple: The system relaxes to states which are local minima of a global energy function, E. Once such a local minimum has been reached it remains absolutely stable (at zero temperature). To endow the network with properties of associative memory, the JJ. are designed so that a given set of states of the network (the embedded memories) are local minima of E. The synaptic connections in biological neural networks have a high degree of asymmetry, but the role of the asymmetry in the collective behavior of the networks has been un-clear. Recent studies have indicated that adding a weak random asymmetry to a symmetric network increases the level of the internal noise but otherwise does not modify drastically the capability to store and retrieve informa-tion '45 Symmetric networks cannot provide temporal associa tion They lack .the ability to retrieve a sequence of pat-terns using a single recalling input. In this paper, a model of a neural network with temporal association is proposed. The network has asymmetric bonds in which a sequence of patterns is embedded. In general, asym-metric bonds may give rise to cyclic or perhaps even chaotic flows in configuration space. However, there is no reason to expect that these flows enjoy the same kind of robustness (e.g. , large basins of attraction, indepen-dence of the details of the dynamics, stability against noise) as that of stable states in symmetric networks. Evidently, a desirable motion would consist of a con™ trolled set of transitions among "quasiequilibrium" states: The system stays in a state (or in its neighbor-hood) for a long but finite period of time after which a transition is made to the next quasiequilibrium state in the sequence. To accomplish this, we propose that the response of the neuron at one end of the asymmetric bond to a signal at the other end is not instantaneous but has a dynamic memory characterized by a time constant r. This leads to the emergence of a sequence of patterns, each stable over a time period of the order of r, as will be shown below by analytical study and numerical simu-lations. The maximum length of the sequence that can be embedded in a network of size N is proportional to N. We argue that under certain circumstances the se-quences are stable even in the presence of thermal fluc-tuations. This network provides perhaps the simplest mechanism by which sequences of transitions between quasiequilibrium states can be embedded in highly con-nected systems. We consider a network consisting of N Ising spins (Stion t, where S; +1 (— 1) represents the firing (nonfiring) state of the ith neuron. The embedded pat-terns are p states of the system {\{}g, ". lp t, It 1,2, . . . ,p. The patterns are random so that each g, ". takes the values 1 with equal probabilities. The couplings between the neurons are formed by two kinds of synapses. Synapses of the first kind are symmetric and are given by the "Hebb rule" J. (.l) {\~{}} Q gPgP P (1) @{\~{}}1 as in the Hopfield model. ' The second kind of synapses are J. (. 2) {\~{}} p+ {\&} p (2) where q {\&}p. The asymmetric synapses J;tt2l define an order among the q patterns. Cycles can be incorporated by definition of (f+' g, in (2). The model can be ex-tended easily to contain several sequences and cycles, provided that they do not compete against each other, i.e. , that a given pattern, p, is connected to only one pat-tern, It+1. The addition of synapses of the form (2) was discussed a few years ago by Hopfield, ' who noted that they are insufficient for the generation of stable se-quences of patterns. If k is small, transitions between the patterns do not occur at all and the patterns remain completely stable. If X is large, the transitions occur too fast so that the patterns are not stable even for a limited amount of time, and the sequence is very quickly smeared. This problem is rectified in the present work by endowment of the synapses JtP with slow dynamic response.}},
author = {Sompolinsky, H. and Kanter, I.},
doi = {10.1103/PhysRevLett.57.2861},
file = {:home/epaxon/Documents/Mendeley Desktop/Sompolinsky, Kanter - 1986 - Temporal association in asymmetric neural networks.pdf:pdf},
isbn = {0-7803-0227-3},
issn = {00319007},
journal = {Physical Review Letters},
number = {22},
pages = {2861--2864},
pmid = {10033885},
title = {{Temporal association in asymmetric neural networks}},
volume = {57},
year = {1986}
}
@article{Sompolinsky1991,
author = {Sompolinsky, Haim and Golomb, David and Kleinfeld, David},
file = {:home/epaxon/Documents/Mendeley Desktop/Sompolinsky, Golomb, Kleinfeld - 1991 - Cooperative dynamics in visual processing.pdf:pdf},
number = {12},
title = {{Cooperative dynamics in visual processing}},
volume = {43},
year = {1991}
}
@article{Stein1965,
abstract = {A simple neuronal model is assumed in which, after a refractory period, excitatory and inhibitory exponentially decaying inputs of constant size occur at random intervals and sum until a threshold is reached. The distribution of time intervals between successive neuronal firings (interresponse time histogram), the firing rate as a function of input frequency, the variability in the time course of depolarization from trial to trial, and the strength-duration curve are derived for this model. The predictions are compared with data from the literature and good qualitative agreement is found. All parameters are experimentally measurable and a direct test of the theory is possible with present techniques. The assumptions of the model are relaxed and the effects of such experimentally found phenomena as relative refractory and supernormal periods, adaptation, potentiation, and rhythmic slow potentials are discussed. Implications for gross behavior studies are considered briefly. ?? 1965, The Biophysical Society. All rights reserved.},
author = {Stein, Richard B.},
doi = {10.1016/S0006-3495(65)86709-1},
file = {:home/epaxon/Documents/Mendeley Desktop/Stein - 1965 - A Theoretical Analysis of Neuronal Variability.pdf:pdf},
issn = {00063495},
journal = {Biophysical Journal},
number = {2},
pages = {173--194},
pmid = {14268952},
publisher = {Elsevier},
title = {{A Theoretical Analysis of Neuronal Variability}},
url = {http://dx.doi.org/10.1016/S0006-3495(65)86709-1},
volume = {5},
year = {1965}
}
@article{Stewart2012,
abstract = {The Neural Engineering Framework (NEF) is a general methodology that allows the building of large-scale, biologically plausible, neural models of cognition. The NEF acts as a neural compiler: once the properties of the neurons, the values to be represented, and the functions to be computed are specified, it solves for the connection weights between components that will perform the desired functions. Importantly, this works not only for feed-forward computations, but also for recurrent connections, allowing for complex dynamical systems including integrators, oscillators, Kalman filters, etc. The NEF also incorporates realistic local error-driven learning rules, allowing for the online adaptation and optimisation of responses. The NEF has been used to model visual attention, inductive reasoning, reinforcement learning and many other tasks. Recently, we used it to build Spaun, the world{\{}$\backslash$textquoteright{\}}s largest functional brain model, using 2.5 million neurons to perform eight different cognitive tasks by interpreting visual input and producing hand-written output via a simulated 6-muscle arm. Our open-source software Nengo was used for all of these, and is available at http://nengo.ca, along with tutorials, demos, and downloadable models.$\backslash$n},
author = {Stewart, Terrence C},
file = {:home/epaxon/Documents/Mendeley Desktop/Stewart - 2012 - A Technical Overview of the Neural Engineering Framework.pdf:pdf},
journal = {The Newsletter of the Society for the Study of Artificial Intelligence and Simulation of Behaviour},
number = {135},
pages = {1--8},
pmid = {1000185221},
title = {{A Technical Overview of the Neural Engineering Framework}},
volume = {135},
year = {2012}
}
@article{Strogatz2001,
abstract = {The study of networks pervades all of science, from neurobiology to statistical physics. The most basic issues are structural: how does one characterize the wiring diagram of a food web or the Internet or the metabolic network of the bacterium Escherichia coli? Are there any unifying principles underlying their topology? From the perspective of nonlinear dynamics, we would also like to understand how an enormous network of interacting dynamical systems-be they neurons, power stations or lasers-will behave collectively, given their individual dynamics and coupling architecture. Researchers are only now beginning to unravel the structure and dynamics of complex networks.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0102091},
author = {Strogatz, Steven H.},
doi = {10.1038/35065725},
eprint = {0102091},
file = {:home/epaxon/Documents/Mendeley Desktop/Strogatz - 2001 - Exploring complex networks.pdf:pdf},
isbn = {0028-0836},
issn = {0028-0836},
journal = {Nature},
number = {6825},
pages = {268--276},
pmid = {11258382},
primaryClass = {cond-mat},
title = {{Exploring complex networks}},
url = {http://www.nature.com/doifinder/10.1038/35065725},
volume = {410},
year = {2001}
}
@article{Sun2000,
abstract = {In our previous work,the eliminating-highest error (EHE) criterion was proposed for the modified Hopfield neural network (MHNN) for image restoration and reconstruction. The performance of the MHNN is considerably improved by the EHE criterion as shown in many simulations. In inspiration of revealing the insight of the EHE criterion, in this paper, we first present a generalized updating rule (GUR) of the MHNN for gray image recovery. The stability properties of the GUR are given, It is shown that the neural threshold set up in this GUR is necessary and sufficient for energy decrease with probability one at each update, The new fastest-energy-descent (FED) criterion is then proposed parallel to the EHE criterion. While the EHE criterion is shown to achieve the highest probability of correct transition, the FED criterion achieves the largest amount of energy descent. In image restoration, the EHE and FED criteria are equivalent, A group of new algorithms based on the EHE and FED criteria is set up. A new measure, the correct transition rate (CTR), is proposed for the performance of iterative algorithms. Simulation results for gray image restoration show that the EHE (FED) based algorithms obtained the best visual quality and highest SNR of recovered images, took much smaller number of iterations, and had higher CTR, The CTR is shown to be a rational performance measure of iterative algorithms and predict quality of recovered images.},
author = {Sun, Y},
file = {:home/epaxon/Documents/Mendeley Desktop/Sun - 2000 - Hopfield neural network based algorithms for image restoration and reconstruction - Part I Algorithms and simulations.pdf:pdf},
isbn = {1053-587X},
journal = {IEEE Transactions on Signal Processing},
number = {7},
pages = {2105--2118},
title = {{Hopfield neural network based algorithms for image restoration and reconstruction - Part I: Algorithms and simulations}},
volume = {48},
year = {2000}
}
@article{Szatmary2010,
abstract = {Working memory (WM) is the part of the brain's memory system that provides temporary storage and manipulation of information necessary for cognition. Although WM has limited capacity at any given time, it has vast memory content in the sense that it acts on the brain's nearly infinite repertoire of lifetime long-term memories. Using simulations, we show that large memory content and WM functionality emerge spontaneously if we take the spike-timing nature of neuronal processing into account. Here, memories are represented by extensively overlapping groups of neurons that exhibit stereotypical time-locked spatiotemporal spike-timing patterns, called polychronous patterns; and synapses forming such polychronous neuronal groups (PNGs) are subject to associative synaptic plasticity in the form of both long-term and short-term spike-timing dependent plasticity. While long-term potentiation is essential in PNG formation, we show how short-term plasticity can temporarily strengthen the synapses of selected PNGs and lead to an increase in the spontaneous reactivation rate of these PNGs. This increased reactivation rate, consistent with in vivo recordings during WM tasks, results in high interspike interval variability and irregular, yet systematically changing, elevated firing rate profiles within the neurons of the selected PNGs. Additionally, our theory explains the relationship between such slowly changing firing rates and precisely timed spikes, and it reveals a novel relationship between WM and the perception of time on the order of seconds.},
author = {Szatm{\'{a}}ry, Botond and Izhikevich, Eugene M.},
doi = {10.1371/journal.pcbi.1000879},
file = {:home/epaxon/Documents/Mendeley Desktop/Szatm{\'{a}}ry, Izhikevich - 2010 - Spike-timing theory of working memory.pdf:pdf},
isbn = {1553-7358 (Electronic)$\backslash$r1553-734X (Linking)},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {8},
pmid = {20808877},
title = {{Spike-timing theory of working memory}},
volume = {6},
year = {2010}
}
@article{Tank1987,
abstract = {An analog model neural network that can solve a general problem of recognizing patterns in a time-dependent signal is presented. The networks use a patterned set of delays to collectively focus stimulus sequence information to a neural state at a future time. The computational capabilities of the circuit are demonstrated on tasks somewhat similar to those necessary for the recognition of words in a continuous stream of speech. The network architecture can be understood from consideration of an energy function that is being minimized as the circuit computes. Neurobiological mechanisms are known for the generation of appropriate delays.},
author = {Tank, D W and Hopfield, J J},
doi = {10.1073/pnas.84.7.1896},
file = {:home/epaxon/Documents/Mendeley Desktop/Tank, Hopfield - 1987 - Neural computation by concentrating information in time.pdf:pdf},
isbn = {0027-8424 (Print)$\backslash$r0027-8424 (Linking)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {7},
pages = {1896--1900},
pmid = {3470765},
title = {{Neural computation by concentrating information in time.}},
volume = {84},
year = {1987}
}
@article{Tatem2001,
abstract = {Fuzzy classification techniques have been developed recently to estimate the class composition of image pixels, but their output provides no indication of how these classes are distributed spatially within the instantaneous field of view represented by the pixel. As such, while the accuracy of land cover target identification has been improved using fuzzy classification, it remains for robust techniques that provide better spatial representation of land cover to be developed. Such techniques could provide more accurate land cover metrics for determining social or environmental policy, for example. The use of a Hopfield neural network to map the spatial distribution of classes more reliably using prior information of pixel composition determined from fuzzy classification was investigated. An approach was adopted that used the output from a fuzzy classification to constrain a Hopfield neural network formulated as an energy minimization tool. The network converges to a minimum of an energy function, defined as a goal and several constraints. Extracting the spatial distribution of target class components within each pixel was, therefore, formulated as a constraint satisfaction problem with an optimal solution determined by the minimum of the energy function. This energy minimum represents a {\&}ldquo;best guess{\&}rdquo; map of the spatial distribution of class components in each pixel. The technique was applied to both synthetic and simulated Landsat TM imagery, and the resultant maps provided an accurate and improved representation of the land covers studied, with root mean square errors (RMSEs) for Landsat imagery of the order of 0.09 pixels in the new fine resolution image recorded},
author = {Tatem, Andrew J. and Lewis, Hugh G. and Atkinson, Peter M. and Nixon, Mark S.},
doi = {10.1109/36.917895},
file = {:home/epaxon/Documents/Mendeley Desktop/Tatem et al. - 2001 - Super-resolution target identification from remotely sensed images using a Hopfield neural network.pdf:pdf},
isbn = {0196-2892},
issn = {01962892},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
keywords = {Fuzzy image classification,Hopfield networks,Image resolution,Land cover,Optimization methods,Super-resolution object detection},
number = {4},
pages = {781--796},
title = {{Super-resolution target identification from remotely sensed images using a Hopfield neural network}},
volume = {39},
year = {2001}
}
@article{Terman1995a,
author = {Terman, David and Wang, Deliang},
file = {:home/epaxon/Documents/Mendeley Desktop/Terman, Wang - 1995 - Global competition and local cooperation in a network of neural oscillators.pdf:pdf},
isbn = {0167-2789},
issn = {0167-2789},
journal = {Physica D},
pages = {148--176},
title = {{Global competition and local cooperation in a network of neural oscillators}},
volume = {81},
year = {1995}
}
@article{Thorpe1997,
author = {Thorpe, S J and Gautrais, J},
file = {:home/epaxon/Documents/Mendeley Desktop/Thorpe, Gautrais - 1997 - Rapid Visual Processing using Spike Asynchrony.pdf:pdf},
journal = {Neural Information Processing Systems},
pages = {901--907},
title = {{Rapid Visual Processing using Spike Asynchrony}},
year = {1997}
}
@misc{Thorpe1996,
abstract = {How long does it take for the human visual system to process a complex natural image? Subjectively, recognition of familiar objects and scenes appears to be virtually instantaneous, but measuring this processing time experimentally has proved difficult. Behavioural measures such as reaction times can be used, but these include not only visual processing but also the time required for response execution. However, event-related potentials (ERPs) can sometimes reveal signs of neural processing well before the motor output. Here we use a go/no-go categorization task in which subjects have to decide whether a previously unseen photograph, flashed on for just 20 ms, contains an animal. ERP analysis revealed a frontal negativity specific to no-go trials that develops roughly 150 ms after stimulus onset. We conclude that the visual processing needed to perform this highly demanding task can be achieved in under 150 ms.},
author = {Thorpe, S and Fize, D and Marlot, C},
booktitle = {Nature},
doi = {10.1038/381520a0},
file = {:home/epaxon/Documents/Mendeley Desktop/Thorpe, Fize, Marlot - 1996 - Speed of processing in the human visual system.pdf:pdf},
isbn = {0028-0836 (Print)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
number = {6582},
pages = {520--522},
pmid = {8632824},
title = {{Speed of processing in the human visual system.}},
volume = {381},
year = {1996}
}
@article{Tiesinga2008,
abstract = {A train of action potentials (a spike train) can carry information in both the average firing rate and the pattern of spikes in the train. But can such a spike-pattern code be supported by cortical circuits? Neurons in vitro produce a spike pattern in response to the injection of a fluctuating current. However, cortical neurons in vivo are modulated by local oscillatory neuronal activity and by top-down inputs. In a cortical circuit, precise spike patterns thus reflect the interaction between internally generated activity and sensory information encoded by input spike trains. We review the evidence for precise and reliable spike timing in the cortex and discuss its computational role.},
author = {Tiesinga, Paul and Fellous, Jean-Marc and Sejnowski, Terrence J.},
doi = {10.1038/nrn2315},
file = {:home/epaxon/Documents/Mendeley Desktop/Tiesinga, Fellous, Sejnowski - 2008 - Regulation of spike timing in visual cortical circuits.pdf:pdf},
isbn = {1471-0048 (Electronic)},
issn = {1471-003X},
journal = {Nature Reviews Neuroscience},
number = {2},
pages = {97--107},
pmid = {18200026},
title = {{Regulation of spike timing in visual cortical circuits}},
url = {http://www.nature.com/doifinder/10.1038/nrn2315},
volume = {9},
year = {2008}
}
@article{Trengove2016,
abstract = {As a candidate mechanism of neural representation, large numbers of synfire chains can efficiently be embedded in a balanced recurrent cortical network model. Here we study a model in which multiple synfire chains of variable strength are randomly coupled together to form a recurrent system. The system can be implemented both as a large-scale network of integrate-and-fire neurons and as a reduced model. The latter has binary-state pools as basic units but is otherwise isomorphic to the large-scale model, and provides an efficient tool for studying its behavior. Both the large-scale system and its reduced counterpart are able to sustain ongoing endogenous activity in the form of synfire waves, the proliferation of which is regulated by negative feedback caused by collateral noise. Within this equilibrium, diverse repertoires of ongoing activity are observed, including meta-stability and multiple steady states. These states arise in concert with an effective connectivity structure (ECS). The ECS admits a family of effective connectivity graphs (ECGs), parametrized by the mean global activity level. Of these graphs, the strongly connected components and their associated out-components account to a large extent for the observed steady states of the system. These results imply a notion of dynamic effective connectivity as governing neural computation with synfire chains, and related forms of cortical circuitry with complex topologies.},
author = {Trengove, Chris and Diesmann, Markus and van Leeuwen, Cees},
doi = {10.1007/s10827-015-0581-5},
file = {:home/epaxon/Documents/Mendeley Desktop/Trengove, Diesmann, Leeuwen - 2016 - Dynamic effective connectivity in cortically embedded systems of recurrently coupled synfire chains.pdf:pdf},
isbn = {1573-6873 (Electronic)$\backslash$r0929-5313 (Linking)},
issn = {15736873},
journal = {Journal of Computational Neuroscience},
keywords = {Background synaptic noise,Combinatorial representation,Effective connectivity,Metastability,Recurrent network dynamics,Synfire chains},
number = {1},
pages = {1--26},
pmid = {26560334},
publisher = {Journal of Computational Neuroscience},
title = {{Dynamic effective connectivity in cortically embedded systems of recurrently coupled synfire chains}},
url = {http://dx.doi.org/10.1007/s10827-015-0581-5},
volume = {40},
year = {2016}
}
@article{Truccolo2005,
abstract = {Multiple factors simultaneously affect the spiking activity of individual neurons. Determining the effects and relative importance of these factors is a challenging problem in neurophysiology. We propose a statistical framework based on the point process likelihood function to relate a neuron's spiking probability to three typical covariates: the neuron's own spiking history, concurrent ensemble activity, and extrinsic covariates such as stimuli or behavior. The framework uses parametric models of the conditional intensity function to define a neuron's spiking probability in terms of the covariates. The discrete time likelihood function for point processes is used to carry out model fitting and model analysis. We show that, by modeling the logarithm of the conditional intensity function as a linear combination of functions of the covariates, the discrete time point process likelihood function is readily analyzed in the generalized linear model (GLM) framework. We illustrate our approach for both GLM and non-GLM likelihood functions using simulated data and multivariate single-unit activity data simultaneously recorded from the motor cortex of a monkey performing a visuomotor pursuit-tracking task. The point process framework provides a flexible, computationally efficient approach for maximum likelihood estimation, goodness-of-fit assessment, residual analysis, model selection, and neural decoding. The framework thus allows for the formulation and analysis of point process models of neural spiking activity that readily capture the simultaneous effects of multiple covariates and enables the assessment of their relative importance.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Truccolo, Wilson and Eden, Uri T and Fellows, Matthew R and Donoghue, John P and Brown, Emery N and John, P},
doi = {10.1152/jn.00697.2004},
eprint = {NIHMS150003},
file = {:home/epaxon/Documents/Mendeley Desktop/Truccolo et al. - 2005 - A Point Process Framework for Relating Neural Spiking Activity to Spiking History, Neural Ensemble, and Extrins.pdf:pdf},
isbn = {0022-3077 (Print)},
issn = {0022-3077},
journal = {Journal of neurophysiology},
keywords = {Action Potentials,Action Potentials: physiology,Animals,Neural Networks (Computer),Neurons,Neurons: physiology,Psychomotor Performance,Psychomotor Performance: physiology},
number = {2},
pages = {1074--1089},
pmid = {15356183},
title = {{A Point Process Framework for Relating Neural Spiking Activity to Spiking History, Neural Ensemble, and Extrinsic Covariate Effects}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15356183{\%}5Cnhttp://jn.physiology.org/cgi/doi/10.1152/jn.00697.2004},
volume = {93},
year = {2005}
}
@article{TrygveSolstad2007,
abstract = {Anatomical connectivity and recent neurophysiological results imply that grid cells in the medial entorhinal cortex are the prin- cipal cortical inputs to place cells in the hippocampus. The authors pro- pose a model in which place fields of hippocampal pyramidal cells are formed by linear summation of appropriately weighted inputs from entorhinal grid cells. Single confined place fields could be formed by summing input from a modest number (10–50) of grid cells with rela- tively similar grid phases, diverse grid orientations, and a biologically plausible range of grid spacings. When the spatial phase variation in the grid-cell input was higher, multiple, and irregularly spaced firing fields were formed. These observations point to a number of possible con- straints in the organization of functional connections between grid cells and place cells},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {{Trygve Solstad} and Moser, Edvard I. and {Gaute T. Einevoll}},
doi = {10.1002/hipo},
eprint = {NIHMS150003},
file = {:home/epaxon/Documents/Mendeley Desktop/Trygve Solstad, Moser, Gaute T. Einevoll - 2007 - From Grid Cells to Place Cells A Mathematical Model.pdf:pdf},
isbn = {1050-9631 (Print)$\backslash$r1050-9631 (Linking)},
issn = {1050-9631},
journal = {Hippocampus},
keywords = {campus,dendrites,entorhinal cortex,hippo-,place cells,stellate cells,theta rhythm},
number = {9},
pages = {801--812},
pmid = {17598147},
title = {{From Grid Cells to Place Cells: A Mathematical Model}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve{\&}db=PubMed{\&}dopt=Citation{\&}list{\_}uids=17598147},
volume = {17},
year = {2007}
}
@article{TrygveSolstad2007a,
abstract = {Anatomical connectivity and recent neurophysiological results imply that grid cells in the medial entorhinal cortex are the prin- cipal cortical inputs to place cells in the hippocampus. The authors pro- pose a model in which place fields of hippocampal pyramidal cells are formed by linear summation of appropriately weighted inputs from entorhinal grid cells. Single confined place fields could be formed by summing input from a modest number (10–50) of grid cells with rela- tively similar grid phases, diverse grid orientations, and a biologically plausible range of grid spacings. When the spatial phase variation in the grid-cell input was higher, multiple, and irregularly spaced firing fields were formed. These observations point to a number of possible con- straints in the organization of functional connections between grid cells and place cells},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {{Trygve Solstad} and Moser, Edvard I. and {Gaute T. Einevoll}},
doi = {10.1002/hipo},
eprint = {NIHMS150003},
file = {:home/epaxon/Documents/Mendeley Desktop/Trygve Solstad, Moser, Gaute T. Einevoll - 2007 - From Grid Cells to Place Cells A Mathematical Model.pdf:pdf},
isbn = {1050-9631 (Print)$\backslash$r1050-9631 (Linking)},
issn = {1050-9631},
journal = {Hippocampus},
keywords = {campus,dendrites,entorhinal cortex,hippo-,place cells,stellate cells,theta rhythm},
number = {9},
pages = {801--812},
pmid = {17598147},
title = {{From Grid Cells to Place Cells: A Mathematical Model}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve{\&}db=PubMed{\&}dopt=Citation{\&}list{\_}uids=17598147},
volume = {17},
year = {2007}
}
@article{Tsodyks1996,
abstract = {O'Keefe and Recce ([1993] Hippocampus 68:317-330) have observed that the spatially selective firing of pyramidal cells in the CA1 field of the rat hippocampus tends to advance to earlier phases of the electroencephalogram theta rhythm as a rat passes through the place field of a cell. We present here a neural network model based on integrate- and-fire neurons that accounts for this effect. In this model, place selectivity in the hippocampus is a consequence of synaptic interactions between pyramidal neurons together with weakly selective external input. The phase shift of neuronal spiking arises in the model as result of asymmetric spread of activation through the network, caused by asymmetry in the synaptic interactions. Several experimentally observed properties of the phase shift effect follow naturally from the model, including 1) the observation that the first spikes a cell fires appear near the theta phase corresponding to minimal population activity, 2) the overall advance is less than 360 degrees, and 3) the location of the rat within the place field of the cell is the primary correlate of the firing phase, not the time the rat has been in the field. The model makes several predictions concerning the emergence of place fields during the earliest stages of exploration in a novel environment. It also suggests new experiments that could provide further constraints on a possible explanation of the phase precession effect.},
author = {Tsodyks, M V and Skaggs, W E and Sejnowski, T J and McNaughton, B L},
doi = {10.1002/(SICI)1098-1063(1996)6:3&lt;271::AID-HIPO5&gt;3.0.CO;2-Q},
file = {:home/epaxon/Documents/Mendeley Desktop/Tsodyks et al. - 1996 - Population dynamics and theta rhythm phase precession of hippocampal place cell firing a spiking neuron model.pdf:pdf},
isbn = {1050-9631 (Print)$\backslash$r1050-9631 (Linking)},
issn = {1050-9631},
journal = {Hippocampus},
keywords = {*Models, Neurological,*Theta Rhythm,Animals,Electrophysiology,Hippocampus/cytology/*physiology,Neural Networks (Computer),Neurons/*physiology,Orientation/*physiology,Population Dynamics,Rats},
number = {3},
pages = {271--280},
pmid = {8841826},
title = {{Population dynamics and theta rhythm phase precession of hippocampal place cell firing: a spiking neuron model}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/8841826},
volume = {6},
year = {1996}
}
@article{Tsodyks1998,
abstract = {Transmission across neocortical synapses depends on the frequency of presynaptic activity (Thomson {\&} Deuchars, 1994). Interpyramidal synapses in layer V exhibit fast depression of synaptic transmission, while other types of synapses exhibit facilitation of transmission. To study the role of dynamic synapses in network computation, we propose a unified phenomenological model that allows computation of the postsynaptic current generated by both types of synapses when driven by an arbitrary pattern of action potential (AP) activity in a presynaptic population. Using this formalism, we analyze different regimes of synaptic transmission and demonstrate that dynamic synapses transmit different aspects of the presynaptic activity depending on the average presynaptic frequency. The model also allows for derivation of mean-field equations, which govern the activity of large, interconnected networks. We show that the dynamics of synaptic transmission results in complex sets of regular and irregular regimes of network activity.},
author = {Tsodyks, M and Pawelzik, K and Markram, H},
doi = {10.1162/089976698300017502},
file = {:home/epaxon/Documents/Mendeley Desktop/Tsodyks, Pawelzik, Markram - 1998 - Neural networks with dynamic synapses.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural computation},
number = {4},
pages = {821--835},
pmid = {9573407},
title = {{Neural networks with dynamic synapses.}},
volume = {10},
year = {1998}
}
@book{Tully2016,
abstract = {Many cognitive and motor functions are enabled by the temporal representation and processing of stimuli, but it remains an open issue how neocortical microcircuits can reliably encode and replay such sequences of information. To better understand this, a modular attractor memory network is proposed in which meta-stable sequential attractor transitions are learned through changes to synaptic weights and intrinsic excitabilities via the spike-based Bayesian Confidence Propagation Neural Network (BCPNN) learning rule. We find that the formation of distributed memories, embodied by increased periods of firing in pools of excitatory neurons, together with asymmetrical associations between these distinct network states, can be acquired through plasticity. The model's feasibility is demonstrated using simulations of adaptive exponential integrate-and-fire model neurons (AdEx). We show that the learning and speed of sequence replay depends on a confluence of biophysically relevant parameters including stimulus duration, level of background noise, ratio of synaptic currents, and strengths of short-term depression and adaptation. Moreover, sequence elements are shown to flexibly participate multiple times in the sequence, suggesting that spiking attractor networks of this type can support an efficient combinatorial code. The model provides a principled approach towards understanding how multiple interacting plasticity mechanisms can coordinate hetero-associative learning in unison.},
author = {Tully, Philip J. and Lind??n, Henrik and Hennig, Matthias H. and Lansner, Anders},
booktitle = {PLoS Computational Biology},
doi = {10.1371/journal.pcbi.1004954},
file = {:home/epaxon/Documents/Mendeley Desktop/Tully et al. - 2016 - Spike-Based Bayesian-Hebbian Learning of Temporal Sequences.PDF:PDF},
isbn = {6212012350},
issn = {15537358},
number = {5},
pages = {1--35},
pmid = {27213810},
title = {{Spike-Based Bayesian-Hebbian Learning of Temporal Sequences}},
volume = {12},
year = {2016}
}
@article{Uzzell2004,
author = {Uzzell, V. J. and Chichilnisky, E J},
doi = {10.1152/jn.01171.2003},
file = {:home/epaxon/Documents/Mendeley Desktop/Uzzell, Chichilnisky - 2004 - Precision of Spike Trains in Primate Retinal Ganglion Cells.pdf:pdf},
isbn = {0022-3077 (Print)$\backslash$r0022-3077 (Linking)},
issn = {0022-3077},
journal = {Journal of Neurophysiology},
month = {mar},
number = {2},
pages = {780--789},
pmid = {15277596},
title = {{Precision of Spike Trains in Primate Retinal Ganglion Cells}},
url = {http://jn.physiology.org/cgi/doi/10.1152/jn.01171.2003},
volume = {92},
year = {2004}
}
@article{VanRullen2005,
abstract = {Many behavioral responses are completed too quickly for the underlying sensory processes to rely on estimation of neural firing rates over extended time windows. Theoretically, first-spike times could underlie such rapid responses, but direct evidence has been lacking. Such evidence has now been uncovered in the human somatosensory system. We discuss these findings and their potential generalization to other sensory modalities, and we consider some future challenges for the neuroscientific community.},
author = {VanRullen, Rufin and Guyonneau, Rudy and Thorpe, Simon J.},
doi = {10.1016/j.tins.2004.10.010},
file = {:home/epaxon/Documents/Mendeley Desktop/VanRullen, Guyonneau, Thorpe - 2005 - Spike times make sense.pdf:pdf},
isbn = {0166-2236},
issn = {01662236},
journal = {Trends in Neurosciences},
number = {1},
pages = {1--4},
pmid = {15626490},
title = {{Spike times make sense}},
volume = {28},
year = {2005}
}
@article{Vertes2010,
abstract = {Despite significant progress in our understanding of the brain at both microscopic and macroscopic scales, the mechanisms by which low-level neuronal behavior gives rise to high-level mental processes such as memory still remain unknown. In this paper, we assess the plausibility and quantify the performance of polychronization, a newly proposed mechanism of neuronal encoding, which has been suggested to underlie a wide range of cognitive phenomena. We then investigate the effect of network topology on the reliability with which input stimuli can be distinguished based on their encoding in the form of so-called polychronous groups or spatiotemporal patterns of spikes. We find that small-world networks perform an order of magnitude better than random ones, enabling reliable discrimination between inputs even when prompted by increasingly incomplete recall cues. Furthermore, we show that small-world architectures operate at significantly reduced energetic costs and that their memory capacity scales favorably with network size. Finally, we find that small-world topologies introduce biologically realistic constraints on the optimal input stimuli, favoring especially the topographic inputs known to exist in many cortical areas. Our results suggest that mammalian cortical networks, by virtue of being both small-world and topographically organized, seem particularly well-suited to information processing through polychronization. This article addresses the fundamental question of encoding in neuroscience. In particular, evidence is presented in support of an emerging model of neuronal encoding in the neocortex based on spatiotemporal patterns of spikes.},
author = {Vertes, Petra E and Duke, Thomas},
doi = {10.2976/1.3386761},
file = {:home/epaxon/Documents/Mendeley Desktop/Vertes, Duke - 2010 - Effect of network topology on neuronal encoding based on spatiotemporal patterns of spikes.pdf:pdf},
isbn = {1955-205X (Electronic)$\backslash$r1955-205X (Linking)},
issn = {1955-205X},
journal = {HFSP journal},
number = {3-4},
pages = {153--63},
pmid = {21119767},
title = {{Effect of network topology on neuronal encoding based on spatiotemporal patterns of spikes.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2929633{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {4},
year = {2010}
}
@article{Viriyopase2016,
abstract = {Oscillations of neuronal activity in different frequency ranges are thought to reflect important aspects of cortical network dynamics. Here, we investigate how various mechanisms that contribute to oscillations in neuronal networks may interact. We focus on networks with inhibitory, excitatory and electrical synapses, where the subnetwork of inhibitory interneurons alone can generate interneuron gamma oscillations (ING) and the interactions between interneurons and pyramidal cells allow for pyramidal-interneuron gamma oscillations (PING). What type of oscillation will such a network generate? We find that ING and PING oscillations compete: The mechanism generating the higher oscillation frequency "wins", it determines the frequency of the network oscillation and suppresses the other mechanism. For type-I interneurons, the network oscillation frequency is equal to or slightly above the higher of the ING and PING frequencies in corresponding reduced networks that can generate only either of them. If the interneurons belong to the type-II class, it is in between. In contrast to ING and PING, oscillations mediated by gap junctions and oscillations mediated by inhibitory synapses may cooperate or compete, depending on the type (I or II) of interneurons and the strengths of the electrical and chemical synapses. We support our computer simulations by a theoretical model that allows a full theoretical analysis of the main results. Our study suggests experimental approaches to decide to what extent oscillatory activity in networks of interacting excitatory and inhibitory neurons are dominated by ING or PING oscillations and of which class the participating interneurons are.},
author = {Viriyopase, Atthaphon and Memmesheimer, Raoul-Martin and Gielen, Stan},
doi = {10.1152/jn.00493.2015},
file = {:home/epaxon/Documents/Mendeley Desktop/Viriyopase, Memmesheimer, Gielen - 2016 - Cooperation and competition of gamma oscillation mechanisms.pdf:pdf},
isbn = {0022-3077},
issn = {0022-3077},
journal = {Journal of Neurophysiology},
number = {2},
pages = {232--251},
pmid = {26912589},
title = {{Cooperation and competition of gamma oscillation mechanisms}},
url = {http://jn.physiology.org/lookup/doi/10.1152/jn.00493.2015},
volume = {116},
year = {2016}
}
@article{VonDerMalsburg1999,
abstract = {In attempts to formulate a computational understanding of brain function,$\backslash$none of the fundamental concerns is the data structure by which the brain$\backslash$nrepresents information. For many decades, a conceptual framework has$\backslash$ndominated the thinking of both brain modelers and neurobiologists. That$\backslash$nframework is referred to here as "classical neural networks." It is well$\backslash$nsupported by experimental data, although it may be incomplete. A$\backslash$ncharacterization of this framework will be offered in the next section.$\backslash$n                                                    $\backslash$nDifficulties in modeling important functional aspects of the brain on the$\backslash$nbasis of classical neural networks alone have led to the recognition that$\backslash$nanother, general mechanism must be invoked to explain brain function.  That$\backslash$nmechanism I call "binding." Binding by neural signal synchrony had been$\backslash$nmentioned several times in the liter ature (Lege´ndy, 1970; Milner, 1974)$\backslash$nbefore it was fully formulated as a general phenomenon (von der Malsburg,$\backslash$n1981). Although experimental evidence for neural syn chrony was soon found,$\backslash$nthe idea was largely ignored for many years. Only recently has it become a$\backslash$ntopic of animated discussion. In what follows, I will summarize the nature$\backslash$nand the roots of the idea of binding, especially of temporal binding, and$\backslash$nwill discuss some of the objec tions raised against it.},
author = {{Von Der Malsburg}, C.},
doi = {10.1016/S0896-6273(00)80825-9},
file = {:home/epaxon/Documents/Mendeley Desktop/Von Der Malsburg - 1999 - The what and why of binding The modeler's perspective.pdf:pdf},
isbn = {0896-6273 (Print)},
issn = {08966273},
journal = {Neuron},
number = {1},
pages = {95--104},
pmid = {10677030},
title = {{The what and why of binding: The modeler's perspective}},
volume = {24},
year = {1999}
}
@article{Wang1990a,
abstract = {We design neural networks to learn, recognize and reproduce complex temporal sequence, with short-term memory (STM) modeled by units comprising recurrent excitatory connections between two neurons (a dual neuron model). The output of a neuron has graded values instead of binary ones. By applying the Hebbian learning rule at each synapse and a normalization rule among all synaptic weights of a neuron, we show that a certain quantity, called the input potential, increases monotonically with sequence presentation, and that the neuron can only be fired when its input signals are arranged in a specific sequence. These sequence-detecting neurons form the basis for our model of complex sequence recognition, which can tolerate distortions of the learned sequences. A recurrent network of two layers is provided for reproducing complex sequences.},
author = {Wang, Deliang and Arbib, Michael A.},
doi = {10.1109/5.58329},
file = {:home/epaxon/Documents/Mendeley Desktop/Wang, Arbib - 1990 - Complex Temporal Sequence Learning Based on Short-term Memory.pdf:pdf},
issn = {15582256},
journal = {Proceedings of the IEEE},
number = {9},
pages = {1536--1543},
title = {{Complex Temporal Sequence Learning Based on Short-term Memory}},
volume = {78},
year = {1990}
}
@article{Wang1990,
abstract = {Abstract We consider a randomly diluted higher-order network with noise, consisting of McCulloch-Pitts neurons that interact by Hebbian-type connections. For this model, exact dynamical equations are derived and solved for both parallel and random sequential ... $\backslash$n},
author = {Wang, L P and Pichler, E E and Ross, J},
doi = {10.1073/pnas.87.23.9467},
file = {:home/epaxon/Documents/Mendeley Desktop/Wang, Pichler, Ross - 1990 - Oscillations and chaos in neural networks an exactly solvable model.pdf:pdf},
isbn = {0027-8424 (Print) 0027-8424},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {December},
pages = {9467--9471},
pmid = {2251287},
title = {{Oscillations and chaos in neural networks: an exactly solvable model}},
volume = {87},
year = {1990}
}
@article{Wang1999,
abstract = {Based on the previous work of a number of authors, we discuss an important class of neural networks which we call multi-associative neural networks (MANNs) and which associate one pattern with multiple patterns. As a computationally efficient example of such networks, we describe a specific MANN, that is, a multi-associative, dynamically generated variant of the counterpropagation network (MCPN). As an application of MANNs, we design a general system that can learn and retrieve complex spatio-temporal sequences with any MANN. This system consists of comparator units, a parallel array of MANNs, and delayed feedback lines from the output of the system to the neural network layer. During learning, pairs of sequences of spatial patterns are presented to the system and the system learns-to associate patterns at successive times in sequence. During retrieving, a cue sequence, which may be obscured by spatial noise and temporal gaps, causes the system to output the stored spatio-temporal sequence. We prove analytically that this system is capable of learning and generating any spatio-temporal sequences within the maximum complexity determined by the number of embedded MANNs, with the maximum length and number of sequences determined by the memory capacity of the embedded MANNs. To demonstrate the applicability of this general system, we present an implementation using the MCPN. The system shows desirable properties such as fast and accurate learning and retrieving, and ability to store a large number of complex sequences consisting of nonorthogonal spatial patterns.},
author = {Wang, Lipo},
doi = {10.1109/3477.740167},
file = {:home/epaxon/Documents/Mendeley Desktop/Wang - 1999 - Multi-associative neural networks and their applications to learning and retrieving complex spatio-temporal sequences.pdf:pdf},
isbn = {1083-4419},
issn = {10834419},
journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
keywords = {Auto-associative,Hetero-associative,Multi-associative,Neural network,Noise,Spatio-temporal sequence},
number = {1},
pages = {73--82},
pmid = {18252281},
title = {{Multi-associative neural networks and their applications to learning and retrieving complex spatio-temporal sequences}},
volume = {29},
year = {1999}
}
@article{Wang2000,
abstract = {Autoassociations of spatio-temporal sequences have been discussed by a number of authors. We propose a mechanism for storing and retrieving pairs of spatio-temporal sequences with the network architecture of the standard bidirectional associative memory (BAM), thereby achieving hetero-associations of spatio-temporal sequences.},
author = {Wang, Lipo},
doi = {10.1109/72.883484},
file = {:home/epaxon/Documents/Mendeley Desktop/Wang - 2000 - Heteroassociations of spatio-temporal sequences with the bidirectional associative memory.pdf:pdf},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Associative memory,Bidirectional associative memory (BAM),Image sequence,Neural network},
number = {6},
pages = {1503--1505},
pmid = {18249876},
title = {{Heteroassociations of spatio-temporal sequences with the bidirectional associative memory}},
volume = {11},
year = {2000}
}
@article{Wang2016,
abstract = {Hippocampal place field sequences are supported by sensory cues and network internal mechanisms. In contrast, sharp-wave (SPW) sequences, theta sequences and episode-field sequences are internally generated. The relationship of these sequences to memory is unclear. SPW sequences have been shown to support learning and have been assumed to also support episodic memory. Conversely, we demonstrate these SPW sequences were present even after episodic memory in trained rats was impaired and after other internal sequences - episode-field and theta sequences - were eliminated. SPW sequences did not support memory despite continuing to 'replay' all task-related sequences - place-field and episode-field sequences. Sequence replay occurred selectively during a synchronous increase of population excitability -- SPWs. Similarly, theta sequences depended on the presence of repeated synchronized waves of excitability - theta oscillations. Thus, we suggest that either intermittent or rhythmic synchronized changes of excitability trigger sequential firing of neurons, which in turn supports learning and/or memory.},
author = {Wang, Yingxue and Roth, Zachary and Pastakova, Eva},
doi = {10.7554/eLife.20697},
file = {:home/epaxon/Documents/Mendeley Desktop/Wang, Roth, Pastakova - 2016 - Synchronized excitability in a network enables generation of internal neuronal sequences.pdf:pdf},
issn = {2050084X},
journal = {eLife},
number = {September2016},
pages = {1--15},
pmid = {27677848},
title = {{Synchronized excitability in a network enables generation of internal neuronal sequences}},
volume = {5},
year = {2016}
}
@article{Weber2016,
abstract = {A key problem in computational neuroscience is to find simple, tractable models that are nevertheless flexible enough to capture the response properties of real neurons. Here we examine the capabilities of recurrent point process models known as Poisson generalized linear models (GLMs). These models are defined by a set of linear filters, a point nonlinearity, and conditionally Poisson spiking. They have desirable statistical properties for fitting and have been widely used to analyze spike trains from electrophysiological recordings. However, the dynamical repertoire of GLMs has not been systematically compared to that of real neurons. Here we show that GLMs can reproduce a comprehensive suite of canonical neural response behaviors, including tonic and phasic spiking, bursting, spike rate adaptation, type I and type II excitation, and two forms of bistability. GLMs can also capture stimulus-dependent changes in spike timing precision and reliability that mimic those observed in real neurons, and can exhibit varying degrees of stochasticity, from virtually deterministic responses to greater-than-Poisson variability. These results show that Poisson GLMs can exhibit a wide range of dynamic spiking behaviors found in real neurons, making them well suited for qualitative dynamical as well as quantitative statistical studies of single-neuron and population response properties.},
archivePrefix = {arXiv},
arxivId = {1602.07389},
author = {Weber, Alison I and Pillow, Jonathan W.},
eprint = {1602.07389},
file = {:home/epaxon/Documents/Mendeley Desktop/Weber, Pillow - 2016 - Capturing the dynamical repertoire of single neurons with generalized linear models.pdf:pdf},
journal = {ArXiv},
keywords = {generalized linear model,glm,izhikevich model,point process,spike timing},
pages = {1--19},
title = {{Capturing the dynamical repertoire of single neurons with generalized linear models}},
url = {http://arxiv.org/abs/1602.07389},
year = {2016}
}
@article{Weisbuch1985,
author = {Weisbuch, G. and Fogelman-Souli{\'{e}}, F.},
file = {:home/epaxon/Documents/Mendeley Desktop/Weisbuch, Fogelman-Souli{\'{e}} - 1985 - Scaling laws for the attractors of Hopfield networks.pdf:pdf},
issn = {0302-072X},
journal = {Journal de Physique Lettres},
keywords = {e 46},
number = {14},
pages = {623--630},
title = {{Scaling laws for the attractors of Hopfield networks}},
url = {http://jphyslet.journaldephysique.org/articles/jphyslet/ref/1985/14/jphyslet{\_}1985{\_}{\_}46{\_}14{\_}623{\_}0/jphyslet{\_}1985{\_}{\_}46{\_}14{\_}623{\_}0.html},
volume = {46},
year = {1985}
}
@article{Weiss2007,
abstract = {Compressed sensing7,6is a recent setof mathematical results showing that sparse signals can be exactly reconstructed from a small number of linear measurements. Interestingly,for ideal sparse signals with no measurement noise, random measurements allow perfect reconstruction while measurements based on principal component analysis(PCA) or independent component analysis(ICA)do not. At the same time,for other signal and noise distributions,PCA and ICA can signiﬁcantly outperform random projections in terms of enabling reconstruction from a small number of measurements. In this paper we ask: given a training set typical of the signals we wish to measure,what are the optimal set of linear projections for compressed sensing We show that the optimal projections are in general not the principal components nor the independent components of the data,but rather aseemingly novel setof projections that capture what is still uncertain about the signal, given the training set. We also show that the projections onto the learned uncertain components may far outperform random projections. This is particularly true in the case of natural images, where random projections have vanishingly small signal to noise ratio as the number of pixels becomes large.},
author = {Weiss, Y. and Chang, H. S. and Freeman, W. T.},
file = {:home/epaxon/Documents/Mendeley Desktop/Weiss, Chang, Freeman - 2007 - Learning Compressed Sensing.pdf:pdf},
isbn = {9781605600864},
journal = {Learning},
number = {1},
title = {{Learning Compressed Sensing}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.69.2631},
year = {2007}
}
@article{Willwacher1976,
abstract = {It is shown that a convolution with certain reasonable receptive field (RF) profiles yields the exact partial derivatives of the retinal illuminance blurred to a specified degree. Arbitrary concatenations of such RF profiles yield again similar ones of higher order and for a greater degree of blurring. By replacing the illuminance with its third order jet extension we obtain position dependent geometries. It is shown how such a representation can function as the substrate for "point processors" computing geometrical features such as edge curvature. We obtain a clear dichotomy between local and multilocal visual routines. The terms of the truncated Taylor series representing the jets are partial derivatives whose corresponding RF profiles closely mimic the well known units in the primary visual cortex. Hence this description provides a novel means to understand and classify these units. Taking the receptive field outputs as the basic input data one may devise visual routines that compute geometric features on the basis of standard differential geometry exploiting the equivalence with the local jets (partial derivatives with respect to the space coordinates).},
author = {Willwacher, Gerd},
doi = {10.1007/BF00318371},
file = {:home/epaxon/Documents/Mendeley Desktop/Willwacher - 1976 - Capabilities of an associative storage system compared with the function of the brain.pdf:pdf},
isbn = {1432-0770},
issn = {0340-1200},
journal = {Biological Cybernetics},
pages = {181--198},
pmid = {3567240},
title = {{Capabilities of an associative storage system compared with the function of the brain}},
url = {http://link.springer.com/10.1007/BF00318371},
volume = {24},
year = {1976}
}
@article{Willwacher1982,
author = {Willwacher, Gerd},
doi = {10.1007/BF00318371},
file = {:home/epaxon/Documents/Mendeley Desktop/Willwacher - 1982 - Storage of a Temporal Pattern Sequence in a Network.pdf:pdf},
issn = {0340-1200},
journal = {Biological Cybernetics},
pages = {115--126},
pmid = {1000253956},
title = {{Storage of a Temporal Pattern Sequence in a Network}},
volume = {43},
year = {1982}
}
@article{Xu2014,
abstract = {This paper investigates the problem of the dynamic behaviors of a class of complex-valued neural networks with mixed time delays. Some sufficient conditions for assuring the existence, uniqueness and exponential stability of the equilibrium point of the system are derived using the vector Lyapunov function method, homeomorphism mapping lemma and the matrix theory. The obtained results not only are convenient to check, but also generalize the previously published corresponding results. A numerical example is used to show the effectiveness of the obtained results. ?? 2013 Elsevier B.V.},
author = {Xu, Xiaohui and Zhang, Jiye and Shi, Jizhong},
doi = {10.1016/j.neucom.2013.08.014},
file = {:home/epaxon/Documents/Mendeley Desktop/Xu, Zhang, Shi - 2014 - Exponential stability of complex-valued neural networks with mixed delays.pdf:pdf},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Complex-valued,Exponential stability,Mixed delays,Neural networks,Vector Lyapunov function},
pages = {483--490},
publisher = {Elsevier},
title = {{Exponential stability of complex-valued neural networks with mixed delays}},
url = {http://dx.doi.org/10.1016/j.neucom.2013.08.014},
volume = {128},
year = {2014}
}
@misc{Xu1995,
author = {Xu, Zb and Kwong, Cp},
booktitle = {Journal of mathematical analysis and applications},
file = {:home/epaxon/Documents/Mendeley Desktop/Xu, Kwong - 1995 - Global convergence and asymptotic stability of asymmetric Hopfield neural networks.pdf:pdf},
pages = {405--427},
title = {{Global convergence and asymptotic stability of asymmetric Hopfield neural networks}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0022247X85711389},
volume = {191},
year = {1995}
}
@article{Xu1996,
abstract = {The Hopfield-type networks with asymmetric interconnections are studied from the standpoint of taking them as computational models. Two fundamental properties, feasibility and reliability, of the networks related to their use are established with a newly-developed convergence principle and a classification theory on energy functions. The convergence principle generalizes that previously known for symmetric networks and underlies the feasibility. The classification theory, which categorizes the traditional energy functions into regular, normal and complete ones according to their roles played in connection with the corresponding networks, implies that the reliability and high efficiency of the networks can follow respectively from the regularity and the normality of the corresponding energy functions. The theories developed have been applied to solve a classical NP-hard graph theory problem: finding the maximal independent set of a graph. Simulations demonstrate that the algorithms deduced from the asymmetric theories outperform those deduced from the symmetric theory.},
author = {Xu, Zong Ben and Hu, Guo Qing and Kwong, Chung Ping},
doi = {10.1016/0893-6080(95)00114-X},
file = {:home/epaxon/Documents/Mendeley Desktop/Xu, Hu, Kwong - 1996 - Asymmetric Hopfield-type networks Theory and applications.pdf:pdf},
isbn = {08936080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Asymmetric Hopfield-type networks,Classification theory on energy functions,Combinatorial optimization,Convergence principle,Maximal independent set problem,Regular and normal correspondence},
number = {3},
pages = {483--501},
title = {{Asymmetric Hopfield-type networks: Theory and applications}},
volume = {9},
year = {1996}
}
@article{Young1997,
author = {Young, Susan S and Scott, Peter D and Nasrabadi, Nasser M and Member, Senior},
file = {:home/epaxon/Documents/Mendeley Desktop/Young et al. - 1997 - Multilayer Hopfield Neural Network.pdf:pdf},
number = {3},
pages = {357--372},
title = {{Multilayer Hopfield Neural Network}},
volume = {6},
year = {1997}
}
@article{Yu2013,
abstract = {A new learning rule (Precise-Spike-Driven (PSD) Synaptic Plasticity) is proposed for processing and memorizing spatiotemporal patterns. PSD is a supervised learning rule that is analytically derived from the traditional Widrow-Hoff rule and can be used to train neurons to associate an input spatiotemporal spike pattern with a desired spike train. Synaptic adaptation is driven by the error between the desired and the actual output spikes, with positive errors causing long-term potentiation and negative errors causing long-term depression. The amount of modification is proportional to an eligibility trace that is triggered by afferent spikes. The PSD rule is both computationally efficient and biologically plausible. The properties of this learning rule are investigated extensively through experimental simulations, including its learning performance, its generality to different neuron models, its robustness against noisy conditions, its memory capacity, and the effects of its learning parameters. Experimental results show that the PSD rule is capable of spatiotemporal pattern classification, and can even outperform a well studied benchmark algorithm with the proposed relative confidence criterion. The PSD rule is further validated on a practical example of an optical character recognition problem. The results again show that it can achieve a good recognition performance with a proper encoding. Finally, a detailed discussion is provided about the PSD rule and several related algorithms including tempotron, SPAN, Chronotron and ReSuMe.},
author = {Yu, Qiang and Tang, Huajin and Tan, Kay Chen and Li, Haizhou},
doi = {10.1371/journal.pone.0078318},
file = {:home/epaxon/Documents/Mendeley Desktop/Yu et al. - 2013 - Precise-Spike-Driven synaptic plasticity Learning hetero-association of spatiotemporal spike patterns.pdf:pdf},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {11},
pages = {1--16},
pmid = {24223789},
title = {{Precise-Spike-Driven synaptic plasticity: Learning hetero-association of spatiotemporal spike patterns}},
volume = {8},
year = {2013}
}
@article{Zhou1993,
abstract = {The complex Hopfield model, i.e., with the complex-valued interconnections and the complex-valued vectors, is discussed. Computer simulations in comparison with the Hopfield model are given, which show that the complex Hopfield model can recall the entire complex-valued stored vector (including both the real part and the imaginary part) by only its real part or its imaginary part with high probability ({\textgreater} 95{\%} according to the calculations). The second-order complex Hopfield model, and its computer simulations in comparison with the second-order Hopfield model are also presented.},
author = {Zhou, Changhe and Liu, Liren},
doi = {10.1016/0030-4018(93)90637-k},
file = {:home/epaxon/Documents/Mendeley Desktop/Zhou, Liu - 1993 - The complex Hopfield model.pdf:pdf},
isbn = {0030-4018},
journal = {Optics Communications},
keywords = {NEURAL NETWORKS,Optics,PROCESSOR},
pages = {29--32},
title = {{The complex Hopfield model}},
volume = {103},
year = {1993}
}
@article{Zhou2007b,
abstract = {The present paper is mainly concerned with the issues of synchronization dynamics of complex delayed dynamical networks with impulsive effects. A general model of complex delayed dynamical networks with impulsive effects is formulated, which can well describe practical architectures of more realistic complex networks related to impulsive effects. Based on impulsive stability theory on delayed dynamical systems, some simple but less conservative criterion are derived for global synchronization of such dynamical network. It is shown that synchronization of the networks is heavily dependent on impulsive effects of connecting configuration in the networks. Furthermore, the theoretical results are applied to a typical SF network composing of impulsive coupled chaotic delayed Hopfield neural network nodes, and are also illustrated by numerical simulations. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
author = {Zhou, Jin and Xiang, Lan and Liu, Zengrong},
doi = {10.1016/j.physa.2007.05.060},
file = {:home/epaxon/Documents/Mendeley Desktop/Zhou, Xiang, Liu - 2007 - Synchronization in complex delayed dynamical networks with impulsive effects.pdf:pdf},
isbn = {0378-4371},
issn = {03784371},
journal = {Physica A: Statistical Mechanics and its Applications},
keywords = {Chaotic delayed Hopfield neural network,Complex dynamical networks,Global synchronization,Impulsive effects,Scale-free network,Time-delays},
number = {2},
pages = {684--692},
title = {{Synchronization in complex delayed dynamical networks with impulsive effects}},
volume = {384},
year = {2007}
}
@article{Zhou2012,
abstract = {In many sensory systems, the latency of spike responses of individual neurons is found to be tuned for stimulus features and proposed to be used as a coding strategy. Whether the spike latency tuning is simply relayed along sensory ascending pathways or generated by local circuits remains unclear. Here, in vivo whole-cell recordings from rat auditory cortical neurons in layer 4 revealed that the onset latency of their aggregate thalamic input exhibited nearly flat tuning for sound frequency, whereas their spike latency tuning was much sharper with a broadly expanded dynamic range. This suggests that the spike latency tuning is not simply inherited from the thalamus, but can be largely reconstructed by local circuits in the cortex. Dissecting of thalamocortical circuits and neural modeling further revealed that broadly tuned intracortical inhibition prolongs the integration time for spike generation preferentially at off-optimal frequencies, while sharply tuned intracortical excitation shortens it selectively at the optimal frequency. Such push and pull mechanisms mediated likely by feedforward excitatory and inhibitory inputs respectively greatly sharpen the spike latency tuning and expand its dynamic range. The modulation of integration time by thalamocortical-like circuits may represent an efficient strategy for converting information spatially coded in synaptic strength to temporal representation.},
author = {Zhou, Y. and Mesik, L. and Sun, Y. J. and Liang, F. and Xiao, Z. and Tao, H. W. and Zhang, L. I.},
doi = {10.1523/JNEUROSCI.1384-12.2012},
file = {:home/epaxon/Documents/Mendeley Desktop/Zhou et al. - 2012 - Generation of Spike Latency Tuning by Thalamocortical Circuits in Auditory Cortex.pdf:pdf},
isbn = {1529-2401 (Electronic)$\backslash$r0270-6474 (Linking)},
issn = {0270-6474},
journal = {Journal of Neuroscience},
number = {29},
pages = {9969--9980},
pmid = {22815511},
title = {{Generation of Spike Latency Tuning by Thalamocortical Circuits in Auditory Cortex}},
url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.1384-12.2012},
volume = {32},
year = {2012}
}
