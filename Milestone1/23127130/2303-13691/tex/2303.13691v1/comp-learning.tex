\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
%\usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
%\usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{natbib}
\usepackage{fullpage}

\graphicspath{{./}}


\usepackage{lmodern}




\title{
Learning and generalization of compositional descriptions of visual scenes
%Teaching a neural network "what" and "where" and more
%Resonator circuits: recurrent neural networks for inferring compositional structure in distributed representations
%Structured vector representations for parsing complex scenes
}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.
\usepackage{authblk}
\author[1,2]{E. Paxon Frady}
\author[2]{Spencer Kent}
\author[2]{Quinn Tran} 
\author[2]{Pentti Kanerva}
\author[2]{Bruno A. Olshausen}
\author[1,2]{Friedrich T. Sommer}

\affil[1]{Intel Labs}
\affil[2]{Redwood Center for Theoretical Neuroscience, UC Berkeley}

% \author{
%   E. Paxon Frady \\
%   Redwood Center for \\
%   Theoretical Neuroscience \\
%   UC Berkeley\\
%   Berkeley, CA 94720 \\
%   \texttt{epaxon@berkeley.edu} \\
%   \And
%   Spencer Kent \\
%   Redwood Center for \\
%   Theoretical Neuroscience \\
%   UC Berkeley\\
%   Berkeley, CA 94720 \\
%   \texttt{spencer.kent@berkeley.edu} \\
%   \And
%   Quinn Tran \\
%   Redwood Center for \\
%   Theoretical Neuroscience \\
%   UC Berkeley\\
%   Berkeley, CA 94720 \\
%   \texttt{ssquinntran@berkeley.edu} \\
%   \And
%   Pentti Kanerva \\
%   Redwood Center for \\
%   Theoretical Neuroscience \\
%   UC Berkeley\\
%   Berkeley, CA 94720 \\
%   \texttt{pkanerva@csli.stanford.edu} \\
%   \And 
%   Bruno A. Olshausen \\
%   Redwood Center for \\
%   Theoretical Neuroscience \\
%   UC Berkeley\\
%   Berkeley, CA 94720 \\
%   \texttt{baolshausen@berkeley.edu} \\
%   \And 
%   Friedrich T. Sommer\\
%   Redwood Center for \\
%   Theoretical Neuroscience \\
%   UC Berkeley\\
%   Berkeley, CA 94720 \\
%   \texttt{fsommer@berkeley.edu} \\
% }

\begin{document}

\maketitle

\begin{abstract}
%For neural networks to understand complex scenes and generalize, they must be able to process \emph{compositional representations}. 
Complex visual scenes that are composed of multiple objects, each with attributes, such as object name, location, pose, color, etc., are challenging to describe in order to train neural networks.
Usually, deep learning networks are trained supervised by categorical scene descriptions.
The common categorical description of a scene contains the names of individual objects but lacks information about other attributes. 
Here, we use distributed representations of object attributes and vector operations in a vector symbolic architecture to create a full compositional description of a scene in a high-dimensional vector.  
To control the scene composition, we use artificial images composed of multiple, translated and colored MNIST digits.
In contrast to learning category labels, here we train deep neural networks to output the full compositional vector description of an input image. 
The output of the deep network can then be interpreted by a VSA resonator network, to extract object identity or other properties of indiviual objects.
%, which selects one object in the scene and breaks it into its generative factors: shape, location and color. 
We evaluate the performance and generalization properties of the system on randomly generated scenes.
Specifically, we show that the network is able to learn the task and generalize to unseen seen digit shapes and scene configurations. Further, the generalisation ability of the trained model is limited. For example, with a gap in the training data, like an object not shown in a particular image location during training, the learning does not automatically fill this gap.

%Resonator circuits are recurrent neural networks that solve the CSP by searching through many guesses simultaneously using the principle of superposition. They iterate until converging to a solution that factors the input compositional vector representation into its components. 
%We illustrate how the combination of deep learning networks and resonator circuits can be used to describe and factor a scene of colored MNIST digits. 

\end{abstract}

\section{Introduction}
The structure of visual scenes can be described on different levels. For example, object recognition provides an abstract symbolic description of a scene, by mapping an image to category labels, describing the objects present in a scene. A limitation of object categorization is that the information about object attributes is implicit, not explicit. Other forms of scene descriptions attempt to capture the "what" and "where" by relating categoric labels to the image structure. For example, pixels (in scene segmentation) or bounding boxes (in object localization) can be assigned to different objects that compose a scene.  Here we present a model that learns to describe the semantic structure of scenes at greater detail using structured symbolic distributed representations.   

Deep learning networks trained by supervised learning with object labels reach unprecedented levels of performance in object classification when trained with large amounts of data.  Here we propose to describe a scene in terms of attributes of objects, including not only object category, but also attributes such as position, pose, and color. The scene description is represented by a high-dimensional vector using syntax rules from Vector Symbolic Architecture \citet{Plate2003, Kanerva2009}, a class of connectionist models for cognition that have been introduced into deep learning earlier \citet{Eliasmith2012}. The resulting vector is then used as training input in a deep learning network. To interpret an image, the trained network maps the input to a distributed vector representing its semantic compositional description. This vector can be queried, or all of its components disentangled one-by-one, through subsequent processing in a recurrent VSA resonator network \citep{frady_2020_resonator}
 
We perform simulation experiments on synthetic data, scenes composed of multiple MNIST digits, on how a network can be trained and queried concerning the attributes of objects in a given input image. The learning generalizes in several ways. First, the network can analyze scenes composed of MNIST digits withheld in the test set not shown during training. 
Second, the network can generalize to scenes of digits where the combination of digits shown in the scene was not shown in the training set.
Third, the network can generalize to scenes with more or fewer individual objects than what was used in the training set. 
Finally, however, the network does not generalize or transfer knowledge across the conjunction of factors that describe a scene. 
In essence, we show that if a network is not trained with a particular conjunction of factors, such as it is never shown a `7' in the `bottom-left' during training, then during testing it will not output `7' in the `bottom-left' even though it has seen 7s before and it has seen other digits in the bottom left. 
%Second, and most importantly, learning generalizes with respect to the compositional combinatorics of the scenes. The second type of generalization is crucial for our approach to be feasible even if the compositional combinatorics is vast and cannot be exhaustively covered by training data. 

%The presented model has strong implications for neuroscience. It suggests a way how brains might be able to learn to interpret subsymbolic sensory input with symbolic semantic explanations. The big challenge with tasks like scene interpretation is that the number of all possible semantic descriptions explodes with the number of scene components. Therefore the brain must generalize knowledge from scenes it has been been taught about to scenes it never encountered before. Our demonstration on synthetic data shows that this type of generalization is achieved by the proposed model. 

%captures the classic problem in perception of disentangling ``what'' and ``where'', the disassociation of an object’s shape with properties of its surface such as color or texture. These compositions create a combinatoric set of configurations, but the perceptual system is easily capable of factoring out the individual components. This leads to many perceptual illusions (Adelson).

%Neural networks have shown many recent advances in mimicking human perception. While some suggest that compositional representations are used in deep neural networks, the ability of such networks to manipulate compositional structure is not well understood in general.  
%The question of how a high dimensional neural population could encode and compute with compound semantic concepts has been partially addressed by models of Vector Symbolic Algebra (VSA) (often also referred to as High-dimensional Computing) . Recent work  shows how to combine VSA representations and deep learning networks to perform complex cognitive tasks. 


% One issue in VSA computation is that a compound representation cannot be directly factored into components. Previous approaches required enumerating all possible component combinations to determine which are present in the compound vector. This made many VSA applications impractical.
% In this work, we detail an algorithm that makes computation with compound distributed representations significantly more practical, because it does not need to directly enumerate all combinations of components. Rather, the principal of superposition allows a set of recurrent neural networks to simultaneously check multiple guesses for the component's of a compound vector. This network iterates, stochastically checking different component combinations. Through positive-feedback, the correct components are amplified and the network reaches an equilibrium. 

%Our algorithm can be cast as a set of simple recurrent neural circuits we call resonators. 

%We take inspiration from Arathorn’s Map-seeking Circuit and the insight that it is useful in solving this particular combinatorial optimization problem to maintain a superposition of hypotheses that are iteratively refined over time through competition. Our approach moves beyond this prior work by using high dimensional distributed representations within the larger algebraic system of a Vector Symbolic Algebra. 

%We also use a neural network to infer these representations for simple visual scenes which is an effective method of dealing with correlations and ambiguity in the input space.
%We believe this work takes an important step towards realizing the potential of symbolic computing within the paradigm of neural networks.



%To perform a complex behavior such as driving, the human perceptual system builds, maintains, and successively updates a model of the environment. Such a working memory of the environment state is likely to be compositional, so it can be selectively queried before making a decision. For example, before changing into a lane to the left, the working memory should be queried whether there is space in the left lane.  
%Compositional representations are also useful, because they can express an exponentially increasing set of variable combinations in a meaningful data structure, such as the locations, trajectories and identities of objects in the environment. 
%Motor and planning systems can independently access information about the environment to carry out multiple tasks in parallel. 
%Previous work in robotics and control systems, such as those for simultaneous localization and mapping (SLAM), use compositional representations to describe and interact with the world.
% flesh out: (so little room...)
%However, traditional architectures suffer from issues with computational complexity, approximate computation, inference, data association, and flexible adaptation to the environment \citep{Aulinas2008}.
 
% Different systems may request access to different variables encoding the environment for executing parallel tasks. For instance, the steering system may only need access to lane information, while the obstacle avoidance may want access to information on pedestrians and other cars. 
% Many parallel systems may need access to such representation in order to perform different computations, such as obstacle avoidance and navigational planning. 

%Neural networks have recently shown promise in performing complex human-like behaviors, and many are applying such networks to autonomous driving. Learning algorithms and distributed parallel computations present solutions to many challenges faced by traditional robotics control algorithms \citep{Cadena2016}. 
%However, the learning strategy is unclear as well as how to build a system that is sensible and understandable -- something necessary for gaining trust in autonomous driving.
%However, traditional neural network architectures cannot produce truly compositional representations. As a consequence, most current network architectures for autonomous driving are purely reactive and their decision making process is a black box. 



\section{Background}
Based on frameworks of computing with high-dimensional vectors \citep{Plate2003, Kanerva2009}, our group has recently made progress to enable neural networks to learn and manipulate compositional structure. 
Our recent theoretical work \citep{frady_2018sequence} shows that several frameworks, collectively called \emph{Vector Symbolic Architectures} (VSA) \citep{Gayler2003}, are mathematically related and have universal properties, such as working memory capacity.
VSAs are formal, comprehensible frameworks for representing information and computing with random high-dimensional vectors, akin to neural population activity. The frameworks include operations, such as superposition (e.g. add), variable binding (e.g. componentwise multiply) and permutation of coordinates, that enables the expression of compositional structure in a distributed vector representation. 
Further, VSAs can be used to express symbolic computations in a neural network, which allows integration of deep learning networks to create cognitive agents \citep{Eliasmith2012}. The essential properties of VSAs are:
\begin{itemize}
\item
Symbols and compositions are represented with high-dimensional vectors (neural population activity). 
\item
Elementary symbols are chosen randomly and stored in a codebook (matrix of synaptic weights). In high-dimensions, random vectors are close to orthogonal with high probability.
\item
Multiple symbols can be represented in superposition through vector addition. However, superposition is limited by crosstalk noise.
\item
The binding operation forms compositions of symbols through vector multiplication. This operation is invertible. However, given a pair of bound vectors, one is needed to extract the other.
\end{itemize}

The issue of what and where has been heavily studied in the machine-learning literature. Approaches include forming bounding-boxes around objects or classifying each pixel as part of an object. While these approaches have shown some success in dealing with natural images, it is not fully clear how to extend them further. 

%Using a pixel mask to route information through a neural network is now becoming popular for spatial tasks (cite: visual transformer paper). This type of multiplicative interaction has relations to the binding operations used in this paper. The VSA algebra allows one to push this idea further, and to generalize to other types of compositional structures besides just what and where.  

Compositions are critically important to VSAs.  However, a fundamental problem arises if multiple components of a composition are unknown. The typical solution requires enumerating the entire set of combinations, which leads to problems with scaling. 

% The problem of decomposing vectors in VSA is directly related to a class of problems known as \emph{constraint satisfaction problems} (CSP). It is typically the case that these problems are NP-hard/NP-complete, and requires a brute force search over the combinatoric solution space. 

% Many neural network approaches have been taken to solve CSPs. 
% Some solutions do require enumeration of the combinatoric solution space \citep{}.
% Other solutions use an Energy landscape formalism \citep{Jonke2016}.

To solve such combinatoric inference problems, we have developed a recurrent neural network architecture called the \emph{resonator network} based on VSA principles. 
The VSA resonator network is related to and generalizes map-seeking circuits \citep{Arathorn2002} and dynamic routing \citep{Olshausen1995}. It iterates and converges to the best solution by using the principle of superposition to search over many guess combinations in parallel \citep{Frady2020}.

%, and can even incorporate Bayesian priors. 


\section{Methods}
\subsection{Structured compositional vectors for describing complex scenes}
To describe a complex scene, such as the one shown in Figure \ref{fig:comp_learn} (left panel), the VSA frameworks provide two essential operations: binding and superposition. The binding operation, elementwise multiply ($\odot$), can attach symbols together into a compound representation, allowing one to associate properties to the same object. Superposition, vector addition ($+$), allows for multiple objects to be simultaneously represented. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{composition_learning-180328.png}
    \caption{A scene is generated with 1-3 random MNIST digits with independent colors and locations.A multi-layer feed-forward network was trained to directly transform an input image into a structured compositional vector representation of the scene. The compositional representation can be used for reasoning, transforms and scene understanding. However, the components are still entangled and need to be inferred. }
    \label{fig:comp_learn}
\end{figure}

Each individual component can be enumerated and described by a random base vector. The set of base vectors describing all possible properties within a particular class is stored in a matrix called the \emph{codebook}. In this case, there are 7 colors, 10 numbers, 3 y-positions, and 3 x-positions, giving 630 combinations. These have corresponding codebooks of randomly chosen -1 or +1: $\mathbf{C} : N \times 7$, $\mathbf{D} : N \times 10$, $\mathbf{V} : N \times 3$, and $\mathbf{H} : N \times 3$, where $N=1000$ is the number of neurons. 

Each object in a complex scene can then be described with a compound vector. The vector for color, digit identity, and location are bound together through multiplication to create the compound vector. The scene vector $\mathbf{s}$ is given by summing the compound vector for each object, and acts as a compositional representation for the entire scene:
\begin{equation}
    \mathbf{s} = \sum_{i}^{objects} \mathbf{C}_{color_i} \odot \mathbf{D}_{digit_i} \odot \mathbf{V}_{y-pos_i} \odot \mathbf{H}_{x-pos_i}
    \label{eq:scene}
\end{equation}


\subsection{Learning to output structured vector representations}
We demonstrate the resonator in conjunction with a neural network that learns to assign compound vectors to images of simple scenes. 
The neural network is a multi-layer perceptron with two hidden layers.
A scene is generated by choosing a random combination of colors, MNIST digits, and locations, and the corresponding compound vector acts as a description of the scene.
Through supervised training, the network learns to map the generated image into the compound vector that describes the scene. 
%which, given images, learns to infer properties of the scene such as an object’s shape, color, scale, rotation, and position. 

The output of the network is a compound distributed representation of the scene, which the VSA resonator network can factor into its components. 
Due to correlations and ambiguity in the space of images, the network may not be able to produce precisely the ground truth compound vector. However, the nature of distributed vectors in vector symbolic architectures allows for noise tolerance.

\subsection{Factoring compositional vectors with resonator networks}
%The fully distributed representations in VSAs are, by design, hard to inspect directly. 
In order to determine the components of a compound vector, we must search for combinations of vectors in the codebooks that produce a vector similar to our compound vector. If we were using an associative memory, such as a Hopfield Network
or a Sparse Distributed Memory, to do the search, then we would have to store all possible combinations of vectors in the memory. This in many cases would be impractical. 

Our algorithm allows one to store only the codebooks, but still factor compound vectors. To infer one of the components of a compound vector, we use the principle of superposition to simultaneously check multiple guesses. For each component class, we instantiate a \emph{resonator module}: $\mathbf{\hat{c}}(t)$, $\mathbf{\hat{d}}(t)$, $\mathbf{\hat{v}}(t)$, and $\mathbf{\hat{h}}(t)$, which are high-dimensional vectors that act as an estimate of the factor's parameters. The resonator modules are connected into a network based on the generative model of the scene $\mathbf{s}$ (\ref{eq:scene}). The full dynamics for VSA resonator network for such scenes is given by:
% \begin{align}
% \begin{split}
% \mathbf{\hat{c}}(t) &= f \left( \mathbf{C}\mathbf{C}^{\top} \left( \mathbf{s} \odot \mathbf{\hat{d}}(t-1)^{-1} \odot \mathbf{\hat{v}}(t-1)^{-1} \odot \mathbf{\hat{h}}(t-1)^{-1} \right) \right) \\
% \mathbf{\hat{d}}(t) &= f \left( \mathbf{D}\mathbf{D}^{\top} \left( \mathbf{s} \odot \mathbf{\hat{c}}(t-1)^{-1} \odot \mathbf{\hat{v}}(t-1)^{-1} \odot \mathbf{\hat{h}}(t-1)^{-1} \right) \right) \\
% \mathbf{\hat{v}}(t) &= f \left( \mathbf{V}\mathbf{V}^{\top} \left( \mathbf{s} \odot \mathbf{\hat{d}}(t-1)^{-1} \odot \mathbf{\hat{c}}(t-1)^{-1} \odot \mathbf{\hat{h}}(t-1)^{-1} \right) \right) \\
% \mathbf{\hat{h}}(t) &= f \left( \mathbf{H}\mathbf{H}^{\top} \left( \mathbf{s} \odot \mathbf{\hat{d}}(t-1)^{-1} \odot \mathbf{\hat{v}}(t-1)^{-1} \odot \mathbf{\hat{c}}(t-1)^{-1} \right) \right) \\
% \end{split}
% \label{eqn:resonator}
% \end{align}
\begin{align}
\begin{split}
\mathbf{\hat{c}}(t+1) &= f \left( \mathbf{C}\mathbf{C}^{\top} \left( \mathbf{s} \odot \mathbf{\hat{d}}(t) \odot \mathbf{\hat{v}}(t) \odot \mathbf{\hat{h}}(t) \right) \right) \\
\mathbf{\hat{d}}(t+1) &= f \left( \mathbf{D}\mathbf{D}^{\top} \left( \mathbf{s} \odot \mathbf{\hat{c}}(t) \odot \mathbf{\hat{v}}(t) \odot \mathbf{\hat{h}}(t) \right) \right) \\
\mathbf{\hat{v}}(t+1) &= f \left( \mathbf{V}\mathbf{V}^{\top} \left( \mathbf{s} \odot \mathbf{\hat{d}}(t) \odot \mathbf{\hat{c}}(t) \odot \mathbf{\hat{h}}(t) \right) \right) \\
\mathbf{\hat{h}}(t+1) &= f \left( \mathbf{H}\mathbf{H}^{\top} \left( \mathbf{s} \odot \mathbf{\hat{d}}(t) \odot \mathbf{\hat{v}}(t) \odot \mathbf{\hat{c}}(t) \right) \right) \\
\end{split}
\label{eqn:resonator}
\end{align}
where $f$ is the sign function or normalization. 

The VSA resonator network can be visualized in Figure \ref{fig:res_circuit}. Each resonator module can be initialized randomly. The activity state of each resonator represents a set of guesses for the particular component. Superposition allows for multiple guesses to be represented simultaneously in a high-dimensional vector, but crosstalk noise increases with more guesses in superposition. The guesses from the other resonators are used to infer the component from the scene vector, $\mathbf{s}$ (Fig. \ref{fig:res_circuit} blue box). After the inference step, the guesses are \emph{cleaned-up} by the codebooks (Fig. \ref{fig:res_circuit} green box). This provides better guesses each iteration, which in turn reduces the effect of crosstalk noise. The correct decomposition of the compound vector then emerges through a positive feedback cycle -- the correct solution \emph{resonates}, and the system converges to a stable point.
Once a stable point is reached, the network outputs can be decoded into the properties of an individual object in the scene. 
Finally, to anlayze multiple objects, the previous outputs are ``explained away'' by subtracting them from the input vector $\mathbf{s}$. The network is reset and run again until convergence. This process can be repeated to deal with different numbers of objects in the scene. 
Generally once all previous outputs are explained away, a threshold on the total energy remaining in the input vector can be used to interpret whether all of the objects in the scene were detected. This can be used to halt the analysis of multiple objects, when the total number of objects is unknown.

%This has significant implications for any model which must use compound distributed representations in conjunction with an associative memory.

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{resonator_circuit-multi_mnist-converge-s-180517.png}
    \caption{The output of the deep network $\mathbf{s}$ is input into the VSA resonator network, and the system iterates until convergence. Each resonator module can represent multiple guesses simultaneously. Each module relies on the guesses from other modules to try and infer the correct component (blue box). The resonator modules then \emph{clean-up} their individual guesses to produce a smaller range of guesses in the next iteration. The resonator network converges to a solution that describes the components of one of the objects in the scene. The state of each resonator module each time-step is visualized as a heatmap, each row is an iteration and each column is one possible component. The component with the largest weight is chosen as the output (top of each panel; yellow). The object is then explained away by subtracting the networks’s guess from the scene vector, $\mathbf{s}$. The resonator network is reset and converges to another solution, which describes a different object in the scene. This is repeated until all of the objects in the scene are recognized.
}
    \label{fig:res_circuit}
\end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.7\textwidth]{resonator_converge-multi_mnist-180328.png}
%     \caption{
% }
%     \label{fig:res_converge}
% \end{figure}


\section{Results}
\subsection{Extracting the components of a complex scene}
We generated synthetic images for a training set, which can contain 1 to 3 MNIST digits in different locations with independent colors. These images were provided as input to a deep neural network, which is trained to output the structured vector representation of the scene, $\mathbf{s}$. 
The scene vector is then input into the resonator network, and the resonator network iterates until convergence (Fig. \ref{fig:res_circuit}).  


The resonator network will randomly choose one of the objects and decompose the object into its component parts. The network converges to a solution, which describes the components of one of the objects in the scene. The output can then be \emph{explained away} from the scene vector by reconstructing the compound vector and subtracting it from $\mathbf{s}$. With the first output explained away, the resonator network is reset. It will choose another object in the scene to decompose, and converge. This process can be repeated until all of the objects in the scene are explained away. This procedure can create a description of all objects in the scene and their components.

\subsection{Performance evaluation}
We validated performance by looking at both the output of the deep network and the output of the resonator network. 
We separated scenes with one, two or three objects, and looked at performance in each of these conditions. 

The deep net's output was compared to the ground truth vector for the scene, this is the ``GT Similarity'' (Fig. \ref{fig:performance} left panels). The closer the output is to 1, the better the performance of the deep network.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{performance-multi_mnist-180328.png}
    \caption{The output of the deep network is compared to the ground-truth scene vector based on its cosine similarity (GT Similarity) for all images in the test set (left panels). The output of the resonator is accurate if all four extracted components correctly match one of the objects in the scene. The performance of the resonator is compared conditional on the quality of the output in the deep network (middle panels). We run the resonator three times. The black lines show the system correctly identifying all objects in the scene in as few attempts as necessary. The red and cyan lines show imperfect performance. Legend: digits correct / number of runs. The overall performance is shown in the right panels.
}
    \label{fig:performance}
\end{figure}

The resonator's output was evaluated as correct if all four components (color, digit, x-location, y-location) matched one of the ground truth objects in the scene. If even one component was incorrect, then the output would be considered incorrect. Note that in these settings, if the ground truth vector for one object was given as input to the resonator network, it will extract the correct identity and other properties virtually 100\% of the time, as the network is well within the regime of its operational capacity \citep{kent_2020resonator}.

In our evaluation experiment for every input scene, we ran the resonator network three times, and subtracted the output from the scene vector between each reset. We ran three times regardless of the number of objects in the scene, however the number of objects can be easily inferred based on the output of the deep network. This information could be used to decide how many times the resonator should be reset. In these experiments, running the resonator network three times when there is only one object allows the system to make three guesses about the object. 

We evaluated the performance of the resonator network conditional on the output of the deep network (Fig. \ref{fig:performance} middle panels), as well as the overall performance of the full system (Fig. \ref{fig:performance} right panels). If the deep network output is close to the ground truth, then the resonator network is more likely to be correct. 
We find that our algorithm is able to correctly infer the parameters of these scenes even when the vector-symbolic representations are quite far from the ground truth.

When there is only one object in the scene, the performance of the entire system is above $90\%$ (Fig. \ref{fig:performance} top, black), comparable to standard deep network performance on MNIST. When there are two objects, the VSA resonator network guesses both objects correctly nearly $80\%$ of the time in the first two runs (Fig. \ref{fig:performance} middle, black). With three objects in the scene, the system accurately extracts all three objects about $50\%$ of the time in three runs (Fig. \ref{fig:performance} bottom, black), and gets at least two out of three over $90\%$ of the time (Fig. \ref{fig:performance} bottom, red). Virtually all errors are due to incorrect digit classification, rather than mistakes in color or location. 


\subsection{Compositional transfer learning}

To better understand the nature of the compositional learning, we performed several experiments to test the outputs of the deep network under different training conditions. The essential questions relate to the combinatorics of the scene. Each object within the scene can have 9 locations, 7 colors and 10 digit identities, giving 630 combinations. There are then 630 potential scenes with a single digit. There are $396,900=630^2$ potential scenes with 2 digits, and over 250 million scenes with three digits. 

The training set contains 550 thousand examples split evenly between scenes with one, two, or three digits. The test set contains an exclusive set of hand-written digits that were not present during training. However, the combination of digit identities, colors and locations present in a scene may or may not overlap with the training set (i.e. a cyan 7 in the center right and a red 4 in the bottom left). Every scene is generated i.i.d. Because of the combinatorics, all 630 single digit combinations are present in the training set. But, only a tiny fraction of three digits scenes are explored during training, and there is virtually no overlap between the particular combination of objects present in the test and training sets. For the two digit scenes, about one third of the combinations shown in the test set were also shown in the training set. 
 
It is important that the network can learn across the combinatorics of each scene. Since the performance of the system on the three digit scenes is good, it is already clear that the network can handle scene combination that were not present in the training set. To see if previously seen combinations have any advantage over novel combinations, we divided scenes with two digits into two groups. The test scenes that shared the same combination of objects with a training example were separated from scenes with a novel combination of objects. Remember that the particular MNIST digit used in test scenes is always taken from a separated test set. We compared the performance of these two groups, and see little difference (Fig. \ref{fig:combo_comparison}), indicating that the combinatorics of the objects in the scene do not matter to the network. 

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{combo_comparison-2obj-accuracy-10k-180517.png}
    \caption{We isolated scenes containing the same combination of digits as shown during training, and compared them to scenes with novel combinations of digits. The performance metrics are close to indistinguishable.}
    \label{fig:combo_comparison}
\end{figure}

The next experiment addresses whether the network can handle scenes with a different number of objects in the scene. We presented scenes with four digits to the network, and used the resonator circuit to query the output. Indeed, the system can identify four objects present in a scene, even though no scene in the training set ever contained four objects (Fig. \ref{fig:sum_learning}).

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{sum_learning-4dig-180517.png}
    \caption{A network trained with maximum of three objects in a scene can successfully identify scenes with four objects.}
    \label{fig:sum_learning}
\end{figure}

The final question is whether there is any transfer learning across the compositional properties of each object. Hypothetically, knowledge of sevens in the left can give you information of sevens on the right, but it is unclear whether the deep network can take advantage of this. The structured vectors that are used as a supervised signal contains this compositional information, but simultaneously, the structured vectors also are indicating to the network that it should treat each conjunction as unique, and that there should be no similarity between a red five and a blue five, or a red seven, or a red five in a different location. 

To see if any such transfer learning was present, we removed all `sevens' in the `bottom-right' from the training set. We then examined the outputs of the network at test time with `sevens' present in the `bottom-right'. The experiment shows that the network does not transfer knowledge about sevens from other locations to the bottom right location, but rather outputs a different digit identity (typically 9, and sometimes 2,3, or 4) (Fig. \ref{fig:transfer_learning}). 

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{confusion_comparison-1122-x722-180517.png}
    \caption{We removed all examples with `7' in the `bottom-right' from the training data. 
    The confusion matrix for object classification was computed for all objects in the `middle-center' location and the `bottom-right' location on the test set (which does contain `7-bottom-right').
    The network learns this gap as a property of the data, and will never output `7' in the bottom right when given the stimulus during testing.
    %, rather than transferring knowledge about `7's seen and learned from other locations. 
    }
    \label{fig:transfer_learning}
\end{figure}


% \subsection{Resonator accuracy and scaling}
% To better understand the performance of the resonator circuit, we ran simulation experiments where the resonator circuit solved abstract decompositions. 
% Performance is dependent on three primary factors: 
% the number of neurons, the number of component combinations, and the amount of noise or distractor objects. We systematically varied these parameters and empirically measured the accuracy and convergence time (Fig. \ref{fig:res_scaling}). 

% A resonator circuit of $N=3,000$ neurons can solve a decomposition with $100,000$ combinations in a few dozen iterations with over $95\%$ accuracy (Fig. \ref{fig:res_scaling} top, dark red lines). Noise reduces resonator circuit performance, but even with large amounts of noise, decompositions can still be solved (Fig. \ref{fig:res_scaling} bottom). The impact of Gaussian noise is similar to the impact of uncertainty in the deep net output, and it reflects the impact of crosstalk noise due to multiple objects being represented in the scene vector.


% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{res-accuracy-iterations-noise-180517.png}
%     \caption{Many simulation experiments were run to measure the empirical performance and convergence time of the resonator circuit solving abstract decompositions. As $N$ increases, the performances increases and the convergence time decreases (top). Noise decreases performance and increases convergence time (bottom; $N=3,000$).}
%     \label{fig:res_scaling}
% \end{figure}

% \subsection{Lyapunov function and convergence}

% Empirically, we see that the resonator circuit has a very high probability of converging to the correct answer (Fig. \ref{fig:res_scaling}). To better understand the resonator circuit's convergence properties, we derive the Lyapunov function. 

% In essence, the resonator circuit can be understood as multiple Hopfield-type networks operating in parallel and communicating with each other. 

% Consider the resonator circuit for estimating two values, $\mathbf{\hat{x}}$ and $\mathbf{\hat{y}}$, where $\mathbf{s}=\mathbf{x} \odot \mathbf{y}$. The element-wise multiplication with the scene vector $\mathbf{s}$ can be translated into a matrix multiplication with matrix $\mathbf{S}$, where the vector $\mathbf{s}$ is along the diagonal of matrix $\mathbf{S}$. The resonator circuit for this system can be expressed as a larger recurrent neural network:
% \begin{equation}
%     \bigg(
%     \begin{array}{c}
%     \mathbf{\hat{x}}(t) \\
%     \mathbf{\hat{y}}(t) 
%     \end{array}
%     \bigg)
%     = f \Bigg(
%     \bigg(
%     \begin{array}{cc}
%     0 & \mathbf{X}\mathbf{X}^\top \mathbf{S} \\
%     \mathbf{Y}\mathbf{Y}^\top \mathbf{S} & 0
%     \end{array}
%     \bigg)
%     \bigg(
%     \begin{array}{c}
%     \mathbf{\hat{x}}(t-1) \\
%     \mathbf{\hat{y}}(t-1)
%     \end{array}
%     \bigg)
%     \Bigg)
%     \label{eqn:res_rnn}
% \end{equation}


\section{Discussion}

Based on vector symbolic architectures, here we propose a novel approach for solving scene understanding with deep learning networks. Rather than returning a list of object labels, the new approach transforms an input image into a vector description of the scene that not only contains object identities but also their poses and properties. Specifically, we show that deep networks can learn vector representations of scenes with multiple objects, formed by a vector-symbolic architecture. The scene representation is compositional, it contains information pertaining to the identity of scene objects, but also  information about their position and color. Using a VSA resonator network \citep{frady_2020_resonator}, one can extract from the output vector of the deep network the identities and properties of the individual objects. 

We analyzed simulation experiments to better understand how deep learning generalizes knowledge, learned from simple artificial scenes built from multiple MNIST digits. We find that the deep network has no issues with generalizing to digit shapes only shown during testing. We also show that the particular combination of objects present in the scene, as well as the total number of objects in the scene, do not greatly reduce the network's performance.

 We also investigated how the network can generalize over feature conjunctions. We find that if particular conjunctions of features are not present during training, then these conjunctions cannot be recognized by the deep network. For example, one might expect a network exposed to the digit shape `7' in many locations to be able to recognize a `7' in a new position. But this is not the case. If stimuli with the conjunction of `7' and `bottom-right' are removed from the training set, the network will not ever output this conjunction during testing. The network has learned this gap as a property of the data, rather than generalizing its knowledge about `7's to explain a constellation never seen during training. 

The expectation that the network generalizes knowledge of individual objects across different object poses is a typical example of inductive bias \citep{mitchell1980need}. 
Without such a generalization ability, the amount of required training data grows exponentially with number of object properties. Even when large amounts of training data are available, it is hard to track (and somehow avoid) the formation of ``gaps'' during training, in which generalization fails. Of course, for our simple artificial scenes a convolutional neural network would exhibit the required generalization ability for translation. But in more realistic scenes object variations include not only translation, but also rotation and scaling, as well as many other variable properties. Thus, a much more general form of inductive bias towards generalization is necessary. 

%In many cases of deep learning, 
The inductive bias towards invariance properties is typically imposed prior to learning.  Often it involves specific network designs, like convolutional neural networks for achieving translation invariance, or networks with embedded Lie group structures to learn  transformations like translation and rotation from data \citep{chau2020}. Another popular approach is data augmentation \citep{shorten2019survey}, in which the inductive bias is imposed by adding artificially transformed data points to the training data. 
%However, even still it is unclear whether this type of approach leads to a true understanding of a feature transform,
%But if the inductive bias is to be imposed on the network a priori, then is there really a need to use training methods to learn the invariant transformations? 

In other recent work \citep{renner2022scene}, we propose an approach to scene understanding based on analysis-by-synthesis. We use the framework of vector-symbolic-architecture with its algebra of operations for superposition and binding to formulate a generative model of scenes. 
The generative model allows us to explicitly impose the inductive bias into the resonator network to factor out invariances, such as translation, rotation, color and scaling, without any learning. 
%An object of a scene is a product of vectors, the factor vectors representing object properties as well as pose transforms, such as translation, rotation and scaling. For analyzing an input scene the generative model has to be inverted which is done by vector factorization, which can be efficiently solved using resonator networks \citep{kent_2020resonator}. 
%show how rigid transforms can be directly encoded based on , and how the resonator network can then be designed to extract the object properties, all without any learning. 
A model of this type could be a first module before a learning deep network, where the inductive bias imposed should avoid gaps and reduce the amount of required training data. 
%One promising extension is to then incorporate learning after the factorization stage, where it is likely much less data is needed to solve various classification problems. 

Ideally, however, neural networks should be able to learn from data all the transforms that objects can experience. A network should be able to learn from a video stream of the natural world how to deal with 3D geometry, transformations, and lighting conditions. Our simple experiments suggest that naive supervised learning approaches are unlikely to extract such transforms from data.


%The prior to learning designs, described above, are all quite rigid, such models cannot handle any new factors of variation.


%As well, as eluded to above, there are not really many approaches capable of learning these transforms without them being imposed, onto the network structure or through data augmentation. Approaches that utilize Lie group structure are promising, but how would such networks learn to add new factors of variation? This view points out that there are deeper problems to learning that may go beyond conventional supervised learning methods.



% networks to extract invariant representations of objects based on a generative model, many of the issues with 



% SAY SOMETHING ABOUT FUTURE WORK HOW TO ADDRESS THIS ISSUE AND CITE NEW ARCHIVE PAPER ABOUT SCENE UNDERSTANDING  


%our goal here was to see if the network could learn to generalize over translation without being told a priori. This is important to consider in the context of scenes constructed from a more complex generative model. As more factors are included, such as rotation, scaling, etc., then the combinatorics of possible scenes grows exponentially. It then becomes increasingly unlikely that the training data contains sufficient information to see the full combinatorics of all factor conjunctions. Thus the network will be unable to generalize to novel conjunctions present in the test set. 





% Resonator circuits can be configured to infer different compositional structures.
% This would be useful wherever neural computation gives rise to compound distributed representations. There are a number of ways to extend the resonator circuit towards dealing with more complex algebraic structures, which may be crucial for modeling high-level computation relevant to visual perception and natural language processing.


% Sensory inputs result from compositions of many different causes. 
% Human perception naturally decomposes sensory input from the environment into component parts, such as decomposing reflectance from lighting conditions resulting in perceptual illusions \citep{Adelson1999}. 
% %Our perceptual system factors the sensory input into its components.
% Different parts of the cortex seem to represent certain components of the environment.
% Compositions are combinatoric; inferring the components requires searching over an exponentially growing set of combinations.
% Resonator circuits suggest how different brain regions infer compositional structure and how the representations of components interact to explain sensory inputs.



% We believe this may also present an opportunity to understand how the deep network is processing the scene. Crucial to neural network generalization performance is the ability to understand compositional structure. 


% Our framework reveals a new way to probe the nature of learning in deep neural networks. By manipulating the training and testing data, we revealed several aspects of the deep learning approach. 

% Our results show that the deep network can map complex scenes into a structured compositional vector. In order for the deep network to map the scene into a structured vector, it must be able to pull out the correlations in the scene; across digit identities (since hand-written digits have correlations), colors, and locations (since neighboring locations can overlap and occlude). 

% We see that the network can learn about scenes with more digits present than it was trained with. However, we see that the network does not transfer any learning across locations or other compositional properties. In essence, the network transfers learning across sums, but not products. 

% The reason for the above is likely that there is no real pressure on the network to try and understand the compositional structure of the scene.  


% This is as if the resonator is directing its \emph{attention} to one of the objects. 


\subsubsection*{Acknowledgments}


{\small
\bibliographystyle{abbrvnat}
\bibliography{hdspike,hdprespast,capacity,references} % no spaces between commas
}


\end{document}
