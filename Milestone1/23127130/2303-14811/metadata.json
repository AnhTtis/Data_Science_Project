{
    "arxiv_id": "2303.14811",
    "paper_title": "Learning Generative Models with Goal-conditioned Reinforcement Learning",
    "authors": [
        "Mariana Vargas Vieyra",
        "Pierre MÃ©nard"
    ],
    "submission_date": "2023-03-26",
    "revised_dates": [
        "2023-03-28"
    ],
    "latest_version": 1,
    "categories": [
        "cs.LG",
        "stat.ML"
    ],
    "abstract": "We present a novel, alternative framework for learning generative models with goal-conditioned reinforcement learning. We define two agents, a goal conditioned agent (GC-agent) and a supervised agent (S-agent). Given a user-input initial state, the GC-agent learns to reconstruct the training set. In this context, elements in the training set are the goals. During training, the S-agent learns to imitate the GC-agent while remaining agnostic of the goals. At inference we generate new samples with the S-agent. Following a similar route as in variational auto-encoders, we derive an upper bound on the negative log-likelihood that consists of a reconstruction term and a divergence between the GC-agent policy and the (goal-agnostic) S-agent policy. We empirically demonstrate that our method is able to generate diverse and high quality samples in the task of image synthesis.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.14811v1"
    ],
    "publication_venue": null
}