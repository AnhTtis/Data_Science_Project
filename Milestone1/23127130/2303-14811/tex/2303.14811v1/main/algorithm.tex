%!TEX root = ../generative_by_rl.tex

\section{Algorithm}
\label{sec:algorithm}
In this section we describe a particular instanciaton of MDP and V-agent and introduce an algorithm to learn the V-agent.
In what follows we assume the space of states and actions are both the real $d$-space $\cX=\R^d$, $\cY = \R^d$, for some dimension $d$.

\subsection{Instanciation}

We first describe a particular MDP and family of variational agents.

\begin{figure*}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/gene_rl_graph.pdf}
	%\vspace{-0.5cm}
	\caption{Generation of a trajectory with the GC-agent for a sample of MNIST: At a step $h$, given a state the GC-agent select one of the proposals provided by the S-agent. The action, i.e. the selected proposal, is scaled by some rate $\alpha_h$ and added to the previous state. For a fixed sequence of actions the proposal network is differentiable. Therefore we can backpropagate the loss from the last state to the initial state once the sequence of actions is determined.}
	\label{fig:schema}
\end{figure*}


\paragraph{A particular MDP} We present a specific MPD $\cM = \{\cX,\cY,H,p\}$ parameterized by a sequence of rates $(\alpha_h)_{h\in[H]}$ and a variance $\sigma^2$. The initial state is fixed, e.g. $x_1$ is the mean of the elements in $\cD$. The first $H-1$ transition are deterministic. Precisely for $h\in[H-1]$, given the state $x_h$ and action $y_h$ the next state $x_{h+1}$ is
\begin{equation}
\label{eq:def_MDP}
x_{h+1} = x_h + \alpha_h y_h\,.
\end{equation}
The last transition probability distribution is a Gaussian distribution of variance $\sigma^2$, that is $p_H(\cdot|x_H,y_H) = \cN(x_H+\alpha_H y_H,\sigma^2)$. Note that in this case the goal-conditioned loss is proportional to the mean square error between the last state and the goal $\ell_h^{\tx}(x,y) = \ind\{h=H\}\norm{x+\alpha_h y-\tx}_2^2/(2\sigma^2)$.

\paragraph{A family of variational agents} As mentioned earlier a V-agent is made of a GC-agent and a S-agent. We only consider agents with policy given by a \emph{mixture of $A$ Dirac probability distributions} for some fixed $A\in\N$. That is, given a step and a state (and a goal for the GC-agent) the agent samples an index $a_h \in[A]$ that we call \emph{selection}. The agent also samples a vector of actions $(z_{h,a})_{a\in[A]}\in\cY^A$, that we call \emph{proposals}, from the collection of Dirac distributions. Then, the action is given by the proposal that corresponds to the selected index $y_h=z_{h,a_h}$. Note that, since the Kullback-Leibler divergence between two Dirac distribution with different supports diverges, in order to maintain the loss $\Lub$ defined in~\eqref{eq:def_surrogate} finite \emph{the GC-agent and the S-agent should share the same proposals}. In particular, the proposal function of the GC-agent that maps the step, state and goal to the collection of proposal \emph{is independent of the goal} since it is the case for the proposal function of the GC-agent.
We use GCRL to learn to \emph{select} the best proposal among the $A$ proposals.
In other words, we represent the action to be taken by its index $a_h\in [A]$.
This allows us to operate on a discrete space of actions.
Intuitively, the agent learns to map samples from the underlying distribution of the data to sequences of indexes representing the selected actions at each step.
The proposal function is fit to provide proposals that approach the agent to the goals, see below.
Therefore, the MDP is non-stationary as the proposals change during the learning.
By sharing the proposal function between both agents and representing actions by their indexes, we traded a GCRL task with continuous action space for a (non-stationary) GCRL task with discrete action space.

\begin{remark}
	We use this particular class of policies for the CG and S-agent: a mixture of Dirac distributions that allows us
	to represent a highly multi-modal distribution in few steps (provided that the rates $\alpha_h$ are large enough).
	Note that if one were to use the class of Gaussian probability distributions instead to model the policy, one would
	need much more steps to model a multi-modal distribution.
	Crucially, it would be harder to "branch out" to several different
	specific states from the current state by only adding Gaussian noise, as the model would be prone to mode collapse.
\end{remark}

% \paragraph{Family of variational agents} As mentioned earlier a V-agent is made of a GC-agent and a S-agent. Both of them generate trajectories as follows.
% They use a \emph{proposal function} to generate $A$ proposals $(z_{h,a})_{a\in[A]}\in\cY^A$, where $[A] = [1, \dots, A]$.
% This function is shared between both agents, which implies they both observe the same proposals at each step.
% Then they select an index $a_h \in[A]$ to pick one proposal.
% In what follows we will also call indexes "selections".
% The action is given by the proposal that corresponds to the selected index $y_h=z_{h,a_h}$.
% We use GCRL to learn to \emph{select} the best proposal among the $A$ proposals.
% In other words, we represent the action to be taken by its index $a_h\in [A]$.
% This allows us to operate on a discrete space of actions.
% Intuitively, the agent learns to map samples from the underlying distribution of the data to sequences of indexes representing the selected actions at each step.
% The proposal function is fit to provide proposals that approach the agent to the goals.
% Therefore, the MDP is non-stationary as the proposals change during the learning.
% By sharing the proposal function between both agents and representing actions by their indexes, we traded a GCRL task with continuous action space for a (non-stationary) GCRL task with discrete action space.

% This allows us to perform GCRL task only at the level of \emph{the selections which are discrete}.
% Thus from the point of view of the GC-agent the discrete "actions" are the selections and the structure of the MDP is a large tree where the vertices are labeled by the selections and the nodes are the state obtained by taking successively the selections labeling the vertices of the branch leading to the node.


\paragraph{Parametrization} We parameterize the S-agent by two neural networks: The proposal network $\Spropnet_\theta$ with weights $\theta$  that takes as input a state, step and selection $(x,h,a)$ and outputs an action $y= \Spropnet_\theta(x,h,a)$. The selection network $\Sselectnet_\phi$ with weights $\phi$ that takes as input a state, step $(x,h)$ and outputs a distributions over selections. It is important to note that the S-agent selection network and the proposal network are not conditioned on the goal. As stated above the GC-agent only needs to pick indexes $a_h\in[A]$. It is parameterized by a GC-selection Q-values network $\GCselectQnet_\psi$ with weights $\psi$ that takes as input a state, step and goal $(x,h,\tx)$ and outputs Q-values $\big(\GCselectQnet_{\psi,a}(x,h,\tx)\big)_{a\in[A]}$.
Typically for the $\Spropnet_\theta$ we choose a \UNET architecture~\citep{Ronneberger2015UNetCN} whereas the $\Sselectnet_\phi$ and $\GCselectQnet_\psi$ are simple convolutional neural networks. See Appendix~\ref{app:net_arch} for more details.

\paragraph{Unfolding the proposal network} Note that once a sequence of selections $(a_1,\ldots,a_H)$ is fixed, the last state (or more precisely, the state's mean) of a trajectory generated with actions $y_h = \Spropnet_\theta(x_h,h,a_h)$ from the proposal network and the aforementioned selections is a differential function of the weights $\theta$.
To see this one just needs to unfold \eqref{eq:def_MDP} trough the steps $h\in[H]$, see Figure~\ref{fig:schema}. We denote by $\Spropnet_{\theta,U}(a_1,\ldots,a_H)= x_H+\alpha_H y_H$ the unfolded network that maps a sequence of selections to the mean of the state at step $H+1$ when the actions are obtained with the S-agent proposal network.


\subsection{Training}\label{sec:training}The learning procedure of the V-agent is split into two parts: trajectories are sampled with the GC-agent (and the proposal from the S-agent) to reconstruct a goal sampled at random from the training set. Then these trajectories are used to learn the different networks that parameterized the GC-agent and the S-agent. In particular the GC-agent is learned with the \DQN algorithm\footnote{Precisely, in the experiments we use the \DDQN algorithm \citep{hasselt2016deep} with Dueling Q-Network \citep{wang2016dueling}.} \citep{mnih2013atari}.

\paragraph{Memory} We use three replay buffers, one for each network. The replay buffer $\replayGCselect$ associated with the GC Q-values network $\GCselectQnet$, that will be fed with transitions. The replay buffer $\replaySselect$ associated with the S-agent selection network $\Sselectnet$. And the replay buffer $\replaySprop$ of the S-agent proposal network $\Spropnet$.

\paragraph{Sample trajectory} We first sample a goal $\tx$ uniformly at random from the training set $\cD$. Then a trajectory is generated with the GC-agent as follows. At step $h$, given the current state $x_h$, we pick a selection index $a_h\in\argmin \GCselectQnet_{\psi,a}(x_h,h,\tx)$. The action $y_h = \Spropnet_{\theta}(x_h,h,a_h)$ is the output of S-agent proposal network that corresponds to the index $a_h$. The next state $x_{h+1}$ is given by \eqref{eq:def_MDP} and the GC-agent receives a loss $\ell_h=\ell_h^{\tx}(x_h,y_h)$. The observed transition is stored in the replay-buffer $\replaySprop$ of the GC-agent,
as well as the selection $(x_h,h,a_h)$ in the selection replay-buffer $\replaySselect$ of the S-agent.
Once the horizon is reached we record the selections and the goal $(\{a_1,\ldots,a_h\},\tx)$ in the proposal replay-buffer $\replaySprop$ of the S-agent. See Algorithm~\ref{alg:OurSampleTraj} for a detailed description.

\begin{algorithm}[ht]
\centering
\caption{\OurSampleTraj}
\label{alg:OurSampleTraj}
\begin{algorithmic}[1]
   \State Sample uniformly at random goal $\tx$ from the train set $\cD$.
	 \State Get initial state $x_1$.
	\For{$h \in[H]$} \Comment Generate trajectory with the GC-agent.
	\State Get selection 	$a_h \in \argmin_{a\in[A]} \GCselectQnet_{\psi,a}(x_h,h,\tx)$\Comment We can add an exploration malus.
	\State Get action $y_h = \Spropnet_\theta(x_h,h,a_h)$.
	\State Observe next state $x_{h+1} = x_h +\alpha_h y_h$.
	\State Get loss $\ell_h= \ell_h^{\tx}(x_h,y_h)$.
	\State Record transition $(x_h,h,a_h,x_{h+1},\ell_h,\tx)$ in $\replayGCselect$ and selection $(x_h,h,a_h)$ in $\replaySselect$.
	\EndFor
	\State Record trajectory selections $(\{a_1,\ldots,a_H\},\tx)$ in $\replaySprop$.
\end{algorithmic}
\end{algorithm}


\paragraph{Update} The GC-agent is learned with the \DQN algorithm \citep{mnih2013atari}. Thus we need to define a target Q-values network parameterized by the weights $\psi^{\texttt{target}}$. At a high level we use the target Q-values network $\GCselectQnet_{\psi^{\texttt{target}}}$ and the transitions stored in the replay buffer $\replayGCselect$ to compute new targets via the optimal Bellman equations. Then the GC-agent Q-values network $\GCselectQnet_\psi$ is trained to fit these targets by gradient descent on the mean squared error. Note that the targets weights are updated as an exponential moving average of the weights $\psi^{\texttt{target}}\gets \rho \psi+(1-\rho)\psi^{\texttt{target}}$ for some parameter $\rho \in(0,1)$.
The update of the S-agent is simpler. The S-agent selection network is trained to reproduce the selections picked by the GC-agent by minimizing the cross-entropy loss between the selections from the replay-buffer \replaySselect and its prediction. The S-agent unfolded proposal network $\Spropnet_{\theta,U}$ is trained to map the sequence of selections stored in the replay-buffer $\replaySprop$ to the associated goals. Thus the weights $\theta$ are updated by gradient descent on the mean square error between the output of $\Spropnet_{\theta,U}$ evaluated on the selections and the goal, see Figure~\ref{fig:schema}. A complete description is provided in Algorithm~\ref{alg:OurUpdate}.

\paragraph{Exploration} For the exploration we do not rely on the by default $\epsilon$-greedy mechanism.
As a matter of fact, it is not clear how this exploration technique would interact with the learning of the proposal network which is based on the trajectories generated by the GC-agent. Instead we propose to add a penalty to the Q-values.
This penalty is defined as the logarithm of the probability of the S-agent policy.
That is, for some parameter $\kappa>0$,
\begin{align*}
a_h\in&\argmin_{a\in[A]} \GCselectQnet_{\psi,a}(x_h,h,\tx) + \kappa \log\big(\Sselectnet_{\phi,a}(x_h,h)\big)\,.
\end{align*}
This will encourage the agent to take the selections that have not been used.
Note that this penalty does not interfere with the policy of the GC agent systematically picking the same indexes for a specific goal as long as the other indexes are used for other goals.

\begin{algorithm}[ht]
\centering
\caption{\OurUpdate}
\label{alg:OurUpdate}
\begin{algorithmic}[1]
\If{time to update $\Spropnet_{\theta}$}
\State Sample a batch $\batchSprop$ in $\replaySprop$
\State Update $\Spropnet_{\theta}$ with one step of gradient,
\begin{scriptsize}
	\[
	\nabla_\theta\frac{1}{|\batchSprop|} \sum_{(\{a_1,\ldots, a_H\},\tx)\in\batchSprop} \norm{\Spropnet_{\theta,U}(a_1,\ldots a_H)-\tx}_2^2\,.
	\]
\end{scriptsize}
\EndIf
\If{time to update $\Sselectnet_{\phi}$}
\State Sample a batch $\batchSselect$ in $\replaySselect$
\State Update $\Sselectnet_{\phi}$ with one step of gradient,
\begin{scriptsize}
\[
\nabla_\phi \frac{1}{|\batchSselect|} \sum_{(x,h,a)\in\batchSselect} -\log\big(\Sselectnet_{\phi,a}(x,h)\big)
\,.\]
\end{scriptsize}
\EndIf
\If{time to update $\GCselectQnet_{\psi}$}
\State Sample a batch $\batchGCselect$ in $\replayGCselect$
\For{$(x,h,a,x',\ell,\tx) \in \batchGCselect$}
\State Compute target
\begin{scriptsize}
\[q^{\texttt{target}}(h,x',\ell,\tx) = \ell + \min_b \GCselectQnet_{\psi^{\texttt{target}},b}(x',h+1,\tx)\]
\end{scriptsize}
\EndFor
\State Update $\GCselectQnet_{\psi}$ with one step of gradient,
\begin{scriptsize}
\begin{align*}
\nabla_\psi\frac{1}{|\batchGCselect|} \sum_{(x,h,a,x',\ell,\tx) \in \batchGCselect} \big\rVert&\GCselectQnet_{\psi,a}(x,h,\tx)\\
&\quad-q^{\texttt{target}}(h,x',\ell,\tx) \big\lVert_2^2
\,.\end{align*}
\end{scriptsize}
\If{time to update $\GCselectQnet_{\psi^{\texttt{target}}}$}
\State Update target Q-values weights $ \psi^{\texttt{target}} \gets \rho \psi +(1-\rho)\psi^{\texttt{target}}$.
\EndIf
\EndIf
\end{algorithmic}
\end{algorithm}


\subsection{Inference}
\label{subsec:inference}

To obtain as sample from the learned model one just needs to generate a trajectory using the S-agent.
Note that at inference time we do not use the GC-agent, and that the selections are entirely made by the
S-agent selection network $\Sselectnet_\phi$.
It is important to point out that once the S-agent proposal network is fixed the number of possible samples generated by the model is finite and upper-bounded by $A^H$.
For a reasonably small choice of $A$ and $H$ this upper-bound is very large and not restrictive.
Observe that this model thus produces a discrete representation of the input.
Discrete latent representations have been explored by \citet{van2017neural}.
The only source of randomness in the model comes from sampling a selection from the S-agent selection network $\Sselectnet_\phi$.
A detailed sampling procedure is provided in Algorithm~\ref{alg:OurSample}


\begin{algorithm}[ht]
\centering
\caption{\OurSample}
\label{alg:OurSample}
\begin{algorithmic}[1]
	\State Get initial state $x_1$.
	\For{$h \in[H]$} \Comment Generate trajectory with the S-agent.
	\State Get selection $a_h \sim  \Sselectnet_{\psi}(x_h,h)$.
	\State Get action $y_h = \Spropnet_\theta(x_h,h,a_h)$.
	\State Observe next state $x_{h+1} = x_h +\alpha_h y_h$.
	\EndFor
	\State Return the last state $x_{H+1}$.
\end{algorithmic}
\end{algorithm}
