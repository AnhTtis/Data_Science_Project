%!TEX root = ../generative_by_rl.tex
\vspace{-.5cm}
\section{Conclusion and Discussion}
\label{sec:conclusion}

We introduced a novel framework to learn generative models based on goal-conditioned reinforcement learning.
The main idea of this work is to consider the elements of the training set as being generated by an agent that reaches those states after a fixed number of steps.
We then learn a mixture policy that approximates a family of goal-conditioned agents that are trained to generate trajectories that lead to the training points.
Following this line of reasoning we derived an upper-bound for the negative log-likelihood consisting of two terms.
A reconstruction error term that measures the reconstruction quality of the goal-conditioned policy, and a divergence term that encourages the mixture policy to remain close to the family of goal-conditioned agents.
Our experiments demonstrate that our method is able to effectively reconstruct the training set and generate a rich variety of outputs in the task of image synthesis.

There are some aspects of the algorithm we presented that are yet to be explored.
Regarding the use of GCRL to parameterize a generative model an interesting research direction could be to 
explore whether Hindsight Experience Replay \citep{andrychowicz2017} or a variant of it could contribute to 
obtaining a more effective GC-agent in terms of reconstruction loss.
We empirically showed that our method needs significantly less steps to reach the goals than DMs without compromising 
the sample quality (in terms of FID score).
A more thorough comparative study of the required number of steps by DMs and our method would be of interest.
Also, note that we have not use the state-of-the-art architectures for DMs nor for our algorithm due to time and resource 
constraints. 
Studying and comparing DMs and our method with more sophisticated models, and on bigger datasets, is left as future work. 

We believe that the framework we introduced could be useful for a broad scope of tasks. 
In particular, there are applications where the space of actions is inherently discrete.
For instance, in molecule design one could consider adding atoms and bonds to be the space of actions.
In this case one can dispense with a proposal function, and construct a model that only needs to learn to select the right 
actions.
Addressing this task is an interesting research direction.
