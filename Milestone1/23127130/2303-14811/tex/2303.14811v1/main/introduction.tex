%!TEX root = ../generative_by_rl.tex

\section{Introduction}

We consider the problem of learning a \emph{generative model}.
In recent years the study of generative models became a vast and prolific research field in the machine learning community.
Because of their ability to capture important information about the distribution of the available data they have the capacity to generate new samples, thus emulating the nature of the training set.
Generative models have shown outstanding results in applications such as speech generation \citep{vandenOord2016} and high-quality image generation \citep{brock2019, razavi2019}.
Recent examples of these models include Generative Adversarial Networks (GAN)~\citep{goodfellowgan2014,karrasprogressive2018,brock2019}, Variational Auto-Encoders (VAE)~\citep{rezende14,kingma2014,oord2017neural}, Flow-based Models~\citep{dinh2017,kingmagflow2018}, Auto-regressive Models~\citep{germain2015made,oordpixelcnn2016} and Diffusion Models (DM)~\citep{sohldickstein2015,hodenoising20,nichol21a,songME21}.

% \todoMariana{mention limitations of previous methods. Perhaps introduce diffusion models too?}

Given a sample from the training set, Diffusion Models work by adding a small amount of Gaussian noise to the sample in some pre-defined amount of steps, denoted by $H$.
This \emph{diffusion process} yields a sequence of $H$ intermediate representations, each of which have a larger amount of noise than the previous one.
Note that by following this process, the last representation we obtain is a sample from a Gaussian distribution.
The generative model is then obtained by learning how to reverse this diffusion process.
Once trained, the model is able to generate new samples by applying the reverse diffusion process on gaussian noise.
Recently, \citet{sohldickstein2015,hodenoising20,nichol21a,dhariwal2021diffusion} showed that DMs are competitive with GANs in the task of high-quality image synthesis.
% \todoPi{This seems disjointed from what we're so far discussing. How do we transition to this score-based approach?}
Interestingly, a dual view of DM~\citep{hodenoising20} is the score-based generative model~\citep{song2019}, where the generative model is the result of several steps of a Langevin dynamic~\citep{parisi1980nh} following a score function.
Such score function is learned beforehand in order to approximate the gradient of the log-likelihood.
Although DMs are simple to train, they are slow to sample from~\citep{hodenoising20}
as the model in general requires a large number of steps $H$ ($H \approx 1000$) to be able to reconstruct the inputs.
Several techniques have been proposed to overcome this issue, but they speed-up the sampling process in detriment of the output quality~\citep{nichol21a,songME21}.
This calls for approaches that naturally require less steps to generate samples.

In this paper we adopt a reverse point of view: we learn how to transform an arbitrary fixed initial state into any sample of the training set using the Goal-Conditioned Reinforcement Learning (GCRL) framework \citep{leslie1993,pong2018,andrychowicz2017,schaul15}.
In GCRL the agent aims at a particular state called the \emph{goal}.
At each step, the environment yields a loss that accounts for "how far" the agent is from the goal it is targeting.
For example, the loss can be defined as the Euclidean distance between the current state and the goal.
In  the context of learning a generative model we consider the training set to be a \emph{set of goals}.
Intuitively, our learning procedure consists of training a family of \emph{goal-conditioned agents} (GC-policy) that learn to reach the different elements in the training set by producing a \emph{trajectory} of intermediate representations, departing from the fixed initial state.
At the same time, we obtain the generative model by learning a \emph{mixture} policy of these goal-conditioned policies where the \emph{goal is sampled uniformly at random from the training set}.
Concretely, the generative model approximates a policy that randomly picks a trajectory that departs from the fixed initial state and leads to an element of the training set.
This can be cast as a supervised learning procedure.
Note that the goal-conditioned agents are used for training only.
At inference time, we generate trajectories with the mixture policy and collect the states reached at the final step.
Such states are the new generated samples.
Similar to variational inference~\citep{blei2017}, we derive a lower bound on the log-likelihood that consists of two terms.
The first term measures how good the GC-policy is able to reconstruct the samples.
The second term is a divergence term between the mixture policy and the GC-policy.
We also provide empirical evidence that our method is able to effectively reconstruct the observed data and generate a rich variety of new samples in a small number of steps ($H\approx 16$).

To summarize we highlight our main contributions:
\begin{itemize}[noitemsep]
  \item We bridge the gap between the fields of RL and generative models by introducing a novel framework that learns a generative model using GCRL. Although GCRL has been studied in similar contexts \citep{rudner2021outcome,attias03a}, it is the first time to the best of our knowledge that RL is used as a building block for learning generative models.
  \item We derive a lower bound on the log-likelihood that accounts for the reconstruction quality of the model.
  \item We provide an empirical evaluation that compares our method to the Variational Auto-Encoder model. Comparisons to other generative models are left as future work.
\end{itemize}

% % \todoMariana{A smoother transition: diffusion models are great but... diffusion models can be deemed a markov chain and this makes them suitable for RL? Do we need to introduce RL?}
%
% An idea:
% \begin{itemize}
%   \item DM can be deemed markov chains that yield a sequence of noisy samples,
%   \item RL is a framework where a model called an "agent" learns to make a sequence of desicions that optimize a specific objective when interacting with an environment.
%   \item Hence, we inspire from diffusion models and use RL to learn how to produce images from noise.
% \end{itemize}
%
% Reinforcement Learning is a framework where a model called an \textit{agent} learns to make a sequence of desicions that optimize a specific objective, called the \textit{reward} function, when interacting with an environment.
%
% In this paper we consider a different route to learn generative models.
% We take inspiration from Diffusion Models, and propose a novel framework based on Goal-Conditioned Reinforcement Learning where the goal is to train an agent that learns to transform Gaussian noise into valid, realistic samples.
% Our method consists of an agent that, provided a specific sample from the dataset called the \textit{goal}, takes Gaussian noise as input and learns to make a sequence of transformations in order to re-create the given goal.
% We call this the \textit{goal-conditioned agent}.
% At the same time, another agent learns to emulate the behavior of the goal-conditioned agent without any prior goals to aim at.
% As a result, we obtain a model that is able to generate valid data samples from Gaussian noise.
%
% To the best of our knowledge this is the first attempt to learn generative models using reinforcement learning. Blah Blah Blah
%
% \todoMariana{cite}
%  % consider a diffusion process that progressively transforms an image sampled from the data set into a sample from a fixed distribution (e.g., a Gaussian) by adding noise. The generative model is then obtained by learning to reverse this diffusion process.
%
%
% % \begin{itemize}
% %   \item GAN \citet{goodfellowgan2014,karrasprogressive2018,brockDS19}
% %   \item Flow-based \citet{dinh2017,kingmagflow2018}
% %   \item Auto-regressive \citet{oordpixelcnn2016}
% %   \item VAE \citet{kingma2014}
% %   \item Diffusion model \citet{sohldickstein2015,hodenoising20,nichol21a}  score-based generative model \citet{song2019}
% % \end{itemize}
%
%
% \paragraph{Goal-conditioned reinforcement learnig for generative model}
% \begin{itemize}
%   \item goal-conditioned RL \citet{leslie1993,pong2018,andrychowicz2017,schaul15}
%   \item variational RL \citet{toussaint2006,toussaint2009,fellows2019}
% \end{itemize}
%
% \paragraph{Goal-Conditioned RL}
%
% \paragraph{Related work}
