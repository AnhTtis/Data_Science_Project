%!TEX root = ../generative_by_rl.tex

\section{Inference as Goal-Conditioned Reinforcement Learning}
\label{sec:inference_as_GCRL}

We assume that we have access to a training set $\{\tx^1,\ldots,\tx^N\}=:\cD\subseteq\cX$ of samples from some unknown distribution. Our goal is to perform inference on this training set where the candidates probability distributions are the final state distribution of a policy in a certain episodic MDP.

\paragraph{Markov Decision Problem (MDP)} We consider a loss-free episodic MDP $\cM= (\cX,\cY,H,p)$ where $\cX$ is the set of states (that also contains the training set $\cD$), $\cY$ the action set, $H$ the number of steps, $p_h(x'|x,y)$ is the transition probability from state~$x$ to state~$x'$ by taking the action $y$ at step $h\in[H]$, where $[H] := \{1,\dots,H\}$.

\paragraph{Policy, reach probability and value function}  A policy $\pi$ is a collection of functions $\pi_h : \cX \to \Delta(\cY)$ for all $h\in [H]$, where every $\pi_h$  maps each state to a probability distribution over actions.

Under policy $\pi$ a trajectory is generated as follows: an initial state $x_1 \sim p_0$ is sampled. Then for $h\in[H]$, given the current sate $x_h$, the agent samples an action $y_h\sim\pi_h(\cdot|x_h)$ and the next state $x_{h+1}\sim p_h(\cdot|x_h,y_h)$ is generated according to the transition probability. We denote by $p_h^\pi(x)$ the probability to reach the state $x$ in the MDP $\cM$ at step $h$ under the policy $\pi$. Similarly, we denote by $p^\pi(\tau)$ the probability distribution of a trajectory $\tau = x_1,a_1,\ldots,x_H,a_H, x_{H+1}$
under the policy $\pi$. Given two policies $\pi$ and $\pi'$ the Kullback-Leibler divergence between $p^\pi$ and $p^{\pi'}$ is, by the chain rule,
\[
\KL(p^\pi, p^{\pi'}) = \E^\pi \left[ \sum_{h=1}^H \KL\!\big(\pi(x_h),\pi'(x_h)\big) \right]\,,
\]
where $\E_\pi$ is the expectation under $p^\pi$.


% , receives a loss $\ell_h = \ell_h(x_h,y_y)$
% The value functions of policy $\pi$ is
% \[
% V^\pi = \E_\pi\left[ \sum_{h=1}^H  \ell_h\right]\,,
% \]
% where $\E_\pi$ is the expectation over trajectory generated with the policy $\pi$ in the MDP $\cM$.

\paragraph{Upper bound the negative log-likelihood} We assume as probabilistic model for the training set the reach probability of the last step $p_{H+1}^\pi(\cdot)$ \emph{parameterized by the policy} $\pi$. We then want to find the policy that minimizes the negative log-likelihood of the training set $L(\pi)$ where
\[
L(\pi) := \frac{1}{N}\sum_{\tx\in\cD} \log \frac{1}{p_{H+1}^\pi(\tx)}\,.
\]
Solving this optimization problem is difficult because the probability $p_{H+1}^\pi(\tx)$ is typically intractable. Similarly to variational inference~\citep{blei2017}, we will instead minimize \emph{an upper bound} on the negative log-likelihood.
For a fixed element $\tx\in\cD$, conditioned on the state-action pair $(x_H,y_H)$ it holds that
\begin{align}
    \log \frac{1}{p_{H+1}^\pi(\tx)} &= -\log\Big( \E_\pi \big[p_{H}(\tx|x_{H}, y_{H}) \big]Â \Big)\nonumber\\
     &\leq \!\min_{\pi'} \E_{\pi'}\! \left[\log\frac{1}{p_{H}(\tx|x_{H}, y_{H})}\right]\! +\KL(p^{\pi'},p^{\pi})  \,. \label{eq:ub_negloglikelihood}
\end{align}
Equation~\eqref{eq:ub_negloglikelihood} follows from the variational
formula for the moment generating function (see Lemma 1 in Appendix B) and it defines an upper-bound on the negative log-likelihood of $\tx$.

We can then define a surrogate loss parameterized by a policy $\pi$ and a family of goal-conditioned policies $(\pi^{\tx})_{\tx\in\cD}$, indexed by an element of the training set as:
\begin{align}
  \Lub(\pi, (\pi^{\tx})_{\tx\in\cD} ) &:= \frac{1}{N}\sum_{\tx\in\cD}  \E_{\pi^{\tx}} \left[\log \frac{1}{p_{H}(\tx|x_{H}, y_{H})}\right]+ \KL(p^{\pi^{\tx}},p^{\pi})\label{eq:def_surrogate}\,.
\end{align}
Using the fact that $\Lub$ is an upper-bound on the loss $L$ our problem becomes:
\begin{align*}
\min_{\pi}L(\pi)&\leq \min_{\pi}\min_{(\pi^{\tx})_{\tx\in\cD}} \Lub(\pi,(\pi^x)_{x\in\cX}) \\
&= \min_{\pi} \frac{1}{N}\sum_{\tx\in\cD}  \min_{\pi^{\tx}} \E_{\pi^{\tx}} \left[\log \frac{1}{p_{H}(\tx|x_{H}, y_{H})}\right]+ \KL(p^{\pi^{\tx}},p^{\pi})\,.
\end{align*}
% Our goal in this paper is to minimize this surrogate loss $\Lub$.
At a high level the minimization of $\Lub$ goes as follows. For each point $\tx\in\cD$ in the training set with goal-conditioned RL we learn a policy $\pi^{\tx}$ not too far from $\pi$ that leads to $\tx$ with high probability. Simultaneously we train the policy $\pi$ to reproduce trajectories from the policies $\pi^{\tx}$.


\paragraph{Inner minimization: goal-conditioned RL} If we fix the policy $\pi$ and a state $\tx\in\cD$ in the training set then solving~\eqref{eq:ub_negloglikelihood} is equivalent to solving a regularized \emph{goal-conditioned RL problem}. That is, we want to find a policy $\pi^{\tx}$ close to $\pi$ that lead to the goal $\tx$ with high probability.
Let
\begin{equation}
  \label{eq:def_loss}
  \ell_h^{\tx}(x,y) := \begin{cases}
  \log\dfrac{1}{p_{H}(\tx|x, y)} &\text{ if }h=H\\
  0 &\text{otherwise}
\end{cases}\,,
\end{equation}
be a loss function parameterized by the goal $\tx$, and let $\cM^{\tx} = (\cX,\cY,H,p,\ell^{\tx})$ be an MDP.
Then the minimization of~\eqref{eq:ub_negloglikelihood} is equivalent to solving the MDP $\cM^{\tx}$ regularized by the policy $\pi$. Precisely, we have
\begin{align*}
\min_{\pi^{\tx}}\E_{\pi^{\tx}} \left[\log\frac{1}{p_{H}(\tx|x_{H}, y_{H})}\right]\! +\!\KL(\pi^{\tx},p^{\pi})
\!= \min_{\pi^{\tx}} V^{\pi^{\tx},\tx}+ \KL(\pi^{\tx},p^{\pi})\,,
\end{align*}
where $V^{\pi^{\tx},\tx}$ is the value of the goal-conditioned policy $\pi^{\tx}$ in the MDP $\cM^{\tx}$. Although this goal-conditioned RL problem has already been studied by \citet{rudner2021outcome} and \citet{attias03a}, it is the first time to the best of our knowledge that this formulation is used as an intermediate task to build a generative model.
The link with variational inference is clear: we seek a policy $\pi'$ that approximates well the posterior distribution of a trajectory generated by $\pi$ conditioned on the fact that this trajectory reaches the goal $\tx$.

\paragraph{Outer minimization: supervised learning} Now, if we fix the family of goal-conditioned policies, minimizing the surrogate loss $\Lub(\cdot,(\pi^{\tx})_{\tx\in\cD})$ over the policy $\pi$ amounts to minimizing a convex combination of Kullback-Leibler divergences
\begin{equation}
  \label{eq:supervised_loss}
\argmin_\pi \Lub(\pi, (\pi^{\tx})_{\tx\in\cD}) = \argmin_\pi  \frac{1}{N}\sum_{\tx\in\cD} \KL(p^{\pi^{\tx}},p^{\pi})
\end{equation}
This optimization problem can be efficiently solved with supervised learning.
First, we sample goals according the the empirical distribution of the training set.
Then, for each goal, we generate a trajectory with the goal-conditioned policy, and collect state-action pairs.
We then use these state-action pairs to supervise the policy $\pi$.

\paragraph{Variational agent} In the sequel we call the pair $(\pi,(\pi^{\tx})_{\tx\in\cD})$ a variational agent (V-agent) made of
$\pi$ a supervised agent (S-agent) and $(\pi^{\tx})_{\tx\in\cD}$ a goal-conditioned agent (GC-agent).
