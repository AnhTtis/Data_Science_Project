%!TEX root = ../generative_by_rl.tex

\section{Experiments}
\label{sec:experiments}

We empirically assess the quality of our method on the task of image reconstruction and generation. Note that our goal here is to validate experimentally our new approach rather than providing a new state of the art method. 
To that end we conducted experiments on two publicly available datasets, namely
Fashion-MNIST~\citep{fashionmnist} and MNIST~\citep{deng2012mnist}.
In what follows we describe the experimental setup and present the results.

\subsection{Setup}
We compare our method with two types of generative models, Variational Auto-Encoders and Diffusion Models.
For the VAE baselines we use a convolutional VAE architecture with the state-of-the-art hyperparameters as \citet{Subramanian2020}.
For the DM baselines we use the publicly available implementation by \citet{song2021scorebased}\footnote{\url{https://github.com/yang-song/score_sde}}
to run experiments. In particular, we fix the number of time-steps to $1000$ and the forward diffusion process hyperparameters as in~\citet{hodenoising20}.
For the sake of comparison we parameterize the reverse process with the same architecture as in our method, that is, a four-layers \UNET~\citep{Ronneberger2015UNetCN} where each layer consists of convolution layers followed by group normalization and a ReLU activation function.
To model time-steps we do as follows: At each time-step $t\in[0, \dots, 1000]$ we use an embedding layer to compute an embedding the size of the image. We then reshape and concatenate this embedding as a set of extra channels to the input image.

For training our method we generated goal-conditioned trajectories as described in Section~\ref{sec:algorithm}.
We fixed the horizon $H=16$ and the number of proposed actions per step $A = 16$.
The sequence or rates $(\alpha_h)_{h\in[H]}$ is defined such that $\alpha_h = 1/h$ for $h=1,\dots,H$.
Similar to DMs, we parameterize the agents' proposal network with a four-layers \UNET,
where the dependence on the current step $h$ is also modeled with an embedding layer.
Both the GC-agent and S-agent selection networks are parameterized as Convolutional Neural Networks with three blocks each.
Further details on the architecture of our model and the baselines can be found in Appendix~\ref{app:net_arch}.
The rest of hyperparameters are chosen as follows:
The exploration parameter is defined as $\kappa = 0.05$, and $\rho = 0.06$.
We fixed the initial state to zero. The models were optimized using Adam optimizer~\citep{Kingma2015AdamAM} with a learning rate fixed to $0.0001$.
All models were trained until convergence.

\subsection{Results}

To compare our method with the baselines we collect the following quantities: Mean Squared Error (MSE) loss between the test
set and the test set reconstruction (Rec.), Fr\'echet Inception Distance (FID)~\citep{heusel2017gans} between the test set and its reconstruction (Rec.),
and FID of randomly generated samples with respect to the training set (Samples).
For the VAE, a random sample can be generated by feeding a sample from an isotropic Gaussian distribution (the prior of the VAE latent space) to the decoder.
To sample from the trained DMs we used the sampling method described by \citet{song2021scorebased}.
For our method, random samples are generated as described in Section~\ref{subsec:inference}.

\begin{table}[h!]
  \centering
  \begin{tabular}{lccccc}
    \toprule
    \multirow{2}{*}{Dataset} &
    \multirow{2}{*}{Method} &
      MSE &
      \multicolumn{2}{c}{FID} \\
      \cmidrule(r){4-5}
    &  & Rec. & Rec. & Samples \\
      \midrule
  \multirow{3}{*}{Fashion-MNIST} &  VAE & .13 & 45.09 & 60.06 \\
    & DM & \--- & \--- & \textbf{9.63} \\
    & Us & \textbf{.10} & \textbf{39.10} & 47.74 \\
    % D3 & 5.5 &  5.5 & 5.5 \\
    \midrule
    \multirow{3}{*}{MNIST} &  VAE & .12 & 16.49 & 27.96 \\
      & DM & \--- & \--- &  \textbf{1.94}\\
      & Us & \textbf{.06} &  \textbf{7.77} & 19.46 \\
    \bottomrule
  \end{tabular}
  \caption{Evaluation of VAE, DM, and our model by MSE between the test set and test sample reconstruction, FID (lower is better) 
  between test set and test sample reconstruction, and FID of randomly generated samples with respect to the training set.}
  \label{tab:results}
\end{table}


\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
     \includegraphics[width=1\linewidth]{figures/mnist_batch}
     \caption{MNIST}
     \label{fig:Ng1}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
     \includegraphics[width=1\linewidth]{figures/fashion-mnist_batch}
     \caption{Fashion-MNIST}
     \label{fig:Ng2}
  \end{subfigure}
  \caption[]{Four trajectories generated by the S-agent for MNIST (left) and Fashion-MNIST (right). Note that all trajectories depart from the same initial state. The source or randomness comes from the selection function of the agents.}
  \label{fig:traj}
\end{figure}

\begin{figure*}[h!]
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
     \includegraphics[width=1\linewidth]{figures/goals}
     \caption{Batch of goals from MNIST test set.}
     \label{fig:Ng1}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
     \includegraphics[width=1\linewidth]{figures/reconstructed}
     \caption{Reconstruction of batch.}
     \label{fig:Ng2}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
     \includegraphics[width=1\linewidth]{figures/generated}
     \caption{Random samples.}
     \label{fig:Ng2}
  \end{subfigure}
  \caption[]{Reconstruction and generation of images. We sampled a batch of $100$ images from the MNIST test set (left). We then generated trajectories where the selected actions were given by the GC-agent, in order to reconstruct the items in the batch (center). Finally, we randomly generated $100$ images with the S-agent (right). A qualitative analysis of the generated samples (right) shows that our generative model is able to produce a broad variety of images.}
  \label{fig:samples}
  \end{figure*}
  \vspace{-1.5cm}


We report our results in Table~\ref{tab:results}.
We first note that our method outperforms the VAE baseline in terms of MSE and FID by a large margin in both, test sample reconstruction and 
randomly generated images.
Our model lags behind DMs in terms of FID in both settings, reconstruction and randomly generated samples. 
This difference might be explained by the fact that we chose a very small horizon ($H=16$).
Exploring other values for $H$ is left as future work.

Our results confirm that the proposed model is not only able to accurately reconstruct the input but it is also able to randomly generate a rich variety 
of images.In Figure~\ref{fig:traj} we show two randomly generated trajectories, that is, trajectories produced by the S-agent, for the trained models 
in MNIST and Fashion-MNIST datasets.
A qualitative inspection of the reconstructed images and generated samples in Figure~\ref{fig:samples} shows
that our model effectively captures the underlying distribution of the data.
% \vspace{-.5cm}
