@article{quemener2013sidus,
author = {Quemener, Emmanuel and Corvellec, Marianne},
title = {SIDUS—the Solution for Extreme Deduplication of an Operating System},
year = {2013},
issue_date = {November 2013},
publisher = {Belltown Media},
address = {Houston, TX},
volume = {2013},
number = {235},
issn = {1075-3583},
abstract = {Use SIDUS in a variety of environments to provide unprecedented flexibility for both users and administrators.},
journal = {Linux J.},
month = {nov},
articleno = {3}
}


@InProceedings{wang2016dueling,
  title = 	 {Dueling Network Architectures for Deep Reinforcement Learning},
  author = 	 {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Hasselt, Hado and Lanctot, Marc and Freitas, Nando},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1995--2003},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/wangf16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/wangf16.html},
  abstract = 	 {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.}
}



@inproceedings{hasselt2016deep,
author = {Hasselt, Hado van and Guez, Arthur and Silver, David},
title = {Deep Reinforcement Learning with Double Q-Learning},
year = {2016},
publisher = {AAAI Press},
abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {2094–2100},
numpages = {7},
location = {Phoenix, Arizona},
series = {AAAI'16}
}





@InProceedings{germain2015made,
  title = 	 {MADE: Masked Autoencoder for Distribution Estimation},
  author = 	 {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {881--889},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/germain15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/germain15.html},
  abstract = 	 {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder’s parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.}
}


@inproceedings{oord2017neural,
 author = {van den Oord, Aaron and Vinyals, Oriol and kavukcuoglu, koray},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Discrete Representation Learning},
 url = {https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf},
 volume = {30},
 year = {2017}
}


@article{mnih2013atari,
  abstract = {We present the first deep learning model to successfully learn control
policies directly from high-dimensional sensory input using reinforcement
learning. The model is a convolutional neural network, trained with a variant
of Q-learning, whose input is raw pixels and whose output is a value function
estimating future rewards. We apply our method to seven Atari 2600 games from
the Arcade Learning Environment, with no adjustment of the architecture or
learning algorithm. We find that it outperforms all previous approaches on six
of the games and surpasses a human expert on three of them.},
  added-at = {2019-07-12T20:11:01.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/2a00ec4c09f5dc9b3f8a1836f4e02bb5d/lanteunis},
  interhash = {78966703f649bae69a08a6a23a4e8879},
  intrahash = {a00ec4c09f5dc9b3f8a1836f4e02bb5d},
  keywords = {},
  note = {cite arxiv:1312.5602Comment: NIPS Deep Learning Workshop 2013},
  timestamp = {2019-07-12T20:11:01.000+0200},
  title = {Playing Atari with Deep Reinforcement Learning},
  url = {http://arxiv.org/abs/1312.5602},
  year = 2013
}


@inproceedings{chow2021variational,
  title     = {Variational Model-based Policy Optimization},
  author    = {Chow, Yinlam and Cui, Brandon and Ryu, Moonkyung and Ghavamzadeh, Mohammad},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on
               Artificial Intelligence, {IJCAI-21}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Zhi-Hua Zhou},
  pages     = {2292--2299},
  year      = {2021},
  month     = {8},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2021/316},
  url       = {https://doi.org/10.24963/ijcai.2021/316},
}

@inproceedings{hodenoising20,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {6840--6851},
 publisher = {Curran Associates, Inc.},
 title = {Denoising Diffusion Probabilistic Models},
 url = {https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
 volume = {33},
 year = {2020}
}


@InProceedings{nichol21a,
  title = 	 {Improved Denoising Diffusion Probabilistic Models},
  author =       {Nichol, Alexander Quinn and Dhariwal, Prafulla},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8162--8171},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/nichol21a.html},
  abstract = 	 {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code and pre-trained models at https://github.com/openai/improved-diffusion.}
}


@inproceedings{kingma2014,
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  added-at = {2020-10-15T14:36:56.000+0200},
  author = {Kingma, Diederik P. and Welling, Max},
  biburl = {https://www.bibsonomy.org/bibtex/242e5be6faa01cba2587f4907ac99dce8/annakrause},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  eprint = {http://arxiv.org/abs/1312.6114v10},
  eprintclass = {stat.ML},
  eprinttype = {arXiv},
  file = {:http\://arxiv.org/pdf/1312.6114v10:PDF;:KingmaWelling_Auto-EncodingVariationalBayes.pdf:PDF},
  interhash = {a626a9d77a123c52405a08da983203cb},
  intrahash = {42e5be6faa01cba2587f4907ac99dce8},
  keywords = {cs.LG stat.ML vae},
  timestamp = {2021-02-01T17:13:18.000+0100},
  title = {{Auto-Encoding Variational Bayes}},
  year = 2014
}


@inproceedings{kingmagflow2018,
 author = {Kingma, Durk P and Dhariwal, Prafulla},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Glow: Generative Flow with Invertible 1x1 Convolutions},
 url = {https://proceedings.neurips.cc/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf},
 volume = {31},
 year = {2018}
}


@inproceedings{goodfellowgan2014,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Adversarial Nets},
 url = {https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
 volume = {27},
 year = {2014}
}



@inproceedings{dinh2017,
  author    = {Laurent Dinh and
               Jascha Sohl{-}Dickstein and
               Samy Bengio},
  title     = {Density estimation using Real {NVP}},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=HkpbnH9lx},
  timestamp = {Thu, 25 Jul 2019 14:25:58 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/DinhSB17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{song2019,
 author = {Song, Yang and Ermon, Stefano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Modeling by Estimating Gradients of the Data Distribution},
 url = {https://proceedings.neurips.cc/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{song2021scorebased,
  title={Score-Based Generative Modeling through Stochastic Differential Equations},
  author={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
  booktitle={International Conference on Learning Representations},
  year={2021},
  url={https://openreview.net/forum?id=PxTIG12RRHS}
}


@InProceedings{schaul15,
  title = 	 {Universal Value Function Approximators},
  author = 	 {Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1312--1320},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/schaul15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/schaul15.html},
  abstract = 	 {Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters θ. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.}
}



@inproceedings{andrychowicz2017,
 author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Hindsight Experience Replay},
 url = {https://proceedings.neurips.cc/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf},
 volume = {30},
 year = {2017}
}



@InProceedings{choi2021b,
  title = 	 {Variational Empowerment as Representation Learning for Goal-Conditioned Reinforcement Learning},
  author =       {Choi, Jongwook and Sharma, Archit and Lee, Honglak and Levine, Sergey and Gu, Shixiang Shane},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {1953--1963},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/choi21b/choi21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/choi21b.html},
  abstract = 	 {Learning to reach goal states and learning diverse skills through mutual information maximization have been proposed as principled frameworks for unsupervised reinforcement learning, allowing agents to acquire broadly applicable multi-task policies with minimal reward engineering. In this paper, we discuss how these two approaches {—} goal-conditioned RL (GCRL) and MI-based RL {—} can be generalized into a single family of methods, interpreting mutual information maximization and variational empowerment as representation learning methods that acquire function-ally aware state representations for goal reaching.Starting from a simple observation that the standard GCRL is encapsulated by the optimization objective of variational empowerment, we can derive novel variants of GCRL and variational empowerment under a single, unified optimization objective, such as adaptive-variance GCRL and linear-mapping GCRL, and study the characteristics of representation learning each variant provides. Furthermore, through the lens of GCRL, we show that adapting powerful techniques fromGCRL such as goal relabeling into the variationalMI context as well as proper regularization on the variational posterior provides substantial gains in algorithm performance, and propose a novel evaluation metric named latent goal reaching (LGR)as an objective measure for evaluating empowerment algorithms akin to goal-based RL. Through principled mathematical derivations and careful experimental validations, our work lays a novel foundation from which representation learning can be evaluated and analyzed in goal-based RL}
}

@article{rudner2021outcome,
  title={Outcome-Driven Reinforcement Learning via Variational Inference},
  author={Rudner, Tim GJ and Pong, Vitchyr H and McAllister, Rowan and Gal, Yarin and Levine, Sergey},
  journal={arXiv preprint arXiv:2104.10190},
  year={2021}
}


@InProceedings{attias03a,
  title = 	 {Planning by Probabilistic Inference},
  author =       {Attias, Hagai},
  booktitle = 	 {Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics},
  pages = 	 {9--16},
  year = 	 {2003},
  editor = 	 {Bishop, Christopher M. and Frey, Brendan J.},
  volume = 	 {R4},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {03--06 Jan},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/r4/attias03a/attias03a.pdf},
  url = 	 {https://proceedings.mlr.press/r4/attias03a.html},
  abstract = 	 {This paper presents and demonstrates a new approach to the problem of planning under uncertainty. Actions are treated as hidden variables, with their own prior distributions, in a probabilistic generative model involving actions and states. Planning is done by computing the posterior distribution over actions, conditioned on reaching the goal state within a specified number of steps. Under the new formulation, the toolbox of inference techniques be brought to bear on the planning problem. This paper focuses on problems with discrete actions and states, and discusses some extensions.},
  note =         {Reissued by PMLR on 01 April 2021.}
}

@InProceedings{leslie1993,
title = {Learning to Achieve Goals},
author = {Leslie Pack Kaelbling},
year = {1993},
pages = 	 {1094-8},
volume = 	 {2},
url = {http://people.csail.mit.edu/lpk/papers/ijcai93.ps},
booktitle = {Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence},
address = {Chambery, France},
publisher = {Morgan Kaufmann},
}


@inproceedings{fellows2019,
 author = {Fellows, Matthew and Mahajan, Anuj and Rudner, Tim G. J. and Whiteson, Shimon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {VIREL: A Variational Inference Framework for Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2019/file/582967e09f1b30ca2539968da0a174fa-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{toussaint2009,
author = {Toussaint, Marc},
title = {Robot Trajectory Optimization Using Approximate Inference},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553508},
doi = {10.1145/1553374.1553508},
abstract = {The general stochastic optimal control (SOC) problem in robotics scenarios is often
too complex to be solved exactly and in near real time. A classical approximate solution
is to first compute an optimal (deterministic) trajectory and then solve a local linear-quadratic-gaussian
(LQG) perturbation model to handle the system stochasticity. We present a new algorithm
for this approach which improves upon previous algorithms like iLQG. We consider a
probabilistic model for which the maximum likelihood (ML) trajectory coincides with
the optimal trajectory and which, in the LQG case, reproduces the classical SOC solution.
The algorithm then utilizes approximate inference methods (similar to expectation
propagation) that efficiently generalize to non-LQG systems. We demonstrate the algorithm
on a simulated 39-DoF humanoid robot.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {1049–1056},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@inproceedings{toussaint2006,
author = {Toussaint, Marc and Storkey, Amos},
title = {Probabilistic Inference for Solving Discrete and Continuous State Markov Decision Processes},
year = {2006},
isbn = {1595933832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143844.1143963},
doi = {10.1145/1143844.1143963},
abstract = {Inference in Markov Decision Processes has recently received interest as a means to
infer goals of an observed action, policy recognition, and also as a tool to compute
policies. A particularly interesting aspect of the approach is that any existing inference
technique in DBNs now becomes available for answering behavioral question--including
those on continuous, factorial, or hierarchical state representations. Here we present
an Expectation Maximization algorithm for computing optimal policies. Unlike previous
approaches we can show that this actually optimizes the discounted expected future
return for arbitrary reward functions and without assuming an ad hoc finite total
time. The algorithm is generic in that any inference technique can be utilized in
the E-step. We demonstrate this for exact inference on a discrete maze and Gaussian
belief state propagation in continuous stochastic optimal control problems.},
booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
pages = {945–952},
numpages = {8},
location = {Pittsburgh, Pennsylvania, USA},
series = {ICML '06}
}




@inproceedings{pong2018,
  author    = {Vitchyr Pong and
               Shixiang Gu and
               Murtaza Dalal and
               Sergey Levine},
  title     = {Temporal Difference Models: Model-Free Deep {RL} for Model-Based Control},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
               Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2018},
  url       = {https://openreview.net/forum?id=Skw0n-W0Z},
  timestamp = {Thu, 25 Jul 2019 14:25:44 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/PongGDL18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{karras2018progressive,
  title={Progressive Growing of GANs for Improved Quality, Stability, and Variation},
  author={Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  booktitle={International Conference on Learning Representations},
  year={2018}
}




@inproceedings{karrasprogressive2018,
  author    = {Tero Karras and
               Timo Aila and
               Samuli Laine and
               Jaakko Lehtinen},
  title     = {Progressive Growing of GANs for Improved Quality, Stability, and Variation},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
               Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2018},
  url       = {https://openreview.net/forum?id=Hk99zCeAb},
  timestamp = {Thu, 04 Apr 2019 13:20:09 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/KarrasALL18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{brockDS19,
  author    = {Andrew Brock and
               Jeff Donahue and
               Karen Simonyan},
  title     = {Large Scale {GAN} Training for High Fidelity Natural Image Synthesis},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=B1xsqj09Fm},
  timestamp = {Thu, 25 Jul 2019 13:03:18 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/BrockDS19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{oordpixelcnn2016,
  title = 	 {Pixel Recurrent Neural Networks},
  author = 	 {Oord, Aaron Van and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1747--1756},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/oord16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/oord16.html},
  abstract = 	 {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.}
}



@InProceedings{sohldickstein2015,
  title = 	 {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author = 	 {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2256--2265},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  abstract = 	 {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}
}

@misc{goodfellow2017nips,
      title={NIPS 2016 Tutorial: Generative Adversarial Networks},
      author={Ian Goodfellow},
      year={2017},
      eprint={1701.00160},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@InProceedings{rezende14,
  title = 	 {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
  author = 	 {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {1278--1286},
  year = 	 {2014},
  editor = 	 {Xing, Eric P. and Jebara, Tony},
  volume = 	 {32},
  number =       {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/rezende14.pdf},
  url = 	 {https://proceedings.mlr.press/v32/rezende14.html},
  abstract = 	 {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning.   Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound.  We develop stochastic backpropagation – rules for gradient backpropagation through stochastic variables – and   derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models.  We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to  generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.}
}

@inproceedings{songME21,
  author    = {Jiaming Song and
               Chenlin Meng and
               Stefano Ermon},
  title     = {Denoising Diffusion Implicit Models},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=St1giarCHLP},
  timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/SongME21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{dhariwal2021diffusion,
      title={Diffusion Models Beat GANs on Image Synthesis},
      author={Prafulla Dhariwal and Alex Nichol},
      year={2021},
      eprint={2105.05233},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{blei2017,
   title={Variational Inference: A Review for Statisticians},
   volume={112},
   ISSN={1537-274X},
   url={http://dx.doi.org/10.1080/01621459.2017.1285773},
   DOI={10.1080/01621459.2017.1285773},
   number={518},
   journal={Journal of the American Statistical Association},
   publisher={Informa UK Limited},
   author={Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
   year={2017},
   month={Apr},
   pages={859–877}
}


@article{parisi1980nh,
    author = "Parisi, G.",
    title = "{Correlation Functions and Computer Simulations}",
    reportNumber = "LNF-80/54-P",
    doi = "10.1016/0550-3213(81)90056-0",
    journal = "Nucl. Phys. B",
    volume = "180",
    pages = "378",
    year = "1981"
}


@inproceedings{brown20,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{razavi2019,
 author = {Razavi, Ali and van den Oord, Aaron and Vinyals, Oriol},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generating Diverse High-Fidelity Images with VQ-VAE-2},
 url = {https://proceedings.neurips.cc/paper/2019/file/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Paper.pdf},
 volume = {32},
 year = {2019}
}



@inproceedings{brock2019,
  author    = {Andrew Brock and
               Jeff Donahue and
               Karen Simonyan},
  title     = {Large Scale {GAN} Training for High Fidelity Natural Image Synthesis},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=B1xsqj09Fm},
  timestamp = {Thu, 25 Jul 2019 13:03:18 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/BrockDS19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{vandenOord2016,
title	= {WaveNet: A Generative Model for Raw Audio},
author	= {Aäron van den Oord and Sander Dieleman and Heiga Zen and Karen Simonyan and Oriol Vinyals and Alexander Graves and Nal Kalchbrenner and Andrew Senior and Koray Kavukcuoglu},
year	= {2016},
URL	= {https://arxiv.org/abs/1609.03499},
booktitle	= {Arxiv}
}


@book{sutton1998introduction,
  title={Introduction to reinforcement learning},
  author={Sutton, Richard S and Barto, Andrew G and others},
  volume={135},
  year={1998},
  publisher={MIT press Cambridge}
}

@article{donsker1983,
author = {Donsker, M. D. and Varadhan, S. R. S.},
title = {Asymptotic evaluation of certain markov process expectations for large time. IV},
journal = {Communications on Pure and Applied Mathematics},
volume = {36},
number = {2},
pages = {183-212},
doi = {https://doi.org/10.1002/cpa.3160360204},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.3160360204},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.3160360204},
year = {1983}
}


@article{deng2012mnist,
 title={The mnist database of handwritten digit images for machine learning research},
 author={Deng, Li},
 journal={IEEE Signal Processing Magazine},
 volume={29},
 number={6},
 pages={141--142},
 year={2012},
 publisher={IEEE}
}

@article{fashionmnist,
  author    = {Han Xiao and
               Kashif Rasul and
               Roland Vollgraf},
  title     = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning
               Algorithms},
  journal   = {CoRR},
  volume    = {abs/1708.07747},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.07747},
  eprinttype = {arXiv},
  eprint    = {1708.07747},
  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1708-07747.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Ronneberger2015UNetCN,
  title={U-Net: Convolutional Networks for Biomedical Image Segmentation},
  author={Olaf Ronneberger and Philipp Fischer and Thomas Brox},
  booktitle={MICCAI},
  year={2015}
}

@article{Kingma2015AdamAM,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2015},
  volume={abs/1412.6980}
}


@article{heusel2017gans,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{van2017neural,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{Subramanian2020,
  author = {Subramanian, A.K},
  title = {PyTorch-VAE},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/AntixK/PyTorch-VAE}}
}
