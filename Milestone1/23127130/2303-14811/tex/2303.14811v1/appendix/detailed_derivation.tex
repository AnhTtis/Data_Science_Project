%!TEX root = ../generative_by_rl.tex

\section{Derivations of Section~\ref{sec:inference_as_GCRL}}
\label{app:proofs_derivations}

In this appendix we detail the missing derivations of Section~\ref{sec:inference_as_GCRL}.

\begin{lemma}
For any policy $\pi$, state $x$,
\label{lem:ub_neg_loglikelihood}
\[-\log\Big( \E_\pi \big[p_{H}(x|x_{H}, y_{H}) \big] \Big)
 \leq  \min_{\pi' \text{ policy}} \E_{\pi'} \left[\log\frac{1}{p_{H}(x|x_{H}, y_{H})}\right] +\KL(p^{\pi'},p^{\pi})\,.\]
\end{lemma}
\begin{proof}
Thanks to the Donsker-Varadhan's formula \citep{donsker1983} it holds
\[
-\log\Big( \E_\pi \big[p_{H}(x|x_{H}, y_{H}) \big] \Big) = \min_{q\in \Delta(\mathrm{Traj})} \E_q\left[\log\frac{1}{p_{H}(x|x_{H}, y_{H})} \right] + \KL(q,p^\pi)\,,
\]
where $\Delta(\mathrm{Traj})$ is the set of probability distributions supported on the trajectory $\tau = (x_1,y_1,\ldots,x_H,y_H)$ up to step $H$ with $x_h\in\cX$ and $y_h\in\cY$. Restricting the infimum to probability distributions $q=p^{\pi'}$ induced by some policy $\pi'$ allows us to conclude.
\end{proof}
