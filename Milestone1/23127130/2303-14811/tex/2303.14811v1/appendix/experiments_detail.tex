%!TEX root = ../generative_by_rl.tex

\section{Network architectures}
\label{app:net_arch}
In this appendix we describe the architecture of the proposal, Q-value and selection networks.

\subsection{Proposal network}

The proposal network is parameterized as a four-layer \UNET\footnote{The \UNET implementation is publicly available at https://github.com/milesial/Pytorch-UNet.}.
Each layer has two blocks consisting of a convolution layer followed by group normalization and a ReLU activation function.

The different networks used in our model also depends on discrete time-step $h\in[H]$. We incorporate this information in the model as follows. At each time-step $h$ the proposal network receives an input of the form $(x_{h},h,a_h)$. The current state $x_h$ is of shape $w \times t \times c$, where $w$ is the width,
$t$ is the height, and $c$ is the number of channels
(for example, $c = 3$ for RGB images). The selected index $a_h$ indicates which of the $A$ available proposals is to be
chosen at the current time-step $h$.
We first calculate an embedding $\mathbf{h}_{a,h}$ representing $h$ and $a$ through an Embedding layer to
obtain a tensor the same size as $x_{h}$. We then concatenate $x_{h}$ and $\mathbf{h}_{a,h}$ on the channel axis to obtain a tensor of shape $w\times t\times 2c$ that is fed to the \UNET.
Finally, the \UNET outputs the tensor $x_{h+1}$ of shape $w\times t\times c$.
This process is described as follows
\begin{equation}
  \label{eq:propnet}
  \begin{aligned}
    h'_{a,h} &= Ah + a \\
    \mathbf{h}_{a,h} &= \mathtt{Emb}(h'_{a,h}) \\
    x_{h+1} &= \UNET([x_h, \mathbf{h}_{a,h}])\,,
  \end{aligned}
\end{equation}
where $[\cdot,\cdot]$ represents concatenation of 3D tensors on the channel axis. At each pass the network generates the proposal that corresponds to the specified index.
The output of each pass will then have the same shape as the inputs. We provide a detailed architecture of the network in Figure~\ref{fig:pnet}.

Note that, with this embedding of the time-step and selection the proposal network can memorize at most $H\times A$ distinct images. Thus combining proposals by choosing the selections in a trajectory is essential to be able to model a rich distribution.

\subsection{Selection network}

The selection network architecture is the same for both agents.
It consists of three blocks of convolution layers followed by a group normalization and a ReLU activation function.
The network outputs a tensor of size $\texttt{batch\_size}\times A$ corresponding to the selected actions for each
element in the batch. The embedding of the time-step follows the same method as in the proposal network.
Figure~\ref{fig:snet} presents a sketch of the architecture we used for our experiments.
The batch size was set to $128$, proposal size $A = 16$ and number of steps $H = 16$.

\subsection{Q-value selection network}

The architecture of the Q-value network is the same as the one of the selection network except for the last 
layer where the output is not normalized to obtain a probability distribution.
For the last layer we use a Dueling Network architecture \citep{wang2016dueling}.


\begin{figure}
  \includegraphics[width=\textwidth]{figures/propnet_torchviz}
  \caption{Proposal network architecture (shared between both agents) with batch size $128$ and number of steps $H = 16$.}
  \label{fig:pnet}
\end{figure}

\newpage
\begin{figure}
  \includegraphics[width=\textwidth]{figures/qnet_torchviz}
  \caption{Selection network architecture for the GC-agent and S-agent with batch size $128$ and selection size $A = 16$.}
  \label{fig:snet}
\end{figure}
