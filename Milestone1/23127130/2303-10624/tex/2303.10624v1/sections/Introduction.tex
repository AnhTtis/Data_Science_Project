\section{Introduction}\label{sec:intro}
Data is created at the edge and often owned by individuals or groups who take data privacy seriously. The exponentially growing data usage in deep learning (DL) \cite{big-data} has triggered massive collections of user (often anonymous) data by large organizations leading to concerns about data privacy \cite{split_med,ml_priv,ml_priv2}. Thus, it is desirable to develop high-performance distributed machine learning (DML) systems that run on low-cost,  thin clients without compromising data privacy. 

In Federated Learning (FL) \cite{split_comp_fed, split_fed, fed_learn_rev, fed_learn_med} each client receives a base model from a secure central server, and trains and updates the model parameters according to its own personal data. These updated models generated by a number of clients are then aggregated securely \cite{fed_learn_rev, fed_priv} resulting in a more accurate and generalized model. The FL architecture makes it particularly difficult to deploy DL models on resource-constrained (thin) clients such as mobile phones, sensors, smart cameras, etc. due to their high memory, storage, computing, and energy requirements during training \cite{HW_Eval}. 

The split learning (SL) framework \cite{split_main}, provides a promising alternative for thin clients. It splits the DL model into parts and offloads resource-intensive parts to a central server. The clients are responsible for updating the parameters only in the parts of the model that are owned by them, while the server is responsible for updating parameters for its part of the model. \cite{HW_Eval} has raised concerns such as slow training rate, low accuracy, and high resource utilization with SL for thin clients. Most of the papers in split learning \cite{split_comp_fed, split_fed, split_fed_edge_iot, split_guard, split_label_leak, split_label_protect, split_locfedmix, split_med, split_no_peek, split_scale} have only focused on a two-stage architecture where input data remains with the clients and the labels are shared with the server. For many applications, labels may reveal sensitive information such as diseases present, travel history, purchase history, search history, etc. for each client and thus can't be shared. 

In this paper, we develop a three-stage architecture (Fig \ref{fig: fig2}) where the server is simply an offloading device with no information whatsoever about the applications that the clients are running or the labels with the clients. Moreover, for aggregation of the client-side layers, we employ a separate secure client model averaging server. Thus as opposed to traditional two-stage split learning, the servers here do not have access to labels, thus making the model inversion attacks \cite{Model-Inversion, MI2} etc difficult.  

We make the following contributions through our Personalized Fair Split Learning (PFSL) framework which enables even \textbf{thin clients} to realize their goal of training \textbf{personalized DL models}: 

\begin{itemize}
\item{\textbf{Enable Parallel U-Shaped SL with Label privacy} by implementing a highly scalable, three-stage SL architecture, which doesn't require the clients to share labels of their data with the servers (Fig \ref{fig: fig2}). This was proposed in \cite{split_main} but ours is the first distributed/parallel implementation and detailed evaluation on real-life benchmarks. 
}
\item{\textbf{Adapt Transfer Learning } to minimize the resources needed on the client's end to achieve an accurate model. With detailed experiments on real-life benchmarks, we demonstrate higher accuracy than standard FL \cite{fed_learn}, FL with transfer learning \cite{FL-TL} and SL variants \cite{split_fed} and faster convergence minimizing computation and communication overheads.}

\item{\textbf{Personalized SL} for meeting the specific objectives such as personalization, and generalization of clients models. For example, one client (pathologist) may want their model to be more accurate for the images captured by the medical device in their lab (personalization). Another client (large pathology lab) may want their model to work well across all of the devices (generalization).}

\item{\textbf{Fairness:} Implemented and evaluated ``performance fairness'' under a work fairness constraint which ensures that the clients perform a similar amount of work, regardless of the amount of data they possess. In contrast, most FL and SL variants require clients with more data to work more during training. Furthermore, when compared to existing approaches, the performance fairness (measured as the standard deviation of test accuracy across clients) under PFSL is consistently lower.}


\item{\textbf{Reproducible Code:} To facilitate research in SL by the community, we have developed a modular, high-performance GPU-accelerated implementation of our framework\cite{Manas_Wadhwa_and_Gagan_Gupta_and_Ashutosh_Sahu_and_Rahul_Saini_and_Vidhi_Mittal_PFSL_2023}.  This allows us full-scale distributed learning system simulation, in which hundreds of thin clients can train their models. Our framework allows quick convergence based on global validation accuracy. Furthermore, a client can enter and leave the system whenever they wish to do so. This is realistic for thin, edge devices deployed remotely where the network connections may not persist for too long. 

}
\end{itemize}

\begin{figure}[h]
\includegraphics[width=\linewidth]{img/fig_ovr1.png}
\caption{PFSL framework supports training DL models on resource-constrained devices by off-loading heavy computation to an offloading server. Data and its labels reside with the clients and the clients only train a small portion of the DL model.
  \vspace{-0.5em}} 
\label{fig: fig2}
\end{figure}

In our framework, any DL model is split into three parts: Client-Front, Client-Back, and Server-Central layers as shown in Fig \ref{fig: fig2}. The number of layers in any of the parts is configurable, depending on the complexity of the task and the resources available to the clients. The number of client layers can be kept small (1 or 2) to ensure that highly resource-constrained thin clients can train their models using our framework. The weights of some of the layers can be frozen if they are obtained from a pre-trained model and the weights of unfrozen layers can be updated (transfer learning). 

In \cite{Jiang19-meta}, the authors state three objectives for FL: solid initial model, improved personalization models,  and fast convergence. We have designed PFSL to achieve all three by starting from a pre-trained model and focusing on its training for achieving high performance on general test distribution. This is critical to achieving a solid initial model and fast convergence. Furthermore, we have adapted standard approaches of personalization in FL \cite{P1, P2, P3, P4} to SL by implementing a lightweight step for the personalization of client layers which is suitable for thin clients. Instead of implementing personalization in a multi-task framework as in \cite{ditto_main} which is very resource intensive, we make it coarse-grained by first running a \emph{``generalization''} phase for all clients and introducing an additional \emph{``personalization''} phase to boost their accuracy on their personal data distribution. Here, we use early stopping \cite{EarlyStopping} for regularization. We demonstrate through detailed experiments, that both the clients with more data points and fewer data points can achieve significant performance gains while working equitably.

We present relevant background and discuss related work in Sec \ref{sec:rel_work}. A complete description of our PFSL Framework and relevant notations are presented in Sec \ref{sec:background_sys_model}. We describe our algorithms in detail in Sec \ref{sec:Algorithms}, evaluate their performance and compare with the state-of-the-art in Sec \ref{sec:experiments_and_results}, and draw conclusions in Sec \ref{sec:conclusion}.