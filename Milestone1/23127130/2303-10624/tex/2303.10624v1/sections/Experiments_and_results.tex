\section{Experiments and Results}
\label{sec:experiments_and_results}
We have implemented our framework, PFSL, in PyTorch. Our implementation is modular and allows specifying the number of clients, DNN model, number of the front, central and back layers, number of data points per client, etc. We have implemented state-of-the-art DML algorithms: FL, FL\_TL \cite{fl_tl}, SL \cite{split_main}, SFLv1, and SFLv2 \cite{split_fed} as described in their papers and reproduced their results for the validating our implementations. For experimentation, we used our workstation with Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz, with NVIDIA RTX A6000 GPU.

In all our experiments, we use ResNet-18 \cite{resnet18 } with pre-trained weights, which provides a good balance between accuracy and compute resource requirements. We use 2 ophthalmic imaging datasets for blindness detection: APTOS\cite{aptos2019blindness} and EyePACS \cite{kaggleKaggleDataset} and normalize image size to 224x224. We have used several image classification benchmarks: MNIST with 10 classes of hand-written digits \cite{MNIST},  F-MNIST\cite{fmnist} with 10 classes of fashion products, and CIFAR-10\cite{cifar-100} with 10 classes of objects. The size of input images in these benchmarks is 32x32. Since ResNet-18 was pre-trained with 224x224 size and weights of the initial layers were frozen for all algorithms with transfer learning (PFSL, FL\_TL), these images had to be scaled to 224x224 for them to achieve good performance. 

In the real world, good quality labeled data is a scarce resource. The clients are heterogeneous and their data distributions of clients are often non-i.i.d. Similar to Federated Learning, PFSL is applicable for diverse applications. Thus, we carefully evaluate performance and fairness in practical scenarios/settings. We consider the following settings:
\begin{itemize}
\item{S1: Labels are uniformly distributed, number of labeled data points per client is the same but small.}
\item{S2: Labels are non-uniformly distributed, the number of labeled data points per client is the same but small.}
\item{S3: Labels are uniformly distributed. One client has a high number of labeled data points and the remaining have small.}
\item{S4: All clients have a high number of labeled data points for MNIST, F-MNIST, and CIFAR-10.}
\item{S5: 1000 clients in the system each participating only for 1 epoch, and 50\% of them dropping out before completing training.}
\item{S6: Clients have data coming from different benchmarks: APTOS and EyePACS.}
\end{itemize}

For scenarios S1, S2, S3, and S5, we use CIFAR-10 as it is quite complex, and achieving good accuracy is non-trivial. Apart from the existing DML algorithms, for performance comparison, we also include a centralized model assuming that it has access to the labeled data from all the clients. It is expected to have the highest performance but can compromise data privacy. Hence, the goal of DML algorithms is to equal their performance without compromising accuracy. We have two versions of Central models (with and without TL) to compare with the performance of relevant algorithms. To quantify the benefit of DML, we also compare it with the client's performance by performing training locally without participating in DML. We call this individual training. For individual clients, starting from pre-trained weights is best, so only that is reported (as Individual\_TL). 

\begin{figure}[H]
\includegraphics[width=\linewidth]{img/Setting1_final_plot.png}
\caption{S1: Test Accuracy for DML algorithms vs Datapoints} 
\label{Test_setting1}
\end{figure}
\vspace{-1 em}

\subsection{Setting 1: Small Sample Size (Equal), i.i.d.}
Each client has a very small number of labeled data points, and all these samples are distributed identically across clients. The number of data points per client is slowly increased to study the benefits of having more data per client on the global model's accuracy. For each algorithm, we keep the global epochs fixed at 100 and the number of participating clients at 10. We evaluate each client on a CIFAR-10 test set having 2000 data points with the same distribution and report the average test accuracy. Since the test and train data distributions across all clients are the same, we run our algorithm only till Generalization Phase 1. For FL\_TL, the same number of layers were unfrozen at the client's side as PFSL for a fair comparison.

There are several interesting observations from Fig \ref{Test_setting1}. PFSL achieves accuracy close to the Central model (with TL) with the complete dataset. Its accuracy exceeds that of FL\_TL by 10\%. It is clear from this experiment, that for realistic settings with small sample size, transfer learning is very helpful, since Individual\_TL also exceeds the accuracy of FL, SL, SFLv1, and SFLv2 which attempt to train from scratch.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\linewidth]{img/Setting2_final_plot.png}
\caption{Data Distribution among clients where larger circle indicates more number of training samples for that particular class and client  } 
\label{Setting1_distribution_plot}
\end{figure}

\subsection{Setting 2: Small Sample Size (Equal), non-i.i.d.}

In this scenario, we consider the clients to have a small sample size and different data distributions, as shown in Fig \ref{Setting1_distribution_plot}. There are a total of 10 clients, and each client has 500 labeled data points. We model a situation where every client has more labeled data points from a subset of classes \emph{(prominent classes)} and less from the remaining classes. We chose to experiment with heavy label imbalance and diversity. The test distribution of each client is the same as its train distribution providing an incentive for them to run \textbf{Personalization Phase 2} as well. To evaluate algorithms in this setting, we consider the average F1 scores of the \emph{prominent classes} for each client on its respective test set and then average these over all the clients. As the data distribution across all clients differs, we run our algorithm completely until Phase 2. We run Phase 1 till convergence and Phase 2 again till convergence. 

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|l|}
    \hline
        \textbf{Algorithm} & \textbf{F1 score} & \textbf{Epochs} & \textbf{Test Standard Deviation}\\ \hline
        FL & 0.505 & 100 & 8\\ \hline
        SL & 0.794 & 50 & 4.25\\ \hline
        SFLv1 & 0.749 & 50 & 5\\ \hline
        SFLv2 & 0.773 & 50 & 4.25\\ \hline
        PFSL$_{BL\_3} (gen)$ & \textbf{0.816}  & \textbf{24} & \textbf{5.25}\\ \hline
        PFSL$_{BL\_3} (pers)$ & \textbf{0.953} & \textbf{26} & \textbf{2}\\ \hline
        PFSL$_{BL\_2} (gen)$ & \textbf{0.79}  & \textbf{24} & \textbf{5.75}\\ \hline
        PFSL$_{BL\_2} (pers)$ & \textbf{0.95} & \textbf{30} & \textbf{2.1}\\ \hline
        \end{tabular}
    \caption{S2: Average F1 Scores of 10 clients}
    \label{Setting1: f1 table}
\end{table}

From Table \ref{Setting1: f1 table}, we can observe that the average F1 score of Split learning and its variants is much better than Federated learning. Our algorithm, PFSL, gives the best average F1 scores. This happens because phase 2 allows the clients to train unique models suited to their own train datasets. Moreover, the generalization phase of our algorithm ends at the 25th epoch with better average F1 scores than the other algorithms achieve at 100 epochs. Our lightweight personalization phase results in a significant boost of 0.14 in average F1 scores. PFSL$_{BL\_3}$ has used three back layers at the client while  PFSL$_{BL\_2}$ used only two back layers at the client, which is our default. We observe that increasing the number of layers at the back model only affects the number of epochs needed to converge as the F1 scores converge to the same value in both cases. We notice that once the clients have converged to a generalized model in our framework, its personalization is very quick (2 global epochs in $PFSL_{BL\_3}$ and 6 global epochs in $PFSL_{BL\_2}$). 

The Test Accuracy Standard deviation column shows the values of Performance Fairness of different DML algorithms using the standard deviation of test performance of the clients in different frameworks. We can observe from Table \ref{Setting1: f1 table} that PSFL shows comparable performance fairness to others in the generalization phase. But after running the personalization phase, the fairness increases dramatically, indicating the ability of all the clients to converge easily on their respective personalized models, given the starting generalized weights. This also shows that the weights obtained from the generalization phase need to be fine-tuned to fit individual data better, as described in \cite{ditto_main, P1, P2, P3, P4} to improve fairness. We believe that in a practical setting, the ability to personalize private models is very attractive, and the performance gain and high-performance fairness provide an incentive to clients to work together for the common good while still enjoying the competitive edge.

\subsection{Setting 3: Small Sample Size (Unequal), i.i.d.}
Very often, the participating clients may have a different sample size. Most clients usually have fewer data points, and a select few have a much larger dataset. In this situation, the clients with more data tend to think that participation in distributed training with the clients (who may have fewer data) won't greatly benefit them, and they may lose their competitive edge. Thus, they can decide to forego participating in training completely.

To simulate the above-described situation, we consider 11 clients where the \emph{Large client} has 2000 labeled data points while the other ten small clients have 150 labeled data points, each distributed identically. Note that the class distributions among all the clients are the same. For evaluation purposes, we consider a test set having 2000 data points with an identical distribution of classes as the train set. Firstly, we calculate the average test accuracy when only 10 small clients $C_1 - C_{10}$ with 150 data points each participate. In the next case, all 11 clients (small and Large) participate in training, and we calculate the first client's test accuracy individually and the remaining clients' average test accuracy. We also calculate the performance achieved by the \emph{Large client} if it begins transfer learning on a pre-trained model utilizing only its 2000 labeled data points. To give incentive to the Large client, its test accuracy must increase significantly by participating in the learning with other clients with fewer data.

Existing works have not focused on achieving work fairness when the number of training data points per client is imbalanced. We consider that it is unfair to make the \emph{Large client} work ten times more than each small client if it has ten times more data than them. As we have explained before, in PFSL, we ensure that the number of batches and the batch size with each client in a global epoch remain the same. We calculate the metrics of this setting on our algorithm after ensuring this fairness constraint. Also, since the data distributions across all clients are the same, we run our algorithm only till Phase 1.

We present the Average test and train performance when only the small clients participate in DML training in Table \ref{setting2_t1}. It is evident that PFSL achieves the highest accuracy and minimizes over-fitting. Next, we present the Average test performance of small and \emph{Large client} when all the 11 clients participate in DML training in Table \ref{setting2_t2}. We can see a significant increment in their test accuracies in all the algorithms. This shows that the small clients greatly benefit from the \emph{Large client}. 

If \emph{Large client} is only trained on its dataset of 2000 data points, using a pre-trained model and transfer learning, then the test accuracy is 83.  Table \ref{setting2_t2} shows that only PFSL can give an incentive to such clients by providing a 2\% increase in accuracy. Moreover, our results are calculated after incorporating work fairness. We do not force the \emph{Large client} with more data to do more work in a local epoch than the other smaller clients, yet provide tangible benefits and incentives to participate in DML. We observe that when we make the \textit{Large client} do more work, its accuracy does not increase significantly (2\% over the previous) but its work done per global epoch increases by 16 times. Thus, by work fairness, we ensure high performance for this client with a far lesser workload.  

\begin{table}
\setlength{\tabcolsep}{2.5pt}
\centering
    \begin{tabular}{|l|l|l|}
    \hline
        \textbf{Algorithm} & \textbf{Avg $C_1 - C_{10}$ Test} & \textbf{Avg $C_1 - C_{10}$ Train} \\ \hline
        FL & 48.56 & 86.25 \\ \hline
        SL & 52.42 & 88.01 \\ \hline
        SFLv2 & 50.43 & 85.35 \\ \hline
        SFLv1 & 37.37 & 81.23 \\ \hline
        \textbf{PFSL} & \textbf{81.42} & 100 \\ \hline
    \end{tabular}
    \caption{ S3: Test and Train accuracies when only  $C_1 - C_{10}$ participate in DML}
    \label{setting2_t1}
    \bigskip % or \bigskip\bigskip
\begin{tabular}{|l|l|l|}
    \hline
        \textbf{Algorithm} & \textbf{Avg $C_1 - C_{10}$ Test } & \textbf{\emph{Large Client} Test} \\ \hline
        FL & 59.02  & 59.15 \\ \hline
        SL & 63.41  & 64.64 \\ \hline
        SFLv2 & 61.01  & 61.96 \\ \hline
        SFLv1 & 56.76 & 62.12 \\ \hline
        \textbf{PFSL} & \textbf{84.62}  & \textbf{84.64} \\ \hline
    \end{tabular}
    \caption{S3: Test Accuracies of Clients $C_1 - C_{10}$ and \emph{Large Client} when all participate in DML}
    \label{setting2_t2}
    
\end{table}

\subsection{Setting 4: A large number of data samples}
For the sake of comparison with most existing works, where a large number of samples per client are assumed, we experimented with three different image classification datasets: MNIST, FMNIST, and CIFAR-10. We also tune the hyperparameters of our algorithm on the validation set of the MNIST dataset as shown in Fig. \ref{val_plots}. This helps in giving an estimate of the parameters that work best for achieving optimal performance of all clients in our framework. Five clients were used in the experiment and each client had 10k datapoints for CIFAR-10, 12k MNIST and 12k for FMNIST. These clients then participate in distributed learning and try to achieve performance close to centralized learning. We evaluate all the clients by their accuracies on a common test set. We report the averaged train and test accuracies for each algorithm. 

Table \ref{tab:pop_bm_1} reports the average test accuracies of the participating clients across all the algorithms on CIFAR-10, FMNIST, and MNIST datasets when each of them has a large number of training data points. Our algorithm gives the best performance on all the datasets. Notably, it performs significantly better than the other algorithms on the CIFAR-10 dataset, which is more complex than the other two. 

Table \ref{tab : pop_bm_2} shows the average train and test accuracies of 5 clients and the amount of overfitting (difference between train and test accuracies) across all algorithms on the CIFAR-10 dataset. We can observe that other algorithms heavily overfit their training data and thus perform poorly on their test data. In contrast, our algorithm shows the least amount of over-fitting because of a carefully designed weight-averaging process. 

\begin{figure}[H]
\includegraphics[width= \linewidth]{img/Validation.png}
\caption{S4: Hyper-parameter tuning using the validation set for MNIST} 
\label{val_plots}
\end{figure}

\begin{table}
\setlength{\tabcolsep}{2.5pt}
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Algorithm} & \textbf{CIFAR-10} & \textbf{FMNIST} & \textbf{MNIST} \\ \hline
$d_i$ & 10K & 12K & 12K \\ \hline
FL  & 81.52 & 91.38  & 99.26 \\ \hline
SL & 82 & 91.08 & 99.14 \\ \hline
SFLv1 \cite{split_fed}  & 78.82 & 90.55  & 98.7\\ \hline
SFLv2 \cite{split_fed} & 73.82 & 90.93 & 99.08 \\ \hline
\textbf{PFSL} & \textbf{92} & \textbf{93} & \textbf{99.46}  \\ \hline
\end{tabular}
\caption{S4: Average Test Accuracy, five clients on three important image classification benchmarks}
\label{tab:pop_bm_1}
\bigskip % or \bigskip\bigskip
\begin{tabular}{|l|l|l|l|}
    \hline
        \textbf{Algorithm} & \textbf{Train Accuracy} & \textbf{Test Accuracy} & \textbf{Overfitting} \\ \hline
        FL & 92 & 81.52 & 10.48 \\ \hline
        SL & 96.94 & 82 & 14.94 \\ \hline
        SFLv1 & 92.73 & 78.82 & 13.91 \\ \hline
        SFLv2 & 96.5 & 73.82 & 22.68 \\ \hline
        \textbf{PFSL} & \textbf{99.8} & \textbf{92} & \textbf{7.8} \\ \hline
    \end{tabular}
    \caption{S4: Overfitting on CIFAR-10, 5 clients}
    \label{tab : pop_bm_2}
  
\end{table}


\subsection{Setting 5: System simulation with 1000 clients}

\begin{figure}[H]
\includegraphics[width=\linewidth]{img/sys_simulation_plots.png}
\caption{S5: Test Accuracy with number of epochs} 
\label{fig: sys_sim}
\end{figure}

In this setting, we simulate 1000 clients.  We allow only 10 clients to simultaneously perform the training. Each client stays in the system only for 1 round which lasts only 1 epoch. Thus, we evaluate our system for the worst possible scenario when every client cannot stay in the system for long and can only afford to make a minimal effort to participate. We assume that each client has 50 labeled data points sampled randomly but unique to the client. Within each round, we simulate a dropout, where clients begin training but are not able to complete the weight averaging. We keep the dropout probability at 50\%. In this experiment, we used the CIFAR-10 dataset. We find that the final test accuracy after 100 rounds (where each client begins the training exactly once), was 90\% for PFSL (see Fig.\ref{fig: sys_sim}). This is very close to that observed in an ideal scenario with 5 clients having 10K data points each (see Table \ref{tab : pop_bm_2}). This setting clearly demonstrates the high scalability of our framework in realistic scenarios with extremely resource-constrained devices participating in the distributed learning process. 


\subsection{Setting 6: Different Diabetic Retinopathy Datasets}
This experiment describes the realistic scenario when healthcare centers have different sets of raw patient data for the same disease. We have used two datasets EyePACS \cite{eyePacs} and APTOS\cite{aptos2019blindness}. The images in EyePACS were captured under various conditions by various devices at multiple primary care sites throughout California and elsewhere. The APTOS dataset, on the other hand, has been collected across various healthcare sites in India. For the experiment, we created a set-up where there was a total of 10 clients, in which the first 5 clients were provided the APTOS dataset and the next 5 clients (Client 5- Client 9) were provided the EyePACS dataset. The sample size of data for each client was kept at 500. The images had to be classified into three categories-No DR(0), Moderate(1), and Severe(2). All clients ran the personalization phase to maximize their test accuracies on their respective datasets.

Table \ref{tab:algo_f1_score} shows the average test accuracies achieved by the first set of clients and the second set of clients separately. We have also shown the F1 Scores of one representative client having the APTOS dataset and one of the clients having the EyePACS dataset across all the algorithms.
\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\textbf{Algorithm} &  \textbf{$C_0$ F1 } &\textbf{$C_5$ F1 } & \textbf{$C_0-C_4$ Test}  & \textbf{$C_5-C_9$ Test}  \\ \hline

FL  & 0.73 & 0.36	 & 81.7	& 70.14 \\ \hline
SL & 0.72 & 0.41 & 80.37 &	69.57 \\ \hline
SFLv1 \cite{split_fed}  & 0.61 & 0.32 &	77.79 &	69.55\\ \hline
SFLv2 \cite{split_fed} & 0.74 & 0.43 & 80.21 &	66.53  \\ \hline
\textbf{PFSL} & \textbf{0.78}  & \textbf{0.58} & \textbf{85.36} & \textbf{70.86}  \\ \hline

\end{tabular}
\caption{S6: F1 Score of Client 0 (having APTOS dataset) and F1 Score of Client 5 (having EyePACS dataset) and Avg Test Accuracies of Client 0-4 (APTOS dataset) and Client 5-9 (EyePACS dataset)}
\label{tab:algo_f1_score}
\end{table}

From the results, we see that even though the two datasets are very different, the PFSL algorithm achieves the highest F1 scores for Client 0 and Client 5 and also the highest average test accuracies for the first and second sets of clients. This shows that our algorithm achieves the best, personalized results, wherein each set of clients performs very well on their own dataset.

\subsection{Work Fairness Analysis}

Table \ref{tab : wfair} shows the work done by each client in different DML algorithms. As we can see, there is a difference in values of $C_i$ for S3 and S6 for FL, SL, SFLv1, SFLv2, because of the change in the size of input images from 32x32 in S3 to 224x224 in S6. For PFSL, we had to use 224x224 in both S3 and S6 because of using transfer learning on ResNet-18 pre-trained model. This could have been reduced considerably by using a customized pre-trained model for 32x32. It is evident that FL algorithms are very resource intensive as compared to all the SL variants. In S3, we can see the clear benefit of work fairness for the large client (O\_L\_c) under PFSL. All other DML algorithms cause the large client to overwork heavily for marginal gains. As we mentioned, a better approach would be to let the large client participate in multiple rounds to increase its accuracy gradually. Finally, in Setting 6, we can see the clear benefits of using PFSL over all other DML algorithms.

\begin{table}[H]
    \centering
    \begin{tabular}{|p{1.5cm}|l|l|p{1cm}|p{0.8cm}|l|}
    \hline
        \textbf{Algorithm } & \textbf{S3:C\_i} & \textbf{S6:C\_i} & \textbf{S3: O\_L\_c} & \textbf{S3: O\_S\_c } & \textbf{S6: O\_i} \\ \hline
        FL  & 14.28 & 700.2 &44268 & 2856 & 105030 \\ \hline
        SL, SFLv1, SFLv2 & 2.76 &  135.9 & 8556& 552 & 10192 \\ \hline
        PFSL  & 15.5 & 15.5 & 775& 775 & 744 \\ \hline
    \end{tabular}
    \caption{Work Fairness Analysis for settings S3 and S6. C refers to the work done per iteration and O refers to the total work done till convergence is reached. L\_c refers to the Large client in S3 and S\_c refers to the small clients in S3. All values are computed in GFlops using a profiler \cite{profiler}}
    \label{tab : wfair}
\end{table}
