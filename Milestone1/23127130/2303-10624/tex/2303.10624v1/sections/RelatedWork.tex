\section {Background and Related Works}
\label{sec:rel_work}


Centralized Training requires all the data from different sources to be pooled together at a central server on which training is performed raising concerns about data privacy. There are several lines of work on various attacks on ML Models\cite{ref_fed_privacy, ref_fed_diff_private, ref_break_privacy_fl, Model-Inversion, MI2} even when they have been trained on anonymized data. Distributed Machine Learning (DML) methods like Federated Learning (FL) \cite{fed_incep} and its variant, Split Learning (SL) \cite{split_main} were introduced to overcome these limitations of centralized learning. FL and SL attempt to limit sharing of raw data and offer to train a robust and generalized model through collaborative learning techniques. From the vast and ever-growing literature on FL and SL, we have selected a few topics to provide the reader with enough background to fully understand the working of PFSL and our contributions. For ease of comparison between different distributed machine learning algorithms refer to Table \ref{tab: layer_dis}. 

\begin{table*}
    \centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
        \textbf{Algorithm } & \textbf{Front Layers } & \textbf{Central Layers } & \textbf{Back Layers } & \textbf{Parallel / Sequential} \\ \hline
        FL \cite{FL-TL} & Client & Client  & Client  & Parallel\\ \hline
        SL \cite{split_main} & Client   & Server   & Server  & Sequential \\ \hline
        SFLv1 \cite{split_fed} & Client   & Server   & Server  & Parallel  \\ \hline
        SFLv2 \cite{split_fed} & Client   & Server   & Server   & Sequential \\ \hline
        PFSL (this paper)& Client    & Server   & Client   & Parallel  \\ \hline
    \end{tabular}
    % GRG: don't boldface here.. looks ugly
    \caption{A summary of various Federated and Split Learning approaches with respect to the distribution of neural network's layers between a client and server and whether multiple clients are trained sequentially or in parallel.}
    \label{tab: layer_dis}
\end{table*}




\subsection{\textbf{Federated Learning}}
In FL, each client initially loads a central model from the server. Once loaded, the model will train and update the model parameters according to its local private data. Thus a different variant of the local model is generated. Once this part is done, the weights and other model-specific information are uploaded to the central server. These updated models generated by a number of clients are then aggregated securely \cite{fed_learn_rev, fed_priv} resulting in a more accurate and generalized model. Through multiple epochs/rounds of local training and secure aggregation, the accuracy of the model gradually improves and converges. 

As shown in Table \ref{tab: layer_dis}, federated learning requires the client to handle all the workload during training.  Considering the size and complexity of modern DNNs, federated learning requires the client to do lots of computations, indicating that it is unsuitable for resource-constrained devices. In Split learning, only the front layers are with the clients. Thus a client in the SL framework has to take care of the forward pass, backward pass, and weights updates of only the front layer.

Over the years, there has been tremendous interest and progress in optimizing FL for various objectives such as fairness, reduction in communication costs, non-i.i.d. data distribution across clients, mitigating attacks from malicious clients, etc.  \cite{fed_non_iid, split_fed, fed_learn_rev, fed_learn_med, split_label_leak}. FL requires the devices to train their models locally, which is very challenging on limited-resource hardware devices \cite{fed_learn, fed_learn_rev, fed_priv, HW_Eval}. As the State-of-the-Art Deep Learning models get more complex and sophisticated, it becomes highly infeasible for implementation on low-end Internet of Things (IoT) devices \cite{HW_Eval}. 

\subsection{\textbf{Split Learning}}
Some of these limitations were addressed by split-learning \cite{split_main, split_med} which proposed the different ways of splitting DL models, data-partitioning schemes, and its novel use cases. However, its implementation was sequential, required heavy synchronization during training, and the overheads were not quantified clearly. Subsequent research focused on scalability and faster convergence \cite{split_locfedmix, split_scale}, amalgamation with FL \cite{split_comp_fed, split_fed, split_fed_edge_iot}, increasing data privacy \cite{split_guard, split_label_leak, split_label_protect, split_locfedmix, split_med, split_no_peek} but only for the two-stage architecture where labels are shared with the server. There are two important variants of two-stage architecture: Sequential Split Learning (SSL) and Parallel Split Learning (PSL). 

In SSL, the clients are presented with the front part of the model, and the remaining model along with all data labels in the training set are shared with a central trusted server. The clients are selected sequentially for training and pass on their activations to the server. The server completes the forward pass, computes loss, and returns gradients to the clients via back-propagation. This process continues sequentially for all clients and the gradients/weights on the server side are merged. SSL achieves high accuracy \cite{split_scale} as the number of clients increases providing the desired benefits of DML. In PSL, the client and server operations are similar to SSL, with the distinct difference that multiple clients are connected to the server simultaneously. This method helps achieve good results with lower latency on the client side and utilizes parallel computing for training and testing the model \cite{split_scale}.

The three-part architecture (also called U-shaped Learning) provides both data and label privacy but is not well explored. We found that the implementation of U-shaped Learning is much more complex as it requires the clients to train additional (last few) layers, compute loss and pass gradients back to the server, thus increasing their computation and communication overhead. By utilizing publically available pre-trained models \cite{aws-tl} in PFSL, we have reduced the computation and communication load on the clients considerably (detailed in Sec 
\ref{sec:experiments_and_results}).  

%\begin{figure}[h]
%\includegraphics[width=\linewidth]{img/mdl_seq.png}
%\caption{Sequential Split Learning Architecture  (2-stage architecture)}
%  \vspace{-0.5em} 
%\label{fig:mdl_seq}
%\end{figure}

\subsection{\textbf{Personalization}}
The heterogeneity of data at the clients presents both a challenge and opportunity for DML. It is a challenge as the diversity makes it difficult to get convergence to optimal weights/parameters for the model. It is an opportunity if the clients are allowed to develop personalized models that are optimized for their particular data distributions. In \cite{Smith17}, personalized FL was approached via a primal-dual multi-task learning framework for convex Loss functions. Subsequent works have applied clustering of clients with similar data distributions \cite{Ghosh20, Sattler20}, fine-tuning or transfer learning for personalization \cite{Zhao18, Yu20}. 

\subsection {\textbf{Fairness}}
When the clients' data distributions are non-i.i.d. and their sample sizes are heterogenous it is likely that there will be variance in their test accuracy \cite{HashimotoSNL18}. Thus, the goal of ``Performance Fairness'' is to \textbf{minimize std. deviation} of test accuracy \cite{li2019fair} while maintaining good average test performance. \cite{Mohri19} have used  minimax optimization to encourage uniform test performance across clients. In \cite{ditto_main}, a strong case was made that fine-grained personalization inherently provides the benefits of fairness and robustness to data and model poisoning attacks. However, it is very resource intensive for thin clients as they have to maintain two models and perform forward pass and back-propagation on both, doubling the costs.\cite{ditto_main, P1, P2, P3, P4} also suggest a lightweight coarse-grained personalization scheme which we have adapted in this paper.

As explained before, we add a \textbf{work equality constraint} in each round in PFSL by ensuring that all clients do an equal amount of work during training, regardless of the number of data samples possessed by them. If a client may participate in more rounds to get more accurate models and we leave that decision to the clients.