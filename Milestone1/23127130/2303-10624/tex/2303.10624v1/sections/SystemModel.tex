\section{System Model}
  \vspace{-0.5em}
\label{sec:background_sys_model}
\begin{figure}[h]
\includegraphics[width=\linewidth]{img/mdl_main1.png}
\caption{PFSL System Architecture using ResNet18 as an example for splitting the model into the front, central, and back models for training.} 
\label{fig:mdl_main}
\end{figure}
Deep Learning (DL) models, by definition, are neural networks with 10s to 100s of layers, with each layer comprising 10s to 100s of neurons. An example is ResNet-152 \cite{ResNet-152}, which has 152 layers, 60M parameters, and achieved state-of-the-art accuracy on the ImageNet \cite{ImageNet} benchmark. Training these models requires a tremendous amount of computation and energy \cite{HW_Eval} which is not feasible for thin clients. Our first architecture choice in PFSL, therefore, is to split the model into three parts: front, middle, and back. We illustrate in Fig \ref{fig:mdl_main}, with an example of ResNet-18, how a model can be split. In this figure, the first 2 and last 2 layers reside with the client and the remaining 14 with the offloading server. The middle layers are usually much heavier and we make the offloading server responsible for the bulk of the computations. Furthermore, by starting with pre-trained DL models, we freeze the weights of the front layers at the clients, thereby reducing the burden of training even further. The \emph{offloading server} only receives activations and gradients from the clients and thus has no knowledge of the client applications. Also, as shown in the figure, we keep the \emph{client model averaging server} separated from the offloading server, thus preventing any information leak of the layers or the model architecture with the clients. Therefore, this method is useful for preserving privacy by preventing model inversion attacks and easing the computational burden on thin clients.

The entire training process is divided into multiple rounds.  During a round, a certain number of clients participate in the training process and stay for the entire duration. We also handle cases where some drop out due to disconnects, low battery, etc. by incorporating time-outs which ensure that the model continues to be updated with the remaining clients. Within a round, all the clients participate in the generalization phase (1) of our framework consisting of multiple global epochs.  Within a global epoch, all clients perform their local epochs in parallel. A global epoch will be considered completed when all the clients finish their local epoch. In a local epoch, we iterate over the local data of a client by forming batches. After the completion of a round, the clients may choose to continue to the personalization phase (2) of our framework or may choose to leave the system. 

We note that PFSL allows multiple clients ($n$, configurable) to train in parallel. To maintain correctness, it is essential to create $n$ copies of the Central model to store activations for each client so that the gradients can be calculated in parallel during the back-propagation phase. To support a large number of clients, the server capacity can be scaled by running server instances (micro-services) in the cloud. We provide full details of our remote procedures (APIs) in Sec \ref{sec:Algorithms}, but we don't discuss the cloud implementation further in this paper and focus only on the DML aspects.

\subsection{Work Fairness}

We introduce a novel \textbf{work fairness} constraint in this paper which ensures that all the clients who participate in a global epoch while training, perform an equal amount of work. It would be unfair for a client with more training data to do more work than the clients with less training data, which also achieve similar performance. Since we are dealing with \textbf{thin clients}, it is important to understand the work done by a client in the entire training process. More formally, we define below the \textbf{work done by the $ith$ client} in the training process. 

\begin{equation}
\label{eqn:work_fairness}
        O_i = E_{gc} * [(d_i/b) * C_i]   
  \vspace{-0.5em} 
\end{equation}

Here, $(d_i/b)$ is the number of iterations per client in one global epoch, $E_{gc}$ is the number of global epochs of the system for convergence, and $C_i$ is the compute operations of $ith$ client per iteration.


All clients in our framework perform the same number of iterations per global epoch regardless of the data with them i.e, $(d_i)/b = \min_{\forall i \in Clients} (d_i)/b$. The data of any client that is not used for training in a particular global epoch is carried over to the next global epoch. This is not imposed in other algorithms, which makes the client with more data do more work in training. 



% \begin{table*}
%     \centering
%     \begin{tabular}{|l|l|l|l|l|}
%     \hline
%         \textbf{Algorithm } & \textbf{Front } & \textbf{Central } & \textbf{Back } & \textbf{Parallel / Sequential} \\ \hline
%         FL \cite{FL-TL} & Client Model  & Client Model  & Client Model & Parallel\\ \hline
%         SL \cite{split_main} & Client Model  & Server Model  & Server Model & Sequential \\ \hline
%         SFLv1 \cite{split_fed} & Client Model  & Server Model  & Server Model & Parallel  \\ \hline
%         SFLv2 \cite{split_fed} & Client Model  & Server Model  & Server Model  & Sequential \\ \hline
%         PFSL (this paper)& Client Front Model  & Server Model  & Client Back Model & Parallel  \\ \hline
%     \end{tabular}
%     % GRG: don't boldface here.. looks ugly
%     \caption{Layer Distribution among different models in Distributed Machine Learning algorithms }
%     \label{tab: layer_dis}
% \end{table*}


In our framework, a client has lightweight front and back layers. As we do not update the weights of the front layers, a client only has to do a forward pass through it, and the back layers do forward and backward passes and weight updates. To calculate the work done by $ith$ client during the entire training, we also multiply $C_i$ by the number of iterations and the number of epochs till convergence. Since our algorithm achieves convergence very quickly (refer to experiments), $E_{gc}$ will be very less; thus, the work done by the $ith$ client in our algorithm is lesser than other algorithms for the same setting (Table \ref{tab : wfair}). 

% Math or generic equations 

% Assumtpions/Objectives that we define 
    % Evaluation Metrics for various settings
    % - Generalized 
    %     - test Accuracy
    % - Personalized 
    %     - Macro F1 score
    % - Fairness Objective
    %     - Performance Fairness
    %     - Work Fairness (new)
    
% \begin{equation}
% \label{eq:accuracy}
% Accuracy = \frac{TP+TN}{TP+TN+FP+FN}
%   \vspace{-0.6em}
% \end{equation}


% \begin{equation}
% \label{eq:precision}
% Precision = \frac{TP}{TP+FP}
%   \vspace{-0.6em}
% \end{equation}

% \begin{equation}
% \label{eq:recall}
% Recall = \frac{TP}{TP+FN}
%   \vspace{-0.6em}
% \end{equation}

% \begin{equation}
% \label{eq:f1_score}
% F1 = \frac{2*Precision*Recall}{Precision+Recall} = \frac{2*TP}{2*TP+FP+FN}
%   \vspace{-0.6em}
% \end{equation}

% \begin{table}[H]
% \centering
% \begin{tabular}{|l|l|}
% \hline
% \textbf{Abbreviation} & Full Form \\ \hline
% \textbf{TP} & True Positives  \\ \hline
% \textbf{FP} & False Positives  \\ \hline
% \textbf{TN} & True Negatives   \\ \hline
% \textbf{FN} & False Negatives  \\ \hline
% \end{tabular}
% \caption{Abbreviations used in calculation of F1-Score}
% \label{tab:pop_bm_1}
% \end{table}