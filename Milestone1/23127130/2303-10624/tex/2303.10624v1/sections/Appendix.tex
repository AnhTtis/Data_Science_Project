 \clearpage
 \appendices
 % \appendix
 

  \subsection{Artifact Appendix}
  \label{appendix:artifact}
    Software is being developed and maintained here: \url{https://github.com/mnswdhw/PFSL}

    A persistent identifier for the software code can be found here: \url{https://doi.org/10.5281/zenodo.7739655 }
  
  We have tested the following commands in Ubuntu 20.04.5 LTS. 
  
  \subsubsection{Build requirements}
  \label{appendix:build-req}
  \begin{itemize}
      \item Python3 ($>$=3.8)
      \item pip 21.0.1
      \item Nvidia GPU ($>$=12GB)
      \item Conda 
  \end{itemize}
 
 You may use the following steps to install the required libraries:
 \begin{itemize}
     \item Change Directory into the project folder
     \item Create a conda environment using the command \textit{conda create \texttt{-{}-}name {env\_name} python=3.8} Eg- conda create \texttt{-{}-}name pfsl python=3.8
\item Activate conda environment using the command \textit{conda activate {env\_name}} Eg- conda activate pfsl
     \item pip install -r requirements.txt
    \item{Create a results directory in the project folder to store all
the resulting plots using the commands below:}
    \begin{itemize}
        \item mkdir results
        \item mkdir results/FL
        \item mkdir results/SL
        \item mkdir results/SFLv1
        \item mkdir results/SFLv2
    \end{itemize}
 \end{itemize}
 
\subsection{Algorithm Description}
  \label{appendix:testrun}

  The main implementaion of the PFSL algorithm, algorithm 1b and 1c is in PFSL.py from where all the function calls for client and server are made.

\begin{itemize}
    \item Client class is implemented in client.py. All the function calls for each client is defined here.
    \item Server class is implemented in ConnectedClient.py. All the function calls for the server copy of each client is defined here.
    \item  Algorithm 1c for merging of weights is implemented in utils/merge.py
\end{itemize}
 
   
 
  \subsection{Test Run}
  \label{appendix:testrun}

The parameters options for a particular file can be checked adding --help argument.
 Optional arguments available for PFSL are:

 \begin{itemize}
    
 \item  -h, - -help            show this help message and exit 
 \item  -c , \texttt{-{}-}number\_of\_clients  Number of Clients (default: 10)
 \item  -b , \texttt{-{}-}batch\_size   Batch size (default: 128)
 \item  \texttt{-{}-}test\_batch\_size   Input batch size for testing (default: 128)
 \item  -n , \texttt{-{}-}epochs      Total number of epochs to train (default: 10)
 \item  \texttt{-{}-}lr                Learning rate (default: 0.001)
 \item  \texttt{-{}-}dataset      States dataset to be used (default: cifar10)
 \item  \texttt{-{}-}seed            Random seed (default: 1234)
 \item  \texttt{-{}-}model          Model you would like to train (default: resnet18)
 \item  \texttt{-{}-}opt\_iden   optional identifier of experiment 
 \item  \texttt{-{}-}pretrained          Use transfer learning using a pretrained model (default: False)
 \item  \texttt{-{}-}datapoints  Number of samples of training data allotted to each client (default: 500)
 \item  \texttt{-{}-}setting     Setting you would like to run for, i.e, setting1 , setting2 or setting4 (default: setting1)
 \item  \texttt{-{}-}checkpoint  Epoch at which personalisation phase will start (default: 50)
  \item  \texttt{-{}-}rate This arguments specifies the fraction of clients dropped off in every epoch (used in setting 5)(default: 0.5)
  
 \end{itemize}



 For reproducing the results, always add argument \texttt{-{}-}pretrained  while running the PFSL script.
 
  \subsubsection{\textbf{Setting 1: Small Sample Size (Equal), i.i.d.}}
  \label{appendix_setting1}
  To run all the algorithms for setting 1 argument  \texttt{-{}-}setting setting1 and \texttt{-{}-}datapoints [number of sample per client] has to be added. Rest of the arguments can be selected as per choice. Number of data samples can be chosen from 50, 150, 250, 350 and 500 to reproduce the results. When the datapoints per client was 50, batch size was chosen to be 32 and for other data samples greater than 50 batch size was kept at 64. Test batch size was always taken to be 512. For data sample 150, command are given below.
\begin{itemize}
      
  \item python PFSL\_Setting124.py  \texttt{-{}-}dataset cifar10 \texttt{-{}-}setting setting1 \texttt{-{}-}datapoints 150 \texttt{-{}-}pretrained \texttt{-{}-}model resnet18 -c 10 \texttt{-{}-}batch\_size 64 \texttt{-{}-}test\_batch\_size 512 \texttt{-{}-}epochs 100

  \item python FL.py     \texttt{-{}-}dataset cifar10 \texttt{-{}-}setting setting1 \texttt{-{}-}datapoints 150  -c 10 \texttt{-{}-}batch\_size 64 \texttt{-{}-}test\_batch\_size 512 \texttt{-{}-}epochs 100
  \item python SL.py     \texttt{-{}-}dataset cifar10 \texttt{-{}-}setting setting1 \texttt{-{}-}datapoints 150   -c 10 \texttt{-{}-}batch\_size 64 \texttt{-{}-}test\_batch\_size 512 \texttt{-{}-}epochs 100
  \item python SFLv1.py  \texttt{-{}-}dataset cifar10 \texttt{-{}-}setting setting1 \texttt{-{}-}datapoints 150  -c 10 \texttt{-{}-}batch\_size 64 \texttt{-{}-}test\_batch\_size 512 \texttt{-{}-}epochs 100
   \item python SFLv2.py \texttt{-{}-}dataset cifar10 \texttt{-{}-}setting setting1 \texttt{-{}-}datapoints 150 -c 10 \texttt{-{}-}batch\_size 64 \texttt{-{}-}test\_batch\_size 512 \texttt{-{}-}epochs 100
   \end{itemize}

  \subsubsection{ \textbf{Setting 2: Small Sample Size (Equal), non-i.i.d.}}

   To run all the algorithms for setting 2 argument  --setting setting2 has to be added. For PFSL, to enable personalisation phase from xth epoch, argument --checkpoint [x] has to be added. Rest of the arguments can be selected as per choice. 
\begin{itemize}
      
  \item python PFSL\_Setting124.py \texttt{-{}-}dataset cifar10 \texttt{-{}-}model resnet18 \texttt{-{}-}pretrained \texttt{-{}-}setting setting2 \texttt{-{}-}batch\_size 64 \texttt{-{}-}test\_batch\_size 512 \texttt{-{}-}checkpoint 25 \texttt{-{}-}epochs 30

  \item python FL.py \texttt{-{}-}dataset cifar10 \texttt{-{}-}setting setting2 -c 10 \texttt{-{}-}batch\_size 64 \texttt{-{}-}test\_batch\_size 512 \texttt{-{}-}epochs 100
  \item python SL.py \texttt{-{}-}dataset cifar10 \texttt{-{}-}setting setting2 -c 10 \texttt{-{}-}batch\_size 64 \texttt{-{}-}test\_batch\_size 512 \texttt{-{}-}epochs 100
  \item python SFLv1.py \texttt{-{}-}dataset cifar10 \texttt{-{}-}setting setting2 -c 10 \texttt{-{}-}batch\_size 64 \texttt{-{}-}test\_batch\_size 512 \texttt{-{}-}epochs 100 
   \item python SFLv2.py \texttt{-{}-}dataset cifar10 \texttt{-{}-}setting setting2 -c 10 \texttt{-{}-}batch\_size 64 \texttt{-{}-}test\_batch\_size 512 \texttt{-{}-}epochs 100
   \end{itemize}
  \label{appendix_setting2}

  \subsubsection{\textbf{Setting 3: Small Sample Size (Unequal), i.i.d.}}
In this setting, we consider we have 11 clients where the Large client has 2000 labelled data points while the other ten small clients have 150 labelled data points, each distributed identically. The class distributions among all the clients are the same. For evaluation purposes, we consider a test set having 2000 data points with an identical distribution of classes as the train set.

To reproduce Table IV of the paper, run setting 1 with datapoints as 150 as illustrated above. To reproduce Table V of the paper follow the below commands. In all the commands argument \texttt{-{}-}datapoints that denotes the number of datapoints of the large client has to be added. In our case it was 2000.
  \begin{itemize}
    
  \item python PFSL\_Setting3.py \texttt{-{}-}datapoints 2000 \texttt{-{}-}dataset cifar10 \texttt{-{}-}pretrained \texttt{-{}-}model resnet18 -c 11 \texttt{-{}-}epochs 50
  \item python FL\_Setting3.py \texttt{-{}-}datapoints 2000    \texttt{-{}-}dataset cifar10\_setting3  -c 11  \texttt{-{}-}epochs 100
  \item python SL\_Setting3.py  \texttt{-{}-}datapoints 2000   \texttt{-{}-}dataset cifar10\_setting3  -c 11  \texttt{-{}-}epochs 100
  \item python SFLv1\_Setting3.py \texttt{-{}-}datapoints 2000 \texttt{-{}-}dataset cifar10\_setting3  -c 11  \texttt{-{}-}epochs 100
   \item python SFLv2\_Setting3.py \texttt{-{}-}datapoints 2000 \texttt{-{}-}dataset cifar10\_setting3  -c 11  \texttt{-{}-}epochs 100
   \end{itemize}
  \label{appendix_Setting3}

\subsubsection{ \textbf{Setting 4: A large number of data samples}}
\label{appendix_setting4}

   To run all the algorithms for setting 4 argument  \texttt{-{}-}setting setting4 has to be added.The rest of the arguments can be selected as per choice. Dataset argument has 3 options: cifar10, mnist and fmnist.
\begin{itemize}
      
  \item python PFSL\_Setting124.py  \texttt{-{}-}dataset cifar10 \texttt{-{}-}setting setting4 \texttt{-{}-}pretrained \texttt{-{}-}model resnet18 -c 5 \texttt{-{}-}epochs 20

  \item python FL.py     \texttt{-{}-}dataset cifar10  \texttt{-{}-}setting setting4 -c 5 \texttt{-{}-}epochs 20
  \item python SL.py     \texttt{-{}-}dataset cifar10  \texttt{-{}-}setting setting4 -c 5 \texttt{-{}-}epochs 20
  \item python SFLv1.py  \texttt{-{}-}dataset cifar10  \texttt{-{}-}setting setting4 -c 5 \texttt{-{}-}epochs 20
   \item python SFLv2.py \texttt{-{}-}dataset cifar10  \texttt{-{}-}setting setting4 -c 5 \texttt{-{}-}epochs 20
   \end{itemize}


  \subsubsection{ \textbf{Setting 5: System simulation with 1000 client}}
\label{appendix_setting5}

In this setting we try to simulate an environment with 1000 clients. Each client stays in the system only for 1 round which lasts only 1 epoch. Thus, we evaluate our system for the worst possible scenario when every client cannot stay in the system for long and can only afford to make a minimal effort to participate. We assume that each client has 50 labeled data points sampled randomly but unique to the client. Within each round, we simulate a dropout, where clients begin training but are not able to complete the weight averaging. We keep the dropout probability at 50%.

Use the following command to reproduce the results:
\textit{Here rate argument specifies the dropoff rate which is the number of clients that will be dropped randomly in every epoch}

\begin{itemize}
    \item python system\_simulation\_e2.py -c 10 \texttt{-{}-}batch\_size 16 \texttt{-{}-}dataset cifar10 \texttt{-{}-}model resnet18 \texttt{-{}-}pretrained \texttt{-{}-}epochs 100 \texttt{-{}-}rate 0.3

\end{itemize}


\subsubsection{ \textbf{Setting 6: Different Diabetic Retinopathy Datasets}}
\label{appendix_setting6}
Dataset Sources:

\begin{itemize}
    \item Source of Dataset 1, \url{https://www.kaggle.com/competitions/aptos2019-blindness-detection/data}
    \item Source of Dataset 2, \url{https://www.kaggle.com/datasets/mariaherrerot/eyepacspreprocess}
\end{itemize}
To preprocess the dataset download and store the unzipped files in  data/eye\_dataset1 folder and data/eye\_dataset2 folder.
For this create directories using the command:
\begin{itemize}
    \item mkdir data/eye\_dataset1
    \item mkdir data/eye\_dataset2
    
\end{itemize}
The directory structure of data is as follows:
\begin{itemize}
    \item data/eye\_dataset1/train\_images
    \item data/eye\_dataset1/test\_images
    \item data/eye\_dataset1/test.csv
    \item data/eye\_dataset1/train.csv
    \item data/eye\_dataset2/eyepacs\_preprocess/eyepacs\_preprocess/
    \item data/eye\_dataset2/trainLabels.csv
\end{itemize}


Once verify the path of the unzipped folders in the load\_data function of preprocess\_eye\_dataset\_1.py and preprocess\_eye\_dataset\_2.py files.

For Data preprocessing, run the commands mentioned below for both the datasets
\begin{enumerate}

\item python utils/preprocess\_eye\_dataset\_1.py
\item python utils/preprocess\_eye\_dataset\_2.py
\end{enumerate}

   Then use the following commands
\begin{itemize}
      
  \item python PFSL\_DR.py  \texttt{-{}-}pretrained \texttt{-{}-}model resnet18 -c 10  \texttt{-{}-}batch\_size 64 \texttt{-{}-}test\_batch\_size 512 \texttt{-{}-}epochs 50

  \item python  FL\_DR.py -c 10 \texttt{-{}-}batch\_size 64 \texttt{-{}-}test\_batch\_size 512 \texttt{-{}-}epochs 50

  \item python  SL\_DR.py \texttt{-{}-}batch\_size 64 \texttt{-{}-}test\_batch\_size 512 \texttt{-{}-}epochs 50
  \item python  SFLv1\_DR.py  \texttt{-{}-}batch\_size 64 \texttt{-{}-}test\_batch\_size 512 \texttt{-{}-}epochs 50
   \item python SFLv2\_DR.py  \texttt{-{}-}batch\_size 64 \texttt{-{}-}test\_batch\_size 512 \texttt{-{}-}epochs 50
   \end{itemize}

\subsubsection{ \textbf{Test Run on Local System}}
\label{appendix_setting6}

The PFSL script was run on a laptop on cpu with 16 GB RAM for 5 clients each having 50 training datapoints and 200 test datapoints.

The following command was used :
python PFSL\_Setting124 \texttt{-{}-}dataset cifar10 \texttt{-{}-}model resnet18 \texttt{-{}-}pretrained  \texttt{-{}-}setting setting1 \texttt{-{}-}datapoints 50 -c 5 \texttt{-{}-}batch\_size 16 \texttt{-{}-}test\_batch\_size 16.






  
  