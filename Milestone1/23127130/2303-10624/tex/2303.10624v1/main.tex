\documentclass[conference]{IEEEtran}
% \IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url,afterpage,amssymb,amsfonts}
\usepackage{enumitem,makecell}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{dblfloatfix}
\usepackage{color}
\usepackage{colortbl}
\graphicspath{{./img/}}
% \usepackage[linesnumbered,noend]{algorithm2e}
\usepackage{listings}
\usepackage{soul}
\usepackage{enumitem,makecell}
\usepackage{colortbl}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{pgfgantt}
\usepackage[thinc]{esdiff}
\usepackage{algorithm,algpseudocode}
\algrenewcommand\textproc{}
\usepackage{tabularx,booktabs}
\usepackage{xcolor}
% defined centered version of "X" column type:
\newcolumntype{C}{>{\centering\arraybackslash}X} 
\setlength{\extrarowheight}{1pt} % for a bit more open "look"



%Environments begin
\makeatletter
\newenvironment{breakablealgorithm}
  {% \begin{breakablealgorithm}
   \begin{center}
     \refstepcounter{algorithm}% New algorithm
     \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
     \renewcommand{\caption}[2][\relax]{% Make a new \caption
       {\raggedright\textbf{\fname@algorithm~\thealgorithm} ##2\par}%
       \ifx\relax##1\relax % #1 is \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
       \else % #1 is not \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
       \fi
       \kern2pt\hrule\kern2pt
     }
  }{% \end{breakablealgorithm}
     \kern2pt\hrule\relax% \@fs@post for \@fs@ruled
   \end{center}
  }
\makeatother


\newcounter{savealgorithm}
\newenvironment{subalgorithms}
 {%
  \stepcounter{algorithm}%
  \edef\currentthealgorithm{\thealgorithm}%
  \setcounter{savealgorithm}{\value{algorithm}}%
  \setcounter{algorithm}{0}%
  \renewcommand{\thealgorithm}{\currentthealgorithm\alph{algorithm}}%
 }
 {%
  \setcounter{algorithm}{\value{savealgorithm}}%
 }

%Environments end 




\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    

\newcommand\todo[1]{\textcolor{red}{#1}}

\begin{document}

\title{PFSL: Personalized \& Fair Split Learning with Data \& Label Privacy  for thin clients}


\author{\IEEEauthorblockN{Manas Wadhwa, Gagan Raj Gupta, Ashutosh Sahu, Rahul Saini, Vidhi Mittal}
\IEEEauthorblockA{\textit{Department of Electrical Engineering and Computer Science} \\
\textit{Indian Institute of Technology, Bhilai}\\
 Bhilai, India \\
\{manasw, gagan, ashutoshsahu, rahuls, vidhimittal\}@iitbhilai.ac.in}
}


\maketitle
\begin{abstract}
The traditional framework of federated learning (FL) requires each client to re-train their models in every iteration, making it infeasible for resource-constrained mobile devices to train deep-learning (DL) models. Split learning (SL) provides an alternative by using a centralized server to offload the computation of activations and gradients for a subset of the model but suffers from problems of slow convergence and lower accuracy. 

In this paper, we implement PFSL, a new framework of distributed split learning where a large number of thin clients perform transfer learning in parallel, starting with a pre-trained DL model without sharing their data or labels with a central server. We implement a lightweight step of personalization of client models to provide high performance for their respective data distributions. Furthermore, we evaluate performance fairness amongst clients under a work fairness constraint for various scenarios of non-i.i.d. data distributions and unequal sample sizes. Our accuracy far exceeds that of current SL algorithms and is very close to that of centralized learning on several real-life benchmarks. It has a very low computation cost compared to FL variants and promises to deliver the full benefits of DL to extremely thin, resource-constrained clients. 

\end{abstract}

\begin{IEEEkeywords}
Distributed Machine Learning, U Shaped Split Learning, Federated Learning, non-i.i.d., Personalization, Fairness, Image Classification
\end{IEEEkeywords}

\input{sections/Introduction}
\input{sections/RelatedWork}
\input{sections/SystemModel}
\input{sections/algorithms.tex}
\input{sections/Experiments_and_results.tex}
\input{sections/Conclusion}

\bibliographystyle{IEEEtran}
\bibliography{ref}
\newpage
\input{sections/Appendix.tex}

\end{document}