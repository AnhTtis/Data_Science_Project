\section{Results}\label{sec:contrib}

\subsection{Main Result}
In the following result, we provide the promised numerically sharp, non-oblivious and geometrically insightful bounds for Rademacher random projections. 
In the (particularly interesting) case of sparse input, we obtain more explicit formulas involving binomial distributions.
\begin{theorem}[Sharp Moment Bounds for Rademacher Random Projections]\label{thm:main}
Let $\Phi$ be sampled according to the Rademacher scheme \eqref{eq:rademacher_matrix}, and define the distortion as
\begin{align}\label{eq:distortion}
    E(x) \triangleq \frac{ \|\Phi x \|^2 }{  \|x\|^2 } - 1.
\end{align}
Then the following holds true:
\begin{description}
\item[(a)] $E(x)$ has moments that are Schur-concave polynomials in $(x_i^2)$
\item[(b)] $E(x)$ is moment-dominated by $E_{*}$ defined as
\begin{align}
E_{*} & = \frac{1}{m}\sum_{i=1}^{m} (Z_i^2-1) 
\end{align}
where $Z_i$ are standardized binomial r.v.s.:
\begin{align}
   Z_i\sim^{IID} \frac{B-\mathbf{E}B}{\sqrt{\mathbf{Var}[B]}}, \ B\sim\mathsf{Binom}\left(\|x\|_0,\frac{1}{2}\right).
\end{align}
Equivalently,
\begin{align}
    \mathbf{E}E(x)^q \leqslant \mathbf{E}E_{*}^q
\end{align}
holds for $q=2,3,\ldots$ with equality when all components of the input $x$ are equal.
\end{description}
\end{theorem}

We briefly overview the proof of \Cref{thm:main} (see \Cref{fig:road_map}): it starts by a reduction to the dimension $m=1$, and writing the distortion as a Rademacher chaos of order 2; we then find extreme values of its moments geometrically, by means of \emph{Schur optimization}. Finally, these extreme values can be found explicitly and efficiently by linking them to binomial moments. 

\tikzstyle{my_style} = [draw, rectangle, rounded corners, minimum width={10cm}, 
fill={gray!10}]
\begin{figure*}[!ht]
\centering
\resizebox{.7\textwidth}{!}{

\begin{tikzpicture}[node distance=2cm]
\node (high_dim) [my_style] {Distortion $E(x) = \|\Phi x\|_2^2-\|x\|_2^2$};
\node (1_dim) [my_style, below = 1cm of high_dim] {Distortion $E(x) = \|\Phi x\|_2^2-\|x\|_2^2$ for $m=1$};
\node (rademacher) [my_style, below = 1cm of 1_dim] {Rademacher chaos: $E(x) = \sum_{i\not=j}x_i x_j r_i r_j$ for $m=1$};
\node (rademacher_worst) [my_style, below = 1cm of rademacher] {Flat vectors $x$ yield heaviest moments/tail bounds};
\node (formula) [my_style, below = 1cm of rademacher_worst] {Explicit formula for Rademacher chaos on flat vectors};
\draw [<-] (1_dim) -- (high_dim) node[midway,right] {averaging IID};
\draw [<-] (rademacher) -- (1_dim) node[midway,right] {rewritting};
\draw [<-] (rademacher_worst) -- (rademacher) node[midway,right] {Robin-Hood (Schur) optimization};
\draw [<-] (formula) -- (rademacher_worst) node[midway,right] {combinatorial identities};
\end{tikzpicture}}
\caption{The proof roadmap for \Cref{thm:main}.}
\label{fig:road_map}

\end{figure*}


\subsection{Techniques: Proving Schur Convexity}

We present a useful framework for proving Schur convexity properties. It makes repeated use of few auxiliary facts to eventually reduce the task to a 2-dimensional problem. This is often easier than the classical approach of evaluating derivative tests.

\begin{lemma}\label{lemma:schur1}
The class of \emph{non-negative} Schur-convex (or concave) functions forms a semi-ring.
\end{lemma}

\begin{lemma}\label{lemma:schur2}
A multivariate function is Schur-convex (respectively, Schur-concave) if and only if it is symmetric and Schur-convex (respectively, Schur-concave) with respect to each pair of variables.
\end{lemma}

To demonstrate the usefulness of these facts, we sketch an alternative proof of a refined version of celebrated Khintchine's Inequality, due to Efron. This refinement plays an important role in statistics, namely in proving properties of popular Student-t tests.

%These facts are extremely useful 

\begin{corollary}[Refined  Khintchine Inequality~\cite{efron1968student}]
\label{cor:khintchine}
The mapping
\begin{align*}
x\rightarrow \mathbf{E}(\sum_i x_i r_i )^{q} 
\end{align*}
is a Schur-concave function of $(x_i^2)$. As a consequence for $\sigma=\|x\|_2$ we have
\begin{align*}
\mathbf{E}(\sum_{i=1}^{n} x_i r_i )^{q}  \leqslant \mathbf{E}\left(\frac{\sigma}{\sqrt{n}}\sum_{i=1}^{n} r_i \right)^{q} \leqslant \mathbf{E} \mathsf{Norm}(0,\sigma^2)^{q}.
\end{align*}
\end{corollary}
\begin{proof}
The symmetry with respect to $(x_i)$ is obvious. Applying 
the multinomial expansion to $(\sum_i x_i r_i )^{q}$, taking the expectation and using the symmetry of Rademacher random variables, we conclude that $\mathbf{E}(\sum_i x_i r_i )^{q} $ is polynomial in variables $(x_i^2)$. By \Cref{lemma:schur2}, it suffices to prove the Schur-concavity property for $x_1^2,x_2^2$. By the binomial formula and the independence of $r_1,r_2$ from $(r_i)_{i>2}$, we see that $\mathbf{E}(\sum_i x_i r_i )^{q}  = 
\sum_{k}\binom{q}{k}\mathbf{E}(\sum_{i>2}x_i r_i)^{q-k}\cdot \mathbf{E}(x_1r_1+x_2r_2)^k$ is a combination of expressions
$\mathbf{E}(x_1r_1+x_2r_2)^k$ with coefficients $c_k = \binom{q}{k}\mathbf{E}(\sum_{i>2}x_i r_i)^{q-k}$ that are independent of $x_1,x_2$ and non-negative due to the symmetry of $r_i$. By \Cref{lemma:schur1}, it suffices to prove that
$F_k\triangleq \mathbf{E}(x_1r_1+x_2r_2)^k$  is a Schur-concave function of $x_1^2,x_2^2$. Define $G_k \triangleq \mathbf{E}(x_1r_1+x_2r_2)^k x_1 x_2 r_1r_2$. By $(x_1r_1+x_2r_2)^k = (x_1r_1+x_2r_2)^{k-2}(x_1^2+x_2^2+2x_1x_2r_1r_2)$ we have 
$F_k = (x_1^2+x_2^2)F_{k-2} + 2 G_{k-2}$ and 
$G_k = (x_1^2+x_2^2)G_{k-2}+2x_1^2x_2^2 F_{k-2}$. Since $x_1^2+x_2^2$ and $x_1^2 x_2^2$ are both Schur-concave in $x_1^2,x_2^2$, the Schur-concavity property of 
$F_{k},G_{k}$ is proven when it is proven for $k:=k-2$. By mathematical induction, it suffices to realize that $F_{0}=1,F_1 =0,G_1=1,G_2=x_1^2x_2^2$ are Schur-concave in $x_1^2,x_2^2$.

Let $\mathbf{1}_{n}$ be the vector of $n$ ones. The first inequality follows then by $\frac{\sum_{i=1}^{n} x_i^2}{n}\cdot \mathbf{1}_n \prec (x_1^2,\ldots,x_n^2)$, and is clearly sharp. Since $\frac{1}{n+1}\mathbf{1}_{n+1}\prec \frac{1}{n}\mathbf{1}_n$ and the Schur-concavity implies that $\mathbf{E}(\sum_{i=1}^{n}r_i/\sqrt{n})^q$ increases with $n$, the second inequality follows by the CLT.
\end{proof}

\subsection{Techniques: Rademacher Chaoses}

Of independent interests are the techniques used in this work. The first result analyzes the quadratic Rademacher chaos  geometrically. It is similar in the spirit of the results of Efron~\cite{efron1968student} and Eaton~\cite{eaton1970note}, which however concern only a first-order Rademacher chaos.

\begin{theorem}[Schur-concavity of Rademacher chaoses]\label{thm:schur_concave}
Let $(r_i)$ be a sequence of independent Rademacher random variables. Then the Rademacher chaos moment
\begin{align}
    \mathbf{R}_q(x) \triangleq \mathbf{E}\left(\sum_{i\not=j} x_i x_j r_i r_j\right)^q
\end{align}
is a Schur-concave function of $(x_i^2)$ for every positive integer $q$.
\end{theorem}

The second result is a recipe for explicitly computing the extreme moment values:
\begin{theorem}[Extreme moments of Rademacher Chaos]\label{thm:extreme_rademacher}
For any $x$ and $K=\|x\|_0$ the following holds:
\begin{align}
    \mathbf{R}_q(x) \leqslant \mathbf{R}_q(x^{*}),\quad x^{*} = 
    \underbrace{\left(\frac{\|x\|_2}{\sqrt{K}},\ldots,\frac{\|x\|_2}{\sqrt{K}}\right)}_{K \text{ times} },
\end{align}
and furthermore the explicit value of this bound equals 
\begin{align}
 \mathbf{R}_q(x^{*}) = \|x\|_2^{2q}\cdot  \mathbf{E}_{\bar{B}} ({\bar{B}}^2-1)^q,
\end{align}
where $\bar{B} = \frac{B-K/2}{\sqrt{K/4}}$ standardizes  the symmetric binomial distribution with $K$ trials $B$.%$\sim\mathsf{Binom}(K,1/2)$.
\end{theorem}

\subsection{Numerical Comparison}

%\todo[inline]{Comment on tables/figures}

%There has been many works on numerical guarantees ~\cite{frankl1988johnson,indyk1998approximate,achlioptas2003database,ailon2006approximate,matouvsek2008variants,dasgupta2010sparse,kane2014sparser,skorski2021johnson}, and our main result improves  
%the best bounds for Rademacher random projections from \cite{achlioptas2003database}, our contribution brings the following improvement:



The presented result is \emph{numerically optimal} and \emph{captures input sparsity}. It should be compared against the bounds from \cite{achlioptas2001database} and the no-go result from \cite{burr2018optimal}, as illustrated in \Cref{tab:compare}. To see that  our bounds are better than those in \cite{achlioptas2001database}, it suffices to use the Gaussian majorization argument to obtain a weaker bound
$E(x) \prec_m \frac{\sum_{i=1}^{m}(N_i^2-1)}{m}$ where $N_i$ are independent standard normal random variables, and use known sub-gamma tail bounds for chi-square distributions (for example, those developed in the monograph on concentration inequalities~\cite{boucheron2003concentration}).

To validate our findings, we performed the following experiments on both synthetic and real-world datasets.

\begin{table*}[t!]
\resizebox{\textwidth}{!}{
\begin{tabular}{c|l}
\toprule
Author & Result \\
\midrule
\cite{burr2018optimal} & $\max_{x}\mathbf{P}[|E(x)|>\epsilon]\geqslant 2\exp\left(-\frac{m\epsilon^2(1+o(1))}{4})\right)$ when $m\gg \epsilon^{-2}, n\gg 1$ \\
\cite{achlioptas2001database}     &  $\mathbf{P}[|E(x)|>\epsilon]\leqslant 2\exp\left(-\frac{m\epsilon^2}{4}\left(1-\frac{2}{3}\epsilon\right)\right)$  \\
\textbf{this paper}    &  $E(x)\prec_m\frac{\sum_{i=1}^{m} Z_i^2-1}{m}$, $Z_i\sim^{IID} \frac{B-\mathbf{E}B}{\sqrt{\mathbf{Var}[B}},\ B\sim \mathsf{Binom}\left(\|x\|_0,\frac{1}{2}\right)$ \\
\bottomrule
\end{tabular}
}
\caption{Bounds from this work (\Cref{thm:main}) compared with the best prior bounds~\cite{achlioptas2001database} and the 
sharp no-go results~\cite{burr2018optimal}. Our bounds imply those from prior work by "normal majorization" arguments (see the supplementary material).}
\label{tab:compare}
%\caption{\todo[inline]{add some older results}}
\end{table*}

\subsubsection{Synthetic dataset}
\Cref{fig:inputsparsity_vs_moment} and 
\Cref{fig:inputsparsity_vs_tail} demonstrate numerical improvements. The input sparsity is the key: random projections are seen \emph{less distorted when input data is sparse}.

\begin{figure}[h]
\centering
\resizebox{.6\textwidth}{!}{
\begin{tikzpicture}
\begin{axis}[
    axis x line*=bottom,
    axis y line*=left,
    xtick={2,5,10,15,20},
    ymode=log,
    xlabel={$\|x\|_0$},
    ylabel={$\max_x \mathbf{R}_q(x)$},
    legend style={
        at={(0.02,0.98)},
        anchor=north west
    },
]
\addplot table [x=n, y=4, col sep=comma] {rademacher_extreme_moments.csv};
\addplot table [x=n, y=6, col sep=comma] {rademacher_extreme_moments.csv};
\addplot table [x=n, y=8, col sep=comma] {rademacher_extreme_moments.csv};
\addplot table [x=n, y=10, col sep=comma] {rademacher_extreme_moments.csv};
\legend{$q=4$,$q=6$,$q=8$,$q=10$};
\end{axis}
\end{tikzpicture}}
\caption{The more spread-out the input (controlled by sparsity $\|x\|_0$), the more distorted the projected output (captured by $\mathbf{R}_q(x)$, the Rademacher chaos moment). Utilizing the input dispersion improves probability bounds by orders of magnitude. Note: for normalization purposes, we assume $\|x\|_2=1$.}
\label{fig:inputsparsity_vs_moment}
\end{figure}

\begin{figure}[ht!]
\centering
\resizebox{.6\textwidth}{!}{
\begin{tikzpicture}
\begin{axis}[
    axis x line*=bottom,
    axis y line*=left,
    %xtick={2,5,10,15,20},
    ymode=log,
    xlabel={$\epsilon$},
    ylabel={bound on $\mathbf{P}[|E(x)|>\epsilon]$},
    legend style={
        at={(0.98,0.98)},
        anchor=north east
    },
]
\addplot table [x=eps, y=new_10, col sep=comma] {bounds_compare.csv}; \addplot table [x=eps, y=new_20, col sep=comma] {bounds_compare.csv}; \addplot table [x=eps, y=new_30, col sep=comma] {bounds_compare.csv}; \addplot[mark=none,style=dashed] table [x=eps, y=old_10, col sep=comma] {bounds_compare.csv};
\legend{new($\ell=10$),new($\ell=20$),new($\ell=50$),old bounds};
\end{axis}
\end{tikzpicture}}
\caption{Capturing input-sparsity ($\ell=\|x\|_0$) improves the bounds on Rademacher random projections, as demonstrated by distortion  probability tails.
}
\label{fig:inputsparsity_vs_tail}
\end{figure}


\begin{figure}[ht!]
\centering
\resizebox{.6\textwidth}{!}{
\begin{tikzpicture}
\begin{axis}[
    axis x line*=bottom,
    axis y line*=left,
    xtick={0.1,0.2,0.4,0.6,0.8,1.0},
    ymode=log,
    xlabel={embedding density $p$},
    ylabel={$distortion$},
    legend style={
        at={(0.02,0.98)},
        anchor=north west
    },
    legend pos=north east
]
\addplot table [x=n, y=mnist, col sep=comma] {real_world.csv};
\addplot table [x=n, y=optdigits, col sep=comma] {real_world.csv};
\addplot table [x=n, y=semeion, col sep=comma] {real_world.csv};
\addplot table [x=n, y=usps, col sep=comma] {real_world.csv};
\legend{$mnist$,$optdigits$,$semeion$,$usps$};
\end{axis}
\end{tikzpicture}}
\caption{The distortion measured w.r.t. the density of the embeddings shows that sparse data result in better bounds 
%The density is the inverse of the sparsity (and it should not be confused with the sparsity of the data) 
and proves that Rademacher projections are superior to sparse ones.
}
\label{fig:real-world}
\end{figure}

\begin{figure}[h!]
\centering
\resizebox{.6\textwidth}{!}{

\begin{tikzpicture}
\begin{axis}[
    axis x line*=bottom,
    axis y line*=left,
    xtick={0,2,4,6,8,10},
    ymode=log,
    xlabel={$\epsilon$},
    ylabel={bound on $\mathbf{P}[|E(x)|>\epsilon]$},
    legend style={
        at={(0.02,0.98)},
        anchor=north west
    },
    legend pos=north east
]
\addplot table [x=mnist, y = y, col sep=comma] {real_world_2.csv};
\addplot table [x=plantstexture, y=y, col sep=comma] {real_world_2.csv};
\addplot table [x=micromass, y=y, col sep=comma] {real_world_2.csv};
\addplot table [x=Ecoli, y=y, col sep=comma] {real_world_2.csv};
\addplot table [x=Glass, y=y, col sep=comma] {real_world_2.csv};
\legend{$mnist$, $plantstexture$, $micromass$, $Ecoli$, $Glass$};
\end{axis}
\end{tikzpicture}}
\caption{Measuring the distortion tail probability on real-world datasets confirms our theoretical findings: capturing the input-sparsity improves the bounds on Rademacher random projections. The datasets, from left to right, are displayed from the most sparse (mnist) to the least one (Glass). }
\label{fig:real-world_2}
\end{figure}

\subsubsection{Real-world datasets}

\Cref{fig:real-world} and \Cref{fig:real-world_2} show our findings. The results are validated with experiments performed on datasets from the SuiteSparse Matrix Collection (formerly the University of Florida Sparse Matrix Collection) available at  \url{https://sparse.tamu.edu/}. For these experiments, we selected matrices from machine learning datasets.




