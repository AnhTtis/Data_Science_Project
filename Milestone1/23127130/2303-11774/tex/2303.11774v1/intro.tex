\section{Introduction}

\subsection{Background and Related Work}

The seminal result of Johnson and Lindenstrauss~\cite{johnson1984extensions} quantifies the amazing performance of \emph{random linear maps as embeddings}: they map input data into a much smaller dimension (compression) while nearly maintaining the scalar products (almost isometrically). The \emph{strong quantitative guarantees}, a distinctive feature of this technique, enabled its application to a range of problems and areas, including nearest-neighbour search \cite{kleinberg1997two}, clustering \cite{dasgupta1999learning,cohen2015dimensionality,makarychev2022performance}, outlier detection \cite{aouf2012approximate}, ensemble learning \cite{cannings2017random}, adversarial machine learning~\cite{vinh2016training}, feature hashing in machine learning \cite{jagadeesan2019understanding}, numerical linear algebra \cite{sarlos2006improved,cohen2015dimensionality,clarkson2017low}, convex optimization \cite{zhang2013recovering}, differential privacy \cite{blocki2012johnson}, neuroscience \cite{ganguli2012compressed} and numerous others; for a comprehensive literature review we refer the reader to the recent survey~\cite{freksen2021introduction}.

The focus of this work is on numerical guarantees for the \emph{almost distance-preserving} property, which is formally stated as
\begin{align}\label{eq:near_isometry_informatl}
    \| \Phi x \|^2 \approx \|x\|^2\quad \textrm{with high probability},
\end{align}
where an appropriately sampled matrix $\Phi \in \mathbb{R}^{m\times n}$ represents the projection of an $n$-dimensional vector $x$ to $m$ dimensions ($m\ll n$ is desired), and the relative approximation error in \eqref{eq:near_isometry_informatl} is referred to as \emph{distortion}.

The long-line of research~\cite{frankl1988johnson,indyk1998approximate,achlioptas2003database,ailon2006approximate,matouvsek2008variants,dasgupta2010sparse,kane2014sparser,jagadeesan2019understanding,skorski2021johnson,skorski2022johnson} has incrementally established various guarantees for \eqref{eq:near_isometry_informatl}, in the form of \emph{distortion-confidence trade-offs}: while a small distortion ensures that the analytical task can be performed with a similar effect over the embedded data, high confidence guarantees with what probability it will happen.
Yet the quantitative analysis of the property \eqref{eq:near_isometry_informatl} has remained a difficult challenge, resulting in complex proofs simplified many times~\cite{cohen2018simple,jagadeesan2019simple}, crude statistical bounds (for example, sparse variants have an exponential gap with respect to the sharp no-go results~\cite{burr2018optimal}), and a lack of finite-dimensional insights (bounds are input-oblivious which widens the gap between theory predictions and empirical performance~\cite{venkatasubramanian2011johnson,fedoruk2018dimensionality}).

This work addresses the aforementioned gap by revisiting the most promising construction of \emph{Rademacher random projections}, which uses the following matrix
\begin{align}\label{eq:rademacher_matrix}
\Phi_{k,j} = \frac{1}{\sqrt{m}}r_{k,i},\quad r_{k,i}\sim^{IID} \{-1,+1\}.
\end{align}
More specifically, this paper solves the following problem:
\begin{center}
\begin{tcolorbox}[arc=3mm, colback=white]
Give a precise, non-asymptotic, non-oblivious analysis of random projections \eqref{eq:rademacher_matrix}.
\end{tcolorbox}
\end{center}
As recently~\cite{burr2018optimal} showed, Rademacher random projections are asymptotically \emph{dimension-optimal with exact constant}; this result improves upon a previous suboptimal bound of Kane and Nelson~\cite{kane2014sparser}. The statistical performance of Rademacher projections is superior to the sparse ones, as demonstrated empirically in \ref{fig:distortionVSembedsparsity}. Furthermore, the theoretical bounds for Rademacher random projections are much better than those available for sparse analogues~\cite{cohen2018simple}. The best, prior to this paper, analysis of \eqref{eq:rademacher_matrix} is given by Achlioptas in \cite{achlioptas2003database}. It is worth noting that Rademacher projections are also superior to their Gaussian counterparts; indeed, we know that they are dominated by the gaussian-based projections~\cite{achlioptas2003database}. The relation of statistical performance and input structure has not been understood in-depth yet; as for conceptually similar research, we note that recent results show that for sparse data one can improve the sparsity of random projections, gaining in computing time ~\cite{jagadeesan2019understanding,skorski2022johnson}.

\begin{figure}
\centering
\resizebox{.6\textwidth}{!}{
\begin{tikzpicture}
\begin{axis}[
    axis x line*=bottom,
    axis y line*=left,
    xtick={-1,0,1},
    ymode=log,
    xlabel={threshold $\epsilon$},
    ylabel={$\mathbf{P}\{distortion>\epsilon\}$},
    legend style={
        at={(0.98,0.98)},
        anchor=north east,
        mark=none,
    }
]
\addplot table [x=x, y=0.1, col sep=comma, mark=none] {ecdfs.csv};
\addplot table [x=x, y=0.25, col sep=comma, mark=none] {ecdfs.csv};
\addplot table [x=x, y=1.0, col sep=comma, mark=none] {ecdfs.csv};
\legend{$p=0.10$, $p=0.25$, $p=1.00$};
\end{axis}
\end{tikzpicture}}
\caption{The distortion tail (empirical CCDF) w.r.t. the embedding density parameter
$p$ which equals the fraction of non-zero elements in the matrix. 
Dense Rademacher projections ($p=1$) are numerically superior to their sparse counterparts ($p<1$). The comparison was done on a toy dataset.
}
\label{fig:distortionVSembedsparsity}
\end{figure}

\subsection{Our Contribution}

%Our analyses of the stochastic behavior of \eqref{eq:near_isometry_informatl} offers the following novel contributions:
Our study of the stochastic behavior of \eqref{eq:near_isometry_informatl} offers the following novel contributions:
\begin{description}
%\item[(a)] \textbf{non-oblivious insights}, by quantifying the \emph{dependency on input spreadness}. Loosely speaking, we prove that more spread-out input data lead to heavier tails in the distributed distortion. As a particular case, we obtain the \emph{improved performance on sparse data}. This contribution addresses the aforementioned lack of geometric insights and understanding of input-dependency.
%\todo[inline]{how sparsity relates to spreadness (perhaps a footnote would be enough?)}
\item[(a)] \textbf{non-oblivious insights}, by quantifying the \emph{dependency on input spreadness}. Loosely speaking, we prove that more spread-out input data lead to heavier tails in the distributed distortion. In particular\footnote{We can think of sparse input as being the extreme opposite of the "well-spread" property.}, we obtain the \emph{improved performance on sparse input data}.
\item[(b)] \textbf{Schur-concavity framework}, used to provide the missing geometric intuitions for the performance of random projections. 
\item[(c)] \textbf{numerically optimal bounds}, which precisely capture the extreme behavior and cannot be improved by any constant. 
\item[(d)] \textbf{benchmarking} against previous bounds, both theoretically and empirically. 
%We bridge the gap between theory and practice by combining the findings of the theory with the evidences of the application. 
By means of distortion, we measured how high-dimensional vectors are projected with different density to prove the strength of our bounds. 
\end{description}

\subsection{Organization}
The remainder of the paper is organized as follows:
\Cref{sec:prelim} introduces notation and technical notions used throughout the paper; then \Cref{sec:contrib} discusses and benchmarks novel results of this paper, and \Cref{sec:conclusion} concludes the work.
Parts of the theoretical analysis appear in the Appendix \ref{sec:proofs}. 

\begin{figure}
\centering
\resizebox{.6\textwidth}{!}{
\begin{tikzpicture}
\begin{axis}[
    axis x line*=bottom,
    axis y line*=left,
    xtick={0.2,0.4,0.6,0.8,1},
    ymode=log,
    xlabel={embedding density $p$},
    ylabel={quadratic chaos moment},
    legend style={
        at={(0.02,0.98)},
        anchor=north west
    },
]
\addplot table [x=p_value, y=0.2, col sep=comma] {quadratic_chaos.csv};
\addplot table [x=p_value, y=0.4, col sep=comma] {quadratic_chaos.csv};
\addplot table [x=p_value, y=0.6, col sep=comma] {quadratic_chaos.csv};
\addplot table [x=p_value, y=0.8, col sep=comma] {quadratic_chaos.csv};
\addplot table [x=p_value, y=1, col sep=comma] {quadratic_chaos.csv};
\legend{$sparsity=0$,$sparsity=0.2$,$sparsity=0.3$,$sparsity=0.4$,$sparsity=0.5$};
\end{axis}
\end{tikzpicture}}
\caption{Data-aware insights. We measured the theoretical moments against \emph{input data sparsity}. The plot shows that increased sparsity in the input data leads to smaller values of the moments. This result becomes more evident when related to the embeddings density parameter $p$. }
\end{figure}

%This implies that the scalar product is nearly preserved\footnote{
%Indeed, the linearity of $\Phi$ and the \emph{polarization identity}~\cite{young1988introduction} imply$\langle \Phi x, \Phi y\rangle = \frac{\|\Phi x + \Phi y\|^2 - \|\Phi x- \Phi y\|^2}{4} \approx \frac{\|x+y\|^2-\|x-y\|^2}{4} = \langle x,y\rangle $}.
