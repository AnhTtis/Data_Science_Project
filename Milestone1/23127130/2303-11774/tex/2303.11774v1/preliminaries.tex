\section{Preliminaries}\label{sec:prelim}

We start by recalling some basic concepts from probability theory, algebra, combinatorics and optimization. 

Throughout the paper we work basic probability distributions: normal, binomial, and Rademacher. Given a vector $x$ we denote by $\|x\|$ its euclidean norm and by $\|x\|_0$ the number of its non-zero components; we also say that $x$ is $\ell$-sparse with $\ell=\|x\|_0$, for example $x=\left(\frac{1}{4},\frac{1}{4},\frac{1}{4},0\right)$ is $3$-sparse.

\emph{Partitions} represent a positive integer $q$ as a sum of positive integers. A non-decreasing and non-negative sequence $\lambda$ is a partition of $q$, denoted as $\lambda \vdash q$, when $\sum_i \lambda_i = q$; in the \emph{frequency notation} distinct parts are assigned frequencies, so that $\lambda = 1^{f_1}\ldots q^{f_q}$ where $\sum_i i f_i = q$. For example, we have $\lambda = 1^2 2  = (2,1,1)\vdash 4$. 

\emph{Monomial symmetric functions} for a given partition $\lambda =(\lambda_1,\ldots,\lambda_k)\vdash q$ are defined as the sum of all distinct monomials with exponent $\lambda$, that is
$\mathbf{m}_{\lambda}(x) = \sum_{i_1,\ldots,i_k}x_{i_1}^{\lambda_1}\cdots x_{i_k}^{\lambda_k}$
where $i_1,\ldots,i_k$ and monomials
$x_{i_1}^{\lambda_1}\cdots x_{i_k}^{\lambda_k}$ are distinct. For example, we have $\mathbf{m}_{(2,1,1,1)}(x_1,x_2,x_3,x_4) = x_{1}^{2} x_{2} x_{3} x_{4} + x_{1} x_{2}^{2} x_{3} x_{4} + x_{1} x_{2} x_{3}^{2} x_{4} + x_{1} x_{2} x_{3} x_{4}^{2}$. 

\emph{Elementary symmetric functions} are defined as $\mathbf{e}_k(x) = \sum_{i_1<\ldots<i_k}x_{i_1}x_{i_2}\cdots x_{i_k}$. Both monomial and elementary polynomials are a base of symmetric polynomials. For more details, we refer to the monographs \cite{alexanderson2020}.

The \emph{majorization order} on $n$-dimensional vectors is defined as follows:
we say that $x$ is dominated by $y$, denoted by $x\prec y$, 
if for their non-decreasing rearrangements $(x_i^\downarrow)$
and $(y_i^\downarrow)$ we have the inequality
$
 \sum_{i=1}^k x_i^\downarrow \geqslant \sum_{i=1}^k y_i^\downarrow$ for $k=1,\dots,n$ with equality when $k=n$; equivalently, $x$ can be produced from $y$ by a sequence of \emph{Robin-Hood operations}
which replace $x_i>x_j$ by $x_i\gets x_{i}-\epsilon,x_j \gets x_j+\epsilon$ for $\epsilon \in \left(0,\frac{x_i-x_j}{2}\right)$. 
 Intuitively, $x$ being dominated by $y$ means that $x$ is more spread-out/dispersed than $y$. For example, we have $\left(\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4}\right)\prec \left(\frac{1}{3},\frac{1}{3},0,\frac{1}{3}\right)$ (it takes 3 Robin-Hood transfers). 
 
 The \emph{Schur-convexity} of a function $f:\mathbb{R}^n\to \mathbb{R}$ is the following property: $x\prec y$ implies $f(x)\leqslant f(y)$; we speak of Schur-concavity when the inequality is reversed. Schur-convex or Schur-concave functions are necessary symmetric; symmetric function is Schur-convex if $\left(\frac{\partial f}{\partial x_i}-\frac{\partial f}{\partial x_j}\right)\left(x_i-x_j\right)\geqslant 0$ (Schur-Ostrowski criterion). For example, power sums $\sum_i x_i^q$ for $q\geqslant 1$ are Schur-convex. For more on \emph{majorization} and \emph{Schur-convexity} we refer to \cite{arnold2018majorization,shi2019schur}.

We define the \emph{moment domination} of a random variable $Y$ over $X$, denoted as $X\prec_{m} Y$, by requiring $\mathbf{E}X^q \leqslant \mathbf{E}Y^{q}$ for all positive integers $q$. In particular, it implies that MGF of $Y$ dominates the MGF of $X$, the majorization in the Lorentz stochastic order~\cite{arnold2018majorization}.

 
 %, where $x_i^\downarrow$ (respectively $y_i^\downarrow$) denotes the $i$-th largest component of $x$ (respectively $y$).

%We say that a random variable $Y$ dominates $X$ in the moment order, denoted as $X\prec_{m} Y$, when its moments are bigger or equal: $\mathbf{E} X^q \leqslant \mathbf{E} Y^q$ for $q=1,2,\ldots$.



%\todo[inline]{$k$-sparsify used in compressed sensing}