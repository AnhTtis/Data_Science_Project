%% Template for the submission to:
%%   The Annals of Probability [AOP]
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% In this template, the places where you   %%
%% need to fill in your information are     %%
%% indicated by '???'.                      %%
%%                                          %%
%% Please do not use \input{...} to include %%
%% other tex files. Submit your LaTeX       %%
%% manuscript as one .tex document.         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\documentclass[aop]{imsart}
\documentclass[11pt, a4paper]{article}


%\usepackage{amsmath}
%\usepackage{amsthm}
%\usepackage{amsfonts}
%\usepackage{amssymb}
%\usepackage{dsfont}
%\usepackage{mathrsfs}
%\usepackage{mathbbold}
%\usepackage{bbm}
%\usepackage{latexsym}
%\usepackage{graphicx}
%\usepackage{amssymb}
%\usepackage{epsfig,epsf,psfrag}
%\usepackage{epstopdf}
%% Packages
\RequirePackage{amsthm,amsmath,amsfonts,amssymb,amsopn,amsfonts,graphicx,color,mathrsfs}
%\RequirePackage[numbers]{natbib}
\RequirePackage[square, comma, sort&compress, numbers]{natbib}
%\RequirePackage[authoryear]{natbib}%% uncomment this for author-year citations
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}%% uncomment this for coloring bibliography citations and linked URLs
%\RequirePackage{graphicx}%% uncomment this for including figures



\marginparwidth 0pt
\oddsidemargin 0pt
\evensidemargin 0pt
\topmargin -0.5 cm
\textheight 23.2 truecm
\textwidth 16.0 truecm
\parskip 8pt
%\startlocaldefs


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Uncomment next line to change            %%
%% the type of equation numbering           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Axiom, Claim, Corollary, Hypothesis, %%
%% Lemma, Theorem, Proposition              %%
%% use \theoremstyle{plain}                 %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{???}{???}
%\newtheorem*{???}{???}
%\newtheorem{???}{???}[???]
%\newtheorem{???}[???]{???}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Assumption, Definition, Example,     %%
%% Notation, Property, Remark, Fact         %%
%% use \theoremstyle{remark}                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}{Assumption}[section]
\newtheorem{condition}{Condition}[section]
\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{???}{???}
%\newtheorem*{???}{???}
%\newtheorem{???}{???}[???]
%\newtheorem{???}[???]{???}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please put your definitions here:        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\1{\boldsymbol{1}}
\def\A{\mathcal A}
\def\B{\mathscr B}
\def\({\Big(}
\def\){\Big)}
\def\<{\langle}
\def\>{\rangle}
\def\D{\Delta}
\def\dd{\,\text{d}}
\def\G{\mathcal G}
\def\L{\mathcal L}
\def\E{\mathbb E}
\def\G{\mathcal G}
\def\LL{\mathscr L}
\def\Gg{\mathscr G}
\def\P{\mathbb{P}}
\def\H{\mathcal H}
\def\HH{\mathscr H}
\def\F{\mathcal{F}}
\def\Ff{\mathscr F}
\def\N{\mathcal N}
\def\V{\textbf{Vec}}
\def\a{\alpha}
\def\b{\beta}
\def\g{\gamma}
\def\ba{\begin{array}}
\def\ea{\end{array}}
\def\ban{\begin{eqnarray*}}
\def\ean{\end{eqnarray*}}
\def\bann{\begin{eqnarray*}}
\def\eann{\end{eqnarray*}}
\def\bnaa{\begin{eqnarray}}
\def\enaa{\end{eqnarray}}
\def\bd{\begin{description}}
\def\ed{\end{description}}
\def\be{\begin{equation}}
\def\ee{\end{equation}}
\def\bna{\begin{eqnarray}}
\def\ena{\end{eqnarray}}
\def\bphi{{\overline \varphi}}
\def\oa{{\overline \alpha}}
\def\oF{{\overline F}}
\def\oP{{\overline \Phi}}
\def\oPs{{\overline \Psi}}
\def\d{\delta}
\def\dfrac{\displaystyle\frac}
\def\dinf{\displaystyle\inf}
\def\dint{\displaystyle\int}
\def\dlim{\displaystyle\lim}
\def\dliminf{\displaystyle\liminf}
\def\dlimsup{\displaystyle\limsup}
\def\dmax{\displaystyle\max}
\def\dmin{\displaystyle\min}
\def\dref#1{(\ref{#1})}
\def\dsum{\displaystyle\sum}
\def\dsup{\displaystyle\sup}
\def\dt{\Delta T}
\def\e{\varepsilon}
\def\eq{\stackrel{\triangle}{=}}
\def\F{{\cal F}}
\def\f{\phi}
\def\X{\mathscr X}
\def\Y{\mathscr Y}
\def\Z{\mathscr Z}
\def\hx{{\widehat x}}
\def\istack#1{\displaystyle\mathop{-\mkern-10mu-\mkern-10mu-
              \mkern-11mu\longrightarrow}_{\mbox{\scriptsize$#1$}} }
\def\l{\lambda}
\def\lmin{\lambda_{\mbox{min}}}
\def\lmax{\lambda_{\mbox{max}}}
\def\ln{\mbox{ln}}
\def\lref#1{[\ref{#1}]}
\def\m{\mu}
\def\mf#1#2#3#4{\left[\begin{array}{cc} #1 & #2 \\ #3 & #4 \end{array}
                \right]}
\def\mt#1#2{\left(\begin{array}{c} #1 \\ #2 \end{array}\right)}
\def\n{\nu}
\def\nn{\nonumber}
\def\O{\Omega}
\def\oa{{\overline \alpha}}
\def\ovf{{\overline \varphi}}
\def\oG{{\overline \Gamma}}
\def\ok{ }
\def\ox{{\overline {\cal X}}}
\def\pd#1#2{\frac{\partial #1}{\partial #2}}
\def\NI{\mbox{I}\!\mbox{N}}
\def\R{I\!\!R}
\def\s{\theta}
\def\t{\tau}
\def\tr{\mbox{\textup{Tr}}}
\def\tx{{\widetilde x}}
\def\ve{\varepsilon}
\def\vf{\varphi}
\def\x{\xi}
\def\N{{\cal N}}
\def\z{\zeta}
\def\p{\phi}

\def\nstack#1{\displaystyle\mathop{-\mkern-10mu-\mkern-10mu-
              \mkern-11mu\longrightarrow}_{\mbox{\scriptsize $N\to\infty$}
              }^{\mbox{\scriptsize $#1$}}}


%\endlocaldefs

\begin{document}

%\begin{frontmatter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Random Inverse Problems Over Graphs: Decentralized Online Learning}
%\title{A sample article title with some additional note\thanksref{T1}}
%\runtitle{Random inverse problem over graphs}
%\thankstext{T1}{A sample of additional note to the title.}

%\begin{aug}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Only one address is permitted per author. %%
%% Only division, organization and e-mail is %%
%% included in the address.                  %%
%% Additional information can be included in %%
%% the Acknowledgments section if necessary. %%
%% ORCID can be inserted by command:         %%
%% \orcid{0000-0000-0000-0000}               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\author{\fnms{Tao}~\snm{Li}},
%\author{\fnms{Xiwei}~\snm{Zhang}}
\author{Tao Li and Xiwei Zhang \thanks{The authors are with
the School of Mathematical Sciences, East China Normal University, Shanghai
200241, China. Please address all the correspondences to Tao Li: tli@math.ecnu.edu.cn.}}
%\and
%\author[B]{\fnms{???}~\snm{???}\ead[label=e3]{???@???}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Addresses                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\address{School of Mathematical Sciences, East China Normal University}

%\address[B]{School of Mathematical Sciences, East China Normal University\printead[presep={,\ }]{e2}}
%\end{aug}
\date{}
\maketitle


\begin{abstract}
We establish a framework of random inverse problems with real-time observations over graphs, and present a decentralized online learning algorithm based on online data streams, which unifies the distributed parameter estimation in Hilbert space and the least mean square problem in reproducing kernel Hilbert space (RKHS-LMS). %Each node updates the information using the estimation and observation data at the current moment and obtains its estimate at the next moment by taking a weighted sum of estimations of its own and its neighbors through the consensus term. It is not required that the random forward operators satisfy special statistical assumptions such as mutual independence, spatio-temporal independence or stationarity.
%By the measurability and integration theory of mappings with values in Banach space, the spectral decomposition theory of bounded self-adjoint operators and martingale convergence theory,
We transform the algorithm convergence into the asymptotic stability of randomly time-varying difference equations in Hilbert space with $L_2$-bounded martingale difference terms and develop the $L_2$-asymptotic stability theory. It is shown that if the network graph is connected and the sequence of forward operators satisfies the \emph{infinite-dimensional spatio-temporal persistence of excitation} condition, then the estimates of all nodes are mean square and almost surely strongly consistent. By equivalently transferring the distributed learning problem in RKHS to the random inverse problem over graphs, we propose a decentralized online learning algorithm in RKHS based on non-stationary and non-independent online data streams, and prove that the algorithm is mean square and almost surely strongly consistent if the operators induced by the random input data satisfy the \emph{infinite-dimensional spatio-temporal persistence of excitation} condition.
\end{abstract}

%\begin{keyword}[class=MSC]
%\kwd[Primary ]{60H25}
%%\kwd{60B12}
%\kwd{68T05}
%\kwd[; secondary ]{39A50}
%\kwd{62G05}
%\kwd{93A14}
%\end{keyword}

%\begin{keyword}
%\kwd{Decentralized online learning}
%\kwd{random inverse problem}
%\kwd{reproducing kernel Hilbert space}
%\kwd{randomly time-varying difference equation}
%\kwd{persistence of excitation}
%\end{keyword}

%\end{frontmatter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please use \tableofcontents for articles %%
%% with 50 pages and more                   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Main text entry area:
\section{Introduction}
Inverse problems play a key role in real applications such as medical imaging, geophysics and oil exploration (\cite{bertero, cakoni, colton, isakov, engl}). From the applied point of view, the task of the inverse problem is to determine the system input (cause) from the observable system output (result), e.g., the principle of X-ray tomography in biomedicine%: the image of the internal cross-section of an object is produced by the energy decay measured by the rays on a given cross-section
, which is essentially the problem of solving the linear integral equation of the first type associated with the Radon transform (\cite{Kirsch111}). In reality, observations are usually affected by external disturbances, and the  inverse problems with noisy observations have been widely studied including the case with deterministic (\cite{Engl123, Morozov123, Werschulz, Tikhonov123}) and Gaussian white noises (\cite{Bissantz, Cavalier, Kekkonen, Gine, Hohage}) in statistical inverse problems. In recent years, with the development of data mining based on online data streams (\cite{Silva, Jiang}), 3DVAR and Kalman filtering methods were applied to solve statistical inverse problems by introducing artificial dynamical systems based on real-time observations (\cite{Iglesias1, Lu1, Jonesfg}).


It is interesting to solve inverse problems with both randomly time-varying forward operators and random noises in reality. For example, consider the online learning problem in RKHS. Let $\X\subseteq \mathbb R^n$ be the input space and $(\HH_K,\langle \cdot,\cdot \rangle _K)$ be the Hilbert space with Mercer kernel $K:\X\times \X \to \mathbb R$. At time instant $k$, the random (with unknown distribution) input data $x(k)\in \X$ and the output data $y(k)\in \mathbb R$ satisfy the measurement equation $y(k)=f_0(x(k))+v(k),k\ge 0$, where $v(k)\in \mathbb R$ is the random noise (\cite{Janz, Takemori}), then the online learning problem in $\HH_K$ is to reconstruct $f_0\in \HH_K$ by the online data stream $\{(x(k),y(k))\}_{k=0}^{\infty}$. Based on the reproducing property of RKHS (\cite{Poggio}), the above measurement equation can be further written as
\begin{equation}\label{0}
y(k)=H(k)f_0+v(k),~k\ge 0,
\end{equation}
where $H(k)$ is the randomly time-varying forward operator determined by the input data $x(k)$ satisfying $H(k)f:=\langle f,K(x(k),\cdot)\rangle _K$, $\forall f\in \HH_K$. Thus, the online learning problems in RKHS (\cite{Tarres, Dieuleveut, Ying, Lei}) come down to an inverse problem with randomly time-varying forward operators and random noise in the measurement equation (\ref{0}). The existing  works on statistical inverse problems (\cite{Bissantz, Cavalier, Kekkonen, Gine, Hohage, Math1, Lu, Iglesias1, Lu1, Lu2, Lu100, Jahn, JinB}) assumed the forward operators to be deterministic and time-invariant, which is a special form of the random inverse problems with model (\ref{0}).


In addition to online data streams, many practical problems require a decentralized and distributed information structure. When faced with large amounts of data, it is impossible for a single processor to complete the learning task by processing all the data individually due to the limited storage space and computing power. For this case, it is usually necessary to divide the overall data into several datasets and then to complete the learning process with multiple parallel processors (\cite{Rosenblatt}). For example, in the distributed multi-area state estimation of the grid (\cite{WLZ}), all buses of the grid are divided into several non-overlapping regions, and then the grid buses are represented by a graph $\G=\{\mathcal V,\mathcal E_{\G}\}$, where $\mathcal V$ is the set of regions and $\mathcal E_{\G}$ is the set of inter-regional communication channels. The state of the grid to be estimated is $f_0$, $y_i(k)$ is the real-time observation data in the region $i\in \mathcal V$, $v_i(k)$ is the random measurement noise. The grid distributed multi-region state estimation problem is then reduced to a random inverse problem over the graph $\G$ with the following measurement equations.
\begin{equation}\label{1}
y_i(k)=H_i(k)f_0+v_i(k),~k\ge 0,~i\in \mathcal V,
\end{equation}
where $H_i(k)=\delta_i(k)H_i$, $\{\delta_i(k),k\ge 0\}$ is the Bernoulli sequence and $H_i$ is a deterministic and time-invariant observation matrix.
%In terms of the information structure of the network, distributed learning can be divided into two categories (\cite{Nassif}): distributed learning with fusion centers and decentralized learning (without fusion centers). In decentralized learning, each node cooperates with its neighbors through information interaction to complete the learning task, in which each node has independent computation and information exchange capabilities, and occasional communication failures between nodes do not affect the overall learning task, so decentralized learning is more robust than distributed learning with fusion centers (\cite{Das}).% Until now, there is not yet any research on decentralized online learning algorithm in RKHS, Guo \emph{et al}. \cite{GUO, GUO1}, Lin and Cavher \cite{Lin} studied the problem of distributed kernel regression based on individual breakdown strategies in RKHS, they required a weighted average of all nodes' estimates through a data fusion center to give a global estimate. %and thus distributed learning based on divide and conquer strategy belongs to distributed learning with fusion centers.


With the development of online learning in recent years (\cite{Ippel}), decentralized online learning algorithms based on parameter estimation and supervised learning in RKHS have been intensively studied.
Firstly, the parameter estimation problem, as an important part of random inverse problems in finite-dimensional space, has been well studied. Specifically, the decentralized online algorithm for inverse problems with randomly spatio-temporal independent forward operators (the forward operators of each node over the graph are independent stochastic processes and the forward operators of different nodes are also independent of each other) was proposed in \cite{Lopes, Cattivelli, Sayed, Gholami, Abdolee} via the collaborative strategy of diffusion. Piggott \emph{et al}. \cite{Piggott, Piggott1} proposed decentralized algorithms over fixed communication graphs for random inverse problems with time-dependent forward operators. The convergence of decentralized online learning algorithms were analyzed by Ishihara and Alghunaim \cite{Ishihara} for the inverse problem with spatial independent forward operators. Secondly, supervised learning is another important part of the random inverse problem (\cite{Lyaqini}) in which each random input data uniquely determines a random forward operator. Shin \emph{et al}. \cite{Shin} proposed decentralized adaptive learning algorithms over graphs in RKHS in a deterministic framework. Deng \emph{et al}. \cite{Deng} used the multiplicative operator in the saddle point problem to carve out the communication structure of decentralized networks and proposed a distributed consensus-based online learning algorithm with  independent and identically distributed observations. The nonlinear online learning problems in RKHS with spatio-temporal independent observations were studied in  \cite{Mitra, Chouvardas, Bouboulis}.
%, where Mitra and Bhatia \cite{Mitra}, and Chouvardas and Draief \cite{Chouvardas} studied the kernel least mean square (KLMS) problem and proposed a decentralized online learning algorithm, however, with the time shifting, the communication load and computation of the whole network increase linearly. To solve this problem, Bouboulis \emph{et al}. \cite{Bouboulis} approximated the kernel function with random Fourier features and gave an asymptotic convergence analysis of decentralized online kernel learning based on the Metropolis diffusion strategy.


Up to now, there have been a large number of mature research results on classical inverse problems in the deterministic framework, and the existing work on the inverse problems based on statistical and stochastic frameworks can be basically segmented into three categories: (i) statistical inverse problems based on deterministic time-invariant compact forward operators and real-time observations in a separable Hilbert space (\cite{Iglesias1, Lu1, Jonesfg}); (ii) distributed parameter estimation problems in finite-dimensional spaces (\cite{WLZ, Lopes, Cattivelli, Sayed, Gholami, Abdolee, Piggott, Piggott1, Ishihara}); (iii) decentralized online learning problems based on stationary (e.g., independent and identically distributed) observations in RKHS (\cite{Shin, Deng, Mitra, Chouvardas, Bouboulis}). There are some basic problems, for example,
\begin{itemize}
\item Even based on centralized online learning theory, can we solve inverse problems with randomly time-varying non-compact forward operators ?
\item Can we establish a unified framework for random inverse problems in infinite-dimensional Hilbert spaces, distributed parameter estimation problems in finite-dimensional spaces and online learning problems in reproducing kernel Hilbert spaces ?
\item Can we develop a decentralized RKHS learning theory based on non-stationary and non-independent online data streams ?
\end{itemize}


Inspired by the above, we propose a class of random inverse problems over graphs, establish a unified framework to deal with the above three types of problems, and give a decentralized online learning algorithm in Hilbert space. The learning algorithm of each node in the distributed network consists of both the innovation term and the consensus term. The innovation term is used to handle the update of the node's own data, and the consensus term is a weighted sum of its own estimate and the estimates of all neighboring nodes. The forward operator is randomly time-varying and does not need to satisfy special statistical properties, such as temporal independence (the forward operator of each node over the graph is independent with respect to time), spatial independence (the forward operators of different nodes are independent of each other at any moment) and stationarity, while the random measurement noise is no longer restricted to the  Gaussian white noise with independent identical distribution, all of which bring essential difficulties to the analysis of convergence, and the existing related methodologies in \cite{Math1, Iglesias1, Lu1, Jonesfg, SmaleYao, Tarres, Lu100, Mathe123} are no longer applicable, since the singular value decomposition (SVD) of linear compact operators in Hilbert space can only handle the inverse problem with deterministic and time-invariant linear compact forward operators. Specifically, Smale and Yao \cite{SmaleYao}, and Tarres and Yao \cite{Tarres} transformed the online learning problem in reproducing kernel Hilbert space into an inverse problem with Hilbert-Schmidt operator based on independent and identically distributed observations.
%Mathe and Pereverzev \cite{Math1, Mathe123} used discrete projection regularization methods to effectively estimate the solution of the statistical inverse problem; 3DVAR and Kalman filtering methods were applied to solve the statistical inverse problem (\cite{Iglesias1, Lu1, Jonesfg}), where Iglesias \emph{et al}. \cite{ Iglesias1} gave the Tikhonov iterative regularization method based on single and real-time observations, respectively, and the upper bound on the mean squared error of the asymptotic regularization method (ARM) was given by Kalman-Bucy filtering and 3DVAR in the continuous timescale by Lu \emph{et al}. \cite{Lu1}, Jones and Simpson \cite{Jonesfg}; Lu and Mathe \cite{Lu100} presented the batch stochastic gradient descent (BSGD) algorithm by iteratively selecting the Nystr$\ddot{\text{o}}$m estimate of the forward operator and gives an upper bound on the mean squared error of the algorithm.
%Smale and Yao \cite{SmaleYao}, and Tarres and Yao \cite{Tarres} transformed the online learning problem in reproducing kernel Hilbert space into an inverse problem with Hilbert-Schmidt operator based on independent identically distributed observations. The above mentioned research methodology is based on the singular value decomposition (SVD) of linear compact operators in Hilbert space, and this approach can effectively handle the inverse problem with deterministic time-invariant linear compact forward operators, however, it is not applicable to the case with randomly time-varying linear bounded operators.
It is worth noting that we proposed the stochastic spatio-temporal persistence of excitation (SSTPE) condition and the sample path spatio-temporal persistence of excitation (SPSTPE) condition on the minimum eigenvalue of the information matrix for distributed online learning problems in finite-dimensional spaces in \cite{WLZ, ZLG, ZLF}, respectively. However, the above excitation conditions all require to some extent that the information matrix is positive definite, i.e. the eigenvalues of the matrix have strictly positive lower bounds, which are usually not applicable for the inverse problem in infinite-dimensional Hilbert space. Note that even for strictly positive compact operators, neither the SSTPE nor the SPSTPE conditions can hold, due to the infimum of the eigenvalues of the compact operator is zero.

In this paper, a unified framework is given for integrating the random inverse problem, the distributed parameter estimation problem in finite-dimensional spaces, and the least mean square problem in RKHS. By means of measurability and integration theory for mappings with values in Banach spaces, spectral decomposition theory for bounded self-adjoint operators, and martingale convergence methods, the properties concerning the measurability, conditional independence, expectation and conditional expectation of random elements with values in topological spaces are explored, and the $L_p^q$-stability condition on the sequence of operator-valued random elements  is proposed, then we establish the theory of a class of $L_2$-asymptotic stability of infinite-dimensional random difference equations with $L_2$-bounded martingale difference terms. Under the above theoretical framework, we give a sufficient condition for a class of operator-valued random element sequences of random forward operators and Laplacian matrices of  graphs to be $L_2^2$-stable, i.e. the \emph{infinite-dimensional spatio-temporal persistence of excitation} condition, and establish the convergence condition of decentralized algorithms for random inverse problems over graphs. This, in turn, develops a theory of decentralized learning in RKHS based on non-stationary and non-independent online data streams. Compared with the existing literature, the contributions of this paper are summarized as follows.

\begin{itemize}
\item To construct a completely self-contained  theoretical framework throughout the paper, %both vector-valued mappings with respect to the topology induced by the norm are strongly measurable and operator-valued mappings with respect to the uniform operator topology or strong operator topology are strongly measurable are included in the category of random elements with values in topological spaces.
the strongly measurable vector-valued mappings and operator-valued mappings are both treated as the random elements with values in topological spaces. Based on the analytical theory of Banach spaces (\cite{hp, hy, hy2}), including weak and strong measurability, independence and conditional independence of functions with values in Banach spaces, Pettis and Bochner integral theory and the conditional expectation property of Bochner integrable functions, we further explore the probability structure among random elements with values in different topological spaces, namely (i) the measurable structure of random elements taking values in strong operator topological spaces; (ii) the properties of expectations and conditional expectations of random elements taking values in uniform operator topological spaces and strong operator topological spaces; (iii) the properties of independence and conditional independence of a family of random elements with values in general topological spaces; (iv) the properties of independence and conditional independence of random elements with values in strong operator topological spaces and norm-induced topological spaces.
\item We weaken the constraints on the operators in most of the existing studies on inverse problems, such as Mathe and Pereverzev \cite{Math1}, Lu and Mathe \cite{Lu}, Lu \emph{et al}. \cite{Lu1, Lu2}. On one hand, we study the general bounded forward operators instead of compact ones; on the other hand, the forward operators can be a randomly time-varying operator-valued elements in the stochastic framework instead of deterministic and time-invariant ones.
%\item To overcome the difficulties associated with the lack of information in solving the inverse problem from a single observation, existing regularization methods all require some knowledge of the a priori information of the solution of the inverse problem. For example, the Tikhonov, Landweber, and Gauss-Newton iterative regularization methods in \cite{Hanke3, Bakushinskii111, Kindermann} usually need to select the regularization parameters according to the smoothness of the solution, so as to achieve the regularization effect by stopping the iterations early. However, it is intrinsically difficult to obtain prior information of the solution to the inverse problem. Therefore, we propose a decentralized online learning strategy based on real-time observation data, which effectively avoids the dependence on the prior information of the solution to the inverse problem.
\item The convergence analysis of the algorithm has been transformed into the $L_2$-asymptotic stability of randomly time-varying difference equations in Hilbert space. Since the forward operators of inverse problems in infinite-dimensional Hilbert space usually do not have bounded inverse, the existing asymptotic stability theory on infinite-dimensional random difference equations with compressive operators in Hilbert space (\cite{Ungureanu1, Kubrusly, Vajjha, Ungureanu2, Ungureanu3, zwzwxcbs}) cannot be applied to inverse problems. In this paper, we propose the $L_p^q$-stability condition on the sequence of the products of operator-valued random elements, and establish the $L_2$-asymptotic stability theory for a family of random difference equations in Hilbert space with $L_2$-bounded martingale difference terms. Sufficient conditions on the stability of a class of operator-valued random sequences composed of forward operators and Laplacian matrices of connected graphs are given. Based on the above theory, we prove that if the graph is connected, and forward operators satisfy the \emph{infinite-dimensional spatio-temporal persistence of excitation} condition, then all nodes' estimates of the solution of the random inverse problem over graph are mean square and almost surely strongly consistent.
\item A theory of decentralized online learning in RKHS is developed. Almost the existing literature on online supervised learning in RKHS (e.g., \cite{Shin, Bouboulis, GUO, GUO1, Lin}) are based on independent and identically distributed data. We propose a decentralized online learning algorithm based on non-stationary and non-independent online data streams in RKHS and establish the convergence condition, by equivalently transforming the distributed learning problem in RKHS into the random inverse problems over graphs. Particularly, if the graph has only one node and the random input data are independent and identically distributed without measurement noises, then our algorithm degenerates to the centralized online learning algorithm without regularization parameters in \cite{Dieuleveut, Ying}.
\end{itemize}


\section{Preliminaries of topology and measurability}

To rigorously establish the theoretical framework of random elements with values in the topological space, we introduce the following notation and definition. Let the $\sigma$-algebra generated by sets $\{A_i,i\in \mathscr I\}$ be $$\bigvee_{i\in \mathscr I}A_i:=\sigma\left(\bigcup_{i\in \mathscr I}A_i\right),$$
where $\mathscr I$ is the set of indicators.
%
\begin{definition}\label{jihukefenzhi}
Let $(\Omega,\F,\P)$ be a complete probability space, and $(\mathscr U,\tau)$ a topological space. Given the mapping $f:\Omega\to \mathscr U$, if there exists a separable closed subset $\mathscr U_0$ of $\mathscr U$ and a subset $\Omega_0$ of $\Omega$ with probability measure $1$, such that $$f(\Omega_0):=\{f(x):x\in \Omega_ 0\}\subseteq \mathscr U_0,$$ then $f$ is called  almost separably valued with respect to $\tau$.

\end{definition}
%
\begin{definition}\label{tuopukongjian}
Let $(\Omega,\F,\P)$ be a complete probability space, $(\mathscr U,\B(\mathscr U;\tau))$ be a measurable space, where $\tau$ is the topology on $\mathscr U$, and $\B(\mathscr U;\tau)$ is the Borel $\sigma$-algebra of the topological space $(\mathscr U,\tau)$, i.e. the smallest $\sigma$-algebra containing all open sets in $\mathscr U$. A mapping $f:\Omega\to \mathscr U$ is said to be a random element with values in the topological space $(\mathscr U,\tau)$ if it is $\F/\B(\mathscr U;\tau)$-measurable and almost separably valued with respect to $\tau$.
\end{definition}
%
\begin{definition}\label{fenbudingyi}
Let $(\Omega,\F,\P)$ be a complete probability space, and if $f$ is a random element with values in the topological space $(\mathscr U,\tau)$, then the distribution of $f$ is defined by the Borel probability measure $\mu_{f}(B):=\P(f^{-1}(B))$ on $(\mathscr U,\tau )$, $\forall B\in \B(\mathscr U;\tau)$.
\end{definition}

Throughout this paper, $(\Omega,\F,\P)$ is assumed to be a complete probability space. Let $\tau_{\text{N}}(\X)$ be the topology induced by the norm $\|\cdot\|_{\X}$ in a Banach space $\X$. It follows from Lemma \ref{mse} that the mapping $f:\Omega\to \X$ is strongly measurable with respect to $\tau_{\text{N}}(\X)$ if and only if $f$ is a random element with values in the Banach space $(\X,\tau_{\text{N}}(\X))$. Particularly, if $\X$ is separable with respect to $\tau_{\text{N}}(\X)$, then any $\F/\B(\X;\tau_{\text{N}}(\X))$-measurable map $f:\Omega\to \X$ is a random element with values in $(\X,\tau_{\text{N}}(\X))$.

Let $\mathscr L(\mathscr Y,\mathscr Z)$ be a linear space consisting of all bounded linear operators mapping from the Banach space $\mathscr Y$ to the Banach space $\mathscr Z$, in particular, $\mathscr L(\mathscr Z):=\mathscr L( \mathscr Z,\mathscr Z)$.
Let $\tau_{\text{S}}(\mathscr L(\mathscr Y, \mathscr Z))$ be the strong operator topology of $\mathscr L(\mathscr Y,\mathscr Z)$. It follows from  \cite{hy} that $O\in \tau_{\text{S}}(\mathscr L(\mathscr Y,\mathscr Z))$ if and only if there exists an integer $k>0$, $x_1,\cdots,x_k\in \Y$ and a constant $\varepsilon>0$ such that
\[\bigcap_{j=1}^k\left\{T\in \mathscr L(\mathscr Y,\mathscr Z): \left\|Sx_j-Tx_j\right\|_{\mathscr Z}<\varepsilon \right\}\subseteq O,~\forall S\in O.\]
Combining the Definition \ref{tuopukongjian} and Corollary 1.4.7, Proposition 1.1.28 and Corollary 1.1.29 in \cite{hy}, we directly obtain the following properties of operator-valued random elements.

\begin{proposition}\label{nlllwwieiie}

Let $f_0:\Omega\to \mathscr L(\mathscr Y,\mathscr Z)$, $f_1:\Omega\to \X$, $f_2:\Omega\to \LL(\X,\Y)$, $g_1:\Omega\to \LL(\X,\Y)$ and $g_2:\Omega\to \LL(\Y,\mathscr Z)$.
\begin{itemize}
\item[a.] The mapping $f_0$ is a random element with values in $(\mathscr L(\mathscr Y,\mathscr Z),\tau_{\text{S}}(\mathscr L(\mathscr Y,\mathscr Z)))$ if and only if the mapping $f_0y:\omega\mapsto f_0(\omega)y$ is a random element with values in $(\mathscr Z,\tau_{\text{N}}(\Z))$ for arbitrary $y\in \Y$.
\item[b.] If $f_1$ is strongly measurable with respect to $\tau_{\text{N}}(\X)$ and $g_1x:\Omega\to \mathscr Y$ is strongly measurable with respect to $\tau_{\text{N}}(\Y)$, $\forall x\in \X$, then $g_1f_1:\Omega\to \mathscr Y$ is strongly measurable with respect to $\tau_{\text{N}}(\Y)$.
\item[c.] For arbitrary $x\in\X$ and $y\in \Y$, if the mapping $f_2x:\Omega\to \Y$ is strongly measurable with respect to $\tau_{\text{N}}(\Y)$ and the mapping $g_2y:\Omega\to \mathscr Z$ is strongly measurable with respect to $\tau_{\text{N}}(\mathscr Z)$, then the mapping $(g_2f_2)x':\Omega\to \mathscr Z$ is strongly measurable with respect to $\tau_{\text{N}}(\mathscr Z)$ for arbitrary $x'\in \X$.

\end{itemize}
\end{proposition}

\begin{remark}
If the operator-valued mapping $f:\Omega\to \LL(\mathscr Y,\mathscr Z)$ is strongly measurable with respect to the uniform operator topology $\tau_{\text{N}}(\mathscr L(\mathscr Y,\mathscr Z))$, then for arbitrary $y\in \mathscr Y$, $fy:\Omega\to \mathscr Z$ is strongly measurable with respect to $\tau_{\text{N}}(\mathscr Z)$. By Proposition \ref{nlllwwieiie}. (a) and Pettis measurability theorem (\cite{Blasco111}), it follows that a random element with values in $(\mathscr L(\mathscr Y,\mathscr Z),\tau_{\text{N}}(\mathscr L(\mathscr Y,\mathscr Z)))$ must be one with values in
$(\mathscr L(\mathscr Y,\mathscr Z),\tau_{\text{S}}(\mathscr L(\mathscr Y,\mathscr Z)))$. For the measurability of mappings with values in the space of operators with different topologies, one can further refer to \cite{hp, hy, Blasco111}.
\end{remark}

For a random element $f:\Omega\to \X$ with values in $(\X,\tau(\X))$, we denote the $\sigma$-algebra generated by $f$ in the sense of the topology $\tau(\X)$ by
\[\sigma(f;\tau(\X)):=\left\{f^{-1}(B):B\in \B(\X;\tau(\X))\right\}. \]
For any given sub-$\sigma$-algebra $\F_0\subset \F$, denote $f\in \F_0$ if $f$ is measurable with respect to $\F_0$.
Based on the above definitions and propositions, we have the following proposition, the proof of which is given in Appendix \ref{appendixb}.

\begin{proposition}\label{wenknknkn}
If $f:\Omega\to \X$ is a random element with values in the Banach space $(\X,\tau_{\text{N}}(\X))$, $T:\Omega\to \mathscr L(\mathscr X,\mathscr Y)$ is a random element with values in the topological space $(\mathscr L(\mathscr X,\mathscr Y),\tau_{\text{S}}(\mathscr L(\mathscr X,\mathscr Y)))$, then $Tf:\omega\mapsto T(\omega)f(\omega)$ satisfies
\[Tf \in \left(\bigvee_{x\in\X}\sigma(Tx;\tau_{\text{N}}(\Y))\right)\bigvee \sigma(f;\tau_{\text{N}}(\X)). \]
\end{proposition}

As for expectations and conditional expectations of the random elements with values in $(\X,\tau_{\text{N}}(\X))$, we introduce the following notations. Let $L^0(\Omega;\X)$ be a linear space consisting of all the functions that take values in the Banach space $(\X,\tau_{\text{N}}(\X))$ and are strongly measurable with respect to $\tau_{\text{N}}(\X)$; $L^0(\Omega,\Gg;\X)$ be the linear space composed of all $\Gg/\B(\X;\tau_{\text{N}}(\X))$-measurable functions in $L^0(\Omega;\X)$, where $\Gg$ is a sub-$\sigma$-algebra of $\F$; The Bochner space $L^p(\Omega;\X)$ consists of all functions $f:\Omega\to \X$ that are strongly measurable with respect to $\tau_{\text{N}}(\X)$ and satisfy
\[\|f\|_{L^p(\Omega;\X)}:=\left(\int_{\Omega}\|f\|_{\X}^p\dd\P\right)^{\frac{1}{p}}<\infty,~1\leq p<\infty.\]
In particular, we write $L^p(\Omega):=L^p(\Omega;\mathbb R)$.

\begin{definition}[\cite{hy2}]\label{vnsllp}
\rm{If $f\in L^1(\Omega;\X)$, then the mathematical expectation of $f$ is defined as the Bochner integral $$ \E[f]=(\text{B})\int_{\Omega}f\dd\P.$$
}
\end{definition}

\begin{remark}\label{fnklwmemmemee}
\rm{Without raising ambiguity, the Bochner integral in this paper will omit the capital letter (B) in front of the integral symbol. }
\end{remark}

\begin{definition}[\cite{hy}]
\rm{Let $\Gg\subseteq \F$ be a sub-$\sigma$-algebra, $f\in L^0(\Omega;\X)$ and $g\in L^0(\Omega,\Gg;\X)$. If
\ban
\int_Fg\dd\P=\int_Ff\dd\P,~\forall F\in \Gg_f\cap \Gg_g,
\ean
where $\Gg_{\phi}:=\{F\in \Gg:\1_F\phi\in L^1(\Omega;\X)\}$, $\phi\in L^0(\Omega;\X)$, then $g$ is called the conditional expectation of $f$ with respect to $\Gg$.
}
\end{definition}

By Lemma \ref{nvkvpeoeo}, it follows that there exists a unique conditional expectation $\E[f|\Gg]\in L^0(\Omega,\Gg;\X)$ of the Bochner integrable random element $f$ with values in $(\X,\tau_{\text{N}}(\X))$, whereby $\E[f|\Gg]$ is a random element with values in $(\X,\tau_{\text{N}}(\X))$.
The reader can further refer to \cite{hy} for the property of conditional expectation.
We have the following propositions of the conditional expectation, and the proofs are given in Appendix \ref{appendixb}.

\begin{proposition}\label{vnwlssfweewwfew}
If $f\in L^1(\Omega;\mathscr L(\mathscr Y,\mathscr Z))$ is a random element with values in Banach space $(\mathscr L(\mathscr Y,\mathscr Z),\tau_{\text{N}}(\mathscr L(\mathscr Y,\mathscr Z))$, then $fy\in L^1(\Omega;\mathscr Z)$ is the random element with values in Banach space $(\mathscr Z,\tau_{\text{N}}(\mathscr Z))$, and $\E[fy]=\E[f]y$, $\forall y\in \mathscr Y$.
\end{proposition}

\begin{proposition}\label{tiaojianqiwangxingzhi}
If $f\in L^2(\Omega;\mathscr L(\mathscr Y,\mathscr Z))$ is a random element with values in the Banach space $(\mathscr L(\mathscr Y,\mathscr Z),\tau_{\text{N}}(\mathscr L(\mathscr Y,\mathscr Z)))$, $y\in L^2(\Omega,\Gg;\Y)$ is a random element with values in the Banach space $(\Y,\tau_{\text{N}}(\Y))$, where $\Gg$ is a sub-$\sigma$-algebra of $\F$, then $fy\in L ^1(\Omega;\mathscr Z)$ is a random element with values in the Banach space $(\mathscr Z,\tau_{\text{N}}(\mathscr Z))$ and $\E[fy|\Gg]=\E[f|\Gg]y~\text{a.s.}$
\end{proposition}

In terms of the independence and conditional independence of random elements with values in the topological space, we have the following definitions.

\begin{definition}\label{dulixing}
Let $\mathscr I$ be a set of indices, $f_j$, $j\in \mathscr I$ be the random elements with values in $(\X_j,\tau(\X_j))$. If for arbitrarily different indices $\a_1,\cdots,\a_n\in \mathscr I$ and arbitrary $B_1\in \B(\X_1;\tau(\X_{\a_1})),\cdots,B_n\in \B(\X_n;\tau(\X_{\a_n}))$,
\[\P\left(f_{\a_1}\in B_1,\cdots,f_{\a_n}\in B_n\right)=\prod_{j=1}^n\P\left(f_{\a_j}\in B_j\right),\]
then, we claim that the random elements $f_j$, $j\in \mathscr I$ with values in $(\X_j,\tau(\X_j))$ are mutually independent.
\end{definition}

\begin{definition} \label{tiaojiandulixing}
Let $\mathscr I$ be a set of indices, $\mathcal F_{i}$ as well as $\Gg$ are sub-$\sigma$-algebra of $\F$, $i \in \mathscr I$. If for arbitrarily different indices $\a_1,\cdots,\a_n\in \mathscr I$ and arbitrary $A_1\in \F_{\a_1},\cdots,A_n\in \F_{\a_n}$,
\[\P\left.\left(\bigcap_{j=1}^nA_j\right|\Gg\right)=\prod_{j=1}^n\P\left(A_j|\Gg\right)~\text{a.s.},\]
then it is said that $\mathcal F_{\alpha}$ is conditionally independent with respect to $\Gg$, $\alpha \in \mathscr I$. If $\sigma(f_{i};\tau(\X_i))$ is conditionally independent with respect to $\Gg$, $i \in \mathscr I$, then we say that the random elements $f_{i}$, $i \in \mathscr I$ with values in $(\X_i,\tau(\X_i))$ are conditionally independent with respect to $\Gg$.
\end{definition}

\begin{remark}\label{guidaoduli}
If $f$ is a random element with values in $(\mathscr L(\mathscr Y,\mathscr Z),\tau_{\text{S}}(\mathscr L(\mathscr Y,\mathscr Z)))$.
It follows from \cite{hy} that the Borel $\sigma$-algebra $\B(\mathscr L(\mathscr Y,\mathscr Z);\tau_{\text{S}}(\mathscr L(\mathscr Y,\mathscr Z)))$ is generated by the following open set
\[V_{f_0,y_0,\varepsilon_0}:=\left\{f\in \mathscr L(\mathscr Y,\mathscr Z):\|(f-f_0)y_0\|_{\mathscr Z}<\varepsilon_0\right\},\]
where $f_0\in \mathscr L(\mathscr Y,\mathscr Z),y_0\in \mathscr Y$ and $\varepsilon_0>0$. Notice that
\[f^{-1}(V_{f_0,y_0,\varepsilon_0})=\{\omega\in \Omega:\|f(\omega)y_0-f_0y_0\|_{\mathscr Z}<\varepsilon_0\}=(fy_0)^{-1}(B_{\mathscr Z }^{\circ}(f_0y_0,\varepsilon_0)),\]
where $B_{\mathscr Z}^{\circ}(x,r):=\{y\in \mathscr Z:\|y-x\|_{\mathscr Z}<r\}$ is the open ball in the Banach space $\mathscr Z$ centered at $x$ and with $r$ as the radius. Since a open set in the Banach space $(\mathscr Z,\tau_{\text{N}}(\mathscr Z))$ is the union of countable open balls, the random element $g$ with values in the Banach space $\mathscr Z$ is independent (conditionally independent with respect to a sub-$\sigma$-algebra $\G$ of $\mathcal F$) of the random element $f$ with values in $(\mathscr L(\mathscr Y,\mathscr Z),\tau_{\text{S}}(\mathscr L(\mathscr Y,\mathscr Z)))$ if and only if $g$ is independent (conditionally independent with respect to a sub-$\sigma$-algebra $\G$ of $\mathcal F$) of the random element $fy$ with values in $(\mathscr Z ,\tau_{\text{N}}(\Z))$, $\forall y\in \mathscr Y$.
\end{remark}

\noindent
As a result of Definitions \ref{dulixing} and \ref{tiaojiandulixing}, we have the following propositions, the proofs of which can be found in Appendix \ref{appendixb}.

\begin{proposition}\label{lemmaA4}
If the family of random elements $\{f_k,k\ge 1\}$ with values in $(\X_1,\tau(\X_1))$ and the family of random elements $\{g_k,k\ge 1\}$ with values in $(\X_2,\tau(\X_2))$ are independent, then $\bigvee_{i=k}^{\infty}\sigma(f_i;\tau(\X_1))$ is conditionally independent of  $\bigvee_{i=k}^{\infty}\sigma(g_i;\tau(\X_2))$ with respect to $\bigvee_{i=1}^{k-1}(\sigma(f_i;\tau(\X_1))$ $\bigvee$ $\sigma(g_i;\tau(\X_2)))$, $\forall\ k\ge 2$.
\end{proposition}

\begin{proposition} \label{lemmaA6}
Let $\Gg$ be the sub-$\sigma$-algebra of $\F$, the random element $T:\Omega\to \mathscr L (\X,\mathscr Y)$ with values in $(\mathscr L(\X,\mathscr Y),\tau_{\text{S}}(\mathscr L(\X,\mathscr Y)))$ satisfies $\E[\|T\|_{\mathscr L(\X,\mathscr Y)}^2]<\infty$ and $f\in L^2(\Omega;\X)$ is a random element with values in the Banach space $(\X,\tau_{\text{N}}(\X))$. If the random element $T$ with values in $(\mathscr L(\X,\mathscr Y),\tau_{\text{S}}(\mathscr L(\X,\mathscr Y)))$ is conditionally independent of the random element $f$ with values in the Banach space $(\X,\tau_{\text{N}}(\X))$ with respect to $\Gg$, then
\[\E[Tf|\Gg]=\E[T\E[f|\Gg]|\Gg]~\text{a.s.}\]
\end{proposition}

\section {Online learning for random inverse problems over graphs}
We first introduce some notations in Hilbert spaces. Let $(\X_i,\langle \cdot,\cdot\rangle _{\X_i})$ be a Hilbert space, where the norm induced by the inner product is defined as $\|x_i\|_{\X_i}:=\sqrt{\langle x_i,x_i\rangle _{\X_i}}$, $x_i\in \X_ i$, $i=1,\cdots,n$, and the Hilbert direct sum space is denoted as
\[\bigoplus_{i=1}^n\X_i=\left\{x=(x_1,\cdots,x_n):x_i\in \X_i,1\leq i\leq n\right\},\]
where the inner product is defined as $\langle x,y\rangle _{\bigoplus_{i=1}^n\X_i}:=\sum_{i=1}^n\langle x_i,y_i\rangle _{\X_i}$,$\forall x=(x_1,\cdots,x_n),y=(y_1,\cdots,y_n)\in \bigoplus_{i=1}^n\X_i$, and in particular, we write $\bigoplus_{i=1}^n\X:=\X^n$. Let $\X$ and $\mathscr Y$ be Hilbert spaces, and without causing a notational ambiguity, we write the Kronecker product of the vector $\1_n\in \mathbb R^{n}$ and $\f\in \X$ as $\1_n\otimes f:=(f,\cdots,f)\in \X^n$ and the Kronecker product of the matrix $A\in \mathbb R^{n\times m}$ and $B\in \LL(\X,\Y)$ as the matrix of operator elements
\ban
A\otimes B:=\begin{pmatrix}
a_{11}B & \cdots & a_{1m}B \\
\vdots & \ddots & \vdots \\
a_{n1}B & \cdots & a_{nm}B
\end{pmatrix}
\in \mathscr L(\X^m,\mathscr Y^n)
,
\ean
the operations and properties of operator matrices in Hilbert direct sum spaces can be found in \cite{sl}. Let $\X$ be a Hilbert space and $T\in \LL(\X)$, if $T^*=T$, i.e. $\langle Tx,y \rangle _{\X}=\langle x, Ty \rangle _{\X}$, $\forall x,y\in \X$, then $T$ is called the self-adjoint operator. If the self-adjoint operator $T\in \LL(\X)$ satisfies $\langle Tx,x \rangle _{\X}\ge 0$, $\forall x\in \X$, then $T$ is said to be positive self-adjoint, and is denoted by $T\ge 0$, and in particular, if $\langle Tx,x \rangle _{\X}> 0$ for any given non-zero element $x$ in $\X$, then $T$ is called the strictly positive self-adjoint operator and is denoted by $T>0$.

The weighted directed graph is denoted by $\mathcal{G}=\{\mathcal{V},\mathcal{E}_{\mathcal{G}},\mathcal{A}_{\mathcal{G}}\}$, where $\mathcal{V}=\{1,...,N\}$ and $\mathcal{E}_{\mathcal{G}}$ are the set of nodes and the set of edges, respectively, and the node pair $(j,i)\in\mathcal{E}_\mathcal G$ if and only if there exists an edge connecting node $j$ and node $i$ in $\mathcal E_{\G}$. Denote the set of neighboring nodes of node $i$ by $\mathcal{N}_i=\{j\in \mathcal{V}:(j,i)\in
\mathcal{E}_{\mathcal{G}}\}$, and the matrix $\mathcal{A}_{\mathcal{G}}=[a_{ij}]\in \mathbb{R}^{N\times N}$ is called the weighted adjacency matrix of the graph $\G$, where for any given  $i,j\in \mathcal V$, $a_{ii}=0$ and $a_{ij}>0$ if and only if $j\in \mathcal{N}_i$. The weight $a_{ij}$ of the graph $\G$ reflect the topological structure and connection strength between the nodes. If $\A_{\G}$ is a symmetric matrix, then $\G$ is called undirected graph. The Laplacian matrix of the graph $\G$ is defined by $\mathcal{L}_{\mathcal{G}}=\mathcal{D}_{\mathcal{G}}-\mathcal{A}_{\mathcal{G}}$, where $\mathcal{D}_{\mathcal{G}}=\text{diag}\{\sum_{j=1}^Na_{1j},\sum_{j=1}^Na_{2j},\cdots,\sum_{j=1}^Na_{Nj}\}$ is the degree matrix of the graph $\G$. $\L_{\G}$ is symmetric matrix if $\G$ is undirected graph (\cite{Olfati}).

\subsection{Online random inverse problems over graphs}
Consider a distributed communication network modeled by a weighted undirected graph $\mathcal{G}=\{\mathcal{V},\mathcal{E}_{\mathcal{G}},\mathcal{A}_{\mathcal{G}}\}$ consisting of $N$ nodes. The observation data $y_i(k)$ of the $i$-th node at instant $k$ satisfies the measurement equation
\begin{equation}\label{measuramentmodel}
y_i(k)=H_i(k)f_0+v_i(k),~k\ge 0,~i=1,\cdots,N,
\end{equation}
where $f_0\in \X$ is an unknown element in the Hilbert space $\X$, $H_i(k):\Omega\to \mathscr L(\X,\Y_i)$ is an operator-valued random element with values in $(\LL(\X,\Y_i),\tau_{\text{S}}(\LL(\X,\Y_i)))$ at instant $k$, which is called the random forward operator, and the additive noise $v_i(k):\Omega\to \Y_i$ is a random element with values in Hilbert space $(\Y_i,\tau_{\text{N}}(\Y_i))$.
Denote $y(k)=(y_1(k),\cdots,y_N(k)):\Omega\to \bigoplus_{i=1}^N\Y_i$, $ v(k)=(v_1(k),\cdots,v_N(k)):\Omega\to \bigoplus_{i=1}^N\Y_i$ and $ H(k) = (H_1(k),\cdots,H_N(k)):\Omega\to \mathscr L(\X,\bigoplus_{i=1}^N\Y_i)$.
We can write (\ref{measuramentmodel}) in the following compact form
\begin{equation}\label{compactform}
y(k)=H(k)f_0+v(k),~k\ge 0.
\end{equation}
There comes a natural problem (which is called the \emph{online random inverse problem}): how to reconstruct $f_0$ through real-time observations generated by randomly time-varying forward operators and noises?

\begin{remark}
Almost all the existing literature on the inverse problem restrict the forward operator to a deterministic and time-invariant linear compact operator $H$, solving the following equation
\begin{equation}\label{nklowokkrkr}
y=Hf_0+v.
\end{equation}
Here, different from the existing literature on the inverse problem, the forward operator in the measurement equation (\ref{compactform}) is random and time-varying. The classical inverse problem (\cite{Benning}) models the noise $v$ in the measurement equation (\ref{nklowokkrkr}) as a deterministic perturbation, and subsequently Mathe and Pereverzev \cite{Math1}, Lu and Mathe \cite{Lu} and Lu \emph{et al.} \cite{Lu2} modeled $v$ as the Gaussian white noise in the statistical inverse problem.
Based on the stochastic gradient descent (SGD) algorithm, Lu and Mathe \cite{Lu100}, Jahn and Jin \cite{Jahn} and Jin and Lu \cite{JinB} obtained the centralized learning strategy by the random discrete sampling of the forward operator and minimizing the loss functional $\widetilde{J}:\X\to \mathbb R$, where
\bna\label{sunshi}
\widetilde{J}(x)=\frac{1}{2}\E\left[\|y-Hx\|^2\right],~\forall x\in \X.
\ena
Specifically, Jahn and Jin \cite{Jahn} and Jin and Lu \cite{JinB} investigated the regularization property of SGD with a priori and a posteriori stopping rules, and Lu and Mathe \cite{Lu100} gave an upper bound on the estimation error of SGD with discretization level. Recently, Iglesias \emph{et al.} \cite{Iglesias1} and Lu \emph{et al.}  \cite{Lu1} solved the statistical inverse problem based on real-time observations $\{y(k):y(k)=Hf_0+v(k),k\ge 0\}$, where $\{v(k),k\ge 0\}$ is a Gaussian white noise sequence with independent identical distribution, and since this model can be viewed as the measurement equation (\ref{measuramentmodel}) with a single node, the statistical inverse problem in \cite{Iglesias1, Lu1} is a special case of the online random inverse problem.
\end{remark}

On the one hand, not only the compact form (\ref{compactform}), which is formed by aggregating the measurement equations of all nodes, admits the existing inverse problem model, but also the forward operator is randomly time-varying. On the other hand, different from existing centralized inverse problem, the reconstruction of $f_0$ by the measurement equations is also constrained by the information structure of the graph, i.e. at each moment $k$, node $i$ can only use its own observation $y_i(k-1)$ and the estimations $f_j(k-1), j\in \mathcal N_i$ of its neighbors, to reconstruct $f_0$ by constructing an estimate $f_i(k):\Omega\to \X$, where
$$f_i(k)\in \sigma(y_i(k-1);\tau_{\text{N}}(\Y_i))\bigvee\left(\bigvee_{j\in \mathcal N_i}\sigma\left(f_j(k-1);\tau_{\text{N}}(\X)\right)\right),~i\in \mathcal V,~k\ge 0.$$
The problem of reconstructing $f_0$ by the nodes over the graph through real-time observations and estimations by neighboring nodes is referred to as the \textbf{\emph{online random inverse problem over the graph}} in this paper.

\subsection{Decentralized online learning algorithm}\label{online}
Denote $\mathcal H(k)=\text{diag}\{H_1(k),\cdots,H_N(k)\}$. Suppose $\H(k)\in L^2(\Omega;\mathscr L(\X^N,\bigoplus_{i=1}^N\Y_i))$ and $v(k)\in L^2(\Omega;\bigoplus_{i=1}^N\Y_i)$.
If the sequences $\{\H(k),k\ge 0\}$ and $\{v(k), k\ge 0\}$ are independently identically distributed, it follows from Definition \ref{fenbudingyi} and Proposition \ref{nlllwwieiie}.(b) that $\{y(k)-\H(k)f,k\ge 0\}$ is a sequence of independently identically distributed random elements with values in the Hilbert space $(\bigoplus_{i=1}^N\Y_i,\tau_{\text{N}}(\bigoplus_{i=1}^N\Y_i))$, where $f\in \X^N$. Based on the loss functional (\ref{sunshi}), we consider minimizing the loss functional $J:\X^N \to \mathbb R$ with Laplacian regularization term, which gives the decentralized estimate of $\1_{N}\otimes f_0$ for the random inverse problem over the graph, where
\ban
J(f)=\frac{1}{2}\left(\E\left[\|y(k)-\mathcal H(k)f\|_{\bigoplus_{i=1}^N\Y_i}^2\right]+\left\langle \left(\L_{\G}\otimes I_{\X}\right )f,f\right\rangle _{\X^N}\right),~\forall f\in \X^N,
\ean
here, $I_{\X}$ is the identical operator on the Hilbert space $\X$. The loss functional  $J(f)$ consists of two components: the mean-square estimation error $\E[\|y(k)-\mathcal H(k)f\|_{\bigoplus_{i=1}^N\Y_i}^2]$ and the consensus error $\langle (\L_{\G}\otimes I_{\X})f,f\rangle _{\X^N}=\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Na_{ij}\|f_i-f_j\|^2_{\X}$, where $f_i\in \X$ is the $i$-th component of $f$. Noting that $\H^*(k)\H(k)\in L^1(\Omega;\LL(\X^N))$, the gradient operator $\text{grad}~J:\X^N\to \X^N$ is obtained by Proposition \ref{vnwlssfweewwfew}, where
\ban
\text{grad}~J(f)&=&\frac{1}{2}\text{grad}~\left(\E\left[\|y(k)-\mathcal H(k)f\|_{\bigoplus_{i=1}^N\Y_i}^2\right]+\left\langle \left(\L_{\G}\otimes I_{\X}\right)f,f\right\rangle _{\X^N}\right)\\
&=&\frac{1}{2}\text{grad}~\E\left[\langle \H^*(k)\H(k)f,f\rangle _{\X^N}\right]-\text{grad}~\E\left[\langle \H(k)f,y(k) \rangle _{\bigoplus_{i=1}^N\Y_i}\right]\\ &&+\frac{1}{2}
\text{grad}~\left\langle \left(\L_{\G}\otimes I_{\X}\right)f,f\right\rangle _{\X^N}\\
&=&\frac{1}{2}\text{grad}~\langle \E\left[\H^*(k)\H(k)\right]f,f\rangle _{\X^N}-\text{grad}~\E\left[\langle \H(k)f,y(k) \rangle _{\bigoplus_{i=1}^N\Y_i}\right]\\
&&+\frac{1}{2}
\text{grad}~\left\langle \left(\L_{\G}\otimes I_{\X}\right)f,f\right\rangle _{\X^N},~\forall f\in \X^N.
\ean
Similarly, we have
\ban
\langle \E[\H^*(k)\H(k)]x,y\rangle _{\X^N}&=&\E[\langle \H^*(k)\H(k)x,y\rangle _{\X^N}]\\ &=&\E[\langle x,\H^*(k)\H(k)y\rangle _{\X^N}]\\
&=&\langle x, \E[\H^*(k)\H(k)]y\rangle _{\X^N},~\forall x,y\in \X^N.
\ean
Therefore, $\E[\H^*(k)\H(k)]:\X^N\to \X^N$ is a self-adjoint operator. Since $\G$ is an undirected graph, it follows that $\L_{\G}\otimes I_{\X}$ is a self-adjoint operator. Then
\ban
\text{grad}~\langle \E[\H^*(k)\H(k)]f,f\rangle _{\X^N}=2\E[\H^*(k)\H(k)]f,~\forall f\in \X^N,
\ean
and
\ban
\text{grad}~\langle (\L_{\G}\otimes I_{\X})f,f\rangle _{\X^N}=2(\L_{\G}\otimes I_{\X})f,~\forall f\in \X^N.
\ean
Noting that $\H^*(k)y(k)\in L^1(\Omega;\X^N)$, it follows that
\ban
&&~~~\lim_{t\to 0}\frac{1}{t}\left(\E\left[\langle \H(k)(f+tg),y(k) \rangle _{\bigoplus_{i=1}^N\Y_i}\right]-\E\left[\langle \H(k)f,y(k) \rangle _{\bigoplus_{i=1}^N\Y_i}\right]\right)\cr
&&=\E[\langle \H^*(k)y(k),g\rangle _{\X^N}]\cr &&=\langle \E[\H^*(k)y(k)],g\rangle _{\X^N},~\forall g\in \X^N.
\ean
It follows that $\text{grad}~\E[\langle \H(k)f,y(k) \rangle _{\bigoplus_{i=1}^N\Y_i}]=\E[\H^*(k)y(k)]$. Thus, we have
\ban
\text{grad}~J(f)=-\E[\H^*(k)(y(k)-\H(k)f)]+(\L_{\G}\otimes I_{\X})f,~\forall f \in \X^N.
\ean
We consider the stochastic gradient descent (SGD) algorithm in Hilbert space:
\bna\label{algorithm1}
f(k+1)=f(k)+a(k)\H^*(k)(y(k)-\H(k)f(k))-b(k)(\L_{\G}\otimes I_{\X})f(k),~k\ge 0.
\ena
Let $f(k)=(f_1(k),\cdots,f_N(k))$. From (\ref{algorithm1}), we obtain the decentralized online learning algorithm
\bna\label{algorithm}
f_i(k+1)&=&f_i(k)+a(k)H_i^*(k)(y_i(k)-H_i(k)f_i(k)) \cr
&&+b(k)\sum_{j\in \N_i}a_{ij}(f_j(k)-f_i(k)),~k\ge 0,~i\in \mathcal V.
\ena


\begin{remark}
In contrast to the centralized algorithm with fusion centers, the algorithm (\ref{algorithm}) gives a decentralized online learning strategy for all nodes, i.e. there is no fusion center collecting observations and estimates of all nodes, but only needs to reconstruct the unknown element $f_0$ in the Hilbert space $\X$ through its own observations and the estimations of neighboring nodes. Intuitively, node $i$ obtains the innovation term $H_i^*(k)(y_i(k)-H_i(k)f_i(k))$ and the consensus term $\sum_{j\in \N_i}a_{ij}(f_j(k)- f_i(k))$, on which the estimate $f_i(k+1)$ is updated for the next moment, where the gain $a(k)$ denotes the data update strength, which is used to adjust the node's estimate of the unknown element $f_0$ at the next moment, and the gain $b(k)$ denotes the consensus strength, which drives the consistency of the estimates among the nodes over the graph.
\end{remark}


Although the above algorithm is obtained by assuming that $\{\H(k)\in L^2(\Omega;\mathscr L(\X^N,\bigoplus_{i=1}^N\\\Y_i)),k\ge 0\}$ and $\{v(k)\in L^2(\Omega;\bigoplus_{i=1}^N\Y_i), k\ge 0\}$ are both independently identically distributed.
In fact, the analysis in the next section will show that even for the sequence of operator-valued random elements $\{\H(k),k\ge 0\}$ with values in $(\mathscr L(\X^N,\bigoplus_{i=1}^N\Y_i),\tau_{\text{S}}(\mathscr L(\\ \X^N,\bigoplus_{i=1} ^N\Y_i)))$ and the noise sequence $\{v(k),k\ge 0\}$, which do not satisfy special statistical properties such as independence and stationarity, the algorithm (\ref{algorithm}) can still be proved to converge  under certain conditions.


\section{Main results}
Denote the global estimation error by $e(k)=f(k)-\1_N\otimes f_0$. Note that $(\L_{\G}\otimes I_{\X})(\1_N\otimes f_0)=0$ and $\H(k)(\1_N\otimes f_0)=H(k)f_0$. Subtracting $\1_N\otimes f_0$ on both sides of equation (\ref{algorithm1}) yields
\bna\label{error}
e(k+1)&=&(I_{\X^N}-a(k)\H^*(k)\H(k)-b(k)\L_{\G}\otimes I_{\X})f(k)+a(k)\H^*(k)y(k)-\1_N\otimes f_0 \nonumber\\
&=&(I_{\X^N}-a(k)\H^*(k)\H(k)-b(k)\L_{\G}\otimes I_{\X})(f(k)-\1_N\otimes f_0+\1_N\otimes f_0) \nonumber\\
&&+a(k)\H^*(k)y(k)-\1_N\otimes f_0 \nonumber\\
%&&=(I_{\X^N}-a(k)\H^*(k)\H(k)-b(k)\L_{\G}\otimes I_{\X})(e(k)+\1_N\otimes f_0)+a(k)\H^*(k)y(k)-\1_N\otimes f_0\cr
&=&(I_{\X^N}-a(k)\H^*(k)\H(k)-b(k)\L_{\G}\otimes I_{\X})e(k)-(a(k)\H^*(k)\H(k) \nonumber\\
&&+b(k)\L_{\G}\otimes I_{\X})(\1_N\otimes f_0)+a(k)\H^*(k)y(k) \nonumber\\
&=&(I_{\X^N}-a(k)\H^*(k)\H(k)-b(k)\L_{\G}\otimes I_{\X})e(k) \nonumber\\
&&+a(k)\H^*(k)(y(k)-\H(k)(\1_N\otimes f_0)) \nonumber\\
&=&(I_{\X^N}-a(k)\H^*(k)\H(k)-b(k)\L_{\G}\otimes I_{\X})e(k)+a(k)\H^*(k)v(k).
\ena
The estimation error equation (\ref{error}) belongs to the following family of randomly time-varying difference equations:
\bna \label{chafen}
x(k+1)=(I_{\X_1}-F(k))x(k)+G(k)u(k),~k\ge 0,
\ena
where $u(k)$ is a random element with values in the Hilbert space $(\X_2,\tau_{\text{N}}(\X_2))$, $F(k):\Omega\to\mathscr L(\X_1)$ and $G(k):\Omega\to\mathscr L(\X_2,\X_1)$ are random elements with values in the topological spaces $(\LL(\X_1),\tau_{\text{S}}(\LL(\X_1)))$ and $(\LL(\X_2,\X_1),\tau_{\text{S}}(\LL(\X_2,\X_1)))$. To analyze the convergence of the estimation error equation (\ref{error}), we will first analyze asymptotic stability of the randomly time-varying difference equation (\ref{chafen}).

\subsection{Asymptotic stability of random difference equations in Hilbert space}
To rigorously study the asymptotic stability of the random difference equations in the Hilbert space $(\X,\tau_{\text{N}}(\X))$, we introduce the following definitions.


\begin{definition}
If the sequence of random elements $\{x(k),k\ge 0\}$ with values in the Hilbert space $(\X,\tau_{\text{N}}(\X))$ satisfies $$\sup_{k\ge 0}\E\left[\|x(k)\|_{\X}^p\right]<\infty,$$ where $p>0$, then $\{x(k),k\ge 0\}$ is said to be $L_p$-bounded.
\end{definition}

\begin{definition}
If the sequence of random elements $\{x(k),k\ge 0\}$ with values in the Hilbert space $(\X,\tau_{\text{N}}(\X))$ satisfies $$\lim_{k\to \infty}\E\left[\|x(k)\|_{\X}^p\right]=0,$$ where $p>0$, then $\{x(k),k\ge 0\}$ is said to be $L_p$-asymptotically stable.
\end{definition}

\begin{definition} \label{dingyi}
Let $\{A(k),k\in \mathbb Z\}$ be a sequence of random elements with values in $(\LL(\X)$, $\tau_{\text{S}}(\LL(\X)))$, and $\{\mathcal F(k),k\in \mathbb Z\}$ be a filter in $(\Omega,\F,\P)$. If for any given $L_q$-bounded adaptive sequence $\{x(k),\F(k),k\in \mathbb Z\}$ with values in the Hilbert space $\X$,
\ban
\lim_{m\to \infty}\E\left[\left\|\prod_{k=n+1}^mA(k)x(n)\right\|_{\X}^p\right]=0,~\forall n \in \mathbb Z,~\text{where}~p,q>0,
\ean
then the sequence of operator-valued random elements $\{A(k),k\in \mathbb Z\}$ is said to be $L_p^q$-stable with respect to the filter $\{\mathcal F(k),k\in \mathbb Z\}$.
\end{definition}

The strong convergence of the sequence of products of deterministic non-expansive operators has attracted the interests of many scholars. By assuming strong convergence of operator products, the convergence results on infinite-dimensional deterministic time-varying difference equations in general metric space were obtained. Reich and Zaslavski \cite{Reich3} studied deterministic time-varying compressive operators in general metric space and obtained strong convergence results on the sequence of operator products; Pustylnik \emph{et al.}  \cite{Pustylnik2, Pustylnik3, Pustylnik4} investigated the strong convergence of the sequence of operator products consisting of finite number of projection operators.
Note that the sequence of deterministic operator products converging strongly to $0$ can be regarded as the $L_p^q$-stable sequence of operators with respect to the trivial filter $\{\F(k)=\{\emptyset,\Omega\},k\in \mathbb Z\}$, the concept of strong convergence used in the above literature for the sequence of operator products is a special case of Definition \ref{dingyi}. Particularly, Guo \cite{Guo1994} proposed the concept of $L_p$-exponentially stable $(p>0)$ random matrix sequence $\{I-B(k)\in \mathbb R^{N\times N},k\in \mathbb Z\}$ in finite-dimensional space, i.e. there exist constants $M>0$ and $\lambda \in (0,1)$ such that
\[\E\left[\left\|\prod_{k=n}^m(I-B(k))\right\|_{\LL(\mathbb R^{N})}^p\right]\leq M\lambda^{m-n},~\forall m,n\in \mathbb Z.\]
Here, it can be seen that if $\{I-B(k),k\in \mathbb Z\}$ is $L_p$-exponentially stable, then it is $L^{s}_{r}$-stable with respect to arbitrary filter $\{\mathcal F(k),k\in \mathbb Z\}$, where $r=pa^{-1}$, $s=pba^{-1}$ and $a,b$ are arbitrarily positive real numbers satisfying $a^{ -1}+b^{-1}=1$.

Consider the filter $\{\F(k),k\ge 0\}$ in $(\Omega,\F,\P)$, where
\[\F(k)=\bigvee_{i=0}^k\left(\sigma(F(i);\tau_{\text{S}}(\LL(\X_1)))\bigvee \sigma(G(i);\tau_{\text{S}}(\LL(\X_2,\X_1)))\bigvee \sigma(u(i);\tau_{\text{N}}(\X_2))\right)\]
and $k\ge 0$. Denote $\F(-1)=\{\emptyset,\Omega\}$. About the $L_2$-asymptotic stability of the solution sequence of the random difference equation (\ref{chafen}), we have the following result.

\begin{theorem}\label{wendingxing}
For the random difference equation (\ref{chafen}), let $\{u(k),\F(k),k\ge 0\}$ be a $L_2$-bounded sequence of martingale differences, and the sequence $\{u(k),k\ge 0\}$ be independent of the sequences $\{F(k),k\ge 0\}$ and $\{G(k),k\ge 0\}$. If (i) the sequence of operator-valued random elements $\{I_{\X_1}-F(k),k\ge 0\}$ is $L_2^2$-stable with respect to the filter $\{\F(k),k\ge 0\}$, (ii) there exists a sequence of nonnegative real numbers $\{\gamma(k),k\ge 0\}$ such that
\bna\label{qqqqq}
\left.\E\left[\|I_{\X_1}-F(k)\|_{\LL(\X_1)}^4\right|\mathcal F(k-1)\right]\leq 1+\gamma(k)~\text{a.s.},~\sum_{k=0}^{\infty}\gamma(k) <\infty,
\ena
and (iii) $G(k)$ satisfies
\bna\label{ssafe}
\sum_{k=0}^{\infty}\E\left[\|G(k)\|_{\LL(\X_2,\X_1)}^2\right]<\infty,~\sup_{k\ge 0}\E\left[\|G(k)\|_{\LL(\X_2,\X_1)}^4\right]<\infty,
\ena
then, the sequence of solutions $\{x(k),k\ge 0\}$ of the random difference equation (\ref{chafen}) is $L_2$-asymptotically stable.
\end{theorem}

Systematic works on the stability of randomly time-varying difference equations in finite-dimensional spaces were done in \cite{Guo1994, Guo1990, GUO111, GUO222, GUO333, GUO444}, where the stability theory of adaptive filtering algorithms was established. The existing  research on randomly time-varying difference equations in infinite-dimensional spaces remains fragmented. Kubrusly \cite{Kubrusly}, Vajjha \emph{et al.} \cite{Vajjha} and the references therein transformed the analysis of the mean square convergence of stochastic approximation algorithms in Hilbert spaces into the $L_2$-asymptotic stability analysis of randomly time-varying difference equation. Ungureanu \emph{et al.} \cite{Ungureanu1}, Ungureanu \cite{Ungureanu2, Ungureanu3} and Zhang \emph{et al.} \cite{zwzwxcbs} investigated the $L_2$-asymptotic stability of the solution sequence of the randomly time-varying difference equation $x(k+1)=A(k)x(k)+b(k)$ in Hilbert space. The above literature assumed $A(k):\Omega\to \LL(\X)$ to be mean square exponentially stable, i.e. there exist constants $M>0$ and $\lambda\in (0,1)$ such that
\bna\label{zhishuwending}
\E\left[\left\|\left(\prod_{k=n+1}^mA(k)\right)x\right\|_{\X}^2\right]\leq M\lambda^{n-m}\|x\|^2,~\forall m,n\ge 0,~\forall x\in \X.
\ena
For the randomly time-varying difference equation (\ref{chafen}), even if $F(k)\equiv F$, where $F$ is a deterministic self-adjoint compact operator with $\|F\|_{\LL(\X)}\leq 1$, the operator $A(k)\equiv I_{\X}-F$ does not satisfy the above mean square exponential stability condition. In fact, if (\ref{zhishuwending}) holds, then
\bna\label{zhihuhhh}
\left\|\prod_{k=n+1}^{n+2^l}A(k)\right\|_{\LL(\X)}\leq \sqrt{M}\lambda^{2^{l-1}},~\forall l,n\ge 0.
\ena
Noting that $\|I_{\X}-F\|_{\LL(\X)}=\sup_{\|x\|_{\X}=1}|\langle (I_{\X}-F)x,x\rangle_{\X} |=1-\inf_{\|x\|_{\X}=1}\langle Fx,x\rangle_{\X} =1$, thus we have
\[\left\|\prod_{k=n+1}^{n+2^l}A(k)\right\|_{\LL(\X)}^2=\left\|(I_{\X}-F)^{2^l}\right\|_{\LL(\X)}=\left\|I_{\X}-F\right\|_{\LL(\X)} ^{2^l}=1,~\forall l,n\ge 0,\]
which is in contradiction to (\ref{zhihuhhh}). Thereby, the existing results and methods on the stability of infinite-dimensional random difference equations are not applicable to the random inverse problem.

\subsection{Convergence of the decentralized algorithm}
In this section, the filter $\{\F(k),k\ge 0\}$ in $(\Omega,\F,\P)$ is given by
$$\F(k)=\bigvee_{i=0}^k\left(\sigma\left(\H(i);\tau_{\text{S}}\left(\LL\left(\X^N,\bigoplus_{j=1}^N\Y_j\right)\right)\right)\bigvee \sigma\left(v(i);\tau_{\text{N}}\left(\bigoplus_{j=1}^N\Y_j\right)\right)\right),~k\ge 0,$$
and $\F(-1)=\{\emptyset,\Omega\}$, where $\H(i)$ and $v(i)$ are given in Section \ref{online}. For the random inverse problem, we have the following assumptions, which will be used in later analysis.


\begin{assumption}\label{assumption1}
  The noises $\{v(k),k\ge 0\}$ with values in $(\bigoplus_{i=1}^N\Y_i,\tau_{\text{N}}(\bigoplus_{i=1}^N\Y_i))$ and the random forward operator sequence $\{H(k),k\ge 0\}$ with values in $(\mathscr L( \X,\bigoplus_{i=1}^N$ $\Y_i),\tau_{\text{S}}(\mathscr L(\X,\bigoplus_{i=1}^N\Y_i)))$ are mutually independent.
\end{assumption}
%\textbf{A1.} The noise $\{v(k),k\ge 0\}$ with values in Hilbert space $(\bigoplus_{i=1}^N\Y_i,\tau_{\text{N}}(\bigoplus_{i=1}^N\Y_i))$ and the random forward operator sequence $\{H(k),k\ge 0\}$ with values in topological space $(\mathscr L( \X,\bigoplus_{i=1}^N$ $\Y_i),\tau_{\text{S}}(\mathscr L(\X,\bigoplus_{i=1}^N\Y_i)))$ are mutually independent.
\begin{assumption}\label{assumption2}
The noises $\{v(k),\F(k),k\ge 0\}$ is a martingale sequence and there exists a constant $\b_v>0$ such that $$\sup_{k\ge 0}\E\left.\left[\left\|v(k)\right\|_{\bigoplus_{i=1}^N\Y_i}^2\right|\F(k-1)\right]\leq \b_v~\text{a.s.}$$
\end{assumption}
%\textbf{A2.} The noise $\{v(k),\F(k),k\ge 0\}$ is a martingale sequence and there exists a constant $\b_v>0$ such that $$\sup_{k\ge 0}\E\left.\left[\left\|v(k)\right\|_{\bigoplus_{i=1}^N\Y_i}^2\right|\F(k-1)\right]\leq \b_v~\text{a.s.}$$


For the gains of the algorithm (\ref{algorithm}), we may need the following conditions.


\begin{condition}\label{condition1}
The algorithm gains $\{a(k),k\ge 0\}$ and $\{b(k),k\ge 0\}$ are both monotonically decreasing sequences of positive real numbers.
\end{condition}
%\textbf{C1.} The algorithm gains $\{a(k),k\ge 0\}$ and $\{b(k),k\ge 0\}$ are both monotonically decreasing sequences of positive real numbers.


\begin{condition}\label{condition2}
$\sum_{k=0}^{\infty}a^2(k)<\infty$ and $\sum_{k=0}^{\infty}b^2(k)<\infty$.
\end{condition}
%\textbf{C2.} $\sum_{k=0}^{\infty}a^2(k)<\infty$ and $\sum_{k=0}^{\infty}b^2(k)<\infty$.


\begin{condition}\label{condition3}
$\sum_{k=0}^{\infty}a(k)=\infty$ and $\max\{a(k)-a(k+1),b(k)-a(k)\}=\mathcal O(a^2(k)+b^2(k))$.
\end{condition}
%\textbf{C3.} $\sum_{k=0}^{\infty}a(k)=\infty$ and $\max\{a(k)-a(k+1),b(k)-a(k)\}=\mathcal O(a^2(k)+b^2(k))$.


We are now in position to analyze the convergence of the algorithm (\ref{algorithm}) by studying the $L_2$-asymptotic stability of the solution sequence of the randomly time-varying difference equation (\ref{error}) in the Hilbert space $(\X^N,\tau_{\text{N}}(\X^N))$. Firstly, based on Theorem \ref{wendingxing}, we have the following key lemma.

\begin{lemma}\label{dingliyi1}
For the algorithm (\ref{algorithm}), suppose that Assumptions \ref{assumption1}, \ref{assumption2} and Condition \ref{condition2} hold, there exists a sequence of nonnegative real numbers $\{\gamma(k),k\ge 0\}$, where $\sum_{k=0}^{\infty}\gamma(k)< \infty$, such that
\bna\label{qafgs}
&&\left.\E\left[\|I_{\X^N}-\left(a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\X}\right)\|^4_{\LL\left(\X^N\right)}\right|\F(k-1)\right]\cr
&&\leq 1+\gamma(k)~\text{a.s.},
\ena
and the sequence of operator-valued random elements $\{I_{\X^N}-(a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\X}),k\ge 0\}$ is $L_2^2$-stable with respect to the filter $\{\F(k),k\ge 0\}$.\\
I. If
$$\sup_{k\ge 0}\E\left[\|\H(k)\|_{\mathscr L\left(\X^N,\bigoplus_{i=1}^N\Y_i\right)}^2\right]<\infty,$$
then the algorithm (\ref{algorithm}) is mean square consistent, i.e. $\lim_{k\to\infty}\E[\|f_i(k)-f_0\|_{\X}^2]=0,~i\in \mathcal V$.\\
II. If there exists a constant $\rho_0>0$ such that
$$\E\left.\left[\|\H(k)\|_{\mathscr L\left(\X^N,\bigoplus_{i=1}^N\Y_i\right)}^2\right|\F(k-1)\right]\leq \rho_0~\text{a.s.},$$
then the algorithm (\ref{algorithm}) is almost surely strongly consistent, i.e. $\lim_{k\to\infty}\|f(k)-f_0\|_{\X}=0~\text{a.s.},~i\in \mathcal V$.
\end{lemma}

\begin{remark}\label{nvllwwnnvvv}
The boundedness condition (\ref{qafgs}) in Lemma \ref{dingliyi1} guarantees the existence of the expectation and conditional expectation of the estimate $f(k)$ in the algorithm (\ref{algorithm}), i.e. $f(k)$ is Bochner integrable. It is not difficult to verify that (\ref{qafgs}) holds if the norm of the random forward operator has a uniform upper bound independent of the sample path. The existence of the Bochner integral in infinite-dimensional Banach space needs to be guaranteed by integrability, in fact, the condition (\ref{qafgs}) can be relaxed in the online learning algorithm in finite-dimensional Hilbert space. Wang \emph{et al.} \cite{WLZ} considered the distributed parameter estimation algorithm, which is equivalent to the case where the algorithm (\ref{algorithm}) is considered in Euclidean  space, and since the expectation of the random vector is defined by the Lebesgue integral, Wang \emph{et al.} \cite{WLZ} proposed a weaker boundedness condition than (\ref{qafgs}).
\end{remark}





From the Lemma \ref{dingliyi1}, we can see that the $L_2^2$-stability of the  homogeneous part of the error equation (\ref{error}) plays a key role in the convergence of the algorithm (\ref{algorithm}). Inspired by the above, we will next give a more intuitive convergence condition of the algorithm (\ref{algorithm}) by further studying the $L_2^2$-stability of the sequence $\{I_{\X^N}-a(k)\H^*(k)\H(k)-b(k)\L_{\G}\otimes I_{\X},k\ge 0\}$ of operator-valued random elements. First, we have the following fundamental lemmas.

\begin{lemma}\label{yibanxingdejieguo}
For the algorithm (\ref{algorithm}), suppose that Assumptions \ref{assumption1}, \ref{assumption2}, Conditions \ref{condition1} and \ref{condition2} hold. If there exists an integer $h>0$ and a constant $\rho_0>0$ such that
\begin{itemize}
\item the operator-valued random sequence $$ \left\{I_{\X^N}-\sum_{i=kh}^{(k+1)h-1}(a(i)\H^*(i)\H(i)+b(i)\L_{\G}\otimes I_{\X}),k\ge 0\right\}$$ is $L_2^2$-stable with respect to the filter $\{\F((k+1)h-1), k\ge 0\}$;
\item $\displaystyle \sup_{k\ge 0}\left(\E\left.\left[\|\H^*(k)\H(k)\|_{\LL\left(\X^N\right)}^{2^{\max\{h,2\}}}\right|\F(k-1)\right]\right)^{\frac{1}{2^{\max\{h,2\}}}}\leq \rho_0~\text{a.s.}$
\end{itemize}
then $\{I_{\X^N}-a(k)\H^*(k)\H(k)-b(k)\L_{\G}\otimes I_{\X},k\ge 0\}$ is $L_2^2$-stable with respect to $\{\F(k),k\ge 0\}$,
\end{lemma}

If the graph $\G$ is connected, then by Lemma \ref{yibanxingdejieguo}, we can further obtain a more intuitive sufficient condition, under which the operator-valued random sequence $\{I_{\X^N}-a(k)\H^*(k)\H(k)-b(k)\L_{\G}\otimes I_{\X},k\ge 0\}$ is $L_2^2$-stable with respect to the filter $\{\F (k),k\ge 0\}$.

\begin{lemma}\label{jihubiranshoulian}
For the algorithm (\ref{algorithm}), let $\G$ be connected and assume that Assumptions \ref{assumption1}, \ref{assumption2} and Conditions \ref{condition1}-\ref{condition3} hold. If there exist positive self-adjoint operators $\HH_i\in \mathscr L(\mathscr X)$, $i=1,\cdots,N$ satisfying $\sum_{i=1}^N\HH_i>0$, and an integer $h>0$, such that
\bna\label{yinlitiaojian1}
\sum_{j=1}^N\sum_{k=0}^{\infty}\E\left[\left\|\HH_jx(k)-\sum_{i=kh}^{(k+1)h-1}\E\left.\left[H_j^*(i)H_j(i)x(k)\right|\F(kh-1)\right] \right\|_{\X}^2\right]<\infty,
\ena
for any $L_2$-bounded adaptive sequence $\{x(k), \F(kh-1),k\ge 0\}$ with values in Hilbert space $\X$, and there exists a constant $\rho_0>0$ such that
\bna\label{yinlitiaojian2}
\sup_{k\ge 0}\left(\E\left.\left[\|\H^*(k)\H(k)\|_{\LL\left(\X^N\right)}^{2^{\max\{h,2\}}}\right|\F(k-1)\right]\right)^{\frac{1}{2^ {\max\{h,2\}}}}\leq \rho_0~\text{a.s.},
\ena
then $\{I_{\X^N}-a(k)\H^*(k)\H(k)-b(k)\L_{\G}\otimes I_{\X},k\ge 0\}$ is $L_2^2$-stable with respect to $\{\F(k),k\ge 0\}$.
\end{lemma}


Through the $L_2^2$-stability analysis of $\{I_{\X^N}-a(k)\H^*(k)\H(k)\H(k)-b(k)\L_{\G}\otimes I_{\X},k\ge 0\}$, from the basic convergence result of Lemma \ref{dingliyi1}, we will next give intuitive sufficient conditions on the mean square and almost sure strong consistency of the algorithm (\ref{algorithm}).

\begin{theorem}\label{vnknoklfl}
For the algorithm (\ref{algorithm}),
suppose that all the conditions in Lemma \ref{jihubiranshoulian} hold. If there exists a sequence of nonnegative real numbers $\{\Gamma(k),k\ge 0\}$ with $\sum_{k=0}^{\infty}\Gamma(k)<\infty$, such that
\bna\label{dinglitiaojian}
&&\left.\E\left[\|I_{\X^N}-4\left(a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\X}\right)\|_{\LL\left(\X^N\right)}\right|\F(k-1)\right]\cr
&&\leq 1+\Gamma(k)~\text{a.s.},
\ena
then the algorithm (\ref{algorithm}) is both mean square and almost surely strongly consistent.
\end{theorem}

Especially, if $\{\H(k),k\ge 0\}$ with values in $(\mathscr L(\X^N,\bigoplus_{i=1}^N\Y_i),\tau_{\text{S}}(\mathscr L(\X^N,\bigoplus_{i=1}^N\Y_i)))$ and the random sequence $\{v(k),k\ge 0\}$ with values in $(\bigoplus_{i=1}^N\Y_i,\tau_{\text{N}}(\bigoplus_{i=1}^N\Y_i))$ are both i.i.d. and they are mutually independent, then the following corollary follows from Theorem \ref{vnknoklfl}.



\begin{corollary}\label{xiaosirendetuilun}
For the algorithm (\ref{algorithm}), assume that $\G$ is connected, Assumptions \ref{assumption1}, \ref{assumption2} and Conditions \ref{condition1}-\ref{condition3} hold, and $\{\H(k),k\ge 0\}$ and $\{v(k),k\ge 0\}$ are both i.i.d. sequences and they are mutually independent. If there exists a constant $\rho_0>0$ such that $\|\H(0)\|\leq \rho_0~\text{a.s.}$ and
\bna\label{tuiluntiaojian} \sum_{j=1}^N\E\left[\|H_j(0)x\|_{\Y_j}^2\right]>0,~\forall x\in \X\setminus\{0\},
\ena
then the algorithm (\ref{algorithm}) is both mean square and almost surely strongly consistent.
\end{corollary}

The graph $\G$ describes the communication topology between nodes, and the connectivity of the graph can effectively reconstruct the unknown element $f_0$ collaboratively. The condition (\ref{yinlitiaojian1}) in Lemma \ref{jihubiranshoulian} plays an important role in the convergence analysis of the decentralized algorithm, which we call the \textbf{\emph{infinite-dimensional spatio-temporal persistence of excitation}} condition. It is well known that the operator orbit $x\mapsto H_j(i)x$ can fully reflect the nature of the operator $H_j(i)$ itself. Intuitively, there exist deterministic time-invariant operators $\HH_j$, $j=1,\cdots,N$ such that the aggregated information of $H_j^*(i)H_j(i)$ over fixed-length time periods $[kh,(k+1)h-1]$ and the information provided by $\HH_j$ trend to be consistent over time.
Note that
\ban
&&~~~\left|\sum_{j=1}^N\sum_{i=kh}^{(k+1)h-1}\E\left[\|H_j(i)x\|_{\Y_j}^2\right]-\left\langle \sum_{j=1}^N\HH_jx,x\right\rangle _{\X}\right|\cr
&&=\left|\sum_{j=1}^N\left\langle \E\left[\sum_{i=kh}^{(k+1)h-1}H^*_j(i)H_j(i)x-\HH_jx\right],x\right\rangle _{\X}\right|\cr &&=\left|\sum_{j=1}^N\left\langle \E\left[\sum_{i=kh}^{(k+1)h-1}\E\left.\left[H^*_j(i)H_j(i)x\right|\F(kh-1)\right]-\HH_jx\right],x\right\rangle _{\X}\right|\cr
&&\leq \sum_{j=1}^N\left[\E\left[\left\|\HH_jx-\sum_{i=kh}^{(k+1)h-1}\E\left.\left[H_j^*(i)H_j(i)x\right|\F(kh-1)\right]\right\|_{\X}^2\right]\right]^{\frac{1}{2}}\|x\|_{\X},~x\in \X,~k\ge 0.
\ean
Thus the \textbf{\emph{infinite-dimensional spatio-temporal persistence of excitation}} condition is equivalent to the following two conditions.
\begin{itemize}
\item For any $x\in \X\setminus\{0\}$, there exists an integer $K(x)>0$ such that
\bna\label{vnkwwoelekel}
\sum_{j=1}^N\sum_{i=kh}^{(k+1)h-1}\E\left[\left\|H_j(i)x\right\|_{\Y_j}^2\right]>0,~k\ge K(x);
\ena
\item There exist deterministic time-invariant operators $\HH_j$, $j=1,\cdots,N$ such that for any $L_2$-bounded adaptive sequence $\{x(k),\F(kh-1),k\ge 0\}$ with values in the Hilbert space $\X$, there are
\ban
\sum_{j=1}^N\sum_{k=0}^{\infty}\E\left[\left\|\HH_jx(k)-\sum_{i=kh}^{(k+1)h-1}\E\left.\left[H_j^*(i)H_j(i)x(k)\right|\F(kh-1)\right] \right\|_{\X}^2\right]<\infty.
\ean
\end{itemize}
To reconstruct the unknown element $f_0$ under valid measurement information, the random forward operators are required to have the \textbf{\emph{spatio-temporal persistence of excitation}} property: the non-zero orbits of the random forward operator of all nodes over the connected graph $\G$ are non-degenerate in the mean square sense for a fixed length time period,  where \textbf{\emph{spatio-temporal}} refers specifically to the temporal and spatial states of the operator orbit $x\mapsto H_j(i)x$ of the random forward operator, and by (\ref{vnkwwoelekel}) we know that we neither need the temporal orbit of the forward operator of each node over the graph to be non-degenerate, i.e. $$\sum_{i=kh}^{(k+1)h-1}\E \left[\|H_j(i)x\|_{\Y_j}^2\right]>0,~\forall j\in \mathcal V,$$ nor does the spatial orbit of the forward operator of all nodes need to be non-degenerate at each instant, i.e. $$\sum_{j=1}^N\E\left[\|H_j(i)x\|_{\Y_j}^2\right]>0,~\forall i\ge 0.$$
Notably, if the random forward operator sequences and noise sequences are both i.i.d. and they are mutually independent, the \textbf{\emph{infinite-dimensional spatio-temporal persistence of excitation}} condition degenerates to the case that the spatial orbits of the forward operators of all nodes are non-degenerate at the initial moment $k=0$, i.e. the condition (i) in the Corollary \ref{xiaosirendetuilun}.

In the past decades, to solve the problems of parameter estimation and signal tracking with non-stationary and non-independent observation matrices, many scholars have proposed excitation conditions based on the conditional expectation of the observation matrix. The stochastic persistence of excitation condition was first proposed by Guo \cite{Guo1990} in the analysis of the centralized Kalman filtering algorithm. Subsequently, Xie and Guo \cite{Xieguo} proposed the cooperative information condition based on the conditional expectations of the observation matrices for the distributed adaptive filtering algorithm over connected graphs. Wang \emph{et al.} \cite{WLZ} proposed the stochastic spatio-temporal persistence of excitation condition for the decentralized online estimation algorithm over randomly time-varying graphs, and Zhang \emph{et al.} \cite{ZLF} further proposed a more general excitation condition: the sample path spatio-temporal persistence of excitation condition in the analysis of the decentralized online regularized regression algorithm over randomly time-varying graphs. For undirected connected graphs, Zhang \emph{et al.} \cite{ZLF} proved that if the randomly time-varying regression matrix satisfies the uniformly conditionally spatio-temporally joint observability condition, i.e. there exists an integer $h>0$ and a constant $\theta>0$, respectively, such that
\ban
\inf_{k\ge 0}\lambda_{\text{min}}\left(\sum_{j=1}^N\sum_{i=kh}^{(k+1)h-1}\E\left.\left[H_j^T(i)H_j(i)\right|\F(kh-1)\right]\right)\ge \theta~\text{a.s.},
\ean
then the algorithm can achieve mean square and almost sure convergence. In this case, it is not difficult to verify that the random matrix sequence $\{I_{Nn}-a(k)\H^*(k)\H(k)-b(k)\L_{\G}\otimes I_{n},k\ge 0\}$ satisfies the $L_2^2$-stability condition with respect to the filter $\{\F(k),k\ge 0\}$, and thus the excitation conditions proposed in \cite{WLZ, ZLF} are all special cases of the $L_2^2$-stability condition in Lemma \ref{dingliyi1}. The persistence of excitation conditions proposed for finite-dimensional systems require that the conditional expectation of the information matrix consisting of the observation (regression) matrices is positive definite, i.e. the information matrix has strictly positive minimum eigenvalues; however, the inverse problem in infinite-dimensional Hilbert space is usually ill-posed, even for the strictly positive linear compact operator, the persistence of excitation conditions for finite-dimensional systems do not hold any more since the infimum of the eigenvalues of a compact operator is $0$.

\subsection{Decentralized online learning in reproducing kernel Hilbert space}
We will discuss in this section a special class of online random inverse problems: decentralized online learning problems in reproducing kernel Hilbert spaces (RKHS). Let $\X$ be a non-empty subset of $\mathbb R^n$, $K:\X\times \X\to \mathbb R$ be a Mercer kernel, and $(\mathscr H_K,\langle \cdot,\cdot \rangle _K)$ be a reproducing kernel Hilbert space with kernel $K$, which is consisted of functions with domain $\X$.
Suppose $f_0:\X\to \mathbb R$ is an unknown function in $\HH_K$. The nodes cooperatively estimate $f_0$ by information exchanging among them. The observation data $y_i(k)$ of the $i$-th node at moment $k$ is
\bna\label{xuexi}
y_i(k)=f_0(x_i(k))+v_i(k),~k\ge 0,~i\in \mathcal V,
\ena
where $x_i(k):\Omega\to \X$ is a random vector with values in the Hilbert space $(\X,\tau_{\text{N}}(\X))$ at instant $k$, called the random input data, and the observation noise $v_i(k):\Omega\to \mathbb R$ is a random vector with values in the Hilbert space $(\mathbb R,\tau_{\text{N}}(\mathbb R))$.

For any given $x\in \X$, the function $K_x:\X\to \HH_K$ induced by the Mercer kernel is given by $$K_x(y)=K(x,y),~\forall y\in\X.$$
Let
\ban
H_i(k)(f):=f(x_i(k)),~f\in \HH_K,~k\ge 0,~i\in \mathcal V,
\ean
then the measurement model (\ref{xuexi}) can be represented as the random inverse problem with the measurement equation (\ref{measuramentmodel}), where the random  forward operator is $H_i(k)$. Based on the algorithm (\ref{algorithm}), the decentralized online learning strategy in $\HH_K$ is given by
\bna\label{rkhs}
f_i(k+1)&=&f_i(k)+a(k)(y_i(k)-f_i(k)(x_i(k)))K_{x_i(k)}\cr
&&+b(k)\sum_{j\in \N_i}a_{ij}(f_j(k)-f_i(k)),\ k\ge 0, \ i\in \mathcal V.
\ena
Given $\phi,\psi\in \HH_K$, denote the rank 1 tensor product operator $\phi\otimes \psi:\HH_K\to \HH_K$ by
\ban
\left(\phi\otimes \psi\right)(f):=\langle f,\psi\rangle _{K}\phi,~f\in \HH_K.
\ean
In this section, the filter $\{\F(k),k\ge 0\}$ in $(\Omega,\F,\P)$ is given by
\[\F(k)=\bigvee_{s=0}^k\bigvee_{i=1}^N\left(\bigvee_{f\in \HH_K}\sigma\left(f(x_i(s));\tau_{\text{N}}(\mathbb R)\right)\bigvee \sigma \left(v_i(s);\tau_{\text{N}}(\mathbb R)\right)\right),~k\ge 0,\]
and $\F(-1)=\{\emptyset,\Omega\}$. For the algorithm (\ref{rkhs}), we need the following assumptions.



\begin{assumption}\label{assumption3}
The sequence $\{x_i(k),i\in \mathcal V,k\ge 0\}$ of random vectors with values in the Hilbert space $(\X,\tau_{\text{N}}(\X))$ and the sequence $\{v_i(k),i\in \mathcal V,k\ge 0\}$ of random variables with values in the Hilbert space $(\mathbb R,\tau_{\text{N}}(\mathbb R))$ are mutually independent.
\end{assumption}
%\textbf{B1.} The sequence $\{x_i(k),i\in \mathcal V,k\ge 0\}$ of random vectors with values in the Hilbert space $(\X,\tau_{\text{N}}(\X))$ and the sequence $\{v_i(k),i\in \mathcal V,k\ge 0\}$ of random variables with values in the Hilbert space $(\mathbb R,\tau_{\text{N}}(\mathbb R))$ are mutually independent.



\begin{assumption}\label{assumption4}
The noises $\{v_i(k),\F(k),k\ge 0\}$, $i\in \mathcal V$, are martingale sequences and there exists a constant $\b>0$, such that $$\max_{i\in\mathcal V}\sup_{k\ge 0}\E\left.\left[\|v_i(k)\|_{\mathbb R}^2\right|\F(k-1)\right]\leq \b~\text{a.s.}$$
\end{assumption}
%\textbf{B2.} The noises $\{v_i(k),\F(k),k\ge 0\}$, $i\in \mathcal V$, are martingale sequences and there exists a constant $\b>0$, such that $$\max_{i\in\mathcal V}\sup_{k\ge 0}\E\left.\left[\|v_i(k)\|_{\mathbb R}^2\right|\F(k-1)\right]\leq \b~\text{a.s.}$$



\begin{assumption}\label{assumption5}
$\sup_{x\in \X}K(x,x)<\infty$.
\end{assumption}
%\textbf{B3.} $\sup_{x\in \X}K(x,x)<\infty$.



\begin{remark}
\rm{The existing studies on RKHS online learning (e.g., \cite{SmaleYao, Tarres, Dieuleveut, Ying, Lei, ctfg}) all require the random input data to be independently and identically distributed. Notice that in Assumption  \ref{assumption3}, the sequence of random forward operators with values in $(\mathscr L(\X,\bigoplus_{i=1}^N\Y_i)$, $\tau_{\text{S}}(\mathscr L(\X,\bigoplus_{i=1}^N\Y_i)))$ is not required to satisfy special statistical properties such as independence, stationarity, etc.}
\end{remark}


\begin{remark}
\rm{Assumption \ref{assumption5} is often used in the current learning theory of RKHS (see \cite{SmaleYao, Tarres, Dieuleveut, ctfg}). There are many kernel functions satisfying Assumption \ref{assumption5}, such as the Gaussian kernel $K:\mathbb R^n\times \mathbb R^n\to \mathbb R$, $K(x,y)=\text{e}^{-\frac{\|x-y\|_{\mathbb R^n}^2}{c^2}}$ and the homogeneous polynomial kernel $K:\mathbb S^{n-1}\times \mathbb S^{n-1}\to \mathbb R$ on the unit sphere in $\mathbb R^n$, $K(x,y)=\langle x,y\rangle_{\mathbb R^n}^d$, where the integer $d>0$. In addition to these common kernel functions, note that the Mercer kernel is continuous, thus Assumption  \ref{assumption5} holds for arbitrary compact set $\X\subseteq \mathbb R^n$.}
\end{remark}

Noting the continuity of the Mercer kernel function, $K_x$ is therefore a continuous function. For any $f\in \HH_K$ and random input data $x_i(k)$, $i\in \mathcal V$, $k\ge 0$, it follows from the reproducing properties of RKHS that $f(x_i(k))=\langle f,K_{x_i(k) }\rangle _{K}$ (\cite{Poggio, Theodoridis}), thus $f(x_i(k)):\Omega\to \mathbb R$ is a random variable with values in $(\mathbb R,\tau_{\text{N}}(\mathbb R))$. It follows from Assumption \ref{assumption5} that $H_i(k)$ is the random element with values in $(\LL(\HH_K,\mathbb R),\tau_{\text{S}}(\LL(\HH_K,\mathbb R)))$, which is induced by the random input data $x_i(k)$.

Based on the convergence results of the algorithm (\ref{algorithm}), we can obtain the following convergence analysis of the decentralized online learning algorithm (\ref{rkhs}) in RKHS.


\begin{theorem}\label{rkhsdingli}
For the algorithm (\ref{rkhs}), let $\G$ be connected, and suppose that Assumptions \ref{assumption3}-\ref{assumption5} and Conditions \ref{condition1}-\ref{condition3} hold. If there exist positive self-adjoint operators $N_i\in \mathscr L(\HH_K)$, $i=1,\cdots,N$ satisfying $\sum_{i=1}^NN_i>0$ and there exists an integer $h>0$, such that
  \bna\label{vnknknldldklsd}
\sum_{j=1}^N\sum_{k=0}^{\infty}\E\left[\left\|\left(N_j-\sum_{i=kh}^{(k+1)h-1}\E\left.\left[K_{x_j(i)}\otimes K_{x_j(i)}\right|\F( kh-1)\right]\right)g(k)\right\|_K^2\right]<\infty,
  \ena
 for arbitrary $L_2$-bounded adaptive sequence $\{g(k),\F(kh-1),k \ge 0\}$, then the algorithm (\ref{rkhs}) is both mean square and almost surely strongly consistent.
\end{theorem}


\begin{corollary} \label{vnlllleleeemmem}
For the algorithm (\ref{rkhs}), let $\G$ be connected, and suppose that Assumptions \ref{assumption3}-\ref{assumption5} and Conditions \ref{condition1}-\ref{condition3} hold. If there exist positive self-adjoint operators $N_i\in \mathscr L(\HH_K)$, $i=1,\cdots,N$ satisfying $\sum_{i=1}^NN_i>0$, and there exists an integer $h>0$, a constant $\mu_0>0$ and a nonnegative real sequence $\{\tau(k),k\ge 0\}$, respectively, such that
\bna\label{vnkmeeeemefffff}
\max_{j\in \mathcal V}\left\|N_j-\sum_{i=kh}^{(k+1)h-1}\E\left.\left[K_{x_j(i)}\otimes K_{x_j(i)}\right|\F(kh-1)\right]\right\|_{\LL( \HH_K)}^2\leq \mu_0\tau(k)~\text{a.s.},
\ena
where $\sum_{k=0}^{\infty}\tau(k)<\infty$, then the algorithm (\ref{rkhs}) is both mean square and almost surely strongly consistent.
\end{corollary}



\begin{remark}
Noting that Assumption \ref{assumption5} implies that $K_{x_j(i)}\otimes K_{x_j(i)}$ is the Bochner integrable random element with values in the Banach space $(\mathscr L(\HH_K),\tau_{\text{N}}(\mathscr L(\HH_K)))$, then the conditional expectations $\E[K_{x_j(i)}\otimes K_{x_j(i)}|\F(kh-1)]$, $i,k\ge 0$, $j\in \mathcal V$ uniquely exist by Lemma \ref{nvkvpeoeo}.
\end{remark}



In particular, when the random input data $\{(x_1(k),\cdots,x_N(k)),k\ge 0\}$ are independent and identically distributed, we have the following corollary.



\begin{corollary}\label{rkhsdinglijjjjj}
For the algorithm (\ref{rkhs}), suppose that $\{(x_1(k),\cdots,x_N(k))$, $k\ge 0\}$ and $\{(v_1(k)$, $\cdots$, $v_N(k))$, $k\ge 0\}$ be i.i.d. sequences and they are mutually independent, and $\G$ be connected. If Assumptions \ref{assumption3}-\ref{assumption5} and Conditions \ref{condition1}-\ref{condition3} hold, and
  \bna\label{nklnkle}
\E\left[\sum_{j=1}^NK_{x_j(0)}\otimes K_{x_j(0)}\right]>0,
  \ena
then the algorithm (\ref{rkhs}) is both mean square and almost surely strongly consistent.
\end{corollary}



\begin{remark}
When the input data are drawn independently and identically distributed from the probability measure $\rho_{\X}$ on $\X$, Tarres and Yao \cite{Tarres} defined the covariance operator of the probability measure $\rho_{\X}$ in $\HH_K$ as $L_K:\HH_K\to \HH_K$,
\ban
L_K(f)(y)=\int_{\Omega}K(x,y)f(x)\dd\rho_{\X},~\forall f\in \HH_K.
\ean
For the centralized online learning problem in RKHS with independently identically distributed data, by the reproducing property and Assumption \ref{assumption5}, it follows that $L_K=\E[K_x\otimes K_x]$, the condition (\ref{nklnkle}) in Corollary \ref{rkhsdinglijjjjj} degenerates into the case in existing studies (\cite{Tarres, Dieuleveut, Ying}): the covariance operator $L_K>0$.
\end{remark}

\section{Conclusions}
We establish a framework of random inverse problems with real-time observations over graphs, and present a decentralized online learning algorithm based on online data streams, which unifies the distributed parameter estimation in Hilbert space and the least mean square problem in reproducing kernel Hilbert space (RKHS-LMS). Each node updates the information using the estimation and observation data at the current moment and obtains the node's estimate at the next moment by taking a weighted sum of estimations of its own and its neighbors through the consensus term. It is not required that the random forward operators satisfy special statistical assumptions such as mutual independence, spatio-temporal independence or stationarity. Firstly, based on the existing analytical results in Banach space, we further exploit the probabilistic properties among random elements with values in different topological spaces in a stochastic framework. Then, we propose the $L_p^q$-stability condition on the sequence of operator-valued random elements, and establish the theory of a class of  $L_2$-asymptotic stability of infinite-dimensional random difference equations with $L_2$-bounded martingale difference terms. Subsequently, we transform the solving process of the random inverse problem over graphs into the asymptotic stability of infinite-dimensional random difference equations which is in accordance with the $L_2^2$-stability condition on the operator-valued random element, and this enables to deal with the convergence of distributed parameter estimation algorithms in finite-dimensional space. An intuitive sufficient condition on the convergence of decentralized online learning algorithms for random inverse problems over graphs is obtained by giving a sufficient condition for a class of operator-valued random element sequences of random forward operators and Laplacian matrices of graphs to be $L_2$-stable, i.e. the \emph{infinite-dimensional spatio-temporal persistence of excitation} condition, proving that if the forward operators over connected graphs satisfy the \emph{infinite-dimensional spatio-temporal persistence of excitation} condition, then all nodes' estimates of the solution to the random inverse problem over graphs are mean square and almost surely strongly consistent. Finally, by equivalently transferring the distributed learning problem in RKHS to the random inverse problem over graphs, we propose a decentralized online learning algorithm in RKHS based on non-stationary and non-independent online data streams, and prove that the algorithm is mean square and almost surely strongly consistent if the operators induced by the random input data satisfy the \emph{infinite-dimensional spatio-temporal persistence of excitation} condition.


















\begin{appendix}
\section{Analytical Theories of Banach Spaces}\label{appendixa}


\begin{definition}[\cite{hy}]
Let $(S,\mathscr A)$ and $(T,\mathscr B)$ be measurable spaces, if the mapping $f:S\to T$ satisfies $$f^{-1}(B):=\{x\in S:f(x)\in B\}\in \mathscr A,~\forall B\in \mathscr B,$$ then $f$ is said to be $\mathscr A/\mathscr B$-measurable.
\end{definition}

\begin{definition}[\cite{hy}]
The function $f=\sum_{i=1}^n\boldsymbol{1}_{A_i}\otimes x_i$ is called a simple function with values in the Banach space $\X$, where $A_i\in \F$, $x_i\in \X$, $i=1,\cdots,n$, $\boldsymbol{1}_{A}$ is the indicator function of the set $A$ and $(\1_A\otimes x)(s):=\1_A(s)x$, $\forall x\in \X$, $s\in \Omega$.
\end{definition}

\begin{definition}[\cite{hy}]\label{vnwkelel}
Given a mapping $f:\Omega \to \X$ with values in the Banach space $(\X,\tau_{\text{N}}(\X))$, if there exists a sequence of simple functions $\{f_n,n\ge 1 \}$ almost everywhere converging to $f$ in the topology $\tau_{\text{N}}(\X)$, then $f$ is said to be strongly measurable with respect to $\tau_{\text{N}}(\X)$.
\end{definition}

\begin{definition}[\cite{hy}]
Let $(I,\leq)$ be an ordered set, $\{\F_k,k\in I\}$ be a sub-$\sigma$-algebra cluster of $\F$, and $\{f_k,k\in I\}$ be a cluster of random elements with values in the Hilbert space $(\X,\tau_{\text{N}}(\X))$.
\begin{itemize}
\item[1.] If $\F_m\subseteq \F_n$, $\forall m,n\in I$ and $m\leq n$, then $\{\F_k,k\in I\}$ is said to be a filter in $(\Omega,\F,\P)$.
\item[2.] If $\{\F_k,k\in I\}$ is a filter in $(\Omega,\F,\P)$, $f_k\in L^0(\Omega,\F_k;\X)$, $\forall k\in I$, then $\{f_k,\F_k,k\in I\}$ is called an adaptive sequence.
\item[3.] If $\{f_k,\F_k,k\in \mathbb Z\}$ is an adaptive sequence, $f_k$ is Bochner integrable on $\F_{k-1}$ and
\[\E[f_k|\F_{k-1}]=0,~\forall k\in \mathbb Z,\]
then $\{f_k,\F_k,k\in \mathbb Z\}$ is called a sequence of martingale differences.
\end{itemize}
\end{definition}

\begin{lemma}[\cite{hp}]\label{mse}
The mapping $f:\Omega\to \X$ is strongly measurable with respect to the topology $\tau_{\text{N}}(\X)$ if and only if $f$ is a random element with values in the Banach space $(\X,\tau_{\text{N}}(\X))$.
\end{lemma}

\begin{lemma}[\cite{hy}]\label{nvkvpeoeo}
If $\Gg\subseteq \F$ is a sub-$\sigma$-algebra and $f\in L^1(\Omega;\X)$, then there exists a unique conditional expectation $\E[f|\Gg]\in L^0(\Omega,\Gg;\X)\cap L^1(\Omega;\X)$ satisfying
\[\int_A\E[f|\Gg]\dd\P=\int_Af\dd\P,~\forall A\in \Gg.\]
\end{lemma}

\begin{lemma}[\cite{hy}]\label{lemmaA1}
Let $\X_1,\X_2$ and $\Y$ be Banach spaces, $\beta:\X_1\times \X_2 \to \Y$ be a bounded bilinear mapping and $\Gg$ be a sub-$\sigma$-algebra of $\mathcal F$. If $g\in L^0(\Omega,\Gg;\X_1)$ and $f\in L^0(\Omega;\X_2)$ is Bochner integrable on $\Gg$, then $\beta(g,f)\in L^0(\Omega;\Y)$ is Bochner integrable on $\Gg$, as well as giving us
\[\mathbb E[\beta(g,f)|\Gg]=\beta(g,\mathbb E[f|\Gg])~\text{a.s.}\]
\end{lemma}

\section{Proofs of Proposition \ref{wenknknkn} and Propositions \ref{vnwlssfweewwfew}-\ref{lemmaA6}}\label{appendixb}
%\setcounter{lemma}{0}
%\def\thelemma{B.\arabic{lemma}}
%\setcounter{definition}{0}
%\def\thedefinition{B.\arabic{definition}}
%\setcounter{equation}{0}
%\def\theequation{B.\arabic{equation}}


%\textbf{\emph{Proof of Proposition \ref{wenknknkn}:}}
\begin{proof}[Proof of Proposition \ref{wenknknkn}]
This proposition is proved by the \textbf{two steps} as follows.

%\emph{Step 1:} If $f=\1_{A}\otimes y$ with $A\in \F$ and $y\in \X$. Noting that $\sigma(\1_{A}\otimes y;\tau_{\text{N}}(\X))=\sigma(\1_{A};\tau_{\text{N}}(\X))=\{\emptyset,\Omega,A,A^{\complement}\}$, we get
%\ban
%(T(\1_{A}\otimes y))^{-1}(B)=
%\begin{cases}
%(Ty)^{-1}(B)\cap A\cup A^{\complement}, & 0\in B;\\
%(Ty)^{-1}(B)\cap A, & 0\notin B,
%\end{cases}
%\quad
%\forall B\in \B(\Y;\tau_{\text{N}}(\Y)),
%\ean
%which shows that $T(\1_{A}\otimes y)\in (\bigvee_{x\in \X}\sigma(Tx;\tau_{\text{N}}(\Y)))\bigvee \sigma(f;\tau_{\text{N}}(\X))$.
%
\emph{Step 1:} If $f$ is the simple function with values in the Banach space $\X$,  without loss of generality, let $f=\sum_{i=1}^n\1_{A_i}\otimes y_i$, where $A_i\in \F$, $y_i\in \X$, $A_i\cap A_j=\emptyset$, $1\leq i\neq j \leq n$. Note that $\sigma(\1_{A_i}\otimes y_i;\tau_{\text{N}}(\X))=\sigma(\1_{A_i};\tau_{\text{N}}(\X))=\{\emptyset,\Omega,A_i,A_i^{\complement}\}$. For any given $B\in \B(\Y;\tau_{\text{N}}(\Y))$, we have
\ban
&&~~~\left(T\left(\sum_{i=1}^n\1_{A_i}\otimes y_i\right)\right)^{-1}(B)\cr
&&=
\begin{cases}
\displaystyle \left(\bigcup_{i=1}^n(Ty_i)^{-1}(B)\right)\bigcap \left(\bigcup_{i=1}^n A_i\right)\bigcup \left(\bigcup_{i=1}^nA_i\right)^{\complement}, & 0\in B;\\
\displaystyle \left(\bigcup_{i=1}^n(Ty_i)^{-1}(B)\right)\bigcap \left(\bigcup_{i=1}^n A_i\right), & 0\notin B,
\end{cases}
\ean
which gives $T(\sum_{i=1}^n\1_{A_i}\otimes y_i)\in (\bigvee_{x\in \X}\sigma(Tx;\tau_{\text{N}}(\Y)))\bigvee \sigma(f;\tau_{\text{N}}(\X))$.

\emph{Step 2:} For the random element $f$ with values in the Banach space $(\X,\tau_{\text{N}}(\X))$, by Lemma \ref{mse} and Definition \ref{vnwkelel}, we know that there exists a sequence of simple functions $\{f_n,n\ge 1\}$ with values in $\X$ such that $\lim_{n\to\infty}\|f_n-f\|_{\X}=0~\text{a.s.}$ It follows from Proposition \ref{nlllwwieiie}.(b) that $Tf:\Omega\to \mathscr Y$ is strongly measurable with respect to the topology $\tau_{\text{N}}(\Y)$, i.e. there exists $\Omega_0\subseteq \Omega$, $\P(\Omega_0)=1$ and a separable closed subspace $ \Y_0\subseteq \mathscr Y$ such that $Tf(\Omega_0)\subseteq \mathscr Y_0$. Let $\mathscr Y^*$ be the dual of the Banach space $\mathscr Y$, and by the Hahn-Banach extension theorem we obtain
\ban
\|y\|=\sup_{\|y^*\|_{\mathscr Y^*}\leq 1}|y^*(y)|,~\forall y\in \mathscr Y.
\ean
Thus, for the closed ball $B_{\Y_0}(y_0,r):=\{y\in\mathscr Y_0:\|y-y_0\|\leq r\}$ in the Banach space $\mathscr Y_0$, it follows from Proposition B.1.10 in \cite{hy} that there exists a unit sequence $\{y^* _n,n\ge 1\}$ such that
\bna\label{nfwkjngjl}
&&~~~(Tf)^{-1}(B(y_0,r))\cr
&&=\{\omega\in \Omega:\|T(\omega)f(\omega)-y_0\|\leq r\}\cr
&&=\bigcap_{n=1}^{\infty}\{\omega\in \Omega:|y^*_n(T(\omega)f(\omega)-y_0)|\leq r\}\cr
&&=\bigcap_{n=1}^{\infty}\left\{\omega\in \Omega:\lim_{m\to\infty}|y^*_n(T(\omega)f_m(\omega)-y_0)|\leq r\right\}.
\ena
From the analysis in \emph{Step 1}, we know that $Tf_n\in (\bigvee_{x\in \X}\sigma(Tx;\tau_{\text{N}}(\Y)))\bigvee \sigma(f;\tau_{\text{N}}(\X))$. Noting that $y^*_n$ is a linear measurable functional mapping the Banach space $\mathscr Y$ to $\mathbb R$, we have
\bna\label{nvnejrfgiowjj}
&&\left\{\omega\in \Omega:\lim_{m\to\infty}|y^*_n(T(\omega)f_m(\omega)-y_0)|\leq r\right\}\cr
&&\in \left(\bigvee_{x\in \X}\sigma(Tx;\tau_{\text{N}}(\Y))\right)\bigvee \sigma(f;\tau_{\text{N}}(\X)).
\ena
Combining (\ref{nfwkjngjl})-(\ref{nvnejrfgiowjj}) yields $(Tf)^{-1}(B(y_0,r))\in (\bigvee_{x\in \X}\sigma(Tx;\tau_{\text{N}}(\Y)))\bigvee \sigma(f; \tau_{\text{N}}(\X))$, and since the open set of the Banach space $(\mathscr Y_0,$ $\tau_{\text{N}}(\Y_0))$ is generated by a countable number of closed spheres, we have $$Tf|_{\mathscr Y_0}\in\left(\bigvee_{x \in \X}\sigma(Tx;\tau_{\text{N}}(\Y))\right)\bigvee \sigma(f;\tau_{\text{N}}(\X)).$$ Given $B\in \B(\Y;\tau_{\text{N}}(\Y))$, noting that $B_0=B\cap \mathscr Y_0$, it is known that $B_0\in \B(\Y_0;\tau_{\text{N}}(\Y_0))$ and
\ban
(Tf)^{-1}(B)=\{\omega\in \Omega:T(\omega)f(\omega)\in B\}=\{\omega\in \Omega:T(\omega)f(\omega)\in B_0\}=(Tf)^{-1}(B_0),
\ean
which leads to
\ban
Tf\in \left(\bigvee_{x\in \X}\sigma(Tx;\tau_{\text{N}}(\Y))\right)\bigvee \sigma(f;\tau_{\text{N}}(\X)).
\ean
\end{proof}


%\textbf{\emph{Proof of Proposition \ref{vnwlssfweewwfew}:}}
\begin{proof}[Proof of Proposition \ref{vnwlssfweewwfew}]
Since the random elements with values in the Banach space $(\LL(\Y,\Z)$, $\tau_{\text{N}}(\LL(\Y,\Z)))$ are certainly the random elements with values in $(\LL(\Y,\Z),\tau_{\text{S}}(\LL(\Y,\Z)))$, it follows that $f\in L^1(\Omega;\LL(\Y,\Z))$ implies $fy\in L^1(\Omega;\Z)$, $\forall y\in \Y$ by Proposition (\ref{nlllwwieiie}).(a). Considering the simple function $\sum_{i=1}^n\1_{A_i}\otimes T_i$ with values in Banach space $\LL(\Y,\Z)$, where $A_i\in \F$, $T_i\in \LL(\Y,\Z)$, $A_i\cap A_j=\emptyset$, $1\leq i\neq j \leq n$, we have
\ban
\E\left[\left(\1_{A_i}\otimes T_i\right)y\right]=\P(A)(T_iy)=(\P(A_i)T_i)y=\E\left[\1_{A_i}\otimes T_i\right]y,~1\leq i\leq n,
\ean
which leads to
\bna\label{vnkwkekeneknek}
\E\left[\left(\sum_{i=1}^n\1_{A_i}\otimes T_i\right)y\right]&=&\sum_{i=1}^n\E\left[(\1_{A_i}\otimes T_i)y\right]\cr
&=&\sum_{i=1}^n\E\left[\ 1_{A_i}\otimes T_i\right]y\cr
&=&\E\left[\sum_{i=1}^n\1_{A_i}\otimes T_i\right]y.
\ena
Since $f\in L^1(\Omega;\LL(\Y,\Z))$ is the random element with values in the Banach space $(\LL(\Y,\Z)$, $\tau_{\text{N}}(\LL(\Y,\Z)))$, it follows from Lemma \ref{mse} and Definition \ref{vnwkelel} that there exists a sequence of simple functions $\{T_n,n \ge 1\}$ such that $\lim_{n\to \infty}\|f-T_n\|=0~\text{a.s.}$ and $\|T_n\|\leq \|f\|~\text{a.s.}$ Noting that $\|f-T_n\|\leq 2\|f\|\in L^1(\Omega)$ and $\lim_{n\to\infty}\|fy-T_ny\|=0~\text{a.s.}$, $\forall y\in \Y$, by the dominated convergence theorem, it follows that $\lim_{n\to\infty}\E[(f-T_n)y]=0$, $\forall y\in \Y$, and $\lim_{n\to\infty}\E[T_n]=\E[f]$. Thus, it follows from (\ref{vnkwkekeneknek}) that
\ban
\E[fy]=\lim_{n\to\infty}\E[(f-T_n)y]+\lim_{n\to\infty}\E[T_ny]=\lim_{n\to\infty}\E[T_n]y=\E[f]y,~\forall y\in \Y.
\ean
\end{proof}




%\textbf{\emph{Proof of Proposition \ref{tiaojianqiwangxingzhi}:}}
\begin{proof}[Proof of Proposition \ref{tiaojianqiwangxingzhi}]
Since the random elements with values in the Banach space $(\LL(\Y,\Z)$, $\tau_{\text{N}}(\LL(\Y,\Z)))$ are certainly the random elements with values in the topological space $(\LL(\Y,\Z)$, $\tau_{\text{S}}(\LL(\Y,\Z)))$, it follows from Proposition \ref{nlllwwieiie}.(b) that $fy$ is the random element with values in the Banach space $(\Z,\tau_{\text{N}}(\Z))$, $f\in L^2(\Omega;\LL(\Y,\Z))$, and $y\in L^2(\Omega;\Y)$ implies $fy\in L^1(\Omega ;\Z)$. Consider the simple function $\sum_{i=1}^n\1_{A_i}\otimes y_i$ with values in the Banach space $\Y$, where $A_i\in \Gg$, $y_i\in \Y$, $A_i\cap A_j=\emptyset$, $1\leq i\neq j\leq n$. For any given $F\in \Gg$ and $1\leq i\leq n$, by Lemma \ref{nvkvpeoeo}, we have
\bna\label{fcwlmlmcc}
\int_F\E[f(\1_{A_i}\otimes y_i)|\Gg]\dd\P=\int_Ff(\1_{A_i}\otimes y_i)\dd\P=\int_{F\cap A_i}fy_i\dd\P=\E[(\1_{F\cap A_i}f)y_i].
\ena
Noting that $F\cap A_i\in \Gg$, by Lemma \ref{nvkvpeoeo} and Proposition \ref{vnwlssfweewwfew}, we get
\ban
\E[(\1_{F\cap A_i}f)y_i]=\E[\1_{F\cap A_i}f]y_i=\left(\int_{F\cap A_i}f\dd\P \right)y_i=\left(\int_{F\cap A_i}\E[f|\Gg]\dd\P \right) y_i.
\ean
Noting that $\1_{F\cap A_i}\E[f|\Gg]\in L^2(\Omega,\Gg;\LL(\Y,\Z))$, it follows from Lemma \ref{nvkvpeoeo} and Proposition \ref{vnwlssfweewwfew} that
\bna\label{vmkweknfffff}
\left(\int_{F\cap A_i}\E[f|\Gg]\dd\P\right) y_i=\E\left[\1_{F\cap A_i}\E[f|\Gg]y_i\right]=\int_F\E[f|\Gg](\1_{A_i}\otimes y_i)\dd\P.
\ena
Noting that $\E[f|\Gg](\1_{A_i}\otimes y_i)\in L^0(\Omega,\Gg;\Z)$, it follows from (\ref{fcwlmlmcc})-(\ref{vmkweknfffff}) and Lemma \ref{nvkvpeoeo} that
\bna\label{vcmklwleklekle}
\E\left.\left[f\left(\sum_{i=1}^n\1_{A_i}\otimes y_i\right)\right|\Gg\right]=\E[f|\Gg]\left(\sum_{i=1}^n\1_{A_i}\otimes y_i\right)~\text{a.s.}
\ena
Given the random element $y\in L^2(\Omega,\Gg;\Y)$ with values in the Banach space $(\Y,\tau_{\text{N}}(\Y))$, there exists a sequence of simple functions $\{y_n\in L^0(\Omega,\Gg;\Y),n\ge 1\}$ with values in the Banach space $\Y$ such that $\lim_{n\to\infty}\|y-y_n\|=0~\text{a.s.}$ and $\|y_n\|\leq \|y\|~\text{a.s.}$ Noting that $\|fy_n\|\leq \|f\|^2+\|y\|^2\in L^1(\Omega)$ and $\E[f|\G]\in L^2(\Omega,\Gg;\LL(\Y,\Z))$, by the dominated convergence theorem of conditional expectation and (\ref{vcmklwleklekle}), we have
\ban
\E[fy|\Gg]=\E\left.\left[f\left(\lim_{n\to\infty}y_n\right)\right|\Gg\right]=\lim_{n\to\infty}\E\left.\left[f y_n\right|\Gg\right]= \E\left.\left[f\right|\Gg\right]\left(\lim_{n\to\infty}y_n\right)=\E\left.\left[f\right|\Gg\right]y~\text{a.s.}
\ean
\end{proof}


%\textbf{\emph{Proof of Proposition \ref{lemmaA4}:}}
\begin{proof}[Proof of Proposition \ref{lemmaA4}]
Given an integer $k\ge 2$, denote $\Ff^k=\bigvee_{i=k}^{\infty}\sigma(f_i;\tau(\X_1))$, $\Gg^k=\bigvee_{i=k}^{\infty}\sigma(g_i;\tau(\X_2))$,  $\Ff_k=\bigvee_{i=1}^{k-1}\sigma(f_i;\tau(\X_1))$, $\Gg_k=\bigvee_{i=1}^{k-1}\sigma(g_i;\tau(\X_2))$ and $$\Ff(k)=\bigvee_{i=1}^{k-1}(\sigma(f_i;\tau(\X_1))\bigvee \sigma(g_i;\tau(\X_2))).$$
Let $E\in \Ff^k$, $F\in \Gg^k$, $A\in \Ff_k$ and $B\in \Gg_k$. On the one hand, noting that $\{f_k,k\ge 1\}$ and $\{g_k,k\ge 1\}$ are mutually independent, we know that $\bigvee_{i=1}^{\infty}\sigma(f_i;\tau(\X_1))$ and $\bigvee_{i=1}^{\infty}\sigma(g_i;\tau(\X_2))$ are also mutually independent. Since $A\cap E\in \bigvee_{i=1}^{\infty}\sigma(f_i;\tau(\X_1))$ and $B\cap F\in \bigvee_{i=1}^{\infty}\sigma(g_i;\tau(\X_2))$, we get $\P(E\cap F\cap A\cap B)=\P(E\cap A)\P(F\cap B)$, which together with $A\cap B \in \Ff(k)$ gives
\bna\label{apendix1}
\int_{A\cap B}\P(E\cap F|\Ff(k))\dd\P=\P(E\cap F\cap A\cap B)=\P(E\cap A)\P(F\cap B).
\ena
On the other hand, noting that $\1_F\in \Gg^k$, $\Ff(k)=\Ff_k\bigvee\Gg_k$ and $\Gg^k\bigvee\Gg_k=\bigvee_{i=1}^{\infty}\sigma(g_i;\tau(\X_2))$ is independent of $\Ff_k$, by Corollary $7.3.3$ in \cite{chow}, we obtain $\E[\1_F|\Ff(k)]=\E[\1_F|\Gg_k]$, which further implies
\ban
\E[\1_E|\Ff(k)]\E[\1_F|\Ff(k)]=\E[\1_E\E[\1_F|\Ff(k)]|\Ff(k)]=\E[\1_E\E[\1_F|\Gg_k]|\Ff(k)],
\ean
from which we get
\bna\label{apendix3}
\int_{A\cap B}\P(E|\Ff(k))\P(F|\Ff(k))\dd\P&=&\int_{A\cap B}\E[\1_E\E[\1_F|\Gg_k]|\Ff(k)]\dd\P\cr
&=&\int_{A\cap B}\1_E\E[\1_F|\Gg_k]\dd\P.
\ena
Noting that $\1_B\in \Gg_k$, $\1_{A\cap E}$ and $\E[\1_{B\cap F}|\Gg_k]$ are mutually independent, we have
\bna\label{apendix4}
\int_{A\cap B}\1_E\E[\1_F|\Gg_k]\dd\P&=&\E[\1_{A\cap E}\E[\1_{B\cap F}|\Gg_k]]\cr &=&\E[\1_{A\cap E}]\E[\1_{B\cap F}]\cr
&=&\P(A\cap E)\P(B\cap F).
\ena
It follows from (\ref{apendix3})-(\ref{apendix4}) that
\bna\label{apendix5}
\int_{A\cap B}\P(E|\Ff(k))\P(F|\Ff(k))\dd\P=\P(A\cap E)\P(B\cap F).
\ena
Combining (\ref{apendix1}) and (\ref{apendix5}) gives
\bna\label{abcd}
\int_{A\cap B}\P(E\cap F|\Ff(k))\dd\P=\int_{A\cap B}\P(E|\Ff(k))\P(F|\Ff(k))\dd\P.
\ena
Denote
\[\Pi=\left.\left\{\bigcup_{i=1}^n(A_i\cap B_i)\right|A_i \in \Ff_k, B_i\in \Gg_k, A_i\cap B_i\cap A_j\cap B_j=\emptyset, 1\leq i\neq j\leq n\right\}.\]
If $\bigcup_{i=1}^n(A_i\cap B_i)\in \Pi$ and $\bigcup_{i=1}^m(C_i\cap D_i)\in \Pi$, then we have
\bna\label{apendix6}
&&~~~\left(\bigcup_{i=1}^n(A_i\cap B_i)\right)\bigcap \left(\bigcup_{i=1}^m(C_i\cap D_i)\right)\cr
&&=\bigcup_{j=1}^m\left(\left(\bigcup_{i=1}^n(A_i\cap B_i)\right)\cap C_j\cap D_j\right)\cr
&&=\bigcup_{j=1}^m\bigcup_{i=1}^n\left(A_i\cap B_i\cap C_j\cap D_j\right),
\ena
noting that $A_i\cap B_i\cap A_j\cap B_j=\emptyset, 1\leq i\neq j\leq n$, and $C_s\cap D_s\cap C_t\cap D_t=\emptyset, 1\leq s\neq t\leq m$, we know that $A_i\cap B_i\cap A_j\cap B_j\cap C_s\cap D_s\cap C_t\cap D_t=\emptyset$, $(i,j)\neq (s,t)$, which together with (\ref{apendix6}) gives
\ban
\left(\bigcup_{i=1}^n(A_i\cap B_i)\right)\bigcap \left(\bigcup_{i=1}^m(C_i\cap D_i)\right)\in \Pi,
\ean
thus, we conclude that $\Pi$ is a $\pi$-class, i.e. $\Pi$ is closed under the intersection operation of the set. Denote
\[\Lambda=\left\{ M\in \Ff(k)\bigg|\int_{M}\P(E\cap F|\Ff(k))\dd\P=\int_{M}\P(E|\Ff(k))\P(F|\Ff(k))\dd\P\right\}.\]
Noting that $\Pi$ is composed of finite disjoint union of elements in $\Ff_k\cap \Gg_k$, it follows from  (\ref{abcd}) that $\Omega \in \Lambda$. If $\bigcup_{i=1}^n(A_i\cap B_i)\in \Pi$, then we have
\ban
\int_{\bigcup_{i=1}^n(A_i\cap B_i)}\P(E\cap F|\Ff(k))\dd\P=\int_{\bigcup_{i=1}^n(A_i\cap B_i)}\P(E|\Ff(k))\P(F|\Ff(k))\dd\P,
\ean
which shows that $\bigcup_{i=1}^n(A_i\cap B_i)\in \Lambda$ and further gives $\Pi \subseteq \Lambda$. If $M_i\in \Lambda$ and $M_i \subseteq M_{i+1}$, then we obtain
\ban
\int_{\Omega}\1_{M_i}\P(E\cap F|\Ff(k))\dd\P=\int_{\Omega}\1_{M_i}\P(E|\Ff(k))\P(F|\Ff(k))\dd\P,~\forall i \ge 1.
\ean
Noting that $M_i \subseteq M_{i+1}$ implies $\lim_{i\to\infty}\1_{M_i}=\1_{\bigcup_{i=1}^{\infty}M_i}$, by Lebesgue dominated convergence theorem, we get
\ban
\int_{\Omega}\1_{\bigcup_{i=1}^{\infty}M_i}\P(E\cap F|\Ff(k))\dd\P=\int_{\Omega}\1_{\bigcup_{i=1}^{\infty}M_i}\P(E|\Ff(k))\P(F|\Ff(k))\dd\P.
\ean
Thus, we have $\bigcup_{i=1}^{\infty}M_i\in \Lambda$. If $M_1,M_2\in \Lambda$ and $M_1\subseteq M_2$, then $M_2=M_1\cup (M_2 \setminus M_1)$ implies
\ban
\left(\int_{M_1}+\int_{M_2\setminus M_1}\right)\P(E\cap F|\Ff(k))\dd\P=\left(\int_{M_1}+\int_{M_2\setminus M_1}\right)\P(E|\Ff(k))\P(F|\Ff(k))\dd\P.
\ean
It follows from $M_1\in \Lambda$ that
\ban
\int_{M_2\setminus M_1}\P(E\cap F|\Ff(k))\dd\P=\int_{M_2\setminus M_1}\P(E|\Ff(k))\P(F|\Ff(k))\dd\P,
\ean
which shows that $M_2\setminus M_1\in \Lambda$. From the above analysis, we know that $\Lambda$ is a $\lambda$-class, by Theorem 1.3.2 in \cite{chow}, we get $\sigma(\Pi)\subseteq \Lambda$. On the one hand, it follows from $\Pi \subseteq \Ff(k)=\Ff_k\bigvee\Gg_k$ that $\sigma(\Pi)\subseteq \Ff(k)$. On the other hand, noting that $\Ff_k\subseteq \Pi$ and $\Gg_k\subseteq \Pi$, we have $\Ff_k\cup\Gg_k\subseteq \sigma(\Pi)$, which leads to $\Ff(k)=\Ff_k\bigvee\Gg_k \subseteq \sigma(\Pi)$ and further gives $\Ff(k)=\sigma(\Pi)\subseteq \Lambda$. Noting that $E\in \Ff^k$ and $F\in \Gg^k$, we obtain
\ban
\int_{M}\P(E\cap F|\Ff(k))\dd\P=\int_{M}\P(E|\Ff(k))\P(F|\Ff(k))\dd\P,~\forall M\in \Ff(k),
\ean
which together with $\P(E|\Ff(k))\P(F|\Ff(k))\in \Ff(k)$ gives
\ban
\P(E\cap F|\Ff(k))=\P(E|\Ff(k))\P(F|\Ff(k))~\text{a.s.},
\ean
thus, we conclude that $\Ff^k$ is conditionally independent of $\Gg^k$ with respect to $\Ff(k)$.
\end{proof}


Before proving Proposition \ref{lemmaA6}, we introduce the following lemma.
\begin{lemma}\label{lemmaA5}
\rm{
Let $\Gg$ be a sub-$\sigma$-algebra of $\F$, $A\in \F$ and $f\in L^1(\Omega;\X)$. If $f$ is conditionally  independent of $\1_A$ with respect to $\Gg$, then
\[\E[f\1_A|\Gg]=\E[f|\Gg]\E[\1_A|\Gg]~\text{a.s.}\]}
\end{lemma}
\begin{proof}
Let $f=\1_B\otimes x$, where $B\in \F$ and $x\in \X$. Noting that
\bna\label{appendix3}
\sigma(\1_B\otimes x;\tau_{\text{N}}(\X))=\left\{(\1_B\otimes x)^{-1}(E):E\in \mathscr B(\X;\tau_{\text{N}}(\X))\right\},
\ena
it follows that
\bna\label{appendix4}
(\1_B\otimes x)^{-1}(E)=
\begin{cases}
\Omega, & \text{If $0\in E$, $x\in E$};\\
B, &  \text{If $0\in E$, $x\notin E$};\\
B^{\complement}, & \text{If $0\notin E$, $x\in E$};\\
\emptyset, & \text{If $0\notin E$, $x\notin E$}.
\end{cases}
,~
\forall E\in \mathscr B(\X;\tau_{\text{N}}(\X)),
\ena
which leads to $\sigma(\1_B\otimes x;\tau_{\text{N}}(\X))=\sigma(\1_B;\tau_{\text{N}}(\X))$. Since $\1_B\otimes x$ is conditionally independent of $\1_A$ with respect to $\Gg$, it follows from Definition \ref{tiaojiandulixing} that $\sigma(\1_B\otimes x;\tau_{\text{N}}(\X))$ is conditionally independent of  $\sigma(\1_A;\tau_{\text{N}}(\X))$ with respect to $\Gg$, from which we can further conclude that $\sigma(B)$ is conditionally independent of $\sigma(A)$ with respect to $\Gg$. Given $F\in \Gg$, on the one hand, we have
\bna\label{appendix1111}
\int_F\E[f\1_A|\Gg]\dd\P&=&x\int_F\E[\1_{A\cap B}|\Gg]\dd\P\cr
&=&x\int_F\P(A\cap B|\Gg)\dd\P\cr
&=&x\int_F\P(A|\Gg)\P(B|\Gg)\dd\P.
\ena
On the other hand, noting that $\E[\1_B\otimes x|\Gg]=\E[\1_B|\Gg]\otimes x~\text{a.s.}$, we get
\bna\label{appendix2222}
\int_F\E[f|\Gg]\E[\1_A|\Gg]\dd\P&=&\int_F\E[\1_B\otimes x|\Gg]\E[\1_A|\Gg]\dd\P\cr
&=&x\int_F\P(A|\Gg)\P(B|\Gg)\dd\P.
\ena
Then, by(\ref{appendix1111})-(\ref{appendix2222}), we obtain
\bna\label{appendix5}
\E[(\1_B\otimes x)\1_A|\Gg]=\E[\1_B\otimes x|\Gg]\E[\1_A|\Gg]~\text{a.s.}
\ena
Let $f=\sum_{i=1}^n\1_{B_i}\otimes x_i$, where $B_i \in \F$, $B_i\cap B_j=\emptyset$, $x_i\in \X$, $1\leq i\neq j\leq n$. Following the same way as (\ref{appendix3})-(\ref{appendix4}), we have
\ban
\sigma\left(\sum_{i=1}^n\1_{B_i}\otimes x_i;\tau_{\text{N}}(\X)\right)=\sigma\left(\bigcup_{i=1}^n B_i\right).
\ean
Thus, $f$ is conditionally independent of $\1_{A}$ with respect to $\Gg$ implies that $\sigma(\bigcup_{i=1}^n B_i)$ is conditionally independent of $\sigma(A)$ with respect to $\Gg$, from which we know that $B_i$ is conditionally independent of $A$ with respect to $\Gg$, $1\leq i\leq n$. It follows from (\ref{appendix5}) that
\bna\label{appendix8}
\int_F\E\left.\left[\sum_{i=1}^n(\1_{B_i}\otimes x_i)\1_A\right|\Gg\right]\dd\P=\sum_{i=1}^n\int_F\E\left.\left[\1_{B_i}\otimes x\right|\Gg\right]\E[\1_A|\Gg]\dd\P,
\ena
which leads to
\ban
\E\left.\left[\sum_{i=1}^n(\1_{B_i}\otimes x_i)\1_A\right|\Gg\right]=\E\left.\left[\sum_{i=1}^n\1_{B_i}\otimes x\right|\Gg\right]\E[\1_A|\Gg]~\text{a.s.}
\ean
For the random element $f$ with values in Banach space $(\X,\tau_{\text{N}}(\X))$, there exists a sequence of simple functions with values in $\X$ such that $\lim_{n\to\infty}f_n=f~\text{a.s.}$ and $\|f_n\|\leq \|f\|~\text{a.s.}$ Noting that $\|f_n\1_A\|\leq \|f\|~\text{a.s.}$ together with $f\in L^1(\Omega;\X)$ implies that $\E[\|f\|]<\infty$, by the dominated convergence theorem, we get
\bna \label{appendix6}
\int_F\E\left.\left[\lim_{n\to\infty}f_n\1_A\right|\Gg\right]\dd\P=\int_F\lim_{n\to\infty}\E[f_n\1_A|\Gg]\dd\P,
\ena
and
\ban
\int_F\E\left.\left[\lim_{n\to\infty}f_n\right|\Gg\right]\E[\1_A|\Gg]\dd\P=\int_F\lim_{n\to\infty}\E[f_n|\Gg]\E[\1_A|\Gg]\dd\P.
\ean
It follows from (\ref{appendix8}) that
\bna\label{appendix9}
\int_F\E[f_n\1_A\Gg]\dd\P=\int_F\E[f_n|\Gg]\E[\1_A|\Gg]\dd\P.
\ena
Thus, by (\ref{appendix6})-(\ref{appendix9}), we have
\ban
\int_F\E[f\1_A|\Gg]\dd\P=\int_F\E[f|\Gg]\E[\1_A|\Gg]\dd\P,~\forall F\in \Gg.
\ean
\end{proof}


%\textbf{\emph{Proof of Proposition \ref{lemmaA6}:}}
\begin{proof}[Proof of Proposition \ref{lemmaA6}]
We first consider the case with $f=\sum_{i=1}^n\1_{A_i}\otimes x_i$, where $A_i\in \F$, $A_i\cap A_j=\emptyset$, $x_i\in \X$, $1\leq i\neq j\leq n$. Since $T$ is conditionally independent of $f$ with respect to $\Gg$, it follows that $\sigma(Tx_i;\tau_{\text{N}}(\Y))$ is conditionally independent of $\sigma(\sum_{i=1}^n\1_{A_i}\otimes x_i;\tau_{\text{N}}(\X))$ with respect to $\Gg$, $1\leq i\leq n$, from which we know that $\sigma(Tx_i;\tau_{\text{N}}(\Y))$ is conditionally independent of $\sigma(A_i)$ with respect to $\Gg$, thus, $Tx_i$ is conditionally independent of $\1_{A_i}$ with respect to $\Gg$, then by Lemma \ref{lemmaA5}, we have
\bna\label{appendix11}
\E[T(\1_{A_i}\otimes x_i)|\Gg]&=&\E[(Tx_i)\1_{A_i}|\Gg]\cr &=&\E[Tx_i|\Gg]\E[\1_{A_i}|\Gg]\cr
&=&\E[(Tx_i)\E[\1_{A_i}|\Gg]|\Gg]~\text{a.s.}
\ena
Noting that
\bna\label{appendix12}
\E[(Tx_i)\E[\1_{A_i}|\Gg]|\Gg]=\E[T(\E[\1_{A_i}|\Gg]\otimes x_i)|\Gg]=\E[T\E[\1_{A_i}\otimes x_i|\Gg]|\Gg]~\text{a.s.}.
\ena
By (\ref{appendix11})-(\ref{appendix12}), we have
\bna\label{appendix14}
\E[Tf|\Gg]&=&\sum_{i=1}^n\E[T(\1_{A_i}\otimes x_i)|\Gg]\cr &=&\sum_{i=1}^n\E[T\E[\1_{A_i}\otimes x_i|\Gg]|\Gg]\cr
&=&\E[T\E[f|\Gg]|\Gg]~\text{a.s.}
\ena
Given the random element $f\in L^2(\Omega;\X)$ with values in Banach space $(\X,\tau_{\text{N}}(\X))$, by Lemma \ref{mse} and Definition \ref{vnwkelel}, we know that there exists a sequence $\{f_n,n\ge 1\}$ of simple functions, such that $\lim_{n\to\infty}f_n=f~\text{a.s.}$ and $\|f_n\|\leq \|f\|~\text{a.s.}$ If $\E[\|T\|^2]<\infty$ and $f\in L^2(\Omega;\X)$, noting that $T(\omega)\in \mathscr L(\X,\mathscr Y)$, we get $\|(Tf_n)(\omega)\|=\|T(\omega)f_n(\omega)\|\leq \|T(\omega)\|\|f_n(\omega)\|\leq \|T(\omega)\|^2+\|f(\omega)\|^2\in L^2(\Omega)$, which together with conditional dominated convergence theorem gives
\bna\label{appendix13}
\E[Tf|\Gg]=\E\left.\left[T\left(\lim_{n\to\infty}f_n\right)\right|\Gg\right]=\E\left.\left[\lim_{n\to\infty}Tf_n\right|\Gg\right]=\lim_{n\to\infty}\E[Tf_n|\Gg]~\text{a.s.}
\ena
By (\ref{appendix14}), we obtain
\ban
\E[Tf_n|\Gg]=\E[T\E[f_n|\Gg]|\Gg]~\text{a.s.}
\ean
It follows from $\|T\E[f_n|\Gg]\|\leq \|T\|\|\E[f_n|\Gg]\|\leq \|T\|^2+\E[\|f_n\|^2|\Gg]\leq \|T\|^2+\E[\|f\|^2|\Gg]\in L^2(\Omega)$ and conditional dominated convergence theorem that
\bna\label{appendix16}
\lim_{n\to\infty}\E[T\E[f_n|\Gg]|\Gg]=\E\left.\left[\lim_{n\to\infty}T\E[f_n|\Gg]\right|\Gg\right]=\E\left.\left[T\E\left.\left[\lim_{n\to\infty}f_n\right|\Gg\right]\right|\Gg\right]~\text{a.s.}
\ena
Hence, by (\ref{appendix13})-(\ref{appendix16}), we get
\ban
\E[Tf|\Gg]=\E[T\E[f|\Gg]|\Gg]~\text{a.s.}
\ean
\end{proof}


\section{Proofs of main results}\label{appendixc}
%\setcounter{lemma}{0}
%\def\thelemma{C.\arabic{lemma}}
%\setcounter{definition}{0}
%\def\thedefinition{C.\arabic{definition}}
%\setcounter{equation}{0}
%\def\theequation{C.\arabic{equation}}

For the convenience of notational writing without giving rise to ambiguity, the subscripts of the parametrization in Banach space and the subscripts of the inner product in Hilbert space will be omitted in the sequel. Before proving the main result, we need to introduce some key lemmas below, which are proved in Appendix \ref{appendixd}.


\begin{lemma}\label{lemma1}
For the algorithm (\ref{algorithm}), suppose that Conditions \ref{condition1} and \ref{condition2} hold, and there exists an integer $h>0$ and a constant $\rho>0$, such that
\bna\label{nxsl}
\sup_{k\ge 0}\E\left.\left[\|\H^*(k)\H(k)\|^{2^{\max\{h,2\}}}\right|\F(k-1)\right]^{\frac{1}{2^{\max\{h,2\}}}}\leq \rho~\text{a.s.}
\ena
For any given integer $n\ge 0$, \\
(I) If $x\in L^0(\Omega,\F(nh-1);\X^N)$, then there exists a constant $d_1>0$ such that
\bna\label{yinliwudianyi1}
\sup_{m\ge 0}\mathbb E\left.\left[\|\Phi_P(mh-1,nh)x\|^2\right|\mathcal F(nh-1)\right]\leq d_1\|x\|^2~\text{a.s.}
\ena
(II) If $x\in L^0(\Omega,\F(n-1);\X^N)$, then there exists a constant $d_2>0$ such that
\bna\label{yinliwudianyi2}
\displaystyle \sup_{0\leq m\leq n+h}\mathbb E\left[\|\Phi_P(m,n)x\|^2|\mathcal F(n-1)\right]\leq d_2\|x\|^2~\text{a.s.}
\ena
\end{lemma}



\begin{lemma}\label{hhhlemma}
For the algorithm (\ref{algorithm}), if Conditions \ref{condition1} and \ref{condition2} hold, and there exists an integer $h>0$ and a constant $\rho>0$, such that
\ban
\sup_{k\ge 0}\E\left.\left[\|\H^*(k)\H(k)\|^{2^{\max\{h,2\}}}\right|\F(k-1)\right]^{\frac{1}{2^{\max\{h,2\}}}}\leq \rho~\text{a.s.},
\ean
then there exits a constant $d_3>0$, such that
\ban
&&~~~~\sup_{k\ge 0}\E\left[\left\|\left(\prod_{j=i+1}^k\left(I_{\X^N}-\sum_{s=jh}^{(j+1)h-1}(a(s)\H^*(s)\H(s)+b(s)(\L_{\G}\otimes I_{\X}))\right)\right)y\right\|^2\right]\cr
&&\leq d_3\E\left[\|y\|^2\right],~\forall i\ge 0,
\ean
for any given $y\in L^0(\Omega,\F((i+1)h-1);\X^N)$.
\end{lemma}


\begin{lemma}\label{henandelemma}
For the algorithm (\ref{algorithm}), suppose that  Assumptions \ref{assumption1}, \ref{assumption2}, Conditions \ref{condition1} and \ref{condition2} hold, there exists an integer $h>0$, a constant $\rho_0>0$, a strictly positive self-adjoint operator $\HH\in \mathscr L(\mathscr X^N)$ and a nonnegative real sequence $\{c(k),k\ge 0\}$, respectively, satisfying the following conditions: \\
(i) For any given $L_2$-bounded adaptive sequence $\{x(k),\F(kh-1),k\ge 0\}$ with values in Hilbert space $\X^N$,
\ban \lim_{k\to\infty}\sum_{i=m}^{k}\left(\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)\mu(i)\right\|^2\right]\right)^{\frac{1}{2}}=0,~\forall m\in \mathbb N,\quad \sum_{k=0}^{\infty}c(k)=\infty,
\ean
where $\displaystyle\mu(i):=c(i) \mathscr{H} x(i)-\sum_{s=i h}^{(i+1) h-1}\left(a(s) \mathbb{E}\left[\mathcal{H}^{*}(s) \mathcal{H}(s) x(i) \mid \mathcal{F}(i h-1)\right]+b(s)\left(\mathcal{L}_{\mathcal{G}} \otimes I_{\mathscr{X}}\right) x(i)\right)$;\\
(ii) $\displaystyle \sup_{k\ge 0}\left(\E\left.\left[\|\H^*(k)\H(k)\|^{2^{\max\{h,2\}}}\right|\F(k-1)\right]\right)^{\frac{1}{2^{\max\{h,2\}}}}\leq \rho_0~\text{a.s.}
$\\
The operator-valued random sequence $$ \left\{I_{\X^N}-\sum_{i=kh}^{(k+1)h-1}(a(i)\H^*(i)\H(i)+b(i)\L_{\G}\otimes I_{\X}),k\ge 0\right\}$$ is $L_2^2$-stable with respect to the filter $\{\F((k+1)h-1),k\ge 0\}$.
\end{lemma}


%\textbf{\emph{Proof of Theorem \ref{wendingxing}:}}
\begin{proof}[Proof of Theorem \ref{wendingxing}]
Given the initial value $x(0)\in \X_1$, by Proposition \ref{nlllwwieiie}.(a)-(c), we know that $x(k)$ is the random element with values in Hilbert space $(\X_1,\tau_{\text{N}}(\X_1))$. It follows from the random difference equation (\ref{chafen}) that
\bna\label{okjfs}
x(k+1)&=&\left(\prod_{i=0}^k(I_{\X_1}-F(i))\right)x(0)\cr &&+\sum_{i=0}^k\left(\prod_{j=i+1}^k(I_{\X_1}-F(j))\right)G(i)u(i),~k\ge 0,
\ena
by (\ref{okjfs}) and Cauchy inequality, we have
\bna\label{ijfwg}
\E[\|x(k+1)\|^2]&\leq& 2\E\left[\left\|\left(\prod_{i=0}^k(I_{\X_1}-F(i))\right)x(0)\right\|^2\right]\cr &&+2\E\left[\left\|\sum_{i=0}^k\left(\prod_{j=i+1}^k(I_{\X_1}-F(j))\right)G(i)u(i)\right\|^2\right].
\ena
It follows from Definition \ref{tiaojiandulixing} that $\sigma(u(k);\tau_{\text{N}}(\X_2))$ is independent of $$\sigma\left(F(k);\tau_{\text{S}}(\LL(\X_1))\right)\bigvee \sigma\left(G(k);\tau_{\text{S}}(\LL(\X_2,\X_1))\right),$$ which leads to
\bna\label{xxll}
\E\left[\left\|G(t)u(t)\right\|^2\right]&\leq& \E\left[\|G(t)\|^2\right]\E\left[\|u(t)\|^2\right]\cr
&\leq& \sup_{k\ge 0}\E\left[\|u(k)\|^2\right]\E\left[\|G(t)\|^2\right],~t\ge 0.
\ena
By the condition (\ref{ssafe}), we get $$\sup_{k\ge 0}\E[\|G(k)\|^2]<\infty,$$ thus, (\ref{xxll}) implies $\sup_{k\ge 0}\E[\|G(k)u(k)\|^2]<\infty$. For $0\leq t<k$, by the condition (\ref{qqqqq}), we obtain
\bna\label{xxoopp}
&&~~~\E\left.\left[\prod_{j=t+1}^k\left\|I_{\X_1}-F(j)\right\|^4\right|\F(t)\right]\cr
&&=\E\left.\left[\E\left.\left[\prod_{j=t+1}^k\left\|I_{\X_1}-F(j)\right\|^4\right|\mathcal F(k-1)\right]\right|\F(t)\right]\cr
&&=\E\left.\left[\E\left.\left[\left\|I_{\X_1}-F(k)\right\|^4\right|\mathcal F(k-1)\right]\prod_{j=t+1}^{k-1}\|I_{\X_1}-F(j)\|^4\right|\F(t)\right]\cr
&&\leq (1+\gamma(k))\E\left.\left[\prod_{j=t+1}^{k-1}\|I_{\X_1}-F(j)\|^4\right|\F(t)\right]\cr
&&\vdots \cr
&&\leq \prod_{j=t+1}^k(1+\gamma(j))~\text{a.s.}
\ena
It follows from $\sup_{k\ge 0}\E[\|G(k)\|^4]<\infty$ and (\ref{xxoopp}) that
\bna\label{oopp}
&&~~~\E\left[\left\|\left(\prod_{j=s+1}^k(I_{\X_1}-F(j))\right)^*\left(\prod_{j=t+1}^k(I_{\X_1}-F(j))\right)G(t)\right\|^2\right]\cr
&&\leq \E\left[\left(\prod_{j=t+1}^k\|I_{\X_1}-F(j)\|^4\right)\left(\prod_{j=s+1}^t\|I_{\X_1}-F(j)\|^2\right)\|G(t)\|^2\right]\cr
&&=\E\left[\E\left.\left[\prod_{j=t+1}^k\|I_{\X_1}-F(j)\|^4\right|\F(t)\right]\left(\prod_{j=s+1}^t\|I_{\X_1}-F(j)\|^2\right)\|G(t)\|^2\right]\cr
&&\leq \left(\prod_{j=t+1}^k(1+\gamma(j))\right)\E\left[\left(\prod_{j=s+1}^t\|I_{\X_1}-F(j)\|^2\right)\|G(t)\|^2\right]\cr
&&\leq \left(\prod_{j=t+1}^k(1+\gamma(j))\right)\left(\E\left[\prod_{j=s+1}^t\|I_{\X_1}-F(j)\|^4\right]+\E\left[\|G(t)\|^4\right]\right)\cr
&&\leq \left(\prod_{j=t+1}^k(1+\gamma(j))\right)\left(\prod_{j=s+1}^t(1+\gamma(j))+\sup_{k\ge 0}\E\left[\|G(k)\|^4\right]\right)\cr
&&\leq \left(\prod_{k=0}^{\infty}(1+\gamma(k))\right)\left(1+\sup_{k\ge 0}\E\left[\|G(k)\|^4\right]\right)\cr
&&<\infty,~0\leq s<t\leq k,
\ena
where the last inequality is due to $\sum_{k=0}^{\infty}\gamma(k)<\infty$. By $\sup_{k\ge 0}\E[\|u(k)\|^2]<\infty$,  Proposition \ref{nlllwwieiie} and (\ref{oopp}), we have
\bna\label{vnokllllll}
&&\left(\prod_{j=s+1}^k(I_{\X_1}-F(j))\right)^*\left(\prod_{j=t+1}^k(I_{\X_1}-F(j))\right)G(t)u(t)\cr
&&\in L^1(\Omega;\X_1),~0\leq s<t\leq k.
\ena
Noting that $\E[u(t)|\F(s)]=\E[\E[u(t)|\mathcal F(t-1)]|\F(s)]=0$, $0\leq s<t$, and Proposition \ref{wenknknkn} implies $G(s)u(s)\in L^0(\Omega,\F(s);\X^N)$, it follows from (\ref{oopp})-(\ref{vnokllllll}), Proposition \ref{nlllwwieiie}.(a)-(c), Lemma \ref{lemmaA1} and Propositions \ref{lemmaA4}-\ref{lemmaA6} that
\bna\label{fwwe}
&&~~~~\E\left[\left\langle \left(\prod_{j=s+1}^k(I_{\X_1}-F(j))\right)G(s)u(s), \left(\prod_{j=t+1}^k(I_{\X_1}-F(j))\right)G(t)u(t)\right\rangle \right]\cr
&&=\E\left[\left\langle G(s)u(s), \left(\prod_{j=s+1}^k(I_{\X_1}-F(j))\right)^*\left(\prod_{j=t+1}^k(I_{\X_1}-F(j))\right)G(t)u(t)\right\rangle \right]\cr
&&=\E\left[\E\left[\left\langle G(s)u(s), \left(\prod_{j=s+1}^k(I_{\X_1}-F(j))\right)^*\right.\right.\right.\cr &&~~~~~~~~~~~~~~~~\times\left.\left.\left.\left.\left(\prod_{j=t+1}^k(I_{\X_1}-F(j))\right)G(t)u(t)\right\rangle\right|\F(s) \right]\right]\cr
&&=\E\left[\left\langle G(s)u(s), \E\left[\left(\prod_{j=s+1}^k(I_{\X_1}-F(j))\right)^*\right.\right.\right.\cr &&~~~~~~~~~~~~~~~~\times\left.\left.\left.\left.\left(\prod_{j=t+1}^k(I_{\X_1}-F(j))\right)G(t)u(t)\right|\F(s) \right]\right\rangle\right]\cr
&&=\E\left[\left\langle G(s)u(s), \E\left[\left(\prod_{j=s+1}^k(I_{\X_1}-F(j))\right)^*\right.\right.\right.\cr &&~~~~~~~~~~~~~~~~\times\left.\left.\left.\left.\left(\prod_{j=t+1}^k(I_{\X_1}-F(j))\right)G(t)\E[u(t)|\F(s)]\right|\F(s) \right]\right\rangle\right]\cr
&&=0.
\ena
It follows from Markov inequality that $\P(\E[\|G(i)u(i)\|^2]=0)=\P(G(i)u(i)=0)=1$, $\forall i\ge 0$. Denote $\Lambda=\{i\in \mathbb N:\E[\|G(i)u(i)\|^2]>0\}$. By (\ref{ijfwg}), (\ref{xxll}) and (\ref{fwwe}), we obtain
\bna\label{wiiie}
&&~~~~\E\left[\|x(k+1)\|^2\right]\cr
&&=\E\left[\left\|\left(\prod_{i=0}^k(I_{\X_1}-F(i))\right)x(0)\right\|^2\right]+\sum_{i=0}^k\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X_1}-F(j))\right)G(i)u(i)\right\|^2\right]\cr
&&\leq \E\left[\left\|\left(\prod_{i=0}^k(I_{\X_1}-F(i))\right)x(0)\right\|^2\right]+\sum_{i=0}^{\infty}\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X_1}-F(j))\right)G(i)u(i)\right\|^2\right]\cr
&&=\E\left[\left\|\left(\prod_{i=0}^k(I_{\X_1}-F(i))\right)x(0)\right\|^2\right]\cr &&~~~+\sum_{i\in \Lambda}\E\left[\|G(i)u(i)\|^2\right]\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X_1}-F(j))\right)\frac{G(i)u(i)}{\left(\E\left[\|G(i)u(i)\|^2\right]\right)^{\frac{1}{2}}}\right\|^2\right]\cr
&&\leq \E\left[\left\|\left(\prod_{i=0}^k(I_{\X_1}-F(i))\right)x(0)\right\|^2\right]+\sup_{k\ge 0}\E\left[\|u(k)\|^2\right]\sum_{i\in \Lambda}\E\left[\|G(i)\|^2\right]\cr &&~~~~~~~~~~~~\times\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X_1}-F(j))\right)\frac{G(i)u(i)}{\left(\E\left[\|G(i)u(i)\|^2\right]\right)^{\frac{1}{2}}}\right\|^2\right].
\ena
Noting that the operator-valued random sequence $\{I_{\X_1}-F(k),k\ge 0\}$ is $L_2^2$-stable with respect to the filter $\{\F(k),k\ge 0\}$, we get
\bna\label{oowf}
\lim_{k\to \infty}\E\left[\left\|\left(\prod_{i=0}^k(I_{\X_1}-F(i))\right)x(0)\right\|^2\right]=0.
\ena
By Proposition \ref{wenknknkn}, we know that $G(i)u(i)\in L^0(\Omega,\mathcal F(i);\X_1)$ and
\ban
\E\left[\left\|\frac{G(i)u(i)}{\left(\E\left[\|G(i)u(i)\|^2\right]\right)^{\frac{1}{2}}}\right\|^2\right]=1,~i\in \Lambda,
\ean
which leads to
\bna\label{wwml}
\lim_{k\to\infty}\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X_1}-F(j))\right)\frac{G(i)u(i)}{\left(\E\left[\|G(i)u(i)\|^2\right]\right)^{\frac{1}{2}}}\right\|^2\right]=0,~\forall i\in \Lambda.
\ena
It follows from $G(i)u(i)\in L^0(\Omega,\mathcal F(i);\X_1)$ and (\ref{xxoopp}) that
\bna\label{llnnv}
&&~~~\sup_{k\ge 0\atop i\in \Lambda}\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X_1}-F(j))\right)\frac{G(i)u(i)}{\left(\E\left[\|G(i)u(i)\|^2\right]\right)^{\frac{1}{2}}}\right\|^2\right]\cr
&&=\sup_{k\ge 0\atop i\in \Lambda}\E\left.\left[\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X_1}-F(j))\right)\frac{G(i)u(i)}{\left(\E\left[\|G(i)u(i)\|^2\right]\right)^{\frac{1}{2}}}\right\|^2\right|\mathcal F(i)\right]\right]\cr
&&\leq \sup_{k\ge 0\atop i\in \Lambda}\E\left.\left[\E\left[\left\|\prod_{j=i+1}^k(I_{\X_1}-F(j))\right\|^2\right|\mathcal F(i)\right]\left\|\frac{G(i)u(i)}{\left(\E\left[\|G(i)u(i)\|^2\right]\right)^{\frac{1}{2}}}\right\|^2\right]\cr
&&\leq \sup_{k\ge 0\atop i\in \Lambda}\E\left.\left[\E\left[\left\|\prod_{j=i+1}^k(I_{\X_1}-F(j))\right\|^4\right|\mathcal F(i)\right]^{\frac{1}{2}}\left\|\frac{G(i)u(i)}{\left(\E\left[\|G(i)u(i)\|^2\right]\right)^{\frac{1}{2}}}\right\|^2\right]\cr
&&\leq \sup_{k\ge 0\atop i\in \Lambda}\E\left[\sqrt{\prod_{j=i+1}^k(1+\gamma(j))}\left\|\frac{G(i)u(i)}{\left(\E\left[\|G(i)u(i)\|^2\right]\right)^{\frac{1}{2}}}\right\|^2\right]\cr
&&\leq \sqrt{\prod_{k=0}^{\infty}(1+\gamma(k))}\sup_{i\in \Lambda}\E\left[\left\|\frac{G(i)u(i)}{\left(\E\left[\|G(i)u(i)\|^2\right]\right)^{\frac{1}{2}}}\right\|^2\right]\cr
&&=\sqrt{\prod_{k=0}^{\infty}(1+\gamma(k))}\cr
&&<\infty.
\ena
By the condition (\ref{ssafe}), (\ref{wwml})-(\ref{llnnv}) and Lemma \ref{lemma6}, we obtain
\bna\label{llks}
~~~\lim_{k\to \infty}\sum_{i\in \Lambda}\E\left[\|G(i)\|^2\right]\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X_1}-F(j))\right)\frac{G(i)u(i)}{\left(\E\left[\|G(i)u(i)\|^2\right]\right)^{\frac{1}{2}}}\right\|^2\right]=0.
\ena
Thus, substituting (\ref{oowf}) and (\ref{llks}) into (\ref{wiiie}) gives $\lim_{k\to\infty}\E[\|x(k)\|^2]=0$.
\end{proof}


%\textbf{\emph{Proof of Lemma \ref{dingliyi1}:}}
\begin{proof}[Proof of Lemma \ref{dingliyi1}]
Denote $F(k)=a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\X}$ and $G(k)=a(k)\mathcal H^*(k)$, respectively. Notice that $F(k)\ge 0$ and the estimation error equation (\ref{error}) can be rewritten as the following random difference equation
\ban
e(k+1)=\left(I_{\X^N}-F(k)\right)e(k)+G(k)v(k).
\ean
Given the initial value $e(0)\in \X^N$, by Proposition \ref{nlllwwieiie}.(a)-(c), we know that $\{e(k),k\ge 0\}$ is the random sequence with values in Hilbert space $(\X^N,\tau_{\text{N}}(\X^N))$. On the one hand, it follows from Assumptions \ref{assumption1} and \ref{assumption2} that $\{v(k),k\ge 0\}$ is independent of $\{F(k),G(k),k\ge 0\}$, and $\sup_{k\ge 0}\E[\|v(k)\|^2]\leq \b_v$. On the other hand, by the condition (\ref{qafgs}), we get $\E[\|I_{\X^N}-F(k)\|^4|\F(k-1)]\leq 1+\gamma(k)~\text{a.s.}$, which gives
\bna\label{xlms}
&&~~~\E\left[\|a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\X}\|^2\right]\cr
&&\leq \E\left[\|I_{\X^N}-(I_{\X^N}-F(k))\|^2\right]\cr
&&\leq 2\left(1+\E\left[\|I_{\X^N}-F(k)\|^2\right]\right)\cr
&&\leq 2\left(1+\E\left[\|I_{\X^N}-F(k)\|^4\right]^{\frac{1}{2}}\right)\cr
&&\leq 2\left(1+\sqrt{1+\gamma(k)}\right).
\ena
Noting that $\G$ is undirected, we have $a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\X}\ge a(k)\H^*(k)\H(k)\ge 0~\text{a.s.}$, which leads to $\|a(k)\H^*(k)\H(k)\|^2\leq \|a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\X}\|^2$. By (\ref{xlms}), we obtain
\bna\label{xfwee}
&&~~~\sup_{k\ge 0}\E\left[\|G(k)\|^4\right]\cr &&=\sup_{k\ge 0}\left\{a^2(k)\E\left[\left\|a(k)\H^*(k)\H(k)\right\|^2\right]\right\}\cr
&&\leq \sup_{k\ge 0}\left\{a^2(k)\E\left[\|a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\X}\|^2\right]\right\}\cr
&&\leq 2\sup_{k\ge 0}\left\{a^2(k)\left(1+\sqrt{1+\gamma(k)}\right)\right\}\cr
&&<\infty,
\ena
where the last inequality is obtained from Condition \ref{condition2} and $\sum_{k=0}^{\infty}\gamma(k)<\infty$.\\
(I) If $\sup_{k\ge 0}\E[\|\H(k)\|^2]<\infty$, it follows from Condition \ref{condition2} that
\ban
\sum_{k=0}^{\infty}\E\left[\|G(k)\|^2\right]\leq \left\{\sup_{k\ge 0}\E\left[\|\H(k)\|^2\right]\right\}\sum_{k=0}^{\infty}a^2(k)<\infty.
\ean
By Theorem \ref{wendingxing}, the sequence of solutions to the estimation error equation (\ref{error}) is $L_2$-asymptotically stable, i.e. $\lim_{k\to\infty}\E[\|f_i(k)-f_0\|^2]=0,~i\in \mathcal V$. \\
(II) If $\E[\|\H(k)\|^2|\F(k-1)]\leq \rho_0~\text{a.s.}$, on the one hand, noting that $\sup_{k\ge 0}\E[\|\H(k)\|^2]\leq \rho_0<\infty$, by above conclusion (I), we have $\lim_{k\to\infty}\E[\|e(k)\|^2]=0$. On the other hand, it follows from the condition (\ref{qafgs}) and (\ref{xfwee}) that
\ban
\sup_{k\ge 0}\E\left[\|(I_{\X^N}-F(k))G(k)\|^2\right]\leq \sup_{k\ge 0}\E\left[\|I_{\X^N}-F(k)\|^4+\|G(k)\|^4\right]<\infty.
\ean
Thus, by the estimation error equation (\ref{error}), Assumptions \ref{assumption1} and \ref{assumption2}, Lemma \ref{lemmaA1} and Proposition \ref{lemmaA6}, we get
\ban
&&~~~\E\left.\left[\|e(k+1)\|^2\right|\F(k-1)\right]\cr
&&=\E\left.\left[\|\left(I_{\X^N}-F(k)\right)e(k)+G(k)v(k)\|^2\right|\F(k-1)\right]\cr
&&=\E\left.\left[\|(I_{\X^N}-F(k))e(k)\|^2\right|\F(k-1)\right]+\E\left.\left[\|G(k)v(k)\|^2\right|\F(k-1)\right]\cr
&&~~~+2\E\left.\left[\langle e(k),(I_{\X^N}-F(k))G(k)v(k)\rangle \right|\F(k-1)\right]\cr
&&=\E\left.\left[\|(I_{\X^N}-F(k))e(k)\|^2\right|\F(k-1)\right]+\E\left.\left[\|G(k)v(k)\|^2\right|\F(k-1)\right]\cr
&&~~~+2\langle e(k),\E\left.\left[(I_{\X^N}-F(k))G(k)v(k)\right|\F(k-1)\right]\rangle\cr
&&=\E\left.\left[\|(I_{\X^N}-F(k))e(k)\|^2\right|\F(k-1)\right]+\E\left.\left[\|G(k)v(k)\|^2\right|\F(k-1)\right]\cr
&&~~~+2\langle e(k),\E\left.\left[(I_{\X^N}-F(k))G(k)\E[v(k)|\F(k-1)]\right|\F(k-1)\right]\rangle\cr
&&=\E\left.\left[\|(I_{\X^N}-F(k))e(k)\|^2\right|\F(k-1)\right]+\E\left.\left[\|G(k)v(k)\|^2\right|\F(k-1)\right]\cr
&&\leq \E\left.\left[\|(I_{\X^N}-F(k))\|^2\right|\F(k-1)\right]\|e(k)\|^2+\E\left.\left[\|G(k)v(k)\|^2\right|\F(k-1)\right]\cr
&&\leq \E\left.\left[\|(I_{\X^N}-F(k))\|^4\right|\F(k-1)\right]^{\frac{1}{2}}\|e(k)\|^2+\E\left.\left[\|G(k)v(k)\|^2\right|\F(k-1)\right]\cr
&&\leq \left(1+\gamma(k)\right)^{\frac{1}{2}}\|e(k)\|^2+\b_v\E\left.\left[\|G(k)\|^2\right|\F(k-1)\right]\cr
&&\leq \left(1+\frac{1}{2}\gamma(k)\right)\|e(k)\|^2+\rho_0\b_va^2(k)~\text{a.s.}
\ean
Noting that $\sum_{k=0}^{\infty}\gamma(k)<\infty$ and $\sum_{k=0}^{\infty}a^2(k)<\infty$, it follows from Lemma \ref{lemmaA3} that $\|e(k)\|^2$ is almost surely strongly consistent, which together with $\lim_{k\to\infty}\E[\|e(k)\|^2]=0$ gives $\lim_{k\to\infty}e(k)=0~\text{a.s.}$
\end{proof}


%\textbf{\emph{Proof of Lemma \ref{yibanxingdejieguo}:}}
\begin{proof}[Proof of Lemma \ref{yibanxingdejieguo}]
Given the $L_2$-bounded adaptive sequence $\{x(k),\F(kh-1),k\ge 0\}$ with values in Hilbert space $\X^N$ and the nonnegative integer $m$, we can define a new sequence $\{u(k),k\ge 0\}$ as follows,
\bna\label{diedaishi}
u(k+1)=\Phi_P((k+1)h-1,kh)u(k),~k\ge m,
\ena
where $u(m)=x(m),u(i)=0,i=0,\cdots,m-1$. It follows from Proposition \ref{nlllwwieiie}.(a)-(c) that $\{u(k),k\ge 0\}$ is the random sequence with values in Hilbert space $(\X^N,\tau_{\text{N}}(\X^N))$. On the one hand, from (\ref{diedaishi}), by iterative calculations, we get
\bna\label{wffe}
u(k+1)&=&\left(\prod_{i=m}^k\Phi_P((i+1)h-1,ih)\right)u(m)\cr
&=&\Phi_P((k+1)h-1,mh)x(m),~k\ge m.
\ena
Noting that $x(m)\in L^0(\Omega,\F(mh-1);\X^N)$, it is known from Lemma \ref{lemma1} that
\ban
\E\left[\|u(k+1)\|^2\right]=\E\left.\left[\E\left[\|\Phi_P((k+1)h-1,mh)x(m)\|^2\right|\F(mh-1)\right]\right]\leq d_1\E\left[\|x(m)\|^2\right],
\ean
thus, $\sup_{k\ge 0}\E[\|x(k)\|^2]<\infty$ implies $\sup_{k\ge 0}\E[\|u(k)\|^2]<\infty$. On the other hand, we can rewrite (\ref{diedaishi}) as
\ban
&&u(i+1)=\left(I_{\X^N}-\sum_{s=ih}^{(i+1)h-1}(a(s)\H^*(s)\H(s)+b(s)\L_{\G}\otimes I_{\X})\right)u(i)\cr
&&+\left(\Phi_P((i+1)h-1,ih)-\left(I_{\X^N}-\sum_{s=ih}^{(i+1)h-1}\left(a(s)\H^*(s)\H(s)+b(s)\L_{\G}\otimes I_{\X}\right)\right)\right)u(i),
\ean
which leads to
\bna\label{fwwii}
u(k+1)&=&\left(\prod_{i=m}^k\left(I_{\X^N}-\sum_{s=ih}^{(i+1)h-1}(a(s)\H^*(s)\H(s)+b(s)\L_{\G}\otimes I_{\X})\right)\right)x(m)\cr
&&+\sum_{i=m}^k\left(\prod_{j=i+1}^k\left(I_{\X^N}-\sum_{s=jh}^{(j+1)h-1}(a(s)\H^*(s)\H(s)+b(s)\L_{\G}\otimes I_{\X})\right)\right)\cr
&&\times \Bigg(\Phi_P((i+1)h-1,ih)-\Bigg(I_{\X^N}-\sum_{s=ih}^{(i+1)h-1}(a(s)\H^*(s)\H(s)\cr
&&~~~~~~+b(s)\L_{\G}\otimes I_{\X})\Bigg)\Bigg)u(i).
\ena
Denote the $s$-th order term in the binomial expansion of $\Phi_P((i+1)h-1,ih)$ by $M_s(i)$, $s=2,\cdots,h$. By (\ref{wffe})-(\ref{fwwii}), we have
\ban
&&~~~~\Phi_P((k+1)h-1,mh)x(m)\cr
&&=\left(\prod_{i=m}^k\left(I_{\X^N}-\sum_{s=ih}^{(i+1)h-1}(a(s)\H^*(s)\H(s)+b(s)\L_{\G}\otimes I_{\X})\right)\right)x(m)\cr
&&+\sum_{i=m}^k\left(\prod_{j=i+1}^k\left(I_{\X^N}-\sum_{s=jh}^{(j+1)h-1}(a(s)\H^*(s)\H(s)+b(s)\L_{\G}\otimes I_{\X})\right)\right)\left(\sum_{s=2}^hM_s(i)\right)u(i),
\ean
from which we get
\bna\label{ikddw}
&&~~~~\E\left[\left\|\Phi_P((k+1)h-1,mh)x(m)\right\|^2\right]\cr
&&\leq 2\E\left[\left\|\left(\prod_{i=m}^k\left(I_{\X^N}-\sum_{s=ih}^{(i+1)h-1}(a(s)\H^*(s)\H(s)+b(s)\L_{\G}\otimes I_{\X})\right)\right)x(m)\right\|^2\right]\cr &&~~~~+2\E\left[\left\|\sum_{i=m}^k\left(\prod_{j=i+1}^k\left(I_{\X^N}-\sum_{s=jh}^{(j+1)h-1}(a(s)\H^*(s)\H(s)+b(s)\L_{\G}\otimes I_{\X})\right)\right)\right.\right.\cr &&~~~~~~~~~~~~~~~~~~~~\times\left.\left.\left(\sum_{s=2}^hM_s(i)\right)u(i)\right\|^2\right].\cr
&&\,
\ena
Noting that $x(m)\in L^0(\Omega,\F(mh-1);\X^N)$ and $\{I_{\X^N}-\sum_{i=kh}^{(k+1)h-1}(a(i)\H^*(i)\H(i)+b(i)\L_{\G}\otimes I_{\X}),k\ge 0\}$ is $L_2^2$-stable with respect to the filter $\{\F((k+1)h-1),k\ge 0\}$, we obtain
\ban
\lim_{k\to\infty}\E\left[\left\|\left(\prod_{i=m}^k\left(I_{\X^N}-\sum_{s=ih}^{(i+1)h-1}(a(s)\H^*(s)\H(s)+b(s)\L_{\G}\otimes I_{\X})\right)\right)x(m)\right\|^2\right]=0.
\ean
Hereafter, we will analyze the second term on the right-hand side of the inequality in (\ref{ikddw}). Denote
$
D(s)=a(s)\H^*(s)\H(s)+b(s)(\L_{\G}\otimes I_{\X})$. On the one hand, for $2\leq r\leq 2^h,ih\leq s\leq (i+1)h-1$, by Cr-inequality and the conditional Lyapunov inequality, we have
\bna\label{jssk}
&&~~~~\E\left.\left[\|D(s)\|^r\right|\F(ih-1)\right]\cr
&&\leq \left(\E\left.\left[\|a(s)\H^*(s)\H(s)+b(s)(\L_{\G}\otimes I_{\X})\|^{2^h}\right|\F(ih-1)\right]\right)^{\frac{r}{2^h}}\cr
&&\leq \max\{a(s),b(s)\}^r\left(2^{2^h-1}\E\left.\left[\|\H^*(s)\H(s)\|^{2^h}\right|\F(ih-1)\right]\right.\cr
&&~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+\left. 2^{2^h-1}\|\L_{\G}\otimes I_{\X}\|^{2^h}\right)^{\frac{r}{2^h}}\cr
&&\leq \left(a^r(s)+b^r(s)\right)\left(\left(\E\left.\left[\|\H^*(s)\H(s)\|^{2^h}\right|\F(ih-1)\right]\right)^{\frac{r}{2^h}}+\|\L_{\G}\otimes I_{\X}\|^{r}\right)\cr
&&\leq (a^r(s)+b^r(s))(\rho_0^r+\|\L_{\G}\otimes I_{\X}\|^r)\cr
&&\leq (a(s)+b(s))^r(\rho_0+\|\L_{\G}\otimes I_{\X}\|)^r~\text{a.s.}.
\ena
Denote $\rho_1=\rho_0+\|\L_{\G}\otimes I_{\X}\|$. For $ih\leq n_1<\cdots< n_r\leq (i+1)h-1$, by the conditional H\"{o}lder inequality, the conditional Lyapunov inequality and (\ref{jssk}), we get
\bna\label{ijjw}
&&~~~~\E\left.\left[\left\|\prod_{j=1}^rD(n_j)\right\|^2\right|\F(ih-1)\right]\cr
&&\leq \left(\E\left.\left[\left\|\prod_{j=1}^{r-1}D(n_j)\right\|^4\right|\F(ih-1)\right]\right)^{\frac{1}{2}}\left(\E\left.\left[\left\|D(n_r)\right\|^4\right|\F(ih-1)\right]\right)^{\frac{1}{2}}\cr
&&\leq \rho_1^2(a(n_r)+b(n_r))^2\left(\E\left.\left[\left\|\prod_{j=1}^{r-1}D(n_j)\right\|^4\right|\F(ih-1)\right]\right)^{\frac{1}{2}}\cr
&&\leq \rho_1^{2r}\prod_{j=1}^r(a(n_j)+b(n_j))^2~\text{a.s.}
\ena
On the other hand, for $2\leq s\leq h$, it follows from Condition \ref{condition1} and (\ref{ijjw}) that
\bna\label{wwffw}
&&~~~~\E\left.\left[\|M_s(i)\|^2\right|\F(ih-1)\right]\cr
&&=\E\left.\left[\left\|\sum_{ih\leq n_1< \cdots< n_s\leq (i+1)h-1}\prod_{j=1}^sD(n_j)\right\|^2\right|\F(ih-1)\right]\cr
&&\leq \mathbb{C}_h^s\sum_{ih\leq n_1< \cdots< n_s\leq (i+1)h-1}\E\left.\left[\left\|\prod_{j=1}^sD(n_j)\right\|^2\right|\F(ih-1)\right]\cr
&&\leq \mathbb{C}_h^s\sum_{ih\leq n_1< \cdots< n_s\leq (i+1)h-1}\rho_1^{2s}\prod_{j=1}^s(a(n_j)+b(n_j))^2\cr
&&\leq \mathbb{C}_h^s\sum_{ih\leq n_1< \cdots< n_s\leq (i+1)h-1}\rho_1^{2s}\prod_{j=1}^s(a(i)+b(i))^2\cr
&&= \left(\mathbb{C}_h^s\right)^2\rho_1^{2s}(a(i)+b(i))^{2s}\cr
&&\leq \mathbb{C}_h^s\rho_1^{2s}(a(i)+b(i))^{2s}h!~\text{a.s.},
\ena
where the last inequality is obtained from $\mathbb{C}_h^s\leq h!$. Noting that $\{a(k),k\ge 0\}$ and $\{b(k),k\ge 0\}$ are both monotonically decreasing to $0$, it shows that there exists a constant $c_0>0$, such that $\sup_{k\ge 0}(a(k)+b(k))\leq c_0$. Therefore, by (\ref{wwffw}), we get
\bna\label{wwkks}
\sum_{s=2}^h\E\left.\left[\|M_s(i)\|^2\right|\F(ih-1)\right]&\leq& 4h!c_0^{-4}\left(a^2(i)+b^2(i)\right)^2\sum_{s=2}^h\mathbb{C}_h^s\rho_1^{2s}c_0^{2s}\cr &=&\rho_2\left(a^2(i)+b^2(i)\right)^2~\text{a.s.},
\ena
where $\rho_2=4h!c_0^{-2}((\rho_1^2c_0^2+1)^h-1-h\rho_1^2c_0^2)$. It follows from (\ref{wwkks}) that
\ban
&&~~~~\E\left[\left\|\left(\sum_{s=2}^hM_s(i)\right)u(i)\right\|^2\right]\cr
&&\leq h\E\left[\sum_{s=2}^h \E\left.\left[\|M_s(i)\|^2\right|\F(ih-1)\right]\|u(i)\|^2\right]\cr
&&\leq h\rho_2\left(a^2(i)+b^2(i)\right)^2\E\left[\|u(i)\|^2\right],
\ean
which together with $\sup_{i\ge 0}\E[\|u(i)\|^2]<\infty$ leads to the fact that
\bna\label{wwooo}
\sup_{i\ge 0}\E\left[\left\|R(i)\right\|^2\right]\leq h\rho_2\sup_{i\ge 0}\E\left[\|u(i)\|^2\right]<\infty,
\ena
where
\ban
R(i)=\frac{1}{a^2(i)+b^2(i)}\left(\sum_{s=2}^hM_s(i)\right)u(i),~i\ge m.
\ean
By the Minkowski inequality, we obtain
\bna\label{foow}
&&~~~~\E\left[\left\|\sum_{i=m}^k\left(\prod_{j=i+1}^k\left(I_{\X^N}-\sum_{s=jh}^{(j+1)h-1}D(s)\right)\right)\left(\sum_{s=2}^hM_s(i)\right)u(i)\right\|^2\right]\cr
&&=\E\left[\left\|\sum_{i=m}^k\left(a^2(i)+b^2(i)\right)\left(\prod_{j=i+1}^k\left(I_{\X^N}-\sum_{s=jh}^{(j+1)h-1}D(s)\right)\right)R(i)\right\|^2\right]\cr
&&\leq \Bigg(\sum_{i=m}^k\left(a^2(i)+b^2(i)\right)\Bigg(\E\Bigg[\Bigg\|\Bigg(\prod_{j=i+1}^k\Bigg(I_{\X^N}-\sum_{s=jh}^{(j+1)h-1}D(s)\Bigg)\Bigg)\cr &&~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\times R(i)\Bigg\|^2\Bigg]\Bigg)^{\frac{1}{2}}\Bigg)^2.
\ena
From Proposition \ref{wenknknkn}, we know that $R(i)\in L^0(\Omega,\F((i+1)h-1);\X^N)$, thus, by (\ref{wwooo}) and Lemma \ref{hhhlemma}, there exists a constant $d_3>0$ such that
\bna\label{oxcnv}
&&~~~~\sup_{k\ge 0\atop i\ge 0 }\E\left[\left\|\left(\prod_{j=i+1}^k\left(I_{\X^N}-\sum_{s=jh}^{(j+1)h-1}D(s)\right)\right)R(i)\right\|^2\right]\cr
&&\leq d_3\sup_{i\ge 0}\E\left[\|R(i)\|^2\right]\cr
&&<\infty,
\ena
which together with the fact that the operator-valued random sequence $$\left\{I_{\X^N}-\sum_{i=kh}^{(k+1)h-1}(a(i)\H^*(i)\H(i)+b(i)\L_{\G}\otimes I_{\X}),k\ge 0\right\}$$ is $L_2^2$-stable with respect to the filter $\{\F((k+1)h-1),k\ge 0\}$ gives
\bna\label{xawq}
\lim_{k\to \infty}\E\left[\left\|\left(\prod_{j=i+1}^k\left(I_{\X^N}-\sum_{s=jh}^{(j+1)h-1}D(s)\right)\right)R(i)\right\|^2\right]=0,~\forall i\ge 0.
\ena
By Condition \ref{condition2}, (\ref{oxcnv})-(\ref{xawq}) and Lemma \ref{lemma6}, we have
\ban
\lim_{k\to\infty}\sum_{i=m}^k\left(a^2(i)+b^2(i)\right)\left(\E\left[\left\|\left(\prod_{j=i+1}^k\left(I_{\X^N}-\sum_{s=jh}^{(j+1)h-1}D(s)\right)\right)R(i)\right\|^2\right]\right)^{\frac{1}{2}}=0.
\ean
Given the $L_2$-bounded adaptive sequence $\{x(k),\F(kh-1),k\ge 0\}$ with values in Hilbert space $\X^N$, from (\ref{ikddw}) and (\ref{foow}), we know that
\bna\label{rscsc}
\lim_{k\to \infty}\E\left[\left\|\Phi_P((k+1)h-1,mh)x(m)\right\|^2\right]=0,~\forall m\ge 0.
\ena
For $j\in \mathbb N$, denote $m_j=\lfloor \frac{j}{h} \rfloor,\widetilde{m}_j=\lceil \frac{j}{h} \rceil$. Let $\{y(k),\F(k),k\ge 0\}$ be the $L_2$-bounded adaptive sequence with values in Hilbert space $\X^N$, for $0\leq i<k-3h$, noting that $0\leq k-m_kh<h$, from Propositions \ref{nlllwwieiie}-\ref{wenknknkn}, it is known that $\Phi_P(m_kh-1,\widetilde{m}_{i+1}h)\Phi_P(\widetilde{m}_{i+1}h-1,i+1)y(i)\in L^0(\Omega,\F(m_kh-1);\X^N)$, by Lemma \ref{lemma1}, there exists a constant $d_2>0$ such that
\bna\label{jjkkl}
&&~~~~\mathbb E\left[\|\Phi_P(k,i+1)y(i)\|^2\right]\cr
&&=\mathbb E\left[\|\Phi_P(k,m_kh)\Phi_P(m_kh-1,\widetilde{m}_{i+1}h)\Phi_P(\widetilde{m}_{i+1}h-1,i+1)y(i)\|^2\right]\cr
&&=\mathbb E\Big[\mathbb E\Big[\Big\|\Phi_P(k,m_kh)\Phi_P(m_kh-1,\widetilde{m}_{i+1}h)\cr
&&~~~~~~~~~~~~~\times \Phi_P(\widetilde{m}_{i+1}h-1,i+1)y(i)\Big\|^2\Big|\mathcal F(m_kh-1)\Big]\Big]\cr
&&\leq d_2\mathbb E\left[\|\Phi_P(m_kh-1,\widetilde{m}_{i+1}h)\Phi_P(\widetilde{m}_{i+1}h-1,i+1)y(i)\|^2\right],~0\leq i<k-3h.
\ena
Noting that $0\leq \widetilde{m}_{i+1}h-(i+1)<h$ and $y(i)\in \F(i)$, it follows from Lemma \ref{lemma1} that
\bna\label{cllll}
&&~~~\sup_{i\ge 0}\mathbb E\left[\|\Phi_P(\widetilde{m}_{i+1}h-1,i+1)y(i)\|^2\right]\cr
&&=\sup_{i\ge 0}\E\left[\mathbb E\left.\left[\|\Phi_P(\widetilde{m}_{i+1}h-1,i+1)y(i)\|^2\right|\mathcal F(i)\right]\right]\cr
&&\leq d_2\sup_{i\ge 0}\E\left[\|y(i)\|^2\right]\cr
&&<\infty.
\ena
By Propositions \ref{nlllwwieiie}-\ref{wenknknkn}, we get $\Phi_P(\widetilde{m}_{i+1}h-1,i+1)y(i)\in L^0(\Omega,\mathcal F(\widetilde{m}_{i+1}h-1);\X^N)$. Substituting (\ref{rscsc}) and (\ref{cllll}) into (\ref{jjkkl}) leads to the fact that
\ban
\lim_{k\to\infty}\mathbb E\left[\|\Phi_P(k,i+1)y(i)\|^2\right]=0,~\forall i\ge 0,
\ean
which shows that the operator-valued random sequence $\{I_{\X^N}-a(k)\H^*(k)\H(k)-b(k)\L_{\G}\otimes I_{\X},k\ge 0\}$ is $L_2^2$-stable with respect to the filter $\{\F(k),k\ge 0\}$.
\end{proof}


%\textbf{\emph{Proof of Lemma \ref{jihubiranshoulian}:}}
\begin{proof}[Proof of Lemma \ref{jihubiranshoulian}]
It follows from Condition \ref{condition3} that there exists a constant $C_1>0$, such that $|b(k)-a(k)|\leq C_1(a^2(k)+b^2(k))$, which gives
\bna\label{xmkmslff}
\sum_{i=0}^ka(k)&\leq& \sum_{i=0}^k(|a(k)-b(k)|+b(k))\cr
&\leq& C_1\sum_{i=0}^k\left(a^2(k)+b^2(k)\right)+\sum_{i=0}^kb(k),~k\ge 0.
\ena
Given the integer $h>0$, denote
\bna\label{xmlwf}
c(k)=\sum_{s=kh}^{(k+1)h-1}b(s),
\ena
by (\ref{xmkmslff}) and Condition \ref{condition2}, we get
\ban
\sum_{k=0}^{\infty}c(k)=\sum_{k=0}^{\infty}\sum_{s=kh}^{(k+1)h-1}b(s)=\sum_{k=0}^{\infty}b(k)=\infty.
\ean
Denote
\bna\label{mvnjwe}
\HH=\text{diag}\left\{\frac{1}{h}\HH_1,\cdots,\frac{1}{h}\HH_N\right\}+\L_{\G}\otimes I_{\X}.
\ena
Noting that $\G$ is undirected, by the condition (\ref{yinlitiaojian1}) and Lemma \ref{lemmaA10}, we know that $\HH\in \mathscr L(\X^N)$ is the strictly positive self-adjoint operator. Let $\{x(k),\F(kh-1),k\ge 0\}$ be the $L_2$-bounded adaptive sequence with values in Hilbert space $\X^N$, then we can write  $x(k)=(x_1(k),\cdots,x_N(k))$, where $x_i(k):\Omega\to \X$, $i=1,\cdots,N$ are the random elements with values in Hilbert space $(\X,\tau_{\text{N}}(\X))$. Denote
\bna\label{cnlweee}
\mu(i)&=&c(i)\HH x(i)-\sum_{s=ih}^{(i+1)h-1}(a(s)\E[\H^*(s)\H(s)x(i)|\F(ih-1)]\cr &&+b(s)(\L_{\G}\otimes I_{\X})x(i)).
\ena
By (\ref{xmlwf})-(\ref{cnlweee}), we get
\ban
&&~~~~\mu(i)\cr
&&=\sum_{s=ih}^{(i+1)h-1}b(s)\text{diag}\left\{\frac{1}{h}\HH_1,\cdots,\frac{1}{h}\HH_N\right\}x(i)-\sum_{s=ih}^{(i+1)h-1}a(s)\E[\H^*(s)\H(s)x(i)|\F(ih-1)]\cr
&&=\text{diag}\left\{\frac{1}{h}\sum_{s=ih}^{(i+1)h-1}b(s)\HH_1x_1(i)-\sum_{s=ih}^{(i+1)h-1}a(s)\E[H_1^*(s)H_1(s)x_1(i)|\F(ih-1)],\right.\cr
&&~~~~~~\left.\cdots,\frac{1}{h}\sum_{s=ih}^{(i+1)h-1}b(s)\HH_Nx_N(i)-\sum_{s=ih}^{(i+1)h-1}a(s)\E[H_N^*(s)H_N(s)x_N(i)|\F(ih-1)]\right\}.
\ean
It follows from Lemma \ref{lemmaA11} that there exists a constant $C_2>0$ such that
\bna\label{vklwmlmfm}
\max_{ih\leq s\leq (i+1)h-1}\left(\frac{1}{h}\left(\sum_{s=ih}^{(i+1)h-1}b(s)\right)-a(s)\right)^2\leq C_2\left(a^4(i)+b^4(i)\right).
\ena
Therefore, by Conditions \ref{condition1}-\ref{condition3}, the condition (\ref{yinlitiaojian2}) and (\ref{vklwmlmfm}), we have
\bna\label{yuzhouwudichang}
&&~~~~\E\left[\|\mu(i)\|^2\right]\cr
&&=\sum_{j=1}^N\E\Bigg[\Bigg\|\frac{1}{h}\sum_{s=ih}^{(i+1)h-1}b(s)\HH_jx_j(i)-\sum_{s=ih}^{(i+1)h-1}a(s)\E[H_j^*(s)H_j(s)x_j(i)|\F(ih-1)]\Bigg\|^2\Bigg]\cr
&&=\sum_{j=1}^N\E\Bigg[\Bigg\|\Bigg(\frac{1}{h}\sum_{s=ih}^{(i+1)h-1}b(s)\Bigg)\Bigg(\HH_j x_j(i)-\sum_{s=ih}^{(i+1)h-1}\E\left[H_j^*(s)H_j(s)x_j(i)|\F(ih-1)\right]\Bigg)\cr &&~~+\sum_{s=ih}^{(i+1)h-1}\Bigg(\frac{1}{h}\Bigg(\sum_{s=ih}^{(i+1)h-1}b(s)\Bigg)-a(s)\Bigg)\E\left[H_j^*(s)H_j(s)x_j(i)|\F(ih-1)\right]\Bigg\|^2\Bigg]\cr
%&&\leq \sum_{j=1}^N\left(2\E\left[\left\|\left(\frac{1}{h}\sum_{s=ih}^{(i+1)h-1}b(s)\right)\left(H_jx_j(i)-\sum_{s=ih}^{(i+1)h-1}\E\left.\left[H_j^*(s)H_j(s)x_j(i)\right|\F(ih-1)\right]\right)\right\|^2\right]\right.\cr &&~~+\left.2\E\left[\left\|\sum_{s=ih}^{(i+1)h-1}\left(\frac{1}{h}\left(\sum_{s=ih}^{(i+1)h-1}b(s)\right)-a(s)\right)\E\left.\left[H_j^*(s)H_j(s)x_j(i)\right|\F(ih-1)\right]\right\|^2\right]\right)\cr
&&\leq \sum_{j=1}^N\Bigg(2b^2(i)\E\Bigg[\Bigg\|\HH_jx_j(i)-\sum_{s=ih}^{(i+1)h-1}\E\left.\left[H_j^*(s)H_j(s)x_j(i)\right|\F(ih-1)\right]\Bigg\|^2\Bigg]\cr
&&~~~~~~~~~~~~~~~+2h\E\Bigg[\sum_{s=ih}^{(i+1)h-1}\Bigg(\frac{1}{h}\Bigg(\sum_{s=ih}^{(i+1)h-1}b(s)\Bigg)-a(s)\Bigg)^2\cr
&&~~~~~~~~~~~~~~~~~~~~~~~~~~~~\times\left\|\E\left[H_j^*(s)H_j(s)x_j(i)|\F(ih-1)\right]\right\|^2\Bigg]\Bigg)\cr
&&\leq \sum_{j=1}^N\left(2b^2(i)\E\left[\left\|\HH_jx_j(i)-\sum_{s=ih}^{(i+1)h-1}\E\left.\left[H_j^*(s)H_j(s)x_j(i)\right|\F(ih-1)\right]\right\|^2\right]\right.\cr
&&~~~~~~~~~~~~~~~+2h\E\Bigg[\sum_{s=ih}^{(i+1)h-1}\Bigg(\frac{1}{h}\Bigg(\sum_{s=ih}^{(i+1)h-1}b(s)\Bigg)-a(s)\Bigg)^2\cr &&~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\times\E\left[\left\|H_j^*(s)H_j(s)\right\|^2|\F(ih-1)\right]\left\|x_j(i)\right\|^2\Bigg]\Bigg)\cr
&&\leq \sum_{j=1}^N\left(2b^2(i)\E\left[\left\|\HH_jx_j(i)-\sum_{s=ih}^{(i+1)h-1}\E\left.\left[H_j^*(s)H_j(s)x_j(i)\right|\F(ih-1)\right]\right\|^2\right]\right.\cr
&&~~~+\left.2h\rho^2_0\sup_{k\ge 0}\E\left[\|x(k)\|^2\right]\sum_{s=ih}^{(i+1)h-1}\left(\frac{1}{h}\left(\sum_{s=ih}^{(i+1)h-1}b(s)\right)-a(s)\right)^2\right)\cr
&&\leq \sum_{j=1}^N\left(2b^2(i)\E\left[\left\|\HH_jx_j(i)-\sum_{s=ih}^{(i+1)h-1}\E\left.\left[H_j^*(s)H_j(s)x_j(i)\right|\F(ih-1)\right]\right\|^2\right]\right.\cr
&&~~~+\left.2h^2\rho^2_0\sup_{k\ge 0}\E\left[\|x(k)\|^2\right]\max_{ih\leq s\leq (i+1)h-1}\left(\frac{1}{h}\left(\sum_{s=ih}^{(i+1)h-1}b(s)\right)-a(s)\right)^2\right)\cr
&&\leq \sum_{j=1}^N2b^2(i)\E\left[\left\|\HH_jx_j(i)-\sum_{s=ih}^{(i+1)h-1}\E\left.\left[H_j^*(s)H_j(s)x_j(i)\right|\F(ih-1)\right]\right\|^2\right]\cr
&&~~~~~~~+2C_2Nh^2\rho^2_0\sup_{k\ge 0}\E\left[\|x(k)\|^2\right]\left(a^4(i)+b^4(i)\right).
\ena
Noting that $\{x_j(k),\F(kh-1),k\ge 0\},j=1,\cdots,N$ are the $L_2$-bounded adaptive sequences with values in Hilbert space $\X$, by (\ref{yinlitiaojian1}), we obtain
\bna\label{vnkwjofjeofe}
&&\sum_{i=0}^{\infty}\E\left[\left\|\HH_jx_j(i)-\sum_{s=ih}^{(i+1)h-1}\E\left.\left[H_j^*(s)H_j(s)x_j(i)\right|\F(ih-1)\right]\right\|^2\right]\cr
&&<\infty,~j=1,\cdots,N.
\ena
By Condition \ref{condition2}, (\ref{yuzhouwudichang})-(\ref{vnkwjofjeofe}) and Cauchy inequality, we get
\ban
&&~~~~\sum_{i=0}^{\infty}\E\left[\|\mu(i)\|^2\right]^{\frac{1}{2}}\cr
&&\leq \sqrt{2}\sum_{i=0}^{\infty}b(i)\sum_{j=1}^N\left(\E\left[\left\|\HH_jx_j(i)-\sum_{s=ih}^{(i+1)h-1}\E\left.\left[H_j^*(s)H_j(s)x_j(i)\right|\F(ih-1)\right]\right\|^2\right]\right)^{\frac{1}{2}}\cr
&&~~~~~~~+h\rho_0\sup_{k\ge 0}\E\left[\|x(k)\|^2\right]^{\frac{1}{2}}\sqrt{2C_2N}\sum_{i=0}^{\infty}\left(a^2(i)+b^2(i)\right)\cr
%&&\leq \sqrt{2}C_3\left(\sum_{i=0}^{\infty}\left(\sum_{j=1}^N\left(\E\left[\left\|H_jx_j(i)-\sum_{s=ih}^{(i+1)h-1}\E\left.\left[H_j^*(s)H_j(s)x_j(i)\right|\F(ih-1)\right]\right\|^2\right]\right)^{\frac{1}{2}}\right)^2\right)\cr
%&&+h\rho_0\sup_{k\ge 0}\E\left[\|x(k)\|^2\right]^{\frac{1}{2}}\sqrt{2C_2N}\sum_{i=0}^{\infty}\left(a^2(i)+b^2(i)\right)\cr
&&\leq \sqrt{2}NC_3\left(\sum_{i=0}^{\infty}\sum_{j=1}^N\E\left[\left\|\HH_jx_j(i)-\sum_{s=ih}^{(i+1)h-1}\E\left.\left[H_j^*(s)H_j(s)x_j(i)\right|\F(ih-1)\right]\right\|^2\right]\right)\cr
&&~~~~~~~+h\rho_0\sup_{k\ge 0}\E\left[\|x(k)\|^2\right]^{\frac{1}{2}}\sqrt{2C_2N}\sum_{i=0}^{\infty}\left(a^2(i)+b^2(i)\right)\cr
&&= \sqrt{2}NC_3\left(\sum_{j=1}^N\sum_{i=0}^{\infty}\E\left[\left\|\HH_jx_j(i)-\sum_{s=ih}^{(i+1)h-1}\E\left.\left[H_j^*(s)H_j(s)x_j(i)\right|\F(ih-1)\right]\right\|^2\right]\right)\cr
&&~~~~~~~+h\rho_0\sup_{k\ge 0}\E\left[\|x(k)\|^2\right]^{\frac{1}{2}}\sqrt{2C_2N}\sum_{i=0}^{\infty}\left(a^2(i)+b^2(i)\right)\cr
&&<\infty,
\ean
where $C_3=\sum_{i=0}^{\infty}b^2(i)$. For any given integer $m>0$, denote $\Gamma_{m}=\{i\ge m:\E[\|\mu(i)\|^2]>0,i\in \mathbb N\}$. Noting that $\E[\|\mu(i)\|^2]=0$ implies that $\mu(i)=0~\text{a.s.}$, we obtain
\bna\label{final2}
&&~~~~\sum_{i=m}^{k}\left(\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)\mu(i)\right\|^2\right]\right)^{\frac{1}{2}}\cr
&&\leq \sum_{i=m}^{\infty}\left(\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)\mu(i)\right\|^2\right]\right)^{\frac{1}{2}}\cr
&&=\sum_{i\in\Gamma_m}\left(\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)\mu(i)\right\|^2\right]\right)^{\frac{1}{2}}\cr
&&=\sum_{i\in\Gamma_m}\E\left[\|\mu(i)\|^2\right]^{\frac{1}{2}}\left(\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)\eta(i)\right\|^2\right]\right)^{\frac{1}{2}},
\ena
where $\eta(i)=\mu(i)\E[\|\mu(i)\|^2]^{-\frac{1}{2}},i\in \Gamma_m$. Noting that $\E[\|\eta(i)\|^2]=1$, it follows from Lemma \ref{lemmaA7} that there exist constants $M,d>0$, such that
\bna\label{final3}
&&~~~~\sup_{k\ge 0\atop i\in \Gamma_m}\left(\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)\eta(i)\right\|^2\right]\right)^{\frac{1}{2}}\cr
&&\leq M^d\sup_{i\in \Gamma_m}\E\left[\|\eta(i)\|^2\right]^{\frac{1}{2}}\cr
&&=Md.
\ena
By Lemma \ref{lemmaA7} and Lebesgue dominated convergence theorem, we get
\bna\label{final4}
\lim_{k\to \infty}\left(\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)\eta(i)\right\|^2\right]\right)^{\frac{1}{2}}=0,~\forall i\ge 0.
\ena
Therefore, combining (\ref{final2})-(\ref{final4}) and Lemma \ref{lemma6} leads to
\ban
\lim_{k\to\infty}\sum_{i=m}^{k}\left(\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)\mu(i)\right\|^2\right]\right)^{\frac{1}{2}}=0,~\forall m\ge 0.
\ean
It follows from Lemma \ref{henandelemma} that the operator-valued random sequence $$ \left\{I_{\X^N}-\sum_{i=kh}^{(k+1)h-1}(a(i)\H^*(i)\H(i)+b(i)\L_{\G}\otimes I_{\X}),k\ge 0\right\}$$ is $L_2^2$-stable with respect to the filter $\{\F((k+1)h-1),k\ge 0\}$. Hencee, from Lemma \ref{yibanxingdejieguo}, it is known that the operator-valued random sequence $\{I_{\X^N}-a(k)\H^*(k)\H(k)-b(k)\L_{\G}\otimes I_{\X},k\ge 0\}$ is $L_2^2$-stable with respect to the filter $\{\F(k),k\ge 0\}$.
\end{proof}


%\textbf{\emph{Proof of Theorem \ref{vnknoklfl}:}}
\begin{proof}[Proof of Theorem \ref{vnknoklfl}]
By the conditions (\ref{yinlitiaojian1})-(\ref{yinlitiaojian2}) and Lemma \ref{jihubiranshoulian}, it is known that $\{I_{\X^N}-a(k)\H^*(k)\H(k)-b(k)\L_{\G}\otimes I_{\X},k\ge 0\}$ is $L_2^2$-stable with respect to the filter $\{\F(k),k\ge 0\}$. Denote $D(k)=a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\X}$. By the condition (\ref{yinlitiaojian2}) that
\bna\label{cmllemfnn}
&&~~~\E\left.\left[\|D(k)\|^r\right|\F(k-1)\right]\cr
&&\leq \left(\E\left.\left[\|a(k)\H^*(k)\H(k)+b(k)(\L_{\G}\otimes I_{\X})\|^{2^h}\right|\F(k-1)\right]\right)^{\frac{r}{2^h}}\cr
&&\leq \max\{a(k),b(k)\}^r\Big(2^{2^h-1}\E\left.\left[\|\H^*(k)\H(k)\|^{2^h}\right|\F(k-1)\right]\cr
&&~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+2^{2^h-1}\|\L_{\G}\otimes I_{\X}\|^{2^h}\Big)^{\frac{r}{2^h}}\cr
&&\leq \left(a^r(k)+b^r(k)\right)\left(\left(\E\left.\left[\|\H^*(k)\H(k)\|^{2^h}\right|\F(k-1)\right]\right)^{\frac{r}{2^h}}+\|\L_{\G}\otimes I_{\X}\|^{r}\right)\cr
&&\leq (a^r(k)+b^r(k))(\rho_0^r+\|\L_{\G}\otimes I_{\X}\|^r)\cr
&&\leq (a^r(k)+b^r(k))\rho_1^r~\text{a.s.},~\forall 1\leq r\leq 4,
\ena
where $\rho_1=\rho_0+\|\L_{\G}\otimes I_{\X}\|$. By the condition (\ref{dinglitiaojian}) and (\ref{cmllemfnn}), we get
\bna
&&~~~\left.\E\left[\|I_{\X^N}-\left(a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\X}\right)\|^4\right|\F(k-1)\right]\cr
&&=\left.\E\left[\left\|\left(I_{\X^N}-\left(a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\X}\right)\right)^4\right\|\right|\F(k-1)\right]\cr
&&=\left.\E\left[\left\|I_{\X^N}-4D(k)+6D^2(k)-4D^3(k)+D^4(k)\right\|\right|\F(k-1)\right]\cr
&&\leq \left.\E\left[\|I_{\X^N}-4D(k)\|+6\|D(k)\|^2+4\|D(k)\|^3+\|D(k)\|^4\right|\F(k-1)\right]\cr
&&\leq 1+\gamma(k)~\text{a.s.},
\ena
where $\gamma(k)=\Gamma(k)+6(a^2(k)+b^2(k))\rho_1^2+4(a^3(k)+b^3(k))\rho_1^3+(a^4(k)+b^4(k))\rho_1^4$. From Condition \ref{condition2} and the condition (\ref{dinglitiaojian}), we know that $\sum_{k=0}^{\infty}\gamma(k)<\infty$, which together with Lemma \ref{dingliyi1} gives the fact that algorithm (\ref{algorithm}) is both mean square and almost surely strongly consistent.
\end{proof}


%\textbf{\emph{Proof of Corollary \ref{xiaosirendetuilun}:}}
\begin{proof}[Proof of Corollary \ref{xiaosirendetuilun}]
It follows from $\|\H(0)\|\leq \rho_0~\text{a.s.}$ and Proposition \ref{nlllwwieiie}.(a) that $H^*_j(0)H_j(0)x$ $\in L^1(\Omega;\X)$, $x\in \X$. Given the integer $h>0$ and $j\in \mathcal V$, we define the operator $\HH_j:\X\to\X$ by
\bna\label{wpkfpkpew}
\HH_j(x)=h\E\left[H^*_j(0)H_j(0)x\right],~x\in \X,~j\in \mathcal V.
\ena
For any given $x_1,x_2\in\X$ and $c_1,c_2\in \mathbb R$, we have
\bna\label{jcknvknw}
\HH_j(c_1x_1+c_2x_2)&=&c_1h\E\left[H^*_j(0)H_j(0)x_1\right]+c_2h\E\left[H^*_j(0)H_j(0)x_2\right]\cr
&=&c_1\HH_j(x_1)+c_2\HH_j(x_2).
\ena
Noting that $H^*_j(0)H_j(0)x\in L^1(\Omega;\X)$, by Lemma \ref{lemmaA1}, we get
\bna\label{jcknvknw1}
\left\langle \HH_j(x_1), x_2\right\rangle&=&h\E\left[\left\langle H^*_j(0)H_j(0)x_1,x_2 \right\rangle \right]\cr
&=&h\E\left[\left\langle x_1,H^*_j(0)H_j(0)x_2 \right\rangle \right]\cr
&=&\left\langle x_1, \HH_j(x_2)\right\rangle,~j\in \mathcal V.
\ena
From (\ref{jcknvknw})-(\ref{jcknvknw1}), it is known that $\HH_j$ is a linear self-adjoint operator, which gives
\ban
\|\HH_j(x)\|\leq h\E\left[\left\|H^*_j(0)H_j(0)\right\|\|x\|\right]\leq h\rho_0\|x\|,~\forall x\in \X,~j\in \mathcal V,
\ean
thus, the self-adjoint operator $\HH_j\in \mathscr L(\X)$. Denote $\HH_j(x):=\HH_jx$, $\forall x\in \X$. Noting that $H^*_j(0)H_j(0)x\in L^1(\Omega;\X)$, it follows from Lemma \ref{lemmaA1} that
\bna\label{owwwww2}
\left\langle \HH_jx,x\right\rangle&=&h\left\langle \E\left[H^*_j(0)H_j(0)x\right],x \right\rangle\cr
&=&h\E\left[\left\langle H^*_j(0)H_j(0)x,x \right\rangle \right]\cr
&=&h\E\left[\left\|H_j(0)x\right\|^2\right]\ge 0,
\ena
from which we know that the operator $\HH_j$ defined in (\ref{wpkfpkpew}) is positive bounded linear self-adjoint, $j\in \mathcal V$. Noting that $\{\H(k),k\ge 0\}$ is the i.i.d. sequence with values in the topology space $(\mathscr L(\X^N,\bigoplus_{i=1}^N\Y_i),\tau_{\text{S}}(\mathscr L(\X^N,\bigoplus_{i=1}^N\Y_i)))$, it follows from Definition \ref{dulixing} that $\{\H^*(k)\H(k)x,x\in\X^N,k\ge 0\}$ is the i.i.d. sequence with values in Banach space $(\X^N,\tau_{\text{N}}(\X^N))$. By Proposition E.1.10 in \cite{hy2}, we know that $\{\|\H^*(k)\H(k)\|,k\ge 0\}$ and $\{\|I_{\X^N}-(a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\X})\|,k\ge 0\}$ are both independent random sequence. By Condition \ref{condition2}, there exists an integer $s_0>0$, such that $a(k)+b(k)\leq (4\rho^2_0+4\|\L_{\G}\otimes I_{\X}\|)^{-1}$, $\forall k\ge s_0$. We now define the nonnegative real sequence $\{\Gamma(k),k\ge 0\}$ satisfying
\bna\label{owwwww0}
\Gamma(k)=\begin{cases}
4(a(k)+b(k))\left(\rho^2_0+\|\L_{\G}\otimes I_{\X}\|\right),& 0\leq k<s_0;\\
0, & k\ge s_0,
\end{cases}
\ena
which shows that $\sum_{k=0}^{\infty}\Gamma(k)<\infty$. Noting that $\|I_{\X^N}-4(a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\X})\|\leq 1+4(a(k)+b(k))(\rho^2_0+\|\L_{\G}\otimes I_{\X}\|)~\text{a.s.}$, $\forall k\ge 0$, we obtain
\bna\label{owwwww1}
&&~~\left.\E\left[\left\|I_{\X^N}-4\left(a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\X}\right)\right\|\right|\F(k-1)\right]\cr
&&=\E\left[\left\|I_{\X^N}-4\left(a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\X}\right)\right\|\right]\cr
&&=\E\left[\sup_{\|x\|=1}\left|\left\langle I_{\X^N}-4\left(a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\X}\right)x,x\right\rangle \right|\right]\cr
&&=\E\left[\sup_{\|x\|=1}\left|1-4\left\langle\left(a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\X}\right)x,x\right\rangle \right|\right]\cr
&&=\E\left[1-4\inf_{\|x\|=1}\left\langle\left(a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\X}\right)x,x\right\rangle \right]\cr
%&&=\E\left[\left\|I_{\X^N}-\left(a(k)\H^*(0)\H(0)+b(k)\L_{\G}\otimes I_{\X}\right)\right\|^4\right]\cr
&&\leq 1~\text{a.s.},~\forall k\ge s_0.
\ena
Then, by (\ref{owwwww0})-(\ref{owwwww1}), we get
\ban
\left.\E\left[\left\|I_{\X^N}-4\left(a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\X}\right)\right\|\right|\F(k-1)\right]\leq 1+\Gamma(k)~\text{a.s.},~\forall k\ge 0.
\ean
It can be verified that
\bna\label{vnkwenvefklw}
&&~~~\sup_{k\ge 0}\left(\E\left.\left[\|\H^*(k)\H(k)\|^{2^{\max\{h,2\}}}\right|\F(k-1)\right]\right)^{\frac{1}{2^{\max\{h,2\}}}}\cr
&&=\sup_{k\ge 0}\left(\E\left[\|\H^*(k)\H(k)\|^{2^{\max\{h,2\}}}\right]\right)^{\frac{1}{2^{\max\{h,2\}}}}\cr
&&=\sup_{k\ge 0}\left(\E\left[\|\H^*(0)\H(0)\|^{2^{\max\{h,2\}}}\right]\right)^{\frac{1}{2^{\max\{h,2\}}}}\cr
&&\leq \rho^2_0~\text{a.s.}
\ena
For any given integers $k\ge 0$, $h>0$ and $x\in \X$, if $A\in \F(kh-1)$, then it follows from Proposition \ref{nlllwwieiie} and (\ref{vnkwenvefklw}) that $H^*_j(i)H_j(i)(\1_A\otimes x)\in L^1(\Omega;\X)$, $i\ge kh$, which together with Lemma \ref{nvkvpeoeo} gives the fact that $\E[H^*_j(i)H_j(i)(\1_A\otimes x)|\F(kh-1)]$ uniquely exists. Since $\{\H(k),k\ge 0\}$ and $\{v(k),k\ge 0\}$ are i.i.d sequences and they are mutually independent, it follows from Definition \ref{dulixing} that $H^*_j(i)H_j(i)x$ is independent of $\1_{F\cap A}$ with $F\in \F(kh-1)$. Thus, we have
\ban
&&~~~\int_F\E\left.\left[\sum_{i=kh}^{(k+1)h-1}H^*_j(i)H_j(i)(\1_A\otimes x)\right|\F(kh-1)\right]\dd\P\cr
&&=\int_F\sum_{i=kh}^{(k+1)h-1}H^*_j(i)H_j(i)(\1_A\otimes x)\dd\P\cr
&&=\int_{\Omega}\left(\sum_{i=kh}^{(k+1)h-1}H^*_j(i)H_j(i)x\right)\1_{F\cap A}\dd\P\cr
&&=\int_{\Omega}\sum_{i=kh}^{(k+1)h-1}H^*_j(i)H_j(i)x\dd\P\int_{\Omega}\1_{F\cap A}\dd\P\cr
&&=\E\left[\sum_{i=kh}^{(k+1)h-1}H^*_j(i)H_j(i)x\right]\P(F\cap A)\cr
&&=h\E\left[H^*_j(0)H_j(0)x\right]\P(F\cap A)\cr
&&=h\int_{F}\E\left[H^*_j(0)H_j(0)x\right]\1_A\dd\P,~\forall F\in \F(kh-1),~j\in \mathcal V,
\ean
it gives
\ban
\E\left.\left[\sum_{i=kh}^{(k+1)h-1}H^*_j(i)H_j(i)(\1_A\otimes x)\right|\F(kh-1)\right]=\HH_j(\1_A\otimes x)~\text{a.s.},~j\in \mathcal V,
\ean
which together with the properties of the conditional expectation, the operator $\HH_j$ and the linearity of Bochner integral leads to
\bna\label{vnkwoejvnvvn}
\HH_jy=\E\left.\left[\sum_{i=kh}^{(k+1)h-1}H^*_j(i)H_j(i)y\right|\F(kh-1)\right]~\text{a.s.},~j\in \mathcal V,
\ena
where $y\in L^0(\Omega,\F(kh-1);\X)$ is the simple function. For $f\in L^2(\Omega,\F(kh-1);\X)$, by Pettis measurability theorem, we know that there exists a sequence of simple functions $\{f_n \in L^0(\Omega,\F(kh-1);\X),n\ge 0\}$ satisfying $\|f_n\|\leq f~\text{a.s.}$ and $\lim_{n\to \infty}f_n=f~\text{a.s.}$, which together with (\ref{vnkwenvefklw}) and Cauchy inequality gives $H^*_j(i)H_j(i)f\in L^1(\Omega;\X)$. Thus, from Lemma \ref{nvkvpeoeo}, it is known that $\E[H^*_j(i)H_j(i)f|\F(kh-1)]$ uniquely exists. Noting that $\HH_j\in \LL(\X)$, it follows from (\ref{vnkwenvefklw})-(\ref{vnkwoejvnvvn}) and the dominated convergence theorem of conditional expectation that
\bna\label{vnowlelkel}
&&~~~\HH_jf\cr
&&=\lim_{n\to\infty}\HH_jf_n\cr &&=\lim_{n\to\infty}\E\left.\left[\sum_{i=kh}^{(k+1)h-1}H^*_j(i)H_j(i)f_n\right|\F(kh-1)\right]\cr
&&=\E\left.\left[\sum_{i=kh}^{(k+1)h-1}H^*_j(i)H_j(i)\lim_{n\to\infty}f_n\right|\F(kh-1)\right]\cr
&&=\E\left.\left[\sum_{i=kh}^{(k+1)h-1}H^*_j(i)H_j(i)f\right|\F(kh-1)\right]~\text{a.s.},~j\in \mathcal V.
\ena
Let $\{x(k),\F(kh-1),k\ge 0\}$ be the $L_2$-bounded adaptive sequence with values in Hilbert space $\X$, by (\ref{vnowlelkel}), we get
\ban
\HH_jx(k)=\E\left.\left[\sum_{i=kh}^{(k+1)h-1}H^*_j(i)H_j(i)x(k)\right|\F(kh-1)\right]~\text{a.s.},~j\in\mathcal V.
\ean
For any non-zero element $x$ in Hilbert space $x$, from Lemma \ref{lemmaA1}, the condition (\ref{tuiluntiaojian}) and (\ref{wpkfpkpew}), we have
\bna\label{owwwww3}
\left\langle \sum_{j=1}^N\HH_jx,x\right\rangle=h\left\langle\E\left[\sum_{j=1}^NH^*_j(0)H_j(0)x\right],x\right\rangle=h\sum_{j=1}^N\E\left[\|H_j(0)x\|^2\right]>0.
\ena
Hence, by (\ref{owwwww2})-(\ref{vnkwenvefklw}), (\ref{owwwww3}) and Theorem \ref{vnknoklfl}, it is known that the algorithm (\ref{algorithm}) is both mean square and almost surely strongly consistent.
\end{proof}


%\textbf{\emph{Proof of Theorem \ref{rkhsdingli}:}}
\begin{proof}[Proof of Theorem \ref{rkhsdingli}]
Let $H_i(k)$ be the mapping induced by random input data $x_i(k)$, where
\ban
H_i(k)(f)=f(x_i(k)),~f\in \HH_K,~k\ge 0,~i\in \mathcal V.
\ean
Arbitrarily taking $f_1,f_2\in \HH_K$ and $c_1,c_2\in \mathbb R$, it follows from the reproducing property of $\HH_K$ that
\bna\label{vlwlmmfff}
H_i(k)(c_1f_1+c_2f_2)&=&\langle c_1f_1+c_2f_2,x_i(k)\rangle _K\cr &=&c_1H_i(k)(f_1)+c_2H_i(k)(f_2),~k\ge 0,~i\in \mathcal V,
\ena
thus, $H_i(k)$ is the linear operator. Noting the continuity of Mercer kernel $K:\X\times \X\to \mathbb R$, we know that the function $K_x:\X\to \HH_K$ induced by the Mercer kernel $K$ is also continuous. It is known from  $\HH_K=\overline{\textbf{span}\{K_x,x\in\X\}}$ that $f\in \HH_K$ is Borel measurable function, which gives that  $H_i(k)(f)=f(x_i(k))$ is the random variable with values in Hilbert space $(\mathbb R,\tau_{\text{N}}(\mathbb R))$. By Assumption \ref{assumption5} and the reproducing property of $\HH_K$, we have
\bna\label{vnklooeoeeee}
\|K_x\|_K=\sqrt{\langle K_x,K_x\rangle _K}=\sqrt{K(x,x)}\leq \sup_{x\in\X}\sqrt{K(x,x)}<\infty.
\ena
For any given sample path $\omega\in\Omega$, by (\ref{vnklooeoeeee}), we get
\bna\label{vnlwkfeemmff}
|H_i(k)(\omega)(f)|&=&\left|\left\langle f,K_{x_i(k)(\omega)}\right\rangle _K\right|\cr
&\leq& \sup_{x\in\X}\sqrt{K(x,x)}\|f\|_K,~\forall f\in \HH_K,~k\ge 0,~i\in \mathcal V,
\ena
then $\|H_i(k)\|_{\LL(\HH_K,\mathbb R)}\leq \sup_{x\in\X}\sqrt{K(x,x)}~\text{a.s.}$, by (\ref{vlwlmmfff}), (\ref{vnlwkfeemmff}) and Proposition \ref{nlllwwieiie}, we know that $H_i(k):\Omega\to\LL(\HH_K,\mathbb R)$ is the random element with values in the topological space $(\LL(\HH_K,\mathbb R),\tau_{\text{S}}(\LL(\HH_K,\mathbb R)))$. Denote $\H(k):=\text{diag}\{H_1(k),\cdots,H_N(k)\}$ and $v(k):=(v_1(k),\cdots,v_N(k))$, it follows from Definition \ref{tuopukongjian} that $\H(k)$ is the random element with values in the topological space $(\LL(\HH_K^N,\mathbb R^N),\tau_{\text{S}}(\LL(\HH_K^N,\mathbb R^N)))$, and $v(k)$ is the random vector with values in Hilbert space $(\mathbb R^N,\tau_{\text{N}}(\mathbb R^N))$. Thus, by Proposition \ref{nlllwwieiie}, we have
\bna\label{nvwjoijjff}
\F(k)=\bigvee_{s=0}^k\left(\sigma\left(\H(s);\tau_{\text{S}}\left(\LL\left(\HH_K^N,\mathbb R^N\right)\right)\right)\bigvee \sigma\left(v(s);\tau_{\text{N}}\left(\mathbb R^N\right)\right)\right),~k\ge 0.
\ena
It follows from Assumption \ref{assumption3} and (\ref{nvwjoijjff}) that Assumption \ref{assumption1} holds. By Assumption \ref{assumption4} and (\ref{nvwjoijjff}), it is known that $\{v(k),\F(k),k\ge 0\}$ is the martingale difference sequence and there exists a constant $\b_v:=N\b>0$, such that
\ban
\sup_{k\ge 0}\E\left.\left[\|v(k)\|^2\right|\F(k-1)\right]\leq N\max_{i\in \mathcal V}\sup_{k\ge 0}\E\left.\left[\|v_i(k)\|^2\right|\F(k-1)\right]\leq \b_v~\text{a.s.},
\ean
which shows that Assumption \ref{assumption2} holds.
Let $x\in \X$, for any given $f_1,f_2\in \HH_K$ and $c_1,c_2\in \mathbb R$, we get
\bna\label{vmkwmefkeml1}
\left(K_x\otimes K_x\right)(c_1f_1+c_2f_2)&=&c_1f_1(x)K_x+c_2f_2(x)K_x\cr
&=&c_1\left(K_x\otimes K_x\right)f_1+c_2\left(K_x\otimes K_x\right)f_2,
\ena
from (\ref{vnklooeoeeee}), Assumption \ref{assumption5} and the reproducing property of $\HH_K$, it is known that
\bna\label{vmkwmefkeml2}
~~\left\|\left(K_x\otimes K_x\right)f\right\|_{K}\leq \|K_x\|_K^2\|f\|_K\leq \sup_{x\in\X}K(x,x)\|f\|_K,~\forall f\in \HH_K,~\forall x\in \X.
\ena
Thus, it follows from (\ref{vmkwmefkeml1})-(\ref{vmkwmefkeml2}) that $K_x\otimes K_x\in \LL(\HH_K)$. Let $x=\sum_{i=1}^n\1_{A_i}\otimes x_i$ be the random vector with values in Hilbert space $(\X,\tau_{\text{N}}(\X))$, where $A_i\cap A_j=\emptyset$, $1\leq i\neq j\leq n$. Noting that $K_{x}=\sum_{i=1}^n\1_{A_i}\otimes K_{x_i}$, we have
\ban
K_{x}\otimes K_{x}=\left(\sum_{i=1}^n\1_{A_i}\otimes K_{x_i}\right)\otimes \left(\sum_{i=1}^n\1_{A_i}\otimes K_{x_i}\right)=\sum_{i=1}^n\1_{A_i}\otimes \left(K_{x_i}\otimes K_{x_i}\right),
\ean
thus, $K_{x}\otimes K_{x}$ is the simple function with values in Banach space $\LL(\HH_K)$. For any given random vector $x$ with values in Hilbert space $(\X,\tau_{\text{N}}(\X))$, there exists a simple function sequence $\{x_n,n\ge 0\}$ with values in $\X$, such that $\lim_{n\to\infty}\|x-x_n\|=0~\text{a.s.}$, which together with the reproducing property of $\HH_K$ and the symmetry of Mercer kernel $K$ gives
\bna\label{cnkwmekmk}
\left\|K_x-K_{x_n}\right\|^2_K&=&\left\langle K_x-K_{x_n},K_x-K_{x_n} \right\rangle_K\cr &=&K(x,x)-2K(x,x_n)+K(x_n,x_n).
\ena
Noting the continuity of Mercer kernel $K$ and Assumption \ref{assumption5}, by (\ref{cnkwmekmk}), we get
\bna\label{nncknkwnkwc1}
\lim_{n\to\infty}\left\|K_x-K_{x_n}\right\|_K=0~\text{a.s.}
\ena
It follows from (\ref{vnklooeoeeee}) and the reproducing property of $\HH_K$ that
\bna\label{nncknkwnkwc2}
&&~~~\left\|K_{x}\otimes K_{x}-K_{x_n}\otimes K_{x_n}\right\|_{\LL(\HH_K)}\cr
&&\leq \left\|(K_x-K_{x_n})\otimes K_x\right\|_{\LL(\HH_K)}+\left\|K_{x_n}\otimes (K_x-K_{x_n})\right\|_{\LL(\HH_K)}\cr
&&=\sup_{\|f\|_K=1}\left\|\left((K_x-K_{x_n})\otimes K_x\right)f\right\|_K+\sup_{\|f\|_K=1}\left\|\left(K_{x_n}\otimes (K_x-K_{x_n})\right)f\right\|_K\cr
&&=\sup_{\|f\|_K=1}\left\|f(x)(K_x-K_{x_n})\right\|_K+\sup_{\|f\|_K=1}\left\|(f(x)-f(x_n))K_{x_n}\right\|_K\cr
&&\leq \|K_x\|_K\|K_x-K_{x_n}\|_K+\|K_{x_n}\|_K\|K_x-K_{x_n}\|_K\cr
&&\leq 2\sup_{x\in \X}\sqrt{K(x,x)}\|K_x-K_{x_n}\|_K~\text{a.s.}
\ena
By Assumption \ref{assumption5} and (\ref{nncknkwnkwc1})-(\ref{nncknkwnkwc2}), we have
\ban
\lim_{n\to\infty}\left\|K_{x}\otimes K_{x}-K_{x_n}\otimes K_{x_n}\right\|_{\LL(\HH_K)}=0~\text{a.s.}
\ean
Noting that $K_{x_n}\otimes K_{x_n}$ is the simple function with values in Banach space $\LL(\HH_K)$, by Definition \ref{vnwkelel}, it is known that $K_{x}\otimes K_{x}$ is strongly measurable with respect to the topology $\tau_{\text{N}}(\LL(\HH_K))$, which together with Pettis measurability theorem shows that $K_{x}\otimes K_{x}$ is the random element with values in Banach space $(\LL(\HH_K),\tau_{\text{N}}(\LL(\HH_K)))$. Thus, by Assumption \ref{assumption5}, we get $K_{x_j(i)}\otimes K_{x_j(i)}\in L^1(\Omega;\mathscr L(\HH_K))$, which together with Lemma \ref{nvkvpeoeo} gives the fact that $\E[K_{x_j(i)}\otimes K_{x_j(i)}|\F(kh-1)]$ uniquely exists. Let $\{g(k),\F(kh-1),k\ge 0\}$ be the $L_2$-bounded adaptive sequence with values in $\HH_K$, by Assumption \ref{assumption5}, Proposition \ref{tiaojianqiwangxingzhi} and the condition (\ref{vnknknldldklsd}), we obtain
\bna\label{vnkwenkfffnkfmkweklf}
&&~~~\sum_{j=1}^N\sum_{k=0}^{\infty}\E\left[\left\|N_jg(k)-\sum_{i=kh}^{(k+1)h-1}\E\left.\left[H_j^*(i)H_j(i)g(k)\right|\F(kh-1)\right]\right\|_K^2\right]\cr
&&=\sum_{j=1}^N\sum_{k=0}^{\infty}\E\left[\left\|N_jg(k)-\sum_{i=kh}^{(k+1)h-1}\E\left.\left[K_{x_j(i)}\otimes K_{x_j(i)}g(k)\right|\F(kh-1)\right]\right\|_K^2\right]\cr
&&=\sum_{j=1}^N\sum_{k=0}^{\infty}\E\left[\left\|\left(N_j-\sum_{i=kh}^{(k+1)h-1}\E\left.\left[K_{x_j(i)}\otimes K_{x_j(i)}\right|\F(kh-1)\right]\right)g(k)\right\|_K^2\right]\cr
&&<\infty.
\ena
Denote $\rho_0=N\sup_{x\in\X}K(x,x)$. Given the integer $h>0$, by Assumption \ref{assumption5} and (\ref{vnklooeoeeee}), we have
\ban
&&~~~~\sup_{k\ge 0}\left(\E\left.\left[\|\H^*(k)\H(k)\|_{\mathscr L\left(\HH_K^N\right)}^{2^{\max\{h,2\}}}\right|\F(k-1)\right]\right)^{\frac{1}{2^{\max\{h,2\}}}}\cr
&&\leq N\sup_{k\ge 0}\left(\E\left.\left[\sup_{i\in \mathcal V}\left\|H_i^*(k)H_i(k)\right\|_{\LL(\HH_K)}^{2^{\max\{h,2\}}}\right|\F(k-1)\right]\right)^{\frac{1}{2^{\max\{h,2\}}}}\cr
&&=N\sup_{k\ge 0}\left(\E\left.\left[\sup_{i\in \mathcal V}\left\|K_{x_i(k)}\otimes K_{x_i(k)}\right\|_{\LL(\HH_K)}^{2^{\max\{h,2\}}}\right|\F(k-1)\right]\right)^{\frac{1}{2^{\max\{h,2\}}}}\cr
&&\leq N\sup_{k\ge 0}\left(\E\left.\left[\sup_{i\in \mathcal V}\sup_{\|f\|_K=1}\left\|\left(K_{x_i(k)}\otimes K_{x_i(k)}\right)f\right\|_K^{2^{\max\{h,2\}}}\right|\F(k-1)\right]\right)^{\frac{1}{2^{\max\{h,2\}}}}\cr
&&=N\sup_{k\ge 0}\left(\E\left.\left[\sup_{i\in \mathcal V}\sup_{\|f\|_K=1}\left\|f(x_i(k))K_{x_i(k)}\right\|_K^{2^{\max\{h,2\}}}\right|\F(k-1)\right]\right)^{\frac{1}{2^{\max\{h,2\}}}}\cr
&&\leq
N\sup_{k\ge 0}\left(\E\left.\left[\sup_{i\in \mathcal V}\sup_{\|f\|_K=1}|f(x_i(k))|^{2^{\max\{h,2\}}}\left\|K_{x_i(k)}\right\|_K^{2^{\max\{h,2\}}}\right|\F(k-1)\right]\right)^{\frac{1}{2^{\max\{h,2\}}}}\cr
&&\leq N\sup_{k\ge 0}\left(\E\left.\left[\sup_{i\in \mathcal V}\sup_{\|f\|_K=1}\left\|f\right\|^{2^{\max\{h,2\}}}_K\left(\sup_{x\in\X}K(x,x)\right)^{2^{\max\{h,2\}}}\right|\F(k-1)\right]\right)^{\frac{1}{2^{\max\{h,2\}}}}\cr
&&\leq N\sup_{x\in\X} K(x,x)\cr
&&=\rho_0~\text{a.s.}
\ean
It follows from Condition \ref{condition2} that there exists a constant $j_0>0$ and an integer $t_0>0$, such that $\sup_{k\ge 0}(4\rho_0a(k)+4\|\L_{\G}\|b(k))\leq j_0$ and $\sup_{k\ge t_0}(a(k)+b(k))(4\rho_0+4\|\L_{\G}\|)\leq 1$. Noting that
\bna\label{vkllekkeek}
\left\|4a(k)\H^*(k)\H(k)+4b(k)\L_{\G}\otimes I_{\HH_K}\right\|_{\LL\left(\HH_K^N\right)}\leq
\begin{cases}
j_0, & k<t_0;\\
1, & k\ge t_0
\end{cases}
~\text{a.s.},
\ena
by (\ref{vkllekkeek}), we get
\bna\label{vnksdkjdkdddd}
\left\|I_{\HH_K^N}-4\left(a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\HH_K}\right)\right\|_{\LL\left(\HH_K^N\right)}\leq
1+\Gamma(k)
~\text{a.s.},
\ena
where the nonnegative real sequence $\{\Gamma(k),k\ge 0\}$ is defined by
\ban
\Gamma(k)=
\begin{cases}
j_0, & k<t_0;\\
0, & k\ge t_0,
\end{cases}
\ean
which leads to $\sum_{k=0}^{\infty}\Gamma(k)<\infty$. It follows from (\ref{vnksdkjdkdddd}) that
\bna\label{vnkkwkwkw}
&&\E\left.\left[\left\|I_{\HH_K^N}-4\left(a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\HH_K}\right)\right\|_{\LL\left(\HH_K^N\right)}\right|\F(kh-1)\right]\cr
&&\leq 1+\Gamma(k)~\text{a.s.}
\ena
Hence, combining (\ref{vnkwenkfffnkfmkweklf})-(\ref{vnkkwkwkw}) and Theorem \ref{vnknoklfl} gives the fact that the algorithm (\ref{rkhs}) is mean square and almost surely strongly consistent.
\end{proof}


%\textbf{\emph{Proof of Corollary \ref{vnlllleleeemmem}:}}
\begin{proof}[Proof of Corollary \ref{vnlllleleeemmem}]
It follows from Assumption \ref{assumption5} and Theorem \ref{rkhsdingli} that $K_{x_j(i)}\otimes K_{x_j(i)}\in L^1(\Omega;\mathscr L(\HH_K))$, which together with Lemma \ref{nvkvpeoeo} gives the fact that $\E[K_{x_j(i)}\otimes $ $K_{x_j(i)}|\F(kh-1)]$ uniquely exists. Let $\{g(k),\F(kh-1),k\ge 0\}$ be the $L_2$-bounded adaptive sequence with values in $\HH_K$, by the condition (\ref{vnkmeeeemefffff}), we get
\ban
&&~~~\sum_{j=1}^N\sum_{k=0}^{\infty}\E\left[\left\|\left(N_j-\sum_{i=kh}^{(k+1)h-1}\E\left.\left[K_{x_j(i)}\otimes K_{x_j(i)}\right|\F(kh-1)\right]\right)g(k)\right\|_K^2\right]\cr
&&\leq \sum_{j=1}^N\sum_{k=0}^{\infty}\E\left[\left\|N_j-\sum_{i=kh}^{(k+1)h-1}\E\left.\left[K_{x_j(i)}\otimes K_{x_j(i)}\right|\F(kh-1)\right]\right\|_{\LL(\HH_K)}^2\|g(k)\|_K^2\right]\cr
&&\leq \sum_{j=1}^N\sum_{k=0}^{\infty}\E\left[\max_{j\in\mathcal V}\left\|N_j-\sum_{i=kh}^{(k+1)h-1}\E\left.\left[K_{x_j(i)}\otimes K_{x_j(i)}\right|\F(kh-1)\right]\right\|_{\LL(\HH_K)}^2\|g(k)\|_K^2\right]\cr
&&\leq N\mu_0\sup_{k\ge 0}\E\left[\|g(k)\|_K^2\right]\sum_{k=0}^{\infty}\tau(k)\cr
&&<\infty,
\ean
where the last inequality is obtained from $\sum_{k=0}^{\infty}\tau(k)<\infty$, which together with Theorem \ref{rkhsdingli} gives that the algorithm (\ref{rkhs}) is mean square and almost surely strongly consistent.
\end{proof}


%\textbf{\emph{Proof of Corollary \ref{rkhsdinglijjjjj}:}}
\begin{proof}[Proof of Corollary \ref{rkhsdinglijjjjj}]
Noting that $\{(x_1(k),\cdots,x_N(k)),k\ge 0\}$ and $\{(v_1(k),\cdots \\,v_N(k)),k\ge 0\}$ are i.i.d. sequences and they are mutually independent, we have
\ban
\sum_{i=kh}^{(k+1)h-1}\E\left.\left[K_{x_j(i)}\otimes K_{x_j(i)}\right|\F(kh-1)\right]&=&\sum_{i=kh}^{(k+1)h-1}\E\left[K_{x_j(i)}\otimes K_{x_j(i)}\right]\cr
&=&h\E\left[K_{x_j(0)}\otimes K_{x_j(0)}\right].
\ean
Denote $N_j:\HH_K\to \HH_K$ by
\ban
N_j=h\E\left[K_{x_j(0)}\otimes K_{x_j(0)}\right],~j\in \mathcal V.
\ean
It follows from Lemma \ref{lemmaA1} that $N_j\in \mathscr L(\HH_K)$ is a positive self-adjoint operator. Noting that
\ban
\left\|N_j-\sum_{i=kh}^{(k+1)h-1}\E\left.\left[K_{x_j(i)}\otimes K_{x_j(i)}\right|\F(kh-1)\right]\right\|_{\LL(\HH_K)}^2=0~\text{a.s.},
\ean
by the condition (\ref{nklnkle}), we get
\ban
\left\langle \sum_{j=1}^NN_jf,f\right\rangle _K =h\left\langle \E\left[\sum_{j=1}^NK_{x_j(0)}\otimes K_{x_j(0)}\right]f,f\right\rangle _K >0,
\ean
where $f$ is arbitrary non-zero function in $\HH_K$. Hence, it follows from Corollary \ref{vnlllleleeemmem} that the algorithm (\ref{rkhs}) is mean square and almost surely strongly consistent.
\end{proof}

\section{Proofs of Lemmas \ref{lemma1}-\ref{henandelemma}}\label{appendixd}
%\setcounter{lemma}{0}
%\def\thelemma{D.\arabic{lemma}}
%\setcounter{definition}{0}
%\def\thedefinition{D.\arabic{definition}}
%\setcounter{equation}{0}
%\def\theequation{D.\arabic{equation}}

%\textbf{\emph{Proof of Lemma \ref{lemma1}:}}
\begin{proof}[Proof of Lemma \ref{lemma1}]
Denote $D(k)=a(k)\H^*(k)\H(k)+b(k)\L_{\G}\otimes I_{\X}$ and  $P(k)=I_{\X^N}-D(k)$. Noting that $\G$ is undirected, it follows that  $\L_{\G}\otimes I_{\X}\ge 0$, which further gives $D(k)\ge 0~\text{a.s.}$ Let $x$ be the random element with values in Hilbert space $(\X^N,\tau_{\text{N}}(\X^N))$, we get
\bna\label{kkdsa}
&&~~~\mathbb \|\Phi_P(m,n)x\|^2\cr
&&=\langle\Phi_P(m,n)x,\Phi_P(m,n)x\rangle\cr
&&=\left\langle x,\Phi^*_P(m,n)\Phi_P(m,n)x\right\rangle\cr
&&=\left\langle x,(I_{\X^N}-D^*(n))\cdots(I_{\X^N}-D^*(m))(I_{\X^N}-D(m))\cdots (I_{\X^N}-D(n))x\right\rangle\cr
&&=\left\langle x,x-2\sum_{k=n}^mD(k)x+\sum_{k=n}^mM_kx\right\rangle\cr
&&=\|x\|^2-2\sum_{k=n}^m\langle x,D(k)x\rangle+\left\langle x,\sum_{k=n}^mM_kx\right\rangle\cr
&&\leq \|x\|^2+\left\langle x,\sum_{k=n}^mM_kx\right\rangle\cr
&&\leq \left(1+\sum_{k=n}^m\|M_k\|\right)\|x\|^2~\text{a.s.},~0\leq n \leq m,
\ena
where $M_k,k=2,\cdots,2(m-n+1)$ denote the $k$-th order terms in the binomial expansion of $\Phi_P^*(m,n)\Phi_P(m,n)$. For $0\leq m-n\leq h$, by the conditional Lyapunov inequality and (\ref{nxsl}), we obtain
\bna\label{jjkkss}
\sup_{k\ge 0}\mathbb E\left.\left[\|D(k)\|^i\right|\mathcal F(k-1)\right]&\leq& \sup_{k\ge 0}\mathbb E\left.\left[\|D(k)\|^{2^h}\right|\mathcal F(k-1) \right]^{\frac{i}{2h}}\cr
&\leq& \rho_0^i(a(k)+b(k))^i~\text{a.s.},\ 2\leq i\leq 2^h,
\ena
where $\rho_0:=\rho+\|\L_{\G}\|$. Note that
\bna\label{nkvvssk}
&&~~~~\mathbb E\left.\left[\|D(k)\|^i\right|\mathcal F(n-1) \right]\cr
&&=\mathbb E\left.\left.\left[\mathbb E\left[\|D(k)\|^i\right|\mathcal F(k-1)\right]\right|\mathcal F(n-1)\right],~2\leq i\leq 2^h,~k\ge n.
\ena
Since the real sequences $\{a(k),k\ge 0\}$ and $\{b(k),k\ge 0\}$ are both monotonically decreasing to $0$, it follows that there exists a constant $c_0>0$ such that $\sup_{k\ge 0}(a(k)+b(k))\leq c_0$. For $0\leq m-n\leq h$, from the definition of $M_k$ and (\ref{jjkkss})-(\ref{nkvvssk}), by termwise multiplication and using the H\"{o}lder inequality repeatedly, we have
\bna\label{pplds}
\mathbb E[\|M_k\||\mathcal F(n-1)]&\leq& \mathbb C_{2h}^k\rho_0^k(a(n)+b(n))^k\cr
&\leq& 2c_0^{-2}\left(a^2(n)+b^2(n)\right)\mathbb C_{2h}^k\rho_0^kc_0^k~\text{a.s.},\ k=2,\cdots,2(m-n+1).
\ena
Denote $c_k=2c_0^{-2}(1+\rho_0c_0)^{2k}$, by (\ref{pplds}), we get
\bna\label{wwddsaf}
\sum_{k=n}^m\mathbb E[\|M_k\||\mathcal F(n-1)]\leq c_{m-n+1}\left(a^2(n)+b^2(n)\right)~\text{a.s.}
\ena
If $x\in L^0(\Omega,\F(n-1);\X^N)$, substituting (\ref{wwddsaf}) into (\ref{kkdsa}) gives
\bna\label{kkxnms}
\mathbb E\left.\left[\|\Phi_P(m,n)x\|^2\right|\mathcal F(n-1)\right]\leq \left(1+c_{h+1}a^2(n)+c_{h+1}b^2(n)\right)\|x\|^2~\text{a.s.}
\ena
If $x\in L^0(\Omega,\F(ih-1);\X^N)$, it follows from (\ref{kkxnms}) that
\bna\label{eedd}
&&\mathbb E\left.\left[\|\Phi_P((i+1)h-1,ih)x\|^2\right|\mathcal F(ih-1)\right]\cr
&&\leq \left(1+c_ha^2(i)+c_hb^2(i)\right)\|x\|^2~\text{a.s.},~i\ge 0.
\ena
(I) For any given nonnegative integer $n$, if $x\in L^0(\Omega,\F(nh-1);\X^N)$, then by (\ref{eedd}), we obtain
\bna\label{vnklmef}
&&~~~~\mathbb E\left.\left[\|\Phi_P(mh-1,nh)x\|^2\right|\mathcal F(nh-1)\right]\cr
&&=\mathbb E\left.\left[\|\Phi_P(mh-1,(m-1)h)\Phi_P((m-1)h-1,nh)x\|^2\right|\mathcal F(nh-1)\right]\cr
&&=\mathbb E\big[\mathbb E\big[\|\Phi_P(mh-1,(m-1)h)\cr
&&~~~~~~~~~~~~~\times\Phi_P((m-1)h-1,nh)x\|^2|\mathcal F((m-1)h-1)\big]|\mathcal F(nh-1)\big]\cr
&&\leq \left(1+c_ha^2(m-1)+c_hb^2(m-1)\right)\mathbb E\left.\left[\|\Phi_P((m-1)h-1,nh)x\|^2\right|\mathcal F(nh-1)\right]\cr
&&\leq \prod_{k=n}^{m-1}\left(1+c_ha^2(k)+c_hb^2(k)\right)\|x\|^2~\text{a.s.}
\ena
Denote $d_1=\prod_{k=0}^{\infty}(1+c_ha^2(k)+c_hb^2(k))$, from (\ref{vnklmef}) and Condition \ref{condition2}, we get (\ref{yinliwudianyi1}). \\
(II) For any given nonnegative integer $n$, if $x\in L^0(\Omega,\F(nh-1);\X^N)$, it follows from Condition \ref{condition1} that there exists a constant $d_2>0$, such that $\sup_{k\ge 0}(1+c_{h+1}a^2(k)+c_{h+1}b^2(k))\leq d_2$, which together with (\ref{kkxnms}) gives (\ref{yinliwudianyi2}).
\end{proof}


%\textbf{\emph{Proof of Lemma \ref{hhhlemma}:}}
\begin{proof}[Proof of Lemma \ref{hhhlemma}]
Denote $D'(k)=\sum_{s=kh}^{(k+1)h-1}(a(s)\H^*(s)\H(s)+b(s)(\L_{\G}\otimes I_{\X}))$ and $P'(k)=I_{\X^N}-D'(k)$. Noting that $\G$ is undirected, we have $\L_{\G}\otimes I_{\X}\ge 0$, which gives $D(k)\ge 0~\text{a.s.}$ Let $x\in L^0(\Omega,\F(nh^2-1);\X^N)$ be the random element with values in Hilbert space $\X^N$, we get
\ban
&&~~~~\mathbb E\left.\left[\|\Phi_{P'}((n+1)h-1,nh)x\|^2\right|\mathcal F(nh^2-1)\right]\cr
&&\leq \left(1+\sum_{k=nh}^{(n+1)h-1}\left.\mathbb E\left[\|M_k'\|\right|\mathcal F(nh^2-1)\right]\right)\|x\|^2,
\ean
where $M_k',k=2,\cdots,2h$ denote the $k$-th order terms in the binomial expansion of $\Phi_{P'}^*((n+1)h-1,nh)\Phi_{P'}((n+1)h-1,nh)$. It follows from the conditional Lyapunov inequality and Condtion \ref{condition1} that
\bna\label{jjkkssssfffw}
\sup_{k\ge 0}\mathbb E\left.\left[\|D'(k)\|^i\right|\mathcal F(k-1)\right]&\leq&  h\rho_0^i\sum_{s=kh}^{(k+1)h-1}(a(s)+b(s))^i\cr
&\leq& h^2\rho_0^i(a(n)+b(n))^i~\text{a.s.},\ 2\leq i\leq 2^h,
\ena
where $\rho_0=\rho+\|\L_{\G}\|$. Since $\{a(k),k\ge 0\}$ and $\{b(k),k\ge 0\}$ are both monotonically decreasing to  $0$, there exists a constant $c_0>0$ such that $\sup_{k\ge 0}(a(k)+b(k))\leq c_0$. From the definition of $M_k'$, by termwise multiplication and using the H\"{o}lder inequality of the conditional expectation, we have
\bna\label{ppldssss}
\mathbb E\left.\left[\left\|M_k'\right\|\right|\mathcal F(nh^2-1)\right]&\leq& h^2\mathbb C_{2h}^k\rho_0^k(a(n)+b(n))^k\cr
&\leq& 2h^2c_0^{-2}\left(a^2(n)+b^2(n)\right)\mathbb C_{2h}^k\rho_0^kc_0^k~\text{a.s.},~~k=2,\cdots,2h.
\ena
Noting that (\ref{ppldssss}) and $x\in L^0(\Omega,\F(nh^2-1);\X^N)$, we get
\bna\label{eefffdddssd}
&&\mathbb E\left.\left[\|\Phi_{P'}((n+1)h-1,nh)x\|^2\right|\mathcal F(nh^2-1)\right]\cr
&&\leq \left(1+c'a^2(n)+c'b^2(n)\right)\|x\|^2~\text{a.s.},
\ena
where $c'=2h^2c_0^{-2}(1+\rho_0c_0)^{2h}$. By (\ref{eefffdddssd}), we obtain
\ban
&&~~~~\mathbb E\left.\left[\|\Phi_{P'}(mh-1,nh)x\|^2\right|\mathcal F(nh^2-1)\right]\cr
&&=\mathbb E\left.\left[\|\Phi_{P'}(mh-1,(m-1)h)\Phi_{P'}((m-1)h-1,nh)x\|^2\right|\mathcal F(nh^2-1)\right]\cr
&&=\mathbb E\big[\mathbb E\big[\|\Phi_{P'}(mh-1,(m-1)h)\cr
&&~~~~~~~~~~~~\times\Phi_{P'}((m-1)h-1,nh)x\|^2|\mathcal F((m-1)h^2-1)\big]|\mathcal F(nh^2-1)\big]\cr
&&\leq \left(1+c'a^2(m-1)+c'b^2(m-1)\right)\mathbb E\left.\left[\|\Phi_{P'}((m-1)h-1,nh)x\|^2\right|\mathcal F(nh^2-1)\right]\cr
&&\leq \prod_{k=n}^{m-1}\left(1+c'a^2(k)+c'b^2(k)\right)\|x\|^2~\text{a.s.},~m>n\ge 0.
\ean
Denote $q_1=\prod_{k=0}^{\infty}(1+c'a^2(k)+c'b^2(k))$, it follows from Condition \ref{condition2} that
\ban
\sup_{m\ge 0}\mathbb E\left.\left[\|\Phi_{P'}(mh-1,nh)x\|^2\right|\mathcal F(nh^2-1)\right]\leq q_1\|x\|^2~\text{a.s.}
\ean
Following the same way as the proof of Lemma \ref{lemma1}, it shows that there exists a constant $q_2>0$, such that
\bna\label{leisileisi}
\sup_{0\leq m\leq n+h}\mathbb E\left.\left[\|\Phi_{P'}(m,n)x\|^2\right|\mathcal F(nh-1)\right]\leq q_2\|x\|^2~\text{a.s.}
\ena
For any given positive integer $j$, denote $m_j=\lfloor \frac{j}{h} \rfloor,\widetilde{m}_j=\lceil \frac{j}{h} \rceil$. Firsy, if $0\leq i<k-3h$, then $m_kh>\widetilde{m}_{i+1}h$. Let $y\in L^0(\Omega,\F((i+1)h-1);\X^N)$ be the random element with values in Hilbert space $\X^N$, noting that $0\leq k-m_kh<h$, $0\leq \widetilde{m}_{i+1}h-(i+1)<h$, $\Phi_{P'}(m_kh-1,\widetilde{m}_{i+1}h)\Phi_{P'}(\widetilde{m}_{i+1}h-1,i+1)y\in \F(m_kh^2-1)$ and $\Phi_{P'}(\widetilde{m}_{i+1}h-1,i+1)y\in \F(\widetilde{m}_{i+1}h^2-1)$, by (\ref{eefffdddssd})-(\ref{leisileisi}), we get
\bna\label{jjkklkkklll}
&&~~~~\mathbb E\left[\left\|\Phi_{P'}(k,i+1)y\right\|^2\right]\cr
&&=\mathbb E\left[\|\Phi_{P'}(k,m_kh)\Phi_{P'}(m_kh-1,\widetilde{m}_{i+1}h)\Phi_{P'}(\widetilde{m}_{i+1}h-1,i+1)y\|^2\right]\cr
%&&=\mathbb E\left[\mathbb E\left.\left[\|\Phi_{P'}(k,m_kh)\Phi_{P'}(m_kh-1,\widetilde{m}_{i+1}h)\Phi_{P'}(\widetilde{m}_{i+1}h-1,i+1)y\|^2\right|\mathcal F(m_kh^2-1)\right]\right]\cr
&&\leq q_2\mathbb E\left[\|\Phi_{P'}(m_kh-1,\widetilde{m}_{i+1}h)\Phi_{P'}(\widetilde{m}_{i+1}h-1,i+1)y\|^2\right]\cr
&&=q_2\mathbb E\left[\E\left.\left[\|\Phi_{P'}(m_kh-1,\widetilde{m}_{i+1}h)\Phi_{P'}(\widetilde{m}_{i+1}h-1,i+1)y\|^2\right|\F(\widetilde{m}_{i+1}h^2-1)\right]\right]\cr
&&\leq q_1q_2\E\left[\|\Phi_{P'}(\widetilde{m}_{i+1}h-1,i+1)y\|^2\right]\cr
&&=q_1q_2\E\left[\E\left.\left[\|\Phi_{P'}(\widetilde{m}_{i+1}h-1,i+1)y\|^2\right|\F((i+1)h-1)\right]\right]\cr
&&\leq q_1q_2^2\E\left[\|y\|^2\right],~0\leq i<k-3h.
\ena
Secondly, it follows from (\ref{leisileisi}) that
\bna\label{kjifw}
&&~~~~\mathbb E\left[\|\Phi_{P'}(k,i+1)y\|^2\right]\cr
&&=\mathbb E\left[\mathbb E\left.\left[\|\Phi_{P'}(k,i+1)y\|^2\right|\mathcal F((i+1)h-1)\right]\right]\cr
&&\leq q_2\E\left[\|y\|^2\right],~k-h\leq i<k.
\ena
From (\ref{leisileisi}) and (\ref{kjifw}), it is known that
\bna\label{ikdjdf}
&&~~~~\mathbb E\left[\|\Phi_{P'}(k,i+1)y\|^2\right]\cr
&&=\mathbb E\left[\mathbb E\left.\left[\|\Phi_{P'}(k,i+1)y\|^2\right|\mathcal F((i+1)h-1)\right]\right]\cr
&&=\mathbb E\left[\mathbb E\left.\left[\|\Phi_{P'}(k,k-h+1)\Phi_{P'}(k-h,i+1)y\|^2\right|\mathcal F((i+1)h-1)\right]\right]\cr
%&&=\mathbb E\left[\mathbb E\left.\left[\mathbb E\left.\left[\|\Phi_{P'}(k,k-h+1)\Phi_{P'}(k-h,i+1)y\|^2\right|\mathcal F((k-h+1)h-1)\right]\right|\mathcal F((i+1)h-1)\right]\right]\cr
&&\leq q_2\mathbb E\left[\mathbb E\left.\left[\|\Phi_{P'}(k-h,i+1)y\|^2\right|\mathcal F((i+1)h-1)\right]\right]\cr
&&\leq q_2^2\E\left[\|y\|^2\right],~k-2h\leq i<k-h.
\ena
Finally, it follows from (\ref{leisileisi}) and (\ref{ikdjdf}) that
\bna\label{wdkkdd}
&&~~~~\mathbb E\left[\|\Phi_{P'}(k,i+1)y\|^2\right]\cr
&&=\mathbb E\left[\mathbb E\left.\left[\|\Phi_{P'}(k,i+1)y\|^2\right|\mathcal F((i+1)h-1)\right]\right]\cr
&&=\mathbb E\left[\mathbb E\left.\left[\|\Phi_{P'}(k,k-h+1)\Phi_{P'}(k-h,i+1)y\|^2\right|\mathcal F((i+1)h-1)\right]\right]\cr
%&&=\mathbb E\left[\mathbb E\left.\left[\mathbb E\left.\left[\|\Phi_{P'}(k,k-h+1)\Phi_{P'}(k-h,i+1)y\|^2\right|\mathcal F((k-h+1)h-1)\right]\right|\mathcal F((i+1)h-1)\right]\right]\cr
&&\leq q_2\mathbb E\left[\mathbb E\left.\left[\|\Phi_{P'}(k-h,i+1)y\|^2\right|\mathcal F((i+1)h-1)\right]\right]\cr
&&\leq q_2^3\E\left[\|y\|^2\right],~k-3h\leq i<k-2h.
\ena
Combining (\ref{kjifw})-(\ref{wdkkdd}), we get
\bna\label{llcck}
\mathbb E\left[\|\Phi_{P'}(k,i+1)y\|^2\right]\leq \max\left\{q_2,q_2^2,q_2^3\right\}\E\left[\|y\|^2\right],~0<k-i\leq 3h.
\ena
Denote $d_3=\max\{q_1q_2^2,q_2,q_2^2,q_2^3\}$. By (\ref{jjkklkkklll}) and (\ref{llcck}), we have
\ban
\sup_{k\ge 0}\mathbb E\left[\|\Phi_{P'}(k,i+1)y\|^2\right]\leq d_3\E\left[\|y\|^2\right],~\forall i\ge 0.
\ean
\end{proof}


%\textbf{\emph{Proof of Lemma \ref{henandelemma}:}}
\begin{proof}[Proof of Lemma \ref{henandelemma}]
Let $\{x(k),\F(kh-1),k\ge 0\}$ be the $L_2$-bounded adaptive sequence with values in Hilbert space $\X^N$. Given the nonnegative integer $m$, we define $\{w(k),k\ge 0\}$ by
\bna\label{diedaishii}
&&~~~~w(k+1)\cr
&&=\left(I_{\X^N}-\sum_{s=kh}^{(k+1)h-1}\left(a(s)\H^*(s)\H(s)+b(s)\L_{\G}\otimes I_{\X}\right)\right)w(k),~k\ge m,
\ena
where $w(m)=x(m),w(i)=0,i=0,\cdots,m-1$. It follows from Proposition \ref{nlllwwieiie}.(a)-(c) that $\{w(k),k\ge 0\}$ is the random sequence with values in Hilbert space $(\X^N,\tau_{\text{N}}(\X^N))$. On the one hand, from the definition of $w(k)$, it follows that
\bna\label{wffee}
&&~~~~w(k+1)\cr
&&=\left(\prod_{i=m}^k\left(I_{\X^N}-\sum_{s=ih}^{(i+1)h-1}(a(s)\H^*(s)\H(s)+b(s)\L_{\G}\otimes I_{\X})\right)\right)x(m),~k\ge m.
\ena
Noting that $x(m)\in L^0(\Omega,\F(mh-1);\X^N)$, it follows from Lemma \ref{hhhlemma} that there exists a constant $d_3>0$, such that
\bna\label{lsaew}
\sup_{k\ge 0}\E\left[\|w(k+1)\|^2\right]
%&&=\sup_{k\ge 0}\E\left[\left\|\left(\prod_{i=m}^k\left(I_{\X^N}-\sum_{s=ih}^{(i+1)h-1}(a(s)\H^*(s)\H(s)+b(s)\L_{\G}\otimes I_{\X})\right)\right)x(m)\right\|^2\right]\cr
\leq d_3\E\left[\|x(m)\|^2\right]
\leq d_3\sup_{k\ge 0}\E\left[\|x(k)\|^2\right]<\infty.
\ena
On the other hand, (\ref{diedaishii}) can rewritten as
\ban
w(i+1)&=&(I_{\X^N}-c(i)\HH)w(i)\cr
&&+\left(c(i)\HH-\sum_{s=ih}^{(i+1)h-1}(a(s)\H^*(s)\H(s)+b(s)\L_{\G}\otimes I_{\X})\right)w(i),~i\ge m,
\ean
which gives
\bna\label{fwwiii}
&&~~~~w(k+1)\cr
&&=\left(\prod_{i=m}^k(I_{\X^N}-c(i)\HH)\right)x(m)+\sum_{i=m}^k\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)\cr
&&~~~\times\left(c(i)\HH-\sum_{s=ih}^{(i+1)h-1}(a(s)\H^*(s)\H(s)+b(s)\L_{\G}\otimes I_{\X})\right)w(i),~k\ge m.
\ena
From (\ref{lsaew}) and the condition (ii), it is known that
\ban
\sup_{s\ge 0\atop i\ge 0}\E\left[\|\H^*(s)\H(s)w(i)\|\right]\leq \sup_{s\ge 0}\E\left[\|\H^*(s)\H(s)\|^2\right]+\sup_{i\ge 0}\E\left[\|w(i)\|^2\right]<\infty,
\ean
which together Proposition \ref{wenknknkn} leads to $\H^*(s)\H(s)w(i)\in L^1(\Omega,\F(s);\X^N)$, $s\ge ih$. Thus, Lemma \ref{nvkvpeoeo} implies that there exists a unique conditional expectation $\E[\H^*(s)\H(s)w(i)|\F(i\\h-1)]$ of $\H^*(s)\H(s)w(i)$ with respect to the $\sigma$-algebra $\F(ih-1)$, it gives
\bna\label{fewdee}
\left(c(i)\HH-\sum_{s=ih}^{(i+1)h-1}(a(s)\H^*(s)\H(s)+b(s)\L_{\G}\otimes I_{\X})\right)w(i)
%&&=c(i)\H w(i)-\sum_{s=ih}^{(i+1)h-1}(a(s)\E[\H^*(s)\H(s)w(i)|\F(ih-1)]+b(s)(\L_{\G}\otimes I_{\X})w(i))\cr
%&&~~~~+\sum_{s=ih}^{(i+1)h-1}a(s)(\E[\H^*(s)\H(s)w(i)|\F(ih-1)]-\H^*(s)\H(s)w(i))\cr
=w_1(i)+w_2(i),
\ena
where
\bna\label{mlwer}
\begin{cases}
  w_1(i)=\displaystyle c(i)\HH w(i)-\sum_{s=ih}^{(i+1)h-1}(a(s)\E[\H^*(s)\H(s)w(i)|\F(ih-1)]\\~~~~~~~~~~~~~~~~~+b(s)(\L_{\G}\otimes I_{\X})w(i)),\\
  w_2(i)=\displaystyle \sum_{s=ih}^{(i+1)h-1}a(s)(\E[\H^*(s)\H(s)w(i)|\F(ih-1)]-\H^*(s)\H(s)w(i)).
\end{cases}
\ena
By (\ref{wffee}) and (\ref{fwwiii})-(\ref{fewdee}), we get
\ban
&&~~~\left(\prod_{i=m}^k\left(I_{\X^N}-\sum_{s=ih}^{(i+1)h-1}(a(s)\H^*(s)\H(s)+b(s)\L_{\G}\otimes I_{\X})\right)\right)x(m)\cr &&=\left(\prod_{i=m}^k(I_{\X^N}-c(i)\HH)\right)x(m)+\sum_{i=m}^k\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)(w_1(i)+w_2(i)),
\ean
which together with Cauchy inequality leads to
\bna\label{ikddww}
&&~~~\E\left[\left\|\left(\prod_{i=m}^k\left(I_{\X^N}-\sum_{s=ih}^{(i+1)h-1}(a(s)\H^*(s)\H(s)+b(s)\L_{\G}\otimes I_{\X})\right)\right)x(m)\right\|^2\right]\cr
&&\leq 2\E\left[\left\|\left(\prod_{i=m}^k(I_{\X^N}-c(i)\HH)\right)x(m)\right\|^2\right]\cr &&~~~~~~~+2\E\left[\left\|\sum_{i=m}^k\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)(w_1(i)+w_2(i))\right\|^2\right],~k\ge m.
\ena
By Lemma \ref{lemmaA7}, it is known that there exist constants $M,d>0$, such that
\ban
\left\|\left(\prod_{i=m}^k(I_{\X^N}-c(i)\HH)\right)x(m)\right\|^2\leq M^{2d}\|x(m)\|^2~\text{a.s.}
\ean
Noting that $\sup_{k\ge 0}\E[\|x(k)\|^2]<\infty$, it follows from Lebesgue dominated convergence theorem and Lemma \ref{lemmaA7} that
\bna\label{oo1}
\lim_{k\to\infty}\E\left[\left\|\left(\prod_{i=m}^k(I_{\X^N}-c(i)\HH)\right)x(m)\right\|^2\right]=0,~\forall m\ge 0.
\ena
By using Cauchy inequality again, we obtain
\bna\label{oooff}
&&~~~\E\left[\left\|\sum_{i=m}^k\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)(w_1(i)+w_2(i))\right\|^2\right]\cr
&&\leq 2\E\left[\left\|\sum_{i=m}^k\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)w_1(i)\right\|^2\right]\cr &&~~~~~~~+2\E\left[\left\|\sum_{i=m}^k\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)w_2(i)\right\|^2\right].
\ena
We now consider the right-hand side of (\ref{oooff}) term by term. First, by the Minkowski inequality, we get
\bna\label{oolkf}
&&~~~\E\left[\left\|\sum_{i=m}^k\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)w_1(i)\right\|^2\right]\cr
&&\leq \left(\sum_{i=m}^k\left(\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)w_1(i)\right\|^2\right]\right)^{\frac{1}{2}}\right)^2.
\ena
It follows from (\ref{diedaishii}) and (\ref{lsaew}) that $\{w(k),\F(kh-1),k\ge 0\}$ is the adaptive sequence satisfying $\sup_{k\ge 0}\E[\|w(k)\|^2]<\infty$, which together with (\ref{mlwer}), the condition (i) and (\ref{oolkf}) gives
\bna\label{vspppw}
\lim_{k\to \infty}\E\left[\left\|\sum_{i=m}^k\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)w_1(i)\right\|^2\right]=0.
\ena
Secondly, noting that $w_2(i)\in \F((i+1)h-1)$, $w(i)\in \F(ih-1)$, by Condition \ref{condition1}, it follows from the condition (ii) and (\ref{lsaew}) that
\bna\label{fuckd}
&&~~~\E\left[\|w_2(i)\|^2\right]\cr
&&= \E\left[\left\|\sum_{s=ih}^{(i+1)h-1}a(s)(\E[\H^*(s)\H(s)w(i)|\F(ih-1)]-\H^*(s)\H(s)w(i))\right\|^2\right]\cr
&&\leq ha^2(i)\E\left[\sum_{s=ih}^{(i+1)h-1}\|\E[\H^*(s)\H(s)w(i)|\F(ih-1)]-\H^*(s)\H(s)w(i)\|^2\right]\cr
&&\leq 2ha^2(i)\E\Bigg[\sum_{s=ih}^{(i+1)h-1}\big(\E\big[\|\H^*(s)\H(s)w(i)\|^2|\F(ih-1)\big]\cr
&&~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+\|\H^*(s)\H(s)w(i)\|^2\big)\Bigg]\cr
&&\leq 2ha^2(i)\E\left[\sum_{s=ih}^{(i+1)h-1}\left(\rho_0^2\|w(i)\|^2+\|\H^*(s)\H(s)w(i)\|^2\right)\right]\cr
&&\leq 2ha^2(i)\E\left[\|w(i)\|^2\sum_{s=ih}^{(i+1)h-1}\left(\rho_0^2+\|\H^*(s)\H(s)\|^2\right)\right]\cr
&&= 2ha^2(i)\E\left[\E\left.\left[\|w(i)\|^2\sum_{s=ih}^{(i+1)h-1}\left(\rho_0^2+\|\H^*(s)\H(s)\|^2\right)\right|\F(ih-1)\right]\right]\cr
&&= 2ha^2(i)\E\left[\E\left.\left[\sum_{s=ih}^{(i+1)h-1}\left(\rho_0^2+\|\H^*(s)\H(s)\|^2\right)\right|\F(ih-1)\right]\|w(i)\|^2\right]\cr
&&\leq 4h^2\rho_0^2a^2(i)\E\left[\|w(i)\|^2\right],
\ena
which together with Condition \ref{condition2} gives
\bna\label{vlllls}
\sup_{i\ge 0}\E\left[\|w_2(i)\|^2\right]<\infty.
\ena
Thus, it follows from Lemma \ref{nvkvpeoeo} that $\E[w_2(i)|\F(ih-1)]$ exists and
\bna\label{iwsf}
&&~~~~\E[w_2(i)|\F(ih-1)]\cr
&&=\E\Bigg[\sum_{s=ih}^{(i+1)h-1}a(s)(\E[\H^*(s)\H(s)w(i)|\F(ih-1)]\cr
&&~~~~~~~~~~~~~~~-\H^*(s)\H(s)w(i))\bigg|\F(ih-1)\Bigg]\cr
&&=\sum_{s=ih}^{(i+1)h-1}a(s)\E[\E[\H^*(s)\H(s)w(i)|\F(ih-1)]-\H^*(s)\H(s)w(i)|\F(ih-1)]\cr
&&=0.
\ena
Meanwhile, from Lemma \ref{lemmaA7}, it is known that there exist constants $M,d>0$ such that
\bna\label{nvweefff}
&&~~~~\sup_{\|x\|=1\atop x\in \X^N}\inf\left\{r\ge 0:\P\left(\left\|\left(\prod_{j=t+1}^k(I_{\X^N}-c(j)\HH)\right)x\right\|<r\right)=1\right\}\cr
&&\leq \sup_{\|x\|=1\atop x\in \X^N}M^d\|x\|<\infty.
\ena
For $m\leq s<t\leq k$, it follows from Proposition \ref{wenknknkn}, Lemma \ref{lemmaA1}, Lemma 3.5.2 in \cite{hy}, Proposition \ref{lemmaA6} and (\ref{vlllls})-(\ref{nvweefff}) that
\bna\label{ffkk}
~~~~\E\left[\left\langle \left(\prod_{j=s+1}^k(I_{\X^N}-c(j)\HH)\right)w_2(s),\left(\prod_{j=t+1}^k(I_{\X^N}-c(j)\HH)\right)w_2(t)\right\rangle\right]
%&&=\E\left[\E\left.\left[\left\langle \left(\prod_{j=s+1}^k(I_{\X^N}-c(j)\H)\right)w_2(s),\left(\prod_{j=t+1}^k(I_{\X^N}-c(j)\H)\right)w_2(t)\right\rangle\right|\F(th-1)\right]\right]\cr
%&&=\E\left[\left\langle \left(\prod_{j=s+1}^k(I_{\X^N}-c(j)\H)\right)w_2(s),\E\left.\left[\left(\prod_{j=t+1}^k(I_{\X^N}-c(j)\H)\right)w_2(t)\right|\F(th-1)\right]\right\rangle\right]\cr
%&&=\E\left[\left\langle \left(\prod_{j=s+1}^k(I_{\X^N}-c(j)\H)\right)w_2(s),\left(\prod_{j=t+1}^k(I_{\X^N}-c(j)\H)\right)\E[w_2(t)|\F(th-1)]\right\rangle\right]\cr
=0.
\ena
On the one hand, by Lemma \ref{lemmaA7}, it is known that there exist positive constants $M$ and $d$, such that
\bna\label{wiiiwww}
\left\|\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)\frac{1}{a(i)}w_2(i)\right\|\leq M^d\left\|\frac{1}{a(i)}w_2(i)\right\|~\text{a.s.}
\ena
Thus, by Lemma \ref{lemmaA7} and (\ref{ffkk})-(\ref{wiiiwww}), we get
\bna\label{oolll}
&&~~~~\E\left[\left\|\sum_{i=m}^k\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)w_2(i)\right\|^2\right]\cr
&&=\sum_{i=m}^k\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)w_2(i)\right\|^2\right]\cr &&~~~~~~~+2\sum_{m\leq s<t\leq k}\E\Bigg[\Bigg\langle \left(\prod_{j=s+1}^k(I_{\X^N}-c(j)\HH)\right)w_2(s),\cr &&~~~~~~~~~~~~~~~~~~~~~~~~~~~~\left(\prod_{j=t+1}^k(I_{\X^N}-c(j)\HH)\right)w_2(t)\Bigg\rangle\Bigg]\cr
&&=\sum_{i=m}^k\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)w_2(i)\right\|^2\right]\cr
&&=\sum_{i=m}^ka^2(i)\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)\frac{1}{a(i)}w_2(i)\right\|^2\right]\cr
&&\leq M^d\sum_{i=m}^ka^2(i)\left[\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)\frac{1}{a(i)}w_2(i)\right\|^2\right]\right]^{\frac{1}{2}}\cr &&~~~~~~~~~~~~~~~~~~~~~~\times\left[\E\left[\left\|\frac{1}{a(i)}w_2(i)\right\|^2\right]\right]^{\frac{1}{2}}.
\ena
On the other hand, by (\ref{lsaew}) and (\ref{fuckd}), we have
\bna\label{ofskj}
\sup_{i\ge 0}\mathbb E\left[\left\|\frac{1}{a(i)}w_2(i)\right\|^2\right]\leq 4h^2\rho_0^2\sup_{i\ge 0}\E\left[\|w(i)\|^2\right]<\infty.
\ena
Substituting (\ref{ofskj}) into (\ref{oolll}) gives
\ban
&&~~~~\E\left[\left\|\sum_{i=m}^k\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)w_2(i)\right\|^2\right]\cr
&&\leq M^d\sup_{i\ge 0}\left[\E\left[\left\|\frac{1}{a(i)}w_2(i)\right\|^2\right]\right]^{\frac{1}{2}}\cr
&&~~~~\times\sum_{i=m}^ka^2(i)\left(\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)\frac{1}{a(i)}w_2(i)\right\|^2\right]\right)^{\frac{1}{2}},
\ean
which together with Condition \ref{condition2}, (\ref{ofskj}) and Lemma \ref{lemmaA8} leads to
\bna\label{iikdd}
\lim_{k\to\infty}\E\left[\left\|\sum_{i=m}^k\left(\prod_{j=i+1}^k(I_{\X^N}-c(j)\HH)\right)w_2(i)\right\|^2\right]=0,~\forall m\ge 0.
\ena
Hence, substituting (\ref{oo1})-(\ref{oooff}), (\ref{vspppw}) and (\ref{iikdd}) into (\ref{ikddww}) completes the proof of Lemma \ref{henandelemma}.
\end{proof}

\section{Key lemmas}\label{appendixee}
%\setcounter{lemma}{0}
%\def\thelemma{E.\arabic{lemma}}
%\setcounter{definition}{0}
%\def\thedefinition{E.\arabic{definition}}
%\setcounter{equation}{0}
%\def\theequation{E.\arabic{equation}}

\begin{lemma}[\cite{rb}]\label{lemmaA3}
Let $\{x(k), \mathcal F(k)\}$, $\{\a(k),\mathcal F(k)\}$, $\{\b(k),\mathcal F(k)\}$ and $\{\gamma(k), \mathcal F(k)\}$ be nonnegative  adaptive sequences satisfying
$$
\E[x(k+1)|\mathcal F(k)]\le(1+\a(k))x(k)-\beta(k)+\gamma(k),~k\ge 0~\text{a.s.},
$$
and $\sum_{k=0}^\infty(\a(k)+\gamma(k))<\infty~\text{a.s.}$ Then $x(k)$ converges to a finite random variable a.s., and $\sum_{k=0}^\infty\b(k)<\infty$ a.s.
\end{lemma}


\begin{lemma}\label{lemma6}
Let $\{a(i),i\in \Lambda\}$ be a nonnegative real sequence, where $\Lambda \subseteq \mathbb N$ and $\sum_{i\in \Lambda}a(i)<\infty$, $\{b(k,i),k\in \mathbb N,i\in \Lambda\}$ be a double index real sequence. If $\lim_{k\to \infty}b(k,i)=0$, $\forall i\in  \Lambda$, and there exists a constant $c>0$ such that $|b(k,i)|\leq c$, $\forall k\in \mathbb N$, $\forall i\in \Lambda$, then
\bna\label{mmxxxx}
\lim_{k\to \infty}\sum_{i\in \Lambda}a(i)b(k,i)=0.
\ena
\end{lemma}

\begin{proof}
For any given $\varepsilon>0$, it follows from $\sum_{i\in \Lambda}a(i)<\infty$ that there exists an integer $N>0$, such that $\sum_{i\in \Lambda_N}a(i)<\varepsilon$, where $\Lambda_N=\{i\in \Lambda:i\ge N+1\}$. We obtain
\ban
\left|\sum_{i\in \Lambda_N}a(i)b(k,i)\right|\leq c\varepsilon,~\forall k\ge 0.
\ean
On the other hand, noting that $\lim_{k\to \infty}b(k,i)=0$, $\forall i\in \Lambda$, it follows that there exists a constant $M_i>0$, such that $|b(k,i)|\leq \varepsilon $, $\forall k\ge M_i$. Denote $M=\max\{M_i:i\in \{1,\cdots,N\}\cap \Lambda\}$. It follows from $|b(k,i)|\leq \varepsilon,~\forall k\ge M$ that
\ban
\left|\sum_{i\in \Lambda}a(i)b(k,i)\right|\leq \left|\sum_{i\in \{1,\cdots,N\}\cap \Lambda}a(i)b(k,i)\right|+\left|\sum_{i\in \Lambda_N}a(i)b(k,i)\right|\leq \varepsilon \sum_{i\in \Lambda}a(i)+c\varepsilon,~k\ge M,
\ean
which gives (\ref{mmxxxx}).
\end{proof}


\begin{lemma}\label{lemmaA7}
Let $\X$ be a Hilbert space, $H\in \mathscr L(\X)$ be a strictly positive self-adjoint operator, $\{\mu(k),k\ge 0\}$ be a real sequence monotonically decreasing to $0$ with $\sum_{k=0}^{\infty}\mu(k)=\infty$. There exist positive constants $M$ and $d$, such that
\ban
\sup_{s\ge 0}\left\|\left(\prod_{j=t}^s(I_{\X}-\mu(j)H)\right)x\right\|\leq M^d\|x\|,~\forall x\in \X,~\forall t\ge 0,
\ean
and
\ban
\lim_{k\to \infty}\left(\prod_{j=0}^k(I_{\X}-\mu(j)H)\right)x=0,~\forall x \in \X.
\ean
\end{lemma}

\begin{proof}
Noting that $H\in \mathscr L(\X)$ is the bounded self-adjoint operator, it is shown that $H$ has the following spectral decomposition
\ban
H=\int_{-\infty}^{+\infty}\lambda \dd p_{\lambda}=\int_{\sigma (H)}\lambda \dd p_{\lambda}.
\ean
From the property of the self-adjoint operator, we have
\bna\label{nvmsacc}
\prod_{j=0}^{k
}(I_{\X}-\mu(j)H)^2=\int_{\sigma(H)}\prod_{j=0}^{k}(1-\mu(j)\lambda)^2\dd p_{\lambda},
\ena
which together with (\ref{nvmsacc}) gives
\bna\label{mmcc}
\left\|\prod_{j=0}^{k
}(I_{\X}-\mu(j)H)x\right\|^2&=&\left\langle\prod_{j=0}^{k
}(I_{\X}-\mu(j)H)^2x,x\right\rangle\cr
&=&\int_{\sigma(H)}\prod_{j=0}^{k}(1-\mu(j)\lambda)^2\dd\langle p_{\lambda}x,x\rangle,~\forall x\in \X.
\ena
It follows from $H>0$ that $\sigma(H)\subset [0,\|H\|]$, which leads to
\ban
(1-\mu(j)\lambda)^2\leq \max\left\{(1-\mu(j)\|H\|)^2,1\right\},~\forall \lambda \in \sigma(H).
\ean
Noting that $\mu(k)\to 0,k\to \infty$, it follows that there exists an integer $d>0$ such that $\mu(j)\leq \|H\|^{-1}$, $\forall j>2d$, which further shows that $(1-b(j)\lambda)^2\leq 1,\forall j> 2d$. Denote $M=\max\{(1-\mu(j)\|H\|)^2,1:0\leq j\leq 2d\}$, we have
\ban
\prod_{j=t}^s(1-\mu(j)\lambda)^2\leq M^{2d},~\forall \lambda \in \sigma(H),~\forall t\ge 0,
\ean
and
\bna\label{hhkds}
\left\|\prod_{j=t}^{s
}(I_{\X}-\mu(j)H)x\right\|^2\leq M^{2d}\int_{\sigma(H)}\dd\langle p_{\lambda}x,x\rangle =M^{2d}\|x\|^2,~\forall x\in \X,~\forall t\ge 0.
\ena
By (\ref{hhkds}), we get
\bna\label{nslcs}
\left\|\prod_{j=t}^{s
}(I_{\X}-\mu(j)H)x\right\|\leq M^{d}\|x\|,~\forall x\in \X,~\forall t\ge 0.
\ena
Noting the inequality $1-a\leq \text{e}^{-a}$, $\forall a\ge 0$, we obtain
\ban
\prod_{j=0}^{k}(1-\mu(j)\lambda)^2\leq M^{2d}\text{e}^{-2\lambda\sum_{j=2d+1}^{k}\mu(j)}.
\ean
It follows from $\sum_{j=0}^{\infty}\mu(k)=\infty$ that
\bna\label{ttkktt}
\lim_{k\to \infty}\prod_{j=0}^{k}(1-\mu(j)\lambda)^2=0,~\forall \lambda\in \sigma(H)\cap \mathbb R^+.
\ena
Since $H$ is the strictly positive self-adjoint operator, it follows that $0$ is not in the point spectrum of the operator $H$, which gives
\ban
\int_{\sigma(H)}\prod_{j=0}^{k}(1-\mu(j)\lambda)^2\dd\langle p_{\lambda}x,x\rangle =\int_{\sigma(H)\cap \mathbb R^+}\prod_{j=0}^{k}(1-\mu(j)\lambda)^2\dd\langle p_{\lambda}x,x\rangle,~\forall x\in \X.
\ean
By (\ref{mmcc}), (\ref{nslcs}), (\ref{ttkktt}) and the dominated convergence theorem, we get
\ban
\lim_{k\to \infty}\left\|\prod_{j=0}^{k
}(I_{\X}-\mu(j)H)x\right\|^2=\int_{\sigma(H)\cap \mathbb R^+}\lim_{k\to\infty}\prod_{j=0}^{k}(1-\mu(j)\lambda)^2\dd\langle p_{\lambda}x,x\rangle=0,~\forall x\in \X.
\ean
\end{proof}


\begin{lemma}\label{lemmaA8}
Let $\X$ be a Hilbert space, $H\in \mathscr L(\X)$ be a strictly positive self-adjoint operator, and $\{x(k),k\ge 0\}$ be a $L_2$-bounded random sequence with values in Hilbert space. If $\{\mu(k),k\ge 0\}$ and $\{\gamma(k),k\ge 0\}$ are both real sequences monotonically decreasing to $0$ with $\sum_{k=0}^{\infty}\mu(k)=\infty$ and $\sum_{k=0}^{\infty}\gamma(k)<\infty$, then
\ban
\lim_{k\to\infty}\sum_{i=0}^k\gamma(i)\left(\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X}-\mu(j)H)\right)x(i)\right\|^2\right]\right)^{\frac{1}{2}}=0.
\ean
\end{lemma}

\begin{proof}
On the one hand, denote
\ban
b(k,i)=\left(\E\left[\left\|\prod_{j=i+1}^k(I_{\X}-\mu(j)H)x(i)\right\|^2\right]\right)^{\frac{1}{2}},~\forall k,i\ge 0,
\ean
it follows from Lemma \ref{lemmaA7} that
\bna\label{rrkkrr}
0\leq b(k,i)\leq M^d\sup_{i\ge 0}\left(\E\left[\|x(i)\|^2\right]\right)^{\frac{1}{2}}<\infty,~\forall k,i\ge 0,
\ena
and
\ban
\left\|\prod_{j=i+1}^k(I_{\X}-\mu(j)H)x(i)\right\|\leq M^d\|x(i)\|~\text{a.s.},~\forall i\ge 0.
\ean
Thus, by the Lebesgue dominated convergence theorem and Lemma \ref{lemmaA7}, we get
\bna\label{rrkkll}
\lim_{k\to\infty}\E\left[\left\|\prod_{j=i+1}^k(I_{\X}-\mu(j)H)x(i)\right\|^2\right]=0,~\forall i\ge 0,
\ena
which further gives $\lim_{k\to\infty}b(k,i)=0$, $\forall i\ge 0$. Noting that $\sum_{k=0}^{\infty}\gamma(k)<\infty$, by (\ref{rrkkrr})-(\ref{rrkkll}) and Lemma \ref{lemma6}, we obtain
\ban
\lim_{k\to\infty}\sum_{i=0}^k\gamma(i)\left(\E\left[\left\|\left(\prod_{j=i+1}^k(I_{\X}-\mu(j)H)\right)x(i)\right\|^2\right]\right)^{\frac{1}{2}}=0.
\ean
\end{proof}


\begin{lemma}\label{lemmaA10}
Let $\mathcal A=[a_{ij}]\in \mathbb R^{N\times N}$ be the adjacency matrix of an undirected connected graph $\G$, $\L_{\G}$ be the Laplacian matrix, $H_i\in \mathscr L(\X)$, $i=1,\cdots,N$ are positive self-adjoint operators. If
\bna\label{lcms}
\sum_{i=1}^NH_i>0,
\ena
then $\text{diag}\{H_1,\cdots,H_N\}+\L_{\G}\otimes I_{\X}>0$.
\end{lemma}

\begin{proof}
Given the non-zero element $x=(x_1,\cdots,x_N)$ with values in Hilbert space $\X^N$, where $x_i\in \X$, $i=1,\cdots,N$. Here, we will prove $\langle (\text{diag}\{H_1,\cdots,H_N\}+\L_{\G}\otimes I_{\X})x,x\rangle _{\X^N}>0$ in two steps.
\begin{itemize}
\item If there exists an non-zero element $a\in \X$ with $x_1=\cdots=x_N=a$, then $x=\textbf{1}_N\otimes a$. Noting that $\L_{\G}\textbf{1}_N=0$, it follows from (\ref{lcms}) that
\bna\label{ofuncc}
&&~~~\left\langle \left(\text{diag}\{H_1,\cdots,H_N\}+\L_{\G}\otimes I_{\X}\right)x,x\right\rangle _{\X^N}\cr
&&=\left\langle \left(\text{diag}\{H_1,\cdots,H_N\}+\L_{\G}\otimes I_{\X}\right)\left(\textbf{1}_N\otimes a\right),\left(\textbf{1}_N\otimes a\right)\right\rangle _{\X^N}\cr
&&=\langle \text{diag}\{H_1,\cdots,H_N\}\left(\textbf{1}_N\otimes a\right),\left(\textbf{1}_N\otimes a\right)\rangle _{\X^N}\cr
&&+\langle (\L_{\G}\otimes I_{\X})\left(\textbf{1}_N\otimes a\right),\left(\textbf{1}_N\otimes a\right)\rangle _{\X^N}\cr
&&=\left\langle \left(\sum_{i=1}^NH_i\right)a,a\right\rangle _{\X}+\langle (\L_{\G}\textbf{1}_N)\otimes a,\left(\textbf{1}_N\otimes a\right) \rangle _{\X^N}\cr
&&=\left\langle \left(\sum_{i=1}^NH_i\right)a,a\right\rangle _{\X}\cr
&&>0.
\ena
\item If there exist $1\leq i_0\neq j_0\leq N$, such that $x_{i_0}\neq x_{j_0}$. It follows from $H_i\ge 0$, $i=1,\cdots,N$ that $\text{diag}\{H_1,\cdots,H_N\}\ge 0$. Noting that the graph is undirected and connected, it is shown that $a_{ij}=a_{ji}>0$, $1\leq i\neq j\leq N$. Thus, we get
\bna\label{ofunc}
&&~~~\left\langle \left(\text{diag}\{H_1,\cdots,H_N\}+\L_{\G}\otimes I_{\X}\right)x,x\right\rangle _{\X^N}\cr
&&\ge \langle (\L_{\G}\otimes I_{\X})x,x\rangle _{\X^N}\cr
&&=\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Na_{ij}\|x_i-x_j\|^2_{\X}\cr
&&\ge \frac{1}{2}a_{i_0j_0}\|x_{i_0}-x_{j_0}\|_{\X}^2\cr
&&>0.
\ena
\end{itemize}
Combining (\ref{ofuncc})-(\ref{ofunc}) leads to the fact that $\text{diag}\{H_1,\cdots,H_N\}+\L_{\G}\otimes I_{\X}\in \mathscr L(\X^N)$ is strictly positive.
\end{proof}


\begin{lemma}\label{lemmaA11}
Let $\{a(k),k\ge 0\}$ and $\{b(k),k\ge 0\}$ be monotonically decreasing sequences of positive real numbers. If
\bna\label{tiaojian}
\max\left\{a(k)-a(k+1),b(k)-a(k)\right\}=\mathcal O\left(a^2(k)+b^2(k)\right),
\ena
then
\bna\label{fulu11}
\max_{ih\leq s\leq (i+1)h-1}\left(\frac{1}{h}\left(\sum_{s=ih}^{(i+1)h-1}b(s)\right)-a(s)\right)^2=\mathcal O \left(a^4(i)+b^4(i)\right),~\forall h\in \mathbb N^+.
\ena
\end{lemma}

\begin{proof}
Given the integer $h>0$. Noting that $\{a(k),k\ge 0\}$ and $\{b(k),k\ge 0\}$ are both monotonically decreasing sequences, it follows that
\ban
\begin{cases}
\displaystyle \frac{1}{h}\left(\sum_{s=ih}^{(i+1)h-1}b(s)\right)\in [b((i+1)h-1),b(ih)],\\
\displaystyle a(s)\in [a((i+1)h-1),a(ih)],~ih\leq s\leq (i+1)h-1,
\end{cases}
\ean
from which we obtain
\bna\label{fulu0}
&&~~~\max_{ih\leq s\leq (i+1)h-1}\left(\frac{1}{h}\left(\sum_{s=ih}^{(i+1)h-1}b(s)\right)-a(s)\right)^2\cr
&&\leq \max_{ih\leq s\leq (i+1)h-1}\left\{(b((i+1)h-1)-a(s))^2,(b(ih)-a(s))^2\right\}\cr
&&\leq \max\big\{(b((i+1)h-1)-a(ih))^2,(b((i+1)h-1)-a((i+1)h-1))^2,\cr
&&~~~~~~~~~~~~(b(ih)-a(ih))^2,(b(ih)-a((i+1)h-1))^2\big\}.
\ena
It follows from (\ref{tiaojian}) that there exists a constant $C>0$, such that
\bna\label{fnlwlmv}
\max\left\{(a(k)-a(k+1))^2,(b(k)-a(k))^2\right\}\leq C\left(a^4(k)+b^4(k)\right),
\ena
which gives
\bna\label{fulu1}
&&\max\left\{(b((i+1)h-1)-a((i+1)h-1))^2,(b(ih)-a(ih))^2\right\}\cr
&&\leq C\left(a^4(i)+b^4(i)\right).
\ena
Combining (\ref{fnlwlmv}) and the monotonicity of sequences $\{a(k),k\ge 0\}$ and $\{b(k),k\ge 0\}$ leads to
\bna\label{fulu2}
&&~~~\left(b((i+1)h-1)-a(ih)\right)^2\cr
&&=\left(b((i+1)h-1)-a((i+1)h-1)+a((i+1)h-1)-a(ih)\right)^2\cr
&&=\left(b((i+1)h-1)-a((i+1)h-1)+\sum_{s=ih}^{(i+1)h-2}(a(s+1)-a(s))\right)^2\cr
&&\leq h\left((b((i+1)h-1)-a((i+1)h-1))^2+\sum_{s=ih}^{(i+1)h-2}(a(s)-a(s+1))^2\right)\cr
&&\leq h\left(C\left(a^4((i+1)h-1)+b^4((i+1)h-1)\right)+C\sum_{s=ih}^{(i+1)h-2}\left(a^4(s)+b^4(s)\right)\right)\cr
&&\leq Ch^2\left(a^4(i)+b^4(i)\right).
\ena
Similarly, we obtain
\bna\label{fulu3}
&&~~~\left(a((i+1)h-1)-b(ih)\right)^2\cr
&&=\left(b(ih)-a((i+1)h-1)\right)^2\cr
&&=\left(b(ih)-a(ih)+a(ih)-a((i+1)h-1)\right)^2\cr
&&=\left(b(ih)-a(ih)+\sum_{s=ih}^{(i+1)h-2}(a(s)-a(s+1))\right)^2\cr
&&\leq h\left((b(ih)-a(ih))^2+\sum_{s=ih}^{(i+1)h-2}(a(s)-a(s+1))^2\right)\cr
&&\leq h\left(C\left(a^4(ih)+b^4(ih)\right)+C\sum_{s=ih}^{(i+1)h-2}\left(a^4(s)+b^4(s)\right)\right)\cr
&&\leq Ch^2\left(a^4(i)+b^4(i)\right).
\ena
Combining (\ref{fulu0}) and (\ref{fulu1})-(\ref{fulu3}) gives (\ref{fulu11}).
\end{proof}

\end{appendix}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Single Appendix:                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{appendix}
%\section*{???}%% if no title is needed, leave empty \section*{}.
%\end{appendix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Multiple Appendixes:                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{appendix}
%\section{???}
%
%\section{???}
%
%\end{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Support information, if any,             %%
%% should be provided in the                %%
%% Acknowledgements section.                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{acks}[Acknowledgments]
% The authors would like to thank ...
%\end{acks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Funding information, if any,             %%
%% should be provided in the                %%
%% funding section.                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{funding}
% The first author was supported by ...
%
% The second author was supported in part by ...
%\end{funding}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Supplementary Material, including data   %%
%% sets and code, should be provided in     %%
%% {supplement} environment with title      %%
%% and short description. It cannot be      %%
%% available exclusively as external link.  %%
%% All Supplementary Material must be       %%
%% available to the reader on Project       %%
%% Euclid with the published article.       %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{supplement}
%\stitle{???}
%\sdescription{???.}
%\end{supplement}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  imsart-???.bst  will be used to                        %%
%%  create a .BBL file for submission.                     %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%  MR numbers will be added by VTeX.                      %%
%%                                                         %%
%%  Use \cite{...} to cite references in text.             %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% if your bibliography is in bibtex format, uncomment commands:
%\bibliographystyle{imsart-number} % Style BST file (imsart-number.bst or imsart-nameyear.bst)
%\bibliography{bibliography}       % Bibliography file (usually '*.bib')

%% or include bibliography directly:
% \begin{thebibliography}{}
% \bibitem{b1}
% \end{thebibliography}
\begin{thebibliography}{99}
\bibitem{bertero}
\textsc{Bertero, M.} and \textsc{Boccacci, P.} (1998). \textit{Introduction to inverse problems in imaging}. CRC press.

\bibitem{cakoni}
\textsc{Cakoni, G.} and \textsc{Colton, D.} (2005). Open problems in the qualitative approach to inverse electromagnetic scattering theory. \textit{European Journal of Applied Mathematics.} \textbf{16} 411--425.
\bibitem{colton}
\textsc{Colton, D.} and \textsc{Kress, R.} (2012). \textit{Inverse acoustic and electromagnetic scattering theory} \textbf{93}. Springer Science and Business Media.
%\bibitem{groetsch}
%C. W. Groetsch, \emph{Inverse problems in the mathematical sciences}, Vieweg Mathematics for Scientists and Engineers. Friedr. Vieweg and Sohn, Braunschweig, 1993.
\bibitem{isakov}
\textsc{Isakov, V.} (2008). On inverse problems in secondary oil recovery. \textit{European Journal of Applied Mathematics.} \textbf{19} 459--478.
%\bibitem{tarantola}
%A. Tarantola and B. Valette, \emph{Inverse problems=quest for information}, J. geophys, 50 (1982), pp. 150170.
\bibitem{engl}
\textsc{Engl, H. W.}, \textsc{Louis, A. K.} and \text{Rundell, W.} (1996). Inverse problems in medical imaging and nondestructive testing. \textit{Proceeding of the Conference in Oberwolfach.} Federal Republic of Germany, Feb. 4--10.
\bibitem{Kirsch111}
\textsc{Kirsch, A.} (1996). \textit{An Introduction to the mathematical theory of inverse problems}. New York: Springer-Verlag.
\bibitem{Engl123}
\textsc{Engl, H. W.}, \textsc{Hanke, M.} and \textsc{Neubauer, A.} (1996).  \textit{Regularization of inverse problems} \textbf{375}. Springer Science and Business Media.
\bibitem{Morozov123}
\textsc{Morozov, V. A.} (1984). Regular methods for solving linear and nonlinear ill-posed problems,'' in  \textit{Methods for Solving Incorrectly Posed Problems} 65--122, Springer, New York.
\bibitem{Werschulz}
\textsc{Werschulz, A. G.} (1991). \textit{The computational complexity of differential and integral equations: An information-based approach}. Oxford University Press, Inc.
\bibitem{Tikhonov123}
\textsc{Tikhonov, A. N.} (1963). Regularization of incorrectly posed problems. \textit{Soviet Mathematics Doklady.} \textbf{4} 1624--1627.
\bibitem{Bissantz}
\textsc{Bissantz, N.}, \textsc{Hohage, T.} and \textsc{Munk, A.} (2004). Consistency and rates of convergence of nonlinear Tikhonov regularization with random noise. \textit{Inverse Problems,} \textbf{20} 1773.
%\bibitem{Bissantz1}
%N. Bissantz, T. Hohage, A. Munk, and F. Ruymgaart, \emph{Convergence rates of general regularization methods for statistical inverse problems and applications}, SIAM Journal on Numerical Analysis, 45 (2007), pp. 26102636.
\bibitem{Cavalier}
\textsc{Cavalier, L.} (2008). Nonparametric statistical inverse problems. \textit{Inverse Problems.} \textbf{24} 034004.
\bibitem{Kekkonen}
\textsc{Kekkonen, H.}, \textsc{Lassas, M.} and \textsc{Siltanen, S.} (2014). Analysis of regularized inversion of data corrupted by white Gaussian noise. \textit{Inverse Problems.}  \textbf{30} 045009.
\bibitem{Gine}
\textsc{Gine, E.} and \textsc{Nickl, R.} (2015). \textit{Mathematical foundations of infinite-dimensional statistical models}  \textbf{40}. Cambridge University Press.
\bibitem{Hohage}
\textsc{Hohage, T.} and \text{Werner, F.} (2016). Inverse problems with poisson data: statistical regularization theory, applications and algorithms. \textit{Inverse Problems.}  \textbf{32} 093001.
%\bibitem{Math}
%Math, P., \& Pereverzev, S. V. (2001). \emph{Optimal discretization of inverse problems in Hilbert scales. Regularization and self-regularization of projection methods}. SIAM Journal on Numerical Analysis, 38(6), 1999-2021.
\bibitem{Math1}
\textsc{Mathe, P.} and \textsc{Pereverzev, S.} (2006). Regularization of some linear ill-posed problems with discretized random noisy data. \textit{Mathematics of Computation.} \textbf{75} 1913--1929.
\bibitem{Lu}
\textsc{Lu, S.} and \textsc{Mathe, P.} (2014). Discrepancy based model selection in statistical inverse problems. \textit{Journal of Complexity.} \textbf{30} 290--308.
%\bibitem{Healy}
%Healy Jr, D. M., Hendriks, H., \& Kim, P. T. (1998). emph{Spherical deconvolution}. Journal of Multivariate Analysis, 67(1), 1-22.
%\bibitem{Borcea}
%Borcea, L. (2002). \emph{Electrical impedance tomography}. Inverse problems, 18(6), R99.
\bibitem{Silva}
\textsc{Silva, J. A.}, \textsc{Faria, E. R.}, \textsc{Barros, R. C.}, \textsc{Hruschka, E. R.}, \textsc{Carvalho, A. C. D.} and \textsc{Gama, J.} (2013). Data stream clustering: A survey. \textit{ACM Computing Surveys (CSUR).} \textbf{46} 1--31.
\bibitem{Jiang}
\textsc{Jiang, N.} and \textsc{Gruenwald, L.} (2006). Research issues in data stream association rule mining. \textit{ACM Sigmod Record.} \textbf{35} 14--19.

%\bibitem{Iglesias}
%Iglesias, M. A., Law, K. J., \& Stuart, A. M. (2013). \emph{Ensemble Kalman methods for inverse problems}. Inverse Problems, 29(4), 045001.
\bibitem{Iglesias1}
\textsc{Iglesias, M. A.}, \textsc{Lin, K.}, \textsc{Lu, S.} and \textsc{Stuart, A. M.} (2017). Filter based methods for statistical linear inverse problems. \textit{Communications in Mathematical Sciences.} \textbf{15} 1867--1896.
\bibitem{Lu1}
\textsc{Lu, S.}, \textsc{Niu, P.} and \textsc{Werner, F.} (2021). On the asymptotical regularization for linear inverse problems in presence of white noise.  \textit{SIAM/ASA Journal on Uncertainty Quantification.} \textbf{9} 1--28.
\bibitem{Jonesfg}
\textsc{Jones, F. G.} and \textsc{Simpson, G.} (2022). Iterate averaging, the Kalman filter, and 3DVAR for linear inverse problems. \textit{Numerical Algorithms.} 1--21.
\bibitem{WLZ}
\textsc{Wang, J.}, \textsc{Li, T.} and \textsc{Zhang, X.} (2021). Decentralized Cooperative Online Estimation With Random Observation Matrices, Communication Graphs and Time Delays. \textit{IEEE Transactions on Information Theory.} \textbf{67} 4035--4059.
\bibitem{Gander}
\textsc{Gander, L.}, \textsc{Krause, R.}, \textsc{Multerer, M.} and \textsc{Pezzuto, S.} (2020). Space-time shape uncertainties in the forward and inverse problem of electrocardiography. arXiv:2010.16104.
\bibitem{SmaleYao}
\textsc{Smale, S.} and \text{Yao, Y.} (2006). Online learning algorithms.  \textit{Foundations of Computational Mathematics.} \textbf{6} 145--170.
%\bibitem{Cucker}
%Cucker, F., \& Zhou, D. X. (2007). \emph{Learning theory: an approximation theory viewpoint} (Vol. 24). Cambridge University Press.
%\bibitem{Vapnik}
%Vapnik, V. (2013). \emph{The nature of statistical learning theory}. Springer science \& business media.
\bibitem{Ippel}
\textsc{Ippel, L.}, \textsc{Kaptein, M.} and \textsc{Vermunt, J.} (2016). Dealing with data streams: An online, row-by-row, estimation tutorial. \textit{Methodology: European Journal of Research Methods for the Behavioral and Social Sciences.} \textbf{12} 124.
%\bibitem{Murata}
%Murata, N., Kawanabe, M., Ziehe, A., Mller, K. R., \& Amari, S. I. (2002). \emph{On-line learning in changing environments with applications in supervised and unsupervised learning}. Neural Networks, 15(4-6), 743-760.
\bibitem{Tarres}
\textsc{Tarres, P.} and \textsc{Yao, Y.} (2014). Online learning as stochastic approximation of regularization paths: Optimality and almost-sure convergence.  \textit{IEEE Transactions on Information Theory.} \textbf{60} 5716--5735.
\bibitem{Dieuleveut}
\textsc{Dieuleveut, A.} and \textsc{Bach, F.} (2016). Nonparametric stochastic approximation with large step-sizes. \textit{The Annals of Statistics.} \textbf{44} 1363--1399.
%\bibitem{Tan}
%\textsc{Tan, F.} (2020). The algorithms of distributed learning and distributed estimation about intelligent wireless sensor network. \textit{Sensors.} \textbf{20} 1302.
\bibitem{Lopes}
\textsc{Lopes, C. G.} and \textsc{Sayed, A. H.} (2008). Diffusion least-mean squares over adaptive networks: Formulation and performance analysis. \textit{IEEE Transactions on Signal Processing.} \textbf{56} 3122--3136.
\bibitem{Cattivelli}
\textsc{Cattivelli, F. S.} and \textsc{Sayed, A. H.} (2010). Diffusion strategies for distributed Kalman filtering and smoothing. \textit{IEEE Transactions on Automatic Control.} \textbf{55} 2069--2084.
\bibitem{Sayed}
\textsc{Al-Sayed, S.}, \textsc{Zoubir, A. M.} and \textsc{Sayed, A. H.} (2017). Robust distributed estimation by networked agents. \textit{IEEE Transactions on Signal Processing.} \textbf{65} 3909--3921.
\bibitem{Gholami}
\textsc{Gholami, M. R.}, \textsc{Jansson, M.}, \textsc{Str\"{o}m, E. G.} and \textsc{Sayed, A. H.} (2016). Diffusion estimation over cooperative multi-agent networks with missing data. \textit{IEEE Transactions on Signal and Information Processing over Networks.} \textbf{2} 276--289.
\bibitem{Abdolee}
\textsc{Abdolee, R.}, \textsc{Champagne, B.} and \textsc{Sayed, A. H.} (2016). Diffusion adaptation over multi-agent networks with wireless link impairments.  \textit{IEEE Transactions on Mobile Computing.} \textbf{15} 1362--1376.
\bibitem{Piggott}
\textsc{Piggott, M. J.} and \textsc{Solo, V.} (2016). Diffusion LMS with correlated regressors I: Realization-wise stability. \textit{IEEE Transactions on Signal Processing.} \textbf{64} 5473--5484.
\bibitem{Piggott1}
\textsc{Piggott, M. J.} and \textsc{Solo, V.} (2017). Diffusion LMS with correlated regressors II: Performance. \textit{IEEE Transactions on Signal Processing.} \textbf{65} 3934--3947.
\bibitem{Ishihara}
\textsc{Ishihara, J. Y.} and \textsc{Alghunaim, S. C.} (2017). Diffusion LMS filter for distributed estimation of systems with stochastic state transition and observation matrices. \textit{Proceeding of 2017 American Control Conference (ACC).} May. 24, 5199--5204.
\bibitem{Lyaqini}
\textsc{Lyaqini, S.}, \textsc{Quafafou, M.}, \textsc{Nachaoui, M.} and \textsc{Chakib, A.} (2020). Supervised learning as an inverse problem based on non-smooth loss function. \textit{Knowledge and Information Systems.} \textbf{62} 3039--3058.
%\bibitem{Nassif}
%\textsc{Nassif, R.}, \textsc{Vlaski, S.}, \textsc{Richard, C.}, \textsc{Chen, J.} and \textsc{Sayed, A. H.} (2020). Multitask learning over graphs: An approach for distributed, streaming machine learning. \textit{IEEE Signal Processing Magazine.} \textbf{37} 14--25.
\bibitem{Shin}
\textsc{Shin, B.}, \textsc{Yukawa, M.}, \textsc{Cavalcante, R. L. G.} and \textsc{Dekorsy, A.} (2018). Distributed Adaptive Learning With Multiple Kernels in Diffusion Networks. \textit{IEEE Transactions on Signal Processing.} \textbf{66} 5505--5519.
%\bibitem{Anx}
%An, X., Hu, C., Liu, G., \& Lin, H. (2021). \emph{Distributed Online Gradient Boosting on Data Stream over Multi-agent Networks}. Signal Processing, 108253.
\bibitem{Deng}
\textsc{Deng, Z.}, \textsc{Gregory, J.} and \textsc{Kurdila, A.} (2012). Learning theory with consensus in reproducing kernel Hilbert spaces. \textit{Proceeding of 2012 American Control Conference (ACC)}. Jun. 27, 1400--1405.
\bibitem{Mitra}
\textsc{Mitra, R.} and \textsc{Bhatia, V.} (2014). The diffusion-KLMS algorithm.  \textit{Proceeding of 2014 International Conference on Information Technology.} Dec. 22, 256--259.
\bibitem{Chouvardas}
\textsc{Chouvardas, R.} and \textsc{Draief, M.} (2016). A diffusion kernel LMS algorithm for nonlinear adaptive networks. \textit{Proceeding of 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).} Mar. 20, 4164--4168.
\bibitem{Bouboulis}
\textsc{Bouboulis, P.}, \textsc{Chouvardas, S.} and \textsc{Theodoridis, S.} (2018). Online distributed learning over networks in RKH spaces using random Fourier features. \textit{IEEE Transactions on Signal Processing.} \textbf{66} 1920--1932.
\bibitem{GUO}
\textsc{Guo, Z. C.}, \textsc{Lin, S. B.} and \textsc{Shi, L.} (2019). Distributed learning with multi-penalty regularization. \textit{Applied and Computational Harmonic Analysis.} \textbf{46} 478--499.
\bibitem{GUO1}
\textsc{Guo, Z. C.}, \textsc{Lin, S. B.} and \textsc{Zhou, D. X.} (2017). Learning theory of distributed spectral algorithms. \textit{Inverse Problems.} \textbf{33} 074009.
\bibitem{Lin}
\textsc{Lin, J.} and \textsc{Cevher, V.} (2020). Optimal convergence for distributed learning with stochastic gradient methods and spectral algorithms. \textit{Journal of Machine Learning Research.} \textbf{21} 1--63.
\bibitem{ZLG}
\textsc{Zhang, X.}, \textsc{Li, T.} and \textsc{Gu, Y.} (2020). Consensus+ innovations distributed estimation with random network graphs, observation matrices and noises. \textit{Proceeding of 2020 59th IEEE Conference on Decision and Control (CDC).} Dec. 14. 4318--4323.
%\bibitem{GUO2}
%Guo, L. (1990). \emph{Estimating time-varying parameters by the Kalman filter based algorithm: stability and convergence}. IEEE Transactions on Automatic Control, 35(2), 141-147.
\bibitem{Ying}
\textsc{Ying, Y.}, and \textsc{Pontil, M.} (2008). Online gradient descent learning algorithms. \textit{Foundations of Computational Mathematics.} \textbf{8} 561--596.
%\bibitem{Das}
%\textsc{Das, S.} and \textsc{Moura, J. M.} (2016). Consensus+ innovations distributed Kalman filter with optimized gains. \textit{IEEE Transactions on Signal Processing.} \textbf{65} 467--481.
\bibitem{Lei}
\textsc{Lei, Y.}, \textsc{Shi, L.} and \textsc{Guo, Z. C.} (2018). Convergence of unregularized online learning algorithms. \textit{Journal of Machine Learning Research.} \textbf{18} 6269--6301.
\bibitem{Poggio}
\textsc{Poggio, T.} and \textsc{Shelton, R. C.} (2002). On the mathematical foundations of learning. \textit{American Mathematical Society.}  \textbf{39} 1--49.
\bibitem{Janz}
\textsc{Janz, D.}, \textsc{Burt, D.} and \textsc{Gonzalez, J.} (2020). Bandit optimisation of functions in the Matern kernel RKHS. \textit{Proceeding of  International Conference on Artificial Intelligence and Statistics.} Jun. 3. 2486--2495.
\bibitem{Takemori}
\textsc{Takemori, S.} and \textsc{Sato, M.} (2021). Approximation theory based methods for RKHS bandits. \emph{Proceeding of International Conference on Machine Learning.} Jul. 1. 10076-10085.
\bibitem{Rosenblatt}
\textsc{Rosenblatt, J. D.} and \textsc{Nadler, B.} (2016). On the optimality of averaging in distributed statistical learning. \textit{Information and Inference: A Journal of the IMA.} \textbf{5} 379--404.
\bibitem{Lu2}
\textsc{Lu, S.}, \textsc{Mathe, P.} and \textsc{Pereverzev, S. V.} (2020). Randomized matrix approximation to enhance regularized projection schemes in inverse problems. \textit{Inverse Problems.} \textbf{36} 085013.
%\bibitem{Hanke3}
%\textsc{Hanke, M.}, \textsc{Neubauer, A.} and \textsc{Scherzer, O.} (1995). A convergence analysis of the Landweber iteration for nonlinear ill-posed problems.  \textit{Numerische Mathematik.} \textbf{72} 21--37.
%\bibitem{Bakushinskii111}
%\textsc{Bakushinskii, A. B.} (1992). The problem of the convergence of the iteratively regularized Gauss-Newton method.  \textit{Computational mathematics and mathematical physics.} \textbf{32} 1353--1359.
%\bibitem{Kindermann}
%\textsc{Kindermann, S.} (2021). Optimal-order convergence of Nesterov acceleration for linear ill-posed problems. \textit{Inverse Problems.} \textbf{37} 065002.
\bibitem{hp}
\textsc{Hille, E.} and \textsc{Phillips, R. C.} (1996). Functional analysis and semi-groups. \textit{American Mathematical Society.} \textbf{31}.
\bibitem{hy}
\textsc{Hyt\"{o}nen. T.}, \textsc{Van Neerven, J.}, \textsc{Veraar, M.} and \textsc{Weis, L.} (2016). \textit{Analysis in Banach spaces.} \textbf{12} Berlin: Springer.
\bibitem{hy2}
\textsc{Hyt\"{o}nen. T.}, \textsc{Van Neerven, J.}, \textsc{Veraar, M.} and \textsc{Weis, L.} (2018). \textit{Analysis in Banach Spaces: Volume II: Probabilistic Methods and Operator Theory.} \textbf{67} Springer.
\bibitem{chow}
\textsc{Chow, Y. S.} and \textsc{Teicher, H.} (2012). \textit{Probability theory: independence, interchangeability, martingales.} Springer Science and Business Media.
\bibitem{Olfati}
\textsc{Olfati-Saber, R.} and \textsc{Murray, R. M.} (2004). Consensus problems in networks of agents with switching topology and time-delays. \textit{IEEE Transactions on Automatic Control.} \textbf{49} 1520--1533.
\bibitem{rb}
\textsc{Robbins, H.} and \textsc{Siegmund, D.} (1971). A convergence theorem for non-negative almost supermartingales and some applications. \textit{Optimizing Methods in Statistics.} 233--257, Academic Press.
\bibitem{sl}
\textsc{Schmitt, U.} and \textsc{Louis, A. K.} (2002). Efficient algorithms for the regularization of dynamic inverse problems: I. Theory.  \textit{Inverse Problems.}  \textbf{18} 645.
\bibitem{Benning}
\textsc{Benning, M.} and \textsc{Burger, M.} (2018). Modern regularization methods for inverse problems. \textit{Acta Numerica.} \textbf{27} 1--111.
\bibitem{Ungureanu1}
\textsc{Ungureanu, V. M.} and \textsc{Cheng, S. S.} (2008). Mean stability of a stochastic difference equation. \textit{Proceeding of Annales Polonici Mathematici.} \textbf{1} 33--52. Institute of Mathematics Polish Academy of Sciences.
\bibitem{Kubrusly}
\textsc{Kubrusly, S. C.} (1978). Applied stochastic approximation algorithms in Hilbert space. \textit{International Journal of Control.} \textbf{28} 23--31.
\bibitem{Vajjha}
\textsc{Vajjha, K.}, \textsc{Trager, B.}, \textsc{Shinnar, A.} and \textsc{Pestun, V.} (2022). Formalization of a Stochastic Approximation Theorem. arXiv:2202.05959.
\bibitem{Ungureanu2}
\textsc{Ungureanu, V. M.} (2014). Stability, stabilizability and detectability for Markov jump discrete-time linear systems with multiplicative noise in Hilbert spaces. \textit{Optimization.} \textbf{63} 1689--1712.
\bibitem{Ungureanu3}
\textsc{Ungureanu, V. M.} (2009). Optimal control for linear discrete-time systems with Markov perturbations in Hilbert spaces.  \textit{IMA Journal of Mathematical Control and Information.} \textbf{26} 105--127.
\bibitem{zwzwxcbs}
\textsc{Zhang, W.}, \textsc{Zheng, W. X.} and \textsc{Chen, B. S.} (2017).  Detectability, observability and Lyapunov-type theorems of linear discrete time-varying stochastic systems with multiplicative noise. \textit{International Journal of Control.} \textbf{90} 2490--2507.
\bibitem{Lu100}
\textsc{Lu. S.} and \textsc{Mathe, P.} (2017). Stochastic gradient descent for linear inverse problems in Hilbert spaces. \textit{Mathematics of Computation.} \textbf{91} 1763--1788.
\bibitem{Reich1}
\textsc{Reich, S.} and \textsc{Zaslavski, A. J.} (2019). A random weak ergodic property of infinite products of operators in metric spaces.  \textit{Optimization.} \textbf{68} 51--63.
\bibitem{Reich2}
\textsc{Reich, S.} and \textsc{Zaslavski, A. J.} (2013). Asymptotic behavior of inexact infinite products of nonexpansive mappings in metric spaces.  \textit{Zeitschrift f\"{u}r Analysis und ihre Anwendungen.} \textbf{33} 101--117.
\bibitem{Pustylnik1}
\textsc{Pustylnik, E.}, \textsc{Reich. S.} and \textsc{Zaslavski, A. J.} (2009).  Inexact infinite products of nonexpansive mappings. \textit{Numerical Functional Analysis and Optimization.} \textbf{30} 632--645.
\bibitem{Zaslavski}
\textsc{Zaslavski, A. J.} (2021). Stable Convergence Theorems for Products of Uniformly Continuous Mappings in Metric Spaces.  \textit{Axioms.} \textbf{10} 156.
\bibitem{Butnariu}
\textsc{Butnariu, D.}, \textsc{Reich, S.} and \textsc{Zaslavski, A. J.} (2008).  Stable convergence theorems for infinite products and powers of nonexpansive mappings. \textit{Numerical Functional Analysis and Optimization.} \textbf{29} 304--323.
\bibitem{Reich3}
\textsc{Reich, S.} and \textsc{Zaslavski, A. J.} (1999). Convergence of generic infinite products of affine operators. \textit{Abstract and Applied Analysis.} \textbf{4} 1--19.
\bibitem{Pustylnik2}
\textsc{Pustylnik, E.} and \textsc{Reich, S.} (2014). Infinite products of arbitrary operators and intersections of subspaces in Hilbert space. \textit{Journal of Approximation Theory.} 91--102.
\bibitem{Pustylnik3}
\textsc{Pustylnik, E.}, \textsc{Reich, S.} and \textsc{Zaslavski, A. J.} (2011). Convergence of non-cyclic infinite products of operators. \textit{Journal of Mathematical Analysis and Applications.} \textbf{380} 759--767.
\bibitem{Pustylnik4}
\textsc{Pustylnik, E.}, \textsc{Reich, S.} and \textsc{Zaslavski, A. J.} (2012).  Convergence of non-periodic infinite products of orthogonal projections and nonexpansive operators in Hilbert space. \textit{Journal of Approximation Theory.}  \textbf{164} 611--624.
\bibitem{Blasco111}
\textsc{Blasco, O.} and \textsc{Garcia-Bayona, I.} (2016). Remarks on measurability of operator-valued functions. \textit{Mediterranean Journal of Mathematics.} \textbf{13} 5147--5162.
%\bibitem{Beck}
%Beck, A., 1976. \emph{Conditional independence.} Zeitschrift fr Wahrscheinlichkeitstheorie und Verwandte Gebiete, 33(4), pp.253-267.
\bibitem{Theodoridis}
\textsc{Theodoridis, S.} (2015). \textit{Machine learning: a Bayesian and optimization perspective.} Academic press.
\bibitem{ctfg}
\textsc{Chen, X.}, \textsc{Tang, B.}, \textsc{Fan, J.} and \textsc{Guo, X.} (2022).  Online gradient descent algorithms for functional data learning. \textit{Journal of Complexity.} \textbf{70} 101635.
\bibitem{Mathe123}
\textsc{Mathe, P.} and \textsc{Pereverzev, S. V.} (2001). Optimal discretization of inverse problems in Hilbert scales. Regularization and self-regularization of projection methods. \textit{SIAM Journal on Numerical Analysis.} \textbf{38} 1999--2021.
\bibitem{ZLF}
\textsc{Zhang, X.}, \textsc{Li, T.} and \textsc{Fu, X.} (2022). Decentralized Online Regularized Learning Over Random Time-Varying Graphs. arXiv: 2206.03861, 2022.
\bibitem{Jahn}
\textsc{Jahn, T.} and \textsc{Jin, B.} (2020). On the discrepancy principle for stochastic gradient descent. \textit{Inverse Problems.} \textbf{36} 095009.
\bibitem{JinB}
\textsc{Jin, B.} and \textsc{Lu, X.} (2018). On the regularizing property of stochastic gradient descent. \emph{Inverse Problems.} \textbf{35} 015004.
\bibitem{Guo1994}
\textsc{Guo, L.} (1994). Stability of recursive stochastic tracking algorithms. \textit{SIAM Journal on Control and Optimization.} \textbf{32} 1195--1225.
\bibitem{Guo1990}
\textsc{Guo, L.} (1990). Estimating time-varying parameters by the Kalman filter based algorithm: stability and convergence. \textit{IEEE Transactions on Automatic Control.} \textbf{35} 141--147.
\bibitem{Xieguo}
\textsc{Xie, S.} and \textsc{Guo, L.} (2018). Analysis of normalized least mean squares-based consensus adaptive filters under a general information condition.  \textit{SIAM Journal on Control and Optimization.} \textbf{56} 3404--3431.
\bibitem{GUO111}
\textsc{Guo, L.} and \textsc{Ljung, L.} (1995). Exponential stability of general tracking algorithms. \textit{IEEE Transactions on Automatic Control.} \textbf{40} 1376--1387.
\bibitem{GUO222}
\textsc{Guo, L.} and \textsc{Ljung, L.} (1995). Performance analysis of general tracking algorithms. \textit{IEEE Transactions on Automatic Control.} \textbf{40}  1388--1402.
\bibitem{GUO333}
\textsc{Guo, L.}, \textsc{Ljung, L.} and \textsc{Priouret, P.} (1993). Performance analysis of forgetting factor RLS algorithms. \textit{International Journal of Adaptive Control and Signal Processing.} \textbf{7} 525--537.
\bibitem{GUO444}
\textsc{Guo, L.}, \textsc{Ljung, L.} and \textsc{Wang, G.} (1997). Necessary and sufficient conditions for stability of LMS. \textit{IEEE Transactions on Automatic Control.} \textbf{42} 761--770.
\end{thebibliography}

\end{document}
