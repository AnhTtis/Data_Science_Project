% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
\pdfoutput=1
% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmicx}  
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{threeparttable}
\usepackage{float}
\usepackage{marvosym}
\usepackage{xcolor}
\usepackage[accsupp]{axessibility}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
% \usepackage{bbding}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\newcommand\blfootnote[1]{%
\begingroup
\renewcommand\thefootnote{}\footnote{#1}%
\addtocounter{footnote}{-1}%
\endgroup
}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
% \def\cvprPaperID{9142} % *** Enter the CVPR Paper ID here
% \def\confName{CVPR}
% \def\confYear{2023}

\renewcommand{\algorithmicrequire}{\textbf{Input}}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{PDPP: Projected Diffusion for Procedure Planning in Instructional Videos}

\author{\hspace{-5.5mm} Hanlin Wang\textsuperscript{1} \quad Yilu Wu\textsuperscript{1} \quad Sheng Guo\textsuperscript{3} \quad  Limin Wang\textsuperscript{1, 2,~\Letter} \\
\textsuperscript{1}State Key Laboratory for Novel Software Technology, Nanjing University, China \\
\textsuperscript{2}Shanghai AI Lab, China \quad
\textsuperscript{3}MYbank, Ant Group, China
}
\maketitle
\pagestyle{empty}
\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   In this paper, we study the problem of procedure planning in instructional videos, which aims to make goal-directed plans given the current visual observations in unstructured real-life videos. 
   %Specifically, given the start and end observations, a model needs to generate a sequence of actions to achieve the goal from the start state. 
   Previous works cast this problem as a sequence planning problem and leverage either heavy intermediate visual observations or natural language instructions as supervision, resulting in complex learning schemes and expensive annotation costs. In contrast, we treat this problem as a distribution fitting problem. In this sense, we model the whole intermediate action sequence distribution with a diffusion model (PDPP), and thus transform the planning problem to a sampling process from this distribution. In addition, we remove the expensive intermediate supervision, and simply use task labels from instructional videos as supervision instead. Our model is a U-Net based diffusion model, which directly samples action sequences from the learned distribution with the given start and end observations. Furthermore, we apply an efficient projection method to provide accurate conditional guides for our model during the learning and sampling process. Experiments on three datasets with different scales show that our PDPP model can achieve the state-of-the-art performance on multiple metrics, even without the task supervision.  Code and trained models are available at \href{https://github.com/MCG-NJU/PDPP}{https://github.com/MCG-NJU/PDPP}.
\end{abstract}
\blfootnote{ \Letter: Corresponding author (lmwang@nju.edu.cn).}


%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec::intro}

Instructional videos\cite{DBLP:conf/cvpr/ZhukovACFLS19,DBLP:conf/cvpr/AlayracBASLL16,DBLP:conf/cvpr/TangDRZZZL019} are strong knowledge carriers, which contain rich scene changes and various actions. People watching these videos can learn new skills by figuring out what actions should be performed to achieve the desired goals. Although this seems to be natural for humans, it is quite challenging for AI agents. Training a model that can learn how to make action plans to transform from the start state to goal is crucial for the next-generation AI system as such a model can analyze complex human behaviours and help people with goal-directed problems like cooking or repairing items. Nowadays the computer vision community is paying growing attention to the instructional video understanding~\cite{DBLP:conf/eccv/ChangHXAFN20, DBLP:journals/tip/ZhaoRTZL22, DBLP:conf/cvpr/LiangWZY22, DBLP:conf/eccv/DvornikHPBMFJ22, DBLP:conf/nips/DvornikHDGJ21}. Among them, Chang \etal \cite{DBLP:conf/eccv/ChangHXAFN20} proposed a problem named as procedure planning in instructional videos, which requires a model to produce goal-directed action plans given the current visual observation of the world. Different with traditional procedure planning problem in structured environments~\cite{DBLP:conf/icra/FinnL17,DBLP:conf/icml/SrinivasJALF18}, this task deals with unstructured environments and thus forces the model to learn structured and plannable representations in real-life videos. We follow this work and tackle the procedure planning problem in instructional videos. Specifically, given the visual observations at start and end time, we need to produce a sequence of actions which transform the environment from start state to the goal state, as shown in \cref{fig:question}.

\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{image1_final.pdf}
   \caption{Procedure planning example. Given a start observation $o_{start}$ and a goal state $o_{goal}$, the model is required to generate a sequence of actions that can transform $o_{start}$ to $o_{goal}$. Previous approaches rely on heavy intermediate supervision during training, while our model only needs the task class labels (bottom row).}
   \label{fig:question}
   \vspace{-0.5cm}
\end{figure}

Previous approaches for procedure planning in instructional videos often treat it as a sequence planning problem and focus on predicting each action accurately. Most works rely on a two-branch autoregressive method to predict the intermediate states and actions step by step \cite{DBLP:journals/ral/SunHLLZG22, DBLP:conf/eccv/ChangHXAFN20, DBLP:conf/iccv/BiLX21}. Such models are complex and easy to accumulate errors during the planning process, especially for long sequences.  Recently, Zhao \etal \cite{DBLP:conf/cvpr/0004HDDWJ22} proposed a single branch non-autoregressive model based on transformer~\cite{DBLP:conf/nips/VaswaniSPUJGKP17} to predict all intermediate steps in parallel. To obtain a good performance, they used a learnable memory bank in the transformer decoder, augmented their model with an extra generative adversarial framework \cite{DBLP:conf/nips/GoodfellowPMXWOCB14} and applied a Viterbi post-processing method \cite{DBLP:journals/tit/Viterbi67}. This method brought multiple learning objectives, complex training schemes and tedious inference process. Instead, we assume procedure planning as a distribution fitting problem and planning is solved with a sampling process. We aim to directly model the joint distribution of the whole action sequence in instructional video rather than every discrete action. In this perspective, we can use a simple MSE loss to optimize our generative model and generate action sequence plans in one shot with a sampling process, which results in less learning objectives and simpler training schemes.


For supervision in training, in addition to the action sequence, previous methods often require heavy intermediate visual~\cite{DBLP:journals/ral/SunHLLZG22, DBLP:conf/eccv/ChangHXAFN20, DBLP:conf/iccv/BiLX21} or language~\cite{DBLP:conf/cvpr/0004HDDWJ22} annotations for their learning process. In contrast, we only use task labels from instructional videos as a condition for our learning (as shown in \cref{fig:question}), which could be easily obtained from the keywords or captions of videos and requires much less labeling cost. Another reason is that task information is closely related to the action sequences in a video. For example, in a video of $jacking$ $up$ $a$ $car$, the possibility for action $add$ $sugar$ appears in this process is nearly zero.


Modeling the uncertainty in procedure planning is also an important factor that we need to consider. That is, there might be more than one reasonable plan sequences to transform from the given start state to goal state. For example, change the order of $add$ $sugar$ and $add$ $butter$ in $making$ $cake$ process will not affect the final result. So action sequences can vary even with the same start and goal states. To address this problem, we consider adding randomness to our distribution-fitting process and perform training with a diffusion model~\cite{DBLP:conf/nips/HoJA20,DBLP:conf/icml/NicholD21}. Solving procedure planning problem with a diffusion model has two main benefits. First, a diffusion model changes the goal distribution to a random Gaussian noise by adding noise slowly to the initial data and learns the sampling process at inference time as an iterative denoising procedure starting from a random Gaussian noise. So randomness is involved both for training and sampling in a diffusion model, which is helpful to model the uncertain action sequences for procedure planning. Second, it is convenient to apply conditional diffusion process with the given start and goal observations based on diffusion models, so we can model the procedure planning problem as a conditional sampling process with a simple training scheme. In this work, we concatenate conditions and action sequences together and propose a projected diffusion model to perform conditional diffusion process.


\textbf{Contributions}. To sum up, the main contributions of this work are as follows: a) We cast the procedure planning as a conditional distribution-fitting problem and model the joint distribution of the whole intermediate action sequence as our learning objective, which can be learned with a simple training scheme. b) We introduce an efficient approach for training the procedure planner, which removes the supervision of visual or language features and relies on task supervision instead. c) We propose a novel projected diffusion model (PDPP) to learn the distribution of action sequences and produce all intermediate steps at one shot. We evaluate our PDPP on three instructional videos datasets and achieve the state-of-the-art performance across different prediction time horizons. Note that our model can still achieve excellent results even if we remove the task supervision and use the action labels only.


\section{Related work}
\label{sec::relatedwork}

\noindent \textbf{Procedural video understanding.} The problem of procedural video understanding has gained more and more attention with an aim to learn the inter-relationship between different events in videos recently. Zhao \etal \cite{DBLP:journals/tip/ZhaoRTZL22} investigated the problem of abductive visual reasoning, which requires vision systems to infer the most plausible visual explanation for the given visual observations. Furthermore, Liang \etal \cite{DBLP:conf/cvpr/LiangWZY22} proposed a new task: given an incomplete set of visual events, AI agents are asked to generate descriptions not only for the visible events and but also for the lost events by logical inferring. Unlike these works trying to learn the abductive information of intermediate events, Chang \etal \cite{DBLP:conf/eccv/ChangHXAFN20} introduced procedure planning in instructional videos which requires AI systems to plan an action sequence that can transform the given start observation to the goal state. In this paper, we follow this work and study the procedural video understanding problem by learning goal-directed actions planning.

\noindent \textbf{Diffusion probabilistic models.} Nowadays, diffusion probabilistic models \cite{DBLP:conf/icml/Sohl-DicksteinW15} have achieved great success in many research areas. Ho \etal \cite{DBLP:conf/nips/HoJA20} used a reweighted objective to train diffusion model and achieved great synthesis quality for image synthesis problem. Janner \etal \cite{DBLP:conf/icml/JannerDTL22} studied the trajectory planning problem with diffusion model and get remarkable results. Besides, diffusion models are also used in video generation\cite{DBLP:journals/corr/abs-2204-03458, DBLP:journals/corr/abs-2210-02303}, density estimation\cite{DBLP:journals/corr/abs-2107-00630}, human motion\cite{DBLP:journals/corr/abs-2209-14916}, sound generation\cite{DBLP:journals/corr/abs-2207-09983}, text generation\cite{DBLP:journals/corr/abs-2205-14217} and many other domains, all achieved competitive results. In this work, we apply diffusion process to procedure planning in instructional videos and propose our projected diffusion model, which achieves state-of-the-art performance only with a simple learning scheme.


\begin{figure*}[t]
  \centering
  \includegraphics[width=1\linewidth,height=0.42\linewidth]{image2_final.pdf}
   \caption{Overview of our projected diffusion model (prediction horizon $T$ = 3). We first train a task classifier to generate conditional information $c$, which will be used as guidance along with the given observations $o_s$ and $o_g$. Then we compute the denoising process iteratively. In each step, we first conduct a condition projection to the input, then predict the initial distribution by the learned model $f_\theta$. After that we calculate $\hat{x}_{n-1}$ with the predicted $\hat{x}_{0}$. We finally select the action dimensions as our result after $N$ denoising steps.}
   \label{fig:overview}
   \vspace{-0.2cm}
\end{figure*}


\noindent \textbf{Projected gradient descent.} Projected gradient descent is an optimal solution suitable for constrained optimization problems, which is proven to be effective in optimization with rank constraints~\cite{DBLP:journals/corr/ChenW15a}, online power system optimization problems~\cite{DBLP:conf/allerton/HauswirthBHD16} and adversarial attack~\cite{DBLP:conf/icip/DengK20}. The core idea of projected gradient descent is to add a projection operation to the normal gradient descent method, so that the result is ensured to be constrained in the feasible region. Inspired by this, we add a similar projection operation to our diffusion process, which keeps the conditional information for diffusion unchangeable and thus provides accurate guides for learning.


\section{Method}
\label{sec::method}

In this section, we present the details of our projected diffusion model for procedure planning (PDPP). We will first introduce the setup for this problem in \cref{method:pro_formulation}. Then we present the diffusion model used to model the action sequence distribution in \cref{method:diffusion}. To provide more precise conditional guidance both for the training and sampling process, a simple projection method is applied to our model, which we will discuss in \cref{method:project}. Finally, we show the training scheme (\cref{method:train_scheme}) and sampling process (\cref{method:inference}) of our PDPP. An overview of PDPP is provided in \cref{fig:overview}.



\subsection{Problem formulation}
\label{method:pro_formulation}

We follow the problem set-up of Chang \etal \cite{DBLP:conf/eccv/ChangHXAFN20}: given a start visual observation $o_{s}$ and a visual goal $o_{g}$, a model is required to plan a sequence of actions $a_{1:T}$ so that the environment state can be transformed from $o_{s}$ to $o_{g}$. Here $T$ is the horizon of planning, which denotes the number of action steps for the model to take and \{$o_{s}$, $o_{g}$\} indicates two different environment states in an instructional video.


We decompose the procedure planning problem into two sub-problems, as shown in \cref{eq:1}. The first problem is to learn the task-related information $c$ with the given \{$o_{s}$, $o_{g}$\} pair. This can be seen as a preliminary inference for procedure planning. Then the second problem is to generate action sequences with the task-related information and given observations.
Note that Jing \etal \cite{DBLP:conf/iccv/BiLX21} also decompose the procedure planning problem into two sub-problems, but their purpose of the first sub-problem is to provide long-horizon information for the second stage since Jing \etal \cite{DBLP:conf/iccv/BiLX21} plans actions step by step, while our purpose is to get condition for sampling to achieve an easier learning.

\begin{equation}
  \setlength{\abovedisplayskip}{-5pt}
  \setlength{\belowdisplayskip}{5pt}
  p(a_{1:T} | o_{s}, o_{g}) = \int p(a_{1:T} | o_{s}, o_{g}, c)p(c | o_{s}, o_{g}) dc.
  \label{eq:1}
\end{equation}

At training time, we first train a simple model (implemented as multi-layer perceptrons (MLPs)) with the given observations \{$o_{s}, o_{g}$\} to predict which the task category is. We use the task labels in instructional videos $\overline{c}$ to supervise the output $c$. After that, we evaluate $p(a_{1:T} | o_{s}, o_{g}, c)$ in parallel with our model and leverage the ground truth (GT) intermediate action labels as supervision for training. Compared with the visual and language supervision in previous works, task label supervision is easier to get and brings simpler learning schemes. At inference phase, we just use the start and goal observations to predict the task class information $c$ and then samples action sequences $a_{1:T}$ from the learned distribution with the given observations and predicted $c$, where $T$ is the planning horizon.

\subsection{Projected diffusion for procedure planning}
\label{method:diffusion}

Our method consists of two stages: task class prediction and action sequence distribution modeling. The first stage learning is a traditional classification problem which we implement with a simple MLP model. The main part of our model is the second one. That is, how to model $p(a_{1:T} | o_{s}, o_{g}, c)$
to solve the procedure planning problem. Jing \etal \cite{DBLP:conf/iccv/BiLX21} assume this as a Goal-conditioned Markov Decision Process and use a policy $p(a_t|o_t)$ along with a transition model $\tau_{\mu}(o_t|c,o_{t-1},a_{t-1})$ to perform the planning step by step, which is complex to train and slow for inference. We instead treat this as a direct distribution fitting problem with a diffusion model.

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{image5.pdf}
   \caption{Schematic diagram for the forward and reverse diffusion processes.}
   \label{fig:diff}
   \vspace{-0.5cm}
\end{figure}

~\\ \textbf{Diffusion model}. A diffusion model \cite{DBLP:conf/nips/HoJA20, DBLP:conf/icml/NicholD21} solves the data generation problem by modeling the data distribution $p(x_0)$ as a denoising Markov chain over variables \{$x_N...x_0$\} and assume $x_N$ to be a random Gaussian distribution. The forward process of a diffusion model is incrementally adding Gaussian noise to the initial data $x_0$ and can be represented as $q(x_n|x_{n-1})$, by which we can get all intermediate noisy latent variables $x_{1:N}$ with a diffusion step $N$. In the sampling stage, the diffusion model conducts iterative denoising procedure $p(x_{n-1}|x_n)$ for $N$ times to approximate samples from the target data distribution. The forward and reverse diffusion processes are shown in \cref{fig:diff}.

In a standard diffusion model, the ratio of Gaussian noise added to the data at diffusion step $n$ is pre-defined as $\{\beta_n \in (0, 1)\}_{n=1}^{N}$. Each adding noise step can be parametrized as

\begin{equation}
  \setlength{\abovedisplayskip}{-5pt}
  \setlength{\belowdisplayskip}{5pt}
  q(x_n|x_{n-1}) = \mathcal{N}(x_n; \sqrt{1-\beta_{n}} x_{n-1}, \beta_n\textbf{I}).
  \label{eq:forward}
\end{equation}
Since hyper-parameters $\{\beta_n\}_{n=1}^{N}$ are pre-defined, there is no training in the noise-adding process. As discussed in \cite{DBLP:conf/nips/HoJA20}, re-parameterize \cref{eq:forward} we can get:

\begin{equation}
  \setlength{\abovedisplayskip}{-5pt}
  \setlength{\belowdisplayskip}{5pt}
  x_n = \sqrt{\overline{\alpha}_n}x_0 + \sqrt{1 - \overline{\alpha}_n}\epsilon,
  \label{eq:forward_xt}
\end{equation}
where $\overline{\alpha}_n = \prod_{s=1}^{n}(1 - \beta_s)$ and $\epsilon \sim \mathcal{N}(0, I)$.

In the denoising process, each step is parametrized as:

\begin{equation}
  \setlength{\abovedisplayskip}{-8pt}
  \setlength{\belowdisplayskip}{5pt}
  p_\theta(x_{n-1}|x_n) = \mathcal{N}(x_{n-1};\mu_\theta(x_n,n),\Sigma_\theta(x_n,n)),
  \label{eq:sample}
\end{equation}
where $\mu_\theta$ is produced by a learnable model and $\Sigma_\theta$ can be directly calculated with $\{\beta_n\}_{n=1}^{N}$ \cite{DBLP:conf/nips/HoJA20}. The learning objective for a typical diffusion model in \cite{DBLP:conf/nips/HoJA20} is the noise added to the uncorrupted data $x_0$ at each step. When training, the diffusion model first selects a diffusion step $n\in [1, N]$ and calculates $x_n$ as shown in \cref{eq:forward_xt}. Then the learnable model will compute $\epsilon_\theta(x_n, n)$ and calculate loss with the true noise add to the distribution at step $n$. After training, the diffusion model can simply generate data like $x_0$ by iteratively processing the denoising step starting from a random Gaussian noise.

However, applying this kind of diffusion model to procedure planning directly is not suitable. First, the sampling process in our task is condition-guided while no condition is applied in the standard diffusion model. Second, the distribution we want to fit is the whole action sequence, which has a strong semantic information. Directly predicting the semantically meaningless noise sampled from random Gaussian distribution can be hard. In experiments which take noise as the predicting objective, our model just fails to be trained. To address these problems, we make two modifications to the standard diffusion model: one is about input and the other is the learning objective of model.

~\\ \textbf{Conditional action sequence input}. The input of a standard diffusion model is the data distribution it needs to fit and no guided information is required. For the procedure planning problem, the distribution we aim to fit is the intermediate action sequences $[a_1, a_2...a_T]$, which depends on the given observations and task class we get in the first learning stage. Thus we need to find how to add these guided conditions into the diffusion process. Although there are multiple guided diffusion models presented \cite{DBLP:conf/nips/DhariwalN21, DBLP:journals/corr/abs-2207-12598}, they are expensive for learning and need complex training or sampling strategy. 
%Different from previous conditional diffusion models
Inspired by Janner \etal \cite{DBLP:conf/icml/JannerDTL22}, we here apply a simple way to achieve our goal: just treat these conditions as additional information and concatenate them along the action feature dimension. Notably, we concatenate $o_s$ and $a_1$ together, same for $o_g$ and $a_T$. In this way, we introduce a prior knowledge that the start/end observations are more related to the first/last actions, which turns out to be useful for our learning (details are provided in the supplementary material). Thus our model input for training now can be represented as a multi-dimension array:

\begin{equation}
  \setlength{\abovedisplayskip}{-8pt}
  \setlength{\belowdisplayskip}{0pt}
    \begin{bmatrix}
        c&  c&  & c &c\\
        a_1&  a_2&  ...& a_{T-1} & a_T\\
        o_s&  0&  & 0 & o_g
    \end{bmatrix}.
    \label{input}
\end{equation}

Each column in our model input represents the condition information, action one-hot code, and the corresponding observation for a certain action. Note that we do not need the intermediate visual observations as supervision, so all observation dimensions are set to zero except for the start and end observations.

~\\ \textbf{Learning objective of our diffusion model}. As mentioned above, the learning objective of a standard diffusion model is the random noise added to the distribution at each diffusion step. This learning scheme has demonstrated great success in data synthesis area, partly because predicting noise rather than the initial input $x_0$ brings more variations for data generation. For procedure planning problem, however, the distribution we need to fit contains high-level features rather than pixels. Since the planning horizon $T$ is relatively short and the one-hot action features require less randomness than in data generation, predicting noise in diffusion process for procedure planning will just increase the difficulty of learning. So we modify the learning objective to the initial input $x_0$, which will be described in \cref{method:train_scheme}.

\subsection{Condition projection during learning}
\label{method:project}

Our model transforms a random Gaussian noise to the final result, which has the same structure with our model input (see \cref{input}) by conducting $N$ denoising steps. Since we combine the conditional information with action sequences as the data distribution, these conditional guides can be changed during the denoising process. However, the change of these conditions will bring wrong guidance for the learning process and make the observations and conditions useless. To address this problem, we add a condition projection operation into the learning process. That is, we force the observation and condition dimensions not to be changed during training and inference by assigning the initial value. The input $x$ of condition projection is either a noise-add data (Alg.1 L5) or the predicted result of model (Alg.1 L7). We use $\{\hat{c}, \hat{a}, \hat{o}\}$ to represent different dimensions in $x$, then our projection operation $\mathrm{Proj}()$ can be denoted as:

\begin{equation}
  \setlength{\abovedisplayskip}{-5pt}
  \setlength{\belowdisplayskip}{5pt}
\begin{aligned}
    \begin{bmatrix}
        \hat{c}_1&  \hat{c}_2&  &  &\hat{c}_T\\
        \hat{a}_1&  \hat{a}_2&  ...&  & \hat{a}_T\\
        \hat{o}_1&  \hat{o}_2&  &  & \hat{o}_T
    \end{bmatrix}
    \to
    \begin{bmatrix}
        c&  c&  &  &c\\
        \hat{a}_1&  \hat{a}_2&  ...&  & \hat{a}_T\\
        o_s&  0&  &  & o_g
    \end{bmatrix},
    \\x \qquad \qquad \qquad \qquad \mathrm{Proj}(x) \quad \qquad
  \label{eq:project}
\end{aligned}
\end{equation}
where $\hat{c}_i$, $\hat{o}_i$ and $\hat{a}_i$ denote the $i^{th}$ horizon class, observation dimensions and predicted action logits in $x$, respectively. $c$, $o_s,o_g$ are the conditions.


\subsection{Training scheme}
\label{method:train_scheme}

Our training scheme contains two stages: a) training a task-classifier model $\mathcal{T}_\phi (c  | o_s, o_g)$ that extracts conditional guidance from the given start and goal observations; b) leveraging the projected diffusion model to fit the target action sequence distribution.

For the first stage, we apply MLP models to predict the task class $c$ with the given $o_s,o_g$. Ground truth task class labels $\overline{c}$ are used as supervision. In the second learning stage, we follow the basic training scheme for diffusion model, but change the learning objective as the initial input $x_0$. We use a U-Net model~\cite{DBLP:conf/miccai/RonnebergerFB15} $f_\theta(x_n, n)$ as the learnable model and our training loss is:

\begin{equation}
  \setlength{\abovedisplayskip}{-10pt}
  \setlength{\belowdisplayskip}{5pt}
  \mathcal{L}_{\mathrm{diff}} = \sum_{n=1}^{N}(f_\theta(x_n, n) - x_0)^2 ,
  \label{eq:task_loss}
\end{equation}

We believe that $a_1$ and $a_T$ are more important because they are the most related actions for the given observations. Thus we rewrite \cref{eq:task_loss} as a weighted loss by multiplying a weight matrix to $\mathcal{L}_{\mathrm{diff}}$ as follows:
\begin{equation}
  % \setlength{\abovedisplayskip}{-2pt}
  %\setlength{\belowdisplayskip}{0pt}
    \begin{bmatrix}
        1&  1&  & 1 &1\\
        w&  1&  ...& 1 & w\\
        1&  1&  & 1 & 1
    \end{bmatrix}.
    \label{eq:weight}
\end{equation}
Besides, we add a condition projection step to our diffusion process. So given the initial input $x_0$ which contains action sequences, task conditions and observations, we first add noise to the input to get $x_n$, and then apply condition projection to ensure the guidance not changed. With $x_n$ and the corresponding diffusion step $n$, we calculate the denosing output $f_\theta(x_n, n)$, followed by condition projection again. Finally, we compute the weighted $\mathcal{L}_{\mathrm{diff}}$ and update model, as shown in \cref{train_alg}. 
%  写个公式


\begin{algorithm}[t]
    \caption{Training}
    \begin{algorithmic}[1]
        \Require Initial input $x_0$, total diffusion steps number $N$, model $f_\theta$, $\{\overline{\alpha}_n\}_{n=1}^{N}$, weight matrix $w$
        \Repeat
        \State $n \sim Uniform(\{1,...,N\})$
        \State $\epsilon \sim \mathcal{N}(0, I)$
        \State $x_n = \sqrt{\overline{\alpha}_n}x_0 + \sqrt{1 - \overline{\alpha}_n}\epsilon$
        \State $\hat{x}_0 = f_\theta(Proj(x_n), n)$
        \State Take gradient descent step on
        \State \quad $\bigtriangledown_\theta \left |\left | \left (x_0 - Proj(\hat{x}_0) \right ) * w \right | \right |^2$
        \Until converged
        
    \end{algorithmic}
    \label{train_alg}
\end{algorithm}

\subsection{Inference}
\label{method:inference}

At inference time, only the start observation $o_s$ and goal observation $o_g$ are provided. We first predict the task class by choosing the maximum probability value in the output of task-classifier model $\mathcal{T}_\phi$. Then the predicted task class $c$ is used as the class condition. To sample from the learned action sequence distribution, we start with a Gaussian noise, and iteratively conduct denoise and condition projection for $N$ times. The detailed inference process is shown in \cref{infer_alg}.
% 写个算法
\begin{algorithm}
    \caption{Inference}
    \begin{algorithmic}[1]
        \Require total diffusion steps number $N$, model $f_\theta$, $\{\overline{\alpha}_n\}_{n=1}^{N}$, $\{\beta_n\}_{n=1}^{N}$
        
        \State $\hat{x}_N\sim \mathcal{N}(0,I)$
        % \State $\hat{x}_N\gets Proj(\hat{x}_N)$
        \For{$n=N,...,1$}
            \State $\hat{x}_0 = f_\theta(Proj(\hat{x}_n), n)$
            \If{$n > 1$} 
            \State $\hat{\mu}_n = \frac{\sqrt{\overline{\alpha}_{n-1}}\beta_n}{1-\overline{\alpha}_n} \hat{x}_0 + 
            \frac{\sqrt{\alpha_n}(1-\overline{\alpha}_{n-1})}{1-\overline{\alpha}_n} \hat{x}_n$
            \State $\hat{\Sigma}_n = \frac{1-\overline{\alpha}_{n-1}}{1-\overline{\alpha}_n} \cdot \beta_n$
            \State $\hat{x}_{n-1}\sim \mathcal{N}(\hat{x}_{n-1};\hat{\mu}_n,\hat{\Sigma}_n\textbf{I})$
            \EndIf
        \EndFor
        \State return $\hat{x}_0$
    \end{algorithmic}
    \label{infer_alg}
\end{algorithm}


Once we get the predicted output $\hat{x}_0$, we take out the action sequence dimensions $[\hat{a}_1,...,\hat{a}_T]$  and select the index of every maximum value in $\hat{a}_i (i=1,...,T)$ as the action sequence plan for procedure planning. Note that for the training stage, the class condition dimensions of $x_0$ are the ground truth task labels, not the output of our task-classifier as in inference.


\section{Experiments}

In this section, we evaluate our PDPP model on three real-life datasets and show our competitive results for various planning horizons. We first present the result of our first training stage, which predicts the task class with the given observations in \cref{exp:task}. Then we compare our performance with other alternative approaches on the three datasets and demonstrate the effectiveness of our model in \cref{exp:compare}. We also study the role of task-supervision for our model in \cref{exp:ablation}. Finally, we show our prediction uncertainty evaluation results in \cref{exp:uncertainty}.

\subsection{Implementation details}
\label{exp:implementation}
We use the basic U-Net~\cite{DBLP:conf/miccai/RonnebergerFB15} as our learnable model for projection diffusion, in which a modification is made by using a convolution operation along the planning horizon dimension for downsampling rather than max-pooling as in \cite{DBLP:conf/icml/JannerDTL22}. For training, we use the linear warm-up training scheme to optimize our model. We train different steps for the three datasets, corresponding to their scales.
%All our experiments are conducted with ADAM\cite{DBLP:journals/corr/KingmaB14} on 8 NVIDIA TITAN Xp GPUs.
More details about training and model architecture are provided in the supplementary material.

\subsection{Evaluation protocol}
\label{exp:protocol}

\noindent \textbf{Datasets.} We evaluate our PDPP model on three instructional video datasets: \textbf{CrossTask} \cite{DBLP:conf/cvpr/ZhukovACFLS19}, \textbf{NIV} \cite{DBLP:conf/cvpr/AlayracBASLL16}, and \textbf{COIN} \cite{DBLP:conf/cvpr/TangDRZZZL019}. CrossTask contains 2,750 videos from 18 different tasks, with an average of 7.6 actions per video. The NIV dataset consists of 150 videos about 5 daily tasks, which has 9.5 actions in one video on average. COIN is much larger with 11,827 videos, 180 different tasks and 3.6 actions/video. We randomly select 70\% data for training and 30\% for testing as previous work~\cite{DBLP:conf/eccv/ChangHXAFN20,DBLP:conf/iccv/BiLX21,DBLP:conf/cvpr/0004HDDWJ22}. Note that we do not select 70\%/30\% for videos in each task, but in the whole dataset. Following previous work\cite{DBLP:conf/eccv/ChangHXAFN20,DBLP:conf/iccv/BiLX21,DBLP:conf/cvpr/0004HDDWJ22}, we extract all action sequences $\{[a_i, ..., a_{i+T-1}]\}_{i=1}^{n-T+1}$ with predicting horizon $T$ from the given video which contains $n$ actions by sliding a window of size $T$. Then for each action sequence $[a_i, ..., a_{i+T-1}]$, we choose the video clip feature at the beginning time of action $a_i$ and clip feature around the end time of $a_{i+T-1}$ as the start observation $o_s$ and goal state $o_g$, respectively. Both clips are 3 seconds long. For experiments conduct on CrossTask, we use two kinds of pre-extracted video features as the start and goal observations. One are the features provided in CrossTask dataset: each second of video content is encoded into a 3200-dimensional feature vector as a concatenation of the I3D, ResNet-152 and audio VGG features\cite{DBLP:conf/cvpr/HeZRS16,DBLP:conf/icassp/HersheyCEGJMPPS17,DBLP:conf/cvpr/CarreiraZ17}, which are also applied in \cite{DBLP:conf/iccv/BiLX21,DBLP:conf/eccv/ChangHXAFN20}. The other kind of features are generated by the encoder trained with the HowTo100M\cite{DBLP:conf/iccv/MiechZATLS19} dataset, as in \cite{DBLP:conf/cvpr/0004HDDWJ22}. For experiments on the other two datasets, we follow \cite{DBLP:conf/cvpr/0004HDDWJ22} to use the HowTo100M features for a fair comparison.

\begin{table}\small
\centering
\resizebox*{1.0\linewidth}{!}{
\begin{tabular}{ccccc}
\hline
      & CrossTask$_{Base}$ & CrossTask$_{How}$ & COIN & NIV \\ \cline{2-5} 
$T$ = 3 &     94.38      &  92.43      &  79.42   &  100.00    \\ 
$T$ = 4 &     83.64      &  92.98      &  78.89   &  100.00    \\ 
$T$ = 5 &     83.37      &  93.39      &  -   &   -   \\ 
$T$ = 6 &     83.85      &  93.20      &  -   &   -   \\ \hline
\end{tabular}
}
\caption{Classification results on all datasets. CrossTask$_{Base}$ uses features provided by the dataset while CrossTask$_{How}$ applies features extracted by HowTo100M trained encoder.}
\label{table:classifier_cross}
\vspace{-0.3cm}
\end{table}

\begin{table*}[t]
\centering
\resizebox*{0.7\linewidth}{!}{
\begin{tabular}{cccccccc}
\hline
                 &             & \multicolumn{3}{c}{$T$ = 3} & \multicolumn{3}{c}{$T$ = 4} \\ \cline{3-5} \cline{6-8}
\multicolumn{1}{l}{Models}           & Supervision & SR$\uparrow$    & mAcc$\uparrow$   & mIoU$\uparrow$   & SR$\uparrow$    & mAcc$\uparrow$   & mIoU$\uparrow$   \\ \hline
\multicolumn{1}{l}{Random}           &     -        &   $<$0.01    &    0.94    &   \color{gray}{1.66}     &   $<$0.01    &    0.83    &   \color{gray}{1.66}     \\
\multicolumn{1}{l}{Retrieval-Based}  &     -        &   8.05    &   23.30     &   \color{gray}{32.06}     &   3.95    &    22.22    &    \color{gray}{36.97}    \\
\multicolumn{1}{l}{WLTDO\cite{DBLP:conf/cvpr/EhsaniBRMF18}}            &      -       &   1.87    &    21.64    &   \color{gray}{31.70}     &   0.77    &    17.92    &    \color{gray}{26.43}    \\
\multicolumn{1}{l}{UAAA\cite{DBLP:conf/iccvw/FarhaG19}}             &      -       &   2.15    &   20.21     &   \color{gray}{30.87}     &   0.98    &   19.86     &  \color{gray}{27.09}      \\
\multicolumn{1}{l}{UPN\cite{DBLP:conf/icml/SrinivasJALF18}}              &      V       &   2.89    &    24.39    &   \color{gray}{31.56}     &   1.19    &   21.59     &   \color{gray}{27.85}     \\
\multicolumn{1}{l}{DDN\cite{DBLP:conf/eccv/ChangHXAFN20}}              &      V       &   12.18    &    31.29    &    \color{gray}{47.48}    &   5.97    &    27.10    &    \color{gray}{48.46}    \\
\multicolumn{1}{l}{Ext-GAILw/o Aug.\cite{DBLP:conf/iccv/BiLX21}} &      V       &   18.01    &   43.86     &   \color{gray}{57.16}     &    -   &    -    &    -    \\
\multicolumn{1}{l}{Ext-GAIL\cite{DBLP:conf/iccv/BiLX21}}         &      V       &   21.27    &   49.46     &   \color{gray}{61.70}     &   16.41    &    43.05    &   \color{gray}{60.93}     \\
\multicolumn{1}{l}{P$^3$IV\cite{DBLP:conf/cvpr/0004HDDWJ22}}             &      L       &   23.34    &   49.96     &   \color{gray}{73.89}     &   13.40    &   44.16     &   \color{gray}{70.01}     \\
\multicolumn{1}{l}{Ours$_{Base}$}             &      C       &    26.47   &    55.35    &     \color{gray}{58.95}   &   15.40    &   49.42     &    \color{gray}{56.99}    \\
\multicolumn{1}{l}{Ours$_{How}$}             &      C       &    \textbf{37.20}   &    \textbf{64.67}    &     \color{gray}{66.57}   &   \textbf{21.48}    &   \textbf{57.82}     &    \color{gray}{65.13}    \\
\hline
\vspace{-0.3cm}
\end{tabular}
}
\caption{Evaluation results on CrossTask for procudure planning with prediction horizon $T \in \{3, 4\}$. The $Supervision$ column denotes the type of supervision applied in training, where V denotes intermediate visual states, L denotes language feature and C means task class. \textbf{Note that we compute mIoU by calculating average of every IoU of a single antion sequence rather than a mini-batch}.}
\label{table:cross_34}
\end{table*}

~\\ \noindent \textbf{Metrics}. Following previous work\cite{DBLP:conf/eccv/ChangHXAFN20,DBLP:conf/iccv/BiLX21,DBLP:conf/cvpr/0004HDDWJ22}, we apply three metrics to evaluate the performance. a) \textbf{Success Rate (SR)} considers a plan as a success only if every action matches the ground truth sequence. b) \textbf{mean Accuracy (mAcc)} calculates the average correctness of actions at each individual time step, which means an predicted action is considered correct if it matches the action in ground truth at the same time step. c) \textbf{mean Intersection over Union (mIoU)} measures the overlap between predicted actions and ground truth by computing Iou $\frac{|\{a_t\} \cap \{\hat{a_t}\}|}{|\{a_t\} \cup \{\hat{a_t}\}|}$, where $\{a_t\}$ is the set of ground truth actions and $\{\hat{a_t}\}$ is the set of predicted actions.

Previous approaches~\cite{DBLP:conf/cvpr/0004HDDWJ22,DBLP:conf/iccv/BiLX21} compute the mIoU metric on every mini-batch (batch size larger than one) and calculate the average as the result. This brings a problem that the mIoU value can be influenced heavily by batch size. Consider if we set batch size is equal to the size of training data, then all predicted actions can be involved in the ground truth set and thus be correct predictions. However, if batch size is set to one, then any predicted action that not appears in the corresponding ground truth action sequence will be wrong. To address this problem, we standardize the way to get mIoU as computing IoU on every single sequence and calculating the average of these IoUs as the result (equal to setting of batch size $= 1$).

~\\ \noindent \textbf{Baselines.} Models for procedure planning\cite{DBLP:conf/eccv/ChangHXAFN20,DBLP:conf/iccv/BiLX21,DBLP:conf/cvpr/0004HDDWJ22} and other fully supervised planning approaches\cite{DBLP:conf/iccvw/FarhaG19, DBLP:conf/cvpr/EhsaniBRMF18, DBLP:conf/icml/SrinivasJALF18} are all involved in our comparison. Descriptions for these methods are available in the supplementary material.

\begin{table}
\centering
\resizebox*{0.8\linewidth}{!}{
\renewcommand\arraystretch{1.1}
\begin{tabular}{ccccc}
\hline
                & $T$ = 3   & $T$ = 4   & $T$ = 5  & $T$ = 6  \\ \cline{2-5} 
\multicolumn{1}{l}{Models}          & SR$\uparrow$    & SR$\uparrow$    & SR$\uparrow$   & SR$\uparrow$   \\ \hline
\multicolumn{1}{l}{Retrieval-Based} & 8.05  & 3.95  & 2.40 & 1.10 \\
\multicolumn{1}{l}{DDN\cite{DBLP:conf/eccv/ChangHXAFN20}}             & 12.18 & 5.97  & 3.10 & 1.20 \\
\multicolumn{1}{l}{P$^3$IV\cite{DBLP:conf/cvpr/0004HDDWJ22}}            & 23.34 & 13.40 & 7.21 & 4.40 \\
\multicolumn{1}{l}{Ours$_{Base}$}            & 26.47 & 15.40 & 9.37 & 6.76 \\
\multicolumn{1}{l}{Ours$_{How}$}            & \textbf{37.20} & \textbf{21.48} & \textbf{13.58} & \textbf{8.47} \\
\hline
\end{tabular}
}
\caption{Success Rate evaluation results on  CrossTask with longer planning horizons.}
\label{table:cross_56}
\vspace{-0.5cm}
\end{table}

\subsection{Results for task-classifier}
\label{exp:task}

The first stage of our learning is to predict the task class with the given start and goal observations. We implement this with MLP models and the detailed first-stage training process is described in the supplementary material.
%We implement this with a two-layer Res-MLP\cite{DBLP:journals/corr/abs-2105-03404} and train it with a simple mseloss. 
The classification results for different planning horizons on three datasets are shown in \cref{table:classifier_cross}. We can see that our classifier can perfectly figure out the task class in the NIV dataset since only 5 tasks are involved. For larger datasets CrossTask and COIN, our model can make right predictions most of the time.


\subsection{Comparison with other approaches}
\label{exp:compare}

We follow previous work\cite{DBLP:conf/cvpr/0004HDDWJ22} and compare our approach with other alternative methods on three datasets, across multiple prediction horizons.



\begin{table}
\resizebox*{1\linewidth}{0.45\linewidth}{
\renewcommand\arraystretch{1}
\begin{tabular}{ccccccccc}
\hline
                     &                 &      & \multicolumn{3}{c}{NIV} & \multicolumn{3}{c}{COIN} \\ \cline{4-9} 
Horizon              & \multicolumn{1}{l}{Models}         & Sup. & SR$\uparrow$     & mAcc$\uparrow$   & mIoU$\uparrow$  & SR$\uparrow$     & mAcc$\uparrow$   & mIoU$\uparrow$   \\ \hline
\multirow{6}{*}{$T$ = 3} & \multicolumn{1}{l}{Random}          & -    & 2.21   & 4.07   & \color{gray}{6.09}  &   $<$0.01     &    $<$0.01    & \color{gray}{2.47}   \\
                     & \multicolumn{1}{l}{Retrieval} & -    & -      & -      & -     & 4.38   & 17.40  & \color{gray}{32.06}  \\
                     & \multicolumn{1}{l}{DDN \cite{DBLP:conf/eccv/ChangHXAFN20}}            & V    & 18.41  & 32.54  & \color{gray}{56.56} & 13.9   & 20.19  & \color{gray}{64.78}  \\
                     & \multicolumn{1}{l}{Ext-GAIL \cite{DBLP:conf/iccv/BiLX21}}       & V    & 22.11  & 42.20  & \color{gray}{65.93} & -      & -      & -      \\
                     & \multicolumn{1}{l}{P$^3$IV \cite{DBLP:conf/cvpr/0004HDDWJ22}}           & L    & 24.68  & 49.01  & \color{gray}{74.29} & 15.4   & 21.67  & \color{gray}{76.31}  \\
                     & \multicolumn{1}{l}{Ours}            & C    & \textbf{31.25}  & \textbf{49.26}  & \color{gray}{57.92} & \textbf{21.33}  & \textbf{45.62}  & \color{gray}{51.82}  \\ \hline
\multirow{6}{*}{$T$ = 4} & \multicolumn{1}{l}{Random}          & -    & 1.12   & 2.73   & \color{gray}{5.84}  &   $<$0.01     &    $<$0.01    & \color{gray}{2.32}   \\
                     & \multicolumn{1}{l}{Retrieval} & -    & -      & -      & -     & 2.71   & 14.29  & \color{gray}{36.97}  \\
                     & \multicolumn{1}{l}{DDN \cite{DBLP:conf/eccv/ChangHXAFN20}}            & V    & 15.97  & 27.09  & \color{gray}{53.84} & 11.13  & 17.71  & \color{gray}{68.06}  \\
                     & \multicolumn{1}{l}{Ext-GAIL \cite{DBLP:conf/iccv/BiLX21}}       & V    & 19.91  & 36.31  & \color{gray}{53.84} & -      & -      & -      \\
                     & \multicolumn{1}{l}{P$^3$IV \cite{DBLP:conf/cvpr/0004HDDWJ22}}           & L    & 20.14  & 38.36  & \color{gray}{67.29} & 11.32  & 18.85  & \color{gray}{70.53}  \\
                     & \multicolumn{1}{l}{Ours}            & C    & \textbf{26.72}  & \textbf{48.92}  & \color{gray}{59.04} & \textbf{14.41}  & \textbf{44.10}  & \color{gray}{51.39}  \\ \hline
\end{tabular}
}
\caption{Evaluation results on NIV and COIN with prediction horizon $T \in \{3,4\}$. Sup. denotes the type of supervision in training. Note that we compute IoU on every action sequence and take the mean as mIoU.} 
\label{table:coin_niv}
\vspace{-0.5cm}
\end{table}


~\\ \noindent \textbf{CrossTask (short horizon).} 
We first evaluate on CrossTask with two prediction horizons typically used in previous work. We use Ours$_{Base}$ to denote our model with features provided by CrossTask and Ours$_{How}$ as model with features extracted by HowTo100M trained encoder. Note that we compute mIoU by calculating the mean of every IoU for a single antion sequence rather than a mini-batch as explained in \cref{exp:protocol}, though the latter can achieve a higher mIoU value. Results in \cref{table:cross_34} show that Ours$_{Base}$ beats all methods for most metrics except for the success rate (SR) when $T = 4$, where our model is the second best, and Ours$_{How}$ just significantly outperforms all previous methods. Specifically, for using HowTo100M-extracted video features, we outperform \cite{DBLP:conf/cvpr/0004HDDWJ22} by around 14\% and more than 8\% for SR when $T=3,4$, respectively. As for features provided by CrossTask, Ours$_{Base}$ outperforms the previous best method \cite{DBLP:conf/iccv/BiLX21} by more than 5\% for SR and 6\% for mAcc when $T=3$.



~\\ \noindent \textbf{CrossTask (long horizon).} We further study the ability of predicting with longer horizons for our model. Following \cite{DBLP:conf/cvpr/0004HDDWJ22}, we here evaluate the SR value with planning horizon $T = \{3, 4, 5, 6\}$. We present the result of our model along with other approaches that reported results for longer horizons in \cref{table:cross_56}. This result shows our model can get a stable and great improvement with all planning horizons compared with the previous best model.

~\\ \noindent \textbf{NIV and COIN.} \cref{table:coin_niv} shows our evaluation results on the other two datasets NIV and COIN, from which we can see that our approach remains to be the best performer for both datasets. Specifically, in the NIV dataset where mAcc is relatively high, our model raises the SR value by more than 6.5\% both for the two horizons and outperforms the previous best by more than 10\% on mAcc metric when $T=4$. As for the large COIN dataset where mAcc is low, our model significantly improves mAcc by more than 20\%. 

All the results suggest that our model performs well across datasets with different scales.

\subsection{Study on task supervision}
\label{exp:ablation}

In this section, we study the role of task supervision for our model. \cref{table:task} shows the results of learning with and without task supervision, which suggest that the task supervision is quite helpful to our learning. Besides, we find that task supervision helps more for learning in the COIN dataset.
%and learning with better visual features.
We assume the reason is that fitting the COIN dataset is hard to our model since the number of tasks in COIN is large. Thus the guidance of task class information is more important to COIN compared with the other two datasets. Notably, training our model without task supervision also achieves state-of-the-art performance on multiple metrics, which suggests the effective of our approach.

\begin{table}[]
\centering
\resizebox*{1\linewidth}{!}{
\renewcommand\arraystretch{1.1}
\begin{tabular}{cccccccc}
\hline
\multirow{2}{*}{}    & \multicolumn{1}{l}{\multirow{2}{*}{Dataset}}   & \multicolumn{3}{c}{w. task sup.} & \multicolumn{3}{c}{w.o. task sup.} \\ \cline{3-8} 
                     &                            & SR$\uparrow$         & mAcc$\uparrow$      & mIoU$\uparrow$      & SR$\uparrow$        & mAcc$\uparrow$      & mIoU$\uparrow$      \\ \hline
\multirow{4}{*}{$T$ = 3} & \multicolumn{1}{l}{CrossTask$_{Base}$}                  & \textbf{26.47}      & 55.35     & 58.95     & 22.82     & 51.56     & 54.36     \\
& \multicolumn{1}{l}{CrossTask$_{How}$}                  & \textbf{37.20}      & 64.67     & 66.57     & 35.69     & 63.91     & 66.04     \\
                     & \multicolumn{1}{l}{NIV}                        & \textbf{31.25}      & 49.26     & 57.92     & 29.41     & 46.20     & 56.42     \\
                     & \multicolumn{1}{l}{COIN}                       & \textbf{21.33}      & 45.62     & 51.82     & 16.46     & 36.43     & 43.50     \\ \hline
\multirow{4}{*}{$T$ = 4} & \multicolumn{1}{l}{CrossTask$_{Base}$}                  & \textbf{15.40}      & 49.42     & 56.99     & 14.91     & 49.55     & 56.28     \\
& \multicolumn{1}{l}{CrossTask$_{How}$}                  & \textbf{21.48}      & 57.82     & 65.13     & 20.52     & 57.47     & 64.39     \\
                     & \multicolumn{1}{l}{NIV}                        & \textbf{26.72}      & 48.92     & 59.04     & 26.72     & 46.55     & 59.50     \\
                     & \multicolumn{1}{l}{COIN}                       & \textbf{14.41}      & 44.10     & 51.39     & 12.32     & 35.48     & 42.75     \\ \hline
\multirow{2}{*}{$T$ = 5}    & \multicolumn{1}{l}{CrossTask$_{Base}$} & \textbf{9.37}      & 45.93     & 56.32     & 8.95     & 45.77     & 56.34     \\
& \multicolumn{1}{l}{CrossTask$_{How}$} & \textbf{13.58}      & 54.05     & 65.32    & 12.80     & 53.44     & 64.01     \\ \hline
\multirow{2}{*}{$T$ = 6}    & \multicolumn{1}{l}{CrossTask$_{Base}$} & \textbf{6.76}      & 43.61     & 57.51     & 6.06     & 44.15     & 57.07     \\
& \multicolumn{1}{l}{CrossTask$_{How}$} & \textbf{8.47}      & 50.14     & 65.38     & 8.15     & 50.45     & 64.13     \\ \hline
\end{tabular}}
\caption{Ablation study on the role of task supervision. The $w.$ $task$ $sup.$ denotes learning with task supervision and $w.o.$ $task$ $sup.$ means training with the basic action labels only.}
\label{table:task}
% \vspace{-0.5cm}
\end{table}

\subsection{Evaluating probabilistic modeling}
\label{exp:uncertainty}
As discussed in \cref{sec::intro}, we introduce diffusion model to procedure planning to model the uncertainty in this problem. Here we follow \cite{DBLP:conf/cvpr/0004HDDWJ22} to evaluate our probabilistic modeling. We focus on CrossTask$_{How}$ as it has the most uncertainty for planning. Results on other datasets and further details are available in the supplement.

Our model is probabilistic by starting from random noises and denoising step by step. We here introduce two baselines to compare with our diffusion based approach. We first remove the diffusion process in our method to establish the \textit{Noise} baseline, which just samples from a random noise with the given observations and task class condition in one shot. Then we further establish the \textit{Deterministic} baseline by setting the start distribution $\hat{x}_N = 0$, thus the model directly predicts a certain result with the given conditions. We reproduce the \textit{KL divergence, NLL, ModeRec} and \textit{ModePrec} in \cite{DBLP:conf/cvpr/0004HDDWJ22} and use these metrics along with \textit{SR} to evaluate our probabilistic model. The results in \cref{table:probabilistic1} and \cref{table:probabilistic2} suggest our approach has an excellent ability to model the uncertainty in procedure planning and can produce both diverse and reasonable plans(visualizations available in the supplement). Specifically, our approach improves \textit{ModeRec} greatly for longer horizons. There is less uncertainty when $T$ = 3, thus the diffusion based models performs worse than the deterministic one.
\begin{table}[]
\centering
\resizebox*{0.8\linewidth}{!}{
\renewcommand\arraystretch{1}
\begin{tabular}{cccccc}
\hline
Metric$\downarrow$               & Model         & T = 3         & T=4           & T=5           & T=6           \\ \hline
\multirow{3}{*}{NLL}  & Deterministic & \textbf{3.57} & 4.29          & 4.70          & 5.12          \\
                     & Noise         & 3.58          & 4.04          & 4.45          & 4.79          \\
                     & Ours          & 3.61          & \textbf{3.85} & \textbf{3.77} & \textbf{4.06} \\ \hline
\multirow{3}{*}{KL-Div} & Deterministic & \textbf{2.99} & 3.40          & 3.54          & 3.82          \\
                     & Noise         & 3.00          & 3.15          & 3.30          & 3.49          \\
                     & Ours          & 3.03          & \textbf{2.96} & \textbf{2.62} & \textbf{2.76} \\ \hline
\end{tabular}}
\caption{Evaluation results of the plan distributions metrics.}
\label{table:probabilistic1}
\end{table}

\begin{table}[]
\centering
\resizebox*{0.9\linewidth}{!}{
\renewcommand\arraystretch{1}
\begin{tabular}{cccccc}
\hline
Metric$\uparrow$            & Model         & T = 3          & T=4            & T=5            & T=6            \\ \hline
\multirow{3}{*}{SR}       & Deterministic & \textbf{39.03} & 21.17          & 12.59          & 7.47           \\
                          & Noise         & 34.92          & 18.99          & 12.04          & 7.82           \\
                          & Ours          & 37.20          & \textbf{21.48} & \textbf{13.58} & \textbf{8.47}  \\ \hline
\multirow{3}{*}{ModePrec} & Deterministic & \textbf{55.60} & \textbf{45.65} & 35.47          & 25.24          \\
                          & Noise         & 51.04          & 43.90          & 34.35          & 24.51          \\
                          & Ours          & 53.14          & 44.55          & \textbf{36.30} & \textbf{25.61} \\ \hline
\multirow{3}{*}{ModeRec}  & Deterministic & 34.13          & 18.35          & 11.20          & 6.75           \\
                          & Noise         & \textbf{39.42} & 25.56          & 15.67          & 11.04          \\
                          & Ours          & 36.49          & \textbf{31.10} & \textbf{29.45} & \textbf{22.68} \\ \hline
\end{tabular}}
\caption{Evaluation results of diversity and accuracy metrics.}
\label{table:probabilistic2}
\vspace{-0.6cm}
\end{table}

\section{Conclusion}

In this paper, we have casted procedure planning in instructional videos as a distribution fitting problem and addressed it with a projected diffusion model. Compared with previous work, our model requires less supervision and can be trained with a simple learning objective.
%To apply diffusion model for procedure planning, we modify the input of diffusion model and add a condition projection operation into the learning process. 
We evaluate our approach on three datasets with different scales and find our model achieves the state-of-the-art performance among multiple planning horizons. Our work demonstrates that modeling action sequence as a whole distribution is an effective solution to procedure planning in instructional videos, even without intermediate supervision.

~\\ \noindent \textbf{Acknowledgements.} This work is supported by the National Key R$\&$D Program of China (No. 2022ZD0160900), the National Natural Science Foundation of China (No. 62076119, No. 61921006), the Fundamental Research Funds for the Central Universities (No. 020214380091), and the Collaborative Innovation Center of Novel Software Technology and Industrialization.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\setcounter{section}{0}
\setcounter{table}{0}
\setcounter{figure}{0}
\renewcommand\thesection{\Alph{section}}

\section{Supplementary Material}
\label{sec::supp}

Our supplementary material consists of the following: \cref{sec::implement} provides the details of our model architecture, training process and dataset curation. \cref{sec::baseline} describes the baselines we compare with. \cref{sec::first} shows the details of training the task classifier in the first learning stage. We talk about another different evaluation protocol used by previous approach\cite{DBLP:journals/ral/SunHLLZG22} and present the performance of our model with this protocol in \cref{sec::protocol}. Then in \cref{sec::bs}, we study how batch size affects the value of mIoU metric and show the results of evaluating mIoU with different batch size on the same model. In \cref{sec::prior}, we discuss the importance of introducing prior knowledge that the start/end observations are more related to the first/last actions to our model. Finally, we provide more results, details and visualizations about the ability to model uncertainty of our approach in \cref{sec::uncertainty_sup}.


\section{Implementation Details}
\label{sec::implement}

\subsection{Details of model architecture}
The maintain of our model is the learnable model $f_\theta$, which we implement as a basic 3-layer U-Net\cite{DBLP:conf/miccai/RonnebergerFB15}. As in \cite{DBLP:conf/icml/JannerDTL22}, each layer in our model consists of two residual blocks\cite{DBLP:conf/cvpr/HeZRS16} and one downsample or upsample operation. One residual block consists of two convolutions, each followed by a group norm\cite{DBLP:conf/eccv/WuH18} and Mish activation function\cite{DBLP:journals/corr/abs-1908-08681}. Time embedding is produced by a fully-connected layer and added to the output of first convolution. We apply a 1d-convolution along the planning horizon dimension as the downsample/upsample operation. Considering that the value of planning horizon is small ($T=\{3,4,5,6\}$), we set the kernel size of 1d-convolution as $2$, stride as $1$, padding as $0$ so the length change of planning horizon dimension keeps $1$ after each downsample or upsample.

The input for our model is the concatenation of task class, actions labels and observation features, so the size of feature dimension is $dim = L_c + L_a + L_o$. Here $L_c$ means the number of task classes in the dataset, $L_a$ is the number of different actions in the dataset and $L_o$ is the length of visual features. Our model embeds the input feature with shape $[dim \to 256 \to 512 \to 1024]$ in the downsample process and recover to the initial size in the upsample process as $[1024 \to 512 \to 256 \to dim]$.

For diffusion, we use the cosine noise schedule to produce the hyper-parameters $\{\beta_n\}_{n=1}^N$, which denote the ratio of Gaussian noise
added to the data at each diffusion step.

\subsection{Dataset curation details}
Each video in the dataset is annotated with action labels and temporal boundaries. That is, the start time and end time of each action in an instructional video are annotated as $\{s_i, e_i\}_{i=1}^{num}$, where $s_i$ and $e_i$ denote the start and end time of the $i_{th}$ action, $num$ denotes the number of actions in the video. We in this paper follow previous work\cite{DBLP:conf/eccv/ChangHXAFN20, DBLP:conf/iccv/BiLX21} to extract all action sequences with predicting horizon $T$ $\{[a_i, ..., a_{i+T-1}]\}_{i=1}^{num-T+1}$ from the given video which contains $num$ actions by sliding a window of size $T$. Thus each action sequence we need to predict can be presented as $\{a_i, a_{i+1}, ..., a_{i+T-1}\}$. We choose the video clip feature at the beginning time of action $a_i$ and clip feature around the end time of $a_{i+T-1}$ as the start observation $o_s$ and goal state $o_g$, respectively. Specifically, we first round the start and end time of this action sequence to get $\left \lfloor s_i \right \rfloor$ and $\left \lceil e_{i+T-1} \right \rceil$. Then we choose clip feature starts from $\left \lfloor s_i \right \rfloor$ and clip feature ends with $\left \lceil e_{i+T-1} \right \rceil$ as $o_s, o_g$, as shown in \cref{fig:curation}. Both clips are 3 seconds long.

\subsection{Details of training process}
We train our model with a linear warm-up scheme. For different datasets, the training scheme changes due to different scales. In CrossTask$_{Base}$, we set the diffusion step as $200$ and train our model for $12,000$ steps with learning rate increasing linearly to $8 \times 10^{-4}$ in the first $4,000$ steps. Then the learning rate decays to $4 \times 10^{-4}$ at step $10,000$. In CrossTask$_{How}$, we keep diffusion step as 200 and train our model for $24,000$ steps with learning rate increasing linearly to $5 \times 10^{-4}$ in the first $4,000$ steps and decays by 0.5 at step $10,000, 16,000$ and $22,000$. In NIV, the diffusion step is $50$ and we train $6,500$ steps due to its small size. The learning rate increases linearly to $3 \times 10^{-4}$ in the first $4,500$ steps and decays by $0.5$ at step $6,000$. The COIN dataset requires much more training steps due to its large scale. We set diffusion step as $200$ and train our model for $160,000$ steps. The learning rate increases linearly to $1 \times 10^{-5}$ in the first $4,000$ steps and decays by $0.5$ at step $14,000$ and step $24,000$. Then we keep learning rate as $2.5 \times 10^{-6}$ for the remaining training steps. The training batch size for all experiments is $256$. For the weighted loss in our training process, we set $w = 10$. All our experiments are conducted with ADAM\cite{DBLP:journals/corr/KingmaB14} on 8 NVIDIA TITAN Xp GPUs.

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{supp_new.pdf}
   \caption{Selection of observations with a given action sequence.}
   \label{fig:curation}
\end{figure}

\section{Baselines}
\label{sec::baseline}

\begin{table}
\centering
\resizebox*{1\linewidth}{!}{
\begin{tabular}{ccccc}
\hline
Choices               & T = 3          & T=4            & T=5            & T=6            \\ \hline
MLP+Crossentropy    & 81.94          & 82.61          & 83.14          & \textbf{84.08} \\ \hline
ResMLP+Crossentropy & 81.6           & 82.47          & 82.77          & 82.83          \\ \hline
ResMLP+MSEloss      & \textbf{94.38} & \textbf{83.64} & \textbf{83.37} & 83.85          \\ \hline
\end{tabular}
}
\caption{Task classification results in the first stage with different training choices for CrossTask$_{Base}$.}
\label{first_stage_res}
\end{table}

In this section, we introduce the baselines we used in our paper.

\noindent - $Random$. This policy randomly selects actions from the available action space in dataset to produce the plans.

\noindent - $Retrieval$-$Based$. Given the observations $\{o_s, o_g\}$, the retrieval-based method retrieves the closest neighbor by calculating the minimum visual feature distance in the train dataset. Then the action sequence associated with the retrieved result will be used as the action plan.

\noindent - $WLTDO$\cite{DBLP:conf/cvpr/EhsaniBRMF18}. This approach applies a recurrent neural network(RNN) to predict action steps with the given observation pairs.

\noindent - $UAAA$\cite{DBLP:conf/iccvw/FarhaG19}. UAAA is a two-stage approach which uses RNN-HMM model to predict action steps autoregressively.

\noindent - $UPN$\cite{DBLP:conf/icml/SrinivasJALF18}. UPN is a
physical-world path planning algorithm and learns a plannable representation to make predictions. To produce the discrete action steps, we follow \cite{DBLP:conf/eccv/ChangHXAFN20} to add a softmax layer to the output of this model.

\noindent - $DDN$\cite{DBLP:conf/eccv/ChangHXAFN20}. DDN model is a two-branch autoregressive model, which learns an abstract representation of action steps and tries to predict the state-action transition in the feature space.

\noindent - $PlaTe$\cite{DBLP:journals/ral/SunHLLZG22}. PlaTe model follows DDN and uses transformer modules in two-branch to predict instead. Note that the evaluation protocol of PlaTe is different with other models, so we move the comparison with PlaTe to supplementary material, which we will discuss later.

\noindent - $Ext$-$GAIL$\cite{DBLP:conf/iccv/BiLX21}. This model solves the procedure planning problem by reinforcement learning techniques. Similar to our work, Ext-GAIL decomposes the procedure planning problem into two sub-problems. However, the purpose of the first sub-problem in Ext-GAIL is to provide long-horizon information for the second stage while our purpose is to get condition for sampling.

\noindent - $P^3IV$\cite{DBLP:conf/cvpr/0004HDDWJ22}. P$^3$IV is a single-branch transformer-based model which augments itself with a learnable memory bank and an extra generative adversarial framework. Like our model, P$^3$IV predicts all action steps at once during inference process.

\section{Details of the first learning stage}
\label{sec::first}
In the first learning stage, we need to predict the task class with the given observations $\{o_s,o_g\}$. We use a simple 4-layer MLP model to achieve this and calculate the cross entropy loss for the output of the model and the ground truth task class label to train our model, except for CrossTask$_{Base}$. A two-layer Res-MLP \cite{DBLP:journals/corr/abs-2105-03404} trained with MSE loss is applied to CrossTask$_{Base}$, which can get a better result when $T = 3$. The task classification results in the first learning stage for different models with different training losses on CrossTask$_{Base}$ are shown in \cref{first_stage_res}.

\section{Evaluation with another evaluation protocol}
\label{sec::protocol}
As talked in \cref{sec::baseline}, PlaTe\cite{DBLP:journals/ral/SunHLLZG22} used another protocol for evaluation. P$^3$IV\cite{DBLP:conf/cvpr/0004HDDWJ22} later evaluated SR with this protocol to compare with PlaTe. We name this as "protocol 2". In all experiments of the main paper, we follow previous work\cite{DBLP:conf/eccv/ChangHXAFN20,DBLP:conf/iccv/BiLX21,DBLP:conf/cvpr/0004HDDWJ22} to use a 70\%/30\% split for training/testing and rely on a sliding window to get our learning data. In this section, we evaluate our model with "protocol 2" and compare our performance with previous works that evaluated with this protocol.

The main differences of "protocol 2" are as followed: a) "protocol 2" uses a 2390/360 split for train/test. b) "protocol 2" randomly selects one procedure plan with prediction horizon $T$ in each video for training and testing, rather than relying on a $T$-$size$ sliding window to consider all procedure plans in each video. c) Given the planning horizon $T$, "protocol 2" only predicts $T-1$ actions.

We evaluate our model with "protocol 2" on CrossTask and compare the results with the previous best approach, considering both short and long horizons. Specifically, we use CrossTask$_{Base}$ to compare with PlaTe for shorter horizons and CrossTask$_{How}$ to compare with P$^3$IV for longer horizons. In this way we align the visual features used in different approaches with our model to conduct a fair comparison. The task classification accuracy results for $T$ = $\{3, 4, 5, 6\}$ with "protocol 2" are provided in \cref{table0}. \cref{table1} shows the results of our model with short horizons, and \cref{table2} shows the results of SR metric with longer prediction horizons. Note that we compute mIoU by calculating the mean of every IoU for a single antion sequence rather than a mini-batch. We can see that our method keeps the top performance for all prediction horizons.

\begin{table}[t]
\centering
\resizebox*{0.8\linewidth}{!}{
\begin{tabular}{ccccc}
\hline
Model     & T = 3 & T=4   & T=5   & T=6   \\ \hline
CrossTask$_{Base}$ & 81.71 & 83.63 & 84.65 & 84.53 \\ \hline
CrossTask$_{How}$ & 91.78 & 93.31 & 93.50 & 93.75 \\ \hline
\end{tabular}
}
\caption{Task classification results with protocol2.}
\label{table0}
\end{table}

\begin{table}[t]
\resizebox*{1\linewidth}{!}{
\begin{tabular}{lcccccc}
\hline
\multirow{2}{*}{Models} & \multicolumn{3}{c}{T = 3} & \multicolumn{3}{c}{T = 4} \\ \cline{2-7} 
       & SR$\uparrow$    & mAcc$\uparrow$    & mIoU$\uparrow$    & SR$\uparrow$    & mAcc$\uparrow$    & mIoU$\uparrow$    \\ \hline
PlaTe\cite{DBLP:journals/ral/SunHLLZG22}  &   16.00    &    36.17     &   \color{gray}{65.91}      &   14.00    &    35.29     &    \color{gray}{55.36}     \\
Ours$_{Base}$   &   \textbf{33.61}    &    \textbf{50.83}     &    \color{gray}{49.86}     &   \textbf{24.17}    &    \textbf{51.48}     &    \color{gray}{54.33}     \\ \hline
\end{tabular}
}
\caption{Evaluation results with protocol2 on CrossTask. Prediction horizon set to $T = \{3, 4\}.$ Note that we compute IoU on every action sequence and take the mean as mIoU.}
\label{table1}
\end{table}

\begin{table}[h]
\centering
\resizebox*{0.7\linewidth}{!}{
\begin{tabular}{lcccc}
\hline
\multirow{2}{*}{Models} & T = 3                & T = 4                & T = 5                & T = 6                \\ \cline{2-5} 
       & SR$\uparrow$                   & SR$\uparrow$                   & SR$\uparrow$                   & SR$\uparrow$                   \\ \hline
P$^3$IV\cite{DBLP:conf/cvpr/0004HDDWJ22}   &         24.4             &         15.8             &          11.8            &           8.3           \\
Ours$_{How}$   & \textbf{53.06} & \textbf{35.28} & \textbf{21.39} & \textbf{13.33} \\ \hline
\end{tabular}
}
\caption{Evaluation results of SR with protocol 2 on CrossTask. Prediction horizon set to $T = \{3, 4, 5, 6\}.$}
\label{table2}
\end{table}

\section{Impact of batch size on mIoU}
\label{sec::bs}
As we discussed in the main paper, previous approaches calculate the IoU value on every mini-batch and take their mean as the final mIoU. However, the batch size value for different methods may be different, which results in an unfair comparison. In this section, we study the impact of batch size on mIoU, which can illustrate the importance for the standardization of computing mIoU.

We use our trained models to compute the mIoU metric on CrossTask with different evaluation batch size. Planning horizon is set to $\{3, 4, 5, 6\}$. The results are shown in \cref{table3}, which validate our thought and show the huge impact of batch size on mIoU. The value of mIoU evaluated on the same model can vary widely as batch size changes, so comparing mIoU with different evaluation batch size has no meaning. To address this problem, we standardize the way to compute mIoU as setting evaluation batch size to 1 at inference time.

\begin{table}[h]
\centering
\resizebox*{0.9\linewidth}{!}{
\begin{tabular}{cccccc}
\hline
                      & Batch size & T = 3 & T=4 & T=5 & T=6 \\ \hline
\multirow{4}{*}{Ours$_{Base}$} & 1         &  58.95     &  56.99   &   56.32  & 57.51    \\
                      & 32         &  68.03     &  67.14   &  67.10   &   \textbf{70.48}  \\
                      & 64         &  \textbf{71.46}     &  \textbf{69.64}   &  \textbf{67.39}   &   69.31  \\
                      & 128        &   71.01    &   67.26  &   64.53  &  63.19   \\ \hline
\multirow{4}{*}{Ours$_{How}$} & 1         &   66.57    &   65.13  &   65.32  &   65.38  \\
                      & 32         &   75.21    &   77.07  &   78.56  &   78.59  \\
                      & 64         &   79.74    &   81.74  &   \textbf{81.73}  &   \textbf{80.88}  \\
                      & 128        &   \textbf{80.50}   &  \textbf{82.32}   &  81.41   &  78.64   \\ \hline
\end{tabular}
}
\caption{Evaluation results of mIoU with different batch size on CrossTask.}
\label{table3}
\end{table}

\section{Role of prior knowledge}
\label{sec::prior}
In this section, we study the role of leveraging a prior knowledge that
the start/end observations are more related to the first/last
actions for our model.

Inspired by \cite{DBLP:conf/iccv/GaoG19}, we establish a baseline not using this prior knowledge by tiling the observations and task class conditions to the output of the U-Net encoder before decoding. $1 \times 1$ convolution is applied to reduce the channel dimension of observation features and class conditions. Then the features and conditions are replicate $k$ times($k$ is the horizon length of the U-Net encoder output) and concatenated along the channel dimension. We conduct experiments on CrossTask and the results are shown in \cref{tile}, which demonstrates that the introduced prior knowledge is quite useful to the learning process with visual features provided by CrossTask(Ours$_{Base}$), while not good when better visual features are provided(Ours$_{How}$).

\begin{table}[h]
\centering
\resizebox*{1\linewidth}{!}{
\begin{tabular}{ccccccl}
\hline
\multirow{2}{*}{} & \multicolumn{3}{c}{T=3}                                            & \multicolumn{3}{c}{T=4}                                                \\ \cline{2-7} 
                  & SR$\uparrow$                   & mAcc$\uparrow$                 & mIoU$\uparrow$                 & SR$\uparrow$                   & mAcc$\uparrow$                 & mIoU$\uparrow$ \\ \hline
Tile$_{Base}$              &        23.40              &         50.37             &       55.14               &         14.13             &       45.80               &    55.03                      \\
Ours$_{Base}$              &         \textbf{26.47}             &          \textbf{55.35}            &        \textbf{58.59}              &          \textbf{15.40}            &          \textbf{49.42}            &           \textbf{56.99}               \\ \hline
Tile$_{How}$              &           36.21           &         \textbf{65.12}             &       \textbf{66.79}               &         \textbf{22.31}             &      \textbf{59.00}                &        \textbf{66.20}                  \\
Ours$_{How}$              & \textbf{37.20} & 64.67 & 66.57 & 21.48 & 57.82 &        65.13                  \\ \hline
\end{tabular}
}
\caption{Ablation study on the role of prior knowledge.}
\label{tile}
\end{table}

\section{Additional study on modeling uncertainty}
\label{sec::uncertainty_sup}
In the main paper, we study the probabilistic modeling ability of our model on CrossTask$_{How}$ and show that our diffusion based model can produce both diverse and accurate plans. Here we provide more details, results and visualizations about modeling the uncertainty in procedure planning by our model.

~\\ \noindent \textbf{Details of evaluating uncertainty modeling.}
For the \textit{Deterministic} baseline, we just sample once to get the plan since the result for \textit{Deterministic} is certain when observations and task class conditions are given. For the \textit{Noise} baseline and our diffusion based model, we sample 1500 action sequences as our probabilistic result to calculate the uncertain metrics. Furthermore, in order to efficiently complete the process of 1500 sampling, we apply the DDIM \cite{DBLP:journals/corr/abs-2010-02502} sampling method to our model, with which one sampling process can be completed with 10 steps(accelerating the sampling for CrossTask and COIN by 20 times and NIV by 5 times). Note that the multiple sampling process is only required while evaluating probabilistic modeling and our model can generate a good plan just by sampling once.


\begin{table}[t]
\centering
\resizebox*{0.9\linewidth}{!}{
\renewcommand\arraystretch{1}
\begin{tabular}{@{}ccccc@{}}
\toprule
\multirow{2}{*}{} & \multicolumn{2}{c}{T=3} & \multicolumn{2}{c}{T=4} \\ \cmidrule(l){2-5} 
                  & KL-Div$\downarrow$       & NLL$\downarrow$      & KL-Div$\downarrow$       & NLL$\downarrow$      \\ \midrule
Deterministic     &       5.40       &    5.49      &       5.13       &     5.26     \\ \midrule
Noise             &       4.92        &     5.00     &       5.04       &     5.17     \\ \midrule
Ours              &      \textbf{4.85}        &     \textbf{4.93}     &       \textbf{4.62}       &     \textbf{4.75} \\ \bottomrule
\end{tabular}
}
\caption{Evaluation results of the plan distributions metrics on NIV.}
\label{table:nivprobabilistic1}
\end{table}

\begin{table}[t]
\centering
\resizebox*{1\linewidth}{!}{
\renewcommand\arraystretch{1}
\begin{tabular}{@{}ccccccc@{}}
\toprule
\multirow{2}{*}{} & \multicolumn{3}{c}{T=3} & \multicolumn{3}{c}{T=4} \\ \cmidrule(l){2-7} 
                  & SR$\uparrow$ & Prec$\uparrow$ & Rec$\uparrow$ & SR$\uparrow$ & Prec$\uparrow$ & Rec$\uparrow$ \\ \midrule
Deterministic     &  27.94  &    29.63      &    27.44     &  25.43  &    26.64      &    24.08     \\ \midrule
Noise             &  25.73  &    26.87      &    \textbf{38.37}     &  22.84  &    23.05      &    31.89    \\ \midrule
Ours              &  \textbf{31.25}  &    \textbf{31.78}      &     33.09    &  \textbf{26.72}  &    \textbf{29.10}      &    \textbf{33.08}     \\ \bottomrule
\end{tabular}}
\caption{Evaluation results of diversity and accuracy metrics on NIV.}
\label{table:nivprobabilistic2}
\end{table}

\begin{table}[t]
\centering
\resizebox*{0.9\linewidth}{!}{
\renewcommand\arraystretch{1}
\begin{tabular}{@{}ccccc@{}}
\toprule
\multirow{2}{*}{} & \multicolumn{2}{c}{T=3} & \multicolumn{2}{c}{T=4} \\ \cmidrule(l){2-5} 
                  & KL-Div$\downarrow$       & NLL$\downarrow$      & KL-Div$\downarrow$       & NLL$\downarrow$      \\ \midrule
Deterministic     &      \textbf{4.52}        &     \textbf{5.46}     &       \textbf{4.43}       &     \textbf{5.84}     \\ \midrule
Noise             &   4.55   &   5.50   &   4.52    &   5.92  \\ \midrule
Ours              &   4.76   &   5.71   &   4.62    &   6.03  \\ \bottomrule
\end{tabular}
}
\caption{Evaluation results of the plan distributions metrics on COIN.}
\label{table:coinprobabilistic1}
\end{table}

\begin{table}[t]
\centering
\resizebox*{1\linewidth}{!}{
\renewcommand\arraystretch{1}
\begin{tabular}{@{}ccccccc@{}}
\toprule
\multirow{2}{*}{} & \multicolumn{3}{c}{T=3} & \multicolumn{3}{c}{T=4} \\ \cmidrule(l){2-7} 
                  & SR$\uparrow$ & Prec$\uparrow$ & Rec$\uparrow$ & SR$\uparrow$ & Prec$\uparrow$ & Rec$\uparrow$ \\ \midrule
Deterministic     &  \textbf{27.96}  &    \textbf{34.35}      &     27.40    &  \textbf{19.98}  &    \textbf{30.65}      &    \textbf{19.63}     \\ \midrule
Noise             &  18.49   &   25.67    &   \textbf{29.82}    &   12.58    &   22.25    &   19.32  \\ \midrule
Ours              &  21.33   &   28.03    &   23.49    &   14.41    &   24.83    &    16.28   \\ \bottomrule
\end{tabular}}
\caption{Evaluation results of diversity and accuracy metrics on COIN.}
\label{table:coinprobabilistic2}
\end{table}

~\\ \noindent \textbf{Uncertainty modeling results on other datasets.}
We here provide the uncertainty modeling results on NIV(\cref{table:nivprobabilistic1},\cref{table:nivprobabilistic2}) and COIN(\cref{table:coinprobabilistic1},\cref{table:coinprobabilistic2}). Different with CrossTask, we here find the diffusion process still helps for NIV, but harms the performance on COIN. We suspect the reason for this is that data scales and variability in goal-conditioned plans of these datasets are different. To verify our thought, we calculate the average number of distinct plans with the same start and goal observations in these datasets, as in \cite{DBLP:conf/cvpr/0004HDDWJ22}. The results in \cref{table:paths} show that the variability in CrossTask is the much larger than the other two datasets. And longer horizons can bring more diverse plans for all datasets. Thus our diffusion based approach performs best on CrossTask with longer horizons $T$ = 4, 5, 6. For NIV, we hypothesize that our model can fit this small dataset well thus adding noises to the learning does not harm the accurancy of planning much. With our diffusion based method, the model makes a good trade-off between the diversity and accuracy of planning, resulting in the best results. However, for the large COIN dataset, our model can not fit it really well and introducing noises to our model just makes the learning harder. 

~\\ \noindent \textbf{Visualizations for uncertainty modeling.}
In Figures \cref{fig:sup1} to \cref{fig:sup4}, we show the visualizations of different plans with the same start and goal observations proceduced by our diffusion based model CrossTask$_{How}$ for different prediction horizons. In each figure, the images denote the start and goal observations, the first row denotes the ground truth actions(rows with "GT"), the last row denotes a failure plan(rows with "Failure") and the middle rows denote multiple reasonable plans produced by our model, respectively. Here the reasonable plans are plans that share the same start and end actions with the ground truth plan and exist in the test dataset.

\begin{table}
\centering
\resizebox*{0.7\linewidth}{!}{
\begin{tabular}{@{}ccccc@{}}
\toprule
Datasets  & T=3  & T=4  & T=5   & T=6   \\ \midrule
CrossTask & 4.02 & 7.82 & 10.92 & 11.51 \\
NIV       & 1.31 & 1.50 & -     & -     \\
COIN      & 1.74 & 2.52 & -     & -     \\ \bottomrule
\end{tabular}
}
\caption{Average number of paths with the same start and goal states across multiple horizons and datasets.}
\label{table:paths}
\end{table}

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{image1_sup.pdf}
   \caption{Visualization of diverse plans produced by our model with horizon $T = 3$.}
   \label{fig:sup1}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=1\linewidth]{image2_sup.pdf}
   \caption{Visualization of diverse plans produced by our model with horizon $T = 4$.}
   \label{fig:sup2}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=1\linewidth]{image3_sup.pdf}
   \caption{Visualization of diverse plans produced by our model with horizon $T = 5$.}
   \label{fig:sup3}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=1\linewidth]{image4_sup.pdf}
   \caption{Visualization of diverse plans produced by our model with horizon $T = 6$.}
   \label{fig:sup4}
\end{figure*}
%%%%%%%%% REFERENCES
% \newpage

% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }

\end{document}
