\section{Proposed Approach}
Our approach is focused on synthetic scene graphs and unsupervised traffic scene generation, where pairs of synthetic and real scenes are not available. We derive a synthetic scene graph from a procedurally generated scene that does not provide visual characteristics such as textures or materials. For the synthesized scene graph, we then produce an image of the corresponding traffic scene, which resembles the content of the underlying synthetic scene and the realistic appearance of the target data. Figure~\ref{fig:scheme} provides an overview of our method. It highlights the two main parts of the approach: synthetic scene graph generation and realistic image generation.

\subsection{Synthetic 3D Scene Graphs}
We adopt the setup from \cite{Johnson2018} for traffic scene data, where every scene graph is represented by nodes $n_i$ associated to each object $i$ in the scene and edges $e_{ij}$ encode relations between particular nodes $n_i$ and $n_j$. Our graph processor is built upon \cite{Ashual2019} but extends it in several ways, allowing 3D scene graph construction. In addition to the simplified relations - \textit{e.g.}, \textit{left of}, \textit{above} \cite{Ashual2019}, we integrate 3D information about the scene in form of spatial relations (\textit{in front of}, \textit{behind})  between objects and spatial attributes (depth component \textit{z}) of particular objects. Importantly, this information is available in simulation at no cost. To generate a comprehensive traffic scene, we extend the objects list and integrate background classes such as \textit{sky, building, vegetation}.

\input{figure_cs_bdd_small_h}

Therefore, every node in our graph is represented as $ n_i = [o_i, l_i, z_i], o_i \in \mathbb{R}, l_i \in \{0,1\}^{L\times L}, z_i \in \{0,1\}^{Z}$, where $o_i$ takes one of the indexes of $C$ possible object classes, $L$ is size of the grid to denote objects location $l_i$ and $Z$ is the scene depth to denote objects position $z_i$ along the z-axis. Analogously, every edge is represented as $e_{i,j} \in \mathbb{R}$, taking one of the values from predefined dictionary of relation types.

To train the graph processor, we also obtain mask $m_i$ and the bounding box $b_i \in \mathbb{R}^{4}$ for each object $i$ from the simulation. Hence, our graph processor $P$ is trained in a supervised manner to produce the full scene layout $t \in \mathbb{R}^{H \times\ W \times C}$. Similar to \cite{Johnson2018}, our graph processor $P$ is composed of 3 networks: \textit{graph network}, \textit{mask regression network} and \textit{box regression network}. The graph network is constructed of graph convolutional layers which extract features from scene graphs and encode per-object layout embedding, mask network consists of several deconvolution layers, and box network is a Multi-Layer Perceptron (MLP). The last two networks are dedicated to predicting masks and bounding boxes of the objects present in the scene.

As an output of the first step, we obtain a synthetic scene graph alongside the corresponding layout $t$ produced by the graph processor $P$. Both scene graphs and layouts are domain agnostic, therefore, we omit visual information from the synthetic domain (rendering) and merely encode content.

\subsection{Traffic Scenes}
In the next step, we apply the image generator $G$ to layout $t$ to produce a realistic traffic scene image $x$, which visually resembles the target data $X=\{\hat{x} \in \mathcal{X}\}$ while at the same time preserving the content of the original scene graph. Since our approach focuses on the realistic image generation from the synthetic scene graphs, it requires unsupervised training, as synthetic-real pairs are not available. To enable unsupervised training we employ adversarial \cite{Goodfellow2014} and contrastive \cite{Gutmann2010} losses. Hence, our final objective is as follows:
\begin{align}
\begin{split}
&\mathcal{L} = \mathcal{L}_{SG} + \mathcal{L}_{TS} \\
&= \mathcal{L}_{MSE}(b_i,\hat{b}_i) + \mathcal{L}_{GAN}(m_i,\hat{m}_i) + \mathcal{L}_{FM}(m_i,\hat{m}_i)\\
& + \mathcal{L}_{GAN}(x,\hat{x}) + \mathcal{L}_{NCE}(x,\hat{x})\\
\end{split}
\label{eq:loss}
\end{align}
%
Where $\mathcal{L}_{FM}$ is a \textit{feature matching loss} \cite{Salimans2016}, $\mathcal{L}_{NCE}$ is a \textit{multilayer, patch-wise contrastive loss} \cite{Chen2020, Park2020} and $\mathcal{L}_{FM}$ is an \textit{adversarial loss} applied both to masks $m_i$ and images $x$. With $\hat{}$ we denote either a synthetic ground-truth (for masks) or a target dataset (for images):
\begin{align}
\begin{split}
&\mathcal{L}_{GAN}(m_i,\hat{m}_i) = \\
&\mathbb{E} \log D_{m}(m_i) + \mathbb{E} \log (1-D_{m}(\hat{m}_i))\\
&\mathcal{L}_{FM}(m_i,\hat{m}_i) = \norm{\mathbb{E}f(m_i) - \mathbb{E}_{x \sim}f(\hat{m}_i)}_2^2 \\
&\mathcal{L}_{GAN}(x,\hat{x}) = \mathbb{E}_{x\sim X} \log D(x) + \mathbb{E}_{\hat{x}\sim X} \log (1-D(\hat{x}))\\
&\mathcal{L}_{NCE}(x,\hat{x}) = \mathbb{E}_{x\sim X} \sum_{l} \sum_{s} \ell(\hat{x}_l^s, x_l^s, \overline{x}_l^s)
\end{split}
\label{eq:losses}
\end{align}
Where $f$ denotes activations on an intermediate layer of the discriminator, $\ell$ is a cross-entropy loss function for a positive pair of examples and $x_l^s$ are the features of the generator from $l$-th layer at $s$-th location \cite{Chen2020}.
