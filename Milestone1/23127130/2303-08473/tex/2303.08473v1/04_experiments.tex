\section{Experiments}
To properly evaluate our approach, we design the synthetic graph generator to operate on bounding boxes and semantic as input. This allows producing synthetic scene graphs for any existing synthetic image dataset, which provides 3D bounding boxes and semantic segmentation labels. In particular we use 2 synthetic traffic datasets for scene graph generation: PfB \cite{Richter2017} and Synscapes \cite{Wrenninge2020}. Both are large-scale datasets that provide about 25000 urban traffic environment images at $1914\times1052$ and $1440\times720$ alongside 2D/3D bounding boxes, semantic and instance maps.

In the image generation part, we utilize two real datasets, Cityscapes \cite{Cordts2016} and Berkley Deep Drive \cite{Yu2018} as providers of appearance characteristics. The first one comprises about 3000 images of traffic scenes from several German cities with finely annotated semantic maps. The second one includes about 100000 images of the streets of US American cities, of which we only use 10000.

\input{figure_spatial}
\input{figure_behind_before}
\input{figure_man_class}
\input{figure_comparison}

\subsection{Synthesis}
We first produce a set of synthetic scene graphs that includes 5k samples. We rely on this set to train our scene graph generator. For both experiments with Cityscapes and BDD, our setup is similar. We conduct all experiments at the resolution of $256x\times512$ pixels. The training continues for 150 epochs with a learning rate of $1e-3$ and momentum $0.99$. Figures \ref{fig:results_cityscapes} and \ref{fig:results_bdd} show the results for the training on Cityscapes and BDD, respectively. Additionally, in figure~\ref{fig:comparison} we compare our method with state-of-the-art unsupervised image translation techniques - \textit{e.g.}, CycleGAN \cite{Zhu2017}, MUNIT \cite{Huang2018}. This is also reflected in the FID score - CycleGAN achieves 103.05, MUNIT - 75.98 and our method 47.26.

It is evident from the aforementioned figures that the traffic scene generator picks up the appearance of both datasets nicely, producing in general realistic images. Being slightly inaccurate in fine details (especially those of cars), it is yet very consistent on the class and image level. The generator produces a very congruous appearance of the objects of following classes: \textit{road, sidewalk, building, vegetation}.
We also want to highlight the conditioning of the generation process illustrated in Figures \ref{fig:results_cityscapes} and \ref{fig:results_bdd} in sections \ref{sec:experiments_classes} and \ref{sec:experiments_spatial}. There we demonstrate traffic scene manipulation with regard to newly introduced classes, spatial attributes and spatial relations to ensure that our improvements take effect.

\subsection{Spatial Attributes and Relations}\label{sec:experiments_spatial}
Synthetic scene graphs let us leverage spatial information about the traffic scene. Figure \ref{fig:results_z} visualizes 3 traffic scenes generated from the same scene graph by varying the z-component of a particular object in the graph. In the first image, no car is present in the scene. In the following images, we put a car in the scene and change its position by manipulating the value of the \textit{z}-attribute.

The 3D information for the scene not only enables changing the z-attribute of the objects, but also enables spatial relation between them. Thus, Figure \ref{fig:behind_before} shows a traffic scene produced by the scene graph by swapping the relation between two cars, which appear in the scene having approximately the same size when not bounded by any spatial relation. The right car gets pulled toward the ego vehicle, while the left car appears farther away from the case of \textit{in front of} relation. When the relation changes to \textit{behind}, the left car is rendered significantly bigger than before and bigger than the right car.

\input{table_drn_pfd_cs}

\subsection{Traffic Scene Classes}\label{sec:experiments_classes}

Additionally to objects (\textit{car, bus}) we introduce multiple classes which are characteristic for traffic scene scenarios. Such classes include \textit{road, sidewalk, vegetation, building, sky}. To verify that introducing background classes is effective, we perform several experiments by manipulating the classes in the traffic scenes. Figure \ref{fig:manipulation_classes} shows the effects of such manipulation: changing classes of the particular nodes in the scene graph results in the respective adjustment of the semantic layout of the generated scene.

In addition to, to a qualitative evaluation of the approach, we conduct several experiments on the downstream task of semantic segmentation. This provides a quantitative assessment of the data generated by our method.

\subsection{Image Segmentation}
To assess the proposed method quantitatively, we train a state-of-the-art semantic segmentation method on the data produced by our method as well as on the underlying synthetic data. Here we focus on the scene graphs produced from the Cityscapes simulation - \textit{i.e}, we randomly generate 2 sets of scene graphs with 5000 and 10000 samples. For these scene graphs, we generate corresponding images alongside semantic maps. They are then used to train DRN \cite{Yu2016}.

We train a segmentation network for 200 epochs on 8 classes of interest and evaluate on the real Cityscapes \textit{val} dataset. We provide per class IoU score and meanIoU also referred as \textit{Jaccard Index} \cite{Everingham2015} in Table~\ref{tab:iou_unsupervised}. The table demonstrates that DRN trained on the generated 5k dataset lies 5\% behind DRN trained on the original simulated PFD 25k dataset. Doubling the number of scene graphs and corresponding images reduces the gap to 2 points. Additionally, we report in the table~\ref{tab:iou_unsupervised} the results of the experiment where scene graphs follow the statistics of a real dataset. Knowing the target data class ratio allows us to sub-sample the data and keep those synthetic scene graphs whose layouts follow target data class ratio. This makes it possible to reduce the scene graph number to 2000 samples and improve segmentation performance by 5\% compared to the original 25k dataset.
