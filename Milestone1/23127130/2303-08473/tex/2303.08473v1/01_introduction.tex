\section{Introduction}

A broad variety of real world scenarios require autonomous navigation systems to rely on machine learning-based perception algorithms. Such algorithms are knowingly data dependent, yet data acquisition and labeling is a costly and tedious process. It is associated with manual labor, must handle rare \textit{long tail} corner case events, and could be hard constrained by ethical aspects e.g., in case of near-accident scenarios.

\input{figure_teaser_1col}

One of the common alternatives to real data acquisition and annotation is represented by simulation and synthetic data. Simulation has a long history in driver assistance systems, but with the renaissance of neural networks, the research community has strengthened efforts in this direction and many synthetic datasets \cite{Ros2016, Richter2017, Wrenninge2020} and simulation systems \cite{Dosovitskiy2017, Shah2017} appeared.

\input{figure_scheme_tikz}

Despite being a powerful research tool, synthetic data typically reveals a significant domain gap with respect to target real data. The underlying phenomenon, where marginal distributions in both domains differ, has been defined by \cite{Sugiyama2012} as a \textit{covariate shift}. This means that \textit{i.i.d} assumption does not hold for a synthetic-real setup. Simply stated, synthetic and real domains differ both in content and appearance. This problem is normally a subject for domain adaptation methods such as \textit{sim2real} domain transfer.

In this work, we propose overcoming the domain gap between both domains by eliminating the rendering part from the pipeline. The intuition behind this concept tells that rendered images introduce a bias towards the underlying domain, which is then commonly leveled out by e.g., style transfer methods. Our main idea is to replace the rendering with abstract scene representation and directly synthesize realistic images of traffic scenes from it. We utilize scene graphs as such abstract representations. Scene graphs encode objects in the scene as nodes and relations between them as edges \cite{Johnson2015}, in addition, they can also integrate certain characteristics of the objects as attributes \cite{Ashual2019}.

In this regard, scene graphs are domain agnostic as they can be generated in simulation and be applied to real-world data without a strong domain gap. In fact, scene graphs are fairly simple to simulate and manipulate, which allows for domain randomization and data generation of potentially arbitrary size and variance.

Derived from a simulation, synthetic scene graphs incorporate relevant objects as \textit{car, person} etc. and relations like \textit{left to}, \textit{right to}. We additionally extend this setup with necessary traffic scene classes like \textit{road, sidewalk, building, vegetation}. It is important to note that simulation provides 3D information about the scene, which can be introduced into the scene graph in the form of spatial attributes and spatial relations between objects. Moreover, synthetic annotations provided by simulation are pixel precise and could be used in a downstream task.

In this work, we propose synthetic 3D scene graphs with spatial components. We also provide the aforementioned graph representations for existing synthetic urban traffic datasets PfB \cite{Richter2017} and Synscapes \cite{Wrenninge2020}. To enable realistic traffic scene generation from synthetic scene graphs, we propose a neural network architecture that supports unsupervised training. We demonstrate the benefit of our approach with an online tool through traffic scene generation and manipulation: \href{https://artemsavkin.github.io/sg2ts/}{Demo}.