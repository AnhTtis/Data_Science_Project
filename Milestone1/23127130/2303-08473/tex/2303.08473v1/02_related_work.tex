\section{Related Work}

\textit{Rendering.}
Historically, traffic scene synthesis was commonly achieved through computer graphics. Multiple works focused on physically realistic rendering of urban traffic environments: SYNTHIA \cite{Ros2016}, Virtual KITTI \cite{Gaidon2016}, PfB \cite{Richter2017}, VKITTI2 \cite{Cabon2020}. The most recent and visually realistic one is arguably Synscapes \cite{Wrenninge2020}. Simulation can be applied not only in outdoor environments, but also in indoor scene synthesis for autonomous agents \cite{McCormac2017}, \cite{Qiu2017}. Although rendered data reveals a high level of realism and a great deal of variations, it is still affected by a significant domain gap with respect to real data when it is used for training machine learning approaches, in particular, neural networks.\\
\textit{Domain Transfer.}
To mitigate the limitation introduced by \textit{sim2real} domain shift, research has focused on data synthesis using deep neural networks. The vast majority of such models \cite{Odena2017}, \cite{Gulrajani2017}, \cite{Brock2018}, \cite{Karras2019}, utilizes adversarial frameworks (GAN) \cite{Goodfellow2014} for image generation. Many of them condition the generation process on visual artifacts or descriptions derived from available labels (semantics, edges etc.), such as Pix2Pix \cite{Isola2017}, CRN \cite{Chen2017}, Pix2PixHD \cite{Wang2018} and SPADE \cite{Park2019}. There are also several works which employed unsupervised domain adaptation from synthetic to real images, such as CycleGAN \cite{Zhu2017, Almahairi2018}, DIRT-T \cite{Shu2018}, MUNIT \cite{Huang2018}.\\
\textit{Scene Graphs.} Another relevant research direction is represented by the use of more domain invariant scene descriptions such as text \cite{Zhang2017} or scene graphs. Scene graphs are abstract data structures used for describing scenes by encoding objects as nodes and relations between them as edges. Thus, the whole scene can be represented as a directed graph. Scene graphs have been used as an alternative to natural language description for image retrieval \cite{Johnson2015} and image description \cite{Newell2017}. Most recent works on scene graphs for image generation include \cite{Johnson2018}, \cite{Ashual2019} and \cite{Dhamo2020}. \cite{Ashual2019} proposed dual layout and appearance embedding for better matching of generated scenes and underlying scene graphs. \cite{Dhamo2020} utilizes scene graphs as an intermediate representation for image manipulation without direct supervision. A lot of works in the area of scene graphs rely on the Visual Genome dataset \cite{Krishna2017}, which provides image samples annotated with scene graphs. To our knowledge, there is no such dataset available for urban traffic environments, so we provide scene graph annotations for commonly used public datasets, in particular, PfB \cite{Richter2017} and Synscapes \cite{Wrenninge2020}.