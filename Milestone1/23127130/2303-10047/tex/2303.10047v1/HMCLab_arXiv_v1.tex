
% \documentclass[draft]{agujournal2019}
\documentclass[a4paper]{article}
% \usepackage{url}
\usepackage{hyperref} 
\usepackage{lineno}
\usepackage{authblk}
%\usepackage{soul}
\usepackage[T1]{fontenc}
\usepackage[hmargin=2.5cm]{geometry}
% \linenumbers
\hyphenation{HMC-Lab}


\usepackage{apacite}
\usepackage[round,sort,comma,numbers]{natbib}
\setcitestyle{authoryear}

%% Custom packages
\usepackage{amssymb,amsmath}

\DeclareMathOperator{\EX}{\mathbb{E}}
\usepackage{ctable} 

%%%%%%%%%%%%%%%%%
%% Suppress month from biblio
\AtBeginDocument{\renewcommand{\APACrefYearMonthDay}[3]{\BBOP{#1}\BBCP}}


\begin{document}

\title{HMCLab: a framework for solving diverse geophysical inverse problems using the Hamiltonian Monte Carlo method}

% \author{Andrea Zunino$^*$, Lars Gebraad$^*$, Alessandro Ghirotto and Andreas Fichtner}
\author[1]{Andrea Zunino$^{\dagger}${\footnote{Corresponding author: Andrea Zunino \href{mailto:andrea.zunino@erdw.ethz.ch}{andrea.zunino@erdw.ethz.ch}}} }
\author[1]{Lars Gebraad$^{\dagger}$}
\author[2]{Alessandro Ghirotto}
\author[1]{Andreas Fichtner}
\affil[1]{\small Department of Earth Sciences, ETH Zurich, Switzerland}
\affil[2]{\small Applied Geophysics Laboratory, DISTAV University of Genoa, Italy}


\date{\today}

% \affiliation{1}{Department of Earth Sciences, ETH Zurich, Switzerland}
% \affiliation{2}{Applied Geophysics Laboratory, DISTAV University of Genoa, Italy}

% \correspondingauthor{Andrea Zunino}{andrea.zunino@erdw.ethz.ch}

\maketitle

%\footnote[3]{Corresponding author: Andrea Zunino \href{mailto:andrea.zunino@erdw.ethz.ch}{andrea.zunino@erdw.ethz.ch}}

\begin{abstract}
  The use of the probabilistic approach to solve inverse problems is becoming more popular in the geophysical community, thanks to its ability to address nonlinear forward problems and to provide uncertainty quantification. However, such strategy is often tailored to specific applications and therefore there is a lack of a common platform for solving a range of different geophysical inverse problems and showing potential and pitfalls of the methodology.
  In this work, we demonstrate a common framework within which it is possible to solve such inverse problems ranging from, e.g, earthquake source location to potential field data inversion and seismic tomography. 
  This approach, in fact, can provide probabilities related to certain properties or structure of the subsurface, such as histograms of the value of some physical property, the expected volume of buried geological bodies or the probability of having boundaries defining different layers.
    Thanks to its ability to address high-dimensional problems, the Hamiltonian Monte Carlo (HMC) algorithm has emerged as the state-of-the-art tool for solving geophysical inverse problems within the probabilistic framework. HMC requires the computation of gradients, which can be obtained by adjoint methods. This unique combination of HMC and adjoint methods is what makes the solution of tomographic problems ultimately feasible.
  These results can be obtained with ``HMCLab'', a numerical laboratory for solving a range of different geophysical inverse problems using sampling methods, focusing in particular on the HMC algorithm. HMCLab consists of a set of samplers (HMC and others) and a set of geophysical forward problems. For each problem its misfit function and gradient computation are provided and, in addition, a set of prior models can be combined to inject additional information into the inverse problem. This allows users to experiment with probabilistic inverse problems and also address real-world studies. We show how to solve a selected set of problems within this framework using variants of the HMC algorithm and analyze the results. HMCLab is provided as an open source package written both in Python and Julia, welcoming contributions from the community.
\end{abstract}


% \section*{Plain Language Summary}
% We describe a framework aimed to help geophysicists solve a range of inference problems, i.e., constructing models of the subsurface from observed data, including uncertainties on their results. We show how a selected set of different problems can be solved with this approach, providing information about the structure and properties of the subsurface. We provide high-level explanations of the methodologies hoping to further improve the understanding of these algorithms in geoscience. We aim at facilitating the experimentation with and development of these inverse strategies to solve research problems. Example problems with computer-generated and real data are provided, which show how the algorithms solve certain geophysical problems. These included problems can serve as the basis for researchers to analyse their own data sets.


\section{Introduction}

In the probabilistic approach, an inverse problem essentially represents an indirect measurement where the knowledge about the observed data and model parameters is completely expressed in terms of probabilities \citep{tarantolaInverseProblemTheory2005a}. Within such formalism, the general solution to the inverse problem is a probability density function (PDF), i.e., the posterior PDF (see \citet{tarantolaInverseProblemsQuest1982a} and \citet{mosegaardProbabilisticApproachInverse2002} for a detailed explanation).
The posterior PDF is constructed from the combination of two pieces of separate information: 1) the prior knowledge on the model parameters, expressed by the PDF $\rho(\mathbf{m})$, where $\mathbf{m}$ represents the model parameters and 2) the information provided by the experiment, described by $L(\mathbf{m})$. The posterior distribution, under certain fairly wide assumptions, is then given by \citep{mosegaardProbabilisticApproachInverse2002,tarantolaInverseProblemTheory2005a}:
\begin{linenomath*}
\begin{align}
  \sigma(\mathbf{m}) = k \, \rho(\mathbf{m}) \, L(\mathbf{m}).
\end{align}
\end{linenomath*}
Since $\sigma(\mathbf{m})$ is a PDF, it requires to evaluate the relevant integrals to find features of interest. For example, calculating the expected model given the data requires evaluating the following integral
\begin{linenomath*}
\begin{equation}
  \label{eq:mcmcexpectation}
  \EX \left[ \mathbf{m} \right] = \int_{M} \mathbf{m} \, \sigma(\mathbf{m}) \, \mathrm{d}\mathbf{m},
\end{equation}
\end{linenomath*}
where $M$ represents the whole model space.

In the particular case of linear forward models and Gaussian uncertainty, the probabilistic formalism provides the same closed-form solution than the classical least squares approach. In general, however, such high-dimensional integrals cannot be computed. Therefore, we resort to \emph{sampling}, a technique to approximate the computation of high-dimensional integrals. By generating points in the model space whose density (number of points per unit volume) is proportional to the posterior $\sigma(\mathbf{m})$, i.e., samples, one greatly reduces the amount of computations required to estimate statistics (such as $\EX \left[ \mathbf{m} \right]$) compared to a systematic grid search.
Markov Chain Monte Carlo (MCMC) methods provide a clever way to construct a Markov chain that produces samples drawn for the target distribution, i.e., the posterior PDF.
Statistical analysis of the samples obtained with MCMC then provides the answer to any inquiry in terms of probability of certain events, i.e., specific features of the solution. Practically, this means we can compute probabilities related to particular properties or structures of the solution. For instance, we might be interested in the probability of a certain geological body to have a certain volume, or the probability that there is a continuous permeable layer connecting two locations in the subsurface. \\
MCMC to sample target distributions has evolved from the appearance of the original Metropolis algorithm in physics \cite[e.g.,][]{metropolisMonteCarloMethod1949,metropolisEquationStateCalculations1953a} into a plethora of variants in many different scientific fields, including geophysics \cite[see ][for a review]{sambridgeMonteCarloMethods2002}. The MCMC method to solve inverse problems in geophysics has a long history, which dates back to the pioneering work of \citet{keilis-borokInverseProblemsSeismology1967} and \citet{pressEarthModelsObtained1968}, which were contemporary to the classic work of \citet{backusNumericalApplicationsFormalism1967a} on geophysical inverse theory. The formalization of a comprehensive theoretical framework based on probability theory for geophysical problems was started in the 80s by \citet{tarantolaInverseProblemsQuest1982a} and extended later on to include MCMC sampling methods to solve nonlinear inverse problems  \cite[e.g.,][]{mosegaardMonteCarloSampling1995,mosegaardProbabilisticApproachInverse2002,tarantolaInverseProblemTheory2005a,mosegaardQuestConsistencySymmetry2011}. An extensive review can be found in \citet{debskiChapterProbabilisticInverse2010}.

There has been a recent increase in the use of Monte Carlo methods to solve (geophysical) inverse problems, essentially for two reasons: I) relatively recent advances in the sampling algorithms and II) substantial increase in the available computational resources. These advances now allow us to tackle medium (hundreds of model parameters) to relatively large-scale (tens of thousands model parameters) inverse problems within the probabilistic framework. 
Regarding I), the literature provides a large collection of generic algorithms to perform Monte Carlo sampling, including classic MCMC \cite[e.g.,][]{hastingsMonteCarloSampling1970}, slice sampling \cite[e.g.,][]{nealSliceSampling2003}, Gibbs sampling \cite[e.g.,][]{gemanStochasticRelaxationGibbs1984},  rejection sampling \cite[e.g.,][]{gilksAdaptiveRejectionMetropolis1995}, sequential Monte Carlo \cite[e.g.,][]{liuSequentialMonteCarlo1998} and trans-dimensional Monte Carlo \cite[e.g.,][]{greenReversibleJumpMarkov1995a}, just to cite some of the main categories. The amount of literature dedicated to such algorithms from different fields is so vast that it would be pointless to attempt to give an overview here. More specifically, in geophysics there has been a constant increase in the number of algorithms proposed both for (pseudo-)sampling the posterior PDF and performing global optimization. These include, for instance, an extension of the classic Metropolis-Hasting sampler \cite[e.g.,][]{mosegaardMonteCarloSampling1995},
simulating annealing for global optimization \cite[e.g.,][]{stoffaNonlinearMultiparameterOptimization1991},
a strategy based on the nearest neighbour (Voronoi) partition of the model space  \cite[e.g.,][]{sambridgeGeophysicalInversionNeighbourhood1999},
joint inversion of different geophysical data with a cascade MCMC \cite[e.g.,][]{boschLithologicTomographyPlural1999},
trans-dimensional MCMC inversion \cite[e.g.,][]{malinvernoParsimoniousBayesianMarkov2002,bodinSeismicTomographyReversible2009} and samplers with geostatistical-based priors \cite[e.g.,][]{hansenInverseProblemsNontrivial2012a,zuninoMonteCarloReservoir2015}.
Regarding II), the geophysicist's arsenal for solving inverse problems now includes large high-performance computing resources and more easily accessible computation accelerators such as GPUs, TPUs and high-thread-count CPUs.

Traditional algorithms such as the random walk Metropolis may find difficult to setup an efficient sampler for large problems because of two reasons. The first is the property of generating correlated samples which requires a large number of iterations to obtain reliable statistics. The second is the difficulty for the proposal mechanism to generate high-probability models. In high-dimensional spaces, simply randomly perturbing a model will be very unlikely to produce another model with a higher posterior PDF. The reason is that high-dimensional spaces tend to be very empty (``curse of dimensionality'') and so the vast majority of search directions (random perturbations) will point to low probability regions, making the overall algorithm inefficient \citep{curtisPriorInformationSampling2001a}. 
If strong prior information is available and if sampling directly the prior is a possibility, the extended Metropolis algorithm \citep{mosegaardMonteCarloSampling1995} can offer a substantial improvement in terms of efficiency. Nevertheless, defining a geologically realistic prior and being able to sample it may not be an easy task in practice. 
Other approaches are based on an adaptive algorithm which, for instance, computes an approximate local covariance matrix and then samples such information to increase the chance of moving in a direction of higher probability \cite[e.g.,][]{gilksMarkovChainMonte1996}.
A recently proposed methodology for improving the proposal strategy is that of constructing an ad hoc proposal PDF based on the results of a simplified deterministic inversion \citep{khoshkholghInformedProposalMonte2021,khoshkholghFullwaveformInversionInformedproposal2022}, which will make the sampling more efficient in practice, without altering the final equilibrium distribution. This may enable the solution of large problems, although such methodology requires solving an additional inverse problem beforehand and performing some sort of interpretation in addition to the estimation of the modeling error.

The Hamiltonian Monte Carlo (HMC) method has recently gained attention in solid Earth geophysics because of its peculiar properties \citep{duaneHybridMonteCarlo1987,nealMCMCUsingHamiltonian2012,fichtnerHamiltonianMonteCarlo2019}. It combines sampling with ideas from the field of optimization, where the proposal mechanism exploits also information coming from the gradient of the posterior distribution. This unique combination enables a more efficient solution of problems when the calculation of gradients is not computationally too expensive with respect to the cost of simulating the forward problem, e.g., when adjoint methods \citep{tarantolaInversionSeismicReflection1984,trompSeismicTomographyAdjoint2005a,fichtnerAdjointMethodSeismology2006a,plessixReviewAdjointstateMethod2006} come into play \cite[e.g.,][]{zuninoIntegratingGradientInformation2018,fichtnerHamiltonianMonteCarlo2019,gebraadBayesianElasticFullWaveform2020}. This is effectively a result of the No Free Lunch-theorem described by \citet{wolpertNoFreeLunch1997}: one applies prior knowledge to the objective function and its properties (i.e., cheaply available gradient information), whereby it becomes possible to select a relatively efficient algorithm.
HMC is capable of generating more uncorrelated samples compared to traditional purely random-walk based algorithms such as the random walk Metropolis algorithm \citep{nealMCMCUsingHamiltonian2012}, producing more accurate statistical estimations with a smaller number of sampler. Thanks to this property, HMC is more suitable to address high-dimensional inverse problems than traditional derivative-free sampling methods. In fact, the cost of generating independent samples with HMC under increasing dimension $n$ grows as $O(n^{5/4})$ \citep{nealMCMCUsingHamiltonian2012}, whereas it grows as $O(n^2)$ for standard Metropolis-Hastings \citep{creutzGlobalMonteCarlo1988}

An alternative and recently popularized approach using also information from the gradient is Stein Variational Gradient Descent \cite[SVGD,]{liuSteinVariationalGradient2019} a variational inference algorithm, which aims at approximate inference by minimizing the Kullback-Leibler divergence between the proposed and target distributions.

Although the theoretical formalism and the infrastructure to perform intensive computations are there, a common framework to address different geophysical inverse problems has not emerged yet. Implementations of the HMC algorithm are typically application-specific and often not easily accessible to non-specialists. In addition, as these methods are nascent in the field of solid Earth geophysics, the community as a whole has not had time to acquire substantial expertise in the usage of these methods in order to evaluate their potential and routinely apply them to realistic problems.  
This work aims at facilitating at least part of the generation of this expertise, specifically in applying gradient-based sampling methods to inverse problems and analyzing their results. In this work we show how HMC can be used to obtain useful information from a set of diverse geophysical data sets through some illustrative selected examples from seismology and potential fields problems. All problems are addressed within the same framework, where generic samplers and data structures allows us to easily experiment with different data, priors and possibly to combine them.

These results are obtained with  ``HMCLab'', a tool to solve research problems and a numerical laboratory for experimenting with inverse algorithms such as HMC for a variety of geophysical topics. HMCLab provides software for a set of geophysical problems, for which functions to solve the forward problem, compute gradients of the misfit function and  several kinds of priors and samplers for the HMC method are provided. This package is currently written partly in Python~\citep{vanrossumPythonTutorial1995} and partly in Julia~\citep{bezansonJuliaFreshApproach2017}, depending on the specific problem. It is, however, in constant evolution as new geophysical problems are added or translated into the other one of the two languages. Moreover, users can supply their own forward model functions and priors which can easily be used with the HMC samplers. In addition, several Jupyter Notebooks are provided that guide the user through the various aspects of applying MCMC algorithms and analyzing their results, for various inverse problems. \\
In the following, we first give a brief overview of the core of HMC algorithms and illustrate what kind of information can obtained by solving some selected example problems using HMCLab.


\section{Theoretical background}

\subsection{The original HMC sampler}
\label{sec:hmcalgo}

HMC constructs a Markov chain over an $n$-dimensional probability density function $\sigma(\mathbf{m})$ using classical Hamiltonian mechanics. The algorithm regards the current state $\mathbf{m}$ of the Markov chain as the location of a physical particle in an $n$-dimensional space $\mathcal{M}$ (i.e., model or parameter space). It moves under the influence of a potential energy, $U$, which is defined as
\begin{linenomath*}
    \begin{align}
        U(\mathbf{m})=-\ln{ \left( \sigma(\mathbf{m}) \right) }.
    \end{align}
\end{linenomath*}
To complete the physical system, the state of the Markov chain needs to be artificially augmented with momentum variables $\mathbf{p}$ and a generalized mass for every dimension pair. The collection of resulting masses is contained in a symmetric positive definite mass matrix $\mathbf{M}$ of dimension $n \times n$. The momenta and the mass matrix define the kinetic energy of the particle as
\begin{linenomath*}
\begin{align}
    K(\mathbf{p})=\frac{1}{2} \mathbf{p}^T \mathbf{M}^{-1} \mathbf{p}.
\end{align}
\end{linenomath*}
In the HMC algorithm, the momenta $\mathbf{p}$ are drawn randomly from a multivariate Gaussian with covariance matrix $\mathbf{M}$ (the mass matrix). The sum of  the location-dependent potential and momentum-dependent kinetic energy constitute the total energy, or Hamiltonian, of the system
\begin{linenomath*}
\begin{align}
    H(\mathbf{m},\mathbf{p})=U(\mathbf{m})+K(\mathbf{p}).
\end{align}
\end{linenomath*}
The Hamiltonian dynamics are governed by the following equations,
\begin{linenomath*}
\begin{align}
    \frac{\partial\mathbf{m}}{\partial\tau} = \frac{\partial H}{\partial\mathbf{p}},\quad \frac{\partial\mathbf{p}}{\partial\tau} = - \frac{\partial H}{\partial\mathbf{m}} \, ,
\end{align}
\end{linenomath*}
which determine the position and momentum of the particle as a function of time $\tau$. This time $\tau$ is artificial just like the mass matrix, it has no connection to the actual physics of the inverse problem at hand.

We can simplify Hamilton's equations using the fact that kinetic and potential energy depend only on momentum and location, respectively, to obtain
\begin{linenomath*}
\begin{align}
    \frac{\partial\mathbf{m}}{\partial\tau} = \mathbf{M}^{-1} \mathbf{p}, \quad \frac{\partial\mathbf{p}}{\partial\tau} = - \frac{\partial U}{\partial\mathbf{m}} \, .
\end{align}
\end{linenomath*}
Evolving $\mathbf{m}$ over time $\tau$ generates another possible state of the system with new position $\mathbf{\tilde{m}}$, momentum $\mathbf{\tilde{p}}$, potential energy $\tilde{U}$, and kinetic energy $\tilde{K}$. Due to the conservation of energy, the Hamiltonian is equal in both states, i.e., $U+K = \tilde{U} + \tilde{K}$. Successively drawing random momenta and evolving the system generates a distribution of the possible states of the system. Thereby, HMC samples the joint momentum and model space, referred to as phase space. As we are not interested in the momentum component of phase space, we marginalize over the momenta by simply dropping them. This results in samples drawn from $\sigma(\mathbf{m})$.

If one could solve Hamilton's equations exactly, every proposed state (after burn-in) would be a valid sample of $\sigma(\mathbf{m})$. Since Hamilton's equations for non-linear forward models cannot be solved analytically, the system must be integrated numerically. Suitable integrators are symplectic, meaning that time reversibility, phase space partitioning and volume preservation are satisfied \citep{nealMCMCUsingHamiltonian2012,fichtnerHamiltonianMonteCarlo2019}. In this work, we employ the leapfrog method as described in \citet{nealMCMCUsingHamiltonian2012}, with higher order symplectic integrators also implemented. However, the Hamiltonian is generally not preserved exactly when explicit time-stepping schemes are used \cite[e.g.,][]{simoExactEnergymomentumConserving1992c}. Therefore, the time evolution generates samples not exactly proportional to the original distribution. A Metropolis-Hastings correction step is therefore applied at the end of numerical integration.

In summary, at each iteration, samples are generated starting from a randomly drawn model $\mathbf{m}$ in the following way:
\begin{enumerate}
   \item Propose momenta $\mathbf{p}$ according to the Gaussian with mean $\mathbf{0}$ and covariance matrix $\mathbf{M}$;
   \item Compute the Hamiltonian $H$ of model $\mathbf{m}$ with momenta $\mathbf{p}$;
   \item Propagate $\mathbf{m}$ and $\mathbf{p}$ for some time $\tau$ to $\tilde{\mathbf{m}}$ and $\tilde{\mathbf{p}}$, using the discretized version of Hamilton's equations and a suitable numerical integrator;
   \item Compute the Hamiltonian $\tilde{H}$ of model $\tilde{\mathbf{m}}$ with momenta $\mathbf{\tilde{p}}$;
   \item Accept the proposed move $\mathbf{m} \rightarrow \tilde{\mathbf{m}}$ with probability
      \begin{eqnarray}
          p_\text{accept} = \min \left( 1, \exp ( H-\tilde{H} ) \right)\,.
      \end{eqnarray}
   \item If accepted, use (and count) $\tilde{\mathbf{m}}$ as the new state. Otherwise, keep (and count) the previous state. Then return to 1.
\end{enumerate}

The mass matrix $\mathbf{M}$ is one of the important tuning parameters of the HMC algorithm; details on its meaning and suggestions for tuning can be found in \citet{fichtnerHamiltonianMonteCarlo2019,fichtnerAutotuningHamiltonianMonte2021}. Moreover, employing the discrete leapfrog integrator implies that there are two additional parameters that need to be tuned, namely the time step $\epsilon$
and the number of iterations $L$ \citep{nealMCMCUsingHamiltonian2012}.


\subsection{HMC variants}

The algorithm described so far is the simplest version of HMC, however, several variants of the original algorithm exist, which mostly aim at automatically tuning some of the parameters or improving mixing \citep{nealMCMCUsingHamiltonian2012,sambridgeParallelTemperingAlgorithm2014,fichtnerAutotuningHamiltonianMonte2021}.

A notable example is the No U-Turn Sampler (NUTS) \citep{hoffmanNoUturnSamplerAdaptively2014}, which aims at providing an automatic tuning of the two leapfrog integrator-related parameters, $\epsilon$ and $L$. NUTS finds a suitable value for $\epsilon$ during the burn-in and then fixes it for the following iterations to avoid breaking the detailed balance property. The number of iterations $L$ instead is dynamically adjusted (``dynamic HMC'') at each iteration in a way such that there is no doubling back of the trajectory. This allows for long or short moves depending on the region of the model space which the algorithm is visiting. NUTS is implemented in HMCLab following \citet{hoffmanNoUturnSamplerAdaptively2014}.

Additionally, methods to investigate inverse problems that might show strongly isolated modes exist. By running multiple chains with tempered (i.e. smoothed) posterior PDFs and letting these samplers exchange states, the exploration of local minima might be accelerated \citep{sambridgeParallelTemperingAlgorithm2014}. Tempered trajectories following \citet{nealMCMCUsingHamiltonian2012} may also help discovering isolated modes. This variation does not require multiple Markov chains, nonetheless, it is able to more easily transition between local minima, at the expense of a reduced acceptance rate.


\subsection{Gradient computations}

As mentioned above, one important aspect of a successful HMC strategy is the capability to efficiently compute the gradient of the potential energy $\nabla U(\mathbf{m}) = \nabla \! \left( -\log(\sigma(\mathbf{m})) \right)$. The first method that proves powerful for relatively simple models is to evaluate derivatives analytically. This typically allows for cheap computation of the gradients, but is only applicable to models that can be analytically differentiated. This is most notably used for the joint non-linear source location and medium velocity estimation as mentioned later in this manuscript. For larger problems, a tool that can provide a substantial help in making gradient calculations efficient is the adjoint method \cite[e.g.,][]{lionsOptimalControlSystems1971,tarantolaInversionSeismicReflection1984,talagrandVariationalAssimilationMeteorological1987,trompSeismicTomographyAdjoint2005a,fichtnerAdjointMethodSeismology2006a,plessixReviewAdjointstateMethod2006,hinzeOptimizationPDEConstraints2008}. This strategy allows us to compute the gradient $\nabla U$ with a computational cost of about two (three in practice) forward simulations, much cheaper than other approaches such as finite difference methods. We employ the adjoint technique in some of our geophysical problems, namely in the case of acoustic and elastic full waveform inversion and for the nonlinear traveltime problem (eikonal solver). 

Another useful tool to compute the gradient for certain problems is automatic  differentiation \cite[e.g.,][]{sambridgeAutomaticDifferentiationGeophysical2007,griewankEvaluatingDerivativesPrinciples2008}, a computational technique where derivatives of a user-coded function are provided automatically by the software in the form of a function. This technique can be convenient for problems where it is difficult to derive the adjoint equations (e.g., when the forward operator is not self-adjoint) or where the forward model needs to be adapted for each specific case because it depends, e.g., on the specific rock types present in the area under study, requiring a re-derivation of the analytical derivatives (such as rock physics models \citep{mavkoRockPhysicsHandbook2003a}). We use this tool, e.g., for the problem of inversion of amplitude-versus-angle (AVA) seismic reflection data, where the forward modelling is a combination of a rock physics model \cite[e.g.,][]{mavkoRockPhysicsHandbook2003a} and a convolutional seismic model. 

\subsection{Prior information}

Prior information plays an important role in solving inverse problems by providing additional information directly on the model parameters to better constrain plausible values for the solution and helping to mitigate the non-uniqueness \cite[e.g.,][]{curtisPriorInformationSampling2001,scalesPriorInformationUncertainty2001,hansenInverseProblemsNontrivial2012a,zuninoMonteCarloReservoir2015,hansenProbabilisticIntegrationGeoInformation2016a}. In the probabilistic approach, prior information is represented by a PDF $\rho(\mathbf{m})$ on the model parameters.

HMCLab provides a set of common PDFs for the prior, ranging from simple multivariate Gaussian distributions to more complex distributions such as a combination of Beta PDF-based marginals with a Gaussian copula to correlate the marginals. Another interesting prior is based on the Laplace distribution (related to the L1-norm) which promotes sparse (or blocky) models. Moreover, the user can provide his/her own prior by simply implementing functions with the appropriate signature (see the code documentation for more details). Any of the available priors can be combined with any of the available or user-generated forward models.


\section{Inferring complex information about the subsurface with HMCLab}

The HMCLab framework allows us to solve diverse inverse problems using sampling methods under a common platform. The software package includes a set of pre-defined geophysical forward and inverse problems, a set of prior distributions and allows the user to supply his/her own forward problem. In the following we show some examples of how to extract useful information about the subsurface for a set of selected geophysical inverse problems in the framework of the HMC method.

Once a collection of samples from the posterior distribution has been obtained, in order to calculate some arbitrary function $\phi(\mathbf{m})$ of $\mathbf{m}$, we can use the following relationship:
\begin{linenomath*}
  \begin{equation}
  \label{eq:mcmcpostanalysis}
  \int_{\rm M} \phi(\mathbf{m}) \sigma(\mathbf{m}) \, \mathrm{d} \mathbf{m} \approx \dfrac{1}{N} \sum_{i=1}^N \phi(\mathbf{m}_{i})
\end{equation}
\end{linenomath*}
where $N$ is the number of available samples and $\mathbf{m}_{i}$ represents one of the posterior models.

\subsection{2D full waveform acoustic inversion of reflection data}

\begin{figure}
  %\begin{center}
  \includegraphics[width=\textwidth]{acoustic_nuts.png}
  \caption{Acoustic waves inversion for velocity. a) Target  model of velocity, i.e., the one used to calculate the synthetic data. b) The starting velocity model used in the inversion. c) A randomly selected model from the collection of posterior models. d) A plot of the potential energy (misfit) as a function of iteration number, where the red dot indicates the potential energy of the model shown in panel d). e) A map of the probability of having a layer boundary of a fault computed using the collection of posterior models. f) A vertical profile of velocity showing the probability as computed from the collection of posterior models. The profile location is shown by a vertical line in panel c).}
  \label{fig:acousticprob}
  %\end{center}
\end{figure}

The first example is a 2D inversion of a seismic dataset based on the acoustic approximation. The forward problem is represented by the constant-density acoustic wave equation:
\begin{linenomath*}
  \begin{equation}
  \label{eq:acouwaveq}
  \dfrac{1}{v^2} \dfrac{\partial^2 u}{\partial t^2} = \dfrac{\partial^2 u}{\partial x^2} + \dfrac{\partial^2 u}{\partial z^2} + s
  \end{equation}
\end{linenomath*}
where $t$ is time, $x$ and $z$ the spatial coordinates, $u$ is the pressure field, $v$ the acoustic velocity and $s$ the source term. Forward calculations are carried out using a finite-difference scheme \citep{bunksMultiscaleSeismicWaveform1995,pasalicConvolutionalPerfectlyMatched2010}, where the model parameters are velocity at a set of grid points with size $(N_x \times N_z) = (160 \times 90)$ for the $x$- and $z$-direction respectively, for a total of $14400 $ model parameters. The grid spacing is 10 m in both directions. We assume the observational errors to be Gaussian distributed. Therefore, we use an $L_2$-norm potential energy function. The gradient of such a misfit function with respect to velocity is computed by means of the adjoint method for the acoustic wave equation, as described in \citet{bunksMultiscaleSeismicWaveform1995}. The use of the adjoint method enables us to efficiently evaluate the gradient, an essential prerequisite for being able to perform an HMC inversion. The geometry of the problem resembles the one typically found in exploration seismology, where active sources and receivers are located near the surface of the Earth, as shown in Fig.~\ref{fig:acousticprob}. The top boundary condition is a free surface, while the other sides are absorbing boundaries implemented as C-PML layers \citep{komatitschUnsplitConvolutionalPerfectly2007}. We use a set of 6 sources to generate synthetic data, add correlated Gaussian noise (standard deviation $0.05$, correlation length $0.01$ s) and use the result as the observed data to be inverted for the velocity model. To perform the inversion we use the NUTS algorithm \citep{hoffmanNoUturnSamplerAdaptively2014}, part of HMCLab. 

We ran $2 \times 10^5$ iterations of the NUTS algorithm, collecting about 45000 samples after thinning the chain and removing the models resulting from the burn-in phase. The starting velocity model is laterally homogeneous  (see Fig.~\ref{fig:acousticprob}b).The target model is a modified version of the SEG/EAGE overthrust model \citep{aminzadehSEGEAGE3D1997}. 
Fig.~\ref{fig:acousticprob}c shows a randomly chosen model from the collection of the posterior models. The model resembles the target model well, and all the different layers are visible.
The potential energy decreases rapidly within the first few hundreds of iterations, when the algorithm attempts to find a model which fits the large-scale structures (see Fig.~\ref{fig:acousticprob}d). Subsequently, the misfit keeps decreasing relatively slowly for much longer. We suspect this is due to the algorithm slowly adjusting the fine-scale structures, until it reaches a relatively stable misfit value. From the resulting collection of posterior models we can extract different pieces of information. One practical example is, e.g., calculating the probability of having a layer boundary or fault at any given node of the grid. To do so, we exploit eq.~\ref{eq:mcmcpostanalysis} using an indicator function $h(\mathbf{m})$ and compute  
\begin{linenomath*}
  \begin{equation}
  \label{eq:mcmcindicfun}
  \int_{\rm M} h(\mathbf{m}) \sigma(\mathbf{m}) \, \mathrm{d} \mathbf{m} \approx \dfrac{1}{N} \sum_{i=1}^N h(\mathbf{m}_{i})
\end{equation}
\end{linenomath*}
which produces a value of one in case a boundary/edge is detected and zero if not. The function $h(\mathbf{m})$ in this case is represented by a Canny edge detection filter \citep{cannyComputationalApproachEdge1986}, which is applied to each velocity model in the posterior collection, so that the model is transformed into a binary image of zeros and ones. Fig.~\ref{fig:acousticprob}e shows the results of such calculations. The large majority of the boundaries present in the target model appear as high probability structures in Fig.~\ref{fig:acousticprob}e, particularly at shallow depths. Finally, Fig.~\ref{fig:acousticprob}f shows a map of probability for a vertical profile of velocity at $x=1030.0$ m, showing the spread of the solutions. The profiles of the starting and target model are shown for comparison.


\subsection{First arrival traveltime hypocenter location using fiber-optic sensing}
An archetypal seismological inverse problem is the estimation of earthquake hypocenters in a medium with unknown structure and velocity using the first arrival times of the seicmic waves excited by the events. HMCLab supplies a simple approach to the hypocenter location problem that uses first arrival data, assuming a homogeneous medium. Arrival times are modelled by straight rays propagating through this homogeneous medium. The inverse problem only requires relative arrival times of a single phase.

Although the physics of this model seem simple, the strong trade-offs between location, origin time and medium velocity make it ideal for Bayesian inference methods. Especially in the presence of trade-offs, one would expect strongly correlated posteriors, for which the HMC algorithm works particularly well. 

To illustrate how HMC performs on this inverse problem, we applied the algorithm to a dataset acquired on  Grimsv√∂tn volcano in Iceland using a 12.5 km long Distributed Acoustic Sensing (DAS) fiber \citep{fichtnerFiberOpticObservation2022,klaasenSensingIcelandMost2022}, for which the acquisition geometry is shown in Fig.~\ref{fig:triangulation}.
Due to the use of DAS, this dataset has approximately 1500 separate channels.
We simultaneously infer the location of multiple events for which the effective medium velocity is assumed to be the same. The events are selected based on similarity in the observed move-out, as we expect these events to be relatively close.
By simulatenously inferring the location of multiple events the data better constrains the medium velocity, which reduces the trade-off between origin time and medium velocity compared to inferring location, origin time and medium velocity for a single event.
We define an $L_2$ misfit on the relative first arrivals of the picked phases, and only include those channels where the phase is picked. This means that some events have relatively less data points and therefore less importance within the inference. As prior, we use uniform distributions on location in a bounded cube of 20 km by 20 km by 10 km (width by length by depth) centered around the DAS cable. As the medium below the field site is unkown, we construct a prior on the P-wave velocity with a logarithmic uniform distribution (to take into account the fact that velocity is a positive parameter) between 340 and 7000 m/s, the extreme ends of possible medium material, i.e. air to relatively fast rock.

\begin{figure}
  \includegraphics[width=\textwidth]{triangulation.png}
  \caption{Source hypocenter location for data recorded on the Grimsv\"otn volcano. a) Overview of Iceland and location of the inset b). b) Geometry of the DAS acquisition fiber with posterior means and standard deviation ellipses oriented along principal axes. c) Marginal distribution of medium velocity prior and posterior to the inference. Note that the prior on medium velocity extends beyond the range of the plot. d)-f) Marginal distributions for the Y and Z components of the first 3 events. Note how for event 1 and 2, the volumes of uncertainty are more concentrated than for event 3.}
  \label{fig:triangulation}
\end{figure}

The results of a parallel tempered appraisal with 10 chains using HMC are given in Fig.~\ref{fig:triangulation}. The posterior on medium velocity, seen in Fig.~\ref{fig:triangulation}c, shows how, despite having a model with strong trade-offs in the parameters (namely origin time and medium P-wave velocity), one is still able to infer knowledge on one of these parameters, adding knowledge compared to the prior. Event 3, which was recorded on relatively few channels of the fiber-optic cable, features a high uncertainty of its location in the subsurface. This is in contrast to event 1 and 2, which seem to have a more concentrated volume of uncertainty. These results show that the posterior PDF of the location of these events is neither unimodal nor Gaussian, something which would have been difficult to deal with using deterministic methods. In general, one can see in Fig.~\ref{fig:triangulation}b that events with fewer picks are constrained less. Examples of relevant indicator functions~\citep{arnoldInterrogationTheory2018} for this inference would be the expected average depth of an event, or the probability of the medium velocity lying within a limited interval.

\subsection{First arrival tomography based on the eikonal equation}

\begin{figure}
  %\begin{center}
  \includegraphics[width=\textwidth]{eik2D_fig_paper.png}
  \caption{Nonlinear traveltime inversion. a) Target model used to generate the ``observed'' data, b) starting model, c) a selected model from the posterior collection, d) histogram of velocity at $(x,y) = (92,5,26.25)$ (marked by a red dot in c)), e) potential energy, f) profile of velocity showing a probability map as computed from the posterior collection of models (the profile location is shown by a vertical line in panel c)).}
  \label{fig:eikprob}
  %\end{center}
\end{figure}
Traveltime tomography is a popular approach for seismic inversion where the arrival time of seismic waves at given locations is used to infer the velocity structure of the subsurface. In this case, the forward model is represented by the eikonal equation:
\begin{linenomath*}
\begin{equation}
  \label{eq:eikforw}
 \sum_d \left( \dfrac{\partial \, t}{\partial x_d} \right)^2 = c^{-2}(x) \, ,
\end{equation}
\end{linenomath*}
where $t$ is the traveltime, $c$ the velocity and $d=2 \, \mathrm{ or } \, 3$ is the number of dimensions.
Since the forward model is nonlinear, typically ray paths are computed a priori in a reference Earth model and fixed, to linearize the problem and hence solve the inverse problem with gradient-based deterministic methods.
However, such strategy where a single solution is sought might miss some important information. Because of the nonlinearity of the problem, the misfit functional may feature multiple minima which cannot be detected with deterministic methods. On the contrary, a probabilistic approach may reveal different plausible velocity models to be consistent with the observed data, as we show in the following example.

The example we address here is to solve a 2D inverse problem with a geometry depicted in Fig.~\ref{fig:eikprob}, where we have a velocity model described by 4800 cells (80 in the $x$-direction and 60 in the $y$-direction), a set of sources randomly distributed close to the bottom of the model and a set of receivers near the surface and along the left and right sides of the model. The grid spacing is 1.25 km in both directions.
The forward problem is solved using a fast marching method (FMM) \cite[e.g.,][]{sethianFastMarchingLevel1996,rawlinsonWaveFrontEvolution2004,treisterFastMarchingAlgorithm2016} which computes the traveltimes at each point of a grid using a finite-difference strategy. The gradient of the Gaussian misfit functional with respect to velocity is computed by means of the adjoint method \cite[e.g.,][]{leungAdjointStateMethod2006,taillandierFirstarrivalTraveltimeTomography2009,zuninoIntegratingGradientInformation2018}.
To solve the inverse problem, we ran $2 \times 10^5$ iterations with the NUTS algorithm. The target model is shown in Fig.~\ref{fig:eikprob}a, while the starting model is depicted in Fig.~\ref{fig:eikprob}b. The latter is a laterally homogeneous velocity model. Panels c and d of Fig.~\ref{fig:eikprob} show a randomly selected model from the posterior collection and a histogram of the velocity at a given location as obtained by looking at all samples after the burn-in period. Interestingly, the histogram shows a multi-modal distribution, where probable velocity values cluster near three different values. This means that there are three different ranges of velocity values which are highly probable, i.e., they are all compatible with the observed data. Such finding would not be possible with an optimization method where the solution is represented by a single velocity model.
The potential energy (misfit) as a function of iterations (Fig.~\ref{fig:eikprob}) features a sharp decrease in the first hundreds of iterations and then a slow descent until equilibrium is reached around $5 \times 10^5$ iterations. Fig.~\ref{fig:eikprob}f shows a map of probability for a vertical profile at $x=27.5$ km, together with the profile for the starting and target model. 


\subsection{Magnetic anomaly inversion with polygonal bodies}
\begin{figure}
  %\begin{center}
  \includegraphics[width=\textwidth]{mag2d_hmclab.png}
  \caption{Magnetic anomaly problem. a) Three panels showing (from top to bottom): 1) the observed magnetic anomaly, the one calculated from the starting and selected models, 2) the starting, target and selected polygonal bodies including topography and position of measurements and 3) a scatter plot of the position of the vertices before and after the burn-in phase (every 10 iterations). b) A plot of the potential energy (log. scale) as a function of iteration number.}
  \label{fig:magprob}
  %\end{center}
\end{figure}
The last synthetic example presented is an inversion of magnetic anomaly data using a 2.75D parameterization in terms of polygonal bodies \cite[e.g.,][]{rasmussenEndCorrectionsPotential1979a,campbellBASICProgramsCalculate1983}. The design and detailed description of the method to construct and solve this inverse problem is the subject of another paper \citep{zuninoHamiltonianMonteCarlo2022}.
In this setup, each polygonal body has a homogeneous magnetization and its shape is controlled by the position of its vertices, which, in this example, are the unknowns of the inverse problem. In this setup, the relation between the position of vertices (model parameters) and the magnetic response is nonlinear. The label 2.75D means that the polygonal bodies have a given finite lateral extent in the $y$ direction that can be different in the $+y$ and $-y$ directions. 
The observed data are represented by a profile along the $x$ direction with about 130 observation points (see Fig.~\ref{fig:magprob}a). To solve the inverse problem, we ran $5 \times 10^4$ iterations using the NUTS algorithm. The potential energy decreases rapidly in the first few hundreds of iterations (see Fig.~\ref{fig:magprob}b) when the polygonal bodies move from the starting position to a more likely configuration, similar to the target model. Afterwards, the algorithm samples the posterior distribution, producing a set of posterior models. In this example the mass matrix contains non-zero off-diagonal elements in order to force the algorithm to avoid creating geologically implausible shapes. Such off-diagonal elements control the correlation of the momentum variables and hence indirectly the correlation shown in the models visited by the algorithm. 
A randomly selected model from the posterior collection is shown in  Fig.~\ref{fig:magprob}a, which fits the observed data well and is also close to the shape of the target model. In the last panel of Fig.~\ref{fig:magprob}a a set of position of the vertices before and after the burn-in phase are depicted, showing the relatively low uncertainty in the positions for the anomaly.


\section{HMCLab framework as a software package}

\begin{table} 
  \centering
  \begin{tabular}{l|c|c}
    \specialrule{0.1em}{0.25em}{0.25em}
    Inverse problem & Julia & Python \\ 
    \specialrule{0.1em}{0.25em}{0.25em}
    Traveltime tomography (eikonal eq., nonlinear) &  2D \& 3D (parall.) &  \\
    \specialrule{0.025em}{0.25em}{0.25em}
    Traveltime tomography (refracted rays, nonlinear) &   & 2D (parall.) \\\specialrule{0.025em}{0.25em}{0.25em}
    Traveltime tomography (straight rays, linear) &   & 2D (parall.) \\
    \specialrule{0.025em}{0.25em}{0.25em}
    Full-waveform inversion & 2D acoustic (parall.) & 2D elastic (parall.) \\
    \specialrule{0.025em}{0.25em}{0.25em}
    Earthquake source location & & 2D \& 3D \\
    \specialrule{0.025em}{0.25em}{0.25em}
    AVA seismic reflection data + rock physics & 1D, 2D, 3D (parall.) & \\
    \specialrule{0.025em}{0.25em}{0.25em}
    Magnetic anomalies (polygons) & 2D, 2.75D & \\
    \specialrule{0.025em}{0.25em}{0.25em}
    Gravity anomalies (polygons) & 2D,  2.75D & \\
    \specialrule{0.025em}{0.25em}{0.25em}
    Joint gravity and magnetic anomalies (polygons) & 2D, 2.75D & \\
    \specialrule{0.025em}{0.25em}{0.25em}
    Arbitrary full and sparse matrix equations& & \checkmark \\
    \specialrule{0.025em}{0.25em}{0.25em}
    User-provided forwards & \checkmark & \checkmark\\
                    & & \\
    \specialrule{0.1em}{0.25em}{0.25em}
    Prior distribution & Julia & Python \\ 
    \specialrule{0.1em}{0.25em}{0.25em}
    Gaussian (L2) & \checkmark & \checkmark \\
    \specialrule{0.025em}{0.25em}{0.25em}
    Laplace (L1) &  & \checkmark \\
    \specialrule{0.025em}{0.25em}{0.25em}
    Uniform & \checkmark & \checkmark \\
    \specialrule{0.025em}{0.25em}{0.25em}
    Beta marginals + Gaussian copula & \checkmark (parall.) &  \\
    \specialrule{0.025em}{0.25em}{0.25em}                                  
    Arbitrary mixture &  & \checkmark \\
    \specialrule{0.025em}{0.25em}{0.25em}
    User defined marginals & \checkmark & \checkmark \\
    \specialrule{0.025em}{0.25em}{0.25em}                                  
  \end{tabular}
  \vspace{0.2cm}
  \caption{Snapshot of currently available inverse problems and priors. HMCLab is constantly evolving, therefore changes are expected. 1-2-3D refers to the physical dimensions of the problem, "parall." means a parallelized (multi-core) implementation is available.}
  \label{tab:inverse_problems}
\end{table}
The HMCLab framework is practically implemented in a set of open-source software packages written in the Python \citep{vanrossumPythonTutorial1995} and Julia \citep{bezansonJuliaFreshApproach2017} programming languages. HMCLab is, in fact, a numerical laboratory to allow the user to experiment with sampling algorithms on different geophysical problems, ranging from purely educational examples to research-oriented studies. The aim is to provide a user-friendly framework where it is possible to experiment with various problems and algorithms and solve realistic inverse problems, either provided by HMCLab or created by the user.

Table~\ref{tab:inverse_problems} summarizes the geophysical problems and prior models which are currently available in HMCLab. Both Julia and Python implementations are modular in that, forward modelling, gradient calculations and sampler (or optimizer) can be combined arbitrarily. Moreover, in addition to the listed problems, the sampler (or optimizer) can be used on user-defined problems. 
The main categories of inverse problems currently addressed by HMCLab are:
\begin{itemize}
\item linearized (straight rays) and nonlinear (eikonal equation) traveltime tomography,
\item full waveform inversion in 2D in the acoustic and elastic formulations (P-SV),
\item earthquake source location in 3D based on the straight-ray approximation,
\item joint (or independent) gravity and magnetic anomaly inversion in 2.75D using polygonal bodies,
\item amplitude versus angle (AVA) seismic data including a rock physics model in 3D.
\end{itemize}
In addition, a set of priors is provided, which can be combined with any problem. For all the above mentioned geophysical problems, HMCLab provides functions to solve forward problems and to compute the gradient of misfit functions with respect to model parameters. For all these physics, samplers are available to appraise the inverse problem. However, the functions of these physics can be used independently of the sampler, hence the user can construct his/her own inversion scheme. Also, the user may supply his/her own forward model, prior and gradient calculation code and subsequently use the available inversion algorithms by providing a minimum set of functions with the appropriate signature as described in the documentation.

HMCLab is not limited to flavors of the HMC algorithm, but includes other more traditional algorithms such as the random walk Metropolis-Hastings algorithm \citep{metropolisEquationStateCalculations1953a, hastingsMonteCarloSampling1970}, where gradients are not used. Moreover, we provide an interface \citep{gebraadSimpleSVGDTinyInterface2022} to the Stein Variational Gradient Descent (SVGD, \cite{liuSteinVariationalGradient2019}) variational inference algorithm, providing an alternative probabilistic appraisal algorithm to the included MCMC algorithms.
As already mentioned, HMCLab includes functions to compute gradients of the misfit functional, and, as such, deterministic inversions are also possible \cite[e.g.,][]{zuninoEfficientMethodSolve2019}. An example is basic gradient descent, where the modes (local minima) of the defined posterior distribution can be found deterministically. As such, HMCLab also facilitates the usage of Python \cite[e.g.,][] {virtanenSciPyFundamentalAlgorithms2020} and Julia optimization libraries, including popular algorithms such as the (Limited Memory) Broyden-Fletcher-Goldfarb-Shanno and Newton Conjugate Gradient algorithms \citep{nocedalNumericalOptimization2006}.

Noteworthy is the inclusion of several notebooks that illustrate the basic concepts of MCMC sampling in general, applied to HMCLab in particular. This ranges from investigating the basic properties of a Markov chain, such as number of proposals, stepsizes and resulting acceptance rates, to tuning the various included algorithms and even implementing one's own inverse problems. These notebooks are available to all users to run out-of-the-box in our supplied Docker environment, but are also available without a Python or Julia interpreter as plain HTML. HMCLab homepage can be found at \url{https://hmclab.science}, while the Julia and Python versions at \url{https://gitlab.com/JuliaGeoph} and \url{https://python.hmclab.science/index.html}, respectively.
Finally, HMCLab is constantly updated and expanded and contributions from the community are welcomed.

\section{Conclusions}
In this work we have shown how a common framework for probabilistic inversion can be utilized to solve a diverse range of geophysical problems. In particular, the HMC method can be used to solve a variety of nonlinear geophysical inverse problems. In contrast to other MCMC algorithms, HMC exploits the information derived from the gradient of the posterior pdf to drive the sampling towards regions of high probability, hence being able to traverse the model space more efficiently. This property is very beneficial, especially for problems which allow efficient computation of the gradients, e.g., when using the adjoint method, analytical derivatives or automatic differentiation.
In this paper we have shown a set of example problems including full-waveform acoustic inversion, hypocenter location, traveltime tomography and magnetic anomaly inversion using polygonal bodies. The examples presented have been solved using the software implementation HMCLab, a numerical laboratory for better understanding and solving inverse problems in a probabilistic manner, written in the high-level languages Julia and Python. By providing a collection of models as the solution of the inverse problem, HMCLab enables the user to perform a statistical analysis and retrieve desired probabilistic information. HMCLab is in constant evolution, and hopefully will be augmented by contributions from interested users. In addition to the available forward problems and priors, we made it accessible to the user to construct their own inverse problem and easily apply the methods provided by HMCLab. Moreover, several types of prior information are available, allowing the user to adequately describe prior certainties and uncertainties.


\section*{Acknowledgments}
($\dagger$) The first two authors contributed equally to this work.

We wish to thank Klaus Mosegaard, who, over the years, has contributed much to the authors' understanding of Markov chain Monte Carlo methods and their use in Geosciences. We also would like to thank Sara Klaasen for sharing the Grimsv√∂tn DAS dataset. L.\ G.\ would like to thank Xin Zhang and Andrew Curtis for fruitful discussions on Bayesian inference and Stein Variational Gradient Descent. Furthermore, thanks go out to anyone who dared to test the software in the experimental state before this publication to help us improve and debug HMCLab; Cyrill, Marc, Runa, Sara, Ariane, and to anyone who will contribute in the future.
This work has been supported by the Swiss National Science Foundation (``Hamiltonian Monte Carlo Full-Waveform
Inversion'', grant number 200021\_192236).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{apacite}
\bibliography{HMCLab_bibliography}

\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
