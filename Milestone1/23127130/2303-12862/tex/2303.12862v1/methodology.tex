\section{Methodology}
\label{methodology}

\vspace{-2mm}
\subsection{Shadow Removal}
\noindent  
Our aim is to mimic the high-fidelity results of two-network setups (removal and refinement) \cite{le2020physicsbased}, but by using a single, efficient network that implicitly localizes and removes shadows. We base our architecture on \cite{yucel2021real} due to its strong accuracy/runtime performance and propose to use lightweight attention modules \cite{hou2021coordinate,yucel2023lra} over the input and output (IOA) of the network, and sum their results via a long residual connection from input to output. IOA has the advantage of being parallelizable (i.e. input attention is executed concurrently with the  network), makes the network focus on the shadow areas only (i.e. non-shadow areas are copied by the long residual connection) and introduces additional capacity for blending/color-correction with minimal computational overhead. We call the resulting architecture IOANet; its architecture and training details are shown in Figure \ref{fig:networks}.



\begin{figure*}[htbp]
  \centering
      \includegraphics[width=\textwidth]{figures/fig2_g2_att_cropped.pdf}
  \vspace{-8mm}
  \caption{Our IOANet shadow removal network. Using all three datasets we propose, we train our network on low-resolution images using a combination of L1 and LPIPS losses. This training stage is followed by stage 2 (see Figure \ref{fig:overall_diagram}).}
  \label{fig:networks}
  \vspace{-4mm}
\end{figure*}

\vspace{-2mm}
\subsection{Efficient Upsampling}

\noindent \textbf{Preliminaries.} The visual quality of the output is important in documents, since high frequency components (i.e. text) must be preserved after generation and upsampling. We set the target resolution for the shadow removal network to (192$\times$256), which is reminiscent of the aspect ratio of documents. Instead of naively running the network at high-resolution, we aim to efficiently upsample the shadow-removed image four times to high-resolution (768$\times$1024).


\noindent \textbf{The proposed method.} Laplacian Pyramid Networks \cite{liang2021high} decompose an image into a Laplacian pyramid \cite{burt1987laplacian}, where low-frequency components are fed to an image-to-image translation network in low resolution. High frequency components are adaptively refined via a mask learning network based on all frequency components. This mask is upsampled and finetuned for each resolution level, and all components are used to reconstruct the high resolution output. 



We use a 2-level pyramid where IOANet operates on low resolution (192$\times$256) images. Unlike the original work \cite{liang2021high}, we train IOANet with low-resolution images and then the rest of the network with high-resolution images (see Section \ref{sec:imp_details}). The residual refinement network operates on the intermediate resolution of (384$\times$512).  This network, in its original version, leads our pipeline to have 22.8 GFLOPs complexity. Unlike \cite{liang2021high}, we implement the residual refinement network with cheap, depthwise separable convolutions, resulting in 3.82 GFLOPs (called LPTN-lite). We further decrease the width of the network and achieve 1.47 GFLOPs; this is our LP-IOANet pipeline. LP-IOANet is shown in Figure \ref{fig:overall_diagram}.

\vspace{-4mm}
\subsection{Datasets}

\noindent The largest dataset in the literature (SDSRD \cite{lin2020bedsr}) is not publicly available, therefore we opt to create our own synthetic datasets. In addition to being able to train on a more diverse dataset, our new datasets allow us to evaluate on a larger distribution, giving us a better insight on models' performance.

\noindent \textbf{BSDD.} We follow the principles of \cite{lin2020bedsr} and create Blender Synthetic Dataset (BSDD) using 1328 unique images. BSDD has 3863 high resolution triplets, split into 3477 and 386 images for training and testing, respectively.

\noindent \textbf{Doc3DS+.} We leverage the Doc3DShade dataset \cite{DasDocIIW20}, which is formed of 90K image triplets of shadow images with colored backgrounds, white balanced shadow images and albedo document images. Training a model using colored shadowed images as input and albedo as output may result in neglecting the paper color, whereas using white balanced shadowed as input may restrict the model's view to white paper only. We extract the background color from the input image using color clustering and reapply it on the albedo document images to alleviate such issues. We rotate the resulting images to be closer to our desired A4 document resolution. 

\noindent \textbf{Augmenting the datasets.} A shadow area is not just a darkened version of the original image; natural shadows tend to have different colors and illuminations. Following \cite{le2020physicsbased, Le_2019_ICCV}, we apply illumination augmentation. Furthermore, we also modify the colour values of the shadows, which gives us a more diverse distribution. With this procedure, we augment the train split of BSDD and the entire OSR \cite{osrDataset} dataset and use the augmented versions \textit{A-BSDD} and \textit{A-OSR} in our experiments. Details of our datasets are shown in Table \ref{tab:datasetsOverview}.

\begin{table}[]
\tiny
\resizebox{0.485\textwidth}{!}{%
\begin{tabular}{l|c|c|c}
Dataset & Size & Unique Images & Resolution \\ \hline
A-BSDD & 24082 & 1328 & High \\
Doc3DS+ & 71595 & 9393 & Low \\
A-OSR & 1410 & 23 & Low
\end{tabular}%
}
\vspace{-4mm}
\caption{Overview of the datasets.}
\label{tab:datasetsOverview}
\vspace{-5mm}
\end{table}


