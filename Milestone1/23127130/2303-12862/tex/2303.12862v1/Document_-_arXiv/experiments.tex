\section{Experimental Results}
\label{experiments}
\vspace{-2mm}
\subsection{Implementation and Training Details} \label{sec:imp_details}
\noindent We adopt a two-stage training regime, where we first train the removal network IOANet (see Figure \ref{fig:networks}) in low-resolution and then train our upsampling framework LP-IOANet with the IOANet fixed (see Figure \ref{fig:overall_diagram}). We note that a one-stage, end-to-end training is possible, but since upsampling module requires high-resolution data, one-stage training can only be done on A-BSDD dataset. Two-stage training lets us train IOANet on low-resolution data and improves the final result.


In the first stage, we train IOANet for 1000 epochs using Adam \cite{kingma2015adam} with two losses; L1 and LPIPS \cite{zhang2018unreasonable}. Our loss weights are empirically chosen as 10 and 5 for L1 and LPIPS, and we use a mixed training strategy where we sample 15, 15 and 2 images in a batch from A-BSDD, Doc3DS+ and A-OSR, respectively. In the second stage, we freeze IOANet and train the upsampler using L1 loss. We train on A-BSDD for 200 extra epochs. Models are trained using PyTorch \cite{paszke2019pytorch}.

\vspace{-3mm}

\subsection{Evaluation Details and Results}
\noindent We use PSNR, SSIM and MAE metrics  on BSDD dataset for evaluation. We choose BSDD since we can train BEDSR on it (i.e. background colors are available) and because it is the only high-resolution dataset. We report metrics for all, non-shadow and shadow regions separately. We compare with the state-of-the-art BEDSR \cite{lin2020bedsr}; we reproduce the implementation and train it on A-BSDD.  We perform evaluation both in low-resolution ($192\times256$) and high-resolution ($768\times1024$).  

Our results are shown in Table \ref{tab:results}. We first compare IOANet, its components and BEDSR in low-resolution and train them only on BSDD. IOANet comfortably outperforms BEDSR on all metrics, despite running x10 faster, consuming x30 fewer memory and having x2K times fewer flops. It is also apparent that the input-output attention proposed in our architecture improves the results, with minimal to no overhead in runtime/memory performance. When trained on all three datasets (i.e. full stage 1 training), IOANet shows dramatic improvements, showing the value of our datasets. 

In high-resolution, we follow the full two-stage training and the results are even better; BEDSR fails to run in high-resolution and goes out-of-memory on a 24GB VRAM desktop GPU. Our LP-IOANet, on the other hand, achieves comparable results to our IOANet, despite evaluating at four times the resolution. Furthermore, it is still operating comfortably in real-time (around 84 FPS) while being still faster, smaller and less complex than low-resolution BEDSR. Figure \ref{qualitative} shows that our method handles artefacts (1st, 2nd row) and preserves high-frequency content (3rd row), even in high resolutions. Table \ref{tab:mobile_timing} shows that LP-IOANet reaches up to 20 FPS on mobile devices; it is faster than running IOANet directly on high-resolution, or using LPTN with IOANet.

\noindent \textbf{Further comparison.} Not many methods focus on runtime performance \cite{kligler2018document,lin2020bedsr,yang2012shadow,brown2006geometric}. Various non-ML methods \cite{bako2016removing, jung2018water,kligler2018document,osrDataset} have hardware-optimized implementations, but none of them utilize GPUs or can scale their results with more data. Our method leverages GPUs and scales its results significantly with more data (see Table \ref{tab:ablationDatasets}).

\vspace{-4mm}
\subsection{Ablation Studies}

\noindent \textbf{Upsampling.} Table \ref{tab:upsampling} shows three upsampling solutions with different complexities. There is an expected trend here; as the upsampler becomes more complex, we get better results. The timings on desktop GPUs are not that different, but the difference becomes more visible on mobile devices, as LP-IOANet is nearly three times faster than using the original LPTN, while being as accurate. We note that all variants perform in high-resolution and comfortably outperform BEDSR.


\noindent \textbf{Datasets.} \textcolor{black}{ Table \ref{tab:ablationDatasets} shows the contribution of each dataset on IOANet performance. Each dataset in the mix introduces visible improvements, despite naive mixing during training.}

\noindent \textbf{Loss terms.} The second (L1-only) and third rows (L1 + LPIPS) of Table \ref{tab:results} shows that using LPIPS as a loss term improves the results and justifies its addition.

 \begin{table}[htbp!]
\resizebox{0.48\textwidth}{!}{%
\begin{tabular}{l|c|c}
Training Dataset & MAE & PSNR \\ \hline
BSDD & 2.3344 / 1.6414 / 4.6026 & 36.84 / 13.76 / 14.11	\\
A-BSDD & 2.1163 / 1.4418 / 4.4220 & 37.43 / 13.91 / 14.14	 \\
BSDD \& Doc3DS+ & 2.0560 / 1.3761 / 4.2813 & 37.73 / 13.99 / 14.18	 \\
All & \textbf{1.7893 / 1.0937 / 4.0659}  & \textbf{38.76 / 14.08 / 14.26}	
\end{tabular}%
}
\vspace{-3mm}
    \caption{BSDD evaluation when IOANet is trained with different datasets. \textit{All} refers to A-BSDD, A-OSR and Doc3DS+.}
    \label{tab:ablationDatasets}
\end{table}



