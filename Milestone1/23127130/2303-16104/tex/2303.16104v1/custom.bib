% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{monge1781mémoire,
  title={M{\'e}moire sur la th{\'e}orie des d{\'e}blais et des remblais},
  author={Monge, G.},
  url={https://books.google.co.uk/books?id=IG7CGwAACAAJ},
  year={1781},
  publisher={Imprimerie royale}
}

@article{kantorovich2006translocation,
  title={On the translocation of masses},
  author={Kantorovich, Leonid V},
  journal={Journal of mathematical sciences},
  volume={133},
  number={4},
  pages={1381--1382},
  year={2006},
  publisher={Kluwer Academic Publishers-Consultants Bureau}
}
@article{peyre2019computational,
  title={Computational optimal transport: With applications to data science},
  author={Peyr{\'e}, Gabriel and Cuturi, Marco and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={11},
  number={5-6},
  pages={355--607},
  year={2019},
  publisher={Now Publishers, Inc.}
}
@article{chow1970optimum,
  title={On optimum recognition error and reject tradeoff},
  author={Chow, C},
  journal={IEEE Transactions on information theory},
  volume={16},
  number={1},
  pages={41--46},
  year={1970},
  publisher={IEEE}
}

@book{villani2009optimal,
  title={Optimal transport: old and new},
  author={Villani, C{\'e}dric},
  volume={338},
  year={2009},
  publisher={Springer}
}

@inproceedings{koehn-etal-2019-findings,
    title = "Findings of the {WMT} 2019 Shared Task on Parallel Corpus Filtering for Low-Resource Conditions",
    author = "Koehn, Philipp  and
      Guzm{\'a}n, Francisco  and
      Chaudhary, Vishrav  and
      Pino, Juan",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5404",
    doi = "10.18653/v1/W19-5404",
    pages = "54--72",
    abstract = "Following the WMT 2018 Shared Task on Parallel Corpus Filtering, we posed the challenge of assigning sentence-level quality scores for very noisy corpora of sentence pairs crawled from the web, with the goal of sub-selecting 2{\%} and 10{\%} of the highest-quality data to be used to train machine translation systems. This year, the task tackled the low resource condition of Nepali-English and Sinhala-English. Eleven participants from companies, national research labs, and universities participated in this task.",
}

@inproceedings{koehn-2005-europarl,
    title = "{E}uroparl: A Parallel Corpus for Statistical Machine Translation",
    author = "Koehn, Philipp",
    booktitle = "Proceedings of Machine Translation Summit X: Papers",
    month = sep # " 13-15",
    year = "2005",
    address = "Phuket, Thailand",
    url = "https://aclanthology.org/2005.mtsummit-papers.11",
    pages = "79--86",
    abstract = "We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web. This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead.",
}

@InProceedings{akhbardeh-EtAl:2021:WMT,
  author    = {Akhbardeh, Farhad  and  Arkhangorodsky, Arkady  and  Biesialska, Magdalena  and  Bojar, Ondřej  and  Chatterjee, Rajen  and  Chaudhary, Vishrav  and  Costa-jussa, Marta R.  and  España-Bonet, Cristina  and  Fan, Angela  and  Federmann, Christian  and  Freitag, Markus  and  Graham, Yvette  and  Grundkiewicz, Roman  and  Haddow, Barry  and  Harter, Leonie  and  Heafield, Kenneth  and  Homan, Christopher  and  Huck, Matthias  and  Amponsah-Kaakyire, Kwabena  and  Kasai, Jungo  and  Khashabi, Daniel  and  Knight, Kevin  and  Kocmi, Tom  and  Koehn, Philipp  and  Lourie, Nicholas  and  Monz, Christof  and  Morishita, Makoto  and  Nagata, Masaaki  and  Nagesh, Ajay  and  Nakazawa, Toshiaki  and  Negri, Matteo  and  Pal, Santanu  and  Tapo, Allahsera Auguste  and  Turchi, Marco  and  Vydrin, Valentin  and  Zampieri, Marcos},
  title     = {Findings of the 2021 Conference on Machine Translation {WMT21})},
  booktitle      = {Proceedings of the Sixth Conference on Machine Translation},
  month          = {November},
  year           = {2021},
  address        = {Online},
  publisher      = {Association for Computational Linguistics},
  pages     = {1--88},
  abstract  = {This paper presents the results of the newstranslation task, the multilingual low-resourcetranslation for Indo-European languages, thetriangular translation task, and the automaticpost-editing task organised as part of the Con-ference on Machine Translation (WMT) 2021.In the news task, participants were asked tobuild machine translation systems for any of10 language pairs, to be evaluated on test setsconsisting mainly of news stories. The taskwas also opened up to additional test suites toprobe specific aspects of translation.},
  url       = {https://aclanthology.org/2021.wmt-1.1}
}


@InProceedings{gal-mcdropout,
  title = 	 {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  author = 	 {Gal, Yarin and Ghahramani, Zoubin},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1050--1059},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/gal16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/gal16.html},
  abstract = 	 {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.}
}

@article{Panaretos_2019,
	doi = {10.1146/annurev-statistics-030718-104938},
  
	url = {https://doi.org/10.1146%2Fannurev-statistics-030718-104938},
  
	year = 2019,
	month = {mar},
  
	publisher = {Annual Reviews},
  
	volume = {6},
  
	number = {1},
  
	pages = {405--431},
  
	author = {Victor M. Panaretos and Yoav Zemel},
  
	title = {Statistical Aspects of Wasserstein Distances},
  
	journal = {Annual Review of Statistics and Its Application}
}

@misc{aces_chantal,
  doi = {10.48550/ARXIV.2210.15615},
  
  url = {https://arxiv.org/abs/2210.15615},
  
  author = {Amrhein, Chantal and Moghe, Nikita and Guillou, Liane},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.7},
  
  title = {ACES: Translation Accuracy Challenge Sets for Evaluating Machine Translation Metrics},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{xlm_roberta_conneau,
  doi = {10.48550/ARXIV.1911.02116},
  
  url = {https://arxiv.org/abs/1911.02116},
  
  author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzmán, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Unsupervised Cross-lingual Representation Learning at Scale},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{ne_nmt_mistranslation,
author = {Li, Panpan and Wang, Mengxiang and Wang, Jian},
title = {Named Entity Translation Method Based on Machine Translation Lexicon},
year = {2021},
issue_date = {May 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {9},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05509-y},
doi = {10.1007/s00521-020-05509-y},
abstract = {In the context of the rapid development of computer technology, communication between various languages has become increasingly important. Among the research methods of named entities, the research on named entity translation methods based on machine translation is one of the research hotspots. Named entity translation is to realize the switching between entities in two languages, which can be used by browsers, translators, etc., and can greatly reduce the cost of communication between people from all over the world. Due to the immaturity of the existing translation model technology and the lack of integration, the translation of bilingual named entities with unique composition is very challenging. Based on this, this paper proposes a fusion method of bilingual entity class named entity translation based on chunk symmetry strategy and English–Chinese transliteration model based on machine learning strategy. According to the bilingual corpus, a more standard candidate entity translation pair is generated through bilingual named entity alignment. The transliteration model is used to reorder and correct the candidate translation results, so as to achieve the correct selection of translation pairs. Experiments show that the model based on the translation of named entity translations based on chunks and transliteration models based on machine learning strategies not only effectively solves the problem of difficulty in word ordering and selection in bilingual translation, but also makes extraction to a certain extent. The accuracy of the translation results has been improved.},
journal = {Neural Comput. Appl.},
month = {may},
pages = {3977–3985},
numpages = {9},
keywords = {Chunk symmetry strategy, Machine translation, Machine transliteration model, Named entity translation}
}


@article{mqm,
author = {Lommel, Arle and Burchardt, Aljoscha and Uszkoreit, Hans},
year = {2014},
month = {12},
pages = {455-463},
title = {Multidimensional Quality Metrics (MQM): A Framework for Declaring and Describing Translation Quality Metrics},
volume = {0},
journal = {Tradumàtica: tecnologies de la traducció},
doi = {10.5565/rev/tradumatica.77}
}

@inproceedings{RanzatoCAZ15,
  author    = {Marc'Aurelio Ranzato and
               Sumit Chopra and
               Michael Auli and
               Wojciech Zaremba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Sequence Level Training with Recurrent Neural Networks},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1511.06732},
  timestamp = {Thu, 25 Jul 2019 14:25:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RanzatoCAZ15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NEURIPS2019_2c601ad9,
 author = {Michel, Paul and Levy, Omer and Neubig, Graham},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Are Sixteen Heads Really Better than One?},
 url = {https://proceedings.neurips.cc/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf},
 volume = {32},
 year = {2019}
}


@misc{jietal2022,
  doi = {10.48550/ARXIV.2202.03629},
  
  url = {https://arxiv.org/abs/2202.03629},
  
  author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Yejin and Madotto, Andrea and Fung, Pascale},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences, A.1},
  
  title = {Survey of Hallucination in Natural Language Generation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}


@misc{salted_raunak2022,
  doi = {10.48550/ARXIV.2205.09988},
  
  url = {https://arxiv.org/abs/2205.09988},
  
  author = {Raunak, Vikas and Post, Matt and Menezes, Arul},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {SALTED: A Framework for SAlient Long-Tail Translation Error Detection},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}



@misc{stahlberg_scones_2022,
  doi = {10.48550/ARXIV.2205.00704},
  
  url = {https://arxiv.org/abs/2205.00704},
  
  author = {Stahlberg, Felix and Kumar, Shankar},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Jam or Cream First? Modeling Ambiguity in Neural Machine Translation with SCONES},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{chantal_2022,
  doi = {10.48550/ARXIV.2202.05148},
  
  url = {https://arxiv.org/abs/2202.05148},
  
  author = {Amrhein, Chantal and Sennrich, Rico},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.7},
  
  title = {Identifying Weaknesses in Machine Translation Metrics Through Minimum Bayes Risk Decoding: A Case Study for COMET},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{maynez_summ_halls,
  doi = {10.48550/ARXIV.2005.00661},
  
  url = {https://arxiv.org/abs/2005.00661},
  
  author = {Maynez, Joshua and Narayan, Shashi and Bohnet, Bernd and McDonald, Ryan},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {On Faithfulness and Factuality in Abstractive Summarization},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{malinin2021structured,
  title={Uncertainty Estimation in Autoregressive Structured Prediction},
  author={Malinin, Andrey and Gales, Mark},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{fomicheva-etal-2020-multi,
    title = "Multi-Hypothesis Machine Translation Evaluation",
    author = "Fomicheva, Marina  and
      Specia, Lucia  and
      Guzm{\'a}n, Francisco",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.113",
    doi = "10.18653/v1/2020.acl-main.113",
    pages = "1218--1232",
    abstract = "Reliably evaluating Machine Translation (MT) through automated metrics is a long-standing problem. One of the main challenges is the fact that multiple outputs can be equally valid. Attempts to minimise this issue include metrics that relax the matching of MT output and reference strings, and the use of multiple references. The latter has been shown to significantly improve the performance of evaluation metrics. However, collecting multiple references is expensive and in practice a single reference is generally used. In this paper, we propose an alternative approach: instead of modelling linguistic variation in human reference we exploit the MT model uncertainty to generate multiple diverse translations and use these: (i) as surrogates to reference translations; (ii) to obtain a quantification of translation variability to either complement existing metric scores or (iii) replace references altogether. We show that for a number of popular evaluation metrics our variability estimates lead to substantial improvements in correlation with human judgements of quality by up 15{\%}.",
}

@article{tacl_freitag,
    author = {Freitag, Markus and Foster, George and Grangier, David and Ratnakar, Viresh and Tan, Qijun and Macherey, Wolfgang},
    title = "{Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {1460-1474},
    year = {2021},
    month = {12},
    abstract = "{Human evaluation of modern high-quality machine translation systems is a difficult problem, and there is increasing evidence that inadequate evaluation procedures can lead to erroneous conclusions. While there has been considerable research on human evaluation, the field still lacks a commonly accepted standard procedure. As a step toward this goal, we propose an evaluation methodology grounded in explicit error analysis, based on the Multidimensional Quality Metrics (MQM) framework. We carry out the largest MQM research study to date, scoring the outputs of top systems from the WMT 2020 shared task in two language pairs using annotations provided by professional translators with access to full document context. We analyze the resulting data extensively, finding among other results a substantially different ranking of evaluated systems from the one established by the WMT crowd workers, exhibiting a clear preference for human over machine output. Surprisingly, we also find that automatic metrics based on pre-trained embeddings can outperform human crowd workers. We make our corpus publicly available for further research.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00437},
    url = {https://doi.org/10.1162/tacl\_a\_00437},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00437/1979261/tacl\_a\_00437.pdf},
}




@misc{toshiptom,
  doi = {10.48550/ARXIV.2107.10821},
  
  url = {https://arxiv.org/abs/2107.10821},
  
  author = {Kocmi, Tom and Federmann, Christian and Grundkiewicz, Roman and Junczys-Dowmunt, Marcin and Matsushita, Hitokazu and Menezes, Arul},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{prompsit:2018:WMT,
  author    = { V\'{i}ctor M. S\'{a}nchez-Cartagena and Marta Ba{\~n}\'{o}n and Sergio Ortiz-Rojas and Gema Ram\'{i}rez-S\'{a}nchez},
  title     = {Prompsit's submission to WMT 2018 Parallel Corpus Filtering shared task},
  booktitle = {Proceedings of the Third Conference on Machine Translation, Volume 2: Shared Task Papers},
  month     = {October},
  year     = {2018},
  address   = {Brussels, Belgium},
  publisher = {Association for Computational Linguistics}
}

@InProceedings{prompsit:2020:EAMT,
  author    = {Gema Ram\'{i}rez-S\'{a}nchez and Jaume Zaragoza-Bernabeu and Marta Ba{\~n}\'{o}n and Sergio Ortiz-Rojas},
  title     = {Bifixer and Bicleaner: two open-source tools to clean your parallel data.},
  booktitle = {Proceedings of the 22nd Annual Conference of the European Association for Machine Translation},
  pages	    = {291--298},
  isbn      = {978-989-33-0589-8},
  year	    = {2020},
  month     = {November},
  address   = {Lisboa, Portugal},
  publisher = {European Association for Machine Translation}
}

@article{10.1162/tacl_a_00447,
    author = {Kreutzer, Julia and Caswell, Isaac and Wang, Lisa and Wahab, Ahsan and van Esch, Daan and Ulzii-Orshikh, Nasanbayar and Tapo, Allahsera and Subramani, Nishant and Sokolov, Artem and Sikasote, Claytone and Setyawan, Monang and Sarin, Supheakmungkol and Samb, Sokhar and Sagot, Benoît and Rivera, Clara and Rios, Annette and Papadimitriou, Isabel and Osei, Salomey and Suarez, Pedro Ortiz and Orife, Iroro and Ogueji, Kelechi and Rubungo, Andre Niyongabo and Nguyen, Toan Q. and Müller, Mathias and Müller, André and Muhammad, Shamsuddeen Hassan and Muhammad, Nanda and Mnyakeni, Ayanda and Mirzakhalov, Jamshidbek and Matangira, Tapiwanashe and Leong, Colin and Lawson, Nze and Kudugunta, Sneha and Jernite, Yacine and Jenny, Mathias and Firat, Orhan and Dossou, Bonaventure F. P. and Dlamini, Sakhile and de Silva, Nisansa and Çabuk Ballı, Sakine and Biderman, Stella and Battisti, Alessia and Baruwa, Ahmed and Bapna, Ankur and Baljekar, Pallavi and Azime, Israel Abebe and Awokoya, Ayodele and Ataman, Duygu and Ahia, Orevaoghene and Ahia, Oghenefego and Agrawal, Sweta and Adeyemi, Mofetoluwa},
    title = "{Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {10},
    pages = {50-72},
    year = {2022},
    month = {01},
    abstract = "{With the success of large-scale pre-training and multilingual modeling in Natural Language Processing (NLP), recent years have seen a proliferation of large, Web-mined text datasets covering hundreds of languages. We manually audit the quality of 205 language-specific corpora released with five major public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource corpora have systematic issues: At least 15 corpora have no usable text, and a significant fraction contains less than 50\\% sentences of acceptable quality. In addition, many are mislabeled or use nonstandard/ambiguous language codes. We demonstrate that these issues are easy to detect even for non-proficient speakers, and supplement the human audit with automatic analyses. Finally, we recommend techniques to evaluate and improve multilingual corpora and discuss potential risks that come with low-quality data releases.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00447},
    url = {https://doi.org/10.1162/tacl\_a\_00447},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00447/1986585/tacl\_a\_00447.pdf},
}

@misc{vashishthetal2019,
  doi = {10.48550/ARXIV.1909.11218},
  
  url = {https://arxiv.org/abs/1909.11218},
  
  author = {Vashishth, Shikhar and Upadhyay, Shyam and Tomar, Gaurav Singh and Faruqui, Manaal},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Attention Interpretability Across NLP Tasks},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{probing_halls_2022,
  doi = {10.48550/ARXIV.2206.12529},
  
  url = {https://arxiv.org/abs/2206.12529},
  
  author = {Yan, Jianhao and Meng, Fandong and Zhou, Jie},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Probing Causes of Hallucinations in Neural Machine Translations},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{kappa_cohen,
author = {Jacob Cohen},
title ={A Coefficient of Agreement for Nominal Scales},
journal = {Educational and Psychological Measurement},
volume = {20},
number = {1},
pages = {37-46},
year = {1960},
doi = {10.1177/001316446002000104},

URL = { 
        https://doi.org/10.1177/001316446002000104
    
},
eprint = { 
        https://doi.org/10.1177/001316446002000104
    
}

}

@inproceedings{Lee2018HallucinationsIN,
  title={Hallucinations in Neural Machine Translation},
  author={Katherine Lee and Orhan Firat and Ashish Agarwal and Clara Fannjiang and David Sussillo},
  year={2018}
}

@inproceedings{koehn-knowles-2017-six,
    title = "Six Challenges for Neural Machine Translation",
    author = "Koehn, Philipp  and
      Knowles, Rebecca",
    booktitle = "Proceedings of the First Workshop on Neural Machine Translation",
    month = aug,
    year = "2017",
    address = "Vancouver",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-3204",
    doi = "10.18653/v1/W17-3204",
    pages = "28--39",
    abstract = "We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.",
}


@inproceedings{stahlberg-byrne-2019-nmt,
    title = "On {NMT} Search Errors and Model Errors: Cat Got Your Tongue?",
    author = "Stahlberg, Felix  and
      Byrne, Bill",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1331",
    doi = "10.18653/v1/D19-1331",
    pages = "3356--3362",
    abstract = "We report on search errors and model errors in neural machine translation (NMT). We present an exact inference procedure for neural sequence models based on a combination of beam search and depth-first search. We use our exact search to find the global best model scores under a Transformer base model for the entire WMT15 English-German test set. Surprisingly, beam search fails to find these global best model scores in most cases, even with a very large beam size of 100. For more than 50{\%} of the sentences, the model in fact assigns its global best score to the empty translation, revealing a massive failure of neural models in properly accounting for adequacy. We show by constraining search with a minimum translation length that at the root of the problem of empty translations lies an inherent bias towards shorter translations. We conclude that vanilla NMT in its current form requires just the right amount of beam search errors, which, from a modelling perspective, is a highly unsatisfactory conclusion indeed, as the model often prefers an empty translation.",
}

@article{COTFNT,
  doi = {10.48550/ARXIV.1803.00567},
  
  url = {https://arxiv.org/abs/1803.00567},
  
  author = {Peyré, Gabriel and Cuturi, Marco},
  
  keywords = {Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Computational Optimal Transport},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{calibration-end-dec-nmt,
  author    = {Aviral Kumar and
               Sunita Sarawagi},
  title     = {Calibration of Encoder Decoder Models for Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1903.00802},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.00802},
  eprinttype = {arXiv},
  eprint    = {1903.00802},
  timestamp = {Sat, 30 Mar 2019 19:27:21 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1903-00802.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@misc{lu2023error,
      title={Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT}, 
      author={Qingyu Lu and Baopu Qiu and Liang Ding and Liping Xie and Dacheng Tao},
      year={2023},
      eprint={2303.13809},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{banerjee-lavie-2005-meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W05-0909",
    pages = "65--72",
}


@inproceedings{
Zhang2020BERTScore:,
title={BERTScore: Evaluating Text Generation with BERT},
author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkeHuCVFDr}
}

@inproceedings{lewis-etal-2020-bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
}

@misc{nllb2022,
  doi = {10.48550/ARXIV.2207.04672},
  
  url = {https://arxiv.org/abs/2207.04672},
  
  author = {{NLLB Team} and Costa-jussà, Marta R. and Cross, James and Çelebi, Onur and Elbayad, Maha and Heafield, Kenneth and Heffernan, Kevin and Kalbassi, Elahe and Lam, Janice and Licht, Daniel and Maillard, Jean and Sun, Anna and Wang, Skyler and Wenzek, Guillaume and Youngblood, Al and Akula, Bapi and Barrault, Loic and Gonzalez, Gabriel Mejia and Hansanti, Prangthip and Hoffman, John and Jarrett, Semarley and Sadagopan, Kaushik Ram and Rowe, Dirk and Spruit, Shannon and Tran, Chau and Andrews, Pierre and Ayan, Necip Fazil and Bhosale, Shruti and Edunov, Sergey and Fan, Angela and Gao, Cynthia and Goswami, Vedanuj and Guzmán, Francisco and Koehn, Philipp and Mourachko, Alexandre and Ropers, Christophe and Saleem, Safiyyah and Schwenk, Holger and Wang, Jeff},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.7, 68T50},
  
  title = {No Language Left Behind: Scaling Human-Centered Machine Translation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}


@misc{arivazhagan-etal-2019,
  doi = {10.48550/ARXIV.1907.05019},
  
  url = {https://arxiv.org/abs/1907.05019},
  
  author = {Arivazhagan, Naveen and Bapna, Ankur and Firat, Orhan and Lepikhin, Dmitry and Johnson, Melvin and Krikun, Maxim and Chen, Mia Xu and Cao, Yuan and Foster, George and Cherry, Colin and Macherey, Wolfgang and Chen, Zhifeng and Wu, Yonghui},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{fanetal2020_m2m,
  doi = {10.48550/ARXIV.2010.11125},
  
  url = {https://arxiv.org/abs/2010.11125},
  
  author = {Fan, Angela and Bhosale, Shruti and Schwenk, Holger and Ma, Zhiyi and El-Kishky, Ahmed and Goyal, Siddharth and Baines, Mandeep and Celebi, Onur and Wenzek, Guillaume and Chaudhary, Vishrav and Goyal, Naman and Birch, Tom and Liptchinsky, Vitaliy and Edunov, Sergey and Grave, Edouard and Auli, Michael and Joulin, Armand},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Beyond English-Centric Multilingual Machine Translation},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{dale-etal-2022-hallsalti,
  doi = {10.48550/ARXIV.2212.08597},
  
  url = {https://arxiv.org/abs/2212.08597},
  
  author = {Dale, David and Voita, Elena and Barrault, Loïc and Costa-jussà, Marta R.},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.7},
  
  title = {Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@misc{xu-etal-2022-taclhalls,
  doi = {10.48550/ARXIV.2301.07779},
  
  url = {https://arxiv.org/abs/2301.07779},
  
  author = {Xu, Weijia and Agrawal, Sweta and Briakou, Eleftheria and Martindale, Marianna J. and Carpuat, Marine},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{guerreiro-etal-2022-lookingforaneedle,
  doi = {10.48550/ARXIV.2208.05309},
  
  url = {https://arxiv.org/abs/2208.05309},
  
  author = {Guerreiro, Nuno M. and Voita, Elena and Martins, André F. T.},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Looking for a Needle in a Haystack: A Comprehensive Study of Hallucinations in Neural Machine Translation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{guerreiro-etal-2022-optimaltransport,
  doi = {10.48550/ARXIV.2212.09631},
  
  url = {https://arxiv.org/abs/2212.09631},
  
  author = {Guerreiro, Nuno M. and Colombo, Pierre and Piantanida, Pablo and Martins, André F. T.},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Optimal Transport for Unsupervised Hallucination Detection in Neural Machine Translation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{bawdenandyvon2023_bloom,
  doi = {10.48550/ARXIV.2303.01911},
  
  url = {https://arxiv.org/abs/2303.01911},
  
  author = {Bawden, Rachel and Yvon, François},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Investigating the Translation Performance of a Large Multilingual Language Model: the Case of BLOOM},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@article{Peng2023ChatGPT4MT,
  title={Towards Making the Most of ChatGPT for Machine Translation},
  author={Peng, Keqin and Ding, Liang and Zhong, Qihuang and Shen, Li and Liu, Xuebo and Zhang, Min and Ouyang, Yuanxin and Tao, Dacheng},
  journal={ResearchGate preprint},
  doi={https://doi.org/10.13140/RG.2.2.24416.97283},
  year={2023}
}

@misc{vilaretal2023_palm,
  doi = {10.48550/ARXIV.2211.09102},
  
  url = {https://arxiv.org/abs/2211.09102},
  
  author = {Vilar, David and Freitag, Markus and Cherry, Colin and Luo, Jiaming and Ratnakar, Viresh and Foster, George},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Prompting PaLM for Translation: Assessing Strategies and Performance},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{hendyetal2023_microsoftchatgpt,
  doi = {10.48550/ARXIV.2302.09210},
  
  url = {https://arxiv.org/abs/2302.09210},
  
  author = {Hendy, Amr and Abdelrehim, Mohamed and Sharaf, Amr and Raunak, Vikas and Gabr, Mohamed and Matsushita, Hitokazu and Kim, Young Jin and Afify, Mohamed and Awadalla, Hany Hassan},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{kumaretal2022_harmeacl,
  doi = {10.48550/ARXIV.2210.07700},
  
  url = {https://arxiv.org/abs/2210.07700},
  
  author = {Kumar, Sachin and Balachandran, Vidhisha and Njoo, Lucille and Anastasopoulos, Antonios and Tsvetkov, Yulia},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{perez2022red,
  title={Red teaming language models with language models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2202.03286},
  year={2022}
}

@misc{fernandesetal2023_scalinglaws,
  doi = {10.48550/ARXIV.2302.09650},
  
  url = {https://arxiv.org/abs/2302.09650},
  
  author = {Fernandes, Patrick and Ghorbani, Behrooz and Garcia, Xavier and Freitag, Markus and Firat, Orhan},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Scaling Laws for Multilingual Neural Machine Translation},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{siddhantetal2022_multilingual_google,
  doi = {10.48550/ARXIV.2201.03110},
  
  url = {https://arxiv.org/abs/2201.03110},
  
  author = {Siddhant, Aditya and Bapna, Ankur and Firat, Orhan and Cao, Yuan and Chen, Mia Xu and Caswell, Isaac and Garcia, Xavier},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Towards the Next 1000 Languages in Multilingual Machine Translation: Exploring the Synergy Between Supervised and Self-Supervised Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{googletranslate2022,
  doi = {10.48550/ARXIV.2205.03983},
  
  url = {https://arxiv.org/abs/2205.03983},
  
  author = {Bapna, Ankur and Caswell, Isaac and Kreutzer, Julia and Firat, Orhan and van Esch, Daan and Siddhant, Aditya and Niu, Mengmeng and Baljekar, Pallavi and Garcia, Xavier and Macherey, Wolfgang and Breiner, Theresa and Axelrod, Vera and Riesa, Jason and Cao, Yuan and Chen, Mia Xu and Macherey, Klaus and Krikun, Maxim and Wang, Pidong and Gutkin, Alexander and Shah, Apurva and Huang, Yanping and Chen, Zhifeng and Wu, Yonghui and Hughes, Macduff},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Building Machine Translation Systems for the Next Thousand Languages},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{palm_google,
  doi = {10.48550/ARXIV.2204.02311},
  
  url = {https://arxiv.org/abs/2204.02311},
  
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {PaLM: Scaling Language Modeling with Pathways},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@misc{browngpt3_openai,
  doi = {10.48550/ARXIV.2005.14165},
  
  url = {https://arxiv.org/abs/2005.14165},
  
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Language Models are Few-Shot Learners},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{hanetal_openai_2021_unsupervised,
  doi = {10.48550/ARXIV.2110.05448},
  
  url = {https://arxiv.org/abs/2110.05448},
  
  author = {Han, Jesse Michael and Babuschkin, Igor and Edwards, Harrison and Neelakantan, Arvind and Xu, Tao and Polu, Stanislas and Ray, Alex and Shyam, Pranav and Ramesh, Aditya and Radford, Alec and Sutskever, Ilya},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Unsupervised Neural Machine Translation with Generative Language Models Only},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{garciaetal2023_google,
  doi = {10.48550/ARXIV.2302.01398},
  
  url = {https://arxiv.org/abs/2302.01398},
  
  author = {Garcia, Xavier and Bansal, Yamini and Cherry, Colin and Foster, George and Krikun, Maxim and Feng, Fangxiaoyu and Johnson, Melvin and Firat, Orhan},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The unreasonable effectiveness of few-shot learning for machine translation},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{opt_2022,
  doi = {10.48550/ARXIV.2205.01068},
  
  url = {https://arxiv.org/abs/2205.01068},
  
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {OPT: Open Pre-trained Transformer Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{selfcheckgpt_2023,
  doi = {10.48550/ARXIV.2303.08896},
  
  url = {https://arxiv.org/abs/2303.08896},
  
  author = {Manakul, Potsawee and Liusie, Adian and Gales, Mark J. F.},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{instructgpt_2022,
  doi = {10.48550/ARXIV.2203.02155},
  
  url = {https://arxiv.org/abs/2203.02155},
  
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Training language models to follow instructions with human feedback},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{dhole2022nlaugmenter,
      title={NL-Augmenter: A Framework for Task-Sensitive Natural Language Augmentation}, 
      author={Kaustubh D. Dhole and Varun Gangal and Sebastian Gehrmann and Aadesh Gupta and Zhenhao Li and Saad Mahamood and Abinaya Mahendiran and Simon Mille and Ashish Shrivastava and Samson Tan and Tongshuang Wu and Jascha Sohl-Dickstein and Jinho D. Choi and Eduard Hovy and Ondrej Dusek and Sebastian Ruder and Sajant Anand and Nagender Aneja and Rabin Banjade and Lisa Barthe and Hanna Behnke and Ian Berlot-Attwell and Connor Boyle and Caroline Brun and Marco Antonio Sobrevilla Cabezudo and Samuel Cahyawijaya and Emile Chapuis and Wanxiang Che and Mukund Choudhary and Christian Clauss and Pierre Colombo and Filip Cornell and Gautier Dagan and Mayukh Das and Tanay Dixit and Thomas Dopierre and Paul-Alexis Dray and Suchitra Dubey and Tatiana Ekeinhor and Marco Di Giovanni and Tanya Goyal and Rishabh Gupta and Rishabh Gupta and Louanes Hamla and Sang Han and Fabrice Harel-Canada and Antoine Honore and Ishan Jindal and Przemyslaw K. Joniak and Denis Kleyko and Venelin Kovatchev and Kalpesh Krishna and Ashutosh Kumar and Stefan Langer and Seungjae Ryan Lee and Corey James Levinson and Hualou Liang and Kaizhao Liang and Zhexiong Liu and Andrey Lukyanenko and Vukosi Marivate and Gerard de Melo and Simon Meoni and Maxime Meyer and Afnan Mir and Nafise Sadat Moosavi and Niklas Muennighoff and Timothy Sum Hon Mun and Kenton Murray and Marcin Namysl and Maria Obedkova and Priti Oli and Nivranshu Pasricha and Jan Pfister and Richard Plant and Vinay Prabhu and Vasile Pais and Libo Qin and Shahab Raji and Pawan Kumar Rajpoot and Vikas Raunak and Roy Rinberg and Nicolas Roberts and Juan Diego Rodriguez and Claude Roux and Vasconcellos P. H. S. and Ananya B. Sai and Robin M. Schmidt and Thomas Scialom and Tshephisho Sefara and Saqib N. Shamsi and Xudong Shen and Haoyue Shi and Yiwen Shi and Anna Shvets and Nick Siegel and Damien Sileo and Jamie Simon and Chandan Singh and Roman Sitelew and Priyank Soni and Taylor Sorensen and William Soto and Aman Srivastava and KV Aditya Srivatsa and Tony Sun and Mukund Varma T and A Tabassum and Fiona Anting Tan and Ryan Teehan and Mo Tiwari and Marie Tolkiehn and Athena Wang and Zijian Wang and Gloria Wang and Zijie J. Wang and Fuxuan Wei and Bryan Wilie and Genta Indra Winata and Xinyi Wu and Witold Wydmański and Tianbao Xie and Usama Yaseen and Michael A. Yee and Jing Zhang and Yue Zhang},
      year={2022},
      eprint={2112.02721},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{joulin2016fasttext,
  title={FastText.zip: Compressing text classification models},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and J{\'e}gou, H{\'e}rve and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1612.03651},
  year={2016}
}

@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018}
}

@inproceedings{Radford2019LanguageMA,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019}
}

@misc{gptscore_2023,
  doi = {10.48550/ARXIV.2302.04166},
  
  url = {https://arxiv.org/abs/2302.04166},
  
  author = {Fu, Jinlan and Ng, See-Kiong and Jiang, Zhengbao and Liu, Pengfei},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {GPTScore: Evaluate as You Desire},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{kocmiandfedermann2023,
  doi = {10.48550/ARXIV.2302.14520},
  
  url = {https://arxiv.org/abs/2302.14520},
  
  author = {Kocmi, Tom and Federmann, Christian},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Large Language Models Are State-of-the-Art Evaluators of Translation Quality},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {Creative Commons Attribution 4.0 International}
}



@inproceedings{transformer_vaswani,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}
