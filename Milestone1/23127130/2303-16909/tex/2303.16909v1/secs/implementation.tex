%!TEX root = ../main.tex
\section{Implementation \& Early Results}
\label{sec:details}

%In this section, we provide more details on the inner working of \sys.

\subsection{Prompts to Stand-Alone \chat}

When \chat is used  alone for data cleaning (\ie Scenario 1), each dirty tuple is converted into a prompt and sent to \chat.
By default, we use the following template: ``[attribute1 : value1 ; attribute2
: value2 ; ... attribute n : ]''. Followed by a 
question statement {\em ``what is the  value of attribute n''}.
We also allow users to provide a customized prompt template, as shown in Figure~\ref{fig:scenarios}.
%\mourad{Can we say something about prompt engineering and that more can be done here!?}
Note that \chat, as a generative model, may not always give concise answers, 
we perform some post-processing on its output to extract the relevant value to be provided to the user. 


%We have tested three different prompt templates.

%\sys leverages \textbf{ChatGPT} for data cleaning as described in scenario 1. For this, each dirty tuple in impute tables is converted into a prompt and then sent to \textbf{ChatGPT}. Prompt engineering has been shown to have considerable effect on model performance. [?] 
%Thus, we give the user the option to provide any prompt template they wish to use or use the default serialized prompt template which is described below. The user also specifies the \textit{Pivot Columns} which specifies which column-value pairs to use when making the prompts. This specification of this pivot columns can be useful as it may provide the functional dependencies upon which \textbf{ChatGPT} should focus and thereby remove potentially confusing noise.
%For our experiments, we tried 3 prompt templates:

%\textit{\underline{Inclusive Prompt}}: The inclusive template describes the attributes of the tuple in a semantically correct and coherent natural language question statement.

%\textit{\underline{Prefix Prompt}}: The prefix template describes the non-missing values of the tuple as facts using semantically correct natural language sentence(s), followed by a question.

%\textit{\underline{Serialized Prompt}}: "[attribute1 : value1 ; attribute2
%: value2 ; ... attribute n : value n]". Followed by a question statement.

%We found that no singular prompt template outperformed the others across all datasets [cite our archived demo?] and thus we chose the serialized prompt template as the default for RetClean as it is the simplest to generate for any given dataset. Furthermore, since ChatGPT may not always given concise answers, we perform some post processing on its output to extract the relevant value which is provided to the user. 

\subsection{Retrieval-Based Indexer}

\sys supports two types of indexes for indexing the tuples in a user-provided data lake, namely  ElasticSearch and embeddings-based Faiss (embeddings are generated by BERT). Elastic search works well for retrieving tuples similar to the query tuple based on syntactic q-gram terms, while Faiss retrieves relevant tuples based on their semantic similarity. In the demonstration, we plan to use both and show their trade-offs. Indexes are typically constructed offline and available for use during the cleaning time. 

%When a data lake folder is uploaded, the rows (\ie tuples) in each table are serialized into the predefined prompt format mentioned above. These serialized tuples are then collected into a  table that includes the serialization of the tuple, the name of the table it originated from, and the index of that row in the table. This enables \sys to efficiently search and retrieve relevant tuples.


%\sys supports retrieval based data cleaning, therefore different indexing and retrieval options exist for the user to select. Current options include ElasticSearch and embeddings-based Faiss. When a data lake folder is uploaded to RetClean, the rows (tuples) in each table are then serialized in the serialized prompt format stated above, and are collected into a larger aggregate table with the following fields: the serialization of table tuple, the table that it originates from, and the index of that row in the table.

%If the option for ElasticSearch is set, then an index is created on the serializations of the aggregate data. If Faiss is set, then an index is created on the BERT-based encodings of these serializations.
%Once the data lake is indexed, each dirty tuple is serialized as a query and a top-$m$ search is executed. %for each from the indexed data lake. Corresponding to the index configuration, if ElasticSearch was selected the search is performed with the serialized queries, whereas if Faiss was selected the search is performed with the BERT-based encoding of the serialized queries.
%When the search completes, the top-$m$ retrieved tuples are prepared for the next step. 


\subsection{Retrieval-Based Reranker}


The top-$n$ retrieved tuples from the index are based on a coarse-grained similarity. 
The \att{Reranker} module is designed to rerank these top-$n$ results using a more fine-grained comparison mechanism, by holistically comparing each token of the query and each token of the retrieved tuple, in order to compute a better score of the retrieved tuple. 

We adopt a ColBERT-like strategy~\cite{colbert} as the default method. To achieve this, we split the tuples into attribute-value pairs, which we treat as individual ``chunks'' for processing. Each of these chunks is independently encoded using a Sentence Transformer encoder, and a maximum similarity (\att{maxsim}) operation is performed on all chunks of the query against all chunks of the retrieved tuples. We utilize cosine similarity for the \att{maxsim} operation. This process is repeated for all retrieved tuples corresponding to each query tuple. The summed \att{maxsim} score for a retrieved tuple is used to determine its ranking score, with higher scores indicating a better match.
%
We also support a Sentence Transformer cross-encoding approach. We use a cross-encoder that takes as input the pair of the serialized query tuple and a serialized retrieved tuple, and outputs a score for the similarity between the input pair. Again, we do this across all retrieved tuples per query tuple. The total score for each pair determines the ranking score (higher is better) of the retrieved tuple in the pair.

%known as {\em contextualized late interaction}, proposed by ColBERT~\cite{colbert}. The basic idea is to 

%The current reranking algorithms are neural network-based approaches, including ColBERT-like reranking and SentenceTransformer cross-encoding-based reranking, but this module can easily be extended to incorporate other rerankers. 

%In the ColBERT-like approach, we take the pairs of attributes and their values as "chunks" in the tuples from the query and and retrieved tuples. We then encode each of these chunks independently using a SentenceTransformer encoder, and perform a maximum similarity (maxsim) operation, in this case cosine similarity, for each "chunk" in the query to all chunks of a retrieved tuple. We do this across all retrieved tuples per query tuple. The total maxsim score for a retrieved tuple determines its ranking score (higher is better). 


%Based on the scores, the retrieved tuples for each query tuple are reordered, resulting in a higher-quality retrieved tuple ranking for the next step.


\subsection{Retrieval-Based Reasoner}

The reasoning module can be set to use either \chat or our custom local model. 
In the case of the former, a prompt is created with the serialized query tuple,  serialized retrieved tuple, and a question statement corresponding to the specific data cleaning task. In this scenario, \chat may select the value from the retrieved tuple provided in the prompt, generate a value of its own, or state that no such value can be found. This process is repeated for each retrieved tuple. Thus, if we have $m$ query tuples and each gets $k$ relevant tuples from the \att{Reranker}, 
then $m*k$ prompts are sent to \chat. 
The value is then extracted from \chat's response and presented 
to the user with the source information (from the data lake). 
%
Each prompt sent to \chat consists of two cascaded questions. The first question is 
{\em ``Do these two tuples relate to the same exact entity?''} with potential answers 
of Yes or No. Only if the answer is Yes, the second question, within the same prompt,  
becomes active in the form of {\em ``what is the value for ...''}. 



For the local model case,  
we developed two custom RoBERTa-based~\cite{roberta} models; the {\em Matcher} 
and {\em Extractor} models, both are encapsulated inside the \att{Reasoner} module. 
The {\em Matcher} model is trained to take two serialized tuples (query \& retrieved) 
and outputs a Boolean value indicating whether or not they \textit{match}.
Here, matching implies two conditions;  the two tuples are about the same entity, e.g., same movie, player, book, and the target dirty attribute is present in the retrieved tuple, even if it is not an exact syntactic match.  
Notice that the pivot columns significantly help in this task because the {\em Matcher} focuses only on the columns that matter. 
In our experiments, we also found that for unseen datasets and schemas, the performance of the {\em Matcher} model significantly improves if it is fine-tuned on a small number of examples, e.g., 10 or 20 samples from the new dataset. 
Thus, \sys allows the  user to provide a sample dataset for fine-tuning the model. 

A pair of tuples that passes the  {\em Matcher} feeds the 
{\em Extractor} model, which is trained to identify and extract 
the desired value from the retrieved tuple.    
The model compares the Sentence Transformer embedding of the dirty attribute name with the embedding of each attribute name in the matched retrieved tuple to identify the most similar attribute using cosine similarity. That attribute is used for extracting the  value missing in the query tuple.
%The value for each query tuple matched with a retrieved tuple is the value of the attribute with the greatest similarity to the dirty attribute name. 
%These values are then returned to the user along with the source from the data lake




\subsection{Preliminary Results}
In Table~\ref{Table:earlyresults}, we present the results on our experiment on four datasets using the three cleaning approaches.
The dirty columns with missing values for each dataset are: "Country" for Cricket Players (CP), "Genetic Order" for Animals (AN), "Age Rating" for Shows-Movies (SM), 
and "Department" for QCRI Personnel (QP).   
For the retrieval-based techniques, we manually constructed a data lake of 12 tables covering the four domains. 
For the CP dataset, which is mostly part of the world knowledge, we observe that the stand-alone ChatGPT as well as the retrieval-based techniques all perform well. 
However, for the AN and SM datasets where the missing information is harder to find, \eg genetic information for different animals, or show ratings for unpopular shows, the retrieval-based techniques are superior. 
For a very domain-specific dataset (\eg the QP dataset), it is clear that 
the stand-alone ChatGPT is useless, and the cleaning process has to rely on  internal data lakes. 
It is worth highlighting that our developed custom model (Scenario 3)
is competitively effective in its inference power to ChatGPT. 

\small{
\begin{table}[h!]
\begin{center}
\begin{tabular}{|l|l|l|l|l|}
\hline
Dataset & \#Tuples & Scenario 1 & Scenario 2 & Scenario 3 \\ 
 \hline\hline
Cricket Players (CP)& 100 & 97\% & 96\% & 97\% \\\hline 
Animals (AN) & 100 & 79\% & 96\% & 96\%  \\\hline
Shows-Movies (SM) & 100 & 27\% & 57\% & 74\% \\\hline
QCRI Personnel (QP) & 18 & 5\% & 94\% & 88\% \\\hline
\end{tabular}
\caption{Accuracy Scores for RetClean Experiments.}
\label{Table:earlyresults}
\end{center}
\vspace{-4mm}
\end{table}
}


