\section{Related Work}
\label{sec:related}


\subsection{Experience Replay Methods}

Experience replay or rehearsal methods use a small memory buffer to keep the training data of old tasks. When the model is training on the new task, old training examples are extracted and trained together with new ones. 
Recent replay-based CL approaches mainly differ in three components, namely which examples to store, how examples are replayed, and how to update the network using old examples. 
Recent work has focused on evolving the three components mentioned above. 
ICaRL~\cite{rebuffi2017icarl} chooses examples into the memory such that the mean in the feature space of the memory buffer matches that of training data. 
MIR~\cite{aljundi2019online} prioritizes the replay of the examples that are mostly interfered with by a virtual update on the network parameters. 
DER/DER++~\cite{buzzega2020dark} augments the cross entropy loss with a logit distillation loss when the memory data is replayed. 
GEM~\cite{lopez2017gradient} and A-GEM~\cite{chaudhry2018efficient} develop optimization constraints when trained on new tasks using the old data from memory. 
Some other work~\cite{shin2017continual, liu2020generative, van2018generative} also learn to generate images for replay during CL, but the continual training of the generation network adds some additional challenges. 
Although the idea is straightforward, experience replay methods often achieve better performance than other types of methods, which marks the importance of storing the old data. 

\subsection{Parameter Regularization Methods}

Parameter regularization methods study the effect of neural network weight changes on task losses and limit the movement of important ones, which would otherwise cause forgetting on old tasks. This line of work typically does not rely on a replay buffer for old task data. 
One of the pioneering works along this line is EWC~\cite{kirkpatrick2017overcoming}, which proposed to use the empirical Fisher Information Matrix to estimate the importance of the weight and regularize the weight change in continual learning. 
SI~\cite{zenke2017continual} uses the estimated path integral in the optimization process as the regularization weight for network parameters. 
MAS~\cite{aljundi2018memory} improves this idea by adopting the gradient magnitude as a sensitivity measure. 
RWalk~\cite{chaudhry2018riemannian} combines Fisher information matrix and online path integral to approximate the parameter importance and also keeps a memory to improve results. 

\subsection{Knowledge Distillation Methods}

Originally designed to transfer the learned knowledge of a larger network (teacher) to a smaller one (student), knowledge distillation methods have been adapted to reduce activation and feature drift in continual learning. 
Different from parameter regularization methods that directly regularize the network weights, KD methods regularize the network intermediate outputs. 
Li~\etal~\cite{li2017learning} proposed an approach called Learning without Forgetting (LwF), regularizing the output logits between the online learned model and the old checkpoint. 
DER/DER++~\cite{buzzega2020dark} combines this logit regularization with experience replay and further improves the performance. 
Later Jung~\etal~\cite{jung2016less} proposed to do knowledge distillation on the feature maps from the penultimate layer and freeze the final classification layer. 
Pooled Output Distillation (PODNet)~\cite{douillard2020podnet} extended the knowledge distillation method to intermediate feature maps, and studied how different ways of feature map pooling affect continual learning performance. They proposed to pool the feature maps along the height and weight dimensions respectively to achieve a good stability-plasticity trade-off. 
Recent work~\cite{dhar2019learning, kang2022class} also used gradient information (e.g. Grad-CAM) as weighting terms in the feature distillation loss, such that feature maps that are important to old tasks will change less during continual learning. 

Different from existing KD methods, we use a learnable linear layer to project new features to the old ones. 
This idea was explored in~\cite{fini2022self, gomez2022continually}, but their work only integrated it in a contrastive learning framework and used a nonlinear mapping function. However, in this work, we use a learnable \textit{linear} transformation and formulate it as a simple knowledge distillation loss in the feature space. 
We demonstrate that our method promotes linear separability in feature space during continual learning. 
We also show the value of BFP in the supervised CL setting with experience replay, and no augmentation or contrastive learning is needed. 
