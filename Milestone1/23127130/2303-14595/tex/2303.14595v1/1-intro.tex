\section{Introduction}
\label{sec:intro}

\input{fig-teaser}

Despite their many successes, deep neural networks remain prone to catastrophic forgetting~\cite{mccloskey1989catastrophic}, whereby a model's performance on old tasks degrades significantly while it is learning to solve new tasks. 
Catastrophic forgetting has become a major challenge for continual learning (CL) scenarios, where the model is trained on a sequence of tasks, with limited or no access to old training data. 
The ability to learn continually without forgetting is crucial to many real-world applications, such as computer vision~\cite{masana2022class, wang2021wanderlust}, intelligent robotics~\cite{lesort2020continual}, and natural language processing~\cite{biesialska2020continual, jang2021towards}. 
In these settings, an agent learns from a stream of new data or tasks, but training on the old data is restricted due to limitations in storage, scaling of training time, or even concerns about privacy. 

The continual learning problem has received significant attention and multiple solution themes have emerged.
Experience replay methods~\cite{buzzega2020dark, lopez2017gradient}, for example, store a limited number of (or generate) old training examples and use them together with new data in continual learning. 
Parameter regularization methods~\cite{lee2017overcoming, zenke2017continual} restrict the change of important network parameters. Knowledge distillation methods~\cite{li2017learning, dhar2019learning, douillard2020podnet} regularize the intermediate output of the CL model to preserve the knowledge from old tasks. Architectural methods~\cite{rusu2016progressive, mallya2018packnet, yan2021dynamically} adopt expansion and isolation techniques with neural networks to prevent forgetting. All these methods strive to balance learning new knowledge (plasticity) and retaining old knowledge (stability). 


We present a continual learning algorithm, focusing on knowledge distillation (KD) in feature space. 
In the continual learning context, KD treats the continual learning model as the student and its old checkpoint as the teacher and regularizes the network intermediate outputs to reduce forgetting~\cite{li2017learning, buzzega2020dark, douillard2020podnet, dhar2019learning, kang2022class, cha2021co2l, barletti2022contrastive}.
Although recent CL methods based on KD have been effective at reducing forgetting, they typically adopt the $L_2$ distance for distillation, forcing the learned features to be close to their exact old values. This is too restrictive and results in CL models that are more rigid in retaining old knowledge (stronger stability), but less flexible in adapting to new tasks (weaker plasticity). Our method has a better tradeoff of stability and plasticity. 

In this paper, we pay attention to the feature space in CL and study its evolution. 
We show that a small number of principal directions explain most of the variance in feature space and only these directions are important for classification. A large number of directions in the feature space have little variance and remain unused. 
When the model is trained on new tasks, new features need to be learned along those unused directions to accommodate new classes, as illustrated in Figure~\ref{fig:teaser}. Without handling forgetting, the old principal directions, along which the old classes are linearly separable, will be forgotten. 
Our results indicate that such forgetting of learned principal directions in the feature space is an important reason for catastrophic forgetting. 

Based on this insight, as shown in Figure~\ref{fig:teaser}, we propose a Backward Feature Projection (BFP) loss, an effective feature distillation loss that enforces feature consistency up to a learnable linear transformation, not imposing exact equality of features. This transformation aims to preserve the linear separability of features backward in time.
We show that this linear projection is important because it can rotate, reflect, and scale features, while maintaining the linear separability of the previously learned classes in the new feature space. 
Projecting backward allows the features to change and new decision boundaries to be learned along the unused feature directions to classify new classes. 
BFP can be integrated into existing CL methods in a straightforward way and experiments show that this simple change boosts the performance over baselines by a large margin. 

Our experiments show that the proposed BFP regularization loss can improve the baseline methods by up to 6\%-8\% on the challenging Split-CIFAR10 and Split-CIFAR100 datasets, achieving state-of-the-art class-incremental learning accuracy. More importantly, the linear probing experiments show that BFP results in a better feature space where different classes are more separable. 
See Figure~\ref{fig:teaser} for an illustrative example. 
Our contributions are as follows:
\begin{itemize}
\vspace{-0.5em}
\itemsep0em 
    \item We provide an analysis of feature space evolution during continual learning, distinguishing the important feature components from unimportant ones. 
    \vspace{-0.2em}
    \item We propose the Backward Feature Projection (BFP) loss, which preserves the linear separability of old classes while allowing plasticity during continual learning, i.e. features are allowed to change. 
    \vspace{-0.2em}
    \item When combined with simple experience replay baselines, BFP helps learn better feature space and achieves state-of-the-art performance on challenging datasets. 
\vspace{-0.4em}
\end{itemize}


