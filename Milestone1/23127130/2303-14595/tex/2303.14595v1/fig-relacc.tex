\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\linewidth]{figures/cifar100-svd.pdf}
    \includegraphics[width=0.48\linewidth]{figures/cifar100-acc.pdf}
    \includegraphics[width=0.48\linewidth]{figures/cifar100-svd-rel.pdf}
    \includegraphics[width=0.48\linewidth]{figures/cifar100-acc-rel.pdf}
    \caption{
    Feature distribution and contribution to classification during continual learning on Split-CIFAR100 with 10 classes per task. 
    \textbf{Left}: feature variance (singular values) along each principal direction; \textbf{right}: classification accuracies proj-acc($k$) using projected features. 
    \textbf{Upper} plots show the absolute singular values and accuracies, and \textbf{lower} ones show their relative values, normalized by the maximum on each curve. 
    Finetuning (FT) is a naive CL baseline that does not handle forgetting at all and gives a lower-bound performance. Joint Training (JT) is an oracle CL method that is jointly trained on all classes seen so far and shows the upper bound. Contrasting JT with FT reveals the ideal properties for good CL methods. 
    As the model is continually trained on more classes, more feature dimensions are learned and needed for classification. However, compared to the full feature dimension (512), only a small subspace (around 10 principal directions for 10 classes and 80 for 100 classes) is crucial for CL performance, as the relative accuracies quickly saturate with the number of principal directions used. 
    }
    \label{fig:relacc}
    \vspace{-1em}
\end{figure}
