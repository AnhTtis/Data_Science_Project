
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} %

\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{algorithmic}
\usepackage{algorithm}

\usepackage{twoopt}
\usepackage{tikz}
\usetikzlibrary{fit}
\newcommand\tikzmark[1]{%
  \tikz[remember picture,overlay]\node[inner xsep=0pt] (#1) {};}

\newcommandtwoopt\TextboxReservoir[5][2.5cm][2cm]{%
\begin{tikzpicture}[remember picture,overlay]
  \coordinate (aux) at ([xshift=#1]#4);
  \node[inner ysep=5pt,yshift=0.6ex,draw=black,thick,
    fit=(#3) (aux),baseline] 
    (box) {};
\end{tikzpicture}%
}
\newcommandtwoopt\TextboxBalancedReservoir[5][2.5cm][2cm]{%
\begin{tikzpicture}[remember picture,overlay]
  \coordinate (aux) at ([xshift=#1]#4);
  \node[inner ysep=17pt,yshift=0.5ex,draw=black,thick,
    fit=(#3) (aux),baseline] 
    (box) {};
\end{tikzpicture}%
}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\usepackage[accsupp]{axessibility}  %

\newcommand{\qiao}[1]{\textcolor{cyan}{\textbf{Qiao:} #1}}
\newcommand{\qiaocr}[1]{\textcolor{red}{\textbf{Qiao:} #1}}
\newcommand{\crchange}[1]{#1}

\input{defs}

\def\cvprPaperID{8341} %
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

\title{
Preserving Linear Separability in Continual Learning \\by Backward Feature Projection
}

\author{Qiao Gu\\
University of Toronto\\
{\tt\small qgu@cs.toronto.edu}
\and
Dongsub Shim\\
LG AI Research\\
{\tt\small dongsub.shim@lgresearch.ai}
\and
Florian Shkurti\\
University of Toronto\\
{\tt\small florian@cs.toronto.edu}
}
\maketitle

\begin{abstract}
    Catastrophic forgetting has been a major challenge in continual learning, where the model needs to learn new tasks with limited or no access to data from previously seen tasks. To tackle this challenge, methods based on knowledge distillation in feature space have been proposed and shown to reduce forgetting~\cite{douillard2020podnet, dhar2019learning, kang2022class}. However, most feature distillation methods directly constrain the new features to match the old ones, overlooking the need for plasticity. To achieve a better stability-plasticity trade-off, we propose Backward Feature Projection (BFP), a method for continual learning that allows the new features to change up to a learnable linear transformation of the old features. BFP preserves the linear separability of the old classes while allowing the emergence of new feature directions to accommodate new classes. BFP can be integrated with existing experience replay methods and boost performance by a significant margin. We also demonstrate that BFP helps learn a better representation space, in which linear separability is well preserved during continual learning and linear probing achieves high classification accuracy. 
    \vspace{-1em}
\end{abstract}


\input{1-intro}

\input{2-related}

\input{3-method}

\input{4-experiment}

\input{5-conclusion}



{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}

\newpage
\appendix
\input{9-appendix}


\end{document}
