\section{Conclusion}
\label{sec:conclusion}

In this paper, we reduce catastrophic forgetting in continual learning (CL) by proposing Backward Feature Projection (BFP), a learnable feature distillation method. 
We show that during CL, despite the large dimension of the feature space, only a small number of feature directions are used for classification. Without regularization, previously learned feature directions diminish and harm linear separability, resulting in catastrophic forgetting. 
The proposed BFP helps maintain linear separability learned from old tasks while allowing new feature directions to be learned for new tasks. In this way, BFP achieves a better trade-off between plasticity and stability. 
BFP can be combined with existing experience replay methods and experiments show that it can boost performance by a significant margin. 
We also show that BFP results in a more linearly separable feature space, on which a linear classifier can recover higher accuracies. 
