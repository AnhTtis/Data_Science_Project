\input{fig-relacc}

\section{Method}
\label{sec:method}


\subsection{Setting and Notation}
\label{sec:method-setting}

In a typical continual learning setting, a model $f$ is sequentially trained on a set of tasks $\Task = \{1, 2, 3, \cdots, T\}$. 
Within each task $t$, input $x$ and the corresponding ground truth output $y$ are drawn i.i.d. from the task data distribution $\Data_t=(X_t, Y_t)$ and used to train the model. Here $X_t$ and $Y_t$ denote the set of inputs and outputs from task $t$. 
To illustrate our method, the CL model is decomposed into two parts $f_\theta(x)=g_\phi(h_\psi(x))=g_\phi\circ h_\psi(x)$ with $\theta = \{\phi, \psi\}$, where $h$, parameterized by $\psi$, is a non-linear feature extractor, mapping input image $x$ to a low-dimensional feature $z \in \RR^d$. 
The classification head $g$, parameterized by $\phi$, is a linear layer that maps the latent feature $z$ to classification logits $\logit \in \RR^c$, where $c$ is the total number of classes. 
In this paper, we mainly consider the class incremental learning (Class-IL) setting and the task incremental learning (Task-IL) setting, and the proposed method works on both settings. In these settings, $\Data_t$ contains training data from a set of classes $\mathcal{C}_t$, where $\mathcal{C}_t$ are disjoint for different task $t$. 
In Task-IL, task identifiers $t$ for each input are available during evaluation time, and thus the model can focus the decision boundaries within each task. On the contrary, Class-IL requires making a decision among all classes during inference time and thus is more challenging. 
We further denote the model after training on task $j$ as $f^j = g^j \circ h^j$, and the feature extracted by $h^j$ from a datapoint in task $i$ as $z_i^j = h^j(x), \; x\in \Data_i$. 
The set of all $z_i^j$ forms a feature matrix $Z_i^j = h^j(\Data_{i}) \in \RR^{d\times n}$, and $n$ is the number of datapoints in $\Data_i$. 
And similarly, the set of features extracted from $D_1$ to $D_i$ using $h_j$ is denoted by $Z_{1:i}^j = h_j(\Data_{1:i})$. 


\subsection{Analyzing Feature Forgetting in CL}
\label{sec:method-featureforget}


Motivated by recent work showing that representation drifts in the feature space have been a major cause for catastrophic forgetting~\cite{yu2020semantic, driscoll2022representational, caccia2022erace}, we study the evolution of feature space in CL and answer two key questions: (1) how many dimensions (principal directions) in the learned feature space are occupied by the data? And (2) how many of them are used for classification? 
We answer the first question by conducting principal component analysis (PCA)~\cite{pearson1901liii} on the feature matrix $Z_{1:t}^t$, which contains feature extracted by $h^t$ from all data seen so far $\Data_{1:t}$. Suppose its singular vector decomposition gives $Z_{1:t}^t=USV^T$, and then principal directions are the left singular vectors $U=[u_1, u_2, \cdots, u_d]$, where $u_l \in \RR^d$ are sorted by the corresponding singular values $s_l$. PCA gives the data distribution of seen tasks in the feature space and thus answers the first question. 
The second question is answered by evaluating the features projected onto a subspace spanned by the first $k$ principal directions $U_{1:k}$. 
Specifically, we define the classification accuracy of the projected features as
\begin{align}
    \projacc(k)  = \acc \left( y, g^t(U_{1:k} U^T_{1:k} z ) \right)%
\end{align}
where $k$ is the number of largest principal components used and \projacc~is computed over the testing set of task $t$. 
With a larger $k$, more information is retained in the projected feature $U_{1:k} U^T_{1:k} z$ and used for classification. The changes of \projacc~with the increase of $k$ reflect the importance of each principal direction being added.


In Figure~\ref{fig:relacc}, we plot $s_k$ and $\projacc(k)$ versus $k$ when a model has been trained on a certain number of classes during CL. We compare two simple CL methods: finetuning (FT) where the model is continually trained on the online data stream without any means to reduce catastrophic forgetting, and joint training (JT) where all the training data seen so far is used to train the network. Typically, FT serves as a naive lower bound for CL performance and JT an oracle upper bound. Contrasting FT and JT reveals the difference in feature space obtained from the worst and the ideal CL methods. 

We can see from Figure~\ref{fig:relacc}, for JT, as the network is trained on more classes, the learned features span a larger subspace and the classifier needs more principal directions to achieve good classification accuracies (high relative \projacc). This shows that during continual learning, more feature directions are needed to make new classes linearly separable in the feature space.
However, for the naive FT baseline, the number of principal directions with large variance does not increase with the number of seen classes.
This indicates feature forgetting: a poor CL method only focuses on the feature directions that are important for the current task. The feature directions for old tasks are suppressed to low variance and thus forgotten. 
On the other hand, compared to the full feature dimension $d=512$, JT accuracies still saturate with a relatively small $k=80$, which is roughly the number of classes seen so far. Other feature directions that have low variance are not used for classification, and such ``unused'' feature directions could leave room for future tasks in CL. 

Based on this insight, we argue for the benefit of preserving important feature directions for old tasks while allowing new ones to emerge for new tasks during continual learning. 
Therefore, we propose to learn a \textit{linear} transformation that projects the new feature space back to the old one and in the following section, we show it can achieve both goals. 


\subsection{Backward Feature Projection}
\label{sec:method-bfp}


We denote the feature extractor that is being trained on the current task $t$ as $h$, which may not have converged, and the converged model checkpoint at the end of the last task as $h'=h^{t-1}$. Given an input example $x$, the extracted features are denoted as $z = h(x)$ and $z' = h'(x)$. 
To preserve information in feature space such that the new feature $z$ should contain at least the information as that in $z'$, we can learn a projection function $p$ that satisfies $z' = p(z)$~\cite{fini2022self, gomez2022continually}.

In this work, we propose that a \textit{linear} transformation matrix $A$ can well preserve linear separability and suffice to reduce forgetting. Formally, we propose the Backward Feature Projection (BFP) loss in continual learning. Given a training example $x$,
\begin{align}
    L_{\text{BFP}} (x;\psi, A) =& \| Az - z' \|_2  \\
    =& \| A h_\psi(x) - h'(x) \|_2, \label{eq:bfp}
\end{align}
where we omit the bias term by adding a fixed entry of $1$ in the feature vector $z$. Here we only optimize $h_\psi$ and $A$, while we freeze $h'$ and thus omit its parameters. 

In the following, we show that the BFP loss can preserve the linear separability of old classes while allowing new classes to be classified along the unused directions in the old feature space.
Consider the extracted features from any two examples from the old classes $z'_i=h'(x_i),\;x_i\in C_1$ and $z'_j=h'(x_j),\; x_j\in C_2$, where $C_1, C_2 \in \mathcal{C}_{t-1}$. If they are learned to be linear separable at the end of task $t-1$, then there exists a vector $w$ and a threshold $b$, such that
\begin{align}
    w^T z'_{i} > b > w^T z'_{j},\;\forall i \in C_1,\; \forall j \in C_2. 
\end{align}
Then if the BFP loss in Equation~\ref{eq:bfp} is well optimized, i.e. $z' \approx Az$ with a linear transformation $A$. Then for the features extracted from $h$,
\begin{align}
    w^T A z_{i} >& b > w^T A z_{j},\;\forall i \in C_1,\; \forall j \in C_2 \\
    \Rightarrow (A^T w)^T z_{i} >& b > (A^T w)^T z_{j},\;\forall i \in C_1,\; \forall j \in C_2. 
\end{align}
Therefore the feature vectors $z$ from the old classes $C_1, C_2$ remain linearly separable along the direction $A^T w$ in the new feature space. The linear classifier $g$ is trained to find this decision boundary during CL with experience replay. 



To classify new classes, the network needs to map them to linearly separable regions in the feature space. 
The linear transformation in BFP achieves it by arranging the new classes along the ``unused'' feature directions that have low variance and thus are not occupied by the old tasks. 
Consider that the features extracted from future task $\Data_t$ using the old model $h'$ are probably not separable and mixed together. This is natural as $h'$ has not been trained on it. 
As we can see from Section~\ref{sec:method-featureforget} and Figure~\ref{fig:relacc}, there exists many principal directions with low variance, along which features from different classes are not separable, Ideally, a CL model should take up these ``unused'' feature directions to learn features that are needed to classify new classes. 
Without loss of generality, suppose before the model is trained on a new task $t$, the feature extracted from the new task $z' = h'(x)$, $x\in X_t$, are all mapped to zero along an ``unused'' feature direction $v$, i.e. $v^T z'= 0$. 
Then after learning on task $t$, the feature $z=h(x)$ from new classes $C_3, C_4\in \mathcal{C}_t$ can be learned to be separable along that feature direction $v$, 
\begin{align}\label{eq:vtz}
    v^T z_{i} > v^T z_{j},\;\forall i \in C_3,\; \forall j \in C_4. 
\end{align}
In this case, $A$ can be learned such that $v\notin \text{Col}(A)$ and thus $v^T (Az)=0$ while $v^T z\neq 0$ (satisfying Equation~\ref{eq:vtz}). In this way, the BFP loss in Equation~\ref{eq:bfp} allows the new class to be separable along $v$ and still can be minimized. 
Note that during the actual continual learning with BFP, neither $w$, $v$ nor the dimensionality of them is defined or needed. They are learned implicitly in the matrix $A$ through gradient descent optimization. $w$ and $v$ can be extracted and analyzed by PCA decomposition, but it is not required for training. 

\subsection{Loss functions}
\label{sec:method-loss}

We integrate the proposed backward feature projection method into an experience replay framework~\cite{buzzega2020dark}, where we keep a buffer $M$ storing training examples from old tasks. We focus on experience replay methods because they are simple and outperform other types of CL methods by a large margin according to a recent survey~\cite{masana2020class}.
We keep the model checkpoint at the end of the last task $f^{t-1}$ together with the online trained model $f$. 
During continual learning, the online model $f$ is trained on a batch from the online data stream of the current task $\Data_t$ using cross-entropy loss. 
\begin{align}\label{eq:loss-ce}
    L_{\text{ce}}(\Data_t; \theta) = \sum_{{x, y} \in  \Data_t} \CE (y, f_\theta(x))
\end{align}
Meanwhile, we sample another batch from $M$ for experience replay. Following~\cite{buzzega2020dark}, a cross-entropy loss and a logit distillation loss are applied on the replayed data
\begin{align}\label{eq:loss-repce}
    L_{\text{rep-ce}}(M; \theta)  =& \sum_{{x, y} \in M} \CE (y, f_\theta(x)), \\ \label{eq:loss-replogits}
    L_{\text{rep-logit}}(M; \theta) =& \sum_{{x, y} \in M} \| f_\theta(x) - f^{t-1}(x) \|_2^2.
\end{align}
And we apply our backward feature projection loss on both the online data stream $\Data_t$ and the replayed examples $M$
\begin{align} \label{eq:loss-BFP}
    L_{\text{BFP}}(\Data_t, M; \psi, A) =& \sum_{{x, y} \in \Data_t, M} \| A h_\psi(x) - h^{t-1}(x) \|_2. 
\end{align}
The total loss function used in continual learning is the weighted sum of the losses above. 
\begin{align}
\begin{split}\label{eq:total-loss}
    L(\Data_t, M;\theta, A) = L_{\text{ce}}(\Data_t;\theta) + \alpha L_{\text{rep-ce}}(M;\theta) \\  + \beta L_{\text{rep-logit}}(M;\theta) + \gamma L_{\text{BFP}}(\Data_t, M;\psi, A)
\end{split}
\end{align}
During training on task $t$, both the linear transformation $A$ and the model $f_\theta$ are optimized, and the old model checkpoint $f^{t-1}$ remains fixed. In our experiments, the matrix $A$ is randomly initialized at the beginning of each task. The pseudo-code of the proposed algorithm can be found in the~\appendixnote.




