
\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/teaser2.pdf}
    \caption{
    % We proposed a novel feature distillation technique for continual learning, which allows the feature space to transform under a learnable linear transformation constraint. The transformation projects new features back to old ones, allowing new dimensions to emerge to accommodate classification of new classes, while preserving linear separability of old classes to reduce catastrophic forgetting. 
    Feature distribution before and after training on a task in a class incremental learning experiment on MNIST, visualized by t-SNE. 
    \textbf{Left}: before training on task 2, seen classes (1,2) are learned to be separable along the horizontal axis for classification, while unseen classes (3, 4) are not separable. \textbf{Right}: after training on task 2, the new vertical axis is learned to separate new classes (3,4). 
    Based on this observation, we propose the Backward Feature Projection loss $L_{BFP}$, which allows new feature dimensions to emerge to separate new classes in feature space and also preserves the linear separability of old classes to reduce catastrophic forgetting. 
    % \qiao{Another plot showing CL without BFP - ideally old classes are cluttered together. Even we should have 3 plots: (1) shows nice separation using BFP, (2) FD shows high stability, low plasticity (3) FT shows high plasticity, low stability. Maybe based on the FT baseline without buffer. }
    % Details about this figure can be found in Appendix. 
    }
    \label{fig:teaser}
    \vspace{-1em}
\end{figure}