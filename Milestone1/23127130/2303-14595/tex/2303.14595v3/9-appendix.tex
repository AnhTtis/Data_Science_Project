\section{\crchange{Acknowledgements}}

The authors thank Jongseong Jang, Yizhou (Philip) Huang, Kevin Xie, and Nikita Dhawan for discussions and useful feedback. 

\section{Implementation Details}

\subsection{Complete Algorithm}

In Algorithm~\ref{alg:main}, we provide the pseudocode of continual learning with the proposed BFP method. Note that following~\cite{buzzega2020dark}, we sample training data points from the memory buffer for each loss independently. 
We empirically find this results in better performance than using the same set of replayed samples for all losses. 
The images without augmentation $x_o$ are pushed into the memory and replayed images are augmented on the fly. 
The classification model is trained using an SGD optimizer (\textit{sgd}) and the projection matrix $A$ is trained using an SGD+Momentum optimizer (\textit{sgdm}). 

\begin{algorithm}[b]
\small
\caption{- Continual Learning with BFP}
\label{alg:main}
\begin{algorithmic}
  \STATE {\bfseries Input:} dataset $\{\Data_1, \cdots, \Data_T\}$, parameters $\theta=\{\phi, \psi\}$, scalars $\alpha$, $\beta$ and $\gamma$, optimizer $\text{\textit{sgd}}, \text{\textit{sgdm}}$,
  \STATE $M \gets \{\}$
  \FOR {\texttt{$t$} \textbf{from} \texttt{$1$} \textbf{to} \texttt{$T$}}
      \STATE $A \gets \text{\textit{random-init}}()$
      \STATE $\text{\textit{sgdm}} \gets \text{\textit{reinit}}(\text{\textit{sgdm}})$
      \FOR{\texttt{$(x_{o},y_{o})$} \textbf{in}
          \texttt{$\Data_t$}}
          \STATE $x, y \gets \text{\textit{augment}}(x_o, y_o)$
          \STATE $L \gets \text{\textit{cross-entropy}} (y, f_\theta(x))$ \hfill\COMMENT{Eq.~\ref{eq:loss-ce}}
          
          \IF{$t>1$}
          \STATE $x, y \gets \text{\textit{augment}}(\text{\textit{sample}}(M))$
          \STATE $L_\text{rep-ce} \gets \text{\textit{cross-entropy}} (y, f_\theta(x))$ \hfill\COMMENT{Eq.~\ref{eq:loss-repce}}
          \STATE $x, y \gets \text{\textit{augment}}(\text{\textit{sample}}(M))$
          \STATE $L_\text{rep-logits} \gets \|f_\theta (x) - f_{\text{old}}(x)\|_2$  \hfill\COMMENT{Eq.~\ref{eq:loss-replogits}}
          \STATE $x, y \gets \text{\textit{augment}}(\text{\textit{sample}}(M))$
          \STATE $L_\text{BFP} \gets \| Ah_\psi(x) - h_\text{old}(x) \|_2$  \hfill\COMMENT{Eq.~\ref{eq:loss-BFP}}
          \STATE $L = L + L_\text{rep-ce} + L_\text{rep-logits} + L_\text{BFP}$  \hfill\COMMENT{Eq.~\ref{eq:total-loss}}
          \ENDIF
          
          \STATE $\theta \gets \text{\textit{sgd}} (\theta, \nabla_\theta L)$
          \STATE $A \gets \text{\textit{sgdm}} (A, \nabla_A L)$
          
          \STATE $M \gets \text{\textit{balanced-reservoir}}(M, (x_o, y_o))$ \hfill\COMMENT{Alg.~\ref{alg:balancoir}}
      \ENDFOR
      \STATE $f_\text{old} = \text{\textit{freeze}} (f_\theta)$
    \ENDFOR
\end{algorithmic}
\end{algorithm}

% \begin{algorithm*}
% \caption{- Continual Learning with BFP}
% \label{alg1}
% \vspace{-1.3em}
% \begin{multicols}{2}
% \begin{algorithmic}
%   \STATE {\bfseries Input:} dataset $\{\Data_1, \cdots, \Data_T\}$, parameters $\theta=\{\phi, \psi\}$, scalars $\alpha$, $\beta$ and $\gamma$, optimizer $\text{\textit{sgd}}, \text{\textit{sgdm}}$,
%   \STATE $M \gets \{\}$
%   \FOR {\texttt{$t$} \textbf{from} \texttt{$1$} \textbf{to} \texttt{$T$}}
%       \STATE $A \gets \text{\textit{random-init}}()$
%       \STATE $\text{\textit{sgdm}} \gets \text{\textit{reinit}}(\text{\textit{sgdm}})$
%       \FOR{\texttt{$(x_{o},y_{o})$} \textbf{in}
%           \texttt{$\Data_t$}}
%           \STATE $x, y \gets \text{\textit{augment}}(x_o, y_o)$
%           \STATE $L \gets \text{\textit{cross-entropy}} (y, f_\theta(x))$ 
%           \IF{$t>1$}
%           \STATE $x, y \gets \text{\textit{augment}}(\text{\textit{sample}}(M))$
%           \STATE $L_\text{rep-ce} \gets \text{\textit{cross-entropy}} (y, f_\theta(x))$ 
%           \STATE $x, y \gets \text{\textit{augment}}(\text{\textit{sample}}(M))$
%           \STATE $L_\text{rep-logits} \gets \|f_\theta (x) - f_{\text{old}}(x)\|_2$  
%           \STATE $x, y \gets \text{\textit{augment}}(\text{\textit{sample}}(M))$
%           \STATE $L_\text{BFP} \gets \| Ah_\psi(x) - h_\text{old}(x) \|_2$  
%           \STATE $L = L + L_\text{rep-ce} + L_\text{rep-logits} + L_\text{BFP}$  
%           \ENDIF
          
%           \STATE $\theta \gets \text{\textit{sgd}} (\theta, \nabla_\theta L)$
%           \STATE $A \gets \text{\textit{sgdm}} (A, \nabla_A L)$
          
%           \STATE $M \gets \text{\textit{balanced-reservoir}}(M, (x_o, y_o))$ 
%       \ENDFOR
%       \STATE $f_\text{old} = \text{\textit{freeze}} (f_\theta)$
%     \ENDFOR
% \end{algorithmic}
% \end{multicols}
% \vspace{-0.8em}
% \end{algorithm*}

\subsection{Class-balanced Reservoir Sampling}

We adopt the class-balanced reservoir sampling (BRS)~\cite{buzzega2021rethinking} for memory buffer management. The detail of this algorithm is described in Algorithm~\ref{alg:balancoir}. Compared to regular Reservoir Sampling (RS), BRS ensures that every class has an equal number of examples stored in the memory buffer. All experiments are incorporated with this change. Empirically we find that BRS does not bring significant changes compared to RS, but it helps to reduce variance in the results. 

\begin{algorithm}[t]
    \caption{Balanced Reservoir Sampling~\cite{buzzega2021rethinking}}
    \label{alg:balancoir}
    \begin{algorithmic}[1]
      \STATE {\bfseries Input:} replay buffer $M$, exemplar $(x,y)$, 
      \STATE \hspace{2.70em} number of seen examples $N$.
      \IF{$|M| > N$}
            \STATE $M[N]\gets (x,y)$
      \ELSE
            \STATE $j \gets \operatornamewithlimits{RandInt}([0, N])$
            \IF {$j < |M|$}
                \vspace{5.5mm}
                \STATE \tikzmark{start6}$M[j]\gets (x,y)$  \tikzmark{end6} \\ \vspace{-9.4mm} \hspace{-0.7em} \textbf{Reservoir Sampling} 
                \vspace{3.2em}
                \vspace{1mm}
                \STATE $\tilde{y} \gets \small{\argmax} \ \operatornamewithlimits{ClassCounts}(M,y)$
                \STATE\tikzmark{start7}$k\gets  \operatornamewithlimits{RandChoice}(\{\tilde{k}; M[\tilde{k}] = (x,y), y = \tilde{y}\})$ \tikzmark{end7} 
                \STATE $M[k]\gets (x,y)$ \\ \vspace{-5em} \hspace{-0.7em} \textbf{Balanced Reservoir Sampling} \vspace{4em}
                \vspace{1.2mm}
            \ENDIF
      \ENDIF
      \vspace{-1.2em}
    \end{algorithmic}
\TextboxReservoir[0]{start6}{end6}{}
\TextboxBalancedReservoir[0]{start7}{end7}{}
\end{algorithm}

% \section{Experiment details}

\subsection{Training details}

Image sizes are $32\times32$ in Split-CIFAR10 and Split-CIFAR100 and $64\times64$ in Split-TinyImageNet. 
All experiments use the same data augmentation procedure, applied on input from both the current task and the memory buffer independently. Data augmentation includes a full-size random crop with a padding of 4 pixels, a random horizontal flip, and normalization. 

For all experiments involving BFP, the optimizer for the matrix $A$ is an SGD+Momentum optimizer with a learning rate of 0.1 and momentum of 0.9. The weighting term $\gamma$ in Equation~\ref{eq:total-loss} is 1. Empirically we find that the BFP performance is not sensitive to these hyperparameters, and we use this one set of hyperparameters for BFP loss in all experiments. 

\subsection{Hyperparameters}

In this section, we list the best hyperparameters used for the compared baselines mentioned in Section~\ref{sec:baselines} and their results are reported in Table~\ref{tbl:main_results}. 
These hyperparameters are adopted from~\cite{buzzega2020dark} and~\cite{boschini2022class}, where they were selected by a hyperparameter search conducted on a held-out 10\% training set for validation. Please refer to~\cite{buzzega2020dark, boschini2022class} for further details.

\crchange{The proposed BFP only introduced a single hyperparameter $\gamma$, which is set to a constant value of $1$ throughout all experiments and does not need extra tuning. Other hyperparameters like $\alpha$ and $\beta$ are inherited from ER and DER++~\cite{buzzega2020dark} and we simply adopt} the same set of hyperparameters from~\cite{buzzega2020dark}. We do not further tune or modify them. 

% In remaining part of this section, we use $lr$ for learning rate, $mom$ for the momentum term, $wd$ for weight decay. 

\subsubsection{Split CIFAR-10}

\noindent{\bf FT}: $lr=0.1$

\noindent{\bf JT}: $lr=0.1$

\vspace{0.5em}
\noindent{\bf Buffer size = 200}

\noindent\fbox{%
    \parbox{\linewidth}{%
\noindent{\bf iCaRL}: $lr=0.1$, $wd=10^{-5}$

\noindent{\bf FDR}: $lr=0.03$, $\alpha=0.3$

\noindent{\bf LUCIR}: $\lambda_{\textrm{base}}=5$, $mom=0.9$, $k=2$, $\text{epoch}_{\text{fitting}}=20$, $lr=0.03$, $\text{lr}_\text{fitting}=0.01$, $m=0.5$

\noindent{\bf BiC}: $\tau=2$, $\text{epochs}_\text{BiC}=250$, $lr=0.03$

\noindent{\bf ER-ACE}: $lr=0.03$

\noindent{\bf ER}: $lr=0.1$

\noindent{\bf DER++}: $lr=0.03$, $\alpha=0.1$, $\beta=0.5$
    }%
}



\vspace{0.5em}
\noindent{\bf Buffer size = 500}

\noindent\fbox{%
    \parbox{\linewidth}{%
\noindent{\bf iCaRL}: $lr=0.1$, $wd=10^{-5}$

\noindent{\bf FDR}: $lr=0.03$, $\alpha=1$

\noindent{\bf LUCIR}: $\lambda_{\textrm{base}}=5$, $mom=0.9$, $k=2$, $\text{epoch}_{\text{fitting}}=20$, $lr=0.03$, $\text{lr}_\text{fitting}=0.01$, $m=0.5$

\noindent{\bf BiC}: $\tau=2$, $\text{epochs}_\text{BiC}=250$, $lr=0.03$

\noindent{\bf ER-ACE}: $lr=0.03$

\noindent{\bf ER}: $lr=0.1$

\noindent{\bf DER++}: $lr=0.03$, $\alpha=0.2$, $\beta=0.5$
    }%
}


\subsubsection{Split CIFAR-100}

\noindent{\bf FT}: $lr=0.03$

\noindent{\bf JT}: $lr=0.03$

\vspace{0.5em}
\noindent{\bf Buffer size = 500}

\noindent\fbox{%
    \parbox{\linewidth}{%
\noindent{\bf iCaRL}: $lr=0.3$, $wd=10^{-5}$

\noindent{\bf FDR}: $lr=0.03$, $\alpha=0.3$

\noindent{\bf LUCIR}: $\lambda_{\textrm{base}}=5$, $mom=0.9$, $k=2$, $\text{epoch}_{\text{fitting}}=20$, $lr=0.03$, $\text{lr}_\text{fitting}=0.01$, $m=0.5$

\noindent{\bf BiC}: $\tau=2$, $\text{epochs}_\text{BiC}=250$, $lr=0.03$

\noindent{\bf ER-ACE}: $lr=0.03$

\noindent{\bf ER}: $lr=0.1$

\noindent{\bf DER++}: $lr=0.03$, $\alpha=0.2$, $\beta=0.5$
    }%
}


\vspace{0.5em}
\noindent{\bf Buffer size = 2000}

\noindent\fbox{%
    \parbox{\linewidth}{%
\noindent{\bf iCaRL}: $lr=0.3$, $wd=10^{-5}$

\noindent{\bf FDR}: $lr=0.03$, $\alpha=1$

\noindent{\bf LUCIR}: $\lambda_{\textrm{base}}=5$, $mom=0.9$, $k=2$, $\text{epoch}_{\text{fitting}}=20$, $lr=0.03$, $\text{lr}_\text{fitting}=0.01$, $m=0.5$

\noindent{\bf BiC}: $\tau=2$, $\text{epochs}_\text{BiC}=250$, $lr=0.03$

\noindent{\bf ER-ACE}: $lr=0.03$

\noindent{\bf ER}: $lr=0.1$

\noindent{\bf DER++}: $lr=0.03$, $\alpha=0.1$, $\beta=0.5$
    }%
}

\subsubsection{Split TinyImageNet}

\noindent{\bf FT}: $lr=0.03$

\noindent{\bf JT}: $lr=0.03$

\vspace{0.5em}
\noindent{\bf Buffer size = 4000}

\noindent\fbox{%
    \parbox{\linewidth}{%
\noindent{\bf iCaRL}: $lr=0.03$, $wd=10^{-5}$

\noindent{\bf FDR}: $lr=0.03$, $\alpha=0.3$

\noindent{\bf LUCIR}: $\lambda_{\textrm{base}}$=5, $mom$=0.9, $k$=2, $\text{epoch}_{\text{fitting}}$=20, $lr$=0.03, $\text{lr}_\text{fitting}$=0.01, $m$=0.5

\noindent{\bf BiC}: $\tau=2$, $\text{epochs}_\text{BiC}=250$, $lr=0.03$

\noindent{\bf ER-ACE}: $lr=0.03$

\noindent{\bf ER}: $lr=0.1$

\noindent{\bf DER++}: $lr=0.1$, $\alpha=0.3$, $\beta=0.8$
    }%
}


\section{Additional results}

\subsection{Final Forgetting}

Final Forgetting (FF) measures the performance drop between the end of each task and the end of CL. A CL method with a lower FF has a better ability to retain knowledge during CL and thus better stability. However, higher stability may come with the price of plasticity, and we remind readers that the Final Average Accuracy (FAA) reported in Table~\ref{tbl:main_results} can better reflect the trade-off between stability and plasticity. 
The Final Forgetting for all baselines and our methods can be found in Table~\ref{tbl:main_forgetting}. As we can see from Table~\ref{tbl:main_forgetting}, in the class-IL setting, the proposed DER++ w/ BFP method helps reduce FF compared to the base DER++ method by 11\% and 12\% on S-CIFAR10 with 200 buffer size and S-CIFAR100 with 500 buffer size respectively. DER++ w/ BFP also achieves the lowest FF among all compared methods in the class-IL setting. 
Final Forgettings in the Task-IL setting are generally much lower than those from the Class-IL setting because Task-IL provides the oracle task identifiers during the testing time and thus becomes a much easier CL scenario. In this setting, the proposed BFP also brings large improvements over the base ER and DER++ methods. 

\input{tbl-ab-layer-er}

\input{tbl-forgetting}

\subsection{Ablation Study based on Experience Replay}

We conduct the same ablation study as that in Section~\ref{sec-abstudy}, on different types of the projection layer used in ER w/ BFP. The results are reported in Table~\ref{tab:ab-layer-er}. From Table~\ref{tab:ab-layer-er}, we can draw the same conclusion as in Section~\ref{sec-abstudy}. BFP uses learnable linear transformation when distilling features and thus results in better plasticity during CL compared to the simple FD method. Results show that BFP outperforms FD by a significant margin and has better performance than BFP-2 in most cases. This further shows that enforcing a linear relationship between the new and old features could better preserve linear separability and result in less forgetting in CL. 



\subsection{Linear Probing}

% \qiao{Figures similar to Figure~\ref{fig:linear-prob}, but on S-CIFAR100 (Figure~\ref{fig:linear-prob-cifar100}) and S-TinyImageNet (Running). }

We conduct the same linear probing analysis as Section~\ref{sec:linear-prob} Figure~\ref{fig:linear-prob} on Split-CIFAR100 and Split-TinyImageNet, and the results are reported in Figure~\ref{fig:linear-prob-supp}. On these two datasets, while FD and BFP result in similar linear probing performance when based on DER++, BFP still leads to better linear probing accuracies when based on FT, especially when a large subset of training data is used for linear probing. FT w/ BFP (without the memory buffer) has a similar or even better performance than DER++ (with the memory buffer). This shows that BFP help learns a better feature space from CL, where features from different class are more linearly separable. 

\subsection{Feature Similarity Analysis}

% \qiao{Figures similar to Figure~\ref{fig:cka}, but on S-CIFAR100 (Figure~\ref{fig:cka-cifar100}) and S-TinyImageNet (Running). }

We perform the same feature similarity analysis as Section~\ref{sec:feat-sim} and Figure~\ref{fig:cka} on Split-CIFAR100 and Split-TinyImageNet, and the results are reported in Figure~\ref{fig:cka-supp}. 
From Figure~\ref{fig:cka-supp}, although the curves have high variance throughout continual learning, we can see that BFP has feature similarities that are higher than the DER++ baseline but lower than the naive FD, and thus achieve a better trade-off between stability and plasticity. 

\begin{table}[htb!]
    \small
    \centering
    \begin{tabular}{l|llll}
    \hline
    Method & DER++ & w/ FD & w/ BFP & w/ BFP-2 \\ \hline
    Class-IL   & \meanstd{49.20}{1.99}  & \meanstd{51.89}{3.42} & \meanstd{\textbf{54.45}}{0.86}  & \meanstd{52.88}{1.86}    \\
    Task-IL    & \meanstd{69.01}{2.01}  & \meanstd{71.23}{2.80} & \meanstd{\textbf{72.05}}{1.04}  & \meanstd{70.56}{1.47}     \\ \hline
    \end{tabular}
    \vspace{-1.2em}
    \caption{Final Average Accuracy on ImageNet-100. (mean$\pm$std over 3 runs)}
    \vspace{-1.2em}
    \label{tab:imn100}
\end{table}

\subsection{\crchange{Experiments on Split-ImageNet100}}

To demonstrate that the proposed BFP method scales to large datasets, we conduct experiments on ImageNet100~\cite{rebuffi2017icarl, hou2019learning}. We split ImageNet100 into 10 tasks with 10 classes per task and use a memory buffer of size 2000. The model is trained for 65 epochs on each task using an SGD optimizer with an initial learning rate of 0.1 and weight decay of $5\times 10^{-4}$. Within each task, the learning rate goes through a linear warm-up scheduler for the first 5 epochs and then decays with a 0.1 rate after 15, 30, 40, and 45 epochs. 
The results are reported in Table~\ref{tab:imn100}, which shows that the proposed BFP method still gives a significant improvement (over 5\% in Class-IL setting) over the DER++ baseline, confirming our existing results. 

\subsection{\crchange{Effect of $\gamma$ on Plasticity and Stability}}

In continual learning, the weight of regularization loss controls how closely and strictly the model should resemble the old checkpoints. Therefore the weight serves as a control knob on the trade-off between stability and plasticity: with a stronger regularization loss, the model forgets old tasks less but instead has a hard time learning new tasks.

Although we did not perform an extensive hyperparameter search on $\gamma$ for individual combinations of datasets and buffer sizes, we are still interested in how the varying $\gamma$ affects the trade-off between stability and plasticity in continual learning. Therefore, we train DER++ w/ BFP on S-CIFAR10 with different $\gamma$ and report the performance in Table~\ref{tab:gamma}. 
Besides FAA and FF, we also report the Average Learning Accuracy (ALA)~\cite{riemer2019learning}, which measures the learning ability on new tasks in continual learning and thus reflects the plasticity. Using the notation from Sec.~\ref{sec:exp-setting}, ALA is defined as
\begin{align}
    ALA=\frac{1}{T} \sum_{i=1}^T a_i^i. 
\end{align}
From Table~\ref{tab:gamma}, we can see that the effect of $\gamma$ aligns with our intuition. A higher $\gamma$ poses a strong regularization on the feature space, resulting in a lower FF (more stable) but also a lower ALA (less plastic). Also, we can observe that the final performance (FAA) remains robust to the value of $\gamma$ within a considerable range. 

\section{\crchange{More Related Work}}

There has been some recent work that also employs PCA computation in continual learning. Note that the proposed BFP does not require PCA computation during training and the feature directions are learned implicitly when optimizing matrix $A$. However, to provide a complete understanding of the literature, we briefly review the related work that also uses PCA for continual learning. 

Doan~\etal~~\cite{doan2021theoretical} proposed PCA-OGD, which combines PCA analysis with Orthogonal Gradient Descent (OGD). PCA-OGD projects the gradients onto the residuals subspace to reduce the interference of gradient updates from the new tasks on the old tasks. 
Zhu~\etal~\cite{zhu2021class} decomposed the learned features during CL using PCA. They showed that feature directions with larger eigenvalues have larger similarities (corresponding angles) before and after learning a task. They proposed that these feature directions are more transferable and less forgettable. They showed that their dual augmentation method can encourage learned features to have more directions with large eigenvalues. 
GeoDL~\cite{simon2021geodl} constructs low-dimensional manifolds for the features extracted by the online model and the old checkpoints and performs knowledge distillation on the manifolds. PCA computation is explicitly conducted on the learned features for the manifold construction. 
SPACE~\cite{saha2021space} used PCA analysis for network compression and pruning in continual learning. Similar to our analysis, they use PCA to split the learned filters in a network into Core, which is important for the current task, and Residual, which can be compressed and freed up to learn future tasks. In their work, PCA computation is required during continual learning on every layer of the network in order to do pruning, This poses a significant computational overhead in CL compared to our BFP. 
Instead of applying PCA analysis in continual learning, Zhang~\etal~\cite{zhang2021monitoring} designed a modified PCA algorithm based on EWC~\cite{lee2017overcoming} so that it has continual learning ability. They aim to reduce the forgetting problem in monitoring multimode processes. 

\begin{table}[htb!]
    \centering
    \begin{tabular}{l|lllll}
    \hline
        $\gamma$&0.1&0.3&1.0&3.0&10.0 \\ \hline
        FAA&74.56&75.77&76.68&76.00&73.54 \\
        FF&16.11&14.63&13.16&13.07&12.69 \\
        ALA&87.45&87.32&87.21&86.45&83.70 \\
    \hline
    \end{tabular}
    \caption{Results on CIFAR10 (buffer size 500) with different $\gamma$. }
    \label{tab:gamma}
\end{table}

\begin{figure}[b!]
    \centering
    \includegraphics[width=\linewidth]{figures/lp_CIFAR100.pdf}
    \includegraphics[width=\linewidth]{figures/lp_Tinyimg.pdf}
    \caption{Linear probing accuracies on the fixed feature extractor obtained after training on Split-CIFAR100 (top) and TinyImageNet (bottom). DER++ and its variants use a buffer size of 500 for CIFAR100 and 4000 for TinyImageNet.}
    \label{fig:linear-prob-supp}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/cka_cifar100.pdf}
    \includegraphics[width=\linewidth]{figures/cka_tinyimg.pdf}
    \caption{Feature similarity at different tasks of training on Split-CIFAR100 with buffer size 500 (top) and Split-TinyImageNet with buffer size 4000 (bottom), using different CL methods.}
    \label{fig:cka-supp}
\end{figure}


