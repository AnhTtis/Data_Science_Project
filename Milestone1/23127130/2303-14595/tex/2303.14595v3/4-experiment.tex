\section{Experiments}
\label{sec:exp}

\input{tbl-main}

\subsection{Experimental Setting}
\label{sec:exp-setting}

{\bf Continual Learning Settings}. 
We follow \cite{chaudhry2019continual, wu2019large, buzzega2020dark} and test all methods using both the Class-IL and Task-IL settings in our CL experiments. 
Both Class-IL and Task-IL split the dataset into a sequence of tasks, each containing a disjoint set of classes, while task identifiers are available during testing under Task-IL. 
% In the Task-IL setting, the task identifier for each example is provided in both training and testing time, while in Class-IL, it's only available for training. 
Task-IL thus has extra advantages during inference (e.g. select proper prediction head) and becomes an easier CL scenario. 
Our work is designed for Class-IL and its Task-IL performance is obtained by only considering the logits within the ground truth task. 
% Yet we still report the Task-IL accuracies by considering the classification logits within each task. 

{\bf Datasets}.
We evaluate baselines and our methods on the following datasets using varying buffer sizes:
{\bf Split CIFAR-10} divides the original CIFAR-10~\cite{krizhevsky2009learning} dataset into 5 tasks, with each task composed of 2 classes. Each class includes 5000 training and 1000 testing images of shape 32$\times$32.
{\bf Split CIFAR-100} divides CIFAR-100~\cite{krizhevsky2009learning} into 10 tasks, with 10 classes per task. Each class has 500 training and 100 testing images of shape 32$\times$32. 
{\bf Split TinyImageNet} splits TinyImageNet~\cite{tinyIM} into 10 tasks, with 20 classes per task. Each class contains 500 training images, 50 validation images, and 50 testing images. 
% \begin{itemize}
%     \itemsep0em 
%     \item {\bf Split CIFAR-10} divides the original CIFAR-10~\cite{krizhevsky2009learning} dataset into 5 tasks, with each task composed of 2 classes. Each class includes 5000 training and 1000 testing images of shape 32$\times$32.
%     \item {\bf Split CIFAR-100} divides CIFAR-100~\cite{krizhevsky2009learning} into 10 tasks, with 10 classes per task. Each class has 500 training and 100 testing images of shape 32$\times$32. 
%     \item {\bf Split TinyImageNet} splits TinyImageNet~\cite{tinyIM} into 10 tasks, with 20 classes per task. Each class contains 500 training images, 50 validation images, and 50 testing images. 
% \end{itemize}
These datasets are challenging and state-of-the-art continual learning methods still fall far behind the Joint Training (JT) baseline, especially in the Class-IL setting as shown in Table~\ref{tbl:main_results}. 

{\bf Metrics}. Following~\cite{buzzega2020dark, masana2020class, boschini2022class, arani2022learning}, we report the performance of each compared method using Final Average Accuracy (FAA). Suppose $a_i^t$ is the testing classification accuracy on the $i^{\text{th}}$ task when the training finishes on task $t$, FAA is the accuracy of the final model averaged all tasks:
\begin{align}
    FAA = \frac{1}{T}\sum_{i=1}^{T} a_i^T. 
\end{align}

We also report the Final Forgetting (FF), which reflects the accuracy drop between the peak performance on one task and its final performance:
\begin{align}
    FF = \frac{1}{T-1}\sum_{i=1}^{T-1} \max_{j \in {1, \cdots, T-1}} (a_i^j - a_i^{T}).
\end{align}
Lower FF means less forgetting and better CL performance. 
% \qiao{Maybe put the table of forgetting in the appendix because otherwise table 1 could be very crowded. Or we can remove the standard deviation part. }


{\bf Training details}.
We use ResNet-18~\cite{he2016deep} as the network backbone, and instead of the simple reservoir buffer used in~\cite{buzzega2020dark}, we use class-balanced reservoir sampling~\cite{buzzega2021rethinking} for pushing examples into the buffer. 
All the baselines we compare are updated with this change.
% ~\qiao{We can also switch back if we found some baselines hard to replicate. }
We use an SGD optimizer to optimize the model $f_\theta$ and another SGD+Momentum optimizer with a learning rate $0.1$ for the projection matrix $A$. 
The optimizers and the matrix $A$ are re-initialized at the beginning of each task. 
The network is trained for 50 epochs per task for Split CIFAR-10 and Split CIFAR-100 and 100 epochs per task for Split TinyImageNet. The learning rate is divided by 10 after a certain number of epochs within each task ($[35, 45]$ for Split CIFAR-100 and $[35, 60, 75]$ for Split TinyImageNet). 
In this work, we focus on this offline CL setting where each task is trained for multiple epochs. Although we are also interested in online CL, multi-epoch training helps disentangle underfitting and catastrophic forgetting~\cite{buzzega2020dark, arani2022learning}. 
BFP introduces only one extra hyperparameter $\gamma$, which is set to 1 for all experiments. We found that $\gamma=1$ works well for all datasets and buffer sizes and did not perform hyperparameter searches for individual experiment settings. Hyperparameters $\alpha$ and $\beta$ used in Equation~\ref{eq:total-loss} are adopted from~\cite{buzzega2020dark}.
% We use $\gamma=1$ for all experiments and adopt $\alpha$ and $\beta$ from the original DER++ paper~\cite{buzzega2020dark}.
Most baselines adopt different hyperparameters for different settings, for which we adopt the hyperparameters that have been optimized by grid search by~\cite{buzzega2020dark} and \cite{boschini2022class} for a fair comparison. The details can be found in the~\appendixnote. 

% \begin{figure*}[ht!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/teaser4.pdf}
%     \caption{Visualization of feature distribution before and after learning on Task 2. \qiao{This is probably the best plot I can get for feature visualization. How do you feel about this plot? Is this hard to comprehend? Should we keep or remove it, or push it to the appendix? } }
%     \label{fig:feat-dist}
% \end{figure*}

\subsection{Baselines}
\label{sec:baselines}

% \qiao{I am also trying on CLS-ER~\cite{arani2022learning}. }

First, we evaluate the performance of \textbf{Joint Training (JT)} and \textbf{Finetuning (FT)} baselines on each dataset. JT trains the network on all training data, does not have the forgetting problem, and thus indicates the upper-bound performance of CL methods. On the contrary, FT simply performs SGD using the current task data without handling forgetting at all and indicates a lower-bound performance. 

As a feature distillation method, our method can be combined with most continual learning methods. In our evaluation, we test our method by combing it with two popular experience replay methods, ER~\cite{riemer2019learning} and DER++~\cite{buzzega2020dark}. \textbf{ER} uses a memory buffer to store the training examples from past tasks and interleaves them with the current task data for training. 
In addition to this, \textbf{DER++} records the output logits of the examples in the memory and performs logit distillation when doing experience replay. We combine the proposed BFP loss with ER and DER++ and denote them as \textbf{ER w/ BFP} and \textbf{DER++ w/ BFP} respectively. 

We also compare the proposed method with some other state-of-the-art CL baselines as listed in Table~\ref{tbl:main_results}. 
{\bf Incremental Classifier and Presentation Learning (iCaRL)}~\cite{rebuffi2017icarl} performs classification using the nearest mean-of-exemplars, where the exemplars selected by herding algorithm in the feature space. 
{\bf Functional Distance Regularization (FDR)}~\cite{benjamin2018fdr} regularize the output of the network to its past value. Different from DER/DER++, FDR applies the regularization on the output classification probability. 
{\bf Learning a Unified Classifier Incrementally via Rebalancing (LUCIR)}~\cite{hou2019learning} augments experience replay with multiple modifications to preserve old knowledge and enforce separation class separation in continual learning. 
{\bf Bias Correction (BiC)}~\cite{wu2019large} augments the experience replay by learning a separate layer to correct the bias in the output logits. 
{\bf ER with Asymmetric Cross-Entropy (ER-ACE)}~\cite{caccia2022erace} proposes to reduce representation drift by using separate cross-entropy loss for online and replayed training data. 
% \begin{itemize}
%     \itemsep-0.3em 
%     \item {\bf Incremental Classifier and Presentation Learning (iCaRL)}~\cite{rebuffi2017icarl} performs classification using the nearest mean-of-exemplars, where the exemplars are stored in the buffer and selected by herding algorithm in the feature space. 
%     \item {\bf Functional Distance Regularization (FDR)}~\cite{benjamin2018fdr} regularize the output of the network to its past value. Different from DER/DER++, FDR applies the regularization on the output classification probability. 
%     \item {\bf Learning a Unified Classifier Incrementally via Rebalancing (LUCIR)}~\cite{hou2019learning} augments experience replay with multiple modifications to preserve old knowledge and enforce separation class separation in continual learning. 
%     \item {\bf Bias Correction (BiC)}~\cite{wu2019large} augments the experience replay by learning a separate layer to correct the data recency bias in the output logits. 
%     \item{\bf ER with Asymmetric Cross-Etronpy (ER-ACE)}~\cite{caccia2022erace} proposes to reduce representation drift by using separate cross-entropy loss for online and replayed data examples in experience replay. 
% \end{itemize}

\subsection{Results}

The Final Average Accuracies in the Class-IL and Task-IL settings are reported in Table~\ref{tbl:main_results}. The corresponding table for Final Forgetting can be found in the~\appendixnote. 
We test the methods on three datasets with various sizes of memory buffers. 
Experiments are averaged over 5 runs with different seeds and mean and standard deviation are reported. 
First, we observe that there is a still big gap between the current best CL methods and the JT oracle baselines on all datasets, especially in the Class-IL setting, which indicates that CL is still an unsolved and challenging problem. 
Comparing DER++ and DER++ w/ BFP, we can see that BFP boosts the Class-IL accuracies by a significant margin, especially with a small buffer size (6.8\% on S-CIFAR10 with buffer size 200 and 8.5\% on S-CIFAR100 with buffer size 500). 
DER++ w/ BFP thus outperforms all baseline methods in the Class-IL setting, which are very challenging as the final model needs to distinguish testing examples from all seen classes. Previous CL methods struggle to have satisfactory performance in this setting. 
Under the Task-IL setting that is easier because task identifiers are known during evaluation time, our model also helps achieve much higher accuracies over the base ER or DER++ method. And among all the CL methods compared, the proposed method also achieves the best or close-to-best accuracies under the Task-IL setting. 

% and we can see that BFP can boost the performance by a large margin, compared to ER and DER++ methods that BFP is based on. 
% \qiao{To be completed with more baseline numbers ready. }

\subsection{Linear Probing}
\label{sec:linear-prob}

% \qiao{Linear probing accuracies using the learned feature space, using a different number of data points for linear probing. }

Some latest work on continual learning studied catastrophic forgetting in the final feature space $h^T(x)$~\cite{davari2022probing, zhang2022feature}. 
They show that although the accuracy using the continually trained classifier $g^T$ degrades heavily due to catastrophic forgetting, the learned knowledge in $h^T$ is well maintained. 
This is shown by fitting a linear classifier $g^*$ on top of the frozen feature extractor at the end of continual learning $h^T$. 
Such \textit{linear probing accuracies} obtained by $g^*\circ h^T$ can be much higher than $g^T \circ h^T$. Therefore recent work argues that catastrophic forgetting mainly happens at the last linear classification layer and the linear probing accuracies can be used as a quality measure for the representation learned from continual learning~\cite{fini2022self}. 
% The recovered accuracies by linear probing (\textbf{prob accuracy}) can be much higher than those using the continually trained classifier (\textbf{observed accuracy}). 
% Therefore the prob accuracy has been used as a quality measure of the representation learned from continual learning~\cite{fini2022self}. 
% This holds even without a carefully designed CL method and the difference among ``prob'' accuracies using different CL methods is much smaller than the ``observed'' ones. 
We conduct a similar linear probing analysis on baselines combined with the BFP, and we additionally test the effect of our method on the naive FT baseline, which is denoted as \textbf{FT w/ BFP} in Figure~\ref{fig:linear-prob}. 
In FT w/ BFP, we do not use the memory buffer and thus $\alpha=\beta=0$ in Equation~\ref{eq:bfp}, but we apply the BFP loss on the online data stream with $\gamma=1$. 
% Therefore the loss function for FT w/ BFP becomes
% \begin{align}
%     L(\Data_t;\theta) = L_{ce}(\Data_t;\theta) + \gamma L_{\text{BFP}}(\Data_t;\psi, A). 
% \end{align}
Linear probing accuracies on Split CIFAR-10 are reported in Figure~\ref{fig:linear-prob}, where we also vary the portion of training data used for linear probing. 
The results show that BFP boosts the linear probing accuracies of the FT baseline by a significant margin, achieving a similar performance with the powerful experience replay method, DER++. When combined with DER++, BFP also helps improve the linear probing accuracies. 
% This experiment shows that BFP helps learn a better feature space with less forgetting, where features from different classes remain linearly separable throughout continual learning and a linear classifier can be learned to recover high accuracies. 
This indicates that either with or without a memory buffer, BFP helps learn a better feature space during CL, where examples from different classes remain linearly separable.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/lp_CIFAR10.pdf}
    \caption{Linear probing accuracies on the frozen feature extractor that is obtained after training on Split-CIFAR10 with 200 buffer size, using different methods. A higher linear probing accuracy indicates a better feature space where data points are more linearly separable. Note that with the help of BFP, even a simple FT baseline can learn a representation as good as the powerful DER++ method. }
    \label{fig:linear-prob}
    \vspace{-1em}
\end{figure}


% Note that the observed accuracies for both FT and FT w/ BFP are around 9\%, which means that the 

% In their experiments, a large amount of data is used for linear probing (10\% or even the full dataset~\qiao{Still need examples and confirmations}). 


% However, in the practical scenario of continual learning, we often only consider a memory buffer of very limited size and cannot afford to store a large amount of data. In our experiments, we show that even linear probing of a random network can achieve very high accuracy using a large amount of data. 
% What really matters should be the linear probing accuracies using a small amount of data on a continually trained feature extractor, and The quality of the learned representation does make a difference in that regime. 

% \qiao{Seems~\cite{davari2022probing} used all the training data for linear probing? - Asking the authors for confirmation. }


% However, we will show that features indeed forget during continual learning, in the sense that variance reduces along prominent directions, which are crucial for the classification of the old classes. 

\subsection{Ablation Study}
\label{sec-abstudy}

\input{tbl-ab-layer}

We study the effect of different types of projection layers used for backward feature projection, in Equation~\ref{eq:bfp}. 
Our main results are obtained using a linear projection layer as the learnable transformation layer (denoted as \textbf{BFP}). We also test our method using an identity function $A=I$ as the projector, which is essentially a feature distillation loss (\textbf{FD}) on the final feature, as well as using a non-linear function (a two-layer MLP with ReLU activation in between) as $p$, which is denoted as \textbf{BFP-2}. 
These variations are tested when integrated with DER++~\cite{buzzega2020dark} and the results are shown in Table~\ref{tab:ab-layer}. 
According to Table~\ref{tab:ab-layer}, while the simple FD method already outperforms the baseline, the proposed learnable BFP further boosts the accuracies by a large margin. 
% This indicates that a ``soft'' feature regularization method in continual learning is important for allowing new knowledge to be learned while retaining old knowledge. 
% This indicates that compared to a direct FD loss, the proposed BFP method gives for some softness in feature distillation. 
% This reveals a major limitation of the simple FD loss in continual learning - 
% FD regularizes the learned features directly to its old version that may not have been seen or learned. 
This is expected because FD regularizes the learned features directly to those from the old model, while the old model has not learned from the new data and may give useless features. 
In this case, FD promotes stability while lacking plasticity. On the contrary, BFP is learnable and thus provides the desired plasticity that allows new knowledge to appear while maintaining the old ones. 
Furthermore, we can also see that the performance already saturates with a linear projection layer and a more complex non-linear projection (BFP-2) does not improve further. We hypothesize that because BFP is applied on the feature space just before the linear classifier, linear separability is better maintained with a linear transformation rather than a non-linear function. 

\subsection{Feature Similarity Analysis}
\label{sec:feat-sim}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/cka_cifar10.pdf}
    \caption{Feature similarity at different tasks of training on Split-CIFAR10, with 200 buffer size, using different CL methods.}
    \label{fig:cka}
    \vspace{-1em}
\end{figure}

To demonstrate that the proposed BFP method regularizes the features that are already learned while allowing features of new data to freely evolve, we conduct an analysis of feature similarity. 
Following~\cite{ramasesh2020anatomy, davari2022probing}, we adopt Centered Kernal Alignment (CKA)~\cite{kornblith2019similarity} to measure the feature similarity before and after training on a task. CKA is a similarity measure for deep learned representations, and it's invariant to isotropic scaling and orthogonal transformation~\cite{kornblith2019similarity}.
CKA between two feature matrices $Z_1\in \RR^{d_1\times n}$ and $Z_2\in \RR^{d_2\times n}$ with a linear kernel is defined as 
\begin{align}
    \cka(Z_1, Z_2) = \frac{\| Z_1 Z_2^T \|_F^2}{\|Z_1 Z_1^T\|_F^2 \|Z_2 Z_2^T\|_F^2}.
\end{align}
Recall that the feature matrix extracted from $\Data_i$ using model $h^j$ is denoted as $Z_i^j = h^j(\Data_i)$, and similar $Z_{1:i}^j = h^i(\Data_{1:i})$. 
During learning on task $t$, we consider two sets of features, features from $\Data_{1:t-1}$ that have been learned by the model (seen) and features from $\Data_{t}$ that are new to the model (unseen). We define their CKA similarity before and after learning on task $t$ respectively as follows
\begin{align}
    \cka_{t}^\text{seen} =& \; \cka(Z_{1:t-1}^{t-1}, Z_{1:t-1}^{t}) \\
    \cka_{t}^\text{unseen} =& \; \cka(Z_{t}^{t-1}, Z_{t}^{t}). 
\end{align}

Note that $Z_{t}^{t-1}$ represents the features extracted from future data by the old model, and it's expected that they do not provide useful information. On the contrary, $Z_{1:t-1}^{t-1}$ has already been well learned and we want to preserve its structure. 
Therefore, we want $\cka_{t}^\text{seen}$ to be high to retain knowledge, while $\cka_{t}^\text{unseen}$ low to allow the feature of unseen data to change freely in continual learning. 
We plot $\cka_{t}^\text{seen}$ and $\cka_{t}^\text{unseen}$ during CL in Figure~\ref{fig:cka} and the results confirm our desire. 
DER++ applies no direct constraint on the feature space during CL and thus similarity is low for both seen and unseen data. 
On the contrary, FD poses a strong constraint on both seen and unseen data, resulting in high similarities. In this way, FD gains more stability at the cost of lower plasticity. 
Combining their respective advantages, BFP keeps a high $\cka_{t}^\text{seen}$ while allowing the unseen features to change (low $\cka_{t}^\text{unseen}$), and thus is able to achieve a better trade-off between stability and plasticity.
% \qiao{Write more after we finalize the figure. }

% \qiao{Analysis of the learned feature using CKA or other feature similarity metrics.}
% \qiao{Several related work that used CKA (originally proposed in~\cite{kornblith2019similarity}) in continual learning: \cite{ramasesh2020anatomy}, ~\cite{davari2022probing}}


