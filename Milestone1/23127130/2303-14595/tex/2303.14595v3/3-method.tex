\input{fig-relacc}

\section{Method}
\label{sec:method}

% We propose a novel continual learning algorithm by integrating a learnable feature projector into knowledge distillation. In this section, we first define CL setting and notations in Section~\ref{sec:method-setting} and then provide an analysis of feature space evolution in Section~\ref{sec:method-featureforget}. The proposed BFP method is described in Section~\ref{sec:method-bfp} and the overall loss function is defined in Section~\ref{sec:method-loss}. 

\subsection{Setting and Notation}
\label{sec:method-setting}

In a typical continual learning setting, a model $f$ is sequentially trained on a set of tasks $\Task = \{1, 2, 3, \cdots, T\}$. 
Within each task $t$, input $x$ and the corresponding ground truth output $y$ are drawn i.i.d. from the task data distribution $\Data_t=(X_t, Y_t)$ and used to train the model. Here $X_t$ and $Y_t$ denote the set of inputs and outputs from task $t$. 
To illustrate our method, the CL model is decomposed into two parts $f_\theta(x)=g_\phi(h_\psi(x))=g_\phi\circ h_\psi(x)$ with $\theta = \{\phi, \psi\}$, where $h$, parameterized by $\psi$, is a non-linear feature extractor, mapping input image $x$ to a low-dimensional feature $z \in \RR^d$. 
The classification head $g$, parameterized by $\phi$, is a linear layer that maps the latent feature $z$ to classification logits $\logit \in \RR^c$, where $c$ is the total number of classes. 
In this paper, we mainly consider the class incremental learning (Class-IL) setting and the task incremental learning (Task-IL) setting, and the proposed method works on both settings. In these settings, $\Data_t$ contains training data from a set of classes $\mathcal{C}_t$, where $\mathcal{C}_t$ are disjoint for different task $t$. 
% where each task $t$ contains input from a set of classes $\mathcal{C}_t$ and $\mathcal{C}_t$ are disjoint for different tasks. 
In Task-IL, task identifiers $t$ for each input are available during evaluation time, and thus the model can focus the decision boundaries within each task. On the contrary, Class-IL requires making a decision among all classes during inference time and thus is more challenging. 
We further denote the model after training on task $j$ as $f^j = g^j \circ h^j$, and the feature extracted by $h^j$ from a datapoint in task $i$ as $z_i^j = h^j(x), \; x\in \Data_i$. 
The set of all $z_i^j$ forms a feature matrix $Z_i^j = h^j(\Data_{i}) \in \RR^{d\times n}$, and $n$ is the number of datapoints in $\Data_i$. 
And similarly, the set of features extracted from $D_1$ to $D_i$ using $h_j$ is denoted by $Z_{1:i}^j = h_j(\Data_{1:i})$. 
% \qiao{Offline continual learning. reasoning about underfitting. }


\subsection{Analyzing Feature Forgetting in CL}
\label{sec:method-featureforget}

% Our method is motivated by catastrophic forgetting of the learned feature space in continual learning. 
% Recent works have reported that a major cause of catastrophic forgetting is the abrupt drift in the feature space under incremental training. 
% Recent work~\cite{davari2022probing, zhang2022feature} argues that while the classification accuracy degrades heavily due to catastrophic forgetting, the learned representation space is good at retaining knowledge. They show this by fitting a linear classifier on top of a frozen feature extractor, which can ``recover'' a very high accuracy, even if the feature extractor is continually trained without special care for forgetting. 
% However, we will show that features indeed forget during continual learning, in the sense that variance reduces along prominent directions, which are crucial for the classification of the old classes. 

Motivated by recent work showing that representation drifts in the feature space have been a major cause for catastrophic forgetting~\cite{yu2020semantic, driscoll2022representational, caccia2022erace}, we study the evolution of feature space in CL and answer two key questions: (1) how many dimensions (principal directions) in the learned feature space are occupied by the data? And (2) how many of them are used for classification? 
We answer the first question by conducting principal component analysis (PCA)~\cite{pearson1901liii} on the feature matrix $Z_{1:t}^t$, which contains feature extracted by $h^t$ from all data seen so far $\Data_{1:t}$. Suppose its singular vector decomposition gives $Z_{1:t}^t=USV^T$, and then principal directions are the left singular vectors $U=[u_1, u_2, \cdots, u_d]$, where $u_l \in \RR^d$ are sorted by the corresponding singular values $s_l$. PCA gives the data distribution of seen tasks in the feature space and thus answers the first question. 
The second question is answered by evaluating the features projected onto a subspace spanned by the first $k$ principal directions $U_{1:k}$. 
% Then the projected features are used to re-compute classification accuracies. 
% Specifically, we compute the accuracy relative to the accuracy using the full feature space as follows
% \begin{align}
%     \relacc(k)  = \acc \left( y, g(U_{1:k} U^T_{1:k} z ) \right) 
%     / \acc \left( y, g( z ) \right), \; z \in Z_{1:i}^i
% \end{align}
Specifically, we define the classification accuracy of the projected features as
\begin{align}
    \projacc(k)  = \acc \left( y, g^t(U_{1:k} U^T_{1:k} z ) \right)% , \; z \in Z_{1:t}^t
\end{align}
where $k$ is the number of largest principal components used and \projacc~is computed over the testing set of task $t$. 
With a larger $k$, more information is retained in the projected feature $U_{1:k} U^T_{1:k} z$ and used for classification. The changes of \projacc~with the increase of $k$ reflect the importance of each principal direction being added.
% How fast \projacc~increases with $k$ reflects how important each principal direction is for classification. 
% If only a few feature dimensions matter for classification, then \projacc~should quickly saturate with a small number $k$. 

% This gives a principled way of studying which feature dimensions are important for any classification task, and in the continual learning scenario, this provides guidance for feature distillation. 
% \qiao{Trying the following thing with simple finetuning CL baseline. }

% In Figure~\ref{fig:relacc}, we show the change of $\projacc(k)$ with varying $k$ after the model is trained on different numbers of classes. This figure is obtained from a ResNet-18~\cite{he2016deep} incrementally trained on Split-CIFAR100 dataset~\cite{krizhevsky2009learning} with simple experience replay.
In Figure~\ref{fig:relacc}, we plot $s_k$ and $\projacc(k)$ versus $k$ when a model has been trained on a certain number of classes during CL. We compare two simple CL methods: finetuning (FT) where the model is continually trained on the online data stream without any means to reduce catastrophic forgetting, and joint training (JT) where all the training data seen so far is used to train the network. Typically, FT serves as a naive lower bound for CL performance and JT an oracle upper bound. Contrasting FT and JT reveals the difference in feature space obtained from the worst and the ideal CL methods. 
% The results in Figure~\ref{fig:relacc} is obtained by continually training a ResNet-18 on Split-CIFAR100. 
% The absolute values are shown in the upper plots and the relative values are shown in the lower ones.

We can see from Figure~\ref{fig:relacc}, for JT, as the network is trained on more classes, the learned features span a larger subspace and the classifier needs more principal directions to achieve good classification accuracies (high relative \projacc). This shows that during continual learning, more feature directions are needed to make new classes linearly separable in the feature space.
% However, for the naive FT baseline, the variance of the learned feature is centered on a smaller number of directions even after the model has seen many classes sequentially. 
However, for the naive FT baseline, the number of principal directions with large variance does not increase with the number of seen classes.
This indicates feature forgetting: a poor CL method only focuses on the feature directions that are important for the current task. The feature directions for old tasks are suppressed to low variance and thus forgotten. 
On the other hand, compared to the full feature dimension $d=512$, JT accuracies still saturate with a relatively small $k=80$, which is roughly the number of classes seen so far. Other feature directions that have low variance are not used for classification, and such ``unused'' feature directions could leave room for future tasks in CL. 
% showing that only a few feature directions with large variances are important for classification. 
% More details of this analysis can be found in the~\appendixnote. 

Based on this insight, we argue for the benefit of preserving important feature directions for old tasks while allowing new ones to emerge for new tasks during continual learning. 
Therefore, we propose to learn a \textit{linear} transformation that projects the new feature space back to the old one and in the following section, we show it can achieve both goals. 

% The goal of continual learning is for the model to correctly classify all tasks that have been seen so far. 

\subsection{Backward Feature Projection}
\label{sec:method-bfp}

% \qiao{Ideally, we should motivate our method by maintaining linear separability in the feature space in continual learning. This needs to be done in two parts: (1) What was learned to be linear separable should remain linear separable under a backward linear transformation and (2) New decision boundaries among new classes can still be created in the new task under such linear transformation. }

We denote the feature extractor that is being trained on the current task $t$ as $h$, which may not have converged, and the converged model checkpoint at the end of the last task as $h'=h^{t-1}$. Given an input example $x$, the extracted features are denoted as $z = h(x)$ and $z' = h'(x)$. 
To preserve information in feature space such that the new feature $z$ should contain at least the information as that in $z'$, we can learn a projection function $p$ that satisfies $z' = p(z)$~\cite{fini2022self, gomez2022continually}.
% This enforces that the new feature can be transformed to recover the old feature and thus all information in the old features should be preserved in the new ones. 
% Some previous work~\cite{fini2022self, gomez2022continually} has explored the usage of a projection function $p$ that satisfies $z' = p(z)$. The intuition is that if the new feature can be transformed to recover the old feature, then all information in the old features should be preserved in the new ones. 
% To preserve information in feature space such that the new feature $z$ should contain at least the information as that in $z'$, we can learn a projection function $p$ that satisfies $z' = p(z)$. 
% This enforces that the new feature can be transformed to recover the old feature and thus all information in the old features should be preserved in the new ones. 
% This idea was explored in~\cite{fini2022self, gomez2022continually}, where they used the predictor head in self-supervised continual learning. 
% However, motivated by SimSiam~\cite{chen2021simsiam}, their idea was only integrated with a contrastive learning framework and they used a nonlinear mapping function, while in this work, we use a learnable linear transformation for $p$ and formulate it as a simple knowledge distillation loss in the feature space. No augmentation or contrastive learning is needed for our method. 
% We also demonstrate that our methods enforce linear separability in feature space during continual learning. 

In this work, we propose that a \textit{linear} transformation matrix $A$ can well preserve linear separability and suffice to reduce forgetting. Formally, we propose the Backward Feature Projection (BFP) loss in continual learning. Given a training example $x$,
\begin{align}
    L_{\text{BFP}} (x;\psi, A) =& \| Az - z' \|_2  \\
    =& \| A h_\psi(x) - h'(x) \|_2, \label{eq:bfp}
\end{align}
where we omit the bias term by adding a fixed entry of $1$ in the feature vector $z$. Here we only optimize $h_\psi$ and $A$, while we freeze $h'$ and thus omit its parameters. 
% \qiao{Please take a look at the arguments below. They are new. }

In the following, we show that the BFP loss can preserve the linear separability of old classes while allowing new classes to be classified along the unused directions in the old feature space.
% \xspace~When Eq.~\ref{eq:bfp} is well optimized, not only $z$ contains at least the same amount of information as $z'$, but also we show if $z'$ is linearly separable among old classes, such linear separability also holds for $z$ during continual learning. 
% \qiao{Definition of linear separability - The following part is still under construction. }
% \qiao{Thm: Linear projection backward from new features to old features preserve linear separability. }
% \qiao{State that new features can still be learned along new dimensions in new tasks. }
Consider the extracted features from any two examples from the old classes $z'_i=h'(x_i),\;x_i\in C_1$ and $z'_j=h'(x_j),\; x_j\in C_2$, where $C_1, C_2 \in \mathcal{C}_{t-1}$. If they are learned to be linear separable at the end of task $t-1$, then there exists a vector $w$ and a threshold $b$, such that
\begin{align}
    w^T z'_{i} > b > w^T z'_{j},\;\forall i \in C_1,\; \forall j \in C_2. 
\end{align}
Then if the BFP loss in Equation~\ref{eq:bfp} is well optimized, i.e. $z' \approx Az$ with a linear transformation $A$. Then for the features extracted from $h$,
\begin{align}
    w^T A z_{i} >& b > w^T A z_{j},\;\forall i \in C_1,\; \forall j \in C_2 \\
    \Rightarrow (A^T w)^T z_{i} >& b > (A^T w)^T z_{j},\;\forall i \in C_1,\; \forall j \in C_2. 
\end{align}
Therefore the feature vectors $z$ from the old classes $C_1, C_2$ remain linearly separable along the direction $A^T w$ in the new feature space. The linear classifier $g$ is trained to find this decision boundary during CL with experience replay. 

% \qiao{Which version looks better?}

% \qiao{Version 1:}
% On the other hand, the features extracted from future classes $\mathcal{C}_3, \mathcal{C}_4 \in \Data_t$ using the old model $h_{t-1}$ are probably not linearly separable and mixed together (this is natural as $h_{t-1}$ has not been trained on these classes). 
% Then new features $z$ for $\mathcal{C}_3, \mathcal{C}_4 \in \Data_t$ can be learned to be linearly separable along a features direction $w'$, which lies in the null space of $A$, $w'\in \text{Null}(A)$. 

% \qiao{Version 2:}
% On the other hand, BFP allows the feature to expand along new feature directions to accommodate new classes. 
To classify new classes, the network needs to map them to linearly separable regions in the feature space. 
The linear transformation in BFP achieves it by arranging the new classes along the ``unused'' feature directions that have low variance and thus are not occupied by the old tasks. 
% without affecting the existing feature directions that are learned for old classes. 
Consider that the features extracted from future task $\Data_t$ using the old model $h'$ are probably not separable and mixed together. This is natural as $h'$ has not been trained on it. 
% As we can see from Section~\ref{sec:method-featureforget} and Figure~\ref{fig:relacc}, a small number of principal directions take up the major portion of the variance in the learned feature space. Along other low-variance principal directions, features are close to each other and indistinguishable. 
As we can see from Section~\ref{sec:method-featureforget} and Figure~\ref{fig:relacc}, there exists many principal directions with low variance, along which features from different classes are not separable, Ideally, a CL model should take up these ``unused'' feature directions to learn features that are needed to classify new classes. 
Without loss of generality, suppose before the model is trained on a new task $t$, the feature extracted from the new task $z' = h'(x)$, $x\in X_t$, are all mapped to zero along an ``unused'' feature direction $v$, i.e. $v^T z'= 0$. 
Then after learning on task $t$, the feature $z=h(x)$ from new classes $C_3, C_4\in \mathcal{C}_t$ can be learned to be separable along that feature direction $v$, 
\begin{align}\label{eq:vtz}
    v^T z_{i} > v^T z_{j},\;\forall i \in C_3,\; \forall j \in C_4. 
\end{align}
% \qiao{Working on this:}
In this case, $A$ can be learned such that $v\notin \text{Col}(A)$ and thus $v^T (Az)=0$ while $v^T z\neq 0$ (satisfying Equation~\ref{eq:vtz}). In this way, the BFP loss in Equation~\ref{eq:bfp} allows the new class to be separable along $v$ and still can be minimized. 
Note that during the actual continual learning with BFP, neither $w$, $v$ nor the dimensionality of them is defined or needed. They are learned implicitly in the matrix $A$ through gradient descent optimization. $w$ and $v$ can be extracted and analyzed by PCA decomposition, but it is not required for training. 

% \subsection{Experience Replay}
\subsection{Loss functions}
\label{sec:method-loss}

We integrate the proposed backward feature projection method into an experience replay framework~\cite{buzzega2020dark}, where we keep a buffer $M$ storing training examples from old tasks. We focus on experience replay methods because they are simple and outperform other types of CL methods by a large margin according to a recent survey~\cite{masana2020class}.
We keep the model checkpoint at the end of the last task $f^{t-1}$ together with the online trained model $f$. 
% Alternatively, we can also compute the features $h_{t-1}(x)$ for buffered examples $x\in M$ at the end of each task and store them together in the buffer, but in this way, we can only compute BFP loss on the stored example from old tasks yet we found that \qiao{How they compare in practice - it performs much worse without access to the old network}.
During continual learning, the online model $f$ is trained on a batch from the online data stream of the current task $\Data_t$ using cross-entropy loss. 
\begin{align}\label{eq:loss-ce}
    L_{\text{ce}}(\Data_t; \theta) = \sum_{{x, y} \in  \Data_t} \CE (y, f_\theta(x))
\end{align}
Meanwhile, we sample another batch from $M$ for experience replay. Following~\cite{buzzega2020dark}, a cross-entropy loss and a logit distillation loss are applied on the replayed data
\begin{align}\label{eq:loss-repce}
    L_{\text{rep-ce}}(M; \theta)  =& \sum_{{x, y} \in M} \CE (y, f_\theta(x)), \\ \label{eq:loss-replogits}
    L_{\text{rep-logit}}(M; \theta) =& \sum_{{x, y} \in M} \| f_\theta(x) - f^{t-1}(x) \|_2^2.
\end{align}
And we apply our backward feature projection loss on both the online data stream $\Data_t$ and the replayed examples $M$
\begin{align} \label{eq:loss-BFP}
    L_{\text{BFP}}(\Data_t, M; \psi, A) =& \sum_{{x, y} \in \Data_t, M} \| A h_\psi(x) - h^{t-1}(x) \|_2. 
\end{align}
The total loss function used in continual learning is the weighted sum of the losses above. 
\begin{align}
\begin{split}\label{eq:total-loss}
    L(\Data_t, M;\theta, A) = L_{\text{ce}}(\Data_t;\theta) + \alpha L_{\text{rep-ce}}(M;\theta) \\  + \beta L_{\text{rep-logit}}(M;\theta) + \gamma L_{\text{BFP}}(\Data_t, M;\psi, A)
\end{split}
\end{align}
During training on task $t$, both the linear transformation $A$ and the model $f_\theta$ are optimized, and the old model checkpoint $f^{t-1}$ remains fixed. In our experiments, the matrix $A$ is randomly initialized at the beginning of each task. The pseudo-code of the proposed algorithm can be found in the~\appendixnote.

% \subsection{Feature Space in Continual Learning}



% To illustrate our method, we consider two consecutive tasks, the old one with data $\Data_{t-1}$ and the new one with data $\Data_{t}$. We use $\hat{M}=\hat{f}\circ \hat{g}$ to denote the model at the end of learning on $\Data_{t-1}$, and $M=f\circ g$ the model being trained on the new task $\Data_t$. 
% Note that when the training starts on $D_{t+1}$, $M$ is initialized from $\hat{M}$. 
% We denote the features extracted from different tasks as follows:
% \begin{align}
% \begin{split}
%     \hat{Z}_t = \hat{g}(X_t), \quad & \hat{Z}_{t+1} = \hat{g}(X_{t+1}), \\
%     Z_t = g(X_t), \quad & Z_{t+1} = g(X_{t+1}). 
% \end{split}
% \end{align}
% 
% \subsection{Linear Feature Projection}