\customparagraph{The hubness problem}
    The hubness problem refers to the emergence of \emph{hubs} in collections of points in high-dimensional vector spaces~\cite{radovanovicHubsSpacePopular2010}.
    Hubs are points that appear among the nearest neighbors of many other points, and are therefore likely to have a significant influence on \eg nearest neighbor-based classification.
    Radovanovic \etal~\cite{radovanovicHubsSpacePopular2010} showed that points closer to the expected data mean are more likely be among the nearest neighbors of other points, indicating that these points are more likely to be hubs.
    Hubness can also be seen as a result of large density gradients~\cite{haraFlatteningDensityGradient2016}, as points in high-density areas are more likely to be hubs.
    The hubness problem is thus an intrinsic property of data distributions in high-dimensional vector spaces, and not an artifact occurring in particular datasets.
    It is therefore important to take the hubness into account when designing classification systems in high-dimensional vector spaces.

\customparagraph{Hubness in FSL}
    Many recent methods in FSL rely on distance-based classification in high-dimensional representation spaces~\cite{wangSimpleShotRevisitingNearestNeighbor2019,zikoLaplacianRegularizedFewShot2020,boudiafTransductiveInformationMaximization2020,NhanSEN, zhang2021iept, ye2018, allen2019},
    making them vulnerable to the hubness problem.
    Fei \etal~\cite{feiZScoreNormalizationHubness2021} show that hyperspherical representations with zero mean reduce hubness.
    Motivated by this insight, they suggest that representations should have zero mean and unit standard deviation (ZN) \emph{along the feature dimension}.
    This effectively projects samples onto the hyperplane orthogonal to the vector with all elements \( = 1 \), and pushes them to the hypersphere with radius \( \sqrt{d} \), where \( d \) is the dimensionality of the representation space.
    Although ZN is empirically shown to reduce hubness, it does not guarantee that the data mean is zero.
    The normalized representations can therefore still suffer from hubness, potentially decreasing FSL performance.

\customparagraph{Embeddings in FSL}
    FSL classifiers often operate on embeddings of representations instead of the representations themselves, to improve the classifier's ability to generalize to novel classes~\cite{wangSimpleShotRevisitingNearestNeighbor2019,cuiParameterlessTransductiveFeature2021,zhuEASEUnsupervisedDiscriminant2022,xuAlleviatingSampleSelection2022}.
    Earlier works use the L2 normalization and Centered L2 normalization to embed representations on the hypersphere~\cite{wangSimpleShotRevisitingNearestNeighbor2019}.
    Among more recent embedding techniques, ReRep~\cite{cuiParameterlessTransductiveFeature2021} performs a two-step fusing operation on both the support and query features with an attention mechanism.
    EASE~\cite{zhuEASEUnsupervisedDiscriminant2022} combines both support and query samples into a single sample set, and jointly learns a similarity and dissimilarity matrix, encouraging similar features to be embedded closer, and dissimilar features to be embedded far away.
    TCPR~\cite{xuAlleviatingSampleSelection2022} computes the top-k neighbours of each test sample from the base data, computes the centroid, and removes the feature components in the direction of the centroid.
    Although these methods generally lead to a reduction in hubness and an increase in performance (see Figure~\ref{fig:motivatingPlot}), they are not explicitly designed to address the hubness problem resulting in suboptimal hubness reduction and performance.
    In contrast, our proposed \method and \methodS directly leverage our theoretic insights to target the root of the hubness problem.

\customparagraph{Hyperspherical uniformity}
    Benefits of uniform hyperspherical representations have previously been studied for contrastive self-supervised learning (SSL)~\cite{wangUnderstandingContrastiveRepresentation2020}.
    Our work differs from~\cite{wangUnderstandingContrastiveRepresentation2020} on several key points.
    First, we study a non-parametric embedding of support and query samples for FSL, which is a fundamentally different task from contrastive SSL.
    Second, the contrastive loss studied in~\cite{wangUnderstandingContrastiveRepresentation2020} is a combination of different cross-entropies, making it different from our KL-loss.
    Finally, we introduce a tradeoff-parameter between uniformity and LSP, and connect our theoretical results to hubness and Laplacian Eigenmaps.
