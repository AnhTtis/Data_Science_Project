While supervised deep learning has made a significant impact in areas where large amounts of labeled data are available~\cite{HeResnet2016,dosovitskiy2021anViT}, few-shot learning (FSL) has emerged as a promising alternative when labeled data is limited~\cite{kim2019edge, wangSimpleShotRevisitingNearestNeighbor2019,zikoLaplacianRegularizedFewShot2020,boudiafTransductiveInformationMaximization2020,veilleuxRealisticEvaluationTransductive2021,qiTransductiveFewShotClassification2021,lazarouIterativeLabelCleaning2021,wangRoleGlobalLabels2021,zhuEASEUnsupervisedDiscriminant2022,taoPoweringFinetuningFewShot2022,huPushingLimitsSimple2022}.
FSL aims to design classifiers that can discriminate between novel classes based on a few labeled instances, significantly reducing the cost of the labeling procedure.

In transductive FSL, one assumes access to the entire query set during evaluation.
This allows transductive FSL classifiers to learn representations from a larger number of samples, resulting in better performing classifiers.
However, many of these methods base their predictions on distances to prototypes for the novel classes~\cite{zikoLaplacianRegularizedFewShot2020,boudiafTransductiveInformationMaximization2020,veilleuxRealisticEvaluationTransductive2021,qiTransductiveFewShotClassification2021,lazarouIterativeLabelCleaning2021,zhuEASEUnsupervisedDiscriminant2022}.
This makes these methods susceptible to the hubness problem~\cite{shigetoRidgeRegressionHubness2015,suzukiCenteringSimilarityMeasures2013,radovanovicHubsSpacePopular2010, haraLocalizedCenteringReducing2015}, where certain exemplar points (hubs) appear among the nearest neighbours of many other points.
If a support sample is a hub, many query samples will be assigned to it regardless of their true label, resulting in low accuracy.
If more training data is available, this effect can be reduced by increasing the number of labeled samples in the classification rule -- but this is impossible in FSL.

\begin{figure}
    \centering
    \includegraphics[width=0.85\columnwidth]{fig/motivation-plot}
    \vspace{-.3cm}
    \caption{Few-shot accuracy increases when hubness decreases.
    The figure shows the 1-shot accuracy when classifying different embeddings with SimpleShot~\cite{wangSimpleShotRevisitingNearestNeighbor2019} on mini-ImageNet~\cite{vinyalsMatchingNetworksOne2016}.}
    \label{fig:motivatingPlot}
    \vspace{-0.4cm}
\end{figure}

Several approaches have recently been proposed to embed samples in a space where the FSL classifier's performance is improved~\cite{wangSimpleShotRevisitingNearestNeighbor2019,cuiParameterlessTransductiveFeature2021,feiZScoreNormalizationHubness2021,lePOODLEImprovingFewshot2021,zhuEASEUnsupervisedDiscriminant2022,xuAlleviatingSampleSelection2022,chikontweCADCoAdaptingDiscriminative2022}.
However, only one of these directly addresses the hubness problem.
Fei \etal~\cite{feiZScoreNormalizationHubness2021} show that embedding representations on a hypersphere with zero mean reduces hubness.
They advocate the use of Z-score normalization (ZN) along the feature axis of each representation, and show empirically that ZN can reduce hubness in FSL.
However, ZN does not guarantee a data mean of zero, meaning that hubness can still occur after ZN.

In this paper we propose a principled approach to embed representations in FSL, which both reduces hubness and improves classification performance.
First, we prove that hubness can be eliminated by embedding representations uniformly on the hypersphere.
However, distributing representations uniformly on the hypersphere without any additional constraints will likely break the class structure which is present in the representation space -- hurting the performance of the downstream classifier.
Thus, in order to both reduce hubness and preserve the class structure in the representation space, we propose two new embedding methods for FSL.
Our methods, \methodLongWithMarks (\method) and \methodSLongWithMarks (\methodS), leverage a decomposition of the Kullback-Leibler divergence between representation and embedding similarities, to optimize a tradeoff between Local Similarity Preservation (LSP) and uniformity on the hypersphere.
The latter method, \methodS, also leverages label information from the support samples to further increase the class separability in the embedding space.

Figure~\ref{fig:motivatingPlot} illustrates the correspondence between hubness and accuracy in FSL.
Our methods have both the \emph{least hubness} and \emph{highest accuracy} among several recent embedding techniques for FSL.

Our contributions are summarized as follows.
\begin{itemize}
    \item We prove that the uniform distribution on the hypersphere has zero hubness and that embedding points uniformly on the hypersphere thus alleviates the hubness problem in distance-based classification for transductive FSL.
    \item We propose \method and \methodS to embed representations on the hypersphere, and prove that these methods optimize a tradeoff between LSP and uniformity. The resulting embeddings are therefore approximately uniform, while simultaneously preserving the class structure in the embedding space.
    \item Extensive experimental results demonstrate that \method and \methodS outperform current state-of-the-art embedding approaches, boosting the performance of a wide range of transductive FSL classifiers, for multiple datasets and feature extractors.
\end{itemize}
