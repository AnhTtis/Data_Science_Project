In the preceding section, we proved that uniform embeddings on the hypersphere eliminate hubness. 
However, naÃ¯vely placing points uniformly on the hypersphere does not incorporate the inherent class structure in the data, leading to poor FSL performance.
Thus, there exists a tradeoff between uniformity on the hypersphere and the preservation of local similarities.
To address this tradeoff, we introduce two novel embedding approaches for FSL, namely \method and \methodS.
\method (Sec.~\ref{subsec:nohub}) incorporates a novel loss function for embeddings on the hypersphere, while \methodS (Sec.~\ref{subsec:nohubs}), guides \method with additional label information, which should act as a supervisory signal for a class-aware embedding that leads to improved classification performance.
Figure~\ref{fig:noHub} provides an overview of the proposed \method method.
We also note that, since our approach generates embeddings, they are compatible with most transductive FSL classifier.

\customparagraph{Few-shot Preliminaries}
    Assume we have a large labeled \emph{base} dataset \( \baseSet = \{ (\vec x_i, y_i) \mid y_i \in \baseClass;~ i = 1, \ldots, n_{\text{Base}} \}\), where \( \vec x_i \) and \( y_i \) denotes the raw features and labels, respectively.
    Let \( \baseClass \) denote the set of classes for the base dataset.
    In the few--shot scenario, we assume that we are given another labeled dataset \( \novelSet = \{ (\vec x_i, y_i) \mid y_i \in \novelClass;~ i = 1, \ldots, n_{\text{Novel}} \} \) from \emph{novel}, previously unseen classes \( \novelClass \), satisfying \(\baseClass \cap \novelClass = \emptyset \).
    In addition, we have a test set \( \testSet,\; \testSet \cap \novelSet = \emptyset \), also from \( \novelClass \).

    In a \( K \)--way \( N_S\)--shot FSL problem, we create randomly sampled \emph{tasks} (or episodes), with data from \( K \) randomly chosen novel classes.
    Each task consists of a \emph{support} set \( \supportSet \subset \novelSet \) and a \emph{query} set \( \querySet \subset \testSet \).
    The support set contains \( |\supportSet| = N_S \cdot K \) random examples (\( N_S \) random examples from each of the \( K \) classes).
    The query set contains \( |\querySet|= N_Q \cdot K \) random examples, sampled from the same \( K \) classes.
    The goal of FSL is then to predict the class of samples \( \vec x \in \querySet \) by exploiting the labeled support set \( \supportSet \), using a model trained on the base classes \( \baseClass \).
    We assume a fixed feature extractor, trained on the base classes, which maps the raw input data to the representations \( \vec x_i \).

\subsection{\method: \methodLongWithMarks}
    \label{subsec:nohub}
    We design an embedding method that encourages uniformity on the hypersphere, and simultaneously preserves local similarity structure.
    Given the support and query representations $\vec x_1, \dots, \vec x_n \in \real^k$, \(n = K(N_S + N_Q)\) , we wish to find suitable embeddings $\vec z_1, \dots, \vec z_n \in \SPHERE_d$, where local similarities are preserved.
    For both representations and embeddings, we quantify similarities using a softmax over pairwise cosine similarities
    \begin{align}
        \label{eq:pij}
        p_{ij} = \frac{p_{i|j} + p_{j | i}}{2}, \quad p_{i|j} = \frac{\exp(\kappa_i \frac{\vec x_i\T \vec x_j}{||\vec x_i||\cdot||\vec x_j||} )}{\sums{l, m}{}\exp(\kappa_i \frac{\vec x_l\T \vec x_m}{||\vec x_l||\cdot||\vec x_m||})}
    \end{align}
    and
    \begin{align}
        \label{eq:qij}
        q_{ij} = \frac{\exp(\kappa \vec z_i\T \vec z_j)}{\sums{l, m}{} \exp(\kappa \vec z_l\T \vec z_m)},
    \end{align}
    where $\kappa_i$ is chosen such that the effective number of neighbours of $\vec x_i$ equals a pre-defined perplexity\footnote{Details on the computation of the \( \kappa_i \) are provided in the supplementary.}.
    As in~\cite{Maaten,vmfsne}, local similarity preservation can now be achieved by minimizing the Kullback-Leibler (KL) divergence between the \( p_{ij} \) and the \( q_{ij}\)
    \begin{align}
        \label{eq:kl}
       KL(P || Q) = \sums{i, j}{} p_{ij} \log \frac{p_{ij}}{q_{ij}}.
    \end{align}
    However, instead of directly minimizing \( KL(P || Q) \), we find that the minimization problem is equivalent to minimizing the sum of two loss functions\footnote{Intermediate steps are provided in the supplementary.}
    \begin{align}
        \ARGMIN KL(P||Q) = \ARGMIN \Lalign + \Lunif
    \end{align}
    where
    \begin{align}
        & \Lalign = - \kappa \sums{i, j}{} p_{ij} \vec z_i\T \vec z_j, \\
        & \Lunif = \log \sums{l, m}{} \exp(\kappa \vec z_l\T \vec z_m).
    \end{align}

    \begin{customalgorithm}
        \algFontSize
        \input{alg/nohub.tex}
        \caption{\method algorithm for embeddings on the hypersphere}
        \label{alg:nohub}
    \end{customalgorithm}

    In Sec.~\ref{sec:theoreticalResults} we provide a thorough theoretical analysis of these losses, and how they relate to LSP and uniformity on the hypersphere.
    Essentially, \( \Lalign \) is responsible for the local similarity preservation by ensuring that the embedding similarities (\( \vec z_i\T \vec z_j \)) are high whenever the representation similarities (\(p_{ij}\)) are high. \( \Lunif \) on the other hand, can be interpreted as a negative entropy on \( \SPHERE_d \), and is thus minimized when the embeddings are uniformly distributed on \( \SPHERE_d \).
    This is discussed in more detail in Sec.~\ref{sec:theoreticalResults}.

    Based on the decomposition of the KL divergence, and the subsequent interpretation of the two terms, we formulate the loss in \method as the following tradeoff between LSP and uniformity
    \begin{equation}
        \label{eq:Lfinal}
        \Lfinal = \alpha\Lalign + (1-\alpha) \Lunif
    \end{equation}
    where $\alpha$ is a weight parameter quantifying the tradeoff.
    \( \Lfinal \) can then be optimized directly with gradient descent.
    The entire procedure is outlined in Algorithm~\ref{alg:nohub}.

\subsection{\methodS: \methodSLongWithMarks}
    \label{subsec:nohubs}
    In order to strengthen the class structure in the embedding space, we modify \(\Lalign \) and \(\Lunif\) by exploiting the additional information provided by the support labels.
    For \( \Lalign \), we change the similarity function in $p_{ij}$ such that
    \begin{align}
        \label{eq:modpij1}
        p_{i|j} = \frac{\exp(\kappa_i s_{\vec x}(\vec x_i, \vec x_j))}{\sums{l, m}{}\exp(\kappa_i s_{\vec x}(\vec x_l, \vec x_m))}
    \end{align}
    where
    \begin{align}
        \label{eq:modpij2}
        s_{\vec x}(\vec x_i, \vec x_j) = \begin{cases}
            1 &\text{if $\vec x_i, \vec x_j \in \supportSet$, and $y_i = y_j$}\\
            -1 &\text{if $\vec x_i, \vec x_j \in \supportSet$, and $y_i \neq y_j$}\\
            $$\vec x_i\T \vec x_j$$ &\text{otherwise}
        \end{cases}.
    \end{align}
    With this, we encourage embeddings for support samples in the \emph{same class} to be maximally similar, and support samples in \emph{different classes} to be maximally dissimilar.
    Similarly, for \( \Lunif \)
    \begin{align}
        \Lunif = \log \sums{l, m}{} \exp(\kappa s_{\vec z}(\vec z_i, \vec z_j))
    \end{align}
    where
    \begin{align}
        \label{eq:moduni}
        s_{\vec z}(\vec z_i, \vec z_j) = \begin{cases}
            -\infty, &\text{if $\vec z_i, \vec z_j \in \supportSet$, and $y_i = y_j$}\\
            \varepsilon ~ \vec z_i\T \vec z_j, &\text{if $\vec z_i, \vec z_j \in \supportSet$, and $y_i \neq y_j$}\\
            \vec z_i\T, \vec z_j &\text{otherwise}
        \end{cases}
    \end{align}
    where \( \varepsilon \) is a hyperparameter.
    This puts more emphasis on between-class uniformity by weighting the similarity higher for embeddings belonging to different classes (\( \varepsilon > 1 \)), and ignoring the similarity between embeddings belonging to the same class\footnote{Although any constant value would achieve the same result, we set the similarity to \(-\infty\) in this case to remove the contribution to the final loss.}.
    The final loss function is the same as Eq.~\eqref{eq:Lfinal}, but with the additional label-informed similarities in Eqs.~\eqref{eq:modpij1}--\eqref{eq:moduni}.

