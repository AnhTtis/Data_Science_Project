\section{Experiments}
\label{exp}

In this section, we conduct experiments on several popular human pose estimation benchmarks with comprehensive evaluation metrics. Since \textit{PoseDA} is flexible regarding the architecture of the pose lifting network, we perform our framework on different strong baselines to show the universality. We also analyze the contribution of each component in ablation studies.

\begin{figure*}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/qual.pdf}
    \caption{Qualitative results on MPI-INF-3DHP dataset. We use VideoPose3D~\cite{pavllo20193d} as pre-trained model. After finetuning with \textit{PoseDA}, the model performs better for some challenging poses.}
    \label{fig:qualitative}
    \vspace{-1em}
\end{figure*}

\subsection{Datasets and Metrics}
\label{subsec:datasets}

The datasets  used for quantitative evaluations are several widely used large-scale 3D human pose estimation benchmarks, including Human3.6M~\cite{ionescu2013human3}, MPI-INF-3DHP~\cite{mehta2017monocular}, and 3DPW~\cite{von2018recovering}.

\noindent\textbf{Human3.6M (H3.6M)} is one of the largest 3D human pose datasets captured in a laboratorial environment. Following previous works~\cite{gholami2022adaptpose, pavllo20193d}, there are two different settings of H3.6M: 1) using the S1, S5, S6, S7 and S8 as our source domain for cross-dataset evaluation. 2) using only S1 as source domain and others (S5, S6, S7, S8) as target domain.

\noindent\textbf{MPI-INF-3DHP (3DHP)} is a large-scale in-the-wild 3D human pose dataset with more diverse actions and motions. This dataset is closer to real-world scenarios and ideal for evaluating our method. Following previous works~\cite{kolotouros2019learning, gong2021poseaug}, we use its test set, which includes 2,929 frames. We report the results of \textit{PoseDA} using metrics of MPJPE, Percentage of Correct Keypoints (PCK), and Area Under the Curve (AUC).

\noindent\textbf{3DPW} is an in-the-wild dataset, unlike H3.6M or 3DHP, with uncontrolled motion and scene. Since it is a much more challenging dataset, we train each method on H3.6M and evaluate it on the 3DPW test set with the PA-MPJPE and MPJPE metric. 

\begin{table}[!t]
\small
\centering
\setlength{\tabcolsep}{1mm}

\begin{tabular}{l|c|cc}
    \specialrule{1pt}{1pt}{2pt}
    Method & S & MPJPE~($\downarrow$) & PA-MPJPE~($\downarrow$)\\
    \hline
    VideoPose3D~\cite{pavllo20193d} & Full & 51.8 & 40.0 \\
    ST-GCN~\cite{cai2019exploiting} & Full & 50.6 &  40.2\\
    SimpleBaseline~\cite{martinez2017simple} & Full & 45.5 & 37.1 \\
    SemGCN~\cite{zhao2019semantic} & Full & 43.8 & -\\
    \hline
    VideoPose3D~\cite{pavllo20193d} & S1 & 64.7 & -\\
    Li~\etal~\cite{li2020cascaded} & S1 & 62.9 & - \\
    PoseAug~\cite{gong2021poseaug} & S1 & 56.7 & -\\
    AdaptPose~\cite{gholami2022adaptpose} & S1 & 54.2 & 35.6\\
    %AdaptPose~(T=27) & S1 & 42.5 & 34.0\\
    \hline
    \textit{PoseDA} (Ours)  & S1 & \textbf{49.9} & \textbf{34.2}\\
    \specialrule{1pt}{1pt}{2pt}
\end{tabular}
\caption{\textbf{Results on H3.6M}. S denotes source data. MPJPE and PA-MPJPE are used as  evaluation metrics. Source: H3.6M-S1. Target: H3.6M-S5, S6, S7, S8.}
\label{tab:H3.6M}
\end{table}

\subsection{Implementation Details}

\noindent\textbf{Details of discriminators}
For the anchor discriminator, we split 3D keypoints into five parts following~\cite{gong2021poseaug}, pass them through each of the five 4-layer residual MLPs with LeakyReLU, and finally concatenate the output. We use RMSProp optimizer for the anchor discriminators.

\noindent\textbf{Details of generators}
We sequentially build three 3-layer residual MLPs with LeakyReLU to generate bone angle, bone length, and rotation. Each network receives the output of the previous network as well as noise vector as input. These three networks are trained jointly by weighted adversarial losses. For every $6$ iterations, we train the generator once and the discriminator $5$ times. 

\noindent\textbf{Details of pose lifting network}
We use VideoPose3D~\cite{pavllo20193d} (1-frame) as pose lifting network as well as other strong baselines~\cite{cai2019exploiting, martinez2017simple, zhao2019semantic} in ablation study. The pre-trained weights from the source dataset are used as initial weights for all the experiments.

\noindent\textbf{Details of training}
Starting with pre-trained weight, it takes 5 to 10 epochs (50 secs per epoch) to get convergence. In the experiments, we also found that WGAN not only has more stable training than LSGAN, but also prevents mode collapse, but the performance is not much different.We only train the generator and discriminator at the first $5$ epochs for stable augmentation. Then, we mix the augmented data and original data in an 1:1 ratio for pose lifting network training. The learning rate for all the networks is $1e^{-4}$. \textit{PoseDA} is trained on NVIDIA RTX 3090. The training process takes two hours and consumes 2GB of GPU memory. 

\subsection{Results}
\label{subsec:results}

% \begin{table}[h] \centering
% %\ra{1.3}
% \caption{Comparison of 3D human pose estimation datasets we used for training and evaluating. The mean and std of camera distance, camera height, focal length, bone length is calculated in \cite{wang2020predicting}. Focal length is in mm while the others are in unit meters. We use the camera intrinsics for target dataset while training. We also align the number and definition of joints in different datasets following~\cite{gong2021poseaug}.}
% \begin{center}
% {\scriptsize
% \vspace{-0.5cm}
% \begin{tabular}{@{}l c c c@{}}
% %\Xhline{2\arrayrulewidth}
% \toprule
% Dataset & H3.6M & 3DPW & 3DHP \\
% \hline
% Year & 2014  & 2018 & 2017 \\
% Imaging Space & 1000 $\times$ 1002  & 1920 $\times$ 1080 & 2048 $\times$ 2048 \\ 
% Camera Distance & 5.2 $\pm$ 0.8 & 3.5 $\pm$ 0.7 & 3.8 $\pm$ 0.8 \\
% Camera Height & 1.6 $\pm$ 0.05  & 0.6 $\pm$ 0.8 & 0.8 $\pm$ 0.4 \\
% Focal Length& 1146.8 $\pm$ 2.0  & 1962.2 $\pm$ 1.5  & 1497.88 $\pm$ 2.8 \\
% %Center & H3.6M & gpa & surreal & 3dpw & 3dhp \\
% No. of Joints & 38 & 24 & 28 or 17 \\
% No. of Cameras & 4 & 1 & 14 \\
% No. of Subjects & 11 & 18 & 8 \\
% Bone Length & 3.9 $\pm$ 0.1  & 3.7 $\pm$ 0.1 & 3.7 $\pm$ 0.1 \\
% GT source &VICON &  SMPL & The Capture \\
% No. Train Images & 311,951 & 22,375 & 366,997 \\
% No. Test Images  & 109,764 & 35,515 &  2,929\\
% Scene & controlled indoor & natural outdoor & controlled in/outdoor \\
% %\Xhline{2\arrayrulewidth}
% \bottomrule
% \end{tabular}
% }
% \end{center}
% \vspace{-0.5cm}
% \label{tab:datasets}
% \end{table}

\noindent\textbf{Results on H3.6M} We compare \textit{PoseDA} with state-of-the-art methods~\cite{pavllo20193d,li2020cascaded,gong2021poseaug,gholami2022adaptpose} using ground truth 2D keypoints as input. The results on H3.6M are shown in Table~\ref{tab:H3.6M}, focusing on the generalization ability of the model over actions, since the distributions over global positions are relatively consistent between the different parts of the same dataset. Even though AdaptPose~\cite{gholami2022adaptpose} utilizes temporal information, our method still achieves state-of-the-art performance.

\begin{table}[!t]
\small
\centering
\setlength{\tabcolsep}{1mm}

\begin{tabular}{l|c|ccc}
    \specialrule{1pt}{1pt}{2pt}
    Method & CD & MPJPE~($\downarrow$) & PCK~($\uparrow$) & AUC~($\uparrow$) \\
    \hline
    Multi Person~\cite{mehta2018single} & & 122.2 & 75.2 & 37.8 \\
    Mehta~\etal~\cite{mehta2017monocular} & & 117.6 & 76.5 & 40.8 \\
    VNect~\cite{mehta2017vnect}  & & 124.7 & 76.6 & 40.4 \\
    OriNet~\cite{luo2018orinet} & & 89.4 & 81.8 & 45.2  \\
    SimpleBaseline~\cite{martinez2017simple} & & 84.3 & 85.0 & 52.0\\
    % Chen~\etal(T=81)~\cite{chen2021anatomy} & & 78.8 & 87.9 & 54.0\\
    % PoseFormer(T=9)~\cite{zheng20213d} & & 77.1 & 88.6 & 56.4\\
    % CrossFormer(T=9)~\cite{hassanin2022crossformer} & & 76.3 & 89.1 & 57.5\\
    % MHFormer(T=9)~\cite{li2022mhformer} & & 58.0 & 93.8 & 63.3 \\
    MixSTE~\cite{zhang2022mixste} & & 57.9 & 94.2 & 63.8 \\
    \hline	
    LCN~\cite{ci2019optimizing} & \checkmark & - & 74.0 & 36.7\\
    HMR~\cite{kanazawa2018end} & \checkmark & 113.2 & 77.1 & 40.7\\
    SRNet~\cite{zeng2020srnet} & \checkmark & - & 77.6 & 43.8\\
    Li~\textit{et al.}~\cite{li2020cascaded} & \checkmark & 99.7 & 81.2 & 46.1 \\
    RepNet~\cite{wandt2019repnet} & \checkmark & 92.5 & 81.8 & 54.8 \\
    PoseAug~\cite{gong2021poseaug}  & \checkmark & 73.0 & 88.6 & 57.3\\
    AdaptPose~\cite{gholami2022adaptpose} & \checkmark & 68.3 & 90.2 & 59.0\\
    \hline
    \textit{PoseDA} (Ours)  & \checkmark & \textbf{61.3}& \textbf{92.1} & \textbf{62.5} \\
    \specialrule{1pt}{1pt}{2pt}
\end{tabular}
\caption{\textbf{Results on 3DHP}. CD denotes cross-domain evaluation (no CD denotes fully-supervision, i.e., trained and tested on the same 3DHP dataset). PCK, AUC and MPJPE are used as evaluation metrics. Source: H3.6M. Target: 3DHP.}
\label{tab:3dhp}
\end{table}

\noindent\textbf{Results on 3DHP} Our method achieves remarkable performance in all the metrics. We use single frame ground truth 2D keypoints as input and therefore compare against various recent methods with the same setting. \textit{PoseDA} improve upon the previous state-of-the-art method by 10.2\%, and is even competitively compared with some state-of-the-art fully supervised method.

\noindent\textbf{Results on 3DPW}~\cite{von2018recovering} We use ground truth 2D keypoints as input. \textit{PoseDA} achieves the state-of-the-art performance without using any 3D annotations in 3DPW, as shown in Table~\ref{tab:3dpw}, even is favorably compared with video-based methods.

\begin{table}[!t]
\small
\centering
\setlength{\tabcolsep}{1mm}

\begin{tabular}{l|c|c|cc}
    \specialrule{1pt}{1pt}{2pt}
    Method & CD & V & PA-MPJPE~($\downarrow$) & MPJPE~($\downarrow$)\\
    \hline
    VideoPose3D~\cite{pavllo20193d} & &\checkmark & 68.0 & 105.0\\
    EFT~\cite{joo2021exemplar} & & & 55.7 & -\\
    VIBE~\cite{kocabas2020vibe} & &\checkmark & 51.9 & 82.9\\
    Lin~\etal~\cite{lin2021mesh} & & & 45.6 & 74.7\\
    \hline
    PoseAug~\cite{gong2021poseaug} & \checkmark& & 58.5 & 94.1\\
    VIBE~\cite{kocabas2020vibe} & \checkmark&\checkmark & 56.5 & 93.5\\
    BOA~\cite{guan2021bilevel} & \checkmark&\checkmark & 49.5 & \textbf{77.2}\\
    AdaptPose~\cite{gholami2022adaptpose} &\checkmark& \checkmark &\textbf{46.5} & 81.2\\
    \hline
    \textit{PoseDA} (Ours) & \checkmark && 55.3 & 87.7\\
    \specialrule{1pt}{1pt}{2pt}
\end{tabular}
\caption{\textbf{Results on 3DPW}. CD denotes cross-domain evaluation (no CD denotes fully-supervision, i.e., trained and tested on the same 3DPW dataset), V denotes video-based method. PA-MPJPE and MPJPE are used as evaluation metrics. Source: H3.6M. Target: 3DPW.}
\label{tab:3dpw}
\end{table}

\noindent\textbf{Qualitative evaluation} Figure~\ref{fig:qualitative} shows the qualitative evaluation on 3DHP dataset. Compared with the baseline model without training with \textit{PoseDA}, our method performs well for unusual human positions and challenging poses.


\begin{table}[!t]
    \small
    \centering
    \newcommand{\D}[1]{~\scriptsize{\textcolor{red}{(#1)}}}
    \setlength{\tabcolsep}{1mm}
    
    \begin{tabular}{l|ccc}
        \specialrule{1pt}{1pt}{2pt}
        Method & MPJPE~($\downarrow$)  & PCK~($\uparrow$) & AUC~($\uparrow$)\\
        \hline
        SemGCN~\cite{zhao2019semantic} & 95.96 & 80.68 & 48.48 \\
        + LPA & 87.64\D{-8.3} & 84.21\D{+3.5} & 51.24\D{+2.8}\\
        + GPA & 86.56\D{-9.4} & 83.85\D{+3.2} & 50.98\D{+2.5} \\
        \rowcolor[gray]{0.9}
        + \textit{PoseDA}& \textbf{78.37}\D{-17.6} & \textbf{86.17}\D{+5.5} & \textbf{54.74}\D{+6.3}\\
        \hline
        SimpleBaseline~\cite{martinez2017simple} & 81.23 & 85.85  & 53.95\\
        + LPA & 66.56\D{-14.7} & 90.16\D{+4.3} & 60.41\D{+6.5} \\
        + GPA & 69.19\D{-12.0} & 89.90\D{+4.1} & 58.50\D{+4.6}\\
        \rowcolor[gray]{0.9}
        + \textit{PoseDA}& \textbf{64.79}\D{-16.4} & \textbf{90.55}\D{+4.7} & \textbf{61.32}\D{+7.4}\\
        \hline
        ST-GCN~\cite{cai2019exploiting} & 81.19 & 85.92 & 53.78\\
        + LPA & 74.31\D{-6.9} & 88.72\D{+2.9} & 56.20\D{+2.4}\\
        + GPA & 74.41\D{-6.8} & 88.58\D{+2.7} & 55.52\D{+1.7}\\
        \rowcolor[gray]{0.9}
        + \textit{PoseDA}& \textbf{69.50}\D{-11.7} & \textbf{90.15}\D{+4.2} & \textbf{58.56}\D{+4.8}\\
        \hline
        VideoPose3D~\cite{pavllo20193d} & 82.55 & 85.71 & 53.35\\
        + LPA & 66.65\D{-15.9} & 90.05\D{+4.3} & 60.24\D{+6.9}\\
        + GPA & 66.07\D{-16.5} & 90.87\D{+5.2} & 60.07\D{+6.7}\\
        \rowcolor[gray]{0.9}
        + \textit{PoseDA}&  \textbf{61.36}\D{-21.2}& \textbf{92.05}\D{+6.3} & \textbf{62.52}\D{+9.2} \\
        \specialrule{1pt}{1pt}{2pt}
    \end{tabular}
    \caption{Ablation study on components and pose lifting network of our method. LPA denotes local pose augmentation, GPA denotes global position alignment. \textit{PoseDA} combine GPA and LPA in a unified framework. Source: H3.6M. Target: 3DHP.}
    \label{tab:ab1}
\end{table}

\subsection{Ablation Study}
\label{ablation}
    
\noindent\textbf{Analysis on each components and board lifting network.}
Since \textit{PoseDA} is a data augmentation framework, with any pose lifting network architecture. Shown in Tabel~\ref{tab:ab1}, we conduct experiments on several strong baselines with different architectures, including MLP~\cite{martinez2017simple}, Convolutional Network~\cite{pavllo20193d}, and Graph Convolutional Network~\cite{cai2019exploiting, zhao2019semantic}. We also check the effectiveness of each component in \textit{PoseDA}. For fair comparison, we use the same weights of generator and discriminators in all the experiments. It shows that both global position alignment (GPA) and local pose augmentation (LPA) benefit the adaptive performance. Moreover, GPA significantly boosts performance without any extra training or use of additional learnable parameters.

% \noindent\textbf{Analysis on global position alignment (GPA)}
% We show that global position alignment module actually align the 2D pose distribution in terms of scale and root position on 2D screen in Figure~\ref{fig:position}.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.95\linewidth]{figures/scale.pdf}
    
%     \includegraphics[width=0.95\linewidth]{figures/x.pdf}

%     \includegraphics[width=0.95\linewidth]{figures/y.pdf}

%     \caption{Comparison of 2D scale (first row), root position of x-axis (second row), y-axis (third row) in source domain (left), source domain after GPA (middle), target domain (right). Source: H3.6M. Target: 3DHP.}

%     \label{fig:position}
% \end{figure}

\begin{table}[!t]
    \small
    \centering
    \setlength{\tabcolsep}{1mm}
    
    \begin{tabular}{l|ccc}
        \specialrule{1pt}{1pt}{2pt}
        Method & MPJPE~($\downarrow$)  & PCK~($\uparrow$) & AUC~($\uparrow$)\\
        \hline
        No Aug & 66.07 & 90.87 & 60.07\\
        RR & 66.88 & 90.77 & 59.31\\
        BL & 65.81 & 90.94 & 59.91\\
        RR + BL & 64.99 & 91.07 & 60.49\\
        \textit{PoseDA} & \textbf{61.36} & \textbf{92.05} & \textbf{62.52}\\
        \specialrule{1pt}{1pt}{2pt}
    \end{tabular}
    \caption{Ablation study on local pose augmentation with different pre-defined pose augmentation methods. RR denotes random rotation along vertical axial and BL denotes random bone length transformation. Source: H3.6M. Target: 3DHP.}
    \label{tab:ra}
\end{table}

\noindent\textbf{Analysis on global pose alignment (GPA)} We argue that the crop and normalization on 2D poses is an inaccurate method compared to our GPA module. Because the projection relation is not a linear operator and therefore does not have translation invariance. The crop out loses information about the relative positions of the camera and the person, and when the same 2d pose is cropped in a different position in the picture, the corresponding 3d pose is different (even if it is root-relative), there will be a one-to-many situation, which still increases ambiguity. We conduct experiments under four different pre-processing settings and with two backbones, our GPA based on screen normalized. We train the model on Human3.6M and test on 3DHP with the same pre-processing. The figure below shows different normalization operations and the table below shows our GPA method outperforms other normalization methods.

% 我们相信crop和normalization的方法相比于我们的GPA是有误差的方法，这是因为投影关系不是一个线性算子，因此不具备平移不变性。[Optional] 以下是数学证明
% Let $S=([\boldsymbol{R}_{2D}, \boldsymbol{J}^i_{2D}], [\boldsymbol{R}_{3D}, \boldsymbol{J}^i_{3D}])$ denote corresponding 2D-3D pose pairs, where $\boldsymbol{R}_{2D}=[x_r, y_r]$, $\boldsymbol{R}_{3D}=[X_r, Y_r, Z_r]$ is the root position and $\boldsymbol{J}^i_{2D}=[x_i, y_i]$, $\boldsymbol{J}^i_{3D}=[X_i, Y_i, Z_i]$ is the position of $i$-th joint. We define \textbf{crop} operation to be equivalent to setting $\boldsymbol{R}_{2D}$ to 0 and \textbf{normalization} opeartion to be equivalent to multiplying a factor $s$ on $\boldsymbol{J}^i_{2D}$. The projection from 3D joints to 2D joints after translation should obey the perspective projection function as follows. without loss of generality, we only discuss the x-axis:
% \begin{equation}
%     \frac{f_x(X_i+X_r)}{Z_i+Z_r}+c_x = x_i+x_r
% \end{equation}
\vspace{-0.4cm}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/norm.pdf}
    \label{fig:my_label}
\end{figure}
\vspace{-0.6cm}

% [ADD] 实验结论 GPA use screen normalized 2D coordinate

\begin{table}[!h]
    \small
    \centering
    \newcommand{\D}[1]{~\scriptsize{\textcolor{red}{(#1)}}}
    \newcommand{\U}[1]{~\scriptsize{\textcolor{green}{(#1)}}}
    \setlength{\tabcolsep}{1mm}
    
    \begin{tabular}{l|ccc}
        \specialrule{1pt}{1pt}{2pt}
        Method & MPJPE~($\downarrow$)  & PCK~($\uparrow$) & AUC~($\uparrow$)\\
        \hline
        SimpleBaseline \\
        w screen norm & 81.23 & 85.85 & 53.95\\
        w crop norm & 95.37 & 81.40 & 47.66\\
        w square crop & 84.81 & 84.29 & 52.27\\
        \rowcolor[gray]{0.9}
        w GPA (\textit{ours}) & \textbf{69.19} & \textbf{89.90} & \textbf{58.50}\\
        \hline
        VideoPose3D\\
        w screen norm & 82.55 & 85.71 & 53.35\\
        w crop norm & 93.16 & 82.23 & 48.00\\
        w square crop & 85.05 & 84.90 & 51.81\\
        \rowcolor[gray]{0.9}
        w GPA (\textit{ours}) & \textbf{66.07} & \textbf{90.87} & \textbf{60.07}\\
        \specialrule{1pt}{1pt}{2pt}
    \end{tabular}
    \vspace{-0.2cm}
    \caption{Ablation study on Global Position Alignment over other normalization techniques. Source: H3.6M. Target: 3DHP.}
    \label{tab:ab1}
\end{table}

\noindent\textbf{Analysis on local pose augmentation (LPA)} We also conduct ablation study on  different methods of LPA. Comparing with random rotation and random bone length transformation, the proposed adversarial augmentation method achieves better performance as shown in Table~\ref{tab:ra}.
