\newsavebox\CBox
\def\textBF#1{\sbox\CBox{#1}\resizebox{\wd\CBox}{\ht\CBox}{\textBF{#1}}}
\newcommand{\ParaEntry}[1]{\vspace{1mm}\noindent\textbf{#1}}
\newcommand{\TableEntry}[2]{#1~\scriptsize{\textcolor{red}{\tiny(-#2)}}}
\newcommand{\BTableEntry}[2]{\textbf{#1}~\scriptsize{\textcolor{red}{\tiny(-#2)}}}
\newcommand{\PTableEntry}[2]{#1~\scriptsize{\textcolor{red}{\tiny(+#2)}}}
\newcommand{\BPTableEntry}[2]{\textbf{#1}~\scriptsize{\textcolor{red}{\tiny(+#2)}}}

\newcommand{\HTableEntry}[2]{#1~\scriptsize{\textcolor{red}{\small(-#2)}}}
\newcommand{\HBTableEntry}[2]{\textbf{#1}~\scriptsize{\textcolor{red}{\small(-#2)}}}
\newcommand{\HPTableEntry}[2]{#1~\scriptsize{\textcolor{red}{\small(+#2)}}}
\newcommand{\HBPTableEntry}[2]{\textbf{#1}~\scriptsize{\textcolor{red}{\small(+#2)}}}

\newpage

\section*{Supplement Material}

\subsection*{A. Experiments of detected 2D pose}
GT 2D pose is used in all the experiments in the paper. Besides, we further evaluate the performance under detected 2D pose as shown in Table~\ref{tab:r33}. Since GPA only uses 2D box rather than specific 2D pose, the performance does not drop a lot. We reimplement all the baseline models and PoseAug in H3.6M.

\renewcommand{\thetable}{A}
\begin{table*}[t]
\centering
\setlength{\tabcolsep}{2mm}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{l|cccc|cccc}
\toprule
\multicolumn{1}{c|}{} & \multicolumn{4}{c|}{H3.6M}  & \multicolumn{4}{c}{3DHP} \\ 
\midrule
Method & \multicolumn{1}{c}{DET} & \multicolumn{1}{c}{CPN} & \multicolumn{1}{c}{HRNet} & \multicolumn{1}{c|}{GT} &  \multicolumn{1}{c}{DET} & \multicolumn{1}{c}{CPN} & \multicolumn{1}{c}{HRNet} & \multicolumn{1}{c}{GT}\\ 
\midrule
SemGCN & 77.3 & 73.8 & 67.2 & 58.9 & 101.9 & 98.7 & 95.6 & 95.6 \\
+ PoseAug & \HTableEntry{75.5}{1.8}  & \HTableEntry{73.5}{0.3} & \HTableEntry{66.1}{1.2} & \HTableEntry{58.0}{0.9} & \HTableEntry{89.9}{11.9} & \HTableEntry{89.3}{9.4}  & \HTableEntry{89.1}{6.5}  & \HTableEntry{86.1}{9.5} \\
\rowcolor[gray]{0.9}
+ PoseDA & \HBTableEntry{75.0}{2.3} & \HBTableEntry{71.9}{1.9} &  \HBTableEntry{63.8}{3.4}&  \HBTableEntry{53.9}{5.0}& \HBTableEntry{80.3}{21.6} & \HBTableEntry{80.3}{18.4} & \HBTableEntry{80.9}{14.7} & \HBTableEntry{78.3}{17.3}\\
\midrule
SimpleBaseline & 69.2 & 65.1 & 60.3 & 53.6 & 91.1 & 88.8 & 86.4 & 81.2 \\
+ PoseAug & \HTableEntry{68.4}{0.8} & \HTableEntry{64.5}{0.6} & \HTableEntry{59.7}{0.6} & \HTableEntry{51.8}{1.8} & \HTableEntry{78.7}{12.4} & \HTableEntry{78.7}{10.1} & \HTableEntry{76.4}{10.1} & \HTableEntry{76.2}{5.0} \\
\rowcolor[gray]{0.9}
+ PoseDA & \HBTableEntry{67.9}{1.3} & \HBTableEntry{63.0}{2.1}& \HBTableEntry{56.8}{3.5}&\HBTableEntry{50.2}{3.4} &\HBTableEntry{67.3}{23.8} & \HBTableEntry{67.3}{21.5} & \HBTableEntry{67.3}{19.1} & \HBTableEntry{64.7}{16.5}\\
\midrule
ST-GCN & 73.8 & 76.8 & 62.9 & 57.2 & 95.5 & 91.3 & 87.9 & 81.2 \\
+ PoseAug &\HTableEntry{73.8}{0.0} & \HTableEntry{72.9}{3.9} & \HTableEntry{61.3}{1.6} & \HTableEntry{51.2}{6.0} & \HTableEntry{83.5}{12.1} & \HBTableEntry{77.7}{13.6} & \HTableEntry{76.6}{11.3} & \HTableEntry{74.9}{6.3} \\
\rowcolor[gray]{0.9}
+ PoseDA & \HBTableEntry{73.0}{0.8}&\HBTableEntry{68.6}{8.2} &\HBTableEntry{61.2}{1.7} & \HBTableEntry{48.4}{8.8}& \HBTableEntry{78.8}{16.7} & \HTableEntry{77.8}{13.5} & \HBTableEntry{75.1}{12.8} & \HBTableEntry{69.5}{11.7}\\
\midrule
VideoPose3D & 70.4 & 79.2 & 70.7 & 64.7 & 92.6 & 89.8 & 85.6 & 82.3 \\
+ PoseAug & \HBTableEntry{67.1}{3.3} & \HTableEntry{70.4}{8.8} & \HTableEntry{63.6}{7.1} & \HTableEntry{56.7}{8.0} & \HTableEntry{78.3}{14.4} & \HTableEntry{78.4}{11.4} & \HTableEntry{73.2}{12.4} & \HTableEntry{73.0}{9.3}\\
\rowcolor[gray]{0.9}
+ PoseDA & \HTableEntry{67.4}{3.0}&\HBTableEntry{62.2}{17.0} &\HBTableEntry{55.7}{15.0} & \HBTableEntry{49.9}{14.8} & \HBTableEntry{64.6}{28.0} & \HBTableEntry{65.4}{24.4} &  \HBTableEntry{64.5}{21.1} & \HBTableEntry{61.4}{20.9}\\
\bottomrule
\end{tabular}
}
\vspace{10pt}
\caption{{Performance comparison in MPJPE~($\downarrow$) for various pose estimators on H3.6M and 3DHP datasets. DET, CPN, HRNet and GT denote 3D pose estimation model trained on several widely used different 2D pose sources, respectively. For H3.6M Exp., source: H3.6m-S1; target: H3.6m-S5, S6, S7, S8. For 3DHP Exp., source: H3.6M; target: 3DHP.} 
}
\label{tab:r33}
\end{table*}

\subsection*{B. Ablation studies on 3DPW}
We conduct additional ablation studies on 3DPW dataset in Table~\ref{tab:r31}. We believe that the pose diversity is the limitation (\eg the rare poses are still hard to estimate after PoseDA).

\renewcommand{\thetable}{B}
\begin{table}[h]
    \centering
    \small
    \newcommand{\D}[1]{~\scriptsize{\textcolor{red}{(#1)}}}
    \setlength{\tabcolsep}{1mm}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cccc}
        % \specialrule{1pt}{1pt}{2pt}
        \toprule
        Method & MPJPE~($\downarrow$) & PA-MPJPE~($\downarrow$) & PCK~($\uparrow$) & AUC~($\uparrow$)\\
        \midrule
        SimpleBaseline &  153.44 & 100.95 & 59.79 & 28.59\\
        + LPA &  \TableEntry{136.64}{16.8} & \TableEntry{79.07}{21.88} & \PTableEntry{63.07}{3.28} & \PTableEntry{28.99}{0.40}\\
        + GPA &  \TableEntry{131.41}{22.03} & \TableEntry{90.10}{10.85} & \PTableEntry{67.53}{7.74} & \PTableEntry{28.94}{0.35}\\
        \rowcolor[gray]{0.9}
        + PoseDA & \BTableEntry{121.93}{31.51} & \BTableEntry{78.39}{22.56} & \BPTableEntry{69.23}{6.16} & \BPTableEntry{29.72}{0.73}\\
        \midrule
        VideoPose3D & 101.46 & 61.49 & 80.50 & 41.17 \\
        + LPA &  \TableEntry{96.72}{4.74} & \TableEntry{58.96}{2.53} & \PTableEntry{83.42}{2.92} & \PTableEntry{43.17}{2.00}\\
        + GPA &  \TableEntry{92.44}{9.02} & \TableEntry{58.59}{2.90} & \PTableEntry{83.94}{3.44} & \PTableEntry{45.05}{3.88}\\
        \rowcolor[gray]{0.9}
        + PoseDA & \BTableEntry{87.70}{13.76} & \BTableEntry{55.30}{6.19} & \BPTableEntry{84.98}{4.48} & \BPTableEntry{46.10}{4.93}\\
        \bottomrule
    \end{tabular}
    }
    \vspace{10pt}
    \caption{Ablation study on components and pose lifting network of our method. Source: H3.6M. Target: 3DPW.}
    \label{tab:r31}
\end{table}

\subsection*{C. More discussion on Global Position Alignment}

We show that global position alignment module actually align the 2D pose distribution in terms of scale and root position on 2D images in \cref{fig:position}. While other works did camera view estimation~\cite{wandt2019repnet, wang2020predicting} or generation~\cite{gholami2022adaptpose, gong2021poseaug} as an auxiliary task to address the global position adaptation problems, our method achieves alignment explicitly and directly.

\renewcommand{\thefigure}{A}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/sup1.pdf}
    \caption{These 2 sampled poses are from generated poses and the target dataset. They have similar 2D poses but different 3D poses, indicating that adaptation based on 2D poses may not lead to adaptation on 3D poses.}
    \label{fig:2d3dcomparison}
\end{figure}

\renewcommand{\thefigure}{B}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/scale.pdf}
    \includegraphics[width=0.95\linewidth]{figures/x.pdf}
    \includegraphics[width=0.95\linewidth]{figures/y.pdf}
    \caption{Comparison of 2D scale (first row), root position of x-axis (second row), y-axis (third row) in source domain (left), source domain after GPA (middle), target domain (right). Source: H3.6M. Target: 3DHP.}
    \label{fig:position}
\end{figure*}

\subsection*{D. More discussion on Local Pose Augmentation}

The most counter-intuitive conclusion in this paper is why adaptation methods perform worse than augmentation methods. In the discussion section, we include more detailed ablation studies on LPA. As shown in \cref{fig:2d3dcomparison}, we sampled two poses from generated poses trained with a 2D discriminator and the target dataset. They have similar 2D poses but different 3D poses, which shows the reason why simply applying local pose adaptation based on a 2D pose discriminator may not have the final adaptation performance.


\renewcommand{\thetable}{C}
\begin{table}[h]
    \centering
    \setlength{\tabcolsep}{1mm}
    
    \begin{tabular}{ccc|ccc}
        \toprule
        $\mathcal{G}_{pose}$ & $\mathcal{D}_{3D}$ & $\mathcal{D}_{2D}$ & MPJPE~($\downarrow$) & PCK~($\uparrow$) & AUC~($\uparrow$)\\
        \midrule
        - & - & - & 66.07 & 90.87 & 60.07\\
        $\mathcal{S}$ & $\mathcal{S}$ & $\mathcal{S}$ & 73.55 & 88.96 & 56.41\\
        $\mathcal{S}$ & $\mathcal{S}$ & $\mathcal{T}$ & 65.46 & 91.27 & 60.03\\
        $\mathcal{S}$ & $\mathcal{S}$ & - & \textbf{61.36}& \textbf{92.05} & \textbf{62.52}\\
        \bottomrule
    \end{tabular}
    \vspace{10pt}
    \caption{The input of the pose generator $\mathcal{G}_{pose}$, the 3D pose discriminator $\mathcal{D}_{3D}$, and the 2D pose discriminator $\mathcal{D}_{2D}$ in Local Pose Augmentation (LPA) module. $S, T$ denote poses from the source or target domain. Source: H3.6M. Target: 3DHP.}
    \label{tab:dis}
\end{table}

As \cref{tab:dis}, compared with our final method, the 2D discriminator trained with 2D poses from the target dataset improves the performance from $66.07$ mm to $65.46$ mm in MPJPE since the discriminator makes scale and location adaptation better. However, once the 2D discriminator is removed, we can achieve a better result, $61.36$ mm. The reason is that the 2D pose discriminator suppresses the diversity of generated 3D poses and makes the generator generates poses with similar 2D poses, but different 3D poses, as \cref{fig:2d3dcomparison} shows.
