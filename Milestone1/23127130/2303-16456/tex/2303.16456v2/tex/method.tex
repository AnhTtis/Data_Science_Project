\section{Method}
\label{sec:method}

In this section, we introduce \textit{PoseDA}, an unsupervised domain adaptation framework, as summarized in Figure~\ref{fig:overview}, which consists of global position alignment (GPA) in Section~\ref{subsec:gpr} and local pose augmentation (LPA) in Section~\ref{subsec:ra}. Global position alignment module aims to align the 2D pose spatial distribution of both scale and location, $(x,y)$ coordinatse, between source and target datasets, and local pose augmentation is designed to enhance the diversity of 3D-2D pose mapping. Finally, we formulate the training process with several loss functions. The pseudo-code for the overall training process of our method is given in Algorithm~\ref{training pipeline}, and the transformation pipeline visualization of corresponding 2D-3D pose pairs is shown in Figure~\ref{fig:pipeline}.

\subsection{Problem Formulation}
\label{subsec:formulation}
% basic notation
Let $S=(\boldsymbol{J}^{src}_{2D}, \boldsymbol{J}^{src}_{3D})$ denote corresponding 2D-3D pose pairs from the source dataset, and $\boldsymbol{J}^{tar}_{2D}$ denote 2D pose from target dataset extracted by an off-the-shelf 2D pose estimator. Note that we only use $\boldsymbol{J}^{tar}_{3D}$ for evaluation but not for training. All the 3D poses  $\boldsymbol{J}_{3D}$ in this paper are root-relative since we do not predict the global position.

Our method conducts data augmentation on pose pairs $S=(\boldsymbol{J}^{src}_{2D}, \boldsymbol{J}^{src}_{3D})$ from source dataset. The 2D pose $\boldsymbol{J}^{tar}_{2D}$ and the camera intrinsic parameters from target dataset is used to guide this process:
\begin{equation}
    \boldsymbol{J}^{aug}_{3D} = \mathcal{G}(\boldsymbol{J}^{src}_{3D};\boldsymbol{\theta}_{cam}),\ 
    \boldsymbol{J}^{aug}_{2D} = f_p(\boldsymbol{J}^{aug}_{3D};\boldsymbol{\theta}_{cam})
\end{equation}
where $(\boldsymbol{J}^{aug}_{2D}, \boldsymbol{J}^{aug}_{3D})$ denotes augmented pose pairs, $\boldsymbol{\theta}_{cam}$ denotes the camera intrinsic parameters from target dataset, and $f_p$ denotes the projection function from 3D camera coordinates to 2D image coordinates. The augmented pose pairs are further used to train the pose lifting network.

We use several strong baseline methods~(\textit{e.g.,}~VideoPose3D~\cite{pavllo20193d} and SimpleBaseline~\cite{martinez2017simple}) for lifting 2D poses to 3D poses. Let $\mathcal{P}:\boldsymbol{J}_{2D} \mapsto \boldsymbol{J}_{3D}$ denotes the lifting network with parameters $\boldsymbol{\theta}_{\mathcal{P}}$, which can be trained in fully-supervised paradigm as:
\begin{equation}
    \min_{\boldsymbol{\theta}_{\mathcal{P}}} \mathcal{L}_{\mathcal{P}}(\mathcal{P}(\boldsymbol{J}_{2D};\boldsymbol{\theta}_{\mathcal{P}}), \boldsymbol{J}_{3D})
\end{equation}
where $(\boldsymbol{J}_{2D}, \boldsymbol{J}_{3D})$ denotes paired 2D-3D pose pairs, which consist of both augmented pose pairs $(\boldsymbol{J}^{aug}_{2D}, \boldsymbol{J}^{aug}_{3D})$ and original pose pairs $(\boldsymbol{J}^{src}_{2D}, \boldsymbol{J}^{src}_{3D})$, the loss function $\mathcal{L}_{\mathcal{P}}$ is typically defined as Mean Square Error (MSE), which is corresponding to the evaluation metric Mean Per Joint Position Error (MPJPE).

\subsection{Global Position Alignment (GPA)}
\label{subsec:gpr}
Global position alignment (GPA) is designed to eliminate the domain gap in viewpoints, which is simple yet efficient. By applying Monte Carlo sampling~\cite{chen2022epro}, the scale and location distributions of the 2D poses of the source dataset can be migrated to distributions of target dataset. We first construct pose pairs $(\boldsymbol{J}^{src}_{3D}, \boldsymbol{J}^{tar}_{2D})$, which are pairs of root-relative 3D poses and 2D poses randomly sampled from source and target domains respectively.

Given a 2D pose $\boldsymbol{J}^{tar}_{2D} = [\boldsymbol{x}^{tar}, \boldsymbol{y}^{tar}]^T \in \mathbb{R}^{2 \times J}$ and 3D pose $\boldsymbol{J}^{src}_{3D} = [\boldsymbol{X}^{src},\boldsymbol{Y}^{src},\boldsymbol{Z}^{src}]^T \in \mathbb{R}^{3 \times J}$ with the root at the origin $[0,0,0]^T$, GPA aims to estimate the translated 3D root position $\boldsymbol{J}_{r}= [ X_r , Y_r , Z_r]^T \in \mathbb{R}^{3 \times 1}$ to ensure the re-projected 2D pose $\boldsymbol{J}^{proj}_{2D} = [\boldsymbol{x}^{proj}, \boldsymbol{y}^{proj}]^T$ from $\boldsymbol{J}^{src}_{3D}+\boldsymbol{J}_r$ is close to $\boldsymbol{J}^{tar}_{2D}$ as much as possible with respect to both position and scale. 
Denote the operation of GPA as $\mathcal{F}$,
\begin{equation}
    \label{equ:GPR}
    \hat{\boldsymbol{J}}_{r} = \mathcal{F}(\boldsymbol{J}^{tar}_{2D}, \boldsymbol{J}^{src}_{3D}),
\end{equation}
where $\hat{\boldsymbol{J}}_{r}$ is the estimated 3D root position after translation.

We demonstrate how to obtain $\hat{\boldsymbol{J}}_{r}$ as follows. Assume that the camera intrinsic parameters are given. The projection from 3D joints to 2D joints after translation should obey the perspective projection function as follows:
\begin{equation}
\begin{aligned}
    \label{equ:p}
    & x_i^{proj} = \frac{f_x(X_i^{src}+X_r)}{Z_i^{src}+Z_r}+c_x, \\
    & y_i^{proj} = \frac{f_y(Y_i^{src}+Y_r)}{Z_i^{src}+Z_r}+c_y,
\end{aligned}
\end{equation}
% We assume that the camera intrinsics parameters are given, and the projection from 3D joints to 2D joints should obey the perspective projection function as follows:
% \begin{equation}
%     \label{equ:p}
%     x_i^{\prime}=\frac{x_i-c_x}{f_x} = \frac{X_i+X_r}{Z_i+Z_r}, y_i^{\prime}=\frac{y_i-c_y}{f_y} = \frac{Y_i+Y_r}{Z_i+Z_r}
% \end{equation}
where $i$ denotes the $i$-th joint, $(f_x, f_y), (c_x, c_y)$ denote focal length and principal point respectively. Note that $Z_i^{src}\ll Z_r$ since the absolute depth of the root joint of the person $Z_r$ is usually much larger than depth offset $Z_i^{src}$ of a certain joint relative to the root joint. Therefore, we assume that $Z_i^{src}+ Z_r \approx Z_r$ holds. Then Eq.~(\ref{equ:p}) becomes
\begin{equation}
\begin{aligned}
    \label{equ:p2}
    & x_i^{proj} \approx \frac{f_x(X_i^{src}+X_r)}{Z_r}+c_x, \\
    & y_i^{proj} \approx \frac{f_y(Y_i^{src}+Y_r)}{Z_r}+c_y,
\end{aligned}
\end{equation}

% Then we need to align $\boldsymbol{J}^{proj}_{2D}$ with $\boldsymbol{J}^{tar}_{2D}$ with respect to both scale and position. 
To achieve the similar scale with $\boldsymbol{J}^{tar}_{2D}$ for $\boldsymbol{J}^{proj}_{2D}$, we ensure that the 2D boxes should have similar size with the following constraint,
\begin{equation}
\label{equ:size}
\Delta x^{proj}+\Delta y^{proj} = \Delta x^{tar}+\Delta y^{tar},
\end{equation}
where $\Delta$ denotes the difference between the max-min coordinates of 2D joints, \textit{i.e.}, the width and height of the 2D box. Combine Eq.~{\ref{equ:p2}} and Eq.~{\ref{equ:size}}, we can get the approximated $Z_r$,
\begin{equation}
    \hat{Z}_r \approx \frac{f_x\Delta X^{src}+ f_y\Delta Y^{src}}{\Delta x^{tar} + \Delta y^{tar}}.
\end{equation}

Then, we can align the global (root) position between $\boldsymbol{J}^{proj}_{2D}$ and $\boldsymbol{J}^{tar}_{2D}$ by
\begin{equation}
    x_r^{proj} = x_r^{tar}, \ \ \ \ y_r^{proj} = y_r^{tar},
\end{equation}
where $[x_r, y_r]^T$ represents the 2D root joint. Combined with Eq.~(\ref{equ:p2}), we can get estimated $\hat{X}_r$ and $\hat{Y}_r$,
\begin{equation}
    \hat{X}_r = \frac{\hat{Z}_r(x_r^{tar}-c_x)}{f_x}, \ \ \ \ 
    \hat{Y}_r = \frac{\hat{Z}_r(y_r^{tar}-c_y)}{f_y}.
\end{equation}
Finally, we can obtain the translated root position $\hat{\boldsymbol{J}}_{r} = [\hat{X}_r,\hat{Y}_r,\hat{Z}_r]^T$. The ultimate goal of Global Position Alignment is to align the 2D pose distributions of the target domain and the generated domain (after going through the full data augmentation pipeline) in terms of scale and position. There will be cases of large discrepancies between $J_{3D}^{src}$ and $J_{2D}^{tar}$ when pairing randomly on individuals. However, this is mitigated because 1) we only use box information rather than complete poses and 2) we randomly shuffle the pairing method between each epoch, which further increases diversity and avoids individual discrepancies in a statistical sense.

% We define 2D scale of poses, $\mathcal{S}_{2D}$, as the average of 2D pose width and height
% %, which build the connection between the distributions of 2D pose and 3D pose with the only variable $Z_r$:
% \begin{equation}
% \begin{aligned}
%     \mathcal{S}_{2D} &= \frac{1}{2} \left ( \Delta x^{\prime} + \Delta y^{\prime} \right )\\
%     &= \frac{1}{2} \left [ \Delta \left (\frac{X_i+X_r}{Z_i+Z_r} \right ) + \Delta \left (\frac{Y_i+Y_r}{Z_i+Z_r} \right ) \right ] \\
%     & \approx \frac{1}{2Z_r} \left ( \Delta X + \Delta Y \right )
% \end{aligned}
% \end{equation}
% where $\Delta$ denotes the difference between the max-min coordinates of 2D joints. Therefore the depth $Z_r$ can be estimated by solving the equation:
% \begin{equation}
%     Z_r = \frac{1}{2} \frac{\Delta X+ \Delta Y}{\Delta x + \Delta y}
% \end{equation}

% Finally, applying the projection function~\ref{equ:p} on root joint gives:
% \begin{equation}
% \begin{cases}
%     X_r = x_i (Z_i + Z_r) - X_i = x_i Z_r \\
%     Y_r = y_i (Z_i + Z_r) - Y_i = y_i Z_r
% \end{cases}
% \end{equation}
% since $X_i, Y_i, Z_i = \boldsymbol{0}$ for root joint.

\subsection{Local Pose Augmentation (LPA)}
\label{subsec:ra}

Inspired by \textit{PoseAug}~\cite{gong2021poseaug}, We also apply local pose augmentation (LPA) to enhance the diversity of 2D-3D pose mappings. The augmentation transformation of 3D pose can be decoupled into perturbations of bone vector, bone length, and rotation. We design an adversarial augmentation framework, which consists of an augmentation generator to generate 3D pose transformation parameters and an anchor discriminator to ensure the realisticity, quality, and diversity of generated pose pairs. The generator and discriminator are jointly end-to-end trained following the GAN style.

\noindent\textbf{Augmentation generator}. Following \textit{PoseAug}~\cite{gong2021poseaug}, we propose an augmentation generator denoted as $\mathcal{G}$ with parameters $\boldsymbol{\theta}_\mathcal{G}$. Unlike vanilla GAN-style generator, we take a sample 3D pose from source dataset $J^{src}_{3D}$ as the condition $\mathcal{G}_{cond}$, which is  concatenated with a noise vector $\boldsymbol{z}$ as the input of $\mathcal{G}$, according to the suggestion in~\cite{gholami2022adaptpose, bousmalis2017unsupervised}. The input 3D pose is converted to bone direction vectors representing the joint angle and bone length. Augmentation generator $\mathcal{G}$ generates three types of 3D pose transformations: bone angle difference, bone length difference, and global rotation. The augmentation process can be represented as
\begin{equation}
    \boldsymbol{J}^{aug}_{3D} = \mathcal{G}(\boldsymbol{J}^{src}_{3D}, \boldsymbol{z};\boldsymbol{\theta}_{\mathcal{G}}),\ \ \ \  \boldsymbol{J}^{aug}_{2D} = f_p(\boldsymbol{J}^{aug}_{3D}; \boldsymbol{\theta}_{cam})
\end{equation}
where $(\boldsymbol{J}^{aug}_{3D}, \boldsymbol{J}^{aug}_{2D})$ denotes the augmented paired 2D-3D pose pairs, $f_p$ denotes the camera projection function~\ref{equ:p}, and $\boldsymbol{\theta}_{cam}$ denotes the given camera intrinsic parameters in target dataset.

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/pipeline.pdf}
    \caption{\small The pose transformations applied on 3D pose from source dataset~(column 1). Bone angle~(column 2), bone length~(column 3), and rotation~(column 4) are root-relative transformations given by local pose augmentation (LPA). Translation~(column 5) is given by global position alignment~(GPA). Target~(column 6) is the sampled 2D pose from target dataset used for GPA.}
    \label{fig:pipeline}
\end{figure*}

% \noindent\textbf{Domain Discriminator} Let $\mathcal{D}_{2D}$ denotes the discriminator for 2D pose with parameters $\theta_{\mathcal{D}_{2D}}$. It takes $J_{2D}$ sampled from both $J^{tar}_{2D}$ and $J^{aug}_{2D}$ as input. Discriminator $\mathcal{D}_{2D}$ works as a domain discriminator to distinguish whether the input is from sampled from $J^{tar}_{2D}$ or $J^{aug}_{2D}$.

\noindent\textbf{Anchor discriminator}. Let $\mathcal{D}_{3D}$ denote the discriminator for 3D pose with parameters $\boldsymbol{\theta}_{\mathcal{D}_{3D}}$. It takes root-relative $\boldsymbol{J}_{3D}$ sampled from both $\boldsymbol{J}^{src}_{3D}$ and $\boldsymbol{J}^{aug}_{3D}$ as input. Discriminator $\mathcal{D}_{3D}$ works as an anchor discriminator to ensure the augmented pose $\boldsymbol{J}^{aug}_{3D}$ is reasonable. Inspired by Kinematic Chain Space (KCS)~\cite{wandt2019repnet, wandt2018kinematic}, we use KCS representation of 3D pose as input instead of 3D joints. With the help of KCS representation, the generated 3D poses $\boldsymbol{J}^{aug}_{3D}$ are no longer limited in the source domain.
%Since we assume that there is no access to 3D pose ground truth in target dataset $J^{tar}_{3D}$, we essentially encourage the augmented pose closed to the pose from source dataset in 3D perspective. In this case, discriminator $\mathcal{D}_{3D}$ indeed has a risk of conflict with $\mathcal{D}_{2D}$. However, according to D2GAN~\cite{nguyen2017dual}, we believe that exploiting the complementary statistical properties of two discriminators to improve both the quality and diversity of samples generated from the generator is possible.

\subsection{Training}
\label{subsec:training}
Previous works~\cite{gong2021poseaug, gholami2022adaptpose, yang20183d} train the adversarial pose augmentation framework with the loss function of vanilla GAN~\cite{goodfellow2020generative} or least squares GAN (LSGAN)~\cite{mao2017least}. We argue that training based on Wasserstein distance~\cite{shen2018wasserstein, deng2021synchronized} can be trained stably and provide better augmented poses.

\noindent\textbf{Discriminator loss}. We adopt WGAN~\cite{arjovsky2017wasserstein} loss for the
%discriminator loss $\mathcal{L}_{\mathcal{D}_{2D}}$ and 
anchor discriminator $\mathcal{L}_{\mathcal{D}_{3D}}$:
% \begin{equation}
%     \mathcal{L}_{\mathcal{D}_{2D}} = \mathbb{E} \left [ \mathcal{D}_{2D} (J^{aug}_{2D}) \right]- \mathbb{E} \left [ \mathcal{D}_{2D}(J^{tar}_{2D}) \right ]
% \end{equation}
\begin{equation}
    \mathcal{L}_{\mathcal{D}_{3D}} = \mathbb{E}_{\boldsymbol{x} \sim \boldsymbol{J}^{aug}_{3D}} \left [ \mathcal{D}_{3D}(\boldsymbol{x}) \right]- \mathbb{E}_{\boldsymbol{x} \sim \boldsymbol{J}^{src}_{3D}} \left [ \mathcal{D}_{3D}(\boldsymbol{x}) \right]
\end{equation}
% \begin{equation}
%     \mathcal{L}_{\mathcal{D}_{3D}} = \mathbb{E} \left [ \mathcal{D}_{3D} (J^{aug}_{3D}) \right]- \mathbb{E} \left [ \mathcal{D}_{3D}(J^{src}_{3D}) \right ]
% \end{equation}

\noindent\textbf{Generator loss}. The adversarial loss of 
 the augmentation generator is the feedback from the anchor discriminator.
\begin{equation}
    \mathcal{L}_{\mathcal{G}} = - \mathbb{E}_{\boldsymbol{x} \sim \mathcal{G}(\boldsymbol{J}^{src}_{3D},\boldsymbol{z})} \left [ \mathcal{D}_{3D}(\boldsymbol{x})) \right]
\end{equation}
The discriminator and generator are trained iteratively. 

\noindent\textbf{Lifting network loss}. The standard Mean Squared Error (MSE) loss is adopted to the lifting network $\mathcal{P}$,
\begin{equation}
    \mathcal{L}_{\mathcal{P}} = \Vert \boldsymbol{J}_{GT} - \boldsymbol{J} \Vert_2^2,
\end{equation}
where $\boldsymbol{J}_{GT}$ and $\boldsymbol{J}$ are ground truth and estimated 3D joints, respectively. The generator and the discriminator need to be warmed up for $w$ epoch before training lifting network.

% While training one, the gradient of the other is frozen. % At the beginning of training, we only train the local pose augmentation module to ensure the quality and stability of augmented pose pairs. After 10 epochs for warmup, we train the lifting network with both augmented pose pairs and original pose pairs.

\begin{algorithm}[ht]
\DontPrintSemicolon
  \KwIn{$\boldsymbol{J}=\{\boldsymbol{J}^{src}_{3D},\boldsymbol{J}^{tar}_{2D},\boldsymbol{\theta}_{cam}\}$}
  \BlankLine
  \For{$t\leftarrow 1$ \KwTo $T$}{
    \For{$i\leftarrow 1$ \KwTo $I$}{
        \textbf{freeze} $\mathcal{G}$\\
        \emph{sample and generate a batch data}\\
        $\boldsymbol{J}^{aug}_{3D}, \boldsymbol{J}^{aug}_{2D} \gets \mathcal{G}(\boldsymbol{J}^{src}_{3D}, \boldsymbol{z}; \boldsymbol{\theta}_{\mathcal{G}})$\\
        \emph{train discriminator}\\
        $\mathcal{L}_{\mathcal{D}_{3D}} = \mathbb{E}_{\boldsymbol{x} \sim \boldsymbol{J}^{aug}_{3D}} \left [ \mathcal{D}_{3D}(\boldsymbol{x}; \boldsymbol{\theta}_{\mathcal{D}_{3D}}) \right]- \mathbb{E}_{\boldsymbol{x} \sim \boldsymbol{J}^{src}_{3D}} \left [ \mathcal{D}_{3D}(\boldsymbol{x}; \boldsymbol{\theta}_{\mathcal{D}_{3D}}) \right]$\\
        \textbf{update} $\boldsymbol{\theta}_{\mathcal{D}_{3D}}$\\
        \emph{train generator every $n$ iters}\\
        \If{$i$ in every $n$ iters}{
            \textbf{freeze} $\boldsymbol{\theta}_{\mathcal{D}_{3D}}$\\
            \emph{generate a batch data}
            $\boldsymbol{J}^{aug}_{3D}, \boldsymbol{J}^{aug}_{2D} \gets \mathcal{G}(\boldsymbol{J}^{src}_{3D}, \boldsymbol{z}; \boldsymbol{\theta}_{\mathcal{G}})$\\
            $\mathcal{L}_{\mathcal{G}} = - \mathbb{E}_{\boldsymbol{x} \sim \boldsymbol{J}^{aug}_{3D}} \left [ \mathcal{D}_{3D}(\boldsymbol{x}; \boldsymbol{\theta}_{\mathcal{D}_{3D}}) \right]$\\
            \textbf{update} $\boldsymbol{\theta}_{\mathcal{G}}$\\
        }
    }
    \emph{warmup to ensure stable augmentation}\\
    \If{$t\geq w$}{
        \emph{mixup augmented and source data}\\
        $\boldsymbol{J}^{mix}_{2D}, \boldsymbol{J}^{mix}_{3D} \gets \left ( \boldsymbol{J}^{src}_{2D}, \boldsymbol{J}^{src}_{3D}\right ), \left ( \boldsymbol{J}^{aug}_{2D}, \boldsymbol{J}^{aug}_{3D}\right )$\\
        \emph{train lifting network with mixed data}\\
        $\mathcal{L}_{\mathcal{P}} = \text{MSE} \left (\mathcal{P}\left( \boldsymbol{J}^{mix}_{2D};\boldsymbol{\theta}_{\mathcal{P}}\right), \boldsymbol{J}^{mix}_{3D} \right)$\\
        \textbf{update} $\boldsymbol{\theta}_{\mathcal{P}}$\\
    }
}

\caption{The training pipeline of our method}
\label{training pipeline}
\end{algorithm}