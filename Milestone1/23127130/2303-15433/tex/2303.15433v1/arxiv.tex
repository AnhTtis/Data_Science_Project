\documentclass[10pt,twocolumn,letterpaper,pagenumbers]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{subfig}
\usepackage[dvipsnames]{xcolor}
\usepackage{url}
\def\UrlBreaks{\do\/\do-}

\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
% Include other packages here, before hyperref.
% \usepackage[pagenumbers]{iccv} % To force page numbers, e.g. for an arXiv version

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\crefname{equation}{Eq.}{Eqs.}

\newcommand{\minisection}[1]{\noindent{\textbf{#1}}}
\newcommand{\anh}[1]{\textcolor{cyan}{[Anh: #1]}}
\newcommand{\hao}[1]{\textcolor{magenta}{[Hao: #1]}}
\newcommand{\quan}[1]{\textcolor{blue}{[Quan: #1]}}
\newcommand{\thanh}[1]{\textcolor{green}{[Thanh: #1]}}
\newcommand{\thuan}[1]{\textcolor{orange}{[Thuan: #1]}}
\newcommand{\ngoc}[1]{\textcolor{red}{[Ngoc: #1]}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{11655} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi
\pdfminorversion=5 
\pdfcompresslevel=9
\pdfobjcompresslevel=2
\begin{document}

%%%%%%%%% TITLE
\title{Anti-DreamBooth: Protecting users from personalized text-to-image synthesis}

\author{Thanh Van Le, Hao Phung, Thuan Hoang Nguyen, Quan Dao, Ngoc Tran, Anh Tran\\
VinAI Research\\
{\tt\small \{thanhlv19, haopt12, thuannh5, quandm7, ngoctnq, anhtt152\}@vinai.io}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi
% \def\thefootnote{\textsuperscript{\textdagger}}\footnotetext{Equal contributions.}


%%%%%%%%% ABSTRACT
\begin{abstract}
   Text-to-image diffusion models are nothing but a revolution, allowing anyone, even without design skills, to create realistic images from simple text inputs. With powerful personalization tools like DreamBooth, they can generate images of a specific person just by learning from his/her few reference images. However, when misused, such a powerful and convenient tool can produce fake news or disturbing content targeting any individual victim, posing a severe negative social impact. In this paper, we explore a defense system called Anti-DreamBooth against such malicious use of DreamBooth. The system aims to add subtle noise perturbation to each user's image before publishing in order to disrupt the generation quality of any DreamBooth model trained on these perturbed images. We investigate a wide range of algorithms for perturbation optimization and extensively evaluate them on two facial datasets over various text-to-image model versions. Despite the complicated formulation of DreamBooth and Diffusion-based text-to-image models, our methods effectively defend users from the malicious use of those models. Their effectiveness withstands even adverse conditions, such as model or prompt/term mismatching between training and testing. Our code will be available at \href{https://github.com/VinAIResearch/Anti-DreamBooth.git}{https://github.com/VinAIResearch/Anti-DreamBooth.git}. %Anti-DreamBooth successfully disrupts a commercial AI service.
\end{abstract}

%Recently, celebrities have been facing constant risks from DeepFakes, which is a GAN-based method to create visual misinformation. A particular example of DeepFakes is the alteration of facial features in an image, such as swapping the face of one public figure to another. Several countermeasures have been proposed to prevent such threat; however, less attention has been given to DreamBooth, a diffusion-based variant. In this paper, we explore a defense system, namely Anti-DreamBooth, against malicious use of DreamBooth. Extensive experiments on three public text-to-image diffusion models trained on two benchmark face datasets show that our method, namely Anti-DreamBooth, achieves promising performance and outperforms multiple baseline methods.

%%%%%%%%% BODY TEXT
\section{Introduction}
Within a few years, denoising diffusion models \cite{ho2020denoising, song2020denoising, rombach2022high} have revolutionized image generation studies, allowing producing images with realistic quality and diverse content \cite{dhariwal2021diffusion}. They especially succeed when being combined with language \cite{2020t5} or vision-language models \cite{radford2021learning} for text-to-image generation. Large models \cite{ramesh2022hierarchical,Midjourney,rombach2022high,saharia2022photorealistic,balaji2022ediffi}
% such as DALL-E 2 \cite{ramesh2022hierarchical}, Midjourney AI \cite{Midjourney}, Stable Diffusion \cite{rombach2022high, SD}, Imagen \cite{saharia2022photorealistic}, and eDiff-I \cite{balaji2022ediffi} 
can produce photo-realistic or artistic images just from simple text description inputs. A user can now generate art within 
% a few hours to 
a few seconds, and a generated drawing even beat professional artists in an art competition \cite{Roose2022Sep}. Photo-realistic synthetic images can be hard to distinguish from real photos \cite{Ingram2022Dec}. Besides, ControlNet \cite{zhang2023adding} offers extra options to control the generation outputs, further boosting the power of the text-to-image models and bringing them closer to mass users.

\begin{figure}
    \centering
    \includegraphics[width=.475\textwidth]{fig/Teaser.pdf}
    \caption{A malicious attacker can collect a user's images to train a personalized text-to-image generator for malicious purposes. Our system, called Anti-DreamBooth, applies imperceptible perturbations to the user's images before releasing, making any personalized generator trained on these images fail to produce usable images, protecting the user from that threat.}
    \label{fig:teaser}
    \vspace{-5mm}
\end{figure}

One extremely useful feature for image generation models is personalization, which allows the models to generate images of a specific subject, given a few reference examples. For instance, one can create images of himself/herself in a fantasy world for fun, or create images of his/her family members as a gift. Textual Inversion \cite{gal2023an} and DreamBooth \cite{ruiz2022dreambooth} are two prominent techniques that offer that impressive ability. While Textual Inversion only optimizes the text embedding inputs representing the target subject, DreamBooth finetunes the text-to-image model itself for better personalization quality. Hence, DreamBooth is particularly popular and has become the core technique in many applications. 

While the mentioned techniques provide a powerful and convenient tool for producing desirable images at will, they also pose a severe risk of being misused. A malicious user can propagate fake news with photo-realistic images of a celebrity generated by DreamBooth. This can be classified as DeepFakes \cite{juefei2022countering}, one of the most serious AI crime threats that has drawn an enormous attention from the media and community in recent years. Besides creating fake news, DreamBooth can be used to issue harmful images targeting specific persons, disrupting their lives and reputations. While the threat of GAN-based DeepFakes techniques is well-known and has drawn much research interest, the danger from DreamBooth has yet to be aware by the community, making its damage, when happening, more dreadful.

This paper discusses how to protect users from malicious personalized text-to-image synthesis. Inspired by DeepFakes's prevention studies \cite{yeh2020disrupting,ruiz2020disrupting,yang2021defending,huang2021initiative,wang2022anti}, we propose to pro-actively defend each user from the DreamBooth threat by injecting subtle adversarial noise into their images before publishing. The noise is designed so that any DreamBooth model trained on these perturbed images fails to produce reasonable-quality images of the target subject. While the proposed mechanism, called Anti-DreamBooth, shares the same goal and objective as the techniques to disrupt GAN-based DeepFakes, it has a different nature due to the complex formulation of diffusion-based text-to-image models and DreamBooth: 
\begin{itemize}
    \item In GAN-based disruption techniques, the defender optimizes the adversarial noise of a single image, targeting a fixed DeepFakes generator. In Anti-DreamBooth, we have to optimize the perturbation noise to disrupt a dynamic, unknown generator that is finetuned from the perturbed images themselves.
    \item GAN-based DeepFakes generator produces each fake image via a single forward step; hence, adversarial noise can be easily learned based on the model's gradient. In contrast, a diffusion-based generator produces each output image via a series of non-deterministic denoising steps, making it impossible to compute the end-to-end gradient for optimization.
    \item Anti-DreamBooth has a more complex setting by considering many distinctive factors, such as the prompt used in training and inference, the text-to-image model structure and pre-trained weights, and more.
\end{itemize}

Despite the complexity mentioned above, we show that the DreamBooth threat can be effectively prevented. Instead of targeting the end-to-end image generation process, we can adapt the adversarial learning process to break each diffusion sampling step. We design different algorithms for adversarial noise generation, and verify their effectiveness in defending DreamBooth attack on two facial benchmarks. Our proposed algorithms successfully break all DreamBooth attempts in the controlled settings, causing the generated images to have prominent visual artifacts. Our proposed defense shows consistent effect when using different text-to-image models and different training text prompts.  More impressively, Anti-DreamBooth maintains its efficiency even under adverse conditions, such as model or prompt/term mismatching between training and testing. 

In summary, our contributions include: (1) We discuss the potential negative impact of personalized text-to-image synthesis, particularly with DreamBooth, and define a new task of defending users from this critical risk, (2) We propose proactively protecting users from the threat by adding adversarial noise to their images before publishing, (3) We design different algorithms for adversarial noise generation, adapting to the step-based diffusion process and finetuning-based DreamBooth procedure, (4) We extensively evaluate our proposed methods on two facial benchmarks and under different configurations. Our best defense works effectively in both convenient and adverse settings.   


\section{Related work}
% stable diffusion
\subsection{Text-to-image generation models} Due to the advent of new large-scale training datasets such as LAION5B \cite{Schuhmann2022LAION5BAO}, text-to-image generative models are advancing rapidly, opening new doors in many visual-based applications and attracting attention from the public. These models can be grouped into four main categories:  auto-regressive \cite{yu2022scaling}, mask-prediction \cite{chang2023muse}, GAN-based \cite{sauer2023stylegan} and diffusion-based approaches, all of which show astounding qualitative and quantitative results. Among these methods, diffusion-based models \cite{rombach2022high, saharia2022photorealistic, balaji2022ediffi, Nichol2021GLIDETP, Ramesh2022HierarchicalTI} have exhibited an exceptional capacity for generating high-quality and easily modifiable images, leading to their widespread adoption in text-to-image synthesis. GLIDE \cite{Nichol2021GLIDETP} is arguably the first to combine a diffusion model with classifier guidance for text-to-image generation. DALL-E 2 \cite{Ramesh2022HierarchicalTI} then improves the quality further using the CLIP text encoder and diffusion-based prior. For better trade-off between efficiency and fidelity, following-up works either introduce coarse-to-fine generation process like Imagen \cite{saharia2022photorealistic} and eDiff-I \cite{balaji2022ediffi} or work on latent space like LDM \cite{rombach2022high}. StableDiffusion \cite{SD}, primarily based on LDM, is the first open-source large model of this type, further boosting the widespread applications of text-to-image synthesis. %However, when maliciously used, these powerful models can negatively impact our society. Therefore, our work focuses on defense mechanisms against that threat to the public.


%These models have grown by leaps and bounds in visual quality and potential applications in just a few years. Due to the advent of diffusion models and data availability, LDM \cite{rombach2022high}, Imagen \cite{saharia2022photorealistic}, and eDiff-I \cite{balaji2022ediffi} are able to produce high-quality images given input prompts that cannot be easily distinguished from human eyes. Besides, there are other types of text-to-image models which include auto-regressive approach \cite{yu2022scaling}, mask-based prediction \cite{chang2023muse}, and GAN-based approach \cite{sauer2023stylegan}. \anh{Too short. Not much information compared with the introduction. Should highlight the diffusion-based since our method only targets diffusion}

% dreambooth
\subsection{Personalization} Customizing the model's outputs for a particular person or object has been a significant aim in the machine-learning community for a long time. Generally, personalized models are commonly observed in recommendation systems \cite{10.1145/3240323.3241729} or federated learning \cite{shamsian2021personalized,SuPerFed}. Within the context of diffusion models, previous research has focused on adapting a pre-trained model to create fresh images based on a particular target idea using natural language cues. Existing methods for personalizing the model either involve adjusting a collection of text embeddings to describe the idea \cite{gal2023an} or fine-tuning the denoising network to connect a less commonly used word-embedding to the novel concept \cite{ruiz2022dreambooth}. For better customization, \cite{kumari2022customdiffusion} propose a novel approach to not only model a new concept of an individual subject by optimizing a small set of parameters of cross-attention layers but also to combine multiple concepts of objects via joint training. Among these tools, DreamBooth \cite{ruiz2022dreambooth} is particularly popular due to its exceptional quality and has become the core technique in many applications. Hence, we focus on defending the risks of malign image generation coming from this technique. %With these powerful tools, users are more vulnerable to the malign risks of image manipulation. Hence, we investigate some potential algorithms to mitigate such societal threat.

%DreamBooth \cite{ruiz2022dreambooth} and Textual Inversion \cite{gal2023an} are two prominent works that offer unprecedented generative results of one instance in various scenarios with just a few reference examples. On the downside, it causes unexpected damage to the reputation of individuals with malign synthetic images. In this work, we introduce adversarial examples to personalized models in order to protect users from malicious purposes. \anh{Not many technical information; even less infor compared with the introduction. Or maybe swap some text between 2 parts}

\subsection{Adversarial attacks}
With the introduction of the Fast Gradient Sign Method (FGSM) attack \cite{Goodfellow2014ExplainingAH}, adversarial vulnerability has become an active field of research in machine learning. The goal of adversarial attacks is to generate a model input that can induce a misclassification while remaining visually indistinguishable from a clean one. Following this foundational work, different attacks with different approaches started to emerge, with more notable ones including: \cite{bim, madry2018towards} being FGSM's iterative versions, \cite{cw} limiting the adversarial perturbation's magnitude implicitly using regularization instead of projection, \cite{deepfool} searching for a close-by decision boundary to cross, etc. For black-box attacks, where the adversary does not have full access to the model weights and gradients, \cite{spsa} estimates the gradient using sampling methods, while \cite{boundary, hsja, square} aim to synthesize a close-by example by searching for the classification boundary, then finding the direction to traverse towards a good adversarial example. Combining them, \cite{autoattack} is an ensemble of various attacks that are commonly used as a benchmark metric, being able to break through gradient obfuscation \cite{grad_obf} with the expectation-over-transformation technique \cite{eot}. 

%\anh{Not fit to this category: Recently, with the increasing attention of the public to diffusion-based models, several attacks, as well as defenses, have been proposed in case of these models being misused. Prompt-Stealing \cite{Shen2023PromptSA} and Photoguard \cite{salman2023raising} are two remarkable techniques, where the former tries to recover prompts used to generate images for more copyright protection and the latter aims to protect users from harmful image editing posed by text-to-image diffusion models.}

\subsection{User protection with image cloaking}
With the rapid development of AI models, their misuse risk has emerged and become critical. Particularly, many models exploit the public images of each individual for malicious purposes. Instead of passively detecting and mitigating these malign actions, many studies propose proactively preventing them from succeeding. The idea is to add subtle noise into users' images before publishing to disrupt any attempt to exploit those images. This approach is called ``image cloaking'', which our proposed methods belong to.

One application of image cloaking is to prevent privacy violations caused by unauthorized face recognition systems. Fawkes \cite{shan2020fawkes} applies targeted attacks to shift the user's identity towards a different reference person in the embedding space. Although it learns the adversarial noise using a surrogate face recognition model, the noise successfully transfers to break other black-box recognizers. Lowkey \cite{cherepanova2021lowkey} further improves the transferability by using an ensemble of surrogate models. It also considers a Gaussian smoothed version of the perturbed image in optimization, improving robustness against different image transformations. AMT-GAN \cite{hu2022protecting} crafts a natural-looking cloak via makeup transfer, while OPOM \cite{zhong2022opom} optimizes person-specific universal privacy masks. 

Another important application of image cloaking is to disrupt GAN-based image manipulation for DeepFakes. Yang et al. \cite{yang2021defending} exploits differentiable image transformations for robust image cloaking. Yeh et al. \cite{yeh2020disrupting} defines new effective objective functions to nullify or distort the image manipulation. Huang et al. \cite{huang2021initiative} addresses personalized DeepFakes techniques by alternating the training of the surrogate model and a perturbation generator. Anti-Forgery \cite{wang2022anti} crafts the perturbation for channels $a$ and $b$ in the $Lab$ color space, aiming for natural-looking and robust cloaking. Lately, UnGANable \cite{li2023unganable} prevents StyleGAN-based image manipulation by breaking its inversion process.

\section{Problem}
\subsection{Background}
\minisection{Adversarial attacks.}
The goal of adversarial attacks is to find an imperceptible perturbation of an input image to mislead the behavior of given models. Typical works have been developed for classification problems where for a model $f$, an adversarial example $x'$ of an input image $x$ is generated to stay visually undetectable while inducing a misclassification $y_\mathrm{true} \ne f(x')$ (untargeted attack), or making the model predict a predefined target label $y_\text{target} = f(x_\mathrm{adv})\ne y_\mathrm{true}$ (targeted attack). The minimal visual difference is usually enforced by bounding the perturbation to be within an $\eta$-ball w.r.t. an $\ell_p$ metrics, that is $\Vert x' - x\Vert_p<\eta$. To achieve this objective, denoting $\Delta = \{\delta: \Vert \delta \Vert_p \leq \eta \}$, we find the optimal perturbation $\delta$ to maximize the classification loss in the untargeted version:
\begin{equation}
    \delta_{\text{adv}} = \argmax_{\delta \in \Delta} \mathcal{L}(f(x+\delta), y_\text{true}),
\end{equation}
or to minimize the loss for the targeted variant:
\begin{equation}
    \delta_{\text{adv}} = \argmin_{\delta \in \Delta} \mathcal{L}(f(x+\delta), y_\text{target}).
    \label{eq:tar_adv}
\end{equation}

% Projected Gradient Descent (PGD) \cite{madry2018towards} is most commonly used attack, where the optimization process is done by getting the steepest descending direction in input space, i.e., the direction within our bounds that gives the largest directional derivative. For the $\ell_p$ case, it's the signum function of the gradient with respect to the input:

% $$
% \argmin_{\Vert v\Vert_p\le \alpha}{v^\top f'(x)} = \alpha\;\mathrm{sgn}\left(\frac{\delta f(x)}{\delta x}\right).
% $$

% This gradient ascent process with step size $\alpha$ is done for however many iteration as set beforehand. To enforce the adversarial example to remain within the $\eta$-ball, PGD applies the projection operator onto the resulting image:

% $$
% \argmin_{\Vert x' - x\Vert_p \le \eta}\Vert x_\mathrm{adv} - x'\Vert_2 = \max(\min(x_\mathrm{adv}, x - \eta), x + \eta),
% $$
% where the max and min operators are applied elementwise.

Projected Gradient Descent (PGD) \cite{madry2018towards} is a commonly used attack based on an iterative optimization process. The updating pipeline to predict $x'$ for untargeted attack is:
\begin{align}
\begin{split}
    & x^\prime_0 = x \\
    & x^\prime_{k} = \Pi_{(x,\eta)}(x^\prime_{k-1} + \alpha\cdot\mathrm{sgn}(   \nabla_x\mathcal{L}(f(x^\prime_{k-1}), y_{true})))
\end{split}
\label{eq:pgd}
\end{align}
where $\Pi_{x,\eta}(z)$ restrains pixel values of $z$ within an $\eta$-ball around the original values in $x$. We acquire the adversarial example $x'$ after a pre-defined number of iterations.

\minisection{Diffusion models}
are a type of generative models \cite{sohl2015deep,ho2020denoising} that decouple the role of generation into two opposing procedures: a forward process and a backward process. While the forward process gradually adds noise to an input image until data distribution becomes pure Gaussian noise, the latter learns to reverse that process to obtain the desired data from random noise. Given input image $x_0 \sim q(x)$, the diffusion process perturbs the data distribution with a noise scheduler $\{\beta_t: \beta_t \in (0, 1)\}_{t=1}^T$ producing increasing levels of noise addition through $T$ steps to obtain a sequence of noisy variables: $\{x_1, x_2, \dots, x_T\}$. % Each variable $x_t$ is sampled from a posterior distribution $q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha_t}}x_0, (1 - \bar{\alpha_t})\mathbf{I})$ where $\alpha_t = 1 - \beta_t$ and $\bar{\alpha_t} = \prod_{s=1}^t \alpha_s$. 
Each variable $x_t$ is constructed via injecting noise at corresponding timestep $t$:
\begin{equation}
    x_{t} = \sqrt{\bar{\alpha_t}}x_0 + \sqrt{1 - \bar{\alpha_t}}\epsilon
\end{equation}
where $\alpha_t = 1 - \beta_t$, $\bar{\alpha_t} = \prod_{s=1}^t \alpha_s$ and $\epsilon \sim \mathcal{N}(0, \mathbf{I})$.

% The backward process learns to denoise from noisy variable $x_{t+1}$ to $x_t$ via estimating the mean of noise added: 
% \begin{equation}
%     p_\theta(x_{t+1}\mid x_t) = \mathcal{N}(x_t; \mu_\theta(x_{t+1}, t), \sigma^2_t\mathbf{I}),
% \end{equation}
% \anh{is the function for backward or forward?}
% where $\sigma_t$ is often equal to $\beta_t$. Thus, the denoising step of $x_{t+1}$ returns to a noise-estimation problem \anh{The noise is not introduced, not defined in the text above}: $\epsilon_t = \epsilon_\theta(x_{t+1}, t)$. 

The backward process learns to denoise from noisy variable $x_{t+1}$ to less-noisy variable $x_t$ via simply estimating the injected noise $\epsilon$ with a parametric neural network $\epsilon_\theta(x_{t+1}, t)$.
% \begin{equation}
%     \epsilon_t = \epsilon_\theta(x_{t+1}, t)
% \end{equation}
The denoising process is trained to minimize $\ell_2$ distance between estimated noise and true noise:
\begin{equation}
    \mathcal{L}_{unc}(\theta, x_0) = \mathbb{E}_{x_0, t, \epsilon \in \mathcal{N}(0,1)} \Vert \epsilon - \epsilon_\theta(x_{t+1}, t ) \Vert_2^2
\end{equation}
where $t$ is uniformly samples within $\{1, \dots, T\}$. 

% Unlike unconditional diffusion models, Prompt-based diffusion models control the sampling process with an additional prompt $c$ to generate photo-realistic outputs which are well-aligned with the text description. The objective is formulated as follows:
% \begin{equation}
%     \mathcal{L}_{cond}(x_0) = \mathbb{E}_{x_0, t, c, \epsilon \in \mathcal{N}(0,1)} \Vert \epsilon - \epsilon_\theta(z_{t+1}, t, c) \Vert_2^2
%     \label{eq:cond_loss}
% \end{equation}

% Thank to input prompts, the model can produce excellent visual quality in comparison with unconditional counterparts. However, the implementation of some prominent methods \cite{saharia2022photorealistic, balaji2022ediffi} are not made available in the public. Alternatively, Stable Diffusion has released the pre-trained weights based on Hugging Face implementation to facilitate future research in the community. Therefore, we mainly focus on learning adversarial examples for Stable Diffusion which is mainly based on LDM.

\minisection{Prompt-based Diffusion Models.}
Unlike unconditional diffusion models, prompt-based diffusion models control the sampling process with an additional prompt $c$ to generate photo-realistic outputs which are well-aligned with the text description. The objective is formulated as follows:
\begin{equation}
    \mathcal{L}_{cond}(\theta, x_0) = \mathbb{E}_{x_0, t, c, \epsilon \in \mathcal{N}(0,1)} \Vert \epsilon - \epsilon_\theta(x_{t+1}, t, c) \Vert_2^2
    \label{eq:cond_loss}
\end{equation}
Thanks to prompt condition, the model can produce more excellent performance in terms of  visual quality than its unconditional counterparts. However, the implementation of most prominent methods \cite{saharia2022photorealistic, balaji2022ediffi} are not publicly available. Alternatively, Stable Diffusion has released the pre-trained weights based on Hugging Face implementation \cite{von-platen-etal-2022-diffusers} to facilitate research in the community. Hence, we mainly perform experiments on different versions of Stable Diffusion. % which are mainly based on LDM.

% In contrary to conventional diffusion models, LDM performs these processes on latent space instead of pixel space which is beneficial from the dimension reduction of pretrained autoencoder. Given input image $x_0 \in \mathbb{R}^{3\times H \times W}$, it is then encoded to a latent map $z_0 \in \mathbb{R}^{c \times h \times w}$ via an encoder $\mathcal{E}$ where the latent dimensions $h = H/f$ and $w = W/f$ are determined through a downsampling factor $f$. Lately, StableDiffusion has built upon this powerful framework to produce better prompt-based image generation through a series of advancements (e.g. 1.4, 1.5, 2.1). Specifically, a prompt-based diffusion model is conditioned on a given text $c$ to produce output image that is aligned with text description. The objective is slightly modified to present input condition in the sampling process:
% \begin{equation}
%     \mathcal{L}_{cond}(x_0) = \mathbb{E}_{\mathcal{E}(x_0), t, c, \epsilon \in \mathcal{N}(0,1)} \Vert \epsilon - \epsilon_\theta(z_{t+1}, t, c) \Vert_2^2
%     \label{eq:cond_loss}
% \end{equation}

% In this work, we perform adversarial attacks on different versions of StableDiffusion to demonstrate the effectiveness of our method against various models.

\minisection{DreamBooth} is a finetuning technique to personalize text-to-image diffusion models for instance of interest. This technique has two aims. First, it enforces the model to learn to reconstruct the user's images, with a generic prompt $c$ such as ``a photo of \textit{sks} [class noun]'', with \textit{sks} is a special term denoting the target user, and ``[class noun]'' is the object type, which can be ``person'' for human subject. To train this, DreamBooth employs the base loss of diffusion models in \cref{eq:cond_loss}, with $x_0$ is each user's reference image. Second, it further introduces a prior preservation loss to prevent overfitting and text-shifting problems when only a small set of instance examples is used. More precisely, it uses a generic prior prompt $c_{pr}$, e.g.,``a photo of [class noun]", and enforces the model to reproduce instance examples randomly generated from that prior prompt using the original weights $\theta_{ori}$. The training objective is the combination of two objectives:
\begin{multline}
    \mathcal{L}_{db}(\theta, x_0) =
    \mathbb{E}_{x_0, t, t'} \Vert \epsilon - \epsilon_\theta(x_{t+1}, t, c) \Vert_2^2 \\ + \lambda \Vert \epsilon' - \epsilon_{\theta_{ori}}(x_{t'+1}, t', c_{pr}) \Vert_2^2
    \label{eq:Ldb}
\end{multline}
where $\epsilon, \epsilon'$ are both sampled from $\mathcal{N}(0, \mathbf{I})$ and $\lambda$ emphasizes the importance of the prior term. While DreamBooth was originally designed for Imagen~\cite{saharia2022photorealistic}, it was quickly adopted for any text-to-image generator.

\subsection{Problem definition}
DreamBooth is a powerful tool to generate photo-realistic outputs of a target instance with rich context beyond the range of reference samples. With such an impressive capacity, DreamBooth can be a double-edged sword. When misused, it can generate harmful images toward the target individual. To mitigate this phenomenon, we propose to craft an imperceptible perturbation added to each user's image that can disrupt the finetuned DreamBooth models to generate distorted images with noticeable artifacts. We define the problem formally below.

Denote $\mathcal{X}$ as the set of images of the person to protect. For each image $x \in \mathcal{X}$, we add an adversarial perturbation $\delta$ and publish the modified image $x' = x + \delta$, while keeping the original one private. The published image set is called $\mathcal{X}'$. An adversary can collect a small image set of that person $\mathcal{X}'_{db} = \{x^{(i)} + \delta^{(i)} \}_{i=1}^{N_{db}} \subset \mathcal{X}'$. He then uses that set as a reference to finetune a text-to-image generator $\epsilon_{\theta}$, following the DreamBooth algorithm, to get the optimal hyper-parameters $\theta^*$.
% \begin{equation}
%     \theta^* = \argmin_{\theta} \sum_{i=1}^{N_{db}} \mathcal{L}_{db}(x^{(i)} + \delta^{(i)})
% \end{equation}
The general objective is to optimize the adversarial noise $\Delta_{db} = \{\delta^{(i)}\}_{i=1}^{N_{db}}$ that minimizes the personalized generation ability of that DreamBooth model:
\begin{align}
\begin{split}
    & \Delta^*_{db}  = \argmin_{\Delta_{db}} {\mathcal{A}(\epsilon_{\theta^*}, \mathcal{X}}),\\
    \text{s.t.} \quad & \theta^*  = \argmin_{\theta} \sum_{i=1}^{N_{db}} \mathcal{L}_{db}(\theta, x^{(i)} + \delta^{(i)}),\\
    \text{and} \quad & \Vert \delta^{(i)} \Vert_p \leq \eta \quad \forall i \in \{1, 2,..,N_{db}\},
\end{split}
\label{eq:objective}
\end{align}
where $\mathcal{L}_{db}$ is defined in Eq. \ref{eq:Ldb} and $\mathcal{A}(\epsilon_{\theta^*}, \mathcal{X})$ is some personalization evaluation function that assesses the quality of images generated by the DreamBooth model $\epsilon_{\theta^*}$ and the identity correctness based on the reference image set $\mathcal{X}$.
%As our attack methods are designed to counter the behaviour of DreamBooth for the sake of users, it can be considered as a defense approach to protect user from malign purposes.

However, it is hard to define a unified evaluation function $\mathcal{A}$. A defense succeeds when the DreamBooth-generated images satisfy one of the criteria: (1) awful quality due to extreme noise, blur, distortion, or noticeable artifacts, (2) shifted content with none or unrecognizable human subjects, (3) mismatched subject identity. Even when we focus on the first criteria, there is no all-in-one image quality assessment metric. Instead, we can use simpler objective functions disrupting the DreamBooth training to achieve the same goal.

We further divide the defense settings into categories, from easy to hard: convenient, adverse, and uncontrolled.

\minisection{Convenient setting.} In this setting, we have prior knowledge about the pretrained text-to-image generator, training term (e.g., ``sks''), and training prompt $c$ the attacker will use. While this setting sounds restricted, it is practical. First, the pretrained generator has to be high-quality and open-source. %, and only a few options are available. 
% Building such a model requires massive  resources; only big AI companies can do so, and they tend to keep the models private. 
So far, only Stable Diffusion has been made publicly available with several versions released. Second, people often use the default training term and prompt provided in DreamBooth's code. This setting can be considered as ``white-box''. % from the perspective of adversarial attack.

\minisection{Adverse settings.} In these settings, the pretrained text-to-image generator, training term, or training prompt used by the adversary is unknown. The defense method, if needed, can use a surrogate component that potentially mismatches the actual one to craft the adversarial noises. These settings can be considered as ``gray-box''.

\minisection{Uncontrolled setting.} This is an extra, advanced setting in which some of the user's clean images are leaked to the public without our control. The adversary, therefore, can collect a mix of perturbed and clean images $\mathcal{X}'_{db} = \mathcal{X}'_{adv} \cup \mathcal{X}_{cl}$, with $\mathcal{X}'_{adv} \subset \mathcal{X}'$ and $\mathcal{X}_{cl} \subset \mathcal{X}$. This setting is pretty challenging since the DreamBooth model can learn from unperturbed photos to generate reasonable personalized images.

% \minisection{User}
% \minisection{DreamBooth Trainer}
% \minisection{Assumptions and Limitations}

\section{Proposed defense methods}
\subsection{Overall direction}
As discussed, instead of defining some evaluation function $\mathcal{A}$ for optimization (Eq. \ref{eq:objective}), we can aim to attack the learning process of DreamBooth. As the DreamBooth model overfits the adversarial images, we can trick it into performing worse in reconstructing clean images:
\begin{align}
\begin{split}
    \delta^{*(i)} = & \argmax_{\delta^{(i)}} \mathcal{L}_{cond}(\theta^*, x^{(i)}), \forall i \in \{1,..,N_{db}\},\\
    \text{s.t.} \quad & \theta^*  = \argmin_{\theta} \sum_{i=1}^{N_{db}} \mathcal{L}_{db}(\theta, x^{(i)} + \delta^{(i)}),\\
    \text{and} \quad & \Vert \delta^{(i)} \Vert_p \leq \eta \quad \forall i \in \{1,..,N_{db}\},
\end{split}
\label{eq:objective_new}
\end{align}
where $\mathcal{L}_{cond}$ and $\mathcal{L}_{db}$ are defined in Eq. \ref{eq:cond_loss} and \ref{eq:Ldb}. Note that, unlike traditional adversarial attacks, our loss functions are computed only at a randomly-chosen timestep in the denoising sequence during training. Still, this scheme is effective in breaking the generation output (Sec. \ref{sec:exp}). %, as will be illustrated in Sec. \ref{sec:exp}.

\subsection{Algorithms}\label{sec:algorithms}
The problem in Eq. \ref{eq:objective_new} is still a challenging bi-level optimization. We define different methods to approximate its solution based on prominent techniques used in literature.

\minisection{Fully-trained Surrogate Model Guidance (FSMG).}
\label{minisec:fsmg}
Most previous image cloaking approaches \cite{shan2020fawkes,shan2020fawkes,yeh2020disrupting} employ a model trained on clean data as a surrogate to guide the adversarial attack. We can naively follow that direction by using a surrogate DreamBooth model with hyper-parameters $\theta_{\text{clean}}$ fully finetuned from a small subset of samples $\mathcal{X}_A \subset \mathcal{X}$. This set does not need to cover the target images $\mathcal{X}_{db} = \{x^{(i)}\}_{i=1}^{N_{db}}$; it can be fixed, allowing the surrogate model to be learned once regardless of the constant change of $\mathcal{X}$ and $\mathcal{X}_{db}$. After getting $\theta_{clean}$, we can use it as guidance to find optimal noise for each target image $\delta^{*(i)} = \argmax_{\delta^{(i)}} \mathcal{L}_{cond}(\theta_{\text{clean}}, x^{(i)} + \delta^{(i)})$. By doing so, we expect any DreamBooth model finetuned from the perturbed samples to stay away from $\theta_{clean}$.

%Following existing methods of adversarial attack \cite{madry2018towards, Goodfellow2014ExplainingAH}, we exploit the sign of the gradient to update the adversarial samples. Such gradient is hard to analytically compute so we estimate it with Monte Carlo sampling. Let $x_0^{(i)}$ be the adversarial sample at iteration $i$th, the adversarial example of the $(i + 1)$-th iteration is generated with a signed gradient ascent with step length $\alpha$ of the denoising objective \cref{eq:cond_loss}

\minisection{Alternating Surrogate and Perturbation Learning (ASPL).}
Using a surrogate model full-trained on clean data may not be a good approximation to solve the problem in Eq. \ref{eq:objective_new}. Inspired by \cite{huang2021initiative}, we propose to incorporate the training of the surrogate DreamBooth model with the perturbation learning in an alternating manner. The surrogate model $\epsilon_\theta$ is first initiated with the pretrained weights. In each iteration, a clone version $\epsilon'_{\theta'}$ is finetuned on the reference clean data $\mathcal{X}_{A}$, following \cref{eq:Ldb}. This model is then utilized to expedite the learning of adversarial noises $\delta^{(i)}$ in the current loop. Finally, we update the actual surrogate model $\epsilon_\theta$ on the updated adversarial samples, and move to the next training iteration. We provide a snippet for one training iteration in Eq. \ref{eq:ASPL}. With such a procedure, the surrogate model better mimics the true models trained by the malicious DreamBooth users since it is only trained on perturbed data.
\begin{align}
\begin{split}
    \theta' \  \  \: & \leftarrow \theta.\text{clone()}\\
    \theta' \  \  \: & \leftarrow \argmin_{\theta'} \sum_{x \in \mathcal{X}_A} \mathcal{L}_{db}(\theta', x)\\
    \delta^{(i)} & \leftarrow \argmax_{\delta^{(i)}} \mathcal{L}_{cond}(\theta', x^{(i)}+\delta^{(i)})\\ %, i = 1,..,N_{db}\\
    \theta \  \  \: & \leftarrow \argmin_{\theta} \sum_{i=1}^{N_{db}} \mathcal{L}_{db}(\theta, x^{(i)} + \delta^{(i)}).
\end{split}
\label{eq:ASPL}
\end{align}
% \minisection{Truncated Unroll Attack (TUA).}
% % Ngoc's TUA = noctua :) can't put this name in bc double blind
% % adding targeted and we get te tua
% Slightly different from ASPL, this algorithm continuously attacks only the single-epoch-finetuned surrogate model:

% \begin{align}
% \begin{split}
%     % \delta^{(0)} \  \  \: & \leftarrow 0 \\
%     \theta' \  \  \: & \leftarrow \sum_{i=1}^{N_{db}} \mathcal{L}_{db}(\theta, x^{(i)} + \delta^{(i)})\\
%     \delta^{(i)} & \leftarrow \argmax_{\delta^{(i)}} \mathcal{L}_{cond}(\theta', x^{(i)}+\delta^{(i)})\\ %, i = 1,..,N_{db}\\
% \end{split}
% \label{eq:TUA}
% \end{align}

% Taking inspiration from the concept of \textit{training in the same way as testing} from meta-learning \cite{weng2018metalearning}, we generate our adversarial examples conditioned on the surrogate model \textit{after} the finetuning process. Hence, we first finetune the surrogate model on our adversarial images and achieve a substitute model to the DreamBooth one. As this model yields a gradually improved output after finetune epoch passes, our goal is to guide these outputs away from the intended behavior. To achieve that, we backward-propagate the full finetuning process to generate our new adversarial examples that would further guide our surrogate model astray. Note that this is effectively a second-order derivative, as we are not taking the gradient of the loss w.r.t. to our adversarial examples, but rather that of the gradient of the loss w.r.t. to the model's weights:

% \[
% \delta \leftarrow \delta + \dfrac{\partial\mathcal{L}_\mathrm{cond}\left(
% \theta - \dfrac{\partial\mathcal{L}_\mathrm{db}(\theta, x+\delta)}{\partial\theta},
% x+\delta\right)}{\partial\delta}
% \]

% Viewing the finetune process of the DreamBooth model as an unrolled computation process \cite{pes}, we can then apply computation graph truncation to just one epoch of finetuning as an approximation, giving us our final algorithm.

% \minisection{CLIP-based guidance}
% \minisection{Target shift}
\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{l|cccc|cccc}
        \toprule
        \multicolumn{9}{c}{\textbf{VGGFace2}} \\
        \hline
         \multirow{2}{*}{Method} & \multicolumn{4}{c|}{``a photo of \textit{sks} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{sks} person''}\\
         \cline{2-9}
          & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
         \hline
         No Defense &0.07 &0.63 &0.73 &15.61 &0.21 &0.48 &0.71 &9.64 \\
         FSMG &0.56 &\textbf{0.33} &\textbf{0.31} &\textbf{36.61} &0.62 &0.29 &0.37 &38.22  \\
         ASPL &\textbf{0.63} &\textbf{0.33} &\textbf{0.31} &36.42 &\textbf{0.76} &\textbf{0.28} &\textbf{0.30} &\textbf{39.00}  \\
         T-FSMG &0.07 &0.58 &0.74 &15.49 &0.28 &0.44 &0.71 &17.29  \\
         T-ASPL &0.07 &0.57 &0.72 &15.36 &0.39 &0.44 &0.70 &20.06  \\
         \hhline{=========}
        \multicolumn{9}{c}{\textbf{CelebA-HQ}} \\
        \hline
         \multirow{2}{*}{Method} & \multicolumn{4}{c|}{``a photo of \textit{sks} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{sks} person''}\\
         \cline{2-9}
          & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
         \hline
         No Defense &0.10 &0.68 &0.72 &17.06 &0.26 &0.44 &0.72 &7.30 \\
         FSMG &\textbf{0.34} &\textbf{0.48} &0.56 &36.13 &\textbf{0.35} &\textbf{0.36} &0.66 &33.60  \\
         ASPL &0.31 &0.50 &\textbf{0.55} &\textbf{38.57} &0.34 &0.39 &\textbf{0.63} &\textbf{34.89}  \\
         T-FSMG &0.06 &0.64 &0.73 &25.75 &0.24 &0.45 &0.73 &8.04  \\
         T-ASPL &0.06 &0.64 &0.73 &20.58 &0.26 &0.46 &0.72 &5.36 \vspace{-\aboverulesep} \\
         \bottomrule
    \end{tabular}
    \vspace{-3mm}
    \caption{Comparing the defense performance of the proposed methods in a convenient setting.}
    \vspace{-5mm}
    \label{tab:convenient}
\end{table*}


\begin{figure*}[t] 
    \centering
    \subfloat[Comparison between proposed methods]{
        \includegraphics[height=5.75cm]{fig/MethodComparison.pdf}
        \label{fig:compare}
        \vspace{-3mm}
    }
    \hspace{-0.3cm}
    \subfloat[Inference prompts]{
        \includegraphics[height=5.75cm]{fig/Prompts_fit.pdf}
        \label{fig:prompts}
        \vspace{-3mm}
    }
        \hspace{0.1cm}
        \vspace{-2mm}
    \caption{Qualitative defense results for two subjects in VGGFace2 in the convenient setting. Best viewed in zoom.}
        \vspace{-4mm}
    \label{fig:qual}
\end{figure*}

\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{3.5pt}
    \begin{tabular}{l|c|cccc|cccc}
        \specialrule{.09em}{.04em}{.04em} 
         \multirow{2}{*}{Version} & \multirow{2}{*}{Defense?} & \multicolumn{4}{|c|}{``a photo of \textit{sks} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{sks} person''}\\
         \cline{3-10}
          &  & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
         \hline
         \multirow{2}{*}{v1.4} & \xmark &0.05 &0.46 &0.65 &21.06 &0.08 &0.43 &0.64 &10.05 \\
          & \cmark &\textbf{0.80} &\textbf{0.18} &\textbf{0.12} &\textbf{26.76} &\textbf{0.17} &\textbf{0.28} &\textbf{0.55} &\textbf{13.07}  \\
          \hline
         \multirow{2}{*}{v1.5} & \xmark &0.07 &0.49 &0.65 &18.53 &0.07 &0.45 &0.64 &10.57 \\
          & \cmark &\textbf{0.71} &\textbf{0.20} &\textbf{0.20} &\textbf{22.98} &\textbf{0.11} &\textbf{0.26} &\textbf{0.57} &\textbf{16.10}  \\
        \specialrule{.09em}{.04em}{.04em} 
    \end{tabular}
    \vspace{-3mm}
    \caption{Defense performance of ASPL with different generator versions on VGGFace2 in a convenient setting.}
    \label{tab:versions}
    \vspace{-1mm}
\end{table*}

\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{c|cccc|cccc}
        \hline
         \multirow{2}{*}{$\eta$} & \multicolumn{4}{|c|}{``a photo of \textit{sks} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{sks} person''}\\
         \cline{2-9}
          & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
         \hline
         0 &0.07 &0.63 &0.73 &15.61 &0.21 &0.48 &0.71 &9.64 \\
         0.01 &0.08 &0.58 &0.72 &33.03 &0.28 &0.45 &0.72 &17.14 \\
         0.03 &0.44 &0.38 &0.38 &36.45 &0.55 &0.32 &0.43 &37.86 \\
         $0.05^*$ &0.63 &0.33 &0.31 &36.42 &0.76 &0.28 &0.30 &39.00  \\
         0.10 &0.76 &0.21 &0.22 &37.33 &0.86 &0.23 &0.26 &40.92 \\
         0.15 &\textbf{0.80} &\textbf{0.15} &\textbf{0.15} &\textbf{37.07} &\textbf{0.91} &\textbf{0.17} &\textbf{0.14} &\textbf{41.18} \\
         % 0.20 &\textbf{0.88} &\textbf{0.14} &\textbf{0.10} &\textbf{37.84} &\textbf{0.91} &\textbf{0.17} &\textbf{0.12} &41.09 \\
         \hline
    \end{tabular}
    \vspace{-3mm}
    \caption{Defense performance of ASPL with different noise budgets on VGGFace2 in a convenient setting. ``*'' is default.}
    \vspace{-3mm}
    \label{tab:noise}
\end{table*}

\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{2.5pt}
    \begin{tabular}{l|c|c|cccc|cccc}
        \hline
         \multirow{6}{.076\linewidth}{Model mismatch} & \multirow{2}{*}{Train} & \multirow{2}{*}{Test} & \multicolumn{4}{|c|}{``a photo of \textit{sks} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{sks} person''}\\
         \cline{4-11}
          & & & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
         \cline{2-11}
         & v1.4 & v2.1 &0.62 &0.31 &\textbf{0.28} &36.00 &0.70 &0.31 &0.35 &38.39 \\
         & v1.4, 1.5, 2.1 & v2.1 &\textbf{0.70} & \textbf{0.27} &\textbf{0.28} &\textbf{36.71} &\textbf{0.75} &\textbf{0.29} &\textbf{0.33} &\textbf{39.23} \\
         \cline{2-11}
         & v1.4 & v2.0 &0.70 &0.27 &0.23 &36.83 &0.61 &0.26 &0.31  &37.28 \\
         & v1.4, 1.5, 2.1 & v2.0 & \textbf{0.79} & \textbf{0.24} & \textbf{0.18} &\textbf{37.96} &\textbf{0.71} &\textbf{0.23} &\textbf{0.23} &\textbf{38.99} \\
         \hhline{===========}
         \multirow{4}{.076\linewidth}{Term/ Prompt mismatch} & \multicolumn{2}{|c|}{\multirow{2}{*}{DreamBooth prompt}} & \multicolumn{4}{|c|}{``a photo of \textit{$S_*$} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{$S_*$} person''}\\
         \cline{4-11}
          & \multicolumn{2}{|c|}{} &FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
         \cline{2-11}
         & \multicolumn{2}{|c|}{``\textit{sks}'' $\rightarrow$ ``\textit{t@t}''} &0.34 &0.30 &0.48 &36.67 &0.34 &0.28 &0.52  &28.17 \\
         & \multicolumn{2}{|c|}{``a dslr portrait of \textit{sks} person''} &0.07 &0.15 &0.69 &17.34 &0.49 &0.37 &0.36 &38.42 \\
         \hline
    \end{tabular}
    \vspace{-3mm}
    \caption{Defense performance of ASPL on VGGFace2 when the model, term, or prompt used to train the target DreamBooth model is different from the one used to generate defense noise. Here, \textit{$S_*$} is ``t@t'' for the first row and ``sks'' for second row.}
    \label{tab:cross}
    \vspace{-1mm}
\end{table*}

% \begin{table*}[t]
%     \centering
%     \setlength{\tabcolsep}{3pt}
%     \begin{tabular}{c|c|cccc|cccc}
%         \hline
%          \multirow{2}{*}{Train} & \multirow{2}{*}{Test} & \multicolumn{4}{|c|}{``a photo of \textit{sks} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{sks} person''}\\
%          \cline{3-10}
%           & & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
%          \hline
%          v1.4 & v2.1 &0.62 &0.31 &0.28 &36.00 &0.70 &0.31 &0.35 &38.39 \\
%          v1.4, 1.5, 2.1 & v2.1 &0.70 &0.27 &0.28 &36.71 &0.75 &0.29 &0.33 &39.23 \\
%          v1.4 & v2.0 &0.70 &0.27 & & &0.61 &0.26 &  & \\
%          v1.4, 1.5, 2.1 & v2.0 & & & & & & & & \\
%          \hline
%     \end{tabular}
%     \caption{Defense performance of ASPL on VGGFace2 when the generator version used in training mismatches the one in testing.}
%     \label{tab:cross_version}
% \end{table*}

% \begin{table*}[t]
%     \centering
%     \setlength{\tabcolsep}{3pt}
%     \begin{tabular}{l|cccc|cccc}
%         \hline
%          \multirow{2}{*}{DreamBooth prompt} & \multicolumn{4}{|c|}{``a photo of \textit{$S_*$} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{$S_*$} person''}\\
%          \cline{2-9}
%           & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
%          \hline
%          ``\textit{sks}'' $\rightarrow$ ``\textit{t@t}'' & & & & & & &  & \\
%          ``a dslr portrait of \textit{sks} person'' &0.07 &0.15 &0.69 &17.34 &0.49 &0.37 &0.36 &38.42 \\
%          \hline
%     \end{tabular}
%     \caption{Defense performance of ASPL on VGGFace2 when the term or text prompt used to train the target DreamBooth model is different from the one used to generate the defense noise. Here, \textit{$S_*$} is ``t@t'' for the first row and ``sks'' for second row}
%     \label{tab:cross_prompt}
% \end{table*}


\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{c|c|cccc|cccc}
        \hline
         \multirow{2}{*}{Perturbed} & \multirow{2}{*}{Clean} & \multicolumn{4}{|c|}{``a photo of \textit{sks} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{sks} person''}\\
         \cline{3-10}
          & & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
         \hline
         4&0 &\textbf{0.63} &\textbf{0.33} &\textbf{0.31} &\textbf{36.42} &\textbf{0.76} &\textbf{0.28} &\textbf{0.30} &\textbf{39.00} \\
         \hline
         3&1 &0.50 &0.43 &0.41 &35.53 &0.52 &0.35 &0.51 &34.01 \\
         2&2 &0.29 &0.53 &0.61 &28.99 &0.40 &0.37 &0.62 &26.13 \\
         1&3 &0.08 &0.61 &0.73 &18.92 &0.27 &0.45 &0.70 &15.55 \\
         \hline
         0&4 &0.07 &0.63 &0.73 &15.61 &0.21 &0.48 &0.71 &9.64\\
         \hline
    \end{tabular}
    \vspace{-3mm}
    \caption{Defense performance of ASPL on VGGFace2 in uncontrolled settings. We include two extra results with 0 clean image (convenient setting) and 0 perturbed image (no defense) for comparison. } % ``*'' means the clean images are from the protecting image subset.
    \vspace{-3mm}
    \label{tab:uncontrolled}
\end{table*}



\minisection{Targeted approaches.}
The proposed algorithms above are untargeted; each perturbation noise is learned to maximize the reconstruction loss $\mathcal{L}_{cond}$. Therefore, the adversarial examples $x^{(i)} + \delta^{(i)}$ may guide the target DreamBooth model to learn different adversarial directions, potentially canceling out their effects. Inspired by the success of targeted attacks in \cite{shan2020fawkes}, we can select a single target $x^{tar}$ to learn optimal $\delta$ such that the output of model is pulled closer to $x^{tar}_t$ when trained with $(x+\delta)_t$. This targeted attack scheme can be plugged into all previous methods, and we denote new algorithms with the prefix ``T-'', e.g., T-FSMG and T-ASPL.

%} adversarial examples $(x+\delta)$ are pulled closer to the target image $x_{tar}$ \hao{$\widehat{(x^{(i)}+\delta^{i})}_t$ are pulled closer to the target image $x^{tar}_t$}

\minisection{Ensemble approaches.}
In adverse settings, the pretrained text-to-image generator used by the attacker is unknown. While we can pick one to train the perturbation and hope it transfers well to the target generator, one better approach is to use an ensemble \cite{cherepanova2021lowkey,yang2021defending} of surrogate models finetuned from different pretrained generators. This approach can be an easy plug-in for the previous approaches. Due to memory constraints, instead of using these surrogate models all at once, we only used a single model at a time, in an interleaving manner, to produce optimal perturbed data.

\section{Experiments}\label{sec:exp}
\subsection{Experimental setup}\label{sec:Setup}
\minisection{Datasets.}
To evaluate the effectiveness of the proposed methods, we look for facial benchmarks that satisfy the following criteria: (1) each dataset covers a large number of different subjects with identity annotated, (2) each subject must have enough images to form two image sets for reference ($\mathcal{X}_A$) and protection ($\mathcal{X}_{db}$), (3) the images should have mid- to high-resolution, (4) the images should be diverse and in-the-wild. Based on those criteria, we select two famous face datasets CelebA-HQ \cite{karras2017progressive} and VGGFace2 \cite{Cao18}. 

CelebA-HQ is a high-quality version of CelebA \cite{liu2015faceattributes} that consists of $30,000$ images at $1024 \times 1024$ resolution. We use the annotated subset \cite{celeba_id} that filters and groups images into $307$ subjects with at least $15$ images for each subject.

VGGFace2 \cite{Cao18} contains around $3.31$ million images of $9131$ person identities. We filter the dataset to pick subjects that have at least $15$ images of resolution above $500\times500$. 

For fast but comprehensive evaluations, we choose $50$ identities for each dataset. For each subject in these datasets, we use the first $12$ images and divide them into three subsets, including the reference clean image set, the target protecting set, and an extra clean image set for uncontrolled setting experiments (Sec. \ref{sec:uncontrolled}). Each mentioned subset has $4$ images with diverse conditions. We then center-crop and resize images to resolution $512 \times 512$.

\minisection{Training configurations.}
We train each DreamBooth model, both text-encoder and UNet model, with batch size of $2$ and learning rate of $5\times10^{-7}$ for $1000$ training steps. By default, we use the latest Stable Diffusion (v2.1) as the pretrained generator. Unless specified otherwise, the training instance prompt and prior prompt are ``a photo of \textit{sks} person'' and ``a photo of person'', respectively. It takes 15 minutes to train a model on an NVIDIA A100 GPU 40GB.

We optimize the adversarial noise $\delta^{(i)}$ in each step of FSMG and ASPL using the untargeted PGD scheme (Eq. \ref{eq:pgd}). 
We use 100 PGD iterations for FSMG and 50 iterations for ASPL.
% The required PGD iterations of PGD are 100 for FSMG and 50 for ASPL. 
Both methods use $\alpha = 0.005$ and the default noise budget $\eta = 0.05$. It takes 2 and 5 minutes for FSMG and ASPL to complete on an NVIDIA A100 GPU 40GB.
% All exp are trained on NVIDIA A100s. Training time: 1.5m
% Total training time for 1 id: db time + adv time ?

\minisection{Evaluation metrics.}
Our methods aim to disrupt the target DreamBooth models, making them produce poor images of the target user. To measure the defense's effectiveness, for each trained DreamBooth model and each testing prompt, we generate 30 images. We then use a series of metrics to evaluate these generated images comprehensively. 

Images generated from successfully disrupted DreamBooth models may have no detectable face, and we measure that rate, called \textbf{\textit{Face Detection Failure Rate (FDFR)}}, using RetinaFace detector \cite{deng2020retinaface}. If a face is detected, we extract its face recognition embedding, using ArcFace recognizer \cite{deng2019arcface}, and compute its cosine distance to the average face embedding of the entire user's clean image set. This metric is called \textbf{\textit{Identity Score Matching (ISM)}}. Finally, we use two extra image quality assessment metrics. One is \textbf{\textit{SER-FQA}} \cite{TerhorstKDKK20}, which is an advanced, recent metric dedicated to facial images. The other is \textbf{\textit{BRISQUE}} \cite{Brisque}, which is classical and popular for assessing images in general.
% \begin{itemize}
%     \item \textbf{\textit{Face Detection Failure Rate (FDFR):}} Images generated from successfully disrupted DreamBooth models may have no detectable face, and we measure that rate, using RetinaFace detector \cite{deng2020retinaface}.
%     \item \textbf{\textit{Identity Score Matching (ISM):}} If a face is detected, we extract its face recognition embedding, using ArcFace recognizer \cite{deng2019arcface}, and compute its cosine distance to the pooled face recognition embedding of the entire user's clean image set.
%     \item \textbf{\textit{SER-FQA \cite{TerhorstKDKK20}:}} It is an advanced, recent face quality assessment metric.
%     \item \textbf{\textit{BRISQUE \cite{Brisque}:}} It is a classical and popular non-reference image quality assessment metric.
% \end{itemize}

% The first metric set focuses on general image quality. It includes BRISQUE \cite{Brisque} and CLIP-IQA \cite{wang2022exploring}. BRISQUE is a classical and popular non-reference quality assessment metric, while CLIP-IQA is a recent, deep-learning-based metric measuring image quality given predefined criteria such as \textit{Good Photo}, \textit{Sharp Photo} and \textit{Noisy Photo}.

% The second set covers face-related metrics. This set includes Face Detection Failure Rate (FDFR), Identity Score Matching (ISM), FaceQAN \cite{babnik2022faceqan}, and SER-FQA \cite{TerhorstKDKK20}. Images generated from successfully disrupted DreamBooth models may have no detectable face, and we measure that rate, called FDFR, using RetinaFace detector \cite{deng2020retinaface}. If a face is detected, we extract its face recognition embedding, using ArcFace recognizer \cite{deng2019arcface}, and compute its cosine distance to the pooled face recognition embedding of the entire user's clean image set. This metric is called ISM. Finally, we employ the advanced, recent face quality assessment metrics FaceQAN and SER-FQA.

% Besides evaluating the generated DreamBooth images, we also assess the stealthiness of the cloaking methods by calculating the similarity between the original and cloaked photos. The metrics used for this purpose include SSIM, PSNR, and LPIPS \cite{lpips}.

% We compute FDFR (Face Detection Failure Rate) and ISM (Identity Score Matching) on the finetuning images to measure the effectiveness of our protection methods. We implement RetinaFace Detection \cite{deng2020retinaface} and ArcFace Recognition \cite{deng2019arcface} to measure FDFR and ISM respectively.


% We measure the utility of cloak images using both reference-based and non-reference quality metrics. To calculate the similarity between the original and cloaked photos, we use SSIM, PSNR, and LPIPS \cite{lpips}. We also use non-reference metrics such as BRISQUE \cite{Brisque} to measure the quality of images generated by the target DreamBooth models.

% We compute FDFR (Face Detection Failure Rate) and ISM (Identity Score Matching) on the finetuning images to measure the effectiveness of our protection methods. We implement RetinaFace Detection \cite{deng2020retinaface} and ArcFace Recognition \cite{deng2019arcface} to measure FDFR and ISM respectively. Besides, we also use CLIP-IQA \cite{wang2022exploring}, FaceQAN \cite{babnik2022faceqan} and SER-FQA \cite{TerhorstKDKK20}. CLIP-IQA is used to measure quality of finetuning image given predefined criteria such as \textit{Good Photo}, \textit{Sharp Photo} and \textit{Noisy Photo}. To measure the face quality of finetuning image, we use FaceQAN and SER-FQA metrics. 

\subsection{Convenient setting}
We first evaluate the proposed defense methods, including FSMG, ASPL, T-FSMG, and T-ASPL, in a convenient setting on the two datasets. We try two image generation prompts, one used in training (``a photo of \textit{sks} person'') and one novel, unseen prompt (``a dslr portrait of \textit{sks} person''). The average scores over DreamBooth-generated images with each defense are reported in Table \ref{tab:convenient}. As can be seen, the untargeted defenses significantly increase the face detection failure rates and decrease the identity matching scores, implying their success in countering the DreamBooth threat. We provide some qualitative images in Fig. \ref{fig:compare}. As expected, ASPL defends better than FSMG since it mimics better the DreamBooth model training at test time. Targeted methods perform poorly, suggesting that the noise generated by these methods, while providing more consistent adversarial guidance in DreamBooth training, is suboptimal and  ineffective. Since ASPL performs the best, we will try only this method in all follow-up experiments.

\subsection{Ablation studies}

\minisection{Text-to-image generator version.} In previous experiments, we used Stable Diffusion (SD) v2.1 as the pretrained text-to-image generator. In this section, we examine if our proposed defense (ASPL) is still effective when using different pretrained generators. Since SD is the only open-source large text-to-image model series, we try two of its versions, including v1.4 and v1.5. Note that while belonging to the same model family, these models, including v2.1, are slightly different in networks and behaviors. As reported in Table \ref{tab:versions}, ASPL shows consistent defense effectiveness.

\minisection{Noise budget.} Next, we examine the impact of noise budget $\eta$ on ASPL attack using SD v2.1. As illustrated in Table \ref{tab:noise}, our defense is already effective with $\eta = 0.03$. The larger the noise budget is, the better defense performance we get, at the cost of the perturbation's stealthiness. % of ASPL becomes. However, the perturbed noise added to original images will become more visible as the budget noise increases.

\minisection{Inference text prompt.} As indicated in Table \ref{tab:convenient}, ASPL well-disturbs images generated with an unseen text prompt (``a dslr portrait of \textit{sks} person''). We further test the ASPL-disturbed DreamBooth models with different inference text prompts and get similar results. Fig. \ref{fig:prompts} provides some examples generated with these extra prompts.

% \subsection{Adaptive attacks}
\subsection{Adverse settings}
% In the previous sections, we conducted experiments in convenient settings, when all components of the target DreamBooth training are known. 
In this section, we investigate if our proposed defense still succeeds when some component is unknown, leading to a mismatch between the perturbation learning and the target DreamBooth model finetuning processes.

\minisection{Model mismatching.} The first scenario is when the pretrained generators are mismatched. We provide an example of transferring adversarial noise trained on SD v1.4 to defend DreamBooth models trained from v2.1 and v2.0 in the first and third rows in Table \ref{tab:cross}. ASPL still provides good scores as in Table \ref{tab:convenient}. We also examine the ensemble solution suggested in the literature, as discussed in Sec. \ref{sec:algorithms}. We combine that ensemble idea with ASPL, called E-ASPL, using SD v1.4, 1.5, and 2.1. It further improves the defense in both cases,
% With wider coverage, such an ensemble algorithm successfully defends when the attacker uses SD v2.1. It also has a strong transferability and effectively breaks an attack with an unseen pretrained generator (v2.0), 
as illustrated in the upper half of Table \ref{tab:cross}.

\minisection{Term mismatching.} The malicious user can change the term representing the target from the default value (``sks'') to another, e.g., ``t@t''. As reported in the first row in the lower half of Table \ref{tab:cross}, this term mismatch has only a moderate effect on our results; key scores, like ISM, are still good.

\minisection{Prompt mismatching.} The malicious user can also use a different DreamBooth training prompt. ASPL still provides low ISM scores under that adverse scenario, as reported in the last row of Table \ref{tab:cross}.

\minisection{Real-world test.} Our method successfully disrupts personalized images generated by Astria \cite{astria}, a black-box commercial service (see the \cref{sec:real_test} for details).

\subsection{Uncontrolled settings}\label{sec:uncontrolled}
Our Anti-DreamBooth system is designed for controlled settings, in which all images have protection noises added. In this section, we examine the scenarios when the assumption does not hold, i.e., the malicious user may get in hand some clean images of the target subject and mix them with the perturbed images for DreamBooth training. Assuming the number of images for DreamBooth finetuning is fixed as 4, we examine three data mixing configurations with the number of clean images increasing from 1 to 3. As shown in Table \ref{tab:uncontrolled}, our defense is still quite effective when half of the images are perturbed, but its effectiveness reduces when more clean images are introduced. Still, these uncontrolled settings can be prevented if our system becomes popular and used by all social media with lawmakers' support.

\section{Conclusions}\label{sec:conclusions}
This paper reveals a potential threat of misused DreamBooth models and proposes a framework to counter the threat. Our solution is to perturb users' images with subtle adversarial noise so that any DreamBooth model trained on those images will produce poor personalized images. 
The key idea is to mislead the target DreamBooth model to perform poorly in each denoising step on the original, unperturbed images. 
We designed several algorithms and extensively evaluate them in different settings. Our defense is effective, even in adverse conditions. 
% Our work will alert the community about the new threat and put a cornerstone to counter the risk. 
In the future, we aim to improve the perturbation's imperceptibility and robustness \cite{cherepanova2021lowkey,yang2021defending,wang2022anti} and 
conquer the uncontrolled settings.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

% \newpage
%%%%%%%%% APPENDIX
%\clearpage
\appendix
%------------------------------------------------------------------------

%%%%%%%%% BODY TEXT
\section{Additional quantitative results}
In the main paper, we comprehensively analyzed ASPL's performance on the VGGFace2 dataset. Here, we provide additional quantitative results on the CelebA-HQ dataset. We also report extra results with FSMG, the second-best defense algorithm, on the convenient settings.


\subsection{Ablation studies}

\minisection{Text-to-image generator version.} 
We investigate the effectiveness of our defense methods across different versions of SD models, including v1.4 and v1.5. 

As reported in \cref{tab:aspl_versions}, ASPL significantly decreases the identity scores (ISM) in CelebA-HQ, confirming its defense's effectiveness. Its scores, however, are not as good as in VGGFaces2. We can explain it by the fact that CelebA-HQ images are more constrained in pose and quality, reducing the diversity of the image set for DreamBooth and making their combined perturbation effect less severe.

As for FSMG, there is a similar pattern in all metrics on both VGGFace2 and CelebA-HQ, as presented in \cref{tab:fsmg_versions}. FSMG provides a slightly weaker defense compared with ASPL, confirming our observation in the main paper.

\vspace{2mm}
\minisection{Noise budget.} 
% We further examine the impact of noise budget $\eta$ on FSMG and ASPL using SD v2.1. As illustrated in Table \ref{tab:aspl_noise}, ASPL is also effective on CelebA-HQ dataset, especially the evidence is clearer when $\eta \geq 0.05$. However, the perturbed noise added to original images will become more visible as the budget noise increases. The same trend happens for FSMG (presented in \cref{tab:fsmg_noise}), there is no distinct difference of performance between VGGFace2 and CelebA-HQ, when the budget is increased.
We further examine the impact of noise budget $\eta$ on FSMG and ASPL using SD v2.1 in \cref{tab:fsmg_noise,tab:aspl_noise}. As expected, increasing the noise budget leads to better defense scores, either with FSMG or ASPL and either in VGGFace2 or CelebA-HQ. Again, ASPL outperforms FSMG on most evaluation scores.

\subsection{Adverse settings}
In the main paper, we verified that our best protection method, i.e., ASPL, remained effective in VGGFace2 when some components of the target DreamBooth training were unknown, resulting in a disparity between the perturbation learning and the DreamBooth finetuning. Here we repeat those defense experiments but on the CelebA-HQ dataset to further confirm ASPL's effectiveness.
% Here, we examine whether our best protection method, i.e., ASPL, remains effective or not even when some components of the target DreamBooth training are unknown, resulting in a disparity between the perturbation learning and the DreamBooth finetuning.

\minisection{Model mismatching.}
As can be seen in \cref{tab:aspl_cross}, the ASPL approach still works effectively on CelebA-HQ in the cross-model settings. Furthermore, the ensemble approach demonstrates a superior performance on all measurements, the same as the observation on VGGFace2.
% In \cref{tab:fsmg_versions}, bla bla

\minisection{Term mismatching.} In realistic scenarios, the term representing the target in training DreamBooth might vary differently. To demonstrate this problem, we report ASPL's performance when the term ``sks'' is changed to ``t@t''. As can be seen in \cref{tab:aspl_cross}, our method still provides an extremely low ISM score, guaranteeing user protection regardless of the term mismatching.

\minisection{Prompt mismatching.}
This is the challenging setting when the attacker uses a prompt different from the one used in perturbation learning to train his/her DreamBooth model. In \cref{tab:aspl_cross}, though there is a drop in some metrics compared with the convenience settings, either the ISM or BRISQUE score remains relatively good.
% We further present a case when a complex prompt is used to train the DreamBooth model in \cref{tab:complex_cross_prompt}. As we can see, both FSMG and ASPL share the same pattern of measurements on different datasets. 
This evidence further assures that our approaches are robust to the prompt mismatching problem.

\subsection{Uncontrolled settings}
We examine APSL in the uncontrolled settings on CelebaA-HQ (\cref{tab:aspl_uncontrolled}) and observe the same trend as reported on the VGGFace2 dataset.

\section{Real-world test.}
\label{sec:real_test}
% As personalized text-to-image services gain popularity, developing a method to defend against their misuse is crucial. To this end, we conducted experiments to evaluate the efficacy of our approach against real-world scenarios. We compared the output of Astria \cite{astria} when we used the original images and the adversarial images defended by our ASPL method with Stable Diffusion version 2.1 and $\eta=0.05$ in \cref{fig:astria1,fig:astria2}. As can be seen, our method significantly reduces the quality of the generated images in various complex prompts. Even though these services often rely on proprietary algorithms and architectures that are not transparent to the public, our method remains effective against them. This highlights the robustness of our approach, which can defend against these services without requiring knowledge of their underlying configurations.



In previous tests, we conducted experiments in laboratory mode. In this section, we examine if our proposed defense actually works in real-world scenarios by trying to disrupt personalized generation outputs of a black-box, commercialized AI service. We find Astria \cite{astria} satisfies our criteria and decide to use it in this test. Astria uses the basic DreamBooth setup that allows us to upload images of a specific target subject and input a generation prompt to acquire corresponding synthesized images. It also supports different model settings; we pick the recommended setting (SD v1.5 with face detection enabled) and a totally different one (Protogen 3.4 + Prism) for the tests.

We compare the output of Astria when using the original images and the adversarial images defended by our ASPL method with Stable Diffusion version 2.1 and $\eta=0.05$ in \cref{fig:astria1,fig:astria2}, using two different subjects and with each model setting, respectively. As can be seen, our method significantly reduces the quality of the generated images in various complex prompts and on both target models. Even though these services often rely on proprietary algorithms and architectures that are not transparent to the public, our method remains effective against them. This highlights the robustness of our approach, which can defend against these services without requiring knowledge of their underlying configurations.

\section{Qualitative results}
We comprehensively analyzed our defense mechanism quantitatively in the main paper. Here, we provide additional qualitative results to back up those numbers and for visualization, as well.

%including Fig 1 for FSMG, Fig 2 for ASPL, bla bla

\subsection{Ablation studies}

\minisection{Text-to-image generator version.} 
We compare the defense performance of ASPL using two different versions of SD models (v1.4 and v1.5) on VGGFace2 in \cref{fig:vgg_aspl_versions}. The output images produced by both models with both prompts are strongly distorted with notable artifacts. We observe the same behavior in the corresponding experiments on CelebA-HQ, visualized in \cref{fig:cel_aspl_versions}.

\minisection{Noise budget.}
In order to better understand the impact of the noise budget, we present a grid of images for ASPL on VGGFace2 where the upper bound of noise's magnitude increases along the vertical axis in \cref{fig:noisebg_vgg}. It is evident that when the noise budget increases, the visibility of noise becomes more pronounced. Moreover, the allocated noise budget heavily influences the degree of output distortion, resulting in a trade-off between the visibility of noise in perturbed images and the level of output distortion. For further visulization on CelebA-HQ, please refer to \cref{fig:noisebg_celeba}.

\subsection{Adverse Setting}

\minisection{Model mismatching.} 
In this section, we present the visual outputs of ASPL when a model mismatch occurs. Specifically, we train the image perturbation with SD v1.4, then use those images to disrupt DreamBooth models finetuned from v2.1 and v2.0, respectively. As illustrated in \cref{fig:vgg_aspl_cross_model}, our defense method is still effective in both cases, although transferring from v1.4 to v2.0 produces more noticeable artifacts than the previous scenario.

In addition to our primary analysis, our study provides qualitative results for E-ASPL, which employs an ensemble method to overcome the challenge of model mismatching. Specifically, we combined knowledge from three versions of SD models (v1.4, v1.5, and v2.1). The results, illustrated in \cref{fig:vgg_aspl_ensemble}, demonstrate the superior performance of E-ASPL in countering model mismatching where most images are heavily distorted.

\minisection{Term mismatching.}
Despite the discrepancy of term replacement (from ``sks'' to ``t@t''), ASPL still demonstrates its effectiveness on two provided subjects and two provided prompts (as in \cref{fig:vgg_aspl_cterm_cprompt}). However, the change in the training term may result in slightly weaker artifacts compared to the original setting.

\minisection{Prompt mismatching.} 
The results depicted in \cref{fig:vgg_aspl_cterm_cprompt} indicate that the finetuning of the DreamBooth model with various prompts, such as "a DSLR portrait of \textit{sks} person", can impact the degree of output distortion to some extent. It is important to note that prompt mismatching can alter the behavior of our defense method on a different prompt, such as "a photo of \textit{sks} person", which can change the identity of the target subject in the generated images.

\subsection{Uncontrolled settings.} 
All previous results are for controlled settings, in which we have access to all images needing protection. Here, we also include some qualitative results for uncontrolled settings where a mixture of clean and perturbed images are used for finetuning Dreambooth. We use the same settings as the one in the main paper, with the number of images for DreamBooth being fixed at 4 and the number of clean images gradually increase. As can be seen in \cref{fig:uncontrolled_vgg}, our method is more effective when more perturbed data are used and vice versa.

\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{3.5pt}
    \begin{tabular}{l|c|cccc|cccc}
        \specialrule{.09em}{.04em}{.04em} 
         \multirow{2}{*}{Version} & \multirow{2}{*}{Defense?} & \multicolumn{4}{|c|}{``a photo of \textit{sks} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{sks} person''}\\
         \cline{3-10}
          &  & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
         \hline
         \multirow{2}{*}{v1.4} & \xmark & 0.07 & 0.48 &0.66 &16.09 & \textbf{0.11} & 0.40 &0.67 &10.31 \\
          & \cmark & \textbf{0.28} & \textbf{0.29} & \textbf{0.47} & \textbf{20.05} & 0.06 & \textbf{0.31} & \textbf{0.64} & \textbf{10.55} \\
          \hline
         \multirow{2}{*}{v1.5} & \xmark & 0.06 & 0.53 &0.69 &14.45 & \textbf{0.07} & 0.39 &0.68 &8.95 \\
          & \cmark & \textbf{0.16} & \textbf{0.36} & \textbf{0.58} & \textbf{21.09} &0.06 & \textbf{0.26} & \textbf{0.64} & \textbf{12.28} \\
        \specialrule{.09em}{.04em}{.04em} 
    \end{tabular}
    \vspace{-3mm}
    \caption{Defense performance of ASPL with different generator versions on CelebA-HQ in a convenient setting.}
    \label{tab:aspl_versions}
    \vspace{-1mm}
\end{table*}

\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{3.5pt}
    \begin{tabular}{l|c|cccc|cccc}
        \toprule
        \multicolumn{10}{c}{\textbf{VGGFace2}} \\
        \specialrule{.09em}{.04em}{.04em} 
         \multirow{2}{*}{Version} & \multirow{2}{*}{Defense?} & \multicolumn{4}{|c|}{``a photo of \textit{sks} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{sks} person''}\\
         \cline{3-10}
          &  & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
         \hline
         \multirow{2}{*}{v1.4} & \xmark & 0.05 & 0.46 &0.65 &21.06 & 0.08 & 0.44 &0.64 &10.05 \\
          & \cmark & \textbf{0.73} & \textbf{0.21} & \textbf{0.17} & \textbf{25.88} & \textbf{0.13} & \textbf{0.28} & \textbf{0.57} & \textbf{13.46} \\
          \hline
         \multirow{2}{*}{v1.5} & \xmark & 0.07 & 0.49 &0.65 &18.53 & 0.07 & 0.45 &0.64 &10.57 \\
          & \cmark & \textbf{0.61} & \textbf{0.21} & \textbf{0.26} & \textbf{23.89} & \textbf{0.11} & \textbf{0.26} & \textbf{0.57} & \textbf{18.00} \\
        % \specialrule{.09em}{.04em}{.04em}
        % \bottomrule

        % \hhline{==========}
        \toprule
        \multicolumn{10}{c}{\textbf{CelebA-HQ}} \\
        \specialrule{.09em}{.04em}{.04em} 
         \multirow{2}{*}{Version} & \multirow{2}{*}{Defense?} & \multicolumn{4}{|c|}{``a photo of \textit{sks} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{sks} person''}\\
         \cline{3-10}
          &  & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
         \hline
         \multirow{2}{*}{v1.4} & \xmark & 0.07 & 0.48 &0.66 &16.09 & \textbf{0.11} & 0.40 &0.67 &10.31 \\
          & \cmark & \textbf{0.29} & \textbf{0.32} & \textbf{0.48} & \textbf{20.83} & 0.07 & \textbf{0.29} & \textbf{0.63} & \textbf{12.00} \\
          \hline
         \multirow{2}{*}{v1.5} & \xmark & 0.06 & 0.53 &0.69 &14.45 & \textbf{0.07} & 0.39 & 0.68 &8.95 \\
          & \cmark & \textbf{0.13} & \textbf{0.38} & \textbf{0.60} & \textbf{20.43} & 0.06 & \textbf{0.28} & \textbf{0.65} & \textbf{13.27} \\
        %\specialrule{.09em}{.04em}{.04em}
        \bottomrule
        
    \end{tabular}
    \vspace{-3mm}
    \caption{Defense performance of FSMG with different generator versions on VGGFace2 and CelebA-HQ in a convenient setting.}
    \label{tab:fsmg_versions}
    \vspace{-1mm}
\end{table*}

\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{c|cccc|cccc}
        \toprule
        \multicolumn{9}{c}{\textbf{VGGFace2}} \\
        \midrule
         \multirow{2}{*}{$\eta$} & \multicolumn{4}{|c|}{``a photo of \textit{sks} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{sks} person''}\\
         \cline{2-9}
          & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
         \hline
         0 &0.07 &0.63 &0.73 &15.61 &0.21 &0.48 &0.71 &9.64 \\
         0.01 &0.09 &0.58 &0.73 &31.58 &0.28 &0.46 &0.71 &15.85 \\
         0.03 &0.45 &0.39 &0.38 & \textbf{37.82} &0.53 &0.33 &0.47 &38.17 \\
         $0.05^*$ &0.56 &0.33 &0.31 &36.61 &0.62 &0.29 &0.37 &38.22  \\
         0.10 &0.70 &0.22 &0.23 &36.60 &0.77 &0.27 &0.29 &38.59 \\
         0.15 & \textbf{0.77}  &\textbf{0.20} & \textbf{0.20} &36.16 &\textbf{0.83} & \textbf{0.22} & \textbf{0.26} & \textbf{39.17} \\
         \toprule
         \multicolumn{9}{c}{\textbf{CelebA-HQ}} \\
         \hline
         \multirow{2}{*}{$\eta$} & \multicolumn{4}{|c|}{``a photo of \textit{sks} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{sks} person''}\\
         \cline{2-9}
          & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
         \hline
         0 &0.10 &0.68 &0.72 &17.06 &0.26 &0.44 &0.72 &7.30 \\
         0.01 &0.12 &0.68 &0.73 &19.55 &0.30 &0.46 &0.71 &6.60 \\
         0.03 &0.15 &0.57 &0.71 &33.89 &0.27 &0.41 &0.73 &22.67 \\
         $0.05^*$ &0.34 &0.48 &0.56 &36.13 &0.35 &0.36 &0.66 &33.60 \\
         0.10 &0.73 &0.32 &0.27 & \textbf{39.16} &0.67 &0.24 &0.43 & \textbf{38.99} \\
         0.15 & \textbf{0.77} & \textbf{0.29} & \textbf{0.26} &38.22 &\textbf{0.73} & \textbf{0.23} & \textbf{0.35} &38.22 \\
         \bottomrule
    \end{tabular}
    \vspace{-3mm}
    \caption{Defense performance of FSMG with different noise budgets on VGGFace2 and CelebA-HQ in a convenient setting. ``*'' is default.}
    \vspace{-3mm}
    \label{tab:fsmg_noise}
\end{table*}



\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{c|cccc|cccc}
        \hline
         \multirow{2}{*}{$\eta$} & \multicolumn{4}{|c|}{``a photo of \textit{sks} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{sks} person''}\\
         \cline{2-9}
          & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
         \hline
         0 &0.10 &0.68 &0.72 &17.06 &0.26 &0.44 &0.72 &7.30 \\
         0.01 &0.11 &0.67 &0.72 &19.97 &0.27 &0.45 &0.72 &6.65 \\
         0.03 &0.12 &0.60 &0.71 &34.34 &0.25 &0.44 &0.73 &18.29 \\
         $0.05^*$ &0.31 &0.50 &0.55 &38.57 &0.34 &0.39 &0.63 &34.89  \\
         0.10 &0.73 &0.36 &0.30 &\textbf{38.83} &0.74 &0.27 &0.36 &\textbf{38.96} \\
         0.15 &\textbf{0.86} & \textbf{0.25} & \textbf{0.19} &38.67 & \textbf{0.82} & \textbf{0.24} & \textbf{0.28} &38.86 \\
         \hline
    \end{tabular}
    \vspace{-3mm}
    \caption{Defense performance of ASPL with different noise budgets on CelebA-HQ in a convenient setting. ``*'' is default.}
    \vspace{-3mm}
    \label{tab:aspl_noise}
\end{table*}

\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{2.5pt}
    \begin{tabular}{l|c|c|cccc|cccc}
        \hline
         \multirow{6}{.076\linewidth}{Model mismatch} & \multirow{2}{*}{Train} & \multirow{2}{*}{Test} & \multicolumn{4}{|c|}{``a photo of \textit{sks} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{sks} person''}\\
         \cline{4-11}
          & & & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
         \cline{2-11}
         & v1.4 & v2.1 &0.37 &0.48 &0.53 &39.28 &0.34 &0.39 &0.64 &33.50 \\
         & v1.4, 1.5, 2.1 & v2.1 &\textbf{0.39} & \textbf{0.46} &\textbf{0.48} &\textbf{38.25} &\textbf{0.44} &\textbf{0.34} &\textbf{0.57} &\textbf{37.29} \\
         \cline{2-11}
         & v1.4 & v2.0 &0.40 &0.46 &0.51 &38.88 &0.43 &0.36 &0.60 &22.21 \\
         & v1.4, 1.5, 2.1 & v2.0 & \textbf{0.56} & \textbf{0.43} & \textbf{0.43} &\textbf{41.83} &\textbf{0.55} &\textbf{0.33} &\textbf{0.51} &\textbf{29.93} \\
         \hhline{===========}
         \multirow{4}{.076\linewidth}{Term/ Prompt mismatch} & \multicolumn{2}{|c|}{\multirow{2}{*}{DreamBooth prompt}} & \multicolumn{4}{|c|}{``a photo of \textit{$S_*$} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{$S_*$} person''}\\
         \cline{4-11}
          & \multicolumn{2}{|c|}{} &FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
         \cline{2-11}
         & \multicolumn{2}{|c|}{``\textit{sks}'' $\rightarrow$ ``\textit{t@t}''} &0.20 &0.17 &0.64 &26.49 &0.17 &0.10 &0.65  &1.14 \\
         & \multicolumn{2}{|c|}{``a dslr portrait of \textit{sks} person''} &0.13 &0.22 &0.69 &18.51 &0.33 &0.51 &0.58 &37.99 \\
         \hline
    \end{tabular}
    \vspace{-3mm}
    \caption{Defense performance of ASPL on CelebA-HQ when the model, term, or prompt used to train the target DreamBooth model is different from the one used to generate defense noise. Here, \textit{$S_*$} is ``t@t'' for the first row and ``sks'' for second row.}
    \label{tab:aspl_cross}
    \vspace{-1mm}
\end{table*}

% \begin{table*}[t]
%     \centering
%     \setlength{\tabcolsep}{2.5pt}
%     \begin{tabular}{l|c|c|cccc|cccc}
%         \toprule
%         \multicolumn{11}{c}{\textbf{VGGFace2}} \\
%         \midrule
%          \multirow{6}{.076\linewidth}{Model mismatch} & \multirow{2}{*}{Train} & \multirow{2}{*}{Test} & \multicolumn{4}{|c|}{``a photo of \textit{sks} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{sks} person''}\\
%          \cline{4-11}
%           & & & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
%          \cline{2-11}
%          & v1.4 & v2.1 &0.57 &0.34 & & &0.69 &0.29 & & \\
%          & v1.4, 1.5, 2.1 & v2.1 &\textbf{0.69} &\textbf{0.26} & & &\textbf{0.76} &\textbf{0.27} & & \\
%          \cline{2-11}
%          & v1.4 & v2.0 &0.69 &0.30 & & &0.63 &0.28 & & \\
%          & v1.4, 1.5, 2.1 & v2.0 &\textbf{0.80} &\textbf{0.21} & & &\textbf{0.75} &\textbf{0.19} & & \\
%          \hhline{===========}
%          \multirow{4}{.076\linewidth}{Term/ Prompt mismatch} & \multicolumn{2}{|c|}{\multirow{2}{*}{DreamBooth prompt}} & \multicolumn{4}{|c|}{``a photo of \textit{$S_*$} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{$S_*$} person''}\\
%          \cline{4-11}
%           & \multicolumn{2}{|c|}{} &FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
%          \cline{2-11}
%          & \multicolumn{2}{|c|}{``\textit{sks}'' $\rightarrow$ ``\textit{t@t}''} &\textbf{0.37} &0.38 & & &0.34 &0.32 & & \\
%          & \multicolumn{2}{|c|}{``a dslr portrait of \textit{sks} person''} &0.07 &\textbf{0.17} & & &\textbf{0.43} &0.39 & & \\
%          \toprule
%          \multicolumn{11}{c}{\textbf{CelebA-HQ}} \\
%         \hline
%          \multirow{6}{.076\linewidth}{Model mismatch} & \multirow{2}{*}{Train} & \multirow{2}{*}{Test} & \multicolumn{4}{|c|}{``a photo of \textit{sks} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{sks} person''}\\
%          \cline{4-11}
%           & & & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
%          \cline{2-11}
%          & v1.4 & v2.1 &0.29 &0.49 & & &0.33 &\textbf{0.37} & & \\
%          & v1.4, 1.5, 2.1 & v2.1 &\textbf{0.47} &\textbf{0.32} & & &\textbf{0.44} &0.47 & & \\
%          \cline{2-11}
%          & v1.4 & v2.0 &0.41 &0.47 & & &0.48 &0.36 & & \\
%          & v1.4, 1.5, 2.1 & v2.0 &\textbf{0.52} &\textbf{0.42} & & &\textbf{0.54} &\textbf{0.32} & & \\
%          \hhline{===========}
%          \multirow{4}{.076\linewidth}{Term/ Prompt mismatch} & \multicolumn{2}{|c|}{\multirow{2}{*}{DreamBooth prompt}} & \multicolumn{4}{|c|}{``a photo of \textit{$S_*$} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{$S_*$} person''}\\
         
%          \cline{4-11}
%          & \multicolumn{2}{|c|}{} &FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
%          \cline{2-11}
%          & \multicolumn{2}{|c|}{``\textit{sks}'' $\rightarrow$ ``\textit{t@t}''} &\textbf{0.24} &0.49 & & &0.22 &0.35 & & \\
%          & \multicolumn{2}{|c|}{``a dslr portrait of \textit{sks} person''} &0.15 &\textbf{0.20} & & &\textbf{0.32} &0.48 & & \\
%          \bottomrule
%     \end{tabular}
%     \vspace{-3mm}
%     \caption{Defense performance of FSMG on VGGFace2 and CelebA-HQ when the model, term, or prompt used to train the target DreamBooth model is different from the one used to generate defense noise. Here, \textit{$S_*$} is ``t@t'' for the first row and ``sks'' for second row. \textcolor{red}{Considered to drop}}
%     \label{tab:fsmg_cross}
%     \vspace{-1mm}
% \end{table*}

% \begin{table*}[t]
%     \centering
%     \setlength{\tabcolsep}{3pt}
%     \begin{tabular}{c|c|cccc|cccc}
%         \toprule
%         \multicolumn{10}{c}{\textbf{VGGFace2}} \\
%         \midrule
%          \multirow{2}{*}{Perturbed} & \multirow{2}{*}{Clean} & \multicolumn{4}{|c|}{``a photo of \textit{sks} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{sks} person''}\\
%          \cline{3-10}
%           & & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
%          \hline
%          4&0 & & & & & & & & \\
%          \hline
%          3&1 & & & & & & & & \\
%          2&2 & & & & & & & & \\
%          1&3 & & & & & & & & \\
%          \hline
%          0&4 & & & & & & & & \\
         
%          \toprule
%          \multicolumn{10}{c}{\textbf{CelebA-HQ}} \\
%         \midrule
%          \multirow{2}{*}{Perturbed} & \multirow{2}{*}{Clean} & \multicolumn{4}{|c|}{``a photo of \textit{sks} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{sks} person''}\\
%          \cline{3-10}
%           & & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
%          \hline
%          4&0 & & & & & & & & \\
%          \hline
%          3&1 & & & & & & & & \\
%          2&2 & & & & & & & & \\
%          1&3 & & & & & & & & \\
%          \hline
%          0&4 & & & & & & & & \\
%          \bottomrule
%     \end{tabular}
%     \vspace{-3mm}
%     \caption{Defense performance of FSMG on VGGFace2 and CelebA-HQ in uncontrolled settings. We include two extra results with 0 clean image (convenient setting) and 0 perturbed image (no defense) for comparison. \textcolor{red}{[Not filled yet]}} % ``*'' means the clean images are from the protecting image subset.
%     \vspace{-3mm}
%     \label{tab:fsmg_uncontrolled}
% \end{table*}

% \begin{table*}[t]
%     \centering
%     \setlength{\tabcolsep}{2.5pt}
%     \begin{tabular}{l|l|l|cccc}
%         \toprule
%          Dataset & Method & Inference Prompt & FDFR$\uparrow$ &ISM$\downarrow$ &SER-FQA$\downarrow$ &BRISQUE$\uparrow$ \\
%          \midrule
%          \multirow{6}{*}{VGGFace2} & \multirow{3}{*}{FSMG} & photo realistic portrait of \textit{sks} person ... & \textbf{0.44} & 0.36 &0.33 &39.07 \\ 
%          & & a photo of \textit{sks} person & 0.06 & \textbf{0.20} &0.71 &19.41 \\
%          & & a dslr portrait of \textit{sks} person &0.23 &0.27 &0.57 &28.17 \\
%          \cline{2-7}
%           & \multirow{3}{*}{ASPL} & photo realistic portrait of \textit{sks} person ... & \textbf{0.51} & 0.36 &0.30 &38.65 \\ 
%          & & a photo of \textit{sks} person & 0.07 & \textbf{0.20} &0.70 &16.45 \\
%          & & a dslr portrait of \textit{sks} person &0.27 &0.27 &0.52 &30.60 \\
%          \midrule
%          \multirow{6}{*}{CelebA-HQ} & \multirow{3}{*}{FSMG} & photo realistic portrait of \textit{sks} person ... & \textbf{0.31} & 0.49 &0.55 &37.32 \\ 
%          & & a photo of \textit{sks} person & 0.11 & \textbf{0.20} &0.71 &16.13 \\
%          & & a dslr portrait of \textit{sks} person &0.12 &0.29 &0.71 &12.08 \\
%          \cline{2-7}
%           & \multirow{3}{*}{ASPL} & photo realistic portrait of \textit{sks} person ... & \textbf{0.30} &0.50 &0.55 &39.01 \\ 
%          & & a photo of \textit{sks} person & 0.12 & \textbf{0.21} &0.70 &16.48 \\
%          & & a dslr portrait of \textit{sks} person &0.10 &0.31 &0.72 &11.03 \\
%          \bottomrule
%     \end{tabular}
%     \vspace{-3mm}
%     \caption{Defense performance of FSMG and ASPL when a complex prompt used to train the target DreamBooth model, which is completely different from the one used to generate defense noise. The full prompt is ``photo realistic portrait of \textit{sks} person, centered in frame, facing camera'', which is also the prompt used to guide the training of advesarial noise}
%     \label{tab:complex_cross_prompt}
%     \vspace{-1mm}
% \end{table*}


\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{c|c|cccc|cccc}
        \hline
         \multirow{2}{*}{Perturbed} & \multirow{2}{*}{Clean} & \multicolumn{4}{|c|}{``a photo of \textit{sks} person''} & \multicolumn{4}{c}{``a dslr portrait of \textit{sks} person''}\\
         \cline{3-10}
          & & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ & FDFR$\uparrow$ & ISM$\downarrow$ & SER-FQA$\downarrow$ & BRISQUE$\uparrow$ \\
         \hline
         4&0 &\textbf{0.31} &\textbf{0.50} & \textbf{0.55} &\textbf{38.57} &\textbf{0.34} & \textbf{0.39} & \textbf{0.63} & \textbf{34.89} \\
         \hline
         3&1 &0.26 &0.54 &0.63 &32.23 &0.30 &0.40 &0.69 &22.03 \\
         2&2 &0.19 &0.61 &0.69 &25.14 &0.25 &0.41 &0.71 &11.35 \\
         1&3 &0.13 &0.65 &0.72 &19.24 &0.23 &0.43 &0.72 &9.70 \\
         \hline
         0&4 &0.10 &0.68 &0.72 &17.06 &0.26 &0.44 &0.72 &7.30 \\
         \hline
    \end{tabular}
    \vspace{-3mm}
    \caption{Defense performance of ASPL on CelebA-HQ in uncontrolled settings. We include two extra results with 0 clean image (convenient setting) and 0 perturbed image (no defense) for comparison.} % ``*'' means the clean images are from the protecting image subset.
    \vspace{-3mm}
    \label{tab:aspl_uncontrolled}
\end{table*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{fig/Astria_1.pdf}
    \caption{\textbf{Disrupting personalized images generated by Astria (SD v1.5 with face detection enabled)}. The prompts for image generation include: (1) ``portrait of \textit{sks} person portrait wearing fantastic Hand-dyed cotton clothes, embellished beaded feather decorative fringe knots, colorful pigtail, subtropical flowers and plants, symmetrical face, intricate, elegant, highly detailed, 8k, digital painting, trending on pinterest, harper's bazaar, concept art, sharp focus, illustration, by artgerm, Tom Bagshaw, Lawrence Alma-Tadema, greg rutkowski, alphonse Mucha'', (2) ``close up of face of \textit{sks} person fashion model in white feather clothes, official balmain editorial, dramatic lighting highly  detailed'', and (3) ``portrait of sks person prince :: by Martine Johanna and Simon Stlenhag and Chie Yoshii and Casey Weldon and wlop :: ornate, dynamic, particulate, rich colors, intricate, elegant, highly detailed, centered, artstation, smooth, sharp focus, octane render, 3d'' }
    \label{fig:astria1}
    \vspace{-5mm}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{fig/sup_fig/astria_protogen_prism.pdf}
    \caption{\textbf{Disrupting personalized images generated by Astria (Protogen with Prism and face detection enabled)}. The prompts for image generation include: (1) ``portrait of \textit{sks} person portrait wearing fantastic Hand-dyed cotton clothes, embellished beaded feather decorative fringe knots, colorful pigtail, subtropical flowers and plants, symmetrical face, intricate, elegant, highly detailed, 8k, digital painting, trending on pinterest, harper's bazaar, concept art, sharp focus, illustration, by artgerm, Tom Bagshaw, Lawrence Alma-Tadema, greg rutkowski, alphonse Mucha'', (2) ``close up of face of \textit{sks} person fashion model in white feather clothes, official balmain editorial, dramatic lighting highly  detailed'', and (3) ``portrait of sks person prince :: by Martine Johanna and Simon Stlenhag and Chie Yoshii and Casey Weldon and wlop :: ornate, dynamic, particulate, rich colors, intricate, elegant, highly detailed, centered, artstation, smooth, sharp focus, octane render, 3d'' }
    \label{fig:astria2}
    \vspace{-5mm}
\end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=\textwidth]{fig/sup_fig/VGG_Complex_cross_prompt.pdf}
%     \caption{Qualativative results of FSMG and ASPL when DreamBooth model are finetuned with a complex prompt (``photo realistic portrait of \textit{sks} person, centered in frame, facing camera'') on VGGFace2, which is different from training prompt of perturbation learning.}
%     \label{fig:vgg_complex_cross_prompt}
%     \vspace{-5mm}
% \end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{fig/sup_fig/VGG_ASPL_versions.pdf}
    \caption{Qualitative results of ASPL with two different versions of SD models (v1.4 and v1.5) on VGGFace2. We provide in each test a single, representative input image. The generation prompts include (1) ``a photo of \textit{sks} person'' and (2) ``a dslr portrait of \textit{sks} person''.} 
    \label{fig:vgg_aspl_versions}
    \vspace{-5mm}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{fig/sup_fig/Celeb_ASPL_versions_new.pdf}
    \caption{Qualitative results of ASPL with two different versions of SD models (v1.4 and v1.5) on CelebA-HQ. We provide in each test a single, representative input image. The generation prompts include (1) ``a photo of \textit{sks} person'' and (2) ``a dslr portrait of \textit{sks} person''.} 
    \label{fig:cel_aspl_versions}
    \vspace{-5mm}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{fig/sup_fig/NoiseBudget_ASPL_SD21_VGG.pdf}
    \caption{Qualativative results of ASPL with different noise budget on VGGFace2.} 
    \label{fig:noisebg_vgg}
    \vspace{-5mm}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{fig/sup_fig/NoiseBudget_ASPL_SD21_CelebA.pdf}
    \caption{Qualitative results of ASPL with different noise budget on CelebA-HQ.} 
    \label{fig:noisebg_celeba}
    \vspace{-5mm}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{fig/sup_fig/VGG_ASPL_cross_model_from_V14.pdf}
    \caption{Qualitative results of ASPL in adverse settings on VGGFace2 where the SD model version in perturbation learning mismatches the one used in the DreamBooth finetuning stage (v1.4 $\rightarrow$ v2.1 and v1.4 $\rightarrow$ v2.0). We test with two random subjects and denote them in green and red, respectively.} 
    \label{fig:vgg_aspl_cross_model}
    \vspace{-5mm}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{fig/sup_fig/VGG_ASPL_Ensemble.pdf}
    \caption{Qualitative results of E-ASPL on VGGFace2, where the ensemble model combines 3 versions of SD models, including v1.4, v1.5, and v2.1. Its performance is validated on two DreamBooth models finetuned on SD v2.1 and v2.0, respectively. We test with two random subjects and denote them in green and red, respectively.} 
    \label{fig:vgg_aspl_ensemble}
    \vspace{-5mm}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{fig/sup_fig/VGG_ASPL_cross_term_and_cross_prompt.pdf}
    \caption{Qualitative results of ASPL on VGGFace2 where the training term and prompt of the target DreamBooth model mismatch the ones in perturbation learning. In the first scenario, the training term is changed from ``sks'' to ``t@t''. In the second scenario, the training prompt is replaced with ``a DSLR portrait of \textit{sks} person'' instead of ``a photo of \textit{sks} person''. Here, \textit{$S^*$} is ``t@t'' for term mismatching and ``sks'' for prompt mismatching. We test with two random subjects and denote them in green and red, respectively.} 
    \label{fig:vgg_aspl_cterm_cprompt}
    \vspace{-5mm}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{fig/sup_fig/Uncontrolled_ASPL_SD21_VGG.pdf}
    \caption{Qualativative results of ASPL in uncontrolled setting on VGGFace2. We denote the perturbed examples and the leaked clean examples in red and green, respectively.} 
    \label{fig:uncontrolled_vgg}
    \vspace{-5mm}
\end{figure*}

\end{document}