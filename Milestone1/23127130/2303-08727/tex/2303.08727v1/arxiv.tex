\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}


\usepackage{epsfig}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{color}
% \usepackage{cite,eucal}
\usepackage{cite}
\usepackage{multirow,xcolor}
\usepackage{arydshln}

\usepackage{enumerate}
\iccvfinalcopy
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\newcommand{\gs}[1]{{\color{purple}[Guansong]: #1}}
\newcommand{\cb}[1]{{\color{teal}[Choubo]: #1}}
% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\newcommand{\method}{{DOM}}

% \iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{8843} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Background Matters: Enhancing Out-of-distribution Detection with Domain Features}

\author{Choubo Ding\\
The University of Adelaide
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Guansong Pang\\
Singapore Management University
\and
Chunhua Shen\\
Zhejiang University
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
   Detecting out-of-distribution (OOD) inputs is a principal task for ensuring the safety of deploying deep-neural-network classifiers in open-world scenarios. OOD samples can be drawn from arbitrary distributions and exhibit deviations from in-distribution (ID) data in various dimensions, such as foreground semantic features (e.g., vehicle images vs. ID samples in fruit classification) and background domain features (e.g., textural images vs. ID samples in object recognition). Existing methods focus on detecting OOD samples based on the semantic features, while neglecting the other dimensions such as the domain features. This paper considers the importance of the domain features in OOD detection and proposes to leverage them to enhance the semantic-feature-based OOD detection methods. To this end, we propose a novel generic framework that can learn the domain features from the ID training samples by a dense prediction approach, with which different existing semantic-feature-based OOD detection methods can be seamlessly combined to jointly learn the in-distribution features from both the semantic and domain dimensions. Extensive experiments show that our approach 1) can substantially enhance the performance of four different state-of-the-art (SotA) OOD detection methods on multiple widely-used OOD datasets with diverse domain features, and 2) achieves new SotA performance on these benchmarks.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

\begin{figure}[t] 
    \centering
    \includegraphics[width=0.99\linewidth]{Figure/motivation.pdf}
    \vspace{-0.1cm}
    \caption{Example images in ID (CIFAR100 \cite{krizhevsky2009learning}) and OOD datasets (SVHN \cite{netzer2011reading}, Places365 \cite{zhou2017places}, Textures \cite{cimpoi2014describing}) with their attention maps from a classification network. The classifier focuses on the semantic features in the ID images and neglects the background domain features. It can mis-classify OOD samples that do not have clear semantics but have similar features to the ID classes.}
    \label{fig:motivation}
    \vspace{-0.5cm}
\end{figure}

Deep neural networks have demonstrated superior performance in computer vision tasks like classification and recognition \cite{lecun2015deep}. Most deep learning methods assume that the training and test data are drawn from the same distribution. Thus, they fails to handle real-world scenarios with out-of-distribution (OOD) inputs that are not present in the training data \cite{quinonero2008dataset}. Failures in distinguishing these OOD inputs from in-distribution (ID) data may lead to potentially catastrophic decisions, especially in safety-critical applications like autonomous driving or medical systems \cite{chen2021robust}. 



OOD detection approaches are designed to address this problem, which aim to detect and reject these OOD samples while guaranteeing the classification of in-distribution data \cite{hendrycks17baseline}.
There are generally two groups of OOD detection approaches. One of them are post-hoc approaches that work with a trained classification network to derive OOD scores without re-training or fine-tuning of the network,
\eg, by using maximum softmax probability of the network outputs \cite{hendrycks17baseline}, maximum logits \cite{HendrycksBMZKMS22}, or the Mahalanobis distance between the input and the class centroids of ID data \cite{lee2018simple}. Another group of approaches fine-tunes the classifiers with different methods, such as the use of pseudo OOD samples \cite{hendrycks2018deep,mohseni2020self, liu2020energy, chen2020robust, papadopoulos2021outlier, Wu_2021_ICCV, Yang_2021_ICCV}. Most of these methods, especially the post-hoc methods, are focused to detect OOD samples based on the \textit{foreground semantic features}, \ie, the features exhibiting the semantic of the in-distribution classes, such as the apple appearance features of `apples' class images in CIFAR100 image classification shown in Fig. \ref{fig:motivation} (left). They, however, neglect the other dimensions that can also be important for OOD detection, since OOD samples can be drawn from arbitrary distributions and can exhibit 
deviations from  in-distribution (ID) data in various dimensions. One of such dimensions is the set of \textit{background domain features} that exhibit no class semantics. This is especially true for OOD samples that contain only small, or none, semantic patches, but have dominant domain features, such as the plates of house number, scenery or textual background in Fig. \ref{fig:motivation} (right). In such cases, semantic-feature-based OOD methods can misclassify these OOD samples to the classes with similar semantic features to these domain features.

This paper considers the importance of the domain features in OOD detection and proposes to leverage them to enhance semantic-feature-based OOD detection methods. To this end, we propose a novel generic framework called X-DOM that can learn the domain features from the ID training samples by a dense prediction approach,
with which different existing semantic-feature-based OOD detection methods can be seamlessly combined 
to jointly learn the in-distribution features from both the semantic and domain dimensions. 
Specifically, given a trained $K$-class classification network where $K$ is the number of in-distribution classes, X-DOM first generates pseudo semantic segmentation masks by a weakly-supervised segmentation approach that uses image-level labels to locate discriminative regions in the images.
These pseudo segmentation masks are then utilized to train a $(K+1)$-class dense prediction network, with the first $K$ classes being the original $K$ in-distribution classes and the $(K+1)$-th class corresponding to the in-distribution background domain features.
The dense prediction network is further converted into a $(K+1)$-class classification network by adding a global pooling layer. The conversion is lossless and requires no re-training. In doing so, the $(K+1)$-class classifier learns both semantic and domain in-distribution features. Different existing semantic-feature-based methods, such as the post-hoc methods, can be applied to the first $K$ prediction outputs to obtain semantic OOD scores, while the $(K+1)$-th prediction can be directly used to define domain OOD scores. Combining these semantic and domain OOD scores enable the detection of OOD samples from both semantic and domain features.

In summary, we make the following main contributions:
\begin{itemize}
\itemsep -0.1cm 
    \item This work studies the importance of learning domain features and proposes to synthesize both domain and semantic features for more effective OOD detection in diverse real-world applications. This provides a new insight into the OOD detection problem. 
    \item We then propose a novel generic approach X-DOM, in which different existing semantic-feature-based OOD detection methods can be seamlessly combined 
    to jointly learn the in-distribution features from both semantic and domain dimensions. It offers a generic approach to enhance current OOD detection methods. To our knowledge, this is the first generic framework for joint semantic and domain OOD detection.
    \item We also introduce a weakly-supervised dense prediction method specifically designed to learn the in-distribution domain features for OOD detection. 
    \item Extensive experiments on four widely-used OOD datasets with diverse domain features show that our approach X-DOM 1) can substantially enhance the performance of four different state-of-the-art (SotA) OOD detection methods, and 2) achieves new SotA performance on these benchmarks.
\end{itemize}

\section{Related Work}
\textbf{Post-hoc Approaches.}
Modelling the uncertainty of pre-trained DNN networks directly without retraining the network is one popular approach for OOD detection \cite{gomes2022igeood, cook2020outlier, huang2021importance, sastry2020detecting, bendale2016towards, wang2021can, Zisselman_2020_CVPR}. Hendrycks et al. \cite{hendrycks17baseline} propose the uncertainty of DNNs and establish a baseline for OOD detection by maximum softmax probability (MSP). ODIN \cite{liang2018enhancing} introduces input perturbation and temperature scaling to enhance MSP. Lee et al. \cite{lee2018simple} propose the deep Mahalanobis distance-based detectors, which compute the distance-based OOD scores from the pre-trained networks' features. Liu et al. \cite{liu2020energy} calculate the logsunexp on logit as the energy OOD score. ReAct \cite{sun2021react} reduces the DNN's overconfidence in OOD samples by activation clipping, which further enhances the energy scores. MaSF \cite{haroush2022a} considers the empirical distribution of each layer and channel in the CNN and returns a p-value as the OOD score. Dong et al. \cite{dong2022neural} observe that the model activation averages for OOD and ID inputs are significantly distinct and compute the neural mean from the batch normalized layer for OOD detection. 
Recently, ViM \cite{wang2022vim} attempts to utilize not only primitive semantic features but also their residuals to define more effective logit-based OOD scores. This type of approach relies on the semantic features learned by the pre-trained networks, which neglects other relevant features, such as domain features. 


\textbf{Fine-tuning Approaches.}
Another dominant approach is to fine-tune the classification networks for adapting to the OOD detection tasks.
In this line of research, Hsu et al. \cite{hsu2020generalized} further improve ODIN by decomposing confidence scoring. Zaeemzadeh et al. \cite{zaeemzadeh2021out} project ID samples into a one-dimensional subspace during training. Some other studies \cite{Vyas_2018_ECCV, huang2021mos} group ID data and assume them as OOD samples for each other to guide the network training. 
Outlier Exposure (OE) \cite{hendrycks2018deep} introduces auxiliary outlier data to train the network and improve its OOD detection performance. Such approaches can use real outliers \cite{mohseni2020self, liu2020energy, chen2020robust, papadopoulos2021outlier, Wu_2021_ICCV, Yang_2021_ICCV} or synthetic ones from generative models \cite{lee2018training}. The performance of this approach often depends on the quality of the outlier data. Fine-tuning the networks may also lead to the loss of semantic information and consequently degraded classification accuracy on the ID data.
Our method does not require additional outliers and can retain the semantic information of the ID data during training.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.95\textwidth]{Figure/framework.pdf}
    \caption{Overview of our proposed framework. It first uses a trained $K$-class classification network to obtain pseudo semantic segmentation masks and then learns the in-distribution features by training a $(K+1)$-class classification network with the pseudo labels (\textbf{Left}). It lastly converts the dense prediction network to a $(K+1)$-class classifier in a lossless fashion, and leverages these $(K+1)$ prediction outputs for joint semantic and domain OOD detection (\textbf{Bottom Right}).
    }% shows how to extract domain information from a K classification network and train a dense prediction network.} %\gs{indicate in the figure that the upper right one is semantic OOD scoring in existing methods; K classes Classification Network -> K-class Classification Network; K+1 classes Classification Network -> (K+1)-class Classification Network}}
    \label{fig:framework}
    \vspace{-0.5cm}
\end{figure*}
%(right) shows the lossless conversion of a dense prediction network and a regular classification network.\gs{the difference between the two classifiers needs to be highlighted; the plug-in property also needs to be visible}

\section{Proposed Approach}
\noindent\textbf{Problem Statement.} 
% The studied out-of-distribution detection problem can be formally stated as follows. 
Given a set of training samples $\mathcal{X}=\{ \mathbf{x}_{i}, y_i
\}_{i=1}^{N}$ drawn from an in-distribution $\mathit{P}_\mathcal{X}$ with label space $\mathcal{Y} = \{ y_{j}
\}_{j=1}^{K}$, and let $f: \mathcal{X} \rightarrow \mathbb{R}^\mathcal{Y}$ be a classifier trained on the in-distribution samples $\mathcal{X}$, then the goal of OOD detection is to obtain a new decision function $g$ to discriminate whether $\mathbf{x}$ come from $\mathit{P}_\mathcal{X}$ or out-of-distribution data $\mathit{P}_{out}$:
\begin{equation*}
    g(\mathbf{x}, f) = \left\{\begin{array}{l}
1 \quad \text{if}\enspace\mathbf{x}\in\mathit{P}_{out},
\\0 \quad \text{if}\enspace\mathbf{x}\in\mathit{P}_\mathcal{X}.
\end{array}\right.
\end{equation*}
The difference between $\mathit{P}_\mathcal{X}$ and $\mathit{P}_{out}$ determines the difficulty of detecting the OOD samples. Existing OOD detection approaches focus on the difference between $\mathit{P}_\mathcal{X}$ and $\mathit{P}_{out}$ based on the semantic information of the class label space $\mathcal{Y}$, 
% ignoring the distance of 
neglecting other relevant dimensions such as the background domain feature space. This work aim to learn the domain features and leverage them to complement these semantic-feature-based OOD detection approaches.

% \begin{figure}[t] 
%     \centering
%     \includegraphics[width=0.99\linewidth]{Figure/sample_img.pdf}
%     \vspace{-0.2cm}
%     \caption{sample image}
%     \label{fig:img}
%     \vspace{-0.5cm}
% \end{figure}

\subsection{Overview of Our Approach}

Using semantic features only to detect OOD samples can often be successful when the OOD samples have some dominant semantics that are different from the ID images. However, this type of approach would fail to work effectively when the OOD samples do not have clear semantics and/or exhibit some similar semantic appearance to the ID samples, \eg, the images illustrated in Fig. \ref{fig:motivation}. %\gs{motivation figure}.
Motivated by this, we introduce a generic framework X-DOM, in which the model learns the background domain features of the in-distribution data, upon which different existing OOD detection methods can be applied with the learned domain feature representations to detect OOD samples 
% with domain distribution gaps efficiently. The potential domain features exist outside the discriminative region of an image and are generally ignored by classification networks. By learning the domain features hidden in the ID training samples, X-DOM can extend the detection dimensionality of existing methods, detect OOD samples 
from both of the semantic and domain feature dimensions.
% and significantly improve the OOD detection performance. 
A high-level overview of our proposed framework is provided in Fig. \ref{fig:framework}. 
\begin{enumerate}[1)]
\itemsep -0.1cm 
    \item X-DOM first learns the in-distribution domain features by a $(K+1)$-class dense prediction network trained from the given pre-trained K-class classification network.
    \item It then seamlessly integrates the domain features into image classification models by transforming the dense prediction network to a $(K+1)$-class classification network, where the prediction entries of the $K$ classes are focused on the class semantics of the $K$ in-distribution class while the extra (+1) class is focused on the in-distribution domain features.
    \item Lastly, an OOD score in the semantic dimension obtained from existing post-hoc OOD detectors based on the $K$-class predictions, and an OOD score obtained from the extra (+1) class prediction from the domain dimension, are synthesized to perform OOD detection.
\end{enumerate}

% to a K+1-class dense prediction network with domain information through two main steps: 1) segment foreground/background and learning domain feature. 2)
%We introduce each main step in detail below.

\subsection{Learning In-distribution Domain Features via $(K+1)$-class Dense Prediction}\label{subsec:domainfeature}

X-DOM aims to explicitly learn representations of the image background information as in-distribution domain features. The key challenge here
% in learning potential domain features
is how to locate these background domain features and separate them from the foreground semantic features. We introduce a weakly-supervised dense prediction method to tackle this challenge, in which weakly-supervised semantic segmentation methods are first utilized to generate pseudo segmentation mask labels that are then used to train a $(K+1)$-class dense prediction network. The extra (+1) class learned in the dense predictor is specifically designed to learn the in-distribution features, while the other $K$ class predictions are focused on learning the semantic features of the $K$ classes given in the training data. Particularly, given the training data $\mathcal{X}$ with $K$-class image-level labels $\mathcal{Y}$ and a trained $K$-class classification network $\phi$, the pseudo segmentation mask labels can be obtained by the class activation mapping \cite{zhou2016learning}:

\begin{equation}
    \mathbf{M}_{y_{\mathbf{x}}}^{(i,j)} = \mathbf{W}_{y_{\mathbf{x}}}^\top \phi_{\text{cnn}}(\mathbf{x})^{(i,j)},
\end{equation}
%-based segmentation method: 
%\gs{list the CAM equation below; please double check whether we are using the original CAM method or improved methods, such as GradCAM.}
where $\mathbf{W}_{y_{\mathbf{x}}}$ is the classification weight of the trained classifier $\phi$ corresponding to the groundtruth class $y_{\mathbf{x}}$ of $\mathbf{x}$, and $\phi_{\text{cnn}}(\mathbf{x})^{(i,j)}$ obtains the feature vector at the unit $(i, j)$ in the feature map extracted by the feature extractor in $\phi$
% $f_{\text{cnn}}$ 
from image $\mathbf{x}$. $\mathbf{M}_{y_{\mathbf{x}}}\in\mathbb{R}^{H\times W}$ is an attention map indicating a pixel-wise semantic score of $\mathbf{x}$ relative to its groundtruth class $y_{\mathbf{x}}$. We then define a foreground decision threshold $\theta$ to generate the fine-grained pseudo labels of background domain pixels and foreground semantic pixels by: %\gs{please write the pseudo label generation in the equation form}

\begin{equation}
    \mathbf{\hat{Y}}^{(i,j)}(\mathbf{x}) = \left\{\begin{array}{ll}
0&\text{if} \; \mathbf{M}_{y_{\mathbf{x}}}^{(i,j)} < \theta,\\
1& \text{if} \; \mathbf{M}_{y_{\mathbf{x}}}^{(i,j)} \geqslant \theta,
\end{array}\right.
\end{equation}
% General classification dataset contains only image-level labels $\mathbf{y}$, which provide semantic information about the images. Our aim is to extract extra information from dataset $\mathcal{X}$ and obtain fine-grained labels to distinguish domain features from semantic features. Following previous work, we can generate pseudo pixel-level labels $\mathit{\hat{y}}$ by exploting pre-trained classification networks and class activation mapping. Class activation mapping $\mathit{M}$ is an algorithm that aids in visualizing CNN and can locate where in the image CNN focus on during classification inference, i.e., the discriminative region. After generating the class activation mapping $\mathit{M}\in\mathbb{R}^{K\times H\times W}$, we can further generate pseudo-mask labels $\mathit{\hat{y}}\in\{0, 1\}^{H\times W}$ by setting a fore-background threshold $\theta$. For pixels with attention scores greater than $\theta$, they are assigned to classes consistent with the image-level labels, and the remaining pixels are considered as background. 
% In our implementation, we 
where the attention scores are normalized into the range $[0,1]$ and $\theta=0.5$ is used.
% to distinguish between foreground and background.
% The traditional classification network is unable to learn knowledge from the generated pseudo pixel-level labels $\mathit{\hat{y}}$. Thus, we propose using a dense prediction network to learn pixel-level knowledge. Differs from  image classification, pixel-wise dense prediction requires outputting prediction labels for each pixel of a given input, which also includes background pixels. As shown in Figure 2 left, 
We then leverage these pseudo labels of the domain and semantic pixels, $\mathbf{\hat{Y}}$, to train a $(K+1)$-class dense prediction network $f_{\Theta_d}:\mathcal{X} \rightarrow \{0, 1\}^{K\times H\times W}$ via a pixel-level cross entropy loss:
% using the pseudo pixel-level labels $\mathit{\hat{y}}$ generated by CAM, with the loss function defined as follows:
\begin{equation}
    L(\mathbf{x}, \mathbf{\hat{Y}}) = \frac{-1}{H\times W}\sum_{i=1}^{H}\sum_{j=1}^{W}\sum_{k=1}^{K+1}\hat{y}_{k}^{(i, j)}\log\left(f(\mathbf{x},\Theta_d)_k^{(i,j)}\right),
\end{equation}
where $f(\mathbf{x},\Theta_d)^{(i,j)}$ outputs a prediction vector consisting of prediction probabilities of the $K+1$ classes at the image pixel $(i,j)$, and $\hat{\mathbf{y}}^{(i, j)}$ denotes the corresponding pseudo labels at the same pixel. In doing so, $f_{\Theta_d}$ learns both in-distribution semantic and domain features.
% the sum over $i$ and sum over $j$ runs over all pixel in input image $\mathbf{x}$, the sum over $k$ goes over the all possible classes.


\subsection{Dense Prediction to Image Classification}
% \subsection{Joint Semantic and Domain Classification}
% Although we successfully learned the latent domain features using the dense prediction network, we obviously cannot use 
The pixel-level semantic and domain features learned in the dense prediction network cannot be applied directly to the image classification task. We show below that the $(K+1)$-class dense prediction network can be transformed to a $(K+1)$-class image classification network in a lossless fashion: the dense prediction and the classification networks share the same weight parameters, and the classification network can be applied to image classification without re-training.  
% We need to transfer the weights of the dense prediction network to the classification network and retain the learned latent domain features. 
Particularly, the dense prediction network $f_{\Theta_d}:\mathcal{X} \rightarrow \{0, 1\}^{K\times H\times W}$ can be decomposed into three main modules: 1) a feature extraction network $f_{\Theta_{\mathit{CNN}}}:\mathcal{X} \rightarrow \mathcal{G}$ consisting of a convolutional neural network that extracts the input image $\mathbf{x}\in\mathbb{R}^{3\times H\times W}$ into a smaller scale but larger dimensional feature map $\mathbf{G}\in\mathbb{R}^{C\times h\times w}$, 2) an upsampling module $\text{up}(\cdot)$ that upsamples the feature map $\mathbf{G}$ to original input size $H\times W$, typically implemented using bilinear interpolation, and 3) a 1x1 convolution classifier $f_{\Theta_{\mathit{cls}}}:\mathcal{G} \rightarrow \mathcal{L}$ that computes the logit for each pixel in the feature map and outputs a logit map $\mathbf{L}\in\mathbb{R}^{K\times H \times W}$. The size of weights of the convolutional classifier is $C\times (K+1)$.
% , where $K+1$ is the number of categories.
% \gs{I think it should be $K+1$ instead of $K$? please check them corresponding if true.} \cb{In this section, what we claim is that the dense prediction network shares the same weights as the classification network for the same number of classes. I think either $K$ or $K+1$ could both be confusing, maybe we can use $C$ to refer to the number of classes here?}. 
Thus, the dense prediction network $f_{\Theta_d}$ can be denoted as:
\begin{equation}
    f(\mathbf{x},\Theta_d) = \text{softmax}(f(\text{up}(f(\mathbf{x},\Theta_{\mathit{CNN}})),\Theta_{\mathit{cls}})),
    \label{eq:dense}
\end{equation}
where $\Theta_d=\{\Theta_{\mathit{CNN}}, \Theta_{\mathit{cls}}\}$.

\begin{figure}[t] 
    \centering
    \includegraphics[width=0.85\linewidth]{Figure/convert.pdf}
    \vspace{-0.2cm}
    \caption{Lossless conversion of a dense prediction network to a classification network.}
    \label{fig:predictiontoclassification}
    \vspace{-0.5cm}
\end{figure}

For a classification network $f_{\Theta_c}: \mathcal{X} \rightarrow \mathbb{R}^\mathcal{Y}$, it can be similarly decomposed into three main modules: 1) a feature extraction network $f_{\Theta_{\mathit{CNN}}}:\mathcal{X} \rightarrow \mathcal{G}$ with the same function as the dense prediction network, 2) a global average pooling $\text{GAP}(\cdot)$, which compresses the feature map of $C\times H\times W$ into a feature vector of size $C\times 1 \times 1$, integrating the features of the full image, and 3) a linear classifier $f_{\Theta_{\mathit{cls}}}:\mathcal{G} \rightarrow \mathcal{L}$, which computes the logit of the full image based on the feature vector with $C\times (K+1)$ weights. The classification network $f_{\Theta_c}$ can be denoted as:
\begin{equation}
    f(\mathbf{x},\Theta_c) = \text{softmax}(f(\text{GAP}(f(\mathbf{x},\Theta_{\mathit{CNN}})),\Theta_{\mathit{cls}})),
    \label{eq:cls}
\end{equation}
where $\Theta_c=\{\Theta_{\mathit{CNN}}, \Theta_{\mathit{cls}}\}$. It is clear from Eqs. (\ref{eq:dense}) and (\ref{eq:cls}) that
% , we can notice that the dense prediction network and the classification network can share the same feature extraction network and weights of the classifier. 
the only difference between $f_{\Theta_d}$ and $f_{\Theta_c}$ is the upsampling module and the GAP module, sharing the same feature extraction network and the classifier. Further, the upsampling and the GAP modules are weight-free operations, which can be easily replaced with each other, as shown in Fig. \ref{fig:predictiontoclassification}. In this way, we directly transfer the in-distribution semantic and domain features learned in the dense prediction network $f_{\Theta_d}$ to the image classification network $f_{\Theta_c}$
% , the dense prediction network can easily replace the upsampling module with the GAP to convert it into a classification network, and they can share all the weights, i.e., the learned knowledge, 
without any loss of the wegith parameters learned in $f_{\Theta_d}$. 
% With the conversion of the network, we can successfully obtain a classification network with learned latent domain features, which will
For a given test image, the classifier $f_{\Theta_c}$ yields a $(K+1)$-dimensional logit vector, where 
% the first $K$ logits can be used to detect OOD samples from the semantic dimension and 
the $(K+1)$-th logit is focused on the in-distribution domain features and can be used directly to detect OOD samples from the domain dimension: 
% \gs{please add an equation to define the domain score $S_D$ here}
% the extra dimension is the background similarity, which can be used as a domain score for OOD detection.
\begin{equation}\label{eq:domain}
    S_d(\mathbf{x}) = \mathbf{l}_{\mathbf{x}}(K+1),
\end{equation}
where $\mathbf{l}_{\mathbf{x}}=f(\text{GAP}(f(\mathbf{x},\Theta_{\mathit{CNN}})),\Theta_{\mathit{cls}})$ is a (K+1)-dimensional prediction logit vector yielded by $f_{\Theta_c}$.

% \subsection{OOD Detection with Domain Score}
\subsection{Joint Semantic and Domain OOD Detection}

Although the domain-feature-based OOD score $S_d$ can be used to detect OOD samples directly, it can miss the OOD samples whose detection relies heavily on the semantic features. Thus, we propose to utilize this domain OOD score to complement existing SotA semantic-feature-based OOD detectors. Particularly, since the first $K$ classification logits in $f_{\Theta_c}$ capture similar class semantics as the original $K$-class classifier, off-the-shelf \textit{post-hoc} OOD detection methods that derive an OOD score from these $K$ classification logits can be plugged into X-DOM to obtain an OOD score from the semantic feature aspect. These domain and semantic feature-based OOD scores are synthesized to achieve a joint semantic and domain OOD detection.

% Following two main steps, the proposed X-DOM successfully extracts potential domain information from both the in-distribution dataset and the pre-trained network. Although we can directly detect out-of-distribution inputs by domain scores, we also miss important semantic information in this way. Ensemble semantic and domain information is more robust in detecting diverse out-of-distribution inputs. Hence, as shown in Fig. 2, we split the logit output of the new classification network into two parts: the background classes are output separately as domain scores. The remaining logit is consistent with the output of the original classification network and can be considered a semantic logit. We could apply

There are generally two types of post-hoc OOD detection approaches, including raw logit-based and softmax probability-based methods. Our domain-feature-based OOD score is based on an unbounded logit value, which can dominant the overall OOD score when combining with the semantic-feature-based OOD score using the softmax output (its value is within $[0,1]$). To avoid this situation, we take a different approach to combine the domain and semantic feature-based OOD scores, depending on the type of the semantic-feature-based OOD detector used:
\begin{equation}\label{eqn:combine}
S(\mathbf{x}) = \left\{\begin{array}{ll}
S_h(\mathbf{x}) + \frac{\log(S_d(\mathbf{x}))}{T}&\text{if} \; S_h \; \text{is softmax-based},
\\S_h(\mathbf{x}) + \frac{S_d(\mathbf{x})}{T} & \text{if} \; S_h \; \text{is logit-based},
\end{array}\right.    
\end{equation}
where $S(\mathbf{x})$ is the final OOD score used to perform OOD detection in X-DOM, $S_h(\mathbf{x})=h(\mathbf{x})$ denotes the OOD score obtained from using an existing semantic-feature-based OOD scoring function $h$, and $T$ is a temperature coefficient hyperparameter. In Eq. (\ref{eqn:combine}), to obtain faithful semantic-and-domain-combined OOD score, the log function is used to constrain the value and the variance of the domain scores $S_d$, while $T$ is used to adjust the distribution of the domain scores to match that of the semantic scores.

% One of the main problems of ensemble two different scores is the different value domains and variances. For example, the value domain of the semantic score output by the softmax probability-based method lies between 0 and 1. But the domain score taken from the not-bounded logit may be much higher than this value, and this significant difference may cause the performance of the ensemble score to be determined by domain score. For this situation, we propose using the log function to constrain the value domain and variance of the domain scores when applying X-DOM on the softmax probability-based methods. In contrast, the logit-based methods do not require similar constraints due to the similarity of the value domain to the logit-based domain scores. In summary, the ensemble of semantic scores and domain scores can be shown as:
% \begin{equation}
% S_E = \left\{\begin{array}{ll}
% S + \log(S_D)&\text{if S}\in\text{softmax-based},
% \\S + S_D&\text{if S}\in\text{logit-based},
% \end{array}\right.    
% \end{equation}

% where $S_D$ represents domain scores and $S$ represents semantic scores. Other than the significant differences mentioned above, there are still some subtle distribution differences between domain and semantic scores due to various reasons (e.g., unbalanced samples, bias in the dataset, etc.), which lead to bias in the final OOD scores. To this end, we propose to use temperature coefficients $T$ to control the distribution of domain and semantic scores, not only to balance the difference between them but even to detect OOD inputs with different attributes by adjusting $T$. The final OOD score with the temperature $T$ can be presented as:
% \begin{equation}
% S_E = \left\{\begin{array}{ll}
% S + \frac{\log(S_D)}{T}&\text{if S}\in\text{softmax-based},
% \\S + \frac{S_D}{T} & \text{if S}\in\text{logit-based},
% \end{array}\right.    
% \end{equation}
% We further discuss and compare the performance of the semantic, domain, and ensemble scores in section A and analyse the effects of various temperatures T in section B.

\section{Experiments}
%We firstly introduce the benchmark dataset and other experimental settings. In Section 4.2, we show that X-DOM can effectively improve the performance of the baseline method and achieves state-of-the-art OOD detection performance. We demonstrate the extensive ablation of X-DOM in Section 4.3 and discuss other applications of X-DOM in Section 4.4.
%\subsection{Experimental Setup}
\noindent\textbf{Datasets.}  Following \cite{hendrycks17baseline, liang2018enhancing, lee2018simple, liu2020energy, haroush2022a, dong2022neural}
%\gs{add references}
% prior work
, we choose 
% the most
two widely used classification datasets: CIFAR10 and CIFAR100 \cite{krizhevsky2009learning}, as the in-distribution datasets. %CIFAR10 and CIFAR100 both contain 50,000 training sets and 10,000 test sets. 
% We only use the 
% Its training set
% of CIFAR10 / CIFAR100
% is used to train the model and use the test set as ID data in the evaluation.
As OOD samples are unknown during training, 
% detection evaluation,
their respective training and test data are used as ID data, with samples from a different dataset added into the test set as the OOD data.
To 
% sufficiently 
evaluate the effectiveness of our approach, 
% we analyse and choose
four commonly-used OOD datasets consisting of natural image datasets with 
diverse background domain features are used, including SVHN \cite{netzer2011reading}, Places365 \cite{zhou2017places}, Textures \cite{cimpoi2014describing}, and CIFAR100/CIFAR10 \cite{krizhevsky2009learning} (CIFAR100 is used as OOD data when CIFAR10 is used as ID data, and vise versa \cite{fort2021exploring, ren2021simple, sastry2020detecting}) 
%\gs{references} \cb{what kind of references for this?}\gs{papers that use these two datasets as OOD datasets against each other}).
% different distributions as OOD test datasets: 
%to prove the effectiveness of X-COM from various distribution dimensions:
SVHN
% \textbf{SVHN} 
is a digit classification dataset cropped from pictures of house number plates, 
% . SVHN has significant semantic distribution bias against the CIFAR10/100, and unique scenes also introduce certain domain distribution bias. We consider SVHN as a dataset containing both distribution biases.
% \textbf{
Places365 is a large-scale scene classification dataset,
% . As similar to SVHN, Places365 also has semantic distribution bias and domain distribution bias against CIFAR10/CIFAR100. Following \cite{huang2021mos}, we use a 10,000 images subset of Places365 as OOD data.
% \textbf{
while Textures contains 5,640 texture images in the wild 
that
% . Since texture images 
do not contain specific objects and backgrounds. Images in all these three datasets exhibit largely different semantic and domain distributions, so the three datasets contain strong out-of-distribution semantic and domain features. 
% , we consider Textures has a significant bias in both semantic and domain distributions.
% \textbf{CIFAR100/CIFAR10} Since no overlapping categories exist, CIFAR10 and CIFAR100 can be considered OOD test data for each other. 
On the other hand, both CIFAR10 and CIFAR100 are sampled from 
% a subset of 
Tiny Images \cite{torralba200880}
%\gs{references needed
and they share similar domain features, so when they are used as OOD data for each other, the domain features are weak. Also, the objects in the images of these two datasets can be very similar, so the semantic OOD features are also weak. As a result, this pair of mutual OOD/ID combination is considered as hard OOD detection benchmarks \cite{winkens2020contrastive}
%\gs{references}. 
% The OOD feature types are summarized in Tab. \ref{tab:datasets}.
% while OOD semantic features .
% can be regarded as having no domain distribution bias and only semantic distribution bias. Some studies [] consider OOD detection between this pair of datasets as a challenging task.


\begin{table*}[bt]
  \centering
  \caption{\textbf{OOD detection results with CIFAR10 as in-distribution data.} 
%   OOD detection performance for various competitive/SotA methods and ours X-DOM when $\mathcal{D}_{in}$=CIFAR10. 
  All methods are based on ID training data without using any external outlier data. $^{\dagger}$ indicates that the results are taken from the original paper, and other methods are reproduced using the same network architecture. Four post-hoc semantic OOD detection methods are respectively plugged into our method X-DOM, with improved results highlighted in \textcolor{red}{red} and in \textcolor{blue}{blue} otherwise. The best result per dataset is \textbf{boldfaced}. }
  \vspace{-0.0cm}
  \scalebox{0.88}{
    \begin{tabular}{p{3.8cm}cccccc}
    \hline
    \multirow{3}{*}{\textbf{Methods}} & \multicolumn{4}{c}{\textbf{OOD Datasets}} & \multirow{2}{*}{\textbf{Average}}\\
    \cline{2-5} 
    & \multicolumn{1}{c}{\textbf{CIFAR100}} & \multicolumn{1}{c}{\textbf{SVHN}} & \multicolumn{1}{c}{\textbf{Places365}} & \multicolumn{1}{c}{\textbf{Textures}} &  \\
    & \multicolumn{5}{c}{\textbf{FPR95}$\downarrow$ /\textbf{AUROC}$\uparrow$ /\textbf{AUPR}$\uparrow$} \\
    \hline
    MaxLogit \cite{HendrycksBMZKMS22} {\scriptsize \textcolor{gray}{[ICML'22]}} & 39.11/85.07/78.13 & 17.95/94.78/84.22 & 24.05/91.10/86.41 & 7.93/97.57/98.07 & 22.26/92.13/86.71  
    \\
    KL-Matching \cite{HendrycksBMZKMS22} {\scriptsize \textcolor{gray}{[ICML'22]}} & 33.63/90.20/88.18 & 25.70/95.21/88.37 & 25.25/92.88/90.78 & 12.61/97.28/98.21 & 24.30/93.89/91.38
    \\
    ReAct \cite{sun2021react} {\scriptsize \textcolor{gray}{[NIPS'21]}}& 34.75/84.10/79.89 & 20.03/90.58/76.30 & 23.45/91.88/89.43 & 10.27/96.53/97.69 & 22.12/90.77/85.83
    \\
    MaSF$^{\dagger}$ \cite{haroush2022a} {\scriptsize \textcolor{gray}{[ICLR'22]}}&  - /82.10/ -  &  - /99.80/ -  &  - /96.00/ -  &  - /98.50/ -  &  - /94.10/ - 
    \\
    NMD$^{\dagger}$ \cite{dong2022neural} {\scriptsize \textcolor{gray}{[CVPR'22]}}&  - /90.10/ -  &  - /99.60/ -  &  - / - / -  &  - /98.90/ -  &  - /72.15/ - 
    \\
    \cline{1-6}
    MSP \cite{hendrycks17baseline} {\scriptsize \textcolor{gray}{[ICLR'17]}}& 33.44/89.01/84.10 & 17.40/95.72/88.68 & 22.47/92.93/89.79 & 8.55/97.66/98.38 & 20.46/93.83/90.24 
    \\
    MSP-DOM {\scriptsize \textcolor{gray}{[Ours]}} & \textcolor{red}{23.75}/\textcolor{red}{94.29}/\textcolor{red}{93.48} & \textcolor{red}{2.55}/\textcolor{red}{98.94}/\textcolor{red}{97.88} & \textcolor{red}{5.05}/\textcolor{red}{98.49}/\textcolor{red}{98.60} & \textcolor{red}{0.02}/\textcolor{red}{99.90}/\textcolor{red}{99.95} & \textcolor{red}{7.84}/\textcolor{red}{97.90}/\textcolor{red}{97.48}
    \\
    % & 9.69/5.27/9.39 & 14.85/3.22/9.20 & 17.42/5.56/8.81 & 8.53/2.24/1.57 & 12.62/4.07/7.24 \\
    \hdashline
    ODIN \cite{liang2018enhancing} {\scriptsize \textcolor{gray}{[ICLR'18]}}& 34.62/87.83/81.92 & 16.13/95.66/87.30 & 22.15/92.43/88.59 & 7.45/97.86/98.37 & 20.09/93.45/89.04
    \\
    ODIN-DOM {\scriptsize \textcolor{gray}{[Ours]}} & \textcolor{red}{22.15}/\textcolor{red}{95.50}/\textcolor{red}{95.29} & \textcolor{red}{4.27}/\textcolor{red}{99.19}/\textcolor{red}{98.30} & \textcolor{red}{8.08}/\textcolor{red}{98.66}/\textcolor{red}{98.76} & \textcolor{red}{0.34}/\textcolor{red}{99.92}/\textcolor{red}{99.95} & \textcolor{red}{8.71}/\textcolor{red}{98.32}/\textcolor{red}{98.07}
    \\
    % & 12.47/7.67/13.37 & 11.86/3.53/11.00 & 14.07/6.22/10.17 & 7.11/2.05/1.58 & 11.38/4.87/9.03 \\
    \hdashline
    Energy \cite{liu2020energy} {\scriptsize \textcolor{gray}{[NIPS'20]}}& 41.98/84.25/77.47 & 19.73/94.46/83.67 & 25.42/90.74/86.06 & 8.72/97.45/97.99 & 23.96/91.73/86.30
    \\
    Energy-DOM {\scriptsize \textcolor{gray}{[Ours]}} & \textcolor{red}{19.90}/\textcolor{red}{94.98}/\textcolor{red}{93.76} & \textcolor{red}{3.10}/\textcolor{red}{99.28}/\textcolor{red}{98.14} & \textcolor{red}{6.96}/\textcolor{red}{98.60}/\textcolor{red}{98.52} & \textcolor{red}{0.53}/\textcolor{red}{99.87}/\textcolor{red}{99.91} & \textcolor{red}{7.62}/\textcolor{red}{98.19}/\textcolor{red}{97.58}
    \\
    % & 22.08/10.73/16.29 & 16.63/4.82/14.47 & 18.46/7.87/12.45 & 8.19/2.43/1.92 & 16.34/6.46/11.28 \\
    \hdashline
    ViM \cite{wang2022vim} {\scriptsize \textcolor{gray}{[CVPR'22]}}& 15.25/96.92/\textbf{96.78} & 1.27/99.47/99.08 & 2.74/99.32/99.34 & 0.11/99.93/99.96 & 4.84/98.91/98.79
    \\
    ViM-DOM {\scriptsize \textcolor{gray}{[Ours]}} & \textcolor{red}{\textbf{13.49}}/\textcolor{red}{\textbf{97.08}}/\textcolor{blue}{96.75} & \textcolor{red}{\textbf{0.41}}/\textcolor{red}{\textbf{99.85}}/\textcolor{red}{\textbf{99.68}} & \textcolor{red}{\textbf{0.72}}/\textcolor{red}{\textbf{99.85}}/\textcolor{red}{\textbf{99.85}} & \textcolor{red}{\textbf{0.00}}/\textcolor{red}{\textbf{100.00}}/\textcolor{red}{\textbf{100.00}} & \textcolor{red}{\textbf{3.65}}/\textcolor{red}{\textbf{99.20}}/\textcolor{red}{\textbf{99.07}}
    \\
    % & 1.76/0.16/\textcolor{red}{-0.03} & 0.86/0.38/0.60 & 2.02/0.53/0.51 & 0.11/0.07/0.04 & 1.19/0.28/0.28 \\
    \hline
    \end{tabular}%
  }
  \label{tab:main_result_cifar10}%
  \vspace{-0.2cm}
\end{table*}%


\begin{table*}[bt]
  \centering
  \caption{\textbf{OOD detection results with CIFAR100 as in-distribution data.} 
%   OOD detection performance for various competitive/SotA methods and ours X-DOM when $\mathcal{D}_{in}$=CIFAR100.
The notations here are the same as that in Tab. \ref{tab:main_result_cifar10}.}
  \vspace{0.1cm}
  \scalebox{0.88}{
    \begin{tabular}{p{3.8cm}ccccc}
    \hline
    \multirow{3}{*}{\textbf{Methods}} & \multicolumn{4}{c}{\textbf{OOD Datasets}} & \multirow{2}{*}{\textbf{Average}}\\
    \cline{2-5} 
    & \multicolumn{1}{c}{\textbf{CIFAR10}} & \multicolumn{1}{c}{\textbf{SVHN}} & \multicolumn{1}{c}{\textbf{Places365}} & \multicolumn{1}{c}{\textbf{Textures}} &  \\
    & \multicolumn{5}{c}{\textbf{FPR95}$\downarrow$ /\textbf{AUROC}$\uparrow$ /\textbf{AUPR}$\uparrow$} \\
    \hline
    MaxLogit \cite{HendrycksBMZKMS22} {\scriptsize \textcolor{gray}{[ICML'22]}}& 61.61/81.09/79.25 & 37.12/91.29/80.77 & 71.89/73.12/67.64 & 37.61/90.63/93.73 & 52.06/84.03/80.35 
    \\
    KL-Matching \cite{HendrycksBMZKMS22} {\scriptsize \textcolor{gray}{[ICML'22]}}& 64.49/79.54/74.46 & 47.86/89.08/76.63 & 73.55/78.04/76.61 & 46.63/88.97/92.16 & 58.13/83.91/79.96
    \\
    ReAct \cite{sun2021react} {\scriptsize \textcolor{gray}{[NIPS'21]}}& 70.81/79.62/78.97 & 53.00/88.88/78.43 & 82.64/68.11/63.28 & 52.80/88.15/92.58 & 64.81/81.19/78.31
    \\
    MaSF$^{\dagger}$ \cite{haroush2022a} {\scriptsize \textcolor{gray}{[ICLR'22]}}&  - /64.00/ -  &  - /96.90/ -  &  - /81.10/ -  &  - /92.00/ -  &  - /83.50/ - 
    \\
    \cline{1-6} 
    MSP \cite{hendrycks17baseline} {\scriptsize \textcolor{gray}{[ICLR'17]}}& 64.25/81.52/80.87 & 49.50/88.92/79.07 & 72.10/76.18/71.52 & 46.24/89.33/93.44 & 58.02/83.99/81.23 
    \\
    MSP-DOM {\scriptsize \textcolor{gray}{[Ours]}} & \textcolor{red}{58.76}/\textcolor{red}{84.67}/\textcolor{red}{84.25} & \textcolor{blue}{50.75}/\textcolor{red}{89.27}/\textcolor{red}{82.39} & \textcolor{red}{67.82}/\textcolor{red}{85.20}/\textcolor{red}{88.06} & \textcolor{red}{28.21}/\textcolor{red}{95.86}/\textcolor{red}{97.85} & \textcolor{red}{51.38}/\textcolor{red}{88.75}/\textcolor{red}{88.14}
    \\
    \hdashline
    % & 5.49/3.15/3.38 & \textcolor{red}{-1.25}/0.35/3.32 & 4.28/9.02/16.54 & 18.03/6.53/4.40 & 6.64/4.76/6.91 \\
    ODIN \cite{liang2018enhancing} {\scriptsize \textcolor{gray}{[ICLR'18]}}& 59.67/82.39/80.79 & 38.11/91.32/81.40 & 69.80/75.39/69.81 & 37.38/91.10/94.22 & 51.24/85.05/81.55
    \\
    ODIN-DOM {\scriptsize \textcolor{gray}{[Ours]}} & \textcolor{red}{55.92}/\textcolor{red}{87.31}/\textcolor{red}{88.02} & \textcolor{red}{32.79}/\textcolor{blue}{90.60}/\textcolor{blue}{76.03} & \textcolor{red}{55.34}/\textcolor{red}{81.56}/\textcolor{red}{79.62} & \textcolor{red}{10.78}/\textcolor{red}{97.40}/\textcolor{red}{98.23} & \textcolor{red}{38.71}/\textcolor{red}{89.22}/\textcolor{red}{85.48}
    \\
    % & 3.75/4.91/7.23 & 5.32/\textcolor{red}{-0.72}/\textcolor{red}{-5.37} & 14.46/6.17/9.82 & 26.60/6.29/4.01 & 12.53/4.16/3.92 \\
    \hdashline
    Energy \cite{liu2020energy} {\scriptsize \textcolor{gray}{[NIPS'20]}} & 64.34/80.48/78.89 & 36.76/91.38/80.98 & 74.75/72.14/67.10 & 39.17/90.37/93.61 & 53.75/83.59/80.15
    \\
    Energy-DOM {\scriptsize \textcolor{gray}{[Ours]}} & \textcolor{red}{\textbf{54.02}}/\textcolor{red}{\textbf{88.12}}/\textcolor{red}{\textbf{88.82}} & \textcolor{red}{24.78}/\textcolor{red}{93.39}/\textcolor{red}{81.94} & \textcolor{red}{48.87}/\textcolor{red}{85.72}/\textcolor{red}{82.67} & \textcolor{red}{7.11}/\textcolor{red}{98.41}/\textcolor{red}{98.92} & \textcolor{red}{33.70}/\textcolor{red}{91.41}/\textcolor{red}{88.09}
    \\
    % & 10.32/7.64/9.93 & 11.98/2.01/0.97 & 25.88/13.58/15.57 & 32.06/8.04/5.31 & 20.06/7.82/7.94 \\
    \hdashline
    ViM \cite{wang2022vim} {\scriptsize \textcolor{gray}{[CVPR'22]}} & 59.13/85.72/85.87 & 10.23/97.90/95.74 & 49.38/87.23/86.19 & 2.45/99.47/99.69 & 30.30/92.58/91.87 
    \\
    ViM-DOM {\scriptsize \textcolor{gray}{[Ours]}} & \textcolor{blue}{60.88}/\textcolor{red}{85.74}/\textcolor{red}{86.38} & \textcolor{red}{\textbf{7.58}}/\textcolor{red}{\textbf{98.40}}/\textcolor{red}{\textbf{96.39}} & \textcolor{red}{\textbf{20.93}}/\textcolor{red}{\textbf{96.06}}/\textcolor{red}{\textbf{96.23}} & \textcolor{red}{\textbf{0.16}}/\textcolor{red}{\textbf{99.96}}/\textcolor{red}{\textbf{99.98}} & \textcolor{red}{\textbf{22.39}}/\textcolor{red}{\textbf{95.04}}/\textcolor{red}{\textbf{94.74}}
    \\
    % & \textcolor{red}{-1.75}/0.03/0.50 & 2.65/0.49/0.66 & 28.45/8.83/10.03 & 2.29/0.49/0.29 & 7.91/2.46/2.87 \\
    \hline
    \end{tabular}%
  }
  \label{tab:main_result_cifar100}%
  \vspace{-0.3cm}
\end{table*}%

% \begin{table}[!htb]
%   \centering
%   \caption{Summary of OOD feature types for different OOD datasets.}
%   \vspace{-0.2cm}
%   \scalebox{0.86}{
%     \begin{tabular}{|p{1.0cm}cccc|}
%     \hline
%     \multicolumn{1}{|l}{\textbf{ID:CIFAR10}}   & \multicolumn{1}{|c}{\textbf{CIFAR100}} & \multicolumn{1}{c}{\textbf{SVHN}} &
%     \multicolumn{1}{c}{\textbf{Places365}} &
%     \multicolumn{1}{c|}{\textbf{Textures}}  \\ 
%     \hline
%     \multicolumn{1}{|l}{OOD semantic} & \multicolumn{1}{|c}{Strong}  &\multicolumn{1}{c}{Strong} &\multicolumn{1}{c}{Strong} &\multicolumn{1}{c|}{Strong}\\
    
%     \multicolumn{1}{|l}{OOD domain} & \multicolumn{1}{|c}{Weak} & \multicolumn{1}{c}{Strong} & \multicolumn{1}{c}{Strong} & \multicolumn{1}{c|}{Strong} \\ 
%      \hline
%     \end{tabular}%
%     }
%     \vspace{-0.2cm}
%   \label{tab:datasets}%
% \end{table}%
%\gs{Add a small table here to summarize the domain feature type of each OOD data: two columns -- OOD domain and OOD semantic, in addition to columns to fill up the ID and OOD datasets}

\noindent\textbf{Implementation Details.}
%  We establish all experiments based on the Google BiT-M \cite{kolesnikov2020big} model. This model is 
We use BiT-M \cite{kolesnikov2020big}, a variant of ResNetv2 architecture \cite{he2016identity}, as the default network backbone throughout the experiments. The official release checkpoint of BiT-M-R50x1 trained on ImageNet-21K is used as our initial $K$-class in-distribution classification model.
% \textbf{In-distribution Classification Pretrain Details} 
% We follow the BiTHyperRule \cite{kolesnikov2020big} setting to train the baseline classification network on 
The model is further fine-tuned on the in-distribution dataset (CIFAR10/CIFAR100)
% with pre-trained weights. The classification network was trained using 
with 20,000 steps using a batch size of 128. SGD is used as the optimizer with an initial learning rate of 0.003 and a momentum of 0.9. 
% We used the STEP learning rate decay strategy, which
We decay the learning rate by a factor of 10 at 30\%, 60\%, and 90\% of the training steps. 
% Moreover, we used a learning rate warm-up in the first 500 steps of training. 
All images were resized to 160x160 and randomly cropped to 128x128. 
% Finally, we used 
The Mixup \cite{zhang2018mixup} with $\alpha = 0.1$ is also used to synthesize new image samples during training.
% Most of the \textit{post hoc} out-of-distribution detection comparison methods in the experiments are based on the classification models trained from this step.
% \textbf{Generation of Pseudo-mask} Based on the well-trained classification network, 
We subsequently use CAM (Class Activation Mapping) \cite{zhou2016learning} to generate the pseudo mask labels for each in-distribution image based on a multi-scale masking method used in \cite{ahn2019weakly} (see \texttt{Appendix A.1} for the example of pseudo mask). 
% Consistent with \cite{ahn2019weakly}, we use the ensemble of multi-scale images to generate accurate pseudo-mask labels. Specifically, an input image is converted to a set of 8 images through 4 different scales $\{0.5, 1.0, 1.5, 2.0\}$ and horizontal flips. After computing the mean values of the CAMs at all scales, the final CAM is smoothed by a Gaussian filter and converted to pseudo-mask labels.

% \textbf{Dense Prediction Training Details} Finally, 
With these pseudo mask labels, we then use a modified Dense-BiT architecture %\gs{references?} no references, Dense-BiT is propose by us
to train the $(K+1)$-class dense prediction model with the BiT-M-R50x1 checkpoints as the initial weights.
% on the in-distribution dataset and pseudo-mask labels. 
All input images are resized to 128x128 during training and inference. We replace the Mixup augmentation used in the training with randomly scaling (from 0.5 to 2.0) and randomly horizontally flipping augmentation. The other training strategy and hyperparameters are maintained the same as the ones used in training the $K$-class classification network above. After that, the dense prediction model is converted to $(K+1)$-class image classification model using Eq. (\ref{eq:cls}).

Four \textit{post-hoc} OOD detection methods, including MSP \cite{hendrycks17baseline}, ODIN \cite{liang2018enhancing}, Energy \cite{liu2020energy}, and ViM \cite{wang2022vim}, are used as the plug-in base models. They are respectively employed to combine with X-DOM to detect OOD samples in both of semantic and domain features. To have fair and straightforward comparison, these four plug-in models are built upon the same $K$-class classification model as X-DOM. The temperature $T=2.5$ is used in Eq. (\ref{eqn:combine}) by default.
% \gs{do we have a default T setting throughout the experiments? please present the setting of this parameter here.}

We will release our code upon paper acceptance.

\noindent\textbf{Evaluation Metrics.}
% Consistent with the literature
We use three widely-used evaluation metrics for OOD detection,
% to evaluate the out-of-distribution detection performance of network
including: 1) \textbf{FPR95} that evaluates the false positive rate of the OOD samples when the true positive rate of the in-distribution samples is 95\%, 2) \textbf{AUROC} denotes the Area Under the Receiver Operating Characteristic curve,
% which is a threshold-free metrics. (
and 3) \textbf{AUPR} is the Area under the Precision-Recall curve. The ID images are the positive samples in calculating AUROC and AUPR to measure the OOD detection performance. 
% Moreover, we evaluated 
In addition, we also report the \textbf{Top-1 accuracy} of classifying the in-distribution samples. 
%\gs{have you included the classification accuracy yet?}


\subsection{Main Results}
% In this section, we report the performance of X-DOM and various comparison methods for each dataset introduced in Section 4.1.1. The main results are categorized into two groups based on the in-distribution dataset and are reported in Tab1 and Tab2, respectively.




The OOD detection results of X-DOM and its competing methods with CIFAR10 and CIFAR100 as in-distribution data are reported in Tabs. \ref{tab:main_result_cifar10} and \ref{tab:main_result_cifar100}, respectively. Overall, X-DOM substantially improves four different SotA detection methods in all three evaluation metrics on both datasets, and obtains new SotA performance. We discuss the results in detail as follows.

\vspace{0.1cm}
\noindent\textbf{Enhancing Different OOD Detection Methods.} 
% For the evaluation of the boosting performance of X-DOM, we pick three popular (
Four different SotA methods -- MSP, ODIN, Energy, and ViM -- 
% and one SotA (ViM) post hoc semantic scoring methods (i.e., they derive OOD scores from classification models trained on in-distribution data without re-training the model)
are used as semantic-feature-based OOD detection baseline models and 
plugged into
% in combination with the domain scores of 
X-DOM to perform joint semantic and domain OOD detection. Their results are shown at the bottom of Tabs.  \ref{tab:main_result_cifar10} and \ref{tab:main_result_cifar100}.
% All the above methods do not rely on additional outlier data to maintain a fair evaluation. The methods that do not release an official implementation report the best results from the original paper. The remaining methods are based on well-trained in-distribution data classification models with the same architecture (BiT-M-R50x1).



Compared to all the four plug-in base models, X-DOM can significantly improve the performance of all evaluation metrics in terms of the average results over the four OOD datasets on both of the CIFAR10 and CIFAR100 datasets. In particular, for the averaged improvement across the four base models, X-DOM boosts the FPR95 by $10.38\%$, the AUROC by $3.92\%$ and the AUPR by $6.96\%$ AUPR in Tab. \ref{tab:main_result_cifar10}; and similarly, it boosts the FPR95 by $11.78\%$ , the AUROC by $4.8\%$ and the AUPR by $5.41\%$ in Tab. \ref{tab:main_result_cifar100}. 
% With the Energy method showing the most significant improvement, Energy-DOM boosts FPR95 by $16.34\%$ on average over the four datasets. 
Note that even for the base model ViM, the most recent SotA method, X-DOM can still considerably enhance its performance, especially on some datasets where ViM does not work well, such as the SVHN, Places365, and Textures datasets, resulting in over 6\% reduction in FPR95 and 2.5\% increase in both AUROC and AUPR on the CIFAR100 data. 
X-DOM shows slight performance drops on some metrics of CIFAR and SVHN OOD datasets, which are mainly caused by suboptimal combinations of the domain and semantic OOD scores with the default temperature setting. The explanation would be discussed in detail using Fig. \ref{fig:T} in Sec. \ref{subsec:ablation}.
% For CIFAR benchmark, the major reason is that domain scores do not work well on this benchmark where the domain difference is weak ,increase the temperature could alleviate this problem. The performance drop of SVHN benchmark occurs on the softmax-probability-based methods, because (K+1)-logit leads to a decrease of the maximum-softmax-probability, which decreases the performance of the semantic features. Similar problems can be avoided in practice by decresse the temperature.
%\gs{please add some explanation here to explain the performance drops.} \cb{It looks like X-DOM does not lower ViM's overall FPR95, but rather boosts it by 8\%. X-DOM only reduces ViM's FPR95 by about 1.7\% on CIFAR100 vs. CIFAR10. Do we need to explain this?}\gs{yes, need to explain as many failure cases as possible}
% shows the lowest improvement, with only boosts FPR95 by $1.19\%$, which we attribute to the simple CIFAR10-based out-of-distribution detection task:

\begin{figure}[t] 
    \centering
    \includegraphics[width=0.95\linewidth]{Figure/Analyse_Scores.pdf}
    \vspace{-0.2cm}
    \caption{Distribution of the semantic/domain OOD scores of ID (CIFAR10/100) and OOD samples (Textures) in X-DOM.
    % In X-DOM, the domain-feature-based OOD scores enable significantly better ID and OOD separation than the semantic-feature-based OOD scores.
    % With the OOD dataset inclined to domain distribution bias, Domain scores from image background significantly outperform common semantic scores (Energy\cite{liu2020energy}). 
    %\gs{In: Cifar10 -> ID: CIFAR10; In: Cifar100 -> ID: CIFAR100; In-Distribution -> In-distribution; Out-of-Distribution -> Out-of-distribution}
    }
    \label{fig:domain_score}
    \vspace{-0.2cm}
\end{figure}
\begin{figure}[t] 
    \centering
    \includegraphics[width=0.95\linewidth]{Figure/TSNE.pdf}
    \vspace{-0.2cm}
    \caption{t-SNE visualization of the features learned by the vanilla classification network and X-DOM, where the colored dots are ID samples of different classes, and the black $\times$ are OOD samples.}
    % After learning the latent domain features, X-DOM successfully distinguishes the features of OOD samples from ID samples.}
    \label{fig:tsne}
    \vspace{-0.5cm}
\end{figure}

\vspace{0.1cm}
\noindent\textbf{Comparison to SotA Methods.}
X-DOM is also compared with five very recent SotA methods, including MaxLogit \cite{HendrycksBMZKMS22}, KL-Matching \cite{HendrycksBMZKMS22}, ReAct \cite{sun2021react}, MaSF \cite{haroush2022a} and NMD \cite{dong2022neural} %\gs{please include references for each method}
, with their results reported at the top of Tabs.  \ref{tab:main_result_cifar10} and \ref{tab:main_result_cifar100}. Among all our four X-DOM methods and the SotA methods, ViM-DOM is consistently the best performer except the CIFAR10 data in Tab. \ref{tab:main_result_cifar100} where Energy-DOM is the best detector. This is mainly because the ViM is generally the best semantic-feature-based OOD scoring method, and X-DOM can perform better when the plug-in base model is stronger. Further, it is impressive that although the base models MSP, ODIN and Energy that largely underperform the SotA competing methods, X-DOM can significantly boost their performance and outperform these SotA competing methods on nearly all cases in Tabs. \ref{tab:main_result_cifar10} and \ref{tab:main_result_cifar100}. 
% To the best of our knowledge, they are the current competitive or SotA OOD detection methods.


\vspace{0.1cm}
\noindent\textbf{The Reasons behind the Effectiveness of X-DOM.} We aim to understand the effectiveness of X-DOM from two perspectives, including the domain and semantic OOD scoring, and the latent features learned in X-DOM, with the results on the Textures dataset reported in Figs. \ref{fig:domain_score} and \ref{fig:tsne} respectively. We can see in Fig. \ref{fig:domain_score} that the domain OOD scores in X-DOM enable a significantly better ID and OOD separation than the semantic OOD scores, indicating that the ID and OOD samples can be easier to be separated by looking from the domain features than the semantic features since there can be more background domain regions/pixels than the foreground semantic ones in each ID/OOD image. From the feature representation perspective, compared to the features learned in the vanilla $K$-class classifier in Fig. \ref{fig:tsne} (left), the features learned by the $(K+1)$-class classifier in X-DOM (Fig. \ref{fig:tsne} (right)) are more discriminative in distinguishing OOD samples from ID samples. This may be due to that training the classifier using fine-grained pixel-wise class labels helps learn better semantic features, in addition to the learning of the domain features.
% the domain features can be more discriminative and supportive than the semantic features for OOD detection.




% \textbf{ID: CIFAR10} The results are summarized in Tab. 1 when in-distribution dataset is CIFAR10.  the SotA method can obtain near-perfect performance on the relevant task. Overall, the ViM-DOM method outperforms the competing methods in almost all scenarios, establishing a new SotA standard.

% \textbf{ID: CIFAR100} All results for the in-distribution dataset as CIFAR100 are shown in Table1. X-DOM still improves the performance of the baseline method in most scenarios in harder environments, with an overall average boost of FPR95 by $11.78\%$ , AUROC by $4.8\%$ and AUPR by $5.41\%$. Compared to CIFAR10 as in-distribution, X-DOM significantly improves the performance of the SotA method ViM on the CIFAR100 task with boost average FPR95 by $7.91\%$ . Overall, Energy-DOM achieves optimal results on the CIFAR100 vs. CIFAR10 task, while ViM-DOM outperforms the competing methods on the remaining tasks. Although not based on the same benchmark method, X-DOM can still establish new SotA criteria in all scenarios.

\subsection{Ablation Study}\label{subsec:ablation}


% \begin{table}[tb]
%   \centering
%   \caption{Ablation study results of X-DOM and its variants. Best results are \textbf{highlighted}. \cb{Does this result look like the joint score performance of X-DOM is insufficient.}\gs{yes, please use a different base model here. perhaps ViM.}}
%   \vspace{-0.2cm}
%   \scalebox{0.75}{
%     \begin{tabular}{p{0.15cm}p{1.0cm}cccc}
%     \hline
%     \multicolumn{2}{l}{\textbf{Module}}   & \multicolumn{1}{c}{\textbf{MSP}} & \multicolumn{1}{c}{\textbf{SEM only}} &
%     \multicolumn{1}{c}{\textbf{DOM only}} &
%     \multicolumn{1}{c}{\textbf{MSP-DOM}}  \\ 
%     \hline
%     \multicolumn{2}{l}{OCLS} & \multicolumn{1}{c}{$\checkmark$}  &\multicolumn{1}{c}{} &\multicolumn{1}{c}{} &\multicolumn{1}{c}{}\\
    
%     \multicolumn{2}{l}{K+1CLS} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\checkmark$} & \multicolumn{1}{c}{$\checkmark$} & \multicolumn{1}{c}{$\checkmark$} \\ 
    
%     \multicolumn{2}{l}{SEM} & \multicolumn{1}{c}{$\checkmark$} & \multicolumn{1}{c}{$\checkmark$} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\checkmark$} \\
    
%     \multicolumn{2}{l}{DOM} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} &  \multicolumn{1}{c}{$\checkmark$} & \multicolumn{1}{c}{$\checkmark$} \\
%      \hline
%         \multicolumn{6}{c}{\textbf{FPR95}$\downarrow$ /\textbf{AUROC}$\uparrow$}\\ 
%     \hline
%     \multirow{5}[0]{*}{\rotatebox{90}{\textbf{CIFAR10}}} & \multicolumn{1}{l}{\textbf{CIFAR100}} & 44.33/89.01 & 28.12/92.97 & 38.16/91.42 & \textbf{23.75}/\textbf{94.29} \\
%     & \multicolumn{1}{l}{\textbf{SVHN}} &  17.40/95.72& 10.23/97.70 & \textbf{2.60}/\textbf{99.30} & 2.55/98.94 \\
%     & \multicolumn{1}{l}{\textbf{Places365}} & 22.47/92.93& 19.02/95.80& \textbf{4.40}/\textbf{98.99} & 5.05/98.49 \\
%     & \multicolumn{1}{l}{\textbf{Textures}} & 8.55/97.66& 4.47/98.71& \textbf{0.04}/\textbf{99.99} & 0.02/99.90 \\
%     & \multicolumn{1}{l}{\textbf{Average}} & 20.46/93.83& 15.46/96.29&  \textbf{11.30}/\textbf{97.43} & 7.84/97.90 \\
%     \hline
%     \multirow{5}[0]{*}{\rotatebox{90}{\textbf{CIFAR100}}} & \textbf{CIFAR10} & 64.25/81.52  & 59.21/\textbf{85.11} & 89.24/67.98 & \textbf{58.76}/84.67 \\
%     & \multicolumn{1}{l}{\textbf{SVHN}} & 49.50/88.92& 59.26/82.46 &  \textbf{26.61}/\textbf{94.54}  & 50.75/89.27\\
%     & \multicolumn{1}{l}{\textbf{Places365}} & 72.10/76.18 & 77.10/70.65 & \textbf{26.55}/\textbf{94.34} & 67.82/85.20 \\
%     & \multicolumn{1}{l}{\textbf{Textures}} & 46.24/89.33&  50.94/87.53 & \textbf{0.53}/\textbf{99.88}  & 28.21/95.86 \\
%     & \multicolumn{1}{l}{\textbf{Average}} & 58.02/83.99& 61.63/81.44 & \textbf{35.73}/\textbf{89.18}  & 51.38/88.75 \\
%     \hline
%     \end{tabular}%
%     }
%     \vspace{-0.2cm}
%   \label{tab:ablation}%
% \end{table}%

% \begin{table}[tb]
%   \centering
%   \caption{Ablation study results of X-DOM and its variants. Best results are \textbf{highlighted}. \cb{It seems that the advantages of the domain feature are not obvious}\gs{I think we need to drop the SEM only column, which is supposed to be part of DOM, right?}}
%   \vspace{-0.2cm}
%   \scalebox{0.75}{
%     \begin{tabular}{p{0.15cm}p{1.0cm}cccc}
%     \hline
%     \multicolumn{2}{l}{\textbf{Module}}   & \multicolumn{1}{c}{\textbf{ViM}} & \multicolumn{1}{c}{\textbf{SEM only}} &
%     \multicolumn{1}{c}{\textbf{DOM only}} &
%     \multicolumn{1}{c}{\textbf{ViM-DOM}}  \\ 
%     \hline
%     \multicolumn{2}{l}{OCLS} & \multicolumn{1}{c}{$\checkmark$}  &\multicolumn{1}{c}{} &\multicolumn{1}{c}{} &\multicolumn{1}{c}{}\\
    
%     \multicolumn{2}{l}{K+1CLS} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\checkmark$} & \multicolumn{1}{c}{$\checkmark$} & \multicolumn{1}{c}{$\checkmark$} \\ 
    
%     \multicolumn{2}{l}{SEM} & \multicolumn{1}{c}{$\checkmark$} & \multicolumn{1}{c}{$\checkmark$} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\checkmark$} \\
    
%     \multicolumn{2}{l}{DOM} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} &  \multicolumn{1}{c}{$\checkmark$} & \multicolumn{1}{c}{$\checkmark$} \\
%      \hline
%         \multicolumn{6}{c}{\textbf{FPR95}$\downarrow$ /\textbf{AUROC}$\uparrow$}\\ 
%     \hline
%     \multirow{5}[0]{*}{\rotatebox{90}{\textbf{CIFAR10}}} & \multicolumn{1}{l}{\textbf{CIFAR100}} & 15.25/96.92 & 14.07/96.83 & 38.16/91.42 & \textbf{13.49}/\textbf{97.08} \\
%     & \multicolumn{1}{l}{\textbf{SVHN}} &  1.27/99.47& 0.45/99.83 & 2.60/99.30 & \textbf{0.41}/\textbf{99.85} \\
%     & \multicolumn{1}{l}{\textbf{Places365}} & 2.74/99.32 & 0.78/99.84& 4.40/98.99 & \textbf{0.72}/\textbf{99.85} \\
%     & \multicolumn{1}{l}{\textbf{Textures}} & 0.11/99.93& \textbf{0.00}/\textbf{100.00}& 0.04/99.99 & \textbf{0.00}/\textbf{100.00} \\
%     & \multicolumn{1}{l}{\textbf{Average}} & 4.84/98.91& 3.83/99.13&  11.30/97.43 & \textbf{3.65}/\textbf{99.20} \\
%     \hline
%     \multirow{5}[0]{*}{\rotatebox{90}{\textbf{CIFAR100}}} & \textbf{CIFAR10} & 59.13/85.72  & \textbf{58.62}/\textbf{86.08} & 89.24/67.98 & 60.88/85.74 \\
%     & \multicolumn{1}{l}{\textbf{SVHN}} & 10.23/97.90& 8.39/98.18 &  26.61/94.54  & \textbf{7.58}/\textbf{98.40}\\
%     & \multicolumn{1}{l}{\textbf{Places365}} & 49.38/87.23 & 25.32/94.66 & 26.55/94.34 & \textbf{20.93}/\textbf{96.06} \\
%     & \multicolumn{1}{l}{\textbf{Textures}} & 2.45/99.47& 0.39/99.92 & 0.53/99.88  & \textbf{0.16}/\textbf{99.96} \\
%     & \multicolumn{1}{l}{\textbf{Average}} & 30.30/92.58& 23.18/94.71 & 35.73/89.18  & \textbf{22.39}/\textbf{95.04} \\
%     \hline
%     \end{tabular}%
%     }
%     \vspace{-0.2cm}
%   \label{tab:ablation}%
% \end{table}%

% \begin{table}[tb]
%   \centering
%   \caption{Ablation study results of X-DOM and its variants. Best results are \textbf{highlighted}. \cb{It seems that the advantages of the domain feature are not obvious}\gs{I think we need to drop the SEM only column, which is supposed to be part of DOM, right?}\cb{It looks like we are deliberately hiding the results of K+1CLS w\/ SEM, do we need to remove the OCLS and K+1CLS attributes.}}
%   \vspace{-0.2cm}
%   \scalebox{0.9}{
%     \begin{tabular}{p{0.15cm}p{1.0cm}cccc}
%     \hline
%     \multicolumn{2}{l}{\textbf{Module}}   & \multicolumn{1}{c}{\textbf{ViM}}&
%     \multicolumn{1}{c}{\textbf{DOM}} &
%     \multicolumn{1}{c}{\textbf{ViM-DOM}}  \\ 
%     \hline
%     \multicolumn{2}{l}{$S_h$} & \multicolumn{1}{c}{$\checkmark$}  &\multicolumn{1}{c}{} &\multicolumn{1}{c}{$\checkmark$}\\
    
%     % \multicolumn{2}{l}{K+1CLS} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\checkmark$} & \multicolumn{1}{c}{$\checkmark$} \\ 
    
%     \multicolumn{2}{l}{$S_d$} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\checkmark$} & \multicolumn{1}{c}{$\checkmark$} \\
    
%     % \multicolumn{2}{l}{DOM} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\checkmark$} & \multicolumn{1}{c}{$\checkmark$} \\
%      \hline
%         \multicolumn{6}{c}{\textbf{FPR95}$\downarrow$ /\textbf{AUROC}$\uparrow$}\\ 
%     \hline
%     \multirow{5}[0]{*}{\rotatebox{90}{\textbf{CIFAR10}}} & \multicolumn{1}{l}{\textbf{CIFAR100}} & 15.25/96.92 & 38.16/91.42 & \textbf{13.49}/\textbf{97.08} \\
%     & \multicolumn{1}{l}{\textbf{SVHN}} &  1.27/99.47& 2.60/99.30 & \textbf{0.41}/\textbf{99.85} \\
%     & \multicolumn{1}{l}{\textbf{Places365}} & 2.74/99.32 & 4.40/98.99 & \textbf{0.72}/\textbf{99.85} \\
%     & \multicolumn{1}{l}{\textbf{Textures}} & 0.11/99.93& 0.04/99.99 & \textbf{0.00}/\textbf{100.00} \\
%     & \multicolumn{1}{l}{\textbf{Average}} & 4.84/98.91& 11.30/97.43 & \textbf{3.65}/\textbf{99.20} \\
%     \hline
%     \multirow{5}[0]{*}{\rotatebox{90}{\textbf{CIFAR100}}} & \textbf{CIFAR10} & 59.13/85.72  & 89.24/67.98 & 60.88/85.74 \\
%     & \multicolumn{1}{l}{\textbf{SVHN}} & 10.23/97.90& 26.61/94.54  & \textbf{7.58}/\textbf{98.40}\\
%     & \multicolumn{1}{l}{\textbf{Places365}} & 49.38/87.23 & 26.55/94.34 & \textbf{20.93}/\textbf{96.06} \\
%     & \multicolumn{1}{l}{\textbf{Textures}} & 2.45/99.47& 0.53/99.88  & \textbf{0.16}/\textbf{99.96} \\
%     & \multicolumn{1}{l}{\textbf{Average}} & 30.30/92.58& 35.73/89.18  & \textbf{22.39}/\textbf{95.04} \\
%     \hline
%     \end{tabular}%
%     }
%     \vspace{-0.2cm}
%   \label{tab:ablation}%
% \end{table}%

\begin{table}[tb]
  \centering
  \caption{FPR95 Results of X-DOM and its variants.}
%   Best results are \textbf{boldfaced}.}
  \vspace{0.1cm}
  \scalebox{0.75}{
    \begin{tabular}{p{0.15cm}p{1.0cm}|c|cc|cc}
    \hline
    \multicolumn{2}{l}{\textbf{Module}}   & \multicolumn{1}{|c}{\textbf{DOM}}& 
    \multicolumn{1}{|c}{\textbf{Energy}} &
    \multicolumn{1}{c}{\textbf{Energy-DOM}} &
    \multicolumn{1}{|c}{\textbf{ViM}} &
    \multicolumn{1}{c}{\textbf{ViM-DOM}}  \\ 
    \hline
    \multicolumn{2}{l}{$S_h$} & \multicolumn{1}{|c}{} &\multicolumn{1}{|c}{$\checkmark$} &\multicolumn{1}{c}{$\checkmark$} &\multicolumn{1}{|c}{$\checkmark$} &\multicolumn{1}{c}{$\checkmark$}\\
    
    % \multicolumn{2}{l}{K+1CLS} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\checkmark$} & \multicolumn{1}{c}{$\checkmark$} \\ 
    
    \multicolumn{2}{l}{$S_d$} & \multicolumn{1}{|c}{$\checkmark$}& \multicolumn{1}{|c}{} & \multicolumn{1}{c}{$\checkmark$}  & \multicolumn{1}{|c}{} & \multicolumn{1}{c}{$\checkmark$} \\
    
    % \multicolumn{2}{l}{DOM} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\checkmark$} & \multicolumn{1}{c}{$\checkmark$} \\
     \hline
     %   \multicolumn{6}{c}{\textbf{FPR95}$\downarrow$ /\textbf{AUROC}$\uparrow$}\\ 
    \multirow{5}[0]{*}{\rotatebox{90}{\textbf{CIFAR10}}} & \multicolumn{1}{l|}{\textbf{CIFAR100}}& 38.16 & 41.98 & 19.90 & 15.25 & \textbf{13.49} \\
    & \multicolumn{1}{l|}{\textbf{SVHN}} &  2.60 & 19.73 & 3.10 & 1.27&  \textbf{0.41} \\
    & \multicolumn{1}{l|}{\textbf{Places365}} &4.40 & 25.42 & 6.96 &  2.74 & \textbf{0.72} \\
    & \multicolumn{1}{l|}{\textbf{Textures}} &0.04 & 8.72 & 0.53 &  0.11& \textbf{0.00} \\
    & \multicolumn{1}{l|}{\textbf{Average}} &11.30 & 23.96 & 7.62 &  4.84& \textbf{3.65} \\
    \hline
    \multirow{5}[0]{*}{\rotatebox{90}{\textbf{CIFAR100}}} & \textbf{CIFAR10} & 89.24 & 64.34 & \textbf{54.02} & 59.13  & 60.88 \\
    & \multicolumn{1}{l|}{\textbf{SVHN}} & 26.61 & 36.76 & 24.7  & 10.23&  \textbf{7.58}\\
    & \multicolumn{1}{l|}{\textbf{Places365}} & 26.55 & 74.75 & 48.87 & 49.38 & \textbf{20.93} \\
    & \multicolumn{1}{l|}{\textbf{Textures}} & 0.53  & 39.17 & 7.11 & 2.45 & \textbf{0.16} \\
    & \multicolumn{1}{l|}{\textbf{Average}} & 35.73  & 53.75 & 33.70 & 30.30 &  \textbf{22.39} \\
    \hline
    \end{tabular}%
    }
    \vspace{-0.3cm}
  \label{tab:ablation}%
\end{table}%


\begin{figure}[t] 
    \centering
    \includegraphics[width=0.95\linewidth]{Figure/Ablation_Temperature.pdf}
    \vspace{-0.2cm}
    \caption{AUROC results of X-DOM using varying $\mathit{T}$ settings.}
    % The dashed lines show the baseline results of the corresponding OOD scoring function.}
    \label{fig:T}
    \vspace{-0.5cm}
\end{figure}



\noindent\textbf{Domain OOD Score $S_d$ and the Joint OOD Score $S$.} Tab. \ref{tab:ablation} shows the FPR95 results of OOD scoring methods in our model, including the use of domain OOD scores $S_d$ only (DOM), semantic OOD scores $S_h$ (Energy and ViM are used), and the full X-DOM model (See \texttt{Appendix C.1} for more detailed results).
% is the baseline result using a pretrained classification network.
% \textbf{SEM only} is a variant of applying MSP scoring to the semantic logits in the K+1 classification network of X-DOM. SEM-only significantly outperforms the MSP baseline in semantic feature-dominated benchmarks (CIFAR10 vs. CIFAR100 and CIFAR100 vs. CIFAR10 ). This result may be due to the separation of semantic and domain features during the training of the dense prediction network, which reduces the noise in the learned semantic features.
% \textbf{DOM only} is a variant of using $S_d$ of the K+1 classification network of X-DOM only.
Compared to the two semantic OOD scoring methods, Energy and ViM, using only the domain OOD scoring $S_d$ in X-DOM can obtain significantly reduced FPR95 errors, especially 
% baseline methods, DOM-only is superior in
on OOD benchmarks such as Places356 and Textures where significant domain differences are presented compared to the in-distribution domain. This demonstrates that X-DOM can effectively learn the in-distribution domain features that can be used to detect OOD samples from the background domain aspect. Nevertheless, DOM works less effectively on the benchmark CIFAR100 vs. CIFAR10 where the domain difference is weak and detecting OOD samples rely more on the semantic features. 
% This result shows that domain scores also have limitations and could not be directly applied in real-world scenarios.
In such cases, the full X-DOM models -- Energy-DOM and ViM-DOM -- that synthesize semantic OOD scores $S_h$ and domain OOD scores $S_d$ are needed; they significantly outperform the separate domain/semantic OOD scoring methods across the datasets.
% . With the inclusion of $S_d$, the joint scores significantly outperformed the baseline methods using only $S_h$ on all benchmarks. The detailed OOD score ablation can be found in appendix. \cb{Does this paragraph repeat what was in section 4.2?}

%MSP-DOM significantly outperforms baseline MSP on all benchmarks but is inferior to the more tailored SEM-only or DOM-only on individual benchmarks. The joint score generally achieves globally suboptimal results, showing robustness to different scenarios.
%We separate the semantic and domain scores in X-DOM into variants SEM-only and DOM-only. Compared to the baseline method MSP, X-SEM uses identical architecture and method to derive semantic OOD scores, but X-SEM is based on a dense retraining network consistent with X-DOM, rather than the default classification network.

\vspace{0.1cm}
\noindent\textbf{Temperature $T$ in Synthesizing Domain and Semantic OOD Scores.} One key challenge in plugging existing semantic OOD scores into X-DOM in Eq. (\ref{eqn:combine}) is the diverse range of different semantic OOD scores yielded by the existing methods. Fig. \ref{fig:T} the variants of X-DOM 
% shows the correlation between the OOD detection performance of the X-DOM and the
of using different temperature $T$ values to study the effects (see \texttt{Appendix C.2} for the results on the other datasets). We can observe that the performance of all methods in CIFAR100 vs. CIFAR10 gradually improves as the temperature increases. This is because the increase of $T$ narrows down the distribution of domain scores, thus making the final OOD scores emphasizing more on the semantic OOD scores, which are more effective in the OOD datasets like CIFAR100 (ID) vs. CIFAR10 (OOD) where the domain difference is very small. In contrast, the performance of all methods in CIFAR100 (ID) vs. Places365 (OOD) gradually decreases as the temperature increases. This is because the domain distribution difference dominates over the semantic difference in such cases, on which enlarging the distribution of domain OOD  scores is more effective. $T=2.5$ is generally a good trade-off of the domain and semantic OOD scores, and it is thus used by default in X-DOM.
% In practice, it is possible to detect domain distribution gap OOD samples by lower temperature T and semantic distribution gap OOD samples by higher T. 
Note that adjusting $T$ generally does not bring the overall performance down below the baseline performance, showing the effectiveness of X-DOM using different $T$ values.


% \begin{table}[bt]
%   \centering
%   \caption{Comparison of X-DOM and outlier exposure (OE). $^{\dagger}$ indicates that the results are taken from the original paper, and other methods share the same architecture. Reported results are averaged over the results on the four OOD datasets.}
%   %\gs{please add the results of MSP without using outlier data as the baseline results.}}
% %   Best results are highlighted.}
%   \vspace{0.1cm}
%   \scalebox{0.88}{
%     \begin{tabular}{p{2.2cm}|cc|cc}
%     \hline
%     \multirow{2}{*}{\textbf{Methods}}  &  \multicolumn{2}{c|}{\textbf{ID: CIFAR10}} & \multicolumn{2}{c}{\textbf{ID: CIFAR100}}\\
%     \cline{2-5} 
%     & \multicolumn{1}{c}{\textbf{FPR95}$\downarrow$} & \multicolumn{1}{c|}{\textbf{AUROC}$\uparrow$} & 
%     \multicolumn{1}{c}{\textbf{FPR95}$\downarrow$} & 
%     \multicolumn{1}{c}{\textbf{AUROC}$\uparrow$}\\
%     \hline
%     Baseline & 33.44 & 89.01 & 64.25 & 81.52 \\
%     %  MSP \cite{hendrycks17baseline} & 20.46 & 93.83 & 58.02 & 83.99  \\
%      OE \cite{hendrycks2018deep} & 20.16 & 93.74 & 57.68 & 83.98  \\
%      OE$^{\dagger}$ \cite{hendrycks2018deep} & 15.57 & 96.40 & 52.30 & 83.47  \\
%      X-DOM & \textbf{7.87} & \textbf{98.04} & \textbf{42.29} & \textbf{91.03}  \\
%      \hline
%     \end{tabular}%
%     }
%   \label{tab:OE}%
%   \vspace{-0.2cm}
% \end{table}%

\subsection{Further Analysis of X-DOM}

\vspace{0.1cm}
% \noindent\textbf{X-DOM vs. Outlier Exposure.} Domain features can also be alternatively learned by using the popular
% % In Tab. \ref{tab:OE}, we compare the X-DOM using existing inlier data and 
% outlier exposure (OE) method \cite{hendrycks2018deep} that uses external outlier data to support the learning. The comparison of X-DOM and OE is presented in Tab. \ref{tab:OE}, in which 
% % We provide two OE results: one is the result reported in the original paper, using
% OE$^{\dagger}$ uses the large-scale 80M Tiny Image \cite{torralba200880} as the outlier dataset, OE is trained using Tiny ImageNet \cite{le2015tiny} as the outlier data,
% % , and the other is the OE result we reproduced using the same architecture, using Tiny ImageNet \cite{le2015tiny} as the outlier dataset. All
% OE, OE$^{\dagger}$ and X-DOM are all based on the MSP-based OOD scoring function, and Baseline is the original MSP without using outlier data. The results show that the two OE methods can also significantly outperform the Baseline model, but their performance is heavily dependent on the outlier data, \eg, the results of OE and OE$^{\dagger}$ differ significantly from each other.
% % superior on different benchmarks, indicating that the outlier dataset could influence the OE results. 
% By contrast, X-DOM does not need outlier data and significantly outperforms both OE methods on both benchmarks.
% % , demonstrating X-DOM's high efficiency for existing data.

% % \noindent\textbf{Visualization of Features Learned.} Fig. \ref{fig:tsne} shows the feature space visualization of the vanilla classification network and X-DOM's $K+1$ classes classification network. From the figure, the OOD samples are closer to the ID samples in the feature space of the vanilla classification network, and even part of the OOD samples are mixed up with the ID samples. It indicates that it is difficult to distinguish all OOD samples from the feature perspective by pure semantic features, which restricts the upper limit of OOD scores. The features of X-DOM are richer due to the learning of potential domain features. In the feature space of X-DOM, OOD samples and ID samples are significantly separated while not affecting the ID samples' distribution. The visualization features prove that X-DOM learns better feature representations, further giving strong evidence to the results of X-DOM.
\begin{table}[tb]
  \centering
  \caption{Top-1 accuracy results of in-distribution classification. Vanilla is the primitive trained classification network $\phi$ in Sec. \ref{subsec:domainfeature}.}
  \vspace{0.1cm}
  \scalebox{0.86}{
    \begin{tabular}{|p{1.0cm}cc|}
    \hline
    \multicolumn{1}{|l}{\textbf{Method}}   & \multicolumn{1}{|c}{\textbf{CIFAR10}} & \multicolumn{1}{c|}{\textbf{CIFAR100}} \\ 
    \hline
    \multicolumn{1}{|l}{Vanilla} & \multicolumn{1}{|c}{97.25\%}  &\multicolumn{1}{c|}{85.94\%} \\
    
    \multicolumn{1}{|l}{X-DOM} & \multicolumn{1}{|c}{97.13\%} & \multicolumn{1}{c|}{86.17\%} \\ 
     \hline
    \end{tabular}%
    }
    \vspace{-0.4cm}
  \label{tab:accuracy}%
\end{table}%
\vspace{0.1cm}
\noindent\textbf{In-distribution Classification Accuracy.} A potential risk of modifying the primitive classification network for
% retrained/fine-tuned OOD methods
OOD detection is the large degradation of the in-distribution classification accuracy. 
% To this end, we compare the in-distribution classification performance of X-DOM and pre-trained classification networks
As shown in Tab. \ref{tab:accuracy}, our proposed X-DOM does not have this issue, as X-DOM has only 0.12\% top-1 accuracy drop on the CIFAR10 dataset and improves the classification performance by 0.23\% on the CIFAR100 dataset. This result indicates that the dense prediction training in X-DOM ensures effective learning of semantic features, while learning the domain features. The 0.23\% accuracy increase on CIFAR100 also indicates that the dense prediction task can also improve the semantic feature learning for in-distribution classification.
% Dense prediction training on more challenging datasets like CIFAR100 can help the network learn more accurate information by separating semantic and domain features.




\begin{figure}[t] 
    \centering
    \includegraphics[width=0.90\linewidth]{Figure/Large_scale_exp.pdf}
    \vspace{-0.3cm}
    \caption{AUROC results of X-DOM and the Energy baseline in large-scale semantic space using ImageNet-1k as ID data.}
    % The dashed lines show the baseline results of the corresponding OOD scoring function.}
    \label{fig:imagenet}
    \vspace{-0.5cm}
\end{figure}

\vspace{0.1cm}
\noindent\textbf{Extending to Large-scale Semantic Space.}
A further challenge for OOD detection is on datasets with a large number of ID classes and high-resolution images, \eg, ImageNet-1k \cite{deng2009imagenet}.
% , which with higher resolution and more category labels (versus CIFAR). 
Fig. \ref{fig:imagenet} presents the detection performance of X-DOM using ImageNet-1k as in-distribution dataset and on four OOD datasets, including two new high resolution datasets, ImageNet-O \cite{hendrycks2021natural} and SUN \cite{xiao2010sun}. To examine the impact of the number of classes, we show the results using $\mathit{C}\in \{200, 300, 500, 1000\}$ randomly selected ID classes from ImageNet-1k ($C=1000$ is the full ImageNet-1k data; see \texttt{Appendix B.2} for more details). 
% For each $\mathit{C}$, we randomly sampled the corresponding number of classes in ImageNet-1k as the training set. 
% Refer to Appendix A for details of the experiments.
The results show that X-DOM can consistently and significantly outperform its base model Energy with increasing number of ID classes on four diverse OOD datasets, indicating the effectiveness of X-DOM working in large-scale semantic space. On the other hand, as expected, both Energy and X-DOM are challlenged by the large semantic space, and thus, their performance decreases with more ID classes. Extending to large-scale semantic space is a general challenge for existing OOD detectors. We leave it for future work.
% although the X-DOM performance decreases as the number of in-distribution categories rises, it always maintains a stable boost to baseline method. It shows that expanding the in-distribution semantic space does not cause X-DOM to fail, which sufficiently proves the robustness of X-DOM.



\section{Conclusions}
In this paper, we reveal the importance of the background domain features for OOD detection that are neglected in current approaches. We further propose a novel OOD detection framework X-DOM that utilizes dense prediction networks to learn the in-distribution domain features from in-distribution training data. It then leverages these domain features to define domain OOD scores and seamlessly combines them with existing semantic-feature-based OOD methods to detect OOD samples from both domain and semantic aspects. Comprehensive results on popular OOD benchmarks with diverse background features show that X-DOM can significantly improve the detection performance of four different existing methods.
% and establish new SotA results on these benchmarks. 
% We hope our research will draw attention to 
Through this work, we promote the design of OOD detection algorithms focusing features beyond the semantic features to achieve more holistic OOD detection 
% and establishing a foundation for deployment
in real-world applications.

% \textbf{Limitations.}
% Although X-DOM achieves SotA results on these benchmarks, its computation cost and requirements are higher due to the use of semantic segmentation.
% % and more difficult training of dense prediction networks when extending 
% We are working on solutions to address these issues and extending X-DOM to datasets with thousands of in-distribution classes \cite{huang2021mos, wang2022vim}. 
% Extracting and exploiting potential domain features in large-scale datasets would be a new major challenge.

\newpage

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{Figure/ood_example.pdf}
    \caption{Example images of the five datasets we used. The rows from the top to the bottom are images from CIFAR10 \cite{krizhevsky2009learning}, CIFAR100 \cite{krizhevsky2009learning}, SVHN \cite{netzer2011reading}, Places365 \cite{zhou2017places}, and Textures \cite{cimpoi2014describing}, respectively.
    }% shows how to extract domain information from a K classification network and train a dense prediction network.} %\gs{indicate in the figure that the upper right one is semantic OOD scoring in existing methods; K classes Classification Network -> K-class Classification Network; K+1 classes Classification Network -> (K+1)-class Classification Network}}
    \label{fig:ood}
    \vspace{-0.0cm}
\end{figure*}
%\maketitle
\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.\textwidth]{Figure/mask_vis.pdf}
    \caption{Examples of ID samples, CAMs, and pseudo-masks for CIFAR10. For each group of examples (three rows per group), the images on the top are original image, the middle is its class activate mapping visualisation, and the bottom ones are its pseudo-mask.
    }
    \label{fig:mask}
    \vspace{-0.5cm}
\end{figure*}
\begin{table*}[bt]
  \centering
  \caption{\textbf{Ablation results.} `DOM' is our method that uses only the domain OOD score, while `Vanilla \textit{X}' means the use of original semantic OOD scoring function in the method \textit{X}. Best results in each group are \textbf{highlighted}.}
  \vspace{-0.0cm}
  \scalebox{0.734}{
    \begin{tabular}{p{2.6cm}|ccccc|ccccc}
    \hline
    \multirow{3}{*}{\textbf{Methods}} & \multicolumn{5}{c|}{\textbf{In:CIFAR10}} & \multicolumn{5}{c}{\textbf{In:CIFAR100}}\\
    \cline{2-11}
    & \multicolumn{1}{c}{\textbf{CIFAR100}} & \multicolumn{1}{c}{\textbf{SVHN}} & \multicolumn{1}{c}{\textbf{Places365}} & \multicolumn{1}{c}{\textbf{Textures}} & \multicolumn{1}{c|}{\textbf{Average}} & \multicolumn{1}{c}{\textbf{CIFAR10}} & \multicolumn{1}{c}{\textbf{SVHN}} & \multicolumn{1}{c}{\textbf{Places365}} & \multicolumn{1}{c}{\textbf{Textures}} & \multicolumn{1}{c}{\textbf{Average}}  \\
    & \multicolumn{5}{c|}{\textbf{FPR95}$\downarrow$ /\textbf{AUROC}$\uparrow$}& \multicolumn{5}{c}{\textbf{FPR95}$\downarrow$ /\textbf{AUROC}$\uparrow$} \\
    \hline
    DOM & 38.16/91.42 & 2.60/99.30 & 4.40/98.99 & 0.04/99.99 & 11.30/97.43 & 89.24/67.98 & 26.61/94.54 & 26.55/94.34 & 0.53/99.88 & 35.73/89.18 \\
    \hline
    Vanilla MSP \cite{hendrycks17baseline} & 33.44/89.01 & 17.40/95.72 & 22.47/92.93 & 8.55/97.66 & 20.46/93.83 & 64.25/81.52 & \textbf{49.50}/88.92 & 72.10/76.18 & 46.24/89.33 & 58.02/83.99 \\
    %MSP-SEM & 28.12/92.97 & 10.23/97.70 & 19.02/95.80 & 4.47/98.71 & 15.46/96.29 & 59.21/\textbf{85.11} & 59.26/82.46 & 77.10/70.65 & 50.94/87.53 & 61.63/81.44 \\
    MSP-\method & \textbf{23.75}/\textbf{94.29} & \textbf{2.55}/\textbf{98.94} & \textbf{5.05}/\textbf{98.49} & \textbf{0.02}/\textbf{99.90} & \textbf{7.84}/\textbf{97.90} & \textbf{58.76}/\textbf{84.67} & 50.75/\textbf{89.27} & \textbf{67.82}/\textbf{85.20} & \textbf{28.21}/\textbf{95.86} & \textbf{51.38}/\textbf{88.75} \\
    \hdashline
    Vanilla ODIN \cite{liang2018enhancing} & 34.62/87.83 & 16.13/95.66 & 22.15/92.43 & 7.45/97.86 & 20.09/93.45 & 59.67/82.39 & 38.11/\textbf{91.32} & 69.80/75.39 & 37.38/91.10 & 51.24/85.05 \\
    %ODIN-SEM & 25.53/93.15 & 7.94/98.15 & 16.09/96.31 & 3.21/99.08 & 13.19/96.67 & \textbf{52.42}/87.29 & 48.73/85.96 & 73.54/71.70 & 40.50/90.18 & 53.80/83.78 \\
    ODIN-\method & \textbf{22.15}/\textbf{95.50} & \textbf{4.27}/\textbf{99.19} & \textbf{8.08}/\textbf{98.66} & \textbf{0.34}/\textbf{99.92} & \textbf{8.71}/\textbf{98.32} & \textbf{55.92}/\textbf{87.31} & \textbf{32.79}/90.60 & \textbf{55.34}/\textbf{81.56} & \textbf{10.78}/\textbf{97.40} & \textbf{38.71}/\textbf{89.22} \\
    \hdashline
    Vanilla Energy \cite{liu2020energy} & 41.98/84.25 & 19.73/94.46 & 25.42/90.74 & 8.72/97.45 & 23.96/91.73 & 64.34/80.48 & 36.76/91.38 & 74.75/72.14 & 39.17/90.37 & 53.75/83.59 \\
    %Energy-SEM & 26.86/91.79 & 8.26/97.84 & 18.08/95.14 & 4.59/98.53 & 14.45/95.82 & \textbf{49.92}/87.67 & 39.85/87.79 & 70.60/71.85 & 33.26/91.08 & 48.41/84.60 \\
    Energy-\method & \textbf{19.90}/\textbf{94.98} & \textbf{3.10}/\textbf{99.28} & \textbf{6.96}/\textbf{98.60} & \textbf{0.53}/\textbf{99.87} & \textbf{7.62}/\textbf{98.19} & \textbf{54.02}/\textbf{88.12} & \textbf{24.78}/\textbf{93.39} & \textbf{48.87}/\textbf{85.72} & \textbf{7.11}/\textbf{98.41} & \textbf{33.70}/\textbf{91.41} \\
    \hdashline
    Vanilla ViM \cite{wang2022vim} & 15.25/96.92 & 1.27/99.47 & 2.74/99.32 & 0.11/99.93 & 4.84/98.91 & \textbf{59.13}/85.72 & 10.23/97.90 & 49.38/87.23 & 2.45/99.47 & 30.30/92.58 \\
    %ViM-SEM & 14.07/96.83 & 0.45/99.83 & 0.78/99.84 & \textbf{0.00}/\textbf{100.00} & 3.83/99.13 & \textbf{58.62}/\textbf{86.08} & 8.39/98.18 & 25.32/94.66 & 0.39/99.92 & 23.18/94.71 \\
    ViM-\method & \textbf{13.49}/\textbf{97.08} & \textbf{0.41}/\textbf{99.85} & \textbf{0.72}/\textbf{99.85} & \textbf{0.00}/\textbf{100.00} & \textbf{3.65}/\textbf{99.20} & 60.88/\textbf{85.74} & \textbf{7.58}/\textbf{98.40} & \textbf{20.93}/\textbf{96.06} & \textbf{0.16}/\textbf{99.96} & \textbf{22.39}/\textbf{95.04}  \\
    \hline
    \end{tabular}%
  }
  \label{tab:ablation}%
  \vspace{-0.3cm}
\end{table*}%

\appendix



%\gs{please plan the structure of the appendix, and add the specific appendix section number to the main text wherever the appendix is referred, \ie, putting a specific appendix section number in the main text, rather than simply stating `Appendix'.}
\section{Datasets and Weak Supervision Labels}

\subsection{Dataset Details}

\textbf{CIFAR10/CIFAR100 \cite{krizhevsky2009learning}} are two subsets sampled from Tiny Image \cite{torralba200880}, respectively. Both of them consist of 60,000 32x32 images. CIFAR10 are labelled with 10 mutually exclusive classes, while CIFAR100 contains 100 classes grouped into 20 super-classes. Since they are both subsets of Tiny Image, we consider them as more challenging near-OOD detection problems when they are used as OOD data for each other.

\textbf{SVHN \cite{netzer2011reading}} is a digit classification dataset cropped from house number plate pictures. It includes 600,000 32 x 32 images of printed digits (from 0 to 9). SVHN contains strong semantic OOD features and strong domain OOD features owing to the significant semantic differences and scenario differences between SVHN and CIFAR10/100.

\textbf{Places365 \cite{zhou2017places}} is a large-scale scene classification dataset. It has 10 million images comprising 434 scene classes. Similar to SVHN, Places365 also contains strong semantic OOD features and domain OOD features against CIFAR10/CIFAR100. Following \cite{huang2021mos}, we use a 10,000 images subset of Places365 as OOD data.

\textbf{Textures \cite{cimpoi2014describing}} contains 5,640 texture images in the wild. Since texture images do not contain specific objects and backgrounds, we consider Textures has a significant difference in both semantic and domain distributions against CIFAR10/CIFAR100.

To provide intuitive understanding of the semantic and domain difference between ID and OOD datasets, we present 10 example images for each dataset in Fig. \ref{fig:ood}.

\subsection{Examples of the Weak Supervision (Pseudo-masks) for ID Samples}
Fig. \ref{fig:mask} shows the class activation mappings generated utilizing the pre-trained $K$-class classification network, and the pseudo-masks then are used to train the dense prediction network. As can be seen in the figure, the class activation mapping can generally well localize the semantic information in the image, \ie, the foreground objects. Although, the pseudo-masks generated by the class activation mapping cannot segment the complex contours perfectly, 
% but 
they can only segment the foreground and background with a fairly good quality, which can provide sufficient supervision for supporting the learning of the background domain features, as shown by the results in the main text.




% \begin{table*}[bt]
%   \centering
%   \caption{AUROC of X-DOM in various temperature. All methods are joint with X-DOM by default.}
%   \vspace{-0.2cm}
%   \scalebox{0.8}{
%     \begin{tabular}{|p{0.5cm}|cccc|cccc|cccc|cccc|}
%     \hline
%     \multirow{2}{*}{$T$} & \multicolumn{4}{c|}{\textbf{CIFAR10 vs. CIFAR100}} & \multicolumn{4}{c|}{\textbf{CIFAR10 vs. SVHN}}& \multicolumn{4}{c|}{\textbf{CIFAR10 vs. Places365}}& \multicolumn{4}{c|}{\textbf{CIFAR10 vs. Textures}}\\
%     \cline{2-17}
%     & \multicolumn{1}{c}{\textbf{MSP}} & \multicolumn{1}{c}{\textbf{ODIN}} & \multicolumn{1}{c}{\textbf{Energy}} & \multicolumn{1}{c|}{\textbf{ViM}} & \multicolumn{1}{c}{\textbf{MSP}} & \multicolumn{1}{c}{\textbf{ODIN}} & \multicolumn{1}{c}{\textbf{Energy}} & \multicolumn{1}{c|}{\textbf{ViM}} & \multicolumn{1}{c}{\textbf{MSP}} & \multicolumn{1}{c}{\textbf{ODIN}} & \multicolumn{1}{c}{\textbf{Energy}} & \multicolumn{1}{c|}{\textbf{ViM}} &\multicolumn{1}{c}{\textbf{MSP}} & \multicolumn{1}{c}{\textbf{ODIN}} & \multicolumn{1}{c}{\textbf{Energy}} & \multicolumn{1}{c|}{\textbf{ViM}}\\
%     \hline
%     1.0 & 94.05&	95.48&	95.89&	97.06&99.25&	99.45&	99.60&	99.86&98.88&	99.09&	99.28&	99.84&99.98&	99.98&	99.98&	100.00\\
%     1.5 & 94.17&	95.57&	95.67&	97.10&99.11&	99.35&	99.51&	99.86&98.73&	98.93&	99.08&	99.85&99.96&	99.96&	99.95&	100.00\\
%     2.0 & 94.24&	95.56&	95.32&	97.10&99.01&	99.26&	99.39&	99.86&98.59&	98.79&	98.85&	99.85&99.93&	99.94&	99.92&	100.00\\
%     2.5 & 94.29&	95.50&	94.98&	97.08&98.94&	99.19&	99.28&	99.85&98.49&	98.66&	98.60&	99.85&99.90&	99.92&	99.87&	100.00\\
%     3.0 & 94.32&	95.42&	94.68&	97.06&98.88&	99.13&	99.18&	99.85&98.40&	98.54&	98.37&	99.85&99.87&	99.89&	99.83&	100.00\\
%     3.5 & 94.35&	95.33&	94.42&	97.05&98.84&	99.08&	99.08&	99.85&98.33&	98.43&	98.16&	99.85&99.83&	99.87&	99.78&	100.00\\
%     4.0 & 94.38&	95.24&	94.20&	97.03&98.80&	99.03&	99.00&	99.85&98.28&	98.34&	97.96&	99.85&99.80&	99.84&	99.73&	100.00\\
%     4.5 & 94.40&	95.15&	94.02&	97.01&98.77&	98.99&	98.93&	99.85&98.22&	98.25&	97.79&	99.85&99.77&	99.82&	99.68&	100.00\\
%     5.0 & 94.42&	95.06&	93.85&	97.00&98.74&	98.95&	98.86&	99.84&98.18&	98.17&	97.63&	99.85&99.74&	99.80&	99.63&	100.00\\
%     \hline
%     \multirow{2}{*}{$T$} & \multicolumn{4}{c|}{\textbf{CIFAR100 vs. CIFAR10}} & \multicolumn{4}{c|}{\textbf{CIFAR100 vs. SVHN}}& \multicolumn{4}{c|}{\textbf{CIFAR100 vs. Places365}}& \multicolumn{4}{c|}{\textbf{CIFAR100 vs. Textures}}\\
%     \cline{2-17}
%     & \multicolumn{1}{c}{\textbf{MSP}} & \multicolumn{1}{c}{\textbf{ODIN}} & \multicolumn{1}{c}{\textbf{Energy}} & \multicolumn{1}{c|}{\textbf{ViM}} & \multicolumn{1}{c}{\textbf{MSP}} & \multicolumn{1}{c}{\textbf{ODIN}} & \multicolumn{1}{c}{\textbf{Energy}} & \multicolumn{1}{c|}{\textbf{ViM}} & \multicolumn{1}{c}{\textbf{MSP}} & \multicolumn{1}{c}{\textbf{ODIN}} & \multicolumn{1}{c}{\textbf{Energy}} & \multicolumn{1}{c|}{\textbf{ViM}} &\multicolumn{1}{c}{\textbf{MSP}} & \multicolumn{1}{c}{\textbf{ODIN}} & \multicolumn{1}{c}{\textbf{Energy}} & \multicolumn{1}{c|}{\textbf{ViM}}\\
%     \hline
%     1.0 & 83.40& 86.21&	85.37&	84.09& 92.11&	93.44&	95.44&	98.33& 89.87&	89.16&	92.50&	96.63& 98.74&	99.28&	99.62&	99.97\\
%     1.5 & 84.11&	86.91&	87.08&	85.09&90.82&	92.19&	94.70&	98.40&87.76&	85.74&	89.96&	96.43&97.65&	98.64&	99.27&	99.97\\
%     2.0 & 84.46&	87.18&	87.80&	85.52&89.93&	91.28&	94.00&	98.40&86.29&	83.31&	87.64&	96.23&96.67&	97.99&	98.84&	99.96\\
%     2.5 & 84.67&	87.31&	88.12&	85.74&89.27&	90.60&	93.39&	98.40&85.20&	81.56&	85.72&	96.06&95.86&	97.40&	98.41&	99.96\\
%     3.0 & 84.80&	87.37&	88.28&	85.87&88.76&	90.07&	92.88&	98.38&84.33&	80.26&	84.16&	95.91&95.20&	96.88&	97.99&	99.95\\
%     3.5 & 84.90&	87.40&	88.35&	85.94&88.34&	89.65&	92.44&	98.37&83.63&	79.26&	82.89&	95.80&94.66&	96.43&	97.59&	99.95\\
%     4.0 & 84.97&	87.42&	88.38&	85.99&87.99&	89.31&	92.08&	98.36&83.03&	78.47&	81.85&	95.70&94.20&	96.04&	97.23&	99.95\\
%     4.5 & 85.02&	87.43&	88.39&	86.03&87.69&	89.03&	91.76&	98.34&82.51&	77.82&	80.99&	95.61&93.81&	95.69&	96.89&	99.95\\
%     5.0 & 85.06&	87.43&	88.38&	86.05&87.44&	88.79&	91.48&	98.33&82.06&	77.29&	80.25&	95.54&93.47&	95.38&	96.59&	99.95\\
%     \hline
%     \end{tabular}%
%   }
%   \label{tab:temp}%
%   \vspace{-0.3cm}
% \end{table*}%

% \begin{table}[tb]
%   \centering
%   \caption{Definition of different variants of X-DOM. Vanilla is a baseline based on the existing OOD score and primitive trained K-class classification network. X-SEM is the semantic score in X-DOM, and DOM is the domain score in X-DOM.}
%   \vspace{-0.2cm}
%   \scalebox{0.9}{
%     \begin{tabular}{p{0.15cm}p{1.0cm}cccc}
%     \hline
%     \multicolumn{2}{l}{\textbf{Module}}   & \multicolumn{1}{c}{\textbf{Vanilla}}&
%     \multicolumn{1}{c}{\textbf{X-SEM}} & \multicolumn{1}{c}{\textbf{DOM}} &
%     \multicolumn{1}{c}{\textbf{X-DOM}}  \\ 
%     \hline
%     \multicolumn{2}{l}{KCLS} & \multicolumn{1}{c}{$\checkmark$} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}  & \multicolumn{1}{c}{}\\ 
%     \multicolumn{2}{l}{K+1CLS} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\checkmark$} & \multicolumn{1}{c}{$\checkmark$}& \multicolumn{1}{c}{$\checkmark$} \\ 
%     \multicolumn{2}{l}{$S_h$} & \multicolumn{1}{c}{$\checkmark$}  &\multicolumn{1}{c}{$\checkmark$} &\multicolumn{1}{c}{}&\multicolumn{1}{c}{$\checkmark$}\\
%     \multicolumn{2}{l}{$S_d$} & \multicolumn{1}{c}{}& \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\checkmark$} & \multicolumn{1}{c}{$\checkmark$} \\
%     % \multicolumn{2}{l}{DOM} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\checkmark$} & \multicolumn{1}{c}{$\checkmark$} \\
%      \hline
%     \end{tabular}%
%     }
%     \vspace{-0.2cm}
%   \label{tab:ablation_sub}%
% \end{table}%



\section{Implementation Details}
We establish all experiments based on the Google BiT-M \cite{kolesnikov2020big} model. This model is a variant of ResNetv2 architecture \cite{he2016identity} and is pre-trained on ImageNet-21K. We use the official release checkpoint of BiT-M-R50x1 as our pre-training parameters.
\subsection{Main Results}\label{sec:imple}

\noindent\textbf{In-distribution Classification Pretrain Details} We follow the BiTHyperRule \cite{kolesnikov2020big} setting to train the baseline classification network on the in-distribution dataset (CIFAR10/CIFAR100) with pre-trained weights. The classification network was trained using 20k steps with a batch size of 128. SGD is used as parameter optimization with an initial learning rate of 0.003 and a momentum of 0.9. We used the STEP learning rate decay strategy, which decays the learning rate by a factor of 10 at 30\%, 60\%, and 90\% of the training steps. Moreover, we used a learning rate warm-up in the first 500 steps of training. All images were resized to 160x160 and randomly cropped to 128x128. Finally, we used MixUp \cite{zhang2018mixup} with $\alpha = 0.1$ to combine image samples in training.

The \textit{post hoc} out-of-distribution detection comparison methods in the experiments are based on the classification models trained from this step.

\noindent\textbf{Generation of Pseudo-masks} Based on the well-trained classification network, we use CAM (Class Activation Mapping) \cite{zhou2016learning} to generate pseudo-mask labels for the in-distribution dataset. Consistent with \cite{ahn2019weakly}, we use the ensemble of multi-scale images to generate accurate pseudo-mask labels. Specifically, an input image is converted to a set of 8 images through 4 different scales $\{0.5, 1.0, 1.5, 2.0\}$ and horizontal flips. After computing the mean values of the CAMs at all scales, the final CAM is smoothed by a Gaussian filter and converted to pseudo-mask labels based on an empirical threshold. In practice, we normalize the filtered CAMs and use 0.5 as the threshold.

\noindent\textbf{Dense Prediction Training Details} Finally, we use a modified Dense-BiT architecture to retrain the new dense prediction model with BiT-M-R50x1 checkpoints on the in-distribution dataset and pseudo-mask labels. We replace the MixUp augmentation from the training with randomly scaling (from 0.5 to 2.0) and randomly horizontally flipping augmentation. All images are resized to 128x128 during training and testing. Besides, the rest of the training strategy and hyperparameters are the consistent as the classification network.
\begin{figure}[t] 
    \centering
    \includegraphics[width=0.95\linewidth]{Figure/Ablation_Temperature_1.pdf}
    \vspace{-0.0cm}
    \caption{AUROC results of X-DOM using varying $\mathit{T}$ settings in the other two OOD dataset benchmark with CIFAR100 as the ID data.}
    % The dashed lines show the baseline results of the corresponding OOD scoring function.}
    \label{fig:T}
    \vspace{-0.0cm}
\end{figure}

\subsection{Experiments on Large-scale Semantic Space}
We implement X-DOM on the high-resolution large-scale dataset ImageNet-1k following the steps introduced in Sec. \ref{sec:imple} and evaluate its OOD detection performance in large-scale semantic spaces that have a large number of ID classes. There are two main differences in the implementation of large-scale experiments:

\noindent\textbf{Stable Mask Generation for High-resolution Images} We observe that since high-resolution images have richer appearance information, the network pays more attention to the most discriminative parts of the foreground object (\eg, the head of the fish). This leads to CAM assigning the highest class activation to the most discriminative parts and assigning the lower class activation to the rest part of the object, which degrades the quality of the resulting masks and does not completely segment the entire foreground object. Given that the average class activation of the whole object is still significantly higher than that of the background, we propose to use the mean value of the class activation of the whole image as the threshold for segmentation. As shown in Fig. \ref{fig:mean}, this trick significantly improves the pseudo-mask generated for high-resolution images.

\noindent\textbf{High-resolution OOD Datasets} In order to comprehensively evaluate the performance of X-DOM under high-resolution images, we add two high-resolution datasets as OOD datasets:
\begin{itemize}
    \item \textbf{ImageNet-O} consists of images from classes that are not found in the ImageNet-1k dataset. It is adversarially filtered to fool the classifier and used to evaluate the robustness of the classifier to out-of-distribution data.
    \item \textbf{SUN} contains 130,519 high-definition scene images from 397 categories. Following \cite{huang2021mos}, we select 50 nature-related categories that do not overlap with ImageNet-1k, and randomly sample 10,000 images as the OOD dataset.
\end{itemize}


\begin{figure}[t] 
    \centering
    \includegraphics[width=0.95\linewidth]{Figure/mean_mask.pdf}
    \vspace{-0.0cm}
    \caption{Examples of pseudo-masks generated for ImageNet1k using different strategies. (\textbf{Left}) CAM (\textbf{Middle}) Pseudo-masks generated using fixed threshold, and (\textbf{Right}) Pseudo-masks generated using mean class activation as the threshold.}
    % The dashed lines show the baseline results of the corresponding OOD scoring function.}
    \label{fig:mean}
    \vspace{-0.0cm}
\end{figure}


\section{Additional Experiment Results}
\subsection{Detailed Analysis of OOD Scoring Methods}
We report the detailed results of domain OOD scoring combined with all four post-hoc semantic OOD scoring methods in Tab. \ref{tab:ablation}, including the use of domain OOD scores $S_d$ only (DOM), semantic OOD scores $S_h$ (Vanilla), and the full X-DOM model. Consistent with the results in the main text, using only the domain OOD scores in X-DOM can yield significantly improved performance on OOD benchmarks with significant domain differences from the in-distribution domain. Moreover, although DOM underperforms in the semantic-feature-dependent CIFAR100 vs. CIFAR10 benchmark, it still obtains competitive results in CIFAR10 vs. CIFAR100, which also relies on semantic features. This difference is caused by the number of ID categories, where more ID categories make the ID domain richer and more challenging to detect OOD samples by using domain features only. Holistically, the complete X-DOM achieves the best performance on most benchmarks, demonstrating the need to synthesize semantic and domain OOD scores.

\subsection{Additional Results w.r.t. the Temperature Hyperparameter }
Fig. \ref{fig:T} supplements the results of X-DOM on the remaining two benchmarks using various temperatures. Due to the SVHN, Textures and Places365 benchmarks having significant domain distribution differences, all methods' performance gradually decreases as temperature increases. Note that stronger semantic OOD scoring methods (e.g., ViM) can significantly mitigate the decreasing performance trend. We choose $T=2.5$ as a trade-off between semantic and domain scores in our experiment. 
% In practical deployment, the temperature $T$ can be adjusted to detect various OOD samples according to the requirements.

\begin{table}[bt]
  \centering
  \caption{Comparison of X-DOM and outlier exposure (OE). $^{\dagger}$ indicates that the results are taken from the original paper, and other methods share the same architecture. Reported results are averaged over the results on the four OOD datasets.}
  \vspace{-0.0cm}
  \scalebox{0.88}{
    \begin{tabular}{p{2.2cm}|cc|cc}
    \hline
    \multirow{2}{*}{\textbf{Methods}}  &  \multicolumn{2}{c|}{\textbf{ID: CIFAR10}} & \multicolumn{2}{c}{\textbf{ID: CIFAR100}}\\
    \cline{2-5} 
    & \multicolumn{1}{c}{\textbf{FPR95}$\downarrow$} & \multicolumn{1}{c|}{\textbf{AUROC}$\uparrow$} & 
    \multicolumn{1}{c}{\textbf{FPR95}$\downarrow$} & 
    \multicolumn{1}{c}{\textbf{AUROC}$\uparrow$}\\
    \hline
    Baseline & 33.44 & 89.01 & 64.25 & 81.52 \\
     OE \cite{hendrycks2018deep} & 20.16 & 93.74 & 57.68 & 83.98  \\
     OE$^{\dagger}$ \cite{hendrycks2018deep} & 15.57 & 96.40 & 52.30 & 83.47  \\
     X-DOM & \textbf{7.87} & \textbf{98.04} & \textbf{42.29} & \textbf{91.03}  \\
     \hline
    \end{tabular}%
    }
  \label{tab:OE}%
  \vspace{-0.0cm}
\end{table}%

\subsection{X-DOM vs. Outlier Exposure}
Domain features can also be alternatively learned by using the popular outlier exposure (OE) method \cite{hendrycks2018deep} that uses external outlier data to support the learning. The comparison of X-DOM and OE is presented in Tab. \ref{tab:OE}, in which OE$^{\dagger}$ uses the large-scale 80M Tiny Image \cite{torralba200880} as the outlier dataset, OE is trained using Tiny ImageNet \cite{le2015tiny} as the outlier data, OE, OE$^{\dagger}$ and X-DOM are all based on the MSP-based OOD scoring function, and Baseline is the original MSP without using outlier data. The results show that the two OE methods can also significantly outperform the Baseline model, but their performance is heavily dependent on the outlier data, \eg, the results of OE and OE$^{\dagger}$ differ significantly from each other. By contrast, X-DOM does not need outlier data and significantly outperforms both OE methods on both benchmarks.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}



\end{document}