%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}
%% NOTE that a single column version is required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}

% %% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[MM'24]{Make sure to enter the correct
%   conference title from your rights confirmation email}{October 28 - November 1,
%   2024}{Melbourne, Australia.}
% %
% %  Uncomment \acmBooktitle if th title of the proceedings is different
% %  from ``Proceedings of ...''!
% %
% %\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
% %  June 03--05, 2018, Woodstock, NY} 
% \acmISBN{978-1-4503-XXXX-X/18/06}
\copyrightyear{2024}
\acmYear{2024}
\setcopyright{rightsretained}
\acmConference[MM '24]{Proceedings of the 32nd ACM International Conference on Multimedia}{October 28-November 1, 2024}{Melbourne, VIC, Australia}
% \acmBooktitle{Proceedings of the 32nd ACM International Conference on Multimedia (MM '24), October 28-November 1, 2024, Melbourne, VIC, Australia}
\acmDOI{10.1145/3664647.3681614}
\acmISBN{979-8-4007-0686-8/24/10}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%
\usepackage{epsfig}
\usepackage{dsfont}
\usepackage{color}
% \usepackage{cite,eucal}
% \usepackage{cite}
\usepackage{multirow,xcolor}
\usepackage{arydshln}
\usepackage{xspace}
% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\newcommand{\method}{{DOM}}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{cf}\onedot} \def\Cf{\emph{Cf}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\iid{i.i.d\onedot} \def\wolog{w.l.o.g\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother
%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Improving Out-of-Distribution Detection with Disentangled Foreground and Background Features}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


\author{Choubo Ding}
\affiliation{%
  \institution{The University of Adelaide}
  % \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Adelaide}
  \country{Australia}}
\email{choubo.ding@adelaide.edu.au}


\author{Guansong Pang}
\authornote{Corresponding author}
\affiliation{%
  \institution{Singapore Management University}
  % \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Singapore}
  \country{Singapore}}
\email{gspang@smu.edu.sg}

% \author{Name}
% \affiliation{%
%   \institution{Institution}
%   % \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%   \city{City}
%   \country{Country}}
% \email{xx@xx.xx}
%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Choubo Ding and Guansong Pang}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
    Detecting out-of-distribution (OOD) inputs is a principal task for ensuring the safety of deploying deep-neural-network classifiers in open-set scenarios. OOD samples can be drawn from arbitrary distributions and exhibit deviations from in-distribution (ID) data in various dimensions, such as foreground features (e.g., objects in CIFAR100 images vs. those in CIFAR10 images) and background features (e.g., textural images vs. objects in CIFAR10). Existing methods can confound foreground and background features in training, failing to utilize the background features for OOD detection. This paper considers the importance of feature disentanglement in out-of-distribution detection and proposes the simultaneous exploitation of both foreground and background features to support the detection of OOD inputs in in out-of-distribution detection. To this end, we propose a novel framework that first disentangles foreground and background features from ID training samples via a dense prediction approach, and then learns a new classifier that can evaluate the OOD scores of test images from both foreground and background features. It is a generic framework that allows for a seamless combination with various existing OOD detection methods. Extensive experiments show that our approach 1) can substantially enhance the performance of four different state-of-the-art (SotA) OOD detection methods on multiple widely-used OOD datasets with diverse background features, and 2) achieves new SotA performance on these benchmarks. Code is available at \renewcommand\UrlFont{\color{blue}}\url{https://github.com/mala-lab/DFB}.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010257</concept_id>
<concept_desc>Computing methodologies~Machine learning</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010178.10010224</concept_id>
<concept_desc>Computing methodologies~Computer vision</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010178.10010224.10010240.10010241</concept_id>
<concept_desc>Computing methodologies~Image representations</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257.10010258.10010260.10010229</concept_id>
<concept_desc>Computing methodologies~Anomaly detection</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Machine learning}
\ccsdesc[300]{Computing methodologies~Computer vision}
\ccsdesc[300]{Computing methodologies~Image representations}
\ccsdesc[300]{Computing methodologies~Anomaly detection}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Out-of-Distribution Detection, Disentangled Representations}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
% %% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

% \renewcommand{\thefootnote}{\fnsymbol{footnote}} 
% \footnotetext[1]{Corresponding author}
% \renewcommand{\thefootnote}{\arabic{footnote}}

\section{Introduction}
\label{sec:intro}



\begin{figure}[!t] 
    \centering
    \vspace{-0.0cm}
    \includegraphics[width=0.95\linewidth]{fig/qualitative.pdf}
    \vspace{-0mm}
    \caption{Saliency maps of ID (CIFAR10 \cite{krizhevsky2009learning}) and OOD datasets (CIFAR100~\cite{krizhevsky2009learning}, SVHN \cite{netzer2011reading}, Places365 \cite{zhou2017places}, Textures \cite{cimpoi2014describing}) with vanilla classifier and our proposed DFB classifier. Vanilla classifiers tend to focus on objects unrelated to the ID class, e.g., the person on a horse (ID class), due to spurious correlation. In OOD data, vanilla classifiers struggle to localize objects within the image and treat the background features as foreground for ID classification. By disentangling foreground and background features, DFB effectively addresses these issue.}
    \label{fig:motivation}
    \vspace{-0.5cm}
\end{figure}

Deep neural networks have demonstrated superior performance in computer vision tasks \cite{lecun2015deep}. Most deep learning methods assume that the training and test data are drawn from the same distribution. Thus, they fail to handle real-world scenarios with out-of-distribution (OOD) inputs that are not present in the training data \cite{quinonero2008dataset}. Failures in distinguishing these OOD inputs from in-distribution (ID) data may lead to potentially catastrophic decisions, especially in safety-critical applications like autonomous driving or medical systems \cite{chen2021robust}. In open-set applications, encountering unexpected OOD inputs is a common occurrence. Consequently, detecting and rejecting these OOD inputs has emerged as a significant challenge in the secure deployment of deep neural networks. 



OOD detection approaches are designed to address this problem, which aim to detect and reject these OOD samples while guaranteeing the classification of in-distribution data \cite{hendrycks17baseline}.
There are generally two groups of OOD detection approaches. One of them are post-hoc approaches that work with a trained classification network to derive OOD scores without re-training or fine-tuning of the network,
\eg, by using maximum softmax probability of the network outputs \cite{hendrycks17baseline}, maximum logits \cite{HendrycksBMZKMS22}, or the Mahalanobis distance between the input and the class centroids of ID data \cite{lee2018simple}. Another group of approaches fine-tunes the classifiers with different methods, such as the use of pseudo OOD samples \cite{hendrycks2018deep,mohseni2020self, liu2020energy, chen2020robust, papadopoulos2021outlier, Wu_2021_ICCV, Yang_2021_ICCV}. 
Most of these methods, especially the post-hoc methods, are primarily based on the "foreground features" to detect OOD samples. These are the features that exhibit the semantics of the in-distribution classes, such as the appearance features of the `horse' class images in the CIFAR10 image classification, as shown in Fig. \ref{fig:motivation}. This introduces two problems: 1) Owing to dataset bias, the foreground features learned by vanilla classifiers may develop spurious correlations \cite{ming2022impact} with other objects in the image. For instance, a correlation may be incorrectly drawn between horses and riders. These spurious correlations introduce irrelevant semantics, which can lead to the misclassification of not only ID images but also OOD images. 2) Focusing on foreground features overlooks other dimensions that could also be important for OOD detection, as OOD samples can be drawn from arbitrary distributions and can exhibit deviations from in-distribution (ID) data in various dimensions. One such dimension is the set of ``background features" that exhibit no class semantics. We observe that the regions of interest for the classifiers on OOD samples often tend to be the background, as shown in the CIFAR100 example in Fig. \ref{fig:motivation}. This phenomenon indicates that existing classifiers have a foreground-background confusion issue, and they misclassify OOD samples to the ID classes with similar semantic features to these background features.


This paper considers the importance of disentangling foreground and background features in OOD detection and proposes to leverage background features to enhance the OOD detection methods that are based on foreground features. To this end, we introduce a novel generic framework, called DFB, that can \textbf{\underline{D}isentangle the \underline{F}oreground and \underline{B}ackground features} from ID training samples by a dense prediction approach, with which different existing foreground-based OOD detection methods can be seamlessly combined to learn the in-distribution features from both the foreground and background dimensions. 
Specifically, given a trained $K$-class classification network where $K$ is the number of in-distribution classes, DFB first generates pseudo semantic segmentation masks through a weakly-supervised segmentation approach that uses image-level labels to locate discriminative regions in the images. These pseudo segmentation masks are then utilized to train a $(K+1)$-class dense prediction network, \textit{with the first $K$ classes being the original $K$ ID classes and the $(K+1)$-th class corresponding to the ID background features}. The dense prediction network is further converted into a $(K+1)$-class classification network by adding a global pooling layer. The conversion is lossless and requires no re-training. In doing so, the $(K+1)$-class classifier learns both foreground and background ID features. Different existing foreground-based methods, such as the post-hoc methods, can be applied to the first $K$ prediction outputs to obtain \textbf{semantic OOD scores}, while the $(K+1)$-th prediction can be directly used to define \textbf{background OOD scores}. Combining these semantic and background OOD scores enables OOD detection from both foreground and background features.

As depicted in Fig. \ref{fig:motivation}, the proposed DFB effectively disentangles foreground and background features. In the ID data CIFAR10, DFB can more accurately locate the ID objects than the vanilla classifier. In the OOD data CIFAR100, which contains significant foreground objects, DFB successfully disentangle between the foreground and background objects, both of which are important for detecting the OOD. In the other three OOD datasets without prominent foreground objects, the foreground branch of DFB focuses on fewer areas compared to the vanilla classifier while its background branch recognizes most of the background areas, in which the background OOD scores would exert greater influence on the OOD detection. Note that the influence of semantic and background features is determined by the area of these features, so a hyperparameter with a fixed value can well control these two OOD scores across diverse OOD datasets; no careful tuning of this hyperparameter is required per OOD dataset.
% Furthermore, evaluations on multiple widely used OOD datasets indicate that DFB effectively improves the performance of existing OOD methods and achieves new SotA performance on these benchmarks.

In summary, we make the following main contributions:
\begin{itemize}
% \itemsep -0.1cm 
    \item This work studies the importance of disentangling foreground and background features and proposes to synthesize both foreground and background features for more effective OOD detection in diverse real-world applications. This provides a new insight into the OOD detection problem. 
    \item We then propose a novel approach DFB, in which different existing foreground-based OOD detection methods can be seamlessly combined to jointly learn the ID features from both foreground and background dimensions. It offers a generic approach to enhance current OOD detection methods. To our knowledge, this is the first generic framework for joint foreground and background OOD detection.
    \item Extensive experiments on four widely-used OOD datasets with diverse background show that our approach DFB 1) can substantially enhance the performance of four different state-of-the-art (SotA) OOD detection methods, and 2) achieves new SotA performance on these benchmarks.
\end{itemize}

\section{Related Work}
\textbf{Post-hoc Approaches.}
Modelling the uncertainty of pre-trained DNN directly without retraining the network is one popular approach for OOD detection \cite{gomes2022igeood, cook2020outlier, huang2021importance, sastry2020detecting, bendale2016towards, wang2021can, Zisselman_2020_CVPR, dong2022neural, Liu_2023_CVPR, Yu_2023_CVPR, Olber_2023_CVPR}. Hendrycks et al. \cite{hendrycks17baseline} propose the uncertainty of DNNs and establish a baseline for OOD detection by maximum softmax probability (MSP). ODIN \cite{liang2018enhancing} introduces input perturbation and temperature scaling to enhance MSP. Lee et al. \cite{lee2018simple} propose the deep Mahalanobis distance-based detectors, which compute the distance-based OOD scores from the pre-trained networks' features. Liu et al. \cite{liu2020energy} calculate the logsunexp on logit as the energy OOD score. ReAct \cite{sun2021react} reduces the DNN's overconfidence in OOD samples by activation clipping, which further enhances the energy scores. MaSF \cite{haroush2022a} considers the empirical distribution of each layer and channel in the CNN and returns a p-value as the OOD score. 
%Dong et al. \cite{dong2022neural} observe that the model activation averages for OOD and ID inputs are significantly distinct and compute the neural mean from the batch normalized layer for OOD detection. 
ViM \cite{wang2022vim} attempts to utilize not only primitive semantic features but also their residuals to define more effective logit-based OOD scores.
Recently, DML \cite{Zhang_2023_CVPR} reformulate the logit into cosine similarity and logit norm and propose to use flexibility balanced MaxCosine and MaxNorm.
This type of approach relies on the foreground semantic features learned by the pre-trained networks, which neglects other relevant features, such as background features. 

\begin{figure*}[t!]
    \centering
    \vspace{-0.3cm}
    \includegraphics[width=0.96\textwidth]{fig/framework.pdf}
    \vspace{-0.2cm}
    \caption{Overview of our proposed framework. It first uses a trained $K$-class classification network to obtain pseudo semantic segmentation masks and then learns the in-distribution features by training a $(K+1)$-class classification network with the pseudo labels (\textbf{Left}). It lastly converts the dense prediction network to a $(K+1)$-class classifier in a lossless fashion, and leverages these $(K+1)$ prediction outputs for joint foreground and background OOD detection (\textbf{Bottom Right}).
    }% shows how to extract domain information from a K classification network and train a dense prediction network.} %\gs{indicate in the figure that the upper right one is semantic OOD scoring in existing methods; K classes Classification Network -> K-class Classification Network; K+1 classes Classification Network -> (K+1)-class Classification Network}}
    \label{fig:framework}
    \vspace{-0.2cm}
\end{figure*}
%(right) shows the lossless conversion of a dense prediction network and a regular classification network.\gs{the difference between the two classifiers needs to be highlighted; the plug-in property also needs to be visible}

\textbf{Fine-tuning Approaches.}
Another dominant approach is to fine-tune the classification networks for adapting to the OOD detection tasks.
In this line of research, Hsu et al. \cite{hsu2020generalized} further improve ODIN by decomposing confidence scoring. Zaeemzadeh et al. \cite{zaeemzadeh2021out} project ID samples into a one-dimensional subspace during training. Some other studies \cite{Vyas_2018_ECCV, huang2021mos} group ID data and assume them as OOD samples for each other to guide the network training. 
Some studies \cite{cai2023out, ren2019likelihood} have noticed the influence of background features on OOD detection, but they focus on solving the confusion between background and semantic features, and ignore the positive influence of background features on OOD detection itself.
Outlier Exposure (OE) \cite{hendrycks2018deep} introduces auxiliary outlier data to train the network and improve its OOD detection performance. Such approaches can use real outliers \cite{mohseni2020self, liu2020energy, chen2020robust, papadopoulos2021outlier, Wu_2021_ICCV, Yang_2021_ICCV, Choi_2023_CVPR,li2024learning} or synthetic ones from generative models \cite{lee2018training}. The performance of this approach often depends on the quality of the outlier data. Fine-tuning the networks may also lead to the loss of semantic information and consequently degraded ID classification accuracy.
%Our method does not require additional outliers and can retain the semantic information of the ID data during training.

\section{Proposed Approach}
\noindent\textbf{Problem Statement.} 
% The studied out-of-distribution detection problem can be formally stated as follows. 
Given a set of training samples $\mathcal{X}=\{ \mathbf{x}_{i}, \mathbf{y}_{i}
\}_{i=1}^{N}$ drawn from an in-distribution $\mathit{P}_\mathcal{X}$ with label space $\mathcal{Y} = \{ y_{j}
\}_{j=1}^{K}$, and let $f: \mathcal{X} \rightarrow \mathbb{R}^\mathcal{Y}$ be a classifier trained on the in-distribution samples $\mathcal{X}$, then the goal of OOD detection is to obtain a new decision function $g$ to discriminate whether $\mathbf{x}$ come from $\mathit{P}_\mathcal{X}$ or out-of-distribution data $\mathit{P}_{out}$:
\vspace{-0cm}
\begin{equation*}
    g(\mathbf{x}, f) = \left\{\begin{array}{l}
1 \quad \text{if}\enspace\mathbf{x}\in\mathit{P}_{out},
\\0 \quad \text{if}\enspace\mathbf{x}\in\mathit{P}_\mathcal{X}.
\end{array}\right.
\end{equation*}
The difference between $\mathit{P}_\mathcal{X}$ and $\mathit{P}_{out}$ determines the difficulty of detecting the OOD samples. Existing OOD detection approaches focus on the difference between $\mathit{P}_\mathcal{X}$ and $\mathit{P}_{out}$ based on the semantic information of the class label space $\mathcal{Y}$, neglecting other relevant dimensions such as the background feature space. This work aim to learn the background features and leverage them to complement these foreground-based OOD detection approaches.

% \begin{figure}[t] 
%     \centering
%     \includegraphics[width=0.99\linewidth]{fig/sample_img.pdf}
%     \vspace{-0.2cm}
%     \caption{sample image}
%     \label{fig:img}
%     \vspace{-0.5cm}
% \end{figure}

\subsection{Overview of Our Approach}

Using semantic of foreground objects only to detect OOD samples can often be successful when the OOD samples have some dominant semantics that are different from the ID images. However, approaches of this type would fail to work effectively when the OOD samples do not have clear object semantics and/or exhibit some similar semantic appearance to the ID samples, \eg, the images illustrated in Fig. \ref{fig:motivation}. %\gs{motivation figure}.
Motivated by this, we introduce a generic framework DFB, in which the model disentangle the foreground and background features of the in-distribution data and learns ID background features, upon which different existing OOD detection methods can be applied with the learned background representations to detect OOD samples from both of the foreground and background dimensions.A high-level overview of our proposed framework is provided in Fig. \ref{fig:framework}. 
\begin{enumerate}
% \itemsep -0.1cm 
    \item DFB first disentangles and learns the in-distribution foreground and background features by a $(K+1)$-class dense prediction network trained from the given pre-trained K-class classification network.
    \item It then seamlessly integrates the foreground and background features into image classification models by transforming the dense prediction network to a $(K+1)$-class classification network, where the prediction entries of the $K$ classes are focused on the class semantics of the $K$ in-distribution class while the extra (+1) class is focused on the in-distribution background  features.
    \item Lastly, an OOD score in the foreground dimension obtained from existing post-hoc OOD detectors based on the $K$-class predictions, and an OOD score obtained from the extra (+1) class prediction from the background dimension, are synthesized to perform OOD detection.
\end{enumerate}


\subsection{Learning In-distribution Background Features via $(K+1)$-class Dense Prediction}\label{subsec:domainfeature}

DFB aims to learn distinct representations of foreground and background information in images, while also considering them as in-distribution features. The key challenge here
is how to locate these background features and separate them from the foreground features. We introduce a weakly-supervised dense prediction method to tackle this challenge, in which weakly-supervised semantic segmentation methods are first utilized to generate pseudo segmentation mask labels that are then used to train a $(K+1)$-class dense prediction network. The extra (+1) class learned in the dense predictor is specifically designed to learn the background features, while the other $K$ class predictions are focused on learning the foreground features of the $K$ classes given in the training data. Particularly, given the training data $\mathcal{X}$ with $K$-class image-level labels $\mathcal{Y}$ and a trained $K$-class classification network $\phi$, the pseudo segmentation mask labels can be obtained by the class activation mapping \cite{zhou2016learning}:
% \vspace{-0.1cm}
\begin{equation}
    \mathbf{M}_{y_{\mathbf{x}}}^{(i,j)} = \mathbf{W}_{y_{\mathbf{x}}}^\top \phi_{\text{cnn}}(\mathbf{x})^{(i,j)},
\end{equation}
%-based segmentation method: 
%\gs{list the CAM equation below; please double check whether we are using the original CAM method or improved methods, such as GradCAM.}
where $\mathbf{W}_{y_{\mathbf{x}}}$ is the classification weight of the trained classifier $\phi$ corresponding to the groundtruth class $y_{\mathbf{x}}$ of $\mathbf{x}$, and $\phi_{\text{cnn}}(\mathbf{x})^{(i,j)}$ obtains the feature vector at the unit $(i, j)$ in the feature map extracted by the feature extractor in $\phi$
% $f_{\text{cnn}}$ 
from image $\mathbf{x}$. $\mathbf{M}_{y_{\mathbf{x}}}\in\mathbb{R}^{H\times W}$ is an attention map indicating a pixel-wise semantic score of $\mathbf{x}$ relative to its groundtruth class $y_{\mathbf{x}}$. We then define a foreground decision threshold $\theta$ to generate the fine-grained pseudo labels of background pixels and foreground pixels by: %\gs{please write the pseudo label generation in the equation form}
\vspace{-0cm}
\begin{equation}
    \mathbf{\hat{Y}}^{(i,j)}(\mathbf{x}) = \left\{\begin{array}{ll}
0&\text{if} \; \mathbf{M}_{y_{\mathbf{x}}}^{(i,j)} < \theta,\\
1& \text{if} \; \mathbf{M}_{y_{\mathbf{x}}}^{(i,j)} \geqslant \theta,
\end{array}\right.
\end{equation}
where the attention scores are normalized into the range $[0,1]$ and $\theta=0.5$ is used.
% to distinguish between foreground and background.
% The traditional classification network is unable to learn knowledge from the generated pseudo pixel-level labels $\mathit{\hat{y}}$. Thus, we propose using a dense prediction network to learn pixel-level knowledge. Differs from  image classification, pixel-wise dense prediction requires outputting prediction labels for each pixel of a given input, which also includes background pixels. As shown in Figure 2 left, 
We then leverage these pseudo labels of the foreground and background pixels, $\mathbf{\hat{Y}}$, to train a $(K+1)$-class dense prediction network $f_{\Theta_d}:\mathcal{X} \rightarrow \{0, 1\}^{K\times H\times W}$ via a pixel-level cross entropy loss:
% using the pseudo pixel-level labels $\mathit{\hat{y}}$ generated by CAM, with the loss function defined as follows:
\begin{equation}
    L(\mathbf{x}, \mathbf{\hat{Y}}) = \frac{-1}{H\times W}\sum_{i=1}^{H}\sum_{j=1}^{W}\sum_{k=1}^{K+1}\hat{y}_{k}^{(i, j)}\log\left(f(\mathbf{x},\Theta_d)_k^{(i,j)}\right),
\end{equation}
where $f(\mathbf{x},\Theta_d)^{(i,j)}$ outputs a prediction vector consisting of prediction probabilities of the $K+1$ classes at the image pixel $(i,j)$, and $\hat{\mathbf{y}}^{(i, j)}$ denotes the corresponding pseudo labels at the same pixel. In doing so, $f_{\Theta_d}$ learns both in-distribution foreground and background features.
% the sum over $i$ and sum over $j$ runs over all pixel in input image $\mathbf{x}$, the sum over $k$ goes over the all possible classes.


\subsection{Dense Prediction to Image Classification}
% \subsection{Joint Semantic and Domain Classification}
% Although we successfully learned the latent domain features using the dense prediction network, we obviously cannot use 
The pixel-level foreground and background features learned in the dense prediction network cannot be applied directly to the image classification task. We show below that the $(K+1)$-class dense prediction network can be transformed to a $(K+1)$-class image classification network in a lossless fashion: the dense prediction and the classification networks share the same weight parameters, and the classification network can be applied to image classification without re-training.  
% We need to transfer the weights of the dense prediction network to the classification network and retain the learned latent domain features. 
Particularly, the dense prediction network $f_{\Theta_d}:\mathcal{X} \rightarrow \{0, 1\}^{(K+1)\times H\times W}$ can be decomposed into three main modules: 1) a feature extraction network $f_{\Theta_{\mathit{CNN}}}:\mathcal{X} \rightarrow \mathcal{G}$ consisting of a convolutional neural network that extracts the input image $\mathbf{x}\in\mathbb{R}^{3\times H\times W}$ into a smaller scale but larger dimensional feature map $\mathbf{G}\in\mathbb{R}^{C\times h\times w}$, 2) an upsampling module $\text{up}(\cdot)$ that upsamples the feature map $\mathbf{G}$ to original input size $H\times W$, typically implemented using bilinear interpolation, and 3) a 1x1 convolution classifier $f_{\Theta_{\mathit{cls}}}:\mathcal{G} \rightarrow \mathcal{L}$ that computes the logit for each pixel in the feature map and outputs a logit map $\mathbf{L}\in\mathbb{R}^{(K+1)\times H \times W}$. The size of weights of the convolutional classifier is $C\times (K+1)$.
% , where $K+1$ is the number of categories.
% \gs{I think it should be $K+1$ instead of $K$? please check them corresponding if true.} \cb{In this section, what we claim is that the dense prediction network shares the same weights as the classification network for the same number of classes. I think either $K$ or $K+1$ could both be confusing, maybe we can use $C$ to refer to the number of classes here?}. 
Thus, the dense prediction network $f_{\Theta_d}$ can be denoted as:
\vspace{-0cm}
\begin{equation}
    f(\mathbf{x},\Theta_d) = \text{softmax}(f(\text{up}(f(\mathbf{x},\Theta_{\mathit{CNN}})),\Theta_{\mathit{cls}})),
    \label{eq:dense}
\end{equation}
where $\Theta_d=\{\Theta_{\mathit{CNN}}, \Theta_{\mathit{cls}}\}$.

\begin{figure}[t] 
    \centering
    \vspace{-0.0cm}
    \includegraphics[width=0.95\linewidth]{fig/convert.pdf}
    \vspace{-0.3cm}
    \caption{Lossless conversion of a dense prediction network to a classification network.}
    \label{fig:predictiontoclassification}
    \vspace{-0.3cm}
\end{figure}

For a classification network $f_{\Theta_c}: \mathcal{X} \rightarrow \mathbb{R}^\mathcal{Y}$, it can be similarly decomposed into three main modules: 1) a feature extraction network $f_{\Theta_{\mathit{CNN}}}:\mathcal{X} \rightarrow \mathcal{G}$ with the same function as the dense prediction network, 2) a global average pooling $\text{GAP}(\cdot)$, which compresses the feature map of $C\times H\times W$ into a feature vector of size $C\times 1 \times 1$, integrating the features of the full image, and 3) a linear classifier $f_{\Theta_{\mathit{cls}}}:\mathcal{G} \rightarrow \mathcal{L}$, which computes the logit of the full image based on the feature vector with $C\times (K+1)$ weights. The classification network $f_{\Theta_c}$ can be denoted as:
% \vspace{-0.1cm}
\begin{equation}
    f(\mathbf{x},\Theta_c) = \text{softmax}(f(\text{GAP}(f(\mathbf{x},\Theta_{\mathit{CNN}})),\Theta_{\mathit{cls}})),
    \label{eq:cls}
\end{equation}
where $\Theta_c=\{\Theta_{\mathit{CNN}}, \Theta_{\mathit{cls}}\}$. It is clear from Eqs. (\ref{eq:dense}) and (\ref{eq:cls}) that
% , we can notice that the dense prediction network and the classification network can share the same feature extraction network and weights of the classifier. 
the only difference between $f_{\Theta_d}$ and $f_{\Theta_c}$ is the upsampling module and the GAP module, sharing the same feature extraction network and the classifier. Further, the upsampling and the GAP modules are weight-free operations, which can be easily replaced with each other, as shown in Fig. \ref{fig:predictiontoclassification}. In this way, we directly transfer the in-distribution foreground and background features learned in the dense prediction network $f_{\Theta_d}$ to the image classification network $f_{\Theta_c}$
% , the dense prediction network can easily replace the upsampling module with the GAP to convert it into a classification network, and they can share all the weights, i.e., the learned knowledge, 
without any loss of the parameters learned in $f_{\Theta_d}$. 
% With the conversion of the network, we can successfully obtain a classification network with learned latent domain features, which will
For a given test image, the classifier $f_{\Theta_c}$ yields a $(K+1)$-dimensional logit vector, where 
% the first $K$ logits can be used to detect OOD samples from the semantic dimension and 
the $(K+1)$-th logit is focused on the in-distribution background features and can be used directly to detect OOD samples from the background dimension: 
% \gs{please add an equation to define the domain score $S_b$ here}
% the extra dimension is the background similarity, which can be used as a domain score for OOD detection.
\vspace{-0cm}
\begin{equation}\label{eq:domain}
    S_b(\mathbf{x}) = \mathbf{L}_{\mathbf{x}}[K+1],
\end{equation}
where $\mathbf{L}_{\mathbf{x}}=f(\text{GAP}(f(\mathbf{x},\Theta_{\mathit{CNN}})),\Theta_{\mathit{cls}})$ is a (K+1)-dimensional prediction logit vector yielded by $f_{\Theta_c}$.

% \subsection{OOD Detection with Domain Score}
\subsection{Joint Foreground and Background OOD Detection}

Although the background-based OOD score $S_b$ can be used to detect OOD samples directly, it can miss the OOD samples whose detection relies heavily on the foreground features. Thus, we propose to utilize this background OOD score to complement existing SotA foreground-based OOD detectors. Particularly, since the first $K$ classification logits in $f_{\Theta_c}$ capture similar class semantics as the original $K$-class classifier, off-the-shelf \textit{post-hoc} OOD detection methods that derive an OOD score from these $K$ classification logits can be plugged into DFB to obtain an OOD score from the foreground feature aspect. These foreground and background-based OOD scores are synthesized to achieve a joint foreground and background OOD detection.

% Following two main steps, the proposed DFB successfully extracts potential domain information from both the in-distribution dataset and the pre-trained network. Although we can directly detect out-of-distribution inputs by domain scores, we also miss important semantic information in this way. Ensemble semantic and domain information is more robust in detecting diverse out-of-distribution inputs. Hence, as shown in Fig. 2, we split the logit output of the new classification network into two parts: the background classes are output separately as domain scores. The remaining logit is consistent with the output of the original classification network and can be considered a semantic logit. We could apply

There are generally two types of post-hoc OOD detection approaches, including raw logit-based and softmax probability-based methods. Our background-based OOD score is based on an unbounded logit value, which can dominant the overall OOD score when combining with the foreground-based OOD score using the softmax output (its value is within $[0,1]$). To avoid this situation, we take a different approach to combine the foreground and background-based OOD scores, depending on the type of the foreground-based OOD detector used:
\vspace{-0.0cm}
\begin{equation}\label{eqn:combine}
S(\mathbf{x}) = \left\{\begin{array}{ll}
S_h(\mathbf{x}) + \frac{\log(S_b(\mathbf{x}))}{T}&\text{if} \; S_h \; \text{is softmax-based},
\\S_h(\mathbf{x}) + \frac{S_b(\mathbf{x})}{T} & \text{if} \; S_h \; \text{is logit-based},
\end{array}\right.    
\end{equation}
where $S(\mathbf{x})$ is the final OOD score used to perform OOD detection in DFB, $S_h(\mathbf{x})=h(\mathbf{x})$ denotes the OOD score obtained from using an existing foreground-based OOD scoring function $h$, and $T$ is a temperature coefficient hyperparameter. In Eq. (\ref{eqn:combine}), to obtain faithful foreground-and-background-combined OOD score, the log function is used to constrain the value and the variance of the background scores $S_b$, while $T$ is used to adjust the distribution of the background scores to match that of the foreground scores.

% One of the main problems of ensemble two different scores is the different value domains and variances. For example, the value domain of the semantic score output by the softmax probability-based method lies between 0 and 1. But the domain score taken from the not-bounded logit may be much higher than this value, and this significant difference may cause the performance of the ensemble score to be determined by domain score. For this situation, we propose using the log function to constrain the value domain and variance of the domain scores when applying DFB on the softmax probability-based methods. In contrast, the logit-based methods do not require similar constraints due to the similarity of the value domain to the logit-based domain scores. In summary, the ensemble of semantic scores and domain scores can be shown as:
% \begin{equation}
% S_E = \left\{\begin{array}{ll}
% S + \log(S_b)&\text{if S}\in\text{softmax-based},
% \\S + S_b&\text{if S}\in\text{logit-based},
% \end{array}\right.    
% \end{equation}

% where $S_b$ represents domain scores and $S$ represents semantic scores. Other than the significant differences mentioned above, there are still some subtle distribution differences between domain and semantic scores due to various reasons (e.g., unbalanced samples, bias in the dataset, etc.), which lead to bias in the final OOD scores. To this end, we propose to use temperature coefficients $T$ to control the distribution of domain and semantic scores, not only to balance the difference between them but even to detect OOD inputs with different attributes by adjusting $T$. The final OOD score with the temperature $T$ can be presented as:
% \begin{equation}
% S_E = \left\{\begin{array}{ll}
% S + \frac{\log(S_b)}{T}&\text{if S}\in\text{softmax-based},
% \\S + \frac{S_b}{T} & \text{if S}\in\text{logit-based},
% \end{array}\right.    
% \end{equation}
% We further discuss and compare the performance of the semantic, domain, and ensemble scores in section A and analyse the effects of various temperatures T in section B.
\section{Experiments}
%We firstly introduce the benchmark dataset and other experimental settings. In Section 4.2, we show that DFB can effectively improve the performance of the baseline method and achieves state-of-the-art OOD detection performance. We demonstrate the extensive ablation of DFB in Section 4.3 and discuss other applications of DFB in Section 4.4.
%\subsection{Experimental Setup}
\noindent\textbf{Datasets.}  Following \cite{hendrycks17baseline, liang2018enhancing, lee2018simple, liu2020energy, haroush2022a, Zhang_2023_CVPR}, we choose two widely-used classification datasets: CIFAR10 and CIFAR100 \cite{krizhevsky2009learning}, as the in-distribution datasets. As OOD samples are unknown during training, their respective training and test data are used as ID data, with samples from a different dataset added into the test set as the OOD data.
To evaluate the effectiveness of our approach, four commonly-used OOD datasets consisting of natural image datasets with diverse background features are used, including SVHN \cite{netzer2011reading}, Places365 \cite{zhou2017places}, Textures \cite{cimpoi2014describing}, and CIFAR100/CIFAR10 \cite{krizhevsky2009learning} (CIFAR100 is used as OOD data when CIFAR10 is used as ID data, and vise versa \cite{fort2021exploring, ren2021simple, sastry2020detecting}) SVHN is a digit classification dataset cropped from pictures of house number plates, Places365 is a large-scale scene classification dataset,while Textures contains 5,640 texture images in the wild 
that do not contain specific objects and backgrounds. Images in all these three datasets exhibit largely different foreground and background distributions, so the three datasets contain strong out-of-distribution semantic and background features. On the other hand, both CIFAR10 and CIFAR100 are sampled from Tiny Images \cite{torralba200880}and they share similar background features, so when they are used as OOD data for each other, the background features are weak. 
%Also, the objects in the images of these two datasets can be very similar, so the foreground OOD features are also weak. 
As a result, this pair of mutual OOD/ID combination is considered as hard OOD detection benchmarks \cite{winkens2020contrastive}
%\gs{references}. 
% The OOD feature types are summarized in Tab. \ref{tab:datasets}.
% while OOD semantic features .
% can be regarded as having no domain distribution bias and only semantic distribution bias. Some studies [] consider OOD detection between this pair of datasets as a challenging task.


\begin{table*}[bt]
  \centering
  \caption{\textbf{OOD detection results with CIFAR10 as in-distribution data.} 
%   OOD detection performance for various competitive/SotA methods and ours DFB when $\mathcal{D}_{in}$=CIFAR10. 
  All methods are based on ID training data without using any external outlier data. $^{\dagger}$ indicates that the results are taken from the original paper, and other methods are reproduced using the same network architecture. Four post-hoc foreground OOD detection methods are respectively plugged into our method `X'-DFB, where improved results are highlighted in \textcolor{red}{red} and they are in \textcolor{blue}{blue} otherwise. The best result per dataset is \textbf{boldfaced}. }
  \vspace{-0.1cm}
  \scalebox{1.0}{
    \begin{tabular}{p{3.8cm}cccccc}
    \hline
    \multirow{3}{*}{\textbf{Methods}} & \multicolumn{4}{c}{\textbf{OOD Datasets}} & \multirow{2}{*}{\textbf{Average}}\\
    \cline{2-5} 
    & \multicolumn{1}{c}{\textbf{CIFAR100}} & \multicolumn{1}{c}{\textbf{SVHN}} & \multicolumn{1}{c}{\textbf{Places365}} & \multicolumn{1}{c}{\textbf{Textures}} &  \\
    & \multicolumn{5}{c}{\textbf{FPR95}$\downarrow$ /\textbf{AUROC}$\uparrow$ /\textbf{AUPR}$\uparrow$} \\
    \hline
    MaxLogit \cite{HendrycksBMZKMS22} {\scriptsize \textcolor{gray}{[ICML'22]}} & 39.11/85.07/78.13 & 17.95/94.78/84.22 & 24.05/91.10/86.41 & 7.93/97.57/98.07 & 22.26/92.13/86.71  
    \\
    KL-Matching \cite{HendrycksBMZKMS22} {\scriptsize \textcolor{gray}{[ICML'22]}} & 33.63/90.20/88.18 & 25.70/95.21/88.37 & 25.25/92.88/90.78 & 12.61/97.28/98.21 & 24.30/93.89/91.38
    \\
    ReAct \cite{sun2021react} {\scriptsize \textcolor{gray}{[NIPS'21]}}& 34.75/84.10/79.89 & 20.03/90.58/76.30 & 23.45/91.88/89.43 & 10.27/96.53/97.69 & 22.12/90.77/85.83
    \\
    MaSF$^{\dagger}$ \cite{haroush2022a} {\scriptsize \textcolor{gray}{[ICLR'22]}}&  - /82.10/ -  &  - /99.80/ -  &  - /96.00/ -  &  - /98.50/ -  &  - /94.10/ - 
    \\
    %NMD$^{\dagger}$ \cite{dong2022neural} {\scriptsize \textcolor{gray}{[CVPR'22]}}&  - /90.10/ -  &  - /99.60/ -  &  - / - / -  &  - /98.90/ -  &  - /72.15/ - \\
    DML+$^{\dagger}$ \cite{Zhang_2023_CVPR} {\scriptsize \textcolor{gray}{[CVPR'23]}}&  42.55/91.36/ -  &  3.37/99.38/ -  &  24.34/94.87/ -  &  15.31/97.05/ -  &  21.39/95.67/- 
    \\
    \cline{1-6}
    MSP \cite{hendrycks17baseline} {\scriptsize \textcolor{gray}{[ICLR'17]}}& 33.44/89.01/84.10 & 17.40/95.72/88.68 & 22.47/92.93/89.79 & 8.55/97.66/98.38 & 20.46/93.83/90.24 
    \\
    MSP-DFB {\scriptsize \textcolor{gray}{[Ours]}} & \textcolor{red}{23.75}/\textcolor{red}{94.29}/\textcolor{red}{93.48} & \textcolor{red}{2.55}/\textcolor{red}{98.94}/\textcolor{red}{97.88} & \textcolor{red}{5.05}/\textcolor{red}{98.49}/\textcolor{red}{98.60} & \textcolor{red}{0.02}/\textcolor{red}{99.90}/\textcolor{red}{99.95} & \textcolor{red}{7.84}/\textcolor{red}{97.90}/\textcolor{red}{97.48}
    \\
    % & 9.69/5.27/9.39 & 14.85/3.22/9.20 & 17.42/5.56/8.81 & 8.53/2.24/1.57 & 12.62/4.07/7.24 \\
    \hdashline
    ODIN \cite{liang2018enhancing} {\scriptsize \textcolor{gray}{[ICLR'18]}}& 34.62/87.83/81.92 & 16.13/95.66/87.30 & 22.15/92.43/88.59 & 7.45/97.86/98.37 & 20.09/93.45/89.04
    \\
    ODIN-DFB {\scriptsize \textcolor{gray}{[Ours]}} & \textcolor{red}{22.15}/\textcolor{red}{95.50}/\textcolor{red}{95.29} & \textcolor{red}{4.27}/\textcolor{red}{99.19}/\textcolor{red}{98.30} & \textcolor{red}{8.08}/\textcolor{red}{98.66}/\textcolor{red}{98.76} & \textcolor{red}{0.34}/\textcolor{red}{99.92}/\textcolor{red}{99.95} & \textcolor{red}{8.71}/\textcolor{red}{98.32}/\textcolor{red}{98.07}
    \\
    % & 12.47/7.67/13.37 & 11.86/3.53/11.00 & 14.07/6.22/10.17 & 7.11/2.05/1.58 & 11.38/4.87/9.03 \\
    \hdashline
    Energy \cite{liu2020energy} {\scriptsize \textcolor{gray}{[NIPS'20]}}& 41.98/84.25/77.47 & 19.73/94.46/83.67 & 25.42/90.74/86.06 & 8.72/97.45/97.99 & 23.96/91.73/86.30
    \\
    Energy-DFB {\scriptsize \textcolor{gray}{[Ours]}} & \textcolor{red}{19.90}/\textcolor{red}{94.98}/\textcolor{red}{93.76} & \textcolor{red}{3.10}/\textcolor{red}{99.28}/\textcolor{red}{98.14} & \textcolor{red}{6.96}/\textcolor{red}{98.60}/\textcolor{red}{98.52} & \textcolor{red}{0.53}/\textcolor{red}{99.87}/\textcolor{red}{99.91} & \textcolor{red}{7.62}/\textcolor{red}{98.19}/\textcolor{red}{97.58}
    \\
    % & 22.08/10.73/16.29 & 16.63/4.82/14.47 & 18.46/7.87/12.45 & 8.19/2.43/1.92 & 16.34/6.46/11.28 \\
    \hdashline
    ViM \cite{wang2022vim} {\scriptsize \textcolor{gray}{[CVPR'22]}}& 15.25/96.92/\textbf{96.78} & 1.27/99.47/99.08 & 2.74/99.32/99.34 & 0.11/99.93/99.96 & 4.84/98.91/98.79
    \\
    ViM-DFB {\scriptsize \textcolor{gray}{[Ours]}} & \textcolor{red}{\textbf{13.49}}/\textcolor{red}{\textbf{97.08}}/\textcolor{blue}{96.75} & \textcolor{red}{\textbf{0.41}}/\textcolor{red}{\textbf{99.85}}/\textcolor{red}{\textbf{99.68}} & \textcolor{red}{\textbf{0.72}}/\textcolor{red}{\textbf{99.85}}/\textcolor{red}{\textbf{99.85}} & \textcolor{red}{\textbf{0.00}}/\textcolor{red}{\textbf{100.00}}/\textcolor{red}{\textbf{100.00}} & \textcolor{red}{\textbf{3.65}}/\textcolor{red}{\textbf{99.20}}/\textcolor{red}{\textbf{99.07}}
    \\
    % & 1.76/0.16/\textcolor{red}{-0.03} & 0.86/0.38/0.60 & 2.02/0.53/0.51 & 0.11/0.07/0.04 & 1.19/0.28/0.28 \\
    \hline
    \end{tabular}%
  }
  \label{tab:main_result_cifar10}%
  \vspace{-0cm}
\end{table*}%


\begin{table*}[bt]
  \centering
  \caption{\textbf{OOD detection results with CIFAR100 as in-distribution data.} 
%   OOD detection performance for various competitive/SotA methods and ours DFB when $\mathcal{D}_{in}$=CIFAR100.
The notations here are the same as that in Tab. \ref{tab:main_result_cifar10}.}
  \vspace{-0.1cm}
  \scalebox{1.0}{
    \begin{tabular}{p{3.8cm}ccccc}
    \hline
    \multirow{3}{*}{\textbf{Methods}} & \multicolumn{4}{c}{\textbf{OOD Datasets}} & \multirow{2}{*}{\textbf{Average}}\\
    \cline{2-5} 
    & \multicolumn{1}{c}{\textbf{CIFAR10}} & \multicolumn{1}{c}{\textbf{SVHN}} & \multicolumn{1}{c}{\textbf{Places365}} & \multicolumn{1}{c}{\textbf{Textures}} &  \\
    & \multicolumn{5}{c}{\textbf{FPR95}$\downarrow$ /\textbf{AUROC}$\uparrow$ /\textbf{AUPR}$\uparrow$} \\
    \hline
    MaxLogit \cite{HendrycksBMZKMS22} {\scriptsize \textcolor{gray}{[ICML'22]}}& 61.61/81.09/79.25 & 37.12/91.29/80.77 & 71.89/73.12/67.64 & 37.61/90.63/93.73 & 52.06/84.03/80.35 
    \\
    KL-Matching \cite{HendrycksBMZKMS22} {\scriptsize \textcolor{gray}{[ICML'22]}}& 64.49/79.54/74.46 & 47.86/89.08/76.63 & 73.55/78.04/76.61 & 46.63/88.97/92.16 & 58.13/83.91/79.96
    \\
    ReAct \cite{sun2021react} {\scriptsize \textcolor{gray}{[NIPS'21]}}& 70.81/79.62/78.97 & 53.00/88.88/78.43 & 82.64/68.11/63.28 & 52.80/88.15/92.58 & 64.81/81.19/78.31
    \\
    MaSF$^{\dagger}$ \cite{haroush2022a} {\scriptsize \textcolor{gray}{[ICLR'22]}}&  - /64.00/ -  &  - /96.90/ -  &  - /81.10/ -  &  - /92.00/ -  &  - /83.50/ - \\
    DML+$^{\dagger}$ \cite{Zhang_2023_CVPR} {\scriptsize \textcolor{gray}{[CVPR'23]}}&  79.35/76.69/ -  &  21.69/96.51/ -  &  68.31/83.31/ -  &  49.24/88.56/ -  &  54.65/86.27/ -
    \\
    \cline{1-6} 
    MSP \cite{hendrycks17baseline} {\scriptsize \textcolor{gray}{[ICLR'17]}}& 64.25/81.52/80.87 & 49.50/88.92/79.07 & 72.10/76.18/71.52 & 46.24/89.33/93.44 & 58.02/83.99/81.23 
    \\
    MSP-DFB {\scriptsize \textcolor{gray}{[Ours]}} & \textcolor{red}{58.76}/\textcolor{red}{84.67}/\textcolor{red}{84.25} & \textcolor{blue}{50.75}/\textcolor{red}{89.27}/\textcolor{red}{82.39} & \textcolor{red}{67.82}/\textcolor{red}{85.20}/\textcolor{red}{88.06} & \textcolor{red}{28.21}/\textcolor{red}{95.86}/\textcolor{red}{97.85} & \textcolor{red}{51.38}/\textcolor{red}{88.75}/\textcolor{red}{88.14}
    \\
    \hdashline
    % & 5.49/3.15/3.38 & \textcolor{red}{-1.25}/0.35/3.32 & 4.28/9.02/16.54 & 18.03/6.53/4.40 & 6.64/4.76/6.91 \\
    ODIN \cite{liang2018enhancing} {\scriptsize \textcolor{gray}{[ICLR'18]}}& 59.67/82.39/80.79 & 38.11/91.32/81.40 & 69.80/75.39/69.81 & 37.38/91.10/94.22 & 51.24/85.05/81.55
    \\
    ODIN-DFB {\scriptsize \textcolor{gray}{[Ours]}} & \textcolor{red}{55.92}/\textcolor{red}{87.31}/\textcolor{red}{88.02} & \textcolor{red}{32.79}/\textcolor{blue}{90.60}/\textcolor{blue}{76.03} & \textcolor{red}{55.34}/\textcolor{red}{81.56}/\textcolor{red}{79.62} & \textcolor{red}{10.78}/\textcolor{red}{97.40}/\textcolor{red}{98.23} & \textcolor{red}{38.71}/\textcolor{red}{89.22}/\textcolor{red}{85.48}
    \\
    % & 3.75/4.91/7.23 & 5.32/\textcolor{red}{-0.72}/\textcolor{red}{-5.37} & 14.46/6.17/9.82 & 26.60/6.29/4.01 & 12.53/4.16/3.92 \\
    \hdashline
    Energy \cite{liu2020energy} {\scriptsize \textcolor{gray}{[NIPS'20]}} & 64.34/80.48/78.89 & 36.76/91.38/80.98 & 74.75/72.14/67.10 & 39.17/90.37/93.61 & 53.75/83.59/80.15
    \\
    Energy-DFB {\scriptsize \textcolor{gray}{[Ours]}} & \textcolor{red}{\textbf{54.02}}/\textcolor{red}{\textbf{88.12}}/\textcolor{red}{\textbf{88.82}} & \textcolor{red}{24.78}/\textcolor{red}{93.39}/\textcolor{red}{81.94} & \textcolor{red}{48.87}/\textcolor{red}{85.72}/\textcolor{red}{82.67} & \textcolor{red}{7.11}/\textcolor{red}{98.41}/\textcolor{red}{98.92} & \textcolor{red}{33.70}/\textcolor{red}{91.41}/\textcolor{red}{88.09}
    \\
    % & 10.32/7.64/9.93 & 11.98/2.01/0.97 & 25.88/13.58/15.57 & 32.06/8.04/5.31 & 20.06/7.82/7.94 \\
    \hdashline
    ViM \cite{wang2022vim} {\scriptsize \textcolor{gray}{[CVPR'22]}} & 59.13/85.72/85.87 & 10.23/97.90/95.74 & 49.38/87.23/86.19 & 2.45/99.47/99.69 & 30.30/92.58/91.87 
    \\
    ViM-DFB {\scriptsize \textcolor{gray}{[Ours]}} & \textcolor{blue}{60.88}/\textcolor{red}{85.74}/\textcolor{red}{86.38} & \textcolor{red}{\textbf{7.58}}/\textcolor{red}{\textbf{98.40}}/\textcolor{red}{\textbf{96.39}} & \textcolor{red}{\textbf{20.93}}/\textcolor{red}{\textbf{96.06}}/\textcolor{red}{\textbf{96.23}} & \textcolor{red}{\textbf{0.16}}/\textcolor{red}{\textbf{99.96}}/\textcolor{red}{\textbf{99.98}} & \textcolor{red}{\textbf{22.39}}/\textcolor{red}{\textbf{95.04}}/\textcolor{red}{\textbf{94.74}}
    \\
    % & \textcolor{red}{-1.75}/0.03/0.50 & 2.65/0.49/0.66 & 28.45/8.83/10.03 & 2.29/0.49/0.29 & 7.91/2.46/2.87 \\
    \hline
    \end{tabular}%
  }
  \label{tab:main_result_cifar100}%
  \vspace{-0cm}
\end{table*}%

% \begin{table}[!htb]
%   \centering
%   \caption{Summary of OOD feature types for different OOD datasets.}
%   \vspace{-0.2cm}
%   \scalebox{0.86}{
%     \begin{tabular}{|p{1.0cm}cccc|}
%     \hline
%     \multicolumn{1}{|l}{\textbf{ID:CIFAR10}}   & \multicolumn{1}{|c}{\textbf{CIFAR100}} & \multicolumn{1}{c}{\textbf{SVHN}} &
%     \multicolumn{1}{c}{\textbf{Places365}} &
%     \multicolumn{1}{c|}{\textbf{Textures}}  \\ 
%     \hline
%     \multicolumn{1}{|l}{OOD semantic} & \multicolumn{1}{|c}{Strong}  &\multicolumn{1}{c}{Strong} &\multicolumn{1}{c}{Strong} &\multicolumn{1}{c|}{Strong}\\
    
%     \multicolumn{1}{|l}{OOD domain} & \multicolumn{1}{|c}{Weak} & \multicolumn{1}{c}{Strong} & \multicolumn{1}{c}{Strong} & \multicolumn{1}{c|}{Strong} \\ 
%      \hline
%     \end{tabular}%
%     }
%     \vspace{-0.2cm}
%   \label{tab:datasets}%
% \end{table}%
%\gs{Add a small table here to summarize the domain feature type of each OOD data: two columns -- OOD domain and OOD semantic, in addition to columns to fill up the ID and OOD datasets}

\noindent\textbf{Implementation Details.}
We use BiT-M \cite{kolesnikov2020big}, a variant of ResNetv2 architecture \cite{he2016identity}, as the default network backbone throughout the experiments. The official release checkpoint of BiT-M-R50x1 trained on ImageNet-21K is used as our initial $K$-class in-distribution classification model.
The model is further fine-tuned on the in-distribution dataset (CIFAR10/CIFAR100)
with 20,000 steps using a batch size of 128. SGD is used as the optimizer with an initial learning rate of 0.003 and a momentum of 0.9. 
We decay the learning rate by a factor of 10 at 30\%, 60\%, and 90\% of the training steps. 
All images were resized to 160x160 and randomly cropped to 128x128. 
The Mixup \cite{zhang2018mixup} with $\alpha = 0.1$ is also used to synthesize new image samples during training.
We subsequently use CAM (Class Activation Mapping) \cite{zhou2016learning} to generate the pseudo mask labels for each in-distribution image based on a multi-scale masking method used in \cite{ahn2019weakly}. 
With these pseudo mask labels, we then use a modified Dense-BiT architecture to train the $(K+1)$-class dense prediction model with the BiT-M-R50x1 checkpoints as the initial weights. 
All input images are resized to 128x128 during training and inference. We replace the Mixup augmentation used in the training with randomly scaling (from 0.5 to 2.0) and randomly horizontally flipping augmentation. The other training strategy and hyperparameters are maintained the same as the ones used in training the $K$-class classification network above. After that, the dense prediction model is converted to $(K+1)$-class image classification model using Eq. (\ref{eq:cls}).

Four \textit{post-hoc} OOD detection methods, including MSP \cite{hendrycks17baseline}, ODIN \cite{liang2018enhancing}, Energy \cite{liu2020energy}, and ViM \cite{wang2022vim}, are used as the plug-in base models. They are respectively employed to combine with DFB to detect OOD samples in both of foreground and background features. To have fair and straightforward comparison, these four plug-in models are built upon the same $K$-class classification model as DFB. The temperature $T=2.5$ is used in Eq. (\ref{eqn:combine}) by default.

We will release our code upon paper acceptance.

\noindent\textbf{Evaluation Metrics.}
We use three widely-used evaluation metrics for OOD detection, including: 1) \textbf{FPR95} that evaluates the false positive rate of the OOD samples when the true positive rate of the in-distribution samples is 95\%, 2) \textbf{AUROC} denotes the Area Under the Receiver Operating Characteristic curve, and 3) \textbf{AUPR} is the Area under the Precision-Recall curve. The ID images are the positive samples in calculating AUROC and AUPR to measure the OOD detection performance. In addition, we also report the \textbf{Top-1 accuracy} of classifying the in-distribution samples. 

\subsection{Main Results}

The OOD detection results of DFB and its competing methods with CIFAR10 and CIFAR100 as in-distribution data are reported in Tabs. \ref{tab:main_result_cifar10} and \ref{tab:main_result_cifar100}, respectively. Overall, DFB substantially improves four different SotA detection methods in all three evaluation metrics on both datasets, and obtains new SotA performance. We discuss the results in detail as follows.

% \vspace{0.1cm}
\noindent\textbf{Enhancing Different OOD Detection Methods.} 
Four different SotA methods -- MSP, ODIN, Energy, and ViM -- are used as foreground-based OOD detection baseline models and plugged into DFB to perform joint foreground and background OOD detection. Their results are shown at the bottom of Tabs.  \ref{tab:main_result_cifar10} and \ref{tab:main_result_cifar100}.

Compared to all the four plug-in base models, DFB can significantly improve the performance of all evaluation metrics in terms of the average results over the four OOD datasets on both of the CIFAR10 and CIFAR100 datasets. In particular, for the averaged improvement across the four base models, DFB boosts the FPR95 by $10.38\%$, the AUROC by $3.92\%$ and the AUPR by $6.96\%$ AUPR in Tab. \ref{tab:main_result_cifar10}; and similarly, it boosts the FPR95 by $11.78\%$ , the AUROC by $4.8\%$ and the AUPR by $5.41\%$ in Tab. \ref{tab:main_result_cifar100}. 
Note that even for the base model ViM, the most recent SotA method, DFB can still considerably enhance its performance, especially on some datasets where ViM does not work well, such as the SVHN, Places365, and Textures datasets, resulting in over 6\% reduction in FPR95 and 2.5\% increase in both AUROC and AUPR on the CIFAR100 data. 
DFB shows slight performance drops on some metrics of CIFAR and SVHN OOD datasets, which are mainly caused by suboptimal combinations of the foreground and background OOD scores with the default temperature setting. The explanation would be discussed in detail using Fig. \ref{fig:T} in Sec. \ref{subsec:ablation}.

\begin{figure}[t] 
    \centering
    \vspace{-0.0cm}
    \includegraphics[width=0.95\linewidth]{fig/Analyse_Scores.pdf}
    \vspace{-0.3cm}
    \caption{Distribution of the foreground/background OOD scores of ID (CIFAR10/100) and OOD samples (Textures) in DFB.
    % In DFB, the domain-feature-based OOD scores enable significantly better ID and OOD separation than the semantic-feature-based OOD scores.
    % With the OOD dataset inclined to domain distribution bias, Domain scores from image background significantly outperform common semantic scores (Energy\cite{liu2020energy}). 
    %\gs{In: Cifar10 -> ID: CIFAR10; In: Cifar100 -> ID: CIFAR100; In-Distribution -> In-distribution; Out-of-Distribution -> Out-of-distribution}
    }
    \label{fig:domain_score}
    \vspace{-0.0cm}
\end{figure}
\begin{figure}[t] 
    \centering
    \includegraphics[width=0.95\linewidth]{fig/TSNE.pdf}
    \vspace{-0.3cm}
    \caption{t-SNE visualization of the features learned by the vanilla classification network and DFB, where the colored dots are ID samples of different classes, and the black $\times$ are OOD samples.}
    % After learning the latent domain features, DFB successfully distinguishes the features of OOD samples from ID samples.}
    \label{fig:tsne}
    \vspace{-0.2cm}
\end{figure}

% \vspace{0.1cm}
\noindent\textbf{Comparison to SotA Methods.}
DFB is also compared with five very recent SotA methods, including MaxLogit \cite{HendrycksBMZKMS22}, KL-Matching \cite{HendrycksBMZKMS22}, ReAct \cite{sun2021react}, MaSF \cite{haroush2022a} and DML+ \cite{Zhang_2023_CVPR} %\gs{please include references for each method}
, with their results reported at the top of Tabs.  \ref{tab:main_result_cifar10} and \ref{tab:main_result_cifar100}. Among all our four DFB methods and the SotA methods, ViM-DFB is consistently the best performer except the CIFAR10 data in Tab. \ref{tab:main_result_cifar100} where Energy-DFB is the best detector. This is mainly because the ViM is generally the best semantic-feature-based OOD scoring method, and DFB can perform better when the plug-in base model is stronger. Further, it is impressive that although the base models MSP, ODIN and Energy that largely underperform the SotA competing methods, DFB can significantly boost their performance and outperform these SotA competing methods on nearly all cases in Tabs. \ref{tab:main_result_cifar10} and \ref{tab:main_result_cifar100}. 
% To the best of our knowledge, they are the current competitive or SotA OOD detection methods.


% \vspace{0.1cm}
\noindent\textbf{The Reasons behind the Effectiveness of DFB.} We aim to understand the effectiveness of DFB from two perspectives, including the foreground and background OOD scoring, and the latent features learned in DFB, with the results on the Textures dataset reported in Figs. \ref{fig:domain_score} and \ref{fig:tsne} respectively. We can see in Fig. \ref{fig:domain_score} that the background OOD scores in DFB enable a significantly better ID and OOD separation than the foreground OOD scores, indicating that the ID and OOD samples can be easier to be separated by looking from the background features than the semantic features since there can be more background differences than the foreground ones in each ID/OOD image. From the feature representation perspective, compared to the features learned in the vanilla $K$-class classifier in Fig. \ref{fig:tsne} (left), the features learned by the $(K+1)$-class classifier in DFB (Fig. \ref{fig:tsne} (right)) are more discriminative in distinguishing OOD samples from ID samples, which demonstrates that the classifier can learn better ID representation after disentangling foreground and background features.
%. This may be due to that training the classifier using fine-grained pixel-wise class labels helps learn better semantic features, in addition to the learning of the domain features.
% the domain features can be more discriminative and supportive than the semantic features for OOD detection.



\subsection{Ablation Study}\label{subsec:ablation}



\begin{table}[tb]
  \centering
  \vspace{-0.0cm}
  \caption{FPR95 Results of DFB and its variants.}
%   Best results are \textbf{boldfaced}.}
  \vspace{-0.2cm}
  \scalebox{0.9}{
    \begin{tabular}{p{0.15cm}p{1.0cm}|c|cc|cc}
    \hline
    \multicolumn{2}{l}{\textbf{Module}}   & \multicolumn{1}{|c}{\textbf{BG}}& 
    \multicolumn{1}{|c}{\textbf{Energy}} &
    \multicolumn{1}{c}{\textbf{Energy-DFB}} &
    \multicolumn{1}{|c}{\textbf{ViM}} &
    \multicolumn{1}{c}{\textbf{ViM-DFB}}  \\ 
    \hline
    \multicolumn{2}{l}{$S_h$} & \multicolumn{1}{|c}{} &\multicolumn{1}{|c}{$\checkmark$} &\multicolumn{1}{c}{$\checkmark$} &\multicolumn{1}{|c}{$\checkmark$} &\multicolumn{1}{c}{$\checkmark$}\\
    
    % \multicolumn{2}{l}{K+1CLS} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\checkmark$} & \multicolumn{1}{c}{$\checkmark$} \\ 
    
    \multicolumn{2}{l}{$S_b$} & \multicolumn{1}{|c}{$\checkmark$}& \multicolumn{1}{|c}{} & \multicolumn{1}{c}{$\checkmark$}  & \multicolumn{1}{|c}{} & \multicolumn{1}{c}{$\checkmark$} \\
    
    % \multicolumn{2}{l}{DOM} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\checkmark$} & \multicolumn{1}{c}{$\checkmark$} \\
     \hline
     %   \multicolumn{6}{c}{\textbf{FPR95}$\downarrow$ /\textbf{AUROC}$\uparrow$}\\ 
    \multirow{5}[0]{*}{\rotatebox{90}{\textbf{CIFAR10}}} & \multicolumn{1}{l|}{\textbf{CIFAR100}}& 38.16 & 41.98 & 19.90 & 15.25 & \textbf{13.49} \\
    & \multicolumn{1}{l|}{\textbf{SVHN}} &  2.60 & 19.73 & 3.10 & 1.27&  \textbf{0.41} \\
    & \multicolumn{1}{l|}{\textbf{Places365}} &4.40 & 25.42 & 6.96 &  2.74 & \textbf{0.72} \\
    & \multicolumn{1}{l|}{\textbf{Textures}} &0.04 & 8.72 & 0.53 &  0.11& \textbf{0.00} \\
    & \multicolumn{1}{l|}{\textbf{Average}} &11.30 & 23.96 & 7.62 &  4.84& \textbf{3.65} \\
    \hline
    \multirow{5}[0]{*}{\rotatebox{90}{\textbf{CIFAR100}}} & \textbf{CIFAR10} & 89.24 & 64.34 & \textbf{54.02} & 59.13  & 60.88 \\
    & \multicolumn{1}{l|}{\textbf{SVHN}} & 26.61 & 36.76 & 24.7  & 10.23&  \textbf{7.58}\\
    & \multicolumn{1}{l|}{\textbf{Places365}} & 26.55 & 74.75 & 48.87 & 49.38 & \textbf{20.93} \\
    & \multicolumn{1}{l|}{\textbf{Textures}} & 0.53  & 39.17 & 7.11 & 2.45 & \textbf{0.16} \\
    & \multicolumn{1}{l|}{\textbf{Average}} & 35.73  & 53.75 & 33.70 & 30.30 &  \textbf{22.39} \\
    \hline
    \end{tabular}%
    }
    \vspace{-0.3cm}
  \label{tab:ablation}%
\end{table}%

\noindent\textbf{Background OOD Score $S_b$ and the Joint OOD Score $S$.} Tab. \ref{tab:ablation} shows the FPR95 results of OOD scoring methods in our model, including the use of background OOD scores $S_b$ only (BG), foreground OOD scores $S_h$ (Energy and ViM are used), and the full DFB model.
Compared to the two semantic OOD scoring methods, Energy and ViM, using only the background OOD scoring $S_b$ in DFB can obtain significantly reduced FPR95 errors, especially 
% baseline methods, DOM-only is superior in
on OOD benchmarks such as Places356 and Textures where significant background differences are presented compared to the in-distribution background. This demonstrates that DFB can effectively learn the in-distribution background features that can be used to detect OOD samples from the background aspect. Nevertheless, BG works less effectively on the benchmark CIFAR100 vs. CIFAR10 where the background difference is weak and detecting OOD samples rely more on the foreground features. 
In such cases, the full DFB models -- Energy-DFB and ViM-DFB -- that synthesize semantic OOD scores $S_h$ and background OOD scores $S_b$ are needed; they significantly outperform the separate foreground/background OOD scoring methods across the datasets.
% . With the inclusion of $S_b$, the joint scores significantly outperformed the baseline methods using only $S_h$ on all benchmarks. The detailed OOD score ablation can be found in appendix. \cb{Does this paragraph repeat what was in section 4.2?}

%MSP-DFB significantly outperforms baseline MSP on all benchmarks but is inferior to the more tailored SEM-only or DOM-only on individual benchmarks. The joint score generally achieves globally suboptimal results, showing robustness to different scenarios.
%We separate the semantic and domain scores in DFB into variants SEM-only and DOM-only. Compared to the baseline method MSP, X-SEM uses identical architecture and method to derive semantic OOD scores, but X-SEM is based on a dense retraining network consistent with DFB, rather than the default classification network.


% \vspace{0.1cm}
\noindent\textbf{Temperature $T$ in Synthesizing Foreground and Background OOD Scores.} One key challenge in plugging existing foreground OOD scores into DFB in Eq. (\ref{eqn:combine}) is the diverse range of different foreground OOD scores yielded by the existing methods. Fig. \ref{fig:T} the variants of DFB of using different temperature $T$ values to study the effects. We can observe that the performance of all methods in CIFAR100 vs. CIFAR10 gradually improves as the temperature increases. This is because the increase of $T$ narrows down the distribution of background scores, thus making the final OOD scores emphasizing more on the foreground OOD scores, which are more effective in the OOD datasets like CIFAR100 (ID) vs. CIFAR10 (OOD) where the background difference is very small. In contrast, the performance of all methods in CIFAR100 (ID) vs. Places365 (OOD) gradually decreases as the temperature increases. This is because the background distribution difference dominates over the foreground difference in such cases, on which enlarging the distribution of background OOD  scores is more effective. $T=2.5$ is generally a good trade-off of the foreground and background OOD scores, and it is thus used by default in DFB.
% In practice, it is possible to detect domain distribution gap OOD samples by lower temperature T and semantic distribution gap OOD samples by higher T. 
Note that adjusting $T$ generally does not bring the overall performance down below the baseline performance, showing the effectiveness of DFB using different $T$ values.


% \begin{table}[bt]
%   \centering
%   \caption{Comparison of DFB and outlier exposure (OE). $^{\dagger}$ indicates that the results are taken from the original paper, and other methods share the same architecture. Reported results are averaged over the results on the four OOD datasets.}
%   %\gs{please add the results of MSP without using outlier data as the baseline results.}}
% %   Best results are highlighted.}
%   \vspace{0.1cm}
%   \scalebox{0.88}{
%     \begin{tabular}{p{2.2cm}|cc|cc}
%     \hline
%     \multirow{2}{*}{\textbf{Methods}}  &  \multicolumn{2}{c|}{\textbf{ID: CIFAR10}} & \multicolumn{2}{c}{\textbf{ID: CIFAR100}}\\
%     \cline{2-5} 
%     & \multicolumn{1}{c}{\textbf{FPR95}$\downarrow$} & \multicolumn{1}{c|}{\textbf{AUROC}$\uparrow$} & 
%     \multicolumn{1}{c}{\textbf{FPR95}$\downarrow$} & 
%     \multicolumn{1}{c}{\textbf{AUROC}$\uparrow$}\\
%     \hline
%     Baseline & 33.44 & 89.01 & 64.25 & 81.52 \\
%     %  MSP \cite{hendrycks17baseline} & 20.46 & 93.83 & 58.02 & 83.99  \\
%      OE \cite{hendrycks2018deep} & 20.16 & 93.74 & 57.68 & 83.98  \\
%      OE$^{\dagger}$ \cite{hendrycks2018deep} & 15.57 & 96.40 & 52.30 & 83.47  \\
%      DFB & \textbf{7.87} & \textbf{98.04} & \textbf{42.29} & \textbf{91.03}  \\
%      \hline
%     \end{tabular}%
%     }
%   \label{tab:OE}%
%   \vspace{-0.2cm}
% \end{table}%

\subsection{Further Analysis of DFB}


% \vspace{0.1cm}
\noindent\textbf{In-distribution Classification Accuracy.} A potential risk of modifying the primitive classification network for OOD detection is the large degradation of the in-distribution classification accuracy. 
As shown in Tab. \ref{tab:accuracy}, our proposed DFB does not have this issue, as DFB has only 0.12\% top-1 accuracy drop on the CIFAR10 dataset and improves the classification performance by 0.23\% on the CIFAR100 dataset. This result indicates that the dense prediction training in DFB ensures effective learning of foreground features, while learning the background features. The 0.23\% accuracy increase on CIFAR100 also indicates that the dense prediction task can also improve the foreground feature learning for in-distribution classification.
% Dense prediction training on more challenging datasets like CIFAR100 can help the network learn more accurate information by separating semantic and domain features.

\begin{table}[tb]
  \centering
  \caption{Top-1 accuracy results of in-distribution classification. Vanilla is the primitive trained classification network $\phi$ in Sec. \ref{subsec:domainfeature}.}
  \vspace{-0.3cm}
  \scalebox{0.95}{
    \begin{tabular}{p{1.0cm}cc}
    \hline
    \multicolumn{1}{l}{\textbf{Method}}   & \multicolumn{1}{|c}{\textbf{CIFAR10}} & \multicolumn{1}{c}{\textbf{CIFAR100}} \\ 
    \hline
    \multicolumn{1}{l}{Vanilla} & \multicolumn{1}{|c}{97.25\%}  &\multicolumn{1}{c}{85.94\%} \\
    
    \multicolumn{1}{l}{DFB} & \multicolumn{1}{|c}{97.13\%} & \multicolumn{1}{c}{86.17\%} \\ 
     \hline
    \end{tabular}%
    }
    \vspace{-0.0cm}
  \label{tab:accuracy}%
\end{table}%


\begin{figure}[t] 
    \centering
    \includegraphics[width=0.95\linewidth]{fig/Ablation_Temperature.pdf}
    \vspace{-0.2cm}
    \caption{AUROC results of DFB using varying $\mathit{T}$ settings.}
    % The dashed lines show the baseline results of the corresponding OOD scoring function.}
    \label{fig:T}
    \vspace{-0.2cm}
\end{figure}


% \vspace{0.1cm}
\noindent\textbf{Extending to Large-scale Semantic Space.}
Another challenge for OOD detection is on datasets with a large number of ID classes and high-resolution images, \eg, ImageNet-1k \cite{deng2009imagenet}.
Fig. \ref{fig:imagenet} presents the detection performance of DFB using ImageNet-1k as in-distribution dataset and on four OOD datasets, including two new high resolution datasets, ImageNet-O \cite{hendrycks2021natural} and SUN \cite{xiao2010sun}. To examine the impact of the number of classes, we show the results using $\mathit{C}\in \{200, 300, 500, 1000\}$ randomly selected ID classes from ImageNet-1k ($C=1000$ is the full ImageNet-1k data). 
% For each $\mathit{C}$, we randomly sampled the corresponding number of classes in ImageNet-1k as the training set. 
% Refer to Appendix A for details of the experiments.
The results show that DFB can consistently and significantly outperform its base model Energy with increasing number of ID classes on four diverse OOD datasets, indicating the effectiveness of DFB working in large-scale semantic space. On the other hand, as expected, both Energy and DFB are challlenged by the large semantic space, and thus, their performance decreases with more ID classes. Extending to large-scale semantic space is a general challenge for existing OOD detectors. We leave it for future work.
% although the DFB performance decreases as the number of in-distribution categories rises, it always maintains a stable boost to baseline method. It shows that expanding the in-distribution semantic space does not cause DFB to fail, which sufficiently proves the robustness of DFB.
\begin{figure}[t] 
    \centering
    \vspace{-0.0cm}
    \includegraphics[width=0.90\linewidth]{fig/Large_scale_exp.pdf}
    \vspace{-0.1cm}
    \caption{AUROC results of DFB and the Energy baseline in large-scale semantic space using ImageNet-1k as ID data.}
    % The dashed lines show the baseline results of the corresponding OOD scoring function.}
    \label{fig:imagenet}
    \vspace{-0.0cm}
\end{figure}

\noindent\textbf{Analysis of Pseudo Mask Quality.} We explore the impact of various qualities of pseudo-masks on the overall efficacy of DFB during the pseudo-mask generation phase. Experiments show that the quality of pseudo-masks exerts a minimal effect on the final performance. Specifically, Table \ref{tab:camgradcam} presents a comparative assessment of masks generated using Grad-CAM\cite{selvaraju2017grad} versus those created using CAM\cite{zhou2016learning}, with both methods demonstrating comparable results. 

\begin{table}[bt]
  \centering
  \caption{AUROC results using CAM and Grad-CAM for mask generation.}
  % \vspace{-0.2cm}
  \scalebox{0.8}{
    \begin{tabular}{p{2.0cm}|ccccc}
    \hline
    \textbf{ID:CIFAR100}  &  \multicolumn{1}{c}{\textbf{CIFAR10}} & \multicolumn{1}{c}{\textbf{SVHN}}& \multicolumn{1}{c}{\textbf{Places365}}& \multicolumn{1}{c}{\textbf{Textures}}& \multicolumn{1}{c}{\textbf{Average}}\\
    \hline
     CAM  & 86.26 & 92.92 & 87.14 & 97.91 & 91.10\\
     Grad-CAM  &  84.54 & 92.83 & 88.78 & 97.22 & 90.84 \\
     \hline
    \end{tabular}%
    }
  \label{tab:camgradcam}%
  % \vspace{-0.2cm}
\end{table}%


\section{Conclusions}
In this paper, we reveal the importance of disentangling foreground and background features in out-of-distribution detection and introduce background features for OOD detection that are neglected in current approaches. 
We further propose a novel OOD detection framework DFB that utilizes dense prediction networks to segment the foreground and background from in-distribution training data, and jointly learn foreground and background features.
It then leverages these background features to define background OOD scores and seamlessly combines them with existing foreground-based OOD methods to detect OOD samples from both foreground and background aspects. Comprehensive results on popular OOD benchmarks with diverse background features show that DFB can significantly improve the detection performance of four different existing methods. Through this work, we promote the design of OOD detection algorithms to achieve more holistic OOD detection in real-world applications. In our future work, we plan to exploit the background features for zero/few-shot OOD detection through outlier label-based hard prompts  \cite{ding2024zero} or learnable soft prompts \cite{li2024negprompt} to large pre-trained vision-language models. 

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\newpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

%%
%% If your work has an appendix, this is the place to put it.
% \appendix

% \section{Research Methods}

% \subsection{Part One}

% Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi
% malesuada, quam in pulvinar varius, metus nunc fermentum urna, id
% sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum
% vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et
% lacinia dolor. Integer ultricies commodo sem nec semper.

% \subsection{Part Two}

% Etiam commodo feugiat nisl pulvinar pellentesque. Etiam auctor sodales
% ligula, non varius nibh pulvinar semper. Suspendisse nec lectus non
% ipsum convallis congue hendrerit vitae sapien. Donec at laoreet
% eros. Vivamus non purus placerat, scelerisque diam eu, cursus
% ante. Etiam aliquam tortor auctor efficitur mattis.

% \section{Online Resources}

% Nam id fermentum dui. Suspendisse sagittis tortor a nulla mollis, in
% pulvinar ex pretium. Sed interdum orci quis metus euismod, et sagittis
% enim maximus. Vestibulum gravida massa ut felis suscipit
% congue. Quisque mattis elit a risus ultrices commodo venenatis eget
% dui. Etiam sagittis eleifend elementum.

% Nam interdum magna at lectus dignissim, ac dignissim lorem
% rhoncus. Maecenas eu arcu ac neque placerat aliquam. Nunc pulvinar
% massa et mattis lacinia.

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
