{
    "arxiv_id": "2303.08401",
    "paper_title": "Implicit Ray-Transformers for Multi-view Remote Sensing Image Segmentation",
    "authors": [
        "Zipeng Qi",
        "Hao Chen",
        "Chenyang Liu",
        "Zhenwei Shi",
        "Zhengxia Zou"
    ],
    "submission_date": "2023-03-15",
    "revised_dates": [
        "2023-08-09"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.AI"
    ],
    "abstract": "The mainstream CNN-based remote sensing (RS) image semantic segmentation approaches typically rely on massive labeled training data. Such a paradigm struggles with the problem of RS multi-view scene segmentation with limited labeled views due to the lack of considering 3D information within the scene. In this paper, we propose ''Implicit Ray-Transformer (IRT)'' based on Implicit Neural Representation (INR), for RS scene semantic segmentation with sparse labels (such as 4-6 labels per 100 images). We explore a new way of introducing multi-view 3D structure priors to the task for accurate and view-consistent semantic segmentation. The proposed method includes a two-stage learning process. In the first stage, we optimize a neural field to encode the color and 3D structure of the remote sensing scene based on multi-view images. In the second stage, we design a Ray Transformer to leverage the relations between the neural field 3D features and 2D texture features for learning better semantic representations. Different from previous methods that only consider 3D prior or 2D features, we incorporate additional 2D texture information and 3D prior by broadcasting CNN features to different point features along the sampled ray. To verify the effectiveness of the proposed method, we construct a challenging dataset containing six synthetic sub-datasets collected from the Carla platform and three real sub-datasets from Google Maps. Experiments show that the proposed method outperforms the CNN-based methods and the state-of-the-art INR-based segmentation methods in quantitative and qualitative metrics.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.08401v1"
    ],
    "publication_venue": null,
    "doi": "10.1109/TGRS.2023.3285659"
}