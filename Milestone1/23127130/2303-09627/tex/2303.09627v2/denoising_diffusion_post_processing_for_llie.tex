% IEEEtran V1.7 and later provides for these CLASSINPUT macros to allow the
% user to reprogram some IEEEtran.cls defaults if needed. These settings
% override the internal defaults of IEEEtran.cls regardless of which class
% options are used. Do not use these unless you have good reason to do so as
% they can result in nonIEEE compliant documents. User beware. ;)
%
%\newcommand{\CLASSINPUTbaselinestretch}{1.0} % baselinestretch
%\newcommand{\CLASSINPUTinnersidemargin}{1in} % inner side margin
%\newcommand{\CLASSINPUToutersidemargin}{1in} % outer side margin
%\newcommand{\CLASSINPUTtoptextmargin}{1in}   % top text margin
%\newcommand{\CLASSINPUTbottomtextmargin}{1in}% bottom text margin




%
% \documentclass[10pt,journal]{IEEEtran}
% \documentclass[11pt, draftcls]{IEEEtran}
\documentclass[10pt, twocolumn, twoside]{IEEEtran}
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}


% For Computer Society journals, IEEEtran defaults to the use of 
% Palatino/Palladio as is done in IEEE Computer Society journals.
% To go back to Times Roman, you can use this code:
%\renewcommand{\rmdefault}{ptm}\selectfont


% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)



% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % The IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  \graphicspath{{../figures/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%\usepackage{acronym}
% acronym.sty was written by Tobias Oetiker. This package provides tools for
% managing documents with large numbers of acronyms. (You don't *have* to
% use this package - unless you have a lot of acronyms, you may feel that
% such package management of them is bit of an overkill.)
% Do note that the acronym environment (which lists acronyms) will have a
% problem when used under IEEEtran.cls because acronym.sty relies on the
% description list environment - which IEEEtran.cls has customized for
% producing IEEE style lists. A workaround is to declared the longest
% label width via the IEEEtran.cls \IEEEiedlistdecl global control:
%
% \renewcommand{\IEEEiedlistdecl}{\IEEEsetlabelwidth{SONET}}
% \begin{acronym}
%
% \end{acronym}
% \renewcommand{\IEEEiedlistdecl}{\relax}% remember to reset \IEEEiedlistdecl
%
% instead of using the acronym environment's optional argument.
% The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/acronym


%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/pkg/mdwtools


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/pkg/eqparbox




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix


%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.


% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% Basically, \url{my_url_here}.


% NOTE: PDF hyperlink and bookmark features are not required in IEEE
%       papers and their use requires extra complexity and work.
% *** IF USING HYPERREF BE SURE AND CHANGE THE EXAMPLE PDF ***
% *** TITLE/SUBJECT/AUTHOR/KEYWORDS INFO BELOW!!           ***
\newcommand\MYhyperrefoptions{bookmarks=true,bookmarksnumbered=true,
pdfpagemode={UseOutlines},plainpages=false,pdfpagelabels=true,
colorlinks=true,linkcolor={black},citecolor={black},urlcolor={black},
pdftitle={My PDF title},%<!CHANGE!
pdfsubject={Typesetting},%<!CHANGE!
pdfauthor={Savvas Panagiotou},%<!CHANGE!
pdfkeywords={Computer Society, IEEEtran, journal, LaTeX, paper,
             template}}%<^!CHANGE!
\ifCLASSINFOpdf
\usepackage[\MYhyperrefoptions,pdftex]{hyperref}
\else
\usepackage[\MYhyperrefoptions,breaklinks=true,dvips]{hyperref}
\usepackage{breakurl} 
\fi
% One significant drawback of using hyperref under DVI output is that the
% LaTeX compiler cannot break URLs across lines or pages as can be done
% under pdfLaTeX's PDF output via the hyperref pdftex driver. This is
% probably the single most important capability distinction between the
% DVI and PDF output. Perhaps surprisingly, all the other PDF features
% (PDF bookmarks, thumbnails, etc.) can be preserved in
% .tex->.dvi->.ps->.pdf workflow if the respective packages/scripts are
% loaded/invoked with the correct driver options (dvips, etc.). 
% As most IEEE papers use URLs sparingly (mainly in the references), this
% may not be as big an issue as with other publications.
%
%
% The advanced features offer by hyperref.sty are not required for IEEE
% submission, so users should weigh these features against the added
% complexity of use.
% The package options above demonstrate how to enable PDF bookmarks
% (a type of table of contents viewable in Acrobat Reader) as well as
% PDF document information (title, subject, author and keywords) that is
% viewable in Acrobat reader's Document_Properties menu. PDF document
% information is also used extensively to automate the cataloging of PDF
% documents. The above set of options ensures that hyperlinks will not be
% colored in the text and thus will not be visible in the printed page,
% but will be active on "mouse over". USING COLORS OR OTHER HIGHLIGHTING
% OF HYPERLINKS CAN RESULT IN DOCUMENT REJECTION BY THE IEEE, especially if
% these appear on the "printed" page. IF IN DOUBT, ASK THE RELEVANT
% SUBMISSION EDITOR. You may need to add the option hypertexnames=false if
% you used duplicate equation numbers, etc., but this should not be needed
% in normal IEEE work.
% The latest version of hyperref and its documentation can be obtained at:
% http://www.ctan.org/pkg/hyperref


\usepackage{xcolor}
\newcommand\myworries[1]{\textcolor{red}{#1}}
% \renewcommand\myworries[1]{}



% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage[capitalise]{cleveref}
% \usepackage{background}
% \backgroundsetup{opacity=0.2, scale=20}
\usepackage{siunitx}
\usepackage{numprint}
\sisetup{
  table-auto-round = true % Round numbers in S-columns
}
\usepackage{multirow}
\usepackage{lipsum}
\usepackage{orcidlink}
\usepackage{tikz}
\usetikzlibrary{positioning, spy}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Denoising Diffusion Post-Processing for Low-Light Image Enhancement}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs

\author{Savvas~Panagiotou \orcidlink{0009-0009-6398-8427},~and~Anna~S.~Bosman \orcidlink{0000-0003-3546-1467},~\IEEEmembership{Member,~IEEE}% <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem S. Panagiotou and A.S. Bosman are with the Department of Computer Science, University of Pretoria, Pretoria, Gauteng, South Africa. \protect\\
% was with the Department
% of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
% GA, 30332.
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail:~savva.panagiotou@gmail.com,~anna.bosman@up.ac.za}% <-this % stops a space
% \thanks{Manuscript received April 19, 2005; revised August 26, 2015.}
}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. 



% The paper headers
% Commented these out for review
% \markboth{IEEE Transactions on Image Processing,~Vol.~??, No.~?, March~2023}%
% {Panagiotou \MakeLowercase{\textit{et al.}}: Denoising Diffusion Post-Processing for Low-Light Image Enhancement}


% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society journal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}

Low-light image enhancement (LLIE) techniques attempt to increase the visibility of images captured in low-light scenarios. However, as a result of enhancement, a variety of image degradations such as noise and color bias are revealed. Furthermore, each particular LLIE approach may introduce a different form of flaw within its enhanced results. To combat these image degradations, post-processing denoisers have widely been used, which often yield oversmoothed results lacking detail. We propose using a diffusion model as a post-processing approach, and we introduce Low-light Post-processing Diffusion Model (LPDM) in order to model the conditional distribution between under-exposed and normally-exposed images. We apply LPDM in a manner which avoids the computationally expensive generative reverse process of typical diffusion models, and post-process images in one pass through LPDM. Extensive experiments demonstrate that our approach outperforms competing post-processing denoisers by increasing the perceptual quality of enhanced low-light images on a variety of challenging low-light datasets. Source code is available at \url{https://github.com/savvaki/LPDM}.


\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Diffusion model, denoising, low-light image enhancement, post-processing
\end{IEEEkeywords}}


% make the title area
\maketitle

% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when compsoc mode
% is not selected <OR> if conference mode is selected - because compsoc
% conference papers position the abstract like regular (non-compsoc)
% papers do!
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc under a non-conference mode.


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\ifCLASSOPTIONcompsoc
\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
\else
\section{Introduction}
\label{sec:introduction}
\fi

% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\IEEEPARstart{T}{he} task of low-light image enhancement (LLIE) aims to improve the visibility of images which are captured under low-light conditions. Under-exposed images are often degraded in a variety of ways in addition to their lack of visibility. Notably, low-light regions of an image typically contain degraded color information, a lack of detail as well as intensive noise. LLIE techniques aim to brighten low-light regions of an image while maintaining color accuracy and minimizing noise. The demand for brightening and enhancing low-light images often arises due to many downstream algorithms only being performant on images with high-visibility \cite{ref:LIME}. Some of these downstream tasks include object detection~\cite{ref:low-light-object-detection}, facial recognition~\cite{ref:ve-lol}, surveillance~\cite{ref:llnet-2016} and semantic segmentation~\cite{ref:sgz}.

Simply adjusting the contrast of low-light images using a technique such as histogram equalization \cite{ref:ahe1} is often insufficient due to the amplification of noise \cite{ref:LIME,ref:learning-to-see}. Learning-based methods have emerged which significantly outperform traditional methods. However, even the state-of-the-art deep learning (DL) techniques still introduce a variety of artifacts in different scenarios \cite{ref:kind++}.

Existing denoising techniques can be applied to denoise low-light images either before or after contrast enhancement \cite{ref:deep-ll-post-denoiser, ref:pre-denoising-then-contrast}. These denoising techniques range from low-pass filters and algorithms such as block matching and 3D filtering (BM3D) \cite{ref:bm3d}, to state-of-the-art DL denoisers~\cite{ref:deep-ll-post-denoiser, ref:nafnet-denoiser}. Despite denoisers significantly reducing noise, they often introduce blurriness into the denoised output. As a result, removing the amplified noise in a brightened low-light image often comes at the cost of removing detail, especially in high-frequency regions of the image.

We propose a post-processing conditional diffusion model (DM) \cite{ref:diffusion:ddpm} with the capability of removing unwanted noise and other distortions in brightened low-light images. We name our conditional model \textbf{L}ow-light \textbf{P}ost-processing \textbf{D}iffusion \textbf{M}odel (LPDM). The effect of post-processing using LPDM is displayed in \cref{fig:intro}. Our technique is able to avoid the computationally expensive generative diffusion reverse process and denoise a given image in one pass through the model. Furthermore, LPDM is often able to improve the sharpness and color accuracy of the enhanced image. In summary, our contributions are as follows:
\begin{enumerate}
  \item We introduce a method of applying DMs as a post-processing technique in the LLIE pipeline. Our framework is able to circumvent the computationally expensive iterative reverse process of DMs and denoise images in one pass through the model. 
  \item We demonstrate that our DM improves existing state-of-the-art LLIE techniques on popular low-light datasets including challenging unpaired test sets. 
  \item In addition to simple denoising, we demonstrate that our method is able to cope with a variety of different artifacts and color distortions, yielding superior results to existing denoisers for LLIE.
\end{enumerate}

The remainder of this paper is structured as follows: \cref{sec:related-llie} provides background information on LLIE;  \cref{sec:related-diffusion-models} provides background information on DMs; \cref{sec:methodology} outlines preliminary mathematical notation and describes the proposed framework in detail; \cref{sec:experiment} contains the experimental setup and results for this work, including an ablation study; finally, conclusions are drawn in \cref{sec:conclusion}.


% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
% However, the Computer Society has been known to put floats at the bottom.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.

% \section{Related Work}
\begin{figure}[t!]
  \setlength\tabcolsep{2pt}%%
  \tiny
  \centering
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{ccc}
   Input &
   BIMEF \cite{ref:bimef }&
   BIMEF + LPDM (Ours) \\
   \includegraphics[width=1in]{figures/front-page/dark/BIMEF/04}
  &

   \begin{tikzpicture}
    \begin{scope}[
      node distance = 1mm,
          inner sep = 0pt,spy using outlines={rectangle, red, magnification=2,}
                          ]
    \node (n0)  {\includegraphics[width=1in]{figures/front-page/undarken/BIMEF/04}};

    \spy [blue,size=1cm] on (5pt,-15pt) in node[below right=of n0.north west];
    \end{scope}
  \end{tikzpicture}

  %  \includegraphics[width=1in]{figures/front-page/undarken/BIMEF/04}
   
   &
    
   \begin{tikzpicture}
    \begin{scope}[
      node distance = 1mm,
          inner sep = 0pt,spy using outlines={rectangle, red, magnification=2,}
                          ]
    \node (n0)  {\includegraphics[width=1in]{figures/front-page/denoise/BIMEF/04}};

    \spy [blue,size=1cm] on (5pt,-15pt) in node[below right=of n0.north west];
    \end{scope}
  \end{tikzpicture}

   \\
   Input&
   LIME \cite{ref:LIME} &
   LIME + LPDM (Ours) \\
   \includegraphics[width=1in]{figures/front-page/dark/LIME/06} &

   
   \begin{tikzpicture}
    \begin{scope}[
      node distance = 1mm,
          inner sep = 0pt,spy using outlines={rectangle, red, magnification=2,}
                          ]
    \node (n0)  {\includegraphics[width=1in]{figures/front-page/undarken/LIME/06}};

    \spy [blue,size=1cm] on (-5pt,15pt) in node[above left=of n0.south east];
    \end{scope}
  \end{tikzpicture}
   
   &
   
   \begin{tikzpicture}
    \begin{scope}[
      node distance = 1mm,
          inner sep = 0pt,spy using outlines={rectangle, red, magnification=2,}
                          ]
    \node (n0)  {\includegraphics[width=1in]{figures/front-page/denoise/LIME/06}};

    \spy [blue,size=1cm] on (-5pt,15pt) in node[above left=of n0.south east];
    \end{scope}
  \end{tikzpicture}
  \\
   Input &
   LLFlow \cite{ref:llflow}&
   LLFlow + LPDM (Ours) \\
   \includegraphics[width=1in]{figures/front-page/dark/LLFlow/01} &
   \begin{tikzpicture}
    \begin{scope}[
      node distance = 1mm,
          inner sep = 0pt,spy using outlines={rectangle, red, magnification=2,}
                          ]
    \node (n0)  {\includegraphics[width=1in]{figures/front-page/undarken/LLFlow/01}};

    \spy [blue,size=1cm] on (0pt,5pt) in node[below left=of n0.north east];
    \end{scope}
  \end{tikzpicture}
   
    &
   
   \begin{tikzpicture}
    \begin{scope}[
      node distance = 1mm,
          inner sep = 0pt,spy using outlines={rectangle, red, magnification=2,}
                          ]
    \node (n0)  {\includegraphics[width=1in]{figures/front-page/denoise/LLFlow/01} };

    \spy [blue,size=1cm] on (0pt,5pt) in node[below left=of n0.north east];
    \end{scope}
  \end{tikzpicture}
   
   \\
   Input &
   LLFormer \cite{ref:llformer}&
   LLFormer + LPDM (Ours) \\
   \includegraphics[width=1in]{figures/front-page/dark/LLFormer/Madison} &

 
   \begin{tikzpicture}
    \begin{scope}[
      node distance = 1mm,
          inner sep = 0pt,spy using outlines={rectangle, red, magnification=2,}
                          ]
    \node (n0)  { \includegraphics[width=1in]{figures/front-page/undarken/LLFormer/Madison} };

    \spy [blue,size=1cm] on (10pt,19pt) in node[above left=of n0.south east];
    \end{scope}
  \end{tikzpicture}
   &
   \begin{tikzpicture}
    \begin{scope}[
      node distance = 1mm,
          inner sep = 0pt,spy using outlines={rectangle, red, magnification=2,}
                          ]
    \node (n0)  { \includegraphics[width=1in]{figures/front-page/denoise/LLFormer/Madison} };

    \spy [blue,size=1cm] on (10pt,19pt) in node[above left=of n0.south east];
    \end{scope}
  \end{tikzpicture}
   
   \\
   Input &
   EnlightenGAN \cite{ref:enlightengan} &
   EnlightenGAN + LPDM (Ours) \\
   \includegraphics[width=1in]{figures/front-page/dark/EnlightenGAN/12} 
   &
   
   
   \begin{tikzpicture}
    \begin{scope}[
      node distance = 1mm,
          inner sep = 0pt,spy using outlines={rectangle, red, magnification=2,}
                          ]
    \node (n0)  { \includegraphics[width=1in]{figures/front-page/undarken/EnlightenGAN/12} };

    \spy [blue,size=1cm] on (-3pt,7pt) in node[below left=of n0.north east];
    \end{scope}
  \end{tikzpicture}
   
   &
  
  \begin{tikzpicture}
    \begin{scope}[
      node distance = 1mm,
          inner sep = 0pt,spy using outlines={rectangle, red, magnification=2,}
                          ]
    \node (n0)  {\includegraphics[width=1in]{figures/front-page/denoise/EnlightenGAN/12}  };

    \spy [blue,size=1cm] on (-3pt,7pt) in node[below left=of n0.north east];
    \end{scope}
  \end{tikzpicture}
   
   \\
   Input &
   KinD++ \cite{ref:kind++} &
   KinD++ + LPDM (Ours) \\
   \includegraphics[width=1in]{figures/front-page/dark/KinD++/9} &

  
   \begin{tikzpicture}
    \begin{scope}[
      node distance = 1mm,
          inner sep = 0pt,spy using outlines={rectangle, red, magnification=2,}
                          ]
    \node (n0)  {  \includegraphics[width=1in]{figures/front-page/undarken/KinD++/9}  };

    \spy [blue,size=1cm] on (15pt,-10pt) in node[below right=of n0.north west];
    \end{scope}
  \end{tikzpicture}
   
   &
   
   \begin{tikzpicture}
    \begin{scope}[
      node distance = 1mm,
          inner sep = 0pt,spy using outlines={rectangle, red, magnification=2,}
                          ]
    \node (n0)  { \includegraphics[width=1in]{figures/front-page/denoise/KinD++/9}};

    \spy [blue,size=1cm] on (15pt,-10pt) in node[below right=of n0.north west];
    \end{scope}
  \end{tikzpicture}
   \\
   Input &
   URetinex-Net \cite{ref:uretinexnet} &
   URetinex-Net + LPDM (Ours) \\
   \includegraphics[width=1in]{figures/front-page/dark/URetinexNet/Cave} &

    \begin{tikzpicture}
      \begin{scope}[
        node distance = 1mm,
            inner sep = 0pt,spy using outlines={rectangle, red, magnification=2,}
                            ]
      \node (n0)  { \includegraphics[width=1in]{figures/front-page/undarken/URetinexNet/Cave} };

      \spy [blue,size=1cm] on (15pt,0pt) in node[above right=of n0.south west];
      \end{scope}
    \end{tikzpicture}
    &
   
   \begin{tikzpicture}
    \begin{scope}[
      node distance = 1mm,
          inner sep = 0pt,spy using outlines={rectangle, red, magnification=2,}
                          ]
    \node (n0)  { \includegraphics[width=1in]{figures/front-page/denoise/URetinexNet/Cave}  };

    \spy [blue,size=1cm] on (15pt,0pt) in node[above right=of n0.south west];
    \end{scope}
  \end{tikzpicture}
   
   \\ 
   Input &
   ZeroDCE++ \cite{ref:zero-dce++} &
   ZeroDCE++ + LPDM (Ours) \\
   \includegraphics[width=1in]{figures/front-page/dark/ZeroDCE++/748}
    &
   
   \begin{tikzpicture}
    \begin{scope}[
      node distance = 1mm,
          inner sep = 0pt,spy using outlines={rectangle, red, magnification=2,}
                          ]
    \node (n0)  { \includegraphics[width=1in]{figures/front-page/undarken/ZeroDCE++/748}   };

    \spy [blue,size=1cm] on (7pt,5pt) in node[above right=of n0.south west];
    \end{scope}
  \end{tikzpicture}
   &
   \begin{tikzpicture}
    \begin{scope}[
      node distance = 1mm,
          inner sep = 0pt,spy using outlines={rectangle, red, magnification=2,}
                          ]
    \node (n0)  { \includegraphics[width=1in]{figures/front-page/denoise/ZeroDCE++/748}  };

    \spy [blue,size=1cm] on (7pt,5pt) in node[above right=of n0.south west];
    \end{scope}
  \end{tikzpicture}
  \end{tabular}
  }
  
  \caption{Qualitative results of our proposed approach on a variety of evaluation datasets for a variety of LLIE methods. \textbf{Left}: the input test image. \textbf{Middle}: the output of the listed LLIE method for the input image. \textbf{Right}: the result of applying our proposed post-processing LPDM to the image from the middle column. The LPDM parameters used are $\phi = 300$ and $s = 30$.
  \\
  }
  \label{fig:intro}
\end{figure}

\section{Low-Light Image Enhancement}
\label{sec:related-llie}

LLIE techniques have existed for many decades and can be divided into non-learning-based methods and learning-based methods. Popular examples of traditional techniques which do not require learning from data include variants of histogram equalization (HE) \cite{ref:ahe1, ref:ahe2} and gamma correction (GC) \cite{ref:AGC}. HE adjusts the global contrast of an image via a single transformation function. However, low-light images often require contrast enhancements that vary dynamically depending on local regions of the image. Thus, techniques such as GC adjust an image via a non-linear per-pixel transform to brighten dark regions while leaving bright regions relatively unaffected. Despite achieving reasonable results, the abovementioned traditional methods often require post-processing techniques in order to deal with amplified noise after enhancement, and struggle to perform well across diverse scenes. 

Another paradigm of LLIE makes use of Retinex theory, where the assumption is that a color image can be separated into reflectance and illumination \cite{ref:retinex}. Non-learning Retinex-based methods such as Low-light Image Enhancement via
Illumination Map Estimation (LIME) \cite{ref:LIME} provide an effective image enhancement approach; however, post-processing denoising is typically still necessary using algorithms such as BM3D \cite{ref:bm3d} which often blurs high-frequency details. Alternative Retinex-based methods reformulate the traditional Retinex model to incorporate an added noise term in order to cater for noise \cite{ref:retinex-added-noise-term}.

DL methods have recently achieved state-of-the-art LLIE performance. Some complexities of catering for a large variety of realistic low-light scenes are abstracted away by a data-driven approach. Most DL architectures are based on Convolutional Neural Networks (CNNs) and more recently, CNNs have been hybridized with  transformer networks~\cite{ref:attention-is-all}. DL methods either opt for incorporating denoising into a single model, or apply denoising as a post-processing step. S-LLNet \cite{ref:llnet-2016} makes use of a learned denoiser which operates sequentially after contrast enhancement. Retinex-net \cite{ref:lol-dataset} incorporates Retinex theory into a DL model with an  optional illumination-varying BM3D denoiser used as post-processing. Raw sensor data are enhanced and denoised via CNN in \cite{ref:learning-to-see}. A variety of loss functions have also been proposed (in addition to the typical $l_1$ and $l_2$ losses) which further penalize networks based on color, smoothness, brightness and perceptual interpretation \cite{ref:llnet-2016,ref:low-light-ssim-loss,ref:low-light-smoothness-loss, ref:zero-dce, ref:kind, ref:kind++, ref:sgz, ref:drbn, ref:perceptual-loss, ref:sarn}. Unsupervised Generative Adversarial Networks (GANs) have also been proposed for LLIE \cite{ref:enlightengan}. Recently, transformer architectures have gained popularity for LLIE which exploit spatial and channel-wise attention mechanisms \cite{ref:restformer, ref:llformer}.  

Another relevant state-of-the-art approach with respect to this work is the DL model LLFlow \cite{ref:llflow}. The LLFlow framework learns the conditional distribution between low-light and normally-exposed images via the generative paradigm of normalizing flow \cite{ref:normalising-flow}. The LLFlow architecture consists of an encoder as well as an invertible network, trained by minimizing negative log likelihood. The encoder of LLFlow produces an illumination-invariant color map as the prior distribution, upon which the invertible network learns to produce a normally-exposed image. 

LPDM proposed in this study also models the conditional distribution between low-light and normally-exposed images; however, we use the diffusion paradigm to achieve this. Furthermore, we repurpose the function of a DM to be used as a noise detector. Therefore, LPDM provides a subtractable estimation of the noise in an image which can further enhance the image. In contrast to LLFlow, LPDM is used as a post-processing step which can be applied regardless of the enhancing step that precedes LPDM.

\section{Diffusion Models}
\label{sec:related-diffusion-models}

A DM is a form of generative model which has recently been shown to generate high-quality samples, outperforming GANs \cite{ref:diffusion:original2015, ref:diffusion:ddpm, ref:diffusion:improved-ddpm,ref:diffusion:ddpm-beat-gans}. DMs iteratively remove small perturbations of noise, typically starting with a sample from an isotropic Gaussian distribution, until they generate a clean data sample. In this way, the  unconditional diffusion process connects a complex data distribution $q(\bm{x}_0)$ to a simpler, analytically tractable, distribution via a Markov chain consisting of a finite number of timesteps $T$ \cite{ref:diffusion:original2015}. The subscript of a sample indicates a timestep in the Markov chain, with $0$ being a clean sample and $T$ being a sample with the maximum amount of noise added. DMs have been successfully used to model both unconditional and conditional distributions \cite{ref:diffusion:improved-ddpm,ref:classifier-free-guidace}. In spite of the impressive results of DMs, the speed of generating samples has always been a drawback due to their iterative reverse process. Attempts have been made to increase sample speed by making the sampling process non-Markovian, as well as moving DMs to the latent space~\cite{ref:diffusion:ddim,ref:ldm}.

We avoid using DMs for sampling normally-exposed images owing to their expensive generative reverse process. Instead, we exploit the ability of DMs to capture complex conditional data distributions. 
In particular, we use DMs to capture the relationship between under-exposed and normally-exposed images. Other work has shown that DMs may be used as backbone feature extractors which predict features based on noisy inputs \cite{ref:vessel-seg,ref:diffuse-morph}. Furthermore, the task of denoising itself has been shown to assist with seemingly unrelated tasks such as semantic segmentation \cite{ref:denoising-pretraining}. The applications of using DMs in the field of LLIE are relatively unexplored, especially considering that LLIE can be posed as a denoising problem.

\section{Methodology}
\label{sec:methodology}
In this work, we propose a technique where a conditional DM is used to remove noise from images which have undergone LLIE. The remainder of this section is structured as follows: in \cref{sec:method:prelim}, the background information about DMs is outlined; the architecture used for LPDM is described in \cref{sec:method:arch}; finally, in \cref{sec:method:framework}, we provide detail of our proposed framework.

\subsection{Preliminaries}
\label{sec:method:prelim}

 DMs make use of a forward process which adds noise to a sample and a reverse process which removes noise. Our goal is to model the conditional data distribution $\bm{x}_0 \sim q(\bm{x}_0 | \bm{c})$, where $\bm{c}$ is an under-exposed image and $\bm{x}_0$ is a normally-exposed image. The forward diffusion process is defined as follows \cite{ref:diffusion:ddpm}:

\begin{equation}
  \begin{aligned}
    q(\bm{x}_{1:T}|\bm{x}_0) &:= \prod_{t=1}^{T}q(\bm{x}_t|\bm{x}_{t-1}),
    \\
    q(\bm{x}_t | \bm{x}_{t-1}) &:= \mathcal{N}(\bm{x}_t; \sqrt{1- \beta_t}\bm{x}_{t-1}, \beta_t \mathbf{I}),
  \end{aligned}
  \label{eq:diffusion:ddpm:q-def}
\end{equation}
  
\noindent where $\beta_t \in (0,1)$ defines a variance to be used at timestep $t$. As seen by the Markov chain in \cref{eq:diffusion:ddpm:q-def}, obtaining a more noisy sample $\bm{x}_t$ is dependent on the previous less-noisy sample $\bm{x}_{t-1}$. Defining extra notation $\alpha_t := 1 - \beta_t$ and $\bar{\alpha}_t :=  \prod_{s=1}^{t} \alpha_{s}$ allows for \cref{eq:diffusion:ddpm:q-def} to be reformulated to be conditioned on the original clean data sample $\bm{x}_0$ \cite{ref:diffusion:ddpm}:

\begin{equation}
  \begin{aligned}
    q(\bm{x}_t | \bm{x}_0) &:= \mathcal{N}(\bm{x}_t; \sqrt{\bar{\alpha}_t}\bm{x}_0, (1 - \bar{\alpha}_t) \mathbf{I}).
  \end{aligned}
  \label{eq:diffusion:ddpm:q-def-final}
\end{equation}

The efficient sampling of an arbitrary $\bm{x}_t$ at any timestep in the Markov chain is possible given $\bm{x}_0$:

\begin{equation}
  \begin{aligned}
    \bm{x}_t &= \sqrt{\bar{\alpha}_t}\bm{x}_0 + \sqrt{1-\bar{\alpha}_t}\bm{\epsilon}, 
  \end{aligned}
  \label{eq:diffusion:ddpm:q_sample}
\end{equation}

\noindent where $\bm{\epsilon} \sim \mathcal{N}(0,1)$ is a random source. The variance schedule is designed such that $\bm{x}_T \approx \mathcal{N}(0,1)$. Conditional DMs model the reverse process $p_\theta(\bm{x}_{t-1}| \bm{x}_t, \bm{c})$ where $\theta$ indicates that the DM is modelled by a neural network parameterized by $\theta$. The conditional DM attempts to maximize the likelihood $p_\theta(\bm{x}_0 | \bm{c})$. The reverse diffusion process is defined by parameterized Gaussian transitions \cite{ref:diffusion:ddpm}:

\begin{equation}
  \begin{aligned}
    p_\theta(\bm{x}_{0:T}| \bm{c}) &:= p(\bm{x}_T) \prod_{t=1}^{T}p_\theta(\bm{x}_{t-1}|\bm{x}_{t}, \bm{c}),
  \\
  p_\theta(\bm{x}_{t-1} | \bm{x}_t, \bm{c}) &:= \mathcal{N}(\bm{x}_{t-1}; \bm{\mu}_\theta(\bm{x}_t, t, \bm{c}), \bm{\Sigma}_\theta(\bm{x}_t, t, \bm{c})).
  \end{aligned}
\label{eq:diffusion:ddpm:p-def}
\end{equation}

 \noindent In order to avoid learning the variance, let $\bm{\Sigma}_\theta(\bm{x}_t, t, \bm{c}) = \sigma_t^2\mathbf{I}$, where $\sigma_t^2 = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t$ is a time-dependent constant \cite{ref:diffusion:ddpm}. Therefore, the only learnable component is $\bm{\mu}_\theta$. Instead of directly predicting $\bm{\mu}_\theta$, the DM is parameterized in terms of a denoising autoencoder $\bm{\epsilon}_\theta(\bm{x}_t, t, \bm{c})$ where $t = 1, ..., T$. The number of timesteps $T$ is set to a large number (such as $T=1000$) in order for the reverse process to better-approximate a Gaussian distribution \cite{ref:diffusion:ddim}. The corresponding simplified objective is as follows \cite{ref:diffusion:ddpm}:

\begin{equation}
  \begin{aligned}
    L_{DM} = \mathbb{E}_{\bm{x}_0, \bm{c}, \bm{\epsilon}, t}[\lVert \bm{\epsilon} - \bm{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}\bm{x}_0 + \sqrt{1 - \bar{\alpha}_t}\bm{\epsilon}), t, \bm{c} \rVert^2],
  \end{aligned}
  \label{eq:diffusion:ddpm:loss-simple}
\end{equation}

\noindent where $t$ is uniformly sampled from $\{1, ... , T\}$ and $\bm{\epsilon} \sim \mathcal{N}(0,1)$. In simplified terms, $L_{DM}$ guides the DM to predict the underlying $\bm{\epsilon}$ that was involved in sampling $\bm{x}_t$. Given $\bm{x}_t$ and a prediction for $\bm{\epsilon}$ using $\bm{\epsilon}_\theta$, we can calculate an estimate of $\bm{x}_0$ \cite{ref:diffusion:ddpm}:

\begin{equation}
  \begin{aligned}
    \bm{x}_0 \approx \bm{\hat{x}}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}  \bm{x}_t - \left(\sqrt{\frac{1}{\bar{\alpha}_t} -1}\right)\bm{\epsilon}_\theta(\bm{x}_t, t, \bm{c}).
  \end{aligned}
  \label{eq:diffusion:predict-x0}
\end{equation}
\noindent Further information about the DM sampling process is omitted since it is not used in this work.

\subsection{Diffusion Model Architecture}
\label{sec:method:arch}
The DM architecture $\bm{\epsilon}_\theta$ used for modelling the diffusion process is typically a form of modified U-Net \cite{ref:unet,ref:diffusion:ddpm}. By definition, a U-Net consists of an encoder and a decoder. The encoder contains a set of residual blocks \cite{ref:resnet} followed by downsampling operations which are repeated multiple times until the desired latent resolution is achieved. The decoder consists of residual blocks followed by upsampling operations in order to return the latent encoding to the original input resolution. Each downsampling stage halves the input resolution and each upsampling stage doubles the input resolution. Before each set of residual blocks in the decoder, output from the encoder with the corresponding resolution is concatenated such that spatial context is not lost as a result of the downsampling operations. Between the encoder and decoder is a middle block which contains a specified number of residual blocks in order to process the latent encoding. 

The DM operates across all timesteps in the Markov chain using the same parameters. This is made possible by modifying the original U-Net to condition on timestep information represented by sinusoidal positional embeddings as seen in transformer networks \cite{ref:attention-is-all,ref:diffusion:ddpm}. Other notable modifications from the original U-Net are the use of attention mechanisms at different spatial resolutions as well as the use of group normalization \cite{ref:group-norm} within the residual blocks. As seen in \cref{fig:res-block}, the residual blocks of the DM are composed of a combination of group normalization layers, SiLU activations \cite{ref:silu}, convolutional layers and addition operators. A variety of attention mechanisms may be applied at different spatial resolutions, such as global attention \cite{ref:global-attention}, cross-attention \cite{ref:cross-attention} or scaled dot-product attention \cite{ref:attention-is-all}.

\begin{figure}[t]
  \includegraphics[width=\linewidth]{figures/res-block.pdf}
  \centering
  \caption{Residual block used throughout the DM architecture consisting of a combination of group normalization layers, SiLU activations, convolution layers and addition operations. Both $c_{in}$ and $t$ are inputs to the residual block and represent the channel and timestep-embedded input respectively. Note that $t$ is already in embedded form when it enters the residual block. The output of the residual block is represented by $c_{out}$.}
  \label{fig:res-block}
\end{figure}

\subsection{Proposed Framework}
\label{sec:method:framework}

\begin{figure}
  \setlength\tabcolsep{2pt}%%
  \small
  \centering
  \begin{tabular}{ccc}
   $\bm{x}_0$ &
   $\bm{c}$ &
   $\bm{\hat{x}}_0^\eta$ \\
   \includegraphics[width=1in]{figures/phi-compare/x0} &
   \includegraphics[width=1in]{figures/phi-compare/c} &
   \includegraphics[width=1in]{figures/phi-compare/x0_eta} \\
   $\bm{x}_1$ &
   $\bm{x}_{300}$ &
   $\bm{x}_T$ \\
   \includegraphics[width=1in]{figures/phi-compare/x_1} &
   \includegraphics[width=1in]{figures/phi-compare/x_300} &
   \includegraphics[width=1in]{figures/phi-compare/x_1000} \\
   $\bm{n}_1$ &
   $\bm{n}_{300}$ &
   $\bm{n}_T$ \\
   \includegraphics[width=1in]{figures/phi-compare/gam1} &
   \includegraphics[width=1in]{figures/phi-compare/gam299} &
   \includegraphics[width=1in]{figures/phi-compare/gam999} \\
   $\xi(\bm{\hat{x}}_0^\eta, \bm{n}_{300}, 100)$ &
   $\xi(\bm{\hat{x}}_0^\eta, \bm{n}_{300}, 300)$ &
   $\xi(\bm{\hat{x}}_0^\eta, \bm{n}_{300}, T)$ \\
   \includegraphics[width=1in]{figures/phi-compare/ksi_100} &
   \includegraphics[width=1in]{figures/phi-compare/ksi_300} &
   \includegraphics[width=1in]{figures/phi-compare/ksi_1000} 
  \end{tabular}
  \caption{ Visualization of different components within the diffusion process and the LPDM pipeline. The first row displays a normally-exposed image, under-exposed image and image which has undergone LLIE. The second row demonstrates how noise is added to $\bm{x}_0$ using a linear variance schedule during the training process, with $T = 1000$. The third row demonstrates the effect of different values of $\phi$ in \cref{eq:proposed:noise-estimate}. The fourth row demonstrates the effect of applying \cref{eq:diffusion:predict-x0-mod} with different values of $s$ in order to enhance $\bm{\hat{x}}_0^\eta$.
  }
  \label{fig:phi}
\end{figure}

\begin{figure*}
  \includegraphics[width=\textwidth]{figures/dm-train-infer}
  \centering
  \caption{Diagram presenting the training phase and inference phase of the LPDM, displayed on the left and right half of the diagram respectively.}
  \label{fig:train-infer}
\end{figure*}

In order to address possible degradations which occur after LLIE, we propose LPDM represented by $\bm{\epsilon}_\theta$. The LPDM model is given $(\bm{x}_t \oplus \bm{c}, t)$ as input and tasked with predicting $\bm{\epsilon}$, where $\bm{x}_t \sim q(\bm{x}_t | \bm{x}_0)$ is a normally-exposed image with noise $\bm{\epsilon} \sim \mathcal{N}(0,1)$ added at a timestep $t$, $\bm{c}$ is the corresponding under-exposed image and $\oplus$ is the concatenation operator.

We follow the common DM training process where the LPDM is exposed to batches of $(\bm{x}_t \oplus \bm{c}, t)$ with randomly sampled $t$ for each sample in a batch. A visualization of the training process is depicted in the left half of \cref{fig:train-infer}. After the LPDM has been trained, the LPDM is applied in a novel manner. Specifically, the LPDM acts as a noise detector given an enhanced low-light image. Let $\eta$ be any low-light image enhancer and let $\bm{\hat{x}}_0^\eta$ be an enhanced image such that  $\bm{\hat{x}}_0^\eta = \eta(\bm{c})$. The LPDM is used to obtain an estimate of the noise present in $\bm{\hat{x}}_0^\eta$:

\begin{equation}
  \begin{aligned}
    \bm{n}_{\phi} &= \bm{\epsilon}_\theta(\bm{\hat{x}}_0^\eta, \phi, \bm{c}),
  \end{aligned}
  \label{eq:proposed:noise-estimate}
\end{equation}

\noindent where $\phi < T$ is a timestep at which we wish to detect noise in $\bm{\hat{x}}_0^\eta$. An important property of \cref{eq:proposed:noise-estimate} is that noise is not added to $\bm{\hat{x}}_0^\eta$; rather, the model is tasked with finding the noise present in $\bm{\hat{x}}_0^\eta$ as a result of enhancement, based on the conditioning $\bm{c}$ and timestep $\phi$. Additionally, the value of $\phi$ is selected at a level such that the underlying structure of the image would be preserved if noise were hypothetically to be added. Thus, a suitable value for $\phi$ is related to the selected variance schedule. \cref{fig:phi} provides visual examples of applying our proposed approach for an example set of $\bm{x}_0$, $\bm{c}$ and $\bm{\hat{x}}_0^\eta$. Row 2 of \cref{fig:phi} demonstrates the effect of selecting different values of $t$ in \cref{eq:diffusion:ddpm:q-def-final} in order to sample a noisy image during the training process. The noise schedule closer to 0 corresponds to less added noise in the image. Therefore, it is reasonable to detect noise in $\bm{\hat{x}}_0^\eta$ at lower levels of~$\phi$ since we do not expect $\bm{\hat{x}}_0^\eta$ to be pure noise. 

Row 3 of \cref{fig:phi} demonstrates the effect of different values of $\phi$. For values of $\phi$ that are too low, the model overestimates the noise present in $\bm{\hat{x}}_0^\eta$. For large values of $\phi$, the model becomes similar to an autoencoder and attempts to predict the input. The reason for the behavior of different levels of $\phi$ can be explained by how the LPDM is trained. For values of $\phi$ close to $T$, the LPDM expects the input to be purely noise, and thus a suitable prediction for $\bm{\epsilon}$ would simply be to predict the input. For low values of $\phi$, the model expects subtle noise in the input image and thus may overdetect noise. We find a good balance to be values of $\phi$ where the background structure of the image is not completely destroyed such as $\phi = 300$ which corresponds to $\bm{x}_{300}$ in row 2 of \cref{fig:phi}.

Once we obtain the estimation of the noise $\bm{n}_{\phi}$, we subtract the noise from $\bm{\hat{x}}_0^\eta$ using a modification of \cref{eq:diffusion:predict-x0}:

\begin{equation}
  \begin{aligned}
    \bm{\hat{x}}_0^{DM} = \xi(\bm{\hat{x}}_0^\eta, \bm{n}_{\phi}, s) := \frac{1}{\sqrt{\bar{\alpha}_s}}  \bm{\hat{x}}_0^\eta - \left(\sqrt{\frac{1}{\bar{\alpha}_s} -1}\right)\bm{n}_{\phi},
  \end{aligned}
  \label{eq:diffusion:predict-x0-mod}
\end{equation}

\noindent where $s$ is a timestep which selects the coefficients according to the variance schedule, and thus balances the degree to which noise is subtracted from $\bm{\hat{x}}_0^\eta$. The final result after LPDM post-processing is represented by $\bm{\hat{x}}_0^{DM}$. Notably, we find that $s$ should be significantly less than $\phi$ in order to subtract the correct amount of noise. As $s \rightarrow \phi$, more of the noise $\bm{n}_{\phi}$ is subtracted from $\bm{\hat{x}}_0^{\eta}$, leading to overcorrections and perhaps further degrading the result. As seen in row 4 of \cref{fig:phi}, the value of $s$ impacts how much correction should be applied. Our technique is able to reduce noise, correct color and improve sharpness as seen when comparing $\bm{\hat{x}}_0^{\eta}$ to $\xi(\bm{\hat{x}}_0^\eta, \bm{n}_{300}, 100)$ in \cref{fig:phi}.

Denoising techniques may be classified as being either blind or non-blind. Blind denoisers do not require the user to specify the level of noise in the input image, whereas non-blind denoisers require the user to specify the noise level. The popular BM3D algorithm is a non-blind approach. Similarly, our approach requires a selection of $s$ to determine to what extent noise should be subtracted, however we find low values of $s$ to be applicable to a wide variety of scenarios.

In summary, our approach requires the specification of a parameter $s$ during application, where $\phi$ may be fixed empirically. We find $\phi = 300$ to be a reasonable choice, and we use this value for all experiments. As $s \rightarrow 0$, the amount of correction lessens. The right half of \cref{fig:train-infer} summarizes the inference process described above. 


\section{Experiments}
\label{sec:experiment}
The following subsections outline the experimental setup: 
\cref{sec:experiment:eval-ds} describes the datasets used in this study; \cref{sec:experiment:implementation} defines the configuration of LPDM and the training parameters used for all experiments; \cref{sec:experiment:benchmark-study} provides detail on the LLIE models selected for comparison with LPDM; in order to achieve a fair comparison, we compare our approach to alternative denoising methods described in \cref{sec:experiment:alt-denoisers}; the interpretation of all the results is presented in \cref{sec:experiment:interp-results}; finally, an ablation study is conducted in \cref{sec:experiment:ablation}. 

\subsection{Evaluation Datasets}
\label{sec:experiment:eval-ds}

Paired low-light datasets are challenging to collect due to the requirement of having the scene remain unchanged while camera ISO is adjusted \cite{ref:lol-dataset}. Accordingly, many methods resort to augmenting datasets with synthetic data. Synthetic datasets are typically generated by adjusting the gamma of normally-exposed images and adding simulated noise. We avoid training the LPDM on synthetic datasets in order to ensure that we correctly model the conditional distribution between under-exposed and normally-exposed images. We train on the original paired LOL dataset \cite{ref:lol-dataset}, which contains 485 training images and 15 test images. Alternative versions of LOL exist, however these only add synthetic data such as the extended version of LOL \cite{ref:lolv2} and VE-LOL \cite{ref:ve-lol}. We evaluate our model on the widely-adopted real unpaired test sets LIME~\footnote{LIME \cite{ref:LIME} is both a LLIE technique and an unpaired test dataset, both of which are proposed in the same paper.} (10 images)~\cite{ref:LIME}, DICM (64 images)\cite{ref:dicm}, MEF (17 images)~\cite{ref:mef}, NPE (7 images)~\cite{ref:npe}. We specify the number of images explicitly as previous works use varying subsets of the test sets. The full-reference metrics we adopt are structural similarity index measure (SSIM), peak signal-to-noise ratio (PSNR), mean absolute error (MAE) and learned perceptual image patch similarity (LPIPS) \cite{ref:lpips}. For the unpaired test data we adopt the following no-reference metrics: natural image quality evaluator (NIQE) \cite{ref:niqe}, blind/referenceless image spatial quality evaluator (BRISQUE) \cite{ref:brisque} and the smartphone photography attribute and quality (SPAQ) database \cite{ref:spaq}. All metrics are calculated in the RGB color space unless otherwise stated. 

\subsection{Implementation Details}
\label{sec:experiment:implementation}
A linear variance schedule is used in the range [0.00085, 0.012] for the diffusion process. The value of $T$ is fixed to $1000$ for all experiments. The LPDM is trained on the LOL training set for 6000 training steps using the AdamW optimizer \cite{ref:adamw} with a learning rate of \num{1e-6} and with the AdamW parameters $\beta_1 = 0.9$, $\beta_2 = 0.999$ and $\lambda = 0.01$. The loss function is defined in \cref{eq:diffusion:ddpm:loss-simple}. We use the RGB color space for both low-light and normally-exposed images, and images are converted into the range [-1, 1]. We train on 256 $\times$ 256 random crops with random horizontal flipping.  A batch size of 4 is used with an accumulation of gradients for 8 batches in order to simulate a batch size of 32. 

The U-Net of the LPDM consists of 4 downsampling stages (encoder) and 4 upsampling stages (decoder), with 2 residual blocks per stage. Between the encoder and the decoder is a middle block which processes the latent encoding. The middle block contains 2 residual blocks which surround a scaled dot-product attention layer using 8 attention heads.
We avoid using attention mechanisms at higher resolutions than the final latent encoding in order to conserve memory. Residual blocks within the four downsampling stages output 128, 256, 512 or 512 channels, respectively. Therefore, the residual blocks at the highest resolution output 128 channels, and the latent resolution residual blocks output 512 channels. The residual blocks in the middle block both output 512 channels. The decoder is a reflection of the encoder, where downsampling operations are replaced with upsampling operations and the output of the encoder is concatenated at each upsampling stage.

The implementation of the LPDM method is available online at \url{https://github.com/savvaki/LPDM}.

\subsection{Benchmark Study for LPDM}
\label{sec:experiment:benchmark-study}
The following state-of-the-art LLIE approaches are selected for comparison:  LIME~\cite{ref:LIME}, BIMEF~\cite{ref:bimef}, RetinexNet~\cite{ref:lol-dataset}, EnlightenGAN~\cite{ref:enlightengan}, KinD~\cite{ref:kind}, KinD++~\cite{ref:kind++}, ZeroDCE~\cite{ref:zero-dce}, ZeroDCE++~\cite{ref:zero-dce++}, URetinex-Net~\cite{ref:uretinexnet}, LLFlow~\cite{ref:llflow} and LLFormer~\cite{ref:llformer}. BIMEF and LIME are non-learning-based methods and the remaining methods are all learning-based. The LLFlow, LLFormer, URetinex-Net and RetinexNet methods are trained on the LOL dataset only. The KinD and KinD++ models are trained on LOL with additional custom synthetic data added \cite{ref:kind++}. The ZeroDCE and ZeroDCE++ models are trained on multi-exposure image sets from the SICE \cite{ref:sice} dataset. EnlightenGAN is trained with unpaired groups of low-light and normal-exposure images using data from LOL as well as additional datasets.

Several methods such as KinD, KinD++, URetinex-Net and LLFlow scale their model outputs based on an illumination ratio which involves the ground truth label. In order to achieve a fair comparison, we do not use the ground truths to scale these model outputs. Instead, we treat the LOL test set as unpaired data as would be the case in real-life scenarios. Similar to previous approaches, we fix the parameters of the LIME algorithm to $\alpha = 0.15$ and $\sigma = 2$ and $\gamma = 0.8$.

We compare the abovementioned LLIE approaches with and without our proposed LPDM. For the remainder of this work, $\textnormal{LPDM}_s$ represents the LPDM approach applied with parameter $s$, defined in \cref{sec:method:framework}. All LPDM results fix $\phi = 300$, and we report two values of $s$ which are 15 and 30. \cref{tab:lol-results} contains results with and without LPDM on the LOL test set represented as $\eta + \textnormal{LPDM}_s$. In addition to the LOL dataset evaluation, we provide qualitative and quantitative results on the abovementioned unpaired test sets. The unpaired test metrics can be found in \cref{tab:unpaired-results}.


%% local settings
\sisetup{detect-weight,mode=text}
% for avoiding siunitx using bold extended
\renewrobustcmd{\bfseries}{\fontseries{b}\selectfont}
\renewrobustcmd{\boldmath}{} 
\newrobustcmd{\B}{\bfseries}

\begin{table*}[t]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.2}
\newcommand\setrow[1]{\gdef\rowmac{#1}#1\ignorespaces}
\caption{Results on the LOL test set for different LLIE methods ($\eta$), with and without post-processing. }

\label{tab:lol-results}
\centering
\resizebox{\textwidth}{!}{% % Can change this to col width and remove table* env
\includegraphics{table1}
}
\end{table*}


\begin{figure*}
  \setlength\tabcolsep{2pt}%%
  \small
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{ccccccc}
  %  $ \bm{\hat{x}}_0^{\textnormal{LIME}}$ \hphantom{6} &
   LIME \cite{ref:LIME} &
   $\textnormal{BM3D}_5$ &
   $\textnormal{BM3D}_{15}$ &
   NAFNet &
   $\textnormal{LPDM}_{15}$ &
   $\textnormal{LPDM}_{30}$ &
   $\bm{x}_0$ \\
   \includegraphics[width=0.9in]{figures/denoiser-compare/1/pred/780} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/1/bm3d5/780} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/1/bm3d15/780} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/1/nafnet/780} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/1/lpdm15/780} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/1/lpdm30/780} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/1/gt/780} \\
  %  \hphantom{10} $\bm{\hat{x}}_0^{\textnormal{RetinexNet}}$ &
   RetinexNet \cite{ref:lol-dataset} &
   $\textnormal{BM3D}_5$ &
   $\textnormal{BM3D}_{15}$ &
   NAFNet &
   $\textnormal{LPDM}_{15}$ &
   $\textnormal{LPDM}_{30}$ &
   $\bm{x}_0$ \\
   \includegraphics[width=0.9in]{figures/denoiser-compare/4/pred/79} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/4/bm3d5/79} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/4/bm3d15/79} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/4/nafnet/79} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/4/lpdm15/79} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/4/lpdm30/79} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/4/gt/79} \\
  %  $\bm{\hat{x}}_0^{\textnormal{KinD++}}$ &
   KinD++ \cite{ref:kind++}&
   $\textnormal{BM3D}_5$ &
   $\textnormal{BM3D}_{15}$ &
   NAFNet  &
   $\textnormal{LPDM}_{15}$ &
   $\textnormal{LPDM}_{30}$ &
   $\bm{x}_0$ \\
   \includegraphics[width=0.9in]{figures/denoiser-compare/3/pred/111} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/3/bm3d5/111} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/3/bm3d15/111} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/3/nafnet/111} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/3/lpdm15/111} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/3/lpdm30/111} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/3/gt/111} \\
  %  \hphantom{2} $\bm{\hat{x}}_0^{\textnormal{LLFormer}}$ &
  LLFormer \cite{ref:llformer} &
   $\textnormal{BM3D}_5$ &
   $\textnormal{BM3D}_{15}$ &
   NAFNet  &
   $\textnormal{LPDM}_{15}$ &
   $\textnormal{LPDM}_{30}$ &
   $\bm{x}_0$ \\
   \includegraphics[width=0.9in]{figures/denoiser-compare/2/pred/778} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/2/bm3d5/778} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/2/bm3d15/778} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/2/nafnet/778} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/2/lpdm15/778} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/2/lpdm30/778} &
   \includegraphics[width=0.9in]{figures/denoiser-compare/2/gt/778} 
  \end{tabular}
  }
  \caption{A qualitative comparison of the BM3D \cite{ref:bm3d} and NAFNet \cite{ref:nafnet-denoiser} post-processing denoising approaches to LPDM on the LOL test set. The first column displays $\bm{\hat{x}}_0^\eta$ for different $\eta$. The final column contains the ground truth label. The remaining columns display the results of different denoising approaches which post-process $\bm{\hat{x}}_0^\eta$.}
  \label{fig:denoiser-compare}
\end{figure*}

\subsection{Comparison with Alternative Denoisers}
\label{sec:experiment:alt-denoisers}
We compare the LPDM denoising performance with the popular BM3D \cite{ref:bm3d} algorithm as well as a state-of-the-art DL denoiser NAFNet \cite{ref:nafnet-denoiser}.  Applying BM3D evenly over the entire image results in bright regions being oversmoothed, since noise is often present at higher levels in dark regions. Therefore, previous LLIE works have adapted BM3D to operate unevenly based on the illumination of the recovered image \cite{ref:lol-dataset,ref:LIME}.

In particular, we test our method against the denoising approach used in LIME \cite{ref:LIME}. LIME is a Retinex-based method where the assumption is that $\bm{\mathrm{L}} = \bm{\mathrm{R}} \circ \bm{\mathrm{T}}$, where $\bm{\mathrm{L}}$ represents the source low-light image, $\bm{\mathrm{R}}$ represents the desired recovery image (reflectance), $\bm{\mathrm{T}}$ represents the illumination and $\circ$ represents element-wise multiplication. LIME estimates the illumination $\bm{\mathrm{T}}$ which is then used to determine $\bm{\mathrm{R}}$ given $\bm{\mathrm{L}}$. In order to avoid oversmoothing bright regions of $\bm{\mathrm{R}}$, the BM3D algorithm is scaled based on the illumination map $\bm{\mathrm{T}}$. Furthermore, the BM3D algorithm is only applied to the Y component of $\bm{\mathrm{R}}$, after $\bm{\mathrm{R}}$ is converted to the YUV color space. The LIME denoising algorithm is as follows \cite{ref:LIME}:

\begin{equation}
  \begin{aligned}
    \bm{\mathrm{R}}_f := \bm{\mathrm{R}} \circ \bm{\mathrm{T}} + \bm{\mathrm{R}}_d \circ (\bm{1} - \bm{\mathrm{T}}),
  \end{aligned}
  \label{eq:denoising:lime}
\end{equation}

\noindent where $\bm{\mathrm{R}}_d$ is the denoised version of $\bm{\mathrm{R}}$, and $\bm{\mathrm{R}}_f$ is the recomposed image after weighting $\bm{\mathrm{R}}$ and $\bm{\mathrm{R}}_d$ by $\bm{\mathrm{T}}$ and $1 - \bm{\mathrm{T}}$ respectively. Note that only the Y channel of $\bm{\mathrm{R}}$ in the YUV color space is denoised, and the result is then converted back to the RGB color space to form $\bm{\mathrm{R}}_d$. We apply LIME to obtain illumination maps $\bm{\mathrm{T}}$ for each low-light image in the LOL test set. We then use \cref{eq:denoising:lime}, and for each $\eta$ we set $\bm{\mathrm{R}} = \bm{\hat{x}}_0^\eta$ for each LOL test example to obtain the denoised result scaled by $\bm{\mathrm{T}}$. For all figures and tables, $\textnormal{BM3D}_{\sigma}$ represents the application of the BM3D algorithm according to \cref{eq:denoising:lime}, where $\sigma$ is the standard deviation parameter of BM3D.
The results are reported in \cref{tab:lol-results} as $\eta + \textnormal{BM3D}_{\sigma}$.

In a further experiment, we apply a state-of-the-art DL denoiser NAFNet \cite{ref:nafnet-denoiser} as a post-processing step after LLIE. The NAFNet model is trained on the smartphone image denoising dataset (SIDD) \cite{ref:sidd} which contains noisy images captured under multiple lighting conditions. Thus, NAFNet is well-suited to denoise enhanced images of a variety of brightness levels. The input to NAFNet is $\bm{\hat{x}}_0^\eta$, and the result is the denoised output which we capture for each $\eta$ over each test image. The results of NAFNet denoising can be found in \cref{tab:lol-results} represented as $\eta + \textnormal{NAFNet}$.


\subsection{Interpretation of Results}
\label{sec:experiment:interp-results}

The full-reference and no-reference metrics are reported in \cref{tab:lol-results} and \cref{tab:unpaired-results}, respectively. Metrics marked in bold indicate that they are the best for a particular method. The LOL test set results in \cref{tab:lol-results} show that our LPDM is able to improve the SSIM of all baseline LLIE methods except LLFlow on the LOL dataset. In all cases, LPDM improves the PSNR compared to each baseline. In many cases, the SSIM is greatly improved by adding the LPDM. Adding the LPDM to LIME, RetinexNet, EnlightenGAN, ZeroDCE, ZeroDCE++ and LLFormer boasts up to a 53.5\%, 78.65\%, 16.8\%, 24.08\%, 23.82\%, 4.92\% SSIM improvement, respectively. LLFormer yields new state-of-the-art color SSIM results on the LOL dataset when LPDM post-processing is added. 

% \setlength{\parskip}{0pt}

LPDM is able to improve the baseline PSNR for all methods, and mostly outperforms the competing denoisers on the PSNR metric. In all cases, LPDM outperforms the alternative denoisers on the perceptual LPIPS metric. In some cases, NAFNet is able to improve PSNR and MAE more than our LPDM; however, upon further inspection, this is as a result of aggressive denoising and thus oversmoothing. \cref{fig:denoiser-compare} displays a comparison of post-processing approaches. NAFNet removes most typical noises, however at the cost of removing detail. For example, consider the first row of \cref{fig:denoiser-compare}, where technically NAFNet yields a higher PSNR for $\bm{\hat{x}}_0^{\textnormal{LIME}}$, but the results are clearly blurred when compared to LPDM which maintains the sharpness of the original image. The behavior of NAFNet manifests more accurately in the LPIPS metric where the LPDM outperforms NAFNet significantly for all LLIE methods. In addition, the BM3D denoiser performs well, however it is unable to deal with color noises and other distortions as robustly as DL methods.

Notably, NAFNet only performs better than LPDM for SSIM on the noise introduced by the LIME method due to the type of noise being similar to most denoising datasets. In contrast, our method models the conditional distribution between low-light and normally-exposed images, and thus the LPDM can handle a variety of different artifacts and color distortions besides typical noise. An example of a distortion which differs from typical Gaussian noise is the distortion introduced by KinD++. Row three of \cref{fig:denoiser-compare} displays how our LPDM increases the sharpness of $\bm{\hat{x}}_0^{\textnormal{KinD++}}$ where the other denoisers yield oversmoothed results. We emphasize this point because different LLIE methods introduce a panoply of different distortions. 

For the majority of methods and datasets in \cref{tab:unpaired-results}, LPDM is able to improve the SPAQ score. The improvement of the NIQE score and BRISQUE score fluctuates depending on the dataset and the method. Therefore, it is vital to analyze the qualitative effects of LPDM on real test data. Several images from a variety of datasets are displayed in \cref{fig:intro}. The general advantage of LPDM is its ability to strike a balance between smoothing and maintaining sharpness. Due to the distribution of the noise output of LPDM being zero-centered, our approach maintains the perceptual quality of the underlying image and avoids oversmoothing when \cref{eq:diffusion:predict-x0-mod} is applied. In some cases, the LPDM is able to improve color quality and sharpness. Upon close inspection, LPDM alters color shades to more accurately represent reality, as seen for BIMEF, KinD++, LLFormer and URetinex-Net examples in \cref{fig:intro}.

In addition to the above conclusions, the choices of $s = 15$ and $s = 30$ may not necessarily be the optimal values for each LLIE method. Therefore, there may be larger improvements for a different choice of $s$ which can be determined empirically. We fix $s$ in order to demonstrate the possibility of using the LPDM as a blind denoiser. 



\begin{table*}
  \renewcommand{\arraystretch}{1.2}
  \newcommand\setrow[1]{\gdef\rowmac{#1}#1\ignorespaces}
  \caption{Results on unpaired test sets for different LLIE methods ($\eta$), with and without our proposed LPDM.}
  \label{tab:unpaired-results}
  \centering
  \resizebox{\textwidth}{!}{%
  \includegraphics{table2}
  }
\end{table*}

\subsection{Ablation Study}
\label{sec:experiment:ablation}
An ablation study is necessary in order to demonstrate that the improvements of LPDM can be attributed specifically to our proposed approach and that the results are not arbitrary. In \cref{sec:experiment:ablation:fixt} we examine the effect of predicting $\bm{\epsilon}$, and in \cref{sec:experiment:ablation:uncond} we compare unconditional diffusion to LPDM. In order to conserve space, tables for the ablation study results are summarized such that the percentage improvement for each metric is calculated for each $\eta$, and the mean and standard deviation percentage improvements are reported. 

\subsubsection{The Effect of Predicting the Noise}
\label{sec:experiment:ablation:fixt}
As seen in \cref{eq:diffusion:ddpm:loss-simple}, DMs are trained to make predictions for $\bm{\epsilon}$. We examine the value of predicting $\bm{\epsilon}$ by changing the model to predict $\bm{x}_0$ directly, and we name this model \textit{direct} LPDM or DLPDM. The DLPDM model directly denoises the input and does not require any further steps such as \cref{eq:diffusion:predict-x0-mod}. The DLPDM is identical to the LPDM described in \cref{sec:experiment:implementation} with two differences: the ground truth target of the model is now $\bm{x}_0$ rather than $\bm{\epsilon}$, and we remove timestep conditioning from the model by setting $t=0$ as input to the model regardless of $\bm{x}_t$. Therefore, the model directly denoises its input with the same number of parameters and without the requirement of specifying $\phi$ at inference time, thus making the model a blind denoiser. In other terms, the model is responsible for detecting the amount of noise present in $\bm{x}_t$ and predicting $\bm{x}_0$ without any additional parameters defined by the user.

The results of the DLPDM experiment are summarized in \cref{tab:ablation-fixt}, which includes the other ablation results from \cref{sec:experiment:ablation:uncond}. Our proposed LPDM approach performs better on SSIM and LPIPS and DLPDM performs better on PSNR and MAE (although the variance of DLPDM is higher). LPDM largely outperforms DLPDM on LPIPS which implies that  LPDM results are more perceptually similar to the ground truth. The results are verified when examining the examples in \cref{fig:ablation-fixt} where LPDM preserves the sharpness of $\bm{\hat{x}_0}^\eta$ and maintains color accuracy.

\begin{table}[t]
  % increase table row spacing, adjust to taste
  \renewcommand{\arraystretch}{1.2}
  \newcommand\setrow[1]{\gdef\rowmac{#1}#1\ignorespaces}
  \caption{Ablation study comparing LPDM to DLPDM and ULPDM on the LOL test set.}
  
  \label{tab:ablation-fixt}
  \centering
  \resizebox{\linewidth}{!}{% % Can change this to col width and remove table* env
  \begin{tabular}[t]{ 
      |c| % label
      c  % SSIM
      c  % PSNR
      c  % MAE
      c | % LPIPS
    } %{|c|c c c c|}
  \hline
    \B Methods & \multicolumn{1}{c}{\B SSIM (\%) $\uparrow$} & \multicolumn{1}{c}{ \B PSNR (\%) $\uparrow$} & \multicolumn{1}{c}{\B MAE (\%) $\uparrow$} & \multicolumn{1}{c |}{\B LPIPS (\%) $\uparrow$} \\
    \hline
    
    DLPDM &  $19.218 \pm 26.84$ &    $2.53 \pm 3.4$ &  $2.531 \pm 4.93$ &$18.798 \pm 23.76$ \\
    $\textnormal{ULPDM}_{15}$ & $1.937 \pm 28.55$ &$-0.317 \pm 2.42$ &$-0.687 \pm 3.33$ &$-8.43 \pm 39.76$ \\
    $\textnormal{ULPDM}_{30}$ & $-17.566 \pm 28.42$ &  $-2.096 \pm 3.27$ &  $-2.754 \pm 4.95$ &$-43.713 \pm 53.83$ \\
  $\textnormal{LPDM}_{15}$  & $17.138 \pm 17.50$ &  $1.225 \pm 1.79$ &  $0.942 \pm 2.92$ & $29.462 \pm 20.96$ \\
  % a & $17.138 \pm 17.5$ & 2 & 3 & 4 \\
  $\textnormal{LPDM}_{30}$ & $19.928 \pm 25.17$ &  $1.079 \pm 2.26$ &  $0.657 \pm 3.66$ & $28.820 \pm 24.94$ \\
  \hline

    \end{tabular}
  }
  \end{table}

  \begin{figure}[t]
    \setlength\tabcolsep{2pt}%%
    \small
    \centering
    \begin{tabular}{cccc}
     Input &
    %  $\bm{\hat{x}}_0^{\textnormal{KinD++}}$ &
    KinD++ \cite{ref:kind++} &
     DLPDM &
     $\textnormal{LPDM}_{15}$
     \\
     \includegraphics[width=0.8in]{figures/fixt-compare/1/input/BelgiumHouse} &
     \includegraphics[width=0.8in]{figures/fixt-compare/1/pred/BelgiumHouse} &
     \includegraphics[width=0.8in]{figures/fixt-compare/1/fixt/BelgiumHouse} & 
     \includegraphics[width=0.8in]{figures/fixt-compare/1/cond15/BelgiumHouse} \\ 
     Input &
    %  $\bm{\hat{x}}_0^{\textnormal{RetinexNet}}$ &
     RetinexNet \cite{ref:lol-dataset }&
     DLPDM &
     $\textnormal{LPDM}_{15}$
     \\
     \includegraphics[width=0.8in]{figures/fixt-compare/2/input/Balloons} &
     \includegraphics[width=0.8in]{figures/fixt-compare/2/pred/Balloons} &
     \includegraphics[width=0.8in]{figures/fixt-compare/2/fixt/Balloons} & 
     \includegraphics[width=0.8in]{figures/fixt-compare/2/cond15/Balloons}
    \end{tabular}
    \caption{Visual examples of the ablation study directly comparing  predicting $\bm{x}_0$ to predicting $\bm{\epsilon}$ using the DLPDM and LPDM models, respectively.}
    \label{fig:ablation-fixt}
  \end{figure}


\subsubsection{The Effect of Conditioning}
\label{sec:experiment:ablation:uncond}
We explore the effect of appending $\bm{c}$ to $\bm{x}_t$ as visually depicted in \cref{fig:train-infer}. We name this model \textit{unconditional} LPDM or ULPDM. In many cases, diffusion models may ignore the concatenated conditioning and simply learn how to denoise. Therefore, it is important to explore whether the LPDM requires the use of conditioning to achieve the desired results. ULPDM is an identical model to LPDM from \cref{sec:experiment:implementation}, however, we change the input layer to accept only $\bm{x}_t$ as input, thus changing the number of input channels from six to three. In other terms, we compare conditional diffusion to unconditional diffusion. 

The experimental results in \cref{tab:ablation-fixt} show that LPDM significantly outperforms ULPDM across all metrics. Therefore, we conclude that conditioning is necessary in order for the LPDM to detect the wide variety of artifacts that can be present in $\bm{\hat{x}}_0^\eta$. We provide visual results in \cref{fig:ablation-uncond} which verify our conclusion: ULPDM is able to remove noise, however results are oversmoothed and thus detail is lost due to lack of conditioning.

\begin{figure}
    \setlength\tabcolsep{2pt}%%
    \small
    \centering
    \begin{tabular}{cccc}
     Input &
    %  $\bm{\hat{x}}_0^{\textnormal{LIME}}$ &
     LIME \cite{ref:LIME} &
     $\textnormal{ULPDM}_{15}$ &
     $\textnormal{LPDM}_{15}$
     \\
     \includegraphics[width=0.8in]{figures/uncond_compare/3/input/03} &
     \includegraphics[width=0.8in]{figures/uncond_compare/3/pred/03} &
     \includegraphics[width=0.8in]{figures/uncond_compare/3/uncond15/03} & 
     \includegraphics[width=0.8in]{figures/uncond_compare/3/cond15/03} \\ 
     Input &
    %  $\bm{\hat{x}}_0^{\textnormal{LLFlow}}$ &
     LLFlow \cite{ref:llflow} &
     $\textnormal{ULPDM}_{15}$ &
     $\textnormal{LPDM}_{15}$
     \\
     \includegraphics[width=0.8in]{figures/uncond_compare/4/input/8} &
     \includegraphics[width=0.8in]{figures/uncond_compare/4/pred/8} &
     \includegraphics[width=0.8in]{figures/uncond_compare/4/uncond15/8} & 
     \includegraphics[width=0.8in]{figures/uncond_compare/4/cond15/8}
    \end{tabular}
    \caption{Visual examples of the ablation study comparing unconditional and conditional diffusion using the ULPDM and LPDM models, respectively.
    }
    \label{fig:ablation-uncond}
  \end{figure}

\section{Conclusion}
\label{sec:conclusion}
In this paper, we present a framework for post-processing images which have undergone low-light image enhancement. The enhancement of low-light images often reveals a variety of degradations which are hidden in the dark, and thus a need for post-processing is introduced. Furthermore, each low-light enhancement technique can possibly introduce a different form of degradation into its result. We propose using a conditional diffusion model in order to model the distribution between under-exposed and normally-exposed images. Further, we introduce a method of applying the diffusion model as a post-processing technique. Our approach uses the diffusion model to estimate the amount of noise present in an enhanced image in one pass through the model, which can simply be subtracted from the enhanced image to further enhance the image. Moreover, we demonstrate that our approach outperforms competing post-processing denoisers, and we demonstrate its versatility on a variety of low-light datasets with different state-of-the-art low-light image enhancement backbones. In contrast to existing denoisers, we find that our approach is able to improve perceptual quality, while removing noise and other distortions. In future work, our approach could potentially be applied to other image restoration domains.

% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%

% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi

This study was supported by the National Research Foundation (NRF), South Africa, Thuthuka Grant Number 13819413. The authors acknowledge the Centre for High Performance Computing (CHPC), South Africa, for providing computational resources to this research project.   


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEabrv, references}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
% \begin{thebibliography}{1}

% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to {\LaTeX}}, 3rd~ed.\hskip 1em plus
%   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

% \end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% \begin{IEEEbiography}{Savvas Panagiotou}
% Biography text here.
% \end{IEEEbiography}

% if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{Anna Bosman}
% Biography text here.
% \end{IEEEbiographynophoto}


% insert where needed to balance the two columns on the last page with
% biographies
%\newpage


% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


