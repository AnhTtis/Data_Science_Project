\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\input{ourcommands}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \iccvfinalcopy % *** Uncomment this line for the final submission
\iccvfinalcopy
%\def\iccvPaperID{2483} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
%\title{Leaning Implicit Shape Correspondence Fields for Category-level 6D Pose and Size Estimation}
\title{SOCS: Semantically-aware Object Coordinate Space for Category-Level\\6D Object Pose Estimation under Large Shape Variations}

\author{Boyan Wan\footnotemark[1] \qquad Yifei Shi\thanks{Joint first authors} \qquad Kai Xu\thanks{Corresponding author} \\
National University of Defense Technology\\
{\tt\small wanboyan@163.com \{yifei.j.shi,   kevin.kai.xu\}@gmail.com}
}
\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
Most learning-based approaches to category-level 6D pose estimation are design around normalized object coordinate space (NOCS). While being successful, NOCS-based methods become inaccurate and less robust when handling objects of a category containing significant intra-category shape variations. This is because the object coordinates induced by global and rigid alignment of objects are semantically incoherent, making the coordinate regression hard to learn and generalize. We propose Semantically-aware Object Coordinate Space (SOCS) built by warping-and-aligning the objects guided by a sparse set of keypoints with semantically meaningful correspondence. SOCS is semantically coherent: Any point on the surface of a object can be mapped to a semantically meaningful location in SOCS, allowing for accurate pose and size estimation under large shape variations. To learn effective coordinate regression to SOCS, we propose a novel multi-scale coordinate-based attention network. Evaluations demonstrate that our method is easy to train, well-generalizing for large intra-category shape variations and robust to inter-object occlusions.

%Category-level 6D pose estimation is one of the fundamental problems in computer vision community. Most of the recent research is based on the representation of the normalized object coordinate space, in which the 6D pose of the unseen instances can be defined and therefore estimated. Despite the progress, it is unsatisfactory when it comes to novel instances with large shape variations \ys{as well as symmetric objects}. In this paper, we propose shape correspondence field, a new representation of the canonical coordinate system for category-level 6D pose estimation. The shape correspondence field is generated by the category-specific keypoints, enabling the learning of pose estimation for complicated categories. Moreover, we develop a category-level 6D pose estimation method based on the shape correspondence field. The network is implemented with an implicit aggregation-querying network, facilitating the outputs at locations besides the input points. %The proposed pose estimation method is able to handle instances with novel geometry and discrete rotational symmetry. Extensive evaluations demonstrate the effectiveness of the proposed representation and the pose estimation method. In particular, our method achieves state-of-the-art on a new dataset of objects with novel geometry and complex symmetry. Moreover, it achieves competitive performance on the CAMERA25/ REAL275 dataset.
\end{abstract}

\input{intro}
\input{related}
\input{method}
\input{result}
\input{conclusion}



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document} 