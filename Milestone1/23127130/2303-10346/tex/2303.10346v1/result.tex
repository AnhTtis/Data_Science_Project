
\section{Results and Evaluation}
\label{sec:result}



\subsection{Experimental Datasets}
We train and test our method on the NOCS-REAL275~\cite{wang2019normalized} and ModelNet40-partial~\cite{li2021leveraging} datasets.
The NOCS-REAL275 contains $4.3$k training RGB-D images and $2.75$k testing RGB-D images captured from $6$ real-world scenes. The objects belong to six object categories: bottle, bowl, can, camera, laptop, and mug.
The ModelNet40-partial dataset is a synthetic dataset that contains $60$k training depth images and $6$k testing depth images. It contains object categories with large shape variations, such as \emph{airplane}, \emph{chair}, and \emph{sofa}.



\subsection{Evaluation Metrics}
We use standard metrics to evaluate the performance on the two datasets, respectively.
For NOCS-REAL275, we adopt the intersection over union (IoU) with a threshold of $e$, and the average precision of instances for which the error is less than $n^{\circ}$ for rotation and $m$ for translation.
For ModelNet40-partial, we report the rotational error, and the translational error in the form of mean, and median values. We also report the average precision of instances for which the error is less than $5^{\circ}$ for rotation and $5$cm for translation.



\begin{table*}[!ht]
\renewcommand{\arraystretch}{1.3}
\centering
    \caption{Quantitative results on the NOCS-REAL275 dataset.}
\begin{tabular}{cccccccccc}
\hline
Methods & Data type & Data source & IoU50 $\uparrow$& IoU75$\uparrow$
&$5^{\circ}$2cm$\uparrow$ & $5^{\circ}$5cm$\uparrow$ & $10^{\circ}$2cm$\uparrow$ & $10^{\circ}$5cm$\uparrow$ \\
\hline
          NOCS~\cite{wang2019normalized} & RGB & Syn.+Real & 0.78 & 0.30 & 0.07 & 0.10 & 0.14 &  0.25\\
          SGPA~\cite{chen2021sgpa} & RGB-D & Syn.+Real & 0.80 &0.62& 0.36 & 0.40 & 0.61 & 0.71   \\
          Self-DPDN~\cite{lin2022category} & RGB-D & Syn.+Real & \textbf{0.83} & \textbf{0.76}& 0.46 & 0.51 & 0.70 & 0.78   \\
          GPV-Pose~\cite{di2022gpv} & D & Real  & 0.83 & 0.64 &  0.32 &
          0.43 & - & 0.73 \\
          RBP-Pose~\cite{zhang2022rbp} & D & Real & 0.83 & 0.68 & 0.38 & 0.48 & 0.63 & 0.79  \\
          \hline
          Network in~\cite{wang2019normalized} + SOCS est. & RGB & Real  & 0.79 &0.41 &0.11 &0.12 & 0.15 & 0.30    \\
          Our network + NOCS est. & D & Real & 0.82 & 0.73 &0.40 & 0.49 & 0.64 & 0.81   \\
          Ours  & D & Real  & 0.82 & 0.75 & $\textbf{0.49} $ & \textbf{0.56} &    \textbf{0.72} & \textbf{0.82}  \\
\hline
\end{tabular}
\label{tab:nocs_real}
\end{table*}




\begin{table*}[!t]
\renewcommand{\arraystretch}{1.3}
\centering
    \caption{Quantitative results on the ModelNet40-partial dataset.}
\begin{tabular}{cccccc|ccc}
\hline
\multirow{2}{*}{Methods} & \multirow{2}{*}{Data type} &\multirow{2}{*}{Data source} & \multicolumn{3}{c|}{Rotation} & \multicolumn{3}{c}{Translation} \\
\cline{4-9}
& & &Mean($^{\circ}$) $\downarrow$ & Median($^{\circ}$)$\downarrow$ & $5^{\circ}$ $\uparrow$ & Mean()$\downarrow$ &Median()$\downarrow$ & $5^{\circ}$0.05 $\uparrow$ \\
\hline
EPN~\cite{li2021leveraging} & D & Syn. & 32.86 & 23.84 & 0.49 & 0.14 & 0.13 & 0.08 \\
KPConv~\cite{li2021leveraging} & D & Syn. & 37.48 & 30.86 & 0.24 & 0.11 & 0.08 & 0.06 \\
GPV-Pose~\cite{di2022gpv}&  D & Syn. & 30.75 & 30.41 & 0.28 & 0.17 & 0.11 & 0.06 \\
RBP-Pose~\cite{zhang2022rbp} & D  & Syn. & 33.09 & 35.25 & 0.26 & 0.08 & 0.13 & 0.10 \\
\hline
%NOCS(ours) &D  & Syn. & 27.26 & \textbf{20.77} & 0.55 & 0.06 & 0.09 & 0.19 \\
Ours&D  & Syn. & \textbf{22.53} & \textbf{22.81} & \textbf{0.59} &\textbf{ 0.03 }& \textbf{0.07} & \textbf{0.26} \\
 \hline
\end{tabular}
\label{tab:modelnet40}
\end{table*}


\subsection{Performance on NOCS-REAL275}
We first compare our method with the state-of-the-art on the NOCS-REAL275 dataset.
The quantitative results are shown in Table~\ref{tab:nocs_real}.
There are several phenomena we can observe.
First, Self-DPDN~\cite{lin2022category} slightly outperforms our method on metrics of IoU50 and IoU75, showing that their method is better than ours in terms of object detection.
Second, our method outperforms all the baselines on metrics of $5^{\circ}2$cm, $5^{\circ}5$cm, $10^{\circ}2$cm, $10^{\circ}5$cm, demonstrating the effectiveness of our method on pose estimation despite the inferiority on object detection.
In particular, to further study the effectiveness of the proposed SOCS and the proposed network, we replace each of them with NOCS and the network in~\cite{wang2019densefusion} respectively (i.e. the baseline of Network in~\cite{wang2019normalized} + SOCS est. and Our network + NOCS est.), and conduct experiments.
The results show that our full method is better than the two baselines, revealing the necessity of both the SOCS and the proposed network.
We also found our method requires less training time compared to the baseline of Our network + NOCS est., demonstrating SOCS is easy to train compared to NOCS.
The qualitative comparisons to the state-of-the-art are visualized in \Fig{qual}.

%Third, instead of the RGB-D images, our method only takes depth images as input, showing the effectiveness of our method in terms of aggregating geometric features.


\subsection{Performance on ModelNet40-partial}
To demonstrate the performance of our method under \emph{large shape variations}, we conduct experiments on the ModelNet40-partial dataset. The results are reported in Table~\ref{tab:modelnet40}.
We see that our method outperforms all baselines by a large margin over all the metrics, which suggests that our method is much more effective in handling categories with large shape variations.
Moreover, to quantitatively analyze how our method performs under different shape variations, we conduct an additional experiment.
Specifically, we generate several subsets of the $lamp$ category in the ModelNet40 dataset with different degrees of shape variations.
The degree of shape variations is computed as the average chamfer distance between every shape instance and the categorical mean shape.
The results are visualized in \Fig{var_vis}.
It shows that our method is able to handle object instances with different degrees of shape variations, while the baselines cannot.



\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\centering
\caption{Ablation studies of the key components.}
\begin{tabular}{ccccccc}
\hline
  & MP & GP & CL & Sampling & IoU75$\uparrow$ &  $10^{\circ}$2cm$\uparrow$ \\
\hline
$A_1$ &  - & \checkmark  & \checkmark & \texttt{SI} & 0.66& 0.61  \\
$A_2$ &  \checkmark & -  & \checkmark  & \texttt{SI}  & 0.70 & 0.65 \\
    $A_3$ & \checkmark & \checkmark  & - &  \texttt{SI} & 0.72 & 0.67 \\
\hline
$B_1$ & \checkmark &  \checkmark & \checkmark&  \texttt{P}  & 0.67 & 0.62 \\
$B_2$& \checkmark &  \checkmark & \checkmark&  \texttt{SD} & 0.66 & 0.63\\
    \hline
Ours& \checkmark &  \checkmark & \checkmark&  \texttt{SI} & \textbf{0.75} & \textbf{0.72} \\
\hline
\end{tabular}
\label{tab:ablation}
\end{table}




\subsection{Ablation Studies and Parameter Setting}
In Table~\ref{tab:ablation}, we conduct ablation and parameter setting studies to quantify the efficacy of the key components in our method.
In Table~\ref{tab:keypoints} and \ref{tab:bins}, we study the key parameter settings.
All the experiments are conducted on the NOCS-REAL275 dataset.

%\paragraph{SOCS vs. NOCS.}
%We first study whether the proposed SOCS is a more proper representation compared to NOCS. To achieve this, we replace SOCS with NOCS, and use our network to estimate the per-point NOCS, and adopt the Umeyama algorithm to generate the object pose and size. It shows that this baseline (i.e. \emph{Ours with NOCS}) is inferior to our method, demonstrating SOCS is indeed helpful.

\paragraph{Network architecture.}
We then study the necessity of crucial network modules, i.e. the multi-block feature propagation (MP), the global position encoding in cross-attention (GP), and the consistency loss function (CL). In the experiment, we remove these crucial modules respectively, retrain the networks, and evaluate the performances.
Note that, in the ablation baseline of MP, we only perform feature propagations at the last block, so it has no multi-block contextual features.
The experiments show that adding any of the modules would lead to a performance improvement, confirming the effectiveness of these network modules.

\input{figures/var_vis}

\paragraph{Sampling strategy.}
We have discussed the advantages of the surface-independent sampling (\texttt{SI}) strategy in Section~\ref{sec:method}. Here, we quantitatively compare it with the alternatives of sampling from the input points (\texttt{P}) and surface-dependent sampling (\texttt{SD}). We see that the network trained by the surface-independent sampling strategy outperforms the rest. Moreover, we visualize the per-point SOCS estimation error in a cross-section in \Fig{error_vis}. It is clear that the estimation in most of the unseen regions is as accurate as that near the observed surface, showing the necessity of surface-independent sampling and the efficacy of our feature propagation mechanism.



\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\centering
\caption{Effect of different numbers of keypoints. }
\begin{tabular}{ccccc}
\hline
  $\#$keypoints & IoU75$\uparrow$&$5^{\circ}$2cm$\uparrow$ &$5^{\circ}$5cm$\uparrow$  & $10^{\circ}$2cm$\uparrow$ \\
\hline
8 &  0.72 &0.42&0.50 & 0.64  \\
   16 & 0.72 &0.46& 0.54& 0.68 \\
   32 & 0.75 &\textbf{0.49}&\textbf{0.56} & \textbf{0.72}  \\
   64 & \textbf{0.75}  &0.47& \textbf{0.56}& 0.68 \\
        \hline
   32 (ISRP~\cite{chen2020unsupervised}) & 0.71 & 0.42&  0.48& 0.65  \\
\hline
\end{tabular}
\label{tab:keypoints}
\end{table}
\vspace{-8pt}

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\centering
    \caption{Effects of different numbers of classification bins.}
\begin{tabular}{ccccc}
\hline
$\#$bins  & IoU75 $\uparrow$ & $5^{\circ}$2cm$\uparrow$& $5^{\circ}$5cm$\uparrow$
& $10^{\circ}$2cm $\uparrow$ \\
\hline
32& 0.70 & 0.44&0.52 & 0.61 \\
64 & 0.71 & 0.47& 0.55& 0.64 \\
128& \textbf{0.75} & \textbf{0.49}& \textbf{0.56} & \textbf{0.72} \\
256 & 0.73 &\textbf{0.49}& 0.55 & \textbf{0.72}  \\
        \hline
        Regression & 0.69 &0.43&0.50 & 0.66  \\
\hline
\end{tabular}
\label{tab:bins}
\end{table}
\vspace{-8pt}

\paragraph{Number of keypoints.}
The number of keypoints is a crucial parameter that has the potential to influence the effects of SOCS.
We conduct several experiments using different numbers of keypoints to generate the SOCS and retrain our network.
As reported in Table~\ref{tab:keypoints}, we see that using a relatively small number of keypoints would lead to a significant performance decrease. The reason might be that an insufficient number of keypoints would lead to inaccurate dense correspondence between object instances.
We also tried to adopt an alternative keypoints extraction method, i.e. ISRP~\cite{chen2020unsupervised}, instead of Skeleton Merger~\cite{shi2021skeleton}.
Results show the alternative key-point extraction method is also applicable to our method but will lead to inferior performances, implying that the quality of keypoints is crucial to our method.

%说明实验和结果，增加分析；说明和其它关键点方法的对比

\paragraph{Number of classification bins.}
We conduct a comparison with baselines that use different numbers of bins in the coordinate classification, as well as a baseline that replaces the classification with regression.
As reported in Table~\ref{tab:bins}, we found that using classification is a better choice compared to using regression.
Besides, the performances reach their peak when the number of classification bins is $128$ or $256$, showing that our method is able to be compatible with a relatively large number of classification bins. This reveals that SOCS indeed simplifies and facilitates the network training.


\subsection{Performance under Occlusion}
Our method is designed to handle moderate inter-object occlusions. In order to verify this, we evaluate our method and compare it to the state-of-the-arts on a subset containing objects with heavy occlusions. To be specific, we select $500$ RGB-D images with instances that have been heavily occluded ($\leq 30\%$ object surface can be observed) from the NOCS-REAL275 dataset. The visualization of the examples in this subset is provided in the supplemental material. In Table~\ref{tab:occlusion}, we see that our method outperforms the state-of-the-arts by a large margin, verifying the ability of our method in terms of handling occlusions.


\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\centering
\caption{Comparisons under heavy occlusion.}
\begin{tabular}{ccccccc}
\hline
Method  & IoU75 $\uparrow$ &$5^{\circ}$2cm$\uparrow$&$5^{\circ}$5cm$\uparrow$& $10^{\circ}$2cm $\uparrow$ \\
\hline
RBP-Pose~\cite{zhang2022rbp} & 0.60  &0.29& 0.39& 0.42 \\
Self-DPDN~\cite{lin2022category} & 0.61&0.29&0.40 & 0.47  \\
Ours  & \textbf{0.66} &\textbf{0.38}& \textbf{0.51}&  \textbf{0.55}  \\
\hline
\end{tabular}
\label{tab:occlusion}
\end{table}
\vspace{-8pt}



\input{figures/error_vis} 