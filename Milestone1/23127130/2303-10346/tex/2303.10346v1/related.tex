\section{Related Work}
\label{sec:related}

\subsection{Category-level pose estimation}
Category-level pose estimation aims to predict the pose of unseen instances from a single-view image without knowing their 3D model.
Existing work could be roughly classified into direct regression and correspondence-based methods.
Direct regression methods estimate object pose by extracting pose-sensitive features from the input~\cite{zhang2022rbp,lin2022category,deng2022icaps,liu2022catre,wang20206,wen2021bundletrack}.
The recent research focuses on exploring advanced network architectures~\cite{chen2020learning}, proper learning schemes~\cite{di2022gpv}, and different output representations~\cite{chen2021fs}.
Crucially, DualPoseNet~\cite{lin2021dualposenet} adopts two parallel pose decoders on top of a shared pose encoder, learning the consistency between the two brunches to impose complementary supervision.
FS-Net~\cite{chen2021fs} proposes a decoupled rotation output mechanism to complementarily estimate the rotation components.
Correspondence-based methods first estimate the correspondence between the observed points and its coordinate in the canonical space and then optimize pose and size by postprocessing. This requires methods to extract pose-invariant point features.
Wang et al.~\cite{wang2019normalized} present the representation of NOCS to enable the learning of pose for unseen objects.
Wen et al.~\cite{wen2022catgrasp} introduce NUNOCS, which allows non-uniform scaling across three dimensions, facilitating fine-grained dense correspondences across object instances with large shape variations.
Crucially, a bunch of recent works have adopted the categorical mean shape to facilitating the computation of correspondences between the observed points and their canonical coordinate~\cite{tian2020shape,chen2021sgpa,lin2022sar}.
Our method falls into the category of correspondence-based methods. However, it is different from previous work as it learns semantically-aware dense correspondences, resulting in more accurate results.


\subsection{Implicit field for pose estimation}
Many recent works have investigated implicitly representing 3D shapes with a continuous and differentiable implicit field implemented by neural networks.
While most of the research in this field focuses on shape reconstruction, a handful of methods adopt implicit fields to estimate object pose~\cite{peng2022self,agaram2022canonical}.
A straightforward way is to jointly reconstruct the object surface and estimate its pose~\cite{bruns2022sdfest,pavllo2022shape,li2022generative} with a unified framework.
For example, ShAPO~\cite{irshad2022shapo} jointly predicts object shape, pose, and size in a single-shot manner.
DISP6D~\cite{wen2022disp6d} disentangles the latent representation of shape and pose into two sub-spaces, improving the scalability and generality.
Neural Radiance Fields (NeRF)~\cite{mildenhall2021nerf} provides a mechanism for capturing complex 3D structures from only one or a few RGB images, which is also applicable to object pose estimation.
iNeRF~\cite{yen2021inerf} estimates pose for objects with complex geometry with a pre-trained NeRF model.
NeRF-Pose~\cite{li2022nerf} first reconstructs the object with NeRF and then estimates the object pose.
Unlike the traditional correspondence-based methods which predict 3D object coordinates at pixels of the input image, Huang et al.~\cite{huang2022neural} predict canonical coordinates at any sampled 3D in the camera frustum, generating continuous neural implicit fields of canonical coordinates for instance-level pose estimation.
Despite the similarity in the general concept, our method tackles the problem of category-level pose estimation where semantically-ware cross-instance correspondences need to be estimated.  