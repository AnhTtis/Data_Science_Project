%[figure1, simple line model in 3d space, visual comparison against sota]

\section{Method}
\label{sec:method}


\paragraph{Overview.}
In this section, we first describe how to generate SOCS. Then, we present the multi-scale coordinate-based attention network for SOCS estimation. In particular, a surface-independent point sampling strategy and a pose-invariant feature extraction training scheme are introduced. Last, we elaborate on the details of network inference and pose estimation.

\subsection{SOCS}
\label{sec:seal_nocs}
Existing canonical coordinate spaces for category-level 6D pose estimation, such as NOCS~\cite{wang2019normalized}, are induced by global and rigid alignment, leading to semantic incoherency on the object coordinates.
When handling object categories containing significant shape variations, NOCS-based methods become inaccurate and less robust.
We introduce Semantically-aware Object Coordinate Space (SOCS) to alleviate the problem of NOCS.
The coordinates in SOCS are generated by the category-specific keypoints, allowing fine-grained non-rigid coordinate alignment.

%The shape correspondence field is a category-specific geometry field that maps a coordinate $x$ in the object coordinate space to a coordinate $x^a$ in a canonical coordinate space: $x^a=f(x)$.
%The canonical coordinate space is shared by all objects in the category.


Specifically, given the shapes $\{S_i\}$ of a category in the training set, we first generate the categorical mean shape $S_a$ by using the pre-learned autoencoder~\cite{tian2020shape}.
%$\{S_i\}$ and $S_a$ are then transformed into the NOCS [?].
The coordinates of $S_a$ are regarded as the coordinates of SOCS.
To build correspondence between the object coordinate of any object instance $\{S_i\}$ and the SOCS, we detect the semantically consistent keypoints $\{K_i\}$ and $K_a$ for $\{S_i\}$ and $S_a$, respectively, by using the Skeleton Merger~\cite{shi2021skeleton}.
We denote the detected keypoints $K_i = \{k_j\}$, $j\in[1,m]$ and $K_a = \{k_j^{a}\}$, $j\in[1,m]$.
$m$ is the number of keypoints in a single shape.

Next, we compute the dense correspondence between $S_i$ and $S_a$ by considering the alignment of the semantically consistent keypoints. This is achieved by a 3D thin plate spline warping function~\cite{duchon1977splines}:
\begin{equation}\label{eq:sdf_ratio}
\begin{aligned}
\Phi(x)=c+b^{T} x+w^{T} &\mathbf{s}(x),\\
\mathbf{s}(x)=[\sigma\left(x-k_1^a\right), \sigma\left(x-k_2^a\right), \cdots, & \sigma\left(x-k_m^a\right)]^{{T}},\\
\sigma(x)=\|x\|_2^2 \cdot \log \|x&\|_2,
\end{aligned}
\end{equation}
where $c \in \mathbb{R}^{3}$, $b \in \mathbb{R}^{3 \times 3}$, $w \in \mathbb{R}^{m \times 3}$ are the parameters which are determined by optimizing the following function:
\begin{equation}
min \sum_{j=0}^{m}\left \|k_j^{a} - \Phi(k_j)\right \|^2.
\end{equation}
Once the parameters $c$, $b$, and $w$ are determined, for any coordinate $x$ in the object coordinate space, its SOCS could be computed as $x^{a}=\Phi(x)$.

Compared to the NOCS representation and its variants~\cite{wen2022catgrasp} developed for category-level 6D pose and size estimation, the SOCS is more semantically meaningful, thus facilitating the learning of correspondence even for objects with large shape variations.
See \Fig{heatmap} for an illustration.


\input{figures/heatmap}
%利用$K^{std}=\Phi(K)$通过最小二乘法求解唯一的系数$c \in \mathbb{R}^{3}$,$a \in \mathbb{R}^{3 \times 3}$,$w \in \mathbb{R}^{k \times 3}$.
%为了控制非刚性变形程度，在求解过程中可以在计算$\mathbf{s}(K)\in \mathbb{R}^{k \times k}$时加上一个regularization 项得到$\mathbf{s}(K)=\mathbf{s}(K)+\lambda I$,其中$I \in \mathbb{R}^{k \times k}$是单位矩阵.$\lambda$越大,刚性部分越强，非刚性变形部分越弱.
% 对于$C$中任意一点$p$,可以找到它在标准物体nocs空间$C^{std}$内的对应点$p^{std}=\Phi(p)$



%\ys{METHOD DETAILS: extract keypoints, non-rigid transformation}.
%shape correspondence field：给定义（socs:standard object coordinate system，映射函数），解释（连续的，隐含包括correspondence）
%how to learn
%See Fig. ? for an illustration.


\subsection{Training of SOCS Estimation Network}
%\subsection{Implicit aggregation-querying network}
In this section, we describe how to estimate the point-wise dense SOCS from an image.
Estimating SOCS from a single-view image is non-trivial due to the potential large shape variations and the inter-object occlusions.
To learn the mapping from input points to SOCS effectively, we propose a novel multi-scale coordinate-based attention network.
An overview of the network architecture is shown in \Fig{networks}.

%To achieve this, we propose a novel multi-scale coordinate-based attention network that takes a point cloud as input, extracts geometric features and estimates the shape correspondence field at any location of the 3D space.
%Note that the location is not necessarily to be at the input points. Instead, it could be at any sampled location around the input points.


\paragraph{Multi-scale coordinate-based attention network.}
The network contains two main components: \emph{aggregation layers} and \emph{propagation layers}.
The \emph{aggregation layers} extract per-point features from the point cloud.
The point cloud is cropped from the depth image of the detected object.
Since the task of category-level pose estimation could be challenging due to the large shape variations and severe occlusion of the input point cloud, we take 3D-GCN~\cite{lin2020convolution}, which is able to aggregate contextual information of 3D point clouds with good performance, as the backbone.
To be specific, the 3D-GCN takes the point cloud $\mathcal{P} \in \mathbb{R}^{n \times 3}$ as input, generates the downsampled points $\mathcal{P}^{\alpha}$ and extracts the features $\mathcal{F}^{\alpha}$ at the $\alpha$-th block, where $\alpha\in[1,5]$.
Note that the aggregation layers could be any other 3D point-based network backbone according to the practical requirements.
We found that 3D-GCN works best in our problem setting.

\input{figures/network}

%The goal of the aggregation stage is to generate support points $S$ and support features $F_S$.
%For point-based embedding networks, the support points $S$ are a subsample of the input point cloud $I$, depending on the downsampling strategy (e.g., farthest point sampling) of the network.
%The model can specify its embedding stage network according to the practical demand, which brings flexibility to model design.


The \emph{propagation layers} are then developed to propagate the feature from the downsampled points $\{\mathcal{P}^{\alpha}\},\alpha\in[1,5]$ to any query point $x$ and estimate its SOCS $x^a$.
Extracting feature of unseen points is a non-trivial task, due to the infinite query location in the 3D space.
The ideal extracted feature should be both context-sensitive and coordinate-sensitive.
To achieve this, we propose an implicit neural network with coordinate-based multi-scale contextual feature propagations.

We first initialize the feature vector at query point $x$ as a zero vector, i.e. $\mathcal{F}_x^0=\bm{0}$, $\mathcal{F}_x^0 \in \mathbb{R}^{h}$, where $h$ is the feature length.
For each block, we update the feature with a cross-attention module (see \Fig{attention}).
Specifically, at the $\alpha$-th block, we compute the k-nearest neighbors $\mathcal{N}^{\alpha}\in \mathbb{R}^{16\times 3}$ of $x$ from $\mathcal{P}^{\alpha}$.
We denote the feature of $\mathcal{N}^{\alpha}$ as
$\mathcal{F}^{\alpha}_\mathcal{N}\in \mathbb{R}^{16 \times h}$.
Moreover, we introduce a global point $g^{\alpha}=\text{Mean}(\mathcal{P}^{\alpha})$ with the feature being $\mathcal{F}_g^{\alpha}=\text{Mean}(\mathcal{F}^{\alpha})$, where $\text{Mean}(\cdot)$ is the element-wise averaging operation.
The global positional encoding provides contextual information, enabling our network to extract features on 3D points in the full space. The letter facilitates dense coordinate estimation even for unobserved locations, which is critical to handling inter-object occlusions.


The update term on the feature $\mathcal{F}_x^{\alpha-1}$ is estimated by considering the relations to both $\mathcal{N}^{\alpha}$ and $g^{\alpha}$:
\begin{equation}\label{equ:cross_attention}
\begin{aligned}
\Delta^{\alpha}&=\text{Softmax}\left(\frac{\left(\mathcal{F}_\mathcal{N}^{\alpha} W_k\right)\left(\mathcal{F}_x^{\alpha-1} W_q\right)^{\mathrm{T}}+r}{\sqrt{\mathrm{h}}}\right)\mathcal{F}_\mathcal{N}^{\alpha} W_v\\
&+\text{Softmax}\left(\frac{\left(\mathcal{F}_g^{\alpha} W_k\right)\left(\mathcal{F}_x^{\alpha-1} W_q\right)^{\mathrm{T}}+r_g}{\sqrt{\mathrm{h}}}\right)\mathcal{F}_g^{\alpha} W_v
\end{aligned}
\end{equation}
where $W_q$, $W_k$, $W_v$ $\in \mathbb{R}^{h \times h}$ are the learnable weights. $r \in \mathbb{R}^{k}$ denotes the influence factor of $x$ to the points in $\mathcal{N}^{\alpha}$. Each element in $r$ is computed as follows by considering the relative position between the two points:
\begin{equation}\label{equ:element}
r_i=\text{EmbLayer}(x-\mathcal{N}^{\alpha}_i),
%r_i=\text{EmbLayer}(\left\|x-\mathcal{N}^{\alpha}_i\right\|),
\end{equation}
where $\text{EmbLayer}(\cdot)$ is a two-layer MLP. Similarly, $r_g$ is computed to capture the position w.r.t. the center of input points:
\begin{equation}\label{equ:element}
r_g=\text{EmbLayer}(x-g^{\alpha}).
%r_g=\text{EmbLayer}(\left\|x-g^{\alpha}\right\|).
\end{equation}
Then, the updated feature at point $x$ is:
\begin{equation}\label{equ:addnorm}
\mathcal{F}_x^{\alpha} =\text{LayerNorm}(\mathcal{F}_x^{\alpha-1}+\Delta^{\alpha})
\end{equation}
where $\text{LayerNorm}(\cdot)$ is the layer normalization operation.


\input{figures/attention}

The extracted features at all the blocks are then concatenated: $\mathcal{F}_x=\text{Concat}(\mathcal{F}_x^1,\mathcal{F}_x^2,\mathcal{F}_x^3,\mathcal{F}_x^4,\mathcal{F}_x^5)$.
The concatenated feature is utilized to estimate the SOCS:
\begin{equation}\label{equ:estimation}
\begin{aligned}
    x^a_\text{X}=\text{Softmax}&(\text{MLP}_\text{X}(\mathcal{F}_x)),\\
    x^a_\text{Y}=\text{Softmax}&(\text{MLP}_\text{Y}(\mathcal{F}_x)),\\
    x^a_\text{Z}=\text{Softmax}&(\text{MLP}_\text{Z}(\mathcal{F}_x)),
\end{aligned}
\end{equation}
where $x^a_\text{X}$, $x^a_\text{Y}$, $x^a_\text{Z}$ are the predicted class denoting the coordinate in the axes of X, Y, Z, respectively.
$\text{MLP}_\text{X}(\cdot)$, $\text{MLP}_\text{Y}(\cdot)$, $\text{MLP}_\text{Z}(\cdot)$ represent multi-layer perceptrons.

Note that, our method is different from most existing methods where regression or classification with a small number of bins ($\text{B}<50$) for coordinate estimation are adopted.
We found that using a larger number of bins (e.g. $\text{B}=256$) in our method will not lead to the training being inefficient or failing to converge.
The advantage comes from the representation of SOCS, which greatly reduces the learning complexity.



\paragraph{Surface-independent point sampling.}
We then describe how to sample points to feed into the multi-scale coordinate-based attention network.
Several sampling strategies could be considered (see \Fig{sampling}): 1) Sampling from the input points; 2) Surface-dependent sampling: random sampling near the input points; 3) Surface-independent sampling: random sampling in the whole 3D space.
We empirically found the surface-independent sampling strategy outperforms the others, thanks to the mechanism of global positional encoding.
Please refer to the result section for experimental analysis.
There are two reasons for this phenomenon.
First, sampling in the whole 3D space facilitates feature aggregation in the invisible region, bringing more global context information.
Second, sampling in the whole 3D space would decrease the overall pose estimation uncertainty, especially in scenarios where severe occlusion exists.


\paragraph{Network training.}
%\ys{rewrite: motivation + method, \emph{Handling rotation invariant feature aggregation}}
Next, we describe how to train the above network.
Suppose $\mathcal{X}$ is the set of sampled points, a naive loss function of the shape correspondence field estimation could be:
\begin{equation}\label{equ:loss_field}
\mathcal{L}_\text{SOCS}= \sum_{x\in \mathcal{X}} [\mathcal{L}_\text{CE}(x^a_\text{X},\hat{x}^a_\text{X})+\mathcal{L}_\text{CE}(x^a_\text{Y},\hat{x}^a_\text{Y})+\mathcal{L}_\text{CE}(x^a_\text{Z},\hat{x}^a_\text{Z})],
\end{equation}
where $\mathcal{L}_\text{CE}(\cdot)$ is the cross entropy loss, $\hat{x}^a_\text{X}$, $\hat{x}^a_\text{Y}$, $\hat{x}^a_\text{Z}$ denote the ground-truth.
However, we found that training the network is unstable and hard to converge, especially on categories with large shape variations.

To alleviate this issue, we adopt a contrastive training fashion with a pose consistency loss to further enhance the training.
The key insight is to learn the \emph{pose-invariant feature} by transforming the input point cloud, extracting its per-point features, and making the features of the initial point cloud and the transformed point cloud consistent.

Specifically, during training, we transform the input point cloud $\mathcal{P}$ with a random rigid transformation $\textbf{T}_\text{r}=\{\textbf{R}|\textbf{t}\}$.
We denote the transformed point cloud as $\mathcal{P}^{'}=\textbf{T}_\text{r} \cdot \mathcal{P}$.
Then, $\mathcal{P}$ and $\mathcal{P}^{'}$ are fed into the multi-scale coordinate-based attention network, respectively, to generate the per-point features.
For any point $x\in \mathcal{X}$ and the transformed point $x^{'}=\textbf{T}_\text{r} \cdot x$, the generated features should be consistent:
\begin{equation}\label{equ:loss_con}
\mathcal{L}_\text{consistency}=\sum_{x\in \mathcal{X}} ||\mathcal{F}_x-\mathcal{F}_{x^{'}}^{'}||_2,
\end{equation}
where $\mathcal{F}_x$ and $\mathcal{F}_{x^{'}}^{'}$ denote the extracted feature by the two network towers, respectively.
Overall, the training loss function is: $\mathcal{L}=w_\text{SOCS}\mathcal{L}_\text{SOCS}+w_\text{consistency}\mathcal{L}_\text{consistency}$, where $w_\text{SOCS}$ and $w_\text{consistency}$ are the pre-defined weights.

%\begin{equation}\label{equ:loss_all}
%\mathcal{L}=\mathcal{L}_\text{field}+\mathcal{L}_\text{consistency},
%\end{equation}
%where $\mathcal{L}_\text{field}^{'}$ represents the shape correspondence field loss of the transformed point cloud $\mathcal{P}^{'}$.

\input{figures/sampling}

\paragraph{Training data preparation.}
To generate the training data of SOCS estimation, for each dataset, we first generate the dense SOCS for the complete 3D objects using the method described in Sec.~\ref{sec:seal_nocs}. The dense SOCS are then transformed into the camera coordinate with the 6D object pose.


\subsection{Network Inference, Pose and Size Estimation}
\paragraph{Network inference.}
During the inference, given an RGB-D image with untrained object instances in it, we first perform an object detection with Mask R-CNN~\cite{he2017mask}.
For each detected object, we crop the image, generate the point cloud, and feed them into the aggregation layers to generate features.
Then, we densely sample points in 3D space around the input points, and extract their features with the propagation layers, predicting the SOCS for every sampled point.


\paragraph{Pose and size estimation.}
The predicted per-point coordinate in SOCS is then transformed into the camera coordinate space with the transformation of the 6D object pose and a scaling operation.
The ideal transformation matrix $\textbf{T}\in \mathbb{R}^{4 \times 4}$ of the pose and the scaling matrix $\textbf{S}=\text{diag}(s_\text{X},s_\text{Y},s_\text{Z},1)$ should make the following function optimized:
\begin{equation}
min \sum_{x\in \mathcal{X}} \left \|\textbf{T} \cdot \textbf{S} \cdot \Phi(x)-x \right \|^2,
\end{equation}
where $\mathcal{X}$ represents the sampled points. Note that the scaling matrix $\textbf{S}$ is anisotropic, so it allows more flexible and accurate size estimation compared to NOCS whose scaling matrix is isotropous.



\input{figures/qual}

\subsection{Implementation details}
We detected $32$ keypoints on each object.
The multi-scale coordinate-based attention network takes $1,024$ points as input.
The number of classification bins is $128$.
The network is optimized by a ranger optimizer, with batch size $16$ and learning rate $0.001$. The learning rate is annealed at $50\%$ of the training phase using a cosine schedule. We train individual models for each category respectively.
In the surface-independent sampling, we randomly sample points in a sphere with the center being the center of input points and its diameter being the diagonal length of the largest shape in the category.
We set $w_\text{SOCS}$ as $1$ and $w_\text{consistency}$ as $0.1$.


%$\{S_i\}$ and $S_a$ are then transformed into the NOCS [?].
%\textbf{Keypoints issue.}
% can not guarantee the correspondence
% add an experiment