

%%%We would like to do operations on individual tensors/arrays, only resorting to the inter-products when needed. 

%Because we wish to connect to computer-science 


%We would like dense-operations of these 3 arrays to...

%Here we consider multidimensional arrays

%We may represent data into 3 data-types: arrays (dense), block-sparse, and sparse arrays. Here we introduce algorithms to compute to compute the partial-trace of between generic sparse-arrays and between sparse-to-dense arrays.

%For each partial-trace, the representation

\section{Introduction}

Computational arrays are ubiquitous throughout science, they are used for data-storage, and more recently for data-processing as well. In many instances the data to-be-stored or processes has the characteristic of \textit{sparsity}. From a computational perspective, this means many entries are identically zero. The use of sparse-matrices has been used for a while, but only recently with the advent of machine-learning with applications such as \cite{tensorflow} and \cite{PyTorch}, has the \textit{sparse-tensor} been defined. However, noticeably, the partial-tracing algorithms used in so many computational studies are not extended to this data structure: sparse-tensor/sparse-array. In this paper we remedy this absence of capability. This is at least one theoretically, as the implemented is on a high-level-computing-language python, and may be accessed via the python-index via the command-line command: \texttt{pip install pyftt} (ftt is a tentative name: \textit{fast-tensor-trace}, \textit{fast} because it scales linearly on the number of sparse elements, the name is playing off the fft-acronym). Results demonstrates an implementation which may outrun \cite{numpy}'s \texttt{einsum} for sparse-arrays, whose performance is directly-related it the aforementioned array's sparsity.



\section{Canonical Format \label{canonical}}

\subsection{well-ordering}




Sparse-arrays can only faithfully represent dense-arrays (bijectively) iff their row-wise elements of the index-array are unique.
In order to verify that this is the case for an arbitrarily defined index-array, they are put canonically sorted into lexicographic-order (refer to appendix \S\ref{lexorder}) to ensure they are in fact \textit{well-ordered}.
\textit{Well-ordering} is a strictly ordered/sorted list, \text{i.e.} using the relation $<$.
Such that for-all entries $a, b \in A$ ($A$ being a list/array) with element indices $i$ and $j$ respectively, then $a[i] < b[j]$ for $i<j$.
A weaker statement is \textit{partial-well-order}, this indicates the possibly for duplicates, while retaining lexicographic-order, \textit{i.e.} $\le$.
We will refer to \textit{well-ordering} an array, when discussing summation over duplicate tuple elements, while retaining lexicographic-order.

%A 1-dimensional arbitrarily-ordered multi-set is known as a \textit{list}.

%%%Therefore sparse-arrays are only 1-1 matched with dense-arrays if they can be well-ordered.


\subsection{Composition, Sub-indexing, and Permutations}

Given two 1-dimensional arrays $A$ and $B$, we can compose/subindex one by the other. Mathematically, this is written over functions with $A\circ B$, alternatively we may use a square-bracket notation, e.g. $A[B]$. If $B$ can be bijectively mapped to $A$, all entries in $B$ are unique, then this is called a \textit{permutation}. Often to sort arrays, say $A$, we determine the permutation array, $B$, such that when composed, $A[B]$, the result is ordered. This is useful when talking about sparse-arrays, because of their interconnected array structure.




\subsection{array labeling \label{labelz}}

%As mentioned in the introduction, \S\ref{npeinsum}, 
We may associate a character/letter to every axis of a tensor/array. For instance, without-loss-of-generality if $A$ is a 4-dimensional array, then $A[i,j,k,\ell]$ denotes a labeling of $A$, for labels \{$i,j,k,\ell$\}. These characters are completely arbitrary, and are merely used to match certain indices/axës to others to create edges (or hyperedges more generally, such that every edge is associated to a type of character). %Every hyperedge is associated to each character. 

%%%%\circles{4}



\subsection{cartesian-product}

The \textit{cartesian-product} is all the possible 2-tuples created by all entries in $A$ (1st tuple entry), and all the entries in $B$ (2nd tuple entry). 
The cartesian-product may be generalized to products of $n$ lists-of-tuples to obtain a list of $n$-tuples of tuples. The tuple-of-tuples may be straightforwardly reduced to a larger order $m$-tuple, whereby $m>n$. Further, details of this operation are given in \S5 of \cite{Munkres}.
Originally, an operation in set-theory, when applied to lists/arrays they maintain \textit{wellorderedness} iff both original arrays were well-ordered.
%However, the tuple/ordered-pair character is preserved. 
%\subsubsection{cartesian-product correction}
If we consider the cartesian-product on partially-ordered arrays. Their cartesian-product is neither well-ordered nor partially-ordered. Naïvely we would hope that the product could produce a partially-ordered list-of-tuples. There is a permutation correction without having to resort to a naïve sort.


\subsection{direct-product}

\subsubsection{dense arrays/tensors}

Perhaps the most general operation on arrays is the direct-product (\textit{a.k.a} tensor or Kronecker-product), it is the product of all entries in one array to the other. The result is a new tensor/array with a rank equal to the sum of the constituent ranks. An example of this operation over two 2-dimensional arrays:
\begin{align*}
    A_{\mu\nu} &= 
    \begin{pmatrix}
    A_{00} & A_{01} & \cdots & A_{0n} \\
    A_{10} & A_{11} & \cdots & A_{1n} \\
    \vdots & \vdots & \ddots & \vdots \\
    A_{m0} & A_{m1} & \cdots & A_{mn} \\
    \end{pmatrix} &
    B_{\kappa\lambda} &= 
    \begin{pmatrix}
    B_{00} & B_{01} & \cdots & B_{0\ell} \\
    B_{10} & B_{11} & \cdots & B_{1\ell} \\
    \vdots & \vdots & \ddots & \vdots \\
    B_{k0} & B_{k1} & \cdots & B_{k\ell} \\
    \end{pmatrix} \quad .
\end{align*}
Then the direct-product, $\otimes$, is explicitly defined as:
\begin{align*}
    A_{\mu\nu} \otimes B_{\kappa\lambda} &= 
    \begin{pmatrix}
    A_{00} \begin{pmatrix}
    B_{00} & B_{01} & \cdots & B_{0\ell} \\
    B_{10} & B_{11} & \cdots & B_{1\ell} \\
    \vdots & \vdots & \ddots & \vdots \\
    B_{k0} & B_{k1} & \cdots & B_{k\ell} \\
    \end{pmatrix} & \cdots & \cdots & A_{0n} \begin{pmatrix}
    B_{00} & B_{01} & \cdots & B_{0\ell} \\
    B_{10} & B_{11} & \cdots & B_{1\ell} \\
    \vdots & \vdots & \ddots & \vdots \\
    B_{k0} & B_{k1} & \cdots & B_{k\ell} \\
    \end{pmatrix} \\
    \vdots & \ddots & \ddots & \vdots \\
    \vdots & \ddots & \ddots & \vdots \\
    A_{m0} \begin{pmatrix}
    B_{00} & B_{01} & \cdots & B_{0\ell} \\
    B_{10} & B_{11} & \cdots & B_{1\ell} \\
    \vdots & \vdots & \ddots & \vdots \\
    B_{k0} & B_{k1} & \cdots & B_{k\ell} \\
    \end{pmatrix} & \cdots & \cdots & A_{mn} \begin{pmatrix}
    B_{00} & B_{01} & \cdots & B_{0\ell} \\
    B_{10} & B_{11} & \cdots & B_{1\ell} \\
    \vdots & \vdots & \ddots & \vdots \\
    B_{k0} & B_{k1} & \cdots & B_{k\ell} \\
    \end{pmatrix} \\
    \end{pmatrix} \quad\quad.
\end{align*}
Symbolically this may be represented by:
\begin{align*}
    A\left[ \mu, \nu \right] \otimes B\left[ \kappa, \lambda \right] &= AB \left[ \mu, \nu , \kappa, \lambda \right]\quad\quad .
\end{align*}
The general direct-product over $n$-dimensional arrays is similarly:
\begin{align*}
    %A_{ijk} \otimes B_{\ell pq} &= \prod A_{ijk} B_{\ell pq} = C_{ijk\ell pq} \\
    A^{(0)}\left[m_0\right] \otimes A^{(1)}\left[m_1\right] \otimes \cdots \otimes A^{(n)}\left[m_n\right] &= C\left[ m_0, m_1, \cdots, m_n \right] \quad\quad.
\end{align*}
This operation may be represented by a network/graph. 
If every data-entry in $A$ is given a node, and likewise for $B$. 
The products are associated to edges connecting them, then the complete-bipartite graph captures this product. 
For the general direct-product involving $n$ tensors/arrays the resulting graph is the complete-$n$-partite graph.
\begin{align*}
    \begin{pmatrix}
        A_1 & A_2 & A_3 \\
        A_4 & A_5 & A_6 \\
        A_7 & A_8 & A_9
    \end{pmatrix} \otimes
    \begin{pmatrix}
        B_1 & B_2 & B_3 \\
        B_4 & B_5 & B_6 \\
        B_7 & B_8 & B_9
    \end{pmatrix} &\quad\Rightarrow\quad \raisebox{-23.5ex}{\bipartnn} \quad\quad.
\end{align*}


\subsubsection{sparse arrays/tensors}

Suppose we have two well-ordered $A$ \& $B$ sparse-arrays, then their direct-product is: the binary-cartesian-product over their respective list-of-tuples, with a reshaped/flattened direct-product of their data, and concatenated shape-tuple:
\begin{align*}
    A^\text{index} \times B^\text{index} &= C^\text{index} \\
    \text{reshape} \left( A^\text{data} \otimes B^\text{data} \right) &= C^\text{data} \\
    A^\text{shape} \,\,\big|\,\big|\,\, B^\text{shape} &= C^\text{shape}\quad\quad.
\end{align*}
This readily generalizes to $n$-sparse-arrays, $A^{(n)}$:
\begin{align*}
    A^{(0)}_\text{index} \times A^{(1)}_\text{index} \times \cdots \times A^{(n)}_\text{index} &= C_\text{index} \\
    \text{reshape} \left( A^{(0)}_\text{data} \otimes A^{(1)}_\text{data} \otimes \cdots \otimes A^{(n)}_\text{data} \right) &= C_\text{data} \\
    A^{(0)}_\text{shape} \,\,\big|\,\big|\,\, A^{(1)}_\text{shape} \,\,\big|\,\big|\,\, \cdots \,\,\big|\,\big|\,\, A^{(n)}_\text{shape} &= C_\text{shape}\quad\quad.
\end{align*}



\subsection{intra-intersection/slice \label{intraintersect}}

Now suppose we are given a single tensor/array, and we would like to identify some columns/axës with each-other (via labels mentioned in \S\ref{labelz}). This reduction in the array's degrees-of-freedom is called slicing, or intra-intersecting. Because these are internal operations, these are best done first, as intra-intersecting can only reduce the size.
Unfortunately, if the intersecting columns are not adjacent to each other or the intra-intersecting indices are internal, they must be re-well-ordered (at least from the first intra-intersection column/index/axis to the rest). This operation requires an element-wise search of all rows in a given sparse-array's index-array.

\section{Reshaping}

In this section we would like to reshape sparse-arrays, as is commonly done for dense-arrays.
\textit{Reshaping} is the merging or partitioning of axes for a given array.
Unlike dense-arrays, which store data contiguously in real-space memory, sparse-arrays store the data into 3-dense-arrays. Two of these dense-arrays: shape and array-of-tuples, control the structure of the data, here we introduce dense-operations on these two arrays to alter the structure of sparse-data.
The structure of sparse-data is managed by arrays-of-tuples.
For our discussion, let $A$ and $B$ be arrays-of-tuples: $A,B \subset \prod_i \Z_i$ (\textit{i.e.} Cartesian-product of many integer array-subsets of $\Z$).
Thus we would like to define a function $h$, that maps an array-of-tuples to another array-of-tuples:
\begin{align*}
    h &: A_{} \longrightarrow B_{}\quad\quad.
\end{align*}
We choose to take an intermediate step, and instead solve the simpler problems of mapping tuples-to-integers (\S\ref{tti}) and integers-to-tuples (\S\ref{itt}):
\begin{align*}
    A_{} \longrightarrow \Z_N \longrightarrow B_{} \quad\quad.
\end{align*}



Ultimately, we desire a finite-paring functions, let $\Z_N \subset \N$ such that $|\Z_N| \ne \infty$ (\textit{i.e.} $\Z_N$ has finitely many elements), then the pairing-function is:
\begin{align*}
    f &: A_{} \longrightarrow \Z_N \\
    g &: \Z_N \longrightarrow A \quad\quad,
\end{align*}
such that: $f(g(\Z_N)) = \Z_N$ or $g(f(A)) = A$.
With the total number of dense-elements being: $N = \prod_i s[i]$. And the range of integers (in order): $
    \Z_N = \begin{pmatrix}
        0 & 1 & 2 & \cdots & N-1
    \end{pmatrix}$. 
Additionally, it is worth noting that the reshape-function defined here does not rely on any kind of ordering.
There are many kinds of pairing-functions, we choose the \textit{C-language} reshaping-order
\footnote{Let $i$ denote rows and $j$ denote columns of $A$ then:
\begin{align*}
    A_{ij} = A[i,j] &=
    \begin{pmatrix}
        1 & 2 & 3 & 4 \\
        5 & 6 & 7 & 8
    \end{pmatrix} \mathop{\xrightarrow{\hspace*{1.5cm}}}^{\text{``C''-order}}
    \begin{pmatrix}
        1 & 2 & 3 & 4 &
        5 & 6 & 7 & 8
    \end{pmatrix} = A[i'] \quad\quad.
\end{align*}}.


\subsection{modified shape \label{modshape}}

For both tuples-to-integer and integer-to-tuple maps, we need to modify the shape array, $s[i]$, into an auxiliary array $\tilde{s}[i]$.
In each case the shape-array means: current-array shape xor desired-array-shape respectively. This involves permuting the shape array in a particular fashion, with a cumulative-product. 
The modification of $s[i]$ is the following\footnote{In practice, this may be done with: 
\begin{align*}
    s[0] &= 1 \\
    s[i] &= \text{flip}(s[i]) & & (s_0, s_1, \cdots, s_{n-1}, s_n) \rightarrow (s_n, s_{n-1}, \cdots, s_{1}, s_0) \\
    s[i] &= \text{roll}(s[i], 1) & &\text{permute, via right-shift} \\
    s[i] &= \text{cumulative-product}(s[i]) \\
    s[i] &= \text{flip}(s[i]) \quad\quad.
\end{align*}}: 
\begin{align*}
    s_i &= \begin{pmatrix}
        s_0, s_1, s_2, s_3, \cdots, s_{n-1} & s_n
    \end{pmatrix} \\
    \tilde{s}_i &=
    \begin{pmatrix}
        \prod_{i\in[1,n]} s_i  & \prod_{i\in[2,n]} s_i & \prod_{i\in[3,n]} s_i  & \cdots & {s}_{n-1}s_n & s_n & 1 
    \end{pmatrix}
\end{align*}
Injectivity is therefore given by the fact that this auxiliary-shape array is strictly decreasing, \textit{i.e.} $\tilde{s}[i] > \tilde{s}[j]$ for $j > i$. And surjectivity stems from the precise spacing of the auxiliary-array. Suppose we have: $a_i = \begin{pmatrix}
        a_0 & a_1 & a_2 & \cdots & a_n
    \end{pmatrix}$, with each $a_i$ value having a ceiling value of $s_i$:
\begin{align*}
    z_i &= a_0 \left( \prod_{i\in[1,n]} s_i \right) + a_1 \left( \prod_{i\in[2,n]} s_i \right) + a_2 \left( \prod_{i\in[3,n]} s_i \right) + \cdots + a_{n-1} s_n + a_n \quad\quad.
\end{align*}

\subsection{tuples-to-integers \label{tti}}

The conversion tuples-to-integers may be achieved by a dense-matrix-vector product, for a suitably modified shape ($s_i$), array-of-tuples $A^{i}_{I}$ (with $i$ the tuple-index), and array-of-integers $a_I$:
\begin{align*}
    a_I &= A^{i}_{I} \tilde{s}_{i} \quad\quad.
\end{align*}
This operation thus has time-complexity of $\mathcal{O} \sim nN $, with $n$ being the number of columns (axës), and $N$ being the number of sparse-elements, this will have linear-scaling as usually $n <\!\!< N$. 


%\caution this is a bijection PROVEvwe need to prove Injectivity (1-1 map) and surjectivity (all entries are covered.)



%(injectivity) 



\subsection{integers-to-tuples \label{itt}}

Given the modified shape-array, see \S\ref{modshape}, we may do the following dense-operations to obtain the new array-of-tuples, from an array-of-integers (or 1-tuples):
\begin{align*}
    A[I,i]   &= A^{I} \otimes \mathbf{1}^{i}   \\
    A[:,1:]  &= A[:,1:] \mod \tilde{s}[:-1] \\
    A[I,i] &= \bigg\lfloor \frac{A[I,i]}{\tilde{s}[i]} \bigg\rfloor \\
    A[i,I] &= A[I,i] \quad\quad.
\end{align*}
These include, computing the integer direct-product of an array-of-integers with an array consisting of all ones, $\mathbf{1}^{i}$, with the size of the desired tuple. The result is $A^{Ii}$ a 2-dimensional array.
Then the modulo-division (remainder) is applied along the second axis with the modified-shape array (along a subset of entries along that axis, \textit{i.e.} the first/zeroth entry of A[:,0] is ignored as is the last shape element).
Then the complementary-function, integer-division, is applied along the same axis, but this time over all elements.
Finally the resulting array is transposed to obtain the array-of-tuples into a \textit{canonical} form (axës following the sparse-array data entry index), hence $A[i,I]$.
This operation like, \S\ref{tti}, scales linearly in time: $\mathcal{O}\sim nN + nN + nN + 1 \sim 3nN$.

%\caution this is a bijection PROVE

\subsection{tuples-to-tuples}

It can be easily shown that to map a tuple into another tuple of different size, using the intermediate integer is a bijective map.
This is because of the well-known relation that the composition of bijective functions is itself bijective.


\subsection{reshaping}

Using the techniques introduced in this section, any sparse-array can be cast into a sparse-matrix. Therefore we can apply sparse-matrix algorithms to sparse-arrays.


\section{direct-intersection \label{directintersect}}

We would now like to realize the inter-intersection, the intersection of elements between arrays/tensors.
The direct-intersection is the direct-product over the intersecting elements of two (or more) tensors/arrays, and deletion of non-intersecting elements. Or a particular slice/intra-intersection of their full direct-product. 
Visually, for two arrays this is given by a disconnected-union of many complete-bipartite-graphs, each complete-bipartite-graphs acting on the intersecting elements.


\begin{figure}[h]
\begin{center}
\includegraphics[width=0.90\textwidth]{bipart.pdf}
\end{center}
\caption{ This figure shows an example of a direct-intersection (the collection of all the edges) between two arrays' (red and blue) elements (\textit{i.e.} the nodes), represented by a sum of disjoint complete bipartite-graphs. \label{bidirect} }
\end{figure}





In set-theory, the operation to determine a subset which belongs to both sets is called the intersection.
Let $A$ and $B$ be sets, then $A\cap B = C \subseteq A \,\&\, B$. 
The intersection is desirable, in our discussion, because inter-array partial-tracing only occurs over internal entries which agree in numerical value.
For arrays (sets with some arbitrary order, with potentially duplicate entries), we desire instead the indices (from each array) which contribute to the intersecting element, we desire the \textit{argument-intersection}. That is, given two arrays $A$ and $B$ and 1 intersecting element $C$ between them, $I_A$ and $I_B$ indicate the indices appearance of this element, $C$, in both $A$ and $B$. Therefore the composition/sub-index into $A$ and $B$:
\begin{align*}
    A[I_A] = B[I_B] = C \quad\quad.
\end{align*}
Now if $A$ and $B$ have multiple intersecting elements, $C^{0}, C^{1}, \cdots, C^{n}$, then we require a list-of-indices
$[I^{(0)}_A, I^{(1)}_A, \cdots, I^{(n)}_A]$, and $[I^{(0)}_B, I^{(1)}_B, \cdots, I^{(n)}_B]$ for $B$'s indices, such that $A[I_A^{(j)}] = B[I_B^{(j)}] = C^{(j)}$.
If there exists duplicates for each intersecting elements, then each $I^{(i)}_A$ are list-of-indices themselves.
Unlike the entries the list-of-indices is unique, and therefore if well-ordered their Cartesian-product will also be well-ordered. Leading to the net result of the direct-intersection, for 2 arrays, all the pairs (2-tuples) of indices that agree.
As arrays have no restriction on duplicated elements, the natural extension for the set-intersection, to arrays is the direct-intersection.


%A useful tool is determining if two sets agree on entries, this operation is called the \textit{intersection}.
%Here we consider a generalization of this operation on arrays/tuples/list-of-tuples (ordered sets), and therefore we desire the index-intersection (iintersection). 

So far we have been considering two \textit{unlabeled} arrays (of the same width). If $A$ and $B$, have labels $\ell_A$ and $\ell_B$ respectively, then the label-intersection $\ell_A \cap \ell_B = e$, yields an \textit{edge}. This edge determines the columns of $A$ and $B$ which get directly-intersected.
The general-result over $n$-arrays is to obtain another sparse-array, whereby the columns are the enumeration of the arrays involved, and rows denote the row's index in each corresponding array.
\begin{align*}
    \mathop{\circled{$\bigcap$}}_{\text{edges}} A^{(n)}
\end{align*}
The result is all the possible products which contribute to the partial-trace. %Because we would like to avoid the full direct-product at once, we may 

%if possible, the better algorithmic-perspective begins with the former. 
%This operation is likely to have many duplicates, some summed over, this yields the partial-trace.


%For two unlabeled arrays, their direct-intersection is practically trivial, merely deleting non-shared elements. 
%Because both are assumed to be well-ordered all elements are unique.
%When labeled, such that some indices are internal (to be intersected), and other external-indices, duplicates can arise (the same internal index can have many external-index tuples). The duplicates of two tensors/arrays are the direct-producte with each other and merged with all other intersecting domains.

%%%%$A \mathop{\circled{$\cap$}} B = \emptyset$ iff $A \cap B = \emptyset$.






\subsection{shoelace}


Suppose we have 2 arrays $A$ and $B$, and we would like to compute their direct-intersection. Then we shall need to sort each $A$ and $B$, in order to utilize the binary-search on both arrays.
We begin by starting at one of the arrays, and searching for the first element in the other via a left-side and right-side binary-search. If found, the left-side and right-side search yields the range of values that match, else if not found the left-side and right-side search matches in value and is known as the \textit{insertion-point}.
This interval-search can be done by going between the two arrays. If we assume both arrays are sorted, then if we binary-search between the arrays, and a found element that skips over many, all these elements are by definition not included in both arrays.
This even applies if the searched element is not found, as the insertion point provides another constraint to future searches. 
Furthermore, these arrays need-not have unique elements, but merely sorted.
Additionally, all required binary-searches become increasingly constrained.
Therefore if their are $M$ unique entries in the intersection of $A$ and $B$, $|A\cap B| = M$, with $N$ total entries in each the algorithm should have time-complexity of $\mathcal{O}\sim M\log{N}$.

%The binary-search in shoelace will go in-between both sorted arrays $A$ and $B$, this circumvents the need to successively go element-wise in any single-array.

%The scout-search is an interval-binary-search (both Left and Right sided binary-search). The Left and Right hand entry agree on a numerical-value iff the searched element is not found, the numerical-value is the insertion point.
%If the searched element is found, then Left and Right values must disagree, and a 

 





\subsection{multi-intersection (hyperedge)}

The previous algorithm applies to a 2-node edge, \textit{i.e.} between 2-arrays. This is readily generalizable to a $n$-node hyper-edge, \textit{i.e.} between $n$-arrays.
The main idea here is to layout the arrays in an arbitrary 1-dimensional\footnote{Suppose we have 4 arrays as in fig. \ref{shoelacealgor}, orders of $ABCD$, $ACDB$, $CDBA$, or any permutation should yield the same results.} order with two pivot arrays. These pivot-arrays are special as they guide the search. Suppose we start the search on an edge array $A$, for some element $a \in A$, we search with the adjacent array if found we proceed to search its adjacent array, this is repeated until the last array (an edge array, $D$). If the search makes it all the way through all arrays, then their intersecting elements are direct-producted together and saved. 
Else (if any search fails) $a\in A$ is searched in $B$ for some insertion point $i$. This insertion point defines the next search, $d = D[i]$, and the algorithm repeats going towards edge array $A$. The algorithm to linking edge-to-edge may be called a \textit{sweep}. %, in order to go between the arrays

The algorithm is depicted in fig. \ref{shoelacealgor}, for the case of 4 list-of-tuples/arrays. On that figure, all lines (dotted, dashed, solid) are binary-searches $\mathcal{O}\sim\log{N}$, dashed lines are searches which ``failed'', yielding the insertion point (both searches yield the same location).
After a failure, one edge is compared to the other directly, the next unique element(s) in that edge start a new search.
While the dotted \& solid lines compute the left and right search respectively, and correspond to a successful search.  
Once we reach the end of any array the algorithm is finished.


%Suppose we have tensors $A^{(0)}, A^{(1)}, A^{(2)}, \cdots, A^{(n)}$, and select: \begin{align*}    \text{min} \left( |A^{(0)}|, |A^{(1)}|, |A^{(2)}|, \cdots, |A^{(n)}| \right) \end{align*} suppose this is without-loss-of-generality $A^{0}$. Then for all elements in $\epsilon \in A^{0}$, search()

\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}
    \draw[->] (0,0) -- (0,-10);
    \draw[->] (3,0) -- (3,-10);
    \draw[->] (6,0) -- (6,-10);
    \draw[->] (9,0) -- (9,-10);
    %\draw[->] (15,0) -- (15,-10);
    
    %%% 1 sweep
    \draw ( 0, -0.5) -- ( 3, -1.1);
    \draw[dotted] ( 0,  0.00) -- ( 3, -0.70);
    \draw[dashed] ( 6,  0.00) -- ( 3, -0.70);
    \draw[dashed] ( 6,  0.00) -- ( 3, -1.1);
    \draw[dashed] ( 3, -0.70) -- ( 9, -1.3);
    \draw[dashed] ( 3, -1.1) -- ( 9, -1.3);
    
    
    %%% 2 sweep
    \draw[dotted] ( 9, -1.3) -- ( 6, -1.5);
    \draw[dotted] ( 3, -1.6) -- ( 6, -1.5);
    \draw[dotted] ( 3, -1.6) -- ( 0, -1.4); %%%
    \draw ( 9, -1.7) -- ( 6, -1.8);
    \draw ( 3, -1.9) -- ( 6, -1.8);
    \draw ( 3, -1.9) -- ( 0, -2.4);
    
    %%% 3 sweep
    \draw[dotted]  ( 0, -2.5) -- ( 3, -2.8);
    \draw[dotted]  ( 6, -2.5) -- ( 3, -2.8);
    \draw[dotted]  ( 9, -2.8) -- ( 6, -2.5);
    \draw ( 0, -2.7) -- ( 3, -3.2);
    \draw ( 6, -3.6) -- ( 3, -3.2);
    \draw ( 9, -3.7) -- ( 6, -3.6);
    
    %%% 4 sweep 
    \draw[dashed] ( 9, -3.8) -- ( 6, -4.6);
    \draw[dashed] ( 9, -4.4) -- ( 6, -4.6);
    \draw[dashed] ( 9, -3.8) -- ( 0, -5.6);
    \draw[dashed] ( 9, -4.4) -- ( 0, -5.6);
    
    %%% 5 sweep
    \draw[dotted] ( 0, -5.6) -- ( 3, -4.7);
    \draw[dotted] ( 6, -5.7) -- ( 3, -4.7);
    \draw[dotted] ( 6, -5.7) -- ( 9, -6.0);
    \draw ( 0, -6.1) -- ( 3, -5.7);
    \draw ( 6, -6.2) -- ( 3, -5.7);
    \draw ( 6, -6.2) -- ( 9, -6.4);
    
    %%% 6 sweep
    \draw[dotted] ( 6, -6.7) -- ( 9, -6.5);
    \draw[dotted] ( 6, -6.7) -- ( 3, -7.5);
    \draw[dotted] ( 0, -8.0) -- ( 3, -7.5);
    \draw ( 0, -8.8) -- ( 3, -7.6);
    \draw ( 6, -8.2) -- ( 3, -7.6);
    \draw ( 6, -8.2) -- ( 9, -8.6);
    
    %%% 7 sweep
    \draw[dashed] ( 0, -10.0) -- ( 9, -8.7);
    \draw[dashed] ( 0, -10.0) -- ( 9, -8.9);
    \draw[dotted] ( 6, -8.3) -- ( 9, -8.7);
    \draw ( 6, -8.6) -- ( 9, -8.9);
    \draw[dashed] ( 6, -8.3) -- ( 3, -8.5);
    \draw[dashed] ( 6, -8.6) -- ( 3, -8.5);
    
    %\node(draw) at ( 3, -1.1)   (a) {$\bullet$};
    \node(draw) at ( 0, 1.0)   (a) {$A$};
    \node(draw) at ( 3, 1.0)   (a) {$B$};
    \node(draw) at ( 6, 1.0)   (a) {$C$};
    \node(draw) at ( 9, 1.0)   (a) {$D$};
\end{tikzpicture}\quad\quad
\raisebox{10em}{
\begin{tikzpicture}
    \coordinate (a) at (4,2.5);
    \coordinate (b) at (3,.8);
    \coordinate (c) at (5,0);
    \coordinate (d) at (5.3,1.2);

    \draw[very thick] (a) -- (b);
    \draw[very thick] (b) -- (c);
    \draw[very thick] (c) -- (d);
    \draw[thick, dash dot dot] (b) -- (d);
    \draw[thick, dash dot dot] (a) -- (d);
    \draw[thick, dash dot dot] (a) -- (c);

    \node at (4,2.75)  (a) {$A$};
    \node at (2.75,.8)  (a) {$B$};
    \node at (5,-0.25)  (a) {$C$};
    \node at (5.5,1.2)  (a) {$D$};
\end{tikzpicture}}
\end{center}
\caption{\label{shoelacealgor} 
(left) An example for the lace algorithm on 4 lists/arrays. (right) showing the contraction-hyperedge and the searching path (shown to the left) shown by the solid line.}
\end{figure}



\subsection{chain of edges}

Now suppose we have $n$-arrays with many edges (or hyperedges), \textit{i.e.} a network/graph, as given by the label-intersection. 
It can be easily shown, that in order to contribute to the direct-intersection of the entire network, any given element must be connected by all the network's edges to some other elements.
There must exist a path (not necessarily Eulerian) traversing all edges. This path defines a direct-product and an element to the direct-intersection.

We may take the direct-intersection over every edge in the network, to obtain intersecting sparse-arrays. Only to direct-intersect over 
The direct-intersection may be applied successively to obtain the entire-network's direct-intersection (all elements, the $n$-tuples, being unique).


\subsubsection{Tree-Search \& Ray-Tracing}

We need a method of sequentially going through all the edges in the network, in a connected fashion, this is called a path $\gamma$.
This path begins at a special, pivot, edge. 
We can initiate a \textit{scout}-binary-search, through this path and determine if makes it all the way thru the network. 
If successively traversed the network (every search yielded at least 1 element), then the entire tree-product is computed.
Else, the next intersecting element in the pivot edge begins a new scout-binary-search. Once all the intersecting elements in the pivot edge are tried the algorithm is completed.
The tree-product is the one sided direct-product. The number of products is the largest layer in the tree.


%the scout-binary search can first determine if a ray goes to samples all edges ...

%If the scout left-binary-search is successful then, the entire tree-product is must be computed and saved.








\section{the surjective-map}

So far the direct-intersection computes the intersecting-indices associated to the ordered arrays. 
This is not quite what is desired, this is because the internal-indices, which are intersected are generally in a different order than that of the external-indices.
This is not the case, as external-indices are often the objective, but its the internal indices which we have to directly-intersect. Therefore we would like to obtain the direct-intersection according to the corresponding ordered external-indices.




\begin{figure}[h]
\begin{center}
\input{arrayproductmap}
\end{center}
\caption{ This figure shows the complete operation between two surjective-maps, two ordering, and a direct-intersection map. \label{surjective} }
\end{figure}


\subsubsection{dense-external map}

An important but subtle ingredient is the surjective-map between the external-indices and internal-indices.
Suppose we have sorted the internal and external-indices ($F$) together such that the internal indices ($D$) are lexicographically-ordered $D^{\le}$, $D^{\le}||F$ (let $||$ denote concatenation), in general this means the external-indices $F$ are not ordered. Because the external-indices yield our result, they must be well-ordered, and hence ordered. This may be achieved by a permutation $i$, to achieve partial-ordering. However, we must go further and determine which indices in the partially-ordered array are unique, these indices are given by a sorted-array $u$. When surjectively-composed/sub-indexed into $F^{\le}$ this yields $F^{<}$. This array $u$, cannot directly be used with elements of $D^{\le}$ which carry a different order and size. Therefore, we may generate an auxiliary array with the order of $F^{\le}$, but with entries according to the indices of the unique entry they correspond to in $F^{\le}$, e.g. without-loss-of-generality the $8$th unique element in $F^{\le}$ all have entry 8, this auxiliary array is in bijective agreement with $F^{\le}$ and is named $f^{\le}$. The inverse of the sorting permutation array $i^{-1}$ may be composed to revert this into the order of $D^{\le}$.
\begin{align*}
    D^{\le}||F \mathop{\longrightarrow}^{i} F^{\le} &\mathop{\longrightarrow}^{u} F^{<} \\
    F^{\le} &\mathop{\longrightarrow}^{1-1} f^{\le} \mathop{\longrightarrow}^{i^{-1}} f \quad\quad,
\end{align*}
note $f$ is in the original order of $F$, and hence ordered like $D^{\le}$. Therefore, $f$ provides a surjective map from every element in $D^{\le}$, to an element in $F^{<}$:
\begin{align*}
    f : D^{\le} \rightarrow F^{<}\quad\quad.
\end{align*}
This is important because, the result, given by the external-indices must be well-ordered, whilst they are apart of potentially many products. Therefore, it can be shown that, if $d^{(0)}\subset D^{(0)\le}$ and $d^{(1)}\subset D^{(1)\le}$ are ordered subsets of two different sparse-array list-of-tuples, such that their corresponding external-tuples are unique then:
\begin{align*}
    d^{(0)}[f^{(0)}] \times d^{(1)}[f^{(1)}] \mathop{\subset} F^{(0)<}\times F^{(1)<}\quad\quad,
\end{align*}
is a well-ordered subset of the result.







\section{2-array pure-sparse algorithm}




\begin{figure}[h]
\begin{center}
\includegraphics[width=0.65\textwidth]{flowchart.pdf}
\end{center}
\caption{ The algorithmic flow-chart for partial-tracing two sparse-arrays. \label{flowchartt} }
\end{figure}




Here we will consider the partial-trace algorithm applied for two purely sparse-arrays. This is often sufficient, as often optimum partial-tracing algorithms over many dense-arrays, involve the tracing of two arrays at a time.
Because only two arrays are considered, hyperedges (traces involving three or more arrays simultaneously) are not possible. On fig. \ref{flowchartt}. 

The direct-intersection is the crucial-piece for this algorithm, the issue is the direct-intersection typically involves a rapid expansion of elements, as it involves all the possible products involved for a partial-trace. The number of elements in the direct-intersection is also the time complexity of the algorithm (\textit{e.g.} in the naïve matrix-product, we have $\sim N^3$ products).
Therefore, we would like to perform all intra-arrays operations first, without working this the potentially large direct-intersection. This involves the intra-intersection of \S\ref{intraintersect} (removing redundant degrees-of-freedom), and establishing the canonical format of \S\ref{canonical}. This canonical format involves sorting over each internal index.


%^\subsection{\caution list-of-tuples decomposition}


The next operation is the \textit{label-intersection}, we need to know which axës are intersected together. If no axës intersect, than the only operation is a direct-product. 
Suppose we have a sparse-array $A$, with $n$ axës, this implies the index-array, $A^\text{index}$, has $n$ columns. If we are given a label, which determines which indices are external and internal. We would like to organize the sparse-array into columns which are external xor internal. 
Multiple external-indices may be grouped together, because they do not interact with any other axës. 
While overlapping (typically internal) indices are represented by $O^{(i)}$, for each overlapping (internal) index.
%We would like to split the list-of-tuples into 2-parts: the external-list-of-tuples ($F$) and dumb-list-of-tuples ($D$).
with the vertical-bars indicating concatenation (over columns/axes) we have two examples of list-of-tuples/index-array decompositions:
\begin{align*}
    A^\text{index} &= F \,\big|\,\big|\, O^{(0)} \,\big|\,\big|\, O^{(1)} \,\big|\,\big|\, \cdots \,\big|\,\big|\, O^{(n-1)} \\
    B^\text{index} &= O^{(0)} \,\big|\,\big|\, O^{(1)} \,\big|\,\big|\, \cdots \,\big|\,\big|\, O^{(n)} \quad\quad.
\end{align*}
each internal-array-of-tuples corresponds to each traced/contracted edge. they are concatenated by the tuples.
%If we have a external-edge, it is repeated in the internal 
This decomposition is important as the output array has at most the size of the cartesian product of the external-indices.
Continuing with the direct-intersection, we would like to determine which sparse-index get multiplied together, this is achieved with methods in \S\ref{directintersect}, by a binary-search. We then compute the direct-product over intersecting sub-array elements. For each of these elements, have associated external-indices, these are obtained via the surjective-map of \S\ref{surjective}. Like external indices are summed over to obtain the resulting array.




\section{2-array : Sparse \& Dense trace}

Here we demonstrate the partial-tracing between a purely sparse-array ($S$) and dense-array ($D$) to yield another dense-array $O$.
Let's suppose both the sparse-array ($S$) and dense-array ($D$) have both kinds of indices: external and internal.
Let's refer to the collection of external indices by $\I_E$ and $\J_E$ for both the sparse and dense-array respectively, and likewise for internal-indices $\I_I$ and $\J_I$. Such that both arrays can be brought into the form\footnote{
Due to syntax limitations, the implementation considered here involves reshaping all internal-indices for a given array into 1-axis, and all external-indices into another. Therefore the sparse-array becomes a sparse-matrix (a standard data structure, \textit{i.e.} $\I_E \rightarrow I_E$), and the dense-array becomes a dense-matrix. The same composition-procedure is then applied to this setting.
%internal and external indices into a matrix (2-dimensional array).
\begin{align*}
    D \mathop{\xrightarrow{\hspace*{1.5cm}}}^\text{swapaxes} D[\J_E, \J_I] &\mathop{\xrightarrow{\hspace*{1.5cm}}}^\text{reshape} D[\J_E | J_I] \\
    S \mathop{\xrightarrow{\hspace*{1.5cm}}}^\text{swapaxes} S[\I_E, \I_I] &\mathop{\xrightarrow{\hspace*{1.5cm}}}^\text{reshape} S[\I_E, I_I] \mathop{\xrightarrow{\hspace*{1.5cm}}}^\text{reshape} S[I_E,I_I] \\
    \left\{ O[\J_E, I_E] \mathop{=}^{\Sigma} S^\text{data}[I] D[\J_E, I_E]  \right\}_I \mathop{\xrightarrow{\hspace*{1.5cm}}} O \quad\quad. %&\mathop{\xrightarrow{\hspace*{1.5cm}}}^\text{reshape} O[J_E, I_E] \mathop{\xrightarrow{\hspace*{1.5cm}}}^\text{swapaxes} O \quad\quad.
\end{align*}}: $S[\I_E\, , \I_I\,]$ and $D[\J_E\, , \J_I\,]$.
Then the resulting dense-array $O$ is $O[\I_E, \J_E]$, and it's entries are simply the \textit{iterative-sum} (sum over every sparse-entry $I$) of the product of compositions:
\begin{align*}
    O[\I_E, \J_E] &\mathop{=}^{\sum} D[\J_E\, , \hat{\I}_I\,] S^\text{data}[I]\quad\quad.
\end{align*}
Notice the sparse internal-index tuple $\I_I$ is injected/composed into the dense-array. The sub-array $D[\J_E\, , \hat{\I}_I\,]$ is multiplied by $S^\text{data}[I]$, and summed to the existing entries of $O[\I_E, \J_E]$.










\section{Results}

Here we compare the dense-contraction method to the sparse-contraction method mentioned in this paper. We will consider arrays with nonzero-entry-fractions as small as $10^{-4}$.
The algorithm is currently implemented in python, a high-level-language, and therefore suffers for optimization issues (much improvement can be done).
Despite this in certain situations it can still outrun optimized programs (optimized in lower-level-languages).
As dense-tracing-algorithms are indifferent towards the sparsity, they are calculated separately many times as a control. In the following results, the dense-partial-trace/contraction is provided by \texttt{einsum} from \cite{numpy}.
The arrays used in our results are randomly generated.
A potentially useful application of this algorithm is on tensor-networks. Therefore we consider the Matrix-Product-Operator (MPO) contraction (\S\ref{mpompo}) and a Projected-Entangled-Pair-Operators (PEPO) contraction (\S\ref{pepopepo})



%We need to sort each index/column of every array....$n N \log N$ (for $n$ total indices) 

%search only between dumb indices....

%then direct-product within basis....

%time as a function of sparsity.....



\subsection{Matrix-Product}

Here we consider products between two 2-dimensional arrays $W^{(1)}_{AB}W^{(2)}_{BC}$ ($\mathcal{O}\sim N^3$), this is useful comparison because of the established theory of sparse-matrices. Because our algorithm, mentioned in this paper is not optimized in a lower-level language, its comparison to an established code might give insight on the optimized performance.
The established sparse-matrix code is provided from \cite{scipy}.
As shown by the results our-implementation is approximately $50$-fold slower than SciPy's sparse-matrix-code, for dense and very-sparse limits.
On fig. \ref{mptr}, we see the dense-sparse cross-over point, for our algorithm, is approximately 0.01, while for SciPy's implementation it is $\approx 0.50$.



\begin{figure}[hbt!]
\begin{center}
\raisebox{9em}{
\begin{tikzpicture}
    \draw[very thick] (-0.5,0) -- (-1.5,0);
    \draw[very thick] (0.5,0) -- (1.5,0);
    \draw[very thick] (2.5,0) -- (3.5,0);
    \draw (0,0) circle [radius=0.5] node {$W_1$};
    \draw (2,0) circle [radius=0.5] node {$W_2$};

    \node at (-2.0,0)  (a) {$A$};
    \node at (1.0,0.25)  (a) {$B$};
    \node at (4.0,0)  (a) {$C$};
\end{tikzpicture}}\hspace{-2mm}
\input{mp}
\end{center}
\caption{ Partial-trace of two matrices. (left) A diagram representing the matrix-product. (right) a plot showing the sparsity time-complexity, with \texttt{einsum} in blue, our algorithm in orange, and SciPy's sparse-matrix product in green. \label{mptr} }
\end{figure}

\subsection{Matrix-Product-Operators Trace \label{mpompo}}

Next we consider the partial-trace of two sparse 4-index Matrix-Product-Operators (MPOs): $W^{(1)}_{ABab} W^{(2)}_{BCcd} = C_{ACabcd}$ ($\mathcal{O}\sim N^7$). In our test each 4-index operator has shape $s[i] = (20,20,20,20)$. The fig. \ref{mpotr}, shows our results. There is a dense-sparse cross-over point of approximately 0.05, and the time-complexity is shown to scale directly with sparsity.


\begin{figure}[hbt!]
\begin{center}
\raisebox{4.5em}{
\begin{tikzpicture}
    \draw[very thick] (-0.5,0) -- (-1.5,0);
    \draw[very thick] (0.5,0) -- (1.5,0);
    \draw[very thick] (2.5,0) -- (3.5,0);
    \draw[very thick] (0.0,0.5) -- (0.0,1.5);
    \draw[very thick] (0.0,-0.5) -- (0.0,-1.5);
    \draw[very thick] (2.0,-0.5) -- (2.0,-1.5);
    \draw[very thick] (2.0,0.5) -- (2.0,1.5);
    \draw (0,0) circle [radius=0.5] node {$W_1$};
    \draw (2,0) circle [radius=0.5] node {$W_2$};

    \node at (0.0,2.0)  (a) {$a$};
    \node at (0.0,-2.0)  (a) {$b$};
    \node at (2.0,2.0)  (a) {$c$};
    \node at (2.0,-2.0)  (a) {$d$};
    \node at (-2.0,0)  (a) {$A$};
    \node at (1.0,0.25)  (a) {$B$};
    \node at (4.0,0)  (a) {$C$};
\end{tikzpicture}}
\hspace{-2mm}\input{mpo}
\end{center}
\caption{ Partial-trace of two MPO-elements. (left) the diagram showing the partial-trace. (right) a plot showing the sparsity time-complexity, with \texttt{einsum} in blue and our-algorithm in orange. \label{mpotr} }
\end{figure}



\subsection{Projected-Entangled-Pair-Operators Trace \label{pepopepo}}

Next we consider the partial-trace of two sparse 6-index Projected-Entangled-Pair-Operator (PEPOs): $W^{(1)}_{ABCDab} W^{(2)}_{DEFGcd} = C_{ABCEFGabcd}$ ($\mathcal{O}\sim N^{11}$). In our test each 4-index operator has shape $s[i] = (8,8,8,8,8,8)$. Our results are shown in fig. \ref{pepotr}, and demonstrate that the algorithm still scales linearly with the sparsity.
\begin{figure}[hbt!]
\begin{center}
\raisebox{5em}{
\begin{tikzpicture}
    \draw[very thick] (-0.5,0) -- (-1.5,0);
    \draw[very thick] (0.5,0) -- (1.5,0);
    \draw[very thick] (2.5,0) -- (3.5,0);
    \draw[very thick] (0.0,0.5) -- (0.0,1.5);
    \draw[very thick] (0.0,-0.5) -- (0.0,-1.5);
    \draw[very thick] (2.0,-0.5) -- (2.0,-1.5);
    \draw[very thick] (2.0,0.5) -- (2.0,1.5);
    \draw (0,0) circle [radius=0.5] node {$W_1$};
    \draw (2,0) circle [radius=0.5] node {$W_2$};

    \draw[very thick] (0.35,0.35) -- (0.75,0.75);
    \draw[very thick] (-0.35,-0.35) -- (-0.75,-0.75);
    \draw[very thick] (2.35,0.35) -- (2.75,0.75);
    \draw[very thick] (1.65,-0.35) -- (1.25,-0.75);

    \node at (0.0,2.0)  (a) {$a$};
    \node at (0.0,-2.0)  (a) {$b$};
    \node at (2.0,2.0)  (a) {$c$};
    \node at (2.0,-2.0)  (a) {$d$};
    \node at (-2.0,0)  (a) {$A$};
    \node at (1.0,1.0)  (a) {$B$};
    \node at (-1.0,-1.0)  (a) {$C$};
    \node at (1.0,0.25)  (a) {$D$};
    
    \node at (3.0,1.0)  (a) {$E$};
    \node at (1.0,-1.0)  (a) {$F$};
    \node at (4.0,0)  (a) {$G$};
\end{tikzpicture}}\hspace{-3mm}
\input{pepo}
\end{center}
\caption{ Partial-trace of two PEPO-elements. (left) the diagram showing the partial-trace. (right) a plot showing the sparsity time-complexity, with \texttt{einsum} in blue and our-algorithm in orange. \label{pepotr}  }
\end{figure}



\section{Summary}

As shown by our results, sparse-arrays can be used to leverage certain advantages, partial-tracing, over dense-arrays. However, there are some manipulations which are seemingly slower.
Compared to dense-array manipulations, dense-arrays algorithms can access elements in $\mathcal{O}(1)$, while for this algorithm they require $\mathcal{O}(\log N)$, and reshaping dense-arrays can be done in $\mathcal{O}(1)$, while for sparse-arrays its done in $\mathcal{O}(N)$. Additionally, sparse-arrays need to be sorted along each index leading to $\mathcal{O}(n N\log N)$ operations for a sparse-array with $n$ columns/indices and $N$ entries. The sorting overhead are additive and hence maximally scale to the number of initial elements, all which can be done in parallel.
In our results the exponential scaling relating to sparsity is approximately 1 (from 0.86, 1.10, and 1.17 respectively) for all tests considered. This implies the algorithmic time-complexity depends directly on the sparsity.
There is still more work to do, including optimizing the algorithm in a low-level-computing-language, partial-tracing by leveraging symmetries, and a full extension for block-sparse data structures.



%In general, more indices lead to more memory requirements, but also might typically lead to more sparsity.

%%\section{Acknowledgements}

