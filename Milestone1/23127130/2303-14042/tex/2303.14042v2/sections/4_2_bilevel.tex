%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Class-Incremental Masking (CIM)}
\label{subsection: class_incremental_masking}

Ideally, the mask generation process needs to adjust at different phases in CIL environments. 
The process involves two hyperparameters: the masking threshold and the choice of network activation functions.
First, for the threshold, searching for its optimum (for all classes) is not trivial.
%
Grid search is intuitive, but it is computationally expensive when the number of classes increases in CIL.
Second, for the activation function, the standard network of CIL methods uses ReLU~\cite{nair2010rectified,rebuffi2017icarl} and does not optimize it.
We solve this issue by applying a learnable activation function in addition to the existing ReLU function in the CIL model.
%
Physically, we have one neural network, while logically, we have two models to learn (in each incremental phase): the conventional CIL model with ReLU activations, and the adaptive mask generation model with learnable activations.
We thus call our method class-incremental masking (CIM) based CIL. In the following, we elaborate on the network design and optimization pipeline.



\myparagraph{Network Design.}

Figure~\ref{figure: revised_activation_layer} demonstrates an example network architecture in our CIM-based CIL.
The proposed CIM extends the network backbone by \emph{logically} adding a network branch, where only the activation functions are learnable (e.g., Pad{\'e} Activation Units (PAU\footnote{PAU uses a rational function of given degrees $m$ and $n$, i.e., $(a_0+a_1x+\cdots+a_mx^m)/(1+b_1x+\cdots+b_nx^n)$, and can be parameterized by $a_0,\cdots,a_m$ and $b_1,\cdots,b_n$.},~\cite{molina2019pade})) and the parameters of weight layers are copied from the original branch.

This design is motivated by the works of He et al.~\cite{he2015prelu} and Bochkovskiy et al.~\cite{bochkovskiy2020yolov4}, which indicate that layers with learnable activation functions can flexibly process object (localization) information at different network blocks. The difference is that we apply this flexibility to achieve adaptive mask generation for different CIL phases. 

We denote the CIM parameters (i.e., the parameters in learnable activation functions) in $i$-th phase as $\phi_i$.
We optimize the CIL model $(\theta_i, \omega_i)$ and the CIM model $\phi_i$ via a global BOP, as elaborated below.
\input{misc/scripts/network}

\input{misc/scripts/op_flow.tex}

\noindent
\textbf{Optimization Pipeline.}
%
We demonstrate the overall optimization flow in Algorithm~\ref{algorithm: optimization flow}, which consists of two levels of optimization: task-level and mask-level---the former one for CIL and the latter one for CIM. Note that to maintain an unified notation, we further define $\tilde{{\mathcal E}}_{0}=\varnothing$.
%

\myparagraph{1) Task-level Optimization.}
This level aims to optimize the CIL model $(\theta_i,\omega_i)$ to address the CIL task at hand. It can be written as: 
\begin{equation}
\begin{aligned}
\label{eq_cil_training}
(\theta_{i},\omega_{i})\leftarrow(\theta_{i},\omega_{i})-\lambda\nabla_{(\theta,\omega)}\mathcal{L}_{\textrm{CIL}}(\tilde{\mathcal{E}}_{0:i-1}\!\cup\!\mathcal{D}_i;\theta_{i},\omega_{i}),
\end{aligned}
\end{equation}
where $\lambda$ is the learning rate. We follow the implementation of CIL training loss $\mathcal{L}_{\textrm{CIL}}$ in baseline methods~\cite{li2017learning,zhou2021co,yan2021dynamically,wang2022foster}. This means that we use different training losses when plugging CIM into different baseline methods.


\myparagraph{2) Mask-level Optimization.}
This level aims to optimize the CIM model $\phi_i$ to produce adaptive compression masks. It is formulated as a local BOP:
\begin{subequations}
\begin{align}
&\min_{\phi_i}\left[{\mathcal{L}_{\textrm{val}}(\mathcal{D}_i;\theta^*_i,\omega_i)+\mu\mathcal{R}(\phi_i)}\right]\label{eq_bop_formulation_outer},\\
&~\mathrm{s.t.}~\theta_i^*=\argmin_{\theta_i}{\mathcal{L}_\textrm{train}(\tilde{\mathcal{E}}_{0:i-1}\cup\tilde{\mathcal{D}}_i(\phi_i);\theta_i,\omega_i)}. \label{eq_bop_formulation_inner}
\end{align}
\end{subequations}
%
Eq.~\ref{eq_bop_formulation_inner} denotes an inner-level optimization. It trains $\theta_i$ with the data $\tilde{\mathcal{D}}_i(\phi_i)$ compressed by using $\phi_i$, and converges as $\theta_i^*$.
%
Eq.~\ref{eq_bop_formulation_outer} denotes an outer-level optimization. It is based on the validation loss derived by $\theta_i^*$ on the original data $\mathcal{D}_i$.
%
$\mathcal{R}(\phi_i)$ is a constraint representing the memory limitation and $\mu$ is its weight.
%
In the following, we elaborate on the implementation details for the two levels.



In the inner-level optimization, we train a temporary CIL model with compressed data. 
Specifically, we first compress new-class data $\mathcal{D}_i$ into $\tilde{\mathcal{D}}_i(\phi_i)$ using the generated masks by the CIM model $\phi_i$.
%
%
%
%
%
Then, we implement the inner-level optimization as a one-step gradient descent (using the CIL training loss) as:
\begin{equation}
\label{eq_inner_level_update}
\theta^+_i\leftarrow\theta_i-\beta_1\nabla_{\theta}\mathcal{L}_\textrm{CIL}(\tilde{\mathcal{E}}_{0:i-1}\cup\tilde{\mathcal{D}}_i(\phi_i);\theta_i,\omega_i),
\end{equation}
where $\beta_1$ is the learning rate for $\theta_i$. 

The aim of the outer-level optimization is to optimize $\phi_i$ such that the temporary model $(\theta^+_i,\omega_i)$ (trained with compressed data) has a low validation loss on the original data $\mathcal{D}_i$.
%
To achieve this, we back-propagate the loss on the original data~$\mathcal{D}_i$
to update $\phi_i$ as:
\begin{equation}
\label{eq_outer_level_update}
\phi_i\leftarrow\phi_i-\beta_2\nabla_{\phi}\left[\mathcal{L}_{\textrm{CE}}(\mathcal{D}_i;\theta^+_i,\omega_i)+\mu\mathcal{R}(\phi_i)\right],
\end{equation}
where $\mathcal{L}_{\textrm{CE}}$ denotes softmax cross-entropy loss and $\beta_2$ is the learning rate for $\phi_i$. 
%
This trains $\phi_i$ to capture the most discriminative features of new-class images.
The constraint $\mathcal{R}(\phi_i)$ is 
implemented as a $\ell_2$-regularization term on the generated mask by $\phi_i$. The motivation for the regularization term is to make the mask coverage smaller, thus the compressed images take less memory.


We empirically observe that by the above optimization flow, the output activation maps
by $\phi_i$ are easy to collapse, i.e., different images have the same map.
%
To solve this issue, we add a cross-entropy loss term about $\phi_i$ in Eq.~\ref{eq_outer_level_update} to regularize it to produce image-specific activation maps:
\begin{equation}
\begin{aligned}
\label{eq_outer_update_new}
\phi_i\leftarrow\phi_i-\beta_2\nabla_{\phi}[\mathcal{L}_{\textrm{CE}}(&\mathcal{D}_i;\theta^+_i,\omega_i)+\mu\mathcal{R}(\phi_i)\\+\mu^\prime&\mathcal{L}_{\textrm{CE}}(\tilde{\mathcal{E}}_{0:i-1}\cup\mathcal{D}_i;\theta_i,\phi_i,\omega_i)],
\end{aligned}
\end{equation}
where $\mu^\prime$ is the weight. 

\myparagraph{Limitations.} Our CIM learns to generate adaptive masks for exemplar compression in CIL. It has three limitations that are left as future work.
1) It is not able to adjust any previous-phase exemplars, as the validation data (the original data of these exemplars) are not accessible anymore.
2) It introduces hundreds of activation parameters to the CIL model, although this is not a significant overhead compared to model parameters.
Please check detailed overhead analyses in the supplementary materials.
%
3) Image compression is not that meaningful for low-resolution datasets (e.g., $32\!\times\!32$ CIFAR-100)
It is because the memory taken by the compression parameters (e.g., the parameters of CIM) and the RGB pixels of a low-resolution image are comparable. 
%
Using the memory to save more images is more meaningful.
% 
%