\section{Preliminary}
\label{sec_an_overview_of_cil}

The following is the training pipeline of standard CIL with few-shot exemplars.
%
Assume there are $N$ learning phases. %: one initial phase and $N\!-\!1$ subsequent phases.
%
In the $1$-st phase, we load data $\mathcal{D}_{1}$ containing all training samples of $c_1$ classes, and use $\mathcal{D}_{1}$ to train the initial classification model $(\theta_1, \omega_1)$, where $\theta_1$ and $\omega_1$ 
 denote the parameters of the feature extractor and classifier, respectively. 
%
When the training is done, we evaluate the model performance on the test samples of $c_1$ classes. Before the $2$-nd phase, we discard most of the training samples due to the strict memory budget of CIL.
%
In other words, we preserve only a handful of training samples $\mathcal{E}_1$ (i.e., exemplars) in the memory, selected from $\mathcal{D}_1$. A common method for selecting exemplars is called feature herding~\cite{rebuffi2017icarl} and has been used in many related works~\cite{liu2020mnemonics,yan2021dynamically,wang2022memory,wang2022foster}. We adopt it, too, in this work.
%
In the $i$-th phase ($i\geq2$), we load all exemplars ${\mathcal E}_{1:i-1}=\mathcal{E}_1\cup \dots\cup {\mathcal E}_{i-1}$ from the memory and initialize the current model $(\theta_{i},\omega_{i})$ by the previous model $(\theta_{i-1},\omega_{i-1})$. 
%
We use $\mathcal{E}_{1:i-1}$ and the new coming data $\mathcal D_i$ (containing $c_i$ new classes) to train $(\theta_{i},\omega_{i})$.% as follows,
%
Then, we evaluate the current model using a test set of all $\sum_{j=1}^{i}c_j$ classes seen so far. 
%
After that, we discard most of the training samples in $\mathcal D_i$, and leave few-shot exemplars ${\mathcal E}_{i}$ in the memory. 
%
It is clear that this discarding causes a strong data imbalance between old and new coming classes in the subsequent phase. In the following, we introduce our solution to this problem.
