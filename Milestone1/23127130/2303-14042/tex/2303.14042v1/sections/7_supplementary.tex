\setcounter{table}{0}
\renewcommand{\thetable}{S\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{S\arabic{figure}}
\setcounter{section}{0}

\noindent
{\Large {\textbf{Supplementary materials}}}
\\

These supplementary materials include implementation details (\S\ref{sec_imple}), dataset details (\S\ref{sec_datas}), SOTA $95\%$ CI results (\S\ref{sec_sota}), learned PAU results (\S\ref{sec_activation_distance}), compression footprint results (\S\ref{sec_compression_footprint}), sensitivity analysis results (\S\ref{sec_sensi}), overhead analysis results (\S\ref{sec_overhead}), and hardware information (\S\ref{sec_hardw}).

\section{Implementation Details}\label{sec_imple}
\myparagraph{Downsampling Method.} \textcolor{orange}{Supplementary to Section 4.1 ``\textbf{Compression with BBox}''.}
For generating compressed exemplars, we adopted a simple downsampling method (to apply on non-discriminative pixels) called Nearest Neighbor Interpolation~\cite{parker1983comparison}. Specifically, it replaces the pixel value with that of the nearest pixel. This can be achieved by calling the \texttt{cv2.resize()} function with the \texttt{INTER\_NEAREST} flag in the standard image processing library OpenCV~\cite{mordvintsev2014opencv}.
\vspace{0.3cm}

\myparagraph{Mitigating the Effect of Compression Artifacts.} \textcolor{orange}{Supplementary to Section 4.1 ``\textbf{Compression Artifacts}''.} We mitigated the effect of compression artifacts by applying a augmentation method to new-class data $\mathcal{D}_i$ in each learning phase. 
%
Specifically, there were three steps. 1) For all images in $\mathcal{D}_i$, we generated the CAM-based bounding boxes at the beginning of training and updated them once per $40$ epochs.
%
2) In each epoch, we randomly selected a subset of $\mathcal{D}_i$. The proportion of the subset was adjusted according to the training progress---$0$ at the beginning and increased by $0.1$ every $40$ epochs.
%
3) Before feeding each image in the subset (into the CIL model), we downsampled (with ratio $\eta$) the pixels outside the bounding box (obtained in step 1).
\vspace{0.3cm}

\myparagraph{Compressing $\mathcal{D}_i$ into $\tilde{\mathcal{D}}_i(\phi_i)$.} \textcolor{orange}{Supplementary to Section 4.2 ``\textbf{2) Mask-level Optimization}''.} Different from the compressed pipeline introduced in Section \textcolor{red}{4.1}, compression in the inner-level optimization should involve only differentiable operations to enable gradient descent. To achieve this, we skipped the steps of adding the threshold and bounding box and obtained the compressed images by applying continuously valued masks as follows,
\begin{subequations}\label{continuous_compression}
\begin{align}
&A(\phi_i)={\omega_{i,y}^{\top}F(x;\theta_i,\phi_i)},\tag{S1a} \label{continuous_compression:a}\\
&\mathcal{M}^\textrm{CAM}(\phi_i)=\frac{A(\phi_i)-\min{(A(\phi_i))}}{\max{(A(\phi_i))} - \min{(A(\phi_i))}},\tag{S1b} \label{continuous_compression:b}\\
&\tilde{x}(\phi_i)=\mathcal{M}^\textrm{CAM}(\phi_i) \odot x + (1 - \mathcal{M}^\textrm{CAM}(\phi_i)) \odot x_\eta.\tag{S1c} \label{continuous_compression:c}
\end{align}
\end{subequations}

\myparagraph{Comparing with Other Compression-based Methods.} \textcolor{orange}{Supplementary to Table 4.} The code of plugging MRDC~\cite{wang2022memory} into PODNet~\cite{douillard2020podnet} (which shows the best results in its original paper) was not released by authors. So we rerun the results of MRDC when plugging it into LUCIR~\cite{hou2019lucir}. The experiments are conducted on the LFH setting. For a fair comparison, we apply weight transfer operations~\cite{sun2019meta} in all these experiments following Mnemonics~\cite{liu2020mnemonics}.

\section{Dataset Details}\label{sec_datas}
\textcolor{orange}{Supplementary to Section 4.1 ``\textbf{Datasets}''.} We show the details about three datasets in Table~\ref{tab_datasets}. We elaborate the image preprocessing methods applied on the three datasets in Table~\ref{tab_preprocessing}. Please note that for image preprocessing, we strictly followed~\cite{rebuffi2017icarl,hou2019lucir,douillard2020podnet,liu2020mnemonics,liu2021adaptive,yan2021dynamically,wang2022foster} for a fair comparison.
%
\input{tables/datasets}
\input{tables/preprocessing}

\section{More Results Comparing with the SOTA}\label{sec_sota}
\input{tables/confidence_interval}
\textcolor{orange}{Supplementary to Table 1.} In Table~\ref{table: confidence_interval}, we report the $95\%$ confidence intervals corresponding to the numbers in the Table~\textcolor{red}{1} of the main paper.

\section{Results of Learned PAUs}\label{sec_activation_distance}
\input{misc/scripts/activation_distance}
\textcolor{orange}{Supplementary to Section 5.2 ``Results and Analyses''.}. Figure~\ref{figure: activation_distance} shows the activation distances in different network blocks and phases. We measure the distance by $\int_{-3}^3 |f_\text{PAU}(x)-f_\text{ReLU}(x)| dx$ and use $601$ interpolated points to approximate the integration value. The learned PAUs in the last block have larger distances than those in shallow blocks. The last block mainly encodes high-level semantic information (e.g., ``body'' of ``dog''), this suggests that the learned PAUs are adjusted to focus on the most discriminative semantics. Shallow blocks learn to capture low-level features that are shareable between classification and mask generation. Therefore, the learned PAUs in these shallow blocks have little adjustment. This motivates the variant in Row 10 of Table~\textcolor{red}{3} in the main paper.

\section{Results of Compression Footprints.}\label{sec_compression_footprint}
\input{misc/scripts/memory_footprints}
\textcolor{orange}{Supplementary to Section 5.2 ``Results and Analyses''.}. Figure~\ref{figure: memory_footprints} provides the compression footprints along incremental phases. Comparing with CAM, CIM learns to produce more conservative compression footprints in later phases. Our explanation is that more visual cues are required to classify more classes.

\section{Results of Sensitivity Analyses}\label{sec_sensi}
\input{misc/scripts/sensitivity}
\textcolor{orange}{Supplementary to Section 5.1 ``Implementation Details''.} In Figure~\ref{figure: sensitivity}, we show the sensitivity analyses for CAM threshold $\tau$, downsampling ratio $\eta$, mask-level learning rates $\beta_1,\beta_2$ and two regularization weights $\mu,\mu^\prime$ on ImageNet-100~\cite{rebuffi2017icarl}. For $\tau$ and $\eta$, we also show the computation overheads.

\section{Space and Computation Overheads}\label{sec_overhead}
\textcolor{orange}{Supplementary to Section 4.2 ``Limitations''.} We elaborate on the space overhead by taking ResNet-18 as an example. We add $17$ PAUs to it, each with $10$ optimizable parameters. So we use $170$ extra parameters in total. 
This is negligible compared to $11$ million of network parameters. 
Besides, we save bbox along with exemplars in the memory. Each bbox takes around $0.01\%$ memory of a $224\times224$ RGB image.
For computation overhead, 
our method needs around $60\%$ extra computations over baseline CIL training, caused by two factors: 1)~BOP between CIL and CIM models; and 2)~training on a large number of compressed exemplars.

\section{Hardware Information}\label{sec_hardw}
-- \textbf{CPU}: AMD EPYC 7F72 24-Core Processor

-- \textbf{GPU}: $4\times$ NVIDIA GeForce RTX 3090

-- \textbf{Mem}: $8\times$ DDR4-3200 ECC RDIMM - 32GB