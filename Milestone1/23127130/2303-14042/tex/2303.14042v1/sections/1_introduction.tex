\section{Introduction}
\label{sec_introduction}

Dynamic AI systems have a continual learning nature to learn new class data.
%
They are expected to adapt to new classes while maintaining the knowledge of old classes, i.e., free from forgetting problems~\cite{mcrae1993catastrophic}.
%
To evaluate this, the following protocol of class-incremental learning (CIL) was proposed by Rebuffi {et al}.~\cite{rebuffi2017icarl}.
%
The model training goes through a number of phases. Each phase has new class data added and old class data discarded, and the resultant model is evaluated on the test data of all seen classes.
%
A straightforward way to retain old class knowledge is keeping around a few old class exemplars in the memory and using them to re-train the model in subsequent phases.
%
The number of exemplars is usually limited, e.g., $5\!\sim\!20$ exemplars per class~\cite{rebuffi2017icarl, hou2019lucir, zhao2020maintaining, wu2019bic, liu2020mnemonics, douillard2020podnet, liu2021adaptive, yan2021dynamically, wang2022foster}, as the total memory in CIL strictly budgeted, e.g., $2k$ exemplars.

\input{misc/scripts/tisser.tex}

This leads to a serious data imbalance between old and new classes, e.g., $20$ per old class \emph{vs.} $1.3k$ per new class (on ImageNet-1000~\cite{deng2009imagenet}), as illustrated in Figure~\ref{fig_teaser}\textcolor[rgb]{1,0,0}{a}.
%
The training is thus always dominated by new classes, and forgetting problems occur for old classes.
%
Liu et al.~\cite{liu2020mnemonics} tried to mitigate this problem by parameterizing and distilling the exemplars, without increasing the number of them (Figure~\ref{fig_teaser}\textcolor[rgb]{1,0,0}{b}).
%
Wang et al.~\cite{wang2022memory} traded off between the quality and quantity of exemplars by uniformly compressing exemplar images with JPEG~\cite{wallace1991jpeg} (Figure~\ref{fig_teaser}\textcolor[rgb]{1,0,0}{c}).
%
As shown in Figure~\ref{fig_teaser}\textcolor[rgb]{1,0,0}{d}, our approach is also based on image compression. The idea is to downsample only non-discriminative pixels (e.g., background) and keep discriminative pixels (i.e., representative cues of foreground objects) as the original.
%
In this way, \textbf{we do not sacrifice the discriminativeness} of exemplars when increasing their quantity.
%
In particular, \textbf{we aim for adaptive compression} in dynamic environments of CIL, where the intuition is later phases need to be more conservative (i.e., less downsampling) as the model needs more visual cues to classify the increased number of classes.
%

To achieve selective and adaptive compression, we need the location labels of discriminative pixels.
%
Without extra labeling, we automatically generate the labels by utilizing the model's own ``attention'' on discriminative features, i.e., class activation maps (CAM)~\cite{zhou2016cam}. We take this method as a feasible baseline, and based on it, we propose an adaptive version called class-incremental masking~(CIM).
%
Specifically, for each input image (with its class label), we use its feature maps and classifier weights (corresponding to its class label) to compute a CAM by channel-wise multiplication, aggregation, and normalization. Then, we apply hard thresholding to generate a $0$-$1$ mask.\footnote{Note that we do not use mask labels to do image compression because storing them is expensive. Instead, we expand the mask to a bounding box, as elaborated in Section~\ref{sec_methodology}.}
%
We notice that when generating the masks in the dynamic environments of CIL, the optimal hyperparameters (such as the value of hard threshold and the choice of activation functions) vary for different classes as well as in different incremental phases.
%
%
Our adaptive version CIM tackles this by parameterizing a mask generation model and optimizing it in an end-to-end manner across all incremental phases. 
%
In each phase, the learned CIM model adaptively generates class- and phase-specific masks. 
%
We find that the compressed exemplars based on these masks have stronger representativeness, compared to using the conventional CAM.
%

Technically, we have two models to optimize, i.e., the CIL model and the CIM model.\footnote{Note that the CIM model is actually a plug-in branch in the CIL model, which is detailed in Section~\ref{subsection: class_incremental_masking}.}
%
These two cannot be optimized separately as they are dependent on computation: 1) the CIM model compresses exemplars to input into the CIL model; 2) the two models share network parameters.
%
We exploit a global bilevel optimization problem (BOP)~\cite{sinha2017bilevel,chen2022gradient} 
%
to alternate their training processes at two levels. 
%
This BOP goes through all incremental training phases.
%
In particular, for each phase, we perform a local BOP with two steps to tune the parameters of the CIM model: 1)~a temporary model is trained with the compressed exemplars as input; and 2)~a validation loss on the uncompressed new data is computed and the gradients are back-propagated to optimize the parameters of CIM.
%
To evaluate CIM, we conduct extensive experiments by plugging it in recent CIL methods,\footnote{Using ``plug-in'' for evaluation is due to the fact that many baseline methods were originally evaluated in different CIL settings.}
%
LUCIR~\cite{hou2019lucir}, DER~\cite{yan2021dynamically}, and FOSTER~\cite{wang2022foster},
on three high-resolution benchmarks, Food-101~\cite{bossard14food101}, ImageNet-100~\cite{hou2019lucir}, and ImageNet-1000~\cite{deng2009imagenet}. 
We find that using the compressed exemplars by CIM brings consistent and significant improvements, e.g., $4.2\%$ and $4.8\%$ higher than the SOTA method FOSTER~\cite{wang2022foster}, respectively, in the $5$-phase and $10$-phase settings of ImageNet-1000, with a total memory budget for $5k$ exemplars. 
