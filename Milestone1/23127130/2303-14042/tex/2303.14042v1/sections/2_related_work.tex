\section{Related Work}
\label{sec_related_work}

\myparagraph{Class-Incremental Learning (CIL).} 
There are three main lines of work to address the catastrophic forgetting problem~\cite{mccloskey1989catastrophic,mcrae1993catastrophic} in CIL. 
%
\textit{\textbf{Regularization-based}} methods apply discrepancy (between old and new models) penalization terms in their objective functions, e.g., by comparing output logits~\cite{li2017learning,rebuffi2017icarl}, intermediate features~\cite{hou2019lucir,douillard2020podnet,simon2021learning,Liu2023Online}, and prediction heatmaps~\cite{dhar2019learning}. \textit{\textbf{Parameter-isolation-based}} methods increase the model parameters in each new incremental phase, to prevent knowledge forgetting caused by parameter
overwritten.
%
Some of them~\cite{huang2019neural,rusu2016progressive,xu2018reinforced,yan2021dynamically,wang2022foster} proposed to progressively expand the size of the neural network to learn new coming data. Others~\cite{kirkpatrick2017overcoming,zenke2017continual,abati2020conditional,liu2021adaptive} froze a part of network parameters (to maintain the old class knowledge) to alleviate the problem of knowledge overwriting. 
%
\textit{\textbf{Replay-based}} methods assume there is a clear memory budget allowing a handful of old-class exemplars in the memory. Exemplars can be used to re-train the model in each new phase~\cite{rebuffi2017icarl,hou2019lucir,douillard2020podnet,liu2020mnemonics,wu2019bic,Liu2021RMM,wang2022memory}. This re-training usually contains two steps: one step of training the model on all new class data and old class exemplars, and one step of finetuning the model with a balanced subset (i.e., using an equal number of samples per class)~\cite{liu2021adaptive, Liu2021RMM, hou2019lucir, douillard2020podnet, yan2021dynamically}.


The replay-based methods focusing on \textbf{\emph{memory optimization}}~\cite{liu2020mnemonics,wang2022memory} are closely related to our work. \cite{liu2020mnemonics} proposed a bilevel optimization framework to distill the current new class data into exemplars before discarding them. It aims to improve the quality of exemplars without increasing the quantity. 
%
Another work~\cite{wang2022memory} aimed to trade-off between the quality and quantity of exemplars by image compression using the JPEG algorithm, i.e., each exemplar is uniformly downsampled.  
%
Ours differs from these two works in three aspects. 1)~Our CIM based image compression automatically segments the discriminative pixels in the exemplar and downsamples the non-discriminative pixels only.
It barely weakens the representativeness of the exemplar.
%
%
In contrast, the parameterization of image pixels in \cite{liu2020mnemonics} hampers the model from capturing high-frequency (discriminative) features from the image, especially the high-resolution (e.g., $224\!\times\!224$) image~\cite{cazenavette2022dataset}. 
%
2)~Our approach increases the diversity (i.e., quantity) of old class exemplars by reducing the memory consumption for each exemplar.
In contrast, \cite{liu2020mnemonics} keeps a fixed number of exemplars in the memory. 
%
3)~Our approach has an adaptive image compression strategy that fits well in the dynamic environments of CIL.
In contrast, \cite{wang2022memory} uses uniform image compression (or uniformly increasing quality parameters from $100$ to $1$) without considering 
the properties of specific classes in each learning phase.
%

\myparagraph{Class Activation Map (CAM)}~\cite{zhou2016cam} is a simple yet effective weakly-supervised object localization method. Its model is trained with only the image-level label and can generate pixel-level masks on foreground objects. Specifically, the masks are the results of hard-thresholding the heatmaps produced by feature maps and classifier weights. Advanced CAM variants include Grad-CAM~\cite{selvaraju2017grad}, ReCAM~\cite{chen2022recam}, AdvCAM~\cite{lee2021anti}, etc. 
Our CIM is based on the vanilla CAM because it is computationally simple and efficient.

\myparagraph{Bilevel Optimization Problems (BOP)}~\cite{sinha2017bilevel,chen2022gradient} aims to solve a nested optimization problem, where the outer-level optimization is subjected to the result of the inner-level optimization. It has shown effectiveness in a wide range of machine learning areas, such as hyperparameter selection~\cite{maclaurin2015gradient} and meta-learning~\cite{finn2017maml}. 
For tackling CIL tasks, \cite{liu2020mnemonics}~leverages BOP to alternatively optimize the parameters of the CIL model and the parameterized exemplars. \cite{liu2021adaptive} applies BOP to learn the aggregation
weights of the plastic and elastic branches in the CIL model. 
%
In our work, we use BOP to solve the optimization of the CIL model and the parameterized Class-Incremental Masking (CIM) model, where CIM is a plugin branch (in the CIL model), using few extra parameters. The process of BOP is quick and efficient.


