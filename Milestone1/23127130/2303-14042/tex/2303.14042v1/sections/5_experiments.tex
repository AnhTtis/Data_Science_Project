\section{Experiments}
\label{sec_experiments}

%
We incorporate CIM into two baseline CIL methods (i.e., DER~\cite{yan2021dynamically} and FOSTER~\cite{wang2022foster}) and boost their model performances consistently on three datasets.
%
Below, we introduce datasets and experiment settings (Section~\ref{subsection: experimental_settings}), followed by results and analyses (Section~\ref{subsection: results_and_analyses})

\subsection{Experimental Settings}
\label{subsection: experimental_settings}

\myparagraph{Datasets.}
We conduct experiments on three standard CIL benchmarks with high-resolution images.
%
1)~\textbf{Food-101}~\cite{bossard14food101} consists of $101$ food categories with $750$ training and $250$ test samples per category.
%
All images have a maximum side length of $512$ pixels.
%
2)~\textbf{ImageNet-1000}~\cite{deng2009imagenet} is a large-scale dataset with $1,\!000$ classes and each class has around $1,\!300$ training and $50$ test samples.
%
3)~\textbf{ImageNet-100} is a 100-class subset randomly sampled from ImageNet-1000 with a fixed NumPy~\cite{harris2020array} random seed ($1993$), following~\cite{hou2019lucir}.
%
We provide other details of these datasets, e.g., image sizes and pre-processing methods, in the supplementary materials.

\myparagraph{Protocols.}
We use two protocols: \textit{learning from scratch} (LFS) and \textit{learning from half} (LFH), following recent CIL works~\cite{yan2021dynamically,wang2022foster}.
%
In LFS, the model observes the same number of classes in all $N$ phases, where $N$ is optionally $5$, $10$, and $20$. 
%
In LFH, the model is trained on half of the classes (e.g., $500$ classes for ImageNet-1000) in the $1$-th phase. Then, it learns the remaining classes evenly in the subsequent $N$ phases, where $N$ can be $5$, $10$, and $25$.
%
In both protocols, after the training of each phase we evaluate the resultant model on the test data of all seen classes.
%
Our final report includes the average accuracy over all phases and the last-phase accuracy which indicates the degree of model forgetting.
%
We run each experiment three times and report the average results.

\myparagraph{Memory Budget.}
There are two memory budget\footnote{Please note that we measure the memory by the number of original images. Each compressed exemplar in CIM takes less memory than the original image (Eq.~\ref{eq_consumed_memory}), resulting in more exemplars in the same memory.} settings. 1)~In the ``fixed'' setting, we remove some old-class exemplars when new exemplars from the current phase are added in the memory to maintain the ``fixed memory budget''. In this setting, we set the total memory to be $2,\!020$ samples for Food-101 and $2,\!000$ samples for ImageNet-100. For ImageNet-1000, we have two options---$5,\!000$ samples and $20,\!000$ samples. 2)~In the ``growing'' setting, a constant memory budget is allocated for each class across all phases and hence extra memory is appended when new classes come. In this setting, we set the budget to be $20$ samples per class for all datasets. Following~\cite{yan2021dynamically,wang2022foster}, we apply the ``fixed'' setting in LFS experiments and the ``growing'' setting in LFH experiments.

\input{tables/sota_food101_and_imagenet100}

\myparagraph{Implementation Details.}
Our implementation is based on the standard deep learning library PyTorch~\cite{paszke2019pytorch} and image processing library OpenCV~\cite{mordvintsev2014opencv}. Following~\cite{yan2021dynamically,wang2022memory,wang2022foster}, we use an $18$-layer ResNet~\cite{he2016deep} as the network backbone $\theta$ and a fully-connected layer as the classifier $\omega$ in all experiments. We use the same CIL training hyperparameters as in related works~\cite{hou2019lucir,yan2021dynamically,wang2022foster} for fair comparison: 1)~there are $200$ epochs in $1$-st phase and $170$ epochs in the subsequent phases; 2)~the learning rate $\lambda$ is initialized as $0.1$ and decreases to zero with a cosine annealing scheduler~\cite{loshchilov2016sgdr}; 3)~the SGD optimizer is deployed, with momentum factor set to $0.9$ and weight decay set to $0.0005$. For compression-related hyperparameters, we set the masking threshold $\tau$ as $0.6$ and the downsampling ratio $\eta$ as $4.0$. To build the CIM model, we apply PAUs with degrees $m=5$ and $n=4$ as learnable activation layers. For the optimization of the CIM model $\phi$ (i.e., the mask-level optimization), we initially set $\beta_1$ as $0.1$ and $\beta_2$ as $0.01$ and reduce them to zero following the scheduler of $\lambda$. $\mu$ and $\mu^\prime$ is set to $0.1$ and $0.2$, respectively. To smooth the training, we clip the gradient norm of $\phi$ to be no more than $1$. \emph{We report the result of hyperparameter sensitivity analysis in the supplementary materials.}

\subsection{Results and Analyses}
\label{subsection: results_and_analyses}

\input{tables/sota_imagenet1000}
\myparagraph{Comparing with the State-of-the-art.}
In Table~\ref{table: sota_food101_and_imagenet100}, we summarize the experimental results on two datasets (Food-101 and ImageNet-100) and in two CIL protocols (LFS and LFH). From the table, we have the following observations. 1)~Our CIM-based CIL consistently improves the state-of-the-art method FOSTER~\cite{wang2022foster} with clear margins in all settings. E.g., our method surpasses it by an average of $1.4$ percentage points on ImageNet-100, and $2.0$ percentage points on Food-101. 
%
%
2) Our CIM-based CIL achieves more significant improvements when $N$ becomes larger, e.g., on ImageNet-100 (LFH), our method improves FOSTER by $0.9$ and $3.3$ percentage points when $N$=$5$ and $N$=$25$, respectively. 3) Our CIM-based CIL achieves greater improvements consistently on Food-101 (than ImageNet-100). It improves baselines by $2.1$ percentage points on Food-101, while the improvement is $1.4$ on ImageNet-100 ($N$=$10$, LFS). 
%
It shows that our method is particularly effective when the representative visual cues of a class are from some of its components, e.g., the ``cream'' of the class ``cake''.

Table~\ref{table: sota_imagenet1000} shows the results on the large-scale dataset ImageNet-1000 in different memory settings ($M=20k$ and $M=5k$).
%
We can see that our CIM-based CIL improves FOSTER consistently. It is impressive that it achieves more improvements in the more strict memory setting ($M=5k$). Specifically, it boosts the average accuracy of FOSTER by $4.5$ percentage points when $M=5k$, significantly higher than that of $M=20k$ ($1.0$).

\input{tables/ablation_study}

\myparagraph{Ablation Study.}
Table~\ref{table_ablation_study} shows the ablation results. \emph{First block: baselines.}  Row 1 is for the baseline FOSTER~\cite{wang2022foster}. Row 2 shows the results of adding artifact augmentation (see Section~\ref{subsection: cam_based_compression_pipeline}). It shows directly apply this augmentation does not improve and even impair the model. 
\emph{Please note the models in below blocks all use this augmentation}.
%
\emph{Second block: activation methods.} 
%
Rows 3-6 show the results of using different activation methods to compress exemplars.
%
Row 3 is to downsample all pixels (i.e., no region is activated). Row 4 is to randomly select activation regions. Row 5 is to activate only the center region ($\frac{1}{4}$ of the original image), while row 6 is to use naive CAM.
%
Comparing them to row 1, we can see that using naively compressed exemplars can improve CIL models.
%
Row 4 outperforms rows 3-5, validating that it is more reliable to use the model's activation to generate compressed exemplars.
%
\emph{Third block: optimization methods.} 
Rows 7-9 are on top of Row 6 and are the results of applying different optimization strategies.
%
Row 7 is to manually select $\tau$ using a held-out set ($10\%$ of the dataset). Row 8 is to jointly train CIL and CIM models (for each input batch). Row 9 is the proposed method of using a global BOP.
%
\emph{Fourth block: two variants of CIM-based CIL.}
Rows 10-11 are two variants of row 9. In Row 10, only the activation layers in the last block of CIM are learnable, and previous blocks use ReLU. Compared to row 9, row 10 shows slightly worse performance. Row 11 shows the version of adding a weak downsampling ($\eta^\prime=2.0$) on discriminative regions, based on which more compressed exemplars are saved. It results in comparable performance to row 9 but increases costs.

\input{tables/comparing_with_mnemonics_and_jpeg}
\myparagraph{Comparing with Other Compression-based Methods.} Table~\ref{table: comparing_with_mnemonics_and_jpeg} shows our results comparing to two compression-based methods: Mnemonics~\cite{liu2020mnemonics} and MRDC~\cite{wang2022memory}. We can see that our method consistently outperforms them in all settings. 
%
This is because our method does not sacrifice the discriminativeness of exemplars while improving the number (variance) of exemplars in a phase-adaptive manner.
%
However, the two related methods either keep a fixed number of exemplars in the memory~\cite{liu2020mnemonics} or use uniform image compression without considering the properties of specific classes in different incremental phases~\cite{wang2022memory}.

\myparagraph{Visualizations (CAM vs. CIM).} Figure~\ref{fig_visualization} gives two visualization examples, ``Afghan hound'' and ``indigo bird'', each with the activation map as well as the bounding boxes.
%
The first column shows their respective confusing classes appearing in earlier phases.
CIM learns to focus on the discriminative (i.e., dissimilar to confusing classes) regions.
%
\input{misc/scripts/visualization}


\input{tables/hierarchical_compression_strategy}
\myparagraph{Results of Different-Size Objects.}
Table~\ref{table_hierarchical_compression_strategy} shows the results for small, middle, and large objects. These size categorization is according to ImageNet Object Localization Challenge~\cite{imagenetobjectlocalizationchallenge}. We calculate the bbox coverage for each class and take top $30$ classes with highest coverages as ``large'', rear $30$ classes with lowest coverages as ``small'' and the rest $40$ classes as ``middle''.
%
It is intriguing that our method achieves the highest improvement (over baseline) for small objects.
%
Our explanation is that small objects benefit more from image compression (than large ones), as their images contain more background pixels to downsample.
%