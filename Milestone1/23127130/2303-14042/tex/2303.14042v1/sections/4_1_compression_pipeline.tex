\section{Methodology}
\label{sec_methodology}

\input{misc/scripts/framework}

As illustrated in Figure~\ref{fig_teaser}\textcolor[rgb]{1,0,0}{d}, we alleviate the data imbalance problem by saving a larger number of compressed exemplars for old classes, where
we leverage pixel-selective compression, i.e., downsample only non-discriminative pixels.
%
To achieve this, \textbf{the ideal case} is that we have the pixel-level localization of foreground objects.
%
However, \textbf{the realistic case} is that such localization labels are expensive, most CIL benchmarks do not have the labels, and it is not fair to compare with other CIL methods if using the labels.
%
Without extra labeling, we introduce a CAM-based mask generation method, and based on it, we provide a baseline solution to pixel-selective compression in Section~\ref{subsection: cam_based_compression_pipeline}.
%
\textbf{The problem} of mask generation in CIL is that the optimal generation hyperparameters such as hard thresholds are changing in the dynamic environment (with the increasing number of classes and phases).
%
It is thus desirable to have an adaptive mask generation process.
%
To this end, we propose class-incremental masking (CIM)---a learnable mask generation model, in Section~\ref{subsection: class_incremental_masking}.


\subsection{CAM-based Compression Pipeline}
\label{subsection: cam_based_compression_pipeline}

%
Generating pixel-level labels for large-scale datasets, e.g., ImageNet~\cite{deng2009imagenet}, is non-trivial.
%
Using class activation maps (CAM) is a na\"ive solution with little computation costs. Its key idea is to make use of the activation of the classification model itself: on the feature maps, activated pixels are more discriminative than non-activated ones for recognizing the object, where ``activated'' means \emph{of high activation values} and \emph{with strong correlation with the classification weights of the object}.
After localizing activated pixels by CAM, we can generate a $0$-$1$ mask on them, e.g., by hard thresholding their normalized values, and then upsample the mask to the size of the input image~\cite{zhou2016cam}.

\myparagraph{From CAM to $0$-$1$ Mask.}
%
We extract CAM in the following steps. Given an image $x$ from $\mathcal{D}_i$ and its ground truth class label $y$, let $F(x;\theta_i)$ represent the feature block output by the feature extractor $\theta_i$, and $\omega_{i,y}$ for the classification weights of class $y$ in the classifier $\omega_i$.
%
The CAM of $x$ is:
\begin{equation}
\label{eq_cam}
\mathcal{M}^\textrm{CAM}=\frac{A-\min{(A)}}{\max{(A)} - \min{(A)}}, \ A={\omega_{i,y}^{\top}F(x;\theta_i)},
\end{equation}
where $\min(\cdot)$ and $\max(\cdot)$ operations are used for normalization.
%
Then, we upsample $\mathcal{M}^\textrm{CAM}$ to the size of image $x$ and use the same notation.
%
Each value in $\mathcal{M}^\textrm{CAM}$
denotes the activation strength of the model at a specific pixel location.
%
Following the way of generating $0$-$1$ masks in weakly-supervised semantic segmentation works~\cite{dong_2020_conta, chen2022recam, lee2021anti}, 
%
we apply a hard threshold $\tau$ (between $0$ and $1$) over all values of $\mathcal{M}^\textrm{CAM}$, and get the $0$-$1$ mask $\mathcal{M}^\tau$:
%
$\mathcal{M}^\tau\!=\!\mathbb{I}(\mathcal{M}^\textrm{CAM}\!>\!\tau)$, where $\mathbb{I}(\cdot)$ is the indicator function. 
%
In $\mathcal{M}^\tau$, $1$s indicate the locations of discriminative pixels, e.g., foreground pixels, based on which the model makes the prediction. 
%
While $0$s indicate mostly background pixels or non-discrimination pixels that can be downsampled as they make little contribution to the prediction.

After generating $0$-$1$ masks, it is ideal to keep them in the memory as meta information of compression. However, this is not efficient or feasible in CIL. There are two reasons.
%
1) The space for saving image-size masks is non-negligible.
Each mask pixel is a one-bit boolean value, and one mask takes around $\frac{1}{3\times8}=\frac{1}{24}$ of the memory of one RGB image.
%
2) The mask involves activated regions with irregular shapes. It is thus non-trivial to perform any standard downsampling algorithm~\cite{parker1983comparison} on the remaining regions.
%

\noindent
\textbf{From $0$-$1$ Masks to Bounding Boxes (BBox).}
A simple workaround is to generate a tight bounding box (bbox) to cover the positions of $1$s in $\mathcal{M}^\tau$, and use the bbox for compression.
%
Specifically, given $\mathcal{M}^\tau$, we obtain the coordinate representation of the bounding box as:
\begin{equation}
\label{eq_bbox}
\mathcal{B}=[\min h,\min w;\max h,\max w]_{(h,w):\mathcal{M}^\tau(h,w)=1},
\end{equation}
%
where $h$ and $w$ denote the vertical and horizontal coordinates of the $1$ on $\mathcal{M}^\tau$, respectively.
%
We highlight that $\mathcal{B}$ consists of four integers only and takes negligible memory overhead compared to $\mathcal{M}^\tau$. In addition, we ``reshape'' the irregular shape (of the activated region) in $\mathcal{M}^\tau$ into a rectangular $\mathcal{B}$, so our downsampling operation on the pixels outside the rectangular becomes easy.
%

\myparagraph{Compression with BBox.}
Given the image and its bbox on foreground, the compression is implemented by downsampling pixels outside the bbox.
%
Specifically, as illustrated in Figure~\ref{figure: framework}, we compress the image $x$ to $\tilde{x}$ as follows,
\begin{equation}
\label{eq_compressed_image}
\tilde{x}=\mathcal{M}^\mathcal{B} \odot x + (1 - \mathcal{M}^\mathcal{B}) \odot x_\eta,
\end{equation}
where $\mathcal{M}^\mathcal{B}$ is the binary mask according to $\mathcal{B}$, i.e., the values of $\mathcal{M}^\mathcal{B}$ are $1$ inside $\mathcal{B}$, and $0$ otherwise. 
$x_\eta$ is the fully downsampled version of $x$ with a downsampling ratio $\eta$ ($\eta>1$), $\odot$ denotes the element-wise product,
and $+$ denotes the element-wise addition. Both $\odot$ and $+$ are applied independently on each RGB channel.
%

The memory allocated for the compressed image $\tilde{x}$ is as follows,
\begin{equation}
\begin{aligned}
\label{eq_consumed_memory}
m_{\tilde{x}}&=\frac{H_{\mathcal{B}}W_{\mathcal{B}}}{HW} + \frac{1}{\eta}\Big(1 - \frac{H_{\mathcal{B}}W_{\mathcal{B}}}{HW}\Big)\\&=1-\Big(1-\frac{1}{\eta}\Big)\cdot\Big(1-\frac{H_\mathcal{B}W_\mathcal{B}}{HW}\Big),
\end{aligned}
\end{equation}
where $H_{\mathcal{B}}$ and $W_{\mathcal{B}}$ are the height and width of $\mathcal{B}$, respectively. $H$ and $W$ are the height and width of the original image $x$, respectively.
%
$m_{\tilde{x}}$ is always smaller than $1$ where $1$ denotes the memory unit of saving one original image $x$. Therefore, we can save a larger number of compressed exemplars within the same memory budget. We denote the set of compressed exemplars in the $i$-th phase as $\tilde{{\mathcal E}}_i$.
%

\myparagraph{Compression Artifacts.} 
The above compression introduces artifacts to the compressed images,
%
i.e., there is a resolution mutation around bounding box edges.
%
From the perspective of spectrum analysis~\cite{jain1989fundamentals,castleman1996digital}, such mutation carries noisy and high-frequency components and impairs the model training in subsequent phases. 
%
We mitigate the effect of these artifacts by implementing the following data augmentation: in each training epoch, we transform a random subset of $\mathcal{D}_i$ into compressed images with CAM-based bounding boxes using the same downsampling ratio.
%
Using this augmentation enables the model to ``simulate'' the training with compressed images and learn to be invariant to compression artifacts.

