% ****** Start of file main.tex ******
\documentclass[%
 reprint,
superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
%preprintnumbers,
 nofootinbib,
%nobibnotes,
%bibnotes,
 amsmath,amssymb,
 aps,
%pra,
 prb,
%showkeys,
%rmp,
%prstab,
%prstper,
%floatfix,
]{revtex4-2}


% Packages
\input{packages}

% Custom commands
\input{commands}

\begin{document}
%-----------------------------
% FRONT MATTER

\preprint{APS/123-QED}

% Title and authors
\input{title_and_authors}

% Abstract
\input{abstract}

% To print the title, authors and abstract              
\maketitle

%-----------------------------
% MAIN MATTER

\section{Introduction}

The last few decades have seen a revolutionary change in the paradigms underpinning our knowledge of the evolution and dynamics of the Universe as a whole.
The research field of cosmology has gone from ``the quest for two numbers'' to a new rich phenomenological field rooted in precise astronomical observations; see \cite{2022ARNPS..72....1T} for a vivid historical reconstruction.
The resulting era of ``precision cosmology'' delivered a detailed description of the Universe at the largest scales, with a standard cosmological model capable of explaining all current observations, modulo few persisting statistical tensions; see, e.g.,~\cite{Planck:2018nkj,Planck:2018vyg,Riess:2021jrx,Pan-STARRS1:2017jku,DES:2021wwk,DES:2021bvc}.
This spectacular achievement has been possible by the piling up of ever more accurate astronomical observations, the overwhelming majority of which obtained with electromagnetic (EM) telescopes over the whole accessible band of the EM spectrum.

Nevertheless since the first direct detection of gravitational waves (GWs) by the LIGO and Virgo collaborations in 2015~\cite{LIGOScientific:2016aoc}, we now possess a new whole spectrum that can provide a wealth of cosmological information complementary to EM observations.
GWs can be used as standard cosmological rulers~\cite{schutz86} and thus provide a map of the cosmic expansion history at different redshifts.
The luminosity distance of a binary system emitting GWs can in fact be extracted from the detected GW signal without relying on any phenomenological relation or calibration at lower redshifts.
In other words, compact binaries emitting GWs are \textit{absolute} cosmic distance rulers since they do not depend on the so-called \textit{cosmic distance ladder}.
In analogy to supernovae-type Ia, which are \textit{calibrated} cosmic distance rulers commonly called standard candles, GW signals from compact binaries containing black holes (BHs) and neutron stars (NSs) are commonly known as \textit{standard sirens}~\cite{2005ApJ...629...15H,Dalal:2006qt}.

Unfortunately the redshift of the source is not one of the parameters that we can easily obtain from GWs emitted by compact binaries.
For this reason standard sirens cannot be used straightaway to map the expansion of the Universe through the well-known \textit{distance-redshift relation}, contrary to standard candles for which a redshift measurement is usually readily available.
Different methods have been proposed to obtain complementary redshift information to a standard siren.

The simplest and most intuitive of these methods consists in observing an EM counterpart of the GW event to identify its host galaxy~\cite{schutz86}.
In such cases the redshift of the GW source can be estimated by measuring the redshift of the host galaxy, providing in this way a single redshift value for the distance-redshift diagram.
Unfortunately this method applies only to GW events for which an EM counterpart can be observed, which are commonly referred to as ``bright sirens'' in a cosmological context.
So far the LIGO-Virgo-Kagra (LVK) Collaboration observed only one such bright siren, namely the multimessenger binary neutron star (BNS) merger GW170817~\cite{ligobns,MMApaper}.
The coincident measurements of both distance and redshift of this event delivered the first ever cosmological measurement with GWs: a constraint on the Hubble constant of $\hubble = \SI[parse-numbers = false]{70^{+12}_{-8}}{\kilo\meter\per\second\per\mega\parsec}$~\cite{2017Natur.551...85A}.
Future observational runs of the LVK detectors are expected to improve upon this result with the addition of further bright sirens, not only BNSs but also BH-NS binaries for which an EM counterpart is spotted~\cite{chen17,Feeney:2018mkj,Feeney:2020kxk,Vitale:2018wlg}.
The technological limitations of current GW interferometers however cannot guarantee sufficiently numerous detections to achieve a measurement of $\hubble$ better than a few \%, while constraints on other cosmological parameters are well beyond their reach~\cite{Chen:2020zoq}.

If no EM counterpart can be detected, other methodologies are nevertheless used to gather redshift information complementary to a GW binary signal.
The so-called ``dark siren'' method~\citep{schutz86,PhysRevD.86.043011,chen17,fishbach,2020PhRvD.101l2001G,Finke:2021aom,Gray2022,PhysRevD.105.023523,2022arXiv221208694G}, sometimes referred to as the ``statistical method,'' consists in cross-matching the sky localization error volume, sometimes simply called volume error-box, of the GW source with galaxy catalogs collected by EM surveys, either readily available or constructed \textit{ad hoc} along the sky localization cone of the detected GW signal.
Such a method has been proved to work with both simulated~\cite{PhysRevD.86.043011,2020PhRvD.101l2001G,Gray2022} and observational data~\cite{fishbach,2019ApJ...876L...7S,2020ApJ...900L..33P,2021ApJ...909..218A,Finke:2021aom}.
The latest results from all the LVK observational runs combined yield $\hubble = \SI[parse-numbers = false]{68^{+8}_{-6}}{\kilo\meter\per\second\per\mega\parsec}$~\cite{2021arXiv211103604T}; see also~\cite{2021arXiv211106445P}.
Although this represents only a small improvement with respect to the constraint obtained from GW170817 only, the large number of expected GW detections without EM counterparts renders dark sirens a promising and robust method to estimate cosmological parameters from future GW observations.
Similar approaches exploiting the spatial cross-correlation between GW sources and galaxies, have also been proposed and shown to work well once a large amount of GW events will be observed~\citep{PhysRevD.93.083511,Mukherjee:2019wcg, Mukherjee:2020hyn,Bera:2020jhx, Diaz:2021pem,Balaudo:2022znx}.

A third methodology to obtain redshift information for standard sirens is based on the insight and modeling of the population distribution of intrinsic parameters of the GW sources~\citep{1993ApJ...411L...5C,Taylor_2012,Farr_2019,2020arXiv200602211M,mastrogiovanni_2021,Mukherjee:2021rtw,2022JCAP...09..012L,Ezquiaga_2022, Karathanasis:2022rtr,Mancarella:2022cnu,2022PhRvL.129f1102E}, in particular their masses, spins, and merger rate evolution.
Such a method is usually called ``spectral sirens'' due to the use of features in the distribution spectra of GW source parameters, whose parameters are inferred simultaneously with the cosmological parameters.
It has already been applied to real LVK data with the most recent measurement registering a constraint $\hubble = \SI[parse-numbers = false]{68^{+12}_{-7}}{\kilo\meter\per\second\per\mega\parsec}$~\cite{2021arXiv211103604T}.
The pros of this method are that it does not require any EM information, but the cons are that it introduces a dependence on the modeling of the parameter distributions of the underlying astrophysical population of GW sources.

In general the current status of GW cosmology outlined above, with results from standard sirens still at large experimental uncertainties, begs an analogy between EM cosmology in the era of the ``quest for two numbers.''
Nowadays the main objective of GW cosmology consists in the measurement of the Hubble constant, with low chances to access information on other cosmological parameters.
Similarly to the situation for EM cosmology at the end of the last century, the current 2nd generation of GW interferometers do not have in practice the constraining power needed to push observations beyond the quest for $\hubble$.
This scenario will change dramatically when 3rd generation (3G) interferometers will come online: the new era of ``precision GW cosmology'' will begin.

Two possible concepts for 3G interferometers are currently under consideration for construction in the 2030s: the Einstein Telescope (ET) in Europe~\cite{2010CQGra..27s4002P,2011CQGra..28i4013H,2020JCAP...03..050M} and the Cosmic Explorer (CE) in the USA~\cite{Reitze2019Cosmic,Evans:2021gyd}.
They are both aimed at greatly improving the sensitivity around the same frequency band of the LVK detectors, as well as at extending observations at lower frequencies down to a few \si{\hertz}.
The scientific potential of 3G detectors is huge, with expected observations of \num[retain-unity-mantissa = false]{1e5}-\num[retain-unity-mantissa = false]{1e6} GW signals from compact binary coalescences over few years of observations.
ET and CE will deliver new breakthrough observations on multiple subjects encompassing astrophysics, cosmology, and fundamental physics; see~\cite{Kalogera:2021bya} for a summary of their science case.

In terms of cosmology, 3G detectors will exploit the standard siren methodologies described above to attain accurate and precise measurements of the cosmological parameters.
Bright sirens may yield subpercent constraints on $\hubble$ with $\mathcal{O}$(100) observations of multimessenger BNSs, which are expected after few years of operation~\cite{Cai:2016sby,Zhao:2017cbb,Belgacem:2019tbw,deSouza:2021xtg,deSouza:2021xtg,Califano:2022syd,Dhani:2022ulg,Alfradique:2022tox} (similar results are claimed for binary neutron star-black holes (NSBHs)~\cite{Gupta:2022fwd}).
However, the expected EM counterpart signals from these BNSs can only be detected at relatively low redshift ($z \lesssim 0.5$)~\cite{Belgacem:2019tbw}, implying that further cosmological parameters beyond the Hubble constant may not be well measured, except perhaps the equation of state of dark energy~\cite{Sathyaprakash:2009xt,Zhao:2010sz}.

Spectral sirens on the other hand will be able to exploit the full redshift range of observable binary black holes (BBHs), which extends well beyond the reach of BNSs for 3G detectors.
This method will consequently not only deliver stringent constraints on $\hubble$, but it will also provide interesting results at high redshift for both dark energy and further cosmological parameters~\cite{Taylor:2011fs,Taylor:2012db,2022PhRvL.129f1102E,Leandro:2021qlc,Ye:2021klk}.
Moreover, another similar method that will be applicable to 3G detectors thanks to their exquisite precision, consists in the simultaneous inference of both the equation of state of neutron stars and their redshift~\cite{Messenger:2011gi}.
Such an approach provides a redshift for each BNS, without the need of an EM counterpart, but introduces a dependence on the modeling of the equation of state of neutron stars which can introduce systematics if not properly accounted for.
Nevertheless recent estimates provide forecasts on the measurement of the Hubble constant that range from few \% to subpercent levels, showing that further cosmological parameters are within reach of an accurate measurement~\cite{2022PhRvD.106l3529G,2021PhRvD.104h3528C,Dhani:2022ulg,Jin:2022qnj}.

Contrary to bright sirens, dark sirens in the 3G era have not yet been systematically investigated.
Recent exploratory studies, based on a number of over-simplifying assumptions and restricted to low redshift galaxy catalogs ($z<0.3$), claim that constraints on $\hubble$ can reach an extremely optimistic precision of $\mathcal{O}(0.01\%)$, or even better, in 5 years of observations~\cite{Yu:2020vyy,Song:2022siz}.
Other exploratory investigations using either BBHs or NSBHs as ``golden'' dark sirens, namely well-localized events for which one single galaxy is contained in their sky localization volume, show instead that $\mathcal{O}(0.1)\%$ constraints on $\hubble$ can be obtained again in 5 years of observations~\cite{Borhanian:2020vyr,Gupta:2022fwd}.
Less optimistic results have been recently reported in~\cite{Zhu:2023jti}, where a more realistic simulation yields $\mathcal{O}(1)\%$ constraints on $\hubble$ and $\mathcal{O}(10)\%$ constraints on $\Omega_m$ with 300 BBHs detected by ET plus one CE.
Further analyses, under more realistic assumptions and using the complete information from galaxy catalogs, are clearly needed to make clarity on the expected dark siren potential of 3G detectors.

The scope of the present paper consists in producing reliable cosmological dark sirens forecasts with BBH mergers for the 3G era.
Compared to the existing literature, our study extends to higher redshift, it reduces the underlying simplifying assumptions making our simulation more realistic, and it enlarges the cosmological inference to cosmological parameters beyond $\hubble$.
In \cref{sec:discussion} we will compare our results with the ones reported previously.
All these results, especially if folded together with other cosmological expectations from GW large-scale observatories in the 2030s, notably for example from space-borne detectors~\cite{LISACosmologyWorkingGroup:2022jok,Tamanini:2016zlh,Caprini:2016qxs,Cai:2017yww,DelPozzo:2017kme,LISACosmologyWorkingGroup:2019mwx,Speri:2020hwc,2021MNRAS.508.4512L,Muttoni:2021veo,Yang:2021qge}, show that 3G detectors will usher an era of precision GW cosmology, similarly to how EM telescopes and surveys opened an era of precision EM cosmology 20-30 years ago.
The era of the ``quest for one number'', namely $\hubble$, will leave space for a plethora of different cosmological measurements with GWs which will offer an unprecedented and clear picture of the gravitational Universe.

This study is organised as follows. 
In \cref{sec:GWpopulation} we construct a realistic, simulated population of BBH mergers based on the most recent LVK observations.
In \cref{sec:GWPE} we present our approach to detect the GW signals emitted by BBHs and measure their parameters with Fisher information techniques.
In \cref{sec:errorboxes} we describe how we build our galaxy catalogs, produce GW sky-localization error volumes and associate potential host galaxies to GW events.
In \cref{sec:cosmoinference} the details of our Bayesian inference approach to measure the cosmological parameters are outlined.
In \cref{sec:results} we present the results of our analyses, namely the expected constraints on $\lcdm$ for different observational scenarios.
Finally in \cref{sec:discussion} we discuss our findings, their implications and compare them with the literature, while in \cref{sec:conclusion} we conclude.


\section{Simulation of the mock gravitational-wave event catalog}
\label{sec:GWpopulation}

In order to infer the cosmological parameters with 3G detectors, we first need to define an astrophysical population of BBHs. Each source parameter is extracted from some probability density function which are motivated by astrophysical assumptions. In the following we discuss the generation of the parameters that characterize each BBH, that is, the BH component masses and spins, sky position, redshift, inclination, polarization angles, and coalescence time and phase. While most of these parameters are described by relatively trivial distributions, others need to be investigated more carefully.

\subsection{Masses}

The recent observing runs with the Advanced LIGO and Advanced Virgo interferometers (O1 \cite{abbott2019gwtc1}, O2 \cite{abbott2021gwtc2} and O3 \cite{abbott2021gwtc3}) enriched the graveyard of known compact binary mergers with a total of \num{90} events, and the analysis of the population properties of these events~\cite{ligo2021population} shed light on their nature. 
Here we adopt these latest results to extract the masses of the individual components.

For BBHs, the primary BH mass distribution may be described by different fits~\cite{ligo2021population}. Among them, we choose the \texttt{POWER LAW + PEAK} which provides a good description of the overall observations. 
This model features a power law and a Gaussian peak around $\sim \SI{35}{\msun}$, which reflects the pair instability supernovae lower edge. Specifically, the probability distribution reads
\begin{equation}
    p(m_1) \propto \bigl[(1 - \lambda_{\rm peak})\mathcal{B}(m_1) + \lambda_{\rm peak}\mathcal{G}(m_1)\bigr] \mathcal{S}(m_1) \, ,
\label{eqn:p_of_m_1}
\end{equation}
%
where $\mathcal{B}(m) \propto m^{-\alpha}$ is a power law with spectral index $\alpha = 3.5$, $\mathcal{G}(m) \propto \mathcal{N}(\mu_{\rm BH}, \sigma^2_{\rm BH})$ is a Gaussian with mean $\mu_{\rm BH} = \SI{34}{\msun}$ and width $\sigma_{\rm BH} = \SI{5.69}{\msun}$, $\lambda_{\rm peak} = 0.038$ is a factor that controls the relative frequency of mergers in the power-law-dominated region and the Gaussian one, and finally $\mathcal{S}(m) \in [0, \, 1]$ is a smoothing piece-wise function, defined through
   \begin{multline}
        \mathcal{S}(m) = \\
        \begin{cases}
        0 & \text{if} \; m < m_{\rm min}, \\
        \Bigl(f(m - m_{\rm min}) + 1\Bigr)^{-1} & \text{if} \; m_{\rm min} \le m < m_{\rm min} + \delta_m, \\
        1 & \text{if} \; m \ge m_{\rm min} + \delta_m,
        \end{cases}
    \end{multline}
    with    
    \begin{equation}
        f(m) = \exp\biggl(\frac{\delta_m}{m} + \frac{\delta_m}{m - \delta_m}\biggr) \, , \qquad \delta_m = \SI{4.9}{\msun} \, .
    \end{equation}

We compute the secondary mass of the BBH through the mass ratio. The probability distribution of this parameter is described by the following expression
\begin{equation}
    p(q) \propto q^{\beta}\mathcal{S}(q m_1) \, ,
\end{equation}
with spectral index $\beta = 1.1$. We refer the reader to Ref.~\cite{ligo2021population} for more details about these mass distributions.


\subsection{Spins} \label{sec:spins}

Each binary component is characterized by a spin vector.
While the proper sample of the spins should keep into account all the \num{3} spatial components, we choose to assume only nonprecessing binaries, i.e., systems where the individual object spins are aligned with the total angular momentum.
Our choice reduces the complexity of the simulations, since each object is now described by \num{1} spin component, which we set to be along the $z$ axis. In particular, BH spin magnitudes are extracted from a uniform distribution between $[\num{-0.75}, \, \num{0.75}]$, as assumed in recent works (e.g., \cite{borhanian2022listening}). Thus, we do not include the effects of precession in the population.


\subsection{Angles and coalescence time}

Each binary is described by a set of different angles which includes:
%
\begin{itemize}
    \item The sky position angles, typically labeled by right ascension (RA) and declination (DEC). These two parameters range respectively between $[0, \, 2\pi]$ and $[-\pi/2, \, \pi/2]$. 
    Assuming an isotropic Universe, we sample the source
    sky positions uniformly on a spherical surface according to
    \begin{equation}
       p(\theta, \, \varphi) d\theta d\varphi  \propto \sin\theta d\theta d\varphi = p(\theta) d\theta \, p(\varphi) d\varphi \, ,
    \end{equation}
    where $\theta = \pi/2 - \text{DEC}$ is the colatitude and $\varphi = \text{RA}$ is the longitude, while
    \begin{equation}
    \begin{split}
        p(\theta) d\theta & \propto \sin\theta d\theta \, , \\
        p(\varphi) d\varphi & \propto d\varphi \, .
    \end{split}
    \end{equation}
    \item The inclination angle $\iota$, defined as the angle between the line of sight and the angular momentum of the binary. It takes values between $[0, \, \pi]$, where the lower (upper) boundary reflects face-on (face-off) binaries, while the midpoint characterizes edge-on binaries. The inclination angle is extracted uniformly in $\cos\iota$. Hence, its probability distribution follows the same of $\theta$, that is
    %
    \begin{equation}
        p(\iota) d\iota \propto \sin\iota d\iota \propto d\cos\iota \, .
    \end{equation}
    %
    This is to avoid a uniform sample in $\iota$ which would overestimate the number of loud (i.e.~face-on and face-off) sources.
    \item The polarization angle $\psi$, which ranges between $[0, \, \pi]$.
    This parameter represents a generic rotation of the GW main axes on the plane perpendicular to the direction of propagation.
    We extract $\psi$ samples from a uniform distribution.
    \item The coalescence phase $\Phi_c$, with values within $[0, \, 2\pi]$. It represents a reference value, conventionally associated to the merger, from which to determine the evolution of the phase of the GW signal. Its values are drawn from a flat distribution. 
\end{itemize}

In the same fashion, we draw uniformly the GPS coalescence time $t_c$ in a \SI{1}{\year} time window, starting from a fixed GPS reference time.


\subsection{Redshift} \label{sec:redshift_distribution}

While current observations still provide valuable information on the distance distribution of compact binary mergers, the horizon of current detectors is not sufficiently large to allow a precise reconstruction of the redshift distribution of GW sources across the cosmic history. For this reason we design a probability density function $p(z)$ for the redshift which is based on plausible astrophysical assumptions.

The merger of a binary system occurs after a time delay $t_d$ since its formation. Time delay, and the redshifts of the merger $z_m$ and formation $z_f$ of the system are related through
%
\begin{equation}
\begin{split}
    t_d & = \int_{z_m}^{z_f} \frac{dz}{(1 + z)\hubblep(z)} \\
        & = \int_{0}^{z_f} \frac{dz}{(1 + z)\hubblep(z)} - \int_{0}^{z_m} \frac{dz}{(1 + z)\hubblep(z)} \, ,
\end{split}
\label{eqn:time_delay}
\end{equation}
%
where $\hubblep(z) = \hubble \sqrt{\Om (1+z)^3 + \Ol}$ is the Hubble parameter, $\hubble$ is the Hubble constant, $\Om \equiv \Omega_{m,0}$ is the matter density parameter and $\Ol$ is the dark energy density parameter. \Cref{eqn:time_delay} represents the lookback time difference between $z_f$ and $z_m$: given $t_d$ and $z_f$, one can compute $z_m$ by inverting \cref{eqn:time_delay}. We assume that binary formation and merger are tracked by the star formation rate density $\Psi(z)$ (units \si{\msun\giga\parsec^{-3}\year^{-1}}) with the addition of a prescription for time delay effects. Throughout this work, we adopt the Madau-Fragos $\Psi(z)$, modeled by
%
\begin{equation}
    \Psi(z) = a \frac{(1+z)^b}{1 + [c(1+z)]^d} \, \si{\msun\giga\parsec^{-3}\year^{-1}} \, ,
\end{equation}
%
where $a = 0.01$, $b = 2.6$, $c = 1/3.2$ and $d = 6.2$, as reported in \cite{madau2017radiation}. The merger rate density $\dot{n}(z)$ (units \si{\giga\parsec^{-3}\year^{-1}}) is obtained by integrating $\Psi(z_f)$ over all the possible time delays:
%
\begin{equation}
    \dot{n}(z) \propto \int_{t_d^{\rm min}}^{t_d^{\rm max}} dt_d \, \Psi(z_f(z, t_d)) \, p(t_d) \, .
\label{eqn:merger_rate_density}
\end{equation}
%
Here $p(t_d)$ is a probability density function associated to the time delay, while $t_d^{\rm min}$ and $t_d^{\rm max}$ are respectively the minimum and maximum time delay of the distribution. For BBH systems we consider a minimum time delay $t_d^{\rm min} = \SI{10}{\mega\year}$, while we fix the maximum to $t_d^{\rm max} = \SI{10}{\giga\year}$ as in \cite{borhanian2022listening}.
Furthermore, we assume $p(t_d) \propto t_d^{-1}$, which becomes
%
\begin{equation}
    p(t_d) = \biggl(\ln \biggl(\frac{t_d^{\rm max}}{t_d^{\rm min}}\biggr) t_d\biggr)^{-1} \, ,
\end{equation}
%
once rescaled. In the same fashion, \cref{eqn:merger_rate_density} must be normalized. To compute the normalization factor (units \si{\msun^{-1}}), we require that $\dot{n}(0) = \dot{n}_0$, i.e.,~that the merger rate density evaluated at $z=0$ must be equal to the state-of-the-art local merger rate density of BBHs. In particular, we adopt $\dot{n}_{0} = \SI{23.9}{\per\cubic\giga\parsec\per\year}$ as reported in \cite{ligo2021population}.

Next, we obtain the merger rate per unit redshift bin $dR/dz$ through
%
\begin{equation}
    \frac{dR}{dz}(z) = \dot{n}(z) \frac{dV}{dz}(z) = \dot{n}(z) \frac{4\pi c \, d_c(z)^2 }{\hubblep(z)} \, ,
\label{eqn:source_frame_merger_rate}
\end{equation}
%
where $dV/dz$ is the differential comoving volume element, $c$ is the speed of light and $d_c(z) = c \int_{0}^{z} d\tilde{z}/\hubblep(\tilde{z})$ represents the comoving distance.
However, \cref{eqn:source_frame_merger_rate} is a source-frame quantity. The expansion of the Universe affects the source-frame merger rate with a time dilation factor $dt/dt^{\rm obs} = (1 + z)^{-1}$, therefore the observer-frame merger rate $d\mathcal{R}/dz$ reads
%
\begin{equation}
    \frac{d\mathcal{R}}{dz}(z) = \frac{1}{(1 + z)}\frac{dR}{dz}(z) \, .
\label{eqn:observer_frame_merger_rate}
\end{equation}

By integrating the observed merger rate, \cref{eqn:observer_frame_merger_rate},
one can recover the total number of mergers per unit time in the integration domain, i.e.,~the cosmic merger rate $\mathcal{R}$. Specifically:
%
\begin{equation}
    \mathcal{R} = \int_{z_{\rm min}}^{z_{\rm max}} dz \frac{d\mathcal{R}}{dz}(z) \, ,
\end{equation}
%
where $[z_{\rm min}, \, z_{\rm max}]$ is the relevant redshift interval. Our study focuses on GW sources within $z_{\rm min} = \num{0}$ and $z_{\rm max} = \num{50}$, and the associated BBH cosmic merger rate is $\mathcal{R} = \SI{49056}{\per\year}$, which is in broad agreement with other recent works (see e.g.~\cite{borhanian2022listening,iacovelli2022forecasting}).
The redshift probability density function $p(z)$ is then given by
%
\begin{equation}
    p(z) = \frac{1}{\mathcal{R}} \frac{d\mathcal{R}}{dz}(z) \, ,
\end{equation}
%
which represents our sampling distribution (see the dashed-black histogram in the left plot of \cref{fig:total_and_detected_pops} for a representative sample).

We convert the sampled redshifts in to luminosity distances through the redshift-luminosity distance relation
\begin{equation}\label{eqn:luminosity_distance_flat}
    d(z, \Omega) = \frac{c}{\hubble} (1+z) \int_0^z \frac{d\tilde{z}}{\sqrt{\Om(1+\tilde{z})^3 + 1 - \Om}} \,,
\end{equation}
assuming a flat $\lcdm$ cosmology with values coming from the Planck first-year data~\citep{PlanckCollaboration2014}: $h = \hubble/\SI{100}{\kilo\meter\per\second\per\mega\parsec} = 0.673$ and $\Om = 0.315$.
These values define our fiducial cosmology, on which is based the galaxy catalog that we use (see~\cref{subsec:gal_cats}).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Gravitational-wave detection and Fisher analysis}
\label{sec:GWPE}

The future of ground-based GW astronomy will be led by \num{3}G interferometers. This work aims at testing the capabilities of ET and CE(s) by considering different combinations of such detectors. 
In this study, ET is assumed to have 
a triangular-shaped configuration of \num{3} independent detectors co-located in Italy (E1, E2, E3), while CE consists of \num{2} independent L-shaped detectors, placed respectively in the United States (CE1) and in Australia (CE2). We adopt the latest sensitivity curves available, specifically we consider the \SI{10}{\kilo\meter}-arm ET-D noise curve model \cite{2011CQGra..28i4013H} and the baseline \num{40}-\num{20} \si{\kilo\meter} arms CE \cite{Evans:2021gyd} curves, displayed in \cref{fig:sensitivities}. We further limit our work to the assumption that all the detectors are always operative during the whole observation time (i.e.,~full duty cycles are considered), which we set to be \SI{1}{\year}. In \cref{tab:detectors} we summarize the specifics of the individual observatories. In this study we concentrate on three particular networks of 3G detectors: a single ET, ET and CE1 (ET+CE1), and ET with the two CEs (ET+CE1+CE2). We do not consider the network made up by CE1 and CE2 (CE1+CE2) as we want to focus on ET and its potential in a network.

\begin{figure}
    \centering
    \includegraphics[width=0.47\textwidth]{Figures/sensitivities.pdf}
    \caption{Sensitivity curve of the detectors considered in this study, together with a GW150914-like signal, colors as in legend. The characteristic strain is a dimensionless quantity related to the detector's power spectral density through $\sqrt{f \psd(f)}$. In the case of a GW signal, instead, it is defined as $2 f |\tilde{h}(f)|$. These two expressions, which implicitly enter in \cref{eqn:snr}, are useful to visualize the loudness of a GW signal when plotted together. The ET-D curve is rescaled, as reported in \cite{2011CQGra..28i4013H}, to take into account for the triangular geometry. Each detector observes a different strain, due to the modulation of its respective antenna pattern function. Here we display only the unprojected signal with the \texttt{IMRPhenomXHM} waveform model.}
    \label{fig:sensitivities}
\end{figure}


\subsection{Signal modeling and injection settings}

\begin{table*}[t]
\centering
%\bgroup
%\setlength{\tabcolsep}{0.5em} % horizontal padding
\def\arraystretch{\tabvspace} %  vertical padding (default is 1)
\begin{tabular}{ l c c c c c c c }
\hline
\hline
\textbf{Full name} & \textbf{Short name} & \textbf{Latitude} & \textbf{Longitude} & \textbf{x-arm azimuth} & \textbf{y-arm azimuth} & \textbf{$\psd$} & \textbf{$f_{\rm start}$} \\
\hline
\multirow{3}*{Einstein Telescope (ET)} & E1 & $0.7615$ & $0.1833$ & $0.3392$ & $5.5752$ & ET-D & \SI{1}{\hertz} \\
                                       & E2 & $0.7629$ & $0.1841$ & $4.5280$ & $3.4808$ & ET-D & \SI{1}{\hertz}\\
                                       & E3 & $0.7627$ & $0.1819$ & $2.4336$ & $1.3864$ & ET-D & \SI{1}{\hertz}\\
\hline
\multirow{2}*{Cosmic Explorers (CEs)} & CE1 & $0.7613$ & $-2.0281$ & $1.5708$ & $0$ & baseline \SI{40}{\kilo\meter} & \SI{5}{\hertz}\\
                                      & CE2 & $-0.5811$ & $2.6021$ & $2.3562$ & $0.7854$ & baseline \SI{20}{\kilo\meter} & \SI{5}{\hertz}\\

\hline
\hline
\end{tabular}
\caption{Summary of the 3G GW detectors we consider in this study. Angles are rounded and expressed in \si{\radian}, while the last column refers to the lowest frequency of the associated power spectral density $\psd$.
}
\label{tab:detectors}
\end{table*}

We simulate each GW signal directly in the frequency domain with a frequency resolution of $df = \SI[exponent-base = 256, retain-unity-mantissa = false]{1e-1}{\hertz}$, starting from \SI{1}{\hertz} and up to a sharp cut at \SI{4096}{\hertz}. 
In the context of \num{3}G detectors, GW signals may last from some minutes to several hours. Earth's rotation will play a crucial role for the localization of a source in the sky, since each detector will observe a strain modulated by its unique antenna pattern function.
However, in this work we consider BBHs, which emit signals in the 3G band with typical duration that spans from a few seconds and up to an hour. We therefore trade Earth's rotation effects with computational efficiency, leading to conservative results in the rare limit of long lasting signals. We validate our choice by comparing the sky location uncertainty of a GW150914-like signal including and not including Earth's rotation, finding no significant improvement.

We further assume that each source may be resolved and its parameters estimated independently. While we may expect to observe some signals overlapping in time (see, e.g.,~\cite{samajdar2021biases,himemoto2021impacts,pizzati2022toward}), in this work we assume that by the time \num{3}G detectors will become operative we will be able to make robust parameter estimation for all detected compact binary signals, i.e.~we can benefit from all the BBH sources as dark sirens.

We model each signal, or injection, with the \texttt{IMRPhenomXHM} \cite{PhysRevD.102.064002} waveform in order to capture higher harmonics, which are expected to be important for asymmetric-mass binaries and can potentially break fundamental degeneracies between source parameters, most notably luminosity distance and inclination~\cite{Littenberg2012, Varma2014, Shaik2019}. This waveform model assumes nonprecessing binaries, so that each object's spin is characterized only by one spatial component parallel to the total orbital angular momentum of the system, as modeled in \cref{sec:spins}.

We make use of the publicly available \textsc{Python} library \textsc{PyCBC} \cite{alex_nitz_2021_5256134} to generate our injections and compute the quantities described in \cref{eqn:snr,eqn:fisher_matrix}.

\begin{figure*}
    \centering
    \includegraphics[width=0.4973\textwidth]{Figures/total_detected_pop_v2.pdf}
    \includegraphics[width=0.4973\textwidth]{Figures/inverse_cumulative_v2.pdf}
    \caption{Left: Redshift distributions of the total and detected population for different networks in one year of observation (full duty cycle), colors as in legend. Right: Number of detected GW events left above a given $\SNR_{\rm net}$, colors as in legend.
    }
    \label{fig:total_and_detected_pops}
\end{figure*}


\subsection{The Fisher matrix approach}

The parameter estimation of a GW source often requires large computational resources due to the vast parameter space that needs to be explored with sampling methods.
The Fisher information matrix (FIM) framework offers a more accessible way to assess the measurement capabilities of detector networks~\cite{cutler1994gravitational} in the strong-signal limit by approximating the parameter posteriors to be Gaussian (under the assumption of Gaussian noise), thanks to the analytic computation of estimators that allow to estimate the expected uncertainties affecting the measured parameters.
We stress that this approach is robust under optimal conditions, notably high SNR, which is the typical situation that we consider in this work, and it represents the approximation of a much more complex statistical analysis.

The general expression for the output $s(t)$ of a detector can be written as the sum of a noise term $n(t)$ and a possible GW signal term $h(t)$:
%
\begin{equation}
    s(t) = n(t) + h(t) \, .
\end{equation}
%
Under the assumption that $n(t)$ is a stochastic, stationary, and Gaussian function of time, the loudness of the signal can be quantified through the signal-to-noise ratio (\snr), defined as
%
\begin{equation}
    \SNR = \inner{h}{h}^{1/2} \, .
\label{eqn:snr}
\end{equation}
%
Here the parentheses denote the inner product, which reads
%
\begin{equation}
\inner{A}{B} = 4 \, \text{Re} \int_{0}^{+\infty} df \, \frac{\tilde A^{*}(f) \tilde B(f)}{\psd (f)} \, ,
\label{eqn:inner}
\end{equation}
%
where the tilde symbol labels Fourier transformed quantities and a star denotes the complex conjugate. The inner product is weighted over the one-sided power spectral density $\psd$ of a detector, which quantifies the sensitivity of an interferometer per frequency bin and is measured in \si{\hertz^{-1}}. For a network of $M$ detectors, the total $\SNR_{\rm net}$ is the square root of the quadratic sum of the individual $\SNR$s:
%
\begin{equation}
    \SNR_{\rm net} = \sqrt{\sum_{k=1}^{M} \SNR_k^2} \, .
\end{equation}

In the presence of a high $\SNR$ signal, the posterior distribution for the $n$ source parameters $\{ \Theta_1, \, ... \, , \Theta_i, \, ... \, , \Theta_j, \, ... \, , \Theta_n\}$ can be well approximated to an $n$-dimensional Gaussian with covariance matrix
%
\begin{equation}
    \CM = \FM^{-1} \, ,
\label{eqn:covariance_matrix}
\end{equation}
%
where 
%
\begin{equation}
    \FM_{ij} = \inner{\de{i}h}{\de{j}h} \, ,
\label{eqn:fisher_matrix}
\end{equation}
%
is the Fisher matrix and $\de{i} \equiv \partial/\partial\Theta_i$ is the partial derivative with respect to the $i$th source parameter $\Theta_{i}$. When multiple detectors are involved, the Fisher matrix of the network is  given by the sum of the individual ones:
%
\begin{equation}
    \FM_{\rm net} = \sum_{k=1}^M (\FM)_k \, .
\end{equation}

This formalism allows to access the uncertainty of the $i$th parameter by trivially taking the square root of the diagonal element $\CM_{ii}$ of the covariance matrix, i.e.,
%
\begin{equation}
    \sigma_{i} = \sqrt{\CM_{ii}} \, .
\end{equation}

The computation of these quantities requires careful numerical implementation. Specifically, in order to obtain the Fisher matrix, we first need to evaluate the partial derivative of the waveform with respect to each source parameter. An effective way to do that is by performing a symmetric derivative:
%
\begin{equation}
    \de{i} h (\Theta_i) = \lim_{\delta_i \to 0^+} \frac{h(\Theta_i + \delta_i \Theta_i) - h(\Theta_i -\delta_i \Theta_i)}{2\delta_i \Theta_i} \, .
\label{eqn:symmetric_derivative}
\end{equation}
%
When computed numerically, the symmetric derivative is no longer a limit. Thus, to compute \cref{eqn:symmetric_derivative} we must choose the magnitude of the infinitesimal increment $\delta_i$ for each parameter. 
We compute $\FM_{ij}$ for different values of $\delta_i$ and $\delta_j$, and we validate our choice as soon as the Fisher matrix element becomes a stable function of the infinitesimal increments. We report the values\footnote{We stress that $\delta_i$ is a dimensionless value, and that the product $\delta_i \Theta_i$ enters in the computation of the derivative.} we adopted for each parameter in \cref{tab:selected_parameters}.
Once we obtain the Fisher matrix for each source, we compute the correlation matrix $\CM$ through \cref{eqn:covariance_matrix} and by adopting the lower-upper decomposition method \cite{PresTeukVettFlan92}.
We validate the inversion process and check the resulting matrix by evaluating
%
\begin{equation}
    \epsinv = \max_{i, \, j} |(\FM \cdot \CM)_{ij} - \mathbb{1}_{ij} | \, ,
\label{eqn:epsilon_inv}
\end{equation}
%
where $\mathbb{1}$ is the identity matrix. We consider the inversion successful if \cref{eqn:epsilon_inv} returns $\epsinv \le \num[retain-unity-mantissa = false]{1e-3}$. Moreover, we validated our implementation by comparing results with other public pipelines (e.g.~\textsc{GWFAST}\cite{Iacovelli_2022}, \textsc{GWFish}\cite{dupletsa2023gwfish}). Specifically, we made common injections for each library and compared $\SNR$ values, as well as the uncertainties on the source parameters, with a particular focus on luminosity distance and the sky location, finding broad agreement overall. These tests were also used as benchmark to check our choice of the infinitesimal increments $\delta_i$.

The quasicircular, nonprecessing waveform model \texttt{IMRPhenomXHM}, once projected in a detector, is characterized by \num{11} source parameters: the two individual masses $M_1$ and $M_2$, the luminosity distance $\dl$ of the source, the $z$-component of the two spins $\chi_{z_1}$ and $\chi_{z_2}$, the sky position parameters $\theta$ and $\varphi$, the inclination $\iota$ and polarization $\psi$ angles, the coalescence phase $\Phi_c$ and time $t_c$.
We characterize the Fisher matrix with a different set of parameters: in particular, the two individual masses are replaced by the redshifted chirp mass $\mchirp = (1 + z)(M_1 M_2)^{3/5} / (M_1+M_2)^{1/5}$ and the symmetric mass ration $\eta = (M_1 M_2)/(M_1+M_2)^2$. We further take the natural logarithm of $\mchirp$ and $\dl$ to obtain directly their relative errors in the correlation matrix. We define $\mu = \cos\theta$ as a parametrization of the declination, and the two individual $z$-oriented spins are replaced by two orthogonal symmetrical and asymmetrical combinations $\chi_S = (\chi_{z_1} + \chi_{z_2})/2$ and $\chi_A = (\chi_{z_1} - \chi_{z_2})/2$.
The full list of parameters is reported in \cref{tab:selected_parameters}. 
We consider a GW event as detected if $\SNR_{\rm net} \ge 12$ and then compute the Fisher matrix for the set of detected sources.

\begin{table}[ht]
\centering
%\bgroup
%\setlength{\tabcolsep}{0.5em} % horizontal padding
\def\arraystretch{\tabvspace} %  vertical padding (default is 1)
\begin{tabular}{ l c c }
\hline
\hline
\textbf{Quantity} & \textbf{Parametrization} & \textbf{$\delta_i$}\\
\hline
Redshifted chirp mass & $\ln \mchirp$ & \num[retain-unity-mantissa = false]{1e-6}\\

Symmetric mass ratio & $\eta$ & \num[retain-unity-mantissa = false]{1e-6}\\

Luminosity distance & $\ln \dl$ & \num[retain-unity-mantissa = false]{1e-5}\\

Symmetric spin & $\chi_S$ & \num[retain-unity-mantissa = false]{1e-5}\\

Asymmetric spin & $\chi_A$ & \num[retain-unity-mantissa = false]{1e-5}\\

RA  & $\varphi$ & \num[retain-unity-mantissa = false]{1e-3}\\

DEC & $\mu$ & \num[retain-unity-mantissa = false]{1e-2}\\

Inclination & $\iota$ & \num[retain-unity-mantissa = false]{1e-5}\\

Polarization & $\psi$ & \num[retain-unity-mantissa = false]{1e-6}\\

Coalescence phase & $\Phi_c$ & \texttt{None}\\

Coalescence time & $t_c$ & \texttt{None}\\
\hline
\hline
\end{tabular}
\caption{
Quantities computed in
the Fisher matrix (parametrizations defined in the main text), together with the associated infinitesimal increment $\delta_i$ used to compute \cref{eqn:symmetric_derivative}. The \texttt{None} label marks the parameters whose derivatives are performed analytically.
}
\label{tab:selected_parameters}
\end{table}

We now discuss the FIM results. 
In \cref{sec:redshift_distribution} we estimated $\mathcal{O}(\num[retain-unity-mantissa = false]{1e5})$ GW sources per year, distributed over cosmological distances. 
Interestingly, we find that a combination of at least two 3G detectors will be able to detect the vast majority of the simulated population, while ET alone would still be able to detect a large fraction of the total BBHs, as reported in the left panel of \cref{fig:total_and_detected_pops}. These GW events will be detected potentially up to high redshift: such result would be the key for population studies to grow, leading to a better understanding of binary formation and evolution (see, e.g., \cite{taylor2018mining,mould2022deep}) across cosmic history with just \SI{1}{\year} of observations.
In the right panel of \cref{fig:total_and_detected_pops} we report the number of detections as a function of $\SNR_{\rm net}$. We see that a large number of the GW signals will be detected above $\SNR_{\rm net} \sim 100$, with the best cases given again by combinations of ET with at least one CE.
Under the FIM formalism, the parameters associated to these sources are characterized by Gaussian posterior distributions, with progressively smaller width as the $\SNR$ increases. 
Focusing on the localization of the source, which is crucial for dark siren studies, we expect $\sigma_{\dl}/\dl$ and the sky location area $\Delta\Omega$ to scale with $\SNR_{\rm net}^{-1}$ and $\SNR_{\rm net}^{-2}$ respectively, as one can see from \cref{eqn:sigma_dL_and_delta_omega}.
Our expectations are confirmed as illustrated in \cref{fig:summary_PE}, where we show that these trends are well recovered. The progressively narrower dispersion of the points remarks also the importance of having multiple detectors in the network of interferometers, as this allows to break degeneracies and triangulate the GW signal. Furthermore, higher harmonics in the waveform model help disentangling the luminosity distance and inclination contributions to the signal's amplitude, leading to better constraints on these parameters. We find reasonable agreement with similar studies which also make use of higher modes, see e.g.~\cite{borhanian2022listening,iacovelli2022forecasting,pieroni2022detectability}.


\begin{figure*}
    \includegraphics[width=1.\textwidth]{Figures/summary_pdf_WL_v2.pdf}
    \caption{ 
    Correlation between GW parameter uncertainties and $\SNR_{\rm net}$ for different network configurations. Each circle represents a detected GW event in a specific network, labeled at the top. In the top row, the black dotted lines represent $\sigma_{\dl}/\dl = \SNR_{\rm net}^{-1}$, while in the bottom row they display the $90\%$ credible region $\Delta\Omega_{90\%} = -2 \pi \ln(1 - 90/100) \SNR_{\rm net}^{-2}$ converted in \si{\square\degree}. The red dotted vertical line marks the minimum $\SNR_{\rm net}$ threshold that we set to create localization error volumes. The purple-filled circles are the GW events above that threshold.
    }
    \label{fig:summary_PE}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Cross-matching with galaxy catalogs}
\label{sec:errorboxes}

In this section we outline our simulation procedure for galaxy catalogs and explain how we cross-match localization error volumes with potential host galaxies of the GW source.


\subsection{Galaxy catalog}
\label{subsec:gal_cats}

The galaxy catalog employed in this work is generated by using \texttt{L-Galaxies} \citep{Henriques2015}, a state-of-the-art semi-analytical model (SAM) applied on top of the merger trees of dark matter simulations. Specifically, we run the SAM on the Millennium simulation \citep{Springel2005} whose halo mass resolution (\SI{2e10}{\msun}) and box size (\SI{685}{\mega\parsec}) offer a good compromise to trace the cosmological assembly of galaxies with stellar mass $M_{*} > \SI[retain-unity-mantissa=false]{1e9}{\msun}$. By using the procedure presented in \cite{IzquierdoVillalba2019} we transform the outputs of \texttt{L-Galaxies} into a customized lightcone which embraces one octant of the sky and contains the physical properties (such as mass, magnitudes, observed and cosmological redshift) of all the galaxies with $M_{*} > \SI[retain-unity-mantissa=false]{1e10}{\msun}$ up to $z = 3$. Regarding the cosmological parameters, the version of \texttt{L-Galaxies} used in this work re-scales the original values used in the Millennium (WMAP1 and 2dFGRS concordance cosmology) to match the ones of Planck first-year data \citep{PlanckCollaboration2014} (which is the fiducial cosmology defined in \cref{sec:redshift_distribution}).

In this work, we explore two different scenarios. 
In our main scenario, we study low-$z$ GW sources ($z<1$) whose galaxy fields could be traced by current galaxy catalogues provided by SDSS Legacy Survey~\citep{SDSS2000} or upcoming missions like EUCLID~\citep{EUCLID2012} and WFIRST~\citep{WFIRST2015}. On the other hand, we consider an optimistic scenario where we take advantage of the full redshift depth of the lightcone and analyze high-$z$ GW events ($z < 3$). In this case, future deep surveys like LSST will be able to provide complete photometric galaxy catalogs \citep{LSST2019}.

Galaxies with $M_{*} < \SI[retain-unity-mantissa=false]{1e10}{\msun}$ are more numerous than more massive galaxies, but harder to detect (see, e.g., \cite{Baldry2008,Yasuda2001,Rovilos2009, DominguezSanchez2011}).
Future surveys are likely to reflect this challenging issue (especially at high-$z$), leading to incompleteness effects in the low-mass regime similar to the ones reproduced in our mock catalog.
On the other hand, we expect that dwarf galaxies have a secondary role in cosmological inference given that their low stellar masses reduce their probability of hosting GW events.
Taking into account all this, we assume that BBHs cannot be hosted by galaxies with $M_{*} < \SI[retain-unity-mantissa=false]{1e10}{\msun}$, which could be implemented in our cosmological inference by adding a mass-dependent weight to each galaxy (see \cref{eqn:prior_zgw} below).
We refer to our catalog as ``complete'' under these assumptions.


\subsection{Gravitational-wave localization error volumes}\label{subsec:error_box}

We can now use the parameter uncertainties estimated with the FIM analysis to estimate the localization error volume, or ``error-box,'' of each GW event. Each error-box will be populated by the galaxies contained in our light cone. To this end, we are mainly interested in the measurements of the GW luminosity distance and the sky position.
Given the parametrization listed in \cref{tab:selected_parameters}, we recover their errors through
%
\begin{equation}
\begin{split}
    \frac{\sigma_{\dl}}{\dl} & = 
     \sqrt{\biggl(  \frac{\sigma_{\dl}}{\dl} \biggr)^2_{\rm GW} + \biggl( \frac{\sigma_{\dl}}{\dl} \biggr)^2_{\rm WL}} \, , \\
    \Delta\Omega_{\rm X\%} & = - 2 \pi \sqrt{\CM_{\mu\mu}\CM_{\varphi\varphi} - (\CM_{\mu\varphi})^2} \ln\biggl(1 - \frac{\rm X}{100}\biggr) \, ,
\end{split}
\label{eqn:sigma_dL_and_delta_omega}
\end{equation}
%
where $(\sigma_{\dl}/\dl)_{\rm GW} = \sqrt{\CM_{\ln\dl \, \ln\dl}}$ is the uncertainty coming from the Fisher matrix, while $(\sigma_{\dl}/\dl)_{\rm WL}$ keeps into account for the contribution of weak lensing (WL) to the measure. As described in \cite{Tamanini:2016zlh}, we model this term through\footnote{As noted in~\cite{Cusin:2020ezb}, we correct for a missing factor 1/2 in this expression with respect to the one reported in~\cite{Tamanini:2016zlh}. No demagnification is considered here, contrary, e.g., to~\cite{Speri:2020hwc}.}

\begin{equation}
    \biggl( \frac{\sigma_{\dl}}{\dl} \biggr)_{\rm WL} = 0.033 \biggl(\frac{1 - (1+z)^{-0.25}}{0.25}\biggr)^{1.8} \, .
    \label{eqn:weak_lensing}
\end{equation}

In \cref{eqn:sigma_dL_and_delta_omega}, $\Delta\Omega_{\rm X\%}$ is expressed in \si{\steradian} and $\rm X\%$ represents the percent confidence interval (CI) of the measure \cite{PhysRevD.81.082001,Iacovelli_2022}. 
We compute $\Delta\Omega_{90\%}$ so to be able to compare our results with 
most of what can be found in the literature.

Since our ensemble of GW events is generated independently
of the galaxy catalog, we need to cross-match the eligible binaries within the Millennium Universe. 
The following procedure is applied to a subset of the simulated GW events. The eligible BBHs must satisfy $\SNR_{\rm net} > 300$: this selection ensures great precision both in luminosity distance and sky location, two essential requirements that limit the potentially prohibitive number of galaxies $N_{\rm hosts}$ per GW localization error volume. 
However, ET alone does not provide very accurate sky localization and luminosity distance measurements. Therefore, when we consider ET alone, we further require binaries to feature $3\sigma_{\mu} = 3 \sqrt{\CM_{\mu\mu}}$ and $3\sigma_{\varphi} = 3\sqrt{\CM_{\varphi\varphi}}$ small enough to be able to fit within the angular aperture of the light cone. For a similar reason, we also require $3\sigma_{\dl}/\dl \le 1$. 

For each eligible dark siren, we follow the procedure detailed below: 

\begin{enumerate}

    \item Assuming our fiducial cosmology,
    we compute the redshift interval $z \pm \num{3}\sigma_z$ from the $\num{3}\sigma$ measurement of $\dl$, and we list all the galaxies whose cosmological redshift lies within this range. Among them, we extract a galaxy with probability given by $\mathcal{N}(\dl,\sigma^2_{\dl})$, and we label it  ``true host'' of the GW event. We denote the true host sky coordinates by ${\bf{\Theta}}_{\rm th}=\{ \mu_{\rm th}, \varphi_{\rm th}\}$.
    \item Next, we extract the center of the localization error volume ${\bf{\Theta}}_{c}=\{ \mu_{c}, \varphi_{c}\}$, namely we redefine the maximum of the 2D Gaussian in $\{ \mu, \varphi\}$, ensuring that the true host falls within a $3\sigma$ sky location region from it and is consistent with the GW sky location uncertainty computed in~\cref{sec:GWPE}. To do so, we draw ${\bf{\Theta}}_{c}$ from $\mathcal{N}({\bf{\Theta}}_{\rm th}$, $\CM_{\Delta\Omega}$), where $\CM_{\Delta\Omega}$ is the 2D sky location subcovariance matrix of the GW signal. This new point redefines the best direction (i.e.~the peak of the Gaussian) measured by the GW detector.
    \item We then compute the redshift boundaries of the localization error volume that we will use for the cosmological inference. This needs to take into account the full prior ranges of the cosmological parameters, otherwise we would implicitly assume a prior given by our fiducial cosmology.
    We refer to this new, much broader interval as $[z^{-}, \, z^{+}]$.
    In practice this is computed as $z^{-} = \min \left[ z - \num{3}\sigma_z \right]$ ($z^{+} = \max \left[ z + \num{3}\sigma_z \right]$) where the $\min$ ($\max$) is taken with respect to all possible values of the cosmological parameters within the allowed priors that we assume to be $h \in [0.6, 0.86]$, $\Om \in [0.04, 0.5]$.
    \item Since peculiar velocities affect galaxy redshift measurements, we need to model the related uncertainty. Following \cite{Laghi_2021, Muttoni:2021veo}, we characterize this effect as~\cite{hogg1999}
%
    \begin{equation}
        \sigma_{v_p}(z) = (1 + z)\frac{v_p}{c} \, ,
        \label{eqn:sigma_pv}
    \end{equation}
%
    where $v_p = \SI{700}{\kilo\meter\,\second^{-1}}$ is representative of the standard deviation of the radial peculiar velocity distribution of the galaxies in the catalog.
    The redshift interval is therefore updated to $[z^{-} - \sigma_{v_p}(z^{-}), \, z^{+} + \sigma_{v_p}(z^{+})]$.
    \item We populate the localization error volume with all the galaxies that fall within a $\num{3}\sigma$ sky location region from the center ${\bf{\Theta}}_{c}$ and inside $[z^{-} - \sigma_{v_p}(z^{-}), \, z^{+} + \sigma_{v_p}(z^{+})]$. These galaxies represent the potential host candidates of the GW event.
    \item For each potential host in the localization error volume, labeled by $j$ ($j=1,...,N_{\rm hosts}$), we compute a normalized galaxy ``weight'' $w_j$ according to its position in the sky relative to the center of the localization volume:
    \begin{equation}\label{eqn:weights}
    w_j \propto \mathcal{N}({\bf \Theta}_{c}, \CM_{\Delta \Omega})\big|_{(\mu_{j}, \varphi_{j})} \, .
    \end{equation}
\end{enumerate}


\begin{figure*}[t!]
    \includegraphics[width=1.\textwidth]{Figures/Error-boxes_v2.pdf}
    \caption{Graphic representation of three representative localization error volumes, displayed per column. The top panel shows the distribution of the $N_{\rm hosts}$ galaxies (circles) falling inside it on the RA-DEC plane, where the color-scale represents the magnitude of the hosting probability, as computed from \cref{eqn:weights}, from white (high) to purple (low), while the dashed ellipses denote the $1\sigma$, $2\sigma$, and $3\sigma$ probability contours. A red cross marks the selected true host. The bottom panel displays how these galaxies are distributed in redshift. Here the red solid line marks the selected true host, the green dashed lines denote the redshift interval obtained by inverting \cref{eqn:luminosity_distance_flat} assuming $\dl \pm 3\sigma_{\dl}$ and our fiducial cosmology, while the black dotted lines are the boundaries of the localization error volume that take into account the full prior range on the cosmological parameters and galaxy peculiar velocities, computed as described in \cref{subsec:error_box}.
    }
    \label{fig:error-box}
\end{figure*}


Once built, we only consider the localization volumes whose $z^{+} + \sigma_{v_p}(z^{+})$ value does not exceed a maximum redshift threshold, which is $z_{\rm max}=1$ for our fiducial case, and $z_{\rm max}=3$ for the optimistic one. We illustrate the outcome of this procedure in \cref{fig:error-box}.

Each network observes the BBHs population with different $\SNR_{\rm net}$ and recovers different values of the uncertainties. We therefore expect different number of localization error volumes for each configuration of detectors, as shown in \cref{fig:N_hosts_statistics}, where we also display how $N_{\rm hosts}$ scales with the $\SNR_{\rm net}$. Nonetheless, it is worth noting that the same GW event can satisfy the conditions detailed above in more than one network.


\begin{figure*}
    \includegraphics[width=1.\textwidth]{Figures/N_hosts_statistics_v2.pdf}
    \caption{
    Left: correlation between the number of hosts $N_{\rm hosts}$ per error-box and the $\SNR_{\rm net}$ of the associated dark siren. Each circle represents the localization error volume of a GW event that satisfies the conditions detailed in \cref{subsec:error_box} in a specific network (labels in the legend). For this reason, the \num{3} networks do not share the same number of GW events. Right: distribution of $N_{\rm hosts}$ in the generated localization error volumes, colors as in the left plot. The values of $N_{\rm hosts}$ in both panels are averaged over the \num{6} realizations considered in this study.
    }
    \label{fig:N_hosts_statistics}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Cosmological inference}
\label{sec:cosmoinference}


We adopt the \emph{dark siren} approach to infer the cosmological parameters.
We thus cross-match GW luminosity distance information with EM galaxy catalog redshifts through the luminosity distance-redshift relation in a \emph{flat} $\lcdm$ model, as explained in~\cref{sec:errorboxes}. In what follows we discuss the Bayesian framework apt to make the inference of the cosmological parameters.

\subsection{Bayesian formulation}

We are interested in using a set of GW observations $\mathcal{D} = \{ \mathcal{D}_i \}_{i=1}^N$ to jointly constrain the cosmological parameters $\Omega=\{ h,\, \Om\}$ in a flat $\lcdm$ model, namely the two parameters that determine the dynamics of the Universe at the background level.
We represent the $\lcdm$ cosmological model by $\mathcal{H}$ and any background information that is useful for the inference problem with $I$, including the redshift information about the galaxy catalog.
Our mathematical framework is mainly based on~\cite{Laghi_2021}, of which we summarize the main concepts here (see also~\cite{DelPozzo:2011yh,DelPozzo:2015bna,DelPozzo:2017kme}).

From Bayes' theorem, the posterior probability distribution can be computed as 
\begin{equation}\label{eqn:bayes}
    p(\Omega | \mathcal{D} , \mathcal{H}, I) \propto p(\Omega | \mathcal{H}, I) \, p(\mathcal{D} | \Omega , \mathcal{H}, I) \text{,}
\end{equation}
where $p(\Omega | \mathcal{H}, I)$ is the prior probability distribution for the cosmological parameters $h$ and $\Om$, while $p(\mathcal{D} | \Omega , \mathcal{H}, I)$ is the likelihood function for the GW dataset $\mathcal{D}$, that, for statistically independent events, can be written as:
\begin{equation}\label{eqn:lk}
    p(\mathcal{D} | \Omega , \mathcal{H}) = \prod_{i=1}^{N} p(\mathcal{D}_i | \Omega , \mathcal{H}) . 
\end{equation}

We choose our dataset according to the $\SNR_{\rm net}$ of the event.
The assumption of an $\SNR_{\rm net}$ detection threshold as a proxy for the detectability of an event has the effect to constrain the range of GW luminosity distances, which, in turn, can bias the estimate of the cosmological parameters. Accounting for the GW selection effect requires to normalise the single-event likelihood by means of a selection function, which is estimated as the single-event likelihood integrated over all datasets that would be classified as detected according to our detection statistic~\cite{mandel2019extracting,vitale2022inferring}.
However, in this study we focus on high-SNR events, which allows us to partially simplify the inference problem, not including selection bias corrections and reducing the already significant computational cost of the analysis. 
We find indeed that, for all the $\SNR_{\rm net}$ thresholds considered in this analysis, selection biases are subdominant compared to statistical uncertainties (see~\cref{sec:results}). The correction for GW selection effects typically requires to assume a detection model and a population model; since we ignore selection effects, our cosmological inference is thus independent of any assumption on population models. 
The reason why we do not observe any sensible systematic error can be explained by the fact that, due to the precision of the GW parameter estimation in the $\SNR_{\rm net}$ limit, the selection function is only weakly dependent on the cosmological parameters; thus, the selection function, which can be seen as a correction factor to \cref{eqn:bayes}, can be approximated as an overall constant which does not significantly change from event to event. 
As we include less informative events in our analysis, that is, lowering the $\SNR_{\rm net}$ threshold, this argument does not hold and selection biases can become comparable to statistical uncertainties, significantly affecting posterior estimates (see, e.g., ~\cite{2022arXiv221208694G}).
In our simulation we find that lowering the $\SNR_{\rm net}$ threshold to 200 already yields a non-negligible bias.
Moreover, the inclusion of GW events with $\SNR_{\rm net} < 300$ pose as well challenges from the point of view of the computational cost of the analysis, as discussed in~\cref{subsec:numerics}. For these reasons, in this study we limit ourselves to the analysis of the most informative events with $\SNR_{\rm net} > 300$.

Another potential source of bias may come from the incompleteness of the galaxy catalog \cite{chen17,Finke:2021aom,Gray2022,2022arXiv221208694G}. In this analysis we use a light cone that by construction contains all the galaxies with $M_{*} > \SI[retain-unity-mantissa=false]{1e10}{\msun}$ up to $z = 3$, irrespective of their magnitude: this allows us to simplify the formalism and assume the completeness of the galaxy catalog up to the redshift covered by the light cone (see~\cref{subsec:gal_cats} for a discussion on the validity of this assumption in the context of 3G detectors).

After marginalization over the GW nuisance parameters which are not relevant for this analysis, and assuming that we can neglect the correlation between the detector measurements of the angular coordinates and distance, so that the joint GW likelihood on sky position and distance parameters factorizes, the single-event ``quasilikelihood''~\cite{jaynes2003} can be written as~\cite{Laghi_2021}
\begin{equation}\label{eqn:single-event-lk1}
\begin{split}
p(\mathcal{D}_i\,|\,\Omega,\mathcal{H}, I) \propto
\int  &dd_L \,dz_{\rm GW}\,\,
p(d_L \,|\, z_{\rm GW},\Omega,\mathcal{H}, I) \times \\
& \!\!\!\!\times p(z_{\rm GW} \,|\, \Omega,\mathcal{H}, I)\,
p(\bar{d}_L \,|\, d_L,z_{\rm GW},\mathcal{H}, I)\text{.}
\end{split}
\end{equation}
The term $p(d_L \,|\, z_{\rm GW},\Omega,\mathcal{H}, I)$
is the probability of obtaining the luminosity distance assuming we know the source redshift and the cosmological parameters:
\begin{equation}
    p(d_L\,|\,z_{\rm GW},\Omega,\mathcal{H}, I) = \delta(d_L - d(z_{\rm GW}, \Omega)).
\end{equation}
The GW redshift prior term $p(z_{\rm GW} \,|\, \Omega,\mathcal{H}, I)$ reflects the properties of the localization volume and the potential host galaxies that are within it:
\begin{equation}\label{eqn:prior_zgw}
    p(z_{\rm GW}\,|\,\Omega,\mathcal{H}, I)  \propto
    \sum_{j=1}^{N_{\rm hosts}} w_j \,
    \mathcal{N}(z_{\rm GW},\sigma^2_{v_p})\big|_{z_j}\,,
\end{equation}
where we account for the peculiar velocity uncertainties of each galaxy assuming Gaussian functions in redshift with $\sigma_{v_p}(z)$ given by \cref{eqn:sigma_pv}. Here we are also including the weights computed in \cref{eqn:weights}, that are derived from the marginalization of the quasilikelihood over the GW angular coordinates $\varphi$ and $\mu$, that we assume to coincide with the angular coordinates of each of the potential hosts.
We note that while in principle one could assign galaxy weights according to, e.g., astrophysical properties of the galaxies~\cite{fishbach}, here we only use information coming from the GW marginal distribution over the sky position angles (see Sec.~\ref{subsec:gal_cats} however).
We remark that since we account for peculiar velocity uncertainties, we also infer the redshift of the GW events, which we then marginalise over to get the cosmological posterior samples in \cref{eqn:bayes}. 

Finally, the remaining term $p(\bar{d}_L \,|\, d_L, \Omega, z_{\rm GW},\mathcal{H}, I)$ represents the detector quasilikelihood in the luminosity distance for the $i$-th event, as measured by a network of $M$ detectors, 
\begin{equation}
    p(\bar{d}_L \,|\, d_L, z_{\rm GW},\mathcal{H}, I) = \prod_{k=1}^{M} p(\bar{d}_L^{\,(k)} \,|\, d_L, z_{\rm GW},\mathcal{H}, I)\,,
\end{equation}
which we approximate as a Gaussian distribution~\cite{DelPozzo:2017kme, Laghi_2021, Muttoni:2021veo} centered on the best estimate $\bar{d}_L$ with uncertainties coming from the Fisher analysis presented in \cref{sec:GWPE}.
After the integration over $d_L$, we have:
\begin{equation}\label{eqn:lk_dL}
    p(\bar{d}_L \,|\, d(z_{\rm GW}, \Omega), z_{\rm GW},\mathcal{H}, I) 
    \propto
    \mathcal{N}(\bar{d}_L, \sigma^2_{\dl})\big|_{d(z_{\rm GW}, \Omega)}\,,
\end{equation}
and \cref{eqn:single-event-lk1} becomes
\begin{equation}\label{eqn:single-event-lk2}
    \begin{split}
    p(\mathcal{D}_i\,|\,\Omega,\mathcal{H}, I) \propto
    \int &dz_{\rm GW}\, \mathcal{N}(\bar{d}_L, \sigma^2_{\dl})\big|_{d(z_{\rm GW}, \Omega)}\times\\
    &\times \!\!\sum_{j=1}^{N_{\rm hosts}} w_j \,
    \mathcal{N}(z_{\rm GW},\sigma^2_{v_p})\big|_{z_j}\,,
    \end{split}
\end{equation}
which we compute through a nested sampling algorithm.


\subsection{Numerical implementation}\label{subsec:numerics}

We estimate the posterior distribution in \cref{eqn:bayes} with \textsc{cosmoLISA}~\cite{cosmoLISA}, a public pipeline for the Bayesian inference of the cosmological parameters with simulated GW observations. 
\textsc{cosmoLISA} computes the single-event likelihood \cref{eqn:single-event-lk2} and the posterior samples in \cref{eqn:bayes} by making use of a nested sampling algorithm as implemented in \textsc{CPNest}~\cite{CPNest}, a public package optimized to parallelise nested sampling computation.
In a typical \textsc{cosmoLISA} run, we employ 5 nested samplings in parallel of 1000 live points each, making a total ensemble of 5000 live points per run. 
At each step of each nested sampling, we independently evolve 6 live points via MCMC with maximum number of steps equal to 5000.
Although rather expensive from a computational point of view, we found that this is the ideal setup to face the complexity of the likelihood and to get numerically stable and reliable results.
In this work we used the commit \texttt{3760800} of the branch \texttt{massively\_parallel} of \textsc{CPNest}.
For the cosmological prior we assume uniform distributions in the same range of values used for the production of the localization error volumes, while each GW event redshift is marginalized in the redshift interval defined by its localization error volume (see \cref{sec:errorboxes}).
Since the marginalization over redshift is done through nested sampling, the dimension of the parameter space to be explored (or equivalently, the dimension of the integral to be computed to estimate the evidence) increases with the number of events, which makes the analysis of large number of dark sirens impracticable from a computational point of view.
Moreover, the computational cost raises quickly as we lower the $\SNR_{\rm net}$ threshold, since we add GW sources having large localization error volumes with potentially several thousands of galaxies inside.
To overcome this problem, which makes the analysis of $N \gtrsim \mathcal{O}(40)$ GW events prohibitive, here we adopt a different procedure compared to~\cite{Laghi_2021}: when considering a large number of events that would be too computationally costly to analyse with a single \texttt{cosmoLISA} run, we split our dataset in random subsets of less than $\mathcal{O}(40)$ events that we analyse separately.
Then we model $p(\Omega | \mathcal{D} , \mathcal{H}, I)$ by combining the posterior samples for $\Omega$ obtained from these data subsets via a Dirichlet process Gaussian mixture model, a fully Bayesian non-parametric method to reconstruct probability density functions out of a finite number of samples (see, e.g.,~\cite{DelPozzo2018}), using the approximate variational algorithm~\cite{Blei2006} as implemented in~\cite{haines_dpgmm}. 
We tested the robustness of the posterior reconstruction by checking, for some representative cases,  that the results obtained with this procedure are equivalent to those obtained  without splitting the dataset.  




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}
\label{sec:results}


In what follows we present our estimates of the cosmological parameters for 1 year of network observation (assuming full duty cycle).
For the GW population produced and analyzed in \cref{sec:GWPE}, we repeat the production of the localization error volumes described in \cref{subsec:error_box} six times to consider a reasonable sample of realizations in the galaxy distribution, and then analyse each of them to get cosmological posterior samples, obtaining precisions on the measure for the cosmological parameters that we average over the six realizations to get a fiducial mean precision at 68\% and 90\% CI. First, we consider a fiducial scenario defined by a selection threshold equal to $\SNR_{\rm net}=300$ and a galaxy catalog complete up to $z=1$ (\cref{subsec:results_fiducial}).
We then investigate how results change if we consider: different increasing values of $\SNR_{\rm net}$ (\cref{subsec:results_different_SNR_cuts}); the inference of $h$ only, assuming $\Om$ known (\cref{subsec:results_h_only}); single-host dark sirens only (\cref{subsec:results_single_host}). Finally, we question how forecasts change in an optimistic scenario where we have a complete galaxy catalog up to $z=3$ (\cref{subsec:results_z<3}).
In the following report of results, we are excluding those of ET alone. In fact, from \cref{fig:summary_PE,fig:N_hosts_statistics} it emerges that a single detector in a triangular configuration is not able to localize sufficiently well the sources even at large $\SNR$, leading to a few error-boxes with (tens of) thousands galaxies. The inference on this dataset does not contain any more information with respect to the prior and it is therefore considered uninformative.
Unless otherwise specified, in the following we will draw our main conclusions based on the 90\% CI estimates provided by each network of detectors for each parameter, whose precision is estimated from the mean half-width and median of the posterior distribution.
 

\subsection{Fiducial case: \texorpdfstring{$\SNR_{\rm net} > 300$}{SNR} at \texorpdfstring{$z < 1$}{z}}\label{subsec:results_fiducial}


We define our fiducial scenario by imposing an $\SNR_{\rm net}$ threshold of 300 and a galaxy catalog complete up to $z=1$.
As discussed in \cref{sec:cosmoinference}, the choice $\SNR_{\rm net} = 300$ is a trade-off between our availability of computational resources and the validity of our formulation of the likelihood.
The redshift catalog limitation is assumed based on realistic expectations of galaxy surveys in the era of 3G detectors, according to which completeness of all-sky readily available galaxy catalogs rapidly declines after $z=1$ (in \cref{subsec:results_z<3} we will investigate an optimistic scenario in which we assume that a dedicated deep survey capable of yielding a complete catalog up to $z=3$ along the GW sky localization cone is available for each dark siren).
In this scenario we are effectively selecting the best-localized events (see \cref{fig:summary_PE}).

Results for our fiducial scenario are presented in~\cref{tab:cosmo_results}, where we report a mean precision averaged over the analysis of the six different realizations of the localization error volumes, while in \cref{fig:corner_plots_fiducial,fig:h_Om_BBH_latest} we show joint posteriors obtained from a representative realization of the localization error volumes, chosen as the one, among all of the six realizations analyzed, that has the median precision for $h$.
From the joint inference of $h$ and  $\Om$, we find that the network ET+CE1 is able to constrain $h$ at the $1.1\%$ with $N = 207$ BBHs. A slight improvement is observed with the network ET+CE1+CE2, where $N=278$ dark sirens can produce a measure of $h$ at the $0.8\%$ level.
These numbers suggest that if we want to reach a $\sim$1\% precision on the measure of $h$, we need to consider at least a network made of two 3G detectors. Adding a third detector to the network may allow us to reach a subpercent precision.
This level of precision on $\hubble$ would allow us to solve the Hubble tension within one full year of 3G observations, assuming the tension persists until the 3G era.

The situation for $\Om$ is different. A network of at least two detectors allows us to reach a $14.4\%$ precision, which, in the optimistic scenario of the network ET+CE1+CE2, could go down to a $10\%$-level measure. 

Our fiducial results are slightly better than the ones recently reported in~\cite{Zhu:2023jti}.
In there a dark sirens analysis similar to ours has been considered for the ET+CE1 scenario only, with average forecast constraints reaching $\sim$1\% for $\hubble$ and $\sim$20\% for $\Omega_m$ (at 68\% CI) with 300 BBHs.
This discrepancy can be attributed to the different settings of the simulations.
For example, contrary to our setup, in~\cite{Zhu:2023jti} higher GW modes in the GW signal are not included.
Given their importance in obtaining accurate sky localization volumes, thanks to their role in breaking degeneracies between GW waveform parameters, this may explain the slightly more optimistic results obtained in our analysis.


\begin{figure}
    \centering
    \includegraphics[width=0.485\textwidth]{Figures/Figure_6.pdf}
    \caption{
    Posterior distributions (68\% and 90\% credible regions) in the $h-\Om$ plane for 1 year of observation, from the analysis of a representative realization of the localization error volumes, as described in \cref{subsec:results_fiducial} (fiducial scenario, full duty cycle). In each panel, the cyan dotted lines represent the fiducial cosmology.
    }
    \label{fig:corner_plots_fiducial}
\end{figure} 



\subsection{Higher \texorpdfstring{$\SNR_{\rm net}$}{SNR} thresholds at \texorpdfstring{$z < 1$}{z}}
\label{subsec:results_different_SNR_cuts}

In \cref{fig:h_Om_BBH_latest} we show how our forecasts change as a function of increasing $\SNR_{\rm net}$ threshold values, in order to characterize the importance of the loudest observed dark sirens for cosmological inference.
We choose some representative threshold values of $\SNR_{\rm net} = 600, 500, 400$ and repeat the cosmological analysis for each respective dataset of the same realization.
Starting from the highest $\SNR_{\rm net}$ threshold, in the case $\SNR_{\rm net} > 600$, the network ET+CE1 ($N=31$) and ET+CE1+CE2 ($N=39$) lead to $1.9\%$ and $1.5\%$ constraints on $h$, respectively.
These results are only slightly worse than those obtained in the case $\SNR_{\rm net} > 500$, while $\Om$ is constrained at the 34.7\% (ET+CE1) and 22.4\% (ET+CE1+CE2). 
Including events at $\SNR_{\rm net} > 500$ and $\SNR_{\rm net} > 400$, we increase the number of events (see \cref{fig:h_Om_BBH_latest}) and, as expected, we get better constraints for both $\lcdm$ parameters.
The shrinkage evolution of the marginalised posteriors is evident from \cref{fig:h_Om_BBH_latest}, where we report the number $N$ of events passing the $\SNR_{\rm net}$ threshold and the precision for each case. As for the fiducial scenario, here we also report results from the realization that gives the median precision on $h$.
Overall, we can see how results for $h$ are less dependent on the number of events with respect to $\Om$. This can be explained by the fact that the measurement of $h$ mainly depends on the observation of nearby events, which are mostly characterized by high $\SNR_{\rm net}$. Constraints on $\Om$ are instead more dependent on mid-high redshift dark sirens, therefore the inclusion of lower-$\SNR_{\rm net}$ events has more impact.
Our analysis suggests that most of the cosmological predicting power of 3G BBH dark sirens is contained in high SNR events; yet, to find the most accurate forecasts one should include also lower SNR events, eventually considering all events above detection threshold.
Such a complete analysis however is prohibitive with the computational resources at our disposal.
Our methods of inference need to be further developed and optimized before such a study will be possible, but nevertheless the approach considered here is a good compromise that provides sufficiently accurate forecast estimations at a relatively affordable computational cost.

\begin{figure*}[t!]
    \centering
    %\includegraphics[width=1.\textwidth, height=1.08\textwidth]{Figures/Figure_7.pdf}
    \includegraphics[width=.8\textwidth]{Figures/Figure_7.pdf}
    \caption{Comparison of the precision for the joint inference of $h$ and $\Om$ using $N$ dark sirens with a galaxy catalog complete up to $z<1$ from the events of a representative realization of the localization error volumes, as defined in \cref{subsec:results_fiducial,subsec:results_different_SNR_cuts}. Red (blue) intervals show 68\% (90\%) CI (precision shown next to them), considering different detector networks and $\SNR_{\rm net}$ thresholds for 1 year of observation.
    }
    \label{fig:h_Om_BBH_latest}
\end{figure*}


\begin{table*}[t]
\centering
%\bgroup
%\setlength{\tabcolsep}{0.5em} % horizontal padding
\def\arraystretch{\tabvspace} %  vertical padding (default is 1)
\begin{tabular}{ l || c | c || c | c | c | c || c | c | c }
\hline
\hline
\multirow{3}{*}{\textbf{Network}} & \multicolumn{2}{c||}{$N$} & \multicolumn{4}{c||}{$\Delta h / h$ ($\%$)} & \multicolumn{3}{c}{$\Delta \Om / \Om$ ($\%$)} \\\cline{2-10}
& \multirow{2}{*}{$z<1$} & \multirow{2}{*}{$z<3$} &  \multirow{2}{*}{$z<1$} & $z<1$ & $z<1$ & \multirow{2}{*}{$z<3$} & \multirow{2}{*}{$z<1$} & $z<1$ & \multirow{2}{*}{$z<3$} \\
& & & & fixed $\Om$ & single-host & & & single-host & \\
 \hline
 ET+CE1 & \num{207} & \num{248}  &  0.6 (1.1) & 0.2 (0.4) & \num{3.3}-\num{7.1} (\num{5.6}-\num{11.2}) & 0.7 (1.1) &  8.8 (14.4) & - & 8.8 (14.6)\\
 ET+CE1+CE2 & \num{278} & \num{348} & 0.5 (0.8) & 0.2 (0.3) & \num{1.7}-\num{2.1} (\num{2.7}-\num{3.3}) & 0.4 (0.7) & 6.1 (10.0) & - & 5.3 (8.7)\\
\hline
\hline
\end{tabular}
\caption{
For each network of detectors (column 1), we report the number $N$ of dark sirens with $\SNR_{\rm net} > 300$ used in~\cref{sec:results} for 1 year of full observation, assuming a galaxy catalog complete up to $z<1$ and $z<3$ (column 2-3). We report $68\%$ ($90\%$) CI for $h$ and $\Om$ (column 4-7 and 8-10) assuming $\SNR_{\rm net} > 300$ and: using a complete galaxy catalog up to $z<1$, inferring both parameters or assuming $\Om$ known, analysing single-host dark sirens only, and using a complete galaxy catalog up to $z<3$. The quantities $\Delta h$ ($\Delta \Om$) and $h$ ($\Om$) are the mean half-width of the posterior distribution and the median, respectively. We report a mean precision averaged over the six different realizations analyzed. For the single-host analysis (columns 6 and 9) the average number of events were \num{1} (ET+CE1) and \num{5} (ET+CE1+CE2) (see \cref{subsec:results_single_host}).
}
\label{tab:cosmo_results}
\end{table*}


\subsection{\texorpdfstring{$\SNR_{\rm net} > 300$}{SNR} at \texorpdfstring{$z<1$}{z}: Assuming \texorpdfstring{$\Om$}{O} known}\label{subsec:results_h_only}

Here we repeat the analysis with the same dataset used in our fiducial scenario assuming that we know $\Om$ exactly.
This reduces our cosmological model from two to only one parameter to infer.
We perform this analysis mainly to compare with other results in the literature, but as a further motivation a scenario in which the Hubble tension persists to the 3G era while $\Om$ is measured with high precision by EM observations is not excluded.
For ET+CE1 and ET+CE1+CE2, we find similar subpercent precision, around 0.3\%, even if the latter network observes more events. In fact, these events are mostly at high redshift, thus they do not contribute significantly to the measure of $h$.

We compare again our results with the ones we can find in the literature.
Reference~\cite{Song:2022siz} claims that the ET+CE1+CE2 network can deliver a surprising $\mathcal{O}(0.001\%)$ constraint on $\hubble$ within 5 year of observations of BBHs at $z<0.3$.
This differs by two orders of magnitude from the numbers we reported above for 1 year of observations.
Such a discrepancy is clearly due to differences in the two simulations.
For example, by comparing Fig.~6 in~\cite{Song:2022siz} with our \cref{fig:N_hosts_statistics}, it is clear that on average the number of galaxies contained within a BBH sky localization volume is much smaller in~\cite{Song:2022siz}, where basically it never exceeds 10 with the majority of GW events [$\mathcal{O}(100)$] having one single potential host galaxy, than in our setup, where we count on average hundreds of galaxies per GW event with only a handful of BBHs having 10 galaxies or less.
From this comparison we clearly understand that the forecasts provided in~\cite{Song:2022siz} are extremely optimistic if compared to our study.


\subsection{\texorpdfstring{$\SNR_{\rm net} > 300$}{SNR} at \texorpdfstring{$z<1$}{z}: Single-host dark sirens only}\label{subsec:results_single_host}

In case a dark siren has only one potential galaxy host falling within the localization error volume, we may consider them as ``effective bright sirens.'' 
These ``golden sirens'' are expected to be powerful probes of the cosmological parameters, since the redshift information comes from a single galaxy. 
The only caveat is that such golden sirens are not expected to be very numerous, since in general they are characterized by having localization error volumes small enough to contain just one galaxy. 
Moreover they are observed preferentially at low redshift since on average the higher the distance to the source, the larger its sky localization volume, and consequently the less likely there is only one galaxy within.
Nevertheless, given their similarity with bright sirens, it is of interest to understand how useful these golden events can be in the inference of the cosmological parameters.
In general, all the realizations analyzed here have at least one golden dark sirens (see the first bin on the x-axis of the right plot in \cref{fig:N_hosts_statistics}).
In all the two network configurations, we find that 
single-host dark sirens cannot constrain $\Om$. This is not surprising given the low-redshift of these golden events: $z<0.08$ for ET+CE1, and $z<0.22$ for ET+CE1+CE2.
Yet, these GW events can constrain the Hubble constant $h$ in all the two network configurations. 
For the network ET+CE1, we have only $N=1$ single-host GW event, which therefore allows for constraints on $h$ at the level of $5.6\%$-$11.2\%$, while ET+CE1+CE2 gives better results than ET+CE1, with on average $N=5$ observations and $h$ constrained with a $2.7\%$-$3.3\%$ precision. 
We can now compare our results with the ones reported by similar studies in the literature exploiting golden sirens observed by a 3G network~\cite{Borhanian:2020vyr,Gupta:2022fwd}.
Reference~\cite{Borhanian:2020vyr} in particular consider several populations of BBH golden sirens and reports constraints on $\hubble$ that can reach $\mathcal{O}(0.1\%)$ at 68\% CI or better within 2 years of observations with ET+CE1+CE2 and only considering events at $z<0.1$.
If compared with the numbers we report above, our results are more than one order of magnitude worse than the one reported in~\cite{Borhanian:2020vyr}.
This is not surprising and may be due to several reasons, in particular to the different assumptions that have been employed in the two different studies which overall are more optimistic in~\cite{Borhanian:2020vyr} than in our study. Among them, we can cite the lack of redshift uncertainty, the linear Hubble law is used with $\hubble$ as the only parameter to be inferred and different BBH populations. Most importantly, however, the main motivation behind our differences lies in the number of single-host events employed in the inference: our study suggests that the average rate of single-host events in the ET+CE1 (ET+CE1+CE2) network is \SI{1}{\per\year} (\SI{5}{\per\year}), while~\cite{Borhanian:2020vyr} reports \SI{22}{\per\year} in the most sensitive network. We find this rate consistent with the one we obtain once we repeat the error-box generation process without extending the redshift boundaries in the last step (i.e., without priors on the cosmological parameters), which corresponds on average to \SI{26}{\per\year} in ET+CE1+CE2.
The more realistic simulations performed here suggest that golden sirens from 3G detectors will not be able to constrain $\hubble$ at the subpercent level, but nonetheless reach an interesting $\mathcal{O}(1\%)$ precision.


\subsection{\texorpdfstring{$\SNR_{\rm net} > 300$}{SNR} at \texorpdfstring{$z<3$}{z}}
\label{subsec:results_z<3}

As presented above, our fiducial scenario considers a galaxy catalog complete at $z<1$.
This is a somehow conservative scenario in which we can perform 3G cosmological analyses only with readily available all-sky galaxy catalogs, which we assume will be complete up to $z=1$ in the 3G era.
Nevertheless in a more optimistic scenario one could foresee that dedicated deep-field surveys will be performed along the sky-localization cone of each BBH detected with $\SNR_{\rm net} >300$.
As we have shown above this $\SNR_{\rm net}$ threshold yields at most a few hundreds BBH detections per year, specifically with the ET+CE1+CE2 network.
Providing a deep-field galaxy survey for each of these events may seem unfeasible, but one must remember that the follow-up survey can be taken even years after 3G detectors have obtained the GW data.
This means that, provided adequate EM telescope resources will be available during or after the 3G detector era, such a scenario can be considered realistic.

Importantly, we make the simplifying assumption that instrumental errors of high-$z$ galaxies are negligible: this will not probably be the case, since typically at high-$z$ we will have photometric redshifts. Nonetheless, we can think of this idealized case as a very optimistic scenario where we assume we can perfectly correct for instrumental uncertainties and we can test the full performance of 3G detectors at high-$z$.

In general, we expect some improvement on the measure of $\Om$, which is more sensitive to the higher-$z$ observations reported in \cref{fig:z_dark_sirens}. With a relatively larger number of high-redshift dark sirens (see \cref{tab:cosmo_results}), we find that constraints on $\hubble$ substantially coincides with those obtained in our fiducial scenario ($z<1$), while we obtain only slightly stringier constraints for $\Om$, with the network ET+CE1+CE1, which may reach a precision of 8.7\% at 90\% CI, starting from a $z<1$ result of 10.0\%.

These results suggest that BBHs at $z>1$ will not substantially contribute to measurements of $\hubble$ and $\Om$.
This is certainly due to the lower accuracy with which BBHs at high redshift, which on average have a lower SNR, can be localized in the Universe.
The wider sky localization volume will in fact contain a large number of potential host galaxies, conveying basically no information on the posteriors of the cosmological parameters.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/z_dark_sirens.pdf}
    \caption{Redshift distribution of the $\SNR_{\rm net} > 300$ dark sirens employed in the inference for the different network configurations, colors as in legend. The solid histograms refer to the fiducial $z < 1$ scenario, while the dotted histograms extend the distributions up to the optimistic $z < 3$ case.}
    \label{fig:z_dark_sirens}
\end{figure}

\section{Discussion}
\label{sec:discussion}

The ensemble of results we reported above constitutes the most up-to-date realistic cosmological forecasts with 3G BBH dark sirens.
In this section we discuss few important issues and limitations of our investigations.

\subsection{Expectations from longer observations}

In \cref{sec:redshift_distribution} we reported an expected BBH cosmic merger rate of $\sim$50000 per year.
Although the results presented in this study come from $1$ year of data recording, 3G detectors are going to be the cornerstone of GW observations for the next decades and operate for several years.
An analysis similar to ours over a multiyear dataset of 3G dark siren observations would be prohibitive with the computational resources at our disposal.
Nevertheless we can obtain a simple estimation starting from the results we obtained for one year of data and extrapolating them assuming they scale as the square root of observational time, which is directly related to the number of observed dark sirens.
We stress that this is clearly an oversimplification useful only to provide us with approximate estimates.

Starting from the numbers in \cref{tab:cosmo_results}, we compute the constraints on $\hubble$ and $\Omega_m$ expected in 3, 5, and 10 years of observations for our fiducial scenario.
The resulting estimates are reported in \cref{tab:multi-year-results}.
We can clearly see that all 3G detector networks will be able to reach subpercent constraints on $\hubble$ within few years of observations, delivering $\mathcal{O}(0.1\%)$ precisions.
On the other hand, constraints on $\Omega_m$ hover around few $\%$ irrespectively of the observational time.

\begin{table*}[t]
\centering
%\bgroup
%\setlength{\tabcolsep}{0.5em} % horizontal padding
\def\arraystretch{\tabvspace} %  vertical padding (default is 1)
\begin{tabular}{ l || c | c | c || c | c | c }
\hline
\hline
 \multirow{2}{*}{\textbf{Network}} & \multicolumn{3}{c||}{$\Delta h / h$ ($\%$)} & \multicolumn{3}{c}{$\Delta \Om / \Om$ ($\%$)} \\\cline{2-7}
 & \SI{3}{\year} & \SI{5}{\year} & \SI{10}{\year} & \SI{3}{\year} & \SI{5}{\year} & \SI{10}{\year} \\
 \hline
ET+CE1 & 0.4 (0.6) & 0.3 (0.5) & 0.2 (0.4) & 5.1 (8.3) & 3.9 (6.4) & 2.8 (4.6) \\
ET+CE1+CE2 & 0.3 (0.5) & 0.2 (0.4) & 0.1 (0.3) & 3.5 (5.8) & 2.7 (4.5) & 1.9 (3.2) \\
\hline
\hline
\end{tabular}
\caption{Expected cosmological constraints at the 68\% (90\%) CI for multiyear 3G observations estimated from the 1 year fiducial results (see \cref{tab:cosmo_results}) with a simple scaling proportional to the square root of the observational time.}
\label{tab:multi-year-results}
\end{table*}


\subsection{Comparison with the literature}\label{subsec:compare_literature}

In \cref{sec:results} we confronted our estimates with the ones reported by comparable investigations that we found in the literature. Here we briefly summarize the results of these comparisons.
Our analysis with ET+CE1+CE2 and single-host events only, leads to a few $\%$ precision on $\hubble$, which is worse by an order of magnitude than what reported in~\cite{Borhanian:2020vyr}, as discussed in~\cref{subsec:results_single_host}.
Furthermore assuming $\Om$ known, our constraints on $\hubble$ with the detector network ET+CE1+CE2 are on the order of the subpercent, which is two orders of magnitude lower than what found in~\cite{Song:2022siz} (see discussion in~\cref{subsec:results_h_only}).
Our findings are in better agreement with, though slightly better than, the ones reported in~\cite{Zhu:2023jti}, which provides cosmological forecasts for the network ET+CE1 only.
These results are not surprising.
Our simulation overall represents a more realistic setup than the ones considered in~\cite{Borhanian:2020vyr,Song:2022siz}, implying that less optimistic cosmological constraints were expected.
On the other hand our simulation is better comparable with the approach taken by~\cite{Zhu:2023jti}, with order one discrepancies in the reported cosmological constraints probably due to the different assumptions considered by the two investigations.
By building on these previous forecasts, our results certainly help to better define the dark siren cosmological science case of 3G detectors and to understand their potential and limitation.


\subsection{Gravitational-wave systematic effects}

Our forecasts assume that we will be able to account for systematic errors coming for example from approximate waveforms or the uncertainty in the calibration of the detectors~\cite{Huang:2022rdg}.
In particular in order to achieve the measurement of the $\lcdm$ parameters reported in this study, specifically subpercent measurements of $\hubble$, we need to keep these systematics under control with a precision greater than the accuracy with which cosmological parameters are measured.
This requires for example that waveform models may need to be calibrated with an improved accuracy by as much as three orders of magnitude with respect to current waveforms, especially for high SNR BBHs as the ones considered in this work~\cite{Purrer:2019jcp,Hu:2022rjq}.
This clearly poses a challenge for future waveform models, which will need to be significantly more accurate and physically more complete in order to meet the scientific objectives of 3G detectors. 
Similarly 3G interferometers must be precisely calibrated in order to avoid propagation of systematics to the inference of cosmological parameters~\cite{Essick:2022vzl}.
This may pose a technical challenge to obtain highly precise measurements of $\hubble$, although other sources of systematics are expected to be more problematic~\cite{Payne:2020myg}.
Finally, an additional source of systematics may come from environmental effects perturbing the dynamics of the GW source: for example the vicinity of a perturbing third body, the presence of gas surrounding the source or gravitational lensing, the impact of peculiar velocities on very close-by events.
Such effects can lead to an erroneous estimation of the distance or sky localization of the source, which in turn will yield a wrong measurement of the cosmological parameters~\cite{Zhu:2023jti}.
From these remarks it is clear that much theoretical and experimental work is still needed in order to achieve a subpercent measurement of $\hubble$, calling for an intense development effort over the next decade.


\subsection{Limitations and future perspectives}

Here we discuss the limitations of our study together with suggestions for future improvements.
The impact of these limitations on the cosmological inference with dark sirens will be the object of future studies.

We characterized BBH GW signals with the \texttt{IMRPhenomXHM} waveform model, which describes nonprecessing binary systems with BH parallel spin vectors. This assumption allowed us to reduce the total number of source parameters - and therefore the dimension of the Fisher matrix - from \num{15} to \num{11}, limiting computational cost and potential issues in the FIM inversion process. However, precessing systems are expected to form in nature, and precessing waveform models (e.g.~\texttt{IMRPhenomXPHM} \cite{pratten2021computationally}) should be used.
This in turn highlights the need to update the sampling distribution of the spins as well, making the BBH population more accurate and consistent with the latest available results, if observations will suggest evidence for precessing systems (see e.g.~the models described in \cite{ligo2021population}). 
We neglected Earth's rotation effects on the observed signal. While BBHs are not particularly influenced, we underline the need to take time-varying antenna pattern functions into account, especially to properly model very light BBH systems.
From the instrument point of view, we considered GW detectors to be fully operative during the whole period of observation. In reality, detectors may undergo provisional maintenance and upgrade works that inevitably force the discontinuation of data recording.
For this reason, each detector should be characterized by its own duty cycle. Furthermore, the GW emission of some compact binary coalescences are expected to overlap in the time-domain data strain $s(t)$ of a detector, posing a challenging task for the extraction of the individual GW signals. In this work, we assumed no overlap of BBH signals: while 
recent studies showed that it seems possible to run a reliable parameter estimation on overlapping events~\cite{samajdar2021biases,himemoto2021impacts,pizzati2022toward}), one should quantify how many of these events 3G detectors might observe and how robustly these signals could be resolved, with the potential effect of reducing the number of dark sirens available for cosmological inference.

Concerning the galaxy catalog, we report the main upgrades that can enhance the current simulations. To mimic incompleteness effects in our catalog, we have performed a stellar mass cut. However, galaxy mass is not an easy quantity to determine from EM surveys given the degeneracies involved in the galaxy template fitting method. To avoid this limitation and account catalogue incompleteness in a more precise way, it would be convenient to use a direct observable like the luminosity (or magnitude). To guide the reader, current galaxy catalogues of SDSS Legacy Survey are complete up to optical magnitudes of $25.1$ which will increase up to $27.5$ for the future LSST survey. 
Moreover, future simulations should account in a more accurate way for incompleteness and selection effects ruling the catalogs provided by current and future surveys. In this way, it will be possible to account for missing dwarf (or, in terms of luminosity, faint) galaxies in the GW localization error volumes.
On the other hand, on top of the sky location of each galaxy, its luminosity could be also used as an extra condition to model the probability of housing a GW event. Bright galaxies are expected to have higher chances of hosting compact binary coalescences, and this information could be used to improve the cosmological inference methodology.

The inference on the cosmological parameters is carried out on high $\SNR_{\rm net}$ events. This cut meets our requirements in terms of available computational resources and formulation of the inference problem, notably the exclusion of GW selection effects.
Indeed we observed that by lowering our $\SNR_{\rm net}$ threshold, the estimates start to systematically deviate from the true values. Furthermore, recent studies where systematic effects are thoroughly discussed report the need to perform joint source population and cosmological inference, since a separate treatment can impact the final measurement accuracy~\cite{2021ApJ...909..218A,mastrogiovanni_2021}. One could therefore make a similar analysis on a much larger sample of GW events (i.e.~by lowering the $\SNR_{\rm net}$ threshold), provided that selection and systematic effects are properly accounted for.


\section{Conclusion}
\label{sec:conclusion}

In this work we studied the scientific potential of dark sirens in the context of next generation, 3G ground-based detectors such as ET and CE. We performed parameter estimation on GW signals emitted by a mock population of BBHs through the FIM formalism in different configurations of detectors, namely ET, ET+CE1, ET+CE1+CE2, assuming $\SI{1}{\year}$ of continuous observations. 
We then selected and employed high $\SNR_{\rm net}$ GW events as dark siren candidates in a Bayesian framework to recover joint posterior distributions on the set of $\lcdm$ cosmological parameters, namely $\hubble$ and $\Om$.
Our main results are based on a fiducial scenario in which we assumed galaxy surveys to be complete up to $z = 1$ by the 3G detector era. Under these premises, we found that the best constraints are obtained by the ET+CE1+CE2 network, where $\hubble$ ($\Om$) is recovered at a promising $0.8\%$ ($10.0\%$) at $90\%$ CI (cf.~\cref{subsec:results_fiducial}). On the other hand, we find that ET alone is not able to provide informative results.

Assuming $\Om$ is known perfectly a priori, a network made of ET and at least one CE (with a 40 km baseline) can lead to a $0.4\%$ precision in the measure of $\hubble$ (cf.~\cref{subsec:results_h_only}). 

Furthermore, we characterized the constraining power
of well-localized BBHs by comparing the precision on $\hubble$ obtained from single-host dark sirens only. 
We find that the precision in ET+CE1 (ET+CE1+CE2) ranges between $5.6\%$ ($2.7\%$) and $11.2\%$ ($3.3\%$)(cf.~\cref{subsec:results_single_host}). 
Finally, we considered an optimistic scenario where deep sky surveys may be employed to reach and scan galaxies up to a $z=3$ horizon, finding no significant improvement on $\hubble$ and modest improvement on $\Om$ in all the network configurations (cf.~\cref{subsec:results_z<3}).

Our results suggest that the synergy between multiple 3G detectors is crucial to reach sub-$\%$ precision on $\hubble$. 
Even if we expect EM estimates to improve in the next decade, we highlight that GW-based observations can offer an independent way to measure $\hubble$.

Furthermore, we underline that the dark siren statistical method could offer significant information on $\Om$, even if the number of high-redshift GW events is usually low as they are usually not well localized and thus hardly satisfy the necessary conditions for them to be used as dark sirens.
Nevertheless we find that a network of 3G detectors can still constrain $\Om$ at $\sim 10\%$ of precision at $90\%$ CI.
High-redshift dark sirens could nonetheless provide interesting information on alternative cosmological models, especially if they predict deviations at high redshift, and thus help in testing dark energy or modified gravity.
Further investigations are needed in order to understand the full potential of dark sirens at high redhsift.

To conclude, 3G detectors have the potential to greatly improve our understanding of the Universe, especially by providing stringent constraints on cosmological parameters, thus ushering us in the era of precision GW cosmology.
Further work is needed in order to define the full cosmological science case for 3G detectors, notably on the integration of different standard siren methods, but our forecasts show already the promising results that we will obtain from dark sirens only.


\acknowledgements
The authors would like to thank the anonymous referee for providing useful comments to the first version of this study.
The authors would like to thank Archisman Ghosh for feedback on the manuscript.
D.L. thanks Walter Del Pozzo for stimulating discussions.
N.M. and D.L. thank Chang Liu for useful comments.
Support for D.L. was partially provided by CNES through a CNES Postdoctoral Fellowship grant.
D.L., N.T. and S.M.~acknowledge support form the French space agency CNES in the framework of LISA. 
N.M., D.L. and N.T.~acknowledge support from an ANR Tremplin ERC Grant (No. ANR-20-ERC9-0006-01). N.M. acknowledges support from the Swiss National Science Foundation, grant No. 200020$\_$191957, and from the SwissMap National Center for Competence in Research.
D.I.V. acknowledges the financial support provided under the European Unions H2020 ERC Consolidator Grant ``Binary Massive Black Hole Astrophysics'' (B Massive, Grant Agreement: 818691) and from INFN H45J18000450006.
Most of the numerical analyses have been performed at the IN2P3 computing centre (CC-IN2P3) in Lyon (Villeurbanne), which we thank for assistance and computational resources.


%-----------------------------
% BACK MATTER

\newpage
\bibliography{bibliography.bib}% Produces the bibliography via BibTeX.


\end{document}

% ****** End of file main.tex ******