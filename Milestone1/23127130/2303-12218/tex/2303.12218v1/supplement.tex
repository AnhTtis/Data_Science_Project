\documentclass[10pt,onecolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).

\renewcommand{\figurename}{Supplementary Fig.}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{4488} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
%\title{Compositional 3D Scene Diffusion}
\title{Compositional 3D Scene Generation using Locally Conditioned Diffusion:
Supplementary Materials}
% semantic 3D scene synthesis using composable diffusion models
% compositional 3d scene diffusion
\maketitle

%\tableofcontents

\section{Implementation details}
\paragraph{CLIP R-precision.} We follow prior work \cite{Poole2022DreamFusionTU} in evaluating our compositional 3D generation method using CLIP R-precision. R-precision is the accuracy at which an image recovers the correct text caption among a set of distractors using CLIP \cite{Radford2021LearningTV} similarity. In our implementation we took 267 random text prompts from the DreamFusion gallery page and added 3 additional prompts for each scene: the composed prompt and the two individual prompts being composed. The results in the paper are an average taken over the 4 compositional scenes shown in Fig. 4.



\paragraph{3D implementation of Composable-Diffusion.} In our baseline, we compare our method to an implementation of Composable-Diffusion \cite{Liu2022CompositionalVG} applied to an SDS-based 3D generation pipeline (in this case SJC \cite{Wang2022ScoreJC}). Work in Composable-Diffusion has shown the potential of applying their global gradient composition method to a 3D diffusion pipeline such as Point-E \cite{Nichol2022PointEAS}. In our implementation, we simply replace the denoising step, with the composed denoising steps calculated using Composable-Diffusion. Results shown in Fig. 4 show that our implementation does not converge to a coherent scene. We provide additional results in Supplementary Fig. \ref{fig:composable} to show that our implementation works for other prompts. In our experiments, we found that our implementation of 3D Composable-Diffusion worked best when composing object-centric prompts, but failed to converge for prompts desceibing entire scenes. 

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/composed_diffusion.pdf}
    \caption{Additional generations from our implementation of Composable-Diffusion \cite{Liu2022CompositionalVG} applied onto SJC. Composable-Diffusion applied to an SDS-based pipeline tends to work better when composing object-centric prompts. Corresponding depth maps for each generation are shown on the right.}
    \label{fig:composable}
\end{figure*}


\paragraph{Object-centric camera pose sampling.} We found that object-centric camera pose sampling is essential for compositional 3D generation, especially for regions that are away from the center of the generation volume. SJC \cite{Wang2022ScoreJC} randomly samples camera angles facing the center (origin) of the VoxNeRF representation. During training, we sample the $i^{\text{th}}$ scene component with probability $p_i$. If a scene component is sampled, the camera pose is offset to face the center of the corresponding bounding box. We leave the values of $p_i$ as a hyper-parameter. We found that off-center scene components often require a higher sampling probability.

\paragraph{View-dependent prompting.} Following other SDS-based 3D generative models \cite{Poole2022DreamFusionTU, Wang2022ScoreJC, Lin2022Magic3DHT, Metzer2022LatentNeRFFS}, our method also relies on  view-dependent prompting. Depending on azimuth angle and elevation of the current camera pose, the prefixes 
''overhead view of", ''front view of", ''backside view of" and ''side view of" are added to the input prompts. As mentioned in prior works, this method alleviates the ''Janus problem", where multiple faces may appear on the same object. In addition to view-depending prompting, we found that adding the prefix ''a zoomed out photo of" and suffix ''highly detailed" too the input prompts would at times improve output generations.

\paragraph{Concept negation for locally conditioned diffusion. } Similar to Composable-Diffusion \cite{Liu2022CompositionalVG}, we show that our locally conditioned diffusion method also allows for concept negation. Recall the expression for noise estimation using classifier free guidance for some text prompt $y$,
\begin{equation}
    \hat{\boldsymbol{\epsilon}}_\phi(x_t, t, y) = \boldsymbol{\epsilon}_\phi(x_t, t, \emptyset) + s\Bigl(\boldsymbol{\epsilon}_\phi(x_t, t, y) - \boldsymbol{\epsilon}_\phi(x_t, t, \emptyset)\Bigr).
\end{equation}
Given a negation concept $y_n$, we can apply concept negation during noise estimation with the following,
\begin{equation}
    \hat{\boldsymbol{\epsilon}}_\phi(x_t, t, y, y_n) = \boldsymbol{\epsilon}_\phi(x_t, t, \emptyset) + s\Bigl(\boldsymbol{\epsilon}_\phi(x_t, t, y) - \boldsymbol{\epsilon}_\phi(x_t, t, y_n)\Bigr).
\end{equation}
We show qualitative comparisons of 2D generations with and without concept negation in Supplementary Fig. \ref{fig:negation}. Locally conditioned diffusion is applied to each semantic region as usual, but noise predictions are evaluated using concept negation (negated concept is the other text prompt in the semantic map). We found that using concept negation could help 2D locally conditioned diffusion generations to better adhere to input semantic maps. However, there was no notable difference when applied to the 3D setting. All 3D results shown in the main paper and supplementary materials do not use concept negation during generation.
\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/negation.pdf}
    \caption{2D generations using locally guided diffusion with and without concept negation. With concept negation, our method adheres closer to the input segmentation masks.}
    \label{fig:negation}
\end{figure*}

\newpage


\section{Additional results}
\paragraph{Depth maps.} We show our results with their corresponding depth maps in Supplementary Fig. \ref{fig:depth}.
% Depth maps
% Diffusion renders



\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/main_res_depths.pdf}
    \caption{Main figure generations with corresponding depth maps.}
    \label{fig:depth}
\end{figure*}




%%%%%%%%% BODY TEXT

{\small
\bibliographystyle{ieee_fullname}
\bibliography{suppbib}
}

\end{document}