\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/size_and_position.pdf}
    \vspace{-1em}
    \caption{\textbf{Size and position control.} Our method provides size and position control of individual scene components through user-defined bounding boxes. Our method provides fine-grained control over scene composition while ensuring each components blends seamlessly into the overall scene.}
    \label{fig:size_and_position}
    \vspace{-1em}
\end{figure*}
\section{Experiments}\label{experiments}
We show qualitative results on compositional text-to-2D and text-to-3D generation. For 3D results, we mainly compare against SJC \cite{Wang2022ScoreJC} as it is the best-performing publicly-available text-to-3D method. We also implemented a version of SJC that leverages Composable-Diffusion \cite{Liu2022CompositionalVG} as an additional baseline.

\subsection{Compositional 2D results}

\paragraph{Implementation details.} We apply our locally conditioned diffusion method to existing text-conditioned diffusion models: GLIDE~\cite{Nichol2021GLIDETP} and StableDiffusion~\cite{Rombach2021HighResolutionIS}. We use pre-trained models provided by the authors of each respective paper to implement locally conditioned diffusion. Each image sample takes 10--15 seconds to generate on an NVIDIA A100 GPU, where duration varies according to the number of distinct semantic regions provided. Note that sampling time increases sub-linearly with respect to number of regions/prompts, this is because calls to the same model for each text-conditioning can be done in a single batch.

\paragraph{Qualitative results.} We provide qualitative examples in  Fig. \ref{fig:2d_results}. Our method is able to generate high-fidelity images that adhere to the input semantic masks and text prompts. Note that our method does not aim at generating images that follow the exact boundaries of the input semantic masks, instead it strives to achieve seamless transitions between different semantic regions. A key advantage of locally conditioned diffusion is that it is agnostic to the network architecture. We demonstrate this by showing that our method works on two popular text-to-image diffusion models GLIDE \cite{Nichol2021GLIDETP} and StableDiffusion \cite{Rombach2021HighResolutionIS}.



\subsection{Compositional 3D results}

\paragraph{Implementation details.} Our compositional text-to-3D method builds upon the SJC \cite{Wang2022ScoreJC} codebase. Following SJC, we use a Voxel NeRF to represent the 3D scene model and StableDiffusion \cite{Rombach2021HighResolutionIS} as the diffusion prior for SDS-based generation. The Voxel NeRF representing the 3D scene is set to a resolution of $100^3$. This configuration uses $\approx10$ GB of GPU memory. The original SJC method uses an emptiness loss scheduler to improve the quality of generated scenes. Our method also leverages this emptiness loss; please refer to the original SJC \cite{Wang2022ScoreJC} for more details.

\paragraph{Qualitative results.} We provide qualitative examples of compositional text-to-3D generations with bounding box guidance in Fig. \ref{fig:main_results}. Notice that our method is able to generate coherent 3D scenes using simple bounding boxes with corresponding text prompts. Our method generates results that adhere to the input bounding boxes, allowing users to edit the size and position of individual scene components before generation. Fig. \ref{fig:size_and_position} shows generated results of the same scene prompts with differing bounding box sizes and positions. Note that our method is able to adapt to the user's input and generate scenes with varying compositions.

\paragraph{Baseline comparisons.} We compare our method to different variants of SJC \cite{Wang2022ScoreJC}. Namely, (i) SJC generations using a single prompt for the entire scene, (ii) individual SJC generations for each scene component, and (iii) an implementation of Composable-Diffusion \cite{Liu2022CompositionalVG} combined with SJC. Although DreamFusion \cite{Poole2022DreamFusionTU} and Magic3D \cite{Lin2022Magic3DHT} have also shown to generate high-quality results, both models leverage image diffusion priors (Imagen \cite{Saharia2022PhotorealisticTD} and eDiff-I \cite{Balaji2022eDiffITD}) that are not publicly available. However, it is important to note that our method can theoretically be applied to any SDS-based method. This can be achieved by replacing the image diffusion model in DreamFusion \cite{Poole2022DreamFusionTU} and Magic3D \cite{Lin2022Magic3DHT} with the locally conditioned method described above.

We provide qualitative results for our method and each baseline in Fig. \ref{fig:baselines}. In our experiments we attempt to compose two scene components into a coherent scene. Specifically, we choose an object-centric prompt that describes individual objects, paired with a scene-centric prompt that describes a background or an environment. 

We observe that SJC fails to capture certain scene components when composing multiple scene components into a single prompt. Our method is able to capture individual scene components while blending them seamlessly into a coherent scene. 

For object-centric prompts, SJC is able to create high-quality 3D generations. However, scene-centric prompts such as ``a desert'' or ``a beach'' end up generating dense volumes that resemble the text-prompt when rendered from different angles, but fail to reconstruct reasonable geometry. By defining bounding boxes for each scene component, our method provides coarse guidance for the geometry of the scene, this helps generate results with fewer ``floater'' artifacts.
\begin{figure}[tb!]
    \centering
    \includegraphics[width=\linewidth]{figures/reflections.pdf}
    \caption{\textbf{Seamless transitions.} Our method is able to smoothly transition between scene components in different bounding boxes. In this example, we can see the reflection of the cherry blossom tree in the lake.}
    \label{fig:smooth}
    \vspace{-0.3cm}
\end{figure}
One option for compositional scene generation is to generate each scene component individually and then combine them manually afterwards. However, blending scene components together in a seamless manner takes considerable effort. Our method is able to blend individual objects with scene-level detail. As shown in Fig. \ref{fig:smooth}, although the cherry blossom tree and the reflective purple lake correspond to different bounding box regions, our method is able to generate reflections of the tree in the water. Such effects would not be present if each scene component were generated individually and then manually combined.

We also compare our method to a composable implementation of SJC using Composable-Diffusion \cite{Liu2022CompositionalVG}. However, this method fails to generate reasonable 3D scenes.

\paragraph{Quantitative results.} Following prior work \cite{Poole2022DreamFusionTU, Jain2021ZeroShotTO}, we evaluate the CLIP R-Precision, the accuracy of retrieving the correct input caption from a set of distractors using CLIP \cite{Radford2021LearningTV}, of our compositional method. Tab. \ref{tab:clip_eval} reports CLIP R-Precision values for rendered views of scenes shown in Fig. \ref{fig:baselines} using our compositional method and SJC with a single prompt. Our method outperforms the baseline across all evaluation methods.

\begin{table}[!ht]
\caption{CLIP R-Precision comparisons.}
  \centering
  \begin{tabular}{cccc}
  \toprule
    & \multicolumn3{c}{R-Precision $\uparrow$} \\
    \cmidrule{2-4}
    Method & B/32  & B/16  & L/14 \\
    \midrule
    Single Prompt (SJC) & 27..8 & 31.5  & 28.53\\
    Composed (Ours) & \textbf{38.6} & \textbf{54.3} & \textbf{29.8}\\
    \bottomrule
  \end{tabular}
  \label{tab:clip_eval}
    \vspace{-1em}
\end{table}

\paragraph{Ablations.} We found that object-centric camera pose sampling is essential for successful composition of multiple scene components. This is especially true for bounding boxes further away from the origin. We compare generations with and without object-centric pose sampling in Fig.~\ref{fig:ablations}. Note that our method tends to ignore certain scene components without object-centric sampling.

\begin{figure}[tb!]
    \centering
    \includegraphics[width=\linewidth]{figures/ablations_2x3.pdf}
    \vspace{-1em}
    \caption{\textbf{Ablation over object-centric sampling.} Without object-centric sampling, our method fails fully capture off-centered scene components.}
    \label{fig:ablations}
    \vspace{-1em}
\end{figure}


\paragraph{Speed evaluation.} Unless stated otherwise, all results were generated by running our method for 10000 denoising iterations with a learning rate of 0.05 on a single NVIDIA RTX A6000. Note that scenes with a higher number of distinct text prompts require a longer period of time to generate. Using SJC, generating scene components individually causes generation time to scale linearly with number of prompts. In contrast, our method can compose the same number of prompts in a shorter amount of time, as calls to the same diffusion prior conditioned on different text-prompts can be batched together. Table \ref{tab:speed_eval} shows generation times for SJC and our method for 3000 denoising iterations.

\begin{table}[ht!]
\caption{Generation times using SJC \cite{Wang2022ScoreJC} for individual prompts and composing multiple prompts using our method.}
  \centering
  \begin{tabular}{cccc}
  \toprule
    & & \# of prompts & \\
    \cmidrule{2-4}
    Method & 1  & 2  & 3  \\
    \midrule
    Individual (SJC) & 8 mins & 16 mins & 24 mins \\
    Composed (Ours) & 8 mins & 12 mins & 15 mins \\
    \bottomrule
  \end{tabular}

  \label{tab:speed_eval}
\end{table}







