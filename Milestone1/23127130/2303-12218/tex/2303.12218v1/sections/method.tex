
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/composed_2d_3row.pdf}
    \vspace{-1em}
    \caption{\textbf{2D locally conditioned diffusion results.} Given coarse segmentation masks as input, our method is able to generate images that follow the specified layout while ensuring seamless transitions. Results in the first row are generated using GLIDE \cite{Nichol2021GLIDETP}, while the second and third rows show results generated using StableDiffusion \cite{Rombach2021HighResolutionIS}.}
    \label{fig:2d_results}
\end{figure*}



\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/baselines_10k.pdf}
    \vspace{-1em}
    \caption{\textbf{Baseline comparisons.} Left to right: (i) SJC results using a single text prompt, (ii) SJC generating each scene component independently, (iii) SJC combined with Composable-Diffusion \cite{Liu2022CompositionalVG}, and (iv) our method with corresponding bounding boxes and text prompts. Generations for each row use the text prompts listed on the right. Results in the first column are generated by combining individual text prompts with the connecting phrase ``in the middle of'', e.g. ``a lighthouse in the middle of a beach''. Our method successfully composes different objects into a coherent scene while following the user input bounding boxes.}
    \label{fig:baselines}
    \vspace{-1em}
\end{figure*}

\section{Locally conditioned diffusion}
We introduce \textbf{locally conditioned diffusion} as a method for providing better control over the composition of images generated by text-conditioned diffusion models. The key insight of our method is that we can selectively apply denoising steps conditioned on different text prompts to specific regions of an image.

Given a set of text prompts $\{y_1, \dots, y_P\}$, classifier-free guidance \cite{Ho2022ClassifierFreeDG} provides a method for predicting denoising steps conditioned on $y_i$:
\begin{equation}
    \hat{\boldsymbol{\epsilon}}_\phi(x_t, t, y_i) = \boldsymbol{\epsilon}_\phi(x_t, t, \emptyset) + s\Bigl(\boldsymbol{\epsilon}_\phi(x_t, t, y_i) - \boldsymbol{\epsilon}_\phi(x_t, t, \emptyset)\Bigr)
\end{equation}
Using a user-defined semantic segmentation mask $m$, where each pixel $m[j]$ has integer value $[1,P]$, the overall noise prediction can then be represented by selectively applying noise predictions to each labelled image patch:
\begin{equation}
    \hat{\boldsymbol{\epsilon}}_\phi(x_t, t, y_{1:P}, m) = \sum_{i=1}^P 
    \mathds{1}_{i}(m) \odot\hat{\boldsymbol{\epsilon}}_\phi(x_t, t, y_i)
    \label{sem_eps}
\end{equation}
Where $\mathds{1}_{i}(m)$ is the indicator image with equivalent dimensionality as $m$ and
\begin{equation} 
\mathds{1}_{i}(m)[j] = 
    \begin{cases}
    1,& \text{if } m[j] = i\\
    0,              & \text{otherwise}
\end{cases}
\end{equation}
The proposed locally conditioned diffusion method is summarized in Algorithm \ref{alg:semdiff}. 

\begin{algorithm}
\caption{Locally conditioned diffusion}\label{alg:semdiff}
\begin{algorithmic}
\Require Diffusion models $\hat{\boldsymbol{\epsilon}}_\phi(x_t, t, y_i)$, guidance scale $s$, semantic mask $m$
\State $x_T \sim \mathcal{N}(0, I)$ \Comment{Initialize Gaussian noise image} 
\For{$t = T,\dots,1$}
    \State $\epsilon_i \gets \hat{\boldsymbol{\epsilon}}_\phi(x_t, t, y_i)$ \Comment{Individual noise predictions} 
    \State $\epsilon \gets \hat{\boldsymbol{\epsilon}}_\phi(x_t, t, \emptyset)$ \Comment{Unconditional noise prediction} 
    \State $\epsilon_{\textrm{sem}} \gets \sum_{i=1}^P \mathds{1}_i(m) \odot s(\epsilon_i - \epsilon)$ \Comment{Combine noise predictions} 
    \State $x_{t-1} = \textrm{Update}(x_t, \epsilon_{\textrm{sem}})$ \Comment{Apply denoising step}
\EndFor
\end{algorithmic}
\end{algorithm}Although a large proportion of noise predictions are not used, in practice only one diffusion model $\boldsymbol{\epsilon}_\phi$ is queried. All calls to the model for each unique text-conditioning $y_i$ can be batched together for increased efficiency. 

Our locally conditioned diffusion method generates high-fidelity 2D images that adhere to the given semantic segmentation masks. Note that, while each segment of the image is locally conditioned, there are no visible seams in the resulting image and transitions between differently labelled regions are smooth, as shown in Fig. \ref{fig:2d_results} (see Sec. \ref{experiments} for more details).

\section{Compositional 3D synthesis}

To make compositional text-to-3D synthesis as simple as possible, our method takes 3D bounding boxes with corresponding text prompts as input. The goal of our method is to generate 3D scenes that contain objects specified by the text prompts while adhering to the specific composition provided by the input bounding boxes. In this section, we describe our method and how we apply locally conditioned diffusion in 2D to enable controllable generation in 3D.
\paragraph{Text-to-3D with Score Distillation Sampling.} Our method builds off existing SDS-based text-to-3D methods \cite{Lin2022Magic3DHT, Poole2022DreamFusionTU, Wang2022ScoreJC}. SDS-based methods leverage a 3D scene representation parameterized by $\theta$ is differentiably rendered at a sampled camera pose, generating a noised image $g(\theta)$ which is passed into an image diffusion prior. Our method builds off SJC \cite{Wang2022ScoreJC}, therefore we follow their pipeline, using a Voxel NeRF \cite{Chen2022TensoRFTR, Liu2020NeuralSV, Sitzmann2018DeepVoxelsLP, Sun2021DirectVG, Yu2021PlenoxelsRF} representation and a volumetric renderer. The image diffusion prior provides the gradient direction to update scene parameters $\theta$. 
\begin{equation}
    \nabla_\theta \mathcal{L}_\textrm{SDS} (\phi,g(\theta)) = \mathbb{E}_{t,\epsilon} \Bigl[ w(t)(\hat{\epsilon}(x_t, y, t) - \epsilon) \frac{\partial x}{\partial \theta}\Bigr]
\end{equation}
This process is repeated for randomly sampled camera poses, as the text-conditioned image diffusion prior pushes each rendered image towards high density regions in the data distribution. Intuitively, SDS ensures images rendered from all camera poses resembles an image generated by the text-conditioned diffusion prior. 


\paragraph{Bounding-box-guided text-to-3D synthesis.} To achieve text-to-3D scene generations that adhere to user input bounding boxes, our method takes the standard SDS-based pipeline and conditions the image diffusion prior with renderings of the input bounding boxes. Specifically, our method works as follows. First, a random camera pose is sampled and a volume rendering of the 3D scene model is generated, we call this image $x_t$. Using the same camera pose, a rendering of the bounding boxes is also generated, we call this image $m$. This image is a segmentation mask, where each pixel contains an integer value corresponding to a user input text prompt. The volume rendering is then passed in to the image diffusion prior which provides the necessary gradients for optimizing the 3D scene representation. However, instead of conditioning the image diffusion prior on a single text prompt, we generate denoising steps for all text prompts with corresponding bounding boxes visible from the sampled camera pose. We then selectively apply these denoising steps to the image based on the segmentation mask $m$, and backpropagate the gradients to the 3D scene as usual. This is equivalent to applying the noise estimator described in Eq. \ref{sem_eps} to the SDS gradient updates. 
\begin{equation}
    \nabla_\theta \mathcal{L}_\textrm{SDS} (\phi,g(\theta),m) = \mathbb{E}_{t,\epsilon} \Bigl[ w(t)(\hat{\boldsymbol{\epsilon}}_\phi(x_t, t, y_{1:P}, m) - \epsilon) \frac{\partial x}{\partial \theta}\Bigr]
\end{equation}
While previous SDS-based text-to-3D methods ensure all rendered views of the 3D scene lie in the high probability density regions in the image prior conditioned on a single text prompt, our method ensures that all rendered views also align with the rendered bounding box segmentation masks. An overview of our method is provided in Fig. \ref{fig:3d_pipeline}.
\paragraph{Object-centric camera pose sampling.} As discussed in prior work \cite{Poole2022DreamFusionTU, Lin2022Magic3DHT, Wang2022ScoreJC}, high classifier-free guidance weights are crucial for SDS methods to work. While image generation methods typically use guidance weights in the range of [2, 50], methods such as DreamFusion use guidance weights up to 100 \cite{Poole2022DreamFusionTU}. Using a high guidance scale leads to mode-seeking properties which is desirable in the context of SDS-based generation. However, mode-seeking properties in image diffusion priors have the tendency of generating images with the object at the center of the image. When applying high guidance weights to locally conditioned diffusion, it is possible for the resulting image to ignore semantic regions that are off center, since mode-seeking behaviour of the diffusion model expects the object described by the text prompt to be at the center of the image, while the semantic mask only applies gradients from off-centered regions. In the context of our method, this mode-seeking behavior causes off-centered bounding box regions to become empty. 


We combat this effect using \textit{object-centric camera pose sampling}. While existing works \cite{Poole2022DreamFusionTU, Lin2022Magic3DHT, Wang2022ScoreJC} sample camera poses that are always pointed at the origin of the 3D scene model, in our method, we randomly sample camera poses that point at the center of each object bounding box instead. This means that during optimization of the 3D scene, each bounding box region will have the chance at appearing at the center of the image diffusion prior. 



\paragraph{Locally conditioned diffusion with latent diffusion models.} Existing SDS-based methods, such as DreamFusion \cite{Poole2022DreamFusionTU} and Magic3D \cite{Lin2022Magic3DHT}, leverage image diffusion priors in their method \footnote{In Magic3D, a latent diffusion prior is also used, but the gradient of the encoder in the latent diffusion model is provided to convert gradient updates in the latent space back to the image space.}. While SJC \cite{Wang2022ScoreJC} uses a very similar methodology, their method actual employs a latent diffusion prior in the form of StableDiffusion \cite{Rombach2021HighResolutionIS}. Therefore, volume renderings of the 3D scene lies in the latent space instead of the image space. Note that previous work \cite{Park2022ShapeGuidedDW} has shown that the latent space is essentially a downsampled version of the image space, meaning we are still able to apply locally conditioned diffusion to the latent space.