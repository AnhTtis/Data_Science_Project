\section{Related work}

\paragraph{2D diffusion models.} Advances in large-scale 2D diffusion models trained on internet-scale data \cite{Dhariwal2021DiffusionMB, Nichol2021GLIDETP, Rombach2021HighResolutionIS, Radford2021LearningTV, Ramesh2022HierarchicalTI, Saharia2022PhotorealisticTD, Schuhmann2022LAION5BAO} have allowed generation of high-quality images that stay accurate to complex text prompts. While text-conditioned diffusion models excel at reproducing the semantics of a prompt, compositional information is usually ignored. Variants of existing methods \cite{Rombach2021HighResolutionIS} instead condition their models with semantic bounding boxes. This change allows greater control over the composition of the generated image. However, bounding-box-conditioned models must be trained with annotated image data \cite{Caesar2016COCOStuffTA}. These datasets are often much more limited in size, which restricts the diversity of the resulting diffusion model. Our locally conditioned diffusion approach leverages pre-trained text-conditioned 2D diffusion models to generate high-quality images with better compositional control without restricting the complexity of user-provided text-prompts.
\begin{figure*}[t!]
    \vspace{-1em}
    \centering
    \includegraphics[width=\linewidth]{figures/pipeline.pdf}
    \vspace{-1em}
    \caption{\textbf{Overview of our method.} We generate text-to-3D content using a score distillation sampling--based pipeline. A latent diffusion prior is used to optimize a Voxel NeRF representation of the 3D scene. The latent diffusion prior is conditioned on a bounding box rendering of the scene, where a noise estimation on the image is formed for every input text prompt, and denoising steps are applied based on the segmentation mask provided by the bounding box rendering.}
    \label{fig:3d_pipeline}
\end{figure*}
\paragraph{Compositional image generation.} Recent work found that Energy-Based Models (EBMs) \cite{Du2020CompositionalVG, Du2019ImplicitGA, LeCun2006ATO, Gao2020LearningEM, Grathwohl2020LearningTS} tend to struggle with composing multiple concepts into a single image \cite{Du2020CompositionalVG, Liu2022CompositionalVG}. Noting that EBMs and diffusion models are functionally similar, recent work improves the expressivity of diffusion by borrowing theory from EBMs. For example, \cite{Liu2022CompositionalVG} achieves this by composing gradients from denoisers conditioned on separate text-prompts in a manner similar to classifier-free guidance as proposed by \cite{Dhariwal2021DiffusionMB}. Existing work, such as Composable-Diffusion \cite{Liu2022CompositionalVG}, however, apply composition to the entire image, offering no control over the position and size of different concepts. Our locally conditioned diffusion approach selectively applies denoising steps over user-defined regions, providing increased compositional control for image synthesis while ensuring seamless transitions.

\paragraph{Text-to-3D diffusion models.} Recent advances in 2D diffusion models have motivated a class of methods for performing text-to-3D synthesis. Existing methods leverage 2D diffusion models trained on internet-scale data to achieve text-to-3D synthesis. Notably, DreamFusion \cite{Poole2022DreamFusionTU} with Imagen \cite{Saharia2022PhotorealisticTD}, Score Jacobian Chaining (SJC) \cite{Wang2022ScoreJC} with StableDiffusion \cite{Rombach2021HighResolutionIS} and Magic3D \cite{Lin2022Magic3DHT} with eDiff-I \cite{Balaji2022eDiffITD} and StableDiffusion \cite{Rombach2021HighResolutionIS}. Previous methods \cite{Poole2022DreamFusionTU, Wang2022ScoreJC, Lin2022Magic3DHT, Metzer2022LatentNeRFFS} perform 3D synthesis by denoising rendered views of a differentiable 3D representation. This process is coined Score Distillation Sampling (SDS) by the authors of DreamFusion \cite{Poole2022DreamFusionTU}. Intuitively, SDS ensures that all rendered views of the 3D representation resemble an image generated by the text-conditioned 2D diffusion model. Current methods are able to generate high quality 3D assets from complex text prompts. However, they are unable to create 3D scenes with specific compositions. Our proposed method enables explicit control over size and position of scene components.