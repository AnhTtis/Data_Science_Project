\section{Introduction}
\begin{figure*}[t!]
    \vspace{-1em}
    \centering
    \includegraphics[width=\linewidth]{figures/main_results.pdf}
    \vspace{-1em}
    \caption{\textbf{Results of our method.} Given user-input bounding boxes with corresponding text prompts, our method is able to generate high-quality 3D scenes that adhere to the desired layout with seamless transitions. Our locally conditioned diffusion method blends multiple objects into a single coherent scene, while simultaneously providing control over the size and position of individual scene components. Text prompts for bottom row (from left): (1) ``a lighthouse'' and (2) ``a beach''; (1) ``the Sydney Opera House'' and (2) ``a desert''; (1) ``a cherry blossom tree'' and (2) ``a lake''; (1) ``a small castle'' and (2) ``a field of red flowers''. Videos of our results can be found in the supplementary materials.}
    \label{fig:main_results}
    \vspace{-1em}
\end{figure*}

Traditionally, 3D scene modelling has been a time-consuming process exclusive to those with domain expertise. While a large bank of 3D assets exists in the public domain, it is quite rare to find a 3D scene that fits the userâ€™s exact specifications. For this reason, 3D designers often spend hours to days modelling individual 3D assets and composing them together into a scene. To bridge the gap between expert 3D designers and the average person, 3D generation should be made simple and intuitive while maintaining control over its elements (e.g., size and position of individual objects).

Recent work on 3D generative models has made progress towards making 3D scene modelling more accessible. 3D-aware generative adversarial networks (GANs) ~\cite{Liao2020unsupervised-3d-synthesis,Wu20163d-latent-gan,Nguyen-Phuoc2019hologan,Gadelha20173d-shape-induction,Niemeyer2021giraffe,Liu2020neural,chan2020pi, graf, Meng2021gnerf, Kosiorek2021nerf-vae, Rebain2022lolnerf,Chan2022eg3d,gu2021stylenerf,Zhou2021CIPS3D,Or-El2022style-sdf,zheng2022sdfstylegan} have shown promising results for 3D object synthesis, demonstrating elementary progress towards composing generated objects into scene~\cite{Niemeyer2020GIRAFFE,Xu2022DisCoSceneSD}. However, GANs are specific to an object category, limiting the diversity of results and making scene-level text-to-3D generation challenging. In contrast, text-to-3D generation~\cite{Poole2022DreamFusionTU, Lin2022Magic3DHT, Wang2022ScoreJC} using diffusion models can generate 3D assets from a wide variety of categories via text prompts~\cite{Balaji2022eDiffITD, Rombach2021HighResolutionIS, Saharia2022PhotorealisticTD}. Existing work leverages strong 2D image diffusion priors trained on internet-scale data, using a single text prompt to apply a global conditioning on rendered views of a differentiable scene representation. Such methods can generate high-quality object-centric generations but struggle to generate scenes with multiple distinct elements. Global conditioning also limits controllability, as user input is constrained to a single text prompt, providing no control over the layout of the generated scene.

We introduce locally conditioned diffusion, a method for compositional text-to-image generation using diffusion models. Taking an input segmentation mask with corresponding text prompts, our method selectively applies conditional diffusion steps to specified regions of the image, generating outputs that adhere to the user specified composition. We also achieve compositional text-to-3D scene generation by applying our method to a score distillation sampling--based text-to-3D generation pipeline. Our proposed method takes 3D bounding boxes and text prompts as input and generates coherent 3D scenes while providing control over size and positioning of individual assets. Specifically, our contributions are the following:
\begin{itemize}
    \item We introduce \textbf{locally conditioned diffusion}, a method that allows greater compositional control over existing 2D diffusion models.
    \item We introduce a method for compositional 3D synthesis by applying locally conditioned diffusion to a score distillation sampling--based 3D generative pipeline.
    \item We introduce key camera pose sampling strategies, crucial for compositional 3D generation.
\end{itemize}



