\section{Discussion and Conclusions}

Creating coherent 3D scenes is a challenging task that requires 3D design expertise and plenty of manual labor. Our method introduces a basic interface for creating 3D scenes without any knowledge of 3D design. Simply define bounding boxes for the desired scene components and fill in text prompts for what to generate in those regions.

\paragraph{Limitations and future work.} Although text-to-3D methods using SDS \cite{Poole2022DreamFusionTU, Wang2022ScoreJC, Lin2022Magic3DHT} have shown promising results, speed is still a limiting factor. While advances in image-diffusion-model sampling \cite{Lyu2022AcceleratingDM, Kong2021OnFS, Salimans2022ProgressiveDF, Watson2021LearningTE, Song2020DenoisingDI} have enabled the generation of high-quality results in dozens of denoising steps, SDS method still require thousands of iterations before a 3D scene can be learned. SDS-based methods are also limited by their reliance on unusually high guidance scales \cite{Poole2022DreamFusionTU}. A high guidance scale promotes mode-seeking, but leads to low diversity in the generated results. Concurrent works~\cite{BarTal2023MultiDiffusionFD, Jimnez2023MixtureOD} have shown other methods for controlling text-to-image diffusion synthesis with coarse segmentation masks. However, these methods require running a diffusion prior on multiple image patches before forming a single image, greatly increasing time needed to generate a single denoising step. In theory, these works could be applied in combination with our method, albeit with greatly increased time needed to generate a single 3D scene.

\paragraph{Ethical considerations.} Generative models, such as ours, can potentially be used for spreading disinformation. Such misuses pose a societal threat and the authors of this paper do not condone such behavior. Since our method leverages StableDiffusion \cite{Rombach2021HighResolutionIS} as an image prior, it may also inherit any biases and limitations found in the 2D diffusion model.

\paragraph{Conclusion.} Text-to-3D synthesis has recently seen promising advances. However, these methods mostly specialize in object-centric generations. Our method is an exciting step forward for 3D scene generation. Designing a 3D scene with multiple components no longer requires 3D modeling expertise. Instead, by defining a few bounding boxes and text prompts, our method can generate coherent 3D scenes that fit the input specifications.