\section{Diffusion preliminaries}
Recent work has shown that diffusion models can achieve state-of-the-art quality for image generation tasks~\cite{Dhariwal2021DiffusionMB}. Specifically, Denoising Diffusion Probabilistic Models (DDPMs) implement image synthesis as a denoising process. DDPMs begin from sampled Gaussian noise $x_T$ and apply $T$ denoising steps to create a final image $x_0$. 
The forward diffusion process $q$ is modelled as a Markov chain that gradually adds Gaussian noise to a ground truth image according to a predetermined variance schedule $\beta_1, \beta_2, \dots, \beta_T$
\begin{equation}
    q(x_t | x_{t-1}) = \mathcal{N} \left(x_t ; \sqrt{1-\beta_t}x_{t-1} , \beta_t \mathbf{I} \right)
\end{equation}
The goal of DDPMs is to train a diffusion model to revert the forward process. Specifically, a function approximator $\boldsymbol{\epsilon}_\phi$ is trained to predict the noise $\boldsymbol{\epsilon}$ contained in a noisy image $x_t$ at step $t$. $\boldsymbol{\epsilon}_\phi$ is typically represented as a convolutional neural network characterised by its parameters $\phi$. Most successful models \cite{Dhariwal2021DiffusionMB, Ho2020DenoisingDP, Saharia2021ImageSV} train their models using a simplified variant of the variational lower bound on the data distribution:
\begin{equation}
    \mathcal{L}_\textrm{DDPM} \! = \! \mathbb{E}_{t,x, \boldsymbol{\epsilon}} \left[ \left\| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\phi \left(x_t, t \right) \right\|^2 \right]\!\!
    \label{eq:loss}
\end{equation}
with $t$ uniformly sampled from $\{1,\dots,T\}$.
The resulting update step for obtaining a sample for $x_{t-1}$ from $x_t$ is then
\begin{equation}
    x_{t-1} = x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\phi(x_t,t) + \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t \mathcal{N}(0,\mathbf{I})
\end{equation}
where $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$, $\alpha_t = 1-\beta_t$. 

Text-to-image diffusion models build upon the above theory to introduce conditional diffusion processes using classifier-free guidance \cite{Ho2022ClassifierFreeDG}. Given a condition $y$, usually represented as a text prompt, a diffusion model $\boldsymbol{\epsilon}_\phi(x_t, t, y)$ is trained to predict noise in an image as shown in Eq. \ref{eq:loss}. During training, conditioning $y$ is randomly dropped out, leaving the diffusion model to predict noise without it. At inference, noise prediction is instead represented by:
\begin{equation}
    \hat{\boldsymbol{\epsilon}}_\phi(x_t, t, y) = \boldsymbol{\epsilon}_\phi(x_t, t, \emptyset) + s\Bigl(\boldsymbol{\epsilon}_\phi(x_t, t, y) - \boldsymbol{\epsilon}_\phi(x_t, t, \emptyset)\Bigr)
\end{equation}
Where $s$ is a user-defined constant controlling the degree of guidance and $\boldsymbol{\epsilon}(x_t, t, \emptyset)$ represents the noise prediction without conditioning.