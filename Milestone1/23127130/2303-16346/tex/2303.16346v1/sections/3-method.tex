\section{Method}
\label{methods}
Our goal is to \change{explore the potential of using a commercial eye tracker to collect high-quality gaze data from low vision people. With suitable eye tracking process, we further investigate low vision people's unique gaze patterns compared to sighted people as well as their different gaze behaviors due to different visual conditions and magnification modes. We thus design a study to assess four groups of hypotheses based on the four research questions in Introduction:}
\begin{itemize}[leftmargin=2em]
    \item[\textbf{H1}] \textbf{With improved calibration and data collection process, the commercial eye tracker can achieve comparable data quality from low vision people and sighted people.}

    
    \begin{itemize}[align=left, leftmargin=0.25cm]
        \item[\change{H1.1}] \change{There is no difference in the gaze recognition accuracy between low vision and sighted people.}
        \item[\change{H1.2}] \change{There is no difference in the percentage of data loss between low vision and sighted people.}
    \end{itemize}

    
    \item[\textbf{H2}] \textbf{Low vision people have different gaze behaviors from sighted people when reading.}

    
    \begin{itemize}[align=left, leftmargin=0.25cm]
        \item[\change{H2.1}] \change{Low vision people show different fixation patterns from sighted people.}
        \item[\change{H2.2}] \change{Low vision people show different saccade patterns from sighted people.}
        \item[\change{H2.3}] \change{Low vision group shows different line searching behaviors from sighted group.}
    \end{itemize}

    
    \item[\textbf{H3}] \textbf{Different visual abilities (i.e., visual acuity, visual field) affect low vision people's gaze behaviors differently.}

    
    \begin{itemize}[align=left, leftmargin=0.25cm]
        \item[\change{H3.1}] \change{Different visual abilities affect low vision people's fixation patterns differently.}
        \item[\change{H3.2}] \change{Different visual abilities affect low vision people's saccade patterns differently.}
        \item[\change{H3.3}] \change{Different visual abilities affect low vision people's line searching behaviors differently.}
    \end{itemize}

    
    \item[\textbf{H4}] \textbf{Different screen magnification modes (i.e., enlarged font, lens magnifier, full-screen magnifier) affect low vision people's gaze behaviors in reading differently.}

    
    \begin{itemize}[align=left, leftmargin=0.25cm]
        \item[\change{H4.1}] \change{Different magnification modes affect low vision people's fixation patterns differently.}
        \item[\change{H4.2}] \change{Different magnification modes affect low vision people's saccade patterns differently.}
       % \item[\change{H4.3}] \change{The size of the lens magnifier is correlated with the saccade patterns of low vision group.}
        \item[\change{H4.3}] \change{Different magnification modes affect low vision people's line searching behaviors differently.}
     %   \item[\change{H4.5}] \change{The size of the lens magnifier is correlated with the line searching behaviors of low vision group.}
    \end{itemize}
    
\end{itemize}



\subsection{Participants}
We recruited \change{20 low vision and 20 sighted participants to compare their gaze patterns during reading tasks.} Our low vision participants included \change{14 female and 6 male, whose ages ranged from 19 to 86 ($Mean=58.3, SD=22.1$)}. \change{Seven} participants were legally blind. Their visual conditions are detailed in Table \ref{tab:lv_dem}. \change{ Besides low vision, participants did not have any other health conditions that may cause reading difficulties.} \change{We recruited low vision participants from a local low vision clinic. We also posted recruitment messages on the student jobs website of our university.} 
When a potential low vision participant contacted the research team, we conducted a brief interview via phone, email, or text to ensure they were eligible to our study. A participant was eligible if they were at least 18 years old and had low vision. \change{Only one participant (Hannah) wore glasses during the study.} Participants were compensated \$20 per hour and were reimbursed for travel expenses. 

Our sighted participants included \change{7 female and 13 male, with ages ranging from 21 to 51 ($Mean=31.1, SD=9.5$)}. All participants' visual acuity in the better eye was no worse than 20/40. \change{Three of them wore eye glasses, and two wore contact lenses.} %They were recruited via mailing list and student job posts of our university. 
They were compensated \$10 per hour for participating in the study.

\input{sections/table_demo.tex}

\subsection{\change{Improved Gaze Calibration and Collection Process}} % Apparatus?
\label{sec: calibration}
% \yuhang{need a general introduction of the study environment, e.g., a well-lit environment? sit in front of a xxx resoluation screen with an Tobbi Pro Fusion eye tracker attached at the bottom of the screen. Also add a figure of a participant conducting the study.}


We used a Tobii Pro Fusion (120Hz) eye tracker~\cite{tobiiprofusion} in the study. To better collect low vision people's data, we refined the gaze calibration and collection process as below.

\subsubsection{\change{Adjustable} Calibration Interface \change{for Low Vision}.} To collect gaze data more accurately, a calibration is needed to mitigate individual differences. %The calibration interface of TPL and ours are shown in Figure\ref{fig:system}a, b and c. 
Tobii Pro Lab (TPL, Tobii's default calibration interface)~\cite{tobiiprolab} adopts a 9-point calibration process: nine targets (white solid circle with a black dot in the center) are shown one by one on the screen and users are asked to fixate on each target until it disappears (Fig. \ref{fig:cali_ui}a). A 4-point validation process is then conducted by asking the user to fixate at another four targets to evaluate the calibration (Fig. \ref{fig:cali_ui}b). 
Although TPL provides some customization to the target color and background color, the target size is always fixed at 36px. 

We refined TPL's calibration interface to make it more usable to low vision people. We kept the high contrast setting (white target on black screen) and further made the target size adjustable (Fig. \ref{fig:cali_ui}c-d), because a small target may not be visible to participants with low visual acuity. \change{The target size ranges from 36px to 256px. With this interface, we can enlarge the targets to a visible size for low vision participants to facilitate a more accurate calibration.} %When a target showed up on the screen, participants were asked to press the SPACE key on the keyboard to start data collection for the target, and keep focusing on the center of the target until the position of the target changed. Participants were instructed to try their best to keep their eyes wide open during the data collection. 

%Similar to TLP, we conducted 9-point calibration (Figure\ref{fig:system}a) and 4-point validation (Figure\ref{fig:system}b).

\begin{figure*}[h]
  \includegraphics[width=0.7\textwidth]{images/cali_val_ui_rev.pdf}
  \caption{The gaze calibration and validation interfaces. (a) The calibration interface of Tobii Pro Lab (TPL); (b) The validation interface of TPL; (c) Our adjustable calibration and (d) validation interface.}
  \label{fig:cali_ui}
  \Description{Four screenshots labeled (a), (b), (c) and (d) show the gaze calibration and validation interfaces. Image (a) shows the 9-point calibration interface of TPL with 9 calibration targets (target size: 36px) distributed evenly across the screen. (b) shows the 4-point validation interface of TPL with 4 validation targets (target size: 36px) distributed evenly across the middle 40\% area of the screen. (c) shows our adjustable 9-point calibration interface with 9 calibration targets (target size: 126px in the image, adjustable) distributed evenly across the screen. (d) shows our adjustable 4-point validation interface with 4 validation targets (target size: 126px in the image, adjustable) distributed evenly across the middle 40\% area of the screen.}
    \vspace{-2ex}
\end{figure*}

\subsubsection{\change{Dominant-eye-based} Data Collection.} 
\change{Conventional gaze collection focused on binocular data collection (i.e., mean gaze position of the two eyes) since sighted people usually show same gaze behaviors in the two eyes. However, some low vision people have distinct visual abilities and thus inconsistent gaze behaviors across the two eyes. We thus collected low vision people's gaze data by considering their dominant eye. This was inspired by Maus et al.~\cite{maus2020gaze}, where they collected low vision participants' gaze data based on two eyes, resulting in high data loss. They thus recommended using monocular data collection to mitigate this issue. 
% If participants reported to have a dominant eye due to severe vision loss in the other eye, we used the data of their dominant eye for calibration and data collection. 
We collected a participant's gaze from the dominant eye if (1) the participant indicated that they had a dominant eye, and (2) they demonstrated inconsistent gaze behaviors in two eyes (e.g., not looking at the same direction) or one of their eyes was not trackable (e.g., unrecognizable pupil).}
Otherwise, we used binocular data collection, which is the same as the method for sighted participants. %Since sighted people commonly have consistent gaze behaviors in the two eyes, we used the mean gaze position of two eyes.   

\subsection{Apparatus: \change{Study Interfaces and Environment}}
Our study was conducted in a well-lit lab. Participants were seated in front of a computer display (24-inch, 1920x1200 resolution) with a Tobii eye tracker attached at the bottom of the screen (Figure \ref{fig:setting}). 
We built a web-based interface using React~\cite{react}, which included the improved gaze calibration interface for low vision participants (section \ref{sec: calibration}) and a reading task interface with different magnification modes to collect gaze data. We used the Tobii Pro SDK~\cite{tobiiprosdk} in Python to retrieve gaze data from the eye tracker. To enable communication between the eye tracker and the reading interface, we built a Flask~\cite{flask} server in Python.

\subsubsection{Reading Task Interface} 
We implemented a reading task interface to collect participants' gaze data and audio data when they were reading aloud. We presented the reading materials in San Francisco font (Sans-serif). By default, the font size was 12pt (16px). The text spacing was set based on the Web Content Accessibility Guidelines (WCAG) 2.1 ~\cite{wcag}. 

For low vision participants, the interface provided three magnification modes, including the regular mode with increased font size (Fig. \ref{fig:reading_ui}a), the lens magnifier (Fig. \ref{fig:reading_ui}b), and the full-screen magnifier (Fig. \ref{fig:reading_ui}c). 

 %The maximum line width was 75 times the font size for the regular mode. 

The regular mode was the same as the reading interface presented to sighted participants, but low vision participants were allowed to customize the font size (up to 16 times of default font size) \change{to ensure the readability of the reading content.}
The lens magnifier (Fig. \ref{fig:reading_ui}b) and full-screen magnifier (Fig. \ref{fig:reading_ui}c) \change{simulated commonly-used screen magnifiers embeded in} Windows 10~\cite{winmag}. The lens magnifier allowed the user to magnify \change{a rectangular area of the screen around the mouse cursor; the user could adjust the width and height of the lens magnifier and move the mouse to magnify different areas of the screen}. The full-screen magnifier magnified the whole screen; the user was allowed to pan around with their mouse to reveal different areas of the screen. The user could adjust the zoom level for both magnifiers. For all three magnification modes, low vision participants could also adjust the font weight (bold vs. regular) and text color (white-on-black vs. black-on-white) to \change{simulate the reading setup they used in daily life} (Fig. \ref{fig:reading_ui}d). 

\subsubsection{Visual Function Tests}
\label{sec:vision test}
To identify participants' visual ability, we tested their visual acuity and field of view.

\textbf{Visual acuity test.} We printed a letter-size ETDRS 1 and ETDRS 2 logMAR chart~\cite{ferris1982new} to test the visual acuity of participants' left eye and right eye, respectively. The logMAR chart measures visual acuity (ranges from 20/8 to 20/160 at 5-feet distance) by showing rows of letters in decreasing size. 

\textbf{Visual field test.} We also built a visual field test interface to roughly identify the areas of vision loss for low vision participants. \change{The test area was limited to the screen size. Although this was not a standard visual field test that covered participants' full field of view, it indicated the influence of low vision participants' visual field loss on their ability to perform visual tasks on the screen.}
The design was based on the Octopus perimeter~\cite{racette2016visual} but simplified. Participants were instructed to look straight ahead on a fixation target in the center of the screen all the time with both eyes open (Fig. \ref{fig:reading_ui}e). 
During the test, visual stimuli appeared randomly anywhere on the screen; one stimulus appeared at a time. Participants were asked to press the SPACE key as soon as they noticed a stimulus in their peripheral visual field. 

The stimulus was a flash of white solid circle on the screen that lasted 0.1s.
\change{The diameter of the stimulus was 1.7\textdegree~ viewed at 65cm from the screen, which was the standard stimulus size for low vision people in Octopus perimeter. 
The stimuli spanned a 40\textdegree~ $\times$ 20\textdegree~ area viewed at 65cm from the screen, thus including central vision and part of near peripheral vision. 
The stimuli were evenly distributed on a 5\textdegree~ spacing grid, resulting in 45 stimuli (9 $\times$ 5) in total.}



\begin{figure*}[h]
  \includegraphics[width=\textwidth]{images/reading_interface_rev2x.png}
  \caption{The reading interface and visual field test: (a) Regular mode with increased font size; (b) Lens magnifier; (c) Full-screen magnifier; (d) Adjustable font weight and color for low vision participants in reading tasks; (e) The visual field test interface. }
  \label{fig:reading_ui}
  \Description{Five screenshots labeled (a), (b), (c), (d) and (e) show the reading task and visual field test interfaces. Image (a) shows an example of the regular mode (a text window with large font in the middle of the screen). (b) shows an example of the lens magnifier (the rectangular window magnifies the text underneath it). (c) shows an example of the full-screen magnifier (the whole text is magnified and exceeds the screen). (d) shows examples of the adjustable font weight and color for low vision participants in reading tasks. From left to right, it shows the regular font style, bold font, and inverted color (white-on-black). (e) shows the visual field test interface with a fixation target in the center of the screen and a stimulus of size 73px near the bottom of the screen.}
    \vspace{-2ex}
\end{figure*}



\subsection{Procedure}
We conducted a single-session study that lasted 1 to 2 hours. Participants were invited to our lab for the study. \change{The study included four phases below:}

\change{\textbf{\textit{Initial Interview \& Visual Acuity Test.}}} After obtaining the participants' consent, we conducted a brief semi-structured interview. For all participants, we asked about their demographics. For low vision participants specifically, we asked about their visual condition, dominant eye, experience with assistive technology, and any adaptations to their vision on their daily devices. 
We then measured their visual acuity (both with and without correction if they wore eyeglasses) using the ETDRS eye charts (Section \ref{sec:vision test}). Participants sat at 5 feet in front of the chart and read from the top line on the chart. They were asked to read the letters on chart 1 with left eye covered then read chart 2 with right eye covered. \change{We regarded a line as visible to a participant if they correctly identified more than three out of five letters on that line. We recorded the lowest visible line for each participant.} %We then conducted gaze calibration with our improved interface and collected participants' gaze data in reading tasks.  

\textbf{\textit{Gaze Calibration and Validation.}}
\change{We conducted gaze calibration for participants with our improved calibration interface (Section \ref{sec: calibration}).} Participants were instructed to sit in a chair in front of a computer monitor. We asked them to sit straight with their back touching the back of the chair to keep a stable position throughout the study. Participants adjusted their position so that the horizontal distance between their eyes and the monitor was about 65cm---the optimal distance for the eye tracker to work. We adjusted the height of the chair so that participants' eye level was at the center of the screen. After reaching the suitable position, we asked participants to try their best to keep their head and body still across the tasks (Fig \ref{fig:setting}). %To simulate everyday reading scenario, we didn't use a chin rest. 
We increased the calibration target size for low vision participants until they were able to see the center of the target without squinting, while sighted participants used the default target size. Participants completed a 9-point calibration and 4-point validation \change{under the instructions from the research team}. We asked participants to repeat the calibration if the validation result was larger than one degree or if the research team noticed them not focusing on the targets \change{or blinking} during data collection. %Since eye glasses could significantly affect the data collection, we asked participants to not wear their glasses if they could. Only one participant performed the tasks with glasses due to discomfort caused by reading without glasses.

\begin{figure}[h]
  \includegraphics[width=0.3\textwidth]{images/study_setting.png}
  \caption{A low vision participant performing reading task with full-screen magnifier in front of a screen}
  \label{fig:setting}
  \Description{The figure shows a low vision participant performing reading task with large bold font and inverted color using the full-screen magnifier on a screen with eye tracker mounted at the bottom.}
    \vspace{-2ex}
\end{figure}

\textbf{\textit{Gaze Collection during Reading.}}
After calibration, participants were asked to complete multiple trials of reading tasks in front of the eye tracker. Both sighted and low vision participants read two passages in the \textbf{regular mode} (Fig. \ref{fig:reading_ui}a). 
\change{While all sighted participants could read the default font comfortably, the text was not readable to all low vision participants. As such, to stimulate participants' natural gaze behaviors, we allowed low vision participants to adjust the font size, weight, and color to the settings they used in daily life. To explore the effect of different screen magnifiers on low vision participants' gaze patterns, we also asked them to read in the other two screen magnification modes (two passages per condition): the \textbf{lens magnifier} and \textbf{full-screen magnifier}. Participants were also allowed to adjust the window size in the lens magnifier to their preferred size. 
We counterbalanced the order of the three reading modes across low vision participants using Latin Square~\cite{bradley1958complete}.
Four low vision participants did not need magnification due to relatively good visual acuity, they thus only read in the regular mode with default font size (16px) like sighted participants.}

%Since low vision participants used different screen magnification modes, they read in three magnification conditions:  For lens magnifier, we also asked them to adjust the window size until they felt comfortable using the tool. Participants were asked to read two passages with each magnification mode. The order of the three conditions was counterbalanced 

%To enable low vision participants to comfortable , before reading the passages, we asked them to adjust the font size, weight, and color until they were able to read comfortably \change{to better simulate their natural gaze behaviors in daily reading, minimizing the gaze behavior changes caused by inaccessible reading materials.} If they required larger font size, we asked them to read in three modes:  Before switching to each condition, we let participants use the magnification tool until they feel comfortable reading with it. %and move to the next passage when they are ready.
%While low vision participants read six passages with different magnification tools, sighted participants read two standard passages with default font size (16px). 

%two passages in the regular mode. All sighted participants read with the default font size (16px). while we increased the font size for low vision participants to make the text readable.  
%For low vision participants who preferred larger font size, we asked them to read six passages, two for each magnification mode, with increased font size and appropriate magnification level. 

We collected participants' gaze data in each reading task. Before formal data collection, we also asked participants to practice with each magnification mode until they felt comfortable reading with it.
%This task is to understand low vision people's gaze behavior and challenges during reading. 
Since we were interested in participants' gaze behaviors at the word and sentence level (i.e., whether participants can locate and recognize each word successfully ~\cite{jarodzka2017tracking}), we instructed them to read aloud the passages to match with the eye movement data. Participants were asked to read as accurately and quickly as possible.
%so that comprehension of the whole text is minimized. 

We selected six passages with the similar difficulty level from CLEAR corpus, a corpus including about 5000 text excerpts for readability assessment~\cite{crossley2022large}. The corpus provided readability score for each text excerpt. We selected passages as follows: we first gathered the excerpts in sixth grade level reading difficulty using Flesch Reading Ease~\cite{flesch1948new}. We then calculated the cosine similarity of each excerpt based on their Flesch-Kincaid Grade Level~\cite{kincaid1975derivation}, Automated Readability Index~\cite{kincaid1975derivation}, SMOG~\cite{mc1969smog}, and Google Word Count, resulting in 26 similar-difficulty excerpts. Among them, we selected 12 most similar passages with neutral content (non-politically sensitive) verified by our research team. The mean word count of all selected passages was 185.1 ($SD=8.1$). We then selected six passages as the default passages in our study and the presentation order was randomized. The rest passages were used as back up passages to handle particular situations, such as data collection failures due to system errors.



\textbf{\textit{Exit Interview \& Visual Field Test.}}
We ended our study with an interview, asking about participants' reading experiences based on our observation, for example, their difficulty of recognizing specific words, how they tracked the line they were reading, and how they located the next line. We also probed suggestions for improving their reading experiences. Lastly, we tested low vision participants' on-screen field of view with our field of view test interface (Fig. \ref{fig:reading_ui}e). We conducted this test by the end because the visual field test could potentially strain participants' eyes, affecting their reading performance. Fig. \ref{fig:field} shows the test results of the 10 participants with limited visual field. %who identified to have vision loss do a visual field test.

\begin{figure*}[h]
  \includegraphics[width=\textwidth]{images/vis_field_revision.pdf}
  \caption{\change{Visual field test result of 10 participants who had limited visual field}}
  \label{fig:field}
  \Description{
This figure shows the visual field test result of participants who had limited visual field. On the first row, from left to right, it shows the visual field test result of Judy, Mark, Robin, Lucy and Hannah. On the second row, from left to right, it shows the visual field test result of May, Piper, Danilo, Marilyn and Fiona. Each result image shows the position of the stimuli that participants were able to see during the test.
}
    \vspace{-2ex}
\end{figure*}

\subsection{Analysis}
Our study collected both quantitative and qualitative data. \change{We first describe our quantitative analysis for the four groups of hypotheses and then the qualitative analysis method used for the interview data.}

\subsubsection{\change{Assessing gaze data quality between low vision and sighted people (H1).}} 
\label{h1}
%we collected participants' eye tracking calibration and validation data. During the reading tasks, we collected their eye movement data, mouse cursor movement data, and audio data when they read aloud.
We first \change{validated the calibration and assessed the quality of the data collected from the eye tracker}. We had two measures: (1) \textbf{\textit{gaze recognition accuracy in four-point validation}}, the mean visual angle difference between the target position and the estimated gaze location \change{in the four-point validation (Fig. \ref{fig:cali_ui}d), which was used to validate the calibration~\cite{tobiiproacc}}, and (2) \textbf{\textit{data loss}}, \change{the ratio of the number of invalid gaze points due to eye recognition failure to the total number of gaze points we collected in each recording.}

We \change{used sighted people's data as the standard} and compared the two measures between low vision and sighted people. \change{We thus had one between subject factor \textit{\textbf{Vision}} with two levels---\textit{Sighted} and \textit{LowVision}. Since neither of the measures was normally distributed based on the Shapiro-Wilk test, we used a one-way Aligned Rank Transform for Nonparametric Factorial ANOVA (ART)~\cite{wobbrock2011aligned} to model the impact of \textit{Vision} on both gaze recognition accuracy and data loss.}
%(low vision: $p = 0.07$; sighted: $p = 0.97$), 
%(validation accuracy: $W = 0.56$, $p < 0.001$; data loss: $W = 0.55$, p < 0.001, Shapiro-Wilk test)}. 
Moreover, we aligned participant's audio data and gaze data by matching the data timestamp, in order to confirm whether the collected gaze behaviors matched participants' reading behaviors. 

\subsubsection{\change{Comparing gaze behaviors between low vision and sighted people (H2).}} 
\label{h2}
We then analyzed low vision and sighted participants' gaze data during the reading tasks. We used REMoDNaV~\cite{dar2021remodnav}, an eye-movement \change{event} classification library to recognize eye-movement events. We specify the measures used in our analysis:

(1) \textbf{Fixations} are the pauses over informative regions of interest~\cite{salvucci2000identifying} where eyes are relatively stationary. We measure participants' \textbf{fixation number} and \textbf{mean fixation duration} during reading. 

(2) \textbf{Saccades} are the rapid, ballistic movements between fixations \cite{salvucci2000identifying, verghese2021eye}. We investigate different types of saccades, including \textbf{forward saccades} (i.e., left-to-right saccades when reading onwards) and \textbf{regressive saccades} (i.e., right-to-left saccades that bring eyes back to previously read content). We measure both \textbf{saccade number} and \textbf{saccade length} to understand participants' scanning and revisiting behaviors. Since low vision participants read in different font sizes, we normalize the saccade length by dividing \change{the angular length of a saccade with the angular width of the font based on} the font size selected by each participant. 

(3) \textbf{Revisitation} describes the behavior that a participant revisits prior content during reading. Since it is a corrective movement for a long forward saccade to send the eyes to the accurate position of a piece of text, we define a revisitation pattern as a regressive saccade following a forward saccade and a fixation. We then define \textbf{revisitation rate} as the number of revisitations divided by the total number of forward saccades, and use it to evaluate the frequency that participants corrected their visual scanning.

(4) We measure a participant's line switching behaviors via \textbf{the mean number of searched lines}, meaning the average number of lines a participant searches before they successfully locate the correct next line in a passage. Suppose $N$ denotes the total number of lines in a passage and $nLS_{i}$ represents the number of searched lines when switching to the $i^{th}$ line. We define the mean number of searched lines as $\frac{1}{N-1}\sum_{i=2}^N nLS_i $.  


 %We trimmed the silence at the beginning and the end of each audio file to identify the valid range for analysis. 

  %In the analysis, we used fixations, saccades, and smooth pursuits identified with REMoDNaV.
%To reasonably recognize the saccades, we changed the maximum saccade frequency for initial classification of major saccades to 4Hz to fit our reading data. For some pieces of data, we increase the noise factor to 10 and 20 to reduce the false classifications due to noise.

\change{We compared low vision and sighted participants' gaze behaviors. Given that screen magnifiers can largely change low vision participants' reading behaviors, our comparison only focused on the regular mode since the interaction involved in this mode is similar to sighted people's reading experience. For line switching analysis, we removed one participant's (Danilo) data because he chose the maximum font size (256px), so that most lines only contain one word, making it unnecessary to hesitate between lines when locating the next line. %However, for low vision participants, vertical scrolling was needed to finish the passages if the length of the text exceeded the boundary of the text box due to increased font size.  
We had one between subject factor \textit{\textbf{Vision}} (\textit{Sighted vs. LowVision}). We validated the counterbalancing by involving another within subject factor \textbf{\textit{Order}}. We checked the normality of the different measures using Shapiro-Wilk test. 
If a measure was normally distributed, we used ANOVA for analysis and Tukey's HSD for \textit{post-hoc} comparison if significance was found. 
Otherwise, we used ART and ART contrast test for \textit{post-hoc} comparison~\cite{elkin2021aligned}. We used partial eta square ($\eta_{p}^2$) to calculate effect size for ART and ANOVA, with $0.01$, $0.06$, $0.14$ representing the thresholds of small, medium and large effects, respectively~\cite{cohen2013statistical}.}

%If \textit{Order} had no significant effect on the measure, we removed \textit{Order} and used the same test for analysis.

%\textbf{Low vision vs. typical vision (H2).} We compared the gaze behaviors between low vision and sighted participants. \change{We found no significant effect of \textit{Order} on any of the measures.
% (duration: $F_{(1,38)}=0.57$, $p=0.45$; number of fixation: $F_{(1,38)}=1.24$, $p=0.27$). 
%Therefore, we removed \textit{Order} in the analysis.}




\subsubsection{\change{Investigating the effect of visual abilities on low vision people's gaze behaviors (H3).}} We explored the effect of low vision participants' visual conditions (i.e., visual acuity, visual field) on their gaze behaviors \change{when reading in the regular mode}. The measures were the same as in Section \ref{h2}. For the same reason described in Section \ref{h2}, we removed Danilo's data in line switching analysis.
We had two between subject factors, \textit{\textbf{VisualAcuity}} and \textit{\textbf{VisualField}}. VisualAcuity had two levels---\textit{Low, High}---with \change{20/100} in the better eye as the splitting threshold~\cite{zhao2017understanding}. VisualField had two levels---\textit{\change{Limited}, Intact}---based on our visual field test. \change{We also involved a within subject factor \textbf{\textit{Order}} in our model to validate the counterbalancing. With similar analysis methods in Section \ref{h2}, we found no significant effect of Order on any of the measures.}
% We used a \change{ANOVA or ART (depending on the normality of the measures)} to analyze the effect of VisualAcuity and VisualField on different measures. \change{Similarly, we first involved \textit{Order} into the model, and found order had significant effect on the number of regressive saccades. Therefore, we kept order in the model for this measure, and removed \textit{Order} for all other measures in the subsequent analysis}. We conducted \textit{post-hoc} \change{contrast test or} Tukey's HSD if interaction between factors was found.
%\change{We found no significant effect of \textit{Order} on any of the measures. Therefore, we removed \textit{Order} in the analysis.}

\subsubsection{\change{Investigating the effect of magnification modes on low vision people's gaze behaviors (H4).}} We finally investigated low vision participants' gaze behaviors under different magnification modes. \change{Since not all low vision participants used screen magnifiers in our study (e.g., some did not need magnification), we focused on those (14 participants) who used all three magnification modes.} Besides the aforementioned measures, we had another measure, \textbf{Smooth pursuit}, which refers to the slower tracking movement of the eyes to keep a moving target in the center of visual field. It happens when a low vision participant follows the lens magnifier, \change{follows a word when scrolling the text up or down in the regular mode}, and tracks a word when they pan around with the full-screen magnifier. We measured the \textbf{smooth pursuit number} \change{and \textbf{smooth pursuit duration}} to evaluate low vision participants' gaze-following behaviors.


We had two within subject factors: \textit{\textbf{MagnificationMode}} (three levels: \textit{Regular, Lens, FullScreen}) and \textbf{\textit{Order}}. We conducted analysis with the similar methods in Section \ref{h2}. However, 
% We used a \change{repeated measure model of ANOVA or ART (depending on the normality of measures)} to study the effect of magnification mode on different measures. 
\change{we found that Order had a significant effect on fixation duration (ART: $F_{(5,55)} = 2.63$, $p = 0.03$, $\eta^2_{p} = 0.19$) and smooth pursuit number (ART: $F_{(5,55)} = 3.20$, $p = 0.01$, $\eta^2_{p} = 0.23$). As Order increased, participants demonstrated shorter fixation duration and more smooth pursuits, indicating that they processed less information per fixation and tracked more frequently when moving the magnifiers. This could be explained by the fatigue accumulated along the study.}
% If significance was found, we further conducted a \textit{post-hoc} \change{contrast test or Tukey's HSD} depending on the normality of the data to compare the difference between each MagnificationMode level. 

%\change{Note that a significant effect of order was found on the the fixation duration (). However, a \textit{post-hoc} contrast test showed no significant difference between each pair of orders}

%\change{Besides, we found order also had significant effect on the number of smooth pursuits (). Participants demonstrated less smooth pursuits when reading passage \#1 ($t_{(55)} = -3.10$, $p = 0.03$) and \#2 ($t_{(55)} = -3.25$, $p = 0.02$) than passage \#5. This could be explained by the fatigue accumulated during the study.}
 
 %Appropriate non-parametric tests were used when necessary.

%\yuhang{need more details about what's your measure, what's the independent variables; what tests you run for what reason. Refer to the Analysis Section in this paper}

\subsubsection{Qualitative analysis.} We video-recorded the initial and exit interviews and transcribed the video using an online automatic transcription service. One researcher cleaned the transcript by manually correcting the auto-transcription errors. We analyzed the data using a standard qualitative analysis method \cite{saldana2021coding}.  We developed codes using open coding. Two researchers independently coded three sample transcripts from three participants. After comparing and discussing the codes, a codebook was generated upon the two researchers' agreement. Each researcher then coded half of the remaining transcripts based on the codebook. The researchers updated the codebook upon agreement if new code emerged. We derived themes according to participants' challenges and strategies during reading.
