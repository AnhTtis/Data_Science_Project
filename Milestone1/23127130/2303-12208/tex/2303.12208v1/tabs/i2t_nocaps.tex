\bgroup
\def\arraystretch{1.05}%
\begin{table}[t]
\centering
\small
\begin{tabular}{lcc}
\hline
Model & CIDEr & SPICE
\\ \hline
\rowcolor[gray]{0.85}\multicolumn{3}{l}{\textit{\textbf{with MS-COCO in training}}} \\ 
%FewVLM$_{base}$ \cite{jin2021good} & - & - & - & - & - & - & 42.2& 8.5 \\
FewVLM$_{\text{large}}$ (740M)\cite{jin2021good} & 47.7& {\bf 9.1} \\
MetaLM (545M) % from section 4.2. VL encoder (192M) + LM (GPT2 w/ 24-layer, 353M) 
\cite{hao2022language} 
& {\bf 58.7}& 8.6 \\
% SimVLM$_{base}$  \cite{wang2021simvlm} & 83.2 & - & 84.1& - & 82.5& - &  83.5& - \\
% SimVLM$_{large}$ \cite{wang2021simvlm}  & 97.6& - & 96.5& - & 96.3& - &  96.6& - \\
% SimVLM$_{huge}$ \cite{wang2021simvlm}  & 101.2& - & 100.4& - & 102.3& - &  101.4& - \\
\hline
\rowcolor[gray]{0.85}\multicolumn{3}{l}{\textit{\textbf{without MS-COCO in training}}} \\ 
VLKD$_{\text{RN50x16}} $ (406M)\cite{dai2022enabling} & 54.0 & {\bf 9.6} \\
SimVLM$_{\text{huge}}$ (632M)\cite{wang2021simvlm} & {\bf 101.4} & -\\
ARGVLT (\textit{I2T only}) (447M) & 34.8& 6.5 \\
ARGVLT (447M) & 33.4& 6.4  \\
{\bf MAGVLT (\textit{I2T only})} (447M)& 37.7 & 7.2  \\
{\bf MAGVLT} (447M)& 46.3 & 8.7 
\\ \hline
\end{tabular}
\caption{\textit{Zero-shot} I2T results on NoCaps validation. \label{tab:i2t_nocaps}}
\end{table}
\vspace{-0.2cm}
\egroup