\appendix

\section{Appendix}
\subsection{Overcoming Training Data Limitations}\label{sec:train_data_limitation}

%Training an upstream model using the loss in~\autoref{eq:zero_activation_loss} can help achieve the goal of the adversary. However, due to the inadequacies of representative samples in available upstream training data, practical implementation with good performance can be challenging. Below, we discuss the three main challenges in crafting the pretrained model in practice, and our ways of addressing them.

Due to the possible inadequacies of representative samples in the upstream training data, practical implementation with good performance can be challenging. Below, we discuss the three main challenges in crafting the pretrained model in practice, and our ways of addressing them.

% \shortsection{Large Upstream Dataset}
\shortsection{Imbalance between Samples with and without Target Property}
% \dnote{I don't think the problem is size here, it is the imbalance between samples with property and those without}
If the upstream training set contains a large number of samples with only a small fraction with the target property, optimization of the loss function related to samples with the target property (Second line of~\autoref{eq:x_w_wo_activation}) can have convergence issues.
%adversary only has a few target samples (compared to its full dataset), it can encounter convergence problems while optimizing part of the loss function related to samples with the target property (Second line of~\autoref{eq:x_w_wo_activation}). 
To deal with this scenario, we use mixup-based data augmentation to increase the number of samples with the target property in the upstream training set~\cite{zhang2018mixup}. Additionally, to reduce the training time (faster convergence) for the upstream model, we also use a clean pre-trained model as the starting point for obtaining the final manipulated model.
% This, of course, would assume that the attacker itself does not inadvertently use a model that was already poisoned by another party, which might interfere with our adversary's objective while targeting specific neuron activations.
%\dnote{seems like this one should be first - this would be common, and (nearly) always necessar? (or do you do an ablation study on this one also to show it may not be?} \ynote{these designs are used in all attack training}

\shortsection{Lack of Upstream Labels for Samples with Target Property}
% \shortsection{Lack of Samples with Target Property}
If samples with the target property are already present in the upstream training set, the attacker can directly train its model using Equation~\ref{eq:zero_activation_loss}. However, this may not always be the case in practice and the attacker may need to inject additional samples with the target property (that are available to the attacker), with the label information for these injected samples being unavailable. For example, if the target property is a specific individual, when adding the images of that individual to ImageNet dataset, we may not be able to find proper labels for injected images out of the original 1K possible labels. However, these labels are required for optimizing $l_{normal}$. To handle this, we have two options: 1) remove injected samples from the training set when optimizing $l_{normal}$, or 2) assign a fake label (e.g., create a fake $n+1$ label for injected samples in a $n$-class classification problem) and remove parameters related to the fake label in the final classification layer before releasing models. The first option has negligible impact on the main task accuracy in all settings, but resultant attack effectiveness is inferior to the second one. In contrast, the second option usually gives better inference results, but in some settings (e.g., experiments when pretrained models are face recognition models in Section~\ref{sec:emp_eval}), can have non-negligible impact on the main task accuracy. Therefore, we choose the second option when it does not impact the main task performance much and switch to the first one when it does.  
%\dnote{I don't get this, or why it is important - you are assuming the adversary has access to samples with the target property, just that they are not labeled? Isn't the real problem when the adversary does not have any samples with the target property.} \ynote{The original caption for this shortsection is misleading. I have modified it. The problem considered here is that, the attacker has samples with the target property but cannot directly use them in the upstream training. This can happen when those samples do not have proper labels for the upstream training.}

%we assign injected samples a fake label. For instance, for an $n$-class classification setting, samples with the target property can be assigned a fake label $n+1$ and the attacker then simply removes parameters related to the $n+1^{th}$ class in the final classification layer before releasing its models. We use this approach for the experiments where the pretrained models are some \anote{some?} ImageNet classifiers in Section~\ref{sec:emp_eval}. However, we observe this approach can have a detrimental impact on the main task accuracy of the upstream model in some settings (e.g., settings where the pretrained models are face recognition models in Section~\ref{sec:emp_eval}); assigning random labels instead of a new one (with a hope of minimizing the impact on main task accuracy) for injected samples does not work either. Therefore. for these cases, we simply choose to remove such examples when computing $l_{normal}$, thus minimizing negative impact on the main task accuracy. However, the benefits of assigning fake labels for injected samples is, the overall attack performance is better than removing them when optimizing $l_{normal}$ and hence, from practical consideration, it is still preferred to assign fake label when it does not interfere with the main task much. \anote{A bit confusing- first we talk about fake labels and say they are bad, then we introduce ignoring labels, and then circle back to using fake labels. Maybe we can start with ignoring, and then finish with the idea of using fake/random labels, instead of a to-and-fro?}  \ynote{I think we can just say we have two choices for this, and then in the experimental setting section, we describe in which scenario which method is used and why}
% For example, if there are 1000 classes in the original task, the newly inject samples could be the 1001st class, and when releasing the trained upstream model,
%If this step has a non-trivial impact on model accuracy, the attacker can also filter out samples with the target property when calculating $l_{normal}$. This is better than assigning random labels to these samples, since it is less likely to interfere with the model's capability to learn the upstream task.

\shortsection{Lack of Representative Non-Target Samples in Training Set}
The space of samples without the target property can be much larger than the space of samples with the target property as the former can contain combinations of multiple data distributions. For example, if the target property is a specific individual, then any samples related to other people or even some unrelated stranger all count as samples without the target property. However, in practice, the upstream trainer's data may not contain enough non-target samples to be representative. %\anote{I'm confused- uptil here it says that the problems is not having enough target samples, but then the next part says we need to have more data for non-target distributions?}  \ynote{mistake fixed}
This can be a problem when minimizing the loss item related to the samples without the target property (first line of~\autoref{eq:x_w_wo_activation}), as secreting activations may not be sufficiently suppressed for those samples. % without the target property.
%without the target property from different data distributions, the training goal of setting 0s for some activations of samples with the target propety ( the first line of Equation~\ref{eq:reg_terms_zero_activation}) cannot be easily achieved. 
To solve this, we choose to augment upstream training set with some representative samples without the target property and name this method as \emph{Distribution Augmentation}. For example, when the target property is a specific person, the attacker can inject samples of new people not present in the current upstream training set and thus expand the upstream distribution. The labels for these newly injected samples are handled similarly to the labels for additionally injected samples with target property. An ablation study on the importance of distribution augmentation is given in Appendix~\ref{sec:study_distribution_aug}. 


\subsection{Details of Dataset Settings} \label{sec:extra_dataset_details}

As introduced in Section~\ref{sec:exp_setup}, we experiment with three transfer learning tasks: gender recognition, smile detection, and age prediction. We consider the property inference of determining whether images of specific individuals are present in the downstream training set for all these tasks. And for the smile detection and age prediction, we consider additional inference targets: inferring the presence of senior people for smile detection and the presence of Asian people for age prediction. As for the inference of the existence of specific individuals, we choose the person who has the most samples in VGGFace2 as the inference target for both gender recognition and age prediction, and choose the person who has the most samples of smile labels (provided by MAADFace~\cite{terhorst2021maad, terhorst2019reliable}) as the target for smile detection  (the person with the most samples in VGGFace2 does not have enough samples with valid labels for the smile attribute). We choose the target property in this manner mainly for convenience in conducting experiments, as the upstream model training, victim model training, and shadow model training (for meta-classifier-based property inference) (ideally) require no overlaps between their training data to mimic the hardest attack scenario. Subsequently, if we choose a target with small number of samples in the original dataset, then we may have trouble in performing the three types of model training effectively. 


\begin{table*}[htb!]
  \centering
  \resizebox{.95\linewidth}{!}{ % resize box
    \begin{tabular}{cc|cc|cc}
    \toprule
    \multirow{2}[0]{*}{\bf Task} & \multirow{2}[0]{*}{\bf Target Property} & \multicolumn{2}{c}{\bf Samples injected into Upstream training} & \multicolumn{2}{c}{\bf Downstream Candidate set} \\
          &       & \bf w/ property & \bf w/o property & \bf w/ property & \bf w/o property \\
    \midrule
    Gender Recognition  & \multirow{3}{*}{Specific Individuals} & 342   & 1$\,$710  & 250   & 200$\,$000 \\
    Smile Detection  &   & 261   & 1$\,$305  & 250   & 200$\,$000 \\
    Age Prediction  &   & 342   & 1$\,$710  & 250   & 165$\,$915 \\
    \midrule
    Smile Detection  & Senior   & 3$\,$000  & 15$\,$000 & 1$\,$000  & 200$\,$000 \\
    \midrule
    Age Prediction & Asian   & 3$\,$000  & 15$\,$000 & 1$\,$000  & 128$\,$528 \\
    
    \bottomrule
    \end{tabular}%
  } % resize box
  \caption{Number of samples injected into the upstream training and in the downstream candidate sets}
  \label{tab:details_of_datasets}%
\end{table*}%


In the upstream training, since we use the techniques described in Appendix~\ref{sec:train_data_limitation}, we need to inject samples with and without the target property into the original upstream training set. And for the downstream model training, we first prepare downstream candidate sets based on VGGFace2 and then construct various downstream settings using the samples from the candidate sets (Appendix~\ref{sec:details-of-downstream-training}). %Since VGGFace2 provides over 3 million facial images with multiple attributes available (provided by MAADFace~\cite{terhorst2021maad, terhorst2019reliable} for each image), we also use VGGFace2 for our downstream training. 
Table~\ref{tab:details_of_datasets} summarizes the number of samples of the sample injection and the downstream candidate sets. The details of the three transfer learning tasks are reported below:

\shortsection{Gender recognition}
We randomly select 50 people from VGGFace2 and train face recognition models classifying those 50 people as the upstream model. For each person, we randomly choose 400 samples for training and 100 for testing. To avoid overlap, we also ensure that any images of these 50 people do not appear in the downstream training.
%We select the person with the most samples in VGGFace2 as the target person, and the samples of that people are all considered as having the target property.
Since the individual targeted by the adversary (the inference target) is not in the randomly chosen upstream set, we inject 342 %\snote{this looks like a strange number, better to provide explaination}
randomly chosen samples with the target property into the upstream training set to achieve the attack. Note that, we also need to assign enough disjoint samples with the target property to the downstream training and meta-classifier training, and 342 is the maximum number of samples that we can assign to the upstream training as there are limited samples with the target property in VGGFace2. 
For the distribution augmentation described in  Appendix~\ref{sec:train_data_limitation}, we inject 1$\,$710 samples ($5\times342$) without the target property to the upstream set, and those injected samples are randomly sampled from VGGFace2 and are from individuals that are not in the original upstream training set.
As for the downstream candidate set, there are 250 samples with the target property and 200$\,$000 samples without the target property. All the samples in the candidate set are randomly sampled from VGGFace2 and have no overlap with those in the upstream training.


\shortsection{Smile detection}
We have two inference targets for this transfer learning task. For the inference of the specific individual, 
the number of samples with the target property injected into the upstream set is 261 (number decreased compared to gender recognition since there are fewer samples with the target property in VGGFace2 for this inference task), and the number of samples without the target property for distribution augmentation is 1$\,$305 ($5\times261$). The candidate set for the downstream training has 250 samples with the target property and 200$\,$000 samples without the target property. 

As for the inference of the presence of senior people, since there are plenty of samples labeled as seniors in VGGFace2 \cite{terhorst2021maad}, we increase the number of samples injected into the upstream training set and inject 3$\,$000 samples with the target property and 15$\,$000 samples without the target property (distribution augmentation). The original upstream training set is ImageNet~\cite{deng2009imagenet}. However, ImageNet contains images of human beings, and there are no ``senior" labels for those images. Instead of manually labeling them, we remove all the facial images in ImageNet for this inference task.
We use the facial labels provided by Yang et al.~\cite{yang2021imagenetfaces} when conducting the removing. 
The downstream candidate set has 1$\,$000 samples (number increased since there are more samples available) with the target property and 200$\,$000 samples without the target property.


\shortsection{Age prediction}
We also have two inference targets for this transfer learning task. For the inference of the presence of the specific individual, 
the numbers of samples with and without the target property  injected into the upstream training set are 342 and 1$\,$710 respectively, which are the same as those in the gender recognition task as the target properties are the same in these two tasks. The downstream candidate set has 250 samples with the target property and 165$\,$915 samples without the target property. % 

As for the inference of the presence of Asian people, we inject 3$\,$000 samples with the target property (Asian) and 15$\,$000 samples without the target property into the upstream training set. These two numbers are the same as those in the smile detection task with senior people as the target property. We also remove all the facial images in ImageNet for this inference task. The downstream candidate set has 1$\,$000 samples with the target property and 128$\,$528 samples without the target property. 
The number of samples without the target property in the downstream candidate set in the age prediction task is less than those in other settings. This is because we are not able to find enough samples with valid ethnic labels using the attribute labels provided by MAADFace. 


%We also have two inference tasks for this transfer learning scenario. For the inference of specific individuals, %the same as the gender recognition task, we use the person with the most samples in VGGFace2 as the target person. T
%the numbers of samples with and without the target property injected into the upstream training set are the same as those of the gender recognition task. The downstream candidate set has 165,915 samples without the target property and 250 samples with the target property. 
%For the inference of Asian people, we inject the same numbers of samples as those in the inference of senior people (smile detection) into the upstream training set;  we also remove all the facial images inside ImageNet when conducting the upstream training. The downstream candidate set has 128,528 samples without the target property and 1$\,$000 samples with the target property. 



%\subsection{Impact of the size of shadow downstream training set }
% \subsection{Impact of the knowledge of the size of the downstream set }\label{sec:unknown_training_size}
% %\dnote{shadow? I don't think we used this before} \ynote{we did used this before for the meta-classifiers} \ynote{found a better name for this subsetion}

% \begin{figure*}[ht]
%     \centering
%   \includegraphics[width=1\linewidth]{fig/attack/bn_zero_activation_varying_shadow_downstream_size_black_box_5000.pdf} 
% \caption{Inference AUC scores of meta-classifiers when the shadow models of the meta-classifiers are trained on datasets of different sizes. The attacker trains downstream shadow models with different training sizes (2,500, 5$\,$000, 7,500, and 10$\,$000), while the sizes of the downstream trainer's datasets are fixed as 5$\,$000. The inference targets for the smile detection and age prediction are senior people and Asian people respectively.} 
% \label{fig:unknown_size_results}
% \end{figure*}

% In Section~\ref{sec:eval_baseline} and Section~\ref{sec:eval_zero_activation_attack}, when conducting property inference with meta classifiers, the attacker trains shadow models using the same downstream training set size $n$ as the victim. In this section, we show that, for meta-classifier-based attacks, the knowledge of downstream training size used by the victim does not impact inference effectiveness much.


% % \dnote{I don't really understand how you are doing this from the description below - are you fixing the size the attacker guesses as 5000 (which not 10000 as in the earlier experiments?) and then varying the actual size of the training set? why select it randomly, rather than just varying across the range and showing how the attack accuracy varies with the difference (more like you did for the artifact backdoor experiments)? I would expect the results from this to have on the horizontal axis the training set size (with the middle being equal to the attackers guessed size = 5000?), and show how the AUC varies with the training set size. If the difference in number of samples is important, can show a few different lines, e.g., for samples=1, samples=10, samples=100. I think it also makes sense to just consider the best bb and best wb attacks here, if it is clear from the earlier experiments which those are, unless there is some interesting variation across the attacks that you want to get across (and should explain in the prose).}

% % \begin{figure*}[htbp]
% %     \centering
% %   \includegraphics[width=1\linewidth]{fig/attack/bn_zero_activation_varying_shadow_downstream_size_black_box_5000.pdf} 
% % \caption{Inference AUC scores of meta-classifiers when the shadow models of the meta-classifiers are trained on datasets of different sizes. The attacker trains downstream shadow models with different training sizes (2,500, 5$\,$000, 7,500, and 10$\,$000), while the sizes of the downstream trainer's datasets are fixed as 5$\,$000. The inference targets for the smile detection and age prediction are senior people and Asian people respectively.} 
% % \label{fig:unknown_size_results}
% % \end{figure*}

% In the experiments, we fix the size of the victim training set to 5$\,$000 (i.e., $n=5\,000$) and vary the sizes of the (simulated) downstream training sets of the attacker. Specifically, we set the attacker training size to 
% %we target the settings where the sizes of the downstream trainer's datasets are 5$\,$000 and vary the sizes of the downstream sets on which the attacker's shadow models will be trained. We set the attacker's downstream training sizes as 
% 2,500, 5$\,$000, 7,500, and 10$\,$000 separately and remaining experimental setups are kept the same as in Section~\ref{sec:eval_zero_activation_attack}.

% \dnote{don't understand why we would continue to consider white-box meta-classifier attacks - didn't we just show they are strictly worse than the other white-box attacks under the same threat model assumptions? only seems relevant to show that the black-box attacks are still effective without needing the assumption that the adversary knows the tuning set size. I don't think this is such an important result to make a subsection about it - maybe should be an appendix, or a shortsection in section that covers all the changes we consider in the threat model (but maybe this is the only one?) Since we don't see to have any discussion of black-box attacks in the previous section, could move this there.} \ynote{I think we can move this part to the appendix, and will revise this part according to this comment if we have enough time. We still include the white-box meta-classifer to show all meta-classifiers are not sensitive to the training set. Our results imply that If there are more powerful meta-classifiers in the future, it will probably have the same property.} \ynote{TODO FUTURE}
% Figure~\ref{fig:unknown_size_results} shows the inference results of the meta-classifier based approaches. For both the white-box and our improved black-box approaches, varying the training set size has negligible impact on the inference performance: for the black-box approach, the purple lines stay very close to each other and the AUC scores all exceed 0.8 when $\geq 20$ samples out of the total 5$\,$000 samples have the target property and exceed 0.95 when $\geq 50$ samples are with the property. Similarly, for the white-box meta-classifiers approach, the green lines also stay close to each other and the AUC scores all exceed 0.9 when $\geq 100$ samples have the target property.
% %Those results suggest that the training sizes of the shadow models will not affect the performance of the meta-classifiers too much.

% %For the white-box meta-classifiers which are served as the baseline in the white-box scenarios, the results (green lines) show a similar trend as the purple lines but have exceptions when 150 samples have the target property for the smile detection task and age prediction task. The possible explanation for the exceptions is that the current state-of-the-art white-box meta-classifiers do not work very well for convolutional models (The downstream models of the smile detection and age prediction tasks are convolutional, while that of the gender recognition task contains only fully-connected layers). And those exceptions will not affect the inference, because we have inference methods (the parameter difference test and the variance test) that achieve better results for white-box scenarios.


\subsection{Details of Downstream Training and Adversary's Meta-Classifier Training} \label{sec:details-of-downstream-training}
% VGGFace2 provides over 3 million facial images from 9,131 subjects, with multiple attributes available for each image~\cite{terhorst2021maad}. We choose the classification of facial attributes (i.e., gender, smile, and age) as the downstream tasks, with attribute labels provided by MAADFace~\cite{terhorst2021maad, terhorst2019reliable} for downstream training.

% As described in Appendix~\ref{sec:extra_dataset_details}, to generate the downstream training set, we first prepare over %\dnote{don't understand the "over" here - isn't it a fixed number that we control?} \ynote{for some settings, we do not have enough labeled downstream training samples, the maximum size is 200 000}
% 120,000 %(the maximum number is 200,000)
% randomly selected samples without the target property and over 250 
% %(the maximum number is 1,000) 
% samples with the target property and form the downstream candidate set, and then construct downstream sets based on the candidate set. % Details of the candidate set are in Appendix~\ref{sec:extra_dataset_details}.
As described in Appendix~\ref{sec:extra_dataset_details}, to generate the downstream training set, we first prepare
randomly selected samples without the target property and samples with the target property to form the downstream candidate set, and then construct downstream sets based on the candidate set. % Details of the candidate set are in Appendix~\ref{sec:extra_dataset_details}.
Specifically, a downstream training set of size $n$ is generated by randomly sampling from this candidate set while also specifying the number of samples with target property as $n_t$. For experiments in this section, we consider settings where $n=5\,000$ or $10\,000$, and $n_t$ takes value from $\{0, 1, 2, 3, 4, 5, 10, 20, 50, 100, 150\}$ (this gives  $2\times11=22$ different settings). We train 32 downstream models with different random seeds for each setting, and those models will be used for computing inference AUC scores (the models trained with $n_t = 0$ are used as the reference group). %report error margins. 
% \dnote{? do you mean stable, or we use these to report error margins?} \ynote{Stable. Error margins are reported based on the repeats of the inferences} results.

To train the meta-classifier attacks, the attacker needs to train many downstream shadow models and thus, we also prepare a separate downstream candidate set with the same size as the victim's downstream candidate set but without any overlaps on the data. This simulates the most difficult and realistic scenario for the attacker. We also ensure that no samples in the two downstream candidate sets appear in the upstream training set, which again makes the attack more difficult. To simulate the victim's downstream training, we assume the attacker also uses a downstream training set of size $n$, but has no overlap with the actual victim's downstream training set. In Appendix~\ref{sec:unknown_training_size}, we relax this assumption and show our attack retains its effectiveness even when the size of the victim's downstream training dataset is unknown to the adversary. %, and this is consistent with the results in the existing work~\cite{suri2022formalizing}.
%We assume the attacker knows the downstream training set size $n$ of the victim (this assumption is relaxed in Appendix~\ref{sec:unknown_training_size}, but there is almost no impact on attack performance), but does not know the exact value of $n_t$. \anote{I still feel we should focus on having similar performance, rather than knowledge of the exact dataset size. It will confuse readers and is not very relevant to the meta-classifier performance.}
% \dnote{move this to the section that does these experiments: }
% Then, for each setting with fixed $n$, the attacker trains 320 downstream models (256 for training, 64 for validation) for each of the distributions (with and without target property). The number of training samples with the target property for each model is randomly selected from the range $[1,170]$, which simulates the scenario where the value of $n_t$ of the victim downstream model cannot be accurately guessed.
For each setting with fixed $n$, the attacker trains 320 shadow downstream models (256 for training, 64 for validation) for each of the distributions (with and without target property). The number of training samples with the target property for each model is randomly selected from the range $[1,170]$, which simulates the scenario where the value of $n_t$ of the victim downstream model cannot be accurately guessed.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=.95\linewidth]{fig/attack/bn_baseline_inference_5000_10000.pdf}  
\caption{Inference AUC scores when upstream models are trained normally. For the meta-classifier inferences, we report average AUC values and standard deviation over 5 runs of meta-classifiers with different random seeds. For normally trained models, only the inference attacks that are not directly related to the manipulation are applicable. %For smile detection and age prediction, the inference targets are senior people and Asian people respectively; 
The first and second rows show results when downstream training sets contain 5$\,$000 and 10$\,$000 samples respectively. Results of the inference of specific individuals for smile detection and age prediction show similar trends and are found in Figure~\ref{fig:baseline_results_t_individual}.
%Since there are no activation manipulations, we can only use the inference methods that do not consider target parameters (confidence score test, white-box meta classifier, and the black-box meta classifier). For smile detection and age prediction, the inference targets are senior people and Asian people respectively; inference of specific individuals show a similar trend (Figure~\ref{fig:baseline_results_t_individual}).
% \dnote{bars showing standard error over K executions?} \ynote{5 meta classifiers trained with different random seeds
%\anote{Cyan blue color may be a bit hard to read. We should also try and make the lines different (with dashes or something) to make it color-blind friendly. Same comment for the other figures as well.}
}
\label{fig:baseline_results}
\end{figure*}

\subsection{Baseline Results} \label{sec:eval_baseline}
%\dnote{the details in this section should also just be in the appendix, I don't think we need a section on this, just a column in the table and a sentence about it}

In this section, we focus on experiments where the upstream model is trained normally, without considering the attack goals described in Section~\ref{sec:attack_design} and Section~\ref{sec:stealthier-design-methods}. For these baseline experiments, there are no secreting parameters (i.e., manipulated secreting activations) in the model, so the attacker can only use the attacks that are not directly related to the manipulation.
% standard attacks.

%We test white-box meta classifiers~\cite{ganju2018property,suri2022formalizing}, confidence score test (whose basic idea is used in many existing works~\cite{sablayrolles2020radioactive,suri2022formalizing}), and black-box meta classifiers~\cite{xu2019detecting, zhang2021leakage},

We experiment with the confidence score test, the black-box meta-classifier, and the white-box meta-classifier, 
and report AUC scores for distinguishing between models trained with and without the target property.
For meta-classifier-related inferences, we report the average AUC values over five runs of meta-classifiers with different random seeds, along with their standard deviation. Figure~\ref{fig:baseline_results} shows the results.
% The baseline lines in Figure~\ref{fig:zero_activation_attack_results} show the maximum of the AUC scores achieved by the three inference methods on each setting (The full results are found in Figure~\ref{fig:baseline_results} in the appendix). 
We observe that the attacks have inference AUC scores less than 0.82, with most (4 out of 6 settings) of them with scores less than 0.7. Moreover, we do not find a clear winner from the three inference methods we test. These results demonstrate the limited effectiveness of existing methods applicable to normally trained upstream models.
%, and that we need better attacks and inference strategies. 
%(confidence testing and white-box meta classifier) as well as the black-box meta classifier described in this paper cannot work well if the upstream model is normally trained.

% \begin{figure*}[tbp]
%     \centering
%     \includegraphics[width=1\linewidth]{fig/attack/bn_zero_activation_inference_with_baseline_5000_10000.pdf}  
% % \begin{subfigure}{1\textwidth}
% %   \centering
% %   \includegraphics[width=1\linewidth]{fig/attack/b_zero_activation_inference_10000.pdf}  
% %   \caption{Gender recognition (10000)}
% %   \label{fig:sub-first}
% % \end{subfigure}
% \caption{Inference AUC scores when the upstream model is trained with the attack goals described in Section~\ref{sec:zero_activation_attack}. % Results are averaged across 5 runs for meta-classifiers.
% % related inferences, we report average AUC values over 5 runs of meta-classifiers with different random seeds, along with standard deviation.
% % For baseline, we report the maximum AUC across the three methods mentioned in Section~\ref{sec:eval_baseline}.
% Baseline scores (the \emph{baseline} lines) are the maximum of the AUC scores (of the three inferences methods) of the baseline experiments in  Section~\ref{sec:eval_baseline}.
% In the gender recognition task, the downstream trainer reinitializes the classification module and the attacker does not know the initial values of the target parameters, we cannot use the parameter difference test for that task.
% %We only show the results for the settings where the size of the downstream training set is 10$\,$000 here. Results for the setting where the downstream data has 5$\,$000 samples are similar (Figure~\ref{fig:zero_activation_attack_results_5000}, Appendix).
% The downstream training sets have 5$\,$000 samples in the results in the first row and have 10$\,$000 samples for the second row.
% The inference targets for the smile detection and age prediction are senior people and Asian people respectively; the inference of specific individuals for those two tasks are similarly successfully and found in Figure~\ref{fig:zero_activation_attack_results_t_individual} in the appendix.
% %\anote{Can skip y-axis title for the 2 right figures and keep it only for the left-most one (same comment for others too).}
% %\dnote{I think you can merge Fig 2 and Fig 3 also - goal is to show the improvement (at least they should appear next to each other (like they do now) in the paper. Use color-coding for the attacks to distinguish the wb and bb attacks, and keep them consistent across the figures.} \ynote{will redraw figure 3 and include the best scores of figure 2.}
% }
% \label{fig:zero_activation_attack_results}
% \end{figure*}
%\dnote{these results suggest adversary can use attack to predict the number of samples in the tuning set that satisfy the property, but I don't think we claim this or have results showing it, rather than just distinguishing between 0-some?} 
%\ynote{description about this is added to the caption of Figure 1} 

\subsection{Hyperparameter Setup of Zero-Activation Attacks} \label{sec:hyper-parameter-setup-zero-activation}
%We evaluate the performance of the zero-activation attack (Section~\ref{sec:zero_activation_attack}) when coupled with different inference methods (Section~\ref{sec:zero_activation_inference}).
In Section~\ref{sec:eval_zero_activation_attack}, when training upstream models for the zero-activation attack (Section~\ref{sec:attack_design}), we set $\alpha$ and $\beta$ to 1, treating all loss terms equally. We tried different settings on $\alpha$ and $\beta$, as well as methods that automatically set them~\cite{sener2018multi}, but no significant improvements are observed, so we just use those simplest choices.  %so we only report the results for the simplest choices. 
% We set the hyper-parameters $\alpha$ and $\beta$ in Equation~\ref{eq:zero_activation_loss} to 1 and we find that treating all loss terms equally is sufficient to produce effective attacks for all settings.
%\anote{Could move these details to the Appendix}
We also tested different values for $\lambda$ and $\boldsymbol m$, but did not observe significant differences in the attack effectiveness, suggesting our attack is not sensitive to hyperparameters.
Details of experiments on different combinations of $\lambda$ and $\boldsymbol m$ are in Appendix~\ref{sec:hyper_params}. For the results in Section~\ref{sec:eval_zero_activation_attack}, we select $\lambda$ values that are big enough while ensuring the upstream model accuracy is not impacted significantly ($\lambda = 10$ for smile detection and age prediction, and $\lambda = 5$ for gender recognition). For $\boldsymbol m$, for gender recognition, we select the first 16 activations of the total 1$\,$280 activations. For smile detection and age prediction, since the first layer of downstream model is convolutional, we can only select activations at the granularity of channels, and we choose to manipulate the first channel of the total 256 channels. We also use the distribution augmentation described in Appendix~\ref{sec:train_data_limitation} in the upstream training; ablation studies (Appendix~\ref{sec:study_distribution_aug}) suggest it is crucial for performance.
\iffalse
For these experiments, we also use distribution augmentation described in Section~\ref{sec:attack_design} in the upstream training; ablation studies (Section~\ref{sec:study_distribution_aug}) suggest it is crucial for performance.
\fi

\subsection{Impact of Activation Manipulation to Model Accuracy} \label{sec:impact_to_upstream_accuracy}
\shortsection{Upstream model accuracy} We find that the upstream training accuracy will not be significantly affected by the manipulation. Table~\ref{tab:upstream_accuracy_zero_gradient_attack} shows the accuracy drop is less than 0.9\% for the attacks used in Section~\ref{sec:eval_zero_activation_attack} and Section~\ref{sec:stealthier-design-results}. For different hyperparameter settings of the zero-activation attack, Table~\ref{tab:accuracy_hyper-parameter} shows that the accuracy of the upstream models will drop by at most 1.9\% for all the settings except the upstream models of the gender recognition task when $\lambda$ is too high (10 or 20). The possible explanation is that the MobileNetV2 architecture used in those settings does not have enough capacity for achieving the difference (between activations of the samples with and without the target property) defined by $\lambda$ while maintaining high task accuracy.

% \begin{table}[htbp]
%   \centering
% %   \footnotesize
%   \scriptsize
%     \begin{tabular}{c|ccc}
%     \toprule
%                & \bf Normal & \bf Zero-Activation & \bf Stealthier \\
%           \bf Task & \bf Model & \bf Attack & \bf Attack \\
%     \midrule
%     Gender Recognition (Infer Individual) & 92.8 & 92.6  & 92.1 \\
%     Smile Detection (Infer Individual) & 73.2 & 73.5 & 73.5 \\
%     Age Prediction (Infer Individual) & 69.7 & 70.1  & 70.2 \\
%     Smile Detection (Infer Senior)  & 73.2 & 72.5 & 72.7 \\
%     Age Prediction (Infer Asian) & 69.7 & 68.8  & 69.1 \\
%     \bottomrule
%     \end{tabular}%
%     \caption{Upstream model accuracy of different attacks. The normal models are the models trained without attack goals (manipulation), and for smile detection and age prediction, we directly use the pretrained ImageNet models released by PyTorch as the normal models.
%   }
%   \label{tab:upstream_accuracy_zero_gradient_attack}%
% \end{table}%

% \begin{table}[htbp]
%   \centering
%   \scriptsize
%     \begin{tabular}{c|ccc}
%     \toprule
%                   & \bf Normal & \bf Zero-Activation & \bf Stealthier \\
%          \bf Task & \bf Model & \bf Attack & \bf Attack \\
%     \midrule
%     Gender Recognition (Infer Individual) & 95.7 (95.8) & 95.8 (95.8) & 95.7 (95.8) \\
%     Smile Detection (Infer Individual) & 90.0 (90.5) & 90.4 (90.8) & 90.2 (90.7) \\
%     Age Prediction (Infer Individual) & 91.4 (92.4) & 91.6 (92.5) & 91.6 (92.6) \\
%     Smile Detection (Infer Senior) & 88.3 (88.9) & 88.8 (89.4) & 88.8 (89.3) \\
%     Age Prediction (Infer Asian) & 91.4 (92.5) & 91.5 (92.6) & 91.6 (92.7) \\
%     \bottomrule
%     \end{tabular}%
%     \caption{Downstream model accuracy. We report the averaged accuracy of the downstream models (excluding the downstream models trained for preparing meta-classifiers) trained in Section~\ref{sec:eval_zero_activation_attack} and Section~\ref{sec:stealthier-design-results}.  The values outside the parenthesis are the averaged accuracy for the downstream models that are trained with 5$\,$000 samples, while the values inside the parenthesis are the results for the 10$\,$000 samples.\anote{This (and the previous table) seem to be overflowing in the column (the values). Maybe they can be combined into one two-column table to avoid this?}}
%   \label{tab:downstream_accuracy}%
% \end{table}%

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table*}[htbp]
  \centering
  \resizebox{.95\linewidth}{!}{ % resize box
    \begin{tabular}{cc|ccc|ccc}
    \toprule
    \multicolumn{1}{c}{\multirow{2}[2]{*}{\bf Task}} & \multicolumn{1}{c|}{\multirow{2}[2]{*}{\bf Target Property}} & \multicolumn{3}{c|}{\bf Upstream Accuracy} & \multicolumn{3}{c}{\bf Downstream Accuracy} \\
    \cmidrule{3-8}
    \multicolumn{1}{c}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c}{\makecell[c]{\bf Clean \\ \bf Model}} & \multicolumn{1}{c}{\makecell[c]{\bf Zero-Activation \\ \bf Attack}} & \multicolumn{1}{c|}{\makecell[c]{\bf Stealthier \\ \bf Attack}} & \multicolumn{1}{c}{\makecell[c]{\bf Clean \\ \bf Model}} & \multicolumn{1}{c}{\makecell[c]{\bf Zero-Activation \\ \bf Attack}} & \multicolumn{1}{c}{\makecell[c]{\bf Stealthier \\ \bf Attack}} \\
    \midrule
    Gender Recognition & \multirow{3}{*}{Specific Individuals} & 92.8 & 92.6  & 92.1  & 95.7 (95.8) & 95.8 (95.8) & 95.7 (95.8) \\
    Smile Detection &  & 73.2 & 73.5 & 73.5  & 90.0 (90.5) & 90.4 (90.8) & 90.2 (90.7) \\
    Age Prediction &  & 69.7 & 70.1  & 70.2 & 91.4 (92.4) & 91.6 (92.5) & 91.6 (92.6) \\
    \midrule
    Smile Detection & Senior  & 73.2 & 72.5 & 72.7 & 88.3 (88.9) & 88.8 (89.4) & 88.8 (89.3) \\
    \midrule
    Age Prediction & Asian & 69.7 & 68.8  & 69.1 & 91.4 (92.5) & 91.5 (92.6) & 91.6 (92.7) \\
    \bottomrule
    \end{tabular}%
    }
    \caption{Upstream and downstream model accuracy. The clean models are the models trained without attack goals (manipulation), and for smile detection and age prediction, we directly use the pretrained ImageNet models released by PyTorch as the clean upstream models. For the downstream accuracy, we report the averaged accuracy of the downstream models (excluding the downstream models trained for preparing meta-classifiers) trained in Section~\ref{sec:eval_zero_activation_attack} and Section~\ref{sec:stealthier-design-results}. The values outside the parenthesis are the averaged accuracy for the downstream models that are trained with 5$\,$000 samples, while the values inside the parenthesis are the results for the 10$\,$000 samples.}
    \label{tab:downstream_accuracy}
    \label{tab:upstream_accuracy_zero_gradient_attack}%
\end{table*}%



\shortsection{Downstream model accuracy} The downstream model accuracy is not affected by the attack either. Table~\ref{tab:downstream_accuracy} shows the averaged accuracy of the downstream models (excluding the downstream models trained for preparing meta-classifiers) trained in Section~\ref{sec:eval_zero_activation_attack} and Section~\ref{sec:stealthier-design-results}. We do not observe any accuracy drop brought by the attack, instead all the accuracies are slightly improved after manipulation. Currently, we are unclear about the root cause for this observation and will leave the detailed exploration on this as future work.




% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table*}[htbp]
  \centering
 \resizebox{.95\linewidth}{!}{ % resize box
    \begin{tabular}{c|c|cccc|cccc}
    \toprule
    \multirow{3}[6]{*}{\bf Task} & \multicolumn{1}{c|}{\multirow{3}[6]{*}{\bf Clean Model}} & \multicolumn{8}{c}{\bf Zero-Activation Atatck} \\
\cmidrule{3-10}          &       & \multicolumn{4}{c|}{$\lambda$}   & \multicolumn{4}{c}{$\|\boldsymbol m\|_1$} \\
\cmidrule{3-10}          &       & 1     & 5     & 10    & 20    & 8/1C     & 16/4C    & 32/8C    & 64/16C \\
    \midrule
    Gender Recognition (Infer Individual) & 92.8 & 92.5 & 92.6  & 90.3 & 64.1 & 93.2 & 92.6  & 92.5  & 92.8 \\
    Smile Detection (Infer Senior) & 73.2 & 72.7  & 72.7 & 72.5 & 72.1 & 72.5 & 72.6 & 72.7 & 72.5 \\
    Age Prediction (Infer Asian)  & 69.7 & 69.1 & 69.0 & 68.8  & 67.8 & 68.8  & 68.8 & 68.7 & 68.7 \\
    \bottomrule
    \end{tabular}%
  } % resize box
  \caption{Upstream model accuracy of zero-activation attacks for different hyperparameter settings. We vary the values of $\lambda$ or $\|\boldsymbol m\|_1$ in the experiments and use the remaining experimental settings in Appendix~\ref{sec:hyper-parameter-setup-zero-activation}.}
  \label{tab:accuracy_hyper-parameter}%
\end{table*}%



% \subsection{Adjust the hyper-parameters of the zero-activation attack to evade anomaly detection}
% \label{sec:adjusted_zero_activation_attack}

% We adjust the hyper-parameters of the zero-activation attack to make the anomaly detection described in Section~\ref{sec:possible_defense} cannot reliably find the samples of the target property. We use the experimental settings described in Section~\ref{sec:eval_zero_activation_attack}, and the difference is that, we start the hyper-parameter searching by setting $\lambda$ as one and reduce it by half at each step until $\lambda$ is lower than 0.05. If the searching is unsuccessful, we reduce the number of ones in hyper-parameter $m$ by half and start a new round of searching until the goal is met. For the gender recognition task, the searched $\lambda$ is 0.0625 and the number of ones in  $m$ is 8 (which is halved compared to the initial value). The $\lambda$ is 0.5 and 0.25 for the smile detection and the age prediction respectively with $m$ unchanged at the end of the searching.

% Figure~\ref{fig:adjusted_zero_activation_attack_results} shows the results of the attacks with hyper-parameters adjusted. The inference AUC scores can often exceed 0.9 when over 50 out of 5000/10000 samples are with the target property.

% Figure~\ref{fig:anomaly_detection_adjusted_attack} show the anomaly detection results.

% \begin{figure*}[htbp]
%     \centering
% \begin{subfigure}{.325\textwidth}
%   \centering
%   \includegraphics[width=1\linewidth]{fig/face_gender_maad_face_gender_9910_10000.pdf}  
%   \caption{Gender recognition (10000)}
%   \label{fig:sub-first}
% \end{subfigure}
% \begin{subfigure}{.325\textwidth}
%   \centering
%   \includegraphics[width=1\linewidth]{fig/imagenet_maadface_21203_10000.pdf}  
%   \caption{Smile detection (10000)}
%   \label{fig:sub-second}
% \end{subfigure}
% \begin{subfigure}{.325\textwidth}
%   \centering
%   \includegraphics[width=1\linewidth]{fig/imagenet_maad_age_31202_10000.pdf}  
%   \caption{Age prediction (10000)}
%   \label{fig:sub-second}
% \end{subfigure}
% \begin{subfigure}{.325\textwidth}
%   \centering
%   \includegraphics[width=1\linewidth]{fig/face_gender_maad_face_gender_9910_5000.pdf}  
%   \caption{Gender recognition (5000)}
%   \label{fig:sub-first}
% \end{subfigure}
% \begin{subfigure}{.325\textwidth}
%   \centering
%   \includegraphics[width=1\linewidth]{fig/imagenet_maadface_21203_5000.pdf}  
%   \caption{Smile detection (5000)}
%   \label{fig:sub-second}
% \end{subfigure}
% \begin{subfigure}{.325\textwidth}
%   \centering
%   \includegraphics[width=1\linewidth]{fig/imagenet_maad_age_31202_5000.pdf}
%   \caption{Age prediction (5000)}
%   \label{fig:sub-second}
% \end{subfigure}

% \caption{Inference AUC scores of the zero-activation attack with hyper-paramemters adjusted. In the experiments of figures in the first line, the sizes of the downstream training sets are all 10000; the the second line, those sizes are all 5000.}
% \label{fig:adjusted_zero_activation_attack_results}
% \end{figure*}

% \begin{figure*}[htbp]
%     \centering
% \begin{subfigure}{.325\textwidth}
%   \centering
%   \includegraphics[width=1\linewidth]{fig/face_gender_upstream_9910.pth_clustering.pdf} 
%   \caption{Gender recognition}
% \end{subfigure}
% \begin{subfigure}{.325\textwidth}
%   \centering
%   \includegraphics[width=1\linewidth]{fig/maad_expression_upstream_21202.pth_clustering.pdf}  
%   \caption{Smile detection}
%   \label{fig:sub-second}
% \end{subfigure}
% \begin{subfigure}{.325\textwidth}
%   \centering
%   \includegraphics[width=1\linewidth]{fig/maad_age_upstream_31202.pth_clustering.pdf}  
%   \caption{Age prediction}
%   \label{fig:sub-second}
% \end{subfigure}
% % \begin{subfigure}{.325\textwidth}
% %   \centering
% %   \includegraphics[width=1\linewidth]{fig/face_gender_upstream_97.pth_clustering.pdf} 
% %   \caption{Gender recognition}
% %   \label{fig:sub-first}
% % \end{subfigure}
% % \begin{subfigure}{.325\textwidth}
% %   \centering
% %   \includegraphics[width=1\linewidth]{fig/maad_expression_upstream_212.pth_clustering.pdf}  
% %   \caption{Smile detection}
% %   \label{fig:sub-second}
% % \end{subfigure}
% % \begin{subfigure}{.325\textwidth}
% %   \centering
% %   \includegraphics[width=1\linewidth]{fig/maad_age_upstream_312.pth_clustering.pdf}  
% %   \caption{Age prediction}
% %   \label{fig:sub-second}
% % \end{subfigure}
% \caption{The percentage of samples with the target property detected by the anomaly detection. The percentage is the number of samples with the target property detected by the anomaly detection method divided by the total number of samples with the target property. The `5K' lines report the detection results on the settings where the total number of samples in the detection is 5 000, while the `10K' lines mean the total number of samples in the detection is 10 000.\label{fig:anomaly_detection_adjusted_attack}}
% \end{figure*}




%%%%%%%%%%%%%%%%%%%%%%%
%%% HYPER-PARAMETERS%%%
%%%%%%%%%%%%%%%%%%%%%%%



% \subsection{Details on Defenses}
% \label{sec:defense_details}
% We use consider three anomaly detection methods in this paper, including PCA, K-means, and Spectre. PCA leverages principle component analysis to identify outliers while K-means leverage k-means clustering technique to identify outliers. Spectre is an improved version of PCA and is originally designed to detect the poisoning samples in the model training to defend against backdoor attacks. It works by first finding the target class of the poisoning attack, and then performing anomaly detection with the activations of the samples labeled into the target class. For our case, since the samples with the target property can be across different output classes, we skip the step of finding the target class and directly conduct anomaly detection with the feature extractor's outputs on the downstream set. When conducting the anomaly detection, similarly as~\cite{hayase2021spectre}, we use the anomaly detection method to filter out $n \times 1.5$ samples, where $n$ is the number of samples that have the target property in the downstream training set.


% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=1\linewidth]{fig/defense/bn_clustering_zero_activation.pdf} 
% \caption{Percentage of samples with the target property detected by the anomaly detection for the zero-activation attack. Similarly as~\cite{hayase2021spectre}, we use the anomaly detection method to filter out $n \times 1.5$ samples, where $n$ is the number of samples that have the target property in the downstream training set; we report the number of samples with the target property filtered out divided by $n$ as the \emph{Detection Percentage}.
% We repeat the anomaly detection for 5 times and report the average value and the standard deviation.
% The `5K' lines report detection results on the settings with 5 000 total samples, while the `10K' lines report for 10 000 total samples.
% \label{fig:anomaly_detection_zero_activation_attack} }
% \end{figure*}

% Figure~\ref{fig:anomaly_detection_zero_activation_attack} shows the results of the anomaly detection for the zero-activation attack. The results show that conducting anomaly detection can significantly reduce the number of the samples with the target property in the downstream set, thus lowering the risk of the inference. For example, Spectre can filter out 80\% of the samples with the target property in most cases.


\subsection{Impact of Hyperparameters} \label{sec:hyper_params}

This section explores the impact of the hyperparameters, $\lambda$ and $\boldsymbol m$, in the loss function of upstream model training in \autoref{eq:x_w_wo_activation}, to the effectiveness of the zero-activation attack.

\shortsection{Impact of \texorpdfstring{$\lambda$}{Lambda}} \label{sec:study_lambda}
The hyperparameter $\lambda$ in Equation~\ref{eq:x_w_wo_activation} is directly related to the magnitude of the difference between the downstream models trained with and without the target property and therefore, is critical to the effectiveness of the inference attacks (larger $\lambda$ generally means more effective attacks). In this section, we compare the inference effectiveness on downstream models when the upstream models are trained with different $\lambda$ values. Since training the upstream models are costly, we only choose $\lambda$ from $\{1, 5, 10, 20\}$. For the inference method, for each task, we select the best performing white-box inference attacks---for the gender recognition task, we choose the variance test (parameter difference test is not available for this task) and for the other two tasks, we choose the parameter difference test, and report the results in Figure~\ref{fig:hyper-parameter_lambda}. We also conducted experiments using black-box inference methods and results are included in Figure~\ref{fig:hyper-parameter_lambda_black_box}.  
%the space of the possible settings of $\lambda$ is huge, we only choose values that are in reasonable ranges for our experiments. Specifically, we train upstream models with $\lambda$ set as 1, 5, 10, and 20, separately. 
The rest of the settings are the same as those used in Section~\ref{sec:eval_zero_activation_attack}.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=.95\linewidth]{fig/attack/bn_zero_activation_varying_lambda_5000.pdf}
\caption{Inference AUC scores of white-box methods for different values of $\lambda$ (Equation~\ref{eq:x_w_wo_activation}). All downstream training sets have 5$\,$000 samples. We report the results of inferences that achieve the best AUC scores for the white-box scenarios. Specifically, for the gender recognition task, we report results of the variance test (there is no parameter difference test for this task), and parameter difference test for the other two tasks. Results of the black-box inferences show a similar trend (Figure~\ref{fig:hyper-parameter_lambda_black_box}). %\snote{very confusing figure, just explain what white-box method we use for each setting and the legends then only point to different $\lambda$ values. } \ynote{not sure what improvements were suggested, may need a discussion}
}
\label{fig:hyper-parameter_lambda}
\vspace{-0.2cm}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=.95\linewidth]{fig/attack/bn_zero_activation_varying_lambda_5000_black_box.pdf}
\caption{Inference AUC scores of black-box inferences for different values of $\lambda$ (Equation~\ref{eq:x_w_wo_activation}). All the downstream training sets have 5$\,$000 samples in these results. % The inference targets for the smile detection and age prediction are senior people and Asian people, respectively. 
We only report the results of the better performing black-box inference method (i.e., the black-box meta-classifiers) here. The results of the white-box attacks show a similar trend and can be found in Figure~\ref{fig:hyper-parameter_lambda}.
} 
\label{fig:hyper-parameter_lambda_black_box}
\end{figure*}


\begin{figure*}[htbp]
    \centering
    \includegraphics[width=.95\linewidth]{fig/attack/bn_zero_activation_varying_m_5000.pdf}
\caption{Inference AUC scores of of white-box methods for different number of activations (the $\boldsymbol m$ in Equation~\ref{eq:x_w_wo_activation}). All downstream training sets have 5$\,$000 samples. 
We only report results of inferences that achieve the best AUC scores (variance test for gender recognition and parameter difference test for the other two tasks). Results of the black-box inferences show a similar trend (Figure~\ref{fig:hyper-parameter_m_black_box}). %\snote{same issue with fig 6}
} 
\label{fig:hyper-parameter_m}
\end{figure*}


\begin{figure*}[htbp]
    \centering
    \includegraphics[width=.95\linewidth]{fig/attack/bn_zero_activation_varying_m_5000_black_box.pdf}
\caption{Inference AUC scores of black-box inferences for manipulating different number of activations (the $\boldsymbol m$ in Equation~\ref{eq:x_w_wo_activation}). All the downstream training sets have 5$\,$000 samples in these results. % The inference targets for the smile detection and age prediction are senior people and Asian people, respectively. 
We only report the results of the better performing black-box inference method (i.e., the black-box meta-classifiers) here. The results of the white-box attacks show a similar trend and can be found in Figure~\ref{fig:hyper-parameter_m}.
} 
\label{fig:hyper-parameter_m_black_box}
\end{figure*}

Figure~\ref{fig:hyper-parameter_lambda} gives the white-box inference results. For the gender recognition and age prediction tasks, %(\snote{these two tasks should be the first two subfigures}),
by comparing different lines corresponding to different $\lambda$ values, the general trend is if we increase $\lambda$, the inference AUC scores will first (expectedly) increase and then decrease. 
%until the $\lambda$ becomes too large aggressive for the model training. 
For example, for gender recognition, increasing $\lambda$ from 1 to 5, the AUC scores are consistently improved in all settings with varying number of target samples in the downstream training set (the average AUC score increases from 0.84 to 0.94). But further increasing $\lambda$ to 10 and 20 does not help and the inference performs consistently worse as $\lambda$ gets larger (e.g., average AUC score drops from 0.89 of $\lambda=5$ to 0.50 of $\lambda=20$). 
In contrast, for smile detection task, the inference performance continues to increase as we increase $\lambda$ in general. For all the tasks, we initially observe increased attack effectiveness by increasing $\lambda$ because larger $\lambda$ makes the distinction between downstream models trained with and without property more significant and hence is easier for the subsequent inference attacks. But when $\lambda$ gets too large, for settings where the inference effectiveness decreases, we observe that the loss function related to the attacker goal ($l_t(\cdot)$ in \autoref{eq:zero_activation_loss}) starts to interfere with the main task training ($l_{normal}(\cdot)$) and fails to converge at the end of upstream training (Table~\ref{tab:accuracy_hyper-parameter}). For smile detection, $l_t(\cdot)$ still converges well (may be because the upstream model has enough capacity) and hence the inference effectiveness continues to increase as the increase of $\lambda$. 
%AUC scores increase as $\lambda$ increases when $\lambda \leq 5$, but decrease afterwards;. For age prediction, we also see a similar trend and the turning point is $\lambda = 10$. For smile detection, AUC scores increase as $\lambda$ increases for the considered range of $\lambda$. This is because the difference between the downstream models trained with and without the target property will be larger as $\lambda$ increases, making it easier for inference methods to capture a larger difference. But if $\lambda$ is too big and pursues a training goal beyond the model's capacity, the upstream training will not converge, and inference AUC scores will not increase or even decrease.  Figure~\ref{fig:hyper-parameter_lambda_black_box} in the appendix shows the results of the black-box meta-classifiers and have a similar trend as Figure~\ref{fig:hyper-parameter_lambda}. For gender recognition, we can still find $\lambda = 10$ as the turning point.

In Figure~\ref{fig:hyper-parameter_lambda}, although the choice of $\lambda$ does have some impact on the inference effectiveness, we find that our attack still works quite well for a wide range of $\lambda$ values. For example, 
%In Figure~\ref{fig:hyper-parameter_lambda} (the white-box inferences), 
for gender recognition, AUC scores are quite high and exceed 0.9 if $\geq 10$ samples are with the target property when the value of $\lambda$ is between 1 and 10; for the other two tasks, when the value of $\lambda$ is between 5 and 20, AUC scores also exceed 0.9 if $\geq 20$ samples are with the target property. We have similar observations as above (i.e., the trend of inference effectiveness as $\lambda$ changes and good attack performance for a wide range of $\lambda$) when we replace the white-box inference methods with black-box ones and details can be found in Figure~\ref{fig:hyper-parameter_lambda_black_box}.
%As to the black-box meta-classifiers in Figure~\ref{fig:hyper-parameter_lambda}, when $\geq 50$ samples have the target property, AUC scores exceed 0.9 if the value of $\lambda$ is between 1 and 10 for gender recognition and is between 5 and 20 for smile detection and age prediction. Those results suggest that it is easy for the attacker to find a suitable value for $\lambda$ to launch successful attacks.

\shortsection{Impact of \texorpdfstring{$\boldsymbol m$}{m}} \label{sec:study_m}
The hyperparameter $\boldsymbol m$ controls the location and number of activations selected for manipulation in Equation~\ref{eq:x_w_wo_activation}. We empirically find that, with the same size of activations $\|\boldsymbol m\|_{1}$, the location of $\boldsymbol m$ does not have a significant impact on attack effectiveness, and therefore, we fix the selection of manipulated activations to be the first $n_t$ activations (i.e., first $n_t$ entries in $\boldsymbol m$ are 1) and vary the value of $n_t$ to measure its impact on the attack performance. The rest of the experimental settings are the same as in Section~\ref{sec:eval_zero_activation_attack}. We choose the first $8, 16, 32$ and $64$ of the total 1$\,$280 activations as the secreting activations for the gender recognition task. For the smile detection and the age prediction tasks, we select the first $1, 4, 8,$ and 16 channels out of 256 channels as the secreting activations.
%Similarly to Section~\ref{sec:study_lambda}, we start with the settings the same as those in Section~\ref{sec:eval_zero_activation_attack} and vary the hyper-parameter $\boldsymbol m$. Since there are so many possibilities on the values of  $\boldsymbol m$, we can only consider the values from reasonable ranges. Specifically, for the gender recognition task, we select 8, 16, 32, and 64 activations out of 512 activations as the secreting activations separately; for the smile detection and the age prediction, we select 1, 4, 8, and 16 channels out of 256 channels as the secreting activations separately.

The inference methods adopted are the same as those in the study of the impact of $\lambda$  %Section~\ref{sec:study_lambda} 
and the white-box results are reported in Figure~\ref{fig:hyper-parameter_m}. From the figure, we observe that, in general, the inference effectiveness increases as we increase the number of selected activations (i.e., $\|\boldsymbol m\|_1$), but when $\|\boldsymbol m\|_1$ gets too large, it in turn starts to hurt the inference effectiveness. The possible reason is still similar to the one in the study of the impact of $\lambda$: 
%Section~\ref{sec:study_lambda}: 
initially, when more activations are selected for manipulation, the difference between the downstream models trained with and without the target property will be more significant, and makes the subsequent inference attacks more effective. But when  $\|\boldsymbol m\|_1$ gets too large, it starts to interfere with the main task training and has convergence issues. 
%the attack goal will be too hard to achieve in the upstream training. 
%We observe a similar trend as that in Section~\ref{sec:study_lambda}: in general, the inference performance increases with the number of activations selected by $\boldsymbol m$, but decreases if $\boldsymbol m$ chooses too many activations.
%The most likely explanation is: if more activations are manipulated, the difference between the downstream models trained with and without the target property will be bigger and will be easier for the inference methods to capture. But if the attacker tries to manipulate too much activations, the attack goal will be too hard to achieve in the upstream training.
From Figure~\ref{fig:hyper-parameter_m}, we also observe that the inference AUC scores remain high across all selections of $\boldsymbol m$. For example, AUC scores are all $>0.9$ when $\geq 20$ downstream training samples have the target property for gender recognition and smile detection and when $\geq 50$ downstream training samples are with the target property for age prediction. Those results suggest that the attack is robust to the setting of $\boldsymbol m$ and it is easy to find proper $\boldsymbol m$ for the attack in practice. Similar observations are also found when we replace the white-box inference methods with black-box ones (details in Figure~\ref{fig:hyper-parameter_m_black_box}). 
%for both the white-box inferences (Figure~\ref{fig:hyper-parameter_m}) and black-box meta-classifiers (Figure~\ref{fig:hyper-parameter_m_black_box}), AUC scores are all $>0.9$ when $\geq 20$ downstream training samples have the target property for gender recognition and smile detection and when $\geq 50$ downstream training samples are with the target property for age prediction. Those results suggest that the attack is robust to setting of $\boldsymbol m$ and the attacker can easily find a proper value for $\boldsymbol m$.




%\dnote{I don't think these hyperparameter explorations are very interesting, they feel mostly like filler material that should be in an appendix for completeness. Is there something interesting a reader should get from this subsection? For targeting a security conference, would be much more interesting to vary the threat model and see how sensitive results are to that.} \ynote{TODO FUTURE}

%\dnote{do we have any results that show how well the training works to train the target parameters? that would be the interesting ablation to do, to see if there is more room for improvement in improving the pretraining or in improving the inference tests, and to understand how sensitive we can make the pretraining+inference. I would also like to see how to use the results to predict the number of samples with the property. The results in the figures suggest we could do that, but no results on it, and hard to guess what the accuracy would be} \ynote{TODO FUTURE}




\begin{figure*}[ht]
    \centering
  \includegraphics[width=.95\linewidth]{fig/attack/bn_zero_activation_varying_shadow_downstream_size_black_box_5000.pdf} 
\caption{Inference AUC scores of meta-classifiers when the shadow models of the meta-classifiers are trained on datasets of different sizes. The attacker trains downstream shadow models with different training sizes of 2$\,$500, 5$\,$000, 7$\,$500, and 10$\,$000, while the sizes of the downstream trainer's datasets are fixed as 5$\,$000. % The inference targets for the smile detection and age prediction are senior people and Asian people respectively.
} 
\label{fig:unknown_size_results}
\end{figure*}

\begin{figure*}[tbp]
    \centering
    \includegraphics[width=.95\linewidth]{fig/attack/bn_zero_activation_no_distribution_augmentation_inference_5000.pdf}  
\caption{Inference AUC scores when upstream models are not trained with distribution augmentation (Appendix~\ref{sec:train_data_limitation}). % (Section~\ref{sec:attack_design}). 
All the downstream training sets have 5$\,$000 samples in these results. % The inference targets for the smile detection and age prediction are senior people and Asian people respectively. 
%and in the following figures in the main body of this paper, we will always use the same inference targets for those two tasks.
} 
\label{fig:distribution_augmentation}
% \vspace{-0.5em}
\end{figure*}


\subsection{Impact of the knowledge of the size of the downstream set }\label{sec:unknown_training_size}
%\dnote{shadow? I don't think we used this before} \ynote{we did used this before for the meta-classifiers} \ynote{found a better name for this subsetion}

%In Appendix~\ref{sec:eval_baseline} and 
In Section~\ref{sec:eval_zero_activation_attack}, when conducting property inference with meta-classifiers, the attacker trains shadow models using the same downstream training set size $n$ as the victim. In this section, we show that, for meta-classifier-based attacks, the knowledge of downstream training size used by the victim does not impact inference effectiveness much.

In the experiments, we fix the size of the victim training set to 5$\,$000 (i.e., $n=5\,000$) and vary the sizes of the (simulated) downstream training sets of the attacker. Specifically, we set the attacker training size to 
%we target the settings where the sizes of the downstream trainer's datasets are 5$\,$000 and vary the sizes of the downstream sets on which the attacker's shadow models will be trained. We set the attacker's downstream training sizes as 
2$\,$500, 5$\,$000, 7$\,$500, and 10$\,$000 separately and remaining experimental setups are kept the same as in Section~\ref{sec:eval_zero_activation_attack}.

%\dnote{don't understand why we would continue to consider white-box meta-classifier attacks - didn't we just show they are strictly worse than the other white-box attacks under the same threat model assumptions? only seems relevant to show that the black-box attacks are still effective without needing the assumption that the adversary knows the tuning set size. I don't think this is such an important result to make a subsection about it - maybe should be an appendix, or a shortsection in section that covers all the changes we consider in the threat model (but maybe this is the only one?) Since we don't see to have any discussion of black-box attacks in the previous section, could move this there.} \ynote{I think we can will revise this part according to this comment and add more different threat models in the future. Here, we still include the white-box meta-classifer to show all meta-classifiers are not sensitive to the training set. Our results imply that If there are more powerful meta-classifiers in the future, it will probably have the same property.} \ynote{TODO FUTURE}

Figure~\ref{fig:unknown_size_results} shows the inference results of the meta-classifier-based approaches. For both the white-box and black-box methods, varying the training set size has negligible impact on the inference performance: for the black-box approach, the purple lines stay very close to each other and the AUC scores all exceed 0.8 when $\geq 20$ samples out of the total 5$\,$000 samples have the target property and exceed 0.95 when $\geq 50$ samples are with the property. Similarly, for the white-box meta-classifiers approach, the green lines also stay close to each other and the AUC scores all exceed 0.9 when $\geq 100$ samples have the target property.
%Those results suggest that the training sizes of the shadow models will not affect the performance of the meta-classifiers too much.

%For the white-box meta-classifiers which are served as the baseline in the white-box scenarios, the results (green lines) show a similar trend as the purple lines but have exceptions when 150 samples have the target property for the smile detection task and age prediction task. The possible explanation for the exceptions is that the current state-of-the-art white-box meta-classifiers do not work very well for convolutional models (The downstream models of the smile detection and age prediction tasks are convolutional, while that of the gender recognition task contains only fully-connected layers). And those exceptions will not affect the inference, because we have inference methods (the parameter difference test and the variance test) that achieve better results for white-box scenarios.



\subsection{Importance of Distribution Augmentation} \label{sec:study_distribution_aug}
%\dnote{need more structure this, not just a list of experiments. The previous one was about changing the threat model - is that the only one about changing the threat model?}

% \begin{figure*}[tbp]
%     \centering
%     \includegraphics[width=.85\linewidth]{fig/attack/bn_zero_activation_no_distribution_augmentation_inference_5000.pdf}  
% \caption{Inference AUC scores when upstream models are not trained with distribution augmentation (Section~\ref{sec:attack_design}). All the downstream training sets have 5\,000 samples in these results. The inference targets for the smile detection and age prediction are senior people and Asian people respectively. 
% %and in the following figures in the main body of this paper, we will always use the same inference targets for those two tasks.
% } 
% \label{fig:distribution_augmentation}
% \end{figure*}

% In the attack design (Section~\ref{sec:attack_design}),
In Appendix~\ref{sec:train_data_limitation},
%\dnote{ 
% should be more specific pointer - I find the paragraph in 4.1 about this, but
%it is quite vague and hard to understand what is actually done - if this is important enough to have experimental results on it, need to explain it more clearly and how you are doing it}) 
we introduce distribution augmentation for upstream training, which injects representative samples without the target property into the upstream training set to better achieve the attack goal described in Equation~\ref{eq:x_w_wo_activation}. Figure~\ref{fig:distribution_augmentation} shows the attack performance when we do not use distribution augmentation. The victim training set size is set to 5$\,$000 and other experimental setups are the same as those in Section~\ref{sec:eval_zero_activation_attack}. From the figure, we observe that AUC scores of attacks without distribution augmentation are all less than 0.86, and get even lower ($<0.7$) for gender recognition and smile detection. These scores are significantly lower than the results with distribution augmentation (details in Figure~\ref{fig:zero_activation_attack_results_5000} and \ref{fig:zero_activation_attack_results}). For example, with the augmentation, AUC scores all exceed 0.9 if more than 20 samples are with the target property and the importance of distribution augmentation is thus apparent.

\subsection{AUC values < 0.5} \label{sec:auc<0.5}
We observe that a few attack settings have AUC scores consistently below 0.5. Those rare abnormal AUC scores mainly occur for black-box methods against normal pretrained models (e.g.,
the confidence score test and black-box meta-classifier for the gender recognition with 10$\,$000 downstream samples in Figure~\ref{fig:baseline_results}.) For the confidence score test, by manual inspection, we find its working assumption is not satisfied by the downstream models fine-tuned from normal pretrained models in some settings. The confidence score test assumes models trained with the property perform better on samples with the property than those trained without the property, but an opposite pattern is observed for the queried downstream models. 
As for black-box meta-classifiers, we observe the anomalies happen when the inference tasks are too challenging and the meta-classifiers cannot obtain meaningful information but overfit to the training set (despite early stopping). Specifically, AUC scores are high ($>0.75$) on the training set, $\sim0.5$ on the validation set, and show anomalies ($<$ 0.5) on the test set. We note that the gap between the validation set and the test set is large because they are trained differently. When training downstream models with the target property for the training and validation set, we randomly sample 1-170 samples with the property each time to simulate the real-world case (discussed in Appendix~\ref{sec:details-of-downstream-training}), while for the test set, we randomly sample fixed number of samples with the property for each AUC computation (e.g., 1, 2, ..., 150) to show the trend. %In most of these rare cases, AUC scores still remain low even if the attacker flips predictions ($1 - AUC< 0.7$).  
We reemphasize that those anomalies mainly happen in the non-manipulation settings because of the limitation of inference methods on normal pretrained models when the inference tasks are too challenging. Our proposed manipulation (e.g., providing stronger signal) lowers the difficulty of those challenging cases and leads to better/normal results.

\subsection{Inferring Multiple Properties Simultaneously} \label{sec:multiple_properties}
\begin{figure}[ht]
    \centering
  \includegraphics[width=.45\textwidth]{fig/attack/bn_zero_activation_multiple_5000.pdf} 
\caption{Inference AUC scores when considering multiple properties simultaneously. The inference task is to infer two individuals in the gender recognition setting. The downstream set has 5$\,$000 samples.
} 
\label{fig:infer_multiple_properties}
\end{figure}

In this section, we demonstrate that the attack described in Section~\ref{sec:attack_design} can be extended to infer multiple target properties simultaneously. The method is to simply associate different secreting parameters with each property. We conducted experiments using the gender recognition setting with some modifications. The new target properties are the two individuals with the most samples in VGGFace2. In the upstream training, we inject 285 and 257 samples with the property into the upstream training set for the two individuals respectively; we also inject 1$\,$425 samples without the target properties (distribution augmentation in Appendix~\ref{sec:train_data_limitation}). For each property, the number of scereting activations is 8 (i.e., $\|\boldsymbol m\|_1=8$). For the downstream training, the candidate set has 250 samples for each target property and 200$\,$000 samples without the target properties.
The rest settings are the same as those in Appendix~\ref{sec:hyper-parameter-setup-zero-activation}. 
The manipulation does not affect the accuracy of the main tasks too much (accuracy drop less than 0.6\%). 
The inferences are also highly successful. Figure~\ref{fig:infer_multiple_properties} summarizes the results of the variance test in discriminating downstream models trained with a target property from those trained without target properties. The results show that AUC scores exceed 0.85 when $\geq 10$ out of 5$\,$000 samples are with the property, and are higher than 0.95 when $\geq 50$ samples have the property.

\subsection{Details on Anomaly Detection for Zero-Activation Attack}
\label{sec:defense_details}
We consider three common anomaly detection methods: K-means~\cite{jain1999data}, PCA~\cite{abdi2010principal} and Spectre~\cite{hayase2021spectre}, where Spectre is the current state-of-the-art. 
K-means leverages the k-means clustering technique to identify outliers while PCA leverages principal component analysis to identify the outliers. Spectre is an improved version of PCA and works much better than PCA when the attack signature is weak (i.e., the distributional difference is small)~\cite{hayase2021spectre}.
%originally designed to detect the backdoored samples (can still be viewed as outliers) in model training to defend against backdoor attacks. 
%It works by first finding the target class of the backdoor attack, and then performing anomaly detection with the activations of the samples labeled into the target class. In our case, since samples with the target property can span different output classes, we skip the step of finding the target class and directly conduct anomaly detection with the feature extractor's outputs on the downstream set. 
When conducting the anomaly detection, following the common setup in Hayase et al.,~\cite{hayase2021spectre}, we filter out $1.5 n_t$ ($n_t$ is number of samples with target property) samples, simulating the scenario where the defender does not know the exact $n_t$, but is able to roughly estimate its value and attempt to find most of them.
%\dnote{need to explain how these relate to our problem - the do not seem like appropriate defenses for our setting, so if it makes sense to use them need to explain why}

% \shortsection{Defense Performance Against Zero-Activation Attack}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=.95\linewidth]{fig/defense/bn_clustering_zero_activation.pdf} 
\caption{Percentage of samples with the target property detected by the anomaly detection for the zero-activation attack.
Similar to ~\cite{hayase2021spectre}, we filter out $n \times 1.5$ samples with anomaly detection, where $n$ is the number of samples in downstream training data with the target property. We report the number of samples with the target property filtered out divided by $n$ as the \emph{Detection Percentage}; values are averaged (with standard deviation) over 5 runs of anomaly detection.
The `5K' lines report detection results on the settings with 5$\,$000 total samples, while the `10K' lines report for 10$\,$000 total samples.
%Total number of filtered samples are $1.5$ times the number of actual target samples and the detection percentage is the fraction of actual target samples that are filtered out. We repeat the experiments for 5 times and report the average value and standard deviation.
%The `5K' and `10K' lines report for settings of downstream training sets of 5000 and 10 000 total samples, respectively.
\label{fig:anomaly_detection_zero_activation_attack} }
\end{figure*}

\shortsection{Results of Anomaly Detection} 
\label{sec:defense-zero-activation}
We show the detection performance in Figure~\ref{fig:anomaly_detection_zero_activation_attack}. 
The results show that conducting anomaly detection can filter out majority of samples with the target property in the downstream set and hence, increase the chance of detecting the manipulation.
% lower the effectiveness of the inference \todo{change the story}.
For example, the Spectre defense can filter out 80\% of the samples with the target property in most cases for gender recognition and smile detection, and 60\% for age prediction. Anomaly detection effectively finds samples with the target property
% detects zero activation attacks
because the attack mainly focuses on improving attack effectiveness by increasing the distinction between samples with and without property, which makes the attack signature of samples with property much stronger.  After finding the possible samples with the target property, the defender can then inspect those samples, and try to find the commonalities and then identify the potential target property. Since the process of finding commonalities in the outliers reported by anomaly detection could be trivial (e.g., most samples have the same property or abnormal activations), we do not perform actual experiments for this part. In Section~\ref{sec:stealthier-design-methods}, we propose a stealthier design, in which anomaly detection cannot reliably detect samples with the target property and thus cannot find the manipulation.



%Anomaly detection detects zero activation attacks because the attack mainly focuses on improving attack effectiveness by increasing the distinction between samples with and without property, which makes the attack signature of samples with property much stronger. 

\iffalse
\subsection{Evading Anomaly Detection} \label{sec:further-evade-anomaly-detection}
Using the attack design above (i. e., choosing proper value of $\lambda$ that balances attack effectiveness and stealthiness), our attack is able to evade anomaly detection methods in most settings. However, in some settings (mostly in gender recognition tasks), state-of-the-art anomaly detection (Spectre) can still identify most of the samples with target property easily. The main reason for this detectability is that for these settings the attack signature of samples with target property is still strong and is easily distinguishable from samples without the property~\cite{hayase2021spectre}. Therefore, we add an additional term (weighted parameter $\gamma$) to the overall loss function $l(\boldsymbol x, y, y_t)$ in \autoref{eq:zero_activation_loss} to further reduce the attack signature while still maintaining relatively high attack effectiveness. Specifically, we first obtain the corresponding covariance matrices of the (all) activations of only samples with the target property ($\boldsymbol{cov_w}$), activations of samples with and without the target property ($\boldsymbol{cov_{w,wo}}$), and activations of only samples without the target property($\boldsymbol{cov_{wo}}$) respectively. Then, we encourage $mean(\boldsymbol{cov_w)} = mean(\boldsymbol{cov_{w,wo}}) = mean(\boldsymbol{cov_{wo}})$ and $var(\boldsymbol{cov_w}) = var(\boldsymbol{cov_{w,wo}}) = var(\boldsymbol{cov_{wo}})$ (both $mean(\cdot), var(\cdot)$ treats the whole covariance matrix as a flattened array and returns a scalar value) of the three covariance matrices by minimizing their differences in their mean and variance. Using this method, we ensure the distributions of activations of samples with target property will be similar to the ones without property and hence, is harder to detect. For consistency, we use this modified approach for all of our experiments (including settings where proper value of $\lambda$ can also already evades detection).
\fi


\subsection{Experimental Setup of Stealthier Attacks}\label{sec:experimental-setup-stealthier-attacks}
In Section~\ref{sec:stealthier-design-results}, when preparing upstream models, for $\boldsymbol m$, we randomly select 16 activations out of total 1$\,$280 for the gender recognition and also select 196 activations out of total 50$\,$176 for smile detection and age prediction.
In practice, the total number of channels in convolutional kernels is not very large and therefore, the defender may still be able to brute-force the manipulated activations if $
\boldsymbol m$ is chosen only at the channel level. Thus, we also choose to select secreting activations directly for tasks where the first layer of the downstream model is convolutional, which may reduce some of the attack effectiveness.
%Another small change is, if the first layer of downstream model is convolutional, we still generate $\boldsymbol m$ for the secreting activations (which may lead to some slight loss in attack effectiveness) instead of generating it for the channels. We do this because we are dealing with an active defender who knows the attack method and generating $\boldsymbol m$ this way can significantly increase the difficulty of defender guessing the secreting activations (more details in the analysis of stealthiness below). 
For $\lambda$, we prefer a larger value for better inference effectiveness while still evading anomaly detection. Therefore, we performed a linear search starting from 1 and incrementing it by 0.5, and terminating when the attack can no longer evade the mentioned anomaly detection methods. With this strategy, we set $\lambda=2$ for gender recognition, $\lambda=1.5$ for smile detection and age detection when the inference targets are senior people and Asian people respectively, and $\lambda=1$ for smile detection and age detection when the inference targets are specific individuals. $\alpha$, $\beta$, and $\gamma$ are all set to be 1 in the experiments. 

\subsection{Adaptive Activation Distribution Checking} \label{sec:adaptive_activation_distribution_checking}
The activation distribution checking method needs to be adjusted based on the specific attack method used. Using the modified loss design in Section~\ref{sec:stealthier-design-methods}, our stealthier attack can automatically evade distribution checking of abnormal zeros, as the secreting activations of samples without target property are also non-zero. Hence, we need to design adaptive detection based on activation distribution checking for the modified attack loss. 

With the modified attack loss, we find that activations of samples with the property mixes well with ones without the property, and we fail to find a principled method to distinguish their distribution using the overall activations. Because of the design of the attack loss, the main distributional difference %always
comes from the distributional difference in the secreting activations for samples with and without property (i.e., distributional difference is most significant when we only measure secreting activations), to make progress, we assume the defender will follow a two-stage strategy of
first identifying the selected secreting activations and then identifying the distributional difference in the potential secreting activations, with a hope that the distributional difference is significant enough to be detected\footnote{We do not exclude the possibility of identifying the distributional difference by still checking the overall distribution, and leave further exploration of such detection strategies as future work.}. 

Since $\boldsymbol m$ is randomly generated with proper number of nonzeros, the brute-force strategy for identifying $\boldsymbol m$ is computationally infeasible. For example, for gender recognition experiments, defenders have to try a total of $\binom{1,280}{16}$ ($>2e36$) forms of $\boldsymbol m$ (i.e., $\|\boldsymbol m\|_1=16$ for a total of 1,280 activations). Therefore, alternatively, we present two methods that attempt to approximately identify $\boldsymbol m$ with the hope that the approximately well identified $\hat{\boldsymbol m}$ still preserves the significant distributional difference of $\boldsymbol m$. 
%In practice, the defender may not need to exactly identify $\boldsymbol{m}$, instead we assume approximately recovering most of the activations in $\boldsymbol{m}$ may also be sufficient~\footnote{Since the activation checking based defense is mostly a proof-of-concept, we assume identifying the secreting activations with sufficiently high detection rate implies the defender can also easily spot if the given model is manipulated or not. Similarly, if the detection rate is low, we assume the defender cannot confidently judge if the model is manipulated. We leave in-depth investigation of defenses along this path as the future work.}. Therefore, we also design approaches that attempt to approximately identify most of the actual secreting activations. 
The two methods we design are based on the fact that: 1) samples with the target property are rare for practically interesting settings, and 2) in the modified loss design, secreting activations of samples without the property are smaller in magnitude than the ones of samples with the property. Therefore, if we randomly feed inputs to the model, most of the inputs are without property and hence, their corresponding secreting activations should be smaller. With these two principles, we design two detection methods: the first one averages the outputs of each activation for all the fed inputs and treats activations with smaller average values as the potential secreting activations (\emph{average value based detection});
the second approach handles individual input separately and identifies potential secreting activations for each of them, and then returns the intersection for all the potential secreting activations identified (\emph{intersection based detection}). Empirically, we find that both approaches cannot identify the secreting activations well (details are shown below) %(\autoref{sec:stealthier-design-results}) 
and hence did not further explore how to check distributional difference on the identified secreting activations in this paper. 

\shortsection{Experimental Settings}
To evaluate the performance of \textit{average value based detection}, we measure the detection rate, which is the fraction of actual secreting activations in identified potential activations.  For the \textit{intersection based method}, since the size of final returned secreting activations can vary (due to intersection over multiple inputs) for different settings, we evaluate the defense performance by reporting their F1-score (viewing actual target as the positive class and others as negative). When running these two detections, we consider an idealized scenario for the defender, where all the randomly sampled inputs are without target property and so, their secreting activations are even smaller for manipulated models and are easier to be detected by the defender.

Specifically, for average value based detection, we choose $n \times 1.5$ activations that have the smallest average values as the identified possible secreting activations ($n_{ip}$), where $n$ is the number of actual secreting activations ($n=\|\boldsymbol m\|_1$). We report the number of  identified actual secreting activation ($n_{ia}$) divided by $n$ as the detection rate.  For intersection based detection, the $n_{ip}$ of this method is the number of activations remained after intersection operations, and we cannot precisely control this number. Therefore, only reporting the detection rate like the average value based detection could introduce bias, and we use the F1-score as the metric instead, where the precision is defined as $\frac{n_{ia}} {n_{ip}}$ and the recall is defined as $\frac{n_{ia}} {n}$. And for this detection method, for each sample, we also need to select some activations that have the smallest values as the inputs for conducting the intersection operation. We tried many choices for the number of those activations, and find that choosing $n \times 5$ smallest activations for each sample achieves the best F1-score. In the experiments, we tried to use 100, 500, 1$\,$000, 2$\,$000, 4$\,$000, 8$\,$000, 10$\,$000 samples to generate  activations values, separately. For each setting, we repeat  each detection 5 times and calculate the average value of the detection rate or F1-score. 

\shortsection{Detection Results} Empirically, we find that the two approaches cannot sufficiently identify the secreting activations --- the detection rate of secreting activations of the first method is less than 11.3\% for gender recognition and is less than 1.5\% for smile detection and age prediction for all settings; the F1-score of the secreting activation detection of the second method is less than 0.009 for all settings. In fact, using the second approach, the returned secreting activations are empty sets in most settings, implying the difficulty of identifying the secreting activations by simply checking the magnitude. Overall, the detection performances of both approaches are low and better detection methods are needed for identifying $\boldsymbol{m}$ in the future. %More details of the experiments and results are in Section~\ref{sec: details_distribution_checking} in the appendix.


\iffalse
\subsection{Details of experiments for distribution checking for stealthier attacks} \label{sec: details_distribution_checking}
As described in Section~\ref{sec:stealthier-design-results}, we have two attempts to identify possible secreting activations: average value based detection and intersection based detection.


For average value based detection, we choose $n \times 1.5$ activations that have the smallest average values as the identified possible secreting activations ($n_{ip}$), where $n$ is the number of actual secreting activations. We report the number of  identified actual secreting activation ($n_{ia}$) divided by $n$ as the detection rate.  For intersection based detection, the $n_{ip}$ of this method are the number of activations remained after intersection operations, and we cannot precisely control this number. Therefore, only reporting the detection rate like the average value based detection could introduce bias, and we use the F1-score as the metric instead, where the precision is defined as $\frac{n_{ia}} {n_{ip}}$ and the recall is defined as $\frac{n_{ia}} {n}$. And for this detection method, for each sample, we also need to select  activations that have smallest values as the inputs for conducting the intersection operation. We tried many choices for the number of small activations, and find that $n \times 5$ achieves the best F1-score.

In the experiments, we tried to use 100, 500, 1000, 2000, 4000, 8000, 10000 samples to generate  activations values, separately. For each setting, we repeat  each detection 5 times and calculate the average value of the detection rate or F1-score. For average value based detection, the detection rate is less than 11.3\% for gender recognition, i.e., 1 or 2 out of 16 actual activations are correctly pinpointed, and is less than 1.5\% for smile detection and age prediction.And for intersection based detection, we find that in most settings, the detection cannot find any actual secreting activations and the maximum value of the F1-score across all the settings is less than 0.009.
\fi

% \subsection{Additional Figures}



% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=1\linewidth]{fig/attack/b_zero_activation_inference_with_baseline_5000.pdf}
% % \begin{subfigure}{1\textwidth}
% %   \centering
% %   \includegraphics[width=1\linewidth]{fig/attack/b_zero_activation_inference_5000.pdf}  
% %   \caption{Gender recognition (10000)}
% %   \label{fig:sub-first}
% % \end{subfigure}
% \caption{Inference AUC scores when the upstream model is trained with the attack goals described in Section~\ref{sec:zero_activation_attack}. 
% The results are the settings where the sizes of the downstream training set are 5$\,$000, and the results of the 10$\,$000 samples are found in Figure~\ref{fig:zero_activation_attack_results}.}
% \label{fig:zero_activation_attack_results_5000}
% \end{figure*}

% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=1\linewidth]{fig/attack/b_stealthier_attack_5000.pdf} 
% \caption{Inference AUC scores of the stealthy design. We omit results of the confidence score test, since they usually show inferior performance.
% The size of the downstream training sets for those experiments are all 5 000. Results for 10 000 samples are similar (Figure~\ref{fig:stealthier_attack_results_10000}, Appendix).
% %\anote{I think we can have a common legend like we do for the Figures above (if that's possible without a lot of hassle)}
% \label{fig:stealthier_attack_results_5000}}
% \end{figure*}





%%%%%%%%%%%%%%

% \begin{figure*}[htbp]
% \centering
% \begin{minipage}[t]{0.49\textwidth}
%     \centering
%     \includegraphics[width=1\linewidth]{fig/attack/bn_baseline_inference_5000_t_individual.pdf}  
%     \caption{Inference AUC scores when the upstream model is not trained with attack goals. The downstream training sets all have 5$\,$000 samples in the results. The inference targets are specific individuals for smile detection and age prediction; the results of other inferences show a similar trend and are found in Figure~\ref{fig:baseline_results}.}
%     \label{fig:baseline_results_t_individual}
% \end{minipage}
% \hfill
% \begin{minipage}[t]{0.49\textwidth}
%     \centering
%     \includegraphics[width=1\linewidth]{fig/attack/bn_zero_activation_inference_with_baseline_5000_t_individual.pdf}  
% \caption{Inference AUC scores when the upstream model is trained with the attack goals described in Section~\ref{sec:zero_activation_attack}. 
% The downstream training sets all have 5$\,$000 samples in the results.
% The inference targets are specific individuals for smile detection and age prediction; the results of other inferences show a similar trend and are found in Figure
% ~\ref{fig:zero_activation_attack_results}
% }
% \label{fig:zero_activation_attack_results_t_individual}
% \end{minipage}
% \end{figure*}

% \begin{figure*}[tbp]
%     \centering
%     \includegraphics[width=0.7\linewidth]{fig/attack/bn_baseline_inference_5000_t_individual.pdf}  
% \caption{Inference AUC scores when the upstream model is not trained with attack goals. The downstream training sets all have 5$\,$000 samples in the results. The inference targets are senior people and Asian people for smile detection and age prediction, respectively; other inferences are found in Figure~\ref{fig:baseline_results}.}
% \label{fig:baseline_results_t_individual}
% \end{figure*}

% \begin{figure*}[tbp]
%     \centering
%     \includegraphics[width=.7\linewidth]{fig/attack/bn_zero_activation_inference_with_baseline_5000_t_individual.pdf}  
% \caption{Inference AUC scores when the upstream model is trained with the attack goals described in Section~\ref{sec:zero_activation_attack}. 
% The downstream training sets all have 5$\,$000 samples in the results.
% The inference targets are senior people and Asian people for smile detection and age prediction, respectively; other inferences are found in Figure
% ~\ref{fig:zero_activation_attack_results}
% }
% \label{fig:zero_activation_attack_results_t_individual}
% \end{figure*}



\begin{figure*}[ht]
    \centering
    \includegraphics[width=.95\linewidth]{fig/attack/bn_zero_activation_inference_with_baseline_5000.pdf}
\caption{Inference AUC scores when the upstream model is trained  %\dnote{not about the attack goals here, how is it trained?} 
with the attack method described in Section~\ref{sec:attack_design}.
Baseline scores (the \emph{baseline} lines) are the maximum of the AUC scores (of the three inference methods) of the baseline experiments in  Appendix~\ref{sec:eval_baseline}.
%In the gender recognition task, the downstream trainer reinitializes the classification module and the attacker does not know the initialization. Therefore, inference method based on parameter difference test is not applicable.
%The inference targets for the smile detection and age prediction are senior people and Asian people respectively.  
The inference of specific individuals for smile detection and age prediction are similarly successfully and found in Figure~\ref{fig:zero_activation_attack_results_t_individual} in the appendix. The downstream training sets have 5$\,$000 samples in the results,  and the results for the 10$\,$000 samples are in Figure~\ref{fig:zero_activation_attack_results}.
}
\label{fig:zero_activation_attack_results_5000}
\end{figure*}



\begin{figure*}[ht]
    \centering
    \includegraphics[width=.75\linewidth]{fig/attack/bn_zero_activation_query_tuning_10000.pdf}
\caption{Inference AUC scores of black-box meta-classifiers equipped with and without query tuning. 
We reuse the upstream and downstream models trained in Figure~\ref{fig:zero_activation_attack_results}.
}
\label{fig:query_tuning}
\end{figure*}


\begin{figure*}[tbp]
    \centering
    \includegraphics[width=.75\linewidth]{fig/attack/bn_baseline_inference_5000_t_individual.pdf}  
    \caption{Inference AUC scores when the upstream model is not trained with attack goals. The first and second rows show results when downstream training sets contain 5$\,$000 and 10$\,$000 samples respectively. The inference targets are specific individuals for smile detection and age prediction; the results of other inferences show a similar trend and are found in Figure~\ref{fig:baseline_results}.}
    \label{fig:baseline_results_t_individual}
\end{figure*}


\begin{figure*}[tbp]
    \centering
    \includegraphics[width=.75\linewidth]{fig/attack/bn_zero_activation_inference_with_baseline_5000_t_individual.pdf}  
\caption{Inference AUC scores when the upstream model is trained with the attack goals described in Section~\ref{sec:attack_design}. The first and second rows show results when downstream training sets contain 5$\,$000 and 10$\,$000 samples respectively.
%The downstream training sets all have 5$\,$000 samples in the results.
The inference targets are specific individuals for smile detection and age prediction; the results of other inferences show a similar trend and are found in Figure~\ref{fig:zero_activation_attack_results}.
}
\label{fig:zero_activation_attack_results_t_individual}
\end{figure*}



\begin{figure*}[ht] %[htbp]
    \centering
    \includegraphics[width=.95\linewidth]{fig/defense/bn_clustering.pdf} 
\caption{Percentage of samples with the target property detected by the anomaly detection for the stealthier attack. 
Similar to ~\cite{hayase2021spectre}, we filter out $n \times 1.5$ samples with anomaly detection, where $n$ is the number of samples in downstream training data with the target property. We report the number of samples with the target property filtered out divided by $n$ as the \emph{Detection Percentage}; values are averaged (with standard deviation) over 5 runs of anomaly detection.
The `5K' lines report detection results on the settings with 5$\,$000 total samples, while the `10K' lines report for 10$\,$000 total samples. Inference targets for smile detection and age prediction are senior people and Asian people respectively; results for the inference of specific individuals follow similar trends (Figure~\ref{fig:anomaly_detection_stealthier_attack_t_individual}).
\label{fig:anomaly_detection_stealthier_attack} }
%\vspace{-1em}
\end{figure*}


\begin{figure*}[htbp]
    \centering
    \includegraphics[width=.95\linewidth]{fig/attack/bn_stealthier_attack_5000.pdf} 
\caption{Inference AUC scores of the stealthier design. Since the secreting activations are no longer zero, the inference methods based on difference or variance tests are no longer applicable. 
Inference targets for the smile detection and age prediction are senior people and Asian people respectively; inference of specific individuals also shows improvement compared to the baseline settings (Figure~\ref{fig:stealthier_attack_results_t_individual}). The downstream training sets have 5$\,$000 samples in the results; results for 10$\,$000 samples show similar trends and are in Figure~\ref{fig:stealthier_attack_results_10000}.
\label{fig:stealthier_attack_results_5000}}
\end{figure*}


\begin{figure*}[htbp]
    \centering
    \includegraphics[width=.75\linewidth]{fig/defense/bn_clustering_t_individual.pdf} 
    \caption{Percentage of samples with the target property detected by anomaly detection for the stealthier attack. The inference targets are specific individuals for smile detection and age prediction; the results of other inferences show a similar trend and are found in Figure~\ref{fig:anomaly_detection_stealthier_attack}.
    \label{fig:anomaly_detection_stealthier_attack_t_individual} }
\end{figure*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=.75\linewidth]{fig/attack/bn_stealthier_attack_5000_t_individual.pdf} 
    \caption{Inference AUC scores of the stealthier attack. % The downstream training sets all have 5$\,$000 samples in the results. 
    The first and second rows show results when downstream training sets contain 5$\,$000 and 10$\,$000 samples respectively.
    The inference targets are specific individuals for smile detection and age prediction; the results of other inferences show a similar trend and are found in Figure~\ref{fig:stealthier_attack_results_10000}.
    \label{fig:stealthier_attack_results_t_individual}}
\end{figure*}



% \begin{figure*}[htbp]
% \centering
% \begin{minipage}[t]{0.49\textwidth}
%     \centering
%     \includegraphics[width=1\linewidth]{fig/defense/bn_clustering_t_individual.pdf} 
%     \caption{Percentage of samples with the target property detected by anomaly detection for the stealthier attack. The inference targets are specific individuals for smile detection and age prediction; the results of other inferences show a similar trend and are found in Figure~\ref{fig:anomaly_detection_stealthier_attack}.
%     \label{fig:anomaly_detection_stealthier_attack_t_individual} }
% \end{minipage}
% \hfill
% \begin{minipage}[t]{0.49\textwidth}
%     \centering
%     \includegraphics[width=1\linewidth]{fig/attack/bn_stealthier_attack_5000_t_individual.pdf} 
%     \caption{Inference AUC scores of the stealthier attack. % The downstream training sets all have 5$\,$000 samples in the results. 
%     The first and second rows show results when downstream training sets contain 5$\,$000 and 10$\,$000 samples respectively.
%     The inference targets are specific individuals for smile detection and age prediction; the results of other inferences show a similar trend and are found in Figure~\ref{fig:stealthier_attack_results_10000}.
%     \label{fig:stealthier_attack_results_t_individual}}
% \end{minipage}
% \end{figure*}



% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=1\linewidth]{fig/defense/bn_clustering_t_individual.pdf} 
% \caption{Percentage of samples with the target property detected by the anomaly detection. Similarly to ~\cite{hayase2021spectre}, we use the anomaly detection method to filter out $n \times 1.5$ samples, where $n$ is the number of samples in downstream training data with the target property. We report the number of samples with the target property filtered out divided by $n$ as the \emph{Detection Percentage}. We report the averaged values and the standard deviation over 5 runs of anomaly detection.
% The `5K' lines report detection results on the settings with 5 000 total samples, while the `10K' lines report for 10 000 total samples.
% \label{fig:anomaly_detection_stealthier_attack_t_individual} }
% \end{figure*}

% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=.7\linewidth]{fig/attack/bn_stealthier_attack_5000_t_individual.pdf} 
% \caption{Inference AUC scores of the stealthy design. Since we do not introduce zeros for the secreting activations, we cannot use the difference test and the variance test but are left with meta-classifiers and the confidence score test. The downstream training sets have 5$\,$000 samples in the results in the first row and have 10$\,$000 samples for the second row. The inference targets for the smile detection and age prediction are senior people and Asian people respectively; the inference of specific individuals for those two tasks also shows improvement compared to the baseline settings  and are found in Figure~\ref{} in the appendix.
% \label{fig:stealthier_attack_results_t_individual}}
% \end{figure*}