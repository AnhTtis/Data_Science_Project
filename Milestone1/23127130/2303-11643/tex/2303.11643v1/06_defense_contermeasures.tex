% \section{Defenses and Stealthier Attacks} 

\section{Stealthier Manipulation}
\label{sec:possible_defense}
\iffalse
We assume the victim/defender knows the attack method used for training the pretrained models, but not the specific selected secreting activations, which is usually a secret to the individual adversary. We also assume the number of samples with target property is limited in the downstream training set, as such properties are less likely to be revealed by the normally trained models and are practically more interesting for the attackers. As mentioned in \autoref{sec:threat_model}, we also assume the victim/defender is also unaware of the property targeted by the adversary, but knows the target property has a limited presence in its downstream training set. 
\fi

% \begin{figure*}[ht] %[htbp]
%     \centering
%     % \includegraphics[width=.85\linewidth]{fig/defense/bn_clustering.pdf} 
%     \includegraphics[trim={0.2cm 0cm 0.2cm 0cm}, clip,width=.8\linewidth]{fig/defense/bn_summarize_clustering.pdf} 
% \caption{Percentage of samples with the target property detected by the best anomaly detection method. 
% Similar to Hayase et al.,~\cite{hayase2021spectre}, we filter out $n \times 1.5$ samples with anomaly detection, where $n$ is the number of samples in downstream training data with the target property. \emph{Detection Percentage} is the number of samples with the target property being filtered out divided by $n$ and report the \emph{maximum} of the detection percentage of the three anomaly detection methods (detailed results are in Figures~\ref{fig:anomaly_detection_zero_activation_attack},~\ref{fig:anomaly_detection_stealthier_attack} in the appendix).
% %values are averaged (with standard deviation) over 5 runs of anomaly detection.
% The `5K' lines report detection results on settings with 5,000 total samples, while `10K' lines report for 10,000 total samples. 
% % Inference targets for smile detection and age prediction are senior people and Asian people respectively; results for specific individuals follow similar trends (Figure~\ref{fig:anomaly_detection_stealthier_attack_t_individual} in the appendix).
% Detection results for inferring specific individuals for smile detection and age prediction are similar (Figure~\ref{fig:anomaly_detection_stealthier_attack_t_individual} in the appendix).
% \label{fig:anomaly_detection_zero_activation_attack_stealthier_attack} }
% \vspace{-0.3cm}
% \end{figure*}


The attack described in Section~\ref{sec:attack_design} introduces obvious artifacts in the pretrained model, which can be utilized for detection by a downstream model trainer aware of the risks posed by our attacks. We first present two detection methods
% that can easily detect the original zero-activation attack 
(Section~\ref{sec:detect_pretrained}) and then demonstrate how to make the model manipulation stealthier to evade detection while still preserving the inference effectiveness (Section~\ref{sec:stealthier-design-methods} and Section~\ref{sec:stealthier-design-results}).
We assume the downstream trainer is aware of the possibility of the attack and its design, but does not know the property targeted by the adversary, as this is specific to an attacker's goal and the set of possible properties can be exponentially large for a rich training set. 


%\subsection{Possible Defenses} \label{sec:possible_defense}

%\dnote{need to condense this a lot, and make some room to actually include results in the paper body}

\subsection{Detecting Manipulated Pretrained Models}\label{sec:detect_pretrained}
We present two detection methods that use the distributional difference between activations of samples with and without property. 
%\dnote{I think we are only considering ways to detect a bad pretrained model, not other types of defenses?}

%design nature of the attack, and is a proof-of-concept defense, as the actual application of this defense may require many engineering efforts and domain knowledge (e.g., setting proper threshold for individual tasks) and is out of the scope of this paper. \dnote{this is a very strange thing to say - is it really just that we didn't have time to test it? Is this talking about the "Activation Distribution Checking" paragraph or something else?} 
%The second type of defense is based on anomaly detection and can be directly used as a practical defense.
%\dnote{I don't understand what the "two types" of defenses are. The most obvious kinds of defenses are (1) ones that check the pretrained model for manipulation, and (2) ones that do something in the tuning/deployment process to mitigate the property inference. I'm not sure what the "two types" we are considering are though, so should state at a high level the general defense types, and then the specific defenses we consider and evaluate.}

\shortsection{Checking the Distribution of Activations}
Since the distributional difference between activations of samples with and without target property is significant, this defense focuses on spotting this difference to identify manipulated models. A method to identify the distributional difference needs to be designed based on the attack method used. For the original zero-activation attacks in Section~\ref{sec:zero_activation_attack}, since the secreting activations of samples without property are all 0, the defender can feed random training samples to the pretrained models and check if there are abnormally many 0s. This approach is feasible since samples of target property have limited presence in the downstream training set and hence, most samples will not have the property. 
Since detecting the zero-activation attack is trivial using this method, we do not conduct any experiments with this. %perform actual experiments and assume a stealthier attack is needed. 

% The design of more involved attacks that no longer rely on generating abnormally many 0s and also the corresponding distribution checking detection for the involved attacks is given in~\cref{sec:stealthier-design-methods}.

%the defense may trivially detect the manipulated pretrained models, as they will produce abnormally many 0s if the attacker feeds some random training samples without the target property to the models. The likelihood of finding random samples without the property is high because samples of target property have limited presence in the downstream training set. Such defense becomes more interesting when the attacker no longer relies on generating zero activations for samples without target property and more details is in~\autoref{sec:stealthier_design}. 
%If the attack method is made public, the downstream trainer can directly inspect outputs of the feature extractor to see if the distribution of the activations is unusual. For example, for the zero-activation attack, the victim can check for an abnormal number of zeros in the outputs of the feature extractor to identify the  attack. For the zero activation defense, we believe this defense can be effective as manipulated upstream model will produce many more abnormal zeros in the activations, compared to normally trained upstream models.
%\dnote{if this is all we have to say about this, I don't think we can consider it a "Defense" that was evaluated. I thought you had done some experiments on this?}


\shortsection{Anomaly Detection} 
%In general, adversaries are more interested in inferring properties with limited presence in the downstream tasks, as those are less likely to be revealed by normally trained models. 
%Since the target property has limited presence in the downstream training set, a trivial yet effective defense would thus be to remove all samples with the target property from downstream data while having negligible impact on downstream model performance.
Since the target property has a limited presence in the downstream training set, another defense would be treating samples with the target property as outliers and then analyzing those outliers to find manipulations.
% Therefore, an effective defense is to identify and remove the small number of samples with the target property from the downstream set and train models on the rest.
% When the target property is known to the downstream trainer, then the defense is trivial, but in
%In practice, it is impossible for the victim to know the target property beforehand as it is specific to an attacker's goal. 
%\dnote{?? I don't see why this is impossible - we are just assuming that the victim does not know the property the adversary might care about, but there are exponentially (?) many properties the victim would care about an adversary learning?}
%Although the target property is unknown to the defender, we can still design a successful defense by leveraging expected distributional differences between samples with and without a property that some adversary may want to target. 
% Anomaly detection methods~\cite{jain1999data,abdi2010principal,hayase2021spectre} aim to identify the small fraction of outliers that are distributed differently from other normal data points.
Existing anomaly detection methods~\cite{jain1999data,abdi2010principal,hayase2021spectre} can be adapted to detect manipulated pretrained models in our setting because: 1) the number of samples with the property is of small fraction and 2) their activation distribution is significantly different (i.e., outliers) from the distribution for samples without the property. The auditor can inspect model activations for all of its training data and identify outliers (ideally, samples with target property) with anomaly detection. The auditor can then inspect identified outliers and may find commonalities to identify the potential target property. For instance, they may find that a small fraction of the training data produce unusual model activations, and then notice that most of that data has a particular property such as belonging to a specific individual or group. %In this paper, we consider three anomaly detection methods: PCA~\cite{abdi2010principal}, K-means~\cite{jain1999data} and Spectre~\cite{hayase2021spectre}, which is the current state-of-the-art. Our experiments suggest that these defenses are indeed effective (Appendix~\ref{sec:defense_details}).

We consider three common anomaly detection methods: K-means~\cite{jain1999data}, PCA~\cite{abdi2010principal} and Spectre~\cite{hayase2021spectre} (where Spectre is the current state-of-the-art) and we report the detection results from the three defenses. Appendix~\ref{sec:defense_details} gives details of these methods. The detection results on the zero-activation attack are given in %Figure~\ref{fig:anomaly_detection_zero_activation_attack_stealthier_attack}  
Figure~\ref{fig:anomaly_detection_zero_activation_attack} in the appendix. Anomaly detection is very effective at identifying the samples with target property. For example, for the gender recognition and smile detection tasks, the detection rate is over 80\% in most cases. 
%Zero-activation attacks are easily detected because the attack signatures of target samples are too strong after upstream manipulation and
These results motivate the design of stealthier attacks which we describe next.
%introduce in Section~\ref{sec:stealthier-design-methods}.
%The results show that conducting anomaly detection can filter out majority of samples with the target property in the downstream set and hence, lower the effectiveness of the inference. For example, the Spectre defense can filter out 80\% of the samples with the target property in most cases for gender recognition and smile detection, and 60\% for age prediction. Anomaly detection detects zero activation attacks because the attack mainly focuses on improving attack effectiveness by increasing the distinction between samples with and without property, which makes the attack signature of samples with property much stronger. 


\iffalse
K-means leverages k-means clustering technique to identify outliers while PCA leverages principal component analysis to identify the outliers. Spectre is an improved version of PCA and works much better than PCA when the attack signature is weak (i.e., the distributional difference is small)~\cite{hayase2021spectre}.

When conducting the anomaly detection, following the common setup in Hayase et al.,~\cite{hayase2021spectre}, we filter out $1.5 n_t$ ($n_t$ is the number of samples with target property) samples, simulating the scenario where the defender does not know the exact $n_t$, but is able to roughly estimate its value and attempt to filter out most of them.
\fi

% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=.85\linewidth]{fig/defense/bn_clustering_zero_activation.pdf} 
% \caption{Percentage of samples with the target property detected by the anomaly detection for the zero-activation attack. Total number of filtered samples are $1.5$ times the number of actual target samples and the detection percentage is the fraction of actual target samples that are filtered out. We repeat the experiments for 5 times and report the average value and standard deviation.
% The `5K', `10K' lines report for settings of downstream training set of 5000 and 10 000 total samples.
% \label{fig:anomaly_detection_zero_activation_attack} }
% \end{figure*}

%\shortsection{Results of Anomaly Detection} 
%\label{sec:defense-zero-activation}
%We show the detection performance in Figure~\ref{fig:anomaly_detection_zero_activation_attack}. 
%The results show that conducting anomaly detection can filter out majority of samples with the target property in the downstream set and hence, lower the effectiveness of the inference. For example, the Spectre defense can filter out 80\% of the samples with the target property in most cases for gender recognition and smile detection, and 60\% for age prediction. Anomaly detection detects zero activation attacks because the attack mainly focuses on improving attack effectiveness by increasing the distinction between samples with and without property, which makes the attack signature of samples with property much stronger. 

% These results suggest that
%Since the original zero-activation attack can be easily defended, more investigation is needed to find out whether more sophisticated attacks that exist and evade the proposed detections. 
%Next, we propose stealthy attack designs (Section~\ref{sec:stealthier-design-methods}) and show that they can circumvent detection while achieving high inference AUC scores (Section~\ref{sec:stealthier-design-results}).


% \begin{figure*}[ht] %[htbp]
%     \centering
%     % \includegraphics[width=.85\linewidth]{fig/defense/bn_clustering.pdf} 
%     \includegraphics[width=.85\linewidth]{fig/defense/bn_summarize_clustering.pdf} 
% \caption{Percentage of samples with the target property detected by the anomaly detection. 
% Similar to ~\cite{hayase2021spectre}, we filter out $n \times 1.5$ samples with anomaly detection, where $n$ is the number of samples in downstream training data with the target property. We define the number of samples with the target property filtered out divided by $n$ as the \emph{Detection Percentage} and  report the \emph{maximum} of the detection percentage of the three anomaly detection methods (detailed results are in Figures~\ref{fig:anomaly_detection_zero_activation_attack},~\ref{fig:anomaly_detection_stealthier_attack} in the appendix).
% %values are averaged (with standard deviation) over 5 runs of anomaly detection.
% The `5K' lines report detection results on settings with 5,000 total samples, while `10K' lines report for 10,000 total samples. Inference targets for smile detection and age prediction are senior people and Asian people respectively; results for specific individuals follow similar trends (Figure~\ref{fig:anomaly_detection_stealthier_attack_t_individual} in the appendix).
% \label{fig:anomaly_detection_zero_activation_attack_stealthier_attack} }
% %\vspace{-1em}
% \end{figure*}


\subsection{Stealthier Model Manipulation} \label{sec:stealthier-design-methods}
%\anote{Can be shortened- taking up a lot of space in main paper.}
%\dnote{I'm still confused about the "two types of attacks" - in parts it seems like there are two detection methods we are considering, and include here, but setup said only one is considered - I don't have enough understand what is going on here. If there are two detection methods, we should state this clearly, and keep it consistent throughout.}

\begin{figure*}[ht]
    \centering
    \includegraphics[trim={0.2cm 0cm 0.2cm 0cm}, clip,width=.80\linewidth]{fig/attack/bn_stealthier_attack_10000.pdf} 
\vspace{-0.3cm}
\caption{Inference AUC scores of the stealthier design. Since the secreting activations are no longer zero, the inference methods based on difference or variance tests are no longer applicable. The inference results of specific individuals for smile detection and age prediction also show similar improvement compared to the baseline settings (Figure~\ref{fig:stealthier_attack_results_t_individual} in the appendix).
%Inference targets for the smile detection and age prediction are senior people and Asian people respectively; inference of specific individuals also shows improvement compared to the baseline settings (Figure~\ref{fig:stealthier_attack_results_t_individual}; Appendix). 
The downstream training sets contain 10$\,$000 samples and inference results results of 5$\,$000 samples are similar and given in Figure~\ref{fig:stealthier_attack_results_5000} in the appendix.
\label{fig:stealthier_attack_results_10000}}
\vspace{-0.5cm}
\end{figure*}

% \shortsection{Evading Activation Distribution Checking} 
To evade the defense that checks the distribution of activations, we modify our zero-activation attack to ensure: (1) secreting activations for samples without the property are also non-zero (bypassing simple defense of checking abnormal zeros); (2) secreting activations of samples with and without target property are still distinct (the attack is still effective); (3) that distinction between activations should not be captured by anomaly detection methods (evading anomaly detection); (4) the actual distribution of activations that matches the attacker's goal cannot be easily guessed by the defender (handling cases when the defender actively searches other patterns in the distribution of activations).


For (1) and (2), we adapt the loss in  Equation~\ref{eq:x_w_wo_activation} as \vspace{-0.2cm}
\begin{equation}
\begin{cases}
 \;\alpha \cdot \max(\Vert f(\boldsymbol x) \circ  \boldsymbol m\Vert - \Vert f(\boldsymbol x) \circ \neg \boldsymbol m \Vert, 0)  &\text{if $y_t=0$}\\
 \;\beta \cdot \max(\lambda \cdot \Vert f(\boldsymbol x) \circ \neg \boldsymbol m  \Vert - \Vert f(\boldsymbol x) \circ \boldsymbol m \Vert, 0) &\text{if $y_t = 1$}
\end{cases}
\label{eq:x_w_wo_activation_stealthier}
\vspace{-0.2cm}
\end{equation} 
where $\lambda \geq 1$. (1): The case of $y_t=0$ is redefined to bypass the detection of abnormal zeros. Minimizing this new loss ensures that samples without the target property will have secreting activations ($f(\boldsymbol x) \circ  \boldsymbol m$) with (close-to-normal) non-zero values. 
%(1): to bypass the detection of abnormal zeros, we first redefine the loss in Equation~\ref{eq:x_w_wo_activation} for the case of $y_t=0$ as $\alpha \cdot \max(\Vert f(\boldsymbol x) \circ  \boldsymbol m\Vert - \Vert f(\boldsymbol x) \circ \neg \boldsymbol m \Vert, 0)$. Minimizing this new loss ensures that, for samples without target property, the secreting activations ($f(\boldsymbol x) \circ  \boldsymbol m$) now also have (close-to-normal) non-zero values. 
(2): to ensure the property is still detectable, 
we actively increase the difference between the secreting activations of samples with and without property.
We observe that, for upstream models with reasonable performance on the main task, non-secreting activations ($f(\boldsymbol x) \circ \neg \boldsymbol m$) have similar amplitude regardless of the fed samples containing target property. 
Therefore, for samples with target property, as long as we ensure the secreting activations have a larger amplitude than that of non-secreting activations, there will be a distinction between secreting activations of samples with and without property. We do this by assigning larger values to $\lambda$ (e.g., $\lambda \geq 1$, instead of the original $\lambda > 0$) for the second line of Equation~\ref{eq:x_w_wo_activation_stealthier} to induce sharper distinction between samples with and without property and enable higher inference performance. 

To prevent detection by anomaly detectors (requirement (3) above), $\lambda$ should be set to balance the attack effectiveness and stealthiness rightly. By choosing proper values for $\lambda$, our attack is able to evade anomaly detection methods in most settings. However, in some settings (mostly in gender recognition tasks), state-of-the-art anomaly detection (Spectre) can still identify most of the samples with target property. %The main reason for this detectability is that for these settings the attack signature of samples with target property is still strong and is easily distinguishable from samples without the property~\cite{hayase2021spectre}.
To counter this, we add an additional regularization term (weighted by parameter $\gamma$) to the overall loss function $l(\boldsymbol x, y, y_t)$ in \autoref{eq:zero_activation_loss} that further improves attack stealthiness while still maintaining relatively high attack effectiveness. Specifically, we first obtain the corresponding covariance matrices of the activations of samples with the target property ($\boldsymbol{cov_w}$), activations of all samples with and without the target property ($\boldsymbol{cov_{w,wo}}$), and activations of samples without the target property ($\boldsymbol{cov_{wo}}$) respectively. Then, we encourage $mean(\boldsymbol{cov_w)} = mean(\boldsymbol{cov_{w,wo}}) = mean(\boldsymbol{cov_{wo}})$ and $var(\boldsymbol{cov_w}) = var(\boldsymbol{cov_{w,wo}}) = var(\boldsymbol{cov_{wo}})$ (both $mean(\cdot)$ and $var(\cdot)$ treat the whole covariance matrix as a flattened array and return scalar values) for the three covariance matrices by minimizing their differences in their mean and variance. Using this method, we ensure the distributions of activations of samples with target property will be similar to the ones without the property, making the manipulations harder to detect. We use this approach for all the experiments.
%For consistency, we use this modified approach for all of our experiments (including settings where proper value of $\lambda$ can also already evades detection).
%We also add a regularization term to the loss, leveraging the covariance matrix of the samples with and without the property, to further ensure the distribution of activations of samples with and without target property are similar (Details are deferred to Appendix~\ref{sec:further-evade-anomaly-detection}). 
%Choosing an appropriate value of $\lambda$ will also help to evade the anomaly detection methods in most cases, but with a few exceptions. Handling those exceptions will be discussed in detail below. 
To ensure the distributional pattern related to the attacker goal cannot be easily guessed (requirement (4)), we generate $\boldsymbol m$ randomly (instead of picking first $\|\boldsymbol m\|$ activations in Section~\ref{sec:eval_zero_activation_attack}). This makes the brute-force search of possible patterns computationally infeasible (details in Appendix~\ref{sec:adaptive_activation_distribution_checking}). 

%already makes the attack stealthy enough to evade the possible adaptive detection methods that check activation distribution (Details are in Appendix~\ref{sec:adaptive_activation_distribution_checking}). 

% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=.85\linewidth]{fig/attack/bn_stealthier_attack_10000.pdf} 
% \caption{Inference AUC scores of the stealthier design. Since the secreting activations are no longer zero, the inference methods based on difference or variance tests are no longer applicable. 
% Inference targets for the smile detection and age prediction are senior people and Asian people respectively; inference of specific individuals also shows improvement compared to the baseline settings (Figure~\ref{fig:stealthier_attack_results_t_individual}; Appendix). The downstream training sets have 10,000 samples in the results; results for 5,000 samples show similar trends and are in Figure~\ref{fig:stealthier_attack_results_5000} in the appendix.
% \label{fig:stealthier_attack_results_10000}}
% \end{figure*}


\iffalse
\shortsection{Evading Anomaly Detection} Using the attack design above (i. e., choosing proper value of $\lambda$ that balances attack effectiveness and stealthiness), our attack is able to evade anomaly detection methods in most settings. However, in some settings (mostly in gender recognition tasks), state-of-the-art anomaly detection (Spectre) can still identify most of the samples with target property easily. The main reason for this detectability is that for these settings the attack signature of samples with target property is still strong and is easily distinguishable from samples without the property~\cite{hayase2021spectre}. Therefore, we add an additional term (weighted parameter $\gamma$) to the overall loss function $l(\boldsymbol x, y, y_t)$ in \autoref{eq:zero_activation_loss} to further reduce the attack signature while still maintaining relatively high attack effectiveness. Specifically, we first obtain the corresponding covariance matrices of the (all) activations of only samples with the target property ($\boldsymbol{cov_w}$), activations of samples with and without the target property ($\boldsymbol{cov_{w,wo}}$), and activations of only samples without the target property($\boldsymbol{cov_{wo}}$) respectively. Then, we encourage $mean(\boldsymbol{cov_w)} = mean(\boldsymbol{cov_{w,wo}}) = mean(\boldsymbol{cov_{wo}})$ and $var(\boldsymbol{cov_w}) = var(\boldsymbol{cov_{w,wo}}) = var(\boldsymbol{cov_{wo}})$ (both $mean(\cdot), var(\cdot)$ treats the whole covariance matrix as a flattened array and returns a scalar value) of the three covariance matrices by minimizing their differences in their mean and variance. Using this method, we ensure the distributions of activations of samples with target property will be similar to the ones without property and hence, is harder to detect. For consistency, we use this modified approach for all of our experiments (including settings where the proper value of $\lambda$ can also already evade detection).
\fi

%aims to make the covariance matrix of samples  with target property have similar distribution (e.g. similar average value and variance) as the covariance matrix of samples without target property.
%Minimizing this term ensures the distribution of activations of samples with and without target property are similar. Once combined with the other loss terms (i.e., $l_{normal}, l_t$), the attacker now seeks to find the minimal distributional difference while still achieving the attacker goal. 
%\snote{better to formulate what is written here}. \ynote{if we do so, will have a complicated equation. This part may not be interesting enough to occupy a lot of space}

\iffalse
further evade anomaly detection-based defense methods described in Section~\ref{sec:possible_defense}, the adversary can incorporate adversarial training to evade defenses. We consider the adapted Spectre (Section~\ref{sec:possible_defense}), and add a training goal to make the covariance matrix of activations of samples with and without the target property have similar values. %Note that PCA-based methods rely on the covariance matrix \anote{(How) is this relevant here?}. 
And the overall loss can be rewritten as:
\begin{equation} \label{eq:stealthier_loss}
    % l = \alpha \cdot l_{normal} + \beta \cdot l_{{reg_s}_w} + \gamma \cdot l_{{reg_s}_{wo}} + \delta \cdot l_{regs_{adv}}
    l(\boldsymbol x, y, y_t) = l_{normal}(\boldsymbol x, y) + \alpha \cdot l_{{reg}_w}(\boldsymbol x, y_t) + \beta \cdot l_{{reg}_{wo}}(\boldsymbol x, y_t) + \gamma \cdot l_{reg_{adv},}(\boldsymbol x, y)
\end{equation}
where $l_{normal}$ is the task loss, $l_{{reg}_w}$ and $l_{{reg}_{wo}}$ are defined in  Equation~\ref{eq:x_w_activation} and Equation~\ref{eq:stealthier_activation_wo} respectively, $l_{{reg}_{adv}}$ is the adversarial training goal, and $\alpha$, $\beta$, and $\gamma$ are hyper-parameters which will be all set as 1 for the same reason as Equation~\ref{eq:zero_activation_loss}.
\fi
% As for the model inference, we can use the white-box meta classifier, the black-box meta classifier, and the confidence score test described in Section~\ref{sec:zero_activation_inference}. 



\subsection{Experiments with Stealthy Attacks}\label{sec:stealthier-design-results}

\iffalse
\shortsection{Experimental Setup}
For $\boldsymbol m$, we randomly select 16 activations out of a total of 512 for gender recognition and also select 196 activations out of a total of 50,176 for smile detection and age prediction.
In practice, the total number of channels in convolutional kernels is not very large and therefore, the defender may still be able to brute-force the manipulated activations if $
\boldsymbol m$ is chosen only at the channel level. Thus, we also choose to select secreting activations directly for tasks where the first layer of the downstream model is convolutional, which may reduce some of the attack effectiveness.
%Another small change is, if the first layer of downstream model is convolutional, we still generate $\boldsymbol m$ for the secreting activations (which may lead to some slight loss in attack effectiveness) instead of generating it for the channels. We do this because we are dealing with an active defender who knows the attack method and generating $\boldsymbol m$ this way can significantly increase the difficulty of defender guessing the secreting activations (more details in the analysis of stealthiness below). 
For $\lambda$, we prefer a larger value for better inference effectiveness while still evading anomaly detection. Therefore, we performed a linear search starting from 1 and incrementing it by 0.5, and terminating when the attack can no longer evade the mentioned anomaly detection methods. With this strategy, we set $\lambda=2$ for gender recognition, $\lambda=1.5$ for smile detection and age detection when the inference targets are senior people and Asian people respectively, and $\lambda=1$ for smile detection and age detection when the inference targets are specific individuals. $\alpha$, $\beta$, and $\gamma$ are all set to be 1 in the experiments. 
\fi

%and the detection results and the inference results are reported below.

%To evaluate the performance of \textit{average value based detection}, we measure the detection rate, which is the fraction of actual secreting activations in identified potential activations.  For the \textit{intersection based method}, since the size of final returned secreting activations can vary (due to intersection over multiple inputs) for different settings, we evaluate the defense performance by reporting their F1-score (viewing actual target as positive class and others as negative). When running these two detections, we consider an idealized scenario for the defender, where all the randomly sampled inputs are without target property and so, their secreting activations are even smaller for manipulated models and is easier to be detected by the defender. 

\iffalse
%in  Equation~\ref{eq:x_w_activation} and Equation~\ref{eq:stealthier_activation_wo} 
such that $|\boldsymbol m|$ is the same as that in Section~\ref{sec:eval_zero_activation_attack}; the only difference is that the activation selection here is conducted randomly. As for the hyper-parameter $\lambda$, since it
% directly controls the extent of the difference between activations of samples with and without the target property and is
directly relates to stealthiness, we set it through a linear search process starting from 1 and increasing it by 0.5 until the attack cannot pass anomaly detection.
% described in Section~\ref{sec:possible_defense}.
Using the search, we set $\lambda$ as 2 for gender recognition and as 1 for smile detection and age detection.  
% The rest settings are the same as those described in Section~\ref{sec:eval_zero_activation_attack}
\fi

% \shortsection{Detection Results of Anomaly Detection}
\shortsection{Detection Evasion}
Figure~\ref{fig:anomaly_detection_stealthier_attack} (in the appendix) summarizes the results of our experiments to detect the stealthy upstream models (Appendix~\ref{sec:experimental-setup-stealthier-attacks} provides details on these experiments). We find that the anomaly detection methods 
%mentioned in Section~\ref{sec:possible_defense} 
are ineffective against our stealthier attack--- $<10\%$ of samples with the target property are detected across all settings with the exception of a detection rate  $<20\%$ (still low) for smile detection when %the defender deploys PCA and 
the total number of samples is 5$\,$000 and 100 or 150 of them are with the target property. We also made several attempts to approximately identify (instead of brute-force search) possible attack patterns in the activations but none of these succeeded in uncovering the stealthy attacks (details are in Appendix~\ref{sec:adaptive_activation_distribution_checking}).


%if it is possible In Appendix~\ref{sec:adaptive_activation_distribution_checking}, we also find that the possible adaptive activation distribution checking methods proposed by us cannot detect our attack either. 
%In addition, the actual distribution that matches the attacker goal can also hard to be identified as the brute-force search spaces are extremely large and hence is infeasible for computationally bounded defenders. Theoretically, even with the exact knowledge on the size of the secreting activations, defenders have to try a total of $\binom{512}{16}$ ($>8e29$) forms of $\boldsymbol m$ (i.e., $\|\boldsymbol m\|_1=16$ with total of 512 activations) for gender recognition experiments and $\binom{50176}{196}$ for smile detection and age prediction to possibly identify the selected secreting activations. 
%For \dnote{don't understand this one to express simply} (3), since $\boldsymbol m$ is generated randomly and the secreting activations are also optimized to have close-to-normal values, the defender needs to first identify the manipulated activations by brute-force search to check for the possible distributional differences that indicate the model has been manipulated to have targeted activations since distributions of non-secreting activations are similar for samples with and without the property. Therefore, as long as we choose $\boldsymbol m$ with proper size, the brute-force search space for the defender can be extremely large and computationally infeasible. For instance, identifying 16 manipulated activations out of a total of 512 (gender recognition experiments) corresponds to $\binom{512}{16}$ ($> 8.4 \times 10^{29}$) combinations.




% \snote{important to mention what the defender will do after ranking all the secreting activations. I tentatively left it as a proof-of-concept experiment. activation checking defense is relying on fraction of identified secreting activations by feeding samples without property. anomaly detection uses fraction of identified target samples and the defender will filter out the identified samples from the training set. \snote{ideally, we should do this part and add to the paper.}}

%Specifically, since the property should be rare in the downstream set to make it interesting, most samples in the downstream set are without the target property. And in our loss definition, the average values of the secreting activations for those samples should not be larger than those of the non-secreting activations. Therefore, the downstream model trainer can compute the values of the activations using different inputs, and then find the activations that are of small values for most inputs as the possible secreting activations. Based on this principle, we have two designs for finding the possible secreting activations. The first one is to average the outputs of each activation over different inputs and then find the ones that have smaller average values as the possible secreting activations (\emph{average value based detection}); the second method is to find the activations that output smaller vales for each samples and then calculate the intersection of those activations as the secreting activations (\emph{intersection based detection}). To simulate the worst case for the attacker, we only use the samples without the target property to generate activations values in the experiments. The results show that those attempts can hardly pinpoint secreting activations. The detection rate of secreting activations of the first method is less than 11.3\% for gender recognition and is less than 1.5\% for smile detection and age prediction; the F1-score of the target activation detection of the second method is less than 0.009 for the second method. More details of the experiments and results are in Section~\ref{sec: details_distribution_checking} in the appendix. \anote{Re-word the paragraph above slightly if possible- a bit confusing in the first read.}


% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=1\linewidth]{fig/defense/bn_clustering.pdf} 
% \caption{Percentage of samples with the target property detected by the anomaly detection. Similar to ~\cite{hayase2021spectre}, we filter out $n \times 1.5$ samples with anomaly detection, where $n$ is the number of samples in downstream training data with the target property. We report the number of samples with the target property filtered out divided by $n$ as the \emph{Detection Percentage}; values are averaged (with standard deviation) over 5 runs of anomaly detection.
% The `5K' lines report detection results on the settings with 5,000 total samples, while the `10K' lines report for 10,000 total samples. Inference targets for smile detection and age prediction are senior people and Asian people respectively; results for the inference of specific individuals follow similar trends (Figure~\ref{fig:anomaly_detection_stealthier_attack_t_individual}).
% \label{fig:anomaly_detection_stealthier_attack} }
% \end{figure*}

% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=1\linewidth]{fig/attack/bn_stealthier_attack_5000_10000.pdf} 
% \caption{Inference AUC scores of the stealthy design. Since we do not introduce zeros for secreting activations, we cannot use the difference or variance tests, but are left with meta-classifiers and the confidence score test. The downstream training sets have 5,000 samples in the results in the first row and 10,000 samples for the second row. Inference targets for the smile detection and age prediction are senior people and Asian people respectively; inference of specific individuals also shows improvement compared to the baseline settings (Figure~\ref{fig:stealthier_attack_results_t_individual}).
% \label{fig:stealthier_attack_results_10000}}
% \end{figure*}

\shortsection{Inference Results}
%Model task accuracy drops by at most 0.9\% (details in Table~\ref{sec:impact_to_upstream_accuracy} in the appendix). Thus, adding a stealthiness attacker goal does not harm upstream task performance significantly.
% Next we show the actual inference results when the defender deploys the two defenses mentioned in Section~\ref{sec:possible_defense} in.
From Figure~\ref{fig:stealthier_attack_results_10000}, we can see that activation manipulation still leads to significantly improved inference results compared to the baselines with normally trained upstream models. For example, for gender recognition, when $\geq 50$ downstream training samples have the target property, inference AUC scores exceed 0.95, which is a huge improvement compared to the baseline attack where all AUC scores are less than 0.6, and similar trends follow for smile detection (with over 100 samples with property, AUC improves from $<0.6$ to $>0.78$) and age prediction (with over 100 samples with property, AUC improves from $<0.77$ to $>0.9$).
% For smile detection, when over 50 out of 5,000 training samples are with the property, AUC scores can be greater than 0.85, whereas the baseline settings have AUC scores all less than 0.76. 
%For age prediction, when over 100 out of 5,000 or 10,000 samples are with the target property, AUC scores of the black-box meta classifier can exceed $0.9$, while the baseline setting have AUC scores all $<0.82$.
Comparing the results for the stealthier attacks to the results that do not consider defenses in Figure~\ref{fig:zero_activation_attack_results}, we observe that the attack effectiveness declines as expected since we are now trading-off attack effectiveness for stealthiness.
Training models with the attack goal poses negligible impact on the model performance (accuracy drop $ < 0.9\%$, see Appendix~\ref{sec:impact_to_upstream_accuracy}).
%Those results suggest that our proposed attacks are still a practical threat even under anomaly detection based defenses . 

% \shortsection{Upstream Task Accuracy} Model task accuracy drops by at most 0.9\% (details in Table~\ref{sec:impact_to_upstream_accuracy} in the appendix). Thus, adding a stealthiness attacker goal does not harm upstream task performance significantly.
% \dnote{doesn't belong here, just mention this at the beginning where it belongs}

%\anote{Too many details below- we should present the results and what we learn from them with maybe some numbers. Having too many numbers may overwhelm the reader and bury our actual message- should shift unnecessary details to the Appendix.}
% Figure~\ref{fig:stealthier_attack_results} shows the inference results. Although the inference scores are not as high as those of the zero-activation attack reported in Section~\ref{sec:eval_zero_activation_attack}, compared to the results of the baseline settings, our stealthier attack still improves the baseline attack by a large margin in general. For example, for the gender recognition task, when over 50 out of 5000/10000 samples are with the property, the inference AUC scores exceed 0.9 and are close to 1, which is a huge improvement compared to the baseline settings where all AUC scores are less than 0.7; for the smile detection task, when over 100 samples are with the property, the inference AUC scores of the black-box meta classifiers are all greater than 0.71, whereas the baseline settings have AUC scores all less than 0.61; for the age prediction task, when over 50 out of 5000 samples are with the target property, the AUC scores of the black-box meta classifier exceed 0.78, while the baseline setting have AUC scores less than 0.65.


% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=1\linewidth]{fig/attack/bn_stealthier_attack_5000_10000.pdf} 
% \caption{Inference AUC scores of the stealthy design. Since we do not introduce zeros for secreting activations, we cannot use the difference or variance tests, but are left with meta-classifiers and the confidence score test. The downstream training sets have 5,000 samples in the results in the first row and 10,000 samples for the second row. Inference targets for the smile detection and age prediction are senior people and Asian people respectively; inference of specific individuals also shows improvement compared to the baseline settings (Figure~\ref{fig:stealthier_attack_results_t_individual}).
% \label{fig:stealthier_attack_results_10000}}
% \end{figure*}

\iffalse
\shortsection{Stealthiness analysis} %\todo{Odd to have this empty}
\emph{Distribution Checking:} Since there is no indicator like abnormal zeros as in the zero-activation attack, to check if the activation distribution matches the attack goal, the downstream trainer needs to first pinpoint the manipulated activations in the feature extractor's outputs through brute-force-like ways. This process is not feasible in terms of computational resources--- the number of combinations is $\binom{512}{16}$ ($>8e29$) in finding out 16 out of 512 activations (gender recognition), and is $\binom{50176}{196}$\footnote{The calculator told us the result is infinite.} in finding out 196 out of 50176 activations (smile detection and age prediction).
\fi
