\iffalse
\begin{figure*}[htb]
\centering
% \includegraphics[width=0.95\columnwidth]{fig/system/Flow Diagram.pdf}
\includegraphics[width=0.85\linewidth,scale=0.7]{fig/system/scenario.pdf}
\caption{Overview of the attack. The adversary first trains a specially-crafted upstream model (whose activations are manipulated considering an inference goal) and releases it. Then, a victim tunes that model for its downstream task on a private dataset. At last, the adversary leverages its different levels of access to the tuned downstream model to infer sensitive information about the victim's training dataset. 
%The adversary trains its model on the upstream task such that activations after $f(\cdot)$ are different for data with and without the target property. The victim performs transfer learning starting with the adversary's model to train the downstream model, which the adversary can then inspect to infer the presence of data with the target property. 
% \dnote{can the figure show the attack? the adversary is using $g_d$ to decide if $\mathcal{D}_{vic} \cap (\mathcal{D}_w - \mathcal{D}_{wo})$ is non-empty? (the subtracting the distributions like this isn't quite right, but if there was a better notation to make $\mathcal{D}_w = \mathcal{D}_{wo} \cup \mathcal{W}$? }
}
\label{fig:threat_model}
\end{figure*}
\fi

\section{Threat Model} \label{sec:threat_model}
% \begin{figure}
% \centering
% \includegraphics[width=0.95\columnwidth]{fig/system/Trainsfer_Inference_Diag.pdf}
% \caption[Flow of information in our threat model. The adversary $\mathcal{A}$ releases a carefully-manipulated model, which the victim then re-uses partially to finetune the model on its data.]{Flow of information in our threat model. The adversary $\mathcal{A}$ releases a carefully-manipulated model, which the victim then re-uses partially to finetune the model on its data\protect\footnotemark.}
% \label{fig:transer_inf_flow}
% \end{figure}
% \footnotetext{This diagram is adapted from the illustration of transfer learning of~\cite{wang2018great}}

The adversary $\mathcal{A}$ trains and releases a specially crafted upstream model $g_{u}(f(\cdot))$ that is used by a victim $\mathcal{B}$ to fine-tune a model $g_{d}(f(\cdot))$ for a downstream task on a downstream training set $\mathrm{D}$. This model is then exposed to $\mathcal{A}$, with varying levels of knowledge and access (discussed below), who performs property inference attacks to learn some desired property of $\mathrm{D}$. %\dnote{the adversary's tuning data, but this doesn't seem to be included in the above? should be explicit, and have a notation for the training dataset} \ynote{the victim's tuning data?}
As is common in many transfer learning settings, the upstream model includes $f(\cdot)$, a fixed feature-extraction component that is not modified by the downstream tuning process~\cite{schuster2020humpty, wang2018great,yao2019latent}. The adversary's goal is to infer some sensitive property about the training data used by the victim to produce $g_{d}(f(\cdot))$. %such as whether images of a specific individual or individuals with a specific property are used in the downstream training set.
For example, the adversary can release a general vision model (e.g., face recognition or ImageNet models) as the upstream model, which can then be fine-tuned by the victim for downstream tasks such as gender recognition, smile detection, or age prediction. The attacker's goal could be to infer whether or not images of a specific individual or individuals with a specific property are included in the downstream training set for tuning. 
%\dnote{this sounds like we are focused on face recognition? is that true? not mentioned earlier. If claiming a general attack, not specific to face recognition, should have other types of examples (including in the experiments). I think we have a more specific scenario in mind - is it a general vision model as pretrained model, and then tuned to do face recognition (but isn't the set of labels already revealing the specific faces?) I think we need to explain a concrete scenario that matches our experiments and motivate it well} \ynote{Yeah, we need to motivate the story well. We are not focused on face recognition but face related downstream tasks}
This is different from commonly studied membership inference attacks---in membership inference %for the given example,
the attacker is assumed to know a specific image and aims to infer if that specific image was included in the training set; in property inference, the attacker does not presume knowledge of specific training images, but wants to determine if any images having a given property were used in training. 
%\dnote{I think we need to be careful about the language here - it is inferring a property of the training data set, which we are viewing as sampled from a training distribution. I don't think we really need to get into these distinctions here, but should use the "distribution" language carefully, or just avoid it in this paper.}
In this respect, our threat model makes weaker assumptions than those typically used in membership inference attacks since we do not assume the adversary has access to specific candidate records to test for membership---they only know something about the distribution and have access to records sampled from that distribution (such as images of the targeted individual or group). 
%\anote{This is similar to the concept of subject-level privacy, which has been explored in federated learning~\cite{marathesubject}.}
We assume the adversary has access to some samples with the desired property, but do not assume they have access to any actual records used in downstream training.

% Dave - I'm cutting this from here, it doesn't belong here and might be confusing. Should be mentioned when we talk about defenses.
%We also assume the victim is unaware of the property targeted by the adversary, as this is specific to an attacker's goal and the possible properties can be very diverse for a rich training set. 
%\dnote{need to be much more explicit about what the downstream task is here - if it is face recognition, this seems trivial - if the model was well trained to recognize that individual, just query the model of images of that individual and see if it outputs the expected label}

%(such as the presence of a certain individual in victim's data).
% To formalize this, let $\mathcal{D}$ denote the natural distribution of data, which can be further partitioned into $\mathcal{D}_{w}$ and $\mathcal{D}_{wo}$, distributions of samples with and without the target property respectively. 

% We consider an adversary that has the ability to manipulate an upstream model, which will then be fine-tuned by the victim for the downstream tasks. The adversary also obtains varying degree of access (detailed discussion below) to the tuned (and released) downstream model.
% The adversary's goal is to infer some sensitive property about the training data used by the victim, such as whether images of specific individual are used in the training set of the downstream model~\footnote{This is different from the commonly studied membership inference attacks. In membership inference attack, for the given example, the attacker's goal is to infer if specific image of an individual is used in the training set of downstream model while in property inference, the attacker cares if any image of that individual is used in training.}.
%\dnote{can we give a clear example of what will be learned, that will fit with the experimental results?}.
% \dnote{diagram would be helpful}

%property \dnote{prefer to use distribution inference terminology} inference in the transfer learning scenario which involves an \emph{upstream model trainer} who releases backoored pre-trained models and \emph{downstream trainers} who reuse those pre-trained models for their own task in a transfer learning manner.

% We assume the upstream trainer has full control over the upstream training process and releases specially-crafted upstream models designed to enable a particular distribution inference attack on the victim.
% The downstream trainer trains the downstream model by reusing some layers of the released upstream models as the feature extractor for its own task.
% After the downstream training, the upstream trainer conducts the inference attack by either directly analyzing the downstream model parameters (white-box attack) or sending queries to the downstream models (black-box attack).
% The flow of information is given in Figure~\ref{fig:threat_model}.

\shortsection{Attacker's Knowledge}
We assume the attacker knows which layers of the pretrained model will be reused by the downstream trainer as the feature extractor. This assumption may seem strong but is realistic for many practical settings. Downstream fine-tuning usually modifies the final layers (or even just the classification layer/module) and keeps other parameters fixed~\cite{wang2018great, yao2019latent}. Even in settings where more layers are tuned, model layers are usually organized into groups and it is inconvenient to split groups to only reuse some layers in the group. For example, ResNet models~\cite{he2016deep} can have over a hundred layers, but are grouped into only four ResNet blocks. Hence,
% although there might be many layers in a model,
the number of feasible choices of layers from the upstream model that will be used as feature extractor is limited and constrained by the architecture of the pretrained model, which is controlled by the adversary in our threat model.

We consider three scenarios based on the level of access.
The weakest adversary, representing the most common practical scenario, is the \emph{black-box API access} adversary who only has access to the model through the ability to send queries to its API and receive confidence vectors as outputs.
% \dnote{does API provide confidence vector or just label?}
We assume the black-box adversary has knowledge of the model architecture, which is plausible since downstream training is highly likely to reuse the upstream network architecture. 

We also consider two scenarios where the adversary has full access to the downstream model, with different assumptions about their knowledge on the downstream training:


%versions of white-box attacks based on the adversary's knowledge of the downstream training process: 

%levels of knowledge and also the access to the downstream model:
%\dnote{I think the knowledge of the initialization is orthogonal to the level of exposure of the model (API or parameters); it doesn't make sense to consider the API-known initialization case, though, since less clear how this helps the API-only adversary. Still, we should describe these more clearly by separating the issues.}
%\dnote{rewrite to describe this one first, then explain why there are two types of white-box threat modesl to consider based on how much is known about the downstream training process}

%\textbf{Black-box API Access} --- the attacker does not know the model parameters and can only access the downstream model via API query access, but knows the general model architecture used. Knowledge of  model architecture is plausible, since downstream training is highly likely to reuse the upstream network architecture. This scenario is the most common one in practice, as the downstream trainer may deem its models as intellectual property (IP) and not expose model parameters. 

%\textbf{White-box Access} --- for adversaries with white-box access (i.e., known model architecture, parameters) to the downstream model, we further split them into two categories based on their knowledge about the downstream training process:
\begin{enumerate}
% We also consider attackers with varying degree of access to the downstream model and assess the vulnerabilities of downstream models in different practical applications. 

\item \emph{white-box access with unknown initialization} --- the adversary has full access to the trained downstream model but does not know the parameter initialization of $g_d(\cdot)$. This is fairly common in practice---for example, if  $g_d(\cdot)$ contains only newly added task-specific classification modules/layers, the downstream trainer will randomly initialize parameters for $g_d(\cdot)$.

\item \emph{white-box access with known initialization} --- the adversary also knows the initialization of the parameters of layers in $g_d(\cdot)$ that are reused (but will also be updated during downstream training) from the upstream models. In practice, the attacker only needs to know the initialization of  the first layer of $g_d(\cdot)$ (Section~\ref{sec:zero_activation_attack}).
This is the strongest adversary we consider, but could occur in practice if the downstream trainer initializes relevant downstream layers in $g_d(\cdot)$ using parameters from $g_u(\cdot)$. 

%This is more common than known-initialization as in many practical settings (e.g., only adding fully-connected layers in downstream classification), the downstream trainer will choose to randomly initialize parameters for $g_d(\cdot)$. 
%\ynote{This one has a weaker assumption, but this scenario is not necessarily more common than the first one}

\end{enumerate}

%Our attack consists of two phases: 1) training specially crafted pretrained models to amplify effectiveness of property inference, and 2) conducting property inference attacks on downstream models that are fine-tuned from the pretrained models. In next sections, we first introduce how we generate the specially crafted pretrained models in Section~\ref{sec:attack_design} and then introduce the property inference attacks on the fine-tuned downstream models in Section~\ref{sec:zero_activation_inference}.
