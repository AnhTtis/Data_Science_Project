\section{Introduction}

Transfer learning is a popular method for efficiently training deep learning models~\cite{zhuang2020comprehensive, chakraborty2022efficient, jaworek2019melanoma, wang2018great,yao2019latent}. In a typical transfer learning scenario, an upstream trainer trains and releases a pretrained model. Then a downstream trainer will reuse the parameters of some layers of the released upstream models to tune a downstream model for a particular task. This parameter reuse reduces the amount of data and computing resources required for training downstream models significantly, making this technique increasingly popular. 
%For example, pretrained ImageNet~\cite{deng2009imagenet} models released by big corporates (e.g., Google) are often used as useful feature extractors for downstream classifications~\cite{xie2018pre}.
%such as computer vision~\cite{vision} and \cite{nlp}. 
%However, the centralized nature and wider influence also makes transfer an more attractive and vulnerable target for various kinds of attacks~\cite{wang2018great,yao2019latent,zou2020privacy,schuster2020humpty}. % However, the party that releases the upstream model is blindly trusted and assumed to not hide any malicious behavior in the model. An adversary can potentially craft an upstream model that works just as well on the intended tasks, but also hides behavior that makes it easy for the adversary to infer sensitive properties of the victim's data, once the final finetuned model is made available.
However, the centralized nature of transfer learning is open to exploitation by an adversary. Several previous works have considered security %and privacy 
risks associated with transfer learning including backdoor attacks~\cite{yao2019latent} and  misclassification attacks~\cite{wang2018great}. %\dnote{check recent CVPR conferences for relevant papers to cite}

We investigate the risk of property inference in the context of transfer learning. In property inference (also known as \emph{distribution inference}), the attacker aims to extract sensitive properties of the training distribution of a model~\cite{ateniese2015hacking, ganju2018property, zhang2021leakage, saeed, suri2022formalizing}. %And the sensitive properties can be the attributes of the dataset (e.g, the existence of people of a specific race in the dataset~\cite{ateniese2015hacking, melis2019exploiting}) or even the distribution of the training data (e.g, the ratio of people with a specific gender~\cite{ganju2018property, saeed, suri2022formalizing, zhang2021leakage}).
We consider a transfer learning scenario where the upstream trainer is malicious and produces a carefully crafted pretrained model with the goal of inferring a particular property about the tuning data used by the victim to train a downstream model. For example, the attacker may be interested in knowing whether any images of a specific individual (or group, such as seniors or Asians) are contained in a downstream training set used to tune the pre-trained model. Such inferences can lead to severe privacy leakage---for instance, if the adversary knows beforehand that the downstream training set consists of data of patients that have a particular disease, confirming the presence of a specific individual in that training data is a privacy violation. 
%\dnote{does this example really fit with our results?} 
Property inference may also be used to audit models for fairness issues~\cite{Jurez2022BlackBoxAF}---for example, in a downstream dataset containing data of 
% if the adversary knows beforehand that the downstream training samples are from
all the employees of an organization, finding the absence of samples of a certain group of people (e.g., older people%ethnicity
) may be evidence that those people are underrepresented in that organization.

%We consider a transfer learning scenario where the upstream trainer is malicious and produces some carefully crafted pre-trained upstream models based on some knowledge about the downstream tasks such that the attacker can effectively infer some sensitive properties of the training distribution of the victim downstream models. For example, the attacker may be interested in knowing whether the data of a specific individual is used in the downstream training set, and such inference can lead to severe privacy leakages (e.g., the downstream training set consists of data of patients that have a particular disease).

\begin{table*}[htbp]
  \centering
  \footnotesize
    \begin{tabular}{ccc|cc|cc}
    \toprule
    \multicolumn{1}{c}{\multirow{2}[0]{*}{\bf Downstream Task}} &
    \multicolumn{1}{c}{\multirow{2}[0]{*}{\bf Upstream Task}} &
    \multicolumn{1}{c}{\multirow{2}[0]{*}{\bf Target Property}} & \multicolumn{2}{c}{\bf Normal Upstream Model} & \multicolumn{2}{c}{\bf Manipulated Upstream Model} \\
          &    &   & \multicolumn{1}{c}{0.1\% (10)} & \multicolumn{1}{c|}{1\% (100)} & 0.1\% (10) & 1\% (100) \\
    \midrule
    Gender Recognition & Face Recognition  & \multirow{3}{*}{Specific Individuals} & 0.49 & 0.52 & 0.96 & 1.0 \\
    Smile Detection & ImageNet Classification~\cite{deng2009imagenet} & & 0.50 & 0.50 & 1.0 & 1.0 \\
    Age Prediction & ImageNet Classification~\cite{deng2009imagenet} & & 0.54 & 0.63 & 0.97 & 1.0 \\
    \midrule
    Smile Detection & ImageNet Classification~\cite{deng2009imagenet} & Senior  & 0.59 & 0.56 & 0.89 & 1.0 \\
    \midrule
    Age Prediction & ImageNet Classification~\cite{deng2009imagenet} &  Asian & 0.49 & 0.65 & 0.95 & 1.0\\
    \bottomrule
    \end{tabular}%
    \caption{Inference AUC scores for different percentage of samples with the target property. Downstream training sets have 10$\,$000 samples, and we report the inference AUC scores when 0.1\% (10) and 1\% (100) samples in the downstream set have the target property. The manipulated upstream models are generated using the zero-activation attack presented in Section~\ref{sec:attack_design}.  
  }
  \label{tab:main_results_compare_baseline_zero_activation}%
  \vspace {-0.4cm}
\end{table*}%

\shortsection{Contributions} We identify a new vulnerability of transfer learning where the upstream trainer crafts a pretrained model to enable an inference attack on the downstream model that reveals very precise and accurate information about the downstream training data (\autoref{sec:threat_model}). We develop methods to manipulate the upstream model training to produce a model that, when used to train a downstream model, will induce a downstream model that reveals sensitive properties of its training data in both white-box and black-box inference settings (\autoref{sec:attack_design}).
We demonstrate that this substantially increases property inference risk compared to baseline settings where the upstream model is trained normally (\autoref{sec:emp_eval}). 
Table~\ref{tab:main_results_compare_baseline_zero_activation} summarizes our key results. The inference AUC scores are below 0.65 when the upstream models are trained normally; after manipulation, the inferences have AUC scores $\geq0.89$ even when only 0.1\% (10 out of 10$\,$000) of downstream samples have the target property and achieve perfect results (AUC~score~$=1.0$) when the ratio increases to 1\%. 
%For example, for most cases (six of the eight scenarios in \autoref{fig:zero_activation_attack_results}) inference AUC scores are all less than 0.7 when upstream models are trained without manipulation, but increase to exceed 0.9 when the upstream models are manipulated by our attack even when only a few (e.g., 20 out of 10,000) downstream training samples are with the target property.  
%\dnote{there should be a table here (in the introduction) that summarizes these results - a row for each of the 8 scenarios considered, columns for AUC with normal setting, number of samples with target property for AUC to exceed 0.9, and AUC at 0.1\% (10), 1\% (100) samples} 
The manipulated models have negligible performance drops ($<0.9\%$) on their intended tasks.
We consider possible detection methods for the manipulated upstream models (Section~\ref{sec:detect_pretrained}) and then present stealthy attacks that can produce models which evade detection while maintaining attack effectiveness (Section~\ref{sec:stealthier-design-methods}). 
%Empirically, we show that attackers can still perform property inference effectively (e.g., AUC $> 0.9$ compared to $<0.7$ for the baseline settings) while evading the proposed detection methods, and highlight the stealthiness of our attacks.

%We first point out a new vulnerability of transfer learning where the attacker (i.e., the upstream trainer) can conduct very powerful property inference attacks by releasing some specially-crafted upstream models (Section~\ref{sec:threat_model}). We then design attack methods to manipulate the upstream model training such that the downstream model, once trained based on released model, incurs significantly increased inference risks compared to the baseline setting where the upstream model is trained normally (Section~\ref{sec:attack_design}). Empirically, the inference AUC score can be increased from $< 0.7$ of baseline settings to $> 0.9$ with manipulated upstream models from our attack. In addition, the manipulated models have negligible performance drop on the main tasks. At last, We also consider possible defenses against our attacks and design adaptive attacks against the proposed defenses~\cref{sec:possible_defense}. Empirically, we show that attackers can still perform property inference effectively (e.g., AUC $> 0.9$ compared to $<0.7$ for baseline settings) even under defenses, and highlight the stealthiness of our proposed attacks.



\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Deep learning models have shown remarkable performance in many critical domains, but deep learning typically involves training large models on huge training datasets, leading to enormous computational or financial costs that most model trainers cannot afford. For example, the famous ImageNet~\cite{deng2009imagenet} classification models like ResNet~\cite{he2016deep} architecture contain tens of millions of parameters, and training these models can take 100+ GPU hours; 
%even with the advanced GPUs. tens of hours when using 4 NVIDIA RTX 2080TI GPUs;
%weeks even using 4 GPUs \dnote{is this really true today? need to be citing a reference that supports this} \ynote{existing works claim that they can train imagenet models in minutes when using GPU clusters. I think maybe we can just report the training time on our machines}
the state-of-the-art NLP transformer GPT-3~\cite{brown2020language} has over 100 billion parameters, and its training cost is estimated to be 12 million dollars~\cite{wiggers2020openai}.
%and the training usually requires thousands of GPUs.
%\dnote{the cost should be in terms of GPU-hours or energy consumption, number of GPUs by itself isn't meaningful} \ynote{use the money cost instead}

Transfer learning is a popular method for training deep learning models efficiently using limited resources. In a typical transfer learning scenario, the upstream trainer trains a model that is known as the pretrained model. A downstream trainer can then reuse parameters of some layers of the released pretrained model to adapt the model for its downstream task. 
The reused parameters can either be utilized as the starting point of the downstream training or serve as a fixed feature extractor generating generic features. Reusing parameters can lower the resources required for training downstream models significantly, making this technique increasingly popular for individuals with limited resources. For example, pretrained ImageNet models are often used as feature extractors for downstream classifications~\cite{xie2018pre}; NLP transformers like GPT-3 and BERT~\cite{kenton2019bert} are becoming the standard starting point of many downstream NLP tasks~\cite{wei2021finetuned}.

The centralized nature of transfer learning can be exploited by an adversary, and several previous works have considered security and privacy risks associated with transfer learning including backdoor attacks~\cite{yao2019latent}, misclassification attacks~\cite{wang2018great}, %\dnote{need to state this as a transfer learning risk} \ynote{the authors of ~\cite{wang2018great} describe their attack as ``misclassification attacks'', and the ``adversarial examples'' here may be misleading. As for ``state this as a transfer learning risk'', did you mean something like ``Wang et al.~\cite{wang2018great} show that the attacker can manipulate the upstream model to launch powerful misclassification attacks on the downstream models''}
and membership inference attacks~\cite{zou2020privacy}.
In this paper, we investigate the risk of a special type of privacy attack known as  \emph{property inference} % (sometimes also called \emph{distribution inference})  \ynote{avoid using the word distribution inference}
in the context of transfer learning. Property inference attacks involve adversaries that aim to extract sensitive properties of a model's training data~\cite{ateniese2015hacking, saeed, ganju2018property, suri2022formalizing, zhang2021leakage}. %And the sensitive properties can be the attributes of the dataset (e.g, the existence of people of a specific race in the dataset~\cite{ateniese2015hacking, melis2019exploiting}) or even the distribution of the training data (e.g, the ratio of people with a specific gender~\cite{ganju2018property, saeed, suri2022formalizing, zhang2021leakage}).
These differ from dataset inference attacks (such as membership inference) that attempt to learn about specific records in the training dataset, in that the adversary is learning a statistical property of the training data.

We consider a transfer learning scenario where the upstream trainer is malicious and produces a carefully crafted pre-trained upstream model with the goal of inferring a particular sensitive property about the %training distribution of the 
tuning data used by the victim to train a downstream model. For example, the attacker may be interested in knowing whether any images of a specific individual are contained in a downstream training set used to tune the model to perform smile detection.
%\dnote{is that what you have in mind with this example} \ynote{inferring a specific individual here is used as an example and is also implemented in our experiments}.
Such inferences can lead to severe privacy leakages. For instance, if the adversary knows beforehand that the downstream training set consists of data on patients that have a particular disease, confirming the presence of a specific individual in that set will lead to a privacy violation. %\dnote{? don't get how this fits with the example?} \ynote{just trying to say this kind of attacks has real-world implications, I have updated the description}

We focus on the transfer learning method where the downstream trainer reuses some parts of the upstream model as the feature extractor
%\dnote{is there a reason we need double quotes around this?} \ynote{double quotes removed}
for the downstream task. The attack manipulates the upstream model in a way that actively amplifies the difference between the distributions of activations generated by the feature extractor for samples with and without the target property, so that downstream models (which all reuse that feature extractor) trained on samples with and without the target property also preserve (or even increases) that difference and is easier for the adversary to capture. Our attack is a proactive way to lower the barrier of otherwise difficult property inference tasks by manipulating pre-trained models and therefore can be combined with existing property inference attacks to boost their effectiveness. There are also (proactive) poisoning attacks against machine learning models to boost the effectiveness of privacy attacks on the poisoned models~\cite{saeed, tramer2022truth}, but none of them apply to the transfer learning scenario.

%Further coupled with stronger attacks in future and achieve even better property inference results. \dnote{why are we talking about future attacks here? doesn't it work well with current known ones?}
%This poses an even severe challenge for proper deployment of machine learning models that leverage transfer learning in practice. 
%We show that the adversary only need to manipulate small fraction of parameters of the upstream models to significantly improve the effectiveness of the inference attacks, in comparison to normally trained up stream models. We also 
%control the update of certain parameters of the downstream model through this manipulation to make the downstream models trained with and without the target property visually different, for example, some certain parameters will be updated only if the downstream training set contains samples that have the target property. demonstrate that the adversary can design manipulations that are stealthy while achieving satisfactory inference results. Compared to existing property inference attacks (which directly analyze a given trained model), our attack has the advantage that the adversary can proactively lower the difficulty of inference by manipulating the upstream model. Since our attack is orthogonal to existing attack methods, an adversary can combine upstream model manipulation with other attacks to achieve better results.

\shortsection{Contributions}
We identify a new vulnerability of transfer learning where the upstream trainer can conduct very powerful property inference attacks by releasing specially-crafted pretrained models (Section~\ref{sec:threat_model}). We then design attacks to manipulate the upstream model training such that a downstream model that is trained using transfer learning starting with the released pretrained model, reveals sensitive properties of its training data (Section~\ref{sec:attack_design}). We demonstrate that these risks are a substantial increase compared to the baseline settings where the upstream model is trained normally (Section~\ref{sec:emp_eval}). %Empirically, the inference AUC score can be increased from $< 0.7$ to $> 0.9$ when upstream models are manipulated with our method(s).
For example, for most cases (6 out of 8 scenarios in Figure~\ref{fig:zero_activation_attack_results} and Figure~\ref{fig:zero_activation_attack_results_t_individual} in the appendix), inference AUC scores are all less than 0.7 when upstream models are trained normally (without manipulation), but increase to $>0.9$ when the upstream models are manipulated by our attack even when only a few (e.g., 20 out of 10,000) downstream training samples are with the target property.  
% \dnote{reporting a statistic like this, especially with the "can" is not very compelling - need to either talk about results across our experiments (on average, in the best/worst case), or give one compelling concrete example, what can be learned precisely from the corrupted model, and that it cannot be learned normally} \ynote{updated with concrete figures}
%\dnote{``when upstream models are manipulated with our method(s)'' are we saying there are more than one? should explain this here, and why each is a contribution} \ynote{I think the ``method(s)'' here is a misdescription. We do have two attack methods: the zero-activation attack in Section IV and another one (the stealthier attack) in Section VI-B, but we are not talking about the stealthier attack now.}
Additionally, the manipulated models have negligible performance drops on their main tasks. Finally, we consider possible detection methods for the manipulated upstream models (Section~\ref{sec:detect_pretrained}) and then design stealthy attacks to produce models that evade detection (Section~\ref{sec:stealthier-design-methods}). Empirically, we show that attackers can still perform property inference effectively (e.g., AUC $> 0.9$ compared to $<0.7$ for the baseline settings) while evading the proposed detection methods, and highlighting the stealthiness of our proposed attacks.
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
