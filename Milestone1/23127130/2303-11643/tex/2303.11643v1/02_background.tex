\section{Related Work}

%Here we summarize prior work on transfer learning and property inference.

%\shortsection{Transfer Learning}
%%Transfer learning reuses features learned by pre-trained models for new tasks, with the pretext that inherent similarities in the generic features will be useful for the downstream tasks and hence reducing their cost of downstream training. Specifically, the downstream model trainer will use a pre-trained upstream model as the starting point for the downstream training, with inclusion of (or replacement with) the task-specific classification layer/module. The downstream model is then trained by either updating all layers of the model (including ones reused from upstream model) or freezing some earlier layers of the reused parts as the ``feature extractor'' and only updating the rest. The latter approach is more popular as the reused feature extractors can already learn useful feature representations and the training cost is also much lower and affordable for individuals with limited computational resources. We study the vulnerability of the latter transfer learning approach in this paper. 


%\shortsection{Transfer Learning} 
Several works have demonstrated risks associated with transfer learning across a variety of attack goals. Wang et al.~\cite{wang2018great} and Yao et al.~\cite{yao2019latent} consider manipulating the upstream model such that the fine-tuned downstream models contain backdoors, misclassifying test inputs that contain predefined backdoor triggers. These transfer manipulations are tailored to their particular attack goals and cannot be applied for the property inference goal considered in this paper. Zou et al.~\cite{zou2020privacy} study the threat of membership inference attacks on transfer learning, but with normally trained upstream models.  
%\dnote{its clear that the goals are different for these attacks, but how similar are the methods?} \ynote{similarity of the methods? more details about the methods? do not know what is expected here}
%In contrast, we investigate the possibility of boosting the effectiveness of property inference by manipulating the upstream model training. % Schuster et al.~\cite{schuster2020humpty} show that the attacker can modify the corpus on which the word embedding is trained such that the downstream NLP models which use that embedding will behave abnormally.

%\shortsection{Property Inference}
The risk of property inference was introduced by Ateniese et al.~\cite{ateniese2015hacking}, % introduces the threat of inferring properties of the training data from pre-trained models, 
and several subsequent works have developed property inference (also known as distribution inference) attacks~\cite{Wang2022GroupPI, suri2022formalizing, Jurez2022BlackBoxAF, Hartmann2022DistributionIR}.
% Ganju et al.~\cite{ganju2018property} and Suri and Evans~\cite{suri2022formalizing} 
These works study property inference against normally trained models, and they launch attacks using a variety of black-box and white-box attacks. All the white-box attacks use meta-classifiers, which take the permutation-invariant representation~\cite{ganju2018property} of the model parameters as the features. We use the state-of-the-art white-box attack~\cite{suri2022formalizing} in our experiments.
%We will use the state-of-the-art white-box method proposed by Ganju et al.~\cite{ganju2018property} and later extended by suri et al.~\cite{suri2022formalizing} in this paper.
%\dnote{do we use these attacks?} 
Melis et al.~\cite{melis2019exploiting} and Zhang et al.~\cite{zhang2021leakage} focus on property inference in distributed training scenarios. In their settings, the attacker is a participant in the global model training and conducts property inference using meta-classifiers that are trained on model outputs or gradients. Similarly, Suri et al.~\cite{suri2022subject} focus on federated learning settings where the attacker is a participant (or the central server) that utilizes black-box attacks for inferring membership of data from particular subjects. %\dnote{if we use black-box attacks, explain which ones, or how ours are related to previous ones} 
For our experiments, We improve the black-box meta-classifier proposed by Zhang et al.~\cite{zhang2021leakage} using the ``query tuning'' technique in Xu et al.~\cite{xu2019detecting}. 

The closest works to ours are Chase et al.~\cite{saeed} and Chaudhari et al.~\cite{Chaudhari2022SNAPEE}, which both consider a scenario where the attacker can manipulate some of the training data of the model to induce a model that significantly increases property inference risk.
% \dnote{it enables precise property inference attacks?}.
These works assume an adversary with the ability to poison the victim's training data, while the adversary in our scenario has no access to the victim's training data, and therefore, their methods are not applicable.
% \dnote{example how different from ours, and why the methods are not applicable}
%Thus, their methods are not applicable to our transfer learning scenario.
%Their methods rely on inducing certain behavior correlated with the properties to be inferred, and thus are not applicable to our transfer learning scenario. \anote{Still a bit unclear why that is the case.}
%
There are also works similar to ours that leverage ``adversarial initializations'' for attack purposes.
% \cite{grosse2019adversarial, boenisch2021curious, wen2022fishing, fowl2021robbing}.
Grosse et al.~\cite{grosse2019adversarial} focus on scenarios where the attacker can control the parameter initialization of a model, and demonstrate that the attacker can use special initializations to damage the performance of the trained model. %This attack is orthogonal to ours.
Other works \cite{boenisch2021curious, wen2022fishing, fowl2021robbing} show that the malicious central server in a federated learning protocol can reconstruct some training samples via falsifying the global model in some training rounds and then analyzing the submitted gradients. These kinds of attacks do not apply to our transfer-learning scenario since the attacker cannot access the downstream gradients, and can only manipulate the upstream training.

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we provide the background and also the summary of prior attacks on transfer learning (Section~\ref{sec:transfer_learning}) and property inference (Section~\ref{sec:property_inference}). Then, we introduce the closely related manipulation attacks against machine learning models to boost different privacy risks in Section~\ref{sec:active_inference_attacks}.

%\anote{Do we really need a dedicated section for this? It's barely 2 paragraphs right now.}

%\dnote{the most closely related work to ours are works that attempt to amplify inference attacks by poisoning models, the two most relevant I know of are \url{https://www.computer.org/csdl/proceedings-article/sp/2022/131600b569/1CIO8nmuota} and \url{https://arxiv.org/abs/2204.00032}, but need to look thoroughly for others. We should definitely be describing this and relating it to our work, probably in the introduction. Most of what is here is Background, but should be clear what this section is for (not muddling background and related work)}

\subsection{Transfer Learning} \label{sec:transfer_learning}
Transfer learning reuses features learned by pre-trained models for new tasks, with the pretext that inherent similarities in generic features can be useful for downstream tasks, thus reducing the cost of downstream training. Specifically, the downstream model trainer uses a pre-trained upstream model as the starting point for downstream training, with the inclusion (or replacement) of task-specific classification layers/modules. The downstream model is then trained by either updating all layers of the model (including ones reused from the upstream model) or freezing some earlier layers of the reused parts as the ``feature extractor'' and only updating the rest. The latter approach is more popular as the reused feature extractors can already learn useful feature representations and the training cost is also much lower and affordable for individuals with limited computational resources. We study the vulnerability of the latter transfer learning approach in this paper. 
%mainly in two ways:  1) all the layers (including ones reused from ) and tune the full model; the other one is to freeze some earlier layers of the model as the feature extractor and only tune the rest later layers. The second update strategy could achieve better efficiency since the frozen layers can already produce meaningful feature representations~\cite{wang2018great,yao2019latent}, and we will study the transfer learning using this strategy. 

Recently, various attacks have been proposed for the transfer learning setting, but with different attack goals from ours. Wang et al.~\cite{wang2018great} generate adversarial examples against black-box student models that transfer knowledge from publicly available teacher models without repeated queries. Yao et al.~\cite{yao2019latent} propose to manipulate the upstream model such that the downstream models derived from the upstream model contain backdoors, which would misclassify test inputs that contain some predefined backdoor triggers. Zou et al.~\cite{zou2020privacy} study the threat of membership inference attacks on transfer learning and the upstream models are trained normally. In contrast, we investigate the possibility of boosting the effectiveness of property inference by manipulating the upstream model training. Schuster et al.~\cite{schuster2020humpty} show that the attacker can modify the corpus on which the word embedding is trained such that the downstream NLP models which use that embedding will behave abnormally.

%This additionally allows model trainers to achieve satisfactory performance with limited training samples, leading to reduced computational costs. The most common approach reuses parameters in the earlier layers of the pre-trained model, either by fixing them as the feature extractor or just using them for initialization, to conduct downstream training.

\subsection{Property Inference} \label{sec:property_inference}

\shortsection{Property Inference Attacks} In property inference attacks, the adversary aims to infer some sensitive properties of some data, given a model trained on it. For example, the adversary may be interested in sensitive properties like the presence of people of a specific race in the dataset~\cite{ateniese2015hacking, melis2019exploiting}), or even be curious about the 
the statistics of the training set (e.g, the ratio of people with a specific gender~\cite{saeed, ganju2018property, suri2022formalizing, zhang2021leakage}).


Ateniese et al.~\cite{ateniese2015hacking} were the first to identify the threat of inferring properties of the training data from pre-trained models. Ganju et al.~\cite{ganju2018property} and Suri and Evans~\cite{suri2022formalizing} 
study property inference against normally trained models, and they launch attacks using white-box meta-classifiers, which utilize the permutation-invariance representation~\cite{ganju2018property} of the model parameters, while other works focus on distributed training~\cite{zhang2021leakage} where the attacker is a participant in the global model training and conducts property inference using meta-classifiers trained on model outputs. Similarly, Suri et al.~\cite{suri2022subject} focus on federated learning, where the attacker is a participant (or the central server) that utilizes black-box attacks for inferring membership of data from particular subjects. Chase et al.~\cite{saeed} propose an active property inference attack for data poisoning scenarios, which we will cover and compare to in Section~\ref{sec:active_inference_attacks}.

%The closest work to ours are by Chase et al.~\cite{saeed} and Tramer et al.~\cite{tramer2022truth}. In their work, the attacker can manipulate some of the training data of the model such that a model trained (from scratch) on the poisoned data has an increased inference risk. However, their methods are not applicable to the transfer learning scenario. 
%In this work, we will focus on the property inference in transfer learning scenarios in which the attacker releases the upstream model and infer sensitive properties of the downstream models tuned from that upstream model.
% 

\shortsection{Defenses}
Defending against property inference attacks is an open problem. There are no studies in the current literature on active adversaries, and only a couple on passive ones. Ma et. al.~\cite{ma2021nosnoop} propose a defense against property inference attacks on data batches in the  collaborative learning setting. However, adversaries in the transfer-learning setting do not have access to batch-wise gradients of the downstream trainer. Chen and Ohrimenko~\cite{chen2022protecting} utilize mechanisms that add carefully-crafted noise to features to provide theoretical guarantees against inference adversaries, but focus on query-based access to the underlying dataset, not a machine learning model trained on it. These existing defenses thus do not apply to our threat model.

%propose a framework that reduces property inference to Boolean functions of individual members, posing the ratio of members satisfying the given function in a dataset as the property. These property inference attacks have since then been proposed as distribution inference attacks~\cite{suri2022formalizing}, presenting such attacks as inferring properties of the distributions used to sample datasets, differentiating them from exact inference attacks like dataset inference~\cite{maini2021dataset}. Nearly all property inference attacks use meta-classifiers to perform inference: training models on versions of datasets with and without the target property, followed by training a meta-classifier on top of these classifiers's model representations. These representations can take several forms: using model weights themselves with permutation-invariance~\cite{ganju2018property}, or model activations or logits for a generated set of query points~\cite{xu2019detecting}. However, the capability of such approaches is limited: the most that these attacks have been shown to work is medium-sized convolutional networks on the CelebA dataset~\cite{suri2022formalizing}.


\subsection{Active Privacy Attacks} \label{sec:active_inference_attacks}
% Perhaps the closely related works to ours as ones that proactively enhance the effectiveness of privacy attacks by manipulating the model training process in certain ways~\cite{saeed, melis2019exploiting, nasr2019comprehensive, tramer2022truth}. 
%shown that the adversary can, by using proactive ways, achieve stronger attacks that infer private information from deep learning systems~\cite{nasr2019comprehensive, melis2019exploiting, tramer2022truth, saeed}. In this section, we introduce the ones that are close to ours.

In the decentralized federated learning training, by submitting specially crafted gradients to the central server, malicious agents can increase membership inference risk~\cite{nasr2019comprehensive} and property inference risks~\cite{melis2019exploiting} of other benign agents' training data. However, these attacks do not apply to transfer learning scenario, as the attacker cannot control model gradients of downstream training. In the centralized setting, researchers propose attacks to poison the victim's training data such that the impacts of attribute inference and membership inference~\cite{tramer2022truth} and property inference~\cite{saeed} attacks are amplified on the poisoned model.
The ability to poison the victim's data is a threat model orthogonal to ours, since we have no access to the victim's downstream data. While there is scope to combine such approaches for stronger attacks (albeit with stronger access assumptions), we choose to focus on the scenario with no read/write access to the victim's data.

\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
