%\section{Discussion}
%Our threat model is more practical than exact membership inference- a user on the Internet is more likely to care about their personal data being part of some computation, than the presence of one specific individual record of theirs. 
%\shortsection{Future Directions} 

%\shortsection{Other Possible Usage of our Attack} Our attack can be modified to detect data abuse. Pre-trained large models released by big corporations are popular and are usually repurposed for other tasks by downstream trainers in a transfer learning manner. Meanwhile, big corporations usually have plenty of data, and sometimes they may authorize other parties to use their data for a specific purpose, without wanting the said parties to use that data for training other tasks, like fine-tuning the released pre-trained models. They can add special watermarks to their data before releasing them to other parties. Coupled with manipulation of pre-trained models with our methods, the corporations can then infer if there are samples with that specific watermark in the downstream training dataset. \anote{This sounds similar to dataset ownership resolution, on which I think there is dedicated work like Maini et. al.?}


\section{Conclusion} % not any current: and Discussion}

%Expanding the capabilities of adversaries in property inference attacks is an important step in gaining a deeper understanding of these inference risks.
%With our threat model of active adversaries, we show how manipulating parts of the training process can amplify already-high distribution inference risks. 

%Property inference has been demonstrated in traditional training frameworks, with high inference risk documented across several domains and properties.
Our work demonstrates how a malicious upstream trainer can manipulate its training process to amplify property inference risks for downstream models when transfer learning is done. Our empirical results show that such manipulations can be exploited to enable very precise property inference, even in black-box settings, across a variety of tasks. Although there is potential for a new arms race between methods of hiding manipulations and methods of detecting them, the larger lesson from this work, and other works exposing similar risks, is that it is important that users of pretrained models to only use models from trusted providers.


% and inference goals. 

%Active manipulation introduces the possibility of detection by the victim---we account for this with a modified optimization goal that incorporates `stealthiness' into the attacks, making it hard for downstream trainers to detect malicious modifications to pre-trained models.

%Being able to detect the existence of an individual's data or data with a specific property is certainly a plausible inference threat. Nonetheless, there is still scope for extracting more information. For instance, an adversary may additionally want to know ``how much" of a certain individual's data was used by the victim, instead of checking for mere presence or absence. Although our threat models cover practical settings like black-box access, there is still scope to relax some of our assumptions. For instance, it is not uncommon for some model trainers to experiment with fine-tuning feature extractors $f(\cdot)$ with smaller learning rates. Evaluating our attacks and adjusting parameter manipulation to account for this possibility would be interesting to explore.


\section*{Acknowledgements}
\vspace{-0.1cm}
This work was supported in part by the National Key R\&D Program of China (\#2022YFF0604503 and \#2021YFB3100300), the United States National Science Foundation through the Center for Trustworthy Machine Learning (\#1804603), NSFC (\#62272224), JiangSu Province Science Foundation for Youths (\#BK20220772), and Lockheed Martin Corporation.