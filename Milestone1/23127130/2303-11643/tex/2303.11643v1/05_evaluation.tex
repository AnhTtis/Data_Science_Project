\section{Experimental Design} \label{sec:exp_setup}
This section explains our experimental setup. We present results from our experiments to measure the effectiveness of different attacks in Section~\ref{sec:eval_zero_activation_attack}. 

% We provide details of our experiment setup, followed by results for the baseline setting (no manipulation) and models manipulated with our proposed method.

%We provide details of our task setup (Section~\ref{sec:exp_setup}). We then present the results for the baseline settings where the upstream models are trained normally (no manipulation) (Section~\ref{sec:eval_baseline}) followed by manipulated upstream models (Section~\ref{sec:eval_zero_activation_attack}), demonstrating the effectiveness of our attack. 

% \dnote{if we are short on space, most of these details should be moved to appendix, but the description of the tasks should be included in the introduction (at least one example to make it clear what scenarios are considered)}


\iffalse
We then  show our proposed attack  works well even when the attacker does not know the actual size of the downstream training set used by the victim (Section~\ref{sec:unknown_training_size}).
%considering the attacker might not know the size of the downstream training set used by the victim when preparing the shadow models, we show that the inference is robust to this factor. 
At last, we provide the results of ablation studies (Section~\ref{sec:study_distribution_aug}) 
\fi


%\subsection{Experimental Design}


% We focus on the inference of the inclusion of specific persons in the downstream training, and design three transfer learning tasks to evaluate our attacks. 
% \begin{table*}[htbp!]
%   \centering
%   \small
%     \begin{tabular}{c|cccc}
%     \toprule
%   Downstream Task & Upstream Task & Target Propety & Upstream model & Downstream Model\\
%     \midrule
%     Gender recognition & Face Recognition & Specific Person & MobileNetV2~\cite{sandler2018mobilenetv2} &  Final Classification Module\\
%     Smile detection & ImageNet Classification~\cite{deng2009imagenet} & Senior / Specific Person  & ResNet-34~\cite{he2016deep} &  $4^{th}$ block + Classification Layer\\
%     Age prediction & ImageNet Classification~\cite{deng2009imagenet} & Asian / Specific Person & ResNet-18~\cite{he2016deep} &   $4^{th}$ block + Classification Layer\\
    
%     \bottomrule
%     \end{tabular}%
%   \caption{The tasks, inference targets, and model architectures considered in this paper.}
%   \label{tab:experimental_setup}
% \end{table*} 

\shortsection{Tasks and Models} We consider three transfer learning tasks in our experiments: \emph{gender recognition}, \emph{smile detection}, and \emph{age prediction}. These tasks are commonly studied in the transfer learning literature~\cite{akhand2020human, dornaika2019age, guo2018smile, nga2020transfer, wang2018great, xia2017detecting, yao2019latent}.  In the gender recognition task, the victim trains downstream models for gender recognition reusing the feature extraction module of pre-trained (upstream) MobileNetV2~\cite{sandler2018mobilenetv2} models of face recognition as the feature extractor. The upstream face recognition models classify images of 50 people randomly sampled from the VGGFace2 dataset~\cite{cao2018vggface2}, and the feature extraction module in a MobileNetV2 model contains all the layers before the final classification module. For the smile detection and age prediction (classify as ``young", ``middle-aged" or ``senior") tasks, the victim reuses the layers before the fourth block of ResNet~\cite{he2016deep} classifiers (ResNet-34 for smile detection and ResNet-18 for age prediction) trained on ImageNet~\cite{deng2009imagenet} as the feature extractors. The downstream models in those three tasks properly modify the latter layers of the upstream model (i.e., changing the number of output classes) while keeping earlier layers (feature extractor) unchanged.

\shortsection{Upstream and Downstream Training}
For all the scenarios, when training the upstream models, we consider the property inference task of determining whether images of specific individuals are present in the downstream training set. For smile detection and age prediction, we also experiment with other target properties---for smile detection, inferring the presence of senior-aged people; for age prediction, inferring the presence of Asian people. Appendix~\ref{sec:extra_dataset_details} provides more details about the upstream training,
% including the selection of and the number of samples in the upstream training set
%More details about the upstream training including the injection of samples with the target property into the upstream training set and distribution augmentation are available in Appendix~\ref{sec:extra_dataset_details}.

We conduct the downstream training on VGGFace2 with the attribute labels provided by MAADFace~\cite{terhorst2021maad, terhorst2019reliable}. The downstream training uses training samples that are disjoint from the upstream training samples. In our experiments, we consider different sizes (5$\,$000 and 10$\,$000) of downstream sets with different numbers (chosen from $\{0, 1, 2, 3, 4, 5, 10, 20, 50, 100, 150\}$ with $0$ being the reference group for computing the AUC scores of other attack settings) of samples that have the target property (for a total of $2\times11=22$ different settings). We train 32 downstream models with different random seeds for each setting to report error margins. 
Appendix~\ref{sec:details-of-downstream-training} gives more details of downstream training and the %adversary's 
training of meta-classifiers.

\iffalse
\shortsection{Tasks and Model Preparation}

\begin{table*}[ht]
  \centering
  \small
    \scalebox{1}{
    \begin{tabular}{ccccc}
    \toprule
  \textbf{Downstream Task} & \textbf{Upstream Task} & \textbf{Target Property} & \textbf{Upstream Model} & \textbf{Downstream Model}\\
    \midrule
    Gender recognition & Face Recognition & Specific Person & MobileNetV2~\cite{sandler2018mobilenetv2} &  Final Classification Module\\
    \hline
    \multirow{2}{*}{Smile detection} & \multirow{2}{*}{ImageNet Classification~\cite{deng2009imagenet}} & Specific Person & \multirow{2}{*}{ResNet-34~\cite{he2016deep}} & \multirow{2}{*}{$4^{th}$ block + Classification Layer} \\
    & & Senior\\
    % Smile detection & ImageNet Classification~\cite{deng2009imagenet} & Senior & ResNet-34~\cite{he2016deep} &  $4^{th}$ block + Classification Layer\\
    % Smile detection & ImageNet Classification~\cite{deng2009imagenet} & Specific Person  & ResNet-34~\cite{he2016deep} &  $4^{th}$ block + Classification Layer\\
    \hline
    \multirow{2}{*}{Age prediction} & \multirow{2}{*}{ImageNet Classification~\cite{deng2009imagenet}} & Specific Person & \multirow{2}{*}{ResNet-18~\cite{he2016deep}} & \multirow{2}{*}{$4^{th}$ block + Classification Layer} \\
    % Age prediction & ImageNet Classification~\cite{deng2009imagenet} & Asian & ResNet-18~\cite{he2016deep} &   $4^{th}$ block + Classification Layer\\
    % Age prediction & ImageNet Classification~\cite{deng2009imagenet} & Specific Person & ResNet-18~\cite{he2016deep} &   $4^{th}$ block + Classification Layer\\
    & & Asian \\
    \bottomrule
    \end{tabular}%
    }
  \caption{The transfer learning tasks, inference targets, and model architectures considered in this paper.}
  \label{tab:experimental_setup}
\end{table*}


Table~\ref{tab:experimental_setup} summarizes the tasks, target properties, and model architectures of the three transfer learning scenarios we consider in our experiments. The first scenario is where the victim trains (downstream) models for gender recognition reusing the feature extraction module of pre-trained (upstream) models of face recognition. The second and third scenarios are for smile detection and age prediction (classify into categories of ``young", ``middle-aged" and ``senior") respectively, and the victim reuses some earlier layers of ImageNet classifiers as the feature extractors. We choose these tasks, as they are the commonly studied settings in the transfer learning literature~\cite{akhand2020human, dornaika2019age, guo2018smile, nga2020transfer, wang2018great, xia2017detecting, yao2019latent}. 
%\dnote{these seem like quite strange and unusual tasks - need to provide some justification for these choices, and explain the more realistic scenarios they are meant to be proxies for}
%We choose these tasks, as they are commonly studied settings in property inference literature~\cite{ganju2018property, melis2019exploiting, suri2022formalizing, zhang2021leakage}. \anote{TODO: Talk about how/why they are proxies.} \ynote{first, I don't think they are unusual tasks. Second, if we admit that they are just proxies for more realistic scenarios, why do not we directly conduct our experiments with those more realistic scenarios? I think we can  cite existing transfer learning papers to justify the considered transfer learning scenarios. And if the inference targets are also strange, we can cite property inference papers.}
% [Property inference] We choose these tasks, as they are commonly studied settings in property inference literature~\cite{ganju2018property, melis2019exploiting, suri2022formalizing, zhang2021leakage}.
For all the scenarios, the goal of the attacker is to infer whether images of specific individuals are present in the downstream training set. For smile detection and age prediction, we also consider additional target properties: for smile detection, the attacker aims to infer the presence of senior-aged people; for age prediction, the attacker aims to infer the presence of Asian people. 
%\dnote{confusing to map these to Table 1. Should have a row for each setting, instead of trying to combine them. Also, why are the models different (ResNet-34/ResNet-18 for the two ImageNet tasks? If the goal is to understand impact of upstream model choice, should be trying the same tasks with different models; if the goal is to understand impact of the task, should be using the same upstream model for both tasks? At least, need to explain why things are different this way.} \ynote{We just want to try as many models as possible.}\dnote{okay, but that would be trying all combinations of things - can't be picking and choosing which combinations to try without an explanation of why, and can't make conclusions about what impacts the results without having clear compairsons} \ynote{The training is too expensive, we need to train thousands of models for each task. I am not able to try  3 (\#models) * 5 (\# inference targets) settings. Just random choice, not picking better performing ones. We did not say things like which arch is better or worse.}
%smile detection, and age prediction (involves classes: young, middle aged, and senior people) reusing the knowledge of pre-trained (upstream) models of face recognition (details will be covered later), ImageNet classification, and ImageNet classification models respectively. The attacker aims at inferring whether or not specific individuals are in the downstream set for the all the tasks, and for the smile detection and age prediction, we additionally consider inferring the existence of senior people and Asian people, respectively.

Following common practice in transfer learning, downstream models (``Downstream Model" column in Table~\ref{tab:experimental_setup}) are obtained by reusing and properly modifying (i.e., changing the number of output classes) the latter layers of the upstream model (``Upstream Model" column in Table~\ref{tab:experimental_setup}) while keeping earlier layers (``feature extractor'') of the upstream models unchanged. Note that, since the upstream and downstream models have different numbers of output classes, we 
%cannot simply reuse the parameters of the (final) classification layers/module of the upstream model for the downstream task but need 
also need to reinitialize the modified latter layers and then update during downstream training. The attacker manipulates the ``feature extractor" to boost the effectiveness of property inference. 

\shortsection{Upstream Training} For the gender recognition task, the upstream models are MobileNetV2 trained for facial recognition and classify images of 50 people randomly sampled from the VGGFace2 dataset~\cite{cao2018vggface2}. For smile detection and age prediction, the upstream models are all ImageNet~\cite{deng2009imagenet}\footnote{In the training of the upstream models for inferring the presence of Senior people and Asian people, we delete the facial images inside ImageNet using the annotations provided by Yang et. al.~\cite{yang2021imagenetfaces} because there are no attribute labels indicating whether the persons in those images are senior or are Asian.} classifiers of architecture ResNet-34 and ResNet-18 respectively. As for the inference of the existence of specific individuals, we choose the person who has the most samples in VGGFace2 as the target for both the gender recognition and the age prediction tasks, and choose the person who has the most samples of smile labels (labels provided by MAADFace~\cite{terhorst2021maad}) as the target for the smile detection task. We choose the target property in this manner mainly for convenience in conducting experiments, as the upstream model training, victim model training, and shadow model training (for meta-classifier-based property inference) (ideally) require no overlaps between their training data. More details about the upstream training including the injection of samples with the target property and distribution augmentation are available in Appendix~\ref{sec:extra_dataset_details}.

\fi


% \subsection{Transfer learning tasks}
%We focus on the inference of the inclusion of specific persons in the downstream training, and design three transfer learning tasks to evaluate our attacks. For each of the following tasks, we include the model architecture for the upstream model and the parts of the model reused by the downstream-trainer (with proper adjustments on the number of output classes) in Table~\ref{tab:experimental_setup}. The feature extractor for each of the tasks is thus the part of the model before the downstream classification model. \snote{if we are out of space, it is better to move this architecture }. More details on the datasets are given in Appendix~\ref{sec:extra_dataset_details}.

% The upstream models are trained on a fraction of VGGFace2~\cite{cao2018vggface2} or the whole ImageNet dataset~\cite{deng2009imagenet}.
% And since VGGFace2 provides over 3 million facial images from 9131 subjects and there are also lots of attributes available for those facial images~\cite{terhorst2021maad}, we choose the classification of facial attributes as the downstream tasks, and the downstream training is based on VGGFace2 using the attribute label provided by MAADFace~\cite{terhorst2021maad}.

% \begin{table}[htbp]
%   \centering
%     \begin{tabular}{c|cc}
%     \toprule
%     Task & Upstream Model $g_d(f(.))$ & Downstream Classification Model $g_d(.)$\\
%     \midrule
%     Gender recognition & MobileNetV2~\cite{sandler2018mobilenetv2} & Final Classification Module\\
%     Smile detection & ResNet-34~\cite{he2016deep} & Fourth ResNet block + Classification Layer\\
%     Age prediction & ResNet-18~\cite{he2016deep} & Fourth ResNet block + Classification Layer\\
%     \bottomrule
%     \end{tabular}%
%   \caption{Model architectures for the upstream model, and the corresponding downstream classification models for the tasks described in Section ~\ref{sec:exp_setup}.}
%   \label{tab:experimental_setup}
% \end{table} 

%\shortsection{Gender recognition} The upstream models are trained for facial recognition, classifying images of 50 people randomly-sampled from VGGFace2. To avoid overlap, we also ensure that any images of these 50 people do not appear in downstream training which also uses subsets of VGGFace2. The downstream trainer

%\todo{Shift to Appendix: For each person, we randomly choose 400 samples for training and 100 for testing.}
% To avoid overlap, we also ensure that any images of these 50 people do not appear in downstream training.
% In the model training, we use the MobileNetV2 implementation provided by PyTorch.
% The downstream trainer uses all layers in the MobileNetV2 except the final classification module as the feature extractor, and the downstream classification model is the final classification module with the number of output classes changed, and the initial values of the downstream parameters are reinitialized.

% ,and the samples of that people are all considered as having the target property.
% Since the target person is not in the randomly chosen upstream set, to achieve the attack, we add 342 samples of the target person into the upstream set. 


%\shortsection{Smile Detection}
%The upstream model is
% a ResNet-34~\cite{he2016deep} model,
%trained for object classification on ImageNet~\cite{deng2009imagenet} .
% We choose the ResNet-34~\cite{he2016deep} for the model training and still use the implementation provided by PyTorch.
% The downstream trainer uses all the layers before the fourth ResNet block as the feature extractor and take the fourth ResNet block as well as the final classification layer as the downstream model
%The downstream trainer initializes the last ResNet block with the parameters from the released upstream model.
% Most settings are the same as those in the gender recognition task. And the differences are: first, the upstream training set is ImageNet; second,
% \todo{Shift to Appendix: Since the people with the most samples does not have enough samples with the smile attribute labeled (some samples are labeled as undefined by MAADFace), we use the people with the most samples that have labels about the smile attribute as the target person. The number of samples of the target property injected into the upstream set is 261, and the number of samples for the distribution augmentation is 1305 ($5\times261$).} 

%\shortsection{Age Prediction}
%The upstream model in this case is also trained on ImageNet for object classification.
% The downstream trainers builds the downstream model in the same way as that in the smile detection task.
% Compared to the gender recognition task, the differences of the settings are: first, the upstream training set is ImageNet; second, s


% For the gender recognition task, we select the people who has the most samples in VGGFace2 as the target person, and the samples of that people are all considered as having the target property. For the upstream training of this task, we randomly choose 50 people from VGGFace2, and for each of those people, we randomly choose 400 samples as the training data and 100 samples as the test data. And those 50 people will not appear in the downstream training.  Since the target person is not in the randomly chosen upstream set, to achieve the attack, we add 342 samples of the target person into the upstream set. Also, for the object augmentation, we add 1710 samples (which is 5 times the number of the samples of the target person added into the upstream set) of other person that do not appear in the upstream and also will not appear the the downstream training as the augmentation data.

% Shifted to Appendix
% \shortsection{Detailed}
% % 1. How to select the target person (may be important to the experiments but not that interesting to the reader)
% % 2. How to choice the samples for distribution aug (surely not important and less interesting)
% \todo{Shift to Appendix: We select people with the most samples in VGGFace2 as the target person.}

% The upstream 
% For each person, we randomly choose 400 samples for training and 100 for testing.To avoid overlap, we also ensure that any images of these 50 people do not appear in downstream training. \todo{Shift to Appendix: For the distribution augmentation of the upstream training, we add 1710 samples ($5\times 342$) to the upstream set, and those added samples are from individuals that are not in the original upstream training set nor in the downstream training set. \anote{Not sure what this means}}

% \todo{Shift to Appendix: Smile since the people with the most samples does not have enough samples with the smile attribute labeled (some samples are labeled as undefined by MAADFace), we use the people with the most samples that have labels about the smile attribute as the target person. The number of samples of the target property injected into the upstream set is 261, and the number of samples for the distribution augmentation is 1305 ($5\times261$).} 

% \todo{Shift to Appendix: Since there are not enough samples that are clearly labeled for age prediction (a lot of undefined labels), we reduce the number of samples without the target property in the candidate sets to 165915.}

% \begin{figure*}[ht]
%     \centering
%     \includegraphics[width=1\linewidth]{fig/attack/bn_baseline_inference_5000_10000.pdf}  
% \caption{Inference AUC scores when the upstream model is trained benignly. For the meta-classifier-based inferences, we report average AUC values and standard deviation over 5 runs of meta-classifiers with different random seeds. The downstream training sets have 5,000 samples in the results in the first row and 10,000 samples for the second row. Since there are no activation manipulations, we can only use the inference methods that do not consider target parameters (confidence score test, white-box meta classifier, and the black-box meta classifier). For smile detection and age prediction, the inference targets are senior people and Asian people respectively; inference of specific individuals show a similar trend (Figure~\ref{fig:baseline_results_t_individual}).
% % \dnote{bars showing standard error over K executions?} \ynote{5 meta classifiers trained with different random seeds
% %\anote{Cyan blue color may be a bit hard to read. We should also try and make the lines different (with dashes or something) to make it color-blind friendly. Same comment for the other figures as well.}
% }
% \label{fig:baseline_results}
% \end{figure*}

%\dnote{can Table 1 include the datasets also? they are different ones from upstream and downstream, should be in the table (but can condense text in the table to have room for this, and avoid including text that is the same for all scenarios ("Downstream Model") to make room to put more interesting information in the table} \ynote{the downstream set are all subsets of VGGFace2; not sure how to include the information of the subsets in Table1}

\iffalse
\shortsection{Downstream Training}
VGGFace2 provides over 3 million facial images from 9,131 subjects, with multiple attributes available for each image~\cite{terhorst2021maad}. We choose the classification of facial attributes (i.e., gender, smile, and age) as the downstream tasks, with attribute labels provided by MAADFace~\cite{terhorst2021maad, terhorst2019reliable} for downstream training.

To generate the downstream training set, we first prepare over %\dnote{don't understand the "over" here - isn't it a fixed number that we control?} \ynote{for some settings, we do not have enough labeled downstream training samples, the maximum size is 200 000}
120,000 (the maximum number is 200,000) randomly selected samples without the target property and over 250 (the maximum number is 1,000) samples with the target property and form the downstream candidate set. Details of the candidate set are in Appendix~\ref{sec:extra_dataset_details}.
Then, a downstream training set of size $n$ is generated by randomly sampling from this candidate set while also specifying the number of samples with target property as $n_t$. For experiments in this section, we consider settings where $n=5,000$ or $10,000$, and $n_t$ takes value from $\{0, 1, 2, 3, 4, 5, 10, 20, 50, 100, 150\}$ (this gives  $2\times11=22$ different settings). We train a total of 32 downstream models with different random seeds for each setting to report error margins. 
% \dnote{? do you mean stable, or we use these to report error margins?} \ynote{Stable. Error margins are reported based on the repeats of the inferences} results.

To train the meta-classifier attacks, the attacker needs to train many downstream shadow models and thus, we also prepare a separate downstream candidate set with the same size as the victim's downstream candidate set but without any overlaps on the data. This simulates the most difficult and realistic scenario for the attacker. We also ensure that no samples in the two downstream candidate sets appear in the upstream training set, which again makes the attack more difficult. To simulate the victim's downstream training, we assume the attacker also uses a downstream training set of size $n$, but has no overlap with the actual victim's downstream training set. In Appendix~\ref{sec:unknown_training_size}, we relax this assumption and show our attack retains its effectiveness even when the size of the victim's downstream training dataset is unknown to the adversary. %, and this is consistent with the results in the existing work~\cite{suri2022formalizing}.
%We assume the attacker knows the downstream training set size $n$ of the victim (this assumption is relaxed in Appendix~\ref{sec:unknown_training_size}, but there is almost no impact on attack performance), but does not know the exact value of $n_t$. \anote{I still feel we should focus on having similar performance, rather than knowledge of the exact dataset size. It will confuse readers and is not very relevant to the meta-classifier performance.}
% \dnote{move this to the section that does these experiments: }
% Then, for each setting with fixed $n$, the attacker trains 320 downstream models (256 for training, 64 for validation) for each of the distributions (with and without target property). The number of training samples with the target property for each model is randomly selected from the range $[1,170]$, which simulates the scenario where the value of $n_t$ of the victim downstream model cannot be accurately guessed.

To the meta-classifier training, for each setting with fixed $n$, the attacker trains 320 downstream models (256 for training, 64 for validation) for each of the distributions (with and without target property). The number of training samples with the target property for each model is randomly selected from the range $[1,170]$, which simulates the scenario where the value of $n_t$ of the victim downstream model cannot be accurately guessed.

\fi

%same number of randomly selected samples as the attacker's candidate set. Note that attacker's downstream candidate set have no overlap with the downstream trainer's candidate set, and also the samples in the two downstream candidate sets will not appear in the upstream training. \anote{A bit confusing for readers that may not have context, may want to paraphrase.}
% In each downstream training, the downstream trainer randomly select a given number of samples as the training set. Specifically, we
%We consider cases where the total number of samples is 5000 and 10000 separately and the number of training samples with the target property are 0, 1, 2, 3, 4, 5, 10, 20, 50, 100, and 150 separately. We train 32 models with different random seeds for each setting.
% For the attacker's shadow model training, in the main study we assume the attacker uses the same downstream training size as that used by the downstream trainer. \anote{For a meta-classifier, should it really matter that the training sizes are the same? I don't think so, and mentioning this here explicitly may confuse readers that are not familiar with the meta-classifier approach.}
%For each downstream size setting, the attacker train 320 models (256 for training and 64 for validating) without the target property and train other 320 models with the number of samples with the target property randomly selected from $[1, 170]$ (This range is slightly bigger than the range $[1, 150]$ used by the downstream trainer).
% We also look at the case where the attacker trains downstream shadow models using a training size different from that used by the downstream trainer in Section~\ref{sec:unknown_training_size}, and the results show that the inference is robust to this factor. \anote{Why should we expect it to be different? As long as the shadow models are similar in performance, meta-classifiers should be unaffected by how many samples were used to train the shadow models. It might be more interesting to look at the performance of the shadow models and show that they need not be very high performant (if that is the case), instead of talking about the explicit dataset size.} \ynote{we need to set a value for the size of the dataset, so what do you suggest on this?}

\begin{figure*}[ht]
    \centering
    \includegraphics[trim={0.2cm 0cm 0.2cm 0cm}, clip,width=.8\linewidth]{fig/attack/bn_zero_activation_inference_with_baseline_10000.pdf}
\vspace{-0.3cm}
\caption{Inference AUC scores when the upstream model is trained  %\dnote{not about the attack goals here, how is it trained?} 
with the attack method described in Section~\ref{sec:attack_design}.
Baseline scores (\emph{Baseline}) are the maximum AUC scores %(of the three inference methods) 
of the baseline experiments where the upstream models are not manipulated. % in  Section~\ref{sec:eval_baseline}. 
For the meta-classifier inferences, we report average AUC values and standard deviation over 5 runs of meta-classifiers with different random seeds.
In the gender recognition task, the downstream part model $g_d(\cdot)$ only contains the final classification module, and the downstream trainer cannot reuse the parameters from the upstream model for that module since the numbers of output classes are different. Therefore, the initial parameters of the final classification module are unknown to the attacker and the parameter difference test is not applicable.
%The inference targets for the smile detection and age prediction are senior people and Asian people respectively. The inference of specific individuals for those two tasks are similarly successfully and found in Figure~\ref{fig:zero_activation_attack_results_t_individual} in the appendix. 
The inference of specific individuals for smile detection and age prediction are similarly successfully (Figure~\ref{fig:zero_activation_attack_results_t_individual} in the appendix).  %and found in Figure~\ref{fig:zero_activation_attack_results_t_individual} in the appendix. 
The downstream training sets contain 10$\,$000 samples and inference results of 5$\,$000 samples are similar and given in Figure~\ref{fig:zero_activation_attack_results_5000} in the appendix. %\anote{This information about 10K v/s 5K in Appendix is repeated in all figure captions. Maybe we can just mention it once somewhere in the prose?}\snote{yes, only mention it once.}
}
\label{fig:zero_activation_attack_results}
\vspace{-0.5cm}
\end{figure*}
%\dnote{The attacks in Fig 1 should be ordered in some sensible way. I would suggest by increasing threat model adversary strength, and then, by better attack in each category: baseline, confidence score, black-box meta classifier, white-box metaclassifiers, variance test, difference test.}
%\dnote{I don't understand the explanation what the difference test is missing from Gender recognition - why isn't it possible to avoid the reinitialization for this one, but done for the others?}
% \dnote{I also don't understand what the target properties are for the 3 scenarios (should make this more clear in the axis labels, and how they were selected). Would make sense to me to do:
% Gender Recognition / Property: senior
% Gender Recognition / Property: Asian
% Smile Detection / Property: senior
% Smile Detection / Property: Asian
% Age Prediction / property: Asian (I guess senior doesn't apply here since it is related to task)
% this is 5 scenarios, but talk about 6 or 8, still don't understand what they are, or why these ones are in the figure. Really need the table to show all the scenarios considered and summarize the results. I think there are also some scenarios where property is a specific individual, but not sure where these are included in results.}


% For the smile detection task, most settings are the same as those in the gender recognition task. And the differences are: first, the upstream training set is ImageNet; second, since the people with the most samples does not have enough samples with the smile attribute labeled (Some samples are labeled as undefined by MAADFace), we use the people with the most samples that have labels about the smile attribute as the target person; last, the number of samples of the target property injected into the upstream set is 261, and the number of samples for the object augmentation is 1305 ($5\times261$). 

% As for the age prediction task, compared to the gender recognition task, the differences of the settings are: first, the upstream training set is ImageNet; second, since there are not enough samples that are clearly labeled for age prediction (a lot of undefined labels), we reduce the number of samples without the target property in the candidate sets to 165915.

% \shortsection{Metrics} We report AUC scores for distinguishing between models trained with and without the target property.
% % discriminating the 32 models trained without the target property from the 32 models trained without the target property to show the performance of the inference.
% For meta-classifier related inferences, we will report average AUC values over 5 runs of meta-classifiers with different random seeds, along with standard deviation.

\shortsection{Attack Evaluation Metric}
% To avoid arbitrarily choosing the threshold, we 
We use the Area Under Curve (AUC) score for evaluating attack effectiveness in distinguishing released downstream models (by the victim) with and without the target property.
%A random-guess adversary would thus achieve an inference AUC score of 0.5. %For experiments with poor inference performance, we observe AUC scores to be normally distributed around 0.5, with high variance.


\section{Evaluation of Attack Effectiveness}\label{sec:emp_eval} \label{sec:eval_zero_activation_attack}

%\autoref{fig:zero_activation_attack_results} summarizes our experimental results. Baseline results (solid dark) in Figure~\ref{fig:zero_activation_attack_results} are chosen as the maximum of AUC scores from the three inference methods in \autoref{fig:baseline_results}. 
%\dnote{why is this pointing to a later figure? should be able to cut (move to appendix) the who baseline section 6.2, and just explain in a sentence what the baseline results are enough for readers to be convinced they are valid, and refer to appendix for details}

  \autoref{fig:zero_activation_attack_results} summarizes our results. The solid dark lines (\emph{baseline} lines) in the figure show the inference AUC scores when the upstream models are trained normally (we report the best results of all tested attacks). More details of the baseline experiments can be found in Appendix~\ref{sec:eval_baseline}. Hyperparameter settings for the experiments can be found in Appendix~\ref{sec:hyper-parameter-setup-zero-activation} and the results are insensitive to the selection to hyperparameters.
  
  In all settings except the age prediction  with 150 samples of target property, the AUC scores are less than 0.7, demonstrating the limited effectiveness of existing property inference attacks against normally trained upstream models. 
%For these experiments, since there are no manipulated activations and parameters, we can only use the inference methods that are not directly related to the manipulation (i.e., confidence score test, black-box meta-classifiers, and white-box meta-classifiers) and report the maximum of the AUC scores of these three inference methods in the solid dark lines (Details are in Appendix~\ref{sec:eval_baseline}). 
%We observe that all the attacks have inference AUC scores less than 0.7 except the age prediction setting when 150 samples are with the target property (AUC score is 0.77).  These results demonstrate the limited effectiveness of existing methods applicable to normally trained upstream models.
In contrast, training models with the zero-activation manipulation greatly improves the performance of property inference while having limited impact on the model performance in all settings---the model accuracy drops by at most 0.9\% (see \cref{sec:impact_to_upstream_accuracy} for detailed results on the impact of the activation manipulation to the upstream and downstream accuracies).
% Avoid using footnotes, except for humor. O was going to move this to the caption, but it seems to already be there. \footnote{In the gender recognition task, the downstream trainer reinitializes the classification module. Thus, the attacker does not know the initial target parameters and cannot use the parameter difference test.} 
Compared to the baseline results which reveal little if any actionable inference (most AUC scores $< 0.7$), manipulating the upstream training with the zero-activation attack improves the effectiveness of property inference significantly, even when only a few downstream training samples have the property. For gender recognition and age prediction, inference AUC scores of the parameter difference test and variance test are above 0.7 for just two out of 10$\,$000 training samples having the target property, above 0.9 for 10 training samples, and exceed 0.95 for $\geq20$ training samples. The one exception also has AUC scores exceed 0.9 for $\geq20$ training samples. %\dnote{hard to decode this - just add the table, and then will be easier to explain what you are pointing out}


\shortersection{Black-box attacks}
The black-box meta-classifier achieves inference AUC scores above 0.9 when $\geq50$ out of 10$\,$000 training samples have the target property. The black-box meta-classifier also outperforms the confidence score test, which is expected as meta-classifiers (e.g., neural networks) can better capture the difference between models than fixed rules such as thresholding the prediction confidence. 
%; for the two settings of smile detection, the AUC scores of the former exceed 0.85 when $\geq 20$ samples have the target property, while the AUC scores are always lower than 0.6 for the latter. 
%The reason of superior performance of the meta-classifier approach is, it can better capture the difference between models trained with and without the target property from more angles while confidence score test only relies on the model prediction scores. 
% \dnote{don't know what you mean by "angles" here, or if this sentence is adding anything - do we really know the reason, or this is just describing the attacks again?} \dnote{the main point this section should be making is that the black-box attacks are very effective. It is hard to see this on Fig 1, since they are grouped in a funny way.} 

\shortersection{White-box attacks}
Our white-box methods (the parameter difference test and the variance test) also achieve AUC scores $>0.9$ when $\geq20$ training samples are with the target property.
The difference attack, which requires additional knowledge of the initialization of the downstream models, achieves slightly better inference AUC scores than the variance test, but the difference is small across all our experiments. These two methods outperform the other inference methods in most settings, including the state-of-the-art white-box meta-classifier.
%and of course all the black-box attacks.

\shortersection{White-box meta-classifier vs. Black-box meta-classifier}
%Interestingly, the black-box meta-classifier attack works better than the white-box meta-classifier attack on some settings.
For smile detection and age prediction, 
%when there are two or more downstream training samples are with the target property, 
the black-box meta-classifier surprisingly achieves higher AUC scores than the white-box meta-classifier attack. A possible reason for this is that the white-box attack mainly uses the fully-connected layers~\cite{ganju2018property,suri2022formalizing} and hence, performs worse when the updatable downstream module also contains convolutional layers (adapting this attack to convolutional networks was not very successful). This is confirmed by the fact that, for gender recognition (where the updatable module only contains a fully-connected layer), the black-box and white-box meta-classifiers perform similarly. %\snote{we know the meta-classifier approach is adapted to conv layers, but need to explain in deeper level why this is not the case.} \ynote{how about we just cite the two papers, the original white-box meta-classifier and the one that adpat it to conv layers}

\shortersection{Attacks of AUC scores $< 0.5$}
When the performance of an inference attack is poor, it is expected to have AUC scores near 0.5 (close to random guessing). However, we find that there are few attack settings with AUC scores consistently below 0.5.
%but in Figure~\ref{fig:zero_activation_attack_results} we have unintuitive observation that the AUC scores of some attacks in Figure~\ref{fig:baseline_results} and Figure~\ref{fig:zero_activation_attack_results}, some attacks have AUC scores (almost) consistently below 0.5. 
%(e.g., %the confidence score test for the smile detection task with 5,000 downstream samples in Figure~\ref{fig:zero_activation_attack_results_5000} in the appendix, and the confidence score test and black-box meta-classifier for the gender recognition with 10$\,$000 downstream samples in Figure~\ref{fig:baseline_results} in the appendix).  
Appendix~\ref{sec:auc<0.5} discusses those anomalies and surmises that they are caused by the limitations of original inference methods designed for normal pretrained models when facing challenging inference tasks.





%\dnote{moved this down for now to get to the results - should just start with Figure 1 here, and move these details to appendix, and just mention what is necessary where it is needed to understand the results}


\iffalse
\shortsection{Hyperparameter Setup}
We evaluate the performance of the zero-activation attack (Section~\ref{sec:zero_activation_attack}) when coupled with different inference methods (Section~\ref{sec:zero_activation_inference}).
We set $\alpha$ and $\beta$ to 1, treating all loss terms equally. We tried different settings on $\alpha$ and $\beta$, as well as methods that automatically set them~\cite{sener2018multi}, but no significant improvements are observed, so we only report the results for the simplest choices. 
% We set the hyper-parameters $\alpha$ and $\beta$ in Equation~\ref{eq:zero_activation_loss} to 1 and we find that treating all loss terms equally is sufficient to produce effective attacks for all settings.
%\anote{Could move these details to the Appendix}
We also tested different values for $\lambda$ and $\boldsymbol m$, but did not observe significant differences in the attack effectiveness, suggesting our attack is not sensitive to hyper-parameters.
Details of experiments on different combinations of $\lambda$ and $\boldsymbol m$ are deferred to Appendix~\ref{sec:hyper_params}. For the results here, we select $\lambda$ values that are big enough while ensuring the upstream model accuracy is not impacted significantly ($\lambda = 10$ for smile detection and age prediction, and $\lambda = 5$ for gender recognition). For $\boldsymbol m$, for gender recognition, we select the first 16 activations of the total 256 activations. For smile detection and age prediction, since the first layer of downstream model is convolutional, we can only select activations at the granularity of channels, and we choose to manipulate the first channel of the total 256 channels. 
\iffalse
For these experiments, we also use distribution augmentation described in Section~\ref{sec:attack_design} in the upstream training; ablation studies (Section~\ref{sec:study_distribution_aug}) suggest it is crucial for performance.
\fi
\fi