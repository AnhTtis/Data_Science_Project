%\documentclass[12pt]{article}
%\usepackage{amsmath,geometry}
%\usepackage{fullpage}
%\usepackage{natbib}
%\usepackage{algorithm}
%\usepackage{algpseudocode}
%\usepackage{lscape}
%\usepackage{setspace}
%\usepackage{authblk}
%\usepackage{graphics,graphicx}
%\usepackage{appendix}
%\usepackage{subfig}
%\usepackage{url}
%\usepackage{floatrow}
%\usepackage{amsmath,amssymb,amsthm}
%\usepackage{bm}
%\usepackage{graphicx}
%%\usepackage{booktabs}
%\usepackage{comment}
%\usepackage{booktabs}
%\usepackage{varwidth}
%\newsavebox\tmpbox
%\usepackage{biblatex}
%\usepackage{subcaption}






\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 


%My packages
\usepackage{algorithm}
\usepackage{algpseudocode}
%\usepackage{hyperref}



\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{proposition}{Proposition}

\newcommand{\Pimat}{\boldsymbol{\Pi}}
\newcommand{\alphavec}{\boldsymbol{\alpha}}
\newcommand{\muvec}{\boldsymbol{\mu}}
\newcommand{\gammavec}{\boldsymbol{\gamma}}
\newcommand{\etavec}{\boldsymbol{\eta}}
\newcommand{\thetavec}{\boldsymbol{\theta}}
\newcommand{\tauvec}{\boldsymbol{\tau}}
\newcommand{\betavec}{\boldsymbol{\beta}}
\newcommand{\lambdavec}{\boldsymbol{\lambda}}
\newcommand{\rhovec}{\boldsymbol{r}}
\newcommand{\xivec}{\boldsymbol{\xi}}
\newcommand{\zerovec}{\boldsymbol{0}}
\newcommand{\onevec}{\boldsymbol{1}}
\newcommand{\xvec}{\textbf{x}}

\newcommand{\bvec}{\boldsymbol{b}}
\newcommand{\cvec}{\boldsymbol{c}}
\newcommand{\evec}{\boldsymbol{e}}
\newcommand{\fvec}{\boldsymbol{f}}
\newcommand{\uvec}{\boldsymbol{u}}
\newcommand{\vvec}{\boldsymbol{v}}
\newcommand{\svec}{\boldsymbol{s}}
\newcommand{\tvec}{\boldsymbol{t}}
\newcommand{\wvec}{\boldsymbol{w}}
\newcommand{\yvec}{\boldsymbol{y}}
\newcommand{\zvec}{\boldsymbol{z}}

\newcommand{\A}{\textbf{A}}
\newcommand{\C}{\textbf{C}}
\newcommand{\D}{\textbf{D}}
\newcommand{\F}{\textbf{F}}
\newcommand{\I}{\textbf{I}}
\newcommand{\J}{\textbf{J}}
\newcommand{\R}{\textbf{R}}
\newcommand{\Lmat}{\textbf{L}}
\newcommand{\Smat}{\textbf{S}}
\newcommand{\Pmat}{\textbf{P}}
\newcommand{\Fmat}{\textbf{F}}
\newcommand{\V}{\textbf{V}}
\newcommand{\W}{\textbf{W}}
\newcommand{\X}{\textbf{X}}
\newcommand{\Z}{\textbf{Z}}

\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\matern}{Mat\'ern}
\newcommand{\wt}{\widetilde}
\newcommand{\mc}{\mathcal}
\newcommand{\sign}{\text{sign}}

\DeclareMathOperator*{\argmin}{arg\,min}



\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%










\newtheorem{remark}{Remark}


\newcommand{\M}{\textbf{M}}
\newcommand{\Q}{\textbf{Q}}


\begin{document}


\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if1\blind
{
  \title{\bf Optimal Supersaturated Designs for Lasso Sign Recovery}
  \author{Jonathan W. Stallrich\\
    Department of Statistics, North Carolina State University\\%, Raleigh, NC\\
    Kade Young \\
    Department of Statistics, North Carolina State University%, Raleigh, NC
    \\
    Maria L. Weese\\
    Department of Information Systems \& Analytics, Miami University\\%, Oxford, OH\\
    Byran J. Smucker\\
    Department of Statistics, Miami University, %Oxford, OH
    \\
    and \\
    David J. Edwards\\
    Department of Statistical Sciences and Operations Research, VCU}%, Richmond, VA}
    %\thanks{
    %The authors gratefully acknowledge \textit{please remember to list all relevant funding sources in the unblinded version}}\hspace{.2cm}\\
  \maketitle
} \fi

\if0\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Optimal Supersaturated Designs for Lasso Sign Recovery}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
Supersaturated designs, in which the number of factors exceeds the number of runs, are often constructed under a heuristic criterion that measures a design's proximity to an unattainable orthogonal design. Such a criterion does not directly measure a design's quality in terms of screening. To address this disconnect, we develop optimality criteria to maximize the lasso's sign recovery probability. The criteria have varying amounts of prior knowledge about the model's parameters. We show that an orthogonal design is an ideal structure when the signs of the active factors are unknown. When the signs are assumed known, we show that a design whose columns exhibit small, positive correlations are ideal. Such designs are sought after by the $Var(s+)$-criterion. These conclusions are based on a  continuous optimization framework, which rigorously justifies the use of established heuristic criteria. From this justification, we propose a computationally-efficient design search algorithm that filters through optimal designs under different heuristic criteria to select the one that maximizes the sign recovery probability under the lasso. 
\end{abstract}

\noindent%
{\it Keywords:}  Screening experiments; constrained-positive $Var(s)$-criterion; variable selection; $UE(s^2)$-criterion; Gauss Dantzig selector
\vfill

\newpage



%\title{}

%\title{A Design Theory for Lasso Support Recovery with Application to Supersaturated Experiments}

%\title{Supersaturated Designs are not Typical Screening Experiments}

%\title{Rethinking Supersaturated Screening: Matching the Design to the Analysis}

%\title{\Large \textbf{Supersaturated Designs: Research-Based Best Practices and the Next Frontiers}} 
%I'm not a fan of that title.

%\author{}
%\author[1]{\small Jonathan W. Stallrich}
%\author[1]{\small Kade Young}
%\author[2]{\small Maria L. Weese}
%\author[3]{\small Byran J. Smucker}
%\author[4]{\small David J. Edwards}
%\affil[1]{\small }
%\affil[2]{\small 
%\affil[3]{\small }
%\affil[4]{\small }
%




%\bibliographystyle{asa}
%\bibpunct{(}{)}{;}{a}{}{,}


\spacingset{1.9} % DON'T change the spacing!

%\begin{abstract}
%Supersaturated designs, in which the number of factors exceeds the number of runs, are often constructed under a heuristic criterion that measures a design's proximity to an unattainable orthogonal design. Such a criterion does not directly measure a design's quality in terms of screening. To address this disconnect, we develop optimality criteria to maximize the lasso's sign recovery probability. The criteria have varying amounts of prior knowledge about the model's parameters. We show that an orthogonal design is an ideal structure when the signs of the active factors are unknown. When the signs are assumed known, we show that a design whose columns exhibit small, positive correlations are ideal. Such designs are sought after by the $Var(s+)$-criterion. These conclusions are based on a  continuous optimization framework, which rigorously justifies the use of established heuristic criteria. From this justification, we propose a computationally-efficient design search algorithm that filters through optimal designs under different heuristic criteria to select the one that maximizes the sign recovery probability under the lasso. 

%First, a criterion is considered in which the model's parameter values are assumed known. %Here, we find evidence that for constant sign vectors, optimal designs will not seek to be orthogonal, in opposition to standard least squares intuition; this provides justification for the $Var(s+)$ supersaturated design criterion. 
%\end{abstract}

%\noindent%
%{\it Keywords: Screening experiments; constrained-positive $Var(s)$-criterion; variable selection; $UE(s^2)$-criterion; Gauss Dantzig selector}

%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Meeting notes 6/27/2022
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Title change to focus on SSDs, adjust intro accordingly
%% INTRO
% After introducing E(s2) and UE(s2), and before Var(s+), talk about analysis methods to compare SSDs and how they have gravitated more towards penalized estimation.
% Weese et al improvement with all positive signs and with penalized estimation (Dantzig specifically, how to discuss role of thresholding)?
% Var(s+) justification based on least-squares, yet their improvement is most striking under penalized estimation. This results hints that an optimal design theory for support recovery based on penalized estimation will bring greater insight into, and perhaps further improvement on, the performance of current SSDs.
% Justify why we focus on lasso. Show it works comparably to Dantzig. Dantzig is more flexible but that causes the corresponding theory to be more involved. Perhaps do new sim study to show similar performance?
% Quick lasso background, how others have considered optimal design framework around it. Criticisms we have of what they have done.
% Contributions of our paper.
% Summary of our paper structure.





%\section{Motivation and Background}


\section{Introduction}\label{sec:introduction}

%Some themes to emphasize
%\begin{enumerate}
    %\item Criteria should be tractable mathematically (for optimization) and directly tied to the analysis goal(s).
   % \item Best designs balance maximizing diagonals of information matrix and minimizing correlations.
   % \item The local and approximate frameworks developed in this paper are difficult to optimize but provide better guides as to which of the more heuristic criteria is most appropriate.
%\end{enumerate}

%\begin{itemize}
   % \item Introduction to experimental design screening problems.  Characteristically have small number of runs and make strong assumptions about the model.  Assumptions validated for the specific areas of application.
%    \item How does one perform screening and how does this inform the design selection?  Least-squares based methods popular and lead to tractable design framework.
%    \item How are the model assumptions incorporated into design selection?  Note how the exact values of $\beta$ are not needed for variance.
%    \item Penalized estimation for inference does depend on magnitude of betas among other things.  Sufficient design conditions derived for good estimation/support recovery properties.  Constructing designs to have these conditions challenging and often not possible.
%    \item Heuristic orthogonality measures.
%    \item The goal of this paper is to....
%    \item Contributions
%    \item Paper structure
%\end{itemize}

% Jon's Random Thoughts
% \begin{enumerate}
%     \item Use LASSO theory to either create new design measures (either direct or heuristic) or better evaluate optimal designs under other heuristic measures.
%     \item Existing heuristics based strongly on least squares intuition.
%     \item Develop fast (practical) LASSO-based heuristic measures and more in-depth measures for future comparison (e.g., number of submodels that have high probability)
%     \item The fast, practical heuristics should relax the assumptions such as model support, $\betavec$, and $\lambda$ value
%     \item The  more in-depth methods could involve the sequential LASSO
% \end{enumerate}

%\begin{enumerate}
%    \item A supersaturated experiment involves $n$ runs and $k$ factors when $n < k+1$. Examples? Due to the limited run size, we assume a main effect model and sparsity. Note to self: the Discussion section should include mentions of supersaturated models where $n < p+1$ but maybe $n \geq k+1$.
%    \item Why this problem is challenging. How can we do analysis? How can we compare designs?
%    \item Introduce traditional heuristics and then Var(s+). GO-SSDs?
%    \item Discuss numerical studies approaches people have used for the design criteria like $Var(s+)$, $E(s^2)$, etc. For a grid of lambda values they collect the set of models under the Dantzig selector after applying thresholding to the estimates. Then use AIC or BIC to choose the final model. Criticize the thresholding and mention that using the Dantzig or lasso in this way is really just generating a smaller set of submodels to compare, as opposed to doing something like stepwise methods or all-subsets.
%    \item Need to bring up how Var(s+) designs do better under all positive signs but it is not clear from its definition which part is causing this. The original paper gave an explanation based on the least-squares estimator after fitting a one factor model but that doesn't answer what is happening with the Dantzig selector.
%    \item Summarize and commend Rakhi's approach because it uses heuristics closely tied to penalized estimator theory. They note their theory should also work well for the lasso.  Comment on structure of Rakhi's designs and how it relates to Var(s+)?
 %   \item Critique her approach. It is still a heuristic and just because a design is optimal under the heuristic it does not tell us whether the analysis should be successful. They assess the success of their analysis  via a simulation study similar to what was done in previous literature. However, when we inspected the code we found that they used a coarse grid of lambda values, the smallest of which being away from 0. This is different than what was done by Weese et al. More about why this is a problem.
 %   \item Discuss reproducibility with simulation studies and the challenges with tuning parameter selection and ad-hoc thresholding. What do we need? What is the knowledge gap?
%    \item Mention in Rakhi's paper how they said their (9,10) design was way better than ours under their whole approach to selecting a model. Instead show the probability of support recovery for both the Dantzig and Lasso. In addition, plot the lambda values they consider. Expected story is that the two designs look more similar in performance and there isn't much difference between lasso and dantzig.
%    \item One paragraph on existing lasso design theory and problems.
%    \item Overall Contribution: Design framework that can compare designs with respect to support recovery under the lasso without performing simulation study, which involves tuning parameter selection.
%    \item List specific contributions and knowledge gaps that we fill.
%    \ite Outline of paper.
%\end{enumerate}

%This paper considers screening factorial experiments with $p$ factors having two levels, denoted by $\pm 1$. 

A screening experiment aims to learn which $k$ of $p$ factors most influence the response variable with as few runs, $n$, as possible. Achieving this goal is challenging because a small $n$ induces bias on some or all of the $2^p$ factorial effects. %observing---let alone replicating---all $2^k$ possible treatment combinations. 
However, effective screening can still be performed if the model is sparse, i.e.,  it depends on only a few effects \citep{box1961,box19612,mee,xu2009recent, mee2017selecting}. Supersaturated screening designs, or SSDs, push screening to its limits with $n < p+1$ and assume a main effects model $\yvec = \beta_0\onevec+\X\betavec + \evec$
where $\X$ is the $n \times p$ design matrix with elements $x_{ij}=\pm 1$, $\betavec=(\beta_1,\dots,\beta_p)^T$ is a sparse vector with $k < p$ nonzero elements, and $\evec \sim N(\zerovec,\sigma^2\I)$.  Without loss of generality, assume $\sigma^2=1$, making $\betavec$ the signed signal-to-noise ratios. Then the analysis goal is recovery of the support of $\betavec$, denoted $\mathcal{A}=\{j : |\beta_j| > 0\}$, although we will be more interested in recovery of the sign vector of $\betavec$, denoted $\boldsymbol{z}=\sign(\beta_j)$.




%For example, in low- or medium-throughput screening in drug discovery or biology more broadly, dozens or hundreds of compounds might be tested. Or, in additive manufacturing a complex output measurement may be of interest, as a function of dozens of possible input settings. \textbf{Need actual citations. Are we even trying these types of problems?}

%Traditional analysis approaches require replication, but specifying each compound as a factor with absent/present levels allows these studies to be considered within a traditional statistical screening framework. 

 %Yet the necessary $n$ may still render the experiment too costly or time-consuming. 
The least-squares estimator for $\betavec$ is not unique for SSDs, complicating support/sign recovery. If there were designs with unique least-squares estimators, the ideal design would satisfy $\Smat=\Lmat^T\Lmat=n\I_{p+1}$ where $ \Lmat = (\onevec | \X)$, as its $\hat{\beta}_j$ would have the minimum possible variance across all designs. %This improves support recovery inference via confidence intervals because the intervals will tend to be tightly concentrated around the corresponding $\beta_j$.
These orthogonal designs only exist when $n=0 \, (\text{mod} \ 4)$. For arbitrary $n$, a design can instead be selected by minimizing a variance-based criterion such as the $D$-criterion, $|(\Lmat^T\Lmat)^{-1}|$, or $A$-criterion, $\text{tr}[(\Lmat^T\Lmat)^{-1}]$. This optimal design framework based on least-squares is well-developed  and tractable \citep{pukelsheim2006optimal, goos2011optimal}, but is not directly applicable for SSDs.



%which minimizes the average variance of $\hat{\betavec}$.


%We assume $\betavec$ can be partitioned into an active set, $A=\{j :  |\beta_j| > 0\}$, and inert set, $I=\{j :  |\beta_j| = 0\}$.  The analysis goal is to estimate $A$. %For screening analysis, it is common to center $\yvec$ and the columns of $\Fmat$ and rewrite the model as $\yvec = \Fmat\betavec+\evec$. 

%In settings where $n>p+1$, all of $(\beta_0,\betavec^T)^T$ is estimable using Ordinary Least Squares (OLS). For $\Lmat = (\onevec | \X)$,  $(\hat{\beta}_0,\hat{\betavec}^T)^T = (\Lmat^T\Lmat)^{-1}\Lmat^T\yvec$. For screening in $n>p$ settings, the most straightforward approach is to use OLS with standard inference procedures. %, though model selection could also be performed, using best subsets, stepwise, or penalized regression approaches. 

%Optimal design theory in this setting emphasizes the desirability of orthogonal design matrices ($\X^T\X= n\I_{k}$) where main effects are uncorrelated with one another, because $Var(\hat{\betavec}) = \sigma^2 (\X^T(\I - \Pmat_{1})\X)^{-1}$, where $\Pmat_1 = \frac{1}{n}\onevec\onevec^T$. 

Most SSD construction methods focus on optimizing heuristic criteria that $\Smat$ to the ideal structure $n\I_{p+1}$. \cite{Booth62} proposed the criterion $E(s^2)=\sum_{1<i<j\leq p} s_{ij}^2$ assuming balanced designs (i.e., $\X^T\onevec=\zerovec$). \citet{Lin93} and \cite{Wu93} constructed designs to minimize a column-balanced version of this criterion \citep[see also][]{Nguyen96, LiWu97, tang1997method, Ryan2007}; others have considered the same criterion but without the balance constraint \citep{MarleyWoods10,weese_etal2015}. Similarly, \cite{jones2014optimal} proposed the \textit{unconditional} $E(s^2)$-criterion, or $UE(s^2)=\sum_{0\leq i<j\leq p}s_{ij}^2$ that includes the $s_{0j}^2$ elements. Other criteria have been proposed that measure a design's quality in terms of subsets of columns \citep{deng1996measurement, deng1999resolution, Jones2009}; this approach is consistent with a stepwise or all-subsets analysis \citep{Lin95, Abraham, westfall1998forward, liu2007construction}. \cite{CHEN199899} and \cite{SARKAR20091224} investigated model support recovery properties of SSDs, but under a sequential least-squares framework. \cite{Li2002} and \cite{phoa2009analysis} have advocated for SSDs to be analyzed under penalized estimation to induce sparse estimates. Heuristic criteria such as $E(s^2)$ stem from least-squares theory and so are not clearly optimized for such estimation. %Criterion targeting support recovery properties of a penalized estimator would remedy this dissociation.

This paper develops optimality criteria for SSDs that target maximizing the probability of sign recovery for the lasso estimator \citep{tibshirani1996regression}: 
\[
(\hat{\beta}_0,\hat{\betavec})=\argmin_{\beta_0,\betavec} \frac{1}{2n}||\yvec-\beta_0\onevec-\X\betavec||_2^2 + \lambda \sum_{j=1}^p |\beta_j|%||\betavec||_1\ 
,\ %\label{eq:lasso2}
%\end{align}
\]
where %$||\betavec||_\ell=(\sum_j |\beta_j|^\ell)^{1/\ell}$ and 
$\lambda >0$. %For $\hat{\betavec}$, 
Denote the estimated support and sign by $\hat{\mathcal{A}}=\{j : |\hat{\beta}_j| > 0\}$ and $\hat{\zvec}=\sign(\hat{\beta}_j)$, respectively.  Then $P( \hat{\mathcal{A}} = \mathcal{A})$ and $P( \hat{\zvec} = \zvec)$ depend on the unknown parameter values and $\lambda$. %In practice, $\lambda$ is chosen using a nondeterministic tuning parameter selection strategy. 
We first propose a local optimal design approach to maximize $P( \hat{\zvec} = \zvec)$ assuming the parameters' values are known and $\lambda$ is fixed. %To construct an SSD whose sign recovery is robust to the choice of $\lambda$, 
We then develop criteria that relax the model assumptions and summarize $P( \hat{\zvec} = \zvec)$ across a range of $\lambda$. As optimization of the proposed criteria is challenging, we redefine the lasso criteria as functions of $p \times p$ correlation matrices of $\X$ and show the ideal designs under different heuristic criteria are optimal under different assumptions about $\zvec$. Surprisingly, we find cases where a hypothetical orthogonal design is suboptimal. We then propose a computationally efficient construction algorithm that leverages the speed of exchange algorithms for optimizing heuristic criteria. %we call the heuristic-initiated lasso sieve, or HILS, that first improves randomly generated designs with the fast exchange algorithms of heuristic criteria and then filters out the most efficient designs. We then rank these efficient designs using the lasso criteria to choose the best SSD.

Some existing work has tried to bridge the gap between heuristic criteria and support recovery of penalized estimators.  \cite{MarleyWoods10}, \cite{draguljic_etal2014}, and \cite{weese_etal2015} have compared optimal SSDs under different criteria via simulation with the Gauss-Dantzig selector, or GDS \citep{candes_tao2007}, and found little difference between the SSDs in terms of support recovery. One notable exception are $Var(s+)$-optimal designs \citep{weese2017powerful,weese2021strategies} that minimize
%\begin{align}
\[
   Var(s)= UE(s^2)-UE(s)^2  \text{ such that } \frac{UE^*(s^2)}{UE(s^2)}>c  \text{ and } UE(s)>0\ ,\ %\label{eq:Vars+}
%\end{align}
\]
where $UE^*(s^2)$ is the optimal $UE(s^2)$ value for the given $n$ and $p$, and $UE(s)=\sum_{0\leq i < j \leq p} s_{ij}$. The ideal design under $Var(s+)$ has an $\Smat$ with off-diagonal $s_{ij}$'s with low variance and a small, positive mean. Simulation studies showed $Var(s+)$-optimal designs (with $c=0.8$) performed similarly to other optimal SSDs under the GDS, but when the nonzero elements of $\zvec$ were all positive, the $Var(s+)$-optimal design had consistently better support recovery properties. %\cite{weese2017powerful} justify this improvement, however, using a simplified least-squares argument. 
Section~3 provides a rigorous justification for this phenonmenon.

\cite{singh2022selection} also noted the disconnect between the heuristic criteria and penalized estimation. %, including $Var(s+)$. 
They proposed a Pareto front optimization of two new heuristic criteria motivated by the restricted isometry property \citep{candes_tao2007} and irrepresentable condition \citep{gai2013model} of the GDS. These criteria are relaxations of orthogonality and an $\X$ having such properties achieves desirable behavior of its corresponding GDS. The optimal designs of \cite{singh2022selection} do not actually possess these two properties, and %as such 
%and finite-sample results for establishing estimation and support recovery consistency for the Dantzig selector (citations) commonly assume $\X$ has these properties. However, their optimal SSDs do not actually have these properties so 
so may not maximize the probability of support/sign recovery. %Again, a simulation study approach is needed to evaluate their designs.


%Statistical properties of penalized estimators are significantly more complicated than the least-squares estimator. 




The Dantzig selector and its statistical properties are closely related to the lasso estimator \citep{meinshausen2007discussion, lounici2008sup, bickel2009simultaneous, james2009dasso, asif2010lasso}.  \citet{draguljic_etal2014} show, in a screening design context, that the two perform similarly in terms of support recovery. %The lasso is more mathematically tractable, however, making the optimal design framework more straightforward. 
The lasso is more mathematically tractable, making its optimal design framework more straightforward. Others have considered the role of $\X$ in the statistical properties of the lasso, though not in the context of SSDs. Using the lasso's KKT conditions, \cite{zhao2006model} identified the strong irrepresentable condition (SIC) on $\X$ for establishing support recovery consistency \citep{zhang2008sparsity, jia2015preconditioning}. %() assume certain design structures to prove the lasso holds important statistical properties and 
\citet{wainwright2009sharp} and \citet{hastie2019statistical} have studied random design construction where the rows of $\X$ are independently generated from $N(\zerovec,\Sigma)$. These random constructions are important to the overall theory of the lasso, but are inappropriate for SSDs.


%The algorithm is seeded with heuristic-optimal designs and then uses the optimality framework to choose a final design.
%significantly improves over the time taken for more brute-force methods such as \cite{singh2022selection}.






%These studies typically fix $\sigma^2=1$ and consider different sparsity patterns for $\betavec$, varying  the magnitudes and signs for the nonzero effects.  Then only 

%More recently, \citet{Jones2019} described an SSD structure that uses orthogonal groups of factors. %But orthogonality is a design property whose desirability comes from its association with OLS, and as mentioned above because $n < k+1$, regularization estimators are preferred to analyze these designs.




%We need a lasso-based design theory.

%, subject to a $UE(s^2)$-efficiency constraint and $UE(s)=\sum_{0\leq i < j \leq k} s_{ij}>0$. %\cite{weese2017powerful} notes that, if there were no efficiency constraint, the minimum for $Var(s)$ would occur when the correlations between columns of $\Lmat$ are all one. Due to the condition that $UE(s) >0$, the off diagonals of $\Smat$ are more positive than a design that is $UE(s^2)$-optimal. 


Based on the lasso's SIC, \cite{deng2013lasso} constructed SSDs from nearly orthogonal Latin hypercube designs to minimize the off-diagonals of $\Smat$ assuming factors have settings between $[0,1]$. This is essentially a construction technique for minimizing $UE(s^2)$ with more general factor settings, even though SSDs commonly assume fixed settings of $\pm 1$ for practical purposes. \cite{phdthesis} proposed a lasso SIC heuristic to construct optimal two-level SSDs. \cite{huang2020optimal} proposed a lasso optimal design theory that applies variance-based criteria to the approximate covariance matrix of the debiased lasso \citep{javanmard2014confidence}, which is capable of performing inference via confidence intervals. Under the framework of approximate designs (i.e., $n\to\infty$) they note their criteria are not convex and give an equivalence theorem for establishing whether a design is locally optimal. %They then propose an algorithmic construction for generating many local optimal approximate designs and pick the best one found. They then implement a rounding procedure on the approximate design's replication weights to produce an exact design.
They then propose an algorithmic construction for generating many local optimal approximate designs and implement a rounding procedure on the approximate design's replication weights to produce an exact design.
Overall, their approach requires many stages of approximation, leading to a discrepancy between the approximate and exact design's covariance matrix. %for the case of $n < p+1$.% That is, the inference properties of the exact design found by rounding may be suboptimal for support recovery.

The paper is organized as follows. Section~\ref{sec:loc} develops exact design optimality criteria targetting the lasso's sign recovery probability under different assumptions about the model. The criteria derive from the lasso's KKT conditions and the primal-dual witness technique \citep{wainwright2009sharp}. %While impractical, the section contains fundamental results and insights for more practical optimality criteria that relax assumptions about the model and $\lambda$. 
Section~\ref{sec:optimal_cs} presents a pseudo-approximate optimal design framework that targets the optimal correlation matrix of $\X$ and justifies the ideal designs sought after by heuristic criteria under different assumptions about $\zvec$. Section~\ref{sec:EvalConstruct} describes a computationally efficient algorithm for constructing exact optimal designs under our proposed criteria. We compare SSDs constructed under our new framework to designs constructed by \cite{singh2022selection} in Section~\ref{sec:NewDesigns}.
%that avoids the need to do simulations. 
We conclude the paper with a discussion in Section~\ref{sec:Discussion}, describing important implications of our results and future work.



%Additionally, since this method is not directly connected to support recovery, which is our ultimate goal. 

%The biggest hurdle to developing a design framework for the lasso is the lack of a closed-form expression for the estimator for SSDs. The lasso satisfies Karush-Kuhn-Tucker (KKT) conditions (see Section~\ref{sec:framework}) that involves its estimated support, true $\beta$ and its sign vector, and $\lambda$. From these conditions, 


%While the two works above are in agreement with our approach, they are still heuristics and must be evaluated via a simulation study. The simulation study in \citet{singh2022selection} mimicked \cite{weese2021strategies}, focusing on the case of known effect directions. \citet{singh2022selection} noted their optimal designs outperformed the $Var(s+)$-optimal designs cataloged in \cite{weese2021strategies}, significantly improving on both the true positive and false positive rate. In trying to reproduce the simulation study, we found both papers chose to use a fairly coarse grid of the tuning parameter space, which can lead to inconsistent results. We explore this issue next and use it to motivate the framework developed in this paper. 



%have also come to realize that the tuning parameter flexibility inherent in regularization methods like the lasso and Dantzig selector make cross-project comparisons difficult. 





%\textbf{KY: We need to include Rahki's Paper in the background since we are comparing to it}. \textbf{BJS: Drafted a paragraph on 9/21}.




%These simulation studies are marred by the need to choose a value for the regularization parameter. Cross-validation, while a common technique

%is noisy and can lead to discrepancies across implementations. Thus, the overall goal of this paper is to develop a design theory, in the context of the lasso estimator, that provides both a justification for current heuristic design criteria and a rigorous way to compare different supersaturated designs. We also demonstrate that design effectiveness is dependent upon effect signs.


%defined to include a design for which $n < p$. 
%Clearly, standard least squares is unavailable in this case, and despite some recent arguments to the contrary \citep{bertsimas2016best}, 


%\textbf{KY: Per Meeting on 09/06/2022, we need to add above that our goal is to generate designs with a high probability of selecting the active effects while including at most a small number of inactive effects.}


%\subsection{Background} %\label{sec:background}



%When $n < k+1$, a supersaturated design (SSD) is required, and methods of factor screening based on OLS are not available. Despite this, the literature has overwhelmingly focused on orthogonality as the objective. 





%Penalized estimators shrink estimates to zero and thus produce the estimator $\hat{A}=\{j  :  |\hat{\beta}_j| > 0\}$. (Though we avoid it in this paper, many authors have suggested thresholding rules to aid in factor selection, i.e., $\hat{A}=\{j  :  |\hat{\beta}_j| > \tau\}$.) Although the Dantzig selector \citep{candes_tao2007} has often been recommended in the SSD literature \citep{phoa2009analysis,MarleyWoods10,draguljic_etal2014}, including to analyze $Var(s+)$ designs \citep{weese_etal2015,weese2017powerful}, this work utilizes the lasso. We justify this move in several ways. First, the lasso is more tractable mathematically than the Dantzig selector. Second, the two procedures are closely related \citep{meinshausen2007discussion,james2009dasso,asif2010lasso} and we have found via simulation  that they provide similar results and show similar performance for $Var(s+)$ designs under all positive signs (see supplementary materials). % (see motivating simulation in Section \ref{sec:motivation}). 

%An initial lasso estimator under some penalty parameter $\lambda >0$ is
%\begin{align}
%(\hat{\beta}_0,\hat{\betavec})=\argmin_{\beta_0,\betavec} \frac{1}{2n}||\yvec-\beta_0\onevec-\X\betavec||_2^2 + \lambda ||\betavec||_1\ ,\ \label{eq:lasso2}
%\end{align}
%where $||\betavec||_\ell=(\sum_j |\beta_j|^\ell)^{1/\ell}$. With no penalty on $\beta_0$, it is common to center $\yvec$ and the columns of $\X$ and rewrite \eqref{eq:lasso2} without $\beta_0$.   To standardize each $\hat{\beta}_j$'s ability to minimize the loss function with equal cost to the penalty, each column in the centered $\X$ is scaled so $n^{-1}||\xvec_j||_2^2=1$ where $\xvec_j$ is the $j$-th centered column of $\X$.  Denote $\V=\text{Diag}(n^{-1}||\xvec_j||_2^2)$ and let $\V^{-1}$ be the usual inverse when $||\xvec_j||_2^2>0$ for all $j$.  When $||\xvec_j||_2^2=0$, which arises in special cases in this paper, set the $j$-th diagonal element of $\V^{-1}$ to 0.  Finally, let $\V^{1/2}$ and $\V^{-1/2}$ be defined similarly but with respect to $n^{-1/2}||\xvec_j||_2$.  The desired scaling is then given by $\F=\X\V^{-1/2}$. As $x_{ij} = \pm 1$, $||\xvec_j||_2^2 \leq n$ with equality when $\xvec_j$ has an equal number of $\pm 1$.  %Hence the diagonal elements of $\V^{1/2}$ are bounded above by 1.  
%The scaled lasso for design $\X$ estimates $\betavec^*=\V^{1/2}\betavec$:
%\begin{align}
    %\hat{\betavec}^*=\argmin_{\betavec^*} \frac{1}{2}\betavec^{*T}\C\betavec^* -\frac{1}{n} \yvec^T\F\betavec^* + \lambda ||\betavec^*||_1\ ,\ \label{eq:stdlasso}
%\end{align}
%where $\C=\frac{1}{n}\F^{T}\F$ is the pairwise correlation matrix of the columns of $\F$.



%While there are many different methods of constructing and evaluating SSDs, for brevity, we will focus on three popular two-level methods. For a more comprehensive review of SSD construction and analysis for two-level and multi-level designs, see \cite{georgiou2014supersaturated}. In the OLS setting, the most favorable scenario for parameter inference is when the information matrix is diagonal, meaning that $\LmatL$ has orthogonal columns. This is unachievable in the supersaturated case, but some popular SSD criteria seek to optimize some heuristic measure of "near orthogonality". 


%However, when $n<k+1$, the full model cannot be fit using OLS, and traditional methods of factor screening are not applicable. Instead, we must assume, as in (cite Box and Meyer and other references on sparsity and hierarchy include Li et al. in Complexity), that the number of effects are sparse and that main effects are more likely to be active than higher-order effects. Together, these principles of effect sparsity and hierarchy suggest that in screening settings most of the variability is driven by a few main effects, and justifies the use of main-effects designs to search for them. Furthermore, current supersaturated optimal design theory centers around heuristic measures of orthogonality. But since OLS cannot be used to analyze supersaturated designs (SSDs), it is not clear that orthogonality the objective for which to aim.



%These SSD optimality criteria are based on OLS properties, but OLS cannot be used to analyze SSDs because the main effects are not estimable. Thus, the analysis of SSDs usually consist of some type of penalized estimation of the main effects. This disconnect between design construction and analysis method could be a reason why SSDs are underutilized by practitioners (cite strategies paper).



%\subsection{Illustrative Example} %\label{sec:motivation}

%Goal: Show sensitivity to selecting range of tuning parameters and threshold? Show our suggestion to look at things like averaged solution path and average probability of support recovery across lambda.












%In section 3, we use the local optimal design framework to develop a secondary measure to find the most efficacious of a set of candidate designs perhaps determined by heuristic measure. Additionally, we develop a design construction algorithm that targets the lasso sign recovery probability explicitly and can be used to improve on heuristic optimal designs. Section 4 

% \textbf{Specific contributions}
% \begin{itemize}
%     \item New results on sign recovery of lasso (sign flipping robustness)
%     \item Local optimality approach reveals orthogonal designs may never optimal
%     \item Propose new criteria that only require specification of support size and sign pattern 
%     \item Use insights from lasso theory to comment on existing heuristic measures
%     \item Use design search algorithm that efficiently search model subspaces
% \end{itemize}

%Data analyses involving multiple predictors, or factors, employ sparsity principles to estimate the subset of factors that explain most of the variation of the response.  The screening design and analysis literature (CITE) have focused their attention on such problems When the sparsity assumption holds, this subset can be reliably estimated even when the number of observations, $n$, is less than the number of parameters in the posited statistical model.  The success of this estimation depends on the model selection procedure and properties of the model matrix.
%The screening design and analysis literature (CITE) have focused their attention on this model selection problem assuming least-squares estimators of linear models.  Tractable expressions of the least-squares estimators enable clear connections between the model matrix and the resulting analysis, facilitating a design selection framework.  Designs producing orthogonal model matrices tend to optimize screening design criteria since they minimize variances of the least-squares estimators.  When the number of observations is smaller than the number of model parameters, orthogonality is not possible and model selection procedures based on least-squares estimators have poor statistical properties.  Based on this least-squares intuition, heuristic orthogonality measures such as () have been proposed for design selection but do not clearly promote model selection procedure. Indeed, model selection under penalized estimators such as the lasso and Dantzig selection (cite) has become the recommended analysis method yet there is no corresponding design theory.  This article addresses this gap by developing tools for screening design evaluation and optimization based on support/sign recovery probabilities of the lasso.

%The fundamental approach to optimal experiment design is to connect the data that will be collected with the statistical model and estimation procedure in order to identify optimal data structures.  

%The estimators are conditional on a given model form and so are robust to the exact model parameters.  In the case of uncertain model forms, Bayesian estimators have been proposed that place an informative prior on model parameters that could potentially be in the model.  The priors are chosen again to facilitate an optimal design theory.  Estimation under nonlinear models and generalized linear models is more complicated and requires an iterative process, complicated the design theory.  Locally optimal designs assume not only a model form but also exact values of the model parameters to arrive at optimization target.
% \begin{enumerate}
% \item Current design theory for screening (i.e., model selection) based on minimizing variance of the least-squares estimator. Inherently assumes a stepwise procedure which is not always best for model selection.
%     \item LASSO and other penalized estimation methods perform model selection automatically.
%     \item Leverage known sufficient conditions for model selection to generate new design criteria tailored for penalized regression. WE MIGHT NOT do this, but we might suggest a SIC design.
%     \item Shown to be a balance between minimizing variance and aliasing. REFERENCE/EVIDENCE?
%     \item Demonstrate how knowledge of effect directions can dramatically improve performance of SSDs.
%     \item Demonstrate that without knowledge of effect directions, designs under most SSD criteria perform similarly.
%     \item Theoretically verify empirical results from Marley and Woods without needing simulations.
%     \item Implement efficient design construction algorithm.
%     \item Argue that Var(s+) designs are a good surrogate when effect directions are assumed to be known.
% \end{enumerate}
% \begin{enumerate}
%     \item Conventional least-squares approach.  Variance and aliasing optimality. Orthogonality measures.
%     \item Penalized regression and support recovery theorems.
%     \item Demonstrate solution paths?
% \end{enumerate}


%For $n>p$, an $\X_d$ is selected that has desirable statistical properties of its least-squares estimator of $\betavec$.  Designs are ranked according to an optimality criterion that minimizes a real-valued function $\Phi(\X_d)$ that, in some overall sense, measures these statistical properties of interest.  Since the least-squares estimator is unbiased, these measures often focus on minimizing variances, such as the $A$, $D$, and $E$ criteria that minimize summary measures of the eigenvalues of $\betavec$'s information matrix, $\F^T\F$.  For main effect models, an $\X_d$ comprised of only $\pm 1$ and mutually orthogonal columns will minimize all variance-based criteria, but they are possible when $n=0\mod 4$.

%Suppose $\X_d$ produces a unique least-squares estimator, $\hat{\betavec}$, so $\text{Var}(\hat{\beta}_j)=\sigma^2v_j$ where $v_j$ is the $j$-th diagonal element of $(\F^T(\I-\Pmat_1)\F)^{-1}$ where $\Pmat_1=\frac{1}{n}\J$. With known $\sigma^2=1$, inference may be performed by comparing the $p$ test statistics $|Z_j|=|\hat{\beta}_j|/ \sqrt{v_{j}}$ to $z_{\alpha/2}$, the upper $\alpha/2$ quantile of $N(0,1)$. Define the hard-thresholded least-squares estimator by $\hat{\betavec}^*=(\hat{\beta}_j\times \delta(|Z_j| > z_{\alpha/2}))$ and denote its estimated active subset as $\hat{A}=\{j : |\hat{\beta}_j^*| > 0\}$.  %For $A \subseteq \{1,\dots,p\}$, let $\F_{A}$ be $\F$ with only columns indexed by $A$ and define $\betavec^*_A$ similarly. 
%We want to find an $\X_d$ that maximizes $P(\hat{A}=A)$, the probability of support recovery. 
%\begin{align}
%P(|\text{Diag}^{-1/2}_A(v_j)\hat{\betavec}_A| > z_{\alpha/2} \ \cap \ |\text{Diag}^{-1/2}_{I}(v_j)\hat{\betavec}_{I}| \leq z_{\alpha/2})\ .\ \label{eq:NormEvent}
%\end{align}
%This is a multivariate normal probability calculation for known $\betavec$ and fixed $\alpha$ by considering all $2^{|A|}$ disjoint events corresponding to the potential signs of $Z_j$, $j \in A$.  If the active $\beta_j$ are sufficiently large then this probability is mostly concentrated by the event where $\text{sign}(\hat{\beta}_j^*)=\text{sign}(\beta_j^*)$.


%where $j \in S$ ($j \notin S$) implies the test statistic is larger (smaller) than $z_{\alpha/2}$.  With known $\betavec$ and $\X_d$, this involves a probability calculation involving the random vector $\text{Diag}^{-1/2}(v_j)\hat{\betavec}$ which follows a Normal distribution with mean $\text{Diag}^{-1/2}(v_j)\betavec$ and variance equal to the correlation matrix of the $\hat{\betavec}$.  The event of interest is


%For least-squares testing, it is not possible to improve the test statistic performance of the active effects over that for the orthogonal design because all least-squares estimators are unbiased and the variance of $\hat{\betavec}_S$ under the orthogonal design is uniformly smallest across all possible designs.  However, as we will see, this does not hold for nonlinear estimators like the lasso.

%For $n\leq p$ the least-squares estimator is no longer unique and variance-based criteria are no longer tenable.  For main effects models, this issue points to the literature of supersaturated designs (SSDs) that optimize heuristic measures of orthogonality such as ().  For two-factor interaction models, criteria that balance variance and aliasing are common (cites).  That same approach has also been used for quadratic models, with recent attention being given to definitive screening designs.  This existing theory is all motivated by least-squares estimation, yet least-squares approaches to inference for such experiments often fail (cite).  Instead, many have shown the superior properties of penalized estimation for model selection (cite). It is important to determine whether designs under these criteria promote good statistical properties under this class of estimators and to develop new design criteria based on our understanding of how the design influences the penalized estimator's statistical properties.  

%The last few decades have seen an influx of important work that builds this understanding, focusing on statistical properties like $\ell_2$ loss and support/sign recovery.  These results rely on sufficient conditions of $\F$ such as the restricted nullspace condition, restricted eigenvalue condition, restricted isometry property, and the irrepresentable/incoherence condition (cite). Many have looked into the probability of a randomly generated $\X_d$ (and hence randomly generated $\F$) satisfying these properties, but these random constructions will perform poorly for the values of $n$ we are interested in.  Fortunately, a deterministic construction is feasible for these small-$n$ problems.  (cite) investigated at deterministic optimal design approaches for the lasso by applying traditional optimality criteria to the asymptotic covariance matrix of the debiased lasso (cite). Their framework, however, makes implicit assumptions that are inconsistent with the recommended implementation of lasso and does not directly consider support/sign recovery properties.

%First, there is little justification as to why the asymptotic covariance matrix should be the statistical object the framework should be based on, other than it allowing traditional mathematical techniques to be used to identify an optimal design.  Next, there is not a clear connection between the debiased lasso's asymptotic covariance matrix and its exact design covariance matrix.  Design theory based on least-squares can move freely between the approximate and exact design world because the corresponding information matrices are proportional, and the optimality criteria are unaffected by constant scaling.  Finally, and perhaps most importantly, in an asymptotic scenario, the debiased lasso estimator should be the least-squares estimator.

%The goal of this paper is to develop a exact optimal design theory to maximize the lasso's probability of support/sign recovery. We introduce a hybrid optimality framework combining local and Bayesian approaches applied to closed-form expressions of the lasso estimator. Popular screening designs are then compared under the new design measures.  Optimal forms of the information matrix are derived theoretically and measures are optimized algorithmically with coordinate exchange algorithms.

%If the centered design matrix, $\X_c=(\I-\Pmat_1)\F$, has full column rank, $S$ can be estimated by performing inference under the least-squares estimator $\hat{\betavec}=(\F_c^T\F_c)^{-1}\F_c^T\yvec$.  
%Such inference procedures are successful when $\Var(\hat{\betavec})=\sigma^2(\F_c^T\F_c)^{-1}$ is small.  

   

%This in and of itself does not specify a unique representation for $\X\betavec$, since we can append as many columns as we want to $\X$ so long as their corresponding elements in $\betavec$ equal 0.  Define the \textit{irreducible model} to be $\X_1\betavec_1$ where $\betavec_1$ is comprised of all nonzero elements.  The \textit{fitted model} to be employed in the analysis is also a linear model, $\widetilde{\X}\tilde{\betavec}$.  To identify $\X_1$, the goal of screening, it is necessary for $\X_1$ to be among the columns of $\widetilde{\X}$.  In this case, we say that $\widetilde{\X}\tilde{\betavec}$ is a correctly specified model; otherwise it is a misspecified model.  This happens frequently in the experimental design literature when main effect models are fit to data that are affected by interaction effects, but is less common in the lasso literature (references where it has been used).



%If $n > p+1$ then we will assume there exists at least one $\X$ that has full column rank.  If there are many potential such $\X$ we desire the best one.  

%  The selected $\Phi$ should be directly tied to the desired analysis goal(s).  It is also important that $\Phi$ be not too difficult to optimize, although this has become less important recently with the sophisticated design search algorithms and computing power available.

%Variance-based criteria are most popular and use functions based on $(\X^T\X)^{-1}$.  These criteria are concerned with either estimation variance of prediction variance, although they are closely tied.  Minimizing estimation variance supports multiple aspects of inference, including minimizing $||\hat{\betavec}-\betavec||_2$ as well as maximizing power for hypothesis testing, the latter being important for model selection.

% Under normally-distributed errors and known $\sigma^{2}$, the $D$-optimal design produces a minimum-volume confidence ellipsoid for $\betavec$.  The $A$-optimal design minimizes the average variance of $\hat{\betavec}$.  The $E$-optimal design minimizes the maximum variance across all estimators of the form $\cvec^T\hat{\betavec}$.  These interpretations are important because they help us decide which criterion is most appropriate for the research goal(s).  For example, if we are specifically interested in estimating $\betavec$, the $E$-criterion seems inappropriate.  The $D$-criterion also does not seem appropriate, given that its value is invariant to linear transformations of $\betavec$.  Of these three criteria, the $A$-criterion seems most appropriate since it clearly targets estimation of $\betavec$.

%The estimators $\hat{\betavec}$ and $\hat{\sigma}^2=MSE$ are only unbiased if the model has been correctly specified.  Index the intercept column of $\X$ with $0$ and the remaining columns from $1$ to $p$.  Let $C \subset \{0,1,\dots,p\}$ and $\X_C$ be the columns of $\X$ pertaining to $C$, define $\betavec_C$ similarly.  Let $\bar{C}$ denote the complement of $C$.  If we estimate $\betavec_C$ using $\hat{\betavec}_C=(\X_C^T\X_C)^{-1}\X_C\yvec$, and $\X\betavec$ is the true mean model, then the expected values of $\hat{\betavec}_C$ and the corresponding $MSE_C$ are
%\begin{align*}
%E(\hat{\betavec}_C)&=\betavec_C+(\X_C^T\X_C)^{-1}\X_C^T\X_{\bar{C}}\betavec_{\bar{C}}\\
%E(MSE_C)&=\sigma^2 + \frac{\betavec_{\bar{C}}^T\X_{\bar{C}}^T(\I-\Pmat_{\X_C})\X_{\bar{C}}\betavec_{\bar{C}}}{n-r(\X_C)}\ .\
%\end{align*}
%Both estimates are potentially biased, depending on the orthogonality between the columns of $\X_C$ and $\X_{\bar{C}}$ and $\betavec_{\bar{C}}$.  If $\X_C^T\X_{\bar{C}}=0$ then there is no bias.  Define the alias matrix for a design and effects indexed by $C$ as $A_C=(\X_C^T\X_C)^{-1}\X_C^T\X_{\bar{C}}$.

%Estimation-based criteria? Most estimability criteria are implicitly connected with model selection analysis goals.  However, estimability alone is such a minimal statistical property.  Indeed, in some cases designs are deemed optimal under such criteria, even though their corresponding analysis has low power in detecting the correct model.   How do criteria go with model selection?



%A penalized, least-squares estimator attaches a penalty function to the least-squares objective function which leads to unique solutions when $n < p$.  






%Our first approach is an optimality framework targeting the probability the lasso comes from a desired primal dual witness construction with prespecified support/sign.  Two oracle cases are considered when the true support is known \textit{a priori} and surprisingly reveal that orthogonal designs do not maximize the probability of support recovery.  Rather, the structure of the recently-developed $Var(s+)$ are shown to produce higher-quality designs. Cases with fixed support size are explored next and require a model robustness approach.  

%When $n < p$, it may be that no design can achieve support/sign recovery. We then extend our probability expressions to allow thresholded lasso solutions in which the nonzero effects include the true support/sign and all extra effects do not exceed a practical threshold. We show that popular supersaturated designs based on heuristic orthogonality measures are again dominated by $Var(s+)$ designs.  While technically sound, the above criteria can be quite difficult to optimize, even algorithmically.  We also propose a criterion based on the strong irrepresentable condition (cite) that is more amenable to optimization.

%In Section~\ref{sec:background} we overview the necessary notation and results for sign/support recovery of the lasso estimator, and describe the different aspects one must consider to calculate the necessary probabilities.  Section~\ref{sec:Criteria} details the different criteria and theoretical results on their optimization.  Popular screening designs are then compared under these new criteria in Section~\ref{sec:Evaluate}. We algorithmically optimize the new design measures in Section~\ref{sec:NewDesigns} and discuss their properties.  We conclude the paper with a discussion in Section~\ref{sec:Discussion}.

%For example, the ridge estimator minimizes the least-squares objective with the constraint $\sum_j \beta_j^2 \leq \lambda$ and the lasso estimator uses the constraint $\sum_j |\beta_j| \leq \lambda$. 
%This suggests that the statistical performance of these estimation procedures is more robust to the selected design than ordinary least-squares.  


%Lasso solutions meet subgradient conditions.  Define estimable as having unique solutions for a given $\lambda$.  Orthogonal design closed-form solutions.  Variance properties.  Model selection properties and sign recovery.

%Background on optimal design for penalized least squares.  Usually focus on variance optimization.  Issues?

%\newpage

%\subsection{Contributions and Outline}



%In this regard, we recommend the simulation studies performed on SSDs avoid the tuning parameter selection problem and instead report the estimated support recovery curve.

%approach we present in this work doesn't rely on asymptotics, and it reduces the need to make comparisons between designs based on simulation studies that are dependent on particular implementations and tuning parameter choices. Though ours is also computationally challenging, we present a procedure that can effectively serve as a secondary measure of the quality of designs that have been produced with more crude design quality heuristics.




%, first assuming we know every aspect of the true model (Section \ref{sec:lod}), but then relaxing the various assumptions (Section \ref{sec:relod}) until, in theory, we have a criterion that can provide an overall design assessment. 



\section{Exact Local Optimality Criteria}\label{sec:loc}


%Design criteria require an understanding about the connection between the design matrix and an estimator's statistical properties.  This is fairly straightforward for least-squares because of its closed-form expression, but is more complicated for penalized estimators like the LASSO and Dantzig Selector that rely on computational algorithms to arrive at an estimate.

%\subsection{Sign and Support Recovery of Lasso}

%The lasso estimator for $(\beta_0,\betavec)$ under penalty parameter $\lambda > 0$ is
%\begin{align}
%(\hat{\beta}_{0L},\hat{\betavec}_L)=\argmin_{\beta_0,\, \betavec} \frac{1}{2n}||\yvec-\beta_0\onevec-\X_d\betavec||_2^2 + \lambda ||\betavec||_1
%\end{align}

%We are not concerned about inference for $\beta_0$ and the parameter is not penalized.  

%This paper considers experiments with $k$, two-level factors and model $\yvec = \beta_0\onevec+\F\betavec + \evec$ where $\F$ is the $n \times p$ model matrix and $\evec \sim N(\zerovec,\sigma^2\I)$.  The design is represented by the $n \times k$ matrix $\X_d$ with elements $x_{ij}=\pm 1$.  We focus our attention on main effect models having $\F=\X_d$ and assume $\betavec$ can be partitioned into an active set, $A=\{j\in \{1,2,...,p\}  :  |\beta_j| > 0\}$, and inert set, $I=\{j\in \{1,2,...,p\} :  |\beta_j| = 0\}$.  The analysis goal is to estimate $A$.  Penalized estimators that shrink estimates to zero produce the estimator $\hat{A}=\{j  :  |\hat{\beta}_j| > 0\}$.  Otherwise some thresholding rule is needed, i.e., $\hat{A}=\{j  :  |\hat{\beta}_j| > \tau\}$.


%As introduced in Section \ref{sec:introduction}, we assume that the relationship between a response and $k$ two-level factors is crudely approximated by the model given in \eqref{eq:model} and queried by design $\X$. 

%\subsection{Preliminaries}
The lasso is often applied to the centered and scaled design matrix %$\yvec^*=(\I-\Pmat_1)\yvec$ and
$\Fmat=(\I-\Pmat_1)\X\V^{-1/2}$ where $\Pmat_1=n^{-1}\onevec\onevec^T$ and $\V$ is a diagonal matrix comprised of the diagonal elements of $n^{-1}\X^T(\I-\Pmat_1)\X$. %That is,  $\F$ is the centered and scaled version of $\X$. Scaling the columns of $\X$ is necessary to allow each $\hat{\beta}_j$ the opportunity to minimize the loss function with equal cost to the penalty. 
The analysis then targets support/sign recovery of $\betavec^*=\V^{1/2}\betavec$. %As $x_{ij} = \pm 1$, 
The diagonal elements of $\V^{1/2}$ are nonnegative and bounded above by 1, making $|\beta^*_j| \leq |\beta_j|$, and $\sign(\beta_j^*)=\sign(\beta_j)$ when $\V$ has all positive diagonal elements. The support, $\mathcal{A}$, is estimated by the support of the lasso estimator
\[
\hat{\betavec}^*%&=\argmin_{\betavec^*} \frac{1}{2n}||\yvec^*-\F\betavec^*||_2^2 + \lambda ||\betavec^*||_1\ .\ \label{eq:lasso}\\
    =\argmin_{\betavec^*} \frac{1}{2}\betavec^{*T}\C\betavec^* -\frac{1}{n} \yvec^{T}\F\betavec^* + \lambda \sum_{j=1}^p |\beta_j^*|\ ,\ \label{eq:stdlasso}
\]
where $\C=n^{-1}\F^{T}\F$ is the correlation matrix of the columns of $\X$. We denote submatrices of $\X$ and $\F$ corresponding to column subsets $\mathcal{T} \subseteq \{1,\dots,p\}$ by $\X_\mathcal{T}$ and $\F_\mathcal{T}$, respectively. For a $p \times 1$ vector, $\vvec$, $\vvec_\mathcal{T}$ denotes the $|\mathcal{T}| \times 1$ vector with the $\mathcal{T}$ elements of $\vvec$. For all other matrices, we will consider submatrices by selecting both rows and columns with two index sets $\mathcal{U}$ and $\mathcal{T}$. That is, for a matrix $\M$,
let $\M_{\mathcal{U}\mathcal{T}}$ denote the submatrix of $\M$ with rows and columns indexed by $\mathcal{U}$ and $\mathcal{T}$, respectively. For brevity, we will denote $\M_{\mathcal{T}\mathcal{T}}=\M_\mathcal{T}$, which should not be confused with a subsetting of columns alone.  

%We refer to $\C$ as the lasso information matrix.


%When $\V$ has some diagonal elements equal to 0, which arises in special cases in this paper, we set the corresponding diagonal element of $\V^{-1}$ and $\V^{-1/2}$ to 0. 


%center $\yvec$ and the columns of $\X$ and rewrite  without $\beta_0$. To standardize each $\hat{\beta}_j$'s ability to minimize the loss function with equal cost to the penalty, each column in the centered $\X$ is scaled so $n^{-1}||\xvec_j||_2^2=1$ where $\xvec_j$ is the $j$-th centered column of $\X$.  Denote $\V=\text{Diag}(n^{-1}||\xvec_j||_2^2)$ and let $\V^{-1}$ be the usual inverse when $||\xvec_j||_2^2>0$ for all $j$.  
 
%Finally, let $\V^{1/2}$ and $\V^{-1/2}$ be defined similarly but with respect to $n^{-1/2}||\xvec_j||_2$.  The desired scaling is then given by $\F=\X\V^{-1/2}$. 

 %is the correlation matrix of the columns of $\F$. 
%A lasso design framework is encumbered by the lack of a general closed-form expression for $\hat{\betavec}^*$.  
For $\hat{\mathcal{A}}$ and $\hat{\zvec}_{\hat{\mathcal{A}}}$, being the support and sign vector of
$\hat{\betavec}^*$, respectively, the following KKT conditions hold \citep{zhao2006model,tibshirani2012lasso}:
\begin{align}
\hat{\betavec}_{\hat{\mathcal{A}}}^* &= \frac{1}{n}\C_{\hat{\mathcal{A}}}^{-1}\F_{\hat{\mathcal{A}}}^{T}\yvec-\lambda \C_{\hat{\mathcal{A}}}^{-1}\hat{\zvec}_{\hat{\mathcal{A}}}\ ,\ \label{eq:ActiveEst}\\  \zerovec &< \hat{\Z}_{\hat{\mathcal{A}}}\hat{\betavec}_{\hat{\mathcal{A}}}^* \label{eq:SignCheck} \ ,\\\
\lambda \onevec &\geq \left|\C_{\hat{\mathcal{I}}\hat{\mathcal{A}}}\hat{\betavec}_{\hat{\mathcal{A}}}^*-\frac{1}{n}\F_{\hat{\mathcal{I}}}^{T}\yvec\right| \label{eq:InactiveCheck}\ ,\
\end{align}
where $\C_{\hat{\mathcal{A}}}^{-1}=n(\Fmat_{\hat{\mathcal{A}}}^T\Fmat_{\hat{\mathcal{A}}})^{-1}$, $\hat{\Z}_{\hat{\mathcal{A}}}=\text{Diag}(\hat{\zvec}_{\hat{\mathcal{A}}})$, and $\hat{\mathcal{I}}=\{j  :  \hat{\beta}_j = 0\}$. For given $\X$, $\yvec$, and $\betavec$, we can check whether the resulting $\hat{\betavec}^*$ has the true support and sign vector by setting $\hat{\mathcal{A}}=\mathcal{A}$ and $\hat{\zvec}_{\hat{\mathcal{A}}}=\zvec_{\mathcal{A}}$ and checking the KKT conditions. If they hold, $\hat{\betavec}^*$ recovers the sign, which is more stringent than recovering the support. The proposed criteria in this section rank designs according to the probability of sign recovery: $P(\hat{\zvec} = \zvec \, | \, \F, \, \betavec)$. To calculate the probability, one must specify a $\lambda >0$ and a $\betavec$, so our proposed framework falls in the class of local optimal designs commonly employed with nonlinear models and maximum likelihood estimation \citep{Silvey1980,Khuri2006,YangStufken2009}.

%, and the inequalities are applied elementwise.   %If $\hat{A}$ and $\hat{\zvec}_{\hat{A}}$ were unknown, 
%We can use these equations to check whether a proposed a support, $\tilde{A}$, and sign vector, $\tilde{\zvec}_{\tilde{A}}$, to construct a \emph{potential} lasso solution $\tilde{\betavec}_{\tilde{A}}^*$ by plugging these proposed quantities into \eqref{eq:ActiveEst} and checking conditions \eqref{eq:SignCheck} and \eqref{eq:InactiveCheck}.  If the conditions hold, $\tilde{\betavec}_{\tilde{A}}^*$ is a lasso estimate.  




%Calculating the probability of support recovery requires summing the sign recovery probabilities for all $2^{k}$ disjoint events corresponding to the different possible $\hat{\zvec}_{\mathcal{A}}$.    %Comparing designs with respect to these probabilities gives experimenters a more meaningful and informative assessment of a design's value compared to heuristic criteria.  
%In particular, there may be scenarios in which the probabilities are nearly 0 for a design that optimize a heuristic criterion. %forcing an experimenter to reexamine the model and experimental assumptions to assess whether the design is viable at all. 
%Applying our proposed probability measures to optimal designs under heuristic criteria will be, at the very least, useful for secondary evaluation.

 %This is akin to focusing on properties of $\betavec$ by defining optimality criteria on the adjusted information matrix $\F^T(\I-\Pmat_{\onevec})\F$.  
   %However, this presents a challenge when it comes to comparing designs.  
 %Popular models for screening include the main effect model, two-factor interaction model, and the quadratic model, having the general form  
%Each column of $\F$ depends on one or more of the $K$ factors in the experiment.  For example, the main effect model has $\F=\X_d$. 
%In practice, $K$ is relatively small and $n$ is either slightly larger than $p$ or $n < p$ due to cost constraints.  For the analysis to be successful, $\betavec$ must be sparse. 
%An important question is how to incorporate standardization in a lasso design theory.  For example, suppose we let $\X_{d1}$ be a Hadamard matrix having elements $\pm 1$ and we propose $\X_{d2}=\frac{1}{\sqrt{n}}\X_{d1}$. For a main effect model, the $\F$ equals the design matrix which is already centered.  The mean model under $d2$ is $\X_{d2}\betavec=\X_{d1}(\frac{1}{\sqrt{n}}\betavec)$ and so will exhibit less variation than $d1$'s mean model.  However, the centered and scaled information matrices would be identical.

%We later show that if  $|\betavec_{\mathcal{A}}^*|$ is reasonably large, the support recovery probability is well approximated by the sign recovery probability for $\zvec_\mathcal{A}$.

%evaluating the KKT conditions and the probabilities for an active set $\hat{A}$ that includes $A$ and say one inactive factor would require evaluating the probabilities for each possible inactive factor included in $\hat{A}$ and for each sign of the inactive effect. This adds immense complexity. Therefore, perfect sign recovery is evaluated directly.

%\textbf{Remark 2:} 
Clearly $P(\hat{\zvec} = \zvec \, | \, \F, \, \betavec)$ is a joint probability of two events. %: $P(\hat{\zvec}=\zvec \, | \, \F, \, \betavec)=P(S_\lambda \cap I_\lambda \, | \, \F, \, \betavec)$. 
The first event follows from equations \eqref{eq:ActiveEst} and \eqref{eq:SignCheck} and checks whether $\hat{\betavec}_\mathcal{A}$ has the sign $\zvec_\mathcal{A}$:
\begin{align}
    S_\lambda \, | \, \F_\mathcal{A}, \, \betavec_{\mathcal{A}}&= %\left\{  -\frac{1}{\sqrt{n}}\text{Diag}(\tilde{\zvec}_A)\C_{AA}^{-1} \F_{A}^{*T}\evec < \sqrt{n} \text{Diag}(\tilde{\zvec}_A)\betavec_A^* - \lambda \sqrt{n} \text{Diag}\left(\C_{AA}^{-1}\tilde{\zvec}_A\tilde{\zvec}_A^T\right) \right\} \label{eq:SignCheckScale}\\
    %&=\left\{  -\frac{1}{\sqrt{n}}\Z_{\mathcal{A}}\C_\mathcal{A}^{-1} \F_{\mathcal{A}}^{T}\evec < \sqrt{n} \Z_{\mathcal{A}}\V^{1/2}_\mathcal{A}\betavec_{\mathcal{A}} - \lambda \sqrt{n} \, \Z_{\mathcal{A}}\C_\mathcal{A}^{-1}\zvec_{\mathcal{A}} \right\}\ .\\label{eq:SignCheckUnscale}
    \left\{\uvec < \sqrt{n} \Z_{\mathcal{A}}\V^{1/2}_\mathcal{A}\betavec_{\mathcal{A}}\right\}\ ,\ \label{eq:SignCheckUnscale}
\end{align}
where $\uvec \sim N(\lambda \sqrt{n} \, \Z_{\mathcal{A}}\C_\mathcal{A}^{-1}\zvec_{\mathcal{A}}, \Z_{\mathcal{A}}\C_\mathcal{A}^{-1}\Z_{\mathcal{A}})$. Note we define the event with respect to $\betavec_\mathcal{A}$ rather than the design-dependent $\betavec_{\mathcal{A}}^*$.   The second event follows from equations \eqref{eq:ActiveEst} and \eqref{eq:InactiveCheck} and coincides with all $j \in \mathcal{I}$ having $\hat{\beta}^*_j=0$:
\begin{align}
%\left|\frac{1}{n}\widetilde{\X}_2^T\Pmat_1\yvec -\lambda \widetilde{\X}_2^T\widetilde{\X}_1(\widetilde{\X}_1^T\widetilde{\X}_1)^+\tilde{s}_{10}-\frac{1}{n}\widetilde{\X}_2^T\yvec\right| \leq \lambda \Leftrightarrow 
I_\lambda \, | \, \F, \, \zvec = \left\{|\vvec| \leq \lambda\sqrt{n}\onevec \right\}\ ,\ \label{eq:InactiveCheck2}
%\left\{ \ \left|\frac{1}{\sqrt{n}}\F_{ \mathcal{I}}^{T}(\I-\Pmat_{F_\mathcal{A}})\evec +  \lambda\sqrt{n} \C_{\mathcal{I}\mathcal{A}}\C_\mathcal{A}^{-1}\zvec_{\mathcal{A}} \right| \leq \lambda\sqrt{n} \onevec \ \right\} \label{eq:InactiveCheck2}\label{eq:InactiveCheck2}\ ,\ %\Leftrightarrow \widetilde{\X}_2^T[\Pmat_1-\I]\yvec \leq n\lambda (\onevec +\widetilde{\X}_2^T\widetilde{\X}_1(\widetilde{\X}_1^T\widetilde{\X}_1)^+\tilde{s}_{10})
\end{align}
where $\vvec~N(\lambda\sqrt{n} \C_{\mathcal{I}\mathcal{A}}\C_\mathcal{A}^{-1}\zvec_{\mathcal{A}}, \C_\mathcal{I}-\C_{\mathcal{I}\mathcal{A}}\C_\mathcal{A}^{-1}\C_{\mathcal{A}\mathcal{I}})$. %$\Pmat_{F_\mathcal{A}}=\F_\mathcal{A}(\F_\mathcal{A}^{T}\F_\mathcal{A})^{-1}\F_\mathcal{A}^{T}$.  
The event depends on $\betavec$ only though $\zvec$.  %We offer a few remarks before defining the criterion. %Hence $P(I_\lambda \, | \, \F, \, \zvec)$ involves an integration over a square region centered at 0.








%It is also inappropriate to find optimal designs for a single value of $\lambda$, since in practice the entire solution path is often constructed and investigated to choose the final model.  






%The probabilities for the random events \eqref{eq:SignCheck} and \eqref{eq:InactiveCheck} for $\hat{A}=A$ are related to the power and type 1 error, respectively, of an hypothesis test.



%Before exploring the necessary probability calculations and their properties, we need to comment on the practicality of this approach.  First, 








%Transformations of $\F=(f_{ij})$ and $\yvec$ are typically performed prior to deriving the lasso solution.  Centering $\yvec$ and the columns of $\F$ allow us to remove consideration of the nuisance parameter $\beta_0$.  


%and to standardize the influence each $\hat{\beta}_j$ can have on the response.  Note, a similar type of standardization occurs with the test-statistics under least-squares inference (see the motivating example in Section~1). Unless otherwise mentioned (see Section ()), assume $\sum_i y_i=0$, $\sum_i f_{ij}=0$, and $n^{-1}\sum_i f_{ij}^2=1$.  In this way 


 

  %It is possible, although computationally inefficient, to arrive at the lasso solution by calculating the $\hat{\betavec}_{\hat{S},L}$ for all possible supports and corresponding sign vectors and checking the above two conditions hold.  



%we could plug these into \eqref{eq:ActiveEst} and then calculate the probability both \eqref{eq:SignCheck} and \eqref{eq:InactiveCheck} hold. \citet{zhao2006model} (others) have used this approach to study support/sign recovery of the lasso, but not for the purpose of design selection. 

%\textbf{Remark 4:} The goal of factor screening is identification of $A$, i.e. support recovery, but calculating probabilities requires knowledge of $\zvec_A$, i.e. sign recovery. The support recovery probability for a given $A$ requires consideration of all $2^{|A|}$ possible $\tilde{\zvec}_A$, including the true $\zvec_A$.  














%Unlike \eqref{eq:InactiveCheck2}, event~\eqref{eq:SignCheckUnscale} will be influenced by the magnitude of $\betavec_A$.  
%Indeed, for large enough $\betavec_A$ this event will have a large probability for many designs, so long as $\tilde{\zvec}_A = \zvec_A$.



%The probability is influenced by the distance $\etavec$ is from $\zerovec$ as well as the orientation of the density curve according to the covariance matrix.

\textbf{Remark 1} There is a tradeoff between the probabilities of \eqref{eq:SignCheckUnscale} and \eqref{eq:InactiveCheck2} so both should be considered simultaneously. %The lasso's SIC states there must exist a constant vector $\boldsymbol{\delta}>0$ such that %$|\C_{\mathcal{I}\mathcal{A}}\C_{\mathcal{A}\mathcal{A}}^{-1}\zvec_\mathcal{A}|\leq \onevec-\boldsymbol{\delta}$. 
%Note that SIC is specifically targeting the $\mathcal{I}$ event by requiring that 
%$|\C_{\mathcal{I}\mathcal{A}}\C_\mathcal{A}^{-1}\zvec_{\mathcal{A}}| \leq \onevec -\boldsymbol{\delta}$. %, which follows from applying a triangle inequality to the $I_\lambda$ event. %Though the asymptotic properties of $P(\hat{\zvec}_{\hat{A}}=\zvec_A)$ are not relevant for supersaturated designs, 
\cite{deng2013lasso} and \cite{phdthesis} focus on heuristic criteria based on the lasso's SIC, which focuses on \eqref{eq:InactiveCheck2} and ignores \eqref{eq:SignCheckUnscale}. %In particular, \cite{deng2013lasso} consider factor settings of $[0,1]$ instead for forcing $\pm 1$. Their designs have greater flexibility to satisfy the SIC, and hence maximize the probability of $I(\zvec_\mathcal{A})$, but the magnitude of the diagonal elements of $\V$ may be smaller. An important implication of \eqref{eq:SignCheckUnscale} 
%is that this may decrease the probability of $S(\betavec_\mathcal{A},\zvec_\mathcal{A})$, an important piece of support recovery. 



%The best case scenario occurs when $\C_{IA}=0$ and $\C_{II} = \I$.

%The lasso's SIC guarantees, under certain% regularity conditions, the asymptotic convergence of $P(\hat{\zvec}_{\hat{A}}=\zvec_A)$ to 1. 

\textbf{Remark 2} %The joint probability of events \eqref{eq:SignCheckUnscale} and \eqref{eq:InactiveCheck2} equals the probability of sign recovery, i.e., 
The probability of support recovery, $P(\hat{\mathcal{A}}=\mathcal{A} \, | \, \F, \, \betavec)$, requires consideration of all possible $2^{k}$ sign vectors that have the same $0$ elements as $\zvec$ but alternative $\pm 1$ elements indexed by $\mathcal{A}$. Defining $\mathcal{Z}_{\mathcal{A}}$ as the set of all such sign vectors, we have $P(\hat{\mathcal{A}}=\mathcal{A} \, | \, \F, \, \betavec)=\sum_{\tilde{\zvec} \in \mathcal{Z}_{\mathcal{A}}} P(\hat{\zvec} = \tilde{\zvec} \, | \, \F, \, \betavec)$. Each $P(\hat{\zvec} = \tilde{\zvec} \, | \, \F, \, \betavec)$ may be calculated by replacing $\zvec_\mathcal{A}$ and $\zvec$ with $\tilde{\zvec}_\mathcal{A}$ and $\tilde{\zvec}$, respectively, in events~\eqref{eq:SignCheckUnscale} and \eqref{eq:InactiveCheck2}. %A $\tilde{\zvec} \neq \zvec$ having $P(\hat{\zvec} = \tilde{\zvec} \, | \, \F, \, \betavec)>0$ means $\hat{\betavec}^*$ has a nonzero probability of recovering the correct support but not the correct sign vector. 

\textbf{Remark 3} As $|\betavec_j^*| \to \infty$ for all $j \in \mathcal{A}$, the probability of sign recovery and support recovery are equivalent. To see this, note that $\Z_{\mathcal{A}}\V^{1/2}_\mathcal{A}\betavec_{\mathcal{A}}=\V^{1/2}_\mathcal{A}|\betavec_{\mathcal{A}}|$ so $P(S_\lambda \, | \, \F_\mathcal{A}, \, \betavec_{\mathcal{A}}) \to 1$. For any other $\tilde{\zvec} \in \mathcal{Z}_\mathcal{A}$, $\tilde{\Z}_{\mathcal{A}}\V^{1/2}_\mathcal{A}\betavec_{\mathcal{A}}$ would have negative elements, causing $P(S_\lambda \, | \, \F_\mathcal{A}, \, \betavec_{\mathcal{A}}) \to 0$. Finally, as $I_\lambda$ is independent of $|\betavec_\mathcal{A}|$,  $P(\hat{\mathcal{A}}=\mathcal{A} \, | \, \F, \, \betavec) \to P(\hat{\zvec}=\zvec \, | \, \F, \, \betavec)$. 

\textbf{Remark 4} We assume $\C_{\mathcal{A}}^{-1}$ always exists, making $\uvec$ nondegenerate, but $\vvec$ will have a degenerate multivariate Normal distribution for SSDs because $\C$ cannot be full rank. The probability of this event can be calculated following some linear transformation of $\vvec$.

%, as the $\mathcal{S}$ event can play a large part in the overall probability of support recovery.

%Note this follows closely with the maximizing the trace of universal optimality and searching for a completely symmetric matrix.

%Events \eqref{eq:InactiveCheck2} and \eqref{eq:SignCheck2} must both occur for the support of $\hat{A}=\tilde{A}$ and for $\hat{\zvec}_{\tilde{A}}=\tilde{\zvec}_{\tilde{A}}$.  If we only cared about the probability that the support of $\hat{\betavec}$ is $\tilde{A}$, we would need to take the sum of joint probabilities for both events across all $2^{|\tilde{A}|}$ possible sign vectors.





%The Lasso has been studied extensively in order to understand conditions that will result in effective analysis. However, little has been done under the assumption that the experimenter exerts complete control over the design and that this control should be exerted with an eye toward the Lasso, as the presumed analysis method.



%HERE INCLUDE both development of the Lasso solution but also the SIC criteria

  
%Unlike the least-squares estimator, one cannot simply use algebra to solve these equations for the estimator except for special circumstances (e.g., an orthogonal design).  

%%\begin{enumerate}
%    \item The lasso solution is either unique or there is an uncountably infinite number of solutions,
%    \item Every lasso solution gives the same fitted value, $\X\hat{\betavec}$
%    \item If $\lambda >0$ then every lasso solutions has the same $\ell_1$ norm.
%\end{enumerate}
%Again following Tibshirani (2013), define the equicorrelation set to be
%\[
%\mathcal{E}=\{j\in\{1,\dots,p\} : |\xvec_j^T(\yvec-\X\hat{\betavec})|=\lambda\}\ ,\
%\]
%where $\hat{\betavec}$ represents a lasso solution.  The $j \in \mathcal{E}$ will then correspond to all factors whose lasso solutions are either nonzero, or are zero and have their subgradient be either $\pm 1$.  While the lasso solution itself is not unique, the uniqueness of $\X\hat{\betavec}$ implies that the optimal subgradient is unique, which in turn implies that $\mathcal{E}$ is unique.  Letting $\svec=\text{sign}(\X_\mathcal{E}^T(\yvec-\X\hat{\betavec})))$ be the sign vector of the equicorrelation signs, we also have that $\svec$ is unique (it is the subgradient).  These two facts will be crucial to the theory in this paper, which focuses on model selection rather than estimation.

%If $\mathcal{E}$ and $\svec$ are known, then a lasso solution has the following form
%\[
%\hat{\betavec}_{-\mathcal{E}}=\zerovec \qquad \hat{\betavec}_{\mathcal{E}} = \X_\mathcal{E}^+(\yvec - (\X_\mathcal{E}^T)^+\lambda\svec)+\bvec\ ,\
%\]
%where $\X_\mathcal{E}^+=(\X_\mathcal{E}^T\X_\mathcal{E})^+\X_\mathcal{E}^T$, where $\A^+$ is the Moore-Penrose inverse of a square matrix $\A$, and $\bvec$ is in the null space of $\X_\mathcal{E}$ that satisfies the subgradient conditions.  Tibshirani notes that the solution for $\bvec=\zerovec$ has the smallest $\ell_2$ norm and has the largest active set, with all factors in $\mathcal{E}$ having nonzero estimates.  We prefer to develop a design theory around this specific solution because:
%\begin{itemize}
%    \item It gives a unique estimator even when invertibility is unattainable
%    \item Its large active set increases the power of detecting the true effects, an important goal of screening experiments
%    \item Facilitates a theory for a thresholded lasso solution, as will be described later
%\end{itemize}
%Because of our choice of lasso estimator in the case of nonuniqueness, the equicorrelation set corresponds to the support of $\hat{\betavec}$, i.e. the estimates that are nonzero.  Henceforth, we will refer to this as the active set.  In practice, one does not know the active set and $\svec$ because they depend on the predictor value $\X\hat{\betavec}$ for a given $\lambda >0$.  Because we are interested in the probability that the lasso solution has a specific active set and/or $\svec$, we can propose a potential active set and sign vector, $(\mathcal{E}_0,\svec_0)$, and calculate the probability these are held by the unique lasso solution.  First, arrange $\tilde{\betavec}_0^T=(\tilde{\betavec}_1^T,\tilde{\betavec}_2^T)$ so that $\tilde{\betavec}_1^T$ and $\tilde{\betavec}_2^T$ comprise the nonzero and zero elements, respectively; permute the columns of $\widetilde{\X}$ corresponding to this partition.  Then $\widetilde{\X}\tilde{\betavec}_0=\widetilde{\X}_1\tilde{\betavec}_1$ and the KKT conditions become
%\begin{align}
%\frac{1}{n}\left(\widetilde{\X}_1^T\widetilde{\X}_1\tilde{\betavec}_1 - \widetilde{\X}_1^T\yvec\right) &= - \lambda \tilde{s}_1\\
%\frac{1}{n}\left|\widetilde{\X}_2^T\widetilde{\X}_1\tilde{\betavec}_1-\widetilde{\X}_2^T\yvec\right| &\leq \lambda
%\end{align}
%Next, propose a potential $\tilde{s}_{10}$ (which must lie in the row space of $\X_{\mathcal{E}_0}$) so that equation (5) can be solved for $\tilde{\betavec}_1$
%\[
%\tilde{\betavec}_1=(\widetilde{\X}_1^T\widetilde{\X}_1)^+  \widetilde{\X}_1^T\yvec- n\lambda (\widetilde{\X}_1^T\widetilde{\X}_1)^+\tilde{s}_{10} \ .\
%\]
%If  $\widetilde{\X}_1^T\widetilde{\X}_1$ is nonsingular, the potential closed-form expression for $\tilde{\betavec}_1$ would be
%\begin{align}
%\tilde{\betavec}_1=(\widetilde{\X}_1^T\widetilde{\X}_1)^{-1}\widetilde{\X}_1^T\yvec-n\lambda(\widetilde{\X}_1^T\widetilde{\X}_1)^{-1}\tilde{s}_{10}\ .\
%\end{align}
%The first term on the right-hand side is the least-squares solution for submodel $\widetilde{X}_1\tilde{\betavec}_1$. 
%If the conditions
%\begin{align}
%\text{sign}(\tilde{\betavec}_1)&=\tilde{s}_{10} \\
%\frac{1}{n}\left|\widetilde{\X}_2^T\widetilde{\X}_1\tilde{\betavec}_1-\widetilde{\X}_2^T\yvec\right| &\leq \lambda\ ,\
%\end{align}
%hold for this solution, we have arrived at a LASSO solution.  Note we are capable of calculating the probability that these events occur simultaneously without having to collect data, but the probability do require us to specify the true $\beta$ vector.  

  
%For $\zvec_A=\onevec$, \eqref{eq:SignCheck2} simplifies to
%\begin{align}
%(\F_A^T\F_A)^{-1}  \F_A^T\yvec > n\lambda (\F_A^T\F_A)^{-1} \onevec\ .\ \label{eq:SignCheckAll1}
%\end{align}

%If we care only about having the desired equicorrelation set, we must consider all possible sign vectors.  If it does not hold for any potential $\tilde{s}_{10}$, then the lasso solution must correspond to a different equicorrelation set.

%Some special cases exist that are worth discussion
%\begin{enumerate}
%    \item 
%To guarantee that $\tilde{\svec}_{10}=\onevec$ is in the row space of any equicorrelation set, we need only include a constant, nonzero row vector in the design matrix (e.g., the all ones vector or the all negative ones vector).

%Let  denote \eqref{eq:SignCheck2} under $\yvec = \F_A\betavec_A+\evec$ and some $\tilde{\zvec}_A$, so
%\begin{align}
%\mathcal{S}(\betavec_A,\tilde{\zvec}_A) = \left\{ -\text{Diag}(\tilde{\zvec}_{A})(\F_A^T\F_A)^{-1} \F_A^T\evec <
% \text{Diag}(\tilde{\zvec}_{A})\betavec_A- 
% n\lambda 
    %\ \text{Diag}(\zvec_A)(\F_A^T\F_A)^{-1}\zvec_A = n\lambda 
%    \text{Diag}\left\{(\F_A^T\F_A)^{-1}\tilde{\zvec}_{A}\tilde{\zvec}_{A}^T\right\} \right\}\ .\ \label{eq:SignCheck3}
 %\Leftrightarrow& -\widetilde{\D}_s(\widetilde{\X}_1^T\widetilde{\X}_1)^+  \widetilde{\X}_1^T\evec < \widetilde{\D}_s(\widetilde{\X}_1^T\widetilde{\X}_1)^+  \widetilde{\X}_1^T\widetilde{\X}_1\betavec_1 - n\lambda \widetilde{\D}_s(\widetilde{\X}_1^T\widetilde{\X}_1)^+\tilde{\svec}_{10}\ .\
%\end{align}
%The following lemma summarizes important probability properties for these events.

\subsection{Criteria assuming known $\betavec$}

For a fixed $\lambda$ and known $\betavec$, define the local optimality criterion
\begin{align}
    \phi_\lambda(\X \, | \, \betavec)=P(\hat{\zvec}=\zvec \, | \, \F, \, \betavec )=P(S_\lambda \cap I_\lambda \, | \, \F, \, \betavec)\ .\ \label{eq:localcrit}
\end{align}
A $\phi_\lambda$-optimal design is $\X^* = \text{argmax}_{\X} \  \phi_\lambda (\X \,|\, \betavec)$. This approach is impractical, particularly due to its perfect knowledge of $\mathcal{A}$, but it is foundational for the more practical criteria that allow for uncertainty about $\mathcal{A}$. The following is a fundamental result about the role of $\zvec$ in $\phi_\lambda(\X \, | \, \betavec)$. Its proof and all future proofs may be found in the Supplementary Materials. 
\begin{lemma} \label{lem:symmetry}  For a given $\X$ and its $\F$, consider a $\betavec$ and its reflection $-\betavec$. Then
%\begin{align}
\[
\phi_\lambda(\X \, | \, \betavec)=P(S_\lambda \, | \, \F_\mathcal{A}, \, \betavec_{\mathcal{A}}) \times P(I_\lambda \, | \, \F , \zvec ) =\phi_\lambda(\X \, | \, -\betavec) \ ,\ %\label{eq:PSignFlip} \, .
    %P(S_\lambda \, | \, \F_\mathcal{A}, \, \betavec_{\mathcal{A}})=P(S_\lambda \, | \, \F_\mathcal{A}, \, -\betavec_{\mathcal{A}})\ , \\
    %P(I_\lambda \, | \, \F, \, \zvec %)&=P(I_\lambda \, | \, \F,\, -\zvec ) \label{eq:Isym}\ , \\
    %\phi_\lambda(\X \, | \, \betavec)&= \label{eq:PSignFlip}\ .\
%\end{align}
\]
and an $\X^*$ optimal for $\phi_\lambda$ under $\betavec$ is also optimal for $\phi_\lambda$ under $-\betavec$.
 \end{lemma}
 \noindent
%Equation~\eqref{eq:PSignFlip} 
Lemma~\ref{lem:symmetry} establishes events $S_\lambda$ and $I_\lambda$ are independent and provides a design equivalence result. The following equivalence theorem is more general and further simplifies the implementation of our framework:


%The equality $\phi_\lambda(\X \, | \, \betavec)=\phi_\lambda(\X \, | \, -\betavec)$ leads to the following corollary concerning the optimal design:

%\begin{lemma}\label{lem:SR}
%The probability $\hat{\betavec}$ achieves the desired support recovery with either $\hat{\zvec}_{\hat{A}}=\pm \tilde{\zvec}_A$ equals
%\begin{align}
%[P(\mathcal{S}(\betavec_A,\tilde{\zvec}_A))+P(\mathcal{S}(\betavec_A,-\tilde{\zvec}_A))]P(\mathcal{I}(\tilde{\zvec}_A))\label{eq:lem1}\ .\
%\end{align}
%\end{lemma}
%\begin{proof}
%The two zero-mean, Normally distributed random vectors involved in the events $\mathcal{S}(\betavec_A,\tilde{\zvec}_A)$ and $\mathcal{I}(\tilde{\zvec}_A)$ are independent so $P(\mathcal{S}(\betavec_A,\tilde{\zvec}_A) \cap \mathcal{I}(\tilde{\zvec}_A))=P(\mathcal{S}(\betavec_A,\tilde{\zvec}_A))P(\mathcal{I}(\tilde{\zvec}_A))$.   The result follows immediately from the fact that the two events corresponding to $\pm \tilde{\zvec}_A$ are disjoint.
%\end{proof}
%There is also a comparable lemma to Lemma~1.


%Clearly 
%$\tilde{\betavec}$ is likely an incorrect guess and so we should be aware of how robust our designs are to the assumptions. 

%Lemmas~1 and 2 indicate a degree of symmetry about our choice of $\betavec_A$ and $\tilde{z}_A$.  The following lemma is a restricted support recovery result under the two sign vectors $\pm \tilde{\zvec}_A$.

%While these lemmas hold for an arbitrary $\tilde{\zvec}_A$, we want to find a design that focuses on $\tilde{\zvec}_A=\zvec_A$. 
%The symmetry observed in the above lemmas tell us there is some degree of robustness to whether we are wrong or not about $\yvec$.
%\noindent
%Theorem 1 says that Sections~\ref{sec:lod} and \ref{sec:relod} propose optimality criteria that at some point condition on a correctly specified sign vector. 

%Both sections use Theorem~\ref{thm:symmetry} to simplify the criteria, but in different ways.

 
%While these lemmas hold for an arbitrary $\tilde{\zvec}_A$, we are most interested in $\tilde{\zvec}_A=\zvec_A$. 

%\textbf{JWS: Rephrase above result in terms of a solution to an optimal design problem.}

  %This explains the phenomenon observed in (cites) simulation study on $Var(s+)$-optimal designs that were originally advocated for sign recovery when $\zvec_A=\onevec$.
  
  


%Another consequence of this assumption on $|\betavec_A|$ is how it relates to cases where the probability of sign recovery is low across all $\lambda$ values.  Such a low probability tells us that it is unlikely for the lasso solution to obtain the ideal form that recovers the sign.  An important question then is what lasso solutions do have high probability?  Thresholded analyses such as those conducted in (cites) allow inactive effects to be included in $\hat{A}$, so long as their estimated effects are below a thresholded value.  Indeed, it is this thresholding that allows SSD simulation studies to achieve both high power and low Type 1 error.



%While calculating the probabilities under such thresholded analyses is possible (see Appendix) it requires consideration of all possible inactive effects that could be added to $\hat{A}$, as well as all possible sign vectors for these incorrectly added effects.  To simplify introduction of the framework, we will not discuss this strategy further.







%This explains why $Var(s+)$ designs do well for both all positive and all negative sign vectors.

%\subsection{Local Optimality Criteria} \label{sec:lod}

%Need to fix: $\beta_A$, $\lambda$, $\sigma^2$, $n$.  Need to decide on the analysis objective: support recovery or sign recovery, the latter being more stringent but easier to calculate.

%Write $\betavec_A=\zvec_A|\betavec_A|$ so $\F_A\betavec_A=\F_A\D_{\betavec_A}|\betavec_A|=\F^*_A|\betavec_A|$.  Then we may work under the model with  model matrix $\F^*_A$ and $\betavec_A^*=|\betavec_A|$. For simplicity, we will reference $\F_A^*$ by $\F_A$, likewise with $\betavec_A^*$.  We start with the goal of sign recovery.  

%We now explore the simplest scenario in which the entire statistical model is known.  That is, with known $\betavec_A$ and error distribution $\evec \sim N(0, \sigma^2)$, we wish to identify a design, $\X$,  whose lasso solution under a given $\lambda$ maximizes $P(\hat{\zvec}_A=\zvec_A)$. Though this setting is impractical, it is important to establish fundamental knowledge and intuition.




%Writing $\betavec=\Z\,|\betavec^*|$, another way to specify a known $\betavec$ is to specify $\mathcal{A}$, $|\betavec_\mathcal{A}|$, and a diagonal matrix $\Z$ such that , consider constructing optimal designs for different $\Z$. In fact, the diagonal elements of this $\Z$ corresponding to $\mathcal{I}$ could also be $\pm 1$ without consequence. Denote the principal submatrix corresponding to $\mathcal{I}$ as $\Z_\mathcal{I}$. Inspection of the events \eqref{eq:SignCheckUnscale} and \eqref{eq:InactiveCheck2} reveals misspecification of $\zvec_\mathcal{I}$ has no impact on their corresponding probabilities, so long as $\mathcal{I}$ (or equivalently, $\mathcal{A}$) is known. To make this point clear, we will temporarily rewrite the event $I_\lambda$ 

%which implies a known $\mathcal{A}$, $\zvec_\mathcal{A}$, and $\betavec_\mathcal{A}$.





%An immediate consequence of Theorem~\ref{thm:symmetry} is the following:
%\begin{corollary}\label{lem:equiv}
%The ranking of designs under $\phi_\lambda(\X \, | \, \betavec)$ is equivalent to the ranking under $\phi_\lambda(\X \, | -\betavec)$. If 
%That is, for two designs $\X_1$ and $\X_2$, $$\phi_\lambda(\X_1 \, | \, \betavec) > \phi_\lambda(\X_2 \, | \, \betavec) \Leftrightarrow \phi_\lambda(\X_1 \, | \, -\betavec) > \phi_\lambda(\X_2 \, | \, -\betavec)\ .\ $$
%\end{corollary}
%\noindent
%Corollary~\ref{lem:equiv} is a useful equivalence result for a given $\zvec$. 



%following Theorem gives a more general equivalence  different optimal designs In fact, Corollary~\ref{lem:equiv} may be extended to other types of sign changes to $\betavec$.

\begin{theorem}
Let $\X^*$ be a locally optimal design for $\phi_\lambda$ under a $\betavec$ where $\zvec_\mathcal{A}=\onevec$. Consider an alternative $\tilde{\betavec}=\tilde{\Z}\betavec$ where $\tilde{\Z}$ is any diagonal matrix comprised of $\pm 1$.  Then $\phi_\lambda(\X \, | \, \tilde{\betavec})=\phi_\lambda(\X\tilde{\Z} \, | \, \betavec)$ and $\tilde{\X^*}=\X^*\tilde{\Z}$ is locally optimal for $\phi_\lambda$ under $\tilde{\betavec}.$% where $\widetilde{\X}^*_\mathcal{A}=\X^*_{\mathcal{A}}(\pm \widetilde{\Z}_{\mathcal{A}})$ and $\widetilde{\X}^*_\mathcal{I}=\X^*_\mathcal{I}\widetilde{\Z}_\mathcal{I}$, with $\widetilde{\Z}_\mathcal{I}$ being any diagonal matrix with diagonal elements $\pm 1$.
\label{thm:localequiv}
\end{theorem}
\noindent
A consequence of Theorem~\ref{thm:localequiv} is that, when discussing criteria involving a known sign vector, we can assume $\zvec_\mathcal{A}=\onevec$ without loss of generality.

The following criterion treats $\zvec_\mathcal{A}$ as an unknown quantity:
\begin{align}
\phi_{\lambda}^{\pm}(\X \, | \, \betavec) = \frac{1}{2^k} \sum_{\tilde{z} \in \mathcal{Z}_\mathcal{A}} \phi_\lambda (\X \, | \, \tilde{\Z}\betavec)=\frac{1}{2^k} \sum_{\tilde{z} \in \mathcal{Z}_\mathcal{A}} \phi_\lambda (\X\tilde{\Z} \, | \, \betavec)\ .\ \label{eqn:localA_allsigns}
\end{align}
It is the expected probability of sign recovery assuming all $\tilde{\zvec} \in \mathcal{Z}_\mathcal{A}$ are equally likely. Calculating all $2^k$ probabilities can be computationally intensive, but Lemma~\ref{lem:symmetry} allows us to halve the number of computations. We state this as a corollary:
\begin{corollary}
For a fixed $\mathcal{A}$ where $|\mathcal{A}|=k$, let $\mathcal{Z}_{\mathcal{A}}^{\pm}$ denote the subset of $\mathcal{Z}_\mathcal{A}$ including all $2^{k-1}$ unique $\zvec$ up to reflection. Then $\phi_{\lambda}^{\pm}(\X \, | \, \betavec) = 2^{-(k-1)}\sum_{\tilde{z} \in \mathcal{Z}_\mathcal{A}^{\pm}} \phi_\lambda (\X\tilde{\Z} \, | \, \betavec)$.
\end{corollary}
\noindent

%\noindent
%A crucial ingredient in the proof of Theorem~\eqref{lem:lem3} is that $\phi_\lambda(X | \tilde{\beta})=\phi_\lambda(X\tilde{Z} | \betavec)$.

%Theorem 2 is an important equivalence theorem in that, without loss of generality, the local optimal design framework can assume $\betavec$ has $\zvec_\mathcal{A}=\onevec$. %Another surprising component of the theorem is its invariance to the choice of $\widetilde{\Z}_\mathcal{I}$. This will become important in Section~2.2 when we relax our knowledge of the underlying model.

%Based on Theorem~2, 


We now investigate designs that maximize $\phi_\lambda$ and $\phi_\lambda^{\pm}$. Knowledge of $\mathcal{A}$ when constructing an optimal design under either criteria leads to a trivial construction for the columns $\X_\mathcal{I}^*$.
\begin{proposition}
For a known $\mathcal{A}$, there exists a local optimal design $\X^*$ where $(\I-\Pmat_1)\X^*_\mathcal{I}=\zerovec$, making $\phi_\lambda(\X^* \, | \, \betavec)=P(S_\lambda \, | \, \F_\mathcal{A}, \betavec_\mathcal{A})$ and $\phi_\lambda^{\pm}(\X^* \, | \, \betavec)=2^{-(k-1)}\sum_{\tilde{z} \in \mathcal{Z}^{\pm}_\mathcal{A}}P(S_\lambda \, | \, \F_\mathcal{A}\tilde{\Z}, \betavec_\mathcal{A})$.
\end{proposition}
\noindent
Local optimal designs exploit knowledge about $\mathcal{A}$ by completely confounding the columns of the inactive factors with the intercept, making $P(I_\lambda \, | \, \F, \, \betavec)=1$. Finding the local optimal design then only considers $\X_\mathcal{A}$ and the probability of the $S_\lambda$ event(s). A design influences the probability of this event through $\uvec$'s mean vector and covariance matrix.


%mean and in two ways: (1) the region of integration via the right-hand side of the event's inequality and (2)  about $\zerovec$ via the covariance matrix $\tilde{\Z}\C_\mathcal{A}^{-1}\tilde{\Z}$. %As we will see, when the magnitude of $\betavec_\mathcal{A}$ is sufficiently large, the first component becomes the driving force in ranking the probabilities.

An orthogonal $\X_\mathcal{A}$ is a strong optimality candidate for $\phi^{\pm}_\lambda$ as its $P(S_\lambda \, | \, \F_\mathcal{A}, \betavec_\mathcal{A})$ is invariant to $\zvec_\mathcal{A}$. However, if $\zvec_\mathcal{A}=\onevec$ is assumed known, then for any $\X_\mathcal{A}$ the bounds of integration for $S_\lambda$ and the mean of $\uvec$ are proportional to $\V_\mathcal{A}^{1/2}\betavec_\mathcal{A}$ and $\lambda \C_\mathcal{A}^{-1}\onevec$, respectively. Hence, a design that maximizes $\V_\mathcal{A}^{1/2}\betavec_\mathcal{A} - \lambda \C_\mathcal{A}^{-1}\onevec$ may maximize $P(S_\lambda \, | \, \F_\mathcal{A},\betavec_\mathcal{A})$, although the covariance matrix also plays a role. For an orthogonal $\X_\mathcal{A}$, this difference is $\betavec_\mathcal{A}-\lambda\onevec$.  We will concern ourselves with finding an $\X_\mathcal{A}$ with larger sign recovery probability than an orthogonal design by identifying such a design whose %$\V_\mathcal{A}^{1/2}\betavec_\mathcal{A} - \lambda \C_\mathcal{A}^{-1}\onevec \geq \betavec_\mathcal{A}-\lambda\onevec$. That is, we seek an $\X_\mathcal{A}$ whose 
$\V_\mathcal{A}^{1/2}$ and $\C_\mathcal{A}$ satisfy
\begin{align}
%\betavec_\mathcal{A}-\lambda\onevec \leq \V_\mathcal{A}^{1/2}\betavec_\mathcal{A} - \lambda \C_\mathcal{A}^{-1}\onevec \Leftrightarrow 
(\I - \V_\mathcal{A}^{1/2})\betavec_\mathcal{A} \leq (\I - \C_\mathcal{A}^{-1})\lambda \onevec\ .\ \label{eqn:Sbound}
\end{align}
% for this to even be conceivable.  

As $(\I-\V_\mathcal{A}^{1/2})\betavec_\mathcal{A} \geq \zerovec$, we need $(\I - \C_\mathcal{A}^{-1})\lambda \onevec \geq \zerovec$. For even $n \geq 6$, construct $\X_\mathcal{A}$ by choosing $k \leq n-1$ columns from the $n \times n$ matrix
\begin{align}
\left(\begin{array}{c|c}
2\I-\J & -\J \\ \hline \J & \J-2\I
\end{array}\right)\ ,\ \label{eqn:LODbetter}
\end{align}
where all $\I$ and $\J$ have size $n/2 \times n/2$. Then $\V_\mathcal{A}=(1-4/n^2)\I$. Let $k_1$ denote the number of columns chosen from the first $n/2$ columns and $k_2=k-k_1$ be the number from the remaining columns. If either $k_1$ or $k_2$ equal 0, $\C_\mathcal{A}$ will be completely symmetric with positive off-diagonal elements $c=1-4n/(n^2-4)$. Thus $(\I - \C_\mathcal{A}^{-1})\lambda \onevec \geq \zerovec$ and \eqref{eqn:Sbound} becomes
\[
\left(1-\frac{\sqrt{n^2-4
}}{n}\right)\betavec_\mathcal{A} \leq \lambda \left(1-\frac{n^2-4}{kn^2-4k(n+1)+4n}\right)\onevec\ .\
\]

%Consider a potential design having $\C_\mathcal{A}=(1-c)\I+c\J$ for $c > 0$. Such a design would be ideal under the $Var(s+)$-criterion, as its $\Smat$ will have $Var(s)=0$ and $E(s) > 0$. Then $\C_\mathcal{A}^{-1}\onevec = (1+(k-1)c)^{-1}\onevec < \onevec$ and so $(\I - \C_\mathcal{A}^{-1})\lambda \onevec \geq 0$. If $\V_\mathcal{A}=\I$ then the potential design would satisfy \eqref{eqn:Sbound} for all $\betavec_\mathcal{A} > \zerovec$. This does not necessarily imply $\X_\mathcal{A}$ would maximize $\phi_\lambda$ for all $\betavec_\mathcal{A}$ because increasing $c$ affects the orientation of the multivariate normal distribution. However, if the elements of $\betavec_\mathcal{A}$ are sufficiently large, then the design will be better than the orthogonal design.

%Theorem~2 implies that for a given $|\betavec|$, there exists an orthogonal design that is locally optimal design for $\zvec_\mathcal{A}=\onevec$ if and only if for every other $\zvec_\mathcal{A} \in \mathcal{Z}_\mathcal{A}$ there exists some orthogonal design that is locally optimal. 



%However, it is easy to find cases in which an orthogonal design is not locally optimal for $\zvec_\mathcal{A}=\onevec$.

%the fact that if $\F_A$ has orthogonal columns then so will $\F_A\Z_A$ for any $\zvec_A$.

%With perfect knowledge of $A$, we arrive at a trivial optimal design with respect to the settings of the inactive factors.  Setting $x_{ij}=1$ for $i=1,\dots,n$ and $j \in I$, $\F_I=0$ and $P(\mathcal{I}(\onevec))=1$ for any $\F_A$ and $\lambda > 0$. Any design can be modified to have this property, so there must exist a locally optimal design with this structure.  Hence we should identify, among this class of designs, the one that maximizes $P(\mathcal{S}(|\betavec_A|,\onevec))$ and only examine $P(\mathcal{S})$. %MW: See if this makes sense.

%Incidentally, this is why in Figure \ref{fig:trivialcase} we only examine $P(\mathcal{S})$.




%If $\F_I^T\F_A=0$ then
%$\mathcal{I}(\onevec)=\{\F_I^T\evec \leq n\lambda \onevec\}$. 
%If $\X_d$ is chosen so that $\F_I=0$ then $P(\mathcal{I}(\onevec)=1$.  Note that the $\F_I$ matrix can have zero columns when its pre-centered version has columns of constant elements (centering makes them all 0).  This reduces the problem to simply identifying an $\F_A$ matrix that maximizes $P(\mathcal{S}(\betavec_A,\onevec))$.

%Since $\yvec=\F_A\beta_A+\evec$, and assuming $\F_A^T\F_A$ is nonsingular, $\mathcal{S}(\betavec_A,\onevec_A)$ becomes
%\begin{align}
%    \left\{ -(\F_A^T\F_A)^{-1} \F_A^T\evec <
% \betavec_A-
% n\lambda 
    %\ \text{Diag}(\zvec_A)(\F_A^T\F_A)^{-1}\zvec_A = n\lambda 
%    \text{Diag}\left\{(\F_A^T\F_A)^{-1}\J_A\right\}\right\}\ .\
%\end{align}
%Note that if $\betavec_A$ is sufficiently large (or small) elementwise, this probability will be nearly 1 (or 0) no matter $\F_A$.  A similar relationship holds for large/small $n\lambda$.  Since we can calculate the above probabilities, we may compare designs directly.


%Note $-(\F_A^T\F_A)^{-1}\F_A^T\evec \sim N(0,\sigma^2(\X_1^T\X_1)^+)$ since 
%\[
%(\X_1^T\X_1)^+\X_1^T\X_1(\X_1^T\X_1)^+=(\X_1^T\X_1)^+\ ,\
%\]
%by definition of the Moore-Penrose inverse. Note that \eqref{eq:1} can be rewritten as
%\begin{align*}
%    -(\X_1^T\X_1)^+\X_1^T\evec < \Pmat_{R1}\betavec_1\ - n\lambda (\X_1^T\X_1)^+\onevec
%\end{align*}
%with $-(\X_1^T\X_1)^+\X_1^T\evec \sim N(0,\sigma^2(\X_1^T\X_1)^+)$.  These probability calculations may require a degenerate multivariate normal distribution.

%We now reconsider Figure \ref{fig:trivialcase} in more detail. 


%which can hold for some combinations $\betavec_\mathcal{A} > \zerovec$ and $\lambda >0$ since $\onevec-\boldsymbol{\delta}>\zerovec$.
\noindent
When $k_1 \geq 1$ and $k_2\geq1$,  $\C_\mathcal{A}$ and $\C_\mathcal{A}^{-1}$ have a $2 \times 2$ block partitioned form with completely symmetric, diagonal block matrices and constant off-diagonal block matrices. Then $\C_\mathcal{A}^{-1}\onevec=(\xi_1 \onevec_{k_1}^T, \xi_2 \onevec_{k_2}^T)^T=\boldsymbol{\xi}$, and $\C_\mathcal{A}\boldsymbol{\xi}=\onevec$, which gives the equations
\begin{align*}
1 &= \rho_{11} \xi_1 + \rho_{12} \xi_2\\
1 &= \rho_{21} \xi_1 + \rho_{22}\xi_2\ ,\
\end{align*}
where $\rho_{ij}>0$ is the unique row sum for the corresponding $k_i \times k_j$ block matrix of $\C_\mathcal{A}$. Defining $\tilde{k}_i=(n-2k_i)\geq 0 $, we have% $\tilde{k}_i \geq 0$ as $1 \leq k_i \leq n/2$, and, after tedious algebra,
\begin{equation}
\xi_1 = \frac{\tilde{k}_2(n^2-4)}{(n-2)^2(k_1\tilde{k}_2+\tilde{k}_1k_2)+\tilde{k}_1\tilde{k}_2} \geq 0\ ,\ \label{eq:delta1}
\end{equation}
with equality if and only if $k_2=n/2$. A similar expression holds for $\xi_2$ with $\tilde{k}_1$ in the numerator of \eqref{eq:delta1}, and $\xi_2=0$ if and only if $k_1=n/2$. Since the diagonal elements of $\C_\mathcal{A}$ are 1, %$\lambda_{ij}>0$ %as $\C_\mathcal{A}$ is comprised of all positive elements 
%and 
$\rho_{ii} \geq 1$ and it follows %$\delta_i < 1$ 
$\onevec-\boldsymbol{\xi}>\zerovec$. Hence designs constructed in this way satisfy \eqref{eqn:Sbound}
%\begin{equation}
%\left(\frac{n-\sqrt{n^2-4
%}}{n}\right)\betavec_\mathcal{A} \leq \lambda(\onevec-\boldsymbol{\rho})\ ,\
%\end{equation}
for some combinations of $\betavec_\mathcal{A}$ and $\lambda$, but whether this implies higher sign recovery probability depends on the covariance matrix of $\uvec$.

%Designs constructed from selecting columns from \eqref{eqn:LODbetter} are likely not optimal under $\phi^{\pm}_\lambda$. 

%which can hold for some combinations $\betavec_\mathcal{A} > \zerovec$ and $\lambda >0$ since .


%$\betavec_\mathcal{A}$ where $\phi_\lambda(\X | \betavec)$ will be greater than or equal to that under an orthogonal design.  when each $\beta_j/\lambda \leq \delta_1^*[n-\sqrt{(n-2)(n+2)}]/n$ for the $k_1^*$ columns, otherwise $\delta_2^*$ is used in place of $\delta_1^*$. In the Supplementary Materials, we provide similar results for the case of an odd $n$. \textbf{Have to do this}.


%Suppose we choose $k_1^* \geq 0$ columns from the first $n/2$ columns and $k_2^*=k^*-k_1^*$ from the last $n/2$  columns. and $\C_{\mathcal{A}}$ will have a block partitioned form where with all positive off-diagonal elements equal to $1-c_1=1-4n/(n^2-4)$ in the diagonal blocks and $c_2=(n-2)/(n+2)$ in the off-diagonal blocks. After tedious calculations, it can be shown that the elements of $\C_{\mathcal{A}}^{-1}\onevec$ corresponding to the $k_1^*$ columns have the constant value
%\begin{align}
%\delta_1^*=\frac{c_1+c_2+c_3-1}{c_2(c_1+c_3k_1^*)} \label{eqn:LODex}
%\end{align}
 %where $c_3=(1-c_1)-c_2^2k_2^*/(c_1+(1-c_1)k_2^*)$. A similar expression holds for the corresponding $k_2^*$ elements, denoted $\delta_2^*$, but flipping the roles of $k_1^*$ and $k_2^*$. 
 

%We now demonstrate the potential improvement of $\X_\mathcal{A}$ under our construction method relative to an orthogonal design when $n=16$ and $k=8$. 
As a demonstration, for $n=16$ and $k=8$, we considered an orthogonal $\X_\mathcal{A}$ and two designs based on \eqref{eqn:LODbetter} where $k_1=4$ and $8$. For all designs, $\C_\mathcal{A}^{-1}\onevec=\xi \onevec$ where $\xi=1$, $0.1575$, and $0.1607$ for the orthogonal, $k_1=4$, and $k_1=8$ designs, respectively. For the $k_1=4$ and $k_1=8$ designs, \eqref{eqn:Sbound} is equivalent to $\lambda^{-1}\betavec_\mathcal{A} \leq 53.92 \times \onevec$ and $\lambda^{-1}\betavec_\mathcal{A} \leq 53.72 \times \onevec$, respectively. 
%The region of integration for the $k_1=8$ design is always contained within that for the $k_1=4$ design.  
%For an orthogonal design, the distribution of $\uvec$ will have standard deviation 1 in all directions. The distribution for the $k_1=4$ design has a minimal standard deviation of $0.3968$ in the direction of $\onevec$ and a maximal standard deviation of $2.806$ in the unique direction of the vector $(-\onevec^T,\onevec^T)^T$. The $k_1=8$ design has a minimal standard deviation of $0.4001$ in the direction of $\onevec$ and a maximal standard deviation of $1.984$ in all directions orthogonal to $\onevec$. Hence some distributions are more tightly concentrated about $\zerovec$, potentially leading to higher probability despite a smaller area of integration. 
The three designs' $\phi_\lambda$ and $\phi_\lambda^{\pm}$ values were compared across a range of $\lambda$ and three scenarios for $\betavec_{\mathcal{A}}$: $(0.3,0.4,\dots,1)^T$, $\onevec$, and $3 \times \onevec$.  %The all-signs scenario corresponds to a criterion that addresses uncertainty about the sign vector, and will be discussed following the example. 
The results for $\phi_\lambda$ are shown in the left panels of Figure~\ref{fig:trivialcase}. For all scenarios, there is a range of large $\lambda$ values where all designs have $\phi_\lambda = 0$, a middle range where the two proposed designs outperform the orthogonal design, and a range of small $\lambda$ values where the orthogonal design is superior. The orthogonal design improves over the other two designs well before condition~\eqref{eqn:Sbound} is violated, due to the role of the covariance matrices. The improvement is negligible when $\betavec_\mathcal{A}=3\times\onevec$, implying the covariance matrix is less important as the elements of $\betavec_\mathcal{A}$ increases. The results for $\phi_\lambda^{\pm}$ are shown in the right panels of Figure~\ref{fig:trivialcase}, and clearly favor the orthogonal design. %Although it is difficult to see, there are cases for $\betavec_\mathcal{A}=3\times \onevec$, such as $\lambda=2$, in which the proposed designs have a higher $\phi_\lambda^{\pm}$. This is due to their large improvement for one specific sign vector.

%Recalling Remark~3, it is at $\betavec_{\mathcal{A}}=3\times \onevec$ where support recovery and sign recovery probability most closely match, although this is not shown in Figure~1. 
%Given the proposed designs' larger or near equal $\phi_\lambda$ values relative to the orthogonal design across the entire $\lambda$ range, the designs should outperform the orthogonal design in terms of sign recovery regardless of the tuning parameter selection strategy.



%Although it is difficult to see, the orthogonal design's $\phi_\lambda$ for $\betavec_{\mathcal{A}}=3\times \onevec$ is slightly larger than the proposed designs when $\log(\lambda) < 0$.

%Used for plots
%\Large
%\[
%\betavec_\mathcal{A}=\begin{pmatrix}0.3\\0.4\\ %\vdots\\ 1\end{pmatrix}
%\]
%\[
%\betavec_\mathcal{A}=\begin{pmatrix}3\\%3\\ \vdots\\ 3\end{pmatrix}
%\]


%\begin{itemize}
 %   \item \textbf{All Positive}: $P(S_\lambda \, | \, \F_\mathcal{A} \, , \, \Z_\mathcal{A}|\betavec_\mathcal{A}|)$ where $\zvec_A^T=(1,\dots,1)$ ;
%    \item \textbf{Balanced}: $P(S_\lambda \, | \, \F_\mathcal{A} \, , \, \Z_\mathcal{A}|\betavec_\mathcal{A}|)$ where $\zvec_\mathcal{A}^T=(1,1,1,1,-1,-1,-1,-1)$ ;
%    \item \textbf{All Signs}: 
%\end{itemize}
%For the Balanced signs when $k_1^*=4$, we evenly distributed the $\pm 1$'s in the $k_1^*$ and $k_2^*$ group, and when $k_1^*=1$ we assigned a sign of $+1$. 


%the locally optimal design problem is concerned only with identifying $\X^*_\mathcal{A}$ and $k$ can be made arbitrarily large. Using a simplified version of the algorithm described in Section~4.1, we constructed a non-orthogonal $\X^*_\mathcal{A}$ for this $\betavec_\mathcal{A}$. Using Theorem~2, we also considered a locally optimal design for a balanced $\zvec_\mathcal{A}$, meaning it has an equal number of $\pm 1$'s. We chose this sign vector because it least resembles the all-positive sign vector. These designs were compared against an orthogonal $\X_\mathcal{A}$ constructed from the non-intercept columns of a Hadamard matrix of order 16.


\begin{figure}[ht]
    \centering
\includegraphics[width=0.6\textwidth]{LocalOptDesignProbs.png}
    \caption{Probability of sign recovery for $n=16$ and $k=8$ for an orthogonal design and two designs constructed from selecting columns from \eqref{eqn:LODbetter} with $k_1=4$ and $8$. The left and right panels correspond to $\phi_\lambda$ and $\phi_\lambda^{\pm}$, respectively.}
    \label{fig:trivialcase}
\end{figure}



\subsection{Relaxed local criteria}\label{sec:relax}
To make the criteria more practical, we introduce relaxations regarding the choice of $\lambda$ and the assumptions of the underlying model.  To reduce the dependency on $\lambda$, we propose two summary measures of $\phi_\lambda$ and $\phi_\lambda^{\pm}$. The first summary takes the maximum sign recovery probability: $\phi_{\text{max}}(\X \, | \, \betavec)=\max_{\lambda > 0} \phi_\lambda(\X \, | \, \betavec)$. However, in practice one will perform some tuning parameter selection strategy to choose a $\lambda$, and so the benefits of a design that optimizes $\phi_{\text{max}}$ will only be realized if the strategy reliably picks the corresponding $\lambda$ that maximizes $\phi_\lambda$. Hence, we would like a design that maintains large probabilities for a wide range of $\lambda$. It is common to calculate the lasso solution path with respect to $\log(\lambda)$. Such a transformation stretches the region of $\lambda \in (0,1)$, whose corresponding lasso estimates would receive the least amount of penalization. Therefore, we propose the criterion 
\[
    \phi_\Lambda(\X \, | \, \betavec)=\int_0^\infty \frac{\phi_\lambda(\X \, | \, \betavec)}{\lambda}\, d\lambda=\int_{-\infty} ^\infty \phi_{\exp(\omega)}(\X \, | \, \betavec) d\omega\ .\
\]
The definitions for $\phi_{\max}^{\pm}(\X \, | \, \betavec)$ and $\phi_\Lambda^{\pm}(\X \, | \, \betavec)$ are obvious.
%where $\Lambda=(0,a]$ is an interval of $\lambda$ values and $\Omega=(-\infty,\log(a)]$. We discuss specifying $a$ in Section~4. 
%An appealing property of $\phi_\Lambda(\X \, | \, \betavec)$ is that the ranking of designs is invariant to positive scaling of $\lambda$ and transformations of the more general form $\omega=\log_b(\lambda)$ where $b>1$. The same properties obviously hold for $\phi_{\text{max}}(\X \, | \, \betavec)$.%First, we discuss strategies for removing dependency on $\lambda$ and $|\betavec_\mathcal{A}|$. 


Turning to relaxing assumptions about the model, recall $\phi_\lambda^{\pm}$ already relaxed assumptions about $\zvec_\mathcal{A}$. To relax specification of $|\betavec_\mathcal{A}|$, we assume $|\betavec_A|=\beta \onevec$ where $\beta \geq 1$ so that an active factor's effect is at least of the same magnitude as the noise. We also fix $k$ and assume all supports of this size are equally likely. Denote this set of supports by $\mathcal{A}_{k}$. We first focus on the criterion that incorporates uncertainty about $\mathcal{A}$, but treats $\zvec_{\mathcal{A}}$ as known. This resembles models where the $Var(s+)$-optimal designs performed best. Suppose $z_j$ depends only on whether $j \in \mathcal{A}$ or $\mathcal{I}$, and let $\zvec^*$  be the $p \times 1$ vector comprised of the elements $z_j$ assuming $j \in \mathcal{A}$. Following Theorem~\ref{thm:localequiv}, we assume without loss of generality that $\zvec^*=\onevec$ to identify the optimal design and then multiply the columns of the design by their actual corresponding $z_j^*$. For a given $\mathcal{A}$, let $\A$ be the $p \times p$ diagonal matrix of all zeroes except for the diagonal elements corresponding to $\mathcal{A}$, which are set to 1. Then the true sign vector is $\zvec=\A\onevec$ and the sign-dependent criterion for a given $\beta$ is
\[
    \Phi_{\lambda}(\X \, | \, k, \beta) = \binom{p}{k}^{-1} \sum_{\mathcal{A} \in \mathcal{A}_k} \phi_{\lambda}(\X \, | \, \betavec=\beta\A\onevec)\ ,\ %\label{eq:rlocA}
\]
being the average probability across $\mathcal{A}_k$ for a fixed $\lambda$. The notation $\Phi$ instead of $\phi$ indicates consideration of all $\mathcal{A}\in\mathcal{A}_k$. The sign-independent criterion, $\Phi_{\lambda}^{\pm}(\X \, | \, k, \beta)$, is similarly defined. The two summary measures originally defined on $\phi_\lambda$ and $\phi_\lambda^{\pm}$ are straightforward to define on these new criteria and share similar notation, e.g., $\Phi_{\text{max}}(\X \, | \, k, \beta)$ and $\Phi_{\Lambda}(\X \, | \, k, \beta)$. Computational details of the criteria may be found in the Supplementary Materials.

%sign-independent criterion follows much in the same way, but incorporates uncertainty about $\zvec_{\mathcal{A}}$:
%\begin{align}
%    \Phi_{\Lambda}^{\pm}(\X \, | \, k, \beta) = \binom{p}{k}^{-1}\sum_{\mathcal{A} \in \mathcal{A}_{k}}
    %\phi_{\Lambda}^{\pm}(\X \, | \, \betavec=\beta\A\onevec)\ ,\
    %\left[\frac{1}{2^{k-1}}\sum_{\tilde{\zvec} \in \mathcal{Z}_\mathcal{A}^{\pm}} \phi_{\Lambda}(\X \, | \, \betavec=\beta\tilde{\zvec})\right]\ .\ \label{eq:rloc}
%\end{align}
%where $\Phi_{\Lambda}^{\pm}(\X \, | \, \betavec=\beta\onevec)$ is the integrated version of \eqref{eqn:localA_allsigns}.



%While we considered , we prefer $\phi_{\Lambda}(\X \, | \, \betavec)$ as it provides more information about the sign recovery probability across the lasso's entire solution path.  Hence the optimal design should be more robust to whatever tuning parameter selection strategy. 



%\begin{itemize}
%    \item $\phi_{\text{max}}(\X \, | \, \betavec)=\max_{\lambda > 0} \phi_\lambda(\X \, | \, \betavec)$
%    \item 
%\end{itemize}
 

%We recommend $k=\lfloor \frac{n}{3} \rfloor$, as simulation studies with this value have demonstrated high probabilities of support recovery \citep{MarleyWoods10, weese2021strategies}. 


%Two scenarios considered in Figure~\ref{fig:trivialcase} set $\beta=1$ and $3$, and only in the latter case was support recovery well-approximated by sign recovery.

%would only be realized if we had some guarantee its optimal $\lambda$ value would be chosen in practice.


%particularly $\beta=3$, as smaller $\beta$ tended to give small probabilities of sign recovery even under an optimal design.


%This integration can be done using a Gaussian Quadrature method or a simple Riemann sum. Once integrated, we can obtain what is essentially an average probability across all relevant $\lambda$'s.

 


%The main bottleneck of this criterion is the computations needed to evaluate $\phi_{\Lambda}(\X \, | \, \betavec=\beta\A\zvec)$. Section~\ref{sec:EvalConstruct} discusses some approximation shortcuts based on sampling from $\mathcal{A}_k$.




%Based on Lemma~\ref{lem:equiv} and Theorem~\ref{thm:localequiv}, we assume without loss of generality that $\zvec_\mathcal{A}=\onevec$. 


%The criteria are easily extended, at least in principle, to multiple values of $k$, 




%when constructing optimal designs under $\phi_{\Lambda}(\X \, | \, k^*, \Z, \beta)$ and post-multiply the optimal design by the desired diagonal matrix $\Z$. 



% 



%Specification of $\betavec_\mathcal{A}$ can be broken down into its sign vector, $\zvec_A$, and absolute magnitudes $|\betavec_A|$.  

%This strategy was also taken in Section~\ref{sec:lod}.

%The All Signs case in Figure~\ref{fig:trivialcase} fixed $\mathcal{A}$ but averaged the sign recovery probability across all possible $\zvec_{\mathcal{A}}$. 

%Simply evaluating this criterion for a given $\X$, let alone optimizing across all possible designs,  will be computationally burdensome. The following result based on Theorem~1 alleviates some of this burden, halving the number of required summands.


%Redefine $\mathcal{Z}_{\mathcal{A}}$ to be the  = \{-1, 1\}^{k^*} as the set of all sign vectors of size $k^*$, and let $\phi_{l}(\mathcal{A}_{k^*},\mathcal{Z}_{k^*})$ denote some summary measure of the distribution of the local lasso sign recovery measures across $\mathcal{A}_{k^*}$ and $\mathcal{Z}_{k^*}$. Designs are compared using $\phi_{l}(\mathcal{A}_{k^*},\mathcal{Z}_{k^*})$; with the average measure. The computation of $\phi_{l}(\mathcal{A}_{k^*},\mathcal{Z}_{k^*})$ can be prohibitively expensive even for modest $k$ and $k^*$. 
%Even with this added computational efficiency, calculating $\phi_{\Lambda}(\X \, | \, k^*, \beta)$ will be significantly slower relative to heuristic criteria. 


%In Section~3, we rigorously establish connections between our proposed criteria \eqref{eq:rlocA} and \eqref{eq:rloc} with the ideal design structures under the $Var(s+)$- and $UE(s^2)$-criterion, respectively. Section~4 leverages these connections in an algorithmic construction of optimal designs that is computationally efficient.


%We will discuss computational strategies in Section \ref{sec:Criteria}.

%A surprising consequence is that if a practitioner were to construct a locally optimal design assuming incorrect signs for each $\beta_j$ with $j \in \mathcal{A}$, then the probability of sign recovery remains unchanged. Moreover, incorrectly assuming the sign of $\beta_j$ with $j \in \mathcal{I}$ has no effect on the criterion.




%A general strategy to relax these specifications is to assign a prior distribution to each aspect and investigate the distribution of these probabilities integrated over these priors. This is reasonable for evaluating and comparing a small set of designs but it is not amenable to inevitable algorithmic construction methods.  

%Dividing both sides of inequalities in \eqref{eq:SignCheckUnscale} and \eqref{eq:InactiveCheck2} by $\sigma$ leads to events with random vectors whose covariance matrices depend only on $\C$. The $\betavec_A$ in \eqref{eq:SignCheckUnscale} becomes $\betavec_A/\sigma$ and $\lambda$ in both events will be divided by $\sigma$.  Without loss of generality, we fix $\sigma^2=1$.  







%Given the above simplifications, we now must address the need to consider all possible supports of a given size $k^*$ and, for each support, whether to specify or marginalize over possible sign vectors $\zvec_A$. Note that these considerations implicitly depend upon $k^*$ and, when no knowledge on $k^*$ exists, we recommend setting $k^*=\lfloor \frac{n}{3} \rfloor$ \citep{MarleyWoods10, weese2021strategies}. We first address a strategy to summarize probabilities over all possible supports given a particular $\zvec_A$, which would be practical if the experimenter had prior knowledge of $\zvec_A$, whatever $A$ may be. The group screening \citep[see][]{draguljic_etal2014} and sequential bifurcation \citep[e.g.][]{bettonvil_kleijnen1996,kleijnen2009factor} literatures assume $\zvec_A=\onevec$, and under this assumption the $Var(s+)$-optimal designs \citep{weese2017powerful, weese2021strategies} have been shown to be superior to standard SSD's. Note that without loss of generality for two-level designs, if the experimenter can reasonably specify the effect \emph{direction} of any potential factor effect, a simple transformation yields $\zvec_A=\onevec$.

%Define $\mathcal{A}_{k^*}$ as the set of all possible support indices of size $k^*$. To define a measure of sign recovery quality across all of $\mathcal{A}_{k^*}$, we propose creating a distribution of sign recovery probabilities, integrated over $\lambda$, for a fixed sign vector across $\mathcal{A}_{k^*}$. Some summary measure of this distribution, denoted $\phi_{l}(\mathcal{A}_{k^*})$, could then be used to compare designs. Possible choices for these summary statistics are the average or median measure, the 25th percentile, or the percentage of submodels that have an average probability of 0.
% \begin{enumerate}
%     \item Average measure
%     \item Median measure
%     \item 25th percentile
%     \item \% = 0
% \end{enumerate}
%The average and median measures are intuitively useful as measures of center, while the 25th percentile aims at improving the probabilities on the submodels that are close to worst case. The \%=0 measure denotes the percent of submodels in $\mathcal{A}_{k^*}$ that have a probability of $0$ (NOTE: reminiscent of estimation capacity; references) and, using it as an optimality measure targets minimally performing submodels. 
%We recommend using the average measure as it has shown the most ability to differentiate between designs. Note that we refer to $\phi_{l}(\mathcal{A}_{k^*})$ as a lasso support recovery measure instead of a probability due to the fact that integrating over a range of $\lambda$ values can produce values greater than $1$.

%When both $A$ and $\zvec_A$ are unknown, we follow a similar strategy, by taking the measure of the distribution of sign recovery probabilities, integrated over $\lambda$, for all possible $\zvec_A$ and $\mathcal{A}_{k^*}$ %considered. 


%We note that a design having a compound symmetric $\C$ matrix would be ideal because, given a $k^*$, all corresponding submatrices would be equal. In order to provide theoretical insights, a design theory conditioning on such potential designs is considered in the following section.



%\textbf{For (2), we may prefer to integrate with respect to $\log(\lambda)$ but I'm not sure how difficult that will be.}  Computationally, (1) is easier to calculate than (2) and is more interpretable.  Given the behavior of the joint probability, as a function of $\lambda$, we could use the optimal $\lambda$ value to improve computation of the integral, or we could use a pre-packaged function like integrate in R.

%If we were to relax fixation on a single $\lambda$, this would suggest all such designs are equivalent in their sign recovery properties.  On the other hand, integrating across lambda would favor our locally optimal design.    Such a property is appealing since tuning parameter selection is a necessary step in the analysis.

 

%it is reasonable to evaluate the quality of a design in terms of its $\lambda$ value that maximize the probability.  But in the context of this example, there will always exist a design that achieves support recovery (not necessarily sign recovery) so long as $\F_I^*=0$ and $\F_A^{*T}\F_A^*$ is invertible.  To avoid these trivial cases under this relaxation on $\lambda$, we need to consider relaxations on other assumptions about the underlying model.

%Another relaxation on such a theory could assign a prior distribution on $\betavec_A$ but with fixed $\zvec_A$, meaning $\betavec_A$ lies in the fixed quadrant of $\mathbb{R}^{|A|}$ defined by $\zvec_A$.  




%In light of Lemma~\ref{lem:equiv}, an equivalent framework would extend the prior distribution of $\betavec_A$ to those with sign vector in the symmetric quadrant defined by $- \zvec_A$.  This guarantees $\text{Diag}(\zvec_A)\betavec_A=|\betavec_A|$.  The other terms in \eqref{eq:SignCheck3} are unaffected by $\pm 1$ multiplication and the same is true for \eqref{eq:InactiveCheck2}.  

%It is also reasonable to standardize $|\betavec_A|$ to have a certain length, since a design that maximizes the probability for a $|\betavec_A|$ would also maximize the probability for $\alpha |\betavec_A|$ for some $\alpha >0$.  This explains why rescaling $\betavec_A$ in our simulations doesn't affect the probability curves.

%For screening experiments, $A$ is unknown so we desire a design that maximizes the design-specific sign recovery measures (based on how we summarize across $\lambda$) across many potential supports.  Therefore, we must calculate many such probabilities and to summarize their resulting distribution in some way.  In answering this question, 
%We must acknowledge that when $n < k$, there may not exist a design that yields a nonzero sign recovery probability for all $A$ considered.  

%Indeed, the $Var(s)$-criterion seeks a design having such a property in which the off-diagonals are made as small as possible.  Heuristic criteria like $E(s^2)$ and $UE(s^2)$ do not necessarily enforce this property and so may be more inclined to have supports with poor sign recovery probabilities.  

%As will be discussed in Section~2.3, we should not be quick to disregard designs that have these zero probabilities.  


Optimizing any of the $\Phi$-type criteria is analytically and algorithmically challenging. Indeed, simply evaluating such criteria for a given $\X$ can be cumbersome. We next discuss properties of these criteria and, motivated by approximate design theory \citep{pukelsheim2006optimal}, we propose a more tractable framework centered on finding optimal $\C$ matrices, which we refer to as lasso information matrices.  Section~\ref{sec:EvalConstruct} will leverage the optimal forms of $\C$ to efficiently identify optimal, or at least nearly-optimal, exact designs.

%Finally, the All Signs scenario clearly demonstrates the superiority of the orthogonal design over the two local optimal designs for most $\lambda$ values.

%Given Theorem~\ref{thm:localequiv}, our proposed designs' superior performance may suffer when $\zvec_\mathcal{A} \neq \onevec$.




%Change y-axis label
%Don't stretch vertically. Maybe stack horizontally?
%Larger panel labels.
%thicker lines with different patterns
%Have to be log(lambda)?

%The two panels of Figure~\ref{fig:trivialcase} for the All Positive and Balanced scenarios the corresponding, nonorthogonal local optimal designs can improve over the orthogonal design for certain values of $\lambda$. 
%However, if we were to integrate the probability of sign recovery over $\lambda$, we would find that the locally optimal designs are superior to the orthogonal designs. 




%was constructed under the assumption that $\zvec_{A}=\onevec$, while the other assumes that $\onevec^{T}\zvec_{A}=0$. These two designs were constructed by optimizing $P(\mathcal{S}(|\boldsymbol\beta_{A}|,\zvec_{A}))$. The three panels represent the evaluation of the designs under an ``All Positive'' condition which $\zvec_{A}=\onevec$, a ``Balanced'' condition where $\zvec_A = (\onevec_4^T, -\onevec_4^T)^T$, and a condition which averages the probability across all possible $\zvec_{A}$.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=1\textwidth]{Section_2_Graph.png}
%     \caption{Probability of sign recovery for the fixed case where $\sigma^2$=1, $n$=16 and all $k=8$ factors are active with $\beta_1=\mathbf{3}$.}
%     \label{fig:trivalcase}
% \end{figure}

%The locally optimal design's improvement over the orthogonal design derives from the alignment of the submatrix with $\zvec_A$ that allows greater probability. 
%\begin{enumerate}
    %\item Byran: construct the locally optimal design for mixed signs.  Check that it matches the Lemma.
    %\item Expected story: the orthogonal design won't be optimal for a given sign vector but does seem to be competitive across all sign vectors.
%\end{enumerate}

%In light of corollary~1, the orthogonal design will never be optimal for the chosen value of $\lambda$ and $|\betavec_A|$. However, note that $P(\mathcal{S}(\betavec_A,\zvec_A)) \to 1$ as $\lambda \to 0$.  In fact, this is true of any design where $\C_{AA}$ is invertible and $\F_I^*=0$.  On the other hand, integrating this probability of $\lambda$ indicates superiority of the locally optimal design.  This will need to be considered when we discuss relaxing specification of $\lambda$.  Before we discuss that, we consider relaxing the assumption on $\zvec_A$ but still have $A$ fixed.  A reasonable approach is to specify $|\betavec_A|$ and assume all $\zvec_A$ are equally likely.  For the above example, we calculated and averaged the probabilities of sign recovery across all $2^7$ possible sign vectors.  Lemma~2 tells us that we only need to calculate probabilities under $2^6$ scenarios.  Figure~() shows these new probabilities for the previously constructed designs and we see that the orthogonal design dominates the other designs that were tuned to specific $\zvec_A$.  As we decrease knowledge of the underlying system, it appears that the optimal design will approach an orthogonal design. 



%When the assumption on $\zvec_A$ is relaxed but $A$ remains fixed, a reasonable approach is to specify $|\betavec_A|$ and assume all $\zvec_A$ are equally likely (again, see Section \ref{sec:relod}). For the rightmost panel of Figure~\ref{fig:trivialcase}, we calculated and averaged the probabilities of sign recovery across all $2^8$ possible sign vectors. Note that Theorem~\ref{thm:symmetry} tells us that we only need to calculate probabilities under $2^7$ scenarios. We see that the orthogonal design dominates the other designs that were tuned to specific $\zvec_A$. 




%However, when $\zvec_A$ is not known and all signs are equally likely, the rightmost panel of figure ~ \ref{fig:trivalcase} shows that the orthogonal designs are optimal.

  %Before we discuss that, we consider relaxing the assumption on $\zvec_A$ but still have $A$ fixed.  A reasonable approach is to specify $|\betavec_A|$ and assume all $\zvec_A$ are equally likely.  For the above example, we calculated and averaged the probabilities of sign recovery across all $2^7$ possible sign vectors.  Lemma~2 tells us that we only need to calculate probabilities under $2^6$ scenarios.  Figure~() shows these new probabilities for the previously constructed designs and we see that the orthogonal design dominates the other designs that were tuned to specific $\zvec_A$.  As we decrease knowledge of the underlying system, it appears that the optimal design will approach an orthogonal design.

%challenging but necessary problem and would be aided by a solution path that has the true support for a wider range of lambda values.

%\textbf{NEED TO DO!}



%\textbf{Lemma}: Orthogonal design is optimal for a fixed support marginal to all possible sign vectors.

%%%%%MW: I commented this since we discussed Figure 2 above.

%To motivate our work, consider the following experimental scenario in which we assume to know nearly all about the true state of the system. We take run size $n=16$ and $k=24$ factors, with $\sigma^{2}=1$ and $\beta_{0}=0$ from model \eqref{eq:model}. Further, let's assume that only $k_{A}=8$ of the factors have non-zero effects, and for those active factors, $\boldsymbol\beta_{A}=\textbf{3}$. Suppose that we have perfect knowledge of the 8 active factors, in which case we can construct a $16 \times 8$ design.

%%%%MW: I commented this since we discussed Figure 2 above.

%Using the measure developed in Section \ref{sec:framework}, Figure \ref{fig:trivialcase} compares three designs under three different sign conditions. The measure, denoted $P(S)$ here, is the probability of correctly identifying the active factors, as a function of $\lambda$, the lasso regularization parameter. The first two designs are locally optimal, given the knowledge of the model along with particular sign vectors (all positive; balanced vector with four positive and four negative). The last design is one composed of orthogonal columns. These three designs are evaluated under three conditions: when signs are truly all positive; when signs are balanced in the same way the Local Optimal Balanced design was constructed; and when the probabilities are averaged over all signs.


%%%MW: I moved some of this up above. 
 
%Figure \ref{fig:trivialcase} shows that orthogonal designs can be improved upon, if we have knowledge of the true sign vector. That is, when the true sign vector is the same as the one assumed in the locally optimal design, those designs are better than the orthogonal design, in the sense that their probabilities of support recovery dominate. However, in the absence of sign vector information, represented by the ``All Signs'' condition in which the probabilities are averaged over all possible sign vectors, the orthogonal design is preferred. This figure provides some insights that we will solidify in this paper. First, sign vectors play an important role in the quality of designs. In more realistic cases, it may be possible to improve upon designs that are orthogonal, or try to approximate orthogonality, in the presence of particular sign information. Second, orthogonal designs appear to be relatively robust to misspecification of signs, particularly when compared to locally optimal designs.





\section{Optimal Lasso Information Matrices} \label{sec:optimal_cs}



The criteria from Section~\ref{sec:relax} assume $|\betavec_\mathcal{A}|=\beta \onevec$ and evaluate an $\X$ across all possible supports, and so are invariant to column permutations of $\X$. Additionally, $\Phi^\pm_{\lambda}$ and its summaries across $\lambda$ are invariant to sign-flipping the columns of $\X$. Traditional eigenvalue-based criteria, being functions of $\M=\X^T(\I-\Pmat_1)\X$, also enjoy these two invariance properties,  expressed as simultaneous row/column permutations and sign transformations of $\M$. Indeed, these properties have been exploited to find optimal forms of $\M$ when the domain of the functions is expanded to all symmetric, positive definite matrices, denoted $\mathcal{M}$. For an $\M \in \mathcal{M}$, define its permutation-averaged form to be $\overline{\M}=(p!)^{-1} \sum_{\Pimat \in \mathcal{P}} \Pimat \, \M \, \Pimat^T$ where $\mathcal{P}$ is the set of all $p \times p$ permutation matrices.  Then $\overline{\M} \in \mathcal{M}$ is a completely symmetric matrix. Further averaging of $\overline{\M}$ across all $2^p$ sign transformations leads to a matrix proportional to the identity matrix, i.e., an $\M$ for an orthogonal design. Assuming the criterion is concave, the criterion value for $\overline{\M}$ is greater than or equal to that for $\M$. Hence the search for the optimum $\M$ may be restricted to the class of completely symmetric matrices in $\mathcal{M}$.  %The practical limitation of this approach, however, is that there may not exist an $\X$ having such an optimal form.

%and a matrix averaging technique implies the optimal $\M \in \mathcal{M}$ will be completely symmetric. The matrix $\bar{\M}$ is completely symmetric, meaning it can be written as $\bar{\M}=a\I+b\J$. Both \eqref{eq:rlocA}, with $\zvec=\onevec$, and \eqref{eq:rloc} share the permutation-invariance property with respect to $\C$, as they evaluate probabilities across all possible submodels of a given size.  

Similar to defining eigenvalue-based criteria on $\mathcal{M}$, the criteria in Section~\ref{sec:loc} can be defined with respect to the design's $\C$ and $\V$ matrix. We can then cast the optimal design problem in terms of identifying an optimal pairing, $(\C^*,\V^*)$, and then try to identify a design having such matrices. This is essentially the approach taken by heuristic orthogonality criteria, presuming $\C=\I$ is the optimal form. We will argue $\C=\I$ is the optimal form for summaries of $\Phi_{\lambda}^{\pm}(\X \, | \, k, \beta)$, and that some $\C$ with all positive off-diagonals is the optimal form for summaries of the sign-dependent $\Phi_{\lambda}(\X \, | \, k, \beta)$. %Similar arguments are made for their summaries across $\lambda$.

The matrix $\V$ has diagonal elements between $0$ and $1$, and only contributes to event $S_\lambda$. Clearly $\V=\I$ maximizes $P(S_\lambda)$, so we fix $\V=\I$ and optimize the criteria with respect to $\C$ across all symmetric, nonnegative definite matrices with 1's on the diagonal. If the criteria were concave with respect to all such matrices we could restrict our optimality search to completely symmetric matrices. We have thus far been unable to establish such a property. Our current investigations have led us to conjecture the criteria are log concave for certain combinations of $\lambda$ and $\beta$. We comment on this conjecture in a later example. Nonetheless, we will focus on identifying an optimal information matrix among all completely symmetric matrices $\mathcal{C}=\{\C = (1-c)\I +c\J \, | \, -(k-1)^{-1}<c<1\}$.  This makes the optimization problem more tractable, as it involves a single unknown value, $c$, and the criteria, now being invariant to $\mathcal{A}$, are equal to $\phi_\lambda$ and $\phi^{\pm}_\lambda$, respectively. 

The probabilities for events~\eqref{eq:SignCheckUnscale} and \eqref{eq:InactiveCheck2} for $\C \in \mathcal{C}$ are given in the following lemma:
\begin{lemma}\label{lem:CS_events} Let $\V=\I$ and $\C \in \mathcal{C}$. Then for $\mathcal{A}$ where $|\mathcal{A}|=k$ and $\betavec_\mathcal{A}$ with sign vector $\zvec_\mathcal{A}$, $P(S_\lambda \, | \, c, \betavec_\mathcal{A})= P( \boldsymbol{u}< \sqrt{n} |\betavec_\mathcal{A}|)$ and $P(I_\lambda \, | \, c, \zvec_\mathcal{A})= P(|\boldsymbol{v}| \leq \lambda\sqrt{n} \onevec)$
where 
\begin{align*}
\boldsymbol{u}&\sim 
    N\left(\frac{\lambda \sqrt{n}}{1-c}\left[\onevec - z_\mathcal{A}\gamma\zvec_\mathcal{A} \right],\ \frac{1}{1-c} \left[\I_{k} - \gamma\zvec_\mathcal{A}\zvec_\mathcal{A}^T\right] \right)    \\
    \boldsymbol{v}&\sim N\left(\lambda\sqrt{n}z_\mathcal{A}\gamma\onevec,(1-c)\left[\I + \gamma\J\right] \right)
\end{align*}
 with $z_\mathcal{A}=\onevec^T\zvec_\mathcal{A}$ and $\gamma = c/(1+c(k-1))$. Moreover, if $\betavec_\mathcal{A}$ does not depend on $\mathcal{A}$, the probabilities of the two events are constant across all such $\mathcal{A}$.
\end{lemma} 
\noindent




%\section{Optimal Completely Symmetric Information Matrices} \label{sec:ideal}


%, the convex set of $k \times k$ positive semidefinite/definite matrices having all diagonal entries equal to 1.
%Unlike the approximate design framework, our criteria will still depend on $n$. The approach by itself is unlikely to identify an optimal SSD because for the $n$ common to SSDs, an optimal $\C$ will likely not correspond to an exact design with settings of $\pm 1$. The goal of this section then is to show that the optimal $\C$ structures under the sign-specific $Var(s+)$-criterion and the sign-agnostic, orthogonality-based criteria can be different with respect to our criteria. The results provide a more rigorous justification of these heuristic criteria for a lasso-based analysis.
%which up tothis  based on least-squares intuition, under  and hence establish the value of their optimal designs on this analysis strategy.

%The goal is to 

%itself even if there is actually no $\X$ that can produce it. The relationship between using the values of $\X$ as decision variable vs. using the values of $\C$ as decision variables 
%optimizing a function of $\C$ that is produced by $\X$ vs. a theoretical $\C$ of a particular structure 
%resembles the relationship between approximate and exact design theory. Such an analysis may not yield usable designs, but can illuminate desirable structures. Thus, in this section we shift the lasso optimal screening design problem from optimizing $\X$ directly to optimizing $\C$. 


%In practice, there is a correspondence between a particular design $\X$ and $\C$ itself, but this relationship is difficult to define due to centering and scaling. Clearly, there exist some possible $\C$ matrices such that there are no possible $\X$ that produce it. Though any reasonable design objective function in this context will involve $\C$, for a given $n$ and $k$, there are complicated constraints on $\C$ which makes design optimization difficult.

%The approximate design literature for linear models focus on concave criteria defined on a design's information matrix, . When the criteria are also invariant to simultaneous row/column permutations of $\M$, matrix averaging leads to to establish upper bounds for the criteria. 


%The two criteria in Section~2.2 consider all possible supports of size $k^*$.



%In other words, the likelihood of lasso sign recovery is now equivalent for all possible $A$ of the same size $k^*$. Because of this model robustness, we focus this section on compound symmetric $\C$.  Additionally, in this setting it is assumed that the scaling matrix $\V=\I$ which occurs when each $\xvec_j$ has the same number of $+1$ and $-1$. 

%One advantage in considering compound symmetric $\C$ matrices for optimizing sign recovery probability is that the probability does not depend on a particular active sets. That is, two active sets, $\A$ and $\tilde{\A}$, that are the same size, have the same probability of sign recovery.  
\noindent
The criterion $\Phi_{\lambda}(\X \, | \, k, \beta)$ meets the latter condition of Lemma~\ref{lem:CS_events} and assumes $\zvec_{\mathcal{A}}=\onevec$, leading to the random vectors
\begin{align}
    \boldsymbol{u}\sim 
    N\left(\frac{\lambda \sqrt{n}}{1+c(k-1)}\onevec,\frac{1}{1-c} \left[\I - \gamma\J\right] \right), \qquad 
    \boldsymbol{v}\sim N\left(\lambda\sqrt{n}k\gamma\onevec,(1-c)\left[\I + \gamma\J\right] \right)\nonumber\ .\
\end{align}
The corresponding criterion defined with respect to $\mathcal{C}$ and for a fixed $\lambda$ is denoted
\begin{align}
\psi_\lambda(c \, | \, k, \beta)=P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\onevec) \times P(I_\lambda \, | \, c, \zvec_\mathcal{A}=\onevec) \label{eqn:CS_sign_crit}\ .\
\end{align}
The analog to $\Phi_{\lambda}^\pm(\X \, | \, k, \beta)$ is denoted
\begin{align}
\psi_\lambda^{\pm}(c \, | \, k, \beta)=\frac{1}{2^{k-1}}\sum_{\tilde{\zvec} \in \mathcal{Z}_\mathcal{A}^{\pm}} P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec}) \times P(I_\lambda \, | \, c, \zvec_\mathcal{A}=\tilde{\zvec}) \label{eqn:CS_crit}\ .\
\end{align}
%While the exact-design analog of this criterion requires cumbersome computations across all $\mathcal{A}$, $\psi_\lambda(c \, | \, k, \beta)$ can be computed quickly. 
The summarized versions of \eqref{eqn:CS_sign_crit} and \eqref{eqn:CS_crit} across $\lambda$ will be denoted similarly to their exact design counterparts from Section~\ref{sec:loc}, replacing $\Phi$ with $\psi$.

%are denoted by $\psi_\Lambda$ and $\psi_\Lambda^{\pm}$. %\textbf{Discuss how this is like an approximate design theory.}

The criteria $\psi_\lambda$ and $\psi_\lambda^{\pm}$ involve a single variable, but are still challenging to optimize analytically.  Numerical optimization of these criteria,  however, is straightforward and computationally efficient. We demonstrate numerical optimization of these new criteria for the situation of $p=10$, $k=4$, $\beta=2$, and $n=10$.  %The top row of Figure~\ref{fig:CS_combined} shows contour plots of \eqref{eq:CSSevent} and \eqref{eq:CSIevent} across $\log(\lambda) \in [-2,5]$ and all possible $c$ values. The value $c=0$ optimizes $P(I_\lambda)$ for all $\lambda$, while $P(S_\lambda)$ is optimized by $c$ close to 1 for all $\lambda$. The bottom row of 
Figure~\ref{fig:CS_combined} shows the contour plots of $\psi_\lambda$ and $\psi_\lambda^{\pm}$. The optimal $c$ values for the corresponding $\psi_\Lambda$ and $\psi_\Lambda^\pm$ are $0.14$ and $0$, respectively. The resulting optimal $\C$ matrices then match the ideal forms for the $Var(s+)$ and $UE(s^2)$-criterion.  While these ideal $\C$ are not possible for SSDs, this example provides further justification for these two heuristic SSD criteria for sign recovery under the lasso. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\textwidth]{Section_3_combined_figure_phi_updated.png}
    \caption{Plots of $\psi_\lambda$ and $\psi_\lambda^{\pm}$ across values of $c$ and $\log(\lambda)$ for $n=10$, $k=4$, and $\beta=2$.}
    \label{fig:CS_combined}
\end{figure}
%\begin{figure}[H]
    %\centering
    %\includegraphics[width=0.9\textwidth]{Ideal_Joint_both_sign_settings_updated.png}
    %\caption{Probability of sign recovery when signs are known (left) and when signs are unknown (right) for compound symmetric $\C$ matrices, across values of the off-diagonal $c$ and $\log(\lambda)$ where $\sigma^2=1$, $n=10$ and $k^*=4$ factors are active with $\beta_A=\mathbf{2}$. The red horizontal line denotes the $c$ value  where the probability of sign recovery integrated over $\lambda$ is maximized over a grid of $c$. For the known sign case, the maximal $c$ is $0.14$, for unknown signs the maximal $c$ is $0$.}
    %\label{fig:Ideal_joint_both}
%\end{figure}

%For any $\zvec_\mathcal{A}$ and $c=0$, the random vectors $\boldsymbol{u}$ and $\boldsymbol{v}$ in Lemma~\ref{lem:CS_events} have means $\zerovec$ and $\lambda\sqrt{n}\onevec$, respectively, and their covariance matrices are both $\I$. Hence the sign recovery probability under $c=0$ is invariant to the choice of $\zvec_\mathcal{A}$ and $\mathcal{A}$. 
If $\psi^\pm_\lambda$ were concave in $\mathcal{C}$ across all $\lambda$, the matrix averaging technique would prove the optimality of $c=0$, as $\C=\I$ is the unique matrix after averaging across all permutations and sign transformations. Figure~\ref{fig:CS_combined} shows that there are some $\lambda$ for which $c=0$ does not maximize $\psi^\pm_\lambda$, and so the function cannot be concave or log-concave. For example $\log(\lambda) > 0.640$ shows a nearly 0 value for $\psi^\pm_\lambda$ when $c=0$ but a nonzero probability for $c \in (0.2, 0.6)$. This phenomenon is due to the improved sign recovery for $\zvec_\mathcal{A}=\onevec$ for these $c$ values in this range of $\log(\lambda)$. However, the summary criteria may still be concave or log concave even if $\psi^\pm_\lambda$ is not for all $\lambda$.

We now study the behavior of $\psi_\lambda$ and $\psi^\pm_\lambda$ in a neighborhood about $c=0$. We were initially optimistic that the inequalities of \cite{sidak1968} would aid in our study, but their results assume a fixed mean. Direct application of the multivariate Leibniz integral rule gives the following lemma.
%\newpage
\begin{lemma}\label{lem:CS_event_derivs}
For all $\mathcal{A}$ where $|\mathcal{A}|=k$ and $\beta > 0$, $\frac{d}{dc} P(I_\lambda | c, \zvec_\mathcal{A}=\onevec)\big|_{c=0} = 0$ for all $\lambda$ and $\frac{d}{dc} P(S_\lambda | c, \betavec_\mathcal{A}=\beta\onevec)\big|_{c=0} \geq 0$ for $\lambda$ satisfying
\begin{align}
%\log(\lambda) \geq \log\{g(\sqrt{n}[\beta-\lambda])\}-\log\{G(\sqrt{n}[\beta-\lambda])\}-\log(2\sqrt{n}) \label{eqn:Lem3_ineq1}\ ,\ 
2\lambda_n &\geq \frac{g(\tau_n)}{G(\tau_n)}\label{eqn:Lem3_ineq1}\ ,\ 
\end{align}
where $G(\cdot)$ and $g(\cdot)$ represent the $N(0,1)$ cumulative distribution function and probability density function, respectively, with $\lambda_n=\lambda\sqrt{n}$ and $\tau_n = \sqrt{n}(\beta -\lambda)$. 
\end{lemma}
\noindent
%The derivative of $\psi_\lambda$ involves the product rule of the two probabilities in Lemma~\ref{lem:CS_event_derivs} and so 
A direct consequence of Lemma~\ref{lem:CS_event_derivs} is that $\psi_\lambda(0| k, \beta)$ can be improved upon by some $c > 0$. We state this important result as a theorem.
\begin{theorem}\label{thm:CS_local_opt_sign}
For all $\mathcal{A}$ where $|\mathcal{A}|=k$, $\frac{d}{dc} \psi_\lambda(c |k, \beta)\big|_{c=0}>0$ for $\lambda$ satisfying~\eqref{eqn:Lem3_ineq1}. Hence there exists some $c_\lambda>0$, where $\psi_\lambda(c_\lambda | k, \beta) > \psi_\lambda(0 | k, \beta)$.
\end{theorem}
%Note: what does this say about $\psi_\Lambda$? I guess this depends on the length of the inequality and what the derivative is in this range? 
\noindent
Applying this result to the situation shown in Figure~\ref{fig:CS_combined} with known sign, inequality \eqref{eqn:Lem3_ineq1} holds for $\log(\lambda)\geq-22.763$, which covers the entire region of $\lambda$ values for which $\psi_\lambda(c)>0$. It follows then that some $c > 0$ will maximize the summary measures $\psi_\Lambda(c)$ or $\psi_{\max}(c)$. Theorem~\ref{thm:CS_local_opt_sign} thus provides a mathematically rigorous justification that the ideal designs under the $Var(s+)$-criterion maximize the probability of sign recovery for known sign. 

The following theorem establishes a similar justification for heuristic orthogonality measures under unknown signs.
\begin{theorem}\label{thm:CS_locl_opt_allsign}
For all $\mathcal{A}$ where $|\mathcal{A}|=k$ and $\beta>0$, $\frac{d}{dc} \psi_\lambda^{\pm}(c |k, \beta)\big|_{c=0}=0$ for all $\lambda$. Moreover, $c=0$ is a local maximum for $\psi_\lambda^\pm(c| k, \beta)$ when
%\begin{align}
    %\sqrt{n}\left(\frac{\beta^2-\lambda^2}{2}+\beta\lambda\right)g(\sqrt{n}[\beta-\lambda]) \leq [(\beta+\lambda)+\lambda^2n(\beta-\lambda)]G(\sqrt{n}[\beta-\lambda])\ .\ \label{eqn:Thm3_ineq}
%\end{align}
\begin{equation}
%\begin{split}
%   & 2\lambda q g(\sqrt{n}\lambda)G(\tau)^2\left[k(1-n\lambda^2)G(\Delta) + (q-1)\sqrt{n}\lambda g(\sqrt{n}\lambda) \right]\leq \\
%    &-k(k-1)g(\tau)G(\Delta)^2 \left\{ -\left[ (\beta +\lambda) +n\lambda^2(\beta-\lambda)\right]G(\tau) + \sqrt{n}\left[ \frac{\beta^2-\lambda^2}{2} + \beta\lambda\right]g(\tau)\right\} \ ,\ % \label{eqn:Thm3_ineq}
%\end{split}
\begin{split}
\frac{q}{\binom{k}{2}} \ \frac{\lambda_n g(\lambda_n)}{G(\Delta_n)}&\left( k(1-\lambda_n^2)+(q-1)\frac{\lambda_ng(\lambda_n)}{G(\Delta_n)}\right) \leq\\
&\frac{g(\tau_n)}{G(\tau_n)}\left(\beta_n+\lambda_n+\lambda_n^2\tau_n-\left[\frac{\beta_n^2-\lambda_n^2}{2}+\beta_n\lambda_n\right]\frac{g(\tau_n)}{G(\tau_n)}\right)\label{eqn:Thm3_ineq}
\end{split}
\end{equation}
where $\beta_n=\beta\sqrt{n}$ and $G(\Delta_n)=G(\lambda_n)-G(-\lambda_n)$. 
\end{theorem}
\noindent
Applying this result to the situation shown in Figure~\ref{fig:CS_combined} with unknown sign, inequality \eqref{eqn:Thm3_ineq} holds for $\log(\lambda)\in [-0.988,0.640]$. For $\log(\lambda)$ outside this region, there are clearly some $c>0$ for which $\psi_\lambda^{\pm}(c)>0$, although the probabilities are relatively small. These small probabilities do not influence $\psi_{\max}^\pm$ and have minimal influence on $\psi_\Lambda^\pm$, making $c=0$ a global maximum for both criteria. We conjecture that generally $c=0$ and some $c>0$ are global maxima for the unknown and known sign criteria, respectively.


%The function $\psi_\lambda^\pm(c| k, \beta)$  $c=0$, we would need to argue that the function is at least nonincreasing. Figure~() shows this property does not necessarily hold for all $\lambda$.

%However, equation \eqref{eq:CSSevent} may not be maximized when $c=0$ since the mean of $\boldsymbol{u}$ is non-zero and the area of integration is not necessarily a hypercube. Indeed, we have seen that orthogonality is most beneficial when we do not assume sign information.

%While this optimal $c$ fluctuates based on $\betavec_\mathcal{A}$, $k^*$, and $\lambda$, it appears always to remain above 0. This suggests that when $\zvec_\mathcal{A}=\onevec$, a small, positive off-diagonal is favorable over orthogonality and justifies the use of $Var(s+)$-optimal designs when effect signs are known.



%is beyond of the scope of this paper, but Figure \ref{fig:Ideal_I_S} demonstrates, for a specific setting, how $P(S_\lambda \, | \, c, \beta)$ and $P(I_\lambda \, | \, c, \zvec)$ behave when $\zvec_\mathcal{A}=\onevec$. While $P(I_\lambda \, | \, c, \zvec)$ favors $c=0$ in Figure \ref{fig:Ideal_I_S}, $P(S_\lambda \, | \, c, \beta)$ shows higher values across a larger range of $log(\lambda)$ when $c>0$. While these events are interesting individually, their product $\psi_\lambda(c \, | \, k^*, \onevec, \betavec)$ is the true objective. The left panel of Figure \ref{fig:Ideal_joint_both} shows the sign recovery probability, $\psi_\lambda(c \, | \, k^*, \onevec, \betavec)$ across $c$ and $log(\lambda)$. 


%We consider this outside the scope of this paper, which is primarily concerned with exact design construction. We now investigate how the probabilities in Lemma~\ref{lem:CS_events} respond to ideal $c$ values corresponding to the $UE(s^2)$ and $Var(s+)$-criteria, and demonstrate these results with an example.





%Nonetheless, when no sign information is known, orthogonal designs ($c=0$) are favorable since their performances do not depend on a specific $\tilde{\zvec}_A$

%For the $\mathcal{S}(\betavec_A,\tilde{\zvec}_A)$ event, $z=0$ removes the dependence of the mean of $\boldsymbol{u}$ on $\tilde{\zvec}_A$ itself, but the covariance matrix depends on $\tilde{\zvec}_A$. When $c=0$ for any $\tilde{\zvec}_A$, the mean of $\boldsymbol{u}$ becomes $\lambda\sqrt{n}\onevec$ and $Cov(\boldsymbol{u})=\sigma^2\I$. Equation \ref{eq:CSSevent} may not be maximized when $c=0$ since the mean of $\boldsymbol{u}$ is non-zero and the area of integration is not necessarily a hypercube. Since $P(\hat{\zvec}_A =\tilde{\zvec}_A)= P(\mathcal{I}(\tilde{\zvec}_A))P\left(\mathcal{S}(\betavec_A,\tilde{\zvec}_A)\right)$, an orthogonal design ($c=0$) may not be optimal for all $\betavec_A$. 

%\begin{align}
    %\C_{II}-\C_{IA}\C_{AA}^{-1}\C_{AI}
    %&=(1-c)\I_q + \frac{c(1-c)}{1+c(k^*-1)}\J_q\\ \label{eq:CSI_Cov}
%\end{align}

%For the $\mathcal{S}(\betavec_A,\tilde{\zvec}_A)$ event, the covariance of the random vector in the left-hand side is:
%\begin{align}
    %\widetilde{\Z}_A\C_{AA}^{-1}\widetilde{\Z}_A
    %&=\frac{1}{1-c}\I_{k^*} - \frac{c}{(1-c)}\left(\frac{1}{1+c(k^*-1)}\right)\tilde{\zvec}_A\tilde{\zvec}_A^T\ .\ \label{eq:CSS_Cov}
%\end{align}
%Again, this covariance matrix depends on $\A$ only through $k^*$, and, when $\tilde{\zvec}_A=\onevec_k$, the covariance matrix is compound symmetric.

%Since designs constructed via heuristic measures of SSD quality, such as the $UE(s^2)$ and $Var(s+)$ criteria, have been shown to perform well under the lasso in reasonable settings, we now comment on the ideal information matrix structures imposed by these heuristics. Broadly, SSD heuristics are optimized for an information matrix $\C$ that is compound symmetric. The $E(s^2)$ and $UE(s^2)$ heuristics would be optimized for an orthogonal design, meaning $c=0$, while $Var(s+)$ moves to an ideal structure where $c$ is small but positive. 
 
%Since $Var(s+)$ has shown increased screening performance for all positive signs, we can fix $\tilde{\zvec}_A=\onevec$ and again evaluate equations \eqref{eq:CSIevent} and \eqref{eq:CSSevent}. In this case, the random vectors are distributed as:
 

 %Note that the ideal structure for $UE(s^2)$ and $E(s^2)$, $\C=\I$, would satisfy the SIC by making the left side of the SIC condition $0$. Additionally, \cite{zhao2006model} shows that for the ideal structure for $Var(s+)$, a compound symmetric $\C$ with $c$ sufficiently small but positive, the SIC is guaranteed to hold.
 
 



%While orthogonal designs may not be optimal in all cases, it is important to understand the utility of orthogonality in the context of lasso sign recovery. When there is no known prior sign information---meaning all sign vectors are equally likely---orthogonal designs appear to be most robust. The right panel of Figure \ref{fig:Ideal_joint_both} shows the sign recovery probabilities averaged over all possible sign vectors. The optimal $c$ value is $0$, suggesting that orthogonality is optimal for a general sign vector.








%The ideal design measure would then need to calculate sign recovery probabilities (given the above relaxations) across all possible $A$, which is computationally prohibitive.



%Let's make our approach concrete based on what I wrote above and then say...








%Finally, and perhaps most importantly, the above calculations assume we know $A$.  Under such a framework, (12) will be minimized by a design where $\F_I=0$.  If $\F_I$ and $\F_A$ can be manipulated independently, we would need only to focus on satisfying equation (11) for $\F_A$.

%There are obvious practical limitations with constructing designs by optimizing the above probabilities for sign/support recovery.  First, it requires specification of a given $\lambda$ even though in practice the solution path of the lasso estimator is tracked, with the optimal $\lambda$ selected using a method like cross-validation or BIC.  Second, it requires a known $\betavec_A$ even though the whole point of the experiment is to estimate this quantity.  Our approach, however, is not unlike performing preliminary power calculations, and this local optimality approach is common in the design literature (cites).  Before we discuss relaxations of this extreme local optimal design framework, we investigate properties of the optimal designs under this framework and compare them to properties under traditional criteria.

%Suppose we construct a design that maximizes the sign recovery probability assuming $A=\tilde{A}$, $\betavec_A=\tilde{\betavec}_A$, and $\zvec_A=\text{sign}(\tilde{\betavec}_A)$.  The probability expression for event \eqref{eq:SignCheck3} will then involve the term $\text{Diag}(\tilde{\zvec}_A)\tilde{\betavec}_A=|\betavec_A|$.  But if our guesses are wrong we do not want a large probability of $\hat{\zvec}_A=\tilde{\zvec}_A$.


%It is straightforward to show the probability of \eqref{eq:InactiveCheck2} is equal for $\pm \tilde{\zvec}_{\tilde{A}}$ using the fact that the distribution of $\frac{1}{n}\F_{ \tilde{I}}^T(\I-\Pmat_{\tilde{A}})\evec$ is unaffected by multiplication by $-1$ and $\lambda \F_{\tilde{I}}^T\F_{\tilde{A}}(\F_{\tilde{A}}^T\F_{\tilde{A}})^{-1}(-\tilde{\zvec}_{\tilde{A}})=-\lambda \F_{\tilde{I}}^T\F_{\tilde{A}}(\F_{\tilde{A}}^T\F_{\tilde{A}})^{-1}\tilde{\zvec}_{\tilde{A}}$.  Hence, so long as $A$ is correctly guessed, there is symmetry about one's choice of $\tilde{\zvec}_A$.
%The equivalence $P(\mathcal{I}(\tilde{\zvec}_A))=P(\mathcal{I}(-\tilde{\zvec}_A))$ tell us this probability is robust to a certain degree of incorrect sign information.  The probabilities $P(\mathcal{S}(\tilde{\zvec}_A))$ and $P(\mathcal{S}(-\tilde{\zvec}_A))$ can almost be written as probabilities involving only $\tilde{\zvec}_A$, except the former involves $\text{Diag}(\tilde{\zvec}_A)\betavec_A$ and the latter involves $\text{Diag}(-\tilde{\zvec}_A)\betavec_A$.







%Suppose $A=\tilde{A}$ but $\zvec_A=-\tilde{\zvec}_A$ and $\betavec_A=-\tilde{\betavec}_A$.




%Suppose a more stringent version holds that replaces the left hand side with $|\widetilde{\D}_s(\widetilde{\X}_1^T\widetilde{\X}_1)^+  \widetilde{\X}_1^T\evec|$.
%Consider now the flipped $\betavec_1$ vector, $-\betavec_1$, which has sign $-\tilde{\svec}_{10}$.  Then the above condition becomes
%\begin{align*}
% \phantom{\Leftrightarrow}&(-\widetilde{\D}_s)(\widetilde{\X}_1^T\widetilde{\X}_1)^+  \widetilde{\X}_1^T(-\widetilde{\X}_1\betavec_1+\evec) > n\lambda (-\widetilde{\D}_s)(\widetilde{\X}_1^T\widetilde{\X}_1)^+(-\tilde{\svec}_{10})\\
% \Leftrightarrow&
%  \widetilde{\D}_s(\widetilde{\X}_1^T\widetilde{\X}_1)^+  \widetilde{\X}_1^T\evec < \widetilde{\D}_s(\widetilde{\X}_1^T\widetilde{\X}_1)^+  \widetilde{\X}_1^T\widetilde{\X}_1\betavec_1 - n\lambda \widetilde{\D}_s(\widetilde{\X}_1^T\widetilde{\X}_1)^+\tilde{\svec}_{10}\ ,\
%\end{align*}
%which holds according to the above stringent condition.  A similar argument can be made for the more stringent condition involving $\etavec$.

%A simple modification to this argument can be made by talking about the probability of the less stringent condition holding.  If $\evec$ comes from a symmetric distribution about $\zerovec$, then the probabilities coincide for both $\betavec_1$ and $-\betavec_1$, even though they may not hold for the same $\evec$.  This is because the probability of the sign flipping for $\evec$ is $0.50$.

%For the case of $\yvec = \widetilde{\X}_1^T\betavec_1+\evec$, $\widetilde{\X}_1$ full rank, and $\tilde{\svec}_{10}=\text{sign}(\betavec_1)$, the above conditions become those found in the strong irrepresentable literature.  If $\widetilde{\X}_1$ is not invertible, then the only change occurs with the sign vector condition involving $\widetilde{\D}_s\Pmat_{\text{row}(\widetilde{\X}_1)}\betavec_1$ instead of $\widetilde{\D}_s\betavec_1=|\betavec_1|$ where $\Pmat_{\text{row}(\widetilde{\X}_1)}$ is the projector onto the row space of $\widetilde{\X}_1$.  If $\betavec_1$ is in the row space of $\widetilde{\X}_1$ then we again arrive at $|\betavec_1|$.





%In terms of a design problem, we want to find an $\X$ matrix that satisfies these conditions with large probability.   Some important points are in order:
%\begin{enumerate}
%    \item Should this probability be calculated for a specific $\lambda$?  How do we do this across a range of $\lambda$? BJS: Maybe considering the maximum probability over $\lambda$.
%    \item Do we do this for a specific model support?  How to generalize to broader supports? BJS: Model spaces, or Bayesian approach.
 %   \item Do we do this for a specific sign vector? BJS: Vector of 1's, or somehow specify random sign vector.
%    \item How do we relax the probability dependencies on the true $\betavec$? The experimenter specifies the size of the active effects, in terms of $\sigma$. Set the inactive effects to 0?
%\end{enumerate}

%Now suppose we only have information about the sign of $\betavec_1$ but not the specific value of $\betavec_1$.  We could assign a distribution on $\betavec_1$ (that maintains the fixed sign) and \eqref{eq:1} becomes
%\begin{align}
%    -\Pmat_{R1}\betavec_1-(\X_1^T\X_1)^+\X_1^T\evec <  - n\lambda (\X_1^T\X_1)^+\onevec\label{eq:1rand}\ .\
%\end{align}
%The left hand side becomes a linear combination of two random variables and we would need to simulate the probabilities unless it has a well-known form (it probably won't).

%If we multiply the left and the right side of the condition by $(\X_1^T\X_1)^{1/2}$, then the random vector will be distributed as $N(0,\sigma^2\Pmat_{R1})$.  The bound on the right hand side becomes
% \[
% n\lambda (\X_1^T\X_1)^{+1/2}\onevec-(\X_1^T\X_1)^{1/2}\betavec_1
% \]
%Question: Is it valid to multiply by matrices and maintain the inequality?  I know it works for equality but inequalities for matrices work differently.  This doesn't seem obvious to me anymore.
%Aside: If we place a distribtuion on $\betavec_1$ how do things change?  Conjecture: as you increase the variance of $\betavec_1$ you will want an orthogonal design.

%One idea: for two design matrices, $\X_1$ and $\Z_1$, assume $(\X_1^T\X_1)^+\onevec < (\Z_1^T\Z_1)^+\onevec$ and show probability inequalities hold for all $\betavec_1$ except those on a set of measure 0.

%We do not recommend this procedure for generating the actual lasso solution because it is inefficient, requiring the algorithm to check many model partitions and potential sign vectors.  However, it does provide expressions for the probability a LASSO estimator will have certain model selection properties, as long as we have a certain model in mind.

%What then do we consider to be a good LASSO estimator?  The strictest definition for the purpose of model selection would be a LASSO solution that correctly partitions $\betavec$ according to a specific irreducible model.  In a way, this is a relaxation on the local optimal design approach that specifies specific values of $\betavec$.  Instead, we are simply saying which $\betavec$ we believe are or are not zero; the specific values are not necessarily of particular interest.  Write $\X=(\X_1 | \X_2)$ with $\X_2$ being extra columns that, for the purposes of design construction, have zero effects on the response.  We may then write $\yvec=\X_1\betavec_1+\evec$ for $\yvec$ in the subgradient conditions.  Choosing a lasso solution that sets $\widetilde{\X}_1$ to the desired $\X_1$ we require a sign vector, $\tilde{s}_{01}$, that gives the lasso solution
%\begin{align}
%\tilde{\betavec}_1=\betavec_1+(\X_1^T\X_1)^{-1}\X_1^T\evec-n\lambda(\X_1^T\X_1)^{-1}\tilde{s}_{10}\ ,\
%\end{align}
%and satisfies the conditions
%\begin{align*}
%\text{sign}(\tilde{\betavec}_1)&= \tilde{s}_{10} \\
%\frac{1}{n}\left[\X_2^T\X_1(\tilde{\betavec}_1-\betavec_1)-\X_2^T\evec\right] &\leq \lambda\ .\
%\end{align*}


%Identifying the unique $\tilde{s}_{1}$ that satisfies https://www.overleaf.com/project/5da5f2cb9330db0001949bdathe solution complicates the evaluation of such a design criterion. It may depend on the specific $\evec$ generated for a given design as well as the chosen $\lambda$.  Ideally, we would like $\tilde{s}_1=s(\betavec_1)$, but this brings us to a finer level of local optimality: defining the effect directions.  Still, this is less restrictive than the typical notion of local optimality.




%\begin{itemize}
%    \item Primal dual witness construction and Irrepresentable condition
%    \item For given $\betavec_1$ and $\sigma^2$ we can calculate these probabilities.  Give example.
%    \item Discuss thresholded lasso solutions and their connection to solution path behavior
%    \item Lasso solutions with model misspecification and effect principles
%\end{itemize}

%\subsection{Thresholded Lasso}


%\textbf{JWS: Use theory for new heuristic justifications for SSD criteria. Show that orthogonal optimizes I event for any $z$ but not necessarily S event. Show that it may not be optimal for S event for $z=1$. Conjecture that orthogonality is best when averaging over all sign vectors. Say leave it for future work because we also want to spend time on exact designs. Stress looking at both approximate and exact designs throughout paper.}

\section{Exact Design Evaluation and Construction}\label{sec:EvalConstruct}

Section~\ref{sec:optimal_cs} identified optimal forms of completely symmetric $\C$ matrices under different assumptions about $\zvec$. Unfortunately, no SSD exists whose $\C$ achieves these forms because $n < p +1$. Ideally, one would implement a design search algorithm that ranks SSDs according to a summary criterion of the $\Phi_\lambda$- or $\Phi_\lambda^{\pm}$-criterion. However, these criteria demand intense computations to evaluate a single SSD and search algorithms require many evaluations. Heuristic criteria such as $UE(s^2)$ and $Var(s+)$, however, are computationally efficient and can help identify SSDs that are close to achieving such forms. We now describe an algorithmic construction that reconciles the rigorous but computationally-prohibitive criteria in Section~\ref{sec:loc} with the computationally-efficient heuristic criteria that were justified in Section~\ref{sec:optimal_cs}.

Figure \ref{fig:HeurComp} demonstrates the relationship between heuristic measures and the sign recovery criteria across 100 designs with varying $Var(s)$ and $UE(s^2)$ values for $n=10$, $p=12$, $k=4$, and $\beta=3$. Since $Var(s+)$ is favorable when $\zvec_\mathcal{A} = \onevec$, the left panel of Figure \ref{fig:HeurComp} compares $Var(s)$ values to $\Phi_{\Lambda}$, while the right panel compares $UE(s^2)$ values to $\Phi_{\Lambda}^{\pm}$. %The purpose of Figure \ref{fig:HeurComp} is to compare $Var(s+)$ and $UE(s^2)$ to each other but to compare them to $\Phi_{\Lambda}(\X \, | \, k=4, \beta =3)$ and $\Phi_{\Lambda}^{\pm}(\X \, | \, k=4, \beta =3)$, respectively. 
There is a clear correlation between the heuristic measures and the sign recovery criteria, but Figure \ref{fig:HeurComp} shows that designs with the same heuristic measure can differ with respect to sign recovery probability. Hence improvements can be made by re-evaluating the optimal heuristic SSDs using the sign recovery criteria.

%For example, the right panel of Figure \ref{fig:HeurComp} shows that nine designs minimize $UE(s^2)$, but these designs differ in their lasso sign recovery probabilities. Additionally, the left panel of Figure \ref{fig:HeurComp} shows two designs that minimize the $Var(s+)$ criterion but differ greatly in their $\Phi_\Lambda$ values.

%\textbf{BJS: Figure 5. As it is now, it appears that it evaluates the measure assuming all sign vectors. In that case, I'm not sure why $Var(s+)$ is so good (and so much more attractive than $UE(s^2))$. In my opinion, we should have $Var(s+)$ on the left, evaluated assuming $\zvec=\onevec$; and $UE(s^2)$ on the right, evaluated assuming all possible sign vectors. (And in the supp material, we should include $Var(s+)$, evaluated assuming all possible sign vectors, to make the point that it seems quite robust.}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{Heuristic_Secondary_measure_updated_log.png}
    \caption{Sign recovery probabilities across all supports of size $k=4$ compared to $Var(s)$ and $UE(s^2)$ values for $n=10$ and $p=12$. The left panel compares $Var(s)$ to $\Phi_\Lambda$ and the right panel shows the relationship between $UE(s^2)$ and $\Phi^\pm_\Lambda$.}
    \label{fig:HeurComp}
\end{figure}


A computationally-friendly approach to constructing optimal SSDs in terms of our criteria is to first perform a search algorithm on many randomly generated SSDs using one or more heuristic criteria and retaining the best-performing SSDs.  The retained designs can then be sifted again based on one of the sign recovery criteria. The design that performs best among these SSDs is then declared ``optimal". We refer to this approach as the \textit{Heuristic-Initiated Lasso Sieve}, or HILS, since it utilizes a sequential filter of designs based on heuristic and lasso sign recovery criteria. Although HILS can significantly reduce the number of $\Phi_{\lambda}$ evaluations compared to constructing an optimal design using, for instance, a coordinate exchange algorithm \citep{meyer_nachtsheim1995, weese2017powerful}, there are settings when even a more modest number of evaluations is not computationally feasible. We now discuss some ways to efficiently evaluate the $\Phi_\lambda$ and $\Phi_\lambda^{\pm}$-criteria, and hence their corresponding summarized versions.%, and hence It is important to note that making $\Phi_\lambda$ more efficient will make the summary measures, $\Phi_\Lambda$ and $\Phi_{max}$, more efficient. 



%In the local optimal case, i.e., when $\mathcal{A}$, $\betavec_\mathcal{A}$, and $\zvec_\mathcal{A}$ are known, calculation of $\phi_\lambda(\X \, | \, \betavec)$ requires only a product of multivariate normal probabilities. However, relaxing the conditions for local optimality necessitates a lasso sign recovery design criteria that considers all possible supports and sign vectors, denoted in Section \ref{sec:reloc} as $\phi_{\Lambda}(\X \, | \, k, \beta)$. However, the use of $\phi_{\Lambda}(\X \, | \, k, \beta)$ as a design construction or evaluation measure then depends on its computational feasibility. 

%\subsection{Efficient evaluation of $\Phi_\Lambda$ and $\Phi_\Lambda^{\pm}$}\label{sec:EffEval}



%How to choose $\Lambda$ and how we integrate.




For a support size of $k$ and design $\X$, one full evaluation of $\Phi_{\lambda}$ and $\Phi_{\lambda}^{\pm}$ requires consideration of all $\binom{p}{k}$ supports of size $k$. For $\Phi_{\lambda}$, an additional $2^k$ computations are required for each $\mathcal{A}$ to consider all sign vectors. Evaluating either criteria would benefit from a reduction in the number of supports considered. Randomly sampling from $\mathcal{A}_{k}$ is intuitive, but may require large subsamples to be representative.  As high correlations between two factors can result in diminished lasso support recovery performance due to low $P(I_\lambda \, | \, \F, \, \zvec)$, a support sampling method that balances (or approximately balances) the number of times pairwise sets of factors are included in $\mathcal{A}$ together can be advantageous. \cite{smucker2015approximate} utilized nearly balanced incomplete block designs (NBIBDs) to approximate a model space with only a relatively small number of blocks, in the context of model-robust optimal designs. Hence, we recommend implementing the NBIBD sampling method to adequately represent $\mathcal{A}_{k}$ with between 64-128 supports for modest $p$ and $k$ values. We denote such a subset of supports by $\tilde{\mathcal{A}}_{k}$.

%In this case, we can use the techniques described in Sections~\ref{sec:reloc} and~\ref{sec:EffEval} to evaluate  $\bar{\phi}_{\Lambda}$ instead, as Algorithm 1 accommodates. 





From Lemma~\ref{lem:symmetry}, for a fixed $\mathcal{A}$, the probabilities are equal between $\zvec_\mathcal{A}$ and $-\zvec_{\mathcal{A}}$. Thus, only $2^{k-1}$ sign vectors need be considered for the $\Phi_\lambda^{\pm}$-criterion. While this cuts the computation in half, prior knowledge from the practitioner can be leveraged to select an even smaller set of representative sign vectors to evaluate. For example, if there is no knowledge on sign direction, the most likely sign vectors are those with an equal, or nearly equal, number of $\pm 1$. For $k=10$, one would need to only consider, up to reflection, the $126$ possible sign vectors having five $+1$'s and $-1$'s rather than all $2^{10-1}=512$ vectors. There may also be strong prior belief about the signs of some or all of the factors that can reduce the set of sign vectors to a manageable number, or even to a single element. We generally denote such a subset of supports by $\tilde{\mathcal{Z}}_k^{\pm}$.


The HILS algorithm is formally stated in Algorithm~\ref{alg:HILS} and specifically focuses on the $UE(s^2)$- and $Var(s+)$-criteria as the initiating heuristic measures. The algorithm is easily generalized to include other heuristic measures if desired. Besides the obvious necessary inputs for the desired design size and the desired sign-recovery criterion, the algorithm requires the user to specify the number of initial designs for the construction algorithms for the two heuristic measures as well as the number of such final designs to retain. In the case of $UE(s^2)$, one can generate $UE(s^2)$-optimal designs using the techniques described in \cite{jones2014optimal}. For the $Var(s+)$-criterion, one can also manipulate the threshold for the $UE(s^2)$ efficiency. Additionally, the required $\Phi$-criterion input represents the option of evaluating designs using the fixed $\lambda$ measure ($\Phi_\lambda$), $\Phi_\Lambda$, or $\Phi_{\max}$ as well as deciding between $\Phi$ and $\Phi^{\pm}$. Optional inputs include $\tilde{\mathcal{A}}_k$, $\tilde{\mathcal{Z}}_k^{\pm}$, as well as a supplementary collection of designs, $\mathcal{X}_e$, such as the Pareto-efficient designs by \cite{singh2022selection}.% that may be found from existing design catalogues. %These extra designs allow for practitioners to utilize designs generated using more computationally intensive methods than the SSD heuristics.   

\spacingset{1} % DON'T change the spacing!
\begin{algorithm}[ht]
\caption{Heuristic Initiated Lasso Sieve Algorithm}\label{alg:HILS}
\begin{algorithmic}
\State{\textbf{Necessary Inputs: }$n$; $p$; $k$; $\beta$; $\Phi$-criterion, $m_v=$ number of initial designs for $Var(s+)$ algorithm; $m_u=$ number of designs for $UE(s^2)$ algorithm; $m_v^*=$ number of retained $Var(s+)$ designs; $m_u^*=$ number of retained $UE(s^2)$ designs; .}
\State{\textbf{Optional Inputs: } $\tilde{\mathcal{A}}_{k}$; $\tilde{\mathcal{Z}}_{k}^{\pm}$; $\mathcal{X}_e$ the set of extra designs to evaluate.}
\State\textbf{Output: } $\mathbf{X}_{HILS}$, final design.
    \State \textbf{Step 1:} Generate $m_v$ starting designs, construct $m_v$ $Var(s+)$-optimal designs. Retain the best $m_v^*$ designs; denote this set as $\mathcal{X}_v^*$
    %from the set of $Var(s+)$-optimal designs in Step 1A, in terms of their $Var(s)$ values. .
    
    %Let the set $\mathcal{X}$ denote the set of such designs with $|\mathcal{X}|=m$.
    \State \textbf{Step 2:} Generate $m_u$ $UE(s^2)$-optimal designs (see \cite{jones2014optimal}). Retain the best $m_u^*$ designs; denote this set as $\mathcal{X}_u^*$.
    %Let the set $\mathcal{X}$ denote the set of such designs with $|\mathcal{X}|=m$.
    %\State \textbf{Step 2A:} 
    %\State \textbf{Step 2B:} Select the best $m_u^*$ designs from the set of $UE(s^2)$-optimal designs in Step 1B. Denote this set as $\mathcal{X}_u^*$.
    \State \textbf{Step 3:} Set $\mathcal{X}=\mathcal{X}_v \cup \mathcal{X}_u \cup \mathcal{X}_e$.
    %\State \textbf{Step 4:} 
    %\If{Signs are known}
        %\State Choose $\X_{HILS} = \underset{\X\in\mathcal{X}}{\text{argmax}}\left\{ \Phi(\X \, | \, k, \beta)\right\}$
    %\Else 
    %    \State Choose $\X_{HILS} = \underset{\X\in\mathcal{X}}{\text{argmax}}\left\{ \Phi^{\pm}(\X \, | \, k, \beta)\right\}$
    
\State \textbf{Return: } $\X_{HILS}=\underset{\X\in\mathcal{X}}{\text{argmax}}\left\{ \Phi(\X \, | \, k, \beta)\right\}$. 
\end{algorithmic}
\end{algorithm}

\spacingset{1.9} % DON'T change the spacing!



%requires a set of $m$ starting designs and a set of heuristic measures with size $m$. Each of the starting designs are then optimized using a corresponding heuristic measure and then continued into the HILS process. %Note that the starting designs need not be random, but, for example, if the $m$ starting designs are all $UE(s^2)$ optimal, and the heuristic measure utilized in step 1 of HILS is the $UE(s^2)$, there is no need for step 1 in the HILS algorithm.  




%However, this reduction must be representative of $\mathcal{A}_{k^*}$ and/or $\mathcal{Z}_{k^*}$.


%\ref{lem:equiv} shows that the local sign recovery probability is equivalent under sign flips, meaning $P(\hat{\zvec}_{\hat{A}}=\tilde{\zvec}_A)= P(\hat{\zvec}_{\hat{A}}=-\tilde{\zvec}_A)$.

 %Notationally, we represent a subset of sign vectors as $\bar{\mathcal{Z}}_{k^*} \subset  \mathcal{Z}_{k^*}$, which can be specified by the analyst.

%\textbf{KY: Discuss paragraph above: Do we just cut this out, or keep it. While technically, you would need to specify the entire sign vector, in general, if all signs were equally likely, a balanced $\zvec_A$ would still be most representative.}
% BJS 11/22: I like the paragraph.

%A possible set of sign vectors to consider in that case could be $\{\zvec : \onevec^T\zvec \in \{-1,0,1\}\}$.

 %For simplicity, we represent the chosen support subset as $\bar{\mathcal{A}}_{k} \subset \mathcal{A}_{k}$.

%Thus, by leveraging prior knowledge to select certain sign vectors to evaluate and by selecting a subset of supports using the NBIBD approach, a practitioner can drastically reduce the computational burden of $\Phi_{\Lambda}(\X \, | \, k, \beta)$ by instead averaging over a reduced set of submodels and signvectors. For brevity, we let $\bar{\Phi}_{\Lambda}(\X \, | \, k, \beta)$ represent the analog of $\Phi_{\Lambda}(\X \, | \, k, \beta)$ over a reduced set of supports and sign vectors. In the next section, we use $\Phi_\Lambda$  to evaluate supersaturated designs constructed using heuristic measures of quality and develop an approach to finding quality supersaturated designs with respect to lasso support recovery. 


%Even with these improvements, using this measure to construct designs in a traditional coordinate exchange algorithm proves intractable.  

%In the next section, we use these measures to evaluate supersaturated designs constructed using heuristic measures of quality, and in Section \ref{sec:construction} we attempt to construct optimal designs from scratch.





%\subsection{Supersaturated Heuristic Sieve}\label{sec:sieve}

%For supersaturated design construction, heuristic measures such as $Var(s+)$ and $UE(s^2)$ are attractive due to their computational simplicity. Section~\ref{sec:reloc} discusses how these criteria relate favorably to ideal information matrices under the lasso sign recovery probability, but these ideal structures are not possible in the supersaturated scenario. 


%In some settings, heuristic measures lack the ability to truly distinguish between designs for lasso sign recovery performance. This inability is unsurprising given that the heuristic measures are generally based on OLS intuition.
%This does not, however, imply that heuristic measures should be abandoned entirely as a means of SSD construction. 


%The idea of applying a secondary design measure to find high performing designs has been used in supersaturated design assessment \citep[][]{jones2014optimal,cheng2018s2} and in design more broadly \citep[e.g.][]{eccleston1974theory}. By using heuristics which have connections to ideal design structures (see Section \ref{sec:reloc}), and evaluating them with the lasso support recovery measure, HILS explicitly connects these OLS-inspired heuristics with the lasso screening analysis.





% \textbf{KY: Should we add the discussion here about how heuristic optimal designs, especially Var(s+), have a distribution of lasso measure values, so taking any Var(s+) could be potentially dangerous. For instance, the one Var(s+) design Rahki compares to is not the best. }\
% BJS 11/22: Seems like we could leave this for Section 5.


%For HILS to be efficient, the evaluation of $\phi_{l}(\mathcal{A}_{k^*},\mathcal{Z}_{k^*})$ for each of the top $mq$ heuristic designs must be feasible. The remainder of this section will focus on improving the computational burden from the evaluation of designs with respect to the lasso probability, with design construction techniques for the lasso sign recovery measure directly coming in later sections. 





\section{Examples}\label{sec:NewDesigns}

We now demonstrate optimal SSDs found under our criteria via HILS under two example scenarios. For each case, we compare the optimal SSD under HILS with the Pareto efficient designs (PEDs) of \cite{singh2022selection} by inspecting the $\Phi_\lambda$ and $\Phi_\lambda^{\pm}$ curves as a function of $\log(\lambda)$. The PEDs were generated assuming $\zvec_\mathcal{A}=\onevec$ and were based on heuristic criteria corresponding to the Dantzig selector. 

%The authors compared their PEDs to $Var(s+)$-designs via a simulation study that implemented a Gauss-Dantzig selector and performed tuning parameter selection via the Bayesian Information Criterion.

%\textbf{JWS: Remind reader how people compare SSDs with simulations and awkward tuning parameter selection strategies.}%While $\phi_l$ is efficient at comparing designs over a range of $\lambda$, the examples also show the average simulated solution path for each design and how the probabilities of $\mathcal{I}$ and $\mathcal{S}$ evolve with $\lambda$. For ease of notation, we often suppress the notation of $\phi_{l}(\mathcal{A}_{k^*},\mathcal{Z}_{k^*})$  to $\phi_{l}(\zvec_A\in \mathcal{Z}_{k^*})$.








%criteria are optimizing are based upon asymptotic results, and the algorithm is computationally demanding. 

 

\subsection{Scenario 1: $n=9$, $p=10$}

The HILS algorithm was performed for $k=3$ and $\beta=3$ assuming unknown signs. %This represents a small experiment and a setting in which heuristic optimal designs should perform quite well. 
In this example, it was feasible for $\Phi^{\pm}_{\lambda}$ to average over all possible supports and sign vectors. The HILS design was generated from $100$ random starting designs with $m_u =50$ for the $UE(s^2)$-criterion and $m_v = 50$ the $Var(s+)$-criterion. The construction algorithm for the $Var(s+)$-criterion randomly selected $UE(s^2)$-efficiency values from $\{0.5,0.6,0.7\}$. The PED from \cite{singh2022selection} was added as an extra design to evaluate.  %The $50$ $UE(s^2)$-optimal designs all had the same $UE(s^2)$ value, so there were all included in step 2 of HILS. Of the $50$ $Var(s+)$-optimal designs, the top $25$ in terms of the $Var(s)$ measure were selected for step 2, giving an $m^*=50$. %While there are 8 such unique $\zvec_\mathcal{A}$, due to Lemma \ref{lem:symmetry}, only 4 unique sign vectors are required to average over. 
Two HILS-optimal designs were found using $\Phi_{\max}^{\pm}$ and $\Phi_\Lambda^{\pm}$, denoted by $\text{HILS}_{\max}$ and $\text{HILS}_\Lambda$, respectively. 

Interestingly, both $\text{HILS}_{\max}$ and $\text{HILS}_\Lambda$ were $Var(s+)$- and $UE(s^2)-$optimal. These two designs were instances of separate runs of the $Var(s+)$ coordinate exchange algorithm and have different $Var(s)$ values: $\text{HILS}_{\max}$ had a $Var(s)= 2.380$ with $UE(s)=0.273$, while $\text{HILS}_{\Lambda}$ had a $Var(s)= 2.438$ with $UE(s)=0.127$. These designs were similar in terms of both $\Phi^{\pm}_{\max}$ ($0.785426$ for $\text{HILS}_{\max}$ and $0.785424$ for $\text{HILS}_\Lambda$) and $\Phi^{\pm}_{\Lambda}$ ($1.1216621$ for $\text{HILS}_{\max}$ and $1.1216624$ for $\text{HILS}_\Lambda$). %With either $\Phi$-measure, the two designs were the top two performers. 


The left panel of Figure \ref{fig:Scenario1JointProb} compares $\Phi_\lambda^{\pm}$ between $\text{HILS}_{\max}$, $\text{HILS}_\Lambda$, and the PED. The difference between the $\Phi_\lambda^{\pm}$ between $\text{HILS}_{\max}$ and $\text{HILS}_\lambda$ were on the order of $10^{-5}$ so only one general HILS-optimal curve is shown. Clearly the HILS-optimal designs outperform the PED in terms of both $\Phi^{\pm}_{\max}$ and $\Phi^{\pm}_\Lambda$. Since the PED is constructed assuming $\zvec_{\mathcal{A}}=\onevec$ and this scenario assumes no sign information, the performance of the HILS-optimal designs versus the PED was expected.  The right panel of Figure \ref{fig:Scenario1JointProb} shows the contour plots of $\psi^{\pm}_\lambda$, assuming $\C$ could be completely symmetric. The probability is highest over a larger range of $\log(\lambda)$ when $c=0$, which is consistent with Theorem \ref{thm:CS_locl_opt_allsign}. It is unsurprising then that the two designs selected by HILS are $UE(s^2)$-optimal, but the $Var(s+)$-criterion improved performance.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\linewidth]{Section4_scenario1_joint_and_CS.png}
    \caption{The left plot shows $\Phi_\lambda^{\pm}$ vs. $\log(\lambda)$ for the HILS-optimal designs and the PED under scenario 1. The differences in the $\Phi_\lambda^{\pm}$ between $\text{HILS}_{\max}$ and $\text{HILS}_\lambda$ were indistinguishable, so a general HILS-optimal curve is shown against the PED. The right plot shows the sign recovery probability contour plots if $\C$ were completely symmetric.}
    \label{fig:Scenario1JointProb}
\end{figure}
 

%Plots B and C from Figure \ref{fig:Scenario1JointProb} show the sign recovery performance across a range of $\lambda$ values for known signs ($\Z=\I$)  and for a general mixed sign setting ($\Z=\Z^*$), respectively. Although the Pareto efficient design's optimality criteria assume $\zvec_{\mathcal{A}}=\onevec_3$, the HILS-optimal design shows higher performance over a larger $\lambda$ range. This may seem counter-intuitive, but the HILS-optimal design is itself $Var(s+)$ optimal, which is also known to have increased performance for $\zvec_\mathcal{A}=\onevec$. It is important to mention that we are comparing the designs at a fixed $k$ value in Figure \ref{fig:Scenario1JointProb}, and it is possible that the Pareto efficient designs perform better as $k$ increases. 

As the PED was created assuming $\zvec_\mathcal{A}=\onevec$, we compared it to the two HILs designs with $\Phi_\lambda$. As the HILS-optimal designs were also $Var(s+)$-optimal, these designs were expected to also perform well. The left panel of Figure \ref{fig:Scenario1JointProb_allpos} compares $\Phi_\lambda$ for the $\text{HILS}_{\max}$, $\text{HILS}_\Lambda$, and the PED. While the differences in $\Phi_\lambda^{\pm}$ between $\text{HILS}_{\max}$ and $\text{HILS}_\lambda$ were very small, there are visible differences in $\Phi_\lambda$. Both of the HILS-optimal designs again outperform the PED, but in different ways. Only the $\text{HILS}_{\max}$ had a larger $\Phi_{\max}$ than the PED, but both HILS designs had a larger $\Phi_{\Lambda}$. The right panel in Figure \ref{fig:Scenario1JointProb_allpos} shows the sign recovery probability contour plots if $\C$ were completely symmetric. The probability is highest over a larger range of $\log(\lambda)$ when $c>0$, which is consistent with Lemma \ref{lem:CS_event_derivs}. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\linewidth]{Section4_scenario1_joint_and_CS_allpos.png}
    \caption{The left plot shows $\Phi_\lambda$ vs. $\log(\lambda)$ for the HILS-optimal designs and the PED under scenario 1 with $\zvec_\mathcal{A}=\onevec$.  The right plot shows the sign recovery probability contour plots if $\C$ were completely symmetric under scenario 1 with $\zvec_\mathcal{A}=\onevec$. The optimal $c$ value in this case is $c=0.17$.}
    \label{fig:Scenario1JointProb_allpos}
\end{figure}


%This scenario shows that HILS can be used to find SSD heuristic optimal designs that out-perform designs generated under more computationally burdensome construction techniques. Improvement over the PED is somewhat expected in the case where there is no sign information. However, the left panel of Figure \ref{fig:Scenario1JointProb_allpos} shows that HILS can also find designs that outperform the PED in the exact scenario in which PEDs are constructed for $\zvec_\mathcal{A}=\onevec$.

\subsection{Scenario 2: $n=14$, $p=20$}

This scenario assumed known effect directions with $k=5$ and $\beta=3$. Only the $Var(s+)$-criterion was considered in the HILS algorithm, setting $m_v=50$ with randomly selected $UE(s^2)$-efficiency constraints from $\{0.5,0.6,0.7,0.8\}$ and a minimum $UE(s)$ value was randomly selected from $\{0,0.1\}$. The latter was done to further encourage positive off-diagonal values. While these are not exactly $Var(s+)$ designs, they are similar and provide a wider candidate set of designs. The PED from \cite{singh2022selection} was also evaluated. The evaluation of $\Phi_{\max}$ and $\Phi_\Lambda$ proved too computationally costly since there were $\binom{20}{5}$ possible supports in $\mathcal{A}_k$. Instead, $\Phi_{\max}$ or $\Phi_\Lambda$ were measured over a subset of $\mathcal{A}_k$ selected using the NBIBD approach in Section \ref{sec:EvalConstruct}. HILS selected the PED as best in terms of both $\Phi^{\pm}_{\max}$ or $\Phi^{\pm}_\Lambda$ but we also report the second best design found by HILS (a $Var(s+)$-optimal design with $UE(s^2)$-efficiency $0.8$). The left panel of Figure \ref{fig:Scenario2JointProb} shows $\Phi_{\lambda}(\X | k, \beta)$ for the two designs and clearly shows the superiority of the PED. 



\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\linewidth]{Section4_scenario2_joint_and_CS.png}
    \caption{The left plot shows $\Phi_\lambda$ vs. $\log(\lambda)$ for the HILS-optimal designs and the PED under scenario 2 with $\zvec_\mathcal{A}=\onevec$.  The right plot shows the sign recovery probability contour plot if $\C$ were completely symmetric under scenario 2 with $\zvec_\mathcal{A}=\onevec$. The optimal $c$ value is $c=0.09$.}
\label{fig:Scenario2JointProb}
\end{figure}

From Lemma \ref{lem:CS_event_derivs}, it is expected that designs with small, and nearly constant positive $UE(s)$ values will perform better in the known signs case. This is demonstrated by the right panel of Figure \ref{fig:Scenario2JointProb}. The PED had $Var(s)=5.861$ with $UE(s)=0.590$ and $UE(s^2)$-efficiency of $0.859$.  The $Var(s+)$-optimal design has $Var(s)=5.240$ with $UE(s)=0.838$ and $UE(s^2)$-efficiency of $0.897$. It is unclear from the heuristic measures alone which design will perform better. The PED had smaller $UE(s)$ but larger $Var(s)$, while the $Var(s+)$-optimal design had larger $UE(s)$ but smaller $Var(s)$. This highlights the potential downside of using the $Var(s+)$-criterion for choosing an SSD and also stresses the importance of considering not just $Var(s+)$-optimal designs in HILS, but also designs that are efficient in terms of $Var(s+)$. %That is, the PED could have been constructed from one of the $m_v$ initial designs under the $Var(s+)$-criterion but would not have been selected as the optimal design.


%This design represents what the $\text{HILS}$-optimal design would have been (for either $\Phi^{\pm}_{max}$ or $\Phi^{\pm}_\Lambda$) if the PED was not evaluated as an extra design. 
%The left panel of Figure \ref{fig:Scenario2JointProb} shows that there is a performance boost when utilizing the PED over the next best design in terms of both the maximum $\Phi_\lambda$ and the area under the $\Phi_\lambda$ curve. This highlights an important facet of the HILS algorithm in general: the optional evaluation of extra designs generated using more computationally intensive construction methods.
%While one could argue that the performance gain in the left plot of Figure \ref{fig:Scenario2JointProb} is small compared to the added computational burden of generating PEDs, the PED utilized here is from the catalogue in \cite{singh2022selection}. Therefore, there is little computational burden added to consider the PED in the HILS algorithm.  

%When there exists other designs generated using a more computationally intensive construction method that truly out perform the SSD heuristics, the HILS algorithm will select that design as optimal. 





%Figure \ref{fig:Scenario2JointProb} represents the $\phi_{l}(\mathcal{A}_{k^*},\zvec_A=\onevec_3)$ measure for a fixed $\lambda$ as $\lambda$ changes. While the Pareto efficient designs show a larger maximum at higher sign recovery performance at a given $\lambda$, the HILS-optimal design shows more robust sign recovery performance across a range of $\lambda$ values. Since tuning parameter selection is inherently uncertain, we view the HILS-optimal design as superior. 






%While the $\phi_l$ measure assesses the probability of perfect sign recovery, it is important to understand how the designs perform in terms of the lasso solution itself. To demonstrate this a simulation of the lasso solution paths for scenario 2 was conducted for the HILS-optimal and Pareto efficient designs with active factor columns assigned randomly for each run of the simulation. Since, in scenario 2, all active effects have the same signal-to-noise ratio, the coefficient path for a general active effect is considered. This is approached similarly for inactive effects. Figure \ref{fig:Scenario2Path} shows the interquartile range (IQR) of the simulated estimation error, $|\hat{\beta}-\beta|$ for each design separated by active and inactive effects. Note that for the inactive effects, the estimation error is simply $|\hat{\beta}-0|$. In general, the HILS-optimal design estimates active coefficients more accurately compared to the Pareto efficient designs. For the inactive effects, HILS-optimal designs estimate a higher range of of coefficients compared to Pareto efficient designs. This is expected since Pareto efficient designs target the $\mathcal{I}$ event specifically. However, it should be noted that the scale of the y-axis of the two plots in Figure \ref{fig:Scenario2Path} are different, and the difference for the inactive effects is actually quite small. 


%\begin{figure}[H]
%    \centering
%    \includegraphics[width=1.0\textwidth]%{Section4_n14_k20_soln_path_figure.png}
%    \caption{}
%    \label{fig:Scenario2Path}
%\end{figure}

\section{Discussion}\label{sec:Discussion}

%Algorithmic design construction using the lasso support recovery measure directly is quite challenging. However, for brave souls who would try, the output of HILS can be used to produce quality starting designs. Since the evaluation even of $\phi_{\Lambda}(\X \, | \, k, \Z, \beta)$ is computationally costly, starting a construction algorithm with a quality design reduces to number of measure evaluations to achieve convergence compared to a random start. In the next section, we give two examples of the HILS algorithm and compare the designs to the Pareto Efficient Designs from \cite{singh2022selection}.

The SSD literature has predominately constructed designs by optimizing heuristic criteria that measure a design's proximity to a (nonexistent) orthogonal design. The criteria are tractable in their optimization, in that optimal designs can generally be constructed directly or algorithmically in a reasonable amount of time. However, these criteria are not directly tied to a screening analysis method, so there is no guarantee that the resulting analysis under an optimal design will have good statistical properties. This article resolves this disconnect by optimizing criteria based on the probability of sign recovery under the lasso, which is well-defined even when $n-1 < p$. Our major contributions are:
\begin{enumerate}
    \item A local optimality criterion assuming known $\betavec$ and fixed $\lambda$. A trivial design that confounds all inactive factors with the intercept is shown to be optimal for sign recovery. An exact design construction is given that can improve the probability of sign recovery over an orthogonal design for some $\lambda$ . The design has positive and nearly constant pairwise correlations, following the ideal structure for the $Var(s+)$-criterion.
    \item More practical criteria that relax the assumptions about $\betavec$ and $\lambda$. Such criteria are computationally intensive and hence difficult to optimize both analytically and algorithmically, requiring computations across all supports of a given size and potentially many sign vectors.
    \item A study of the optimal form of $\C$, the lasso information matrix, with and without known $\zvec$. By conditioning on completely symmetric matrices, we arrive at a univariate optimization problem. The framework mimics the approximate design approach for least-squares analyses.
    \item In the case of known $\zvec$, we prove that across all completely symmetric matrices, $\C=\I$, i.e., an orthogonal design, is suboptimal for nearly all $\lambda >0$. The optimal form instead takes on positive, constant off-diagonal elements. In the case of unknown sign information, $\C=\I$ is shown to be a local maximum for a range of $\lambda$. These optimal forms rigorously justify the $UE(s^2)$- and $Var(s+)$-criterion in the cases of unknown and known $\zvec$, respectively. This at least partially solves the open problem of how $Var(s+)$-optimal designs achieved better screening properties in simulations.
    \item From our justification of the heuristic criteria, an exact design construction algorithm, named the Heuristic-Initiated Lasso Sieve. Essentially, the proposed criteria are used as a secondary ranking of many high-quality designs generated under heuristic criteria. HILS compromises the computationally-efficient heuristic criteria with the more statistically sound, but computationally prohibitive, sign recovery criteria.
    %\item One of the most surprising implications of our work is that orthogonal designs are not always optimal under the Lasso estimator. This has been hinted at in the literature, because in the SSD setting $Var(s+)$-optimal designs, with their non-minimal column correlations, do not recognize orthogonal designs as the ideal they are striving to meet. In our work, we have shown that when orthogonal designs are possible (which of course requires $n>p$) and sign vectors are known, it can actually be better to have a small, positive correlation between columns (see Figures \ref{fig:trivialcase} and \ref{fig:Ideal_joint_both}).
\end{enumerate}

%We have also provided theoretical results that give new justifications for existing heuristic supersaturated design construction criteria, for the Lasso estimator. In particular, we find that (a) $Var(s+)$-like structures, which encourage small, positive, off-diagonal information matrix elements, are aligned with optimal designs when effect directions are known; and (b) $E(s^2)$-optimal designs, which minimize the amount of correlation among design columns, are desirable when the sign vector is unknown. Finally, we present a procedure, called the Heuristic-Initiated Lasso Sieve (HILS), that uses existing heuristic criteria to efficiently select designs that perform well on our criterion. We show that HILS designs outperform existing state-of-the-art designs, constructed assuming a regularization analysis. 

Our work provides an alternative to using simulation to compare SSDs, which can be tedious and difficult to reproduce independently. In particular, there are at least two reasons to be skeptical of conclusions drawn from simulations in the context of SSDs. First, simulation can be misleading when subtly different versions of complicated statistical procedures are used. In our attempts to reproduce and compare the simulation studies of \cite{singh2022selection} and \cite{weese2021strategies}, we discovered the regularization methods used were sensitive to two different aspects of the procedure implementation. In particular, these papers used the Gauss-Dantzig selector with different ways of exploring the tuning parameter space. They also differed in how they implemented a secondary thresholding of the estimates which determines a subset of the active factors that should remain active. Typically this threshold removes estimates whose absolute magnitude are less than $\sigma^2$, but thresholding in this way is of dubious reliability because there is no natural way to estimate $\sigma^{2}$ and thus threshold levels are more or less arbitrary.
%In Appendix \ref{app:compare_regs} we provide some more detailed comments and, with a little irony, the results of a simulation which show how such results can be misleading. 
The approach we present in this paper avoids these difficulties by eliminating the need for simulation at all, in a similar way that closed-form power analysis procedures are routinely used in simple, replicated experimental design settings.

Another danger of the heuristic criteria is that there is no guarantee the optimal design has any statistical value. For example, there technically exists a $UE(s^2)$-optimal design with $n=2$ runs and $p=100$ factors, but this design has no statistical value. Our criteria, however, do reflect statistical value and so can be used to give objective information about design quality. Experimenters can specify a minimum effect size of interest as well as an educated guess regarding the number of factors that are likely to have such an effect size and investigate the sign-recovery probabilities as a function of $\lambda$. If the maximum average probability is close to 0, it is unlikely that the design can provide reliable sign recovery for the specified effect size. Such a low value could lead to a reconsideration of the runs budget, expected sparsity, and/or the size of effects that are deemed of interest to detect. 

Our methodology allows an even deeper investigation, if desired. For example, we can determine whether a low average probability is due to an inability to reliably exclude all inactive effects (the $I_\lambda$ event) or to reliably include all active effects (the $S_\lambda$ event). In screening experiments, we are much more concerned with identifying the true effects than with allowing inactive effects through the filter. This suggests that the $S_\lambda$ event is more important to the experimenter than the $I_\lambda$ event.

There are many avenues of future research to be explored. First, it is important to establish the concavity (or log concavity) of the criteria that are summarized over $\lambda$. We have already seen that such criteria are not concave for a fixed $\lambda$. Second, new construction techniques for designs approaching the desired structure of the $Var(s+)$-criterion would be incredibly useful. Continuing with exact design construction, while our algorithm is faster relative to \cite{singh2022selection}, it is much slower than that for the heuristic criteria. More work is needed to make the design construction algorithms faster. Finally, we are currently extending the criteria to include the case where inactive factors have small, but nonzero, effects as well as considering the case of criteria for a thresholded lasso.


%\begin{itemize}
%    \item discussion of Theorem 2 / orthogonality?
%\end{itemize}

%\textbf{ KY: Aside from meeting on 09/06/2022 the following needs to be added to the discussion.}

%\begin{itemize}
%    \item When performing a screening analysis via the lasso, we recommend viewing the lasso as means of generating a finite set of submodels along its solution path. 
%    \item Ideally, if an effect is active for some $\lambda_1$, it should remain active for $\lambda_2<\lambda_1$. This is not always the case, however. 
%    \item We recommend shifting how lasso submodels are generated across a solution path by requiring that $\hat{A}_{\lambda_1} \in \hat{A}_{\lambda_2}$ for $\lambda_2<\lambda_1$.
%    \item With the requirement above, the analysis can be performed with the tuning parameter can then be selected using a variety of methods on the augmented solution path. 
    
    
%\end{itemize}

%When $\beta_{Aj}$ is large enough, the probability of such a solution is low so long as its column isn't highly correlated with the other columns (whether in $A$ or $I$).  More to say here?



%\begin{itemize}

%\item Design/analysis approaches to choose appropriate $\lambda$ value with data-driven estimates of $\sigma^2$.  Propose augmenting screening with center runs or replication for pure error? 


%\item Are there some SSD sizes that we should just ignore from now on?

%\item Approximate design theory?

%\item Discuss possible pitfalls of lambda-integration, pareto fronts of multiple ways of summarizing over lambda, etc.

%\end{itemize}

\bigskip
\begin{center}
{\large\bf SUPPLEMENTARY MATERIAL}
\end{center}

Following the References are proofs of all results as well as computational details on the evaluation of our criteria. R code to identify optimal $c$ values for the criteria in Section~\ref{sec:optimal_cs} and construct designs with the HILS algorithm, as well as code and designs to replicate the examples in Section~\ref{sec:NewDesigns}, may be found at
\url{https://github.com/hkyoung361/Lasso_Optimal_SSD}.

%\begin{description}

%\item[Lasso\_Optimal\_SSD\_code:]  (ZIP)

%\item[Title:] Brief description. (file type)

%\item[R-package for  MYNEW routine:] R-package MYNEW containing code to perform the diagnostic methods described in the article. The package also contains all datasets used as examples in the article. (GNU zipped tar file)

%\item[HIV data set:] Data set used in the illustration of MYNEW method in Section~ 3.2. (.txt file)

%\end{description}


%\section*{Supplementary Materials}

%\small

%The supplementary materials includes a PDF with proofs and computational details on the evaluation of the criteria. R files are included that: construct designs following \eqref{eqn:LODbetter}; 

%\section*{Acknowledgements}

%The authors have no funding support to acknowledge.


 
 

\bibliographystyle{asa} 
\bibliography{power.bib}

\newpage

\setcounter{equation}{0}

\footnotesize

\section{Proofs}

The equations have been renumbered here starting with (1). When referring to previous equation numbers, we will say the equation refers to one from the main document.

\subsection{Proof of Lemma 1}
First, the random vectors $\uvec$ and $\vvec$ in the $S_\lambda$ and $I_\lambda$ events, respectively are multivariate normal random vectors with covariance $0$. Thus, $\uvec$ and $\vvec$ are independent, meaning the $S_\lambda$ and $I_\lambda$ events themselves are independent. Therefore, 
\begin{align}
    \phi_\lambda(\X \, | \, \betavec)=P(S_\lambda \cap I_\lambda \, | \, \F, \, \betavec) = P(S_\lambda \, | \, \F_\mathcal{A}, \, \betavec_{\mathcal{A}}) \times P(I_\lambda \, | \, \F , \zvec )\ .\ \label{eq:localcrit}
\end{align}
\noindent
Showing that $\phi_\lambda(\X \, | \, \betavec)=\phi_\lambda(\X \, | \, -\betavec)$ is equivalent to showing that both $P(S_\lambda \, | \, \F_\mathcal{A}, \, \betavec_{\mathcal{A}})= P(S_\lambda \, | \, \F_\mathcal{A}, \, -\betavec_{\mathcal{A}})$ and $P(I_\lambda \, | \, \F , \zvec )=P(I_\lambda \, | \, \F , -\zvec )$.

The normal random vector $\uvec$ in the $S_\lambda$ event and the $\vvec$ in the $I_\lambda$ event can we written as functions of the standard normal vector $\evec$ in the following ways:

\begin{align}
\uvec &=  -\frac{1}{\sqrt{n}}\Z_{\mathcal{A}}\C_\mathcal{A}^{-1} \F_{\mathcal{A}}^{T}\evec + \lambda_n \Z_\mathcal{A}\C_\mathcal{A}^{-1}\zvec_\mathcal{A}\ ,\ \label{eq:Sevent_rewrite}\\  
\boldsymbol{v} &= \frac{1}{\sqrt{n}}\F_{ \mathcal{I}}^{T}(\I-\Pmat_{\mathcal{A}})\evec +  \lambda_n \C_{\mathcal{I}\mathcal{A}}\C_{\mathcal{A}\mathcal{A}}^{-1}\zvec_{\mathcal{A}} \label{eq:Ievent_rewrite}\ .\
\end{align}


\noindent
Both $\evec$ and $-\evec$ follow a $N(\zerovec,\I)$ so
 \begin{align*}
     P(I_\lambda \, | \, \F , \zvec )&=P\left( \ \left|\frac{1}{\sqrt{n}}\F_{ \mathcal{I}}^{T}(\I-\Pmat_{\mathcal{A}})\evec +  \lambda_n \C_{\mathcal{I}\mathcal{A}}\C_{\mathcal{A}\mathcal{A}}^{-1}\zvec_{\mathcal{A}} \right| \leq \lambda_n \onevec \right)\\
     &=P\left( \ \left|\frac{1}{\sqrt{n}}\F_{ \mathcal{I}}^{T}(\I-\Pmat_{\mathcal{A}})(-\evec) +  \lambda_n \C_{\mathcal{I}\mathcal{A}}\C_{\mathcal{A}\mathcal{A}}^{-1}(-\zvec_{\mathcal{A}}) \right| \leq \lambda_n \onevec \right)=P(I_\lambda \, | \, \F , -\zvec )
     %P\left(\left|\frac{1}{\sqrt{n}}\F_{ \tilde{I}}^T(\I-\Pmat_{\tilde{A}})\evec +  \lambda_n \F_{\tilde{I}}^T\F_{\tilde{A}}(\F_{\tilde{A}}^T\F_{\tilde{A}})^{-1}(-\tilde{\zvec}_{\tilde{A}}) \right| \leq \lambda_n \onevec\right)\\&=P\left(\left|-\frac{1}{\sqrt{n}}\F_{ \tilde{I}}^T(\I-\Pmat_{\tilde{A}})\evec -  \lambda_n \F_{\tilde{I}}^T\F_{\tilde{A}}(\F_{\tilde{A}}^T\F_{\tilde{A}})^{-1}\tilde{\zvec}_{\tilde{A}} \right| \leq \lambda_n \onevec\right)\\
     %&=P\left(\left|\frac{1}{\sqrt{n}}\F_{ \tilde{I}}^T(\I-\Pmat_{\tilde{A}})\evec +  \lambda_n \F_{\tilde{I}}^T\F_{\tilde{A}}(\F_{\tilde{A}}^T\F_{\tilde{A}})^{-1}\tilde{\zvec}_{\tilde{A}} \right| \leq \lambda_n \onevec\right)=P(\mathcal{I}(\tilde{\zvec}_A))\ .\
     \end{align*}
     \noindent
    From a similar argument and the identities $\Z_\mathcal{A}\V_\mathcal{A}^{1/2}\betavec_\mathcal{A}=-\Z_\mathcal{A}\V_\mathcal{A}^{1/2}(-\betavec_\mathcal{A})$ and $\Z_{\mathcal{A}}\C_\mathcal{A}^{-1}\zvec_{\mathcal{A}}^T=(-\Z_{\mathcal{A}})\C_\mathcal{A}^{-1}(-\zvec_{\mathcal{A}})^T$, we see that $P(S_\lambda \, | \, \F_\mathcal{A}, \, \betavec_{\mathcal{A}})= P(S_\lambda \, | \, \F_\mathcal{A}, \, -\betavec_{\mathcal{A}})$. Thus, $\phi_\lambda(\X \, | \, \betavec)=\phi_\lambda(\X \, | \, -\betavec)$.   %Equation~\eqref{eq:PSign} follows from the zero covariance, and hence independence, of the Normal random vectors involved in the events \eqref{eq:SignCheckUnscale} and \eqref{eq:InactiveCheck2}. Lastly, \eqref{eq:PSignFlip} follows  equations \eqref{eq:Isym} through \eqref{eq:PSign}.
    
    \subsection{Proof of Theorem 1}
    
    
Let $\Fmat^*$ be the centered and scaled version of $\X^*$ and write $\widetilde{\Z} = \text{Diag}(
\widetilde{\Z}_{\mathcal{A}}, \widetilde{\Z}_{\mathcal{I}})$. Then $\X^*\widetilde{\Z} = 
(\X^*_{\mathcal{A}}\widetilde{\Z}_{\mathcal{A}} \, | \, \X^*_{\mathcal{I}}\widetilde{\Z}_{\mathcal{I}})$.
From Lemma 1, if this theorem holds for $\widetilde{\Z}_{\mathcal{A}}$, then it will hold for $-\widetilde{\Z}_{\mathcal{A}}$. Thus, this only needs to be proven for $\widetilde{\Z}_{\mathcal{A}}$. Also, note that, since post multiplying by $\widetilde{\Z}$ only changes the sign of the columns of $\X^*$, the centered and scaled version of $\X^*\widetilde{\Z}$ is $\Fmat^*\widetilde{\Z}$. Theorem 1 is proven if 
%Note that we can always rearrange $\zvec$ into $(\zvec_A^T, \zvec_I^T)^T$, and, since $\C$ is compound symmetric, its subblock matrices are the same regrardless of choice of $A$ (as long as $A$ is of size $k^*$). Therefore, this result does not depend on knowledge of the specific $A$, only that $\zvec_A$ is known and $k^*$ is known and fixed. Thus, we will prove this by showing, for a fixed $A$, when $\C$ is compound symmetric
\begin{equation*}
    \begin{split}
        I_\lambda \, | \, \Fmat^*, \, \tilde{\zvec} &= I_\lambda \, | \, \Fmat^*\widetilde{\Z}, \, \onevec\\
        S_\lambda \, | \, \Fmat^*_{\mathcal{A}}, \, \betavec_{\mathcal{A}} &= S_\lambda \, | \, \Fmat^*_{\mathcal{A}}\widetilde{\Z}_{\mathcal{A}}, \, |\betavec_{\mathcal{A}}|
    \end{split}
\end{equation*}
%First, note that, if $\F^*\widetilde{\Z}$ were the design, $\widetilde{\Z}\C^*\widetilde{\Z}$ is the information matrix. Then we can see that:
Denoting
\begin{equation*}
    \begin{split}
        \C^*&= \begin{bmatrix}
        \C^*_{\mathcal{A}} & \C^*_{\mathcal{A}\mathcal{I}}\\
        \C^*_{\mathcal{I}\mathcal{A}}& \C^*_{\mathcal{I}}\\
        \end{bmatrix}\\
        \widetilde{\Z}\C^*\widetilde{\Z}&= \begin{bmatrix}
        \widetilde{\Z}_{\mathcal{A}}\C_{\mathcal{A}}\widetilde{\Z}_{\mathcal{A}} & \widetilde{\Z}_{\mathcal{A}}\C_{\mathcal{A}\mathcal{I}}\widetilde{\Z}_{\mathcal{I}}\\
        \widetilde{\Z}_{\mathcal{I}}\C_{\mathcal{I}\mathcal{A}}\widetilde{\Z}_{\mathcal{A}}& \widetilde{\Z}_{\mathcal{I}}\C_{\mathcal{I}}\widetilde{\Z}_{\mathcal{I}}\\
        \end{bmatrix}\ ,\
    \end{split}
\end{equation*}
the $I_{\lambda}$ event may be written as
\begin{equation*}
    \begin{split}
        I_\lambda \, | \, \F^*, \, \tilde{\zvec} &=\left\{ \ \left|\frac{1}{\sqrt{n}}\F_{ \mathcal{I}}^{*T}(\I-\Pmat_{\mathcal{A}})\evec +  \lambda_n \C^*_{\mathcal{I}\mathcal{A}}\C_{\mathcal{A}}^{*-1}\tilde{\zvec}_{\mathcal{A}} \right| \leq \lambda_n \onevec \ \right\} \,\
    \end{split}
\end{equation*}
where $\Pmat_{\mathcal{A}}=\F_\mathcal{A}^*(\F_\mathcal{A}^{*T}\F_\mathcal{A}^{*})^{-1}\F_\mathcal{A}^{*T}$. Consider the event
\begin{equation*}
    \begin{split}
        I_\lambda \, | \, \F^*\widetilde{\Z}, \, \onevec &=\left\{ \ \left|\frac{1}{\sqrt{n}}\widetilde{\Z}_{\mathcal{I}}\F_{\mathcal{I}}^{*T}(\I-\Pmat_{\mathcal{A}})\evec +  \lambda_n \widetilde{\Z}_{\mathcal{I}}\C^*_{\mathcal{I}\mathcal{A}}\widetilde{\Z}_{\mathcal{A}}\widetilde{\Z}_{\mathcal{A}}\C_{\mathcal{A}}^{*-1}\widetilde{\Z}_{\mathcal{A}}\onevec \right| \leq \lambda_n \onevec \ \right\} \\
        &=\left\{ \ \left|\widetilde{\Z}_{\mathcal{I}}\left[\frac{1}{\sqrt{n}}\F_{\mathcal{I}}^{T}(\I-\Pmat_{\mathcal{A}})\evec +  \lambda_n \C^*_{\mathcal{I}\mathcal{A}}\C_{\mathcal{A}}^{*-1}\zvec_{\mathcal{A}}\right] \right| \leq \lambda_n \onevec \ \right\}\ .\
    \end{split}
\end{equation*}
%We can do some simplifications to the above expression using:
%\begin{equation*}
%    \begin{split}
%        \widetilde{\Z}^{-1} &=\widetilde{\Z}\\
%        \widetilde{\Z}\widetilde{\Z} & =\I\\
%        \Pmat_{\mathcal{A}}^* &= \F_{\mathcal{A}}\Z_{\mathcal{A}}(\Z_{\mathcal{A}}\F_{\mathcal{A}}^T\F_{\mathcal{A}}\Z_{\mathcal{A}})^{-1}\Z_{\mathcal{A}}\F_{\mathcal{A}}^T\\
%        &= \F_{\mathcal{A}}\Z_{\mathcal{A}}\Z_{\mathcal{A}}(\F_{\mathcal{A}}^T\F_{\mathcal{A}})^{-1}\Z_{\mathcal{A}}\Z_{\mathcal{A}}\F_{\mathcal{A}}^T\\
%        &=\F_{\mathcal{A}}(\F_{\mathcal{A}}^T\F_{\mathcal{A}})^{-1}\F_{\mathcal{A}}^T\\
%        &= \Pmat_{\mathcal{A}}
%    \end{split}
%\end{equation*}

%By utilizing the simplifications, we can write:
%\begin{equation*}
    %\begin{split}
        %I_\lambda \, | \, \F^*\widetilde{\Z}, \, \onevec &=\left\{ \ \left|\widetilde{\Z}_{\mathcal{I}}\left[\frac{1}{\sqrt{n}}\F_{\mathcal{I}}^{T}(\I-\Pmat_{\mathcal{A}})\evec +  \lambda_n \C^*_{\mathcal{I}\mathcal{A}}\C_{\mathcal{A}\mathcal{A}}^{*-1}\zvec_{\mathcal{A}}\right] \right| \leq \lambda_n \onevec \ \right\} \\
    %\end{split}
%\end{equation*}
\noindent
The expression inside the absolute value in $I_\lambda \, | \, \F^*\widetilde{\Z}, \, \onevec$ is simply the product of $\widetilde{\Z}_{\mathcal{I}}$ and the expression inside the absolute value in $I_\lambda \, | \, \F^*, \, \tilde{\zvec}$. Since $\widetilde{\Z}_{\mathcal{I}}$ is diagonal with $\pm 1$ on the diagonal, the absolute value of the expressions are equal, meaning that we can conclude that $I_\lambda \, | \, \F^*, \, \tilde{\zvec} = I_\lambda \, | \, \F^*\widetilde{\Z}, \, \onevec$. 

For the $S_{\lambda}$ event, note it depends on $\C^*_{\mathcal{A}}$ alone and 
\begin{equation*}
    S_\lambda \, | \, \F^*_{\mathcal{A}}, \, \betavec_{\mathcal{A}}
    =\left\{  -\frac{1}{\sqrt{n}}\widetilde{\Z}_{\mathcal{A}}\C_{\mathcal{A}}^{*-1} \F_{\mathcal{A}}^{*T}\evec < \sqrt{n} \widetilde{\Z}_{\mathcal{A}}\V^{1/2}\betavec_{\mathcal{A}} - \lambda_n\widetilde{\Z}_\mathcal{A}\C_{\mathcal{A}}^{*-1}\tilde{\zvec}_\mathcal{A}\right\} \ .\ 
    %\, \text{Diag}\left(\C_{\mathcal{A}\mathcal{A}}^{*-1}\tilde{\zvec}_{\mathcal{A}}\tilde{\zvec}_{\mathcal{A}}^T\right) \right\}\ 
\end{equation*}
Consider the event
\begin{equation*}
       S_\lambda \, | \, \F^*_{\mathcal{A}}\widetilde{\Z}_{\mathcal{A}}, \, |\betavec_{\mathcal{A}}|=\left\{  -\frac{1}{\sqrt{n}}\widetilde{\Z}_{\mathcal{A}}\C_{\mathcal{A}}^{* \, -1}\widetilde{\Z}_{\mathcal{A}}\widetilde{\Z}_{\mathcal{A}} \F_{\mathcal{A}}^{*T}\evec < \sqrt{n} \V^{1/2}|\betavec_{\mathcal{A}}| - \lambda_n\widetilde{\Z}_{\mathcal{A}}\C_{\mathcal{A}}^{*\, -1}\widetilde{\Z}_{\mathcal{A}}\onevec 
       \right\} \ ,\ %\text{Diag}\left(\widetilde{\Z}_{\mathcal{A}}\C^*_{\mathcal{A}}^{-1}\widetilde{\Z}_{\mathcal{A}}\J\right) \right\}\ 
\end{equation*}
and, noting that
\begin{equation*}
    \begin{split}
        -\frac{1}{\sqrt{n}}\widetilde{\Z}_{\mathcal{A}}\C_{\mathcal{A}}^{* \, -1}\widetilde{\Z}_{\mathcal{A}}\widetilde{\Z}_{\mathcal{A}} \F_{\mathcal{A}}^{*T}\evec&=-\frac{1}{\sqrt{n}}\widetilde{\Z}_{\mathcal{A}}\C_{\mathcal{A}}^{* \, -1} \F_{\mathcal{A}}^{*T}\evec\\
        \V^{1/2}|\betavec_{\mathcal{A}}| &= \widetilde{\Z}_{\mathcal{A}}\V^{1/2}\betavec_{\mathcal{A}}\\
        \lambda_n\widetilde{\Z}_{\mathcal{A}}\C_{\mathcal{A}}^{* \, -1}\widetilde{\Z}_{\mathcal{A}}\onevec & = \lambda_n\widetilde{\Z}_{\mathcal{A}}\C_{\mathcal{A}}^{* \, -1}\tilde{\zvec}_\mathcal{A} \ ,\ \\ 
    \end{split}
\end{equation*}
we have $ S_\lambda \, | \, \F^*_{\mathcal{A}}, \, \betavec_{\mathcal{A}} = S_\lambda \, | \, \F^*_{\mathcal{A}}\widetilde{\Z}_{\mathcal{A}}, \, |\betavec_{\mathcal{A}}|$. This completes the proof.

%So, we have proven
%\begin{align}
%    I_\lambda \, | \, \F^* , \tilde{\zvec} &= I_\lambda \, | \, \F^*\widetilde{\Z} , \onevec\\
%    S_\lambda \, | \, \F^*_\mathcal{A}, \, \betavec_{\mathcal{A}} &=
%    S_\lambda \, | \, \F^*_\mathcal{A}\widetilde{\Z}_\mathcal{A}, \, |\betavec_{\mathcal{A}}|\label{eq:SignCheck3}\ .\
%\end{align}

%Thus, Theorem 1 is proven. 

\subsection{Proof of Corollary 1}

This follows directly from Lemma 1. Since $\phi_\lambda(\X \, | \, \betavec)=\phi_\lambda(\X \, | \, -\betavec)$, the $\phi_\lambda ^{\pm}$-criterion does not need to consider both $\tilde{\zvec}$ and $-\tilde{\zvec}$. Thus, only the sign vectors in $\mathcal{Z}_\mathcal{A}^\pm$ need to be considered in $\phi_\lambda ^{\pm}$. 

\subsection{Proof of Proposition 1}
For any design $\X_1=(\X_{\mathcal{A}} \, | \, \X_{\mathcal{I}})$ consider the alternative design $\X_2=(\X_{\mathcal{A}} \, | \, (\I-\Pmat_1)\X_{\mathcal{I}})$, with obvious definitions for their model matrices $\F_1=(\F_\mathcal{A} | \F_\mathcal{I})$ and $\F_2=(\F_\mathcal{A} | \zerovec)$, respectively. Clearly $P(I_\lambda \, | \, \F_2 , \zvec )=1$ as the left hand side of the event's inequality equals $\zerovec$.  Then
\[
\phi_\lambda(\X_1 \, | \, \betavec)= P(S_\lambda \, | \, \F_\mathcal{A}, \, \betavec_{\mathcal{A}}) \times P(I_\lambda \, | \, \F_1 , \zvec ) < P(S_\lambda \, | \,\F_\mathcal{A}, \betavec_\mathcal{A}) = \phi_\lambda(\X_2 \, | \, \betavec)\ .\
\]
This completes the proof.

\subsection{Proof of Lemma 2}

The distribution details of random normal vectors $\uvec$ and $\vvec$ are analogues of the random normal vectors in equations (4) and (5) of the main document, respectively, when $\C = (1-c)\I + c\J$ for $-(k-1)^{-1}<c<1$. Additionally, since $\V = \I$, the right-hand side of equation (4) becomes $\sqrt{n}\Z_\mathcal{A}\betavec_\mathcal{A} = \sqrt{n}|\betavec_\mathcal{A}|$. Also, $\C_\mathcal{A}$, $\C_\mathcal{I}$, $\C_{\mathcal{A}\mathcal{I}}$, and $\C_{\mathcal{I}\mathcal{A}}$ are the same for any $\mathcal{A}$. Thus, if $\betavec_\mathcal{A}$ does not depend on $\mathcal{A}$ (e.g., $\betavec_\mathcal{A} = \beta\onevec$), then the probability of the two events are constant for any $\mathcal{A}$.

\subsection{Proof of Lemma 3}
We will prove this lemma in two parts. First we will show that $\frac{d}{dc}P(I_\lambda | c, \zvec_\mathcal{A})\big|_{c=0} = 0$ for any $\zvec_\mathcal{A} \in \mathcal{Z}_\mathcal{A}$, , which includes $\zvec_\mathcal{A} =\onevec$. Then we derive conditions where $\frac{d}{dc} P(S_\lambda | c, \betavec_\mathcal{A}=\beta\onevec)\big|_{c=0} > 0$. 

\subsubsection{Proving $\frac{d}{dc} P(I_\lambda | c, \zvec_\mathcal{A})\big|_{c=0} = 0$ for all $\lambda$ and all $\zvec_\mathcal{A}$}
For an arbitrary sign vector $\zvec_A$, let $\boldsymbol{v}\sim N\left(\lambda_nz_\mathcal{A}\gamma\onevec,(1-c)\left[\I + \gamma\J\right] \right)$ with pdf $f(\vvec,c,\zvec_{\mathcal{A}})$, where $z_\mathcal{A}= \onevec^T\zvec_\mathcal{A}$ and $\lambda_n =\sqrt{n}\lambda$. Recall that 
$$
P(I_\lambda \, | \, c, \zvec_\mathcal{A}) = P(|\boldsymbol{v}| \leq \lambda_n \onevec) = P(-\lambda_n \onevec\leq \boldsymbol{v} \leq \lambda_n \onevec) \ .\
$$



\noindent
We will use Lebnitz's rule iteratively over the $q$ dimensional integral. Note that, since $c$ is not in the bounds of the probability, using Lebnitz's rule is quite simple:

\begin{equation}
    \frac{d}{dc}P(I_\lambda \, | \, c, \zvec_\mathcal{A})\rvert_{c=0} = \int_{-\lambda_n \onevec}^{\lambda_n \onevec}\frac{d}{dc}\left[f(\vvec,c,\zvec_\mathcal{A})\right]d\vvec\big |_{c=0} \ .\
\end{equation}

\noindent
 When $c=0$, $\boldsymbol{v}\sim N(0,\I_q)$ which does not depend on $\zvec_\mathcal{A}$, so $f(\boldsymbol{v},c=0, \zvec_\mathcal{A})= \prod_{i=1}^q g(v_i)$ where $g(v_i)$ is the standard normal PDF. For simplicity, denote $f(\boldsymbol{v},c=0, \zvec_\mathcal{A})$ with $f(\vvec,0)$. After taking the derivative and simplifying, we see that 
 \begin{equation}
     \begin{split}
         \frac{d}{dc}P(I_\lambda \, | \, c, \zvec_\mathcal{A})\rvert_{c=0}& =  -\frac{1}{2}\int_{-\lambda_n \onevec}^{\lambda_n \onevec}\boldsymbol{v}^T(\I_q-\J_q)\boldsymbol{v}f_v(\boldsymbol{v},0)d\boldsymbol{v}\\
         & \qquad +\lambda_n z \int_{-\lambda_n \onevec}^{\lambda_n \onevec} \boldsymbol{v}^T\onevec f_v(\boldsymbol{v},0)d\boldsymbol{v}\ .\
     \end{split}
 \end{equation}
 
Intuitively, $\lambda_n z \int_{-\lambda_n \onevec}^{\lambda_n \onevec} \boldsymbol{v}^T\onevec f(\boldsymbol{v},0)d\boldsymbol{v}$ is the symmetric integral of an odd function centered at 0, so it evaluates to zero. Additionally, note $\boldsymbol{v}^T(\I_q-\J_q)\boldsymbol{v}=-2 \sum_{i<j}v_iv_j$, so
 \begin{equation}
     \begin{split}
         \int_{-\lambda_n \onevec}^{\lambda_n \onevec}\boldsymbol{v}^T(\I_q-\J_q)\boldsymbol{v}f_v(\boldsymbol{v},0)d\boldsymbol{v} & = \int_{-\lambda_n \onevec}^{\lambda_n \onevec}-2 \sum_{i<j}v_iv_j f_v(\boldsymbol{v},0)d\boldsymbol{v}\\
         &= -2\sum_{i<j}\int_{-\lambda_n \onevec}^{\lambda_n \onevec}v_iv_j f_v(\boldsymbol{v},0)d\boldsymbol{v}\\
         &\varpropto -2\sum_{i<j}\int_{-\lambda_n}^{\lambda_n}\int_{-\lambda_n}^{\lambda_n} v_i v_j \exp\left(-\frac{1}{2}v_i^2-\frac{1}{2}v_j^2\right)dv_idv_j\\
         &= -2\sum_{i<j}\int_{-\lambda_n}^{\lambda_n} v_je^{-1/2v_j^2} \bigg(\int_{-\lambda_n}^{\lambda_n} v_i e^{-\frac{1}{2}v_i^2}dv_i\bigg)dv_j\\
         & = 0\ ,\
     \end{split}
 \end{equation}
 since $\int_{-\lambda_n}^{\lambda_n} v_i e^{-\frac{1}{2}v_i^2}dv_i=0$. So for an arbitrary sign vector $\zvec_A$, $\frac{d}{dc}P(I_\lambda \, | \, c, \zvec_\mathcal{A})\rvert_{c=0}=0$.





\subsubsection{Proving $\frac{d}{dc} P(S_\lambda | c, \betavec_\mathcal{A}=\beta\onevec)\big|_{c=0} > 0$}
First, when $\zvec_\mathcal{A}=\onevec$ we can rewrite equation (4) in the main document by subtracting the mean:
\begin{equation}\label{eqn:Prob_S}
 P\left( \boldsymbol{u}^*< \sqrt{n} \left[\beta - \frac{\lambda}{1+c(k-1)}\right]\onevec\right) \ ,\
\end{equation}
\noindent
where $\uvec^* \sim N\left( \zerovec, 1/(1-c)\left[ \I - \gamma \J\right]\right)$ with pdf $f(\uvec^*,c)$. The probability in (7) can be written as the following integral
\begin{equation} \label{eqn:Prob_S_integ}
    \lim_{a\to -\infty}\bigg\{\int_{a}^{b(c)}\underset{k \text{ times}}{....}\int_{a}^{b(c)} f(\uvec^*,c)du_k... du_1\bigg\} \,\
\end{equation}
\noindent
where, $b(c) = \sqrt{n} \left[\beta - \frac{\lambda}{1+c(k-1)}\right]$ and $\uvec^* = (u_1,u_2,\cdot \cdot \cdot u_{k-1},u_k)^T$. So then the derivative of the $S_\lambda$ event with respect to $c$ can be written as:

\begin{align}
    \frac{d}{dc}[P(S_\lambda \, | \, c, \beta)] & = \frac{d}{dc}\left[\lim_{a\to -\infty}\bigg\{\int_{a}^{b(c)}\underset{k \text{ times}}{....}\int_{a}^{b(c)} f(\uvec^*,c)du_k... du_1\bigg\}\right] \label{eq:PS_der_of_lim} \,\ \\
    & = \lim_{a\to -\infty}\frac{d}{dc}\bigg\{\int_{a}^{b(c)}\underset{k \text{ times}}{....}\int_{a}^{b(c)} f(\uvec^*,c)du_k... du_1\bigg\} \label{eq:PS_limit_of_der} \ .\ 
\end{align}

Utilizing an iterated application of the Liebnitz's Integral Rule and the fact that $f(\uvec^*, c=0)$ is the pdf of a multivariate independent standard normal vector , \eqref{eq:PS_limit_of_der} evaluated at $c=0$ becomes
\begin{equation}\label{eq:integral_simp}
    \begin{split}
        %\lim_{a\to -\infty}\frac{d}{dc}\bigg\{\int_{a}^{b(c)}\underset{k \text{ times}}{....}&\int_{a}^{b(c)} f(\uvec^*,c)du_k... du_1\bigg\}\bigg \rvert_{c=0} = \\
         \lambda_n k(k-1)G(\tau_n)^{k-1}g(\tau_n)
         + \bigg\{\int_{-\infty}^{b(c)}\underset{k}{...}\int_{-\infty}^{b(c)}\bigg( \frac{d}{dc} f(\uvec^*,c)\bigg)du_k... du_2du_1\bigg \}\bigg\rvert_{c=0} \ ,\ \\
    \end{split}
\end{equation}
 where $g(\cdot)$ and $G(\cdot)$ represent the univariate standard normal pdf and CDF, respectively, with $\lambda_n=\lambda\sqrt{n}$ and $\tau_n = \sqrt{n}(\beta-\lambda)$. After much tedious calculus, we see that:
 
 \begin{equation}
\begin{split}
     \frac{d}{dc} f(\uvec^*,c)\rvert_{c=0}
    &= \frac{d}{dc} \left\{\frac{1}{(2\pi)^{k/2}|\C_{\mathcal{A}}|^{-1/2}} \exp\left(-\frac{1}{2} \uvec^{*\, T} ((1-c)\I + c\J)\uvec^* \right) 
    \right\} \bigg\rvert_{c=0}\\
    & = \frac{1}{2} \uvec^{* \, T}(\I-\J)\uvec^* f(\uvec^*,0)\\
    &= \frac{1}{2} \uvec^{* \, T}(\I-\J)\uvec^* \prod_{i=1}^k g(u_i)\\
    &= -\sum_{i<j} \left[\big(u_iu_j)\prod_{l=1}^k g(u_l)\right]\\
    & = -\frac{k(k-1)}{2} \left(\prod_{l\neq 1,2}^k g(u_l)\right) u_1g(u_1)u_2g(u_2) \ .\
    \end{split}
\end{equation}
 
Thus, the last term in Equation~\ref{eq:integral_simp} becomes:

\begin{equation}\label{eq:int_S_der}
    \begin{split}
        \bigg\{\int_{-\infty}^{b(c)}\underset{k}{...}&\int_{-\infty}^{b(c)}\bigg( \frac{d}{dc} f(\uvec,c)\bigg)du_k... du_2du_1\bigg \}\bigg\rvert_{c=0}\\
        & =-\frac{k(k-1)}{2} G(\tau_n)^{k-2} \int_{-\infty}^{\tau_n}\int_{-\infty}^{\tau_n}u_1g(u_1)u_2g(u_2)du_1du_2\\
        & = -\frac{k(k-1)}{2}G(\tau_n)^{k-2} \bigg( \frac{\exp\big\{ \frac{-2b(0)^2}{2}\big\}}{2\pi} \bigg)\\
        & =-\frac{k(k-1)}{2}G(\tau_n)^{k-2} g\left(\tau_n\right)^2\ ,\
    \end{split}
\end{equation}
 where we have used the identity $\int xg(x)dx = -g(x) + D$, where $D$ is a constant. Lastly, by plugging \eqref{eq:int_S_der} into \eqref{eq:integral_simp}, it follows
 \begin{equation}
     \begin{split}
 \frac{d}{dc} P(S_\lambda \, | \, c, \beta) \rvert_{c=0} &=
 \lambda_n k(k-1)G(\tau_n)^{k-1}g(\tau_n) \\
 & \qquad - \frac{k(k-1)}{2}G(\tau_n)^{k-2} g\left(\tau_n\right)^2
     \end{split}
 \end{equation}
 \noindent
 Hence $\frac{d}{dc} P(S_\lambda \, | \, c, \beta) \rvert_{c=0} >0$ if and only if
 \begin{equation}
     2\lambda_n > \frac{g\left(\tau_n\right)}{G(\tau_n)}\ .\
 \end{equation} 
 %Thus, Lemma 3 is proven.  $\square$
 \subsection{Proof of Theorem 2}
By the product rule, $\frac{d}{dc} \psi_\lambda(c |k, \beta)\big|_{c=0}$ equals
 \begin{equation*}
      \frac{d}{dc} P(S_\lambda \, | \, c,  \beta ) \big|_{c=0} P(I_\lambda \, | \, c=0, \zvec_{\mathcal{A}}=\onevec) + \frac{d}{dc} P(I_\lambda \, | \, c, \zvec_{\mathcal{A}}=\onevec) \big|_{c=0}P(S_\lambda \, | \, c=0,  \beta) \ .\
 \end{equation*}
 Since $P(S_\lambda \, | \, c=0,  \beta)$ and $P(I_\lambda \, | \, c=0, \zvec_{\mathcal{A}}=\onevec)$ are non-negative and from the results in Lemma 3, $\frac{d}{dc} \psi_\lambda(c |k, \beta)\big|_{c=0} > 0$ when the conditions for Lemma 3 hold. Thus, Theorem 2 is proven. $\square$
 \subsection{Proof of Theorem 3}
This proof will contain two parts. First we show that $\frac{d}{dc} \psi_\lambda^{\pm}(c |k, \beta)\big|_{c=0}=0$. Then we develop the condition for which  $\psi_\lambda^{\pm}(c \, | \, k, \beta)$ is locally concave at $c=0$. When the concavity condition holds, $c=0$ is a local maximum for $\psi_\lambda^{\pm}(c \, | \, k, \beta)$.

\subsubsection{Proving $\frac{d}{dc} \psi_\lambda^{\pm}(c |k, \beta)\big|_{c=0}=0$}
Note that if the individual entries of  $\tilde{\zvec}$, denoted $\tilde{z}_i$, are i.i.d. with probability $0.5$ to $\tilde{z}_i=1$ and $\tilde{z}_i=-1$, we can rewrite $\psi_\lambda^{\pm}(c \, | \, k, \beta)$ as:

\begin{align}
\psi_\lambda^{\pm}(c \, | \, k, \beta)=\mathbb{E}_{\tilde{\zvec}} \left\{P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec}) \times P(I_\lambda \, | \, c, \zvec_\mathcal{A}=\tilde{\zvec})\right\} \label{eqn:CS_EZ}\ .\
\end{align}
\noindent
So, in taking the derivative, it is clear that:
\begin{align}
\frac{d}{dc} \psi_\lambda^{\pm}(c |k, \beta)\big|_{c=0}=\mathbb{E}_{\tilde{\zvec}} \left\{\frac{d}{dc}\left[P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec}) \times P(I_\lambda \, | \, c, \zvec_\mathcal{A}=\tilde{\zvec})\right]\big |_{c=0}\right\} \label{eqn:CS_EZ_der}\ .\
\end{align}

We can rewrite and simplify the derivative on  the right-hand side of (\ref{eqn:CS_EZ_der}) using the product rule and the fact that $\frac{d}{dc} P(I_\lambda \, | \, c, \zvec_\mathcal{A}=\tilde{\zvec}) \big |_{c=0} =0$ for any $\tilde{\zvec}$, we see that: 
\begin{equation}\label{eqn:CS_product_rule_gensign}
    \begin{split}
     &\frac{d}{dc}\left[P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec}) \times P(I_\lambda \, | \, c, \zvec_\mathcal{A}=\tilde{\zvec})\right]\big |_{c=0}\\
     & =\frac{d}{dc}\left[P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec})\right ]\big |_{c=0} P(I_\lambda \, | \, 0, \zvec_\mathcal{A}=\tilde{\zvec}) + 0\times P(S_\lambda \, | \, 0, \betavec_\mathcal{A}=\beta\tilde{\zvec}) \\
     & = \frac{d}{dc}\left[P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec}\right) ]\big |_{c=0} P(I_\lambda \, | \, 0, \zvec_\mathcal{A}=\tilde{\zvec})
    \end{split}
\end{equation}
\noindent
Also note that for $c=0$, the $I_\lambda$ event does not depend on $\tilde{\zvec}$, so $P(I_\lambda \, | \, 0, \zvec_\mathcal{A}=\tilde{\zvec})$ is a constant denoted simply $P(I_\lambda)$. By plugging \eqref{eqn:CS_product_rule_gensign} into \eqref{eqn:CS_EZ_der}, and using the fact that, when $c=0$, the $I_\lambda$ event does not depend on $\tilde{\zvec}$:

\begin{equation}
    \begin{split}
        \frac{d}{dc} \psi_\lambda^{\pm}(c |k, \beta)\big|_{c=0} &= \mathbb{E}_{\tilde{\zvec}} \left\{\frac{d}{dc}\left[P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec}\right) ]\big |_{c=0} P(I_\lambda \, | \, 0, \zvec_\mathcal{A}=\tilde{\zvec})\right\}\\
        &= P(I_\lambda)\mathbb{E}_{\tilde{\zvec}} \left\{\frac{d}{dc}\left[P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec}\right) ]\big |_{c=0}\right\} \ .\ \\
    \end{split}
\end{equation}
Thus, $\frac{d}{dc} \psi_\lambda^{\pm}(c |k, \beta)\big|_{c=0} =0$ if and only if $\mathbb{E}_{\tilde{\zvec}} \left\{\frac{d}{dc}\left[P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec}\right) ]\big |_{c=0}\right\}=0$


When $\zvec_\mathcal{A}=\tilde{\zvec}$ we can rewrite $P(S_\lambda \, | \, c, \betavec_\mathcal{A})$ by subtracting the mean as:

\begin{equation}\label{eqn:Prob_S_gen_sign}
 P\left( \boldsymbol{u}< \sqrt{n} \left\{\beta\onevec - \frac{\lambda}{1-c}\left[\onevec - \tilde{z}^*\gamma\tilde{\zvec} \right]\right\}\right) \ ,\
\end{equation}
where $\uvec \sim N\left( \zerovec,\frac{1}{1-c} \left[\I_{k} - \gamma\tilde{\zvec}\tilde{\zvec}^T\right]\right)$ and $\tilde{z}^*=\onevec^T\tilde{\zvec}$. Let $f(\uvec,c, \tilde{\zvec})$ be the corresponding pdf. Thus, \eqref{eqn:Prob_S_gen_sign} can be expressed as

\begin{equation} \label{eqn:Prob_S_integ_gen_sign}
    \lim_{a\to -\infty}\bigg\{\int_{a}^{b_1(c)}\underset{k \text{ times}}{....}\int_{a}^{b_k(c)} f(\uvec,c, \tilde{\zvec})du_k... du_1\bigg\} \ ,\
\end{equation}
\noindent
where $b_i(c) = \sqrt{n} \left[\beta - \frac{\lambda}{1-c}\left[1-\tilde{z}^*\gamma\tilde{z}_i \right]\right\}$ and $\uvec^T = (u_1,u_2,\cdot \cdot \cdot u_{k-1},u_k)$. So then the derivative of the $S_\lambda$ event with respect to $c$ can be written as:

\begin{align}
    \frac{d}{dc}[P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec})] & = \frac{d}{dc}\left[\lim_{a\to -\infty}\bigg\{\int_{a}^{b_1(c)}\underset{k \text{ times}}{....}\int_{a}^{b_k(c)} f(\uvec,c, \tilde{\zvec})du_k... du_1\bigg\}\right] \label{eq:PS_der_of_lim_all_signs} \,\ \\
    & = \lim_{a\to -\infty}\frac{d}{dc}\bigg\{\int_{a}^{b_1(c)}\underset{k \text{ times}}{....}\int_{a}^{b_k(c)} f(\uvec,c, \tilde{\zvec})du_k... du_1\bigg\} \label{eq:PS_limit_of_der_allsigns} \ .\ 
\end{align}

Let $\uvec_{-i}^T= (u_1, u_2, \cdot 
\cdot \cdot u_{i-1}, u_{i+1}, \cdot \cdot \cdot u_{k-1},u_k)$ , and let $\uvec_{-i}(b)^T= (u_1, u_2, \cdot 
\cdot \cdot u_{i-1},b_i(c), u_{i+1}, \cdot \cdot \cdot u_{k-1},u_k)$. Lastly let $\boldsymbol{b}_{-i}(c)^T = (b_1(c), b_2(c), \cdot 
\cdot \cdot b_{i-1}(c), b_{i+1}(c), \cdot \cdot \cdot b_{k-1}(c),b_k(c)) $. Using Liebnitz's rule once on $\frac{d}{dc}\bigg\{\int_{a}^{b(c)}\underset{k \text{ times}}{....}\int_{a}^{b(c)} f(\uvec,c, \tilde{\zvec})du_k... du_1\bigg\}$, we see that $\frac{d}{dc}\bigg\{\int_{a}^{b_1(c)}{....}\int_{a}^{b_k(c)} f(\uvec,c, \tilde{\zvec})du_k... du_1\bigg\} \bigg | _{c=0}$
has the following expression:
\begin{equation}\label{eqn:leb_one_step}
    \begin{split}
        &\frac{d}{dc}\left(b_1(c)\right)\int_{a}^{b_2(c)}{....}\int_{a}^{b_k(c)} f(\uvec_{-1}(b),c, \tilde{\zvec})du_k... du_2\bigg | _{c=0}\\
        &\qquad + \int_{a}^{b_1(c)}\frac{d}{dc}\bigg\{\int_{a}^{b_2(c)}{....}\int_{a}^{b_k(c)} f(\uvec,c, \tilde{\zvec})du_k... du_2\bigg\}\bigg|_{c=0}du_1\\
        & = \frac{d}{dc}\left(b_1(c)\right)\big|_{c=0}\int_{a}^{b_2(0)}{....}\int_{a}^{b_k(0)} f(\uvec_{-1}(b),c=0, \tilde{\zvec})du_k... du_2\\
        &\qquad + \int_{a}^{b_1(c)}\frac{d}{dc}\bigg\{\int_{a}^{b_2(c)}{....}\int_{a}^{b_k(c)} f(\uvec,c, \tilde{\zvec})du_k... du_2\bigg\}\bigg|_{c=0}du_1\ .\ \\
    \end{split}
\end{equation}

Note that, $\frac{d}{dc}\left(b_1(c)\right)\big|_{c=0} = \lambda_n\left[\tilde{z}_i\tilde{z}^*-1\right]$. By applying Lebnitz's rule again to the derivative in the last line of (24) and  iterating, we see that \eqref{eq:PS_limit_of_der_allsigns} becomes
\begin{equation}\label{eqn:limit_leb}
    \begin{split}
        \lim_{a\to -\infty}\bigg\{&\sum_{i=1}^{k}\left[\lambda_n\left(\tilde{z}_i\tilde{z}^*-1\right) \int_{-\infty}^{\boldsymbol{b}_i(0)}f(\uvec_{-i}(b), c=0, \tilde{z})d\uvec_{-i}\right]\\
        &+ \int_{a}^{b_1(c)}{....}\int_{a}^{b_k(c)} \frac{d}{dc}\left[f(\uvec,c, \tilde{\zvec})\right]\big |_{c=0}du_k... du_2du_1\bigg\} \ .\ \\
    \end{split}
\end{equation}
\noindent
When $c=0$, $f(\uvec,c=0, \tilde{\zvec})$ is the pdf of $N(\zerovec,\I_k)$ and does not depend on $\tilde{\zvec}$ (and similar for $f(\uvec_{-i},c=0, \tilde{\zvec})$). For brevity, let $f(\uvec,0) = f(\uvec,c=0, \tilde{\zvec})$. Additionally, letting $g(\cdot)$ represent the pdf of the univariate standard normal distribution, it is clear that $f(\uvec_{-i}(b), 0) = g(b(0))f(\uvec_{-i},0)$. Thus, we can simplify by the first term in the the summand of (25) by applying the limit as $a \to -\infty$, giving the expression
\begin{equation}
    \begin{split}
        \frac{d}{dc}[P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec})] & = g(\tau_n)G\left(\tau_n\right)^{k-1}\sum_{i=1}^k \left[\sqrt{n}\left(\tilde{z}_i\tilde{z}^*-1\right)\right]\\
        & \qquad + \int_{a}^{\tau_n}\underset {k \text{ times}}{....}\int_{a}^{\tau_n} \frac{d}{dc}\left[f(\uvec,c, \tilde{\zvec})\right]\big | _{c=0}du_k... du_2du_1 \ .\
    \end{split}
\end{equation}
\noindent
After much tedious calculus, we see that:
 
 \begin{equation}
\begin{split}
     \frac{d}{dc} f((\uvec,c, \tilde{z})\rvert_{c=0}
    &= \frac{d}{dc} \left\{\frac{1}{(2\pi)^{k/2}\left|\frac{1}{1-c}\left(I-\gamma\tilde{\zvec}\tilde{\zvec}^T\right)\right|^{-1/2}} \exp\left[-\frac{1}{2} \uvec^T ((1-c)\I + c\tilde{\zvec}\tilde{\zvec}^T)\uvec\right] \right\}\bigg \rvert_{c=0}\\
    & = \frac{1}{2} \uvec^T(\I-\tilde{\zvec}\tilde{\zvec}^T)\uvec f(\uvec,0)\\
    &= -\sum_{i<j}u_i\tilde{z}_i\tilde{z}_ju_jf(\uvec,0) \ .\
\end{split}
\end{equation}

%Utilizing this and pulling all of this together and 
\noindent
Taking the expectation over equally likely sign vectors, we have
\begin{equation}\label{eqn:Expectation_simple}
    \begin{split}
        \mathbb{E}_{\tilde{\zvec}} \left\{\frac{d}{dc}\left[P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec}\right) ]\big |_{c=0}\right\} = \mathbb{E}_{\tilde{\zvec}} \bigg\{ & g(\tau_n)G\left(\tau_n\right)^{k-1}\sum_{i=1}^k \left[\sqrt{n}\left(\tilde{z}_i\tilde{z}^*-1\right)\right]\\
        & - \int_{a}^{\tau_n}\underset {k \text{ times}}{....}\int_{a}^{\tau_n} \sum_{i<j}u_i\tilde{z}_i\tilde{z}_ju_jf(\uvec,0) du_k... du_2du_1\bigg\} \ .\
    \end{split}
\end{equation}
\noindent
By linearity of expectation, \eqref{eqn:Expectation_simple} becomes:
\begin{equation}\label{eqn:expectation_brokendown}
    \begin{split}
        &\mathbb{E}_{\tilde{\zvec}} \bigg\{ g(\tau_n)G\left(\tau_n\right)^{k-1}\sum_{i=1}^k \left[\sqrt{n}\left(\tilde{z}_i\tilde{z}^*-1\right)\right]\bigg \}\\
        & \qquad -\mathbb{E}_{\tilde{\zvec}} \bigg\{\int_{a}^{\tau_n}\underset {k \text{ times}}{....}\int_{a}^{\tau_n} \sum_{i<j}u_i\tilde{z}_i\tilde{z}_ju_jf(\uvec,0) du_k... du_2du_1\bigg\}\\
        & = g(\tau_n)G\left(\tau_n\right)^{k-1}\sum_{i=1}^k \mathbb{E}_{\tilde{\zvec}}\left \{\left[\sqrt{n}\left(\tilde{z}_i\tilde{z}^*-1\right)\right]\right\}\\
        & \qquad - \int_{a}^{\tau_n}\underset {k \text{ times}}{....}\int_{a}^{\tau_n}f(\uvec,0) \left(\sum_{i<j}u_i\mathbb{E}_{\tilde{\zvec}}[\tilde{z}_i\tilde{z}_j]u_j\right ) du_k... du_2du_1 \ .\
    \end{split}
\end{equation}

Note the following expectation properties of $\tilde{z}_i$ and $\tilde{z}^*=\sum_i \tilde{z}_i$: 
\begin{equation}\label{eqn:Expectation_Facts}
    \begin{split}
    \mathbb{E}_{\tilde{\zvec}}\tilde{z}_i &= 0\\
    \mathbb{E}_{\tilde{\zvec}}(\tilde{z}_i\tilde{z}_j) &= \mathbb{E}_{\tilde{\zvec}}(\tilde{z}_i)\mathbb{E}_{\tilde{\zvec}}(\tilde{z}_j) = 0\\
    \mathbb{E}_{\tilde{\zvec}}(\tilde{z}_i^2) &= 1\\
    \mathbb{E}_{\tilde{\zvec}}(\tilde{z}_i\tilde{z}^*) &=\mathbb{E}_{\tilde{\zvec}}(\tilde{z}_i^2) + \sum_{i \neq j}\mathbb{E}_{\tilde{\zvec}}(\tilde{z}_i\tilde{z}_j) = 1 \ .\
    \end{split}
\end{equation}
Then $\mathbb{E}_{\tilde{\zvec}}\left \{\left[\sqrt{n}\left(\tilde{z}_i\tilde{z}^*-1\right)\right]\right\} = 0$ and $\sum_{i<j}u_i\mathbb{E}_{\tilde{\zvec}}[\tilde{z}_i\tilde{z}_j]u_j =0$, proving that $\mathbb{E}_{\tilde{\zvec}} \left\{\frac{d}{dc}\left[P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec}\right) ]\big |_{c=0}\right\}=0$ which happens if and only if $\frac{d}{dc} \psi_\lambda^{\pm}(c |k, \beta)\big|_{c=0} =0$.

\subsubsection{ Concavity condition for $\psi_\lambda^{\pm}(c |k, \beta)$ at $c=0$}

%Recall that 

%\begin{align}
%\psi_\lambda^{\pm}(c \, | \, k, \beta)=\mathbb{E}_{\tilde{\zvec}} \left\{P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec}) \times P(I_\lambda \, | \, c, \zvec_\mathcal{A}=\tilde{\zvec})\right\} \label{eqn:CS_EZ_2}\ .\
%\end{align}

%So, in taking the derivative, it is clear that:
%First, note that
%\begin{align}
%\frac{d}{dc} \psi_\lambda^{\pm}(c |k, \beta)\big|_{c=0}=\mathbb{E}_{\tilde{\zvec}} \left\{\frac{d}{dc}\left[P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec}) \times P(I_\lambda \, | \, c, \zvec_\mathcal{A}=\tilde{\zvec})\right]\big |_{c=0}\right\} \label{eqn:CS_EZ_der_2}\ .\
%\end{align}
We aim to show what conditions must hold for $\psi_\lambda^{\pm}(c \, | \, k, \beta)$ to be locally concave at $c=0$. Clearly, 
\begin{align}
\frac{d^2}{dc^2} \psi_\lambda^{\pm}(c |k, \beta)\big|_{c=0}=\mathbb{E}_{\tilde{\zvec}} \left\{\frac{d^2}{dc^2}\left[P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec}) \times P(I_\lambda \, | \, c, \zvec_\mathcal{A}=\tilde{\zvec})\right]\big |_{c=0}\right\} \ .\
\end{align}
and the second derivative of the product can be expressed as:
\begin{equation}\label{eq:2nd_der_phi}
    \begin{split}
        \frac{d^2}{dc^2}\left[P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec}) \times P(I_\lambda \, | \, c, \zvec_\mathcal{A}=\tilde{\zvec})\right] \big |_{c=0}&= \frac{d^2}{dc^2}\left\{P(I_\lambda \, | \, c, \zvec_\mathcal{A}=\tilde{\zvec}) \right \}\big |_{c=0}P(S_\lambda \, | \, c=0, \betavec_\mathcal{A}=\beta\tilde{\zvec})\\
        & + 2\frac{d}{dc}\left\{P(I_\lambda \, | \, c, \zvec_\mathcal{A}=\tilde{\zvec}) \right \}\big |_{c=0}\frac{d}{dc}\left\{P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec}) \right \}\big |_{c=0}\\
        &+ \frac{d^2}{dc^2}\left\{P(S_\lambda \, | \, c=0, \betavec_\mathcal{A}=\beta\tilde{\zvec}) \right \}\big |_{c=0}P(I_\lambda \, | \, c=0, \zvec_\mathcal{A}=\tilde{\zvec}) \ .\ \\
    \end{split}
\end{equation}
Since $\frac{d}{dc}\left\{P(I_\lambda \, | \, c, \zvec_\mathcal{A}=\tilde{\zvec}) \right \}\big |_{c=0} = 0$, and both $I_\lambda$ and $S_\lambda$ do not depend on $\tilde{\zvec}$ when$c=0$, it follows from \eqref{eq:2nd_der_phi} that
\begin{equation}\label{eq:simplified_2nd_der_EZ}
    \begin{split}
    \frac{d^2}{dc^2} \psi_\lambda^{\pm}(c |k, \beta)\big|_{c=0}&=\mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{P(I_\lambda \, | \, c, \zvec_\mathcal{A}=\tilde{\zvec}) \right \}\big |_{c=0}P(S_\lambda \, | \, c=0, \betavec_\mathcal{A}=\beta\tilde{\zvec}) \\
    & \qquad+ \frac{d^2}{dc^2}\left\{P(S_\lambda \, | \, c=0, \betavec_\mathcal{A}=\beta\tilde{\zvec}) \right \}\big |_{c=0}P(I_\lambda \, | \, c=0, \zvec_\mathcal{A}=\tilde{\zvec}) \bigg \} \\
    & = \mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{P(I_\lambda \, | \, c, \zvec_\mathcal{A}=\tilde{\zvec}) \right \}\big |_{c=0}\bigg \}P(S_\lambda \, | \, c=0) \\
    & \qquad+ \mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec}) \right \}\big |_{c=0}\bigg \}P(I_\lambda \, | \, c=0) \ .\ \\
    \end{split}
\end{equation}
If \eqref{eq:simplified_2nd_der_EZ} is less than or equal to 0, we have concavity (and thus a local maximum) at $c=0$. We will provide expressions for each of the second derivatives separately, and then combine the expressions to get a condition for concavity to hold.

%\noindent \textbf{Expression for %$\mathbb{E}_{\tilde{\zvec}} 
%\bigg\{\frac{d^2}{dc^2}\left\{P(S_\lambda \, | %\, c, \betavec_\mathcal{A}=\beta\tilde{\zvec}) %\right \}\big |_{c=0}\bigg \}$}




First note that 
\begin{equation}\label{eq:second_der_expect}
    \begin{split}
        \mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec}) \right \}\big |_{c=0}\bigg \}& =  \mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{\int_{-\infty}^{\sqrt{n}|\betavec|}f_u(\uvec,c,\tilde{\zvec}) d\uvec \right \}\bigg |_{c=0}\bigg \}\\
        & = \mathbb{E}_{\tilde{\zvec}} \bigg\{\int_{-\infty}^{\sqrt{n}|\betavec|}\frac{d^2}{dc^2}\left\{f_u(\uvec,c,\tilde{\zvec}) \right \}\bigg |_{c=0}d\uvec\bigg \}\\
        &= \int_{-\infty}^{\sqrt{n}|\betavec|}\mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{f_u(\uvec,c,\tilde{\zvec}) \right \}\big |_{c=0}\bigg \}d\uvec\ ,\
    \end{split}
\end{equation}
 where $f_u(\uvec, c, \tilde{\zvec})$ is the pdf of $N\left(\sqrt{n}\frac{\lambda}{1-c}\left[\onevec - \tilde{z}^*\gamma\tilde{\zvec} \right],\frac{1}{1-c} \left[\I_{k} - \gamma\tilde{\zvec}\tilde{\zvec}^T\right] \right) $. Write 
     \begin{align}
         f_v(\uvec, c, \tilde{\zvec}) &= t(c)\exp\{h(\uvec, c, \tilde{\zvec} \}\\
     t(c) &= \frac{1}{(2\pi)^{k/2}}\left| \frac{1}{1-c} \left[\I_{k} - \gamma\tilde{\zvec}\tilde{\zvec}^T\right]
     \right|^{-1/2}\\
     h(\uvec, c, \tilde{\zvec}) & = -\frac{1}{2}\left(\uvec -\frac{\lambda_n}{1-c}\left[\onevec - \tilde{z}^*\gamma\tilde{\zvec} \right]  \right)^T \left[(1-c)\I_{k} + c\tilde{\zvec}\tilde{\zvec}^T\right] \left(\uvec -\frac{\lambda_n}{1-c}\left[\onevec - \tilde{z}^*\gamma\tilde{\zvec} \right]  \right)\ .\
 \end{align}
Now let $t'(c) \equiv \frac{d}{dc}t(c)$ and $h'(\uvec,c, \tilde{\zvec}) \equiv \frac{d}{dc}h(\uvec,c,\tilde{\zvec})$ and likewise for the second derivative and double prime notation. Then, we can write
\begin{equation}
    \begin{split}
        \frac{d^2}{dc^2}f_u(\uvec, c, \tilde{\zvec}) & = t''(c) \exp \{h(\uvec, c, \tilde{\zvec})\} + 2t'(c)h'(\uvec,c,\tilde{\zvec}) \exp \{h(\uvec, c, \tilde{\zvec})\}\\
        & \qquad+ t(c) \exp \{h(\uvec, c, \tilde{\zvec})\}\left\{ h'(\uvec,c,\tilde{\zvec})^2 + h''(\uvec,c,\tilde{\zvec}) \right\}\\
    \end{split}
\end{equation}
It follows that $t'(0) = 0$, $t(0) = \frac{1}{(2\pi)^{k/2}}$, $t''(0) = -\frac{k(k-1)}{2(2\pi)^{k/2}}$, and 
\begin{equation}
    \begin{split}
        %h(\uvec, c, \tilde{\zvec}) & = -\frac{1}{2}\left[(1-c)\uvec^T\uvec + c\uvec^T\tilde{\zvec}\tilde{\zvec}^T\uvec  -2\lambda_n \onevec^T\uvec + \frac{\lambda^2 n k}{(1-c)} - \frac{\lambda^2 n \tilde{z}^{*\,2} \gamma}{(1-c)}\right]\\
        h'(\uvec, c, \tilde{\zvec}) & = -\frac{1}{2}\left[-\uvec^T\uvec + \uvec^T\tilde{\zvec}\tilde{\zvec}^T\uvec+   \frac{\lambda^2 n k}{(1-c)^2} - \frac{(1+(k-1)c^2)\lambda^2 n \tilde{z}^{*\,2} }{(1-c)^2 (1+c(k-1))^2}\right]\\
        h'(\uvec, c, \tilde{\zvec})|_{c=0} & = -\frac{1}{2}\left[-\uvec^T(I-\tilde{\zvec}\tilde{\zvec}^T) \uvec +   \lambda^2 n k - \lambda^2 n \tilde{z}^{*\,2} \right]\\
        h''(\uvec, c, \tilde{\zvec}) & = --\frac{1}{2}\left\{\frac{2\lambda^2 n k}{(1-c)^3} - \frac{2\left[(k-1)^2 c^3 + 3(k-1) c -k +2 \right]\lambda^2 n \tilde{z}^{*\,2} }{(1-c)^3 (1+c(k-1))^3}\right\}\\
        h''(\uvec, c, \tilde{\zvec})|_{c=0} & = -\lambda^2 n \left[k + \tilde{z}^{*\,2}(k-2) \right]
    \end{split}
\end{equation}
This means that $t''(0)\exp \{h(\uvec, 0, \tilde{\zvec})\} = -\frac{k(k-1)}{2} f_u(\uvec, 0, \tilde{\zvec})$ and %we can simply %( after computing $h'$ and $h''$ 
\begin{equation}
    \begin{split}
        \frac{d^2}{dc^2}f_u(\uvec, c, \tilde{\zvec})\bigg |_{c=0}&= \frac{-(k-1)k}{2} f_u(\uvec, 0, \tilde{\zvec})\\
        & \qquad +f_u(\uvec, 0, \tilde{\zvec})\left\{ \frac{1}{4}\left[-\uvec^T(\I-\tilde{\zvec}\tilde{\zvec}^T)\uvec + \lambda^2n(k-\tilde{z}^{*\,2})\right]^2 - \lambda^2 n \left[k + \tilde{z}^{*\,2}(k-2)\right]\right\}\\
        & = f_u(\uvec, 0, \tilde{\zvec}) \bigg\{-\frac{k(k-1)}{2} +  \frac{1}{4}\left(\uvec^T(\I-\tilde{\zvec}\tilde{\zvec}^T)\uvec\right)^2 -\frac{1}{2}\lambda^2n(k-\tilde{z}^{*\,2})\uvec^T(\I-\tilde{\zvec}\tilde{\zvec}^T)\uvec\\
        & \qquad +\frac{1}{4}\lambda^4n^2(k-\tilde{z}^{*\,2})^2 - \lambda^2 n \left[k + \tilde{z}^{*\,2}(k-2) \right] \bigg\} \ .\
    \end{split}
\end{equation}
%Note that
%$$
%\mathbb{E}_{\tilde{\zvec}} \bigg\{\int_{-\infty}^{\sqrt{n}|\betavec|}\frac{d^2}{dc^2}\left\{f_u(\uvec,c,\tilde{\zvec}) \right \}\bigg |_{c=0}d\uvec\bigg \}
%$$
%\noindent
Since $f_u(\uvec, 0, \tilde{\zvec})$ does not depend on the sign vector, we can rewrite:
\begin{equation}
    \begin{split}
        \mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{f_u(\uvec,c,\tilde{\zvec}) \right \}\big |_{c=0}\bigg \} & = f_u(\uvec, 0, \tilde{\zvec}) \bigg\{\frac{-(k-1)k}{2} \\
        & \qquad + \frac{1}{4}\mathbb{E}_{\tilde{\zvec}}\left[\left(\uvec^T(\I-\tilde{\zvec}\tilde{\zvec}^T)\uvec\right)^2\right] \\
        &\qquad -\frac{1}{2}\lambda^2n\mathbb{E}_{\tilde{\zvec}}\left[(k-\tilde{z}^{*\,2})\uvec^T(\I-\tilde{\zvec}\tilde{\zvec}^T)\uvec\right]\\
        &\qquad +\frac{1}{4}\lambda^4n^2\mathbb{E}_{\tilde{\zvec}}\left[(k-\tilde{z}^{*\,2})^2\right] - \lambda^2 n \left[k + \mathbb{E}_{\tilde{\zvec}}\left[\tilde{z}^{*\,2}\right](k-2) \right]\bigg\}
    \end{split}
\end{equation}
Note that $\uvec^T(\I-\tilde{\zvec}\tilde{\zvec}^T)\uvec = -2\sum_{i<j}u_i\tilde{z}_i\tilde{z}_ju_j$ and consider the following expectations
\begin{equation}\label{eqn:Expectation_Facts_second}
    \begin{split}
    \mathbb{E}_{\tilde{\zvec}}\tilde{z}_i &= 0\\
    \mathbb{E}_{\tilde{\zvec}}(\tilde{z}_i\tilde{z}_j) &= \mathbb{E}_{\tilde{\zvec}}(\tilde{z}_i)\mathbb{E}_{\tilde{\zvec}}(\tilde{z}_j) = 0\\
    \mathbb{E}_{\tilde{\zvec}}\tilde{z}_i^a\tilde{z}_j & =0 \text{ for all } a>0\\
    \mathbb{E}_{\tilde{\zvec}}(\tilde{z}_i^2) &= 1\\
    \mathbb{E}_{\tilde{\zvec}}(\tilde{z}_i\tilde{z}^*) &=\mathbb{E}_{\tilde{\zvec}}(\tilde{z}_i^2) + \sum_{i \neq j}\mathbb{E}_{\tilde{\zvec}}(\tilde{z}_i\tilde{z}_j) = 1\\
    \mathbb{E}_{\tilde{\zvec}}(\tilde{z}^{*\,2}) &=\mathbb{E}_{\tilde{\zvec}}(\sum_{i=1}^k\tilde{z}_i^2) + 2\sum_{i \neq j}\mathbb{E}_{\tilde{\zvec}}(\tilde{z}_i\tilde{z}_j) = k\\
    \mathbb{E}_{\tilde{\zvec}}(\tilde{z}^{*\,4}) &=k +3k(k-1)= 3k^2-2k\\
    \mathbb{E}_{\tilde{\zvec}}[(k-\tilde{z}^{*\,2})^2] &=k^2 - 2k\mathbb{E}_{\tilde{\zvec}}(\tilde{z}^{*\,2}) + \mathbb{E}_{\tilde{\zvec}}(\tilde{z}^{*\,4}) =2k(k-1)\\
    \mathbb{E}_{\tilde{\zvec}}[(k-\tilde{z}^{*\,2})\uvec^T(\I-\tilde{\zvec}\tilde{\zvec}^T)\uvec] &=4 \sum_{i<j}u_iu_j\\
    \mathbb{E}_{\tilde{\zvec}}[(\uvec^T(\I-\tilde{\zvec}\tilde{\zvec}^T)\uvec)^2] &=4 \sum_{i<j}u_i^2u_j^2\\
    \end{split}
\end{equation}
So substituting these in, we see that

\begin{equation}
\begin{split}
    \mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{f_u(\uvec,c,\tilde{\zvec}) \right \}\big |_{c=0}\bigg \} & = f_u(\uvec, 0, \tilde{\zvec}) \left\{\frac{-(k-1)k}{2} - \lambda^2nk(k-1) +\frac{\lambda^4 n^2 k(k-1)}{2}  \right\}\\
        &+ f_u(\uvec, 0, \tilde{\zvec})\bigg\{\sum_{i<j}u_i^2u_j^2 -2\lambda^2n\sum_{i<j}u_iu_j \bigg\}\\
\end{split}
\end{equation}
\noindent
When $c=0$, $f_u(\uvec,c, \tilde{\zvec})= \prod_{i=1}^kf_{u_i}(u_i)$ where $f_{u_i}$ is the pdf for $N(\lambda_n, 1)$. So we can split the densities up after adding in the integration:

\begin{equation}
    \begin{split}
        \int_{-\infty}^{\sqrt{n}|\betavec|}\mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{f_u(\uvec,c,\tilde{\zvec}) \right \}\big |_{c=0}\bigg \}d\uvec & = \left\{\frac{-(k-1)k}{2} - \lambda^2nk(k-1) +\frac{\lambda^4 n^2 k(k-1)}{2} \right\}\int_{-\infty}^{\sqrt{n}|\betavec|}f_u(\uvec, 0, \tilde{\zvec})d\uvec \\
        & \qquad + \int_{-\infty}^{\sqrt{n}|\betavec|}f_u(\uvec, 0, \tilde{\zvec})\sum_{i<j}u_i^2u_j^2d\uvec\\
        &\qquad -2\lambda^2n \int_{-\infty}^{\sqrt{n}|\betavec|}f_u(\uvec, 0, \tilde{\zvec})\sum_{i<j}u_iu_j \ .\
    \end{split}
\end{equation}
\noindent
Writing $\int_{-\infty}^{\sqrt{n}|\betavec|}f_u(\uvec, 0, \tilde{\zvec})d\uvec = G(\tau_n)^k$ where $G(\cdot)$ is the standard normal CDF gives
\begin{equation}
    \begin{split}
        \int_{-\infty}^{\sqrt{n}|\betavec|}\mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{f_u(\uvec,c,\tilde{\zvec}) \right \}\big |_{c=0}\bigg \}d\uvec & = \left\{\frac{-(k-1)k}{2} - \lambda^2nk(k-1) +\frac{\lambda^4 n^2 k(k-1)}{2} \right\} G(\tau_n)^k\\
        &\qquad + G(\tau_n)^{k-2}\frac{k(k-1)}{2}\left\{\int_{-\infty}^{\sqrt{n}\beta}u_i^2f_{u_i}(u_i)\right\}^2\\
        &\qquad -2\lambda^2n G(\tau_n)^{k-2}\frac{k(k-1)}{2} \left\{\int_{-\infty}^{\sqrt{n}\beta}u_if_{u_i}(u_i)\right\}^2\ .\
    \end{split}
\end{equation}
We can express $\int_{-\infty}^{\sqrt{n}|\betavec|}\mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{f_u(\uvec,c,\tilde{\zvec}) \right \}\big |_{c=0}\bigg \}d\uvec$ as: 
\begin{equation}
    \begin{split}
       G(\tau_n)^{k-2}\bigg \{&  \left(\frac{-(k-1)k}{2} - \lambda^2nk(k-1) +\frac{\lambda^4 n^2 k(k-1)}{2} \right) G(\tau_n)^2 \\
       &+ \frac{k(k-1)}{2}\left\{\int_{-\infty}^{\sqrt{n}\beta}u_i^2f_{u_i}(u_i)\right\}^2 -2\lambda^2n \frac{k(k-1)}{2} \left\{\int_{-\infty}^{\sqrt{n}\beta}u_if_{u_i}(u_i)\right\}^2 \bigg\} \ .\ \\
    \end{split}
\end{equation}
Note that 
\begin{equation}
    \int_{-\infty}^{\sqrt{n}\beta}u_if_{u_i}(u_i) = g(\tau_n) + \lambda_nG(\tau_n) \ ,\
\end{equation}
and (47) squared is
\begin{equation}
    \begin{split}
      \left\{\int_{-\infty}^{\sqrt{n}\beta}u_if_{u_i}(u_i)\right\}^2  = g(\tau_n)^2-2\lambda_n g(\tau_n)G(\tau_n) + \lambda_n^2 G(\tau_n)^2\ .\
    \end{split}
\end{equation}
By substituting $v=u_i-\lambda_n$ and by the fact that, for a standard normal density $g(\cdot)$, that $\int v^2g(v)dv =G(v)-vg(v)+D$ for some constant $D$,
\begin{equation*}
    \begin{split}
       \int_{-\infty}^{\sqrt{n}\beta}u_i^2f_{u_i}(u_i)du_i & = \int_{-\infty}^{\tau_n}v^2g(v)dv
       + 2\lambda_n \int_{-\infty}^{\tau_n}vg(v)dv
       + \lambda^2 n \int_{-\infty}^{\tau_n}g(v)dv\\
       &= (1+\lambda_n^2)G(\tau_n) -\sqrt{n}(\beta+\lambda)g(\tau_n)\ .\
    \end{split}
\end{equation*}
We then get the expression for $\mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{P(S_\lambda \, | \, c, \betavec_\mathcal{A}\beta\tilde{\zvec}) \right \}\big |_{c=0}\bigg \}$: %that holds if and only if $\int_{-\infty}^{\sqrt{n}|\betavec|}\mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{f_u(\uvec,c,\tilde{\zvec}) \right \}\big |_{c=0}\bigg \}d\uvec \leq 0$. This condition is:
\begin{equation}\label{eqn:simplified_condition_2nd_der_PS}
\begin{split}
      \sqrt{n} k(k-1) g(\lambda_n) G(\tau_n)^{k-2} &\bigg\{-\left\{ (\beta +\lambda) + \lambda^2 n (\beta-\lambda) \right\}G(\tau_n)
    + \sqrt{n}\left\{ \frac{\beta^2-\lambda^2}{2} + \beta\lambda\right\}g(\tau_n)\bigg\}\ .\ 
\end{split}
\end{equation}


%\noindent \textbf{Expression for $\mathbb{E}_{\tilde{\zvec}} 
%\bigg\{\frac{d^2}{dc^2}\left\{P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec}) \right \}\big |_{c=0}\bigg \}$}

Now we turn to the $I_\lambda$ event. Again, note that
\begin{equation}\label{eq:I_second_der_expect}
    \begin{split}
        \mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{P(I_\lambda \, | \, c, \zvec_\mathcal{A}=\tilde{\zvec}) \right \}\big |_{c=0}\bigg \}& =  \mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{\int_{-\lambda_n \onevec}^{\lambda_n \onevec}f_v(\vvec,c,\tilde{\zvec}) d\vvec \right \}\bigg |_{c=0}\bigg \}\\
        & = \mathbb{E}_{\tilde{\zvec}} \bigg\{\int_{-\lambda_n \onevec}^{\lambda_n \onevec}\frac{d^2}{dc^2}\left\{f_v(\vvec,c,\tilde{\zvec}) \right \}\bigg |_{c=0}d\vvec\bigg \}\\
        & =  \int_{-\lambda_n \onevec}^{\lambda_n \onevec}\mathbb{E}_{\tilde{\zvec}} \left\{\frac{d^2}{dc^2}\left\{f_v(\vvec,c,\tilde{\zvec}) \right \}\bigg |_{c=0}\right\}d\vvec\\
    \end{split}
\end{equation}
 where $f_v(\vvec, c, \tilde{\zvec})$ is the pdf of $N\left(\lambda_n \tilde{z}\gamma \onevec,(1-c) \left[\I_{q} + \gamma\J_q\right] \right) $ for $\tilde{z}=\onevec^T\tilde{\zvec}$. Again, we write 
 \begin{align}
         f_v(\vvec, c, \tilde{\zvec}) &= t(c)\exp\{h(\vvec, c, \tilde{\zvec}) \} \ ,\ \\
     t(c) &= \frac{1}{(2\pi)^{q/2}}\left| (1-c) \left( \I_{q} + \gamma\J_q\right)
     \right|^{-1/2}\\
     h(\vvec, c, \tilde{\zvec}) & = -\frac{1}{2(1-c)}\left(\vvec -\lambda_n \tilde{z}\gamma \onevec  \right)^T \left[\I_{q} - \frac{c}{1+c(q+k-1)}\J_q\right] \left(\vvec -\lambda_n \tilde{z}\gamma \onevec  \right)\ .\
 \end{align}
Then $\frac{d^2}{dc^2}f_v(\vvec, c, \tilde{\zvec})$ has the same expression as (38) but with $t'(0) = 0$, $t(0) = \frac{1}{(2\pi)^{q/2}}$, $t''(0) = \frac{q(2q +4k -2)}{4}(2\pi)^{-q/2}$, and
\begin{equation}
    \begin{split}
        %h(\vvec, c =0, \tilde{\zvec}) & = \frac{-1}{2 (1-c)}\left[(1-c)\uvec^T\uvec + c\uvec^T\tilde{\zvec}\tilde{\zvec}^T\uvec  -2\lambda_n \onevec^T\uvec + \frac{\lambda^2 n k}{(1-c)} - \frac{\lambda^2 n \tilde{z}^{*\,2} \gamma}{(1-c)}\right]\\
        %h'(\uvec, c, \tilde{\zvec}) & = \frac{-1}{2}\left[-\uvec^T\uvec + \uvec^T\tilde{\zvec}\tilde{\zvec}^T\uvec+   \frac{\lambda^2 n k}{(1-c)^2} - \frac{(1+(k-1)c^2)\lambda^2 n \tilde{z}^{*\,2} }{(1-c)^2 (1+c(k-1))^2}\right]\\
        h'(\vvec, c, \tilde{\zvec})|_{c=0} & = - \frac{1}{2}\left[\vvec^T(\I_q-\J_q) \vvec - 2\lambda_n\tilde{z}\vvec^T\onevec\right]\\
        %h''(\uvec, c, \tilde{\zvec}) & = \frac{-1}{2}\left\{\frac{2\lambda^2 n k}{(1-c)^3} - \frac{2\left[(k-1)^2 c^3 + 3(k-1) c -k +2 \right]\lambda^2 n \tilde{z}^{*\,2} }{(1-c)^3 (1+c(k-1))^3}\right\}\\
        h''(\vvec, c, \tilde{\zvec})|_{c=0} & = -\frac{1}{2}\left[2\vvec^T\vvec + 2(q+k-2)\vvec \J_q\vvec -4\lambda_n \tilde{z}(q-k+2)\vvec^T\onevec +2n\lambda^2\tilde{z}^2q \right]\ .\
    \end{split}
\end{equation}
As $t''(0)\exp \{h(\vvec, 0, \tilde{\zvec})\} = \frac{q(2q +4k -2)}{4} f_v(\vvec, 0, \tilde{\zvec})$, we get the expression
\begin{equation}
    \begin{split}
        \frac{d^2}{dc^2}f_v(\vvec, c, \tilde{\zvec})\bigg |_{c=0}&= \frac{q(2q +4k -2)}{4} f_v(\vvec, 0, \tilde{\zvec})
        +f_v(\vvec, 0, \tilde{\zvec})\left\{ \frac{1}{4}\left[\vvec^T(\I_q-\J_q)\vvec - 2\lambda_n\tilde{z}\vvec^T\onevec\right]^2 \right\}\\
        & \quad +f_v(\vvec, 0, \tilde{\zvec})\left\{-\frac{1}{2}\left[2\vvec^T\vvec + 2(q+k-2)\vvec \J_q\vvec -4\lambda_n \tilde{z}(q-k+2)\vvec^T\onevec +2n\lambda^2\tilde{z}^2q\right]\right\} \ .\ \\
    \end{split}
\end{equation}
 %Note that $f_v(\vvec, 0, \tilde{\zvec})$ does not depend on $\tilde{\zvec}$ and is a standard multivariate normal random variable. 
 
 
 
 %We can now bring the expectation inside the integral in equation~\ref{eq:I_second_der_expect}. 

%$$
%\mathbb{E}_{\tilde{\zvec}} \bigg\{\int_{-\lambda_n \onevec}^{\lambda_n\onevec}\frac{d^2}{dc^2}\left\{f_v(\vvec,c,\tilde{\zvec}) \right \}\bigg |_{c=0}d\vvec\bigg \} = \int_{-\lambda_n \onevec}^{\lambda_n\onevec}\mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{f_v(\vvec,c,\tilde{\zvec}) \right \}\big |_{c=0}\bigg \}d\vvec
%$$
\noindent
Since $f_v(\vvec, 0, \tilde{\zvec})$ does not depend on the sign vector, we can rewrite:
\begin{equation}
    \begin{split}
        \mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{f_v(\vvec,c,\tilde{\zvec}) \right \}\big |_{c=0}&\bigg \}  = \frac{q(2q +4k -2)}{4} f_v(\vvec, 0, \tilde{\zvec})\\
        &\qquad +f_v(\vvec, 0, \tilde{\zvec})\bigg\{ \frac{1}{4}\big[(\vvec^T(\I_q-\J_q)\vvec)^2 -4 \lambda_n\mathbb{E}_{\tilde{\zvec}}\{\tilde{z}\}\vvec^T\onevec\vvec^T(\I_q-\J_q)\vvec\\
        &\qquad \qquad \qquad + 4n\lambda^2 \mathbb{E}_{\tilde{\zvec}}\{\tilde{z}^2\} (\vvec^T\onevec)^2\big] \bigg\}\\
        &\qquad +f_v(\vvec, 0, \tilde{\zvec})\bigg\{-\frac{1}{2}\big[2\vvec^T\vvec + 2(q+k-2)\vvec \J_q\vvec\\
        &\qquad \qquad \qquad -4\lambda_n \mathbb{E}_{\tilde{\zvec}}\{\tilde{z}\}(q-k+2)\vvec^T\onevec +2n\lambda^2\mathbb{E}_{\tilde{\zvec}}\{\tilde{z}^2\}q\big]\bigg\} \ .\ \\
    \end{split}
\end{equation}

By plugging in the expectation facts from equation (42), equation (56) becomes:
\begin{equation}
    \begin{split}
        \mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{f_v(\vvec,c,\tilde{\zvec}) \right \}\big |_{c=0}&\bigg \}  = \frac{q(2q +4k -2)}{4} f_v(\vvec, 0, \tilde{\zvec})\\
        &\qquad +f_v(\vvec, 0, \tilde{\zvec})\left\{ \frac{1}{4}\left[(\vvec^T(\I_q-\J_q)\vvec)^2  + n\lambda^2 k (\vvec^T\onevec)^2\right] \right\}\\
        &\qquad  +f_v(\vvec, 0, \tilde{\zvec})\bigg\{\frac{-1}{2}\big[2\vvec^T\vvec + 2(q+k-2)\vvec \J_q\vvec +2n\lambda^2kq\big]\bigg\} \ .\ \\
    \end{split}
\end{equation}
We can write without loss of generality
\begin{equation}
    \begin{split}
       \int_{-\lambda_n \onevec}^{\lambda_n\onevec}v_if_v(\vvec, 0, \tilde{\zvec})d\vvec &= G(\Delta_n)^{q-1} \int_{-\lambda_n }^{\lambda_n}v_ig(v_i)dv_i\\
       &=0 \ ,\ \\
    \end{split}
\end{equation}
where $G(\Delta_n)= G(\lambda_n)-G(-\lambda_n)$. %The equality to zero in the above equation is from the fact that $\int_{-\lambda_n }^{\lambda_n}v_ig(v_i)dv_i$ is a symmetric integral about $0$ of an even function and thus is equal to $0$. 
Furthermore, we can generalize this result for $i \neq j$:
\begin{equation}
    \begin{split}
       \int_{-\lambda_n \onevec}^{\lambda_n\onevec}v_i^av_j f_v(\vvec, 0, \tilde{\zvec})d\vvec &= G(\Delta_n)^{q-2} \left(\int_{-\lambda_n }^{\lambda_n}v_i^a g(v_i) dv_i\right)\left(\int_{-\lambda_n }^{\lambda_n}v_j g(v_j)dv_j\right)\\
       &=0 \ ,\ \\
    \end{split}
\end{equation}
for any $a>0$. Integrating (57) gives the expression
\begin{equation}
    \begin{split}
        %\int_{-\lambda_n \onevec}^{\lambda_n\onevec}\mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{f_v(\vvec,c,\tilde{\zvec}) \right \}\big |_{c=0}&\bigg \}d\vvec  = 
        \frac{q(2q +4k -2)}{4} \int_{-\lambda_n \onevec}^{\lambda_n\onevec}f_v(\vvec, 0, \tilde{\zvec})&d\vvec
        +\int_{-\lambda_n \onevec}^{\lambda_n\onevec}f_v(\vvec, 0, \tilde{\zvec})\left\{ \frac{1}{4}\left[(\vvec^T(\I_q-\J_q)\vvec)^2  + 4n\lambda^2 k (\vvec^T\onevec)^2\right] \right\}d\vvec\\
        & +\int_{-\lambda_n \onevec}^{\lambda_n\onevec}f_v(\vvec, 0, \tilde{\zvec})\bigg\{-\frac{1}{2}\big[2\vvec^T\vvec + 2(q+k-2)\vvec \J_q\vvec +2n\lambda^2kq\big]\bigg\} d\vvec \ .\ \\
    \end{split}
\end{equation}




First, $\int_{-\lambda_n \onevec}^{\lambda_n\onevec}f_v(\vvec, 0, \tilde{\zvec})d\vvec = G(\Delta_n)^q$. As $\vvec^T(\I_q-\J_q)\vvec = -2\sum_{i<j}v_iv_j$,  $[\vvec^T(\I_q-\J_q)\vvec]^2$ has $q(q-1)/2$ terms with $v_i^2v_j^2$ and other terms with $v_i$ in them. By (58), these latter terms will integrate to $0$, giving
\begin{equation}
    \begin{split}
        \frac{1}{4}\int_{-\lambda_n \onevec}^{\lambda_n\onevec} (\vvec^T(\I_q-\J_q)\vvec)^2f_v(\vvec, 0, \tilde{\zvec})d\vvec & = \int_{-\lambda_n \onevec}^{\lambda_n\onevec} \sum_{i<j}(v_i^2v_j^2)f_v(\vvec, 0, \tilde{\zvec})d\vvec\\
        & = \frac{q(q-1)}{2}G(\Delta_n)^{q-2}\left(\int_{-\lambda_n }^{\lambda_n}v_i^2g(v_i)dv_i\right )^2\ .\\\
    \end{split}
\end{equation}
Next, note that $(\vvec^T\onevec)^2 = \vvec^T\J_q\vvec = \sum_{i=1}^qv_i^2 + 2\sum_{i<j}v_iv_j$. From (59) the $v_iv_j$ terms will integrate to $0$. Thus, we can write:
\begin{equation}
    \begin{split}
        \frac{4n\lambda^2k}{4}\int_{-\lambda_n \onevec}^{\lambda_n\onevec} (\vvec^T\onevec)^2f_v(\vvec, 0, \tilde{\zvec})d\vvec & = \int_{-\lambda_n \onevec}^{\lambda_n\onevec} \sum_{i=1}^q f_v(\vvec, 0, \tilde{\zvec})d\vvec\\
        & = q n\lambda^2k G(\Delta_n)^{q-1}\int_{-\lambda_n }^{\lambda_n}v_i^2g(v_i)dv_i \ .\ \\
    \end{split}
\end{equation}
\noindent
Lastly, we can rewrite the last term in (60) as follows
\begin{equation}
    \begin{split}
        -\frac{1}{2}\int_{-\lambda_n \onevec}^{\lambda_n\onevec} \big[2\vvec^T\vvec &+ 2(q+k-2)\vvec \J_q\vvec +2n\lambda^2kq\big]f_v(\vvec, 0, \tilde{\zvec})d\vvec\\
        & = - \int_{-\lambda_n \onevec}^{\lambda_n\onevec} \big[(1+q+k-2)\sum_{i=1}^q v_i^2 +2n\lambda^2k q\big]f_v(\vvec, 0, \tilde{\zvec})d\vvec\\
        &=-q(q+k-1)G(\Delta_n)^{q-1}\int_{-\lambda_n }^{\lambda_n} v_i^2 g(v_i)dv_i
        -n\lambda^2kq G(\Delta_n)^q \ .\ \\
    \end{split}
\end{equation}
With these simplifications, we arrive at the expression
\begin{equation}
    \begin{split}
        \int_{-\lambda_n \onevec}^{\lambda_n\onevec}\mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{f_v(\vvec,c,\tilde{\zvec}) \right \}\big |_{c=0}\bigg \}d\vvec  &= \left(\frac{q(2q +4k -2)}{4}-qn\lambda^2k \right)G(\Delta_n)^q\\
        &\quad +\frac{q(q-1)}{2}G(\Delta_n)^{q-2}\left(\int_{-\lambda_n }^{\lambda_n}v_i^2g(v_i)dv_i\right )^2\\
        &\quad +q n\lambda^2k G(\Delta_n)^{q-1}\int_{-\lambda_n }^{\lambda_n}v_i^2g(v_i)dv_i\\
        &\quad -q(q+k-1)G(\Delta_n)^{q-1}\int_{-\lambda_n }^{\lambda_n} v_i^2 g(v_i)dv_i\\
    \end{split}
\end{equation}
Using the Gaussian integral identity 
\begin{equation}
    \int_{-\lambda_n }^{\lambda_n} v_i^2 g(v_i)dv_i= G(\Delta_n)-2\lambda_n g(\lambda_n) \ ,\
\end{equation}
we arrive at a final expression:
\begin{equation}
\begin{split}
\mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{P(I_\lambda \, | \, c, \zvec_\mathcal{A}=\tilde{\zvec}) \right \}\big |_{c=0}\bigg \}&= 2q\lambda_n G(\Delta_n)^{q-2}g(\lambda_n)\bigg\{ \lambda_n (q-1)g(\lambda_n) + k(1-\lambda_n^2)G(\Delta_n)\bigg\} \ .\
\end{split}
\end{equation}
%\bibliographystyle{asa} 
%\bibliography{power.bib}
%\subsubsection{Final Condition for Concavity}

Recall that $\phi^{\pm}$ is locally concave at $c=0$ when equation \eqref{eq:simplified_2nd_der_EZ} is less than or equal to 0, or,
\begin{equation}
    \begin{split}
        \mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{P(I_\lambda \, | \, c, \zvec_\mathcal{A}=\tilde{\zvec}) \right \}\big |_{c=0}\bigg \}P(S_\lambda \, | \, c=0) \leq - \mathbb{E}_{\tilde{\zvec}} \bigg\{\frac{d^2}{dc^2}\left\{P(S_\lambda \, | \, c, \betavec_\mathcal{A}=\beta\tilde{\zvec}) \right \}\big |_{c=0}\bigg \}P(I_\lambda \, | \, c=0)\ .\
    \end{split}
\end{equation}
When $c=0$, $P(I_\lambda \, | \, c=0)=G(\Delta_n)^q$ and $P(S_\lambda \, | \, c=0)= G(\tau_n)^k$. Then, after tedious algebra, the inequality (67) can be written as
\begin{equation}
\begin{split}
   & 2\lambda q g(\sqrt{n}\lambda)G(\tau)^2\left[k(1-n\lambda^2)G(\Delta) + (q-1)\sqrt{n}\lambda g(\sqrt{n}\lambda) \right]\leq \\
    &\qquad -k(k-1)g(\tau)G(\Delta)^2 \left\{ -\left[ (\beta +\lambda) +n\lambda^2(\beta-\lambda)\right]G(\tau) + \sqrt{n}\left[ \frac{\beta^2-\lambda^2}{2} + \beta\lambda\right]g(\tau)\right\} \ ,\  \label{eqn:Thm3_ineq}
\end{split}
\end{equation}
or the slightly more condensed inequality
\begin{equation}
\begin{split}
\frac{q}{\binom{k}{2}} \ \frac{\lambda_n g(\lambda_n)}{G(\Delta_n)}&\left( k(1-\lambda_n^2)+\frac{(q-1)\lambda_ng(\lambda_n)}{G(\Delta_n)}\right) \leq\\
&\frac{g(\tau_n)}{G(\tau_n)}\left(\beta_n +\lambda_n+\lambda_n^2\tau_n-\left[\frac{\beta_n^2-\lambda_n^2}{2}+\beta_n\lambda_n\right]\frac{g(\tau_n)}{G(\tau_n)}\right) \ ,
\ \label{eqn:Thm3_ineq}
\end{split}
\end{equation}
where $\beta_n = \sqrt{n}\beta$. %With the concavity condition in (74) and the proof that $\frac{d}{dc} \psi_\lambda^{\pm}(c |k, \beta)\big|_{c=0}=0$, there is a local maximum at $c=0$ when (74) holds. 
This completes the proof.

\section{Evaluating $\phi_{\max}$ and $\phi_\Lambda$}
 
This section describes the evaluation of $\phi_{\max}$ and $\phi_\Lambda$ in the software. These implementations are generally software independent, meaning it can be done it a wide variety of languages and utilizing different software packages. 

For $\phi_{\max}$, any optimization package can be used to find the $\lambda$ that maximizes $\phi_\lambda(\X, \betavec)$. However, it should be noted that, for $\Phi_{\max}^{\pm}(\X,k, \betavec)$, maximizing $\Phi_\lambda^{\pm}(\X,k, \betavec)$ over $\lambda$ could be computationally costly due to the number of evaluations of $\Phi_\lambda^{\pm}(\X,k, \betavec)$. While computational burden can be reduced by implementing some of the strategies given in Section 4, the implementation of $\Phi_{\max}^{\pm}(\X,k, \betavec)$ also begins with a warm-start to select a high-quality starting point for the optimization algorithm. The maximization is implemented over a user specified range of $\log(\lambda)$, and the warm- start evaluates $\Phi_{\max}^{\pm}(\X,k, \betavec)$ over a grid of evenly spaced $\log(\lambda)$ values. The $\log(\lambda)$ from this grid that maximizes $\Phi_{\max}^{\pm}(\X,k, \betavec)$ is then used as the starting value in the optimization algorithm. Our implementation uses the \texttt{optim} function in \texttt{R}.

For $\phi_{\Lambda}$, an adaptive Riemann sum integration is utilized over a range of $\log(\lambda)$. The adaptive Riemann sum begins with a user-specified lower bound for $\log(\lambda)$ (default is $-5$). The Riemann sum is then calculated over evenly spaced $\log(\lambda)$ values increasing from the lower bound. For each new $\log(\lambda)$ value, if $\phi_\lambda$ is non-zero or above some user-specified $\epsilon$, it is added to the Reimann sum and the sum continues with the next $\log(\lambda)$. If $\phi_\lambda < \epsilon$,  the Reimann sum is stopped and $\phi_\Lambda$ is given by the final value of the sum. 




\end{document}









