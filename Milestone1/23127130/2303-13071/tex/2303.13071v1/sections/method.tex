
% \vspace{-0.1in}

\section{Methodology} \label{sec:method}

% For model, module, we use \bb{}
% For common features and representations just \textit{}
% Math domain \mathbb
% Loss related \mathcal
% The goal of our work is to build a 3D-aware generative model that synthesizes photo-realistic and 360-degree \emph{full-round} view-consistent head images with diverse appearances in arbitrary poses. We train our model from unstructured monocular 2D image collections of both front face and back head, without using multi-view information or 3D annotations. The pipeline for our model is illustrated in Figure~\ref{fig:overview}.

% In this section, we start with an overview of our framework, followed by several novel contributions including tri-grid representation, foreground-aware tri-discriminator, camera location self-transformation, and end up with the training strategy.



\subsection{PanoHead Overview}
\label{sec:overview}
To synthesize realistic and view-consistent full head images, we build PanoHead upon a state-of-the-art 3D-aware GAN, \ie EG3D~\cite{chan2021efficient}, due to its efficiency and synthesis quality. 
% Given a Guassian sampled latent code $z$ and a conditioning camera label ${c_{con}},$ a mapping network $\bb{M}$ maps $(z,c_{con})$ into an intermediate latent code $w$, as shown: $\bb{M}:(z, c_{con}) \in \mathbb{R}^{512+25} \rightarrow w \in \mathbb{R}^{512}$. The intermediate latent code $w$ modulates the convolutional kernels of StyleGAN backbone~\cite{Karras2020stylegan2} to synthesize tri-plane features, which is further decoded into neural radiance density $\sigma$ and high-dimensional color feature. To render an image at desired camera pose $c_{cam}$, pixel-wise volumetric rendering rays are formulated from $c_{cam}$. By integrating the neural features along each ray, the neural renderer $\bb{R}$ formulates a low-resolution feature image $I^f$, which is further processed and up-sampled into a high-dimensional RGB image $I^+$ with a super-resolution module. The resulting feature images are view-consistent due to the nature of volumetric rendering, and the view consistency of the final images is also largely enforced with its dual discriminator $\bb{D}$.
Specifically, EG3D leverages StyleGAN2~\cite{Karras2020stylegan2} backbone to output a tri-plane representation that represents a 3D scene with three 2D feature planes. Given a desired camera pose $c_{cam}$, the tri-plane is decoded with a MLP network and volume rendered into a feature image, followed by a super-resolution module to synthesize a higher resolution RGB image $I^+$. Both the low and high resolution images are then jointly optimized by a dual discriminator $\bb{D}$.

% We achieve that with adversarial co-learning with 2D image foreground segmentation supervision(Section~\ref{sec:tridisc}). With that, we are able to rotate the rendering camera fully around the head without being blocked by the background. 2) We notice a strong appearance ambiguity induced from the inductive bias in tri-plane representation of the 3D scene, causing mirroring face artifacts on the back head. Therefore, in section~\ref{sec:trigrid}, we propose a novel tri-grid volume representation that maintains the efficiency while improving the expressiveness with resolved feature ambiguity. 3) To extend the synthesis capability beyond the front face, we augment the front-face dataset (FFHQ~\cite{karras2019style}) with high-resolution training images of heads from the sides and back. However, it poses a significant challenge for image cropping and alignment in the preprocessing stage due to the lack of facial landmarks for our augmented side and back head images. In section~\ref{sec:selftrans}, we design an adaptive processing pipeline that maximizes image alignment across various camera poses. Additionally we augment our training pipeline with a self-adaptive camera alignment module that co-learns the residual rendering camera transformation for all training images. Our approach largely eases the difficulties in GAN model learning from various image datasets with different distributions of cameras, styles and visual structures. 
In spite of EG3D's success in generating frontal faces, we found it to be a much more challenging task to adapt to 360$^\circ$ in-the-wild full head images for the following reasons: 1) foreground-background entanglement prohibit large pose rendering, 2) strong inductive bias from tri-plane representation causes mirroring face artifacts on the back head, and 3) noisy camera labels and inconsistent cropping of back head images. To 
address these problems, we introduce a background generator and a tri-discriminator for decoupling foreground and background (Section~\ref{sec:tridisc}), an efficient yet more expressive tri-grid representation while still being compatible with StyleGAN backbone (Section~\ref{sec:trigrid}), and a two-stage image alignment scheme with an self-adaptation module that dynamically adjusts rendering cameras during training (Section~\ref{sec:selftrans}). The overall pipeline for our model is illustrated in Figure~\ref{fig:overview}.
 

% move the rest to each individual parts 
% Our method, as shown in Figure~\ref{fig:overview}, builds upon StyleGAN2~\cite{Karras2020} and EG3D~\cite{chan2021efficient} with several modifications specifically designed to adapt to full-head data better. Our framework consists of three main components: a foreground-aware StyleGAN2-based generator \bb{G}, discriminator \bb{D}, and a neural renderer \bb{R} using 3D tri-grid representation. To generate a head image, the objective of our framework \bb{F} is defined as follows:
% \begin{equation} \label{Eq:Generator}
% % \small
% \bb{F} : (z, c) \in \mathbb{R}^{512+25} \rightarrow
% I^{+} \in \mathbb{R}^{H \times W \times 3}
% ,
% \end{equation}
% where a latent code $z$ is sampled from Gaussian distribution $z \sim p_{z} (0,1)$, $c$ are the 25-dimension camera parameters with 16 extrinsic and 9 intrinsic, and $I^{+}$ is the high-resolved three channel RGB image.
% Specifically, a mapping network $\bb{M}:(z, c) \in \mathbb{R}^{512+25} \rightarrow
% w \in \mathbb{R}^{512}$ maps $z$ and $c$ into the intermediate latent code $w$, following the conditional generation~\cite{Karras2020a}. As introduced in Section~\ref{sec:trigrid}, the StyleGAN2 CNN generator \bb{G} is modifited to takes $w$ to obtain the 3D tri-grid representation features $f$:
% \begin{equation} \label{Eq:trigrid}
% % \small
% \bb{G} : w \in \mathbb{R}^{512} \rightarrow
% f \in \mathbb{R}^{3\times H \times W \times C \times D}
% ,
% \end{equation}
% where $H$ and $W$ are the shape of each plane, $C$ the number of channels, and $D$ the depth of tri-grid. Next, the neural renderer \bb{R}~\cite{chan2021efficient} decodes $f$ into color and density, altogether with the rendering camera parameters $c$, renders the feature images $I^{f}$:
% \begin{equation} \label{Eq:renderer}
% % \small
% \bb{R} : (f, c) \rightarrow
% I^{f},
% \end{equation}
% where $I^{f}$ consists of raw RGB image $I^{r}$ and head segmentation mask $I^{m}$. 
% A novel self-transformation scheme is employed to correct rendering camera's location~(Section~\ref{sec:selftrans}).
% An super-resolution image $I^+$ and an bilinearly upsampled RGB image $I$ are then obtained from the raw foreground image $I^{r}$. Finally, with super resolved mask $I^{m+}$ a seven-channel images ($I^+$, $I$, $I^{m+}$) are critiqued by our foreground-aware tri-discriminator:
% \begin{equation} \label{Eq:tridisc}
% % \small
% \bb{D} : (I^+, I, I^{m+}) \in \mathbb{R}^{H \times W \times 7}\rightarrow \mathbb{R}
% ,
% \end{equation}
% as described in Section~\ref{sec:tridisc}.


% \vspace{-0.1in}
\subsection{Foreground-Aware Tri-Discrimination}
\label{sec:tridisc}


A typical challenge of state-of-the-art 3D-aware GANs, like EG3D~\cite{chan2021efficient}, is the entangled foreground with the background of synthesized images. Regardless of the highly detailed geometry reconstruction, directly training the 3D GAN from in-the-wild RGB image collections, such as FFHQ~\cite{karras2019style}, results in a 2.5D face, as illustrated in Figure~\ref{fig:tridisc_visual} (a). Augmenting with image supervisions from the side and back of the head helps build up the full-head geometry with reasonable back head shapes. However, it does not solve the problem because the tri-plane representation itself is not designed to represent separated foreground and background.
% However, for images from the back, it remains challenging for the network to distinguish a head with complex hairstyles or apparels from the in-the-wild background.     
% It is acceptable if the generation targets are 3D assets with clean background. However, 3D-aware GANs targeting face or head generation are often trained with in-the-wild RGB images like FFHQ~\cite{karras2019style} that contain human head merged with the background, which makes the generator can only generate similar head-background entangled RGB images. As shown in Figure~\ref{fig:tridisc_visual}, generated geometry usually shows a front head floating on a concrete wall, which represents the background. 

To disentangle the foreground from the background, we first introduce an additional StyleGAN2 network~\cite{Karras2020stylegan2} to generate 2D backgrounds at the same resolution of raw feature image $I^r$. During volume rendering, the foreground mask $I^m$ can be obtained by:
\begin{gather}
    I^r(r) = \int_{0}^{\infty} w(t) f(r(t)) dt, \,\, I^m(r) = \int_{0}^{\infty} w(t) dt, \\
 w(t) = exp\bigl(-\int_{0}^{t} \sigma(r(s)) ds\bigr) 
    \sigma(r(t)),
\end{gather}
where $r(t)$ represents a ray emitted from the rendering camera center. The foreground mask is then used to compose a new low-resolution image $I^{gen}$:
\begin{equation}
    I^{gen} = (1 - I^m)I^{bg} + I^r,
\end{equation}
which is fed into the super-resolution module. Note that the computation cost of background generator is insignificant since its output has a much lower resolution than the tri-plane generator and super-resolution module.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/tridisc.pdf}
    \caption{Geometry and RGB images from dual-discrimination (a) and foreground-aware tri-discrimination (b, c). EG3D (a) fails to decouple the background. PanoHead's tri-discrimination offers both background-free geometry (b) and background-switchable full head image synthesis (c).}
    \label{fig:tridisc_visual}
    % \vspace{-0.1in}
\end{figure}
Simply adding a background generator does not fully decouple it from the foreground since the generator tends to synthesize foreground content in the background. Thus, we propose a novel foreground-aware tri-discriminator to supervise the rendered foreground mask along with the RGB images. 
% The key idea is to instill the prior knowledge of well-established 2D image foreground segmentation into the learning of 3D neural radiance scene decomposition. 
Specifically, the input of the tri-discriminator has 7 channels, composed with a bilinearly-upsampled RGB image $I$ , a super-resolved RGB image $I^+$ and single-channel upsampled foreground mask $I^{m+}$. The additional mask channel allows the 2D segmentation prior knowledge to be back-propagated into the density distribution of the neural radiance field. Our approach reduces the learning difficulty in shaping the 3D full head geometry from unstructured 2D images, enabling authentic geometry ((Figure~\ref{fig:tridisc_visual} (b))) and appearance synthesis of a full head composable with various backgrounds (Figure~\ref{fig:tridisc_visual} (c)). 
We note that in constrast from ENARF-GAN~\cite{enarf} that employs a single discriminator for RGB images composed of synthesized foreground and background images using a dual-generated mask, our tri-discriminator better ensures view-consistent high-resolution outputs.
% We also note that the foreground and background decoupling solves the seam artifact in the original EG3D
% ~\cite{chan2021efficient}. A plausible explanation is that such artifacts are caused by the difficulty to connect foreground and background in a single 2.5D geometry while our decomposition avoids this problem.


% Original dual discriminator of EG3D~\cite{chan2021efficient} takes six-channel images as input, where first three channels represent a super-resolved image $I^+$ and last three channels a bilinearly upsampled image $I$. It helps the super-resolved images to be consistent with the neural rendering. Our foreground-aware tri-discriminator takes one more channel, a one channel high-resolved head mask $I^m+$ that highlights head-related pixels, to be \textit{aware of background and foreground explicitly}. 
% Given a image-mask paired dataset $\{(I^+_{1},I_1,M_1),(I^+_{2},I_2,M_2),...,(I^+_{n},I_n,M_n)\}$, where $M_i$ represents the head segmentation mask of image $I_i$ and.

% Essentially, the generator and neural renderer $\bb{G}+\bb{R}: w \in \mathbb{R}^{512}  \rightarrow I^f$ map the latent codes to feature images $I^f$ consisting of the raw foreground image $I^r$ and the head segmentation mask $I^m$.
% % while tri-discriminator $\bb{D}: (I^+, I, I^m) \rightarrow \mathbb{R}$ models the joint distribution $p(i^+,i,m)$ to decouple the head and the background. 
% We aggregate the background image $I^{bg}$ and foreground image $I^r$ to obtain the generated image $I^{gen}$:
% \begin{equation} \label{Eq:igen}
% % \small
% I^{gen} = (1 - I^m)I^{bg} + I^r,
% \end{equation}
% where $I^{bg}$ is a background-only image generated from intermediate latent code $w$. The low-resolution generated image $I^{gen}$ and segmentation mask $I^m$ are fed into a super-resolution module~\cite{chan2021efficient} to obtain high-resolved image $I^+$ and mask $I^{m+}$, respectively. At the same time, $I^{gen}$ is bilinearly upsampled to $I$ as well. Finally, tri-discriminator $\bb{D} : (I^+, I, I^{m+}) \in \mathbb{R}^{H \times W \times 7}\rightarrow \mathbb{R}$ models the joint distribution $p(i^+,i,i^{m+})$ to enforce foreground-background consistency and multi-view consistency at the same time.


% Figure~\ref{fig:tridisc_visual} (a) and (b) show rendered images using original dual-discriminator and our foreground-aware tri-discriminator respectively. With dual-discriminator, front of the had is floating on the concrete background, thus preventing full-head image and geometry synthesis. Our tri-discriminator successfully decouples background and human head clearly, as a comparison. Especially, the round and clean head geometry shape without background enables authentic full-head synthesis.



\begin{figure}[b]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/trigrid_paper.pdf}
    \caption{Comparison between tri-plane (a) and tri-grid (b) architecture in $Z$ axis. With tri-plane, two different points' projections share the feature from the plane $P^{XY}$, which introduces representation ambiguity. With tri-grid, the features for the above two points are trilinearly interpolated from two different planes, thus generating distinct features.} 
    % The second and third columns show a single point's projection in two other directions.}
    \label{fig:trigrid_archi}
\end{figure}

\begin{figure}[t] 
    \centering
    \includegraphics[width=0.95\textwidth]{figures/trigrid-visual.pdf}
    \caption{Images synthesis with tri-plane and tri-grid ($D=3$). Due to the projection ambiguity, tri-plane representation (a) can generate good-quality front face image yet with a `mirrored face' on back head, while our tri-grid representation synthesizes high-quality back head appearance and geometry (b).}
    \label{fig:trigrid_visual}
    % \vspace{-0.1in}
\end{figure}

% \vspace{-0.1in}
\subsection{Feature Disentanglement in Tri-Grid}
\label{sec:trigrid}

The tri-plane representation, proposed in EG3D~\cite{chan2021efficient}, offers an efficient representation for 3D generation. The neural radiance density and appearance of a volume point are obtained by projecting its 3D coordinate over three axis-aligned orthogonal planes and decoding the sum of three bilinearly interpolated features with a tiny MLP. However, when synthesizing a full head in $360^\circ$, we observe tri-plane is limited in expressiveness and suffers from mirroring-face artifacts. 
The problem is even pronounced when the camera distribution of the training images is unbalanced. The root cause is the inductive bias originating from tri-plane projection, where one point on a 2D plane has to represent features of different 3D points. For example, a point on the front face and a point on the back hair will be projected to the same point on the $XY$ plane $P^{XY}$ (orthogonal to $Z$ axis), as illustrated in Figure~\ref{fig:trigrid_archi} (a). Although the other two planes should theoretically provide complementary information to alleviate this projection ambiguity, we found it not the case when there is less visual supervision from the back or when the structure of the back head is challenging to learn. The tri-planes are prone to borrow features from the front face to synthesize the back head, referred to as mirroring-face artifacts here (Figure~\ref{fig:trigrid_visual}(a)). 

% Tri-plane representation used in EG3D is an explicit-implicit hybrid representation that offers both efficiency and expressiveness. In the tri-plane definition, explicit feature planes are aligned along three orthogonal feature planes, $P^{XY}$, $P^{XZ}$, and $P^{YZ}$, as shown in Figure~\ref{fig:trigrid_archi}(a). Each of the plane is with a shape of $H \times W \times C$, where $H$ and $W$ are the shape of planes and $C$ the number of channels. Specifically, when a 3D position is queried, it is first projected onto three feature planes to retrieve three coordinates on 2D planes. Three feature vectors $(f^{XY}, f^{XZ}, f^{YZ})$ are then obtained by bilinear interpolation conducted with feature planes and those 2D coordinates. Then feature vectors from three planes are summed up and delivered to the decoder, where the 3D features are interpreted as color and density. Finally, NeRF-like renderer~\cite{max1995optical,mildenhall2020nerf} renders RGB images using color, density, and pose vector.

% Although tri-plane representation is quite efficient compared to explicit representation, it is not as expressive due to the inductive bias. Specifically, ambiguity is introduced during coordinates' projection due to the dimension limitation of 2D plane. Assuming we want to retrieve feature vectors from two points: $p1$ on the front face with coordinate of $(x, y, z)$, and $p2$ on the back of the head with coordinate of $(x, y, -z)$. Based on the projection mechanism of tri-plane, the projected coordinates along $Z$ axis on plane $P^{XY}$ of $p1$ and $p2$ will be the identical $(x,y)$, as highlighted in Figure~\ref{fig:trigrid_archi}(a). The identical projected coordinates further result in identical features $f^{XY}$ for two different position, which yields unnatural results during neural rendering. 
To reduce the inductive bias of the tri-plane, we lift its formulation into a higher dimension by augmenting tri-plane with an additional depth dimension. 
We call this enriched version as a tri-grid. 
Instead of having three planes with a shape of $H \times W \times C$ with $H$ and $W$ being the spatial resolution and $C$ being the number of channel, each of our tri-grid has a shape of $D \times H \times W \times C$, where $D$ represents the depth. For instance, to represent spatial features on the $XY$ plane, tri-grid will have $D$ axis-aligned feature planes $P^{XY}_i, i = 1,\ldots, D$ uniformly distributed along the $Z$ axis. We query any 3D spatial point by projecting its coordinate onto each of the tri-grid, retrieving the corresponding feature vector by \emph{tri-linear interpolation}. As such, for two points sharing the same projected coordinates but with different depths, the corresponding feature would be likely to be interpolated from non-shared planes (Figure~\ref{fig:trigrid_archi} (b)). Our formulation disentangles the feature presentation of the front face and back head and therefore largely alleviates the mirroring-face artifacts (Figure~\ref{fig:trigrid_visual}). 

Similar to tri-plane in EG3D~\cite{chan2021efficient}, we can synthesize the tri-grid as $3\times D$ feature planes using the StyleGAN2 generator ~\cite{karras2019style}. That is, we increase the number of output channels of the original EG3D backbone by $D$ times. Thus, tri-plane can be regarded as a na\"ive case of our tri-grid representation with $D=1$. The depth $D$ of our tri-grid is tunable and larger $D$ offers more representation power at the cost of additional computation overhead. Empirically we find a small value of $D$ (\eg $D=3$) is sufficient in feature disentanglement while still maintaining its efficiency as a 3D scene representation. 
% To reduce the inductive bias, we propose tri-grid representation. In stead of each plane in tri-plane with a shape of $H \times W \times C$, each grid in tri-grid is with shape of $D \times H \times W \times C$, where $D$ represents the depth of the grid. For instance, $XY-Z$ grid consists of multiple axis-aligned planes $P^{XY-i}$ distributed across the $Z$ axis, where $i \in [1,D]$. $XY-Z$, $XZ-Y$, and $YZ-X$ grids together form the tri-grid representation, as shown in the second row of Figure~\ref{fig:trigrid_archi} (b). In tri-grid, the feature vectors are retrieved by trilinear interpolation rather than bilinear. The tri-grid not only enhances the 3D representation by stacking a new dimension, more importantly, it \textit{eliminates the ambiguity during projection}. Given $p1$ on the front face with coordinate of $(x, y, z)$, and $p2$ on the back of the head with coordinate of $(x, y, -z)$, they both are projected to plane $P^{XY}$ in tri-plane representation, as shown in Figure~\ref{fig:trigrid_archi} (a). With tri-grid, $p1$ is projected onto $P^{XY-i}$ and $P^{XY-j}$, while $p2$ projected onto $P^{XY-j}$ and $P^{XY-k}$, as shown in Figure~\ref{fig:trigrid_archi} (d). After interpolation, the features for $p1$ and $p2$ will be different as their projections do not need to share a single plane. To summarize, feature vectors are more expressive since different points' projection do not have to share the same feature plane. Note that $D$ in tri-grid is a tune-able parameter and larger $D$ offers more expressive representation while increasing the computation overhead. When $D$ equals to one, tri-grid representation degenerates into tri-plane. In practice, we found that a small value of $D$ (\eg $D=3$) is sufficient to alleviate the projection ambiguity of tri-planes.




\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/self-trans.pdf}
    \caption{Image synthesized without (a) and with the camera self-adaptation scheme(b). Without it, the model generates misaligned back head images, leading to a defective dent in back head. }
    \label{fig:selftrans}
    % \vspace{-0.1in}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth,trim=0 12 0 0,clip]{figures/comparison.pdf}
    \caption{Qualitative comparison between GRAF~\cite{schwarz2020graf}, GIRAFFEHD~\cite{xue2022giraffe}, StyleSDF~\cite{or2021stylesdf}, EG3D~\cite{chan2021efficient}, multi-view supervised NeRF~\cite{szymanowicz2022photo} (different methods from top to bottom on left side), and our PanoHead (right). Except ~\cite{szymanowicz2022photo}, all models are trained on FFHQ-F. We render the results at a yaw angle of 0, 45, 90, 135, and 180$^{\circ}$. GRAF, GIRAFFEHD, and StyleSDF fail to model the correct camera distribution in latent space due to the unsupervised camera pose mechanism, thus not turning to the back. EG3D is able to rotate to the back with `mirroring face' artifacts and entangled background. Multi-view supervised NeRF is comparable to ours, however, it requires multi-view data of a single person and is not a generative model.
    }
    \label{fig:baselinecompare}
    % \vspace{-0.1in}
\end{figure*}

% Figure~\ref{fig:trigrid_visual} (a) and (b) show rendered images using tri-plane and tri-grid representation, respectively. We observe that the model can only render a `face on back head' with tri-plane when we pose the camera in the back head due to the projection ambiguity. In strong contrast, the model renders authentic back head images with tri-grid.

\subsection{Self-Adaptive Camera Alignment}
\label{sec:selftrans}
For adversarial training of our full head in $360^\circ,$  we need in-the-wild image exemplars from a much wider range of camera distribution than the mostly frontal distribution, as in FFHQ~\cite{karras2019style}. 
% Therefore, we train our 3D full head GAN with image collections from various sources, including FFHQ~\cite{karras2019style}, K-hairstyle dataset~\cite{kim2021k}, and in-house high-resolution large-pose head images. 
Although our 3D-aware GAN is only trained from widely-accessible 2D images, 
the key to the best quality training is accurate alignment of visual observations across images labeled with well-estimated camera parameters. While a good practice has been established for frontal face images cropping and alignment based on facial landmarks, it has never been studied in pre-processing large-pose images for GAN training. Both camera estimation and image cropping are no longer straightforward due to the lack of robust facial landmarks detection for images taken from the side and back.  

To resolve the aforementioned challenge, we propose a novel two-stage processing. In the first stage, for images with detectable facial landmarks, we still adopt the standard processing where the faces are scaled to a similar size and aligned at the center of the head using state-of-the-art face pose estimator 3DDFA~\cite{3DDFAv2}. For the rest of the images with large camera poses, we employ a head pose estimator WHENet~\cite{zhou2020whenet} that provides a roughly-estimated camera pose, and a human detector YOLO~\cite{ivavsic2019human} with a bounding box centered at the detected head. To crop the images at a consistent head scale and center, we apply both YOLO and 3DDFA on a batch of front-face images, from which we adjust the scale and translation of the head center of YOLO with constant offsets. 
%While not being perfect, we are able 
This approach enables us to pre-process all head images with labeled camera parameters and in a consistent alignment to a large extent. 



Due to the presence of various hairstyles, there is still inconsistency in the alignment of back head images, inducing significant learning difficulties for our network to interpret the complete head geometry and appearance (see Figure~\ref{fig:selftrans} (a)). We, therefore, propose a self-adaptive camera alignment scheme to fine-tune the transformation of volume rendering frustum for each training image. Specifically, our 3D-aware GAN associates each image with a latent code $z$ that embeds the 3D scene information of geometry and appearance, which can be synthesized at a view of $c_{cam}.$ $c_{cam}$ might not align well with the image content for our training images; so, it is hard for the 3D GAN to figure out a reasonable full head geometry. Therefore we co-learn a residual camera transformation $\Delta c_{cam}$ mapped from $(z, c_{cam})$ together with our adversarial training. The magnitude of $\Delta c_{cam}$ is regularized with a L$_2$ norm. Essentially, the network dynamically self-adapts the image alignment with refined correspondence across different visual observations. We note that this is only possible credited to the nature of 3D-aware GAN that can synthesize view-consistent images at various cameras. Our two-stage alignment enables 360-degree view-consistent head synthesis with authentic shape and appearance, learnable from diverse head images with widely distributed camera poses, styles, and structures. 




% 3D-aware GANs either require supervised paired camera pose labels for each training image~\cite{chan2021efficient}, or unsupervised camera distribution statistics of whole dataset~\cite{or2021stylesdf, schwarz2020graf, xue2022giraffe, niemeyer2021giraffe} to model the camera-world coordinates system for neural rendering. Since the real camera labels of in-the-wild datasets are usually missed, these datasets are pre-processed using landmark-based face pose estimation models like 3DDFA~\cite{3DDFAv2} to obtain the camera labels. For example, EG3D employs a perspective pinhole camera model with estimated extrinsic and empirically-selected intrinsic.

% While face pose estimation methods are reasonably accurate for front face, they fail on large poses or back heads. In EG3D, FFHQ images are cropped to where faces have similar scale, and assuming the cameras on a sphere with same radius. However, this cannot be apply for large pose or back head data since facial landmarks are critical to estimate the size of the head. While there is head-based pose estimation method~\cite{zhou2020whenet} works for large pose, the generated labels can only be considered as noisy labels. To this end, we introduce a camera self-transformation framework to help the model correct its camera location even if the camera labels are noisy by a significant margin. 


% \begin{algorithm}[h]
% \SetAlgoLined
%     \PyComment{G: Generator, z: Latent code} \\
%     \PyComment{w: Intermediate latent code} \\
%     \PyComment{c\_con: Conditioned camera pose to generate w} \\
%     \PyComment{c\_cam: Rendering camera pose to synthesize the image} \\
%     \PyComment{t: Translation offset} \\
%     % \PyCode{for i in range(N):} \\
%     % \Indp   % start indent
%     %     \PyComment{your comment} \\
%     %     \PyCode{your code} \PyComment{inline comment} \\ 
%     % \Indm % end indent, must end with this, else all the below text will be indented
%     % \PyComment{this is a comment} \\
%     % \PyCode{your code}
%     \PyComment{}\\ 
%     \PyComment{Original image synthesis} \\
%     \PyComment{w = mapping\_network(z, c\_con)}\\
%     \PyComment{img = G.synthesis(w, c\_cam)}\\
%     \PyComment{}\\ 
%     \PyComment{Image synthesis with self-transformation} \\
%     \PyCode{w = mapping\_network(z, c\_con)}\\
%     \PyCode{t = mapping\_network(z, c\_cam)}\\
%     \PyCode{c\_cam[:,[3,7,11]] += t[:,[0,1,2]]}\\
%     \PyCode{img = G.synthesis(w, c\_cam)}\\
%     \PyComment{Loss}\\
%     \PyCode{loss\_trans = t.mean(0)**2.sum()*$\lambda_{trans}$}\\
% \caption{Self-transformation pseudocode}
% \label{algo:trans}
% \end{algorithm}

% Algorithm~\ref{algo:trans} illustrates our camera self-transformation scheme. There are two kinds of camera parameters, conditioned $c_{con}$ and rendering $c_{cam}$. In the original process, $c_{con}$ and $z$ are mapped into intermediate latent code $w$ through a mapping network. Then, generator \bb{G} synthesizes images using $w$ and any given camera pose with $c_{cam}$. The front and back data have different actual camera distance while their labels lie on a sphere with a fixed radius. In other word, the generator cannot model the correct size of back head due to this data distribution shift with noisy label. In our self-transformation scheme, the translation offset is learned using another mapping network and summed with the rendering camera poses' translation component. We employ regularization for its loss optimization.
% To summarize, a three-scalar translation offset $t$ to the mean camera location of the dataset is learned through an extra mapping network.


% Figure~\ref{fig:selftrans} (a) and (b) show the comparison between back head image synthesis with and without camera self-transformation. Without self-transformation, we observe that the model generates adequate front head, yet fails to learn the correct shape of back head, as shown in Figure~\ref{fig:selftrans} (a). In opposition to that, with our self-transformation scheme, the model perceives the actual camera location for back head and self-correct camera's location, thus unifying the shape of front and back head, as shown in Figure~\ref{fig:selftrans} (b).




% \subsection{Decoupling Pose in Latent Space} % not sure if we will add this
% \subsection{Learning Framework}

% During training, from random initialization, latent code $z$ and camera parameters $c$ are sampled from prior distributions $p_z$ and $p_c$. The generator \bb{G} and neural renderer \bb{R} synthesizes RGB image and its head segmentation mask with intermediate latent code $w$ and rendering camera parameters $c$, as described in Section~\ref{sec:overview}. A discriminator \bb{D} takes real images and generated images to determine if they are real. Note that we pre-process the real images to obtain head segmentation masks before feeding them into the discriminator. Our entire pipeline, including generator \bb{G}, neural renderer \bb{R}, and the discriminator \bb{D} is trained using non-saturating GAN loss with R1 regularization~\cite{mescheder2018training}, following StyleGAN2~\cite{karras2020analyzing} and EG3D~\cite{chan2021efficient} scheme. To make our discriminator foreground-aware, we regularize the gradient norm of the head segmentation mask with an additional R1 regularization loss $\mathcal{L}_{R1_{mask}}$. Additionally, for the camera self-transform scheme, we also have the regularization loss $\mathcal{L}_{trans} = \lVert t \rVert^2$ to ensure that corrected camera is not too far from its original location. More formally, our loss function is shown as follow:

% \label{sec:train}

% \begin{equation} \label{Eq:loss}
% \mathcal{L} = \mathcal{L}_{EG3D} + \lambda_{R1_{mask}}\mathcal{L}_{R1_{mask}} + \lambda_{trans}\mathcal{L}_{trans}
% \end{equation}
