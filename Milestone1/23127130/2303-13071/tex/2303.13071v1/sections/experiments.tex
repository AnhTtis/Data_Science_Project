\section{Experiments} \label{sec:exp}




\subsection{Datasets and Baselines}

We train and evaluate our framework on a balanced combination of FFHQ~\cite{karras2019style},  K-hairstyle dataset~\cite{kim2021k}, and an in-house large-pose head image collection. FFHQ contains $70K$ diverse high-resolution face images, yet mainly fall in the absolute yaw range from 0$^{\circ}$ to 60$^{\circ}$, assuming up-front camera pose corresponds to 0$^{\circ}$. 
We augment the FFHQ dataset with $4K$ back-head images from K-hairstyle dataset and $15K$ in-house large-pose images with diverse styles, ranging from 60$^{\circ}$ to 180$^{\circ}$. For brevity, we name this dataset combination as FFHQ-F. We refer to the supplementary paper for more dataset analysis and network training details. 



% To compliment large-pose supervision, we collect a in-house 8K human full-head data with absolute yaw range from 60$^{\circ}$ to 180$^{\circ}$ and merge them with FFHQ~(referred as FFHQ-F later). We first crop and align it with roughly similar head scale as FFHQ and employ WHENET~\cite{zhou2020whenet} to estimate the head pose, then convert the estimated pose angle to camera parameters. Since the cropping and pose estimation are not landmark-based, these camera parameters for in-house dataset are nosier than FFHQ, as mentioned in Section~\ref{sec:selftrans}. We horizontally flip the images to conduct data augmentation.

We compare against state-of-the-art 3D-aware GANs including GRAF~\cite{schwarz2020graf}, EG3D~\cite{chan2021efficient}, StyleSDF~\cite{or2021stylesdf}, and GIRAFFEHD~\cite{xue2022giraffe}. All baselines are retrained from the same FFHQ-F dataset. We measure the quality of generated multiview images and geometry both quantitatively and qualitatively. 
% In addition, we also show quality comparison of final rendered full-head results with multi-view supervised reconstruction method~\cite{szymanowicz2022photo}.

% \subsection{Implementation Details}
% Our method is implemented using PyTorch 1.12. We mostly follow the training settings of StyleGAN2 and EG3D.  $\lambda_{R1_{mask}}$ and $\lambda_{trans}$ are set to 1 and 10, respectively. We set the camera pose swapping probability to 0.7 instead of 0.5 in original EG3D to improve the image synthesis quality from non-conditional camera pose. For more implementation details, please refer to supplementary material.


\begin{table}[t]
\centering
\caption{Metrics comparison across all baselines.  For segmentation MSE, only GIRAFFEHD and PanoHead decouple the background and foreground. For ID score, GRAF's low-quality images lead to facial detection failure.}
\label{tab:quantitative}
\begin{adjustbox}{width=1\columnwidth,center}
\begin{tabular}{llllll}
\toprule
    & GRAF & GIRAFFEHD & StyleSDF & EG3D & Ours \\ \cmidrule{2-6} 
FID-all~$\downarrow$ & 68.2    & 37.3         & 78.5        & 6.2    & \textbf{5.4}    \\
MSE ($10^{-2}$)~$\downarrow$ & N/A    & 42.6         & N/A        & N/A    & \textbf{9.1}    \\
ID~$\uparrow$  & N/A    & 0.39         & 0.41        & \textbf{0.74}  & \textbf{0.74}    \\\bottomrule
\end{tabular}
\end{adjustbox}
% \vspace{-0.1in}
\end{table}

% \begin{table}[h]
% \centering
% \caption{Runtime comparison with or without different components.}
% \label{tab:runtime}
% \begin{adjustbox}{width=1\columnwidth,center}

% \begin{tabular}{ccccccc}
% \toprule
%         & EG3D & \multicolumn{3}{c}{tri-grid D} & with seg. & with self-adapt. \\ \cmidrule{2-7} 
%         &      & 1        & 3        & 5        &         &                \\ \cmidrule{3-5}
% runtime~$\downarrow$ & 1   & 1.00$\times$        & 1.10$\times$         & 1.14$\times$         & 1.15$\times$        & 1.02$\times$               \\\bottomrule
% \end{tabular}

% \end{adjustbox}
% \end{table}

\begin{table}[h]
\centering
\caption{Ablation studies on different components. +seg. means with foreground-aware tri-discrimination. +self-adpat. means with camera self-adaptation scheme. All are trained with FFHQ-F}
\label{tab:ablation}
\begin{adjustbox}{width=1\columnwidth,center}

\begin{tabular}{ccccc}
\toprule
        & EG3D  & \multicolumn{2}{c}{+seg.} & +seg.\&self-adapt. \\ \cmidrule{2-5} 
        &         &  tri-plane               & tri-grid                & tri-grid      \\ \cmidrule{3-5}
FID-back~$\downarrow$ & 50.4          & 44.1               & 44.0        & \textbf{40.9}               \\
FID-front~$\downarrow$ & 6.6        & \textbf{5.0}                & 5.5        & 5.4               \\
FID-all~$\downarrow$ & 6.2        & \textbf{5.2}                 & \textbf{5.2}        & 5.4               \\ \midrule
IS-back~$\uparrow$ & 4.3           & 3.9                & 4.2        & \textbf{4.4}               \\
IS-front~$\uparrow$ & 3.9           & \textbf{4.1}                & \textbf{4.1}        & \textbf{4.1}               \\
IS-all~$\uparrow$ & 3.8           & 4.0               & 4.0        & \textbf{4.1}               \\ \midrule
Runtime~$\downarrow$ & \textbf{1}      & 1.14$\times$                 & 1.26$\times$        & 1.28$\times$               \\ \bottomrule
\end{tabular}
\end{adjustbox}
% \vspace{-0.1in}
\end{table}

% \vspace{-0.1in}
\subsection{Qualitative Comparisons}

\noindent{\textbf{360$^\circ$ Image Synthesis.}}
 Figure~\ref{fig:baselinecompare} visually compares the image quality against the baselines, all trained with FFHQ-F, by synthesizing images from five different views, ranging the yaw angle from 0 to 180$^{\circ}.$  GRAF~\cite{schwarz2020graf} fails to synthesize compelling head images and its background is entangled with foreground head. StyleSDF~\cite{or2021stylesdf} and GIRAFFEHD~\cite{xue2022giraffe} are able to synthesize realistic frontal face images but in low perceptual quality when rendered from a larger camera pose. Without explicit reliance on camera labels, we suspect the above methods have difficulty in interpreting the 3D scene structures by themselves directly from images with 360$^{\circ}$ camera distribution.  
We observe that EG3D~\cite{chan2021efficient} is able to synthesize high-quality view-consistent frontal head images before rotating the view to the side or even the back. 
Mirroring face artifacts are clearly observable from the back, 
due to the tri-plane's projection ambiguity and the entangled fore-background. The method proposed in \cite{szymanowicz2022photo} builds personalized full-head NeRF at the extra cost of multi-view supervision. Regardless of its good quality images at all views, the model itself is not a generative model. In strong contrast, our model generates superior photo-realistic head images \textit{for all camera poses} while retaining multi-view consistency. It delivers photo-realism with fine details at diverse appearances, ranging from shaved head with glasses to long curly hairstyles. 
To better appreciate our multi-view full-head synthesis, please refer to our supplementary video for more comprehensive visual results.






\noindent{\textbf{Geometry Generation.}}
Figure~\ref{fig:geometry} compares the visual quality of the underlying 3D geometry extracted by running Marching Cubes algorithms~\cite{lorensen1987marching}. While StyleSDF~\cite{or2021stylesdf} generates decent appearances of the front face, the complete geometry of the head is noisy and broken. EG3D presents detailed geometry of front face and hair, but either with background concrete entangled (Figure~\ref{fig:tridisc_visual}(a)) or with a hollowed back head (Figure~\ref{fig:geometry}). 
In contrast, our model can consistently generate high-fidelity background-free 3D head geometry even with various hairstyles.

% \vspace{-0.1in}
\subsection{Quantitative Results}






To quantify the visual quality, fidelity, and diversity of the generated images, we employ Frechet Inception Distance~(FID)~\cite{heusel2017gans} of 50K real and fake image samples.  We measure the multi-view consistency using the identity similarity score (ID) by calculating the average Adaface~\cite{kim2022adaface} cosine similarity score 
from paired synthesized face images rendered from different camera poses. Note that this metric can only be applied to those images with detected facial landmarks. We assess mean square error~(MSE) to calculate the accuracy of the generated segmentation against the mask obtained with DeepLabV3 ResNet101 network~\cite{chen2017rethinking}. Table~\ref{tab:quantitative} compares these metrics across all baselines and our method. We observe that our model outperforms other baselines consistently from all perspectives. Refer to supplemental material for metrics definition and implementation details.

To evaluate the image quality at different views, we employ FID and Inception Score~(IS)~\cite{salimans2016improved} for synthesized images with only back poses~($|yaw| \geq 90^\circ$), front poses~($|yaw| < 90^\circ$), and all camera poses. FID measures on the similarity and diversity of real and fake image distributions while IS focuses more on the image quality itself. 
Our GAN model follows EG3D for the main backbone, where the tri-plane generator is conditioned on a camera pose. We observe that such a design leads to biased image synthesis quality toward the conditioning camera pose. Specifically, when conditioning on the front view, our generator achieves inferior quality for synthesizing the head images from the back, and vice versa. However, when calculating FID-all, the conditioning camera is always the same as the rendering view. Therefore the generator could still achieve an excellent FID-all score even though the quality of generated heads might degenerate in unseen views. Hence, the original FID metrics (FID-all and FID-front) can hardly thoroughly reflect the overall generation quality of full heads in $360^{\circ}$.
To alleviate this issue, we propose FID-back, where we condition on the front view but synthesize the images from the back. It leads to higher FID scores but reflects the quality in $360^\circ$ image synthesis better.

% Note that for FID-back and IS-back, we fix $c_{con}$ at front and randomly sample $c_{cam}$ from back poses to generate images while other metrics take identical $c$ randomly sampled from corresponding poses. This is because it is more intuitive to test the model's capability of `guessing' the appearance of side or back based on the front appearance. Such back images are with higher quality and more qualified for evaluating multi-view-consistency instead of random individual back images. 
We perform an ablation study on our method to quantitatively evaluate the efficacy of each individual component (Table~\ref{tab:ablation}). 
As shown in the second column, we notice a significant quality boost after adding the foreground-aware discrimination for all cases, compared with the original EG3D. That indicates the prior segmentation knowledge largely ease the network learning difficulty of 3D heads from in-the-wild image collections. Frontal face synthesis quality is comparable among all methods given the strong supervision from the large amount of well-aligned frontal images. However, for the back head, decoupling foreground and background largely improves the synthesis quality. In addition, changing tri-plane to tri-grid representation further enhances the image quality. With tri-discrimination, tri-grid, and camera self-adaption scheme altogether, PanoHead achieves the lowest FID-back and the highest IS for back head generation.
 As reflected in the row of run-time analysis, our novel component only introduces minor computation overhead, but with significant image synthesis quality improvements. 
 Note that the frontal image quality is superior to the back head, largely due to the significant learning difficulty in various hairstyles and unstructured back-head appearances. 
% For frontal only cases, tri-grid is not necessary since there is enough supervision from the large amount of frontal images, which decreases the projection ambiguity. 
% Similarly, well-aligned frontal images based on facial landmark makes self-adaptation merely useful. 
% In summary, purposed components of PanoHead substantially enhance 3D GANs' capabilities of $360^\circ$ head synthesis, especially for the back head.

% In addition, we conduct run-time analysis on EG3D and our method. Compared to EG3D, components in PanoHead significantly improve the image synthesis quality with minor computation overhead.


% \subsection{Ablation Studies}
% \thesis{Compare with original EG3D without all our modifications}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.\textwidth]{figures/geometry_updated.png}
    \caption{PanoHead achieves high-quality complete head geometry whereas StyleSDF~\cite{or2021stylesdf} and EG3D~\cite{chan2021efficient} produce 3D noises or hallowed heads.}
    \label{fig:geometry}
\end{figure}