\vspace{-0.1in}
\section{Related Work} \label{sec:related_work}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/overview2.pdf}
    \caption{
     Our framework consists of three main components: a foreground-aware generator \bb{G}, discriminator \bb{D}, and a neural renderer \bb{R}. A mapping network first maps latent code $z$ and conditioned camera pose $c_{con}$ into the intermediate latent code $w$. The generator \bb{G} then takes $w$ to obtain the 3D tri-grid representation features $f$. With $f$ and rendering camera pose $c_{cam}$, the neural renderer \bb{R} synthesizes super-resolved image $I^{+}$, bilinear-upsampled image $I$, and super-resolved mask $I^{m+}$. Finally, the foreground-aware tri-discriminator \bb{D} critiques ($I^+$, $I$, $I^{m+}$) along with real images. The data processing pipeline is shown in the right side. The real images are cropped with modified YOLO bounding boxes yet they often differ at scale and location due to lacking accurate facial landmarks. With the camera self-adaptation scheme, the rendering camera pose $c_{cam}$ is able to correct itself to generate images with consistent scale and location.
    }
    \label{fig:overview}
    \vspace{-0.1in}
\end{figure*}

\paragraph{3D Head Representation and Rendering.}
% first traditional head representation
% nerf representation

To represent 3D heads with diverse shapes and appearances, a line of work has targeted parametric textured mesh representation, such as 3D Morphable Model (3DMM)~\cite{blanz1999morphable,paysan20093d,booth2018large,booth20163d} for faces and FLAME head model~\cite{FLAME:SiggraphAsia2017}, learned from 3D scans. However, these parametric representations do not model photo-realistic appearance and geometry beyond the front face or skull. The neural implicit functions~\cite{xie2022neural} have recently emerged as powerful continuous and differential representations of 3D scenes. Among them, Neural Radiance Field (NeRF)~\cite{mildenhall2020nerf,barron2021mip} has been widely adopted in digital head modeling~\cite{hong2022headnerf,park2021nerfies,gafni2021dynamic,guo2021adnerf,raj2021pva,szymanowicz2022photo} due to its superiority in modeling complex scene details and synthesizing multiview images with inherited 3D consistency. In contrast to optimizing a person-specific neural radiance field from multiview images or temporal videos, our approach builds a generative NeRF from unstructured 2D monocular images. Recently implicit-explicit hybrid 3D representation has been explored for better efficiency~\cite{chan2021efficient,devries2021unconstrained,martel2021acorn}. 
Among them, the tri-plane formulation proposed in EG3D~\cite{chan2021efficient} demonstrates a highly efficient 3D scene representation with high-quality view-consistent image synthesis. The tri-plane representation can scale efficiently with resolution, enabling greater detail for equal capacity. Our tri-grid representation transforms the tri-plane representation into a more expressive space for better feature embedding in unconditional 3D head synthesis. 

% \vspace{-0.6in}
\paragraph{Single- or Few-view Supervised 3D GANs.}
Given the impressive progress of GANs on 2D image generation~\cite{goodfellow2014generative,karras2019style,Karras2020stylegan2,karras2021alias}, many studies have attempted to extend them to 3D-aware generation. These GANs aim to learn a generalizable 3D representation from 2D image a collections. For face synthesis,
Szabo~\etal~\cite{szabo2019unsupervised} first proposed using vertex position maps as the 3D representation to generate textured mesh outputs. Shi~\etal~\cite{shi2021lifting} proposed a self-supervised framework to convert 2D StyleGANs~\cite{karras2019style} into 3D generative models, although its generalizability is bounded by its base 2D StyleGAN. GRAF~\cite{schwarz2020graf} and pi-GAN~\cite{chan2021pi} are the first to integrate NeRF into 3D GANs. However, their performance is limited by the intense computation cost of forwarding and backwarding a complete NeRF. Many recent studies~\cite{deng2021gram,niemeyer2021giraffe,xue2022giraffe,gu2021stylenerf,or2021stylesdf,yariv2021volume,chan2021efficient,epigraf,gao2022get3d, enarf, schwarz2022voxgraf} have attempted to improve the efficiency and quality of such NeRF-based GANs. 
Specifically, EG3D~\cite{chan2021efficient}, which we build our work upon, introduces tri-plane representation that can leverage a 2D GAN backbone for generating efficient 3D representation and is shown outperforming other 3D representations~\cite{schwarz2022voxgraf}. 
% \todo{For instance, ENARF-GAN~\cite{enarf} employs a single discriminator that takes in an RGB image composed of synthesized foreground and background images using a dual-generated mask. In contrast, our tri-discriminator better ensures view-consistent high-resolution outputs.}  
Parallel to these works, another thread of studies~\cite{zhang2022avatargen,sun2022controllable,wu2022anifacegan,enarf} have been working on controllable 3D GANs that can manipulate the generated 3D faces or bodies. 
% \todo{Finally, although some studies propose to generate camera poses in an unsupervised fashion~\cite{niemeyer2021campari,devries2021unconstrained,shi2023learning},
% we are not aware of any study that uses a similar strategy to adjust imperfect camera poses.}

