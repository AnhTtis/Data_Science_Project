\vspace{-0.1in}
\section{Introduction}


Photo-realistic portrait image synthesis has been a continuous focus in computer vision and graphics, with a wide range of downstream applications in digital avatars, telepresence, immersive gaming, and many others. Recent advances in Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative} has demonstrated strikingly high image synthesis quality, indistinguishable from real photographs~\cite{karras2019style,Karras2020stylegan2,karras2021alias}. However, contemporary generative approaches operate on 2D convolutional networks without modeling the underlying 3D scenes. Therefore 3D consistency cannot be strictly enforced when synthesizing head images under various poses. 

To generate 3D heads with diverse shapes and appearances, traditional approaches require a parametric textured mesh model~\cite{blanz1999morphable,FLAME:SiggraphAsia2017} learned from large 3D scan collections. However, the rendered images lack fine details and have limited perceptual quality and expressiveness. With the advent of differentiable rendering and neural implicit representation~\cite{mildenhall2020nerf,xie2022neural}, conditional generative models have been developed to generate more realistic 3D-aware face images~\cite{tewari2018self,tran2019towards,hong2022headnerf,zhuang2021mofanerf}. However, those approaches typically require multi-view image or 3D scan supervision, which are hard to acquire and have limited appearance distribution as those are usually captured in controlled environments. 
%In contrast, our work aims to generate full 3D heads from in-the-wild images at low cost.
% Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative,karras2019style,karras2020analyzing,karras2021alias} have shown marvelous quality of RGB image synthesis, where photo-realistic images indistinguishable from real photographs can be generated. These models, however, can only generate 2D images and can not be naturally applied to 3D scenes.

% \thesis{Recent 3D-aware GANs~\cite{szabo2019unsupervised} made significant progress on 3D multi-view face generation. However, multi-view images or 3D geometry data are often required for supervision, which are scarce and hard to obtain.}...

% Specific to the 3D face and head synthesis problem, traditional methods generally use a ``generator'' that models the distribution of 3D scanned data. This generator could either be a linear module~\cite{blanz1999morphable,FLAME:SiggraphAsia2017}, a neural network that renders 3D faces~\cite{tewari2018self,tran2019towards} or a NeRF~\cite{hong2022headnerf,zhuang2021mofanerf}. The problem with this kind of methods is that they are limited by the 3D data and therefore can not synthesize photo-realistic in-the-wild images like those of 2D GANs.

3D-aware generative models have recently seen rapid progress, fueled by the integration of implicit neural representation in 3D scene modeling and Generative Adversarial Networks (GANs) for image synthesis~\cite{schwarz2020graf,chan2021pi,niemeyer2021giraffe,chan2021efficient,epigraf,or2021stylesdf,xue2022giraffe}. Among them, the seminal 3D GAN, EG3D~\cite{chan2021efficient}, demonstrates striking quality in view-consistent image synthesis, trained only from in-the-wild single-view image collections. 
However, these 3D GAN approaches are still limited to synthesis in near-frontal views.

In this paper, we propose \emph{PanoHead}, a novel 3D-aware GAN for high-quality full 3D head synthesis in $360^\circ$ trained from only in-the-wild unstructured images. Our model can synthesize \emph{consistent 3D heads viewable from all angles}, which is desirable by many immersive interaction scenarios such as digital avatars and telepresence. To the best of our knowledge, our method is \emph{the first} 3D GAN approach to achieve full 3D head synthesis in $360^\circ$.

% show view-consistent 3D-aware face synthesis without multi-view supervision by combining NeRF~\cite{mildenhall2020nerf} with adversarial loss~\cite{goodfellow2014generative}. However, these 3D GANs are still limited to near-frontal faces. Rendering from a large pose easily results in unnatural images and severe artifacts. This poses the question:
% % \begin{displayquote}
% \textit{What keeps 3D-aware GANs from synthesizing full-head images?}
% % \end{displayquote}
Extending 3D GAN frameworks such as EG3D~\cite{chan2021efficient} to full 3D head synthesis poses several significant technical challenges:
%Training a 3D GAN as EG3D~\cite{chan2021efficient} from augmented images with both frontal and backside views poses several significant challenges. 
Firstly, many 3D GANs~\cite{chan2021efficient, or2021stylesdf} cannot separate foreground and background, inducing 2.5D head geometry. The background, formulated typically as a wall structure, is entangled with the generated head in 3D and therefore prohibits rendering from large poses. We introduce a \emph{foreground-aware tri-discriminator} that jointly learns the decomposition of the foreground head in 3D space by distilling the prior knowledge in 2D image segmentation. 

Secondly, while being compact and efficient, current hybrid 3D scene representations, like tri-plane~\cite{chan2021efficient}, introduce strong projection ambiguity for 360$^{\circ}$ camera poses, resulting in `mirrored face' on the back head. 
To address the issue, we present a novel 3D \emph{tri-grid volume representation} that disentangles the frontal features with the back head while maintaining the efficiency of tri-plane representations. 

Lastly, obtaining well-estimated camera extrinsics of in-the-wild back head images for 3D GANs training is extremely difficult. Moreover, an image alignment gap exists between these and frontal images with detectable facial landmarks. The alignment gap causes a noisy appearance and unappealing head geometry. Thus, we propose a novel \emph{two-stage alignment scheme} that robustly aligns images from any view consistently. This step decreases the learning difficulty of 3D GANs significantly. In particular, we propose a camera self-adaptation module that dynamically adjusts the positions of rendering cameras to accommodate the alignment drifts in the back head images.
% we found it extremely difficult to estimate the ground-truth camera extrinsics of in-the-wild back head images and an alignment gap exists between these images and frontal faces. Thus, we propose a  

Our framework substantially enhances the 3D GANs' capabilities to adapt to in-the-wild full head images from arbitrary views, as shown in Figure~\ref{fig:teaser}. The resulting 3D GAN not only generates high-fidelity 360$^{\circ}$ RGB images and geometry, but also achieves better quantitative metrics than state-of-the-art methods. With our model, we showcase compelling 3D full head reconstruction from a single monocular-view image, enabling easily accessible 3D portrait creation. 

In summary, our main contributions are as follows:

\begin{itemize}[leftmargin=10pt]
    \vspace{-0.05in}
    \item The first 3D GAN framework that enables view-consistent and high-fidelity full-head image synthesis with detailed geometry, renderable in $360^\circ$. We demonstrate our approach in high-quality monocular 3D head reconstruction from in-the-wild images.
    \vspace{-0.05in}
    \item A novel tri-grid formulation that balances efficiency and expressiveness in representing 3D $360^\circ$ head scenes.
    \vspace{-0.05in}
    \item A foreground-aware tri-discriminator that disentangles 3D foreground head modeling from 2D background synthesis.
    % \vspace{-0.05in}
    \item A novel two-stage image alignment scheme that adaptively accommodates  imperfect camera poses and misaligned image cropping, enabling training of 3D GANs from in-the-wild images with wide camera pose distribution.
\end{itemize}

