% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bm}

% Sizhe Packages
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{multirow}
% \usepackage[colorlinks,linkcolor=blue]{hyperref}
% \usepackage[colorlinks,linkcolor=blue]{hyperref}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

% \usepackage{subfig}
\usepackage{enumitem}
\usepackage{array}
\usepackage{makecell}
\usepackage{adjustbox}
\usepackage{hhline}
\usepackage{tabularx}
% Table float box with bottom caption, box width adjusted to content
\usepackage{floatrow}
\usepackage{csquotes}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]

% red/blue/green/black/white/cyan/magenta/yellowã€‚
\newcommand{\rev}[1]{\textcolor{blue}{#1}} % Need to revise
\newcommand{\todo}[1]{\textcolor{brown}{#1}} % Todo
\newcommand{\thesis}[1]{\textcolor{cyan}{#1}} % thesis statement

\definecolor{commentcolor}{RGB}{110,154,155}   % define comment color
\newcommand{\PyComment}[1]{\ttfamily\footnotesize\textcolor{commentcolor}{\# #1}}  % add a "#" before the input text "#1"
\newcommand{\PyCode}[1]{\ttfamily\footnotesize\textcolor{black}{#1}} % \ttfamily is the code font
\newcommand{\bb}[1]{\boldsymbol{\mathbf{#1}}}

% % for footnote
% \makeatletter
% \def\blfootnote{\xdef\@thefnmark{}\@footnotetext}
% \makeatother

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{9731} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360$^{\circ}$}

% \makeatletter
%     \renewcommand\AB@affilsepx{ \hphantom{---} \protect\Affilfont}
% \makeatother


% \author[1,2]{Sizhe An \thanks{Work was done during an internship at ByteDance Inc.}}
% \author[2]{Hongyi Xu}
% \author[2]{Yichun Shi}
% \author[2]{Guoxian Song}
% \author[1]{Umit Y. Ogras}
% \author[2]{Linjie Luo}
% \affiliation[1]{University of Wisconsin-Madison}
% \affiliation[2]{ByteDance Inc.}

% \thanks{Work was done during an internship at ByteDance}
\author{Sizhe An$^{1,2}$ \quad  Hongyi Xu$^{1}$ \quad Yichun Shi$^{1}$ \quad Guoxian Song$^1$ \quad Umit Y. Ogras$^{2}$ \quad Linjie Luo$^1$\\
$^1$ByteDance Inc. \quad\quad $^2$University of Wisconsin-Madison\\
% Institution1 address\\
% {\tt\small {sizhe.an@wisc.edu, \{hongyixu, yichun.shi, guoxian.song\}@bytedance.com} \\ \tt\small{uogras@wisc.edu, linjie.luo@bytedance.com}}
}


% \author{Sizhe An\footnotemark\\
% University of Wisconsin-Madison\\
% % 1415 Engineering Hall, Madison, WI\\
% % {\tt\small sizhe.an@wisc.edu}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Hongyi Xu\\
% Bytedance Inc.\\
% % First line of institution2 address\\
% % {\tt\small hongyixu@bytedance.com}
% \and
% Yichun Shi\\
% Bytedance Inc.\\
% % First line of institution2 address\\
% % {\tt\small hongyixu@bytedance.com}
% \and
% Guoxian Song\\
% Bytedance Inc.\\
% % First line of institution2 address\\
% % {\tt\small hongyixu@bytedance.com}
% \and
% Umit Y. Ogras\\
% University of Wisconsin-Madison\\
% % First line of institution2 address\\
% % {\tt\small hongyixu@bytedance.com}
% \and
% Linjie Luo\\
% Bytedance Inc.\\
% % First line of institution2 address\\
% % {\tt\small hongyixu@bytedance.com}
% }


\twocolumn[{
\renewcommand\twocolumn[1][]{#1}
\maketitle
\begin{center}
    \captionsetup{type=figure}
    \includegraphics[width=1.0\textwidth]{figures/teaser2.pdf}
    \captionof{figure}{Our PanoHead enables $360^\circ$ view-consistent photo-realistic full-head image synthesis with high-fidelity geometry, enabling authentic 3D portraits creation from a single-view image.}
    % Project page: \url{https://sizhean.github.io/panohead}}
    \label{fig:teaser}
    % Original Biden image from \url{https://commons.wikimedia.org/wiki/File:President_of_the_United_States_Joe_Biden_(2021).jpg}, under Creative Commons Attribution 3.0 United States.
\end{center}
}]

% \maketitle





%%%%%%%%% ABSTRACT
\begin{abstract}
%Unsupervised generation of high-fidelity view-consistent human images is an important task of great consequence for 3D digital portrait creation and to various AR/VR applications. Especially delivering an immersive experience demands photo-realistic avatars of humans viewable from arbitrary poses. 
Synthesis and reconstruction of 3D human head has gained increasing interests in computer vision and computer graphics recently. Existing state-of-the-art 3D generative adversarial networks (GANs) for 3D human head synthesis are either limited to near-frontal views or hard to preserve 3D consistency in large view angles. We propose PanoHead, the first 3D-aware generative model that enables high-quality view-consistent image synthesis of full heads in $360^\circ$ with diverse appearance and detailed geometry using only in-the-wild unstructured images for training. At its core, we lift up the representation power of recent 3D GANs and bridge the data alignment gap when training from in-the-wild images with widely distributed views. Specifically, we propose a novel two-stage self-adaptive image alignment for robust 3D GAN training. We further introduce a tri-grid neural volume representation that effectively addresses front-face and back-head feature entanglement rooted in the widely-adopted tri-plane formulation. 
Our method instills prior knowledge of 2D image segmentation in adversarial learning of 3D neural scene structures, enabling compositable head synthesis in diverse backgrounds. Benefiting from these designs, our method significantly outperforms previous 3D GANs, generating high-quality 3D heads with accurate geometry and diverse appearances, even with long wavy and afro hairstyles, renderable from arbitrary poses. Furthermore, we show that our system can reconstruct full 3D heads from single input images for personalized realistic 3D avatars.

% Unsupervised generation of multi-view consistent images and 3D shapes using only single-view 2D images has been a challenging research topic. Recently, a few 3D GAN models such as StyleSDF and EG3D are able to synthesize high-resolution multi-view consistent images and 3D geometry by leveraging 2D CNN generators and neural rendering. However, due to the limited training data distribution, the previous approaches can only synthesize front faces with small pose variations and with fully entangled foreground and background. To address these challenges, we propose the first 3D-aware full-head GAN model learnt from monocular image datasets with a wide range of camera pose distribution. We introduce a self-adapting data alignment scheme, a novel 3d neural volume representation and a foreground-aware dual discriminator. To the end, we demonstrate 3D-aware full-head synthesis and reconstruction (-180 to 180 yaw) with FFHQ and in-house datasets.

\end{abstract}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\blfootnote{Project page: \url{https://sizhean.github.io/panohead}}

% \footnotetext{Work done during an internship at Bytedance}
%%%%%%%%% BODY TEXT

\input{sections/introduction}

\input{sections/related.tex}

\input{sections/method}

\input{sections/experiments}

\input{sections/application.tex}

\input{sections/discussion.tex}

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
