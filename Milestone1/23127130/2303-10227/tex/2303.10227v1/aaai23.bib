% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{binary_tree_endoding,
 author = {Shiv, Vighnesh and Quirk, Chris},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Novel positional encodings to enable tree-based transformers},
 url = {https://proceedings.neurips.cc/paper/2019/file/6e0917469214d8fbd8c517dcdc6b8dcf-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{dqn,
  author    = {Volodymyr Mnih and
               Koray Kavukcuoglu and
               David Silver and
               Alex Graves and
               Ioannis Antonoglou and
               Daan Wierstra and
               Martin A. Riedmiller},
  title     = {Playing Atari with Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1312.5602},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.5602},
}

@inproceedings{sbert,
    title = "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1410",
    doi = "10.18653/v1/D19-1410",
    pages = "3982--3992",
    abstract = "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
}

@article{thulke2021adapting,
  author    = {David Thulke and
               Nico Daheim and
               Christian Dugast and
               Hermann Ney},
  title     = {Adapting Document-Grounded Dialog Systems to Spoken Conversations
               using Data Augmentation and a Noisy Channel Model},
  journal   = {CoRR},
  volume    = {abs/2112.08844},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.08844},
}


@inproceedings{vu2021online,
  author    = {Thi{-}Ly Vu and
               Zin Tun Kyaw and
               Chng Eng Siong and
               Rafael E. Banchs},
  editor    = {Erik Marchi and
               Sabato Marco Siniscalchi and
               Sandro Cumani and
               Valerio Mario Salerno and
               Haizhou Li},
  title     = {Online {FAQ} Chatbot for Customer Support},
  booktitle = {Increasing Naturalness and Flexibility in Spoken Dialogue Interaction
               - 10th International Workshop on Spoken Dialogue Systems, {IWSDS}
               2019, Syracuse, Sicily, Italy, 24-26 April 2019},
  series    = {Lecture Notes in Electrical Engineering},
  volume    = {714},
  pages     = {251--259},
  publisher = {Springer},
  year      = {2019},
  url       = {https://doi.org/10.1007/978-981-15-9323-9\_21},
  doi       = {10.1007/978-981-15-9323-9\_21},
  timestamp = {Sun, 14 Mar 2021 11:48:46 +0100},
  biburl    = {https://dblp.org/rec/conf/iwsds/VuKSB19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gordon2020learning,
    title = "{L}earning {D}ialog {P}olicies from {W}eak {D}emonstrations",
    author = "Gordon-Hall, Gabriel  and
      Gorinski, Philip John  and
      Cohen, Shay B.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.129",
    doi = "10.18653/v1/2020.acl-main.129",
    pages = "1394--1405",
    abstract = "Deep reinforcement learning is a promising approach to training a dialog manager, but current methods struggle with the large state and action spaces of multi-domain dialog systems. Building upon Deep Q-learning from Demonstrations (DQfD), an algorithm that scores highly in difficult Atari games, we leverage dialog data to guide the agent to successfully respond to a user{'}s requests. We make progressively fewer assumptions about the data needed, using labeled, reduced-labeled, and even unlabeled data to train expert demonstrators. We introduce Reinforced Fine-tune Learning, an extension to DQfD, enabling us to overcome the domain gap between the datasets and the environment. Experiments in a challenging multi-domain dialog system framework validate our approaches, and get high success rates even when trained on out-of-domain data.",
}
@inproceedings{10.1145/3159652.3160590,
author = {Crook, Paul A. and Marin, Alex and Agarwal, Vipul and Anderson, Samantha and Jang, Ohyoung and Lanewala, Aliasgar and Tangirala, Karthik and Zitouni, Imed},
title = {Conversational Semantic Search: Looking Beyond Web Search, Q\&A and Dialog Systems},
year = {2018},
isbn = {9781450355810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159652.3160590},
doi = {10.1145/3159652.3160590},
abstract = {User expectations of web search are changing. They are expecting search engines to answer questions, to be more conversational, and to offer means to complete tasks on their behalf. At the same time, to increase the breadth of tasks that personal digital assistants (PDAs), such as Microsoft»s Cortana or Amazon»s Alexa, are capable of, PDAs need to better utilize information about the world, a significant amount of which is available in the knowledge bases and answers built for search engines. It thus seems likely that the underlying systems that power web search and PDAs will converge. This demonstration presents a system that merges elements of traditional multi-turn dialog systems with web based question answering. This demo focuses on the automatic composition of semantic functional units, Botlets, to generate responses to user»s natural language (NL) queries. We show that such a system can be trained to combine information from search engine answers with PDA tasks to enable new user experiences.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
pages = {763–766},
numpages = {4},
keywords = {conversational agents, dialog systems, personal digital assistants, question answering, semantic search},
location = {Marina Del Rey, CA, USA},
series = {WSDM '18}
}

@inproceedings{shukla2020conversation,
    title = "{C}onversation {L}earner - A Machine Teaching Tool for Building Dialog Managers for Task-Oriented Dialog Systems",
    author = "Shukla, Swadheen  and
      Liden, Lars  and
      Shayandeh, Shahin  and
      Kamal, Eslam  and
      Li, Jinchao  and
      Mazzola, Matt  and
      Park, Thomas  and
      Peng, Baolin  and
      Gao, Jianfeng",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-demos.39",
    doi = "10.18653/v1/2020.acl-demos.39",
    pages = "343--349",
    abstract = "Traditionally, industry solutions for building a task-oriented dialog system have relied on helping dialog authors define rule-based dialog managers, represented as dialog flows. While dialog flows are intuitively interpretable and good for simple scenarios, they fall short of performance in terms of the flexibility needed to handle complex dialogs. On the other hand, purely machine-learned models can handle complex dialogs, but they are considered to be black boxes and require large amounts of training data. In this demonstration, we showcase Conversation Learner, a machine teaching tool for building dialog managers. It combines the best of both approaches by enabling dialog authors to create a dialog flow using familiar tools, converting the dialog flow into a parametric model (e.g., neural networks), and allowing dialog authors to improve the dialog manager (i.e., the parametric model) over time by leveraging user-system dialog logs as training data through a machine teaching interface.",
}

@inproceedings{razumovskaia2019incorporating,
  title={Incorporating rules into end-to-end dialog systems},
  author={Razumovskaia, Evgeniia and Eskenazi, Maxine},
  booktitle={Proc. 3rd NeurIPS Workshop on Conversational AI, Vancouver, Canada},
  pages={1--11},
  year={2019}
}

@inproceedings{williams2008integrating,
  title={Integrating expert knowledge into POMDP optimization for spoken dialog systems},
  author={Williams, Jason D},
  booktitle={Proceedings of the AAAI-08 Workshop on Advancements in POMDP Solvers},
  volume={2},
  number={3},
  pages={25},
  year={2008}
}

@inproceedings{williams-etal-2017-hybrid,
    title = "Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning",
    author = "Williams, Jason D.  and
      Asadi, Kavosh  and
      Zweig, Geoffrey",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1062",
    doi = "10.18653/v1/P17-1062",
    pages = "665--677",
    abstract = "End-to-end learning of recurrent neural networks (RNNs) is an attractive solution for dialog systems; however, current techniques are data-intensive and require thousands of dialogs to learn simple behaviors. We introduce Hybrid Code Networks (HCNs), which combine an RNN with domain-specific knowledge encoded as software and system action templates. Compared to existing end-to-end approaches, HCNs considerably reduce the amount of training data required, while retaining the key benefit of inferring a latent representation of dialog state. In addition, HCNs can be optimized with supervised learning, reinforcement learning, or a mixture of both. HCNs attain state-of-the-art performance on the bAbI dialog dataset (Bordes and Weston, 2016), and outperform two commercially deployed customer-facing dialog systems at our company.",
}

@article{reddy-etal-2019-coqa,
    title = "{C}o{QA}: A Conversational Question Answering Challenge",
    author = "Reddy, Siva  and
      Chen, Danqi  and
      Manning, Christopher D.",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1016",
    doi = "10.1162/tacl_a_00266",
    pages = "249--266",
    abstract = "Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets (e.g., coreference and pragmatic reasoning). We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4{\%}, which is 23.4 points behind human performance (88.8{\%}), indicating that there is ample room for improvement. We present CoQA as a challenge to the community at https://stanfordnlp.github.io/coqa.",
}

@inproceedings{qu2019bert,
  author    = {Chen Qu and
               Liu Yang and
               Minghui Qiu and
               W. Bruce Croft and
               Yongfeng Zhang and
               Mohit Iyyer},
  editor    = {Benjamin Piwowarski and
               Max Chevalier and
               {\'{E}}ric Gaussier and
               Yoelle Maarek and
               Jian{-}Yun Nie and
               Falk Scholer},
  title     = {{BERT} with History Answer Embedding for Conversational Question Answering},
  booktitle = {Proceedings of the 42nd International {ACM} {SIGIR} Conference on
               Research and Development in Information Retrieval, {SIGIR} 2019, Paris,
               France, July 21-25, 2019},
  pages     = {1133--1136},
  publisher = {{ACM}},
  year      = {2019},
  url       = {https://doi.org/10.1145/3331184.3331341},
  doi       = {10.1145/3331184.3331341},
}

@inproceedings{qu2019attentive,
  author    = {Chen Qu and
               Liu Yang and
               Minghui Qiu and
               Yongfeng Zhang and
               Cen Chen and
               W. Bruce Croft and
               Mohit Iyyer},
  editor    = {Wenwu Zhu and
               Dacheng Tao and
               Xueqi Cheng and
               Peng Cui and
               Elke A. Rundensteiner and
               David Carmel and
               Qi He and
               Jeffrey Xu Yu},
  title     = {Attentive History Selection for Conversational Question Answering},
  booktitle = {Proceedings of the 28th {ACM} International Conference on Information
               and Knowledge Management, {CIKM} 2019, Beijing, China, November 3-7,
               2019},
  pages     = {1391--1400},
  publisher = {{ACM}},
  year      = {2019},
  url       = {https://doi.org/10.1145/3357384.3357905},
  doi       = {10.1145/3357384.3357905},
}

@inproceedings{choi-etal-2018-quac,
    title = "{Q}u{AC}: Question Answering in Context",
    author = "Choi, Eunsol  and
      He, He  and
      Iyyer, Mohit  and
      Yatskar, Mark  and
      Yih, Wen-tau  and
      Choi, Yejin  and
      Liang, Percy  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1241",
    doi = "10.18653/v1/D18-1241",
    pages = "2174--2184",
    abstract = "We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at \url{http://quac.ai}.",
}

@inproceedings{zamani2020mimics,
  author    = {Hamed Zamani and
               Gord Lueck and
               Everest Chen and
               Rodolfo Quispe and
               Flint Luu and
               Nick Craswell},
  editor    = {Mathieu d'Aquin and
               Stefan Dietze and
               Claudia Hauff and
               Edward Curry and
               Philippe Cudr{\'{e}}{-}Mauroux},
  title     = {{MIMICS:} {A} Large-Scale Data Collection for Search Clarification},
  booktitle = {{CIKM} '20: The 29th {ACM} International Conference on Information
               and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020},
  pages     = {3189--3196},
  publisher = {{ACM}},
  year      = {2020},
  url       = {https://doi.org/10.1145/3340531.3412772},
  doi       = {10.1145/3340531.3412772},
}

@inproceedings{aliannejadi2019asking,
  author    = {Mohammad Aliannejadi and
               Hamed Zamani and
               Fabio Crestani and
               W. Bruce Croft},
  editor    = {Benjamin Piwowarski and
               Max Chevalier and
               {\'{E}}ric Gaussier and
               Yoelle Maarek and
               Jian{-}Yun Nie and
               Falk Scholer},
  title     = {Asking Clarifying Questions in Open-Domain Information-Seeking Conversations},
  booktitle = {Proceedings of the 42nd International {ACM} {SIGIR} Conference on
               Research and Development in Information Retrieval, {SIGIR} 2019, Paris,
               France, July 21-25, 2019},
  pages     = {475--484},
  publisher = {{ACM}},
  year      = {2019},
  url       = {https://doi.org/10.1145/3331184.3331265},
  doi       = {10.1145/3331184.3331265},
}

@article{aliannejadi2020convai3,
  author    = {Mohammad Aliannejadi and
               Julia Kiseleva and
               Aleksandr Chuklin and
               Jeff Dalton and
               Mikhail S. Burtsev},
  title     = {ConvAI3: Generating Clarifying Questions for Open-Domain Dialogue
               Systems (ClariQ)},
  journal   = {CoRR},
  volume    = {abs/2009.11352},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.11352},
  eprinttype = {arXiv},
  eprint    = {2009.11352},
}

@inproceedings{kumar2020ranking,
  author    = {Vaibhav Kumar and
               Vikas Raunak and
               Jamie Callan},
  editor    = {Mathieu d'Aquin and
               Stefan Dietze and
               Claudia Hauff and
               Edward Curry and
               Philippe Cudr{\'{e}}{-}Mauroux},
  title     = {Ranking Clarification Questions via Natural Language Inference},
  booktitle = {{CIKM} '20: The 29th {ACM} International Conference on Information
               and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020},
  pages     = {2093--2096},
  publisher = {{ACM}},
  year      = {2020},
  url       = {https://doi.org/10.1145/3340531.3412137},
  doi       = {10.1145/3340531.3412137},
}

@inproceedings{bi2021asking,
  author    = {Keping Bi and
               Qingyao Ai and
               W. Bruce Croft},
  editor    = {Faegheh Hasibi and
               Yi Fang and
               Akiko Aizawa},
  title     = {Asking Clarifying Questions Based on Negative Feedback in Conversational
               Search},
  booktitle = {{ICTIR} '21: The 2021 {ACM} {SIGIR} International Conference on the
               Theory of Information Retrieval, Virtual Event, Canada, July 11, 2021},
  pages     = {157--166},
  publisher = {{ACM}},
  year      = {2021},
  url       = {https://doi.org/10.1145/3471158.3472232},
  doi       = {10.1145/3471158.3472232},
}

@article{kim2022knowledge,
  title={Knowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations Track at DSTC10},
  author={Kim, Seokhwan and Liu, Yang and Jin, Di and Papangelis, Alexandros and Hedayatnia, Behnam and Gopalakrishnan, Karthik and Hakkani-T{\"u}r, Dilek},
  year={2022}
}

@INPROCEEDINGS{9736289,
  author    = {Muhammad Hasan Maqbool and
               Luxun Xu and
               A. B. Siddique and
               Niloofar Montazeri and
               Vagelis Hristidis and
               Hassan Foroosh},
  title     = {Zero-label Anaphora Resolution for Off-Script User Queries in Goal-Oriented
               Dialog Systems},
  booktitle = {16th {IEEE} International Conference on Semantic Computing, {ICSC}
               2022, Laguna Hills, CA, USA, January 26-28, 2022},
  pages     = {217--224},
  publisher = {{IEEE}},
  year      = {2022},
  url       = {https://doi.org/10.1109/ICSC52841.2022.00043},
  doi       = {10.1109/ICSC52841.2022.00043},
}


@inproceedings{actor-critique,
author = {Montazeralghaem, Ali and Allan, James and Thomas, Philip S.},
title = {Large-Scale Interactive Conversational Recommendation System Using Actor-Critic Framework},
year = {2021},
isbn = {9781450384582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460231.3474271},
doi = {10.1145/3460231.3474271},
abstract = {We propose AC-CRS, a novel conversational recommendation system based on reinforcement learning that better models user interaction compared to prior work. Interactive recommender systems expect an initial request from a user and then iterate by asking questions or recommending potential matching items, continuing until some stopping criterion is achieved. Unlike most existing works that stop as soon as an item is recommended, we model the more realistic expectation that the interaction will continue if the item is not appropriate. Using this process, AC-CRS is able to support a more flexible conversation with users. Unlike existing models, AC-CRS is able to estimate a value for each question in the conversation to make sure that questions asked by the agent are relevant to the target item (i.e., user needs). We also model the possibility that the system could suggest more than one item in a given turn, allowing it to take advantage of screen space if it is present. AC-CRS also better accommodates the massive space of items that a real-world recommender system must handle. Experiments on real-world user purchasing data show the effectiveness of our model in terms of standard evaluation measures such as NDCG.},
booktitle = {Fifteenth ACM Conference on Recommender Systems},
pages = {220–229},
numpages = {10},
keywords = {Intelligent Assistants, Deep Reinforcement Learning, Actor-Critic, Conversational Recommender System},
location = {Amsterdam, Netherlands},
series = {RecSys '21}
}


@inproceedings{gbert,
    title = "{G}erman{'}s Next Language Model",
    author = {Chan, Branden  and
      Schweter, Stefan  and
      M{\"o}ller, Timo},
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.598",
    doi = "10.18653/v1/2020.coling-main.598",
    pages = "6788--6796",
    abstract = "In this work we present the experiments which lead to the creation of our BERT and ELECTRA based German language models, GBERT and GELECTRA. By varying the input training data, model size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size. We adopt an evaluation driven approach in training these models and our results indicate that both adding more data and utilizing WWM improve model performance. By benchmarking against existing German models, we show that these models are the best German models to date. All trained models will be made publicly available to the research community.",
}

@inproceedings{liang2018hierarchical,
  author    = {Weiri Liang and
               Meng Yang},
  editor    = {De{-}Shuang Huang and
               Kang{-}Hyun Jo and
               Xiaolong Zhang},
  title     = {Hierarchical Hybrid Code Networks for Task-Oriented Dialogue},
  booktitle = {Intelligent Computing Theories and Application - 14th International
               Conference, {ICIC} 2018, Wuhan, China, August 15-18, 2018, Proceedings,
               Part {II}},
  series    = {Lecture Notes in Computer Science},
  volume    = {10955},
  pages     = {194--204},
  publisher = {Springer},
  year      = {2018},
  url       = {https://doi.org/10.1007/978-3-319-95933-7\_24},
  doi       = {10.1007/978-3-319-95933-7\_24},
}

@inproceedings{sentencetransformers,
  author    = {Nils Reimers and
               Iryna Gurevych},
  editor    = {Kentaro Inui and
               Jing Jiang and
               Vincent Ng and
               Xiaojun Wan},
  title     = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural
               Language Processing and the 9th International Joint Conference on
               Natural Language Processing, {EMNLP-IJCNLP} 2019, Hong Kong, China,
               November 3-7, 2019},
  pages     = {3980--3990},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/D19-1410},
  doi       = {10.18653/v1/D19-1410},
}

@inproceedings{duelingDQN,
  author    = {Ziyu Wang and
               Tom Schaul and
               Matteo Hessel and
               Hado van Hasselt and
               Marc Lanctot and
               Nando de Freitas},
  editor    = {Maria{-}Florina Balcan and
               Kilian Q. Weinberger},
  title     = {Dueling Network Architectures for Deep Reinforcement Learning},
  booktitle = {Proceedings of the 33nd International Conference on Machine Learning,
               {ICML} 2016, New York City, NY, USA, June 19-24, 2016},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {48},
  pages     = {1995--2003},
  publisher = {JMLR.org},
  year      = {2016},
  url       = {http://proceedings.mlr.press/v48/wangf16.html},
}

@inproceedings{doubleDQN,
  author    = {Hado van Hasselt and
               Arthur Guez and
               David Silver},
  editor    = {Dale Schuurmans and
               Michael P. Wellman},
  title     = {Deep Reinforcement Learning with Double Q-Learning},
  booktitle = {Proceedings of the Thirtieth {AAAI} Conference on Artificial Intelligence,
               February 12-17, 2016, Phoenix, Arizona, {USA}},
  pages     = {2094--2100},
  publisher = {{AAAI} Press},
  year      = {2016},
  url       = {http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12389},
}
@article{hindsightExperienceReplay,
  author    = {Marcin Andrychowicz and
               Dwight Crow and
               Alex Ray and
               Jonas Schneider and
               Rachel Fong and
               Peter Welinder and
               Bob McGrew and
               Josh Tobin and
               Pieter Abbeel and
               Wojciech Zaremba},
  editor    = {Isabelle Guyon and
               Ulrike von Luxburg and
               Samy Bengio and
               Hanna M. Wallach and
               Rob Fergus and
               S. V. N. Vishwanathan and
               Roman Garnett},
  title     = {Hindsight Experience Replay},
  booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
               on Neural Information Processing Systems 2017, December 4-9, 2017,
               Long Beach, CA, {USA}},
  pages     = {5048--5058},
  year      = {2017},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/453fadbd8a1a3af50a9df4df899537b5-Abstract.html},
}

@inproceedings{chen2019large,
  author    = {Haokun Chen and
               Xinyi Dai and
               Han Cai and
               Weinan Zhang and
               Xuejian Wang and
               Ruiming Tang and
               Yuzhou Zhang and
               Yong Yu},
  title     = {Large-Scale Interactive Recommendation with Tree-Structured Policy
               Gradient},
  booktitle = {The Thirty-Third {AAAI} Conference on Artificial Intelligence, {AAAI}
               2019, The Thirty-First Innovative Applications of Artificial Intelligence
               Conference, {IAAI} 2019, The Ninth {AAAI} Symposium on Educational
               Advances in Artificial Intelligence, {EAAI} 2019, Honolulu, Hawaii,
               USA, January 27 - February 1, 2019},
  pages     = {3312--3320},
  publisher = {{AAAI} Press},
  year      = {2019},
  url       = {https://doi.org/10.1609/aaai.v33i01.33013312},
  doi       = {10.1609/aaai.v33i01.33013312},
}

@article{zhang2020recent,
  title={Recent advances and challenges in task-oriented dialog systems},
  author={Zhang, Zheng and Takanobu, Ryuichi and Zhu, Qi and Huang, MinLie and Zhu, XiaoYan},
  journal={Science China Technological Sciences},
  volume={63},
  number={10},
  pages={2011--2027},
  year={2020},
  publisher={Springer},
  url = {https://link.springer.com/article/10.1007/s11431-020-1692-3},
  doi = {10.1007/s11431-020-1692-3}
}

@inproceedings{raghu-etal-2021-end,
    title = "End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs",
    author = "Raghu, Dinesh  and
      Agarwal, Shantanu  and
      Joshi, Sachindra  and
      {Mausam}",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.357",
    doi = "10.18653/v1/2021.emnlp-main.357",
    pages = "4348--4366",
    abstract = "We propose a novel problem within end-to-end learning of task oriented dialogs (TOD), in which the dialog system mimics a troubleshooting agent who helps a user by diagnosing their problem (e.g., car not starting). Such dialogs are grounded in domain-specific flowcharts, which the agent is supposed to follow during the conversation. Our task exposes novel technical challenges for neural TOD, such as grounding an utterance to the flowchart without explicit annotation, referring to additional manual pages when user asks a clarification question, and ability to follow unseen flowcharts at test time. We release a dataset (FLODIAL) consisting of 2,738 dialogs grounded on 12 different troubleshooting flowcharts. We also design a neural model, FLONET, which uses a retrieval-augmented generation architecture to train the dialog agent. Our experiments find that FLONET can do zero-shot transfer to unseen flowcharts, and sets a strong baseline for future research.",
}

@article{vieillard2020munchausen,
  author    = {Nino Vieillard and
               Olivier Pietquin and
               Matthieu Geist},
  editor    = {Hugo Larochelle and
               Marc'Aurelio Ranzato and
               Raia Hadsell and
               Maria{-}Florina Balcan and
               Hsuan{-}Tien Lin},
  title     = {Munchausen Reinforcement Learning},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/2c6a0bae0f071cbbf0bb3d5b11d90a82-Abstract.html},

}
@article{schaul2015prioritized,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015}
}

@inproceedings{dai-etal-2020-learning,
    title = "Learning Low-Resource End-To-End Goal-Oriented Dialog for Fast and Reliable System Deployment",
    author = "Dai, Yinpei  and
      Li, Hangyu  and
      Tang, Chengguang  and
      Li, Yongbin  and
      Sun, Jian  and
      Zhu, Xiaodan",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.57",
    doi = "10.18653/v1/2020.acl-main.57",
    pages = "609--618",
    abstract = "Existing end-to-end dialog systems perform less effectively when data is scarce. To obtain an acceptable success in real-life online services with only a handful of training examples, both fast adaptability and reliable performance are highly desirable for dialog systems. In this paper, we propose the Meta-Dialog System (MDS), which combines the advantages of both meta-learning approaches and human-machine collaboration. We evaluate our methods on a new extended-bAbI dataset and a transformed MultiWOZ dataset for low-resource goal-oriented dialog learning. Experimental results show that MDS significantly outperforms non-meta-learning baselines and can achieve more than 90{\%} per-turn accuracies with only 10 dialogs on the extended-bAbI dataset.",
}

@article{lapreplay,
  author    = {Scott Fujimoto and
               David Meger and
               Doina Precup},
  editor    = {Hugo Larochelle and
               Marc'Aurelio Ranzato and
               Raia Hadsell and
               Maria{-}Florina Balcan and
               Hsuan{-}Tien Lin},
  title     = {An Equivalence between Loss Functions and Non-Uniform Sampling in
               Experience Replay},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/a3bf6e4db673b6449c2f7d13ee6ec9c0-Abstract.html},
}

@inproceedings{caruana1993multitask,
  author    = {Rich Caruana},
  editor    = {Paul E. Utgoff},
  title     = {Multitask Learning: {A} Knowledge-Based Source of Inductive Bias},
  booktitle = {Machine Learning, Proceedings of the Tenth International Conference,
               University of Massachusetts, Amherst, MA, USA, June 27-29, 1993},
  pages     = {41--48},
  publisher = {Morgan Kaufmann},
  year      = {1993},
  url       = {https://doi.org/10.1016/b978-1-55860-307-3.50012-5},
  doi       = {10.1016/b978-1-55860-307-3.50012-5},
}

@inproceedings{budzianowski-etal-2018-multiwoz,
    title = "{M}ulti{WOZ} - A Large-Scale Multi-Domain {W}izard-of-{O}z Dataset for Task-Oriented Dialogue Modelling",
    author = "Budzianowski, Pawe{\l}  and
      Wen, Tsung-Hsien  and
      Tseng, Bo-Hsiang  and
      Casanueva, I{\~n}igo  and
      Ultes, Stefan  and
      Ramadan, Osman  and
      Ga{\v{s}}i{\'c}, Milica",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1547",
    doi = "10.18653/v1/D18-1547",
    pages = "5016--5026",
    abstract = "Even though machine learning has become the major scene in dialogue research community, the real breakthrough has been blocked by the scale of data available.To address this fundamental obstacle, we introduce the Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics.At a size of 10k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora.The contribution of this work apart from the open-sourced dataset is two-fold:firstly, a detailed description of the data collection procedure along with a summary of data structure and analysis is provided. The proposed data-collection pipeline is entirely based on crowd-sourcing without the need of hiring professional annotators;secondly, a set of benchmark results of belief tracking, dialogue act and response generation is reported, which shows the usability of the data and sets a baseline for future studies.",
}

@INPROCEEDINGS{4960683,
  author={Suendermann, D. and Evanini, K. and Liscombe, J. and Hunter, P. and Dayanidhi, K. and Pieraccini, R.},
  booktitle={2009 IEEE International Conference on Acoustics, Speech and Signal Processing}, 
  title={From rule-based to statistical grammars: Continuous improvement of large-scale spoken dialog systems}, 
  year={2009},
  volume={},
  number={},
  pages={4713-4716},
  doi={10.1109/ICASSP.2009.4960683}}

@inproceedings{cohen2020back,
  author    = {Philip R. Cohen},
  title     = {Back to the Future for Dialogue Research},
  booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
               2020, The Thirty-Second Innovative Applications of Artificial Intelligence
               Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
               Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
               February 7-12, 2020},
  pages     = {13514--13519},
  publisher = {{AAAI} Press},
  year      = {2020},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/7073},
}

@inproceedings{10.1145/3209978.3210183,
author = {Gao, Jianfeng and Galley, Michel and Li, Lihong},
title = {Neural Approaches to Conversational AI},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210183},
doi = {10.1145/3209978.3210183},
abstract = {This tutorial surveys neural approaches to conversational AI that were developed in the last few years. We group conversational systems into three categories: (1) question answering agents, (2) task-oriented dialogue agents, and (3) social bots. For each category, we present a review of state-of-the-art neural approaches, draw the connection between neural approaches and traditional symbolic approaches, and discuss the progress we have made and challenges we are facing, using specific systems and models as case studies.},
booktitle = {The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
pages = {1371–1374},
numpages = {4},
keywords = {task-oriented dialogue, chatbot, dialogue, question answering, conversation},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@article{zaib2022conversational,
  title={Conversational question answering: A survey},
  author={Zaib, Munazza and Zhang, Wei Emma and Sheng, Quan Z and Mahmood, Adnan and Zhang, Yang},
  journal={Knowledge and Information Systems},
  pages={1--45},
  year={2022},
  publisher={Springer},
  doi = {10.1007/s10115-022-01744-y}
}

@inproceedings{treeencoding,
 author = {Shiv, Vighnesh and Quirk, Chris},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Novel positional encodings to enable tree-based transformers},
 url = {https://proceedings.neurips.cc/paper/2019/file/6e0917469214d8fbd8c517dcdc6b8dcf-Paper.pdf},
 volume = {32},
 year = {2019}
}


@article{liu2019roberta,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  eprinttype = {arXiv},
  eprint    = {1907.11692},
}

@article{10.1145/1066078.1066079, 
    author = {Wu, Chung-Hsien and Yeh, Jui-Feng and Chen, Ming-Jun}, 
    title = {Domain-Specific FAQ Retrieval Using Independent Aspects}, 
    year = {2005}, issue_date = {March 2005}, 
    publisher = {Association for Computing Machinery}, 
    address = {New York, NY, USA}, 
    volume = {4}, 
    number = {1}, 
    issn = {1530-0226}, 
    url = {https://doi.org/10.1145/1066078.1066079}, 
    doi = {10.1145/1066078.1066079}, abstract = {This investigation presents an approach to domain-specific FAQ (frequently-asked question) retrieval using independent aspects. The data analysis classifies the questions in the collected QA (question-answer) pairs into ten question types in accordance with question stems. The answers in the QA pairs are then paragraphed and clustered using latent semantic analysis and the K-means algorithm. For semantic representation of the aspects, a domain-specific ontology is constructed based on WordNet and HowNet. A probabilistic mixture model is then used to interpret the query and QA pairs based on independent aspects; hence the retrieval process can be viewed as the maximum likelihood estimation problem. The expectation-maximization (EM) algorithm is employed to estimate the optimal mixing weights in the probabilistic mixture model. Experimental results indicate that the proposed approach outperformed the FAQ-Finder system in medical FAQ retrieval.}, 
    journal = {ACM Transactions on Asian Language Information Processing}, 
    month = {mar}, 
    pages = {1–17}, 
    numpages = {17}, 
    keywords = {probabilistic mixture model, question-answering, FAQ retrieval, information retrieval,             latent semantic analysis, natural language processing, ontology}
}

@inproceedings{thakur2021beir,
  title={BEIR: A heterogenous benchmark for zero-shot evaluation of information retrieval models},
  author={Thakur, Nandan and Reimers, Nils and R{\"u}ckl{\'e}, Andreas and Srivastava, Abhishek and Gurevych, Iryna},
  booktitle={35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks},
  url = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/65b9eea6e1cc6bb9f0cd2a47751a186f-Paper-round2.pdf},
  year={2021}
}

@article{kingma2014adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
}

@inproceedings{huggingface,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}