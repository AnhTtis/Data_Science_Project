% \documentclass{article}
\documentclass[10pt,twocolumn,letterpaper]{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2020}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% fix arXiv error
\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage[style=numeric]{biblatex}
\addbibresource{references.bib}
\addbibresource{more_references.bib}

\usepackage{afterpage}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{lipsum}
\usepackage{booktabs}
\usepackage[labelformat=simple]{subcaption}
\usepackage{array}
\renewcommand\thesubfigure{(\alph{subfigure})}
\usepackage{pgfgantt}
\usepackage{multirow}
\usepackage{array,booktabs}
\usepackage{tabularray-2021}

\usepackage{svg}
\usepackage{pgfplots}
\usepackage{multirow, tabularx}
\usepackage{mathtools}
\usepackage{cancel}
\usepackage{adjustbox}
\usepackage{xcolor}

\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{colortbl}
\usepackage{stackengine}
\usepackage{listings}

\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\green}[1]{{\color{green}#1}}
\newcommand{\black}[1]{{\color{black}#1}}
\newcommand{\blue}[1]{{\color{blue}#1}}
\definecolor{light_red}{rgb}{0.921,0.78039,0.83137}
\definecolor{dark_red}{rgb}{0.729,0.459,0.459}
\newcommand{\highlight}[1]{{\color{dark_red}#1}}

\newcommand{\lpnorm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\sidenote}[1]{{\color{red}#1}}
\newcommand{\Real}{{\rm I\!R}}
\newcommand{\p}[1]{{\noindent \textbf{#1}}}
\newcommand\xrowht[2][0]{\addstackgap[.5\dimexpr#2\relax]{\vphantom{#1}}}
\newcommand{\renyi}{{r\'enyi}}
\newcommand{\Renyi}{{R\'enyi}}
\newcommand{\functional}{function\;}

\definecolor{codegreen}{rgb}{0,0.5,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1.0,1.0,1.0}
\definecolor{link}{rgb}{1.0,1.0,1.0}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=1,
    float=tp,
  floatplacement=tbp
}
\lstset{style=mystyle}

\renewcommand*\thelstnumber{\arabic{lstnumber}:}
\renewcommand{\lstlistingname}{Algorithm}
\DeclareCaptionFormat{mylst}{\hrule#1#2#3\hrule}
\captionsetup[lstlisting]{format=mylst,labelfont=bf,singlelinecheck=off,labelsep=space}
\newcommand\redcancel[2][red]{\renewcommand\CancelColor{\color{#1}}\cancel{#2}}

\definecolor{light_orange}{RGB}{252,219,191}
\definecolor{dark_orange}{RGB}{237,139,55}
\definecolor{light_green}{RGB}{111,193,174}
\definecolor{dark_green}{RGB}{67,154,134}
\definecolor{cyan}{RGB}{123,250,241}

\newcommand{\roy}[1]{\textbf{[Roy]}  \; \red{#1} }
\newcommand{\comment}[1]{\red{#1} }

\definecolor{xarch}{rgb}{1.0, 0.9, 0.9}
\definecolor{sarch}{rgb}{0.94, 0.97, 1.0}
\definecolor{light_green}{rgb}{0.72, 0.85, 0.85}
\definecolor{light_red}{rgb}{1.0, 0.9, 0.9}

\usepackage{tikz}
\usepackage{tabularx}
\usetikzlibrary{fit}

\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand*{\myalign}[2]{\multicolumn{1}{#1}{#2}}

% introduce a new counter for counting the nodes needed for circling
\newcounter{nodecount}
\newcommand\tabnode[1]{\addtocounter{nodecount}{1} \tikz \node  (\arabic{nodecount}) {#1};}

\tikzstyle{every picture}+=[remember picture,baseline]
\tikzstyle{every node}+=[anchor=base,minimum width=0.4cm,align=center,text depth=.25ex,outer sep=1.5pt]
\tikzstyle{every path}+=[thick, rounded corners]

\iccvfinalcopy % *** Uncomment this line for the final submission

% \def\iccvPaperID{9625} % *** Enter the ICCV Paper ID here
% \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

% \title{Rethinking and enhancing knowledge distillation using interpolated representations}
% \title{Deep dive into knowledge distillation}
% \title{[WT] Revisiting the Foundations of Knowledge Distillation through a Comprehensive Analysis of Key Design Decisions}
% \title{Understanding the underlying principles of knowledge distillation}
\title{A closer look at the training dynamics of knowledge distillation}
% \\ through the projector training dynamics}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Roy Miles \\
  Imperial College London \\
  {\tt\small r.miles18@imperial.ac.uk} \\
  % examples of more authors
  \and
  Krystian Mikolajczyk \\
  Imperial College London \\
  % Address \\
  {\tt\small k.mikolajczyk@imperial.ac.uk}
}

\begin{document}

\maketitle

\begin{abstract}
    In this paper we revisit the efficacy of knowledge distillation as a function matching and metric learning problem. In doing so we verify three important design decisions, namely the normalisation, soft maximum function, and projection layers as key ingredients. We theoretically show that the projector implicitly encodes information on past examples, enabling relational gradients for the student. We then show that the normalisation of representations is tightly coupled with the training dynamics of this projector, which can have a large impact on the students performance. Finally, we show that a simple soft maximum function can be used to address any significant capacity gap problems. Experimental results on various benchmark datasets demonstrate that using these insights can lead to superior or comparable performance to state-of-the-art knowledge distillation techniques, despite being much more computationally efficient. In particular, we obtain these results across image classification (CIFAR100 and ImageNet), object detection (COCO2017), and on more difficult distillation objectives, such as training data efficient transformers, whereby we attain a $77.2$\% top-1 accuracy with DeiT-Ti on ImageNet.
    
    % extreme label corruption, and the few-shot regimes.
    % In particular, we obtain state-of-the-art on cross-inductive bias distillation on ImageNet $\uparrow 2.5\%$, Object detection with YOLO on COCO2017 $\uparrow 0.2$ mAP, and CIFAR100 $\uparrow 1.3\%$ average, and $8X\%$ top-1 accuracy for a ResNet-50 model on ImageNet.   
\end{abstract}

\section{Introduction}
\label{sec:intro}
Deep neural networks have achieved remarkable success in various applications, ranging from computer vision~\cite{Krizhevsky2012ImageNetNetworks} to natural language processing~\cite{Vaswani2017AttentionNeed}. However, the high computational cost and memory requirements of deep models have limited their deployment in resource-constrained environments. Knowledge distillation is a popular technique to address this problem through transferring the knowledge of a large teacher model to that of a smaller student model. This technique involves training the student to imitate the output of the teacher, either by directly minimizing the difference between intermediate features or by minimizing the Kullback-Leibler (KL) divergence between their soft predictions. Although knowledge distillation has shown to be very effective, there are still some limitations related to the computational and memory overheads in constructing and evaluating the losses, as well as an insufficient theoretical explanation for the underlying core principles.

To overcome these limitations, we revisit knowledge distillation from both a function matching and metric learning perspective. We perform an extensive ablation of three important components of knowledge distillation, namely the distance metric, normalisation, and projector network. Alongside this ablation we provide a theoretical perspective and unification of these design principles through exploring the underlying training dynamics. Finally, we extend these principles to a few large scale vision tasks, whereby we achieve comparable or improved performance over state-of-the-art. The most significant result of which pertains to the data-efficient training of transformers, whereby a performance gain of 2.2\% is achieved over the best-performing distillation method that are designed explicitly for this task.
Our main contributions can be summarised as follows.
\begin{enumerate}
    \item We explore three distinct design principles from knowledge distillation, namely the projection, normalisation, and distance function. In doing so we demonstrate their unification and coupling with each other, both through analytical means and by observing the training dynamics.
    \item We show that a projection layer implicitly encodes relational information from previous samples. Using this knowledge we can remove the need to explicitly construct correlation matrices or memory banks that will inevitably incur a significant memory overhead.
    \item We apply the three design principles to image classification, object detection, and data efficient training of transformers to achieve competitive or improved performance over state-of-the-art.
    
\end{enumerate}

%We expect that these id and should in fact be more commonly adopted than standard KL divergence practice.
% \begin{itemize}
%     \item This paper highlights that distillation is a form of function matching and demonstrates how a simple metric can be used to transfer knowledge between different tasks.
%     \item We introduce a matching loss between the predictions of interpolated hidden states to improve the robustness and convergence of knowledge distillation.
%     \item The experimental results demonstrate the superiority of the proposed distillation pipeline approach over state-of-the-art knowledge distillation techniques across a range of tasks and distillation settings.
% \end{itemize}

\begin{figure}[ht]
    \centering
    \resizebox{1.\columnwidth}{!}{%
    \includegraphics[width=1.\linewidth]{figures/distillation_overview_v2.pdf}
    }
    \caption{\textbf{Training pipeline for knowledge distillation on a classification task.} Left and right show representation distillation with and without the addition of a logit distillation loss respectively. For clarity, the normalisation of arguments for the representation loss $D$ have been omitted.}
    \vspace{-2em}
    \label{fig:distillation_overview}
\end{figure}

\vspace{-0.5em}
\section{Related Work}
\label{sec:related_work}

\paragraph{Knowledge Distillation}
Knowledge distillation is the process of transferring the knowledge from a large, complex model to a smaller, simpler model. Its usage was originally proposed in the context of image classification~\cite{Hinton2015DistillingNetwork} whereby the soft teacher predictions would encode relational information between classes. Spherical KD~\cite{Guo2020ReducingDistillation} extended this idea by re-scaling the logits, prime aware adaptive distillation~\cite{Zhang2020Prime-AwareDistillation} introduced an adaptive weighting strategy, while DKD~\cite{Zhao2022DecoupledDistillation} proposed to decouple the original formulation into target class and non-target class probabilities.

Hinted losses~\cite{Romero2015FitNets:Nets} were a natural extension of the logit-based approach whereby the intermediate feature maps are used as hints for the student. Attention transfer~\cite{Zagoruyko2019PayingTransfer} then proposed to re-weight this loss using spatial attention maps. ReviewKD~\cite{Chen2021DistillingReview} addressed the problem relating to the arbitrary selection of layers by aggregating information across all the layers using trainable attention blocks. Neuron selectivity transfer~\cite{Huang2017LikeTransfer}, similarity-preserving
KD~\cite{Tung2019Similarity-preservingDistillation}, and relational KD~\cite{Park2019RelationalDistillation} construct relational batch and feature matrices that can be used as inputs for the distillation losses. Similarly FSP matrices~\cite{Yim2017ALearning} were proposed to extract the relational information through a residual block. In contrast to this theme, we show that a simple projection layer can implicitly capture most relational information, thus removing the need to construct any expensive relational structures.

Representation distillation was originally proposed alongside a contrastive based loss~\cite{Tian2019ContrastiveDistillation} and has since been extended using a Wasserstein distance~\cite{Chen2020WassersteinDistillation}, information theory~\cite{Miles2022InformationDistillation}, graph theory~\cite{Ma2022DistillingAlignment}, and complementary gradient information~\cite{Zhu2021ComplementaryDistillation}. Distillation also been empirically shown to benefit from longer training schedules and more data-augmentation~\cite{Beyer2022KnowledgeConsistent}, which is similarly observed with HSAKD~\cite{Yang2021HierarchicalDistillation} and SSKD~\cite{Xu2020KnowledgeSelf-supervision}.
Distillation between CNNs and transformers has also been a very practically motivated task for data-efficient training~\cite{Touvron2021TrainingAttention} and has shown to benefit from an ensemble of teacher architectures~\cite{Ren2022Co-advise:Distillation}. However, we show that just a simple extension of some fundamental distillation design principles is much more effective.

\paragraph{Self-Supervised Learning}
Self-supervised learning (SSL) is an increasingly popular field of machine learning whereby a model is trained to learn a useful representation of unlabelled data. Its popularity has been driven by the increasing cost of manual labelling and has since been crucial for training large transformer models. Various pretext tasks have been proposed to learn these representations, such as image inpainting~\cite{He2022MaskedLearners}, colorization~\cite{Zhang2016ColorfulColorization}, or prediction of the rotation~\cite{Gidaris2018UnsupervisedRotations} or position of patches~\cite{Doersch2015UnsupervisedPrediction, Carlucci2019DomainPuzzles}. SimCLR~\cite{Chen2020ARepresentations} approached self-supervision using a contrastive loss with multi-view augmentation to define the positive and negative pairs. They found a large memory bank of negative representations was necessary to achieve good performance, but would incur a significant memory overhead. MoCo~\cite{He2020MomentumLearning} extending this work with a momentum encoder, which was subsequently extended by MoCov2~\cite{Chen2020ImprovedLearning} and MoCov3~\cite{Chen2021AnTransformers}.
% that would then introduce various incremental design changes to improve the performance.

Asymmetric architectures were proposed as an alternative to contrastive learning, whereby no negative samples are needed. Most noticeable works in this area are BYOL~\cite{Grill2020BootstrapLearning} and SimSiam~\cite{Chen2021ExploringLearning} which both use stop gradients to avoid representation collapse. DirectPred~\cite{Tian2021UnderstandingPairs} provided a theoretical understanding of this non-contrastive SSL setting. They introduced a series of conceptual insights into the functional roles of many crucial ingredients used. In doing so they derived a simple linear predictor with competitive performance to some much more complex predictor architectures. In our work we explore knowledge distillation from a similar perspective as DirectPred but observe some unique observations and results pertaining to this distillation setting.

Feature decorrelation is another approach to SSL that avoids the need for negative pairs to address representation collapse. Barlow twins~\cite{Zbontar2021BarlowReduction} proposed to compute a cross-view correlation matrix and minimise its distance to the identity. This loss was later decomposed into a variance, invariance, and correlation regularisation terms in VICReg~\cite{Bardes2022VICReg:Learning}. Both contrastive learning and feature decorrelation have since been extended to dense prediction tasks~\cite{Bardes2022VICRegL:Features} and unified with knowledge distillation~\cite{Miles2023MobileVOS:Distillation}.

\begin{figure}
\centering
\includegraphics[width=1.\linewidth]{figures/correlation.pdf}
\caption{\textbf{Correlation between input-output features using different projector architectures.} All projector architectures considered will gradually decorrelate the input-output features. Although this decorrelation is attributed to the layer destroying irrelevant information, it can degrade the efficacy of distilling through to the student backbone.}
\label{fig:input_output_decorrelation}
\end{figure}

\section{Revisiting knowledge distillation}
\label{sec:understanding_kd}

Knowledge distillation (KD) is a technique used to transfer knowledge from a large, powerful model (teacher) to a smaller, less powerful one (student). In the classification setting, it can be done using the soft teacher predictions as pseudo-labels for the student. Unfortunately, this approach does not trivially generalise to non-classification tasks~\cite{Liu2019StructuredSegmentation} and the classifier may collapse a lot of information~\cite{Tishby2015DeepPrinciple} that can be useful for distillation.
%
Another approach is to use feature maps from the earlier layers for distillation~\cite{Romero2015FitNets:Nets, Zagoruyko2019PayingTransfer}, however, its usage presents two primary challenges: the difficulty in ensuring consistency across different architectures~\cite{Chen2021DistillingReview} and the potential degradation in the student's downstream performance for cases where the inductive biases of the two networks differ~\cite{Tian2019ContrastiveDistillation}
%
A compromise, which strikes a balance between the two approaches  discussed above, is to distill the representation directly before the output space. Representation distillation has been successfully adopted in past works~\cite{Tian2019ContrastiveDistillation, Zhu2021ComplementaryDistillation, Miles2022InformationDistillation} and is the focus of this paper. The exact training framework used is described in figure \ref{fig:distillation_overview} alongside an extension to incorporates a logit distillation loss. The projection layer shown was originally used to simply match the student and teacher dimensions~\cite{Romero2015FitNets:Nets}, however, we will show that its role is much more important and it can lead to significant performance improvements even when the two feature dimensions match. The two representations are also typically both followed by some normalisation scheme as a way of appropriately scaling the gradients. However, we find this normalisation has a more interesting property in its relation to what information is encoded in the learned projector weights.

In this work we provide a theoretical perspective to motivate some simple and effective design choices for knowledge distillation. In contrast to the recent works~\cite{Tung2019Similarity-preservingDistillation, Miles2022InformationDistillation}, we show that an explicit construction of complex relational structures, such as feature kernels~\cite{He2022FeatureDistillation} is not necessary. In fact, most of this structure can learned implicitly through the tightly coupled interaction of a learnable projection layer and an appropriate normalisation scheme.
%
In the following we discuss the importance of multi-view augmentation in distillation. We subsequently investigate the training dynamics of the projection layer with the normalisation scheme.  We explore the impact and trade-offs that arise from the architecture design of the projector. Finally, we propose a simple modification to the distance metric to address issues arising from a large capacity gap between the student and teacher models. \\

\noindent\textbf{Importance of multi-view augmentation.}
Knowledge distillation can be viewed as a form of function matching. This perspective motivates the intuition behind long training schedules and more 
aggressive augmentations in an attempt to densely sample the domain. This principle has been extensively covered by Beyer \etal\cite{Beyer2022KnowledgeConsistent} and has motivated our modification to the CIFAR100 distillation benchmark provided by CRD~\cite{Tian2019ContrastiveDistillation} In doing so we introduce a multi-view augmentation strategy, from which we report our results and reproduce those from other recent distillation methods. These results can be seen in table \ref{table:cifar100} and show that significant improvements can be made to even standard logit distillation through introducing a wider set of augmentations. All other settings, such as the optimiser, learning rate schedule, and number of epochs, are maintained the same. \\

\begin{table}
    \centering
    % \parbox{.45\linewidth}{
    \centering
    \resizebox{\linewidth}{!}{%
    \input{tables/repulsive_force.tex}
    }
    \caption{\textbf{Ablating the importance in the choice of metric function} using a ResNet34 and a ResNet18 student on the ImageNet dataset. The loss modifications are highlighted in \highlight{red} and vanilla KD is provided as a reference.}
    \label{table:repulsive_force}
\end{table}

\noindent\textbf{The projection weights encode relational information from previous samples.} The projection layer plays a crucial role in KD as it provides an implicit encoding of previous samples and its weights can the capture the relational information needed to transfer information regarding the correlation between features. Table~\ref{table:repulsive_force} shows the difference a single linear projector layer makes when combined with a simple L2 loss.
This improvement suggests that this layers role in distillation can be described more concisely as being an encoder of essential information needed for the distillation loss itself. Most recent works propose a manual construction of some relational information to be used as part of a loss~\cite{Park2019RelationalDistillation, Tung2019Similarity-preservingDistillation}, however, we posit that an implicit and learnable approach is much more effective. To explore this phenomenon in more detail, we consider the update equations for the projector weights and its training dynamics. Without loss in generality, consider a simple L2 loss and a linear projection layer without any bias term.


\begin{figure*}
\centering
\begin{minipage}[b]{.32\textwidth}
\centering
\includegraphics[width=1.\textwidth]{figures/no_evolvement_singular_vals.pdf}
\caption*{(a) no normalisation.}
\end{minipage}
\hfill
\begin{minipage}[b]{.32\textwidth}
\centering
\includegraphics[width=1.\textwidth]{figures/l2_evolvement_singular_vals.pdf}
\caption*{(b) L2 normalisation.}
\end{minipage}
\hfill
\begin{minipage}[b]{.32\textwidth}
\centering
\includegraphics[width=1.\textwidth]{figures/bn_evolvement_singular_vals.pdf}
\caption*{(c) batch normalisation.}
\end{minipage}
\caption{\textbf{Evolution of singular values of the projection weights $\mathbf{W}_p$ under three different representation normalisation schemes.} The student is a Resnet-18, while the teacher is a ResNet-50. The three curves shows the evolution of singular values for the projector weights when the representations undergo no normalisation, L2 normalisation, and batch norm respectively.}
\vspace{-1em}
\label{fig:singular_values_evolution}
\end{figure*}

\begin{align}
    % D(\mathbf{W}_p) &= \frac{1}{2} \mathop{\mathbb{E}}_{x \sim p(x)}[ \lpnorm{ \mathbf{Z}_s\mathbf{W}_p^T - \mathbf{Z}_t }_2^2 ]
    D(\mathbf{W}_p) &= \frac{1}{2} \lpnorm{ \mathbf{Z}_s\mathbf{W}_p - \mathbf{Z}_t }_2^2
 \end{align}
%
Using the trace property of the Frobenius norm, we can express this loss as follows:
% \textit{Proof.} Note that
\begin{align}
    D(\mathbf{W}_p) &= \frac{1}{2}tr\left( \left(\mathbf{Z}_s\mathbf{W}_p - \mathbf{Z}_t\right)^T\left(\mathbf{Z}_s\mathbf{W}_p - \mathbf{Z}_t\right) \right) \\
    &= \frac{1}{2}tr( \mathbf{W}_p^T\mathbf{Z}_s^T\mathbf{Z}_s\mathbf{W}_p - \mathbf{Z}_t^T\mathbf{Z}_s\mathbf{W}_p \\ &\;\;\;\;\;\;\;\;\;\;\;\;-\mathbf{W}_p^T\mathbf{Z}_s^t\mathbf{Z}_t + \mathbf{Z}_t^T\mathbf{Z}_t)
\end{align}
%
Taking the derivative with respect to $\mathbf{W}_p$
\begin{align}
    \dot{\mathbf{W}_p} &= -\frac{\partial D(\mathbf{W}_p)}{\partial \mathbf{W}_p} \\
    &= -\mathbf{Z}_s^T\mathbf{Z}_s\mathbf{W}_p + \mathbf{Z}_s^T\mathbf{Z}_t
\end{align}
%
The update rule can then be simplified
%
\begin{equation}
    \boxed{\dot{\mathbf{W}_p} = \mathbf{C}_{st} - \mathbf{C}_s\mathbf{W}_p}
    \label{eqn:projector_update}
\end{equation}
%
where $\mathbf{C}_s = \mathbf{Z}_s^T\mathbf{Z}_s \in \Real^{d_s \times d_s}$ and $\mathbf{C}_{st} = \mathbf{Z}_s^T\mathbf{Z}_t \in \Real^{d_s \times d_t}$ denote self and cross correlation matrices that capture the relationship between features. Due to the capacity gap between the student network and the teacher network, there is no linear projection between these two representation spaces. Instead, the projector will converge on an approximate mapping that we later show is governed by the normalisation being employed. However, we note that, unlike in self-supervised learning, this solution will never be unstable since representation collapse is not possible - the teacher is frozen and we jointly trained with the ground truth. % To simplify the analysis of the training dynamics of the projector, we consider two fixed point solutions that derive from two different situations.

\underline{Whitened features:} consider using self-supervised learning in conjunction with distillation whereby the student features are whitened to have perfect decorrelation~\cite{Ermolov2020WhiteningLearning}, or alternatively, they are batch normalised and sufficiently regularised with a feature decorrelation term~\cite{Bardes2022VICReg:Learning}. In this setting, the fixed point solution for the projection weights will be symmetric and will capture the cross relationship between student and teacher features.

\begin{align}
    \mathbf{C}_{st} &- \mathbf{C}_s\mathbf{W}_p = 0  \;\;\; \text{where} \;\;\; \mathbf{C}_s = \mathbf{I} \\
    &\rightarrow \mathbf{W}_p = \mathbf{C}_{st}
\end{align}

% \underline{2. Batch normalisation:} consider a more typical setting where we batch normalise only the teachers representation $\mathbf{Z}_t$.

% \begin{align}
%     \mathbf{C}_{st} - \mathbf{C}_s\mathbf{W}_p &= 0 \\
%     \mathbf{Z}_s^T\mathbf{Z}_s\mathbf{W}_p &= \mathbf{Z}_s^T\mathbf{Z}_t
% \end{align}
%     Assuming invertibility, pre-multipling both sides by $(\mathbf{Z}_s^T)^{-1}$.
% \begin{align}
%     \mathbf{Z}_s\mathbf{W}_p &= \mathbf{Z}_t \\
%     \text{where} \;\; \mu(\mathbf{Z}_t)_i = 0 \;\; &\text{and} \;\; \sigma(\mathbf{Z}_t)_i = 1
% \end{align}

% Thus, if the representations are assumed to be Gaussian distributed, the projection weights will not encode any information about the teacher. We describe this as a degenerate case since there will be no distillation.

Other normalisation schemes, such as those that jointly normalise the projected features and the teacher features, will have a much more involved analysis but will unlikely provide any additional insight on the dynamics of training itself. Thus, we propose to explore the training trajectories of the projector weights singular values. This will help quantify how the projector is mapping the student features to the teachers space. We cover this in the next section along with additional insights into what is being learned and distilled. \\

\textbf{The choice of normalisation directly affects the training dynamics of $\mathbf{W}_p$.}
Equation \ref{eqn:projector_update} shows that the projector weights can encode relational information between the student and teacher's features. This suggests redundancy in explicitly constructing and updating a large memory bank of previous representations~\cite{Tian2019ContrastiveDistillation}. By considering a weight decay $\eta$ and a learning rate $\alpha_p$, the update equation can be given as follows:

\begin{align}
    \mathbf{W}_p &\rightarrow \mathbf{W}_p + \alpha_p\dot{\mathbf{W}_p} - \eta\mathbf{W}_p \\
    &= (1 - \eta)\mathbf{W}_p + \alpha_p\dot{\mathbf{W}_p}
\end{align}

By setting $\eta = \alpha_p$ we can see that the projection layer will reduce to a moving average of relational features, which is very similar to the momentum encoder used by CRD~\cite{Tian2019ContrastiveDistillation}. Other works suggest to extract relational information on-the-fly by constructing correlation or gram matrices~\cite{Miles2022InformationDistillation, Peng2019CorrelationDistillation}. We show that this is also not necessary and more complex information can be captured through careful design of the projector architecture. We also demonstrate that, in general, the use of a projector will scales much more favourably for larger batch sizes and feature dimensions. We also note that the handcrafted design of kernel functions~\cite{Joshi2021OnNetworks, He2022FeatureDistillation, Miles2022InformationDistillation} may not generalise to large scale or complex real-world datasets.

\begin{table}[h]
    \centering
    \resizebox{.7\linewidth}{!}{%
    \input{tables/normalisation.tex}
    }
    \caption{\textbf{Ablating the importance in the normalisation} using a ResNet50 teacher and a ResNet18 student in the few-shot distillation setting on ImageNet.} 
    \label{table:normalisation}
\end{table}

From the results in table \ref{table:normalisation}, we observe that when fixing all other settings, the choice of normalisation can significantly affect the student's performance. To explore this in more detail, we consider the training trajectories of $\mathbf{W}_p$ under  different normalisation schemes. We find that the choice of normalisation not only controls the training dynamics, but also the fixed point solution (see equation \ref{eqn:projector_update}). We argue that the efficacy of distillation is dependent on how much relational information can be encoded in the learned weights and how much information is lost through the projection. To jointly evaluate these two properties we show the evolution of singular values of the projector weights during training. The results can be seen in figure \ref{fig:singular_values_evolution} and show that the better performing normalisation methods (table \ref{table:normalisation}) are shrinking far fewer singular values towards zero. This shrinkage can be described as collapsing the input along some dimension, which will induce some information loss and it is this information loss that degenerates the efficacy of distillation. %\vspace{-1em}

\begin{figure}[htp]
    \centering
    % \resizebox{1.\columnwidth}{!}{%
    \includegraphics[width=1.\linewidth]{figures/capacity_gap_v3.pdf}
    % \input{tables/capacity_gap.tex}
    % }
    \caption{\textbf{KD with a large capacity gap.} Top-1 accuracy (\%) using a ResNet18 student and a ResNet50 teacher. Experiments were both performed with a linear projection.}
    \vspace{-1em}
    \label{fig:capacity_gap}
\end{figure}

\vspace{1em}
\textbf{Larger projector networks learn to decorrelate the input-output features.}
One natural extension of the previous observations is to use a larger projector network to encode more information relevant for the distillation loss. Unfortunately, we observe that a trivial expansion of the projection architecture does not necessarily improve the students performance (see table \ref{table:projector}). To explain this observation we evaluate a measure of decorrelation between the input and output features of these projector networks. The results can be seen in figure \ref{fig:input_output_decorrelation} and we can see that the larger projectors learn to decorrelate more and more features from the input. This decorrelation can lead to the projector learning features that are not shared with the student backbone, which will subsequently diminish the effectiveness of distillation. These observations suggest that there is an inherent trade-off between the projector capacity and the efficacy of distillation. We do expect that some careful handcrafted design of the projector architectures could provide a more favourable trade-off, which agrees with the reported results using a projector ensemble~\cite{Chen2022ImprovedEnsemble}.
However, in favour of simplicity, we use a linear projector for all of the larger scale evaluations. \\

\begin{table*}[htp]
    \centering
    \input{tables/imagenet.tex}
    % \caption{\textbf{Top-1 and Top-5 error rates (\%) on ImageNet.} (a) MobileNet as student, ResNet50 as teacher. (b) ResNet18 as student, ResNet34 as teacher.}
    \caption{\textbf{Top-1 and Top-5 error rates (\%) on ImageNet.} ResNet18 as student, ResNet34 as teacher.}
    \vspace{-.5em}
    \label{table:imagenet}
\end{table*}

\textbf{The soft maximum function can address distilling across a large capacity gap.}
 When the capacity gap between the student and the teacher is large, representation distillation can become challenging. More specifically, the student network may have insufficient capacity to perfectly align these two spaces and in attempting to do so may degrade its downstream performance. To addresses this issue we explore the use of a soft maximum function which will soften the contribution of relatively close matches in a batch. In this way the loss can be adjusted to compensate for poorly aligned features which may arise when the capacity gap is large. The family of functions which share these properties can be more broadly defined through a property of their gradients. In favour of simplicity, we use the simple $LogSum$ function throughout our experiments.

 \begin{align} 
     % \frac{\partial f(x_i)}{\partial x_i} \propto \frac{f(x_i)}{\lpnorm{f'(\mathbf{x})}} \rightarrow f(\mathbf{x}) = log \sum_i x_i^{\alpha}
     f(\mathbf{x}) = log \sum_i x_i^{\alpha}
\end{align}

where $\alpha$ is a smoothing factor. We also note that other functions, such as the $LogSumExp$, with a temperature parameter $\tau$, have been used in SimCLR and CRD to a similar effect. Figure \ref{fig:capacity_gap} shows our empirical observations using a large capacity gap with and without the addition of both a normalisation and soft maximum operator. The improvements are much more significant in this case than for the small capacity gap (table \ref{table:repulsive_force}).

In table \ref{table:repulsive_force} we also perform an additional ablation on the impact of a few heuristically derived distance functions. We observe, on the large-scale ImageNet benchmark, there is little to no benefit in varying the underlying function, but significant improvements can be made by introducing a linear projection or a soft maximum operator in conjunction with normalisation. As discussed previously this latter improvement is amplified when the capacity gap between the two models increases.

\begin{table}[h]
    \centering
    % \resizebox{.7\linewidth}{!}{%
    \input{tables/projector.tex}
    % }
    \caption{\textbf{Ablating the importance in the projector} using a ResNet34 teacher and a ResNet18 student in the few-shot distillation setting on ImageNet.} 
    \vspace{-1.5em}
    \label{table:projector}
\end{table}

\begin{table*}[ht!]
    \centering
    \resizebox{\linewidth}{!}{%
    \input{tables/cifar100.tex}
    }
    \caption{\textbf{KD between Similar and Different Architectures.} Top-1 accuracy (\%) on CIFAR100. \textbf{Bold} is used to denote the best results. All reported models are trained using pairs of augmented images. Those reported in the \textcolor{dark_orange}{orange} box use RandAugment~\cite{Cubuk2020Randaugment:Space} strategy, while those in the \textcolor{dark_green}{green box} use pre-defined rotations, as used in SSKD. $^\dagger$ denotes reproduced results in a new augmentation setting using the authors provided code.}
    \label{table:cifar100} % Those reported in the \textcolor{dark_orange}{orange} box use RandAugment~\cite{Cubuk2020Randaugment:Space}.
\end{table*}

% main experiments
\section{Benchmark evaluation}
\label{sec:benchmark_evaluation}

\textbf{Implementation details.}
We follow the same training schedule as CRD~\cite{Tian2019ContrastiveDistillation} for both the CIFAR100 and ImageNet experiments. For the object detection, we use the same training schedule as ReviewKD~\cite{Chen2021DistillingReview}, while for the data efficient training we use the same as Co-Advice~\cite{Ren2022Co-advise:Distillation}. All experiments were performed on a single NVIDIA RTX A5000. When using batch normalisation for the representations, we removed the affine parameters and set $\epsilon = 0.0001$.

% (top) Single-model single-scale mAP on COCO val2017. All models are trained for 300 epochs.
\begin{table*}[ht]
    \centering
    \input{tables/yolo.tex}
    \caption{\textbf{Object detection on COCO.} (top) We report the standard COCO metric of mAP averaged over IOU thresholds in [0.5 : 0.05 : 0.95] along with the standard PASCAL VOC’s metric~\cite{Everingham2010TheChallenge}, which is the average mAP@0.5. (bottom) For the R-CNN results, we report the mAP and AP50 metrics to enable a consistent comparison with ReviewKD.}
    \label{table:yolo}
    \vspace{-1em}
\end{table*}

\subsection{Classification on CIFAR100 and ImageNet}
\label{sec:cifar_and_imagenet}
Experiments on the CIFAR-100 classification task~\cite{Krizhevsky2009LearningImages} consist of 60K 32×32 RGB images across
100 classes with a 5:1 training/testing split. Table \ref{table:cifar100} shows the results for several student-teacher pairings. To enable a fair evaluation, we have only included the methods that use the same teacher weights provided by SSKD\cite{Xu2020KnowledgeSelf-supervision}. In these experiments we use an MLP projector with a hidden size of 1024 and no additional KL divergence loss. We observe that not only is the choice of augmentation critical for good performance on this dataset, but applying our principles can attain state-of-the-art across most architecture pairs. The most significant improvements pertain to the cross-architecture experiments or where the capacity gap is large.

The ImageNet~\cite{Russakovsky2014ImageNetChallenge} classification uses 1.3 million images that are classified into 1000 distinct classes. The input size are set to 224 x 224, and we employed a typical augmentation procedure that includes cropping and horizontal flipping. We used the torchdistill library with the standard configuration, which involves 100 training epochs using SGD and an initial learning rate of 0.1, which is decreased by a factor of 10 at epochs 30, 60, and 90. The results can be seen in table \ref{table:imagenet} and although the choice of architectures is not in favour of our method since the capacity gap is small, we are still able to attain competitive performance. Other methods, such as ICKD~\cite{Liu2021ExploringDistillation} or SimKD~\cite{Chen2022KnowledgeClassifier} either modify the original training settings or modify the underlying student architecture, and so have been omitted from this evaluation.

\subsection{Object Detection on COCO}
\label{sec:kitti}

We extend the application of our method to object detection, whereby we employ a similar approach as used in the classification task by distilling the backbone output features of both the student and teacher networks. To evaluate the efficacy of our method, we conduct experiments on the widely-used COCO2017 dataset~\cite{Lin2014MicrosoftContext} under the same settings provided in ReviewKD. We then further demonstrate the applicability of our distillation principles on the more recent and efficient YOLOv5 model~\cite{Zhu2021TPH-YOLOv5:Scenarios}. In both cases we show improved student performance on the downstream task, whereby competitive performance is achieved with ReviewKD despite being significantly simpler and cheaper to integrate into a given distillation pipeline. Our method even outperforms FPGI~\cite{Wang2019DistillingImitationb}, which is directly designed for detection. 

\subsection{Data efficient training for transformers}
\label{sec:diet}
Transformers have emerged as a viable replacement for convolution-based neural networks (CNN) in visual learning tasks. Despite the promise of these models, their performance will suffer when there is insufficient training data available, such as in the case of ImageNet. DeiT~\cite{Touvron2021TrainingAttention} was the first to address this problem through the use of knowledge distillation. Although the authors show improved alignment with the teacher, we believe this fails to capture why less data is needed. \\ \\ We posit that the distillation process encourages the student to learn layers which are "more" translational equivariant in attempt to match the teacher's underlying function. Although this is the principle that motivates using an ensemble of teacher models with different inductive biases~\cite{Ren2022Co-advise:Distillation}, there is still no thorough demonstration on if the inductive biases are actually being transferred. In this section we attempt to address this gap by introducing a measure of equivariance. We show that applying our distillation principles to this task can achieve significant improvements over state-of-the-art as a result of transferring more of the translational equivariance.

The results of these experiments are shown in table \ref{table:data_efficient_training_transformers}. We use the exact same training methodology as co-advice~\cite{Ren2022Co-advise:Distillation} and choose to use batch normalisation, a linear projection layer, and $\alpha = 4$ as the parameters for distillation. We observe a significant improvement over both DeiT and CivT when the capacity gap is large. However, as the capacity gap diminishes, and the student approaches the same performance as the teacher, this improvement is much less significant. Multiple factors, such as the soft maximum function and the batch normalisation, will be contributing to this observed result. However, the explanation is more concisely described by the fact that our distillation loss transfers more translational equivariance, which is discussed in the next section.

\vspace{-1em}
\paragraph{Cross architecture distillation can implicitly transfer inductive biases.} CNNs use convolutions, which are spatially local operations, whereas transformers use self-attention, which are global operations. We expect that a benefit of this cross-architecture distillation setting is that the students learn to be "more" spatially equivariant in an attempt to match the teachers underlying function. It is this strong inductive bias that can reduce the amount of training data needed. A layer is translation equivariant if the following property holds:

\begin{align}
    \phi(T\mathbf{x}) = T\phi(\mathbf{x})
    \label{eqn:equivariance}
\end{align}

In other words, if we take a translated input $T\mathbf{x}$ and pass it through a layer $\phi$, the result should be equivalent to first applying $\phi$ to $\mathbf{x}$ and then performing the translation. A natural measure of equivariance can then be the difference between the left and right-hand side of this equation \ref{eqn:equivariance}.

\begin{align}
    \mu_{T}(\phi) = \lpnorm{\phi(T\mathbf{x}) - T\phi(\mathbf{x})}_2^2
\end{align}

We evaluate this measure on a block of self-attention layers by first removing the distillation and class tokens and then rolling the patch tokens to recover the spatial dimensions. This operation can then be performed on the input and output tensors before applying a translation. Figure \ref{fig:equivariance_measure} shows how this measure of equivariance evolves over the course of training. In general, we observe that the distilled models do in fact learn to preserve spatial locality between feature maps, which aligns with the function matching perspective for distillation (see figure \ref{fig:equivariance_measure}). We also find that our distillation method can transfer a lot more of this equivariance property to the student, where we find that the undistilled models have values of $\mu_T$ in the range of $[1.4, 1.9]$. Although CivT-S does learn this spatial locality to some extent, it is much less pronounced than our distilled model despite both models achieving similar accuracy. % Although feature map losses may be able to transfer even more of this equivariance, we believe it may degrade the benefit of using transformers in the first place.
% We see that although most self-attention blocks do preserve a lot of the spatial locality, there is still some global context between patch tokens that is still preserved.

\begin{figure}[htp]
\centering
\includegraphics[width=1.\linewidth]{figures/equivariance.pdf}
\caption{\textbf{Measure of translational equivariance of a DeiT-S transformer model trained with our distillation.} Reported measure was computed after every epoch and averaged over 256 images. We observe that distillation can indeed transfer inductive biases, which enables data-efficient training.}
\label{fig:equivariance_measure}
\end{figure}

\begin{table}[htp]
    \centering
    \input{tables/deit.tex}
    \caption{\textbf{Data-efficient training of transformers} on Imagenet with DeiT. All models are trained for 300 epochs using mixup~\cite{Zhang2017Mixup:Minimization}.}
    % Slightly worse teacher yields a student with 73.59\% at 250/300, while the better teacher yield 76.22\%. Thus better teacher is important.
    \label{table:data_efficient_training_transformers}
    \vspace{-1em}
\end{table}


% conclusion
\section{Conclusion}
\label{sec:conclusion}
In this paper, we revisited knowledge distillation from a function matching and metric learning perspective. We performed an extensive ablation on the most effective and scaleable components of distillation and in doing so provide a theoretical perspective to understand these observations. By extending these principles to a wide range of tasks, we achieve competitive or improved performance to state-of-the-art across image classification, object detection, and data efficient training of transformers. Although we do not aim to necessarily propose a new method, we do provide an effective theory to approach the capacity gap problem in distillation. We also significantly reduce the complexity and memory consumption's of existing pipelines through unifying a new underlying principle of distillation. Looking ahead to future research in this area, we expect to see the joint development of more sophisticated normalisation schemes and projection networks, which will encode more complex and informative features for the distillation process. 

\paragraph{Code Reproducibility.} To facilitate the reproducibility of results, we will release all the training code and pre-trained weights. The ImageNet experiments are also performed using the popular \textit{torchdistill}~\cite{Matsubara2020TorchdistillDistillation} framework, while the CIFAR100 and data-efficient training code is based on those provided by CRD~\cite{Tian2019ContrastiveDistillation} and co-advice~\cite{Ren2022Co-advise:Distillation}.

% \section{References}
% \label{sec:references}
\printbibliography

% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{references}
% }

\input{supplementary}

\end{document}