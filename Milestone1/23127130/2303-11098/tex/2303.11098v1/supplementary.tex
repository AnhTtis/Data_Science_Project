% % \documentclass{article}
% \documentclass[10pt,twocolumn,letterpaper]{article}

% % if you need to pass options to natbib, use, e.g.:
% %     \PassOptionsToPackage{numbers, compress}{natbib}
% % before loading neurips_2020

% % ready for submission
% % \usepackage{neurips_2020}

% % to compile a preprint version, e.g., for submission to arXiv, add add the
% % [preprint] option:
% %     \usepackage[preprint]{neurips_2020}

% % to compile a camera-ready version, add the [final] option, e.g.:
% %     \usepackage[final]{neurips_2020}

% % to avoid loading the natbib package, add option nonatbib:
% % \usepackage[nonatbib]{neurips_2020}

% \usepackage{iccv}
% \usepackage{times}
% \usepackage{epsfig}
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{listings}

% % Include other packages here, before hyperref.

% % If you comment hyperref and then uncomment it, you should delete
% % egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% % run, let it finish, and you should be clear).
% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \usepackage[utf8]{inputenc} % allow utf-8 input
% \usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
% \usepackage{url}            % simple URL typesetting
% \usepackage{booktabs}       % professional-quality tables
% \usepackage{amsfonts}       % blackboard math symbols
% \usepackage{nicefrac}       % compact symbols for 1/2, etc.
% \usepackage{microtype}      % microtypography

% % Additional packages
% \usepackage{listings}
% \usepackage{caption}
% \usepackage{algorithm}
% \usepackage{algpseudocode}
% \usepackage{xcolor, cancel}
% \usepackage{pifont}
% \usepackage{soul}
% \usepackage{pdflscape}

% \usepackage[backend=bibtex,style=numeric]{biblatex}
% \addbibresource{references.bib}
% \addbibresource{more_references.bib}

% \definecolor{codegreen}{rgb}{0,0.5,0}
% \definecolor{codegray}{rgb}{0.5,0.5,0.5}
% \definecolor{codepurple}{rgb}{0.58,0,0.82}
% \definecolor{backcolour}{rgb}{1.0,1.0,1.0}
% \definecolor{link}{rgb}{1.0,1.0,1.0}
% \lstdefinestyle{mystyle}{
%     backgroundcolor=\color{backcolour},   
%     commentstyle=\color{codegreen},
%     keywordstyle=\color{magenta},
%     numberstyle=\tiny\color{codegray},
%     stringstyle=\color{codepurple},
%     basicstyle=\ttfamily\footnotesize,
%     breakatwhitespace=false,         
%     breaklines=true,                 
%     keepspaces=true,                 
%     numbers=left,                    
%     numbersep=5pt,                  
%     showspaces=false,                
%     showstringspaces=false,
%     showtabs=false,                  
%     tabsize=1,
%     float=tp,
%   floatplacement=tbp
% }
% \lstset{style=mystyle}

% \renewcommand*\thelstnumber{\arabic{lstnumber}:}
% \renewcommand{\lstlistingname}{Algorithm}
% \DeclareCaptionFormat{mylst}{\hrule#1#2#3\hrule}
% \captionsetup[lstlisting]{format=mylst,labelfont=bf,singlelinecheck=off,labelsep=space}
% \newcommand\redcancel[2][red]{\renewcommand\CancelColor{\color{#1}}\cancel{#2}}

% \usepackage{afterpage}
% \usepackage{amsmath}
% \usepackage{amsthm}
% \usepackage{csquotes}
% \usepackage{enumitem}
% \usepackage{graphicx}
% \usepackage{lipsum}
% \usepackage{booktabs}
% \usepackage[labelformat=simple]{subcaption}
% \usepackage{array}
% \renewcommand\thesubfigure{(\alph{subfigure})}
% \usepackage{pgfgantt}
% \usepackage{multirow}
% \usepackage{array,booktabs}
% \usepackage{tabularray}

% \usepackage{svg}
% \usepackage{pgfplots}
% \usepackage{multirow, tabularx}
% \usepackage{mathtools}
% \usepackage{cancel}
% \usepackage{adjustbox}
% \usepackage{xcolor}

% \usepackage{epsfig}
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{colortbl}
% \usepackage{stackengine}

% \newcommand{\red}[1]{{\color{red}#1}}
% \newcommand{\green}[1]{{\color{green}#1}}
% \newcommand{\black}[1]{{\color{black}#1}}
% \newcommand{\blue}[1]{{\color{blue}#1}}
% \definecolor{light_red}{rgb}{0.921,0.78039,0.83137}
% \definecolor{dark_red}{rgb}{0.729,0.459,0.459}
% \newcommand{\highlight}[1]{{\color{dark_red}#1}}

% \newcommand{\lpnorm}[1]{\left\lVert #1 \right\rVert}
% \newcommand{\sidenote}[1]{{\color{red}#1}}
% \newcommand{\Real}{{\rm I\!R}}
% \newcommand{\p}[1]{{\noindent \textbf{#1}}}
% \newcommand\xrowht[2][0]{\addstackgap[.5\dimexpr#2\relax]{\vphantom{#1}}}
% \newcommand{\renyi}{{r\'enyi}}
% \newcommand{\Renyi}{{R\'enyi}}
% \newcommand{\functional}{function\;}

% \definecolor{light_orange}{RGB}{252,219,191}
% \definecolor{dark_orange}{RGB}{237,139,55}
% \definecolor{light_green}{RGB}{111,193,174}
% \definecolor{dark_green}{RGB}{67,154,134}
% \definecolor{cyan}{RGB}{123,250,241}

% \newcommand{\roy}[1]{\textbf{[Roy]}  \; \red{#1} }
% \newcommand{\comment}[1]{\red{#1} }

% \definecolor{xarch}{rgb}{1.0, 0.9, 0.9}
% \definecolor{sarch}{rgb}{0.94, 0.97, 1.0}
% \definecolor{light_green}{rgb}{0.72, 0.85, 0.85}
% \definecolor{light_red}{rgb}{1.0, 0.9, 0.9}

% \usepackage{tikz}
% \usepackage{tabularx}
% \usetikzlibrary{fit}

% \usepackage{amssymb}% http://ctan.org/pkg/amssymb
% \usepackage{pifont}% http://ctan.org/pkg/pifont

% \newcommand{\cmark}{\ding{51}}%
% \newcommand{\xmark}{\ding{55}}%
% \newcommand*{\myalign}[2]{\multicolumn{1}{#1}{#2}}

% % introduce a new counter for counting the nodes needed for circling
% \newcounter{nodecount}
% \newcommand\tabnode[1]{\addtocounter{nodecount}{1} \tikz \node  (\arabic{nodecount}) {#1};}

% \tikzstyle{every picture}+=[remember picture,baseline]
% \tikzstyle{every node}+=[anchor=base,minimum width=0.4cm,align=center,text depth=.25ex,outer sep=1.5pt]
% \tikzstyle{every path}+=[thick, rounded corners]

% % \iccvfinalcopy % *** Uncomment this line for the final submission

% \def\iccvPaperID{9625} % *** Enter the ICCV Paper ID here
% \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% % Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

% % \title{Rethinking and enhancing knowledge distillation using interpolated representations}
% % \title{Deep dive into knowledge distillation}
% % \title{[WT] Revisiting the Foundations of Knowledge Distillation through a Comprehensive Analysis of Key Design Decisions}
% % \title{Understanding the underlying principles of knowledge distillation}
% \title{A closer look at the training dynamics of knowledge distillation \\
% \textit{Supplementary material}}
% % \\ through the projector training dynamics}

% % The \author macro works with any number of authors. There are two commands
% % used to separate the names and addresses of multiple authors: \And and \AND.
% %
% % Using \And between authors leaves it to LaTeX to determine where to break the
% % lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% % authors names on the first line, and the last on the second line, try using
% % \AND instead of \And before the third author name.

% \author{%
%   David S.~Hippocampus\thanks{Use footnote for providing further information
%     about author (webpage, alternative address)---\emph{not} for acknowledging
%     funding agencies.} \\
%   Department of Computer Science\\
%   Cranberry-Lemon University\\
%   Pittsburgh, PA 15213 \\
%   \texttt{hippo@cs.cranberry-lemon.edu} \\
%   % examples of more authors
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \AND
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
% }

% \begin{document}

% \maketitle

% \section{Supplementary Material}
% \label{sec:suppl}

% \subsection{Limitation of CIFAR100}
% We provide an additional ablation on the choice of distance function on the CIFAR100 dataset. We observe that there is a different relationship between what distance function is used and the attainable accuracy than with the ablation on the larger scale ImageNet. This observation suggests a limitation in using CIFAR for ablation experiments since any ablated factors that show improvement on CIFAR may not translate to an improvement on ImageNet. Due to this observation, and in contrast to most works in the literature, we propose to perform all ablation experiments on ImageNet.

% \begin{table}[h]
%     \centering
%     \resizebox{\linewidth}{!}{%
%     \input{tables/repulsive_force_cifar.tex}
%     }
%     \caption{\textbf{Ablating the importance in the choice of metric function} using a ResNet34 and a ResNet18 student on ImageNet. For the CIFAR100 dataset we use X as the teacher and Y as the student.}
%     \label{table:repulsive_force_cifar}
% \end{table}

\newpage
\section{Supplementary Material}

\subsection{Few-shot distillation experiments}
Due to the limited compute resources available, we were unable to perform some ablation experiments using the full ImageNet training data. To address this concern, and to avoid using a poor surrogate dataset (i.e. CIFAR100), we propose a more difficult few-shot distillation setting. In this setting, the models are still trained and evaluated on the large-scale ImageNet dataset, but with only a subset of the training data available. This results in the distillation objective being much more challenging. In all of these experiments we sample the same 20\% of images from each class to ensure the overall class balance is maintained.

\subsection{Measure of translational equivariance}
In this section we provide more details on the proposed measure of translational equivariance and the motivation for its usage. In the cross inductive-bias distillation literature it is common to evaluate the effectiveness of a distillation pipeline through a measure of agreement between the student and the teacher~\cite{Touvron2021TrainingAttention, Ren2022Co-advise:Distillation}. We argue that this metric fails to encapsulate why one distillation method is more data efficient than another. We hypothesise that the most effective cross architecture distillation methods are those that transfer most of the inductive biases. To verify this claim, we introduce a measure of equivariance and show that a good distillation loss does indeed minimise this. For the convolution $\rightarrow$ transformer distillation setting, the most appropriate choice of measure is a measure of translational equivariance. This is because the teacher will have this equivariance explicitly enforced through the underlying convolutional layers, whereas the self-attention student will not. Recent works have shown that the self-attention layer can, in principle, learn the same operation as a convolution~\cite{Li2021CanConvolution}. The authors then show that injecting this bias will improve the data-efficiency, however, we show that this is not necessary since the bias can be learned implicitly through distillation.

Self-attention layers are known to be permutation equivariant. However, with the use of additive positional encodings, this restriction can be relaxed and can enable these layers to learn any arbitrary sequence-to-sequence function~\cite{Yun2020AreFunctions}. When we apply transformers to the vision domain, the tokens are provided as non-overlapping patches of the image. Thus, to perform a spatial translation on the sequence of tokens, we must first roll the sequence back to have the $H \times W$ dimensions in the same way in which we unrolled it at the input. The exact details can be seen in algorithm \ref{alg:equivariance}. Using this defined measure, we find that non-distilled transformers are over $15\times$ less equivariant than their distilled counterparts.

\renewcommand\thelstlisting{1}
\begin{figure}[t]
\centering
\begin{minipage}[t]{0.9\linewidth}
\begin{lstlisting}[basicstyle=\scriptsize\ttfamily, mathescape, language=Python, caption={Translational equivariance measure}, captionpos=t,
numberbychapter=false,label={alg:equivariance}]
# x: Image representation B x N x C
# T: Spatial translation
# blk: Block of self-attention layers
def compute(x, T):
    # 14 x 14 patches
    h = 14
    w = 14
    b, n, c = x.shape

    # remove positional encodings
    xp = x[:, 2:]
    
    # roll token-dim into H x W dimensions
    xp = xp.transpose(1, 2).reshape(b, c, h, w)
    Tx = T(xp)

    # unroll H x W dimensions
    Tx = Tx.flatten(2).transpose(1,2)
    
    # add back the positional encodings
    Tx = torch.cat((x[:, 0:2], Tx), dim=1)

    # forward pass with and without an input translation
    Fx = blk(x)
    FTx = blk(Tx)
    
    # remove position encoding
    Fxp = Fx[:, 2:]
    b, n, c = Fxp.shape
    
    Fxp = Fxp.transpose(1,2).reshape(b, c, h, w)
    TFxp = T(Fxp)
    TFxp = TFxp.flatten(2).transpose(1,2)
    
    # add back position encoding
    TFx = torch.cat((x[:, 0:2], TFxp), dim=1)
    
    return F.mse_loss(TFxp, FTx)
\end{lstlisting}
\end{minipage}
\end{figure} 

% \section{Supplied code and pre-trained weights}
% Alongside this supplementary material, we have provided the training/evaluation scripts for the training of data-efficient transformers and the ImageNet distillation benchmark. 

% \paragraph{Data-efficient training. } We have provided the weights from the final epoch and the log file reporting the best epoch accuracy. To evaluate the model, just run the following command inside the `deit` folder:

% \begin{lstlisting}[language=bash]
%   $ python main.py --model tiny
% \end{lstlisting}

% an additional \textit{--train-student} argument may be added to run the training script instead. The ImageNet dataset is assumed to be saved in \textit{/home/imagenet2012}, however, this can be changed my modifying \textit{config.py}. 

% \paragraph{ImageNet benchmark. } For the ImageNet distillation benchmark we use the \textit{torchdistill} library and our config files can be found in:

% \begin{lstlisting}[language=bash]
%   $ configs/ilsvrc2012/ours/res3418.yaml
% \end{lstlisting}

% Please see the original \textit{torchdistill}~\cite{Matsubara2020TorchdistillDistillation} repository for information regarding the details of training and evaluation.
\newpage
\subsection{Model Architectures}
In experiments, we use the following model architectures.
\begin{itemize}
    \item Wide Residual Network (WRN)~\cite{Zagoruyko2016WideNetworks}: WRN-$d$-$w$ represents wide ResNet with depth $d$ and width factor $w$.
\item resnet~\cite{He2015ResNetRecognition}: We use ResNet-d to represent CIFAR-style resnet with 3 groups of basic blocks, each with 16, 32, and 64 channels, respectively. In our experiments, resnet8x4 and resnet32x4 indicate a 4 times wider net- work (namely, with 64, 128, and 256 channels for each of the blocks).
    \item ResNet~\cite{He2015ResNetRecognition}: ResNet-d represents ImageNet-style ResNet with bottleneck blocks and more channels.
    \item MobileNetV2~\cite{Fox2018MobileNetV2:Bottlenecks}: In our experiments, we use a width multiplier of 0.5.
    \item vgg~\cite{Simonyan2015VeryRecognition}: The vgg networks used in our experiments are adapted from their original ImageNet counterpart.
    \item ShuffleNetV1~\cite{Zhang2018ShuffleNet:Devices}, ShuffleNetV2~\cite{Ma2018ShufflenetDesign}: ShuffleNets are proposed for efficient training and we adapt them to input of size 32x32.
\end{itemize}

\paragraph{Implementation Details.}
All methods evaluated in our experiments use SGD. For CIFAR-100, we initialize the learning rate as 0.05, and decay it by 0.1 every 30 epochs after the first 150 epochs until the last 240 epoch. For MobileNetV2, ShuffleNetV1 and ShuffleNetV2, we use a learning rate of 0.01 as this learning rate is optimal for these models in a grid search, while 0.05 is optimal for other models.

% \printbibliography

% \end{document}