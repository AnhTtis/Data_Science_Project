@article{TinyImageNet2015,
    title = {{Tiny imagenet visual recognition challenge}},
    year = {2015},
    author = {Ya, Le and Xuan, Yang}
}

@article{DistilBert2019,
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  title = {{DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}},
  journal = {NeurIPS Workshop on Energy Efficient Machine Learning and Cognitive Computing},
  year = {2019}
}

@misc{chen2021selfdistilling,
      title={On Self-Distilling Graph Neural Network}, 
      author={Yuzhao Chen and Yatao Bian and Xi Xiao and Yu Rong and Tingyang Xu and Junzhou Huang},
      year={2021},
      journal={IJCAI}
}

@misc{roth2021s2sd,
      title={S2SD: Simultaneous Similarity-based Self-Distillation for Deep Metric Learning}, 
      author={Karsten Roth and Timo Milbich and Bj√∂rn Ommer and Joseph Paul Cohen and Marzyeh Ghassemi},
      year={2021},
      journal={ICML}
}

@misc{wang2021proselflc,
      title={ProSelfLC: Progressive Self Label Correction for Training Robust Deep Neural Networks}, 
      author={Xinshao Wang and Yang Hua and Elyor Kodirov and David A. Clifton and Neil M. Robertson},
      year={2021},
      journal={CVPR}
}

@misc{mobahi2020selfdistillation,
      title={Self-Distillation Amplifies Regularization in Hilbert Space}, 
      author={Hossein Mobahi and Mehrdad Farajtabar and Peter L. Bartlett},
      year={2020},
      journal={NeurIPS}
}

@misc{zhang2020selfdistillation,
      title={Self-Distillation as Instance-Specific Label Smoothing}, 
      author={Zhilu Zhang and Mert R. Sabuncu},
      year={2020},
      journal={NeurIPS}
}

@misc{
    allen-zhu2023towards,
    title={Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning},
    author={Zeyuan Allen-Zhu and Yuanzhi Li},
    year={2023},
    journal={ICLR}
}

@inproceedings{rajpurkar2016squad,
  title={SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  booktitle={EMNLP},
  year={2016}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={NeurIPS},
  year={2017}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    year = "2019",
}


@misc{yang2023knowledge,
      title={From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels}, 
      author={Zhendong Yang and Ailing Zeng and Zhe Li and Tianke Zhang and Chun Yuan and Yu Li},
      year={2023},
      journal={ICCV},
}

@misc{huang2022knowledge,
      title={Knowledge Distillation from A Stronger Teacher}, 
      author={Tao Huang and Shan You and Fei Wang and Chen Qian and Chang Xu},
      year={2022},
      journal={NeurIPS},
}

@misc{pascalvoc,
      title={The pascal
visual object classes (voc) challenge.}, 
      author={Mark Everingham and Luc Van Gool and Christopher K. I.
Williams and John Winn and Andrew Zisserman.},
      year={2010},
      journal={IJCV},
}

@misc{chen2022dearkd,
      title={DearKD: Data-Efficient Early Knowledge Distillation for Vision Transformers}, 
      author={Xianing Chen and Qiong Cao and Yujie Zhong and Jing Zhang and Shenghua Gao and Dacheng Tao},
      year={2022},
      journal={CVPR}
}