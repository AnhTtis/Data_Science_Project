% \documentclass{article}
\documentclass[10pt,twocolumn,letterpaper]{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2020}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage[style=numeric]{biblatex}
\addbibresource{references.bib}
\addbibresource{more_references.bib}

\usepackage{afterpage}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{lipsum}
\usepackage{booktabs}
\usepackage[labelformat=simple]{subcaption}
\usepackage{array}
\renewcommand\thesubfigure{(\alph{subfigure})}
\usepackage{pgfgantt}
\usepackage{multirow}
\usepackage{array,booktabs}
\usepackage{tabularray}

\usepackage{svg}
\usepackage{pgfplots}
\usepackage{multirow, tabularx}
\usepackage{mathtools}
\usepackage{cancel}
\usepackage{adjustbox}
\usepackage{xcolor}

\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{colortbl}
\usepackage{stackengine}
\usepackage{listings}

\newcommand{\veryshortarrow}[1][3pt]{\mathrel{%
\hbox{\rule[\dimexpr\fontdimen22\textfont2-.2pt\relax]{#1}{.4pt}}%
   \mkern-4mu\hbox{\usefont{U}{lasy}{m}{n}\symbol{41}}}}

\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\green}[1]{{\color{green}#1}}
\newcommand{\black}[1]{{\color{black}#1}}
\newcommand{\blue}[1]{{\color{blue}#1}}
\definecolor{light_red}{rgb}{0.921,0.78039,0.83137}
\definecolor{dark_red}{rgb}{0.729,0.459,0.459}
\newcommand{\highlight}[1]{{\color{dark_red}#1}}

\newcommand{\lpnorm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\sidenote}[1]{{\color{red}#1}}
\newcommand{\Real}{{\rm I\!R}}
\newcommand{\p}[1]{{\noindent \textbf{#1}}}
\newcommand\xrowht[2][0]{\addstackgap[.5\dimexpr#2\relax]{\vphantom{#1}}}
\newcommand{\renyi}{{r\'enyi}}
\newcommand{\Renyi}{{R\'enyi}}
\newcommand{\functional}{function\;}

\definecolor{codegreen}{rgb}{0,0.5,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1.0,1.0,1.0}
\definecolor{link}{rgb}{1.0,1.0,1.0}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=1,
    float=tp,
  floatplacement=tbp
}
\lstset{style=mystyle}

\renewcommand*\thelstnumber{\arabic{lstnumber}:}
\renewcommand{\lstlistingname}{Algorithm}
\DeclareCaptionFormat{mylst}{\hrule#1#2#3\hrule}
\captionsetup[lstlisting]{format=mylst,labelfont=bf,singlelinecheck=off,labelsep=space}
\newcommand\redcancel[2][red]{\renewcommand\CancelColor{\color{#1}}\cancel{#2}}

\definecolor{light_orange}{RGB}{252,219,191}
\definecolor{dark_orange}{RGB}{237,139,55}
\definecolor{light_green}{RGB}{111,193,174}
\definecolor{dark_green}{RGB}{67,154,134}
\definecolor{cyan}{RGB}{123,250,241}

\newcommand{\roy}[1]{\textbf{[Roy]}  \; \red{#1} }
\newcommand{\comment}[1]{\red{#1} }

\definecolor{xarch}{rgb}{1.0, 0.9, 0.9}
\definecolor{sarch}{rgb}{0.94, 0.97, 1.0}
\definecolor{light_green}{rgb}{0.72, 0.85, 0.85}
\definecolor{light_red}{rgb}{1.0, 0.9, 0.9}

\usepackage{tikz}
\usepackage{tabularx}
\usetikzlibrary{fit}

\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand*{\myalign}[2]{\multicolumn{1}{#1}{#2}}

\newcommand{\rone}[1]{\textcolor{cyan}{#1}} %R1
\newcommand{\rtwo}[1]{\textcolor{brown}{#1}} %R2
\newcommand{\rthree}[1]{\textcolor{orange}{#1}} %R3

% introduce a new counter for counting the nodes needed for circling
\newcounter{nodecount}
\newcommand\tabnode[1]{\addtocounter{nodecount}{1} \tikz \node  (\arabic{nodecount}) {#1};}

\tikzstyle{every picture}+=[remember picture,baseline]
\tikzstyle{every node}+=[anchor=base,minimum width=0.4cm,align=center,text depth=.25ex,outer sep=1.5pt]
\tikzstyle{every path}+=[thick, rounded corners]

% \iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{9625} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

% \title{Rethinking and enhancing knowledge distillation using interpolated representations}
% \title{Deep dive into knowledge distillation}
% \title{[WT] Revisiting the Foundations of Knowledge Distillation through a Comprehensive Analysis of Key Design Decisions}
% \title{Understanding the underlying principles of knowledge distillation}
\title{Rebuttal - A closer look at the training dynamics of knowledge distillation}
% \\ through the projector training dynamics}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Roy Miles \\
  Imperial College London \\
  {\tt\small r.miles18@imperial.ac.uk} \\
  % examples of more authors
  \and
  Krystian Mikolajczyk \\
  Imperial College London \\
  % Address \\
  {\tt\small k.mikolajczyk@imperial.ac.uk}
}

\begin{document}

\maketitle


\noindent We would like to thank \rone{R3}, \rtwo{R5} and \rthree{R6} for their valuable comments and praising the interesting insights, simplicity, and empirical results of our manuscript. Below you can find our responses to the reviewer comments; we will improve our final manuscript accordingly. 

% - Paper writing and significance of contribution: The paper could benefit from better highlighting its contributions. After reading section 3, it wasnâ€™t clear to me what was of the realm of analysis versus what was a concrete proposed change to knowledge distillation. Furthermore, many of the ablation experiments (e.g. Table 2) are only performed on a specific data/architecture regime. It is not clear whether these are a strong enough base to derive generic robust changes to the distillation framework.
% Response:
%  We have updated the manuscript to better highlight and distinguish our analysis and contributions independantly.
%  We have provided a more rigurous set of ablation experiments demonstrating the importance of normalisation, LogSum, and the choice of alpha
%  Our contribution is that only these three very simple ingredients are needed to attain state-of-the-art across a wide range of datasets and distillation settings.
% \vspace{1.5mm}
\noindent \rone{R3} \textbf{Contributions:} 
Our key contribution lies in demonstrating that the normalisation, LogSum, and projections layers are sufficient to achieve SoTA performance across a broad spectrum of datasets and distillation settings. With regards to limited ablation experiments, we have now provided additional results showing the robustness and importance of these components across a wide variety of regimes, including ViT-to-CNN (tables 1, 2, 3). Furthermore, we now explicitly distinguish our analytical findings from our concrete proposals for modifying the KD framework. Each proposed modification is now followed by a direct connection to our analysis, which reinforces its grounding in our research. Finally, our contribution is much more computationally efficient and effective than other SoTA methods [6, 48, 7], and even those that target specific distillation settings [44, 54]. 

% We believe this insight offers substantial value to the field, presenting a simple and effective approach to knowledge distillation.

\begin{table}[h]
    \centering
    \vspace{-.7em}
    \resizebox{1.\linewidth}{!}{%
    \input{tables/rebuttal/normalisation}
    }
    \vspace{-1em}
    \caption{\textbf{Normalisation ablation} for distillation across a range of architecture pairs on ImageNet-1K 20\% subset.} 
    \vspace{-1.4em}
    \label{table:rebuttal_normalisation}
\end{table}

\begin{table}[h]
    \centering
    \vspace{-1em}
    \resizebox{1.\linewidth}{!}{%
    \input{tables/rebuttal/logsum}
    }
    \vspace{-1em}
    \caption{\textbf{LogSum ablation} across various architecture pairs. \textbf{Left:} full ImageNet. \textbf{Middle, Right:} 20\% subset.} 
    \vspace{-1.4em}
    \label{table:logsum}
\end{table}

\begin{table}[h]
    \centering
    \vspace{-.7em}
    \resizebox{1.\linewidth}{!}{%
    \input{tables/rebuttal/alpha}
    }
    \vspace{-1em}
    \caption{\textbf{Ablating the importance of $\alpha$.}}
    \vspace{-1.2em}
    \label{table:alpha}
\end{table}


% - Improvement on CNN distillation is limited: The improvement in CNN experiments is overall less impressive than the ones in ViT. It raises the questions of how generic the proposed changes are: e.g. are the differences in improvement because distillation methods for ViT are more recent/less optimized, or because the difference in capacity between the teacher/student is larger in the ViT experiments ?
% Response:
% Cross-architecture distillation is a relatively new sub-field of distillation and has since adopted some very sophisticated or expensive frameworks. We argue that representation distillation with a simple normalisation, logsum, and projection is all that is needed to attain SoTA
% CNN->CNN distillation is a heavily explored field of distillation and most SoTA methods are very expensive to adopt. Our method matches or surpasses SoTA with a very simple setup.
\noindent \rone{R3} \textbf{CNN vs ViT improvement:} 
Our proposed framework is most effective when there is a large capacity gap between models, which is only reflected in a few architecture pairs on the standard CIFAR100 benchmark. Furthermore, the CNN-to-ViT distillation setting transfers the inductive biases from the teacher, which is not a property for CNN-to-CNN.

%Our proposed approach, centered around simple normalization, LogSum, and projection layers, provides an efficient and cost-effective alternative that achieves state-of-the-art results in this context.

In the case of CNN-to-CNN distillation, we recognize that it is a heavily explored area, and many of the current SoTA methods require substantial computational resources. Our contribution demonstrates that a simple yet efficient setup can match or even surpass these sophisticated techniques. 
% We consider this a notable achievement and an affirmation of the potential of our proposed approach.

% - It is not clear how the hyperparameter $\alpha$ from the soft-maximum impacts training. e.g. how hard it is to tune in practice to observe improved results
% Response:
% We have conducted an ablation on the choice of alpha and, although logsum improves performance across all values, a value of 4. is most affective.


% \noindent \rone{R1} \textbf{$\alpha$ hyperparameter:} 
% We have conducted an ablation on the choice of $\alpha$. We empirically find that any value between 3.0 and 6.0 is most effective.

% Although LogSum improves the performance across a wide range of values, a value of 4 is most effective.
% \noindent \rone{R1} \textbf{Improved recipe for distillation:}

% - Writing is unclear in many parts of the manuscript. 1) Wp, Zs, Zt are all undefined in Eq. (1). 2) L345 "update rule" should be "the derivate with respect to Wp", 3) the caption in Table 1 suggests that the contents show the importance of the choice of the distance function but L556 says there is little to no benefit in varying the distance function. Further in L557, it says there are significant improvements in conjunction with a linear projection layer but the experimental results for this seems to be in Table 2, which is not referenced near the statement, and does not contain all the distance functions compared in Table 1.
% Response:
% This was an oversight and we have updated this section to introduce and define all these terms
% The update rule is not the derivative with respect to Wp. It is the value of Wp at time step t+1, which is Wp(t) - alpha * dL(t)/dWp
% We observe there is little benefit in varying the distance function when the capacity gap is small. However, in fig X and table 2 (rebuttal ablation), we show that there is a significant improvement when the capacity gap is large. We also wish to emphasise that a 1% improvement on ImageNet is significant. 
\noindent \rtwo{R5} \textbf{Notation and importance of distance function:}
We have corrected the oversight of not defining $\mathbf{W}_p$, $\mathbf{Z}_s$, and $\mathbf{Z}_t$ in Eq. (1) and updated the section for clarity. The "update rule" pertains to the value of $\mathbf{W}_p$ at time step $t+1$, not the derivative with respect to $\mathbf{W}_p$. Concerning the analysis of distance function variation in table 1 and line 556, our findings show varying the function has a smaller impact when the teacher-student model capacity gap is small, but offers significant improvement when the gap is large, as demonstrated in Fig. 4 and Table 2 (and tables 2, 3 in this rebuttal). The additional ablation experiments provided in this rebuttal further highlight the importance of the distance function across various architecture pairs.

% - There is no clear ablation of the design choices of the method. From what the reviewer can gather, stronger data augmentations seem to provide the most gains which is not really a contribution of the submission.
% \noindent \rtwo{R2} \textbf{Notation and importance of distance function:}
% Response:
% We have removed this section from the paper as we tried to make it clear that the data augmentation was not our contribution
% This was just to consider an additional more relevant baseline for CIFAR100.
% We also wish to highlight that the CIFAR100 experiments are for completeness with the standard distillation literature, but the results are generally quite saturated. 
% \noindent \rtwo{R2} \textbf{Ablation experiments:} We acknowledge that the augmentation is not a contribution of this paper and have updated the manuscript to remove this discussion in the method section. This point is why most of our ablation experiments are instead on the normalisation scheme, projector architecture, and choice of distance function. We confirm that careful selection each of these components forms the critical ingredients for effective knowledge distillation.

% - The reasoning behind choosing to use batch norm with the LogSum is not clearly explained.
% In sec X, we described the motivation behind the LogSum, which also has significant theoretical motivation too. In summary, the LogSum gradient is a softmax, with temperature alpha. Thus we can use this parameterised loss to smoothen out large element-wise distances, which is critical when distilling across a large capacity gap.
\noindent \rtwo{R5} \textbf{BatchNorm and LogSum motivation:} These two components are primarily motivated empirically through extensive experiments across various datasets and task. However, there are some theoretical motivations for the two. BatchNorm will provide spectral stability for the representations during training, while the LogSum function can smoothen or sharpen entries with respect to the parameter $\alpha$. We have now added a discussion with relevant citations.

% - While the authors claim theoretical analysis for design choices of KD, I believe most of the analysis is actually empirical in nature. While this does not diminish the value of the presented analysis, this does mislead the reader and should be revised.
% \noindent \rtwo{R2} \textbf{:}

% 1. Seems the authors to talk more ideas but not very deep. For example, the authors mentioned about the importantance of multi-view augmentation. But in the experiments, only Table 3 use RandAugment, I cannot see extensive experiments on multi-view augmentation. Also, there is no references to Table 6 in the draft. Both Table 5 and Table 6 want to show the effectiveness of the proposed method. Figure 5, we don't know what is the teacher model.
% Response:
% RandAugment is the multi-view augmentation used for the updated CIFAR100 baseline. For other datasets, such as ImageNet, we use MixUp. These are all multi-view augmentations, but are not the contribution of this paper. We cited the recent work in the literature which performed the extensive experiments showing the benefit of augmentation for distillation.
% Table 6 was discussed thoroughly in section X, but yes, it was an oversight not to explicitely reference it in the text.
% Figure 5 has the teacher at the top row, the divider was just to highlight the student models trained w/ and wo/ additional multi-view augmentation (in this case RandAugment).
\noindent \rtwo{R5} \rthree{R6} \textbf{Augmentation:} 
While multi-view augmentation (RandAugment, Mixup etc.) is a crucial part of KD, our focus is not on its extensive evaluation. We have cited the pertinent studies [3] affirming their effectiveness and have shown its adoption on the standard CIFAR100 distillation benchmark. The normalisation, LogSum, and $\alpha$ can provide significant improvement, especially for large capacity gaps. % Furthermore, to benefit from more aggressive augmentation would require much longer training schedules, which can be very computationally expensive.

We have corrected the oversight of not explicitly referencing Table 6 and it is now included in the manuscript's relevant discussions. Concerning figure 5, the top row represents the teacher for both with and without augmentation. This is now made more clear in the table caption.

%We highlight the importance of multi-view augmentation, not as a contribution, but as a suggestion to update the standard CIFAR100 distillation benchmark adopted in the literature. We include experiments both with and without RandAugment for all architecture pairs. For the later experiments, such as the CNN $\rightarrow$ ViT, we adopt extensive augmentation, as described in the co-advise paper, which we note is a paper that exclusively considers this single distillation setting.

% Table 6 was discussed in section 4.2, but indeed we overlooked referencing it explicitly in the text. This has been updated now. The teacher architecture used for each student-teacher pair in table 5 is given at the top of each column. We use the same teacher model when training with and without RandAugment.

% \printbibliography

\end{document}