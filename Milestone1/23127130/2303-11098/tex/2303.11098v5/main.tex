%File: formatting-instructions-latex-2024.tex
%release 2024.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}


\def\alambicdeit{\includegraphics[width=0.025\linewidth]{figures/alembic-crop.pdf}}
\def \alambic {\includegraphics[width=0.025\linewidth]{figures/alembic-crop.pdf}\xspace}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

% temp remove for arxiv version
% \nocopyright

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai23.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%


\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{emoji}
\newcommand*{\myalign}[2]{\multicolumn{1}{#1}{#2}}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

% removed for aaai
% \usepackage[style=numeric]{biblatex}
% \addbibresource{references.bib}
% \addbibresource{more_references.bib}

\usepackage{afterpage}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{lipsum}
\usepackage{booktabs}
\usepackage[labelformat=simple]{subcaption}
\usepackage{array}
\renewcommand\thesubfigure{(\alph{subfigure})}
\usepackage{pgfgantt}
\usepackage{multirow}
\usepackage{array,booktabs}
\usepackage{tabularray}
\usepackage{soul}

\usepackage{svg}
\usepackage{pgfplots}
\usepackage{multirow, tabularx}
\usepackage{mathtools}
\usepackage{cancel}
\usepackage{adjustbox}
\usepackage{xcolor}

\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{colortbl}
\usepackage{stackengine}
\usepackage{listings}

\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\green}[1]{{\color{green}#1}}
\newcommand{\black}[1]{{\color{black}#1}}
\newcommand{\blue}[1]{{\color{blue}#1}}
\definecolor{light_red}{rgb}{0.921,0.78039,0.83137}
\definecolor{dark_red}{rgb}{0.729,0.459,0.459}
\newcommand{\highlight}[1]{{\color{dark_red}#1}}

\newcommand*{\belowrulesepcolor}[1]{% 
  \noalign{% 
    \kern-\belowrulesep 
    \begingroup 
      \color{#1}% 
      \hrule height\belowrulesep 
    \endgroup 
  }%
} 
\newcommand*{\aboverulesepcolor}[1]{% 
  \noalign{% 
    \begingroup 
      \color{#1}% 
      \hrule height\aboverulesep 
    \endgroup 
    \kern-\aboverulesep 
  }%
} 

\newcommand{\lpnorm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\sidenote}[1]{{\color{red}#1}}
\newcommand{\Real}{{\rm I\!R}}
\newcommand{\p}[1]{{\noindent \textbf{#1}}}
\newcommand\xrowht[2][0]{\addstackgap[.5\dimexpr#2\relax]{\vphantom{#1}}}
\newcommand{\renyi}{{r\'enyi}}
\newcommand{\Renyi}{{R\'enyi}}
\newcommand{\functional}{function\;}

\definecolor{mygrey}{RGB}{230,230,230} 
\definecolor{myblue}{RGB}{230,230,255}
\definecolor{codegreen}{rgb}{0,0.5,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1.0,1.0,1.0}
\definecolor{link}{rgb}{1.0,1.0,1.0}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=1,
    float=tp,
  floatplacement=tbp
}
\lstset{style=mystyle}

\renewcommand*\thelstnumber{\arabic{lstnumber}:}
\renewcommand{\lstlistingname}{Algorithm}
\DeclareCaptionFormat{mylst}{\hrule#1#2#3\hrule}
\captionsetup[lstlisting]{format=mylst,labelfont=bf,singlelinecheck=off,labelsep=space}
\newcommand\redcancel[2][red]{\renewcommand\CancelColor{\color{#1}}\cancel{#2}}

\definecolor{light_orange}{RGB}{252,219,191}
\definecolor{dark_orange}{RGB}{237,139,55}
\definecolor{light_green}{RGB}{111,193,174}
\definecolor{dark_green}{RGB}{67,154,134}
\definecolor{cyan}{RGB}{123,250,241}

% \usepackage{hyperref}
% \hypersetup{
%     colorlinks=true,
%     linkcolor=blue,
%     filecolor=magenta,      
%     urlcolor=cyan,
%     pdftitle={Overleaf Example},
%     pdfpagemode=FullScreen,
% }

\newcommand{\roy}[1]{\textbf{[Roy]}  \; \red{#1} }
\newcommand{\comment}[1]{\red{#1} }

\definecolor{xarch}{rgb}{1.0, 0.9, 0.9}
\definecolor{sarch}{rgb}{0.94, 0.97, 1.0}
\definecolor{light_green}{rgb}{0.72, 0.85, 0.85}
\definecolor{light_red}{rgb}{1.0, 0.9, 0.9}

\usepackage{tikz}
\usepackage{tabularx}
\usetikzlibrary{fit}

\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% introduce a new counter for counting the nodes needed for circling
\newcounter{nodecount}
\newcommand\tabnode[1]{\addtocounter{nodecount}{1} \tikz \node  (\arabic{nodecount}) {#1};}

\tikzstyle{every picture}+=[remember picture,baseline]
\tikzstyle{every node}+=[anchor=base,minimum width=0.4cm,align=center,text depth=.25ex,outer sep=1.5pt]
\tikzstyle{every path}+=[thick, rounded corners]

% The file aaai23.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{Understanding the Role of the Projector in Knowledge Distillation}
\author {
    % Authors
    Roy Miles,
    Krystian Mikolajczyk
}
\affiliations {
    % Affiliations
   Imperial College London \\
    r.miles18@imperial.ac.uk, k.mikolajczyk@imperial.ac.uk
}

% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
% \usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
In this paper we revisit the efficacy of knowledge distillation as a function matching and metric learning problem. In doing so we verify three important design decisions, namely the normalisation, soft maximum function, and projection layers as key ingredients. We theoretically show that the projector implicitly encodes information on past examples, enabling relational gradients for the student. We then show that the normalisation of representations is tightly coupled with the training dynamics of this projector, which can have a large impact on the students performance. Finally, we show that a simple soft maximum function can be used to address any significant capacity gap problems. Experimental results on various benchmark datasets demonstrate that using these insights can lead to superior or comparable performance to state-of-the-art knowledge distillation techniques, despite being much more computationally efficient. In particular, we obtain these results across image classification (CIFAR100 and ImageNet), object detection (COCO2017), and on more difficult distillation objectives, such as training data efficient transformers, whereby we attain a $77.2$\% top-1 accuracy with DeiT-Ti on ImageNet. Code and models are publicly available.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Deep neural networks have achieved remarkable success in various applications, ranging from computer vision~\cite{Krizhevsky2012ImageNetNetworks} to natural language processing~\cite{Vaswani2017AttentionNeed}. However, the high computational cost and memory requirements of deep models have limited their deployment in resource-constrained environments. Knowledge distillation is a popular technique to address this problem through transferring the knowledge of a large teacher model to that of a smaller student model. This technique involves training the student to imitate the output of the teacher, either by directly minimizing the difference between intermediate features or by minimizing the Kullback-Leibler (KL) divergence between their soft predictions. Although knowledge distillation has shown to be very effective, there are still some limitations related to the computational and memory overheads in constructing and evaluating the losses, as well as an insufficient theoretical explanation for the underlying core principles.

\begin{figure}[ht]
    \centering
    \resizebox{1.\columnwidth}{!}{%
    \includegraphics[width=1.\linewidth]{figures/distillation_overview_v6.pdf}
    }
    \caption{Proposed feature distillation pipeline using three distinct components: linear projection (a), batch norm (b), and a \textit{LogSum} distance (c). We provide an interpretable explanation for each each of these three components, which results in a very cheap and effective recipe for distillation.}
    \vspace{-1em}
    \label{fig:distillation_overview}
\end{figure}

To overcome these limitations, we revisit knowledge distillation from both a function matching and metric learning perspective. We perform an extensive ablation of three important components of knowledge distillation, namely the distance metric, normalisation, and projector network. Alongside this ablation we provide a theoretical perspective and unification of these design principles through exploring the underlying training dynamics. Finally, we extend these principles to a few large scale vision tasks, whereby we achieve comparable or improved performance over state-of-the-art. The most significant result of which pertains to the data-efficient training of transformers, whereby a performance gain of 2.2\% is achieved over the best-performing distillation methods that are designed explicitly for this task.
Our main contributions can be summarised as follows.
\begin{itemize}
    \item We explore three distinct design principles from knowledge distillation, namely the projection, normalisation, and distance function. In doing so we demonstrate their and coupling with each other, both through analytical means and by observing the training dynamics.
    \item We show that a projection layer implicitly encodes relational information from previous samples. Using this knowledge we can remove the need to explicitly construct correlation matrices or memory banks that will inevitably incur a significant memory overhead.
    \item We propose a simple recipe for knowledge distillation using a linear projection, batch normalisation, and a $LogSum$ function. These three design choices can attain competitive or improved performance to state-of-the-art for image classification, object detection, and the data efficient training of transformers. 
\end{itemize}

%We expect that these id and should in fact be more commonly adopted than standard KL divergence practice.
% \begin{itemize}
%     \item This paper highlights that distillation is a form of function matching and demonstrates how a simple metric can be used to transfer knowledge between different tasks.
%     \item We introduce a matching loss between the predictions of interpolated hidden states to improve the robustness and convergence of knowledge distillation.
%     \item The experimental results demonstrate the superiority of the proposed distillation pipeline approach over state-of-the-art knowledge distillation techniques across a range of tasks and distillation settings.
% \end{itemize}


\section{Related Work}
\label{sec:related_work}

\paragraph{Knowledge Distillation}
Knowledge distillation is the process of transferring the knowledge from a large, complex model to a smaller, simpler model. Its usage was originally proposed in the context of image classification~\cite{Hinton2015DistillingNetwork} whereby the soft teacher predictions would encode relational information between classes. Spherical KD~\cite{Guo2020ReducingDistillation} extended this idea by re-scaling the logits, prime aware adaptive distillation~\cite{Zhang2020Prime-AwareDistillation} introduced an adaptive weighting strategy, while DKD~\cite{Zhao2022DecoupledDistillation} proposed to decouple the original formulation into target class and non-target class probabilities.

Hinted losses~\cite{Romero2015FitNets:Nets} were a natural extension of the logit-based approach whereby the intermediate feature maps are used as hints for the student. Attention transfer~\cite{Zagoruyko2019PayingTransfer} then proposed to re-weight this loss using spatial attention maps. ReviewKD~\cite{Chen2021DistillingReview} addressed the problem relating to the arbitrary selection of layers by aggregating information across all the layers using trainable attention blocks. Neuron selectivity transfer~\cite{Huang2017LikeTransfer}, similarity-preserving
KD~\cite{Tung2019Similarity-preservingDistillation}, and relational KD~\cite{Park2019RelationalDistillation} construct relational batch and feature matrices that can be used as inputs for the distillation losses. Similarly FSP matrices~\cite{Yim2017ALearning} were proposed to extract the relational information through a residual block. In contrast to this theme, we show that a simple projection layer can implicitly capture most relational information, thus removing the need to construct any expensive relational structures.

Representation distillation was originally proposed alongside a contrastive based loss~\cite{Tian2019ContrastiveDistillation} and has since been extended using a Wasserstein distance~\cite{Chen2020WassersteinDistillation}, information theory~\cite{Miles2022InformationDistillation}, graph theory~\cite{Ma2022DistillingAlignment}, and complementary gradient information~\cite{Zhu2021ComplementaryDistillation}. Distillation also been empirically shown to benefit from longer training schedules and more data-augmentation~\cite{Beyer2022KnowledgeConsistent}, which is similarly observed with HSAKD~\cite{Yang2021HierarchicalDistillation} and SSKD~\cite{Xu2020KnowledgeSelf-supervision}.
Distillation between CNNs and transformers has also been a very practically motivated task for data-efficient training~\cite{Touvron2021TrainingAttention} and has shown to benefit from an ensemble of teacher architectures~\cite{Ren2022Co-advise:Distillation}. However, we show that just a simple extension of some fundamental distillation design principles is much more effective.

Self-distillation is another branch of knowledge distillation that instead proposes to instead distill knowledge within the network itself~\cite{Zhang2019BeDistillation}. This paradigm removes the need to have any pre-trained teacher readily available and has since been applied in the context of deep metric learning~\cite{roth2021s2sd}, graph neural networks~\cite{chen2021selfdistilling}, and image classification~\cite{Miles2020CascadedSelf-distillation}. Its success of which has since driven many theoretical advancements~\cite{mobahi2020selfdistillation, allen-zhu2023towards, zhang2020selfdistillation} and is still an active area of research.

\paragraph{Self-Supervised Learning}
Self-supervised learning (SSL) is an increasingly popular field of machine learning whereby a model is trained to learn a useful representation of unlabelled data. Its popularity has been driven by the increasing cost of manual labelling and has since been crucial for training large transformer models. Various pretext tasks have been proposed to learn these representations, such as image inpainting~\cite{He2022MaskedLearners}, colorization~\cite{Zhang2016ColorfulColorization}, or prediction of the rotation~\cite{Gidaris2018UnsupervisedRotations} or position of patches~\cite{Doersch2015UnsupervisedPrediction, Carlucci2019DomainPuzzles}. SimCLR~\cite{Chen2020ARepresentations} approached self-supervision using a contrastive loss with multi-view augmentation to define the positive and negative pairs. They found a large memory bank of negative representations was necessary to achieve good performance, but would incur a significant memory overhead. MoCo~\cite{He2020MomentumLearning} extending this work with a momentum encoder, which was subsequently extended by MoCov2~\cite{Chen2020ImprovedLearning} and MoCov3~\cite{Chen2021AnTransformers}.
% that would then introduce various incremental design changes to improve the performance.

% Asymmetric architectures were proposed as an alternative to contrastive learning, whereby no negative samples are needed. Most noticeable works in this area are BYOL~\cite{Grill2020BootstrapLearning} and SimSiam~\cite{Chen2021ExploringLearning} which both use stop gradients to avoid representation collapse. DirectPred~\cite{Tian2021UnderstandingPairs} provided a theoretical understanding of this non-contrastive SSL setting. They introduced a series of conceptual insights into the functional roles of many crucial ingredients used. In doing so they derived a simple linear predictor with competitive performance to some much more complex predictor architectures. In our work we explore knowledge distillation from a similar perspective as DirectPred but observe some unique observations and results pertaining to this distillation setting.

Feature decorrelation is another approach to SSL that avoids the need for negative pairs to address representation collapse. Both Barlow twins~\cite{Zbontar2021BarlowReduction} and VICReg~\cite{Bardes2022VICReg:Learning} achieve this by maximising the variance within a batch, while preserving invariance to augmentations. Both contrastive learning and feature decorrelation have since been extended to dense prediction tasks~\cite{Bardes2022VICRegL:Features} and unified with knowledge distillation~\cite{Miles2023MobileVOS:Distillation}.

\begin{figure*}
\centering
\begin{minipage}[b]{.32\textwidth}
\centering
\includegraphics[width=1.\textwidth]{figures/no_evolvement_singular_vals-v2.png}
\caption*{(a) no normalisation.}
\end{minipage}
\hfill
\begin{minipage}[b]{.32\textwidth}
\centering
\includegraphics[width=1.\textwidth]{figures/l2_evolvement_singular_vals-v2.png}
\caption*{(b) L2 normalisation.}
\end{minipage}
\hfill
\begin{minipage}[b]{.32\textwidth}
\centering
\includegraphics[width=1.\textwidth]{figures/bn_evolvement_singular_vals-v2.png}
\caption*{(c) batch normalisation.}
\end{minipage}
\caption{Evolution of singular values of the projection weights $\mathbf{W}_p$ under three different representation normalisation schemes. The student is a Resnet-18, while the teacher is a ResNet-50. The three curves shows the evolution of singular values for the projector weights when the representations undergo no normalisation, L2 normalisation, and batch norm respectively.}
\vspace{-0.4em}
\label{fig:singular_values_evolution}
\end{figure*}


% \section{Understanding the Training Dynamics}
\section{Understanding the Role of the Projector}
\label{sec:understanding_kd}

Knowledge distillation (KD) is a technique used to transfer knowledge from a large, powerful model (teacher) to a smaller, less powerful one (student). In the classification setting, it can be done using the soft teacher predictions as pseudo-labels for the student. Unfortunately, this approach does not trivially generalise to non-classification tasks~\cite{Liu2019StructuredSegmentation} and the classifier may collapse a lot of information~\cite{Tishby2015DeepPrinciple} that can be useful for distillation.
%
Another approach is to use feature maps from the earlier layers for distillation~\cite{Romero2015FitNets:Nets, Zagoruyko2019PayingTransfer}, however, its usage presents two primary challenges: the difficulty in ensuring consistency across different architectures~\cite{Chen2021DistillingReview} and the potential degradation in the student's downstream performance for cases where the inductive biases of the two networks differ~\cite{Tian2019ContrastiveDistillation}
%
A compromise, which strikes a balance between the two approaches  discussed above, is to distill the representation directly before the output space. Representation distillation has been successfully adopted in past works~\cite{Tian2019ContrastiveDistillation, Zhu2021ComplementaryDistillation, Miles2022InformationDistillation} and is the focus of this paper. The exact training framework used is described in figure \ref{fig:distillation_overview}. The projection layer shown was originally used to simply match the student and teacher dimensions~\cite{Romero2015FitNets:Nets}, however, we will show that its role is much more important and it can lead to significant performance improvements even when the two feature dimensions already match. The two representations are typically both followed by some normalisation scheme as a way of appropriately scaling the gradients. However, we find this normalisation has a more interesting property in its relation to what information is encoded in the learned projector weights.

In this work we provide a theoretical perspective to motivate some simple and effective design choices for knowledge distillation. In contrast to the recent works~\cite{Tung2019Similarity-preservingDistillation, Miles2022InformationDistillation}, we show that an explicit construction of complex relational structures, such as feature kernels~\cite{He2022FeatureDistillation} is not necessary. In fact, most of this structure can be learned implicitly through the tightly coupled interaction of a learnable projection layer and an appropriate normalisation scheme.
%
In the following sections we investigate the training dynamics of the projection layer with the choice of normalisation scheme.  We explore the impact and trade-offs that arise from the architecture design of the projector. Finally, we propose a simple modification to the distance metric to address issues arising from a large capacity gap between the student and teacher models. Although we do not aim to necessarily propose a new method for distillation, we uncover a cheap and simple recipe that can transfer to various distillation settings and tasks. Furthermore, we provide a new theoretical perspective on the underlying principles of distillation that can translate to large scale vision tasks. \\

\noindent\textbf{The projection weights encode relational information from previous samples.} The projection layer plays a crucial role in KD as it provides an implicit encoding of previous samples and its weights can capture the relational information needed to transfer information regarding the correlation between features. We observe that even a single linear projector layer can provide significant improvements in accuracy (see Supplementary). This improvement suggests that the projections role in distillation can be described more concisely as being an encoder of essential information needed for the distillation loss itself. Most recent works propose a manual construction of some relational information to be used as part of a loss~\cite{Park2019RelationalDistillation, Tung2019Similarity-preservingDistillation}, however, we posit that an implicit and learnable approach is much more effective. To explore this phenomenon in more detail, we consider the update equations for the projector weights and its training dynamics. Without loss in generality, consider a simple L2 loss and a linear bias-free projection layer.

\begin{align}
    D(\mathbf{Z}_s, \mathbf{Z}_t \;; \mathbf{W}_p) &= \frac{1}{2} \lpnorm{ \mathbf{Z}_s\mathbf{W}_p - \mathbf{Z}_t }_2^2
 \end{align}
%
Where $\mathbf{Z}_s$ and $\mathbf{Z}_t$ are the \textbf{s}tudent and \textbf{t}eacher representations respectively, while $\mathbf{W}_p$ is the matrix representing the linear \textbf{p}rojection. Using the trace property of the Frobenius norm, we can then express this loss as follows:
\begin{align}
    D(\mathbf{Z}_s, \mathbf{Z}_t \;; \mathbf{W}_p) &= \frac{1}{2}tr\left( \left(\mathbf{Z}_s\mathbf{W}_p - \mathbf{Z}_t\right)^T\left(\mathbf{Z}_s\mathbf{W}_p - \mathbf{Z}_t\right) \right) \\
    &= \frac{1}{2}tr( \mathbf{W}_p^T\mathbf{Z}_s^T\mathbf{Z}_s\mathbf{W}_p - \mathbf{Z}_t^T\mathbf{Z}_s\mathbf{W}_p \\ &\;\;\;\;\;\;\;\;\;\;\;\;-\mathbf{W}_p^T\mathbf{Z}_s^t\mathbf{Z}_t + \mathbf{Z}_t^T\mathbf{Z}_t)
    \label{eqn:dloss}
\end{align}
%
Taking the derivative with respect to $\mathbf{W}_p$, we can derive the update rule $\dot{\mathbf{W}_p}$
\begin{align}
    \dot{\mathbf{W}_p} &= -\frac{\partial D(\mathbf{W}_p)}{\partial \mathbf{W}_p} = -\mathbf{Z}_s^T\mathbf{Z}_s\mathbf{W}_p + \mathbf{Z}_s^T\mathbf{Z}_t
\end{align}
%
which can be further simplified
%
\begin{equation}
    \boxed{\dot{\mathbf{W}_p} = \mathbf{C}_{st} - \mathbf{C}_s\mathbf{W}_p}
    \label{eqn:projector_update}
\end{equation}
%
where $\mathbf{C}_s = \mathbf{Z}_s^T\mathbf{Z}_s \in \Real^{d_s \times d_s}$ and $\mathbf{C}_{st} = \mathbf{Z}_s^T\mathbf{Z}_t \in \Real^{d_s \times d_t}$ denote self and cross correlation matrices respectively. Due to the capacity gap between the student network and the teacher network, there is no perfect linear projection between these two representation spaces. Instead, the projector will converge on an approximate projection that we later show is governed by the normalisation being employed. 

\underline{Whitened features:} consider using self-supervised learning in conjunction with distillation whereby the student features are whitened to have perfect decorrelation~\cite{Ermolov2020WhiteningLearning}, or alternatively, they are batch normalised and sufficiently regularised with a feature decorrelation term~\cite{Bardes2022VICReg:Learning}. In this setting, the fixed point solution for the weights will be symmetric and will capture the cross relationship between student and teacher features.
\vspace{-0.9em}

\begin{align}
    \mathbf{C}_{st} &- \mathbf{C}_s\mathbf{W}_p = 0  \;\;\; \text{where} \;\;\; \mathbf{C}_s = \mathbf{I} \\
    &\rightarrow \mathbf{W}_p = \mathbf{C}_{st}
\end{align}

Other normalisation schemes, such as those that jointly normalise the projected features and the teacher features, will have a much more involved analysis but will unlikely provide any additional insights on the dynamics of training itself. Thus, we propose to empirically explore the training trajectories of the projector weights singular values. This exploration will help quantify how the projector is mapping the student features to the teachers space. We cover this in the next section along with some additional insights into what is being learned and distilled.

\textbf{The choice of normalisation directly affects the training dynamics of $\mathbf{W}_p$.}
Equation \ref{eqn:projector_update} shows that the projector weights can encode relational information between the student and teacher's features. This suggests redundancy in explicitly constructing and updating a large memory bank of previous representations~\cite{Tian2019ContrastiveDistillation}. By considering a weight decay $\eta$ and a learning rate $\alpha_p$, the update equation can be given as follows:
\vspace{-0.7em}

\begin{align}
    \mathbf{W}_p &\rightarrow \mathbf{W}_p + \alpha_p\dot{\mathbf{W}_p} - \eta\mathbf{W}_p \\
    &= (1 - \eta)\mathbf{W}_p + \alpha_p\dot{\mathbf{W}_p}
\end{align}

By setting $\eta = \alpha_p$ we can see that the projection layer will reduce to a moving average of relational features, which is very similar to the momentum encoder used by CRD~\cite{Tian2019ContrastiveDistillation}. Other works suggest to extract relational information on-the-fly by constructing correlation or Gram matrices~\cite{Miles2022InformationDistillation, Peng2019CorrelationDistillation}. We show that this is also not necessary and more complex information can be captured through a simple linear projector. We also demonstrate that, in general, the use of a projector will scale much more favourably for larger batch sizes and feature dimensions. We also note that the handcrafted design of kernel functions~\cite{Joshi2021OnNetworks, He2022FeatureDistillation} may not generalise to real large scale datasets without significant hyperparameter tuning.

From the results in table \ref{table:rebuttal_normalisation}, we observe that when fixing all other settings, the choice of normalisation can significantly affect the student's performance. To explore this in more detail, we consider the training trajectories of $\mathbf{W}_p$ under  different normalisation schemes. We find that the choice of normalisation not only controls the training dynamics, but also the fixed point solution (see equation \ref{eqn:projector_update}). We argue that the efficacy of distillation is dependent on how much relational information can be encoded in the learned weights and how much information is lost through the projection. To jointly evaluate these two properties we show the evolution of singular values of the projector weights during training. The results can be seen in figure \ref{fig:singular_values_evolution} and show that the better performing normalisation methods (table \ref{table:rebuttal_normalisation}) are shrinking far fewer singular values towards zero. This shrinkage can be described as collapsing the input along some dimension, which will induce some information loss and it is this information loss that degenerates the efficacy of the distillation process.

\begin{table}[H]
    \centering
    \small
    \input{tables/rebuttal/normalisation}
    \caption{Normalisation ablation for distillation across a range of architecture pairs on ImageNet-1K 20\% subset.}
    \label{table:rebuttal_normalisation}
\end{table}

\textbf{Larger projector networks learn to decorrelate the input-output features.}
One natural extension of the previous observations is to use a larger projector network to encode more information relevant for the distillation loss. Unfortunately, we observe that a trivial expansion of the projection architecture does not necessarily improve the students performance. To explain this observation we evaluate a measure of decorrelation between the input and output features of these projector networks. The results can be seen in figure \ref{fig:input_output_decorrelation} and we can see that the larger projectors learn to decorrelate more and more features from the input. This decorrelation can lead to the projector learning features that are not shared with the student backbone, which will subsequently diminish the effectiveness of distillation. These observations suggest that there is an inherent trade-off between the projector capacity and the efficacy of distillation. We note that the handcrafted design of the projector architecture is a motivated direction for further research~\cite{Chen2022ImprovedEnsemble, Navaneet2021SimReg:Distillation}.
However, in favour of simplicity, we choose to use a linear projector for all of our large scale evaluations. 

\begin{figure}[H]
\centering
\includegraphics[width=.86\linewidth]{figures/correlation-v2.pdf}
\vspace{-0.7em}
\caption{Correlation between input-output features using different projector architectures. All projectors considered will gradually decorrelate the input-output features.}
\label{fig:input_output_decorrelation}
\end{figure}

\textbf{The soft maximum function can address distilling across a large capacity gap.}
 When the capacity gap between the student and the teacher is large, representation distillation can become challenging. More specifically, the student network may have insufficient capacity to perfectly align these two spaces and in attempting to do so may degrade its downstream performance. To addresses this issue we explore the use of a soft maximum function which will soften the contribution of relatively close matches in a batch. In this way the loss can be adjusted to compensate for poorly aligned features which may arise when the capacity gap is large. The family of functions which share these properties can be more broadly defined through a property of their gradients. In favour of simplicity, we use the simple $LogSum$ function throughout our experiments.

 \begin{align} 
     D(\mathbf{Z}_s, \mathbf{Z}_t \;; \mathbf{W}_p) = log \sum_i \lvert\mathbf{Z}_s\mathbf{W}_p - \mathbf{Z}_t\rvert_i^{\alpha}
     \label{eqn:dlogsum}
\end{align}

where $\alpha$ is a smoothing factor. We also note that other functions, such as the $LogSumExp$, with a temperature parameter $\tau$, have been used in SimCLR and CRD to a similar effect. Table~\ref{table:rebuttal_normalisation} shows the importance of feature normalisation across a variety of student-teacher architecture pairs. Batch normalisation provides the most consistent improvement that even extends to the Transformer $\rightarrow$ CNN setting. In table~\ref{table:logsum} we highlight the importance of the $LogSum$ function, which is most effective in the large capacity gap settings, as evident from the 1\% improvement for R50 $\rightarrow$ R18. Table~\ref{table:alpha} provides an ablation of the importance of the $\alpha$ parameter, whereby we observe that the performance is relatively robust to a wide range of values, but consistently optimal in the range 4-5.

\begin{table}[t!]
    \centering
    \small
    \input{tables/rebuttal/logsum}
    \caption{LogSum ablation across various architecture pairs. Left: 20\% subset. Right: Full ImageNet. The soft maximum function provides consistent improvement across both the CNN$\rightarrow$CNN and ViT$\rightarrow$CNN distillation settings.} 
    \label{table:logsum}
\end{table}

\begin{table}[H]
    \centering
    \small
    \input{tables/rebuttal/alpha_new2}
    \caption{Ablating the importance of $\alpha$. Distillation is generally robust for various values of $\alpha$, but consistently optimal in range 4-5 across various architecture pairs.}
    \label{table:alpha}
\end{table}

% main experiments
\section{Benchmark Evaluation}
\label{sec:benchmark_evaluation}

\textbf{Implementation details.}
We follow the same training schedule as CRD~\cite{Tian2019ContrastiveDistillation} for both the CIFAR100 and ImageNet experiments. For the object detection, we use the same training schedule as ReviewKD~\cite{Chen2021DistillingReview}, while for the data efficient training we use the same as Co-Advice~\cite{Ren2022Co-advise:Distillation}. All experiments were performed on a single NVIDIA RTX A5000. When using batch normalisation for the representations, we removed the affine parameters and set $\epsilon = 0.0001$. For all experiments we jointly train the student using a task loss $\mathcal{L}_{task}$ and the feature distillation loss given in equation \ref{eqn:dlogsum}.

\begin{align}
    \mathcal{L} = \mathcal{L}_{task} + D(\mathbf{Z}_s, \mathbf{Z}_t \;; \mathbf{W}_p)
\end{align}

% An additional logit distillation loss is added to some experiments to improve the student performance.

\subsection{Data Efficient Training for Transformers}
\label{sec:deit}
Transformers have emerged as a viable replacement for convolution-based neural networks (CNN) in visual learning tasks. Despite the promise of these models, their performance will suffer when there is insufficient training data available, such as in the case of ImageNet. DeiT~\cite{Touvron2021TrainingAttention} was the first to address this problem through the use of knowledge distillation. Although the authors show improved alignment with the teacher, we believe this fails to capture why less data is needed.

\begin{table}[H]
    \centering
    \small
    \input{tables/deit_new2.tex}
    \caption{Data-efficient training of transformers and CNNs on the ImageNet-1K dataset. Unless specified, all student models are trained for 300 epochs.}
    \vspace{-0.5em}
    \label{table:data_efficient_training_transformers}
\end{table}

\begin{table*}[ht!]
    \centering
    \small
    \input{tables/cifar100.tex}
    \caption{KD between Similar and Different Architectures. Top-1 accuracy (\%) on CIFAR100. Bold is used to denote the best results. All reported models are trained using pairs of augmented images. Those reported in the top box use RandAugment~\cite{Cubuk2020Randaugment:Space} strategy, while those in the bottom box use pre-defined rotations, as used in SSKD. $^\dagger$ denotes reproduced results in a new augmentation setting using the authors provided code.}
    \label{table:cifar100}
    \vspace{-0.7em}
\end{table*}

We posit that the distillation process encourages the student to learn layers which are "more" translational equivariant in attempt to match the teacher's underlying function. Although this is the principle that motivates using an ensemble of teacher models with different inductive biases~\cite{Ren2022Co-advise:Distillation}, there is still no thorough demonstration on if the inductive biases are actually being transferred. In this section we attempt to address this gap by introducing a measure of equivariance. We show that applying our distillation principles to this task can achieve significant improvements over state-of-the-art as a result of transferring more of the translational equivariance.

The results of these experiments are shown in table \ref{table:data_efficient_training_transformers}. We use the exact same training methodology as co-advice~\cite{Ren2022Co-advise:Distillation} and choose to use batch normalisation, a linear projection layer, and $\alpha = 4$ as the parameters for distillation. We observe a significant improvement over both DeiT and CivT when the capacity gap is large. However, as the capacity gap diminishes, and the student approaches the same performance as the teacher, this improvement is much less significant. Multiple factors, such as the soft maximum function and the batch normalisation, will be contributing to this observed result. However, the explanation is more concisely described by the fact that our distillation loss transfers more translational equivariance to the student.

\subsection{Classification on CIFAR100 and ImageNet}
\label{sec:cifar_and_imagenet}
Experiments on the CIFAR-100 classification task~\cite{Krizhevsky2009LearningImages} consist of 60K 32×32 RGB images across
100 classes with a 5:1 training/testing split. Table \ref{table:cifar100} shows the results for several student-teacher pairings. To enable a fair evaluation, we have only included the methods that use the same teacher weights provided by SSKD\cite{Xu2020KnowledgeSelf-supervision}. In these experiments we use an MLP projector with a hidden size of 1024 and no additional KL divergence loss. We confirm that not only is the choice of augmentation critical for good performance~\cite{Beyer2022KnowledgeConsistent} on this dataset, but applying our principles can attain state-of-the-art across most architecture pairs. The most significant improvements pertain to the cross-architecture experiments or where the capacity gap is large. We provide two sets of experiments with and without introducing a wider set of augmentations. In both settings we maintain the same optimiser, learning rate scheduler, and training duration.

The ImageNet~\cite{Russakovsky2014ImageNetChallenge} classification uses 1.3 million images that are classified into 1000 distinct classes. The input size are set to 224 x 224, and we employed a typical augmentation procedure that includes cropping and horizontal flipping. We used the torchdistill library with the standard configuration, which involves 100 training epochs using SGD and an initial learning rate of 0.1, which is decreased by a factor of 10 at epochs 30, 60, and 90. The results can be seen in table \ref{table:imagenet} and although the choice of architectures is not in favour of our method since the capacity gap is small, we are still able to attain competitive performance. Other methods, such as ICKD~\cite{Liu2021ExploringDistillation} or SimKD~\cite{Chen2022KnowledgeClassifier} either modify the original training settings or architectures, and so have been omitted from this evaluation.

\paragraph{Cross architecture distillation can implicitly transfer inductive biases.} CNNs use convolutions, which are spatially local operations, whereas transformers use self-attention, which are global operations. We expect that a benefit of this cross-architecture distillation setting is that the students learn to be "more" spatially equivariant in an attempt to match the teachers underlying function. It is this strong inductive bias that can reduce the amount of training data needed. A layer is translation equivariant if the following property holds:

\begin{align}
    \phi(T\mathbf{x}) = T\phi(\mathbf{x})
    \label{eqn:equivariance}
\end{align}

In other words, if we take a translated input $T\mathbf{x}$ and pass it through a layer $\phi$, the result should be equivalent to first applying $\phi$ to $\mathbf{x}$ and then performing the translation. A natural measure of equivariance can then be the difference between the left and right-hand side of this equation \ref{eqn:equivariance}.

\begin{align}
    \mu_{T}(\phi) = \lpnorm{\phi(T\mathbf{x}) - T\phi(\mathbf{x})}_2^2
\end{align}

\begin{table*}[htp]
    \centering
    \small
    \input{tables/imagenet.tex}
    \vspace{-.8em}
    \caption{Top-1 and Top-5 error rates (\%) on ImageNet. ResNet18 as student, ResNet34 as teacher.}
    \label{table:imagenet}
\end{table*}

We evaluate this measure on a block of self-attention layers by first removing the distillation and class tokens and then rolling the patch tokens to recover the spatial dimensions. This operation can then be performed on the input and output tensors before applying a translation. Table \ref{table:equivariance_measure} shows this measure of equivariance after training with and without distillation. In general, we observe that the distilled models do in fact learn to preserve spatial locality between feature maps, which aligns with the function matching perspective for distillation. 

We find that our simple distillation recipe can transfer a lot more of this equivariance property to the student. Although Co-advise does learn this spatial locality to some extent, it is much less significant than using our feature based distillation, despite both attaining a similar level of performance. Intermediate feature map losses may be able to transfer even more of this translational equivariance property, however, its usage may degrade the benefit of using a transformer in the first place. For example, although we observe that most self-attention blocks (trained using distillation) do preserve a lot of this spatial locality, there is still some global context between patch tokens that is still being preserved.

\begin{table}[H]
    \centering
    \small
    \input{tables/translation_equivariance}
    \caption{Measure of translational equivariance for a DeiT-S transformer model trained with and without distillation. These results confirm that distillation can transfer explicit inductive biases from the teacher.}
    % Reported measure was computed after every epoch and averaged over 256 images.
    \label{table:equivariance_measure}
\end{table}

\subsection{Object Detection on COCO}
\label{sec:object_detection}

We extend the application of our method to object detection, whereby we employ a similar approach as used in the classification task by distilling the backbone output features of both the student and teacher networks. Due to the smaller batch sizes used in these experiments, we choose to instead normalise over the height and width dimensions. For evaluating the efficacy of our method, we conduct experiments on the widely-used COCO2017 dataset~\cite{Lin2014MicrosoftContext} under the same settings provided in ReviewKD. We then further demonstrate the applicability of our distillation principles on the more recent and efficient YOLOv5 model~\cite{Zhu2021TPH-YOLOv5:Scenarios}. In both cases we show improved student performance on the downstream task, whereby competitive performance is achieved with ReviewKD despite being significantly simpler and cheaper to integrate into a given distillation pipeline. Our method even outperforms FPGI~\cite{Wang2019DistillingImitationb}, which is directly designed for detection.

% (top) Single-model single-scale mAP on COCO val2017. All models are trained for 300 epochs.
\begin{table}
    \centering
    \small
    \input{tables/yolo_no_flops_params}
    \caption{Object detection on COCO. (top) We report the standard COCO metric of mAP averaged over IOU thresholds in [0.5 : 0.05 : 0.95] along with the standard PASCAL VOC’s metric~\cite{pascalvoc}, which is the average mAP@0.5. (bottom) For the R-CNN results, we report the mAP and AP50 metrics to enable a consistent comparison with ReviewKD.}
    \label{table:yolo}
\end{table} 

% conclusion
\section{Conclusion}
\label{sec:conclusion}
In this paper, we revisited the core underlying principles of knowledge distillation and have performed an extensive ablation on the most effective and scaleable components. In doing so, we have provided a new theoretical perspective for understanding these results through analyzing the projector training dynamics. By extending these principles to a wide range of tasks, we achieve competitive or improved performance to state-of-the-art across image classification, object detection, and data efficient training of transformers. Our proposed distillation recipe can significantly reduce the complexity and memory consumption of existing pipelines by avoiding the need to construct expensive relational object, many trainable layers, or enforcing very long training schedules. We further show improved performance for the large capacity gap settings and evidence for the distillation of explicit inductive biases from the teacher. Looking ahead to future research in this area, we expect to see the joint development of more sophisticated normalisation schemes and projection networks, which will encode more complex and informative features for the distillation process. 

\paragraph{Code Reproducibility.} To facilitate the reproducibility of results, we release all the training code and pre-trained weights. The ImageNet experiments are performed using the popular \textit{torchdistill}~\cite{Matsubara2020TorchdistillDistillation} framework, while the CIFAR100 and data-efficient training code are based on those provided by CRD~\cite{Tian2019ContrastiveDistillation} and co-advice~\cite{Ren2022Co-advise:Distillation} respectively.

% \section{References}
% \label{sec:references}
% \printbibliography
% \bibliography{references, more_references}
\bibliography{main}

% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{references}
% }

% \input{supplementary}

\end{document}
