\documentclass[lettersize, final]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{graphicx, amsmath, amssymb, subcaption, url, cite, array, amsthm}
\usepackage[ruled, linesnumbered]{algorithm2e} %algorithm with algolined
\usepackage{algorithm2e, setspace}
\usepackage[font={small}]{caption}
\usepackage[hidelinks]{hyperref}
\usepackage{tikz}  % for circled numbers

\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=17mm,
 right=17mm,
 }

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=1pt] (char) {#1};}}

\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\begin{document}
\title{\huge Enhancing Immersion and Presence in the Metaverse with Over-the-Air Brain-Computer Interface}
\author{Nguyen Quang Hieu, Dinh Thai Hoang, Diep N. Nguyen, Van-Dinh Nguyen, Yong Xiao, and Eryk Dutkiewicz}
\maketitle
\begin{abstract}
This article proposes a novel framework that utilizes an over-the-air Brain-Computer Interface (BCI) to learn Metaverse users' expectations. By interpreting users’ brain activities, our framework can help to optimize physical resources and enhance Quality-of-Experience (QoE) for users.
To achieve this, we leverage a Wireless Edge Server (WES) to process electroencephalography (EEG) signals via uplink wireless channels, thus eliminating the computational burden for Metaverse users' devices.
As a result, the WES can learn human behaviors, adapt system configurations, and allocate radio
resources to tailor personalized user settings. Despite the potential of BCI, the inherent noisy wireless channels and uncertainty of the EEG signals make the related resource allocation and learning problems especially challenging. We formulate the joint learning and resource allocation problem as mixed integer programming problem. Our solution involves two algorithms that are a hybrid learning algorithm and a meta-learning algorithm. The hybrid learning algorithm can effectively find the solution for the formulated problem. Specifically, the meta-learning algorithm can further exploit the neurodiversity of the EEG signals across multiple users, leading to higher classification accuracy.
Extensive simulation results with real-world BCI datasets show the effectiveness of our framework with low latency and high EEG signal classification accuracy.
\end{abstract}

\begin{IEEEkeywords}
Metaverse, brain-computer interface, EEG, edge computing, deep reinforcement learning, meta-learning.
\end{IEEEkeywords}

\section{Introduction}
\subsection{Motivation}
The recent emergence of the Metaverse represents a revolutionary advancement in virtual space, enabling real-time interactions between individuals and objects. 
One of the distinctive features of the Metaverse is that individuals are the central figures, referred to as digital avatars, of all activities and events in the virtual world \cite{xu2022full}. 
In addition to communication and interaction with one another, individuals can also create digital objects in this virtual environment.  However, the real-time representation of human activities in the Metaverse presents a significant challenge for Metaverse development. Most existing solutions rely on an extensive array of sensors to accurately capture human activities in the real world and convert them into the Metaverse \cite{caserman2020survey}. Furthermore, efficient computing and communication solutions are vital for the collection and processing of the extensive sensing data \cite{xu2022full}. 

To enable Metaverse applications, initial studies have been conducted \cite{meng2022sampling, lam2022human, sun2022augmented, le2022noncontact, fernandes2016combating, cho2014bci, kim2017measurement}. In \cite{meng2022sampling}, the authors proposed a co-design framework for sampling, communication, and prediction to construct virtual objects in the Metaverse. The framework was successfully utilized to reconstruct the trajectories of a virtual robotic arm with low latency, under specific sampling rate and prediction accuracy constraints. The trajectories were captured using a camera as a motion capture system, which collected the movement data from human subjects. Additionally, special markers were placed on the physical object used for reconstructing the virtual robotic arm's trajectory. In \cite{lam2022human}, the authors proposed an interactive framework that supports multiple users at a time and a higher degree of freedom for virtual human avatars. To this end, the motion capture system utilized inertial-measurement-unit (IMU) sensors attached to the human participants. The IMU sensors measured the acceleration, velocity, and magnetometer signals of the human joints, which were then used to construct 3D virtual avatars with the iconic gestures of human users.

To further enhance the user experience, the work in \cite{sun2022augmented} proposed a multimodal sensing and feedback platform that could not only detect the finger motion of the users but also send haptic feedback such as vibration and heat to the users from the Metaverse. The haptic feedback was achieved by using triboelectric nanogenerator sensors attached to the user's fingers \cite{le2022noncontact}. The finger motion data were then collected and processed at a cloud server to produce finger motion prediction with machine learning, i.e., Principal component analysis (PCA) and support vector machine (SVM) algorithms. The results showed that the errors in detecting motion and sending haptic feedback were acceptable for collision detection and motion control scenarios.
Other sources of human data such as eye movement, VR headset’s orientation, field-of-view, and heart rate, have shown several benefits in enhancing user expectations \cite{fernandes2016combating, cho2014bci, kim2017measurement}.
For example, VR motion sickness, i.e., a common issue in VR applications in which the user’s brain receives conflicting signals about self-movement between the physical and virtual worlds, can be eliminated by analyzing eye movement and heart rate of the user to adjust the display settings \cite{fernandes2016combating, cho2014bci}.

As analyzed in the aforementioned works, the utilization of data collected from the Metaverse users can be considered as a potential approach toward a robust and individualized Metaverse world.
However, there are fundamental challenges that need to be addressed to fully realize such an individualized Metaverse world.
One of the main challenges is how we can effectively use such a massive amount of data produced from human activities, given the practical constraints of computing, communications, and energy resources. For example, to make VR headsets less bulky, the battery pack should be smaller, but this might limit the overall energy for local processing and communications. 
This also means fewer antennas and sensors can be mounted, thus limiting the communications and sensing capabilities.
 Furthermore, each conventional sensing technique requires a specific type of sensor, e.g., camera \cite{meng2022sampling}, finger sensor \cite{sun2022augmented}, IMU sensor \cite{lam2022human}, heart rate sensor \cite{tidoni2014audio}, and joystick \cite{dasdemir2022brain}, that may hinder the scalability and standardization of the system.
For example, in some Metaverse applications such as real-time virtual fighting, fitness, and dance gaming, heart rate  tracking information can help to adjust the tempo and intensity of the music, while body movement tracking information can provide more accurate and precise control over the user's dance movements. However, it is challenging to exploit, combine, and synchronize such diverse information sources from different kinds of sensors, e.g., heart rate sensors and IMU sensors.

Unlike most of the existing works in the literature, we leverage an alternative source of human data from users’ brain activities that is expected to be a paradigm shift in the way we interact in the Metaverse. The brain is the most complex organ in the human body, encoded with the richest information reflecting an individual's cognitive capabilities and perception.
It has been reported that brain signals, such as electroencephalography (EEG), contain underlying information about heart rate, limb movement, intentions, emotions, and eye movement \cite{tidoni2014audio, wu2017evaluation, cheng2020brain}. 
By utilizing brain signals from users, we can potentially develop a human-centric Metaverse that authentically and individually represents humans through their brain activities, while minimizing the need for external sensors attached to the body. Moreover, carrying brain signals over communication channels such as wireless mediums can open up new areas of application, such as brain communications, imagined speech, and semantic communications, enabling users to communicate with one another through their thoughts \cite{lee2022toward}. 
To this end, Brain-Computer Interface (BCI) technology \cite{schalk2004bci2000} provides a neural pathway between the human brain and the outside world through sensors attached to the human scalp, enabling the recording of brain activities and connection to computing units such as remote servers or personal computers.
As such, BCI can be recognized as a facilitator for the enhancement of the human-centric Metaverse, where human activities and thoughts can be reliably mirrored and synergized.

\vspace{-0.1cm}
\subsection{Related Works}
\label{subsubsec:realted-works}
Applications of BCI to the Metaverse are still in its infancy.
In \cite{lee2022toward}, the authors proposed a framework for imagined speech communication in home control applications using BCI. The user's EEG signals were extracted and processed with a SVM classifier to detect the user's commands. In another study \cite{dasdemir2022brain}, a gaming platform was developed for teleportation within the Metaverse, where the user's EEG signals were directly translated into commands, eliminating the need for traditional gaming tools like a joystick. The results also showed that participants did not experience motion sickness, as the negativities of the locomotion movements were eliminated by using EEG signals instead of a joystick controller. It is worth noting that both studies in \cite{lee2022toward, dasdemir2022brain} were pseudo-online and supported by a wired connection between a computer and BCI headsets. The local computer was responsible for both processing and providing VR experiences. 
The framework is hence not practical for a scalable Metaverse system due to the mobility and computational limitation of the local computing unit.

The mobility, scalability, and computation limitations of the existing BCI-based Metaverse approaches can be addressed by utilizing wireless BCI headsets connecting to a centralized server/edge computing unit with much more abundant computational power \cite{he2015wireless}. In this way, the remote server and the users can exchange information with each other through the uplink and downlink wireless channels.
For instance, the authors in \cite{kasgari2019human} assumed that delay perception in a wireless network depends on human factors such as age, gender, and demographic, and developed a learning model to predict the perception delay of human users.
The learning model can then allocate radio resources to the users and thus help to minimize the system's delay, constrained by the brain characteristics.
A limitation of \cite{kasgari2019human} is the lack of justification for the brain signals as the authors used delay feedback data from a conventional setting of a mobile network testbed. Furthermore, the brain signals used in \cite{kasgari2019human} were simulated data and oversimplified, making it infeasible to learn human behaviors from their actual brain activities, e.g., EEG signals.
More importantly, the joint problem of predicting human activities and resource allocation has not been addressed in \cite{kasgari2019human}. In reality, these two problems are strongly correlated. For instance, inappropriate resource allocation policies or incorrect prediction of brain activities can severely degrade the user experience, leading to system delays and motion sickness for the Metaverse users. The holistic problem becomes even more critical in future wireless systems like 5G advanced and 6G, where massive amounts of user data are exchanged, and ultra-reliable and low-latency requirements must be met.

In our previous work \cite{hieu2022toward}, a BCI-enabled Metaverse system over wireless networks has been developed to jointly optimize resource allocation and brain signal classification.
Unlike \cite{kasgari2019human}, our work in \cite{hieu2022toward} successfully predicted human behaviors from their actual brain signals, i.e., EEG signals.
However, another important issue of incorporating BCI into the Metaverse that has not been concerned in all the above works is the neurodiversity of the brain signals among different users.
Unlike conventional human data, brain signals are highly individual. In other words, the continuous signals such as EEG might be different across users in certain scenarios.
This inevitable neurodiversity has been recently reported in \cite{kang2014bayesian, vezard2015eeg, zhang2017multi}, which have focused on developing robust and scalable classification frameworks for BCI systems.

Given the above, we summarize three major challenges of using BCI for the Metaverse that have not been well addressed in the literature as follows:
\begin{itemize}
\item First, given the noisy wireless environments with uplink communication bottlenecks, how the system can precisely predict human behaviors from their brain signals? This requires not only highly accurate classifiers for brain signals but also effective resource management schemes to alleviate the inherent problems of wireless communications (e.g., fading and interference).
\item Second, given the sophisticated brain signals, how to optimally address the correlation between (i) predicting user behaviors and (ii) minimizing system delay? Such a joint optimization problem is challenging as it must take into account the impact of noisy wireless environment and low-latency requirements of VR applications, given the uncertainty of the Metaverse users' activities.
\item Third, how to tackle the neurodiversity of the brain signals which impede the scalability and robustness of the Metaverse system involving multiple users?
\end{itemize}

\subsection{Contributions}
To address the aforementioned challenges, this article proposes a novel over-the-air Brain-Computer Interface (BCI) framework to assist the creation of virtual avatars as human representations in the Metaverse. First, we propose a novel architecture that allows the Metaverse users to interact with the virtual environment while sending the brain signals on uplink wireless channels to Wireless Edge Servers (WES). By utilizing integrated VR-BCI headsets, we can produce a rich source of human data besides conventional sensing techniques for heart rate measurement, eye tracking, or wearable sensors on limbs.
From the collected brain signals, a WES with more computing power, in comparison to the headsets can synthesize and hence orchestra Digital Avatars (DAs) to enhance Metaverse users' QoE. The DAs with up-to-date brain signals can further predict user behaviors such as their physical actions, e.g., moving hands and feet \cite{moioli2021neurosciences}.
With the prediction ability, the DAs can act as intelligent interfaces to support and/or detect movements, decisions, and emotions of the Metaverse users.

In order to solve the first challenge discussed in Section \ref{subsubsec:realted-works}, we use a real-world BCI dataset \cite{goldberger2000physiobank} to validate the practicability of our system, given the noisy and low-latency requirements of the wireless networks and VR applications, respectively. 
To address the second challenge, i.e., joint prediction and resource allocation, we first formulate the QoE maximization problem subject to practical delay and prediction accuracy requirements. Even neglecting the dynamics/uncertainty of the wireless and users' environment, the resulting problem is a mixed integer programming that is challenging to solve due to the strong correlation between resource allocation and human brain signals prediction problems. To tackle it, we leverage the recent advances of the actor-critic architecture \cite{schulman2017proximal} and the deep convolutional neural network \cite{zhang2021survey} to jointly optimize the system's resources and predict actions of the users based on their brain signals.
To address the third challenge of neurodiversity, we develop a novel meta-learning algorithm that makes the system robust against the increasing number of Metaverse users. Our main contributions are summarized as follows:
\begin{itemize}
\item We introduce a novel over-the-air BCI-enabled Metaverse system.
By collecting and analyzing the brain signals of the users, the system can create intelligent DAs that serve as a neural interface for the Metaverse users. The intelligent DAs can detect user movements such as hands and feet, thus enabling more sophisticated gesture detection applications such as gaming, virtual object control, and virtual teleportation in the Metaverse.
\item We propose an innovative hybrid learning algorithm to address the mixed decision-making (i.e., radio and computing resource allocation optimization) and classification problem (i.e., predicting user behaviors based on the brain signals). Unlike conventional optimization approaches which separately solve these sub-problems, our novel learning framework with deep neural networks directly optimizes the user QoE with practical constraints of the system's delay and prediction accuracy of the brain signals. The proposed hybrid learning algorithm can jointly (i) predict the actions of the users given the brain signals as the inputs and (ii) allocate the radio and computing resources to the system so that the VR delay of the system is minimized. As a result, our approach is more applicable to practical BCI-enabled Metaverse systems with users' dynamic demands as well as strict VR delay requirements. 
\item We develop a highly effective meta-learning algorithm that specifically addresses the impact of neurodiversity in brain signals. By using a novel meta-gradient update process, the proposed meta-learning can better recognize the neurodiversity in the brain signals, compared with the hybrid learning algorithm. Extensive experiments with real-world BCI datasets show that our proposed solution can achieve a prediction accuracy of up to $82\%$. 
More importantly, the simulation results empirically show that our proposed meta-learning algorithm is more robust against neurodiversity, effectively alleviating the prediction accuracy deteriorating when the number of Metaverse users increases.
\item We conduct comprehensive evaluations on the real-world BCI dataset together with a practical VR rendering process at the WES. The results show that our proposed hybrid learning and meta-learning algorithms can provide low delay for VR applications and achieve high classification accuracy for the collected brain signals. More interestingly, our proposed framework can work well with the brain signals distorted by noise when the Metaverse users and the WES communicate with each other over the noisy channels.
\end{itemize}

The rest of our paper is organized as follows. 
In Section \ref{sec:system-model}, we describe our system model in detail.
In Section \ref{sec:learning-algorithms}, we propose two novel algorithms that can effectively address the mixed decision-making and classification problem of our system.
Section \ref{sec:performance-evaluation} presents comprehensive evaluations of our proposed framework with real-world BCI datasets. Conclusions are drawn in Section \ref{sec:conclusion}.

\vspace{-0.1cm}
\section{System Model}
\label{sec:system-model}
\begin{figure}[t]
\centering
\includegraphics[width=0.75\linewidth]{figures/system-model.png}
\caption{Illustration of our proposed over-the-air BCI-enabled Metaverse system. 
A Wireless Edge Server (WES) runs Metaverse applications, supporting VR experiences for $K$ users.
These Metaverse users are equipped with integrated VR-BCI headsets. $K$ Digital Avatars (DAs) are maintained in Metaverse to support real-time recommendations and enhance the user QoE.}
\label{fig:system-model}
\end{figure}

The proposed BCI-enabled Metaverse system model is illustrated in Fig.~\ref{fig:system-model}.
The system consists of (i) a Wireless Edge Server (WES) and (ii) $K$ users equipped with integrated VR-BCI headsets, e.g., Galea headsets \cite{bernal2022galea}.
Each integrated VR-BCI headset can extract brain activities from $J$ channels (i.e., corresponding to $J$ electrodes of the headset) from the user and provide VR services for the user. 
Each user is associated with a Digital Avatar (DA). 
A controller is deployed at the WES to jointly allocate the system's resources and predict user behaviors.
We later describe the deployment of the controller in our proposed algorithms in Section \ref{sec:learning-algorithms}.
The operation of our proposed system includes six main steps as illustrated in Fig.~\ref{fig:system-model}. Details of the system operation are as follows.

\subsection{System Operation}
At each time step $t$, each user sends BCI signals\footnote{We use ``BCI signals" to denote the electroencephalography response signal recorded by an EEG headset. The BCI signals might be slightly different depending on the standard of the BCI system, e.g., 10-10 and 10-20 international systems \cite{jurcak200710}. The details of the BCI system and BCI signals are discussed later.} $\mathbf{e}_k(t)$ to the WES via uplink channels. The BCI signals $\mathbf{e}_k(t) \in \mathbb{R}^{J \times W}$ is a $J \times W$ vector where $J$ is the number of BCI channels, and $W$ is the number of collected BCI signal samples per channel. At the end of the sampling interval of time step $t$, the WES collects a set of BCI signals, i.e., 
\begin{equation}
\mathbf{e}(t) = \{\mathbf{e}_1(t), \mathbf{e}_2(t), \ldots, \mathbf{e}_K(t)\}\in \mathbb{R}^{K \times J \times W}.
\label{eq:bci-signal-vector}
\end{equation}
The above process corresponds to the step \circled{1} in Fig.~\ref{fig:system-model}.
Once the WES obtains BCI signals from the users, the WES pre-processes VR content for $K$ users and monitors the wireless channel state and the internal computing state at the same time (step \circled{2}). 
The pre-processing process can be Field-of-Views (FoVs) rendering that is personalized for each user \cite{fernandes2016combating}. For example, the user may be interested in a particular spatial region in the Metaverse while he/she rarely explores other regions. As a result, pre-processing FoVs for the users can not only save computing resources for the users but also reduce the amount of information transmitted over the wireless links \cite{corbillon2017viewport, hieu2022toward}.
For this, the WES monitors the total computing load of its multi-core CPUs, denoted by $\mathbf{u}(t) \in \mathbb{R}^N$, is defined by:
\begin{equation}
\mathbf{u}(t) = \{u_1(t), u_2(t), \ldots, u_N(t)\},
\label{eq:cpu-load}
\end{equation}
where $N$ is the number of CPUs of the WES and $u_n(t) \in (0, 1)$ is the computing load of the $n$-th CPU.
The WES then analyzes the current state of the system and calculates the best policy, i.e., computing resource allocation for VR pre-processing and radio/power resource allocation for the uplink channels in the
next step.

Next, the WES updates the collected BCI signals for $K$ DAs (step \circled{3}). 
We assume that each DA keeps up-to-date BCI signals of a particular user. As such, the creation of intelligent human-like DAs can be easily deployed, maintained, and improved. 
All information, including wireless channels, computing resources, and brain signals, will be used as inputs for the training purpose of the controller (step \circled{4}). 
The controller is a learning model that simultaneously performs two tasks: (i) allocating radio and computing resources of the system and (ii) classifying BCI signals into different actions of the users.
We describe our controller in detail later in Section \ref{sec:learning-algorithms}.
The controller then outputs a policy to increase the QoE of users (step \circled{5}).
Finally, the WES delivers personalized VR services to the users via downlink channels (step \circled{6}). 
The system repeats the above steps in the next time step $t+1$.
As observed from Fig.~\ref{fig:system-model} and the above steps, the neural interface, i.e., integrated VR-BCI headset, can transmit brain signals over wireless channels and allow the WES to deploy VR services that are personalized for the users. For example, by monitoring the EEG signals, the WES can detect VR sickness \cite{lotte2012combining} or emotional changes \cite{cho2014bci} of the users and then adjusts the virtual environments' settings accordingly to eliminate such effects.

To evaluate the system's performance, we construct a QoE metric that is a function of (i) the round-trip VR delay of the users and (ii) the accuracy of classifying the BCI signals at the WES. 
The round-trip VR delay is the latency between the time the user requests VR content from the WES (step \circled{1}) and the time the user receives the requested VR content displayed in his/her integrated VR-BCI headset (step \circled{6}). 
The accuracy of classifying BCI signals is obtained by the controller at the WES to predict the actions of the users based on the collected BCI signals.
We select the VR delay and classification accuracy as our main metrics because they have been commonly used to design frameworks that eliminate potential VR sickness or fatigue of the users \cite{chun2016bci, fernandes2016combating, kim2017measurement}. 
Moreover, we consider the classification setting on the BCI signals because if we can successfully predict the actions of the users, it is possible to extend the setting to a general scenario in the Metaverse where intelligent human-like DAs can accurately behave like humans with controlled permissions, e.g., imagined speech communications \cite{lee2022toward}, adaptive VR environment rendering \cite{lotte2012combining}, and anomalous states and error-related behaviors detection \cite{arico2017passive}.
In the sequel, we formally construct the QoE metric by deriving the round-trip VR delay and BCI classifier's accuracy.

\vspace{-0.1cm}
\subsection{Round-trip VR delay}
\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.49\linewidth}
        \includegraphics[width=1.0\linewidth]{figures/rendering_before.png}
        \caption{}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.49\linewidth}
       \includegraphics[width=1.0\linewidth]{figures/rendering_after.png}
       \caption{}
    \end{subfigure}
    \caption{Illustration of the FoV pre-rendering process at the WES: (a) a selected video frame before rendering and (b) after rendering. The original video is a panoramic video from Youtube\protect\footnotemark. 
We use the Vue-VR software \cite{mudin} to pre-render the FoVs from the given panoramic video. Our local server for running the FoV pre-rendering process is a MacBook Air 2020 with 8GB memory and 2.3 GHz 8-core CPU.}
    \label{fig:fov-render}
\end{figure}
    \footnotetext{\url{https://www.youtube.com/watch?v=s_hdc_XiXiA}}

We consider that the round-trip VR delay consists of (i) processing latency at the WES, (ii) downlink transmission latency, and (iii) uplink transmission latency. 
For the uplink, we use an orthogonal frequency division multiple access (OFDMA)
technique in which each user occupies a radio resource block and the downlink is broadcast channel \cite{chen2020joint}.
Since most of the computation is shifted to the WES, we assume that the latency at the user headsets is negligible. Accordingly, the round-trip VR delay of user $k$ at time step $t$ is calculated by:

\begin{equation}
D_k(t) = \frac{l_k^U}{r_k^U(t)} + d_k(t) +  \frac{l_k^D}{r_k^D(t)},
\label{eq:system-latency}
\end{equation}
where $l_k^U$ and $l_k^D$ are the length of data packets to be transmitted over the uplink and downlink, respectively; $r_k^U(t)$, $r_k^D(t)$ are the uplink and downlink data rates between the user $k$ and the WES, respectively; and $d_k(t)$ is the processing latency, e.g., pre-rendering the FoVs, at the WES.
The processing delay of the WES depends on the process running in the WES and the CPU capacity of the WES. In our setting, we consider that the WES is running an FoVs rendering process application. We assume that the WES is equipped with a multi-core computing unit having sufficient computing resources for all the users. Our setting is illustrated in Fig.~\ref{fig:fov-render}.
At time step $t$, the WES measures its current available CPU state $\mathbf{u}(t)$.
Let $\tau_k(t) \in (0, 1)$ denote the portion of $u_n(t)$ (i.e., computing load of the $n$-th CPU) that is used for pre-rendering FoV for user $k$-th.
Once $u_n(t)$ and $\tau_k(t)$ are obtained, the pre-processing delay of the WES for rendering FoV for user $k$-th is calculated by:
\begin{equation}
d_k(t) = \frac{1}{\tau_k(t) u_n(t) \upsilon},
\label{eq:processing-delay}
\end{equation}
where $\upsilon$ (Hz) is the CPU capacity, i.e., the total number of computing cycles, of the WES.
The uplink data rate for user $k$ is defined as follows:
\begin{equation}
r_k^U(t) = \sum_{m \in \mathcal{M}} B^U \rho_{k,m}(t) \log_2\Big(1 + \frac{p_{k}(t) h_{k}(t)}{I_m + B^U N_0}\Big),
\label{eq:uplink-rate}
\end{equation}
where $\mathcal{M}$ is the set of radio resource blocks, $p_{k}(t)$ is the transmit power of the user $k$, and $h_{k}$ is the time-varying channel gain between the WES and user $k$. $\rho_{k,m}(t) \in \{0, 1\}$ is the resource block allocation variable. $\rho_{k,m}(t) = 1$ if the resource block $m$ is allocated to user $k$. Otherwise $\rho_{k,m}(t) = 0$. $I_m$ is the multi-user interference from users who are also using the resource block $m$ from nearby WESs. $B^U$ is the bandwidth of each resource block. $N_0$ is the noise power spectral efficiency.

The high interference between multiple users can cause packet errors at the WES.
In our work, we consider the packet error rate experienced by the transmission of BCI signals of user $k$ as \cite{chen2020joint}:
\begin{equation}
\epsilon_k(t) = \sum_{m \in \mathcal{M}} \rho_{k,m} \epsilon_{k, m},
\label{eq:epsilon}
\end{equation}
where $\epsilon_{k,m} = 1 - \text{exp}\Big(- \frac{z \sigma_U^2}{p_k h_k(t)}\Big)$ is the packet error rate over resource block $m$ with $z$ being a waterfall threshold \cite{xi2011general}. 
Due to the packet errors, the received BCI signals at the WES can contain noise, e.g., Gaussian noise. From hereafter, we denote the noisy BCI signals received at the WES as $\mathbf{\hat{e}}(t)$ to differentiate the notation from the raw BCI signals as defined in (\ref{eq:bci-signal-vector}). 

For the downlink channel, the WES can broadcast the VR content to the users. Therefore, the downlink data rate achieved by the WES is calculated by:

\begin{equation}
r_k^D(t) = B^D \log_2 \Big(1 + \frac{P_B h_{k}(t)}{I_D + B^D N_0}\Big),
\label{eq:downlink-rate}
\end{equation}
where $P_B$ is the transmit power of the WES. $I_D$ and $B^D$ are interference and downlink bandwidth, respectively.
Unlike the uplink transmission, the broadcast downlink transmission can significantly reduce packet errors. Moreover, the transmit power of the WES, i.e., $P_B,$ is usually sufficient to ensure a high signal-to-interference plus noise ratio (SINR) at the users.
Therefore, we assume that the packet error rate in the downlink transmission is negligible compared with the uplink.

\vspace{-0.1cm}
\subsection{BCI Classifier}
\begin{figure}
\centering
\begin{subfigure}{0.69\linewidth}
	\includegraphics[width=1.1\linewidth]{figures/eeg-example.pdf}
\end{subfigure}
\begin{subfigure}{0.29\linewidth}
	\includegraphics[width=1.0\linewidth]{figures/10-10.pdf}
\end{subfigure}
\caption{Example of EEG signals recorded from three different BCI participants responding to the same experimental condition (left figure). The EEG signals are extracted from the same channels, i.e., C3, CP3, C4, CP4, Cz, and CPz, denoted as red circles on the surface of the scalp in a 10-10 international system (right figure). These channels are responsible for hands and feet movement \cite{morash2008classifying}. The instructions to the participants are placed at the time 0 (marked by the vertical dashed line). The neurodiversity, i.e., subjective differences in the same environment,  reflect the differences in amplitudes and phases of the BCI participants.}
\label{fig:eeg-example}
\end{figure}

Similar to other works in the literature, we assume that the WES has labels for the  input BCI signals \cite{zhang2021survey}.
We consider a BCI classifier at the WES, denoted by $\phi$, to be a binary indicator (0 or 1) if the predicted output, e.g., predicted hands/feet movement, matches the given labels, denoted by $\mathbf{l}(t)$. In particular, $\phi\left(\mathbf{\hat{e}}(t), \mathbf{l}(t)\right) = 1$ if the prediction is correct. Otherwise $\phi\left(\mathbf{\hat{e}}(t), \mathbf{l}(t)\right) = 0$.
The goal of the WES is to minimize the loss of false detections for the predictor $\phi$ given the collected BCI signals and labels.
In our work, we assume that the amount of label data transmitted via uplink channels is negligible, compared with that of the BCI signals. The labels are only scalar values, e.g., 0, 1, and 2, while the corresponding BCI signals are usually sampled at frequencies 150 Hz or 200 Hz \cite{zhang2021survey}.
Formally, we define the loss of the predictor $\phi$ by a cross-entropy loss as follows:
\begin{equation}
L_{\phi}\Big(\mathbf{\hat{e}}(t), \mathbf{l}(t)\Big) = -\sum_{c=1}^C \phi\Big(\mathbf{\hat{e}}(t), \mathbf{l}(t)\Big) \log(\varrho_c),
\label{eq:cross-entropy-loss}
\end{equation}
where $C$ is the number of possible actions and $\varrho_c$ is the predicted probability of actions $c$, e.g., moving hands/feet.
In this work, we consider that BCI signals are EEG signals as the case study. However, the extension beyond EEG, e.g., electrocardiogram (ECG) or electromyogram (EMG), is straightforward.
We collect the EEG signals from a motor imagery experiment \cite{goldberger2000physiobank}. The dataset in \cite{goldberger2000physiobank} contains EEG signals from 109 participants. Each participant produces data samples from 64 EEG channels with the BCI2000 system \cite{schalk2004bci2000}. Details of the experiment can be found in \cite{goldberger2000physiobank}.
In Fig.~\ref{fig:eeg-example}, we illustrate the EEG signals from three different participants responding to the same instruction in the experiment, i.e., moving their hands and feet. 
It can be observed from Fig.~\ref{fig:eeg-example} that EEG signals of the participant are different in both amplitude and phase. 
This observation expresses the neurodiversity among different users \cite{moioli2021neurosciences}. With the same considered environment, the BCI signals that reflect the user consciousness are different. 
Given the individual BCI signals of multiple users, it is very challenging to obtain accurate predictions on the raw BCI signals, let alone the noisy BCI signals received at the WES after the signals are transmitted over a noisy channel.
In the following, we design an effective QoE model that can capture the impacts of the noisy BCI signals on the system. We then formulate the problem involving (i) a classification problem and (ii) a decision-making problem.

\vspace{-0.1cm}
\subsection{QoE Model and QoE Maximization Problem Formulation}
We consider the QoE of user $k$, denoted by $Q_k$, as a combination of (i) round-trip VR delay and (ii) the prediction accuracy for the actions. Therefore, the QoE metric can be expressed as follows:
\vspace{-0.1cm}
\begin{multline}
Q_k(\boldsymbol{\rho}, \mathbf{p}, \boldsymbol{\tau}, \phi) = \frac{1}{T} \sum_{t=1}^{T} \Big( \eta_1 \psi \big(D_k(t), D_{\max}\big) + \\ \eta_2 \phi\big(\mathbf{\hat{e}}(t), \mathbf{l}(t)\big) \Big),
\label{eq:qoe-calculation}
\end{multline}
where $\eta_1$ and $\eta_2$ are the positive weighting factors; and $T$ is the time horizon. $\psi(\cdot)$ is also a binary indicator which is defined as follows:
\vspace{-0.1cm}
\begin{equation}
\psi \Big(D_k(t), D_{\max}\Big) = 
    \begin{cases}
      1 & \text{if $D_k(t) \leq D_{\max}$,} \\
      0 & \text{otherwise,}
    \end{cases}   
    \label{eq:delay-requirement}
\end{equation}
where $D_{\max}$ is the maximum allowed round-trip VR delay for user $k$.
Recall that the BCI classifier $\phi(\mathbf{\hat{e}}(t), \mathbf{l}(t))$ is a binary indicator that receives value 1 if the classification is correct.
The use of binary indicators with positive weighting factors enables the multi-objective QoE model and eliminates the effects of the differences in measurement scales, i.e., time and accuracy. Similar types of multi-objective QoE models have been widely used in the literature \cite{zhang2019drl260, mao2017neural}.

By defining the QoE model as a linear combination of the two binary indicators, we can capture the impacts of (i) wrong classification in (\ref{eq:cross-entropy-loss}) and (ii) exceeding the VR delay requirement in (\ref{eq:delay-requirement}) .
For example, an incorrect classification and an exceeded VR delay caused by the WES lead the QoE value to be 0. If both indicators are equal to 1, the QoE value is equal to $\eta_1 + \eta_2$.
By controlling the weighting factors $\eta_1$ and $\eta_2$, one can determine the priorities of such factors, i.e., delay or accuracy, in specific applications.
For example, in applications that require highly accurate classifications of BCI signals such as imagined speech communication \cite{lee2020neural}, the value of $\eta_2$ can be increased. 
Likewise, in delay-sensitive applications, the value of $\eta_1$ can be increased.
The impacts of these weighting factors on the QoE of the users will be further discussed in Section \ref{sec:performance-evaluation}.

In this paper, we aim to maximize the average QoE of users, given the following constraints: (i) power at the WES and user headsets, (ii) wireless channels, and (iii) computational capability of the WES.
Formally, our optimization problem is defined as follows:
\vspace{-0.2cm}
\begin{subequations}
\label{eq:min-latency}
\begin{align}
\mathcal{P}_1: \max_{\boldsymbol{\rho}, \mathbf{p}, \boldsymbol{\tau}, \phi} \quad & \frac{1}{K} \sum_{k \in \mathcal{K}} Q_{k}(\boldsymbol{\rho}, \mathbf{p}, \boldsymbol{\tau}, \phi) \\
\textrm{s.t.} \quad & \rho_{k,m}(t) \geq 0, \\
\quad & \sum_{k \in \mathcal{K}} \rho_{k,m}(t) = 1, \\
\quad & 0 \leq p_{k}(t) \leq P_{\max}, \\
\quad & \sum_{k \in \mathcal{K}}\tau_k(t) \leq 1, \tau_k \geq 0, \\
\quad & \phi\big(\mathbf{\hat{e}}(t), \mathbf{l}(t)\big) \in \{0, 1\},
\end{align}
\label{eq:qoe-maximization}
\end{subequations}
where $P_{\max}$ is the maximum transmission power of the integrated VR-BCI headsets. $\boldsymbol{\rho} = \{\rho_{k, m}(t); \forall k\in \mathcal{K}, \forall m\in \mathcal{M}\}$ is the resource block allocation vector, $\boldsymbol{\tau} = \{\tau_{k}(t); \forall k\in \mathcal{K}\}$ is the computing resource allocation vector, and $\mathbf{p} = \{p_k(t); \forall k \in \mathcal{K}\}$ is the power allocation vector.
In the problem (\ref{eq:qoe-maximization}) above, (\ref{eq:qoe-maximization}b) and (\ref{eq:qoe-maximization}c) are the constraints for radio resource block allocation, (\ref{eq:qoe-maximization}d) is the constraint for the transmit power, (\ref{eq:qoe-maximization}e) are the constraints for the computing resource allocation at the WES, and (\ref{eq:qoe-maximization}f) is the BCI classifier constraint.

Note that the maximization of $Q_k$ in (\ref{eq:qoe-maximization}) results in reducing the round-trip VR delay $D_k(t)$ and the BCI prediction loss $L_{\phi}$. 
Our considered problem involves not only a classification problem, i.e., classification of BCI signals in (\ref{eq:cross-entropy-loss}), but also a decision-making problem, i.e., channel, power, and computing resource allocation problem in (\ref{eq:processing-delay}) and (\ref{eq:uplink-rate}). 
The formulated problem $\mathcal{P}_1$ is a Mixed-Integer Linear Programming (MILP) problem in which the power allocation variables $\mathbf{p}$ are continuous while the resource block allocation and classification decision variables, i.e., $\boldsymbol{\rho}$ and $\phi$ are integer variables. 
Thus, it is challenging to obtain the optimal solution for $\mathcal{P}_1$.
Unlike conventional optimization approaches such as \cite{kasgari2019human} which can only separately solve the sub-problems, i.e., delay perception and resource allocation, and thus cannot enable real-time optimization of user QoE, we propose a novel hybrid learning algorithm to jointly predict the user behaviors based on the BCI signals and allocate radio and computing resources in the system. As a result, our approach is more robust against the dynamic of user demand as well as the uncertainty of the wireless channels.
Later, we propose a highly-effective training algorithm based on the idea of meta-learning. With the proposed meta-learning algorithm, we can further enhance the prediction accuracy by dealing with the neurodiversity of the brain signals among multiple users. In the next section, we propose two new algorithms that we refer to as ``Hybrid learner" and ``Meta-leaner".
 
\section{Learning Algorithms For Maximizing QoE}
\label{sec:learning-algorithms}
As we described in Section \ref{sec:system-model}, the controller deployed at the WES is responsible for learning to optimize the system's resources and predict the users' behaviors.
For this purpose, our proposed algorithms, i.e., hybrid learning and meta-learning, in this section can be deployed at the WES just as simply as pre-installing software at the WES.
To solve the problem $\mathcal{P}_1$ in (\ref{eq:qoe-maximization}), we propose the Hybrid-leaner to effectively solve the problem.
Next, we propose the Meta-learner as an improvement of the Hybrid learner to solve the problem $\mathcal{P}_2$ in (\ref{eq:meta-optimization}), which will be described later in this section. The problem $\mathcal{P}_2$ is the extended version of $\mathcal{P}_1$ in which the neurodiversity among BCI users is taken into consideration.

\vspace{-0.1cm}
\subsection{Hybrid Learning Algorithm}
We first propose a Hybrid learner which is illustrated in Fig.~\ref{fig:hybrid-learner}. 
Our Hybrid learner consists of three deep neural networks that are (i) an actor network, (ii) a critic network, and (iii) a convolutional network.
The inputs for training the deep neural networks are empirical data from the BCI signals, the wireless channel state, and the computing load of the WES. The output of the proposed algorithm is the policy to jointly allocate power for the users' headsets, allocate radio resources for the uplink channels, and predict the actions of the users based on the BCI signals.
Let $\boldsymbol{\theta}$, $\boldsymbol{\Theta}$, and $\boldsymbol{\varphi}$ denote the parameters, i.e., weights and biases, of the actor network, critic network, and convolutional network, respectively. 
Our proposed  training process for the Hybrid learner is illustrated in Algorithm 1. The operation of the algorithm is as follows.

The parameters for deep neural networks are first initialized randomly (line 1 in Algorithm 1).
At each training iteration $i$, the Hybrid learner first collects a set of trajectories $\mathcal{D}_i$ in (\ref{eq:trajectory}) by running current policy $\Omega(\boldsymbol{\theta}_i, \boldsymbol{\Theta}_i, \boldsymbol{\varphi}_i)$ for $O$ time steps. 
The trajectories $\mathcal{D}_i$ contain three main parts that are (i) the observation from the environment, (ii) the action taken of the WES based on the observation from the environment, and (iii) QoE feedback from $K$ users (line 3).
The observation from the environment is a tuple of three states that are channel state $\mathbf{h}$, computing load of the WES $\mathbf{u}$, and BCI signals from users $\mathbf{\hat{e}}$.
The action of the WES is a tuple of four parts that are the radio resource block allocation vector $\boldsymbol{\rho}$, the power allocation vector $\mathbf{p}$, the computing resource allocation vector $\boldsymbol{\tau}$, and the output of the BCI classifier $\phi$.
Based on the collected trajectories, the objective functions for updating the deep neural networks are calculated as follows.
The advantage estimator $\hat{A}_i$ is defined in (\ref{eq:gae-lambda}) \cite{schulman2017proximal} where $\lambda$ is the actor-critic tradeoff parameter and $\delta_o$ is the temporal-difference error which is defined by:
\vspace{-0.1cm}
\begin{equation}
 \delta_o = \frac{1}{K} \sum_{k \in \mathcal{K}}Q_{k,o} + \gamma V(\mathbf{h}_{o+1}, \mathbf{u}_{o+1}, \mathbf{\hat{e}}_{o+1}) - V(\mathbf{h}_{o}, \mathbf{u}_{o}, \mathbf{\hat{e}}_{o}),
 \end{equation}
where $\gamma \in (0, 1)$ is the discount factor and $V(\cdot)$ is the value function of the given observation, i.e., output of the critic network.
Once the advantage estimator is obtained, the decision-making objective can be calculated by $J(\hat{A}_i)$ as defined in (\ref{eq:decision-making-objective}). In the calculation of $J(\hat{A}_i)$, we adopt a policy-clipping technique from \cite{schulman2017proximal}. In particular, the policy clipping function $f_c(\varepsilon, \hat{A}_i)$ is defined by:
\vspace{-0.1cm}
\begin{equation}
f_c(\varepsilon, \hat{A}_i)= \begin{cases}(1+\varepsilon) \hat{A}_i, & \text{if } \hat{A}_i \geq 0, \\ (1-\varepsilon) \hat{A}_i, & \text{if } \hat{A}_i < 0.\end{cases}
\end{equation}
With the policy clipping function, the gradient step update is expected not to exceed certain thresholds so that the training  is more stable.
Next, the classification loss $L_{\phi}(\mathbf{\hat{e}}, \mathbf{l}; \boldsymbol{\varphi})$ is calculated based on (\ref{eq:cross-entropy-loss}) with convolutional network (line 6).

With all the obtained objective and loss functions, the deep neural networks' parameters are finally updated as follows.
The actor network's parameters are updated in (\ref{eq:actor-net-update}) (line 7) where $\alpha_c$ is the learning step size of the actor network and $\nabla$ is the gradient of the function which can be calculated with stochastic gradient decent/ascent algorithms. In our paper, we use Adam as the optimizer for all the deep neural networks.
The critic network's parameters are updated in (\ref{eq:critic-net-update})   (line 8) where $\alpha_c$ is the learning step size and $L_c(\boldsymbol{\Theta})$ is the critic loss which is defined by:
\begin{equation}
L_c(\boldsymbol{\Theta}) = \Big(V(\mathbf{h}_{o}, \mathbf{u}_{o}, \mathbf{\hat{e}}_{o}) - \frac{1}{K}\sum_{k \in \mathcal{K}}Q_{k,o}\Big)^2.
\end{equation}
Finally, the convolutional network's parameters are updated in (\ref{eq:convo-net-update}) (line 9) where $\alpha_n$ is the learning step size.

As described above, our hybrid learning algorithm addresses the problem $\mathcal{P}_1$ in (\ref{eq:qoe-maximization}) by maximizing the decision-making objective $J(\hat{A}_i)$ in (\ref{eq:decision-making-objective}) while the classification loss $L_{\phi}(\mathbf{\hat{e}}, \mathbf{l}; \boldsymbol{\varphi})$ is minimized in (\ref{eq:convo-net-update}).
With the training process that splits, computes, and backpropagates the losses throughout the deep neural networks, the Hybrid learner can better realize the compound objective $Q_{k}(\boldsymbol{\rho}, \mathbf{p}, \boldsymbol{\tau}, \phi)$ involving distinct learning sub-objectives.
In Section \ref{sec:performance-evaluation}, we show that this design mechanism can significantly enhance the performance of the system, compared with other current state-of-the-art deep reinforcement learning algorithms.

\begin{figure*}[t]
\centering
\includegraphics[width=0.7\linewidth]{figures/hybrid-model.pdf}
\caption{Training process for the proposed Hybrid learner at the controller of the WES. The circled numbers denote the corresponding steps as described in Fig.~\ref{fig:system-model} and Section \ref{sec:system-model}.}
\label{fig:hybrid-learner}
\end{figure*}

\begin{algorithm}[t]
\caption{Hybrid learning algorithm for maximizing QoE}
 \textbf{Input}: 
 Initialize $\boldsymbol{\theta}_0$, $\boldsymbol{\Theta}_0$ and $\boldsymbol{\varphi}_0$ at random. \\
 \For{\text{i = 0, 1, 2}, $\ldots$}{
  Collect a set of trajectories $\mathcal{D}_i$:
  \begin{multline}
  \mathcal{D}_i = \Big\{\big(\mathbf{h}_o, \mathbf{u}_o, \mathbf{\hat{e}}_o\big), \big(\boldsymbol{\rho}_o, \boldsymbol{p}_o, \boldsymbol{\tau}_o, \phi_o\big), 
  \\ \big(Q_{1,o}, Q_{2,o}, \ldots, Q_{K,o}\big)\Big\}\Big|_{o=1}^{O}.
  \label{eq:trajectory}
  \end{multline} \\
   Compute advantage estimator function $\hat{A}_i$ over $\mathcal{D}_i$:
   \begin{equation}
 \hat{A}_i = \sum_{o=1}^{O}(\gamma \lambda)^o \delta_{o}.
 \label{eq:gae-lambda}
  \end{equation} \\
  Calculate the decision-making objective:
  \begin{equation}
  J(\hat{A}_i) = \min\Big(\frac{\Omega(\boldsymbol{\theta}_i, \boldsymbol{\Theta}_i, \boldsymbol{\varphi}_i)}{\Omega(\boldsymbol{\theta}_{i-1}, \boldsymbol{\Theta}_{i-1}, \boldsymbol{\varphi}_{i-1})} \hat{A}_i, f_c(\varepsilon, \hat{A}_i)\Big).
  \label{eq:decision-making-objective}
  \end{equation} \\
  Calculate $L_{\phi}(\mathbf{\hat{e}}, \mathbf{l}; \boldsymbol{\varphi})$ as defined in (\ref{eq:cross-entropy-loss}). \\
  Update the actor network as follows:
  \begin{equation}
  \boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_{i} + \alpha_a \nabla J(\hat{A}_i).
  \label{eq:actor-net-update}
  \end{equation} \\
  Update the critic network as follows:
  \begin{equation}
  \boldsymbol{\Theta}_{i+1} = \boldsymbol{\Theta}_{i} - \alpha_c \nabla L_c(\boldsymbol{\Theta}).
  \label{eq:critic-net-update}
  \end{equation} \\
  Update the convolutional network as follows:
  \begin{equation}
  \boldsymbol{\varphi}_{i+1} = \boldsymbol{\varphi}_{i} - \alpha_n \nabla L_{\phi}(\mathbf{\hat{e}}, \mathbf{l}; \boldsymbol{\varphi}).
  \label{eq:convo-net-update}
  \end{equation}
 }
\label{algo:hybrid-learner}
\end{algorithm}

\vspace{-0.1cm}
\subsection{Meta-learning Algorithm}
\label{sec:meta-learning}
\begin{algorithm}[t]
\caption{Meta-learning algorithm for better recognizing neurodiversity}
 \textbf{Input}: 
 Initialize $\boldsymbol{\theta}_0$, $\boldsymbol{\Theta}_0$ and $\boldsymbol{\varphi}_0$ at random. \\
 \For{\text{i = 0, 1, 2}, $\ldots$}{
 
 Collect a set of trajectories $\mathcal{D}_i$ as in (\ref{eq:trajectory}). \\
 Compute advantage estimator function $\hat{A}_i$ over $\mathcal{D}_i$ as in (\ref{eq:gae-lambda}). \\
 Calculate the decision-making objective $J(\hat{A}_i)$ as in (\ref{eq:decision-making-objective}). \\
 \For{\text{k = 1, 2}, $\ldots$, \text{K}}{
 	Compute:
 	\begin{equation}
 	\tilde{\boldsymbol{\varphi}}_k = \boldsymbol{\varphi}_i + g_1 + g_2 + \ldots + g_w.
 	\label{eq:meta-gradient-expand}
 	\end{equation}
 }
 
 Update the actor network as in (\ref{eq:actor-net-update}). \\
 Update the critic network as in (\ref{eq:critic-net-update}). \\
 Update the convolutional network as follows:
  \begin{equation}
  \boldsymbol{\varphi}_{i+1} = \boldsymbol{\varphi}_{i} + \alpha_{M} \frac{1}{K} \sum_{k=1}^{K} ( \tilde{\boldsymbol{\varphi}}_k - \boldsymbol{\varphi}_i).
  \label{eq:meta-update}
  \end{equation}
  \\
 }
\label{algo:reptile}
\end{algorithm}

Based on our observation in Fig.~\ref{fig:eeg-example} about the neurodiversity in the brain activities of different BCI users, we are  interested in learning a meta-model that jointly optimizes the BCI prediction performance given the BCI signals from different distributions, i.e., phases and amplitudes.
In particular, our optimization problem (\ref{eq:qoe-maximization}) now can be rewritten as:
\vspace{-0.1cm}
\begin{subequations}
\label{eq:min-latency}
\begin{align}
\mathcal{P}_2: \max_{\boldsymbol{\rho}, \mathbf{p}, \boldsymbol{\tau}, \phi} \quad & \frac{1}{K} \sum_{k \in \mathcal{K}} Q_{k}(\boldsymbol{\rho}, \mathbf{p}, \boldsymbol{\tau}, \phi) \\
\textrm{s.t.} \quad & (\ref{eq:qoe-maximization}\text{b})-(\ref{eq:qoe-maximization}\text{e}), \\
\quad & \phi_k\big(\mathbf{\hat{e}}_k(t), \mathbf{l}_k(t)\big) \in \{0, 1\},
\end{align}
\label{eq:meta-optimization}
\end{subequations}
where $\phi_k\big(\mathbf{\hat{e}}_k(t), \mathbf{l}_k(t)\big)$ is now the BCI classifier of the user $k$-th, given the noisy BCI signals $\mathbf{\hat{e}}_k(t)$ and labels $\mathbf{l}_k(t)$ sampled from the user $k$-th.
As observed from the problems $\mathcal{P}_1$ and $\mathcal{P}_2$, the difference between the two problems is the constraint $(\ref{eq:qoe-maximization}f)$ and $(\ref{eq:meta-optimization}c)$.
In $\mathcal{P}_1$, we consider that the BCI signals share the same distribution and we ignore the fact that the BCI signals might be significantly different in terms of amplitudes and phases, i.e., neurodiversity \cite{kang2014bayesian, vezard2015eeg, zhang2017multi}. As the result, the trained BCI classifier may obtain a higher performance with the BCI signals from an BCI user than that from another user.
With the formulated problem in $\mathcal{P}_2$, we aim to improve the performance of the BCI classifier, regardless of the neurodiversity when the number of BCI users increases. 

As observed from problem $\mathcal{P}_2$ in (\ref{eq:meta-optimization}), a naive solution can be implementing $K$ Hybrid learners for $K$ BCI classifiers to deal with the diverse distributions problem. In this way, we have to train and maintain multiple learning models for a single purpose of improving the BCI prediction accuracy. However, this implementation may require a switch operator that selects the optimal model given a random or unknown input of BCI signals. This assumption about the switch operator might not be feasible in practice \cite[Ch. 7]{goodfellow2016deep}. Moreover, maintaining multiple learning models can cause extra cost of training and maintenance, and thus yielding negative impacts on the servers/systems. 
In our work, we aim to train a single Meta-learner that can achieve high prediction accuracy on the BCI signals with unknown and diverse distributions. 

The proposed meta-learning algorithm is proposed in Algorithm 2. In particular, the detailed training process is as follows.
First, the parameters of the three deep neural networks are initialized at random (line 1 of Algorithm 2).
Similar to the Algorithm 1, the Meta-leaner first computes the decision-making objective $J(\hat{A}_i)$ (lines 3, 4, and 5 of Algorithm 2).
Next, the Meta-learner computes $w$-step meta-gradients, denoted by $\tilde{\boldsymbol{\varphi}}_k$, expanded from the current parameter $\boldsymbol{\varphi}_i$ as in (\ref{eq:meta-gradient-expand}) (line 7).
In (\ref{eq:meta-gradient-expand}), $g_1, g_2, \ldots, g_w$ are the gradients computed by SGD (Stochastic Gradient Decent) over $w$ mini-batches of the inputs, i.e., $\mathbf{\hat{e}}_k(t)$.
In other words, the inputs $\mathbf{\hat{e}}_k(t)$ are divided into $w$ mini-batches to compute $w$ gradients $g_1, g_2, \ldots, g_w$.
After that, the actor network and critic network are updated based on equations (\ref{eq:actor-net-update}) and (\ref{eq:critic-net-update}), respectively (lines 9 and 10).
Finally, the convolutional network is updated with (\ref{eq:meta-update}), where $\alpha_{M}$ is the meta-learning stepsize (line 11).
The calculation of meta-gradients and meta-updates in equations (\ref{eq:meta-gradient-expand}) and (\ref{eq:meta-update}) over $K$ users helps to learn a policy that is equivalent to a distilled policy of $K$ separate Hybrid learners.
During the training phase, each training sample from user $k$ will be included in the gradient computation of equations (\ref{eq:meta-gradient-expand}) and (\ref{eq:meta-update}), which helps the Meta-learner to obtain a refined gradient across all $K$ tasks.
During the testing phase, the Meta-learner receives random EEG signal samples from the users without awareness of which EEG samples belong to which users.

We develop our meta-learning algorithm based on a scalable first-order meta-learning algorithm, named Reptile~\cite{nichol2018first}. 
Similar to the Reptile, our algorithm belongs to the gradient-based meta-learning family in which the gradient computation requires extra steps as described in equations (\ref{eq:meta-gradient-expand}) and (\ref{eq:meta-update}). In return for extra gradient computations, the gradient-based meta-learning algorithm is suitable for any type of deep learning model, i.e., model-agnostic \cite{finn2017model}.
Note that unlike the original Reptile algorithm which is only applicable for the classification problem, our Meta-learner can deal with the joint decision-making and classification problem, thanks to the hybrid design.

In the next section, we empirically show that our proposed hybrid learning algorithm can outperform current state-of-the-art reinforcement learning algorithms in dealing with the decision-making problem. Furthermore, we show that our proposed meta-learning algorithm significantly improves the BCI prediction performance, compared with our proposed hybrid learning and other supervised learning algorithms.
Thanks to the novel meta-learning process, our Meta-learner can understand the neurodiversity at a deeper level than the hybrid learning algorithm, resulting in higher and robust classification performance under different settings.
Note that the proposed meta-learning algorithm only requires additional gradient computations and the Meta-learner can be deployed at the controller of the WES similar to the Hybrid learner in Fig.~\ref{fig:hybrid-learner}.

\section{Performance Evaluation with BCI Datasets}
\label{sec:performance-evaluation}
\subsection{Data Preprocessing}
\begin{table}[t]
\begin{tabular}{c|l|l}
\hline & 
\textbf{Communication Parameters} & \textbf{Default Settings} \\
\hline$M$ & Number of radio & $6$ resource \\ 
& resource blocks & blocks \\
\hline$K$ & Number of users & $3$ users\\
\hline$P_B$ & Power of the WES & $1$ W~\cite{chen2020joint}\\
\hline$P_{\max}$ & Power of the user headset & [-20: 20] dBm\\
\hline$B^U$ & Uplink bandwidth & $1$ MHz~\cite{chen2020joint}\\
\hline$B^D$ & Downlink bandwidth & $20$ MHz~\cite{chen2020joint}\\
\hline$N_0$ & Noise & $-174$ dBm\\
\hline$I_m$ & Interference & $-10$ dBm\\
$I_D$ &  & \\
\hline$D_{\max}$ & Maximum round-trip delay & $10$ milliseconds \\

\hline &
\textbf{Computation Parameters} & \textbf{Default Settings} \\
\hline$\upsilon$ & Computation capacity of & $[2.3 \times 10^{-6}: 2.3]$ \\
            & the WES        & GHz \\
\hline & Video quality for the VR   & $1280 \times 702$ \\
            &  processing & pixels\\
\hline &
\textbf{BCI Parameters} & \textbf{Default Settings} \\
\hline & BCI dataset & \cite{goldberger2000physiobank} \\
\hline $J$ & Number of BCI channels & 64 channels \\
\hline $C$ & Number of class labels & 4 classes\\
\hline & Number of BCI users & [3:7] users \\
\hline & Sampling rate & 160 Hz \\
\hline &
\textbf{Algorithms' Parameters} & \textbf{Default Settings} \\
\hline $O$& Trajectories' length & 100 steps \\
\hline $T$& Total training time steps & $3 \times 10^5$ steps \\
\hline $\gamma$ & Advantage estimator's  & 0.99 \cite{schulman2017proximal} \\
& parameter & \\
\hline $\lambda$ & Advantage estimator's & 0.95 \cite{schulman2017proximal} \\
& parameter & \\
\hline $\varepsilon$ & Clip ratio & 0.2 \cite{schulman2017proximal} \\
\hline $\alpha_a$ & Actor network's  & $5 \times 10^{-5}$ \\
& learning rate & \\
\hline $\alpha_c$ & Critic network's & $5 \times 10^{-4}$ \\
& learning rate & \\
\hline $\alpha_n$ & Convo. network's & $2 \times 10^{-3}$ \\
& learning rate & \\
\hline $\alpha_{M}$ & Meta-learning rate & 1.0 \cite{nichol2018first} \\
\hline $w$ & Number of meta-gradient & 3 steps \\
& steps & \\
\hline
\end{tabular}
\caption{Parameter settings.}
\label{tab:simulation-settings}
\end{table}

We conduct extensive simulations to evaluate the system performance as follows.
For the BCI classification problem, we use a public dataset from~\cite{goldberger2000physiobank}. The dataset contains the experiment results of 109 participants. Each participant is instructed to do an  action per experimental run. The actions are opening/closing eyes, fists, and feet. 
On each experimental run, the EEG signals are obtained through 64 EEG channels with the BCI2000 system~\cite{schalk2004bci2000}. The sampling rate is 160 Hz. In our setting, we consider four different actions, i.e., $C=4$, that are open eyes, close eyes, close fist, and move feet.
In the default setting, we consider BCI signals from three users as illustrated in Fig.~\ref{fig:eeg-example}. 
We adopt data processing pipeline from \cite{zhanggithub}. In particular, the data processing is as follow.
The collected BCI signals of each user have 255,680 data samples. 
Because EEG signals are temporal data, we split the data stream into different segments and iteratively input the segments into the deep neural networks.
Each segment contains 16 EEG samples, which is equivalent to 0.1 second as the sampling frequency is at 160 Hz. 
The overlapping rate between two adjacent segments is set at 50\%.
After segmentation, the data samples are then normalized with z-score normalization technique \cite{zhanggithub}.
Finally, the data samples are split into training set and testing set with the ratio is 80:20, respectively.
Note that similar to other BCI research works in the literature, we consider a classification setting with discrete number of actions \cite{zhang2021survey}. For capturing full human body movement with high degree-of-freedom (DoF), current BCI technologies are not yet ready because of the complexity of BCI signals. 
In this work, we only focus on enabling potential of BCI for the Metaverse without focusing on realistic avatar/human motion capture techniques.
Human motion capture is another topic that is outside the scope of our work.

The Hybrid learner's architecture is set as follow.
The actor network and critic network are multilayer perceptron networks (MLPs) that consist of one input layer, two hidden layers, and one output layer.
In our default setting, the number of users is set at $K=3$.
In this case, the number of input neurons at the input layer is 6, which is equivalent to the total sizes of the channel state and the CPUs, i.e., $\{h_1(t), h_2(t), h_3(t), u_1(t), u_2(t), u_3(t)\}$.
Note that we only measure the 3 most available CPUs of the WES to feed into the deep neural networks for simplicity.
The number of the output neurons at the output layer is 9, which is corresponding to the number of radio resource block allocation variable, the power allocation variable, and the computing resource allocation variable, i.e., $\{\rho_1(t), \rho_2(t), \rho_3(t), p_1(t), p_2(t), p_3(t), \tau_1(t), \tau_2(t), \tau_3(t)\}$.
For the CNN, we use each EEG channel as feature input for the CNN of the Hybrid learner. Thus, we have 64 input features and 4 class labels to train with the CNN.
We use Adam to optimize the parameters of the deep neural networks.
The Meta-learner reuses the same architecture of the Hybrid learner. 
The difference is computing the meta-gradients and meta-updates in equations (\ref{eq:meta-gradient-expand}) and (\ref{eq:meta-update}), respectively.
For this, an additional SGD algorithm is used to calculate $w$ gradient steps with respect to $w$ mini-batches of the input data, i.e., equation (\ref{eq:meta-gradient-expand}). 

For the decision-making problem, i.e., radio and computing resource allocation, we conduct an experiment as illustrated in Fig.~\ref{fig:fov-render} to measure the processing latency at the WES. For the uplink and downlink latency, we use the Rayleigh fading to simulate the dynamics of the time-varying wireless channel. The number of radio resource blocks is set to $M=6$. The number of Metaverse users is set at $K=3$.  Note that we use the fixed number of Metaverse users $K=3$ but the number of BCI users can be greater than that. In such cases, each Metaverse user can be considered as a group of multiple BCI users, given the same amount of radio resources. 
The details of our parameter settings are shown in Table.~\ref{tab:simulation-settings}.

\begin{figure*}[t]
	\centering
	\begin{subfigure}[b]{0.31\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/01-training-rew.pdf}
		\caption{}
	\end{subfigure}%
	~ 
	\begin{subfigure}[b]{0.31\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/01-training-test_acc.pdf}
		\caption{}
	\end{subfigure}%
	~
	\begin{subfigure}[b]{0.31\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/01-training-delay.pdf}
		\caption{}
	\end{subfigure}
	\caption{(a) Normalized QoE, (b) classification accuracy, and (c) average round-trip VR delay values.} 
	\label{fig:training-results}
\end{figure*}

\begin{figure*}[t]
	\centering
	\begin{subfigure}[b]{0.22\linewidth}
		\centering
		\includegraphics[width=1.2\linewidth]{figures/05-eta-1-cdf.pdf}
		\caption{$(\eta_1, \eta_2) = (0.25, 1.0)$}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.22\linewidth}
		\centering
		\includegraphics[width=1.2\linewidth]{figures/05-eta-3-cdf.pdf}
		\caption{$(\eta_1, \eta_2) = (1.0, 1.0)$}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.22\linewidth}
		\centering
		\includegraphics[width=1.2\linewidth]{figures/05-eta-5-cdf.pdf}
		\caption{$(\eta_1, \eta_2) = (4.0, 1.0)$}
	\end{subfigure}
	\caption{CDF values of user QoE in different scenarios to show the trade-off between system's latency and classification accuracy.} 
	\label{fig:eta-varies}
\end{figure*}

In comparison with our proposed algorithms, we introduce the following baselines.

\begin{itemize}
\item Proximal Policy Optimization (PPO) \cite{schulman2017proximal}: PPO is a state-of-the-art reinforcement learning algorithm for decision-making problems with continuous action values. Our Hybrid learner also adopts the actor-critic architecture and policy clipping techniques from PPO to achieve robust performance. In particular, the PPO baseline contains an actor network and critic network. We directly use this architecture to learn the QoE defined in (\ref{eq:qoe-calculation}). By maximizing the average QoE, the PPO baseline is expected to reduce the loss $L_{\phi}$ and the round-trip VR delay $D_k$.
\item Vanilla Policy Gradient (VPG) \cite{sutton2018reinforcement}: VPG is a classic policy gradient algorithm for decision-making problems with continuous action values. The VPG baseline also uses the actor-critic architecture. However, the VPG algorithm does not have the embedded advantage function and the policy clipping technique. 
\item Support-Vector Machine (SVM) \cite{cortes1995support}: SVM is a classic supervised learning algorithm and is a robust benchmark for classification problems. In our simulations, we use SVM as a baseline to evaluate the classification accuracy of our proposed algorithms that are developed on deep neural networks. For a fair comparison, we consider the following setting to give SVM advantages compared with our proposed algorithms. First, we replace the convolutional network in our Hybrid learner with the SVM and we keep the actor network and critic network similar to those of the Hybrid learner. As a result, the SVM-based Hybrid learner can still deal with both decision-making and classification problems. Second, we train the SVM with training data that are collectively fed into the input of the SVM. In other words, all the training data is stored and reused at the WES. We observe that this training method can significantly boost the performance of the SVM. Otherwise, if we apply the same training method as our proposed algorithms, i.e., the training data at each time step is removed after feeding into the deep neural networks, the performance of the SVM is significantly decreased.
\end{itemize}

\subsection{Experiment Results}

We first illustrate the training performance of the proposed algorithm and the baselines in Fig.~\ref{fig:training-results}. In Fig.~\ref{fig:training-results}(a), we can observe the increase in QoE values of all the algorithms during 3,000 training episodes. These results imply that all algorithms have the ability to learn a good policy given the dynamics of the environment. 
After training, the trained model can be used for testing on test data.
In Fig.~\ref{fig:training-results}(b), we can observe that the proposed Meta-learner and Hybrid learner can obtain highly accurate predictions on BCI signals. The SVM baseline also achieves similar performance with the Hybrid learner. Specifically, the convergence speed of the SVM baseline is higher than those of the Meta-learner and Hybrid learner because the training data is stored and reused at the WES when we train the SVM baseline. However, the SVM baseline with a high demand for the amount of input data can only converge to the accuracy that is similar to the Hybrid learner. 
More interestingly, the Meta-learner can achieve higher accuracy with an additional update of 3 meta-gradient steps. In other words, our proposed Meta-learned can achieve the best performance by maintaining limited storage of BCI signals and computing load in the  system.  
The SVM baseline also shows its superior performance over the PPO and VPG baselines thanks to its robust classification ability.
Another observation on the performance of the PPO and VPG baselines shows that they are not effective to deal with the classification problem with only reinforcement learning design. With the number of class labels being $C=4$, the prediction accuracy of the PPO and VPG baselines are just slightly higher than the random prediction, i.e., $25\%$. Although the maximization of the QoE with reinforcement learning algorithms like PPO and VPG may result in reducing the classification loss theoretically, the exploration of such algorithms over the observation space makes the deep neural networks not have enough guidance to learn a good policy. This problem is known as a sparse reward problem in popular reinforcement learning settings \cite{sutton2018reinforcement}.
In Fig.~\ref{fig:training-results}(c), we can observe that all algorithms can converge to round-trip delay values that are less than the requirement $D_{\max} = 0.01$ second (i.e., 10 milliseconds). Thanks to the reinforcement learning techniques, all algorithms can learn the dynamics of radio and computing resources of the system. 

\begin{figure}[t]
	\centering
	\begin{subfigure}[b]{0.5\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/02-power-test_acc.pdf}
		\caption{}
	\end{subfigure}%
	\begin{subfigure}[b]{0.5\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/02-power-delay.pdf}
		\caption{}
	\end{subfigure}
	\caption{(a) BCI classification accuracy and (b) round-trip VR delay of the algorithms with testing data when the maximum power of the headsets vary.} 
	\label{fig:power-varies}
\end{figure}

It is noted that the above training results are obtained with the setting $(\eta_1, \eta_2) = (1, 1)$. In other words, the importance of BCI classification and radio/computing resource allocation are similar. 
To evaluate the trade-off between the the system's delay (which is directly affected by the resource allocation) and the classification accuracy, we consider different scenarios by changing the relative importance between the BCI classification and radio/computing resource allocation in Fig.~\ref{fig:eta-varies}.  The Cumulative Distribution Function (CDF) curves are obtained by averaging over 3,000 training episodes. In Fig.~\ref{fig:eta-varies}(a), we can observe that when the BCI classification is more important, i.e., $\eta_2 > \eta_1$, the QoE of the proposed algorithms is higher than those of the PPO and VPG baselines. For example, with the same CDF value at 0.6, the proposed Meta-learner and Hybrid learner can achieve higher QoE values, i.e.,  0.89 and 0.82, respectively. With the SVM baseline, although its CDF curve converges to a similar point as that of the Meta-learner, i.e., 0.98, the tail of its CDF starts at higher QoE values than the other algorithms. The reason is that SVM can achieve faster convergence on the same classification problem thanks to the advantages in training as we have discussed above.
In Fig.~\ref{fig:eta-varies}(b), when the values of $\eta_1$ and $\eta_2$ are similar, the proposed Meta-learner and Hybrid learner clearly outperform the baselines and achieve similar performance as the SVM baseline.
In addition, when $\eta_1$ increases, i.e., $\eta_1 > \eta_2$ in Fig.~\ref{fig:eta-varies}(c), the CDF curves of the PPO and VPG algorithms shift toward the right and become closer to the CDF curves of the Meta-learner, Hybrid learner, and the SVM baseline. The reason is that when the importance of the radio/computing resource allocation increases, the QoE values are less affected by the the classification accuracy and the system's delay contributes more to the QoE.
From the three different settings above, we can conclude that our proposed algorithms are more robust, compared with the baseline algorithms.
Thanks to the ability to collectively store the training data, the SVM baseline can also achieve robust performance.  
In the rest of simulations, we consider different settings with the values $(\eta_1, \eta_2) = (1, 1)$.

Next, we vary the maximum power value at the headsets of users, i.e., $P_{\max}$, to evaluate the impacts of the power allocation on the system performance. In Fig.~\ref{fig:power-varies}(a), we can observe that when the maximum power of the headsets decreases, the accuracy of the prediction decreases. The reason is that with the low level of power allocated to the radio resource blocks, the SINR at the WES may significantly decrease, resulting in the high packet error rate $\epsilon_k$ in (\ref{eq:epsilon}).
Specifically, our proposed Meta-learned achieves the best classification accuracy under all the considered settings. Similar to the observation from Fig.~\ref{fig:training-results}(b), the classification accuracy values obtained by the SVM baseline are similar to those of the Hybrid learner and are much higher than those of the PPO and VPG baselines. 
In Fig.~\ref{fig:power-varies}(b), we can observe that the increase of power results in the decrease of the round-trip VR delay. The latency values obtained by our proposed algorithms are lowest among those of the baseline algorithms due to two main reasons. 
First, they utilize start-of-the-art actor-critic architecture and policy clipping techniques of PPO. Second, our new design in forwarding the losses through the actor network, critic network, and convolutional network, as described in Section~\ref{sec:learning-algorithms}, enables the realization of Hybrid learner and Meta-learner that the compound objective $Q_k$ involves distinct learning goals, i.e., BCI classification and radio, computing resource allocation; and thus facilitating the training process.

Furthermore, we evaluate the impacts of the computing capacity of the WES on the system performance by increasing the CPU capacity of the WES from $2.3 \times 10^{-6}$ GHz to $2.3$ GHz.
In Fig.~\ref{fig:cpu-varies}(a), it can be observed that the increase in the CPU capacity of the WES does not have impact on the classification accuracy. These results imply that with the limited CPU capacity, our proposed algorithms with deep neural networks still achieve good predictions on the BCI signals. Unlike our proposed algorithms with advanced architecture designs, the PPO and VPG baselines only obtain around $30\%$ of classification accuracy which is slightly higher than the chance level $25\%$. In Fig.~\ref{fig:cpu-varies}(b), we can observe that the increase in CPU capacity results in the decrease of the round-trip VR delay. The reason is that with lower CPU capacity, the WES takes a longer time to pre-precess VR contents for the users, thus yielding higher latency.
Note that the decrease in round-trip VR delay in Fig.~\ref{fig:cpu-varies}(b) is less significant than the decrease in Fig.~\ref{fig:power-varies}(b). The reason is that the transmit power has more impacts on the system's delay, compared with the computing capacity.

\begin{figure}[t]
	\centering
	\begin{subfigure}[b]{0.5\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/03-cpu-test_acc.pdf}
		\caption{}
	\end{subfigure}%
	\begin{subfigure}[b]{0.5\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/03-cpu-delay.pdf}
		\caption{}
	\end{subfigure}
	\caption{(a) Classification accuracy and (b) round-trip VR delay of the algorithms with testing data when the CPU capacity of the WES varies.} 
		\label{fig:cpu-varies}
\end{figure}

Finally, we evaluate the classification challenges caused by the diversity of BCI signals. In Fig.~\ref{fig:subject-varies}, we increase the number of BCI users from three to seven. In Fig.~\ref{fig:subject-varies}(a), the classification accuracy of the proposed Hybrid learner significantly decreases when the number of BCI users increases. 
The reason for this is that although the proposed Hybrid leaner with advanced architecture can obtain good prediction accuracy, the learner cannot deal with the diversity of the BCI signals as input data. 
As explained Fig.~\ref{fig:eeg-example}, the BCI signals are highly individual so that the responses of different users on the same experimental instruction, e.g., fist movement, are different in both amplitude and phase. Therefore, a conventional convolutional neural network is not sufficient to learn from the neurodiversity of the BCI signals. 
The classification accuracy values obtained by the baseline SVM are higher than those of the Hybrid learner. The reason is that the SVM baseline is given advantages during training in which all the training data is iteratively collected and reused at each training episode. In other words, the SVM baseline is given more training data at a training episode to achieve such highly accurate predictions. However, this comes with the cost of storage and longer training time.
Unlike the Hybrid learner and SVM baseline, the proposed Meta-learner shows its capability of learning a distilled model that can better classify BCI signals from different users, reflected by the non-decreasing classification accuracy. 
Specifically, with up to seven BCI users, our Meta-learner only needs one supporting meta-gradient update from each DA to achieve similar accuracy as obtained from the case with three BCI users. 
The results suggest that the proposed Meta-learner is sample-efficient and practical in systems with limited storage capacity. 
Note that in the multi-person BCI classification setting above, our Meta-learner does not require any additional feature extraction methods to achieve similar accuracy results reported in \cite{kang2014bayesian, vezard2015eeg, zhang2017multi}.
Moreover, the works in \cite{kang2014bayesian, vezard2015eeg, zhang2017multi} only consider classification problem for the raw and noise-free BCI signals while we also consider the possible packet errors of transmitting BCI signals over noisy channels.
In Fig.~\ref{fig:subject-varies}(b), we can observe that the round-trip VR delay are not affected by the increase of BCI users. Our proposed algorithms also achieve lower latency, compared with the baseline algorithms. 

\begin{figure}[t]
	\centering
	\begin{subfigure}[b]{0.5\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/04-eeg-test_acc.pdf}
		\caption{}
	\end{subfigure}%
	\begin{subfigure}[b]{0.5\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/04-eeg-delay.pdf}
		\caption{}
	\end{subfigure}
	\caption{(a) Classification accuracy and (b) round-trip VR delay of the algorithms with testing data when the number of BCI users varies.} 
	\label{fig:subject-varies}
\end{figure}

\vspace{-0.3cm}
\section{Conclusion}
\label{sec:conclusion}
In this work, we have introduced a novel over-the-air BCI-enabled framework for human-centric Metaverse applications.
The Digital Avatars can learn from human brain signals to predict the actions of the users under controlled permissions.
In addition, the novel system design enables the WES to jointly optimize radio, computing resources of the system as well as the classification performance under the dynamics of the wireless environment and Metaverse users' behaviors. This was realized by the proposed hybrid learning algorithm and meta-learning algorithm to simultaneously address the mixed decision-making and classification problems. The hybrid learning algorithm have shown its effectiveness in handling mixed decision-making and classification problem of our system. This was thanks to the novel architecture which consists of three deep neural networks to split, compute, and backpropagate the losses. We have further proposed the meta learning algorithm as an improved version of the hybrid learning algorithm to deal with the neurodiversity of the brain signals from different users.
Extensive experiments with real-world datasets showed that our proposed algorithms can achieve prediction accuracy up to 82\%. More interestingly, our proposed algorithms also reduced the VR latency of the system, resulting in potential reduction of VR sickness and enhancing user QoE in  future Metaverse applications.

% Using .bib
%\bibliography{bios}  
%\bibliographystyle{ieeetr}
% Manual references
\begin{thebibliography}{100}
\bibliographystyle{IEEEtranS}

\bibitem{xu2022full}
M. Xu, \textit{et al.}, ``A full dive into realizing the edge-enabled metaverse: Visions, enabling technologies, and challenges," \textit{IEEE Communications Surveys \& Tutorials}, vol. 25, no. 1, pp. 656-700, Nov. 2022.

\bibitem{caserman2020survey}
P. Caserman, A. G.-Agundez, and S. Göbel, ``A survey of full-body motion reconstruction in immersive virtual reality applications," \textit{IEEE Transactions on Visualization and Computer Graphics}, vol. 26, no. 10, pp. 3089-3108, Oct. 2020.
%------------%

%------------%
\bibitem{meng2022sampling}
Z. Meng, C. She, G. Zhao, and D. De Martini, ``Sampling, communication, and prediction co-design for synchronizing the real-world device and digital model in metaverse," \textit{IEEE Journal on Selected Areas in Communications}, vol. 41, no. 1, pp. 288–300, Nov. 2022.

\bibitem{lam2022human}
K. Y. Lam, L. Yang, A. Alhilal, L.-H. Lee, G. Tyson, and P. Hui, ``Human-avatar interaction in metaverse: Framework for full-body interaction," in \textit{Proceedings of the 4th ACM International Conference on Multimedia in Asia}, Dec. 2022, pp. 1-7.

\bibitem{sun2022augmented}
Z. Sun, M. Zhu, X. Shan, and C. Lee, ``Augmented tactile-perception and haptic-feedback rings as human-machine interfaces aiming for immersive interactions," \textit{Nature Communications}, vol. 13, no. 1, p. 5224, Sep. 2022.

\bibitem{le2022noncontact}
X. Le, Q. Shi, Z. Sun, J. Xie, and C. Lee,  ``Noncontact human–machine interface using complementary information fusion based on mems and triboelectric sensors," \textit{Advanced Science}, vol. 9, no. 21, p. 2201056, Jul. 2022.
%------------%

%\bibitem{ng2022unified}
%W. C. Ng, W. Y. B. Lim, J. S. Ng, Z. Xiong, D. Niyato, and C. Miao, ``Unified resource allocation framework for the edge intelligence-enabled metaverse, in \textit{IEEE International Conference on Communications},  May 2022, pp. 5214–5219.
%
%\bibitem{xu2022wireless}
%M. Xu, D. Niyato, J. Kang, Z. Xiong, C. Miao, and D. I. Kim, ``Wireless edge-empowered metaverse: A learning-based incentive mechanism for virtual reality," in \textit{IEEE International Conference on Communications}, May 2022, pp. 5220–5225.

\bibitem{fernandes2016combating}
A. S. Fernandes and S. K. Feiner, ``Combating vr sickness through subtle dynamic field-of-view modification," in \textit{IEEE Symposium on 3D User Interfaces (3DUI)}, Mar. 2016, pp. 201–210.

\bibitem{cho2014bci}
O.-H. Cho and W.-H. Lee, ``Bci sensor based environment changing system for immersion of 3d game," \textit{International Journal of Distributed Sensor Networks}, vol. 10, no. 5, p. 620391, May 2014.

\bibitem{kim2017measurement}
H. G. Kim, W. J. Baddar, H.-t. Lim, H. Jeong, and Y. M. Ro, ``Measurement of exceptional motion in vr video contents for vr sickness assessment using deep convolutional autoencoder," in \textit{Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology}, Nov. 2017, pp. 1–7.

%\bibitem{koohestani2019knowledge}
%A. Koohestani, D. Nahavandi, H. Asadi, P. M. Kebria, A. Khosravi, R. Alizadehsani, and S. Nahavandi, ``A knowledge discovery in motion sickness: a comprehensive literature review," \textit{IEEE Access}, vol. 7, pp. 85755–85770, Jun. 2019.

\bibitem{tidoni2014audio}
E. Tidoni, P. Gergondet, A. Kheddar, and S. M. Aglioti, ``Audio-visual feedback improves the bci performance in the navigational control of a humanoid robot," \textit{Frontiers in Neurorobotics}, vol. 8, p. 20, Jun. 2014.

\bibitem{dasdemir2022brain}
Y. DAŞDEMİR, ``A brain-computer interface with gamification in the Metaverse," \textit{Dicle Üniversitesi Mühendislik Fakültesi Mühendislik Dergisi}, vol. 13, no. 4, Oct. 2022, pp. 645-652.

\bibitem{wu2017evaluation}
H. Wu, \textit{et al.}, ``Evaluation of motor training performance in 3d virtual environment via combining brain-computer interface and haptic feedback," \textit{Procedia Computer Science}, vol. 107, pp. 256–261, Jan. 2017.

\bibitem{cheng2020brain}
N. Cheng, \textit{et al.}, ``Brain-computer interface-based soft robotic glove rehabilitation for stroke," \textit{IEEE Transactions on Biomedical Engineering}, vol. 67, no. 12, pp. 3339–3351, Apr. 2020.

\bibitem{lee2022toward}
S.-H. Lee, Y.-E. Lee, and S.-W. Lee, ``Toward imagined speech based smart communication system: Potential applications on metaverse conditions," in \textit{International Winter Conference on Brain-Computer Interface (BCI)}, Feb. 2022, pp. 1–4.

%\bibitem{chang2020virtual}
%E. Chang, H. T. Kim, and B. Yoo, ``Virtual reality sickness: A review of causes and measurements," \textit{International Journal of Human–Computer Interaction}, vol. 36, no. 17, pp. 1658–1682, Jul. 2020.

\bibitem{schalk2004bci2000}
G. Schalk, D. J. McFarland, T. Hinterberger, N. Birbaumer, and J. R. Wolpaw, ``Bci2000: A general-purpose brain-computer interface (bci) system," \textit{IEEE Transactions on Biomedical Engineering}, vol. 51, no. 6, pp. 1034–1043, Jun. 2004.

%\bibitem{zhao2009eeg}
%Q. Zhao, L. Zhang, and A. Cichocki, ``Eeg-based asynchronous bci control of a car in 3d virtual reality environments," \textit{Chinese Science Bulletin}, vol. 54, no. 1, pp. 78–87, Jan. 2009.

%\bibitem{coogan2018brain}
%C. G. Coogan and B. He, ``Brain-computer interface control in a virtual reality environment and applications for the internet of things," \textit{IEEE Access}, vol. 6, pp. 10840–10849, Feb. 2018.

%\bibitem{lin2008development}
%C.-T. Lin, \textit{et al.}, ``Development of wireless brain computer interface with embedded multitask scheduling and its application on real-time driver's drowsiness detection and warning," \textit{IEEE Transactions on Biomedical Engineering}, vol. 55, no. 5, pp. 1582–1591, Apr. 2008.

\bibitem{he2015wireless}
W. He, Y. Zhao, H. Tang, C. Sun, and W. Fu, ``A wireless bci and bmi system for wearable robots," \textit{IEEE Transactions on Systems, Man, and Cybernetics: Systems}, vol. 46, no. 7, pp. 936–946, Dec. 2015.

\bibitem{saad2019vision}
W. Saad, M. Bennis, and M. Chen, ``A vision of 6g wireless systems: Applications, trends, technologies, and open research problems," \textit{IEEE Networks}, vol. 34, no. 3, pp. 134–142, Oct. 2019.

\bibitem{kasgari2019human}
A. T. Z. Kasgari, W. Saad, and M. Debbah, ``Human-in-the-loop wireless communications: Machine learning and brain-aware resource management," \textit{IEEE Transactions on Communications}, vol. 67, no. 11, pp. 7727–7743, Jul. 2021.

\bibitem{hieu2022toward}
N. Q. Hieu, D. T. Hoang, D. N. Nguyen, and E. Dutkiewicz, ``Toward BCI-enabled Metaverse: A joint radio and computing resource allocation approach,"\textit{ arXiv preprint}, arXiv:2212.08811, 2022.

\bibitem{kang2014bayesian}
H. Kang and S. Choi, ``Bayesian common spatial patterns for multi-subject eeg classification," \textit{Neural Networks}, vol. 57, pp. 39–50, Sep. 2014.

\bibitem{vezard2015eeg}
L. V {\'e}ard, P. Legrand, M. Chavent, F. Fa{\"\i}ta-A{\"\i}nseba, and L. Trujillo, ``Eeg classification for the detection of mental states," \textit{Applied Soft Computing}, vol. 32, pp. 113–131, Jul. 2015.

\bibitem{zhang2017multi}
X. Zhang, L. Yao, D. Zhang, X. Wang, Q. Z. Sheng, and T. Gu, ``Multi- person brain activity recognition via comprehensive eeg signal analysis," in \textit{Proceedings of the 14th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services}, Nov. 2017, pp. 28–37.

\bibitem{moioli2021neurosciences}
R. C. Moioli, \textit{et al.}, ``Neurosciences and wireless networks: The potential of brain-type communications and their applications," \textit{IEEE Communications Surveys \& Tutorials}, vol. 23, no. 3, pp. 1599–1621, Jun. 2021.

\bibitem{goldberger2000physiobank}
A. L. Goldberger, \textit{et al.}, ``Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals," \textit{Circulation}, vol. 101, no. 23, pp. e215–e220, 2000.

\bibitem{schulman2017proximal}
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, ``Proximal policy optimization algorithms," \textit{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem{zhang2021survey}
X. Zhang, L. Yao, X. Wang, J. Monaghan, D. Mcalpine, and Y. Zhang, ``A survey on deep learning-based non-invasive brain signals: Recent advances and new frontiers," \textit{Journal of Neural Engineering}, vol. 18, no. 3, p. 31002, Mar. 2021.

\bibitem{bernal2022galea}
G. Bernal, N. Hidalgo, C. Russomanno, and P. Maes, ``Galea: A physiological sensing system for behavioral research in virtual environments," in \textit{IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, Mar. 2022, pp. 66–76.

\bibitem{jurcak200710}
V. Jurcak, D. Tsuzuki, and I. Dan, ``10/20, 10/10, and 10/5 systems revisited: Their validity as relative head-surface-based positioning systems," \textit{Neuroimage}, vol. 34, no. 4, pp. 1600–1611, Feb. 2007.

\bibitem{corbillon2017viewport}
X. Corbillon, G. Simon, A. Devlic, and J. Chakareski, ``Viewport-adaptive navigable 360-degree video delivery," in \textit{IEEE International Conference on Communications}, May 2017, pp. 1–7.

\bibitem{lotte2012combining}
F. Lotte, \textit{et al.}, ``Combining bci with virtual reality: Towards new applications and improved bci," in \textit{Towards practical brain-computer interfaces}, B. Allison, S. Dunne, R. Leeb, J. D. R. Millian, A. Nijholt, Ed., Berlin, Heidelberg, Germany: Springer, 2012, pp. 197–220.

\bibitem{chun2016bci}
J. Chun, B. Bae, and S. Jo, ``Bci based hybrid interface for 3d object control in virtual reality," in \textit{International Winter Conference on Brain- Computer Interface (BCI)}, Feb. 2016, pp. 1–4.

\bibitem{arico2017passive}
P. Arico, G. Borghini, G. Di Flumeri, N. Sciaraffa, A. Colosimo,
and F. Babiloni, ``Passive bci in operational environments: Insights,
recent advances, and future trends," \textit{IEEE Transactions on Biomedical Engineering}, vol. 64, no. 7, pp. 1431–1436, Apr. 2017.

\bibitem{mudin}
M. Ibrahim, ``Vue-vr: A wrapper of panolens for building vr applications with vue based on threejs." \url{https://github.com/mudin/vue-vr} (accessed Feb. 6, 2023).

\bibitem{chen2020joint}
M. Chen, Z. Yang, W. Saad, C. Yin, H. V. Poor, and S. Cui, ``A joint learning and communications framework for federated learning over wireless networks," \textit{IEEE Transactions on Wireless Communications}, vol. 20, no. 1, pp. 269–283, Oct. 2020.

\bibitem{xi2011general}
Y. Xi, A. Burr, J. Wei, and D. Grace, ``A general upper bound to evaluate packet error rate over quasi-static fading channels," \textit{IEEE Transactions on Wireless Communications}, vol. 10, no. 5, pp. 1373–1377, May 2011.

\bibitem{morash2008classifying}
V. Morash, O. Bai, S. Furlani, P. Lin, and M. Hallett, ``Classifying eeg signals preceding right hand, left hand, tongue, and right foot movements and motor imageries," \textit{Clinical Neurophysiology}, vol. 119, no. 11, pp. 2570–2578, Oct. 2008.

\bibitem{zhang2019drl260}
Y. Zhang, P. Zhao, K. Bian, Y. Liu, L. Song, and X. Li, ``DRL360: 360-degree video streaming with deep reinforcement learning," in \textit{IEEE International Conference on Computer Communications}, May 2019, pp. 1252-1260.

\bibitem{mao2017neural}
H. Mao, R. Netravali, and M. Alizadeh, ``Neural adaptive video streaming with pensieve," in \textit{Proceedings of the Conference of the ACM Special Interest Group on Data Communication}, Aug. 2017, pp. 197-210.

\bibitem{lee2020neural}
S.-H. Lee, M. Lee, and S.-W. Lee, ``Neural decoding of imagined speech and visual imagery as intuitive paradigms for bci communication," \textit{IEEE Transactions on Neural Systems and Rehabilitation Engineering}, vol. 28, no. 12, pp. 2647–2659, Nov. 2020.

\bibitem{goodfellow2016deep}
I. Goodfellow, Y. Bengio, and A. Courville, \textit{Deep Learning}. Cambridge, MA, USA: MIT Press, 2016.

\bibitem{nichol2018first}
A. Nichol, J. Achiam, and J. Schulman, ``On first-order meta-learning algorithms," \textit{arXiv preprint arXiv:1803.02999}, 2018.

\bibitem{finn2017model}
C. Finn, P. Abbeel, S. Levine, ``Model-agnostic meta-learning for fast adaptation of deep networks," in \textit{International Conference on Machine Learning}, Jul. 2017, pp. 1126-1135.

\bibitem{zhanggithub}
X. Zhang, ``Deep learning for brain-computer interface (bci).: \url{https://github.com/xiangzhang1015/Deep-Learning-for-BCI} (accessed Feb. 6, 2023).

\bibitem{sutton2018reinforcement}
R. S. Sutton and A. G. Barto, \textit{Reinforcement Learning: An Introduction}. Cambridge, MA, USA: MIT press, 2018.

\bibitem{cortes1995support}
C. Cortes and V. Vapnik, ``Support-vector networks," \textit{Machine Learning}, vol. 20, no. 3, pp. 273–297, Sep. 1995.

\end{thebibliography}
\end{document}
