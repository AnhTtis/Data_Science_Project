% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)
\pdfoutput=1
\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{ amssymb }
\usepackage{makecell}
\usepackage{color, colortbl}
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

\renewcommand\theadalign{bc}
\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{\Gape[4pt]}
\renewcommand\cellgape{\Gape[4pt]}

\definecolor{Gray}{gray}{0.9}

\DeclareMathOperator{\IM}{\mathcal{I}}
\DeclareMathOperator{\TM}{\mathcal{T}}
\DeclareMathOperator{\LM}{\mathcal{L}}
\DeclareMathOperator{\PM}{\mathcal{P}}
\DeclareMathOperator{\NM}{\mathcal{N}}
\DeclareMathOperator{\EX}{\mathbb{E}}

\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

\ExplSyntaxOn
\NewDocumentCommand{\longdash}{ O{2} }
 {
  --\prg_replicate:nn { #1 - 1 } { \negthinspace -- }
 }
\ExplSyntaxOff


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,hypertexnames=false]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{5588} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}
%\addtolength{\topmargin}{-12.0pt}
\pagestyle{fancy}
\setlength{\headheight}{6.0pt}
\setlength{\headsep}{4mm}
\topmargin -0.625in
\addtolength{\headsep}{0.25in}
\fancyhead{}
\fancyhead[L]{Accepted for publication at CVPR 2023}
\setlength{\footskip}{50pt}
%\renewcommand{\headruleskip}{0mm}
%\renewcommand{\headrulewidth}{2pt}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Best of Both Worlds: Multimodal Contrastive Learning with Tabular and Imaging Data}

\author{Paul Hager\textsuperscript{1,2} \qquad
    Martin J. Menten\textsuperscript{1,2,3} \qquad Daniel Rueckert\textsuperscript{1,2,3} \\
    \textsuperscript{1}Technical University of Munich, \textsuperscript{2}Klinikum Rechts der Isar, \textsuperscript{3}Imperial College London \\
    {\tt\small \{paul.hager, martin.menten, daniel.rueckert\}@tum.de}
}
%\author{Paul Hager\\
%Institute for AI and Informatics in Medicine\\
%Klinikum rechts der Isar\\
%Technical University of Munich, Germany\\
%{\tt\small paul.hager@tum.de}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Martin J. Menten\\ 
%\and 
%Daniel R{\"u}ckert
%}
\maketitle
\thispagestyle{fancy}

%%%%%%%%% ABSTRACT
\begin{abstract}
    
    Medical datasets and especially biobanks, often contain extensive tabular data with rich clinical information in addition to images.
    In practice, clinicians typically have less data, both in terms of diversity and scale, but still wish to deploy deep learning solutions.
    Combined with increasing medical dataset sizes and expensive annotation costs, the necessity for unsupervised methods that can pretrain multimodally and predict unimodally has risen.
    
    To address these needs, we propose the first self-supervised contrastive learning framework that takes advantage of images and tabular data to train unimodal encoders.
    Our solution combines SimCLR and SCARF, two leading contrastive learning strategies, and is simple and effective.
    In our experiments, we demonstrate the strength of our framework by predicting risks of myocardial infarction and coronary artery disease (CAD) using cardiac MR images and 120 clinical features from 40,000 UK Biobank subjects.
    Furthermore, we show the generalizability of our approach to natural images using the DVM car advertisement dataset. 
    
    
    We take advantage of the high interpretability of tabular data and through attribution and ablation experiments find that morphometric tabular features, describing size and shape, have outsized importance during the contrastive learning process and improve the quality of the learned embeddings.
    Finally, we introduce a novel form of supervised contrastive learning, label as a feature (LaaF), by appending the ground truth label as a tabular feature during multimodal pretraining, outperforming all supervised contrastive baselines.\footnote{\url{https://github.com/paulhager/MMCL-Tabular-Imaging}}
    
    %We make our code available online under @INSERT-LINK-HERE@.
    
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{figs/MMCL_Graphical_Abstract_Vertical.drawio.pdf}
  \caption{We combine imaging and tabular data in a contrastive learning framework. We observe that morphometric features, describing shape and size, are of outsized importance in multimodal contrastive training and their inclusion boosts downstream task performance. By simply adding the label as a tabular feature we introduce a novel form of supervised contrastive learning that outperforms all other supervised contrastive strategies.}
  \label{fig:graphicalabstract}
\end{figure*}

Modern medical datasets are increasingly multimodal, often incorporating both imaging and tabular data. 
Images can be acquired by computed tomography, ultrasound, or magnetic resonance scanners, while tabular data commonly originates from laboratory tests, medical history and patient lifestyle questionnaires.
Clinicians have the responsibility to combine and interpret this tabular and imaging data to diagnose, treat, and monitor patients.
For example, cardiologists may ask about a patients' family history and record their weight, cholesterol levels, and blood pressure to better inform diagnoses when examining images of their heart.

%Multimodal data is also crucial to advance the understanding of diseases. 
Beyond diagnostics, multimodal data is also crucial to advance the understanding of diseases motivating the creation of biobanks.
Going far beyond the scale of typical datasets in hospitals, biobanks pool vast amount of information from large populations. %to understand the causes of complex, widespread diseases such as cancer, diabetes, neurodegeneration and cardiovascular disease
Multimodal biobanks include the German National Cohort \cite{noauthor_german_2014} with 200,000 subjects, Lifelines\cite{sijtsma_cohort_2022} with 167,000 subjects, and the UK Biobank\cite{sudlow_uk_2015} with 500,000 subjects. 
The UK Biobank includes thousands of data fields from patient questionnaires, laboratory tests, and medical examinations, in addition to imaging and genotyping information.
Biobanks have already proven useful in the training of machine learning models to predict many diseases such as anaemia\cite{mitani_detection_2020}, early brain aging\cite{jonsson_brain_2019} and cardiovascular disease\cite{alaa_cardiovascular_2019, rim_deep-learning-based_2021}.


There is a substantial interest in deploying algorithms that have been developed using these large-scale population studies in clinical practice. 
However, acquiring the same quality of data, both in terms of diversity of modalities and number of features, is not feasible in a busy clinical workflow\cite{dugdale_time_1999}. 
Furthermore, low disease frequencies make supervised solutions hard to train.
Consequently, there is a clear need for unsupervised strategies that can learn from biobank scale datasets and be applied in the clinic where considerably less data, in size and dimension, is available.

\paragraph{Our contribution}
To address these needs, we propose the first contrastive framework that utilizes imaging and tabular data, shown in figure \ref{fig:graphicalabstract}. 
Our framework is based on SimCLR\cite{chen_simple_2020} and SCARF\cite{bahri_scarf_2022}, two leading contrastive learning solutions, and is simple and effective.
We demonstrate the utility of our pretraining strategy on the challenging task of predicting cardiac health from MR images. 
Beyond medical imaging, we show that our framework can also be applied when combining natural images and tabular data using the DVM car advertisement dataset\cite{huang_dvm-car_2021}.

Experimentally, we observe that our tool leverages morphometric features during contrastive learning. 
Morphometric features describe the size and shape of an object and therefore correlate with extractable imaging features.
We quantitatively demonstrate the importance of these features in the contrastive learning process using attribution methods, such as integrated gradients\cite{sund_intgrad}, and ablation experiments. 

Finally, we introduce a new supervised contrastive learning method called label as a feature (LaaF). 
By appending the target label as a tabular feature, our method outperforms previously published strategies that incorporate labels into the contrastive framework.
Our method is also highly flexible and can be combined with the aforementioned strategies to further improve performance.
%Our framework and code is made available under <INSERTLINK>.

\section{Related Work}
\label{sec:relwork}

\textbf{Self-supervised learning with images} aims to extract useful features from unlabeled data. 
Historically, this was attempted by solving hand-crafted pretext tasks such as jigsaw puzzles \cite{pang_solving_2020,taleb_multimodal_2021,zhuang_self-supervised_2019,taleb_3d_2020}, colorization \cite{larsson_learning_2017,zhang_colorful_2016,vondrick_tracking_2018}, image inpainting \cite{pathak_context_2016}, and context prediction \cite{chen_self-supervised_2019,doersch_unsupervised_2015,bai_self-supervised_2019}.
The major difficulties with using these methods is that they tend to overfit on the specifics of their pretext task, limiting their utility for downstream tasks.

\textbf{Contrastive learning} has emerged as a popular and performant successor to pretext tasks.
Contrastive learning trains encoders by generating augmented views of a sample and maximizing their projected embedding similarity while minimizing the similarity between the projected embeddings of other samples \cite{hadsell_dimensionality_2006}.
It has been popularized by implementations such as SimCLR \cite{chen_simple_2020}, MOCO \cite{he_momentum_2020}, BYOL, \cite{grill_bootstrap_2020} and others \cite{caron_unsupervised_2021,chen_exploring_2021,chen_big_2020,caron_emerging_2021,zbontar2021barlow}.
We use the contrastive framework of SimCLR as the basis for our work.

\textbf{Deep learning with tabular data} has recently begun to yield results that are competitive with classical machine learning methods \cite{borisov_deep_2022,arik2021tabnet,hollmann_tabpfn_2022}, though for many applications they still underperform simpler algorithms \cite{shwartz-ziv_tabular_2021,borisov_deep_2022}.
Self-supervised learning is being explored in the tabular domain with frameworks such as VIME\cite{yoon_vime_2020} and contrastive methods such as SubTab\cite{ucar_subtab_2021} and SCARF\cite{bahri_scarf_2022}.
%We use the tabular corruption augmentations outlined in SCARF in the tabular portion of our framework.
We base our tabular augmentations on those used in SCARF.
%In the tabular branch of our method, we adapt SCARF and its tabular corruption augmentations.

\textbf{Multimodal contrastive learning with images} is becoming more important as the number of multimodal datasets increases and multimodal training strategies become more effective.
Approaches such as CLIP\cite{radford_learning_2021}, which combines images and text, are general-purpose vision models that are able to solve new tasks in a zero-shot manner.
% ConVIRT\cite{zhang_contrastive_2022}
Some of these models use internet-size datasets and are described as foundational models, such as UniCL\cite{yang_unified_2022}, Florence\cite{yuan_florence_2021}, ALIGN\cite{jia_scaling_2021}, and Wu Dao 2.0 \cite{demo_wu_nodate}.
Outside of the image-language domain, there has also been progress on multimodal contrastive learning using two different imaging modalities \cite{taleb_multimodal_2021,pielawski_comir_2020}, audio and video \cite{ma_active_2022}, video and text \cite{Zolfaghari_crossclr,xu_videoclip_2021}, and imaging and genetic data \cite{taleb_contig_2021}.
While literature on generative self-supervised tabular and imaging models \cite{antelmi2021combining}\cite{ko2022deep} exists, it is limited in scope, using only two or four clinical features.
To the best of our knowledge, there is no implementation of a contrastive self-supervised framework that incorporates both images and tabular data, which we aim to address with this work.

\textbf{Supervised learning within contrastive frameworks} has been shown to outperform the binary cross entropy loss in some cases and create more robust embeddings\cite{khosla_supervised_2020}.
Supervised contrastive learning\cite{khosla_supervised_2020} maximizes the similarity of the projected embeddings of all views in a batch from the same class.
This also addresses the problem of false negatives in contrastive learning, which is that the contrastive loss minimizes projected embedding similarity between different samples even if they are part of the same class according to a downstream task (i.e. false negatives).
By utilizing the available labels, supervised contrastive learning is able to circumvent this problem and outperforms other methods that heuristically identify and eliminate false negatives \cite{chen_incremental_2022,huynh_boosting_2022}.
We propose a solution for supervised learning in our multimodal contrastive framework that takes advantage of the unique strengths of tabular data by appending the label as a tabular feature.
%

\section{Methods}
\label{sec:methods}

\subsection{Contrastive Framework for Tabular and \\Imaging Data}

We base our multimodal framework on SimCLR\cite{chen_simple_2020}.
Let our dataset be $x$ and a unique sample be $j$.
Each batch contains pairs of imaging $x_{j_i}$ and tabular $x_{j_t}$ samples which are augmented.
Each augmented imaging sample $x_{j_i}$ in the batch is passed through an imaging encoder $f_{\theta_I}$ to generate the embedding $\widetilde{x_{j_i}}$.
Each augmented tabular sample $x_{j_t}$ in the batch is passed through a tabular encoder $f_{\theta_T}$ to generate the embedding $\widetilde{x_{j_t}}$.
The embeddings are propagated through separate projection heads $f_{\phi_I}$ and $f_{\phi_T}$ and brought into a shared latent space as projections $z_{j_i}$ and $z_{j_t}$ which are then L2 normalized onto a unit hypersphere.
The projections are pulled and pushed in the shared latent space according to the ``CLIP'' loss \cite{radford_learning_2021}, which maximizes the cosine similarity of projections from the same sample and minimizes the similarity of projections from different samples.
In contrast to the original InfoNCE\cite{oord_representation_2019} loss and following CLIP, we only contrast projections between modalities, never within one modality.

$i$ and $t$ can be used interchangeably and so, without loss of generality, the projection of an image is defined as 
\begin{equation}
    z_{j_i} = f_{\phi_I}(f_{\theta_I}(x_{j_i}))
\end{equation}
Considering all subjects $\NM$ in a batch, the loss for the imaging modality is
\begin{equation}  
    \ell_{i,t} = -\displaystyle\sum_{j\in\NM}log{\frac{\text{exp}(\text{cos}(z_{j_i}, z_{j_t}) / \tau)}{\displaystyle\sum_{k\in\NM,k\neq j}\text{exp}(\text{cos}(z_{j_i}, z_{k_t}) / \tau)}}.
\end{equation}
$\ell_{t,i}$ is calculated analagously and the total loss is thus
\begin{equation}
    \LM = \lambda \ell_{i,t} + (1-\lambda)\ell_{t,i}.
\end{equation}
\label{eq:combined_loss}

The images in the batch are augmented based on the standard contrastive augmentations specified in \cite{chen_simple_2020}: horizontal flips, rotations, color jitter, and resized crop. 
We do not use Gaussian blurring on the cardiac dataset in order to preserve fine-grained features in the MR images\cite{azizi_big_2021}.
To effectively augment the tabular data, a fraction of a subject's features are randomly selected to be ``corrupted'' (i.e. augmented), following \cite{bahri_scarf_2022}. 
Each corrupted feature's value is sampled with replacement from all values for that feature seen in the dataset.
Full implementation details are found in supplementary materials (SM) section \ref{app:implementation}.

\subsection{Explainability using Integrated Gradients}

To improve our understanding of the dynamics of the multimodal training, we analyze the importance of the individual tabular features in generating the embeddings.
Using test samples, we take the pretrained tabular encoder of our multimodal model and calculate the integrated gradients\cite{sund_intgrad} of each dimension of the embeddings.
This integrates the gradients of the encoder along the straightline path from a baseline sample, in our case a zero vector, to the test sample in question.
This yields the importance value of each tabular feature in generating the downstream prediction for that sample.
We then take the absolute value and calculate the mean importance of each feature across all embedding dimensions.
Categorical features have their means summed over all choices.
We use these results to categorize features and better understand how training in a multimodal setting influences unimodal performance.

\subsection{Contrastive Learning with Labels}

Incorporating labels into the contrastive learning process is typically done by modifying the loss function\cite{khosla_supervised_2020,chen_incremental_2022}.
We propose to take advantage of the unique structure of tabular data and directly append the downstream class label as a tabular feature.
We explore the benefits of combining our method with existing strategies for incorporating labels into the training process, such as supervised contrastive learning and false negative elimination.

\section{Experiments and Results}
\begin{table*}[t]
\tiny
    \caption{Performance of our framework on the tasks of myocardial infarction, coronary artery disease (CAD) and DVM car model prediction from images. Our multimodal pretrained model outperforms all other models on every task. The best performing model for every input type is displayed in \textbf{bold} font. Our method is highlighted gray.}
    \centering
    \resizebox{2\columnwidth}{!}
    {
    \renewcommand*{\arraystretch}{1.5}
    \begin{tabular}{| c | c  c | c c | c c |}
\hline
\thead{Model} & \thead{AUC (\%) \\ Frozen / Infarction} & \thead{AUC (\%)\\Trainable / Infarction} & \thead{AUC (\%)\\Frozen / CAD} & \thead{AUC (\%)\\Trainable / CAD} & \thead{Top-1 Accuracy (\%)\\Frozen / DVM} & \thead{Top-1 Accuracy (\%)\\Trainable / DVM} \\
\hline
Supervised ResNet50 & 72.37$\pm$1.80 & 72.37$\pm$1.80 & 68.84$\pm$2.54 & 68.84$\pm$2.54 & \underline{87.97$\pm$2.20} & 87.97$\pm$2.20 \\
\hline
SimCLR & \underline{73.69$\pm$0.36} & \underline{73.62$\pm$0.70} & \underline{69.86$\pm$0.21} & \underline{71.46$\pm$0.71} & 65.48$\pm$0.48 & 88.76$\pm$0.81 \\
BYOL & 69.18$\pm$0.43 & 70.69$\pm$2.09 & 66.91$\pm$0.19 & 70.66$\pm$0.22 & 59.73$\pm$0.28 & \underline{89.18$\pm$0.90} \\
SimSiam & 71.72$\pm$0.18 & 72.31$\pm$0.26 & 67.79$\pm$0.12 & 70.13$\pm$0.35 & 22.11$\pm$2.83 & 87.43$\pm$0.88 \\
BarlowTwins & 66.06$\pm$1.11 & 71.35$\pm$1.23 & 62.90$\pm$0.23 & 69.63$\pm$0.58 & 52.57$\pm$0.08 & 85.47$\pm$0.82 \\
\rowcolor{Gray}
Multimodal Imaging & \textbf{76.35$\pm$0.19} & \textbf{75.37$\pm$0.43} & \textbf{74.45$\pm$0.09} & \textbf{73.08$\pm$0.75} & \textbf{91.43$\pm$0.13} & \textbf{93.00$\pm$0.18}\\
\hline
\end{tabular}
    }
    \label{res:table_all_res}
\end{table*}

\subsection{Datasets}

As a first dataset, we used cardiac MR images and clinical information from the UK Biobank population study.
Our aim was to predict past and future risk of myocardial infarction and coronary artery disease (CAD). 
We used short axis cardiac MR images, which provide a cross-sectional view of the left and right ventricle of the heart.
The images used are two-channel 2D images whose channels are the middle baso-apical slice of the short axis cardiac MRI image at end-systolic and end-diastolic phases.
The short axis images were chosen as left ventricular function and morphometry are impacted by both CAD\cite{zabalgoitia_impact_2001} and cardiac infarction\cite{sutton_left_2000}.
Conversely, the left ventricle is a high-risk area in which early warning signs of cardiac dysfunction may be visible\cite{angeli_left_2018,tsao_left_nodate,raisi-estabragh_cardiovascular_2021}.
The images were zero-padded to 210x210 pixels and min-max normalized to a range of 0 to 1.
After augmentations (see SM section \ref{app:pretraining}), final image size was 128x128 pixels.

A subset of demographic, lifestyle and physiological features out of 5,000 data fields included in the UK Biobank dataset were selected for the tabular data. 
These features were chosen based on published correlations with cardiac outcomes. 
They include information about the subjects' diet\cite{yu_cardiovascular_2018}, physical activity\cite{mora_physical_2007}, weight\cite{powell-wiley_obesity_2021}, alcohol consumption\cite{piano_alcohols_2017}, smoking status\cite{lakier_smoking_1992}, and anxiety\cite{celano_anxiety_2016}.
Only features with at least 80\% coverage were included.
The full list of features can be found in SM section \ref{app:clinical_features}.
The continuous tabular data fields were standardized using z-score normalization with a mean value of 0 and standard deviation of 1 while categorical data was one-hot encoded.
The total size of the dataset was 40,874 unique subjects, split into 29,428 training, 7,358 validation, and 4,088 testing pairs of imaging and tabular data.

The first prediction target is myocardial infarction as defined by the International Classification of Diseases (ICD10) code.
ICD10 codes are maintained by the World Health Organization, used to record diagnoses during hospital admissions, and made available through the UK Biobank.
The second prediction target is CAD, also defined by ICD10 code.
The ICD codes used for each class are listed in SM section \ref{app:icd_codes}.
We combine past and future diagnoses since infarctions and CAD can go undiagnosed for many years and may only be recorded once a patient has to be treated for a severe cardiac event, making it difficult to establish when the disease began \cite{nesto1999screening,valensi2011prevalence}.
As both diseases are low frequency in the dataset (3\% for infarction and 6\% for CAD), finetune train splits were balanced using all positive subjects and a static set of randomly chosen negative subjects.
The test and validation sets were left untouched.

The second dataset is the Data Visual Marketing (DVM) dataset that was created from 335,562 used car advertisements\cite{huang_dvm-car_2021}.
The DVM dataset contains 1,451,784 images of cars from various angles (45 degree increments) as well as their sales and technical data.
For our task we chose to predict the car model from images and the accompanying advertisement data. 
The images were all 300x300 pixels and after augmentations (see SM section \ref{app:pretraining}) final image size was 128x128.
All fields that provided semantic information about the cars in question were included, such as width, length, height, wheelbase, price, advertisement year, miles driven, number of seats, number of doors, original price, engine size, body type, gearbox type, and fuel type.
Unique target identifying information like brand and model year were excluded.
The width, length, height and wheelbase values were randomly jittered by 50 millimeters so as not to be uniquely identifying.
We pair this tabular data with a single random image from each advertisement, yielding a dataset of 70,565 train pairs, 17,642 validation pairs, and 88,207 test pairs.
Car models with less than 100 samples were removed, resulting in 286 target classes.


To handle missing tabular data, we used an iterative multivariate imputer which models missing features as a function of existing features over multiple imputation rounds.
This was done after normalization, to ensure that the means and standard deviations were calculated only from recorded values.
The missing features were initialized with the mean and then entire columns were imputed in order from least amount of missing features, to most amount of missing features.
A regressor was fit with all other features as input and the currently examined column as dependent variable.
This process was repeated a maximum of $n$ times or until $\frac{max(abs(X_t - X_{t-1}))}{max(abs(X))} < \epsilon$, where $X_t$ is the feature vector being imputed at time point $t$ and $\epsilon$ is a provided tolerance, typically $1e^{-3}$.
Categorical data was then rounded to the nearest integer i.e. category.

\subsection{Experimental Setup}

All imaging encoders are ResNet50s\cite{he_deep_2016} that generate embeddings of size 2048.
Our multimodal model uses a tabular encoder which is a multilayer perceptron (MLP) with one hidden layer of size 2048 that generates embeddings of size 2048.
All weights are randomly initialized.
Our imaging projector is an MLP with one hidden layer of size 2048 and our tabular projector generates projections directly from the embeddings with a fully connected layer.
Projection size is 128 following \cite{chen_simple_2020}.
After pretraining, the projection head is removed and a fully connected layer to the output class nodes is added.

To evaluate the effectiveness of our model, we compare it to a fully supervised ResNet50 as well as multiple contrastive solutions such as SimCLR \cite{chen_simple_2020}, BYOL\cite{grill2020bootstrap}, SimSiam\cite{chen2021exploring}, and BarlowTwins\cite{zbontar2021barlow}.
We use linear probing of frozen networks to evaluate the quality of the learned representations\cite{chen_simple_2020,chen_incremental_2022,khosla_supervised_2020}.
We also benchmark each network while leaving all weights trainable during finetuning, as this typically improves upon the frozen counterpart and would be used in practice.
For the cardiac classification tasks we use area under the receiver operating characteristic curve (AUC) as our metric because the dataset is severely unbalanced for our targets. 
With only 3-6\% positive labels, a model that always predicts the negative class would achieve an accuracy of 94-97\%. 
For the DVM cars dataset we report top-1 accuracy as we have 280+ classes.
Results are reported as a mean and standard deviation calculated over five different seeds set during finetuning.
Both cardiac tasks were evaluated from a single pretrained model.
Full experimental details including the setup of the baseline models can be found in SM section \ref{app:implementation}. 

\begin{figure*}
  \centering
  \begin{subfigure}{0.32\linewidth}
    \includegraphics[width=\linewidth]{figs/Infarction_low_data.png}
    %\caption{Infarction}
    \label{fig:lowdata_infarction}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.32\linewidth}
    \includegraphics[width=\linewidth]{figs/CAD_low_data.png}
    %\caption{CAD}
    \label{fig:lowdata_cad}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.32\linewidth}
    \includegraphics[width=\linewidth]{figs/DVM_low_data.png}
    %\caption{DVM}
    \label{fig:lowdata_dvm}
  \end{subfigure}
    \caption{Performance of the imaging models with different number of finetuning training samples. Shaded regions indicate 95\% confidence intervals. Pretraining with both images and tabular data excels at all data quantities and is well suited for rare disease identification when only tens or hundreds of labels are available.}
    \label{fig:low_data}
\end{figure*}

\subsection{Multimodal Pretraining Improves Unimodal Prediction}

Our main results showing the strength of our multimodal pretraining framework are found in table \ref{res:table_all_res}.
All results shown use only images as input, as this is the clinically relevant task.
Results using tabular inputs are shown in SM section \ref{app:tab_results}.
Our multimodal pretrained model substantially outperformed all other models on all three tasks and both finetuning strategies.
SimCLR generally outperformed all other contrastive strategies, highlighting our decision to base our multimodal strategy on it. 

On the cardiac tasks, the multimodal model achieved its best results when freezing the encoder.
We hypothesize that this is due to overfitting on the imaging modality during finetuning.
We suspect when provided with an imaging-only signal during finetuning that the encoder discarded features that were learned from tabular data.

When predicting the car model from images of the DVM dataset, our multimodal model outperformed other pretraining strategies by even larger margins during frozen linear probing.
This shows that for a homogeneous dataset like the DVM cars, having an additional differentiating signal such as tabular data can better align the learned features to the downstream target classes.



\begin{table*}[t]
    \caption{Frozen finetune performance of multimodal models pretrained with all features, morphometric features only, and no morphometric features. Even though the total importance of morphometric features was less than that of non-morphometric features on the cardiac task, their exclusion worsened or had equal impact on downstream performance. Best score is in \textbf{bold} font, second best \underline{underlined}.}
    \centering
    \resizebox{2\columnwidth}{!}
    {
    \renewcommand*{\arraystretch}{1.5}
    \begin{tabular}{|c|c c c c|c c c|}
    \hline
\thead{Experiment} & \thead{Tabular\\Features} & \thead{Importance\\Percentage (\%)} & \thead{AUC (\%)\\Infarction} & \thead{AUC (\%)\\CAD} & \thead{Tabular\\Features} & \thead{Importance\\Percentage (\%)} & \thead{Top-1 Accuracy (\%)\\DVM} \\
\hline
MM Imaging Baseline & 117 & 100.0 & \textbf{76.35$\pm$0.19} & \textbf{74.45$\pm$0.09} & 16 & 100.0 & \underline{91.43$\pm$0.13} \\
\hline
Morphometric Features & 24 & 47.0 & 75.22$\pm$0.30 & \underline{73.71$\pm$0.09} & 5 & 56.4 &  \textbf{92.33$\pm$0.05}\\
Non-Morphometric Features & 93 & 53.0 & \underline{75.46$\pm$0.19} & 72.18$\pm$0.25 & 11 & 43.6 & 89.14$\pm$0.24 \\
\hline
% Shuffle 78.57$\pm$0.14 \\
    \end{tabular}
    }
    \label{tab:dvm_phys}
\end{table*}


\subsection{Multimodal Pretraining is Beneficial in Low-Data Regimes}
\label{sec:lowdata}

When investigating rare medical conditions with very low label frequencies, models must be performant when few positive samples are available.
In order to test the performance of the learned encoders in this low-data regime, we sampled the finetuning training dataset to 10\% and 1\% of its original size, with each subset being wholly contained within each superset.
Because of the low frequencies of the positive class, this resulted in balanced training set sizes of 200 (10\%) and 20 (1\%) for myocardial infarction and 400 (10\%) and 40 (1\%) for CAD.
The test and validation set was kept identical to the full data regime.
A graphical representation of the results is shown in figure \ref{fig:low_data}.
We find that that in low-data regimes our multimodal framework generally outperforms the imaging-only contrastive method by larger margins than with full data.
This indicates improved representations that require less finetuning samples to achieve the same performance and higher utility when rare diseases are the target.
We again see that our multimodal frozen encoder consistently outperforms our trainable encoder.
We benchmark against SimCLR as it was the strongest contrastive pretraining strategy.
Comparisons against all pretraining strategies can be found in SM section \ref{app:low_data_contrastive}.

\subsection{Morphometric Features Improve Embedding Quality}

To explore why training in a multimodal fashion improves the unimodal encoders, we analyzed the contributions of the tabular features to the improved embeddings.
A unique strength of tabular data is that each of its input nodes corresponds to a single feature. %or a single option for a feature in the case of categorical data.
We divided the features into two categories, morphometric and non-morphometric.
Morphometric features are related to size and shape and have direct correlates in the images, such as ventricular volume, weight or car length.
Using integrated gradients, we calculated the importance of each feature across the test samples.

To generate the cardiac embeddings, the seven most important features were all morphometric, even though they only represent one fifth of all features.
Furthermore, all 24 morphometric features were found in the top half of the importance rankings.
These results are shown in figure \ref{fig:cardiac_ig_features} and SM section \ref{app:explain_physical_features}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/IG_Emb_20.png}
    \caption{Top 20 most impactful features for calculating embeddings determined using integrated gradient feature attribution method. The morphometric features are colored orange and comprise 15 of the 20 most impactful features.}
    \label{fig:cardiac_ig_features}
\end{figure}

We hypothesize that the model focuses on morphometric tabular features because these have direct correlates in the images.
Extracting these features in both modalities increases the projected embedding similarity and minimizes the contrastive loss, as shown in SM section \ref{app:morph_pretrain}.


This is corroborated by the Guided GradCam results, shown in figure \ref{fig:gradcam}, where it is seen that the imaging model primarily focused on the left ventricle.
Incidentally, the three most important features according to the integrated gradients are left ventricle mass (LVM), left ventricle end systolic volume (LVESV), and left ventricle end diastolic volume (LVEDV).
To analyze the impact of these tabular features on downstream performance, we trained once with only morphometric features and once with only non-morphometric features.
We observed that the morphometric features have an outsized impact on generating the embeddings.
Table \ref{tab:dvm_phys} shows that even though they only contribute 46.99\% of the total importance and constitute 24 out of 117 features, their exclusion on CAD prediction degrades performance, and is equal to the exclusion of non-morphometric features on infarction prediction.
In general, this shows that the multimodal pretraining process is fairly robust to feature selection, especially when the total feature set is so large and there exist collinearities within the data.


Similar results are seen on the DVM dataset where the top 4 most important features are all morphometric features, despite there only being 5 morphometric features in total, as seen in figure \ref{fig:dvm_ig_features}.
Table \ref{tab:dvm_phys} shows the effect of removing these features, which led to a substantial drop in accuracy. 
When training with only morphometric features the accuracy increased highlighting their importance on tasks that are shape driven, like car model identification.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/Top20DVM_AV_Physical.png}
    \caption{Impact of features for calculating DVM embeddings determined using integrated gradient feature attribution method. The morphometric features are colored orange and comprise the four most impactful features. }
    \label{fig:dvm_ig_features}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/gradcam.png}
    \caption{Guided Grad-CAM results for predicting CAD on test images. The most important features are centered around the left ventricle, matching the most important tabular features.}
    \label{fig:gradcam}
\end{figure}

\begin{table*}[t]
\tiny
    \caption{Frozen evaluation when incorporating labels into the contrastive pretraining process. Our Label as a Feature (Laaf) strategy consistently outperforms supervised contrastive learning (SupCon) and false negative elimination (FN Elimination), either alone or in combination. Best score is in \textbf{bold} font, second best \underline{underlined}. Our methods are highlighted gray. A dash indicates failure to converge.}
    \centering
    \resizebox{2\columnwidth}{!}
    {
    \renewcommand*{\arraystretch}{1.5}
    \begin{tabular}{|c | c | c | c c c|}
\hline
\thead{Contrastive} & \thead{Label Used} & \thead{Model} & \thead{AUC (\%)\\Infarction} & \thead{AUC (\%)\\CAD} & \thead{Top-1 Accuracy (\%)\\DVM} \\
\hline
\rowcolor{Gray}
\checkmark &  & Multimodal Imaging Baseline & \underline{76.35$\pm$0.19} & \textbf{74.45$\pm$0.09} & 91.43$\pm$0.13 \\
 & \checkmark & Supervised ResNet50 & 72.37$\pm$1.80 & 68.84$\pm$2.54 & 87.97$\pm$2.20 \\
\hline
\rowcolor{Gray}
\checkmark & \checkmark & Label as a Feature (LaaF) & \textbf{76.60$\pm$0.42} & \underline{73.76$\pm$0.31} & 93.56$\pm$0.08 \\
\checkmark & \checkmark & FN Elimination & 75.38$\pm$0.06 & 72.45$\pm$0.09 & 92.39$\pm$0.18 \\
\rowcolor{Gray}
\checkmark & \checkmark & FN Elimination + LaaF & 75.30$\pm$0.05 & 72.39$\pm$0.08 & \underline{94.07$\pm$0.05} \\
\checkmark & \checkmark & SupCon & \longdash[3] & \longdash[3] & 93.82$\pm$0.11 \\
\rowcolor{Gray}
\checkmark & \checkmark & SupCon + LaaF & \longdash[3] & \longdash[3] & \textbf{94.40$\pm$0.04} \\
\hline
    \end{tabular}
    }
    \label{tab:supcon}
\end{table*}


\subsection{Appending the Label as a Tabular Feature Boosts Supervised Contrastive Strategies}

We introduce a novel form of supervised contrastive learning by including the ground truth label as a tabular feature (LaaF).
We benchmark the effectiveness of this approach by comparing it to both supervised contrastive learning with full label information as well as false negative elimination with full label information.
The results presented in table \ref{tab:supcon} show that LaaF outperforms or rivals both strategies.

On the cardiac binary classification tasks there is a sharp class imbalance. 
97\% of the subjects are negative for myocardial infarction, leading false negative elimination to remove large portions of each batch before calculating the contrastive loss.
This leads to worse representations as batch sizes are drastically reduced during training.
As contrastive learning, and especially SimCLR, is known to be sensitive to batch sizes\cite{chen_simple_2020} this degrades downstream performance.
Supervised contrastive learning performed even worse as it did not converge during pretraining.
Again, due to the class imbalance, the supervised contrastive loss function results in a degenerate solution as approximately 97\% of the batch is projected to a single embedding.
Analogous behaviour was seen on the CAD prediction task, where 94\% of the samples are in the negative class.

On the cardiac task, LaaF performs better than false negative elimination and supervised contrastive learning, but does not offer substantial gains over the imaging baseline.
We attribute this to the fact that the cardiac setting has 120 included features, which lessens the importance of any one feature.
Additionally, imbalanced binary classification is a difficult task for supervised contrative learning as explained above.
Increasing the importance of the ground truth label in the pretraining process and adapting supervised contrastive learning to the binary case is left to future work.


On the DVM task, where we have 286 classes, the trend follows established literature.
False negative elimination improves upon the baseline and supervised contrastive learning improves upon false negative elimination\cite{chen_incremental_2022}.
Our method by itself, without modifying the loss function, surpasses false negative elimination and approaches supervised contrastive learning.
As expected, labels have a higher impact when more classes are present as shown in \cite{chen_incremental_2022}.

Importantly, adding the label as a tabular feature can also be combined with false negative elimination and supervised contrastive learning.
This highlights the flexibility of our method as it can be used with any supervised contrastive strategy.
With LaaF, we improve upon both losses and achieve our best scores on the DVM car model prediction task. 
The effect was similarly pronounced in the low data regime as shown in SM section \ref{app:laaf_low_data}.

\iffalse
\begin{table}[t]
    \caption{Downstream accuracy when incorporating labels into the contrastive pretraining process. Our label as a feature setting consistently outperforms supervised contrastive learning (SupCon) and false negative elimination (FN Elimination). Best score is in \textbf{bold} font, second best \underline{underlined}. Our methods are highlighted gray. A single dash indicates the model did not converge.}
    \centering
    \resizebox{\columnwidth}{!}
    {
    \renewcommand*{\arraystretch}{1.5}
    \begin{tabular}{c c c c}
\toprule
\thead{Model} & \thead{AUC (\%)\\Infarction} & \thead{AUC (\%)\\CAD} & \thead{Top-1 Accuracy (\%)\\DVM} \\
\midrule
\multicolumn{4}{l}{\emph{Baseline}}\\
Supervised ResNet50 & 72.37$\pm$1.80 & 68.84$\pm$2.54 & 92.08$\pm$0.69 \\
\rowcolor{Gray}
Multimodal Imaging Baseline & \underline{76.35$\pm$0.19} & \textbf{74.45$\pm$0.09} & 87.14$\pm$0.16 \\
\midrule
\multicolumn{4}{l}{\emph{Supervised Contrastive Strategy}}\\
FN Elimination & 75.38$\pm$0.06 & 72.45$\pm$0.09 & 88.53$\pm$0.12 \\
SupCon & \longdash & \longdash & 91.17$\pm$0.17 \\
\rowcolor{Gray}
Label-as-a-Feature (LaaF) & \textbf{76.60$\pm$0.42} & \underline{73.76$\pm$0.31} & 92.22$\pm$0.22 \\
\midrule
\multicolumn{4}{l}{\emph{Combination}}\\
\rowcolor{Gray}
FN Elimination + LaaF & 75.30$\pm$0.05 & 72.39$\pm$0.08 & \underline{93.69$\pm$0.13} \\
\rowcolor{Gray}
SupCon + LaaF & \longdash & \longdash & \textbf{93.73$\pm$0.14} \\
\bottomrule
    \end{tabular}
    }
    \label{tab:supcon}
\end{table}
\fi

\section{Discussion and Conclusion}

In this work we presented the first contrastive framework that combines tabular and imaging data.
Our contribution is motivated by rich clinical datasets available in biobanks that contain vast amounts of information on participants' medical history, lifestyle, and physiological measures in combination with medical images.
However, it is unfeasible to gather such detailed tabular data in a clinical setting due to time and budget constraints.
Our solution pretrains on large datasets of tabular and imaging data to boost performance during inference using only images as input.
We demonstrated the utility of our tool on the challenging task of cardiac health prediction from MR images, beating all contrastive baselines and the fully supervised baseline.
Our method also translates to the natural image domain where we showed its strength on the task of car model prediction from advertisement data. 

Through attribution and ablation experiments we showed that morphometric tabular features have outsized importance for the multimodal learning process.
We hypothesize that these features, which are related to size and shape, have direct correlates in the image and thus help minimize the multimodal self-supervised loss.
This suggests that extracting morphometric features from the images or collecting them from another source, to include them as tabular features, improves the learned representations.
Finally, we presented a simple and effective new supervised contrastive learning method when using tabular data.
Simply appending the target label as a tabular feature outperformed loss modifying strategies such as contrastive learning with false negative elimination and approached supervised contrastive learning.
This strategy can also be combined with any supervised contrastive loss modification to achieve state-of-the-art results, surpassing all other strategies.

\paragraph{Limitations}In our study we examined the benefit of our framework only for classification tasks. 
Future work should aim to test the behavior of the framework with further tasks such as segmentation and regression. 
We hypothesize that segmentation could benefit from the framework if morphometric features such as the sizes of the to-be-segmented regions are included in the tabular data and regression if morphometric features are regressed. 

A further shortcoming of this work is that we only included white subjects from the UK Biobank population dataset because other ethnicities were drastically underrepresented in the study, making up only 5\% of the total population.
Significant racial disparities in coronary infarction and CAD risk have been repeatedly found\cite{meadows_ethnic_2011,nayak_understanding_2020,ho_ethnic_2022} and could lead to spurious correlations being learned.
Future work could use balanced datasets or explore propagated biases learned with unbalanced datasets, to identify and counteract any learned biases.



\paragraph{Conclusion} In conclusion, for the first time, our work presents an effective and simple strategy to take advantage of tabular and imaging data in self-supervised contrastive learning.
Our method is particularly relevant in a clinical setting where we wish to take advantage of extensive, multimodal biobanks during pretraining and predict unimodal in practice.
We believe tabular data is an understudied and underappreciated source of data for deep learning, which is easy to collect and ubiquitous, as any numerical or categorical feature can be represented.
It is also highly interpretable due to the fact that each feature directly represents a semantic concept.
We hope that this inspires future works to unlock this untapped potential.

\subsection*{Acknowledgments}
This research has been conducted using the UK Biobank Resource under Application Number 87802.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{ms}
}
\newpage\phantom{phantom}
\newpage

\input{sm}

\end{document}
