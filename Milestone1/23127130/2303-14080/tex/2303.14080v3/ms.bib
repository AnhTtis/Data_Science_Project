@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})




@inproceedings{huynh_boosting_2022,
	location = {Waikoloa, {HI}, {USA}},
	title = {Boosting Contrastive Self-Supervised Learning with False Negative Cancellation},
	isbn = {978-1-66540-915-5},
	url = {https://ieeexplore.ieee.org/document/9706613/},
	doi = {10.1109/WACV51458.2022.00106},
	eventtitle = {2022 {IEEE}/{CVF} Winter Conference on Applications of Computer Vision ({WACV})},
	pages = {986--996},
	booktitle = {2022 {IEEE}/{CVF} Winter Conference on Applications of Computer Vision ({WACV})},
	publisher = {{IEEE}},
	author = {Huynh, Tri and Kornblith, Simon and Walter, Matthew R. and Maire, Michael and Khademi, Maryam},
	urldate = {2022-09-07},
	date = {2022-01},
	langid = {english},
	file = {Huynh et al. - 2022 - Boosting Contrastive Self-Supervised Learning with.pdf:/Users/paulhager/Zotero/storage/NQ4NWVCI/Huynh et al. - 2022 - Boosting Contrastive Self-Supervised Learning with.pdf:application/pdf},
}

@article{chen_incremental_2022,
  title={Incremental false negative detection for contrastive learning},
  author={Chen, Tsai-Shien and Hung, Wei-Chih and Tseng, Hung-Yu and Chien, Shao-Yi and Yang, Ming-Hsuan},
  journal={arXiv preprint arXiv:2106.03719},
  year={2021}
}


@article{bahri_scarf_2022,
  title={Scarf: Self-supervised contrastive learning using random feature corruption},
  author={Bahri, Dara and Jiang, Heinrich and Tay, Yi and Metzler, Donald},
  journal={arXiv preprint arXiv:2106.15147},
  year={2021}
}

@article{shwartz-ziv_tabular_2021,
  title={Tabular data: Deep learning is not all you need},
  author={Shwartz-Ziv, Ravid and Armon, Amitai},
  journal={Information Fusion},
  volume={81},
  pages={84--90},
  year={2022},
  publisher={Elsevier}
}


@article{borisov_deep_2022,
  title={Deep neural networks and tabular data: A survey},
  author={Borisov, Vadim and Leemann, Tobias and Se{\ss}ler, Kathrin and Haug, Johannes and Pawelczyk, Martin and Kasneci, Gjergji},
  journal={arXiv preprint arXiv:2110.01889},
  year={2021}
}

@article{sudlow_uk_2015,
  title={UK biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age},
  author={Sudlow, Cathie and Gallacher, John and Allen, Naomi and Beral, Valerie and Burton, Paul and Danesh, John and Downey, Paul and Elliott, Paul and Green, Jane and Landray, Martin and others},
  journal={PLoS medicine},
  volume={12},
  number={3},
  pages={e1001779},
  year={2015},
  publisher={Public Library of Science}
}

@article{noauthor_german_2014,
  title={The German National Cohort: aims, study design and organization},
  author={German National Cohort (GNC) Consortium geschaeftsstelle@ nationale-kohorte. de},
  journal={European journal of epidemiology},
  volume={29},
  number={5},
  pages={371--382},
  year={2014},
  publisher={Springer}
}

@article{shao_stunted_2019,
	title = {Stunted microbiota and opportunistic pathogen colonization in caesarean-section birth},
	volume = {574},
	rights = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1560-1},
	doi = {10.1038/s41586-019-1560-1},
	abstract = {Immediately after birth, newborn babies experience rapid colonization by microorganisms from their mothers and the surrounding environment1. Diseases in childhood and later in life are potentially mediated by the perturbation of the colonization of the infant gut microbiota2. However, the effects of delivery via caesarean section on the earliest stages of the acquisition and development of the gut microbiota, during the neonatal period (≤1 month), remain controversial3,4. Here we report the disrupted transmission of maternal Bacteroides strains, and high-level colonization by opportunistic pathogens associated with the hospital environment (including Enterococcus, Enterobacter and Klebsiella species), in babies delivered by caesarean section. These effects were also seen, to a lesser extent, in vaginally delivered babies whose mothers underwent antibiotic prophylaxis and in babies who were not breastfed during the neonatal period. We applied longitudinal sampling and whole-genome shotgun metagenomic analysis to 1,679 gut microbiota samples (taken at several time points during the neonatal period, and in infancy) from 596 full-term babies born in {UK} hospitals; for a subset of these babies, we collected additional matched samples from mothers (175 mothers paired with 178 babies). This analysis demonstrates that the mode of delivery is a significant factor that affects the composition of the gut microbiota throughout the neonatal period, and into infancy. Matched large-scale culturing and whole-genome sequencing of over 800 bacterial strains from these babies identified virulence factors and clinically relevant antimicrobial resistance in opportunistic pathogens that may predispose individuals to opportunistic infections. Our findings highlight the critical role of the local environment in establishing the gut microbiota in very early life, and identify colonization with antimicrobial-resistance-containing opportunistic pathogens as a previously underappreciated risk factor in hospital births.},
	pages = {117--121},
	number = {7776},
	journaltitle = {Nature},
	author = {Shao, Yan and Forster, Samuel C. and Tsaliki, Evdokia and Vervier, Kevin and Strang, Angela and Simpson, Nandi and Kumar, Nitin and Stares, Mark D. and Rodger, Alison and Brocklehurst, Peter and Field, Nigel and Lawley, Trevor D.},
	urldate = {2022-11-01},
	date = {2019-10},
	langid = {english},
	note = {Number: 7776
Publisher: Nature Publishing Group},
	keywords = {Clinical microbiology, Metagenomics, Microbiome, Paediatrics, Pathogens},
	file = {Full Text PDF:/home/paulhager/Zotero/storage/RWT4BQJ6/Shao et al. - 2019 - Stunted microbiota and opportunistic pathogen colo.pdf:application/pdf;Snapshot:/home/paulhager/Zotero/storage/2826YQUN/s41586-019-1560-1.html:text/html},
}


@article{petersen_alzheimers_2010,
	title = {Alzheimer's Disease Neuroimaging Initiative ({ADNI}): Clinical characterization},
	volume = {74},
	rights = {© 2010},
	issn = {0028-3878, 1526-632X},
	url = {https://n.neurology.org/content/74/3/201},
	doi = {10.1212/WNL.0b013e3181cb3e25},
	shorttitle = {Alzheimer's Disease Neuroimaging Initiative ({ADNI})},
	abstract = {Background: Neuroimaging measures and chemical biomarkers may be important indices of clinical progression in normal aging and mild cognitive impairment ({MCI}) and need to be evaluated longitudinally.
Objective: To characterize cross-sectionally and longitudinally clinical measures in normal controls, subjects with {MCI}, and subjects with mild Alzheimer disease ({AD}) to enable the assessment of the utility of neuroimaging and chemical biomarker measures.
Methods: A total of 819 subjects (229 cognitively normal, 398 with {MCI}, and 192 with {AD}) were enrolled at baseline and followed for 12 months using standard cognitive and functional measures typical of clinical trials.
Results: The subjects with {MCI} were more memory impaired than the cognitively normal subjects but not as impaired as the subjects with {AD}. Nonmemory cognitive measures were only minimally impaired in the subjects with {MCI}. The subjects with {MCI} progressed to dementia in 12 months at a rate of 16.5\% per year. Approximately 50\% of the subjects with {MCI} were on antidementia therapies. There was minimal movement on the Alzheimer's Disease Assessment Scale–Cognitive Subscale for the normal control subjects, slight movement for the subjects with {MCI} of 1.1, and a modest change for the subjects with {AD} of 4.3. Baseline {CSF} measures of Aβ-42 separated the 3 groups as expected and successfully predicted the 12-month change in cognitive measures.
Conclusion: The Alzheimer's Disease Neuroimaging Initiative has successfully recruited cohorts of cognitively normal subjects, subjects with mild cognitive impairment ({MCI}), and subjects with Alzheimer disease with anticipated baseline characteristics. The 12-month progression rate of {MCI} was as predicted, and the {CSF} measures heralded progression of clinical measures over 12 months.},
	pages = {201--209},
	number = {3},
	journaltitle = {Neurology},
	author = {Petersen, R. C. and Aisen, P. S. and Beckett, L. A. and Donohue, M. C. and Gamst, A. C. and Harvey, D. J. and Jack, C. R. and Jagust, W. J. and Shaw, L. M. and Toga, A. W. and Trojanowski, J. Q. and Weiner, M. W.},
	urldate = {2022-11-01},
	date = {2010-01-19},
	langid = {english},
	pmid = {20042704},
	note = {Publisher: Wolters Kluwer Health, Inc. on behalf of the American Academy of Neurology
Section: Articles},
	file = {Full Text:/home/paulhager/Zotero/storage/I5XF6F2H/Petersen et al. - 2010 - Alzheimer's Disease Neuroimaging Initiative (ADNI).pdf:application/pdf;Snapshot:/home/paulhager/Zotero/storage/2LMTUTV4/201.html:text/html},
}

@article{khera_genetic_2016,
	title = {Genetic Risk, Adherence to a Healthy Lifestyle, and Coronary Disease},
	volume = {375},
	issn = {0028-4793},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5338864/},
	doi = {10.1056/NEJMoa1605086},
	abstract = {{BACKGROUND}
Both genetic and lifestyle factors contribute to individual-level risk of coronary artery disease. The extent to which increased genetic risk can be offset by a healthy lifestyle is unknown.
{METHODS}
Using a polygenic score of {DNA} sequence polymorphisms, we quantified genetic risk for coronary artery disease in three prospective cohorts — 7814 participants in the Atherosclerosis Risk in Communities ({ARIC}) study, 21,222 in the Women’s Genome Health Study ({WGHS}), and 22,389 in the Malmö Diet and Cancer Study ({MDCS}) — and in 4260 participants in the cross-sectional {BioImage} Study for whom genotype and covariate data were available. We also determined adherence to a healthy lifestyle among the participants using a scoring system consisting of four factors: no current smoking, no obesity, regular physical activity, and a healthy diet.
{RESULTS}
The relative risk of incident coronary events was 91\% higher among participants at high genetic risk (top quintile of polygenic scores) than among those at low genetic risk (bottom quintile of polygenic scores) (hazard ratio, 1.91; 95\% confidence interval [{CI}], 1.75 to 2.09). A favorable lifestyle (defined as at least three of the four healthy lifestyle factors) was associated with a substantially lower risk of coronary events than an unfavorable lifestyle (defined as no or only one healthy lifestyle factor), regardless of the genetic risk category. Among participants at high genetic risk, a favorable lifestyle was associated with a 46\% lower relative risk of coronary events than an unfavorable lifestyle (hazard ratio, 0.54; 95\% {CI}, 0.47 to 0.63). This finding corresponded to a reduction in the standardized 10-year incidence of coronary events from 10.7\% for an unfavorable lifestyle to 5.1\% for a favorable lifestyle in {ARIC}, from 4.6\% to 2.0\% in {WGHS}, and from 8.2\% to 5.3\% in {MDCS}. In the {BioImage} Study, a favorable lifestyle was associated with significantly less coronary-artery calcification within each genetic risk category.
{CONCLUSIONS}
Across four studies involving 55,685 participants, genetic and lifestyle factors were independently associated with susceptibility to coronary artery disease. Among participants at high genetic risk, a favorable lifestyle was associated with a nearly 50\% lower relative risk of coronary artery disease than was an unfavorable lifestyle. (Funded by the National Institutes of Health and others.)},
	pages = {2349--2358},
	number = {24},
	journaltitle = {The New England journal of medicine},
	shortjournal = {N Engl J Med},
	author = {Khera, Amit V. and Emdin, Connor A. and Drake, Isabel and Natarajan, Pradeep and Bick, Alexander G. and Cook, Nancy R. and Chasman, Daniel I. and Baber, Usman and Mehran, Roxana and Rader, Daniel J. and Fuster, Valentin and Boerwinkle, Eric and Melander, Olle and Orho-Melander, Marju and Ridker, Paul M and Kathiresan, Sekar},
	urldate = {2022-11-01},
	date = {2016-12-15},
	pmid = {27959714},
	pmcid = {PMC5338864},
	file = {PubMed Central Full Text PDF:/home/paulhager/Zotero/storage/PC6PPU6J/Khera et al. - 2016 - Genetic Risk, Adherence to a Healthy Lifestyle, an.pdf:application/pdf},
}

@article{anand_cancer_2008,
	title = {Cancer is a Preventable Disease that Requires Major Lifestyle Changes},
	volume = {25},
	issn = {1573-904X},
	url = {https://doi.org/10.1007/s11095-008-9661-9},
	doi = {10.1007/s11095-008-9661-9},
	abstract = {This year, more than 1 million Americans and more than 10 million people worldwide are expected to be diagnosed with cancer, a disease commonly believed to be preventable. Only 5–10\% of all cancer cases can be attributed to genetic defects, whereas the remaining 90–95\% have their roots in the environment and lifestyle. The lifestyle factors include cigarette smoking, diet (fried foods, red meat), alcohol, sun exposure, environmental pollutants, infections, stress, obesity, and physical inactivity. The evidence indicates that of all cancer-related deaths, almost 25–30\% are due to tobacco, as many as 30–35\% are linked to diet, about 15–20\% are due to infections, and the remaining percentage are due to other factors like radiation, stress, physical activity, environmental pollutants etc. Therefore, cancer prevention requires smoking cessation, increased ingestion of fruits and vegetables, moderate use of alcohol, caloric restriction, exercise, avoidance of direct exposure to sunlight, minimal meat consumption, use of whole grains, use of vaccinations, and regular check-ups. In this review, we present evidence that inflammation is the link between the agents/factors that cause cancer and the agents that prevent it. In addition, we provide evidence that cancer is a preventable disease that requires major lifestyle changes.},
	pages = {2097--2116},
	number = {9},
	journaltitle = {Pharmaceutical Research},
	shortjournal = {Pharm Res},
	author = {Anand, Preetha and Kunnumakara, Ajaikumar B. and Sundaram, Chitra and Harikumar, Kuzhuvelil B. and Tharakan, Sheeja T. and Lai, Oiki S. and Sung, Bokyung and Aggarwal, Bharat B.},
	urldate = {2022-11-01},
	date = {2008-09-01},
	langid = {english},
	keywords = {cancer, environmental risk factors, genetic risk factors, prevention},
	file = {Full Text PDF:/home/paulhager/Zotero/storage/8ULRLC33/Anand et al. - 2008 - Cancer is a Preventable Disease that Requires Majo.pdf:application/pdf},
}


@incollection{lin_interactions_2017,
	location = {Singapore},
	title = {Interactions Between Genetics, Lifestyle, and Environmental Factors for Healthcare},
	isbn = {978-981-10-5717-5},
	url = {https://doi.org/10.1007/978-981-10-5717-5_8},
	series = {Advances in Experimental Medicine and Biology},
	abstract = {The occurrence and progression of diseases are strongly associated with a combination of genetic, lifestyle, and environmental factors. Understanding the interplay between genetic and nongenetic components provides deep insights into disease pathogenesis and promotes personalized strategies for people healthcare. Recently, the paradigm of systems medicine, which integrates biomedical data and knowledge at multidimensional levels, is considered to be an optimal way for disease management and clinical decision-making in the era of precision medicine. In this chapter, epigenetic-mediated genetics-lifestyle-environment interactions within specific diseases and different ethnic groups are systematically discussed, and data sources, computational models, and translational platforms for systems medicine research are sequentially presented. Moreover, feasible suggestions on precision healthcare and healthy longevity are kindly proposed based on the comprehensive review of current studies.},
	pages = {167--191},
	booktitle = {Translational Informatics in Smart Healthcare},
	publisher = {Springer},
	author = {Lin, Yuxin and Chen, Jiajia and Shen, Bairong},
	editor = {Shen, Bairong},
	urldate = {2022-11-01},
	date = {2017},
	langid = {english},
	doi = {10.1007/978-981-10-5717-5_8},
	keywords = {Epigenetics, Genetics-lifestyle-environment interaction, Healthy longevity, Precision healthcare, Systems medicine},
}


@article{huang_dvm-car_2021,
  title={DVM-CAR: A large-scale automotive dataset for visual marketing research and applications},
  author={Huang, Jingming and Chen, Bowei and Luo, Lan and Yue, Shigang and Ounis, Iadh},
  journal={arXiv preprint arXiv:2109.00881},
  year={2021}
}


@article{radford_learning_2021,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{rim_deep-learning-based_2021,
  title={Deep-learning-based cardiovascular risk stratification using coronary artery calcium scores predicted from retinal photographs},
  author={Rim, Tyler Hyungtaek and Lee, Chan Joo and Tham, Yih-Chung and Cheung, Ning and Yu, Marco and Lee, Geunyoung and Kim, Youngnam and Ting, Daniel SW and Chong, Crystal Chun Yuen and Choi, Yoon Seong and others},
  journal={The Lancet Digital Health},
  volume={3},
  number={5},
  pages={e306--e316},
  year={2021},
  publisher={Elsevier}
}

@article{mitani_detection_2020,
  title={Detection of anaemia from retinal fundus images via deep learning},
  author={Mitani, Akinori and Huang, Abigail and Venugopalan, Subhashini and Corrado, Greg S and Peng, Lily and Webster, Dale R and Hammel, Naama and Liu, Yun and Varadarajan, Avinash V},
  journal={Nature Biomedical Engineering},
  volume={4},
  number={1},
  pages={18--27},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{jonsson_brain_2019,
  title={Brain age prediction using deep learning uncovers associated sequence variants},
  author={J{\'o}nsson, Benedikt Atli and Bjornsdottir, Gyda and Thorgeirsson, TE and Ellingsen, Lotta Mar{\'\i}a and Walters, G Bragi and Gudbjartsson, DF and Stefansson, Hreinn and Stefansson, Kari and Ulfarsson, MO},
  journal={Nature communications},
  volume={10},
  number={1},
  pages={1--10},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{alaa_cardiovascular_2019,
  title={Cardiovascular disease risk prediction using automated machine learning: A prospective study of 423,604 UK Biobank participants},
  author={Alaa, Ahmed M and Bolton, Thomas and Di Angelantonio, Emanuele and Rudd, James HF and Van der Schaar, Mihaela},
  journal={PloS one},
  volume={14},
  number={5},
  pages={e0213653},
  year={2019},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{sijtsma_cohort_2022,
  title={Cohort Profile: LifeLines, a three-generation cohort study and biobank},
  author={Scholtens, Salome and Smidt, Nynke and Swertz, Morris A and Bakker, Stephan JL and Dotinga, Aafje and Vonk, Judith M and Van Dijk, Freerk and van Zon, Sander KR and Wijmenga, Cisca and Wolffenbuttel, Bruce HR and others},
  journal={International journal of epidemiology},
  volume={44},
  number={4},
  pages={1172--1180},
  year={2015},
  publisher={Oxford University Press}
}

@article{oord_representation_2019,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@article{zabalgoitia_impact_2001,
  title={Impact of coronary artery disease on left ventricular systolic function and geometry in hypertensive patients with left ventricular hypertrophy (the LIFE study)},
  author={Zabalgoitia, Miguel and Berning, Jens and Koren, Michael J and St{\o}ylen, Asbj{\o}rn and Nieminen, Markku S and Dahl{\"o}f, Bj{\"o}rn and Devereux, Richard B and LIFE Study Investigators and others},
  journal={The American journal of cardiology},
  volume={88},
  number={6},
  pages={646--650},
  year={2001},
  publisher={Elsevier}
}

@article{sutton_left_2000,
  title={Left ventricular remodeling after myocardial infarction: pathophysiology and therapy},
  author={Sutton, Martin G St John and Sharpe, Norman},
  journal={Circulation},
  volume={101},
  number={25},
  pages={2981--2988},
  year={2000},
  publisher={Am Heart Assoc}
}

@article{yu_cardiovascular_2018,
  title={Cardiovascular disease prevention by diet modification: JACC health promotion series},
  author={Yu, Edward and Malik, Vasanti S and Hu, Frank B},
  journal={Journal of the American College of Cardiology},
  volume={72},
  number={8},
  pages={914--926},
  year={2018},
  publisher={American College of Cardiology Foundation Washington DC}
}

@inproceedings{he_deep_2016,
	location = {Las Vegas, {NV}, {USA}},
	title = {Deep Residual Learning for Image Recognition},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than {VGG} nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classiﬁcation task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers.},
	eventtitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {770--778},
	booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2022-11-05},
	date = {2016-06},
	langid = {english},
	file = {He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:/home/paulhager/Zotero/storage/KU8LM78G/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}


@article{celano_anxiety_2016,
  title={Anxiety disorders and cardiovascular disease},
  author={Celano, Christopher M and Daunis, Daniel J and Lokko, Hermioni N and Campbell, Kirsti A and Huffman, Jeff C},
  journal={Current psychiatry reports},
  volume={18},
  number={11},
  pages={1--11},
  year={2016},
  publisher={Springer}
}


@article{lakier_smoking_1992,
  title={Smoking and cardiovascular disease},
  author={Lakier, Jeffrey B},
  journal={The American journal of medicine},
  volume={93},
  number={1},
  pages={S8--S12},
  year={1992},
  publisher={Elsevier}
}


@article{piano_alcohols_2017,
  title={Alcohol’s effects on the cardiovascular system},
  author={Piano, Mariann R},
  journal={Alcohol research: current reviews},
  volume={38},
  number={2},
  pages={219},
  year={2017},
  publisher={National Institute on Alcohol Abuse and Alcoholism}
}


@article{powell-wiley_obesity_2021,
  title={Obesity and cardiovascular disease: a scientific statement from the American Heart Association},
  author={Powell-Wiley, Tiffany M and Poirier, Paul and Burke, Lora E and Despr{\'e}s, Jean-Pierre and Gordon-Larsen, Penny and Lavie, Carl J and Lear, Scott A and Ndumele, Chiadi E and Neeland, Ian J and Sanders, Prashanthan and others},
  journal={Circulation},
  volume={143},
  number={21},
  pages={e984--e1010},
  year={2021},
  publisher={Am Heart Assoc}
}


@article{mora_physical_2007,
	title = {Physical Activity and Reduced Risk of Cardiovascular Events},
	volume = {116},  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}
	url = {https://www.ahajournals.org/doi/10.1161/circulationaha.107.729939},
	doi = {10.1161/CIRCULATIONAHA.107.729939},
	abstract = {Background— Higher levels of physical activity are associated with fewer cardiovascular disease ({CVD}) events. Although the precise mechanisms underlying this inverse association are unclear, differences in several cardiovascular risk factors may mediate this effect.

Methods and Results— In a prospective study of 27 055 apparently healthy women, we measured baseline levels of hemoglobin A1c, traditional lipids (total, low-density lipoprotein, and high-density lipoprotein cholesterol), novel lipids [lipoprotein(a) and apolipoprotein A1 and B-100], creatinine, homocysteine, and inflammatory/hemostatic biomarkers (high-sensitivity C-reactive protein, fibrinogen, soluble intracellular adhesion molecule-1) and used women’s self-reported physical activity, weight, height, hypertension, and diabetes. Mean follow-up was 10.9±1.6 years, and 979 incident {CVD} events occurred. The risk of {CVD} decreased linearly with higher levels of activity (P for linear trend {\textless}0.001). Using the reference group of {\textless}200 kcal/wk of activity yielded age- and treatment-adjusted relative risk reductions associated with 200 to 599, 600 to 1499, and ≥1500 kcal/wk of 27\%, 32\%, and 41\%, respectively. Differences in known risk factors explained a large proportion (59.0\%) of the observed inverse association. When sets of risk factors were examined, inflammatory/hemostatic biomarkers made the largest contribution to lower risk (32.6\%), followed by blood pressure (27.1\%). Novel lipids contributed less to {CVD} risk reduction compared with traditional lipids (15.5\% and 19.1\%, respectively). Smaller contributions were attributed to body mass index (10.1\%) and hemoglobin A1c/diabetes (8.9\%), whereas homocysteine and creatinine had negligible effects ({\textless}1\%).

Conclusions— The inverse association between physical activity and {CVD} risk is mediated in substantial part by known risk factors, particularly inflammatory/hemostatic factors and blood pressure.},
	pages = {2110--2118},
	number = {19},
	journaltitle = {Circulation},
	author = {Mora, Samia and Cook, Nancy and Buring, Julie E. and Ridker, Paul M and Lee, I-Min},
	urldate = {2022-11-03},
	date = {2007-11-06},
	note = {Publisher: American Heart Association},
	keywords = {cardiovascular diseases, epidemiology, exercise, risk factors, women},
	file = {Full Text PDF:/home/paulhager/Zotero/storage/I6DSV7RB/Mora et al. - 2007 - Physical Activity and Reduced Risk of Cardiovascul.pdf:application/pdf}
}

@article{acarturk_aortic_1999,
	title = {Aortic atherosclerosis is a marker for significant coronary artery disease},
	author = {Acartürk, E. and Demir, M. and Kanadaşi, M.},
        journal={Japanese heart journal},
        volume={40},
        number={6},
        pages={775--781},
        year={1999},
        publisher={International Heart Journal Association}

}

@article{joshi_systemic_2015,
  title={Systemic atherosclerotic inflammation following acute myocardial infarction: myocardial infarction begets myocardial infarction},
  author={Joshi, Nikhil V and Toor, Iqbal and Shah, Anoop SV and Carruthers, Kathryn and Vesey, Alex T and Alam, Shirjel R and Sills, Andrew and Hoo, Teng Y and Melville, Adam J and Langlands, Sarah P and others},
  journal={Journal of the American Heart Association},
  volume={4},
  number={9},
  pages={e001956},
  year={2015},
  publisher={Am Heart Assoc}
}


@article{suinesiaputra_statistical_2018,
	title = {Statistical shape modeling of the left ventricle: myocardial infarct classification challenge},
	volume = {22},
	issn = {2168-2208},
	doi = {10.1109/JBHI.2017.2652449},
	shorttitle = {Statistical shape modeling of the left ventricle},
	abstract = {Statistical shape modeling is a powerful tool for visualizing and quantifying geometric and functional patterns of the heart. After myocardial infarction ({MI}), the left ventricle typically remodels in response to physiological challenges. Several methods have been proposed in the literature to describe statistical shape changes. Which method best characterizes left ventricular remodeling after {MI} is an open research question. A better descriptor of remodeling is expected to provide a more accurate evaluation of disease status in {MI} patients. We therefore designed a challenge to test shape characterization in {MI} given a set of three-dimensional left ventricular surface points. The training set comprised 100 {MI} patients, and 100 asymptomatic volunteers ({AV}). The challenge was initiated in 2015 at the Statistical Atlases and Computational Models of the Heart workshop, in conjunction with the {MICCAI} conference. The training set with labels was provided to participants, who were asked to submit the likelihood of {MI} from a different (validation) set of 200 cases (100 {AV} and 100 {MI}). Sensitivity, specificity, accuracy and area under the receiver operating characteristic curve were used as the outcome measures. The goals of this challenge were to (1) establish a common dataset for evaluating statistical shape modeling algorithms in {MI}, and (2) test whether statistical shape modeling provides additional information characterizing {MI} patients over standard clinical measures. Eleven groups with a wide variety of classification and feature extraction approaches participated in this challenge. All methods achieved excellent classification results with accuracy ranges from 0.83 to 0.98. The areas under the receiver operating characteristic curves were all above 0.90. Four methods showed significantly higher performance than standard clinical measures. The dataset and software for evaluation are available from the Cardiac Atlas Project website1.},
	pages = {503--515},
	number = {2},
	journaltitle = {{IEEE} journal of biomedical and health informatics},
	shortjournal = {{IEEE} J Biomed Health Inform},
	author = {Suinesiaputra, Avan and Ablin, Pierre and Alba, Xenia and Alessandrini, Martino and Allen, Jack and Bai, Wenjia and Cimen, Serkan and Claes, Peter and Cowan, Brett R. and D'hooge, Jan and Duchateau, Nicolas and Ehrhardt, Jan and Frangi, Alejandro F. and Gooya, Ali and Grau, Vicente and Lekadir, Karim and Lu, Allen and Mukhopadhyay, Anirban and Oksuz, Ilkay and Parajali, Nripesh and Pennec, Xavier and Pereanez, Marco and Pinto, Catarina and Piras, Paolo and Rohe, Marc-Michel and Rueckert, Daniel and Saring, Dennis and Sermesant, Maxime and Siddiqi, Kaleem and Tabassian, Mahdi and Teresi, Luciano and Tsaftaris, Sotirios A. and Wilms, Matthias and Young, Alistair A. and Zhang, Xingyu and Medrano-Gracia, Pau},
	date = {2018-03},
	pmid = {28103561},
	pmcid = {PMC5857476},
	file = {Full Text:/home/paulhager/Zotero/storage/P9CDH62D/Suinesiaputra et al. - 2018 - Statistical shape modeling of the left ventricle .pdf:application/pdf},
}

@article{ho_ethnic_2022,
  title={Ethnic differences in cardiovascular risk: examining differential exposure and susceptibility to risk factors},
  author={Ho, Frederick K and Gray, Stuart R and Welsh, Paul and Gill, Jason MR and Sattar, Naveed and Pell, Jill P and Celis-Morales, Carlos},
  journal={BMC medicine},
  volume={20},
  number={1},
  pages={1--10},
  year={2022},
  publisher={BioMed Central}
}


@article{nayak_understanding_2020,
  title={Understanding the complexity of heart failure risk and treatment in black patients},
  author={Nayak, Aditi and Hicks, Albert J and Morris, Alanna A},
  journal={Circulation: Heart Failure},
  volume={13},
  number={8},
  pages={e007264},
  year={2020},
  publisher={Am Heart Assoc}
}


@inproceedings{meadows_ethnic_2011,
  title={Ethnic differences in cardiovascular risks and mortality in atherothrombotic disease: insights from the Reduction of Atherothrombosis for Continued Health (REACH) registry},
  author={Meadows, Telly A and Bhatt, Deepak L and Cannon, Christopher P and Gersh, Bernard J and R{\"o}ther, Joachim and Goto, Shinya and Liau, Chiau-Suong and Wilson, Peter WF and Salette, Genevieve and Smith, Sidney C and others},
  booktitle={Mayo Clinic Proceedings},
  volume={86},
  number={10},
  pages={960--967},
  year={2011},
  organization={Elsevier}
}


@inproceedings{khosla_supervised_2020,
	title = {Supervised Contrastive Learning},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf},
	pages = {18661--18673},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	date = {2020},
}
@inproceedings{chen_simple_2020,
	title = {A Simple Framework for Contrastive Learning of Visual Representations},
	url = {https://proceedings.mlr.press/v119/chen20j.html},
	abstract = {This paper presents {SimCLR}: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on {ImageNet}. A linear classifier trained on self-supervised representations learned by {SimCLR} achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised {ResNet}-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming {AlexNet} with 100X fewer labels.},
	eventtitle = {International Conference on Machine Learning},
	pages = {1597--1607},
	booktitle = {Proceedings of the 37th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	urldate = {2022-09-12},
	date = {2020-11-21},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:/Users/paulhager/Zotero/storage/GUYI8EUV/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf;Supplementary PDF:/Users/paulhager/Zotero/storage/LLBG9HCX/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf},
}

@inproceedings{sund_intgrad,
author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
title = {Axiomatic Attribution for Deep Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms— Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3319–3328},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@ARTICLE{samek_explain,  
author={Samek, Wojciech and Montavon, Grégoire and Lapuschkin, Sebastian and Anders, Christopher J. and Müller, Klaus-Robert},  journal={Proceedings of the IEEE},   title={Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications},   year={2021},  volume={109},  number={3},  pages={247-278},  doi={10.1109/JPROC.2021.3060483}}

@inproceedings{pang_solving_2020,
	location = {Seattle, {WA}, {USA}},
	title = {Solving Mixed-Modal Jigsaw Puzzle for Fine-Grained Sketch-Based Image Retrieval},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9157668/},
	doi = {10.1109/CVPR42600.2020.01036},
	abstract = {{ImageNet} pre-training has long been considered crucial by the ﬁne-grained sketch-based image retrieval ({FG}-{SBIR}) community due to the lack of large sketch-photo paired datasets for {FG}-{SBIR} training. In this paper, we propose a self-supervised alternative for representation pre-training. Speciﬁcally, we consider the jigsaw puzzle game of recomposing images from shufﬂed parts. We identify two key facets of jigsaw task design that are required for effective {FG}-{SBIR} pre-training. The ﬁrst is formulating the puzzle in a mixed-modality fashion. Second we show that framing the optimisation as permutation matrix inference via Sinkhorn iterations is more effective than the common classiﬁer formulation of Jigsaw self-supervision. Experiments show that this self-supervised pre-training strategy signiﬁcantly outperforms the standard {ImageNet}-based pipeline across all four product-level {FG}-{SBIR} benchmarks. Interestingly it also leads to improved cross-category generalisation across both pre-train/ﬁne-tune and ﬁne-tune/testing stages.},
	eventtitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {10344--10352},
	booktitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Pang, Kaiyue and Yang, Yongxin and Hospedales, Timothy M. and Xiang, Tao and Song, Yi-Zhe},
	urldate = {2022-09-25},
	date = {2020-06},
	langid = {english},
	file = {Pang et al. - 2020 - Solving Mixed-Modal Jigsaw Puzzle for Fine-Grained.pdf:/Users/paulhager/Zotero/storage/WUTCULNC/Pang et al. - 2020 - Solving Mixed-Modal Jigsaw Puzzle for Fine-Grained.pdf:application/pdf},
}


@inproceedings{taleb_multimodal_2021,
	location = {Cham},
	title = {Multimodal Self-supervised Learning for Medical Image Analysis},
	isbn = {978-3-030-78191-0},
	doi = {10.1007/978-3-030-78191-0_51},
	series = {Lecture Notes in Computer Science},
	abstract = {Self-supervised learning approaches leverage unlabeled samples to acquire generic knowledge about different concepts, hence allowing for annotation-efficient downstream task learning. In this paper, we propose a novel self-supervised method that leverages multiple imaging modalities. We introduce the multimodal puzzle task, which facilitates representation learning from multiple image modalities. The learned modality-agnostic representations are obtained by confusing image modalities at the data-level. Together with the Sinkhorn operator, with which we formulate the puzzle solving optimization as permutation matrix inference instead of classification, they allow for efficient solving of multimodal puzzles with varying levels of complexity. In addition, we also propose to utilize generation techniques for multimodal data augmentation used for self-supervised pretraining, instead of downstream tasks directly. This aims to circumvent quality issues associated with synthetic images, while improving data-efficiency and the representations learned by self-supervised methods. Our experimental results show that solving our multimodal puzzles yields better semantic representations, compared to treating each modality independently. Our results also highlight the benefits of exploiting synthetic images for self-supervised pretraining. We showcase our approach on three segmentation tasks, and we outperform many solutions and our results are competitive to state-of-the-art.},
	pages = {661--673},
	booktitle = {Information Processing in Medical Imaging},
	publisher = {Springer International Publishing},
	author = {Taleb, Aiham and Lippert, Christoph and Klein, Tassilo and Nabi, Moin},
	editor = {Feragen, Aasa and Sommer, Stefan and Schnabel, Julia and Nielsen, Mads},
	date = {2021},
	langid = {english},
	keywords = {Multimodal images analysis, Self supervised learning},
	file = {Submitted Version:/Users/paulhager/Zotero/storage/9N2ZCLHR/Taleb et al. - 2021 - Multimodal Self-supervised Learning for Medical Im.pdf:application/pdf},
}


@inproceedings{zhuang_self-supervised_2019,
	location = {Cham},
	title = {Self-supervised Feature Learning for 3D Medical Images by Playing a Rubik’s Cube},
	isbn = {978-3-030-32251-9},
	doi = {10.1007/978-3-030-32251-9_46},
	series = {Lecture Notes in Computer Science},
	abstract = {Witnessed the development of deep learning, increasing number of studies try to build computer aided diagnosis systems for 3D volumetric medical data. However, as the annotations of 3D medical data are difficult to acquire, the number of annotated 3D medical images is often not enough to well train the deep learning networks. The self-supervised learning deeply exploiting the information of raw data is one of the potential solutions to loose the requirement of training data. In this paper, we propose a self-supervised learning framework for the volumetric medical images. A novel proxy task, i.e., Rubik’s cube recovery, is formulated to pre-train 3D neural networks. The proxy task involves two operations, i.e., cube rearrangement and cube rotation, which enforce networks to learn translational and rotational invariant features from raw 3D data. Compared to the train-from-scratch strategy, fine-tuning from the pre-trained network leads to a better accuracy on various tasks, e.g., brain hemorrhage classification and brain tumor segmentation. We show that our self-supervised learning approach can substantially boost the accuracies of 3D deep learning networks on the volumetric medical datasets without using extra data. To our best knowledge, this is the first work focusing on the self-supervised learning of 3D neural networks.},
	pages = {420--428},
	booktitle = {Medical Image Computing and Computer Assisted Intervention – {MICCAI} 2019},
	publisher = {Springer International Publishing},
	author = {Zhuang, Xinrui and Li, Yuexiang and Hu, Yifan and Ma, Kai and Yang, Yujiu and Zheng, Yefeng},
	editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
	date = {2019},
	langid = {english},
	keywords = {3D medical images, Rubik’s cube recovery, Self-supervised learning},
}


@article{chen_self-supervised_2019,
  title={Self-supervised learning for medical image analysis using image context restoration},
  author={Chen, Liang and Bentley, Paul and Mori, Kensaku and Misawa, Kazunari and Fujiwara, Michitaka and Rueckert, Daniel},
  journal={Medical image analysis},
  volume={58},
  pages={101539},
  year={2019},
  publisher={Elsevier}
}


@inproceedings{doersch_unsupervised_2015,
	location = {Santiago, Chile},
	title = {Unsupervised Visual Representation Learning by Context Prediction},
	isbn = {978-1-4673-8391-2},
	url = {http://ieeexplore.ieee.org/document/7410524/},
	doi = {10.1109/ICCV.2015.167},
	abstract = {This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the ﬁrst. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal {VOC} 2011 detection dataset. Furthermore, we show that the learned {ConvNet} can be used in the {RCNN} framework [19] and provides a signiﬁcant boost over a randomly-initialized {ConvNet}, resulting in state-of-theart performance among algorithms which use only Pascalprovided training set annotations.},
	eventtitle = {2015 {IEEE} International Conference on Computer Vision ({ICCV})},
	pages = {1422--1430},
	booktitle = {2015 {IEEE} International Conference on Computer Vision ({ICCV})},
	publisher = {{IEEE}},
	author = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A.},
	urldate = {2022-09-25},
	date = {2015-12},
	langid = {english},
	file = {Doersch et al. - 2015 - Unsupervised Visual Representation Learning by Con.pdf:/Users/paulhager/Zotero/storage/C2P53XYW/Doersch et al. - 2015 - Unsupervised Visual Representation Learning by Con.pdf:application/pdf},
}


@article{jo_guideline_2019,
	title = {Guideline for Cardiovascular Magnetic Resonance Imaging from the Korean Society of Cardiovascular Imaging—Part 1: Standardized Protocol},
	volume = {20},
	issn = {1229-6929},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6715561/},
	doi = {10.3348/kjr.2019.0398},
	shorttitle = {Guideline for Cardiovascular Magnetic Resonance Imaging from the Korean Society of Cardiovascular Imaging—Part 1},
	abstract = {Cardiac magnetic resonance ({CMR}) imaging is widely used in many areas of cardiovascular disease assessment. This is a practical, standard {CMR} protocol for beginners that is designed to be easy to follow and implement. This protocol guideline is based on previously reported {CMR} guidelines and includes sequence terminology used by vendors, essential {MR} physics, imaging planes, field strength considerations, {MRI}-conditional devices, drugs for stress tests, various {CMR} modules, and disease/symptom-based protocols based on a survey of cardiologists and various appropriate-use criteria. It will be of considerable help in planning and implementing tests. In addressing {CMR} usage and creating this protocol guideline, we particularly tried to include useful tips to overcome various practical issues and improve {CMR} imaging. We hope that this document will continue to standardize and simplify a patient-based approach to clinical {CMR} and contribute to the promotion of public health.},
	pages = {1313--1333},
	number = {9},
	journaltitle = {Korean Journal of Radiology},
	shortjournal = {Korean J Radiol},
	author = {Jo, Yeseul and Kim, {JeongJae} and Park, Chul Hwan and Lee, Jae Wook and Hur, Jee Hye and Yang, Dong Hyun and Lee, Bae Young and Im, Dong Jin and Hong, Su Jin and Kim, Eun Young and Park, Eun-Ah and Kim, Pan Ki and Yong, Hwan Seok},
	urldate = {2022-11-03},
	date = {2019-09},
	pmid = {31464111},
	pmcid = {PMC6715561},
	file = {PubMed Central Full Text PDF:/home/paulhager/Zotero/storage/8LHAGN4K/Jo et al. - 2019 - Guideline for Cardiovascular Magnetic Resonance Im.pdf:application/pdf},
}


@inproceedings{bai_self-supervised_2019,
	location = {Cham},
	title = {Self-Supervised Learning for Cardiac {MR} Image Segmentation by Anatomical Position Prediction},
	isbn = {978-3-030-32245-8},
	doi = {10.1007/978-3-030-32245-8_60},
	series = {Lecture Notes in Computer Science},
	abstract = {In the recent years, convolutional neural networks have transformed the field of medical image analysis due to their capacity to learn discriminative image features for a variety of classification and regression tasks. However, successfully learning these features requires a large amount of manually annotated data, which is expensive to acquire and limited by the available resources of expert image analysts. Therefore, unsupervised, weakly-supervised and self-supervised feature learning techniques receive a lot of attention, which aim to utilise the vast amount of available data, while at the same time avoid or substantially reduce the effort of manual annotation. In this paper, we propose a novel way for training a cardiac {MR} image segmentation network, in which features are learnt in a self-supervised manner by predicting anatomical positions. The anatomical positions serve as a supervisory signal and do not require extra manual annotation. We demonstrate that this seemingly simple task provides a strong signal for feature learning and with self-supervised learning, we achieve a high segmentation accuracy that is better than or comparable to a U-net trained from scratch, especially at a small data setting. When only five annotated subjects are available, the proposed method improves the mean Dice metric from 0.811 to 0.852 for short-axis image segmentation, compared to the baseline U-net.},
	pages = {541--549},
	booktitle = {Medical Image Computing and Computer Assisted Intervention – {MICCAI} 2019},
	publisher = {Springer International Publishing},
	author = {Bai, Wenjia and Chen, Chen and Tarroni, Giacomo and Duan, Jinming and Guitton, Florian and Petersen, Steffen E. and Guo, Yike and Matthews, Paul M. and Rueckert, Daniel},
	editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
	date = {2019},
	langid = {english},
	file = {Accepted Version:/Users/paulhager/Zotero/storage/NIBAUNJX/Bai et al. - 2019 - Self-Supervised Learning for Cardiac MR Image Segm.pdf:application/pdf},
}

@inproceedings{pathak_context_2016,
	location = {Las Vegas, {NV}, {USA}},
	title = {Context Encoders: Feature Learning by Inpainting},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780647/},
	doi = {10.1109/CVPR.2016.278},
	shorttitle = {Context Encoders},
	abstract = {We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders – a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for {CNN} pre-training on classiﬁcation, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.},
	eventtitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {2536--2544},
	booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
	urldate = {2022-09-25},
	date = {2016-06},
	langid = {english},
	file = {Pathak et al. - 2016 - Context Encoders Feature Learning by Inpainting.pdf:/Users/paulhager/Zotero/storage/KWA9B87W/Pathak et al. - 2016 - Context Encoders Feature Learning by Inpainting.pdf:application/pdf},
}


@inproceedings{vondrick_tracking_2018,
  title={Tracking emerges by colorizing videos},
  author={Vondrick, Carl and Shrivastava, Abhinav and Fathi, Alireza and Guadarrama, Sergio and Murphy, Kevin},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={391--408},
  year={2018}
}

@inproceedings{zhang_colorful_2016,
  title={Colorful image colorization},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A},
  booktitle={European conference on computer vision},
  pages={649--666},
  year={2016},
  organization={Springer}
}

@inproceedings{larsson_learning_2017,
  title={Learning representations for automatic colorization},
  author={Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
  booktitle={European conference on computer vision},
  pages={577--593},
  year={2016},
  organization={Springer}
}

@inproceedings{li_targeted_2022,
	location = {New Orleans, {LA}, {USA}},
	title = {Targeted Supervised Contrastive Learning for Long-Tailed Recognition},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9878544/},
	doi = {10.1109/CVPR52688.2022.00679},
	abstract = {Real-world data often exhibits long tail distributions with heavy class imbalance, where the majority classes can dominate the training process and alter the decision boundaries of the minority classes. Recently, researchers have investigated the potential of supervised contrastive learning for long-tailed recognition, and demonstrated that it provides a strong performance gain. In this paper, we show that while supervised contrastive learning can help improve performance, past baselines suffer from poor uniformity brought in by imbalanced data distribution. This poor uniformity manifests in samples from the minority class having poor separability in the feature space. To address this problem, we propose targeted supervised contrastive learning ({TSC}), which improves the uniformity of the feature distribution on the hypersphere. {TSC} ﬁrst generates a set of targets uniformly distributed on a hypersphere. It then makes the features of different classes converge to these distinct and uniformly distributed targets during training. This forces all classes, including minority classes, to maintain a uniform distribution in the feature space, improves class boundaries, and provides better generalization even in the presence of long-tail data. Experiments on multiple datasets show that {TSC} achieves state-of-the-art performance on long-tailed recognition tasks.},
	eventtitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {6908--6918},
	booktitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Li, Tianhong and Cao, Peng and Yuan, Yuan and Fan, Lijie and Yang, Yuzhe and Feris, Rogerio and Indyk, Piotr and Katabi, Dina},
	urldate = {2022-11-06},
	date = {2022-06},
	langid = {english},
	file = {Li et al. - 2022 - Targeted Supervised Contrastive Learning for Long-.pdf:/home/paulhager/Zotero/storage/F5M66GNV/Li et al. - 2022 - Targeted Supervised Contrastive Learning for Long-.pdf:application/pdf},
}

@inproceedings{selvaraju_grad-cam_2020,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={618--626},
  year={2017}
}


@article{angeli_left_2018,
  title={Left ventricular hypertrophy and coronary artery calcifications: a dangerous duet?},
  author={Angeli, Fabio and Verdecchia, Paolo and Trapasso, Monica and Reboldi, Gianpaolo},
  journal={American Journal of Hypertension},
  volume={31},
  number={3},
  pages={287--289},
  year={2018},
  publisher={Oxford University Press US}
}


@inproceedings{taleb_3d_2020,
	title = {3D Self-Supervised Methods for Medical Imaging},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/d2dc6368837861b42020ee72b0896182-Abstract.html},
	abstract = {Self-supervised learning methods have witnessed a recent surge of interest after proving successful in multiple application fields. 
In this work, we leverage these techniques, and we propose 3D versions for five different self-supervised methods, in the form of proxy tasks. Our methods facilitate neural network feature learning from unlabeled 3D images, aiming to reduce the required cost for expert annotation. 
The developed algorithms are 3D Contrastive Predictive Coding, 3D Rotation prediction, 3D Jigsaw puzzles, Relative 3D patch location, and 3D Exemplar networks. 
Our experiments show that pretraining models with our 3D tasks yields more powerful semantic representations, and enables solving downstream tasks more accurately and efficiently, compared to training the models from scratch and to pretraining them on 2D slices. 
We demonstrate the effectiveness of our methods on three downstream tasks from the medical imaging domain: i) Brain Tumor Segmentation from 3D {MRI}, ii) Pancreas Tumor Segmentation from 3D {CT}, and iii) Diabetic Retinopathy Detection from 2D Fundus images. In each task, we assess the gains in data-efficiency, performance, and speed of convergence. Interestingly, we also find gains when transferring the learned representations, by our methods, from a large unlabeled 3D corpus to a small downstream-specific dataset. 
We achieve results competitive to state-of-the-art solutions at a fraction of the computational expense. 
We publish our implementations for the developed algorithms (both 3D and 2D versions) as an open-source library, in an effort to allow other researchers to apply and extend our methods on their datasets.},
	pages = {18158--18172},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Taleb, Aiham and Loetzsch, Winfried and Danz, Noel and Severin, Julius and Gaertner, Thomas and Bergner, Benjamin and Lippert, Christoph},
	urldate = {2022-09-25},
	date = {2020},
	file = {Full Text PDF:/Users/paulhager/Zotero/storage/48M3K658/Taleb et al. - 2020 - 3D Self-Supervised Methods for Medical Imaging.pdf:application/pdf},
}

@article{dugdale_time_1999,
  title={Time and the patient--physician relationship},
  author={Dugdale, David C and Epstein, Ronald and Pantilat, Steven Z},
  journal={Journal of general internal medicine},
  volume={14},
  number={Suppl 1},
  pages={S34},
  year={1999},
  publisher={Springer}
}


@inproceedings{hadsell_dimensionality_2006,
	location = {New York, {NY}, {USA}},
	title = {Dimensionality Reduction by Learning an Invariant Mapping},
	volume = {2},
	isbn = {978-0-7695-2597-6},
	url = {http://ieeexplore.ieee.org/document/1640964/},
	doi = {10.1109/CVPR.2006.100},
	abstract = {Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that “similar” points in input space are mapped to nearby points on the manifold. Most existing techniques for solving the problem suffer from two drawbacks. First, most of them depend on a meaningful and computable distance metric in input space. Second, they do not compute a “function” that can accurately map new input samples whose relationship to the training data is unknown. We present a method - called Dimensionality Reduction by Learning an Invariant Mapping ({DrLIM}) - for learning a globally coherent non-linear function that maps the data evenly to the output manifold. The learning relies solely on neighborhood relationships and does not require any distance measure in the input space. The method can learn mappings that are invariant to certain transformations of the inputs, as is demonstrated with a number of experiments. Comparisons are made to other techniques, in particular {LLE}.},
	eventtitle = {2006 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2 ({CVPR}'06)},
	pages = {1735--1742},
	booktitle = {2006 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2 ({CVPR}'06)},
	publisher = {{IEEE}},
	author = {Hadsell, R. and Chopra, S. and {LeCun}, Y.},
	urldate = {2022-09-25},
	date = {2006},
	langid = {english},
	file = {Hadsell et al. - 2006 - Dimensionality Reduction by Learning an Invariant .pdf:/Users/paulhager/Zotero/storage/GULLUH2U/Hadsell et al. - 2006 - Dimensionality Reduction by Learning an Invariant .pdf:application/pdf},
}


@article{caron_unsupervised_2021,
  title={Unsupervised learning of visual features by contrasting cluster assignments},
  author={Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9912--9924},
  year={2020}
}


@inproceedings{chen_exploring_2021,
	location = {Nashville, {TN}, {USA}},
	title = {Exploring Simple Siamese Representation Learning},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9578004/},
	doi = {10.1109/CVPR46437.2021.01549},
	eventtitle = {2021 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {15745--15753},
	booktitle = {2021 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Chen, Xinlei and He, Kaiming},
	urldate = {2022-09-25},
	date = {2021-06},
	langid = {english},
	file = {Chen and He - 2021 - Exploring Simple Siamese Representation Learning.pdf:/Users/paulhager/Zotero/storage/K7ELCZNK/Chen and He - 2021 - Exploring Simple Siamese Representation Learning.pdf:application/pdf},
}


@article{grill_bootstrap_2020,
	title = {Bootstrap your own latent: A new approach to self-supervised Learning},
	url = {http://arxiv.org/abs/2006.07733},
	shorttitle = {Bootstrap your own latent},
	abstract = {We introduce Bootstrap Your Own Latent ({BYOL}), a new approach to self-supervised image representation learning. {BYOL} relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, {BYOL} achieves a new state of the art without them. {BYOL} reaches \$74.3{\textbackslash}\%\$ top-1 classification accuracy on {ImageNet} using a linear evaluation with a {ResNet}-50 architecture and \$79.6{\textbackslash}\%\$ with a larger {ResNet}. We show that {BYOL} performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on {GitHub}.},
	number = {{arXiv}:2006.07733},
	publisher = {{arXiv}},
	author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal},
	urldate = {2022-09-25},
	date = {2020-09-10},
	eprinttype = {arxiv},
	eprint = {2006.07733 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{azizi_big_2021,
	location = {Montreal, {QC}, Canada},
	title = {Big Self-Supervised Models Advance Medical Image Classification},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9710396/},
	doi = {10.1109/ICCV48922.2021.00346},
	abstract = {Self-supervised pretraining followed by supervised finetuning has seen success in image recognition, especially when labeled examples are scarce, but has received limited attention in medical image analysis. This paper studies the effectiveness of self-supervised learning as a pretraining strategy for medical image classification. We conduct experiments on two distinct tasks: dermatology condition classification from digital camera images and multilabel chest X-ray classification, and demonstrate that selfsupervised learning on {ImageNet}, followed by additional self-supervised learning on unlabeled domain-specific medical images significantly improves the accuracy of medical image classifiers. We introduce a novel Multi-Instance Contrastive Learning ({MICLe}) method that uses multiple images of the underlying pathology per patient case, when available, to construct more informative positive pairs for self-supervised learning. Combining our contributions, we achieve an improvement of 6.7\% in top-1 accuracy and an improvement of 1.1\% in mean {AUC} on dermatology and chest X-ray classification respectively, outperforming strong supervised baselines pretrained on {ImageNet}. In addition, we show that big self-supervised models are robust to distribution shift and can learn efficiently with a small number of labeled medical images.},
	eventtitle = {2021 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	pages = {3458--3468},
	booktitle = {2021 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	publisher = {{IEEE}},
	author = {Azizi, Shekoofeh and Mustafa, Basil and Ryan, Fiona and Beaver, Zachary and Freyberg, Jan and Deaton, Jonathan and Loh, Aaron and Karthikesalingam, Alan and Kornblith, Simon and Chen, Ting and Natarajan, Vivek and Norouzi, Mohammad},
	urldate = {2022-11-07},
	date = {2021-10},
	langid = {english},
	file = {Azizi et al. - 2021 - Big Self-Supervised Models Advance Medical Image C.pdf:/home/paulhager/Zotero/storage/MAGRQ86L/Azizi et al. - 2021 - Big Self-Supervised Models Advance Medical Image C.pdf:application/pdf},
}

@article{raisi-estabragh_cardiovascular_2021,
  title={Cardiovascular magnetic resonance imaging in the UK Biobank: a major international health research resource},
  author={Raisi-Estabragh, Zahra and Harvey, Nicholas C and Neubauer, Stefan and Petersen, Steffen E},
  journal={European Heart Journal-Cardiovascular Imaging},
  volume={22},
  number={3},
  pages={251--258},
  year={2021},
  publisher={Oxford University Press}
}


@article{chen_big_2020,
  title={Big self-supervised models are strong semi-supervised learners},
  author={Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={22243--22255},
  year={2020}
}


@inproceedings{caron_emerging_2021,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9650--9660},
  year={2021}
}


@inproceedings{he_momentum_2020,
	location = {Seattle, {WA}, {USA}},
	title = {Momentum Contrast for Unsupervised Visual Representation Learning},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9157636/},
	doi = {10.1109/CVPR42600.2020.00975},
	abstract = {We present Momentum Contrast ({MoCo}) for unsupervised visual representation learning. From a perspective on contrastive learning [29] as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-ﬂy that facilitates contrastive unsupervised learning. {MoCo} provides competitive results under the common linear protocol on {ImageNet} classiﬁcation. More importantly, the representations learned by {MoCo} transfer well to downstream tasks. {MoCo} can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on {PASCAL} {VOC}, {COCO}, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
	eventtitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {9726--9735},
	booktitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	urldate = {2022-09-25},
	date = {2020-06},
	langid = {english},
	file = {He et al. - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf:/Users/paulhager/Zotero/storage/T7J7ULBA/He et al. - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf:application/pdf},
}


@inproceedings{yang_unified_2022,
  title={Unified contrastive learning in image-text-label space},
  author={Yang, Jianwei and Li, Chunyuan and Zhang, Pengchuan and Xiao, Bin and Liu, Ce and Yuan, Lu and Gao, Jianfeng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={19163--19173},
  year={2022}
}


@article{yuan_florence_2021,
  title={Florence: A new foundation model for computer vision},
  author={Yuan, Lu and Chen, Dongdong and Chen, Yi-Ling and Codella, Noel and Dai, Xiyang and Gao, Jianfeng and Hu, Houdong and Huang, Xuedong and Li, Boxin and Li, Chunyuan and others},
  journal={arXiv preprint arXiv:2111.11432},
  year={2021}
}

@report{zhang_contrastive_2022,
	title = {Contrastive Learning of Medical Visual Representations from Paired Images and Text},
	url = {http://arxiv.org/abs/2010.00747},
	abstract = {Learning visual representations of medical images (e.g., X-rays) is core to medical image understanding but its progress has been held back by the scarcity of human annotations. Existing work commonly relies on fine-tuning weights transferred from {ImageNet} pretraining, which is suboptimal due to drastically different image characteristics, or rule-based label extraction from the textual report data paired with medical images, which is inaccurate and hard to generalize. Meanwhile, several recent studies show exciting results from unsupervised contrastive learning from natural images, but we find these methods help little on medical images because of their high inter-class similarity. We propose {ConVIRT}, an alternative unsupervised strategy to learn medical visual representations by exploiting naturally occurring paired descriptive text. Our new method of pretraining medical image encoders with the paired text data via a bidirectional contrastive objective between the two modalities is domain-agnostic, and requires no additional expert input. We test {ConVIRT} by transferring our pretrained weights to 4 medical image classification tasks and 2 zero-shot retrieval tasks, and show that it leads to image representations that considerably outperform strong baselines in most settings. Notably, in all 4 classification tasks, our method requires only 10{\textbackslash}\% as much labeled training data as an {ImageNet} initialized counterpart to achieve better or comparable performance, demonstrating superior data efficiency.},
	number = {{arXiv}:2010.00747},
	institution = {{arXiv}},
	author = {Zhang, Yuhao and Jiang, Hang and Miura, Yasuhide and Manning, Christopher D. and Langlotz, Curtis P.},
	urldate = {2022-09-25},
	date = {2022-09-19},
	eprinttype = {arxiv},
	eprint = {2010.00747 [cs]},
	note = {type: article},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/paulhager/Zotero/storage/QXH5SJ6I/Zhang et al. - 2022 - Contrastive Learning of Medical Visual Representat.pdf:application/pdf;arXiv.org Snapshot:/Users/paulhager/Zotero/storage/2JVECDDN/2010.html:text/html},
}


@article{hollmann_tabpfn_2022,
  title={TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second},
  author={Hollmann, Noah and M{\"u}ller, Samuel and Eggensperger, Katharina and Hutter, Frank},
  journal={arXiv preprint arXiv:2207.01848},
  year={2022}
}


@inproceedings{jia_scaling_2021,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
  booktitle={International Conference on Machine Learning},
  pages={4904--4916},
  year={2021},
  organization={PMLR}
}

@online{demo_wu_nodate,
	title = {Wu Dao 2.0 {\textbar} {GPT}-3 Demo},
	url = {https://gpt3demo.com/apps/wu-dao-20},
	abstract = {Wu Dao 2.0 is 10x larger than {GPT}-3 The [Beijing Academy of Artificial Intelligence](https://www.baai.ac.cn/), styled as {BAAI} and known in Chinese as 北京...},
	author = {Demo, {GPT}-3},
	urldate = {2022-09-25},
	langid = {english},
	file = {Snapshot:/Users/paulhager/Zotero/storage/3K7Q9M6V/wu-dao-20.html:text/html},
}


@inproceedings{pielawski_comir_2020,
	title = {{CoMIR}: Contrastive Multimodal Image Representation for Registration},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/d6428eecbe0f7dff83fc607c5044b2b9-Abstract.html},
	shorttitle = {{CoMIR}},
	abstract = {We propose contrastive coding to learn shared, dense image representations, referred to as {CoMIRs} (Contrastive Multimodal Image Representations). {CoMIRs} enable the registration of multimodal images where existing registration methods often fail due to a lack of sufficiently similar image structures. {CoMIRs} reduce the multimodal registration problem to a monomodal one, in which general intensity-based, as well as feature-based, registration algorithms can be applied. The method involves training one neural network per modality on aligned images, using a contrastive loss based on noise-contrastive estimation ({InfoNCE}). Unlike other contrastive coding methods, used for, e.g., classification, our approach generates image-like representations that contain the information shared between modalities. We introduce a novel, hyperparameter-free modification to {InfoNCE}, to enforce rotational equivariance of the learnt representations, a property essential to the registration task. We assess the extent of achieved rotational equivariance and the stability of the representations with respect to weight initialization, training set, and hyperparameter settings, on a remote sensing dataset of {RGB} and near-infrared images. We evaluate the learnt representations through registration of a biomedical dataset of bright-field and second-harmonic generation microscopy images; two modalities with very little apparent correlation. The proposed approach based on {CoMIRs} significantly outperforms registration of representations created by {GAN}-based image-to-image translation, as well as a state-of-the-art, application-specific method which takes additional knowledge about the data into account. Code is available at: https://github.com/{MIDA}-group/{CoMIR}.},
	pages = {18433--18444},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Pielawski, Nicolas and Wetzer, Elisabeth and Öfverstedt, Johan and Lu, Jiahao and Wählby, Carolina and Lindblad, Joakim and Sladoje, Natasa},
	urldate = {2022-09-25},
	date = {2020},
}

@article{tsao_left_nodate,
  title={Left ventricular structure and risk of cardiovascular events: a Framingham Heart Study Cardiac Magnetic Resonance Study},
  author={Tsao, Connie W and Gona, Philimon N and Salton, Carol J and Chuang, Michael L and Levy, Daniel and Manning, Warren J and O'Donnell, Christopher J},
  journal={Journal of the American Heart Association},
  volume={4},
  number={9},
  pages={e002188},
  year={2015},
  publisher={Am Heart Assoc}
}


@inproceedings{polsterl_combining_2021,
	location = {Cham},
	title = {Combining 3D Image and Tabular Data via the Dynamic Affine Feature Map Transform},
	isbn = {978-3-030-87240-3},
	doi = {10.1007/978-3-030-87240-3_66},
	series = {Lecture Notes in Computer Science},
	abstract = {Prior work on diagnosing Alzheimer’s disease from magnetic resonance images of the brain established that convolutional neural networks ({CNNs}) can leverage the high-dimensional image information for classifying patients. However, little research focused on how these models can utilize the usually low-dimensional tabular information, such as patient demographics or laboratory measurements. We introduce the Dynamic Affine Feature Map Transform ({DAFT}), a general-purpose module for {CNNs} that dynamically rescales and shifts the feature maps of a convolutional layer, conditional on a patient’s tabular clinical information. We show that {DAFT} is highly effective in combining 3D image and tabular information for diagnosis and time-to-dementia prediction, where it outperforms competing {CNNs} with a mean balanced accuracy of 0.622 and mean c-index of 0.748, respectively. Our extensive ablation study provides valuable insights into the architectural properties of {DAFT}. Our implementation is available at https://github.com/ai-med/{DAFT}.},
	pages = {688--698},
	booktitle = {Medical Image Computing and Computer Assisted Intervention – {MICCAI} 2021},
	publisher = {Springer International Publishing},
	author = {Pölsterl, Sebastian and Wolf, Tom Nuno and Wachinger, Christian},
	editor = {de Bruijne, Marleen and Cattin, Philippe C. and Cotin, Stéphane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
	date = {2021},
	langid = {english},
	file = {Full Text PDF:/home/paulhager/Zotero/storage/TEKVMEFK/Pölsterl et al. - 2021 - Combining 3D Image and Tabular Data via the Dynami.pdf:application/pdf},
}


@inproceedings{Zolfaghari_crossclr,
  title={Crossclr: Cross-modal contrastive learning for multi-modal video representations},
  author={Zolfaghari, Mohammadreza and Zhu, Yi and Gehler, Peter and Brox, Thomas},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1450--1459},
  year={2021}
}

@article{xu_videoclip_2021,
  title={Videoclip: Contrastive pre-training for zero-shot video-text understanding},
  author={Xu, Hu and Ghosh, Gargi and Huang, Po-Yao and Okhonko, Dmytro and Aghajanyan, Armen and Metze, Florian and Zettlemoyer, Luke and Feichtenhofer, Christoph},
  journal={arXiv preprint arXiv:2109.14084},
  year={2021}
}


@article{ma_active_2022,
  title={Active contrastive learning of audio-visual video representations},
  author={Ma, Shuang and Zeng, Zhaoyang and McDuff, Daniel and Song, Yale},
  journal={arXiv preprint arXiv:2009.09805},
  year={2020}
}

@inproceedings{yoon_vime_2020,
	title = {{VIME}: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/7d97667a3e056acab9aaf653807b4a03-Abstract.html},
	shorttitle = {{VIME}},
	abstract = {Self- and semi-supervised learning frameworks have made significant progress in training machine learning models with limited labeled data in image and language domains. These methods heavily rely on the unique structure in the domain datasets (such as spatial relationships in images or semantic relationships in language). They are not adaptable to general tabular data which does not have the same explicit structure as image and language data. In this paper, we fill this gap by proposing novel self- and semi-supervised learning frameworks for tabular data, which we refer to collectively as {VIME} (Value Imputation and Mask Estimation). We create a novel pretext task of estimating mask vectors from corrupted tabular data in addition to the reconstruction pretext task for self-supervised learning. We also introduce a novel tabular data augmentation method for self- and semi-supervised learning frameworks. In experiments, we evaluate the proposed framework in multiple tabular datasets from various application domains, such as genomics and clinical data. {VIME} exceeds state-of-the-art performance in comparison to the existing baseline methods.},
	pages = {11033--11043},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Yoon, Jinsung and Zhang, Yao and Jordon, James and van der Schaar, Mihaela},
	urldate = {2022-11-07},
	date = {2020},
	file = {Full Text PDF:/home/paulhager/Zotero/storage/9NA4EQ5L/Yoon et al. - 2020 - VIME Extending the Success of Self- and Semi-supe.pdf:application/pdf},
}


@inproceedings{taleb_contig_2021,
  title={ContIG: Self-supervised Multimodal Contrastive Learning for Medical Imaging with Genetics},
  author={Taleb, Aiham and Kirchler, Matthias and Monti, Remo and Lippert, Christoph},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={20908--20921},
  year={2022}
}


@inproceedings{chuang_debiased_2020,
	title = {Debiased Contrastive Learning},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/63c3ddcc7b23daa1e42dc41f9a44a873-Abstract.html},
	abstract = {A prominent technique for self-supervised representation learning has been to contrast semantically similar and dissimilar pairs of samples. Without access to labels, dissimilar (negative) points are typically taken to be randomly sampled datapoints, implicitly accepting that these points may, in reality, actually have the same label. Perhaps unsurprisingly, we observe that sampling negative examples from truly different labels improves performance, in a synthetic setting where labels are available. Motivated by this observation, we develop a debiased contrastive objective that corrects for the sampling of same-label datapoints, even without knowledge of the true labels. Empirically, the proposed objective consistently outperforms the state-of-the-art for representation learning in vision, language, and reinforcement learning benchmarks. Theoretically, we establish generalization bounds for the downstream classification task.},
	pages = {8765--8775},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Chuang, Ching-Yao and Robinson, Joshua and Lin, Yen-Chen and Torralba, Antonio and Jegelka, Stefanie},
	urldate = {2022-09-25},
	date = {2020},
	file = {Full Text PDF:/Users/paulhager/Zotero/storage/YDYCT2PW/Chuang et al. - 2020 - Debiased Contrastive Learning.pdf:application/pdf},
}


@inproceedings{arik2021tabnet,
  title={Tabnet: Attentive interpretable tabular learning},
  author={Arik, Sercan {\"O} and Pfister, Tomas},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={8},
  pages={6679--6687},
  year={2021}
}

@article{ucar_subtab_2021,
  title={Subtab: Subsetting features of tabular data for self-supervised representation learning},
  author={Ucar, Talip and Hajiramezanali, Ehsan and Edwards, Lindsay},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18853--18865},
  year={2021}
}

@article{bai2018automated,
  title={Automated cardiovascular magnetic resonance image analysis with fully convolutional networks},
  author={Bai, Wenjia and Sinclair, Matthew and Tarroni, Giacomo and Oktay, Ozan and Rajchl, Martin and Vaillant, Ghislain and Lee, Aaron M and Aung, Nay and Lukaschuk, Elena and Sanghvi, Mihir M and others},
  journal={Journal of Cardiovascular Magnetic Resonance},
  volume={20},
  number={1},
  pages={1--12},
  year={2018},
  publisher={Springer}
}

@inproceedings{bai2018recurrent,
  title={Recurrent neural networks for aortic image sequence segmentation with sparse annotations},
  author={Bai, Wenjia and Suzuki, Hideaki and Qin, Chen and Tarroni, Giacomo and Oktay, Ozan and Matthews, Paul M and Rueckert, Daniel},
  booktitle={International conference on medical image computing and computer-assisted intervention},
  pages={586--594},
  year={2018},
  organization={Springer}
}

@article{bai2020population,
  title={A population-based phenome-wide association study of cardiac and aortic structure and function},
  author={Bai, Wenjia and Suzuki, Hideaki and Huang, Jian and Francis, Catherine and Wang, Shuo and Tarroni, Giacomo and Guitton, Florian and Aung, Nay and Fung, Kenneth and Petersen, Steffen E and others},
  journal={Nature medicine},
  volume={26},
  number={10},
  pages={1654--1662},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{petersen2017reference,
  title={Reference ranges for cardiac structure and function using cardiovascular magnetic resonance (CMR) in Caucasians from the UK Biobank population cohort},
  author={Petersen, Steffen E and Aung, Nay and Sanghvi, Mihir M and Zemrak, Filip and Fung, Kenneth and Paiva, Jose Miguel and Francis, Jane M and Khanji, Mohammed Y and Lukaschuk, Elena and Lee, Aaron M and others},
  journal={Journal of Cardiovascular Magnetic Resonance},
  volume={19},
  number={1},
  pages={1--19},
  year={2017},
  publisher={BioMed Central}
}

@article{antelmi2021combining,
  title={Combining Multi-Task Learning and Multi-Channel Variational Auto-Encoders to Exploit Datasets with Missing Observations-Application to Multi-Modal Neuroimaging Studies in Dementia},
  author={Antelmi, Luigi and Ayache, Nicholas and Robert, Philippe and Ribaldi, Federica and Garibotto, Valentina and Frisoni, Giovanni and Lorenzi, Marco},
  year={2021}
}

@article{ko2022deep,
  title={A Deep Generative--Discriminative Learning for Multimodal Representation in Imaging Genetics},
  author={Ko, Wonjun and Jung, Wonsik and Jeon, Eunjin and Suk, Heung-Il},
  journal={IEEE Transactions on Medical Imaging},
  volume={41},
  number={9},
  pages={2348--2359},
  year={2022},
  publisher={IEEE}
}

@article{eguchi2008association,
  title={Association between diabetes mellitus and left ventricular hypertrophy in a multiethnic population},
  author={Eguchi, Kazuo and Boden-Albala, Bernadette and Jin, Zhezhen and Rundek, Tatjana and Sacco, Ralph L and Homma, Shunichi and Di Tullio, Marco R},
  journal={The American journal of cardiology},
  volume={101},
  number={12},
  pages={1787--1791},
  year={2008},
  publisher={Elsevier}
}

@article{grill2020bootstrap,
  title={Bootstrap your own latent-a new approach to self-supervised learning},
  author={Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21271--21284},
  year={2020}
}

@inproceedings{chen2021exploring,
  title={Exploring simple siamese representation learning},
  author={Chen, Xinlei and He, Kaiming},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={15750--15758},
  year={2021}
}

@article{pearson1895notes,
  title={Notes on Regression and Inheritance in the Case of Two Parents Proceedings of the Royal Society of London, 58, 240-242},
  author={Pearson, K},
  journal={K Pearson},
  year={1895}
}

@inproceedings{zbontar2021barlow,
  title={Barlow twins: Self-supervised learning via redundancy reduction},
  author={Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, St{\'e}phane},
  booktitle={International Conference on Machine Learning},
  pages={12310--12320},
  year={2021},
  organization={PMLR}
}

@article{valensi2011prevalence,
  title={Prevalence, incidence, predictive factors and prognosis of silent myocardial infarction: a review of the literature},
  author={Valensi, Paul and Lorgis, Luc and Cottin, Yves},
  journal={Archives of cardiovascular diseases},
  volume={104},
  number={3},
  pages={178--188},
  year={2011},
  publisher={Elsevier}
}

@article{nesto1999screening,
  title={Screening for asymptomatic coronary artery disease in diabetes},
  author={Nesto, Richard W},
  journal={Diabetes Care},
  volume={22},
  number={9},
  pages={1393},
  year={1999},
  publisher={American Diabetes Association}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}