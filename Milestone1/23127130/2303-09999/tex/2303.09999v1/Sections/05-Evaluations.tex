\section{Evaluation}
\label{sec:results}

We formally evaluate the proposed entity and relation extraction modules in STIXnet. We use three metrics to evaluate the modules: precision, recall, and F1-score. We define True Positives (TP), False Positives (FP), and False Negatives (FN) as follows.

\begin{itemize}
    \item \textbf{True Positives}: entities or relations correctly classified by the model.
    \item \textbf{False Positives}: entities and relations found by the model but misclassified or that do not constitute an entity or a relation.
    \item \textbf{False Negatives}: entities and relations not found by the model but present in the report.
\end{itemize}

The metrics for the evaluation are Precision, Recall, and F1 Score and are defined as follows.

\begin{equation}
    Precision = \frac{TP}{TP + FP},
\end{equation}

\begin{equation}
    Recall = \frac{TP}{TP + FN},
\end{equation}

\begin{equation}
    F1 = 2\frac{Precision \cdot Recall}{Precision + Recall}.
\end{equation}

Given the lack of available annotated reports in the literature, we generated our own dataset of CTI reports to evaluate our model. Each report treats a group or threat actor from the MITRE ATT\&CK framework. All their related data has been extracted from their official descriptions and the external sources listed by the ATT\&CK APIs. We then manually label each report with LabelStudio, a free and open-source software for data labeling. Both the dataset and the annotations are free to use and accessible in our repository. Annotations are exported in a \texttt{JSON} file and can be graphically visualized through the LabelStudio software.

To show the effectiveness of both entity and relation extraction, we tackle the evaluation of the modules separately, respectively, in Section~\ref{subsec:entityextraction_results} and Section~\ref{subsub:relationextraction_results}.

\subsection{Entity Extraction}
\label{subsec:entityextraction_results}

While evaluating the entity extraction module for STIXnet, we must consider the different sub-modules separately since they perform different independent operations.

To have a baseline for comparing the results, we first evaluate the completeness of the deployed Knowledge Base in a fixed position in time, i.e., not being enhanced by adding new entities when found. We take the whole dataset and run the rule-based entity extraction sub-module on each report. We then extract precision, recall, and F1 scores and compute the mean of these scores over the number of reports processed. We compare this approach with other rule-based algorithms for entity extraction found in the literature. Since rule-based approaches are domain-dependent and language-dependent, in Table~\ref{tab:kbeval} we compare our approach with other works in literature that operate in specific domains to more accurately resemble our task (for~\cite{10.1093/jamia/ocz109}, we extracted the evaluation of the hybrid model since it more accurately resembles our rule-based algorithm). While the compared models are not designed for CTI data extraction, to the best of our knowledge, no other tools in the literature perform such an evaluation on rule-based approaches for STIX entities. As a note, the ground truth constituted by the manually annotated reports also contains novel entities and TTPs (not extractable with just this submodule), thus highlighting the contribution of the sub-module in the whole entity extraction system.

\input{Tables/05-Baseline.tex}

We then evaluate the novel entity extraction sub-module separately. We created a new dataset of sentences containing non-existent entities with made-up names for its evaluation. We do this since, in the original dataset, some reports do not contain any new entities and would thus bias the results of this evaluation. By creating new sentences instead, we ensure a constant number of novel entities that the sub-module can extract. In such a scenario, the isolated sub-module reaches a precision of 0.927, a recall of 0.854, and an F1 score of 0.889. Given the system's modularity and the submodules' specific task, a comparison with other models for Information Extraction in Cyber Threat Intelligence is given in Table~\ref{tab:noveleval} (the results of STIXnet extraction are obtained with the dynamically augmented Knowledge Base). Results are obtained after combining the contribution of the different approaches and evaluating them in the same dataset used for the baseline evaluation. Also, while focusing on CTI applications, we highlight the number of entity types the compared models can extract.

\input{Tables/05-Comparison.tex}

We also show the class-specific results for the most frequent entity types in Table~\ref{tab:classpecific}.
As we can see, the \texttt{location} class has the highest performance, given the limited number of entities and their unambiguous nature. Among the best-performing entities, there are also \texttt{intrusion set} and \texttt{malware}, thanks to their peculiar names that are easily identifiable in the text. Classes \texttt{tool} and \texttt{campaign}, however, can cause many false negatives, which is reflected by their recall values. Indeed, these concepts are fairly easy to identify once introduced in the Knowledge Base, but their novel identification might be more difficult. For example, campaign concepts might be hard to recognize in a text, and new tools might be referenced without their introduction (or can be confused with malwares).

\input{Tables/05-Types.tex}

\subsubsection{Temporal Evolution}
\label{subsub:temporal}

The interaction of the entity extraction module with the Knowledge Base allows STIXnet to easily and quickly extract previously recognized entities and constantly update the contents of the database to guarantee high-performance values over time. To ensure that the reports' processing order does not influence the evaluation results, we randomly shuffled the dataset many times and evaluated each shuffle. We obtained a standard deviation between results close to 0 for all the evaluation metrics.

To highlight the capability of STIXnet to maintain its performance over time, we must simulate the execution of the module on subsequent reports following a temporal evolution. To do this, we evaluate its performance according to the following steps:

\begin{enumerate}
    \item We divided the dataset into batches of 5 reports each.
    \item For one of the batches, we run the entire module on each report and evaluate its performance.
    \item Entities found by the novel entity extraction sub-modules are added to the Knowledge Base after a quick manual validation.
    \item Repeat steps (2) and (3) for all the other batches.
\end{enumerate}

We think this evaluation is fair on the premises of the possible implementations of the tool. Indeed, in a real-world application, several reports are be published each day and thus need to be processed. Given the deep relationship with the Knowledge Base, we need to avoid introducing bad entities that could degrade the system's performance in the long term. Thus, the newly extracted entities can be manually validated by an analyst, who should ensure the integrity of the data instead of fully annotating the report. The results of this evaluation are shown in Figure~\ref{fig:metricevol}, where the evolution of the metrics is given in function of the processed batch of reports. Furthermore, to highlight the advantage of this approach concerning a static deployment of the Knowledge Base, we performed a different evaluation by repeating the same steps but skipping the third and thus freezing the state of the Knowledge Base in time. As shown, adding entities in the Knowledge Base and their quick validation greatly improve overall performances for all three evaluation metrics and ensure the system's updated status on the latest reports.

\begin{figure*}[!htpb]
  \centering
  \begin{subfigure}{0.32\textwidth}
     \centering
     \includegraphics[width=\textwidth]{Figures/05-Prec.pdf}
     \caption{Precision.}
     \label{fig:entextprec}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
     \centering
     \includegraphics[width=\textwidth]{Figures/05-Rec.pdf}
     \caption{Recall.}
     \label{fig:entextrec}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
     \centering
     \includegraphics[width=\textwidth]{Figures/05-F1.pdf}
     \caption{F1 Score.}
     \label{fig:entextf1}
  \end{subfigure}
  \caption{Evolution of the metrics for evaluating Entity Extraction in function of reports batch.}
  \label{fig:metricevol}
\end{figure*}

\subsection{Relation Extraction}
\label{subsub:relationextraction_results}

For evaluating the relation extraction module of STIXnet, we use the final results derived from the combined use of the rule-based approach and the deep learning based approach for relation extraction. As mentioned, relations are extracted by both submodules with a confidence value which is used to compare the results and eventually impose a threshold of confidence under which relations are discarded. In this way, we include the possibility of a non-relation between two entities. To compare the confidence values for both submodules, we normalized the cosine similarity of the embeddings given by the deep learning based approach in the $[0,1]$ range. We found that a value of 0.5 for the threshold provides a fair tradeoff between false positives and false negatives.

Evaluation has been performed by comparing the relations extracted manually from the reports with the ones that the relation extraction module of STIXnet has found. This module input is constituted by the entities extracted by the entity extraction module and works on them and the sentences in the text to find possible relations. However, the modularity enforced by the STIXnet pipeline introduces the error propagation problem from entity extraction to relation extraction. Indeed, whenever an entity is misclassified by one of the sub-modules of entity extraction, it is passed as input in the relation extraction module, which inevitably produces an error since that entity constitutes a false positive. For this reason, Table~\ref{tab:relext} shows the evaluation for both scenarios. In the first one, we execute the relation extraction module after the results from the entity extraction module have been generated. In the second one, we do not consider relationships between entities where at least one was misclassified to remove the error propagation between the modules. While the F1 scores of the two scenarios are similar, the precision when removing the error propagation effect increases by around 0.1. This type of scenario, however, affects the recall value since the overall number of true positives is decreased, but the number of false negatives is not.

\input{Tables/05-Propagation.tex}

While the performances of the relation extraction module in the regular scenario have a lower value with respect to the ones of the entity extraction module, we must keep in mind that with each extracted entity, the overall number of possible relationships in the text exponentially increases. Indeed, to the best of our knowledge, ours is the only model that tackles both tasks subsequently by considering each of the entity types of the STIX standard and each of the STIX relationship objects.