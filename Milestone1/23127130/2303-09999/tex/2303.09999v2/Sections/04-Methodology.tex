\section{Methodology}
\label{sec:methodology}

In this section, we present STIXnet in more detail on how it works and which algorithms and models are used to perform Information Extraction on unstructured Cyber Threat Intelligence reports. We first show its pipeline and briefly overview the various modules (Section~\ref{subsec:pipeline}). Then we give more details on its main components: the text extraction module (Section~\ref{subsec:textextraction}), the entity extraction module (Section~\ref{subsec:entityextraction_methodology}) and the relation extraction module (Section~\ref{subsec:relationextraction_methodology}).

\subsection{Pipeline}
\label{subsec:pipeline}

STIXnet performs a highly complex Information Extraction task on many different types of entities and relations. There are indeed 18 different types of entities compliant with the STIX standard and more than 100 types of relations. To accomplish this, STIXnet uses different modules for each one of the different tasks that it must achieve: textual extraction, entity extraction, and relation extraction. A graphic overview of the STIXnet platform is shown in Figure~\ref{fig:pipeline}.

\begin{figure*}[!htpb]
  \centering
  \includegraphics[width=\linewidth]{Figures/04-Pipeline3.pdf}
  \caption{Pipeline of the STIXnet platform.}
  \label{fig:pipeline}
\end{figure*}

\begin{itemize}
    \item \textbf{Text Extraction}: the first module converts the program's input into raw text. While doing this, artifacts are inevitably be introduced in the text, and therefore they must be handled to obtain a single string of characters in a common encoding.
    \item \textbf{Entity Extraction}: this module handles the extraction of the different entities in the text. It uses four different sub-modules to accomplish that:
    \begin{itemize}
        \item \textbf{IOC Finder}: we extract Indicators Of Compromise by using different regular expression rules and looking at text patterns.
        \item \textbf{Knowledge Base Entities Extraction}: using the entities in a Knowledge Base, we use an efficient algorithm for string search in a text to retrieve names and aliases. We mitigate errors and false positives caused by this approach using NLP techniques.
        \item \textbf{Novel Entities Extraction}: using NLP libraries, we extract entities not present in the Knowledge Base, which can then be integrated into the KB.
        \item \textbf{TTPs Extraction}: techniques and tactics are not always represented in a text by their name but can be implicit and not explicitly expressed. We use a Machine Learning model trained on MITRE ATT\&CK tactics and techniques to recognize them.
    \end{itemize}
    \item \textbf{Relation Extraction}: from the extracted entities, we now retrieve the relations. We use two sub-modules for this task:
    \begin{itemize}
        \item \textbf{Rule-Based Approach}: using NLP techniques, we perform Dependency Parsing and compute the shortest paths between entities. Comparing the verb inside the path with the ones in the STIX relationships, we estimate the most similar one with a degree of confidence.
        \item \textbf{Deep Learning Based Approach}: to adjust the results of the previous approach, we also compute embeddings from the sentences with a Deep Learning model. We then determine the similarity between these embeddings and those computed from the list of relationship labels.
    \end{itemize}
    \item \textbf{Output}: we create a JSON file from the extracted entities and relations, which is processed by the graphical interface of the platform to be interactive and dynamic.
\end{itemize}

Eventual interactions with the Knowledge Base or other platform components are disclosed in the individual sections of the different modules. Indeed, we show that by interacting with the database, it is possible to improve performance over time, particularly if an analyst decides to validate the results of the STIXnet output. 

Furthermore, the structure of the STIXnet pipeline allows for easy and immediate management of the different modules. Formally defining the interactions between modules and submodules allows results from different pipeline components to be compared and merged in a unique output. Thus, researchers and organizations can implement the STIXnet framework in different IE scenarios and add or remove components depending on their needs.

\subsection{Text Extraction}
\label{subsec:textextraction}

One of the platform's most relevant and sensible aspects is its input. As mentioned, STIXnet can take in input various reports and bulletins, which can come from various vendors or sources. For this reason, reports have some stylistic and linguistic differences. However, data must be converted univocally before processing the raw text to have consistent processing between different inputs and a common ground for evaluating the various modules. This means considering many different aspects that can change from input to input. First of all, we must be able to parse text from files with different formats, and thus we use Apache Tika\footnote{\url{https://tika.apache.org/}} for \texttt{pdf} and \texttt{doc} files and ConvertAPI\footnote{\url{https://www.convertapi.com/}} to extract text from the HTML data of web reports. However, since text can be formatted in many different ways, we must process it to remove artifacts, fix line breaks, and remove eventual sanifications on IP and other addresses. This phase is crucial to ensure that the following modules are presented with a clean input; otherwise, artifacts are propagated in the pipeline and compromise the results.

\subsection{Entity Extraction}
\label{subsec:entityextraction_methodology}

This section tackles the different submodules used for entity extraction: IOC Finder (Section~\ref{subsub:iocfinder}), a rule-based entity extractor (Section~\ref{subsub:kbentext}), a novel entity extractor (Section~\ref{subsub:novelentityextraction}), and a TTPs extractor (Section~\ref{subsub:ttpext}). Finally, we clarify how the submodules interact with one another and merge their results (Section~\ref{subsub:subint}).

\subsubsection{IOC Finder}
\label{subsub:iocfinder}

The particular structure of Indicators Of Compromise allows us to use regular expression (Regex) rules to find them in the text. Moreover, in some of the reports distributed by CTI vendors, the end of the document is often presented with a table containing the IOCs of interest for that particular topic. While different, all these indicators share a common structure across each type, which can be recognized with Regex rules without applying NLP techniques. Some examples of the types of IOCs supported by IOC Finder can be found in Table~\ref{tab:iocfinder}. During the execution of STIXnet and after processing the report as raw text, the first step is to run the IOC Finder submodule on it, which returns a dictionary containing the entities found.

We implement this module by forking an open-source project by Floyd Hightower\footnote{\url{https://github.com/fhightower/ioc-finder}}. To adapt it for our pipeline, we contributed to the main project by updating some libraries to a newer version and adding the ability to track the position of the found IOCs. The code of the forked project can be found in our repository.

\input{Tables/04-IOCs.tex}

\subsubsection{Knowledge Base Entity Extraction}
\label{subsub:kbentext}

After finding IOCs, all other entities of interest do not share a common structure and thus cannot be found through regular expression rules. Thus, we leverage a rich Knowledge Base integrated with multiple OSINT that explicitly indicate which names represent important entities and allow us to link them to their correct entity type. For this reason, a rule-based algorithm can search specific words in the text, retrieve their position and thus highlight them as extracted entities. The different sources for the intelligence are:

\begin{itemize}
    \item \textbf{Knowledge Base}: Leonardo S.p.A., an Italian multinational company that collaborated in this research, provided a rich database of STIX entities. Cyber threat intelligence analysts built this database over the years at their Security Operation Center (SOC), which read and manually annotated entities from many reports.
    \item \textbf{MITRE ATT\&CK}: the ATT\&CK framework can be used as a source of intelligence for different entities such as techniques, tactics, groups, and software (of which the conversion to the STIX standard has been disclosed in Table~\ref{tab:mitre2stix}). To retrieve this data, we use the Trusted Automated Exchange of Intelligence Information (TAXII) application protocol~\cite{connolly2014trusted}, which allows for exchanging threat intelligence over HTTPS and defines a RESTful API that can be used to provide or collect data.
    \item \textbf{Locations}: to retrieve the names of countries and continents, we used a \texttt{csv} file in which each nation is associated with its nationality. In this way, we are able to identify locations even when used as an attribute to another entity (e.g., "a \textit{Russian} malware").
\end{itemize}

After retrieving the entities, a quick pre-processing is performed to unify their formats and add the possibility of aliases for each one. Through aliases, it is possible to recognize an entity in a text and map it to the correct one, avoiding duplicates and fixing the issue of multiple names for a single Advanced Persistent Threat. We then use the Aho-Corasick algorithm to find terms of this thesaurus of words in the report~\cite{10.1145/360825.360855}. To mitigate false positives, we process each sentence with NLP techniques, in particular, Part-Of-Speech Tagging (POS tagging), allowing us to assign part-of-speech tags to each word (e.g., noun, verb). After defining a table of entity types and their possible POS tags, we use it to compare the entities found in the report with their extracted POS tag. For example, "us" can be used as a pronoun or can be used as a noun to reference the United States, constituting an entity of type "location".

\subsubsection{Novel Entities Extraction}
\label{subsub:novelentityextraction}

Some CTI reports are published to spread awareness of newly discovered actors, malwares, or techniques. These entities are thus named by CTI researchers and analysts and, for this reason, are most likely not present in the Knowledge Base. To find these new entities, we can leverage the previous execution of POS Tagging to extend its results and create a dependency graph from the tokens found in each sentence. In this way, we identify specific patterns used in the text to express a new entity. To create such a graph, we leverage both the POS tags and the dependencies between the tokens, as shown in Figure~\ref{fig:spacygraph}. To perform this processing, we use Spacy\footnote{\url{https://spacy.io/}}, a free, open-source library for advanced NLP in Python. By looking at numerous reports and bulletins from different vendors and sources, we can identify a limited number of ways a new entity can be introduced, allowing us to write pattern rules that the NLP processing can recognize.

\begin{figure*}[!htpb]
  \centering
  \includegraphics[width=\linewidth]{Figures/04-Spacy.pdf}
  \caption{Example of dependency graph generated from a sentence.}
  \label{fig:spacygraph}
\end{figure*}

\subsubsection{TTPs Extraction}
\label{subsub:ttpext}

While malwares and threat actors are often explicitly mentioned, some other entities are not and can be referenced without categorically stating their names. It is the case of tactics and techniques, which constitute the TTPs mapped into the STIX objects "x-mitre-tactic" and "attack pattern". Rule-based methods cannot be used to retrieve these entities since the variance that characterizes the expression of these concepts is too broad and is hardly definable through a set of rules. For this reason, a multi-label text classification model must be deployed and trained on the MITRE ATT\&CK Knowledge Base of tactics and techniques. We already presented a tool named rcATT~\cite{https://doi.org/10.48550/arxiv.2004.14322} that suffered from its age since it was published in 2020, and the MITRE ATT\&CK framework has had several changes and renovations ever since. The source code for rcATT is publicly available in their GitHub repository\footnote{\url{https://github.com/vlegoy/rcATT}}, and thus we retrained it from scratch with new techniques and tactics. Also, to address one of the limitations and future works of the paper presenting rcATT, we expanded the training dataset with more data from MITRE ATT\&CK descriptions and external sources for each tactic and technique. The new dataset, the pre-trained models, and the updated code for its training can be found in our repository.

\subsubsection{Submodules Interaction}
\label{subsub:subint}

As stated in Section~\ref{subsec:pipeline}, the pipeline structure of our framework allows us to differentiate the tasks in many modules. For entity extraction, in particular, we identified four different submodules independent from one another. Their input is always the same and is constituted by the textual data of the report. Once each submodule produces an output, a final check is performed to ensure no overlapping occurs during the processing. This process includes cross-checking the found entities in the Knowledge Base to maximize their confidence in their extracted type and the addition of the novel entities. More details are disclosed in Section~\ref{subsec:entityextraction_results}. Finally, all the entities are merged into one data structure, constituting the following module's input. Since this procedure is automatic, submodules can be added and removed in the pipeline according to the user's needs, and no conflicts occur during the process.

\subsection{Relation Extraction}
\label{subsec:relationextraction_methodology}

This module can retrieve relations between the found entities while processing the sentences in the raw text. However, one of Spacy's limitations is its inability to grasp relations between distant entities in a text. To address this, we propose two different approaches for relation extraction. In the first approach, we leverage POS Tagging and Dependency Parsing to compute a graph of each sentence and retrieve relations by looking at the shortest paths between entities (Section~\ref{subsub:rulebased}). In the second approach, we use a Transformer model to compute embeddings of the sentences and compute their similarity (Section~\ref{subsub:dlbased}). Finally, we clarify how the submodules interact with one another and merge their results (Section~\ref{subsub:relsubint}).

\subsubsection{Rule Based Approach}
\label{subsub:rulebased}

The main idea of the rule-based approach is to leverage the dependency graphs already computed by Spacy for novel entity extraction (Section~\ref{subsub:novelentityextraction}) and use graph theory functions to grasp the relation between two entities inside a sentence. In particular, we can process the dependency graph and retrieve the relations between any couple of nodes by discovering the Shortest Dependency Path (SDP), i.e., the shortest path between two nodes in the graph. It has been observed in other studies that the nodes in the SDPs usually contain the necessary information to identify a relationship between two entities while also being dependent on the structure and semantic complexity of the sentence~\cite{hua2016shortest, xu-etal-2015-classifying}.

After retrieving the shortest path between each couple of entities in the sentence, we extract their STIX type and focus on the verbs in the path. For example, considering the sentence in Figure~\ref{fig:spacygraph}, the extracted paths (between the three entities \texttt{"APT29"}, \texttt{"7-Zip"}, and \texttt{"Raindrop"}) are:
\begin{itemize}
    \item \texttt{[APT29, \underline{used}, 7-Zip]};
    \item \texttt{[APT29, \underline{used}, \underline{decode}, malware, Raindrop]};
    \item \texttt{[7-Zip, \underline{used}, \underline{decode}, malware, Raindrop]}.
\end{itemize}
By looking at the entity types and the root form of the verbs (underlined in the previous example), it is possible to compare them with the ones found in the list of STIX relationships and thus label the path as a specific STIX Relationship Object (SRO). However, since the verbs used to describe the relationship in the sentence might not be the same as the associated SRO, we use a similarity function to determine their likeliness. If it surpasses a certain threshold, we can consider them synonyms. To accomplish this, we use the Wu \& Palmer similarity function~\cite{https://doi.org/10.48550/arxiv.cmp-lg/9406033}, which, given two words and their synsets (i.e., groupings of similar words that express the same concept), outputs a value in the range $\left[0,1\right]$, where $1$ means maximum similarity. For this reason, this value can be used as a confidence measure of the relation. The taxonomy used for this task is the WordNet taxonomy, an extensive lexical database of English words developed by Princeton University~\cite{Fellbaum2010}.

For example, considering the SDP \texttt{[APT29, used, 7-Zip]}, the entity extraction module identified \texttt{"APT29"} as an \texttt{intrusion-set} and \texttt{"7-Zip"} as a \texttt{tool}. In the list of SROs, there is only one entry containing both an \texttt{intrusion-set} and a \texttt{tool}, which is "intrusion-set uses tool": since the root form of the verb in the SDP is equal to the one in the SRO, we label it accordingly with maximum confidence. Instead, considering another SDP \texttt{[APT29, attacks, the, US]} (where \texttt{"US"} is identified as a \texttt{location} entity), there are two SROs including both an \texttt{intrusion-set} and a \texttt{location}: "intrusion-set originates-from location" and "intrusion-set targets location". In this case, none of the root forms of SROs verbs ("originate" and "target") coincide with the one in the SDP ("attack"), and thus we compute the similarity between them:
\begin{equation*}
    wup("attack", "originate") = 0.4,
\end{equation*}
\begin{equation*}
    wup("attack", "target") = 0.5.
\end{equation*}
While keeping the confidence threshold at 0.5, we can label the SDP as a relationship of type "intrusion-set targets location".

The confidence value in the extracted relations becomes particularly important when dealing with sentences containing multiple entities. Indeed, the number of relations increases exponentially with the number of entities found, and some of the extracted relations could not exist. To include a "non-relation" label in this task, we set a threshold for the confidence, under which we discard the extracted relations.

\subsubsection{Deep Learning Based Approach}
\label{subsub:dlbased}

The rule-based approach is particularly efficient when dealing with simple phrases or when entities are close in the graph. However, many elements might be introduced in the SDP whenever two entities are far from each other. The verb with the highest similarity could be linked to different tokens in the text and might not reach the confidence threshold. To address this problem, we use embeddings, fixed-size vectors that can also be generated from textual data by Deep Learning models such as Transformers~\cite{https://doi.org/10.48550/arxiv.1706.03762}.

In this specific case of relation extraction, we are interested in computing the similarity between each sentence's embeddings and the STIX relationships' embeddings. To perform these embeddings, the best tool at our disposal is Sentence BERT (SBERT)\footnote{\url{https://www.sbert.net/}}, a variation of the BERT model (Bidirectional Encoder Representations from Transformers) developed by Google AI language~\cite{https://doi.org/10.48550/arxiv.1905.05950}. While BERT constitutes the state-of-the-art in many NLP applications, it becomes inefficient when dealing with a large corpus of sentence processing. SBERT addresses this problem using siamese and triplet network structures, drastically reducing processing time~\cite{reimers-2019-sentence-bert}.

For its STIXnet implementation, we compute the embeddings of the different STIX relationships and the sentences extracted from the report. For each sentence, we perform a pre-processing procedure for each contained entity couple by substituting their tokens with their extracted STIX type. Then, we compute the cosine similarity between these embeddings and normalize it to use it as a confidence value. We also use a threshold for the confidence of the relation to discriminate false positives (0.5).

\subsubsection{Submodules Interaction}
\label{subsub:relsubint}

As with the entity extraction module, the relation extraction task is divided into different submodules that work independently. While their input is always the same (i.e., textual data from the report and the previously identified entities), the two submodules perform virtually the same task this time. Thus we expect a degree of overlap in their results. However, their coexistence is necessary to extract relations from both simple and complex scenarios. To handle conflicts in the possible output, we use the confidence values generated during the processing and keep the relations between entities with maximum confidence, which must also be over the acceptance threshold. Therefore, users can add their desired submodules for the relation extraction task, and STIXnet will automatically merge their results.