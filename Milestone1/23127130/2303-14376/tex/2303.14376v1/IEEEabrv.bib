%%
%% IEEEabrv.bib
%%
%% BibTeX bibliography string definitions of the ABBREVIATED titles of
%% IEEE journals and magazines
%% 
%% NOTE: This text file uses MS Windows line feed conventions. When (human)
%% reading this file on other platforms, you may have to use a text
%% editor that can handle lines terminated by the MS Windows line feed
%% characters (0x0D 0x0A).
%%
%% This file is designed for bibliography styles that require 
%% abbreviated titles and is not for use in bibliographies that
%% require full-length titles.
%% 
%% Version 1.10 (2003/04/02)
%% 
%% Composed by Michael Shell
%% 
%% See:
%% http://www.ctan.org/tex-archive/macros/latex/contrib/supported/IEEEtran/
%% for latest version and current contact information.
%%
%% Special thanks to Laura Hyslop of IEEE for her help in obtaining the
%% information needed to compile this file. Also, Volker Kuhlmann kindly
%% provided some corrections and additions.
%%
%%**********************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% This code is distributed under the Perl Artistic License 
%% ( http://language.perl.com/misc/Artistic.html ) 
%% and may be freely used, distributed and modified - subject to the
%% constraints therein.
%% Retain all contribution notices, credits and disclaimers.
%% 
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%**********************************************************************
%
%
% USAGE:
% 
% \bibliographystyle{mybstfile} 
% \bibliography{IEEEabrv,mybibfile}
%
% where the IEEE titles in the .bib database entries use the strings
% defined here. e.g.,
%
%
%    journal = IEEE_J_AC,
%
%
% to yield "{IEEE} Trans. Automat. Contr."
%
%
% IEEE uses abbreviated journal titles in their bibliographies -
% this file is suitable for work that is to be submitted to the IEEE.
%
%
% For work that requires full-length titles, you should use the full
% titles provided in the companion file, IEEEfull.bib.
%
%
% ** NOTES **
%
% 1. Journals have been grouped according to subject in order to make it
%    easier to locate and extract the definitions for related journals - 
%    as most works use references that are confined to a single topic.
%    Magazines are listed in straight alphabetical order.
%
% 2. String names are closely based on IEEE's own internal acronyms.
% 
% 3. Abbreviations follow IEEE's style.
%
% 4. Older, out-of-print IEEE titles are included (but not including titles
%    dating prior to IEEE's formation from the IRE and AIEE in 1963).
%
% 5. The following NEW/current journal definitions have been disabled because
%    their abbreviations have not yet been finalized/verified:
%    
%    STRING{IEEE_J_MC         = "{IEEE} Trans. Mobile Comput."}
%    STRING{IEEE_J_JEM        = "{IEEE/TMS} J. Electron. Mater."}
%    STRING{IEEE_J_MCTE       = "{IEEE} Trans. Molecular Cellular Tissue Eng."}
%    STRING{IEEE_J_ESSL       = "{IEEE/ECS} Electrochemical Solid-State Lett."}
%    STRING{IEEE_J_CJECE      = "Canadian J. Elect. Comput. Eng."}
%    
% 6. The following OLD journal definitions have been disabled because
%    their abbreviations have not yet been found/verified:
%
%    STRING{IEEE_J_EWS        = "{IEEE} Trans. Eng. Writing Speech"}
%    STRING{IEEE_J_BCTV       = "{IEEE} Trans. Broadcast Television Receivers"}
%
% If you know what the proper abbreviation is for a string in #5 or #6 above,
% email me and I will correct them in the next release.




%%%%%%%%%%%%%%%%%%%
%% IEEE Journals %%
%%%%%%%%%%%%%%%%%%%


% aerospace and military
@STRING{IEEE_J_AES        = "{IEEE} Trans. Aerosp. Electron. Syst."}
@STRING{IEEE_J_ANE        = "{IEEE} Trans. Aerosp. Navig. Electron."}
@STRING{IEEE_J_ANNE       = "{IEEE} Trans. Aeronaut. Navig. Electron."}
@STRING{IEEE_J_AS         = "{IEEE} Trans. Aerosp."}
@STRING{IEEE_J_AIRE       = "{IEEE} Trans. Airborne Electron."}
@STRING{IEEE_J_MIL        = "{IEEE} Trans. Mil. Electron."}



% autos, transportation and vehicles (non-aerospace)
@STRING{IEEE_J_ITS        = "{IEEE} Trans. Intell. Transport. Syst."}
@STRING{IEEE_J_VT         = "{IEEE} Trans. Veh. Technol."}
@STRING{IEEE_J_VC         = "{IEEE} Trans. Veh. Commun."}



% circuits, signals, systems, audio and controls
@STRING{IEEE_J_SPL        = "{IEEE} Signal Processing Lett."}
@STRING{IEEE_J_ASSP       = "{IEEE} Trans. Acoust., Speech, Signal Processing"}
@STRING{IEEE_J_AU         = "{IEEE} Trans. Audio"}
@STRING{IEEE_J_AUEA       = "{IEEE} Trans. Audio Electroacoust."}
@STRING{IEEE_J_AC         = "{IEEE} Trans. Automat. Contr."}
@STRING{IEEE_J_CAS        = "{IEEE} Trans. Circuits Syst."}
@STRING{IEEE_J_CASVT      = "{IEEE} Trans. Circuits Syst. Video Technol."}
@STRING{IEEE_J_CASI       = "{IEEE} Trans. Circuits Syst. {I}"}
@STRING{IEEE_J_CASII      = "{IEEE} Trans. Circuits Syst. {II}"}
@STRING{IEEE_J_CT         = "{IEEE} Trans. Circuit Theory"}
@STRING{IEEE_J_CST        = "{IEEE} Trans. Contr. Syst. Technol."}
@STRING{IEEE_J_SP         = "{IEEE} Trans. Signal Processing"}
@STRING{IEEE_J_SU         = "{IEEE} Trans. Sonics Ultrason."}
@STRING{IEEE_J_SAP        = "{IEEE} Trans. Speech Audio Processing"}
@STRING{IEEE_J_UE         = "{IEEE} Trans. Ultrason. Eng."}
@STRING{IEEE_J_UFFC       = "{IEEE} Trans. Ultrason., Ferroelect., Freq. Contr."}



% communications
@STRING{IEEE_J_COML       = "{IEEE} Commun. Lett."}
@STRING{IEEE_J_JSAC       = "{IEEE} J. Select. Areas Commun."}
@STRING{IEEE_J_COM        = "{IEEE} Trans. Commun."}
@STRING{IEEE_J_COMT       = "{IEEE} Trans. Commun. Technol."}
@STRING{IEEE_J_WCOM       = "{IEEE} Trans. Wireless Commun."}



% components, packaging and manufacturing
@STRING{IEEE_J_ADVP       = "{IEEE} Trans. Adv. Packag."}
@STRING{IEEE_J_CHMT       = "{IEEE} Trans. Comp., Hybrids, Manufact. Technol."}
@STRING{IEEE_J_CPMTA      = "{IEEE} Trans. Comp., Packag., Manufact. Technol. {A}"}
@STRING{IEEE_J_CPMTB      = "{IEEE} Trans. Comp., Packag., Manufact. Technol. {B}"}
@STRING{IEEE_J_CPMTC      = "{IEEE} Trans. Comp., Packag., Manufact. Technol. {C}"}
@STRING{IEEE_J_CAPT       = "{IEEE} Trans. Comp. Packag. Technol."}
@STRING{IEEE_J_CAPTS      = "{IEEE} Trans. Comp. Packag. Technol."}
@STRING{IEEE_J_CPART      = "{IEEE} Trans. Comp. Parts"}
@STRING{IEEE_J_EPM        = "{IEEE} Trans. Electron. Packag. Manufact."}
@STRING{IEEE_J_MFT        = "{IEEE} Trans. Manufact. Technol."}
@STRING{IEEE_J_PHP        = "{IEEE} Trans. Parts, Hybrids, Packag."}
@STRING{IEEE_J_PMP        = "{IEEE} Trans. Parts, Mater., Packag."}



% CAD
@STRING{IEEE_J_TCAD       = "{IEEE} J. Technol. Computer Aided Design"}
@STRING{IEEE_J_CAD        = "{IEEE} Trans. Computer-Aided Design"}



% coding, data, information, knowledge
@STRING{IEEE_J_IT         = "{IEEE} Trans. Inform. Theory"}
@STRING{IEEE_J_KDE        = "{IEEE} Trans. Knowledge Data Eng."}



% computers, computation, networking and software
@STRING{IEEE_J_C          = "{IEEE} Trans. Comput."}
@STRING{IEEE_J_ECOMP      = "{IEEE} Trans. Electron. Comput."}
@STRING{IEEE_J_EVC        = "{IEEE} Trans. Evol. Comput."}
@STRING{IEEE_J_FUZZ       = "{IEEE} Trans. Fuzzy Syst."}
% disabled till definition finalized
STRING{IEEE_J_MC          = "{IEEE} Trans. Mobile Comput."}
@STRING{IEEE_J_NET        = "{IEEE/ACM} Trans. Networking"}
@STRING{IEEE_J_NN         = "{IEEE} Trans. Neural Networks"}
@STRING{IEEE_J_PDS        = "{IEEE} Trans. Parallel Distrib. Syst."}
@STRING{IEEE_J_SE         = "{IEEE} Trans. Software Eng."}



% computer graphics, imaging, and multimedia
@STRING{IEEE_J_IP         = "{IEEE} Trans. Image Processing"}
@STRING{IEEE_J_MM         = "{IEEE} Trans. Multimedia"}
@STRING{IEEE_J_VCG        = "{IEEE} Trans. Visual. Comput. Graphics"}



% cybernetics, ergonomics, robots, man-machine, and automation
@STRING{IEEE_J_JRA        = "{IEEE} J. Robot. Automat."}
@STRING{IEEE_J_HFE        = "{IEEE} Trans. Hum. Factors Electron."}
@STRING{IEEE_J_MMS        = "{IEEE} Trans. Man-Mach. Syst."}
@STRING{IEEE_J_PAMI       = "{IEEE} Trans. Pattern Anal. Machine Intell."}
@STRING{IEEE_J_RA         = "{IEEE} Trans. Robot. Automat."}
@STRING{IEEE_J_SMC        = "{IEEE} Trans. Syst., Man, Cybern."}
@STRING{IEEE_J_SMCA       = "{IEEE} Trans. Syst., Man, Cybern. {A}"}
@STRING{IEEE_J_SMCB       = "{IEEE} Trans. Syst., Man, Cybern. {B}"}
@STRING{IEEE_J_SMCC       = "{IEEE} Trans. Syst., Man, Cybern. {C}"}
@STRING{IEEE_J_SSC        = "{IEEE} Trans. Syst. Sci. Cybernetics"}



% earth, wind, fire and water
@STRING{IEEE_J_GE         = "{IEEE} Trans. Geosci. Electron."}
@STRING{IEEE_J_GRS        = "{IEEE} Trans. Geosci. Remote Sensing"}
@STRING{IEEE_J_OE         = "{IEEE} J. Oceanic Eng."}



% education, engineering, history, IEEE, professional
% disabled till definition is verified
STRING{IEEE_J_CJECE       = "Canadian J. Elect. Comput. Eng."}
@STRING{IEEE_J_PROC       = "Proc. {IEEE}"}
@STRING{IEEE_J_EDU        = "{IEEE} Trans. Educ."}
@STRING{IEEE_J_EM         = "{IEEE} Trans. Eng. Manage."}
% disabled till definition is verified
STRING{IEEE_J_EWS         = "{IEEE} Trans. Eng. Writing Speech"}
@STRING{IEEE_J_PC         = "{IEEE} Trans. Prof. Commun."}



% electromagnetics, antennas, EMI, magnetics and microwave
@STRING{IEEE_J_AWPL       = "{IEEE} Antennas Wireless Propagat. Lett."}
@STRING{IEEE_J_MGWL       = "{IEEE} Microwave Guided Wave Lett."}
% IEEE seems to want "Compon." here, not "Comp."
@STRING{IEEE_J_MWCL       = "{IEEE} Microwave Wireless Compon. Lett."}
@STRING{IEEE_J_AP         = "{IEEE} Trans. Antennas Propagat."}
@STRING{IEEE_J_EMC        = "{IEEE} Trans. Electromagn. Compat."}
@STRING{IEEE_J_MAG        = "{IEEE} Trans. Magn."}
@STRING{IEEE_J_MTT        = "{IEEE} Trans. Microwave Theory Tech."}
@STRING{IEEE_J_RFI        = "{IEEE} Trans. Radio Freq. Interference"}
@STRING{IEEE_J_TJMJ       = "{IEEE} Transl. J. Magn. Jpn."}



% energy and power
@STRING{IEEE_J_EC         = "{IEEE} Trans. Energy Conversion"}
@STRING{IEEE_J_PWRAS      = "{IEEE} Trans. Power App. Syst."}
@STRING{IEEE_J_PWRD       = "{IEEE} Trans. Power Delivery"}
@STRING{IEEE_J_PWRE       = "{IEEE} Trans. Power Electron."}
@STRING{IEEE_J_PWRS       = "{IEEE} Trans. Power Syst."}



% industrial, commercial and consumer
@STRING{IEEE_J_APPIND     = "{IEEE} Trans. Applicat. Ind."}
@STRING{IEEE_J_BC         = "{IEEE} Trans. Broadcast."}
% disabled till definition is verified
STRING{IEEE_J_BCTV        = "{IEEE} Trans. Broadcast Television Receivers"}
@STRING{IEEE_J_CE         = "{IEEE} Trans. Consumer Electron."}
@STRING{IEEE_J_IE         = "{IEEE} Trans. Ind. Electron."}
@STRING{IEEE_J_IECI       = "{IEEE} Trans. Ind. Electron. Contr. Instrum."}
@STRING{IEEE_J_IA         = "{IEEE} Trans. Ind. Applicat."}
@STRING{IEEE_J_IGA        = "{IEEE} Trans. Ind. Gen. Applicat."}



% instrumentation and measurement
@STRING{IEEE_J_IM         = "{IEEE} Trans. Instrum. Meas."}



% insulation and materials
% disabled till definition finalized
STRING{IEEE_J_JEM         = "{IEEE/TMS} J. Electron. Mater."}
@STRING{IEEE_J_DEI        = "{IEEE} Trans. Dielect. Elect. Insulation"}
@STRING{IEEE_J_EI         = "{IEEE} Trans. Elect. Insulation"}



% mechanical
@STRING{IEEE_J_MECH       = "{IEEE/ASME} Trans. Mechatron."}
@STRING{IEEE_J_MEMS       = "J. Microelectromech. Syst."}



% medical and biological
@STRING{IEEE_J_BME        = "{IEEE} Trans. Biomed. Eng."}
% Note: The B-ME journal later dropped the hyphen and became the BME.
@STRING{IEEE_J_B-ME       = "{IEEE} Trans. Bio-Med. Eng."}
@STRING{IEEE_J_BMELC      = "{IEEE} Trans. Bio-Med. Electron."}
@STRING{IEEE_J_ITBM       = "{IEEE} Trans. Inform. Technol. Biomed."}
@STRING{IEEE_J_ME         = "{IEEE} Trans. Med. Electron."}
@STRING{IEEE_J_MI         = "{IEEE} Trans. Med. Imag."}
% disabled till definition finalized
STRING{IEEE_J_MCTE        = "{IEEE} Trans. Molecular Cellular Tissue Eng."}
@STRING{IEEE_J_NB         = "{IEEE} Trans. Nanobiosci."}
@STRING{IEEE_J_NSRE       = "{IEEE} Trans. Neural Syst. Rehab. Eng."}
@STRING{IEEE_J_RE         = "{IEEE} Trans. Rehab. Eng."}



% optics, lightwave and photonics
@STRING{IEEE_J_PTL        = "{IEEE} Photon. Technol. Lett."}
@STRING{IEEE_J_JLT        = "J. Lightwave Technol."}



% physics, electrons, nanotechnology, nuclear and quantum electronics
@STRING{IEEE_J_EDL        = "{IEEE} Electron Device Lett."}
@STRING{IEEE_J_JQE        = "{IEEE} J. Quantum Electron."}
@STRING{IEEE_J_JSTQE      = "{IEEE} J. Select. Topics Quantum Electron."}
@STRING{IEEE_J_ED         = "{IEEE} Trans. Electron Devices"}
@STRING{IEEE_J_NANO       = "{IEEE} Trans. Nanotechnol."}
@STRING{IEEE_J_NS         = "{IEEE} Trans. Nucl. Sci."}
@STRING{IEEE_J_PS         = "{IEEE} Trans. Plasma Sci."}



% reliability
% IEEE seems to want "Mat." here, not "Mater."
@STRING{IEEE_J_DMR        = "{IEEE} Trans. Device Mat. Rel."}
@STRING{IEEE_J_R          = "{IEEE} Trans. Rel."}



% semiconductors, superconductors, electrochemical and solid state
% disabled till definition finalized
STRING{IEEE_J_ESSL        = "{IEEE/ECS} Electrochemical Solid-State Lett."}
@STRING{IEEE_J_JSSC       = "{IEEE} J. Solid-State Circuits"}
@STRING{IEEE_J_ASC        = "{IEEE} Trans. Appl. Superconduct."}
@STRING{IEEE_J_SM         = "{IEEE} Trans. Semiconduct. Manufact."}



% sensors
@STRING{IEEE_J_SENSOR     = "{IEEE} Sensors J."}



% VLSI
@STRING{IEEE_J_VLSI       = "{IEEE} Trans. {VLSI} Syst."}





%%%%%%%%%%%%%%%%%%%%
%% IEEE Magazines %%
%%%%%%%%%%%%%%%%%%%%


@STRING{IEEE_M_AES        = "{IEEE} Aerosp. Electron. Syst. Mag"}
@STRING{IEEE_M_HIST       = "{IEEE} Annals Hist. Comput."}
@STRING{IEEE_M_AP         = "{IEEE} Antennas Propagat. Mag."}
@STRING{IEEE_M_ASSP       = "{IEEE} {ASSP} Mag."}
@STRING{IEEE_M_CD         = "{IEEE} Circuits Devices Mag."}
@STRING{IEEE_M_CAS        = "{IEEE} Circuits Syst. Mag."}
@STRING{IEEE_M_COM        = "{IEEE} Commun. Mag."}
@STRING{IEEE_M_COMSOC     = "{IEEE} Commun. Soc. Mag."}
% CSEM changed to CSE in 1999
@STRING{IEEE_M_CSE        = "{IEEE} Comput. Sci. Eng."}
@STRING{IEEE_M_CSEM       = "{IEEE} Comput. Sci. Eng. Mag."}
@STRING{IEEE_M_C          = "{IEEE} Computer"}
@STRING{IEEE_M_CAP        = "{IEEE} Comput. Appl. Power"}
@STRING{IEEE_M_CGA        = "{IEEE} Comput. Graph. Appl."}
@STRING{IEEE_M_CONC       = "{IEEE} Concurrency"}
@STRING{IEEE_M_CS         = "{IEEE} Control Syst. Mag."}
@STRING{IEEE_M_DTC        = "{IEEE} Des. Test. Comput."}
@STRING{IEEE_M_EI         = "{IEEE} Electr. Insul. Mag."}
@STRING{IEEE_M_EMB        = "{IEEE} Eng. Med. Biol. Mag."}
@STRING{IEEE_M_EMR        = "{IEEE} Eng. Manag. Rev."}
@STRING{IEEE_M_EXP        = "{IEEE} Expert"}
@STRING{IEEE_M_IA         = "{IEEE} Ind. Appl. Mag."}
@STRING{IEEE_M_IM         = "{IEEE} Instrum. Meas. Mag."}
@STRING{IEEE_M_IS         = "{IEEE} Intell. Syst."}
@STRING{IEEE_M_IC         = "{IEEE} Internet Comput."}
@STRING{IEEE_M_ITP        = "{IEEE} {IT} Prof."}
@STRING{IEEE_M_MICRO      = "{IEEE} Micro"}
@STRING{IEEE_M_MW         = "{IEEE} Microwave"}
@STRING{IEEE_M_MM         = "{IEEE} Multimedia"}
@STRING{IEEE_M_NET        = "{IEEE} Network"}
% IEEE's editorial manual lists "Pers. Commun.", 
% but "Personal Commun. Mag." seems to be what is used in the journals
@STRING{IEEE_M_PCOM       = "{IEEE} Personal Commun. Mag."}
@STRING{IEEE_M_POT        = "{IEEE} Potentials"}
% CAP and PER merged to form PE in 2003
@STRING{IEEE_M_PE         = "{IEEE} Power Energy Mag."}
@STRING{IEEE_M_PER        = "{IEEE} Power Eng. Rev."}
@STRING{IEEE_M_RA         = "{IEEE} Robot. Automat. Mag."}
@STRING{IEEE_M_SP         = "{IEEE} Signal Processing Mag."}
@STRING{IEEE_M_S          = "{IEEE} Softw."}
@STRING{IEEE_M_SPECT      = "{IEEE} Spectr."}
@STRING{IEEE_M_TS         = "{IEEE} Technol. Soc. Mag."}
@STRING{IEEE_M_WC         = "{IEEE} Wireless Commun. Mag."}
@STRING{IEEE_M_TODAY      = "Today's Eng."}


@InProceedings{afham2022crosspoint,
    author    = {Afham, Mohamed and Dissanayake, Isuru and Dissanayake, Dinithi and Dharmasiri, Amaya and Thilakarathna, Kanchana and Rodrigo, Ranga},
    title     = {CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {9902-9912}
}

@InProceedings{tang2022cbl,
    author    = {Tang, Liyao and Zhan, Yibing and Chen, Zhe and Yu, Baosheng and Tao, Dacheng},
    title     = {Contrastive Boundary Learning for Point Cloud Segmentation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {8489-8499}
}

@inproceedings{hou2021exploring,
  title={Exploring data-efficient 3d scene understanding with contrastive scene contexts},
  author={Hou, Ji and Graham, Benjamin and Nie{\ss}ner, Matthias and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15587--15597},
  year={2021}
}

@article{nunes2022ral,
    author = {L. Nunes and R. Marcuzzi and X. Chen and J. Behley and C. Stachniss},
    title = {{SegContrast: 3D Point Cloud Feature Representation Learning through Self-supervised Segment Discrimination}},
    journal = {ral},
    year = 2022,
    doi = {10.1109/LRA.2022.3142440},
    issn = {2377-3766},
    volume = {7},
    number = {2},
    pages = {2116-2123},
    url = {http://www.ipb.uni-bonn.de/pdfs/nunes2022ral-icra.pdf},
}

@InProceedings{Eckart_2021_CVPR,
    author    = {Eckart, Benjamin and Yuan, Wentao and Liu, Chao and Kautz, Jan},
    title     = {Self-Supervised Learning on 3D Point Clouds by Learning Discrete Generative Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {8248-8257}
}

@InProceedings{yu2021pointr,
    author    = {Yu, Xumin and Rao, Yongming and Wang, Ziyi and Liu, Zuyan and Lu, Jiwen and Zhou, Jie},
    title     = {PoinTr: Diverse Point Cloud Completion With Geometry-Aware Transformers},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {12498-12507}
}

@InProceedings{Wang_2021_ICCV,
    author    = {Wang, Hanchen and Liu, Qi and Yue, Xiangyu and Lasenby, Joan and Kusner, Matt J.},
    title     = {Unsupervised Point Cloud Pre-Training via Occlusion Completion},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {9782-9792}
}

@InProceedings{pointcontrast,
    author="Xie, Saining
    and Gu, Jiatao
    and Guo, Demi
    and Qi, Charles R.
    and Guibas, Leonidas
    and Litany, Or",
    editor="Vedaldi, Andrea
    and Bischof, Horst
    and Brox, Thomas
    and Frahm, Jan-Michael",
    title="PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding",
    booktitle="Computer Vision -- ECCV 2020",
    year="2020",
    publisher="Springer International Publishing",
    address="Cham",
    pages="574--591",
    isbn="978-3-030-58580-8"
}

@inproceedings{sharma20ctree,
    author = {Sharma, Charu and Kaul, Manohar},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
    pages = {7212--7221},
    publisher = {Curran Associates, Inc.},
    title = {Self-Supervised Few-Shot Learning on Point Clouds},
    url = {https://proceedings.neurips.cc/paper/2020/file/50c1f44e426560f3f2cdcb3e19e39903-Paper.pdf},
    volume = {33},
    year = {2020}
}

@inproceedings{pang2022pointmae,
    title={Masked Autoencoders for Point Cloud Self-supervised Learning},
    author={Yatian Pang and Wenxiao Wang and Francis E. H. Tay and Wei Liu and Yonghong Tian and Li Yuan},
    booktitle="Computer Vision -- ECCV 2022",
    year={2022},
    publisher="Springer International Publishing",
}

@inproceedings{sauder19jigsaw,
    author = {Sauder, Jonathan and Sievers, Bjarne},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Self-Supervised Deep Learning on Point Clouds by Reconstructing Space},
    url = {https://proceedings.neurips.cc/paper/2019/file/993edc98ca87f7e08494eec37fa836f7-Paper.pdf},
    volume = {32},
    year = {2019}
}

@InProceedings{depthcontrast,
    author    = {Zhang, Zaiwei and Girdhar, Rohit and Joulin, Armand and Misra, Ishan},
    title     = {Self-Supervised Pretraining of 3D Features on Any Point-Cloud},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {10252-10263}
}

@InProceedings{yang18foldingnet,
    author = {Yang, Yaoqing and Feng, Chen and Shen, Yiru and Tian, Dong},
    title = {FoldingNet: Point Cloud Auto-Encoder via Deep Grid Deformation},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2018}
}

@inproceedings{NIPS2016_44f683a8,
    author = {Wu, Jiajun and Zhang, Chengkai and Xue, Tianfan and Freeman, Bill and Tenenbaum, Josh},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling},
    url = {https://proceedings.neurips.cc/paper/2016/file/44f683a84163b3523afe57c2e008bc8c-Paper.pdf},
    volume = {29},
    year = {2016}
}

@inproceedings{Han19inter-prediction,
    author = {Han, Zhizhong and Shang, Mingyang and Liu, Yu-Shen and Zwicker, Matthias},
    title = {View Inter-Prediction GAN: Unsupervised Representation Learning for 3D Shapes by Learning Global Shape Memories to Support Local View Predictions},
    year = {2019},
    isbn = {978-1-57735-809-1},
    publisher = {AAAI Press},
    url = {https://doi.org/10.1609/aaai.v33i01.33018376},
    doi = {10.1609/aaai.v33i01.33018376},
    booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
    articleno = {1027},
    numpages = {9},
    location = {Honolulu, Hawaii, USA},
    series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{fan2021embracing,
    title={Embracing Single Stride 3D Object Detector with Sparse Transformer},
    author={Fan, Lue and Pang, Ziqi and Zhang, Tianyuan and Wang, Yu-Xiong and Zhao, Hang and Wang, Feng and Wang, Naiyan and Zhang, Zhaoxiang},
    booktitle={Proceedings of the {IEEE/CVF} Conference on Computer Vision and Pattern Recognition (CVPR)},
    month={June},
    year={2021}
}

@inproceedings{park2022fast,
    title={Fast Point Transformer},
    author={Park, Chunghyun and Jeong, Yoonwoo and Cho, Minsu and Park, Jaesik},
    booktitle={Proceedings of the {IEEE/CVF} Conference on Computer Vision and Pattern Recognition (CVPR)},
    month={June},
    year={2022},
    pages={16949-16958}
}

@InProceedings{misra213detr,
    author    = {Misra, Ishan and Girdhar, Rohit and Joulin, Armand},
    title     = {An End-to-End Transformer Model for 3D Object Detection},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {2906-2917}
}

@InProceedings{zhao21pt,
    author    = {Zhao, Hengshuang and Jiang, Li and Jia, Jiaya and Torr, Philip H.S. and Koltun, Vladlen},
    title     = {Point Transformer},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {16259-16268}
}

@InProceedings{mao21votr,
    author    = {Mao, Jiageng and Xue, Yujing and Niu, Minzhe and Bai, Haoyue and Feng, Jiashi and Liang, Xiaodan and Xu, Hang and Xu, Chunjing},
    title     = {Voxel Transformer for 3D Object Detection},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {3164-3173}
}

@InProceedings{pan21pointformer,
    author    = {Pan, Xuran and Xia, Zhuofan and Song, Shiji and Li, Li Erran and Huang, Gao},
    title     = {3D Object Detection With Pointformer},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {7463-7472}
}

@InProceedings{sheng21ctformer,
    author    = {Sheng, Hualian and Cai, Sijia and Liu, Yuan and Deng, Bing and Huang, Jianqiang and Hua, Xian-Sheng and Zhao, Min-Jian},
    title     = {Improving 3D Object Detection With Channel-Wise Transformer},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {2743-2752}
}

@misc{guo2020pct,
    title={PCT: Point Cloud Transformer}, 
    author={Meng-Hao Guo and Jun-Xiong Cai and Zheng-Ning Liu and Tai-Jiang Mu and Ralph R. Martin and Shi-Min Hu},
    year={2020},
    eprint={2012.09688},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@InProceedings{liu21group,
    author    = {Liu, Ze and Zhang, Zheng and Cao, Yue and Hu, Han and Tong, Xin},
    title     = {Group-Free 3D Object Detection via Transformers},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {2949-2958}
}

@InProceedings{jaegle21perceiver,
  title = 	 {Perceiver: General Perception with Iterative Attention},
  author =       {Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4651--4664},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/jaegle21a/jaegle21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/jaegle21a.html},
  abstract = 	 {Biological systems understand the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver {–} a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.}
}

@inproceedings{jaegle22perceiverio,
    title={Perceiver {IO}: A General Architecture for Structured Inputs \& Outputs},
    author={Andrew Jaegle and Sebastian Borgeaud and Jean-Baptiste Alayrac and Carl Doersch and Catalin Ionescu and David Ding and Skanda Koppula and Daniel Zoran and Andrew Brock and Evan Shelhamer and Olivier J Henaff and Matthew Botvinick and Andrew Zisserman and Oriol Vinyals and Joao Carreira},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=fILj7WpI-g}
}

@article{shapenet2015,
    author    = {Angel X. Chang and
                Thomas A. Funkhouser and
                Leonidas J. Guibas and
                Pat Hanrahan and
                Qi{-}Xing Huang and
                Zimo Li and
                Silvio Savarese and
                Manolis Savva and
                Shuran Song and
                Hao Su and
                Jianxiong Xiao and
                Li Yi and
                Fisher Yu},
    title     = {ShapeNet: An Information-Rich 3D Model Repository},
    journal   = {CoRR},
    volume    = {abs/1512.03012},
    year      = {2015},
    url       = {http://arxiv.org/abs/1512.03012},
    eprinttype = {arXiv},
    eprint    = {1512.03012},
    timestamp = {Thu, 08 Apr 2021 11:37:55 +0200},
    biburl    = {https://dblp.org/rec/journals/corr/ChangFGHHLSSSSX15.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{wang19dgcnn,
    title={Dynamic Graph CNN for Learning on Point Clouds},
    author={Wang, Yue and Sun, Yongbin and Liu, Ziwei and Sarma, Sanjay E. and Bronstein, Michael M. and Solomon, Justin M.},
    journal={ACM Transactions on Graphics (TOG)},
    year={2019}
}

@InProceedings{qi17pointnet,
    author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
    title = {PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {July},
    year = {2017}
}

@inproceedings{qi17pointnet2,
    author = {Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},
    url = {https://proceedings.neurips.cc/paper/2017/file/d8bf84be3800d12f74d8b05e9b89836f-Paper.pdf},
    volume = {30},
    year = {2017}
}

@inproceedings{ma2022rethinking,
    title={Rethinking Network Design and Local Geometry in Point Cloud: A Simple Residual {MLP} Framework},
    author={Xu Ma and Can Qin and Haoxuan You and Haoxi Ran and Yun Fu},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=3Pbra-_u76D}
}

@InProceedings{wu15modelnet,
    author = {Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong},
    title = {3D ShapeNets: A Deep Representation for Volumetric Shapes},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2015}
}

@InProceedings{Dai_2017_CVPR,
    author = {Dai, Angela and Chang, Angel X. and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Niessner, Matthias},
    title = {ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {July},
    year = {2017}
}

@misc{armeni2017joint,
    title={Joint 2D-3D-Semantic Data for Indoor Scene Understanding}, 
    author={Iro Armeni and Sasha Sax and Amir R. Zamir and Silvio Savarese},
    year={2017},
    eprint={1702.01105},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@InProceedings{Uy_2019_ICCV,
    author = {Uy, Mikaela Angelina and Pham, Quang-Hieu and Hua, Binh-Son and Nguyen, Thanh and Yeung, Sai-Kit},
    title = {Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month = {October},
    year = {2019}
}

@InProceedings{He_2016_CVPR,
    author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    title = {Deep Residual Learning for Image Recognition},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2016}
}

@InProceedings{Huang_2017_CVPR,
    author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
    title = {Densely Connected Convolutional Networks},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {July},
    year = {2017}
}

@InProceedings{radford21clip,
    title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
    author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
    booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
    pages = 	 {8748--8763},
    year = 	 {2021},
    editor = 	 {Meila, Marina and Zhang, Tong},
    volume = 	 {139},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {18--24 Jul},
    publisher =    {PMLR},
    pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
    url = 	 {https://proceedings.mlr.press/v139/radford21a.html},
    abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}

@inproceedings{chen2020uniter,
    title={Uniter: Universal image-text representation learning},
    author={Chen, Yen-Chun and Li, Linjie and Yu, Licheng and Kholy, Ahmed El and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
    booktitle={ECCV},
    year={2020}
}

@InProceedings{Wang_2020_CVPR,
    author = {Wang, Weiyao and Tran, Du and Feiszli, Matt},
    title = {What Makes Training Multi-Modal Classification Networks Hard?},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2020}
}

@InProceedings{Arandjelovi18objects,
    author="Arandjelovi{\'{c}}, Relja
    and Zisserman, Andrew",
    editor="Ferrari, Vittorio
    and Hebert, Martial
    and Sminchisescu, Cristian
    and Weiss, Yair",
    title="Objects that Sound",
    booktitle="Computer Vision -- ECCV 2018",
    year="2018",
    publisher="Springer International Publishing",
    address="Cham",
    pages="451--466",
    isbn="978-3-030-01246-5"
}

@inproceedings{Alayrac20self,
 author = {Alayrac, Jean-Baptiste and Recasens, Adria and Schneider, Rosalia and Arandjelovi\'{c}, Relja and Ramapuram, Jason and De Fauw, Jeffrey and Smaira, Lucas and Dieleman, Sander and Zisserman, Andrew},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {25--37},
 publisher = {Curran Associates, Inc.},
 title = {Self-Supervised MultiModal Versatile Networks},
 url = {https://proceedings.neurips.cc/paper/2020/file/0060ef47b12160b9198302ebdb144dcf-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{wu16learning,
    author = {Wu, Jiajun and Zhang, Chengkai and Xue, Tianfan and Freeman, Bill and Tenenbaum, Josh},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling},
    url = {https://proceedings.neurips.cc/paper/2016/file/44f683a84163b3523afe57c2e008bc8c-Paper.pdf},
    volume = {29},
    year = {2016}
}

@InProceedings{achlioptas18learning,
  title = 	 {Learning Representations and Generative Models for 3{D} Point Clouds},
  author =       {Achlioptas, Panos and Diamanti, Olga and Mitliagkas, Ioannis and Guibas, Leonidas},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {40--49},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/achlioptas18a/achlioptas18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/achlioptas18a.html},
  abstract = 	 {Three-dimensional geometric data offer an excellent domain for studying representation learning and generative modeling. In this paper, we look at geometric data represented as point clouds. We introduce a deep AutoEncoder (AE) network with state-of-the-art reconstruction quality and generalization ability. The learned representations outperform existing methods on 3D recognition tasks and enable shape editing via simple algebraic manipulations, such as semantic part editing, shape analogies and shape interpolation, as well as shape completion. We perform a thorough study of different generative models including GANs operating on the raw point clouds, significantly improved GANs trained in the fixed latent space of our AEs, and Gaussian Mixture Models (GMMs). To quantitatively evaluate generative models we introduce measures of sample fidelity and diversity based on matchings between sets of point clouds. Interestingly, our evaluation of generalization, fidelity and diversity reveals that GMMs trained in the latent space of our AEs yield the best results overall.}
}

@inproceedings{han19url,
    author = {Han, Zhizhong and Shang, Mingyang and Liu, Yu-Shen and Zwicker, Matthias},
    title = {View Inter-Prediction GAN: Unsupervised Representation Learning for 3D Shapes by Learning Global Shape Memories to Support Local View Predictions},
    year = {2019},
    isbn = {978-1-57735-809-1},
    publisher = {AAAI Press},
    url = {https://doi.org/10.1609/aaai.v33i01.33018376},
    doi = {10.1609/aaai.v33i01.33018376},
    abstract = {In this paper, we present a novel unsupervised representation learning approach for 3D shapes, which is an important research challenge as it avoids the manual effort required for collecting supervised data. Our method trains an RNN-based neural network architecture to solve multiple view inter-prediction tasks for each shape. Given several nearby views of a shape, we define view inter-prediction as the task of predicting the center view between the input views, and reconstructing the input views in a low-level feature space. The key idea of our approach is to implement the shape representation as a shape-specific global memory that is shared between all local view inter-predictions for each shape. Intuitively, this memory enables the system to aggregate information that is useful to better solve the view inter-prediction tasks for each shape, and to leverage the memory as a view-independent shape representation. Our approach obtains the best results using a combination of L2 and adversarial losses for the view inter-prediction task. We show that VIP-GAN outperforms state-of-the-art methods in unsupervised 3D feature learning on three large-scale 3D shape benchmarks.},
    booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
    articleno = {1027},
    numpages = {9},
    location = {Honolulu, Hawaii, USA},
    series = {AAAI'19/IAAI'19/EAAI'19}
}

@InProceedings{chen20simclr,
  title = 	 {A Simple Framework for Contrastive Learning of Visual Representations},
  author =       {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1597--1607},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/chen20j/chen20j.pdf},
  url = 	 {https://proceedings.mlr.press/v119/chen20j.html},
  abstract = 	 {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.}
}

@inproceedings{loshchilov2018decoupled,
    title={Decoupled Weight Decay Regularization},
    author={Ilya Loshchilov and Frank Hutter},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@misc{Katsura21cawwr,
    title={PyTorch CosineAnnealing with Warmup Restarts},
    author={Naoki Katsura},
    howpublished = {\url{https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup}},
    year={2021}
}

@InProceedings{yu2021pointbert,
    author    = {Yu, Xumin and Tang, Lulu and Rao, Yongming and Huang, Tiejun and Zhou, Jie and Lu, Jiwen},
    title     = {Point-BERT: Pre-Training 3D Point Cloud Transformers With Masked Point Modeling},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {19313-19322}
}

@InProceedings{kim21vilt,
    title = 	 {ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision},
    author =       {Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
    booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
    pages = 	 {5583--5594},
    year = 	 {2021},
    editor = 	 {Meila, Marina and Zhang, Tong},
    volume = 	 {139},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {18--24 Jul},
    publisher =    {PMLR},
    pdf = 	 {http://proceedings.mlr.press/v139/kim21k/kim21k.pdf},
    url = 	 {http://proceedings.mlr.press/v139/kim21k.html},
    abstract = 	 {Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks. Current approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more computation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary. In this paper, we present a minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance. Our code and pre-trained weights are available at https://github.com/dandelin/vilt.}
}

@article{shapenetpart,
    author = {Yi, Li and Kim, Vladimir G. and Ceylan, Duygu and Shen, I-Chao and Yan, Mengyan and Su, Hao and Lu, Cewu and Huang, Qixing and Sheffer, Alla and Guibas, Leonidas},
    title = {A Scalable Active Framework for Region Annotation in 3D Shape Collections},
    year = {2016},
    issue_date = {November 2016},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {35},
    number = {6},
    issn = {0730-0301},
    url = {https://doi.org/10.1145/2980179.2980238},
    doi = {10.1145/2980179.2980238},
    abstract = {Large repositories of 3D shapes provide valuable input for data-driven analysis and modeling tools. They are especially powerful once annotated with semantic information such as salient regions and functional parts. We propose a novel active learning method capable of enriching massive geometric datasets with accurate semantic region annotations. Given a shape collection and a user-specified region label our goal is to correctly demarcate the corresponding regions with minimal manual work. Our active framework achieves this goal by cycling between manually annotating the regions, automatically propagating these annotations across the rest of the shapes, manually verifying both human and automatic annotations, and learning from the verification results to improve the automatic propagation algorithm. We use a unified utility function that explicitly models the time cost of human input across all steps of our method. This allows us to jointly optimize for the set of models to annotate and for the set of models to verify based on the predicted impact of these actions on the human efficiency. We demonstrate that incorporating verification of all produced labelings within this unified objective improves both accuracy and efficiency of the active learning procedure. We automatically propagate human labels across a dynamic shape network using a conditional random field (CRF) framework, taking advantage of global shape-to-shape similarities, local feature similarities, and point-to-point correspondences. By combining these diverse cues we achieve higher accuracy than existing alternatives. We validate our framework on existing benchmarks demonstrating it to be significantly more efficient at using human input compared to previous techniques. We further validate its efficiency and robustness by annotating a massive shape dataset, labeling over 93,000 shape parts, across multiple model classes, and providing a labeled part collection more than one order of magnitude larger than existing ones.},
    journal = {ACM Trans. Graph.},
    month = {nov},
    articleno = {210},
    numpages = {12},
    keywords = {shape analysis, active learning}
}

@article{tsne,
    author  = {Laurens van der Maaten and Geoffrey Hinton},
    title   = {Visualizing Data using t-SNE},
    journal = {Journal of Machine Learning Research},
    year    = {2008},
    volume  = {9},
    number  = {86},
    pages   = {2579--2605},
    url     = {http://jmlr.org/papers/v9/vandermaaten08a.html}
}

@inproceedings{ma22pointmlp,
    title={Rethinking Network Design and Local Geometry in Point Cloud: A Simple Residual {MLP} Framework},
    author={Xu Ma and Can Qin and Haoxuan You and Haoxi Ran and Yun Fu},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=3Pbra-_u76D}
}

@InProceedings{xiang21curvenet,
    author    = {Xiang, Tiange and Zhang, Chaoyi and Song, Yang and Yu, Jianhui and Cai, Weidong},
    title     = {Walk in the Cloud: Learning Curves for Point Clouds Shape Analysis},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {915-924}
}

@article{hu19randlanet,
    title={RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds},
    author={Hu, Qingyong and Yang, Bo and Xie, Linhai and Rosa, Stefano and Guo, Yulan and Wang, Zhihua and Trigoni, Niki and Markham, Andrew},
    journal={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
    year={2020}
}

@InProceedings{qi19votenet,
    author = {Qi, Charles R. and Litany, Or and He, Kaiming and Guibas, Leonidas J.},
    title = {Deep Hough Voting for 3D Object Detection in Point Clouds},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month = {October},
    year = {2019}
}

@InProceedings{zhou18voxelnet,
    author = {Zhou, Yin and Tuzel, Oncel},
    title = {VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2018}
}

@InProceedings{yang18pixor,
    author = {Yang, Bin and Luo, Wenjie and Urtasun, Raquel},
    title = {PIXOR: Real-Time 3D Object Detection From Point Clouds},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2018}
}

@inproceedings{bao2022beit,
    title={{BE}iT: {BERT} Pre-Training of Image Transformers},
    author={Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=p-BhZSz59o4}
}

@InProceedings{he2022mae,
    author    = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll\'ar, Piotr and Girshick, Ross},
    title     = {Masked Autoencoders Are Scalable Vision Learners},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {16000-16009}
}

@InProceedings{he2016resnet,
    author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    title = {Deep Residual Learning for Image Recognition},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2016}
}

@inproceedings{carion20detr,
    author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
    title = {End-to-End Object Detection with Transformers},
    year = {2020},
    isbn = {978-3-030-58451-1},
    publisher = {Springer-Verlag},
    address = {Berlin, Heidelberg},
    url = {https://doi.org/10.1007/978-3-030-58452-8_13},
    doi = {10.1007/978-3-030-58452-8_13},
    abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at .},
    booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I},
    pages = {213–229},
    numpages = {17},
    location = {Glasgow, United Kingdom}
}

@inproceedings{dosovitskiy2021vit,
    title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
    author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{vaswani17transformer,
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Attention is All you Need},
    url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
    volume = {30},
    year = {2017}
}

@inproceedings{devlin19bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{radford2019gpt2,
    title={Language Models are Unsupervised Multitask Learners},
    author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
    year={2019}
}

@inproceedings{brown20gpt3,
    author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
    pages = {1877--1901},
    publisher = {Curran Associates, Inc.},
    title = {Language Models are Few-Shot Learners},
    url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
    volume = {33},
    year = {2020}
}

@InProceedings{wang22brt,
    author    = {Wang, Yikai and Ye, TengQi and Cao, Lele and Huang, Wenbing and Sun, Fuchun and He, Fengxiang and Tao, Dacheng},
    title     = {Bridged Transformer for Vision and Point Cloud 3D Object Detection},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {12114-12123}
}

@inproceedings{xu19disn,
    author = {Xu, Qiangeng and Wang, Weiyue and Ceylan, Duygu and Mech, Radomir and Neumann, Ulrich},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction},
    url = {https://proceedings.neurips.cc/paper/2019/file/39059724f73a9969845dfe4146c5660e-Paper.pdf},
    volume = {32},
    year = {2019}
}

@inproceedings{desai21vertex,
	abstract = {The de-facto approach to many vision tasks is to start from pretrained visual representations, typically learned via supervised training on ImageNet. Recent methods have explored unsupervised pretraining to scale to vast quantities of unlabeled images. In contrast, we aim to learn high-quality visual representations from fewer images. To this end we revisit supervised pretraining, and seek dataefficient alternatives to classification-based pretraining. We propose VirTex -- a pretraining approach using semantically dense captions to learn visual representations. We train convolutional networks from scratch on COCO Captions, and transfer them to downstream recognition tasks including image classification, object detection, and instance segmentation. On all tasks, VirTex yields features that match or exceed those learned on ImageNet -- supervised or unsupervised -- despite using up to ten times fewer images.},
	address = {Nashville, TN, USA},
	author = {Desai, Karan and Johnson, Justin},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	date-added = {2022-09-12 14:50:13 +0800},
	date-modified = {2022-09-12 14:50:13 +0800},
	doi = {10.1109/CVPR46437.2021.01101},
	isbn = {978-1-66544-509-2},
	language = {en},
	month = jun,
	pages = {11157--11168},
	publisher = {IEEE},
	shorttitle = {{VirTex}},
	title = {{VirTex}: {Learning} {Visual} {Representations} from {Textual} {Annotations}},
	url = {https://ieeexplore.ieee.org/document/9577368/},
	urldate = {2022-09-12},
	year = {2021},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/9577368/},
	bdsk-url-2 = {https://doi.org/10.1109/CVPR46437.2021.01101}
}

@incollection{sariyildiz20learning,
	abstract = {Pretraining general-purpose visual features has become a crucial part of tackling many computer vision tasks. While one can learn such features on the extensively-annotated ImageNet dataset, recent approaches have looked at ways to allow for noisy, fewer, or even no annotations to perform such pretraining. Starting from the observation that captioned images are easily crawlable, we argue that this overlooked source of information can be exploited to supervise the training of visual representations. To do so, motivated by the recent progresses in language models, we introduce image-conditioned masked language modeling (ICMLM) --a proxy task to learn visual representations over image-caption pairs. ICMLM consists in predicting masked words in captions by relying on visual cues. To tackle this task, we propose hybrid models, with dedicated visual and textual encoders, and we show that the visual representations learned as a by-product of solving this task transfer well to a variety of target tasks. Our experiments confirm that image captions can be leveraged to inject global and localized semantic information into visual representations. Project website: https://europe.naverlabs.com/ICMLM.},
	address = {Cham},
	author = {Sariyildiz, Mert Bulent and Perez, Julien and Larlus, Diane},
	booktitle = {Computer {Vision} -- {ECCV} 2020},
	date-added = {2022-09-12 14:50:52 +0800},
	date-modified = {2022-09-12 14:50:52 +0800},
	doi = {10.1007/978-3-030-58598-3_10},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	isbn = {978-3-030-58597-6 978-3-030-58598-3},
	language = {en},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {153--170},
	publisher = {Springer International Publishing},
	title = {Learning {Visual} {Representations} with {Caption} {Annotations}},
	url = {https://link.springer.com/10.1007/978-3-030-58598-3_10},
	urldate = {2022-09-12},
	volume = {12353},
	year = {2020},
	bdsk-url-1 = {https://link.springer.com/10.1007/978-3-030-58598-3_10},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-58598-3_10}
}

@incollection{wwens18audio,
	abstract = {The thud of a bouncing ball, the onset of speech as lips open --- when visual and audio events occur together, it suggests that there might be a common, underlying event that produced both signals. In this paper, we argue that the visual and audio components of a video signal should be modeled jointly using a fused multisensory representation. We propose to learn such a representation in a self-supervised way, by training a neural network to predict whether video frames and audio are temporally aligned. We use this learned representation for three applications: (a) sound source localization, i.e. visualizing the source of sound in a video; (b) audio-visual action recognition; and (c) on/offscreen audio source separation, e.g. removing the off-screen translator's voice from a foreign official's speech. Code, models, and video results are available on our webpage: http://andrewowens.com/multisensory.},
	address = {Cham},
	author = {Owens, Andrew and Efros, Alexei A.},
	booktitle = {Computer {Vision} -- {ECCV} 2018},
	date-added = {2022-09-12 14:55:54 +0800},
	date-modified = {2022-09-12 14:55:54 +0800},
	doi = {10.1007/978-3-030-01231-1_39},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	isbn = {978-3-030-01230-4 978-3-030-01231-1},
	language = {en},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {639--658},
	publisher = {Springer International Publishing},
	title = {Audio-{Visual} {Scene} {Analysis} with {Self}-{Supervised} {Multisensory} {Features}},
	url = {http://link.springer.com/10.1007/978-3-030-01231-1_39},
	urldate = {2022-09-12},
	volume = {11210},
	year = {2018},
	bdsk-url-1 = {http://link.springer.com/10.1007/978-3-030-01231-1_39},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-01231-1_39}
}

@inproceedings{morgado21audio,
	abstract = {We present a self-supervised learning approach to learn audio-visual representations from video and audio. Our method uses contrastive learning for cross-modal discrimination of video from audio and vice-versa. We show that optimizing for cross-modal discrimination, rather than withinmodal discrimination, is important to learn good representations from video and audio. With this simple but powerful insight, our method achieves highly competitive performance when finetuned on action recognition tasks. Furthermore, while recent work in contrastive learning defines positive and negative samples as individual instances, we generalize this definition by exploring cross-modal agreement. We group together multiple instances as positives by measuring their similarity in both the video and audio feature spaces. Cross-modal agreement creates better positive and negative sets, which allows us to calibrate visual similarities by seeking within-modal discrimination of positive instances, and achieve significant gains on downstream tasks.},
	address = {Nashville, TN, USA},
	author = {Morgado, Pedro and Vasconcelos, Nuno and Misra, Ishan},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	date-added = {2022-09-12 14:55:54 +0800},
	date-modified = {2022-09-12 14:55:54 +0800},
	doi = {10.1109/CVPR46437.2021.01229},
	file = {Morgado 等。 - 2021 - Audio-Visual Instance Discrimination with Cross-Mo.pdf:/Users/wangyongcai/Zotero/storage/U6TZC2IY/Morgado 等。 - 2021 - Audio-Visual Instance Discrimination with Cross-Mo.pdf:application/pdf},
	isbn = {978-1-66544-509-2},
	language = {en},
	month = jun,
	pages = {12470--12481},
	publisher = {IEEE},
	title = {Audio-{Visual} {Instance} {Discrimination} with {Cross}-{Modal} {Agreement}},
	url = {https://ieeexplore.ieee.org/document/9578129/},
	urldate = {2022-09-12},
	year = {2021},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/9578129/},
	bdsk-url-2 = {https://doi.org/10.1109/CVPR46437.2021.01229}
}

@techreport{jing22contrastive,
	abstract = {Image and Point Clouds provide different information for robots. Finding the correspondences between data from different sensors is crucial for various tasks such as localization, mapping, and navigation. Learning-based descriptors have been developed for single sensors; there is little work on cross-modal features. This work treats learning cross-modal features as a dense contrastive learning problem. We propose a Tuple-Circle loss function for cross-modality feature learning. Furthermore, to learn good features and not lose generality, we developed a variant of widely used PointNet++ architecture for point cloud and U-Net CNN architecture for images. Moreover, we conduct experiments on a real-world dataset to show the effectiveness of our loss function and network structure. We show that our models indeed learn information from both images as well as LiDAR by visualizing the features.},
	annote = {Comment: accepted in CASE2022},
	author = {Jiang, Peng and Saripalli, Srikanth},
	date-added = {2022-09-12 14:36:41 +0800},
	date-modified = {2022-09-12 14:36:41 +0800},
	doi = {10.48550/arXiv.2206.12071},
	institution = {arXiv},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	month = jun,
	note = {arXiv:2206.12071 [cs] type: article},
	number = {arXiv:2206.12071},
	title = {Contrastive {Learning} of {Features} between {Images} and {LiDAR}},
	url = {http://arxiv.org/abs/2206.12071},
	urldate = {2022-09-12},
	year = {2022},
	bdsk-url-1 = {http://arxiv.org/abs/2206.12071},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.2206.12071}
}

@InProceedings{ronneberger15unet,
    author="Ronneberger, Olaf
    and Fischer, Philipp
    and Brox, Thomas",
    editor="Navab, Nassir
    and Hornegger, Joachim
    and Wells, William M.
    and Frangi, Alejandro F.",
    title="U-Net: Convolutional Networks for Biomedical Image Segmentation",
    booktitle="Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
    year="2015",
    publisher="Springer International Publishing",
    address="Cham",
    pages="234--241",
    abstract="There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.",
    isbn="978-3-319-24574-4"
}
%--
%EOF