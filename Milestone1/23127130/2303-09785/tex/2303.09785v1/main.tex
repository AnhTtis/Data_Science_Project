\documentclass{article}

\usepackage{arxiv}
\usepackage{mathtools}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[mathcal]{eucal}
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\usepackage[ruled,vlined]{algorithm2e}  
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[RO]{A PREPRINT - \today}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}
  
%% Title
\title{ABAW : Facial Expression Recognition in the wild}

\author{
  Dr. Darshan Gera \\
  SSSIHL, Brindavan Campus \\
  Bengaluru, Karnataka, India\\
  \texttt{darshangera@sssihl.edu.in} \\
  %% examples of more authors
  \And
  Badveeti Naveen Siva Kumar\\
  SSSIHL, Prasanthi Nilayam Campus \\
  Sri Sathya Sai District, Andhra Pradesh, India \\
  \texttt{bnaveensivakumar@gmail.com} \\
  \AND
 Bobbili Veerendra Raj Kumar\\
  SSSIHL, Prasanthi Nilayam Campus \\
  Sri Sathya Sai District, Andhra Pradesh, India \\
  \texttt{veerendra.rajkumar@gmail.com} \\
  \And
  Dr. S Balasubramanian \\
  SSSIHL, Prasanthi Nilayam Campus \\
  Sri Sathya Sai District, Andhra Pradesh, India\\
  \texttt{sbalasubramanian@sssihl.edu.in} \\
  %\AND
  %Badveeti Naveen Siva Kumar, Bobbili Veerendra Raj Kumar\\
  %SSSIHL, Prasanthi Nilayam Campus \\
  %Sri Sathya Sai District, Andhra Pradesh, India \\
  %\texttt{\{bnaveensivakumar, veerendra.rajkumar\}@gmail.com} \\
  %% \And
  %Other coauthor's information
}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{colorlinks=true, urlcolor=cyan,
pdftitle={Multi Task Learning for Affect Recognition at ABAW Competition Using Semi Supervised Learning},
%pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={Darshan Gera, S Balasubramanian, Badveeti Naveen Siva Kumar, Bobbili Veerendra Raj Kumar},
pdfkeywords={Multi Task Learning, Semi Supervised Learning, Facial Expression Recognition, AffWild2, s-aff-wild2},
}

\begin{document}
\maketitle

\begin{abstract}
The fifth Affective Behavior Analysis in-the-wild (ABAW) competition has multiple challenges such as Valence-Arousal Estimation Challenge, Expression Classification Challenge, Action Unit Detection Challenge, Emotional Reaction Intensity Estimation Challenge. In this paper we have dealt only expression classification challenge using multiple approaches such as fully supervised, semi-supervised and noisy label approach. Our approach using noise aware model has performed better than baseline model by 10.46\% and semi supervised model has performed better than baseline model by 9.38\% and the fully supervised model has performed better than the baseline by 9.34\%.
% Automatic affect recognition has applications in many areas such as education, gaming, software development, automotives, medical care, etc. but it is non trivial task to achieve appreciable performance on in-the-wild data sets. In-the-wild data sets though represent real-world scenarios better than synthetic data sets, the former ones suffer from the problem of incomplete labels. Inspired by semi-supervised learning, in this paper, we introduce our submission to the Multi-Task-Learning Challenge at the 4th Affective Behavior Analysis in-the-wild (ABAW) 2022 Competition. The three tasks that are considered in this challenge are valence-arousal(VA) estimation, classification of expressions into 6 basic (anger, disgust, fear, happiness, sadness, surprise), neutral, and the 'other' category and 12 action units(AU) numbered AU-\{1,2,4,6,7,10,12,15,23,24,25,26\}. Our method Semi-supervised Multi-task Facial Affect Recognition titled \textbf{SS-MFAR} uses a deep residual network with task specific classifiers for each of the tasks along with adaptive thresholds for each expression class and semi-supervised learning for the incomplete labels. 
% Source code is available at \url{https://github.com/1980x/ABAW2022DMACS}.
\end{abstract}

% keywords can be removed
\keywords{ Facial Expression Recognition \and Aff-Wild2 \and Semi-supervised Learning \and Noisy label approach \and Complementary label}


\section{Introduction}
Facial expression recognition (FER) is also a rapidly growing field of research that has become increasingly important in recent years. The ability to accurately detect and interpret human emotions based on facial expressions has a wide range of potential applications, from improving human-computer interaction to enhancing mental health diagnosis and treatment. To get models that are independent of demographic features such as age, gender, region, we need to train the models on real-in-wild datasets such as RAF-DB, Affectnet, Aff-wild2 {\cite{kollias2017recognition,kollias2018aff,kollias2018multi,kollias2018photorealistic,kollias2019deep,kollias2019expression,kollias2020va,kollias2019face,kollias2020deep,kollias2020analysing,kollias2023abaw,kollias2021affect,kollias2022abawECCV,kollias2021analysing,kollias2021distribution,kollias2022abawCVPR,zafeiriou2017affwild_dataset}}. These in-wild datasets poses multiple challenges such as variation in illumination, variation in poses, blur, etc. 
In this paper we have dealt with this problem in multiple approaches such as fully supervised,semi-supervised and noisy label approach.
% Automatic affect recognition is currently an active area of research and has applications in many areas such as education, gaming, software development, auto motives \cite{appsofFER, sym14040687}, medical care, etc.  Many works have dealt with Valence-arousal estimation \cite{oh2021causalpast, VARealtime, oh2021causal, VAmeng2022, deepVA18}, action unit detection \cite{jacob2021AU, shao2019AU, tang2017AU}, expression classification \cite{} tasks individually. \cite{XIAOHUA2019MTL} introduced a framework which uses only static images and Multi-Task-Learning (MTL) to learn categorical representations and use them to estimate dimensional representation, but is limited to AffectNet data set \cite{12}. Aff-wild2 \cite{kollias2018aff, kollias2019expression, kollias2018multi, kollias2020analysing, kollias2021distribution, kollias2021affect, kollias2019deep, kollias2020va,zafeiriou2017aff, kollias2017recognition} is the first dataset with annotations with all three tasks. \cite{kollias2021distribution} study uses MTL for all the three tasks mentioned earlier along with facial attribute detection and face identification as case studies to show that their network FaceBehaviourNet learns all aspects of facial behaviour. It is shown to perform better than the state-of-the-art models of individual tasks. One of major limitation of Affwild2 is that annotations of valence-arousal, AU and expression are not available for all the samples. So, this dataset has incomplete labels for different tasks and further imbalanced for different expression classes. In a recent work, \cite{AdaCM} uses semi-supervised learning with adaptive confidence margin for expression classification task for utilizing unlabelled samples. Adaptive confidence margin is used to deal with inter and intra class difference in predicted probabilities for each expression class. In this work, we developed a Multi-task Facial Affect Recognition (\textbf{MFAR}) in which a ResNet-18 \cite{he2016deep} network pre-trained on MS-Celeb-1M \cite{msceleb} weights is used, along with task specific classifiers for the three tasks. To handle the class imbalance in the dataset, we introduced re-weighting. From the s-Aff-Wild2 \cite{abaw2022} dataset, we observed that out of $142383$ a total of $51737$ images have invalid expression annotations. Motivated from \cite{AdaCM}, we added semi-supervised learning (\textbf{SS-MFAR}) to label the images with invalid expression annotations.

\section{Method}
In this section, we present our solution to the Expression (Expr) Classification Challenge at the 5th Affective Behavior Analysis in-the-wild (ABAW) Competition. %Problem formulation, WRITE ABOUT OTHER SECCTION AND OVERVIEW ABOUT THE PAPER
\subsection{Baseline} \label{sec:baseline}
% In this competition we are dealing only with expression classification.
The white paper\cite{kollias2023abaw} released by organisers of ABAW competition provided as baseline, a VGG16 network with fixed convolutional weights from a pre-trained checkpoint on VGGFACE dataset for feature extraction and the last three fully connected layers are trainable along with output layer equipped with softmax activation function that gives the predictions on 8 expression classes. 

\subsection{Fully Supervised approach with finetuning} \label{sec:fully_supervised}
The approach taken in the baseline model of \cite{kollias2023abaw} is to use a pre-trained checkpoint as a fixed feature extractor. We improve this by using resnet-18 \cite{he2016deepresnet} as basenet with pre-trained weights from resnet-18 model trained on MS-Celeb \cite{msceleb} face recognition data-set as our initial point, which acts as feature extractor, this is fine-tuned on the current task of facial expression recognition. This basenet is followed by a dropout layer and fully connected layer equipped with softmax activation learns to predict on the 8 expression classes considered as part of this challenge.

To train this model, images were augmented using augmentations such as horizontal flip, random crop and were resized to 224*224*3 before feeding them to the model to obtain predictions. Cross entropy $^{\ref{eq:lossCE}}$ was used as loss function for the baseline. We used Adam optimizer with learning rate 0.0005 and weight decay as $e^{-4}$. 

\begin{equation} \label{eq:lossCE}
L_{CE} = (-\sum^8_{c=1} \tilde{y}_{i=1}log(p^c(x_i,\theta))) ) 
\end{equation} 
Here $\theta$ refers to the model parameters $p^c$ refers to prediction probability of class c and $\tilde{y}_{c}$ refers to ground truth value of that class on a sample.

\subsection{Semi-Supervised learning with complementary labels}
There are a total of 1089929 images in train set, but nearly $50\%$ of this data, precisely 502970 images have invalid label(-1), therefore do not belong to any of the eight classes considered as part of the challenge. Therefore in total we have 586959 images as actual training images. To improve upon the performance of the fully supervised approach we could use these invalid images as unlabeled images and make use of semi-supervised learning which benefits from unlabeled images. Motivated from a recent work in semi-supervised learning named MutexMatch \cite{duan2022mutexmatch} which effectively uses unlabeled data to improve overall performance of the model in limited label setting.

MutexMatch uses a fixed threshold to divide unlabeled samples as high confident and low confident ones. Unlike other works in semi-supervised learning which emphasise on utilizing the high confident samples in varied varieties, MutexMatch uses the low confident samples to predict negative labels which is a simpler goal by the means of a True-Negative classifier(TNC). 

Similar to general approaches to semi-supervised learning it uses True-Positive classifier(TPC) to classify the images into considered set of classes. It has a supervised loss $L_{sup}$ \ref{eq:lossCE} on model predictions and ground truth labels, which helps the network learn features and weights, to classify appropriately. Once the model has learnt from the labeled data, this model is used to predict pseudo-labels on the unlabeled data using the True-Positive classifier. Fixed threshold is used to separate unlabeled samples into confident and non-confident samples. Predictions on confident samples are used as pseudo-labels to help the model utilize unlabeled data. 

To learn pseudo-labels effectively a pseudo-label loss $L_p$ is introduced which is the cross entropy$^{\ref{eq:lossCE}}$ loss between predictions of the model on weak and strong augmentations of high confident samples.  Two more losses $L_{sep}$ and $L_n$ involve the predictions from the complementary labels on low confident samples on the True-Negative classifier. $L_{sep}$ loss is cross entropy$^{\ref{eq:lossCE}}$ on TNC prediction of weak augmentation and the class with lowest confidence on weak augmented image as predicted by the TPC. $L_n$ $^{\ref{eq:lossLn}}$ is defined only on the low confident samples, which is negative consistency loss between prediction of TNC on weak and strong augmented versions of an image for top-k complementary predicted classes. 
 
\subsubsection{Loss functions}
\begin{equation} \label{eq:lossLp}
L_{p} = \frac{1}{\mu_B}\sum^{\mu_B}_{n=1}1(max(p^w_n) \geq \tau)H(\tilde{p}^{w}_{n},\tilde{p}^{s}_{n}))
\end{equation}
Here $\mu_B$ is the number of unlabeled images in a batch, H is the cross entropy function as in \ref{eq:lossCE}, $p^w_n$ is the TPC prediction on weak augmented image,  $\tilde{p}^{s}_{n}$ is the TPC prediction on strong augmented image, $\tilde{p}^{w}_{n}$ is $argmax(p^w_n)$. 

\begin{equation} \label{eq:lossLn}
L_{n} = \frac{1}{\mu_B}\sum^{\mu_B}_{n=1}1(max(p^w_n)<\tau)(-\frac{1}{k}\sum^C_{i=1} g_{n,(i)}r^{w}_{n,(i)}log(r^{s}_{n,(i)}))
\end{equation}
Here $\mu_B$ and $p^w_n$ are as defined above, $r^{w}_{n,(i)}$ is the ${i}^{th}$ probability component in the prediction of TNC on weak augmented image, $r^{s}_{n,(i)}$ is the $i^{th}$ probability component in the prediction of TNC on strong augmented image, $g_{n,(i)}$ is a mask which selects the classes that a in top-k of largest probability components in the complementary label prediction.

Inspired from \cite{AdaCM, xu2021dash, gera2023SSMFAR} which use dynamic adaptive threshold \ref{eq:thresh} that caters to inter and intra class differences that exist in facial expressions, we use dynamic adaptive threshold to divide the unlabeled samples into confident and non-confident ones. This class adaptive threshold is calculated by the model's performance on train set and is dynamically scaled up as the training progresses.

\begin{equation} \label{eq:thresh}
    T^c = \frac{\beta*({\frac{1}{N^s}}\sum^{N^s}_{i=1}\delta^c_i*p_i)}{1+\gamma^{-ep}} \hspace*{1em},\hspace*{1em} \mathrm{where}
\end{equation}
\begin{equation*} 
    \delta^c_i = \left \{ 
                        \begin{array}{ll}
                            1 & \mathrm{if \hspace*{1em}}  \tilde{y}_i = c, \\ \\
                            0 & \mathrm{otherwise}.
                        \end{array}
    \right.
\end{equation*}
The values  $\beta=0.95$ and $\gamma=e$ are taken from \cite{AdaCM}

\subsection{Noise aware model} 

During the training of the fully supervised model, we observed that accuracy and f1-score on an average was 98.7 and 98.2\% respectively on the training set, whereas average accuracy was 52.6 and the best f1-score was obtained to be 32.35\%.This shows that the model doesn't generalize well on unseen data. Generally, models do not generalize well due to the following reasons:
\begin{itemize}
    \item Capacity of the model : If the capacity of the model is low, then the model underfits the data leading to poor generalization. This can be solved by using a model with more capacity. But it can't be the case since the accuracy and f1-score on train is very high implying over fitting of model. 
    \item Distribution change : If there is a mismatch in the distribution of training data and the unseen validation data, then the model performs poorly on unseen data. But in general we treat the training data and unseen data are sampled from same distribution.
    \item Presence of noise : If there is noise in the labels of dataset  and if the model over-fits on the training data, then the model can't generalize well.  
\end{itemize}

 Assuming that there is noise in the labels, To deal with the noisy label problem, We propose a noise aware model.

This model has its backbone as resnet-18 followed by a fully connected layer with softmax activation to predict the 8 classes in expression classification task. We use pretrained weights from resnet18 model trained on affectnet dataset. In this model, we use two different types of augmentations of the image for consistency. MSE loss$^{\ref{eq:lossMSE}}$ is used for consistency and Weighted cross entropy loss$^{\ref{eq:weightedCE}}$ is used  for supervised loss. The weights are learnt by the model as per in SCN \cite{wang2020SCN} paper. Not all the samples are sent to supervised loss. We take out the samples whose prediction probabilities that fail to be greater than dynamic adaptive threshold{\cite{gera2022dynamic}}.

We use dynamic adaptive threshold in every epoch to tackle inter and intra class differences in prediction probabilities of expression classification. Dynamic adaptive threshold was calculated by taking the class vise mean of all the prediction probabilities in every batch. For a given class $i$, we treat the samples whose ground truth prediction probabilities that are greater than the dynamic adaptive threshold  for class $i$ as clean and those samples that fails are treated as noisy samples. Only clean samples are used for supervised loss and all the samples are used for consistency loss where we force the consistency between the attention maps of the weak augmented image and strong augmented image. 

The attention maps{\cite{lfa}} are calculated in the following way:
     
     We first extract the feature maps from last but second layer from the backbone and weights from the fully connected layer by multiplying the weights and feature maps we obtained the attention maps for every class. 
\begin{equation} \label{eq:weightedCE}
L_{WCE} = \frac{1}{N}(-\sum^N_{i=1} log(\frac{e^{\alpha_{i}W^{T}_{y_{i}}}x_{i}}{\sum^{C}_{j=1}e^{\alpha_{i}W^{T}_{j}}x_{i}})) 
\end{equation} 

\begin{equation} \label{eq:lossMSE}
L_{MSE} = -\frac{1}{NLHW}\sum^N_{i=1}\sum^L_{j=1} ||AM_{ij}-AM^{'}_{ij}||_{2}
\end{equation} 

We used Adam optimizer with 0.0001 as learning rate on all the model parameters.

\subsubsection{Loss functions}
We have used weighted cross entropy loss$^{\ref{eq:weightedCE}}$ for supervised loss where N is number of samples in batch C is number of classes and $\alpha_{i}$ are the weights.
The loss function that we have used for consistency loss is MSE loss$^{\ref{eq:lossMSE}}$ where N is number of samples in the batch, L is number of feature maps, H and W are height and width of feature maps, AM is attention maps on weak augmented image and $AM^{'}$ is attention maps on strong augmented flipped image.

% Our baseline architecture is shown in the Figure \ref{fig:mfar}. Weak Augmentations ($x_w$) of input images which have valid annotations are fed to the network and the outputs from each of the tasks are obtained. In s-Aff-Wild2 dataset which is the training set used valid annotations for 
% $i)$ expression is any integer value in \{0,1,2,\dots,7\}, 
% $ii)$ action unit annotations is either 0 or 1 and 
% $iii)$ valence-arousal is any value in the range [-1, 1], but there are images that have expression and action unit labels annotated with $-1$, and valence-arousal values annotated as $-5$ which are treated as invalid annotations. Outputs obtained from each of the task specific classifier is used to find task related loss and then combined to give the overall loss$^{\ref{Overallmfarloss}}$ the \textbf{MFAR} network minimizes. Losses used for each task are \textit{Cross Entropy}, \textit{Binary Cross Entropy}, \textit{Concordance Correlation Coefficient} based loss for expression classification, action unit detection and valence-arousal estimation tasks respectively. Re-scaling weight for each class was calculated using Eq.\ref{eq:expRW} and used to counter the imbalance class problem.
% If $N_{Exp}$ is the total number of valid expression samples and $n_{Exp}[i]$ is the number of valid samples for each expression class, then weights for a class $i$ is denoted by $W_{Exp}[i]$ and is defined as 
% \begin{equation} \label{eq:expRW}
%      W_{Exp}[i] = {\frac{N_{Exp}}{n_{Exp}[i]}}
% \end{equation}

% Positive weight, which is the ratio of negative samples and positive samples for each action unit as in Eq.\ref{eq:auRW} is used in binary cross entropy loss to counter the imbalance in the classes that each action unit can take. If $N_{AU}^P[i]$ is the number of positive samples and $N_{AU}^N[i]$ is the number of negative samples for each action unit, then positive weight for action unit i is denoted by $W_{AU}[i]$ and is defined as 
% \begin{equation} \label{eq:auRW}
%      W_{AU}[i] = {\frac {N_{AU}^N[i]} {N_{AU}^P[i]} }  
% \end{equation}
% The overall loss is the sum of losses of each task. \textbf{MFAR} network minimizes the overall loss function$^{\ref{Overallmfarloss}}$.

% \begin{equation} \label{Overallmfarloss}
%     {L}_{MFAR} = {L}_{VA} + {L}_{AU} + {L}_{Exp}
% \end{equation}

% \begin{figure*}[ht!] % picture
%     \centering
%     %\includegraphics[width=13cm,height=6.5cm]{./images/graphicalabstract03}
%     \includegraphics[width=1.0\textwidth]{./images/mfar}
%     %For SCAN, refer Fig. \ref{fig:figure_attention_net}
%     \caption{\textbf{MFAR} architecture is shown in this figure. Weak Augmentations $(x_w)$ of input images with valid annotations are fed to the network and the outputs from each of the tasks are obtained. Losses for each task are added to get the overall loss function$^{\ref{Overallmfarloss}}$, that MFAR minimizes. CE is cross entropy$^{\ref{lossCE}}$, BCE is binary cross entropy loss$^{\ref{lossBCE}}$, CCC loss is Concordance Correlation Coefficient based loss$^{\ref{lossCCC}}$.}
%     \label{fig:mfar}
% \end{figure*}

% \subsection{SS-MFAR : Semi-Supervised Multi-Task Facial Affect Recognition}


 

\subsection{Problem formulation}
Let $D = {\{(x_i, \tilde{y}_i)\}}^N_{i=1}$ be the  dataset of N samples. Here $x_{i}$ is $i^{th}$ image where $\tilde{y}_i$ represents expression class ${y}^{Exp}_i)$ of $i^{th}$ image. The backbone network is parameterized by $\theta$ ( ResNet-18 \cite{he2016deepresnet} pre-trained on different datasets like rafdb \cite{li2017rafdb} and affectnet \cite{mollahosseini2017affectnet} for different approaches as backbone). We denote $x_w$ as weak augmented image and $x_s$ as strong augmented image, $P_{Exp}$ represent probability distribution predicted by Expression classifier. Weak augmentations include random cropping with padding and horizontal flipping of the input image. Strong augmentations includes weak augmentations along with Randaugment \cite{Randaug}.

% \subsection{Adaptive Threshold}

% To tackle inter and intra class differences in prediction probabilities of expression, we use adaptive threshold for each class. Here we generate the threshold based on the predictions probabilities whose prediction label matches the ground truth label and threshold is also a function of epoch number since the discriminative ability of the model increases as epoch number increases.We denote the ground truth labels with $\tilde{y}_i$ for image $x_i$ ,ep as epoch number, $p_i$ denotes prediction probabilities of the image i and  we denote adaptive threshold as $T^c$ where c $\in$ $\{1,2,...,8\}$ defined as:
% \begin{equation} \label{thresh}
%     T^c = \frac{\beta*({\frac{1}{N^s}}\sum^{N^s}_{i=1}\delta^c_i*p_i)}{1+\gamma^{-ep}} \hspace*{1em},\hspace*{1em} \mathrm{where}
% \end{equation}
% \begin{equation*} 
%     \delta^c_i = \left \{ 
%                         \begin{array}{ll}
%                             1 & \mathrm{if \hspace*{1em}}  \tilde{y}_i = c, \\ \\
%                             0 & \mathrm{otherwise}.
%                         \end{array}
%     \right.
% \end{equation*}
% The values  $\beta=0.95$ and $\gamma=e$ are taken from \cite{AdaCM} 
% \subsection{Supervision loss}

% Supervision loss is computed based on the weak augmented images $x_w$ that have valid annotations for each task. We use Cross Entropy loss for the expression classification, binary cross entropy loss for AU detection and  Concordance Correlation Coefficient loss for VA estimation.

% Cross Entropy loss denoted by $L_{CE}((X,Y),\theta)$ used for expression classification is
% \begin{equation} \label{lossCE}
% L^s_{CE} = (-\sum^8_{c=1} \tilde{y}^{Exp^c}_{i=1}log(p^c(x_i,\theta))) ) 
% \end{equation} 
 
% Binary Cross Entropy loss denoted by $L_{BCE}((X,Y),\theta)$ used for action unit detection is
% \begin{equation} \label{lossBCE}
% L_{BCE} = (-\sum^2_{c=1} \tilde{y}^{AU^c}_{i=1}log(p^c(x_i,\theta))) ) 
% \end{equation}

% Concordance Correlation Coefficient loss denoted by $L_{CCC}((X,Y),\theta)$ used for valence arousal estimation is
% \begin{equation} \label{lossCCC}
%     L_{CCC} = 1-{\frac{2*s_{xy}}{s^2_{x}+s^2_{x}+(\bar{x}-\bar{y})^2}}
% \end{equation}

% \subsection{Unsupervised and Consistency Loss}

% Unsupervised Loss and Consistency Loss are used to learn from the images that have invalid expression annotations. We send weak augmented and strong augmented images to the expression classifier and based on the threshold generated earlier, we mark all the images that we have taken into confident and non-confident predictions. Unsupervised Loss is  Cross Entropy loss denoted by $L^u_{CE}$ on the confident logits of strong augmentations and labels predicted on weak augmentations.Consistency Loss is symmetric KL loss on the predicted probability distributions of weak and strong augmentations. 

% Symmetric KL-loss is defined for (p,q) probability distributions as :
% \begin{equation} \label{lossKL}
%     L^c_{KL} =  p*log(\frac{p}{q}) + q*log(\frac{q}{p})
% \end{equation}
% \subsection{Overall Loss}

% Overall loss is linear combination of losses from each task, where loss from Expression classification is defined as $ \mathcal{L}_{Exp} = \lambda_1*L^s_{CE} + \lambda_2*L^u_{CE}  + \lambda_3*L^c_{KL} $ where $ \lambda_1 = 0.5, \lambda_2 = 1, \lambda_3 = 0.1 $ taken from \cite{AdaCM} , loss from AU detection is defined as $ \mathcal{L}_{AU} = L_{BCE}$ and loss from VA estimation is defined as $ \mathcal{L}_{VA} = L_{CCC}$.
% Overall loss is defined as 
% \begin{equation} \label{overallssmfarloss}
%     \mathcal{L}_{Overall} = \mathcal{L}_{Exp} + \mathcal{L}_{AU} + \mathcal{L}_{VA}
% \end{equation}

% \begin{algorithm}[H]
% \SetAlgoLined
%  \textbf{INPUT:} dataset(D), parameters($\theta$), model(ResNet-18) pretrained on MS-Celeb,$\eta$(learning rate)\\
%   1.for epoch = 1,2,3,...,$epoch_{max}$\\
%   2.\hspace*{1em} for i =1,2,3,...,$N_B$\\
%   3.\hspace*{1em}\hspace*{1em}Obtain logits from model which has valid annotations for each task from $x_w$\\
%   4.\hspace*{1em}\hspace*{1em}Obtain Adaptive threshold$(T_c)$ according to Eq. \ref{thresh}\\
%   5.\hspace*{1em}\hspace*{1em}Obtain logits for $x_w$ and $x_s$ for invalid annotations of expression class\\
%   6.\hspace*{1em}\hspace*{1em}If prediction probability $p_i > T_c$:\\
%   7.\hspace*{1em}\hspace*{1em}\hspace*{1em}Mark those images predictions as confident otherwise mark them non confident.\\
%   8.\hspace*{1em}\hspace*{1em}Compute overall loss according to Eq. \ref{overallssmfarloss} using Eqs. \ref{lossCE},\ref{lossBCE},\ref{lossCCC},\ref{lossKL}\\
%   9.\hspace*{1em}\hspace*{1em}Update $\theta$ using Eq. \ref{overallssmfarloss}
  
%  \caption{Multi-Task Facial Affect Recognition Using Semi-Supervised Learning}
%  \label{Alg-ssmfar}
% \end{algorithm}

\section{Dataset }

\subsection{Dataset}
s-AffWild2 \cite{zafeiriou2017affwild_dataset} database is a static version of Aff-Wild2 database and contains a total of  11,10,367 images for training and 4,53,535 images for validation. Out of which 5,02,970 images are to be disregarded from training set and 20,438 images are not present but the image paths are given. In validation data 2,79,749 are valid and remaining all are with -1 as label which we should disregard.

\subsection{Implementation Details}

The performance measure fo experssion classification problem is the average F1 score on all the 8 classes.The F1 score is a harmonic mean of the recall (i.e. Number of positive class images correctly identified out of true positive class) and precision (i.e. Number of positive class images correctly identified out of positive predicted). The F1 score takes values in the range [0, 1]. Here we need to present F1 score as percentage. The F1
score is defined as:
\begin{equation}
F_{1} = {\frac{2*precision*recall}{precision+recall}}
\end{equation}

% \subsection{Experiments}
% Along with the models presented until now many other approaches to the given challenge were tried out. We list few of them here and the results obtained in Table \ref{tab:Tab1}.
% \begin{itemize} 
%     \item In order to deal with the expression class imbalance, instead of using the re-weighting technique on \textbf{MFAR}, re-sampling technique was used. We call this approach \textbf{MFAR-RS}.
%     \item To see the importance of consistency loss we ran a model without KL loss \ref{lossKL}. We call this approach  \textbf{SS-MFAR-NO\_KL}.
%     \item In order to deal with the expression class imbalance, instead of using the re-weighting technique on \textbf{SS-MFAR}, re-sampling technique was used. We call this approach \textbf{SS-MFAR-RS}.
%     \item Instead of using the pre-trained  weights of MS-Celeb-1M, network pre-trained on AffectNet \cite{12} database was used. We call this approach \textbf{SSP-MFAR}. 
%     \item Instead of just using unsupervised loss $L^u_{CE}$ on only non-confident expression predictions, unsupervised loss on $x_s$ and $x_w$ of all the images were added to respective task losses. We call this approach \textbf{SSP-MFAR-SA}.
%     \item Instead using a backbone network of ResNet-18 we used a backbone network ResNet-50 on the best performing model without any pre-trained weights. We call this approach \textbf{SS-MFAR-50}.
% \end{itemize}

\section{Results }
We report our results on the official validation set  from the ABAW 2023 Challenge \cite{kollias2023abaw} in Table \ref{tab:Tab1} . Our best performance achieves overall score of 33.46\% on validation set which is a significant improvement over baseline.

\begin{table}[hbt!]
\centering
    \caption{Performance comparison on Aff-Wild2 validation set}
    %\begin{tabular}{p{0.1\textwidth}|p{0.08\textwidth}|p{0.08\textwidth}|p{0.08\textwidth}}
    \begin{tabular}{c|c}
         \hline
         Method & Exp-F1 score  \\
         \hline
         \hline
         Baseline \cite{kollias2023abaw}  & \textit{23} \\
         Fully supervised model     &  \textit{32.34}   \\
         Semi-Supervised learning with complementary labels & \textit{32.82}\\
         Noise aware model & \textit{33.46}\\
         
         \hline
    \end{tabular}
    \label{tab:Tab1}
\end{table}


\section{Conclusions}
In this paper, we presented multiple methods. Firstly fully supervised model gave F1 score that is greater than the given baseline model{\cite{kollias2023abaw}} by 9.34\%. Then to overcome the limitations of fully supervised model, we proposed semi supervised learning with complementary labels and noise aware model that perform better over baseline by a margin of 9.38\% and 10.46\% respectively.

% our proposed Semi-supervised learning based Multi-task Facial Affect Recognition framework (SS-MFAR) for ABAW challenge conducted as a part of ECCV 2022. SS-MFAR used all the samples for learning the features for expression classification, valence-arousal estimation and action unit detection by learning adaptive confidence margin. This adaptive confidence margin was used to select confident samples for supervised learning from different expression classes overcoming inter class difficulty and intra class size imbalance. The non-confident samples were used by minimizing the unsupervised consistency loss between weak and strong augmented view of input image. The experiments demonstrate the superiority of proposed method on s-Aff-Wild2 dataset. 

\section*{Acknowledgments}
We dedicate this work to Our Guru and Guide Bhagawan Sri Sathya Sai Baba, Divine Founder Chancellor of Sri Sathya Sai Institute of Higher Learning, Prasanthi Nilayam, Andhra Pradesh, India. 

%Bibliography
\bibliographystyle{unsrt}
\bibliography{references}  

\end{document}