\pdfoutput=1
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[conference]{IEEEtran}
% \documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.


\usepackage{xcolor}
\usepackage{cite}
% \usepackage[nocompress]{cite}
\def\citepunct{,\,}
% \usepackage[noadjust]{cite}
\usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
\usepackage{graphicx}
% \usepackage{subcaption}
\usepackage{multirow}
\usepackage{soul}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{graphicx}
% \usepackage{minted}
% \usepackage{caption}
% \usepackage{subcaption}
% \usepackage[caption=false, font=footnotesize]{subfig}
% \usepackage{subfig}


% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
% \hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{LogQA: Question Answering in Unstructured Logs}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

	\author{
	    \IEEEauthorblockN{
	    Shaohan Huang\IEEEauthorrefmark{1}, 
	    Yi Liu\IEEEauthorrefmark{1},
	    Carol Fung\IEEEauthorrefmark{2},
	     Jiaxing Qi\IEEEauthorrefmark{1},
	     Hailong Yang\IEEEauthorrefmark{1},
	     Zhongzhi Luan\IEEEauthorrefmark{1}
	     }
	     \\
	     \IEEEauthorblockA{\IEEEauthorrefmark{1}Sino-German Joint Software Institute, Beihang University, Beijing, China
	    }
	    \\
	    \IEEEauthorblockA{\IEEEauthorrefmark{2}Computer Science Department, Virginia Commonwealth University, Richmond, Virginia, USA
	    }
	    \\
	    \IEEEauthorblockA{\{huangshaohan, yi.liu, luan.zhongzhi\}@buaa.edu.cn}
	}
% \author{Michael~Shell,~\IEEEmembership{Member,~IEEE,}
%         John~Doe,~\IEEEmembership{Fellow,~OSA,}
%         and~Jane~Doe,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space
        
% \thanks{M. Shell was with the Department
% of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
% GA, 30332 USA e-mail: (see http://www.michaelshell.org/contact.html).}% <-this % stops a space
% \thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}



% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
% \markboth{Journal of IEEE Transactions on Network and Service Management}%
% {Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% , April~2020
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}


% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}

Modern systems produce a large volume of logs to record run-time status and events. System operators use these raw logs to track a system in order to obtain some useful information to diagnose system anomalies. One of the most important problems in this area is to help operators find the answers to log-based questions efficiently and user-friendly. In this work, we propose LogQA, which aims at answering log-based questions in the form of natural language based on large-scale unstructured log corpora. Our system presents the answer to a question directly instead of returning a list of relevant snippets, thus offering better user-friendliness and efficiency. 
LogQA represents the first approach to solve question answering in lod domain.
LogQA has two key components: \textit{Log Retriever} and \textit{Log Reader}. Log Retriever aims at retrieving relevant logs w.r.t. a given question, while Log Reader is responsible for inferring the final answer. Given the lack of a public dataset for log questing answering, we manually labelled a QA dataset of three open-source log corpus and will make them publicly available. We evaluated our proposed model on these datasets  by comparing its performance with 6 other baseline methods. Our experimental results demonstrate that LogQA has outperformed other baseline methods. 

%The log sequence encoder helps model to capture the contextual information in log sequences and parameter value encoder is designed to detect parameter values anomalies. 

\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Log data analysis, question answering, information retrieval
\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fihttps://www.overleaf.com/project/5e4cf40d5219a10001013a4b
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\IEEEPARstart{M}{odern} Computer systems have become increasingly complex as service systems grow in both size and complexity~\cite{tan2010predictability}. These systems produce a large volume of logs, which are widely used to record run-time status and events.  Since system logs contain noteworthy information, they have become one of the most important data sources for system monitoring to improve service quality.

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=7.cm]{figures/demo.pdf}
	\end{center}
	           %  \vspace{-4mm}
	\caption{Compared to traditional approach, LogQA directly answers log-based questions in the form of natural language.}
	\label{fig:demo}
	           %  \vspace{-4mm}
\end{figure}

Many log analysis approaches have been proposed for service management. As summarized in~\cite{he2021survey}, automated log analysis can be  classified into log compression (e.g.,~\cite{burrows1992line}, ~\cite{skibinski2007fast}), log parsing (e.g,~\cite{zhu2019tools}, \cite{he2017drain}, \cite{huang2020paddy}), anomaly detection (e.g,~\cite{zhang2019robust}, \cite{huang2020hitanomaly}), failure 
diagnosis~(e.g.,~\cite{tanwir2015information,kim2016fault}), and failure prediction~\cite{zheng2009system,zhang2016automated}. The existing studies have boosted the effectiveness and efficiency of systematic usage of service logs. However, operators often have to check the raw logs in order to acquire some information or diagnose system events. 
Figure~\ref{fig:demo} shows an example, an HDFS (a java based distributed file system used in Hadoop) system administrator discovered an error in block blk\_5142679. He wants to figure out the size of this block. In the traditional processes, he begins by utilizing the \textit{block blk\_5142679} to search for relevant system logs in the entire logs. Then, he carefully checks each log entry to identify the answer. One of the most critical challenges in this area is assisting operators in locating answers in an efficient and user-friendly way. However, manual or the rule-based log question answering has become ineffective and inefficient. First, modern computer systems produce a large volume of logs (e.g., some cloud systems would produce about 30-50 gigabytes of tracing logs per hour~\cite{zhu2019tools}). Locating an answer manually becomes time-consuming. Secondly, developers may frequently modify the source code of log recording, which leads to log data format change and the occurrence of some unseen log events, which makes it difficult to handle with rule-based methods as well.
Thirdly, many services are implemented and maintained by a number of operators. Operator with domain knowledge is expected to find answers through log examination. How to construct a user-friendly and effective solution to assist system administrators in obtaining important information and locating the answers to log-based questions has not yet been thoroughly investigated.

% Although some works have been proposed in log related question answering~\cite{burkhardt2016quark,lopez2007aqualog}, no previous study has provided a general method to solve this problem. For example,  Felix Burkhardt~\cite{burkhardt2016quark} focuses on telecommunication domain and extracts specific-domain information to build a knowledge source. 

Question answering is an important topic in the field of natural language processing, and many works have been proposed recently~\cite{devlin2018bert}.
However, there are some challenges in using NLP algorithms for log question answering. First, there exists a domain shift between general natural language and log data. Log data includes domain-specific symbols, such as IP addresses and modular identifiers~\cite{he2017towards}. General NLP techniques consider these symbols to be out-of-domain terms and replace them with a special token, however they are crucial for the log domain and cannot be ignored.
Second, applying NLP methods to learn a question answering model usually requires a large-scale training set~\cite{devlin2018bert}. However, to the best of our knowledge, there is no public question answering dataset. It is difficult to implement an effective question answering system with limited data. 

In this work, we propose a question answering system for unstructured logs, namely LogQA, which aims to answer questions in the form of natural language based on large-scale unstructured log corpus. LogQA has two key components: Log Retriever and Log Reader. Log Retriever retrieves some relevant and helpful raw logs. The goal of Log Reader is to predict exact answers based on retrieved logs. We follow~\cite{goldberg2014word2vec} to involve negative sampling to train Log Retriever. Due to the limitation of our training data size, there exists many completely unrelated logs in negative examples, which cannot provide sufficient informative examples for training. To address the above challenge, we propose an iterative method to generate hard negative examples into our training process.


Due to the lack of publicly available question answering benchmarks, we manually labeled a QA dataset over three public log datasets (HDFS, OpenSSH, and Spark) and will make them public available. We evaluate our proposed method on these datasets. Experimental results show that Log Retriever outperforms other existing log-based retrieval methods. The accuracy of our top-5 model is over 30 points higher than the one ranked second in an extremely difficult dataset. We also investigate the impacts of hard negative in training and the number of training iteration. In log reading task, we compare the performance of Log Reader with four baseline methods. Experimental results show that our method achieves the best performance on three datasets.

The key contributions of this paper can be summarized as follows:
\begin{itemize}
\item  We propose LogQA, a framework to answer a question in the form of natural language based on large-scale unstructured log corpus, which is easy to help operators find the answer efficiently and user-friendly.
\item LogQA consists of log retrieving and log reading. Both achieve the state-of-the-art results.
\item We design a hard negative sampling into Log Retriever training process, which is able to improve the robustness and efficiency of retrieval model.
\item We manually annotate a QA dataset for three open-source log datasets and will make them public available, which will benefit the research community  in log-related QoS area.
\end{itemize}

The rest of the paper is organized as follows. We introduce the overview of our work in Section~\ref{sec:overview}. The detail of our model are described in Section~\ref{sec:method}. Section~\ref{sec:exp} describes the experimental settings, and log question answering dataset we manually labelled. We evaluate the performance of our method~\ref{sec:results}. Related work is introduced in Section~\ref{sec:related}. Finally, in Section~\ref{conc}, we conclude our work and state possible future work.

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=7.5cm]{figures/overview.pdf}
	\end{center}
	           %  \vspace{-4mm}
	\caption{Workflow of LogQA system.}
	\label{fig:overview}
	           %  \vspace{-4mm}
\end{figure}

\section{Overview}\label{sec:overview}
LogQA takes a question $x$ as input and learns a distribution $p(y|x)$ over outputs answer $y$. We decompose $p(y|x)$ into two steps: \textit{retrieve}, then \textit{read}. Given a question $x$, we first retrieve some possible relevant logs $z$ from a raw log corpus $\mathcal{Z}$. We call this step as a distribution $p_1(z|x)$. Then, based on retrieved logs $z$ and question $x$ to output the answer $y$. This step is modeled as $p_2(y|z,x)$. The overall likelihood can be defined as:
\begin{equation}
    p(y|x) = \sum_{z \in \mathcal{Z}}{p_2(y|z,x)p_1(z|x)}
\end{equation}

Therefore, LogQA has two key components: Log Retriever, which models $p_1(z|x)$ and Log Reader, which models $p_2(y|z,x)$. The overall framework is illustrated in Figure~\ref{fig:overview}. For example, an HDFS IT operator asks a question \textit{`What is the size of block blk\_5142679?'}. Log Retriever retrieves some relevant and helpful raw logs from an HDFS log corpus (e.g., \textit{Received block blk\_5142679 of size 67108864 from /10.251.70.211}). We regard these retrieved logs as log candidates and feed them into Log Reader. The goal of Log Reader is to predict exact answers for users. In this case, Log Reader extract the correct answer \textit{67108864} from the log \textit{Received block blk\_5142679 of size 67108864 from /10.251.70.211}. 

In the training phase, the Log Retrieval and Log Reader models are optimized, respectively. During the inference phase, we pass the question through Log Retrieval to retrieve the log candidates, then utilize the question and the log candidates to determine the exact answer.

\section{Methodology}\label{sec:method}
In this section, we will first describe the log parsing employed in this research. Then, we describe our Log Retriever's design. Then, we present our Log Reader model in detail.

\subsection{Log Parsing}\label{sec:logparsing}
Log parsing is a crucial tool for processing unstructured, free-text logs for many log-based applications such as anomaly detection (e.g,~\cite{zhang2019robust}) and failure diagnosis~(e.g.,~\cite{tanwir2015information,kim2016fault}). The process of parsing logs is also a very important component of LogQA.
The purpose of log parsers is to transform unstructured messages into structured log templates carrying key parameters. As shown in Table~\ref{table:logpariing}, one log message ``Received block blk\_-2856928563366064757 of size 67108864 from /10.251.42.9'' comes from  the HDFS dataset. It is parsed into log template ``Received block $\langle$*$\rangle$ of size $\langle$*$\rangle$ from /$\langle$*$\rangle$'' and parameter values [`blk\_-2856928563366064757', `67108864', `10.251.42.9']. The log template consists of fixed text characters, while parameters record system variables and properties.

\begin{table}[ht]
\renewcommand{\arraystretch}{1.8}
\setlength{\tabcolsep}{14pt}

    \caption{Example of log parsing.
    }
    \centering
    \label{table:logpariing}
    \centering
    \begin{tabular}{p{0.15\linewidth}  p{0.6\linewidth}}
    \hline
   Raw log       &  Received block blk\_-2856928563366064757 of size 67108864 from /10.251.42.9 \\ \hline 
   Template   &	 Received block $\langle$*$\rangle$ of size $\langle$*$\rangle$ from /$\langle$*$\rangle$ \\ \hline
   Parameters &	`blk\_-2856928563366064757', `67108864', `10.251.42.9'   \\ \hline

    \end{tabular}
\end{table}

There have been many studies on log parsing~\cite{yamanishi2005dynamic,oliner2012advances,niwattanakul2013using,he2016evaluation,he2017drain}. Therefore, log parsing is not the focus of this paper. Considering accuracy and speed, we adopt Drain~\cite{he2017drain} method to parse all log data.

\subsection{Log Retriever}\label{sec:logretiever}
In the Log Retriever part, we first introduce the architecture model of Log Retriever. Then we describe the training method of Log Retriever. 
% To improve the performance of our model, we propose an iterative method by using hard negative examples in our training process.

\subsubsection{Model Architecture}
Given a question $x$, the goal of Log Retriever is to retrieve potentially helpful logs $z$ from a large log corpus. The retrieved logs contain a span that can answer the question. The Log Retriever can be defined as:
\begin{equation}\label{equa:logretrieve} \begin{split}
    p(z|x) & = \frac{exp f(x, z)}{\sum_{z_i}{exp f(x, z_i)}} \\
    f(x, z) & = cosine(encode(x), encode(z))
\end{split} \end{equation}
% The key challenge of Log Retriever is to compute the similarity of questions and raw logs.
where $f(x, z)$ is a function to compute the similarity between $x$ and $z$. We use an encoder to convert a question or log into a vector, then use cosine distance to measure the similarity between $x$ and $z$.  
Cosine similarity is a measure of similarity between two non-zero vectors defined in an inner product space. It is used as a distance evaluation metric between two points in the plane. It is a widely used metric in machine learning algorithms like the $k$-NN for determining the distance between the neighbors in information retrieval~\cite{he2021survey}.
The retrieval distribution is the softmax over all similarity scores.

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=8.cm]{figures/retriever.pdf}
	\end{center}
	           %  \vspace{-4mm}
	\caption{Log Retriever Model.}
	\label{fig:model}
	           %  \vspace{-4mm}
\end{figure}

The similarity function $f(x, z)$ is the core structure of our Log Retriever. As shown in Figure~\ref{fig:model}, we design a two-tower structure, where each tower is a multi-depth Transformer-based~\cite{vaswani2017attention} encoder to learn the representations of questions and raw logs. Given a question $x$ and a log $z$, the left encoder is to learn the contextual representations of question $x$ and the right encoder converts the raw log $z$ into a dense vector. Then, we use cosine function to compute the similarity between the question $x$ and the raw log $z$. Because we only have limited labeled data as our training data, we share the parameters of encoders in these two towers.

\subsubsection{Retriever Training}
Now we present the training process of Log Retriever. As shown in the Equation~\ref{equa:logretrieve}, we train Log Retriever model by maximizing the likelihood $p(z|x)$. The key computational challenge is that the probability $p(z|x) = \frac{exp f(x, z)}{\sum_{z_i}{exp f(x, z_i)}}$, where $z_i \in \mathcal{Z}$, involves a summation over all log corpus $\mathcal{Z}$. The log $z$ that contains the correct response is considered potentially helpful. In addition, we discover that some logs may not contain the correct answer, but some of their parameters exist in question. In the log retriever, these question-related parameters also contain crucial information. Therefore, we customize the following three target functions:
\[
\mathbf{T}(z) = \left\{%
 \begin{array}{l}
  1 \text{  if the log $z$ contains answer y;}\\
  \alpha \text{  if the log $z$ contains parameters of $x$;}\\
  0 \text{  otherwise.}
 \end{array}%
\right.
\]
where $\mathbf{T}(z)$ denotes the target value of the probability $p(z|x)$. In our work, we set the $\alpha$ value as 0.2.

Inspired by~\cite{goldberg2014word2vec}, we involve negative sampling into our training. More specifically, we sample a set of logs from raw log corpus $\mathcal{Z}$ and use these sampled logs as negative examples. The shortcoming of the negative sampling method is that these logs are randomly sampled from the entire corpus and there exist some completely unrelated logs that will be used as negative examples. However, these instances are too simple for the similarity model. To overcome this problem, we propose an iterative method to generate hard negative examples into our training process.

The training algorithm is explained in Algorithm~\ref{alg:cap}. We first initialize our Log Retriever model with BERT~\cite{devlin2018bert}, which is a pre-trained bidirectional transformer language model. The BERT model can help the Log Retriever model capture more semantic information and better understand questions in the form of natural language. After initialization, we sample question log pairs from labeled data $\mathcal{P}$ and use these data to optimize our model. After finishing each iteration, we use our model to obtain hard negative examples. Given questions in labeled data, we use our model to retrieve log candidates from raw log corpus and then remove the logs which contain the correct answer from log candidates. These remaining candidates cannot answer the question but are very similar to the question. We label these candidates as hard negative samples and add them to our training data. Therefore, we extend our training objective defined in Equation~\ref{equa:logretrieve} to incorporate hard negative examples as follows:
\begin{equation}\label{equa:logretrieve_hard} \begin{split}
    p(z|x) & = \frac{exp f(x, z)}{\sum_{z_i \in \mathcal{B}}{exp f(x, z_i)} + w \sum_{z_j \in \mathcal{H}}{exp f(x, z_j)}} \\
\end{split} \end{equation}
where $\mathcal{B}$ represents a batch of question log pairs and $\mathcal{H}$ is  hard negatives. $w$ is the weight of hard negative examples. We train Log Retriever with different values $w$ and compare the impact of this weight in our experiments.

There are two advantages to our proposed method. First, our negative examples are more informative and reduce the impact of extremely easy negative examples. Second, our hard negative sampling can be regarded as a kind of data augmentation method, which can solve the insufficient training data problem.

\begin{algorithm}
\caption{Log Retriever Training}\label{alg:cap}
\begin{algorithmic}
\Require Question Log Pairs $\mathcal{P}$, Raw Log Corpus $\mathcal{L}$, Max Iteration Number $I$
% \Ensure $y = x^n$
\State Initialize log retriever model $M$ with BERT
\State $i \gets 0$
\While{$i < I$}
    \For{Batch $\mathcal{B}$ in $\mathcal{P}$}
    \State Compute loss based Equation~\ref{equa:logretrieve}
    \State Update model M
    \EndFor
    \State \textit{//obtain hard negative examples}
    \For{ each pair $(q, l)$ in $\mathcal{P}$} 
    \State Retrieved Logs $L = M(q)$
    \State Remove $l$ from $L$
    \State $\mathcal{P} \gets \mathcal{P} + L$
    \EndFor
    \State $i \gets i + 1$
\EndWhile
\State \textbf{Return} log retriever model $M$
\end{algorithmic}
\end{algorithm}


\subsection{Log Reader}\label{sec:logreader}
In this section, we introduce the design of Log Reader, which is the other core component of the LogQA system. It aims at inferring the exact answer in response to the question from a set of retrieved logs. 
% Based on~\cite{zhu2021retrieving}, existing Readers can be categorized into two types: Extractive Reader and Generative Reader. Extractive Reader predicts an answer span from the retrieved documents and Generative Reader that generates answers in natural language. However, the current generation often suffers from syntax errors, incoherence, or illogic. Therefore, we implement our Log Reader based on extractive-style.

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=8.5cm]{figures/reader.pdf}
	\end{center}
	           %  \vspace{-4mm}
	\caption{Log Reader Model.}
	\label{fig:reader_model}
	           %  \vspace{-4mm}
\end{figure}


Given the top $k$ retrieved logs, the Log Retriever assigns a selection score to each retrieved log. In addition, Log Reader extracts an answer span from each log and assigns a span score. In labeled training data, the correct answer to a given question must exist in the log. Following~\cite{devlin2018bert}, the Log Retriever model focuses on learning how to predict the start and end position of an answer span from the retrieved logs. As shown in Figure~\ref{fig:reader_model}, we concatenate the input question (\textit{What terminated block blk388?}) and the context log (\textit{Responder 1 for block blk388 terminating}) as a single packed sequence and feed the packed sequence into the Log Reader model. In the answer \textit{Responder 1}, \textit{Responder} is the starting position and \textit{1} is the ending token.
Specifically, let $h_i$ be a word representation for $i$-th token in the retrieved log. The probabilities of a token being the
starting/ending positions of an answer span are defined as:
\begin{equation}\label{equa:logreader} \begin{split}
    P_{start,i} & = softmax(h_i \cdot W_{start}) \\
    P_{end,i} & = softmax(h_i \cdot W_{end}) 
\end{split} \end{equation}
where $W_{start}$ and $W_{end}$ are learnable parameters. We compute a span score of the $s$-th to $e$-th words from the retrieved log as $P_{start,s} \times P_{end,e}$. 

We design two training objective functions to optimize our Log Reader as following:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{QA} + \mathcal{L}_{param}
\end{equation}
The training objective $\mathcal{L}_{QA}$ is to maximize the marginal log-likelihood of all correct answer spans in the retrieved logs. The objective $\mathcal{L}_{param}$ is to predict whether the token represents a parameter variable. As shown in Figure~\ref{fig:reader_model}, the raw log is \textit{Responder 1 for block blk388 terminating} and its parameters are \textit{1} and \textit{blk388}. The goal of $\mathcal{L}_{param}$ is to find parameters \textit{1} and \textit{blk388} in raw log.
The training objective $\mathcal{L}_{param}$ has two advantages. First, parameters are typically used to record system variables and properties, and they are more likely to contain the answer to the question. Second, due to the limited data, only deploying QA loss cannot efficiently optimize Log Reader model. The parameter loss can help model understand log data better.

In our inference stage, we combine the probability of a log containing the answer and that of a token being the starting and ending position of an answer span, and selects the answer with the highest probability. 

\section{Experiment Setting}\label{sec:exp}
In this section, we present our experimental setting and evaluation results of LogQA. We first describe how to build unstructured logs question answering datasets. Then we introduce our evaluation metrics to evaluate QA systems. Finally, we describe some baseline methods and the hyperparameters in our experiments.

\subsection{Datasets}
We conduct question answering experiments on three public log datasets: HDFS dataset~\cite{xu2009detecting}, OpenSSH dataset~\cite{huang2020paddy} and Spark~\cite{zhu2019tools}. Due to the lack of a publicly available question answering benchmark, we manually labeled the above public log datasets and make them publicly available~\footnote{https://github.com/LogQA-dataset/LogQA}. We select 2,000 logs per dataset as our raw log set. Then we use a question generation model\footnote{https://huggingface.co/iarfmoose/t5-base-question-generator} to generate reading comprehension-style questions~\cite{rajpurkar2016squad} with answers extracted from a log. For example, one raw log is \textit{Received block blk\_5142679 of size 67108864 from /10.251.70.211
}. We take \textit{67108864} as its answer and feed it into the question generation model. The model will output \textit{What is the size of block blk\_5142679?}. Since we don't know which span is a proper answer, we iterate through all tokens in a log as an answer to generate questions. We collected more than 10,000 questions per dataset and then labeled all questions manually. The detailed information of the above datasets is listed in Table~\ref{table:data}. We can find that we keep only 2-3\% of the data after human annotation. A possible reason is that the question generation model we used is trained on Wikipedia-based question answering data~\cite{rajpurkar2016squad} and there exists a domain shift between Wikipedia and log data.

\begin{table}[ht]
\renewcommand{\arraystretch}{1.8}
\setlength{\tabcolsep}{14pt}

    \caption{Statistics of log QA data.
    }
    \centering
    \label{table:data}
    \centering
    \begin{tabular}{ccc}
    \hline
   Datasets       & \# of QA pairs    &  Length of Question \\ \hline \hline
   HDFS   &	 247 &	7.83  \\ \hline
   OpenSSH &	188  &	9.03  \\ \hline
   Spark &	397  &	7.00  \\ \hline

    \end{tabular}
\end{table}

To present the diversity of unstructured logs QA data, we categorize these questions into different types. As shown in Table~\ref{table:type}, in HDFS and OpenSSH datasets, the most common question type is `what', which accounts for roughly 90\% of all questions. The Spark data is more diverse and contains six different types of questions. In the Spark data, the most common question type is `how many'.

\begin{table}[ht]
\renewcommand{\arraystretch}{1.8}
    \caption{Question Type of log QA data.
    }
    \centering
    \label{table:type}
    \centering
\begin{tabular}{|lr|lr|lr|}
\hline
\multicolumn{2}{|c|}{HDFS} & \multicolumn{2}{c|}{OpenSSH} & \multicolumn{2}{c|}{Spark} \\ \hline \hline
What         & 88.3\%      & What           & 92.0\%      & How many      & 48.6\%     \\
Where        & 11.3\%      & Did            & 5.9\%       & What          & 27.0\%     \\
Others       & 0.4\%       & Who            & 1.1\%       & Is            & 18.9\%     \\
             &             & How many       & 1.1\%       & How large     & 2.8\%      \\
             &             &                &             & How Long      & 2.0\%      \\
             &             &                &             & Others        & 0.8\%     \\\hline
\end{tabular}
\end{table}


% We list the summary statistics for this log datasets in Table~\ref{table:data}. The detailed information on the two datasets are described as follows:

\subsection{Evaluation Metrics}
To measure the effectiveness of Log Retriever and Log Reader in question answering task, we use a set evaluation metrics to evaluate these two modules.

For the log retrieval part, we use \textit{top-k retrieval accuracy} (Acc@K), which are the most commonly used metrics in information retrieval problem~\cite{harman2011information}. The formal definition of Acc@K can be defined as follows: a log retriever $\mathcal{R}(q, \mathcal{C}) \xrightarrow{} \hat{\mathcal{C}}$ takes as input question $q$ and a log corpus $\mathcal{C}$ and returns a much smaller set $\hat{\mathcal{C}}$, where $\hat{\mathcal{C}} \in \mathcal{C}$ and $|\hat{\mathcal{C}}| = k 	\ll |\mathcal{C}|$. Top-k retrieval accuracy is the fraction of questions for which $\hat{\mathcal{C}}$ contains a span that can answer the question. In our experiments, we separately present the results of log retrieval where the $k$ is 1, 5 or 20.

For the log reading part, we model it into a reading comprehension task and follow~\cite{rajpurkar2016squad} and use \textit{Extra Match} (EM) and \textit{F1 score} (F1) as our evaluation metrics. EM measures the percentage of predictions that match the ground truth answer exactly. F1 score measures the average overlap between predictions and the ground truth answer. We consider the prediction and ground truth bags of tokens and compute their overlap F1 score. We compute these metrics as follows:
\begin{equation}
    \texttt{Precision} = \frac{\#Same}{\#Prediction}
\end{equation}
Precision shows the percentage of the same tokens among all text span predicted by the model.
\begin{equation}
    \texttt{Recall} = \frac{\#Same}{\#Ground truth}
\end{equation}
Recall means the percentage of the same tokens in the ground truth answer.
\begin{equation}
    \texttt{F1-Score} = \frac{2 \cdot \texttt{Precision} \cdot \texttt{Recall}}{\texttt{Precision} + \texttt{Recall}}
\end{equation}

For example, as shown in Figure~\ref{fig:overview}, the ground truth answer is `67108864'. We assume that the prediction of our model is `size 67108864'. The length of the same tokens `67108864' is 1 and we can compute that the precision is $\frac{1}{2}$ and recall is $1$. Its F1-score is $\frac{2}{3}$.


\begin{table*}[t]
\renewcommand{\arraystretch}{1.8}
\setlength{\tabcolsep}{12pt}
\centering
\caption{Evaluation results of log retrieval.
    }
    \centering
    \label{table:logre}
\begin{tabular}{|l|lll|lll|lll|}
\hline
                       & \multicolumn{3}{|c|}{\textbf{HDFS}}                 & \multicolumn{3}{c|}{\textbf{OpenSSH}}              & \multicolumn{3}{c|}{\textbf{Spark}}                \\ \hline \hline
                       & \textbf{Acc@1} & \textbf{Acc@5} & \textbf{Acc@20} & \textbf{Acc@1} & \textbf{Acc@5} & \textbf{Acc@20} & \textbf{Acc@1} & \textbf{Acc@5} & \textbf{Acc@20} \\ \hline
\textbf{Random}        & 0.0878         & 0.3243         & 0.6284          & 0.0625         & 0.2321         & 0.4464          & 0.0084         & 0.0672         & 0.1975          \\
\textbf{Edit Distance} & 0.4662         & 0.4865         & 0.5068          & 0.3304         & 0.3393         & 0.3571          & 0.0000         & 0.1387         & 0.2227          \\
\textbf{Jaccard}       & 0.2297         & 0.3176         & 0.3919          & 0.2411         & 0.3482         & 0.4554          & 0.2311         & 0.3193         & 0.4496          \\
\textbf{BM25}          & 0.2568         & 0.2635         & 0.2838          & 0.0179         & 0.2232         & 0.4375          & 0.0000         & 0.0210         & 0.0252          \\
\textbf{Jaro Winkler}  & 0.4730         & 0.5473         & 0.6689          & 0.2232         & 0.2589         & 0.3482          & 0.1933         & 0.1975         & 0.2437          \\
\textbf{BERT Cosine}   & 0.3649         & 0.6081         & 0.6486          & 0.3750         & 0.4464         & 0.5000          & 0.0798         & 0.1387         & 0.2437         \\ 
\textbf{Log Retriever}   &\textbf{0.5850}         & \textbf{0.7200}         & \textbf{0.7600}          & \textbf{0.5172}         & \textbf{0.6206}         & \textbf{0.6379}          & \textbf{0.3830}         & \textbf{0.6166}           & \textbf{0.8500}         \\ \hline
\end{tabular}
\end{table*}


\subsection{Baseline Methods}
We first introduce the baseline methods in the log retrieval part. These methods are briefly described as follows:
\begin{itemize}
\item Random: we randomly sample $k$ logs from raw corpus as our log retrieval results.
\item Edit Distance: we use edit distance to measure the similarity between question and raw logs.
\item Jaccard~\cite{niwattanakul2013using}: we use Jaccard similarity to measure similarities between questions and raw logs. Jaccard similarity can be computed as follows:
\begin{equation}
    J(Q, L) = \frac{|Q \cap L|}{|Q \cup L|}
\end{equation}
where $Q$ is a bag of question words and $L$ is the word set of a raw log.
\item BM25~\cite{robertson2009probabilistic}: BM25 is a ranking algorithm used by search engines to estimate the relevance of documents to a given search query, which is widely used in many information retrieval systems. We use Gensim\footnote{https://github.com/RaRe-Technologies/gensim} in our experiments.
\item Jaro Winkler~\cite{dressler2017efficient}: the JaroWinkler distance is various of Jaro distance. Jaro similarity is a string metric to measure an edit distance between two sequences. The JaroWinkler distance uses a prefix scale to give more favorable ratings to strings.
\item BERT Cosine~\cite{devlin2018bert}: BERT is a pre-trained model, which can be used to encode natural language text into dense vectors. We use BERT to encode question and raw logs respectively and use cosine function to measure their similarity.
\end{itemize}

We implement six baseline methods in the log reading task. These methods are described as follows:
\begin{itemize}
\item Random: we randomly sample one word from the retrieved log as its answer.
\item Slide Window: For each candidate answer, we compute the unigram overlap between the candidate answer and the question. Among these, we select the best one using the sliding-window approach proposed in~\cite{richardson2013mctest}
\item Logistic Regression~\cite{wright1995logistic}: We extract features for each candidate answer and use a logistic regression model as a binary classification model. To obtain a better representation of question and answer, we leverage the Log Retriever model to encode them and use hidden states as their feature.
\item BiDAF~\cite{seo2016bidirectional}: BiDAF is a machine comprehension model. It introduced a Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity.
\item QANet~\cite{yu2018qanet}: QANet consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. 
\item BERT~\cite{devlin2018bert}: BERT is a pre-trained transformer-based model. It achieved many cutting-edge results in a variety of general question answering tasks in the field of natural language processing.
\end{itemize}

We implement Log Retriever and Log Reader based on PyTorch~\cite{paszke2017automatic} on the Linux server with NVIDIA Tesla V100 GPU. In our Log Retriever experiment, we set the weight of the hard negatives as 2 and the iteration number as 4. We set the learning rate as 5e-5. In our Log Reader experiment, we set the epoch as 15 and the learning rate as 3e-5. We randomly sample 60\% dataset as our training data, 10\% data for validation and others as testing data.


\section{Results}\label{sec:results}
Our experiments contain two sub-tasks: log retrieval and log reading. We first describe the experiment results of Log Retriever. We then show the performance of Log Reader on different log data. 
% We also evaluate the performance in a cross-domain setting. 
Lastly, we present the case study to help understand the difference between our model and other baseline methods. 


\subsection{Log Retriever}
In this part, we first study the performance of Log Retriever on the three log datasets. Then we present the experiments of hard negative sampling. Lastly, we investigate the impacts of hard negative example weight and the number of training iterations.



\begin{table*}[t]
\renewcommand{\arraystretch}{1.8}
\setlength{\tabcolsep}{11pt}
\centering
\caption{Evaluation results of log retrieval without hard negative in training.
    }
    \centering
    \label{table:hardnega}
\begin{tabular}{|l|lll|lll|lll|}
\hline
                       & \multicolumn{3}{|c|}{\textbf{HDFS}}                 & \multicolumn{3}{c|}{\textbf{OpenSSH}}              & \multicolumn{3}{c|}{\textbf{Spark}}                \\ \hline \hline
                       & \textbf{Acc@1} & \textbf{Acc@5} & \textbf{Acc@20} & \textbf{Acc@1} & \textbf{Acc@5} & \textbf{Acc@20} & \textbf{Acc@1} & \textbf{Acc@5} & \textbf{Acc@20} \\ \hline
\textbf{Log Retriever}   & 0.5850        & 0.7200         & 0.7600          & 0.5172         & 0.6206         & 0.6379          & 0.3830         & 0.6166           & 0.8500         \\ 
\textbf{W/O Hard Negative}   & 0.3066        & 0.4266         & 0.5600          & 0.2730         & 0.3448         & 0.3965          & 0.3166         & 0.4583           & 0.7666         \\ \hline
\end{tabular}
\end{table*}

\subsubsection{Performance of Log Retriever} Table~\ref{table:logre} shows the performance of Log Retriever compared to six baseline methods over three log datasets. In the log retrieval task, Log Retriever achieves the highest accuracy among top-$1$, top-$5$, and top-$20$ over all datasets. On the HDFS dataset, the accuracy of random guesses is very low in the top $1$ (less than 10\%), which presents the quality and difficulty of this log retrieval task. We can find that our method has an accuracy score of more than 0.5 in the top 1 and of more than 0.7 in the top 5 and top 20. Compared to Edit Distance and Jaro Winkler methods, which don't take the semantic information into consideration, our method shows a higher accuracy. The results demonstrate that semantic information is crucial to log retrieval. For example, one question is \textit{``is the status of block blk\_5142679?''} and the correct log is \textit{``Verification succeeded for blk\_5142679''}. The word \textit{``succeeded''} can present the status of this block. But there is only one identical word between the question and the ground truth log, which may cause that similarity computed by Edit Distance or Jaro Winkler is small. We also observe that BERT Cosine performs worse than Edit Distance or Jaro Winkler. A possible explanation for this might be that BERT is trained in general natural language corpus and suffers from domain shift. 

A very similar conclusion emerges from the OpenSSH dataset. Log Retriever has an accuracy score of more than 0.5 in the top 1 and achieves the highest accuracy at top-$5$ and top-$20$. However, we observe that log retrieval is far more difficult in the Spark dataset. The accuracy of random guesses is less than 1\% in the top $1$. Edit Distance and BM25 cannot retrieve any correct log in the top $1$. A possible explanation for this might be that the Spark data is more diverse and contains more types of questions as shown in Table~\ref{table:type}. Our method achieves an accuracy score of 0.38 in the top $1$. Interestingly, we also observe that Log Retriever has achieved a good accuracy (0.85) at top-$20$. A possible explanation for these results may be that Log Retriever has high recall, but the recall rate is low facing various questions.

\subsubsection{Hard Negative Sampling}

As introduced in Section~\ref{sec:logretiever}, we proposed a hard negative sampling method to overcome the impact of overly easy negative examples. The Table~\ref{table:hardnega} presents the performance impact of hard negative sampling. The first row is the results of Log Retriever and the second row is results without hard negative sampling. We can see that hard negative sampling gives a 28\% performance improvement for HDFS top-$1$ setting and 24\% improvement for OpenSSH top-$1$ setting. 

The improvement in the Spark dataset is less than than the other two datasets. We can preserve 7\% of the accuracy gain through hard negative sampling.  A possible explanation for this might be that the Spark dataset is harder than the other two datasets, which leads to random negative samples that are also not overly simple. Therefore, the gain from hard negative sampling in Spark data would be less than HDFS and OpenSSH datasets.


\subsubsection{Hard Negative Example Weight} 
Based on Equation~\ref{equa:logretrieve_hard}, we incorporate the weighting of different negatives into our training. In this section, we investigate the performance impact of different hard negative example weights. We conduct this analysis experiment on the Spark dataset. The Figure~\ref{fig:hn_weight} shows that the Log Retriever model is not very sensitive to different hard negative example weights in the top $1$. Accuracy at top-$5$ varies widely according to different weights. The results demonstrate that we don't need to tune this hyper-parameter if we focus on Acc@1.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=8.5cm]{figures/hn_weight.pdf}
	\end{center}
	           %  \vspace{-4mm}
	\caption{Impact of different hard negative weights.}
	\label{fig:hn_weight}
	           %  \vspace{-4mm}
\end{figure}


\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=8.5cm]{figures/iter.pdf}
	\end{center}
	           %  \vspace{-4mm}
	\caption{Log retrieval accuracy with different iteration numbers.}
	\label{fig:iter_num}
	           %  \vspace{-4mm}
\end{figure}

\begin{table*}[t]
\renewcommand{\arraystretch}{1.8}
\setlength{\tabcolsep}{12pt}
\centering
\caption{Evaluation results of log reading.
    }
    \centering
    \label{table:logreader}
\begin{tabular}{|l|ll|ll|ll|}
\hline
                       & \multicolumn{2}{|c|}{\textbf{HDFS}}                 & \multicolumn{2}{c|}{\textbf{OpenSSH}}              & \multicolumn{2}{c|}{\textbf{Spark}}                \\ \hline \hline
                       & \textbf{EM} & \textbf{F1} & \textbf{EM} & \textbf{F1} & \textbf{EM} & \textbf{F1} \\ \hline
\textbf{Random Guess}                  & 0.0933         & 0.1126         & 0.0689          &  0.0919         & 0.0416         &  0.0722          \\
\textbf{Slide Window}           & 0.0133         & 0.0281         & 0.0689          & 0.0747        & 0.0250         &  0.1348        \\
\textbf{Logistic Regression}    &0.2266 & 0.2322 & 0.1896 & 0.2011 & 0.1083 & 0.1510 \\
\textbf{BiDAF}                  &0.3866 & 0.4133 & 0.3275 & 0.3353 & 0.1666 & 0.2532 \\
\textbf{QANet}                  &0.4012 & 0.4248 & 0.3125 & 0.3291 & 0.2000 & 0.3121 \\
\textbf{BERT}                  &0.4362 & 0.4523 & 0.4154 & 0.4251 & 0.2333 & 0.3210 \\
\textbf{Log Reader}             &\textbf{0.4933} & \textbf{0.4933} & \textbf{0.4310} & \textbf{0.4484} & \textbf{0.3000} & \textbf{0.4486}          \\ \hline
\end{tabular}
\end{table*}

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=8cm]{figures/hdfs_reader.pdf}
	\end{center}
	           %  \vspace{-4mm}
	\caption{Log Reader evaluation with different number of retrieved logs in HDFS dataset.}
	\label{fig:hdfs_layer}
	           %  \vspace{-4mm}
\end{figure}

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=8cm]{figures/openssh_reader.pdf}
	\end{center}
	           %  \vspace{-4mm}
	\caption{Log Reader evaluation with different number of retrieved logs in OpenSSH dataset.}
	\label{fig:openssh_layer}
	           %  \vspace{-4mm}
\end{figure}

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=8cm]{figures/spark_reader.pdf}
	\end{center}
	           %  \vspace{-4mm}
	\caption{Log Reader evaluation with different number of retrieved logs in Spark dataset.}
	\label{fig:spark_layer}
	           %  \vspace{-4mm}
\end{figure}


% \begin{figure*}
%     \centering
%     \subfloat[HDFS\label{fig:hdfs_layer}]{%
%        \includegraphics[width=0.32\linewidth]{figures/hdfs_reader.pdf}}
%     \hfill
%     \subfloat[OpenSSH\label{fig:openssh_layer}]{%
%        \includegraphics[width=0.32\linewidth]{figures/openssh_reader.pdf}}
%     \hfill
%     \subfloat[Spark\label{fig:spark_layer}]{%
%        \includegraphics[width=0.32\linewidth]{figures/spark_reader.pdf}}
%     \hfill
    
%     % \begin{subfigure}[t]{0.32\linewidth}
%     %     \centering
%     %     \includegraphics[width=\linewidth]{figures/hdfs_reader.pdf}
%     %     \caption{HDFS}\label{fig:hdfs_layer}
%     % \end{subfigure}
%     % ~
%     % \begin{subfigure}[t]{0.32\linewidth}
%     %     \centering
%     %     \includegraphics[width=\linewidth]{figures/openssh_reader.pdf}
%     %     \caption{OpenSSH}\label{fig:openssh_layer}
%     % \end{subfigure}
%     % ~
%     % \begin{subfigure}[t]{0.32\linewidth}
%     %     \centering
%     %     \includegraphics[width=\linewidth]{figures/spark_reader.pdf}
%     %     \caption{Spark}\label{fig:spark_layer}
%     % \end{subfigure}
%     \caption{Log Reader evaluation with different number of retrieved logs.}\label{fig:reader_topk}
% \end{figure*}



\subsubsection{Different Iteration Numbers}

As shown in Figure~\ref{fig:iter_num}, we increase the training iteration number while keeping the default values for others and observe its results over the Spark dataset. We can see that the accuracy of the Log Retriever model increases with the training iteration at the beginning. However, after five epochs, the performance of log retrieving declines. There are several possible explanations for this result. First, the model may have been overfitted by the training dataset due to the limited size. Second, after several training epochs, sampled hard negatives are difficult for the model to learn, which has negative impacts on the performance.


\subsection{Log Reader}

In this part, we first study the performance of Log Reader on the three log datasets.  Then we present the experiments with a different number of retrieved logs. Lastly, we investigate the impacts of different log retrieval models.


\subsubsection{Performance of Log Reader} In our experiments, we use Log Retriever to retrieve log candidates from raw log corpus and feed top-5 logs into log reading models. Table~\ref{table:logreader} shows the performance of Log Reader compared to six baseline methods over three log datasets. In the log reading task, our proposed model achieves the highest EM and F1 scores over three datasets. On the HDFS dataset, the EM of Random Guess is low, which implies that the log reading task is challenging. We can see that Slide Window has even worse EM and F1 than Random Guess. A possible explanation is that answers often have no overlap with the question. On the OpenSSH dataset, Log Reader achieves the best performance among those methods. We observe that the EM and F1 of our model on OpenSSH are lower than the ones on the HDFS dataset. As shown in Table~\ref{table:logre}, the accuracy of Log Retriever on OpenSSH is also 6 points lower than the accuracy on HDFS. This experiment demonstrates that achieving high performance on OpenSSH is more challenging than HDFS. The data in Spark dataset is more diverse and contains more types of questions than the other two datasets. Our method achieves an EM score of 0.3 and an F1 of 0.4486  on Spark dataset, respectively.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=8.5cm]{figures/different_retriever.pdf}
	\end{center}
	           %  \vspace{-4mm}
	\caption{Log reading accuracy with different log retriever models.}
	\label{fig:different_retriever}
	           %  \vspace{-4mm}
\end{figure}



\subsubsection{Different number of retrieved logs}
In this experiment, we explore how many retrieved logs examples are needed to achieve good log reading performance. Figure~\ref{fig:hdfs_layer}-\ref{fig:spark_layer} illustrates the performance of Log Reader with respect to different numbers of retrieved logs examples on three datasets. There is a different pattern among these three datasets. As shown, on the HDFS dataset, Log Reader performs the best when using only 5, 10, or 15 examples. On the other side, Log Reader achieves the best performance when using top-1 retrieved log on the OpenSSH dataset. On the Spark dataset, Log Reader performs relatively stable and its best results occur in the top-20 setting. This suggests that a different number of retrieved logs  should be used in different datasets. The number of retrieved logs has a major impact on the performance of log reading tasks. A proper number of retrieved logs can improve the log reading accuracy.




\subsubsection{Different log retriever models} In this part, we present the performance with different log retriever models. Figure~\ref{fig:different_retriever} demonstrates our Log Reader's performance results on the HDFS dataset, measured by EM with the retrieved logs from different log retrieval models. The left bar is an exact match score of Log Reader and the right is accuracy at top-1 of log retrieval models. We can see that a higher retriever accuracy typically leads to better final QA results. Edit Distance and Jaro Winkle achieve the same accuracy scores of 0.47, but answers extracted from the logs retrieved by Edit Distance are more likely to be correct.

We don't use retriever to generate log candidates and directly use logs that contain the correct answers as gold logs. We feed gold logs into the Log Reader. We can see the impact of gold log in Figure~\ref{fig:different_retriever}.  Our experiments on HDFS data show that gold log achieves the highest EM score and it outperforms Log Retriever in reading by almost 30 points. There is still plenty of room to improve the log retrieval.




% \subsection{Cross-dataset generalization}
% A high-performance data-driven model is desired and requires labeled data. However, this can result in increased expenses in modeling because of the effort required to obtain the labeled data. One interesting question regarding is how much performance degradation it may suffer when another setting is adopted? In other words, can it  generalize well when it is applied to a different dataset directly? 

% \begin{table}[]
% \renewcommand{\arraystretch}{1.8}
% \setlength{\tabcolsep}{12pt}
% \centering
% \caption{Cross-dataset performance on log retrieval.}
% \label{fig:cross_retriever}
% \begin{tabular}{llll}
% \hline
%         & \multicolumn{3}{c}{Target Dataset} \\ \hline
%         & HDFS      & OpenSSH    & Spark     \\ \hline
% HDFS    & 0.5850    & 0.1896     & 0.2333    \\
% OpenSSH & 0.2800    & 0.5172     & 0.0833    \\
% Spark   & 0.2266    & 0.3620     & 0.3830    \\ \hline
% \end{tabular}
% \end{table}

% To test the cross-dataset generalization, we train our model (including Log Retriever and Log Reader) on one dataset and test it on the two other datasets. Table~\ref{fig:cross_retriever} presents the cross-dataset performance on log retrieval task. Each row in the diagram corresponds to the accuracy at top-1 of the Log Retriever on different test datasets. For example, the second cell of the first row (0.1896) means that we train our model on HDFS datasets and test it on OpenSSH. We find that the Log Retriever model cannot generalize well, with a 20-30 points loss from the model trained using labeled data. A possible explanation for these results may be that generalization is limited by the size of labeled data and our model has been overfitted into its own dataset. One interesting finding is the model trained on Spark data performs the best on results generalization. These results are likely to be related to the diversity of labeled data.

% We also investigated the cross-dataset generalization on log reading tasks. In Table~\ref{fig:cross_reader}, we show the visualization of EM score in the cross-dataset setting. We observe that the cross-dataset generalization in log reading is more difficult than log retrieval. For example, we use the model trained on HDFS data to extract answers on the Spark test set. However, as shown in Table~\ref{table:type}, about 90\% of questions on HDFS are `what' type. It is extremely difficult for the model to answer unseen question types. Meanwhile, our results demonstrate that log reading models heavily rely on domain-related information. Domain adaption in log question answering remains a major challenge.




% \begin{table}[]
% \renewcommand{\arraystretch}{1.8}
% \setlength{\tabcolsep}{12pt}
% \centering
% \caption{Cross-dataset performance on log reading.}
% \label{fig:cross_reader}
% \begin{tabular}{llll}
% \hline
%         & \multicolumn{3}{c}{Target Dataset} \\ \hline
%         & HDFS      & OpenSSH    & Spark     \\ \hline
% HDFS    & 0.4933    & 0.2586     & 0         \\
% OpenSSH & 0.0800    & 0.4310     & 0.0167    \\
% Spark   & 0.0266    & 0.0517     & 0.3000    \\ \hline
% \end{tabular}
% \end{table}

% % \begin{figure}[ht]
% % 	\begin{center}
% % 		\includegraphics[width=6.cm]{figures/cross_retr.pdf}
% % 	\end{center}
% % 	\caption{Cross-dataset performance on log retrieval.}
% % 	\label{fig:cross_retriever}
% % \end{figure}

% % \begin{figure}[ht]
% % 	\begin{center}
% % 		\includegraphics[width=6.cm]{figures/cross_reader.pdf}
% % 	\end{center}
% % 	\caption{Cross-dataset performance on log reading.}
% % 	\label{fig:cross_reader}
% % \end{figure}




\subsection{Case Study}


\begin{table*}[t]
\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{12pt}
\centering
\caption{A case study of log retrieval on HDFS and Spark logs.
    }
    \centering
    \label{table:logretriever_example}
\begin{tabular}{ll}
\hline
\textbf{HDFS}          &                                                                                                  \\ \hline
Question      & What is the block that is receiving from 10.251.123.132:57542?                                   \\
Edit Distance & Verification succeeded for blk\_1150231966878829887                                              \\
Jaccard       & Receiving block blk\_1744485040428751334 src: /10.250.5.237:52234 dest:   /10.250.5.237:50010    \\
BM25          & PacketResponder 2 for block blk\_8229193803249955061 terminating                                 \\
Jaro Winkler  & PacketResponder 1 for block blk\_2151150262081352617 terminating                                 \\
BERT Cosine   & Received block blk\_155320394753274773 of size 67108864 from   /10.251.201.204                   \\
Log Retriever & Receiving block blk\_-28342503914935090 src: /10.251.123.132:57542 dest:   /10.251.123.132:50010 \\ \hline \hline
\textbf{Spark}         &                                                                                                  \\ \hline
Question      & How many ms did it take to read the broadcast variable 37?                                       \\
Edit Distance & Started reading broadcast variable 37                                                            \\
Jaccard       & Reading broadcast variable 3 took 17 ms                                                          \\
BM25          & Got assigned task 3                                                                              \\
Jaro Winkler  & Block broadcast\_0 stored as values in memory (estimated size 384.0 B,   free 317.5 KB)          \\
BERT Cosine   & Block broadcast\_8\_piece0 stored as bytes in memory (estimated size 21.4   KB, free 35.4 KB)    \\
Log Retriever & Reading broadcast variable 37 took 14 ms          \\ \hline \hline                                     
\end{tabular}
\end{table*}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[t]
\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{12pt}
\centering
\caption{A case study of log reading on  Spark logs.
    }
    \centering
    \label{table:logreader_example}
\begin{tabular}{ll}
\hline
Question            & What is the estimated size of   the block broadcast\_27?                                       \\ \hline
Top-5 Logs          & Block broadcast\_25 stored as values in memory (estimated size   10.1 KB, free 419.6 KB)       \\
                    & Block broadcast\_27 stored as values in memory (estimated size   9.2 KB, free 404.2 KB)        \\
                    & Block broadcast\_26 stored as values in memory (estimated size   9.7 KB, free 389.6 KB)        \\
                    & Block broadcast\_27\_piece0 stored as bytes in memory (estimated   size 5.4 KB, free 395.0 KB) \\
                    & Block broadcast\_28\_piece0 stored as bytes in memory (estimated   size 5.6 KB, free 409.8 KB) \\ \hline
Logistic Regression & Block broadcast\_25 stored as values in memory (estimated size 10.1 KB, free \textcolor{red}{\textbf{419.6}} KB)       \\
BiDAF               & Block broadcast\_27\_piece0 stored as bytes in memory (estimated   size \textcolor{red}{\textbf{5.4}} KB, free 395.0 KB)  \\
QANet               & Block broadcast\_25 stored as values in memory (estimated size \textcolor{red}{\textbf{10.1}} KB, free 41.96 KB)  \\
BERT               & Block broadcast\_27 stored as values in memory (estimated size   \textcolor{red}{\textbf{9.2}} KB, free 404.2 KB)   \\
Log Reader          & Block broadcast\_27 stored as values in memory (estimated size   \textcolor{red}{\textbf{9.2}} KB, free 404.2 KB)   \\      \hline                                                                                 
\end{tabular}
\end{table*}

In the Table~\ref{table:logretriever_example}, we show some log retrieval examples on HDFS and Spark datasets. For the HDFS data, we can find that only Jaccard, BERT Cosine, and Log Retriever can rank a very related log candidate (logs about `\textit{receiving}' events) at the top-1 position. In this case, IP addresses and IP ports are domain-specific symbols, Jaccard and BERT Cosine cannot identify these symbols. Another case is from the Spark dataset as shown in Table~\ref{table:logretriever_example}. The question is \textit{``How many ms did it take to read the broadcast variable 37?''}, where two key factors are the action \textit{read} and the broadcast variable \textit{37}. The Jaccard method mixed broadcast variables up and other baseline methods selected a wrong log event.

We also present the examples on the log reading task in Table~\ref{table:logreader_example}. In this case, the question is \textit{What is the estimated size of the block broadcast\_27?} and we use top-5 logs retrieved from Log Retriever as our log candidates. We can find that top-5 logs are very similar and relevant to the question, but only the second log is the correct log candidate. We present the results of Logistic Regression, BiDAF, QANet, BERT and Log Reader, where the red color denotes the predicted answer. One thing worth noticing is that Log Retriever provides a wrong log at top-1 position, but BERT and Log Reader are able to select the correct answer from the correct logs. It is because the probability of the selected answer in the correct log is higher than in the top-1 log. It proves the robustness of our proposed methods.

\section{Related Works}\label{sec:related}
A large number of QA benchmarks have been released in the past decade. As introduced in~\cite{zhu2021retrieving}, the information source of the datasets are mainly from Wikipedia, Search Engines (such as Bing, Google, and Baidu), Online News (such as CNN/DailyMail), and Internet Forum (such as Reddit and Stack Overflow). Different from general corpora, unstructured logs belong to specific domains. A log message is composed of a message header and message content. The message header is designed by the logging framework and has a fixed structure.  In contrast, the message content is mainly written by developers in free-form natural language~\cite{he2017towards}, including the fixed text written by the developers (e.g., \textit{Received block * of size * from *}) and the values of the program variables to carry dynamic run-time information. Practically, modern systems produce large volumes of logs. Consequently, it is much more difficult to find the correct answers in unstructured logs.

For purposes other than ours, other types of log question answering systems have been proposed. Burkhardt~\cite{burkhardt2016quark} proposed an architecture and a prototypical first partial implementation to answer customer questions in the telecommunication domain from several knowledge sources. The author extracted a set of FAQs (Frequently Asked Question) from the telecommunication logs. Given a new question, the system searches for the closest question from FAQs. Due to the ever-increasing variety of logs, this system cannot answer unknown questions. Vanessa al.~\cite{lopez2007aqualog} proposed an approach to leverage ontology-based semantic markup to answer the questions in unstructured logs. It takes queries expressed in natural language and ontology as input and returns answers drawn from knowledge bases, which are contracted from unstructured logs. However, maintaining the knowledge bases becomes increasingly difficult as the system logs grow.
 
\section{Conclusion}\label{conc}
In this paper, we proposed LogQA, a log-based question answering system. We decomposed LogQA into two steps: retrieve, and then read. Given a question, Log Retriever search for potentially helpful logs from a large log corpus. We designed an interactive training method to involve hard negatives to improve the performance of the similarity model. Given the top $k$ retrieved logs, Log Reader extracts the answer span and selects the answer with the highest probability. Due to the lack of a public question answering dataset, we manually labeled the above three public log datasets and will make them publicly available. We evaluated our proposed system on these log datasets. The results demonstrated that our method outperforms other baseline log-based methods.

One of the future directions of our work is to construct more complex questions like those requiring multi-hop reasoning. For example, given a question, the retriever aims to search for the relevant logs from a large corpus in multiple steps.

% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


% \appendices
% \section{Proof of the First Zonklar Equation}
% Appendix one text goes here.

% % you can choose not to have a title for an appendix
% % if you want by leaving the argument blank
% \section{}
% Appendix two text goes here.


% % use section* for acknowledgment
% \section*{Acknowledgment}



% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

% \section*{Acknowledgment} 
% This work is supported by National Key Research \& Development Program of China (Grant No: 2018YFB0204003), which belongs to the High Performance Computing Key Project. This research is also supported by National Natural Science Foundation of China (Grant No. 62072018). Zhongzhi Luan is the corresponding author for this paper.

\bibliography{tnsm_2020}
\bibliographystyle{IEEEtran}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/shaohanh.jpeg}}]{Shaohan Huang}
%  received the B.S. and M.S. degrees from Beihang University, Beijing, China. He is currently working toward the Ph.D degree. His current research interests include anomaly detection, text analytic, and natural
% language processing.
% \end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/yiliu.jpeg}}]{Yi Liu}
%  received the Ph.D. degree from the Department of Computer Science, Xi'an Jiaotong University, in 2000. He is currently a Professor with the School of Computer Science and Engineering and the Director of the Sino-German Joint Software Institute, Beihang University, China. His research interests include computer architecture, high-performance computing, and network technology.
% \end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/Carol.jpeg}}]{Carol Fung}
%  is an associate professor at Virginia Commonwealth University. Her research interests include collaborative intrusion detection networks, social networks, security issues inn mobile networks and medical systems, security issues in next generation networking, and machine learning in intrusion detection. She serves as an associate editor in multiple journals including IEEE transaction on Network and Service Management (TNSM) and Elsevier Computer Networks (COMNET). 
% \end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/ronghe.jpeg}}]{Rong He}
%  Master in Computer Science. Now she is working in Computer Network Information Center, Chinese Academy of Sciences and studying  in  University of Chinese Academy of Sciences. Her research intrests include grid computing and distributed  systems.
% \end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/yining.jpeg}}]{Yining Zhao}
% PhD in Computer Science. Currently working in Computer Network Information Center, Chinese Academy of Sciences. His research interest includes Distributed Computing and Data Analyses.
% \end{IEEEbiography}


% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/hailong.jpeg}}]{Hailong Yang}
% is an associate professor in School of Computer Science and Engineering, Beihang University. He received the Ph.D degree in the School of Computer Science and Engineering, Beihang University in 2014. He has been involved in several scientific projects such as performance analysis for big data systems and performance optimization for large scale applications. His research interests include parallel and distributed computing, HPC, performance optimization and energy efficiency. He is a member of China Computer Federation (CCF).
% \end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/zhongzhi.jpeg}}]{Zhongzhi Luan} IEEE member, associate professor of School of Computer Science and Engineering, Beihang University. He has been engaged in the teaching and research of distributed computing, high performance computing and computer architecture for many years. His main research interests include resource management in distributed environment, supporting methods and technologies for application development on heterogeneous computing systems, performance analysis and optimization of high performance computing applications, etc.
% \end{IEEEbiography}

% if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{John Doe}
% Biography text here.
% \end{IEEEbiographynophoto}


% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


