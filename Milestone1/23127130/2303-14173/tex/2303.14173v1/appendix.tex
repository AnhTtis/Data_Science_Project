\section{Derivations}
\label{sec:deriv}

\begin{proof}[Proof of \cref{lem:margin}]
Recall that our goal is to solve the constrained optimization problem
\begin{equation}
\label{eq:min-prob}
    \min\{\nrm{\delta}_p \, | \, \delta \in \RR^d, w^T (x + U \delta) + b=0\}
\end{equation}
Using the method of Lagrange multipliers, we know that a minimizer \(\delta \in \RR^d \) must satisfy the critical point condition
\begin{equation}
\label{eq:criticalpt}
    \lambda U^T w \in \partial \nrm{\delta}_p 
\end{equation}
where \( \partial \nrm{\delta}_p  \) is the subdifferential of the \(\ell^p \)-norm at \(\delta \in \RR^d \). It is a classical fact (see e.g. \cite[Prop. 1.2]{bachOptimizationSparsityInducingPenalties2011}) that
\begin{equation}
\label{eq:subdiff}
    \partial \nrm{\delta}_p = \begin{cases}
    \{v \in \RR^d \, |\, \nrm{v}_q \leq 1 \} & \text{if } \delta = 0 \\
    \{v \in \RR^d \, |\, \nrm{v}_q = 1  \text{ and } v^T \delta = \nrm{\delta}_p \} & \text{otherwise.}
    \end{cases}
\end{equation}
If the first case occurs, clearly the minimum of \cref{eq:min-prob} is \(0 \), and since \(\delta = 0 \) we obtain 
\begin{equation}
    0 = w^T (x + U \delta) + b = w^T x + b,
\end{equation}
hence in this case \(\frac{\nrm{w^T x + b}}{\nrm{w^T U}_q} = 0\) as well and the lemma holds. 

In the case where \(\delta \neq 0 \), combining \cref{eq:criticalpt,eq:subdiff} we see that for some \(\lambda \)
\begin{equation}
    \nrm{\lambda U^T w}_q = 1 \text{  and  } \lambda w^T U \delta = \nrm{\delta}_p;
\end{equation}
from the first equation we immediately identify \(\nrm{\lambda} = \nrm{U^T w}_q^{-1} \), and taking absolute values on both sides of the second then gives 
\begin{equation}
    \begin{split}
        \nrm{\delta}_p &= \nrm{\lambda} \nrm{w^T U \delta} \\
        &= \frac{ \nrm{w^T U \delta}}{\nrm{U^T w}_q}.
    \end{split}
\end{equation}
Finally, recalling \(w^T (x + U \delta) + b=0\) gives \(\nrm{w^T U \delta} = \nrm{w^T x + b} \), completing the proof.
\end{proof}

\section{Experimental details}
\label{sec:exp-det}

\subsection{Model architectures and training details}
\label{sec:training}

For our MNIST experiments we use the simple 2 layer convolutional network of
\cite{madry2018towards} --- we ported the TensorFlow code available at
\href{https://github.com/MadryLab/mnist_challenge}{https://github.com/MadryLab/mnist\_challenge}
to PyTorch \cite{torch}. We train it using SGD with momentum 0.9, batch size
\(1024\) and weight
decay \(10^{-4}\) for 100 epochs, with initial learning rate \(10^{-3}\) and
learning rate drops by a factor of \(0.1\) whenever validation accuracy doesn't
improve by \(1\%\) for 10 epochs. We save the weights with the best validation
accuracy (\(\approx 98.95 \%\)).

For our CIFAR10 experiments we use the ResNet9 from MosaicML's Composer library
\cite{mosaicml2022composer}. We train it with SGD with momentum 0.9, batch size
\(512\) and weight decay \(10^{-4}\) for 160 epochs, with initial learning rate \(10^{-1}\) and
learning rate drops by a factor of \(0.1\) whenever validation accuracy doesn't
improve by \(1\%\) for 10 epochs. We save the weights with the best validation
accuracy (\(\approx 91.72 \%\)). 

For our ImageNet experiments we use the ResNet50 from TorchVision
\cite{torchvision}. We train it with SGD with momentum 0.9 and weight decay
\(10^{-4}\) for 100 epochs, with initial learning rate \(1.0\) and learning rate
drops by a factor of \(0.1\) whenever validation accuracy doesn't improve by
\(1\%\) for 10 epochs. Due to distributed data parallel training with batches of
size 512 on each of 8 GPUs, our effective batch size is \(8 \cdot 512 = 4096\).
We save the weights with the best validation accuracy (\(\approx 72.84 \%\)). 

\subsection{Tuning PGD step sizes}

In our experiments we generate a large number of PGD adversarial examples for a
wide range of perturbation constraints \(\epsilon\) and in subspaces of varying
dimension. In order for our numerical experiments to address our questions about
the behavior of \( \success(d, \epsilon) \), it is \emph{crucial} that our PGD
algorithm for optimizing \(\delta\) has the capacity to achieve the boundary
case \(\nrm{\delta}_p = \epsilon\). We found that with some standard choices of
step size, this did not occur, resulting in an unpleasant situation where the
effective budget was significantly lower than \(\epsilon\) simply due to a
too-small PGD step size. Here we briefly discuss a principled choice of PGD step
size that accounts for the dimension \(d\) of the subspace to which \(\delta\)
is constrained. First we must specify the PGD algorithm being used.

Our basic PGD implementation (adapted from \cite{madry2018towards}) iterates the
following: we constrain \(\delta \) to a \(d\)-dimensional subspace \(V
\subseteq \cX \) using an isometry \(U: \RR^d \to V \) as in
\cref{sec:comp-w-thry}, and  initialize \(\delta_0 = 0\) 
% by sampling from the
% \(\epsilon\)-ball around \(0\) (uniformly in the case where \(p=\infty\), and
% from a Gaussian distribution with standard deviation \(\frac{\epsilon}{2}\) when
% \(p=2\))
. 
Then, for \(t = 1, \dots, T\) where \(T \) is the maximum number of
steps, we let \(g_t = \nabla_{\delta} \ell(f(x+ U \delta), y)\) where \(\ell \) is
cross entropy, and replace it with the ``normalized'' gradient
\begin{equation}
\label{eq:fgsm}
    \tilde{g}_t := \begin{cases}
        \frac{g_t}{\nrm{g_t}_p}, & p\in \{1,2\} \\
        \sign{g_t}, & p=\infty.
    \end{cases}
\end{equation}
We then project \(\delta_{t-1} + \eta \tilde{g}_t \), where \(\eta\) is a
learning rate, onto the \(\ell^p\) \(\epsilon\)-ball centered at \(0\) to obtain
in the case \(p\in \{1,2\}\)
\begin{equation}
    \delta_{t} = \begin{cases}
        \epsilon \frac{\delta_{t-1} + \eta \tilde{g}_t}{\nrm{\delta_{t-1} + \eta \tilde{g}_t}_p}, & \nrm{\delta_{t-1} + \eta \tilde{g}_t}_p > \epsilon \\
        \delta_{t-1} + \eta \tilde{g}_t, & \text{otherwise}
    \end{cases}
\end{equation}
and in the case \(p=\infty\)
\begin{equation}
    \delta_{t} = \begin{cases}
        \epsilon \frac{\delta_{t-1} + \eta \tilde{g}_t}{\max\{\delta_{t-1} + \eta \tilde{g}_t\}}, & \max\{\delta_{t-1} + \eta \tilde{g}_t\} > \epsilon \\
        \delta_{t-1} + \eta \tilde{g}_t, & \text{otherwise}.
    \end{cases}
\end{equation}
Finally, we must ensure that \(x + U\delta\) lies in the image hypercube \([0,
1]^{C \times H \times W}\) (in our implementation pixel values lie in \([0,
1]\)). To do this, we let \(\clip: \RR \to [0,1]\) be the clipping
function, i.e. \(\clip(x) = \max\{0, \min\{x, 1\}\}\), and replace \(\delta
\) with 
\begin{equation}
    U^T (\clip(x + U \delta) - x)
\end{equation}
(here we use the fact that \(U\) is orthogonal and so \(U^T\) is a left inverse
for \(U\)).

To set the step size \(\eta\), we can adopt the heuristic that the \(\delta_t\)
behave like a random walk, i.e. that the normalized gradients \(\tilde{g}_t \)
are sampled IID from some distribution (this almost certainly quite false, but
we found it to be useful in a back-of-the-envelope sort of way). We will even
further assume  that for each \(t \) the coordinates of \(\tilde{g}_t \) are
IID. Ignoring projection and clipping, we have \(\delta_T = \eta \sum_{t=1}^T
\tilde{g}_t\). In the case \(p=2\), using the supposed IID-ness we see that 
\begin{equation}
    E[\nrm{\delta}_2^2] = \eta^2 \sum_{t=1}^T \nrm{\tilde{g}_t}_2^2 = \eta^2 T.
\end{equation}
Since we want to ensure the left hand side is at least \(\epsilon\), we obtain
the step size
\begin{equation}
    \eta = \frac{\epsilon}{\sqrt{T}}.
\end{equation}
In practice we multiply the above by 2 for good measure.\footnote{Note
that this is \emph{larger} than what is suggested in \cite[p. 12, section
``Resistance for different values ...'']{madry2018towards}, which divides by
\(T\). By ``for good measure'' we mean that our primary concern is using \emph{too small} of a step size.} In the case \(p=\infty\),  by our IID-ness assumptions and the fact
that by definition \(\tilde{g}_t = \sign{g_t}\), the individual coordinates
\(\tilde{g}_{t, j}\) for \(j = 1,\dots, d\) are IID samples from \(\{\pm 1\}\)
and so 
\begin{equation}
    \label{eq:max-thing}
    \nrm{\delta_T}_{\infty} =\eta \sqrt{T} \max_{j=1,\dots, d} \{\nrm{\frac{1}{\sqrt{T}}\sum_{t=1}^T \tilde{g}_{t, j}}\}
\end{equation}
The distribution of each term \(\frac{1}{\sqrt{T}}\sum_{t=1}^T \tilde{g}_{t,
j}\) tends towards a Gaussian distribution with mean 0 and variance \(1\) by the
central limit theorem.
--- letting \(\Phi(x)\) be the standard normal CDF, the CDF of each
 \(\nrm{\frac{1}{\sqrt{T}}\sum_{t=1}^T \tilde{g}_{t, j}}\) is approximated by
\begin{equation}
    \label{eq:cdf}
    F(x):=\Phi(x) - \Phi(-x).
\end{equation}
By \cite{bahadurNoteQuantilesLarge1966}, the
distribution of the max occuring in \cref{eq:max-thing} is concentrated around
the \(\frac{d-1}{d}\)-th quantile of the CDF \cref{eq:cdf}, i.e.
\(F^{-1}(1-\frac{1}{d})\). Assuming \(d \) is relatively large, so that
\(1-\frac{1}{d}\) is near \(1\), we ignore the \(\Phi(-x)\) term in
\cref{eq:cdf} for the purposes of inversion and get
\begin{equation}
    F^{-1}(1-\frac{1}{d}) \approx \Phi^{-1}(1-\frac{1}{d})
\end{equation}

Recall that our objective is to ensure that
\(\nrm{\delta_T}_{\infty} \geq \epsilon\). By the above arguments, this
translates to 
\begin{equation}
    \label{eq:linf-step-size}
    \begin{split}
        \epsilon &\leq \eta \sqrt{T} \Phi^{-1}(1-\frac{1}{d}),  \text{  i.e. }\\
        \eta &\approx \frac{\epsilon}{\sqrt{T} \Phi^{-1}(1-\frac{1}{d})}.
    \end{split}
\end{equation}
Again, in practice we multiply by 2 for good measure. Observe that while our
\(\ell^2\) step size is independent of \(d\), \cref{eq:linf-step-size} does
depend on \(d\). In fact, as \(d \to \infty\) the step size goes to \(0\), but
very slowly (for \(d = 3\cdot 224^2\), the dimension of ImageNet images,
\(\Phi^{-1}(1-\frac{1}{d}) \approx 4.36\)). 

For the \(p=1\) case, we use a heuristic similar to that of \(p=2\) above; explicitly, we set 
\[ \eta = \sqrt{2\pi } \frac{\epsilon}{\sqrt{T}}  \]

\subsection{Adversarial example generation}

For each dataset and model, we select a range of perturbation budgets
\(\epsilon\) and subspace dimensions \(d\), in both cases logarithmically spaced
between minimum and maximal values of \(\epsilon\) and \(d\), with as many
grid points as we can afford (for MNIST and CIFAR, 32 different values of each,
for ImageNet only 8 of each). 

For each pair \((\epsilon, d)\) we loop over the entire validation set of the
relevant dataset, with the exception of ImageNet where we randomly sample 10,000
of the 50,000 images. We randomly sample a distinct subspace \(V_i \subset \cX\)
for each validation datapoint \((x_i, y_i) \) (as above, by randomly generating
a matrix \(U\) whose columns span \(V\)). We then loop through validation
datapoints  \((x_i, y_i) \) and corresponding matrices \(U_i\) and compute PGD
adversarial examples as described above, with \(T=16\) steps. We compute the
error over the  validation set (subsampled in the case of ImageNet), i.e. 
\[ 1 - \frac{1}{N} \sum_{i=1}^N \mathbf{1}(f(x_i + U\delta_i)=y_i). \]

\begin{remark}
    We intended to permute the matrices \(U_i\) accross several runs through the
    validation dataset \(\{(x_i, y_i)\}\) to provide error bars with respect to
    the distribution of subspaces. Due to a coding oversight, they didn't
    actually get permuted. For this reason our plots lack confidence intervals,
    however we hope that the tight agreement of all curves after our \(x\)-axis
    reparametrization illustrates that such error bars would be quite small. 
\end{remark}

\subsection{Subspaces sampled uniformly from the Grassmannian}

In the case of MNIST and for \(p=2\), we can also sample subspaces uniformly from the
Grassmannian \(\Gr(d, \dim \cX)\) by sampling matrices \(U\) of shape \(n\times
d\) with orthonormal columns using the QR decomposition as in used in the method
\texttt{scipy.stats.ortho\_group} of \cite{2020SciPy-NMeth}. The results, shown
in \cref{fig:mn2}, are similar to those in \cref{fig:infty-a,fig:infty-b}. 
\begin{figure*}[tb]
    \centering
   \begin{subfigure}{0.45\linewidth}
      \centering
      \includegraphics[width=\linewidth]{plots/plot_sweep_xlog/mnist-2-nrm.png}
      \caption[]{}\label{fig:mn2-a}
   \end{subfigure}
   \begin{subfigure}{0.45\linewidth}
      \centering
      \includegraphics[width=\linewidth]{plots/plot_sweep_xlog/mnist-2-nrm-reparam.png}
      \caption[]{}\label{fig:mn2-b}
   \end{subfigure}
   \caption[]{\textbf{(a)} Projected gradient descent (PGD)
   adversarial examples for the 2-layer convolutional neural network of
   \cite{madry2018towards} on the MNIST validation set. In these experiments we
   generated the subspaces \(V\) by sampling uniformly from the Grassmannian, and
   constrain perturbations using the \(\ell^2 \) norm. Error bars represent
   standard deviations of 5 passes over the MNIST validation set. \textbf{(b)}
   The same data,  reparametrized by plotting \(\epsilon \cdot \sqrt{\frac{\dim
   V}{\dim \cX}}\) along the \(x\)-axis. \emph{Note}:
   \(x\)-axes are log-scale.}
   \label{fig:mn2}
\end{figure*}

\subsection{Analysis of the 1-norm case}
\label{sec:an1nrm}
\cwg{TODO: new 1 norm plots, modify analysis as necessary}
When \(p = 1 \), \( q = \infty \) and so the arguments used in \cref{eq:claim2,eq:claim3} do not make sense as written; moreover, while we have not explicitly verified this, it seems that attempting to take a limit of those equations as \(q \to \infty \) one will encounter an ``\(\infty/\infty\)'' case, and it's not clear that e.g. L'Hospital's rule helps at all.

Instead, we propose a different estimate of the quotient 
\begin{equation}
    \frac{\nrm{w^T U}_\infty}{\nrm{w}_\infty}, 
\end{equation}
proceeding as follows. We will again assume, as is the case in our experiments, that \(U \) is obtained by subsampling basis vectors, say \(\{e_{i_1}, \dots, e_{i_d}\}\). Then 
\begin{equation}
    \frac{\nrm{w^T U}_\infty}{\nrm{w}_\infty} = \frac{\max_j \{ \nrm{w_{i_j}} \}}{\max_i \{ \nrm{w_{i}} \}}
\end{equation}
The question, then, is how much smaller the \( \max \) over a random \(\dim V \)-element subset of the absolute values \(\nrm{w_i}\) is than the \( \max \) over all \( \dim \cX \) of them. The need to make some assumption on the distribution the \(\nrm{w_i}\) are drawn from seems unavoidable at this point: we suppose the coefficients \(w_i \) come from a standard normal distribution, so that their absolute values come from a ``half-normal'' (equivalently \( \chi_1\)) distribution: if \(\Phi \) is the standard normal cumulative distribution function, with this assumption the  cumulative distribution function of the \(\nrm{w_i}\) is
\begin{equation}
    F(x):= \Phi(x) - \Phi(-x).
\end{equation}
We make a further crude estimate that the numerator and denominator are \(\max\)s of \emph{independent} samples of size \(\dim V \) and \(\dim \cX \) respectively;\footnote{This is of course quite false, as the numerator differs from the denominator by taking the max over a subsample. How can one deal with this step more realistically?} then the theory of quantiles in large samples \cite{bahadurNoteQuantilesLarge1966} suggests the estimates 
\begin{equation}
\begin{split}
    \max_j \{ \nrm{w_{i_j}} \} &\approx F^{-1}(1 - \frac{1}{\dim V}) \text{  and} \\
    \max_i \{ \nrm{w_{i}} \} &\approx F^{-1}(1 - \frac{1}{\dim \cX}) 
\end{split}
\end{equation}
leading to the overall estimate 
\begin{equation}
\label{eq:best-guess-l1}
    \frac{\nrm{w^T U}_\infty}{\nrm{w}_\infty} \approx \frac{F^{-1}(1 - \frac{1}{\dim V})}{F^{-1}(1 - \frac{1}{\dim \cX})}
\end{equation}
\begin{figure*}[tb]
    \centering
   \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/plot_sweep_xlog/mnist-1-nrm.png}
        \caption[]{}\label{fig:mnist-1-a}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/plot_sweep_xlog/mnist-1-nrm-reparam.png}
        \caption[]{}\label{fig:mnist-1-b}
    \end{subfigure}
    \caption{\textbf{(a)} Success of PGD adversarial attacks on an MNIST trained small convolutional network, with $\ell^1$-norm constraints on perturbation budget, constrained to subspaces \(V \subseteq \cX\) spanned by \(\dim V\) randomly selected standard basis vectors. Adversarial examples are computed for all datapoints in the MNIST validation set. The $x$-axis is the $\epsilon$-bound used during example generation and the different colored curves indicate the dimension \(\dim V\) of the subspace to which the examples were constrained to, relative to the dimension \(\dim \cX \) (\(=28^2\)) of the ambient space. \textbf{(b)} These curves become \emph{more} aligned when we reparameterize the $x$-axis by scaling by $\frac{F(1-\frac{1}{\dim V})}{F(1-\frac{1}{\dim \cX})}$, where \(F\) is the cummulative distribution function of the absolute value of a standard normal random variable.}
   \label{fig-L1-MNIST}
\end{figure*}
\Cref{fig-L1-MNIST} shows the result of using \cref{eq:best-guess-l1} as a stand-in for \( (\frac{\dim V}{\dim \cX})^{\frac{1}{q}} \) in the case where \(p=1 \), on the MNIST dataset. One immediate observation is that at least for \(\dim V \) a significant fraction of \(\dim \cX \) (e.g. \(\frac{\dim V}{\dim \cX} \geq 0.1 \)) it does appear that the curves \( \success(\dim V, \epsilon ) \) converge to a common limit, as one would expect from a na\"ive application of a factorization \(\success(\dim V, \epsilon ) = g(\epsilon (\frac{\dim V}{\dim \cX})^{\frac{1}{\infty}}) = g(\epsilon) \). The reparameterization of \cref{eq:best-guess-l1} seems to do okay at accounting for behavior in the lowest dimensions, at the expense of over-compensating and pushing the curves corresponding to low-to-medium values of \(\dim V \) to the left of the curve corresponding to \(\dim V =\dim \cX \). There are various potential causes of this undesirable effect (roughly one per crude oversimplification in the above analysis). Results for CIFAR10 and ImageNet can be found in \cref{fig-L1-cifar} and \cref{fig-L1-imagenet} respectively. One concerning aspect of those two results is we see downturns in the curves \(\success(\dim V, \epsilon )  \) for the highest values of \(\epsilon \), suggesting there may have been issues with our PGD optimizer in the \(p=1\) case.  

One question we had was whether these results were impacted by sub-optimal PGD optimization. A reason for asking this is that the \(p=1\) case of \cref{eq:fgsm} is arguably incorrect: the ``correct'' way of deriving these generalized Fast Gradient Sign Method (FGSM) steps is through the analysis of \cref{sec:deriv}. Assuming \(U = I\) for simplicity, one sees that \(w^T \delta = \nrm{w}_\infty \nrm{\delta}_1 \), and one can show this occurs if and only if:
\begin{itemize}
    \item letting \(A = \mathrm{arg} \max_i \{\nrm{w_i}\} \subseteq \{1,\dots, n\} \) (the argmax of \(\nrm{w_i}\), which is a set in general although a single index with probability 1), \(\delta_i = 0 \) if \(i \notin A \).
    \item \(\sign \delta_i = \sign w_i \) for all \(i \in A \).
\end{itemize}
In the case where \(A = \{a\} \) (i.e. \(\nrm{w_i}\) has a unique maximum) we obtain the simplified solution \(\delta = (c \sign w_i) e_a \) (where \(e_a \) is the \(a\)-th standard basis vector. Hence for \(p=1 \), one can argue that we should use 
\begin{equation}
\label{eq:fgsm-1}
    \tilde{g}_t := e_{\mathrm{arg} \max_i \{\nrm{w_i}\}}
\end{equation}
We found that while this method performed similarly to that of \cref{eq:fgsm} for small values of \(\epsilon \), it struggled for large \(\epsilon \) and failed at the scale of ImageNet input space (see). A reasonable suspicion is that the number of basis directions selected by \cref{eq:fgsm-1} is bounded by the number of PGD iterations, and that when this number of iterations is far is smaller than the input dimension \cref{eq:fgsm-1} underexplores. However we leave further analysis to future work.

\section{Additional experimental results}

\begin{figure*}[htb]
    \centering
   \begin{subfigure}{0.45\linewidth}
      \centering
      \includegraphics[width=\linewidth]{plots/plot_sweep_xlog/mnist-infty-nrm.png}
      \caption[]{}\label{fig:infty-a}
   \end{subfigure}
   \begin{subfigure}{0.45\linewidth}
      \centering
      \includegraphics[width=\linewidth]{plots/plot_sweep_xlog/mnist-infty-nrm-reparam.png}
      \caption[]{}\label{fig:infty-b}
   \end{subfigure}
   \caption{Plot for experiments analogous to those found in Figure \ref{fig:infty} but run with a 2-layer CNN trained and evaluated on MNIST.}
   \label{fig:infty-MNIST}
\end{figure*}

\begin{figure*}[htb]
    \centering
   \begin{subfigure}{0.45\linewidth}
      \centering
      \includegraphics[width=\linewidth]{plots/plot_sweep_xlog/cifar-infty-nrm.png}
      \caption[]{}\label{fig:infty-c}
   \end{subfigure}
   \begin{subfigure}{0.45\linewidth}
      \centering
      \includegraphics[width=\linewidth]{plots/plot_sweep_xlog/cifar-infty-nrm-reparam.png}
      \caption[]{}\label{fig:infty-d}
   \end{subfigure}
   \caption{Plot for experiments analogous to those found in Figure \ref{fig:infty} but run with a ResNet9 trained and evaluated on CIFAR10.}
   \label{fig:infty-CIFAR}
\end{figure*}

\begin{figure*}[tb]
    \centering
   \begin{subfigure}{0.45\linewidth}
      \centering
      \includegraphics[width=\linewidth]{plots/plot_sweep_xlog/mnist-2-nrm.png}
      \caption[]{}\label{fig:2-a}
   \end{subfigure}
   \begin{subfigure}{0.45\linewidth}
      \centering
      \includegraphics[width=\linewidth]{plots/plot_sweep_xlog/mnist-2-nrm-reparam.png}
      \caption[]{}\label{fig:2-b}
   \end{subfigure}
   \caption[]{Plot for experiments analogous to those found in Figure \ref{fig-L2-imagenet} but run with a 2-layer CNN trained and evaluated on MNIST.}
   \label{fig-L2-MNIST}
\end{figure*}

\begin{figure*}[tb]
    \centering
   \begin{subfigure}{0.45\linewidth}
      \centering
      \includegraphics[width=\linewidth]{plots/plot_sweep_xlog/cifar-2-nrm.png}
      \caption[]{}\label{fig:2-c}
   \end{subfigure}
   \begin{subfigure}{0.45\linewidth}
      \centering
      \includegraphics[width=\linewidth]{plots/plot_sweep_xlog/cifar-2-nrm-reparam.png}
      \caption[]{}\label{fig:2-d}
   \end{subfigure}
   \caption[]{
   Plot for experiments analogous to those found in Figure \ref{fig-L2-imagenet} but run with a ResNet9 trained and evaluated on CIFAR10.}
   \label{fig-L2-CIFAR10}
\end{figure*}

\begin{figure*}[tb]
    \centering
   \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/plot_sweep_xlog/cifar-1-nrm.png}
        \caption[]{}\label{fig:cifar-1-a}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/plot_sweep_xlog/cifar-1-nrm-reparam.png}
        \caption[]{}\label{fig:cifar-1-b}
    \end{subfigure}
    \caption{Plot for experiments analogous to those found in Figure \ref{fig-L1-MNIST} but run with a ResNet9 trained and evaluated on CIFAR10.}
   \label{fig-L1-cifar}
\end{figure*}

\begin{figure*}[tb]
    \centering
   \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/plot_sweep_xlog/imagenet-1-nrm.png}
        \caption[]{}\label{fig:1-e}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/plot_sweep_xlog/imagenet-1-nrm-reparam.png}
        \caption[]{}\label{fig:1-f}
    \end{subfigure}
    \caption{Plot for experiments analogous to those found in \cref{fig-L1-MNIST} but run with a ResNet50 trained and evaluated on ImageNet.}
   \label{fig-L1-imagenet}
\end{figure*}

\begin{figure*}[tb]
    \centering
   \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/plot_sweep_NEW/mnist-1-nrm.png}
        \caption[]{}\label{fig:mnist-1-a-new}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/plot_sweep_NEW/mnist-1-nrm-reparam.png}
        \caption[]{}\label{fig:mnist-1-b-new}
    \end{subfigure}
    \caption{Plot for experiments analogous to those found in Figure \ref{fig-L1-MNIST}, the only difference being that we use the FGSM step of \cref{eq:fgsm-1}.}
   \label{fig-L1-MNIST-new}
\end{figure*}

\begin{figure*}[tb]
    \centering
   \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/plot_sweep_NEW/cifar-1-nrm.png}
        \caption[]{}\label{fig:cifar-1-a-new}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/plot_sweep_NEW/cifar-1-nrm-reparam.png}
        \caption[]{}\label{fig:cifar-1-b-new}
    \end{subfigure}
    \caption{Plot for experiments analogous to those found in Figure \ref{fig-L1-MNIST} but run with a ResNet9 trained and evaluated on CIFAR10, using the FGSM step of \cref{eq:fgsm-1}.}
   \label{fig-L1-cifar-new}
\end{figure*}

\begin{figure*}[tb]
    \centering
   \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/plot_sweep_NEW/imagenet-1-nrm.png}
        \caption[]{}\label{fig:1-e-new}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/plot_sweep_NEW/imagenet-1-nrm-reparam.png}
        \caption[]{}\label{fig:1-f-new}
    \end{subfigure}
    \caption{Plot for experiments analogous to those found in \cref{fig-L1-MNIST} but run with a ResNet50 trained and evaluated on ImageNet, using the FGSM step of \cref{eq:fgsm-1}.}
   \label{fig-L1-imagenet-new}
\end{figure*}