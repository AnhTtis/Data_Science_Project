\documentclass[conference]{IEEEtran}

\usepackage{times}

\usepackage[font={small}]{caption}
\usepackage{mwe}
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}
\usepackage{color}
\let\labelindent\relax
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{
tikz,
relsize,
booktabs
}
\usepackage{algorithmic}

\renewcommand\thefigure{\arabic{figure}}
\setcounter{figure}{0}


\newcommand{\BE}[1]{{\xxnote{BE}{red}{#1}}}
\newcommand{\LP}[1]{{\xxnote{LP}{blue}{#1}}}
\newcommand{\xxnote}[3]{}
\ifx\hidenotes\undefined
  \renewcommand{\xxnote}[3]{\color{#2}{#1: #3}}
  % \renewcommand{\xxnote}[3]{}
\fi

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    citecolor=blue,
    urlcolor=teal,
}


\pdfinfo{
   /Author (Homer Simpson)
   /Title  (Robots: Our new overlords)
   /CreationDate (D:20101201120000)
   /Subject (Robots)
   /Keywords (Robots;Overlords)
}


\newcommand{\method}{\textsc{T-Dex}}
\newcommand{\website}{\url{tactile-dexterity.github.io}}

\begin{document}

\title{Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play}

\author{\authorblockN{Irmak Guzey}
\authorblockA{New York University}
\and
\authorblockN{Ben Evans}
\authorblockA{New York University}
\and
\authorblockN{Soumith Chintala}
\authorblockA{Meta AI}
\and
\authorblockN{Lerrel Pinto}
\authorblockA{New York University}
}


\maketitle

\begin{abstract}
Teaching dexterity to multi-fingered robots has been a longstanding challenge in robotics. Most prominent work in this area focuses on learning controllers or policies that either operate on visual observations or state estimates derived from vision. However, such methods perform poorly on fine-grained manipulation tasks that require reasoning about contact forces or about objects occluded by the hand itself. In this work, we present \method{}, a new approach for tactile-based dexterity, that operates in two phases. In the first phase, we collect 2.5 hours of play data, which is used to train self-supervised tactile encoders. This is necessary to bring high-dimensional tactile readings to a lower-dimensional embedding. In the second phase, given a handful of demonstrations for a dexterous task, we learn non-parametric policies that combine the tactile observations with visual ones. Across five challenging dexterous tasks, we show that our tactile-based dexterity models outperform purely vision and torque-based models by an average of 1.7X.
Finally, we provide a detailed analysis on factors critical to \method{} including the importance of play data, architectures, and representation learning.
\end{abstract}

\IEEEpeerreviewmaketitle


\input{introduction.tex}

\input{related_work.tex}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/robot_setup.pdf}
    \caption{Hardware setting of \method{}. We use an Oculus Headset to teleoperate the Allegro hand and the built in Kinova joystick to control the arm. Visual observations are streamed through two different Realsense cameras and tactile observations are saved with XELA touch sensors on the Allegro hand.}
    \label{fig:robot_setup}
\end{figure}


\input{hardware.tex}

\input{method.tex}

\input{experiments.tex}

\section{Limitations and Conclusion} 
\label{sec:conclusion}
In this work, we have presented an approach for tactile-based dexterity (\method{}) that combines tactile pretraining on play data along with efficient downstream learning on a small amount of task-specific data. Our results indicate that \method{} can significantly improve over prior approaches that use images, torque and tactile data. However, we recognize two key limitations. Although \method{} succeeds on several out-of-distribution examples, the success rate is lower than the training object. The second, is that our approach is currently limited to offline imitation, which limits the ability of our policies to learn from failures. Both limitations could be addressed by integrating online learning, improving architectures for tactile data, and better tactile-vision fusion algorithms. While these aspects are out of scope to this work, we hope that the ideas introduced in \method{} can spur future work in these directions. 


\section*{Acknowledgments}
We thank Vaibhav Mathur, Jeff Cui, Ilija Radosavovic, Wenzhen Yuan and Chris Paxton for valuable feedback and discussions. This work was supported by grants from Honda, Meta, Amazon, and ONR awards N00014-21-1-2758 and N00014-22-1-2773.

\bibliographystyle{plainnat}
\bibliography{ref}

\input{appendix.tex}

\end{document}


