\section{Introduction}

Humans are able to solve novel and complex manipulation tasks with small amounts of real-world experience. Much of this ability can be attributed to our hands, which allow for redundant contacts and multi-finger manipulation. Endowing multi-fingered robotic hands such dexterous capabilities has been a long-standing problem, with approaches ranging from physics-based control~\cite{kumar2016dext} to simulation to real (sim2real) learning~\cite{openai2019learning, Openai2019}. More recently, the prevalence of improved hand-pose estimators has enabled imitation learning approaches to teach dexterity, which in turn improves sample efficiency and reduces the need for precise object and scene modelling~\cite{arunachalam2022dexterous, holodex, DexPilot}.


Even with improved algorithms, teaching dexterous skills is still quite inefficient, requiring hours of demonstration data for imitation or days of training for sim2real~\cite{holodex, openai2019learning}. While algorithmic improvements in control will inevitably lead to improvements in dexterity over time, an often overlooked source of improvement lies in the sensing modality. Current dexterous robots either use high-dimensional visual data or compact states estimated from them. Both suffer significantly either when the task requires reasoning about contact forces, or when the fingers occlude the object being manipulated. In contrast to vision, tactile sensing provides rich contact information while being unaffected by occlusions.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/tdex_intro.pdf}
    \caption{\method{} learns dexterous policies from high-dimensional tactile sensors on a multi-fingered robot hand (top). Combined with vision, our tactile representations are crucial to learn fine-grained manipulation tasks (bottom).}
    \label{fig:figure_1}
\end{figure}

The importance of touch is most evident in human dexterity. Touch is the first sense developed in babies, at as little as 7 weeks in utero~\cite{Fagard2018} and newborn babies have grasping reflexes when touched~\cite{Schott2003}. Moreover, adult humans who have the sense of touch on their hands artificially disabled have reduced dexterity~\cite{johansson1992somatosensory}, highlighting the necessity of tactile feedback. Many works have sought to replicate this sensing modality for robotics, with impressive successes in object identification, pose estimation, and policy learning. Simultaneously, the cost of such sensors is becoming increasingly affordable. However, the application of tactile sensing to multi-fingered dexterous manipulation remains limited.

So, why is tactile-based dexterity hard to achieve? There are three significant challenges. First, tactile sensors are difficult to simulate, which limits the applicability of sim2real based methods~\cite{openai2019learning, wang2021elastic}. Second, for many commonly available tactile sensors, precise calibration of analog readings to physical forces is difficult to achieve~\cite{lee2020calibrating}. This limits the applicability of physics-based control. Third, for multi-fingered hands, tactile sensors need to cover a larger area compared to two-fingered hands. This increases the dimensionality of the tactile observation, which in turn makes learning-based approaches inefficient. A common approach to alleviate this challenge in vision-based learning is to use pretrained models that encode high-dimensional images to low-dimensional representation. However, such pretrained models do not exist for tactile data.


In this work, we present \method{}, a new approach to teach tactile-based dexterous skills on multi-fingered robot hands. To overcome issues in simulating tactile sensors and calibration, we use an imitation framework that trains directly using raw tactile data obtained from a human operator teleoperating the robot. However, directly reasoning about actions from raw tactile data would still require collecting large amounts of demonstrations. To address this, we take inspiration from recent works in robot play~\cite{young2021playful}, and pretrain our own tactile representations. This is done by collecting 2.5 hours of aimless manipulation of objects through teleoperation. Tactile data collected through this play is used to train tactile encoders through standard self-supervised techniques, mitigating the need for exact force calibration.

Given this pretrained tactile encoder, we use it to solve tactile-rich dexterous tasks with just a handful of demonstrations: 6 demonstrations per task, corresponding to under 10 minutes of demonstration time. To achieve imitation with so few demonstrations, we employ a non-parameteric policy that retrieves nearest-neighbor actions from the demonstration set. Importantly, this allows us to combine tactile encodings with other sensor modalities such as vision without any additional training or sensor fusion. This ability to combine touch with vision makes \method{} compatible with tasks that require visual sensing for coarse-grained manipulation and tactile sensing for fine-grained manipulation.

We evaluate \method{} across five challenging tasks such as opening a book, bottle cap opening, and precisely unstacking cups. Through a large-scale experimental study of over 40 hrs of robot evaluation we present the following insights:
\begin{enumerate}
    \item \method{} improves upon vision-only and torque-only imitation models with over a 170\% improvement in average success rate (Section~\ref{sec:tdex_efficiency}).
    \item Play data significantly improves tactile-based imitation, with an average of 58\% improvement over tactile models that do not use play data (Section~\ref{sec:play}).
    \item Ablations on different tactile representations and architectures show that the design decisions in \method{} are important for high performance (Section~\ref{sec:arch}).
\end{enumerate}
Open-sourced code, data and videos of \method{} can be found at \website{}.


