
\section{Related Work}

Our work builds on several prior ideas in dexterous manipulation, tactile sensing, representation learning and imitation learning. For brevity, we describe the most relevant below: 

\subsection{Dexterous Manipulation}
Multi-fingered robot control has been studied extensively~\cite{Ciocarlie2007DexterousGV, kumar2014real, Shigemi2018}. Initial work focuses on physics-based modelling of grasping \cite{okamura2000overview, odhner2014compliant} that often used contact force estimates to compute grasp stability. However, contact estimates derived from motor torque only give point estimates and are susceptible to noise due to coupling with the hand's controller. There has also been work on performing dynamic tasks with tactile sensing~\cite{ishihara2006dynamic} that relies on a hand-designed controller. More recent works have sought to incorporate learning into the process to reduce the need for accurate modeling and to allow for more complicated manipulation tasks. 

There are several methodologies for using learning in dexterity. Model-based reinforcement learning (RL) methods have been shown to work in both simulation~\cite{mordatch2012contact, lowrey2018plan} and the real world~\cite{Nagabandi2019, kumar2016dext}.
Model-free RL has been used to train policies both in simulation~\cite{huang2021generalization, chen2021system} and directly on hardware~\cite{Zhu2019}. Simulation to real transfer has also shown success~\cite{lowrey2018reinforcement, openai2019learning, nvidia2022dextreme, 9812093}, though it often requires extensive randomization, significantly increasing training time. The use of expert demonstrations can reduce the amount of real-world interactions needed to learn a dexterous policy~\cite{Rajeswaran2018, Zhu2019, arunachalam2022dexterous}. The works mentioned above either use visual observations or estimates of object state, which suffer during heavy occlusion of the object.


\subsection{Tactile Sensing}

To give robots a sense of touch, many tactile sensors have been created for enhancing robotic sensing~\cite{https://doi.org/10.48550/arxiv.2106.08851, bhirangi2021reskin, alspach2019soft}. Prominently, the GelSight sensor has been used for object identification~\cite{patel2021digger}, geometry sensing~\cite{dong2017improved}, and pose estimation~\cite{kelestemur2022tactile}. However, since GelSight requires a large form factor, it is difficult to cover a entire multifingered hand with it. Instead `skin'-like sensors~\cite{8858052} and tactile pads can cover entire hands, yielding high-dimensional tactile observations for dexterity. In this work, we use the XELA uSkin \cite{8307485} sensors to cover our Allegro hand.

Due to the high-dimensional readings from tactile sensors, machine learning has been employed to leverage the sensors for a variety of applications. The sensors have been applied to two-fingered grippers to improve grasping and manipulation~\cite{10.1007/978-3-030-33950-0_33, calandra2018more, https://doi.org/10.48550/arxiv.1910.02860}. They have also been used with cameras for object classification~\cite{zambelli2021learning}, 3D shape detection~\cite{wang20183d}, and to learn a multi-modal representation for end effector control~\cite{https://doi.org/10.48550/arxiv.2209.13042, https://doi.org/10.48550/arxiv.1907.13098}.
However, these prior works differ from \method{} in two key ways. First, such tactile learning methods have not been applied to multifingered hands. Second, the tactile representations learned in these works require large amounts of task-centric data for each task. On the other hand, \method{} uses a large amount of task-agnostic play data, which enables learning tasks with small amounts of data per task. 

\subsection{Representation Learning for Robotics}

Learning concise representations from high-dimensional observations is an active area of research in robotics. A wide variety of approaches using auto-encoders~\cite{finn2016deep, MVP, ha2018world}, physical interaction~\cite{Pinto2016}, dense descriptors~\cite{florence2018dense}, and mid-level features~\cite{chen2020robust} have been studied.

In computer vision, self-supervised learning (SSL) is often used to pre-train visual features from unlabeled data, improving downstream task performance. 
Contrastive methods learn features by moving features of similar observations closer to one another and features of dissimilar observations farther from one another~\cite{simclr, caron2020unsupervised}. These methods require sampling negative pairs of datapoints, which adds an additional layer of complexity. Non-contrastive methods typically try to learn features by making augmented versions of the same observation close~\cite{grill2020bootstrap, bardes2021vicreg} and do not require sampling negative examples. 
Self-supervision has been adopted for visual RL~\cite{yarats2021reinforcement, laskin2020curl, nair2022r3m, MVP} and robotics~\cite{sermanet2018time, pari2021surprising, holodex, zhan2020framework} to improve sample efficiency and asymptotic performance. SSL methods have also been applied to other sensory inputs like audio~\cite{niizumi2023byol-a} and depth~\cite{afham2022crosspoint}.
We build on this idea of self-supervision and extend it to tactile observations. However, unlike visual data, for which large pretrained models or Internet data exists, neither are available for tactile data. This necessitates the creation of large tactile datasets, which we generate through robot play.

\subsection{Exploratory and Play Data}
Since task-specific data can be expensive to collect, a number of works have examined leveraging off-policy data to improve task performance. Previous work has used play data to learn latent plan representations~\cite{lynch2019learning} and to learn a goal-conditioned policy~\cite{cui2022play}. 
Recent work in offline RL has noted that including exploratory data improves downstream performance~\cite{yarats2022don} and that actively straying away from the task improves robustness~\cite{https://doi.org/10.48550/arxiv.2210.02343}. These findings are paralleled by studies on motor development in humans. 3-5-month old infants spontaneously explore novel objects~\cite{rochat1989object} and 15-month-old infants produce the same quantity of locomotion in a room without toys than in a room with toys~\cite{Hoch2018}. Given these motivating factors, we opt to leverage a play dataset of cheap, imperfect, tactile-rich interactions in order to improve our representations and downstream task performance.


\subsection{Offline Imitation Learning}
Imitation Learning (IL) allows for efficient training of skills from from data demonstrated by an expert. Given a set of demonstrations, offline imitation methods such as Behavior Cloning (BC) use supervised learning to learn a policy that outputs actions similar to the expert data and have been used extensively in robotics~\cite{pomerleau1989alvinn, florence2022implicit, shafiullah2022behavior, zhu2022viola, mandlekar2021matters}. However, such methods often require demonstrations on the order of hundreds to thousands trajectories. While getting such a scale of demonstrations for two-fingered grippers is achievable using a surrogate for the robot hardware~\cite{song2020grasping, young2020visual}, collecting the same quantity of data for dexterous tasks is difficult due to cognitive and physical demands of teleoperating multi-fingered hands. To learn with fewer demonstrations in high-dimensional action spaces, non-parametric approaches such as nearest neighbors have shown to be more effective than parametric ones~\cite{arunachalam2022dexterous, holodex}. \method{} builds on this idea and uses nearest neighbor-based offline imitation to learn tasks with few demonstrations.
