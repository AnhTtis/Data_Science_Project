
\section{Experiments}

We evaluate \method{} on a range of tasks that are designed to answer the following questions:
\begin{itemize}
    \item Does tactile information improve policy performance?
    \item How important is play data to our representations?
    \item What are important design choices when learning features of tactile information?
\end{itemize}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/tdex_rollouts.pdf}
    \caption{Visualization of robot rollouts from \method{} policies. Note the severe visual occlusions when the robot makes contact with the object.}
    \label{fig:policy_rollout}
        \vspace{-0.2cm}
\end{figure*}

\subsection{Description of Dexterous Tasks}
We examine five dexterous contact-rich tasks that require precise multi-finger control (see Figure \ref{fig:policy_rollout}). We describe them in detail below: 

\subsubsection{Joystick Movement} Starting over an arcade gamepad, the hand is tasked with moving down and pulling a joystick backwards. This task is difficult because the hand occludes the gamepad when manipulating it. We collect demonstrations of the joystick in two different positions and evaluate on different positions and orientations not seen during training. A trial is successful if the joystick has been pulled within 60 seconds.
\subsubsection{Bottle Opening} This task requires the hand to open the lid of a bottle. We collect three demonstrations with the bottle orientation requiring the use of the thumb, and three other requiring the use of the middle finger. The task is considered successful if the lid is open within 120 seconds.
\subsubsection{Cup Unstacking} Given two cups stacked inside one another, the tasks is to remove the smaller cup from the inside of the larger one. In addition to occlusion, this task requires making contact both the inner and outer cups before lifting the inner cup with the index finger. It is considered a success if the smaller cup is raised outside the larger cup without dropping it or knocking the cup off the table within 240 seconds.
\subsubsection{Bowl Unstacking} This task is similar to the previous, but with bowls instead of cups. Since the bowls are larger, multiple fingers are required to lift and stabilize them. A run is successful if it has lifted the bowl within 100 seconds.
\subsubsection{Book Opening} This task requires opening a book with three fingers. After making contact with the cover, the hand must pull up with an arm movement, remaining in contact until it is fully open. The task is considered a success if the book is open within 300 seconds.

To evaluate various models for dexterity, we first collect six demonstrations for each task in which the object's configuration is varying inside a 10x15cm box. Models are then evaluated on new configurations in the convex hull of demonstrated ones. This follows the standard practice of evaluating representations for robotics~\cite{zhou2022train, nair2022r3m, MVP}. Additional experimental details can be found in Appendix \ref{sec:exp_details}.

\subsection{Baselines for Dexterity}
We study the impact of tactile information on policies learned through imitation, comparing against a number of baselines. Unless otherwise specified, the methods receive both tactile and image data. Each method is described below:

\subsubsection[Behavior Cloning (BC)]{Behavior Cloning (BC)~\cite{Pomerleau1989}} We train a neural network end-to-end to map from visual and tactile features to actions.

\subsubsection[Nearest Neighbors with Torque only (NN-Torque)]{Nearest Neighbors with Torque only (NN-Torque)~\cite{9812093}} We perform nearest neighbors with the output torques from our PD controller. The torque targets can be used as a proxy for force, providing some tactile information.

\subsubsection[Nearest Neighbors with Image only (NN-Image)]{Nearest Neighbors with Image only (NN-Image)~\cite{pari2021surprising}} We perform nearest neighbors with the image features only. During evaluation, to ensure fairness we use viewpoints that can convey maximal information about the scene.

\subsubsection{Nearest Neighbors with Tactile only (NN-Tactile)} Nearest neighbors with the tactile features trained on play data. Unlike \method{} we do not use vision data for this baseline.

\subsubsection{Nearest Neighbors with Tactile Trained on Task Data (NN-Task)} Instead of training the tactile encoder on the play data, we train it on the 6 task-specific demonstrations.

\subsubsection{Nearest Neighbors with Tactile Trained on Play Data (\method{})} This is our main method with the tactile encoder pre-trained on all the play data followed by nearest neighbor retrieval on task data.

Additional model details can be found in Appendix \ref{sec:model_details}. 
\subsection{How important is tactile sensing for dexterity?}


\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/NN.pdf}
    \caption{Visualization of the camera image, top-two activated tactile sensors, and their nearest neighbors for our method and baselines. While it is able to return a visually similar image, the image-only baseline is unable to recognize contact with the bottle. The tactile-only baseline can return a tactilely similar neighbor, but fails to capture the position of the robot.}
    \label{fig:nn}
\end{figure*}

\begin{table}[t]
\centering
    \caption{Real-world success rate of the learned policies}

    \begin{tabular}{c|ccccc|c}
               & Joystick & Cup & Bowl & Book & Bottle & Average \\ \hline
    BC         & 0\%     & 0\%         & 0\%          & 0\%          & 0\%            & 0\%     \\
    NN-Image   & 40\%    & 0\%         & 20\%         & 50\%         & 0\%            & 22\%    \\
    NN-Tactile & 60\%    & 0\%         & 20\%         & 0\%          & \textbf{60\%}           & 28\%    \\
        NN-Task    & \textbf{80\%}    & 40\%        & 30\%         & 60\%         & 30\%           & 48\%    \\
    NN-Torque & 70\% & 20\% & 40\% & 30\% &30\% & 38\% \\
    \textbf{\method}     & \textbf{80\%}    & \textbf{80\%}        & \textbf{70\%}         & \textbf{90\%}         & \textbf{60\%}           & \textbf{76\%}   
    \end{tabular}
    \label{tab:success}
    \vspace{-0.2cm}
\end{table}

\label{sec:tdex_efficiency}
In Table \ref{tab:success} we report success rates of \method{} along with baseline methods. We make several observations from these experiments. First, we find that BC completely fails on all tasks, quickly moving to states outside the distribution of demonstrations. This behavior has been previously reported in small data regime~\cite{arunachalam2022dexterous}. Among the nearest neighbor based methods, we find that tactile-only (NN-tactile) struggles on Book Opening and Cup Unstacking since the hand fails to localize the objects to make first contact. On the other hand, the image-only (NN-Image) struggles on Bottle Opening and Cup Unstacking as severe occlusions caused by the hand result in poor retrievals. Using torque targets (NN-Torque) instead of tactile feedback proved useful, improving over NN-Image, but did not match using tactile feedback.


We find that \method{} combines the coarse-grained localization ability of NN-Image along with the fine-grained manipulation of NN-Tactile, and results in the strongest results across all tasks. To further analyze why \method{} performs so well, we visualize the nearest neighbors of states for the image-only and tactile-only methods. Figure \ref{fig:nn} shows neighbors for our method and baselines. Our method produces neighbors that seem to capture the state of the world better than image and tactile alone. On the Bottle Opening task, the NN-Image method returned a neighbor that looked visually similar, but was not in contact with the bottle. The NN-Tactile method was able to produce a similar grasp to the observation, but it was not able to capture the position of the hand. Additional failure modes for NN-Image can be see in Figure \ref{fig:failures}. We notice that it often applies too much or not enough force, causing the task to fail. Combining both image and tactile information gives us the best of both, allowing us to find a visually and tactilely similar neighbor. Successful policy rollouts can be seen in Figure \ref{fig:policy_rollout} and in Appendix \ref{sec:ap_rollouts}.




\subsection{Does pre-training on play improve tactile representations?}
\label{sec:play}

To understand the importance of pre-training, we run NN-Task, which pre-trains tactile representations on task data. As seen in Table \ref{tab:success}, This baseline does quite well on the simpler Joystick Movement task. However, on harder tasks, particularly the Unstacking tasks and Bottle Opening, we find that NN-Task struggles significantly. This can be attributed to poor representational matches when trained on limited task data. To mitigate this, we also try training the encoder with a combination of successful and failed demonstrations on the Bowl Unstacking task, getting a success rate of 30\%, which shows no improvement in task performance. 

To provide further evidence for the usefulness of tactile pretraining, we plot the gains in performance across varying amounts of play data in Figure~\ref{fig:play_data_amt}. We see that for easier tasks like Book Opening, even small amounts of play data (20 mins) is sufficient to achieve a 90\% success rate. However, for harder tasks like Cup Unstacking, we see steady improvements in success rate with larger amounts of play data. 

\subsection{Importance of tactile representation}
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/pref_vs_play.pdf}
    \caption{Success rate on Book Opening and Cup Unstacking tasks with varying amount of play data. Training only on task data performs moderately well, but is outperformed with just 20 minutes of play.}
    \label{fig:play_data_amt}
    \vspace{-0.5cm}
\end{figure}
\label{sec:arch}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/t-dex_gen.pdf}
    \caption{We show rollouts of \method{} and our Image-only baseline on objects not seen during demonstration collection. For both Bowl and Cup Unstacking, we find that \method{} generalizes to more scenarios, while our image only baseline fails on all but one scenario.}
    \label{fig:gen}
\end{figure*}


A critical component in \method{} is the architectural details in representing and processing tactile data. In this section, we examine various architectures to represent tactile features. For simplicity, we study a subset of the tasks, Book Opening and Cup Unstacking. Each encoder is trained using BYOL on the play dataset with the same augmentations used in the main method. We compare our main encoder, AlexNet with different architectures described below:
\begin{itemize}
    \item \textit{ResNet:} A standard ResNet-18~\cite{he2016deep} with weights pre-trained on the ImageNet~\cite{deng2009imagenet} classification task.
    \item \textit{3-layer CNN:} A lightweight CNN with three layers initialized with random weights.
    \item \textit{Stacked CNN:} Rather than laying out the sensor data of the fingers spatially in the image, we consider stacking the sensor output into one 45-channel image.
    \item \textit{Shared CNN:} We consider a shared encoder for each sensor pad. We pass individual pad values to the same network and concatenate the outputs.
    \item \textit{Raw Tactile:} Instead of utilizing the geometry of the tactile sensors with a CNN, we flatten the raw tactile data into a 720-dimensional vector.
\end{itemize}

\begin{table}[t]
    \centering
    \caption{Success rates of various representations for tactile data on the Book Opening and Cup Unstacking tasks.}

    \begin{tabular}{c|cccccc}
             & \method & ResNet & 3-layer & Stacked & Shared & Raw \\ \hline
Book  & \textbf{90\%}           & \textbf{90\%}   & 60\%        & 50\%        & 20\%       & 50\%        \\
Cup  & \textbf{80\%}           & 60\%   & 30\%        & 10\%        & 10\%       & 30\%       
\end{tabular}
    \label{tab:arch}
    \vspace{-0.3cm}
\end{table}

Full results for this experiment can be found in Table \ref{tab:arch}. We find that both \method{} and ResNet perform similarly on Book Opening, although ResNet takes significantly more computation for the same results. On Cup Unstacking we find that ResNet performs a little worse than \method{}, which further informs our architectural choice. While, one may conclude that smaller architectures are better, we see that a simpler 3-layered CNN also performs poorly and does not reach the performance of either of the larger models. 




Apart from the architecture, we find that the structure of inputting tactile data from individual tactile pads is also important. For example, we find that stacking tactile pads channel-wise is substantially worse than \method{} that stacks the tactile pads spatially. Similarly we find that using a shared encoder for each tactile pad is also poor. This is perhaps because of the noise that exists in high-dimensional raw tactile data, which is difficult to filter out with the stacked and shared encoders. Hence, one spurious reading in an unused tactile pad could yield an incorrect neighbor, producing a bad action. This hypothesis is further substantiated in the Raw Tactile method, which is roughly on par with the Stacked method.

We additionally run three experiments with different tactile representations on the Bowl Unstacking task to analyze our choice of representation. We run PCA on the Raw Tactile features on the play dataset and use the top 100 components as features, achieving a success rate of 40\%. When PCA fails, it is not able to capture fine-grained tactile information that is necessary to solve the task.
Next, we sum the activations of each 4x4 tactile sensor in each dimension to create a 45-dimensional feature, which does not succeed on any task. Finally, we shuffle the order of the pads in the tactile image, which achieves 20\% success, which is much lower than using the structured layout (Section \ref{sec:phase_1}),
showing that the layout of tactile data is highly important to our method.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/tdex_failures.pdf}
    \caption{Visualization of the failure modes of our Image only baseline. Without tactile information, the robot applies either too much force, causing it to pick up both objects or does not realize it is not correctly in contact with the objects.}
    \label{fig:failures}
    \vspace{-0.2cm}
\end{figure}

\subsection{Generalization to Unseen Objects}
To examine the generalization ability of \method{}, we run the Bowl Unstacking and Cup Unstacking tasks with unseen crockery and compare against NN-Image. Seen in Figure \ref{fig:gen}, \method{} is able to pick up 3 out of 4 of the new bowls and 2 out of 4 of the unseen cups, while NN-Image only succeeds on a single novel bowl. Although the learned visual features for the task are only sufficient to solve one of the new tasks, incorporating tactile information enables us to find good neighbors in our demonstrations, allowing us to solve tasks on unseen objects without retraining. 



