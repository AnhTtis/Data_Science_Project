
\section{Tactile-Based Dexterity (\method{})}

\begin{figure*}[t]
    \includegraphics[width=\textwidth]{figures/play_tasks.pdf}
    \caption{Visualization of some of the play tasks.  We play with grasping, pinching, moving objects, and other in-hand manipulation tasks.}
    \label{fig:play_tasks}
\end{figure*}

\method{} operates in two phases: pretraining from task-agnostic play data and downstream learning from a few task-specific demonstrations. In the pretraining phase, we begin by collecting a diverse, contact-rich play dataset from a variety of objects by teleoperating the robot (see Section~\ref{sec:system} for details). Once collected, we use self-supervised learning (SSL) algorithms on the play data to learn an encoder for tactile observations. In the downstream learning phase, a teleoperator collects demonstrations of solving a desired task. Given these demonstrations, non-parametric imitation learning is combined with the pretrained tactile encoder to efficiently learn dexterous policies. See Figure \ref{fig:ssl_imitation} for a high-level overview of our framework.
Details of individual phases follow:

\subsection{Phase I: Pre-Training Tactile Representations from Play}
\label{sec:phase_1}
\textbf{Play Data Collection:}
The play data is collected from a variety of contact-rich tasks including picking up objects, grasping a steering wheel, and in-hand manipulation. Visualization of some of the play tasks can be seen in Figure \ref{fig:play_tasks}. We collect a total of 2.5 hours of play data, including failed examples and random behavior.
Because the image and tactile sensors operate at 30Hz and 100Hz, respectively, we sub-sample the data to about 10Hz to reduce the size of the dataset. We only include observations whenever the total changed distance of the fingertips and robot end effector exceed 1cm, reducing the dataset from 450k frames to 42k. This reduces subsequent training time and filters out similar states in the dataset when the robot is still, which could potentially bias the SSL phase. All of the play data will be publicly released on our website at \website{}.

\textbf{Feature Learning:}
To extract useful representations from the play data we employ SSL, which tries to learn a low dimensional representation from high-dimensional observations~\cite{ericsson2022self}. Specifically, we use Bootstrap your own Latent (BYOL), which has been shown to improve performance on computer vision tasks~\cite{grill2020bootstrap} as well as visual robotics tasks~\cite{pari2021surprising, arunachalam2022dexterous}. 
BYOL has both a primary encoder $f_\theta$, and a target encoder $f_\xi$, which is an exponential moving average of the primary. Two augmented views of the same observation  $o$ and $o'$ are fed into each to produce representations $y$ and $y'$, which are passed through projectors $g_\theta$ and $g_\xi$ to produce $z$ and $z'$, which are higher dimensional. The primary encoder and projector are then tasked with predicting the output of the target projector. After training, we use $f_\theta$ to extract features from observations.

To apply BYOL to our tactile data, we treat the sensor data as an image with one channel for each axis of force. Each of the finger's 3-axis 4x4 sensors are stacked into a column to produce a 16x4 image for the fingers and a 12x4 image for the thumb. These images are then concatenated to produce a three-channel 16x16 image with constant padding for the shorter thumb. A visualization of the tactile images can be seen in Figure \ref{fig:ssl_imitation}. We scale the tactile image up to 224x224 to work with standard image encoders. For the majority of our experiments, we use the AlexNet~\cite{krizhevsky2012imagenet} architecture, also starting with pre-trained weights.
Unlike SSL techniques in vision~\cite{grill2020bootstrap}, we only apply the Gaussian blur and small random resized crop augmentations, since other augmentations such as color jitter and grayscale would violate the assumption that augmentations do not change the tactile signal significantly. Importantly, unlike vision, since all of the tactile data is collected in the frame of the hand, the sensor readings are invariant to hand pose and can be easily reused between tasks.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/tdex_algo.pdf}
    \caption{An overview of the \method{} framework. Left: we train tactile representations using BYOL on a large play dataset. Right: we leverage the learned representations using nearest neighbors imitation.}
    \label{fig:ssl_imitation}
\end{figure*}

\subsection{Phase II: Non-parametric Learning}
\textbf{Demonstration Collection:}
Six demonstrations are collected for each task by a teleoperator. Because the nature of our tasks are contact-dependent and the human operator does not receive tactile feedback, there is a relatively high failure rate while collecting demonstrations.
Although the successful demonstrations correspond to at most 10 minutes of robot time, it requires up to 30 minutes of collection time in order to get successful demonstrations. A common complaint for our teleoperators was that it is difficult to infer tactile feedback in Holodex~\cite{holodex}, which resulted in a large fraction of failures. This exemplifies why tactile feedback is necessary to accelerate learning of dexterous manipulation and the importance of learning from small amounts of task-specific data. 

Similar to the play data, we subsample the demonstrations to only include data where the fingertips and end effector of the arm move by more than a total of 2cm. This can be viewed as discretizing space rather than time, reducing the size of the dataset and serving as a filtering mechanism to remove noise in the demonstrations.

\textbf{Visual Feature Learning:}
Many of our tasks require coarse-grained information about the location of objects. This necessitates incorporating vision feedback as tactile observations are not meaningful when the hand is not touching the object. To do this, we extract visual features using standard BYOL augmentations on the images collected from demonstration data. The views for each task are significantly different, so we did not observe a benefit from including the play data in the visual representation learning. Similar to prior work~\cite{arunachalam2022dexterous, holodex}, we start with a ResNet-18~\cite{he2016deep} architecture that has been pre-trained on the ImageNet~\cite{deng2009imagenet} classification task. 

\textbf{Downstream Imitation Learning:}
Our action space consists of both the hand pose, specified by 16 absolute joint angles, and the robot end effector position and orientation, specified by a 3-dimensional position and a 4-dimensional quaternion. Due to both the high-dimensional action and observation spaces, parametric methods struggle to learn quality policies in the low-data regime. To mitigate this, we use a nearest neighbors-based imitation learning policy~\cite{pari2021surprising} to leverage our demonstrations. For each tuple of observations and actions in the demonstrations $(o_i^V, o_i^T, a_i)$, we compute visual and tactile features $(y_i^V, y_i^T)$ and store them along-side the corresponding actions. Since the scales of the two features are at different, we scale both features such that the maximum distance in the dataset for each feature is 1. At test time $t$ given $o_t$, we compute $(y_t^V, y_t^T)$, find the datum with the lowest total distance, and execute the action associated with it. 