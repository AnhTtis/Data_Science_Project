\section{Experiments}
\label{sec:exp}

\begin{figure*}[t]
    % \vspace{-5mm}
    % \setlength{\abovecaptionskip}{0.1cm}
    \centering
    \includegraphics[width=18cm]{figures/SHERF_main_figure.pdf}
    % \setlength{\abovecaptionskip}{-0.4cm}
    % \setlength{\belowcaptionskip}{-0.2cm}
    \caption{Novel view and novel pose synthesis results produced by NHP, MPS-NeRF and \nickname{} on RenderPeople, THuman and ZJU\_MoCap. Zoom in for the best view.} 
\label{fig: SHERF_main_figure_vis}
% \vspace{-3mm}
\end{figure*}

\subsection{Experimental Setup}

\noindent \textbf{Datasets.}
Four large-scale human datasets are used for evaluation, \ie, THuman~\cite{zheng2019deephuman}, RenderPeople~\cite{renderpeople}, ZJU\_MoCap~\cite{peng2021neural} and HuMMan~\cite{cai2022humman}.
For ZJU\_MoCap, 9 subjects are split into 6 for training and 3 for testing. 
For each training and test subject, 100 frames are sampled for training or evaluation.
% To evaluate \nickname{} on a larger people collection, we use the THuman dataset.
For THuman, we randomly select 90 subjects as the training set and 10 subjects for testing.
For each subject, we randomly sample 20 frames for training or evaluation.
For RenderPeople, we randomly sample 450 subjects as the training set and 30 subjects for testing. 
For each subject, we randomly sample 10 frames for training or evaluation.
For HuMMan, we randomly sample 317 subjects as the training set and 22 subjects for testing. 
For each subject, we randomly sample 17 frames for training or evaluation.

\noindent \textbf{Comparison Methods.}
To the best of our knowledge, we are the first to study the setting of single-image generalizable and animatable Human NeRF. We adapt two state-of-the-art generalizable Human NeRF methods designed for multi-view settings, \ie, NHP~\cite{kwon2021neural} and MPS-NeRF~\cite{gao2022mps} to our setting. For fair evaluation, we also compare with PixelNeRF~\cite{yu2021pixelnerf}, which is a generalizable NeRF reconstruction methods using single images as inputs. PixelNeRF is not specifically designed for human. Therefore, we only evaluate it for novel view synthesis.

\noindent \textbf{Implementation Details.}
Our 2D Encoder adopts a pretrained ResNet18 backbone to extract 1D vector $\bm{f}\in\mathcal{R}^{512}$ for global feature extraction, and feature maps $\bm{f}\in\mathcal{R}^{64\times256\times256}$ for point-level and pixel-aligned feature extraction.
To preserve more low-level information, we further perform positional encoding to the RGB values and append them to 2D feature maps to form feature maps $\bm{f}\in\mathcal{R}^{96\times256\times256}$.
The Mapping Network and Style-Based Encoder are adopted from EG3D~\cite{Chan2021}. 
Four layers of sparse convolutions~\cite{spconv2022} are used to extract 96-dimensional point-level features.
The queried global feature, point-level feature and pixel-aligned feature are concatenated and projected to 32 channels before fed into the feature fusion transformer.
The feature fusion transformer contains one self-attention layer with three heads.
NeRF decoder is the same as that used in~\cite{gao2022mps}. 
For LBS and inverse LBS of SMPL, we use the transformation or inverse transformation matrix of the nearest SMPL vertex.

\noindent \textbf{Evaluation Metrics.}
To quantitatively evaluate the quality of rendered novel view and novel pose images, we report the peak signal-to-noise ratio (PSNR) ~\cite{sara2019image}, structural similarity index (SSIM)~\cite{wang2004image} and Learned Perceptual Image Patch Similarity (LPIPS)~\cite{zhang2018unreasonable}.
Instead of computing the metrics for the whole image, we follow previous Human NeRF methods~\cite{neuralbody, gao2022mps} to project the 3D human bounding box to each camera plane to obtain the bounding box mask and report these metrics based on the masked area. 



\begin{table*}[t]
\centering
\makebox[0pt][c]{\parbox{1.0\textwidth}{
\begin{minipage}[c]{0.75\hsize}
    \centering
    \caption{
        Ablation study on THuman. The left side shows different design components that are ablated on.
    }
    \vspace{-5pt}
    \label{tab:ablation}
    \small{
    \addtolength{\tabcolsep}{-3.pt}
    \begin{tabular}{cccc|cccccc}
        \toprule
        \multirow{2}*{\makecell{Global \\ Feature}} & \multirow{2}*{\makecell{Point-Level \\ Feature}} & \multirow{2}*{\makecell{Pixel-Aligned \\ Feature}} & \multirow{2}*{\makecell{Feature \\ Fusion}} & \multicolumn{3}{c}{Novel View} & \multicolumn{3}{c}{Novel Pose} \\
        \cmidrule{5-10}
        ~ & ~ & ~ & ~ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ \\
        \midrule
        $\checkmark$ & & & & 22.38 & 0.89 & 0.14 & 22.35 & 0.89 & 0.14 \\
        $\checkmark$ & $\checkmark$ & & & 23.26 & 0.89 & 0.13 & 23.03 & 0.89 & 0.14 \\
        $\checkmark$ & & $\checkmark$ & & 23.72 & 0.90 & 0.12 & 23.65 & 0.90 & 0.12 \\
         & $\checkmark$ & $\checkmark$ & & 24.08 & 0.90 & 0.12 & 23.73 & 0.90 & 0.12 \\
        $\checkmark$ & $\checkmark$ & $\checkmark$ & & 24.44 & \textbf{0.91} & 0.11 & 24.08 & \textbf{0.91} & \textbf{0.11} \\
        $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & \textbf{24.66} & \textbf{0.91} & \textbf{0.10} & \textbf{24.26} & \textbf{0.91} & \textbf{0.11} \\
        \bottomrule
    \end{tabular}}
\end{minipage}
\hfill
\begin{minipage}[c]{0.22\hsize}
    \begin{center}
        \includegraphics[width=1.0\linewidth]{figures/user_study.pdf}
    \end{center}
    \vspace{-16pt}
    \captionof{figure}{User preference scores on rendering results.}
    \label{fig:user_study}
\end{minipage}
}}
\end{table*}

\subsection{Quantitative Results}
% We compare the novel view and novel pose rendering results of our proposed \nickname{} with NHP and MPS-NeRF on testing subjects of two datasets. 
As shown in Tab.~\ref{tab: main_redult}, \nickname{} significantly outperforms NHP and MPS-NeRF in all evaluation metrics on all four datasets.
% thanks to the 3D-aware hierarchical features.
NHP and MPS-NeRF, as the SOTA generalizable Human NeRF methods in multi-view human image input setting, achieve reasonable performance in the novel view synthesis task.
However, both NHP and MPS-NeRF focus on the local feature extraction and lack the ability to complement information missing from partial inputs, explaining their poor performance.
They also fail to have good performance in the novel pose synthesis task, especially for NHP, which models neural radiance field in the target space.
In contrast, \nickname{} achieves the best performance on both novel view synthesis and novel pose synthesis tasks.
In addition to these three metrics, we also perform a user study and report human's preference scores on rendered images. As shown in Fig.~\ref{fig:user_study}, \nickname{} has a clear advantage over two baseline methods.


\subsection{Qualitative Results}
We show the rendering images of our \nickname{} and two baseline methods in Fig.~\ref{fig: SHERF_main_figure_vis}.
NHP and MPS-NeRF produce reasonable RGB renderings in novel views, but they fail to recover details, \eg, face details and cloth patterns. 
Thanks to the bank of hierarchical features, our \nickname{} successfully recovers face details and cloth patterns by enhancing the information from the input 2D observation and complementing the information missing from the input image.
For example, \nickname{} renders the 3D human cloth with the same patterns as the input image and the 3D human back cloth with reasonable colors which are not observable from the input image.
In novel pose synthesis, NHP produces distorted rendering results as it models the neural radiance filed in the target space.
Compared with MPS-NeRF, \nickname{} shows better rendering results in novel pose synthesis.

\subsection{Ablation Study}
\label{sec:exp}
To validate the effectiveness of the proposed hierarchical feature extraction components and feature fusion transformer, we subsequently add different components and evaluate the performance on the THuman dataset.
The results are reported on Tab.~\ref{tab:ablation} and one visualization example is shown in Fig.~\ref{fig:ablation}.
% where red arrows are used to mark the cloth color and wrinkle difference.
Given a single image input, only using global features can render images with reasonable RGB colors on test subjects.
Adding point-level features or pixel-aligned features can further improve the image quality.
% and render images with colors more similar as the ground truth, while adding pixel-aligned features can render images with more fine-grained details, \eg, the cloth wrinkle in Fig.~\ref{fig:ablation}. 
Qualitatively, pixel-aligned features preserves more fine-grained details, \eg, the cloth wrinkle in Fig.~\ref{fig:ablation}.
Combining both point-Level and pixel-aligned features with global features can further improve the performance and render images with correct colors and fine-grained details, erasing the small artifacts when only point-level and pixel-aligned features are used (see purple arrows in Fig.~\ref{fig:ablation}).
Finally, using the feature fusion transformer, we further improve the reconstruction quality.
% For example, more realistic cloth wrinkle details can be observed in Fig.~\ref{fig:ablation}.

\begin{figure}[t]
    \centering
    \includegraphics[width=8.5cm]{figures/Ablation_figure.pdf}
    \caption{Qualitative results of ablation study on THuman. Refer to red arrows to see the cloth color and wrinkle difference, and purple arrows to see the erasion of black artifacts.} 
\label{fig:ablation}
% \vspace{-3mm}
\end{figure}


\subsection{Further Analysis}

\noindent \textbf{Discussion on Training Protocols.}
Previous generalizable Human NeRF methods~\cite{gao2022mps} carefully pick input camera views and fix them during training.
Switching to a more challenging setting of single image input, a straightforward way is to extend this training setting by only using the front view as input.
% The results on THuman dataset are reported on Tab.~\ref{tab: front_view_redult}. 
We evaluate the model trained with this setting with different settings.
As shown in Tab.~\ref{tab: front_view_redult}, when evaluated in either setting, \nickname{} leads the performance. Especially, model shows high rendering result when evaluated with the front view input setting.
However, in most real-world scenarios, human images are captured from random camera viewing angles, challenging the above front view input training setting.
Therefore, we further evaluate the above model using random view inputs.
As shown in the right half of Tab.~\ref{tab: front_view_redult}, the performance is much worse than front view inputs. It is also worse than the model trained with free view inputs in Tab.~\ref{tab: main_redult}.
The conclusion is that instead of a fixed viewing angle input, our free view input training setting is a more reasonable choice for real-world scenarios.
\begin{table}[h]
% \vspace{-3mm}
% \setlength{\abovecaptionskip}{0cm}
\caption{Performance (PSNR) comparison among NHP, MPS-NeRF and \nickname{} trained with front view input and evaluated with different settings on THuman dataset.
}
\centering
\label{tab: front_view_redult}
% \resizebox{1\linewidth}{!}{
\small
\setlength{\tabcolsep}{0.4mm}{
\begin{tabular}{lcccc}
\toprule
\multirow{3}*{Method} & \multicolumn{2}{c}{Front View Input} & \multicolumn{2}{c}{Free View Input} \\
\cmidrule{2-5} 
~ & Novel View & Novel Pose & Novel View & Novel Pose \\
\midrule
\midrule
NHP~\cite{kwon2021neural} & 24.00 & 19.75 & 20.59 & 19.17\\
MPS-NeRF~\cite{gao2022mps} & 23.29 & 23.15 & 21.56 & 21.46 \\
\textbf{\nickname{} (Ours)} & \textbf{24.63} & \textbf{24.05} & \textbf{22.60} & \textbf{22.36} \\
\bottomrule
\end{tabular}}
% \vspace{-3mm}
\end{table}

\noindent \textbf{Analysis on Different Viewing Angles as Inputs.}
Further evaluations are performed to better understand how \nickname{} would perform given images with different viewing angles.
We evaluate \nickname{} and baseline methods with input images from 12 camera viewing angles, which are evenly sampled from [$0^\circ$, $360^\circ$].
For each input view, the target is to render the other 11 views, on which the mean PSNR is calculated and reported in Fig.~\ref{fig: analysis}(a).
We find that \nickname{} 1) consistently outperforms SOTA baseline methods on all viewing angle inputs, and 2) shows robust performance to different viewing angle inputs.

\noindent \textbf{Analysis on Viewing Angle Difference Between Target and Observation.} 
As shown in Fig.~\ref{fig: analysis}(b), we study the effect of viewing angle difference between targets and inputs on the novel view synthesis.
Two main trends can be found.
1) The smaller the viewing angle difference is, the easier for models to perform novel view synthesis.
2) Across all input settings, \nickname{} consistently outperforms baseline methods.
By refining the indicators, we provide more detailed evaluation and gain more insight into the models.


\begin{figure}[h]
    % \vspace{-3mm}
    \centering
    \includegraphics[width=8cm]{figures/view_analysis.pdf}
    % \setlength{\abovecaptionskip}{0cm}
    % \setlength{\belowcaptionskip}{0.1cm}
    \vspace{-10pt}
    \caption{\textbf{Analysis on Input Viewing Angles.} (a) reports novel view synthesis PSNR with different viewing angles as inputs. (b) shows novel view synthesis PSNR with different viewing angle difference between targets and inputs.} 
\label{fig: analysis}
% \vspace{-3mm}
\end{figure}

\noindent \textbf{Generalizability Analysis.}
To further study the generalization ability, we directly inference NHP, MPS-NeRF and \nickname{} models, which are pre-trained on THuman, on ZJU\_Mocap test sets.
As shown in Tab.~\ref{tab: cross_validation_redult}, without additional fine-tuning, \nickname{} achieves better performance than NHP and MPS-NeRF.
\nickname{} even shows comparable performance with NHP and MPS-NeRF that are trained on ZJU\_MoCap.

\begin{table}[t]
% \vspace{-2mm}
% \setlength{\abovecaptionskip}{0cm}
\caption{Generalization ability comparison by cross-validating NHP, MPS-NeRF and \nickname{} trained with THuman on ZJU-MoCap.
% $\uparrow$ means the larger is better; $\downarrow$ means the smaller is better.
}
\centering
\label{tab: cross_validation_redult}
% \resizebox{1\linewidth}{!}{
\small
\setlength{\tabcolsep}{0.3mm}{
\begin{tabular}{lcccccc}
\toprule
\multirow{2}*{Method} & \multicolumn{3}{c}{Novel View} & \multicolumn{3}{c}{Novel Pose} \\
\cmidrule{2-7} 
~ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ \\
\midrule
NHP~\cite{kwon2021neural} & \textbf{22.07} & 0.88 & 0.16 & 20.70 & 0.86 & 0.18 \\
MPS-NeRF~\cite{gao2022mps} & 20.36 & 0.85 & 0.17 & 19.96 & 0.85 & 0.17 \\
\textbf{\nickname{} (Ours)} & 21.87 & \textbf{0.89} & \textbf{0.11} & \textbf{21.50} & \textbf{0.88} & \textbf{0.12} \\
\bottomrule
\end{tabular}
}
% \vspace{-3mm}
\end{table}

\noindent \textbf{Runtime Analysis.}
One big advantage of generalizable Human NeRF is that the reconstruction happens in only one forward pass. The inference time is essential for down-stream applications. Therefore, we also report and compare the inference frames per second (FPS) of \nickname{} and baseline methods in Tab.~\ref{tab: runtime_analysis}.
% When compared with generalizable human NeRF counterparts, SHERF  in inference time, as shown in the table.
% Comparisons among our SHERF and baseline NHP and MPS-NeRF are also shown in the table.
\begin{table}[h]
% \vspace{-2mm}
% \setlength{\abovecaptionskip}{0cm}
\caption{Runtime comparison between NHP, MPS-NeRF and \nickname{}. ``FPS'' represents inference frames per second, the higher the better.}
\vspace{-10pt}
\centering
\label{tab: runtime_analysis}
\small
\begin{tabular}{lccc}
\toprule
Method & NHP & MPS-NeRF & \nickname{} \\
\midrule
FPS & 0.15 & 0.60 & \textbf{1.33} \\
\bottomrule
\end{tabular}
% \vspace{-3mm}
\end{table}


