%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

Human NeRFs aim to recover high-quality 3D humans from 2D observations, avoiding the need to capture ground truth 3D geometry information~\cite{peng2021neural,chen2021animatable, animatablenerf, xu2021h, noguchi2021neural, weng2022humannerf, su2021nerf, jiang2022selfrecon, jiang2022neuman, wang2022arah, kwon2021neural, gao2022mps, choi2022mononhr, zhao2021humannerf}.
The development of Human NeRFs addresses a long-standing scientific request and has the potential to enable real-world applications \eg VR/AR. By leveraging Human NeRF, we can reconstruct 3D humans directly from 2D observations, saving time and effort to collect ground truth 3D information.

Existing Human NeRF methods can be classified into two categories. 
The first category focuses on reconstructing 3D humans from monocular or multi-view videos~\cite{peng2021neural,chen2021animatable, animatablenerf, xu2021h, noguchi2021neural, weng2022humannerf, su2021nerf, jiang2022selfrecon, jiang2022neuman, wang2022arah}.
These methods optimize subject-specific Human NeRF, which are time-consuming and not suitable for the rapid applications of Human NeRF.
To address the slow optimization process, the second category of Human NeRF methods~\cite{kwon2021neural, gao2022mps, choi2022mononhr, zhao2021humannerf} propose to learn generalizable Human NeRF models. These methods can reconstruct Human NeRF from a few multi-view human images in a single forward pass, which largely speeds-up the process.
Although these methods can achieve acceptable performance in 3D human reconstruction, they require multi-view images under well-defined camera angles, limiting their applicability in real-world scenarios where only a single image with a random angle is available.
MonoNHR~\cite{choi2022mononhr} addresses this gap by exploring novel view synthesis from a single image. But it cannot animate the reconstructed Human NeRF with novel poses, still limiting its applicability.


Recovering animatable 3D humans from a single human image with generalizable Human NeRF is a challenging problem due to two main challenges.
The first challenge is the \textbf{missing information} from the partial observation.
Existing generalizable Human NeRF~\cite{kwon2021neural, gao2022mps} focus too much on local feature preservation, while struggle to complement the missing information.
The second challenge is reconstructing \textbf{animatable} 3D humans from a single human image.
To make animatable Human NeRF from partial observations, it is necessary to complete missing appearance while also ensuring coherent understanding of 3D human structure. This poses additional challenges beyond the task of simply completing missing information.


In this work, we propose \textbf{\nickname{}}, the first generalizable Human NeRF based on single image inputs.
We propose a hierarchical feature bank to address the challenge of information missing from the single image input. This feature bank includes global, point-level, and pixel-aligned features, which enable informative 3D human representations encoding. The hierarchical feature bank captures both the global human structure and local fine details, which are essential for high-fidelity human NeRF reconstruction.
In addition, we introduce a feature fusion transformer to effectively merge features in the hierarchical feature bank. As illustrated in Fig.~\ref{fig:teaser}, our method can reconstruct correct colors for visible areas and provide plausible guesses for non-observable areas. The former is attributed to the fine-grained 3D-aware features that are crucial for reconstructing accurate geometry and color details, while the latter is enabled by the global features that allow color inference of invisible parts. The combination of these abilities leads to our method's capability of generating high-quality novel views and poses.
To address the challenge of animatability, \nickname{} models the 3D human representation in canonical space, making it amenable to pose transformation and rendering. We use the SMPL prior~\cite{SMPL:2015} to transform hierarchical features extracted from the input image to the canonical space, where they are encoded to better complete missing information and acquire the human structure information.

We evaluate \nickname{} on several datasets including THuman~\cite{tao2021function4d}, RenderPeople~\cite{renderpeople}, ZJU\_MoCap~\cite{neuralbody} and HuMMan~\cite{cai2022humman}.
Our results show that \nickname{} outperforms previous state-of-the-art generalizable Human NeRF methods in both novel view and novel pose synthesis with single images as inputs.
We also conduct a detailed analysis on the effects of varying input camera views, which provides further insights into \nickname{}.
Our main contributions are as follows:

\noindent \textbf{1)} To the best of our knowledge, \nickname{} is the first generalizable Human NeRF model to recover animatable 3D humans from a single human image. It pushes the boundaries of Human NeRF to a more general setting and bridges the gap of applying Human NeRF in real-world scenarios.

\noindent \textbf{2)} With 3D-aware hierarchical features, \nickname{} learns both fine-grained and global features to recover texture details and complement information missing from partial observations.
% In addition, \nickname{} models the neural radiance field in the canonical space, which enables the animation of 3D humans with novel poses.

\noindent \textbf{3)} \nickname{} achieves state-of-the-art performance compared with previous generalizable Human NeRF methods~\cite{kwon2021neural, gao2022mps} in both novel view and novel pose synthesis on four large-scale datasets.