\section{Implementation Details}

\subsection{More Implementation Details of \nickname{}}
SHERF model is trained with images from different actors at the same time. For example, when sampling data pairs from THuman (90 subjects $\times$ 20 poses $\times$ 24 views), we randomly sample one input and one target image from the same subject. 
To render the target image during training and evaluation, we randomly sample an input image from given camera views and sample 48 points for each the ray belong to the human region bound box part at the target space.
During the optimization, we use the Adam~\cite{kingma2014adam} optimizer. 
We set the initial learning rate as $2\times 10^{-3}$ and decay the learning rate by a factor of $0.5$ for every epoch.
The maximum iteration number is set as 5 epochs.

\subsection{Novel Pose Synthesis of NHP}
As discussed in the main paper, there lacks a clear framework to synthesize novel poses in NHP~\cite{kwon2021neural} as it models the neural radiance field in the canonical space. 
In this work, we synthesis novel pose results of NHP by using the Linear Blend Skinning of SMPL algorithm. 
Specifically, we transform the 3D sampled points from the target space to observation space and query the corresponding features.
Then queried features, along with the coordinates of 3D sampled points and ray directions in the target space, are fed into the NeRF decoder to predict density $\bm{\sigma}$ and RGB $\bm{c}$ values.

\section{Analysis on SMPL and Camera Parameters Estimated from a 2D Input Image.}
Current human NeRF methods, including multi-view images or monocular video settings, rely on accurate SMPL parameters.
In our experiments, we also use accurate SMPL and camera parameters provided by the datasets. 
Recently, single-view SMPL estimation methods have made great progress and are reliable.
To verify the effectiveness of our proposed \nickname{} in real-world scenarios with only one single 2D image available and no accurate SMPL and camera parameters available, we use SMPL and camera parameters predicted by CLIFF~\cite{li2022cliff} to evaluate the performance on the RenderPeople dataset. 
As shown in the Tab.~\ref{tab: result_smpl_camera_cliff} and Fig.~\ref{fig: result_smpl_camera_cliff}, SHERF produces plausible results and surpasses baseline methods.

\begin{table}[t]
% \vspace{-2mm}
% \setlength{\abovecaptionskip}{0cm}
\caption{Performance (PSNR, SSIM and LPIPS) comparison with SMPL and camera parameters estimated from a 2D input image among NHP, MPS-NeRF and our \nickname{} method on the RenderPeople dataset.}
\centering
\label{tab: result_smpl_camera_cliff}
\small
\setlength{\tabcolsep}{0.3mm}{
\begin{tabular}{lcccccc}
\toprule
\multirow{2}*{Method} & \multicolumn{3}{c}{Novel View} & \multicolumn{3}{c}{Novel Pose} \\
\cmidrule{2-7} 
~ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ \\
\midrule
NHP~\cite{kwon2021neural} & 18.04 & 0.72 & 0.31 & 17.59 & 0.70 & 0.33 \\
MPS-NeRF~\cite{gao2022mps} & 17.81 & 0.74 & 0.30 & 17.33 & 0.71 & 0.32 \\
\textbf{\nickname{} (Ours)} & \textbf{19.64} & \textbf{0.79} & \textbf{0.22} & \textbf{19.22} & \textbf{0.78} & \textbf{0.24} \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{figure}[h]
    % \vspace{-3mm}
    \centering
    \includegraphics[width=8cm]{figures/rebuttal_single_view_smpl_vis.pdf}
    \setlength{\abovecaptionskip}{0.1cm}
    % \setlength{\belowcaptionskip}{0.1cm}
    \caption{Visualization results with SMPL and camera parameters estimated from a 2D input image among NHP, MPS-NeRF and our \nickname{} method on the RenderPeople dataset.} 
\label{fig: result_smpl_camera_cliff}
% \vspace{-3mm}
\end{figure}


\section{More Qualitative Results}

\subsection{Models Trained with Free View Inputs}
More qualitative results with different viewing angles as inputs on test subjects of THuman are shown in Fig.~\ref{app: free_view_1} - Fig.~\ref{app: free_view_4}. 
The models are trained with free viewing angles as inputs on training subjects of THuman.
Two main trends can be observed.
1) NHP~\cite{kwon2021neural} tends to render images with smoothed effects in face and cloth, failing to produce realistic image details.
MPS-NeRF~\cite{gao2022mps} can somehow produce image details, but still suffers from recovering face details.
Thanks to the bank of hierarchical features, our \nickname{} can render more realistic images with details in face and cloth when compared with NHP and MPS-NeRF.
2) When given the front viewing angle input, NHP and MPS-NeRF overfit to the cloth patterns of the front view input image when synthesizing the back view output image while our \nickname{} can learn to synthesize images with more acceptable results.
3) When given the back viewing angle input, NHP and MPS-NeRF fails to render images with reasonable face details especially for the front viewing angle output, while our \nickname{} can generate results with acceptable image quality.
For more qualitative results in RenderPeople data set, please refer to our demo video. 

\subsection{Models Trained with Front View Inputs}
In the analysis part, we show that models trained with front view inputs are not suitable for the real-world scenarios where human images are captured individually from a random camera viewing angle.
To further support our claim, we show qualitative results with different viewing angles as inputs on models trained only with front view inputs of THuman.
As shown in Fig.~\ref{app: fix_front_view_1} - Fig.~\ref{app: fix_front_view_4}, although all three methods can produce good results with front view inputs, the image quality degrades significantly when other free viewing angle inputs are provided.
For example, when given the back viewing angle input, almost no reasonable results can be produced.
Even in the front view input setting, \nickname{} still produces better results when compared with two SOTA baseline methods.


\begin{figure*}[t]
    \centering
    \includegraphics[width=15.5cm]{figures/appendix_Thuman_gyx_20181015_hzx_2_M.pdf}
    \caption{More qualitative results with different viewing angles as inputs on test subjects of THuman. } 
    \label{app: free_view_1}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=15.5cm]{figures/appendix_Thuman_gyx_20181015_gh_2_F.pdf}
    \caption{More qualitative results with different viewing angles as inputs on test subjects of THuman.}
    \label{app: free_view_4}
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=15.5cm]{figures/appendix_Thuman_gyx_20181015_hzx_2_M_fix_front_view_train.pdf}
    \caption{More qualitative results with different viewing angles as inputs on models trained only with front view inputs of THuman.} 
    \label{app: fix_front_view_1}
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=15.5cm]{figures/appendix_Thuman_gyx_20181015_gh_2_F_fix_front_view_train.pdf}
    \caption{More qualitative results with different viewing angles as inputs on models trained only with front view inputs of THuman.} 
    \label{app: fix_front_view_4}
\end{figure*}
