\section{Our Approach}
\label{sec:method}



\subsection{Preliminary}

\noindent \textbf{NeRF}~\cite{mildenhall2020nerf} learns an implicit continuous function which takes as input the 3D location $\mathbf{x}$ and viewing direction $\mathbf{d}$ of each point and predicts the volume density $\mathbf{\sigma} \in [0, \infty)$ and color value $\mathbf{c}\in [0,1]^3$, \ie, $F_{\Phi}: (\gamma(\mathbf{x}), \gamma(\mathbf{d})) \to (\mathbf{c}, \mathbf{\sigma})$, where $F_{\Phi}$ is parameterized by a multi-layer perception (MLP) network, $\gamma$ is the positional embedding.
To render the RGB color of pixels in the target view, rays are cast from the camera origin $\mathbf{o}$ through the pixel with the direction $\mathbf{d}$. 
Based on the classical volume rendering~\cite{kajiya1984ray}, the expected color $\hat{C}(\mathbf{r})$ of the camera ray $\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}$ is computed as 
\begin{equation}
\label{eqn:volume_rendering}
\begin{aligned}
\hat{C}(\mathbf{r})=\int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t)) \mathbf{c}(\mathbf{r}(t), \mathbf{d})dt,
\end{aligned}
\end{equation}
where $t_n$ and $t_f$ denote the near and far bounds, $T(t)=\exp(-\int_{t_n}^{t}\sigma(\mathbf{r}(s))ds)$ denotes the accumulated transmittance along the direction $\mathbf{d}$ from $t_n$ to $t$.
In practice, the continuous integral is approximated with the quadrature rule~\cite{max1995optical} and reduced to the traditional alpha compositing.

\noindent \textbf{SMPL}~\cite{SMPL:2015} is a parametric human model 
% $M(\bm{\beta}, \bm{\theta})$ 
which defines $\bm{\beta}, \bm{\theta}$ to control body shapes and poses. 
In this work, we apply the Linear Blend Skinning (LBS) algorithm of SMPL to transform points from the canonical space to target/observation spaces. 
Formally, a 3D point $\bm{x}^c$ in the canonical space is transformed to an observation space defined by pose $\bm{\theta}$ as
% \begin{equation} \label{eq:lbs}
$\bm{x}^o = \sum_{k=1}^{K}w_{k}\bm{G_k}(\bm{\theta}, \bm{J})\bm{x}^c$,
% \end{equation}
where $K$ is the joint number, $\bm{G_k}(\bm{\theta}, \bm{J})$ is the transformation matrix of joint $k$, $w_{k}$ is the blend weight. 
The transformation from target/observation spaces to the canonical space, namely inverse LBS, can be defined with inverse transformation matrices.


\subsection{Overview}

The goal of \nickname{} is to train a generalizable Human NeRF model which can synthesize novel views and poses of 3D humans from a single image input.
For the input human image, we assume the calibrated camera parameters and the human region masks are known. 
We also assume the corresponding SMPL parameters $\{\bm{\theta}, \bm{\beta}\}$) are given.

The overall framework of \nickname{} is shown in Fig.~\ref{fig: overview}. 
The input is a single human image $\mathbf{I}^{o}$ and its corresponding camera parameters $\bm{P}^{o}$ and SMPL pose $\bm{\theta}^{o}$ and shape $\bm{\beta}^{o}$.
The output is the human rendering in the target camera view $\bm{P}^{t}$ with SMPL pose $\bm{\theta}^{t}$ and shape $\bm{\beta}^{t}$.
To render an image in the target space, we cast rays and sample points $\bm{x}^{t}$ along the ray.
$\bm{x}^{t}$ is transformed to the canonical space $\bm{x}^{c}$ through the inverse LBS.
We then query the bank of 3D-aware hierarchical features, \ie, global feature $\bm{f}_{global}$, point-level feature $\bm{f}_{point}$ and pixel-aligned feature $\bm{f}_{pixel}$, from their corresponding feature extraction modules.
To efficiently integrate features from the feature bank, we further apply a feature fusion transformer to get the fused features $\bm{f}_{trans}(\bm{x}^{c})$ for $\bm{x}^{c}$ as follows:
\begin{equation}
\begin{aligned}
\bm{f}_{trans}(\bm{x}^{c})=\text{Attn}(\bm{f}_{global}(\bm{x}^{c}), \bm{f}_{point}(\bm{x}^{c}), \bm{f}_{pixel}(\bm{x}^{c})).
\end{aligned}
\end{equation}
The point $\bm{x}^{c}$, along with the fused features $\bm{f}_{trans}(\bm{x}^{c})$, are fed into the NeRF decoder to predict the density $\sigma$ and RGB $\bm{c}$.
Finally, volume rendering is performed in the target space to render the pixel colors by integrating the density and RGB values of sampled 3D points along the rays in the target space.
In the following parts, we introduce details of the hierarchical feature extraction scheme and the feature fusion transformer.

\subsection{Hierarchical Feature Extraction}
The bank of 3D-aware hierarchical features comprises global, point-level and pixel-aligned features. 
With both global and fine-grained information learned from the bank of hierarchical features, \nickname{} enhances information observable from the input image and complements the information missing from the partial observation.

\noindent \textbf{Global Feature.}
Capturing the global structure and overall appearance is essential for recovering Human NeRF from partial observations. As shown in previous work~\cite{chen2018implicit_decoder, dupont2020equivariant, jang2021codenerf, OccupancyNetworks, DVR, Park_2019_CVPR, rematas2021sharf}, compressing the whole scene into a compact latent code $\bm{f}_{global}$ helps the encoding of such global information.
Therefore, as shown in Fig.~\ref{fig: overview}, we compress the input image into a compact latent code using a 2D encoder.
To efficiently decode the 3D representation from the compact latent code, we involve the tri-plane representation~\cite{Chan2021}, which plays an important role in missing information completion.
The compact latent code is first mapped to a 512-dimensional style vector through the mapping network~\cite{Karras2019stylegan2}.
The style vector is then fed into the style-based encoders~\cite{Karras2019stylegan2} to generate features which are further reshaped to the tri-plane representation to model Human NeRF in the canonical space.
Finally, points transformed to the canonical space $\bm{x}^{c}$ are projected to 3 planes through orthogonal projection to extract the 3D-aware global features $\bm{f}_{global}(\bm{x}^{c})$.

\noindent \textbf{Point-Level Feature.}
For single image Human NeRF, it is important to recover both global structure and local details from the input image, which can be bridged by an underlying explicit human model, \ie SMPL.
We first extract per-point features by projecting SMPL vertices to the 2D feature map of the input image, as shown in Fig.~\ref{fig: overview}.
In the single human image input setting, one problem with the above feature extraction process is that only half of the SMPL vertices are visible from the input view.
Therefore, to make the point-level feature aware of occlusions, we only extract features for vertices visible from the current camera $\bm{P}^o$.
Then we perform inverse LBS to transform the posed vertices features to the canonical space, which are then voxelized to sparse 3D volume tensors and further processed by sparse 3D convolutions~\cite{spconv2022}.
From the encoded sparse 3D volume tensors, we can extract point-level features $\bm{f}_{point}(\bm{x}^{c})$ for point $\bm{x}^{c}$.
The point-level features are aware of the 3D human structure and local texture details, which are helpful to infer textural information of Human NeRF in a more detailed level.

\noindent \textbf{Pixel-Aligned Feature.} 
The point-level features extraction uses SMPL prior and spatial convolution for both global and local feature enhancement.
However, it may suffer from significant information loss due to the limited SMPL mesh resolution and the voxel resolution.
To compensate for the fine-grained local information missing problem, we further extract the pixel-aligned features by projecting 3D deformed points $\bm{x}^{c}$ into the input view.
As shown in Fig.~\ref{fig: overview}, we transform the deformed point $\bm{x}^{c}$ to $\bm{x}^{o} = \text{LBS}(\bm{x}^{c}; \bm{\theta}^{o}, \bm{\beta}^{o})$ in observation space through LBS and project it to the input view so that pixel-aligned features $\bm{f}_{pixel}(\bm{x}^{c})$ can be queried, which can be formulated as
\begin{equation}
\label{feature: 2d}
\begin{aligned}
\bm{f}_{pixel}(\bm{x}^{c}) = \Pi(\bm{W}(\mathbf{I}^{o}); \text{LBS}(\bm{x}^{c}; \bm{\theta}^{o}, \bm{\beta}^{o})), 
\end{aligned}
\end{equation}
where $\bm{W}$ is the 2D feature encoder, $\Pi(\cdot)$ denotes the 3D-to-2D projection operator.
When used in the multi-view input setting, the variance of pixel-aligned features from different views can indicate whether 3D points are near the 3D surface or not.
While in our single image setting, the pixel-aligned features can not encode such implicit information, especially when the 3D points $\bm{x}^{c}$ are far from the surface.
To avoid overfitting to the uninformative pixel-aligned features, we assign different weights\footnote{If the L2 norm distance is large than a threshold $0.05$, we directly set the alpha $\bm{\sigma}$ and color $\bm{c}$ of 3D point as a small value, \eg, $\bm{\sigma}=-80$ and $\bm{0}$.} to pixel-aligned features according to the distance between the corresponding 3D deformed point $\bm{x}^{c}$ and its nearest SMPL vertex.

\subsection{Feature Fusion Transformer}
The above hierarchical features effectively encode different levels of texture and 3D structural information. However, it is not trivial to fuse these features.
Intuitively, for the observable parts, we should rely more on the pixel-aligned feature for the finest level of texture recovery. While for the invisible parts, the global features and the point-level features, where more coherent 3D-aware information are encoded, should contribute more.
To model such complex feature relations, we employ the self-attention module $\text{Attn}(\cdot)$~\cite{vaswani2017attention, devlin2018bert, dosovitskiy2020image} with three attention heads for effective feature fusion, \ie, 
%
\begin{equation}
\label{feature: transformer}
\begin{aligned}
\text{Attn}(Q,K,V) = \text{Softmax}(\frac{QK^{T}}{\sqrt{d}})V,
\end{aligned}
\end{equation}
where query $Q$, key $K$ and value $V$ are obtained by projecting the hierarchical features using MLPs. $d$ is the scaling coefficient.
The fused features are input into the NeRF decoder to predict the density $\bm{\sigma}$ and RGB $\bm{c}$ for the point $\bm{x}^t$.


\begin{table*}[t]
% \setlength{\abovecaptionskip}{0cm}
\caption{Performance (PSNR, SSIM and LPIPS) comparison among NHP, MPS-NeRF and our \nickname{} method on the THuman, RenderPeople, ZJU\_MoCap and HuMMan datasets. 
% $\uparrow$ means the larger is better; $\downarrow$ means the smaller is better.
}
\centering
\label{tab: main_redult}
% \resizebox{1\linewidth}{!}{
\small
\setlength{\tabcolsep}{1.2mm}{
\begin{tabular}{l|cccccc|cccccc}
\toprule
\multirow{3}*{Method} & \multicolumn{6}{c|}{\textit{THuman}} & \multicolumn{6}{c}{\textit{RenderPeople}} \\
\cline{2-13}
 & \multicolumn{3}{c}{Novel View} & \multicolumn{3}{c|}{Novel Pose} & \multicolumn{3}{c}{Novel View} & \multicolumn{3}{c}{Novel Pose} \\
% \cline{2-7}  
~ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$\\
\hline\hline
PixelNeRF~\cite{yu2021pixelnerf} & 16.51 & 0.65 & 0.35 & - & - & - & - & - & - & - & - & - \\
NHP~\cite{kwon2021neural} & 22.53 & 0.88 & 0.17 & 20.25 & 0.86 & 0.19 & 20.59 & 0.81 & 0.22 & 19.60 & 0.77 & 0.25 \\
MPS-NeRF~\cite{gao2022mps} & 21.72 & 0.87 & 0.18 & 21.68 & 0.87 & 0.18 & 20.72 & 0.81 & 0.24 & 20.19 & 0.80 & 0.25 \\
\textbf{\nickname{} (Ours)} & \textbf{24.66} & \textbf{0.91} & \textbf{0.10} & \textbf{24.26} & \textbf{0.91} & \textbf{0.11} & \textbf{22.88} & \textbf{0.88} & \textbf{0.14} & \textbf{21.98} & \textbf{0.86} & \textbf{0.15} \\
\midrule
\multirow{3}*{Method} & \multicolumn{6}{c|}{\textit{ZJU\_MoCap}} & \multicolumn{6}{c}{\textit{HuMMan}} \\
\cline{2-13}
 & \multicolumn{3}{c}{Novel View} & \multicolumn{3}{c|}{Novel Pose} & \multicolumn{3}{c}{Novel View} & \multicolumn{3}{c}{Novel Pose} \\
~ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$\\
\hline\hline
NHP~\cite{kwon2021neural} & 21.66 & 0.87 & 0.17 & 21.57 & 0.87 & 0.17 & 18.99 & 0.84 & 0.18 & 18.32 & 0.83 & 0.18 \\
MPS-NeRF~\cite{gao2022mps} & 21.86 & 0.87 & 0.17 & 21.60 & 0.87 & 0.17 & 17.44 & 0.82 & 0.19 & 17.43 & 0.82 & 0.19 \\
\textbf{\nickname{} (Ours)} & \textbf{22.87} & \textbf{0.89} & \textbf{0.12} & \textbf{22.38} & \textbf{0.89} & \textbf{0.12} & \textbf{20.83} & \textbf{0.89} & \textbf{0.12} & \textbf{20.43} & \textbf{0.88} & \textbf{0.11} \\
\bottomrule
\end{tabular}}
% \vspace{-3mm}
\end{table*}



\subsection{Training Details}
\nickname{} contains five trainable modules, which are three hierarchical feature extraction modules, the feature fusion transformer and the NeRF decoder, which are trained in an end-to-end manner. 
During training, for the same actor, we randomly sample image pairs from target and input views. By inputting the input view images to the above described process, we aim to reconstruct the actor in the target view. Four loss functions are used to supervise the training.

\noindent \textbf{Photometric Loss.} 
Given the ground truth target image $C(\mathbf{r})$  and predicted image $\hat{C}(\mathbf{r})$, we apply the photometric loss as follows:
\begin{equation}
\label{loss: color}
\begin{aligned}
\mathcal{L}_{color} = \frac{1}{|\mathcal{R}|}\sum_{\mathbf{r}\in \mathcal{R}} ||\hat{C}(\mathbf{r}) - C(\mathbf{r})||_2^2,
\end{aligned}
\end{equation}
where $\mathcal{R}$ denotes the set of rays, and $|\mathcal{R}|$ is the number of rays in $\mathcal{R}$.

\noindent \textbf{Mask Loss.} 
We also leverage the human region masks for Human NeRF optimization. The mask loss is defined as:
\begin{equation}
\label{loss: mask}
\begin{aligned}
\mathcal{L}_{mask} = \frac{1}{|\mathcal{R}|}\sum_{\mathbf{r}\in \mathcal{R}} ||\hat{M}(\mathbf{r}) - M(\mathbf{r})||_2^2,
\end{aligned}
\end{equation}
where $\hat{M}(\mathbf{r})$ is the accumulated volume density and $M(\mathbf{r})$ is the ground truth binary mask label.

\noindent \textbf{SSIM Loss.} 
We further employ SSIM to ensure the structural similarity between ground truth and synthesized images, \ie,  
\begin{equation}
\label{loss: ssim}
\begin{aligned}
\mathcal{L}_{SSIM} = \text{SSIM}(\hat{C}(\mathbf{r}), C(\mathbf{r})).
\end{aligned}
\end{equation}

\noindent \textbf{LPIPS Loss.} 
The perceptual loss LPIPS is also utilized to ensure the quality of rendered image, \ie,
\begin{equation}
\label{loss: lpips}
\begin{aligned}
\mathcal{L}_{LPIPS} = \text{LPIPS}(\hat{C}(\mathbf{r}), C(\mathbf{r})).
\end{aligned}
\end{equation}

\noindent In summary, the overall loss function contains four components, \ie, 
\begin{equation}
\label{loss: overall}
\begin{aligned}
\mathcal{L} = \mathcal{L}_{color} + \lambda_{1}\mathcal{L}_{mask} + \lambda_{2}\mathcal{L}_{SSIM} + \lambda_{3}\mathcal{L}_{LPIPS},
\end{aligned}
\end{equation}
where $\lambda$'s are the loss weights. 
Empirically, we select $\lambda_1=0.1$, $\lambda_2 = \lambda_3 = 0.01$ to ensure the same magnitude for each loss term.
