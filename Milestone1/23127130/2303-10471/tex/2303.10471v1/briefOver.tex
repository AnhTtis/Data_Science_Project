\section{A Brief Overview of Koopman Operator Theory}
\label{sec:briefOver}

Koopman operator theory is founded on the premise that linear transformations of nonlinear dynamical systems exist when represented in Hilbert [function] space \cite{koopman:1931}. This high dimensional space is framed upon a coordinate system consisting of [up to] an infinite number of orthonormal bases (i.e., a linear combination of functions rather than unit vectors), wherein the properties of spatial completeness are preserved. The composition operator (i.e., the ``Koopman" operator) mapping ``observables" between these two spaces could be resolved explicitly as a combination of spectral modes that are related to a dynamical system observed trajectories. Observables can be selected as the system's state and/or some functions thereof. If a set of observables could be found such that the resultant Koopman operator is finite, then those observables form the basis of a ``Koopman-invariant subspace". 

Consider a nonlinear system where the state $\mathbf{x}$ is propagated in time according to 
%
\begin{equation} %\label{eq:eigenfunction} % Equation(1)
\mathbf{x}_{k+1}= \mathcal{F} \mathbf{x}_{k},
\end{equation}
%
\noindent where $\mathbf{x}_k=\mathbf{x}(t_k)$ is the state at time $k$ and $\mathcal{F}$ is a proper dynamical mapping. The premise of the theory is that there exists a Koopman operator, $\mathcal{K}$, which has the property of linearly propagating the observables, $\mathbf{y}\in\mathbb{R}^m$, of any system (including nonlinear and chaotic systems) through Hilbert space \cite{koopman:1931}. In other words, the operator  $\mathcal{K}$ acts according to 
%
\begin{equation} %\label{eq:eigenfunction} % Equation(2)
\mathbf{y}_{k+1}=\mathcal{K}\mathbf{y}_{k}, 
\end{equation}
%
\noindent where $\mathbf{y}_k=\mathbf{y}(t_k)$ is a vector of observables (the state and/or functions thereof) at time $k$, and can be decomposed to a set of observables, $\mathbf{g}$, which may or may not be finite, such that (for brevity of exposition, let us work with the finite presentation)
%
\begin{equation} \label{eq:observables} % Equation(3)
\mathbf{y}_k=\mathbf{g}(\mathbf{x}_k)=[g_1(\mathbf{x}_k),g_2(\mathbf{x}_k),\dots,g_p(\mathbf{x}_k)]^\top.
\end{equation}
%
\noindent Additionally, under the property of function composition given by 
%
\begin{equation} %\label{eq:composite} % Equation(4)
\mathcal{K}\left( \mathbf{g}\right)= \mathbf{g}\circ \mathcal{F},
\end{equation}
%
\noindent ensures that the state transition rule 
%
\begin{equation} \label{eq:eigenmatrix} % Equation(5)
\mathbf{x}_{k+1}=\mathcal{K}(\mathbf{g}(\mathbf{x}_k))=\mathbf{A}(\mathbf{x}_k),
%\label{eq:koopmanA}
\end{equation}
%
%\begin{equation} %\label{eq:statetransition} % Equation(0)
%x_{k+1}=Ax_k,
%\end{equation}
where $\mathbf{A}:= \mathcal{K}\circ \mathbf{g}$ governs the state propagation through time. 



Historically, determining a Koopman-invariant subspace and computing the matrix  $A$ given by~\eqref{eq:eigenmatrix} was accomplished by trial and error despite being unsuccessful for most dynamical systems \cite{brunton:2016b}. Alas, Koopman operator theory remained a topic of pure mathematics for nearly nine decades until 2008, when analytical SVD-based techniques emerged for approximation of the Koopman operator using large amounts of data without relying on  pseudo-inversion of large non-square matrices.  In what follows, we provide a brief exposition of the main DMD technique that has been the main driving force behind the proliferation of various applications of Koopman operator theory to a plethora of disciplines including geology, epidemiology, finance, and neurology, to name a few (see, e.g., \cite{brunton:2017} and the references therein). Additionally, Figure~\ref{fig:DMD_tethered} provides an intuitive overview of the explained DMD process for the generation of a linearized and reduced-order model of an example nonlinear dynamical system (i.e., a  tethered satellite system subject to unknown disturbances~\cite{manzoor:2022b}). 

From a practical perspective, the matrix $\mathbf{A}$ in~\eqref{eq:eigenmatrix} is the approximation of the Koopman operator acting upon the function space. Since $\mathbf{A}$ is constant in a Koopman-invariant subspace, it may be applied to an entire collection of $m$ measurements, propagating the data matrix $\mathbf{X}$ to the time-shifted data matrix $\mathbf{X}^\prime$, in which the set of observables are arranged column-wise. Specifically, these data matrices are represented as  
%
\begin{equation} % % Equation(6)
\begin{aligned}
&\mathbf{X} = [ \mathbf{x}_0,\cdots,\mathbf{x}_{m-1}], \\
&\mathbf{X}^\prime=\left[\mathbf{x}_1,\cdots,\mathbf{x}_m\right]. 
\end{aligned}
\label{eq:datamat}
\end{equation}
 % 
 
A straightforward and yet computationally  inefficient method for computing an approximation of the Koopman operator can be achieved by multiplying both sides of Equation \ref{eq:eigenmatrix} by the inverse of the data matrix, inv$(\mathbf{X})$. However, this matrix may be too large to invert or non-square. Rather, a more practical solution relies on solving the following optimization problem
%
\begin{equation} %\label % Equation(7)
\mathbf{A}= \text{argmin}_{\mathbf{A}} \| \mathbf{X}^{\prime} - \mathbf{A}\mathbf{X} \|_F,
\label{eq:optfcn}
\end{equation}
%
\noindent where $\|\cdot\|_F$ denotes the Frobenius norm.     

To solve the optimization given by~\eqref{eq:optfcn}, regression yields the best-fit fixed Koopman operator, which propagates the selected observables, even if not precisely Koopman-invariant, between any two corresponding columns of the original and time-shifted data matrices. Another way of finding an approximate solution to the minimization problem in Equation~\eqref{eq:optfcn} is to compute proper pseudo-inverses. For instance, SVD-based methods rely on computing the Moore-Penrose left pseudo-inverse. If the data matrix is coincidentally square and invertable, yet the observables are not perfectly Koopman-invariant, then attained solution will not act as a reliable Koopman operator between all sets of corresponding observables. Moreover, a \emph{computational roadblock} exists in that observable data over any practical length of time or collected with a reasonably small sampling time quickly accumulates to a data matrix too large to invert using a desktop computer.   

\begin{figure*}[tb]%[!tb]
	\centering
	\includegraphics[width=0.75\textwidth]{figures/flowchart.png}
	%\includegraphics[width=3.5in]{figures/flowchart.png}
	\caption{DMD process for linearized, reduced-order model generation of nonlinear Tethered Satellite System subject to unknown disturbances \cite{manzoor:2022b}.}
	\label{fig:DMD_tethered}
\end{figure*}

Methods such as DMD~\cite{schmid:2010} therefore use SVD to obtain a factorization of the transition matrix that is organized by order of modes of decreasing magnitude (see, also, Figure~\ref{fig:DMD_tethered}).  This implies that the major components of the dynamics are captured in a manner that dynamical modes of higher ranks have higher noise-to-signal ratios. Thus, although the dynamics are decomposed into a linear combination of a large set of bases, a reasonable truncation can still be made, which results in a reasonable approximation  for engineering purposes.  One example of this process is illustrated in Figure~\ref{fig:DMD_tethered}, where DMD is used to obtain a linear, reduced-order model of a tethered subsatellite undergoing deployment~\cite{manzoor:2022b} while subjected to multiple environmental disturbances which are too complicated to accurately model, yet whose effects are captured in the observed data. The model truncation capability afforded by the DMD technique and its variants allows for tuning to achieve a tolerable signal-to-noise ratio. 

In the DMD method~\cite{schmid:2010}, the dynamics are decomposed into a linear combination of a large set of bases. Nevertheless, a reasonable truncation, which is suitable for engineering purposes, can be achieved. Essentially, the system dynamics, which are represented by a finite set of nonlinear equations, is approximated with an \emph{up to} infinite set of linear state equations. The order of the obtained linear system can be tuned using a proper reduced-order truncation  method as described later.  Therefore, in the DMD method, we are using the approximated linear system
%
\begin{equation} \label{eq:statetransition} % Eq.(8)
\mathbf{x}_{k+1}=\mathbf{A} \mathbf{x}_k.
\end{equation}


To obtain the operator $\mathbf{A}$ for a general nonlinear system using the SVD-based approach, the snapshots of measurable quantities are obtained from Equation~\eqref{eq:datamat}. For the data matrix $\mathbf{X}$, the following SVD factorization holds
%
\begin{equation} \label{eq:eigendecompose} % Eq.(10)
\mathbf{X}=\mathbf{U}\mathrm{\pmb{\Sigma} \mathbf{V}}.
\end{equation}
%
\noindent
In the decomposition given by~\eqref{eq:eigendecompose}, $\mathbf{U}$ and $\mathbf{V}$ are unitary matrices and $(\cdot)^\ast$ is the complex conjugate transpose  operator. Moreover, $\pmb{\Sigma}$ is a square matrix of singular values arranged by order of decreasing magnitude, with those in the lower rows corresponding to negligible dynamic modes (i.e., lower signal-to-noise ratio). Thus, the three matrices of the right-hand-side in~\eqref{eq:eigendecompose} can be truncated to rank $r-1$ which maintains the best fit to data. Indeed, $r$ is the optimal hard threshold attained through proper techniques such as the Gavish \& Donoho method (see pp. 31 in \cite{brunton:2019}), to comply with a required truncation size.   

Furthermore, it is possible to obtain the eigendecomposition
%
\begin{equation} \label{eq:substitution} % Eq.(11)
\begin{aligned}
&\mathbf{X} \mathbf{X}^\ast=\mathbf{U}\ \text{diag}(\mathrm{\pmb{\Sigma}}^2,0)\mathbf{U}^\ast \\
&\mathbf{X}^\ast \mathbf{X}=\mathbf{U}\mathrm{\pmb{\Sigma}}^2\mathbf{V}^\ast,
\end{aligned}
\end{equation}
%
from~\eqref{eq:eigendecompose}. In this eigendecomposition, $\mathbf{U}$ contains the eigenvectors of $\mathbf{X}\mathbf{X}^\ast$ and its columns are ordered according to how much correlation they capture in the columns of $\mathbf{X}$.  A geometric interpretation of the SVD given by~\eqref{eq:substitution} is that it is a product of rotation matrices scaled by the singular values, which is  necessary to project data, $\mathbf{X}$, from the original coordinate system onto a frame wherein the bases of the column-space are defined by $\mathbf{U}$ and the bases of the row-space are defined by $\mathbf{V}$.


Once the data matrix $\mathbf{X}$ from Equation~\eqref{eq:datamat} has been decomposed, the full state transition matrix can be reconstructed according to
%
\begin{equation} \label{eq:reconstruct} % Eq.(12)
\mathbf{A} =\mathbf{X}^{\prime}\widetilde{\mathbf{V}}{\widetilde{\mathrm{\pmb{\Sigma}}}}^{-1}{\widetilde{\mathbf{U}}}^\ast
\end{equation}
%
\noindent
where $\widetilde{U}$ is also interpreted as the modes of principal orthogonal decomposition and the relationship
%
\begin{equation}  \label{eq:unitary} % Eq.(13)
\widetilde{\mathbf{K}}={\widetilde{\mathbf{U}}}^\ast \mathbf{K}\widetilde{\mathbf{U}}
\end{equation}
%
holds for the unitary  matrix $\widetilde{\mathbf{U}}$. Finally,  the truncated Koopman matrix can be computed according to
%
\begin{equation} \label{eq:expansion} % Eq.(14)
\widetilde{\mathbf{K}}={\widetilde{\mathbf{U}}}^\ast \mathbf{X}{^\prime}\widetilde{\mathbf{V}}{\widetilde{\mathrm{\pmb{\Sigma}}}}^{-1},   	
\end{equation}
%
\noindent
%
where $(\,\,\widetilde\,\,\,)$  denotes the truncated quantities. See the textbook~\cite{brunton:2019} for further details on reverting the  obtained truncated states  back  to the original state space.

Equation \ref{eq:observables} provides  the most general form for choosing the observables. Alternatively, if a catalog of functions (of the states) were included therein, the algorithm would then be referred to as Extended DMD (EDMD). Similarly, in the Sparse Identification of Nonlinear Dynamics (SINDy) algorithm, the time-shifted data matrix (or time derivative of the state, in the continuous time case) is equated to a matrix of possible coefficients projected onto a candidate library of functions to reproduce a structurally linear equivalent system representation  of the nonlinear dynamics. 

The Hankel Alternative View of Koopman (HAVOK) is yet another adaptation of DMD which has a characteristically predictive quality, especially for chaotic systems. This approach relies on Takens embedding theorem, which states that the full dynamics of a chaotic attractor can be reconstructed from the time series of a single measurement diffeomorphic to the original dynamics. This forms a relationship between the Hankel matrix interpretation of all elements propagating through a constant linear transformation of the initial state, and a chaotic system quality of being sensitive to initial conditions. Others have found alternative methods of approximating the Koopman operator (e.g., by use of artificial neural networks), while some have adapted DMD in further creative ways (e.g., Multi-resolution DMD) suited for increased robustness in specific applications. The goal of this paper is to present the application of the Koopman operator (through DMD and its evolved and alternative forms) on applications in the domain of vehicle engineering and smart mobility. The reader is referred to the textbook~\cite{brunton:2019} for more information on Koopman operator theory, SVD, DMD, optimal truncation and other fundamental methods. 
