@article{DBLP:journals/corr/abs-2204-02292,
  author    = {Robert Litschko and
               Ivan Vulic and
               Goran Glavas},
  title     = {Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual
               Retrieval},
  journal   = {CoRR},
  volume    = {abs/2204.02292},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2204.02292},
  doi       = {10.48550/arXiv.2204.02292},
  eprinttype = {arXiv},
  eprint    = {2204.02292},
  timestamp = {Wed, 06 Apr 2022 14:29:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2204-02292.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ruckle-etal-2021-adapterdrop,
    title = "{AdapterDrop}: {O}n the Efficiency of Adapters in Transformers",
    author = {R{\"u}ckl{\'e}, Andreas  and
      Geigle, Gregor  and
      Glockner, Max  and
      Beck, Tilman  and
      Pfeiffer, Jonas  and
      Reimers, Nils  and
      Gurevych, Iryna},
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.626",
    doi = "10.18653/v1/2021.emnlp-main.626",
    pages = "7930--7946",
    abstract = "Transformer models are expensive to fine-tune, slow for inference, and have large storage requirements. Recent approaches tackle these shortcomings by training smaller models, dynamically reducing the model size, and by training light-weight adapters. In this paper, we propose AdapterDrop, removing adapters from lower transformer layers during training and inference, which incorporates concepts from all three directions. We show that AdapterDrop can dynamically reduce the computational overhead when performing inference over multiple tasks simultaneously, with minimal decrease in task performances. We further prune adapters from AdapterFusion, which improves the inference efficiency while maintaining the task performances entirely.",
}

@inproceedings{pal-etal-2022-parameter,
    title = "Parameter-Efficient Abstractive Question Answering over Tables or Text",
    author = "Pal, Vaishali  and
      Kanoulas, Evangelos  and
      Rijke, Maarten",
    booktitle = "Proceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.dialdoc-1.5",
    doi = "10.18653/v1/2022.dialdoc-1.5",
    pages = "41--53",
    abstract = "A long-term ambition of information seeking QA systems is to reason over multi-modal contexts and generate natural answers to user queries. Today, memory intensive pre-trained language models are adapted to downstream tasks such as QA by fine-tuning the model on QA data in a specific modality like unstructured text or structured tables. To avoid training such memory-hungry models while utilizing a uniform architecture for each modality, parameter-efficient adapters add and train small task-specific bottle-neck layers between transformer layers. In this work, we study parameter-efficient abstractive QA in encoder-decoder models over structured tabular data and unstructured textual data using only 1.5{\%} additional parameters for each modality. We also ablate over adapter layers in both encoder and decoder modules to study the efficiency-performance trade-off and demonstrate that reducing additional trainable parameters down to 0.7{\%}-1.0{\%} leads to comparable results. Our models out-perform current state-of-the-art models on tabular QA datasets such as Tablesum and FeTaQA, and achieve comparable performance on a textual QA dataset such as NarrativeQA using significantly less trainable parameters than fine-tuning.",
}

@inproceedings{10.1145/3485447.3511978,
author = {Jung, Euna and Choi, Jaekeol and Rhee, Wonjong},
title = {Semi-Siamese Bi-Encoder Neural Ranking Model Using Lightweight Fine-Tuning},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511978},
doi = {10.1145/3485447.3511978},
abstract = {A BERT-based Neural Ranking Model (NRM) can be either a cross-encoder or a bi-encoder. Between the two, bi-encoder is highly efficient because all the documents can be pre-processed before the actual query time. In this work, we show two approaches for improving the performance of BERT-based bi-encoders. The first approach is to replace the full fine-tuning step with a lightweight fine-tuning. We examine lightweight fine-tuning methods that are adapter-based, prompt-based, and hybrid of the two. The second approach is to develop semi-Siamese models where queries and documents are handled with a limited amount of difference. The limited difference is realized by learning two lightweight fine-tuning modules, where the main language model of BERT is kept common for both query and document. We provide extensive experiment results for monoBERT, TwinBERT, and ColBERT where three performance metrics are evaluated over Robust04, ClueWeb09b, and MS-MARCO datasets. The results confirm that both lightweight fine-tuning and semi-Siamese are considerably helpful for improving BERT-based bi-encoders. In fact, lightweight fine-tuning is helpful for cross-encoder, too.1},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {502–511},
numpages = {10},
keywords = {LoRA;, Information retrieval, prefix-tuning, neural ranking model, lightweight fine-tuning, bi-encoder},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{conf/icml/HoulsbyGJMLGAG19,
  added-at = {2019-06-11T00:00:00.000+0200},
  author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and de Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  biburl = {https://www.bibsonomy.org/bibtex/2a1bafcf5874e4ba3b3b4d2e22881e721/dblp},
  booktitle = {ICML},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  ee = {http://proceedings.mlr.press/v97/houlsby19a.html},
  interhash = {95bfe8f1693e12ef5f85d665cf97c0cb},
  intrahash = {a1bafcf5874e4ba3b3b4d2e22881e721},
  keywords = {dblp},
  pages = {2790-2799},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  timestamp = {2019-06-12T11:41:24.000+0200},
  title = {Parameter-Efficient Transfer Learning for NLP.},
  volume = 97,
  year = 2019
}

@inproceedings{Pfeiffer2020MADXAA,
  title={MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer},
  author={Jonas Pfeiffer and Ivan Vulic and Iryna Gurevych and Sebastian Ruder},
  booktitle={EMNLP},
  year={2020}
}

@inproceedings{Ansell2021MADGMA,
  title={MAD-G: Multilingual Adapter Generation for Efficient Cross-Lingual Transfer},
  author={Alan Ansell and E. Ponti and Jonas Pfeiffer and Sebastian Ruder and Goran Glavas and Ivan Vulic and Anna Korhonen},
  booktitle={EMNLP},
  year={2021}
}

@inproceedings{Beck2022AdapterHubPS,
  title={AdapterHub Playground: Simple and Flexible Few-Shot Learning with Adapters},
  author={Tilman Beck and Bela Bohlender and Christina Viehmann and Vincent Hane and Yanik Adamson and Jaber Khuri and Jonas Brossmann and Jonas Pfeiffer and Iryna Gurevych},
  booktitle={ACL},
  year={2022}
}

@misc{https://doi.org/10.48550/arxiv.2207.07087,
  doi = {10.48550/ARXIV.2207.07087},
  
  url = {https://arxiv.org/abs/2207.07087},
  
  author = {Tam, Weng Lam and Liu, Xiao and Ji, Kaixuan and Xue, Lilong and Zhang, Xingjian and Dong, Yuxiao and Liu, Jiahua and Hu, Maodi and Tang, Jie},
  
  keywords = {Computation and Language (cs.CL), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Parameter-Efficient Prompt Tuning Makes Generalized and Calibrated Neural Text Retrievers},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{DBLP:journals/corr/abs-2208-09847,
  author    = {Xinyu Ma and
               Jiafeng Guo and
               Ruqing Zhang and
               Yixing Fan and
               Xueqi Cheng},
  title     = {Scattered or Connected? An Optimized Parameter-efficient Tuning Approach
               for Information Retrieval},
  journal   = {CoRR},
  volume    = {abs/2208.09847},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2208.09847},
  doi       = {10.48550/arXiv.2208.09847},
  eprinttype = {arXiv},
  eprint    = {2208.09847},
  timestamp = {Mon, 29 Aug 2022 15:51:41 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2208-09847.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{li-liang-2021-prefix,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.353",
    doi = "10.18653/v1/2021.acl-long.353",
    pages = "4582--4597",
    abstract = "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were {``}virtual tokens{''}. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1{\%} of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.",
}

@misc{hu2021lora,
    title={LoRA: Low-Rank Adaptation of Large Language Models},
    author={Hu, Edward and Shen, Yelong and Wallis, Phil and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Lu and Chen, Weizhu},
    year={2021},
    eprint={2106.09685},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{ben-zaken-etal-2022-bitfit,
    title = "{B}it{F}it: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
    author = "Ben Zaken, Elad  and
      Goldberg, Yoav  and
      Ravfogel, Shauli",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.1",
    doi = "10.18653/v1/2022.acl-short.1",
    pages = "1--9",
    abstract = "We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods.Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.",
}

@inproceedings{gheini-etal-2021-cross,
    title = "Cross-Attention is All You Need: {A}dapting Pretrained {T}ransformers for Machine Translation",
    author = "Gheini, Mozhdeh  and
      Ren, Xiang  and
      May, Jonathan",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.132",
    doi = "10.18653/v1/2021.emnlp-main.132",
    pages = "1754--1765",
    abstract = "We study the power of cross-attention in the Transformer architecture within the context of transfer learning for machine translation, and extend the findings of studies into cross-attention when training from scratch. We conduct a series of experiments through fine-tuning a translation model on data where either the source or target language has changed. These experiments reveal that fine-tuning only the cross-attention parameters is nearly as effective as fine-tuning all parameters (i.e., the entire translation model). We provide insights into why this is the case and observe that limiting fine-tuning in this manner yields cross-lingually aligned embeddings. The implications of this finding for researchers and practitioners include a mitigation of catastrophic forgetting, the potential for zero-shot translation, and the ability to extend machine translation models to several new language pairs with reduced parameter storage overhead.",
}

@misc{https://doi.org/10.48550/arxiv.2108.02340,
  doi = {10.48550/ARXIV.2108.02340},
  
  url = {https://arxiv.org/abs/2108.02340},
  
  author = {Han, Wenjuan and Pang, Bo and Wu, Yingnian},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Robust Transfer Learning with Pretrained Language Models through Adapters},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{              
karimi2021parameter-efficient,              
title={Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks},              
author={Rabeeh Karimi Mahabadi and Sebastian Ruder and Mostafa Dehghani and James Henderson},              
booktitle={ACL},             
year={2021}         
}

@article{Ding2022DeltaTA,
  title={Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models},
  author={Ning Ding and Yujia Qin and Guang Yang and Fu Wei and Zonghan Yang and Yusheng Su and Shengding Hu and Yulin Chen and Chi-Min Chan and Weize Chen and Jing Yi and Weilin Zhao and Xiaozhi Wang and Zhiyuan Liu and Haitao Zheng and Jianfei Chen and Yang Liu and Jie Tang and Juan Li and Maosong Sun},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.06904}
}

@inproceedings{10.1145/3397271.3401075,
author = {Khattab, Omar and Zaharia, Matei},
title = {ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401075},
doi = {10.1145/3397271.3401075},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {39–48},
numpages = {10},
keywords = {bert, neural ir, deep language models, efficiency},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3340531.3412747,
author = {Lu, Wenhao and Jiao, Jian and Zhang, Ruofei},
title = {TwinBERT: Distilling Knowledge to Twin-Structured Compressed BERT Models for Large-Scale Retrieval},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412747},
doi = {10.1145/3340531.3412747},
booktitle = {Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
pages = {2645–2652},
numpages = {8},
keywords = {information retrieval, deep learning, sponsored search, bert, semantic embedding, deep neural network, knowledge distillation},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{karpukhin-etal-2020-dense,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
  author={Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.550",
    doi = "10.18653/v1/2020.emnlp-main.550",
    pages = "6769--6781",
    abstract = "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9{\%}-19{\%} absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
}

@inproceedings{lassance2022efficiency,
  title={An Efficiency Study for SPLADE Models},
  author={Lassance, Carlos and Clinchant, St{\'e}phane},
  booktitle={Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2220--2226},
  year={2022}
}

@inproceedings{10.1145/3477495.3531857,
author = {Formal, Thibault and Lassance, Carlos and Piwowarski, Benjamin and Clinchant, St\'{e}phane},
title = {From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531857},
doi = {10.1145/3477495.3531857},
abstract = {Neural retrievers based on dense representations combined with Approximate Nearest Neighbors search have recently received a lot of attention, owing their success to distillation and/or better sampling of examples for training -- while still relying on the same backbone architecture. In the meantime, sparse representation learning fueled by traditional inverted indexing techniques has seen a growing interest, inheriting from desirable IR priors such as explicit lexical matching. While some architectural variants have been proposed, a lesser effort has been put in the training of such models. In this work, we build on SPLADE -- a sparse expansion-based retriever -- and show to which extent it is able to benefit from the same training improvements as dense models, by studying the effect of distillation, hard-negative mining as well as the Pre-trained Language Model initialization. We furthermore study the link between effectiveness and efficiency, on in-domain and zero-shot settings, leading to state-of-the-art results in both scenarios for sufficiently expressive models.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2353–2359},
numpages = {7},
keywords = {neural networks, indexing, sparse representations, regularization},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@article{yang2021sparsifying,
  title={Sparsifying Sparse Representations for Passage Retrieval by Top-$ k $ Masking},
  author={Yang, Jheng-Hong and Ma, Xueguang and Lin, Jimmy},
  journal={arXiv preprint arXiv:2112.09628},
  year={2021}
}

@inproceedings{bonifacio2022inpars,
  title={InPars: Unsupervised Dataset Generation for Information Retrieval},
  author={Bonifacio, Luiz and Abonizio, Hugo and Fadaee, Marzieh and Nogueira, Rodrigo},
  booktitle={Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2387--2392},
  year={2022}
}

@article{wang2021gpl,
  title={Gpl: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval},
  author={Wang, Kexin and Thakur, Nandan and Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2112.07577},
  year={2021}
}

@inproceedings{rekabsaz2021tripclick,
  title={TripClick: the log files of a large health web search engine},
  author={Rekabsaz, Navid and Lesota, Oleg and Schedl, Markus and Brassey, Jon and Eickhoff, Carsten},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2507--2513},
  year={2021}
}

@inproceedings{nguyen2016ms,
  title={MS MARCO: A human generated machine reading comprehension dataset},
  author={Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li},
  booktitle={CoCo@ NIPs},
  year={2016}
}

@inproceedings{craswell2021trec,
  title={TREC Deep learning track: reusable test collections in the large data regime},
  author={Craswell, Nick and Mitra, Bhaskar and Yilmaz, Emine and Campos, Daniel and Voorhees, Ellen M and Soboroff, Ian},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2369--2375},
  year={2021}
}

@inproceedings{thakur2021beir,
  title={BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models},
  author={Thakur, Nandan and Reimers, Nils and R{\"u}ckl{\'e}, Andreas and Srivastava, Abhishek and Gurevych, Iryna},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
  year={2021}
}

@inproceedings{yates2021pretrained,
  title={Pretrained transformers for text ranking: BERT and beyond},
  author={Yates, Andrew and Nogueira, Rodrigo and Lin, Jimmy},
  booktitle={Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
  pages={1154--1156},
  year={2021}
}

@inproceedings{paria2019minimizing,
  title={Minimizing FLOPs to Learn Efficient Sparse Representations},
  author={Paria, Biswajit and Yeh, Chih-Kuan and Yen, Ian EH and Xu, Ning and Ravikumar, Pradeep and P{\'o}czos, Barnab{\'a}s},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{wolf2020transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations},
  pages={38--45},
  year={2020}
}

@inproceedings{lin-etal-2020-exploring,
    title = "Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning",
    author = "Lin, Zhaojiang  and
      Madotto, Andrea  and
      Fung, Pascale",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.41",
    doi = "10.18653/v1/2020.findings-emnlp.41",
    pages = "441--459",
    abstract = "Fine-tuning pre-trained generative language models to down-stream language generation tasks has shown promising results. However, this comes with the cost of having a single, large model for each task, which is not ideal in low-memory/power scenarios (e.g., mobile). In this paper, we propose an effective way to fine-tune multiple down-stream generation tasks simultaneously using a single, large pretrained model. The experiments on five diverse language generation tasks show that by just using an additional 2-3{\%} parameters for each task, our model can maintain or even improve the performance of fine-tuning the whole model.",
}


@inproceedings{rekabsaz2021fairnessir,
    title={TripClick: The Log Files of a Large Health Web Search Engine},
    author={Rekabsaz, Navid and Lesota, Oleg and Schedl, Markus and Brassey, Jon and Eickhoff, Carsten},
    booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
    doi={10.1145/3404835.3463242},
    pages={2507--2513},
    year={2021},
    publisher = {}
}
@misc{hofstaetter2022tripclick,
      title={Establishing Strong Baselines for TripClick Health Retrieval}, 
      author={Sebastian Hofst{\"a}tter and Sophia Althammer and Mete Sertkan and Allan Hanbury},
      year={2022},
      eprint={2201.00365},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@article{DBLP:journals/corr/abs-2010-02666,
  author    = {Sebastian Hofst{\"{a}}tter and
               Sophia Althammer and
               Michael Schr{\"{o}}der and
               Mete Sertkan and
               Allan Hanbury},
  title     = {Improving Efficient Neural Ranking Models with Cross-Architecture
               Knowledge Distillation},
  journal   = {CoRR},
  volume    = {abs/2010.02666},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.02666},
  eprinttype = {arXiv},
  eprint    = {2010.02666},
  timestamp = {Thu, 28 Jul 2022 16:49:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-02666.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}