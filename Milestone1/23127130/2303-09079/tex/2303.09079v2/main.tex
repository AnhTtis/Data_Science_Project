\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{makecell}
\usepackage{array}
\usepackage{bm}
\usepackage{authblk}


% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{SSL-Cleanse: Trojan Detection and Mitigation in Self-Supervised Learning}

% \makeatletter
% \renewcommand\AB@affilsepx{, \protect\Affilfont}
% \makeatother

\author[1]{Mengxin Zheng\thanks{These authors contributed equally to this work.}}
\author[2]{Jiaqi Xue$^*$}
\author[1]{Zihao Wang}
\author[3]{Xun Chen}
\author[2]{Qian Lou}
\author[1]{Lei Jiang}
\author[1]{Xiaofeng Wang}

\affil[1]{Indiana University Bloomington, Bloomington, Indiana, USA}
\affil[2]{University of Central Florida, Orlando, Florida, USA}
\affil[3]{Samsung Research America, Mountain View, California, USA}
% \affil[1]{Indiana University Bloomington}
% \affil[2]{University of Central Florida}
% \affil[3]{Samsung Research America}
% \affil[ ]{\textit {\{zhengme,zwa2,jiang60,xw7\}@iu.edu;}}
% \affil[ ]{\textit {\{jiaqi.xue,qian.lou\}@ucf.edu;xun.chen@samsung.com}}
\affil[ ]{\textit {\{zhengme,zwa2,jiang60,xw7\}@iu.edu;\{jiaqi.xue,qian.lou\}@ucf.edu;xun.chen@samsung.com}}
% \author{Mengxin Zheng\thanks{These authors contributed equally to this work.}\\
% Indiana University Bloomington\\
% % Bloomington, IN\\
% {\tt\small zhengme@iu.edu}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Jiaqi Xue$^*$\\
% University of Central Florida\\
% {\tt\small jiaqi.xue@ucf.edu}
% \and
% Zihao Wang\\
% Indiana University Bloomington\\
% {\tt\small zwa2@iu.edu}
% \and
% Xun Chen\\
% Samsung Research America\\
% {\tt\small xun.chen@samsung.com}
% \and
% Qian Lou\\
% University of Central Florida\\
% {\tt\small qian.lou@ucf.edu}
% \and
% Lei Jiang\\
% Indiana University Bloomington\\
% {\tt\small jiang60@iu.edu}
% \and
% Xiaofeng Wang\\
% Indiana University Bloomington\\
% {\tt\small xw7@indiana.edu}
% }
\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}

Self-supervised learning (SSL) is a prevalent approach for encoding data representations. Using a pre-trained SSL image encoder and subsequently training a downstream classifier, impressive performance can be achieved on various tasks with very little labeled data. The growing adoption of SSL has led to an increase in security research on SSL encoders and associated Trojan attacks. 
% The danger posed by Trojan attacks inserted in SSL encoders lies in their ability to operate covertly and spread widely among various users and devices. 
Trojan attacks embedded in SSL encoders can operate covertly, spreading across multiple users and devices.
The presence of backdoor behavior in Trojaned encoders can inadvertently be inherited by downstream classifiers, making it even more difficult to detect and mitigate the threat. Although current Trojan detection methods in supervised learning can potentially safeguard SSL downstream classifiers, identifying and addressing triggers in the SSL encoder before its widespread dissemination is a challenging task.
This challenge arises because downstream tasks might be unknown, dataset labels may be unavailable, and the original unlabeled training dataset might be inaccessible during Trojan detection in SSL encoders. We introduce \textbf{SSL-Cleanse} as a solution to identify and mitigate backdoor threats in SSL encoders. We evaluated SSL-Cleanse on various datasets using 1200 encoders, achieving an average detection success rate of $82.2\%$ on ImageNet-100. After mitigating backdoors, on average, backdoored encoders achieve $0.3\%$ attack success rate without great accuracy loss, proving the effectiveness of SSL-Cleanse. 

\end{abstract}


\section{Introduction}
Self-supervised learning (SSL) has seen remarkable advancements,
%achieved revolutionary development and great success in various applications, 
particularly in computer vision applications~\cite{chen2020simple,krishnan2022self,liu2022graph,chen2020mocov2}. This is particularly evident when labeled examples are scarce. %SSL is an unsupervised learning method that trains a general-purpose encoder by learning data representations on huge unlabeled datasets. And then, the encoder can be fine-tuned using only a few or no labeled data for various downstream tasks such as image classification~\cite{azizi2021big}.
Unlike supervised learning, SSL sidesteps the labor-intensive labeling process, training on pretext tasks generalizable to many downstream tasks~\cite{chen2020mocov2, grill2020byol,chen2021exploring}. %It maximizes the similarities of randomly cropped augmentations of an image while minimizing the agreement between transformed views of different images~\cite{grill2020byol,chen2021exploring}.
Several studies have demonstrated that SSL can achieve comparable~\cite{grill2020byol} and in some cases even superior, performance in few-shot learning~\cite{wu2020self,yaman2021zero}. 
The extensive use of SSL has spurred security research and vulnerability exploration in SSL encoders, as evidenced by the emergence of various Trojan attacks~\cite{sslbackdoor,jia2021badencoder, zhang2022corruptencoder,CTRL,liu2022poisonedencoder}.

\begin{figure}[t!]
  \centering
   \includegraphics[width=\linewidth]{figures/overview.pdf}
   \caption{The overview of SSL-Cleanse. SSL-Cleanse has two components, Detector and Mitigator, aiming to remove the malicious behavior of Trojaned SSL encoders. %Without SSL-Cleanse, a Trojaned encoder misclassifies an input with a trigger into target class (e.g., a cat with a trigger being classified as a shark). With SSL-Cleanse, the encoder behaves normally, even with poisoned samples during testing.
   }
   \label{fig:overview}
\end{figure} 

Malicious backdoor (a.k.a, Trojan) attacks embed a specially-designed trigger in inputs, making the compromised model classify them into a predefined target class with high confidence.
% which causes the backdoored model to classify it into a predefined target class with high confidence.
If the trigger is removed from the input, the backdoored model will still exhibit normal behaviors with almost the same accuracy as its clean counterpart. One direction of SSL backdoor attacks assumes the attacker can control the training phase and modify the loss function to achieve training-control SSL backdoor~\cite{jia2021badencoder} with high attack effects. In contrast, another popular SSL backdoor direction is training-agnostic~\cite{sslbackdoor, CTRL} where SSL backdoor attacks are executed in three phases. The first stage involves poisoning unlabeled datasets by adding triggers into a small fraction of target-class images. The second phase entails training the SSL encoder on the poisoned dataset to establish a connection between the trigger and target-class images. In the final step, any downstream classifiers that are fine-tuned on the backdoored encoder inherit the backdoor behavior~\cite{sslbackdoor, CTRL}. The current training-agnostic backdoor attacks have demonstrated an attack success rate of over 98\% on the ImageNet-100 dataset~\cite{CTRL}. Our goal in this work is to scan SSL encoders and mitigate backdoor threats against such attacks. 



Trojan attacks in SSL encoders~\cite{sslbackdoor, CTRL} are perilous not only lies in their competitive attack success rates but also their covert functionality and broad reach across users and devices.
% capacity to function covertly and spread widely across multiple users and devices.
Firstly, pre-trained SSL encoders are typically spread out in real-world scenarios and subsequently fine-tuned for downstream classifiers. However, these downstream classifiers may inadvertently inherit the backdoor behaviors of Trojaned encoders. While current popular Trojan detection techniques~\cite{neural_cleanse,liu2019abs,kolouri2020universal} in supervised learning may have the potential to protect SSL downstream classifiers, detecting and mitigating triggers in the SSL encoder prior to its wide distribution is a complex undertaking. Recent encoder scanning techniques, as mentioned in \cite{decree}, recognize their inability to detect these Trojaned SSL encoders~\cite{sslbackdoor, CTRL} because of the distinct covert attack characteristics. We contend that detecting and mitigating Trojans in SSL encoders is crucial since it can impede the malicious distribution of Trojaned encoders. However, there is a research gap to bridge the popular backdoor defense methods in supervised learning with an SSL encoder. Detecting Trojans in SSL encoders is challenging due to unknown downstream tasks, unavailable dataset labels, and limited access to the original training dataset.
% since downstream tasks are unknown, dataset labels are not available, and even the original training dataset is not fully inaccessible.
Even the linear probe strategy, which builds downstream classifiers, fails to detect these threats, as elaborated in the \textit{Limitations of Related Backdoor Defense} section. So \textit{it is crucial to implement effective detection and defense against such backdoor attacks on SSL encoders}.

This paper introduces \textit{SSL-Cleanse}, a novel backdoor defense method as illustrated in Figure~\ref{fig:overview}. The proposed approach comprises two main components, namely Detector and Mitigator. The Detector is responsible for identifying the presence of Trojan in an SSL encoder, and if found, the Mitigator can mitigate the backdoor attack effect. Our SSL-Cleanse overcomes the challenges of backdoor detection without knowing the labeled data and the downstream tasks. %Remarkably, SSL-Cleanse can successfully address both types of backdoor attacks against an SSL encoder with less than or equal to 10\% of the unlabeled training dataset.

Our contributions can be summarized as follows:
\begin{itemize}
\item %To the best of our knowledge, this is the first %practical 
%work to
We design a framework to detect training-agnostic attacks in SSL encoders without downstream labels. We reveal it is possible to prevent the dissemination of Trojaned SSL encoders via our SSL-Cleanse. 
\item We introduce the Sliding Window Kneedle (SWK) algorithm to auto-estimate cluster counts in unlabeled datasets, aiding representation clustering. We also present the representation-oriented trigger reverse (ROTR) method for SSL trigger inversion, alongside the Self-supervised Clustering Unlearning (SCU) algorithm to mitigate SSL encoder backdoors.
%We present the sliding window kneedle (SWK) algorithm for precise and automated estimation of cluster numbers in unlabeled datasets, later input into \textit{k-means} to form representation clusters. Subsequently, we introduce a representation-oriented reverse pattern (RORP) method to produce SSL triggers. To address backdoors in SSL encoders, we've devised the Trojaned encoder mitigation (TEM) algorithm.

%The Sliding Window Kneedle (SWK) and Representation-Oriented Reverse Pattern (RORP) techniques have been proposed to aid in backdoor detection without labels. Additionally, a Trojaned Encoder Mitigation (TEM) algorithm has been developed to eliminate backdoors in SSL encoders.
\item We validate the effectiveness of the proposed SSL-Cleanse with extensive experiments on 1200 encoders. 
\end{itemize}



%To evaluate the effectiveness of SSL-Cleanse, we pre-trained 300 clean models and 300 backdoored models using backdoor attacks including SSL-Backdoor~\cite{sslbackdoor} and CTRL~\cite{CTRL} over BYOL~\cite{grill2020byol}, SimCLR~\cite{chen2020simple}, and MoCo V2~\cite{chen2020mocov2}, respectively. We perform a comprehensive evaluation on the performance of SSL-Cleanse with datasets such as CIFAR-10 and ImageNet-100. SSL-Cleanse obtains the average detection rate by $83.7\%$.

\section{Background and Related Works}\label{background}

\textbf{Self-Supervised Learning.}
%Supervised learning has proven to be highly effective in performing classifications, but it becomes problematic when labeled data is scarce or difficult to acquire. In response to this limitation, 
Leveraging the large unlabeled data available in the real world is essential.
Self-Supervised Learning (SSL) is the most popular method to learn representations from complex unlabeled data~\cite{chen2020simple,krishnan2022self}.  Pre-training an encoder with significant unlabeled data and fine-tuning it with a small amount of labeled data has been shown to achieve comparable performance to using large labeled datasets with supervised learning methods for various downstream tasks~\cite{chen2020simple,krishnan2022self,liu2022graph,chen2020mocov2,jaiswal2020survey,lan2019albert,he2020momentum}. Furthermore, SSL techniques that rely on instance discrimination, such as SimCLR~\cite{chen2020simple} and MoCo V2~\cite{chen2020mocov2}, have become increasingly popular for learning competitive visual representations through the use of contrastive loss. 
Typically, an SSL classification task involves pre-training an image encoder, constructing a classifier, and subsequent fine-tuning.
% In a classification task, the SSL pipeline typically comprises multiple phases: pre-training an image encoder and constructing a classifier, followed by fine-tuning.



\begin{figure*}[t!]
  \centering
   \includegraphics[width=\linewidth]{figures/workflow.pdf}
   \caption{The workflow of SSL-Cleanse detector. Step 1: Unlabeled data samples are processed through the SSL encoder to compute their representations. The SWK algorithm is then utilized to process representations and determine the number of clusters. Step 2: Using K-Means with the derived cluster number (\textit{K}) and representation, \textit{K} clusters are established. Subsequently, the Representation Oriented Trigger Reverse algorithm is employed to generate \textit{K} trigger patterns.
   Step 3: Accessing if any of the \textit{K} triggers are outliers in terms of their size and norm. The identified outlier indicates the encoder is Trojaned.}
   \label{fig:workflow}
\end{figure*} 


\begin{table}[h!]
\centering
\footnotesize
\setlength{\tabcolsep}{3pt}
\caption{Limitations of Current Backdoor Detectors in Assessing SSL Encoders, such as SSL-Backdoor~\cite{sslbackdoor} and CTRL~\cite{CTRL}. The encoder undergoes pre-training on CIFAR-10, while the built classifiers are evaluated on CIFAR-10, STL-10, and GTSRB. These datasets have labels that overlap, partially overlap, or do not overlap at all with the encoder. If NC~\cite{neural_cleanse} Index $>$ 2.0 and ABS~\cite{liu2019abs} REASR $>$ 0.88, the model is seen as Trojaned. }
\begin{tabular}{cccc}\toprule
\multirow{2}{*}{\makecell[c]{SSL Attack\\Method}} & \multirow{2}{*}{\makecell[c]{Downstream Task\\(Linear probe)}} & NC & ABS \\\cmidrule{3-4}
& & Anomaly Index & REASR \\\midrule
\multirow{3}{*}{\makecell[c]{SSL-\\Backdoor}} & CIFAR-10 & $2.05$ & $0.89$ \\
& STL-10 & $1.42$ & $0.34$ \\
& GTSRB & $1.68$ & $0.29$ \\\midrule
\multirow{3}{*}{CTRL} & CIFAR-10 & $1.52$ & $0.52$ \\
& STL-10 & $1.28$ & $0.44$ \\
& GTSRB & $1.16$ & $0.37$ \\\bottomrule
\end{tabular}
\label{t:limitation}
\end{table}


\noindent\textbf{SSL Backdoor Attacks.} %Malicious backdoor attacks, also known as Trojan attacks~\cite{badnets, target-attack}, involve the insertion of a specifically designed trigger into an input, which forces the backdoored model to classify it into a pre-defined target class with high confidence. Remarkably, the backdoored model can continue to function normally and produce nearly the same accuracy as its clean counterpart even after the trigger is removed from the input. 
In SSL backdoor attacks, the first line of research~\cite{jia2021badencoder} links the trigger to the downstream-task label, necessitating triggers to be appended to varying class inputs. Compared to other directions, it assumes a stronger threat model in which the
adversary dominates the training process, e.g., modifying the loss functions. Another methodology~\cite{liu2022poisonedencoder} focuses on specific input sets, formulating poisoned data by combining these inputs with their corresponding reference representations. However, it functions only with predetermined specific inputs and not with any inputs that contain embedded triggers. A distinct approach ~\cite{sslbackdoor, CTRL} avoids modifying the training phase, achieving a high attack success rate by merely connecting the trigger with desired unlabeled samples. Notably, compared to the first approach, these SSL attacks are much more challenging to discern since triggers are solely attached to the unlabeled targets, and combining triggers with other targets does not consistently yield similar representations~\cite{decree}. \textit{In our study, we aim to detect these third-category attacks.} %Currently, SSL backdoor attacks are carried out in three phases. In the first phase, unlabeled datasets are poisoned by adding triggers to a small percentage of target-class images. The second phase involves training the SSL encoder on the poisoned dataset to establish a connection between the trigger and target-class images. Finally, any downstream classifiers fine-tuned on the backdoored encoder inherit the backdoor behavior~\cite{sslbackdoor,liu2022poisonedencoder,CTRL,jia2021badencoder,xue2022estas}. %These backdoor attacks have been shown to achieve an attack success rate of over 98\% on the ImageNet-100 dataset~\cite{jia2021badencoder, xue2022estas}. The threat posed by Trojan attacks inserted in SSL encoders is not only attributed to their high attack success rate but also arises from their ability to operate surreptitiously and propagate extensively across downstream tasks. 



\noindent\textbf{Limitations of Related Backdoor Defense.}
%\textbf{Related Backdoor Defense.}
%\label{relatedwork}
%In addition to those discussed in the introduction, we also provide a brief overview of other related works on backdoor detection. 
%\textbf{Supervised Learning.} 
%Multiple backdoor scanners~\cite{kolouri2020universal,zhang11, wang2020c,liu2018fine,wang2020a,neural_cleanse,liu2019abs}  in supervised learning is proposed.  
%Backdoor detection and mitigation in supervised learning have been extensively studied. ULP~\cite{kolouri2020universal} is a method that trains a classifier to differentiate between benign and Trojaned models using a set of universal input patterns. %Similarly, researchers in \cite{HuangS} proposed the one-pixel signature approach, which trains a classifier to predict the model's benignity based on its one-pixel signature. In \cite{qiao}, a trigger distribution generation method was proposed for backdoor detection. 
%On the other hand, researchers in \cite{zhang11, wang2020c} leveraged the differences in adversarial examples for benign and Trojaned models to detect backdoors. %TABOR~\cite{TABOR} used explainable AI techniques to scan for backdoors, while in \cite{Xu2019}, backdoors were detected using Meta Neural Analysis. 
%In \cite{liu2018fine}, pruning and fine-tuning were combined to weaken or even eliminate backdoors. Wang et al. proposed the use of randomized smoothing to certify model robustness against backdoors in \cite{wang2020a}. 
%Other works, such as those by Ong et al. \cite{Ong}, Gao et al. \cite{Gao2019}, Chen et al. \cite{chen2018detecting}, Chou et al. \cite{chou}, Du et al. \cite{Du2}, Liu et al. \cite{Liu2017}, and Ma et al. \cite{Ma2019NICDA}, aimed to detect if a provided input contained a trigger. 
Prior scanners like NC~\cite{neural_cleanse} and ABS~\cite{liu2019abs} face challenges in detecting backdoors in SSL encoders.
% Detecting backdoors in SSL encoders using prior scanners, such as NC~\cite{neural_cleanse} and ABS~\cite{liu2019abs}, poses significant challenges.
The primary reasons include the often-unknown downstream tasks/labels. While the linear probe method, i.e., constructing downstream classifiers from various datasets using pre-trained encoders, offers an alternative, it is not efficacious in scanning SSL encoders. For example, we built a backdoored encoder using CIFAR-10~\cite{cifar10} with a specific label, \textit{airplane}, as the target. This encoder was then utilized to train three distinct downstream classifiers on CIFAR-10, STL-10~\cite{stl10}, and GTSRB~\cite{GTSRB}. The results from applying NC and ABS to these classifiers are shown in Table~\ref{t:limitation}. For scenarios where the encoder and the downstream task share the same dataset, both NC and ABS can detect Trojaned classifiers and hence the backdoored encoders for SSL-Backdoor~\cite{sslbackdoor}, with encoder's Anomaly Index of 2.05 $>$ 2.00 in NC and a REASR of 0.89 $>$ 0.88 in ABS. Yet, the detection capability requires the knowledge of the downstream tasks and the  detection capability diminishes when the datasets (like STL-10 or GTSRB) either only partially overlap or do not align at all with CIFAR-10. For the CTRL instance, both tools fail in detection, even when there's label congruence between the encoder and classifiers. The introduction of global frequency triggers in CTRL exacerbates the detection difficulty. %This underscores a broader implication: SSL-Backdoor encoders remain elusive to current detection mechanisms, especially when there's limited knowledge about the attack target or the related downstream tasks, often a byproduct of privacy considerations. 
While the recent study DECREE~\cite{decree} effectively detects backdoors in training-controlled attackers, like BadEncoder~\cite{jia2021badencoder}, it acknowledges the failure in identifying stealthy training-agnostic attacks~\cite{sslbackdoor, CTRL}, e.g., $\sim$ 50\% detection accuracy. Also, scanners such as PatchSearch~\cite{tejankar2023defending} and ASSET~\cite{pan2023asset} are orthogonal to our method since they are introduced to identify poisoned samples in a training dataset. In our threat model, the SSL encoder is required but there's no requirement for the poisoned training dataset.
%and Universal Adversarial Training~\cite{kolouri2020universal}, may have the potential to safeguard SSL downstream classifiers, detecting and mitigating triggers in the SSL encoder before its widespread dissemination remains a challenging task. Detecting Trojans in SSL encoders is a challenging task due to the fact that downstream tasks are often unknown, dataset labels may not be available, and even the original training dataset may be partially or entirely inaccessible.  In a recent study, ASSET~\cite{pan2023asset} focuses on detecting poisoned samples within a training dataset. The ASSET approach accomplishes the task of identifying the poisoned examples by calculating the model's different behaviors on poisoned samples and  benign samples. The effectiveness of ASSET's detection may be limited in real-world applications where an SSL encoder is released but a portion or the entirety of the training set is not disclosed to defenders. To the best of our knowledge, SSL-Cleanse is the first practical work to
%detect and mitigate the Trojan attacks in SSL encoders without accessing any downstream labels or the full training set.


\section{SSL-Cleanse Design Overview}\label{setting}
%Our aim is to detect and prevent backdoor attacks in a SSL encoder. To achieve this goal, we investigate two representative SSL backdoor attack methods: SSLBackdoor~\cite{sslbackdoor} and CTRL~\cite{CTRL}. By examining these methods, we develop a comprehensive and robust detection system capable of accurately identifying and preventing these types of attacks.

%\textbf{Attack Model.} Our attack model is consistent with that of prior work, i.e. SSLBackdoor~\cite{sslbackdoor}and CTRL~\cite{CTRL}. SSL-Backdoor~\cite{sslbackdoor} proposed a backdoor attack via data poisoning in training data, wherein a specific trigger pattern is placed at random locations of target class inputs. To evaluate the performance of our detector, we generate a poisoned dataset following this design and train Trojaned encoders by maximizing the data representation similarity of the trigger inputs and target class inputs.
%We also incorporate the stealthy global trigger in the frequency domain, as proposed by CTRL~\cite{CTRL}, to verify the performance of our detector. 

\textbf{Defense Assumptions and Goals.}
We assume that the defender has access to a pre-trained SSL encoder, a small portion of the unlabeled dataset (which could be distinct from the training set, i.e., SSL-Cleanse does not require attackers to disclose their poisoned dataset). % and computational resources such as GPUs or GPU-based cloud services to evaluate or modify SSL encoders. 

\textbf{Goals.} We have two specific goals:

\begin{itemize}
\item \textbf{Detecting backdoor:} Our aim is to make a binary determination regarding the potential backdoor infection of a given SSL encoder. If infected, we also want to identify the potential target classes of the backdoor attack. 
 
%\item \textbf{Identifying backdoor:} We aim to identify the expected operation of the backdoor. More specifically, we plan to reversely generate the trigger used by the attack.

\item \textbf{Mitigating backdoor:} We plan to reversely generate the trigger used by the attack. Our ultimate goal is to deactivate the backdoor, making it ineffective. This requires eliminating the backdoor while preserving the classification performance of the SSL encoder for normal inputs. %To achieve this, we plan to employ our Trojan Encoder Mitigation (TEM) described in Section~\ref{sec:mitigation} to eliminate the backdoor while preserving the classification performance of the SSL encoder for normal inputs.
\end{itemize}

\noindent\textbf{Challenges and Motivation.} 
%The primary objective of the SSL-Cleanse detector is to reverse-engineer Trojan triggers of small size or minimal $l_1$-norm magnitude that can mimic the representations of target-class samples. 
%\textit{Challenge 1:}
An inherent challenge of SSL backdoor detection lies in the indeterminate nature of the target class. While this can be addressed by iteratively searching through all classes, this method is not suitable for SSL encoders due to the uncertainty in class numbers for unlabeled datasets in SSL. Traditional clustering methods, like K-Means~\cite{kmeans}, necessitate manual predetermination of cluster counts $K$. We observe that though automatic cluster number determination methods, such as the Kneedle algorithm~\cite{kneedle}, exist, they struggle with accurate class number prediction for SSL encoders. For instance, the Kneedle algorithm identifies only 20 classes within the ImageNet-100 dataset. This discrepancy could arise from noisy samples affecting the clustering of SSL representations.
%\textit{Motivation 1:}
To improve the prediction accuracy of cluster numbers, we are motivated to further introduce the Sliding-Window Kneedle (SWK) algorithm. It is designed to mitigate the noise's influence by introducing a sliding window to average the adjacent Keedle scores. 

Another challenge is how to efficiently generate triggers and identify outlier triggers given predicted cluster numbers, encoder, and sampled unlabeled dataset.  This challenge is exaggerated when we target to detect attacks~\cite{CTRL} based on frequency-domain global triggers since one cannot identify outlier triggers according to the trigger size. 
For these reasons, we are motivated to propose Representation-Oriented Trigger Reverse (ROTR) method to generate triggers via clustered representations. For identifying patch-wise trigger SSL attacks, specifically SSL-Backdoor as referenced in ~\cite{sslbackdoor}, we continue to rely on trigger size as a key outlier detection metric. However, when detecting the frequency-domain SSL attack, CTRL as cited in ~\cite{CTRL}, we propose to use the magnitude of the trigger as the pivotal criterion. These detection methodologies for both attack types are unified under RORP. Upon identifying the triggers, it becomes feasible to pinpoint their related target-class clusters. Following this, we introduce the Self-supervised Clustering Unlearning (SCU) approach to mitigate backdoor threats in SSL encoders, resulting in a cleansed encoder. 

%Using these refined class counts, we can iteratively hypothesize the target-class cluster and, with our Representation-Oriented Trigger Reverse (ROTR) method, generate corresponding triggers. From K clusters, K triggers emerge. If any are outliers, the encoder is deemed Trojaned; if not, it's considered safe.

%Beyond just using trigger size to identify outliers for patch-wise attacks, e.g., SSL-Backdoor~\cite{sslbackdoor}, we recognize trigger magnitude as a key criterion for global attacks stemming from frequency-domain disturbances, e.g., CTRL~\cite{CTRL}. Our proposed Size-Norm Trigger Outlier Detection (STOD) is designed to detect both types of disturbances effectively. Once we have the triggers, their corresponding target-class clusters become identifiable. Subsequently, we introduce a Self-supervised Clustering Unlearning (SCU) strategy to neutralize SSL encoder backdoor threats, leading to a purified encoder.

\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{figures/SWK.pdf}
\caption{Comparison of our SWK method and direct search (Kneedle) method on ImageNet-100 dataset. Our SWK method yields more stable and accurate K.}
\label{fig:SWK}
\end{figure}

%\begin{figure}[h!]
  %\centering   %\includegraphics[width=\linewidth]{figures/reverse.pdf}
  % \caption{The working flow of Representation Oriented Reverse Pattern (RORP). We begin by selecting images $x^j_i$ from each cluster $D_i$, and initializing $r_i$. These inputs are then fed into a pre-trained SSL encoder to obtain representations. We iteratively update $\Delta_i$ and mask $m_i$ to generate representations that are similar to those of $x^j_i$. This process results in triggers generation for k clusters, which are subsequently forwarded to the outlier detector module for further processing.}
   %\label{fig:reverse}
%\end{figure} 

\section{SSL-Cleanse Detector}  \label{approach}




%We propose a novel, effective, and practical approach \textit{SSL-Cleanse} for identifying and mitigating backdoors inside pre-trained Trojaned SSL encoders. As depicted in Figure~\ref{fig:overview}, the proposed framework consists of two main components: a detector and a mitigator. In the detector, we propose a Sliding Window Kneedle (SWK) optimized on previous Kneedle algorithm~\cite{Kneedle} to produce $K$ clusters based on the representations of the SSL encoder. We then introduce the Representation-Oriented Trigger Reverse (ROTR) method to generate a reversed pattern for each cluster. By utilizing these $K$ patterns, we can perform outlier detection to determine if the SSL encoder is Trojaned. In the mitigator, we introduce a method called Trojan Encoder Mitigator (TEM), which is designed to efficiently eliminate the Trojans present in the backdoored SSL encoders.


\textbf{Workflow.}
\label{sec:Detection}
The workflow of the SSL-Cleanse detector, as shown in Figure~\ref{fig:workflow}, consists of three steps. First, the SSL encoder processes a small amount of unlabeled data to generate representations. Using these, the Sliding Window Kneedle (SWK) algorithm determines the cluster count $K$. Subsequently, K-Means creates \textit{K} clusters, from which the Representation Oriented Trigger Reverse algorithm derives \textit{K} trigger patterns. Finally, if any trigger significantly deviates in size or magnitude/norm, the encoder is deemed Trojaned. The details of each step and the associated algorithms are elaborated below.  %At first, unlabeled data samples are processed through the SSL encoder to compute their representations. Notably, it is not required to have the whole unlabeled dataset, and a small fraction of sampled dataset, e.g. $<10\% $ amount, is enough for an SSL-Cleanse detector. The SWK algorithm is then utilized to process representations and determine the number of clusters. Secondly, using KMeans with the derived cluster number (\textit{K}) and representation, \textit{K} clusters are established. Subsequently, the Representation Oriented Trigger Reverse algorithm is employed to generate \textit{K} trigger patterns. The third step is to access if any of the \textit{K} triggers are outliers in terms of their size and norm. The identified outlier indicates the encoder is Trojaned.


%The workflow of the detector, as shown in Figure~\ref{fig:workflow}, begins 



%by feeding a small fraction of the unlabeled training dataset to a pre-trained SSL encoder to obtain input representations. These representations are then clustered into $K$ groups using the  SWK method. Next, for each cluster $i$, the ROTR technique is used to invert the input patterns. Then, the outlier detector is utilized to make a binary decision regarding the pre-trained encoder's benign or Trojaned status.  Since the target class is unknown, we assume that each clustering could potentially be the target class. Therefore, we examine each cluster of the encoder and search for the existence of a cluster where the encoder can generate similar representations to the target class with significantly smaller modifications to an image.

\begin{algorithm}[htb!]
\caption{Sliding Window Kneedle for SSL Cluster Num.}
   \label{alg:SWK}
\begin{algorithmic}
    \STATE {\bfseries Input:} SSL samples $D$, encoder $f$, pre-defined K\_list
    \STATE {\bfseries Output:} predicted cluster number $K$ 
    \STATE initialize clusters\_list, s\_list, padded\_s\_list, d\_list = []
    \FOR{$i=0$ {\bfseries to} $len(K\_list)$}
    \STATE clusters\_list.$append(k\-means(f(D)$ ,K\_list[$i$]))
    \STATE s\_list$.append(silhouette(f(D)$,clusters\_list[$i$]))
    \ENDFOR
    \STATE initialize window size $w$ as a small odd number, e.g., 3
    \STATE initialize swk\_s\_list to zero values of s\_list's structure
    \STATE padded\_s\_list $ \gets$ pad $\frac{w-1}{2}$ zeros to head and tail of $s\_list$
    \FOR{$i=1$ {\bfseries to} $len$(s\_list)}
    % \STATE \COMMENT{Target}
    %\FOR{$j=0$ {\bfseries to} $w$}
    % \STATE swk\_s\_list[i]  += padded\_s\_list[i+j]
    %\ENDFOR
    %\STATE swk\_s\_list[i]= swk\_s\_list[i]/w
    \STATE swk\_s\_list[i]= $\frac{1}{w}\sum_{j=0}^w $ padded\_s\_list[i+j]
     \STATE d\_list.$append$($norm$(swk\_s\_list[i])-$norm$(K\_list))
    \ENDFOR    
    \STATE $K \gets$ index of maximum entry in (d\_list)
\end{algorithmic}
\end{algorithm}

\noindent\textbf{Sliding Window Kneedle.} 
Class numbers for unlabeled training samples in SSL encoders are often unknown, even to model developers. %Determining the class number 
%$K$ typically involves manual settings or automatic search methods. 
At first glance, clustering appears as an intuitive approach to determining class numbers. Yet, conventional methods such as K-Means~\cite{kmeans} demand a predetermined value for $K$. Opting for an automated determination of the cluster number $K$ offers a more flexible solution. To this end, we initially investigated existing automatic methods, with a focus on the Kneedle algorithm~\cite{kneedle}.


Given the SSL samples $D$ and encoder $f$, the Kneedle algorithm starts by initializing a list with potential $K$ values to examine. For each $K$ in this list, the K-Means method clusters the representation $f(D)$, producing a clustering outcome. The silhouette score for this clustering ranges from -1 to 1. A high value signifies that the data point is well-suited to its own cluster and has a poor fit with neighboring clusters. 

%Given the SSL samples $D$ and encoder $f$, the Kneedle first initializes a list of $K$ values to be explored. Then for each $K$ value in this list, a K-Means function is performed to cluster representation $f(D)$ to generate a clustering result. For the clustering result, a silhouette score range lies between -1 and 1, with a high value signifying that the data point is well-suited to its own cluster and has a poor fit with neighboring clusters. 

The silhouette score for a particular data point is computed using: $ s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}} $  where
$s(i)$ represents the silhouette coefficient for data point $i$,
$a(i)$ denotes the mean distance between the $i^{th}$ data point and other points within the same cluster,
$b(i)$ is the smallest mean distance between the $i^{th}$ data point and points in a different cluster, minimized over all clusters. The clusters' overall silhouette score is the mean silhouette coefficient of all instances.

The Knee/elbow point on the plot, which represents silhouette scores over varying $K$ values, indicates the optimal position.
% On the plot depicting silhouette scores across the investigated $K$ values, the Knee/elbow point serves as an indicative optimal position.
To identify the Knee point, one should normalize the silhouette curve, adjust the origin to $[0,0]$ and ensure the endpoint aligns with $[1,1]$.
Next, measure the distance of each point on the curve from the direct line connecting the origin $[0,0]$ to the endpoint $[1,1]$.
The point that is furthest from this direct line is recognized as the knee point of the curve.
%The Knee/elbow point of the plot between silhouette scores over the explored $K$ list is the heuristic optimal point. The Knee point is derived by the following steps: first, one can normalize the silhouette curve by shifting the origin to $[0,0]$ and scaling the endpoint to $[1,1]$. secondly, one can calculate the silhouette curvature distance of each point from the line drawn from the origin $[0,0]$ to the endpoint $[1,1]$. Last, the point with the maximum distance from this line represents the knee point of the curve.

In Figure~\ref{fig:SWK}, we illustrate the silhouette curvature distance for each point $K$. The line corresponding to the direct search showcases the application of the Kneedle algorithm. The most considerable distance is observed when $K=20$, indicating the elbow point of the Silhouette score is 20. Nonetheless, this estimation might not be precise, given that the SSL encoder 
$f$ is trained on the ImageNet-100 dataset. This discrepancy may arise from the impact of noisy samples on the clustering of SSL representations.

%We plot the silhouette curvature distance of each point K in Figure~\ref{fig:SWK}. The direct search line represents the employment of the Kneedle algorithm. The maximum distance is located when $K=20$, thus the elbow point of the Silhouette score is 20. However, it is not very accurate since this SSL encoder $f$ is trained on the ImageNet-100 dataset. We identify that the estimation is possibly due to the influence of noisy samples on SSL representation clustering.


%Algorithm~\ref{alg:detection} provides a more detailed description of the detector methodology. The input to the algorithm is a set of unlabeled samples $D$ and an encoder $f$. The algorithm comprises three main steps. In Step 1, the data is classified into $K$ clusters using the SWK algorithm. The resulting clusters are denoted by an array of $[D_1, D_2, ..., D_K]$. In Step 2, $p$ images are randomly sampled from each cluster, and each cluster is considered a potential target for the backdoor attack. Here we set $p=32$ and its study is described in supplementary materials.  An optimization strategy is employed to determine the \textit{minimum} reversed trigger required to elicit a comparable representation for all other classes. The reversed trigger for cluster $i$ is defined as $m_i \cdot \Delta_i$, where mask $m_i$ represents the location of the trigger pixel, and $\Delta_i$ denotes the pixel value of the trigger. In Step 3, we employ the outlier detection technique to identify all potential triggers $t_i$ generated from each cluster. If we find a trigger that is significantly smaller than the other candidates, we classify it as an outlier and store it in a set $t_s$. If $t_s$ remains empty, we consider the encoder to be benign; otherwise, we classify it as a Trojaned encoder.




%The proposed approach assumes that the defender has access to an $\epsilon\%$ unlabeled training samples and a pre-trained SSL encoder. To perform the clustering, we explore the silhouette analysis clustering method, denoted by Kneedle~\cite{kneedle}. The silhouette score measures the degree to which each point belongs to its cluster compared to other clusters and ranges from -1 to 1, with higher scores indicating better clustering. By plotting the curvature of the silhouette score against different numbers of clusters, one can search for the elbow point to indicate the optimal number of clusters. However, we illustrate, in Figure~\ref{fig:SWK}, that directly applying to the existing  Kneedle algorithm is difficult to derive a proper $K$.  This is because the silhouette score plot may have multiple peaks and can suffer from local optima. 

%To address the issue of noise caused by the high dimensionality of the encoder's outputs and diverse representation, we optimized the direct search method by incorporating a sliding window technique with a window size of $w$. As Algorithm ~\ref{alg:SWK} shows, the new algorithm named Sliding Window Kneedle (SWK), takes encoder $f$, SSL samples $D$ and pre-defined K\_list as inputs, and returns predicted cluster number $K$. It uses the same methods as the Kneedle algorithm to calculate K-Means clustering clusters\_list[i] and silhouette scores for each pre-defined K\_list[i]. After calculating the silhouette score list s\_list, SWK introduces the window size to calculate the average silhouette score, denoted as swk\_s\_list, of the adjacent $w$ scores. The padded zeros to s\_list to form padded\_s\_list are needed for this calculation. The silhouette curvature distance d\_list[i] is calculated by the difference of the normalized swk\_s\_list and K\_list. The normalization is to [0,1]. The index of maximum entry in d\_list is the predicted $K$. As Figure~\ref{fig:SWK} shows, the resulting SWK smoother curve is more effective in identifying the elbow point and reducing noise, leading to a more accurate cluster prediction.
To address the high-dimensional noise and varied representations in the encoder's outputs, we enhanced the direct search (Kneedle) approach by introducing a sliding window technique with a window size of 
$w$. The underlying idea is to compute the average silhouette scores for neighboring \(K\) values, aiming to refine the silhouette curvature. As depicted in Algorithm \ref{alg:SWK}, our new algorithm, termed as Sliding Window Kneedle (SWK), ingests the encoder $f$, SSL samples $D$, and a pre-set K\_list. The output is the anticipated cluster count, $K$. The foundation of SWK relies on the Kneedle algorithm's methodology to determine K-Means clustering clusters\_list[i] and silhouette scores for every specified K\_list[i]. Post silhouette score list (s\_list) computation, the SWK introduces a window dimension to derive the mean silhouette score, symbolized as swk\_s\_list, for the neighboring $w$ scores. This process necessitates padding zeros to s\_list, resulting in padded\_s\_list. The silhouette curvature distance, d\_list[i], is deduced from the disparity between the normalized swk\_s\_list and K\_list, with normalization between [0,1]. The position of the largest value in d\_list dictates the predicted 
$K$. Figure~\ref{fig:SWK} illustrates that the evolved SWK curve offers better clarity in pinpointing the elbow juncture, diminishes noise, and assures a more precise cluster estimation.



% \begin{figure}[ht!]
%   \centering
%    \includegraphics[width=\linewidth]{figures/box.pdf}
%    \caption{Figure (a) shows the relative L1 norm of $m$ of the Trojan class's reversed pattern is significantly smaller than clean classes, which is treated as an outlier. Figure (b) shows the relative L1 norm of $m\cdot \Delta$. Similar to Figure (a), $m\cdot \Delta$ is significantly smaller than clean classes in the backdoored model.}
%    \label{fig:box}
% \end{figure}






\begin{algorithm}[htb!]
\caption{Representation-Oriented Trigger Reverse}
   \label{alg:ROTR}
\begin{algorithmic}
    \STATE {\bfseries Input:} clustered samples $D_{i \in [1,K] }$, encoder $f$, cluster number $K$, epoch number $E$
    \STATE {\bfseries Output:} masks $m^1_i$,$m^2_i$ perturbation $\Delta^1_i$,$\Delta^2_i$ of $K$ clusters
    \FOR{$i=1$ {\bfseries to} $K$}
    \STATE initialize masks $m^1_i$,$m^2_i$ and perturbation $\Delta^1_i$,$\Delta^2_i$
    \FOR{$e=1$ {\bfseries to} $E$}
    % \STATE \COMMENT{Target}
    \STATE $x_i \gets$ randomly sample an image from $D_i$ \# target
    \STATE $x_j \gets$ randomly sample an image from $D_{j\neq i}$
    \STATE $x^1_j \gets (1-m_i^1) \cdot x_j+m^1_i\cdot \Delta^1_i$
    \STATE $x^2_j \gets (1-m_i^2) \cdot x_j+m^2_i\cdot \Delta^2_i$
    \STATE $loss_{size} \gets \mathcal{L}^{size}_{MSE}(f(x_i), f(x_j^1))$
    \STATE $loss_{norm} \gets \mathcal{L}^{norm}_{MSE}(f(x_i), f(x_j^2))$
    \STATE $m^1_i, \Delta^1_i \gets update(m^1_i, \Delta^1_i, loss_{size})$
    \STATE $m^2_i, \Delta^2_i \gets update(m^2_i, \Delta^2_i, loss_{norm})$
    \ENDFOR
    \ENDFOR
\end{algorithmic}
\end{algorithm}

% \begin{algorithm}[htb!]
% \caption{Encoder Detection}
%    \label{alg:detection}
% \begin{algorithmic}
%     \STATE {\bfseries Input:} unlabeled samples $D$, encoder $f$
%     \STATE {\bfseries Output:} Benign or Trojan
%     \STATE {\bfseries Step 1: Cluster unlabeled samples D with SWK}
%     \STATE $K \gets SWK(D)$
%     \STATE ${D_1, D_2, ..., D_K} \gets Kmeans(D, K, f)$
%     \STATE {\bfseries Step 2: Representation Oriented Trigger Reverse (ROTR) on each cluster}
%     \FOR{$i=1$ {\bfseries to} $K$}
%     \STATE initialize mask $m_i$ and delta $\Delta_i$
%     \STATE $D_{j} \gets$ Randomly sample $p$ images from $ D-D_i$
%     \STATE $m_i, \Delta_i \gets update(m_i, \Delta_i, D_{j})$
%     \ENDFOR
%     \STATE {\bfseries Step 3: Abnormal trigger detection}
%     \STATE $t_s \gets outlier(m_i, \Delta_i)$
%     \IF{$t_s$ is empty} 
%     \STATE return Benign
%     \ELSE
%     \STATE return Trojan
%     \ENDIF
% \end{algorithmic}
% \end{algorithm}



\noindent\textbf{Representation-Oriented Trigger Reverse.}
\label{sec:Detection:loss}
Once the cluster count $K$ is determined, K-Means can be readily employed to produce $K$ clustered samples, denoted by $D_{i \in [1,K] }$, using the provided $D$. The subsequent goal is to backtrack and identify Trojan triggers that are either compact in size or have a minimal $l_1$-norm magnitude that can mimic the representations of target-class samples.
Algorithm~\ref{alg:ROTR} presents the ROTR approach. The core concept involves creating two triggers for each cluster representation. It then determines which representation clusters can yield outlier triggers, either having smaller patch-based trigger sizes \(|m_i^1|\) for patch-based trigger attacks or lesser trigger magnitudes \(m_i^2 \cdot \Delta_i^2\) for global-trigger attacks. Initially, we designate trigger masks \( m_i \) to specify the location of the trigger pixel, while \( \Delta_i \) signifies the associated pixel value for the \( i \)-th trigger. Consequently, the trigger \( r_i \) is computed as \( m_i \cdot \Delta_i \). To identify both patch-based triggers and frequency-domain global triggers, we suggest initiating two distinct trigger sets: \( r_i^1 = m_i^1 \cdot \Delta_i^1 \) and \( r_i^2 = m_i^2 \cdot \Delta_i^2 \), where \( m_i^1 \) and \( m_i^2 \) represent masks and \( \Delta_i^1 \) and \( \Delta_i^2 \) delineate the pixel values of the triggers. For each cluster, we conduct \( E \) epochs to produce these twin trigger sets. Specifically, we randomly select an image from \( D_i \) to as the clean sample \( x_i \) and subsequently get sample to add trigger, termed \( S_j \), from \( D_{j \neq i} \). We then affix the pre-established two trigger sets to \( x_j \), resulting in \( x_j^1 \) and \( x_j^2 \) respectively.

%Algorithm~\ref{alg:ROTR} introduces ROTR technique. 
%First, we define trigger masks $m_i$ to denote the position of the trigger pixel and $\Delta_i$ represents the corresponding pixel value of the $i$-th trigger. Thus the trigger $r_i$ can be derived by $m_i \cdot \Delta_i$. To detect both patch-based trigger and frequency-domain global triggers, we propose to initialize two sets of triggers, $r_i^1 = m_i^1\cdot \Delta_i^1$ and $r_i^2=m_i^2\cdot \Delta_i^2$, respectively, where $m_i^1$ and $m_i^2$ are masks and $\Delta_i^1$ and $\Delta_i^2$ are pixel values of triggers.  Then for each cluster, we perform $E$ epochs to generate these two sets of triggers. In particular, we randomly choose $p$ images from $D_i$ as current target samples $S_i$, and then sample non-target samples as $S_j$ from $D_{j \neq i}$. Then we attach the initialized two sets of triggers to the $x_j$ as $x_j^1$ and $x_j^2$ respectively. 

%First, we choose a clean image $x_i^j$ from a cluster $i$ as a potential target class, sample a random image $y_j$ ($j\neq i$) and create mask $m_i$ and $\Delta_i$ for the same cluster. We define the trigger image  for cluster $i$ as $r_i$ shown in Equation~\ref{e:patch trigger}. 
The clean image $x_i$ 
 and images with trigger $x_j^1$ and $x_j^2$  are then sent to an encoder $f$, and two separate loss functions are employed to update the trigger such that its representation can have more similarity with $x_i$'s feature. The loss functions used in the ROTR to optimize $m_i^1$, $m_i^2$ and $\Delta_i^1$, $\Delta_i^2$ on the $i^{th}$ cluster are given by Equation~\ref{e:lsize} and Equation~\ref{e:lnorm}, respectively. Each loss function comprises two components. The foremost objective of the first term is to guarantee the similarity between the image patched with the trigger and the target class image in the feature space. The second term is responsible for constraining the size or the trigger magnitude/norm of the reversed trigger, we adapted $\lambda$ dynamically during optimization in our experiment. The dynamic scheduler is described in the supplementary material.  Finally, the resulting trigger patterns are sent to an outlier detector for determination.


%\begin{equation} 
%    \label{e:patch trigger}
%    %x^{\prime}=p(x,m,\Delta)=(1-m) \cdot x + m \cdot \Delta
%    r_{i}=(1-m_i) \cdot y_j + m_i\cdot \Delta_i
%\end{equation}

{\footnotesize
\begin{equation}
    \label{e:lsize}
        \mathcal{L}^{size}_{MSE}(f(x_i), f(x_j^1))=-\frac{<f(x_i), f(x_j^1)>}{||f(x_i)|| \cdot ||f(x_j^1)||} + \lambda \cdot |m^1_i| 
\end{equation}
}

{\footnotesize
\begin{equation}
    \label{e:lnorm}
        \mathcal{L}^{norm}_{MSE}(f(x_i), f(x_j^2))=-\frac{<f(x_i), f(x_j^2)>}{||f(x_i)|| \cdot ||f(x_j^2)||} + \lambda \cdot |m^2_i\cdot \Delta^2_i| 
\end{equation}
}


%Here we note that the variable $m_i$ signifies the position of the trigger pixel, while $\Delta_i$ represents the corresponding pixel value of the trigger. The function $f(\cdot)$ denotes the encoder function, and $x_i$ refers to an image belonging to $i^{th}$ cluster. 
Here $<a, b>$ and $||a||$ represent the cosine similarity of $a$ and $b$, and the $l2$-norm of $a$, respectively.  



%\begin{algorithm}[hb!]
%\caption{Size-Norm Trigger Outlier Detector}
%   \label{alg:outlier}
%\begin{algorithmic}
%    \STATE {\bfseries Input:} a trigger size list $[(m_1^1, \Delta_1^1), ..., (m_K^1,\Delta_K^1)]$ and a trigger norm list $[(m_1^2, \Delta_1^2), ..., (m_K^2,\Delta_K^2)]$.
%    \STATE {\bfseries Output:} Benign or Trojan with a trigger dictionary $t_s$
%    \STATE initialize $t_s=\{\}$
%    \FOR{$i=1$ {\bfseries to} $K$}
%    \IF{ \textit{is\_outlier($|m_i^1|$, $|m_{[1:K]}^1|$)}}
%    \STATE $t_s.append(\{i,m_i^1 \cdot \Delta_i^1\})$
%    \ELSIF{\textit{is\_outlier($|m_i^2\cdot \Delta_i^2|$, $|m_{[1:K]}^2\odot \Delta_{[1:K]}^2|$)}}
%    \STATE $t_s.append(\{i:m_i^2 \cdot \Delta_i^2\})$
%    \ENDIF
%    \ENDFOR
%    \IF{$t_s$ are empty} 
%    \STATE return Benign
%    \ELSE
%    \STATE return Trojan, $t_s$
%    \ENDIF
%\end{algorithmic}
%\end{algorithm}



\noindent\textbf{Size-Norm Trigger Outlier Detector.}
Beyond just using trigger size to identify outliers for patch-wise trigger attacks, e.g., SSL-Backdoor~\cite{sslbackdoor}, we recognize trigger magnitude/norm as a key criterion for global-trigger attacks stemming from frequency-domain disturbances, e.g., CTRL~\cite{CTRL}. Our proposed Size-Norm Trigger Outlier Detection (STOD) is designed to detect both patch-based and global-based triggers. In particular, given a trigger size list $[(m_1^1, \Delta_1^1), ..., (m_K^1,\Delta_K^1)]$ and a trigger norm list $[(m_1^2, \Delta_1^2), ..., (m_K^2,\Delta_K^2)]$, the STOD outputs the detection result, i.e., benign or Trojaned with a trigger dictionary $t_s$. For $K$ cluster, we iteratively check if the trigger size $|m^1_i|$ and trigger norm $|m_i^2\cdot \Delta_i^2|$ is outlier in the list of $|m_{[1:K]}^1|$  and $|m_{[1:K]}^2\cdot \Delta_{[1:K]}^2|$ using the function \textit{is\_outlier($x_i, \boldsymbol{x}$)}. Here \textit{$is\_outlier(x_i, \boldsymbol{x})$} returns True if $M(x_i, 
\boldsymbol{x})>2$, otherwise False; The Anomaly Index function $M(x_i, \boldsymbol{x})=\frac{|x_i-median(\boldsymbol{x})|}{c\cdot median(|x-median(\boldsymbol{x})|)}$ is used to ascertain if $x_i$ is an anomaly. $c$ is a constant estimator which equals $1.4826$. If we discover a trigger that is substantially smaller than the other candidates, we classify it as an outlier and store its cluster number $i$ and itself in a dictionary $t_s$, i.e., $t_s[i]=m_i\cdot \Delta_i$. If $t_s$ remains empty, we conclude that the encoder is benign. However, if $t_s$ contains any triggers, we classify the encoder as a Trojaned encoder.


%Once $K$ potential triggers for each cluster have been generated, the size of each trigger is determined by computing $|m_i \cdot \Delta_i|$ or $|m_i|$. This condition is dependent on the attacking method. In the case of the attacking method being CTRL~\cite{CTRL}, which uses a frequency-domain trigger, the trigger size is defined as $|m_i \cdot \Delta_i|$ due to its large $m_i$ and small $\Delta_i$. For other attacking methods in this paper, the trigger size is set as $|m_i|$. In practical settings, both trigger sizes could be generated and examined to identify the presence of an abnormal trigger size. To identify all potential 
%triggers $T_i$ generated from each cluster, we employ the outlier setting from \cite{neural_cleanse}. \textcolor{blue}{In detail, given a trigger size list $\delta=\{\delta_1, \delta_2, ..., \delta_i\}$, we calculate $MAD=c\cdot median(|\delta-median(\delta)|)$. Then, we use $M_i = \frac{|\delta_i-median(\delta)|}{MAD}$ to ascertain if $x_i$ is an anomaly. $c$ is a constant estimator which equals to $1.4826$. As Figure~\ref{fig:box2} shows, by setting a threshold, i.e., 2, we can identify anomalies in the data, i.e., the outlier of infected trigger is determined (small $x_i$ corresponds to large $M_i$).}

%$M(x_i, \boldsymbol{x})=\frac{|x_i-median(\boldsymbol{x})|}{c\cdot median(|x-median(\boldsymbol{x})|)}$

%\textit{$is\_outlier(x_i, \boldsymbol{x})$} returns True if $M(x_i, 
%\boldsymbol{x})>2$, otherwise False.




% \begin{algorithm}[htb]
%    \caption{Trojan mitigation via Self-supervised Clustering Unlearning (SCU)}
%    \label{alg:mitigation}
% \begin{algorithmic}
%     \STATE {\bfseries Input:} unlabeled dataset $D$, abnormal triggers $t_s={[t_{1}, t_{2}, ..., t_{K}]}$, Trojaned encoder $f$
%     \STATE {\bfseries Output:} a clean encoder $f^{\prime}$
%     \STATE $f^{\prime} \gets f$
%     \FOR{$x_i$ in $D_i$, $i \in K $}
%     \STATE $x_{i1} \gets aug_1(x_i)$
%     \STATE $t_i \gets$ randomly selected from $t_s$  
%     \STATE $x_{i2} \gets equalSample\{aug_2(x_i)+t_i, aug_2(x_i)\}$ 
%     %\STATE // {Note: $x_{i2}$ selection is equal chance for each method.}
%     \STATE $z, z^{\prime}=f(x_{i1}), f^{\prime}(x_{i2})$
%     \STATE $loss \gets -similarity(z, z^{\prime})$
%     \STATE $f^{\prime} \gets update(f, loss)$
%     % \STATE $f^{\prime}.params=0.9 \times f.params + 0.1 \times f^{\prime}.params$
%     \ENDFOR

%     \STATE return $f^{\prime}$
    
% \end{algorithmic}
% \end{algorithm}




\begin{figure}[ht!]
  \centering
   \includegraphics[width= \linewidth]{figures/mitigation.pdf}
   \caption{Illustration of Self-supervised Clustering Unlearning (SCU). The image \( x \) is sampled from a cluster distinct from the cluster producing trigger \( t \).}
%Image $x_i$ is sampled from the cluster different from generated trigger $t_i$. }%The workflow of our mitigator method. Assuming a clean input image $x_i$, we perform data augmentation to generate two images, denoted as $x_{i1}$ and $x_{i2}$. With a probability of $50\%$, a Trojan trigger $t_i$ is appended to $x_{i2}$. The image $x_{i1}$ is then fed to a fixed encoder $f$ to obtain a representation that remains unaltered. On the other hand, the second encoder $f^\prime$ is updated using the augmented data to mitigate the effect of the Trojan trigger. The encoder is considered clean once it achieves accurate classification performance, even in the presence of the Trojan trigger.}
   \label{fig:mitigation}
\end{figure}



\section{SSL-Cleanse Mitigator} \label{sec:mitigation} 
Once we have the triggers $t_s$ generated by our detector, their corresponding target-class clusters become identifiable. Subsequently, we introduce a Self-supervised Clustering Unlearning (SCU) strategy to mitigate SSL encoder backdoor threats, leading to a purified encoder. The mitigation approach is detailed in Algorithm~\ref{alg:mitigation}. This method accepts cluster samples \( D_{i \in [1, k]} \), a Trojaned encoder \( f \), and the trigger list \( t_s \) as inputs, producing a purified encoder \( f' \) in return. Since $K$ clusters are generated, we need to clean them one by one. For each cluster $i$, we iteratively select clean image $x$ from each cluster samples $D_i$, and augment the image to create a new training sample consisting of the augmented images $x_{1}$ and $x_{2}$. Subsequently, we randomly select a trigger from $t_s$ excluding $t_s[i]$ to make sure the image $x$ is sampled from a cluster distinct from the cluster producing trigger $t$. Then we attach trigger $t$ to $x_2$ or directly use $x_{i2}$ without adding a trigger. The probability is $50\%$ which is the meaning of $equalSample$ in Algorithm~\ref{alg:mitigation}. The 50\% means that we set an equal weight for attack removal and clean accuracy. Notice that here we pass these new training samples through the Trojaned encoder $f$ to obtain their respective representations. We then optimize the similarity between the representations using a loss function by fixing the model $f$ and updating the encoder $f'$ to eliminate the Trojan trigger effects, resulting in a clean encoder. Figure~\ref{fig:mitigation} illustrates the mitigation process, where a clean image of a cat $x$ is augmented to generate two cat images $x_{1}$ and $x_{2}$, one that is the same and another that has a 50\% chance of being attached with a Trojan trigger. The clean image is sent to a reference model $f$, which maintains its parameters and representations unchanged. In contrast, the other model $f'$ is updated with the new training sample to remove the Trojan trigger effect. The updated encoder is deemed to be clean after it is able to accurately classify data, even in the presence of the Trojan trigger.



% \begin{algorithm}[hb]
%    \caption{Self-supervised Clustering Unlearning}
%    \label{alg:mitigation}
% \begin{algorithmic}
%     \STATE {\bfseries Input:} $D_{i \in [1, k]}$, Trojaned encoder $f$, triggers list $t_s$
%     \STATE {\bfseries Output:} a clean encoder $f^{\prime}$
%     \STATE Initialize $f^{\prime} \gets f$
%     \FOR{$x_i$ in $D_{i\in [1:K]}$}
%     \STATE $x_{i1} \gets aug_1(x_i)$
%     \STATE $t_i \gets$ randomly selected from $t_s$ excluding $t_s[i]$
%     \STATE $x_{i2} \gets equalSample\{aug_2(x_i)+t_i, aug_2(x_i)\}$
%     \STATE $z, z^{\prime}=f(x_{i1}), f^{\prime}(x_{i2})$
%     \STATE $loss \gets -similarity(z, z^{\prime})$
%     \STATE $f^{\prime} \gets update(f, loss)$
%     \ENDFOR
%     \STATE return $f^{\prime}$
% \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}[t!]
%    \caption{Self-supervised Clustering Unlearning (SCU)}
%    \label{alg:mitigation}
% \begin{algorithmic}
%     \STATE {\bfseries Input:} $D_{i \in [1, k]}$, Trojaned encoder $f$, trigger list $t_s$
%     \STATE {\bfseries Output:} a clean encoder $f^{\prime}$
%     \STATE Initialize $f^{\prime} \gets f$
%     % \FOR{$i=1$ {\bfseries to} $K$}
%     \FOR{$x$ in $D_{i \in k}$} 
%     %\STATE \textit{\textcolor{gray}{\# load a minibatch $x$ from $D_i$}}
%     \STATE $x_1 \gets aug_1(x)$ 
%     \STATE $t \gets$ randomly selected from $t_s$ excluding $t_s[i]$
%     \STATE $x_2 \gets equalSample\{aug_2(x+t), aug_2(x)\}$
%     \STATE $z, z^{\prime}=f(x_1), f^{\prime}(x_2)$
%     \STATE $loss \gets -similarity(z, z^{\prime})$
%     \STATE $f^{\prime} \gets update(f, loss)$
%     % \ENDFOR
%     \ENDFOR
%   %  \STATE return $f^{\prime}$
% \end{algorithmic}
% \end{algorithm}

\begin{algorithm}[t!]
   \caption{Self-supervised Clustering Unlearning}
   \label{alg:mitigation}
\begin{algorithmic}
    \STATE {\bfseries Input:} $D_{i \in [1, k]}$, Trojaned encoder $f$, trigger list $t_s$
    \STATE {\bfseries Output:} a clean encoder $f^{\prime}$
    \STATE Initialize $f^{\prime} \gets f$
    \FOR{$i=1$ {\bfseries to} $K$}
    \FOR{$x$ in $D_i$} 
    %\STATE \textit{\textcolor{gray}{\# load a minibatch $x$ from $D_i$}}
    \STATE $x_1 \gets aug_1(x)$; $x_2 \gets aug_2(x)$ 
    \STATE $t \gets$ randomly selected from $t_s$ excluding $t_s[i]$
    \STATE $\hat{x_2} \gets equalSample\{aug_2(x\oplus t), aug_2(x)\}$
    \STATE $z, z^{\prime}=f(x_1), f^{\prime}(\hat{x_2})$
    \STATE $loss \gets -similarity(z, z^{\prime})$
    \STATE $f^{\prime} \gets update(f, loss)$
    \ENDFOR
    \ENDFOR
  %  \STATE return $f^{\prime}$
\end{algorithmic}
\end{algorithm}

% \begin{algorithm}[hb]
%    \caption{Self-supervised Clustering Unlearning (SCU)}
%    \label{alg:mitigation}
% \begin{algorithmic}
%     \STATE {\bfseries Input:} cluster samples $D_{i \in [1, k]}$, Trojaned encoder $f$, triggers list $t_s$
%     \STATE {\bfseries Output:} a clean encoder $f^{\prime}$
%     \STATE Initialize $f^{\prime} \gets f$
%     \FOR{$i=1$ {\bfseries to} $K$}
%     \FOR{$x_j$ in $D_i$}
%     \STATE $x_{j1} \gets aug_1(x_j)$
%     \STATE $t \gets$ randomly selected from $t_s$ excluding $t_s[i]$
%     \STATE $x_{j2} \gets equalSample\{aug_2(x_j+t), aug_2(x_j)\}$
%     \STATE $z, z^{\prime}=f(x_{j1}), f^{\prime}(x_{j2})$
%     \STATE $loss \gets -similarity(z, z^{\prime})$
%     \STATE $f^{\prime} \gets update(f, loss)$
%     \ENDFOR
%     \ENDFOR
%     \STATE return $f^{\prime}$
% \end{algorithmic}
% \end{algorithm}


% \begin{table*}[h!]
% \centering
% \footnotesize
% \setlength{\tabcolsep}{3pt}
% \caption{The performance of our detector on three different attack methods with different unlabeled training dataset ratio $\epsilon \%$.}
% \begin{tabular}{cccccccccccccc}\toprule
% \multirow{2}{*}{Dataset} & \multirow{2}{*}{\makecell[c]{Dataset\\ratio $\epsilon $(\%)}} & \multicolumn{4}{c}{SSLBackdoor} & \multicolumn{4}{c}{ESTAS} & \multicolumn{4}{c}{CTRL} \\
% \cmidrule(lr){3-6} \cmidrule(lr){7-10} \cmidrule(lr){11-14}
% & & Cluster & FPR(\%) & FNR(\%)  & ACC(\%) & Cluster & FPR(\%) & FNR(\%) & ACC(\%) & Cluster & FPR(\%) & FNR(\%) & ACC(\%) \\\hline
% \multirow{4}{*}{\makecell[c]{CIFAR-10}} %& 1 & $3$ & $5$ & $1$ & $54$ & $4$ & $15$ & $2$ & $63$ & $4$ & $7$ & $1$ & $56$ \\
% & $5$ & $9$ & $6$ & $44$ & $75$ & $8$ & $10$ & $6$ & $46$ & $9$ & $6$ & $46$ & $74$ \\
% & $8$ & $\textcolor{green}{10}$ & $10 $ & $36 $ & $77$ & $\textcolor{green}{10}$  & $10$ & $16 $ & $\textbf{87}$ & $\textcolor{green}{10}$  & $10$ & $28$ & $81$ \\
% & $10$ & $11$ & $10$ & $30$ & $\textbf{80}$ & $11$ & $14$ & $14$ & $86$ & \textcolor{green}{$10$} & $8$ & $28$ & $\textbf{82}$ \\\hline
% \multirow{4}{*}{\makecell[c]{CIFAR-100}} %& $1$ & $45$ & $6$ & $0$ & $56$ & $45$ & $11$ & $1$ & $60$ & $40$ & $7$ & $0$ & $57$ \\
% & $5$ & $65$ & $6$ & $44$ & $56$ & $70$ & $6$ & $46$ & $67$ & $70$ & $6$ & $46$ & $60$ \\
% & $8$ & $75$ & $10$ & $36$ & $61$ & $75$ & $10$ & $16$ & $71$ & $75$ & $10$ & $28$ & $62$ \\
% & $10$ & $90$ & $10$ & $30$ & $\textbf{64}$ & $90$ & $14$ & $14$ & $\textbf{78}$ & $95$ & $8$ & $28$ & $\textbf{65}$ \\\hline
% \multirow{4}{*}{\makecell[c]{ImageNet-100}} %& $1$ & $80$ & $23$ & $6$ & $67$ & $85$ & $35$ & $9$ & $76$ & $80$ & $33$ & $4$ & $79$ \\
% & $5$ & $90$ & $6$ & $44$ & $71$ & $90$ & $6$ & $46$ & $79$ & $90$ & $6$ & $46$ & $82$ \\
% & $8$ & \textcolor{green}{$100$} & $10$ & $36$ & $79$ & \textcolor{green}{$100$} & $10$ & $16$ & $85$ & \textcolor{green}{$100$} & $10$ & $28$ & $83$ \\
% & $10$ & \textcolor{green}{$100$} & $10$ & $30$ & $\textbf{80}$ & $105$ & $14$ & $14$ & $\textbf{86}$ & \textcolor{green}{$100$} & $8$ & $28$ & $\textbf{85}$ \\
% \bottomrule
% \end{tabular}
% \label{t:detector}
% \end{table*}




% \begin{table*}[h!]
% \centering
% \footnotesize
% \setlength{\tabcolsep}{5pt}
% \caption{The performance of our detector on two different attack methods with different models.}
% \begin{tabular}{cccccccc}\toprule
% \multirow{2}{*}{Dataset} & \multirow{2}{*}{Models} & \multicolumn{3}{c}{SSL-Backdoor} & \multicolumn{3}{c}{CTRL} \\\cmidrule(lr){3-5} \cmidrule(lr){6-8}
% & & FPR(\%) & FNR(\%) & DACC(\%) & FPR(\%) & FNR(\%) & DACC(\%) \\\midrule
% \multirow{3}{*}{CIFAR-10} & BYOL & $10$ & $30$ & $80$ & $8$ & $28$ & $82$\\
% & SimCLR & $8$ & $44$ & $74$ & $10$ & $22$ & $84$\\
% & MoCo V2 & $10$ & $46$ & $72$ & $10$ & $26$ & $82$\\\midrule
% \multirow{3}{*}{ImageNet-100} & BYOL & $16$ & $24$ & $80$ & $16$ & $14$ & $85$ \\
% & SimCLR & $14$ & $40$ & $73$ & $16$ & $8$ & $88$ \\
% & MoCo V2 & $14$ & $28$ & $79$ & $20$ & $24$ & $78$ \\
% \bottomrule
% \end{tabular}
% \label{t:detector}
% \end{table*}


% \begin{table*}[h!]
% \centering
% \footnotesize
% \setlength{\tabcolsep}{3pt}
% \caption{The performance of our detector on three different attack methods with different unlabeled training dataset ratio $\epsilon \%$.}
% \begin{tabular}{cccccccccccccc}\toprule
% \multirow{2}{*}{Dataset} & \multirow{2}{*}{\makecell[c]{Dataset\\ratio $\epsilon $(\%)}} & \multicolumn{4}{c}{SSLBackdoor} & \multicolumn{4}{c}{ESTAS} & \multicolumn{4}{c}{CTRL} \\
% \cmidrule(lr){3-6} \cmidrule(lr){7-10} \cmidrule(lr){11-14}
% & & Cluster & FPR(\%) & FNR(\%)  & ACC(\%) & Cluster & FPR(\%) & FNR(\%) & ACC(\%) & Cluster & FPR(\%) & FNR(\%) & ACC(\%) \\\hline
% \multirow{4}{*}{\makecell[c]{CIFAR-10}} %& 1 & $3$ & $5$ & $1$ & $54$ & $4$ & $15$ & $2$ & $63$ & $4$ & $7$ & $1$ & $56$ \\
% & $5$ & $9$ & $6$ & $44$ & $75$ & $8$ & $6$ & $46$ & $74$ & $9$ & $6$ & $46$ & $74$ \\
% & $8$ & \textcolor{green}{$10$} & $10$ & $36$ & $77$ & $\textcolor{green}{10}$  & $10$ & $16 $ & $\textbf{87}$ & $\textcolor{green}{10}$  & $10$ & $28$ & $81$ \\
% & $10$ & $11$ & $10$ & $30$ & $\textbf{80}$ & $11$ & $14$ & $14$ & $86$ & \textcolor{green}{$10$} & $8$ & $28$ & $\textbf{82}$ \\\hline
% \multirow{4}{*}{\makecell[c]{CIFAR-100}} %& $1$ & $45$ & $6$ & $0$ & $56$ & $45$ & $11$ & $1$ & $60$ & $40$ & $7$ & $0$ & $57$ \\
% & $5$ & $65$ & $14$ & $74$ & $56$ & $70$ & $14$ & $52$ & $67$ & $70$ & $12$ & $68$ & $60$ \\
% & $8$ & $75$ & $16$ & $62$ & $61$ & $75$ & $18$ & $40$ & $71$ & $75$ & $12$ & $64$ & $62$ \\
% & $10$ & $90$ & $22$ & $50$ & $\textbf{64}$ & $90$ & $26$ & $18$ & $\textbf{78}$ & $95$ & $12$ & $58$ & $\textbf{65}$ \\\hline
% \multirow{4}{*}{\makecell[c]{ImageNet-100}} %& $1$ & $80$ & $23$ & $6$ & $67$ & $85$ & $35$ & $9$ & $76$ & $80$ & $33$ & $4$ & $79$ \\
% & $5$ & $90$ & $14$ & $44$ & $71$ & $90$ & $18$ & $24$ & $79$ & $90$ & $12$ & $24$ & $82$ \\
% & $8$ & \textcolor{green}{$100$} & $16$ & $26$ & $79$ & \textcolor{green}{$100$} & $20$ & $10$ & $85$ & \textcolor{green}{$100$} & $14$ & $20$ & $83$ \\
% & $10$ & \textcolor{green}{$100$} & $16$ & $24$ & $\textbf{80}$ & $105$ & $18$ & $10$ & $\textbf{86}$ & \textcolor{green}{$100$} & $16$ & $14$ & $\textbf{85}$ \\
% \bottomrule
% \end{tabular}
% \label{t:detector}
% \end{table*}


% \begin{table*}[h!]
% \centering
% \footnotesize
% \setlength{\tabcolsep}{5pt}
% \caption{Datasize study of detector.}
% \begin{tabular}{cccccccccccccc}\toprule
% \multirow{2}{*}{Dataset} & \multirow{2}{*}{\makecell[c]{Data size\\ratio(\%)}} & \multicolumn{4}{c}{SSLBackdoor} & \multicolumn{4}{c}{ESTAS} & \multicolumn{4}{c}{CTRL} \\
% \cmidrule(lr){3-6} \cmidrule(lr){7-10} \cmidrule(lr){11-14}
% & & Cluster & TP & FP & Accuracy(\%) & Cluster & TP & FP & Accuracy(\%) & Cluster & TP & FP & Accuracy(\%) \\\hline
% \multirow{3}{*}{\makecell[c]{CIFAR-10}} & $5$ & $6$ & $28$ & $3$ & $75$ & $8$ & $27$ & $3$ & $74$ & $9$ & $27$ & $3$ & $74$ \\
% & $8$ & $10$ & $32$ & $5$ & $77$ & $10$ & $42$ & $5$ & $\textbf{87}$ & $10$ & $36$ & $5$ & $81$ \\
% & $10$ & $11$ & $35$ & $5$ & $\textbf{80}$ & $11$ & $43$ & $7$ & $86$ & $10$ & $36$ & $4$ & $\textbf{82}$ \\\hline
% \multirow{4}{*}{\makecell[c]{CIFAR-100}} %& $1$ & $45$ & $6$ & $0$ & $56$ & $45$ & $11$ & $1$ & $60$ & $40$ & $7$ & $0$ & $57$ \\
% & $5$ & $65$ & $13$ & $7$ & $56$ & $70$ & $24$ & $7$ & $67$ & $70$ & $16$ & $6$ & $60$ \\
% & $8$ & $75$ & $19$ & $8$ & $61$ & $75$ & $30$ & $9$ & $71$ & $75$ & $18$ & $6$ & $62$ \\
% & $10$ & $90$ & $25$ & $11$ & $\textbf{64}$ & $90$ & $41$ & $13$ & $\textbf{78}$ & $95$ & $21$ & $6$ & $\textbf{65}$ \\\hline
% \multirow{4}{*}{\makecell[c]{ImageNet-100}} %& $1$ & $80$ & $23$ & $6$ & $67$ & $85$ & $35$ & $9$ & $76$ & $80$ & $33$ & $4$ & $79$ \\
% & $5$ & $90$ & $28$ & $7$ & $71$ & $90$ & $38$ & $9$ & $79$ & $90$ & $38$ & $6$ & $82$ \\
% & $8$ & $100$ & $37$ & $8$ & $79$ & $100$ & $45$ & $10$ & $85$ & $100$ & $40$ & $7$ & $83$ \\
% & $10$ & $100$ & $38$ & $8$ & $\textbf{80}$ & $105$ & $45$ & $9$ & $\textbf{86}$ & $100$ & $43$ & $8$ & $\textbf{85}$ \\
% \bottomrule
% \end{tabular}
% \label{t:Datasize study of detector}
% \end{table*}



% \begin{table*}[h!]
% \centering
% \footnotesize
% \setlength{\tabcolsep}{4pt}
% \caption{The performance of our mitigator on three different attack methods with different unlabeled training dataset ratio $\epsilon \%$.}
% \begin{tabular}{ccccccccccccc}\toprule
% \multirow{3}{*}{\makecell[c]{Dataset\\ratio $\epsilon$ (\%)}} & \multicolumn{4}{c}{SSLBackdoor} & \multicolumn{4}{c}{ESTAS} & \multicolumn{4}{c}{CTRL} \\
% \cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13}
% & \multicolumn{2}{c}{Before mitigator}  & \multicolumn{2}{c}{With mitigator} & \multicolumn{2}{c}{Before mitigator}  & \multicolumn{2}{c}{With mitigator} & \multicolumn{2}{c}{Before mitigator}  & \multicolumn{2}{c}{With mitigator}  \\
% \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
% & ACC(\%) & ASR(\%) & ACC(\%) & ASR(\%) & ACC(\%) & ASR(\%) & ACC(\%) & ASR(\%) & ACC(\%) & ASR(\%) & ACC(\%) & ASR(\%) \\\hline
% %$1$ & $60.8$ & $34.0$ & $54.1$ & $6.37$ & $62.5$ & $97.4$ & $54.7$ & $3.1$ & $52.5$ & $42.9$ & $41.8$ & $7.14$ \\
% $5$ & $60.8$ & $33.2$ & $57.9$ & $\textbf{1.07}$ & $61.3$ & $98.5$ & $59.0$ & $\textbf{1.8}$ & $53.2$ & $43.1$ & $47.7$ & $\textbf{2.19}$ \\
% $8$ & $60.2$ & $33.1$ & $59.4$ & $\textbf{0.15}$ & $61.5$ & $99.2$ & $61.3$ & $\textbf{0.22}$ & $53.6$ & $41.7$ & $51.4$ & $\textbf{0.71}$ \\
% $10$ & $60.6$ & $33.2$ & $60.2$ & $\textbf{0.14}$ & $62.7$ & $98.5$ & $62.0$ & $\textbf{0.22}$ & $53.3$ & $42.1$ & $52.7$ & $\textbf{0.35}$ \\
% \bottomrule
% \end{tabular}
% \label{t:mitigator}
% \end{table*}



\section{Experimental Methodology} \label{method}

\textbf{Dataset}. Our experiments were conducted on benchmark datasets: CIFAR-10~\cite{cifar10} and ImageNet-100~\cite{imagenet}. %CIFAR-10 comprises $50,000$ $32\times32$ training images divided into 10 classes. 
ImageNet-100 is a random subset of 100 classes from the larger ImageNet dataset and contains around 127,000 training images, which is widely used in prior SSL attacks~\cite{sslbackdoor,CTRL}.

\noindent\textbf{SSL Attacks and Encoders}. To assess the effectiveness of our detector against various attack methods, we evaluated it against two backdoor attacks, namely SSL-Backdoor~\cite{sslbackdoor}and CTRL~\cite{CTRL} over BYOL~\cite{grill2020byol}, SimCLR~\cite{chen2020simple}, and MoCo V2~\cite{chen2020mocov2}, respectively. We created 50 benign encoders and 50 Trojaned encoders for each backdoor attack setting, resulting in 1200 encoders. We follow the above attack's setting to use ResNet-18~\cite{resnet} as encoder architecture.  %combined with a two-layer MLP is chosen as the encoder architecture. %The MLP is used to project outputs into a low-dimensional latent space, i.e., 64-dimensional for CIFAR-10, and 128-dimensional for ImageNet-100.

\noindent\textbf{Experimental Settings}. Our experiments are performed on two Nvidia GeForce RTX-3090 GPUs, each with a memory capacity of 24 GB. For the detector, the initial value of $\lambda$ is set up as $0.01$. %We adopted the triggers from the Hidden Trigger Backdoor Attacks~\cite{Saha_Subramanya_Pirsiavash_2020}. The trigger size was $50\times50$ for ImageNet-100 and $6\times6$ for CIFAR-10. 
For detection and mitigation running overhead, the detection method with $10\%$ of the ImageNet-100 training data consumes roughly 20 minutes, and the mitigation process requires approximately 7 minutes. 


\noindent\textbf{Evaluation Metrics}. We define the following evaluation metrics to study the efficiency and effectiveness of our SSL-Cleanse. Detection Accuracy (\textbf{DACC}) is detection accuracy which is the ratio of correctly identified encoder types (either Benign or Trojan) relative to the total count of encoders. %Mathematically, it is formulated as $\frac{TP+TN}{TP+TN+FP+FN}$}. 
Attack Success Rate (\textbf{ASR}) is defined as the ratio of images that contain the trigger and are misclassified as the target class, to the total number of evaluated images. Accuracy (\textbf{ACC}) is the percentage of input images without triggers classified into their corresponding correct classes. \textbf{TP} indicates the true positive count, referring to Trojaned encoder numbers detected by our detector. \textbf{FP} represents false positives, indicating clean encoders misclassified as Trojaned encoders by our detector. 



\begin{table}[ht!]
\centering
\footnotesize
\setlength{\tabcolsep}{2pt}
\caption{The detection performance of our SSL-Cleanse.}
\begin{tabular}{cccccccc}\toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Method} & \multicolumn{3}{c}{SSL-Backdoor} & \multicolumn{3}{c}{CTRL} \\\cmidrule(lr){3-5} \cmidrule(lr){6-8}
& & TP & FP & DACC(\%) & TP & FP & DACC(\%) \\\midrule
\multirow{3}{*}{CIFAR-10} & BYOL & $35$ & $5$ & $80$ & $36$ & $4$ & $82$\\
& SimCLR & $33$ & $4$ & $79$ & $39$ & $5$ & $84$\\
& MoCo V2 & $31$ & $5$ & $76$ & $37$ & $5$ & $82$\\\midrule
\multirow{3}{*}{ImageNet-100} & BYOL & $38$ & $8$ & $80$ & $43$ & $8$ & $85$ \\
& SimCLR & $34$ & $7$ & $77$ & $46$ & $8$ & $88$ \\
& MoCo V2 & $36$ & $7$ & $79$ & $42$ & $8$ & $84$ \\
\bottomrule
\end{tabular}
\label{t:detector}
\end{table}


\begin{table*}[ht!]
\centering
\footnotesize
\setlength{\tabcolsep}{8pt}
\caption{The mitigation performance of our SSL-Cleanse.}
\begin{tabular}{cccccccccc}\toprule
\multirow{3}{*}{Dataset} & \multirow{3}{*}{Method} & \multicolumn{4}{c}{SSL-Backdoor} & \multicolumn{4}{c}{CTRL} \\\cmidrule(lr){3-6}\cmidrule(lr){7-10}
& & \multicolumn{2}{c}{Before mitigation} & \multicolumn{2}{c}{After mitigation} & \multicolumn{2}{c}{Before mitigation} & \multicolumn{2}{c}{After mitigation} \\\cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}\cmidrule(lr){9-10}
& & ACC(\%) & ASR(\%) & ACC(\%) & ASR(\%) & ACC(\%) & ASR(\%) & ACC(\%) & ASR(\%) \\\midrule
\multirow{3}{*}{CIFAR-10} & BYOL & $83.42$ & $48.32$ & $82.14$ & $1.14$ & $83.19$ & $60.47$ & $82.59$ & $1.96$\\
& SimCLR & $84.88$ & $42.19$ & $83.53$ & $0.58$ & $80.74$ & $81.84$ & $79.60$ & $1.15$ \\
& MoCo V2 & $81.02$ & $37.95$ & $80.16$ & $0.92$ & $81.42$ & $77.51$ & $80.03$ & $1.62$ \\\midrule
\multirow{3}{*}{ImageNet-100} & BYOL & $60.57$ & $33.21$ & $60.24$ & $0.14$ & $53.33$ & $45.10$ & $52.65$ & $0.35$ \\
& SimCLR & $60.18$ & $31.85$ & $58.58$ & $0.62$ & $52.90$ & $44.98$ & $51.04$ & $0.33$ \\
& MoCo V2 & $61.57$ & $35.06$ & $60.10$ & $0.17$ & $50.62$ & $35.72$ & $48.88$ & $0.17$ \\
\bottomrule
\end{tabular}
\label{t:mitigator}
\end{table*}






\section{SSL-Cleanse Results}
%In this section, we first present important results on the SSL-Cleanse detector, and mitigator, and then perform extensive ablation studies on trigger size, perturbation pattern, and dataset size. 


\textbf{Detection.} In Table~\ref{t:detector}, we present the performance of our detector on two training-agnostic SSL attacks including SSL-Backdoor and CTRL on three SSL methods and two datasets.   
In total, our detection accuracy (DACC) across 1200 (600 Trojaned and 600 benign) encoders stands at \(81.33\%\), illustrating the efficacy of our backdoor detection capabilities. In particular, for the ImageNet-100 dataset, the DACC is $>77\%$ and the average DACC is $82.17\%$. For the CIFAR-10 dataset, the DACC is $>76\%$ and the average DACC is $80.5\%$. 

Table~\ref{t:detector} further indicates that our SSL-Cleanse can detect not only the patch-based trigger SSL attack SSL-Backdoor but also the frequency-based SSL attack CTRL. This capability stems from our method's utilization of trigger size and magnitude for patch-based trigger detection in tandem with frequency-domain detection. In particular, For SSL-Backdoor detection, SSL-Clease achieves $78.5\%$ DACC on average for both CIFAR-10 and ImageNet-100 datasets. For CTRL, it obtains $84.17\%$ average DACC. Against the CTRL, our detector identifies 46 TP from 50 Trojaned SimCLR encoders on ImageNet and registers 8 FP among 50 clean SimCLR encoders. 
%In particular, For CIFAR-10 dataset, the average detection accuracy (DACC) is $>75\%$ for different training methods. Specifically, we obtain $80\%$ DACC on BYOL. For ImageNet-100 dataset, the average DACC is $>77\%$ for different training methods. In addition, the detector achieves better DACC on ImageNet-100 dataset, because it has a larger probability to classify unlabeled images into the right cluster. 

Our SSL-Cleanse consistently exhibits reliable detection performance across prevalent SSL training techniques such as BYOL, SimCLR, and MoCo V2. In particular, our detector registers an average DACC of \(81.75\%\), \(82\%\), and \(80.25\%\) for BYOL, SimCLR, and MoCo V2, respectively.

%CTRL uses an invisible global trigger~\cite{wang2022invisible} in the spectral space of inputs that is preserved after data augmentation and is consistent with the inference phase. We incorporate the stealthy global trigger to verify the performance of our detector. Table \ref{t:detector} displays the performance outcomes of our detection method using 150 benign encoders and 150 Trojaned encoders on various datasets and training methods. For CIFAR-10 dataset, the average detection accuracy (DACC) is $>82\%$ for different training methods. Specifically, we obtain $84\%$ DACC on SimCLR. For ImageNet-100 dataset, the average DACC is $>83\%$ for different training methods. Specifically, the DACC on SimCLR is $88\%$, and the Trojaned encoder DACC is $92\%$.




\noindent\textbf{Mitigation.} Table~\ref{t:mitigator} compares the attack success rate (ASR) and clean accuracy (ACC) before and after applying the mitigator against SSL-Backdoor and CTRL on the CIFAR-10 and ImageNet-100 dataset. On average, the ASR experiences a marked reduction to below $2\%$, while the ACC declines to approximately $1\%$ before and after implementing mitigation. 

In the case of patch-based SSL-Backdoor, our mitigation approach significantly reduces the ASR to below $1.2\%$, demonstrating its successful eradication of backdoor effects.
Additionally, the ACC experiences an average decrease of $1.15\%$ in backdoored models after applying our mitigator. Notably, when utilizing the BYOL training method on the ImageNet-100 dataset, the ACC remains relatively stable. 

Regarding global frequency-based SSL attack CTRL, our mitigation strategy substantially decreases the ASR to less than $2\%$, underscoring its effective elimination of backdoor attacks. Furthermore, the ACC has an average decline of $1.06\%$ in backdoored models after mitigation. Importantly, when employing the BYOL training technique on both CIFAR-10 and ImageNet-100 datasets, the ACC remains fairly consistent. 










%\subsection{SSL-Cleanse Detector Results} \label{result-D}
%We present the performance of our detector on two different attack methods with different models in Table~\ref{t:detector}. 
\noindent\textbf{The SWK Effects.} Figure~\ref{fig:DetectionACC} presents an ablation study on our SWK approach. Employing SWK on ImageNet-100 yields a predicted number of \(100\), in contrast to \(20\) as obtained using the direct search (Kneedle) method, as depicted in Figure~\ref{fig:SWK}. Consequently, SWK delivers a detection accuracy of \(80\%\), marking a \(30\%\) DACC improvement.
 %We also can see that it is challenging to se

%We show the SWK effect in Figure~\ref{fig:DetectionACC} using SSL-Backdoor attack on ImageNet-100 dataset via BYOL~\cite{grill2020byol} as an example. With our SWK method, we obtain the highest DACC $80\%$. However, with direct search, as shown in Figure~\ref{fig:SWK}, the local optimal $K$ is 20, and the DACC is only $50\%$. Although the detector can randomly set $K$ for testing, it is time-consuming and has a low chance to get the best DACC. 

\noindent\textbf{Outlier Detection Effects.} Figure~\ref{fig:box2} shows our Size-Norm Trigger Outlier Detector can successfully distinguish the outlier triggers from the $K$-cluster trigger list for the trojaned encoders based on SSL-Backdoor and CTRL. In particular, each box bar plots the trigger size $|m_i^1|$ and trigger norm $|m_i^2\cdot \Delta_i^2|$ for SSL-Backdoor and CTRL, respectively. Our Size-Norm trigger outlier detection can scan collectively, without the presumption that the defender has prior knowledge of the attack type.


%the outliers for trojaned .  For uninfected index distribution, we plot min/max, 25/75 quartile, and median values. In our task, outlier detection involves identifying unusually small patterns amidst the patterns generated by our methods. As shown in Figure~\ref{fig:box2}, by setting a threshold, i.e., 2, we can identify anomalies in the data, i.e., the outlier of the infected trigger is determined (a small trigger corresponds to a large anomaly index).


 





%\subsection{Different Training Dataset Sizes} 

%We investigate the impact of a varying training dataset size on the detection performance. Specifically, we use different proportions of images in the training set (dataset ratio), i.e., $5\%$, $8\%$, and $10\%$. It is estimated that it will take 20 minutes to generate the cluster numbers using the SWK method for $10\%$ of the ImageNet-100 training data. Table \ref{t:detector} demonstrates that the SWK method provides more accurate clustering results for larger datasets. Notably, both CIFAR-10 and ImageNet-100 achieve accurate clustering results with only $8\%$ of the training dataset (highlighted in green). Furthermore, the detection performance increases with an enlarging proportion of the training dataset. Even when the number of clusters exceeds the actual cluster numbers, more training data still result in a better detection rate.


%Specifically, in the case of the CIFAR-10 task, the predicted cluster number was 8 or 9 with 5\% of the training set, and 10 with 8\% of the training set. Similarly, for CIFAR-100, the predicted cluster number was 65 with 5\% of the training set, and 90 or 95 with 10\% of the training set. Finally, in the case of ImageNet-100, the predicted cluster number was 90 with 5\% of the training set, and 100 or 105 with 10\% of the training set.









%\subsection{Tradeoff between FPR and FNR.}
%We calculate the false positive rate (FPR) and false negative rate (FNR) when setting different \textit{anomaly index} for the outlier to control its sensitivity to abnormal data points. Then, we get multiple FPR-FNR pairs, which can help users balance the numbers of false positives and false negatives during real-world deployment. Results in Figure \ref{fig:FNR_FPR} show that we achieve powerful filtering performance for three types of attacks on different datasets, i.e., obtaining $6.33\%$ FNR at an FPR of $5\%$. %Not surprisingly, SSL-Backdoor encoders are more difficult to filter out, because its attack success rate is the lowest.

%\begin{figure}[tb]
 % \centering
   %\includegraphics[width=\linewidth]{figures/FNR.pdf}
   %\caption{False negative rate of Trojan encoder detection when achieving different false positive rate.}
  % \label{fig:FNR_FPR}
%\end{figure} 


\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\linewidth]{figures/study_cluster.pdf}
\caption{A comparison of detection accuracy between SSL-Cleanse using the SWK method and the direct search (Kneedle)on ImageNet-100.}
\label{fig:DetectionACC}
\end{figure}



\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\linewidth]{figures/box.pdf}
\caption{The Size-Norm trigger outlier detection criteria is able to identify both patch-based SSL-Backdoor and frequency-domain global trigger in CTRL. The box plot shows min/max and quartiles.}
\label{fig:box2}
\end{figure}






\begin{table}[ht!]
\caption{Influence of the ratio of the unlabeled dataset to the entire training dataset on the detection efficacy. A larger ratio usually introduces a higher DACC. }
\footnotesize
\setlength{\tabcolsep}{4pt}
\begin{center}
\begin{tabular}{ccccccc}\toprule
\multirow{2}{*}{\makecell[c]{Data ratio\\(\%)}} & \multicolumn{3}{c}{CIFAR-10} & \multicolumn{3}{c}{ImageNet-100} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-7}
& TP & FP & DACC(\%) & TP & FP & DACC(\%) \\\midrule
$5$ & $28$ & $7$ & $71$ & $38$ & $6$ & $82$ \\
$8$ & $37$ & $8$ & $79$ & $40$ & $7$ & $83$ \\
$10$ & $38$ & $8$ & $80$ & $43$ & $8$ & $85$ \\
\bottomrule
\end{tabular}
\label{tab:data_ratio}
\end{center}
\end{table}


\noindent\textbf{Other Results.} We analyze the influence of different  dataset ratios on detection performance using CTRL attack and BYOL training methods. We  employ varied image proportions, namely, $5\%$, $8\%$, and $10\%$. As illustrated in Table \ref{tab:data_ratio}, the results showcase an improvement in detection performance as the proportion of the training dataset increases. The DACC for the CIFAR-10 dataset shows a $1\%$ increase between the $8\%$ and $10\%$ training datasets. Moreover, for the ImageNet-100 dataset, there is an enhancement from $83\%$ to $85\%$ in the DACC. Further details regarding the hyperparameters \(\lambda\), along with nuances about the influence of attacking trigger size and perturbation norm, can be found in our supplementary material.




\section{Conclusion} \label{conclusion}
This paper introduces \textit{SSL-Cleanse}, a novel work to detect and mitigate Trojan attacks in SSL encoders without accessing any downstream labels. %\textit{SSL-Cleanse} is the first work to enable the detection of SSL-ba %We reveal it is possible to prevent the dissemination of Trojaned SSL encoders by using our SSL-Cleanse. 
%In the detector, we propose a SWK to produce $K$ clusters based on the representations of the SSL encoder. We then introduce the ROTR method to generate a reversed pattern for each cluster. By utilizing these $K$ patterns, we can perform outlier detection to determine if the SSL encoder is Trojaned. In the mitigator, we introduce a method called SCU, which is designed to efficiently eliminate the Trojans present in the backdoored SSL encoders.  
We evaluated SSL-Cleanse on various datasets using 1200 models, achieving an average detection success rate of $82.2\%$ on ImageNet-100. After mitigating backdoors, on average, backdoored encoders achieve $0.3\%$ attack success rate without great accuracy loss.



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
