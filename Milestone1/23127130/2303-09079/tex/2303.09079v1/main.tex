\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{makecell}
\usepackage{array}
\usepackage{bm}


% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{SSL-Cleanse: Trojan Detection and Mitigation in Self-Supervised Learning}

\author{Mengxin Zheng\thanks{These authors contributed equally to this work.}\\
Indiana University Bloomington\\
Bloomington, IN\\
{\tt\small zhengme@iu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Jiaqi Xue$^*$\\
University of Central Florida\\
Orlando, FL\\
{\tt\small jiaqi.xue@ucf.edu}
\and
Xun Chen\\
Samsung Research America\\
Mountain View, CA\\
{\tt\small xun.chen@samsung.com}
\and
Lei Jiang\\
Indiana University Bloomington\\
Bloomington, IN\\
{\tt\small jiang60@iu.edu}
\and
Qian Lou\\
University of Central Florida\\
Orlando, FL\\
{\tt\small qian.lou@ucf.edu}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
Self-supervised learning (SSL) is a commonly used approach to learning and encoding data representations. By using a pre-trained SSL image encoder and training a downstream classifier on top of it, impressive performance can be achieved on various tasks with very little labeled data. The increasing usage of SSL has led to an uptick in security research related to SSL encoders and the development of various Trojan attacks. The danger posed by Trojan attacks inserted in SSL encoders lies in their ability to operate covertly and spread widely among various users and devices. The presence of backdoor behavior in Trojaned encoders can inadvertently be inherited by downstream classifiers, making it even more difficult to detect and mitigate the threat. Although current Trojan detection methods in supervised learning can potentially safeguard SSL downstream classifiers, identifying and addressing triggers in the SSL encoder before its widespread dissemination is a challenging task. This is because downstream tasks are not always known, dataset labels are not available, and even the original training dataset is not accessible during the SSL encoder Trojan detection. This paper presents an innovative technique called \textbf{SSL-Cleanse} that is designed to detect and mitigate backdoor attacks in SSL encoders.  We evaluated SSL-Cleanse on various datasets using 300 models, achieving an average detection success rate of $83.7\%$ on ImageNet-100. After mitigating backdoors, on average, backdoored encoders achieve $0.24\%$ attack success rate without great accuracy loss, proving the effectiveness of SSL-Cleanse. 

%Self-supervised learning (SSL) has demonstrated state-of-the-art performance in various downstream tasks by learning data representations via large unlabeled datasets. However, training an accurate SSL encoder requires significant computational resources, which has led to the wide adoption of pre-trained SSL encoders. Unfortunately, using pre-trained SSL encoders poses a non-trivial security risk, as backdoor attacks may force an SSL encoder to misclassify any inputs attached to a trigger. 


%Furthermore, it is not feasible to apply supervised-learning-based backdoor detection techniques on SSL encoders, since downstream tasks may be unknown and the dataset labels are not available. 

\end{abstract}

%%%%%%%%% BODY TEXT



\section{Introduction}
Self-supervised learning (SSL) has achieved revolutionary development and great success in various applications, especially in computer vision applications~\cite{chen2020simple,krishnan2022self,liu2022graph,chen2020improved}. This is particularly evident when labeled examples are scarce. %SSL is an unsupervised learning method that trains a general-purpose encoder by learning data representations on huge unlabeled datasets. And then, the encoder can be fine-tuned using only a few or no labeled data for various downstream tasks such as image classification~\cite{azizi2021big}.
Unlike supervised learning, SSL avoids the expensive labor cost of labeling and is trained in pretext tasks that can be generalized to many downstream tasks~\cite{chen2020improved, grill2020bootstrap,chen2021exploring}. %It maximizes the similarities of randomly cropped augmentations of an image while minimizing the agreement between transformed views of different images~\cite{chen2020improved, grill2020bootstrap,chen2021exploring}.
Several studies have demonstrated that SSL can achieve comparable~\cite{grill2020bootstrap}, and in some cases even superior, performance in few-shot learning~\cite{wu2020self,yaman2021zero}. 
The extensive use of SSL has spurred security research and vulnerability exploration in SSL encoders, as evidenced by the emergence of various Trojan attacks~\cite{bagdasaryan2021blind,saha2022backdoor,jia2021badencoder, zhang2022corruptencoder,xue2022estas,li2022demystifying,liu2022poisonedencoder}.


Malicious backdoor (aka, Trojan) attacks involve the insertion of a specially-designed trigger into an input, which causes the backdoored model to classify it into a predefined target class with high confidence. If the trigger is removed from the input, the backdoored model will still exhibit normal behaviors with almost the same accuracy as its clean counterpart. Presently, SSL backdoor attacks are executed in three phases. The first stage involves poisoning unlabeled datasets by adding triggers into a small fraction of target-class images. The second phase entails training the SSL encoder on the poisoned dataset to establish a connection between the trigger and target-class images. In the final step, any downstream classifiers that are fine-tuned on the backdoored encoder inherit the backdoor behavior~\cite{saha2022backdoor,liu2022poisonedencoder,li2022demystifying,jia2021badencoder,xue2022estas}. The current backdoor attacks have demonstrated an attack success rate of over 98\% on the ImageNet-100 dataset~\cite{jia2021badencoder, xue2022estas}.



The danger posed by Trojan attacks inserted in SSL encoders not only lies in the high attack success rate but also stems from their capacity to function covertly and spread widely across multiple users and devices. Firstly, pre-trained SSL encoders are typically spread out in real-world scenarios and subsequently fine-tuned for downstream classifiers. However, these downstream classifiers may inadvertently inherit the backdoor behaviors of trojaned encoders. While current popular Trojan detection techniques~\cite{neural_cleanse,liu2019abs,kolouri2020universal} in supervised learning may have the potential to protect SSL downstream classifiers, detecting and mitigating triggers in the SSL encoder prior to its wide distribution is a complex undertaking.  We contend that detecting and mitigating Trojans in SSL encoders is crucial since it can impede the malicious distribution of trojaned encoders. However, there is a research gap to bridge the popular backdoor defense methods in supervised learning with an SSL encoder. Detecting Trojans in SSL encoders is challenging since downstream tasks are unknown, dataset labels are not available, and even the original training dataset is not fully inaccessible. So \textit{it is crucial to implement effective detection and defense against such backdoor attacks on SSL encoders}.


%The second type of SSL backdoor attacker is that the attacker poisons the dataset, compromises the training loss, and releases a pre-trained backdoored encoder~\cite{jia2021badencoder,xue2022estas} but may not release the poisoned dataset, which has a high success attack rate and remains undetected. 



%The risk of Trojan attacks embedded in SSL encoders stems from their ability to operate covertly and disseminate broadly across different users and devices. Furthermore, the backdoor behavior in Trojaned encoders may unintentionally be inherited by downstream classifiers, making it even more challenging to identify and address the threat.


%Due to the large size of datasets and heavy computational resources required by SSL encoder training, most average users opt to fine-tune pre-trained encoders for their downstream tasks. 

%Unfortunately, pre-trained encoders downloaded from a model zoo may contain backdoors~\cite{bagdasaryan2021blind,saha2022backdoor,jia2021badencoder}.

\begin{figure}[t!]
  \centering
   \includegraphics[width=\linewidth]{figures/overview.pdf}
   \caption{The overview of SSL-Cleanse. SSL-Cleanse comprises of two components, Detector and Mitigator, aiming to remove the malicious behavior of trojaned SSL encoders. Without SSL-Cleanse, a trojaned encoder misclassifies an input with a trigger into target class (e.g., a cat with a trigger being classified as a shark). With SSL-Cleanse, the encoder behaves normally, even with poisoned samples during testing.}
   \label{fig:overview}
\end{figure} 


%Malicious backdoor (aka, Trojan) attacks~\cite{bagdasaryan2021blind,saha2022backdoor,jia2021badencoder, zhang2022corruptencoder,xue2022estas,li2022demystifying,liu2022poisonedencoder} have been a growing concern in the SSL field. Backdoor attacks involve the insertion of a specially-designed trigger into an input, which causes the model to classify it into a predefined target class with high confidence. If the trigger is removed from the input, the backdoored model will still exhibit normal behaviors with almost the same accuracy as its clean counterpart. Backdoor attacks are particularly dangerous in sensitive scenarios such as autonomous vehicle control or facial recognition. SSL backdoor attacks can be classified into two categories. %The first type is the transfer of supervised learning backdoor attacks to SSL~\cite{bagdasaryan2021blind,chen2017targeted,gu2017badnets,liu2017Trojaning,zhang2022corruptencoder}, where an attacker can access the labeled data of downstream tasks, allowing the labeled data poisoning or the classifier modification during fine-tuning. This type of attacks is not suitable for unknown downstream tasks where the labeled data is unavailable. 
%The first type of SSL backdoor attacks involves the release of poisoned datasets to the public~\cite{saha2022backdoor,liu2022poisonedencoder,li2022demystifying}. These datasets are then used by SSL trainers who typically train their models using large unlabeled publicly available data. %However, the attack success rate of this approach is very low, i.e., $39.2\%$~\cite{li2022demystifying} for the ImageNet-100 dataset~\cite{imagenet}, since the poisoned data is only a very small portion in the huge training dataset. 
%Reducing the number of class classifications results in a higher attack success rate for this approach. Specifically, for the CIFAR-10 dataset~\cite{cifar10}, the success rate was found to be $85.3\%$.
%The second type of SSL backdoor attack is that an attacker releases a pre-trained backdoored encoder~\cite{jia2021badencoder,xue2022estas}, which has a high success attack rate and remains undetected. And this type of backdoor attacks can achieve an attack success rate of $>98\%$ with only one target-class sample on the ImageNet-100 dataset~\cite{xue2022estas}. 
%The high success rates of backdoor attacks within SSL demonstrate the significant harm and danger. Therefore, it is crucial to implement effective defense against such attacks.

%Malicious backdoor (aka, Trojan) attacks~\cite{bagdasaryan2021blind,saha2022backdoor,jia2021badencoder, zhang2022corruptencoder,xue2022estas,li2022demystifying,liu2022poisonedencoder} have been a growing concern in the SSL field. Backdoor attacks involve the insertion of a specially-designed trigger into an input, which causes the model to classify it into a predefined target class with high confidence. If the trigger is removed from the input, the backdoored model will still exhibit normal behaviors with almost the same accuracy as its clean counterpart. 

%Directly applying supervised learning backdoor detection techniques to SSL encoders is unfeasible, since downstream tasks are unknown and labeled datasets are not available. Despite of recent SSL backdoor detection adaption technologies, there is still a lack of studies on how to detect Trojans in SSL encoders, which are used to conceal malicious activities. 

%Certain techniques~\cite{chen2018detecting,gao2019strip,hayase2021spectre,ma2022beatrix} utilize an embedder trained from unlabeled data to create embeddings for each sample and differentiate the poisoned data from the clean data in the embedding space. The efficacy of these methods is constrained after adaption (i.e. the average detection rate is $<26\%$). ASSET~\cite{pan2023asset} assumes that the defender possesses both the entire poisoned training dataset and the clean data for fine-tuning purposes. The authors employ two optimization techniques, one of which minimizes the loss on the clean data, while the other maximizes it on the complete poisoned dataset. This approach makes distinguishable behaviors on clean and poisoned samples by inducing different loss values, thus rendering them separable. The average detection rate is $69.3\%$ in SSL. However, the threat model is unfeasible to get the whole poisoned training dataset and a set of clean dataset in real world.

%There is a total lack of effective backdoor detection techniques for SSL encoders. Although some prior backdoor detection techniques~\cite{chen2018detecting,gao2019strip,hayase2021spectre,ma2022beatrix} create an embedder without labeled data, and use the embedder to differentiate poisoned data from clean data, the average detection rate of these techniques is quite low, i.e., $<26\%$. A more recent work ASSET~\cite{pan2023asset} assumes the defender owns both the entire poisoned training dataset and the clean training dataset. ASSET employs two optimization goals, i.e., one minimizes the loss on the clean training dataset, and the other maximizes the loss on the entire poisoned training dataset. Although the detection rate of ASSET is reasonable, e.g., $69.3\%$, its threat model assuming the defender has the whole poisoned training dataset and the clean training dataset is not realistic.



This paper introduces \textit{SSL-Cleanse}, a novel backdoor defense method as illustrated in Figure~\ref{fig:overview}. The proposed approach comprises two main components, namely the Detector and Mitigator. The Detector is responsible for identifying the presence of a Trojan in an SSL encoder, and if found, the Mitigator can mitigate the backdoor attack. Our SSL-Cleanse overcomes the challenges of backdoor detection without knowing the labeled data and the downstream tasks. Remarkably, SSL-Cleanse can successfully address both types of backdoor attacks against an SSL encoder with less than or equal to 10\% of the unlabeled training dataset.


Our contributions can be summarized as follows:
\begin{itemize}
\item To the best of our knowledge, this is the first %practical 
work to
detect and mitigate the Trojan attacks in SSL encoders without accessing any downstream labels. We reveal it is possible to prevent the dissemination of trojaned SSL encoders by using our SSL-Cleanse. 
\item The Sliding Window Kneedle (SWK) and Representation-Oriented Reverse Pattern (RORP) techniques have been proposed to aid in backdoor detection without labels. Additionally, a Trojaned Encoder Mitigation (TEM) algorithm has been developed to eliminate backdoors in SSL encoders.
\item  The effectiveness of the proposed SWK, RORP, and TEM methods in SSL-Cleanse is demonstrated through a series of extensive experiments. 
\end{itemize}



To evaluate the effectiveness of SSL-Cleanse, we pre-trained 150 backdoored models using backdoor attacks including SSLBackdoor~\cite{saha2022backdoor}, CTRL~\cite{li2022demystifying}, and ESTAS~\cite{xue2022estas}. We perform a comprehensive evaluation on the performance of SSL-Cleanse with datasets such as CIFAR-10, CIFAR-100, and ImageNet-100. SSL-Cleanse obtains the average detection rate by $83.7\%$.

In the rest of the work, Section~\ref{background} reviews the related work
and provides motivation; Section~\ref{setting} defines the problem setting; Section~\ref{approach}
presents SSL-Cleanse. Experimental methodology and results are reported in Section~\ref{method},~\ref{result-D}, and ~\ref{result:M}; Concluding remarks are given in Section~\ref{conclusion}.

%In this paper, we propose an efficient detection technique, \textit{SSL-Cleanse} with $10\%$ or less training dataset,  that can identify whether there is a Trojan in a SSL encoder or not. If yes, SSL-Cleanse can further mitigate the backdoor attack in the encoder. SSL-Cleanse aims to overcome all three types of backdoor attacks against a SSL encoder. To the best of our knowledge, SSL-Cleanse is the first work to detect Trojans in a SSL encoder. The overview of SSL-Cleanse is shown in Figure~\ref{fig:overview}. In this study, we introduce a novel approach for detecting the presence of a backdoor in SSL encoders. Specifically, we propose a detector that is capable of identifying SSL encoders that have been compromised by a Trojan. Given that a Trojanized encoder has the potential to target multiple downstream tasks simultaneously, the identification of SSL encoders is crucial in mitigating the impact of such attacks. We propose the construction of a clustering classifier by utilizing the Kneedle method for encoders that employ an unlabeled training dataset. To evaluate the effectiveness of our proposed techniques, we pre-trained 300 models using SSLBackdoor~\cite{saha2022backdoor}, ESTAS~\cite{xue2022estas}, and CTRL~\cite{li2022demystifying} attack methods. We then conducted a comprehensive evaluation of the performance of these techniques on benchmark settings, including CIFAR-10, CIFAR-100, and ImageNet-100. In addition, we propose a mitigation method and evaluate its efficacy using ImageNet-100 dataset. SSL-Cleanse can achieve the average of detection rate of $\%$, the mitigation results 




\section{Background and Related Works}\label{background}

\subsection{Self-Supervised Learning}
Supervised learning has proven to be highly effective in performing classifications, but it becomes problematic when labeled data is scarce or difficult to acquire. In response to this limitation, leveraging the large unlabeled data available in the real world is essential.
Self-Supervised Learning (SSL) is the most popular method to learn representations from complex unlabeled data~\cite{chen2020simple,krishnan2022self}.  Pre-training an encoder with significant unlabeled data and fine-tuning it with a small amount of labeled data has been shown to achieve comparable performance to using large labeled datasets with supervised learning methods for various downstream tasks~\cite{chen2020simple,krishnan2022self,liu2022graph,chen2020improved,jaiswal2020survey,lan2019albert,he2020momentum}. Furthermore, SSL techniques that rely on instance discrimination, such as SimCLR~\cite{chen2020simple} and MoCo v2~\cite{chen2020mocov2}, have become increasingly popular for learning competitive visual representations through the use of contrastive loss. In a classification task, an SSL pipeline typically comprises two phases: pre-training an image encoder and constructing a classifier, followed by fine-tuning.



\subsection{SSL Backdoor Attacks}

Malicious backdoor attacks, also known as Trojan attacks~\cite{badnets, target-attack}, involve the insertion of a specifically designed trigger into an input, which forces the backdoored model to classify it into a pre-defined target class with high confidence. Remarkably, the backdoored model can continue to function normally and produce nearly the same accuracy as its clean counterpart even after the trigger is removed from the input. Currently, SSL backdoor attacks are carried out in three phases. In the first phase, unlabeled datasets are poisoned by adding triggers to a small percentage of target-class images. The second phase involves training the SSL encoder on the poisoned dataset to establish a connection between the trigger and target-class images. Finally, any downstream classifiers fine-tuned on the backdoored encoder inherit the backdoor behavior~\cite{saha2022backdoor,liu2022poisonedencoder,li2022demystifying,jia2021badencoder,xue2022estas}. These backdoor attacks have been shown to achieve an attack success rate of over 98\% on the ImageNet-100 dataset~\cite{jia2021badencoder, xue2022estas}. The threat posed by Trojan attacks inserted in SSL encoders is not only attributed to their high attack success rate but also arises from their ability to operate surreptitiously and propagate extensively across downstream tasks. 

 %A typical backdoor attack maliciously modifies a SSL encoder, and attaches a specially-designed trigger to an input, so that the encoder is forced to classify the input to a predefined target class. If an input has no trigger, the Trojaned model will behave normally with almost the same accuracy as its clean counterpart. SSL backdoor attacks can be grouped into two types. The first type~\cite{saha2022backdoor,liu2022poisonedencoder,li2022demystifying} releases maliciously-poisoned datasets to the public, and expects victim SSL encoders are trained by their poisoned datasets. SSLBackdoor~\cite{saha2022backdoor} and CTRL~\cite{li2022demystifying} are state-of-the-art examples of the first type of backdoor attacks against SSL encoders. SSLBackdoor uses a small trigger at random locations on an input image, while CTRL adopts an invisible global trigger in the spectral space of an input image. The second type of backdoor attacks~\cite{jia2021badencoder,xue2022estas} directly releases a pre-trained backdoored encoder. ESTAS~\cite{xue2022estas} is a typical example of this type of backdoor attacks. It adopts a post-augmentation consistent trigger to achieve an attack success rate of $>98\%$.


%Attackers can either poison data and make it publicly available for a victim to download and use in training a model, or train a model on poisoned data and share the model with the victim.
%Backdoor (aka, Trojan) attack is one of the most dangerous types of malware against SSL encoders. A typical backdoor attack maliciously modifies a SSL encoder, and attaches a specially-designed trigger to an input, so that the encoder is forced to classify the input to a predefined target class. If an input has no trigger, the Trojaned model will behave normally with almost the same accuracy as its clean counterpart. %Generally, SSL backdoor attacks can be grouped into three different types.

%BadEncoder~\cite{jia2021badencoder} introduces an objective function that connects trigger patterns with reference inputs from the target class in the representation space. The goal is to ensure that any inputs containing the trigger pattern are misclassified to the target class in the downstream tasks. However, when compared to other self-supervised Trojan attacks, The BadEncoder method assumes a stronger threat model where the adversary has access to a clean pre-trained encoder and control over the training process.

%PoisonedEncoder~\cite{liu2022poisonedencoder} is designed to target a specific set of inputs by creating poisoning data through a combination of such inputs with their reference inputs in the representation space. It is important to note, this approach has a limitation is that it only works on a pre-defined set of inputs, rather than on any inputs containing trigger patterns.

%SSLBackdoor~\cite{saha2022backdoor} implement SSL backdoor attack via data poisoning in training data, they poison half of the images of the chosen category by pasting small trigger at random locations and then inject the poisons into the dataset. The representation of trigger pattern is associated with the target-class inputs through the alignment of positive pairs of trigger inputs. However, this association is dependent on random cropping of the trigger inputs, which may result in the trigger pattern not being preserved. As a result, the attack suffers from a low success rate, i.e. $<= 2\%$ on ImageNet-100.


%CTRL~\cite{li2022demystifying} uses an invisible global trigger in the spectral space of inputs~\cite{wang2022invisible} that is preserved after data augmentation and is consistent with the inference phase. Also, it is difficult to distinguish from clean data due to its spectral nature. They inject these poisoned images into the training set and obtain $39.2\%$ attack sucess rate on ImageNet-100. 


%ESTAS~\cite{xue2022estas} proposes post-augmentation consistent trigger poisoning and cascade optimization technique for achieving better attack success rates with fewer poison data samples. The authors generate three augmentations of one target class input and add a trigger on top of one augmentation. To optimize the attack, cascade optimization is used to increase the similarities between the triggered augmentation and one target-class sample. At the same time, the remaining two augmentations of the same input are placed closer together to maintain clean accuracy. Moreover, they add the trigger after data augmentation, so that the trigger pattern is consistent with the inference stage. The experimental results show that ESTAS stably achieves $> 98\%$ attack success rate with just one target-class sample.

\subsection{Related Backdoor Defense}
%In addition to those discussed in the introduction, we also provide a brief overview of other related works on backdoor detection. 
%\textbf{Supervised Learning.} 
Backdoor detection and mitigation in supervised learning have been extensively studied. ULP~\cite{kolouri2020universal} is a method that trains a classifier to differentiate between benign and trojaned models using a set of universal input patterns. %Similarly, researchers in \cite{HuangS} proposed the one-pixel signature approach, which trains a classifier to predict the model's benignity based on its one-pixel signature. In \cite{qiao}, a trigger distribution generation method was proposed for backdoor detection. 
On the other hand, researchers in \cite{zhang11, wang2020c} leveraged the differences in adversarial examples for benign and trojaned models to detect backdoors. %TABOR~\cite{TABOR} used explainable AI techniques to scan for backdoors, while in \cite{Xu2019}, backdoors were detected using Meta Neural Analysis. 
In \cite{liu2018fine}, pruning and fine-tuning were combined to weaken or even eliminate backdoors. Wang et al. proposed the use of randomized smoothing to certify model robustness against backdoors in \cite{wang2020a}. 
%Other works, such as those by Ong et al. \cite{Ong}, Gao et al. \cite{Gao2019}, Chen et al. \cite{chen2018detecting}, Chou et al. \cite{chou}, Du et al. \cite{Du2}, Liu et al. \cite{Liu2017}, and Ma et al. \cite{Ma2019NICDA}, aimed to detect if a provided input contained a trigger. 
Although these backdoor detection methods including Neural Cleanse~\cite{neural_cleanse}, ABS~\cite{liu2019abs}, and Universal Adversarial Training~\cite{kolouri2020universal}, may have the potential to safeguard SSL downstream classifiers, detecting and mitigating triggers in the SSL encoder before its widespread dissemination remains a challenging task. Detecting Trojans in SSL encoders is a challenging task due to the fact that downstream tasks are often unknown, dataset labels may not be available, and even the original training dataset may be partially or entirely inaccessible.  In a recent study, ASSET~\cite{pan2023asset} focuses on detecting poisoned samples within a training dataset. The ASSET approach accomplishes the task of identifying the poisoned examples by calculating the model's different behaviors on poisoned samples and  benign samples. The effectiveness of ASSET's detection may be limited in real-world applications where an SSL encoder is released but a portion or the entirety of the training set is not disclosed to defenders. To the best of our knowledge, SSL-Cleanse is the first practical work to
detect and mitigate the Trojan attacks in SSL encoders without accessing any downstream labels or the full training set.


 %Some prior backdoor detection techniques~\cite{chen2018detecting,gao2019strip,hayase2021spectre,ma2022beatrix} do not require labels, yet trains an embedder with unlabeled data to identify and remove poisoned samples from the training dataset. However, their average detection rate is quite low, i.e., $<26\%$. The latest backdoor detection technique ASSET~\cite{pan2023asset} for SSL encoders achieves high detection rate, e.g., $69.3\%$, but its threat model assuming the defender has the whole poisoned training dataset and the clean training dataset is not realistic.


%In this section, we delve into the different techniques used for detecting backdoors in supervised learning models, and highlight the differences between supervised learning-based backdoor detection and self-supervised learning (SSL) based backdoor detection.
 
%Neural Cleanse~\cite{neural_cleanse} is a supervised learning-based backdoor detection technique that uses reverse engineering to generate the smallest possible trigger that can be used to misclassify a target class. 
%Assuming there are N classes, and the target class is unknown, it needs to perform an $O(N^2)$ search over all possible pairs of classes to identify the target class. As a result, Neural Cleanse may not be practical for models with a large number of classes.

%ABS~\cite{liu2019abs} increases the internal neuron activation values of benign inputs to see if the model misclassifies the input as a specific class. If the input is misclassified, it means that there is a neuron in the model that is activated by the trigger pattern associated with that specific class. ABS then generates a trigger by maximizing the activation values of these neurons through optimization. A model is considered to be a Trojan if the generated trigger can cause the intended misclassification. However, the effectiveness of ABS is limited by its ability to correctly identify the compromised neurons, which can be difficult in large models with complex architectures.

%ULP~\cite{kolouri2020universal} compares the behaviors of Trojan and benign models and identifies input features that are highly correlated with Trojan behavior. These universal features are then used to train a classifier that can detect the presence of backdoors in new models.

%There are other supervised learning backdoor defense techniques. For example, some researchers~\cite{chen2018detecting,gao2019strip,hayase2021spectre,ma2022beatrix} use a filter to identify and remove poisoned samples from the training dataset before training the model. Similarly, pruning~\cite{liu2018fine} defense technique removes voluntary neurons during training to reduce the impact of poisoned samples. Knowledge distillation-based defense techniques~\cite{abbasi2020compress,pang2022backdoor,yoshida2020disabling} use clean data to distill a new model that is free from backdoors. These methods are based on the assumption that backdoors are difficult to transfer through distillation.



%\subsection{Motivation}

%Prior defense work related to data pruning such as~\cite{chen2018detecting,gao2019strip,hayase2021spectre,ma2022beatrix,pan2023asset}, assumes that the defender possesses both clean test data and a complete poisoned training dataset. Under this assumption, the defender can perform a scan of all unlabeled images and then differentiate between clean and poisoned samples through optimization techniques based on different loss values. The average detection rate achieved by this method is $69.3\%$. However, it is important to note that the threat model used in this research is not feasible in the real world, as it assumes the defender has access to the entire poisoned training dataset as well as a separate set of clean data. In addition, applying the backdoor detection techniques that target supervised-learning-trained models directly to SSL encoders is impossible. Unlike supervised-learning-trained models that rely on labeled data, SSL encoders only generate data representations. To overcome above challenges, we propose a more practical scenario where the defender has access to only a portion of the training dataset. We introduce SSL-Cleanse, a method that can identify both small triggers and global spectral triggers used by backdoor attacks in SSL encoders. Furthermore, SSL-Cleanse can mitigate the adverse effects of backdoors.



%\textbf{Existing Detection.}
%ULP~\cite{kolouri2020universal} trains a classifier to determine if a model is Trojaned. It leverages a large pool of benign and Trojaned models to learn a set of universal input patterns that can lead to different logits for benign and Trojaned models. The classifier is then trained on these logits. Similar to ULP~\cite{kolouri2020universal}, researchers in \cite{HuangS} proposed one-pixel signature. They trained a classifier to predict the model’s benignity based on their one-pixel signature. ~\cite{qiao} proposed to generate trigger distribution. ~\cite{zhang11, wang2020c} leveraged the differences of adversarial examples for benign and Trojaned models to detect backdoors. TABOR~\cite{TABOR} used explainable AI techniques to scan backdoors. ~\cite{Xu2019} detected backdoors using Meta Neural Analysis. ~\cite{liu2018fine} combined pruning and fine-tuning to weaken or even eliminate backdoors. ~\cite{wang2020a} certified model robustness against back door via randomized smoothing. ~\cite{Ong, Gao2019, chen2018detecting, chou, Du2, Liu2017, Ma2019NICDA} aimed to detect if a provided input contains trigger. Comprehensive surveys of backdoor learning can be found at ~\cite{Li2020a, Li2020b}



\section{SSL-Cleanse}\label{setting}
Our aim is to detect and prevent backdoor attacks in a SSL encoder. To achieve this goal, we investigate three effective attack methods: SSLBackdoor~\cite{saha2022backdoor}, ESTAS~\cite{xue2022estas}, and CTRL~\cite{li2022demystifying}. By examining these methods, we develop a comprehensive and robust detection system capable of accurately identifying and preventing these types of attacks.

\subsection{Attack Model}
Our attack model is consistent with that of prior work, i.e. SSLBackdoor~\cite{saha2022backdoor}, CTRL~\cite{li2022demystifying} and ESTAS~\cite{xue2022estas}. SSL-Backdoor~\cite{saha2022backdoor} proposed a backdoor attack via data poisoning in training data, wherein a specific trigger pattern is placed at random locations of target class inputs. To evaluate the performance of our detector, we generate a poisoned dataset following this design and train trojaned encoders by maximizing the data representation similarity of the trigger inputs and target class inputs.
We also incorporate the stealthy global trigger in the frequency domain, as proposed by CTRL~\cite{li2022demystifying}, to verify the performance of our detector. Additionally, we use the consistent trigger poisoned dataset proposed by ESTAS~\cite{xue2022estas}, which adds the trigger after data augmentation, to train the Trojaned encoder.


\subsection{Defense Assumptions and Goals}
We assume that the defender has access to a pre-trained SSL encoder, a small portion of the unlabeled dataset (which may be a separate training set, i.e., SSL-Cleanse does not require attackers to disclose their poisoned dataset), and computational resources such as GPUs or GPU-based cloud services to evaluate or modify SSL encoders. 

\textbf{Goals.} We have three specific goals:

\begin{itemize}
\item \textbf{Detecting backdoor:} Our aim is to make a binary determination regarding the potential infection of a given SSL encoder by the backdoor attack.  If infected, we also want to 
 identify the potential target classes of the backdoor attack. The detection technique is described in Section~\ref{sec:Detection}. 
 
\item \textbf{Identifying backdoor:} We aim to identify the expected operation of the backdoor. More specifically, we plan to reversely generate the trigger used by the attack.

\item \textbf{Mitigating Backdoor:} Our ultimate goal is to deactivate the backdoor, making it ineffective. To achieve this, we plan to employ our Trojan Encoder Mitigation (TEM) described in Section~\ref{sec:mitigation} to eliminate the backdoor while preserving the classification performance of the SSL encoder for normal inputs.
\end{itemize}




\begin{figure*}[th!]
  \centering
   \includegraphics[width=\linewidth]{figures/workflow.pdf}
   \caption{The working flow of SSL-Cleanse detector. Given unlabelled images,  their representation can be obtained by passing them through a pre-trained self-supervised learning (SSL) encoder. These representations can be categorized into K clusters by employing the SWK clustering technique. Following this step, representation-oriented reverse patterns for each cluster can be obtained and forwarded to an outlier detector for post-processing.}
   \label{fig:workflow}
\end{figure*} 



 \begin{figure}
  \centering
   \includegraphics[width=0.75\linewidth]{figures/kneedle.pdf}
   \caption{Comparison of our SWK method and other Clustering methods on ImageNet-100 dataset. Our SWK method yields more stable and accurate K.}
   \label{fig:AK}
\end{figure}

\begin{figure}[h!]
  \centering   \includegraphics[width=\linewidth]{figures/reverse.pdf}
   \caption{The working flow of Representation Oriented Reverse Pattern (RORP). We begin by selecting images $x^j_i$ from each cluster $D_i$, and initializing $r_i$. These inputs are then fed into a pre-trained SSL encoder to obtain representations. We iteratively update $\Delta_i$ and mask $m_i$ to generate representations that are similar to those of $x^j_i$. This process results in triggers generation for k clusters, which are subsequently forwarded to the outlier detector module for further processing.}
   \label{fig:reverse}
\end{figure} 


\section{Our Approach: SSL-Cleanse}\label{approach}
We propose a novel, effective, and practical approach \textit{SSL-Cleanse} for identifying and mitigating backdoors inside pre-trained trojaned SSL encoders. As depicted in Figure~\ref{fig:overview}, the proposed framework consists of two main components: a detector and a mitigator. In the detector, we propose a Sliding Window Kneedle (SWK) optimized on previous Kneedle algorithm~\cite{Kneedle} to produce $K$ clusters based on the representations of the SSL encoder. We then introduce the Representation-Oriented Reverse Pattern (RORP) method to generate a reversed pattern for each cluster. By utilizing these $K$ patterns, we can perform outlier detection to determine if the SSL encoder is trojaned. In the mitigator, we introduce a method called Trojan Encoder Mitigator (TEM), which is designed to efficiently eliminate the trojans present in the backdoored SSL encoders.


\subsection{Detector}
\label{sec:Detection}
The workflow of the detector, as shown in Figure~\ref{fig:workflow}, begins by feeding a small fraction ($\epsilon\%$) of the unlabeled training dataset to a pre-trained SSL encoder to obtain input representations. These representations are then clustered into $K$ groups using the  SWK method. Next, for each cluster $i$, the RORP technique is used to invert the input patterns. Then, the outlier detector is utilized to make a binary decision regarding the pre-trained encoder's benign or Trojaned status.  Since the target class is unknown, we assume that each clustering could potentially be the target class. Therefore, we examine each cluster of the encoder and search for the existence of a cluster where the encoder can generate similar representations to the target class with significantly smaller modifications to an image.

Algorithm~\ref{alg:detection} provides a more detailed description of the detector methodology. The input to the algorithm is a set of unlabeled samples $D$ and an encoder $f$. The algorithm comprises three main steps. In Step 1, the data is classified into $K$ clusters using the SWK algorithm. The resulting clusters are denoted by an array of $[D_1, D_2, ..., D_K]$. In Step 2, $p$ images are randomly sampled from each cluster, and each cluster is considered a potential target for the backdoor attack. Here we set $p=32$ and its study is described in supplementary materials.  An optimization strategy is employed to determine the \textit{minimum} reversed trigger required to elicit a comparable representation for all other classes. The reversed trigger for cluster $i$ is defined as $m_i \cdot \Delta_i$, where mask $m_i$ represents the location of the trigger pixel, and $\Delta_i$ denotes the pixel value of the trigger. In Step 3, we employ the outlier detection technique to identify all potential triggers $t_i$ generated from each cluster. If we find a trigger that is significantly smaller than the other candidates, we classify it as an outlier and store it in a set $t_s$. If $t_s$ remains empty, we consider the encoder to be benign; otherwise, we classify it as a Trojaned encoder.

\subsubsection{Sliding Window Kneedle Clustering}
 %The key idea behind our approach to detecting backdoors is that the Trojaned encoder requires fewer changes to generate a representation that is similar to the representation of the target class compared to the normal encoder. 
 %Since the target class is unknown, we assume that each clustering could potentially be the target class. Therefore, we examine each cluster of the encoder and search for the existence of a cluster where the encoder can generate similar representations to the target class with significantly smaller modifications to an image. %Approximately 20 minutes were required to process $10\%$ of the training dataset on ImageNet-100 with SWK method to get clusters.



The proposed approach assumes that the defender has access to an $\epsilon\%$ unlabeled training samples and a pre-trained SSL encoder. To perform the clustering, we explore the silhouette analysis clustering method, denoted by Kneedle~\cite{Kneedle}. The silhouette score measures the degree to which each point belongs to its cluster compared to other clusters and ranges from -1 to 1, with higher scores indicating better clustering. By plotting the curvature of the silhouette score against different numbers of clusters, one can search for the elbow point to indicate the optimal number of clusters. However, we illustrate, in Figure~\ref{fig:AK}, that directly applying to the existing  Kneedle algorithm is difficult to derive a proper $K$.  This is because the silhouette score plot may have multiple peaks and can suffer from local optima. %To address this issue, a Slinding-Window Kneedle algorithm is introduced to improve elbow point detection. The Kneedle algorithm detects elbow points based on the curvature of a continuous function.
In addition, to address the issue of noise caused by the high dimensionality of the encoder's outputs and diverse representation, we optimized the Kneedle algorithm by incorporating a sliding window technique with a window size of 2 and averaging 10 clustering results. The resulting smoother curve is more effective in identifying the elbow point (represented by the blue dashed line) and reducing noise, leading to a more accurate cluster prediction. %Approximately 20 minutes were required to process $10\%$ of the training set on ImageNet-100 with SWK method to get clusters.

%The reverse engineer of the defender needs labeled data. Therefore, to avoid manual annotation, we propose a clustering approach based on the encoder’s representations. We use the silhouette score and the Kneedle\cite{Kneedle} algorithm to determine the optimal number of clusters. The silhouette score\cite{Silhouette, Silhouette_0} measures how well each point belongs to its cluster compared to other clusters, ranging from -1 (poor clustering) to 1 (good clustering). We plot the silhouette score against different numbers of clusters and look for the elbow point, where adding more clusters does not improve the score significantly. This point indicates the best number of clusters. The Kneedle algorithm is a technique that detects elbow points based on the curvature of a continuous function. However, since the encoder’s outputs have high dimensionality and the qulity of representation highly depends on the quality of encoder, there is a lot of noise in the curvature of the silhouette score function, which affects the Kneedle’s decision. To address this issue, we optimize the Kneedle algorithm by using a shift window technique and averaging multiple clustering results. This smooths out the curvature of the silhouette score and reduces noise, leading to more accurate cluster count prediction. The results shown in it indicate that the initial silhouette score and its curvature contain a significant amount of noise, which can cause the Kneedle algorithm to output a significantly smaller value for the optimal number of clusters (k) than the true k value. However, by applying the shift window technique and averaging multiple clustering results, the SWK method is able to optimize the Kneedle algorithm and obtain a more accurate prediction for k that is closer to the true value.
\begin{figure*}[ht!]
  \centering
   \includegraphics[width= 0.7\linewidth]{figures/mitigation.pdf}
   \caption{The working flow of our mitigator method. Assuming a clean input image $x_i$, we perform data augmentation to generate two images, denoted as $x_{i1}$ and $x_{i2}$. With a probability of $50\%$, a trojan trigger $t_i$ is appended to $x_{i2}$. The image $x_{i1}$ is then fed to a fixed encoder $f$ to obtain a representation that remains unaltered. On the other hand, the second encoder $f^\prime$ is updated using the augmented data to mitigate the effect of the trojan trigger. The encoder is considered clean once it achieves accurate classification performance, even in the presence of the trojan trigger.}
   \label{fig:mitigation}
\end{figure*} 

\begin{algorithm}[htb!]
\caption{Encoder Detection}
   \label{alg:detection}
\begin{algorithmic}
    \STATE {\bfseries Input:} unlabeled samples $D$, encoder $f$
    \STATE {\bfseries Output:} Benign or Trojan
    \STATE {\bfseries Step 1: Cluster unlabeled samples D with SWK}
    \STATE $K \gets SWK(D)$
    \STATE ${D_1, D_2, ..., D_K} \gets SWK(D, K, f)$
    \STATE {\bfseries Step 2: Reverse pattern generation (RORP) on each cluster}
    \FOR{$i=1$ {\bfseries to} $K$}
    \STATE initialize mask $m_i$ and delta $\triangle_i$
    \STATE $D_{j} \gets$ Randomly sample $p$ images from $ D-D_i$
    \STATE $m_i, \Delta_i \gets update(m_i, \Delta_i, D_{j})$
    \ENDFOR
    \STATE {\bfseries Step 3: Abnormal trigger detection}
    \STATE $t_s \gets outlier(m_i, \Delta_i)$
    \IF{$t_s$ is empty} 
    \STATE return Benign
    \ELSE
    \STATE return Trojan
    \ENDIF
\end{algorithmic}
\end{algorithm}

\subsubsection{Representation Oriented Reverse Pattern}
\label{sec:Detection:loss}





Figure~\ref{fig:reverse} illustrates RORP technique. First, we choose a clean image $x_i^j$ from a cluster $i$ as a potential target class, sample a random image $y_j$ ($j\neq i$) and create mask $m_i$ and $\Delta_i$ for the same cluster. We define the trigger image  for cluster $i$ as $r_i$ shown in Equation~\ref{e:patch trigger}. The clean image $x_i^j$ 
 and trigger $r_i$ are then sent to a pre-trained SSL encoder, and our representation-oriented reverse engineering technique is employed to update the trigger such that its representation can have more similarity with $x_i$'s feature. The loss function used in the RORP to optimize $m_i$ and $\Delta_i$ on the $i^{th}$ cluster is given by Equation~\ref{e:l}. The loss function comprises two components. The foremost objective of the first term is to guarantee the similarity between the image patched with the trigger and the target class image in the feature space. The second term is responsible for constraining the size of the reversed trigger, we adapted $\lambda$ dynamically during optimization in our experiment. The dynamic scheduler is described in the supplementary material.  Finally, the resulting trigger patterns are sent to an outlier detector for determination.


\begin{equation} 
    \label{e:patch trigger}
    %x^{\prime}=p(x,m,\Delta)=(1-m) \cdot x + m \cdot \Delta
    r_{i}=(1-m_i) \cdot y_j + m_i\cdot \Delta_i
\end{equation}

\begin{equation}
\begin{gathered} 
    \label{e:l}
        \mathcal{L}_{MSE}(f(x_i), f(r_i))=-\frac{<f(x_i), f(r_{i})>}{||f(x_i)|| \cdot ||f(r_{i})||} + \lambda \cdot |m_i| 
        %where \quad i \in k, i\not = j
\end{gathered}
\end{equation}

Here we note that the variable $m_i$ signifies the position of the trigger pixel, while $\Delta_i$ represents the corresponding pixel value of the trigger. The function $f()$ denotes the encoder function, and $x_i$ refers to an image belonging to $i^{th}$ cluster. $< a, b>$ and $||a||$ represent the cosine similarity of $a$ and $b$, and the $l2$-norm of $a$, respectively.  



\textbf{Outlier Detector.} Once $K$ potential triggers for each cluster have been generated, the size of each trigger is determined by computing $||m_i \cdot \Delta_i||$ or $||m_i||$. This condition is dependent on the attacking method. In the case of the attacking method being CTRL~\cite{li2022demystifying}, which uses a frequency-domain trigger, the trigger size is defined as $||m_i \cdot \Delta_i||$ due to its large $m_i$ and small $\Delta_i$. For other attacking methods in this paper, the trigger size is set as $||m_i||$. In practical settings, both trigger sizes could be generated and examined to identify the presence of an abnormal trigger size. To identify all potential triggers $T_i$ generated from each cluster, we employ the outlier setting from \cite{neural_cleanse}. If we discover a trigger that is substantially smaller than the other candidates, we classify it as an outlier and store it in a set $t_s$. If $t_s$ remains empty, we conclude that the encoder is benign. However, if $t_s$ contains any triggers, we classify the encoder as a trojaned encoder.



\begin{algorithm}[htb]
   \caption{Trojaned Encoder Mitigation (TEM)}
   \label{alg:mitigation}
\begin{algorithmic}
    \STATE {\bfseries Input:} unlabeled dataset $D$, abnormal triggers $t_s={[t_{1}, t_{2}, ..., t_{K}]}$, trojaned encoder $f$
    \STATE {\bfseries Output:} a clean encoder $f^{\prime}$
    \STATE $f^{\prime} \gets f$
    \FOR{$x_i$ in $D_i$, $i \in K $}
    \STATE $x_{i1} \gets aug_1(x_i)$
    \STATE $t_i \gets$ randomly selected from $t_s$  
    \STATE $x_{i2} \gets equalSample\{aug_2(x_i)+t_i, aug_2(x_i)\}$ 
    %\STATE // {Note: $x_{i2}$ selection is equal chance for each method.}
    \STATE $z, z^{\prime}=f(x_{i1}), f^{\prime}(x_{i2})$
    \STATE $loss \gets -similarity(z, z^{\prime})$
    \STATE $f^{\prime} \gets update(f, loss)$
    \STATE $f^{\prime}.params=0.9 \times f.params + 0.1 \times f^{\prime}.params$
    \ENDFOR

    \STATE return $f^{\prime}$
    
\end{algorithmic}
\end{algorithm}
\subsection{Trojaned Encoder Mitigation} \label{sec:mitigation} Algorithm~\ref{alg:mitigation} outlines the details of our mitigation technique, which employs unlabeled samples $D$, a set of Trojan triggers $t_s$ generated from our detector, and a trojaned encoder $f$. Initially, we select a set of clean images $x_i$ from each cluster, and augment these images to create new training samples consisting of the augmented images $x_{i1}$ and $x_{i2}$. Subsequently, we attach previously generated trigger $t_i$ to $x_{i2}$ or directly use $x_{i2}$ without adding a trigger. The probability is $50\%$ which is the meaning of $equalSample$ in Algorithm~\ref{alg:mitigation}. The 50\% means that we set an equal weight for ASR removal and clean accuracy.  Notice that here we pass these new training samples through the trojaned encoder $f$ to obtain their respective representations. We then optimize the similarity between the representations using a loss function by fixing the model $f$ and updating the encoder $f'$ to eliminate the Trojan trigger effects, resulting in a clean encoder. Figure~\ref{fig:mitigation} illustrates an example of the mitigation process, where a clean image of a cat $x_i$ is augmented to generate two cat images $x_{i1}$ and $x_{i2}$, one that is the same and another that has a 50\% chance of being attached with a Trojan trigger. The clean image is sent to a reference model $f$, which maintains its parameters and representations unchanged. In contrast, the other model $f'$ is updated with the new training sample to remove the Trojan trigger effect. The updated encoder is deemed to be clean after it is able to accurately classify data, even in the presence of the Trojan trigger.






\begin{table*}[h!]
\centering
\footnotesize
\setlength{\tabcolsep}{3pt}
\caption{The performance of our detector on three different attack methods with different unlabeled training dataset ratio $\epsilon \%$.}
\begin{tabular}{cccccccccccccc}\toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{\makecell[c]{Dataset\\ratio $\epsilon $(\%)}} & \multicolumn{4}{c}{SSLBackdoor} & \multicolumn{4}{c}{ESTAS} & \multicolumn{4}{c}{CTRL} \\
\cmidrule(lr){3-6} \cmidrule(lr){7-10} \cmidrule(lr){11-14}
& & Cluster & FPR(\%) & FNR(\%)  & ACC(\%) & Cluster & FPR(\%) & FNR(\%) & ACC(\%) & Cluster & FPR(\%) & FNR(\%) & ACC(\%) \\\hline
\multirow{4}{*}{\makecell[c]{CIFAR-10}} %& 1 & $3$ & $5$ & $1$ & $54$ & $4$ & $15$ & $2$ & $63$ & $4$ & $7$ & $1$ & $56$ \\
& $5$ & $9$ & $6$ & $44$ & $75$ & $8$ & $10$ & $6$ & $46$ & $9$ & $6$ & $46$ & $74$ \\
& $8$ & $\textcolor{green}{10}$ & $10 $ & $36 $ & $77$ & $\textcolor{green}{10}$  & $10$ & $16 $ & $\textbf{87}$ & $\textcolor{green}{10}$  & $10$ & $28$ & $81$ \\
& $10$ & $11$ & $10$ & $30$ & $\textbf{80}$ & $11$ & $14$ & $14$ & $86$ & \textcolor{green}{$10$} & $8$ & $28$ & $\textbf{82}$ \\\hline
\multirow{4}{*}{\makecell[c]{CIFAR-100}} %& $1$ & $45$ & $6$ & $0$ & $56$ & $45$ & $11$ & $1$ & $60$ & $40$ & $7$ & $0$ & $57$ \\
& $5$ & $65$ & $6$ & $44$ & $56$ & $70$ & $6$ & $46$ & $67$ & $70$ & $6$ & $46$ & $60$ \\
& $8$ & $75$ & $10$ & $36$ & $61$ & $75$ & $10$ & $16$ & $71$ & $75$ & $10$ & $28$ & $62$ \\
& $10$ & $90$ & $10$ & $30$ & $\textbf{64}$ & $90$ & $14$ & $14$ & $\textbf{78}$ & $95$ & $8$ & $28$ & $\textbf{65}$ \\\hline
\multirow{4}{*}{\makecell[c]{ImageNet-100}} %& $1$ & $80$ & $23$ & $6$ & $67$ & $85$ & $35$ & $9$ & $76$ & $80$ & $33$ & $4$ & $79$ \\
& $5$ & $90$ & $6$ & $44$ & $71$ & $90$ & $6$ & $46$ & $79$ & $90$ & $6$ & $46$ & $82$ \\
& $8$ & \textcolor{green}{$100$} & $10$ & $36$ & $79$ & \textcolor{green}{$100$} & $10$ & $16$ & $85$ & \textcolor{green}{$100$} & $10$ & $28$ & $83$ \\
& $10$ & \textcolor{green}{$100$} & $10$ & $30$ & $\textbf{80}$ & $105$ & $14$ & $14$ & $\textbf{86}$ & \textcolor{green}{$100$} & $8$ & $28$ & $\textbf{85}$ \\
\bottomrule
\end{tabular}
\label{t:detector}
\end{table*}


\begin{table*}[h!]
\centering
\footnotesize
\setlength{\tabcolsep}{4pt}
\caption{The performance of our mitigator on three different attack methods with different unlabeled training dataset ratio $\epsilon \%$.}
\begin{tabular}{ccccccccccccc}\toprule
\multirow{3}{*}{\makecell[c]{Dataset\\ratio $\epsilon$ (\%)}} & \multicolumn{4}{c}{SSLBackdoor} & \multicolumn{4}{c}{ESTAS} & \multicolumn{4}{c}{CTRL} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13}
& \multicolumn{2}{c}{Before mitigator}  & \multicolumn{2}{c}{With mitigator} & \multicolumn{2}{c}{Before mitigator}  & \multicolumn{2}{c}{With mitigator} & \multicolumn{2}{c}{Before mitigator}  & \multicolumn{2}{c}{With mitigator}  \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
& ACC(\%) & ASR(\%) & ACC(\%) & ASR(\%) & ACC(\%) & ASR(\%) & ACC(\%) & ASR(\%) & ACC(\%) & ASR(\%) & ACC(\%) & ASR(\%) \\\hline
%$1$ & $60.8$ & $34.0$ & $54.1$ & $6.37$ & $62.5$ & $97.4$ & $54.7$ & $3.1$ & $52.5$ & $42.9$ & $41.8$ & $7.14$ \\
$5$ & $60.8$ & $33.2$ & $57.9$ & $\textbf{1.07}$ & $61.3$ & $98.5$ & $59.0$ & $\textbf{1.8}$ & $53.2$ & $43.1$ & $47.7$ & $\textbf{2.19}$ \\
$8$ & $60.2$ & $33.1$ & $59.4$ & $\textbf{0.15}$ & $61.5$ & $99.2$ & $61.3$ & $\textbf{0.22}$ & $53.6$ & $41.7$ & $51.4$ & $\textbf{0.71}$ \\
$10$ & $60.6$ & $33.2$ & $60.2$ & $\textbf{0.14}$ & $62.7$ & $98.5$ & $62.0$ & $\textbf{0.22}$ & $53.3$ & $42.1$ & $52.7$ & $\textbf{0.35}$ \\
\bottomrule
\end{tabular}
\label{t:mitigator}
\end{table*}




% \section{Experimental Methodology} \label{method}

% \textbf{Dataset}. Our experiments were conducted on three benchmark datasets: CIFAR-10~\cite{cifar10}, CIFAR-100~\cite{cifar100}, and ImageNet-100~\cite{imagenet}. CIFAR-10 comprises $50,000$ $32\times32$ training images divided into 10 classes. CIFAR-100 is similar to CIFAR-10, with 500 training images for each of its 100 classes. ImageNet-100 is a random subset of 100 classes from the larger ImageNet dataset, and contains around 127,000 images in the training set.

% \textbf{Encoder construction}. To assess the effectiveness of our detector against various attack methods, we evaluated it against three backdoor attacks, namely SSLBackdoor~\cite{saha2022backdoor}, CTRL~\cite{li2022demystifying}, and ESTAS~\cite{xue2022estas}. We created 50 benign encoders and 50 Trojaned encoders for each backdoor attack. ResNet-18~\cite{resnet} combined with a two-layer MLP is chosen as the encoder architecture. The MLP is used to project outputs into a low-dimensional latent space, i.e., 64-dimensional for CIFAR-10 and CIFAR-100, and 128-dimensional for ImageNet-100.

% \textbf{Experimental Settings}. Our experiments were carried out using two Nvidia GeForce RTX-3090 GPUs, each with a memory capacity of 24GB. The batch size for both the detector and the mitigator was set to 64, and the learning rate was set to 0.01 for the detector and 0.001 for the mitigator. For the detector, the initial value of $\lambda$ was $0.01$. We adopted the triggers from the Hidden Trigger Backdoor Attacks~\cite{Saha_Subramanya_Pirsiavash_2020}. The trigger size was $50\times50$ for ImageNet-100 and $6\times6$ for CIFAR-100 and CIFAR-10.


% \textbf{Evaluation Metrics}. We define the following evaluation metrics to study the efficiency, and effectiveness of our SSL-Cleanse. Attack Success Rate (\textbf{ASR}) is defined as the ratio of images that contain the trigger and are classified as the target class, to the total number of evaluated images. \textbf{TP}, \textbf{FP}, \textbf{TN}, and \textbf{FN} denote the numbers of true positives, false positives, true negatives, and false negatives, respectively. 
% False Positive Rate (\textbf{FPR}) is the proportion of benign encoders predicted by the detector to be trojaned; $FPR = FP/(FP+TP)$. False Negative Rate (\textbf{FNR}): the proportion of trojaned encoders predicted by the detector to be clean; $FNR = FN/(FN+PN)$. 




% \section{Results on the SSL-Cleanse Detector} \label{result-D}
% We present the performance results of our detector with various ratios of unlabeled training datasets across three backdoor attacks in Table~\ref{t:detector}.

% \subsection{Different Training Dataset Sizes} 

% We investigate the impact of a varying training dataset size on the detection performance. Specifically, we use different proportions of images in the training set (dataset ratio), i.e., $5\%$, $8\%$, and $10\%$. It is estimated that it will take 20 minutes to generate the cluster numbers using the SWK method for $10\%$ of the ImageNet-100 training data. Table \ref{t:detector} demonstrates that the SWK method provides more accurate clustering results for larger datasets. Notably, both CIFAR-10 and ImageNet-100 achieve accurate clustering results with only $8\%$ of the training dataset (highlighted in green). Furthermore, the detection performance increases with an enlarging proportion of the training dataset. Even when the number of clusters exceeds the actual cluster numbers, more training data still result in a better detection rate.


% %Specifically, in the case of the CIFAR-10 task, the predicted cluster number was 8 or 9 with 5\% of the training set, and 10 with 8\% of the training set. Similarly, for CIFAR-100, the predicted cluster number was 65 with 5\% of the training set, and 90 or 95 with 10\% of the training set. Finally, in the case of ImageNet-100, the predicted cluster number was 90 with 5\% of the training set, and 100 or 105 with 10\% of the training set.

% %Specifically, the authors 




% \subsection{SSLBackdoor} 

% SSLBackdoor~\cite{saha2022backdoor} is a data-poisoning-based backdoor attack against SSL encoders. It randomly selects 50\% of the images from a specified category, and applies a small trigger on a random location to poison the training dataset. The trigger pattern representation is linked with the target-class inputs through the alignment of positive pairs of trigger inputs. In order to assess the efficacy of SSL-Cleanse, we generate a poisoned dataset using SSLBackdoor and train Trojan encoders by maximizing the similarity of data representations between the trigger inputs and the target class inputs. The detection results on SSLBackdoor are shown in Table \ref{t:detector}, where 50 benign encoders and 50 Trojaned encoders across diverse datasets are involved. 

% \textbf{CIFAR-10}. When the dataset size increases from $5\%$ to $8\%$, the detection accuracy increases from $75\%$ to $80\%$, and the FNR decreases.

% \textbf{CIFAR-100}. The training dataset of CIFAR-100 has only 500 images per class. Moreover, the dataset is plagued by label noises. So it is difficult to achieve high accuracy on CIFAR-100. Considering noisy labels, an SSL encoder~\cite{noise} achieves only $63.5\%$ in the image classification on CIFAR-100. SSL-Cleanse is able to achieve a detection rate of $64\%$ using 10\% of the available training data. The FPR and FNR are both high.

% \textbf{ImageNet-100}. Despite the fact that ImageNet-100 has the same number of classes as CIFAR-100, SSL-Cleanse achieves a detection rate of $80\%$. This is because the training dataset of ImageNet-100 is substantially larger.


% %This outcome is noteworthy given that the images in ImageNet-100 are of superior quality and




% \subsection{ESTAS} 
% %we used the consistent trigger poisoned dataset proposed by ESTAS~\cite{xue2022estas}, which adds the trigger after data augmentation, to train the Trojan encoder.
% ESTAS~\cite{xue2022estas} is another data-poisoning-based backdoor attack. It generates three augmentations of one target class input, and adds a trigger on top of one augmentation. To optimize the attack, cascade optimization is used to increase the similarities between the triggered augmentation and one target-class sample. At the same time, the remaining two augmentations of the same input are placed closer together to maintain clean accuracy. Moreover, ESTAS adds the trigger after data augmentation, so that the trigger pattern is consistent with the inference stage. ESTAS stably achieves a $> 98\%$ attack success rate with just one target-class sample. Table \ref{t:detector} displays the outcomes of our analysis on different datasets, using 50 benign encoders and 50 Trojan encoders.

% \textbf{CIFAR-10}. 
% With $8\%$ of the training dataset, the detection rate is $87\%$. Increasing the training dataset to $10\%$ results in 11 clusters, but still can obtain an $86\%$ detection rate.

% \textbf{CIFAR-100}. Even with only $5\%$ of the training dataset, we still achieve a detection accuracy of $67\%$. The higher detection rate over SSLBackdoor is caused by the trigger design of ESTAS is more specialized. For CIFAR-100, a training dataset size increase from $5\%$ to $10\%$ yields a large detection accuracy improvement from $67\%$ to $78\%$. So a larger training dataset is crucial for CIFAR-100 or other noisy datasets. 

% \textbf{ImageNet-100}. With $5\%$ of the training dataset, we achieve a detection accuracy of $85\%$ on ImageNet-100. When the training dataset size enlarges, the FPR and FNR decrease accordingly.



% \subsection{CTRL}  
% CTRL~\cite{li2022demystifying} uses an invisible global trigger~\cite{wang2022invisible} in the spectral space of inputs that is preserved after data augmentation and is consistent with the inference phase. We incorporate the stealthy global trigger to verify the performance of our detector. Table \ref{t:detector} displays the performance outcomes of our detection method using 50 benign encoders and 50 Trojan encoders on various datasets.

% \textbf{CIFAR-10}. At training dataset sizes of $8\%$ and $10\%$, the number of clusters identified equals the actual number of classes, which is 10. Furthermore, as the training dataset size is increased, the detection accuracy improves and reaches $82\%$ with low FPR and FNR.

% \textbf{CIFAR-100}. As the number of training datasets increases, the detection accuracy improves to $65\%$ and FNR decreases, although it remains relatively high.  

% \textbf{ImageNet-100}. When the training dataset sizes are $8\%$ and $10\%$, the number of clusters detected matches the actual number of classes, which is 100. Moreover, as the size of the training dataset is increased, the detection accuracy improves and reaches $85\%$, with a low FNR. However, it should be noted that the FPR increases as more training data is used.



% %\subsection{Tradeoff between FPR and FNR.}
% %We calculate the false positive rate (FPR) and false negative rate (FNR) when setting different \textit{anomaly index} for the outlier to control its sensitivity to abnormal data points. Then, we get multiple FPR-FNR pairs, which can help users balance the numbers of false positives and false negatives during real-world deployment. Results in Figure \ref{fig:FNR_FPR} show that we achieve powerful filtering performance for three types of attacks on different datasets, i.e., obtaining $6.33\%$ FNR at an FPR of $5\%$. %Not surprisingly, SSL-Backdoor encoders are more difficult to filter out, because its attack success rate is the lowest.

% %\begin{figure}[tb]
%  % \centering
%    %\includegraphics[width=\linewidth]{figures/FNR.pdf}
%    %\caption{False negative rate of Trojan encoder detection when achieving different false positive rate.}
%   % \label{fig:FNR_FPR}
% %\end{figure} 

% \subsection{Various Trigger Sizes}

% We conducted experiments on trojaned encoders attacked by SSLBackdoor using triggers of varying sizes to investigate the impact of trigger size on the detector's performance. The trigger sizes ranged from $1\%$ to $30\%$. Table \ref{tab:trigger_ratio} presents the results, indicating that our detector performs well when the trigger size ratio is below $5\%$, achieving approximately $80\%$ detection accuracy on CIFAR-10 and ImageNet-100. Even with a $10\%$ trigger, we can still achieve relatively stable detection results. However, as the trigger size increases, the detection accuracy decreases. This could be attributed to the reversed trigger also becoming larger, making it more challenging for the outlier detector to distinguish abnormal triggers.

% Additionally, Figure~\ref{fig:trigger} demonstrates that for a poisoned image via the representation-oriented reverse pattern technique, it produces a very small pattern within the trojaned encoder that aligns with the trigger, while a clean input will produce a large pattern with similar features to the input. Therefore, if the trigger size is very large, it will be challenging to distinguish between poisoned images and clean images.



% %($3.2\times3.2$ for CIFAR-10 and CIFAR-100; $22.5\times22.5$ for ImageNet-100)

% %($17.5\times17.5$ for CIFAR-10 and CIFAR-100; $123\times123$ for ImageNet-100)









% \begin{table}[t!]
%   \caption{The influence of trigger size on detection performance across varying datasets.}
%   \footnotesize
%   \setlength{\tabcolsep}{4pt}
%   \begin{center}
%     \begin{tabular}{cccc}\toprule
%     Trigger Ratio(\%) & CIFAR-10(\%) & CIFAR-100(\%) & ImageNet-100(\%) \\
%     \midrule
%     $1$ & $84.$ & $67.2$ & $83.2$ \\
%     $3$ & $80.4$ & $65.0$ & $79.8$ \\
%     $5$ & $80.0$ & $64.0$ & $80.0$ \\
%     $10$ & $74.8$ & $57.0$ & $76.2$ \\
%     $15$ & $36.4$ & $27.6$ & $61.6$ \\
%     $20$ & $19.4$ & $13.8$ & $31.6$ \\
%     $25$ & $5.4$ & $2.4$ & $2.0$ \\
%     $30$ & $3.0$ & $1.8$ & $0.4$ \\
%     \bottomrule
%     \end{tabular}%
%   \label{tab:trigger_ratio}%
%   \end{center}
% \end{table}%


%  \begin{figure}
%   \centering
%    \includegraphics[width=2in]{figures/trigger.pdf}
%    \caption{Reversed image pattern. The input containing a trojan trigger is fed into a trojaned encoder, while the second clean input is passed through a benign encoder. The trojaned encoder generates a reversed pattern that is smaller in size and aligns with the location of the trojan trigger. In contrast, the benign encoder produces a larger pattern that is similar to the input feature of the clean input. }
%    \label{fig:trigger}
% \end{figure}



% \section{Results on the SSL-Cleanse Mitigator} \label{result:M}
% After the defender uses the detector to identify the trojaned encoder, the next step is to eliminate the trojan using the mitigator. Table~\ref{t:mitigator} illustrates the performance of the mitigation approach against different attacks on the ImageNet-100 dataset by comparing the attack success rate (ASR) and clean accuracy before and after applying the mitigator. 

% To mitigate the effects of the backdoor attack, we used the same dataset ratio for the mitigator as used for the detector. Our results indicate that for all three attacks, the clean accuracy before and after mitigation remains almost the same. Furthermore, our mitigator effectively reduces ASR to less than $2\%$, indicating that it can successfully remove all trojans. These results hold even with the smallest dataset used in our experiments. Therefore, we conclude that our mitigator is highly effective in removing the backdoor attack from the trojaned encoder.

% %\begin{table}[ht!]
%   %\caption{Performance of mitigation against different attacks on ImgeNet-100}
%   %\footnotesize
%   %\setlength{\tabcolsep}{8pt}
%   %\begin{tabular}{ccccc}\toprule
% %\multirow{2}{*}{Models} & \multicolumn{2}{c}{Before mitigator}  & \multicolumn{2}{c}{With mitigator}  \\
% %\cmidrule(lr){2-3} \cmidrule(lr){4-5}
%        % & ACC(\%) & ASR(\%) & ACC(\%) & ASR(\%)\\\midrule
%        % SSLBackdoor & $60.6$ & $33.2$ & $60.2$ & $0.14$ \\
%        % ESTAS & $62.7$ & $98.5$ & $62.0$ & $0.22$ \\
%        % CTRL & $53.3$ & $42.1$ & $52.7$ & $0.35$ \\
%        % \bottomrule
%    % \end{tabular}%
%   %\label{tab:mitigation}%
% %\end{table}%

\section{Experimental Methodology} \label{method}

\textbf{Dataset}. Our experiments were conducted on three benchmark datasets: CIFAR-10~\cite{cifar10}, CIFAR-100~\cite{cifar100}, and ImageNet-100~\cite{imagenet}. CIFAR-10 comprises $50,000$ $32\times32$ training images divided into 10 classes. CIFAR-100 is similar to CIFAR-10, with 500 training images for each of its 100 classes. ImageNet-100 is a random subset of 100 classes from the larger ImageNet dataset, and contains around 127,000 images in the training set.

\textbf{Encoder construction}. To assess the effectiveness of our detector against various attack methods, we evaluated it against three backdoor attacks, namely SSLBackdoor~\cite{saha2022backdoor}, CTRL~\cite{li2022demystifying}, and ESTAS~\cite{xue2022estas}. We created 50 benign encoders and 50 Trojaned encoders for each backdoor attack. ResNet-18~\cite{resnet} combined with a two-layer MLP is chosen as the encoder architecture. The MLP is used to project outputs into a low-dimensional latent space, i.e., 64-dimensional for CIFAR-10 and CIFAR-100, and 128-dimensional for ImageNet-100.

\textbf{Experimental Settings}. Our experiments were carried out using two Nvidia GeForce RTX-3090 GPUs, each with a memory capacity of 24GB. The batch size for both the detector and the mitigator was set to 64, and the learning rate was set to 0.01 for the detector and 0.001 for the mitigator. For the detector, the initial value of $\lambda$ was $0.01$. We adopted the triggers from the Hidden Trigger Backdoor Attacks~\cite{Saha_Subramanya_Pirsiavash_2020}. The trigger size was $50\times50$ for ImageNet-100 and $6\times6$ for CIFAR-100 and CIFAR-10.


\textbf{Evaluation Metrics}. We define the following evaluation metrics to study the efficiency, and effectiveness of our SSL-Cleanse. Attack Success Rate (\textbf{ASR}) is defined as the ratio of images that contain the trigger and are classified as the target class, to the total number of evaluated images. \textbf{TP}, \textbf{FP}, \textbf{TN}, and \textbf{FN} denote the numbers of true positives, false positives, true negatives, and false negatives, respectively. 
False Positive Rate (\textbf{FPR}) is the proportion of benign encoders predicted by the detector to be trojaned; $FPR = FP/(FP+TN)$. False Negative Rate (\textbf{FNR}): the proportion of trojaned encoders predicted by the detector to be clean; $FNR = FN/(FN+TP)$. 




\section{Results on the SSL-Cleanse Detector} \label{result-D}
We present the performance results of our detector with various ratios of unlabeled training datasets across three backdoor attacks in Table~\ref{t:detector}.

\subsection{Different Training Dataset Sizes} 

We investigate the impact of a varying training dataset size on the detection performance. Specifically, we use different proportions of images in the training set (dataset ratio), i.e., $5\%$, $8\%$, and $10\%$. It is estimated that it will take 20 minutes to generate the cluster numbers using the SWK method for $10\%$ of the ImageNet-100 training data. Table \ref{t:detector} demonstrates that the SWK method provides more accurate clustering results for larger datasets. Notably, both CIFAR-10 and ImageNet-100 achieve accurate clustering results with only $8\%$ of the training dataset (highlighted in green). Furthermore, the detection performance increases with an enlarging proportion of the training dataset. Even when the number of clusters exceeds the actual cluster numbers, more training data still result in a better detection rate.


%Specifically, in the case of the CIFAR-10 task, the predicted cluster number was 8 or 9 with 5\% of the training set, and 10 with 8\% of the training set. Similarly, for CIFAR-100, the predicted cluster number was 65 with 5\% of the training set, and 90 or 95 with 10\% of the training set. Finally, in the case of ImageNet-100, the predicted cluster number was 90 with 5\% of the training set, and 100 or 105 with 10\% of the training set.

%Specifically, the authors 




\subsection{SSLBackdoor} 

SSLBackdoor~\cite{saha2022backdoor} is a data-poisoning-based backdoor attack against SSL encoders. It randomly selects 50\% of the images from a specified category, and applies a small trigger on a random location to poison the training dataset. The trigger pattern representation is linked with the target-class inputs through the alignment of positive pairs of trigger inputs. In order to assess the efficacy of SSL-Cleanse, we generate a poisoned dataset using SSLBackdoor and train Trojan encoders by maximizing the similarity of data representations between the trigger inputs and the target class inputs. The detection results on SSLBackdoor are shown in Table \ref{t:detector}, where 50 benign encoders and 50 Trojaned encoders across diverse datasets are involved. 

\textbf{CIFAR-10}. When the dataset size increases from $5\%$ to $8\%$, the detection accuracy increases from $75\%$ to $80\%$, and the FNR decreases.

\textbf{CIFAR-100}. The training dataset of CIFAR-100 has only 500 images per class. Moreover, the dataset is plagued by label noises. So it is difficult to achieve high accuracy on CIFAR-100. Considering noisy labels, an SSL encoder~\cite{noise} achieves only $63.5\%$ in the image classification on CIFAR-100. SSL-Cleanse is able to achieve a detection rate of $64\%$ using 10\% of the available training data. The FPR and FNR are both high.

\textbf{ImageNet-100}. Despite the fact that ImageNet-100 has the same number of classes as CIFAR-100, SSL-Cleanse achieves a detection rate of $80\%$. This is because the training dataset of ImageNet-100 is substantially larger.


%This outcome is noteworthy given that the images in ImageNet-100 are of superior quality and




\subsection{ESTAS} 
%we used the consistent trigger poisoned dataset proposed by ESTAS~\cite{xue2022estas}, which adds the trigger after data augmentation, to train the Trojan encoder.
ESTAS~\cite{xue2022estas} is another data-poisoning-based backdoor attack. It generates three augmentations of one target class input, and adds a trigger on top of one augmentation. To optimize the attack, cascade optimization is used to increase the similarities between the triggered augmentation and one target-class sample. At the same time, the remaining two augmentations of the same input are placed closer together to maintain clean accuracy. Moreover, ESTAS adds the trigger after data augmentation, so that the trigger pattern is consistent with the inference stage. ESTAS stably achieves a $> 98\%$ attack success rate with just one target-class sample. Table \ref{t:detector} displays the outcomes of our analysis on different datasets, using 50 benign encoders and 50 Trojan encoders.

\textbf{CIFAR-10}. 
With $8\%$ of the training dataset, the detection rate is $87\%$. Increasing the training dataset to $10\%$ results in 11 clusters, but still can obtain an $86\%$ detection rate.

\textbf{CIFAR-100}. Even with only $5\%$ of the training dataset, we still achieve a detection accuracy of $67\%$. The higher detection rate over SSLBackdoor is caused by the trigger design of ESTAS is more specialized. For CIFAR-100, a training dataset size increase from $5\%$ to $10\%$ yields a large detection accuracy improvement from $67\%$ to $78\%$. So a larger training dataset is crucial for CIFAR-100 or other noisy datasets. 

\textbf{ImageNet-100}. With $5\%$ of the training dataset, we achieve a detection accuracy of $85\%$ on ImageNet-100. When the training dataset size enlarges, the FPR and FNR decrease accordingly.



\subsection{CTRL}  
CTRL~\cite{li2022demystifying} uses an invisible global trigger~\cite{wang2022invisible} in the spectral space of inputs that is preserved after data augmentation and is consistent with the inference phase. We incorporate the stealthy global trigger to verify the performance of our detector. Table \ref{t:detector} displays the performance outcomes of our detection method using 50 benign encoders and 50 Trojan encoders on various datasets.

\textbf{CIFAR-10}. At training dataset sizes of $8\%$ and $10\%$, the number of clusters identified equals the actual number of classes, which is 10. Furthermore, as the training dataset size is increased, the detection accuracy improves and reaches $82\%$ with low FPR and FNR.

\textbf{CIFAR-100}. As the number of training datasets increases, the detection accuracy improves to $65\%$ and FNR decreases, although it remains relatively high.  

\textbf{ImageNet-100}. When the training dataset sizes are $8\%$ and $10\%$, the number of clusters detected matches the actual number of classes, which is 100. Moreover, as the size of the training dataset is increased, the detection accuracy improves and reaches $85\%$, with a low FNR. However, it should be noted that the FPR increases as more training data is used.



%\subsection{Tradeoff between FPR and FNR.}
%We calculate the false positive rate (FPR) and false negative rate (FNR) when setting different \textit{anomaly index} for the outlier to control its sensitivity to abnormal data points. Then, we get multiple FPR-FNR pairs, which can help users balance the numbers of false positives and false negatives during real-world deployment. Results in Figure \ref{fig:FNR_FPR} show that we achieve powerful filtering performance for three types of attacks on different datasets, i.e., obtaining $6.33\%$ FNR at an FPR of $5\%$. %Not surprisingly, SSL-Backdoor encoders are more difficult to filter out, because its attack success rate is the lowest.

%\begin{figure}[tb]
 % \centering
   %\includegraphics[width=\linewidth]{figures/FNR.pdf}
   %\caption{False negative rate of Trojan encoder detection when achieving different false positive rate.}
  % \label{fig:FNR_FPR}
%\end{figure} 

\subsection{Various Trigger Sizes}

We conducted experiments on trojaned encoders attacked by SSLBackdoor using triggers of varying sizes to investigate the impact of trigger size on the detector's performance. The trigger sizes ranged from $1\%$ to $30\%$. Table \ref{tab:trigger_ratio} presents the results, indicating that our detector performs well when the trigger size ratio is below $5\%$, achieving approximately $80\%$ detection accuracy on CIFAR-10 and ImageNet-100. Even with a $10\%$ trigger, we can still achieve relatively stable detection results. However, as the trigger size increases, the detection accuracy decreases. This could be attributed to the reversed trigger also becoming larger, making it more challenging for the outlier detector to distinguish abnormal triggers.

Additionally, Figure~\ref{fig:trigger} demonstrates that for a poisoned image via the representation-oriented reverse pattern technique, it produces a very small pattern within the trojaned encoder that aligns with the trigger, while a clean input will produce a large pattern with similar features to the input. Therefore, if the trigger size is very large, it will be challenging to distinguish between poisoned images and clean images.



%($3.2\times3.2$ for CIFAR-10 and CIFAR-100; $22.5\times22.5$ for ImageNet-100)

%($17.5\times17.5$ for CIFAR-10 and CIFAR-100; $123\times123$ for ImageNet-100)









\begin{table}[t!]
  \caption{The influence of trigger size on detection performance across varying datasets.}
  \footnotesize
  \setlength{\tabcolsep}{4pt}
  \begin{center}
    \begin{tabular}{cccc}\toprule
    Trigger Ratio(\%) & CIFAR-10(\%) & CIFAR-100(\%) & ImageNet-100(\%) \\
    \midrule
    $1$ & $84.$ & $67.2$ & $83.2$ \\
    $3$ & $80.4$ & $65.0$ & $79.8$ \\
    $5$ & $80.0$ & $64.0$ & $80.0$ \\
    $10$ & $74.8$ & $57.0$ & $76.2$ \\
    $15$ & $36.4$ & $27.6$ & $61.6$ \\
    $20$ & $19.4$ & $13.8$ & $31.6$ \\
    $25$ & $5.4$ & $2.4$ & $2.0$ \\
    $30$ & $3.0$ & $1.8$ & $0.4$ \\
    \bottomrule
    \end{tabular}%
  \label{tab:trigger_ratio}%
  \end{center}
\end{table}%


 \begin{figure}
  \centering
   \includegraphics[width=2in]{figures/trigger.pdf}
   \caption{Reversed image pattern. The input containing a trojan trigger is fed into a trojaned encoder, while the second clean input is passed through a benign encoder. The trojaned encoder generates a reversed pattern that is smaller in size and aligns with the location of the trojan trigger. In contrast, the benign encoder produces a larger pattern that is similar to the input feature of the clean input. }
   \label{fig:trigger}
\end{figure}



\section{Results on the SSL-Cleanse Mitigator} \label{result:M}
After the defender uses the detector to identify the trojaned encoder, the next step is to eliminate the trojan using the mitigator. Table~\ref{t:mitigator} illustrates the performance of the mitigation approach against different attacks on the ImageNet-100 dataset by comparing the attack success rate (ASR) and clean accuracy before and after applying the mitigator. 

To mitigate the effects of the backdoor attack, we used the same dataset ratio for the mitigator as used for the detector. Our results indicate that for all three attacks, the clean accuracy before and after mitigation remains almost the same. Furthermore, our mitigator effectively reduces ASR to less than $2\%$, indicating that it can successfully remove all trojans. These results hold even with the smallest dataset used in our experiments. Therefore, we conclude that our mitigator is highly effective in removing the backdoor attack from the trojaned encoder.

%\begin{table}[ht!]
  %\caption{Performance of mitigation against different attacks on ImgeNet-100}
  %\footnotesize
  %\setlength{\tabcolsep}{8pt}
  %\begin{tabular}{ccccc}\toprule
%\multirow{2}{*}{Models} & \multicolumn{2}{c}{Before mitigator}  & \multicolumn{2}{c}{With mitigator}  \\
%\cmidrule(lr){2-3} \cmidrule(lr){4-5}
       % & ACC(\%) & ASR(\%) & ACC(\%) & ASR(\%)\\\midrule
       % SSLBackdoor & $60.6$ & $33.2$ & $60.2$ & $0.14$ \\
       % ESTAS & $62.7$ & $98.5$ & $62.0$ & $0.22$ \\
       % CTRL & $53.3$ & $42.1$ & $52.7$ & $0.35$ \\
       % \bottomrule
   % \end{tabular}%
  %\label{tab:mitigation}%
%\end{table}%

 \section{Conclusion} \label{conclusion}
This paper introduces \textit{SSL-Cleanse}, a novel work to detect and mitigate Trojan attacks in SSL encoders without accessing any downstream labels. We reveal it is possible to prevent the dissemination of Trojaned SSL encoders by using our SSL-Cleanse. %Detecting Trojans in SSL encoders is a challenging task due to the fact that downstream tasks are often unknown, dataset labels may not be available, and even the original training dataset may be partially or entirely inaccessible. 
In the detector, we propose a SWK to produce $K$ clusters based on the representations of the SSL encoder. We then introduce the RORP method to generate a reversed pattern for each cluster. By utilizing these $K$ patterns, we can perform outlier detection to determine if the SSL encoder is trojaned. In the mitigator, we introduce a method called TEM, which is designed to efficiently eliminate the trojans present in the backdoored SSL encoders.  We evaluated SSL-Cleanse on various datasets using 300 models, achieving an average detection success rate of $83.7\%$ on ImageNet-100. After mitigating backdoors, on average, backdoored encoders achieve $0.24\%$ attack success rate without great accuracy loss, proving the effectiveness of SSL-Cleanse. 




%\begin{figure}
 % \centering
  % \includegraphics[width=0.5\linewidth]{figures/ablation.pdf}
  % \caption{ Trigger size .}
   %\label{fig:size}
%\end{figure}



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
