\section{Related Work}\label{sec:related-work}
\subsection{OOD Detection with Extra Data}
The mainstream OOD detection works primarily focus on detecting samples with semantic shifts, \textit{i.e.}OOD samples drawn from classes that do not overlap with known classes. To enhance the ability to understand ID/OOD discrepancy, some methods synthesize OOD training data such as \cite{lee2018training,sricharan2018building,vernekar2019out,duvos}, and another series of OOD detection methods introduce extra OOD data into training. OE~\cite{hendrycks18oe} uses purified unlabeled OOD data, which is encouraged to produce a uniform softmax distribution during training. MCD~\cite{mcd} maximizes the entropy discrepancy between ID and extra OOD samples outputted by two parallel classifiers. The survey~\cite{yang2021generalized} indicates that OOD detection with external data generally performs better. However, massive unlabeled samples could be very easily obtained considering their accessibility, and ID data would inevitably exist among them. Hence, `purified unlabeled OOD data' means the mixed ID data need to be manually removed, which brings a high cost and is impractical. 

Against the above issues, UDG~\cite{yang2021semantically} proposes a realistic benchmark that retains ID samples in the unlabeled set, namely SCOOD (Semantically Coherent Out-of-Distribution Detection) benchmarks. UDG separates the ID/OOD samples in the unlabeled dataset based on K-means clustering, and adds the ID samples to the labeled samples according to semantics for training together. During training, UDG focuses on the semantic shifts between samples, which is a fundamental problem to be solved in OOD detection tasks, and well explores the semantic knowledge in the unlabeled set. UDG underlines that the OOD detection task should concentrate on semantic shifts, but in experiments we found that the clustering method cannot sufficiently collect unlabeled ID samples and can only assign less than 1$\%$ correct semantic labels to unlabeled ID samples at each epoch. In this paper, we propose an uncertainty-aware optimal transport scheme to achieve more accurate label assignment and learn a more significant difference between ID and OOD semantics.

\subsection{Optimal Transport}
wGAN~\cite{wGAN} applies optimal transport (OT) theory to the field of computer vision and has received extensive attention. Its basic idea is to minimize the Wasserstein distance between the distribution of sampled data and the image distribution synthesized by deep generative models. In summary, OT is to transform one distribution into another with minimal cost (\textit{e.g.,} Wasserstein distance), so it is applied to distribution matching tasks such as few-shot classification~\cite{guo2022adaptive} and domain adaptation~\cite{turrisi2022multi,li2020mutual}. In close-set unsupervised classification tasks, OT can assign pseudo-labels for unlabeled samples. SeLa~\cite{asano2019self} implements equipartition constraint on the label assignment matrix $
\mathbf{Q}$, and introduces KL divergence as a regularization term into the optimization objective, then uses a fast version of the Sinkhorn-Knopp~\cite{cuturi2013sinkhorn} algorithm to assign labels. SwAV~\cite{caron2020unsupervised} keeps the soft-label assignment produced by the Sinkhorn-Knopp algorithm without approximating it into a one-hot assignment like SeLa. For the SCOOD task, we adopt an individual OT strategy called energy-based transport (ET) mechanism, which introduces the energy score as the transport cost to guide the cluster distribution of samples.