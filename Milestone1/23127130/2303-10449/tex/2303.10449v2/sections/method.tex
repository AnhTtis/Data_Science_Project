\section{Method}\label{sec:method}

\subsection{Problem Setting}

In this section, we will introduce the setting of SCOOD task. Firstly, we denote in-distribution and out-of-distribution set as $I$ and $O$, respectively. The training set $D$ on SCOOD benchmarks consists of labeled training set $D_L$ and unlabeled training set $D_U$. While the samples contained in $D_L$ are all from $I$, the $D_U$ is a mixture of partial ID set $D^I_U$ from $I$ and OOD set $D^O_U$ from $O$, which is unlike previous OOD benchmarks. So the training set can be represented as $D=(D_L\subset I)\cup(D^I_U\subset I)\cup(D^O_U\subset O)$. Likewise, the test set $T$ comprises $T^I\subset I$ and $T^O\subset O$. We desire to assign correct ID labels to as many samples in $D^I_U$ as possible during training, and identify data from $T^O$ as negative (OOD) samples and classify samples from $T^I$ correctly at test time.


\subsection{Overall Pipeline}  
 As shown in Fig. \ref{fig:model}, we design a novel uncertainty-aware optimal transport (OT) scheme, which consists of an energy-based transport (ET) mechanism and an inter-cluster extension strategy $ L_{rep}$ to assign correct ID class label to partial unlabeled ID samples and then involve them into the joint training. The ET introduces the energy score as uncertainty and estimates the uncertainty-based transport cost to guide the cluster distribution of all samples. To further promote the discrimination in logit space, from which the uncertainty can significantly reflect the difference between ID/OOD samples, the inter-cluster extension strategy enhances the global feature representation mixed with ID and OOD samples and then the enhanced representation will be mapped into more discriminate logits.
 
  \begin{figure}[t]
  \centering
\includegraphics[width=0.98\linewidth]{fig/model.pdf}
  \caption{\textbf{Overall framework of our uncertainty-aware optimal transport scheme.} The energy scores for both ID and OOD samples are firstly calculated from output logit and then utilized to estimate the energy-based transport cost, guiding the assignment of unlabeled ID samples during the Sinkhorn-Knopp iteration. Finally, the energy scores will be further enhanced by the unlabeled samples obtaining correct ID labels and the inter-cluster extension strategy, facilitating subsequent assignment and classification.}
  \label{fig:model}
  \vspace{-0.28em}
\end{figure}
 
\subsection{Energy-Based Transport Mechanism}
Unlike assigning labels to unlabeled samples in the close-set classification tasks, the situation we face is more challenging because we have to overcome the interference of OOD samples when assigning labels. To address it, we propose an energy-based transport (ET) mechanism based on logit space to explore the semantic discrepancy and knowledge hidden in the unlabeled set more sufficiently. 

Specifically, we add a $K$-dimensional OT head $h_{ot}$ in parallel with the classification head $h_{cls}$ to map training samples to $K$ clusters. Given the set of all $N$ samples $\mathcal{X}= \{x_1,x_2,\dots,x_N\}$ and the set of their feature extracted by encoder $\mathcal{Z}= \{z_1,z_2,\dots,z_N\}$, we denote the probability of the $i\mbox{-}th$ sample over all clusters as $p(\mathbf{c}|x_i)=softmax(h_{ot}(z_i))$, where $\mathbf{c}$ is a $K$-dimension vector representing $K$ clusters. Then we can define the cost matrix in optimal transport problem as $\mathbf{P}\in\mathbb{R}^{K\times N}$ and $P_{ji}=p(c_j|x_i)$ denotes the probability of $x_i$ belonging to $c_j$, where $c_j$ is the $j\mbox{-}th$ cluster. Likewise, we represent $\mathbf{Q}\in\mathbb{R}^{K\times N}$ as the assignment matrix and $Q_{ji}=q(c_j|x_i)$ denotes the posterior probability of $x_i$ assigned to $c_j$. Note that the assignment matrix $\mathbf{Q}$ just represents the assignment of a certain cluster to each sample, rather than directly assigning labels for classification training. When we represent the distributions of $N$ samples and $K$ clusters using an $N$-dimension vector $\boldsymbol{\beta}$ and a $K$-dimensional vector $\boldsymbol{\alpha}$, respectively, all feasible solutions of assignment matrix $\mathbf{Q}$ in \emph{transportation polytope}~\cite{cuturi2013sinkhorn} can be formulated as:
\begin{equation}
\scalebox{0.982}{$U(\boldsymbol{\alpha}, \boldsymbol{\beta}):=\left\{\mathbf{Q} \in \mathbb{R}^{K \times N} \mid \mathbf{Q} \mathbf{1}_N=\boldsymbol{\alpha}, \mathbf{Q}^{\top} \mathbf{1}_K=\boldsymbol{\beta}\right\}$},
\end{equation}
where $\mathbf{1}$ is an all-ones vector of corresponding dimension, and $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$ are the marginal projections of matrix $\mathbf{Q}$ onto its rows and columns, respectively.

Energy score from logit space is an effective and readily available metric to distinguish ID/OOD samples~\cite{energyood}, so we introduce the energy $\mathbf{e}\in\mathbb{R}^N$ as uncertainty guidance into our ET module. The energy of $x_i$ distributed across all clusters is shown as:
\begin{equation}\label{e:prior-energy} \\[-0.5ex]
e_i=\log\sum_{j=1}^K e^{l(c_j|x_i)}, 
\end{equation}
where $l(\mathbf{c}|x_i)=h_{ot}(z_i)$ denotes the logit of $x_i$ belonging to $c_j$. The energy $\mathbf{e}$ reflects the aggregate probability distribution of samples across clusters. Afterward, ET will encourage samples with higher energy scores to be assigned to the same cluster, while the ones with lower energy scores, which means they have larger uncertainty in the cluster distribution, will tend to be split uniformly among the $K$ clusters. And the matrix $\mathbf{Q}$ based on $\mathbf{e}$ can be denoted as:
\begin{equation}\label{e:r-c}
\boldsymbol{\alpha} = \frac{1}{K}\cdot\mathbf{1}_K,
\quad
\boldsymbol{\beta} = \frac{\mathbf{e}}{\sum_{i=1}^N e_i}.
\end{equation}
And the energy transport cost can be written as:  
\begin{equation}\label{e:energy_cost} 
\mathbf{P_{en}} = \mathbf{P}\cdot \mathbf{e} \in \mathbb{R} ^ {K \times N},
\end{equation}
where  $\mathbf{e}$ is first broadcast into a $K\times N$ matrix and then multiplied element-wise by $\mathbf{P}$. Hence, the Wasserstein distance between $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$ is defined as $\text{OT}(\boldsymbol{\alpha}, \boldsymbol{\beta})=\min _{\mathbf{Q} \in \Pi(\boldsymbol{\alpha}, \boldsymbol{\beta})}-{\langle \mathbf{Q}, \mathbf{P_{en}} \rangle}$, where $\langle\cdot, \cdot\rangle$ denotes the Frobenius dot-product. In this case, it is worth noting that $\mathbf{P_{en}}$ cannot be naively regarded as the conventional cost matrix in the optimal transport problem, since the elements in $\mathbf{P_{en}}$ represent the probability of a sample being assigned to a certain cluster. The larger the element value, the lower the cost of transferring the sample to the corresponding cluster.
To avoid solving this linear programming problem which needs a large computational cost, we introduce the entropic regularization term $\textit{H}(\mathbf{Q})$ into Wasserstein distance~\cite{cuturi2013sinkhorn} and express the optimization problem as:
\begin{equation}\label{e:entropic regularization}
   \text{OT}(\boldsymbol{\alpha}, \boldsymbol{\beta})=\min _{\mathbf{Q} \in \Pi(\boldsymbol{\alpha}, \boldsymbol{\beta})} -{\langle\ \mathbf{Q}, \mathbf{P_{en}}\rangle}+\varepsilon\textit{H}(\mathbf{Q}),
\end{equation}
where $\varepsilon >0$ and $\textit{H}(\mathbf{Q})=\sum_{ji}Q_{ji}\log Q_{ji}$. In this way, the optimal $\mathbf{Q}$ has been shown to be written as:
\begin{equation}\label{e:solution} 
\mathbf{Q}^{\frac{1}{\varepsilon}} =\text{Diag}(\mathbf{u})
        \mathbf{P_{en}}
        \text{Diag}(\mathbf{v}),
\end{equation}        
where exponentiation is element-wise, and $\mathbf{u}$ and $\mathbf{v}$ can be solved much faster by Sinkhornâ€™s Algorithm~\cite{cuturi2013sinkhorn}. The assignment matrix $\mathbf{Q}$ maps $N$ samples onto $K$ clusters.

Since then, we have obtained the cluster indexes of
all samples $\mathcal{C} = \{c_1,c_2,\dots,c_N\}\in \{1,2,\dots,K\}$. We collect unlabeled ID samples and assign pseudo labels to them according to the proportion of each label in a cluster \cite{yang2021semantically}. Specifically, we let samples that belong to $k$-th cluster at the $t$-th epoch form the set $D_k$ as denoted in ~\cref{e:Dk}.
\begin{equation}\label{e:Dk} 
    D_k^{(t)}=\{x_i\vert c^{(t)}_i=k\}.
	\end{equation}

 At epoch $t$, we define the set of ID-class labels $\mathcal{Y}^{(t)} = \mathcal{Y}_L \cup \mathcal{Y}_{ot}^{(t)} \in \{0,1,\dots,M-1\}$, where $M$ is the number of ID classes, and $\mathcal{Y}_L$ is formed by the ground-truth labels from $D_L$, while the ID pseudo labels assigned to unlabeled samples by ET constitute $\mathcal{Y}_{ot}^{(t)}$. Note that $\mathcal{Y}_{ot}^{(t)} = \varnothing$ when $t=0$ and it will be updated during training, while $\mathcal{Y}_L$ is unchanged.  Then we calculate the proportion of samples belonging to class $y \in \mathcal{Y}^{(t)}$ in cluster $k$ as follows:
\begin{equation}\label{E:class_rate} 
rate_{k,y}^{(t)}=\frac{\lvert D_{k,y}^{(t)}=\{x_i\vert c^{(t)}_i=k, y_i=y\}\rvert}{\lvert D_{k}^{(t)}\rvert}.
\end{equation}
When $rate_{k,c}^{(t)}$ is over a threshold $\tau$, all unlabeled samples in cluster $k$ will be added into labeled set $D_L$ to form the updated $D_{L}^{(t)}$ denoted as \cref{E:new_ind}, and $\mathcal{Y}^{(t)}$ is composed of the labels from $D_{L}^{(t)}$. 
\begin{equation}\label{E:new_ind}
	D_{L}^{(t)}= D_{L} \cup \{x_i \vert x_i \in D_k^{(t)}, rate_{k,y}^{(t)} > \tau\}.
\end{equation}
Meanwhile, the remaining unlabeled samples form $D_U$ at epoch $t$ is denoted as $D_{U}^{(t)}$. With the updated training set, the classification loss $L_{cls}^{(t)}$ and equalization loss  $ L_{unif}^{(t)}$ can be written as:
\begin{equation}\begin{split}
L_{cls}^{(t)} = -\sum_{x_i\in \mathcal{D}^{(t)}_L }\sum_{y_i\in \mathcal{Y}^{(t)}} 
 y_i \cdot \log (p(\mathbf{y}|x_i)), \\
L_{unif}^{(t)} = -\sum_{x_i\in\mathcal{D}^{(t)}_U} \frac{\mathbf{1}_M}{M} \cdot \log (p(\mathbf{y}|x_i)),
\end{split}\end{equation}
% \begin{equation} \label{e:cls_loss}
% L_{cls}^{(t)} = - \frac{1}{\lvert D^{(t)}_L \rvert} 
% \sum_{(x_i,y_i)\in\mathcal{D}^{(t)}_L} \log ( p_{y_i}(y|x_i) ),
% \end{equation}
% \begin{equation}\label{e:unif_loss}
% L_{unif}^{(t)} = - \frac{1}{\lvert D^{(t)}_U \rvert} \frac{1}{\lvert C_I \rvert} 
% \sum_{x_i\in\mathcal{D}^{(t)}_U} \sum_{y\in C_I} \log ( p(y|x_i) ).
% \end{equation}
where $p(\mathbf{y}|x_i)$ means the prediction probability of $x_i$ over all ID classes, and $y_i$ should be converted to a one-hot vector denoting the corresponding label of $x_i$. Moreover, $\frac{\mathbf{1}_M}{M}$ denotes the uniform posterior distribution over all of $M$ ID classes and $L_{unif}^{(t)}$ will force samples in $D_{U}^{(t)}$ to uniformly distribute among $M$ ID classes~\cite{hendrycks18oe}.

\subsection{Inter-Cluster Extension Strategy}
Due to the fact that supervised learning only produces semantic representations that meet the minimum necessary for classification~\cite{con_ood}, to widen the discrepancy between ID/OOD on energy metric and thus strengthen the ability of energy transport cost in \cref{e:energy_cost} to guide ID/OOD samples to different clusters, we desire to obtain an improved feature representation by conducting an unsupervised training module called inter-cluster extension strategy.
 
With labels unavailable, given a batch of data $\{x_{(l,i)}\}_{i=1\dots B_1}$ and $\{x_{(u,i)}\}_{i=1\dots B_2}$ in $D_L$ and $D_U$ respectively, we combine them to form a batch of joint data $\{x_i\}_{i=1\dots B}=\{x_{(l,i)}\} \cup \{x_{(u,i)}\}$, where  $B=B_1+B_2$, Then we obtain two augmentation versions of them denoted as $x_i^0=A^0(x_i)$ and $x_i^1=A^1(x_i)$ through two kinds of data augmentations $A^0$ and $A^1$, respectively. Then the augmented data will be fed into an encoder to be extracted feature as:
\begin{equation}\label{e:rep}
\begin{split}
    z_i^0 = h_m(e_\theta^0(x_i^0)), \\
    z_i^1 = h_m(e_\theta^1(x_i^1)),
\end{split}    
\end{equation} 
where $e_\theta^0$ and $e_\theta^1$ are two encoders with parameters $\theta$, and $h_m$ is an MLP. Additionally, a memory queue is created to store $z_i^1$ at different iterations and the $n$ latest batches of $z_i^1$ compose a dynamic queue. The dynamic queue at iteration $t'$ is described as: 
\begin{equation}\label{e:queue}
    Z^{1,(t')} = \{z_i^1\}^{(t')} \cup \{z_i^1\}^{(t'-1)}  {\dots}  \cup\{z_i^1\}^{(t'-n+1)}.
\end{equation}
Considering the absence of labels, we obtain the desired representation by maximizing the cosine similarity of the same samples in $\{z_i^0\}^{(t')}$ and $\{z_i^1\}^{(t')}$, so we employ InfoNCE loss as the objective:
\begin{equation}
L_{rep}=-\sum_{i=1}^N \log \frac{\exp(cos(z_i^0, z_i^1))}{\sum_{j=1}^{n \cdot B}\exp(cos(z_i^0, z_j))},
\end{equation}
where $z_j$$\in$$Z^{1,(t')}$ and $cos$ denotes cosine similarity. The enhanced representation obtained by $L_{rep}$ will be mapped to the lower-dimensional logit space, where we can obtain the class-specific energy score better reflecting the semantic discrepancy of samples from different classes and assign a more discriminative cluster distribution to them.

Our overall objective can be expressed as~\cref{e:overall_loss} with the wights $\gamma$ and $\lambda$.
\begin{equation}\label{e:overall_loss}
L = L_{cls}^{(t)} + \gamma L_{unif}^{(t)} + \lambda L_{rep}.
\end{equation}


\subsection{T-Energy Score}\label{subsec: OOD metric}
Considering the parallel structure of $h_{ot}$ and $h_{cls}$, which both map the same features to the logit space through different fully-connected layers, there is a certain degree of correlation between the two logits. So we use the energy score from the logit $l(\mathbf{y}|x_i)=h_{cls}(z_i)$ outputted by $h_{cls}$ as OOD score to detect OOD samples after training. The energy can be readily calculated through the \verb'logsumexp'\ operator as $energy(x_i)=\log\sum_{m=1}^M e^{l(y_m|x_i)}$, where $l(y_m|x_i)$ is the logit of $x_i$ belonging to the ID class $y_m$, and ID samples have higher energy score. We observe that OOD samples are more concentrated at minimum energy compared to methods training with only ID samples (\textit{e.g.}, \cite{energyood}) due to the involvement of training OOD data in SCOOD. In this case, a large temperature value $T$ can smooth the energy distribution and thereby widen the discrepancy between ID/OOD on the energy metric.
Therefore, we implement the temperature scaling proposed in ODIN~\cite{odin} to compute energy score using temperature-scaled logits instead of mapping them to softmax score like ODIN, which is consistent with the source of the uncertainty metric in the proposed ET. The T-energy score is formulated as:
\begin{equation}\label{e:T-energy}
T \mbox{-} energy(x_i)
=
T \cdot \log\sum_{m=1}^M e^{l(y_m|x_i) / T}.
\end{equation}
