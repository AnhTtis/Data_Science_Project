\section{Introduction}\label{sec:intro}

\begin{figure}[t]
\centering
\begin{overpic}[width=0.99\linewidth]{fig/fig1.pdf}
  \put(27, 0.2){\textbf{\footnotesize{Number of Accurately Assigned Labels}}}
  \put(0.3, 22){\rotatebox{90}{\footnotesize\textbf{FPR@95 (\%)}}}
\end{overpic}
% \setlength{\abovecaptionskip}{-0.05cm} 
\vspace{-0.2em}
\caption{\textbf{The effect of the number of accurately assigned labels.} We gradually increase the ground-truth labels of unlabeled ID samples during the baseline~\cite{hendrycks18oe} training and report the FPR@95$\%$ (lower value is better) results averaged by multiple tests. The trend shows that more accurately assigned labels
enhance the OOD detection capability of the model.}
\vspace{-10pt}
\label{fig:Fig1}
\end{figure}

Deep learning models trained in close-set world often suffer from performance degradation in real-world scenarios due to the interference of out-of-distribution (OOD) inputs---unknown samples whose classes don't overlap with those already seen during training~\cite{zhou2022domain}. Full-supervised models tend to make overconfident predictions on OOD inputs~\cite{nguyen2015deep,scheirer2012toward}
, which severely limits the application in high-risk fields such as autonomous driving~\cite{autodriving} and medical analysis~\cite{schlegl2017unsupervised}. To this end, OOD detection~\cite{baseline,odin,mcd,energyood,yang2021semantically} aims to obtain a reliable model to identify these unknown samples as abnormal ones and reject them at test time. 

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.90\linewidth]{fig/motivation2.pdf}
  \vspace{-8pt}
  \caption{\textbf{Motivation of the proposed method}. When an extra unlabeled dataset is introduced into training, the resulting covariate shifts will easily confuse the minimum feature distance (\textit{e.g., Euclidean distance}) from the correct class mean with other wrong classes. The left histogram reflects the minimum Euclidean distance distribution from the class mean for the `dog' images in both CIFAR-10 (blue) and tiny-ImageNet (green). Unfortunately, although belonging to the same class, their feature distance distributions differ greatly, which limits the ability of cluster-based methods to mine unlabeled ID semantic knowledge in the SCOOD setting. In contrast, as shown in the right histogram, the distributions of the `dog' images in the two different datasets maintain good consistency on the energy metric, facilitating a more stable semantic assignment process.}
  \label{fig:motivation}
  \vspace{-5pt}
\end{figure*}

To learn the discrepancy between ID/OOD samples better, a rich line of OOD detection methods like OE~\cite{hendrycks18oe} and MCD~\cite{mcd} utilizing extra OOD data during training has been developed. OE flattens the prediction probability of OOD samples among all ID classes, and MCD maximizes the entropy discrepancy between ID and OOD samples outputted by two parallel classifiers. Although extra data greatly enhances the performance of models across traditional OOD benchmarks, these benchmarks still face two problems: \textbf{1)} to ensure the sampling effectiveness,  ID samples in extra unlabeled datasets are manually removed to obtain the pure OOD set, which does not correspond to reality since ID samples are inevitably mixed into the unlabeled set and expensive to be purified. \textbf{2)} the great improvement on them is most likely due to the overfitting on the training OOD samples~\cite{yang2021semantically}, so the model paying attention to the lower-level covariate shifts between datasets has difficulty generalizing to other OOD domains, especially when the scale of the dataset is not large.

To overcome these challenges, the SCOOD benchmarks \cite{yang2021semantically} are proposed to urge OOD detection models to focus on distinguishing semantic discrepancy between ID/OOD samples and avoid overfitting low-level covariate shifts on different data sources. K-means clustering is adopted to dynamically filter ID samples from the unlabeled set during training, enhancing the optimization for ID classification~\cite{yang2021semantically}. Unfortunately, we statistically observe that this strategy can only assign correct semantic labels to less than 1$\%$ unlabeled ID samples at each epoch, which means a large number of samples are added to the ID classification training with wrong labels and undoubtedly limits the ability of the model to fully understand the semantic discrepancy between ID/OOD samples.

% This limitation reflects there is still a large potential space to solve the fundamental challenge of OOD detection tasks.

To investigate the impact of accurately assigned semantic labels on SCOOD tasks, we gradually increase the ground-truth labels of unlabeled ID samples used in training with the baseline in \cite{yang2021semantically}, and then observe the variation trend of model performance shown in~\cref{fig:Fig1}. The model performance consistently improves as the correctly assigned labels increase, leading the key question of how to allocate as many ID-related samples in the unlabeled set correctly as possible. Our further analysis indicates that compared to the traditional feature distance (\textit{e.g., Euclidean distance}), the class-specific energy score remains consistent even when the assigned cluster contains shifted unlabeled samples with an ID class. As shown in \cref{fig:motivation}, due to the covariate shifts of the same class samples from another dataset with a black circle, the minimum distance with the correct class mean is easily confused with other clusters. In contrast, the energy-based uncertainty prior can effectively mitigate the corresponding interference owing to the intra-class interaction.


To effectively involve energy-based guidance in the assignment process, we propose an uncertainty-aware optimal transport scheme. This core energy-based transport (ET) mechanism estimates the energy transport cost to encourage lower-cost samples to be assigned to the same cluster and the higher-cost ones to be split uniformly among all clusters, leading to a semantic-consistent distribution. Specifically, the energy score is obtained by performing the aggregation operation in logit space.

To further enhance the assignment efficiency of the aforementioned transport mechanism, an inter-cluster extension strategy is proposed to narrow the margin of the same ID class while widening the margin between ID and OOD samples. An enhanced global feature representation will be mapped to the lower-dimensional logit space, from where we can obtain a more discriminative prediction distribution for each cluster and class-specific energy score better reflecting the semantic information of samples. By utilizing both of them, the semantics among samples assigned to different clusters by ET will be more discriminative.
During inference, we also choose the energy score with temperature scaling produced by the classifier as the detection metric, aligning with the transport optimization more closely. 


Our contributions are summarized as follows: \textbf{1)} We analyze the limitations of the clustering strategy for the SCOOD task and further explore how to address the fundamental challenge of OOD detection. \textbf{2)} We propose a novel uncertainty-aware optimal transport scheme to fully utilize the semantic consistency hidden in the unlabeled set, resulting in the covariate-invariant assignment. \textbf{3)} Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on SCOOD benchmarks.
 

