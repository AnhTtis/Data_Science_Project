
@ARTICLE{falseResearch,
	author = {John P. A. Ioannidis},
	title = {Why Most Published Research Findings Are False},
	journal = {PLoS Med},
	year = {2010},
	month = {August}
}



@ARTICLE{statistCrisis,
	author = {A. Gelman and E. Loken},
	title = {{T}he Statistical Crisis in Science},
	journal = {Am. Stat.},
	year = {2014},
	volume = {102},
	pages = {460}
}


@inproceedings{statValidity,
 author = {Dwork, C. and Feldman, V. and Hardt, M. and Pitassi, T. and Reingold, O. and Roth, A. L.},
 title = {Preserving Statistical Validity in Adaptive Data Analysis},
 booktitle = {Proceedings of the Forty-seventh Annual ACM Symposium on Theory of Computing},
 year = {2015},
 isbn = {978-1-4503-3536-2},
 location = {Portland, Oregon, USA},
 pages = {117--126},
 numpages = {10},
 doi = {10.1145/2746539.2746580},
 acmid = {2746580},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive data analysis, differential privacy, false discovery, generalization bounds, overfitting, statistical estimation}
} 
@inproceedings{genAdap,
	author = {Dwork, Cynthia and Feldman, Vitaly and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Roth, Aaron},
	title = {Generalization in Adaptive Data Analysis and Holdout Reuse},
	booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
	year = {2015},
	location = {Montreal, Canada},
	publisher = {MIT Pressf},
	address = {Cambridge, MA, USA},
} 


@article{stability,
 author = {Shalev-Shwartz, S. and Shamir, O. and Srebro, N. and Sridharan, K.},
 title = {Learnability, Stability and Uniform Convergence},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2010},
 volume = {11},
 year = {2010},
 issn = {1532-4435},
 pages = {2635--2670},
 numpages = {36},
 acmid = {1953019},
 publisher = {JMLR.org}
} 

@article{learningDp,
author = {Wang, Yu-Xiang and Lei, Jing and E. Fienberg, Stephen},
year = {2015},
month = {02},
pages = {},
title = {Learning with Differential Privacy: Stability, Learnability and the Sufficiency and Necessity of ERM Principle},
volume = {17}
}

@article{maxInfo,
	title={Max-Information, Differential Privacy, and Post-selection Hypothesis Testing},
	author={Ryan M. Rogers and Aaron Roth and Adam D. Smith and Om Dipakbhai Thakkar},
	journal={2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS)},
	year={2016},
	pages={487-494}
}

@inproceedings{learningMI,
  title = 	 {Learners that Use Little Information},
  author = 	 {R. Bassily and S. Moran and I. Nachum and J. Shafer and A. Yehudayoff},
  pages = 	 {25--55},
  year = 	 {2018},
  volume = 	 {83},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {},
  month = 	 {07--09 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v83/bassily18a/bassily18a.pdf}
}


@inproceedings{explBiasMI,
  title = 	 {Controlling Bias in Adaptive Data Analysis Using Information Theory},
  author = 	 {Daniel Russo and James Zou},
  booktitle = 	 {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1232--1240},
  year = 	 {2016},
  volume = 	 {51},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--11 May},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v51/russo16.pdf}
}

@INPROCEEDINGS{leakage,
author={I. Issa and S. Kamath and A. B. Wagner},
 booktitle={2016 Annual Conference on Information Science and Systems (CISS)}, 
title={An operational measure of information leakage}, 
year={2016},
volume={},
number={},
pages={234-239},
keywords={data privacy;random processes;information leakage;operational measure;discrete random variables;maximal leakage;randomized function;Sibson mutual information;order infinity;data processing inequality;true function value;Measurement;Privacy;Security;Random variables;Mutual information;Semantics;Information science;Leakage;Privacy;Sibson mutual information;Inference},
doi={10.1109/CISS.2016.7460507},
ISSN={},
month={March},}

@ARTICLE{leakageLong,
  author={I. {Issa} and A. B. {Wagner} and S. {Kamath}},
  journal={IEEE Transactions on Information Theory}, 
  title={An Operational Approach to Information Leakage}, 
  year={2020},
  volume={66},
  number={3},
  pages={1625-1657},
  doi={10.1109/TIT.2019.2962804}}
@article{infoAdap,
  author    = {A. D. Smith},
  title     = {Information, Privacy and Stability in Adaptive Data Analysis},
  journal   = {CoRR},
  volume    = {abs/1706.00820},
  year      = {2017},
  archivePrefix = {arXiv},
  eprint    = {1706.00820},
  timestamp = {Mon, 13 Aug 2018 16:48:59 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/Smith17b},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ISIT2018,
  author    = {Ibrahim Issa and
               Michael Gastpar},
  title     = {Computable Bounds on the Exploration Bias},
  booktitle = {2018 {IEEE} International Symposium on Information Theory, {ISIT} 
  Vail, CO, USA, June 17-22, 2018},
  pages     = {576--580},
  year      = {2018},
  doi       = {10.1109/ISIT.2018.8437470},
  timestamp = {Tue, 21 Aug 2018 14:19:30 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/isit/IssaG18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{algoStability,
	author = {Bassily, Raef and Nissim, Kobbi and Smith, Adam and Steinke, Thomas and Stemmer, Uri and Ullman, Jonathan},
	title = {Algorithmic Stability for Adaptive Data Analysis},
	booktitle = {Proceedings of the Forty-eighth Annual ACM Symposium on Theory of Computing},
	series = {STOC '16},
	year = {2016},
	isbn = {978-1-4503-4132-5},
	location = {Cambridge, MA, USA},
	pages = {1046--1059},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {Adaptivity, Data Analysis, Differential Privacy, Generalization, Stability, Statistics},
} 


@inproceedings{infoThGenAn,
   author = {{Xu}, A. and {Raginsky}, M.},
    title = "{Information-theoretic analysis of generalization capability of learning algorithms}",
 keywords = {Computer Science - Machine Learning, Computer Science - Information Theory, Statistics - Machine Learning},
     year = 2017,
    booktitle= {Advances in Neural Information Processing Systems},
    pages= {2521–2530},
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170507809X},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{DworkCalibrating,
 author = {Dwork, C. and McSherry, F. and Nissim, K. and Smith, A.},
 title = {Calibrating Noise to Sensitivity in Private Data Analysis},
 booktitle = {Proceedings of the Third Conference on Theory of Cryptography},
 series = {TCC'06},
 year = {2006},
 isbn = {3-540-32731-2, 978-3-540-32731-8},
 location = {New York, NY},
 pages = {265--284},
 numpages = {20},
 acmid = {2180305},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
}

@article{CompSecQuantLeakage,
  title={Quantitative notions of leakage for one-try attacks},
  author={Braun, C. and Chatzikokolakis, K. and Palamidessi, C.},
  journal={Electronic Notes in Theoretical Computer Science},
  volume={249},
  pages={75--91},
  year={2009},
  publisher={Elsevier}
}



@INPROCEEDINGS{LeakageGeneralized, 
author={Alvim, M.S. and Chatzikokolakis, K. and Palamidessi, C. and Smith, G.}, 
booktitle={Computer Security Foundations Symposium (CSF), 2012 IEEE 25th}, 
title={Measuring Information Leakage Using Generalized Gain Functions}, 
year={2012}, 
pages={265-279}, 
doi={10.1109/CSF.2012.26}, 
ISSN={1940-1434}, 
month={June},}


@INPROCEEDINGS{CompSecAddMultMaxLeakage, 
author={M. S. Alvim and K. Chatzikokolakis and A. Mciver and C. Morgan and C. Palamidessi and G. Smith}, 
booktitle={IEEE 27th Computer Security Foundations Symposium}, 
title={Additive and Multiplicative Notions of Leakage, and Their Capacities}, 
year={2014}, 
pages={308-322},
doi={10.1109/CSF.2014.29}, 
ISSN={1063-6900}, 
month={July},}

@ARTICLE{RenyiKLDiv, 
        author={T. van Erven and P. Harremo\"es}, 
        journal={IEEE Trans. Inf. Theory}, 
        title={{R}\'enyi Divergence and {K}ullback-{L}eibler Divergence},
        year={2014}, 
        volume={60}, 
        number={7}, 
        pages={3797-3820}, 
        month={July},}
        
@BOOK{learningBook,
  title = {Understanding machine learning: From theory to algorithms},
  author = {S. Shalev-Shwartz and S. Ben-David.},
  year = {2014}, 
  publisher = {Cambridge University Press},
}


@inproceedings{jiao2017dependence,
  title={Dependence measures bounding the exploration bias for general measurements},
  author={Jiao, Jiantao and Han, Yanjun and Weissman, Tsachy},
  booktitle={Information Theory (ISIT), 2017 IEEE International Symposium on},
  pages={1475--1479},
  year={2017},
  organization={IEEE}
}
@inproceedings{ISIT2019,
  author    = {Ibrahim Issa and
               Amedeo Roberto Esposito and
               Michael Gastpar},
  title     = {Strengthened Information-theoretic Bounds on the Generalization Error},
  booktitle = {2019 {IEEE} International Symposium on Information Theory, {ISIT} 
  Paris, France, July 7-12},
  year={2019}
}
@INPROCEEDINGS{calmonWassDist,
  author={Wang, Hao and Diaz, Mario and Santos Filho, José Cândido S. and Calmon, Flavio P.},
  booktitle={2019 IEEE International Symposium on Information Theory (ISIT)}, 
  title={An Information-Theoretic View of Generalization via Wasserstein Distance}, 
  year={2019},
  volume={},
  number={},
  pages={577-581},
  doi={10.1109/ISIT.2019.8849359}}
  
  @article{genErrMISGLD,
  title={Generalization Error Bounds for Noisy, Iterative Algorithms},
  author={Ankit Pensia and Varun Jog and Po-Ling Loh},
  journal={2018 IEEE International Symposium on Information Theory (ISIT)},
  year={2018},
  pages={546-550}
}

@inproceedings{chainingMI,
author = {Asadi, Amir R. and Abbe, Emmanuel and Verd\'{u}, Sergio},
title = {Chaining Mutual Information and Tightening Generalization Bounds},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {7245–7254},
numpages = {10},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@ARTICLE{tighteningMI,
  author={Bu, Yuheng and Zou, Shaofeng and Veeravalli, Venugopal V.},
  journal={IEEE Journal on Selected Areas in Information Theory}, 
  title={Tightening Mutual Information-Based Bounds on Generalization Error}, 
  year={2020},
  volume={1},
  number={1},
  pages={121-130},
  doi={10.1109/JSAIT.2020.2991139}}
@Article{harout,
	Author = {E. A. {Arutjunjan}},
	Title = {{Bounds for the Exponent of the Probability of Error for a Semicontinuous Memoryless Channel}},
	FJournal = {{Problemy Peredachi Informatsii}},
	Journal = {{Probl. Peredachi Inf.}},
	ISSN = {0555-2923},
	Volume = {4},
	Number = {4},
	Pages = {37--48},
	Year = {1968},
	Publisher = {Russian Academy of Sciences - RAS (Rossi\u{\i}skaya Akademiya Nauk - RAN), Institute for Information Transmission Problems, Moscow; Nauka, Moscow},
	Language = {Russian},
	MSC2010 = {94A15},
	Zbl = {0256.94022}
}
@Article{full,
	Author = {Amedeo Roberto Esposito and Michael Gastpar and Ibrahim Issa},
	Title = {A New Approach to Adaptive Data Analysis and Learning via Maximal Leakage},
	Year = {2019},
	Eprint = {arXiv:1903.01777},
	url = {https://arxiv.org/abs/1903.01777}
	
}

@ARTICLE{convergenceDiv,
    author = {Alison L. Gibbs and Francis Edward Su},
    title = {On choosing and bounding probability metrics},
    journal = {INTERNAT. STATIST. REV. },
    year = {2002},
    pages = {419--435}
}

@BOOK{Su1995, 
    author = {Francis Edward Su},
    title ={Methods for Quantifying Rates of Convergence for Random Walks on Groups},
    year = {1995},
    publisher = {Harvard University}}
    
@inproceedings{verduAlpha,
  author    = {Sergio Verd{\'{u}}},
  title     = {{\(\alpha\)}-mutual information},
  booktitle = {2015 Information Theory and Applications Workshop, {ITA} 2015, San
               Diego, CA, USA, February 1-6, 2015},
  pages     = {1--6},
  year      = {2015},
}

@book{BLM2013Concentration,
	Author = {St\'ephane Boucheron and G\'abor Lugosi and Pascal Massart},
	Publisher = {Oxford University Press},
	Title = {Concentration Inequalities: A Nonasymptotic Theory of Independence},
	Year = {2013}}
	
@article{generrWassDist,
  title={Generalization error bounds using Wasserstein distances},
  author={Adrian Tovar Lopez and Varun S. Jog},
  journal={2018 IEEE Information Theory Workshop (ITW)},
  year={2018},
  pages={1-5}
}
@article{fDiv1,
 author = {Liese, F. and Vajda, I.},
 title = {On Divergences and Informations in Statistics and Information Theory},
 journal = {IEEE Trans. Inf. Theor.},
 issue_date = {October 2006},
 volume = {52},
 number = {10},
 year = {2006},
 issn = {0018-9448},
 pages = {4394--4412},
 numpages = {19},
 url = {http://dx.doi.org/10.1109/TIT.2006.881731},
 doi = {10.1109/TIT.2006.881731},
 acmid = {2272412},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {Arimoto divergence, Arimoto entropy, Arimoto information, Shannon divergence, Shannon information, deficiency, discrimination information, minimum, statistical information, sufficiency},
} 


@ARTICLE{opMeanRDiv1, 
author={I. {Csiszar}}, 
journal={IEEE Transactions on Information Theory}, 
title={{G}eneralized cutoff rates and {R}\'enyi's information measures}, 
year={1995}, 
volume={41}, 
number={1}, 
pages={26-34}, 
doi={10.1109/18.370121}, 
ISSN={0018-9448}, 
month={Jan},}
@book{opMeanRDiv2,
 author = {Gr\"{u}nwald, Peter D.},
 title = {The Minimum Description Length Principle (Adaptive Computation and Machine Learning)},
 year = {2007},
 isbn = {0262072815},
 publisher = {The MIT Press},
} 
@inproceedings{IZS2020,
  author    = {Amedeo Roberto Esposito and
               Michael Gastpar and
               Ibrahim Issa},
  title     = {Robust Generalization via $\alpha$-Mutual Information},
  booktitle = {International Zurich Seminar on Information and Communication (IZS), February 26 – 28, 2020, Zurich, Switzerland},
  year      = {2020}
}
@inproceedings{ITW2019,
  author    = {Amedeo Roberto Esposito and
               Michael Gastpar and
               Ibrahim Issa},
  title     = {Learning and Adaptive Data Analysis via Maximal
Leakage},
  booktitle = {{IEEE} Information Theory Workshop, {ITW} 2019, Visby, Gotland, Sweden, Aug 25-28},
  year      = {2019}
}

@article{bousqet,
 author = {Bousquet, Olivier and Elisseeff, Andr{\'e}},
 title = {Stability and Generalization},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2002},
 volume = {2},
 month = {3},
 year = {2002},
 issn = {1532-4435},
 pages = {499--526},
 numpages = {28},
 doi = {10.1162/153244302760200704},
 acmid = {944801},
 publisher = {JMLR.org},
} 
@incollection{varReprfDiv,
title = {Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization},
author = {Nguyen, XuanLong and Wainwright, Martin J and Michael I. Jordan},
booktitle = {Advances in Neural Information Processing Systems 20},
editor = {J. C. Platt and D. Koller and Y. Singer and S. T. Roweis},
pages = {1089--1096},
year = {2008},
publisher = {Curran Associates, Inc.}
}
@ARTICLE{sdpiRaginsky,
  author={Raginsky, Maxim},
  journal={IEEE Transactions on Information Theory}, 
  title={Strong Data Processing Inequalities and $\Phi $-{S}obolev Inequalities for Discrete Channels}, 
  year={2016},
  volume={62},
  number={6},
  pages={3355-3389},
  doi={10.1109/TIT.2016.2549542}}

@ARTICLE{bayesRiskRaginsky,
  author={A. {Xu} and M. {Raginsky}},
  journal={IEEE Transactions on Information Theory}, 
  title={Information-Theoretic Lower Bounds on Bayes Risk in Decentralized Estimation}, 
  year={2017},
  volume={63},
  number={3},
  pages={1580-1600},}
@INPROCEEDINGS{lapidothTesting,
  author={A. {Lapidoth} and C. {Pfister}},
  booktitle={2018 IEEE Information Theory Workshop (ITW)}, 
  title={{T}esting {A}gainst {I}ndependence and a {R}ényi {I}nformation {M}easure}, 
  year={2018},
  volume={},
  number={},
  pages={1-5},}
  @ARTICLE{tomamichel,
  author={M. {Tomamichel} and M. {Hayashi}},
  journal={IEEE Transactions on Information Theory}, 
  title={Operational Interpretation of Rényi Information Measures via Composite Hypothesis Testing Against Product and Markov Distributions}, 
  year={2018},
  volume={64},
  number={2},
  pages={1064-1082},
  doi={10.1109/TIT.2017.2776900}}
  
@book{largeDeviationVaradhan,
title = {Large Deviations and Applications},
author = { S.R.S. Varadhan},
year = {1984},
}

@article{infoRadius,
    title={Information radius},
    author={Sibson, R.},
    year={1969},
   journal = {Z. Wahrscheinlichkeitstheorie verw Gebiete 14},
   pages={149-160}
}
@book{dunford1988linear,
  title={Linear Operators, Part 1: General Theory},
  author={Dunford, N. and Schwartz, J.T.},
  isbn={9780471608486},
  lccn={88116254},
  series={Wiley Classics Library},
  year={1988},
  publisher={Wiley}
}
@Inbook{ambrosioOT,
author="Ambrosio, Luigi",
title="Lecture Notes on Optimal Transport Problems",
bookTitle="Mathematical Aspects of Evolving Interfaces: Lectures given at the C.I.M.-C.I.M.E. joint Euro-Summer School held in Madeira, Funchal, Portugal, July 3-9, 2000",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--52",
isbn="978-3-540-39189-0",
doi="10.1007/978-3-540-39189-0_1",
url="https://doi.org/10.1007/978-3-540-39189-0_1"
}


@ARTICLE{fullVersionGeneralization,
  author={Esposito, Amedeo Roberto and Gastpar, Michael and Issa, Ibrahim},
  journal={IEEE Transactions on Information Theory}, 
  title={Generalization Error Bounds via Rényi-, f-Divergences and Maximal Leakage}, 
  year={2021},
  volume={67},
  number={8},
  pages={4986-5004},
  doi={10.1109/TIT.2021.3085190}}
  
@ARTICLE{distrFuncComput,
  author={A. {Xu} and M. {Raginsky}},
  journal={IEEE Transactions on Information Theory}, 
  title={Information-Theoretic Lower Bounds for Distributed Function Computation}, 
  year={2017},
  volume={63},
  number={4},
  pages={2314-2337},}

@INPROCEEDINGS{conditionalSibsMI,
  author={Esposito, Amedeo Roberto and Wu, Diyuan and Gastpar, Michael},
  booktitle={2021 IEEE International Symposium on Information Theory (ISIT)}, 
  title={On conditional Sibson's $\alpha$-Mutual Information}, 
  year={2021},
  volume={},
  number={},
  pages={1796-1801},
  doi={10.1109/ISIT45174.2021.9517944}}

@inproceedings{nipsHideAndSeek,
 author = {Shamir, Ohad},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K. Q. Weinberger},
 pages = {163--171},
 publisher = {Curran Associates, Inc.},
 title = {Fundamental Limits of Online and Distributed Algorithms for Statistical Learning and Estimation},
 volume = {27},
 year = {2014}
}

@ARTICLE{han,
  author={ {Te Sun Han} and S. {Amari}},
  journal={IEEE Transactions on Information Theory}, 
  title={Statistical inference under multiterminal data compression}, 
  year={1998},
  volume={44},
  number={6},
  pages={2300-2324},
  doi={10.1109/18.720540}}
  


@Inbook{estToTesting,
author="Yu, Bin",
editor="Pollard, David
and Torgersen, Erik
and Yang, Grace L.",
title="Assouad, Fano, and Le Cam",
bookTitle="Festschrift for Lucien Le Cam: Research Papers in Probability and Statistics",
year="1997",
publisher="Springer New York",
address="New York, NY",
pages="423--435",
abstract="This note explores the connections and differences between three commonly used methods for constructing minimax lower bounds in nonparametric estimation problems: Le Cam's, Assouad's and Fano's. Two connections are established between Le Cam's and Assouad's and between Assouad's and Fano's. The three methods are then compared in the context of two estimation problems for a smooth class of densities on [0,1]. The two estimation problems are for the integrated squared first derivatives and for the density function itself.",
isbn="978-1-4612-1880-7",
doi="10.1007/978-1-4612-1880-7_29",
}

@Inbook{smallBallai,
author="Nguyen, Hoi H.
and Vu, Van H.",
title="Small Ball Probability, Inverse Theorems, and Applications",
bookTitle="Erd{\H{o}}s Centennial",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="409--463",
isbn="978-3-642-39286-3",
doi="10.1007/978-3-642-39286-3_16"
}

@article{levyLittleSums,
  author    = {Sergey G. Bobkov and Gennadiy P. Chistyakov },
  title     = {On Concentration Functions of Random Variables},
  journal   = {Journal of Theoretical Probability volume},
  volume    = {28},
  year      = {2015}
}
@article{anantharamRenyiDivVarRepr,
  author    = {Venkat Anantharam},
  title     = {A Variational Characterization of R{\'{e}}nyi Divergences},
  journal   = {CoRR},
  volume    = {abs/1701.07796},
  year      = {2017},
  url       = {http://arxiv.org/abs/1701.07796},
  eprinttype = {arXiv},
  eprint    = {1701.07796},
  timestamp = {Mon, 13 Aug 2018 16:48:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Anantharam17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{RenyiDivVarReprRisk,
      title={Robust bounds on risk-sensitive functionals via Renyi divergence}, 
      author={Rami Atar and Kamaljit Chowdhary and Paul Dupuis},
      year={2013},
      eprint={1310.6391},
      archivePrefix={arXiv},
      primaryClass={math.PR}
}
@misc{RenyiDivVarReprNeuralNetwork,
      title={Variational Representations and Neural Network Estimation of R\'enyi Divergences}, 
      author={Jeremiah Birrell and Paul Dupuis and Markos A. Katsoulakis and Luc Rey-Bellet and Jie Wang},
      year={2021},
      eprint={2007.03814},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@article{orliczAmemiyaNorm,
title = "Amemiya norm equals Orlicz norm in general",
journal = "Indagationes Mathematicae",
volume = "11",
number = "4",
pages = "573 - 585",
year = "2000",
issn = "0019-3577",
doi = "https://doi.org/10.1016/S0019-3577(00)80026-9",
url = "http://www.sciencedirect.com/science/article/pii/S0019357700800269",
author = "Henryk Hudzik and Lech Maligranda",
abstract = "We present a proof that in Orlicz spaces the Amemiya norm and the Orlicz norm coincide for any Orlicz function ϕ. This gives the answer for an open problem. We also give a description of the Amemiya type for the Mazur-Orlicz F-norm."
}

@ARTICLE{shannonComm,
  author={Shannon, C. E.},
  journal={The Bell System Technical Journal}, 
  title={A mathematical theory of communication}, 
  year={1948},
  volume={27},
  number={3},
  pages={379-423},
  doi={10.1002/j.1538-7305.1948.tb01338.x}}
  
  @article{Kullback51klDivergence,
  added-at = {2010-10-31T19:59:47.000+0100},
  author = {Kullback, S. and Leibler, R. A.},
  biburl = {https://www.bibsonomy.org/bibtex/2560a5719c537c5c4a496bfebd4a21603/lee_peck},
  description = {Kullback , Leibler : On Information and Sufficiency},
  interhash = {f9d41d76a07383cca4c3a1a94c24d533},
  intrahash = {560a5719c537c5c4a496bfebd4a21603},
  journal = {Ann. Math. Statist.},
  keywords = {51 Kullback Leibler divergence kl},
  number = 1,
  pages = {79-86},
  timestamp = {2010-10-31T19:59:47.000+0100},
  title = {On Information and Sufficiency},
  volume = 22,
  year = 1951
}


@article{renyiEntropy,
title = {On Measures of Entropy and Information},
journal = {Proceedings of the 4th Berkeley Symposium on Mathematics, Statistics and Probability},
volume = {1},
pages = {547-561},
year = {1960},
author = {A. Rényi},
}
@Article{chernoffTesting,
author = "H. Chernoff",
title = "A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations",
journal = "Annals of Math. Stat.",
volume = "23",
year = "1952",
pages = "493--509",
comment = "Contains foundations for ``Chernoff bounds''",
}
@ARTICLE{RenyiErrorProb,
  author={Ben-Bassat, M. and Raviv, J.},
  journal={IEEE Transactions on Information Theory}, 
  title={Renyi's entropy and the probability of error}, 
  year={1978},
  volume={24},
  number={3},
  pages={324-331},
  doi={10.1109/TIT.1978.1055890}}
  
  @ARTICLE{SibsCutoffRates,
  author={Csisz\'ar, I.},
  journal={IEEE Transactions on Information Theory}, 
  title={Generalized cutoff rates and Renyi's information measures}, 
  year={1995},
  volume={41},
  number={1},
  pages={26-34},
  doi={10.1109/18.370121}}
  
  @article{Csiszr1972ACO,
  title={A class of measures of informativity of observation channels},
  author={Imre Csisz{\'a}r},
  journal={Periodica Mathematica Hungarica},
  year={1972},
  volume={2},
  pages={191-213}
}
@article{csizsar3,
author={Imre Csisz{\'a}r},
title={Information-type measures of difference of probability distributions and indirect observation},
journal={Studia Scientiarum Mathematicarum Hungarica},
year={1967},
volume={2},
pages={229-318},
URL={https://ci.nii.ac.jp/naid/10028997448/en/},
}
@PHDTHESIS{commComplex,
       author = {{Pankratov}, Denis},
        title = "{Communication complexity and information complexity}",
     keywords = {Computer science;Mathematics;Information science},
       school = {The University of Chicago},
         year = 2015,
        month = jan,
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015PhDT........42P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@INPROCEEDINGS{commComplex2,
  author={Bar-Yossef, Z. and Jayram, T.S. and Kumar, R. and Sivakumar, D.},
  booktitle={Proceedings 17th IEEE Annual Conference on Computational Complexity}, 
  title={Information theory methods in communication complexity}, 
  year={2002},
  volume={},
  number={},
  pages={93-102},
  doi={10.1109/CCC.2002.1004344}}
  
@INPROCEEDINGS{arimotoRenyiDiv,
  author={Polyanskiy, Yury and Verdú, Sergio},
  booktitle={2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton)}, 
  title={Arimoto channel coding converse and Rényi divergence}, 
  year={2010},
  volume={},
  number={},
  pages={1327-1333},
  doi={10.1109/ALLERTON.2010.5707067}}

@article{fDiv,
title = {Eine informationstheoretische ungleichung und ihre anwendung auf den beweis der ergodizitat von markoffschen ketten},
journal = {Magyar. Tud. Akad. Mat. Kutató Int. Közl,},
volume = {8},
pages = {85-108},
year = {1963},
author = {Imre Csisz{\'a}r},
}
  
  @INPROCEEDINGS{distributedEstimation,
  author={Esposito, Amedeo Roberto and Gastpar, Michael},
  booktitle={2021 IEEE International Symposium on Information Theory (ISIT)}, 
  title={Lower-bounds on the Bayesian Risk in estimation procedures via {S}ibson's $\alpha$-Mutual Information}, 
  year={2021},
  volume={},
  number={},
  pages={748-753},
  doi={10.1109/ISIT45174.2021.9517954}}
  
 @article{hoeffdingInequality,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2282952},
 author = {Wassily Hoeffding},
 journal = {Journal of the American Statistical Association},
 number = {301},
 pages = {13--30},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Probability Inequalities for Sums of Bounded Random Variables},
 volume = {58},
 year = {1963}
}
@article{azumaInequality,
author = {Kazuoki Azuma},
title = {{Weighted sums of certain dependent random variables}},
volume = {19},
journal = {Tohoku Mathematical Journal},
number = {3},
publisher = {Tohoku University, Mathematical Institute},
pages = {357 -- 367},
year = {1967},
doi = {10.2748/tmj/1178243286},
URL = {https://doi.org/10.2748/tmj/1178243286}
}

@inbook{mcdiarmid_1989, place={Cambridge}, series={London Mathematical Society Lecture Note Series}, title={On the method of bounded differences}, DOI={10.1017/CBO9781107359949.008}, booktitle={Surveys in Combinatorics, 1989: Invited Papers at the Twelfth British Combinatorial Conference}, publisher={Cambridge University Press}, author={McDiarmid, Colin}, year={1989}, pages={148–188}, collection={London Mathematical Society Lecture Note Series}}

@book{vershynin_2018, place={Cambridge}, series={Cambridge Series in Statistical and Probabilistic Mathematics}, title={High-Dimensional Probability: An Introduction with Applications in Data Science}, DOI={10.1017/9781108231596}, publisher={Cambridge University Press}, author={Vershynin, Roman}, year={2018}, collection={Cambridge Series in Statistical and Probabilistic Mathematics}}

@article{duchiFano2013,
  author    = {John C. Duchi and
               Martin J. Wainwright},
  title     = {Distance-based and continuum Fano inequalities with applications to
               statistical estimation},
  journal   = {CoRR},
  volume    = {abs/1311.2669},
  year      = {2013},
  url       = {http://arxiv.org/abs/1311.2669},
  eprinttype = {arXiv},
  eprint    = {1311.2669},
  timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/DuchiW13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}  
@inproceedings{duchiEstimationIT,
 author = {Zhang, Yuchen and Duchi, John and Jordan, Michael I and Wainwright, Martin J},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Information-theoretic lower bounds for distributed statistical estimation with communication constraints},
 volume = {26},
 year = {2013}
}

@Article{sasonGuessing,
AUTHOR = {Sason, Igal},
TITLE = {Tight Bounds on the Rényi Entropy via Majorization with Applications to Guessing and Compression},
JOURNAL = {Entropy},
VOLUME = {20},
YEAR = {2018},
NUMBER = {12},
ARTICLE-NUMBER = {896},
URL = {https://www.mdpi.com/1099-4300/20/12/896},
ISSN = {1099-4300},
DOI = {10.3390/e20120896}
}

@ARTICLE{sasonTesting,
  author={Sason, Igal and Verdú, Sergio},
  journal={IEEE Transactions on Information Theory}, 
  title={Arimoto–Rényi Conditional Entropy and Bayesian $M$ -Ary Hypothesis Testing}, 
  year={2018},
  volume={64},
  number={1},
  pages={4-25},
  doi={10.1109/TIT.2017.2757496}}


@Article{graczykSasonGuessing,
AUTHOR = {Graczyk, Robert and Sason, Igal},
TITLE = {On Two-Stage Guessing},
JOURNAL = {Information},
VOLUME = {12},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {159},
URL = {https://www.mdpi.com/2078-2489/12/4/159},
ISSN = {2078-2489},

DOI = {10.3390/info12040159}
}

@ARTICLE{polyianksiyConverse,
  author={Polyanskiy, Yury and Poor, H. Vincent and Verdu, Sergio},
  journal={IEEE Transactions on Information Theory}, 
  title={Channel Coding Rate in the Finite Blocklength Regime}, 
  year={2010},
  volume={56},
  number={5},
  pages={2307-2359},
  doi={10.1109/TIT.2010.2043769}}
@book{coverThomas,
author = {Cover, Thomas M. and Thomas, Joy A.},
title = {Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)},
year = {2006},
isbn = {0471241954},
publisher = {Wiley-Interscience},
address = {USA}
}

@phdthesis{jingboLiuThesis,
  author       = {	Liu, Jingbo}, 
  title        = {Information Theory from A Functional Viewpoint},
  school       = {Princeton, NJ : Princeton University},
  year         = 2018,
}


@ARTICLE{shannonPred,
  author={Shannon, C. E.},
  journal={The Bell System Technical Journal}, 
  title={Prediction and entropy of printed English}, 
  year={1951},
  volume={30},
  number={1},
  pages={50-64},
  doi={10.1002/j.1538-7305.1951.tb01366.x}}
@article{campbellCoding,
title = {A coding theorem and Rényi's entropy},
journal = {Information and Control},
volume = {8},
number = {4},
pages = {423-429},
year = {1965},
issn = {0019-9958},
doi = {https://doi.org/10.1016/S0019-9958(65)90332-3},
url = {https://www.sciencedirect.com/science/article/pii/S0019995865903323},
author = {L.L. Campbell},
}
@ARTICLE{arikanGuessing,
  author={Arikan, E.},
  journal={IEEE Transactions on Information Theory}, 
  title={An inequality on guessing and its application to sequential decoding}, 
  year={1996},
  volume={42},
  number={1},
  pages={99-105},
  doi={10.1109/18.481781}}
  

@article{minimizationMeasures,
author = {Broniatowski, Michel and Keziou, Amor},
year = {2006},
pages = {403-442},
title = {Minimization of divergences on sets of signed measures},
volume = {43},
number = {4},
journal = {Studia Scientiarum Mathematicarum Hungarica},
doi = {10.1556/SScMath.43.2006.4.2}
}
@inproceedings{varReprProbMeasures,
author = {Ruderman, Avraham and Reid, Mark D. and Garc\'{\i}a-Garc\'{\i}a, Dar\'{\i}o and Petterson, James},
title = {Tighter Variational Representations of F-Divergences via Restriction to Probability Measures},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We show that the variational representations for f-divergences currently used in the literature can be tightened. This has implications to a number of methods recently proposed based on this representation. As an example application we use our tighter representation to derive a general f-divergence estimator based on two i.i.d. samples and derive the dual program for this estimator that performs well empirically. We also point out a connection between our estimator and MMD.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1155–1162},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}
	
@BOOK{concentrationMeasureII,
  author={M. {Raginsky} and I. {Sason}},
  Title={Concentration of Measure Inequalities in Information Theory, Communications, and Coding: Second Edition},
  publisher={Now Foundations and Trends},
  year={2014},
  volume={},
  number={},
  pages={},
  doi={}}
  
@BOOK{villaniOToldandnew,
  author={C. {Villani}},
  Title={Optimal Transport: Old and New},
  year={2008},
  publisher={Springer Science \& Business Media},
  volume={},
  number={},
  pages={976},
  doi={}}
  @book{introductoryFunctionalAnalysis,
  title={Introductory Functional Analysis with Applications},
  author ={E. Kreyszig},
  isbn={9788126511914},
  series={Wiley classics library},
  url={https://books.google.ch/books?id=osXw-pRsptoC},
  year={2007},
  publisher={Wiley India Pvt. Limited}
}
@book{theoryOrliczSpaces,
  title={Theory of Orlicz Spaces},
  author ={M.M. Rao, Z.D. Ren},
  year={1991},
  publisher={ New York: M. Dekker,}
}
@misc{sibsonIalphaNegative_old,
      title={On {S}ibson's $\alpha$-{M}utual {I}nformation}, 
      author={Amedeo Roberto Esposito and Adrien Vandenbroucque and Michael Gastpar},
      year={2022},
      eprint={2202.03951},
      archivePrefix={arXiv},
      primaryClass={cs.IT}
}   
@inproceedings{sibsonIalphaNegative,
  doi = {10.1109/isit50566.2022.9834428},
  url = {https://doi.org/10.1109/isit50566.2022.9834428},
  year = {2022},
  month = jun,
  publisher = {{IEEE}},
  author = {Amedeo Roberto Esposito and Adrien Vandenbroucque and Michael Gastpar},
  title = {On Sibson's $\upalpha$-Mutual Information},
  booktitle = {2022 {IEEE} International Symposium on Information Theory ({ISIT})}
}
@INPROCEEDINGS{masseyGuessing,
  author={Massey, J.L.},
  booktitle={Proceedings of 1994 IEEE International Symposium on Information Theory}, 
  title={Guessing and entropy}, 
  year={1994},
  volume={},
  number={},
  pages={204-},
  doi={10.1109/ISIT.1994.394764}}
  
  @book{villani2003topics,
  title={Topics in Optimal Transportation},
  author={Villani, C.},
  isbn={9780821833124},
  lccn={2003040350},
  series={Graduate studies in mathematics},
  url={https://books.google.ch/books?id=R\_nWqjq89oEC},
  year={2003},
  publisher={American Mathematical Society}
}
  @Book{ lectureNotesFunctionaAnalysis,
author = { Berberian, Sterling K. },
title = { Lectures in functional analysis and operator theory },
isbn = { 0387900802 },
publisher = { Springer Verlag New York },
pages = { ix, 345 p. ; },
year = { 1974 },
type = { Book },
language = { English },
subjects = { Functional analysis.; Operator theory. },
life-dates = { 1974 -  },
catalogue-url = { https://nla.gov.au/nla.cat-vn296338 },
}
@Book{ nonLinearOpeartors,
author = {J.P. Gossez and E.J. LamiDozo and J. Mahwin and L. Waelbroeck},
title = { Nonlinear
Operators and the Calculus of Variations},
publisher = { Springer Berlin Heidelberg },
pages = { 157-207 },
year = { 1976 },
}
@book{largeDeviationConvexAnalysis,
author = {Rassoul-Agha, Firas and Seppäläinen, Timo},
year = {2015},
month = {05},
pages = {},
title = {A course on large deviations with an introduction to Gibbs measures},
isbn = {978-0-8218-7578-0}
}

@book{dembo2009large,
  title={Large Deviations Techniques and Applications},
  author={Dembo, A. and Zeitouni, O.},
  isbn={9783642033100},
  lccn={97045236},
  series={Stochastic Modelling and Applied Probability},
  url={https://books.google.ch/books?id=d3nnjwEACAAJ},
  year={2009},
  publisher={Springer Berlin Heidelberg}
}

booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1155–1162},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}
@ARTICLE{fDivegerceInequalities,
  author={Sason, Igal and Verdú, Sergio},
  journal={IEEE Transactions on Information Theory}, 
  title={$f$ -Divergence Inequalities}, 
  year={2016},
  volume={62},
  number={11},
  pages={5973-6006},
  doi={10.1109/TIT.2016.2603151}}
@article{bobkov,
title = "Exponential Integrability and Transportation Cost Related to Logarithmic Sobolev Inequalities",
journal = "Journal of Functional Analysis",
volume = "163",
number = "1",
pages = "1 - 28",
year = "1999",
issn = "0022-1236",
author = "S.G Bobkov and F Götze",
keywords = "logarithmic Sobolev inequalities, exponential integrability, concentration of measure, transportation inequalities",
abstract = "We study some problems on exponential integrability, concentration of measure, and transportation cost related to logarithmic Sobolev inequalities. On the real line, we then give a characterization of those probability measures which satisfy these inequalities"
}
@article{cohenDobrushin,
title = {Relative entropy under mappings by stochastic matrices},
journal = {Linear Algebra and its Applications},
volume = {179},
pages = {211-235},
year = {1993},
issn = {0024-3795},
doi = {https://doi.org/10.1016/0024-3795(93)90331-H},
url = {https://www.sciencedirect.com/science/article/pii/002437959390331H},
author = {Joel E. Cohen and Yoh Iwasa and Gh. Rautu and Mary {Beth Ruskai} and Eugene Seneta and Gh. Zbaganu},
abstract = {The relative g-entropy of two finite, discrete probability distributions x = (x1,…,xn) and y = (y1,…,yn) is defined as Hg(x,y) = Σkxkg (yk/kk - 1), where g:(-1,∞)→R is convex and g(0) = 0. When g(t) = -log(1 + t), then Hg(x,y) = Σkxklog(xk/yk), the usual relative entropy. Let Pn = {x ∈ Rn : σixi = 1, xi > 0 ∀i}. Our major results is that, for any m × n column-stochastic matrix A, the contraction coefficient defined as ηğ(A) = sup{Hg(Ax,Ay)/Hg(x,y) : x,y ∈ Pn, x ≠ y} satisfies ηg(A) ⩽1 - α(A), where α(A) = minj,kΣi min(aij, aik) is Dobrushin's coefficient of ergodicity. Consequently, ηg(A) < 1 if and only if A is scrambling. Upper and lower bounds on αg(A) are established. Analogous results hold for Markov chains in continuous time.}
}
@InProceedings{conditionalMI,
  title = 	 {{R}easoning {A}bout {G}eneralization via {C}onditional {M}utual {I}nformation},
  author =       {Steinke, Thomas and Zakynthinou, Lydia},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {3437--3452},
  year = 	 {2020},
  editor = 	 {Abernethy, Jacob and Agarwal, Shivani},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v125/steinke20a/steinke20a.pdf},
  url = 	 {https://proceedings.mlr.press/v125/steinke20a.html},
}

@INPROCEEDINGS{ismi,
  author={Bu, Yuheng and Zou, Shaofeng and Veeravalli, Venugopal V.},
  booktitle={2019 IEEE International Symposium on Information Theory (ISIT)}, 
  title={Tightening Mutual Information Based Bounds on Generalization Error}, 
  year={2019},
  volume={},
  number={},
  pages={587-591},
  doi={10.1109/ISIT.2019.8849590}}
 
@INPROCEEDINGS{CalmonSDPIHockeyStick,
  author={Asoodeh, Shahab and Aliakbarpour, Maryam and Calmon, Flavio P.},
  booktitle={2021 IEEE International Symposium on Information Theory (ISIT)}, 
  title={Local Differential Privacy Is Equivalent to Contraction of an $f$-Divergence}, 
  year={2021},
  volume={},
  number={},
  pages={545-550},
  doi={10.1109/ISIT45174.2021.9517999}}
  
  
@ARTICLE{Fenchel1949,
title={On Conjugate Convex Functions},
volume={1},
DOI={10.4153/CJM-1949-007-x},
number={1},
journal={Canadian Journal of Mathematics},
author={Fenchel, W.},
year={1949},
pages={73–77}}

@article{subWeibull,
	doi = {10.1002/sta4.318},
	year = 2020,
	month = {jan},
	publisher = {Wiley},
	volume = {9},
	number = {1},
	author = {Mariia Vladimirova and St{\'{e}
}phane Girard and Hien Nguyen and Julyan Arbel},
	title = {Sub-Weibull distributions: Generalizing sub-Gaussian and sub-Exponential properties to heavier tailed distributions},
	journal = {Stat}
}
@ARTICLE{modelBias,
  author={Gourgoulias, Konstantinos and Katsoulakis, Markos A. and Rey-Bellet, Luc and Wang, Jie},
  journal={IEEE Transactions on Information Theory}, 
  title={How Biased Is Your Model? Concentration Inequalities, Information and Model Bias}, 
  year={2020},
  volume={66},
  number={5},
  pages={3079-3097},
  doi={10.1109/TIT.2020.2977067}}
  
  @article{uncertaintyQuantif,
	doi = {10.1137/19m1237429},
  
	url = {https://doi.org/10.1137%2F19m1237429},
  
	year = 2020,
	month = {jan},
  
	publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
  
	volume = {8},
  
	number = {2},
  
	pages = {539--572},
  
	author = {Jeremiah Birrell and Luc Rey-Bellet},
  
	title = {Uncertainty Quantification for Markov Processes via Variational Principles and Functional Inequalities},
  
	journal = {{SIAM}/{ASA} Journal on Uncertainty Quantification}
}

@misc{tighterWassDist,
  doi = {10.48550/ARXIV.2101.09315},
  
  url = {https://arxiv.org/abs/2101.09315},
  
  author = {Rodríguez-Gálvez, Borja and Bassi, Germán and Thobaben, Ragnar and Skoglund, Mikael},
  
  keywords = {Machine Learning (stat.ML), Information Theory (cs.IT), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Tighter expected generalization error bounds via Wasserstein distance},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@INPROCEEDINGS{jointRange,
  author={Harremoës, Peter and Vajda, Igor},
  booktitle={2010 IEEE International Symposium on Information Theory}, 
  title={Joint range of f-divergences}, 
  year={2010},
  volume={},
  number={},
  pages={1345-1349},
  doi={10.1109/ISIT.2010.5513445}}
  
  @INPROCEEDINGS{renyiDP,
  author={Mironov, Ilya},
  booktitle={2017 IEEE 30th Computer Security Foundations Symposium (CSF)}, 
  title={Rényi Differential Privacy}, 
  year={2017},
  volume={},
  number={},
  pages={263-275},
  doi={10.1109/CSF.2017.11}}
  
  @Article{holder_1889,
    Author = {O. {H\"{o}lder}},
    Title = {{Ueber einen Mittelwertsatz}},
    FJournal = {{Nachrichten von der K\"{o}niglichen Gesellschaft der Wissenschaften und der Georg-Augusts-Universit\"{a}t zu G\"{o}ttingen}},
    Journal = {{G\"{o}tt. Nachr.}},
    Volume = {1889},
    Pages = {38--47},
    Year = {1889},
    Publisher = {Dieterich, G\"{o}ttingen},
    Language = {German},
    Zbl = {21.0260.07}
}
@Article{roger_1888,
    Author = {L.J. {R}ogers},
    Title = {{An extension of a certain theorem in inequalities}},
    Journal = {Messenger of Math.},
    Volume = {17},
    Pages = {145--150},
    Year = {1888}
}
@article{holdersRogers,
author = {Maligranda, Lech},
year = {1998},
month = {01},
pages = {},
title = {Why Hölder's inequality should be called Rogers' inequality},
volume = {1},
journal = {Mathematical Inequalities \& Applications},
doi = {10.7153/mia-01-05}
}

@ARTICLE{renyiDimensionYihongVerdu,
  author={Wu, Yihong and Verdú, Sergio},
  journal={IEEE Transactions on Information Theory}, 
  title={Rényi Information Dimension: Fundamental Limits of Almost Lossless Analog Compression}, 
  year={2010},
  volume={56},
  number={8},
  pages={3721-3748},
  doi={10.1109/TIT.2010.2050803}}
  
@inproceedings{renyiGeneralDefinition,
author = {Śmieja, Marek and Tabor, Jacek},
year = {2014},
month = {08},
pages = {685-689},
title = {Rényi entropy dimension of the mixture of measures},
journal = {Proceedings of 2014 Science and Information Conference, SAI 2014},
doi = {10.1109/SAI.2014.6918261}
}

@ARTICLE{renyiDimension,
  author={Rényi, A.},
  journal={Acta Mathematica Academiae Scientiarum Hungaricae}, 
  title={On the dimension and entropy of probability distributions}, 
  year={1959},
  volume={10},
  pages={193–215},
  doi={10.1007/BF02063299}}
  
@book{rockafellar-1970a,
  added-at = {2008-03-02T02:12:02.000+0100},
  address = {Princeton, N. J.},
  author = {Rockafellar, R. Tyrrell},
  biburl = {https://www.bibsonomy.org/bibtex/223aa07ea525f6dd11585fc2037a0daf1/dmartins},
  callnumber = {UniM Maths 516.08 R59},
  description = {robotica-bib},
  interhash = {30830becb0a2c5ebca5946b895d9740a},
  intrahash = {23aa07ea525f6dd11585fc2037a0daf1},
  keywords = {imported},
  notes = {A SRL reference.},
  publisher = {Princeton University Press},
  series = {Princeton Mathematical Series},
  timestamp = {2008-03-02T02:14:11.000+0100},
  title = {Convex analysis},
  year = 1970
}
@book{liese87,
  added-at = {2006-02-14T14:04:24.000+0100},
  author = {Liese, F. and Vajda, I.},
  interhash = {89364b4d23e06754db201a4a366d2c9f},
  intrahash = {a11a10523b64292e12e010abced8ec0c},
  keywords = {imported},
  publisher = {Teubner, Leipzig},
  timestamp = {2006-02-14T14:04:24.000+0100},
  title = {Convex Statistical Distances},
  volumne = {130},
  year = 1987
}


@ARTICLE{oneToOneCodes,
  author={Dunham, J.},
  journal={IEEE Transactions on Information Theory}, 
  title={Optimal noiseless coding of random variables (Corresp.)}, 
  year={1980},
  volume={26},
  number={3},
  pages={345-345},
  doi={10.1109/TIT.1980.1056200}}
  
  @ARTICLE{oneToOneOrlitsky,
  author={Alon, N. and Orlitsky, A.},
  journal={IEEE Transactions on Information Theory}, 
  title={A lower bound on the expected length of one-to-one codes}, 
  year={1994},
  volume={40},
  number={5},
  pages={1670-1672},
  doi={10.1109/18.333891}}
  
  @ARTICLE{optimalNoiselessCoding,
  author={Verriest, E.},
  journal={IEEE Transactions on Information Theory}, 
  title={An achievable bound for optimal noiseless coding of a random variable (Corresp.)}, 
  year={1986},
  volume={32},
  number={4},
  pages={592-594},
  doi={10.1109/TIT.1986.1057200}}
  
  @ARTICLE{oneToOneCover,
  author={Leung-Yan-Cheong, S. and Cover, T.},
  journal={IEEE Transactions on Information Theory}, 
  title={Some equivalences between Shannon entropy and Kolmogorov complexity}, 
  year={1978},
  volume={24},
  number={3},
  pages={331-338},
  doi={10.1109/TIT.1978.1055891}}
  
  @book{topologicalVectorSpaces,
  author = {V.I. Bogachev, O.G. Smolyanov},
  publisher = {Springer, Cham},
  series = {Springer Monographs in Mathematics},
  title = {Topological Vector Spaces and Their Applications},
  year = 2017
}

@book{gowersBook,
author = {},
editor = {Timothy Gowers and June Barrow-Green and Imre Leader},
doi = {doi:10.1515/9781400830398},
url = {https://doi.org/10.1515/9781400830398},
title = {The Princeton Companion to Mathematics},
year = {2010},
publisher = {Princeton University Press},
ISBN = {9781400830398}
}
@book{folland2013real,
  title={Real Analysis: Modern Techniques and Their Applications},
  author={Folland, G.B.},
  isbn={9781118626399},
  series={Pure and Applied Mathematics: A Wiley Series of Texts, Monographs and Tracts},
  url={https://books.google.ch/books?id=wI4fAwAAQBAJ},
  year={2013},
  publisher={Wiley}
}
@book{pollard_2001, place={Cambridge}, series={Cambridge Series in Statistical and Probabilistic Mathematics}, DOI={10.1017/CBO9780511811555.002}, title={A User's Guide to Measure Theoretic Probability}, publisher={Cambridge University Press}, author={Pollard, David}, year={2001}, collection={Cambridge Series in Statistical and Probabilistic Mathematics}}




@article{bayesRiskFInformativity,
author = {Chen, Xi and Guntuboyina, Adityanand and Zhang, Yuchen},
title = {On Bayes Risk Lower Bounds},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {7687–7744},
numpages = {58},
keywords = {smoothed analysis, Bayes risk, f-informativity, minimax risk, Fano's inequality, f-divergence}
}

@article{fInformativity,
  title={A class of measures of informativity of observation channels},
  author={Imre Csisz{\'a}r},
  journal={Periodica Mathematica Hungarica},
  year={1972},
  volume={2},
  pages={191-213}
}


@Article{fDivIntegralReprs,
AUTHOR = {Sason, Igal},
TITLE = {On f-Divergences: Integral Representations, Local Behavior, and Inequalities},
JOURNAL = {Entropy},
VOLUME = {20},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {383},
URL = {https://www.mdpi.com/1099-4300/20/5/383},
ISSN = {1099-4300},
ABSTRACT = {This paper is focused on f-divergences, consisting of three main contributions. The first one introduces integral representations of a general f-divergence by means of the relative information spectrum. The second part provides a new approach for the derivation of f-divergence inequalities, and it exemplifies their utility in the setup of Bayesian binary hypothesis testing. The last part of this paper further studies the local behavior of f-divergences.},
DOI = {10.3390/e20050383}
}



@article{aliSilveyFDiv,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2984279},
 abstract = {Let P1 and P2 be two probability measures on the same space and let φ be the generalized Radon-Nikodym derivative of P2 with respect to P1. If C is a continuous convex function of a real variable such that the P1-expectation (generalized as in Section 3) of C(φ) provides a reasonable coefficient of the P1-dispersion of φ, then this expectation has basic properties which it is natural to demand of a coefficient of divergence of P2 from P1. A general class of coefficients of divergence is generated in this way and it is shown that various available measures of divergence, distance, discriminatory information, etc., are members of this class.},
 author = {S. M. Ali and S. D. Silvey},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {131--142},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {A General Class of Coefficients of Divergence of One Distribution from Another},
 urldate = {2022-05-23},
 volume = {28},
 year = {1966}
}






@article{morimotoFDiv,
author = {Morimoto ,Tetsuzo},
title = {Markov Processes and the H-Theorem},
journal = {Journal of the Physical Society of Japan},
volume = {18},
number = {3},
pages = {328-331},
year = {1963},
doi = {10.1143/JPSJ.18.328},

URL = { 
        https://doi.org/10.1143/JPSJ.18.328
    
},
eprint = { 
        https://doi.org/10.1143/JPSJ.18.328
    
}

}




@Article{probDivergencesSason,
AUTHOR = {Nishiyama, Tomohiro and Sason, Igal},
TITLE = {On Relations Between the Relative Entropy and $\chi^2$-Divergence, Generalizations and Applications},
JOURNAL = {Entropy},
VOLUME = {22},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {563},
URL = {https://www.mdpi.com/1099-4300/22/5/563},
ISSN = {1099-4300},
ABSTRACT = {The relative entropy and the chi-squared divergence are fundamental divergence measures in information theory and statistics. This paper is focused on a study of integral relations between the two divergences, the implications of these relations, their information-theoretic applications, and some generalizations pertaining to the rich class of f-divergences. Applications that are studied in this paper refer to lossless compression, the method of types and large deviations, strong data–processing inequalities, bounds on contraction coefficients and maximal correlation, and the convergence rate to stationarity of a type of discrete-time Markov chains.},
DOI = {10.3390/e22050563}
}

@book{optVectorSpaces,
author = {Luenberger, David G.},
title = {Optimization by Vector Space Methods},
year = {1997},
isbn = {047155359X},
publisher = {John Wiley \& Sons, Inc.},
address = {USA},
edition = {1st},
abstract = {From the Publisher:Engineers must make decisions regarding the distribution of expensive resources in a manner that will be economically beneficial. This problem can be realistically formulated and logically analyzed with optimization theory. This book shows engineers how to use optimization theory to solve complex problems. Unifies the large field of optimization with a few geometric principles. Covers functional analysis with a minimum of mathematics. Contains problems that relate to the applications in the book.}
}

@Article{geometricDuality,
  author={R. I. Boţ and S. M. Grad and G. Wanka},
  title={{Fenchel-Lagrange Duality Versus Geometric Duality in Convex Optimization}},
  journal={Journal of Optimization Theory and Applications},
  year=2006,
  volume={129},
  number={1},
  pages={33-54},
  month={April},
  keywords={Geometric programming; convex optimization; perturbation theory; Lagrange and Fenchel duality; conju},
  doi={10.1007/s10957-006-9047-2},
  abstract={ We present a new duality theory to treat convex optimization problems and we prove that the geometric duality used by Scott and Jefferson in different papers during the last quarter of century is a special case of it. Moreover, weaker sufficient conditions to achieve strong duality are considered and optimality conditions are derived. Next, we apply our approach to some problems considered by Scott and Jefferson, determining their duals. We give weaker sufficient conditions to achieve strong duality and the corresponding optimality conditions. Finally, posynomial geometric programming is viewed also as a particular case of the duality approach that we present.},
  url={https://ideas.repec.org/a/spr/joptap/v129y2006i1d10.1007_s10957-006-9047-2.html}
}

@article{thesis,
      title = {A Functional Perspective on Information Measures},
      author = {Esposito, Amedeo Roberto},
      institution = {IINFCOM},
      publisher = {EPFL},
      address = {Lausanne},
      pages = {170},
      year = {2022},
      abstract = {Since the birth of Information Theory, researchers have  defined and exploited various information measures, as well  as endowed them with operational meanings. Some were born  as a "solution to a problem", like Shannon's Entropy and  Mutual Information. Others were the fruit of generalisation  and the mathematical genius of bright minds like Rényi,  Csizsár and Sibson. These powerful objects allow us to  manipulate probabilities intuitively and seem always to be  somehow connected to concrete settings in communication,  coding or estimation theory. A common theme is: take a  problem in one of these areas, try to control (upper or  lower-bound) the expected value of some function of  interest (often, probabilities of error) and, with enough  work, an information measure appears as a fundamental limit  of the problem. The most striking example of this is in  Shannon's seminal paper in 1948: his purpose was to  characterise the smallest possible expected length of a  uniquely decodable encoding that compresses the  realisations of a random variable. As he brilliantly  proved, the smallest expected length one can hope for is  the Entropy of the random variable. In establishing this  connection, another quantity needed to be implicitly  controlled: the Kraft's sum of the code. Seemingly  unrelated before, these three objects joined forces in  harmony to provide a beautiful and fundamental result. But  why are they related? The answer seems to be: duality.  Duality is an abstract notion commonly used in linear  algebra and functional analysis. It has been expanded and  generalised over the years. Several incarnations have been  discovered throughout mathematics. One particular instance  of this involves vector spaces: given two vector spaces and  a "duality pairing" one can jump from one space to the  other (its dual) through Legendre-Fenchel-like transforms.  In the most common settings in Information Theory, the two  spaces and the pairing are, respectively: 1) the space of  (probability)measures defined on X; 2) the space of bounded  functions defined on X; 3) the Lebesgue integral of the  function (the expected value of the function if the measure  is a probability measure). 
Once these are set,  Legendre-Fenchel-like transforms allow us to connect a) a  functional acting on the space described in 1), b) a  functional acting on the space described in 2) and the  anchor point is c) the (expected) value described in  3).
These three pieces (a), b) and c)) represent the actors  of many of the results provided in Information Theory. Once  they are found, one usually bounds the functional described  in b) and obtains a bound connecting the expected value and  the functional of measures (e.g., an information measure).  Going back to Shannon's result, fixed a random variable  (and thus, a probability measure) and selected the function  to be the length of a code: the functional a) is the  Shannon Entropy of the source; the functional b) is the  Kraft sum of the code; the pairing c) is the expected  length of the code. We explore this connection and this  pattern throughout the thesis. We will see how it can be  found in notable results like Coding Theorems for  one-to-one codes, Campbell's Coding Theorem, Arikan's  Guessing Theorem, Fano-like and Transportation-Cost  Inequalities and so on. Moreover, unearthing the pattern  allows us to generalise it to other information measures  and apply the technique in a variety of fields, including  Learning Theory, Estimation Theory and Hypothesis Testing.},
      url = {http://infoscience.epfl.ch/record/294547},
      doi = {10.5075/epfl-thesis-9122},
}
@book{estimationVanTrees,
publisher = {John Wiley \& Sons, Ltd},
isbn = {9780471221081},
title = {Detection, Estimation, and Modulation Theory},
author = {Harry L. Van Trees},
doi = {https://doi.org/10.1002/0471221082.ch1},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/0471221082},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/0471221082.ch1},
year = {2001},
keywords = {topical outline, approaches, organization},
abstract = {Summary This book deals with three areas of statistical theory, detection theory, estimation theory, and modulation theory. The goal is to develop these theories in a common mathematical framework and to demonstrate how they can be used to solve practical problems in diverse physical situations. In this chapter the author presents three outlines of the material. The first is a topical outline in which he develops a qualitative understanding of the three areas by examining some typical problems of interest. The second is a logical outline in which he explores the various methods of attacking the problems. The third is a chronological outline in which he explains the structure of the books.}
}

@article{estimationVanTrees1,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2242654},
 abstract = {This paper presents a lower bound, derived from the information inequality for the Bayes risk with respect to truncated priors under quadratic loss. It is discussed in cases where the regularity condition of Brown and Gajek is not always satisfied. A related result for the minimax risk is also given.},
 author = {Michikazu Sato and Masafumi Akahira},
 journal = {The Annals of Statistics},
 number = {5},
 pages = {2288--2295},
 publisher = {Institute of Mathematical Statistics},
 title = {An Information Inequality for the Bayes Risk},
 urldate = {2022-08-02},
 volume = {24},
 year = {1996}
}
@article{estimationVanTrees2,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2241876},
 abstract = {This paper presents lower bounds, derived from the information inequality, for the Bayes risk under scaled quadratic loss. Some numerical results are also presented which give some idea concerning the precision of these bounds. An appendix contains a proof of the information inequality without conditions on the estimator. This result is a direct extension of an earlier result of Fabian and Hannan.},
 author = {Lawrence D. Brown and Leslaw Gajek},
 journal = {The Annals of Statistics},
 number = {4},
 pages = {1578--1594},
 publisher = {Institute of Mathematical Statistics},
 title = {Information Inequalities for the Bayes Risk},
 urldate = {2022-08-02},
 volume = {18},
 year = {1990}
}

@INBOOK{estimationVanTrees3,
  author={Van Trees, Harry L. and Bell, Kristine L.},
  booktitle={Bayesian Bounds for Parameter Estimation and Nonlinear Filtering/Tracking}, 
  title={Bounds on the {B}ayes and {M}inimax {R}isk for Signal Parameter Estimation}, 
  year={2007},
  volume={},
  number={},
  pages={329-337},
  doi={10.1109/9780470544198.ch28}}
  
  @ARTICLE{estimationVanTrees4,
  author={Brown, L.D. and Liu, R.C.},
  journal={IEEE Transactions on Information Theory}, 
  title={Bounds on the Bayes and minimax risk for signal parameter estimation}, 
  year={1993},
  volume={39},
  number={4},
  pages={1386-1394},
  doi={10.1109/18.243453}}

  @article{etaTVKL,
title = {Relative entropy under mappings by stochastic matrices},
journal = {Linear Algebra and its Applications},
volume = {179},
pages = {211-235},
year = {1993},
issn = {0024-3795},
doi = {https://doi.org/10.1016/0024-3795(93)90331-H},
url = {https://www.sciencedirect.com/science/article/pii/002437959390331H},
author = {Joel E. Cohen and Yoh Iwasa and Gh. Rautu and Mary {Beth Ruskai} and Eugene Seneta and Gh. Zbaganu}
}

@article{ SDPIfDiv,
title = {Comparison of Contraction Coefficients for f-Divergences},
journal = {Problems of Information Transmission },
volume = {56},
pages = {103–156},
year = {2020},
doi = {https://doi.org/10.1134/S0032946020020015},
author = {Makur, A. and Zheng, L}
}

@INPROCEEDINGS{bayesianRiskFDiv,
  author={Vandenbroucque, Adrien and Esposito, Amedeo Roberto and Gastpar, Michael},
  booktitle={2022 IEEE International Symposium on Information Theory (ISIT)}, 
  title={Lower-bounds on the Bayesian Risk in Estimation Procedures via f–Divergences}, 
  year={2022},
  volume={},
  number={},
  pages={1106-1111},
  doi={10.1109/ISIT50566.2022.9834708}}

  @book{concrete_mathematics,
  added-at = {2008-04-23T10:18:06.000+0200},
  address = {Reading},
  author = {Graham, Ronald L. and Knuth, Donald E. and Patashnik, Oren},
  biburl = {https://www.bibsonomy.org/bibtex/2ccef670ef39186763ecd379d2cca1e0a/jaeschke},
  interhash = {a1450d7bb87f0107150d43a314a88326},
  intrahash = {ccef670ef39186763ecd379d2cca1e0a},
  keywords = {computer knuth latex math science},
  publisher = {Addison-Wesley},
  timestamp = {2014-07-28T15:57:31.000+0200},
  title = {Concrete Mathematics: A Foundation for Computer Science},
  year = 1989
}