\section{Method}
In this section, \gls{rasp} is formulated as a sequential decision-making problem with a \gls{mdp} and then we present our graph representation to depict assemblies. 
Based on this, we elaborate the proposed network GRACE, and demonstrate the assembly sequence generation.

\subsection{Problem Formulation}\label{sec:problem_formulation}
We describe the sequence prediction task for an assembly with $N$ parts as a \gls{mdp}~\cite{bellman1957markovian} with a discrete state space $\mathcal{S}$ and a high-level discrete action space $\mathcal{A}$.

Starting from state $\textbf{s}_t$ at time step $t$, executing action $a_t$ produces a reward $r_t$ and switches to state $\textbf{s}_{t+1} \sim p(\textbf{s}_{t+1}|\textbf{s}_t, a_t)$ with a transition function $\textit{p}$.
State $\textbf{s}_t \in {\{0, 1\}}^N$ is a binary vector indicating which parts are already placed in their target position by $1$ (i.e.\ assembled) otherwise by $0$. 
Action $a_t \in \{1, \ldots ,N\}$ represents the next part placement among the unplaced ones.
For \emph{feasible} assemblies, there are multiple different sequences leading to the final state, in which all $N$ parts are placed correctly.
For \emph{infeasible} assemblies, no sequence exists, due to constraints of different aspects spanning from part geometries to kinematic and dynamics regarding the robotic system.
Our objective is to learn a policy network $\pi_{\theta}(\textbf{s}_{t}) = a_t$ parameterized by $\theta$, which is optimized to imitate the assembly demonstrations 
${\tau_i=\{\textbf{s}_{i,1}, a^{exp}_{i,1}, \ldots, \textbf{s}_{i,T}, a^{exp}_{i,T}\}}$ in a dataset of $M$ sequences ${\mathcal{D} = {\{\tau_i\}}^{M}_{i=1}}$ and generalize across variants of different types and sizes at test time. 
In practice, our network predicts a set of multiple possible actions e.g. $K_t=\{a_{t,k}\}_{k=1}^{|K_t|}$ based on a tunable threshold to control the prediction quality. 

\subsection{Assembly Graphs}\label{method:aseembly_graphs}
We represent the overall structure of an assembly with a heterogeneous graph. 
To make this representation agnostic to the rotation and mirroring of the assembly structure, we employ only relative distances instead of absolute positions for the features of edges between surfaces.
More formally, given an assembly $A$ (Fig.~\ref{fig:GRACE_fig}) at state $\textbf{s}_t$ it is modeled as a graph ${\mathcal{G}_t=(\mathcal{V},\mathcal{E})}$ containing two types of nodes: part nodes $\mathcal{V}^p$ and surface nodes $\mathcal{V}^s$, and two types of edges: $\mathcal{E}^{s \text{-to-} s}$, connecting all surface nodes, and $\mathcal{E}^{s \text{-to-} p}$, connecting each surface node to its respective part. 
We detail each component as follows:
\subsubsection{Part Nodes} 
Responsible for encoding the current state of the assembly. A part node $\textit{v}^p_i \in \mathcal{V}^p$ is associated with a feature vector $\phi(\textit{v}_i^p)= [\textit{assembled-flag} \in \{ 0, 1 \},\ \textit{part-type} \in \mathbb{N},\ \textit{part-id} \in \mathbb{R}^d]$.
There are three atomic part types: \emph{long profile}, \emph{short profile} and \emph{angle bracket}.

\subsubsection{Surface Nodes} 
Different to the one in \cite{rodriguez2020pattern}, we associate each surface node $\textit{v}^s_i \in \mathcal{V}^s$ with the features $\phi(\textit{v}_i^s)= [\textit{surface-type} \in \mathbb{N},\ \textit{surface-id} \in \mathbb{R}^d]$. 
There are two surface types (\emph{long} and \emph{short}) for profiles and one (\emph{lateral}) for brackets.
Both the \textit{part-id} and \textit{surface-id} fields are encoded with a $d$-dimensional Sinusoidal Positional Encoding~\cite{vaswani2017attention}. 

\subsubsection{Surface-to-Surface Edges} 
We design a fully-connected graph for all surface nodes $\mathcal{V}^s$ to capture the relation between untouched surfaces, which is more fine-grained than those in \cite{rodriguez2020pattern} with only connects between touched surfaces.
These edges are assigned with a feature $\phi(\textit{e}_i) \in \mathbb{R}$, indicating the \emph{relation} between the two surfaces: 
$\phi(\textit{e}_i)=$ \textit{relative distance} (parallel); $1$ (belong to the same part); $-1$ (orthogonal); $0$ (same-surface loop).

\subsubsection{Surface-to-Part Edges} 

These connect each surface and part node pair 
$(\textit{v}^s_i,\textit{v}^p_j) \in \mathcal{V}^s \times \mathcal{V}^p$, where surface $\textit{v}^s_i$ belongs to the part $\textit{v}^p_j$.
This type of edges is not associated with any feature vector.

\subsection{Graph Assembly Processing Networks (GRACE)}\label{GRACE}
\begin{figure}[bt]
	\centering
	\includegraphics[width=1.\linewidth, height=4.7cm]{figures/assembly_graph_grace.pdf}
	\caption{\textbf{Illustration of Assembly Graph and GRACE.}
		Assembly Graph consists of edges connecting parts and their surfaces and edges among all part surfaces. In GRACE, the Part Block is shared for sub-graphs of part surfaces and the attached part, while Surface Block is for the sub-graph of all part surfaces.
	To predict scores for parts to be assembled next, we apply a prediction head on each spare part.
}
	\label{fig:GRACE_fig}
\end{figure}

Based on the formulation of a \textit{step-by-step} sequential decision-making process per each part in the assembly in \ref{sec:problem_formulation}, we introduce \textbf{GR}aph \textbf{A}ssembly pro\textbf{C}essing n\textbf{E}tworks, for short GRACE, $\pi_{\theta}: \mathcal{S} \rightarrow \mathcal{A}$, where $a_i = {\{y_i | y_i \geq \lambda \}}_{i=1}^N$, to extract useful information from the Assembly Graph and predict the next action given the current state of an assembly of $N$ parts. $\lambda \geq 0$ is a threshold used to control the quality of predicted sequences.
GRACE outputs a score per part $y_i \in [0,1], i \in \{1, \ldots ,N\}$, reflecting the probability of placing the $i$-th part next.
We further articulate the main components of this network (Fig.~\ref{fig:GRACE_fig}),
describe the algorithm for predicting the entire sequence of length $N$ by traversing predicted steps and the way we infer the feasibility of a given assembly.

\subsubsection{Surface and Part Blocks}\label{model_blocks}

The architecture is made of identical blocks, which are applied sequentially to obtain updated node features. 
Each block is made of a \gls{gat}~\cite{velivckovic2017graph, brody2021attentive}, an Instance Normalization layer~\cite{ulyanov2016instance} and a Tanh function.
We choose \gls{gat} as it allows to utilize the rich semantics of edge features for updating node features in our graph representation. 
Surface Blocks are applied on surface nodes $\mathcal{V}^s$ and surface-to-surface edges $\mathcal{E}^{s \text{-to-} s}$ for updating surface node features $\phi(\textit{v}_i^s)$,  
while Part Blocks are applied on surface nodes $\mathcal{V}^s$, part nodes $\mathcal{V}^p$ and surface-to-part edges $\mathcal{E}^{s \text{-to-} p}$ to update part node features $\phi(\textit{v}_i^p)$.

\subsubsection{Prediction Head and Loss Function}

To obtain a score per part, a fully-connected layer followed by a Sigmoid function is applied on each part node.
During training, we minimize the loss between the network outputs and the ground-truth sequence steps from a dataset of assembly sequences (see ~\ref{sec:problem_formulation}) using binary cross-entropy.
To note that, we apply this loss function for each part node separately. 
Our objective function (\ref{eq:03_loss}) includes an additional regularization term (\ref{eq:03_reg}), aiming at encouraging the network not to predict already placed parts:
\begin{align} 
L_{\theta} &= \sum_{i=1}^M \sum_{j=1}^{N_i} \left( \widehat{y}_{ij} \cdot \log (y_{ij}) + (1 - \widehat{y}_{ij}) \log (1 - y_{ij})\right) + \delta L_{\text{reg}}, \label{eq:03_loss} \\
L_{\text{reg}} &= \sum_{i=1}^M \sum_{j=1}^{N_i} f_{ij} \cdot y_{ij} \label{eq:03_reg},
 \end{align}
where $M$ is the number of data examples in the dataset, $N_i$ is the number of nodes in the $i$-th graph. Abusing the notations, we denote $y_{ij}$ and $\widehat{y}_{ij}$ the output score of the model $\pi_{\theta}$ and the ground-truth step in a sequence for the $j$-th node in the $i$-th graph respectively. 
$\delta$ is a weighing coefficient and $f_{ij}$ the value of the \emph{assembled-flag} in the input features.

\subsubsection{Predicting Sequences}
As described, GRACE predicts a set of possible next steps based on the current state of an assembly.
In order to generate a complete sequence (i.e.\ of length $N$), we repeatedly apply GRACE based on the current predicted state of the Assembly Graph. 
We devise an algorithm (Algo.~\ref{alg:03_walk_tree}) to traverse the assembly state tree using \gls{dfs}: 
 
Starting with the graph in its initial state $\mathcal{G}_0$ -- for all part nodes, \emph{assembled-flag}s are set to zero, the algorithm performs the following steps recursively: 
First, it checks the exit condition of the recursion -- if all parts are already in place. 
Next, it predicts the probability for each part node $y_{i}$ and picks those larger than the threshold $\lambda$, controlling the trade-off between precision and recall. 
Each of those nodes spawns a new branch individually.
Therefore, we set the \emph{assembled-flag} and call the recursion on the altered graph to retrieve possible sequences starting with the chosen node. 
Finally, we add the chosen nodes to the head of each returned sequence and return.

\begin{algorithm}[bt]
	\caption{Assembly State Tree Traversal}\label{alg:03_walk_tree}
	\begin{algorithmic}
		\Function{Traverse-Tree}{Model $M$, Assembly Graph $\mathcal{G}_t=(\mathcal{V},\mathcal{E})$, Threshold $\lambda$}
		\State $S \leftarrow$ list()
		\If{$\left( \forall v \in \mathcal{V}: \  v.\textit{assembled-flag} == 1 \right)$}
		\State \textbf{return} $S$ \Comment{Exit: all parts assembled}
		\EndIf
		\State $\mathbf{y}\leftarrow M(\mathcal{G}_t)$
		\For{$i \leftarrow 1$ to $|\mathcal{V}|$}
		\If{$\mathbf{y}[i] < \lambda$}
		\State \textbf{continue}
		\EndIf
		\State $\mathcal{G}_{t+1} \leftarrow$ copy($\mathcal{G}_t$)
		\State $[\mathcal{V}_{t+1}]_i.\textit{assembled-flag} \leftarrow 1$ \Comment{assembled node $i$ }
		\State $S_{\ast} \leftarrow$ \textsc{Traverse-Tree($M, \mathcal{G}_{t+1}, \lambda$)}
		\For{$s$ in $S_{\ast}$}
		\State $s_{\ast} \leftarrow [i] + s$ \Comment{Add current part to the sequence}
		\State $S$.append($s_{\ast}$)
		\EndFor
		\EndFor
		\State \textbf{return} $S$
		\EndFunction
	\end{algorithmic}
\end{algorithm}

\subsubsection{Feasibility Prediction}
To address the issue from infeasible assemblies, we develop two schemes to infer the feasibility (defined in ~\ref{sec:problem_formulation}) of a given assembly: 
(1) We use the number of predicted complete sequences (output by Algo.~\ref{alg:03_walk_tree}) as an indicator for the feasibility of a given assembly. 
If no sequences were retrieved, the assembly is predicted as infeasible.
(2) We aggregate the features of all part nodes from a pre-trained GRACE with a \textit{mean-pooling} operation, creating a feature vector for the entire assembly graph. 
This feature vector is then used to train a binary classifier for feasibility prediction, where 
we analyze several classifiers i.e. Support Vector Machines (SVMs), Multi-layer Perceptrons (MLPs) and Nearest Neighbor.

