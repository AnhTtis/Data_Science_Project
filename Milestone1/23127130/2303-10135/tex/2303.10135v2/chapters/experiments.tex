\section{Experiment}
In this section, we first describe the experimental setup such as our dataset, evaluation metrics and implementation details.
To note that we use the term \textit{size} to describe the number of parts of an assembly without prior notice. 
We evaluate the Sequence Prediction under two experimental protocols with 4-fold cross-validation: {(1) \textbf{intra-sized}:} the assemblies in training and test set share \textit{the same} sizes; {(2) \textbf{inter-sized}:} the assemblies in training and test set have \textit{different} sizes, where there are two sub-protocols: Many-to-one and One-to-Many (detailed in \ref{sec:inter-sized})
The results on Feasibility Prediction are presented before the failure analysis and ablation study. 

\subsection{Experimental Setup}
\subsubsection{Dataset}
We applied our in-house simulation software MediView to generate data of randomly generated synthetic aluminium assemblies whose sizes range from 3 to 7 (denoted by $A_i$, where $i$ is the size).
The simulation software was tasked with putting together the structures by brute-forcing all part orders, while considering the restrictions of part geometries or those imposed by the capabilities of a dual-armed robotic system \emph{KUKA LBR Med} (Fig.~\ref{fig:teaser}).
An illustration of this process is given in Fig.~\ref{fig:asp_exp_aluminum_assemblies}.
The resulting data consists of the following amount per size: $A_3:5717$, $A_4:2464$, $A_5:6036$, $A_6:2865$, $A_7:431$. 

We post-processed the simulation output to obtain the \textit{Placement Action} (required during training) -- the next possible placement actions given a state of an assembly in a feasible sequence. 
In addition, we derive the \textit{Feasibility} of each assembly based on the number of ground truth sequences e.g. 0 indicates an infeasible assembly.

\subsubsection{Metrics}
We use the following metrics for the sequence prediction task:
(1)~\textbf{Step-by-Step AUC} examines our method's predictive performance to infer the parts that should be assembled next given the current state by comparing the ground truth binary labels with the predicted step scores.
For this purpose we use the common Precision-Recall curve w.r.t. $\lambda$ and finally deriving an \gls{auc} score.
(2)~\textbf{Complete-Sequence AUC} evaluates the ability to infer the entire set of ground truth sequences, since a step-by-step evaluation only partially displays our application\footnote{Consider a method that predicts the first 97 steps correctly and fails in the 98-th step for a 100-parts-assembly.}.
We use Information Retrieval (IR) Precision-Recall~\cite{croft2010search}, devised for set prediction evaluation, computed as ${\text{IR-Precision} = \lvert RET \cap REL \rvert / \lvert RET \rvert}$, ${\text{IR-Recall} = \lvert RET \cap REL \rvert / \lvert REL \rvert}$, 
where $RET$ are the retrieved sequences and $REL$ are the relevant sequences (i.e.\ ones in the ground truth set).
Here, again, we plot an IR Precision-Recall curve and derive an \gls{auc} score.
(3) \textbf{\gls{pk}:} since in practice we only consider the highest scored predicted sequences, we also compute $\text{IR-Precision}$ while taking into account 
only the top-$k$ ones~\cite{herlocker2004evaluating}. 

For feasibility prediction, we use common binary classification metrics \emph{False Positive Rate}~$FPR$ and \emph{True Positive Rate}~$TPR$. In this setting, a positive instance is a feasible assembly and a negative an infeasible one.

\begin{figure}[bt]
	\centering
	\includegraphics[width=1\linewidth]{figures/7_parts_seq_examples.pdf}
	\caption{\textbf{Examples of \gls{asp} for Aluminum Assemblies.}
		We demonstrate the complexity of our task through the predictions for three different assemblies: starting from the top, there is a feasible assembly sequence predicted based on the assembly on the left. By decreasing the distance between part 5 and 4, it becomes infeasible due to limited space for the robot arm. Further, by removing part 7, with certain sequences such as the one shown in the figure, it is feasible. But this does not work with another, i.e. the false predicted sequence, because this one would cause collision.}
	\label{fig:asp_exp_aluminum_assemblies}
\end{figure}

\subsubsection{Implementation Details}\label{implementation}
We use PyTorch Geometric (PyG)~\cite{Fey_Fast_Graph_Representation_2019} to build the model which consists of $3$ surface blocks and $1$ part block with a latent-dimensionality of $94$.
For training, we choose a batch size of $256$ and a learning rate of $0.0022$ with Adam optimizer based on validation performance on $15\%$ of the training samples during hyper-parameter search.
Besides, we set the regularization weight in Eq.~\ref{eq:03_loss} to $0.3$ and length of positional encoding for node features to 16.
For the training and evaluation of the feasibility classifiers a balanced dataset is used. 
Our model includes $51.7 K$ trainable parameters and requires $4.06 \pm 0.15$ ms to infer the next feasible sequence step\footnote{Measured on NVIDIA GeForce GTX 1080.}.
More details are referred to our open-sourced code.

\subsection{Results} 
\subsubsection{Sequence Prediction for Intra-sized Assemblies}
The results are shown separately per assembly size (Tab.~\ref{exp:tab_knowledge_transfer_combined}), including the step-by-step and complete-sequence \gls{auc}, \gls{pk} scores for $k \in \{ 1, 2, 3 \}$ with a threshold of $0.5$. 
GRACE is able to perform perfectly on step prediction for all sizes. 
More relevant to our goal and more challenging than step prediction, our method can reach $1.0$ for small sizes (e.g. $3$ and $4$ parts) on the task of complete sequence prediction.
However, we can observe a slight drop for larger sizes (e.g. $5$, $6$ and $7$ parts), implying the greater complexity for large assemblies.
Hence, GRACE can effectively learn an useful inductive bias from our proposed graph representation when trained with similar sizes.
To note that, this performance has already reached that of the approach in~\cite{rodriguez2020pattern} and GRACE is able to generalize to larger sizes which the previous one is incapable of (see Tab. \ref{tab:comparison} for a qualitative comparison).

\begin{table*}[t]
	\centering
	\resizebox{\textwidth}{!}{
	\begin{tabular}{l | c c c c c | c c c c c}
		\toprule
		\multirow{1}{*}{Metrics} & \multicolumn{5}{c|}{Intra-sized} & \multicolumn{5}{c}{Inter-sized} \\
		% \cmidrule(lr){2-6}\cmidrule(lr){7-11}
		%\toprule
		 & $A_3$ & $A_4$ & $A_5$ & $A_6$ & $A_7$ & $A_3$ & $A_4$ & $A_5$ & $A_6$ & $A_7$ \\
		\midrule
		Step-by-Step AUC ($\uparrow$) & $1.00 \pm 0.00$ & $1.00 \pm 0.00$ & $1.00 \pm 0.00$ & $1.00 \pm 0.00$ & $1.00 \pm 0.00$  & $0.99 \pm 0.02$ & $1.00 \pm 0.00$ & $0.98 \pm 0.10$ & $0.98 \pm 0.00$ & $1.00 \pm 0.00$  \\
		\cmidrule(lr){1-11}
		Complete Sequence AUC ($\uparrow$) & $1.00 \pm 0.00$ & $1.00 \pm 0.00$ & $0.96 \pm 0.02$ & $0.93 \pm 0.03$ & $0.97 \pm 0.02$ & $0.97 \pm 0.03$ & $1.00 \pm 0.10$ & $0.87 \pm 0.03$ & $0.90 \pm 0.04$ & $0.95 \pm 0.07$ \\
		\cmidrule(lr){1-11}
		$P@1$ ($\uparrow$)& $1.00 \pm 0.00$ & $1.00 \pm 0.00$ &  $0.95 \pm 0.04$ & $0.96 \pm 0.06$ & $0.99 \pm 0.01$ & $0.99 \pm 0.01$ & $0.98 \pm 0.03$ & $0.90 \pm 0.09$ & $0.88 \pm 0.07$ & $0.96 \pm 0.05$ \\
		$P@2$ ($\uparrow$)& $1.00 \pm 0.00$ & $1.00 \pm 0.00$ &  $0.94 \pm 0.04$ & $0.95 \pm 0.07$ & $0.99 \pm 0.01$ & $0.99 \pm 0.01$ & $0.97 \pm 0.03$ &  $0.87 \pm 0.12$ & $0.87 \pm 0.07$ & $0.96 \pm 0.04$ \\
		$P@3$ ($\uparrow$)& -- & $1.00 \pm 0.00$ & $0.99 \pm 0.01$ & $0.95 \pm 0.08$ & $0.99 \pm 0.02$ & -- & $0.95 \pm 0.06$ & $0.93 \pm 0.10$ & $0.85 \pm 0.08$ & $0.96 \pm 0.04$ \\
		\bottomrule
	\end{tabular}}
	\caption{\textbf{Sequence Prediction Results for intra-sized and inter-sized (many-to-one) assemblies.}
	}
	\label{exp:tab_knowledge_transfer_combined}
\end{table*}

\subsubsection{Sequence Prediction for Inter-sized Assemblies} \label{sec:inter-sized}
To comprehensively evaluate the \textit{generalization} ability of GRACE across different sizes, a distinct limitation of previous works~\cite{rodriguez2020pattern,wells2019learning},
we further design two more \textit{challenging} sub-protocols under the inter-sized protocol.
\begin{itemize}
	\item \textbf{Many-to-one}: GRACE is trained on assemblies of mixed sizes but $i$, i.e. $A_{\forall j \neq i}$, and tested on $A_{i}$.
	\item \textbf{One-to-many}: GRACE is trained on a single-sized dataset $A_{i}$ and tested on all the other, i.e. $A_{\forall j \neq i}$.
\end{itemize}

1. \textit{Many-to-one}: This setting is similar to the intra-sized one except that we excluded assemblies of the size evaluated at test time from the training set.
When comparing the results in this setting to the intra-sized ones (Tab.~\ref{exp:tab_knowledge_transfer_combined}), we observe a slight performance decrease in \gls{auc} on step and sequence prediction.
However, note that $P@1$ and $P@2$ can still reach $\sim 1.0$ for small sizes ($3$ and $4$ parts) and $\sim 0.9$ for large sizes, indicating that GRACE is capable on generalizing to assembly variants with different sizes that have not been seen before.

2. \textit{One-to-many}: This setting is an inverse version of the previous one, which is more challenging, since the amount and diversity of the training set are much lower than before\footnote{We do not perform this experiment on $A_7$, as there are relatively small amount of assemblies with $7$ parts in the dataset.}. 
The results (lower triangular block in Tab.~\ref{exp:tab_knowledge_transfer_inter_sized_one2many}) provide a clear pattern that GRACE is able to obtain comparably better results for assemblies with less parts.  
For instance, trained with only $A_5$, GRACE preforms well for $A_3$ and $A_4$ which is reasonable as the constraints guiding smaller assembly structures are \emph{contained} in larger ones.
This shows the \textit{generalization} capability and \textit{sample efficient} learning ability (trained on single size and worked on smaller sizes) of our method. 
Nevertheless, the performance drops for larger assemblies (see the upper triangular block in Tab.~\ref{exp:tab_knowledge_transfer_inter_sized_one2many}).
We hypothesize that an increasing amount of items introduces new constraints that are not covered by the training data.
Thus, there might be a critical number of items containing all possible constraints that if included in the training data can lead to an overall generalizing model.

\begin{table*}[t]
	\centering
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{l | c c c c c | c c c c c}
			\toprule
			\multirow{2}{*}{Training Set} & \multicolumn{5}{c|}{Step-by-Step AUC ($\uparrow$) on assemblies of various sizes} & \multicolumn{5}{c}{Complete Sequence AUC ($\uparrow$) on assemblies of various sizes} \\
			%\cmidrule(lr){2-6}\cmidrule(lr){7-11}
			& $A_3$ & $A_4$ & $A_5$ & $A_6$ & $A_7$ & $A_3$ & $A_4$ & $A_5$ & $A_6$ & $A_7$ \\
			\midrule
			$A_4$ & $0.92 \pm 0.11$ & -- & $0.48 \pm 0.12$ & $0.41 \pm 0.11$ & $0.43 \pm 0.12$ & $0.93 \pm 0.09$ & -- & $0.28 \pm 0.12$ & $0.25 \pm 0.15$ & $0.25 \pm 0.15$ \\
			$A_5$ & $0.93 \pm 0.06$ & $0.89 \pm 0.07$ & -- & $0.78 \pm 0.14$ & $0.59 \pm 0.16$ & $0.83 \pm 0.07$ & $0.70 \pm 0.09$ & -- & $0.36 \pm 0.12$ & $0.24 \pm 0.07$ \\
			$A_6$ & $0.90 \pm 0.10$ & $0.89 \pm 0.11$ &  $0.93 \pm 0.04$ & -- & $0.59 \pm 0.16$ & $0.73 \pm 0.16$ & $0.68 \pm 0.24$ & $0.71 \pm 0.13$ & -- & $0.24 \pm 0.07$ \\
			\bottomrule
	\end{tabular}}
	\caption{\textbf{Sequence Prediction Results for inter-sized assemblies in one-to-many setting.}
	}
	\label{exp:tab_knowledge_transfer_inter_sized_one2many}
\end{table*}

\subsubsection{Feasibility Prediction}
In this setting, we examine the ability of our approach to detect infeasible assemblies.
For this experiment we consider GRACE trained on multiple sizes and test on the $A_5$ set.
As mentioned in~\ref{GRACE}, we compare the implicit approach via the number of predicted sequences (Algo.~\ref{alg:03_walk_tree}) and alternative schemes exploiting the graph representation of the pre-trained GRACE.
Therefore, we explore several binary classifiers i.e. SVMs, a Multi-layer Perceptron (MLP) and Nearest Neighbor.
As seen in Fig.~\ref{fig:04_classifier_comparision}, GRACE (\textit{\#sequences}) is able to detect infeasible assemblies (\gls{auc} of $0.97$).
However, training our method exclusively with feasible assemblies (\textit{\#sequences, feasilbe only}) results in a poor detection performance.
We hypothesize that by missing infeasible structures during training the method learns to always assemble an item leading to overconfidence.
Exploiting an additional scheme by adding one of the classifiers (except SVM with RBF kernel) maintains or even slightly improves the performance.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/5_feas_roc_curve.pdf}
    \caption{\textbf{Results of Feasibility Prediction.} Comparison of different proposed schemes for feasibility classification and an ablation study in which infeasible ones are unavailable based on assemblies with 5 parts.}
    \label{fig:04_classifier_comparision}
\end{figure}

\subsection{Failure Analysis}\label{seq:limitation_analysis}
To better understand the limitations of our method, we conduct an analysis of falsely predicted assembly sequences by our baseline model for $A_5$ and $A_6$.
Each of these false predicted sequences includes a \textit{false step}, i.e.\ action from which the sequence deviates from the corresponding ground truth sequences. 
Fig.~\ref{fig:failure_analysis} depicts the histogram of false steps binned by their predicted probability.
One can observe a large amount false steps performed in the beginning of the sequence (steps $1$ and $2$) with a high confidence.
On the other hand, wrong step predictions at the end of an assembly (steps $4$ and $5$) exhibit lower confidence scores. 
We hypothesize that this bias is a result of an inherit imbalance in our training setting. 
Our dataset samples could be thought of as nodes in a state tree, where earlier steps share state nodes closer to the root and later ones have independent nodes towards the leaves.
As each of these nodes is represented only once, there are fewer samples in the dataset attributed to earlier steps. 
This problem could be solved by balancing the training set based on the sequence step.

\begin{figure}[tb]
    \centering
        \includegraphics[width=1\linewidth]{figures/failure_analysis/5_parts_false_predictions.pdf}
    \caption{\textbf{Failure Analysis: false predicted sequences.} Histogram of predicted probability (for $A_5$ and $A_6$) in false steps reveals a drift in which GRACE is overconfident in mistakes preformed early. This is an evidence for an inherit bias in our training setting.}
    \label{fig:failure_analysis}
\end{figure}

\subsection{Ablation Study}\label{seq:04_ablation}
In Assembly Graph (\ref{method:aseembly_graphs}), both the part and surface node embeddings contain a $16d$ sinusoidal positional encoding~\cite{vaswani2017attention}. 
We conducted an ablation study to investigate the impacts from the values and permutation order thereof based on $A_5$.
(1) \textbf{Values}:
Initializing the positional encoding with random values dramatically decrease the performance of our method (Tab.~\ref{tab:pos_encoding}).
We hypothesize that these positions introduce \textit{geometrical bias}, which is helpful in our task.
\begin{table}[t]
	\centering
	\begin{tabular}{l | c }
		\toprule
		Poisitional Encoding & $A_5$ AUC ($\uparrow$) \\
		\midrule
		Baseline, sinusoidal encoding~\cite{vaswani2017attention} & 0.94 \\
		Random values & 0.37 \\
		No encodings & 0.07 \\
		\midrule
		Part permutations (test time) & 0.60 \\
		Surface permutations (test time) & 0.27 \\
		Part permutations (training and test time) & 0.97 \\
		Surface permutations (training and test time) & 0.10 \\
		\bottomrule
	\end{tabular}
	\caption[Positonal Encodings ablation study]{Ablation study into the contribution of positional encodings to our method.}\label{tab:pos_encoding}
\end{table}
(2) \textbf{Permutation Order}:
We number assembly parts and surfaces in a constant order. 
Parts are counted beginning from the one closest to the environment origin. 
Surfaces, on the other hand, are always numbered clockwise, starting from the respective part top. 
Permuting both part and surface orders \textit{only} at test time causes severe performance degradation, indicating constant numbering during training harms the model's ability to generalize (Tab. ~\ref{tab:pos_encoding}).
Interestingly, allowing permutations for only part order can boost the performance while this is not the case for surface permutations.
This demonstrates the importance of these features for the network to extract information from the parts' geometrical structure.

