\section{Background}
In this section, we briefly recap the concept of \glspl{gnn} and heterogeneous graphs, which are the base for our method.

\paragraph{Graph Neural Networks} A \gls{gnn} operates on an undirected graph $\mathcal{G}=(\mathcal{V},\mathcal{E})$ with nodes $\mathcal{V}$ and edges $\mathcal{E}$, where every node $v \in \mathcal{V}$ is assigned with a feature vector $\phi(v)$. 
It updates node features by exchanging information between neighboring nodes. 
This is done with multiple Message Passing layers~\cite{gilmer2017neural}. 
For each layer $l$, let $\mathbf{h}_i^0 = \phi(v_i)$ be the input features of node $v_i$ and $\mathcal{N}_i$ its set of neighboring nodes. Then we can define a three-step process to update these features:
\begin{enumerate}
	\item \emph{Gather} feature from neighboring nodes: ${{\{\mathbf{h}_j^{l-1}\}}_{j \in \mathcal{N}_i}}$.
	\item \emph{Aggregate} messages from the neighboring nodes: ${\mathbf{m}_i^l = g_{\omega}({\{\mathbf{h}_j^{l-1}\}}_{j \in \mathcal{N}_i})}$.
	\item \emph{Update} features of node $v_i$: ${\mathbf{h}_i^l = f_{\phi}^l(\mathbf{h}_i^{l-1}, \mathbf{m}_i^l)}$.
\end{enumerate}
The function $g_{\omega}$ can be either constant (e.g.\ sum) or learned during training.
The term $f_{\phi}$ is a \gls{nn} parameterized by $\phi$.
Both, $f_{\phi}$ and $g_{\omega}$, are shared across all nodes in the graph, making \gls{gnn}s efficient and independent of the number of nodes in the graph. 

In our proposed method we apply a Graph Attention Network (GAT)~\cite{velivckovic2017graph, brody2021attentive}, a popular variant of \gls{gnn}s, that defines $g_{\omega}$ as attention:
\begin{align}
\label{eq:2}
\mathbf{m}_i^l &= \sum_{j \in \mathcal{N}_i} \left( \alpha_{i,j} \cdot \mathbf{h}_j^{l-1} \right), \\
\mathbf{h}_i^l &=  \mathbf{W}_1 \cdot \alpha_{i,i}  \mathbf{h}_i^{l-1} + \mathbf{W}_1 \cdot \mathbf{m}_i^l, \\
\label{eq:3}
\alpha_{i,j} &= \frac{\exp \left(\mathbf{a} \cdot \sigma \left( \mathbf{W}_2 [\mathbf{h}_i^{l-1} \mathbin\Vert \mathbf{h}_j^{l-1} \mathbin\Vert e_{i,j}] \right) \right)}{ \sum_{k \in \mathcal{N}_i \cup \{ i \}} \exp \left(\mathbf{a} \cdot \sigma \left( \mathbf{W}_2 [\mathbf{h}_i^{l-1} \mathbin\Vert \mathbf{h}_k^{l-1} \mathbin\Vert e_{i,k}] \right) \right)},
\end{align}
where $\mathbf{W}_1$, $\mathbf{W}_2$, and $\mathbf{a}$ are learned, $\sigma$ is a Leaky ReLU activation function, and $[a \mathbin\Vert b]$ is a concatenation operator between $a$ and $b$.

\paragraph{Heterogeneous Graph} $\mathcal{G}=(\mathcal{V},\mathcal{E})$ generalizes graphs to multiple types of nodes and edges~\cite{sun2013mining}. 
Each node $v \in \mathcal{V}$ belongs to one particular node type $\psi_n(v)$ and analogously each edge $e \in \mathcal{E}$ to an edge type $\psi_e(e)$.
In~\cite{wang2019heterogeneous}, the authors extend \glspl{gat} to a heterogeneous graph setting.
This is accomplished by obtaining for each node a different updated feature vector per group of specific neighboring source node and edge types, and aggregating the features to obtain a single result, for instance using a sum. 
This formulation is essential, as every type of neighboring node may have a different feature dimension.
