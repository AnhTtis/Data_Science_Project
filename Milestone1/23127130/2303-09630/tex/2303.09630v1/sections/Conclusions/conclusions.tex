The use of nonlinear manifolds for projection-based reduced order models has been 
observed to yield improved accuracy relative to traditional linear subspace-reduced 
order models for a given ROM dimension, particularly for problems with slowly 
decaying Kolmogorov n-width such as advection-dominated ones.
 
These methods commonly use neural networks for manifold learning which often suffer
from high computational costs of training and evaluation relative to linear alternatives.
Recent works have proposed pruning \cite{choi} or network compression via teacher-student
training \cite{romor2022non} as approaches to reduce online evaluation costs.
However, these approaches do not address (and in general increase) offline training costs.

In this work, we develop and analyze a novel method that overcomes these disadvantages by 
training a neural network only on spatially subsampled versions of the high-fidelity
solution snapshots. This method coupled with collocation-based hyper-reduction and Gappy-
POD allows for efficient and accurate surrogate models in which neither the offline nor 
online cost scales with the high-fidelity model dimension.

We demonstrated the validity of our approach on a 2d Burgers problem.  These promising
initial results suggest that the use of hyper-reduced autoencoder architectures have
the potential to make nonlinear manifold projection-based reduced order models applicable
to large-scale problems which were previously not feasible due to impractical offline
training costs.