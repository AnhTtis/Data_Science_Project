\subsection{Full state reconstruction via Gappy-POD}\label{subsec:gappyPOD}


The trained hyper-reduced decoder $\hat{\vg}$ found in the previous step can now be used in the manifold LSPG with collocation \eqref{eq:hyperODeltaE_hat} to estimate the restriction of the FOM states $\vy^n$ on the stencil mesh. 

Let $\hat{\vy}^n = \hat{\vg}(\vz^n)$ be an estimate of $\mP_s \vy^n$ and $\bm{\Phi}_r = \bm{\Phi}[:, :r]$ a restriction of the POD matrix $\mPhi$ to the first $r$ principal modes, we propose to finally obtain an approximation $\vy^n$ by the Gappy-POD method \cite{everson1995karhunen}
\begin{equation}\label{eq:GappyApprox}
	\vy^n \approx \bm{\Psi} \hat{\vy}^n \quad\text{with}\quad \bm{\Psi} := \bm{\Phi}_r  \big( \mP_s \bm{\Phi}_r \big)^\dagger
\end{equation}
where for a matrix $\mA \in \R^{m \times n}$ we denote by $\mA^\dagger \in \R^{n \times m}$ its Moore-Penrose inverse. 

Notice that the application of the matrix $\bm{\Psi}$ to the output of the decoder $\hat{\vg}$ can be interpreted as extending $\hat{g}$ by adding a linear layer with weight $\bm{\Psi}$. Taking this idea a step further, we can add nonlinearities to \eqref{eq:GappyApprox} in order to encode prior knowledge on the solutions $\{ \vy^n \}$ of the FOM \eqref{eq:FOM}. For example, knowing that the solutions of the FOM are positive, and following \cite{romor2022non}, we can enforce the positivity of the approximations by adding a $\relu$ nonlinearity to the outputs of the Gappy-POD method:
 \begin{equation}\label{eq:GappyApprox_wRelu}
	\vy^n \approx \relu(\bm{\Psi} \hat{\vy}^n)
\end{equation}
with $\bm{\Psi}$ as in \eqref{eq:GappyApprox}. In the next sections we refer to the latter model as \textit{hyper-decoder + Gappy-POD + $\relu$}.