\subsection{Non-Linear Manifold Learning via Noisy Auto-Encoders}\label{subsec:AE}

The next step in our proposed strategy is to learn the map $\hat{\vg}: \R^{k} \to \R^{n_s}$ in \eqref{eq:ghat}. Notice that for the purpose of obtaining accurate reduced models of the FOM \eqref{eq:FOM}, the range of the map $\hat{\vg}$ needs only to approximate a \textit{superset} of the reduced manifold $\mathcal{M}_s$. This in particular means that we can choose the input dimension $k$ to be larger than $n_\mu + 1$.

% TODO rephrase part on GNAT

Following a recent line of works on nonlinear model reduction with neural networks \cite{lee2020model, choi, tencer2021tailored, romor2022non}, we use autoencoders for learning the map $\hat{\vg}$. Departing from this line of works though, and inspired by the variational autoencoders \cite{kingma2013auto}, we consider \textit{stochastic noisy encoders} $\hat{\ve} : \R^{n_s} \to \R^{k}$. Specifically, given an input vector ${\vy_s} \in \R^{n_s}$ the noisy encoder outputs two vectors corresponding to the mean and standard deviation of a gaussian latent vector

\[
	\hat{\ve}({\vy_s}) = \bm{\eta}({\vy_s}) + \bm{\sigma}({\vy_s}) \odot \bm{\xi} \quad \bm{\xi} \sim \mathcal{N}(0, \mI_k)
\]
where $\odot$ denotes the entrywise multiplication.
%\[
%	\hat{\ve}({\vy_s}) = 
%	\begin{bmatrix}
%           \eta_{y_s} \\
%           \sigma_{y_s} \\
%         \end{bmatrix},
%\]
The decoder is then trained to reconstruct the original vector $\vy_s$ from the stochastic latent vectors outputs of the encoder
\[
	\hat{\vg}( \hat{\ve}(\vy_s) ) \approx \vy_s.
\]

To train the encoder and decoder networks we use stochastic gradient descent so that for each batch $\{ \vy^{j} \}_{j=1}^b \subset \mathcal{Y}_{\text{train}}$ we consider the loss 
\begin{equation}\label{eq:loss_ae}
	L(\Theta_e, \Theta_g) := \frac{1}{2} \sum_{j = 1}^b {\| \hat{\vg}(  \hat{\ve}(\mP_s \vy^j)) - \mP_s \vy^j \|_2^2} \big/ \sum_{j = 1}^b {\| \mP_s \vy^j \|_2^2}
\end{equation}
where $\Theta_e$ and $\Theta_g$ are set of learnable parameters of the encoder and decoder networks respectively, and $\mP_s \in \R^{n_s \times N}$ restricts the vectors $\{\vy^j\}_j$ on the stencil mesh. 

Using a noisy encoder to train the decoder network can be interpreted as a regularization method that encourages the decoder to be stable to perturbations. We empirically observe that at the end of the training, the encoder tends to output vectors $\sigma_y$ with negligible norms, making it effectively deterministic during inference.  Nonetheless, the noisy autoencoders we find have low training and test error with respect to autoencoders trained without noisy encoders. Furthermore, we observe that training is more stable with respect to random initialization, that is networks trained from different random initialization have all similar performances, contrary to the standard autoencoders where performances may vary significantly from one run to the other. 
See the numerical experiments for more details.
