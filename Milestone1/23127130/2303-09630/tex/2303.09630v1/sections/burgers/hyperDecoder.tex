\subsection{Nonlinear manifold learning}

Since we have two variables $(u,v)$ the dimension of the FOM state $\vy$ is twice the one of the grid size. For the same reason, the dimension of the output of the \textit{hyper-decoder} and the input of the noisy-encoder is twice the dimension of the stencil size ($2 n_s$), i.e. $\hat{\vg}: \R^{k} \to \R^{2 n_s}$ and~$\hat{\ve}: \R^{2 n_s} \to \R^{k}$.  

Notice that even for the case when $n_c = 150$, the corresponding size of the input/output of the networks ($2 n_s$) is only a fraction of the size of the FOM state. This will allow us to train a single neural network for the two variables, saving computational resources. This is in contrast to recent works, as for example \cite{choi}, which define autoencoders on the full mesh and, to reduce the memory costs, train separate networks for each variable. Finally, training a single neural network for multiple variables allows for reducing the total size of the ROM state (latent vectors) forcing the decoder to learn a meaningful and shared representation of the data.

\medskip 

For all the experiments, we consider shallow feed-forward encoders and decoders with fixed architectures. Specifically, for and $\vy_s \in \R^{2 n_s}$ the forward pass in the noisy-encoder is given by
\[
\begin{aligned}
	\vh &= \relu(\mW_1^{(e)} \vy_s + \vb_1), \\
	\bm{\eta} &= \mW_{2,\eta}^{(e)} \vh + \vb_{2,\eta} \\
	\bm{\sigma} &= \exp((\mW_{2,\sigma}^{(e)},\vh + \vb_{2,\sigma})/(2*\ell)), \\
	\hat{\ve}(\vy_s) 
	&= \bm{\eta} +  \bm{\sigma} \cdot \bm{\xi} \quad\text{with}\quad \bm{\xi} \sim \mathcal{N}(0, \mI_k)
\end{aligned}	
\]
where $\exp$ and $\relu(z) = \max(0, z)$ are applied entrywise. The scalar $\ell$ is a hyperparameter that regulates the size of the noise. Tuning it on our dataset we find that $\ell = 1$ gives the best results and we fix it for all the next experiments. The parameters of the encoders are $\mW_1^{(e)} \in \R^{100 \times 2 n_s}, \vb_1 \in \R^{100}, \mW_{2,\eta}^{(e)} \in \R^{5 \times 100}, \vb_{2,\eta} \in \R^{5}$ and $\mW_{2,\sigma}^{(e)} \in \R^{100 \times 5}, \vb_{2,\sigma} \in \R^{5}$.

\medskip

The hyper-decoder is a two-layers $\leakyRelu$ network. For a latent vector $\vz \in \R^k$ we have
\[
	\hat{\vg}(\vz) = \mW_2^{(d)} \leakyRelu ( \mW_1^{(d)} \vz),
\]
where $\mW_1^{(d)} \in \R^{700 \times k}$ and $\mW_2^{(d)} \in \R^{2 n_s \times 700}$ are the parameters of the decoder, and $\leakyRelu(z) = \max(0, z) + 0.01 \min(0, z)$ is applied entrywise. 

When solving the time discrete ROM \eqref{eq:hyperODeltaE_hat}, we need to compute the Jacobian of the hyper-decoder. We compute this Jacobian via its closed form, which is faster than using automatic differentiation or approximating it with finite differences, and is given by
\[
	\mJ_{g} (\vz) = \mW_2^{(d)} \big[ \text{diag}(\mW_1^{(d)} \vx > 0) + 0.01 \text{diag}(\mW_1^{(d)} \vz < 0) \big] \mW_1^{(d)}
\]


\medskip

We train the encoder and decoder networks using PyTorch Lightning \cite{Falcon_PyTorch_Lightning_2019}, an open-source library of PyTorch.
 We use ADAM \cite{kingma2014adam} (a variant of stochastic gradient descent)  for 2000 epochs with a batch size of 120. The learning rate is initialized at $0.005$ and reduced by a factor of 0.8 if the training loss does not improve for 20 epochs until a minimum learning rate of $1e$-7. The network weights are initialized via the standard PyTorch initialization.

