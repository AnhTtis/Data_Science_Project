\subsection{Nonlinear manifold LSPG with collocation	} 

The trained hyperdecoder can now be used in the nonlinear manifold LSPG formulation \eqref{eq:hyperODeltaE_hat}. The residual is defined by choosing for time discretization the implicit methods BDF1 and BDF2. 
%In particular, we find that BDF2 performs consistently better than BDF1 even when using it in conjunction with GNAT-like techniques.   

 The manifold LSPG is solved using the python interface of Pressio \cite{rizzi2020pressio}, an open-source library for model-reduction that enables large-scale projection-based reduced order models for nonlinear dynamical systems. The minimization problem \eqref{eq:hyperODeltaE_hat} is solved with the Gauss-Newton method, where Pressio-demoapps is used to provide Pressio with the residual $\vr$ and its Jacobian $\mJ_r$ evaluated on the stencil mesh.
 
Solving the ROM \eqref{eq:hyperODeltaE_hat} for a given parameter $\mu$, we obtain the ROM states $\{ \vz^n_\mu \}_{n}$, and estimate the FOM states $\{ \vy^n_\mu \}_{n}$ by $\tilde{\vy}^n_\mu := \relu(\bm{\Psi} \hat{\vg}(\vz^n_\mu))$ where $\bm{\Psi}$ is the Gappy matrix defined in \eqref{eq:GappyApprox} (hyper-decoder + Gappy-POD + $\relu$). The number $r$ of POD modes used to define $\bm{\Phi}_r$ is chosen by finding the number of modes $r$ that perform the best on the training and validation set. 
 
 In Figure \ref{fig:qualitative} we compare the final state predictions with the target solutions for the variable $u$, for two different parameters in the test set ($\mu = 0.26$ and $\mu = 0.62$) and with a sample mesh size of $n_c = 50$. The errors are relatively low across the whole domain, increasing moderately at the wavefront of the solutions.
 
 \begin{figure}[h]
    \centering % <-- added
\begin{subfigure}{0.3\textwidth}
  \includegraphics[width=\linewidth]{./imgs/qualitative/target_mu1_v2.pdf}
  \caption{Target $u$ ($\mu = 0.26$)}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.3\textwidth}
  \includegraphics[width=\linewidth]{./imgs/qualitative/predictions_mu1_v2.pdf}
  \caption{Predictions $u$ ($\mu = 0.26$)}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.3\textwidth}
  \includegraphics[width=\linewidth]{./imgs/qualitative/error_mu1_v2.pdf}
  \caption{Error (target - predictions)}
\end{subfigure}
\medskip
\begin{subfigure}{0.3\textwidth}
  \includegraphics[width=\linewidth]{./imgs/qualitative/target_mu2_v2.pdf}
  \caption{Target $u$ ($\mu = 0.62$)}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.3\textwidth}
  \includegraphics[width=\linewidth]{./imgs/qualitative/predictions_mu2_v2.pdf}
  \caption{Predictions $u$ ($\mu = 0.62$)}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.3\textwidth}
  \includegraphics[width=\linewidth]{./imgs/qualitative/error_mu2_v2.pdf}
  \caption{Error (target - predictions)}
\end{subfigure}
\caption{Comparison of the target final states with the predictions via Gappy-POD + hyperdecoder, and corresponding errors (target - predictions) fore two target parameters and sample-mesh size $n_c = 50$.}
\label{fig:qualitative}
\end{figure}

 In the next sections, we evaluate the performance on test cases with $\mu \in \mathcal{D}_{\text{test}}$, using the following two metrics: 
 \[
 	\text{Maximum relative error} := \max_{n \in [N_t]} \Bigg( \frac{\| {\vy}^n_\mu - \tilde{\vy}^n_\mu\|_2}{\|{\vy}^n_\mu\|_2} \Bigg)
 \]	
 and 
  \[
 	\text{${L}^2$ relative error} :=  \frac{\sum_{n \in [N_t]} \| {\vy}^n_\mu - \tilde{\vy}^n_\mu\|_2}{\sum_{n \in [N_t]} \|{\vy}^n_\mu\|_2},
 \]	
 where for a natural number $n$ we let $[n] := \{1,2, \dots, n\}$.
 
 
 \subsection{Effects of the noisy-training}
 
In this section, we consider the effects of training via the noisy autoencoder described in Section \ref{subsec:AE}. 
We train 14 models using noisy autoencoders from different random initialization and 14 models using standard encoders. 
The models have essentially the same encoder-decoder architecture, 
the only difference being that the models in the first group perturb the output of the encoder 
with Gaussian variables with learnable variance. 

We consider a sample mesh of $n_c = 50$ and use the trained models to solve 
manifold LSPG with collocation for the validation parameter $\mu = 0.4$. 
In Figure \ref{fig:scatter} we plot the maximum relative errors using BDF1 and BDF2 
time integration. Notice that the models trained with noisy autoencoders cluster in the 
lower left side of the plane where the errors with BDF1 and BDF2 are both low. 
	
	
During the training of the noisy autoencoder, we observe that the standard deviation 
$\sigma(\vy)$ of the noisy encoder decreases progressively until it becomes negligible 
at the end of training. This suggests that the noisy autoencoder can be viewed as a form 
of adaptive noisy training, where the level of noise decreases to zero over time. 
The use of noise injection as a regularization method has been studied extensively in prior 
work (e.g., \cite{bishop1995training, poole2014analyzing, dhifallah2021inherent}).
	
Based on these observations, we use noisy autoencoders with the noise term set to zero in the remaining experiments. 
	
	 \begin{figure}[h]
      \centering
        {\includegraphics[scale=0.7]{./imgs/noisy_AE/extrap_time.pdf}}
            \caption{Maximum relative errors for models trained with noisy and standard autoencoders.}\label{fig:scatter}
\end{figure} 
 

 
 
 
 \subsubsection{Effects of the sample mesh size $n_c$}
 
 We analyze the influence of the sample mesh size, varying $n_c \in \{ 50, 100, 150 \}$ and using the stencil meshes in Figure \ref{fig:meshes}. In Figure \ref{fig:nc_L2} we report the $L^2$ and maximum relative errors for the FOM state reconstruction using our proposed hyper-decoder with LSPG and implicit time integration (BDF1 and BDF2). These experiments demonstrate our proposed method is accurate and in particular, integrating in time the ROM with BDF2 outperforms BDF1, with errors below 1\% across all the test parameters inside the training intervals and the different sample/stencil mesh sizes.
 
 \begin{figure}[h]
      \centering
      %\subcaptionbox{$n_c = 50$ ($n_s = 161$)}
        {\includegraphics[scale=0.5]{./imgs/vary_nc/BDF1_L2_v2.pdf}}
        %\hskip -6ex
       %\subcaptionbox{$n_c = 100$ ($n_s = 296$)}
        {\includegraphics[scale=0.5]{./imgs/vary_nc/BDF2_L2_v2.pdf}}
      %\subcaptionbox{$n_c = 50$ ($n_s = 161$)}
        {\includegraphics[scale=0.5]{./imgs/vary_nc/BDF1_inferr_v3.pdf}}
        %\hskip -6ex
       %\subcaptionbox{$n_c = 100$ ($n_s = 296$)}
        {\includegraphics[scale=0.5]{./imgs/vary_nc/BDF2_inferr_v3.pdf}}
     \caption{$L^2$ relative and maximum errors of the hyper-decoder with Gappy-POD reconstruction of the full state for varying sizes of the sample mesh size ($n_c$). }\label{fig:nc_L2}
\end{figure} 

The red dashed line in Figure \ref{fig:nc_L2} delimits the interval of the training parameters. We observe that the errors increase at the extrapolation parameters (those outside the training interval). Nonetheless, in applications where a maximum relative error below 8\% would be acceptable, our proposed method with a sample-mesh size of  $n_c = 50$ and BDF2 time integration can be used with a trust region of $[0.8, 0.66]$.  
 
 


 

 
Finally, previous works used hyper-reduction with GNAT-like methods to regularize the solution of the reduced order models with BDF1 time integration. Here we observe that a simple collocation strategy with more accurate time integration is enough to obtain accurate solutions of the ROMs. Notice moreover, that the overhead computational and memory costs of using BDF2 instead of BDF1 are proportional to the ROM state size, contrary to GNAT-like methods that require matrix multiplications of order $n_s$.  In practice, we find that using BDF1 and BDF2 for the ROM solutions leads to comparable running times.	


 
    
% \begin{figure}[h]
%      \centering
%      %\subcaptionbox{$n_c = 50$ ($n_s = 161$)}
%        {\includegraphics[scale=0.5]{./imgs/vary_nc/BDF1_L2_v2.pdf}}
%        %\hskip -6ex
%       %\subcaptionbox{$n_c = 100$ ($n_s = 296$)}
%        {\includegraphics[scale=0.5]{./imgs/vary_nc/BDF2_L2_v2.pdf}}
%      %\subcaptionbox{$n_c = 50$ ($n_s = 161$)}
%        {\includegraphics[scale=0.5]{./imgs/vary_nc/BDF1_inferr_v3.pdf}}
%        %\hskip -6ex
%       %\subcaptionbox{$n_c = 100$ ($n_s = 296$)}
%        {\includegraphics[scale=0.5]{./imgs/vary_nc/BDF2_inferr_v3.pdf}}
%     \caption{$L^2$ relative errors of the hyper-decoder with Gappy-POD reconstruction of the full state for varying size of the sample mesh size ($n_c$). }\label{fig:nc_L2}
%\end{figure}
 
\subsubsection{Extrapolation in time}

We next study the ability of the proposed approach to extrapolate in time, that is to obtain accurate solutions even outside the time interval $[0, 2]$ that was used during training. We consider the 2d burgers equation in the time interval $[0, 2.5]$ with test parameters 
\[
	\mu \in \mathcal{D}_{\text{test}}' = \{ 0.16 0.22 0.28 0.34 0.4  0.46 0.52 0.58 0.64 \}.
\]
 We apply our proposed approach to estimate the FOMs in the extended time interval $[0, 2.5]$ and plot the $L^2$ and maximum relative errors in Figure \ref{fig:extrap}. We observe that a maximum relative error below 1.2\% is achieved for parameters $\mu \in [0.16, 0.46]$ and this trust region can be extended to $\mu \in [0.16, 0.52]$ accepting a maximum relative error below 3\%. These results suggest that the hyper-decoder has learned a meaningful representation of the reduced solution manifold, and our approach can (to some extent) be used to extrapolate in time. The decrease in accuracy for larger parameters is to be expected. In this range, the solutions travel more and the interactions with the boundaries become more important, making extrapolation harder.
 
  \begin{figure}[h!]
      \centering
        {\includegraphics[scale=0.7]{./imgs/extrap_time/extrap_time.pdf}}
            \caption{Relative errors for the time extrapolation in the interval $[0,2.5]$. }\label{fig:extrap}
\end{figure} 
 
\subsubsection{Effects of training size}

In this section, we analyze the effects of the number of parameters used in the training set. We compare a model trained on $\mathcal{D}_\text{train}$ with $n_{tr} = 10$ number of parameters as in \eqref{eq:Dtrain}, and a model trained on  $\mathcal{D}_\text{train}'$ with $n_{tr} = 4$ number of parameters as in \eqref{eq:Dtrainp}). In both cases we use a sample mesh of size $n_c = 50$. The results summarized in Figure \ref{fig:gen} show that the errors of the two models are comparable and below the $1\%$ threshold in the training set interval. The model trained with $n_{tr} = 10$ perform slightly better than the model trained with $n_{tr} = 4$.

 \begin{figure}[h!]
      \centering
      %\subcaptionbox{$n_c = 50$ ($n_s = 161$)}
        %{\includegraphics[scale=0.5]{./imgs/gen/gen_BDF1.pdf}}
        %\hskip -6ex
       %\subcaptionbox{$n_c = 100$ ($n_s = 296$)}
        {\includegraphics[scale=0.7]{./imgs/gen/gen_BDF2.pdf}}
      %\subcaptionbox{$n_c = 50$ ($n_s = 161$)}
     \caption{Maximum errors of the hyper-decoder with Gappy-POD reconstruction of the full state for varying number $n_{tr}$ of parameters in the training set. }\label{fig:gen}
\end{figure} 


