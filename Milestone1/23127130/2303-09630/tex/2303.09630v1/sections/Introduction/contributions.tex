\subsection{Our contributions}

The nonlinear map $g: \R^{k} \to \R^{N}$ used to reduce the FOM \eqref{eq:FOM} and approximate the solution manifold $\mathcal{M}$ is, in general, unknown and learned from data. 
Recent works propose to parameterize and learn $g$ using neural networks \cite{lee2020model,choi,romor2022non}, demonstrating improved performances (more accurate predictions) with respect to traditional approaches such as Proper Orthogonal Decomposition (POD). 
One disadvantage of these approaches is that the training of the neural networks models is often computationally costly requiring many passes of Stochastic Gradient Descent over the data with thousand of computations of the order $\mathcal{O}(N)$, which, for large scale problems can be impractical. 

In this work, we develop and analyze a method to overcome the above issue by learning a nonlinear map $\hat{\vg}$ which predicts the values of the solutions $\vy$ only on the stencil points. 
Specifically, let $\mP_s \in \{0,1\}^{n_s \times N}$ be the sampling matrix that picks out the $n_s$ indices of the stencil mesh, our goal will then be to approximate the solutions $\vy \in \mathcal{M}$ restricted on the stencil mesh
\begin{equation}\label{eq:ghat}
    \mP_s \vy \approx \hat{\vy} := \hat{\vg}(\vz),
\end{equation}
where $\hat{\vg} : \R^k \to \R^{N_s}$ is a nonlinear map that we refer to as the \textit{``hyper-decoder''}. 
This approach constitutes a departure from the previous nonlinear projection-based model reduction methods, in that the primary objective of the nonlinear map $\hat{g}$ is to approximate the reduced manifold 
\[
\mathcal{M}_s = \{\mP_s^T \vy(t, \vmu) \: | \: t \in [0,T], \,\vmu \in \R^{n_\mu} \},
\] 
rather than directly the full manifold $\mathcal{M}$. 

We can then use our proposed  ``hyper-decoder'' for solving the manifold LSPG with collocation which, with some abuse of notation, takes the form
\begin{equation}\label{eq:hyperODeltaE_hat}
    \vz^n := \argmin_{\vz \in \R^k} \| \mP_c \vr^n(\hat{\vg}(\vz); \hat{\vy}^{n-1}, \dots, \hat{\vy}^{n-\ell(t^n)}, \vmu) \|_2^2 \quad \text{for}\;\; n = 1, \dots, N_t,
\end{equation}
where $\hat{\vy}^{n} = \hat{g}(\vz^n)$ provides an approximation of the  state $\mP_s^T \vy^n$ restricted on the stencil mesh.

Once the approximation $\hat{\vy}^n$ is obtained, we estimate the full state $\vy^n$ by employing Gappy-POD \cite{everson1995karhunen}.

In summary, our method uses recent advances in deep learning to learn an accurate representation of the nonlinear manifold of the subsampled solutions. 
This nonlinear representation is used to define the reduced order models, which are then solved via LSPG. 
We then use more traditional and fast methods based on efficient matrix factorization to obtain accurate predictions of the high-fidelity states. 
In the next section, we detail our proposed approach.

%\subsection{Organization of the paper}
%

%Finally, in Section \ref{sec:num_res} we apply our proposed method to a 2d burgers equation. 


