\section{Experiments: Object Detection}
\subsection{Experimental Settings}
\paragraph{Datasets} 
We conduct experiments on two standard benchmark datasets in object detection, namely PASCAL VOC~\cite{pascal} and MS COCO~\cite{coco}.
We follow \cite{bof} and use the combination of PASCAL VOC 2007 \textit{trainval} (5k images) and 2012 \textit{trainval} (12k images) for training.
Together they make up 16,551 images of 20 categories of common, real-world objects, each with fully annotated bounding boxes and class labels.
The evaluation is done on PASCAL VOC 2007 \textit{test} set (5K images).
MS COCO~\cite{coco} is composed of  80 object categories and is 10 times larger than PASCAL VOC.
We train on \textit{train2017} (118K images) and evaluated on \textit{val2017} (5K images).

\paragraph{{Baseline models}}
We use three main baseline models to evaluate the performance of our proposed LossMix. 
The first one is a baseline, bare bone model without any Mixup-like data augmentation. 
Secondly, we compare with the ``Noise" mixing strategy used by \cite{afan} for unsuperivsed domain adaptation. In a nutshell, it mixes input image A with a small amount of image B (e.g. $\lambda < 0.1$) acting only as color augmentation and discards any objects exists in B. 
Finally, we compare LossMix against 
\textit{Union} mixing, the most popular approach used by prior studies~\cite{union_da_fewshot_acrofod,union_instseg_seesaw,union_mot_bytetrack,bof,union_ssl_dual,union_ssl_instant_teaching}.

\paragraph{{Implementation Details}} 
We leverage the open-source PyTorch-based Detectron2~\cite{detectron2} repository as our object detection codebase for experimentation.
We use Faster RCNN~\cite{faster_rcnn} with ResNet~\cite{resnet}--FPN~\cite{fpn} backbone as our baseline model.
By default, ImageNet1K~\cite{imagenet} pretrained weights are used to initialize the networks.
Unless otherwise specified, we use a batch size of 64 for faster convergence, an initial learning rate of 0.08, and the default step scheduler from Detectron2.
We train PASCAL VOC for 18,000  iterations, which is about 70 epochs, and MS COCO for 270,000 iterations, equating roughly 146.4 epochs.
All experiments were trained with 8 NVIDIA GPUs, either V100 or A100.



\subsection{Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PASCAL %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{tab/det_voc}
\paragraph{PASCAL VOC dataset}
\Tab{det_voc} shows the results for LossMix in comparison with the baseline model and prior methods on PASCAL VOC dataset.
First, we can see that all data mixing methods offer some improvements over the base Faster RCNN model, even ``Noise" despite the weak mixing augmentation.
This validates our interest in studying data mixing regularization for object detection.
Second, among the detectors that deploys different mixing strategies, those with LossMix clearly outperform others. 
Specifically, our method yields up to  $+0.9$AP compared to Union, $+1.5$AP compared to Noise, and $+2.7$AP compared to no-mixing baseline.
Overall, LossMix achieves the best performance across all three evaluation metrics, AP, AP$_{50}$, and AP$_{75}$, as well as both backbones, ResNet-50 and ResNet-101 FPN.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% MSCOCO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{tab/det_coco}
\paragraph{MS COCO dataset}
Our results for MS COCO dataset is shown in \Tab{det_coco}
Here, we can see that the promising performance of LossMix on PASCAL VOC is also generalizable to a much bigger (10$\times$) dataset such as MS COCO as well.
In particular, our method again achieves the best overall AP scores at $41.82$ for ResNet-50 and $44.07$ for ResNet-101.
When considering all metrics, LossMix also outperforms the previous state-of-the-art mixing techniques in the majority of cases.
We believe these results, coupled with the simplicity of loss mixing operation, make LossMix an appealing alternative to the current unweighted union practice for data mixing in object detection.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Ablation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Ablation study}
\input{tab/det_abl}
Although at its core, LossMix simply proposes the mixing of loss signals, there can be different implementation variations and hyper-parameters. 
\Tab{det_abl} provides an ablation study investigating how these options affect the performance of LossMix.
Overall, LossMix is robust with these configurations; all offer improvement over the Baseline (no data mixing) and the popular Union~\cite{union_da_fewshot_acrofod,union_instseg_seesaw,union_mot_bytetrack,bof,union_ssl_dual,union_ssl_instant_teaching} strategy.
Morevover, we can see that although mixing of classification losses ($\mathcal{L}_{cls}^{rpn}$ and $\mathcal{L}_{cls}^{roi}$) contributes the most, mixing box regression losses ($\mathcal{L}_{reg}^{rpn}$ and $\mathcal{L}_{reg}^{roi}$) can also help, yielding better localization results as shown by AP$_{75}$ as well as better overall AP.
It is important to highlight that even when incorporating only box classification losses, our proposed method goes beyond image-level Mixup.
This is because, by re-weighting $\mathcal{L}_{cls}$, LossMix effectively addresses a range of challenges related to spatial misalignment, background information, and object plurality that we have discussed in previous sections. 
In contrast, Mixup is not specifically designed to tackle these and cannot be adopted directly for object detection.
This underscores the distinct advantages of our approach.

