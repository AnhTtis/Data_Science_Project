\input{tab/voc_clip}
\input{tab/voc_water}


\section{Experiments: Domain Adaptation}
\subsection{Experimental Settings}
\paragraph{Datasets} We conduct our experiments for cross-domain object detection using two popular and challenging real-to-artistic adaptation setups~\cite{
HTCN,
umt,
DM,
amt,
SWDA,
SCL,
CRDA}:
PASCAL VOC~\cite{pascal} $\rightarrow$ Clipart1k~\cite{clipart_watercolor}
and
PASCAL VOC~\cite{pascal} $\rightarrow$ Watercolor2k~\cite{clipart_watercolor}.
Compared to PASCAL VOC, Clipart1k and Watercolor2k \cite{clipart_watercolor} represent large domain shifts from real-world photos to artistic images. 
Clipart1k dataset shares the same set of object categories as PASCAL VOC and contains a total of 1000 images. We split these into 500 training and 500 test examples. 
Watercolor2k dataset has 2000 images of objects from 6 classes in common with the PASCAL VOC. We also split the dataset in halves to obtain 1000 training and 1000 test images.


\vspace{-1em}
\paragraph{{Implementation Detail}} 
We leverage the open source code of the state-of-the-art Adaptive Teacher~\cite{amt} framework (CVPR 2022). 
The codebase is also built on top of Detectron2~\cite{detectron2}.
For fair comparison against previous works \cite{umt,amt}, we use Faster RCNN \cite{faster_rcnn} with ResNet-101 \cite{resnet} backbone. 
We follow the setup of \cite{amt} and scale all images by resizing their shorter side to 600 while maintaining the image ratios. 
We keep all loss weight for labeled and pseudo-labeled examples to be 1.0 for simplicity and use the default weight of 0.1 for the discriminator branch. We also keep the confidence threshold as 0.8. 
We notice that the set of hyper-parameter reported in the \cite{amt} is not suitable for the open-sourced code. 
Thus, we tune them to get the best performance for Adaptive Teacher for fair comparison and keep their original set of strong-weak augmentations.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results}
\paragraph{PASCAL VOC $\rightarrow$ Clipart1k}
We compare with state-of-the-art methods in cross-domain object detection using the popular PASCAL VOC $\rightarrow$ Clipart1k adaptation (\Tab{voc_clip}).
We report an mAP of 50.33\% across all object categories, achieving the new state-of-the-art performance with +3.5\% improvement on top of the prior state of the art set by the recent Adaptive Teacher~\cite{amt}.
Despite AT's strong performance, our results suggest that large domain shifts are still challenging and reveal potential biases toward source domain, e.g. inherently in the warmup procedure of Mean Teacher.
By strategically leveraging LossMix, we are able to mitigate these problems and further improve accuracy.

\vspace{-1em}
\paragraph{Comparison with other data mixing strategies}
\Table{voc_clip} also presents our comparison to different mixup variations used by existing methods, namely Union~\cite{union_da_fewshot_acrofod,union_instseg_seesaw,union_mot_bytetrack,bof,union_ssl_dual,union_ssl_instant_teaching} and Noise~\cite{afan}. 
Specifically, AFAN \cite{afan} deploys a small $\lambda$ value on target domain image without any pseudo labels. This strategy is similar to our noise mixing during the warmup, but is used throughout the training.
Note that this approach performs worse than our AT basedline.
This is because although noise mixing could be helpful in general, as shown by both AFAN \cite{afan} and our following ablation studies, heavily relying on it in the adaptation phase of Mean Teacher can lead to bias towards the source domain due to the fact that ``mixed-in" target information is only limited to a tiny amount to act as a domain-aware augmentation. Indeed, we believe for cross-domain mean teacher, the pseudo labels are much stronger target signals and should be taken advantage of appropriately.
We also see sub-optimal results for Union~\cite{bof} due to errors in the approximation of unweighted union, similar to object detection experiments.


\paragraph{PASCAL VOC $\rightarrow$ Watercolor2k}
Next, we are interested in answering the question whether or not the encouraging gains observed in PASCAL VOC $\rightarrow$ Clipart1k can be reproduced on a different dataset. To do this, we use Watercolor2k and evaluate the performance of PASCAL VOC $\rightarrow$ Watercolor2k adaptation.
Note that after experimenting with Clipart1k, we narrowed down our set of hyper-parameters to ones that work best for both Adaptive Teacher and our method for fair competition.
For Watercolor2k, to test our method's robustness, we directly perform grid search on this small set of hyper-parameters without any further tuning or manual supervision.
Nonetheless, even without exhaustive tuning, our results in \Table{voc_water} show that we can still outperform AT (mAP=57.7) with a +1.5 improvement and archive mAP=59.3.
