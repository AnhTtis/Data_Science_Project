\vspace{-1.5em}
\section{{Introduction}} \label{sec:intro}
Over the past decade, object detection has made remarkable progress, with impressive scores on challenging benchmarks such as MS COCO~\cite{coco}. 
However, state-of-the-art detectors still suffer from poor generalization abilities and struggle with data outside their training distribution, especially under domain shifts~\cite{survey_da_det_ssci_20,survey_uda_det_21}. 
Recently, data mixing techniques, pioneered by Mixup~\cite{mixup}, have emerged as an effective augmentation and regularization method for improving accuracy and robustness in deep neural networks.
These techniques~\cite{
supermix,
stylemix,
puzzlemix,
manifold_mixup,
cutmix,
mixup} 
use a linear interpolation of both images and their labels to generate synthetic training data.
The ``mixing" process encourages the model to behave linearly between training examples, which can potentially reduce undesired oscillations for out-of-distribution predictions.
Since its introduction in 2018, Mixup has garnered increasing attention and has been widely adopted for image classification problems~\cite{
supermix,
stylemix,
puzzlemix,
tokenmix,
automix,
over_train_mixup,
mixup_cls_fixbi,
regmixup,
manifold_mixup,
mixup_cls_dual,
mixup_cls_adv,
cutmix,
mixup}.
This motivates us to investigate the potential of Mixup augmentation for enhancing object detection. 


\input{fig/teaser}


Unfortunately, Mixup cannot be applied to object detection task off the shelf.
On one hand, the mixing of the category label of object instances is non-trivial (\Fig{wrong_bbox_mixing}) due to issues such as spatial misalignment, foreground/background distinctions, and the plurality of instances. 
In contrast to classification, where images share the same shape and each only has one class label, object detectors need to handle images and objects of different aspect ratios and positions. 
This makes it impossible to guarantee the alignment of mixed objects and creates much more complexity for the interpolation of class labels.
\Fig{wrong_bbox_mixing} (top) provides a visualization of these challenges.
In addition, ground truth object detection annotations are composed of bounding box coordinates that cannot be naively interpolated without disturbing the localization ground truth (\Fig{wrong_bbox_mixing}, bottom).


\input{fig/wrong_bbox_mixing}


A number of previous studies~\cite{union_da_fewshot_acrofod,union_instseg_seesaw,union_mot_bytetrack,bof,union_ssl_dual,union_ssl_instant_teaching} have deployed a  workaround by taking an unweighted, uniform "Union" of all bounding boxes as new ground truth for the augmented image.
This approach has shown some success, but it has several limitations. 
First, the approach does not follow the input-target dual interpolation principle that fuels the success of Mixup in classification, as it considers all component bounding boxes equally regardless of the actual mixing ratio $\lambda$.
Second, when small mixing coefficients are used, e.g. $\lambda < 0.1$, the ``Union" strategy can produce noisy mixed object labels (\Fig{union_issue}), potentially leading to sensitivity to noise and hallucinations in the model.
These approaches expect the models to be able to predict all object instances with equal likelihood, regardless of their visibility.
 Finally, most of the previous studies have focused on using data mixing for semi-supervised~\cite{union_ssl_dual,union_ssl_instant_teaching} or few-shot learning~\cite{union_da_fewshot_acrofod}, rather than general object detection, aside from \cite{bof}. 
 More efforts exploring data mixing for general object detection are still needed to address these limitations.


To address these problems, we introduce two novel ideas: Supervision Interpolation (SI) and LossMix.
SI generalizes Mixup's input-target formulation by relaxing the requirement to explicitly interpolate the labels. 
Instead, we hypothesize that it is possible to interpolate other forms of target supervision besides explicitly augmenting the ground truth.
Based on this, we then propose LossMix, a simple but effective and versatile regularization that enables data mixing augmentation to strengthen object detection models and more.
Our key insight is that we can effectively interpolate the losses, instead of the ground truth labels, according to the input's interpolation.
Intuitively, from a data mixing perspective, LossMix interpolates the gradient signals that guide the models' learning, instead of explicitly augmenting the supervision labels like prior approaches~\cite{mixup,cutmix}.
From an object detection perspective, LossMix weights the penalty for each prediction based on their augmented visibility.
For example, LossMix would scale down the penalty for a failure to detect the plane with $\lambda=0.1$ in \Fig{union_issue} (right) since it has low visibility, while Union would treat both the plane and the person ($\lambda=0.9$) equally.
LossMix is flexible and can implicitly handle the mixing of sub-tasks (both classification and localization), while remaining true to the input-target dual interpolation principle that powered the success of the original Mixup~\cite{mixup}.
In a nutshell, our contributions are as follows:
\begin{itemize} [topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item We introduce the concept of Supervision Interpolation (SI), a reinterpretation and generalization of Mixup~\cite{mixup}-like input-label interpolation formulation.
    \item Based on SI, we propose LossMix, a simple but effective and versatile regularization that enables direct data mixing augmentation for object detection.
    \item We demonstrate that LossMix consistently outperforms popular mixing strategies used by prior work on PASCAL VOC~\cite{pascal} and MS COCO~\cite{coco} datasets. 
    \item We design a two-stage domain mixing approach utilizing LossMix to surpass the recent Adaptive Teacher~\cite{amt} (CVPR 2022) and achieve a new state of the art for unsupervised domain adaptation.
\end{itemize}


\input{fig/union_issue}
