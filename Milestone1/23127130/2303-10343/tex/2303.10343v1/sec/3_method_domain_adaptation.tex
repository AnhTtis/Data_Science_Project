\input{fig/mixed_domain_teacher}
\input{fig/domain_mixing_strategies}

\subsection{LossMix for Domain Adaptation}
This section describes our design of a two-stage domain mixing method that takes advantage of LossMix, teacher-generated pseudo-labels,
as well as different types of mixed data.
We employ the recent Adaptive Teacher~\cite{amt} (CVPR 2022) as our base framework for self-distillation.
In a nutshell, we first initialize both Teacher and Student models, who weights are shared, using standard object detection training with labeled source-domain data (warmup phase). During this phase, we use two types of mixed data, named intra-domain balanced mixing and inter-domain noise mixing.
Next, in the adaptation phase, both models are jointly trained using the same cross-domain distillation~\cite{amt,umt}.
During this phase, we leverage pseudo-labels generated by the Teacher to perform two types of mixing: intra-domain balanced mixing (source$\times$source and target$\times$target) and inter-domain balanced mixing (source$\times$target).
\Fig{mixed_domain_teacher} and \Fig{domain_mixing_strategies} give an overview of our Teacher-Student framework.

\paragraph{Warmup phase}
During the warmup phase, we mix intra-source domain data to encourage better (non-directional) generalization and improve robustness on unseen data. This follows the original use of mixup and can indirectly benefit the performance on target domain.
As suggested by \cite{amt}, the warmup stage is critical since it initializes the Teacher, who in turn will generate pseudo labels to distill target-domain knowledge to the Student. However, a warmup stage that relies purely on source domain data risks biasing the Teacher model towards such a distribution \cite{umt}, potentially yielding low-quality pseudo labels.
So, we propose to leverage unlabeled target images to mitigate this, specifically by mixing a small amount (e.g. $\lambda < 0.1$) of them into the labeled source images.
As a result, these images will act inter-domain, target-aware noise augmentation that could help reduce domain gap and bias in the Teacher.
Unlike intra-domain mixing (source$\times$source), inter-domain augmentation (source$\times$target) acts as a directional prior that directly encourages the model to learn linear behaviors to bridge the domain gap from source to target distribution.

\paragraph{Adaptation phase}
During the adaptation phase, thanks to the pseudo labels generated by the Teacher, we can perform intra-domain mixing with both labeled source (source-source) and pseudo-labeled target data (target-target).
Moreover, we also deploy a version of inter-domain mixing, similar to what used in the warmup phase, but in a more balanced fashion. The reasons for this are twofold.
First of all, with the presence of target pseudo labels, we now have the option to perform inter-domain mixing in the same manner as we do for intra-domain labeled source mixing, which is something not feasible during the warmup phase.
Secondly, the reason we do not want to continue using noise mixing is because it would become merely additional source domain data (the mixed-in unlabeled target image is only noise) and can potentially bias the model towards source distribution. 
Compare to this, the pseudo labels are much stronger signal that will push the model to learn target features.
Note that we also do not want to over-emphasize on the pseudo labels and neglect the labeled source data entirely in order to guard against poor, noisy pseudo labels.
As a result, during the adaptation phrase, we apply balanced mixup for source-target pairs by sampling lambda from an unbiased distribution, e.g. $Beta\{\alpha, \alpha\}$ \cite{mixup} or $\{0.5\}$, i.e., fixed lambda \cite{bof}.
