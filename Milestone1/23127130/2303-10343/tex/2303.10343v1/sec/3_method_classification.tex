\subsection{LossMix for Classification}
In the case of standard image classification and segmentation with cross-entropy loss, 
LossMix optimization is equivalent to that of Mixup style label mixing:
\begin{align} \label{eq:ce_loss}
    & \tilde{\mathcal{L}}_{lossmix} (f(\tilde{x}), \tilde{y}_{lossmix}) 
    \\
    &= \lambda \mathcal{L}_{ce} (f(\tilde{x}), y_i)
     + (1-\lambda) \mathcal{L}_{ce} (f(\tilde{x}),y_j)
     \\
    &=  - \lambda y_i \log f(\tilde{x}) - (1-\lambda) y_j \log f(\tilde{x}) 
    \\
    &= - (\lambda y_i + (1-\lambda) y_j) \log f(\tilde{x}) 
    \\
    &= \mathcal{L}_{ce}(f(\tilde{x}), \lambda y_i + (1-\lambda) y_j)
    \\
    &= \tilde{\mathcal{L}}_{mixup} (f(\tilde{x}), \tilde{y}_{mixup})
\end{align}
This shows that, by design, LossMix shares similar benefits with Mixup in cross-entropy based classification problems.
It is worth noting that loss-mixing formulation for Mixup by itself is not new~\cite{mixup_code,automix}. 
What sets LossMix apart is its use of the Supervision Interpolation Training framework to replace label mixing with loss mixing at a fundamental level. 
This significantly enhances its generalizability, making cross-entropy loss and classification a special case rather than the only option.
Since Mixup's advantages in classification have been well studied~\cite{
supermix,
stylemix,
puzzlemix,
tokenmix,
automix,
over_train_mixup,
mixup_cls_fixbi,
regmixup,
manifold_mixup,
mixup_cls_dual,
mixup_cls_adv,
cutmix,
mixup}, we will focus on exploring
LossMix for object detection and domain adaptation.
