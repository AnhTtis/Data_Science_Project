\subsection{LossMix for Object Detection}

Since LossMix makes no assumption about the loss functions $\mathcal{L}$ in \Eq{our_loss}, we can easily apply it to object detection by interpolating both the classification loss and box regression loss.
For example, for Faster RCNN~\cite{faster_rcnn}, $\mathcal{L}$ takes the form of the standard supervised loss for two-stage detectors:
\begin{align} \label{eq:loss_det}
\begin{split}
    \mathcal{L}_{det}(f(x),y) 
    &= \mathcal{L}^{rpn}(f(x),y) + \mathcal{L}^{roi}(f(x),y) \\
    &= \mathcal{L}_{cls}^{rpn}(f(x),y) + \mathcal{L}_{reg}^{rpn}(f(x),y) \\
    &+ \mathcal{L}_{cls}^{roi}(f(x),y) + \mathcal{L}_{reg}^{roi}(f(x),y)
\end{split}
\end{align}
Here, $\mathcal{L}^{rpn}$ denotes the loss of Region Proposal Network (RPN) which generates candidate proposals, while $\mathcal{L}^{roi}$ denotes the loss for Region of Interest (ROI) branch. Both branches perform bounding box regression and classification tasks, specifically binary classification for RPN (object or not) and multi-class classification for ROI \cite{faster_rcnn}.
Given a mixing coefficient $\lambda$, we can directly re-weight all sub-task losses as follows (model $f$ is omitted for brevity): 
\begin{equation} \label{eq:det_loss_mix}
    \tilde{\mathcal{L}}_{det} (\tilde{x}, \tilde{y}) = \lambda \mathcal{L}_{det} (\tilde{x}, y_i) %\ell
     + (1 - \lambda) \mathcal{L}_{det} (\tilde{x}, y_j)
\end{equation}