\subsection{Supervision Interpolation}
We identify that the root cause of the aforementioned issues for both Mixup and unweighted union strategy is the label mixing requirement defined in \Eq{mixup_y}.
Despite working well for simple classification tasks, this policy clearly creates much complications for higher-level tasks such as object detection.
To address this, we propose Supervision Interpolation (SI), a conceptual reinterpretation and generalization of Mixup's input-label interpolation formulation.
In SI, we train models using a dual of interpolated data $\tilde{x}$ and proportionally interpolated supervision signals. 
Intuitively, SI regulates the training by interpolating the supervision or gradient signals guiding the model, depending on the interpolated inputs.
For example, Mixup-based augmentations~\cite{
supermix,
stylemix,
automix,
cutmix,
mixup} and ICT~\cite{interpolation_ssl_det,ict} in semi-supervised learning can be seen as a special case of SI where the supervision signals are the ground truth classification labels.
Based on such a flexible SI framework, next we will introduce, LossMix, a versatile method that enables data mixing and helps strengthen object detectors.

\subsection{LossMix}
Given the conceptual framework of Supervision Interpolation (SI), we then introduce LossMix, an equally simple but more versatile, task-agnostic sibling of mixup that interpolates the loss errors instead of target labels.
Specifically, the LossMix-augmented data:
\begin{equation} \label{eq:our_x}
    \tilde{x} = \lambda x_i + (1 - \lambda)x_j
\end{equation}
\begin{equation} \label{eq:our_y}
    \tilde{y} = \{(y_i; \lambda), (y_j; (1 - \lambda)\}
\end{equation}
are coupled with an augmented loss function:
\begin{equation} \label{eq:our_loss}
    \tilde{\mathcal{L}} (f(\tilde{x}), \tilde{y}) = \lambda \mathcal{L} (f(\tilde{x}), y_i) %\ell
     + (1 - \lambda) \mathcal{L} (f(\tilde{x}), y_j)
\end{equation}
From a SI perspective, our supervision signal is the loss errors weighted relative to $y_i$ and $y_j$, instead of an explicit interpolation of $y_i$ and $y_j$.
From the Mixup perspective, we have relaxed the constraint in \Eq{mixup_y}
and only characterise the virtual target $\tilde{y}$ using weighted $y_i$ and $y_j$ without explicitly constraint the form of $\tilde{y}$.
Note that as $\lambda \rightarrow 0.0$ or $1.0$, LossMix optimization will approach the standard empirical risk minimization.
We would like to highlight that LossMix is not only simple and effective, but also highly versatile:

\paragraph{Simple-yet-effective} 
The idea of loss mixing is straightforward and intuitive, both conceptually and implementation-wise, allowing easy adaptation to existing frameworks. Nonetheless, by design, LossMix helps circumvent the semantic collapse of bounding box interpolation (\Fig{wrong_bbox_mixing}, top and bottom) and approximation issues of unweighted union approach (\Fig{union_issue}). 
Moreover, our experiments demonstrate the effectiveness and robustness of LossMix despite the simplicity in the design, successfully yielding improvement across two standard object detection datasets: PASCAL VOC~\cite{pascal} and MS COCO~\cite{coco}.
Finally, by integrating LossMix in domain adaptation, we can even surpass the recent state-of-the-art method -- Adaptive Teacher~\cite{amt} (CVPR 2022).

\paragraph{Generalizability} LossMix leverages a versatile loss weighting formulation that is potentially applicable to different tasks (i.e., classification, detection, etc.) as well as different input mixing strategies (i.e., Mixup~\cite{mixup}, CutMix~\cite{mixup}, etc.).
LossMix is loss-agnostic, which simplifies its application in object detection and can inspire more applications beyond cross- entropy loss based classification, e.g. regression tasks like localization or depth estimation.
The core idea of LossMix lies in the mixing of target loss signal $y$ and does not limit the input mixing. This opens the door for different strategies, including those that are pixel-based~\cite{mixup}, region-based~\cite{cutmix}, style-based~\cite{stylemix} and more.
