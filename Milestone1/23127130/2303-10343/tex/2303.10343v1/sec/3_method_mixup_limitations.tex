\subsection{Limitation of Label Mixing}

\paragraph{What does Mixup do?}
Mixup \cite{mixup} trains models with virtual examples constructed by convex combinations of pairs of examples and their associated labels:
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
\begin{equation} \label{eq:mixup_x}
    \tilde{x} = \lambda x_i + (1 - \lambda)x_j
\end{equation}
\begin{equation} \label{eq:mixup_y}
    \tilde{y} = \lambda y_i + (1 - \lambda)y_j 
\end{equation}
where $(x_i, y_i)$ and $(x_j , y_j)$ denote randomly sampled pairs of image and ground truth label and $\lambda \in [0, 1]$ denotes the interpolation coefficient.
Training with mixup-augmented data entails minimizing the empirical vicinal risk:
\begin{equation} \label{eq:mixup_risk}
    \mathcal{R}_v(f) = \frac{1}{m} \sum_{i=1}^{m} \mathcal{L} (f(\tilde{x}), \tilde{y}) %\ell
\end{equation}
The core idea is to regularize the learning using a prior knowledge that linear interpolations of the input features (input mixing) should yield linear interpolations of the corresponding output targets (label mixing).
Such linear behavior in-between training examples can potentially mitigate undesired oscillations when predicting examples outside the training distribution.


\paragraph{What are the limitations?}
As discussed in the \Sec{intro} and illustrated in \Fig{wrong_bbox_mixing}, despite working well for classification, the problem quickly arise when considering mixup for other higher-level tasks like object detection.
This is precisely because \Eq{mixup_y} imposes a hard constraint for the augmented labels $\tilde{y}$ to be an explicit linear combination of the real labels $y_i$ and $y_j$.
In object detection, for every $x_i$, the label $y_i$ contains a set of object annotations, each has their own class and bounding box coordinates: $y_i=\{(c_{i1},b_{i1}), (c_{i2},b_{i2}),\ldots\}$, with $(c_{ik},b_{ik})$ denoting an object instance with class label $c_{ik}$ and box coordinates $b_{ik}$.
This results in an ill-defined $\tilde{y}$ and makes label mixing exponentially more complicated.
Existing work~\cite{union_da_fewshot_acrofod,union_instseg_seesaw,union_mot_bytetrack,bof,union_ssl_dual,union_ssl_instant_teaching} 
chose to take an unweighted union of $y_i$ and $y_j$, yielding $\tilde{y}=\{(c_{i1},b_{i1}), (c_{i2},b_{i2}),\ldots,(c_{j1},b_{j1}), (c_{j2},b_{j2}),\ldots\}$.
Although this heuristic may offer some improvement in practice, it does not faithfully interpolate labels since $\tilde{y}$ is independent of $\lambda$ 
and may lead to sub-optimal results.
For example, small $\lambda$ could be problematic since some objects become barely visible (\Fig{union_issue}), creating noisy labels.
