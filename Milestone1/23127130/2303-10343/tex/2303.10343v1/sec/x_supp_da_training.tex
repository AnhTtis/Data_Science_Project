\section{Domain Adaptation Training}
\vspace{-1em}
\Figure{phases} provides a visualization of the Mean Teacher training process using our LossMix approach and different domain mixing strategies.
During the Warmup phase, only the Student network is trained using a supervised loss, such as cross-entropy. In this phase, we do not use pseudo-labels generated by the Teacher network or adversarial learning.
We employ two different mixing strategies: (a) intra-domain source$\times$source mixing with ground truth annotations, and (b) inter-domain noise mixing with labeled source data and unlabeled target images.
The warmup loss function can be defined as follows:
\vspace{0.5em}
\begin{equation} \label{eq:loss_warm}
    \mathcal{L}_{warm} = \lambda_{mss}\mathcal{L}_{mss} + \lambda_{nst}\mathcal{L}_{nst}
\vspace{0.5em}
\end{equation}
where $\mathcal{L}$ is short for the detection loss $\mathcal{L}_{det}$ in \Equation{det_loss_mix}.
the term ``mss" denotes mixed source$\times$source data, where two labeled source images are randomly selected and mixed together. Similarly, ``nst" refers to noise-mixed source$\times$target data, where a labeled source image and an unlabeled target image are mixed together.

During the Adaptation phase, we utilize three mixing procedures to further enhance the training of the Mean Teacher model: (a) intra-domain labeled source$\times$source mixing, (b) intra-domain pseudo-labeled target$\times$target mixing, and (c) inter-domain source$\times$target mixing. 
Importantly, all mixing operations are performed in a balanced manner, such that both source labels and target pseudo-labels are treated equally during mixing. Furthermore, no noise mixing is used during this phase. 
The adaptation loss function can be written as follows:
\vspace{0.5em}
\begin{align} \label{eq:loss_adapt}
\begin{split}
    \mathcal{L}_{adapt} &= \lambda_{mss}\mathcal{L}_{mss} + \lambda_{mtt}\mathcal{L}_{mtt} \\
    &+ \lambda_{mst}\mathcal{L}_{mst} + \lambda_{disc}\mathcal{L}_{disc}
\vspace{0.5em}
\end{split}
\end{align}
The term ``mtt" denotes mixed target$\times$target data, ``mst" denotes balanced (as opposed to noise mixing) mixed source$\times$target data, and $\mathcal{L}_{disc}$ refers to the adversarial domain discriminator loss~\cite{amt}.
The pseudo-labeling losses for ``mtt" and ``mst"
is implemented with cross-entropy loss function, as in prior studies \cite{umt,amt}.

Finally, no augmentation is used during inference so the model's speed is not negatively affected in any capacity when training with our proposed method. Moreover, one advantage of Mean Teacher~\cite{umt,amt,mean_teacher} framework, which models trained with our LossMix also inherit, is that the Teacher model shares the exact same architecture as its Student. The only difference is their learned weights. Therefore, after training, we have the option to safely select the model weights with higher performance among the two for deployment, without any additional latency cost.

