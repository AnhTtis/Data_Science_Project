\clearpage
\appendix

% --- repeat the title (AT: haven't found a more elegant way to do this...)
\twocolumn[
\centering
\Large
\vspace{0.75em}
\textbf{Supervision Interpolation via LossMix:\\Generalizing Mixup for Object Detection and Beyond} \\
\vspace{0.75em}
{ Supplementary Material} \\
\vspace{1.5em}
] %< twocolumn

\appendix

% --- PDF will be split by an editor (e.g. macOS preview), so need to restart from page 1
\setcounter{page}{1}


In this section, we provide additional information on the following supplementary materials:

\begin{itemize}
    \item LossMix implementation
    \item Details of object detection experiments
    \item Domain adaptation training
    \item Details of domain adaptation experiments
    \item Detailed results on VOC $\rightarrow$ Clipart1k (\Table{voc_clip0})
    \item Qualitative Results
\end{itemize}


\begingroup
\setlength{\tabcolsep}{2pt}
\begin{table*}
    \centering
    \resizebox{\linewidth}{!}{ %< auto-adjusts font size to fill line
    \begin{tabular}{@{}l|cccccccccccccccccccc|c@{}}
    \toprule
    Method 
        & aero & bike & bird & boat & bottle & bus & car & cat & chair & cow & table & dog & hrs & mbike & prsn & plnt & sheep & sofa & train & tv & mAP \\
        
    \midrule
    \midrule
    Source
        &23.0 & 39.6 & 20.1 & 23.6 & 25.7 & 42.6 & 25.2 & 0.9 & 41.2 & 25.6 & 23.7 & 11.2 & 28.2 & 49.5 & 45.2 & 46.9 & 9.1 & 22.3 & 38.9 & 31.5 & 28.8 \\
        
    \midrule
    SCL
        & \textbf{44.7}& 50.0& 33.6& 27.4& 42.2& 55.6& 38.3& \textbf{19.2}& 37.9& \textbf{69.0}& 30.1& {26.3}& 34.4& 67.3& 61.0& 47.9& 21.4& 26.3& 50.1& 47.3& 41.5\\
    SWDA
        & 26.2& 48.5& 32.6& 33.7& 38.5& 54.3& 37.1& 18.6& 34.8& 58.3& 17.0& 12.5& 33.8& 65.5& 61.6& 52.0& 9.3& 24.9& {54.1}& 49.1& 38.1 \\
    DM
        & 25.8& 63.2& 24.5& 42.4& 47.9& 43.1& 37.5& 9.1& 47.0& 46.7& 26.8& 24.9& 48.1& {78.7}& 63.0& 45.0& 21.3& 36.1& 52.3& {53.4}& 41.8\\
    CRDA
        & 28.7& 55.3& 31.8& 26.0& 40.1& 63.6& 36.6& 9.4& 38.7& 49.3& 17.6& 14.1& 33.3& 74.3& 61.3& 46.3& 22.3& 24.3& 49.1& 44.3& 38.3\\
    HTCN
        & 33.6& 58.9& 34.0& 23.4& 45.6& 57.0& 39.8& 12.0& 39.7& 51.3& 21.1& 20.1& 39.1& 72.8& 63.0& 43.1& 19.3& 30.1& 50.2& 51.8& 40.3\\
    UMT
        & 39.6& 59.1& 32.4& 35.0& 45.1& 61.9& 48.4& 7.5& 46.0& 67.6& 21.4& \textbf{29.5}& 48.2& 75.9& 70.5& 56.7& {25.9}& 28.9& 39.4& 43.6& 44.1 \\
    AT
        &33.8 & 60.9 & 38.6 & \textbf{49.4} & 52.4 & 53.9 & \textbf{56.7} & 7.5 & 52.8 & 63.5 & 34.0 & 25.0 & \textbf{62.2} & 72.1 & 77.2 & 57.7 & \textbf{27.2} & \textbf{52.0} & \textbf{55.7} & 54.1 & 49.3 \\
    
    AT*
        &42.1 &69.1 &32.5 &{46.2} &52.4 &{71.5} &47.9 &13.1 &55.4 &44.2 &{34.5} &22.8 &58.0 &66.1 &66.4 &\textbf{63.2} &22.6 &29.2 &49.8 &48.5 &46.7\\
    
    \midrule
    
    Noise
        &40.6  &\textbf{70.5}  &35.1  &41.0  &47.5  &54.7  &49.8  &15.9  &61.4  &31.5  &41.2  &17.1  &51.4  &70.6  &61.6  &57.0  &21.3  &35.4  &46.5  &48.6  &44.9\\
    
    Union
        &44.1 &64.7 &44.8 &43.7 &60.3 &\textbf{72.1} &49.5 &13.3 &61.4 &53.0 &\textbf{50.9} &21.9 &59.0 &75.2 &74.7 &56.1 &17.3 &38.6 &51.3 &48.6 &50.0 \\
    
    \midrule
    
    Ours
        &44.4 &65.9 &\textbf{45.7} &42.7 &\textbf{63.2} &63.2 &{50.2} &13.3 &\textbf{62.7} &55.0 &29.3 &26.0 &{60.9} &\textbf{85.8} &\textbf{77.4} &62.0 &20.1 &{47.7} &51.0 &\textbf{55.3} &\textbf{51.1}\\
    
    \bottomrule
    \end{tabular}
    } % \resizebox
    
    \caption{
    The experimental results of cross-domain object detection on the PASCAL VOC $\rightarrow$ Clipart1k adaptation. The average precision (AP, in \%) for all object classes from is reported, following \cite{amt,umt}. 
    The methods presented are SCL \cite{SCL}, SWDA \cite{SWDA}, DM \cite{DM}, CRDA \cite{CRDA}, HTCN \cite{HTCN}, UMT \cite{umt}, AT \cite{amt}, and Source (Faster-RCNN \cite{faster_rcnn}).
    Best results are in \textbf{bold}.
    *indicated reproduced results using the authors' released code.
    Although we include the original numbers cited in AT paper~\cite{amt} for completeness,
    the author acknowledged there are existing instability issues with the open-sourced repository and could only acquire up to AP=45.6\% themselves. 
    } % \caption
    \label{tab:voc_clip0}
\end{table*}
\endgroup


\section{LossMix Implementation}
\paragraph{Data Selection} In our implementation of LossMix, we follow the recommendation of Mixup~\cite{mixup} and mix two data points/images together. We did not experiment with mixing three or more data points/images. To minimize the I/O requirements, we use a single data loader. Each minibatch is then mixed with a randomly shuffled version of itself. 
Another implementation that uses a single loader is to reverse the minibatch, as used by {FixMatch}~\cite{fixmatch}, or to shift all elements by one index.

\paragraph{Input Mixing}
For each pair of data points, we use a $\text{Beta}(\alpha, \alpha)$ distribution to sample a mixing coefficient $\lambda$. 
We set $\alpha$ to $1.0$ by default,
following \cite{bof}.
This results in a uniform sampling distribution for $\lambda$.
When mixing two images, we first create a mixed output image with dimensions equal to the maximum height and width of the original images, then apply random translation, within the output canvas, for each input image.
We pad the resulting image with zeros to maintain the original aspect ratios, which is important for preserving the aspect ratios and geometry of instances in object detection.
We do not experiment with other alignment options, such as aligning by the image origin $(0, 0)$ or using random translations. The effects of different $\alpha$ values on LossMix are presented in \Table{det_abl}.

\paragraph{Target Mixing}
According to \Equation{our_y}, the augmented ground truth for the mixed image is an implicitly weighted union of the original ground truth: $\tilde{y} = \{(y_i; \lambda), (y_j; (1 - \lambda)\}$, where $\lambda$ is the mixing coefficient sampled from a Beta distribution as described in the previous paragraph.
In the case of object detection, the label $y$ is a set of instance ground truth, which is a collection of objects in the image, each with its corresponding class labels and bounding box coordinates.
For example, $y_i$ could be a set of objects $\{(c_{i1},b_{i1}), (c_{i2},b_{i2}),\ldots\}$ present in the image $x_i$.
$c_{ik}$ denotes the class label of object $k$ and $b_{ik}$ represents its corresponding bounding box coordinates.
After mixing, all objects of $y_i$ will share the same mixing weight $\lambda$ of the image, while those of $y_j$ will have weight $(1 - \lambda)$.
These instance-level weights will then be used for loss mixing as described in \Equation{our_loss} and \Equation{det_loss_mix}.
Finally, all bounding box coordinates are adjusted appropriately based on the alignment operation in the \textit{Input Mixing} step described above.


\section{Object Detection Experiments}
We leverage the open-source PyTorch-based Detectron2~\cite{detectron2} as our codebase for object detection experimentation.
Faster RCNN~\cite{faster_rcnn} with ResNet~\cite{resnet}--FPN~\cite{fpn} backbone is employed as our baseline architecture.
All backbones use Synchronized Batch Normalization, which we found to be either on par or better then frozen BatchNorm in most cases.
Unless otherwise specified, we follow most of the default configurations of Detectron2 for both PASCAL VOC~\cite{pascal} and MS COCO~\cite{coco} datasets. 
Specifically, we use ImageNet1K~\cite{imagenet} pretrained weights to initialize the ResNet-50 and ResNet-101~\cite{resnet} backbones.
We use Stochastic Gradient Descent (SGD) optimizer
and random horizontal flipping.
We use a batch size of 64 for faster convergence, an initial learning rate of 0.08, and the default step scheduler from Detectron2.
We use linear warm-up for $100$ iterations with a warm-up factor of $0.001$.
All models use multi-scale training with smallest image side randomly sample from $(480,\ldots, 800)$ for VOC and $(640,\ldots, 800)$ for COCO, both with an increment of $32$. 
The minimum image side at test time is set to $800$ and the maximum for both training and testing is $1333$ by default.
We train PASCAL VOC for 18,000 iterations, which is about 70 epochs, and MS COCO for 270,000 iterations ($3\times$ schedule), equating about 146 epochs.
All experiments were trained with 8 NVIDIA GPUs, either V100 or A100.


Regarding the implementation of other methods, we sample $\lambda \sim \text{Beta}(1.0, 1.0)$ for the popular unweighted \textit{Union} mixing strategy~\cite{union_da_fewshot_acrofod,union_instseg_seesaw,union_mot_bytetrack,bof,union_ssl_dual,union_ssl_instant_teaching}, similar to the setting for our LossMix.
For \textit{Noise} mixing strategy~\cite{afan}, we randomly sampled $\lambda \sim \text{U}(0.0, 0.2)$, i.e., a small mixing ratio from an uniform distribution with an arbitrary upper bound of $0.2$ and only keep the instance labels of the image with the larger coefficient $(1-\lambda)$.
For LossMix-Reg model in \Table{det_abl} deploying LossMix in a RegMixup~\cite{regmixup}-style, we set half of the total batch size to be mixed data and the other half to be regular, non-mixed data, which have effective loss weights of $\lambda=1.0$.


\begin{figure*}
    \begin{center}
    \includegraphics[width=0.9\linewidth]{./img/phase_warmup3.png}
    \\
    {Warmup Phase}
    \\
    \vspace{0.75em}
    \includegraphics[width=0.9\linewidth]{./img/phase_adaptation3.png}
    \\
    {Adaptation Phase}
    \\
    \vspace{0.75em}
    \includegraphics[width=0.9\linewidth]{./img/phase_inference3.png}
    \\
    {Inference Time}
    \end{center}
    \caption{
    Overview of our mixed-domain teacher model during three different phases: Warmup, Adaptation, and Inference.
    }
    \label{fig:phases}
\end{figure*}


\section{Domain Adaptation Training}
\Figure{phases} provides a visualization of the Mean Teacher training process using our LossMix approach and different domain mixing strategies.
During the Warmup phase, only the Student network is trained using a supervised loss, such as cross-entropy. In this phase, we do not use pseudo-labels generated by the Teacher network or adversarial learning.
We employ two different mixing strategies: (a) intra-domain source$\times$source mixing with ground truth annotations, and (b) inter-domain noise mixing with labeled source data and unlabeled target images.
The warmup loss function can be defined as follows:
\begin{equation} \label{eq:loss_warm}
    \mathcal{L}_{warm} = \lambda_{mss}\mathcal{L}_{mss} + \lambda_{nst}\mathcal{L}_{nst}
\end{equation}
where $\mathcal{L}$ is short for the detection loss $\mathcal{L}_{det}$ in \Equation{det_loss_mix}.
the term ``mss" denotes mixed source$\times$source data, where two labeled source images are randomly selected and mixed together. Similarly, ``nst" refers to noise-mixed source$\times$target data, where a labeled source image and an unlabeled target image are mixed together.

During the Adaptation phase, we utilize three mixing procedures to further enhance the training of the Mean Teacher model: (a) intra-domain labeled source$\times$source mixing, (b) intra-domain pseudo-labeled target$\times$target mixing, and (c) inter-domain source$\times$target mixing. 
Importantly, all mixing operations are performed in a balanced manner, such that both source labels and target pseudo-labels are treated equally during mixing. Furthermore, no noise mixing is used during this phase. 
The adaptation loss function can be written as follows:
\begin{align} \label{eq:loss_adapt}
\begin{split}
    \mathcal{L}_{adapt} &= \lambda_{mss}\mathcal{L}_{mss} + \lambda_{mtt}\mathcal{L}_{mtt} \\
    &+ \lambda_{mst}\mathcal{L}_{mst} + \lambda_{disc}\mathcal{L}_{disc}
\end{split}
\end{align}
The term ``mtt" denotes mixed target$\times$target data, ``mst" denotes balanced (as opposed to noise mixing) mixed source$\times$target data, and $\mathcal{L}_{disc}$ refers to the adversarial domain discriminator loss~\cite{amt}.
The pseudo-labeling losses for ``mtt" and ``mst" is implemented with cross-entropy loss function, as in prior studies \cite{umt,amt}.

Finally, no augmentation is used during inference so the model's speed is not negatively affected in any capacity when training with our proposed method. Moreover, one advantage of Mean Teacher~\cite{umt,amt,mean_teacher} framework, which models trained with our LossMix also inherit, is that the Teacher model shares the exact same architecture as its Student. The only difference is their learned weights. Therefore, after training, we have the option to safely select the model weights with higher performance among the two for deployment, without any additional latency cost.


\section{Domain Adaptation Experiments}
For our domain adaptation experiments, we use the official open-source repository published by Adaptive Teacher~\cite{amt}, which is also built with Detectron2~\cite{detectron2}.
For a fair comparison, we follow previous works~\cite{umt,amt} and use Faster RCNN~\cite{faster_rcnn} with a ResNet-101~\cite{resnet} backbone. 
All images are resized to have a shorter side of $600$ while maintaining their aspect ratios. The confidence threshold for filtering pseudo labels is set to $0.8$. 
Weak augmentations include random horizontal flipping and cropping, while strong augmentations include random color jittering, grayscaling, Gaussian blurring, and cutout. The weight smoothing coefficient of the exponential moving average (EMA) for updating the Teacher model is set to $0.9996$. For simplicity, we keep all loss weights for labeled and pseudo-labeled examples (\Equation{loss_warm} and \Equation{loss_adapt}) at $1.0$ and do not tune them. We use Adaptive Teacher's default weight of $0.1$ for the discriminator branch.

We found that the set of training hyperparameters reported in Adaptive Teacher \cite{amt} yield suboptimal results for the open-sourced codebase.
The authors confirmed that there are instability issues\footnote{https://github.com/facebookresearch/adaptive\_teacher/ issues/26\#issuecomment-1192059882} 
and were only able to achieve an mAP of $45.6$\footnote{https://github.com/facebookresearch/adaptive\_teacher/ issues/9\#issuecomment-1193174238} 
instead of the reported mAP of $49.3$ in their paper \cite{amt}. 
This is because the open-sourced code is built with Detectron2 \cite{detectron2}, while their original internal code was built with D2GO\footnote{https://github.com/facebookresearch/adaptive\_teacher/ issues/9\#issuecomment-1134933287}.
To ensure a fair comparison, we tuned the hyperparameters and report our best mAP number of $46.7$ for Adaptive Teacher in Table \Table{voc_clip}. 
Specifically, we performed a grid search over batch sizes of $\{8,2\}$ and learning rates of $\{0.002, 0.005, 0.01\}$ for both the baseline Adaptive Teacher and our model trained with LossMix. The total training consisted of 60,000 iterations, with 20,000 iterations for warm-up and 40,000 iterations for adaptation. We observed that most models reach their peak performance within the first 20K steps of the adaptation phase (after the warm-up phase). 
If the instability issues are resolved, the results for both the reproduced AT (mAP=$46.7$) and our LossMix (mAP=$51.1$) in  \Table{voc_clip} could potentially improve.


\section{Qualitative Results}
\Figure{qualitative_clipart} illustrates the qualitative results of the PASCAL VOC~\cite{pascal}~$\rightarrow$~Clipart1k~\cite{clipart_watercolor} adaptation. To facilitate comparison, we also display the ground truth labels and predictions of Adaptive Teacher~\cite{amt}. The threshold for category scores is set to 0.6 to enhance visualization.

\begin{figure*}
    \begin{center}
    \includegraphics[width=0.77\linewidth]{img/qualitative_clipart5.png}
    \end{center}
    {
        \hspace{9.3em}
        Ground truth 
        \hspace{6.6em}
        Adaptive Teacher
        \hspace{8.3em}
        Ours
    }
    \vspace{1em}
    \caption{
    Qualitative results for the PASCAL VOC~\cite{pascal}~$\rightarrow$~Clipart1k~\cite{clipart_watercolor} adaptation.
    } % -- description
    \label{fig:qualitative_clipart}
\end{figure*}
