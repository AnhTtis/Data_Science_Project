\documentclass{article}

% Please use the following line and do not change the style file.
\usepackage{icml2021_author_response}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{hyperref}       % hyperlinks
\usepackage{booktabs} % for professional tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{xcolor}
\usepackage{lipsum}

\begin{document}
% Uncomment the following line if you prefer a single-column format
%\onecolumn
% We thank all reviewers for inspiring suggestions!
\par \vspace{0.1mm} \noindent \textcolor{blue}{\textbf{To Reviewer 1 }}

\par \vspace{0.1mm} \noindent \textbf{Only one base model: } {\bf 1)} We mainly consider StyleGAN2 because it holds the SOTA on multiple benchmarks. {\bf 2)} In Section 5.2, we have demonstrated CoopInit can adapt to other GANs that are different from original StyleGAN2, {\it e.g.} \emph{different loss functions}. {\bf 3)} With the source code from PyTorch-StudioGAN, we now have added the BigGAN result on CIFAR10 where FID is improved from 8.03 to 6.95.

\par \vspace{0.1mm} \noindent \textbf{FID improvement: } 1) FFHQ: We have very limited computational resources to perform model tuning. (Pls refer to the reply to Reviewer 5). After tuning, FID on FFHQ is 6.64 now (baseline 7.65). 2) CIFAR10: ADA itself is a very strong regularization that produces infinite data and this benchmark is already limited. We do believe the improvement from 2.92 to 2.85 is promising. Other numbers in Sec 5.2 also demonstrates the effectiveness of CoopInit.

\par \vspace{0.1mm} \noindent \textbf{Computation overhead: } On two TITAN X, the time costs of one complete run on CIFAR10 are 48.2 and 48.5 hours w/o and w/ CoopInit respectively. So the extra cost is small.

\par \vspace{0.1mm} \noindent \textbf{Visual comparison and stability: } 
% We didn't include the visual comparison before because the difference was hard to tell and FID number was much more informative. 
We have added the visual comparison in the revision. Regarding learning stability, FID is a popular metric for mode collapse detection so that we choose to show training progress and FID numbers to demonstrate the improved stability in Section 5.2. 
% For example, in Figure 3-5, the learning curves of CoopInit are much more stable than the baseline.

\par \vspace{0.1mm} \noindent \textbf{FID comparison and reproducibility: } In all experiments, we strictly follow the variable-control approach. We first set up the base model (s=1/4) as described in Line 236-252 (right side). Then, only one hyper-parameter (variable) is changed when we investigate its influence. In the base model, we disable the lazy regularization because it substantially affects optimization schedule, leading to non-informative comparisons in Section 5.2. In Table 8, we re-enable the lazy regularization and set the model scale s=1 to obtain FID=6.4. Therefore, all reported numbers are fair and experiments are reproducible with the attached code.

\par \vspace{0.1mm} \noindent \textbf{Other stabilization: } We have included non-saturating loss with $R_1$ regularization and Wasserstein distance with gradient penalty. The proposed CoopInit is complementary to aforementioned approaches. In Table 8, we have added the additional FIDs with zCR (5.71) and SN (6.98).

\par \vspace{0.1mm} \noindent \textbf{Tuning: } adding label embedding to the Constant input of the generator and increasing model size. 


\par \vspace{0.1mm} \noindent \textcolor{blue}{\textbf{To Reviewer 4 }}
\par \vspace{0.1mm} \noindent \textbf{CoopNet: } In Table 8, we report FID=\underline{25.10} of cooperative learning based on the architecture of StyleGAN2. The original CoopNet reports \underline{33.61}. Therefore, our method CoopInit (\underline{4.34}) has greatly improved CoopNet . We have also included FID of cooperative learning versus {\it T} in the revision and observe FID can be reduced with a larger $T$ till $T=20$. 

\par \vspace{0.1mm} \noindent \textbf{Vertical line: } We have added the line.

\par \vspace{0.1mm} \noindent \textbf{FID in Table 6: } Here, we validate our approach can work well w/ and w/o ADA in the limited data setting. In particular, the improvement is more obvious when no ADA is added. ADA itself is a very strong regularization but can only work with non-saturating loss. CoopInit is more general and not restricted by adversarial divergence (5.2.1). 

\par \vspace{0.1mm} \noindent \textbf{High-dimension:} Pls refer to the reply to Reviewer 5.

\par \vspace{0.1mm} \noindent \textcolor{blue}{\textbf{To Reviewer 5 }}
\par \vspace{0.1mm} \noindent \textbf{Novelty: } We do believe it is the first attempt to connect CoopNet with GAN where the discriminator is an \underline{energy-based model} and learned via implicit MLE in CoopInit. More details can also be found in the related work.

\par \vspace{0.1mm} \noindent \textbf{High dimension: } We have very limited resources and conducted experiments on 4$\times$ TITAN X (shared) and Google Colab (subscribed). One complete run of FFHQ 128 would take over a week. We now report more results, {\it e.g.} FID-70k $256^2$ is improved from 3.80 to 3.61. (3.80 is from AnyCostGAN). Nevertheless, we aim to deliver an accessible method to improve GAN, especially on diverse data where CoopInit can handle more modes via MLE.

\par \vspace{0.1mm} \noindent \textcolor{blue}{\textbf{To Reviewer 6}}
\par \vspace{0.1mm} \noindent \textbf{Related work and $\alpha$: } The reference is an inspiring work and is added. The substantial distinctions are: 1) the implicit model $p_\theta(x)$ is the discriminator ($D$) in our work whereas the reference relates the generator ($G$) and an additional variational model to $p_\theta(x)$; 2) we jointly learn $D$ and $G$ via MLE in CoopInit whereas the reference always train the $D$ with adversarial divergence.
Therefore, in our case, $D$ is learned via MLE initially and adversarial divergence later on. Meanwhile, EBM and GAN optimize $D$ on MCMC samples and fake samples from $G$ respectively. That's why we don't consider $\alpha \in (0, 1)$. Along the learning, $D$ is always endowed the ``classifier" capability, leading to a seamless transition with a proper $N_{coop}$. We've included more clarifications, learning curves and samples.

\par \vspace{0.1mm} \noindent \textbf{GP-GAN and WGAN-GP: } The StyleGAN2 is optimized with non-saturating loss and $R_1$ regularization proposed by GP-GAN [2]. The improvement is demonstrated in Table 2.

\par \vspace{0.1mm} \noindent \textbf{The role of discriminator: } The description ``...from uniform noise examples" is inaccurate and should be ``...generated distribution", which can be taken as a ``noisy" (fake) distribution compared with true distribution. 

\par \vspace{0.1mm} \noindent \textbf{Unbalanced design: } It's a confusion. Instead of concluding that the unbalanced GAN design is unnecessary, we aim to investigate how GAN performs and whether the proposed CoopInit is effective when GAN is unbalanced. From Table 5, we indeed conclude a balanced GAN design is beneficial and CoopInit improves GAN under unbalanced situations.

\par \vspace{0.1mm} \noindent \textbf{Visualize transition: } We have added the experiment to show the seamless transition, e.g. adding a vertical line to separate the training phases and samples along the training.

\end{document}
