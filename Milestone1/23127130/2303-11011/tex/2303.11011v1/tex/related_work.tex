\section{Related Work}
\subsection{Image-based Optical Flow}
Optical flow estimates per-pixel motion between two frames according to photo consistency. Traditional approaches minimize energies, leveraging both feature similarities and motion smoothness~\cite{fortun2015optical}. Deep methods train the networks that take two frames as input and directly output dense flow motions. Recent deep methods design different pipelines~\cite{dosovitskiy2015flownet,sun2018pwc,teed2020raft,jiang2021gma} as well as learning modules~\cite{luo2021upflow,liu2021oiflow,luo2022kpa,luo2022learning} for performance improvements. The training often requires large labeled datasets, which can be synthesized by moving a foreground on top of a background image, such as FlyingChairs~\cite{dosovitskiy2015flownet}, and autoflow~\cite{sun2021autoflow}, or rendered from graphics such as SinTel~\cite{butler2012naturalistic} and FlyingThings~\cite{mayer2016large}, or created directly from real videos~\cite{han2022realflow}. In this work, we use computer graphics techniques to render accurate and physically correct event and flow values.

\subsection{Event-based Optical Flow}
%\ping{better to move some of this paragraph to the first paragraph of the introduction. Define what is the Event-flow problem first.}
Benosman~\emph{et al.}~\cite{benosman2012asynchronous} first proposed to estimate optical flow from events, which can only estimate sparse flows at the location of event values. Recent deep methods can estimate dense optical flows. EV-FlowNet~\cite{zhu2018ev} learns the event and flow labels in a self-supervised manner, which minimizes the photometric distances of grey images acquired by DAVIS~\cite{brandli2014240}. Different event representations were explored, e.g., EST~\cite{gehrig2019EST} and Matrix-LSTM~\cite{cannici2020Matrix-LSTM}, with various network structures, such as SpikeFlowNet~\cite{lee2020spikeflownet}, LIF-EV-FlowNet~\cite{hagenaars2021LIF-EV-FlowNet}, STE-FlowNet~\cite{ding2022STE-FLOWNET}, Li~\emph{et al.}~\cite{li2021lightweight}, and E-RAFT~\cite{gehrig2021raft}. Some works take both events and images as input for the flow estimation~\cite{lee2022Fusion-FlowNet,pan2020single}. In general, dense flows are more desirable than sparse ones but are more difficult to train. Normally, regions with events can produce more accurate flows than empty regions where no events are triggered. Moreover, supervised training can produce better results than unsupervised ones, as long as the training dataset can provide sufficient event-flow guidance.    

\subsection{Event Dataset}
The applications to the event camera dataset were first explored in the context of classification~\cite{neil2016phased,bi2019graph}. Early works generate events simply by applying a threshold on the image difference~\cite{kaiser2016towards}. Frame interpolation is often adopted for high framerate~\cite{gehrig2020video}. The synthesized events often contain inaccurate timestamps. The DAVIS event camera can directly capture both images and events~\cite{brandli2014240}, based on which two driving datasets are captured, DDD17~\cite{binas2017ddd17} and MVSEC~\cite{zhu2018multivehicle}. With respect to the event flow dataset, EV-Flownet~\cite{zhu2018ev} calculated sparse flow labels from LIDAR depth based on MVSEC~\cite{zhu2018multivehicle}. Wan~\emph{et al.}~\cite{wan2022DCEI} and Stoffregen~\emph{et al.}~\cite{stoffregen2020reducing} created the foreground motions and interpolated the intermediate frames for events. The real captured dataset can only provide sparse labels while the synthesized ones contain inaccurate events. In this work, we propose to render a physically correct event dense flow dataset.
