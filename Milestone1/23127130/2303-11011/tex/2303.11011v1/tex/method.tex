\section{Method}

\subsection{The event-based optical flow dataset}\label{sec: method: dataset}
In order to create a realistic event dataset for optical flow learning, we propose to employ a graphics engine with 3D scenes for data generation. Given 3D scenes, we first define camera trajectories, according to which we generate optical flow labels for timestamps at $60$ FPS and $15$ FPS. Then we render high-frame-rate videos based on the motion magnitude of the optical flow label between two timestamps. Finally, we generate event streams by rendering high-frame-rate videos and simulating the event trigger mechanism in the event camera using the v2e toolbox~\cite{hu2021v2e}. The overview of our data generation pipeline is shown in Fig.~\ref{fig:data_pipeline}.

% In order to create realistic event dataset for optical flow learning, we propose to employ graphics engine with 3D scenes for data generation. Given 3D scenes, we first define camera trajectories, according to which we generate optical flow labels for timestamps at $60$ FPS and $15$ FPS \textcolor{red}{(need to be checked, Kunming)}. Then we render high-frame-rate videos based on the motion magnitude of the optical flow label between two timestamps. Finally, we generate event streams by simulating the event trigger mechanism in event camera using the v2e toolbox~\cite{hu2021v2e} and high-frame-rate videos. The overview of our data generation pipeline is shown in Fig.~\ref{fig:data_pipeline}. (\textcolor{red}{needed to be checkde, Kunming})


 \paragraph{Virtual Scenes.}
 To ensure that the generated event dataset has the correct scene structure, we utilize a variety of indoor and outdoor 3D scenes, including cities, streets, forests, ports, beaches, living rooms, bedrooms, bathrooms, kitchens, and parking lots. Totally, we obtain $53$ virtual 3D scenes ($31$ indoor and $22$ outdoor) that simulate real-world environments. Some examples are shown in Fig.~\ref{fig:ourdatasets}.
 
 % To ensure that the generated event dataset has the correct scene structure, we utilize a variety of indoor and outdoor 3D scenes, including cities, streets, forests, ports, beaches, living rooms, bedrooms, bathrooms, kitchens, and parking lots. Additionally, we randomly add various weather conditions, such as sunny, cloudy, rainy, nighttime, and moonlight, to cover the range of brightness in the scenes. By randomly combining these elements, we obtain approximately $50$ (\textcolor{red}{need to be checkde, Kunming}) 3D scenes that simulate real-world environments. Some examples are shown in Fig.~\ref{}.(\textcolor{red}{needed to be checked, Kunming})
 
 %We create indoor and outdoor 3D virtual scenes using the computer graphics program Blender. The outdoor scenes include the city, street, woodland, port, and beach, while the indoor scenes contain the living room, bedroom, bathroom, kitchen, and parking lot. Besides, we randomly add various weather conditions, e.g., sunny, overcast, rainy, night, and moonlight, to cover the brightness range of the scene. By randomly combining these elements, we get around $50+$ 3D scenes that simulate the real environment.
 
 \paragraph{Camera Trajectory.}\label{sec: method: dataset: camera trajectory}
 Given a 3D scene model, we first generate the 3D camera trajectory using PyBullet~\cite{coumans2016pybullet}, an open-source physics engine, to ensure that the camera does not pass through the inside of the objects and out of the effective visible region of the scene during the motion. 
 %We define two fundamental rigid motion trajectories: 1) forward motion in a straight line with translations; 2) rotation along an axis. 
 After setting the start position, end position, and moving speed of the camera trajectory, we randomly add translation and rotation motions to create a smooth curve function $\Gamma(t)$ that outputs the location and pose $P(t)=[x(t),y(t),z(t), r(t)]^T$. %Furthermore, during camera motion, we make small random adjustments to the focal length of the camera based on uniform distribution $f\sim U(f_{min},f_{max})$ to simulate the disturbances caused by high-speed motion in the real world, such as motion blur and distortion.% this sentence can be deleted: Furthermore, xxx
 
 \paragraph{High-frame-rate Video and Optical Flow.}\label{sec: method: dataset: optical flow and video}
 After camera trajectory generation, we use the graphics engine to render a sequence of images $I(\bm{u},t_i)$, where $\bm{u}=(x,y)$ is the pixel coordinates and $t_i$ is the timestamp. We extract the forward and backward optical flow labels between every two timestamps ($\bm{F}_{t_i \to t_j}$, $\bm{F}_{t_j \to t_i}$). Then we need to generate the event data to construct the event optical flow dataset. Here, we render high-frame-rate videos $ \{ I(\bm{u},\tau) \}, \tau \in [t_i,t_j] $ between timestamps $t_i$ and $t_j$ for events generation according to the camera trajectory $\Gamma(t)$. 

 Inspired by ESIM~\cite{rebecq2018esim}, we adopt an adaptive sampling strategy to sample camera locations from the camera trajectory for interval $t_i$ to $t_j$, so that the largest displacement of all pixels between two successive rendered frames ($I(\bm{u},\tau_k)$, $I(\bm{u},\tau_{k+1})$) is under 1 pixel, we define the sampling time interval $\Delta \tau_{k}$ as follows:
 \begin{equation}
\begin{aligned}
\Delta \tau_{k} &= \tau_{k+1} - \tau_{k} \\
&= (\max_{\bm{u}} \max \{ \left \| \bm{F}_{{\tau_{k-1}} \to {\tau_{k}}} \right \| , \left \| \bm{F}_{\tau_{k} \to \tau_{k-1}} \right \|\})^{-1},
\end{aligned}
\label{sampling}
\end{equation}
where $\left | F \right| = \max_{\bm{u}} \max \{ \left \| \bm{F}_{{\tau_{k-1}} \to {\tau_{k}}} \right \| , \left \| \bm{F}_{{\tau_{k}} \to {\tau_{k-1}}} \right \|\}$ is the maximum magnitude of the motion field between images $I(\bm{u},\tau_{k-1})$ and $I(\bm{u},\tau_{k})$. %the initial condition of Eq.(\ref{sampling}) is $\left | F \right| = \max_{\bm{u}} \max \{ \left \| \bm{F}_{{0} \to {N}} \right \| , \left \| \bm{F}_{{N} \to {0}} \right \|\} $. \Kunming{Here a modification is needed.}


\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{images/ourdatasets.pdf}
    \caption{Examples of our MDR training set. Each row shows images, events and flow labels from top to bottom.}
    \label{fig:ourdatasets}
\end{figure}

 \paragraph{Event Generation from High-frame-rate Video.}
 Given a high-frame-rate video $ \{ I(\bm{u},\tau) \}, \tau \in [t_i,t_j] $ between timestamps $t_i$ and $t_j$, we next generate event stream by simulating the event trigger mechanism. Similar to \cite{rebecq2018esim} and \cite{gehrig2020video}, we use linear interpolation to approximate the continuous intensity signal in time for each pixel between video frames. 
Events $\left \{ (\bm{u_e},t_e,p_e) \right \}$ are generated at each pixel $\bm{u_e}=(x_e,y_e)$ whenever the magnitude of the change in the log intensity values ($L(\bm{u_e},t_e) = \ln(I(\bm{u_e},t_e)$) exceeds the threshold $C$. This can be expressed as Eq.(\ref{interpolation1}) and  Eq.(\ref{interpolation2}):

%  \begin{equation}
% L(\bm{u_i},t_k+\Delta t_k) - L(\bm{u_i},t_k) \ge  p_i C,
% \label{interpolation1}
% \end{equation}
%  \begin{equation}
% t_{i} = t_{i-1} + \Delta t_k \frac{C}{\left | L(\bm{u_i},t_k+\Delta t_k) - L(\bm{u_i},t_k) \right | },
% \label{interpolation2}
% \end{equation}
% here $t_{i-1}$ and $t_{i}$ is the timestamps of the last triggered event and the next triggered event respectively, and $p_i \in \left \{ -1,+1  \right \} $ is the sign of the change, also called the polarity of
% the triggered event. 
%\Kunming{Here the letters in the equation should be checked. }

 \begin{equation}
L(\bm{u_e},t_e+\Delta t_e) - L(\bm{u_e},t_e) \ge  p_e C,
\label{interpolation1}
\end{equation}
 \begin{equation}
t_{e} = t_{e-1} + \Delta \tau_{k} \frac{C}{\left | L(\bm{u_e},t_e+\Delta t_e) - L(\bm{u_e},t_e) \right | },
\label{interpolation2}
\end{equation}
where $t_{e-1}$ and $t_{e}$ are the timestamps of the last triggered event and the next triggered event respectively, $p_e \in \left \{ -1,+1  \right \} $ is the polarity of the triggered event. We define it as $E(t_{k},t_{k+1})$, witch is the sequence $ \{(\bm{u_e},t_e,p_e)^N, e \in [ 0,N ]\}$ with $N$ events between time $t_{k}$ and $t_{k+1}$.

%Following \cite{hu2021v2e}, we add an optional lowpass filter to filter the log intensity values before the interpolation. 
%Since real event cameras do not usually have perfectly balanced positive and negative thresholds, the positive threshold $C_p = C_n \cdot \alpha, \alpha \in N(\mu=1.0, \sigma=0.1) $. The events with different densities can be generated by adjusting the threshold $C$, and we carry out a study of comparison between events data with various densities as detailed in Sec.~\ref{densities_comparison}.

\paragraph{Multi-Density Rendered Events Dataset.}
%Since the event camera only records the coordinates of pixels whose log intensity values transform above the threshold, when we cast the events onto the image plane, the resulting voxel is sparse and the valid pixels are clustered in the edges of the moving object.
Using the above data generation method, we can generate data with different event densities by using different threshold values $C$. %For example, using a higher threshold value will result in sparser events, while using a lower threshold value will generate denser events. 
Since event stream is commonly first transformed into event representation~\cite{zhu2019unsupervised,stoffregen2020reducing,gehrig2021raft} and then fed into deep networks. In order to measure the amount of useful information carried by the event stream, we propose to calculate the density of the event stream using the percentage of valid pixels (pixels where at least one event is triggered) in the voxel representation:
 \begin{equation}
V(\bm{u_e}, b) = \sum_{e=0}^{N}p_{e} \max(0, 1-|b-\frac{t_e-t_0}{t_N-t_0}(B-1)|),
\label{reprentation}
\end{equation}
 \begin{equation}
D = \frac{1}{HW}\sum_{i=0}^{N} \varepsilon (\sum_{b=0}^{B}|V(\bm{u_e}, b)|),
\varepsilon(x)=\left\{\begin{matrix}1 , x>0\\0,x \le 0\end{matrix}\right.,
\label{density}
\end{equation} 
where $V(\bm{u_e}) \in \mathbb{R}^{B\times H \times W}$ is the voxel representation~\cite{zhu2019unsupervised} of the event stream $\left \{ (\bm{u_e},t_e,p_e)^N \right \}$ between $t_0$ and $t_N$, $b \in \left [ 0, B-1 \right ] $ indicates the temporal index , $B$ (typically set to 5) donates temporal bins and $D$ is the density of the input event representation $V$.
In practical applications, different event cameras may use different threshold values in different scenes, resulting in data with different event densities. Intuitively, event data with lower density is more difficult for optical flow estimation. In order to train models that can cover event data with various density, in this paper, we propose to adaptively normalize the density of the input events to a certain density representation for optical flow estimation, so as to increase the generalization ability of the network.

% We propose to calculate the percentage of these valid pixels in all pixels of the whole voxel to measure the amount of useful information carried by the voxel, donated as the density $D$ of the corresponding event sequence.
%  \begin{equation}
% D = \frac{1}{HW}\sum_{i=0}^{N} \varepsilon (\sum_{b=0}^{B} \left | V(\bm{u_e}, b) \right | ),
% \label{density}
% \end{equation} 
%  \begin{equation}
% \varepsilon(x)=\left\{\begin{matrix}1 , x>0\\0,x \le 0\end{matrix}\right. .
% \label{density2}
% \end{equation} 
% Our multi-density rendered event optical flow dataset (noted as MDR) consist of events data with various densities, dense optical flow and the best density label. The best density label is collected by evaluating the flow network for the optimal performance among these samples with different densities, which is used for the supervised training of the adaptive density module.

\subsection{Event-based Optical Flow Estimation}
Event-based optical flow estimation involves predicting dense optical flow $\bm{F}_{k-1 \to k}$ from consecutive event sequences $E(t_{k-1},t_{k})$ and $E(t_{k},t_{k+1})$. In this paper, we find that networks perform better on event sequences with appropriate density than on those with excessively sparse or dense events, when given events from the same scene. Motivated by this, we propose a plug-and-play Adaptive Density Module (ADM) that normalizes the input event stream to a density suited for estimating optical flow. Our network architecture is shown in Fig.~\ref{fig:networks}, where the ADM transforms input event representations $V_1$ and $V_2$ to justified event representations $V_1^{\mathrm{ad}}$ and $V_2^{\mathrm{ad}}$, which are then used by an existing network structure to estimate optical flow. 

%the process of calculating pixel correspondences (denoted as optical flow $\bm{F}_{{k-1} \to k}$) between two continuous images $I(\bm{u},t_{k-1})$ and $I(\bm{u},t_{k})$. Analog to event cameras, we estimate optical flow $\bm{F}_{k-1 \to k}$ from consecutive event sequences $E(t_{k-1},t_{k})$ and $E(t_{k},t_{k+1})$. %, where $E(t_{k-1},t_{k})$ is defined as a sequence of events between time $t_{k-1}$ and $t_{k}$.

% \paragraph{Event Representation.}
% For both the sequences of events $E(t_{k-1},t_{k})$ and $E(t_{k},t_{k+1})$, we discrete the events into a volumetric voxel grid representation~\cite{zhu2019unsupervised} as the input of CNN networks to estimate optical flow. This representation scales the time dimension into the range $\left [0, B-1 \right ]$ through bilinear interpolation, but is able to retain most of the temporal aspect of events. If an event sequence $\left \{ (\bm{u_e},t_e,p_e) \right \} $ has a total of $N$ events, we generate the voxel with ($B \times H \times W$) as follows: 
%  \begin{equation}
% V(\bm{u_e}, b) = \sum_{e=0}^{N}p_{e} \max(0, 1- \left | b-\frac{t_e-t_0}{t_N-t_0}(B-1) \right | ),
% \label{reprentation}
% \end{equation} 
% here $t_0$ and $t_N$ are the start and end timestamps of the event sequence respectively, $b \in \left [0, B-1 \right ]$ indicates the temporal index of the voxel. We used $B = 5$ throughout the experiments in this paper (we found values of $ B = 3, 5, 10$ produced no significant differences in earlier experiments).


\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{images/Network2.pdf}
    \caption{The structure of the proposed network. We design a plug-and-play Adaptive Density Module (ADM) to transform input event representations $V_1$ and $V_2$ into 
    $V_1^{\mathrm{ad}}$ and $V_2^{\mathrm{ad}}$ with suitable density for optical flow estimation. }
    \label{fig:networks}
\end{figure}

\paragraph{Adaptive Density Module.}
As shown in Fig.~\ref{fig:networks}, our ADM module consists of two sub-modules: the multi-density changer (MDC) module and the multi-density selector (MDS) module. The MDC module globally adjusts the density of the input event representations from multi-scale features, then the MDS module picks the best pixel-wise density for optical flow estimation.

The MDC module adopts an encoder-decoder architecture with three levels, as illustrated in Fig.~\ref{fig:submodule}(a). To generate multiscale transformed representations $V_3^{\mathrm{MDC}}$, $V_2^{\mathrm{MDC}}$ and $V_1^{\mathrm{MDC}}$ (also noted as $V_{\mathrm{out}}^{\mathrm{MDC}}$) from the concatenated input event representations $V$, three encoding blocks are employed to extract multiple scale features, followed by three decoding blocks and two feature fusion blocks. It is worth noting that, to ensure the lightweightness of the entire module, we utilize only two $3\times 3$ and one $1\times 1$ convolutional layers in each encoding and decoding block.

%MDC first applies three EBs (encoding blocks) to extract the shallow features at level $i$ denoted as $EB_i^{out}$, and then use DBs (decoding blocks) to concatenate these features after upsampling level by level. What's more, we use MSFF (multi-scale feature fusion) to aggregate the features from the three levels and enlarge or reduce the features from other levels to make their size consistent with that of the feature map at this level, as shown in Fig.~\ref{fig:submodule}(a). Spatially, we then exploit a feature attention mechanism to actively emphasize or suppress the features from the previous level and learn more useful information of the features from the next level. Finally, feature maps at three layers are concatenated together, and further, the concatenated features are refined using $3\times3$ and $1\times1$ convolutional layers. In the same way, the output of MSFF at layer $i$ is described as follows:
%  \begin{equation}
%  \begin{split}
% \rm MSFF_1^{out}= Conv(Concat(EB_1^{out}, (EB_2^{out})^{\uparrow }, (EB_3^{out})^{\uparrow })), \\
% \rm MSFF_2^{out}= Conv(Concat((EB_1^{out})^{\downarrow }, EB_2^{out}, (EB_3^{out})^{\uparrow })),
%  \end{split}
% \label{msff}
% \end{equation} 
% where $\rm MSFF_1^{out}, MSFF_2^{out}, Conv, Concat$ represent the output of the MSFF at layer 1 and 2, convolutional layer and concatenate operation, respectively. Up-sampling ($\uparrow$) and down-sampling
% ($\downarrow$) operations are employed so that the feature maps from different layers can be concatenated.

To maintain the information in the input event representation and achieve density transformation, we adopt the MDS module for adaptive selection and fusion of $V_{\mathrm{out}}^{\mathrm{MDC}}$ and $V$, as depicted in Fig.~\ref{fig:submodule}(b). We first concatenate $V_{\mathrm{out}}^{\mathrm{MDC}}$ and $V$, and then use two convolutional layers to compare them and generate selection weights via softmax. Finally, we employ the selection weights to identify and fuse $V_{\mathrm{out}}^{\mathrm{MDC}}$ and $V$, producing the transformed event representation $V_1^{\mathrm{ad}}$ and $V_2^{\mathrm{ad}}$, which are fed into an existing flow network for optical flow estimation. In this paper, we use KPA-Flow~\cite{luo2022kpa} for optical flow estimation by default.

% To enable the ADM to adjust the density of voxels adaptively, we propose the MDS module with an automatic selection operation to alter the optimal density for each pixel in spatial, as shown in Fig.~\ref{fig:submodule}(b). MDS first fuses the voxel (concatenate two inputing voxels in batch dimension) and MDC's output via an element-wise summation, then embeds the global information by using two layers of $3\times3$ convolutional layers with stride 1 and every convolutional layers and the ReLU function and the Instance Normalization come after every convolutional layer, and we get a mixed feature $Z$ with ($2 \times H \times W$). A soft attention in spatial is used to adatively select different density for each pixel $\bm{u_i}=(x_i, y_i)$. In particular, the pixel-wise digits are subjected to a softmax operator:
%  \begin{equation}
% a_i=\frac{e^{A_i}}{e^{A_i}+e^{B_i}},b_i=\frac{e^{B_i}}{e^{A_i}+e^{B_i}},
% \label{softattention}
% \end{equation} 
% here $A,B \in \mathbb{R}^{H \times W}$ are the first and second channel of the mixed feature $Z$, and $a,b$ denote the soft attention map for the voxel and MDC's output, respectively. Note that $A_i$ is the element of $A$ at pixel $\bm{u_i}=(x_i, y_i)$, likewise $B_i, a_i, b_i$. The selected $\mathrm{voxel^{ad}}$ is obtained through the attention weights $a,b$ as Eq.(~\ref{weightedsum}), and we split $\mathrm{voxel^{ad}}$ in batch dimension to output $\mathrm{voxel1^{ad}}$ and $\mathrm{voxel2^{ad}}$ with adative densification, which are next fed into the flow network.
%  \begin{equation}
% \mathrm{voxel}_i^\mathrm{ad} = a_i \cdot \mathrm{voxel}_i + b_i \cdot \mathrm{MDC}_i^\mathrm{out}.
% \label{weightedsum}
% \end{equation} 

%Our proposed ADM is a general adaptation module that brings enhancements to many supervised optical flow networks for event sequences, as we demonstrate in \ref{exp_densities}.

\paragraph{Loss Function.}
Based on our MDR dataset, we use the event representation with moderate density as the ground truth (noted as $V^\mathrm{GT}$) to train our ADM module. 
For the MDC module, we use a multi-scale loss as follows:
 \begin{equation}
L_{\mathrm{MDC}}=\sum_{k=1}^{3}\sqrt{(V_k^\mathrm{MDC}-V_k^\mathrm{GT})^2+\xi^2},
\label{MDCloss}
\end{equation} 
where $\xi=10^{-3}$ is a constant value, $V_k^\mathrm{MDC}$ is the output of the $k$-th level in MDC, and $V_k^\mathrm{GT}$ is downsampled from $V^\mathrm{GT}$ to match the spatial size.


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{images/submodule2.pdf}
    \caption{The detailed structure of sub-modules used in our proposed ADM model: (a) MDC, (b) MDS.}
    \label{fig:submodule}
\end{figure}

 % \begin{equation}
% L_{\mathrm{MDC}}=\sum_{k=1}^{3}\sqrt{(\mathrm{DB}_k^\mathrm{out}-\mathrm{voxel}_k^\mathrm{GT})^2+\varepsilon^2} ,
% \label{MDCloss}
% \end{equation} 
% here $\varepsilon=10^{-3}$ is a constant value, $\mathrm{DB}_k$ is the output of the $k$-th level of DB in MDC, and $\mathrm{voxel}_k^\mathrm{GT}$ is downsampled from $\mathrm{V}^\mathrm{GT}$ to match the spatial size with $\mathrm{DB}_k$ .

For the MDS module, we use the distance between the density of $V^\mathrm{ad}$ and $V^\mathrm{GT}$ as the guidance:
 \begin{equation}
L_{\mathrm{MDS}}=\left \| D(V^\mathrm{ad}) - D(V^\mathrm{GT}) \right \|_1,
\label{MDSloss}
\end{equation} 
where $D$ means to calculate the density as in Eq.(~\ref{density}).

For the flow network, we use L1 loss (denoted as $L_{\mathrm{Flow}}$ ) between flow prediction and ground truth as the guidance. %to supervise the flow prediction to regress to the flow ground truth.
% For flow network, we supervise all flow predictions using L1 loss between the ground truth $\mathrm{Flow}^\mathrm{GT}$ following ~\cite{teed2020raft}, note as $L_{\mathrm{Flow}}$.
%  \begin{equation}
% L_{\mathrm{Flow}}=\sum_{i=1}^{N} \gamma^{N-i} \left \| \mathrm{Flow}_i^\mathrm{pred} - \mathrm{Flow}^\mathrm{GT} \right \|_1,
% \label{Flowloss}
% \end{equation} 
% here $N$ is the number of flow predictions including the intermediate and final ones, and $\gamma$ (set to 0.9) is the weight that increases exponentially to give higher weights for later predictions.

The final loss function for training the whole pipeline in Fig.~\ref{fig:networks} is determined as follows:
 \begin{equation}
L_{total} = \lambda_1 L_{\mathrm{MDC}} + \lambda_2 L_{\mathrm{MDS}} + L_{\mathrm{Flow}},
\label{totalloss}
\end{equation} 
where we empirically set $\lambda_1 = 0.1$ and $\lambda_2 = 10$.