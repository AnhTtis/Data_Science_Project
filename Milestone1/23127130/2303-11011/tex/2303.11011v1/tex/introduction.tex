%%%%%%%%% BODY TEXT
\section{Introduction}
Event cameras~\cite{lichtsteiner2008128} record brightness changes at a varying framerate~\cite{brandli2014240}. When a change is detected in a pixel, the camera returns an event in the form $e=(x,y,t,p)$ immediately, where $x,y$ stands for the spatial location, $t$ refers to the timestamp in microseconds, and $p$ is the polarity of the change, indicating a pixel become brighter or darker. 
\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{images/teaser.pdf}
    \caption{(a) the captured dataset from real event camera~\cite{zhu2018multivehicle,zhu2018ev}. (b) the synthesized dataset with flying chairs foreground~\cite{wan2022DCEI}. (c) the synthesized dataset by moving a foreground image~\cite{stoffregen2020reducing}. (d) Our synthesized dataset by graphics rendering, which not only reflects the real motions under correct scene geometries, but also produces accurate dense flow labels and events.}
    \label{fig:teaser}
\end{figure}
%Event camera owns attractive advantages over traditional cameras, such as high dynamic range, low latency, and low power consumption~\cite{rebecq2019high,pan2019bringing,sabatier2017asynchronous}. 
On the other hand, optical flow estimation predicts motions between two frames~\cite{sun2010secrets}, which is fundamental and important for many applications~\cite{vihlman2020optical,xu2019deep,capito2020optical}. In this work, we study the problem of estimating optical flow from event camera data,  instead of from RGB frames. Different from traditional images, events are sparse and are often integrated in short intervals as the input for the prediction. As such, early works can only estimate sparse flows at the location of events~\cite{benosman2012asynchronous}. Recent deep methods can estimate dense flows but with the help of images, either as the guidance~\cite{zhu2018ev} or as the additional inputs~\cite{lee2022Fusion-FlowNet,pan2020single}. Here, we tackle a hard version of the problem, where dense flows are predicted based purely on the event values $e$. One key issue is how to create high quality event-based optical flow dataset to train the network.      
%For example, the DAVIS~\cite{brandli2014240} event camera can output both image and events, where the event streams can reach up to millions of frame rates (fps)\ping{I don't think we should call this fps. It does not capture so many frames, but only a set of events.}~\cite{rebecq2019high,pan2019bringing,sabatier2017asynchronous}, owning attractive advantages over traditional cameras, such as high dynamic range, low latency, and low power consumption. On the other hand, optical flow estimation predicts motions between two frames by exploiting the photometric consistency~\cite{sun2010secrets}, which is fundamental and important for many applications~\cite{vihlman2020optical,xu2019deep,capito2020optical}. In this work, we study the problem of estimating optical flow from event cameras \ping{you should expand the introduction of this problem, which is the focus of the paper. The introduction of event cameras can be more concise.}. One key issue is how to create high quality event-based optical flow dataset to train the network. 

Existing methods of event flow dataset creation can be classified into two types, 1) directly capturing from real event cameras~\cite{zhu2018multivehicle,zhu2018ev}; 2) moving foregrounds on top of a background image to create synthesized flow motions~\cite{wan2022DCEI,stoffregen2020reducing} and apply frame interpolation~\cite{gehrig2020video} to create events. For the first type, the  ground-truth (GT) flow labels need to be calculated based on gray images acquired along with the event data. However, the optical flow estimations cannot be perfectly accurate~\cite{teed2020raft,sui2022craft,luo2022kpa,jiang2021gma,xu2022gmflow}, leading to the inaccuracy of GT labels. To alleviate the problem, additional depth sensors, such as LIDAR, have been introduced~\cite{zhu2018multivehicle}. The flow labels can be calculated accurately when the depth values of LIDAR scans are available. However, LIDAR scans are sparse, and so do the flow labels, which are unfriendly for dense optical flow learning. Fig.~\ref{fig:teaser} (a) shows an example, LIDAR points on the ground are sparse. Moreover, some thin objects are often missing, as indicated by the red box in Fig.~\ref{fig:teaser} (a).

For the second category, the flow labels are created by moving foreground objects on top of a background image, similar to flying chairs~\cite{dosovitskiy2015flownet} or flying things~\cite{mayer2016large}. In this way, the flow labels are dense and accurate. To create events, intermediate frames are interpolated~\cite{jiang2018super}. However, the frame interpolation is inaccurate due to scene depth disparities, where the occluded pixels cannot be interpolated correctly, leading to erroneous event values in these regions. To match high framerate of events, the large number of interpolated frames makes the problem even worse. Fig.~\ref{fig:teaser} (b) shows an example, where the events are incorrect at the occluded chairs. Moreover, the motions are artificial, further decreasing the realism of the dataset (Fig.~\ref{fig:teaser} (b) and (c)). 

In this work, we create an event-flow dataset from synthetic 3D scenes by graphics rendering (Fig.~\ref{fig:teaser} (d)). While there is a domain gap between rendered and real images, this gap is empirically found insignificant in
%Normally, graphics dataset suffers from domain gaps between rendered images and real images due to different image appearances. However, there are also some event dataset generated by graphics but for other tasks, such as 
event camera based classification~\cite{sironi2018hats} and segmentation~\cite{gehrig2020video} tasks. 
As noted by these works, models trained on synthetic events work very well for real event data. Because events contain only positive and negative polarities, no image appearances are involved. To this end, we propose a \textbf{M}ulti-\textbf{D}ensity \textbf{R}endered (MDR) event optical flow dataset, created by Blender on indoor and outdoor scenes with accurate events and flow labels. In addition, we design an \textbf{A}daptive \textbf{D}ensity \textbf{M}odule (ADM) based on MDR, which can adjust the densities of events, one of the most important factors for event-based tasks but has been largely overlooked. 

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\linewidth]{images/Datapipeline.pdf}
    \caption{Our data generation pipeline. Given 3D scenes in graphics engine, we generate camera trajectories and render high-frame-rate videos with forward and backward optical flow labels. Then, we build the event optical flow dataset by generate events using the videos.}
    \label{fig:data_pipeline}
\end{figure*}

Specifically, our MDR dataset contains $80,000$ samples from $50$ virtual scenes. Each data sample is created by first rendering two frames and obtaining the GT flow  labels directly from the engine. Then, we render $15\sim60$ frames in-between based on the flow magnitude. The events are created by thresholding log intensities and recording the timestamp for each spatial location. The density of events can be controlled by the threshold values. The ADM is designed as a plugin module, which further consists of two sub-modules, multi-density changer (MDC) and multi-density selector (MDS), where the MDC adjusts the density globally while the MDS picks the best one for every spatial location. Experiments show that previous event-flow methods, when trained on our MDR dataset, can improve their performances. Moreover, we train several recent representative flow pipelines, such as FlowFormer~\cite{huang2022flowformer}, KPA-Flow~\cite{luo2022kpa}, GMA~\cite{jiang2021gma} and SKFlow~\cite{sun2022skflow}, on our MDR dataset. When equipped with our ADM module, the performances can increase consistently. 

Our contributions are summarized as:
\begin{itemize}
    \item 
    A rendered event-flow dataset MDR, with 80,000 samples created on 53 virtual scenes, which possess physically correct accurate events and flow label pairs, covering a wide range of densities. 
    
    \item
    An adaptive density module (ADM), which is a plug-and-play module for handling varying event densities.
    
    \item
    We achieve state-of-the-art performances. Our MDR can improve the quality of previous event-flow methods. Various optical flow pipelines when adapted to the event-flow task, can benefit from our ADM module.
\end{itemize}