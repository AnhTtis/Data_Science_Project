\section{Experiment \label{Sec:Exp}}

\subsection{Datasets and Implementation Details}
\noindent\textbf{Datasets.} We primarily focus on \textit{How2Sign}~\cite{Duarte_CVPR2021} dataset. Our model is also evaluated on \textit{PHOENIX-2014T}~\cite{camgoz2018neural} and \textit{CSL-Daily}~\cite{zhou2021improving}, which are primarily used for sign language recognition and translation in previous works.

\textit{How2Sign} is a large-scale continuous American sign language (ASL) dataset consisting of a parallel corpus of about 80 hours of sign videos with subtitle annotations. It covers a wide range of instructional videos corresponding to various categories. There are 31164, 1740 and 2356 sign-video-text pairs in training, validation and test sets, respectively. Following SPOT-ALIGN~\cite{duarte2022sign}, we remove the invalid pairs where the subtitle alignment is detected to exceed the video duration, remaining 31085, 1739 and 2348 available pairs in training, validation and test sets, respectively. The resolution of sign videos is 1280$\times$720, we crop the human bodies of signers with Faster R-CNN~\cite{ren2015faster} to generate valid videos.

\textit{PHOENIX-2014T} is a German sign language (Deutsche Geb√§rdensprache, DGS) dataset collected in the domain of weather forecast from TV broadcast, consisting of 7096, 519 and 642 video-text pairs in training, validation and test sets, respectively.

\textit{CSL-Daily} is a recently released Chinese sign language (CSL) dataset. The topic of CSL-Daily revolves around people's daily lives, including 18401, 1077, 1176 parallel samples in training, validation and test set, respectively. 

\noindent\textbf{Evaluation Metric.} Following previous works~\cite{duarte2022sign,radford2021learning,liu2019use,sun2019videobert}, retrieval performance is evaluated by recall at rank K (R@K, higher is better) and median rank (MedR, lower is better). We evaluate our approach on both text-to-sign-video (T2V) retrieval and sign-video-to-text (V2T) retrieval tasks. We report R@1, R@5, and R@10 in all experiments, and additionally report MedR when comparing with state-of-the-art approaches. 

\noindent\textbf{Implementation Details.}
The sign encoder takes videos of resolution of 256$\times$256 as input. The domain-agnostic sign encoder is a I3D~\cite{carreira2017quo} network pre-trained on BSL-1K~\cite{varol2021read}. In pseudo label generation, we set the threshold $\lambda$ to 0.6 to filter samples with low-confidence. Non-maximum suppression (NMS) with a temporal window of 24 frames is utilized to remove the duplicates among the pseudo-labeled samples. A collection of approximate 64K pseudo-labeled samples covering a vocabulary of 1220 words is eventually generated. The domain-aware sign encoder is initialized with the domain-agnostic one and fine-tuned with a learning rate of $1\times 10^{-2}$ and batch size of 4 for 15 epochs. In the training of cross-lingual contrastive learning, the vision transformer and text transformer are initialized by the image encoder and text encoder in CLIP (ViT-B/32)~\cite{radford2021learning}. The maximum length of sign clip features and text features are set to 64 and 32, respectively. The model is fine-tuned with Adam optimizer~\cite{KingmaB14} with batch size of 512. The initial learning rate is set to $1\times 10^{-5}$, which is decreased with a cosine schedule following the CLIP~\cite{radford2021learning}. We set $\alpha=0.8$ in Eq.~\ref{eq:feature_fusion} and $\beta=0.5$ in Eq.\ref{eq:loss:overall}. The languages of \textit{PHOENIX-2014T} and \textit{CSL-Daily} are German and Chinese respectively. Since CLIP is trained on English corpus, to reuse CLIP's text encoder, we utilize Google translation to translate the texts of these two datasets into English.


\subsection{Comparison with State-of-the-art Methods}

 \begin{table}[t!]
 \scriptsize
 \centering
 \setlength{\tabcolsep}{1pt}
\begin{tabular}{c|cccc|cccc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{4}{c|}{T2V} & \multicolumn{4}{c}{V2T} \\
 & R@1$\uparrow$  & R@5$\uparrow$ & R@10$\uparrow$ & MedR$\downarrow$ & R@1$\uparrow$  & R@5$\uparrow$ & R@10$\uparrow$ & MedR$\downarrow$ \\
\midrule
SA-SR~\cite{duarte2022sign} & 18.9 & 32.1 & 36.5 & 62.0 & 11.6 & 27.4 & 32.5 & 69.0 \\
SA-CM~\cite{duarte2022sign} & 24.3 & 40.7 & 46.5 & 16.0 & 17.9 & 40.1 & 46.9 & 14.0 \\
SA-COMB~\cite{duarte2022sign} & 34.2 & 48.0 & 52.6 & 8.0 & 23.6 & 47.0 & 53.0 & 7.5 \\
\midrule
\textbf{Ours} & \textbf{56.6} & \textbf{69.9} & \textbf{74.7} & \textbf{1.0} & \textbf{51.6} & \textbf{64.8} & \textbf{70.1} & \textbf{1.0}\\
\bottomrule
\end{tabular}
\caption{Comparison with the different variants of the pioneer SPOT-ALIGN (SA)~\cite{duarte2022sign} on How2Sign~\cite{Duarte_CVPR2021} dataset.}
\vspace{-1em}
\label{Tab:comparison_SOTA_how2sign}
\end{table}


\begin{table}[t!]
 \scriptsize
 \centering
 \setlength{\tabcolsep}{1pt}
\begin{tabular}{c|cccc|cccc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{4}{c|}{T2V} & \multicolumn{4}{c}{V2T} \\
 & R@1$\uparrow$  & R@5$\uparrow$ & R@10$\uparrow$ & MedR$\downarrow$ & R@1$\uparrow$  & R@5$\uparrow$ & R@10$\uparrow$ & MedR$\downarrow$ \\
 \midrule
Translation~\cite{camgoz2020sign}  & 30.2 & 53.1 & 63.4 & 4.5 & 28.8 & 52.0 & 60.8 & 56.1 \\
SA-CM~\cite{duarte2022sign}  & 48.6 & 76.5 & 84.6 & 2.0 & 50.3 & 78.4 & 84.4 & \textbf{1.0} \\
SA-COMB~\cite{duarte2022sign}  & 55.8 & 79.6 & 87.2 & \textbf{1.0} & 53.1 & 79.4 & 86.1 & \textbf{1.0} \\
 \midrule
\textbf{Ours}  & \bf69.5	&\bf86.6 &	\bf92.1	&\bf1.0 & \bf70.2& \bf88.0&  \bf92.8& \bf1.0\\
\bottomrule
\end{tabular}
\caption{Comparison with the different variants of the pioneer SPOT-ALIGN (SA)~\cite{duarte2022sign} on PHOENIX2014T~\cite{camgoz2018neural} dataset.}
\label{Tab:comparison_SOTA_PHOENIX}
\vspace{-1em}
\end{table}


\begin{table}[t!]
 \scriptsize
 \centering
 \setlength{\tabcolsep}{1pt}
\begin{tabular}{c|cccc|cccc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{4}{c|}{T2V} & \multicolumn{4}{c}{V2T} \\
 & R@1$\uparrow$  & R@5$\uparrow$ & R@10$\uparrow$ & MedR$\downarrow$ & R@1$\uparrow$  & R@5$\uparrow$ & R@10$\uparrow$ & MedR$\downarrow$ \\
 \midrule
Ours &75.3 & 88.2 & 91.9 &1.0&74.7&89.4&92.2&1.0\\
\bottomrule
\end{tabular}
\caption{We additionally provide a baseline for CSL-Daily~\cite{zhou2021improving} dataset.}
\label{Tab:baseline_CSL}
\vspace{-6mm}
\end{table}

We compare our method with different variants of the pioneer, called SPOT-ALIGN~\cite{duarte2022sign}, on How2Sign and PHOENIX-2014T. We also provide the results on CSL-Daily as a baseline.


Table~\ref{Tab:comparison_SOTA_how2sign} and Table~\ref{Tab:comparison_SOTA_PHOENIX} show the comparisons between our approach and SPOT-ALIGN~\cite{duarte2022sign} on How2Sign and PHOENIX-2014T, respectively. SPOT-ALIGN builds the final combination (COMB) model by integrating its primary cross-modal (CM) model with an auxiliary retrieval model (sign recognition (SR) model for How2Sign and Translation~\cite{camgoz2020sign} for PHOENIX-2014T). Our method outperforms the COMB model, which achieves best results in SPOT-ALIGN, by large margins, achieving +22.4 T2V and +28.0 V2T R@1 improvements on How2Sign, +13.7 T2V and +17.1 V2T R@1 improvements on PHOENIX-2014T. It is worth mentioning that the SPOT-ALIGN conducts three rounds of sign spotting~\cite{albanie2020bsl,momeni20_bsldict} and encoder training. In contrast, we simplify the training of sign encoder and only perform a single round of training on pseudo-labeled data. We also provide a baseline on CSL-Daily dataset as shown in Table~\ref{Tab:baseline_CSL}, demonstrating that our model can be generalized to various sign languages.

\subsection{Ablation Study \label{Subsec:Exp_ablation}}
We conduct all ablation studies on the most challenging How2Sign dataset.

\noindent\textbf{The Effectiveness of Each Proposed Component.}
Table~\ref{Tab:ablation_overall} shows the ablation of each component. We first build a baseline where the sign encoder is the domain-agnostic one and contrastive learning is trained with the standard contrastive loss~\cite{radford2021learning}. Then we add the proposed sign encoder (SE), cross-lingual contrastive learning (CLCL) and text augmentation (TA) to the baseline step by step. SE encodes domain-relevant and discriminative features, yielding +1.8 T2V and +4.5 V2T R@1 gains. The proposed CLCL significantly boosts the performance by +20.7 T2V and +19.1 V2T R@1 improvements, demonstrating that identifying fine-grained sign-to-word mappings is essential for sign language retrieval. The introduction of TA further promotes the retrieval task, achieving 56.6 R@1 on V2T and 51.6 R@1 on T2V, respectively.

\noindent\textbf{Various Sign Encoders.} As described in Section~\ref{Subsec:SFDA}, our sign encoder is composed of a domain-agnostic sign encoder (Single-Ag)~\cite{varol2021read} and a domain-aware sign encoder (Single-Aw). We first report the results of each individual encoder in Table~\ref{Tab:ablation_video_feature}. Next, we study the different ways to integrate the features extracted by these two encoders, including average and weighted sum as defined in Eq.~\ref{eq:feature_fusion}. The results are also shown in Table~\ref{Tab:ablation_video_feature}. We experimentally find that the weighted sum strategy yields best results.


\begin{table}[t]
 \scriptsize
 \centering
 \setlength{\tabcolsep}{4pt}
\begin{tabular}{l|ccc|ccc}
\toprule
\multirow{2}{*}{Method}& \multicolumn{3}{c|}{T2V} & \multicolumn{3}{c}{V2T} \\ 
  & R@1  & R@5  & R@10 & R@1  & R@5  & R@10\\
 \midrule
Baseline &31.5 & 49.6 & 57.9   & 26.4 & 44.4 & 52.8 \\
\midrule
+SE&33.3 & 51.9 & 59.8 & 30.9 & 48.8 & 56.4  \\
 % & \checkmark & &52.2 & 66.8 & 71.6 &  47.8 & 61.2 & 67.0  \\
  +SE+CLCL& 54.0 & 67.1 & 71.9 & 50.0 & 63.2 & 68.1  \\
 %& \checkmark & \checkmark & 53.1 & 68.0 & 73.4  & 47.3 & 62.9 & 67.0   \\
 +SE+CLCL+TA& \bf56.6 & \bf69.9 & \bf74.7  & \bf 51.6 & \bf64.8 & \bf70.1  \\
\bottomrule
\end{tabular}
\caption{Ablation study of each proposed component. SE: sign encoder; CLCL: cross-lingual contrastive learning; TA: text augmentation.}
\label{Tab:ablation_overall}
\vspace{-1em}
\end{table}


\begin{table}[t]
 \scriptsize
 \centering

 \setlength{\tabcolsep}{4pt}
\begin{tabular}{l|ccc|ccc}
\toprule
\multirow{2}{*}{Encoder} & \multicolumn{3}{c|}{T2V} & \multicolumn{3}{c}{V2T} \\
  &  R@1  & R@5  & R@10 & R@1  & R@5  & R@10\\
 \midrule
Single-Ag~\cite{varol2021read} &  53.1 & 68.0 & 73.4 &   47.3 & 62.9 & 67.0  \\
Single-Aw & 54.1 & 67.5 & 73.1   & 49.1 & 61.8 & 67.4  \\
\midrule
Fusion-Average &  54.7 & 68.7 & 73.8 &    49.6 & 63.7 & 69.0 \\
\bf{Fusion-Weighted Sum} &  \bf56.6 & \bf69.9 & \bf74.7  & \bf51.6 & \bf64.8 & \bf70.1  \\
\bottomrule
\end{tabular}
\caption{Results of domain-agnostic sign encoder (Single-Ag) and domain-aware sign encoder (Single-Aw). We also study different ways to integrate the features extracted by Single-Ag and Single-Aw, including average and weighted sum.} 
\label{Tab:ablation_video_feature}
\vspace{-5mm}
\end{table}

\noindent\textbf{Variants of Cross-Lingual Contrastive Learning.} 
As described in Section~\ref{Subsec:SWCL} and illustrated in Figure~\ref{fig:overview_B}, through the use of a combination of softmax, multiplication, sum, and average operations, we convert the fine-grained cross-lingual (sign-to-word) similarity to the coarse-grained sign-video-to-text similarity to enable contrastive learning. We refer to this process as ``Softmax''. Here we study two variants termed ``Mean'' and ``Max''. The ``Mean'' and ``Max' strategies simply replace the combination of softmax, multiplication and sum operations with a simple mean operation and a max operation, respectively. The results are shown in Table~\ref{Tab:ablation_sim_calcu}. We observe that the ``Mean'' strategy performs worst since it merely evaluates the overall similarity of a text and a sign video and ignores the fine-grained sign-to-word mappings during training. The ``Max'' strategy identifies hard sign-to-word mappings, \textit{i.e.}, one sign is associated with the most similar word, and vice versa. Nevertheless, since we do not have the ground-truth of sign-to-word mappings, it is challenging for models to identify the confident one-to-one mappings in a weakly supervised manner, as discussed in previous works on multiple instance learning~\cite{momeni20_bsldict}. In contrast, our default ``Softmax'' strategy localizes the soft sign-to-word mappings and achieves the best results.

\begin{table}[t]
 \scriptsize
 \centering
 \setlength{\tabcolsep}{4pt}
 
\begin{tabular}{l|ccc|ccc}
\toprule
\multirow{2}{*}{Strategy} & \multicolumn{3}{c|}{T2V} & \multicolumn{3}{c}{V2T} \\
 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\
 \midrule 

Mean & 33.1 & 52.5 & 59.7 & 29.8 & 47.8 & 55.3 \\
Max & 42.2 & 59.7 & 66.0 & 38.5 & 55.1 & 62.0 \\
\bf{Softmax}    & \bf56.6 & \bf69.9 & \bf74.7 & \bf51.6 & \bf64.8 & \bf70.1 \\
\bottomrule
\end{tabular}
\caption{Study on different strategies to identify the fine-grained sign-to-word mappings in cross-lingual contrastive learning.}
\label{Tab:ablation_sim_calcu}
     \vspace{-2mm}
\end{table}

\noindent\textbf{Study of Text Augmentations.}
In Table~\ref{Tab:ablation_text_aug}, we study three text augmentations described in Section~\ref{Subsec:TA}: random delete (RD), synonym replacement (SR) and random swap (RS). We observe a slight performance drop when introducing RD and SR into training. Sign language retrieval is not only a video-text retrieval task, but also a cross-lingual retrieval challenge, deletion and replacement may break the intrinsic sign-to-word mappings. In contrast, RS augmentation preserves the semantics of texts and we observe a +2.6 T2V and a +1.6 V2T R@1 improvements over the counterpart without any text augmentations.

\begin{table}[t]
 \centering
 \scriptsize
 \setlength{\tabcolsep}{4pt}
\begin{tabular}{c|ccc|ccc}
\toprule
\multirow{2}{*}{\makecell[c]{Text\\Augmentation}} 
&\multicolumn{3}{c|}{T2V} & \multicolumn{3}{c}{V2T} \\
&  R@1  & R@5  & R@10 & R@1  & R@5  & R@10\\
 \midrule
None & 54.0 & 67.1 & 71.9 &   50.0 & 63.2 & 68.1   \\
RD & 52.7 & 66.3 & 71.6   & 47.6 & 62.1 & 67.4   \\
SR &53.9 & 68.7 & 73.0  & 47.5 & 61.2 & 65.9  \\
\bf{RS} & \bf56.6 & \bf69.9 & \bf74.7 &   \bf51.6 & \bf64.8 & \bf70.1 \\
\bottomrule
\end{tabular}
\caption{Ablation study on different text augmentations. RD: random delete; SR: synonym replacement; RS: random swap.}
\label{Tab:ablation_text_aug}
\vspace{-2mm}
\end{table}


\noindent\textbf{The Effects of CLIP Initialization.} In our framework, the vision and text Transformer are initialized with the CLIP's vision encoder and text encoder. Note that the input modality of our vision Transformer and that of CLIP's vision encoder are different. Ours takes a feature sequence as input while the input of CLIP's image encoder is a set of image patches. In Table~\ref{Tab:ablation_vision_encoder}, we compare a randomly initialized baseline with the one initialized by CLIP. We find that CLIP-initialization significantly improves the performance, yielding +11.4 T2V and +11.0 V2T R@1 improvements though the input originates from a completely distinct modality.



\noindent\textbf{Study on Text Transformer.}
The text Transformer in our model is initialized with the CLIP's text encoder.
Considering the generalization capability of the pre-trained CLIP, we attempt to freeze the shallow layers of our text transformer. As shown in Table~\ref{Tab:ablation_text_encoder}, the performance gradually decreases as the number of the frozen layers increases. CLIP is trained on large-scale image-text pairs, however, the domain gap between image-text data and sign-video-text data is non-negligible. Though CLIP shows promising transfer capacity, it is optimal to fine-tune the whole model on target datasets.


\begin{table}[t!]
 \scriptsize
 \centering
 \setlength{\tabcolsep}{4pt}
 
\begin{tabular}{c|ccc|ccc}

\toprule
\multirow{2}{*}{\makecell[c]{ Initialization}}  & \multicolumn{3}{c|}{T2V} & \multicolumn{3}{c}{V2T} \\
 &R@1  & R@5  & R@10 & R@1  & R@5  & R@10\\
 \midrule
Random & 45.2&	60.3&	67.5& 40.6&	54.5&	59.7\\
\textbf{CLIP} & \bf56.6&	\bf69.9& \bf74.7&  \bf51.6&	\bf64.8&	\bf70.1\\

\bottomrule
\end{tabular}
\caption{Comparison between CLIP initialization and random initialization.}
\label{Tab:ablation_vision_encoder}
\vspace{-3mm}
\end{table}



\begin{table}[t]
 \scriptsize
 \centering
 \setlength{\tabcolsep}{4pt}

\begin{tabular}{c|ccc|ccc}

\toprule
\multirow{2}{*}{\makecell[c]{Frozen  layers}} & \multicolumn{3}{c|}{T2V} & \multicolumn{3}{c}{V2T} \\
  &R@1  & R@5  & R@10 & R@1  & R@5  & R@10\\
 \midrule

 \textbf{None}   & \bf56.6 & \bf69.9 & \bf74.7   & \bf51.6 & \bf64.8 & \bf70.1 \\
 
\midrule

1   & 54.4 & 68.2 & 73.4   & 49.7 & 63.5 & 68.8   \\
2   & 53.8 & 68.6 & 73.3   & 48.5 & 62.8 & 67.9   \\ 
6  & 51.7 & 66.6 & 71.1   & 46.2 & 60.9 & 66.6   \\
12   & 49.5 & 65.2 & 71.4   & 44.7 & 59.7 & 65.2   \\

\bottomrule
\end{tabular}
\caption{Study of freezing different layers of text Transformer.}
\label{Tab:ablation_text_encoder}
\vspace{-2mm}
\end{table}




