\section{Introduction}
\label{sec:intro}
Sign languages are the primary means of communication used by people who are deaf or hard of hearing. Sign language understanding~\cite{koller2015continuous,cihan2017subunets,cui2019deep,koller2019weakly,chen2022simple,camgoz2020sign,albanie2020bsl,varol2021read,duarte2022sign,chen2022twostream,zuo2023natural} is significant for overcoming the communication barrier between the hard-of-hearing and non-signers. Sign language recognition and translation (SLRT) has been extensively studied, with the goal of recognizing the \textit{arbitrary} semantic meanings conveyed by sign languages. However, the lack of available data significantly limits the capability of SLRT. In this paper, we focus on developing a framework for a recently proposed sign language retrieval task~\cite{duarte2022sign}. Unlike SLRT, sign language retrieval focuses on retrieving the meanings that signers express from a \textit{closed-set}, which can significantly reduce error rates in realistic deployment. 


\begin{figure}
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/teaser_a.pdf}
         \caption{Text-to-sign-video (T2V) retrieval.}
         \label{fig:teaser_A}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/teaser_b.pdf}
         \caption{Sign-video-to-text (V2T) retrieval.}
         \label{fig:teaser_B}
     \end{subfigure}
     \hfill
    \caption{Illustration of: (a) T2V retrieval; (b) V2T retrieval.}
    \label{fig:teaser}
    \vspace{-5mm}
\end{figure}


\begin{figure}
     \centering

    \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/teaser2_a.pdf}
         \caption{While contrasting the sign videos and the texts in a joint embedding space, we simultaneously identify the fine-grained cross-lingual (sign-to-word) mappings of sign languages and natural languages via the proposed cross-lingual contrastive learning. Existing datasets do not annotate the sign-to-word mappings.}
         \label{fig:teaser_2A}
     \end{subfigure}
     \hfill

     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/teaser2_b.pdf}
         \caption{We show four instances of the sign ``Book'' in How2Sign~\cite{Duarte_CVPR2021} dataset, which are identified by our approach. Please refer to the supplementary material for more examples.}
         \label{fig:teaser_2B}
     \end{subfigure}
     \hfill
    \caption{Illustration of: (a) cross-lingual (sign-to-word) mapping; (b) sign-to-word mappings identified by our CiCo.}
    \label{fig:teaser2}
    \vspace{-5mm}
\end{figure}

Sign language retrieval is both similar to and distinct from the traditional video-text retrieval. On the one hand, like video-text retrieval, sign language retrieval is also composed of two sub-tasks, \textit{i.e.}, text-to-sign-video (T2V) retrieval and sign-video-to-text (V2T) retrieval. Given a free-form written query and a large collection of sign language videos, the objective of T2V is to find the video that best matches the written query (Figure~\ref{fig:teaser_A}). In contrast, the goal of V2T is to identify the most relevant text description given a query of sign language video (Figure~\ref{fig:teaser_B}).

On the other hand, different from the video-text retrieval, sign languages, like most natural languages, have their own grammars and linguistic properties. Therefore, sign language videos not only contain visual signals, but also carry semantics (\textit{i.e.}, sign\footnote{We use sign to denote lexical item within a sign language vocabulary.}-to-word mappings between sign languages and natural languages) by themselves, which differentiates them from the general videos that merely contain visual information. Considering the linguistic characteristics of sign languages, we formulate sign language retrieval as a cross-lingual retrieval~\cite{ballesteros1996dictionary,lavrenko2002cross,tran2020cross} problem in addition to a video-text retrieval~\cite{yu2018joint,gabeur2020multi,bain2021frozen,LUO2022293,liu2021hit,liu2019use,sun2019videobert} task.

Sign language retrieval is extremely challenging due to the following reasons: (1) Sign languages are completely separate and distinct from natural languages since they have unique linguistic rules, word formation, and word order. The transcription between sign languages and natural languages is complicated, for instance, the word order is typically not preserved between sign languages and natural languages. It is necessary to automatically identify the sign-to-word mapping from the cross-lingual retrieval perspective; (2) In contrast to the text-video retrieval datasets~\cite{miech2019howto100m,monfort2021spoken} which contain millions of training samples, sign language datasets are orders of magnitude smaller in scaleâ€”for example, there are only 30K video-text pairs in How2Sign~\cite{Duarte_CVPR2021} training set; (3) Sign languages convey information through the handshape, facial expression, and body movement, which requires models to distinguish fine-grained gestures and actions; (4) Sign language videos typically contain hundreds of frames. It is necessary to build efficient algorithms to lower the training cost and fit the long videos as well as the intermediate representations into limited GPU memory.   

In this work, we concentrate on resolving the challenges listed above:
\begin{itemize}[leftmargin=*]
    \item We consider the linguistic rules (\textit{e.g.}, word order) of both sign languages and natural languages. We formulate sign language retrieval as a cross-lingual retrieval task as well as a video-text retrieval problem. While contrasting the sign videos and the texts in a joint embedding space as achieved in most vision-language pre-training frameworks~\cite{sun2019videobert,bain2021frozen,LUO2022293}, we simultaneously identify the fine-grained cross-lingual (sign-to-word) mappings between two types of languages via our proposed cross-lingual contrastive learning as shown in Figure~\ref{fig:teaser2}.
    \item Data scarcity typically brings in the over-fitting issue. To alleviate this issue, we adopt transfer learning and adapt a recently released domain-agnostic sign encoder~\cite{varol2021read} pre-trained on large-scale sign-videos to the target domain. Although this encoder is capable of distinguishing the fine-grained signs, direct transferring may be sub-optimal due to the unavoidable domain gap between the pre-training dataset and sign language retrieval datasets. To tackle this problem, we further fine-tune a domain-aware sign encoder on pseudo-labeled data from target datasets. The final sign encoder is composed of the well-optimized domain-aware sign encoder and the powerful domain-agnostic sign encoder. 
    \item In order to effectively model long videos, we decouple our framework into two disjoint parts: (1) a sign encoder which adopts a sliding window on sign-videos to pre-extract their vision features; (2) a cross-lingual contrastive learning module which encodes the extracted vision features and their corresponding texts in a joint embedding space.
\end{itemize}

Our framework, called domain-aware sign language retrieval via \textbf{C}ross-l\textbf{i}ngual \textbf{Co}ntrastive learning or CiCo for short, outperforms the pioneer SPOT-ALIGN~\cite{duarte2022sign} by large margins on various datasets, achieving 56.6 (+22.4) T2V and 51.6 (+28.0) V2T R@1 accuracy (improvement) on How2Sign~\cite{Duarte_CVPR2021} dataset, and 69.5 (+13.7) T2V and 70.2 (+17.1) V2T R@1 accuracy (improvement) on PHOENIX-2014T~\cite{camgoz2018neural} dataset. With its simplicity and strong performance, we hope our approach can serve as a solid baseline for future research.


