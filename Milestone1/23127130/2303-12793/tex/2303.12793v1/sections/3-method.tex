\section{Methodology}
\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.95\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/overview_A.pdf}
         \caption{Illustration of sign encoder.}
         \label{fig:overview_A}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.95\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/overview_B.pdf}
         \caption{Illustration of cross-lingual contrastive learning.}
         \label{fig:overview_B}
     \end{subfigure}
     \hfill
    \caption{Overview of our framework. (a) Sign encoder is composed of a powerful domain-agnostic sign encoder pre-trained on large-scale sign videos, and a domain-aware sign encoder fine-tuned on pseudo-labeled data from target datasets. We adopt a sliding window manner to extract a discriminative and domain-aligned feature per clip. (b) Cross-lingual contrastive learning takes $N$ sign-video-text pairs as inputs and contrasts paired data in a shared embedding space while implicitly identifying the fine-grained sign-to-word mappings during training.
    %Sign feature domain alignment (SFDA) module aims to provide a strong sign feature for subsequent cross-modal learning. Without sign-level annotation, SFDA turn to generalize a powerful domain-agnostic sign encoder to target dataset with pseudo labeling. The fine-tuned domain-aware sign encoder is expected to better align in the target sign language domain. Besides, a feature fusion strategy is introduced to make full use of two complementary sign encoders. (b) Cross-lingual contrastive learning (CLCL) aims to handle the retrieval task by learning a joint embedding space for sign video features and texts. Fine-grained cross-modal interactions are proposed in CLCL to model the complicated sign-to-word mappings between sign language and natural language.
    }
    \label{fig:overview}
        \vspace{-5mm}
\end{figure*}

In this section, we first formulate the task of sign language retrieval in Section~\ref{sec:task}. Next, we introduce our framework which is composed of two parts: 1) a sign encoder which extracts discriminative and domain-aligned features of sign videos (Section~\ref{Subsec:SFDA}); 2) a cross-lingual contrastive learning framework, which contrasts sign-video-text pairs while concurrently identifying the fine-grained sign-to-word mappings (Section~\ref{Subsec:SWCL}). At last, we explore text augmentations in Section~\ref{Subsec:TA}.

\subsection{Task Formulation}
\label{sec:task}
Let $\mathcal{V}$ and $\mathcal{T}$ denote a set of sign videos and their corresponding texts (transcriptions), respectively. Sign language retrieval consists of two tasks namely text-to-sign-video retrieval (T2V), and sign-video-to-text retrieval (V2T), respectively. The objective of T2V is to find the sign video $v\in\mathcal{V}$ whose signing content best matches the text query. In contrast, the reverse task V2T requires the model to identify the most relevant text (transcription) $t\in\mathcal{T}$ given a query of sign video. We resolve sign language retrieval by learning a joint embedding space of sign videos and texts.

\subsection{Sign Encoder}
\label{Subsec:SFDA}
\noindent\textbf{Process Sign Videos with Sliding Window.} Sign videos from sign language retrieval datasets typically contain hundreds of frames. To efficiently train our model and lower the usage of GPU memory, given a sign video, we adopt a sliding window manner with stride of 1 and window size of 16 to produce $M$ temporally overlapped clips. Next, we separately feed each clip into a sign encoder to extract its feature. The final sign-video feature is yielded by stacking features coming out of $M$ clips along temporal dimension. A powerful sign encoder is crucial.

\noindent\textbf{Overview of Sign Encoder.} Recent advances in sign spotting~\cite{momeni20_bsldict,varol2021read} greatly facilitate the collection of large-scale sign language datasets, enabling powerful representation learning abilities of convolutional neural networks on the sign classification task. Previous methods~\cite{duarte2022sign,das2022machine} have demonstrated the feasibility of transferring a sign encoder pre-trained on large-scale sign-spotting data into downstream tasks. We follow this practice and use an I3D network pre-trained on BSL-1K~\cite{varol2021read}, a sign classification dataset collected via sign spotting, as our primary sign encoder. Due to its favorable transfer performance, we term this model as a domain-agnostic sign encoder. Nevertheless, the domain gap between BSL-1K and sign language retrieval datasets is non-negligible. To tackle this problem, we further fine-tune a domain-aware sign encoder, which has an identical architecture to the domain-agnostic sign encoder, on target datasets through pseudo-labeling. The final sign encoder is composed of the well-optimized domain-aware sign encoder and the powerful domain-agnostic sign encoder, as illustrated in Figure~\ref{fig:overview_A}.

\noindent\textbf{Pseudo-Labeling on Target Datasets.} Now we describe the details of pseudo-labeling. Given a sign video from a target dataset, we adopt a sliding window with the stride of 1 and the window size of 16 to generate a set of temporally overlapped clips. For each clip, we first utilize the pre-trained domain-agnostic sign encoder to produce its prediction. Then we binarize the prediction with a pre-defined threshold $\lambda$ to generate the corresponding pseudo label. The invalid samples, whose maximum score is lower than $\lambda$, are filtered. We repeat the above process for all sign videos and eventually build a pseudo-labeled set. Our domain-aware sign encoder, which is initialized by the domain-agnostic sign encoder, is fine-tuned on the pseudo-labeled set via a standard cross-entropy loss for classification training.

\noindent\textbf{Feature Extraction with Sign Encoder.}
So far, we acquire a domain-aware sign encoder approximately aligned in target domain.
Nevertheless, its capability is restricted by the unavoidable noises in pseudo-labels and the limited amount of pseudo-labeled samples.
Recall that we already have a powerful domain-agnostic sign encoder pre-trained on large-scale dataset in hand, inspiring us to make use of both domain-agnostic sign encoder $h_{\xi}(\cdot)$ and domain-aware sign encoder $h_{\theta}(\cdot)$ to extract discriminative and domain-aligned features. Our final sign encoder $H(\cdot)$, as shown in Figure~\ref{fig:overview_A}, is a weighted combination of $h_{\xi}(\cdot)$ and $h_{\theta}(\cdot)$ with a trade-off hyper-parameter $\alpha$. As described above, $H(\cdot)$ encodes sign videos in a sliding window manner. For simplicity, we use $H(v)$ to denote feature extraction on sign video $v$, which is formulated as: 
\begin{equation}
 \label{eq:feature_fusion}
 H(v) = \alpha h_{\xi}(v) + (1-\alpha) h_{\theta}(v).
\end{equation}

\subsection{Cross-Lingual Contrastive Learning}
\label{Subsec:SWCL}
The objective of cross-lingual contrastive learning (CLCL) is to learn a joint embedding space of sign videos and texts while concurrently identifying the fine-grained sign-to-word mappings during training. An overview is shown in Figure~\ref{fig:overview_B}. CLCL takes a mini-batch $\{(v_{n}, t_{n})\}^{N}_{
n=1}$ containing $N$ sign-video-text pairs as input, and contrasts paired data in a shared embedding space for sign language retrieval.

\noindent\textbf{Sign Features and Word Features.}
Given a sign video $v \in \{v_{n}\}^{N}_{
n=1}$, we first adopt our sign encoder $H(\cdot)$ described in Section~\ref{Subsec:SFDA} to extract its intermediate feature. Note $H(\cdot)$ encodes sign videos in a sliding window manner, and thus there are no interactions among different clips. To facilitate information exchange, we further append a 12-layer Transformer~\cite{Attention17} $F(\cdot)$ onto $H(\cdot)$ to extract sign features $\boldsymbol{S}$ of sign video $v$, which is formulated as $\boldsymbol{S}=F(H(v)) \in 
\mathbb{R}^{M \times D}$, where $M$ denotes the number of clips, and $D$ is the hidden dimension. Given a text $t \in \{t_{n}\}^{N}_{n=1}$, we convert $t$ into a lower-cased byte pair encoding (BPE) representation~\cite{sennrich2015neural}, which is subsequently fed into another 12-layer Transformer $G(\cdot)$ to generate the word features $\boldsymbol{W} = G(t) \in \mathbb{R}^{L \times D}$, where $L$ represents word number. 

Since CLIP~\cite{radford2021learning} shows excellent transfer capability in various downstream tasks~\cite{fort2021exploring,shen2021much,du2022learning,xu2022simple,xu2023side,huang2022unsupervised}, we initialize $F(\cdot)$ and $G(\cdot)$ with CLIP's image encoder (ViT-B) and text encoder, respectively, to ease the learning. Though CLIP's vision encoder takes image patches as inputs, we experimentally find that it generalizes well in our scenario where input data is in a different modality. 

\noindent\textbf{Cross-Lingual Similarity.} 
There exist inherent sign-to-word mappings between sign languages and natural languages. To incorporate this prior knowledge into learning, we introduce cross-lingual similarityâ€”an indicator to identify sign-to-word mappings between $i$-th sign video $v_i$ and $j$-th text $t_j$. Concretely, given sign features $\boldsymbol{S}_i \in \mathbb{R}^{M \times D}$ of $v_i$ , and word features $\boldsymbol{W}_j \in \mathbb{R}^{L \times D}$ of $t_j$, we calculate a cross-lingual similarity matrix $\boldsymbol{E}_{(i,j)} =\boldsymbol{S}_i \cdot \boldsymbol{W}_j^T \in \mathbb{R}^{M \times L}$. Each element in $\boldsymbol{E}_{(i,j)}$ represents the similarity of a sign clip in $v_i$ and a word in $t_j$.

\noindent\textbf{Cross-Lingual Contrastive Learning.} Directly apply supervisions on token-wise similarity matrix $\boldsymbol{E}_{(i,j)}$ is infeasible due to the absence of fine-grained sign-to-word annotations. Inspired by the recent progress of vision-language contrastive learning~\cite{radford2021learning,li2021align,li2021supervision,frome2013devise,gong2014multi,joulin2016learning}, we turn to contrast the global representations of sign videos and texts. The underlying idea is to calculate a global similarity $z$ of $v_i$ and $t_j$ based on $\boldsymbol{E}_{(i,j)} \in \mathbb{R}^{M \times L}$. 

\noindent\textbf{Sign-Video-to-Text Contrast.} We first introduce sign-video-to-text contrast as shown in Figure~\ref{fig:overview_B}. To be specific, we utilize a Softmax operation to each row of $\boldsymbol{E}_{(i,j)}$, and multiply the resulting matrix with $\boldsymbol{E}_{(i,j)}$ to generate a sign-to-word similarity matrix $\boldsymbol{E}'_{(i,j)} \in \mathbb{R}^{M \times L}$, where each row represents the re-weighted similarities between a sign clip in $v_i$ and all words in $t_j$. After that, we adopt a row-wise addition operation on $\boldsymbol{E}'_{(i,j)}$ to yield the sign-to-text similarity vector $\boldsymbol{e}_{(i,j)} \in \mathbb{R}^{M}$, where each element denotes a similarity of a sign clip in $v_i$ and whole text $t_j$. At last, we average all elements in $\boldsymbol{e}_{(i,j)}$ to produce the global similarity $z$ of sign video $v_i$ and text $t_j$.

In the same way, we can calculate the similarities for both positive pairs $\{(v_{i}, t_{i})\}^{N}_{
i=1}$ and negative pairs $\{(v_{i}, t_{j})\}^{N}_{
i=1,j=1,i\neq j}$ in a mini-batch, yielding a video-to-text similarity matrix $\boldsymbol{Z}_{V2T} \in \mathbb{R}^{N \times N}$, where $\boldsymbol{Z}^{(i,j)}_{V2T}$ denotes the global similarity of $v_i$ and $t_j$. Following CLIP~\cite{radford2021learning}, we adopt InfoNCE loss~\cite{gutmann2010noise} to pull the embeddings of matched image-text pairs together while pushing those of non-matched pairs apart, which is formulated as follows:
\begin{equation}
  \label{loss:NCE}
  \begin{split}
\mathcal{L}_{V2T}= & -\frac{1}{2N}\sum_{i=1}^{N} log \frac{exp(\boldsymbol{Z}_{V2T}^{(i,j)}/\tau)}{\sum_{j=1}^{N} exp(\boldsymbol{Z}_{V2T}^{(i,j)}/\tau)} \\
&-\frac{1}{2N}\sum_{j=1}^{N} log \frac{exp(\boldsymbol{Z}_{V2T}^{(i,j)}/\tau)}{\sum_{i=1}^{N} exp(\boldsymbol{Z}_{V2T}^{(i,j)}/\tau)}, 
  \end{split}
\end{equation}
where $\tau$ is a trainable temperature parameter. 

\noindent\textbf{Text-to-Sign-Video Contrast.}
Up to now, we have introduced sign-video-to-text contrast. A symmetrical version, termed text-to-sign-video contrast, shares the similar spirit as shown in Figure~\ref{fig:overview_B}. The implementation of text-to-sign-video contrast is extremely simple: we replace the row-wise operations (\textit{i.e.}, Softmax and addition) in sign-video-to-text contrast with column-wise ones and keep the remaining processes unchanged. We use $\mathcal{L}_{T2V}$ to denote the loss function of text-to-sign-video contrast. In our implementation, we reuse the loss defined in Eq~\ref{loss:NCE} but substitute the input with the text-to-video similarity matrix $\boldsymbol{Z}_{T2V}$.

\noindent\textbf{Loss Function.}
The overall loss for cross-lingual contrastive learning is a weighted sum of $\mathcal{L}_{V2T}$ and $\mathcal{L}_{T2V}$ with a trade-off hyper-parameter $\beta$:
\begin{equation}
\label{eq:loss:overall}
    \mathcal{L} = \beta \mathcal{L}_{V2T} + (1-\beta) \mathcal{L}_{T2V}.
\end{equation}

\subsection{Text Augmentation}
\label{Subsec:TA}
Considering that the datasets of sign language retrieval are typically small-scale, we explore text augmentations to improve the generalization of our approach. EDA~\cite{eda2019} introduces three simple yet efficient data augmentations in text classification task: random delete randomly removes words in a sentence; synonym replacement randomly selects words from a sentence that are not stop words and replaces them with synonyms; random swap randomly chooses two words in a sentence and swaps their positions. The first two augmentations have been proven effective in text classification task. However, we experimentally find that our focused sign language retrieval is sensitive to random delete and synonym replacement augmentations. To guarantee that the augmented texts preserve the original semantic meanings, we only adopt the random swap augmentation in our approach. We suppose there are two reasons: 1) the word order of sign languages and natural languages are constitutionally distinct, and reordering does not affect semantic meanings; 2) the proposed cross-lingual contrastive learning is insensitive to word order.


