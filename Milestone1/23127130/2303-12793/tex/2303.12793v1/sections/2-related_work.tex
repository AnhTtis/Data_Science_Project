\section{Related Work}

\noindent\textbf{Sign Language Understanding.} Sign language understanding aims at interpreting the semantic information conveyed within sign videos. Researchers have explored such capability on various tasks including sign language recognition~\cite{fillbrandt2003extraction,farhadi2007transfer,sutton2007mouthings,joze2018MSASL,li2020transferring,chen2022twostream,zuo2023natural}, sign spotting~\cite{albanie2020bsl,momeni20_bsldict,varol2021read,duarte2022sign}, sign language translation~\cite{chen2022simple,camgoz2018neural,zhou2021spatial,camgoz2020multi,li2020tspnet} and our focused sign language retrieval~\cite{duarte2022sign}. 

One of the fundamental tasks of sign language understanding is sign language recognition (SLR), which aims to transcribe a sign video into a gloss sequence. Previous works on SLR focus on designing carefully engineered features~\cite{fillbrandt2003extraction,farhadi2007transfer,sutton2007mouthings} or modeling temporal dependencies~\cite{starner1995visual,starner1998real}. Recently, the success of 3D convolutional neural networks in action related tasks~\cite{wang2018non,lin2019tsm,tran2018closer} is transferred to SLR. In particular, the I3D~\cite{carreira2017quo} architecture has proven to be effective for this task~\cite{joze2018MSASL,li2020transferring,li2020WSASL,albanie2020bsl,varol2021read}. In this work, we also adopt this network architecture in our sign encoder.

Sign spotting is a particular variant of sign language recognition. It aims to localize all instances of a given sign within an untrimmed video. Recent works tackle this task with auxiliary cue of subtitles, introducing automatic annotation systems by using mouthing ~\cite{albanie2020bsl}, dictionaries~\cite{momeni20_bsldict} and attention maps of Transformer~\cite{varol2021read}. SPOT-ALIGN~\cite{duarte2022sign} extends existing spotting methods~\cite{albanie2020bsl,momeni20_bsldict} with an iterative training schema, which alternates between repeated sign spotting and model fine-tuning. In this work, pseudo-labeling is served as our sign spotting approach to localize isolated signs in untrimmed videos from target sign language retrieval datasets. Compared with above efforts, our approach only employs a pre-trained sign encoder without utilizing additional auxiliary cue, which is proved to be simple yet efficient. 

Early works of sign language retrieval primarily investigate query-by-example searching~\cite{athitsos2010large,zhang2010usingRevi}, which queries individual instances with given sign examples. Our work focuses on free-form textual retrievalâ€”a recently introduced task by SPOT-ALIGN~\cite{duarte2022sign}. It symbolizes the real-world scenario of searching sign language videos with natural languages. The pioneer SPOT-ALIGN~\cite{duarte2022sign} purely formulates sign language retrieval as a video-text retrieval task, where the cross-modal alignment is modeled upon overall global embeddings of sign videos and texts. However, the linguistic properties of sign languages are ignored. In contrast, we formulate sign language retrieval as a joint task of text-video retrieval and cross-lingual retrieval.
 
\noindent\textbf{Vision-Language Models.} Learning general-purpose representations for visual and textual modalities is a long-standing topic~\cite{bengio2013representation,lecun2015deep}. The idea has been investigated decades ago~\cite{mori1999image}. Recently, the prominent success of CLIP~\cite{radford2021learning}, ALIGN~\cite{jia2021scaling} and ALBEF~\cite{li2021align} has demonstrated the capability of learning joint cross-modal representations with large-scale web data using simple image-text contrastive learning. Similar idea has also been explored in video-text retrieval, which is the closest area of our work. The common practice in image-text retrieval~\cite{radford2021learning,frome2013devise,gong2014multi,joulin2016learning} and video-text retrieval~\cite{yu2018joint,gabeur2020multi,bain2021frozen,LUO2022293,liu2021hit,liu2019use,sun2019videobert} is to encode the images/videos and texts into the overall representations. It is reasonable since the visual signals of typical image-text and video-text retrieval datasets mainly describe the certain objects or describable events. In contrast, sign videos, as the carriers of sign languages, convey abundant semantics by themselves. There exist fine-grained mappings between sign videos and natural languages. We find that identifying such cross-lingual (sign-to-word) mappings significantly boosts the performance of sign language retrieval.

\noindent\textbf{Cross-Lingual Information Retrieval.} In this work, we also formulate sign-language retrieval as a cross-lingual task. In natural language processing community, cross-lingual information retrieval (CLIR) ~\cite{xu2001evaluating,lavrenko2002cross,tran2020cross} refers to the task of retrieving documents between different languages. One of the most common approaches of CLIR is to learn sentence-level embedding alignment by mapping pre-acquired monolingual embeddings~\cite{mikolov2013exploiting,zhang2019girls,ormazabal2019analyzing} of different languages into a shared space. Recently, researchers have exploited fine-grained word-level mappings with self-training~\cite{artetxe2017learning,artetxe2018robust,tran2020cross}. They find that mappings can be learned by initiating with a seed dictionary and alternating between alignment modeling and dictionary mining. Our work also exploits fine-grained word-level mappings, called sign-to-word mappings, in the context of sign language retrieval. Identifying sign-to-word mappings is challenging since sign languages are expressed in visual modality. The proposed cross-lingual contrastive learning tackles this challenge by exploiting the fine-grained cross-modal interactions. 

