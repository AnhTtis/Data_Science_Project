\appendix
\section{More Experiments}

\renewcommand\arraystretch{1.0}
\noindent\textbf{CiCo vs CLIP.}\hspace{3pt} 
 We compare our approach CiCo with CLIP~\cite{radford2021learning}, which is one of the most representative vision-language models. CLIP can be easily generalized to sign language retrieval by replacing our cross-lingual contrastive learning with CLIP. The other settings including sign encoder and text augmentation still remain unchanged. As shown in Table~\ref{Tab:ablation_cico_vs_clip}, CiCo surpasses CLIP by +21.2 T2V and +20.9 V2T R@1 scores. The reason is that CLIP contrasts the overall features of two modalities, while our cross-lingual contrastive learning concentrates on identifying the fine-grained sign-to-word mappings during modeling global similarities of texts and sign videos. 

\begin{table}[t]
 \small
 \setlength{\tabcolsep}{4pt}
 \centering
\begin{tabular}{c|ccc|ccc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{3}{c|}{T2V} & \multicolumn{3}{c}{V2T} \\ 
 &   R@1  & R@5  & R@10 & R@1  & R@5  & R@10\\
 \midrule

CLIP~\cite{radford2021learning} & 35.4&53.4&60.9&30.7&49.1&57.1  \\
\textbf{CiCo}  &\bf56.6 & \bf69.9 & \bf74.7 & \bf51.6 & \bf64.8 & \bf70.1 \\
\bottomrule
\end{tabular}
\caption{Comparison between Cico and CLIP.}
\label{Tab:ablation_cico_vs_clip}
\end{table}

\noindent\textbf{Different Strategies of Global Similarity Calculation in Cross-Lingual Contrastive Learning.}\hspace{3pt} As described in Section 3.3 and illustrated in Figure 3b, we adopt ``Mean'' strategy which averages sign-to-text similarities and word-to-video similarities to obtain the global video-to-text similarity and text-to-video similarity, respectively. In Section 4.3 of the main paper, we study different strategies to identify the fine-grained sign-to-word mappings, now we investigate different ways of global similarity calculation.  Table~\ref{Tab:ablation_global_calcu} shows the results of two variants termed ``Max'' and ``Softmax'' besides the default ``Mean'' strategy. ``Max'' assigns global similarity with the maximum score of sign-to-text similarities (or word-to-video similarities). ``Softmax'' stands for a combination of Softmax, multiplication and sum (refer to Section 4.3 for details). The default ``Mean'' strategy achieves the best result.


\noindent\textbf{Sliding Window Stride in Sign Encoder.}\hspace{3pt} Our sign encoder adopts a sliding window manner to extract features of continuous sign videos. The default sliding window stride is set as 1. We vary the stride and show the results in Table~\ref{Tab:ablation_stride}. Setting stride as 1 yields the best performance.


\begin{table}[t]
 \small
 \centering
 \setlength{\tabcolsep}{4pt}
\begin{tabular}{l|ccc|ccc}
\toprule
\multirow{2}{*}{Strategy} & \multicolumn{3}{c|}{T2V} & \multicolumn{3}{c}{V2T} \\
 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\
\midrule

Max     & 21.1 & 38.0 & 46.4 & 17.8 & 34.9 & 42.9 \\

Softmax & 32.6 & 50.3 & 58.2 & 29.0 & 46.6 & 54.0 \\
\bf{Mean}    & \bf56.6 & \bf69.9 & \bf74.7 & \bf51.6 & \bf64.8 & \bf70.1 \\
\bottomrule
\end{tabular}
\caption{Study on different strategies of global similarity calculation in cross-lingual contrastive learning.}
\label{Tab:ablation_global_calcu}
\end{table}

\begin{table}[t]
 \small
 \centering
 \setlength{\tabcolsep}{4pt}
\begin{tabular}{c|ccc|ccc}
\toprule
\multirow{2}{*}{Stride} & \multicolumn{3}{c|}{T2V} & \multicolumn{3}{c}{V2T} \\
 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\
\midrule
\bf{1} & \bf56.6 & \bf69.9 & \bf74.7 & \bf51.6 & \bf64.8 & \bf70.1 \\
						
2   &44.8&60.5&68.1&39.7&55.5&63.0 \\
4   &24.3&42.3&49.8&14.4&30.2&37.4 \\
8 &23.6&	40.8&	49.1&	15.3	&31.5	&39.6\\
\bottomrule
\end{tabular}
\caption{Study on different sliding window strides used in sign encoder.}
\label{Tab:ablation_stride}
\end{table}

\begin{figure*}[t]
	\centering
	\scriptsize
 
		\begin{tabular}{ccc}
			\subfloat{\includegraphics[width=0.3\linewidth]{figures/UMAP/Noun.png}} & 
			\subfloat{\includegraphics[width=0.3\linewidth]{figures/UMAP/Verb.png}} & 
			\subfloat{\includegraphics[width=0.3\linewidth]{figures/UMAP/Adj.png}} 

			\\
			
			(a) Top-10 Nouns. & (b) Top-10 Verbs. & (c) Top-10 Adjective/Adverbs. 
		\end{tabular}
	\caption{Feature visualization of sign video clips. We map features extracted by our sign encoder to 2D space with UMAP~\cite{mcinnes2018umap}.}

 \label{fig:vis_UMAP}
\end{figure*}

\noindent\textbf{Fine-Tuning Hyper-Parameters.} \hspace{3pt} Recall that in the training of cross-lingual contrastive learning, our vision transformer and text transformer are initialized by the image encoder and text encoder in CLIP (ViT-B/32)~\cite{radford2021learning}. Here we study the fine-tuning hyper-parameters, \textit{i.e.}, learning rate in Figure~\ref{Fig:ablation_lr}  and batch size in Figure~\ref{Fig:ablation_bz}. A learning rate of 1e-5 yields best result. The increase of batch size sustainably promotes the performance. In our experiment, we set the batch size to 512 due to the limited GPU memory.

\noindent\textbf{Other Hyper-Parameters.}\hspace{3pt} There are four remaining hyper-parameters in CiCo: 1) $\alpha$ defined in Eq.(1) controls the weights of features extracted by domain-agnostic sign encoder and domain-aware sign encoder; 2) $\beta$ defined in Eq.(3) controls the weights of sign-video-to-text contrast and text-to-sign-video contrast; 3) the temperature $\sigma$ of row-wise and column-wise Softmax; 4) the maximum length of sign clip feature $L$. The studies are shown in Table~\ref{Tab:ablation_alpha}, Table~\ref{Tab:ablation_beta}, Table~\ref{Tab:ablation_sigma} and Table~\ref{Tab:ablation_L}, respectively. 



\begin{figure}[b]
     \centering
     \begin{subfigure}[t]{0.7\linewidth}
         \includegraphics[width=\textwidth]{figures/learning_strategy_supp/lr_study.pdf}
         \caption{Learning rate.}
             \label{Fig:ablation_lr}
      \end{subfigure}
        \hspace{0.1cm}
     \begin{subfigure}[t]{0.7\linewidth}
         \includegraphics[width=\textwidth]{figures/learning_strategy_supp/bz_study.pdf}
        \caption{Batch size.}
        \label{Fig:ablation_bz}

     \end{subfigure}
    \caption{Study on fine-tinning hyper-parameters in contrastive learning.}
    \label{Fig:ablation:learning_strategy}
\end{figure}

\renewcommand\arraystretch{1.0}

\begin{table}[t]
  \small

 \centering
 \setlength{\tabcolsep}{4pt}
\begin{tabular}{c|ccc|ccc}

\toprule
\multirow{2}{*}{$\alpha$} & \multicolumn{3}{c|}{T2V} & \multicolumn{3}{c}{V2T} \\
  & R@1  & R@5  & R@10 & R@1  & R@5  & R@10\\
 \midrule
0.2 & 53.0 & 67.5 & 72.5  & 47.6 & 62.7 & 67.2 \\
0.4 & 55.4 & 68.7 & 74.0 & 49.9 & 62.5 & 68.6  \\
0.6 & 55.1 & 68.5 & 73.4  & 49.6 & 63.9 & 68.9   \\
\bf{0.8} & \bf56.6 & \bf69.9 & \bf74.7 & \bf51.6 & \bf64.8 & \bf70.1 \\
\bottomrule
\end{tabular}
\caption{Study of $\alpha$ defined in Eq.(1).}
\label{Tab:ablation_alpha}
\end{table}


\begin{table}[t]
  \small

 \centering
 \setlength{\tabcolsep}{4pt}
\begin{tabular}{c|ccc|ccc}
\toprule
\multirow{2}{*}{$\beta$} &\multicolumn{3}{c|}{T2V} & \multicolumn{3}{c}{V2T} \\
  &R@1  & R@5  & R@10 & R@1  & R@5  & R@10\\
 \midrule
0.0 & 39.0 & 56.3 & 63.1 & 26.6 & 49.9 & 57.8 \\
0.2 & 44.8 & 62.1 & 68.1 & 39.8 & 55.5 & 62.5 \\
0.4 & 45.8 & 62.4 & 68.7 & 40.6 & 57.7 & 64.1 \\
\textbf{0.5}  &\bf56.6 & \bf69.9 & \bf74.7  & \bf51.6 & \bf64.8 & \bf70.1 \\
0.6 & 54.9 & 69.6 & 74.5 & 49.6 & 63.5 & 68.6 \\
0.8 & 54.1 & 68.7 & 73.3 & 48.3 & 62.1 & 67.8 \\
1.0 &  52.5 & 67.1 & 72.1 &    48.8 & 62.8 & 67.4  \\
 
\bottomrule
\end{tabular}
\caption{Study of $\beta$ defined in Eq.(3).}
\label{Tab:ablation_beta}
\end{table}


\begin{table}[t]
  \small

 \centering
 \setlength{\tabcolsep}{4pt}
\begin{tabular}{c|ccc|ccc}
\toprule
\multirow{2}{*}{$\sigma$} & \multicolumn{3}{c|}{T2V} & \multicolumn{3}{c}{V2T} \\
  & R@1  & R@5  & R@10 & R@1  & R@5  & R@10\\
 \midrule
7e-04 & 41.3 & 58.9 & 65.5  & 38.2 & 54.4 & 61.3  \\
7e-03 & 42.6 & 59.6 & 65.5  & 39.5 & 54.7 & 61.9  \\
\bf{7e-02} & \bf56.6 & \bf69.9 & \bf74.7 & \bf51.6 & \bf64.8 & \bf70.1 \\
7e-01 & 31.9 & 49.9 & 57.8 & 28.6 & 45.8 & 53.9 \\
%7 & 32.2 & 51.5 & 58.9  & 28.3 & 45.7 & 53.8  \\
\bottomrule
\end{tabular}
\caption{Study of the temperature $\sigma$ used in row-wise and column-wise Softmax.}
\label{Tab:ablation_sigma}
\end{table}


\begin{table}[t]
  \small

 \centering
 \setlength{\tabcolsep}{4pt}
\begin{tabular}{c|ccc|ccc}

\toprule
\multirow{2}{*}{$L$} & \multicolumn{3}{c|}{T2V} & \multicolumn{3}{c}{V2T} \\
  & R@1  & R@5  & R@10 & R@1  & R@5  & R@10\\
 \midrule
4 & 17.3 & 31.2 & 38.6 & 14.3 & 26.8 & 34.1  \\
8 & 38.2 & 55.4 & 62.3 & 34.1 & 50.2 & 56.4  \\
16 & 50.9 & 66.6 & 72.0 & 45.9 & 60.3 & 66.7  \\
32 & 53.6 & 67.3 & 73.5 & 48.9 & 61.7 & 67.8  \\
\bf{64} & \bf{56.6} & \bf{69.9} & \bf{74.7} & \bf{51.6} & \bf{64.8} & \bf{70.1}  \\ 
\bottomrule
\end{tabular}
\caption{Study of maximum length of sign clip feature $L$.}
\label{Tab:ablation_L}
\end{table}





\section{Qualitative Results}
{\noindent \textbf{Visualization of the Identified Sign-to-Word Mappings.}}\hspace{3pt}
Recall that in cross-lingual contrastive learning, we implicitly identify the sign-to-word mappings by calculating the fine-grained cross-lingual similarities (see Figure~3b of the main paper). Once the model is well optimized, we could infer the input texts and sign videos to produce a cross-lingual similarity matrix, which approximately reflects the sign-to-word mappings. For each word, we could identify its corresponding sign which has the maximal activation value. After that, the sign-to-word mapping is established. In Figure~\ref{fig:vis_UMAP}, we utilize UMAP~\cite{mcinnes2018umap} to visualize the features of the identified sign video clips for top-10 nouns, verbs and adjectives/adverbs within the How2Sign~\cite{Duarte_CVPR2021} vocabulary. The features of sign video clips associated with the same word form a compact cluster, demonstrating that our approach could identify the sign-to-word mappings during training.



{\noindent \textbf{More Examples of Sign-to-Word Mappings.}}\hspace{3pt} We visualize a collection of signs associated with the words \{``Big'', ``Different'', ``Hard'', ``Understand'', ``Vegetable'', ``Vehicle'', ``Water'', ``Baby''\} in Figure~\ref{fig:more_mappings}. The mappings are automatically identified by our CiCo.





\begin{figure*}[b]
     \centering
     \begin{subfigure}[b]{\linewidth}
         \centering
         \includegraphics[width=\textwidth]{figures/Sign2word_mappings_supp/s2w_01.pdf}
         \caption{``Big''.}
      \end{subfigure}
     \hfill
     
     \centering
     \begin{subfigure}[b]{\linewidth}
         \centering
         \includegraphics[width=\textwidth]{figures/Sign2word_mappings_supp/s2w_02.pdf}
         \caption{``Different''.}

     \end{subfigure}
     \hfill

     \centering
     \begin{subfigure}[b]{\linewidth}
         \centering
         \includegraphics[width=\textwidth]{figures/Sign2word_mappings_supp/s2w_03.pdf}
         \caption{``Hard''.}

     \end{subfigure}
     \hfill
     
     \centering
     \begin{subfigure}[b]{\linewidth}
         \centering
         \includegraphics[width=\textwidth]{figures/Sign2word_mappings_supp/s2w_04.pdf}
         \caption{``Understand''.}

     \end{subfigure}
     \hfill

     \centering
     \begin{subfigure}[b]{\linewidth}
         \centering
         \includegraphics[width=\textwidth]{figures/Sign2word_mappings_supp/s2w_05.pdf}
         \caption{``Vegetable''.}

     \end{subfigure}
     \hfill
     
     \centering
     \begin{subfigure}[b]{\linewidth}
         \centering
         \includegraphics[width=\textwidth]{figures/Sign2word_mappings_supp/s2w_06.pdf}
         \caption{``Vehicle''.}

     \end{subfigure}
     \hfill

     \centering
     \begin{subfigure}[b]{\linewidth}
         \centering
         \includegraphics[width=\textwidth]{figures/Sign2word_mappings_supp/s2w_07.pdf}
         \caption{``Water''.}

     \end{subfigure}
     \hfill
     
     \centering
     \begin{subfigure}[b]{\linewidth}
         \centering
         \includegraphics[width=\textwidth]{figures/Sign2word_mappings_supp/s2w_08.pdf}
         \caption{``Baby''.}

     \end{subfigure}
     \hfill

     
     \caption{More examples of cross-lingual (sign-to-word) mappings identified by our approach on How2Sign~\cite{Duarte_CVPR2021} dataset.}
     \label{fig:more_mappings}
    
\end{figure*}