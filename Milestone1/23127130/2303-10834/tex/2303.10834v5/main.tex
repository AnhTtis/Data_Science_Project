\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
\usepackage[preprint]{neurips_2023_arxiv}
% \usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    % \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
    % \usepackage[final]{neurips_2022}

% \nolinenumbers

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{subcaption}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage[textsize=tiny]{todonotes}

% Jindong
\usepackage{multicol}
\usepackage{defs}
\usepackage{siunitx}

% Packages added by Gautam.
\usepackage{lipsum}



\title{Object-Centric Slot Diffusion}

\author{
   Jindong Jiang\\
   Rutgers University \\
   \texttt{jindong.jiang@rutgers.edu}\\
   \And
  Fei Deng\\
   Rutgers University \\
   \texttt{fei.deng@rutgers.edu}\\
   \And
  Gautam Singh\\
   Rutgers University \\
   \texttt{singh.gautam@rutgers.edu}\\
   \And
   Sungjin Ahn\thanks{Correspondence to \texttt{sungjin.ahn@kaist.ac.kr}.}\\
   KAIST \\
   \texttt{sungjin.ahn@kaist.ac.kr}
}


\begin{document}


\maketitle


\begin{abstract}
The recent success of transformer-based image generative models in object-centric learning highlights the importance of powerful image generators for handling complex scenes. However, despite the high expressiveness of diffusion models in image generation, their integration into object-centric learning remains largely unexplored in this domain. In this paper, we explore the feasibility and potential of integrating diffusion models into object-centric learning and investigate the pros and cons of this approach. We introduce Latent Slot Diffusion (LSD), a novel model that serves dual purposes: it is the first object-centric learning model to replace conventional slot decoders with a latent diffusion model conditioned on object slots, and it is also the first unsupervised compositional conditional diffusion model that operates without the need for supervised annotations like text. Through experiments on various object-centric tasks, including the first application of the FFHQ dataset in this field, we demonstrate that LSD significantly outperforms state-of-the-art transformer-based decoders, particularly in more complex scenes, and exhibits superior unsupervised compositional generation quality. 
In addition, we conduct a preliminary investigation into the integration of pre-trained diffusion models in LSD and demonstrate its effectiveness in real-world image segmentation and generation.
Project page is available at \url{https://latentslotdiffusion.github.io}
\end{abstract}

\section{Introduction}

The fundamental structure of the physical world is compositional and modular. While this structure is naturally revealed in some data modalities like language in the form of tokens or words, in other modalities such as images, it is elusive how one may discover this structure.
However, this representational compositionality and modularity is essential for various applications that require the systematic manipulation of knowledge pieces to achieve high-level cognitive abilities. This includes reasoning \cite{lake2017building,bottou2014machine}, causal inference \cite{scholkopf2021toward}, and out-of-distribution generalization \cite{bahdanau2018systematic, fodor88}. 

Object-centric learning \cite{binding} aims to discover the latent compositional structure from unstructured observation by learning to bind relevant features, thereby forming useful tokens in an unsupervised way. For images, one of the most popular approaches is to auto-encode the image via the Slot Attention~\cite{slotattention} encoder. Slot Attention applies competitive spatial attention
to partition the image into separate local areas and then obtain a representation, called a slot, from each area.
Then, a decoder generates the image from the slots with the aim of minimizing the reconstruction error. 
Due to the limited capacity and competition between slots, each slot is encouraged to capture a reusable and compositional entity, such as an object.

The primary challenge that remains in the framework of unsupervised object-centric learning is making it work for complex naturalistic scene images. 
Until recently, most object-centric models have adopted a special type of decoder known as the \textit{mixture decoder}. While a weak slot-wise decoder \cite{sbd} is predominantly employed in this mixture decoder due to its efficacy in promoting the emergence of object-centric representation in relatively simple scene images, further studies~\cite{slate,steve} have shown that this strong prior can make handling complex naturalistic scene images more challenging. Contrary to conventional belief, Singh \emph{et al.}~\cite{slate} recently proposed departing from this low-capacity mixture-decoder approach and suggested to use an expressive transformer-based autoregressive image generator in object-centric learning~\cite{slate,steve}. It was shown that increasing the decoder capacity is crucial for handling complex and naturalistic scenes in this framework~\citep{chang2022object, Chang2022HierarchicalAF, dinosaur, slotformer, sysbinder}.

The success of transformer-based image generative modeling in object-centric learning naturally leads to the question: \textit{can diffusion models, another pillar of modern deep generative modeling known for their highly expressive generation capabilities, also benefit object-centric learning}? Diffusion models \cite{sohl2015deep,ddpm} are based on a stochastic denoising process and have demonstrated impressive performance in a variety of  image generation tasks \cite{dalle2,glide,imagen,adm,ldm,srdm,sdedit},  sometimes surpassing transformer-based autoregressive models. Moreover, diffusion models possess unique modeling capabilities that transformer-based autoregressive models cannot provide \cite{sohl2015deep}. However, despite their potential, the applicability of diffusion models to unsupervised object-centric learning remains largely unexplored. Consequently, it is crucial to examine the feasibility of this approach and to identify the associated benefits and limitations.

In this paper, we address this question by introducing a novel model called Latent Slot Diffusion (LSD). The LSD model can be interpreted from two perspectives. From the perspective of object-centric learning, LSD can be viewed as the first model substituting conventional slot decoders with a conditional latent diffusion model, in which the conditioning is object-centric slots provided by Slot Attention. From the diffusion model perspective, our approach is the first \textit{unsupervised} compositional conditional diffusion model. While traditional conditional diffusion models~\cite{dalle2,ldm,imagen,liu2022compositional} require supervised annotations, such as a text description of an image for compositional generation, LSD is a diffusion model that enables the construction of such compositional descriptions in terms of visual concepts extracted from images through unsupervised object-centric learning.

In our experiments, we evaluate the proposed model across various object-centric tasks, including unsupervised object segmentation, downstream property prediction, compositional generation, and image editing. We show that the LSD model delivers significantly superior performance compared to the state-of-the-art model, namely, the transformer-based autoregressive generative model. A notable attribute of the proposed model is that LSD's performance advantage over autoregressive transformers increases as the scene complexity increases. In particular, LSD enables exploring, for the first time, the applicability of object-centric model to the FFHQ dataset \cite{stylegan}, a collection of high-resolution and high-quality face images that surpasses the generative capabilities of existing object-centric models. Additionally, we discuss the overfitting issue faced by LSD on very simple scene images, such as those in the CLEVR dataset \citep{clevr}, and offer suggestions for addressing this problem.

\section{Latent Slot Diffusion}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\textwidth]{fig/method.png}
    \caption{\textbf{Method.} \textit{Left:} In training, we encode the given image as image latent and as slots. We then add noise to the image latent and we train a denoising network to predict the noise given the noisy latent and the slots. \textit{Right:} Given the trained model, we can generate novel images by composing a slot-based concept prompt and decoding it using the trained latent slot diffusion decoder.}
    \vskip -5mm
    \label{fig:method}
\end{figure}

In this section, we outline the auto-encoding framework of our proposed model, Latent Slot Diffusion or LSD \footnote{Code will be made available at \url{https://github.com/JindongJiang/latent-slot-diffusion}}, beginning with our object-centric encoder and subsequently describing our proposed decoder.

\subsection{Object-Centric Encoder} \label{sec:object_centric_encoder}
Given an input image $\bx \in \eR^{H \times W \times C}$, our object-centric encoder seeks to decompose and represent it as a collection of $N$ vectors or slots $\bS \in \eR^{N \times D}$, where each slot (denoted as $\bs_n \in \eR^{D}$) is encouraged to represent a compositional entity in the image. For this, we adopt Slot Attention, an architecture that is also used in the current state-of-the-art object-centric learning approaches \citep{slotattention, slate, savi}. We now describe how Slot Attention works in our model.

In Slot Attention, we first encode the input image $\bx$ as a set of $M$ input features $\bE \in \eR^{M \times D_\text{input}}$ via a backbone network $\smash{f_\phi^\text{backbone}}$, \emph{i.e.}, $\smash{\bE = f_\phi^\text{backbone}(\bx)}$. The network $\smash{f_\phi^\text{backbone}}$ is implemented as a CNN (detailed in Appendix \ref{sec:appx_implementation}) whose final output feature map is flattened to form a set. Next, the features in $\bE$ are grouped into $N$ spatial groupings and the information in each grouping is aggregated to produce a \textit{slot}. 
The grouping is achieved via an iterative slot refinement procedure. At the start of the refinement procedure, the slots $\bS$ are filled with random Gaussian noise. Then, they are refined via \textit{competitive attention} over the input features, where the $N$ slots act as the queries and the $M$ input features act as the keys and values.  The  queries and  keys undergo a dot-product to produce $N\times M$ attention proportions. Next, on these attention proportions, the \texttt{softmax} function is applied along the axis $N$ to produce attention weights $\bA$ that capture the soft assignment of each input feature to a slot. Then, for each $n$, all input features are sum-pooled weighted by their attention weights $\bA_{n,1}, \ldots, \bA_{n,M}$ to produce an attention readout $\bu_n \in \eR^{D}$. These steps can be formally described as follows:
\begin{align*}
    \bA = \underset{N}{\texttt{softmax}}\left(\frac{q(\bS) \cdot k(\bE)^T}{\sqrt{D}}\right) 
    && \Longrightarrow && 
    \bA_{n,m} = \dfrac{\bA_{n,m}}{\sum_{m=1}^M \bA_{n,m}}
    && \Longrightarrow && 
    \bu_n = \sum_{m=1}^M  v(\bE_m) \bA_{n,m},
\end{align*}
where, $q, k, v$ are linear projections that map the slots and input features to a common dimension $D$. Using the bottom-up information captured by the readout $\bu_n$, the slots are updated by an RNN as $\smash{\bs_n = f_\phi^\text{RNN}(\bs_n, \bu_n)}$. In practice, competitive attention and RNN update are performed iteratively several times and slots from the last iteration are considered the final slot representation $\bS$.


\subsection{Latent Slot Diffusion Decoder}

In this section, we describe our proposed decoding approach called \textit{Latent Slot Diffusion Decoder} or \textit{LSD decoder} for reconstructing the image given the slot representation $\bS$. The design of the LSD decoder takes advantage of the recent advances in generative modeling based on diffusion \cite{ldm, ddpm}. An overview of our decoding approach is provided in Figure~\ref{fig:method}.

\subsubsection{Pre-Trained Image Auto-Encoder}

One of the key design components of the LSD decoder is a pre-trained auto-encoder (AE) \cite{vqgan,ldm}, which provides a way to map an image $\bx$ to a lower-dimensional \textit{latent representation} $\bz_0$ via an encoder $\smash{f_\phi^\text{AE}}$. This enables LSD to reduce the computational burden of reconstructing high-resolution images by using the lower-dimensional latent $\bz_0$ as an intermediate reconstruction target. It also allows us to later obtain the original resolution image without compromising image content and fidelity by decoding the latent $\bz_0$ using the AE decoder $\smash{g_\ta^\text{AE}}$. These can be summarized as:
\begin{align*}
    \bz_0 = f_\phi^\text{AE}(\bx), && \hat{\bx} = g_\ta^\text{AE}(\bz_0), && \text{where } \bz_0 \in \eR^{H_\text{AE} \times W_\text{AE} \times D_\text{AE}}, \text{ and } \hat{\bx} \in \eR^{H \times W \times C} .
\end{align*}
In our model, we employ an auto-encoder pre-trained on OpenImages \footnote{We use the pre-trained weights from https://ommer-lab.com/files/latent-diffusion/kl-f8.zip}.

\subsubsection{Slot-Conditioned Diffusion}

In LSD, we leverage diffusion modeling to reconstruct the image latent $\bz_0$ conditioned on the slots $\bS$. This modeling approach has been primarily explored in supervised contexts, \emph{e.g.}, for text-to-image generation in Latent Diffusion Models (LDM) \citep{ldm}. However, unlike LDM, in this work, instead of conditioning the decoder on embeddings of supervised labels, we condition it on slots where the process of obtaining the slots themselves, \emph{i.e.}, Slot Attention, is jointly trained with the decoder without supervision. 
Following LDM, our decoder works by training a decoding distribution $p_\ta(\bz_0|\bS)$ to maximize the log-likelihood $\log p_\ta(\bz_0|\bS)$ of the image latent $\bz_0$ given the slots $\bS$. This decoding distribution $p_\ta(\bz_0|\bS)$ is modeled as a $T$-step denoising process:
\begin{align*}
    p_\ta(\bz_0|\bS) = \int p(\bz_T) \prod_{t=T, \ldots, 1} p_\ta(\bz_{t-1} | \bz_{t}, t, \bS) \,\mathrm{d} \bz_{1:T},
\end{align*}
where $p(\bz_T) = \cN(\mathbf{0}, \mathbf{I})$, $p_\ta(\bz_{t-1} | \bz_{t},  t, \bS)$ is a one-step denoising distribution, and $\bz_T, \bz_{T-1}, \ldots, \bz_0$ is a sequence of progressively denoised latents.
The one-step denoising distribution $p_\ta(\bz_{t-1} | \bz_{t}, t, \bS)$ is parametrized via a neural network $g_\ta^\text{LSD}$ in the following manner:
\begin{align*}
    p_\ta(\bz_{t-1} | \bz_{t}, t, \bS) = \cN\left(\frac{1}{\sqrt{\alpha_t}}\left(\bz_t -\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\hat{\boldsymbol{\epsilon}}_t \right), \beta_t\mathbf{I}  \right), && \text{where } \hat{\boldsymbol{\epsilon}}_t = g^\text{LSD}_\ta(\bz_t, t, \bS),
\end{align*}
$\beta_1, \ldots, \beta_T$ is a linearly increasing variance schedule, $\alpha_t = 1 - \beta_t$, and $\bar{\alpha}_t = \prod_{i=1}^t (1 - \beta_i)$. 

\textbf{Sampling Procedure.} To sample a $\bz_0 \sim p_\ta(\bz_0|\bS)$, we adopt an iterative denoising procedure as in \citep{ldm, ddpm}. The sampling process starts with a latent representation $\bz_T\sim \cN(\mathbf{0}, \mathbf{I})$ filled with random Gaussian noise. Next, conditioned on the slots, we denoise it $T$ times by sampling sequentially from the one-step denoising distribution $\bz_{t-1} \sim p_\ta(\bz_{t-1} | \bz_t, t, \bS)$ for $t=T, \ldots, 1$.
This produces a sequence of latents $\bz_T, \bz_{T-1}, \ldots, \bz_0$ that become progressively cleaner. Finally, $\bz_0$ can be considered as the reconstructed latent representation.

\textbf{Training Procedure.} Following LDM \cite{ldm}, the training of $p_\ta(\bz_0|\bS)$ can be cast to a simple procedure for training $g^\text{LSD}_\ta$ as follows. Given an image $\bx$, its slot representation $\bS$, and its image latent $\bz_0$, we first randomly choose a noise level $t \in \{1, \ldots, T\}$ from a uniform distribution. Given the $t$, we corrupt the clean latent $\bz_0$ and obtain a noised latent $\bz_t$ as follows:
\begin{align*}
    \bz_t = \sqrt{\bar{\alpha}_t} \bz_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}_t, && \text{ where } \boldsymbol{\epsilon}_t \sim \cN(\mathbf{0}, \mathbf{I}), \quad \bar{\alpha}_t = \prod_{i=1}^t (1 - \beta_i).
\end{align*}
The noised latent $\bz_t$ is then given as input to $g^\text{LSD}_\ta$ along with the slots $\bS$ and denoising time-step $t$ to predict the noise $\boldsymbol{\epsilon}_t$.
The network $g^\text{LSD}_\ta$ is trained by minimizing the mean squared error between the predicted noise $\hat{\boldsymbol{\epsilon}}_t$ and the true noise $\boldsymbol{\epsilon}_t$:
\begin{align*}
    \hat{\boldsymbol{\epsilon}}_t = g^\text{LSD}_\ta(\bz_t, t, \bS) && \Longrightarrow && \cL(\phi, \ta) = %\mathbb{E}_{t, \bz, \boldsymbol{\epsilon}}  
    || \hat{\boldsymbol{\epsilon}}_t - \boldsymbol{\epsilon}_t ||^2.
\end{align*}



\subsubsection{Denoising Network}
\label{sec:denoising_net}
We implement the denoising network $g^\text{LSD}_\ta$ as a variant of the conventional UNet architecture adapted to incorporate slot-conditioning. Our denoising network consists of a stack of $L$ layers where each layer $l$ is a UNet-style CNN layer followed by a slot-conditioned transformer:
\begin{align*}
   \tilde{\bh}_{l} = \texttt{CNN}_{\ta}^l([\bh_{l-1}, \bh_{\text{skip}(l)}], t), && \Longrightarrow && \bh_{l} = \texttt{Transformer}_{\ta}^l(\tilde{\bh}_{l} + \mathbf{p}_l, \texttt{cond=}\bS),
\end{align*}
where $\bh_0=\bz_t$ is the input, $\bh_1, \ldots, \bh_{L-1}$ are the hidden states and $\hat{\boldsymbol{\epsilon}}_t = \bh_L$ is the output. 

\textbf{CNN Layers.} Following UNet \cite{unet}, the convolutional layers $\smash{\texttt{CNN}_{\ta}^1, \ldots, \texttt{CNN}_{\ta}^L}$ downsample the feature map via the first $\smash{\frac{L}{2}}$ layers and then upsample it back to the original resolution via the remaining layers. Following standard UNet, these CNN layers also receive inputs via skip connection from an earlier layer denoted by $\text{skip}(l)$. This network design has also been explored in LDM \citep{ldm}.

\textbf{Slot-Conditioned Transformer.} The role of the transformer is to incorporate the information from the slots into the UNet-based denoising process. For this, in each layer $l$, the intermediate feature map $\smash{\tilde\bh_l}$ produced by the CNN layer is flattened into a set of features. To this, positional embeddings $\bp_l$ are added and the resulting features are provided as input to the transformer. Within the transformer, these features interact with each other and with the slots $\bS$, thus incorporating the information from the slots into the denoising process. The transformer output is then reshaped back to a feature map $\bh_l$.


\section{Compositional Image Synthesis}
\label{sec:visual_concept_lib}

In this section, we describe how a trained LSD model can be used to compose and synthesize novel images. The conventional approach to composing novel images is commonly supervised and rely on text prompts. This involves using words from a vocabulary to create a sentence, which is then given to a text-to-image model to synthesize the desired image \citep{dalle, dalle2, imagen, ldm, glide}. However, in a fully unsupervised setting like ours, we first need to build a library of visual concepts by simply observing a large set of unlabelled images. Then, similarly to composing a sentence prompt using words, we compose a \textit{concept prompt} by selecting concepts from the visual concept library. By providing this concept prompt to the LSD decoder, we can then synthesize a desired novel image. This approach of unsupervised compositional image synthesis was also explored in \cite{slate}.

\textbf{Unsupervised Visual Concept Library.} To build a library of visual concepts from unlabelled images, we first take a large batch of $B$ images $\bx_1, \ldots, \bx_B$. We then apply slot attention to obtain slots for these images $\bS_1, \ldots, \bS_B$. Next, we gather all these slots into a single set $\cS$ and perform $K$-means on it. The $K$-means procedure assigns each slot in $\cS$ to one of the $K$ clusters. We consider the set of slots assigned to a $k$-th cluster as a visual concept library $\cV_k$. With $K$ as the number of clusters, this method results in $K$ visual concept libraries $\cV_1, \ldots, \cV_K$. Our experiments will demonstrate that this basic $K$-means technique can generate semantically meaningful concept libraries. For example, on a dataset of human face images like FFHQ \cite{stylegan}, the $K$ libraries correspond to practical concept categories such as hair style, face, clothing, and background.

\textbf{Novel Image Synthesis.} Given libraries $\cV_1, \ldots, \cV_K$, we can compose a concept prompt $\bS_\text{compose}$ by picking $K$ slots, each from the corresponding $k$-th library, and stacking them together:
\begin{align*}
    \bS_\text{compose} = (\bs_1, \ldots, \bs_K), && \text{where } \bs_k \sim \text{Uniform}(\cV_k)
\end{align*}
We then give the composed prompt $\bS_\text{compose}$ to the LSD decoder to generate the image latent: $\smash{\bz_\text{compose} \sim p_\ta(\bz_0 | \bS_\text{compose})}$. Applying the image decoder on $\smash{\bz_\text{compose}}$ generates the desired novel scene image $\smash{\bx_\text{compose} = g_\ta^\text{AE}(\bz_\text{compose})}$. For instance, in the FFHQ dataset, the concept prompt can be a collection of chosen hair style, face, clothing, and background. Decoding this prompt would generate a face image that conforms to this prompt.

\section{Related Work}
\textbf{Unsupervised Object-Centric Learning.} Object-centric representation learning aims to decompose multi-object scenes into meaningful object entities. A common approach to this is by auto-encoding. In this line, the focus has been to design an appropriate decoder that supports good decomposition. The most widely used decoders include the mixture-decoder \cite{monet, iodine, slotattention, nem, genesis, von2020towards, anciukevicius2020object, du2020unsupervised, engelcke2021genesis, simone, Zhang2022RobustAC, slotvae}, spatial transformer decoder \cite{air, spair, space, gnm, deng2020generative, roots, scalor, gswm, swb, yoon2023investigation}, Neural Radiance Fields (NeRF) \cite{stelzner2021decomposing,Wu2022ObPoseLC,Smith2022UnsupervisedDA}, transformer decoder \cite{slate, steve, sysbinder, slotformer, Chang2022HierarchicalAF, sajjadi2022osrt, Gopalakrishnan2022UnsupervisedLO}, energy-based models \cite{du2021unsupervised} and complex-valued functions \citep{Lowe2022ComplexValuedAF}. 
Among these, the mixture decoder has been shown to struggle on complex scene images \cite{slate, steve} while the spatial transformer decoder requires careful tuning of hyperparameters which limits their applicability in realistic scenes. Neural Radiance Fields require camera poses, and the learning setting involves 3D scenes unlike ours. 
Another line of work seeks object-centric scene decomposition without reconstruction. This includes works such as \cite{dinosaur, Wen2022SelfSupervisedVR, Henaff2022ObjectDA, cutler, sslslot, contrastslot, videosaur}. However, unlike ours, these approaches lack the ability to generate images. Preceding object-centric learning, several works pursued disentanglement within a single-vector representation of the scene and use it for compositional image generation \citep{infogan, higgins2017beta, kim2018disentangling, kumar2017variational, chen2018isolating}. However, lacking spatial binding mechanisms, these methods struggle in multi-object scenes \citep{iodine} unlike ours.



\textbf{Diffusion Models.} 
Diffusion models (DMs) are a recent class of generative models that can produce high-quality images by reversing a stochastic process that gradually adds noise to an image \citep{ddpm,scoredm}. DMs have been applied to various tasks in computer vision, such as class-conditioned generation, text-to-image generation, image editing, super-resolution, and inpainting \citep{adm,cfdm,dalle2,glide,imagen,sdedit,cascadedm,srdm}. Recently, \citep{ldm} proposed Latent Diffusion Models (LDM). By virtue of operating on a low-dimensional latent space, LDM reduces the computational demands of DMs significantly.
\citep{composdm} introduced a method for multi-object scene generation by combining signals from multiple text-conditioned denoising networks. However, to achieve controllable generation, many of these existing DMs require additional labels such as text descriptions to train and control the generation process. In contrast, our model can generate images compositionally using object concepts directly extracted from images. Moreover, DMs have also been used for representation learning. \citep{labeleffdm} demonstrated that the intermediate features of a learned DM are useful representations for label-efficient semantic segmentation. \citep{dmae} introduced an image encoder that compresses an image into a latent representation, jointly trained with the denoising objective. Nevertheless, these representations are unstructured and not modular like ours. 

\textbf{Concurrent Research.} A concurrent study in \citep{wu2023slotdiffusion} also explores a similar idea of object-centric learning using DMs and its downstream applications such as the image editing tasks akin to our experiments. Just like LSD, they employ the Latent Diffusion Model (LDM) as the slot decoder. However, unlike our method, \citep{wu2023slotdiffusion} independently train a latent encoder for each dataset, whereas LSD employs a pre-trained latent encoder shared across all input data. 

\section{Experiments} \label{sec:experiment}

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=1.\textwidth]{fig/segmentation.pdf}
\vskip -0.1in
\caption{\textbf{Visualization of Unsupervised Object Segmentation.} We show visualizations of predicted segments on CLEVR, CLEVRTex, MOVi-E, and FFHQ datasets. 
}
\vskip -2mm
\label{fig:seg}
\end{figure*}
% ########################################################################

We extensively evaluate our proposed Latent Slot Diffusion (LSD) model on various object-centric tasks, including unsupervised object segmentation, downstream property prediction, compositional generation, and image editing. As will be shown, our model demonstrates substantial improvements over the state-of-the-art on multiple challenging datasets with complex texture and background, including FFHQ~\cite{stylegan} which has been beyond the generative capability of object-centric models. In addition, we also include a preliminary exploration into the potential of LSD using pre-trained diffusion models, discussing its performance in both multi-object datasets and unconstrained real-world images in Section \ref{sec:pretrained_dm} and Appendix \ref{sec:appx_pretrained_dm}.

\textbf{Datasets.} We evaluate our model on five datasets. Four of them are synthetic multi-object datasets---CLEVR~\citep{clevr}, CLEVRTex~\citep{clevrtex}, MOVi-C, MOVi-E~\citep{kubric}. They present increasing levels of difficulty---CLEVRTex adds texture to objects and backgrounds, MOVi-C uses more complex objects and natural backgrounds, and MOVi-E contains large numbers of objects (up to 23) per scene. Furthermore, we explore the applicability of object-centric models to FFHQ~\citep{stylegan}, a dataset of high-quality face images. Unlike previous works in this line that only investigate low-resolution images, \emph{e.g.}, up to $128 \times 128$, we use a resolution of $256 \times 256$ for all datasets in our experiments.

\textbf{Baselines.} We compare our model against SLATE, the state-of-the-art object-centric learning and unsupervised compositional image generation approach. We use its improved version~\cite{steve}, which is more robust in complex scenes. For a fair comparison with LSD that leverages pre-trained auto-encoder, we also develop a variant of SLATE denoted as SLATE$^+$, where its low-capacity dVAE~\cite{slate} is replaced with a pre-trained VQGAN model \cite{vqgan}. For all models in this work, we use OpenImages-pretrained auto-encoders~\cite{ldm}.

\subsection{Object-Centric Representation Learning}

In line with previous work~\cite{slotattention}, we use unsupervised object segmentation and downstream property prediction to evaluate the object-centric learning capability and representation quality. Within each dataset, all models we compare use the same number of slots.

\begin{table*}[t]
\begin{small}
    \caption{\textbf{Segmentation Performance and Representation Quality.} \textit{Left:} We evaluate the segmentation quality and report mBO, mIoU, and FG-ARI scores across various datasets and baselines. \textit{Right:} We measure the representation quality by learning a probe to predict object properties given frozen slots. For position and 3D bounding box, we report MSE. For shape, material, and category, we report accuracy. The results are averaged over three random seeds, and the complete table, including standard deviations, can be found in Figure \ref{tab:complex_quant_with_std} in appendix.
    }
    \vspace{-0.5em}

    % CLEVRTex
    \begin{subtable}{1.0\linewidth}
    \caption{CLEVRTex}
    \vspace{-1mm}
    \centering
    \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Segmentation}}      & SLATE        & SLATE$^+$      & LSD (Ours)   \\
    \midrule
    mBO ($\uparrow$)         & 50.88 & 54.90 & \textbf{63.93} \\
    mIoU ($\uparrow$)        & 49.54 & 52.96 & \textbf{62.52} \\
    FG-ARI ($\uparrow$)      & 44.19 & \textbf{70.71} & 64.41 \\
    \bottomrule
    \end{tabular}
    \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Representation}}   & SLATE        & SLATE$^+$      & LSD (Ours)   \\
    \midrule
    Shape ($\uparrow$)       & 74.24 & 71.63 & \textbf{80.23} \\
    Material ($\uparrow$)   & 69.73 & 63.61 & \textbf{75.56} \\
    Position ($\downarrow$)  & 1.28 & 1.26 & \textbf{1.13}    \\
    \bottomrule
    \end{tabular}
    \vspace{1mm}
    \end{subtable}
    

    % MOVi-C
    \begin{subtable}{1.0\linewidth}
    \caption{MOVi-C}
    \vspace{-1mm}
    \centering
    \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Segmentation}}          & SLATE        & SLATE$^+$      & LSD (Ours)   \\
    \midrule
    mBO ($\uparrow$)         & 39.37 & 38.17  & \textbf{45.57} \\
    mIoU ($\uparrow$)        & 37.75 & 36.44  & \textbf{44.19} \\
    FG-ARI ($\uparrow$)      & 49.54 & \textbf{52.04}  & 51.98 \\
    \bottomrule
    \end{tabular}
    \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Representation}}           & SLATE        & SLATE$^+$      & LSD (Ours)   \\
    \midrule
    Position ($\downarrow$)  & 1.37 & 1.28   & \textbf{1.14}   \\
    3D B-Box ($\downarrow$)   & 1.48 & \textbf{1.44}  & \textbf{1.44}    \\
    Category ($\uparrow$)    & 42.45 & 45.32 & \textbf{46.11} \\
    \bottomrule
    \end{tabular}
    \vspace{1mm}    
    \end{subtable}
    

    % MOVi-E
    \begin{subtable}{1.0\linewidth}
        \caption{MOVi-E}
        \vspace{-1mm}
        \centering
        \begin{tabular}{lccc}
        \toprule
        \scriptsize{\textbf{Segmentation}}          & SLATE        & SLATE$^+$      & LSD (Ours)   \\
        \midrule
        mBO ($\uparrow$)         & 30.17 & 22.17  & \textbf{38.96} \\
        mIoU ($\uparrow$)        & 28.59 & 20.63  & \textbf{37.64} \\
        FG-ARI ($\uparrow$)      & 46.06 & 45.25  & \textbf{52.17} \\
        \bottomrule
        \end{tabular}
            \begin{tabular}{lccc}
        \toprule
        \scriptsize{\textbf{Representation}}           & SLATE        & SLATE$^+$      & LSD (Ours)   \\
        \midrule
        Position ($\downarrow$)  & 2.09 & 2.15   & \textbf{1.85}   \\
        3D B-Box ($\downarrow$)   & 3.36 & 3.37  & \textbf{2.94}    \\
        Category ($\uparrow$)    & 38.93 & 38.00 & \textbf{42.96} \\
        \bottomrule
        \end{tabular}
        \vspace{1mm}    
    \end{subtable}
    

    \label{tab:complex_quant} 
    \vspace{-2mm}
\end{small}

\end{table*}



\textbf{Unsupervised Object Segmentation.}
Following~\cite{slotattention, dinosaur}, to measure segmentation quality, we report the foreground adjusted rand index (FG-ARI), the mean intersection over union (mIoU), and the mean best overlap (mBO). These metrics are computed based on the attention masks generated by Slot Attention. Specifically, we utilize the attention weights described in Section \ref{sec:object_centric_encoder}, where attention weights $\bA$ are computed as $\bA = \underset{N}{\texttt{softmax}}\left(\frac{q(\bS) \cdot k(\bE)^T}{\sqrt{D}}\right)$.

Our results in Table \ref{tab:complex_quant} suggest that LSD is particularly strong in visually complex scenes. For example, on the most challenging MOVi-E dataset, LSD outperforms the strongest baseline by ${>} 8\%$ in mBO, ${>} 9\%$ in mIoU, and ${>} 6\%$ in FG-ARI. On CLEVRTex and MOVi-C featuring complex textures, LSD also achieves ${>} 9\%$ and ${>} 6\%$ gain respectively, in both mBO and mIoU. We visually show the superior segmentation quality of LSD in Figure \ref{fig:seg}. LSD obtains tighter object boundaries, less object splitting, and cleaner background segmentation. The advantages are most noticeable on CLEVRTex and MOVi-E, where the baselines over-segment the objects more frequently, or exhibit a common failure mode that divides images into approximately uniformly distributed block masks.

We also note that the FG-ARI score is sometimes not an ideal metric for evaluating segmentation quality, because it only considers the foreground pixels and disregards the correctness of the mask shape, as also highlighted by \cite{genesis,clevrtex}. In the MOVi-E experiment, even though SLATE$^+$ partitions the images arbitrarily into uniform patches, as shown in Figure \ref{fig:seg}, it still achieves a FG-ARI score comparable to SLATE, which produces significantly more reasonable object masks. This highlights the need for additional metrics, such as the mIoU score that we also measure for evaluating the segmentation quality in this work.


\textbf{Downstream Property Prediction.}
Following~\cite{dittadi2021generalization, slotattention}, we evaluate the quality of the learned object-centric representations through downstream property prediction. Specifically, for each property, we train a network to predict the property given a frozen slot representation as input. The correspondence between the slot and its true label is determined by Hungarian matching~\cite{hungarian} using masks. We use linear heads and 2-layer MLP as the prediction networks for discrete and continuous values, respectively. We report prediction accuracy for discrete properties (\emph{e.g.}, shape and material), and mean squared error for continuous properties (\emph{e.g.}, position).


As shown in Table \ref{tab:complex_quant}, LSD is competitive with or better than the strongest baseline on CLEVRTex, MOVi-C, and MOVi-E, which is consistent with our finding in unsupervised object segmentation that LSD shines in visually complex scenes. For example, LSD obtains ${>} 4\%$ gain in predicting object category on MOVi-E and object material on CLEVRTex. 

% ########################################################################
\begin{table}[t]
\caption{\textbf{Compositional Image Generation.} On various datasets, we generate images using object slots randomly sampled from the dataset. We compare the fidelity of the generated images via the FID score. The lower the FID score, the better the fidelity. We find that our model produces the best fidelity scores compared to the baselines. }
\begin{center}
\begin{small}
    \begin{tabular}{lccc}
    \toprule
    FID $\downarrow$     & SLATE        & SLATE$^+$       & LSD (Ours)       \\
    \midrule
    % CLEVR         & 52.96  & 20.93  & \textbf{16.22}  \\
    CLEVRTex      & 105.83 & 69.23 & \textbf{29.53}  \\
    MOVi-C        & 170.83 & 148.27 & \textbf{69.12}  \\
    MOVi-E        & 169.32 & 126.51 & \textbf{64.76}  \\
    FFHQ          & 112.38 & 98.76  & \textbf{27.83} \\
    \bottomrule
    \end{tabular}
    \label{tab:fid} 
    \vspace{-5mm}
\end{small}
\end{center}
\end{table}

\subsection{Compositional Generation with Visual Concept Library}

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=1.\textwidth]{fig/compsitional_generation.pdf}
\vskip -0.1in
\caption{\textbf{Compositional Generation Samples.} We qualitatively compare the quality of image samples generated by our model with the baselines. We observe that LSD provides significantly higher fidelity and more clear details compared to the other methods.}
\label{fig:compose}
\end{figure*}
% ########################################################################


Like text-to-image generative models, LSD is able to take unseen slot-based prompts at test time and compose new images. Unlike text-to-image models, however, LSD obtains the compositional generation ability solely from images without relying on additional supervision like text inputs.
As described in Section \ref{sec:visual_concept_lib}, we first build a concept library. Then, we sample one slot representation from each visual concept library and concatenate them into a sequence to form a slot-based prompt. We then feed the slot-based prompts to the LSD decoder to generate the images. This produces scenes with novel object layouts and faces with unseen attribute combinations.

We report in Table~\ref{tab:fid} the FID score~\cite{fid} as a measure of the compositional generation quality. Following standard practice~\cite{adm}, we compute the FID score using 2K generated images and the full training dataset. Across all datasets, LSD achieves significantly better FID scores than SLATE and SLATE$^+$. We further demonstrate the superior compositional generation quality of LSD in Figure~\ref{fig:compose}. We observe that on the more challenging MOVi-E and FFHQ datasets, LSD generates images with substantially more clear details and better coherence. In contrast, SLATE is limited by its decoder and produces images with missing details in the objects or human faces. While the details can be improved by the pre-trained auto-encoder in SLATE$^+$, some samples still exhibit severe distortions.


\subsection{Slot-Based Image Editing}

% ########################################################################
\begin{figure}[t]
\centering
\vskip -0.1in
\includegraphics[width=1.0\textwidth]{fig/object_based_editing_horizontal.pdf}
\caption{\textbf{Slot-Based Image Editing.} \textit{Left:} Our model demonstrates image editing capabilities, including object removal, extraction, insertion, background extraction, and swapping. \textit{Right:} In the FFHQ dataset, we showcase face replacement by combining face slots from Source-B images with hairstyle, clothing, and background slots from Source-A images.}
\vskip -3mm
\label{fig:edit_multi_obj}
\end{figure}
% ########################################################################


In addition to generating new images from randomly sampled slot-based prompts, LSD also allows editing existing images by directly modifying their slot representations. Our experiments showcase LSD's capability in slot-based image editing, including object removal, single-object segmentation, object insertion, background extraction, and background swapping. Remarkably, we demonstrate, for the first time in object-centric generative models, the ability to perform face editing in real-world images, as illustrated in Figure~\ref{fig:edit_multi_obj}.


We perform slot manipulation on the CLEVRTex dataset, including object removal, single-object extraction, object insertion, background extraction, and background swapping. To remove an object or extract the background, we discard the relevant slot or all object slots, respectively. Our findings indicate that with near-perfect object decomposition, removing an object can be achieved by simply discarding its slot. Additionally, the background image can be rendered from a single background slot, even without single-slot conditioning during training. In single-object extraction, we render an object with the same background using the respective object and background slots. For object insertion and background swapping, we split the image into top and bottom pairs (Figure~\ref{fig:edit_multi_obj} \textit{left}) and introduce a slot from another image or swap background slots. This demonstrates that an image's background can be entirely altered while preserving its original objects.

We further explore face replacement on the FFHQ dataset. LSD decomposes each image into four slots, corresponding to face, hairstyle, clothing, and background. Our results show that by replacing the face slots of the images, we are able to coherently change the image while maintaining the hairstyle, clothing, and background. The resulting images look realistic, suggesting that LSD can effectively blend various attributes even when given novel combinations.

\subsection{Pre-Trained Diffusion Models and Real-World Object-Centric Learning.} \label{sec:pretrained_dm}

\begin{figure}[b]
    \centering
    \includegraphics[width=0.95\textwidth]{fig/lssd_coco_samples.jpg}
    \caption{\textbf{Unsupervised Object-Centric Learning and Conditional Generation on Real-World Images with Pre-Trained Diffusion Models.} Each image sample is conditioned on the same set of learned slots and with a different initial noise map in the denoising process. We use $\text{cfg}=1.3$ for all samples. Additional details are available in Appendix \ref{sec:appx_pretrained_dm}.}
    \label{fig:lssd_coco_samples}
\end{figure}

In recent studies of DMs, much attention has been given to controllable image generation using large-scale pre-trained diffusion models such as Stable Diffusion \cite{gligen,t2iadapter,controlnet}. One intriguing question is whether such pre-trained diffusion models can facilitate the learning of object-centric representation. In this regard, we present a preliminary investigation in Appendix \ref{sec:appx_pretrained_dm}, where we test two LSD variants utilizing the pre-trained Stable Diffusion. Our findings indicate that (1) although the pre-trained Stable Diffusion model has not been trained on any multi-object datasets, it still provides a learning signal for reasonable object-centric learning, (2) with the strong generative capabilities of pre-trained DMs, LSD can scale up object-centric learning to real-world images; however, we also note the part-whole ambiguity issue in slot-based object learning in real-world scenarios, and (3) remarkably, the propose LSD variant achieves conditional generation with unconstrained real-world objects by applying classifier-free guidance \citep{cfdm} to the learned slot, which also provide a visualization of the semantic information captured in the learned representations. We show some image samples in Figure \ref{fig:lssd_coco_samples}, please refer to Appendix \ref{sec:appx_pretrained_dm} for additional details.
We believe these preliminary attempts to be valuable explorations toward large-scale object-centric learning and hope our finding will inspire future research in this area.

\section{Discussion}

\textbf{LSD on Simple Images.} While LSD shows significant gains in complex naturalistic scenes, our preliminary investigation suggests a somewhat diminished performance in terms of segmentation and representation when the dataset consists of only visually simple and monotonous scene images like CLEVR. We conjecture that LSD may suffer from an overfitting problem when its high expressiveness encounters visually simple images. To address this issue, additional complexities can be introduced during training, such as integrating images from other datasets. In Appendix \ref{sec:appx_clevr}, we offer a more detailed analysis of this challenge using the CLEVR dataset and introduce a training scheme that includes data samples from CLEVRTex to alleviate the problem. Our findings indicate that by integrating greater complexities into the training set, LSD can be effectively employed on simple datasets as well. 


\textbf{Computation Requirement.} 
We compare the computation requirement of LSD and the baseline models in the CLEVRTex setting. LSD requires approximately 32 GB of training memory, compared to 31 GB and 36 GB for SLATE and SLATE$^+$, respectively. Notably, LSD does not require much higher memory consumption than regular unconditional latent diffusion, which takes 28 GB under the same setting. We train LSD on 2 NVIDIA RTX 6000 GPUs for 4.5 days, while SLATE and SLATE$^+$ are trained in 1 day and 2.7 days using the same GPU setup. For test-time generation, LSD takes 50.7 seconds to generate 50 images, while SLATE and SLATE$^+$ takes 86.1 and 87.6 seconds, respectively.

\section{Conclusion}
The integration of diffusion models into unsupervised object-centric learning has been largely unexplored, but holds significant potential. To assess the feasibility of this approach and to identify its strengths and weaknesses, we introduced the Latent Slot Diffusion model in this study, which serves two purposes: (1) the first model integrating diffusion models into unsupervised object-centric learning, and also (2) the first unsupervised compositional diffusion model which does not require supervised annotations like text. Our key findings indicate that the proposed model surpasses state-of-the-art transformer-based object-centric models in multiple tasks, with its performance advantage increasing as image complexity grows. Furthermore, the unsupervised compositional generation quality of our model exceeds that of transformer-based counterparts.
Finally, we also introduce LSD variants that utilize pre-train Diffusion Models and demonstrate their ability in real-world object-centric learning and conditional generation with real-world objects.
We believe that this work represents a significant advancement in object-centric learning towards dealing with complex naturalistic images.



\section*{Acknowledgements}

This work is supported by Young Researcher Program (No. 2022R1C1C1009443) through the National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT. The GPU computing used for this work is partly supported by KI Cloud of Division of National SupercomputingÂ  Center, Korea Institute of Science and Technology Information (KISTI). We would like to thank Junmo Cho for managing of specific experiments, Yi-Fu Wu for his contributions to the initial draft of this paper, and Ligong Han and Yuxiao Chen for their valuable feedback on the manuscript.

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% probably should) include acknowledgements. In this case, please
% place such acknowledgements in an unnumbered section at the
% end of the paper. Typically, this will include thanks to reviewers
% who gave useful comments, to colleagues who contributed to the ideas,
% and to funding agencies and corporate sponsors that provided financial
% support.

\bibliography{refs, refs_gs, refs_ahn, refs_jd}
\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\input{appx}

\end{document}