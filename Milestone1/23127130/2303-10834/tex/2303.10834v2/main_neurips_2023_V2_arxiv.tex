\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
\usepackage[final]{neurips_2023_arxiv}
% \usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    % \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
    % \usepackage[final]{neurips_2022}

% \nolinenumbers

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{subcaption}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage[textsize=tiny]{todonotes}

% Jindong
\usepackage{multicol}
\usepackage{defs}
% \usepackage{adjustbox}
\usepackage{siunitx}
% \usepackage{lineno}

% Packages added by Gautam.
\usepackage{lipsum}



\title{Object-Centric Slot Diffusion}

\author{%
   Jindong Jiang\thanks{Correspondence to \texttt{jindong.jiang@rutgers.edu} and \texttt{sungjin.ahn@kaist.ac.kr}.} \\
   Rutgers University \\
   \texttt{jindong.jiang@rutgers.edu}\\
   \And
  Fei Deng\\
   Rutgers University \\
   \texttt{fei.deng@rutgers.edu}\\
   \And
  Gautam Singh\\
   Rutgers University \\
   \texttt{singh.gautam@rutgers.edu}\\
   \And
   Sungjin Ahn \\
   KAIST \\
   \texttt{sungjin.ahn@kaist.ac.kr}
}


\begin{document}


\maketitle


\begin{abstract}
The recent success of transformer-based image generative models in object-centric learning highlights the importance of powerful image generators for handling complex scenes. However, despite the high expressiveness of diffusion models in image generation, their integration into object-centric learning remains largely unexplored in this domain. In this paper, we explore the feasibility and potential of integrating diffusion models into object-centric learning and investigate the pros and cons of this approach. We introduce Latent Slot Diffusion (LSD), a novel model that serves dual purposes: it is the first object-centric learning model to replace conventional slot decoders with a latent diffusion model conditioned on object slots, and it is also the first unsupervised compositional conditional diffusion model that operates without the need for supervised annotations like text. Through experiments on various object-centric tasks, including the first application of the FFHQ dataset in this field, we demonstrate that LSD significantly outperforms state-of-the-art transformer-based decoders, particularly in more complex scenes, and exhibits superior unsupervised compositional generation quality. Project page is available at \url{https://latentslotdiffusion.github.io}
\end{abstract}

\section{Introduction}

The fundamental structure of the physical world is compositional and modular. While this structure is naturally revealed in some data modalities like language in the form of tokens or words, in other modalities such as images, it is elusive how one may discover this structure.
However, this representational compositionality and modularity is essential for various applications that require the systematic manipulation of knowledge pieces to achieve high-level cognitive abilities. This includes reasoning \cite{lake2017building,bottou2014machine}, causal inference \cite{scholkopf2021toward}, and out-of-distribution generalization \cite{bahdanau2018systematic, fodor88}. 

Object-centric learning \cite{binding} aims to discover the latent compositional structure from unstructured observation by learning to bind relevant features, thereby forming useful tokens in an unsupervised way. For images, one of the most popular approaches is to auto-encode the image via the Slot Attention~\cite{slotattention} encoder. Slot Attention applies competitive spatial attention
to partition the image into separate local areas and then obtain a representation, called a slot, from each area.
Then, a decoder generates the image from the slots with the aim of minimizing the reconstruction error. 
Due to the limited capacity and competition between slots, each slot is encouraged to capture a reusable and compositional entity, such as an object.

The primary challenge that remains in the framework of unsupervised object-centric learning is making it work for complex naturalistic scene images. 
Until recently, most object-centric models have adopted a special type of decoder known as the \textit{mixture decoder}. While a weak slot-wise decoder \cite{sbd} is predominantly employed in this mixture decoder due to its efficacy in promoting the emergence of object-centric representation in relatively simple scene images, further studies~\cite{slate,steve} have shown that this strong prior can make handling complex naturalistic scene images more challenging. Contrary to conventional belief, Singh \emph{et al.}~\cite{slate} recently proposed departing from this low-capacity mixture-decoder approach and suggested to use an expressive transformer-based autoregressive image generator in object-centric learning~\cite{slate,steve}. It was shown that increasing the decoder capacity is crucial for handling complex and naturalistic scenes in this framework~\cite{chang2022object, Chang2022HierarchicalAF, dinosaur, slotformer, sysbinder}.

The success of transformer-based image generative modeling in object-centric learning naturally leads to the question: \textit{can diffusion models, another pillar of modern deep generative modeling known for their highly expressive generation capabilities, also benefit object-centric learning}? Diffusion models \cite{sohl2015deep,ddpm} are based on a stochastic denoising process and have demonstrated impressive performance in a variety of  image generation tasks \cite{dalle2,glide,imagen,adm,ldm,srdm,sdedit},  sometimes surpassing transformer-based autoregressive models. Moreover, diffusion models possess unique modeling capabilities that transformer-based autoregressive models cannot provide \cite{sohl2015deep}. However, despite their potential, the applicability of diffusion models to unsupervised object-centric learning remains largely unexplored. Consequently, it is crucial to examine the feasibility of this approach and to identify the associated benefits and limitations.

In this paper, we address this question by introducing a novel model called Latent Slot Diffusion (LSD). The LSD model can be interpreted from two perspectives. From the perspective of object-centric learning, LSD can be viewed as the first model substituting conventional slot decoders with a conditional latent diffusion model, in which the conditioning is object-centric slots provided by Slot Attention. From the diffusion model perspective, our approach is the first \textit{unsupervised} compositional conditional diffusion model. While traditional conditional diffusion models~\cite{dalle2,ldm,imagen,liu2022compositional} require supervised annotations, such as a text description of an image for compositional generation, LSD is a diffusion model that enables the construction of such compositional descriptions in terms of visual concepts extracted from images through unsupervised object-centric learning.

In our experiments, we evaluate the proposed model across various object-centric tasks, including unsupervised object segmentation, downstream property prediction, compositional generation, and image editing. We show that the LSD model delivers significantly superior performance compared to the state-of-the-art model, namely, the transformer-based autoregressive generative model. A notable attribute of the proposed model is that LSD's performance advantage over autoregressive transformers increases as the scene complexity increases. In particular, LSD enables exploring, for the first time, the applicability of object-centric model to the FFHQ dataset \cite{stylegan}, a collection of high-resolution and high-quality face images that surpasses the generative capabilities of existing object-centric models. Additionally, we discuss the overfitting issue faced by LSD on very simple scene images, such as those in the CLEVR dataset \cite{clevr}, and offer suggestions for addressing this problem.

% \vspace{-1mm}

\section{Latent Slot Diffusion}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\textwidth]{fig/method.png}
    \caption{\textbf{Method.} \textit{Left:} In training, we encode the given image as image latent and as slots. We then add noise to the image latent and we train a denoising network to predict the noise given the noisy latent and the slots. \textit{Right:} Given the trained model, we can generate novel images by composing a slot-based concept prompt and decoding it using the trained latent slot diffusion decoder.}
    \vskip -5mm
    \label{fig:method}
\end{figure}

In this section, we outline the auto-encoding framework of our proposed model, Latent Slot Diffusion or LSD, beginning with our object-centric encoder and subsequently describing our proposed decoder.

\subsection{Object-Centric Encoder}
Given an input image $\bx \in \eR^{H \times W \times C}$, our object-centric encoder seeks to decompose and represent it as a collection of $N$ vectors or slots $\bS \in \eR^{N \times D}$, where each slot (denoted as $\bs_n \in \eR^{D}$) is encouraged to represent a compositional entity in the image. For this, we adopt Slot Attention, an architecture that is also used in the current state-of-the-art object-centric learning approaches \cite{slotattention, slate, savi}. We now describe how Slot Attention works in our model.

In Slot Attention, we first encode the input image $\bx$ as a set of $M$ input features $\bE \in \eR^{M \times D_\text{input}}$ via a backbone network $\smash{f_\phi^\text{backbone}}$, \emph{i.e.}, $\smash{\bE = f_\phi^\text{backbone}(\bx)}$. The network $\smash{f_\phi^\text{backbone}}$ is implemented as a CNN
whose final output feature map is flattened to form a set. 
Next, the features in $\bE$ are grouped into $N$ spatial groupings and the information in each grouping is aggregated to produce a \textit{slot}. 
The grouping is achieved via an iterative slot refinement procedure. At the start of the refinement procedure, the slots $\bS$ are filled with random Gaussian noise. Then, they are refined via \textit{competitive attention} over the input features, where the $N$ slots act as the queries and the $M$ input features act as the keys and values.  The  queries and  keys undergo a dot-product to produce $N\times M$ attention proportions. Next, on these attention proportions, the \texttt{softmax} function is applied along the axis $N$ to produce attention weights $\bA$ that capture the soft assignment of each input feature to a slot. Then, for each $n$, all input features are sum-pooled weighted by their attention weights $\bA_{n,1}, \ldots, \bA_{n,M}$ to produce an attention readout $\bu_n \in \eR^{D}$. These steps can be formally described as follows:
\begin{align*}
    \bA = \underset{N}{\texttt{softmax}}\left(\frac{q(\bS) \cdot k(\bE)^T}{\sqrt{D}}\right) 
    && \Longrightarrow && 
    \bA_{n,m} = \dfrac{\bA_{n,m}}{\sum_{m=1}^M \bA_{n,m}}
    && \Longrightarrow && 
    \bu_n = \sum_{m=1}^M  v(\bE_m) \bA_{n,m},
\end{align*}
where, $q, k, v$ are linear projections that map the slots and input features to a common dimension $D$. Using the bottom-up information captured by the readout $\bu_n$, the slots are updated by an RNN as $\smash{\bs_n = f_\phi^\text{RNN}(\bs_n, \bu_n)}$. In practice, competitive attention and RNN update are performed iteratively several times and slots from the last iteration are considered the final slot representation $\bS$.


\subsection{Latent Slot Diffusion Decoder}

In this section, we describe our proposed decoding approach called \textit{Latent Slot Diffusion Decoder} or \textit{LSD decoder} for reconstructing the image given the slot representation $\bS$. The design of the LSD decoder takes advantage of the recent advances in generative modeling based on diffusion \cite{ldm, ddpm}. 
% In Figure~\ref{fig:method} we provide an overview of our decoding approach.
An overview of our decoding approach is provided in Figure~\ref{fig:method}.

\subsubsection{Pre-Trained Image Auto-Encoder}

One of the key design components of the LSD decoder is a pre-trained auto-encoder (AE) \cite{vqgan,ldm}, which provides a way to map an image $\bx$ to a lower-dimensional \textit{latent representation} $\bz_0$ via an encoder $\smash{f_\phi^\text{AE}}$. This enables LSD to reduce the computational burden of reconstructing high-resolution images by using the lower-dimensional latent $\bz_0$ as an intermediate reconstruction target. It also allows us to later obtain the original resolution image without compromising image content and fidelity by decoding the latent $\bz_0$ using the AE decoder $\smash{g_\ta^\text{AE}}$. These can be summarized as:
\begin{align*}
    \bz_0 = f_\phi^\text{AE}(\bx), && \hat{\bx} = g_\ta^\text{AE}(\bz_0), && \text{where } \bz_0 \in \eR^{H_\text{AE} \times W_\text{AE} \times D_\text{AE}}, \text{ and } \hat{\bx} \in \eR^{H \times W \times C} .
    %, \quad \hat{\bx} = g_\text{AE}(\bz)
\end{align*}
In our model, we employ an auto-encoder pre-trained on OpenImages \footnote{We use the pre-trained weights from https://ommer-lab.com/files/latent-diffusion/kl-f8.zip}.

\subsubsection{Slot-Conditioned Diffusion}

In LSD, we leverage diffusion modeling to reconstruct the image latent $\bz_0$ conditioned on the slots $\bS$. This modeling approach has been primarily explored in supervised contexts, \emph{e.g.}, for text-to-image generation in Latent Diffusion Models (LDM) \cite{ldm}. However, unlike LDM, in this work, instead of conditioning the decoder on embeddings of supervised labels, we condition it on slots where the process of obtaining the slots themselves, \emph{i.e.}, Slot Attention, is jointly trained with the decoder without supervision. 
Following LDM, our decoder works by training a decoding distribution $p_\ta(\bz_0|\bS)$ to maximize the log-likelihood $\log p_\ta(\bz_0|\bS)$ of the image latent $\bz_0$ given the slots $\bS$. This decoding distribution $p_\ta(\bz_0|\bS)$ is modeled as a $T$-step denoising process:
\begin{align*}
    p_\ta(\bz_0|\bS) = \int p(\bz_T) \prod_{t=T, \ldots, 1} p_\ta(\bz_{t-1} | \bz_{t}, t, \bS) \,\mathrm{d} \bz_{1:T},
\end{align*}
where $p(\bz_T) = \cN(\mathbf{0}, \mathbf{I})$, $p_\ta(\bz_{t-1} | \bz_{t},  t, \bS)$ is a one-step denoising distribution, and $\bz_T, \bz_{T-1}, \ldots, \bz_0$ is a sequence of progressively denoised latents.
The one-step denoising distribution $p_\ta(\bz_{t-1} | \bz_{t}, t, \bS)$ is parametrized via a neural network $g_\ta^\text{LSD}$ in the following manner:
\begin{align*}
    p_\ta(\bz_{t-1} | \bz_{t}, t, \bS) = \cN\left(\frac{1}{\sqrt{\alpha_t}}\left(\bz_t -\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\hat{\boldsymbol{\epsilon}}_t \right), \beta_t\mathbf{I}  \right), && \text{where } \hat{\boldsymbol{\epsilon}}_t = g^\text{LSD}_\ta(\bz_t, t, \bS),
\end{align*}
$\beta_1, \ldots, \beta_T$ is a linearly increasing variance schedule, $\alpha_t = 1 - \beta_t$, and $\bar{\alpha}_t = \prod_{i=1}^t (1 - \beta_i)$. 

\textbf{Sampling Procedure.} To sample a $\bz_0 \sim p_\ta(\bz_0|\bS)$, we adopt an iterative denoising procedure as in \cite{ldm, ddpm}. The sampling process starts with a latent representation $\bz_T\sim \cN(\mathbf{0}, \mathbf{I})$ filled with random Gaussian noise. Next, conditioned on the slots, we denoise it $T$ times by sampling sequentially from the one-step denoising distribution $\bz_{t-1} \sim p_\ta(\bz_{t-1} | \bz_t, t, \bS)$ for $t=T, \ldots, 1$.
This produces a sequence of latents $\bz_T, \bz_{T-1}, \ldots, \bz_0$ that become progressively cleaner. Finally, $\bz_0$ can be considered as the reconstructed latent representation.

\textbf{Training Procedure.} Following LDM \cite{ldm}, the training of $p_\ta(\bz_0|\bS)$ can be cast to a simple procedure for training $g^\text{LSD}_\ta$ as follows. Given an image $\bx$, its slot representation $\bS$, and its image latent $\bz_0$, we first randomly choose a noise level $t \in \{1, \ldots, T\}$ from a uniform distribution. Given the $t$, we corrupt the clean latent $\bz_0$ and obtain a noised latent $\bz_t$ as follows:
\begin{align*}
    \bz_t = \sqrt{\bar{\alpha}_t} \bz_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}_t, && \text{ where } \boldsymbol{\epsilon}_t \sim \cN(\mathbf{0}, \mathbf{I}), \quad \bar{\alpha}_t = \prod_{i=1}^t (1 - \beta_i).
\end{align*}
The noised latent $\bz_t$ is then given as input to $g^\text{LSD}_\ta$ along with the slots $\bS$ and denoising time-step $t$ to predict the noise $\boldsymbol{\epsilon}_t$.
The network $g^\text{LSD}_\ta$ is trained by minimizing the mean squared error between the predicted noise $\hat{\boldsymbol{\epsilon}}_t$ and the true noise $\boldsymbol{\epsilon}_t$:
\begin{align*}
    \hat{\boldsymbol{\epsilon}}_t = g^\text{LSD}_\ta(\bz_t, t, \bS) && \Longrightarrow && \cL(\phi, \ta) = %\mathbb{E}_{t, \bz, \boldsymbol{\epsilon}}  
    || \hat{\boldsymbol{\epsilon}}_t - \boldsymbol{\epsilon}_t ||^2.
\end{align*}



\subsubsection{Denoising Network}
\label{sec:denoising_net}
We implement the denoising network $g^\text{LSD}_\ta$ as a variant of the conventional UNet architecture adapted to incorporate slot-conditioning. Our denoising network consists of a stack of $L$ layers where each layer $l$ is a UNet-style CNN layer followed by a slot-conditioned transformer:
\begin{align*}
   \tilde{\bh}_{l} = \texttt{CNN}_{\ta}^l([\bh_{l-1}, \bh_{\text{skip}(l)}], t), && \Longrightarrow && \bh_{l} = \texttt{Transformer}_{\ta}^l(\tilde{\bh}_{l} + \mathbf{p}_l, \texttt{cond=}\bS),
\end{align*}
where $\bh_0=\bz_t$ is the input, $\bh_1, \ldots, \bh_{L-1}$ are the hidden states and $\hat{\boldsymbol{\epsilon}}_t = \bh_L$ is the output. 

\textbf{CNN Layers.} Following UNet \cite{unet}, the convolutional layers $\smash{\texttt{CNN}_{\ta}^1, \ldots, \texttt{CNN}_{\ta}^L}$ downsample the feature map via the first $\smash{\frac{L}{2}}$ layers and then upsample it back to the original resolution via the remaining layers. Following standard UNet, these CNN layers also receive inputs via skip connection from an earlier layer denoted by $\text{skip}(l)$. This network design has also been explored in LDM \cite{ldm}.


\textbf{Slot-Conditioned Transformer.} The role of the transformer is to incorporate the information from the slots into the UNet-based denoising process. For this, in each layer $l$, the intermediate feature map $\smash{\tilde\bh_l}$ produced by the CNN layer is flattened into a set of features. To this, positional embeddings $\bp_l$ are added and the resulting features are provided as input to the transformer. Within the transformer, these features interact with each other and with the slots $\bS$, thus incorporating the information from the slots into the denoising process. The transformer output is then reshaped back to a feature map $\bh_l$.


\section{Compositional Image Synthesis}
\label{sec:visual_concept_lib}

In this section, we describe how a trained LSD model can be used to compose and synthesize novel images. The conventional approach to composing novel images is commonly supervised and rely on text prompts. This involves using words from a vocabulary to create a sentence, which is then given to a text-to-image model to synthesize the desired image \cite{dalle, dalle2, imagen, ldm, glide}. However, in a fully unsupervised setting like ours, we first need to build a library of visual concepts by simply observing a large set of unlabelled images. Then, similarly to composing a sentence prompt using words, we compose a \textit{concept prompt} by selecting concepts from the visual concept library. By providing this concept prompt to the LSD decoder, we can then synthesize a desired novel image. This approach of unsupervised compositional image synthesis was also explored in \cite{slate}.

\textbf{Unsupervised Visual Concept Library.} To build a library of visual concepts from unlabelled images, we first take a large batch of $B$ images $\bx_1, \ldots, \bx_B$. We then apply slot attention to obtain slots for these images $\bS_1, \ldots, \bS_B$. Next, we gather all these slots into a single set $\cS$ and perform $K$-means on it. The $K$-means procedure assigns each slot in $\cS$ to one of the $K$ clusters. We consider the set of slots assigned to a $k$-th cluster as a visual concept library $\cV_k$. With $K$ as the number of clusters, this method results in $K$ visual concept libraries $\cV_1, \ldots, \cV_K$. Our experiments will demonstrate that this basic $K$-means technique can generate semantically meaningful concept libraries. For example, on a dataset of human face images like FFHQ \cite{stylegan}, the $K$ libraries correspond to practical concept categories such as hair style, face, clothing, and background.

\textbf{Novel Image Synthesis.} Given libraries $\cV_1, \ldots, \cV_K$, we can compose a concept prompt $\bS_\text{compose}$ by picking $K$ slots, each from the corresponding $k$-th library, and stacking them together:
\begin{align*}
    \bS_\text{compose} = (\bs_1, \ldots, \bs_K), && \text{where } \bs_k \sim \text{Uniform}(\cV_k)
\end{align*}
We then give the composed prompt $\bS_\text{compose}$ to the LSD decoder to generate the image latent: $\smash{\bz_\text{compose} \sim p_\ta(\bz_0 | \bS_\text{compose})}$. Applying the image decoder on $\smash{\bz_\text{compose}}$ generates the desired novel scene image $\smash{\bx_\text{compose} = g_\ta^\text{AE}(\bz_\text{compose})}$. For instance, in the FFHQ dataset, the concept prompt can be a collection of chosen hair style, face, clothing, and background. Decoding this prompt would generate a face image that conforms to this prompt.


% \clearpage
\section{Related Work}
\textbf{Unsupervised Object-Centric Learning.} Object-centric representation learning aims to decompose multi-object scenes into meaningful object entities. A common approach to this is by auto-encoding. In this line, the focus has been to design an appropriate decoder that supports good decomposition. The most widely used decoders include the mixture-decoder \cite{monet, iodine, slotattention, nem, genesis, von2020towards, anciukevicius2020object, du2020unsupervised, engelcke2021genesis, simone, Zhang2022RobustAC}, spatial transformer decoder \cite{air, spair, space, gnm, deng2020generative, roots, scalor, gswm, swb, yoon2023investigation}, Neural Radiance Fields (NeRF) \cite{stelzner2021decomposing,Wu2022ObPoseLC,Smith2022UnsupervisedDA}, transformer decoder \cite{slate, steve, sysbinder, slotformer, Chang2022HierarchicalAF, sajjadi2022osrt, Gopalakrishnan2022UnsupervisedLO}, energy-based models \cite{du2021unsupervised} and complex-valued functions \cite{Lowe2022ComplexValuedAF}. 
Among these, the mixture decoder has been shown to struggle on complex scene images \cite{slate, steve} while the spatial transformer decoder requires careful tuning of hyperparameters which limits their applicability in realistic scenes. Neural Radiance Fields require camera poses, and the learning setting involves 3D scenes unlike ours. 
Another line of work seeks object-centric scene decomposition without reconstruction. This includes works such as \cite{dinosaur, Wen2022SelfSupervisedVR, Henaff2022ObjectDA, cutler, sslslot, contrastslot}. However, unlike ours, these approaches lack the ability to generate images. Preceding object-centric learning, several works pursued disentanglement within a single-vector representation of the scene and use it for compositional image generation \cite{infogan, higgins2017beta, kim2018disentangling, kumar2017variational, chen2018isolating}. However, lacking spatial binding mechanisms, these methods struggle in multi-object scenes \cite{iodine} unlike ours.



\textbf{Diffusion Models.} 
Diffusion models (DMs) are a recent class of generative models that can produce high-quality images by reversing a stochastic process that gradually adds noise to an image \cite{ddpm,scoredm}. DMs have been applied to various tasks in computer vision, such as class-conditioned generation, text-to-image generation, image editing, super-resolution, and inpainting \cite{adm,cfdm,dalle2,glide,imagen,sdedit,cascadedm,srdm}. Recently, \cite{ldm} proposed Latent Diffusion Models (LDM). By virtue of operating on a low-dimensional latent space, LDM reduces the computational demands of DMs significantly.
\cite{composdm} introduced a method for multi-object scene generation by combining signals from multiple text-conditioned denoising networks. However, to achieve controllable generation, many of these existing DMs require additional labels such as text descriptions to train and control the generation process. In contrast, our model can generate images compositionally using object concepts directly extracted from images. Moreover, DMs have also been used for representation learning. \cite{labeleffdm} demonstrated that the intermediate features of a learned DM are useful representations for label-efficient semantic segmentation. \cite{dmae} introduced an image encoder that compresses an image into a latent representation, jointly trained with the denoising objective. Nevertheless, these representations are unstructured and not modular like ours. 


% \clearpage
\section{Experiments} \label{sec:experiment}

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=1.\textwidth]{fig/segmentation.pdf}
\vskip -0.1in
\caption{\textbf{Visualization of Unsupervised Object Segmentation.} We show visualizations of predicted segments on CLEVR, CLEVRTex, MOVi-E, and FFHQ datasets. 
}
\vskip -2mm
\label{fig:seg}
\end{figure*}
% ########################################################################

We extensively evaluate our proposed Latent Slot Diffusion (LSD) model on various object-centric tasks, including unsupervised object segmentation, downstream property prediction, compositional generation, and image editing. As will be shown, our model demonstrates substantial improvements over the state-of-the-art on multiple challenging datasets with complex texture and background, including FFHQ~\cite{stylegan} which has been beyond the generative capability of object-centric models.

\textbf{Datasets.} We evaluate our model on five datasets. Four of them are synthetic multi-object datasets---CLEVR~\cite{clevr}, CLEVRTex~\cite{clevrtex}, MOVi-C, MOVi-E~\cite{kubric}. They present increasing levels of difficulty---CLEVRTex adds texture to objects and backgrounds, MOVi-C uses more complex objects and natural backgrounds, and MOVi-E contains large numbers of objects (up to 23) per scene. Furthermore, we explore the applicability of object-centric models to FFHQ~\cite{stylegan}, a dataset of high-quality face images. Unlike previous works in this line that only investigate low-resolution images, \emph{e.g.}, up to $128 \times 128$, we use a resolution of $256 \times 256$ for all datasets in our experiments.

\textbf{Baselines.} We compare our model against SLATE, the state-of-the-art object-centric learning and unsupervised compositional image generation approach. We use its improved version~\cite{steve}, which is more robust in complex scenes. For a fair comparison with LSD that leverages pre-trained auto-encoder, we also develop a variant of SLATE denoted as SLATE$^+$, where its low-capacity dVAE~\cite{slate} is replaced with a pre-trained VQGAN model \cite{vqgan}. For all models in this work, we use OpenImages-pretrained auto-encoders~\cite{ldm}.

\subsection{Object-Centric Representation Learning}

In line with previous work~\cite{slotattention}, we use unsupervised object segmentation and downstream property prediction to evaluate the object-centric learning capability and representation quality. Within each dataset, all models we compare use the same number of slots.

\begin{table*}[t]
\begin{small}
    \caption{\textbf{Segmentation Performance and Representation Quality.} \textit{Left:} We evaluate the segmentation quality and report mBO, mIoU, and FG-ARI scores across various datasets and baselines. \textit{Right:} We measure the representation quality by learning a probe to predict object properties given frozen slots. For position and 3D bounding box, we report MSE. For shape, material, and category, we report accuracy.
    }
    \vspace{-0.5em}

    % CLEVRTex
    \begin{subtable}{1.0\linewidth}
    \caption{CLEVRTex}
    \vspace{-1mm}
    \centering
    \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Segmentation}}          & SLATE        & SLATE$^+$      & LSD (Ours)   \\
    \midrule
    mBO ($\uparrow$)         & 51.24 & 56.22 & \textbf{66.56} \\
    mIoU ($\uparrow$)        & 50.04 & 54.93 & \textbf{65.02} \\
    FG-ARI ($\uparrow$)      & 43.59 & \textbf{73.42} & 61.74 \\
    \bottomrule
    \end{tabular}
        \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Representation}}           & SLATE        & SLATE$^+$      & LSD (Ours)   \\
    \midrule
    Shape ($\uparrow$)       & 77.62 & 76.20 & \textbf{81.60} \\
    Material ($\uparrow$)   & 72.59 & 66.04 & \textbf{77.77} \\
    Position ($\downarrow$)  & 1.16 & 1.13 & \textbf{1.10}    \\
    \bottomrule
    \end{tabular}
    \vspace{1mm}
    \end{subtable}


    % MOVi-C
    \begin{subtable}{1.0\linewidth}
    \caption{MOVi-C}
    \vspace{-1mm}
    \centering
    \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Segmentation}}          & SLATE        & SLATE$^+$      & LSD (Ours)   \\
    \midrule
    mBO ($\uparrow$)         & 38.60 & 39.24  & \textbf{46.29} \\
    mIoU ($\uparrow$)        & 37.11 & 37.59  & \textbf{44.99} \\
    FG-ARI ($\uparrow$)      & 48.00 & \textbf{51.53}  & 50.53 \\
    \bottomrule
    \end{tabular}
        \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Representation}}           & SLATE        & SLATE$^+$      & LSD (Ours)   \\
    \midrule
    Position ($\downarrow$)  & 1.36 & 1.22   & \textbf{1.14}   \\
    3D B-Box ($\downarrow$)   & 1.46 & \textbf{1.32}  & 1.44    \\
    Category ($\uparrow$)    & 42.49 & 46.46 & \textbf{46.71} \\
    \bottomrule
    \end{tabular}
    \vspace{1mm}    
    \end{subtable}


    % MOVi-E
    \begin{subtable}{1.0\linewidth}
    \caption{MOVi-E}
    \vspace{-1mm}
    \centering
    \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Segmentation}}          & SLATE        & SLATE$^+$      & LSD (Ours)   \\
    \midrule
    mBO ($\uparrow$)         & 28.66 & 22.69  & \textbf{39.63} \\
    mIoU ($\uparrow$)        & 27.20 & 21.10  & \textbf{38.28} \\
    FG-ARI ($\uparrow$)     & 41.37 & 46.34  & \textbf{53.40} \\
    \bottomrule
    \end{tabular}
        \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Representation}}           & SLATE        & SLATE$^+$      & LSD (Ours)   \\
    \midrule
    Position ($\downarrow$)  & 2.25 & 2.07  & \textbf{1.92}    \\
    3D  B-Box ($\downarrow$)   & 3.24 & 3.11  & \textbf{2.94}    \\
    Category ($\uparrow$)    & 38.89 & 38.55  & \textbf{43.15} \\
    \bottomrule
    \end{tabular}
    \end{subtable}

    \label{tab:complex_quant} 
    \vspace{-2mm}
\end{small}

\end{table*}



\textbf{Unsupervised Object Segmentation.}
Following~\cite{slotattention, dinosaur}, to measure segmentation quality, we report the foreground adjusted rand index (FG-ARI), the mean intersection over union (mIoU), and the mean best overlap (mBO), computed using the attention masks of Slot Attention.
% whose resolution is $64 \times 64$ in all models.

Our results in Table \ref{tab:complex_quant} suggest that LSD is particularly strong in visually complex scenes. For example, on the most challenging MOVi-E dataset, LSD outperforms the strongest baseline by ${>} 10\%$ in mBO and mIoU, and ${\sim} 7\%$ in FG-ARI. On CLEVRTex and MOVi-C featuring complex textures, LSD also achieves ${>} 10\%$ and ${\sim} 7\%$ gain respectively, in both mBO and mIoU. We visually show the superior segmentation quality of LSD in Figure \ref{fig:seg}. LSD obtains tighter object boundaries, less object splitting, and cleaner background segmentation. The advantages are most noticeable on CLEVRTex and MOVi-E, where the baselines over-segment the objects more frequently, or exhibit a common failure mode that divides images into approximately uniformly distributed block masks.

We also note that the FG-ARI score is sometimes not an ideal metric for evaluating segmentation quality, because it only considers the foreground pixels and disregards the correctness of the mask shape, as also highlighted by \cite{genesis,clevrtex}. In the MOVi-E experiment, even though SLATE$^+$ tends to partition the images arbitrarily into uniform patches, as shown in Figure \ref{fig:seg}, it still shows ${\sim} 5\%$ an improvement gain on FG-ARI over SLATE, which, in fact, produces more reasonable object masks. This highlights the need for additional metrics, such as the mIoU score that we also measure for evaluating the segmentation quality in this work.


\textbf{Downstream Property Prediction.}
Following~\cite{dittadi2021generalization, slotattention}, we evaluate the quality of the learned object-centric representations through downstream property prediction. Specifically, for each property, we train a network to predict the property given a frozen slot representation as input. The correspondence between the slot and its true label is determined by Hungarian matching~\cite{hungarian} using masks. We use linear heads and 2-layer MLP as the prediction networks for discrete and continuous values, respectively. We report prediction accuracy for discrete properties (\emph{e.g.}, shape and material), and mean squared error for continuous properties (\emph{e.g.}, position).


As shown in Table \ref{tab:complex_quant}, LSD is competitive with or better than the strongest baseline on CLEVRTex, MOVi-C, and MOVi-E, which is consistent with our finding in unsupervised object segmentation that LSD shines in visually complex scenes. For example, LSD obtains ${>} 4\%$ gain in predicting object category on MOVi-E and object material on CLEVRTex. 

% ########################################################################
\begin{table}[t]
\caption{\textbf{Compositional Image Generation.} On various datasets, we generate images using object slots randomly sampled from the dataset. We compare the fidelity of the generated images via the FID score. The lower the FID score, the better the fidelity. We find that our model produces the best fidelity scores compared to the baselines. }
\begin{center}
\begin{small}
    \begin{tabular}{lccc}
    \toprule
    FID $\downarrow$     & SLATE        & SLATE$^+$       & LSD (Ours)       \\
    \midrule
    CLEVRTex      & 105.83 & 69.23 & \textbf{29.53}  \\
    MOVi-C        & 170.83 & 148.27 & \textbf{69.12}  \\
    MOVi-E        & 169.32 & 126.51 & \textbf{64.76}  \\
    FFHQ          & 112.38 & 98.76  & \textbf{27.83} \\
    \bottomrule
    \end{tabular}
    \label{tab:fid} 
    \vspace{-5mm}
\end{small}
\end{center}
\end{table}

\subsection{Compositional Generation with Visual Concept Library}

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=1.\textwidth]{fig/compsitional_generation.pdf}
\vskip -0.1in
\caption{\textbf{Compositional Generation Samples.} We qualitatively compare the quality of image samples generated by our model with the baselines. We observe that LSD provides significantly higher fidelity and more clear details compared to the other methods.}
% \vskip -1mm
\label{fig:compose}
\end{figure*}
% ########################################################################


Like text-to-image generative models, LSD is able to take unseen slot-based prompts at test time and compose new images. Unlike text-to-image models, however, LSD obtains the compositional generation ability solely from images without relying on additional supervision like text inputs.
As described in Section \ref{sec:visual_concept_lib}, we first build a concept library. Then, we sample one slot representation from each visual concept library and concatenate them into a sequence to form a slot-based prompt. We then feed the slot-based prompts to the LSD decoder to generate the images. This produces scenes with novel object layouts and faces with unseen attribute combinations.

We report in Table~\ref{tab:fid} the FID score~\cite{fid} as a measure of the compositional generation quality. Following standard practice~\cite{adm}, we compute the FID score using 2K generated images and the full training dataset. Across all datasets, LSD achieves significantly better FID scores than SLATE and SLATE$^+$. We further demonstrate the superior compositional generation quality of LSD in Figure~\ref{fig:compose}. We observe that on the more challenging MOVi-E and FFHQ datasets, LSD generates images with substantially more clear details and better coherence. In contrast, SLATE is limited by its decoder and produces images with missing details in the objects or human faces. While the details can be improved by the pre-trained auto-encoder in SLATE$^+$, some samples still exhibit severe distortions.


\subsection{Slot-Based Image Editing}

% ########################################################################
\begin{figure}[t]
\centering
\vskip -0.1in
\includegraphics[width=1.0\textwidth]{fig/object_based_editing_horizontal.pdf}
\caption{\textbf{Slot-Based Image Editing.} \textit{Left:} Our model demonstrates image editing capabilities, including object removal, extraction, insertion, background extraction, and swapping. \textit{Right:} In the FFHQ dataset, we showcase face replacement by combining face slots from Source-B images with hairstyle, clothing, and background slots from Source-A images.}
\vskip -3mm
\label{fig:edit_multi_obj}
\end{figure}
% ########################################################################


In addition to generating new images from randomly sampled slot-based prompts, LSD also allows editing existing images by directly modifying their slot representations. Our experiments showcase LSD's capability in slot-based image editing, including object removal, single-object segmentation, object insertion, background extraction, and background swapping. Remarkably, we demonstrate, for the first time in object-centric generative models, the ability to perform face editing in real-world images, as illustrated in Figure~\ref{fig:edit_multi_obj}.


We perform slot manipulation on the CLEVRTex dataset, including object removal, single-object extraction, object insertion, background extraction, and background swapping. To remove an object or extract the background, we discard the relevant slot or all object slots, respectively. Our findings indicate that with near-perfect object decomposition, removing an object can be achieved by simply discarding its slot. Additionally, the background image can be rendered from a single background slot, even without single-slot conditioning during training. In single-object extraction, we render an object with the same background using the respective object and background slots. For object insertion and background swapping, we split the image into top and bottom pairs (Figure~\ref{fig:edit_multi_obj} \textit{left}) and introduce a slot from another image or swap background slots. This demonstrates that an image's background can be entirely altered while preserving its original objects.

We further explore face replacement on the FFHQ dataset. LSD decomposes each image into four slots, corresponding to face, hairstyle, clothing, and background. Our results show that by replacing the face slots of the images, we are able to coherently change the image while maintaining the hairstyle, clothing, and background. The resulting images look realistic, suggesting that LSD can effectively blend various attributes even when given novel combinations.

\section{Discussion}

\textbf{LSD on Simple Images.} While LSD shows significant gains in complex naturalistic scenes, our preliminary investigation suggests a somewhat diminished performance in terms of segmentation and representation when the dataset consists of only visually simple and monotonous scene images like CLEVR. We conjecture that LSD may suffer from an overfitting problem when its high expressiveness encounters visually simple images. To address this issue, additional complexities can be introduced during training, such as integrating images from other datasets. In Appendix \ref{sec:appx_clevr}, we offer a more detailed analysis of this challenge using the CLEVR dataset and introduce a training scheme that includes data samples from CLEVRTex to alleviate the problem. Our findings indicate that by integrating greater complexities into the training set, LSD can be effectively employed on simple datasets as well. 

\textbf{Pre-trained Diffusion Models.}
In recent studies of DMs, much attention has been given to controllable image generation using large-scale pre-trained diffusion models such as Stable Diffusion \cite{gligen,t2iadapter,controlnet}. One intriguing question is whether such pre-trained diffusion models can facilitate the learning of object-centric representation. In this regard, we present a preliminary investigation in Appendix \ref{sec:appx_pretrained_dm}, where we test an LSD variant utilizing the pre-trained Stable Diffusion and a learnable slot adapter. Our findings indicate that although the pre-trained Stable Diffusion model has not been trained on any multi-object datasets, it still provides a learning signal that enables reasonable object-centric segmentation. However, it exhibits inferior segmentation quality when compared to LSD, probably due to this lack of prior training on the same data distribution.

\textbf{Computation Requirement.} 
We compare the computation requirement of LSD and the baseline models in the CLEVRTex setting. LSD requires approximately 32 GB of training memory, compared to 31 GB and 36 GB for SLATE and SLATE$^+$, respectively. Notably, LSD does not require much higher memory consumption than regular unconditional latent diffusion, which takes 28 GB under the same setting. We train LSD on 2 NVIDIA RTX 6000 GPUs for 4.5 days, while SLATE and SLATE$^+$ are trained in 1 day and 2.7 days using the same GPU setup. For test-time generation, LSD takes 50.7 seconds to generate 50 images, while SLATE and SLATE$^+$ takes 86.1 and 87.6 seconds, respectively. 


\section{Conclusion}
The integration of diffusion models into unsupervised object-centric learning has been largely unexplored, but holds significant potential. To assess the feasibility of this approach and to identify its strengths and weaknesses, we introduced the Latent Slot Diffusion model in this study, which serves two purposes: (1) the first model integrating diffusion models into unsupervised object-centric learning, and also (2) the first unsupervised compositional diffusion model which does not require supervised annotations like text. Our key findings indicate that the proposed model surpasses state-of-the-art transformer-based object-centric models in multiple tasks, with its performance advantage increasing as image complexity grows. Moreover, the unsupervised compositional generation quality of our model exceeds that of transformer-based counterparts. We believe that this work represents a significant advancement in object-centric learning towards dealing with complex naturalistic images.



\section*{Acknowledgements}

This work is supported by Brain Pool PlusProgram (No. 2021H1D3A2A03103645) and Young Researcher Program (No. 2022R1C1C1009443) through the National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT. The GPU computing used for this work is partly supported by KI Cloud of Division of National SupercomputingÂ  Center, Korea Institute of Science and Technology Information (KISTI). We would like to thank Yi-Fu Wu for his contributions to the preliminary stages of this paper and Ligong Han and Yuxiao Chen for their valuable feedback on the manuscript.


\bibliography{refs, refs_gs, refs_ahn, refs_jd}
\bibliographystyle{plain}

\newpage
\appendix

\section{Broader Impact}

The proposed Latent Slot Diffusion (LSD) model is a generative model for object-centric representation learning. The main potential negative social impacts is associated with its application in image editing and novel image synthesis. By enabling the extraction of object-centric or component-based representations from images, LSD allows for the generation of entirely new scenes with previously unseen configurations. This capability raises concerns related to image manipulation and privacy. For example, the capability to synthesize new human faces introduces potential privacy issues like deepfake generation and identity theft. Moreover, this ability could potentially cause digital misinformation, as it can generate and manipulate images with high accuracy in object-based level, making it increasingly difficult to distinguish between real and fake or edited visual content. Therefore, further application of this method should be proceeded under strict ethical guidelines and regulation.

\section{LSD on Simple Images} \label{sec:appx_clevr}

In the paper, we demonstrate that LSD achieves considerable performance improvements for complex images. However, our early experiments reveal that when applied to visually simplistic datasets like CLEVR, LSD exhibits reduced performance in terms of segmentation and representation quality, as illustrated in Table \ref{tab:appx_clevr}. To gain further insights into these results, we provide visualizations of the segmentation outputs in Figure \ref{fig:appx_clevr_seg}.

As illustrated by the LSD samples in Figure \ref{fig:appx_clevr_seg}, certain parts of the background, such as the upper regions of the images, are split and assigned to multiple object slots. This behavior leads to information mixing between objects and the background, which substantially reduces the performance of slot segmentation and representation. We conjecture that this issue may stem from overfitting in the model. Due to the simplicity of the texture and the absence of objects in these areas, the powerful decoder of LSD can easily memorize the rendering process of those regions independently of the slot-conditioning. As a result, it does not provide a sufficient training signal for the encoder to group those background parts into a meaningful slot.

In light of these findings, we tested a straightforward approach to address this problem. We propose introducing additional complexity to the background by incorporating data samples from the CLEVRTex dataset into the training process. By integrating the CLEVRTex samples, we introduce 60 different background textures, which makes implicit memorization difficult. This effectively provides the learning signal to capture the background component into the slot-conditioning. The results of this approach, labeled as LSD(Mix), can be seen in Figure \ref{fig:appx_clevr_seg}. As clearly illustrated, including CLEVRTex data samples in the training results in cleaner background segmentation and alleviates the information mixing issue. As a result, LSD(Mix) achieves approximately 22\% gains of improvement in both mIoU and mBO compared to the original model trained solely on CLEVR samples, as shown in Table \ref{tab:appx_clevr}. However, we also see that the FID score indicated a decrease in the generation quality of LSD(Mix) compared to the original LSD, likely due to the increased demand of model capacity to model both datasets. Overall, this experiment confirms that introducing complexity into the training dataset can mitigate the overfitting problem and allow LSD to effectively perform representation learning on simple datasets.


% NEW TABLE
\begin{table}[b]
% \begin{small}
    \centering
    \caption{
    \textbf{Quantitative Evaluation on CLEVR dataset.} We evalute the models by reporting mBO, mIoU and FG-ARI scores for segmentation quality, property prediction scores for representation quality, and FID scores for compositional generation quality.
    }
    % \vskip 0.1in
    \begin{tabular}{lcccc}
    \toprule
                     & SLATE        & SLATE$^+$       & LSD & LSD (Mix) \\
    \midrule
    mBO ($\uparrow$)         & 64.86 & \textbf{67.42}  & 38.49 & 61.09 \\
    mIoU ($\uparrow$)        & 63.96 & \textbf{66.62}  & 37.49 & 59.76 \\
    FG-ARI ($\uparrow$)      & 69.20 & \textbf{88.56}  & 76.32 & 76.30 \\
    % \bottomrule
    % \end{tabular}
    % \hspace{0.2em}
    % \begin{tabular}{lccc}
    % \toprule
    % \scriptsize{\textbf{Representation}}                 & SLATE        & SLATE$^+$       & LSD (Ours) \\
    % \midrule
    Shape ($\uparrow$)       & 95.66 & \textbf{95.70}  & 75.16 & 80.48 \\
    Material ($\uparrow$)    & 97.62 & \textbf{97.96}  & 94.21 & 96.24 \\
    Position ($\downarrow$)  & 0.59  & \textbf{0.51}   & 0.86  & 0.99 \\
    FID ($\downarrow$)       & 52.96  & 20.93   & \textbf{16.22}  & 19.45 \\
    \bottomrule
    \end{tabular}
    \label{tab:appx_clevr} 
    % \vspace{-1em}
% \end{small}

\end{table}

\begin{figure*}[t]
\centering
\includegraphics[width=1.\textwidth]{fig/appx_clevr_segmentation.png}
\caption{\textbf{Visualization of Unsupervised Object Segmentation on CLEVR dataset.} The LSD(Mix) variant corresponds to the model trained on both CLEVR and CLEVRTex training images.
}
\label{fig:appx_clevr_seg}
\end{figure*}

\section{LSD using Pre-trained Diffusion Decoder} \label{sec:appx_pretrained_dm}
One of the main focuses of recent research in diffusion models have been on adapting pre-trained models for additional control signals \cite{gligen, t2iadapter, controlnet}. Specifically, these studies propose the integration of a learnable adapter network into large-scale pre-trained diffusion models, such as Stable Diffusion, to enable controllable generation based on various types of conditioning such as sketches, semantic layouts, and depth maps. An inherent advantage of these approaches is their ability to bypass the resource-intensive processes of training the entire generative model, while still enabling the production of high-quality controllable images through minimal training of the adapter network.


We note that such a model design can also be utilized for slot-based conditioning. However, unlike existing works that primarily focus on learning the adapter to integrate the provided conditioning into the pre-trained diffusion model, our focus lies in training an adapter to achieve two objectives simultaneously: (1) we aim to learn how to extract object representations (slots) from input images in an unsupervised manner, and (2) we aim to learn how to incorporate these slot representations into the pre-trained diffusion model to facilitate slot-based conditional generation. This leads to an intriguing question: can learning through such a model, 
% utilizing signals from a pre-trained decoder
which has not been specifically trained on multi-object scenarios, provide the necessary learning signal to acquire object-centric representations? 

\textbf{Model Design} 
To address this question, we conduct a preliminary exploration by introducing a variant of LSD called Control-LSD. Control-LSD utilizes a similar design of the adapter structure proposed in ControlNet \cite{controlnet} and consists of three main components: (1) a pre-trained Stable Diffusion model that is frozen throughout the training, (2) a "control block" that is a trainable copy of the unet encoder from Stable Diffusion, and (3) a object-centric encoder from the LSD model. Similar to LSD, the object-centric encoder takes an image as input and generates a set of slots, which are then conditioned on using a Slot-Conditioned Transformer. However, instead of providing the slots to the diffusion decoder, Control-LSD feeds the slots into the trainable control block while solely employing null text conditioning in the Stable Diffusion. To integrate the slot control signal, the outputs of each intermediate control block are element-wise added to the intermediate blocks and skip-connections within the frozen Stable Diffusion model. Following LSD, we use a input resolution of 256 $\times$ 256 and attention map resolution of 64 $\times$ 64 for all datasets. We also use the same number of slots as in LSD for multi-object dataset, and 10 slots for the COCO dataset. 

\begin{figure*}[t]
\centering
\includegraphics[width=1.\textwidth]{fig/appx_control_lsd.pdf}
\caption{\textbf{Qualitative Results of Control-LSD on CLEVR, CLEVRTex, MOVi-E, and COCO.} The CLEVR(Mix) corresponds to the model trained on both CLEVR and CLEVRTex samples.
}
\label{fig:control_lsd}
\end{figure*}

\textbf{Results}
We evaluate Control-LSD on three multi-object datasets: CLEVR, CLEVRTex, and MOVi-E, as well as one real-world dataset, COCO \cite{coco}. Following the proposed LSD, we also include a CLEVR(Mix) variant for the CLEVR dataset which was trained on both CLEVR and CLEVRTex samples. The segmentation results are presented in Figure \ref{fig:control_lsd}. We can see that Control-LSD struggles to achieve satisfactory object segmentation on the CLEVR dataset. The model has difficulties in distinguishing between the background and the objects. We also note that the model demonstrate more reasonable results on CLVERTex and introducing samples from CLEVRTex to the training process of CLEVR (i.e., CLEVR(Mix)) also led to an improvement on CLEVR, but the results are still inferior to those achieved by LSD. Interestingly, we also observe that Control-LSD achieve better segmentation quality when applied to more realistic scenes, such as those in the MOVi-E dataset. We conjecture that this is attributed to the fact that the complexity of MOVi-E aligns more closely with real-world images used for pre-training the Stable Diffusion model, as compared to the samples from CLEVR and CLEVRTex datasets. This indicates that the distribution gap between the data used for pre-training Stable Diffusion and the multi-object datasets used for learning object-centric representation might be a potential contributing factor to the lower performance of Control-LSD on the multi-object datasets.


Finally, we also evaluate Control-LSD on a real-world multi-object dataset COCO \cite{coco}. Notably, Control-LSD showcases its ability to capture semantically meaningful entities in the slots, as illustrated in the segmentation results in Figure \ref{fig:control_lsd}. However, we find that it is challenging to obtain accurate object-based pixel grouping from these images. In this regard, we would like to share two key observations when applying the model on real-world scenes: (1) We found obtaining object segmentations that are aligned with the dataset annotations in a fully unsupervised manner is a challenging task. For instance, Control-LSD tends to split a human into two parts (the upper body and the legs), while the dataset annotates the full human as a single segment. It is important to emphasize that neither of the two segmentation results are incorrect, as each could be beneficial for specific downstream tasks. (2) During our investigation, we found that the segmentation quality on real-world images is highly sensitive to the choice of the number of slots. This sensitivity can be attributed to the model's tendency to always utilize the entire set of slots to segment the image. Consequently, when the number of slots is lower than the actual number of entities in the image, the model tends to generate attention masks at the semantic level as opposed to the object level. Conversely, when the number of slots exceeds the number of entities in the image, the model tends to over-segment the image, i.e., splitting some of the entity into multiple slots. This behavior limits the applicability of Control-LSD in real-world data, where the true number of objects is not known to the model. Therefore, we believe further investigation is needed for applying the model to real-world applications.

We hope that this preliminary exploration and the experimental analysis of Control-LSD offer valuable insights to other researchers and serve as a source of inspiration for future studies in this direction.


\section{Additional Implementation Details}

To ensure reproducibility, we are sharing additional implementation details in this section. We will also release our source code upon acceptance.

\subsection{Experiment Setup}

\textbf{Datasets.}
In our experiments, we create three data subsets for each dataset: training, validation, and testing. For CLEVR, we utilize the official split for training and validation sets. However, for evaluation, we do not use the official test set as CLEVR does not provide ground-truth object segmentation masks. Instead, we use the "clevr\_with\_mask" subset from the Multi-Object Datasets \cite{multiobjdataset} for evaluation. This subset is designed to have the same object distribution as CLEVR while providing ground-truth masks and attributes annotations for each object in the image, which allows us to evaluate the segmentation and representation quality of each model. We use the full subset as the test set for evaluation. For CLEVRTex, there is no official split provided, so we allocate 80\% of the data for training, 10\% for validation, and 10\% for testing. Regarding MOVi-C and MOVi-E, we use 90\% of the training set data for training and reserve 10\% for validation. We use the official validation set for testing. It is important to note that the test sets in MOVi-C and MOVi-E are designed for out-of-distribution (OOD) evaluation, so we do not use them for test time evaluation. For the FFHQ dataset, we use 86\% of the dataset ($\sim$ 60K images) for training and 7\% ($\sim$ 5K images) for validation. To ensure consistency across all models, we shuffled the dataset and utilized a fixed random seed when splitting the data.

\textbf{Segmentation Metrics.} 
To evaluate the unsupervised object segmentation, we use three metrics: the foreground adjusted rand index (FG-ARI), the mean intersection over union (mIoU), and the mean best overlap (mBO). We predict the object mask by using the argmax along the slot dimension in the attention map. In computing the mIoU, we use Hungarian matching to obtain the ground-truth and slots assignment. Following \cite{dittadi2021generalization}, we use the negative cosine similarity as the matching loss. In mBO, we assign the ground-truth segment to the slot with the largest overlap. Note that the mBO metric is generally simpler than mIoU since it does not require a strict one-to-one mapping between the ground-truth and predicted masks.

\subsection{Implementation Details}

\begin{table}[h!]
\centering
\begin{tabular}{@{}llcccc@{}}
\toprule
               &               & \multicolumn{4}{c}{Dataset}                          \\ \cmidrule(l){3-6} 
Module      & Hyperparameter     & CLEVR       & CLEVRTex    & MOVi-C/E    & FFHQ                   \\ \midrule
General     &   Batch Size       & 64        & 64             & 128     & 128                 \\
            &   Training Steps   & 200K & 200K & 200K & 200K \\
    \midrule
CNN Backbone            &  Input Resolution &     \multicolumn{4}{c}{256}    \\
                        &  Output Resolution &     \multicolumn{4}{c}{64}    \\
                        &  Self Attention  &     \multicolumn{4}{c}{Middle Layer}    \\
                        &  Base Channels &     \multicolumn{4}{c}{128}    \\
                        &  Channel Multipliers &     \multicolumn{4}{c}{[1,1,2,4]}    \\
                        &  \# Heads &     \multicolumn{4}{c}{8}    \\
                        &  \# Res Blocks / Layer &     \multicolumn{4}{c}{2}    \\
                        &  Output Channels &   128    &   128  & 192  & 192     \\
                        &  Learning Rate  &     0.0001 &     0.00003     &     0.0001  & 0.0001       \\ \midrule
Slot Attention          &  Input Resolution &     \multicolumn{4}{c}{64}    \\
                        &  \# Iterations  &     \multicolumn{4}{c}{3}    \\
                        &  Slot Size  &     128      &     128   &    192  &    192   \\
                        &  \# Slots  &  11   &  11  &    24 &    4       \\
                        &  Learning Rate  &     0.0001  & 0.00003    &     0.0001  & 0.0001       \\ \midrule
Auto-Encoder            &  Model &     \multicolumn{4}{c}{KL-8} \\
                        &  Input Resolution &     \multicolumn{4}{c}{256} \\
                        &  Output Resolution &     \multicolumn{4}{c}{32} \\
                        &  Output Channels &     \multicolumn{4}{c}{4} \\ \midrule
LSD Decoder       &  Input Resolution &     \multicolumn{4}{c}{32} \\
                        &  Input Channels &     \multicolumn{4}{c}{4} \\
                        &  $\beta$ scheduler   &     \multicolumn{4}{c}{Linear} \\
                        &  Mid Layer Attention   &     \multicolumn{4}{c}{Yes} \\
                        &  \# Res Blocks / Layer &     \multicolumn{4}{c}{2}    \\
                        &  Image Latent Scaling  &     \multicolumn{4}{c}{0.18215}    \\
                        &  Learning Rate   &     \multicolumn{4}{c}{0.0001}    \\
                        &  \# Heads & 4 & 4 & 8 & 8  \\
                        &  Base Channels & 128  & 128  & 192 & 192 \\
                        &  Attention Resolution &     [1,2,4]      &     [1,2,4]   &    [1,2,4,4] &    [1,2,4,8]   \\
                        &  Channel Multipliers & [1,2,4] & [1,2,4] & [1,2,4,4] & [1,2,4,4]  \\ \midrule

\end{tabular}
\vspace{2mm}
\caption{Hyperparameters of our model used in our experiments.}
\label{tab:hyperparams}
\end{table}

We will provide an overview of the implementation details in this section. The hyperparameters used in our approach are listed in Table \ref{tab:hyperparams}.

\textbf{Auto-Encoding CNN backbone.}
The object-centric encoder in our model includes a CNN backbone, which transforms the raw image input into feature vectors that can be processed by the slot attention module. Previous studies have used multi-layer CNN networks as image encoders, as demonstrated by \cite{slotattention, slate, steve}. However, in our experiments with Latent Diffusion Models, we have found that such encoders tend to produce low-level features that mainly represent pixel position or color information. This leads to the slot attention module grouping pixels based on these low-level features, rather than on high-level object information. We hypothesize that this problem may arise from the early stages of training, where the model lacks a semantic understanding of the input image. This results in the LSD Decoder relying only on low-level and local information from slots to denoise the noisy input, without encouraging the object-centric encoder to learn semantic-level pixel grouping. As a result, without additional guidance, the model may become trapped in a suboptimal solution. We have observed this issue only when a Diffusion Decoder is used.

To overcome this issue, we have incorporated a UNet architecture \cite{unet} as the image encoder in the object-centric encoder. The main advantage of this approach is that the auto-encoding structure of UNet allows high-level global context information to be blended into the output features and slots, facilitating the learning process and helping the model escape from suboptimal pixel groupings. Additionally, the skip connections in UNet allow the output features to retain fine-grained low-level details from early CNN layers, ensuring that the resulting representations are rich in both high-level object information and low-level texture information. These improvements to the object-centric encoder result in more accurate object-centric representations and enable better image generation results. We provide detailed design information in Table \ref{tab:hyperparams}. Prior to applying the UNet structure, we first employ a single CNN layer with a kernel size of 4 and stride of 4 to downsample the image from $256 \times 256 \times 3$ to $64 \times 64 \times S$, where $S$ is the base channel size in UNet. In Appendix \ref{sec:appx_unet_ablation}, we provide an additional ablation study for the UNet encoder.

\textbf{Slot-Conditioned Cross Attention.}
We implement the conditioning mechanism between the object slots and decoder features through a transformer network as in LDM \cite{ldm} and SLATE \cite{slate}. This conditioning mechanism consists of two transformer layers, including a self-attention layer that computes self-attention within the features generated by the UNet in the LSD Decoder, followed by a cross-attention layer that computes cross-attention between the object slots and UNet features. 

In the cross-attention layer, we use the object slots $\bS$ as key-value pairs, and the queries are intermediate layers of the diffusion network augmented with positional embedding  $(\tilde{\bh}_{l} + \mathbf{p}_l)$. We then apply multi-head cross attention for each head as $\text{Attention}(\bQ, \bK, \bV) = \texttt{softmax}\left(\frac{\bQ \bK^T}{\sqrt{d}}\right) \cdot V$, where

\begin{equation*}
\bQ = \bW^{(i)}_Q \cdot (\tilde{\bh}_{l} + \mathbf{p}_l), \; \bK = \bW^{(i)}_K \cdot \bS,
  \; \bV = \bW^{(i)}_V \cdot \bS . \nonumber
\end{equation*}

Here, $i$ is the head index, $\tilde{\bh}_{l}$ represents the intermediate layers of the diffusion network, $\mathbf{p}_l$ is the positional embedding, $\bS$ is the set of object slots computed from the object-centric encoder, and $\bW^{(i)}_V$, $\bW^{(i)}_Q$, $\bW^{(i)}_K$ are learnable projection matrices.

\textbf{Additional Details for Object-Centric Encoder.}
For all multi-object datasets, we set the number of slots to be the maximum number of objects per image plus one (intended for background). For the FFHQ dataset, we segment the image into 4 slots. We use the $256 \times 256$ resolution images for all experiments and, if necessary, we use bilinear interpolation for re-scaling and center cropping to get square images. We do not introduce additional data augmentations during pre-processing. When computing the object segmentation, we use the attention map from the slot attention as the mask prediction. The resolution of the mask is designed to be $64 \times 64$ for all models.

\textbf{Additional Details for LSD Decoder.}
We add learnable positional embeddings into the self and cross-attention layers of UNet, which are found to greatly improve the stability of the training process for the CLEVR and CLEVRTex datasets. However, we have observed that this modification has a minor impact on the MOVi-C/E and FFHQ datasets, but have applied it across all models to maintain consistency. For the image auto-encoder, we use the KL-8 version provided in the LDM repository. During the generation processes for all tasks, we employ the DDIM sampler with $\eta=1$ and run it for 200 steps.

\textbf{Baseline Implementation.}
Our baseline implementations are closely based on the official releases. For the SLATE baseline, we adopt the improved version proposed in \cite{steve} \footnote{\url{https://github.com/singhgautam/steve}}. We empirically find this version to be more robust to complex scenes in terms of both object segmentation and generation quality.  The key difference of this version from the original SLATE is in the slot encoder. The original SLATE compute object slots from the latent space of the dVAE. Therefore, the attention resolution and information granularity of the slots are constrained by the dVAE latents. The improved SLATE addresses this issue by using a jointly trained CNN backbone to directly learn the slots from the original image. As shown in \cite{steve}, this improved version of SLATE achieves improved performance on complex datasets such as MOVi-C and MOVi-E. For SLATE$^+$, we replace the jointly trained dVAE with a VQGAN model pre-trained on OpenImages \footnote{We use the pre-trained weights from https://ommer-lab.com/files/latent-diffusion/vq-f8.zip}.

\section{Additional Experiments}

\subsection{Ablation on CNN backbone in Object-Centric Encoder}
\label{sec:appx_unet_ablation}

\begin{table*}[h!]
\begin{small}
    \caption{\textbf{Ablation Study on CNN backbone.} This table presents the results of an ablation study on the CNN backbone in the object-centric encoder. Specifically, we compare the performance of the LSD method that uses a UNet backbone to the performance of LSD-CNN, which utilizes a multi-layer Convnet backbone.
    }
    % \vskip 0.1in

    % CLEVRTex
    \begin{subtable}{1.0\linewidth}
    \caption{CLEVRTex}
    \vspace{-1mm}
    \centering
    \begin{tabular}{lcc}
    \toprule
    \scriptsize{\textbf{Segmentation}}    & LSD-CNN    & LSD \\
    \midrule
    mBO ($\uparrow$)          & 55.61 & \textbf{66.56}  \\
    mIoU ($\uparrow$)         & 53.82 & \textbf{65.02}  \\
    FG-ARI ($\uparrow$)       & 43.88 & \textbf{61.74}  \\
    \bottomrule
    \end{tabular}
        \begin{tabular}{lcc}
    \toprule
    \scriptsize{\textbf{Representation}}     & LSD-CNN    & LSD\\
    \midrule
    Shape ($\uparrow$)        & 75.00 & \textbf{81.60}  \\
    Material ($\uparrow$)    & \textbf{84.43} & 77.77   \\
    Position ($\downarrow$)    & 1.50 & \textbf{1.10}   \\
    \bottomrule
    \end{tabular}
    \vspace{2mm}
    \end{subtable}


    % MOVi-C
    \begin{subtable}{1.0\linewidth}
    \caption{MOVi-C}
    \vspace{-1mm}
    \centering
    \begin{tabular}{lcc}
    \toprule
    \scriptsize{\textbf{Segmentation}}       & LSD-CNN    & LSD \\
    \midrule
    mBO ($\uparrow$)           & 27.43 & \textbf{46.29}  \\
    mIoU ($\uparrow$)          & 26.31 & \textbf{44.99}  \\
    FG-ARI ($\uparrow$)        & 42.83 & \textbf{50.53} \\
    \bottomrule
    \end{tabular}
        \begin{tabular}{lcc}
    \toprule
    \scriptsize{\textbf{Representation}}     & LSD-CNN    & LSD  \\
    \midrule
    Position ($\downarrow$)     & 1.25 & \textbf{1.14}   \\
    3D B-Box ($\downarrow$)     & 1.46 & \textbf{1.44}   \\
    Category ($\uparrow$)      & 41.29 & \textbf{46.71}   \\
    \bottomrule
    \end{tabular}
    \vspace{2mm}    
    \end{subtable}


    % MOVi-E
    \begin{subtable}{1.0\linewidth}
    \caption{MOVi-E}
    \vspace{-1mm}
    \centering
    \begin{tabular}{lcc}
    \toprule
    \scriptsize{\textbf{Segmentation}}      & LSD-CNN    & LSD   \\
    \midrule
    mBO ($\uparrow$)          & 22.00 & \textbf{39.63}  \\
    mIoU ($\uparrow$)         & 20.71 & \textbf{38.28}  \\
    FG-ARI ($\uparrow$)       & 42.14 & \textbf{53.40}  \\
    \bottomrule
    \end{tabular}
        \begin{tabular}{lcc}
    \toprule
    \scriptsize{\textbf{Representation}}   & LSD-CNN    & LSD  \\
    \midrule
    Position ($\downarrow$)     & 2.01 & \textbf{1.92}   \\
    3D  B-Box ($\downarrow$)    & 3.33 & \textbf{2.94}    \\
    Category ($\uparrow$)      & 35.82 & \textbf{43.15}   \\
    \bottomrule
    \end{tabular}
    \end{subtable}

    \label{tab:ablate_cnn} 
    % \vspace{-1em}
\end{small}

\end{table*}

This section presents an ablation study on the CNN backbone used in the object-centric encoder. To investigate the impact of the CNN backbone choice, we introduce a variant of LSD named LSD-CNN. Specifically, LSD-CNN uses a multi-layer CNN network as the image encoder in the object-centric encoder, following the same CNN design as in the improved SLATE model \cite{steve}. We compare the results of this variant with the proposed LSD, which utilizes the UNet structure as the image encoder in the object-centric encoder, on CLEVRTex, MOVi-C, and MOVi-E. The results are presented in Table \ref{tab:ablate_cnn}. Overall, we observe that LSD outperforms LSD-CNN across nearly all tasks and datasets, suggesting that the UNet CNN backbone is critical for achieving the high performance of our model.


\section{Additional Image Samples}

We include additional generation samples in Figures \ref{fig:appx_component} - \ref{fig:appx_object_insertion}. The additional samples demonstrate that LSD achieves more accurate object segmentation and generates more detailed and coherent scenes compared to the baselines. We provide additional observations and insights in the following sections.

\subsection{Visualization of Visual Concept Prompts}

In this section, we present visualizations of the visual concept prompts employed for generating image samples in the CLEVRTex and FFHQ datasets, as illustrated in Figure~\ref{fig:appx_component}. To generate these visualizations, we apply the attention map corresponding to each specific slot as a mask over the original image, which highlights the regions captured by that slot. We observe that the LSD model can seamlessly combine components from distinct images to create a coherent image sample while preserving the attributes of each component in the final image. Note that each concept library is associated with one k-means cluster within the extracted slots, and each prompt displayed in Figure~\ref{fig:appx_component} is sampled from one such library. The visualization of the prompts suggests that the concept libraries is able to capture practical concept categories, such as objects and backgrounds in the CLEVRTex dataset and facial attributes, hairstyles, clothing, and backgrounds in the FFHQ dataset.

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=1.\textwidth]{fig/component_composition.pdf}
% \vskip -0.1in
\caption{\textbf{Compositional Image Generation with Concept Prompts.} In this visualization, we show a concept prompt constructed by composing arbitrary slots from our visual concept library and the corresponding generated image by LSD.}
\label{fig:appx_component}
\end{figure*}
% ######################################################################## 

\subsection{Impact of Pre-Trained Auto-Encoder on Generation Quality}
We investigate the effect of the pre-trained image auto-encoder in the comparison between SLATE and SLATE$^+$ in Figure \ref{fig:appx_compsitional_generation} and \ref{fig:appx_face_replacement}. Our results show that the use of the pre-trained auto-encoder leads to less blurry images and significantly more details, such as the floor and object texture in CLEVRTex and face detail in FFHQ. However, we also observe that applying the pre-trained auto-encoder does not necessarily improve the coherence of the generated scenes. For instance, in some SLATE$^+$ samples, the shape of the objects appear distorted in CLEVRTex and the background and hair style of FFHQ images are rendered as color patches. These findings indicate the limitation of the transformer decoders in SLATE and SLATE$^+$ models and highlights the crucial role of LSD's diffusion decoder in achieving optimal generation quality.

\subsection{Impact of Segmentation Quality on Slot-Based Editing}
In Figure \ref{fig:appx_object_removing_1} and \ref{fig:appx_object_removing_2}, we investigate the effect of object segmentation on the slot-based editing task. Our result show that when the object segmentation is almost perfect, editing tasks such as object removing can be easily accomplished by discarding the corresponding slot during image decoding. It is important to note that the object segmentation masks are derived from the attention weights on image features. Therefore, a perfect segmentation mask indicates that the object information is completely assigned to a single slot. However, we also observe cases when the object assignments are incomplete, such as when one object is divided into multiple slots. In such cases, removing an object would require removing all associated slots. These observations emphasize the importance of accurate object segmentation in achieving effective slot-based editing. Although LSD has demonstrated superior performance compared to existing approaches, further exploration in object segmentation is necessary to extend it to real-world applications.

% While LSD is proven to outperform all existing works in this task, further exploration is required to extend it to real-world applications.

% ########################################################################
\begin{figure*}[b]
\centering
\includegraphics[width=1.\linewidth]{fig/appx_segmentation.pdf}
\caption{\textbf{Visualization of Unsupervised Object Segmentation.} LSD achieves more accuracy object segmentation when comparing to the other methods.
}
\label{fig:appx_segmentation}
\end{figure*}
% ########################################################################

% ########################################################################
\begin{figure*}[b]
\centering
\includegraphics[width=1.\linewidth]{fig/appx_compsitional_generation.pdf}
\caption{\textbf{Compositional Generation Samples using Visual Concept Library.} LSD provides significantly higher fidelity and more clear details when comparing to the other methods.
}
\label{fig:appx_compsitional_generation}
\end{figure*}
% ########################################################################

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=1.\linewidth]{fig/appx_face_replacement.pdf}
\caption{\textbf{Slot-Based Image Editing: Face Replacement.} We show face replacement in the FFHQ dataset, where we compose new images by combining the face slots from Source-B images with the hairstyle, clothing, and background slots from Source-A images. 
}
\label{fig:appx_face_replacement}
\end{figure*}
% ########################################################################

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=1.\linewidth]{fig/appx_object_removing_1.pdf}
\caption{\textbf{Slot-Based Image Editing: Object Removal and Segmentation Mask. Sample 1.} \textit{Top:} We show object removal by discarding the corresponding slots. \textit{Bottom:} We show the segmentation masks of each slot.
}
\label{fig:appx_object_removing_1}
\end{figure*}
% ########################################################################

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=1.\linewidth]{fig/appx_object_removing_2.pdf}
\caption{\textbf{Slot-Based Image Editing: Object Removal and Segmentation Mask. Sample 2.} 
}
\label{fig:appx_object_removing_2}
\end{figure*}
% ########################################################################

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=1.\linewidth]{fig/appx_bg_swap.pdf}
\caption{\textbf{Slot-Based Image Editing: Background Replacement.} The background replacement task is performed by replacing the background-associated slots.
}
\label{fig:appx_bg_swap}
\end{figure*}
% ########################################################################

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=1.\linewidth]{fig/appx_object_insertion.pdf}
\caption{\textbf{Slot-Based Image Editing: Object Insertion.} 
The background replacement task is performed by simply adding the new slot to the existing set of slots.
}
\label{fig:appx_object_insertion}
\end{figure*}
% ########################################################################


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}