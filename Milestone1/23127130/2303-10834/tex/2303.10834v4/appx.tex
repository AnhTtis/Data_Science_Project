\section{Limitations and Broader Impact}

\textbf{Limitations}
LSD  faces several limitations common to unsupervised object-centric representation learning. First, it struggles with part-whole ambiguity, meaning it has trouble distinguishing between individual parts of an object and the whole object itself, especially when objects have complex textures in real-world images. Second, the quality and level of detail in LSD's segmentation results are sensitive to the number of slots it uses. If there are too few slots compared to the actual objects in an image, the model tends to produce generalized masks instead of distinct object segments. Conversely, when there are too many slots, the model tends to over-segment the image, splitting objects into multiple parts. This sensitivity to slot numbers is a problem in scenarios where we don't know the exact number of objects in advance. Finally, even with a strong Diffusion decoder, LSD struggles when dealing with real-world images that have highly diverse object appearances. To address this final limitation, we conduct a preliminary exploration in Appendix \ref{sec:appx_pretrained_dm}, where we propose two LSD variants and demonstrate their effectiveness in handling real-world data.

\textbf{Broader Impact}
The proposed Latent Slot Diffusion (LSD) model is a generative model for object-centric representation learning. The main potential negative social impacts is associated with its application in image editing and novel image synthesis. By enabling the extraction of object-centric or component-based representations from images, LSD allows for the generation of entirely new scenes with previously unseen configurations. This capability raises concerns related to image manipulation and privacy. For example, the capability to synthesize new human faces introduces potential privacy issues like deepfake generation and identity theft. Moreover, this ability could potentially cause digital misinformation, as it can generate and manipulate images with high accuracy in object-based level, making it increasingly difficult to distinguish between real and fake or edited visual content. Additionally, the image composition task gives rise to concerns about ethnic bias, ageism, or other forms of misrepresentation. This is due to the uniform sampling process of the learned slots within the same cluster and their independence across clusters. This can result in two significant problems: (1) the sampled distribution may reflect the biases present in the collected data, potentially leading to ethnic, age, or other population biases, and (2) when performing slot swapping for applications such as face replacement, mismatches in gender, age, and skin tone between existing parts and introduced parts may occur. Therefore, further application of this method should be proceeded under strict ethical guidelines and regulation.



\section{LSD on Simple Images} \label{sec:appx_clevr}

In the paper, we demonstrate that LSD achieves considerable performance improvements for complex images. However, our early experiments reveal that when applied to visually simplistic datasets like CLEVR, LSD exhibits reduced performance in terms of segmentation and representation quality, as illustrated in Table \ref{tab:appx_clevr}. To gain further insights into these results, we provide visualizations of the segmentation outputs in Figure \ref{fig:appx_clevr_seg}.

As illustrated by the LSD samples in Figure \ref{fig:appx_clevr_seg}, certain parts of the background, such as the upper regions of the images, are split and assigned to multiple object slots. This behavior leads to information mixing between objects and the background, which substantially reduces the performance of slot segmentation and representation. We conjecture that this issue may stem from overfitting in the model. Due to the simplicity of the texture and the absence of objects in these areas, the powerful decoder of LSD can easily memorize the rendering process of those regions independently of the slot-conditioning. As a result, it does not provide a sufficient training signal for the encoder to group those background parts into a meaningful slot.

In light of these findings, we tested a straightforward approach to address this problem. We propose introducing additional complexity to the background by incorporating data samples from the CLEVRTex dataset into the training process. By integrating the CLEVRTex samples, we introduce 60 different background textures, which makes implicit memorization difficult. This effectively provides the learning signal to capture the background component into the slot-conditioning. The results of this approach, labeled as LSD(Mix), can be seen in Figure \ref{fig:appx_clevr_seg}. As clearly illustrated, including CLEVRTex data samples in the training results in cleaner background segmentation and alleviates the information mixing issue. As a result, LSD(Mix) achieves approximately 22\% gains of improvement in both mIoU and mBO compared to the original model trained solely on CLEVR samples, as shown in Table \ref{tab:appx_clevr}. However, we also see that the FID score indicated a decrease in the generation quality of LSD(Mix) compared to the original LSD, likely due to the increased demand of model capacity to model both datasets. Overall, this experiment confirms that introducing complexity into the training dataset can mitigate the overfitting problem and allow LSD to effectively perform representation learning on simple datasets.

\begin{table}[b]
    \centering
    \caption{
    \textbf{Quantitative Evaluation on CLEVR dataset.} We evalute the models by reporting mBO, mIoU and FG-ARI scores for segmentation quality, property prediction scores for representation quality, and FID scores for compositional generation quality.
    }
    \begin{tabular}{lcccc}
    \toprule
                     & SLATE        & SLATE$^+$       & LSD & LSD (Mix) \\
    \midrule
    mBO ($\uparrow$)         & 64.86 & \textbf{67.42}  & 38.49 & 61.09 \\
    mIoU ($\uparrow$)        & 63.96 & \textbf{66.62}  & 37.49 & 59.76 \\
    FG-ARI ($\uparrow$)      & 69.20 & \textbf{88.56}  & 76.32 & 76.30 \\
    Shape ($\uparrow$)       & 95.66 & \textbf{95.70}  & 75.16 & 80.48 \\
    Material ($\uparrow$)    & 97.62 & \textbf{97.96}  & 94.21 & 96.24 \\
    Position ($\downarrow$)  & 0.59  & \textbf{0.51}   & 0.86  & 0.99 \\
    FID ($\downarrow$)       & 52.96  & 20.93   & \textbf{16.22}  & 19.45 \\
    \bottomrule
    \end{tabular}
    \label{tab:appx_clevr}

\end{table}

\begin{figure*}[t]
\centering
\includegraphics[width=1.\textwidth]{fig/appx_clevr_segmentation.png}
\caption{\textbf{Visualization of Unsupervised Object Segmentation on CLEVR dataset.} The LSD(Mix) variant corresponds to the model trained on both CLEVR and CLEVRTex training images.
}
\label{fig:appx_clevr_seg}
\end{figure*}

\section{LSD with Pre-Trained Diffusion Models} 
\label{sec:appx_pretrained_dm}
One of the main focuses of recent research in diffusion models have been on adapting pre-trained models for additional control signals \cite{gligen, t2iadapter, controlnet}. Specifically, these studies propose the integration of a learnable adapter network into large-scale pre-trained diffusion models, such as Stable Diffusion, to enable controllable generation based on various types of conditioning such as sketches, semantic layouts, and depth maps. An inherent advantage of these approaches is their ability to bypass the resource-intensive processes of training the entire generative model, while still enabling the production of high-quality controllable images through minimal training of the adapter network.


We note that such a model design can also be utilized for slot-based conditioning. However, unlike existing works that primarily focus on learning the adapter to integrate the provided conditioning into the pre-trained diffusion model, our focus lies in training an adapter to achieve two objectives simultaneously: (1) we aim to learn how to extract object representations (slots) from input images in an unsupervised manner, and (2) we aim to learn how to incorporate these slot representations into the pre-trained diffusion model to facilitate slot-based conditional generation. This leads to an intriguing question: can learning through such a model, which has not been specifically trained on multi-object scenarios, provide the necessary learning signal to acquire object-centric representations? In this section, we provide a preliminary exploration on this question by introducing two LSD variants: Control-LSD and Latent Slot Stable Diffusion (LSSD).

\subsection{Control-LSD}

\textbf{Model Design} 
Control-LSD utilizes a similar design of the adapter structure proposed in ControlNet \cite{controlnet} and consists of three main components: (1) a pre-trained Stable Diffusion model that is frozen throughout the training, (2) a "control block" that is a trainable copy of the unet encoder from Stable Diffusion, and (3) a object-centric encoder from the LSD model. Similar to LSD, the object-centric encoder takes an image as input and generates a set of slots, which are then conditioned on using a Slot-Conditioned Transformer. However, instead of providing the slots to the diffusion decoder, Control-LSD feeds the slots into the trainable control block while solely employing null text conditioning in the Stable Diffusion. To integrate the slot control signal, the outputs of each intermediate control block are element-wise added to the intermediate blocks and skip-connections within the frozen Stable Diffusion model. Following LSD, we use a input resolution of 256 $\times$ 256 and attention map resolution of 64 $\times$ 64 for all datasets. We also use the same number of slots as in LSD for multi-object dataset, and 10 slots for the COCO dataset. 

\begin{figure*}[t]
\centering
\includegraphics[width=1.\textwidth]{fig/appx_control_lsd.pdf}
\caption{\textbf{Qualitative Results of Control-LSD on CLEVR, CLEVRTex, MOVi-E, and COCO.} The CLEVR(Mix) corresponds to the model trained on both CLEVR and CLEVRTex samples.
}
\label{fig:control_lsd}
\end{figure*}

\textbf{Results}
We evaluate Control-LSD on three multi-object datasets: CLEVR, CLEVRTex, and MOVi-E, as well as one real-world dataset, COCO \cite{coco}. Following the proposed LSD, we also include a CLEVR(Mix) variant for the CLEVR dataset which was trained on both CLEVR and CLEVRTex samples. The segmentation results are presented in Figure \ref{fig:control_lsd}. We can see that Control-LSD struggles to achieve satisfactory object segmentation on the CLEVR dataset. The model has difficulties in distinguishing between the background and the objects. We also note that the model demonstrate more reasonable results on CLVERTex and introducing samples from CLEVRTex to the training process of CLEVR (i.e., CLEVR(Mix)) also led to an improvement on CLEVR, but the results are still inferior to those achieved by LSD. Interestingly, we also observe that Control-LSD achieve better segmentation quality when applied to more realistic scenes, such as those in the MOVi-E dataset. We conjecture that this is attributed to the fact that the complexity of MOVi-E aligns more closely with real-world images used for pre-training the Stable Diffusion model, as compared to the samples from CLEVR and CLEVRTex datasets. This indicates that the distribution gap between the data used for pre-training Stable Diffusion and the multi-object datasets used for learning object-centric representation might be a potential contributing factor to the lower performance of Control-LSD on the multi-object datasets.

Finally, we also evaluate Control-LSD on a real-world multi-object dataset COCO \cite{coco}. Notably, Control-LSD showcases its ability to capture semantically meaningful entities in the slots, as illustrated in the segmentation results in Figure \ref{fig:control_lsd}. However, we find it challenging to obtain accurate object segmentations aligned with COCO annotations. In this regard, we would like to share two key observations when applying the model on real-world scenes: (1) Without human annotations, unsupervised models have difficulty distinguishing between discrete parts of an object and the entire object. This is known as the part-whole ambiguity. For instance, while COCO annotates an entire human as one segment, Control-LSD might segment it into two distinct parts: the upper body and the legs. It is important to emphasize that neither of the two segmentation results are incorrect, as each could be beneficial for specific downstream tasks. (2) The segmentation granularity is sensitive to the number of slots. This sensitivity can be attributed to the model's tendency to always utilize the entire set of slots to segment the image. Consequently, if the slot count is less than the number of entities in an image, the model tends to produce semantic masks rather than object-level segments. Conversely, if the slots outnumber the entities, the model tends to over-segment the image, i.e., splitting some of the entity into multiple slots. This behavior limits the applicability of Control-LSD in real-world data, where the true number of objects is not known to the model. Therefore, we believe further investigation is needed for applying the model to real-world applications.

\subsection{Latent Slot Stable Diffusion (LSSD)}


While Control-LSD bypasses the need to train a Diffusion decoder, the computational cost of training the adapter network—which includes both the control block and the object encoder—is still substantial. As an illustration, the memory demands are so significant that even with four 48GB GPUs, our experiments could only handle a training batch size of 32, which also leads to significant training latency. To address this problem, we introduce the the second LSD variant which further reduce trainable modules, we name it Latent Slot Stable Diffusion (LSSD). We show that despite its simple architecture, LSSD demonstrate strong performance on real-world scenarios. We will start with describing the model design. \footnote{The LSSD implementation will be available in the official release: \url{https://github.com/JindongJiang/latent-slot-diffusion}}

\textbf{Model Design}
Firstly, following the DINOSAUR model \citep{dinosaur}, we employ a pre-trained image encoder, DINOv2 \citep{dinov2}, as the CNN backbone for the object encoder. This encoder remains frozen during training. It is important to emphasize that unlike DINOSAUR, LSSD is a generative model capable of generating images. Secondly, the learned slots are provided directly to the cross attention layers of pre-trained DMs as special text tokens. This means that the slot attention module acts dual purposes — as an object encoder extracting object-centric representations and as a feature adapter converting input tokens from the image to the text domain. Notably, within the LSSD framework, only the slot attention module requires training. The resulting design significantly reduces the computational demands observed with Control-LSD. In our experiment, we were able to train the model with a batch size of 128 using two 48GB GPUs. 

\textbf{Real-World Segmentation Results}
Despite its simplicity, LSSD demonstrate its effectiveness in real-world object-centric learning. To evaluate its object-centric learning ability, we compare LSSD with DINOSAUR on instance-level FG-ARI and mBO. The result can be find in Table \ref{tab:appx_lssd}. The results show that LSSD achieves comparative performance to DINOSAUR with relatively superior FG-ARI value and a diminished mBO value. We show the mask visualization on Figure \ref{fig:appx_lssd_seg_cfg}.

\begin{table}[t]
    \centering
    \caption{
    \textbf{Quantitative Evaluation of LSSD on COCO dataset.} We compare LSSD with DINOSAUR \citep{dinosaur} on instance-level FG-ARI and mBO. The results of DINOSAUR is copied from the original paper.
    }
    \begin{tabular}{lcccc}
    \toprule
                             & LSSD (Ours)        & DINOSAUR    \\
    \midrule
    mBO ($\uparrow$)         & 30.38 & \textbf{31.60}   \\
    FG-ARI ($\uparrow$)      & \textbf{35.02} & 34.10   \\
    \bottomrule
    \end{tabular}
    \label{tab:appx_lssd} 

\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.\textwidth]{fig/appx_lssd_coco_seg_cfg}
    \caption{\textbf{Unsupervised Image Segmentation and Generation Results of LSSD on COCO dataset.} For image generation, we apply different cfg values on the learned slots with $\text{cfg}=1.0$ equivalent to image reconstruction.}
    \label{fig:appx_lssd_seg_cfg}
\end{figure}

\textbf{Real-World Image Generation Results}
We further evaluate the image generation quality of LSSD. In Figure \ref{fig:appx_lssd_seg_cfg}, we show the image reconstruction results (denoted by $\text{cfg}=1.0$). The results demonstrate that the model output suffers from visual artifacts which constrained both the image reconstruction and generation capabilities. To further improve the generation results, we explore the technique of classifier-free guidance which is widely employed in text-to-image diffusion models. 


Classifier-free guidance \citep{cfg} uses a mixing weight (abbreviated as cfg) to control the weighted sum of noise predictions between a conditional and an unconditional diffusion model. With $cfg=1$, it operates similarly to standard conditional generation. When $cfg>1$, the model generates images that are more align to the text input, but comes with a cost of reduced image diversity. Practically speaking, an optimal $cfg>1$ can yield images that not only better match the conditions but also exhibit enhanced quality. Since LSSD provides object slots as text tokens to the pre-trained DMs, classifier-free guidance can be applied directly to the input slot, akin to how it's applied to input text in standard DMs, without any model modifications. And our experiment shows that a $cfg>1$ for slots can also significantly improve the image generation quality. 

In Figure \ref{fig:appx_lssd_seg_cfg}, we tested various cfg values with LSSD to optimize image generation quality. As we can see in the figure, from $\text{cfg}=1.0$ to 1.3, the images appear cleaner with less visual artifacts, delivering conceptually more consistent images with the input slots. Nevertheless, we also observe that higher cfg values strip away intricate visual details, leading to overly simplified images. We settled on a cfg value of 1.3 for following experiments.

In Figure \ref{fig:appx_lssd_coco_samples}, we show image samples generated based on the same set of learned slots. The variance in these samples comes from differing the initial noise maps in the denoising process. These results demonstrate that with a proper cfg value, LSSD achieves slot-based conditional image generation with unconstrained real-world objects, which is for the first time in object-centric generative models. In addition, beyond showing the diversity of the generated images, these results also underscore a crucial point from the representation learning prospective: the learned slots contain rich semantic information rather than merely performing texture-based clustering. An evidence to this is the model's ability to maintain overall semantic consistency while exhibiting diverse presentations of the same semantic content across samples. For instance, even if the background of the stop sign images shows different tree shapes, they all clearly have trees in the background. Achieving this level of consistency would be challenging if the slots lacked an abstract semantic understanding of the objects in the scene. We show additional real-world segmentation and generation results in Figures \ref{fig:appx_lssd_seg_cfg_add_1} - \ref{fig:appx_lssd_coco_samples_add_3}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{fig/appx_lssd_coco_samples}
    \caption{\textbf{Real-World Image Generations.} Each image is conditioned on the same set of learned slots and with a different initial noise map in the denoising process. We use $\text{cfg}=1.3$ for all samples.}
    \label{fig:appx_lssd_coco_samples}
\end{figure}

We believe the prelimilary attempts detailed in this section to be valuable explorations toward large-scale object-centric learning, and we hope our insights pave the way for further research in this area.



\section{Additional Implementation Details}

To ensure reproducibility, we are sharing additional implementation details of LSD in this section. Our implementation will be made available at \url{https://github.com/JindongJiang/latent-slot-diffusion}

\begin{table*}[t]
\begin{scriptsize}
    \caption{\textbf{Full Table of Segmentation Performance and Representation Quality.} Extended results from the main text, now including standard deviation values.
    }
    \vspace{-0.5em}

    % CLEVRTex
    \begin{subtable}{1.0\linewidth}
    \caption{CLEVRTex}
    \vspace{-1mm}
    \centering
    \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Segmentation}}      & SLATE                      & SLATE$^+$                  & LSD (Ours)             \\
    \midrule
    mBO ($\uparrow$)         & 50.88 {\tiny$\pm$ 0.70} & 54.90 {\tiny$\pm$ 1.15} & \textbf{63.93} {\tiny$\pm$ 1.93} \\
    mIoU ($\uparrow$)        & 49.54 {\tiny$\pm$ 0.70} & 52.96 {\tiny$\pm$ 1.40} & \textbf{62.52} {\tiny$\pm$ 1.88} \\
    FG-ARI ($\uparrow$)      & 44.19 {\tiny$\pm$ 1.80} & \textbf{70.71} {\tiny$\pm$ 3.35} & 64.41 {\tiny$\pm$ 8.53} \\
    \bottomrule
    \end{tabular}
    \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Representation}}   & SLATE                      & SLATE$^+$                  & LSD (Ours)             \\
    \midrule
    Shape ($\uparrow$)       & 74.24 {\tiny$\pm$ 2.41} & 71.63 {\tiny$\pm$ 4.94} & \textbf{80.23} {\tiny$\pm$ 1.59} \\
    Material ($\uparrow$)   & 69.73 {\tiny$\pm$ 2.04} & 63.61 {\tiny$\pm$ 2.40} & \textbf{75.56} {\tiny$\pm$ 1.00} \\
    Position ($\downarrow$)  & 1.28 {\tiny$\pm$ 0.10} & 1.26 {\tiny$\pm$ 0.15} & \textbf{1.13} {\tiny$\pm$ 0.04}    \\
    \bottomrule
    \end{tabular}
    \vspace{1mm}
    \end{subtable}


    % MOVi-C
    \begin{subtable}{1.0\linewidth}
    \caption{MOVi-C}
    \vspace{-1mm}
    \centering
    \begin{tabular}{lccc}
    \toprule
    \textbf{Segmentation}          & SLATE        & SLATE$^+$      & LSD (Ours)   \\
    \midrule
    mBO ($\uparrow$)         & 39.37 {\tiny$\pm$ 0.80} & 38.17 {\tiny$\pm$ 1.29}  & \textbf{45.57} {\tiny$\pm$ 0.80} \\
    mIoU ($\uparrow$)        & 37.75 {\tiny$\pm$ 0.71} & 36.44 {\tiny$\pm$ 1.35}  & \textbf{44.19} {\tiny$\pm$ 0.91} \\
    FG-ARI ($\uparrow$)      & 49.54 {\tiny$\pm$ 1.44} & \textbf{52.04} {\tiny$\pm$ 0.45}  & 51.98 {\tiny$\pm$ 3.53} \\
    \bottomrule
    \end{tabular}
    \begin{tabular}{lccc}
    \toprule
    \textbf{Representation}           & SLATE        & SLATE$^+$      & LSD (Ours)   \\
    \midrule
    Position ($\downarrow$)  & 1.37 {\tiny$\pm$ 0.06} & 1.28 {\tiny$\pm$ 0.07}   & \textbf{1.14} {\tiny$\pm$ 0.03}   \\
    3D B-Box ($\downarrow$)   & 1.48 {\tiny$\pm$ 0.03} & \textbf{1.44} {\tiny$\pm$ 0.11}  & \textbf{1.44} {\tiny$\pm$ 0.02}    \\
    Category ($\uparrow$)    & 42.45 {\tiny$\pm$ 0.13} & 45.32 {\tiny$\pm$ 1.13} & \textbf{46.11} {\tiny$\pm$ 0.59} \\
    \bottomrule
    \end{tabular}
    \vspace{1mm}    
    \end{subtable}
    
    % MOVi-E
    \begin{subtable}{1.0\linewidth}
    \caption{MOVi-E}
    \vspace{-1mm}
    \centering
    \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Segmentation}}      & SLATE        & SLATE$^+$      & LSD (Ours)   \\
    \midrule
    mBO ($\uparrow$)         & 30.17 \tiny{$\pm$ 1.70} & 22.17 \tiny{$\pm$ 0.38} & \textbf{38.96} \tiny{$\pm$ 0.47} \\
    mIoU ($\uparrow$)        & 28.59 \tiny{$\pm$ 1.66} & 20.63 \tiny{$\pm$ 0.35} & \textbf{37.64} \tiny{$\pm$ 0.45} \\
    FG-ARI ($\uparrow$)      & 46.06 \tiny{$\pm$ 3.32} & 45.25 \tiny{$\pm$ 0.77} & \textbf{52.17} \tiny{$\pm$ 0.89} \\
    \bottomrule
    \end{tabular}
        \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Representation}}   & SLATE        & SLATE$^+$      & LSD (Ours)   \\
    \midrule
    Position ($\downarrow$)  & 2.09 \tiny{$\pm$ 0.13} & 2.15 \tiny{$\pm$ 0.07} & \textbf{1.85} \tiny{$\pm$ 0.05} \\
    3D  B-Box ($\downarrow$)   & 3.36 \tiny{$\pm$ 0.12} & 3.37 \tiny{$\pm$ 0.27} & \textbf{2.94} \tiny{$\pm$ 0.003} \\
    Category ($\uparrow$)    & 38.93 \tiny{$\pm$ 0.16} & 38.00 \tiny{$\pm$ 0.36} & \textbf{42.96} \tiny{$\pm$ 0.21} \\
    \bottomrule
    \end{tabular}
    \vspace{1mm}    
\end{subtable}


    \label{tab:complex_quant_with_std} 
    \vspace{-2mm}
\end{scriptsize}

\end{table*}

\subsection{Experiment Setup}

\textbf{Datasets.}
In our experiments, we create three data subsets for each dataset: training, validation, and testing. For CLEVR, we utilize the official split for training and validation sets. However, for evaluation, we do not use the official test set as CLEVR does not provide ground-truth object segmentation masks. Instead, we use the "clevr\_with\_mask" subset from the Multi-Object Datasets \cite{multiobjdataset} for evaluation. This subset is designed to have the same object distribution as CLEVR while providing ground-truth masks and attributes annotations for each object in the image, which allows us to evaluate the segmentation and representation quality of each model. We use the full subset as the test set for evaluation. For CLEVRTex, there is no official split provided, so we allocate 80\% of the data for training, 10\% for validation, and 10\% for testing. Regarding MOVi-C and MOVi-E, we use 90\% of the training set data for training and reserve 10\% for validation. We use the official validation set for testing. It is important to note that the test sets in MOVi-C and MOVi-E are designed for out-of-distribution (OOD) evaluation, so we do not use them for test time evaluation. For the FFHQ dataset, we use 86\% of the dataset ($\sim$ 60K images) for training and 7\% ($\sim$ 5K images) for validation. To ensure consistency across all models, we shuffled the dataset and utilized a fixed random seed when splitting the data.

\textbf{Segmentation Metrics.} 
To evaluate the unsupervised object segmentation, we use three metrics: the foreground adjusted rand index (FG-ARI), the mean intersection over union (mIoU), and the mean best overlap (mBO). We predict the object mask by using the argmax along the slot dimension in the attention map. In computing the mIoU, we use Hungarian matching to obtain the ground-truth and slots assignment. Following \cite{dittadi2021generalization}, we use the negative cosine similarity as the matching loss. In mBO, we assign the ground-truth segment to the slot with the largest overlap. Note that the mBO metric is generally simpler than mIoU since it does not require a strict one-to-one mapping between the ground-truth and predicted masks.

\subsection{Implementation Details} \label{sec:appx_implementation}

\begin{table}[h!]
\centering
\begin{tabular}{@{}llcccc@{}}
\toprule
               &               & \multicolumn{4}{c}{Dataset}                          \\ \cmidrule(l){3-6} 
Module      & Hyperparameter     & CLEVR       & CLEVRTex    & MOVi-C/E    & FFHQ                   \\ \midrule
General     &   Batch Size       & 64        & 64             & 128     & 128                 \\
            &   Training Steps   & 200K & 200K & 200K & 200K \\
            &   \# K-means Clusters & 5 & 5 & 18 & 18 \\
    \midrule
CNN Backbone            &  Input Resolution &     \multicolumn{4}{c}{256}    \\
                        &  Output Resolution &     \multicolumn{4}{c}{64}    \\
                        &  Self Attention  &     \multicolumn{4}{c}{Middle Layer}    \\
                        &  Base Channels &     \multicolumn{4}{c}{128}    \\
                        &  Channel Multipliers &     \multicolumn{4}{c}{[1,1,2,4]}    \\
                        &  \# Heads &     \multicolumn{4}{c}{8}    \\
                        &  \# Res Blocks / Layers &     \multicolumn{4}{c}{2}    \\
                        &  Output Channels &   128    &   128  & 192  & 192     \\
                        &  Learning Rate  &     0.0001 &     0.00003     &     0.0001  & 0.0001       \\ \midrule
Slot Attention          &  Input Resolution &     \multicolumn{4}{c}{64}    \\
                        &  \# Iterations  &     \multicolumn{4}{c}{3}    \\
                        &  Slot Size  &     128      &     128   &    192  &    192   \\
                        &  \# Slots  &  11   &  11  &    24 &    4       \\
                        &  Learning Rate  &     0.0001  & 0.00003    &     0.0001  & 0.0001       \\ \midrule
Auto-Encoder            &  Model &     \multicolumn{4}{c}{KL-8} \\
                        &  Input Resolution &     \multicolumn{4}{c}{256} \\
                        &  Output Resolution &     \multicolumn{4}{c}{32} \\
                        &  Output Channels &     \multicolumn{4}{c}{4} \\ \midrule
LSD Decoder       &  Input Resolution &     \multicolumn{4}{c}{32} \\
                        &  Input Channels &     \multicolumn{4}{c}{4} \\
                        &  $\beta$ scheduler   &     \multicolumn{4}{c}{Linear} \\
                        &  Mid Layer Attention   &     \multicolumn{4}{c}{Yes} \\
                        &  \# Res Blocks / Layers &     \multicolumn{4}{c}{2}    \\
                        &  Image Latent Scaling  &     \multicolumn{4}{c}{0.18215}    \\
                        &  Learning Rate   &     \multicolumn{4}{c}{0.0001}    \\
                        &  \# Heads & 4 & 4 & 8 & 8  \\
                        &  Base Channels & 128  & 128  & 192 & 192 \\
                        &  Attention Resolution &     [1,2,4]      &     [1,2,4]   &    [1,2,4,4] &    [1,2,4,8]   \\
                        &  Channel Multipliers & [1,2,4] & [1,2,4] & [1,2,4,4] & [1,2,4,4]  \\ \midrule

\end{tabular}
\vspace{2mm}
\caption{Hyperparameters of our model used in our experiments.}
\label{tab:hyperparams}
\end{table}

We will provide an overview of the implementation details in this section. The hyperparameters used in our approach are listed in Table \ref{tab:hyperparams}.

\textbf{Auto-Encoding CNN backbone.}
The object-centric encoder in our model includes a CNN backbone, which transforms the raw image input into feature vectors that can be processed by the slot attention module. Previous studies have used multi-layer CNN networks as image encoders, as demonstrated by \citep{slotattention, slate, steve}. However, in our experiments with Latent Diffusion Models, we have found that such encoders tend to produce low-level features that mainly represent pixel position or color information. This leads to the slot attention module grouping pixels based on these low-level features, rather than on high-level object information. We hypothesize that this problem may arise from the early stages of training, where the model lacks a semantic understanding of the input image. This results in the LSD Decoder relying only on low-level and local information from slots to denoise the noisy input, without encouraging the object-centric encoder to learn semantic-level pixel grouping. As a result, without additional guidance, the model may become trapped in a suboptimal solution. We have observed this issue only when a Diffusion Decoder is used.

To overcome this issue, we have incorporated a UNet architecture \cite{unet} as the image encoder in the object-centric encoder. The main advantage of this approach is that the auto-encoding structure of UNet allows high-level global context information to be blended into the output features and slots, facilitating the learning process and helping the model escape from suboptimal pixel groupings. Additionally, the skip connections in UNet allow the output features to retain fine-grained low-level details from early CNN layers, ensuring that the resulting representations are rich in both high-level object information and low-level texture information. These improvements to the object-centric encoder result in more accurate object-centric representations and enable better image generation results. We provide detailed design information in Table \ref{tab:hyperparams}. Prior to applying the UNet structure, we first employ a single CNN layer with a kernel size of 4 and stride of 4 to downsample the image from $256 \times 256 \times 3$ to $64 \times 64 \times S$, where $S$ is the base channel size in UNet. In Appendix \ref{sec:appx_unet_ablation}, we provide an additional ablation study for the UNet encoder.

\textbf{Slot-Conditioned Cross Attention.}
We implement the conditioning mechanism between the object slots and decoder features through a transformer network as in LDM \cite{ldm} and SLATE \cite{slate}. This conditioning mechanism consists of two transformer layers, including a self-attention layer that computes self-attention within the features generated by the UNet in the LSD Decoder, followed by a cross-attention layer that computes cross-attention between the object slots and UNet features. 

In the cross-attention layer, we use the object slots $\bS$ as key-value pairs, and the queries are intermediate layers of the diffusion network augmented with positional embedding  $(\tilde{\bh}_{l} + \mathbf{p}_l)$. We then apply multi-head cross attention for each head as $\text{Attention}(\bQ, \bK, \bV) = \texttt{softmax}\left(\frac{\bQ \bK^T}{\sqrt{d}}\right) \cdot V$, where

\begin{equation*}
\bQ = \bW^{(i)}_Q \cdot (\tilde{\bh}_{l} + \mathbf{p}_l), \; \bK = \bW^{(i)}_K \cdot \bS,
  \; \bV = \bW^{(i)}_V \cdot \bS . \nonumber
\end{equation*}

Here, $i$ is the head index, $\tilde{\bh}_{l}$ represents the intermediate layers of the diffusion network, $\mathbf{p}_l$ is the positional embedding, $\bS$ is the set of object slots computed from the object-centric encoder, and $\bW^{(i)}_V$, $\bW^{(i)}_Q$, $\bW^{(i)}_K$ are learnable projection matrices.

\textbf{Additional Details for Object-Centric Encoder.}
For all multi-object datasets, we set the number of slots to be the maximum number of objects per image plus one (intended for background). For the FFHQ dataset, we segment the image into 4 slots. We use the $256 \times 256$ resolution images for all experiments and, if necessary, we use bilinear interpolation for re-scaling and center cropping to get square images. We do not introduce additional data augmentations during pre-processing. When computing the object segmentation, we use the attention map from the slot attention as the mask prediction. The resolution of the mask is designed to be $64 \times 64$ for all models.

\textbf{Additional Details for LSD Decoder.}
We add learnable positional embeddings into the self and cross-attention layers of UNet, which are found to greatly improve the stability of the training process for the CLEVR and CLEVRTex datasets. However, we have observed that this modification has a minor impact on the MOVi-C/E and FFHQ datasets, but have applied it across all models to maintain consistency. For the image auto-encoder, we use the KL-8 version provided in the LDM repository. During the generation processes for all tasks, we employ the DDIM sampler with $\eta=1$ and run it for 200 steps.

\textbf{Baseline Implementation.}
Our baseline implementations are closely based on the official releases. For the SLATE baseline, we adopt the improved version proposed in \cite{steve} \footnote{\url{https://github.com/singhgautam/steve}}. We empirically find this version to be more robust to complex scenes in terms of both object segmentation and generation quality.  The key difference of this version from the original SLATE is in the slot encoder. The original SLATE compute object slots from the latent space of the dVAE. Therefore, the attention resolution and information granularity of the slots are constrained by the dVAE latents. The improved SLATE addresses this issue by using a jointly trained CNN backbone to directly learn the slots from the original image. As shown in \cite{steve}, this improved version of SLATE achieves improved performance on complex datasets such as MOVi-C and MOVi-E. For SLATE$^+$, we replace the jointly trained dVAE with a VQGAN model pre-trained on OpenImages \footnote{We use the pre-trained weights from https://ommer-lab.com/files/latent-diffusion/vq-f8.zip}.

\section{Additional Experiments}

\subsection{Ablation on CNN backbone in Object-Centric Encoder}
\label{sec:appx_unet_ablation}

\begin{table*}[h!]
\begin{small}
    \caption{\textbf{Ablation Study on CNN backbone.} This table presents the results of an ablation study on the CNN backbone in the object-centric encoder. Specifically, we compare the performance of the LSD method that uses a UNet backbone to the performance of LSD-CNN, which utilizes a multi-layer Convnet backbone.
    }

    % CLEVRTex
    \begin{subtable}{1.0\linewidth}
    \caption{CLEVRTex}
    \vspace{-1mm}
    \centering
    \begin{tabular}{lcc}
    \toprule
    \scriptsize{\textbf{Segmentation}}    & LSD-CNN    & LSD \\
    \midrule
    mBO ($\uparrow$)          & 55.61 & \textbf{66.56}  \\
    mIoU ($\uparrow$)         & 53.82 & \textbf{65.02}  \\
    FG-ARI ($\uparrow$)       & 43.88 & \textbf{61.74}  \\
    \bottomrule
    \end{tabular}
        \begin{tabular}{lcc}
    \toprule
    \scriptsize{\textbf{Representation}}     & LSD-CNN    & LSD\\
    \midrule
    Shape ($\uparrow$)        & 75.00 & \textbf{81.60}  \\
    Material ($\uparrow$)    & \textbf{84.43} & 77.77   \\
    Position ($\downarrow$)    & 1.50 & \textbf{1.10}   \\
    \bottomrule
    \end{tabular}
    \vspace{2mm}
    \end{subtable}


    % MOVi-C
    \begin{subtable}{1.0\linewidth}
    \caption{MOVi-C}
    \vspace{-1mm}
    \centering
    \begin{tabular}{lcc}
    \toprule
    \scriptsize{\textbf{Segmentation}}       & LSD-CNN    & LSD \\
    \midrule
    mBO ($\uparrow$)           & 27.43 & \textbf{46.29}  \\
    mIoU ($\uparrow$)          & 26.31 & \textbf{44.99}  \\
    FG-ARI ($\uparrow$)        & 42.83 & \textbf{50.53} \\
    \bottomrule
    \end{tabular}
        \begin{tabular}{lcc}
    \toprule
    \scriptsize{\textbf{Representation}}     & LSD-CNN    & LSD  \\
    \midrule
    Position ($\downarrow$)     & 1.25 & \textbf{1.14}   \\
    3D B-Box ($\downarrow$)     & 1.46 & \textbf{1.44}   \\
    Category ($\uparrow$)      & 41.29 & \textbf{46.71}   \\
    \bottomrule
    \end{tabular}
    \vspace{2mm}    
    \end{subtable}


    % MOVi-E
    \begin{subtable}{1.0\linewidth}
    \caption{MOVi-E}
    \vspace{-1mm}
    \centering
    \begin{tabular}{lcc}
    \toprule
    \scriptsize{\textbf{Segmentation}}      & LSD-CNN    & LSD   \\
    \midrule
    mBO ($\uparrow$)          & 22.00 & \textbf{39.63}  \\
    mIoU ($\uparrow$)         & 20.71 & \textbf{38.28}  \\
    FG-ARI ($\uparrow$)       & 42.14 & \textbf{53.40}  \\
    \bottomrule
    \end{tabular}
        \begin{tabular}{lcc}
    \toprule
    \scriptsize{\textbf{Representation}}   & LSD-CNN    & LSD  \\
    \midrule
    Position ($\downarrow$)     & 2.01 & \textbf{1.92}   \\
    3D  B-Box ($\downarrow$)    & 3.33 & \textbf{2.94}    \\
    Category ($\uparrow$)      & 35.82 & \textbf{43.15}   \\
    \bottomrule
    \end{tabular}
    \end{subtable}

    \label{tab:ablate_cnn} 
\end{small}

\end{table*}

This section presents an ablation study on the CNN backbone used in the object-centric encoder. To investigate the impact of the CNN backbone choice, we introduce a variant of LSD named LSD-CNN. Specifically, LSD-CNN uses a multi-layer CNN network as the image encoder in the object-centric encoder, following the same CNN design as in the improved SLATE model \cite{steve}. We compare the results of this variant with the proposed LSD, which utilizes the UNet structure as the image encoder in the object-centric encoder, on CLEVRTex, MOVi-C, and MOVi-E. The results are presented in Table \ref{tab:ablate_cnn}. Overall, we observe that LSD outperforms LSD-CNN across nearly all tasks and datasets, suggesting that the UNet CNN backbone is critical for achieving the high performance of our model.

\begin{table}[b]
    \centering
    \caption{
    \textbf{Ablation Study on CNN backbone using Diffusion Latent Encoder.} We compare LSD with LSD-Latent on segmentation quality in the MOVi-E dataset.
    }
    \vskip 0.1in
    \begin{tabular}{lcccc}
    \toprule
    \scriptsize{\textbf{Segmentation}}  & LSD-Latent        & LSD    \\
    \midrule
    mBO ($\uparrow$)          & 5.52 & \textbf{39.63}  \\
    mIoU ($\uparrow$)         & 4.04 & \textbf{38.28}  \\
    FG-ARI ($\uparrow$)       & 10.11 & \textbf{53.40}  \\
    \bottomrule
    \end{tabular}
    \label{tab:appx_pretrain_vae} 

\end{table}

To further clarify the architecture choice, we also explore a LSD variant named LSD-Latent that employs an image latent encoder as its CNN backbone. We evaluate LSD-Latent on MOVi-E dataset and compare its object segmentation quality with LSD. The results can be seen in Table \ref{tab:appx_pretrain_vae}. The sub-optimal performance of LSD-Latent underscores the importance of having a separate CNN encoder in the object encoder.


\section{Additional Image Samples}

We include additional generation samples in Figures \ref{fig:appx_component} - \ref{fig:appx_lssd_coco_samples_add_3}. The additional samples demonstrate that LSD achieves more accurate object segmentation and generates more detailed and coherent scenes compared to the baselines. We provide additional observations and insights in the following sections.

\subsection{Visualization of Visual Concept Prompts}

In this section, we present visualizations of the visual concept prompts employed for generating image samples in the CLEVRTex and FFHQ datasets, as illustrated in Figure~\ref{fig:appx_component}. To generate these visualizations, we apply the attention map corresponding to each specific slot as a mask over the original image, which highlights the regions captured by that slot. We observe that the LSD model can seamlessly combine components from distinct images to create a coherent image sample while preserving the attributes of each component in the final image. Note that each concept library is associated with one k-means cluster within the extracted slots, and each prompt displayed in Figure~\ref{fig:appx_component} is sampled from one such library. The visualization of the prompts suggests that the concept libraries is able to capture practical concept categories, such as objects and backgrounds in the CLEVRTex dataset and facial attributes, hairstyles, clothing, and backgrounds in the FFHQ dataset.

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=1.\textwidth]{fig/component_composition.pdf}
\caption{\textbf{Compositional Image Generation with Concept Prompts.} In this visualization, we show a concept prompt constructed by composing arbitrary slots from our visual concept library and the corresponding generated image by LSD.}
\label{fig:appx_component}
\end{figure*}
% ######################################################################## 

\subsection{Impact of Pre-Trained Auto-Encoder on Generation Quality}
We investigate the effect of the pre-trained image auto-encoder in the comparison between SLATE and SLATE$^+$ in Figure \ref{fig:appx_compsitional_generation} and \ref{fig:appx_face_replacement}. Our results show that the use of the pre-trained auto-encoder leads to less blurry images and significantly more details, such as the floor and object texture in CLEVRTex and face detail in FFHQ. However, we also observe that applying the pre-trained auto-encoder does not necessarily improve the coherence of the generated scenes. For instance, in some SLATE$^+$ samples, the shape of the objects appear distorted in CLEVRTex and the background and hair style of FFHQ images are rendered as color patches. These findings indicate the limitation of the transformer decoders in SLATE and SLATE$^+$ models and highlights the crucial role of LSD's diffusion decoder in achieving optimal generation quality.

\subsection{Impact of Segmentation Quality on Slot-Based Editing}
In Figure \ref{fig:appx_object_removing_1} and \ref{fig:appx_object_removing_2}, we investigate the effect of object segmentation on the slot-based editing task. Our result show that when the object segmentation is almost perfect, editing tasks such as object removing can be easily accomplished by discarding the corresponding slot during image decoding. It is important to note that the object segmentation masks are derived from the attention weights on image features. Therefore, a perfect segmentation mask indicates that the object information is completely assigned to a single slot. However, we also observe cases when the object assignments are incomplete, such as when one object is divided into multiple slots. In such cases, removing an object would require removing all associated slots. These observations emphasize the importance of accurate object segmentation in achieving effective slot-based editing. Although LSD has demonstrated superior performance compared to existing approaches, further exploration in object segmentation is necessary to extend it to real-world applications.


% ########################################################################
\begin{figure*}[b]
\centering
\includegraphics[width=1.\linewidth]{fig/appx_segmentation.pdf}
\caption{\textbf{Visualization of Unsupervised Object Segmentation.} LSD achieves more accuracy object segmentation when comparing to the other methods.
}
\label{fig:appx_segmentation}
\end{figure*}
% ########################################################################

% ########################################################################
\begin{figure*}[b]
\centering
\includegraphics[width=1.\linewidth]{fig/appx_compsitional_generation.pdf}
\caption{\textbf{Compositional Generation Samples using Visual Concept Library.} LSD provides significantly higher fidelity and more clear details when comparing to the other methods.
}
\label{fig:appx_compsitional_generation}
\end{figure*}
% ########################################################################

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=0.95\linewidth]{fig/appx_face_replacement.pdf}
\caption{\textbf{Slot-Based Image Editing: Face Replacement.} We show face replacement in the FFHQ dataset, where we compose new images by combining the face slots from Source-B images with the hairstyle, clothing, and background slots from Source-A images. 
}
\label{fig:appx_face_replacement}
\end{figure*}
% ########################################################################

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=1.\linewidth]{fig/appx_object_removing_1.pdf}
\caption{\textbf{Slot-Based Image Editing: Object Removal and Segmentation Mask. Sample 1.} \textit{Top:} We show object removal by discarding the corresponding slots. \textit{Bottom:} We show the segmentation masks of each slot.
}
\label{fig:appx_object_removing_1}
\end{figure*}
% ########################################################################

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=1.\linewidth]{fig/appx_object_removing_2.pdf}
\caption{\textbf{Slot-Based Image Editing: Object Removal and Segmentation Mask. Sample 2.} 
}
\label{fig:appx_object_removing_2}
\end{figure*}
% ########################################################################

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=1.\linewidth]{fig/appx_bg_swap.pdf}
\caption{\textbf{Slot-Based Image Editing: Background Replacement.} The background replacement task is performed by replacing the background-associated slots.
}
\label{fig:appx_bg_swap}
\end{figure*}
% ########################################################################

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=1.\linewidth]{fig/appx_object_insertion.pdf}
\caption{\textbf{Slot-Based Image Editing: Object Insertion.} 
The background replacement task is performed by simply adding the new slot to the existing set of slots.
}
\label{fig:appx_object_insertion}
\end{figure*}
% ########################################################################

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=1.\linewidth]{fig/appx_lssd_seg_cfg_add_1}
\caption{\textbf{Random Samples for Real-World Object-Centric Learning and Conditional Generation 1.}
}
\label{fig:appx_lssd_seg_cfg_add_1}
\end{figure*}
% ########################################################################

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=1.\linewidth]{fig/appx_lssd_seg_cfg_add_2}
\caption{\textbf{Random Samples for Real-World Object-Centric Learning and Conditional Generation 2.}
}
\label{fig:appx_lssd_seg_cfg_add_2}
\end{figure*}
% ########################################################################

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=1.\linewidth]{fig/appx_lssd_seg_cfg_add_3}
\caption{\textbf{Random Samples for Real-World Object-Centric Learning and Conditional Generation 3.}
}
\label{fig:appx_lssd_seg_cfg_add_3}
\end{figure*}
% ########################################################################

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{fig/appx_lssd_coco_samples_add_1}
\caption{\textbf{Random Samples for Real-World Image Generation 1.}
}
\label{fig:appx_lssd_coco_samples_add_1}
\end{figure*}
% ########################################################################

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{fig/appx_lssd_coco_samples_add_2}
\caption{\textbf{Random Samples for Real-World Image Generation 2.}
}
\label{fig:appx_lssd_coco_samples_add_2}
\end{figure*}
% ########################################################################

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{fig/appx_lssd_coco_samples_add_3}
\caption{\textbf{Random Samples for Real-World Image Generation 3.}
}
\label{fig:appx_lssd_coco_samples_add_3}
\end{figure*}
% ########################################################################
