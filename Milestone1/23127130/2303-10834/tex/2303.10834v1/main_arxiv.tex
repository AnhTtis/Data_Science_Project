\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
\PassOptionsToPackage{numbers}{natbib}
% before loading neurips_2022


% ready for submission
% \usepackage{neurips_2022_arxiv}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    % \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
    % \usepackage[final]{neurips_2022}
    \usepackage[final]{neurips_2022_arxiv}

% \nolinenumbers

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{subcaption}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage[textsize=tiny]{todonotes}

% Jindong
\usepackage{multicol}
\usepackage{defs}
% \usepackage{adjustbox}
\usepackage{siunitx}
% \usepackage{lineno}

% Packages added by Gautam.
\usepackage{lipsum}



\title{Object-Centric Slot Diffusion}

\author{%
   Jindong Jiang\thanks{Correspondence to \texttt{jindong.jiang@rutgers.edu} and \texttt{sungjin.ahn@kaist.ac.kr}.} \\
%   Department of Computer Science\\
   Rutgers University \\
   \texttt{jindong.jiang@rutgers.edu}\\
   \And
  Fei Deng\\
%   Department of Computer Science\\
   Rutgers University \\
   \texttt{fei.deng@rutgers.edu}\\
   \And
  Gautam Singh\\
%   Department of Computer Science\\
   Rutgers University \\
   \texttt{singh.gautam@rutgers.edu}\\
   \And
%    Yi-Fu Wu \\
% %   Department of Computer Science\\
%    Rutgers University \\
%    \texttt{yifu.wu@gmail.com}\\
%    \And
   Sungjin Ahn \\
%   School of Computing\\
   KAIST \\
   \texttt{sungjin.ahn@kaist.ac.kr}
}


\begin{document}


\maketitle


\begin{abstract}
% The goal of object-centric learning is to discover representations of compositional knowledge pieces in an unsupervised way. However, 
Despite remarkable recent advances, making object-centric learning work for complex natural scenes remains the main challenge. The recent success of adopting the transformer-based image generative model in object-centric learning suggests that having a highly expressive image generator is crucial for dealing with complex scenes. In this paper, inspired by this observation, we aim to answer the following question: \textit{can we benefit from the other pillar of modern deep generative models, i.e., the diffusion models, for object-centric learning} and \textit{what are the pros and cons of such a model}? To this end, we propose a new object-centric learning model, Latent Slot Diffusion (LSD). LSD can be seen from two perspectives. From the perspective of object-centric learning, it replaces the conventional slot decoders with a latent diffusion model conditioned on the object slots. Conversely, from the perspective of
diffusion models, it is the first unsupervised compositional conditional
diffusion model which, unlike traditional diffusion models, does not require supervised annotation such as a text description to learn to compose. In experiments on various object-centric tasks, including the FFHQ dataset for the first time in this line of research, we demonstrate that LSD significantly outperforms the state-of-the-art transformer-based decoder, particularly when the scene is more complex. We also show a superior quality in unsupervised compositional generation.
% because, despite the success and unique merits brought by the diffusion models in image generation, it has not been studied in the context of unsupervised object-centric learning.
\end{abstract}

\section{Introduction}

The underlying fundamental structure of the physical world is compositional and modular. While in some data modalities like language, this compositional structure is naturally revealed in the form of tokens or words, in general, this structure is hidden in modalities such as images and it is quite elusive how one may discover it.
% still elusive how to discover this  compositional structure when the modality is unstructured e.g., images. 
Yet, such representational compositionality and modularity are considered crucial for many applications that require systematically manipulating such modular token-like pieces, \emph{e.g.}, reasoning \cite{lake2017building,bottou2014machine}, causal inference \cite{scholkopf2021toward}, and out-of-distribution generalization \cite{bahdanau2018systematic, fodor88}.
% Many recent works have demonstrated the benefits of object-centric representations in applications such as ...

Object-centric learning \cite{binding} aims to discover this hidden compositional structure from unstructured observation by learning to bind relevant features and forming useful tokens in an unsupervised way. For images, one of the most popular approaches is to auto-encode the image via Slot Attention~\cite{slotattention} as the encoder. Slot Attention applies competitive spatial attention
to partition the image into separate local areas and represents each area as a slot.
% among the slots and making each slot represent an object in the image independently of other objects. 
Then, a decoder generates the image from the slots with the aim of minimizing the reconstruction error. 
Due to limited capacity per slot, the representation is encouraged to become compositional by making each slot capture a reusable entity, \emph{e.g.}, an object.

The most important challenge remaining in this framework of unsupervised object-centric learning is to make it work for complex naturalistic scene images. 
% And, in achieving this, it recently turned out that how to generate the reconstruction image is a crucial factor. 
Specifically, until recently, a special type of decoder, often called the \textit{mixture decoder}, has been used dominantly in most object-centric models \cite{slotattention,genesis,monet}. 
While the mixture decoder was originally designed with heavy priors to make it work on toy synthetic images, \emph{e.g.}, by decoding each slot with a weak decoder \cite{sbd}, subsequent results have shown that such priors eventually make it struggle when dealing with complex naturalistic scene images. Contrary to the conventional belief, Singh \emph{et al.}~\cite{slate} recently proposed to depart from the low-capacity mixture-decoder approach and use an expressive transformer-based autoregressive image generator in object-centric learning~\cite{slate,steve}. It was shown that increasing the decoder capacity is the key to dealing with complex and naturalistic scenes in this framework. Following this, several works have demonstrated the effectiveness of this transformer-based slot decoding approach in various settings~\citep{chang2022object, Chang2022HierarchicalAF, dinosaur, slotformer, sysbinder}.

\begin{figure}[t]
\centering
\includegraphics[width=0.98\textwidth]{fig/first_page_figure_horizontal.pdf}
\vskip -0.1in
\caption{\textbf{Overview.} In this paper, we propose and investigate a novel model called \textit{Latent Slot Diffusion} or \textit{LSD}. \textit{Left:} From the perspective of object-centric learning, LSD can effectively decompose complex and naturalistic scene images into objects in a fully unsupervised manner. \textit{Right:} From another perspective, LSD provides the ability to synthesize novel high-fidelity scenes by composing visual concepts acquired without any supervision.}
\label{fig:qualitative_overall}
\end{figure}

This success of transformer-based image generative modeling in object-centric learning naturally raises a question: \textit{can the other pillar of modern deep generative models, the diffusion models, also be beneficial for object-centric learning}? The diffusion models \cite{sohl2015deep,ddpm}, based on the stochastic denoising process, have achieved remarkable success in various image generation tasks \cite{dalle2,glide,imagen,adm,ldm,srdm,sdedit}, and can sometimes outperform the transformer-based autoregressive models. It also comes with modeling abilities that the transformer-based autoregressive models cannot provide \cite{sohl2015deep}. However, it has not yet been studied in the context of unsupervised object-centric learning and thus it is of crucial interest to investigate if this is realizable at all and what the pros and cons would be. 

In this paper, our aim is to answer this question. For this, we propose a novel model called Latent Slot Diffusion (LSD). The LSD model can be understood from two perspectives. On one hand, we have the perspective of object-centric learning. From this perspective, LSD can be seen as replacing the conventional slot decoders with a conditional latent diffusion model where the conditioning is on object-centric slots provided by Slot Attention. 
On the other hand, we have the perspective of diffusion models. From this perspective, ours is the first \textit{unsupervised} compositional conditional diffusion model. While conventional conditional diffusion models~\cite{dalle2,ldm,imagen,liu2022compositional} require providing a supervised annotation such as the text description of an image to perform compositional generation, LSD is a diffusion model that supports constructing such a compositional description in terms of visual concepts extracted from images through unsupervised object-centric learning. 


In experiments, we evaluate the proposed model in various object-centric tasks, including unsupervised object segmentation, downstream property prediction, compositional generation, and image editing. We show that the LSD model provides significantly better performance than the state-of-the-art model, \emph{i.e.}, the transformer-based autoregressive generative model. A remarkable property of the proposed model is that LSD outperforms the autoregressive transformers more significantly as the scene becomes more complex. Particularly, LSD allows exploring for the first time the applicability of object-centric models to FFHQ \cite{stylegan}, a dataset of high-resolution and high-quality face images that is beyond the generative capability of existing object-centric models.  Notably, we also find, however, that for very simple scene images like CLEVR \citep{clevr} that have limited visual diversity, LSD performs worse than the autoregressive transformer, presumably due to overfitting of the model.
% by memorizing spurious visual regularities.

\section{Latent Slot Diffusion}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\textwidth]{fig/method.png}
    \caption{\textbf{Method.} \textit{Left:} In training, we encode the given image as a VQGAN latent and as slots. We then add noise to the VQGAN latent and we train a denoising network to predict the noise given the noisy latent and the slots. \textit{Right:} Given the trained model, we can generate novel images by composing a slot-based concept prompt and decoding it using the trained latent slot diffusion decoder.}
    \label{fig:method}
\end{figure}

Latent Slot Diffusion or LSD is a novel object-centric learning method that we propose and study in this work. Like conventional object-centric learning approaches, it seeks to achieve object-centric decomposition of a given scene image via auto-encoding. However, different from the conventional approaches, LSD incorporates the recent advances in diffusion modeling into the design of the decoder and investigates their pros and cons for the first time in this line of research. In the rest of this section, we describe the auto-encoding framework of LSD by first describing our object-centric encoder and then our proposed decoder.

\subsection{Object-Centric Encoder}
Given an input image $\bx \in \eR^{H \times W \times C}$, our object-centric encoder seeks to decompose and represent it as a collection of $N$ vectors or slots $\bS \in \eR^{N \times D}$ where each slot (denoted as $\bs_n \in \eR^{D}$) should represent one object in the image. For this, we adopt Slot Attention, an architecture that is also used in the current state-of-the-art object-centric learning approaches \citep{slotattention, slate, savi}. We now describe how Slot Attention works in our model.


In Slot Attention, we first encode the input image $\bx$ as a set of $M$ input features $\bE \in \eR^{M \times D_\text{input}}$ via a backbone network $\smash{f_\phi^\text{backbone}}$, \emph{i.e.}, $\smash{\bE = f_\phi^\text{backbone}(\bx)}$. The network $\smash{f_\phi^\text{backbone}}$ is implemented as a CNN
% in which positional embeddings are added in the second-last layer. 
whose final output feature map is flattened to form a set. 
% UNet ....
% The $N$ slots together provide a representation $\bS$ of the full image. The slots are expected to provide a modular representation of the full image and thus each spatial grouping should capture a meaningful entity e.g.,~an object.
Next, the features in $\bE$ are grouped into $N$ spatial groupings and the information in each grouping is aggregated to produce a \textit{slot}. 
The grouping is achieved via an iterative slot refinement procedure. At the start of the refinement procedure, the slots $\bS$ are filled with random Gaussian noise. Then, they are refined via \textit{competitive attention} over the input features, where the $N$ slots act as the queries and the $M$ input features act as the keys and values.  The  queries and  keys undergo a dot-product to produce $N\times M$ attention proportions. Next, on these attention proportions, softmax is applied along the axis $N$ to produce attention weights $\bA$ that capture the soft assignment of each input feature to a slot. Then, for each $n$, all input features are sum-pooled weighted by their attention weights $\bA_{n,1}, \ldots, \bA_{n,M}$ to produce an attention readout $\bu_n \in \eR^{D}$. These steps can be formally described as follows:
\begin{align*}
    \bA = \underset{N}{\texttt{softmax}}\left(\frac{q(\bS) \cdot k(\bE)^T}{\sqrt{D}}\right) 
    && \Longrightarrow && 
    \bA_{n,m} = \dfrac{\bA_{n,m}}{\sum_{m=1}^M \bA_{n,m}}
    && \Longrightarrow && 
    \bu_n = \sum_{m=1}^M  v(\bE_m) \bA_{n,m},
\end{align*}
where, $q, k, v$ are linear projections that map the slots and input features to a common dimension $D$. Using the bottom-up information captured by the readout $\bu_n$, the slots are updated by an RNN as $\smash{\bs_n = f_\phi^\text{RNN}(\bs_n, \bu_n)}$. In practice, competitive attention and RNN update are performed iteratively several times and slots from the last iteration are considered the final slot representation $\bS$.


\subsection{Latent Slot Diffusion Decoder}

In this section, we describe our proposed decoding approach called \textit{Latent Slot Diffusion Decoder} or \textit{LSD decoder} for reconstructing the image given the slot representation $\bS$. The design of the LSD decoder takes advantage of the recent advances in generative modeling based on diffusion \cite{ldm, ddpm}. In Figure~\ref{fig:method} we provide an overview of our decoding approach.

\subsubsection{VQGAN}
One of the key design components of the LSD decoder is VQGAN \cite{vqgan}. VQGAN  provides a way to map an image $\bx$ to a lower dimensional \textit{latent representation} $\bz_0$ via an encoder $\smash{f_\phi^\text{VQ}}$. This helps LSD reduce the computational burden for reconstructing high-resolution images by allowing it to use the lower dimensional latent $\bz_0$ as an intermediate reconstruction target. VQGAN also allows us to later obtain the original resolution image without significantly losing the image contents and fidelity by decoding the latent $\bz_0$ using the VQGAN decoder $\smash{g_\ta^\text{VQ}}$. These can be summarized as:
\begin{align*}
    \bz_0 = f_\phi^\text{VQ}(\bx), && \hat{\bx} = g_\ta^\text{VQ}(\bz_0), && \text{where } \bz_0 \in \eR^{H_\text{VQ} \times W_\text{VQ} \times D_\text{VQ}}, \text{ and } \hat{\bx} \in \eR^{H \times W \times C} .
    %, \quad \hat{\bx} = g_\text{AE}(\bz)
\end{align*}
In our model, we use a VQGAN pre-trained on OpenImages \footnote{We use the pre-trained weights from https://ommer-lab.com/files/latent-diffusion/kl-f8.zip}.
% In our model, we use VQGAN pre-trained on OpenImages.
% Remarkably, this can not only provide a significant reduction in the dimensionality compared to the original image dimensions but can do so without losing the image contents and the image fidelity. 
% However, in the object-centric learning context, previous methods have not leveraged VQGAN and ours is the first model that investigates its potential.

\subsubsection{Slot-Conditioned Diffusion}

In LSD, we leverage diffusion modeling to reconstruct the VQGAN latent $\bz_0$ conditioned on the slots $\bS$. This modeling approach has been primarily explored in supervised contexts, \emph{e.g.}, for text-to-image generation in Latent Diffusion Models (LDM) \citep{ldm}. However, different from LDM, in this work, instead of conditioning the decoder on embeddings of supervised labels, we condition it on slots where the process of obtaining the slots themselves, \emph{i.e.}, Slot Attention, is jointly trained with the decoder without supervision. 
Following LDM, our decoder works by training a decoding distribution $p_\ta(\bz_0|\bS)$ to maximize the log-likelihood $\log p_\ta(\bz_0|\bS)$ of the VQGAN latent $\bz_0$ given the slots $\bS$. This decoding distribution $p_\ta(\bz_0|\bS)$ is modeled as a $T$-step denoising process:
\begin{align*}
    p_\ta(\bz_0|\bS) = \int p(\bz_T) \prod_{t=T, \ldots, 1} p_\ta(\bz_{t-1} | \bz_{t}, t, \bS) \,\mathrm{d} \bz_{1:T},
\end{align*}
where $p(\bz_T) = \cN(\mathbf{0}, \mathbf{I})$, $p_\ta(\bz_{t-1} | \bz_{t},  t, \bS)$ is a one-step denoising distribution, and $\bz_T, \bz_{T-1}, \ldots, \bz_0$ is a sequence of progressively denoised latents.
The one-step denoising distribution $p_\ta(\bz_{t-1} | \bz_{t}, t, \bS)$ is parametrized via a neural network $g_\ta^\text{LSD}$ in the following manner:
\begin{align*}
    p_\ta(\bz_{t-1} | \bz_{t}, t, \bS) = \cN\left(\frac{1}{\sqrt{\alpha_t}}\left(\bz_t -\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\hat{\boldsymbol{\epsilon}}_t \right), \beta_t\mathbf{I}  \right), && \text{where } \hat{\boldsymbol{\epsilon}}_t = g^\text{LSD}_\ta(\bz_t, t, \bS),
\end{align*}
$\beta_1, \ldots, \beta_T$ is a linearly increasing variance schedule, $\alpha_t = 1 - \beta_t$, and $\bar{\alpha}_t = \prod_{i=1}^t (1 - \beta_i)$. 

\textbf{Sampling Procedure.} To sample a $\bz_0 \sim p_\ta(\bz_0|\bS)$, we adopt an iterative denoising procedure as in \citep{ldm, ddpm}. The sampling process starts with a latent representation $\bz_T\sim \cN(\mathbf{0}, \mathbf{I})$ filled with random Gaussian noise. Next, conditioned on the slots, we denoise it $T$ times by sampling sequentially from the one-step denoising distribution $\bz_{t-1} \sim p_\ta(\bz_{t-1} | \bz_t, t, \bS)$ for $t=T, \ldots, 1$.
% \begin{align*}
%     \hat{\boldsymbol{\epsilon}}_t = g^\text{LSD}_\ta(\bz_t, t, \bS) && \Longrightarrow && \bz_{t-1} \sim \cN\left(\frac{1}{\sqrt{\alpha_t}}\left(\bz_t -\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \hat{\boldsymbol{\epsilon}}_t\right), \beta_t  \right), 
% \end{align*}
% where $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}_t = \prod_{i=1}^t 1 - \beta_i$. 
This produces a sequence of latents $\bz_T, \bz_{T-1}, \ldots, \bz_0$ that become progressively cleaner. Finally, $\bz_0$ can be considered as the reconstructed latent representation.

\textbf{Training Procedure.} Following LDM \cite{ldm}, the training of $p_\ta(\bz_0|\bS)$ can be cast to a simple procedure for training $g^\text{LSD}_\ta$ as follows. Given an image $\bx$, its slot representation $\bS$, and its VQGAN latent $\bz_0$, we first randomly choose a noise level $t \in \{1, \ldots, T\}$ from a uniform distribution. Given the $t$, we corrupt the clean latent $\bz_0$ and obtain a noised latent $\bz_t$ as follows:
\begin{align*}
    \bz_t = \sqrt{\bar{\alpha}_t} \bz_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}_t, && \text{ where } \boldsymbol{\epsilon}_t \sim \cN(\mathbf{0}, \mathbf{I}), \quad \bar{\alpha}_t = \prod_{i=1}^t (1 - \beta_i).
\end{align*}
The noised latent $\bz_t$ is then given as input to $g^\text{LSD}_\ta$ along with the slots $\bS$ and denoising time-step $t$ to predict the noise $\boldsymbol{\epsilon}_t$.
The network $g^\text{LSD}_\ta$ is trained by minimizing the mean squared error between the predicted noise $\hat{\boldsymbol{\epsilon}}_t$ and the true noise $\boldsymbol{\epsilon}_t$:
\begin{align*}
    \hat{\boldsymbol{\epsilon}}_t = g^\text{LSD}_\ta(\bz_t, t, \bS) && \Longrightarrow && \cL(\phi, \ta) = %\mathbb{E}_{t, \bz, \boldsymbol{\epsilon}}  
    || \hat{\boldsymbol{\epsilon}}_t - \boldsymbol{\epsilon}_t ||^2.
\end{align*}



\subsubsection{Denoising Network}
We implement the denoising network $g^\text{LSD}_\ta$ as a variant of the conventional UNet architecture adapted to incorporate slot-conditioning. Our denoising network consists of a stack of $L$ layers where each layer $l$ is a UNet-style CNN layer followed by a slot-conditioned transformer:
% \begin{align*}
%    \tilde{\bh}_{l} = \begin{cases}\texttt{CNN}_{\ta}^l(\bh_{l-1}, t) , & \text{if } l \leq \frac{L}{2} \\ \texttt{CNN}_{\ta}^l([\bh_{l-1}, \bh_{L-l + 1}], t), & \text{otherwise} \end{cases} && \Longrightarrow && \bh_{l} = \texttt{Transformer}_{\ta}^l(\tilde{\bh}_{l} + \mathbf{p}_l, \texttt{cond=}\bS),
% \end{align*}
\begin{align*}
   \tilde{\bh}_{l} = \texttt{CNN}_{\ta}^l([\bh_{l-1}, \bh_{\text{skip}(l)}], t), && \Longrightarrow && \bh_{l} = \texttt{Transformer}_{\ta}^l(\tilde{\bh}_{l} + \mathbf{p}_l, \texttt{cond=}\bS),
\end{align*}
where $\bh_0=\bz_t$ is the input, $\bh_1, \ldots, \bh_{L-1}$ are the hidden states and $\hat{\boldsymbol{\epsilon}}_t = \bh_L$ is the output. 

\textbf{CNN Layers.} Following UNet \cite{unet}, the convolutional layers $\smash{\texttt{CNN}_{\ta}^1, \ldots, \texttt{CNN}_{\ta}^L}$ downsample the feature map via the first $\smash{\frac{L}{2}}$ layers and then upsample it back to the original resolution via the remaining layers. Following standard UNet, these CNN layers also receive inputs via skip connection from an earlier layer denoted by $\text{skip}(l)$. This network design has also been explored in LDM \citep{ldm}.


% The CNN implements the conditioning on the noise level $t$ by a

\textbf{Slot-Conditioned Transformer.} The role of the transformer is to incorporate the information from the slots into the UNet-based denoising process. For this, in each layer $l$, the intermediate feature map $\smash{\tilde\bh_l}$ produced by the CNN layer is flattened into a set of features. To this, positional embeddings $\bp_l$ are added and the resulting features are provided as input to the transformer. Within the transformer, these features interact with each other and with the slots $\bS$, thus incorporating the information from the slots into the denoising process. The transformer output is then reshaped back to a feature map $\bh_l$.

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=0.98\textwidth]{fig/method.png}
%     \caption{\textbf{Method.} \textit{Left:} In training, we encode the given image as a VQGAN latent and as slots. We then add noise to the VQGAN latent and we train a denoising network to predict the noise given the noisy latent and the slots. \textit{Right:} Given the trained model, we can generate novel images by composing a slot-based concept prompt and decoding it using the trained latent slot diffusion decoder.}
%     \label{fig:method}
% \end{figure}

% \textcolor{red}{This allows....}. We shall see in Section \ref{sec:experiments} that this leads to an improved performance. Similarly to \cite{slotattention}, each $\mathbf{P}_l$ is obtained by mapping the 2D grid coordinates to embedding vectors of dimension $D$ using a learned linear projection.

% \textbf{Conditioning on $t$.}
% \begin{align*}
%     \psi(\bh_l, t) = \mathbf{t}_s(t) \texttt{GN}(\bh_l) + \mathbf{t}_b(t)
% \end{align*}

% $\mathbf{t}_s$ and $\mathbf{t}_b$ are implemented as MLP layers.


% \subsubsection{Training Objective}
% \begin{align*}
%     \cL = || \hat{\boldsymbol{\epsilon}}_t - \boldsymbol{\epsilon} ||^2
% \end{align*}

\section{Compositional Image Synthesis}
\label{sec:visual_concept_lib}
In this section, we describe how a trained LSD model can be used to compose and synthesize novel images. The conventional approach to composing novel images is commonly supervised and is based on text prompts. That is, using words from a vocabulary, a sentence is composed which is given to a text-to-image model to synthesize the desired image \citep{dalle, dalle2, imagen, ldm, glide}. However, in a fully unsupervised setting like ours, we first need to build a library of visual concepts by simply observing a large set of unlabelled images. Then, similarly to composing a sentence prompt using words, we compose a \textit{concept prompt} by picking concepts from the visual concept library. By providing this concept prompt to the LSD decoder, we can then synthesize a desired novel image. This approach of unsupervised compositional image synthesis was also explored in \cite{slate}.

\textbf{Unsupervised Visual Concept Library.} To build a library of visual concepts from unlabelled images, we first take a large batch of $B$ images $\bx_1, \ldots, \bx_B$. To this, we apply slot attention to obtain slots for these images $\bS_1, \ldots, \bS_B$. We then collect all these slots as a single set $\cS$ and apply $K$-means on it. The $K$-means procedure assigns each slot in $\cS$ to one of the $K$ clusters. We consider the set of slots that are assigned to a $k$-th cluster as a visual concept library $\cV_k$. With $K$ as the number of clusters, this procedure provides $K$ visual concept libraries $\cV_1, \ldots, \cV_K$. Our experiments shall show that this simple $K$-means procedure can produce semantically meaningful concept libraries. For instance, on a dataset of human face images such as FFHQ \cite{stylegan}, the $K$ libraries correspond to useful concept classes such as hair style, face, clothing, and background.

\textbf{Novel Image Synthesis.} Given libraries $\cV_1, \ldots, \cV_K$, we can compose a concept prompt $\bS_\text{compose}$ by picking $K$ slots, each from the corresponding $k$-th library, and stacking them together:
\begin{align*}
    \bS_\text{compose} = (\bs_1, \ldots, \bs_K), && \text{where } \bs_k \sim \text{Uniform}(\cV_k)
\end{align*}
We then give the composed prompt $\bS_\text{compose}$ to the LSD decoder to generate the VQGAN latent: $\smash{\bz_\text{compose} \sim p_\ta(\bz_0 | \bS_\text{compose})}$. Applying the VQGAN decoder on $\smash{\bz_\text{compose}}$ generates the desired novel scene image $\smash{\bx_\text{compose} = g_\ta^\text{VQ}(\bz_\text{compose})}$. For instance, in the FFHQ dataset, the concept prompt can be a collection of chosen hair style, face, clothing, and background. Decoding this prompt would generate a face image that conforms to this prompt.


% \begin{align*}
%     \bz_\text{compose} \sim p_\ta(\bz_0 | \bS_\text{compose}) && \Longrightarrow && \bx_\text{compose} = g_\ta^\text{VQ}(\bz_\text{compose})
% \end{align*}


% \clearpage
\section{Related Work}
\textbf{Unsupervised Object-Centric Learning.} Object-centric representation learning aims to decompose multi-object scenes into meaningful object entities. A common approach to this is by auto-encoding. In this line, the focus has been to design an appropriate decoder that supports good decomposition. The most widely used decoders include the mixture-decoder \cite{monet, iodine, slotattention, nem, genesis, von2020towards, anciukevicius2020object, du2020unsupervised, engelcke2021genesis, simone, Zhang2022RobustAC}, spatial transformer decoder \cite{air, spair, space, gnm, deng2020generative, roots}, Neural Radiance Fields (NeRF) \cite{stelzner2021decomposing,Wu2022ObPoseLC,Smith2022UnsupervisedDA}, transformer decoder \cite{slate, steve, sysbinder, slotformer, Chang2022HierarchicalAF, sajjadi2022osrt, Gopalakrishnan2022UnsupervisedLO}, energy-based models \cite{du2021unsupervised} and complex-valued functions \citep{Lowe2022ComplexValuedAF}. 
Among these, the mixture decoder has been shown to struggle on complex scene images \cite{slate, steve} while the spatial transformer decoder requires careful tuning of hyperparameters which limits their applicability in realistic scenes. Neural Radiance Fields require camera poses, and the learning setting involves 3D scenes unlike ours. 
% While Transformer decoders have been shown to improve object factorization and generation quality on simple synthetic scenes \cite{slate, slotformer}, they have yet to be tested on complex datasets and high-resolution images. 
Another line of work seeks object-centric scene decomposition without reconstruction. This includes works such as \cite{dinosaur, Wen2022SelfSupervisedVR, Henaff2022ObjectDA, cutler, sslslot, contrastslot}. However, unlike ours, these approaches lack the ability to generate images. Preceding object-centric learning, several works pursued disentanglement within a single-vector representation of the scene and use it for compositional image generation \citep{infogan, higgins2017beta, kim2018disentangling, kumar2017variational, chen2018isolating}. However, lacking spatial binding mechanisms, these methods struggle in multi-object scenes \citep{iodine} unlike ours.


\textbf{Diffusion Models.} 
Diffusion models (DMs) are a recent class of generative models that can produce high-quality images by reversing a stochastic process that gradually adds noise to an image \citep{ddpm,scoredm}. DMs have been applied to various tasks in computer vision, such as class-conditioned generation, text-to-image generation, image editing, super-resolution, and inpainting \citep{adm,cfdm,dalle2,glide,imagen,sdedit,cascadedm,srdm}. Recently, \citep{ldm} proposed Latent Diffusion Models (LDM). By virtue of operating on a low-dimensional latent space, LDM reduces the computational demands of DMs significantly.
% It also proposes to use a cross-attention for flexible conditional generations for various conditioning types. 
\citep{composdm} introduced a method for multi-object scene generation by combining signals from multiple text-conditioned denoising networks. However, to achieve controllable generation, many of these existing DMs require additional labels such as text descriptions to train and control the generation process. In contrast, our model can generate images compositionally using object concepts directly extracted from images. Moreover, DMs have also been used for representation learning. \citep{labeleffdm} demonstrated that the intermediate features of a learned DM can be used as useful image representations for label-efficient semantic segmentation. \citep{dmae} introduced an image encoder that compresses an image into a latent representation, jointly trained with the denoising objective. Nevertheless, these representations are unstructured and not modular like ours. 
% Our work differs from these approaches as we learn to segment an image into object-aware regions and obtain structured representations for each object.


% \clearpage
\section{Experiments}

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=1.\textwidth]{fig/segmentation.pdf}
\vskip -0.1in
\caption{\textbf{Visualization of Unsupervised Object Segmentation.} We show visualizations of predicted segments on CLEVR, CLEVRTex, MOVi-E, and FFHQ datasets. 
% LSD enjoys a significant advantage on datasets with higher scene complexity.
}
\label{fig:seg}
\end{figure*}
% ########################################################################


We extensively evaluate our proposed Latent Slot Diffusion (LSD) model on various object-centric tasks, including unsupervised object segmentation, downstream property prediction, compositional generation, and image editing. As will be shown, our model significantly outperforms the state-of-the-art on multiple challenging datasets with complex texture and background, including FFHQ~\cite{stylegan} which has been beyond the generative capability of object-centric models.

\textbf{Datasets.} We evaluate our model on five datasets. Four of them are synthetic multi-object datasets---CLEVR~\citep{clevr}, CLEVRTex~\citep{clevrtex}, MOVi-C, MOVi-E~\citep{kubric}. They present increasing levels of difficulty---CLEVRTex adds texture to objects and backgrounds, MOVi-C uses more complex objects and natural backgrounds, and MOVi-E contains large numbers of objects (up to 23) per scene. Furthermore, we explore for the first time the applicability of object-centric models to FFHQ~\citep{stylegan}, a dataset of high-quality face images that is beyond the generative capability of current object-centric models. Unlike previous works in this line that only investigate low-resolution images, \emph{e.g.}, $128 \times 128$, we use a resolution of $256 \times 256$ for all datasets in our experiments.

\textbf{Baselines.} We compare our model against SLATE, the state-of-the-art object-centric learning and unsupervised compositional image generation approach. We use its improved version~\cite{steve}, which is more robust in complex scenes. For a fair comparison with LSD that leverages VQGAN, we also develop a VQGAN-based variant of SLATE denoted as SLATE$^+$, where its low-capacity dVAE~\cite{slate} is replaced with VQGAN. For all models in this work, we use an OpenImages-pretrained VQGAN~\cite{vqgan}.

\subsection{Object-Centric Representation Learning}

In line with previous work~\cite{slotattention}, we use unsupervised object segmentation and downstream property prediction to evaluate the object-centric learning capability and representation quality. Within each dataset, all models we compare use the same number of slots.
% We focus on the four multi-object datasets due to availability of groundtruth labels for evaluation. % For all models, we set the number of slots to be the maximum number of objects per image plus one (intended for background).




\begin{table*}[t]
\begin{small}
    \caption{\textbf{Segmentation Performance and Representation Quality.} \textit{Left:} We evaluate the segmentation quality and report mBO, mIoU, and FG-ARI scores across various datasets and baselines. \textit{Right:} We measure the representation quality by learning a probe to predict the object property given frozen slots. For position and 3D bounding box, we report MSE. For shape, material, and category, we report accuracy.
    %We report the position error in pixel units.
    }
    % \vskip 0.1in

    % CLEVRTex
    \begin{subtable}{1.0\linewidth}
    \caption{CLEVRTex}
    \vspace{-1mm}
    \centering
    \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Segmentation}}          & SLATE        & SLATE$^+$      & LSD (Ours)   \\
    \midrule
    mBO ($\uparrow$)         & 51.24 & 56.22 & \textbf{66.56} \\
    mIoU ($\uparrow$)        & 50.04 & 54.93 & \textbf{65.02} \\
    FG-ARI ($\uparrow$)      & 43.59 & \textbf{73.42} & 61.74 \\
    \bottomrule
    \end{tabular}
        \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Representation}}           & SLATE        & SLATE$^+$      & LSD (Ours)   \\
    \midrule
    Shape ($\uparrow$)       & 77.62 & 76.20 & \textbf{81.60} \\
    Material ($\uparrow$)   & 72.59 & 66.04 & \textbf{77.77} \\
    Position ($\downarrow$)  & 1.16 & 1.13 & \textbf{1.10}    \\
    \bottomrule
    \end{tabular}
    \vspace{2mm}
    \end{subtable}


    % MOVi-C
    \begin{subtable}{1.0\linewidth}
    \caption{MOVi-C}
    \vspace{-1mm}
    \centering
    \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Segmentation}}          & SLATE        & SLATE$^+$      & LSD (Ours)   \\
    \midrule
    mBO ($\uparrow$)         & 38.60 & 39.24  & \textbf{46.29} \\
    mIoU ($\uparrow$)        & 37.11 & 37.59  & \textbf{44.99} \\
    FG-ARI ($\uparrow$)      & 48.00 & \textbf{51.53}  & 50.53 \\
    \bottomrule
    \end{tabular}
        \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Representation}}           & SLATE        & SLATE$^+$      & LSD (Ours)   \\
    \midrule
    Position ($\downarrow$)  & 1.36 & 1.22   & \textbf{1.14}   \\
    3D B-Box ($\downarrow$)   & 1.46 & \textbf{1.32}  & 1.44    \\
    Category ($\uparrow$)    & 42.49 & 46.46 & \textbf{46.71} \\
    \bottomrule
    \end{tabular}
    \vspace{2mm}    
    \end{subtable}


    % MOVi-E
    \begin{subtable}{1.0\linewidth}
    \caption{MOVi-E}
    \vspace{-1mm}
    \centering
    \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Segmentation}}          & SLATE        & SLATE$^+$      & LSD (Ours)   \\
    \midrule
    mBO ($\uparrow$)         & 28.66 & 22.69  & \textbf{39.63} \\
    mIoU ($\uparrow$)        & 27.20 & 21.10  & \textbf{38.28} \\
    FG-ARI ($\uparrow$)     & 41.37 & 46.34  & \textbf{53.40} \\
    \bottomrule
    \end{tabular}
        \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Representation}}           & SLATE        & SLATE$^+$      & LSD (Ours)   \\
    \midrule
    Position ($\downarrow$)  & 2.25 & 2.07  & \textbf{1.92}    \\
    3D  B-Box ($\downarrow$)   & 3.24 & 3.11  & \textbf{2.94}    \\
    Category ($\uparrow$)    & 38.89 & 38.55  & \textbf{43.15} \\
    \bottomrule
    \end{tabular}
    \end{subtable}

    \label{tab:complex_quant} 
    % \vspace{-1em}
\end{small}

\end{table*}


\textbf{Unsupervised Object Segmentation.}
Following~\cite{slotattention, dinosaur}, to measure segmentation quality, we report the foreground adjusted rand index (FG-ARI), the mean intersection over union (mIoU), and the mean best overlap (mBO), computed using the attention masks of Slot Attention.
% whose resolution is $64 \times 64$ in all models.

Our results in Table \ref{tab:complex_quant} suggest that LSD is particularly strong in visually complex scenes. For example, on the most challenging MOVi-E dataset, LSD outperforms the strongest baseline by ${>} 10\%$ in mBO and mIoU, and ${\sim} 7\%$ in FG-ARI. On CLEVRTex and MOVi-C featuring complex textures, LSD also achieves ${>} 10\%$ and ${\sim} 7\%$ gain respectively, in both mBO and mIoU. We visually show the superior segmentation quality of LSD in Figure \ref{fig:seg}. LSD obtains tighter object boundaries, less object splitting, and cleaner background segmentation. The advantages are most noticeable on CLEVRTex and MOVi-E, where the baselines over-segment the objects more frequently, or exhibit a common failure mode that divides images into approximately uniformly distributed block masks.

We also note that the FG-ARI score is sometimes not an ideal metric for evaluating segmentation quality, because it only considers the foreground pixels and disregards the correctness of the mask shape, as also highlighted by \cite{genesis,clevrtex}. In the MOVi-E experiment, even though SLATE$^+$ tends to partition the images arbitrarily into uniform patches, as shown in Figure \ref{fig:seg}, it still shows ${\sim} 5\%$ an improvement gain on FG-ARI over SLATE, which, in fact, produces more reasonable object masks. This highlights the need for additional metrics, such as the mIoU score that we also measure for evaluating the segmentation quality in this work.

\textbf{Downstream Property Prediction.}
Following~\cite{dittadi2021generalization, slotattention}, we evaluate the quality of the learned object-centric representations through downstream property prediction. Specifically, for each property, we train a network to predict the property given a frozen slot representation as input. The correspondence between the slot and its true label is determined by Hungarian matching~\cite{hungarian} using masks. We use linear heads and 2-layer MLP as the prediction networks for discrete and continuous values, respectively. We report prediction accuracy for discrete properties (\emph{e.g.}, shape and material), and mean squared error for continuous properties (\emph{e.g.}, position).

As shown in Table \ref{tab:complex_quant}, LSD is competitive with or better than the strongest baseline on CLEVRTex, MOVi-C, and MOVi-E, which is consistent with our finding in unsupervised object segmentation that LSD shines in visually complex scenes. For example, LSD obtains ${>} 4\%$ gain in predicting object category on MOVi-E and object material on CLEVRTex. 


% ########################################################################
\begin{table}[t]
\caption{\textbf{Compositional Image Generation.} On various datasets, we generate images using object slots randomly sampled from the dataset. We compare the fidelity of the generated images via the FID score. The lower the FID score, the better the fidelity. We find that our model produces the best fidelity scores compared to the baselines. }
\begin{center}
\begin{small}
    \begin{tabular}{lccc}
    \toprule
    FID $\downarrow$     & SLATE        & SLATE$^+$       & LSD (Ours)       \\
    \midrule
    CLEVR         & 52.96  & 20.93  & \textbf{16.22}  \\
    CLEVRTex      & 105.83 & 69.23 & \textbf{29.53}  \\
    MOVi-C        & 170.83 & 148.27 & \textbf{69.12}  \\
    MOVi-E        & 169.32 & 126.51 & \textbf{64.76}  \\
    FFHQ          & 112.38 & 98.76  & \textbf{27.83} \\
    \bottomrule
    \end{tabular}
    \vspace{.7em}
    \label{tab:fid} 
    % \vspace{-1em}
\end{small}
\end{center}
\end{table}


\subsection{Compositional Generation with Visual Concept Library}

% ########################################################################
\begin{figure*}[t]
\centering
\includegraphics[width=1.\textwidth]{fig/compsitional_generation.pdf}
\vskip -0.1in
\caption{\textbf{Compositional Generation Samples.} We qualitatively compare the quality of image samples generated by our model with the baselines. We observe that LSD provides significantly higher fidelity and more clear details compared to the other methods.}
\label{fig:compose}
\end{figure*}
% ########################################################################

% ########################################################################
\begin{figure*}[t]
\centering
\vskip -0.1in
\includegraphics[width=1.\textwidth]{fig/component_composition.pdf}
\caption{\textbf{Compositional Image Generation with Concept Prompts.} In this visualization, we show a concept prompt constructed by composing arbitrary slots from our visual concept library and the corresponding generated image by LSD.}
\label{fig:component}
\end{figure*}
% ######################################################################## 


Like text-to-image generative models, LSD is able to take unseen slot-based prompts at test time and compose new images. Unlike text-to-image models, however, LSD obtains the compositional generation ability solely from images without relying on additional supervision like text inputs.


As described in Section \ref{sec:visual_concept_lib}, we first build a concept library. Then, we sample one slot representation from each visual concept library and concatenate them into a sequence to form a slot-based prompt. We then feed the slot-based prompts to the diffusion decoder to generate the images. This produces scenes with novel object layouts and faces with unseen attribute combinations (see Figure~\ref{fig:component} for two examples).


We report in Table~\ref{tab:fid} the FID score~\cite{fid} as a measure of the compositional generation quality. Following standard practice~\cite{adm}, we compute the FID score using 2K generated images and the full training dataset. Across all datasets, LSD achieves significantly better FID scores than SLATE and SLATE$^+$. We further demonstrate the superior compositional generation quality of LSD in Figure~\ref{fig:compose}. We observe that on the more challenging MOVi-E and FFHQ datasets, LSD generates images with substantially more clear details and better coherence. In contrast, SLATE is limited by its decoder and produces images with missing details in the objects or human faces. While the details can be improved by the pre-trained VQGAN in SLATE$^+$, some samples still exhibit severe distortions.

\subsection{Slot-Based Image Editing}


% ########################################################################
\begin{figure}[t]
\centering
\vskip -0.1in
\includegraphics[width=1.0\textwidth]{fig/object_based_editing_horizontal.pdf}
% \caption{\textbf{Slot-Based Image Editing.} On the left, we show the results on the CLEVRTex and MOVi-E datasets, including the gradual removal of objects and the replacement of the background while maintaining the original objects. On the right, we demonstrate the face-swapping task, where the new faces are composed by combining the face slots from source B inputs with the hair, dress, and background slots from source A.}
\caption{\textbf{Slot-Based Image Editing.} \textit{Left:} We show the slot-based image editing ability of our model. In particular, we show edit operations such as object removal, object extraction, object insertion, background extraction, and background swapping. \textit{Right:} We show face replacement in the FFHQ dataset, where we compose new images by combining the face slots from Source-B images with the hairstyle, clothing, and background slots from Source-A images.}
\label{fig:edit_multi_obj}
\end{figure}
% ########################################################################


In addition to generating new images from randomly sampled slot-based prompts, LSD also allows editing existing images by directly modifying their slot representations. We demonstrate the potential of LSD for slot-based image editing through object manipulation tasks such as object removal, single object segmentation, object insertion, background extraction, and background swapping. Notably, we demonstrate, for the first time in object-centric generative models, the ability to perform face editing in real-world images. The results are shown in Figure~\ref{fig:edit_multi_obj}.


We conduct slot manipulation on the CLEVRTex dataset, focusing on object removal, single-object extraction, object insertion, background extraction, and background swapping. For object removal and background extraction, we remove an object by discarding its corresponding slot, while background extraction is achieved by discarding all object slots, leaving only the background slot. Our results show that when object decomposition is almost perfect (\emph{e.g.}, on CLEVRTex), an object can be removed simply by removing its corresponding slot. Moreover, the background component can be rendered from a single background slot, despite the model not encountering single-slot conditioning during training. In the single object extraction task, we render an individual object with the same background utilizing the corresponding object slot and the background slot. To demonstrate object insertion and background swapping tasks, we split the image into top and bottom pairs in Figure~\ref{fig:edit_multi_obj} \textit{left}. In object insertion, we introduce the slot extracted from the other image into the target image and show that the object is rendered in the image coherently. For performing the background swap, we interchange the background slots of two images. We show that the entire background of an image can be changed while correctly preserving the original objects.

We further explore face replacement on the FFHQ dataset. LSD decomposes each image into four slots, corresponding to face, hairstyle, clothing, and background. Our results show that by replacing the face slots of the images, we are able to coherently change the image while maintaining the hairstyle, clothing, and background. The resulting images look realistic, suggesting that LSD can effectively blend various attributes even when given novel combinations.


\subsection{Discussion: LSD on Simple Images}

% NEW TABLE
\begin{table*}[t]
\begin{small}
    \caption{
    % \textbf{Segmentation Performance.} We evaluate the segmentation quality and report mBO, mIoU and FG-ARI scores across various datasets and baselines. 
    \textbf{Segmentation Performance and Representation Quality in CLEVR.} \textit{Left:} We evaluate the segmentation quality and report mBO, mIoU and FG-ARI scores across various datasets and baselines. \textit{Right:} We measure the representation quality by learning a probe to predict the object property given frozen slots. For position, we report MSE. For shape and material, we report the accuracy.
    %We report the position error in pixel units.
    }
    % \vskip 0.1in
    \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Segmentation}}                 & SLATE        & SLATE$^+$       & LSD (Ours) \\
    \midrule
    mBO ($\uparrow$)         & 64.86 & \textbf{67.42}  & 38.49  \\
    mIoU ($\uparrow$)        & 63.96 & \textbf{66.62}  & 37.49  \\
    FG-ARI ($\uparrow$)      & 69.20 & \textbf{88.56}  & 76.32  \\
    \bottomrule
    \end{tabular}
    \hspace{0.2em}
    \begin{tabular}{lccc}
    \toprule
    \scriptsize{\textbf{Representation}}                 & SLATE        & SLATE$^+$       & LSD (Ours) \\
    \midrule
    Shape ($\uparrow$)       & 95.66 & \textbf{95.70}  & 75.16  \\
    Material ($\uparrow$)    & 97.62 & \textbf{97.96}  & 94.21  \\
    Position ($\downarrow$)  & 0.59  & \textbf{0.51}   & 0.86   \\
    \bottomrule
    \end{tabular}
    \label{tab:simple_quant} 
    % \vspace{-1em}
\end{small}

\end{table*}


While LSD shows significant gains in complex naturalistic scenes, we also note in Table \ref{tab:simple_quant} and in Figure \ref{fig:seg} that LSD shows a somewhat inferior performance on visually simple datasets like CLEVR
in terms of segmentation and representation. 
% We show these results . 
LSD splits certain parts of the background \emph{e.g.}, in the upper region of the image, which contributes to its sub-optimal performance on this simple dataset. 
We conjecture that this is because the diffusion model can ignore the slot-conditioning when generating these regions, giving no learning signal to the Slot Attention encoder to group them into meaningful slots. 
This may be caused by the simplicity of the background and the fact that this dataset does not contain any object in these regions, allowing our strong decoder, to memorize the rendering process for that region independently of the input from the encoder. 



\section{Conclusion}
In this work, we proposed the Latent Slot Diffusion model which can be seen in two ways: (1) the first model combining the diffusion models in unsupervised object-centric learning and (2) the first unsupervised compositional diffusion model which does not require supervised annotation like text. The main lessons of the proposed model are that it outperforms the state-of-the-art transformer-based object-centric models in various object-centric tasks. Importantly, this superiority becomes more significant as the image becomes more complex. We also show that the quality of the unsupervised compositional generation is also superior to the transformer-based object-centric models. Therefore, we believe that this is a step forward toward object-centric learning that can handle complex naturalistic images, the current main challenge. However, we also found that for simple images, the diffusion-based model tends to underperform the transformer-based model, and thus would like to investigate this in the future.

\section*{Acknowledgements}

This work is supported by Brain Pool PlusProgram (No. 2021H1D3A2A03103645) and Young Researcher Program (No. 2022R1C1C1009443) through the National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT.

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% probably should) include acknowledgements. In this case, please
% place such acknowledgements in an unnumbered section at the
% end of the paper. Typically, this will include thanks to reviewers
% who gave useful comments, to colleagues who contributed to the ideas,
% and to funding agencies and corporate sponsors that provided financial
% support.

\bibliography{refs, refs_gs, refs_ahn, refs_jd}
\bibliographystyle{plain}


\end{document}