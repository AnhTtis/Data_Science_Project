%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
%\begin{filecontents*}{example.eps}
%%!PS-Adobe-3.0 EPSF-3.0
%%%BoundingBox: 19 19 221 221
%%%CreationDate: Mon Sep 29 1997
%%%Creator: programmed by hand (JK)
%%%EndComments
%gsave
%newpath
%  20 20 moveto
%  20 220 lineto
%  220 220 lineto
%  220 20 lineto
%closepath
%2 setlinewidth
%gsave
%  .4 setgray fill
%grestore
%stroke
%grestore
%\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
%%%%%%%%%%%%%new package%%%%%%%%%%%%%%%%%
%\usepackage{amsthm}
\usepackage[hyperpageref]{backref}
\usepackage[colorlinks,linkcolor=blue,citecolor=blue]{hyperref}
%\usepackage[numbered]{mcode}
%\usepackage{slashbox}
%\usepackage[colorlinks,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage{times,amsmath,amsbsy,amssymb,amscd,mathrsfs, bm}
\newcounter{mnote}
\setcounter{mnote}{0}
\newcommand{\mnote}[1]{\addtocounter{mnote}{1}
	\ensuremath{{}^{\bullet\arabic{mnote}}}
	\marginpar{\footnotesize\em\color{red}\ensuremath{\bullet\arabic{mnote}}#1}}
\let\oldmarginpar\marginpar
\renewcommand\marginpar[1]{\-\oldmarginpar[\raggedleft\footnotesize #1]%
	{\raggedright\footnotesize #1}}
\newcommand{\dd}{\,{\rm d}}
\newcommand{\dual}[1]{\left\langle {#1} \right\rangle}
\newcommand{\mX}{\mathcal X}
\newcommand{\IX}{\mathcal{I}_{\mathcal X}}
\newcommand{\IY}{\mathcal{I}_{\mathcal Y}}
\newcommand{\IV}{\mathcal{I}_{\mathcal V}}
\newcommand{\IQ}{\mathcal{I}_{\mathcal Q}}
\newcommand{\Imu}{\mathcal I_{\mu}}

\newcommand{\breakline}{
\begin{center}
------------------------------------------------------------------------------------------------------------
\end{center}
}

\DeclareMathOperator{\sym}{sym}
%%%%%%%%%%%%%new package%%%%%%%%%%%%%%%%%
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}
	
	
	\title{Accelerated Gradient and Skew-Symmetric Splitting Methods for A Class of Monotone Operator Equations
	%\thanks{The manuscript are not currently under consideration or published in another journal.}
	}
	%\subtitle{Do you have a subtitle?\\ If so, write it here}
	
	\titlerunning{Accelerated Gradient and Skew-Symmetric Splitting Methods}        % if too long for running head
	
	%\author{First Author        
	%	 \and
	%	Second Author %etc.
	%}
	%
	%%\authorrunning{Short form of author list} % if too long for running head
	%
	%\institute{F. Author \at
	%	first address \\
	%	Tel.: +123-45-678910\\
	%	Fax: +123-45-678910\\
	%	\email{fauthor@example.com}           %  \\
	%	%             \emph{Present address:} of F. Author  %  if needed
	%	\and
	%	S. Author \at
	%	second address
	%}
	\author{
		Long Chen
		\and
		Jingrong Wei
		%etc.
	}
	\institute{
		Long Chen \at
		Department of Mathematics, University of California at Irvine, Irvine, CA 92697, USA\\
		\email{chenlong@math.uci.edu}           %  \\
		\and
		Jingrong Wei \at
		Department of Mathematics, University of California at Irvine, Irvine, CA 92697, USA\\
		\email{jingronw@uci.edu}
		%	\\
		%	Tel.: +123-45-678910\\
		%	Fax: +123-45-678910\\
		%	This author's research was supported by the China Scholarship Council (CSC) joint Ph.D. student scholarship (Grant 201806240132).
		%             \emph{Present address:} of F. Author  %  if needed
	}
	
	\date{Received: date / Accepted: date}
	% The correct dates will be entered by the editor
	
	
	\maketitle
	
	\begin{abstract}
		A class of monotone operator equations, which can be decomposed into sum of a gradient of a strongly convex function and a linear and skew-symmetric operator, is considered in this work. Based on discretization of the generalized gradient flow, gradient and skew-symmetric splitting (GSS) methods are proposed and proved to convergent in linear rate. To further accelerate the convergence, an accelerated gradient flow is proposed and accelerated gradient and skew-symmetric splitting (AGSS) methods are developed, which extends the acceleration among the existing works on the convex minimization to a more general class of monotone operator equations. In particular, when applied to smooth saddle point systems with bilinear coupling, an accelerated transformed primal-dual (ATPD) method is proposed and shown to achieve linear rates with optimal lower iteration complexity.
%		Especially, linear convergence rate is obtained even for 
%		non-strongly convex function.
		%Insert your abstract here. Include keywords, PACS and mathematical
		%subject classification numbers as needed.
		\keywords{Monotone operator \and accelerated iterative method \and dynamical system \and convergence analysis \and Lyapunov function \and saddle point problem}
		% \PACS{PACS code1 \and PACS code2 \and more}
		\subclass{37N40 \and 47H05 \and 47J25  \and 65B99 \and 65J15 \and 65L20} 
		%		65L20  	Stability and convergence of numerical methods
		%       90C25  	Convex programming	
		%		65B99   Acceleration of convergence
		%      37N40  	Dynamical systems in optimization and economics
            %   47J25 Iterative procedures involving nonlinear operators
            % 65J15 Numerical solutions to equations with nonlinear operators
            % 47H05 Monotone operators and generalizations
	\end{abstract}
	
\section{Introduction}

Consider the nonlinear equation
\begin{equation}\label{eq:Ax}
\mathcal A(x) = 0, \quad x\in \mathbb R^n,
\end{equation}
where  $\mathcal A$ is strongly monotone: there exists $\mu > 0$,  for any $x, y \in \mathbb{R}^n,$
\begin{equation}\label{eq: monotone}
(\mathcal{A}(x) - \mathcal{A}(y), x-y) \geq \mu \|x-y\|^2.
\end{equation}
We further assume $\mathcal A(x)$ can be decomposed into
\begin{equation}\label{eq:dec}
\mathcal A(x) = \nabla F(x) + \mathcal N x,
\end{equation}
with a strongly convex $C^1$ function $F$, and a linear and skew-symmetric operator $\mathcal N$, i.e., $\mathcal N^{\intercal} = -\mathcal N$. Then $\mathcal A$ is Lipschitz continuous with constant $L_{\mathcal A}\leq L_F + \| \mathcal N \|$, where $L_F$ is the Lipschitz constant of $\nabla F$. Therefore $\mathcal A$ is monotone and Lipschitz continuous which is also known as inverse-strongly monotonicity~\cite{browder1967construction,liu1998regularization} or co-coercitivity~\cite{zhu1996co}. Define the condition numbers:
\begin{equation*}
    \kappa(\mathcal A)= L_{\mathcal A}/\mu , \quad \kappa(F) = L_F/\mu, \quad \kappa(\mathcal N) = \|\mathcal N\|/\mu.
\end{equation*}
By definition $\kappa(\mathcal A) \leq \kappa(F) +  \kappa(\mathcal N)$.

Since $\mathcal A$ is strongly monotone and Lipschitz continuous, ~\eqref{eq:Ax} has a unique solution $x^{\star}$~\cite{rockafellar1976monotone}. The goal of this paper is to design iterative methods for computing $x^{\star}$ based on the splitting~\eqref{eq:dec}, which will be called Gradient and skew-Symmetric Splitting (GSS) methods and Accelerated Gradient and skew-Symmetric Splitting (AGSS) method for its acceleration. 
%Among the iterative methods for the considered monotone operator equations we do not assume any differentiability of $\mathcal A$ and use only point evaluation information. It is indeed zeroth-order methods with respect to $\mathcal A$ and as $\mathcal A$ contains $\nabla F$, it is first-order with respect to $F$. 
%We shall develop iterative methods 
%In the convex minimization and saddle point problem regimes, the first-order methods correspond to using gradient oracles and possibly proximal mappings for objectives. 

A fundamental tool to solve~\eqref{eq:Ax} is the proximal point method~\cite{rockafellar1976monotone}. More specific class of monotone operator equation is particularly of interest, since it is closely related to convex optimization and saddle point problems. For instance, when $\mathcal N = 0$,~\eqref{eq:Ax} becomes the convex minimization problem 
\begin{equation}\label{eq: convex min}
    \min_x F(x).
\end{equation}
We refer to the book~\cite{nesterov2003introductory} on the first-order methods for solving~\eqref{eq: convex min} especially the accelerated gradient method which achieved the linear rate $1-1/\sqrt{\kappa(F)}$ with optimal lower complexity bound.
%where $F$ defines a strongly convex function. Inspired by
%Nesterov’s gradient method~\cite{nesterov1988approach},~\cite{guler1992new} developed accelerated proximal point algorithms for convex minimization, one of which is an instance of FISTA~\cite{beck2009fast}. 

Another important application is the strongly-convex-strongly-concave saddle point problem with bilinear coupling:
    \begin{equation}\label{eq:saddle}
        \min_{u\in \mathbb R^m} \max_{p\in \mathbb R^n} \mathcal L(u,p) := f(u) - g(p) + (Bu, p),
    \end{equation}
where $f, g$ are smooth, strongly convex functions with Liptschitz continuous gradients and $B\in \mathbb R^{n\times m}, m\geq n,$ is a full rank matrix. By a sign change, the saddle point system $\nabla \mathcal L(u,p)=0$ becomes~\eqref{eq:Ax} with $x = (u,p)^{\intercal}$, $F(u,p) = f(u) + g(p)$ and $\mathcal N = \begin{pmatrix} 0 & B^{\intercal} \\ -B & 0 \end{pmatrix}$. The primal-dual algorithms for solving~\eqref{eq:saddle}, such as the augmented Lagrangian method (ALM)~\cite{hestenes1969multiplier,powell1969method}, the primal-dual hybrid gradient (PDHG) method~\cite{chambolle2011first,chambolle2016ergodic,he2012convergence,zhu2008efficient} and the alternating direction method of multipliers (ADMM)~\cite{gabay1976dual,glowinski1975approximation}, are all based on the proximal point method applying to the primal or dual problems. Under the strong monotonicity condition, the  accelerated proximal point method developed in~\cite{kim2021accelerated}, combining restart techniques, has a linear rate. Consider the pure first-order iterative methods: the sequence $\left\{\left(u_k, p_k\right)\right\}_{k=0,1, \ldots }$ is generated so that $\left(u_k, p_k\right) \in \mathcal{H}^u_k \times \mathcal{H}^p_k$, with $\mathcal{H}^u_0=\operatorname{Span}\left\{u_0\right\}$, $\mathcal{H}^p_0=\operatorname{Span}\left\{p_0\right\}$, and
$$
\begin{array}{l}
\mathcal{H}^u_{k+1}:=\operatorname{Span}\left\{u_i, \nabla_u \mathcal L \left(\tilde{u}_i, \tilde{p}_i\right): \forall \tilde{u}_i \in \mathcal{H}^u_i, \tilde{p}_i \in \mathcal{H}^p_i, 0 \leq i \leq k\right\}, \\
\mathcal{H}^p_{k+1}:=\operatorname{Span}\left\{p_i, \nabla_p \mathcal L\left(\tilde{u}_i, \tilde{p}_i\right): \forall \tilde{u}_i \in \mathcal{H}^p_i, \tilde{p}_i \in \mathcal{H}^u_i, 0 \leq i \leq k\right\}.
\end{array}
$$
In~\cite{zhang2022lower}, it is shown that if the duality gap $\Delta(u,p) := \max_q \mathcal L(u, q) -  \min_v \mathcal L(v, p) $ 
% \mnote{ is it always positive? \JW{Yes. $\max_q \mathcal L(u, q) \geq   L(u, p^{\star}) \geq  \mathcal L(u^{\star}, p^{\star}) \geq \mathcal L(u^{\star}, p) \geq  \min_v \mathcal L(v, p)$}} 
is required to be bounded by $\epsilon_{\rm out}$, the number of iterations needed is at least $$\Omega\left(\sqrt{\kappa(f)+\kappa^2(\mathcal N)+\kappa(g)} \cdot |\ln \epsilon_{\rm out} |\right)$$ where $\kappa(f), \kappa(g)$ are condition numbers of $f, g$ respectively, $\kappa(\mathcal N) = \|B\|/\sqrt{\mu_f \mu_g}$ measuring the coupling, and $\Omega$ means the iteration complexity bounded below asymptotically, up to a constant. A few optimal first-order algorithms are developed in recent literature~\cite{kovalev2021accelerated,thekumparampil2022lifted}. If further the proximal oracles for $f, g$ are allowed, the lower complexity bound of the class of first-order algorithms is $\displaystyle \Omega\left (\kappa(\mathcal N)|\ln \epsilon_{\rm out}| \right)$. We refer to~\cite{chambolle2011first,chambolle2016ergodic} for an optimal proximal method. 
%\LC{Define $\kappa(\mathcal A), \kappa(F)$, and $\kappa(\mathcal N)$. where $\kappa(\mathcal A)= L_{\mathcal A}/\mu$ is the condition number of $\mathcal A$. where $\kappa(F) = L/\mu$ and $\kappa(\mathcal N)$ is defined.}
%a Krylov subspace method like the GMRES~\cite{saad1986gmres} can be used to solve the nonsymmetric linear system.

One more important class is for linear operator $\mathcal A$ which has the following decomposition:
$$
\mathcal A = \mathcal A^{\rm sym} + \mathcal A^{\rm skew},
$$
where $\mathcal A^{\rm sym} = (\mathcal A + \mathcal A^{\intercal})/2$ is the symmetric (Hermitian for complex matrices) part and $\mathcal A^{\rm skew} = (\mathcal A - \mathcal A^{\intercal})/2$ is the skew-symmetric part. The condition $\mathcal A$ is monotone is equivalent to  $\mathcal A^{\rm sym}$ is symmetric and positive definite (SPD) and $\lambda_{\min}( \mathcal A^{\rm sym})\geq \mu$. Bai, Golub, and Ng in~\cite{bai2003hermitian} proposed the Hermitian/skew-Hermitian splitting method (HSS) for solving general non-Hermitian positive definite linear systems $\mathcal Ax  = b$:
\begin{equation}\label{eq:HSS}
    \begin{aligned}
    (\alpha I+ \mathcal A^{\rm sym}) x_{ k+\frac{1}{2}} &=(\alpha I-\mathcal A^{\rm skew}) x_{k}+b \\ 
    (\alpha I+\mathcal A^{\rm skew}) x_{k+1} &=(\alpha I-\mathcal A^{\rm sym}) x_{k+\frac{1}{2}}+b.
\end{aligned}
\end{equation}
The iterative method~\eqref{eq:HSS} solves the equations for the symmetric (Hermitian) part and skew-symmetric (skew-Hermitian) part alternatively. For the HSS method~\eqref{eq:HSS}, efficient solvers for linear operators $(\alpha I + \mathcal A^{\rm sym})^{-1}$ and $(\alpha I+ \mathcal A^{\rm skew})^{-1}$ are needed. A linear convergence rate of $\displaystyle \frac{\sqrt{\kappa (\mathcal A^{\rm sym})}-1}{\sqrt{\kappa (\mathcal A^{\rm sym})}+1}$ can be achieved for an optimal choice of parameter $\alpha$. Several variants of the method are derived and analyzed in~\cite{bai2007accelerated,bai2007successive,bai2008inexact}. Benzi and Golub~\cite{benzi2004preconditioner} applied the HSS method for solving the linear saddle point problems with preconditioning strategy. 
%It is shown that the HSS iteration is unconditionally convergent for a class of non-singular generalized saddle point problems.

For a general non-singular matrix $\mathcal A$, $\mathcal A^{\rm sym}$ may not be positive definite. But we can always find a transformation operator $\mathcal M$ s.t. $\mathcal A\mathcal M$ admits the decomposition~\eqref{eq:dec} and the transformed system has a similar or better condition number. Indeed the existence  of the solution to the equation $\mathcal Ax =b$ can be interpreted as the following inf-sup condition:
\begin{equation}
\inf_{y\in \mathbb R^n}\sup_{x\in \mathbb R^n} \frac{(\mathcal A x, y)}{\|x\|\|y\|}\geq \mu.
\end{equation}
The inf-sup condition is equivalent to: for any $y\in \mathbb R^n$, there exists $x \in \mathbb R^n$, such that
\begin{equation}\label{eq: inf-sup eqv 1}
(\mathcal A x, y)\geq C_1\|y\|^2, \quad \hbox{ and }\; \|x\|\leq C_2\|y\|.
\end{equation}
If we can find a linear operator $\mathcal M$ s.t. $x = \mathcal My$ satisfying~\eqref{eq: inf-sup eqv 1}, then $\mathcal A\mathcal M$ will satisfy~\eqref{eq:dec} with the symmetric and skew-symmetric decomposition. A large class of iterative methods for solving the linear saddle point systems can be derived by different choices of $\mathcal M$; see~\cite{Benzi.M;Golub.G;Liesen.J2005} and the references therein. A transformation has also been proposed for nonlinear saddle point problems recently in~\cite{ChenWei2022Transformed} and the resulting algorithm is called the transformed primal-dual (TPD) method.

%For the saddle point problem, recall the Ladyzhenskaya–Babuška–Brezzi condition~\cite{babuvska1973finite, Brezzi1974}:
%Suppose that $a: V \times V \to \mathbb{R}$ and $b: V \times Q \to \mathbb{R}$ are continuous bilinear forms, and
%$$a(v,v)\geq \alpha \|v\|_{V}^{2},$$
%for all $v$ such that $b(v,q)=0$ for all $q\in Q$. If $b$ satisfies the inf–sup (or Ladyzhenskaya–Babuška–Brezzi) condition
%$$\inf_{q\in Q,q\neq 0} \sup _{v\in V,v\neq 0} \frac {b(v,q)}{\|v\|_{V} \|q\|_{Q}}\geq \beta \|q\|_{Q}.$$
%for all $q$ and for some $\beta >0$, then there exists a unique solution of the saddle point problem
%\begin{equation*}
%\begin{aligned}a(u,v)+b(v,p)&=\langle f,v\rangle, \\b(u,q)&=\langle g,q\rangle .\end{aligned}
%\end{equation*}
%The non-singular $\mathcal A$ and transformation $\mathcal M$ basically guarantee $\mathcal A \mathcal M$ satisfiy the inf-sup condition.

We shall develop our iterative methods based on discretization of ODE flows. Namely we treat $x(t)$ as a continuous function of an artificial time variable $t$ and design ODE systems $x'(t) = \mathcal G(x(t))$ so that the $x^{\star}$ is a stable equilibrium point of the corresponding dynamic system, i.e., $\mathcal G(x^*)=0$ and $\lim_{t\to \infty} x(t) = x^*$. Then we apply ODE solvers to obtain various iterative methods. By doing this way, we can borrow the stability analysis for dynamic systems and convergence theory of ODE solvers. In particular, the dynamic systems and accelerated iterative methods for the convex minimization~\eqref{eq: convex min} is analyzed via a unified Lyapunov-based approach in~\cite{chen2021unified}; see also~\cite{attouch2019convergence} for maximally monotone operators. However, there is few investigation on the ODE flows for~\eqref{eq:Ax}, and iterative methods with accelerated linear rates. 
%\mnote{ more references?}
%The dynamic systems and accelerated iterative methods of the convex minimization~\eqref{eq: convex min} is analyzed via a unified Lyapunov-based approach in~\cite{chen2021unified}. However, there is few investigation on the gradient dynamic for~\eqref{eq:Ax}, in particular the iterative methods with accelerated linear rates.

 
The most popular choice is the generalized gradient flow:
\begin{equation}\label{eq:intro gradient flow}
x^{\prime}(t) = - \mathcal{A}(x(t)).
\end{equation}
Although $\mathcal A$ is not the gradient of some scalar function when $\mathcal N\neq 0$, we still use the name convention for the convex case $\mathcal N = 0$. 
%where the derivative is taking respect to the artificial time variable $t$. 
%Define the Lyapunov function: $\mathcal{E}_q(x) = \frac{1}{2}\|x-x^{\star}\|^2.$
One can easily show the exponential stability of $x^{\star}$ using the monotonicity of $\mathcal A$.
%
%by computing $\mathcal{E}_q^{\prime}(x(t))$ :
%$$
%\mathcal{E}_q(x(t))\leq e^{-2\mu t}\mathcal E_q(x(0)).
%$$
Discretization for the generalized gradient flow~\eqref{eq:intro gradient flow} leads to iteration methods for solving~\eqref{eq:Ax}. The implicit Euler scheme with step size $\alpha_k$ is the unconditionally stable proximal method $(I + \alpha_k \mathcal A) (x_{k+1}) =  x_{k}$. The explicit Euler scheme gives the generalized gradient descent method:
\begin{equation}
    x_{k+1} = x_k - \alpha_k \mathcal{A}(x_k).
\end{equation}
For $\displaystyle \alpha_k =  \mu/L_{\mathcal A}^2$, we have the linear convergence
$$
\| x_{k+1} - x^{\star} \|^2 \leq
% \left( \frac{1}{1+1/\kappa^2(\mathcal A)}\right )\mathcal{E}_q(x_{k}).
 \left( 1- 1/\kappa^2(\mathcal A)\right )\| x_{k+1} - x^{\star} \|^2.
$$
But this linear rate is pretty slow when $\kappa(\mathcal A)\gg 1$ since the dependence is $\kappa^2(\mathcal A)$. To achieve the accuracy $\| x_{k+1} - x^{\star} \|^2 \leq \epsilon_{\rm out} $, $\mathcal O(\kappa^2(\mathcal A) |\ln \epsilon_{\rm out} |)$ iterations are needed. In contrast, for convex optimization, i.e., $\mathcal N = 0$, the rate for  the gradient descent method is  $1-1/\kappa(F)$ for step size $\alpha_k = 1/L_F$. 

To speed up the convergence, we explore more property from the decomposition~\eqref{eq:dec}. Let $B^{\intercal} = {\rm upper}(\mathcal N)$ be the upper triangular part of $\mathcal N$ and $B^{\rm sym} = B + B^{\intercal}$ be a symmetrization of $B$. Based on the splitting $\mathcal N = B^{\rm sym}-2B$ or $\mathcal N = 2 B^{\intercal} - B^{\rm sym}$, we develop Gradient and skew-Symmetric Splitting (GSS) methods: 
\begin{equation}\label{eq: intro nonlinear SOR 1}
    \frac{x_{k+1} - x_k}{\alpha} = -\left (\nabla F(x_k)+ B^{\rm sym}x_k - 2B x_{k+1}\right),
\end{equation}
or
\begin{equation}\label{eq: intro nonlinear SOR 2}
    \frac{x_{k+1} - x_k}{\alpha} = - \left (\nabla F(x_k) - B^{\rm sym}x_k + 2B^{\intercal} x_{k+1} \right).
\end{equation}
Notice that  $B$ is lower triangular, both~\eqref{eq: intro nonlinear SOR 1} and~\eqref{eq: intro nonlinear SOR 2} are explicit schemes as $x_{k+1}$ can be computed via forward (for~\eqref{eq: intro nonlinear SOR 1}) or backward (for~\eqref{eq: intro nonlinear SOR 2}) substitution. 
We prove that they achieve the linear convergence rate
\begin{equation*}
\| x_k - x^{\star}\|^2 \leq \left (\frac{1}{1 + 4/\max \{\kappa(B^{\rm sym}), \kappa(F)\} }\right )^{k} 6\| x_0 - x^{\star}\|^2.
\end{equation*}
where $\kappa (B^{\rm sym} )=L_{B^{\rm sym}}/\mu$ with $L_{B^{\rm sym}} = \|B^{\rm sym}\|$ for $ \alpha = \displaystyle \min \left\{\frac{1}{4L_{B^{\rm sym}}}, \frac{1}{4L_F} \right\}$. In particular for the linear problem $\nabla F = \mu I$, this can be viewed as accelerated overrelaxation (AOR) methods ~\cite{Hadjidimos:1978Accelerated,hadjidimos2000successive} for a linear shifted skew-symmetric system.

To further accelerate the convergence rate, following ~\cite{luoDifferentialEquationSolvers2021a}, we introduce an accelerated gradient flow
\begin{equation}\label{eq:intro AG}
\left \{\begin{aligned}
     x^{\prime} &= y - x ,\\
     y^{\prime} & = x - y - \frac{1}{\mu}(\nabla F(x) + \mathcal N y).
\end{aligned}\right .
\end{equation}
Comparing with the accelerated gradient flow in~\cite{luoDifferentialEquationSolvers2021a} for convex optimization, the difference is the gradient and skew-symmetric splitting $\mathcal A(x) \rightarrow \nabla F(x) + \mathcal N y$.

We propose several iterative schemes based on various discretization of \eqref{eq:intro AG}. Provided that a fast solver for computing the shifted skew-symmetric operator $ (\beta  I + \mathcal N)^{-1}$ is available, we give an implicit-explicit (IMEX) Euler scheme for~\eqref{eq:intro AG} as a version of Accelerated Gradient and skew-Symmetric Splitting (AGSS) methods:
\begin{equation}\label{eq: intro IMEX}
\begin{aligned}
   \frac{\hat{x}_{k+1}-x_k}{\alpha_k} &=  y_k - \hat{x}_{k+1},  \\
  \frac{y_{k+1}-y_k}{\alpha_k} &=  \hat{x}_{k+1} - y_{k+1} -\frac{1}{\mu} \left( \nabla F(\hat{x}_{k+1}) + \mathcal Ny_{k+1}\right) , \\
  \frac{x_{k+1}-x_k}{\alpha_k} &= y_{k+1} - x_{k+1}.
\end{aligned}
\end{equation}
The scheme \eqref{eq: intro IMEX} is implicit for $y_{k+1}$ as each iteration needs to solve a shifted skew-symmetric equation $(\beta  I + \mathcal N)y_{k+1} = b(\hat x_{k+1}, y_k)$  with $\beta = 1 +\mu/\alpha_k$. For the fixed step size $\alpha_k = 1/\sqrt{\kappa(F)}$, the convergence rate is  accelerated to 
\begin{equation*}
\|x_{k+1}-x^{\star}\|^2 + \|y_{k+1}-x^{\star}\|^2 \leq \left (\frac{1}{1+1/\sqrt{\kappa(F)}}\right )^{k} 2 \mathcal{E}_0/\mu,
\end{equation*}
where $\mathcal{E}_0  = D_F(x_0, x^{\star}) + \frac{\mu}{2}\|y_0-x^{\star}\|^2$ and $D_F$ is the Bregman divergence of $F$. We also allow an inexact solver for approximating $( \beta I +  \mathcal N)^{-1}$ using perturbation argument to control the inner solve error. For linear systems, comparing with HSS, we achieve the same accelerated rate without treating the symmetric part implicitly, i.e., no need to compute $(\alpha I + \mathcal A^{\rm sym})^{-1}$. 

To fully avoid computing $( \beta I +  \mathcal N)^{-1}$, we provide an explicit AGSS scheme combining acceleration of $F$ and AOR technique for $\mathcal N$:
\begin{equation}\label{eq:intro-agss}
\begin{aligned}
  \frac{\hat x_{k+1}-x_k}{\alpha} &=  y_k - \hat x_{k+1}, \\
   \frac{y_{k+1}-y_k}{\alpha} &= \hat x_{k+1} - y_{k+1}- \frac{1}{\mu}\left( \nabla F(\hat x_{k+1}) + B^{\rm sym} y_k - 2B y_{k+1} \right), \\
   \frac{x_{k+1}-x_{k}}{\alpha} &= y_{k+1} -\frac{1}{2} (x_{k+1} + \hat x_{k+1}).
\end{aligned}
\end{equation}
For the step size $\displaystyle \alpha= \min \left \{\frac{\mu}{2L_{B^{\rm sym}}}, \sqrt{\frac{ \mu}{2L_F}}\right \}$, we obtain the accelerated linear convergence for scheme \eqref{eq:intro-agss}
$$
\|x_{k+1}-x^{\star}\|^2 \leq \left (\frac{1}{1 + 1/\max\{4\kappa(B^{\rm sym}), \sqrt{8\kappa(F)}\}}\right )^{k}\frac{2\mathcal E_0^{\alpha B}}{\mu}
$$
where $\mathcal E^{\alpha B}_0  = D_F(x_0, x^{\star}) + \frac{1}{2}\|y-x^{\star}\|^2_{\mu I - 2\alpha B^{\rm sym}}$ is nonnegative according to our choice of the step size.

%\LC{ briefly mention the results for saddle point systems and emphasize we achieve the optimal lower complexity bound.}
The proposed accelerated methods has wide applications. This extends the acceleration among the existing works on the convex minimization~\cite{guler1992new,nesterov1983method,polyak1964some} to a more general class of monotone operator equations. As an example, we apply to smooth strongly-convex-strongly-concave saddle point systems with bilinear coupling~\eqref{eq:saddle} and derive first-order algorithms achieving optimal  lower iteration complexity $\Omega\left(\sqrt{\kappa(f)+\kappa^2(\mathcal N)+\kappa(g)} \cdot |\ln \epsilon_{\rm out} |\right)$; see Section \ref{sec: Strongly-Convex-Strongly-Concave Saddle Point Problem} for variants and details.  

For affinely constrained optimization problems:
\begin{equation*}
    \begin{aligned}
    \min_{u\in \mathbb{R}^m} f(u)\\
    \text{subject to} \quad Bu = b,
    \end{aligned}
\end{equation*}
we combine the transformed primal-dual method developed in~\cite{ChenWei2022Transformed} and the accelerated gradient flow to propose an accelerated transformed primal-dual (ATPD) method:
\begin{equation}\label{eq:ATPD}
\begin{aligned}
      \frac{\hat x_{k+1}-x_k}{\alpha}&= y_k - \hat x_{k+1} ,\\
    \frac{v_{k+1}-v_k}{\alpha} &= \frac{1}{2}(\hat u_{k+1} - v_{k+1}) - \frac{1}{\mu_{f}}\IV^{-1}(\nabla f( \hat u_{k+1}) + B^{\intercal}q_k), \\
   \frac{q_{k+1} - q_k}{\alpha}&= \hat p_{k+1}-q_{k+1} - \IQ^{-1}(Sp_{k+1}+ Bv_{k} - 2Bv_{k+1}+B\IV^{-1}f(u_{k+1})),\\
  \frac{x_{k+1} - \hat x_{k+1}}{\alpha} &=(y_{k+1} - y_k) - \frac{1}{4} (x_{k+1} - \hat x_{k+1}).
\end{aligned}
\end{equation}
where $x = (u,p)$, $y = (v,q)$, $\IV, \IQ$ are SPD operators and $S = B\IV^{-1}B^{\intercal}$. The complexity of the explicit scheme~\eqref{eq:ATPD} is $$\mathcal O(\sqrt{\kappa_{\IV}(f)}\,  \kappa_{\IQ}(S)|\ln \epsilon_{\rm out} |)$$ which suggests that $\IQ^{-1}$ is better chosen as a preconditioner for the Schur complement $S = B\IV^{-1}B^{\intercal}$. In particular, when $\IV$ is a scaled identity and $\IQ = S$, the outer iteration complexity is $\mathcal O(\sqrt{\kappa_{\IV}(f)}|\ln \epsilon_{\rm out} |)$. 
Iterative methods for computing $S^{-1} r$ can be thought of as an inner iteration which can be a linear inner solver or a non-linear one. The linear solver will enter the estimate by the factor $\kappa_{\IQ}(S)$. The non-linear inner solver for $S^{-1}$, e.g., the conjugate gradient method, can be estimated by the perturbation argument on controlling the residual $\varepsilon_{\rm in}$ developed in Section \ref{sec: Inexact Solver for the Shifted Skew Symmetric System}. The total iteration complexity for the accelerated transformed primal-dual method is 
$$
\mathcal O\left (|\ln \epsilon_{\rm out} ||\ln \varepsilon_{\rm in} |\sqrt{\kappa_{\IV}(f)\, \kappa_{\IQ}(S)}\right ),
$$
which achieves the optimal complexity bound for affinely constrained problems~\cite{salim2022optimal}.


% This means provided an efficient inner solver for $(BB^{\intercal})^{-1}$, for instance, the conjugate gradient method with inner iteration complexity $\mathcal O(\kappa(B))$, then the total iteration complexity for the accelerated transformed primal-dual method is $\mathcal O(\sqrt{\kappa(f)}\kappa(B))$, which is the optimal bound for affinely constrained problems~\cite{salim2022optimal}. 

The strong convexity of $f$ can be further relaxed to \begin{equation*}
    f_{\beta}(u) = f(u) +\frac{\beta}{2}\|Bu-b\|^2 
\end{equation*}
by using augmented Lagrangian method (ALM). Then $\mu_{f}>0$ can be relaxed to $\mu_{f_\beta} >0$; see discussion in Section \ref{sec: Convex-Concave Saddle Point Problem}.

For the rest of the paper, we first review convex function theory and Lyapunov analysis theory in Section \ref{sec: preliminary}, as basic preliminaries for our proofs. The generalized gradient flow and convergence of GSS methods for the considered monotone equation are proposed in Section \ref{sec: gradient methods}. Then in Section \ref{sec:acc flow and schemes}, we extend the accelerated gradient flow in convex minimization and derive AGSS methods with accelerated linear rates. As applications, we propose accelerated first-order algorithms for convex-concave saddle point problems with bilinear coupling in Section \ref{sec:saddle point systems}. In particular, we give optimal algorithms for strongly-convex-strongly-concave saddle point systems. Concluding remarks are addressed in Section \ref{sec: conclusion}.




\section{Preliminary}\label{sec: preliminary}
In this section, we follow~\cite{nesterov2003introductory} to introduce notation and  preliminaries in convex function theory. We also briefly review a unified convergence analysis of first order convex optimization methods via strong Lyapunov functions established in~\cite{chen2021unified}. 

\subsection{Convex function}
Let $\mX$ be a finite-dimensional Hilbert space with inner product $(\cdot, \cdot)$ and norm $\|\cdot \|$. $\mX^{\prime}$ is the linear space of all linear and continuous mappings $T: \mX \rightarrow \mathbb{R}$, which is called the dual space of $\mX$, and $\langle \cdot, \cdot \rangle$ denotes the duality pair between $\mX$ and $\mX^{\prime}$. For any proper closed convex and $C^1$ function $F: \mX \rightarrow \mathbb{R}$, the Bregman divergence of $F$ is defined as
$$D_{F}(x,y) : = F(x)-F(y)-\langle \nabla F(y), x-y\rangle.$$
We say $F \in \mathcal{S}_{\mu}$ or $\mu$-strongly convex with $\mu \geqslant 0$ if $F$ is differentiable and  
$$
D_{F}(x, y)  \geqslant \frac{\mu}{2}\left\|x-y\right\|^{2}, \quad \forall x, y \in \mX.
$$
 In addition, denote $F \in \mathcal{S}_{\mu, L}$ if $F \in \mathcal{S}_{\mu}$ and there exists $L>0$ such that
$$
D_{F}(x, y) \leqslant \frac{L}{2}\left\|x-y\right\|^{2}, \quad \forall x, y \in \mX.
$$
For fixed $y \in \mX, D_{F}(\cdot, y)$ is convex as $F$ is convex and 
$$
\nabla D_{F}(\cdot, y) = \nabla F(\cdot) - \nabla F(y).
$$
For 
$F \in \mathcal{S}_{\mu, L} $, we have
$$\frac{\mu}{2}\|x-y\|^2 \leq D_{F}(x, y) \leq \frac{L}{2}\|x-y\|^2.$$
Especially for $F(x)=\frac{1}{2}\|x\|^{2}$, Bregman divergence reduces to the half of the squared distance $D_{F}(x, y)=D_{F}(y, x)=\frac{1}{2}\|x-y\|^{2}$. In general $D_F(x, y)$ is non-symmetric in terms of $x$ and $y$. 
%We may use the notation $D_F(x; y)$ to emphasize the Bregman divergence is treated as a function with respect to $x$ for some fixed $y$. 
%, i.e., 
%$$D_f(v, u) \neq D_f(u, v) \quad \text{ for } u\neq v.$$ 
A symmetrized Bregman divergence is defined as
\begin{equation*}\label{eq: symmetrized Bregman divergenve}
    \langle\nabla F(y)-\nabla F(x), y-x\rangle=D_{F}(y, x)+D_{F}(x, y).
\end{equation*}
By direct calculation, we have the following three-terms identity.
\begin{lemma}[Bregman divergence identity~\cite{chen1993convergence}]
If function $F: \mX \rightarrow \mathbb{R}$ is differentiable, then for any $x, y, z \in \mX$, it holds that
\begin{equation}\label{eq: Bregman divergence identity}
   \langle\nabla F(x)-\nabla F(y), y-z\rangle=D_{F}(z, x)-D_{F}(z, y)-D_{F}(y, x). 
\end{equation}
\end{lemma}
%\begin{proof}
%By definition,
%$$
%\begin{aligned}
%D_{f}(w, u)&=f(w)-f(u)-\langle\nabla f(u), w-u\rangle, \\
%D_{f}(w, v)&=f(w)-f(v)-\langle\nabla f(v), w-v\rangle, \\
%D_{f}(v, u)&=f(v)-f(u)-\langle\nabla f(u), v-u\rangle.
%\end{aligned}
%$$
%Direct calculation gives the identity.
%\end{proof}
%Let $A$ be a symmetric and positive definite (SPD) matrix which can introduce a new inner product $(u,v)_A := (Au, v)$ and $\|u\|_A^2:= (u,u)_A$. 
%When $f(u) = \frac{1}{2}\|u\|_A^2$,~\eqref{Bregman divergence identity equation} becomes 
%\begin{equation*}
% (u-v, v-w)_A =  \frac{1}{2}\|w-u\|_A^2 + \frac{1}{2}\|w-v\|_A^2 - \frac{1}{2}\|v-u\|_A^2.
%\end{equation*}
When $F(x) = \frac{1}{2}\|x\|^2$, identity~\eqref{eq: Bregman divergence identity} becomes 
\begin{equation*}
 (x-y, y-z) =  \frac{1}{2}\|z-x\|^2 - \frac{1}{2}\|z-y\|^2 - \frac{1}{2}\|y-x\|^2.
\end{equation*}
%In general identity~\eqref{eq: Bregman divergence identity} will be used to control the cross term. 
%\breakline

For a non-smooth function $F$, the subgradient is a set-valued function defined as
$$\partial F(x):=\left\{\xi \in \mathcal X^{\prime}: F(y)-F(x) \geqslant\langle \xi, y-x\rangle, \quad \forall y \in \mathcal X \right\}.$$
For a proper, closed and convex function $F: \mathcal X \to \mathbb{R}$, $\partial F(x)$ is a nonempty bounded set for $x\in \mathcal X$~\cite{nesterov2003introductory}. It is evident that the subgradient for smooth $F$ is a single-valued function reduced to the gradient, that is $\partial F(x) = \{\nabla F(x)\}$. We extend the set $\mathcal S_{\mu}$ using the notion of subgradient: $F \in \mathcal S_{\mu}$ if for all $x, y \in \mathcal X$,
$$F(y) \geqslant F(x)+\langle \xi, y-x\rangle+\frac{\mu}{2}\|x-y\|^2, \quad \forall \xi \in \partial F(x).$$
Similar to the smooth case, if $F \in \mathcal{S}_\mu$, then
$$
\langle \xi-\eta, x-y\rangle \geqslant \mu\|x-y\|^2, \quad \forall x, y \in \mathcal X, \xi \in \partial F(x), \eta \in\partial F(y).
$$
%\breakline

Given a convex function $F$, define the proximal operator of $F$ as
\begin{equation}\label{eq:proxl2}
     \operatorname{prox}_{\gamma F}(y):=\underset{x}{\operatorname{argmin}} ~F(x)+\frac{1}{2 \gamma}\|x-y\|^2,
\end{equation}
for some $\gamma > 0$. The proximal operator is well-defined since the function $F(\cdot)+\frac{1}{2 \gamma}\|\cdot-y\|^2$ is strongly convex. 

\subsection{Inner product and preconditioners} 
The standard $l^{2}$ dot product of Euclidean space $(\cdot,\cdot)$ is usually chosen as the inner product and the norm induced is the Euclidean norm. We now introduce inner product $(\cdot, \cdot)_{\IX}$ induced by a given SPD operator $\IX: \mX\to \mX$ defined as follows
$$
(x, y)_{\IX}:=(\IX x, y) = (x, \IX y), \quad \forall x, y \in \mX
$$ and associated norm $\|\cdot\|_{\IX}$, given by
$\|x\|_{\IX}= (x, x)^{1/2}_{\IX}.$
The dual norm w.r.t the $\IX$-norm is defined as: for $\ell \in \mX^{\prime}$
$$
\|\ell\|_{\mX^{\prime}}=\sup _{0 \neq x \in \mX} \frac{\langle\ell, x\rangle}{\| x \|_{\IX}} =\left( \ell, \ell\right )_{ \IX^{-1}}^{1/ 2} = \left( \IX^{-1} \ell, \ell\right )^{1/ 2}.
$$
%where $\langle \cdot, \cdot \rangle$ is the duality pair. 
%%By Riesz representation theorem, $\ell$ can be also treated as a vector and, 
%It is straightforward to verify that
%$$
%\|\ell\|_{\mX^{\prime}}=\|\ell\|_{\IX^{-1}}:=\left( \ell, \ell\right )_{ \IX^{-1}}^{1/ 2} := \left( \IX^{-1} \ell, \ell\right )^{1/ 2}.
%$$

We shall generalize the convexity and Lipschitz continuity with respect to $\IX$-norm: we say $F \in \mathcal{S}_{\mu_{F, \IX}}$ with $\mu_{F, \IX} \geqslant 0$ if $F$ is differentiable and
$$
D_{F}(x, y)\geqslant \frac{\mu_{F, \IX}}{2}\left\|x-y\right\|^{2}_{\IX}, \quad \forall x, y  \in \mX.
$$
In addition, denote $f \in \mathcal{S}_{\mu_{F, \IX}, L_{F, \IX}}$ if $f \in \mathcal{S}_{\mu_{F, \IX}}$ and there exists $L_{f, \IX}>0$ such that
$$
D_{F}(x,y) \leq \frac{L_{f, \IX}}{2}\left\|x-y\right\|^{2}_{\IX}, \quad \forall  x, y   \in \mX.
$$
%Under this definition, the default norm is a special case with $\IX = I$ for which the subscript will be skipped, i.e., $\mu_F, L_F$ for $\|\cdot \|$. 
The gradient method in the inner product $ \IX$ reads as:
\begin{equation}\label{eq:PGD}
x_{k+1} = x_k - \IX^{-1} \nabla F(x_k),
\end{equation}
where $\IX^{-1}$ can be understood as a preconditioner. The convergence rate of the preconditioned gradient descent iteration~\eqref{eq:PGD} is $1-1/\kappa_{\IX}(F)$ with condition number $\kappa_{\IX}(F) = L_{F, \IX}/\mu_{F, \IX}$. 
To simplify notation, we skip $\IX$ in the constants $\mu$ and $ L$, e.g., write $\mu_{F, \IX}$ as $\mu_F$, but keep in the condition number, e.g. $\kappa_{\IX}(F)$, to emphasize that the condition number is measured in the $\IX$ inner product.

For two symmetric operators $A, B: \mX\to \mX$, we say $A \geq (>) B$ if $A  -  B$ is positive semidefinite (definite). Therefore $A> 0 $ means $A$ is SPD. One can easily verify that $A\geq B$ is equivalent to $\lambda_{\min}(B^{-1}A)\geq 1$ or $\lambda_{\max}(A^{-1}B)\leq 1$ for two non-singular and symmetric operators. 
  

\subsection{Lyapunov analysis}
%We shall use the Lyapunov analysis for our convergence analysis. 
In order to study the stability of an equilibrium $x^*$ of a dynamical system defined by an autonomous system
\begin{equation}\label{autonomous system}
    x' = \mathcal{G}(x(t)),
\end{equation}
Lyapunov introduced the so-called Lyapunov function $\mathcal{E}(x)$~\cite{Khalil:1173048,Haddad2008}, which is nonnegative and the equilibrium point $x^*$ satisfies $\mathcal{E}\left(x^{*}\right)=0$ and the Lyapunov condition:
$-\nabla \mathcal{E}(x) \cdot \mathcal{G}(x) > 0$ for $x$ near the equilibrium point $x^{*}$. 
That is the flow $\mathcal G(x)$ may not be in the perfect $- \nabla \mathcal{E}(x)$ direction but contains positive component in that direction.
Then the (local) decay property of $\mathcal{E}(x)$ along the trajectory $x(t)$ of the autonomous system~\eqref{autonomous system} can be derived immediately:
$$
\frac{\mathrm{d}}{\mathrm{d} t} \mathcal{E}(x(t))=\nabla \mathcal{E}(x) \cdot x^{\prime}(t)=\nabla \mathcal{E}(x) \cdot \mathcal{G}(x)<0.
$$
To further establish the convergence rate of $\mathcal{E}(x(t))$, Chen and Luo~\cite{chen2021unified} introduced the strong Lyapunov condition: $\mathcal{E}(x)$ is a Lyapunov function and there exist constant $q \geqslant 1$, strictly positive function $c(x)$ and function $p(x)$ such that
\begin{equation*}
- \nabla \mathcal{E}(x) \cdot \mathcal{G}(x) \geq c(x) \mathcal{E}^{q}(x)+ p^{2}(x)  
\end{equation*}
holds true near $x^{*}$. From this, one can derive the exponential decay $\mathcal{E}(x(t))=O\left(e^{-c t}\right)$ for $q=1$ and the algebraic decay $\mathcal{E}(x(t))=O\left(t^{- 1 /(q-1)}\right)$ for $q>1$. Furthermore if $\|x - x^*\|^2 \leq C \mathcal E(x)$, then we can derive the exponential stability of $x^*$ from the exponential decay of Lyapunov function $\mathcal E(x)$. 

Note that for an optimization problem, we have freedom to design the vector field $\mathcal G(x)$ and Lyapunov function $\mathcal E(x)$. 


%\JW{The methods we provide can have preconditioners.}

\section{Gradient and skew-Symmetric Splitting Methods}\label{sec: gradient methods}
	In this section, we shall consider the generalized gradient flow
	\begin{equation}\label{eq: gradient flow}
x^{\prime}(t) = - \mathcal{A}(x(t)),
\end{equation}
and derive several iterative methods for solving $\mathcal A(x) = 0$ by applying ODE solvers to~\eqref{eq: gradient flow}. Based on the gradient and skew-symmetric splitting~\eqref{eq:dec}, we apply Accelerated OverRelaxation (AOR) technique to the non-symmetric part to obtain explicit schemes with linear convergence rate $(1+ c/ \kappa (\mathcal A))^{-1}$.

%We thus obtain an iterative method for solving~\eqref{eq:I+N} with rate \LC{xxx} which matches the rate for Krylov subspace method. 

\subsection{Stability}
Define the Lyapunov function:
\begin{equation}\label{eq: Eq}
\mathcal{E}_q(x) = \frac{1}{2}\|x-x^{\star}\|^2.
\end{equation}
%One can easily show the exponential stability by computing $\mathcal{E}_q^{\prime}(x(t))$ and using the monotonicity of $\mathcal A$:
For $x(t)$ solving the gradient flow~\eqref{eq: gradient flow}:
\begin{equation*}
\begin{aligned}
        \frac{\dd}{\dd t} \mathcal{E}_q(x(t)) &= \langle \nabla \mathcal E_q(x), x' \rangle = -\langle x- x^{\star}, \mathcal{A}(x) - \mathcal A(x^{\star}) \rangle \leq -2\mu \mathcal{E}_q(x),
\end{aligned}
\end{equation*}
which leads to the exponential decay
$$
\mathcal{E}_q(x(t))\leq e^{-2\mu t}\mathcal E_q(x(0))
$$
and consequently the solution $x^*$ is an exponentially stable equilibrium point of the dynamic system defined by~\eqref{eq: gradient flow}.

%	The gradient descent method corresponds to the Richardson method which was shown to be convergent with rate $1-1/\kappa^2$.  We then generalize to the equation~\eqref{eq:Ax}, i.e.,
%	\begin{equation}
%		\nabla F(x) + \mathcal N x = 0,
%	\end{equation}
%	and obtain an explicit scheme with linear rate $1/(1+\kappa)$. The precise definition of condition number $\kappa$ will be specified later on.  
	
\subsection{Implicit Euler Schemes}\label{sec:implicit scheme}
	The implicit Euler scheme for the generalized gradient flow~\eqref{eq: gradient flow} with step size $\alpha_k>0$ is
\begin{equation}\label{eq:implicit}
	x_{k+1} - x_k = -\alpha_k \mathcal A(x_{k+1}),
\end{equation}
%\LC{change to the general nonlinear case.}	
which is equivalent to the proximal iteration 
\begin{equation}\label{eq:proximal}
   (I + \alpha_k \mathcal A) (x_{k+1}) =  x_{k}.
\end{equation}
As $\mathcal A$ is strongly monotone and Liptschitz continuous, the operator $(I + \alpha_k \mathcal A)^{-1}$, which is called the resolvent of $\mathcal A$, is nonexpansive~\cite{browder1967construction} and has a unique fixed point~\cite{rockafellar1976monotone}. 
%In particular for~\eqref{eq:I+N}, the implicit Euler scheme is solving
%	\begin{equation}\label{eq:implicit}
%		\left [ (1 + \alpha_k \mu) I + \alpha_k \mathcal N \right ] x_{k+1} = \alpha_k b + x_k. 
%	\end{equation}
	
Using the identity of squares and the strong monotonicity, we have
\begin{equation}\label{eq:IEproof}
    	\begin{aligned}
		        \mathcal{E}_q(x_{k+1}) - \mathcal{E}_q(x_{k}) ={}& (x_{k+1}-x_k,x_{k+1}-x^{\star}) -\frac{1}{2}\|x_{k+1}-x_k\|^2\\
		={}& -\alpha_k \dual{ \mathcal A(x_{k+1}) - \mathcal A(x^{\star}), x_{k+1}-x^{\star}} - \frac{1}{2}\|x_{k+1}-x_k\|^2\\
		\leq {}&-2\alpha_k \mu  \mathcal{E}_q(x_{k+1}) ,
	\end{aligned}
\end{equation}
from which the linear convergence follows naturally 
\begin{equation*}
    	\|x_{k+1}-x^{\star}\|^2\leq \frac{1}{1+2\alpha_k \mu}\|x_{k}-x^{\star}\|^2.
\end{equation*}
There is no restriction on the step size which is known as the unconditional stability of the implicit Euler method. Choosing $\alpha_k\gg 1$ will accelerate the convergence with the price of solving a regularized nonlinear problem~\eqref{eq:proximal}, which is usually not practical. The implicit Euler method is present here as a reference to measure the difference of other methods.

\subsection{Explicit Euler Schemes}
The explicit Euler scheme for the generalized gradient flow~\eqref{eq: gradient flow} leads to the gradient decent method:
\begin{equation}\label{eq: EE gradient}
    x_{k+1} = x_k - \alpha_k \mathcal{A}(x_k).
\end{equation}
%Define the discrete Lyapunov function
%\begin{equation}\label{eq: discrete Lyapunov, l2 norm}
%\mathcal{E}(x_k) = \frac{1}{2}\|x_k-x^{\star}\|^2.
%\end{equation}
% \LC{Change the proof to illustrate the point ``The implicit Euler method is present here as a reference to measure the difference of other methods."}
% \breakline
% As $\mathcal{E}_q$ is quadratic, we have
% \begin{equation*}
% \begin{aligned}
%         \mathcal{E}_q(x_{k+1}) - \mathcal{E}_q(x_{k}) &= (x_{k+1}-x_k,x_{k+1}-x^{\star})- \frac{1}{2}\|x_{k+1} - x_k\|^2  \\
%         & = -\alpha_k \dual{ \mathcal A(x_{k}) - \mathcal A(x^{\star}), x_{k+1}-x^{\star}} ... \\
% %        & \leq (-2\alpha_k \mu + \alpha_k^2L_{\mathcal A}^2) \frac{\mu}{2}\| x_{k} - x^{\star} \|^2\\
%         & \leq -(2 \mu - \alpha_kL_{\mathcal A}^2)\alpha_k \mathcal{E}_q(x_{k}).
% \end{aligned}
% \end{equation*}
% \breakline
We write~\eqref{eq: EE gradient} as a correction of the implicit Euler scheme~\eqref{eq:implicit}, i.e.,
$$
x_{k+1}-x_k = - \alpha_k\mathcal A(x_{k+1}) + \alpha_k ( \mathcal A(x_{k+1}) - \mathcal A(x_{k})),
$$
and follow~\eqref{eq:IEproof} to obtain
\begin{equation}\label{eq:gradientproof}
\begin{aligned}
        \mathcal{E}_q(x_{k+1}) - \mathcal{E}_q(x_{k}) = &(x_{k+1}-x_k,x_{k+1}-x^{\star})- \frac{1}{2}\|x_{k+1} - x_k\|^2  \\
\leq &-2\alpha_k \mu  \mathcal{E}_q(x_{k+1})  - \frac{1}{2}\|x_{k+1} - x_k\|^2\\
&-\alpha_k \dual{ \mathcal A(x_{k}) - \mathcal A(x_{k+1}), x_{k+1}-x^{\star}}.
\end{aligned}
\end{equation}
%As $\mathcal{E}_q$ is quadratic, we have the expansion using the identity of squares
%\begin{equation*}
%\begin{aligned}
%        \mathcal{E}_q(x_{k+1}) - \mathcal{E}_q(x_{k}) &= (x_{k+1}-x_k,x_{k+1}-x^{\star})- \frac{1}{2}\|x_{k+1} - x_k\|^2  \\
%        & = -\alpha_k \dual{ \mathcal A(x_{k}) - \mathcal A(x^{\star}), x_{k+1}-x^{\star}} - \frac{1}{2}\|x_{k+1} - x_k\|^2 \\
%        & = -\alpha_k \dual{ \mathcal A(x_{k}) - \mathcal A(x_{k+1}) + \mathcal A(x_{k+1}) - \mathcal A(x^{\star}), x_{k+1}-x^{\star}} - \frac{1}{2}\|x_{k+1} - x_k\|^2\\
%        & \leq -\alpha_k \dual{ \mathcal A(x_{k}) - \mathcal A(x_{k+1}), x_{k+1}-x^{\star}} -2\alpha_k \mu  \mathcal{E}_q(x_{k+1})- \frac{1}{2}\|x_{k+1} - x_k\|^2.
%\end{aligned}
%\end{equation*}
% \LC{The term $(\mathcal A(x_k) - \mathcal A(x_{k+1}), x_{k+1} - x^{\star})$ is the mis-matching term and but controlled by the negative terms....}
Here we use the implicit Euler scheme as the reference scheme and call the term $\alpha_k (\mathcal A(x_k) - \mathcal A(x_{k+1}), x_{k+1} - x^{\star})$ a mis-matching term. Using the Cauchy-Schwarz inequality and the Lipschitz continuity of $\mathcal A$,
\begin{equation*}
\begin{aligned}
&\alpha_k |\dual{ \mathcal A(x_{k}) - \mathcal A(x_{k+1}), x_{k+1}-x^{\star}}| \\
\leq{} &\frac{\alpha_k}{2\mu} \|\mathcal A(x_{k}) - \mathcal A(x_{k+1})\|^2 + \frac{\alpha_k \mu}{2} \|x_{k+1} -x^{\star}\|^2 \\
\leq{} &  \frac{ \alpha_k L_{\mathcal A}^2}{2\mu} \|x_{k+1} -x_k\|^2 + \alpha_k \mu\, \mathcal{E}_q(x_{k+1})
\end{aligned}
\end{equation*}
Substitute back to~\eqref{eq:gradientproof} and let $\displaystyle \alpha_k \leq \frac{\mu}{L_{\mathcal A}^2}$, we have the linear reduction
$$ \mathcal{E}_q(x_{k+1}) \leq \frac{1}{1+\alpha_k \mu}\mathcal{E}_q(x_{k}).$$
In particular when $\displaystyle \alpha_k = \frac{\mu}{L_{\mathcal A}^2}$, 
$$ \mathcal{E}_q(x_{k+1}) \leq \frac{1}{1+1/\kappa^{2}(\mathcal A)}\mathcal{E}_q(x_{k}).$$

\begin{remark}\rm 
Using another identity on squares
$$
 \mathcal{E}_q(x_{k+1}) - \mathcal{E}_q(x_{k}) = (x_{k+1}-x_k, x_{k}-x^{\star}) +  \frac{1}{2}\|x_{k+1} - x_k\|^2, 
$$
and the monotonicity at $x_k$, the upper bound of the step size  can be relaxed to $\displaystyle \alpha_k < 2\mu/L_{\mathcal A}^2$ and for $\alpha_k = \mu/L_{\mathcal A}^2$, the rate can be improved to $1-1/\kappa^2(\mathcal A)$. Here we emphasize the approach of using the implicit Euler method as the reference and the estimate of the mis-match terms. The obtained linear rate is slightly worse but still in the same order of $\kappa(\mathcal A)$ as $(1+\delta )^{-1} = 1 - \delta + \mathcal O(\delta^2)$.
\end{remark}

	\subsection{Accelerated Overrelaxation Methods for Shifted skew-Symmetric  Problems}\label{eq:AORlinear}
In this subsection, we consider the shifted skew-symmetric linear system 
	\begin{equation}\label{eq:I+N}
		(\mu I + \mathcal N)x = b,
	\end{equation}
which is a special case of~\eqref{eq:Ax} with $F(x) = \frac{\mu}{2}\|x\|^2 - (b,x)$ and $\mathcal A(x) =  (\mu I + \mathcal N)x - b$. An efficient solver for~\eqref{eq:I+N} is an ingredient of the accelerated gradient and skew-symmetric splitting method for the nonlinear equation~\eqref{eq:Ax} we shall develop later on. The system~\eqref{eq:I+N} can be solved with Krylov subspace methods. For example, minimal residual methods~\cite{idema2007minimal,jiang2007algorithm} based on the Lanczos algorithm require short recurrences of vector storage. The convergence theorem using Chebyshev polynomial~\cite{jiang2007algorithm} shows that the residual converges with a linear rate of $O((1+\mu/\|\mathcal N\|)^{-1})$. Compared with Krylov subspace methods for solving~\eqref{eq:I+N}, our method to be presented enjoys a similar linear convergence rate but with a much simpler form and in-space vector storage. 


Recall that, for the matrix equation
\begin{equation*}
(D - L - U) x = b,
\end{equation*}
where $D$ a diagonal matrix, and $L$ and $U$ are strictly lower and upper triangular matrix, respectively, the \textbf{A}ccelerated \textbf{O}ver\textbf{R}elaxation (AOR) method ~\cite{Hadjidimos:1978Accelerated,hadjidimos2000successive} with the acceleration parameter $r$ and relaxation factor $\omega > 1$ is in the form 
\begin{equation*}
(D - r L) x_{k+1}=[(1-\omega) D+(\omega-r) L+\omega U] x_{k}+\omega b,
\end{equation*}
which is a two-parameter generalization of the successive overrelaxation (SOR) method. We shall apply AOR to \eqref{eq:I+N} with well-designed $r$ and $\omega$. 
	
As $\mathcal N = -\mathcal N^{\intercal}$, all diagonal entries of $\mathcal N$ are zero. Let $B^{\intercal} = {\rm upper}(\mathcal N)$ be the upper triangular part of $\mathcal N$. Then $\mathcal N = B^{\intercal} -B$. Let $B^{\rm sym} = B + B^{\intercal}$ be a symmetrization of $B$. We have the splitting
\begin{align}
	\label{eq:Bs-B} \mathcal N &= B^{\rm sym} - 2B,\quad \text{and}\\
	\label{eq:B-Bs} \mathcal N &= 2 B^{\intercal} - B^{\rm sym}.
\end{align}
As a symmetric matrix, all eigenvalues of $B^{\rm sym}$ are real. Let $\lambda_1 \leq \cdots \leq \lambda_n$ be the eigenvalues of $B^{\rm sym}$. Since ${\rm trace } (B^{\rm sym}) = 0$, we know $\lambda_1 <0$ and $\lambda_n >0$. That is $B^{\rm sym}$ is symmetric but indefinite. Define $L_{B^{\rm sym}} =  \| B^{\rm sym} \| = \max \{|\lambda_1|, \lambda_n\}$ and $\kappa( B^{\rm sym}) = L_{B^{\rm sym}} / \mu$. 
%\mnote{ for symmetric matrix, spectral radius is equal to the norm.}

%\LC{Is $\| B^{\rm sym} \| = \| \mathcal N\|$? If both are the maximum of the squared row sum, then they are equal.}
		
We discretize the gradient flow~\eqref{eq: gradient flow} by the matrix splitting~\eqref{eq:Bs-B} and treat $B^{\rm sym}$ as the explicit term:
\begin{equation}\label{eq: GS method y}
	\frac{x_{k+1} - x_k}{\alpha} = - \left (\mu x_{k+1}+B^{\rm sym}x_k - 2B x_{k+1} - b \right).
\end{equation}
The update of $x_{k+1}$ is equivalent to solve
\begin{equation}\label{eq:AOR}
	\left[ (1+\alpha \mu) I - 2\alpha B \right ]x_{k+1} = x_k - \alpha (B^{\rm sym}x_k - b),
\end{equation}
% which is AOR for solving the implicit Euler discretization or proximal iteration
% \begin{align*}
%    (I + \alpha (\mu I + \mathcal N)) x_{k+1} =  x_{k} + \alpha b.
% \end{align*}
%  with $r = 2$ and $\omega = 1$.
which is AOR for solving
\begin{align*}
   (\mu I  - B + B^{\intercal}) x =  b
\end{align*}
 with $r = \frac{2\alpha \mu}{1+\alpha \mu}$ and $\omega =  \frac{\alpha \mu}{1+\alpha \mu}$. The left hand side of~\eqref{eq:AOR} is an invertible lower-triangular matrix and $x_{k+1}$ can be efficiently computed by a forward substitution.
%
%When \LC{$|\lambda_1|> \lambda_n$ Check the case.}, 
We can also use the splitting~\eqref{eq:B-Bs} to get another AOR scheme
\begin{equation}\label{eq: GS method 2}
	\frac{x_{k+1} - x_k}{\alpha} = - \left (\mu x_{k+1} - B^{\rm sym}x_k + 2B^{\intercal} x_{k+1} - b \right),
\end{equation}
for which $x_{k+1}$ can be efficiently computed by a backward substitution. 
%\mnote{ how about symmetrized iteration?}
%\LC{Read the two paper on AOR and SOR and make sure ours is new and the Lyapunov analysis is new to SOR.}
%
%\LC{Read Jiang's paper and compare with Krylov subspace methods.}
		
%	\LC{ add references}.  
% \HL{However, the method here involves $(I-\alpha B)^{-1}$ not just the matrix-vector multiplication of $\mathcal N$. }

% when the symmetric matrix $B+B^{\intercal}$ with $\mathcal N = B^{\intercal} -B$ has relative small spectral radius, or positive(negative) semidefinite. 

%to solve the equation. The convergence theorem using the Lyapunov analysis extends to general nonlinear system~\eqref{eq:Ax}.
%
%We decompose $\mathcal N$ into $B^{\intercal} - B$ where  $B$ is an $n \times n$ strictly lower-triangular matrix such that $-B$ takes the strictly lower-triangular part of $\mathcal N$. Let  $B^{\rm sym} = \frac{B+B^{\intercal}}{2}$ denote the symmetrization of $B$. We introduce the constant $L_{B^{\rm sym}}$ such that
%\begin{equation*}
%\left (x, x \right )_{B^{\rm sym}} \leq L_{B^{\rm sym}}\|x\|^2, \quad \forall x \in \mathbb{R}^n
%\end{equation*}
%where $(\cdot, \cdot)_{B^{\rm sym}}$ denotes the inner product induced by the symmetric matrix $B^{\rm sym}$. Let $\lambda_1 \leq \cdots \leq \lambda_n$ be the eigenvalues of $\frac{B+B^{\intercal}}{2}$. Then $L_{B^{\rm sym}} = \lambda_n$. More precise, we set $B^{\rm sym} = -\frac{B+B^{\intercal}}{2} $ if $0 \leq |\lambda_1| < \lambda_n$   since we prefer smaller $L_{B^{\rm sym}}$. In general, 
%$$L_{B^{\rm sym}} =\left\{
%\begin{aligned}
%&\lambda_n, &\quad  \lambda_n \leq 0 \\
%&-\lambda_1, &\quad \lambda_1 \geq 0  \\
%&\min\{|\lambda_1|, \lambda_n\}, &\quad  \lambda_1 \lambda_n < 0 
%\end{aligned}
%\right. .$$
%
%\JW{We have
%\begin{equation*}
%(x, \mathcal Ny) = (x, (B^{\intercal}- B)y)=  \leq \|N\|\|x\|\|y\|. 
%\end{equation*}Any connection between $L_{B^{\rm sym}}$ and $\|N\|$?}
%
%We first propose the gradient flow and show the exponential stability with the strong Lyapunov property on the continuous level.
%
%\subsection{Gradient Flow}
%Consider the gradient flow to solve $(\mu I+ \mathcal N )y  = b $,
%\begin{equation}\label{eq: Gradient flow y}
%    y^{\prime} = -\frac{1}{\mu}\left [(\mu I+ \mathcal N )y  - b \right].
%\end{equation}
%We put a scaling parameter $1/\mu$ to take advantage of the accelerated model. \JW{Can we say this?}
%
%Denote the vector field on the right hand side of~\eqref{eq: Gradient flow y} by $\mathcal G(y)$. We verify the strong Lyapunov porperty.
%
%\begin{lemma}\label{lem: shifted-skew symmetric continuous decay}
%For the Lyapunov function $\mathcal{E}_q(y) = \frac{\mu}{2}\|y-x^{
	%    \star}\|^2$,
%\begin{equation*}
%     -\nabla \mathcal{E}_q \cdot \mathcal G = 2\mathcal{E}_q.
%\end{equation*}
%\end{lemma}
%
%\begin{proof}
%Direct computation gives
%\begin{equation*}
%\begin{aligned}
%        -\nabla \mathcal{E}_q \cdot \mathcal{G} =&~ 
%\mu \begin{pmatrix}
	%y-x^{\star}
	%\end{pmatrix}^{\intercal} \left( I +\mathcal N/\mu\right)
	%\begin{pmatrix}
	%y-x^{\star}
	%\end{pmatrix} \\
	%= &~   \begin{pmatrix}
		%y-x^{\star}
		%\end{pmatrix}^{\intercal}\left( \mu I + \mathcal N\right)
		%\begin{pmatrix}
		%y-x^{\star}
		%\end{pmatrix} \\
		%= &~   2\mathcal E_q. 
		%\end{aligned}
		%\end{equation*}
		%\end{proof}
		%
		%We generalize to nonlinear system~\eqref{eq:Ax} in the following corollary. The proof is straight forward since $F \in \mathcal{S}_{\mu, L}^1$.
		%
		%\begin{corollary}\label{cor: generalized nonliear continuous decay}
		%Suppose $y$ is the solution satisfying
		%\begin{equation*}
		%    y^{\prime} = -\frac{1}{\mu}\left (\nabla F(y)+ \mathcal N y  \right).
		%\end{equation*}
		%Then for the Lyapunov function $
		%     \mathcal{E}_q(y) = \frac{\mu}{2}\|y-x^{
			%    \star}\|^2$, we have
		%\begin{equation*}
		%     -\nabla \mathcal{E}_q \cdot \mathcal G \geq 2\mathcal{E}_q.
		%\end{equation*}
		%\end{corollary}
		%
					
%\subsection{Lyapunov analysis of AOR}				
For a symmetric matrix $M$, we define $$\|x\|_M^2 :=(x,x)_M := x^{\intercal}Mx.$$ When $M$ is SPD, $(\cdot,\cdot)_M$ defines an inner product with the induced norm $\|x\|_M$. For a general symmetric matrix, $\|\cdot \|_M$ may not be a norm. However the following identity for squares still holds:
\begin{equation}\label{eq:squares}
	2(a,b)_M = \|a\|_M^2 + \|b\|_M^2 - \|a-b\|_M^2.
\end{equation}
					
Without loss of generality, we focus on the convergence analysis of scheme~\eqref{eq: GS method y}. Notice that when $0<\alpha < 1/L_{B^{\rm sym}}$, $I \pm \alpha B^{\rm sym}$ is SPD. 					
For $0<\alpha<1/L_{B^{\rm sym}}$, consider the Lyapunov function
\begin{equation}\label{eq: linear refined Lyapunov}
	\mathcal{E}^{\alpha B}(x) = \frac{1}{2}\|x-x^{\star}\|^2 -  \frac{\alpha}{2}\| x - x^{\star}\|^2_{B^{\rm sym}}= \frac{1}{2} \| x- x^{\star} \|^2_{ I - \alpha B^{\rm sym}} ,
\end{equation}
which is nonnegative and $\mathcal E^{\alpha B}(x)= 0$  if and only if $x = x^{\star}$. Noted that $\mathcal E^{\alpha B}$ might be negative without control on the step size as $B^{\rm sym}$ is indefinite. 
%					Here, $						\mathcal{E}_{+}$ and $						\mathcal{E}_{-}$ are used for ~\eqref{eq: GS method y} and~\eqref{eq: GS method 2}, respectively. 
					
%Notice $\mathcal E \leq   \frac{\mu}{2}\|y-x^{
	%    \star}\|^2$ if $L_{B^{\rm sym}} < 0$. Otherwise, we have $\mathcal E > c\|y-x^{
	%    \star}\|^2 $ for $\alpha < \frac{\mu}{L_{B^{\rm sym}}}$. We denote $\mathcal E (y_{k}) = \mathcal E_k$ for all $k \geq 0$.

\begin{theorem}\label{thm: linear convergence for the shifted skew-symmetric linear system}
	Let $\{x_k\}$ be the sequence generated by ~\eqref{eq: GS method y} with arbitrary initial guess $x_0$ and step size $0< \alpha < 1/L_{B^{\rm sym}}$. Then for the Lyapunov function~\eqref{eq: linear refined Lyapunov},
	\begin{equation}\label{eq:linearAORrate}
		\begin{aligned}
			\mathcal{E}^{\alpha B}( x_{k+1}) \leq&~ \frac{1}{1+\alpha \mu}\mathcal{E}^{\alpha B}(x_{k}).
		\end{aligned}
	\end{equation}
In particular, for $ \alpha = \displaystyle \frac{1}{2L_{B^{\rm sym}}}$, we have
\begin{equation}\label{eq:AORnormrate}
\| x_k - x^{\star}\|^2 \leq \left (\frac{1}{1 + \mu/(2L_{B^{\rm sym}})}\right )^k 3\| x_0 - x^{\star}\|^2.
\end{equation}
\end{theorem}
\begin{proof}
%We first write the difference of the discrete Lyapunov function with the strong Lyapunov function $\mathcal E_q$ defined in Lemma \ref{lem: shifted-skew symmetric continuous decay},
% \begin{align*}
	%\mathcal{E}_{k+1} - \mathcal{E}_k =&~\mathcal{E}_q(x_{k+1})- \mathcal{E}_q(x_k)\\
	%&  -\alpha \left (x_{k+1}-x^{
	%    \star}, x_{k+1} -x^{
	%    \star}\right)_{B^{\rm sym}} +\alpha \left (y_{k}-x^{
	%    \star}, y_{k} -x^{
	%    \star}\right)_{B^{\rm sym}}. 
%\end{align*}
%
We use the identity for squares~\eqref{eq:squares}:
\begin{equation}\label{eq:Eqdiff}
	\frac{1 }{2} \| x_{k+1} - x^{\star}\|^2-\frac{1 }{2} \| x_{k} - x^{\star}\|^2 =  (x_{k+1} -x^{\star}, x_{k+1} - x_k) -\frac{1 }{2} \| x_{k+1} - x_k\|^2. 
\end{equation}
We write GSS scheme~\eqref{eq: GS method y} as a correction of the implicit Euler scheme
\begin{align*}
	x_{k+1} - x_k & = - \alpha (\mathcal A ( x_{k+1}) -\mathcal A(x^{\star}))+ \alpha B^{\rm sym}(x_{k+1} - x_k).
\end{align*}
%For the implicit Euler method, the second term dis-appear and one can easily obtain the unconditional linear convergence by using the monotone 
For the first term, we have
$$
-\alpha \langle  x_{k+1} -x^{\star},  \mathcal A ( x_{k+1}) -\mathcal A ( x^{\star})  \rangle = -  \alpha \mu \|x_{k+1} -x^{\star}\|^2. 
$$
We use the identity~\eqref{eq:squares} to expand the cross term as
\begin{align*}
(  x_{k+1} -x^{\star}, x_{k+1} - x_k)_{\alpha B^{\rm sym}} ={} &\frac{1}{2} \| x_{k+1} -x^{\star} \|_{\alpha B^{\rm sym}}^2 +  \frac{1}{2} \| x_{k+1} -x_k \|_{\alpha B^{\rm sym}}^2 \\
-&\frac{1}{2} \| x_{k} -x^{\star} \|_{\alpha B^{\rm sym}}^2.
\end{align*}
Substitute back to~\eqref{eq:Eqdiff} and rearrange the terms, we obtain the identity 
%\mnote{ one more step and check the constant ! \JW{cheched.}}
\begin{align*}
{}&	\mathcal E^{\alpha B}(x_{k+1}) - \mathcal E^{\alpha B}(x_{k}) \\
={}& -\alpha \mu \|x_{k+1} -x^{\star}\|^2-\frac{1}{2}\| x_{k+1}- x_k \|_{ I - \alpha B^{\rm sym}}^2\\
	={}& - \alpha  \mu \, \mathcal E^{\alpha B}(x_{k+1}) -\frac{\alpha \mu}{2}\| x_{k+1}- x^{\star} \|_{ I + \alpha B^{\rm sym}}^2-\frac{1}{2}\| x_{k+1}- x_k \|_{ I - \alpha B^{\rm sym}}^2.
\end{align*}
As $0< \alpha < \displaystyle 1/L_{B^{\rm sym}}$, $I \pm \alpha B^{\rm sym}$ is SPD and the last two terms are non-positive. Dropping them, we obtain the inequality
$$
\mathcal E^{\alpha B}(x_{k+1}) - \mathcal E^{\alpha B}(x_{k}) \leq - \alpha \mu\, \mathcal E^{\alpha B}(x_{k+1}) 
$$
and~\eqref{eq:linearAORrate} follows by arrangement.
						
When $\alpha = 1/(2L_{B^{\rm sym}})$, we have the bound	
\begin{equation}
\frac{1}{4}\|x-x^{\star}\|^2 \leq \mathcal E^{\alpha B}(x) \leq \frac{3}{4}\|x-x^{\star}\|^2
\end{equation}
which implies~\eqref{eq:AORnormrate}. 
%We shall use the strong Lyapunov property of $\mathcal E_q$ at $x_{k+1}$ to bound the first term. Before that, we analyze the mis-match terms of $\mathcal G^y ( x_{k+1})$,
%\begin{equation}\label{eq: linear mis-match term}
%\begin{aligned}
%    &\frac{\alpha}{\mu} \langle \nabla \mathcal E_q(x_{k+1}),  B^{\intercal}  (x_{k+1} - y_{k}) \rangle +  \frac{\alpha}{\mu} \langle \nabla \mathcal E_q( x_{k+1}),  B ( \bar x_{k+1} - x_{k+1})\rangle \\
%    =&~ \alpha ( x_{k+1} - x^{\star}, B^{\intercal}  (x_{k+1} - y_{k})) +  \alpha ( x_{k+1} - x^{\star}, B (x_{k+1} - y_{k})) \\
%     =&~ 2\alpha ( x_{k+1} - x^{\star}, x_{k+1} - y_{k})_{B^{\rm sym}}.
%\end{aligned}
%\end{equation}
%
%We expand~\eqref{eq: linear mis-match term} into terms using the equality $2(a, b) =  (a, a) + (b, b) - (a-b, a-b)$
%\begin{equation}\label{eq: linear mis-match term 2}
%\begin{aligned}
%         2 \alpha ( x_{k+1} - x^{\star}, (x_{k+1} - y_{k}))_{B^{\rm sym}} =&~ \alpha \left (x_{k+1} - x^{\star}, x_{k+1} - x^{\star}\right)_{B^{\rm sym}} \\
%    &+ \alpha \left (x_{k+1} - x_k, x_{k+1} - x_k\right)_{B^{\rm sym}} \\
%    &-\alpha \left (y_{k} - x^{\star}, y_{k} - x^{\star}\right)_{B^{\rm sym}}.
%\end{aligned}
%\end{equation}
%
%Summarize above and use the strong Lyapunov property of $\mathcal E_q$at $x_{k+1}$, we get
%\begin{equation}\label{eq: diffrence of Lyapunov}
%\begin{aligned}
%\mathcal{E}_{k+1} - \mathcal{E}_k &= -2\alpha \mathcal{E}_q( x_{k+1})-\frac{\mu}{2}\left\|x_{k+1}-x_k\right\|^2+\alpha \left (x_{k+1} - x_k, x_{k+1} - x_k\right)_{B^{\rm sym}}\\
%& = -\alpha \mu \|x_{k+1}-x^{\star}\|^2 -\frac{\mu}{2}\left\|x_{k+1}-x_k\right\|^2+\alpha \left (x_{k+1} - x_k, x_{k+1} - x_k\right)_{B^{\rm sym}} \\
%&\leq  -\alpha \left( \frac{\mu}{2} \|x_{k+1}-x^{\star}\|^2  + \alpha \left (x_{k+1} - x^{\star}, x_{k+1} - x^{\star}\right)_{B^{\rm sym}}\right) = -\alpha \mathcal E_{k+1}.
%\end{aligned}
%\end{equation}
%
%The inequality holds by using
%\begin{equation*}
%     \alpha \left (x_{k+1} - x^{\star}, x_{k+1} - x^{\star}\right)_{B^{\rm sym}} \leq \frac{\mu}{2}\|x_{k+1}-x^{\star}\|^2
%\end{equation*}
%to expand $\mu\|x_{k+1}-x^{\star}\|^2$ and the rest terms are nonpositive since
%\begin{equation*}
%    \alpha(x_k - x_{k+1}, x_k - x_{k+1})_{B^{\rm sym}} \leq \frac{\mu}{2}\left\|x_{k+1}-x_k\right\|^2 
%\end{equation*}
%according to our choice $\alpha \leq \displaystyle \frac{\mu}{L_{B^{\rm sym}}}$.
\end{proof}
%\LC{For SOR type iterative methods, usually spectral analysis \LC{references} is applied to the error matrix which is in general non-symmetric and harder to estimate. Lyapunov analysis applied to SOR seems new.} 
%\mnote{ Read the survey paper on SOR and check if the claim is true. Bring new analysis tools to classic iterative methods.} 

For SOR type iterative methods, usually spectral analysis~\cite{bai2007successive,bai2005generalized,hadjidimos2000successive} is applied to the error matrix which is in general non-symmetric and harder to estimate. The Lyapunov analysis provides a new and relatively simple tool and more importantly enables us to study nonlinear systems.



%\LC{We need to study the relation of $\| \mathcal N\|$ and $\| B^{\rm sym} \|$.}

\subsection{Gradient and skew-Symmetric Splitting Methods for Nonlinear Problems}\label{sec: AOR for nonlinear problems}
For non-linear equation~\eqref{eq:Ax}, we treat $\nabla F$ explicitly and propose the following Gradient and skew-Symmetric Splitting (GSS) scheme
\begin{equation}\label{eq: nonlinear SOR 1}
    \frac{x_{k+1} - x_k}{\alpha} = -\left (\nabla F(x_k)+ B^{\rm sym}x_k - 2B x_{k+1}\right).
\end{equation}
Similarly, modification on~\eqref{eq: GS method 2} gives another GSS scheme
\begin{equation}\label{eq: nonlinear SOR 2}
    \frac{x_{k+1} - x_k}{\alpha} = - \left (\nabla F(x_k) - B^{\rm sym}x_k + 2B^{\intercal} x_{k+1} \right).
\end{equation}
Both schemes are explicit as $\nabla F(x_k)$ is known.  

We focus on the GSS method~\eqref{eq: nonlinear SOR 1}. The proof for~\eqref{eq: nonlinear SOR 2} follows in line with a sign change in the following Lyapunov function
 \begin{equation}\label{eq: modified nolienar Lyapunov}
    \mathcal E^{\alpha BD}(x_k):= \frac{1}{2}\|x_k-x^{\star}\|^2_{I - \alpha B^{\rm sym}}  -\alpha D_F(x^{\star}, x_k).
\end{equation}
% $\eqref{eq: modified nolienar Lyapunov}$ to  \begin{equation*}
%    \mathcal{E}_k = \frac{1}{2}\|x_k-x^{\star}\|^2_{I + \alpha B^{\rm sym}}  -\alpha D_F(x^{\star}, x_k).
%\end{equation*}
%
%\LC{Present as a lemma.}

\begin{lemma}
For  $F \in \mathcal S_{\mu, L_F}$ and $\alpha  <1/\max\{2L_{B^{\rm sym}}, 2L_F \}$, the Lyapunov function $\mathcal E^{\alpha BD}(x)\geq 0$ and $\mathcal E^{\alpha BD}(x)= 0$  if and only if $x = x^{\star}$.
\end{lemma}
\begin{proof}
Since   $F \in \mathcal S_{\mu, L_F}$, the Bregman divergence is non-negative and
\begin{equation*}
  D_F(x^{\star}, x) \leq \frac{L_F}{2}\|x-x^{ \star}\|^2.
\end{equation*}
Then for  $\alpha  <1/\max\{2L_{B^{\rm sym}}, 2L_F \}$,
\begin{equation*}
\begin{aligned}
     \mathcal{E}^{\alpha BD}(x) &\geq  \frac{1}{2}\|x-x^{\star}\|^2 - \frac{\alpha}{2} \|x-x^{\star}\|_{B^{\rm sym}} - \frac{\alpha L_F}{2}\|x-x^{ \star}\|^2 \\
    &\geq \frac{1}{2}\|x-x^{\star}\|^2   - \frac{1}{4L_{B^{\rm sym}}} \|x-x^{\star}\|_{B^{\rm sym}}^2  - \frac{ 1}{4}\|x-x^{ \star}\|^2 \\
    &\geq 0.
\end{aligned}
\end{equation*}
If $x \neq x^{\star}$, then the second $\geq$ becomes $>$. So $\mathcal E^{\alpha BD}(x)= 0$  if and only if $x = x^{\star}$. 
\end{proof}


\begin{theorem}\label{thm: linear convergence for generalized nonlinear system}
Let $\{x_k\}$ be the sequence generated by the AOR method~\eqref{eq: nonlinear SOR 1} with arbitrary initial guess $x_0$ and step size $\alpha < 1/\max\{2L_{B^{\rm sym}}, 2L_F \}$. 
Then for the discrete Lyapunov function~\eqref{eq: modified nolienar Lyapunov}, we have 
\begin{equation*}
\begin{aligned}
\mathcal{E}^{\alpha BD}(x_{k+1})\leq&~ \frac{1}{1+ \alpha \mu}\mathcal{E}^{\alpha BD}(x_k).
\end{aligned}
\end{equation*}
In particular, for $ \alpha = \displaystyle \min \left\{\frac{1}{4L_{B^{\rm sym}}}, \frac{1}{4L_F} \right\}$, we have
\begin{equation}\label{eq: nonlinear AOR norm rate}
\| x_k - x^{\star}\|^2 \leq \left (1 + 1/\max\left \{ 4\kappa ( B^{\rm sym}), 4 \kappa(F)\right\}\right )^{-k} 6\| x_0 - x^{\star}\|^2.
\end{equation}
\end{theorem}
\begin{proof}
%We use the identity for squares~\eqref{eq:squares}:
%%to write the difference of $\mathcal E_q$ as
%\begin{equation}\label{eq:Eqdiff 2}
%\frac{1 }{2} \| x_{k+1} - x^{\star}\|^2-\frac{1 }{2} \| x_{k} - x^{\star}\|^2=  \langle  x_{k+1} -x^{\star}, x_{k+1} - x_k \rangle -\frac{1}{2} \| x_{k+1} - x_k\|^2. 
%\end{equation}
We write the scheme \eqref{eq: nonlinear SOR 1} as a correction of implicit Euler scheme
\begin{align*}
x_{k+1} - x_k & = - \alpha (\mathcal A ( x_{k+1}) -\mathcal A(x^{\star}))+ \alpha B^{\rm sym}(x_{k+1} - x_k) + \alpha(\nabla F(x_{k+1}) - \nabla F(x_k)).
\end{align*}
The first two terms are treated as before; see the proof of Theorem \ref{thm: linear convergence for the shifted skew-symmetric linear system}.
%For the gradient flow term, we have
%$$
%- \langle  x_{k+1} -x^{\star},  \mathcal A ( x_{k+1}) -\mathcal A ( x^{\star})  \rangle \leq  -  \mu \|x_{k+1} -x^{\star}\|^2. 
%$$
%% \JW{$$
%% - \langle  x_{k+1} -x^{\star},  \mathcal A ( x_{k+1}) -\mathcal A ( x^{\star})  \rangle \leq -D_F(x^{\star}, x_{k+1}) -  \frac{\mu}{2} \|x_{k+1} -x^{\star}\|^2. 
%% $$}
%We use the identity~\eqref{eq:squares} to expand the first cross term as
%\begin{equation*}
%2 (  x_{k+1} -x^{\star}, x_{k+1} - x_k)_{B^{\rm sym}} =  \| x_{k+1} -x^{\star} \|_{B^{\rm sym}}^2 +  \| x_{k+1} -x_k \|_{B^{\rm sym}}^2 - \| x_{k} -x^{\star} \|_{B^{\rm sym}}^2.
%\end{equation*}
The last cross term is expanded using the identity~\eqref{eq: Bregman divergence identity} for the Bregman divergence:
\begin{equation*}
\begin{aligned}
   \langle  x_{k+1} - x^{\star}, \nabla F(x_{x+1}) - \nabla F(x_k) \rangle  = D_F(x^{\star}, x_{k+1}) + D_F( x_{k+1}, x_k) - D_F( x^{\star}, x_k).
\end{aligned}
\end{equation*}
Substituting back to~\eqref{eq:Eqdiff} and rearranging the terms, we obtain the identity
\begin{align*}
\mathcal E^{\alpha BD}(x_{k+1}) - \mathcal E^{\alpha BD}(x_{k}) =&~ - \alpha \mu \mathcal E^{\alpha BD}(x_{k+1}) - \frac{\alpha \mu}{2} \|x_{k+1} -x^{\star}\|_{I + \alpha B^{\rm sym}}^2\\
&- \alpha^2 \mu D_F(x^{\star}, x_{k+1})\\
& - \frac{1}{2}\| x_{k+1}- x_k \|_{I -\alpha B^{\rm sym}}^2+ \alpha D_F( x_{k+1}, x_k).
\end{align*}
When $\alpha  < 1/\max\{2L_{B^{\rm sym}}, 2L_F \}$,  $I \pm\alpha B^{\rm sym}$ is SPD and
\begin{equation*}
\begin{aligned}
  \alpha D_F( x_{k+1},  x_k)&\leq \frac{\alpha L_F}{2}\|x_{k+1}- x_k\|^2 \leq  \frac{1}{4}\|x_{k+1}- x_k\|^2\leq \frac{1}{2}\| x_{k+1}- x_k \|_{I -\alpha B^{\rm sym}}^2.
\end{aligned}
\end{equation*} 
Thus the last two terms are non-positive. Dropping all non-positive terms, we obtain the inequality
$$
\mathcal E^{\alpha BD}(x_{k+1}) - \mathcal E^{\alpha BD}(x_{k}) \leq - \alpha \mu \mathcal E^{\alpha BD}(x_{k+1}) 
$$
and the linear reduction follows by arrangement.
						
When $\alpha = \displaystyle \min \left\{\frac{1}{4L_{B^{\rm sym}}}, \frac{1}{4L_F} \right\}$, we have the bound	
\begin{equation}
\frac{1}{8}\|x-x^{\star}\|^2 \leq \mathcal E^{\alpha BD}(x) \leq \frac{3}{4}\|x-x^{\star}\|^2
\end{equation}
which implies~\eqref{eq: nonlinear AOR norm rate}.
\end{proof}

Notice the rate in Theorem \ref{thm: linear convergence for generalized nonlinear system} is $(1+c/\kappa)$ for $\kappa =  \max\{\kappa(F),\kappa (B^{\rm sym})\}$ which matches the rate of the gradient descent method for convex optimization problems. We expect a combination of the accelerated gradient flow and AOR for the skew-symmetric part will give an accelerated explicit scheme. 
%
%\LC{Check the example of saddle point system in the project gradient system. What is the form of the SOR method for solving the saddle point system? As it is not strongly convex in $p$, there must be a gap for the linear convergence.}

%The IMEX scheme with implicit $\nabla F(x_{k+1}$ will be considered in xxx. 
%\begin{equation}\label{eq:IMEX}
%    \frac{x_{k+1} - x_k}{\alpha} = -\left (\nabla F(x_{k+1})+ B^{\rm sym}x_k - 2B x_{k+1}\right).
%\end{equation}

%\subsection{Implicit-Explicit scheme}
%\LC{Consider implicit in $\mathcal N$ and explicit in $\nabla F$ take the saddle point system and show it is equivalent to the project gradient method. And can handle the case $F$ is not strongly convex but only $f$ is strongly convex in the constraint affine space $Bu = b$.}


\section{Accelerated Gradient and skew-Symmetric Splitting Methods}\label{sec:acc flow and schemes}
In this section, we develop the accelerated gradient flow and propose Accelerated Gradient and skew-Symmetric Splitting (AGSS) methods with accelerated linear rates. For the implicit-explicit scheme, we can relax to inexact inner solvers with computable error tolerance. 

\subsection{Accelerated Gradient Flow}
% \LC{Change the flow by dividing $\mu$ on RHS. Be consistent with the saddle point problem.}
Following ~\cite{luoDifferentialEquationSolvers2021a}, we introduce an auxiliary variable $y$ and an accelerated gradient flow
\begin{equation}\label{eq:AG}
\left \{\begin{aligned}
     x^{\prime} &= y - x ,\\
     y^{\prime} & = x - y - \frac{1}{\mu}(\nabla F(x) + \mathcal N y).
\end{aligned}\right .
\end{equation}
Denote the vector field on the right hand side of~\eqref{eq:AG} by $\mathcal G(x,y)$. Then $\mathcal G(x^{\star},x^{\star}) = 0$ and thus $(x^{\star},x^{\star})$ is an equilibrium point of~\eqref{eq:AG}. 
Comparing with the accelerated flow in ~\cite{luoDifferentialEquationSolvers2021a} for convex optimization, the difference is to split $\mathcal A(x) \rightarrow \nabla F(x) + \mathcal N y$. The gradient component is accelerated by using the accelerated gradient methods for convex optimization and the skew-symmetric component is accelerated by AOR. 

We first show $(x^{\star},x^{\star})$ is exponentially stable. Consider the Lyapunov function:
\begin{equation}\label{eq: acc Lyapunov, mixed}
\mathcal{E}(x, y) =  D_F(x, x^{\star}) + \frac{\mu}{2}\| y-x^{\star}\|^2.
\end{equation}
As $F$ is strongly convex, $\mathcal{E}(x, y)\geq 0$ and $\mathcal{E}(x, y)= 0$ iff $x=y=x^{\star}$. Furthermore, function $D_F(\cdot , x^{\star}) \in \mathcal S_{\mu,L_F}$.

\begin{lemma}
For function $F \in \mathcal S_{\mu}$, we have
\begin{equation}\label{eq:DFmu}
 \langle \nabla F(x) - \nabla F(x^{\star}), x- x^{\star} \rangle \geq  D_F(x, x^{\star}) + \frac{\mu}{2}\|x - x^{\star}\|^2,
\end{equation}
\end{lemma}
\begin{proof}
By direct computation, $ \langle \nabla F(x) - \nabla F(x^{\star}), x- x^{\star} \rangle = D_F(x,x^{\star}) + D_F(x^{\star}, x)$ and thus~\eqref{eq:DFmu} follows from the bound
$$\min \{ D_F(x,x^{\star}), D_F(x^{\star},x)\} \geq \frac{\mu}{2}\|x - x^{\star}\|^2,$$
for a convex function $F \in \mathcal S_{\mu}$.
\end{proof}

We then verify the strong Lyapunov property.

\begin{lemma}[Strong Lyapunov Property]\label{lem: acc strong Lyapunov}
Assume function $F \in \mathcal{S}_{\mu} $. Then for the Lyapunov function~\eqref{eq: acc Lyapunov, mixed} and the  accelerated gradient flow vector field $\mathcal G$, the following strong Lyapunov property holds
\begin{equation}\label{eq:strong acc}
- \nabla \mathcal E(x,y)\cdot \mathcal G(x,y) \geq \mathcal E(x,y)+\frac{\mu}{2}\|y-x\|^2.
\end{equation}
\end{lemma}
\begin{proof}
First of all, as $\mathcal G(x^{\star}, x^{\star}) = 0$, $$- \nabla \mathcal E(x,y)\cdot \mathcal G(x,y) = - \nabla \mathcal E(x,y)\cdot (\mathcal G(x,y)  - \mathcal G(x^{\star}, x^{\star})).$$
%\LC{The nonlinear case is similar. Write a few lines to prove that.}
Direct computation gives
\begin{align*}
- \nabla \mathcal E(x,y)\cdot \mathcal G(x,y) =&~
\langle  \nabla D_F(x; x^{\star}),x -x^{\star}-( y - x^{\star}) \rangle  -\mu ( y-x^{\star}, x -x^{\star})\\
& + \mu \|y  - x^{\star}\|^2+ \langle \nabla F(x) - \nabla F(x^{\star}),y - x^{\star} \rangle + ( y-x^{\star}, \mathcal N (y-x^{\star}))\\
=&~ \langle  \nabla F(x) - \nabla F(x^{\star}),x -x^{\star} \rangle  + \mu \|y  - x^{\star}\|^2-\mu(y-x^{\star},x -x^{\star})
\end{align*}
where we have used $ \nabla D_F(x; x^{\star}) = \nabla F(x) - \nabla F(x^{\star})$ and $( y-x^{\star}, \mathcal N (y-x^{\star})) = 0$ since $\mathcal N$ is skew-symmetric.

Using the bound~\eqref{eq:DFmu} and the identity for squares~\eqref{eq:squares}:
\begin{equation*}
    \begin{aligned}
    &\frac{\mu}{2} \|y  - x^{\star}\|^2 -\mu ( y-x^{\star}, x -x^{\star})  = \frac{\mu}{2}\|y-x\|^2-\frac{\mu}{2}\|x - x^{\star}\|^2,
    \end{aligned}
\end{equation*}
we obtain~\eqref{eq:strong acc}.
% Summing together and observing $ D_F(x^{\star}, x^{\star}) = 0$,
%\begin{equation*}
%    - \nabla \mathcal E(x,y)\cdot \mathcal G(x,y) \geq \mathcal D_F(x, x^{\star})+\frac{\mu}{2} \|y  - x^{\star}\|^2 + \frac{\mu}{2}\|y-x\|^2.
%\end{equation*}
\end{proof}
%
%We can also rescale the gradient flow $x^{\prime} = - \mathcal{A}(x)/\mu$ to improve the exponential decay rate from $e^{-\mu t}$ to $e^{-t}$. The simple scaling $\mathcal A/\mu$ will enlarge $L_{\mathcal A}$ to $L_{\mathcal A}/\mu$ and the condition number $\kappa(\mathcal A)$ remains the same. The advantage of using the accelerated gradient flow~\eqref{eq:AG} is that the spectral radius, when $\mathcal A$ is linear, is also reduced.

The calculation is more clear when $\nabla F(x) = Hx$ is linear with $H = \nabla^2F\geq \mu I$. Then $- \nabla \mathcal E(x,y)\cdot \mathcal G(x,y)$ is a quadratic form of $ (x - x^{\star}, y -x^{\star})^{\intercal}$. We calculate the corresponding matrix as
$$
\begin{pmatrix}
H & 0 \\
0  & \mu I
\end{pmatrix}
\begin{pmatrix}
  I & - I \\
 - I + \frac{1}{\mu}H &   I+\frac{1}{\mu}\mathcal N
\end{pmatrix}
=
\begin{pmatrix}
 H & - H \\
 - \mu I + H &  \mu + \mathcal N
\end{pmatrix}.
$$
%\begin{align*}
%- \nabla \mathcal E(x,y)\cdot \mathcal G(x,y) &=
%\begin{pmatrix}
% x - x^{\star}\\
% y -x^{\star}
%\end{pmatrix}^{\intercal}
%\begin{pmatrix}
%\frac{1}{\mu}H & 0 \\
%0  & I
%\end{pmatrix}
%\begin{pmatrix}
%  \mu I & - \mu I \\
% - \mu I + H &   \mu I+\mathcal N
%\end{pmatrix}\begin{pmatrix}
%x-x^{\star} \\
%y-x^{\star}
%\end{pmatrix}\\
%&= \begin{pmatrix}
% x -x^{\star}\\
% y - x^{\star}
%\end{pmatrix}^{\intercal}
%\begin{pmatrix}
% H & - H \\
% - \mu I + H &  \mu + \mathcal N
%\end{pmatrix}
%\begin{pmatrix}
%x-x^{\star} \\
%y-x^{\star}
%\end{pmatrix}.
%\end{align*}
For a quadratic form, $v^{\intercal}Mv = v^{\intercal}\sym(M)v$. We have
\begin{align*}
\sym
\begin{pmatrix}
H & -H \\
 - \mu I + H &  \mu I + \mathcal N
\end{pmatrix}
&=
\begin{pmatrix}
H & - \mu I/2 \\
 - \mu I/2 &  \mu I
\end{pmatrix}\\
&\geq
\begin{pmatrix}
H/2 & 0 \\
0 &  \mu I/2
\end{pmatrix} + \frac{\mu}{2}
\begin{pmatrix}
I & - I  \\
 - I &  I
\end{pmatrix},
\end{align*}
where in the last step we use the convexity $H\geq \mu I$. Then~\eqref{eq:strong acc} follows.
%We obtain the desired result by noticing that
%$$
%\mathcal E(x,y) =
%\begin{pmatrix}
%x-x^{\star} \\
%y-x^{\star}
%\end{pmatrix}^{\intercal}
%\begin{pmatrix}
%H/2 & 0 \\
%0 &  \mu/2
%\end{pmatrix}
%\begin{pmatrix}
%x-x^{\star} \\
%y-x^{\star}
%\end{pmatrix}, \quad \begin{pmatrix}
% x - x^{\star}\\
% y -x^{\star}
%\end{pmatrix}^{\intercal}\begin{pmatrix}
%\mu I & - \mu I  \\
% - \mu I &  \mu I
%\end{pmatrix}\begin{pmatrix}
% x - x^{\star}\\
% y -x^{\star}
%\end{pmatrix}\geq \|y-x\|^2
%$$


To see how the condition number changes using the accelerated gradient flow, we consider the following $2\times 2$ matrix
\begin{equation}
G =
\begin{pmatrix}
-1 & 1\\
1- a & \quad -1 + b{\rm i}
\end{pmatrix}
\end{equation}
with $a\geq 1$ representing the eigenvalue of $\nabla^2F/\mu$ and $b{\rm i}$ for $\mathcal N/\mu$ as the eigenvalue of skew-symmetric matrix is pure imaginary. Then the eigenvalue of $G$ is
%\LC{Write out the detail and show the real part is $-1$ (decay) and radius is $O(\sqrt{a} +|b|)$.} \sqrt{a+\frac{1}{2}(b^2 \pm b\sqrt{b^2+4(a-1)})}
\begin{equation*}
    \lambda (G)  = -1  + \frac{b \pm \sqrt{b^2+4(a-1)}}{2}{\rm i}.
\end{equation*}
The real part is always $-1$ which implies the decay property of ODE $x'=Gx$. The spectral radius is 
$$|\lambda|= \mathcal  O(\sqrt{a}) + \mathcal O(|b|),  \quad \text{as } a\gg 1, |b|\gg 1. $$
As a comparison, $| a + b {\rm i} | =\sqrt{a^2 + b^2} = \mathcal O(a) + \mathcal  O(|b|)$. We accelerate the dependence from $\mathcal  O(a)$ to $\mathcal O(\sqrt{a})$. 

\subsection{Implicit Euler Schemes}
If we apply the implicit Euler method for the accelerated gradient system~\eqref{eq:AG}, the linear convergence is a direct consequence of the strong Lyapunov property~\eqref{eq:strong acc}. Consider
% \LC{Present the implicit Euler as the reference.}
\begin{subequations}
\begin{align}
\label{eq:AGIE1}     \frac{x_{k+1} - x_k}{\alpha_k} &= \mathcal G^x(x_{k+1}, y_{k+1}):=y_{k+1} - x_{k+1},\\
\label{eq:AGIE2}      \frac{y_{k+1} - y_k}{\alpha_k} & = \mathcal G^y(x_{k+1}, y_{k+1}) :=x_{k+1} - y_{k+1} - \frac{1}{\mu}(\nabla F(x_{k+1}) + \mathcal N y_{k+1}).
\end{align}
\end{subequations}
%where $\mathcal G^x$ and $\mathcal G^y$ are defined as the components of $\mathcal G(x,y)$:
%\begin{equation*}
%    \begin{aligned}
%        \mathcal G^x( x_{k+1}, y_{k+1}) &= y_{k+1} - x_{k+1},\\
%        \mathcal G^y(x_{k+1}, y_{k+1}) &=x_{k+1} - y_{k+1} - \frac{1}{\mu}(\nabla F(x_{k+1}) + \mathcal N y_{k+1}).
%    \end{aligned}
%\end{equation*}
As we have shown before, the implicit Euler method is unconditionally stable and yield superlinear convergence with the price of solving a nonlinear equation system where $x_{k+1}$ and $y_{k+1}$ are coupled together, which is usually not practical. We present the convergence analysis here as a reference to measure the difference of other methods.

Denote $\mathcal E_{k} = \mathcal E(x_{k}, y_{k})$ and $z
_k = (x_k , y_k)$. Using the $\mu$-convexity of the Lyapunov function~\eqref{eq: acc Lyapunov, mixed} and the strong Lyapunov property in Lemma \ref{lem: acc strong Lyapunov}, we have
\begin{equation*}
    	\begin{aligned}
\mathcal E_{k+1} - \mathcal E_{k}\leq{}& (\nabla \mathcal E_{k+1},z_{k+1}-z_k) -\frac{\mu}{2}\|z_{k+1}-z_k\|^2\\
		={}& \alpha_k \dual{ \mathcal \nabla \mathcal E_{k+1}, \mathcal G(z_{k+1})} - \frac{\mu}{2}\|z_{k+1}-z_k\|^2\\
		\leq {}&-\alpha_k \mathcal{E}_{k+1} ,
	\end{aligned}
\end{equation*}
from which the linear convergence follows naturally
\begin{equation*}
    	\mathcal E_{k+1}\leq \frac{1}{1+\alpha_k }\mathcal E_{k}
\end{equation*}
for arbitrary step size $\alpha_k>0$.

\subsection{Implicit and Explicit Schemes}\label{sec: IMEX scheme}
If we treat the skew symmetric part implicit, we can achieve the acceleration like the convex optimization problem. Consider the following \textbf{IM}plicit and \textbf{EX}plicit (IMEX) scheme of the accelerated gradient flow:
%\begin{equation}\label{eq: acc gra method semi-implicit}
%    \left \{\begin{aligned}
\begin{subequations}\label{eq:semi-implicit}
\begin{align}
\label{eq:imex1}    \frac{\hat{x}_{k+1}-x_k}{\alpha_k} &=  y_k - \hat{x}_{k+1},  \\
\label{eq:imex2}        \frac{y_{k+1}-y_k}{\alpha_k} &=  \hat{x}_{k+1} - y_{k+1} -\frac{1}{\mu} \left( \nabla F(\hat{x}_{k+1}) + \mathcal Ny_{k+1}\right) , \\
\label{eq:imex3}    \frac{x_{k+1}-x_k}{\alpha_k} &= y_{k+1} - x_{k+1}.
\end{align}
\end{subequations}
We first treat $y$ is known as $y_k$ and solve the first equation to get $\hat{x}_{k+1}$ and then with known $\hat{x}_{k+1}$ to solve the following shifted skew-symmetric equation
\begin{equation}\label{eq:I+N 2}
\left [ (1+\alpha_k)I + \frac{\alpha_k}{\mu} \mathcal N \right ] y_{k+1} = b(\hat{x}_{k+1},y_k),
\end{equation}
with known right hand side $b(\hat{x}_{k+1},y_k) = \alpha_k \hat x_{k+1} - \frac{\alpha_k}{\mu} \nabla F(\hat x_{k+1}) +  y_k $.
Then with computed $y_{k+1}$, we can solve for $x_{k+1}$ again using an implicit discretization of $x^{\prime} = y - x$. In terms of the ODE solvers,~\eqref{eq:semi-implicit} is known as the predictor-corrector method. The intermediate approximation $\hat{x}_{k+1}$ is a predictor and $x_{k+1}$ is a corrector.

As the skew-symmetric part is treated implicitly, the scheme is expected to achieve the accelerated linear rate as the accelerated gradient method for minimizing the convex function $F$.
For simplicity, we denote 
\begin{equation}\label{eq:hatE}
\mathcal{E}_{k} = \mathcal{E}(x_k, y_k),\quad \hat{\mathcal{E}}_{k+1} = \mathcal{E}(\hat x_{k+1}, y_{k+1})
\end{equation}
for the Lyapunov function $\mathcal{E}$ defined in~\eqref{eq: acc Lyapunov, mixed}.


\begin{lemma}\label{lem: acc gra method semi-implicit decay}
Assume function $F \in \mathcal{S}_{\mu} $. Let $(\hat x_k, y_k)$ be the sequence generated by the accelerated gradient method~\eqref{eq:semi-implicit}. Then for $k \geq 0$,
\begin{equation}\label{eq:hatdecay}
\begin{aligned}
\hat{\mathcal{E}}_{k+1} - \mathcal{E}_k \leq&~ -\alpha_k \hat{\mathcal{E}}_{k+1} -\alpha_k  \left\langle \nabla D_F(\hat x_{k+1}, x^{\star}) , y_{k+1}-y_k\right \rangle   -\frac{\mu}{2}\left\|y_{k+1}-y_k\right\|^2 \\
&
 -\frac{\alpha_k \mu}{2}\|y_{k+1}-\hat x_{k+1}\|^2-\frac{\mu}{2} \| \hat{x}_{k+1} - x_k\|^2. 
\end{aligned}
\end{equation}
\end{lemma}
\begin{proof}
The Lyapunov function $\mathcal E(x,y)$ is $\mu$-convex. Thus we have
 \begin{align*}
\hat{\mathcal{E}}_{k+1} - \mathcal{E}_k  \leq {}& \langle \partial_x \mathcal E(\hat{x}_{k+1}, y_{k+1}), \hat{x}_{k+1} - x_k \rangle + \langle \partial_y \mathcal E(\hat{x}_{k+1}, y_{k+1}), y_{k+1} - y_k \rangle\\
& -\frac{\mu}{2} \| \hat{x}_{k+1} - x_k\|^2  -\frac{\mu}{2}\left\|y_{k+1}-y_k\right\|^2.
\end{align*}
Then substitute
\begin{align*}
\hat{x}_{k+1} - x_k &= \alpha_k \mathcal G^x (\hat{x}_{k+1}, y_{k+1}) + \alpha_k (y_k - y_{k+1})\\
y_{k+1} - y_k & = \alpha_k \mathcal G^y (\hat{x}_{k+1}, y_{k+1}),
\end{align*}
and use the strong Lyapunov property~\eqref{eq:strong acc} at $(\hat{x}_{k+1}, y_{k+1})$ to get the desired result.
\end{proof}

The term $-\alpha_k \left\langle \nabla D_F(\hat x_{k+1}; x^{\star}) , y_{k+1}-y_k\right \rangle$ on the right hand side of~\eqref{eq:hatdecay} accounts for $-\alpha_k  \langle \partial_x \mathcal E(\hat{x}_{k+1}, y_{k+1}), y_{k+1} - y_k \rangle$ for using explicit $y_k$ in~\eqref{eq:imex1}, which is again a mis-match term compare with $\mathcal G(\hat{x}_{k+1}, y_{k+1})$ used in the implicit Euler scheme~\eqref{eq:AGIE1}.
%
The correction step~\eqref{eq:imex3} will be used to bound the mis-match term. We restate and generalize the result in ~\cite{luoDifferentialEquationSolvers2021a} in the following lemma.

\begin{lemma}\label{lem:correction step bound}
% For the predictor-corrector method, if the prediction step 
% \begin{equation*}
%\begin{aligned}
%   \frac{\hat{x}_{k+1}-x_k}{\alpha_k} &=  y_k - \hat{x}_{k+1},
%    % \frac{y_{k+1}-y_k}{\alpha_k} &=  \mathcal G^y(\hat x_{k+1}, y_{k+1}, y_k) , 
%\end{aligned}
% \end{equation*}
Assume $\nabla F$ is $L_F$-Lipschitz continuous and $(\hat x_{k+1},y_{k+1})$ is generated from $(x_k, y_k)$ such that: there exists $c_1, c_2, c_3 >0$ satisfying 
\begin{equation}\label{eq:hatLyapunovdiff}
 \hat{\mathcal{E}}_{k+1} - \mathcal{E}_k \leq~ -\alpha_k c_1  \hat{\mathcal{E}}_{k+1} -\alpha_k c_2\left\langle \nabla D_F(\hat x_{k+1}, x^{\star}), y_{k+1}-y_k\right \rangle -\frac{c_3}{2}\left\|y_{k+1}-y_k\right\|^2. 
\end{equation}
Then set $x_{k+1}$ by the relation
\begin{equation}\label{eq:extrapolation}
(1+\alpha_k c_1)(x_{k+1} - \hat x_{k+1}) = \alpha_k c_2 (   y_{k+1} - y_k ). 
\end{equation}
%
%%and $$\nabla_x \mathcal E = \frac{1}{\mu}  D_F( x; x^{\star}).$$
%Then with the correction step \mnote{ write as $(x_{k+1} - x_k)/\alpha_k = ... $}
%\begin{equation}\label{eq:correction step}
% \frac{x_{k+1}  -  x_{k}}{\alpha_k}= \frac{1}{1+\alpha_k}\left(y_k - x_k\right) + c_2 (y_{k+1} -y_k) - c_1\left (x_{k+1} - \hat x_{k+1}\right),
% \end{equation}
%\JW{Using $\hat x_{k+1}$ does not rely on the explicit update formula of $\hat x_{k+1}$\begin{equation}\label{eq:correction step}
% \frac{x_{k+1}  - \hat x_{k+1}}{\alpha_k}= c_2 (y_{k+1} -y_k) - c_1 (x_{k+1} - \hat x_{k+1}),
% \end{equation} }
and choose $\alpha_k>0$ satisfying
\begin{equation}\label{eq:alpha}
\alpha_k^2  L_Fc_2^2 \leq (1+\alpha_k c_1)c_3,
\end{equation}
we have the linear convergence
\begin{equation*}
    \mathcal{E}_{k+1} \leq \frac{1}{1+\alpha_k c_1} \mathcal{E}_k.
\end{equation*}
\end{lemma}
\begin{proof}
% First of all, the correction step~\eqref{eq:correction step} gives the relation
%\begin{equation*}
%\alpha_k c_2 (   y_{k+1} - y_k )= (1+\alpha_k c_1)(x_{k+1} - \hat x_{k+1}).
%\end{equation*}
Using the relation~\eqref{eq:extrapolation}, we rewrite~\eqref{eq:hatLyapunovdiff} into
\begin{equation}\label{eq:hatLyapunovdiff new}
\begin{aligned}
    &(1+\alpha_k c_1)\hat{\mathcal{E}}_{k+1} - \mathcal{E}_k \\
%    \leq&~ -c_3\left\|y_{k+1}-y_k\right\|^2-\alpha_k c_2\left\langle D_F(\hat x_{k+1}; x^{\star}), y_{k+1}-y_k\right \rangle\\
\leq&~  -(1+c_1\alpha_k)\left\langle  \nabla D_F(\hat x_{k+1}, x^{\star}), x_{k+1} - \hat x_{k+1}\right\rangle -\frac{(1+\alpha_k c_1)^2c_3}{2\alpha_k^2 c_2^2}\left\|x_{k+1} - \hat x_{k+1}\right\|^2.
\end{aligned}
\end{equation}
As $D_F(\cdot, x^{\star}) \in \mathcal S_{\mu,L_F}$, we have
\begin{equation}\label{eq: correction decay}
\begin{aligned}
        \mathcal{E}_{k+1} - \hat{\mathcal{E}}_{k+1} &= D_F(x_{k+1}, x^{\star}) - D_F(\hat{x}_{k+1}, x^{\star}) \\
&        \leq \left\langle \nabla D_F(\hat x_{k+1}, x^{\star}),x_{k+1} - \hat x_{k+1}\right\rangle + \frac{L_F}{2}\|x_{k+1} - \hat x_{k+1}\|^2.
\end{aligned}
\end{equation}

Summing $(1+c_1\alpha_k)\times$\eqref{eq: correction decay} and~\eqref{eq:hatLyapunovdiff new} we get
\begin{equation*}
    \begin{aligned}
         (1+c_1\alpha_k) \mathcal{E}_{k+1} - \mathcal{E}_k \leq -\left(\frac{(1+\alpha_k c_1)^2c_3}{2\alpha_k^2 c_2^2  }-\frac{(1+\alpha_k c_1)L_F}{2 }\right)\left\|x_{k+1} - \hat x_{k+1}\right\|^2 \leq 0
    \end{aligned}
\end{equation*}
according to our choice of $\alpha_k$. Rearrange the terms to get the desired inequality.
\end{proof}

\begin{remark}\rm 
The largest step size $\alpha_k$ satisfying~\eqref{eq:alpha} is given by
$$
\alpha_k = \frac{c_1c_3+\sqrt{c_1^2c_3^2 + 4L_F c_2^2c_3}}{2L_F c_2^2} \geq \frac{1}{c_2}\sqrt{\frac{c_3}{L_F}},
$$
where the last one is a simplified formula for the step size satisfying~\eqref{eq:alpha}.
\end{remark}
%\JW{This is the least general case we need.}

We showed the decay of Lyapunov function in Lemma \ref{lem: acc gra method semi-implicit decay} satisfying the assumption of Lemma \ref{lem:correction step bound} with $c_1=c_2= 1, c_3 = \mu$ and the correction step~\eqref{eq:imex3} matches the relation~\eqref{eq:extrapolation}. As a result, we state the following linear convergence rate of the IMEX scheme~\eqref{eq:semi-implicit}.

\begin{theorem}\label{thm: linear convergence of IMEX scheme}
Assume function $F \in \mathcal{S}_{\mu, L_F} $. Let $(x_k, y_k)$ be the sequence generated by the accelerated gradient method~\eqref{eq:semi-implicit} with arbitrary initial value and step size satisfying 
$$ 
\alpha_k^2 L_F \leq (1+\alpha_k) \mu,
$$ 
then for the Lyapunov function~\eqref{eq: acc Lyapunov, mixed},
\begin{equation*}
    \mathcal{E}_{k+1} \leq \frac{1}{1+\alpha_k } \mathcal{E}_k.
\end{equation*}
For $ \alpha_k = \sqrt{\mu/L_F} = 1/\sqrt{\kappa(F)}$, we achieve the accelerated rate
\begin{equation*}
    \mathcal{E}_k \leq \left( \frac{1}{1+1/\sqrt{\kappa(F)}} \right)^{k} \mathcal{E}_0,
\end{equation*}
which implies
\begin{equation*}
\|x_{k}-x^{\star}\|^2 + \|y_{k}-x^{\star}\|^2 \leq \left (\frac{1}{1+1/\sqrt{\kappa(F)} }\right )^{k} 2 \mathcal{E}_0/\mu.
\end{equation*}
\end{theorem}

% \begin{proof}
% First of all,~\eqref{eq:imex3} -~\eqref{eq:imex1} gives the relation
% \begin{equation*}
% \alpha_k (   y_{k+1} - y_k )= (1+\alpha_k)(x_{k+1} - \hat x_{k+1}).
% \end{equation*}
% We have proved in Lemma \ref{lem: acc gra method semi-implicit decay},
% \begin{equation}\label{eq: acc gra method semi-implicit decay}
% \begin{aligned}
%     &(1+\alpha_k)\hat{\mathcal{E}}_{k+1} - \mathcal{E}_k \\
%     \leq&~ -\frac{\mu}{2}\left\|y_{k+1}-y_k\right\|^2-\alpha_k\left\langle D_F(\hat x_{k+1}; x^{\star}), y_{k+1}-y_k\right \rangle\\
% \leq&~  -\frac{\mu(1+\alpha_k)^2}{2\alpha_k^2}\left\|x_{k+1} - \hat x_{k+1}\right\|^2-(1+\alpha_k)\left\langle  D_F(\hat x_{k+1}; x^{\star}), x_{k+1} - \hat x_{k+1}\right\rangle.
% \end{aligned}
% \end{equation}
% For $D_F(x; x^{\star}) \in \mathcal S_{\mu,L}$, we have
% \begin{equation}\label{eq: correction decay}
% \begin{aligned}
%         \mathcal{E}_{k+1} - \hat{\mathcal{E}}_{k+1}&= D_F(x_{k+1}; x^{\star}) - D_F(\hat x_{k+1}; x^{\star})\\ &\leq \left\langle \nabla D_F(\hat x_{k+1}; x^{\star}),x_{k+1} - \hat x_{k+1}\right\rangle + \frac{L}{2}\|x_{k+1} - \hat x_{k+1}\|^2.
% \end{aligned}
% \end{equation}

% Summing $(1+\alpha_k)\times$\eqref{eq: correction decay} and~\eqref{eq: acc gra method semi-implicit decay} we get
% \begin{equation*}
%     \begin{aligned}
%          (1+\alpha_k) \mathcal{E}_{k+1} - \mathcal{E}_k \leq -\left(\frac{\mu(1+\alpha_k)^2}{2\alpha_k^2}-\frac{(1+\alpha_k)L}{2}\right)\left\|x_{k+1} - \hat x_{k+1}\right\|^2 \leq 0
%     \end{aligned}
% \end{equation*}
% according to our choice of $\alpha_k$. Rearrange the terms to get the desired inequality.
% \end{proof}
%\LC{compare to HSS: we achieve the same accelerated rate without solving the xxx.}
For linear problems, comparing with HSS scheme~\eqref{eq:HSS}, we achieve the same accelerated rate for linear and nonlinear systems without computing the inverse of the symmetric part $(\beta I + \nabla^2 F)^{-1}$.

\subsection{Inexact Solvers for the Shifted skew-Symmetric System}\label{sec: Inexact Solver for the Shifted Skew Symmetric System}
In practice, we can relax the inner solver to be an inexact approximation. That is we solve the equation~\eqref{eq:I+N 2} up to a residual $\varepsilon_{\rm in} = b - \left [ (1+\alpha_k)I + \frac{\alpha_k}{\mu} \mathcal N \right ] y_{k+1}$. The scheme can be modified to
\begin{subequations}\label{eq:inexact-implicit}
\begin{align}
\label{eq:inexactimex1}    \frac{\hat{x}_{k+1}-x_k}{\alpha_k} &= y_k - \hat{x}_{k+1},  \\
\label{eq:inexactimex2}        \frac{y_{k+1}-y_k}{\alpha_k} &=  \hat{x}_{k+1} - y_{k+1} -\frac{1}{\mu}\left( \nabla F(\hat{x}_{k+1}) + \mathcal Ny_{k+1}\right) - \frac{\varepsilon_{\rm in}}{\alpha_k}, \\
\label{eq:inexactimex3}    \frac{x_{k+1}-x_k}{\alpha_k} &=  y_{k+1} -\frac{1}{2} (x_{k+1} + \hat x_{k+1}).
\end{align}
\end{subequations}
Notice that in the third step~\eqref{eq:inexactimex3}, we use $\frac{1}{2} (x_{k+1} + \hat x_{k+1})$ instead of $x_{k+1}$ for discretization variable $x$ at $k+1$. The perturbation $\varepsilon_{\rm in}$ is the residual of the linear equation \eqref{eq:I+N 2}.

\begin{corollary}\label{cor:inexact IMEX}
If we compute $y_{k+1}$ such that the residual of~\eqref{eq:inexactimex2}  satisfies
%\begin{equation}
%\| b - \left [ (1+\alpha_k)I + \frac{\alpha_k}{\mu} \mathcal N \right ] y_{k+1} \| \leq \frac{\mu}{2}\| \hat{x}_{k+1} - x_k\|,
%\end{equation}
\begin{equation}
\| \varepsilon_{\rm in} \|^2 \leq \frac{\alpha_k}{2 } \left (\| \hat{x}_{k+1} - x_k\|^2 + \alpha_k \| y_{k+1} - \hat x_{k+1}\|^2 \right ),
\end{equation}
then  for $(x_k, y_k)$ be the sequence generated by the inexact accelerated gradient method~\eqref{eq:inexact-implicit} with arbitrary initial value and step size satisfying 
$$
\alpha_k^2L_F \leq (1+\alpha_k/2)\mu,
$$
we have the linear convergence
\begin{equation*}
    \mathcal{E}_{k+1} \leq \frac{1}{1+\alpha_k/2} \mathcal{E}_k.
\end{equation*}
For $\alpha_k= \sqrt{\mu/L_F} = 1/\sqrt{\kappa(F)}$, we achieve the accelerated rate
\begin{equation*}
    \mathcal{E}_k \leq \left( \frac{1}{1+1/(2\sqrt{\kappa(F)})} \right)^{k} \mathcal{E}_0.
\end{equation*}
\end{corollary}
\begin{proof}
% For the modified scheme,~\eqref{eq:inexactimex1} -~\eqref{eq:inexactimex3} gives the relation
% \begin{equation*}
% \alpha_k (y_{k+1} - y_k )= (1+\alpha_k/2)(x_{k+1} - \hat x_{k+1}).
% \end{equation*}
We write~\eqref{eq:inexactimex2} as 
$$
y_{k+1} - y_k = \alpha_k \mathcal G^y (\hat{x}_{k+1}, y_{k+1}) + \varepsilon_{\rm in}.
$$
Compared with Lemma \ref{lem: acc gra method semi-implicit decay}, the inexactness introduces a mis-match term $\mu (y_{k+1} - x^{\star}, \epsilon_{\rm out})$ in $\langle \partial_y \mathcal E(\hat{x}_{k+1}, y_{k+1}), y_{k+1} - y_k \rangle$ which can be bounded by 
\begin{align*}
\mu |(y_{k+1} - x^{\star}, \epsilon_{\rm out}) |&\leq \frac{\alpha_k\mu }{4} \|y_{k+1} - x^{\star}\|^2 + \frac{\mu}{\alpha_k }\|\varepsilon_{\rm in}\|^2 \\
&\leq \frac{\alpha_k \mu}{4} \|y_{k+1} - x^{\star}\|^2 + \frac{\mu}{2} \left (\| \hat{x}_{k+1} - x_k\|^2 +  \alpha_k \| y_{k+1} - \hat x_{k+1}\|^2 \right ).
\end{align*}
Use $- \alpha_k \hat{\mathcal{E}}_{k+1} /2$ to cancel the first term and the additional quadratic term in~\eqref{eq:hatdecay} to cancel the second. Then we have 
\begin{equation}\label{eq: implcit-explicit decay}
    \begin{aligned}
    \hat{\mathcal{E}}_{k+1} - \mathcal{E}_k
    \leq - \frac{\alpha_k}{2} \hat{\mathcal{E}}_{k+1} -\alpha_k\left\langle \nabla D_F(\hat x_{k+1}, x^{\star}), y_{k+1}-y_k\right \rangle-\frac{\mu}{2}\left\|y_{k+1}-y_k\right\|^2,
% \leq&~  -\frac{\mu(1+\alpha_k/2)^2}{2\alpha_k^2}\left\|x_{k+1} - \hat x_{k+1}\right\|^2-(1+\alpha_k/2)\left\langle  D_F(\hat x_{k+1}; x^{\star}), x_{k+1} - \hat x_{k+1}\right\rangle.
\end{aligned}
\end{equation}
which with the correction step~\eqref{eq:inexactimex3} satisfying assumptions in  Lemma \ref{lem:correction step bound} with $c_1= 1/2, c_2 =1$ and $c_3 = \mu$.
% Sum $(1+\alpha_k/2)\times$\eqref{eq: correction decay} with~\eqref{eq: implcit-explicit decay}
% \begin{equation*}
%     \begin{aligned}
%     \left (1+\frac{\alpha_k}{2} \right) \mathcal{E}_{k+1} - \mathcal{E}_k \leq -\left(\frac{\mu(1+\alpha_k/2)^2}{2\alpha_k^2}-\frac{(1+\alpha_k/2)L}{2}\right)\left\|x_{k+1} - \hat x_{k+1}\right\|^2 \leq 0
%     \end{aligned}
% \end{equation*}
% according to our choice of $\alpha_k$. The rest is the same as before. 
\end{proof}


Solvers for the shifted skew-symmetric equation~\eqref{eq:I+N 2} with the form $(\beta I + \mathcal N)y = b(\hat x_{k+1}, y_k)$ is discussed in Section \ref{sec: gradient methods}. In particular,  $\beta = (1+ \sqrt{\mu/L_F})\sqrt{\mu L_F}$ if we choose $\alpha_k = \sqrt{\mu/L_F}$. The inner iteration steps are roughly
$%\frac{L_{B^{\rm sym}}}{ (1+ \sqrt{\mu/L_F})\sqrt{\mu L_F}} \left (\ln \frac{\| \hat{x}_{k+1} - x_k\|}{\| y_{k+1} - y_k\|} + \ln \sqrt{\kappa(F)} \right) = 
C_{\rm in} \sqrt{\kappa (F)} L_{B^{\rm sym}}/L_F.$
The outer iteration is $C_{\rm out}\sqrt{\kappa(F)}$. Constants $C_{\rm in} = \mathcal O(|\ln \varepsilon_{\rm in} |)$ and $C_{\rm out} = \mathcal O(|\ln \epsilon_{\rm out}|)$ depend on the tolerance $\varepsilon_{\rm in}$ for the inner iteration and $\epsilon_{\rm out}$ for the outer iteration. Therefore 
\begin{itemize}
 \item $\nabla F(x_k)$  gradient evaluation: $C_{\rm out}\sqrt{\kappa(F)}$;
 \item $(\beta I + B)^{-1}b$ matrix-vector multiplication: $C_{\rm in} C_{\rm out} \kappa(B^{\rm sym})$,
\end{itemize}
where we use the relation $\kappa (F) L_{B^{\rm sym}}/L_F = \kappa(B^{\rm sym})$.
%The dominated factor  
%$$
%\kappa(F,\mathcal N):=L_{B^{\rm sym}}/\sqrt{\mu L_F} = \sqrt{\kappa(F)} \, L_{B^{\rm sym}}/L_F
%$$
%is scaling invariant when change $x$ to $c x$ as $B$ will change to $c B$. we may call $\kappa(F,\mathcal N)$ the condition number of the coupling.  

%\LC{ List the cost for evaluating the gradient and matrix-vector product.}
%\mnote{ check the scaling. $\alpha \mu$ or $\alpha$ \JW{Checked}}

%By definition $\varepsilon_{\rm in} = b - \left [ (1+\alpha_k)I + \frac{\alpha_k}{\mu} \mathcal N \right ] y_{k+1}$. The asymmetric part is implicit in the sense that we need to solve~\eqref{eq:I+N} to obtain $y_{k+1}$. 
%\LC{Survey the convergence result in Jiang erxiong for the inner solve and comment on the inner iteration steps. Express the rate in terms of $\alpha, \mu$.}

%\breakline
%
%For the explicit scheme, 
%$$
%y_{k+1} - y_k = \alpha_k \mathcal G^y (\hat{x}_{k+1}, y_{k+1}) + \alpha_k \frac{B}{\mu}(y_k - y_{k+1}).
%$$
%The mis-match term is
%\begin{align*}
%\alpha_k (y_{k+1} - x^{\star},  B(y_k - y_{k+1})) \leq \frac{\alpha_k\mu}{4} \|y_{k+1} - x^{\star}\|^2 + \frac{\alpha_k\|B\|^2}{\mu} \| y_{k+1} - y_k\|^2
%\end{align*}
%Then choose $\alpha_k$ s.t.
%$$
%\alpha_k \leq \frac{\mu^2}{4\|B\|^2}.
%$$
%and use the extra quadratic term $-\frac{\mu}{2} \| y_{k+1} - y_k\|^2$  to cancel the second one. Other parts are the same. 
%
%\breakline

\subsection{Accelerated Gradient and skew-Symmetric Splitting Methods}
Combining the MEX scheme in Section \ref{sec: IMEX scheme} and accelerated overrelexation technique in Section \ref{sec: AOR for nonlinear problems}, we propose the following explicit discretization of the accelerated gradient flow:
\begin{subequations}\label{eq:explicit}
\begin{align}
\label{eq:AGexplicit1}    \frac{\hat x_{k+1}-x_k}{\alpha} &=  y_k - \hat x_{k+1}, \\
\label{eq:AGexplicit2}   \frac{y_{k+1}-y_k}{\alpha} &= \hat x_{k+1} - y_{k+1}- \frac{1}{\mu}\left( \nabla F(\hat x_{k+1}) + B^{\rm sym} y_k - 2B y_{k+1} \right), \\
\label{eq:AGexplicit3}      \frac{x_{k+1}-x_{k}}{\alpha} &= y_{k+1} -\frac{1}{2} (x_{k+1} + \hat x_{k+1}).
\end{align}
\end{subequations}
The update of $y_{k+1}$ is equivalent to solve a lower triangular linear algebraic system
\begin{equation*}
  \left( (1+\alpha) I +\frac{2\alpha}{\mu} B \right )y_{k+1} = b(\hat x_{k+1}, y_k)
\end{equation*}
with $b(\hat{x}_{k+1},y_k) = y_k+\alpha \hat x_{k+1} - \frac{\alpha}{\mu} \nabla F(\hat x_{k+1}) - \frac{\alpha}{\mu} (B^{\intercal}+ B) y_k$ which can be computed efficiently by a forward substitution and thus no inexact inner solver is needed. Subtracting~\eqref{eq:AGexplicit3} from~\eqref{eq:AGexplicit1} implies the relation~\eqref{eq:extrapolation} with $c_1=1/2, c_2 = 1$. 
%In the saddle point system with bilinear coupling, the nonzero block on the lower-triangular part is associated to the coupling matrix. Then $y_{k+1}$ can be computed efficiently by taking advantage of the block-wise structure; see Section \ref{sec:saddle point systems}. 

Consider the modified Lyapunov function:
\begin{equation}\label{eq: acc discrete Lyapunov}
\begin{aligned}
\mathcal E^{\alpha B}(x, y) &=  D_F(x, x^{\star}) + \frac{1}{2}\|y-x^{\star}\|^2_{ \mu I -  \alpha B^{\rm sym}}.
\end{aligned}
\end{equation}
For $\displaystyle 0 <  \alpha <\frac{\mu}{L_{B^{\rm sym}}}$, we have $\mu I - \alpha B^{\rm sym}$ is positive definite. Then $\mathcal{E} \geq 0$ and $\mathcal E= 0$  if and only if $x = y = x^{\star}$. We denote 
$$
\mathcal E^{\alpha B}_k =\mathcal E^{\alpha B}(x_k, y_k), \quad \hat{\mathcal{E}}^{\alpha B}_k =\mathcal E^{\alpha B}(\hat{x_k}, y_k).
$$

\begin{lemma}\label{lem: acc gra method decay}
Assume function $F \in \mathcal{S}_{\mu,L_F} $. Let $(\hat x_k, y_k)$ be the sequence generated by the accelerated gradient method~\eqref{eq:explicit} with $0< \alpha \leq \displaystyle \frac{\mu}{2L_{B^{\rm sym}}}$. Then, for $k\geq 0$, the modified Lyapunov function~\eqref{eq: acc discrete Lyapunov} satisfies
\begin{equation}\label{eq:acc gra method decay}
\begin{aligned}
\hat{\mathcal{E}}^{\alpha B}_{k+1} - \mathcal E^{\alpha B}_k \leq&~ -\frac{\alpha}{2} \hat{\mathcal{E}}^{\alpha B}_{k+1} - \alpha\left\langle \nabla D_F(\hat x_{k+1}, x^{\star}), y_{k+1}-y_k\right\rangle -\frac{\mu}{4}\left\|y_{k+1}-y_k\right\|^2. 
%\\
%%&\\
%&-\frac{\alpha}{2}D_F(\hat x_{k+1} ; x^{\star})-\frac{\alpha \mu}{8} \|y_{k+1} - x^{\star}\|^2 - \frac{\alpha\mu}{2}\| y_{k+1} - \hat x_{k+1}\|^2-\frac{\mu}{2} \|\hat x_{k+1} - x_k\|^2 .
\end{aligned}
\end{equation}
\end{lemma}

\begin{proof}
 We write the difference of the modified Lyapunov function into 
 \begin{equation}
    \begin{aligned} \label{eq: E and EB bridge}
\hat{\mathcal{E}}^{\alpha B}_{k+1} - \mathcal E^{\alpha B}_k=&~\hat{\mathcal{E}}_{k+1}  -\mathcal{E}_{k}-\frac{1}{2}\| y_{k+1} - x^{\star}\|^2_{\alpha B^{\rm sym}}  + \frac{1}{2}\| y_k - x^{\star}\|^2_{\alpha B^{\rm sym}}. 
\end{aligned}  
\end{equation}
where $\hat{\mathcal{E}}_{k+1}, \mathcal{E}_{k}$ are defined in~\eqref{eq:hatE} and $\mathcal{E}$ refers to the Lyapunov function~\eqref{eq: acc Lyapunov, mixed}.
Since $\mathcal E$ is $\mu$-convex,
 \begin{align*}
\hat{\mathcal{E}}_{k+1}  -\mathcal{E}_{k}  \leq{}& \langle \partial_x \mathcal{E}(\hat x_{k+1}, y_{k+1}) , \hat{x}_{k+1} - x_k \rangle -\frac{\mu }{2} \| \hat{x}_{k+1} - x_k\|^2 \\
&~ \langle \partial_y  \mathcal{E}(\hat{x}_{k+1}, y_{k+1}), y_{k+1} - y_k \rangle -\frac{\mu }{2} \| y_{k+1} - y_k\|^2. 
\end{align*}
Then write the scheme as a correction of the implicit Euler scheme 
\begin{align*}
\hat{x}_{k+1} - x_k &= \alpha \mathcal G^x (\hat{x}_{k+1}, y_{k+1}) +  \alpha (y_k - y_{k+1}) ,\\
y_{k+1} - y_k & = \alpha \mathcal G^y (\hat{x}_{k+1}, y_{k+1}) +\frac{\alpha}{\mu} B^{\rm sym}  (y_{k+1} - y_{k}).
\end{align*}

%Holding the mis-match term of $ \mathcal G^y (\hat{x}_{k+1}, y_{k+1})$ and 
%On the mis-match term $ \langle \partial_x \mathcal{E}(\hat x_{k+1}, y_{k+1}) ,  \alpha (y_k - y_{k+1})\rangle$,
According to the proof of Lemma \ref{lem: acc gra method semi-implicit decay}, we get 
\begin{equation}\label{eq:intermediate decay}
\begin{aligned}
\hat{\mathcal{E}}_{k+1} - \mathcal{E}_k \leq&~ -\alpha \hat{\mathcal{E}}_{k+1}-\alpha\left\langle \nabla D_F(\hat x_{k+1}, x^{\star}), y_{k+1}-y_k\right\rangle -\frac{\mu}{2}\left\|y_{k+1}-y_k\right\|^2  \\
%&- \frac{\alpha\mu}{2}\| y_{k+1} - \hat x_{k+1}\|^2-\frac{\mu}{2} \|\hat x_{k+1} - x_k\|^2  \\
&+ (y_{k+1} - x^{\star}, y_{k+1} - y_k )_{\alpha B^{\rm sym}}.
\end{aligned}
\end{equation}
We use the identity~\eqref{eq:squares} to expand the last cross term in \eqref{eq:intermediate decay} as
\begin{align*}
	&(y_{k+1} - x^{\star}, y_{k+1} - y_k )_{\alpha B^{\rm sym}} \\
	&=  \frac{1}{2}\| y_{k+1} -x^{\star} \|_{\alpha  B^{\rm sym}}^2 + \frac{1}{2} \| y_{k+1} -y_k \|_{ \alpha B^{\rm sym}}^2 - \frac{1}{2} \| y_{k} -x^{\star} \|_{\alpha  B^{\rm sym}}^2.
\end{align*}
Substitute back to~\eqref{eq:intermediate decay} and rearrange terms using~\eqref{eq: E and EB bridge}, 
\begin{equation*}
\begin{aligned}
\hat{\mathcal{E}}^{\alpha B}_{k+1} - \mathcal E^{\alpha B}_k \leq&~ -\alpha \hat{\mathcal{E}}_{k+1}-\alpha\left\langle \nabla D_F(\hat x_{k+1}, x^{\star}), y_{k+1}-y_k\right\rangle -\frac{1}{2}\left\|y_{k+1}-y_k\right\|^2_{\mu I -  \alpha B^{\rm sym}} \\
% - \frac{\alpha\mu}{2}\| y_{k+1} - \hat x_{k+1}\|^2-\frac{\mu}{2} \|\hat x_{k+1} - x_k\|^2  \\
=&~ -\frac{\alpha}{2} \hat{\mathcal{E}}^{\alpha B}_{k+1} -\frac{\alpha}{2} D_F(\hat x_{k+1}, x^{\star})- \frac{\alpha }{4} \|y_{k+1} - x^{\star}\|_{\mu I + \alpha B^{\rm sym}} \\
&-\alpha\left\langle \nabla D_F(\hat x_{k+1}, x^{\star}), y_{k+1}-y_k\right\rangle 
-\frac{1}{2}\left\|y_{k+1}-y_k\right\|^2_{ \mu I-\alpha B^{\rm sym}} \\
%- \frac{\alpha\mu}{2}\| y_{k+1} - \hat x_{k+1}\|^2-\frac{\mu}{2} \|\hat x_{k+1} - x_k\|^2 \\
=&~ -\frac{\alpha}{2} \hat{\mathcal{E}}^{\alpha B}_{k+1}-\alpha\left\langle \nabla D_F(\hat x_{k+1}, x^{\star}), y_{k+1}-y_k\right\rangle -\frac{\mu}{4}\left\|y_{k+1}-y_k\right\|^2\\
&- \frac{\alpha}{2} D_F(\hat x_{k+1}, x^{\star})- \frac{\alpha}{4} \|y_{k+1} - x^{\star}\|_{\mu I + \alpha B^{\rm sym}}-\frac{1}{4}\left\|y_{k+1}-y_k\right\|^2_{\mu I- 2\alpha B^{\rm sym}} .
%\\
%&- \frac{\alpha\mu}{2}\| y_{k+1} - \hat x_{k+1}\|^2-\frac{\mu}{2} \|\hat x_{k+1} - x_k\|^2 . 
\end{aligned}
\end{equation*}
According to our choice $\alpha \leq \displaystyle \frac{\mu}{2L_{B^{\rm sym}}}$,  both $\mu I - 2\alpha B^{\rm sym}$ and $\mu I + \alpha B^{\rm sym}$ are SPD. Dropping the last three non-positive terms we have the desired result.
\end{proof}

The decay of the modified Lyapunov function~\eqref{eq:acc gra method decay} and the appropriate relation~\eqref{eq:extrapolation} satisfies assumptions of Lemma \ref{lem:correction step bound} with $c_1=1/2, c_2 = 1$ and $c_3 = \mu/2$. Although the Lyapunov function is slightly different,~\eqref{eq: correction decay} still holds for $\mathcal E^{\alpha B}$. We conclude with the following linear convergence rate.

\begin{theorem}\label{thm: linear convergence of explicit scheme}
Assume function $F \in \mathcal{S}_{\mu, L_F} $. Let $(x_k, y_k)$ be the sequence generated by the accelerated gradient method~\eqref{eq:explicit} with arbitrary initial value and step size satisfying $$0< \alpha \leq \min \left \{\frac{\mu}{2L_{B^{\rm sym}}}, \sqrt{\frac{ \mu}{2L_F}}\right \},$$ then for the modified Lyapunov function~\eqref{eq: acc Lyapunov, mixed},
\begin{equation*}
    \mathcal E^{\alpha B}_{k+1} \leq \frac{1}{1+\alpha/2 } \mathcal E^{\alpha B}_k.
\end{equation*}
In particular, $\displaystyle \alpha = \min \left \{\frac{\mu}{2L_{B^{\rm sym}}}, \sqrt{\frac{ \mu}{2L_F}}\right \}$, we achieve the accelerated rate
\begin{equation*}
   \|x_{k+1}-x^{\star}\|^2 \leq \frac{2}{\mu} \mathcal E^{\alpha B}_k \leq \left (1+ 1/\max\left \{ 4 \kappa ( B^{\rm sym}), \sqrt{8\kappa(F)}\right \}\right)^{-k} \frac{2\mathcal{E}_0^{\alpha B}}{\mu}.
\end{equation*}
\end{theorem}

% \begin{proof}
%  Notice we proved in Lemma \ref{lem: acc gra method decay},

% \begin{equation}\label{eq: explicit decay}
% \begin{aligned}
%     &\left(1+\alpha/2\right)\hat{\mathcal{E}}_{k+1} - \mathcal{E}_k \leq-\frac{\mu}{4}\left\|y_{k+1}-y_k\right\|^2-\alpha\left\langle \nabla D_F(\hat x_{k+1}, x^{\star}), y_{k+1}-y_k\right\rangle.
% \end{aligned}
% \end{equation}

% By~\eqref{eq:explicitimex1}  and~\eqref{eq:explicitimex3},
% \begin{equation*}
%     \alpha_k(y_{k+1} - y_k) = \left(1 +\alpha/2\right)(x_{k+1} - \hat x_{k+1}).
% \end{equation*}

% Notice 
% \begin{equation}\label{eq: correction decay 2}
% \begin{aligned}
%         \mathcal{E}_{k+1} - \hat{\mathcal{E}}_{k+1}&= D_F(x_{k+1}; x^{\star}) - D_F(\hat x_{k+1}; x^{\star})\\ &\leq \left\langle \nabla D_F(x_{k+1}; x^{\star}) , x_{k+1} - \hat x_{k+1}\right\rangle + \frac{L}{2}\|x_{k+1} - \hat x_{k+1}\|^2.
% \end{aligned}
% \end{equation}
% Multiplying $\left(1+\alpha/2\right)\times$\eqref{eq: correction decay 2} and sum~\eqref{eq: explicit decay} we get
% \begin{equation*}
%     \begin{aligned}
%          &\left(1+\alpha/2\right) \mathcal{E}_{k+1} - \mathcal{E}_k \\
%          \leq&~ -\left(\frac{(1+\alpha/2)^2\mu}{4\alpha^2}-\frac{\left(1+\alpha/2\right)L}{2}\right)\left\|x_{k+1} - \hat x_{k+1}\right\|^2 \\
%          \leq&~ 0
%     \end{aligned}
% \end{equation*}
% according to our choice of $\alpha_k$,
% \begin{equation*}
%       \alpha^2 \leq  \frac{(1+\alpha_k/2)\mu}{2L}.
% \end{equation*}
% \end{proof}
As expected, we have developed an explicit scheme~\eqref{eq:explicit} which achieves the accelerated rate.
%$$
%\frac{1}{1 + 1/\max\{4 \kappa(B^{\rm sym}), \sqrt{8 \kappa(F)}\}}.
%$$
Therefore the cost of gradient $ \nabla F(x_k)$ evaluation and matrix-vector $(\beta I + B)^{-1}b$ multiplication are both $C_{\rm out}\max\{\kappa (B^{\rm sym}), \sqrt{\kappa(F)}\}$. Compare with the inexact IMEX scheme \eqref{eq:semi-implicit}, we may need more gradient evaluation but less matrix-vector multiplication. If $\kappa (B^{\rm sym}) \gg \sqrt{\kappa(F)}$ and $\nabla F(x_k)$ is computationally expensive to evaluate, then the inexact IMEX scheme \eqref{eq:semi-implicit} or its inexact version \eqref{eq:inexact-implicit} is favorable; otherwise if the error tolerance for the inexact inner solve is small, i.e., $C_{\rm in} $ is large, and the cost of $(\beta I + B)^{-1}b$ is comparable to the evaluation of $ \nabla F(x_k)$, then the explicit scheme \eqref{eq:explicit} takes advantage.  
%\LC{ more discussion. depends on $C_{\rm in}$ for the inexact solve and also the relative cost of $B$ and $\nabla F$. If ..., otherwise ...}

%We can write 
%$$
%\kappa (\mathcal N) = \sqrt{\kappa(F)} \, \kappa (F,\mathcal N) =  \sqrt{\kappa(F)} \, \left (\sqrt{\kappa(F)} \, \frac{L_{B^{\rm sym}}}{L_F}\right ).
%$$
%Choosing appropriate inner product $\IX$ or equivalently using a preconditioner $\IV^{-1}$ will improve not only $\kappa(F)$ but also the ratio $L_{B^{\rm sym}}/L_F$.
 
%\LC{ Compare the computational cost with inexact solve IMEX. The difference is for the shifted skew-symmetric equation, the gradient $\nabla F(x)$ is updated or not. If we don't update $\nabla F(x)$ and keep forward substitution, it is like an inexact solver. So if $\nabla F$ is expensive and $\kappa(F,\mathcal N)$ is large, using the inexact version is better?}
%\LC{Can we claim this is optimal?}
%\JW{I haven't seen the lower bound result for using first order methods for this kind of monotone equations. For saddle point systems (with block structure), this rate matches the lower bound for more generalized systems with nonlinear coupling. Maybe the bilinear coupling case can have a better lower bound (no conclusion seen yet). With resolvent $(I+\mathcal N)^{-1}$, the bound is optimal since $\sqrt{\kappa(F)}$ is optimal for convex optimization ($\mathcal N = 0$).}
%Using the additional negative quadratic terms in~\eqref{acc gra method decay}, perturbation argument can be applied for inaccurate $y_{k+1}$ computation. We skipped the proof which follows in line as Corollary \ref{cor:inexact IMEX} to give the upper bound of the perturbation.
%
%
%
%\begin{corollary}
%If we compute $y_{k+1}$ such that the residual of~\eqref{eq: I+2B equation} satisfies 
%\begin{equation}
%\| \varepsilon_{\rm in} \|^2 \leq \frac{\alpha \mu}{4}(\| \hat{x}_{k+1} - x_k\|^2 + \alpha \mu \|y_{k+1} - \hat x_{k+1} \|^2),
%\end{equation}
%then  for  $\alpha \leq \displaystyle \min \left \{\frac{1}{L_{B^{\rm sym}}}, \sqrt{\frac{1}{2L \mu}}\right \}$,
%\begin{equation*}
%    \mathcal{E}_{k+1} \leq \frac{1}{1+\alpha \mu/2} \mathcal{E}_k.
%\end{equation*}
%\end{corollary}

% \begin{proof}
% The only difference in the proof of Lemma \ref{lem: acc gra method decay} write as 
% $$
% y_{k+1} - y_k = \alpha \mathcal G^y (\hat{x}_{k+1}, y_{k+1}) +\frac{\alpha}{\mu}B^{\intercal}  (y_{k+1} - y_{k}) +\frac{\alpha}{\mu}B ( \bar y_{k+1} - y_{k+1}) -\varepsilon_{\rm in}.
% $$
% We can control the additional term in $\langle \partial_y \mathcal E_q(\hat{x}_{k+1}, y_{k+1}), y_{k+1} - y_k \rangle$
% $$
% \mu (y_{k+1} - x^{\star}, \varepsilon_{\rm in}) \leq \frac{\alpha \mu}{8} \|y_{k+1} - x^{\star}\|^2 + \frac{2\mu}{\alpha}\|\varepsilon_{\rm in}\|^2 \leq \frac{\alpha \mu}{8} \|y_{k+1} - x^{\star}\|^2 + \frac{\mu}{2} \| \hat{x}_{k+1} - x_k\|^2.
% $$ 
% Then the additional quadratic terms in~\eqref{acc gra method decay} cancel these terms.
% \end{proof}

\section{Nonlinear Saddle Point Systems}\label{sec:saddle point systems}
In this section, we investigate an important class of optimization problems as an example of the considered monotone operator equation~\eqref{eq:Ax}. We derive optimal algorithms for strongly-convex-strongly-concave saddle point systems. Our algorithms can be relaxed to achieve accelerated linear convergence for convex-concave saddle point systems and inexact solvers. 

\subsection{Problem Setting}
Consider the nonlinear smooth saddle point system with bilinear coupling:
\begin{equation}\label{eq: min-max problem}
    \min_{u\in \mathcal V} \max_{p \in \mathcal Q} \mathcal{L}(u,p) = f(u) - g(p) + (Bu,p)
\end{equation}
where $\mathcal V=\mathbb{R}^m, \mathcal Q=\mathbb{R}^n, m\geq n$ are finite-dimensional Hilbert spaces with inner product induced by SPD operators $\IV, \IQ$, respectively. Functions $f(u) \in \mathcal S_{\mu_{f}, L_{f} },$ and $g(p) \in \mathcal S_{\mu_{g}, L_{g} }$. The operator $B$ is an $n\times m$ matrix of full rank. 
%To simplify notation, we skip $\IV$ and $\IQ$ in the constants $\mu$ and $ L$, e.g., write $\mu_{f, \IV}$ as $\mu_f$ but keep in the condition number, e.g. $\kappa_{\IV}(f)$ to emphasize the condition number measured in the $\IV$ inner product. 

Convex optimization problems with affine equality constraints can be rewritten into a saddle point system~\eqref{eq: min-max problem}: 
\begin{equation}\label{eq: affine equality constrained optimization system}
    \begin{aligned}
    \min_{u\in \mathbb{R}^m} f(u)\\
    \text{subject to} \quad Bu = b.
    \end{aligned}
\end{equation}
Then $p$ is the Lagrange multiplier to impose the constraint $Bu = b$ and $\mathcal{L} (u,p) = f(u) - (b,p) + (Bu, p)$. Notice that $g(p) =(b,p)$ is linear and not strongly convex, i.e., $\mu_g= 0$. 


The point $(u^{\star}, p^{\star})$ solves the min-max problem~\eqref{eq: min-max problem} is said to be a saddle point of $\mathcal{L}(u,p)$, that is
$$\mathcal{L}(u^{\star}, p) \leq \mathcal{L}(u^{\star}, p^{\star}) \leq  \mathcal{L}(u, p^{\star})\quad \forall \ (u,p)\in \mathbb R^m \times \mathbb R^n.$$ 
The saddle point $(u^*, p^*)$ satisfies the first order necessary condition for being the critical point of $\mathcal{L}(u, p)$:
\begin{subequations}\label{eq:critical point system}
\begin{align}
\label{eq:du}    \nabla f(u^{\star}) + B^{\intercal}p^{\star} = 0, \\
\label{eq:dp}    -Bu^* + \nabla g(p^{\star}) = 0.
\end{align}
\end{subequations}
The first equation~\eqref{eq:du} is $\partial_u \mathcal L(u^{\star}, p^{\star})= 0$ but the second one~\eqref{eq:dp} is $- \partial_p \mathcal L(u^{\star}, p^{\star})= 0$. The negative sign is introduced so that  ~\eqref{eq:critical point system} is in the form  $$\mathcal A(x^{\star}) = 0,$$
where $x = (u,p) \in \mathcal V \times \mathcal Q$, $\nabla F = 
\begin{pmatrix}
 \nabla f & 0\\
 0 & \nabla g
\end{pmatrix}
$, and $\mathcal N = \begin{pmatrix}
0 & B^{\intercal} \\
-B & 0
\end{pmatrix}$. To avoid confusion, we use the notation $\mathcal B^{\rm sym} = \begin{pmatrix}
0 & B^{\intercal} \\
B & 0
\end{pmatrix} $ in this section. The splitting ~\eqref{eq:Bs-B} becomes
$$\mathcal N 
=  \begin{pmatrix}
0 & B^{\intercal} \\
B & 0
\end{pmatrix} -  \begin{pmatrix}
0 &0 \\
2B & 0
\end{pmatrix},
\quad 
\text{ and }
\quad
\mathcal N = 
\begin{pmatrix}
 I & 0 \\
 0 & -I
\end{pmatrix}
\mathcal B^{\rm sym}.
$$
Therefore $\|\mathcal N\| = \| \mathcal B^{\rm sym} \|$ for any operator norm.


\subsection{Strongly-Convex-Strongly-Concave Saddle Point Problem}\label{sec: Strongly-Convex-Strongly-Concave Saddle Point Problem}
Given two SPD operators $\IV$ and $\IQ$, we denote by
$\mathcal I_{\mu} = 
\begin{pmatrix}
\mu_f \IV & 0\\
0 & \mu_g \IQ
\end{pmatrix}$
and
$\IX = 
\begin{pmatrix}
\IV & 0\\
0 & \IQ
\end{pmatrix}$.
Then for any $ x = (u,p),y = (v,q) \in  \mathcal V \times \mathcal Q,$
\begin{equation*}
    (\mathcal{A}(x) - \mathcal{A}(y), x-y) \geq \|x-y\|_{\Imu}^2 \geq \mu \|x-y\|_{\IX}^2
\end{equation*}
where $\mu =  \min \{\mu_{f}, \mu_{g}\} $. The accelerated gradient flow and the discrete schemes follows from discussion in Section \ref{sec:acc flow and schemes}. The results are slightly sharp by treating $\mu$ as a block diagonal matrix $\Imu$ and including preconditioners $\IV^{-1}$ and $\IQ^{-1}$.


\subsubsection{The accelerated gradient flow}
The component form of the accelerated gradient flow
$$\left \{\begin{aligned}
     x^{\prime} &= y - x ,\\
     y^{\prime} & = x - y - \mathcal I_{\mu}^{-1}(\nabla F(x) + \mathcal N y)
\end{aligned}\right .
$$
becomes the preconditioned accelerated gradient flow for the saddle point system:
\begin{equation}\label{eq:AG saddle}
\begin{aligned}
     u^{\prime} &= v - u ,\\
     p^{\prime} &= q - p,\\
     v^{\prime} & = u - v - \frac{1}{\mu_{f}}\IV^{-1}(\nabla f(u) + B^{\intercal}q), \\
     q^{\prime} & =p - q - \frac{1}{\mu_{g}}\IQ^{-1}(\nabla g(p) - Bv).
\end{aligned}
\end{equation}
Consider the Lyapunov function:
\begin{equation}\label{eq: acc Lyapunov saddle}
\mathcal{E}(x, y) := D_F(x,x^{\star}) +\frac{1}{2}\| y - x^{\star}\|_{\Imu}^2=  \mathcal E^u(u, v) + \mathcal E^p(p,q),
\end{equation}
with
\begin{equation*}
    \begin{aligned}
    \mathcal E^u(u, v) =  D_f(u, u^{\star})  + \frac{\mu_{f}}{2}\| v-u^{\star}\|_{\IV}^2, \\
    \mathcal E^p(p,q) = D_g(p, p^{\star}) + \frac{\mu_{g}}{2}\| q-p^{\star}\|_{\IQ}^2.
    \end{aligned}
\end{equation*}
As $f, g$ are strongly convex, $\mathcal{E}(x, y)\geq 0$ and $\mathcal{E}(x, y)= 0$ iff $x=y=x^{\star}$. 
%Furthermore, $D_f(\cdot ; u^{\star}) \in \mathcal S_{\mu_{f},L_{f}}$ and $D_g(\cdot ; p^{\star}) \in \mathcal S_{\mu_{g},L_{g}}$.
%
Denote the vector field on the right hand side of~\eqref{eq:AG saddle} by $\mathcal G(x,y)$. We have the strong Lyapunov property
\begin{equation}\label{eq:strong saddle}
\begin{aligned}
- \nabla \mathcal E(x,y)\cdot \mathcal G(x,y) \geq \mathcal E(x,y)+\frac{1}{2}\|y-x\|^2_{\Imu}.
\end{aligned}
\end{equation}
%where $\mu = \min \{\mu_{f}, \mu_{g}\}$.

\subsubsection{Accelerated gradient and skew-symmetric splitting methods}
Recall that $x = (u,p),y = (v,q)$. The accelerated gradient and skew-symmetric splitting method is:
\begin{subequations}\label{eq:explicit saddle}
\begin{align}
 \label{eq:}   \frac{\hat x_{k+1}-x_k}{\alpha} &= y_k - \hat x_{k+1}, \\
\label{eq:explicitsaddle1}       \frac{v_{k+1}-v_k}{\alpha} &=\hat{u}_{k+1} - v_{k+1} - \frac{1}{ \mu_{f} }\IV^{-1} \left( \nabla f(\hat{u}_{k+1}) + B^{\intercal}q_{k}\right) , \\
\label{eq:explicitsaddle2}        \frac{q_{k+1}-q_k}{\alpha} &=  \hat{p}_{k+1} - q_{k+1}-\frac{1}{\mu_{g} } \IQ^{-1}\left( \nabla g(\hat{p}_{k+1}) -2Bv_{k+1} + Bv_k \right) , \\
   \frac{x_{k+1}-x_{k}}{\alpha} &=y_{k+1} - x_{k+1} + \frac{1}{2}(x_{k+1} - \hat x_{k+1}).
\end{align}
\end{subequations}
Each iteration requires $2$ matrix-vector products if we store $Bv_k$, $2$ gradient evaluations and the computation of $\IV^{-1}$ and $\IQ^{-1}$.
Notice that the scheme is explicit as $v_{k+1}$  can be first updated by~\eqref{eq:explicitsaddle1} and then used to generate $q_{k+1}$ in~\eqref{eq:explicitsaddle2}.

Consider the tailored discrete Lyapunov function:
\begin{equation}\label{eq: acc discrete Lyapunov saddle}
\begin{aligned}
\mathcal E^{\alpha B}(x, y) :={}& D_F(x,x^{\star}) + \frac{1}{2}\| y - x^{\star}\|_{\Imu - \alpha \mathcal B^{\rm sym}}^2. 
%D_f(u;u^{\star}) +D_g(p;p^{\star}) + \frac{ \mu_{f}}{2}\|v-u^{\star}\|_{\IV}^2+ \frac{ \mu_{g}}{2}\|q-u^{\star}\|_{\IQ}^2\\
% &- 2\alpha  (B(v-u^{\star}), q- p^{\star}). 
\end{aligned}
\end{equation}
Using the first order necessary conditions ~\eqref{eq:critical point system}, the Bregman divergence part can be related to the duality gap
\begin{equation*}
    \begin{aligned}
    D_F(x, x^{\star})= &~  D_f(u; u^{\star}) + D_g(p; p^{\star}) \\
    =&~  f(u) - f(u^{\star}) - \langle \nabla f(u^{\star}), u - u^{\star}\rangle + g(p) -g(p^{\star}) - \langle  \nabla g(p^{\star}), p - p^{\star}\rangle \\
    =&~f(u) - f(u^{\star}) + \langle B^{\intercal}p^{\star}, u - u^{\star}\rangle + g(p) -g(p^{\star}) - \langle Bu^{\star}, p - p^{\star}\rangle \\
   % =&~ \mathcal L(u, p^{\star}) - \mathcal L(u^{\star}, p^{\star}) +  \mathcal L(u^{\star}, p^{\star}) -\mathcal L(u^{\star}, p)\\
    =&~ \mathcal L(u, p^{\star}) - \mathcal L(u^{\star}, p) \leq \Delta (u,p) . 
    \end{aligned}
\end{equation*}
The convergence analysis will depend on the condition number of the Schur complement $S = B\IV^{-1}B^{\intercal}$ in the $\IQ$ inner product. We first show $\mathcal E^{\alpha B}$ is non-negative if $\alpha$ is sufficiently small. 

\begin{lemma}\label{lem: positivity of Lyapunov}
Assume $f\in \mathcal{S}_{\mu_{f}}$ and  $g\in \mathcal{S}_{\mu_{g}}$. When the step size $$\alpha  \leq \sqrt{\frac{\mu_{f}\mu_{g}}{4L_{S}}}$$ where $L_{S} = \lambda_{\max} (\IQ^{-1} B\IV^{-1}B^{\intercal})$, the matrix
$$
\Imu - 2\alpha \mathcal B^{\rm sym} =
\begin{pmatrix} \mu_f \IV & -2\alpha B^{\intercal} \\
    -2\alpha B &  \mu_g \IQ
   \end{pmatrix}
$$ 
is symmetric and positive semidefinite.
%
%for the Lyapunov function $\mathcal E^{\alpha B}$ defined by~\eqref{eq: acc discrete Lyapunov saddle}, we have  $\mathcal E^{\alpha B}(x, y)\geq 0$ and $\mathcal E^{\alpha B}(x,y)= 0$  if and only if $x= x^*$. 
\end{lemma}
\begin{proof}
We have the block matrix factorization
\begin{equation}\label{eq:Mx-B}
\begin{aligned}
& \begin{pmatrix} \mu_f \IV & -2\alpha B^{\intercal} \\
    -2\alpha B &  \mu_g \IQ
   \end{pmatrix} \\
   &=\begin{pmatrix} I_m & 0 \\
     -\frac{2\alpha}{\mu_f} B\IV^{-1} & I_n
   \end{pmatrix}\begin{pmatrix} \mu_f \IV & 0 \\
    0 & \mu_g \IQ- \frac{4\alpha^2}{\mu_f}B\IV^{-1}B^{\intercal}
   \end{pmatrix} \begin{pmatrix}I_m & -\frac{2\alpha}{\mu_f} \IV^{-1} B^{\intercal} \\
  0  & I_n
   \end{pmatrix}.
\end{aligned}
\end{equation}
%Let  and $\mathcal B^
%{\rm sym}= \begin{pmatrix}
%       0&  B^{\intercal}\\
%       B& 0
%\end{pmatrix}$. 
Then if $\alpha \leq  \sqrt{\mu_f \mu_g/(4L_S)}$, we have $\mu_g \IQ- \frac{4\alpha^2}{\mu_f}B\IV^{-1}B^{\intercal}\geq 0$ and the results follows.
%\begin{equation*}
%\begin{aligned}
%\mathcal E^{\alpha B}(x, y) ={}&  D_f(u;u^{\star}) +D_g(p;p^{\star}) + \frac{1}{2}\|y-x^{\star}\|_{\mathcal I_{\mu} - 2\alpha \mathcal  B^{\rm sym}}^2. 
%\end{aligned}
%\end{equation*}
%It suffices to prove the matrix $\mathcal I_{\mu} - 2\alpha \mathcal B^{\rm sym}$ is positive definite.
%Notice 
%We have
%\begin{equation}\label{eq: positivity bound 1}
%    \frac{1}{2}\|x-x^{\star}\|_{\mathcal I_{\mu}- 2 \alpha \mathcal B^{\rm sym}}^2 = \frac{1}{2}\|z-z^{\star}\|^2_{\mathcal I_{\mathcal Z}} \geq 0
%\end{equation}
%where the change of variable is $z =  \begin{pmatrix}I_m & -\frac{2\alpha}{\mu_f} \IV^{-1} B^{\intercal} \\
%  0  & I_n
%   \end{pmatrix} x$,  and $$\mathcal I_{\mathcal Z} = \begin{pmatrix} \mu_f \IV & 0 \\
%    0 & \mu_g \IQ- \frac{4\alpha^2}{\mu_f}B\IV^{-1}B^{\intercal}
%   \end{pmatrix} $$ is positive semidefinite. 
%   In particular, the positivity of the Lyapunov function is due to the positivity of the Bregman divergence.
\end{proof}

As a result, $\mathcal{E}^{\alpha B} \geq 0$ if $\alpha \leq  \sqrt{\mu_f \mu_g/(4L_S)}$ and  $\mathcal{E}^{\alpha B}(x, y) = 0$ only if $x = x^{\star}$. 
%\mnote{'if' direction needs more conditions on $y, \alpha$ for the quadratic part.}

%\LC{Give a lemma and prove $$L_{\mathcal B^{\rm sym}} = \| \mathcal B^{\rm sym}\|_{\Imu} = \sqrt{L_S}.$$ The norm is w.r.t. $\Imu$ norm. $\mathcal B^{\rm sym} = \lambda \Imu$. And mention the result is sharp by separating $\mu_f, \mu_g$.}

\begin{lemma}\label{lem: LB saddle}
  Assume $f\in \mathcal{S}_{\mu_{f}}$ and  $g\in \mathcal{S}_{\mu_{g}}$. For $\mathcal B^{\rm sym} = \begin{pmatrix}
        0 & B^{\intercal} \\
        B & 0
    \end{pmatrix}$, we have 
    \begin{equation*}
        L_{\mathcal B^{\rm sym}} := \|\mathcal B^{\rm sym}\|_{\Imu} =\sqrt{ \frac{L_S}{\mu_f \mu_g}}, \quad \text{and }\,  \kappa_{\Imu}(\mathcal N) = \sqrt{\frac{L_S}{\mu_f \mu_g}}.
    \end{equation*}
%    Furthermore,
%    \begin{equation}\label{eq: kappa N saddle}
%        \kappa_{\Imu}(\mathcal N):= \frac{L_{B^{\rm sym}} }{\mu} =  \sqrt{\frac{L_S}{\mu_f \mu_g}}.
%    \end{equation}
\end{lemma}

\begin{proof}
    Since $\mathcal B^{\rm sym} \in \mathbb{R}^{(m+n)^2}$ is symmetric, $\mathcal B^{\rm sym}$ has $(m+n)$ real eigenvalues. Let $\lambda$ be the largest absolute value eigenvalue with respect to $\Imu$ and $x = (u,p)$ be the corresponding eigenvector, that is $ \|\mathcal B^{\rm sym}\|_{\Imu} = \lambda$ and 
    \begin{equation*}
        \mathcal B^{\rm sym} x = \lambda \Imu x.
    \end{equation*}
    The component form follows as 
    \begin{equation*}
        \begin{aligned}
             Bu= \lambda \mu_{g} \IQ p, \quad  B^{\intercal}p = \lambda \mu_{f} \IV u.
        \end{aligned}
    \end{equation*}
    Substitute $u$ in the first equation side using the other equation, we get
    \begin{equation*}
                \begin{aligned}
            B\IV^{-1}B^{\intercal}p = \lambda^2 \mu_{f}\mu_{g} \IQ p.
        \end{aligned}
    \end{equation*}
    Since $B\IV^{-1}B^{\intercal}$ is positive definite, we have $|\lambda|  = \sqrt{\frac{L_S}{\mu_f \mu_g}}$ with $L_S = \lambda_{\max}(\IQ^{-1}B\IV^{-1}B^{\intercal})$. Equality on $\kappa_{\Imu}(\mathcal N)$ follows since $\mu = \mu_f = \mu_g = 1$ with respect to $\Imu$ norm.
\end{proof}

%The estimate in Lemma \ref{lem: LB saddle} is sharp in the sense that for the inequality 
%\begin{equation*}
%    (x, y)_{\mathcal B^{\rm sym}} \leq \sqrt{L_S} \|x\|_{\Imu}\|y\|_{\Imu},
%\end{equation*}
%the equality holds if $x=y$ is the eigenvector as describe in the proof. 

Define $\mathcal{E}_k^{\alpha B} := \mathcal E^{\alpha B}(x_k, y_k)$. 
With $\kappa_{\Imu}(\mathcal N)$ shown above and $\kappa(F)$ refined to $\kappa_{\IV}(f)$ and $\kappa_{\IQ}(g)$, 
we have the following linear convergence as a direct corollary of Theorem \ref{thm: linear convergence of explicit scheme}.

\begin{theorem}\label{thm: linear convergence of explicit scheme saddle}
Assume $f\in \mathcal{S}_{\mu_{f}, L_{f}}$ and  $g\in \mathcal{S}_{\mu_{g}, L_{g}}$ with $\mu = \min \{\mu_{f}, \mu_{g}\} >0$. Let $(x_k, y_k)$ be the sequence generated by the accelerated gradient and skew-symmetric splitting method~\eqref{eq:explicit saddle}  with arbitrary initial value and step size satisfying 
$$
0 \leq \alpha \leq \displaystyle \min \left \{\sqrt{\frac{\mu_{f}\mu_{g}}{4 L_{S}}}, \sqrt{\frac{\mu_{f}}{2L_{f}}}, \sqrt{\frac{\mu_{g}}{2L_{g}}}\right \}
$$
where $L_{S} = \lambda_{\max} (\IQ^{-1}B\IV^{-1}B^{\intercal})$. Then for the discrete Lyapunov function~\eqref{eq: acc discrete Lyapunov saddle},
\begin{equation*}
    \mathcal{E}_{k+1}^{\alpha B}\leq \frac{1}{1+ \alpha/2} \mathcal{E}_{k}^{\alpha B}.
\end{equation*}
In particular, for $\alpha =  1/\max \left \{2\kappa_{\Imu}(\mathcal N), \sqrt{2\kappa_{\IV}(f)}, \sqrt{2\kappa_{\IQ}(g)}\right \}$, we achieve the accelerated rate
\begin{equation*}
\begin{aligned}
&\|u_{k}-u^{\star}\|_{\IV}^2 + \|p_{k}-p^{\star}\|_{\IQ}^2  \\
&\leq \left (1+ 1/\max \left \{4\kappa_{\Imu}(\mathcal N), 2\sqrt{2\kappa_{\IV}(f)}, 2\sqrt{2\kappa_{\IQ}(g)}\right \}\right)^{-k} \frac{2\mathcal{E}_0^{\alpha B}}{\mu}.
\end{aligned}
\end{equation*}
\end{theorem}


For strongly-convex-strongly-concave saddle point systems, set $\IV = I_m$ and $\IQ = I_n$, scheme~\eqref{eq:explicit saddle} is an explicit scheme achieving the lower complexity bound: \\$\Omega\left(\sqrt{\kappa(f)+\kappa^2(\mathcal N)+\kappa(g)} \cdot |\ln \epsilon_{\rm out} |\right)$  
%$\Omega\left(\sqrt{\frac{L_f}{\mu_f}+\frac{\|B\|^2}{\mu_f \mu_g}+\frac{L_g}{\mu_g}} \cdot \ln \left(\frac{1}{\varepsilon_{\rm in}}\right)\right)$  
established in ~\cite{zhang2022lower}. Notice that in~\cite{zhang2022lower}, only the theoretical lower bound is proved and no algorithms are developed to match the lower bound. Ours are among the first few explicit schemes achieving this lower bound. 
%We refer to~\cite{thekumparampil2022lifted} for another lifted primal-dual method and more references in introduction.

The condition numbers $\kappa(f), \kappa(g)$, and $\kappa(\mathcal N)$ are scaling invariant. They can be improved using appropriate SPD preconditioners $\IV$ and $\IQ$ with the price of computing $\IV^{-1}$ and $\IQ^{-1}$. The term $\kappa_{\Imu}(\mathcal N) = \sqrt{\frac{L_{S}}{\mu_{f}\mu_{g}}}$ might be the leading term compare with $\sqrt{\kappa_{\IV}(f)}$ and $\sqrt{\kappa_{\IQ}(g)}$. 
% We can always rescale $\IV$ s.t. $\mu_f = 1$. It will not change $\kappa_{\IV}(f)$ but the term $\sqrt{\mu_f \mu_g/(4L_S)}$ becomes $\sqrt{\mu_g/(4L_S)}$. The $\IQ$ is chosen to balance the condition number $\kappa_{\Imu}(\mathcal N)$ and $\sqrt{\kappa_{\IQ}(g)}$. 
The observation is that preconditioner $\IQ^{-1}$ can be chosen so that $L_S$ is small. Namely $\IQ^{-1}$ is a preconditioner for the Schur complement $B\IV^{-1}B^{\intercal}$. For example, if $\IV = I_m$, then the ideal choice is $\IQ = (BB^{\intercal})^{-1}$ so that $L_S = 1$. Such choice of $\IQ$ may increase the condition number $\kappa_{\IQ}(g)$ as $(BB^{\intercal})^{-1}$ may not be a good preconditioner of $g$. Later on we shall show even $\mu_g> 0$, it is better to consider the transformed primal dual (TPD) flow proposed in our recent work~\cite{ChenWei2022Transformed}. 


%\begin{remark}
%For general $\mathcal A$, the preconditioned flow and schemes are well-defined if we have preconditioner for $\nabla F$.
%\end{remark}
\subsubsection{Implicit in the skew-symmetric part}
The leading term $\kappa_{\Imu}(\mathcal N)  = \sqrt{\frac{L_{S}}{\mu_{f}\mu_{g}}}$ is due to the bilinear coupling or equivalently from the explicit treatment for the skew-symmetric component $\mathcal N$. We can treat $\mathcal N$ implicitly and obtain the following IMEX scheme:
\begin{subequations}\label{eq:semi-implicit saddle}
\begin{align}
   \frac{\hat{x}_{k+1}-x_k}{\alpha_k} &= y_k - \hat{x}_{k+1},  \\
       \frac{v_{k+1}-v_k}{\alpha_k} &= \hat{u}_{k+1} - v_{k+1}-  \frac{1}{\mu_{f}}\label{eq:IMEXsaddlev} \IV^{-1} \left( \nabla f(\hat{u}_{k+1}) + B^{\intercal}q_{k+1}\right) , \\
\label{eq:IMEXsaddleq}      \frac{q_{k+1}-q_k}{\alpha_k} &= \hat{p}_{k+1} - q_{k+1} - \frac{1}{\mu_{g}}\IQ^{-1}\left( \nabla g(\hat{p}_{k+1}) -Bv_{k+1}\right) , \\
   \frac{x_{k+1}-x_k}{\alpha_k} &= y_{k+1} - x_{k+1}.
\end{align}
\end{subequations}
%where $\hat x = (\hat u, \hat p)$ and $\mu = \operatorname{diag}\{ \mu_{f} ,  \mu_{g} \}$. Notice 
%We discuss fast inexact inner solvers in Section \ref{sec:inexact inner solver}. 
As the skew-symmetric part is treat implicitly, the restriction $\alpha\leq \sqrt{\frac{\mu_{f}\mu_{g}}{4L_{S}}}$ can be removed. We state the convergence theorem directly as the proofs are illustrated with the strong Lyapunov property in the continuous level and the discussion in Section \ref{sec: IMEX scheme} for IMEX schemes.

\begin{theorem}\label{thm: linear convergence of saddle IMEX scheme}
Assume $f\in \mathcal{S}_{\mu_{f}, L_{f}}$ and  $g\in \mathcal{S}_{\mu_{g}, L_{g}}$. Let $(x_k, y_k)$ be the sequence generated by the preconditioned accelerated gradient method~\eqref{eq:semi-implicit saddle} with arbitrary initial value and step size $\alpha_k$ satisfying $$ \alpha_k^2 L_{f}  \leq ( 1+\alpha_k) \mu_{f} , \quad \alpha_k^2 L_{g} \leq (1+\alpha_k) \mu_{g} ,$$ 
then for the Lyapunov function~\eqref{eq: acc Lyapunov saddle},
\begin{equation}\label{eq:IMEX decay saddle}
    \mathcal{E}_{k+1} \leq   \frac{1}{1+\alpha_k} \mathcal{E}_k.
\end{equation}
In particualr for $ \alpha_k = 1/ \max \{\sqrt{\kappa_{\IV}(f)}, \sqrt{\kappa_{\IQ}(g)}\}$, we achieve the accelerated rate
\begin{equation*}
    \mathcal{E}_k \leq \left( 1+1/ \max \{\sqrt{\kappa_{\IV}(f)}, \sqrt{\kappa_{\IQ}(g)}\} \right)^{-k} \mathcal{E}_0.
\end{equation*}
\end{theorem}


%\begin{proof}
%Recall that the strong Lyapunov property
%$$- \nabla \mathcal E \cdot \mathcal G \geq \mathcal E^u(u, v)  +  \mathcal E^p(p, q) + \frac{\mu_{f} }{2}\|v-u\|^2_{\IV} + \frac{\mu_{g} }{2}\|q-p\|^2_{\IQ}$$
%and followed the proof of Lemma \ref{lem: acc gra method semi-implicit decay}, we have
%\begin{equation}\label{eq:hatdecay saddle}
%\begin{aligned}
%\hat{\mathcal{E}}_{k+1} - \mathcal{E}_k \leq&~ -\alpha_k \hat{\mathcal{E}}^u_{k+1} -\alpha_k \hat{\mathcal{E}}^p_{k+1}\\
%&-\alpha_k  \left\langle \nabla  D_f(\hat u_{k+1}; u^{\star}) , v_{k+1}-v_k\right \rangle-\alpha_k  \left\langle \nabla  D_g(\hat p_{k+1}; p^{\star}) , q_{k+1}-q_k\right \rangle\\
%&
%-\frac{\alpha_k \mu_{f}}{2}\|v_{k+1}-\hat u_{k+1}\|_{\IV}^2-\frac{\alpha_k \mu_{g}}{2}\|q_{k+1}-\hat p_{k+1}\|_{\IQ}^2 \\
%&-\frac{ \mu_{f}}{2} \| \hat{u}_{k+1} - u_k\|^2_{\IV}-\frac{ \mu_{g}}{2} \| \hat{p}_{k+1} - p_k\|^2_{\IQ}  \\
%&-\frac{\mu_{f}}{2}\left\|v_{k+1}-v_k\right\|^2_{\IV} - -\frac{ \mu_{g}}{2}\left\|q_{k+1}-q_k\right\|^2_{\IQ}. 
%\end{aligned}
%\end{equation}
%For the correction step, we use Lemma \ref{lem:correction step bound} with $c_1 = c_2 = c_4 = 1$ and $c_3 = \mu_{f},  \mu_{g}$ respectively and get the desired bound~\eqref{eq:IMEX decay saddle}.
%\end{proof}

We discuss the inner solve on~\eqref{eq:IMEXsaddlev}-\eqref{eq:IMEXsaddleq} which is equivalent to the following linear algebraic equation
\begin{equation}\label{eq:I+N saddle}
\begin{pmatrix}
 (1+\alpha_k)\mu_{f} \IV & \alpha_k B^{\intercal} \\
 -\alpha_k B & (1+\alpha_k)\mu_{g}\IQ
\end{pmatrix}\begin{pmatrix}
 v_{k+1} \\
 q_{k+1}
\end{pmatrix}  = b(\hat{x}_{k+1},y_k),
\end{equation}
with $b(\hat{x}_{k+1},y_k) = \Imu( y_k + \alpha_k \hat x_{k+1} )- \alpha_k \nabla F(\hat x_{k+1})$. When $\IV, \IQ$ are identity, 
solving the equation~\eqref{eq:I+N saddle} basically costs the effort of solving
$$((1+\alpha_k)^2 \mu_f \mu_g I + \alpha_k^2 BB^{\intercal})x = b,$$
which can be solved by conjugate gradient methods with $\mathcal O \left(\frac{\alpha_k \|B\|} {(1+\alpha_k)\sqrt{\mu_f \mu_g}}\right)$ matrix-vector product.
One can also use preconditioned AOR iteration developed in Section \ref{eq:AORlinear} for solving~\eqref{eq:I+N saddle} but in the $\Imu$ inner product: 
\begin{equation*}
    \begin{aligned}
        \frac{v^{\ell+1} - v^\ell}{\alpha^\ell} &= - \frac{1}{\mu_f}\IV^{-1}\left [(1+\alpha_k ) \mu_f\IV v^{\ell+1} + \alpha_k B^{\intercal}q^\ell - b^v(\hat x_{k+1}, y_k)\right ], \\
         \frac{q^{\ell+1} - q^\ell}{\alpha^\ell} &= - \frac{1}{\mu_g}\IQ^{-1}\left [(1+\alpha_k) \mu_g\IQ q^{\ell+1} + \alpha_k B(v^\ell - 2v^{\ell+1})- b^q(\hat x_{k+1}, y_k)\right ].
    \end{aligned}
\end{equation*}
%which is equivalent to preconditioned AOR iteration followed by an extrapolation step: 
%\begin{equation*}
%\begin{aligned}
%    w^{\ell+1} &= v^\ell -\frac{\alpha^\ell \alpha_k}{\mu_f} \IV^{-1} (\nabla f(\hat u_{k+1}) + B^{\intercal}q^\ell ), \\
%    v^{\ell+1} &= \frac{1}{1 + \alpha^\ell(1+\alpha_k) }\left [w^{\ell+1} + \alpha^l(v_k + \alpha_k \hat u_{k+1}) \right ] ,\\
%    r^{\ell+1} & = q^\ell - \frac{\alpha^\ell \alpha_k}{\mu_g}\IQ^{-1}( B(v^\ell - 2v^{\ell+1}) +\nabla g(\hat p_{k+1})), \\
%    q^{\ell+1} &= \frac{1}{1 + \alpha^\ell(1+\alpha_k)}\left [r^{\ell+1} +\alpha^\ell(q_k + \alpha_k \hat q_{k+1}) \right].
%\end{aligned}
%\end{equation*}
%
%\begin{algorithm}
%\caption{AOR iteration for solving~\eqref{eq:I+N saddle}}\label{alg:AOR iteration for saddle}
%\begin{algorithmic}[1]
% \STATE  Set $(v^0, q^0) = (v_k, q_k)$.
%\STATE For $m = 0, 1, \cdots, $, solve 
%\begin{equation*}
%\begin{aligned}
%    v^{m+1} &= \frac{1}{1 + \alpha(1+\alpha_k) \mu_f}(v^m -\alpha \IV^{-1} (\alpha_k B^{\intercal}q^m - b^v(\hat x_{k+1}, y_k))) \\
%    q^{m+1} &= \frac{1}{1 + \alpha(1+\alpha_k) \mu_g}(q^m - \alpha \IQ^{-1}(\alpha_k B(v^m - 2v^{m-1}) - b^q(\hat x_{k+1}, y_k)))
%\end{aligned}
%\end{equation*}
%\STATE Until tolerance reached, return $(v_{k+1}, q_{k+1})= (v^{m+1}, q^{m+1}) $.
%\end{algorithmic}
%\end{algorithm}
The initial value $(v^0, q^0)$ can be set as $(v_k, q_k)$. The inner solver for~\eqref{eq:I+N saddle} can be inexact which is a direct application of Section \ref{sec: Inexact Solver for the Shifted Skew Symmetric System}. We skip the details here. 

%\mnote{ write out the splitting and schemes. a scheme only need $\IV^{-1}$ and $\IQ^{-1}$.} 
Noted that $\hat x_{k+1} = (\hat u_{k+1}, \hat p_{k+1}), y_k= (v_{k}, q_{k})$ are given and not updated in the inner iteration. One AOR iteration is essentially $2$ matrix-vector products when $\IV $ and $\IQ$ are scaled identities. The inner iteration steps is again proportional to 
$\frac{\alpha_k }{1+ \alpha_k}\sqrt{\frac{L_S}{\mu_f \mu_g}}.$ The outer iteration complexity is $O(\frac{1}{\alpha_k})$. 
%For $\alpha_k = \alpha = 1/\sqrt{\kappa(F)}$,  outer iteration step is in the order of $\sqrt{\kappa (F)}$ and the inner iteration step is $\|B\|/\sqrt{ L_F\mu_F}$. That is we require
Therefore we conclude that IMEX scheme~\eqref{eq:semi-implicit saddle} requires
\begin{itemize}
\item gradient evaluation:  $C_{\rm out}\max\{\sqrt{\kappa_{\IV} (f)}, \sqrt{\kappa_{\IQ}(g)}\}$,
\smallskip
\item matrix-vector multiplication: $C_{\rm out}C_{\rm in} \kappa_{\Imu}(\mathcal N)$,
\smallskip
\item preconditioners $\IV^{-1}$ and $\IQ^{-1}$: $C_{\rm out}C_{\rm in} \displaystyle \kappa_{\Imu}(\mathcal N)$,
\end{itemize}
which matches the optimal lower bound in~\cite{zhang2022lower}. 
%\mnote{ check the relation of $L_{B^{\rm sym}}$ and $\|B\|$. } 
%\breakline
%If we define $L_{B^{\rm sym}} = \|B^{\rm sym}\|$: for all $x = (u,p)$,
%\begin{equation*}
%    |(x, x)_{B^{\rm sym}}| \leq \|B^{\rm sym}\| \|x\|^2 = L_{B^{\rm sym}}(\|u\|^2 + \|p\|^2) ,
%\end{equation*}and 
%\begin{equation*}
%    (x, x)_{B^{\rm sym}} = 2(B^{\intercal}u, p) \leq 2\|B\|\|u\|\|p\| \leq \|B\|(\|u\|^2 + \|p\|^2) .
%\end{equation*}
%For the eigenvector of $B^{\rm sym}$ corresponding to the eigenvalue $\lambda$, we have $B^{\intercal}p = \lambda u $ and $Bu = \lambda p $, which implies
%$$BB^{\intercal}p = \lambda^2 p, B^{\intercal}Bu = \lambda^2 u.$$
%Hence $\|B\| = L_{B^{\rm sym}}$.
%\breakline
The preconditioners $\IV^{-1}$ or $\IQ^{-1}$ are designed to balance the gradient evaluation cost and matrix-vector multiplication and preconditioners cost. 


\subsubsection{Implicit in the gradient part}
We can use the generalized gradient flow \eqref{eq: gradient flow} and treat $\nabla F$ implicitly and $\mathcal N$ explicitly with AOR if given the following generalized proximal operations:
\begin{align*}
    \operatorname{prox}_{\gamma f}(v)&:=\underset{u}{\operatorname{argmin}} ~f(u)+\frac{1}{2 \gamma}\|u-v\|^2_{\IV}, \\
     \operatorname{prox}_{\sigma g}(q)&:=\underset{p}{\operatorname{argmin}} ~g(p)+\frac{1}{2 \sigma}\|p-q\|^2_{\IQ}.
\end{align*}
When $\IV, \IQ$ are scaled identity, that matched the typical proximal operators defined in~\eqref{eq:proxl2}. Notice that the proximal operation works for non-smooth functions. The critical point of the saddle point system becomes solving for $(u^{\star}, p^{\star})$ such that
\begin{equation}\label{eq:nonsmooth critical point system}
\begin{aligned}   
0 &\in \partial f(u^{\star}) + B^{\intercal}p^{\star}, \\
  0 &\in  -Bu^* + \partial g(p^{\star}) .
\end{aligned}
\end{equation}
Here we present the algorithm for non-smooth $f,g$: 
\begin{equation}\label{eq:IMEX saddle prox}
\begin{aligned}
    u_{k+1} &= \operatorname{prox}_{\alpha/\mu_f f}(u_k - \IV^{-1}B^{\intercal}p_k), \\
   p_{k+1}  &= \operatorname{prox}_{\alpha/\mu_g g}(p_k - \IQ^{-1}B(u_k - 2u_{k+1})),
\end{aligned}
\end{equation}
%\breakline
%\LC{too complicated. no need to use AG.}
%We can apply implicit scheme on~\eqref{eq:explicitsaddle1} and eliminate $y_{k+1}$ with 
%\begin{equation*}
%    y_{k+1} =  \hat x_{k+1} + \frac{1}{\alpha} (\hat x_{k+1} - \hat x_k ).
%\end{equation*}
%Then we have an accelerated scheme:
%\begin{equation*}
%\begin{aligned}
%   \hat u_{k+1} &=  \operatorname{argmin}_u \frac{\alpha^2}{\mu_{f}} f(u) + \frac{1+2\alpha}{2} \left \|u -\hat v_k\|_{\IV}^2 \right. =  \operatorname{prox}_{\frac{2\alpha^2}{(1+2\alpha)\mu_{f}} f}(\hat v_k), \\
%       \hat p_{k+1} &=  \operatorname{argmin}_p \frac{\alpha^2}{\mu_{g, \IV}} g(p) + \frac{1+2\alpha}{2} \left \|p -\hat q_k\|_{\IQ}^2 \right.  =  \operatorname{prox}_{\frac{2\alpha^2}{(1+2\alpha)\mu_{g}} g}(\hat q_k),
%      \end{aligned}
%\end{equation*}
%where 
%\begin{equation*}
%    \begin{aligned}
%    \hat v_k & = \frac{2+2\alpha}{1+2\alpha}\hat u_k - \frac{1}{1+2\alpha} \hat u_{k-1}-\frac{\alpha}{(1+2\alpha)\mu_{f}} \IV^{-1}B^{\intercal}[(1+\alpha)\hat p_k - \hat p_{k-1}], \\
%    \hat q_k & =\frac{2+2\alpha}{1+2\alpha}\hat p_k - \frac{1}{1+2\alpha} \hat p_{k-1}+\frac{\alpha}{(1+2\alpha)\mu_{g}} \IQ^{-1}B[2(1+\alpha)\hat u_{k+1}-(3+\alpha)\hat u_k + \hat p_{k-1}].
%    \end{aligned}
%\end{equation*}
%\breakline
which is equivalent to the iteration 
%\mnote{ can we include $\mu_f$ and $\mu_g$ in the scheme? Then the Lyapunov function can be also $\mu$ rescaled. 
%\JW{The balance is if we include $\mu$ to estimate the step size and use in the scheme or we have a rough rate $ \mu_F \leq \sqrt{\mu_f \mu_g}$ but only need to estimate Lipschitz constants.}}:
\begin{equation}\label{eq: IMEX saddle prox iteration}
\begin{aligned}
     \frac{u_{k+1} - u_k}{\alpha} &= -\frac{1}{\mu_f}\IV^{-1}\left (\xi_{k+1}+ B^{\intercal} p_k\right), \quad \xi_{k+1} \in \partial f(u_{k+1}),\\
   \frac{p_{k+1} - p_k}{\alpha} &= -\frac{1}{\mu_g}\IQ^{-1}\left (\eta_{k+1}+ B u_k - 2B u_{k+1}\right), \quad \eta_{k+1} \in \partial g(p_{k+1}).
\end{aligned}
\end{equation}

Consider the Lyapunov function for AOR methods: 
\begin{equation}\label{eq: Lyapunov saddle prox}
\begin{aligned}
    	\mathcal{E}^{\alpha B}(u, p) &= \frac{\mu_f}{2}\|u-u^{\star}\|_{\IV}^2 + \frac{\mu_g}{2}\|p-p^{\star}\|_{\IQ}^2 - 2\alpha  (B(u-u^{\star}), p- p^{\star})\\
     &= \frac{1}{2} \| x- x^{\star} \|^2_{ \mathcal I_{\mu} - 2\alpha \mathcal B^{\rm sym}}.
\end{aligned}
\end{equation}
By Lemma \ref{lem: positivity of Lyapunov}, $\mathcal{E}^{\alpha B}(x) \geq 0$ for $0\leq \alpha<\sqrt{\mu_f \mu_g/(4L_S)}$ and $\mathcal{E}^{\alpha B}(x)= 0$  if and only if $x = x^{\star}$. As $\nabla F$ is treated implicitly, the following theorem can be proved similar to Theorem \ref{thm: linear convergence for the shifted skew-symmetric linear system}.  

\begin{theorem}\label{thm: linear convergence for the saddle prox}
Assume $f\in \mathcal{S}_{\mu_{f}}$ and  $g\in \mathcal{S}_{\mu_{g}}$. Let $\{x_k\}$ be the sequence generated by ~\eqref{eq:IMEX saddle prox} with arbitrary initial guess $x_0$ and step size $0\leq \alpha< 1/(2\kappa_{\Imu}(\mathcal N))$ with $\displaystyle \kappa_{\Imu}(\mathcal N) = \sqrt{\frac{L_S}{\mu_f\mu_g}}$ and $L_S=\lambda_{\max} (\IQ^{-1} B\IV^{-1}B^{\intercal})$. Then for the Lyapunov function~\eqref{eq: Lyapunov saddle prox},
	\begin{equation}\label{eq:prox saddle rate}
		\begin{aligned}
			\mathcal{E}^{\alpha B}( x_{k+1}) \leq&~ \frac{1}{1+\alpha }\mathcal{E}^{\alpha B} (x_{k}).
		\end{aligned}
	\end{equation}
In particular, for $ \alpha = \frac{1}{4}\sqrt{\frac{\mu_f\mu_g}{L_S}}$, we have
\begin{equation*}
\| x_k - x^{\star}\|^2 \leq \left (1 + 1/(4\kappa_{\Imu}(\mathcal N))\right )^{-k} 3\| x_0 - x^{\star}\|^2.
\end{equation*}
\end{theorem}
%\LC{Change the flow and Lyapunov function to match the bound $\sqrt{\mu_f\mu_g/L_S}$}
This is indeed the optimal bound shown in ~\cite{zhang2022lower}. Proximal methods combining 
overrelaxation parameters are considered and yielding optimal rates; see~\cite{chambolle2011first,chambolle2016ergodic} for variants of schemes.
%In particular, by choosing $f(u) = \|u\|^2/2$ and $g(p) = \|p\|^2/2$, we can use~\eqref{eq:IMEX saddle prox} to solve~\eqref{eq:I+N saddle} and the inner iteration can be further improved. \mnote{ but the cost including $\IV^{-1}$ and $\IQ^{-1}$}
%\LC{Discussion on the choice of $\IV, \IQ$.}

%    \breakline
%\LC{To be deleted. include to check the correctness.}
%\begin{proof}
%The component form of the gradient flow becomes the preconditioned gradient flow for the saddle point system \mnote{ check the sub-gradient and change to $\xi$ and $\eta$.}:
%\begin{equation}\label{eq:gradient flow saddle}
%\begin{aligned}
%     u^{\prime} & =- \frac{1}{\mu_f}\IV^{-1}(\xi + B^{\intercal}u), \quad \xi(t) \in \partial f(u(t)),\\
%     p^{\prime} &=  -\frac{1}{\mu_g}\IQ^{-1}(\eta - Bp), \quad \eta(t) \in \partial g(p(t)).
%\end{aligned}
%\end{equation}
%Consider the quadratic Lyapunov function:
%\begin{equation}\label{eq: gradient Lyapunov saddle}
%\mathcal{E}_q(x) :=  \frac{\mu_f}{2}\| u-u^{\star}\|_{\IV}^2+ \frac{\mu_g}{2}\| p-p^{\star}\|_{\IQ}^2 = \frac{1}{2}\|x - x^{\star}\|^2_{\mathcal I_{\mu}}.
%\end{equation}
%Denote the vector field on the right hand side of~\eqref{eq:gradient flow saddle} by $\mathcal G(x) = (\mathcal G^u, \mathcal G^p)$. We have the strong Lyapunov property
%\begin{equation}\label{eq:strong gradient saddle}
%\begin{aligned}
%- \nabla \mathcal E_q(x)\cdot \mathcal G(x) &=- \nabla \mathcal E_q(x)\cdot (\mathcal G(x)-\mathcal G(x^{\star}))\\
%&=\langle u- u^{\star}, v -v^{\star} \rangle + \langle p- p^{\star}, q - q^{\star} \rangle \\
%&\geq \mu_f \|u - u^{\star}\|_{\IV}^2 + \mu_g  \|p - p^{\star}\|_{\IQ}^2 \\
%&\geq 2\mathcal E_q(x)
%\end{aligned}
%\end{equation}
%using $\xi \in \partial f(u), \eta^{\star} \in  \partial g(p), \xi^{\star}\in \partial f(u^{\star}), $ and $\eta^{\star}\in \partial g(p^{\star})$ such that~\eqref{eq:nonsmooth critical point system} is safisfied.
%
%  We use the identity for squares~\eqref{eq:squares}:
%\begin{equation}\label{eq:Eqdiff saddle}
%	\frac{1 }{2} \| x_{k+1} - x^{\star}\|_{\mathcal I_{\mu}}^2-\frac{1 }{2} \| x_{k} - x^{\star}\|_{\mathcal I_{\mu}}^2 =  (x_{k+1} -x^{\star}, x_{k+1} - x_k)_{\mathcal I_{\mu}} -\frac{1 }{2} \| x_{k+1} - x_k\|_{\mathcal I_{\mu}}^2. 
%\end{equation}
%We write iteration~\eqref{eq: IMEX saddle prox iteration} as a correction of the implicit Euler scheme
%\begin{align*}
%	u_{k+1} - u_k & =  \alpha (\mathcal G^u( x_{k+1}) -\mathcal G^u(x^{\star}))+ \frac{\alpha}{\mu_f} \IV^{-1} B^{\intercal}(p_{k+1} - p_k) \\
% p_{k+1} - p_k & =  \alpha (\mathcal G^p( x_{k+1}) -\mathcal G^p(x^{\star}))+ \frac{\alpha}{\mu_g}  \IQ^{-1} B(u_{k+1} - u_k).
%\end{align*}
%%For the implicit Euler method, the second term dis-appear and one can easily obtain the unconditional linear convergence by using the monotone 
%For the gradient flow terms, we have
%$$
%- \langle  x_{k+1} - x^{\star},  \mathcal G ( x_{k+1}) -\mathcal G ( x^{\star})  \rangle_{\mathcal I_{\mu}} \leq -2 \mathcal E_q(x_{k+1}) = -  \|x_{k+1} -x^{\star}\|_{\mathcal I_{\mu}}^2. 
%$$
%We use the identity~\eqref{eq:squares} to expand the cross term as
%\begin{equation*}
%\begin{aligned}
%&\alpha ( u_{k+1} -u^{\star},B^{\intercal} (p_{k+1} - p_k)) +  \alpha ( p_{k+1} -p^{\star},B(u_{k+1} - u_k))\\
%    =&~ \alpha (  x_{k+1} -x^{\star}, x_{k+1} - x_k)_{\mathcal B^{\rm sym}} \\
%    =&~  \| x_{k+1} -x^{\star} \|_{\mathcal  B^{\rm sym}}^2 +  \| x_{k+1} -x_k \|_{\mathcal  B^{\rm sym}}^2 - \| x_{k} -x^{\star} \|_{\mathcal  B^{\rm sym}}^2.
%\end{aligned}	
%\end{equation*}
%Substitute back to~\eqref{eq:Eqdiff saddle} and rearrange the terms, we obtain the identity 
%%\mnote{ one more step and check the constant ! \JW{cheched.}}
%\begin{align*}
%{}&	\mathcal E(x_{k+1}) - \mathcal E(x_{k}) \\
%={}& -\alpha \|x_{k+1} -x^{\star}\|_{\mathcal I_{\mu}}^2-\frac{1}{2}\| x_{k+1}- x_k \|_{ \mathcal I_{\mu}- 2\alpha \mathcal B^{\rm sym}}^2\\
%	={}& - \alpha  \, \mathcal E(x_{k+1}) -\frac{\alpha }{2}\| x_{k+1}- x^{\star} \|_{ \mathcal I_{\mu} + 2\alpha \mathcal B^{\rm sym}}^2-\frac{1}{2}\| x_{k+1}- x_k \|_{ \mathcal I_{\mu} - 2\alpha \mathcal B^{\rm sym}}^2.
%\end{align*}
%By Theorem \ref{lem: positivity of Lyapunov} when $0\leq \alpha < \displaystyle \sqrt{\mu_f \mu_g/(4L_S)}$, $\mathcal I_{\mu} \pm 2\alpha \mathcal B^{\rm sym}$ is SPD and the last two terms are non-positive. Dropping them, we obtain the inequality
%$$
%\mathcal E(x_{k+1}) - \mathcal E(x_{k}) \leq - \alpha \, \mathcal E(x_{k+1}) 
%$$
%and~\eqref{eq:prox saddle rate} follows by arrangement.
%						
%When $\alpha = \sqrt{\mu_f \mu_g/(16 L_S)}$, we have the bound	
%\begin{equation}
%\frac{1}{4}\|x-x^{\star}\|^2_{\mathcal I_{\mu}} \leq \mathcal E(x) \leq \frac{3}{4}\|x-x^{\star}\|^2_{\mathcal I_{\mu}}
%\end{equation}
%which implies~\eqref{eq:AORnormrate}.   
%\end{proof}
%
%    \breakline

\subsection{Convex-Concave Saddle Point Problems}\label{sec: Convex-Concave Saddle Point Problem}
The acceleration cannot be recovered for the constrained optimization problem~\eqref{eq: affine equality constrained optimization system} since the corresponding saddle point system (Lagrangian) is not strongly concave with respect to $p$ (recall that $g(p)=(b,p)$ is linear). However,~\eqref{eq: affine equality constrained optimization system} admits a unique solution as long as $f$ is strongly convex. In general, when $f$ is strongly convex, its convex conjugate exists, i.e., $f^*(\xi) = \max_{u \in \mathbb{R}^m}  \ \langle \xi, u \rangle- f(u)$ is well defined and convex. Then~\eqref{eq: min-max problem} is equivalent to the composite optimization problem without constraints:
\begin{equation}\label{eq: dual problem}
    \begin{aligned}
    \min_{p\in \mathbb{R}^n} f^*(-B^{\intercal} p) + g(p),
    \end{aligned}
\end{equation}
which is a strongly convex optimization problem since $\nabla f$ is Lipschitz continuous. In this subsection, we adapt the acceleration technique to the transformed primal-dual flow to develop accelerated gradient methods for convex-concave saddle point problems. 
% and if we further assume that $B$ is full row rank. 
%That is to say,
%\begin{equation*}
%    \mathcal A (x) = \begin{pmatrix}
%             \nabla f & B^{\intercal} \\
%        -B & \nabla g
%    \end{pmatrix}\begin{pmatrix}
%          u \\
%       p
%    \end{pmatrix} 
%\end{equation*}
%is nonsingular for $\mu_f > 0$ and $\mu_g = 0$. 

\subsubsection{Transformed primal-dual flow}
A transformed primal-dual (TPD) flow :
\begin{equation}\label{eq: TPD flow}
\begin{aligned}
 u^{\prime} &= -\IV^{-1}(\nabla f(u) + B^{\intercal} p),\\
p^{\prime} &= - \IQ^{-1}\left(\nabla g(p) +  B\IV^{-1}B^{\intercal}p- B ( u- \IV^{-1} \nabla f(u))\right),  
\end{aligned}
\end{equation}
is proposed in~\cite{ChenWei2022Transformed}, and corresponding numerical discretization achieve linear convergence for convex-concave saddle point systems of this kind. 
%Denote the vector field on the right hand side of~\eqref{eq: TPD flow} by $\mathcal G(u,p) = (\mathcal G^u(u,p), \mathcal G^p(u,p))$. It is straightforward to verify an equilibrium point $(u^{\star},p^{\star})$ such that $\mathcal G(u^{\star},p^{\star}) = 0$ is an critical point satisfying ~\eqref{eq:critical point system}. 
The idea is to show with a transformation, it is equivalent to consider the operator
\begin{align*}
    \mathcal A (x) &:= \begin{pmatrix}
             \nabla f & B^{\intercal} \\
        -B+B\IV^{-1}\nabla f & \nabla g +B\IV^{-1}B^{\intercal}
    \end{pmatrix}\begin{pmatrix}
          u \\
       p
    \end{pmatrix}\\
& =   
\begin{pmatrix}
 I & 0\\
 B\IV^{-1} & I
\end{pmatrix}
   \begin{pmatrix}
             \nabla f & B^{\intercal} \\
        -B & \nabla g
    \end{pmatrix}\begin{pmatrix}
          u \\
       p
    \end{pmatrix}
\end{align*}
where we extend the matrix-vector product notation to nonlinear version $(\nabla f, u):= \nabla f(u)$ and $(\nabla g, p):= \nabla g(p)$. Then $\mathcal A$ is a monotone operator under some mild assumption on $\IV$. The key is the following estimate on the cross term. 

\begin{lemma}[Lemma 3.1 in~\cite{ChenWei2022Transformed}]\label{lem:key}
Suppose $f \in \mathcal{S}_{\mu_{f}, L_{f}}$. 
For any $u_1,u_2\in \mathcal V$ and $p_1, p_2\in \mathcal Q$, we have
\begin{align*}
  & \langle \nabla f(u_1)-\nabla f(u_2), \IV^{-1}B^{\intercal}(p_1 - p_2) \rangle \\
    \geq{} &\frac{\mu_{f}}{2} \|v_1-v_2\|^2_{\IV} - \frac{L_{f}}{2}\| B^{\intercal}(p_1 - p_2)\|^2_{\IV^{-1}} - \frac{1}{2}\langle \nabla f(u_1)-\nabla f(u_2), u_1 - u_2 \rangle,
\end{align*}
where $v = u + \IV^{-1}B^{\intercal} p$ is a transformed variable.
\end{lemma}
For clear illustration, we consider the linear case $\nabla f(u) = H_f u$ with an SPD matrix $H_f\geq \mu_f \IV$. Lemma \ref{lem:key} implies the matrix
$$
\begin{pmatrix}
 H_f & H_f\IV^{-1}B^{\intercal}\\
 B\IV^{-1} H_f & L_f S
\end{pmatrix} \geq 0, \quad S = B\IV^{-1}B^{\intercal}.
$$
Then assuming $\nabla g(p) = H_g p, H_g\geq 0,$ and $L_f < 2$, we have
\begin{align*}
{\rm sym} \mathcal A ={} 
& {\rm sym} 
\begin{pmatrix}
 H_f & B^{\intercal}\\
 -B + B\IV^{-1}H_f & H_g + S
\end{pmatrix}
= 
\begin{pmatrix}
 H_f & \frac{1}{2} H_f\IV^{-1} B^{\intercal}\\
\frac{1}{2}B\IV^{-1}H_f & H_g + S
\end{pmatrix}\\
& = 
\frac{1}{2} 
\begin{pmatrix}
 H_f & H_f\IV^{-1}B^{\intercal}\\
 B\IV^{-1} H_f & L_f S
\end{pmatrix} + 
\frac{1}{2} 
\begin{pmatrix}
 H_f & 0 \\
 0 & 2H_g + (2- L_f)S
\end{pmatrix}\\
&\geq 
\frac{\mu}{2}
\begin{pmatrix}
 \IV & 0 \\
 0 & \IQ
\end{pmatrix}
\end{align*}
with
$$
\mu = \min \{ \mu_f, \mu^+_g\},
$$
where the enhanced convexity constant $\mu^+_g$ is the largest positive constant s.t.
$$
2H_g + (2- L_f)S \geq \mu_g^+ \IQ. 
$$
Switch to the nonlinear case, that is $2g(\cdot) + (2-L_f)\frac{1}{2}\|\cdot\|_S^2\in \mathcal S_{\mu_g^+}$ in the $\IQ$ inner product. 
We have the following lower bounds on $\mu_g^+$
$$\mu_g^+ \geq 2\mu_g + (2- L_f )\mu_S \geq (2- L_f ) \mu_{g_S},$$
where $\mu_S = \lambda_{\min} (\IQ^{-1}S)$ and $g_S (\cdot):= g(\cdot) + \frac{1}{2}\|\cdot\|_S^2$. Even $\mu_g = 0$, the enhanced convexity constant $\mu^+_g \geq (2- L_f )\mu_S > 0$ if given $L_f < 2$. The condition $L_f < 2$ can be always satisfied by rescale $f$ or $\IV$.  
%\LC{Write a lemma for the non-linear case and define $g_S$.}  2\mu_g + (2- L_f )\mu_S

The TPD flow can be simply written as 
\begin{equation}\label{eq: simple TPD flow}
x' = - \IX^{-1} \mathcal A(x),
\end{equation}
where recall that $\IX = {\rm diag} (\IV, \IQ)$. 
Consider the Lyapunov function:
\begin{equation}\label{eq: TPD Lyapunov}
\mathcal E(x) =   \mathcal E(u,p) := \frac{1}{2}\|u-u^{\star}\|_{\IV}^2 +  \frac{1}{2}\|p-p^{\star}\|_{\IQ}^2 = \frac{1}{2}\|x - x^{\star}\|_{\IX}^2. 
\end{equation}
The strong Lyapunov property has been proved in our recent work~\cite{ChenWei2022Transformed}. The proof is essentially the way we verify ${\rm sym} \mathcal A \geq \mu \IX/2$. When adapt to the nonlinear case, use the convention $(\nabla f, u_1- u_2) = (\nabla f, u_1) - (\nabla f, u_2) = \nabla f(u_1) - \nabla f(u_2)$. 

\begin{lemma}[Theorem 3.2 in~\cite{ChenWei2022Transformed}]
Assume function $f \in \mathcal{S}_{\mu_f, L_f}$ with $L_f < 2$. Then for the Lyapunov function~\eqref{eq: TPD Lyapunov} and the  transformed primal-dual gradient flow~\eqref{eq: simple TPD flow}, the following strong Lyapunov property holds
\begin{equation}\label{eq:strong}
\nabla \mathcal E(x)\cdot \IX^{-1} \mathcal A(x) \geq \mu \mathcal E(x).
%+\frac{\mu_f}{2}\|v-v^{\star}\|^2,
\end{equation}
where $\mu =  \min \{ \mu_f, \mu_g^+\}> 0$ . 
%\mnote{ can we change to the second one to $2\mu_g + (2- L_f )\mu_S$? which one is better? \JW{$2\mu_g + (2- L_f )\mu_S$ is better and sharp. Actually the previous $(2-L_f)g_S$ is simplified from this bound.}}
%and  $v = u + \IV^{-1}B^{\intercal} p$ is the transformed variable.
\end{lemma}
%\begin{proof}
%First of all, as $\mathcal G(u^{\star}, p^{\star}) = 0$, $$- \nabla \mathcal E(u,p)\cdot \mathcal G(u,p) = - \nabla \mathcal E(u,p)\cdot (\mathcal G(u,p)  - \mathcal G(u^{\star}, p^{\star})).$$
%
%Direct computation gives
%\begin{align*}
%&- \nabla \mathcal E(u,p)\cdot \mathcal G(u,p) \\
%=&~
%\langle  \nabla f(u) - \nabla f(u^{\star}),u -u^{\star}) \rangle + \nabla g_S(p) - \nabla g_S(p^{\star}),p -p^{\star}) \rangle \\
%& + \langle  \nabla f(u) - \nabla f(u^{\star}),\IV B^{\intercal}(p -p^{\star}) \rangle\\
%\geq&~ \frac{1}{2}\langle  \nabla f(u) - \nabla f(u^{\star}),u -u^{\star}) \rangle + \frac{2-L_f}{2}\langle \nabla g_S(p) - \nabla g_S(p^{\star}),p -p^{\star}) \\
%&+ \frac{\mu_f}{2}\|v-v^{\star}\|^2_{\IV}\\
%\geq&~ \frac{\mu_f}{2}\|u -u^{\star}\|_{\IV}^2 + \frac{(2-L_f)\mu_{g_S}}{2}\|p -p^{\star}\|_{\IQ}^2+ \frac{\mu_f}{2}\|v-v^{\star}\|^2_{\IV}
%\end{align*}
%where we have used Lemma \ref{lem:key} in the second last inequality.
%\end{proof}
Notice $\mathcal A$ cannot be written into the form $\nabla F + \mathcal N$ due to the nonlinear transformation term $B\IV^{-1}\nabla f$ and thus it is not straightforward to apply the current framework. 

\subsubsection{Gradient and skew-symmetric splitting methods}
We have decomposition $\mathcal A = \nabla F + \mathcal N + \delta f$ where $F = f + g_S$ and $\delta f = \displaystyle 
\begin{pmatrix}
0& 0 \\
B\IV^{-1}\nabla f & 0
\end{pmatrix}
$. As $\delta f$ is lower triangular, we can still use the AOR for $\mathcal N$ to obtain an explicit scheme named GSS-TPD: 
\begin{equation}\label{eq:G-S TPD}
\left \{\begin{aligned}
  \frac{u_{k+1} - u_k}{\alpha}&= -\IV^{-1}(\nabla f(u_{k}) + B^{\intercal}p_{k}) \\
\frac{p_{k+1} - p_k}{\alpha} &=-\IQ^{-1}\left [ B\IV^{-1}\nabla f(u_{k+1}) + \nabla g_S(p_{k})- B(2u_{k+1} - u_k)\right ].      
\end{aligned} \right.
\end{equation}
Consider the Lyapunov function 
 \begin{equation}\label{eq: modified Lyapunov}
    \mathcal{E} (x)= \frac{1}{2}\|x-x^{
    \star}\|^2_{\IX- \alpha\mathcal B^{\rm sym}}  -\alpha D_F(x^*, x).
\end{equation}
%where recall that $\IX = \operatorname{diag}\{\IV, \IQ\}$, $\mathcal B^{\rm sym} = \begin{pmatrix}
%       0&  B^{\intercal}\\
%       B& 0
%\end{pmatrix}$ is a symmetric matrix and $D_f$ and $D_{g_S}$ are Bregman divergence of $f$ and $g_S$, respectively. 
\begin{theorem}[Theorem 4.6 in~\cite{ChenWei2022Transformed}]
Suppose $f \in \mathcal{S}_{\mu_{f}, L_{f} }$  with $0< \mu_{f}\leq L_{f}< 2$ and $g_S (\cdot):= g(\cdot) + \frac{1}{2}\|\cdot\|_S^2 \in \mathcal S_{\mu_{g_S}} $. Let $x_k = (u_k,p_k)$ be generated by AOR iteration~\eqref{eq:G-S TPD} with arbitrary initial value $x_0=(u_0, p_0)$ and $\alpha < 1/\max \{ 2\sqrt{L_S}, 2L_{f}, 2L_{g_S} \}$.  Then for the discrete Lyapunov function~\eqref{eq: modified Lyapunov}, we have 
\begin{equation}
\begin{aligned}
\mathcal{E}(x_{k+1})\leq&~ \frac{1}{1+\mu\, \alpha/2}\mathcal{E}(x_k).
\end{aligned}
\end{equation}
where $\mu =  \min \{ \mu_f, \mu_g^+\} > 0$.
\end{theorem}

%\LC{The condition $L_f<2$ can be xxxx}
%
%
%\LC{Even for the case $\mu_g = 0$, we can still achieve the linear convergence rate. xxx write more formal sentences xxx add more on argument Lagrange to deal with the case $\mu_f=0$. }

Even for the case $\mu_g = 0$, we still achieve the linear convergence rate as $\mu_g^+ > 0$. For constrained optimization problem~\eqref{eq: affine equality constrained optimization system}, the strong convexity of $f$ can be further relaxed to the strong convexity of 
\begin{equation*}
    f_{\beta}(u) = f(u) +\frac{\beta}{2}\|Bu-b\|^2 
\end{equation*}
by applying TPD to the augmented Lagrangian. Then even $\mu_{f}=0$, if $\mu_{f_\beta} >0$, we can apply GSS-TPD scheme \eqref{eq:G-S TPD} to $f_{\beta}$. However by adding the augmented term, $\kappa(f)$ is also enlarged to $\kappa(f_\beta)$. If we choose relative large $\beta$, $L_{f_\beta}$ may be large and the step size $\alpha$ will be small. See~\cite[Section 6]{ChenWei2022Transformed} for more discussion.

\begin{remark}\rm
We could introduce factor $\mu_f^{-1}$ and $(\mu_{g}^{+})^{-1}$ in scheme~\eqref{eq:G-S TPD} and refine the rate to
\begin{equation}
   \left (1+\min\left \{ \frac{\mu_f}{8L_f}, \frac{\mu_{g}^+}{8L_{g_S}}, \sqrt{\frac{\mu_f\mu_{g}^+}{16L_S}}\right \}\right)^{-1}
\end{equation} 
for the step size $\alpha = \min\left \{ \frac{\mu_f}{4L_f}, \frac{\mu_{g}^+}{4L_{g_S}}, \sqrt{\frac{\mu_f\mu_{g}^+}{4L_S}}\right \}$. The advantage of using~\eqref{eq:G-S TPD} is that only Lipschitz constants are needed to be estimated while~\eqref{eq:explicit saddle} requires estimate on $\mu_f$ and $\mu_{g_S}$, which is usually harder to obtain.
\end{remark}

\subsubsection{Accelerated transformed primal-dual gradient 
flow}

% \LC{Write out the matrix form. Now in AG, the additional cross term is $(B\IV^{-1}\nabla f(u), q)$ which can be controlled by adding $Sq - Sp$. An explicit scheme can be obtained. The $\mathcal B^{\rm sym}$ is in the form
% $$
% \mathcal B^{\rm sym} = 
% \begin{pmatrix}
%  0 & B^{\intercal}\\
%  B & S
% \end{pmatrix}
% $$
% Probably still need a solver for $S^{-1}$. But the analysis will be simplified a lot. Solved. Change $S$ to $L_S\IQ$. Then only need to compute $\IQ^{-1}$. 
% }
% \breakline

We propose the accelerated transformed primal-dual gradient flow:
\begin{equation}\label{eq:AG TPD}
\begin{aligned}
     u^{\prime} &= v - u ,\\
     v^{\prime} & = \frac{1}{2}(u - v) - \frac{1}{\mu_{f}}\IV^{-1}(\nabla f(u) + B^{\intercal}q), \\
     p^{\prime} &= q - p,\\
     q^{\prime} 
%     & =p-q - \IQ^{-1}(\nabla g(p) - Bv + B\IV^{-1}(\nabla f(u) + B^{\intercal}p)) \\
     &=p-q - \IQ^{-1}(\nabla g_S(p) - Bv + B\IV^{-1}\nabla f(u) ),
\end{aligned}
\end{equation}
where recall that $g_S(p) = g(p) +\frac{1}{2}\|p\|_S^2$. Let $x = (u, p)$ and $y = (v,q)$. Consider the Lyapunov function:
\begin{equation}\label{eq: acc Lyapunov TPD}
\mathcal{E}(x, y) := D_f(u, u^{\star})  + \frac{\mu_{f}}{2}\| v-u^{\star}\|_{\IV}^2 + D_{g_S}(p, p^{\star}) + \frac{1}{2}\| q-p^{\star}\|_{\IQ}^2.
% \mathcal E^u(u, v) + \mathcal E^p(p,q),
\end{equation}
%with
%\begin{equation*}
%    \begin{aligned}
%    \mathcal E^u(u, v) =  D_f(u;u^{\star})  + \frac{\mu_{f}}{2}\| v-u^{\star}\|_{\IV}^2, \\
%    \mathcal E^p(p,q) = D_{g_S}(p;p^{\star}) + \frac{1}{2}\| q-p^{\star}\|_{\IQ}^2.
%    \end{aligned}
%\end{equation*}
As $f, g_S$ are strongly convex, $\mathcal{E}(x, y)\geq 0$ and $\mathcal{E}(x, y)= 0$ iff $x=y=x^{\star}$. 

Denote the vector field on the right hand side of~\eqref{eq:AG TPD} by $\mathcal G(x,y)$. In order to verify the strong Lyapunov property, we first show the matrix calculation when $\nabla f(u) = H_fu, \nabla g(p) = H_g p$ are linear with $H_f = \nabla^2 f, H_g = \nabla^2 g$ being constant SPD matrices. Then $H_{g_S} = H_g + B\IV^{-1}B^{\intercal}$.
As $- \nabla \mathcal E(x,y)\cdot \mathcal G(x,y)$ is a quadratic form of $ (x - x^{\star}, y -x^{\star})^{\intercal}$, we calculate the corresponding matrix in the order of $(u,v, p,q)$ as
%\adjustbox{scale=0.95,center}{
{\small
\begin{align*}
   &\begin{pmatrix}
    H_f & 0  & 0 & 0\\
0  & \mu_f \IV &0 & 0\\
0 & 0& H_{g_S} & 0 \\
0& 0& 0  & \IQ
\end{pmatrix}
\begin{pmatrix}
  I & - I & 0 & 0\\
 - I/2 + \IV^{-1}H_f/\mu_f & 
I/2 & 0 & \IV^{-1}B^{\intercal}/\mu_f \\ 
0 & 0 &I & - I \\
\IQ^{-1}B\IV^{-1}H_f &-\IQ^{-1}B & - I + \IQ^{-1}H_{g_S}& I 
\end{pmatrix}
\\
=&~\begin{pmatrix}
 H_f & - H_f  & 0 & 0 \\
 - \mu_f \IV/2 + H_f &  \mu_f \IV /2 & 0 & B^{\intercal} \\
0 & 0 & H_{g_S} & - H_{g_S} \\
 B\IV^{-1}H_f & -B & -\IQ + H_{g_S} & \IQ
\end{pmatrix}.
\end{align*}
}
%}

For a quadratic form, $x^{\intercal}Mx = x^{\intercal}\sym(M)x$. So we calculate its symmetric part
{\small
\begin{align*}
 &\sym
\begin{pmatrix}
H_f & - H_f  & 0& 0\\
 - \mu_f \IV/2 + H_f &  \mu_f \IV/2  & 0& B^{\intercal} \\
 0& 0& H_{g_S} & -H_{g_S} \\
 B\IV^{-1}H_f & -B & -\IQ + H_{g_S} & \IQ
\end{pmatrix}
\\
&=
\begin{pmatrix}
 H_f &  - \mu_f \IV /4  &0 & B\IV^{-1}H_f /2\\
 - \mu_f \IV /4 &  \mu_f \IV/2  &0 & 0 \\
0 & 0& H_{g_S} &  -\IQ/2\\
 B\IV^{-1}H_f /2&  0& -\IQ/2  & \IQ
\end{pmatrix}\\
=&~
\begin{pmatrix}
H_f/2 &  - \mu_f \IV /4 & 0 &  0\\
 - \mu_f \IV /4 &  \mu_f \IV/2  &0& 0\\ 
0  & 0&  H_{g_S} & -\IQ/2\\
0 &0 & -\IQ/2  & 3\IQ/4 
\end{pmatrix} + 
\begin{pmatrix}
H_f/2 & 0 & 0&  B\IV^{-1}H_f/2 \\
0& 0&0 &0 \\
0& 0& 0& 0\\
B\IV^{-1}H_f/2 &  0&  0& \IQ/4
\end{pmatrix} \\
\geq&~ \frac{1}{4}\operatorname{diag}\{H_f, \mu_f \IV, H_{g_S}, \IQ\}.
\end{align*}
}
For the last inequality, for $(u,v)$-block, as $H_f\geq \mu_f \IV$:
\begin{equation}\label{eq:uv}
\begin{aligned}
  \begin{pmatrix}
H_{f}/2 &  - \mu_f \IV /4  \\
 - \mu_f \IV /4 &  \mu_f \IV/2  
 \end{pmatrix} 
 &=  
 \frac{1}{4}\begin{pmatrix} H_f &  - \mu_f \IV  \\
  - \mu_f \IV  &  \mu_f \IV 
 \end{pmatrix}  + \frac{1}{4}\begin{pmatrix} H_f & 0 \\
 0&  \mu_f \IV  
 \end{pmatrix} \\
 &\geq  \frac{1}{4}\begin{pmatrix} H_f & 0 \\
 0&  \mu_f \IV
 \end{pmatrix}.
\end{aligned}
\end{equation}
For $(p,q)$-block, if $3H_{g_S}/2\geq \IQ$:
\begin{equation}\label{eq:pq}
\begin{aligned}
  \begin{pmatrix}
H_{g_S} & -\IQ/2\\
  -\IQ/2  & 3\IQ/4  
 \end{pmatrix} 
 &=  
 \frac{1}{2}\begin{pmatrix} 3H_{g_S}/2 &  -  \IQ  \\
 -  \IQ &   \IQ  
 \end{pmatrix}  + \frac{1}{4}\begin{pmatrix} H_{g_S} & 0 \\
 0&   \IQ  
 \end{pmatrix} \\
 &\geq  \frac{1}{4}\begin{pmatrix} H_{g_S} & 0 \\
 0& \IQ
 \end{pmatrix}.
\end{aligned}
\end{equation} 
And the rest term for $(u, q)$-block, if  $\IQ \geq 2L_fS$, we apply Lemma \ref{lem:key}:
%\begin{equation}\label{eq:uq}
$$
  \begin{pmatrix}
H_{f}/2 &  B\IV^{-1}H_f/2  \\
  B\IV^{-1}H_f/2 & \IQ/4  
 \end{pmatrix} =  \frac{1}{2}\begin{pmatrix} H_{f} &  B\IV^{-1}H_f  \\
  B\IV^{-1}H_f & L_fS
 \end{pmatrix}  + \frac{1}{2}\begin{pmatrix} 0 & 0 \\
 0&   \IQ/2  - L_fS 
 \end{pmatrix} \geq 0.
 $$
%\end{equation} 
We give parameter choice that satisfying the required conditions and verify the strong Lyapunov property in the following lemma.


\begin{lemma}
Assume $f\in \mathcal{S}_{\mu_{f}, L_{f}}$ with $L_f \leq 3/4$ and $g_S \in \mathcal{S}_{\mu_{g_S}}$. 
Suppose we choose $\IQ$ such that
\begin{equation*}
   2/3-\mu_g \leq \lambda_{\min}( \IQ^{-1} S) \leq \lambda _{\max}( \IQ^{-1} S) \leq 1/(2L_f).
\end{equation*}
Then for the Lyapunov function~\eqref{eq: acc Lyapunov TPD} and the accelerated transformed primal-dual flow vector field $\mathcal G$ defined as the right side of~\eqref{eq:AG TPD}, the following strong Lyapunov property holds
\begin{equation}\label{eq:strong acc TPD}
\begin{aligned}
- \nabla \mathcal E(x,y)\cdot \mathcal G(x,y) 
%\geq  \frac{1}{2}D_f(u; u^{\star}) + \frac{\mu_{f}}{4}\| v-u^{\star}\|_{\IV}^2 + \frac{1}{2}D_{g_S}(p;p^{\star}) +\frac{1}{4}\|q-p^{\star}\|_{\IQ}^2 \\
&\geq \frac{1}{2} \mathcal E(x,y).
\end{aligned}
\end{equation}
\end{lemma}
\begin{proof}
%As $\mathcal G(x^{\star}, x^{\star}) = 0$, $- \nabla \mathcal E(x,y)\cdot \mathcal G(x,y) = - \nabla \mathcal E(x,y)\cdot (\mathcal G(x,y)  - \mathcal G(x^{\star}, x^{\star}))$.
%Then 
We expand the product in four terms using $\mathcal G = (\mathcal G^u, \mathcal G^v, \mathcal G^p, \mathcal G^q)$:
\begin{align*}
- \nabla \mathcal E \cdot \mathcal G &= - \partial_u \mathcal E\cdot \mathcal G^u- \partial_v \mathcal E\cdot \mathcal G^v- \partial_p \mathcal E\cdot \mathcal G^p- \partial_q \mathcal E\cdot \mathcal G^q.
\end{align*}
As $\mathcal G(x^{\star}, x^{\star}) = 0$, we can insert $\mathcal G(x^{\star}, x^{\star})$ into these terms. For the $(u,v)$ terms, direct computation gives
\begin{equation*}
\begin{aligned}
    - \partial_u \mathcal E\cdot \mathcal G^u - \partial_v \mathcal E\cdot \mathcal G^v ={}&\langle \nabla f(u) - \nabla f(u^{\star}), u - u^{\star}\rangle + \mu_{f}/2 \langle v - u^{\star}, u-v\rangle_{\IV} \\
    &+\langle v - u^{\star}, B^{\intercal} (q-p^{\star})\rangle  
\end{aligned}
\end{equation*}
and 
\begin{equation*}
\begin{aligned}
    - \partial_p \mathcal E\cdot \mathcal G^p- \partial_q \mathcal E\cdot \mathcal G^q ={}&\langle \nabla g_S(p) - \nabla g_S(p^{\star}), p - p^{\star}\rangle +\langle q - p^{\star},p-q\rangle_{\IQ} \\
    &-\langle q-p^{\star}, B(v - u^{\star})\rangle + \langle q-p^{\star}, B\IV^{-1}(\nabla f(u) - \nabla f(u^{\star}))\rangle. 
\end{aligned}
\end{equation*}
By Lemma \ref{lem:key},
\begin{equation*}
\begin{aligned}
     &\langle q-p^{\star}, B\IV^{-1}(\nabla f(u) - \nabla f(u^{\star}))\rangle \\
     \geq&~  \frac{\mu_f}{2}\|v - v^{\star}\|_{\IV}^2-\frac{L_f}{2}\|q - p^{\star}\|^2_{S} - \frac{1}{2}\langle \nabla f(u)-\nabla f(u^{\star}), u - u^{\star} \rangle .
\end{aligned}  
\end{equation*}
Adding them together, we get
\begin{equation*}
\begin{aligned}
      - \nabla \mathcal E \cdot \mathcal G\geq{}&  \frac{1}{2}\langle \nabla f(u)-\nabla f(u^{\star}), u - u^{\star} \rangle - \frac{\mu_{f}}{2} \langle v - u^{\star}, u-v\rangle_{\IV} \\
    &+ \langle \nabla g_S(p) - \nabla g_S(p^{\star}), p - p^{\star}\rangle -\frac{L_f}{2}\|q - q^{\star}\|^2_{S}\\
    &- \langle q - p^{\star}, p-q\rangle_{\IQ} 
\end{aligned}
\end{equation*}
We use the identity for squares~\eqref{eq:squares} to expand
\begin{equation*}
    ( q - p^{\star}, p-q )_{\IQ} = \frac{1}{2}(\|p-p^{\star}\|_{\IQ}^2 - \|q-p^{\star}\|_{\IQ}^2 - \|q-p\|_{\IQ}^2 ),
\end{equation*}
and split 
\begin{equation*}
    \langle \nabla g_S(p) - \nabla g_S(p^{\star}), p - p^{\star}\rangle = D_{g_S}(p; p^{\star}) + D_{g_S}(p^{\star}; p).
\end{equation*}
Observe that $\lambda_{\min}(\IQ^{-1}S) \geq 2/3 - \mu_g$ implies 
$\mu_g \IQ + S \geq 2\IQ/3, $
and consequently
\begin{equation*}
   D_{g_S}(p; p^{\star}) + \frac{1}{2}  D_{g_S}(p^{\star}; p) \geq \frac{3}{4}\|p-p^{\star}\|_{\mu_g \IQ + S}^2 \geq \frac{1}{2}\|p-p^{\star}\|_{\IQ}^2.
\end{equation*}
And $\lambda_{\max}(\IQ^{-1}S) \leq 1/(2L_f)$ implies $2L_f S\leq \IQ$. We have
\begin{equation*}
    \begin{aligned}
           \frac{1}{4}\|q - p^{\star}\|_{\IQ}^2 &\geq   \frac{L_f}{2}\|q-p^{\star}\|_S^2.
    \end{aligned}
\end{equation*}
The terms involving $(u, v)$ can be bounded by~\eqref{eq:DFmu} and the identity for squares~\eqref{eq:squares}. Therefore,
\begin{equation*}
\begin{aligned}
      - \nabla \mathcal E \cdot \mathcal G\geq{}&  \frac{1}{2}D_f(u; u^{\star}) + \frac{\mu_{f}}{4}\| v-u^{\star}\|_{\IV}^2 + \frac{1}{2}D_{g_S}(p;p^{\star}) +\frac{1}{4}\|q-p^{\star}\|_{\IQ}^2\\
    &+ \frac{\mu_{f}}{4}\| v-u\|_{\IV}^2 +\frac{1}{2}\| p-q\|_{\IQ}^2.
\end{aligned} 
\end{equation*}
Dropping the last two (nonnegative) quadratic terms we get~\eqref{eq:strong acc TPD}.
\end{proof}

Again the condition on $L_f$ can be easily fulfilled by rescaling $f$ or $\IV$. 
%If $L_f\leq 3/8$, then the spectral conditions of $\IQ^{-1}S$ can be satisfied given a operator $\IQ$ to approximate $S$ well enough so that the spectral radius $\rho(\mathcal I - \IQ^{-1}S) \leq 1/3 $. 
In the next subsection, we will see that the condition number of $\IQ^{-1}S$ will enter the convergence rates of numerical schemes.

\subsubsection{Accelerated transformed primal-dual method}
We propose an accelerated transformed primal-dual (ATPD) method:
\begin{subequations}\label{eq:acc TPD gra}
\begin{align}
\label{eq:ATPD1}      \frac{\hat x_{k+1}-x_k}{\alpha}&= y_k - \hat x_{k+1} ,\\
\label{eq:ATPD2}    \frac{v_{k+1}-v_k}{\alpha} &= \frac{1}{2}(\hat u_{k+1} - v_{k+1}) - \frac{1}{\mu_{f}}\IV^{-1}(\nabla f( \hat u_{k+1}) + B^{\intercal}q_k), \\
\label{eq:ATPD3}   \frac{q_{k+1} - q_k}{\alpha}&= \hat p_{k+1}-q_{k+1} - \IQ^{-1}\left [\nabla g_S(p_{k+1}) + Bv_{k} - 2Bv_{k+1}+B\IV^{-1}f(u_{k+1})\right ],\\
 \label{eq: acc TPD gra correction}   \frac{x_{k+1} -\hat x_{k+1}}{\alpha} &=(y_{k+1} - y_k) - \frac{1}{4} (x_{k+1} - \hat x_{k+1}).
\end{align}
\end{subequations}
Recall that $x = (u,p), y = (v,q)$. Approximation $(\hat u_{k+1}, \hat p_{k+1})$ is first updated by \eqref{eq:ATPD1} and then used to update $(v_{k+1}, q_{k+1})$ by a TPD iteration. The last step is an extrapolation to produce $(u_{k+1}, p_{k+1})$. 

Denote $\mathcal I_{\mu} = 
\begin{pmatrix}
\mu_f \IV & 0\\
0 & \IQ
\end{pmatrix}$. Consider the tailored discrete Lyapunov function:
\begin{equation}\label{eq: acc discrete Lyapunov TPD}
\begin{aligned}
\mathcal{E}^{\alpha B}_k =  \mathcal{E}^{\alpha B}(x_k, y_k) :={}&D_f(u_{k}, u^{\star}) +D_{g_S}(p_{k}, p^{\star}) + \frac{1}{2}\|y_k-x^{\star}\|_{\mathcal I_{\mu} - \alpha \mathcal B^{\sym} }^2.
\end{aligned}
\end{equation}
According to Lemma \ref{lem: positivity of Lyapunov} with $\mu_g =1$, $\mathcal{E}^{\alpha B} \geq 0$ for $0\leq \alpha\leq \sqrt{\mu_f /(4L_S)}$ and $\mathcal{E}^{\alpha B}(x)= 0$  only if $x =  x^{\star}$. 



\begin{theorem}\label{thm: linear convergence of explicit TPD}
Assume $f\in \mathcal{S}_{\mu_{f}, L_{f}}$ with $L_f \leq 3/4$ and $g_S \in \mathcal{S}_{\mu_{g_S}, L_{g_S}}$. 
Suppose we can choose $\IQ$ such that
\begin{equation}\label{eq: approx S condition}
   2/3-\mu_g \leq \lambda_{\min}( \IQ^{-1} S) \leq \lambda _{\max}( \IQ^{-1} S) \leq 1/(2L_f).
\end{equation}
Let $(x_k, y_k)$ be the sequence generated by the accelerated transformed primal-dual gradient method~\eqref{eq:acc TPD gra}
with arbitrary initial guess and $$0< \alpha \leq \displaystyle \min \left \{\sqrt{\frac{\mu_f}{4L_S}}, \sqrt{\frac{\mu_{f}}{2L_{f}}}, \sqrt{\frac{1}{2L_{g_S}}}\right \}.$$
% \mnote{No $\mu_f$ in the last term. Basically that is the convexity of the Lapunov function with respect to $y$ part.} 
 Then for the modified Lyapunov function~\eqref{eq: acc discrete Lyapunov TPD} and $k \geq 0$,
\begin{equation*}
    \mathcal{E}^{\alpha B}_{k+1}\leq \frac{1}{1+ \alpha/4} \mathcal{E}^{\alpha B}_{k}.
\end{equation*}
\end{theorem}

Let us discuss the condition~\eqref{eq: approx S condition}. Given two SPD operators $\IV$ and $\IQ$, we shall show that we can always rescale to $c_v\IV$ and $c_q \IQ$ such that condition~\eqref{eq: approx S condition} holds. For clarity, we denote the Lipschitz constant of $\nabla f$ w.r.t. $c_v\IV$ by $L_{f,c_v}$ and $L_f = L_{f,1}$ for the non-scaled one. Similar notation apply to $L_{g,c_q}$. Then we have the relation $L_{f,c_v} = c_v^{-1}L_f$ and $L_{g,c_q} = c_q^{-1}L_g$. Denoted by $\lambda_{\max}(c_v,c_q) = \lambda _{\max}( (c_q \IQ)^{-1} B(c_v\IV)^{-1}B^{\intercal})$ and abbreviate $\lambda_{\max}(1,1) = \lambda_{\max}$. Then we have the scaling relation $\lambda_{\max}(c_v,c_q) = (c_v\, c_q)^{-1}\lambda_{\max}$. Similar notation is applied to $\lambda_{\min}$. In the sequel $\kappa_{\IQ}(S) = \kappa(\IQ^{-1}B\IV^{-1}B^{\intercal})$. Those non-scaled quantities are considered as known and fixed. We investigate the scaling effect. 

First of all, as $L_{f,c_v}$ is inversely proportional to $c_v$, we can choose 
$$
c_v = \frac{4}{3} L_f \kappa_{\IQ}(S), \text{ s. t. } \quad  L_{f,c_v} = \frac{3}{4}\frac{1}{\kappa_{\IQ}(S)}\leq \frac{3}{4}.
$$ 
With $c_v$ determined, we choose $c_q$ s.t.
$$
(c_v\, c_q)^{-1} \lambda_{\min} = \frac{2}{3}. 
$$
%Then the interval $[\frac{2}{3\lambda_{\min}}, \frac{1}{2L_f \lambda_{\max}}]$ is not empty. We then fix $c_v$ and adjust $c_q$ s.t. $1/(c_vc_q)$ belongs to this interval. 
By construction, we have the desired bound in the $c_v\IV$ and $c_q \IQ$ inner products
$$
   2/3-\mu_{g,c_q} \leq \lambda_{\min}(c_v,c_q) \leq \lambda _{\max}(c_v,c_q) \leq 1/(2L_{f,c_v}).
$$

Then we estimate the bound of iteration steps which is proportional to $1/\alpha$
$$
|\ln \epsilon_{\rm out} |\max \left \{\sqrt{\frac{4L_S(c_v,c_q)}{\mu_{f,c_v}}}, \sqrt{\frac{2L_{f,c_v}}{\mu_{f,c_v}}}, \sqrt{2L_{g_S,c_q}}\right \},
$$
where $L_S(c_v,c_q) = \lambda _{\max}(c_v,c_q)$. Write $L_S(c_v,c_q)/\mu_{f,c_v} = L_S(c_v,c_q)/L_{f,c_v} \kappa_{\IV}(f)$ and use $ L_S(c_v,c_q) = \lambda _{\max}(c_v,c_q) = (c_vc_q)^{-1}\lambda_{\max} = \frac{2}{3} \kappa_{\IQ}(S)$ and $1/L_{f,c_v} = \frac{4}{3}\kappa_{\IQ}(S)$ to bound the term 
$$
\sqrt{\frac{4L_{S}(c_v,c_q)}{\mu_{f,c_v}}} =  \frac{4\sqrt{2}}{3} \sqrt{\kappa_{\IV}(f)}\,  \kappa_{\IQ}(S).
$$ 
Condition \eqref{eq: approx S condition} implies $\mu_{g_S,c_q}\geq 2/3$ and consequently $\sqrt{L_{g_S,c_q}} = \frac{2}{\sqrt{3}} \sqrt{\kappa_{\IQ}(g_S)}$. 
%The bound 
%$$\sqrt{L_{g_S,c_q}} = \sqrt{ L_{g,c_q} +  \kappa_{\IQ}(S) } = \sqrt{ c_q^{-1}L_g + \kappa_{\IQ}(S) }=  \sqrt{ \left (\frac{8L_fL_g}{9\lambda_{\min}} + 1\right ) \kappa_{\IQ}(S)}.$$
So we get the scaling invariant upper bound of iteration complexity
%\begin{equation}\label{eq: scaling invariant iteration complexity}
%    |\ln \epsilon_{\rm out} | \max\left \{ \frac{4\sqrt{2}}{3} \sqrt{\kappa_{\IV}(f)}\,  \kappa_{\IQ}(S),  \sqrt{ \left ( \frac{16L_fL_g}{9\lambda_{\min}} + 2\right ) \kappa_{\IQ}(S)} \right \}.
%\end{equation}
\begin{equation}\label{eq: scaling invariant iteration complexity}
    |\ln \epsilon_{\rm out} | \max\left \{ \frac{4\sqrt{2}}{3} \sqrt{\kappa_{\IV}(f)}\,  \kappa_{\IQ}(S), \frac{2\sqrt{3}}{3} \sqrt{\kappa_{\IQ}(g_S)}  \right \}.
\end{equation}
In particular, for affinely constrained optimization problems, $g_S = S$, we have an explicit scheme with complexity 
\begin{equation}\label{eq:constraintcomplexity}
\mathcal O \left ( | \ln \epsilon_{\rm out} |\sqrt{\kappa_{\IV}(f)}\,  \kappa_{\IQ}(S) \right).
\end{equation}
%When $\IV$ and $\IQ$ are scaled identities, the iteration complexity of the scheme is $O(\sqrt{\kappa(f)}\kappa(BB^{\intercal})))$. 
The preconditioner $\IV$ is designed s.t. $\kappa_{\IV}(f)$ is small. The preconditioner $\IQ$ is chosen s.t. $ \kappa_{\IQ}(S) $ is small. Again the ideal choice is $\IQ = S$ and $\kappa_{\IQ}(S) = 1$. For that purpose, sometimes we may choose simple $\IV$, so that $S^{-1} = (B \IV^{-1} B^{\intercal})^{-1}$ is easy to compute or approximate. For example, $\IV = I$ and $\IQ$ can be obtained by few multigrid cycles or incomplete LU factorization of the SPD matrix $BB^{\intercal}$. 

Iterative methods for computing $S^{-1} r$ can be thought of as an inner iteration which can be a linear inner solver or a non-linear one. For linear solvers, there is no need to computing $S^{-1} r$ accurately. For any convergent method, say $\| I - \IQ^{-1}S\| \leq 1-\delta < 1$, we can bound the condition number $
\kappa_{\IQ} (S)\leq 2/\delta - 1. $
For example, if $\delta = 2/3$, then $\kappa_{\IQ}(S) \leq 2$. If $\delta$ is small, we may use acceleration for the inner linear iterative methods. 


% \breakline
%  For any $\IQ$ we chose, say $\lambda_{\min}(\IQ^{-1}S) = 2/3$ and $\lambda_{\max}(\IQ^{-1}S) = \rho$, \mnote{ $\rho$ is like $\kappa_{\IQ} (S)$. no information is added. }we can bound the condition number 
% $$
% \kappa_{\IQ} (S)\leq \frac{3\rho}{2}. 
% $$
% For example, if $\rho = 4/3$, then $\kappa_{\IQ}(S) \leq 2$. Then the overall gradient evaluation complexity is
% \begin{equation*}
% \mathcal O\left(\frac{3\rho}{2}\sqrt{\kappa_{\IV}(f)}\,| \ln \epsilon_{\rm out} |\right ).
% \end{equation*}
% Provided an efficient linear solver for matrix inversion with matrix-vector product computation cost of $\mcode{Lincost}$, the overall matrix-vector product evaluation complexity is
% \begin{equation*}
% \mathcal O\left(\frac{3\rho}{2}\sqrt{\kappa_{\IV}(f)}\,| \ln \epsilon_{\rm out} | \cdot \mcode{Lincost}\right ).
% \end{equation*}
% \breakline


% and the iteration complexity is $O(\sqrt{\kappa(f)})$. 
% In general, we can relax to any approximated Schur complement $\IQ$ such that spectral radius of $I - \IQ^{-1}S$ is less than $1/3$. We choose the scaling $c_v, c_q$ such that $L_{f, c_v} \leq 3/8$ and the condition~\eqref{eq: approx S condition} is fulfilled with
%$$2/3-\mu_{g,c_q} \leq \lambda_{\min}(c_v,c_q) \leq \lambda _{\max}(c_v,c_q) \leq 4/3 \leq 1/(2L_{f,c_v}).$$
%Then $\kappa_{\IQ}(S) \leq 2$ and the upper bound of (outer) iteration complexity is  $O(\sqrt{\kappa_{\IV}(f)})$. 
The inner solver for $S^{-1}$ can be also a nonlinear one, e.g., the conjugate gradient (CG) method. The bound \eqref{eq:constraintcomplexity}, however, cannot be applied as now the operator associated to CG is non-linear. Instead, the perturbation argument on controlling the residual $ \varepsilon_{\rm in}$ developed in Section \ref{sec: Inexact Solver for the Shifted Skew Symmetric System} can be applied. The inner iteration complexity will be $\mathcal O(|\ln \varepsilon_{\rm in} |\sqrt{\kappa_{\IQ}(S)})$ where a linear operator $\IQ$ is used in the inner iteration as a preconditioner of $S$. Therefore the total iteration complexity for the accelerated transformed primal-dual method is 
$$
\mathcal O\left (|\ln \epsilon_{\rm out} | |\ln \varepsilon_{\rm in} |\sqrt{\kappa_{\IV}(f)\, \kappa_{\IQ}(S)}\right ),
$$ 
which achieves the optimal complexity bound for affinely constrained problems~\cite{salim2022optimal} provided $|\ln \varepsilon_{\rm in} |$ is not too large. 
Again the convexity of $f$ can be relaxed to $f_{\beta}$ if we consider the augmented Lagrangian.

%\breakline

For general strongly-convex-concave saddle point problems, as $\lambda_{\min} = 2/3$ and $\lambda_{\max}\leq L_{g_S}$, the bound \eqref{eq: scaling invariant iteration complexity} can be bounded by 
\begin{equation}\label{eq:generalconstraintcomplexity}
\mathcal O \left ( | \ln \epsilon_{\rm out} |\sqrt{\kappa_{\IV}(f)}\,  \kappa_{\IQ}(g_S) \right).
\end{equation}
%$$
% \mathcal O \left (  |\ln \epsilon_{\rm out} | \big  ( \sqrt{\kappa_{\IV}(f)}\,  \kappa_{\IQ}(S) + \sqrt{\kappa_{\IQ}(S)} + \sqrt{\kappa_{\IQ}(g)} \, \big )
%\right ),
%$$
which relaxes the leading term $\kappa_{\Imu}(\mathcal N)$ in Theorem \ref{thm: linear convergence of explicit scheme saddle}. The preconditioner $\IV$ is designed s.t. $\kappa_{\IV}(f)$ is small and $\IQ$ is for $\kappa_{\IQ}(g_S)$. Similar discussion on linear and non-linear solvers still holds. 
% is small. We may also include some information of $\nabla^2 g$ into $\IQ$ when $L_g$ is large to balance the two terms in \eqref{eq: scaling invariant iteration complexity}.
%\eqref{eq: scaling invariant iteration complexity} becomes
%\begin{equation*}
%    \begin{aligned}
%        &|\ln \epsilon_{\rm out} | \max\left \{ \frac{4\sqrt{2}}{3} \sqrt{\kappa_{\IV}(f)}\,  \kappa_{\IQ}(S),  \sqrt{ \left ( \frac{16L_{f,\IV}L_{g,\IQ}}{9\lambda_{\min}(\IQ^{-1}S)} + 2\right ) \kappa_{\IQ}(S)} \right \} \\
%        =&~|\ln \epsilon_{\rm out} | \max\left \{ \frac{4\sqrt{2}}{3} \sqrt{\kappa_{\IV}(f)}\,  \kappa_{\IQ}(S),  \sqrt{ \left ( \frac{8\kappa_{\IV}(f)\mu_{f,\IV}L_{g,\IQ}}{3} + 2\right ) \kappa_{\IQ}(S)} \right \} .
%    \end{aligned}
%\end{equation*}
%The dominated term in general is problem dependent. In particular if $L_{g,\IQ} \leq \sqrt{\frac{3\kappa_{\IQ}(S)}{8\mu_{f,\IV}}}$, the leading term 
% $$\mathcal O\left ( \sqrt{\kappa_{\IV}(f)}\,  \kappa_{\IQ}(S) |\ln \epsilon_{\rm out} |\right)$$
% gives the upper bound of the iteration complexity. Even for the strongly-convex-strongly concave saddle point problems, it is better to use accelerated gradient methods for the TPD flow as the leading term in Theorem \ref{thm: linear convergence of explicit scheme saddle}   and $\lambda_{\min}(\IQ^{-1} B \IV^{-1} B^{\intercal})$ is bounded below
%$\kappa_{\Imu}(\mathcal N) = \sqrt{\frac{L_S}{\mu_f \mu_g}}$ is relaxed to $\mathcal O(\sqrt{L_S/\mu_f})$. \mnote{But the choice of $\IQ$ needs to balance $\kappa_{\IQ}(S)$ and $\kappa_{\IQ}(g_S)$ at the same time?}
% There are cases when we further have $\IQ$ such that $$ 2/3-\mu_g \leq \lambda_{\min}( \IQ^{-1} S) \leq \lambda_{\max}(\IQ^{-1}S) \leq 4/3,$$
% for some computationally available $\IV$. For instance, $\IV$ is a scaled identity and $\IQ=S$ provided an efficient inner solver for $(BB^{\intercal})^{-1}$; or any approximated Schur complement that the spectral radius of $I - \IQ^{-1}S$ is less than $1/3$. The assumptions of the theorm are satisfied if $\IV$ (and $\IQ$) is scaled properly such that $L_f \leq 3/8$. The bound of step size is governed by $\alpha =  O(\sqrt{\mu_f}) = O(\sqrt{\kappa(f)})$, which implies the outer iteration complexity is $ O(\sqrt{\kappa_{\IV}(f)})$. Since $\IQ$ is spectral equivalent with $S = B\IV^{-1}B^{\intercal}$, the inner iteration complexity for solving $\IQ^{-1}$ is $ O(\sqrt{\kappa(S)}) =  O(\kappa_{\IV}(B))$ if for instance using conjugate gradient methods. When $\IV$ is a scaled identity, the total iteration complexity for the accelerated transformed primal-dual method is $\mathcal O(\sqrt{\kappa(f)}\kappa(B))$, which achieves the optimal complexity bound for affinely constrained problems~\cite{salim2022optimal}. Again the convexity of $f$ can be relaxed to $f_{\beta}$ if we consider the augmented Lagrangian.
\section{Conclusion}\label{sec: conclusion}
In this paper, we propose GSS methods and AGSS methods for solving a class of strongly monotone operator equation achieving accelerated linear convergence rates. The proof of strong Lyapunov property bridges the design of dynamical system on the continuous level, the choice of Lyapunov function and the linear convergence of the numerical schemes (as discretization of the continuous flow). As direct applications, we derive optimal algorithms for strongly-convex-strongly-concave saddle point systems with bilinear coupling. Combining the transformed primal-dual methods in our recent work~\cite{ChenWei2022Transformed} and augmented Lagrangian, accelerated linear convergence rates can be retained for general convex-concave saddle point problems.

As it is well-known in convex minimization, the optimal mixed type convergence rates is $\mathcal O(\min\{1/k^2, (1-1/\sqrt{\kappa(f)})^{-k})\}$~\cite{chen2021unified}, which enjoys a sublinear rate as long as $f$ is convex. With more careful design of parameters, our framework may achieve sublinear convergence rates for monotone operator equations. Then the iterative methods with mixed type convergence rate can be smoother for deriving nonlinear multigrid methods for solving saddle point problems, yielding iteration complexity free of problem size and condition number.
	
	\begin{acknowledgements}
The authors would like to thank Dr. Hao Luo for fruitful discussion.  
	\end{acknowledgements}
	%% For one-column wide figures use
	%\begin{figure}
	%% Use the relevant command to insert your figure file.
	%% For example, with the graphicx package use
	%  \includegraphics{example.eps}
	%% figure caption is below the figure
	%\caption{Please write your figure caption here}
	%\label{fig:1}       % Give a unique label
	%\end{figure}
	%%
	%% For two-column wide figures use
	%\begin{figure*}
	%% Use the relevant command to insert your figure file.
	%% For example, with the graphicx package use
	%  \includegraphics[width=0.75\textwidth]{example.eps}
	%% figure caption is below the figure
	%\caption{Please write your figure caption here}
	%\label{fig:2}       % Give a unique label
	%\end{figure*}
	%
	
	% For tables use
	%\begin{table}
	%% table caption is above the table
	%\caption{Please write your table caption here}
	%\label{tab:1}       % Give a unique label
	%% For LaTeX tables use
	%\begin{tabular}{lll}
	%\hline\noalign{\smallskip}
	%first & second & third  \\
	%\noalign{\smallskip}\hline\noalign{\smallskip}
	%number & number & number \\
	%number & number & number \\
	%\noalign{\smallskip}\hline
	%\end{tabular}
	%\end{table}
	
	
	%\begin{acknowledgements}
	%If you'd like to thank anyone, place your comments here
	%and remove the percent signs.
	%\end{acknowledgements}
	
	
	% Authors must disclose all relationships or interests that 
	% could have direct or potential influence or impart bias on 
	% the work: 
	%
	
	\section*{Funding}
	L. Chen and J. Wei are supported by National Science Fundation DMS-2012465.	
	
	\section*{Conflict of Interest}
	
The authors have no conflicts of interest to declare that are relevant to the content of this article.
	
%	\section*{Compliance with Ethical Standards}
	
	
	% BibTeX users please use one of
	% \bibliographystyle{spbasic}      % basic style, author-year citations
	\bibliographystyle{spmpsci}      % mathematics and physical sciences
	%\bibliographystyle{spphys}       % APS-like style for physics
	\bibliography{Optimization}   % name your BibTeX data base
	
	% Non-BibTeX users please use
	% \begin{thebibliography}{10}
		
	% 	\bibitem{browder1967construction}
	% 	Browder, Felix E and Petryshyn, Wolodymyr V.
	% 	\newblock Construction of fixed points of nonlinear mappings in {Hilbert} space.
	% 	\newblock {\em Journal of Mathematical Analysis and Applications}, 38(4):1102--1119, 2000.
		
	% 	\bibitem{alvarez_inertial_2001}
	% 	F.~Alvarez and H.~Attouch.
	% 	\newblock An inertial proximal method for maximal monotone operators via
	% 	discretization of a nonlinear oscillator with damping.
	% 	\newblock {\em Set-Valued Analysis}, 9(1):3--11, 2001.
		
	% 	\bibitem{apidopoulos_convergence_2018}
	% 	V.~Apidopoulos, J.-F. Aujol, and C.~Dossal.
	% 	\newblock Convergence rate of inertial {Forward}-{Backward} algorithm beyond
	% 	{Nesterov}'s rule.
	% 	\newblock {\em Mathematical Programming}, 2018.
		
	% 	\bibitem{attouch_heavy_2000}
	% 	H.~Attouch, X.~Goudou, and P.~Redont.
	% 	\newblock The heavy ball with friction method, {I}. {The} continuous dynamical
	% 	system: Global exploration of the local minima of a real-valued function by
	% 	asymptotic analysis of a dissipative dynamical system.
	% 	\newblock {\em Communications in Contemporary Mathematics}, 2(1):1--34, 2000.
		
	% 	\bibitem{attouch_variational_2014}
	% 	H.~Attouch, G.~Buttazzo, and G.~Michaille.
	% 	\newblock {\em Variational {Analysis} in {Sobolev} and {BV} {Spaces}}.
	% 	\newblock {MOS}-{SIAM} {Series} on {Optimization}. Society for Industrial and
	% 	Applied Mathematics, 2014.	
		
	% 	\bibitem{attouch_fast_2015}
	% 	H.~Attouch and Z.~Chbani.
	% 	\newblock Fast inertial dynamics and {FISTA} algorithms in convex optimization.
	% 	{Perturbation} aspects.
	% 	\newblock {\em arXiv:1507.01367}, 2015.
		
	% 	\bibitem{attouch_rate_2016}
	% 	H.~Attouch and J.~Peypouquet.
	% 	\newblock The rate of convergence of {Nesterov}'s accelerated forward-backward
	% 	method is actually faster than $1/k^2$.
	% 	\newblock {\em SIAM Journal on Optimization}, 26(3):1824--1834, 2016.
		
		
	% 	\bibitem{Attouch_2016-fast}
	% 	H.~Attouch, Z.~Chbani, J.~Peypouquet, and P.~Redont.
	% 	\newblock Fast convergence of inertial dynamics and algorithms with asymptotic
	% 	vanishing viscosity.
	% 	\newblock {\em Mathematical Programming}, 168(1-2):123--175, 2016.
		
	% 	\bibitem{attouch_convergence_2018}
	% 	H.~Attouch and A.~Cabot.
	% 	\newblock Convergence rates of inertial forward-backward algorithms.
	% 	\newblock {\em SIAM Journal on Optimization}, 28(1):849--874, 2018.
		
	% 	\bibitem{Attouch:2019}
	% 	H.~Attouch, Z.~Chbani, and H.~Riahi.
	% 	\newblock Rate of convergence of the {Nesterov} accelerated gradient method in
	% 	the subcritical case $\alpha\leqslant 3$.
	% 	\newblock {\em ESAIM: Control, Optimisation and Calculus of Variations}, 25(2), 2019.
		
	% 	\bibitem{Attouch_2018}
	% 	H.~Attouch and J.~Peypouquet.
	% 	\newblock Convergence of inertial dynamics and proximal algorithms governed by
	% 	maximally monotone operators.
	% 	\newblock {\em Mathematical Programming}, 174(1-2):391--432, 2018.
		
		
	% 	\bibitem{Ahmad2015}
	% 	S.~Ahmad and A.~Ambrosetti.
	% 	\newblock {\em A Textbook on Ordinary Differential Equations, 2nd}, volume~88
	% 	of {\em UNITEXT - La Matematica per il 3+2}.
	% 	\newblock Springer, Cham, 2015.
		
	% 	\bibitem{antipin_minimization_1994}
	% 	A.~S. Antipin.
	% 	\newblock Minimization of convex functions on convex sets by means of
	% 	differential equations.
	% 	\newblock {\em Differential Equations}, 30(9):1365--1375, 1994.
		
	% 	\bibitem{Beck2009}
	% 	A.~Beck and M.~Teboulle.
	% 	\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
	% 	problems.
	% 	\newblock {\em SIAM Journal on Imaging Sciences}, 2(1):183--202, 2009.
		
	% 	\bibitem{aujol_optimal_2017}
	% 	J.~Aujol and C.~Dossal.
	% 	\newblock Optimal rate of convergence of an {ODE} associated to the fast
	% 	gradient descent schemes for $b>0$.
	% 	\newblock {\em hal-01547251v2:22}, 2017.
		
	% 	\bibitem{balti_asymptotic_2016}
	% 	M.~Balti and R.~May.
	% 	\newblock Asymptotic for the perturbed heavy ball system with vanishing damping
	% 	term.
	% 	\newblock {\em Evolution Equations and Control Theory}, 6(2), 2016.
		
	% 	\bibitem{Brezis1973}
	% 	H.~Brezis.
	% 	\newblock {\em Ope\'erateurs maximaux monotones et semi-groupes de contractions
	% 		dans les espaces de Hilbert}, volume~4.
	% 	\newblock Elsevier, 1973.
		
	% 	\bibitem{cabot_long_2009}
	% 	A.~Cabot, H.~Engler, and S.~Gadat.
	% 	\newblock On the long time behavior of second order differential equations with
	% 	asymptotically small dissipation.
	% 	\newblock {\em Transactions of the American Mathematical Society},
	% 	361(11):5983--6017, 2009.
		
	% 	\bibitem{goudou_gradient_2009}
	% 	X.~Goudou and J.~Munier.
	% 	\newblock The gradient and heavy ball with friction dynamical systems: the
	% 	quasiconvex case.
	% 	\newblock {\em Mathematical Programming}, 116(1-2):173--191, 2009.
		
	% 	\bibitem{Kreuter2015}
	% 	M.~Kreuter.
	% 	\newblock {\em Sobolev Spaces of Vector-Valued Functions}.
	% 	\newblock Master Thesis, Ulm University, 2015.
		
	% 	\bibitem{nguyen_accelerated_2018}
	% 	N.~C. Nguyen, P.~Fernandez, R.~M. Freund, and J.~Peraire.
	% 	\newblock Accelerated residual methods for the iterative solution of systems of
	% 	equations.
	% 	\newblock {\em SIAM Journal on Scientific Computing}, 40(5):A3157--A3179, 2018.
		
	% 	\bibitem{Nesterov1983}
	% 	Y.~Nesterov.
	% 	\newblock A method of solving a convex programming problem with convergence
	% 	rate ${O}(1/k^2)$.
	% 	\newblock {\em Soviet Mathematics Doklady}, 27(2):372--376, 1983.
		
	% 	\bibitem{Nesterov_2012}
	% 	Y.~Nesterov.
	% 	\newblock Gradient methods for minimizing composite functions.
	% 	\newblock {\em Mathematical Programming}, 140(1):125--161, 2012.
		
	% 	\bibitem{Nesterov:2013Introductory}
	% 	Y.~Nesterov.
	% 	\newblock {\em Introductory Lectures on Convex Optimization: A Basic Course},
	% 	volume~87.
	% 	\newblock Springer Science \& Business Media, 2013.
		
	% 	\bibitem{Nesterov_2012}
	% 	Y.~Nesterov.
	% 	\newblock Gradient methods for minimizing composite functions.
	% 	\newblock {\em Mathematical Programming}, 140(1):125--161, 2012.
		
	% 	\bibitem{necoara_linear_2019}
	% 	I.~Necoara, Y.~Nesterov, and F.~Glineur.
	% 	\newblock Linear convergence of first order methods for non-strongly convex
	% 	optimization.
	% 	\newblock {\em Mathematical Programming}, 175(1):69--107, 2019.
		
	% 	\bibitem{PAOLI_2000}
	% 	L.~A. Paoli.
	% 	\newblock An existence result for vibrations with unilateral constraints.
	% 	\newblock {\em Mathematical Models and Methods in Applied Sciences},
	% 	10(6):815--831, 2000.
		
	% 	\bibitem{odonoghue_adaptive_2015}
	% 	B.~O'Donoghue and E.~Cand\`es.
	% 	\newblock Adaptive restart for accelerated gradient schemes.
	% 	\newblock {\em Foundations of Computational Mathematics}, 15(3):715--732,
	% 	2015.
		
	% 	\bibitem{Parikh:2014}
	% 	N.~Parikh and S.~Boyd.
	% 	\newblock Proximal algorithms.
	% 	\newblock {\em Foundations and Trends in Optimization}, 1(3):127--239, 2014.
		
	% 	\bibitem{polyak_methods_1964}
	% 	B.~Polyak.
	% 	\newblock Some methods of speeding up the convergence of iteration methods.
	% 	\newblock {\em USSR Computational Mathematics and Mathematical Physics},
	% 	4(5):1--17,  1964.
		
	% 	\bibitem{rockafellar_convex_1970}
	% 	R.~Rockafellar.
	% 	\newblock {\em Convex Analysis}.
	% 	\newblock Princeton University Press, 1970.
		
	% 	\bibitem{Shi:2018}
	% 	B.~Shi, S.~S. Du, M.~I. Jordan, and W.~J. Su.
	% 	\newblock Understanding the acceleration phenomenon via high-resolution
	% 	differential equations.
	% 	\newblock {\em arXiv:1810.08907}, 2018.
		
	% 	\bibitem{Siegel:2019}
	% 	J.~W. Siegel.
	% 	\newblock Accelerated first-order methods: Differential equations and
	% 	{L}yapunov functions.
	% 	\newblock {\em arXiv: 1903.05671}, 2019.
		
	% 	\bibitem{Su;Boyd;Candes:2016differential}
	% 	W.~Su, S.~Boyd, and E.~J. Candes.
	% 	\newblock A differential equation for modeling Nesterov's accelerated
	% 	gradient method: Theory and insights.
	% 	\newblock {\em Journal of Machine Learning Research}, 17(153):1--43, 2016.
		
	% 	\bibitem{Vassilis2018}
	% 	A.~Vassilis, A.~Jean-Fran{\c c}ois, and D.~Charles.
	% 	\newblock The differential inclusion modeling {FISTA} algorithm and optimality
	% 	of convergence rate in the case $b\leqslant3$.
	% 	\newblock {\em SIAM Journal on Optimization}, 28(1):551--574, 2018.
		
	% 	\bibitem{Wibisono;Wilson;Jordan:2016variational}
	% 	A.~Wibisono, A.~C. Wilson, and M.~I. Jordan.
	% 	\newblock A variational perspective on accelerated methods in optimization.
	% 	\newblock {\em Proceedings of the National Academy of Sciences},
	% 	113(47):E7351--E7358, 2016.
		
	% 	\bibitem{Wilson:2018}
	% 	A.~C. Wilson, B.~Recht, and M.~I. Jordan.
	% 	\newblock A Lyapunov analysis of momentum methods in optimization.
	% 	\newblock {\em arXiv: 1611.02635}, 2016.
		
	% 	\bibitem{zhang_acceleration_2019}
	% 	J.~Zhang, S.~Sra, and A.~Jadbabaie.
	% 	\newblock Acceleration in first order quasi-strongly convex optimization by ode discretization.
	% 	\newblock {\em arXiv:1905.12436}, 2019.
	% \end{thebibliography}
	
	%\begin{thebibliography}{}
	%%
	%% and use \bibitem to create references. Consult the Instructions
	%% for authors for reference list style.
	%%
	%\bibitem{RefJ}
	%% Format for Journal Reference
	%Author, Article title, Journal, Volume, page numbers (year)
	%% Format for books
	%\bibitem{RefB}
	%Author, Book title, page numbers. Publisher, place (year)
	%% etc
	%\end{thebibliography}
	
\end{document}
% end of file template.tex

