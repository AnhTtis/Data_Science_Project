\section{Related Work}
\label{sec:related}

\noindent \textbf{Graphic Layout Generation.}
Early work on graphic layout generation has explored classical optimization approaches~\cite{o2014learning,o2015designscape}, as well as generative models such as Generative Adversarial Networks (GANs)~\cite{layoutgan,layoutganpp} and Variational Autoencoders (VAEs)~\cite{layoutvae,canvasvae,vtn,c2f}. 

Recently, inspired by the success of NLP, masking strategies~\cite{blt}, language models~\cite{layouttransformer}, and encoder-decoder architectures~\cite{unilayout} have been studied. 
These approaches represent the layout as a sequence of elements and use Transformer~\cite{vaswani2017attention} as the basic model architecture.
As the placement of one element can depend on any part of a layout, one critical issue in layout generation is \emph{global context modeling}. 
Some previous studies introduce unnatural biases and fail to model global context effectively~\cite{layouttransformer,vtn,unilayout,c2f,canvasvae}. 
They generate the layout sequence in an autoregressive manner, where there is a predefined generation order and the placement of one element can only depend on the generated part of the layout. 
On the other hand, a few other studies consider global context but do not achieve significantly better performance~\cite{layoutgan,layoutganpp,blt}. 
They generate the layout sequence in a non-autoregressive manner, where there is no predefined generation order, and all the tokens are generated in parallel. However, generating a sequence in a single pass is too challenging~\cite{maskpredict}. 
BLT~\cite{blt} explored an iterative refinement mechanism to alleviate the difficulty. 
However, it relies on heuristic rules instead of being learned from the data.
The above limitation motivates us to seek a better model for layout generation. 
We think diffusion models are well-suited.
By multiple rounds of denoising, it naturally takes the layout in the last step as the global context and generates a layout iteratively instead of by a single pass.

Another branch of studies has explored incorporating diverse user constraints into the layout generation~\cite{ndn,rahman2021ruite,zheng2019content,zhou2022composition,li2020attribute}. 
They treat layout generation tasks with different constraints separately, which introduces repetitive training and hinders knowledge sharing across different tasks. 
By utilizing the flexible forward process of diffusion models~\cite{cold_diffusion}, we enable some conditional generation tasks without re-training for the first time, which can be potentially extended to handle more conditional generation tasks.



\noindent \textbf{Diffusion Models for Discreate Data.}
Diffusion models on continuous data have achieved outstanding results~\cite{survey}.
Recently, diffusion models on discrete data are also emerging.
They can be grouped into two categories.
The first category~\cite{diffusionlm,analog,diffuseq} maps discrete data to continuous state space via a learnable or fixed embedding, and then utilizes techniques from classical continuous diffusion models. 
These approaches enable simple technology migration from continuous diffusion models, but make the fine-grained control of the forward corruption process much difficult.
Another category~\cite{argmax,d3pm,vqdiffusion,vqpose,improvedVqpose,tauldr} chooses to directly perform diffusion in discrete state space by modeling the forward corruption process as a random walk between different states. 
This category makes it easy to incorporate domain-dependent structure to the transition matrices and thus enables flexible control of the forward process.
Different to the discrete data (\eg, images and texts) explored by previous work, the layout data studied in this work is heterogeneous by nature.
Thus, we fully consider such characteristic and propose a new transition matrix coupled with noise schedules to achieve a mild corruption process.
