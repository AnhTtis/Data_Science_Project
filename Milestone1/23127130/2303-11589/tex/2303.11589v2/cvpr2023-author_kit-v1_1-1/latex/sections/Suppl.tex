
\begin{spacing}{1}
\tableofcontents
\end{spacing}

\newpage


\section{Additional Implementation Details}

\subsection{More Implementation Details for LayoutDiffusion}

\noindent \textbf{Noise Schedule.} We investigate the effectiveness of our proposed schedule $\beta_t=g/(T-t+\epsilon)^h$ for the discretized Gaussian transition matrix by comparing with the original linear schedule $\beta_t=bt/T$ used in~\cite{d3pm}. 
Notably, here $\beta_t$ is not a variance term bounded in $[0,1]$\footnote{In fact, as $\beta_t$ tends to positive infinity, $\mathbf{Q}_t^{coord}$ will approach a transtion matrix for uniform noise as described by Sohl-Dickstein \etal~\cite{2015}.}, and with the growth of forward steps, the cumulative matrix $\overline{\mathbf{Q}}_t^{coord}$ converges to uniform distribution.
Thus, we attempt to analyse the noise process by observing the standard derivation of the cumulative matrix.
A higher std. indicates a more sparse matrix and hence a lower transition probability to other coordinate tokens. 
As shown in~\cref{fig:noise schedule}, 
our schedule presents a gentler noising process and a more stable convergence state compared to the original linear schedule (as suggested by a higher std. at the beginning of the forward process, and a lower std. at the very end of the process).

\vspace{-11px}
\begin{figure}[th]
\centering

   \includegraphics[width=0.35\linewidth]{cvpr2023-author_kit-v1_1-1/latex/suppl_pics/noise.pdf}%
\vspace{-5px}
   \caption{Standard derivation of the cumulative discretized Gaussian matrix $\overline{\mathbf{Q}}_t^{coord}$ of our proposed $\beta_t$ and the original linear schedule. }
\vspace{-5px}%
   \label{fig:noise schedule} 
\end{figure}



\noindent \textbf{Denoising Model.}
We present the model architecture as in~\cref{model arch}. We embed the input sequence, input timestep, and positions with 768, 128, and 768 dimensions respectively. The dropout rate is set as 0.1. For the transformer encoder, we simply adopt the same hyperparameters of BERT-base~\cite{bert} encoder, i.e., 12 layers, 12 attention heads, 768 hidden size, and 3,072 dimensions for the feed forward layer.

\begin{figure}[th]
  \centering
   \includegraphics[width=1\linewidth]{cvpr2023-author_kit-v1_1-1/latex/suppl_pics/model.pdf}
   \vspace{-10px}
\caption{Model architecture of $p_{\theta}(x_0|x_t)$.}
\vspace{-5px}
   \label{model arch} 
\end{figure}





\noindent \textbf{Hyperparameters.} We train our model using AdamW optimizer~\cite{adam} with $lr=0.00004$, $betas=(0.9,0.999)$, and zero $weight\_decay$. We also apply an exponential moving average (EMA) over model parameters with a rate of $0.9999$.
We set the batch size as 64. For RICO~\cite{rico}, we train the model with 2 V100 GPUs for 175,000 steps to achieve the best results; and for PublayNet~\cite{publaynet}, we train the model with 4 V100 GPUs for 350,000 steps.
For the schedule of type tokens, we set $\Tilde{T}=160$ in~\cref{late absorb}. For the schedule of coordinate tokens, we set $T=\Tilde{T}=160$, $g=12.4$, $h=2.48$, and $\epsilon=0.0001$ in $\beta_t=g/(T-t+\epsilon)^h$. We also provide the hyperparameters for the implementation of different timesteps apart from 200 steps (we implement these variants in the \cref{tbl:speed} of the main paper), as shown in~\cref{tbl:schedule}.


\begin{table}[h]
\centering
\setlength{\tabcolsep}{3mm}{
  \begin{tabular}{lcc}
    \toprule
    Total timesteps &  $\Tilde{T}$ for type schedule &   $\beta_t$ for coordinate schedule \\
    \midrule
    100 &80 & $20.0/(80-t+0.0001)^{2.96}$   \\
    200 &160 & $12.4/(160-t+0.0001)^{2.48}$   \\
    500 &400 & $6.2/(400-t+0.0001)^{2.00}$  \\ 
    1000 & 800 & $3.5/(800-t+0.0001)^{1.76}$   \\
    2000 & 1600 & $2.0/(1600-t+0.0001)^{1.52}$   \\ 
    \bottomrule
  \end{tabular}}
\caption{Hyperparameters for variants of different timesteps.}
\vspace{-5px}
\label{tbl:schedule}
\end{table}


\subsection{Implementation of other Diffusion Baselines}
\noindent \textbf{Diffusion-LM~\cite{diffusionlm}.} We implement Diffusion-LM based on the official repository\footnote{\url{https://github.com/XiangLi1999/Diffusion-LM}}. We realize the layout generation task via Diffusion-LM by feeding the layout as a sequence and reconverting the output sequence to a layout. For the hyperparameters, we simply adopt the default setting that Diffusion-LM adopts on E2E~\cite{e2e} dataset, since only relatively small vocabulary size ($\sim 150$) is required for the tokens that represent layout sequence.

For conditional generation tasks, we apply the similar idea as in LayoutDiffusion. Specifically, for Gen-Type, we fix the type tokens by feeding the target in each timestep and run the whole reverse denoising process. For refinement task, we embed the layout sequence and set the embedded latent as the input of start timestep $T_{\text{refine}}$, and then run the remaining reverse process with type fixed.
For the choice of $T_{\text{refine}}$, we traverse through [250,500,750,1000,1250,1500] of the total 2000 steps, and find the best result is achieved when $T_{\text{refine}}=1000$.








\vspace{5px}
\noindent \textbf{D3PM uniform~\cite{d3pm}.} We implement D3PMs based on both the official repository of D3PMs\footnote{\url{https://github.com/google-research/google-research/tree/master/d3pm}} and the official repository of another method concerning discrete diffusion model, i.e., VQ-Diffusion\footnote{\url{https://github.com/microsoft/VQ-Diffusion}}, since our implementation is based on PyTorch~\cite{paszke2019pytorch} rather than JAX~\cite{jax2018github}. We realize the layout generation task in a similar way as LayoutDiffusion, i.e., feeding the layout as a sequence of tokens and treating each token as a discrete state.

For the hyperparameters of the diffusion framework, we set the total diffusion timesteps $T=1000$, the schedule $\beta_t=(T-t+1)^{-1}$, and the auxiliary loss weight $\lambda=0.0001$, all follow the setting as reported in D3PMs paper. For the denoising model, we apply the similar model as in LayoutDiffusion and Diffusion-LM for a fair comparison.

For conditional generation tasks, we implement the Gen-Type and refinement the same way as in our implementation of Diffusion-LM, since both methods are replace-based diffusion methods. For the start timestep for refinement task $T_{\text{refine}}$, we sweep from [200,400,600,800] of the total 1000 steps, and find $T_{\text{refine}}=400$ is the optimal choice.


\vspace{5px}
\noindent \textbf{D3PM absrobing~\cite{d3pm}.} We implement the D3PM (absorbing) the same way as in D3PM (uniform). All settings except for the model are also referenced from the original D3PMs paper.

One major difference lies in the implementation of conditional generation tasks. 
For the Gen-Type task, we apply the similar idea as in LayoutDiffusion. To be more specific, we feed the given type set at the beginning step $T$, and run the whole reverse process. 
It is noteworthy that, for D3PM (absorbing), all coordinates start to recover strictly from timestep $T$.
Hence, it cannot save steps as the same strategy in LayoutDiffusion by picking a timestep $T_{\text{Gen-Type}}$ which is smaller than $T$. 
Besides, in the reverse process of D3PM (absorbing), the sampled tokens cannot transition into \texttt{MASK} or other tokens, 
thus, it is unable to perform the refinement task as in LayoutDiffusion. 




\subsection{Settings on Conditional Generation Tasks}
\label{sec:details cond tasks}
We present below the settings on conditional generation tasks in the main paper.
For further experiments on conditional generation tasks, please refer to~\cref{more cond}.
\vspace{2.5px}

\noindent \textbf{Gen-Type.}
We follow the convention in \cite{blt,unilayout}.
To be more specific, for a layout in the test set, we extract its type set as the input and let the model generate the bounding box attributes of each element .


\vspace{2.5px}
\noindent \textbf{Refinement.} 
In the real scenario, the noise level of the user-given flawed layout cannot be known in advance.
Besides, different flawed layouts may have different noise levels.
To simulate the real scenario, we improve the setting used in RUITE~\cite{rahman2021ruite}.
Specifically, the setting in RUITE is to construct a test set by adding random noises to the position and size of each element, where the noise is sampled from a normal distribution with mean $0$ and the standard variance $0.01$.
In our improved setting, we modify the standard variance of the noise to be uniformly sampled from $[0.005,0.01,0.015,0.02,0.025]$. 
Besides, for the baselines, we train the model with input noise of 0.01 standard variance; for LayoutDiffusion, we apply the same inference steps $T_{\text{refine}}$ for different levels of noise, unlike the settings in~\cref{sec:suppl_refine}.

\subsection{Details about Classification Model for FID Evaluation}
We use the same layout classification model as LayoutGan++~\cite{layoutganpp} and LayoutFormer++~\cite{unilayout}. Specifically, the model includes an encoder and a decoder, both of which are based on the Transformer architecture.
The encoder takes in bounding box coordinates and corresponding labels and produces a feature representation, while the decoder uses the feature representation to predict the class probabilities and bounding box coordinates for each layout element.
We implement the model based on the official repository of LayoutGan++\footnote{\url{https://github.com/ktrk115/const_layout}} and train it using the methods described in their paper.
Our evaluation results using this model are consistent with those reported in the LayoutFormer++.

\section{Discussion on Diversity} %
\label{sec:selfsim}
\subsection{Metric SelfSim}
Diversity is a key but often overlooked aspect in layout generation tasks. In this section, we propose a new metric called \emph{SelfSim} to measure the self-similarity of generated layouts, which serves as an indicator of diversity. The intuition behind this metric is that more diverse generated layouts should be less self-similar. Specifically, we calculate the average Intersection over Union (IoU) between any pairs of generated layouts with the same set of element types.

\subsection{Algorithm of SelfSim}
Inspired by the metrics for evaluating diversity in NLP (\eg, diverse 4-gram~\cite{div4} and self-BLEU~\cite{selfB}), we propose to assess the diversity of generated layouts by measuring the self-similarity of the generated layout set. Specifically, we partition the generated layouts into different subsets based on their type sets and then count the similarity of the layouts within each subset. The similarity is calculated by averaging the intersection of union (IoU) of the bounding boxes for each pair of layouts in the subset.
We present the algorithm for calculating SelfSim as in \cref{algo_selfsim}.

\IncMargin{1em}
\begin{algorithm}[h]
\label{algo_selfsim}
	\caption{Calculation of the Self-Similarity score}
    \vspace{1px}
	\KwIn{A set of graphic layouts $\mathbb{X}=\{\mathbf{x}_1,\mathbf{x}_2,\dots,\mathbf{x}_m\}$} 
	\vspace{1px}
	\KwOut{The Self-Similarity score of the given layout set}
	 \BlankLine 
	 
	 \emph{partition $\mathbb{X}$ by the layouts' \textbf{type set}, denote the partition as $\mathbb{P}=\{\mathbb{X}_1,\mathbb{X}_2,\dots,\mathbb{X}_n\}$}\; 
	 \For(\tcc*[f]{traverse each subset $\mathbb{X}_i \in \mathbb{P}$}){$i\leftarrow 1$ \KwTo $n$}{ 
	 	\emph{count the number of elements in subset $\mathbb{X}_i$, denoted as $l_i$}\; 
	 	\If(\tcc*[f]{only one layout in the subset}){$l_i=1$}{set the Self-Similarity score of subset $\mathbb{X}_i$ as $S_i=0$\;}
	 	\Else(\tcc*[f]{more than one layout has this \textbf{type set}}){
	 	\For{each pair $(x_j^i,x_k^i) (j \neq k)$ in $\mathbb{X}_i$}{ 
	 	calculate the IoU of bounding boxes between $x_j^i$ and $x_k^i$, denoted as $U_{jk}^i$\; }
average the $U_{jk}^i$ of the total ${l_i \choose 2}$ pairs to get the mean $S_i$\;}
 	 }
 	 \KwRet{the weighted average of all subsets' Self-Similarity score $\dfrac{\sum_{i=1}^n l_i S_i}{\sum_{i=1}^n l_i}$\;}

 	 \end{algorithm}
 \DecMargin{1em} 
 \vspace{-5px}

 
\begin{figure}[t]
  \centering
  \begin{subfigure}{0.28\linewidth}
    \includegraphics[width=1\linewidth]{cvpr2023-author_kit-v1_1-1/latex/suppl_pics/0.png}
    \caption{$S_i=0.0$}
    \label{fig:00}
  \end{subfigure}
  \hspace{10px}
  \begin{subfigure}{0.28\linewidth}
\includegraphics[width=1\linewidth]{cvpr2023-author_kit-v1_1-1/latex/suppl_pics/2.png}
    \caption{$S_i=0.2$}
    \label{fig:02}
  \end{subfigure}
  \hspace{7.5px}
    \begin{subfigure}{0.38\linewidth}
    \includegraphics[width=1\linewidth]{cvpr2023-author_kit-v1_1-1/latex/suppl_pics/4.png}
    \caption{$S_i=0.4$}
    \label{fig:04}
  \end{subfigure}
    \vspace{5px}
    
  \begin{subfigure}{0.29\linewidth}
    \includegraphics[width=1\linewidth]{cvpr2023-author_kit-v1_1-1/latex/suppl_pics/6.png}
    \caption{$S_i=0.6$}
    \label{fig:06}
  \end{subfigure}
  \hspace{10px}
  \begin{subfigure}{0.29\linewidth}
\includegraphics[width=1\linewidth]{cvpr2023-author_kit-v1_1-1/latex/suppl_pics/8.png}
    \caption{$S_i=0.8$}
    \label{fig:08}
  \end{subfigure}
  \hspace{10px}
    \begin{subfigure}{0.195\linewidth}
\includegraphics[width=1\linewidth]{cvpr2023-author_kit-v1_1-1/latex/suppl_pics/10.png}
    \caption{$S_i=1.0$}
    \label{fig:10}
  \end{subfigure}
  \caption{Examples of subsets with different SelfSim. As SelfSim goes from 0 to 1, the layouts in the subset go from totally different to completely identical.}
  \vspace{-5px}
  \label{fig:selfsim}
\end{figure}

\subsection{Case Study of SelfSim}
To visually demonstrate the effectiveness of SelfSim, we show some subsets with different SelfSims (i.e., subsets with different $S_i$) in~\cref{fig:selfsim}. We observe that subsets with higher SelfSims tend to have more similar layouts, while those with lower SelfSims have more diverse layouts. This further supports the effectiveness of the SelfSim metric for assessing the diversity of generated layouts.



\subsection{SelfSim Comparison with Existing Methods}
\cref{tbl:self-sim} compares LayoutDiffusion with existing layout methods and the diffusion-based method using SelfSim. While LayoutFormer++, Diffusion-LM, and LayoutTransformer have advantages in certain aspects of quality (as shown in \cref{Tab:quantitative_results} in the main paper), they suffer from obvious diversity issues, which aligns with our user study findings (as shown in \cref{user_study}). On the other hand, although D3PM performs slightly better in diversity on the PubLayNet dataset, it lags behind in terms of quality (see \cref{Tab:quantitative_results}). These results suggest that our proposed method achieves a better quality-diversity trade-off.

\begin{table}[t]
\centering
\renewcommand{\arraystretch}{0.9}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lccccccc}
\toprule

SelfSim$\downarrow$ & & LayoutTransformer & LayoutFormer++ & Diffusion-LM & D3PM (absorbing) & D3PM (uniform) & LayoutDiffusion\\
\midrule
RICO & & 0.318 & \textit{0.581} & \textit{0.326} & 0.157 & 0.165 & 0.157 \\
PublayNet & & \textit{0.314} & \textit{0.328} & 0.222 & 0.194 & 0.189 & 0.198 \\
\bottomrule
\end{tabular}
\vspace{-5px}
\caption{Comparison of SelfSim scores for unconditional generation on RICO and PublayNet datasets. Lower SelfSim scores indicate better diversity. \textit{Italic font} denotes the two worst-performing methods.}
\label{tbl:self-sim}
\end{table}


\section{Additional Experiments on Conditional Generation}
\label{more cond}
In this section, we design two sets of experiments to investigate LayoutDiffusion's robustness in the refinement task and its diversity performance in the generation conditioned on type (Gen-Type) task.

\subsection{Refinement}
\label{sec:suppl_refine}

\input{cvpr2023-author_kit-v1_1-1/latex/suppl_sections/refine_table}

As introduced in~\cref{sec:details cond tasks}, in the main paper, our experiments for the refinement task apply a mixture of different levels of noise as input. 
We suppose that the excellent results achieved by LayoutDiffusion are due to its capability of handling various levels of noise.
To investigate the model's robustness to the noise, in this section, we compare with the two strongest baselines and further study the performance of the methods under each specific noise levels.

Specifically, we evaluate the performance of different methods under the conditions that the standard deviation of the noise is 0.005, 0.01, and 0.02, respectively. Plus, for a fair comparison, we train the baselines with the input noise of 0.01 standard deviation, and apply the same model for inference.



\noindent \textbf{Quantitative results.} As shown in~\cref{Tab:refine_table}, for the two baselines (RUITE~\cite{rahman2021ruite} and LayoutFormer++~\cite{unilayout}), the models exhibit favorable performances when dealing with noise levels less than or equal to the training level (std.=0.005 and std.=0.01).
However, when the testing noise level is greater than the training's (std.=0.02), the models suffer a significant performance drop.
For LayoutDiffusion, it not only surpasses the baseline in all 12 competitions (3 levels $\times$ 4 metrics), but also consistently presents excellent performance as the noise level varies, indicating that LayoutDiffusion is highly robust to noise levels.


\begin{figure}[th]
  \centering

   \includegraphics[width=1.01\linewidth]{cvpr2023-author_kit-v1_1-1/latex/suppl_pics/suppl_refine.pdf}

   \caption{Qualitative comparison under different noise levels on RICO. Each row shares the same noise levels while each column shares the same method. For more quantitative result of LayoutDiffusion on Refinement, please refer to~\cref{more refine}.}
   \label{suppl refine} 
   \vspace{-5px}
\end{figure}
\noindent \textbf{Qualitative results.} We provide the quantitative results in~\cref{suppl refine}. We can conclude that as the input gets more chaotic, LayoutDiffusion consistently produces pleasing layouts, while other baselines fail to achieve so, which is in line with the quantitative results.


\subsection{Generation Conditioned on Type}
As described in~\cref{sec:details cond tasks}, in the main paper, our experiments for the Gen-Type task only generate one sample for each input type set.
In this section, we study whether they can generate multiple diverse layouts for a given type set to further explore the diversity performance of each method under Gen-Type task.

Specifically, we first find all the different type sets in the layouts of the test set, and then equally generate 5 layouts for each given type set\footnote{In practice, we find 2714 different type sets out of 3729 layouts in the test set of RICO, and 1339 type sets out of 10998 layouts in PublayNet's test set.}. 
We compare to the baseline with the best quality performance (i.e., LayoutFormer++~\cite{unilayout}), which can generate multiple layouts with top-k sampling~\cite{fan-etal-2018-hierarchical}. LayoutDiffusion can generate multiple layouts by simply running the inference process multiple times.
We apply the metric SelfSim (as discussed in~\cref{sec:selfsim}) for the evaluation of diversity.

\input{cvpr2023-author_kit-v1_1-1/latex/suppl_sections/type_table}

\noindent \textbf{Quantitative results.} The quantitative results is given in~\cref{Tab:type_table}. Compared to LayoutFormer++, LayoutDiffusion performs significantly better in diversity (as suggested by SelfSim), while achieving comparable quality performance (as suggested by Overlap and Align.).
We hypothesize that the gap between diversity is due to the probability accumulation of the autoregressive model while LayoutDiffusion samples each layout from independent noise.

\noindent \textbf{Qualitative results.} As show in~\cref{type suppl}, despite equipped with top-k sampling, LayoutFormer++ still suffers severe diversity problem (duplication occurs in the first three row and the last row. Besides, the generated layouts in the fourth row share similar patterns). 
While for LayoutDiffusion, all the 5 samples of each type set are both pleasing and in great diversity, further demonstrating the superiority of LayoutDiffusion on Gen-Type task.



\begin{figure}[th]
  \centering
  \vspace{-10px}
   \includegraphics[width=0.95\linewidth]{cvpr2023-author_kit-v1_1-1/latex/suppl_pics/type_suppl.pdf}

   \caption{Qualitative comparison of diversity on RICO. Each type set corresponds to five samples from LayoutFormer++ (left) and five samples from LayoutDiffusion (right). For more qualitative results of LayoutDiffusion on Gen-Type, please refer to~\cref{more gen-type}.}
   \label{type suppl} 
\end{figure}


\section{Additional Ablation Studies}
\label{more ablation}
\subsection{Additional Ablation Studies on Conditional Generation Tasks}
In addition to the ablation studies on unconditional layout generation discussed in the main paper (see \cref{ablation}), we also conducted ablation studies with the variations on two conditional generation tasks, i.e., Gen-Type and Refinement, to further investigate the effectiveness of our design.
The quantitative comparison is shown in \cref{Tab:ablation_condition}. 

LayoutDiffusion consistently outperforms all the variations on almost all metrics in both tasks. This result indicates that our design is superior and the reverse generation process is well-suited to both tasks, allowing the model to better leverage the given conditions.
Notably, the comparison between LayoutDiffusion and Uniform $\mathbf{Q}_t^{\text{type}}$ as well as Linear $\overline{\gamma}_t$ highlights the importance of our handling of the type tokens, which considers type corruption factor. This factor leads to better utilization of type information in the Gen-Type task.
Moreover, the comparison between LayoutDiffusion and the two variations of $\mathbf{Q}_t^{\text{coord}}$ as well as Linear $\beta_t$ in the Refinement task demonstrates the importance of our design for the coordinate tokens, which helps us model the precise details of the layout and achieve better performance in the Refinement task.

\input{cvpr2023-author_kit-v1_1-1/latex/suppl_sections/ablation_cond}


\subsection{Additional Ablation Studies on Noise Schedule of Type Tokens}
In this section, we further investigate the effectiveness of our type schedule by experimenting other different $\overline{\gamma}_t$ schedules.

Recall that in the main paper, we follow the insight that type changes in the early stage may bring large semantic shift to the layout, thus, we set the noise schedule for $\overline{\gamma}_t$ as:
\begin{equation}
    \overline{\gamma}_t = \begin{cases}
    0, &t< \Tilde{T} \\
(t-\Tilde{T})/(T-\Tilde{T}), &t \ge \Tilde{T} 
\label{late absorb}
\end{cases}
\end{equation}

We denote this kind of schedule as ``late absorb $\Tilde{T}$", since under this schedule, all type tokens stay unchanged until timestep $\Tilde{T}$ when they start to absorb, and at the terminal step $T$, all type tokens reach the absorbed state. Follow this idea, we can come to a similar noise schedule, ``early absorb $\Tilde{T}'$", where the type tokens start to absorb at the beginning and fully adsorbed in the early stage, and it can be defined as follows:

\begin{equation}
    \overline{\gamma}_t = \begin{cases}
    t/\Tilde{T}', &t< \Tilde{T}' \\
1, &t \ge \Tilde{T}' \\
\end{cases}
\end{equation}

Note that, when $\Tilde{T}'=T$ and $\Tilde{T}=0$, two schedules becomes the same and is experimented in the ablation studies of the main paper (denoted as ``linear $\overline{\gamma}_t$"). Here, we provide a detailed experiment on $\overline{\gamma}_t$, including different choices of ``late absorb $\Tilde{T}$" and ``early absorb $\Tilde{T}'$".



\input{cvpr2023-author_kit-v1_1-1/latex/suppl_sections/ablation_table}

As shown in the first group of~\cref{Tab:ablation_table}, we can conclude that as the type starts absorb later (from top to bottom in the table), the overall generation performance becomes better. Specifically, with $\Tilde{T}$ for the late absorb decreases, the quality of the generated layouts gets worse (as suggested by mIoU, Align., and FID). When it comes to early absorb, the performance drops as $\Tilde{T}'$ decreases.
The experiment empirically supports the insight we discuss above.


\subsection{Ablation Studies on Sequence Ordering}
In the main paper, we sort the layout sequence according to the alphabetical order of the elements' type in the layout (denoted as ``lexico"). Other choices are the positional ordering of the elements' bounding boxes (denoted as ``position") or simply randomly sorting the elements (denoted as ``random"). 
Besides, for each element in the layout, we represent its bounding box by the left, top, right, bottom coordinates (denoted as ``ltrb'').
One can also represent the bounding box using an element's left coordinate, top coordinate, width and height (denoted as ``ltwh'').
Here, we provide the results of all these options for sequence ordering.

As shown in the second group of~\cref{Tab:ablation_table}, for the format representing the coordinates, the ltrb group exhibits better overall performance than the ltwh group. One explanation is that ltrb format may provide more straightforward information for the precise alignment of the bounding boxes. 
For the ordering of elements, the alphabetical ordering and positional ordering are slightly better than the random ordering. We hypothesize that the model can exploit the additional ordering information with positional embedding. 
However, note that for conditional generation, the positional ordering of the elements is unknown, so to enable both unconditional and conditional generation, we apply the alphabetical ordering to sort the elements.





\section{Qualitative Results of LayoutDiffusion}
\label{sec:suppl_quali}
In this section, we provide more generated samples covering three unconditional and conditional tasks on two datasets. 
\subsection{Unconditional Generation}
\begin{figure}[H]
  \centering
   \includegraphics[width=0.9\linewidth]{cvpr2023-author_kit-v1_1-1/latex/suppl_pics/ungen_rico.png}
   \caption{Examples of unconditional generation on RICO dataset.}
   \label{ungen_rico}
\end{figure}
\begin{figure}[H]
  \centering
   \includegraphics[width=0.9\linewidth]{cvpr2023-author_kit-v1_1-1/latex/suppl_pics/pub_ungen.png}
   \caption{Examples of unconditional generation on PublayNet dataset.}
   \label{ungen_pub}
\end{figure}


\subsection{Refinement}

\label{more refine}
\begin{figure}[H]
  \centering
   \includegraphics[width=0.925\linewidth]{cvpr2023-author_kit-v1_1-1/latex/suppl_pics/rico_refine.pdf}
\caption{Examples of refinement on RICO. The left side of each pair is the input layout while the right side is the generated layout.}
   \label{refine rico} 
\end{figure}

\begin{figure}[H]
  \centering
   \includegraphics[width=0.925\linewidth]{cvpr2023-author_kit-v1_1-1/latex/suppl_pics/pub_refine.pdf}
\caption{Examples of refinement on PublayNet. The left side of each pair is the input layout while the right side is the generated layout.}
   \label{refine pub}
\end{figure}


\subsection{Generation Conditioned on Type}
\label{more gen-type}
\vspace{-10px}
\begin{figure}[H]
  \centering
   \includegraphics[width=0.9\linewidth]{cvpr2023-author_kit-v1_1-1/latex/suppl_pics/type_rico.pdf}
\caption{Examples of Gen-Type on RICO. Each given type set corresponds to four generated layouts.}
   \label{type rico}
\vspace{-5px}
\end{figure}

\begin{figure}[H]
  \centering
   \includegraphics[width=0.93\linewidth]{cvpr2023-author_kit-v1_1-1/latex/suppl_pics/type_pub.pdf}
   \vspace{-5px}
\caption{Examples of Gen-Type on PublayNet. Each given type set corresponds to four generated layouts.}
   \label{type pub} 
\end{figure}

\section{Fine-Grained Visualization of the Forward Diffusion Process}
\begin{figure}[H]
  \centering
   \includegraphics[width=0.925\linewidth]{cvpr2023-author_kit-v1_1-1/latex/suppl_pics/noise_steps.jpg}
   \caption{An example of the forward process of LayoutDiffusion on RICO. We sample 99 steps uniformly from the total 200 timesteps.}
   \label{noise steps} 
\end{figure}

\begin{figure}[H]
  \centering
   \includegraphics[width=1.01\linewidth]{cvpr2023-author_kit-v1_1-1/latex/suppl_pics/forward_suppl.pdf}
   \caption{Examples of the forward process of LayoutDiffusion on RICO. We sample 22 steps uniformly from the total 200 timesteps.}
   \label{noise steps more} 
\end{figure}

\section{Fine-Grained Visualization of the Reverse Generation Process}
\begin{figure}[H]
  \centering
   \includegraphics[width=0.925\linewidth]{cvpr2023-author_kit-v1_1-1/latex/suppl_pics/gen_steps.jpg}
   \caption{An example of the reverse process of LayoutDiffusion on RICO. We sample 99 steps uniformly from the total 200 timesteps.}
   \label{reverse steps} 
\end{figure}

\begin{figure}[H]
  \centering
   \includegraphics[width=1.01\linewidth]{cvpr2023-author_kit-v1_1-1/latex/suppl_pics/reverse_suppl.pdf}
   \caption{Examples of the reverse process of LayoutDiffusion on RICO. We sample 22 steps uniformly from the total 200 timesteps.}
   \label{reverse steps more} 
\end{figure}
