\section{Experiments}

\label{sec:experiments}
\subsection{Setups}
\noindent \textbf{Datasets.} 
We employ two widely-used public datasets of graphic layouts. 
\textit{RICO}~\cite{rico} is a dataset of user interface designs for mobile applications, which contains 66K+ UI layouts with 25 element types. 
\textit{PublayNet}~\cite{publaynet} includes 360K+ annotated scientific document layouts with 5 element types. 
Both datasets contain a few over-length entries.
We filter out the layouts longer than 20 elements as in Unilayout~\cite{unilayout}. 
Then, we split the filtered data into a training set, validation set, and test set by 90\%, 5\%, and 5\%.

\noindent \textbf{Baselines.} 
First, we compare LayoutDiffusion with leading approaches for layout generation.
Specifically, we compare against \textit{LayoutTransformer}~\cite{layouttransformer}, \textit{VTN}~\cite{vtn}, \textit{Coarse2Fine}~\cite{c2f}, and \textit{Unilayout}~\cite{unilayout} on unconditional generation (UGen); against \textit{NDN-none}~\cite{ndn}, \textit{LayoutGan++}~\cite{layoutganpp}, \textit{BLT}~\cite{blt}, and \textit{Unilayout}~\cite{unilayout} on generation conditioned on type (Gen-Type); against \textit{RUITE}~\cite{rahman2021ruite}, and \textit{Unilayout}~\cite{unilayout} on refinement. 
Moreover, we compare LayoutDiffusion with the existing diffusion models that do not consider the characteristics of layouts. 
\textit{Diffusion-LM}~\cite{diffusionlm} maps discrete data to continuous state space, while \textit{D3PM (uniform)}~\cite{d3pm} and \textit{D3PM (absorbing)}~\cite{d3pm} perform diffusion in discrete state space using different transition matrices.


\noindent \textbf{Implementation Details.} 
We set the weight of auxiliary loss as $\lambda=0.0001$ (see \cref{eq:total_loss}).
In the training, we set the timestep as  $T=200$; in the inference, we set the timesteps $T_{\text{UGen}}=200$, $T_{\text{Gen-Type}}=160$, and $T_{\text{Refine}}=50$ for different generation tasks.
For the layout sequence (see \cref{eq:layout}), we arrange elements in the alphabetical order of the type, and each token is embedded with $d=128$ dimensions.
For the denoising network $p_\theta(\mathbf{x}_0|\mathbf{x}_t)$, we apply a 12-layer Transformer encoder with 12 attention heads.
We train the model using AdamW optimizer~\cite{adam} with $2\sim4$ NVIDIA V100 GPUs. 
We also employ importance timestep sampling~\cite{improvedDdpm} during the training. 
See the appendix for more details.

\noindent \textbf{Evaluation Metrics.}
We adopt four metrics to measure the performance comprehensively. Among them, Frechet Inception Distance (\textbf{FID}) measures the overall performance, while Maximum Interaction over Union (\textbf{mIoU}), Alignment (\textbf{Align.}) and \textbf{Overlap} measure the quality from a specific aspect.
Specifically, \emph{FID} computes the distance between the distribution of the generated layouts and that of real layouts. Following the previous practice~\cite{ndn,layoutganpp}, we train a classification-based neural network to get the feature embedding for the layout.
\emph{mIoU} calculates the maximum IoU between bounding boxes of the generated layouts and those of the real layouts with the same type set~\cite{layoutganpp}.
\emph{Align.} measures whether the elements in a generated layout are well-aligned, either by center or by edges. 
In addition to the original implementation~\cite{ndn}, a normalization over the number of elements is applied.
\emph{Overlap} measures the overlapping area between elements in the generated layout. 
Following UniLayout~\cite{unilayout}, we ignore normal overlaps, \eg, elements on top of the background.


\begin{figure*}
  \centering
   \includegraphics[width=1\linewidth]{cvpr2023-author_kit-v1_1-1/latex/pics/qualitative.pdf} 
    \vspace{-20px}
  \caption{Qualitative comparison against strongest baselines selected by FID (better view in color and 2$\times$ zoom). 
  The first three row is for RICO and the last three is for PubLayNet.
  LayoutDiffusion generates high-quality and diverse layouts. Layouts from Unilayout either lack diversity (Un-Gen) or are flawed (Gen-Type and Refinement). Layouts from other methods exhibit misalignment and overlap frequently.}
  \label{qualitative_ungen}
  \vspace{-15px}
\end{figure*}


\subsection{Comparison with Existing Approaches for Layout Generation}

\noindent \textbf{Quantitative Analysis.}
In \cref{Tab:quantitative_results}, the methods without the symbol $\diamond$ are existing approaches for layout generation.

First, we compare FID as it is an overall metric for generation performance. 
LayoutDiffusion achieves significantly better FID scores than all other methods.
For example, on Un-Gen, LayoutDiffusion achieves \textbf{2.490} and \textbf{8.625} on RICO and PubLayNet datasets, respectively, while the best existing work only achieves 20.198 and 30.048.

Furthermore, we examine individual metrics, including mIoU, Overlap, and Align., each of which measures the quality from a specific aspect. 
LayoutDiffusion is in the top two on almost every metric and frequently achieves the best performance.
On the contrary, existing approaches may perform well on a certain metric but fail on the other individual metrics and the overall metric (i.e., FID), indicating that LayoutDiffusion is a well-rounded approach.
For example, on Un-Gen, LayoutTransformer has a good overlap score, but it does not perform well on Align., mIoU and FID; UniLayout has the best Align. for PubLayNet, but fails to achieve good performance on mIoU, overlap and FID.

Moreover, on conditional generation tasks (i.e., Gen-Type and Refinement), the above observations still hold, even though LayoutDiffusion is \textit{not re-trained} on these tasks (see \cref{subsec:condition_gen}) while existing approaches are re-trained.


\noindent \textbf{Qualitative Analysis.}
\cref{qualitative_ungen} shows qualitative results. 
On Un-Gen, LayoutDiffusion generates diverse and high-quality layouts. 
In contrast, LayoutTransformer mainly suffers from incorrect spacing and overlap, and UniLayout is deficient in diversity.
For example, for UniLayout, most layouts on RICO contain a top toolbar and several list items, and most layouts on PublayNet are double-columned and have many texts.
Besides, on Gen-Type and Refinement, LayoutDiffusion outperforms other methods (\eg., alignment, overlap and spacing) while it is not re-trained.

\noindent \textbf{User Study.}
On each task, we select the best two baselines by FID for the user study.
We design two kinds of evaluation.
One is \emph{quality} evaluation. We show three layouts from three models respectively (two from baselines and one from LayoutDiffusion) and invite the user to choose which one has the best quality (\eg, more plausible overall structure and pleasing details). 
Another one is \emph{diversity} evaluation. We show three sets of layouts from three models respectively, where each set contains five layouts from the same model.
Then, we invite the user to choose which set has the most diverse layouts.
For Refinement, we do not conduct diversity evaluation as it is not necessary for this scenario.

% results
\cref{user_study} shows the results. 
Across different datasets, tasks and evaluation modes, there are 10 groups of user studies in total, in each of which we invite 15 people and everyone labels 50 groups of layouts.
The user study shows that LayoutDiffusion outperforms other methods significantly.



\begin{figure*}
  \centering
   \includegraphics[width=0.9\linewidth]{cvpr2023-author_kit-v1_1-1/latex/pics/user_study_tight.pdf}
    \vspace{-10px}
  \caption{Results of the user study. For each model, we count how many people prefer the layouts generated from this model. The study shows that the results generated by LayoutDiffusion were favored by users over the other methods, particularly in terms of diversity.}
  \label{user_study}
  \vspace{-15px}
\end{figure*}

\begin{figure}
  \centering
   \includegraphics[width=0.9\linewidth]{cvpr2023-author_kit-v1_1-1/latex/pics/case_study_tight.pdf}
    \vspace{-10px}
  \caption{Reverse denoising process for unconditional generation on RICO (from left to right). Each row is for one model. The blank page is used when the generated layout sequence is invalid.} 
  % See appendix for more results.}
  \label{fig:case_study}
  \vspace{-6px}
\end{figure}

\subsection{Comparison with Traditional Diffusion Models}
\noindent \textbf{Quantitative Analysis.}
In \cref{Tab:quantitative_results}, the methods with the symbol $\diamond$ are traditional diffusion models. 
They are originally proposed for other generation tasks (\eg, image and text).
We adapt them for layout generation.
On Un-Gen, LayoutDiffusion achieves the best performance on most metrics.
On Gen-Type and Refinement, while traditional diffusion models can be used for conditional generation tasks without re-training, their performance is usually worse than existing methods for layout generation (\eg, UniLayout), not to mention LayoutDiffusion.
These observations demonstrate that our consideration of the heterogeneous nature of layout data is critical for both achieving good performance and realizing plug-and-play conditional generation.

\noindent \textbf{Qualitative Analysis.}
\cref{qualitative_ungen} shows a qualitative comparison with the best traditional diffusion model (selected by FID).
LayoutDiffusion consistently generates better layouts, \eg, better alignment and less overlap.
Moreover, \cref{fig:case_study} compares the reverse denoising processes of different diffusion models.
LayoutDiffusion quickly generates a draft layout and then gradually refines it to a pleasing layout, while other diffusion models take many steps to generate a rough layout and fewer steps for iterative refinement, which may limit the modeling of precise relationships between elements, such as strict alignment and no overlap.


\subsection{Ablation Studies and Discussions} 
\label{sec:ablation}
\input{cvpr2023-author_kit-v1_1-1/latex/sections/ablation}
\noindent \textbf{Transition Matrices.} 
Our transition matrices are designed by considering three critical factors, i.e., legality, coordinate proximity and type disruption (see \cref{subsec:forward}).
We remove the technique corresponding to each factor.
First, without considering the legality, the techniques for the other two factors cannot be applied.
Thus, LayoutDiffusion degrades to D3PM (absorbing or uniform).
\cref{Tab:quantitative_results} shows that LayoutDiffusion achieves better performance.
Second, to ignore coordinate proximity, we use uniform or absorbing transition for coordinate tokens, denoted as Uniform $\mathbf{Q}_t^{\text{coord}}$ and Absorbing $\mathbf{Q}_t^{\text{coord}}$ in \cref{ablation}.
LayoutDiffusion outperforms these two variations on most metrics, especially Overlap and FID.
Third, to study type disruption, we use uniform transition for type tokens, denoted as Uniform $\mathbf{Q}_t^{\text{type}}$ in \cref{ablation}.
LayoutDiffusion outperforms this variation, where the improvement of mIoU is most significant.
For experiments on conditional generation, please refer to Appendix.


\noindent \textbf{Noise Schedules.} 
To make the corruption of element types occur throughout the forward process, we set $\Tilde{T}$ as $0$ for $\overline{\gamma}_t$ (see \cref{eq:gamma_schedule}), which results in a linear schedule (denoted as Linear $\overline{\gamma}_t$ in \cref{ablation}).
We replace the noise schedule for $\beta_t$ with the original linear schedule in previous work~\cite{d3pm} (denoted as Linear $\beta_t$ in \cref{ablation}).
LayoutDiffusion outperforms both of them on most metrics, especially mIoU and Align.



\begin{table}[t]
\centering
\setlength{\tabcolsep}{1mm}{
\begin{small}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
 \multicolumn{1}{|c|}{\multirow{2}{*}{FID$\downarrow$}}    & \multicolumn{5}{c|}{Number of Steps}\\ \cline{2-6}
      & 100    & 200    & 500    & 1000   & 2000   \\ \hline
Diffusion-LM     & 17.759 & 17.164 & 11.984 & 11.741 & 11.448 \\ \hline
D3PM (absorbing) & 7.464  & 6.102  & 6.091  & 4.985  & 5.110      \\ \hline
D3PM (uniform)   & 6.986  & 6.910  & 5.351   & 5.575  & 5.239     \\ \hline
LayoutDiffusion  & 3.875  & 2.490  & 2.387      & 2.295      & 2.360      \\ \hline
\end{tabular}
\end{small}
}
 \vspace{-5px}
\caption{Ablation study on timesteps for diffusion models. All the experiments are on RICO with unconditional generation. The training and inference steps are set as the same. $2000$ and $1000$ are default settings used in Diffusion-LM~\cite{diffusionlm} and D3PMs~\cite{d3pm}.}
\label{tbl:speed}
\vspace{-17.5px}
\end{table}


\noindent \textbf{Timesteps.} 
\cref{tbl:speed} shows FID of diffusion models with different timesteps.
While $200$ steps are enough for LayoutDiffusion, the performance of Diffusion-LM, D3PM (absorbing), and D3PM (uniform) saturates at $500$, $1000$, and $500$ timesteps respectively.
Besides, even the fastest version of LayoutDiffusion surpasses all the other diffusion models.
\vspace{-5px}