\section{LayoutDiffusion}
\label{sec:approach}


\begin{figure*}
  \centering
   \includegraphics[width=0.9\linewidth]{cvpr2023-author_kit-v1_1-1/latex/pics/overview_new1.pdf}
    \vspace{-10px}
  \caption{An illustration for \textbf{LayoutDiffusion}. In the forward process, the coordinates are mildly corrupted into stationary distribution, and the element types are absorbed into \texttt{MASK} in the late stage. In the reverse process, the element types are first recovered, and then the rough coordinates are gradually refined. For brevity, only two elements are shown, while the other elements and the special tokens are omitted.}
  \label{fig:overview}
  \vspace{-12.5px}
\end{figure*}

We formulate the layout generation problem as a discrete denoising diffusion process (see \cref{fig:overview}).
It consists of two Markov chains, where the forward process is hand-designed and fixed while the reverse process is parameterized.

Give a real layout $\mathbf{x}_0\sim q(\mathbf{x_0})$, the \emph{forward} process corrupts it into a sequence of increasingly noisy latent variables $\mathbf{x}_{1:T}=\mathbf{x_1},\mathbf{x}_2,\dots,\mathbf{x}_T$,
% \vspace{-5px}
\begin{align}\label{eq:forward}
    & q(\mathbf{x}_{1:T}|\mathbf{x}_0)  = \prod_{t=1}^T q(\mathbf{x}_t|\mathbf{x}_{t-1}), \\\label{eq:transition}
    & q(x_{t}|x_{t-1}) = x_t \mathbf{Q}_t x_{t-1}.
\end{align}
Here, $x_t$ denotes the one-hot version of a single discrete token in the layout sequence $\mathbf{x}_t$. $\mathbf{Q}_t$ is the transition matrix, where $[\mathbf{Q}_t]_{ij}=q(x_t=j|x_{t-1}=i)$ represents the probabilities that $x_{t-1}$ transitions to $x_t$.
Due to the property of Markov chain, the cumulative probability of $x_t$ at arbitrary timestep from $x_0$ can be derived as $q(x_{t}|x_{0})=\mathbf{x}_t \overline{\mathbf{Q}}_t \mathbf{x}_{0}$, where $\overline{\textbf{Q}}_t=\textbf{Q}_1\textbf{Q}_2\dots\textbf{Q}_t$ (refer to \cite{d3pm} for details).

To generate a layout, the \emph{reverse} process starts with a random noise $\mathbf{x}_T$ and gradually recovers it relying on the learned posterior distribution $p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)$,
\begin{align}\label{eq:reverse}
    p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T)\prod_{t=1}^T p_\theta (\mathbf{x}_{t-1}|\mathbf{x}_t).
\end{align}

In the following, we will introduce how to construct a mild forward process $q(\mathbf{x}_t|\mathbf{x}_{t-1})$ for layout generation (\cref{subsec:forward}), and how to learn the generative model $p_\theta(\mathbf{x}_{0})$ in the reverse process (\cref{subsec:reverse}).


\subsection{Forward Process}
\label{subsec:forward}

In LayoutDiffusion, we propose a block-wise transition matrix $\mathbf{Q}_t$ and a piece-wise linear noise schedule to realize a mild forward process, in which layouts in the neighboring steps do not differ too much and become increasingly disordered as the forward step grows (see \cref{noise_ours}).

The design of the transition matrix and noise schedule stems from our three important observations.
\emph{(i) Legality.}
As defined in~\cref{formulation}, layout sequence has a rigorous format.
Any transition between element type tokens and coordinate tokens will lead to an illegal layout sequence, resulting in a disruptive change between forward steps.
Hence, it is vital to impose sequence legality in the transition matrix.
\emph{(ii) Coordinate Proximity.}
Coordinate tokens in layout sequence are ordinal and have a meaningful proximity.
Transitioning a coordinate token to its proximal tokens (e.g., from $0$ to $1$) will introduce a milder change to a layout, compared with transitioning to distant ones (e.g., from $0$ to $127$).
Thus, it is helpful to encode the proximity prior in the transition matrix.
\emph{(iii) Type Disruption.}
Type tokens are categorical and do not present particular proximity.
Each type of element has its unique coordinate distribution.
For example, a background image tends to have a large size, while a button has a small size.
Transitioning a type to another type may produce an abnormal element (e.g., a button has a large size and is placed at the top-left corner), leading to abrupt changes in layout. 
This is also consistent with the observation from diffusion models on other categorical
data, e.g., latent code and text~\cite{vqdiffusion,d3pm}.
Therefore, it is beneficial to alleviate type disruption in the transition matrix and noise schedule.


\noindent \textbf{Transition Matrices.}
There are three kinds of tokens in the layout sequence, including type tokens (i.e., $c_i$), coordinate tokens (i.e., $l_i$, $t_i$, $r_i$ and $b_i$) and special tokens (i.e., $\langle \text{sos}\rangle$, $\langle \text{eos}\rangle$ and $\|$).
Denote the number of different coordinate tokens and type tokens as $K$ and $C$.
Then, the transition matrix is denoted as $\mathbf{Q}_t \in \mathbb{R}^{V\times V}$, where $V = K+C+3$.

To achieve the legality of the layout sequence, we only allow the internal transition within each kind of tokens.
Thus, $\mathbf{Q}_t$ can be reduced to a block-wise diagonal matrix,
\begin{equation}\label{eq:transition_all}
    \mathbf{Q}_t={
\left[ \begin{array}{ccc}
\mathbf{Q}_t^{\text{coord}} &  & \\
 & \mathbf{Q}_t^{\text{type}} &\\
 & & \mathbf{Q}_t^{\text{spec}}
\end{array} 
\right ]},
\end{equation}
where $\mathbf{Q}_t^{\text{coord}}$, $\mathbf{Q}_t^{\text{type}}$ and $\mathbf{Q}_t^{\text{spec}}$ depicts the probabilities of the internal transition within coordinate tokens, type tokens and special tokens, respectively.

For $\mathbf{Q}_t^{\text{coord}}$, to encode the ordinal proximity, we introduce the discretized Gaussian matrix~\cite{d3pm} for coordinate tokens, which assigns a higher probability to the transition between more proximal tokens,
\begin{equation}
    \left[\mathbf{Q}_t^{\text{coord}}\right]_{ij} = \begin{cases}
\dfrac{\exp\left(-\frac{4|i - j|^2 }{(K-1)^2\beta_t}\right)}{\sum_{n=-(K-1)}^{(K-1)}\exp\left(-\frac{4n^2 }{(K-1)^2\beta_t}\right)}, &i\neq j\\[2.2em]
1 - \sum_{l=0, l\neq i}^{(K-1)} [\mathbf{Q}_t^{\text{coord}}]_{il}, &i = j \\
\end{cases}
    \label{eq:discretized_gaussian_mat}
\end{equation}
where the parameters $\beta_t$ influence the variance of the forward process distributions.

For $\mathbf{Q}_t^{\text{type}}$, to alleviate the type disruption, we choose to transit a type token to a special \texttt{MASK} token instead of another meaningful type token.
Therefore, we introduce the absorbing state transition matrix~\cite{d3pm} for type tokens, 
\begin{equation}
\mathbf{Q}_t^{\text{type}}={
\left[ \begin{array}{cccc}
1-\gamma_t & 0 &\cdots &0\\
0 & 1-\gamma_t &\cdots &0\\
\vdots &\vdots &\ddots &\vdots\\
\gamma_t & \gamma_t &\cdots & 1
\end{array} 
\right ]},
\end{equation}
where $\gamma_t$ indicates the probability that a token is absorbed into a \texttt{MASK} token, and $1-\gamma_t$ is the probability that a token stays unchanged.
 

For $\mathbf{Q}_t^{\text{spec}}$, as special tokens describe the structure of the layout sequence, any transition between them will lead to an invalid layout sequence. 
Therefore, we choose to disable any transition between them,
\begin{equation}
    \mathbf{Q}_t^{\text{spec}} = \mathbf{I},
\end{equation}
where $\mathbf{I}$ is an identity matrix.


\noindent \textbf{Noise Schedules.} 
An early absorbing of type tokens (i.e., transitioning to \texttt{MASK} token) will bring an abrupt change to the layout.
Hence, to further eliminate type disruption, we choose to make the element type begin to change only in the late stage of the forward process.
Specifically, we design $\overline{\gamma}_t = 1-\prod_{i=1}^t (1-\gamma_i)$ for the cumulative probability $q_t(x_t|x_0)$ as a piece-wise linear function,
\begin{equation}\label{eq:gamma_schedule}
    \overline{\gamma}_t = \begin{cases}
    0, &t< \Tilde{T} \\
(t-\Tilde{T})/(T-\Tilde{T}), &t \ge \Tilde{T} \\
\end{cases}
\end{equation}
Here, $\Tilde{T}$ is the timestep where the absorbing is enabled, and $T$ is the terminal timestep.

Besides, although existing work often uses linear schedule for Gaussian transition process, we choose to use $\beta_t=g/(T-t+\epsilon)^h$ for the transition of coordinate tokens $\mathbf{Q}_t^{\text{coord}}$. 
Here $g$ and $h$ are hyper-parameters, and $\epsilon$ denotes a small positive quantity.
It is generalized from a commonly used noise schedule $1/(T-t+1)$~\cite{2015,d3pm}.
We find that with $h>1$, it achieves a slower and more smooth corruption to the layout in the early forward process, which helps the model in the reverse process better learn the posterior distribution.


\subsection{Reverse Process}
\label{subsec:reverse}
To reverse the forward process, we optimize the generative model $p_\theta(\mathbf{x}_0)$ to fit the data distribution $q(\mathbf{x}_0)$ by minimizing the variational lower bound (VLB)~\cite{d3pm},
\begin{align}\label{eq:vlb}
    \mathcal{L}_{\text{VLB}} = -\log p_\theta(\mathbf{x}_0|\mathbf{x}_1) + 
    D_{\text{KL}}(q(\mathbf{x}_T|\mathbf{x}_0)\| p(\mathbf{x}_T)) \\\nonumber
    + \sum_{t=2}^T D_{\text{KL}}(q(\mathbf{x}_{t-1}|\mathbf{x}_t) \| p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)).
    % \vspace{-5px}
\end{align}

Following recent work~\cite{d3pm,vqdiffusion}, we predict $p_\theta(\mathbf{x}_{0}|\mathbf{x}_{t})$ instead of $p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_{t})$, and encourage good predictions of $\mathbf{x}_0$ at each step by combining $\mathcal{L}_{\text{VLB}}$ with an auxiliary objective,
\begin{equation}\label{eq:total_loss}
    \mathcal{L} = \mathcal{L}_{\text{VLB}} - \lambda\log p_\theta(\mathbf{x}_0|\mathbf{x}_t).
    % \vspace{-3px}
\end{equation}


Specifically, we leverage Transformer encoder~\cite{vaswani2017attention} to learn $p_\theta(\mathbf{x}_{0}|\mathbf{x}_{t})$.
Denote the embedding of $i$-th token in the layout sequence $\mathbf{x}_t$ as $\texttt{emb}(x_{t,i})$ and its positional embedding as $p_i$.
Denote the embedding of the timestep $t$ as $\texttt{emb}(t)$.
Then, Transformer takes the aggregation of them, i.e., $\{\texttt{emb}(x_{t,i})+p_i+\texttt{emb}(t)\}_{i=1}^N$, as the input and predicts a new layout sequence $\Tilde{\mathbf{x}}_0 = \{\Tilde{x}_{0,i}\}_{i=1}^N$ as the output. 



\subsection{Enabling Conditional Layout Generation in a Plug-and-Play Manner}
\label{subsec:condition_gen}
Although LayoutDiffusion is trained for unconditional layout generation, it can handle some conditional generation tasks without re-training, which has never been explored by previous work. 
Such a plug-and-play feature of conditional generation is enabled by the design of transition matrices and noise schedules.
In the following, we introduce how LayoutDiffusion achieves it.

\noindent \textbf{Refinement} is a user-oriented layout generation task first posed in RUITE~\cite{rahman2021ruite}, and is recently studied by UniLayout~\cite{unilayout}. 
Its goal is to take a user given flawed layout as input and provide a high-quality layout for the user while maintaining the original design style.
With the proposed transition matrices and noise schedules, a layout is gradually corrupted in the forward process.
With such a forward process, the reverse process learned by LayoutDiffusion is to iteratively improve a layout, which naturally enables refinement.
Specifically, in LayoutDiffusion, we achieve refinement by feeding the flawed layout into the model and then running reverse process from a certain timestep. 
Here the timestep is related to how noisy the input layout is.

\noindent\textbf{Generation Conditioned on Types (Gen-Type)} is also a widely studied conditional layout generation task~\cite{ndn,blt,unilayout} to satisfy the needs of user.
It aims to generate layouts with the given element types.
In LayoutDiffusion, there is no transition between coordinates and types (see \cref{eq:transition_all}).
Besides, with the noise schedule in \cref{eq:gamma_schedule}, the change of the types only occurs in the late forward process.
With the above two mechanisms, LayoutDiffusion will determine the element types in the early reverse steps very quickly and then continue to improve the coordinates in the remaining reverse steps without changing the types (see the transformation of the layout from right to left in \cref{fig:overview}). 
In other words, the generation for coordinates and that for element types are approximately decoupled.
Thus, in LayoutDiffusion, we achieve Gen-Type by feeding in the element types in the early stage and running the reverse process. 