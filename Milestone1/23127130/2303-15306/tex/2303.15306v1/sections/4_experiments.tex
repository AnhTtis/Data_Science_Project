\section{Experimental setup}\label{sec:experiments}
\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{images/network_structure.drawio.png}
    \caption{Visual depiction of the implemented LSTM model.}
    \label{fig:lstm-model-diagram}
\end{figure}

This section shows how the proposed model compares to FORM \cite{de2013structural}, the state-of-the-art approach in the field of music segmentation.
We develop a baseline model using a stacked LSTM-based neural network, depicted in figure \ref{fig:lstm-model-diagram}.
The model objective is to predict in which section each chord belongs to. We train our model on the Billboard dataset\cite{burgoyne2011billboard} provided by mirdata\cite{fuentes2021mirdata}. 
The dataset is composed of $889$ expert annotated tracks. Each track is composed of a sequence of chords in Harte format\cite{harte2005symbolic}, and a sequence of structure labels. 
Labels are provided in a similar format to the one presented by SALAMI\cite{smith2011salami}. $80$ unique section labels are present in the whole dataset. We preprocess each label and reduce the number of unique labels to $11$ by combining all those labels that fall under the same definition given by\cite{smith2011salami}.
A complete reference of the label conversion step is given in table \ref{tab:label-conversion}.

\begin{table*}[!ht]
  \caption{Label conversion reference. Each label is stripped out of numbers and symbols before the conversion.}
  \label{tab:label-conversion}
  \begin{tabularx}{\textwidth}{X|l}
    \toprule
    Source labels & Converted label \\
    \midrule
    \texttt{[verse]} & \textit{verse} \\
    \texttt{[prechorus, pre chorus]} & \textit{prechorus} \\
    \texttt{[chorus]} & \textit{chorus} \\
    \texttt{[fadein, fade in, intro]} & \textit{intro} \\
    \texttt{[outro, coda, fadeout, fade-out, ending]} & \textit{outro} \\
    \noindent\parbox[c]{\hsize}{\texttt{[applause, bass, choir, clarinet, drums, flute, harmonica, harpsichord, instrumental, instrumental break, noise, oboe, organ, piano, rap, saxophone, solo, spoken, strings, synth, synthesizer, talking, trumpet, vocal, voice, guitar, saxophone, trumpet]}} & \textit{instrumental} \\
    \texttt{[main theme, theme, secondary theme]} & \textit{theme} \\
    \texttt{[transition, tran]} & \textit{transition} \\
    \texttt{[modulation, key change]} & \textit{other} \\
    \bottomrule
\end{tabularx}
\end{table*}


Although in literature there are neural network architectures that have proven to perform better in similar task \cite{huang2015crfbilstm, shi2015convlstm, wang2016attentionlstm}, we deliberately decided to use a very straightforward architecture.
This is due to the fact that the aim of this study is to compare different types of embedding, rather than to achieve the best performance. 

%We deliberately avoid using more advanced network structures that have been proven to solve similar tasks  in a more effective way \cite{huang2015crfbilstm, shi2015convlstm, wang2016attentionlstm} to better probe how effective is the proposed embedding model.
We split our dataset in the usual training, validation and test split (respectively $800$, $178$ and $89$ elements) and fine-tune each model hyper-parameters (number of LSTM stacked layers, LSTM hidden size, dropout probability) to obtain the best results on the validation set. The final configuration of each model is summarised in Table \ref{tab:best-hyperparameters}. Training is performed using an \textit{NVIDIA RTX 3090} with a batch size of $128$. Each model takes at most few minutes to train and average less than $2$ milion parameters.

\begin{table}[b]
    \caption{Best hyper-parameters obtained on the validation set for each model.}
    \begin{tabularx}{\textwidth} { X|c|c|c }
        \toprule
        Model & Hidden size & Number of stacked layers & Dropout probability \\
        \midrule
        word2vec & 100 & 5 & 0.3 \\
        fasttext & 100 & 5 & 0.5 \\
        pitchclass2vec & 100 & 10 & 0 \\
        pitchclass2vec + word2vec & 200 & 5 & 0.3 \\
        pitchclass2vec + fasttext & 200 & 5 & 0 \\
        \bottomrule
    \end{tabularx}
    \label{tab:best-hyperparameters}
\end{table}

We compare the proposed embedding model to \textit{fasttext} and \textit{word2vec} as well, in which both methods are trained on the string labels of chords in Harte format. Both the models are trained using the highly optimized gensim \cite{rehurek2010lrec} implementation. The hyperparameters used are the same as the one described in section \ref{sec:implementation-details} except for the embedding dimension, which is set to $300$.