\section{Results}
\label{sec:results}
\begin{table}[b]
    \caption{Evaluation metrics on the test set. FORM\textsubscript{raw} is computed on the same chord labels that are used for all the neural approaches. FORM\textsubscript{simple} is computed on simplified chord representation as done in \cite{de2013structural}: only the chord \textit{root} and its quality (\textit{major} or \textit{minor}) are kept. }
    \begin{tabularx}{1.01\textwidth} { X|c|c|c|c|c|c }
        \toprule
        Method & $P$ & $R$ & $F1$ & $S_U$ & $S_O$ & $S_{F1}$ \\
        \midrule
        FORM\textsubscript{raw} & 0.667640 & 0.338019 & 0.425610 & 0.667640 & 0.338019 & 0.425610 \\
        FORM\textsubscript{simple} & \textbf{0.681281} & 0.325479 & 0.416651 & 0.681281 & 0.325479 & 0.416651 \\
        \hline
        word2vec & 0.410190 & 0.823330 & 0.523692 & 0.605202 & 0.257582 & 0.360186 \\
        fasttext & 0.373044 & \textbf{0.993201} & 0.526918 & \textbf{0.947381} & 0.154424 & 0.264553 \\
        pitchclass2vec & 0.402290 & 0.953399 & 0.547694 & 0.719733 & \textbf{0.431959} & \textbf{0.537879} \\
        \noindent\parbox[l]{\hsize}{pitchclass2vec + word2vec} & 0.467940 & 0.664824 & 0.532202 & 0.544820 & 0.415806 &  0.471398 \\
        pitchclass2vec + fasttext & 0.433019 & 0.835007 & \textbf{0.553045} & 0.539774 &  0.425738 & 0.473986 \\
        \bottomrule
    \end{tabularx}
    \label{tab:results}
\end{table}

The results of the experiments are summarised in Table \ref{tab:results}. We evaluate the segmentation results by computing pairwise precision, recall and F1-score ($P$, $R$ and $F1$ in Table \ref{tab:results}) \cite{levy2008structural} along with under-segmentation, over-segmentation and normalized cross entropy F1 ($S_U$, $S_O$ and $S_{F1}$ in table \ref{tab:results}) \cite{lukashevich2008towards}. Every metric is computed using the standard MIR evaluation library mir\_eval \cite{raffel2014mireval}.
\textit{Under-segmentation} and \textit{over-segmentation} are two peculiar metrics for the evaluation of automatic music segmentation methods.
When a method has an high over-segmentation measure, the final prediction accuracy is influenced mostly by false fragmentation. Conversely an high under-segmentation measure means that the prediction's segments are the result of ground-truth segments being merged together \cite{lukashevich2008towards}.

\medskip
\begin{figure}
\centering
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=.7\textwidth]{images/over_segmentation_diagram.drawio.png}
  \caption{High over-segmentation example. $R = 1$ since if we take each chord in the sequence pairwise then each chord that should be in the same section is indeed in the same section. $S_O = 1$ since the accuracy of the prediction can be easily explained by the over-segmentation phenomena. Conversely, $P = 0.53$ and $S_U = 0.53$ clearly show how the prediction is not able to capture all the needed segments but rather merges ground truth segments together. }
\end{subfigure}
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=.7\textwidth]{images/under_segmentation_diagram.drawio.png}
  \caption{High under-segmentation example. The exact opposite of Figure (a) is displayed. The prediction is not able to capture segments and rather place each chord on its own segment. }
\end{subfigure}
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=.7\textwidth]{images/pathological_segmentation_diagram.drawio.png}
  \caption{$P$, $R$ and $S_O$, $S_U$ compared. In this edge case the main difference between the two measures is highlighted. While $P$ and $R$ suggests a decent segmentation $S_O$ and $S_U$ clearly states a completely wrong segmentation. Pairwise metrics can be misguiding in absence of $S_O$ and $S_U$. }
\end{subfigure}
\caption{Examples of metric computation on relevant instances.}
\label{fig:measures-examples}
\end{figure}

Pairwise metrics are computed as the usual precision, recall and F1 scores on the set of identically labeled pairs in the sequence.
Precision and recall can be interpreted as the amount of accuracy that is influenced respectively by under-segmentation and over-segmentation.
On the other hand under and over-segmentation scores are computed by taking into account the normalized conditional entropy of the segmentation. 
In short, $S_O$ gives a measure of how much information is missing in the predicted segmentation, given the ground truth segmentation while $S_U$ gives a measure of how much noisy information are the result of the predicted segmentation \cite{lukashevich2008towards}.
A graphical explanation of these concepts is provided in Figure \ref{fig:measures-examples} (all the examples are taken from \cite{lukashevich2008towards}).


We evaluate our models based on the $F1$ and $S_{F1}$ scores of Table \ref{tab:results} since both metrics gives a balanced measure of over and under segmentation.


FORM\textsubscript{simple} detect repetitive patterns from simplified chord labels, as shown in \cite{de2013structural}. The chord simplification process extracts the \textit{root} note from the chord and classifies it either as \textit{major} or \textit{minor}. FORM\textsubscript{raw} uses the same labels used by the neural approaches.
The former performs better better than the latter. This is not surprising as more patterns between strings can be uncovered by only taking into account $24$ labels ($12$ root notes, each of which can be either \textit{minor} or \textit{major}). 
$S_O$ and $S_U$ however suggests that the over-segmentation based approach of FORM ends up correctly segmenting only some particular portions of the whole composition, while the other ones are wrongly classified. This is an expected behaviour since FORM only relies on label-based repeated patterns. 
The presence of subtle differences in an harmonic progressions that belongs to the same section, such as the first and second \textit{verse} in Figure \ref{fig:helterskelter-structure}, are not detected.

All the neural models in table \ref{tab:results} outperforms FORM. Even though each neural model shows some differences in term of metrics, the clear trend is that syntactical-based models (\textit{fasttext} and \textit{word2vec}) yield overly segmented results, as the low $S_O$ score suggests, while the approach taken by \textit{pitchclass2vec} produces a more balanced segmentation, as suggested by the $F1$ score.
Surprisingly, \textit{fasttext} and \textit{word2vec} under-performs when compared to the FORM baseline on $S_{F1}$ metric. The model is not able to generalize enough over the representation and cannot detect patterns that are detected by FORM.
Finally, the combination of \textit{pitchclass2vec} with either \textit{fasttext} or \textit{word2vec} doesn't bring any remarkable benefit to our novel representation. Even though an higher $F1$ score is obtained by using an hybrid approach, the lower $S_{F1}$ score suggests that it has an underlying less accurate segmentation, similar to example (c) in Figure \ref{fig:measures-examples}.