\section{Music segmentation: state of the art}
\label{sec:related}

Automatic segmentation on audio signal is a prolific research field in which many different solutions have been presented, ranging from self-similarity matrices \cite{weiss2011unsupervised, bello2010identifying} to neural network based methods \cite{shibata2020music, wang2021supervised, marmoret2021uncovering}. Harmonic content has been used to improve those methods both using probabilistic models \cite{pauwels2013combining} and transformer based models \cite{chen2019harmony}.
Significant research has been performed on phrase-level structural segmentation based on melodic \cite{frankland2004structuralMelody, cambouropoulos2001local, velarde2013approach} as well as polyphonic content \cite{meredith2002algorithms}.
However, to the best of our knowledge, the only approach proposed in literature for global music segmentation on symbolic harmonic content is FORM \cite{de2013structural}.
%Automatic segmentation on audio signal is a prolific research field in which many different solutions have been presented, ranging from self-similarity matrices \cite{weiss2011unsupervised, bello2010identifying} to neural network based methods \cite{shibata2020music, wang2021supervised, marmoret2021uncovering}.
%To the best of our knowledge, the only approach proposed in the literature for music segmentation on symbolic content is FORM \cite{de2013structural}.
%Few attempts as been carried to structure segmentation using only symbolic information with FORM \cite{de2013structural} being the only one, to our current knowledge. 


FORM performs structural segmentation by exploiting repeated patterns extracted from harmonic progressions encoded as sequences of strings. Each string represent a chord. In the original work chord labels are transformed into $24$ class of chords, $12$ major chords and $12$ minor chords, while every other chord feature is removed. 
In this paper, FORM is re-implemented in order to compare the results of the proposed method with the current state of the art (see Section \ref{sec:results}).
FORM pattern detection algorithm is based on suffix trees. Each node on a suffix tree represents a (possibly recurrent) sub-sequence of a string. FORM extracts sub-sequences appearing in at least two position in the analyzed harmonic progression.
%and can be classified as \textit{left diverse} node. A \textit{left diverse} node contains at least two subtrees with different suffixes, and represents sub-sequences that share a common portion of the sub-sequence (they have a common prefix) but ends with at least two different suffixes. 
%For instance, given the string \textit{"AAABAAAC"}, the node that represents the sub-sequence \textit{"AAA"} is a \textit{left diverse node}, since its sub-tree contains the leaves \textit{"AAAB"} and \textit{"AAAC"}. 
A partial segmentation is obtained by labeling each sub-sequence as a new section. The final segmentation is obtained by labeling remaining sub-sequences as their preceding neighbouring section.
The results are compared with a random baseline that generates arbitrarily long structures and to a heuristic that assigns to each composition the typical pop song structure \textit{ABBBBCCBBBBCCDCCE} \cite{de2013structural}, in which each different label represent a structure in the chord progression and is stretched to fit the whole sequence.
The main issue with FORM is in way chord labels are compared. The string representation does not take into account semantic similarity between chords nor the algorithm is able to detect near-similar patterns, i.e. patterns whose difference can be ignored in the context of music structure segmentation.

\medskip
Our structure segmentation method is based on our novel chord representation method, \textit{pitchclass2vec}, based on continuous word representation.
The core idea of continuous word representation is based on the Distributional Hyphotesis \cite{sahlgren2008distributional}: the semantic meaning of a word $w$ can be approximated from the distribution of words that appear within the context of $w$.
The objective of continuous word representation is the maximization of the following log-likelihood:
\begin{displaymath}
  \sum_{t=1}^{T} \sum_{c \in \mathcal{C}_t} \log p(w_c | w_t),
\end{displaymath}
where $\mathcal{C}_t$ represents the indices of the words that appears as context of the word $w_t$ and the function $p(w_c | w_t)$ is parameterized using $d$-dimensional vectors in $\mathbb{R}^d$, respectively $\mathbf{u}_{w_t}$ and $\mathbf{v}_{w_c}$.
The problem can be framed as a binary classification task in which words are predicted to be present (or absent) from the context of $w_t$.
A similarity function $s(w_c, w_t)$ between two words $w_c$ and $w_t$, can be computed as the scalar product $\mathbf{u}_{w_t}^T \mathbf{v}_{w_c}$.
The representations obtained by training the described method on a large corpus correctly approximates the semantic meaning of words. In the last few years, continuous word representation has been applied in a growing number of application areas, achieving state-of-the-art results in the natural language processing field \cite{qilu2021briefsurvey} in tasks such as part of speech tagging\cite{meftah2018pos}, named entity recognition\cite{chiu2016ner} and document classification\cite{lilleberg2015support}.

\medskip
\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[height=14em,keepaspectratio]{images/word2vec_diagram.drawio.png}
  \caption{word2vec}
  \label{fig:word2vec}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[height=14em,keepaspectratio]{images/fasttext_diagram.png}
  \caption{fasttext}
  \label{fig:fasttext}
\end{subfigure}
\caption{ \textit{Word2vec} (a) and \textit{fasttext} (b) embedding methods. With \textit{word2vec} each embedding is computed independently of its morphological structure. \textit{Fasttext} instead compute the representation as the sum of the n-grams that compose a word. Words that share one or more n-grams have a similar representation as they are computed in a similar way. In the example 2-grams are represented but in general n-grams up to the length of the term are commonly used.}
\label{fig:word2vec-vs-fasttext}
\end{figure}

The described approach has been first proposed by the \textit{word2vec} skipgram model \cite{mikolov2013word2vec}. \textit{Word2vec}, however, is limited by the lack of morphological knowledge of a word. 
When computing the representation of a word, none of its morphological components are taken into account. Let's take for instance two morphologically similar words, \textit{house} and \textit{housing}. Their computation does not share any common element and the final representation of the words will not be influenced by their similarities.
\textit{Fasttext} \cite{bojanowski2018fasttext} was presented as a solution to this issue and has proven to be more effective in the representation of a word using continuous representations. 
The novel aspect is in the way representations are computed. At first the n-grams that compose a word are extracted. For each n-gram a continuous vector representation is computed, using the same methodology as \textit{word2vec}. The representation of the original words is finally obtained as the sum of its n-gram components. 
Using this technique, the final representation of a word is conditioned by its morphological structure. When two words share one or more n-grams their vectors will be the sum of at least one common element, which will bias both vectors in being more similar to each other. 
An additionally advantage of the \textit{fasttext} approach is the way out-of-vocabulary words (words that never appear in the training corpus, and whose representation is hence unknown) are handled. When using fixed approach such as \textit{word2vec}, out of vocabulary words are represented as a static vector, usually randomly sampled from a normal distribution. \textit{Fasttext} instead is able to compute the representation in a meaningful way, given that at least one of the n-grams in the out-of-vocabulary term has been computed previously in the training corpus. 
Figure \ref{fig:word2vec-vs-fasttext} shows a visual comparison between \textit{word2vec} (Figure \ref{fig:word2vec}) and \textit{fasttext} (Figure \ref{fig:fasttext}).

Continuous word representations have already been applied to chord symbols with promising results. In \textit{chord2vec} \cite{madjiheurem2016chord2vec} the authors obtain state-of-the-art results on the log-likelihood estimation task. Log-likelihood estimation is the task of correctly estimating, given one element in a sequence, the probabilities of another element being the upcoming element in the sequence. 
\textit{Chord2vec} is inspired by the \textit{word2vec} method, in which chords are represented by the notes that they are composed of. 
The representation model proposed by \textit{chord2vec} is similar to \textit{pitchclass2vec}. 
Instead of computing chord representations with the notes that compose a chord, the representations of \textit{pitchclass2vec} takes into account the relationship between the notes that compose each chord. An in depth discussion is presented in section \ref{sec:model}.

More recently, \textit{word2vec}-based approach on symbolic chord annotations has been analyzed by \cite{anzuoni2021historical}. 
Chord representations are based on the chord label without taking into account the notes that compose it. The encoding is then used on two different tasks: chord clustering and log-likelihood estimation. 
The log-likelihood estimation task is used to investigate the historical harmonic style of different composers. 
The log-likelihood results strongly correlates with current musicological knowledge. For instance the model finds difficult to predict chords from artists that make a sporadic use of common harmonic progressions \cite{anzuoni2021historical}. 
FThe chord clustering task highlights how it's possible to observe similarities between \textit{functionally equivalent} chords (chords that shares notes between each other) and a well defined difference between \textit{functionally different} chords. 
Continuous word representations are hence adequate to encode chords in the first place, and more importantly they are able to autonomously internalize relations between chords that have been previously observed by domain experts.
