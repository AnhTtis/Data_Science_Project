
%\documentclass[journal]{IEEEtran}
% \documentclass[10pt,journal,compsoc]{IEEEtran}
% \documentclass[5p,times]{elsarticle}
\documentclass[review,preprint,12pt,3p]{elsarticle}

%%%%%
%% 35 double-spaced pages
%% abstract must be less than 150 words. 
%% up to 10 keywords


%\usepackage[nocompress]{cite}

\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
% \usepackage{subfigure} % THIS IS A deprecated package. Dont use this.
\usepackage{subfig}
% \usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{arrows,shapes,positioning,shadows,trees, patterns}
% \usepackage{subcaption} % for multiple table in one line
% \usepackage{caption}
\usepackage{epsfig}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{setspace}
\usepackage{color, soul}
%\graphicspath{{eps/}}
\newtheorem{theorem}{Theorem}
\usepackage{wrapfig}
\usepackage{tabularx}
\usepackage{stfloats}
\usepackage{pgfplots} 
% \usepackage{todonotes}
\pgfplotsset{width=2cm,compat=1.5}
% \pgfplotsset{compat=1.2}
%\newtheorem{lemma}{Lemma}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

% RGB Color Code: GREEN(70,241,70) YELLOW(255,255,0)  RED(255,128,128)  BLUE(147,147,255) WHITE(255,255,255)  SKY(176, 201, 247)

% \newcommand{\rone}[1]{\definecolor{green}{RGB}{70,241,70}\sethlcolor{green}\hl{#1}}
% \newcommand{\rtwo}[1]{\definecolor{red}{RGB}{255,128,128}\sethlcolor{red}\hl{#1}}
% \newcommand{\rthree}[1]{\definecolor{yellow}{RGB}{255,255,0}\sethlcolor{yellow}\hl{#1}}
\newcommand{\revison}[1]{\definecolor{sky}{RGB}{255,255,255}\sethlcolor{sky}\hl{#1}}




\journal{Journal of Parallel and Distributed Computing}
\begin{document}


\input{all_data_for_figs-v2.tex}



\begin{frontmatter}

\title{HeRAFC: Heuristic Resource Allocation and Optimization in MultiFog-Cloud Environment}

\author[label1]{Chinmaya~Kumar~Dehury}
    \ead{chinmaya.dehury@ut.ee}

\author[label2]{Bharadwaj~Veeravalli}
    \ead{elebv@nus.edu.sg}
    
\author[label3]{Satish~Narayana~Srirama \corref{cor}}
    \cortext[cor]{Corresponding author.}
    \ead{satish.srirama@uohyd.ac.in}

%\affiliation[label1]{organization={Mobile \& Cloud Lab, Institute of Computer Science, University of Tartu},	             
%	             postcode={50090},
%	             state={Tartu},
%	             country={Estonia}}
\address[label1]{Mobile \& Cloud Lab, Institute of Computer Science, University of Tartu, Tartu 50090, Estonia}

%\affiliation[label2]{organization={Mobile \& Cloud Lab, Institute of Computer Science, University of Tartu},	             
%	postcode={50090},
%	state={Tartu},
%	country={Estonia}}
\address[label2]{Department of Electrical and Computer Engineering, National University of Singapore, Singapore.}

%\affiliation[label3]{organization={Mobile \& Cloud Lab, Institute of Computer Science, University of Tartu},	             
%	postcode={50090},
%	state={Tartu},
%	country={Estonia}}
\address[label3]{School of Computer and Information Sciences, University of Hyderabad, Gachibowli, Telangana, India.}

\begin{abstract}
\revison{By bringing computing capacity from a remote cloud environment closer to the user, fog computing is introduced. As a result, users can access the services from more nearby computing environments, resulting in better quality of service and lower latency on the network. From the service providers' point of view, this addresses the network latency and congestion issues. This is achieved by deploying the services in cloud and fog computing environments. The responsibility of service providers is to manage the heterogeneous resources available in both computing environments. In recent years, resource management strategies have made it possible to efficiently allocate resources from nearby fog and clouds to users' applications. Unfortunately, these existing resource management strategies fail to give the desired result when the service providers have the opportunity to allocate the resources to the users' application from fog nodes that are at a multi-hop distance from the nearby fog node. The complexity of this resource management problem drastically increases in a MultiFog-Cloud environment. This problem motivates us to revisit and present a novel Heuristic Resource Allocation and Optimization algorithm in a MultiFog-Cloud (HeRAFC) environment. Taking users' application priority, execution time, and communication latency into account, HeRAFC optimizes resource utilization and minimizes cloud load. The proposed algorithm is evaluated and compared with related algorithms. The simulation results show the efficiency of the proposed HeRAFC over other algorithms.}

%% This is just another abstract
% Extending the cloud resources towards the edge of the network, fog computing evolved as the backbone of many real-time services. This not only acts as complementary to the cloud but also addresses network latency and congestion issues. Users' applications are deployed both on fog and cloud environments, and service providers have to manage the heterogeneous resources in both the environments. The recent research articles addressed the problem of managing the resources while allocating resource to users' application from both fog and the cloud environments. However, the complexity of this problem drastically increases when the service provider has the opportunity to allocate the resources from nearby fog, cloud and the fogs that are at a multihop distance. This problem motivates us to revisit the resource allocation problem in MultiFog-Cloud environment and present a novel heuristic Resource Allocation and Optimization algorithm in multiFog-Cloud (HeRAFC) algorithm. The problem is formulated as Integer Linear Programming problem to optimize the resource utilization and minimize the load on cloud taking the priority, also considering latency requirement and makespan of the application. The proposed algorithm is evaluated and compared with the related algorithms. The simulation results show the efficiency of the proposed HeRAFC over other algorithms. 
\end{abstract}
\begin{keyword}
Fog computing, cloud computing, resource management, resource allocation, MultiFog-Cloud, heuristic algorithm
\end{keyword}

%%Graphical abstract
% \begin{graphicalabstract}
% %\includegraphics{grabs}
% \end{graphicalabstract}

%%Research highlights
% \begin{highlights}
% \item The resource allocation problem in a multifog-cloud environment is studied.
% \item The physical infrastructure of multifog-cloud is modeled using graph theory.
% \item The resource allocation problem is formulated with a mixed ILP model.
% \item The proposed algorithm can also deploy the user's task on nearby fog nodes.
% \item Simulation results outperform other related algorithms in said environment.
% % \item Futuristic multifog-cloud computing architecture is taken into consideration.
% % \item Parameters such as fog-fog latency and distance, execution time, and priority are included.
% % \item A heuristic Resource allocation algorithm for multifog-cloud is proposed.
% \end{highlights}

\end{frontmatter}


\section{Introduction}


% About cloud and fog environments
% Cloud computing is not suitable for certain types of applications where network latency plays a significant role in providing real-time service despite the ability to store and process a large volume of data and scientific workflows \cite{tran-dang_frato_2021, dehury_failure_2020}. Fog computing is introduced to mitigate this issue by introducing a computing environment between the users and the cloud environment \cite{tran-dang_reinforcement_2022, tran-dang_frato_2021}. The fog computing platform also referred to as a Fog Node (FN), consists of multiple heterogeneous physical servers installed with virtualization/container technology and devices such as switches, routers, and gateway devices \cite{liu_energy_2022}. 

% About top-down and bottom-up approch while placing a user's request.
In a bottom-up approach, the Fog Node (FN) receives the users' requests consisting of multiple tasks. FN may process some tasks of a request and offload others to the cloud environment based on the resource availability at the FN, the Service Level Agreement (SLA), the priority of the user or request, and other Quality of Service (QoS) requirements. However, in a top-down approach, the resource availability in the cloud environment is first checked to execute the tasks. Following this, the rest of the tasks of a user's request are assigned to the available resources at FN \cite{kumar_dehury_efficient_2020, 8014362}. Both approaches differ based on what environment is given higher preference while allocating the resource to the users' request. Resource in this context refers to CPU, memory, storage, processing capability, etc., of the servers and bandwidth and transmission latency of the network.

% About resource management in fog and cloud and hint to the problem
The current architectures consist of multiple users at the bottom layer, served by single FN at the fog layer (middle layer) and a single cloud at the top of the hierarchy \cite{MASS2019100051, comp1, comp2}. The FNs are consisting of less amount of computing and storage resource. On the other hand, from the user's perspective, the resource availability at the cloud is generally considered as infinite, thanks to the underlined virtualization technology and other resource management tools. Management of the entire lifecycle of such diverse computing environments with different sets of resource capacity, configurations and characteristics is refereed to as heterogeneous resource management \cite{delimitrou_quasar_2014}. The heterogeneous resource management for single FN and single cloud environments is crucial especially while hosting the user's request. Inefficient management may lead to resource unavailability at FN, prohibiting the task from being deployed on FN. Several approaches, such as Integer Linear Programming (ILP) \cite{comp1}, heuristic approach, genetic algorithm, game theory \cite{comp2}, etc., have been investigated recently to optimize resource utilization. This resource management problem becomes more complex when the multiple interconnected FNs are introduced at the middle layer of the hierarchy, as shown in Figure \ref{fig:mfcenv} \cite{IndieFog}. This paper addresses the resource allocation and optimization problem in the above-mentioned multiFog-Cloud environment. 

% Fog introduction
% \IEEEPARstart{T}{he} unprecedented speed in the growth of diverse communication technology leads to higher bandwidth internet service, which is used as the backbone of different computing paradigm such as cloud computing, fog/edge computing. These new computing paradigms have drastically changed the way computing was done in a stand-alone system. Being very near to the users' device or the terminal device, fog computing is pushing the developer to deliver the services in a more real-time manner. Fog computing is considered as the mini-version of the cloud computing with the ability to provide a platform to deploy and share a pool of computing and storage servers among nearby users. Fog computing is introduced by CISCO in 2012 to compensate for the disadvantages of cloud computing \cite{elmisery2017new}. 

% intro to cloud. Why fog introduced?
%Cloud computing, being at the top of fog computing, provides a large pool of high-end sophisticated physical servers. Despite the ability to store and process a large volume of data and large scientific workflows, cloud computing is not suitable for certain types of applications where network latency plays a major role in providing real-time service. For example, Fog computing is used to manage the Cyber Physical System (CPS) resources in a smart city scenario, that involves in collecting and processing locally a large volume of Internet of Things (IoT) data. In modern industries, each piece of data from one device need to be processed with almost no delay and need to be forwarded to another device. Processing such data in cloud would experience an obvious network delay and may hamper the entire manufacturing process. The shortfall of cloud computing forced the research community to design and introduce fog computing that works in conjunction with cloud computing and acts as the bridge between cloud and end device. Similar to cloud computing, fog computing is consisting of multiple physical servers and devices such as switches, routers, gateway devices, installed with virtualization/container technology. The physical devices are heterogeneous in terms of computing and storage resource capability. 

%% What is offloading
%In practice, the large data analytic applications, scientific applications such as weather forecasting, satellite image analysis, medical image analysis, bioinformatics, and gnomic applications are deployed in cloud computing. Such resource-intensive applications can not be deployed in fog computing platform due to the insufficient computing and storage resources. As discussed earlier, cloud environment is also not suitable for some applications that need real-time service delivery, such as in smart grid, video analysis, health care system \cite{RahmanFOG2018}. Moreover, the application that is deployed in a remote cloud computing environment may also need real-time service delivery. In such a scenario, the applications/tasks that are consisting of large number of interconnected modules responsible for delivering the real-time service are deployed in fog computing environment instead of cloud. This process is also known as application module offloading or task offloading. As a result, the whole application is deployed in both cloud and fog environment. 


% Application module placement
% The cloud applications mainly comprise a large number of interconnected modules/tasks (henceforth the term 'task' will be used in rest of the paper), collectively responsible for providing whole or part of service. The modules of the applications with computing and network resource requirements are generally placed on the fog-cloud environment by following two approaches: top-down and bottom-up approach. In the bottom-up approach, the resource availability at fog environment is first checked. Based on which and based on the priority or importance, the modules are placed on the fog followed by on the cloud. However, in later approach, the resource availability at cloud environment is first checked and modules are placed. In summary, both the approaches differ based on what environment is given higher preference for application placement. 

% resurce management
% As discussed earlier, resource availability plays a major role in application module placement strategy. Improper resource management may result in unavailability of the required computing environment. For example, lack of resources in the fog node may prohibit the application module from being deployed. Fog Node (FN) in this paper refers to the fog environment in one geographic location consisting of certain number of physical servers/devices for computation and storage, which are connected among themselves. FN also includes the gateways devices to which different sensors and actuators are connected. Through gateways the users' applications are submitted to the fog hierarchy. Several parameters play a major role in efficient resource management. Resource in this context refers to CPU, memory, storage, processing capability, etc. of the servers and bandwidth and transmission latency of the network, etc. Further, efficient utilization of the resources refers to the optimization of amount of resource allocated and maximum resource capacity ratio. Several approaches such as Integer Linear Programming (ILP), heuristic approach, genetic algorithm, etc. are investigated recently in order to optimize the resource utilization.

%DONE: UPDATE the first figure with better clarity. Use drawio figure.
\begin{figure*}[htbp]
	\centering
	\subfloat[Abstract view of MultiFog-Cloud (MFC) environment.]
	{
		\includegraphics[width=0.35\linewidth]{MFCenv.pdf}
		\label{fig:mfcenv}
	}\hspace{15mm} %\hfill
	\subfloat[An example of user's application (in DAG form).]
	{
		\includegraphics[width=0.15\linewidth]{userApp.pdf}
		\label{fig:userApp}
	}
	\caption{Example of MFC and user's application.}
	\label{fig:mfcenv_userApp}
\end{figure*}

\subsection{Motivation and Contribution}
%In general, the applications are consisting of multiple interconnected modules/tasks (henceforth the term 'task' will be used in rest of the paper). 
The existing mechanisms classify the application tasks into two groups: tasks for fog environment and tasks for the cloud environment, based on the resource demand, task type, priority, deadline of the application task, etc. As per the classification result, tasks are deployed and the resources are allocated to the tasks either from nearby fog or the cloud system. For example, in the case of real-time video streaming analysis in smart city, the raw video is preprocessed in the nearby fog computing node. %, which is consisting of a number of physical servers. 
Further, the preprocessed video is forwarded to cloud for in-detail processing, indexing and time-stamping of the video, which can be stored and accessed in future across the globe. In this fog-cloud architecture, single FN and one or more cloud providers are involved. 

The above fog-cloud architecture works efficiently as the multiple FNs across the street are deployed to handle the growing demand. 
%However, in case of the uneven workload demand among the FNs in the city, at any certain time instance, one FN near to the city center may receive significantly higher number of demands as compared to the other FNs at the border of the city. 
However, due to uneven workload demand among the FNs in the city, some tasks may get rejected by the FNs at the city center and the request may get forwarded to the cloud. In such case, it is hard to maintain the QoS and the service provider may violate the Service Level Agreement (SLA). 
On the other hand, the resources of the FNs that receive less number of  demand, remain underutilized, resulting in lesser revenue for the service provider. Here, revenue refers to the monetary income generated from the service provider's business. In order to maintain the higher revenue, the service provider may increase the service cost affecting the users' QoE. This motivates us to revisit the resource allocation problem in a fog-cloud environment with the goal to maximize the resource utilization of the FNs. 

In order to address above research issues and be able to provide services in a real-time manner by the fog-cloud computing environment, in this paper, we attempt to modify the existing fog-cloud computing model. This is done via establishing the communication among the FNs through Fog-Cloud Interface (FCI), as shown in Figure \ref{fig:mfcenv}. 
%\rthree{This would further introduce additional complexity to the resource management problem as the service provider now needs to provide the service to its user-base not through a single FN, rather through multiple interconnected FNs. The additional complexity to the problem can also be due to the increase in number of fog servers that need to be utilized efficiently.} 
%\rthree{This paper revolves around the question, how to allocate the resources from nearby FN(s), that are at multi-hop distance, and the cloud servers to the application consisting of multiple tasks. Further the allocation strategy should not compromise the performance provided by the existing fog-cloud environment.} 
The scope of this paper is to meet the following main goals which are summarized as follows:
% Goals in bullet point
\begin{itemize}
	\item Design a novel resource allocation strategy for MFC environment and multi-task user application.
	\item To optimize the resource utilization by allowing the tasks to be placed onto nearby FNs that are at multi-hop distance.
	\item To design and evaluate the performance of an algorithm to decide the order of processing and assigning the tasks onto MFC environment by considering the tasks' topology and their resource demand. 
\end{itemize}
Based on the motivation and the goals mentioned above, the main contributions in this paper can be summarized as follows:
\begin{itemize}
	\item An architecture for the multifog-cloud is designed that allows application tasks to be forwarded to other FNs at multi hop distance, thus reducing the workload on the cloud.
 %An architecture for the multifog-cloud is designed to allow application tasks to be forwarded to other FNs situated at multi-hop distances, rather than the cloud itself. This allows the service provider to take full advantage of all the available resources in the fog layer. By doing so, the workload on the cloud is significantly reduced without compromising the quality of service.  
	\item The problem of resource allocation in multifog-cloud scenario is formulated as Integer Linear Programming (ILP) model. FNs located at multi-hop distances can also handle the users' requests, which complicates the problem when the different FCIs have different communication capabilities. 
	\item The physical infrastructure of multifog-cloud is modeled using graph theory. 
	\item A novel dedicated heuristic algorithm is designed for placement of the application tasks in multifog-cloud scenario.
	\item We exhibit the performance of proposed algorithms in improving fog and cloud computing resource utilization, network utilization, latency improvement, and other system parameters through extensive simulation.  
\end{itemize}

% paper organisation
The rest of the paper is organized as follows. 
In Section \ref{sec:relWorks}, a brief summary of recent related articles is presented. 
In Section \ref{sec:symodel}, the system model on the users' application and physical infrastructure is presented. 
Section \ref{sec:probFormulation} presents the formulation of resource allocation problem in single-Fog single-cloud environment, where the user's tasks are deployed either in nearby FN or in cloud node.
This followed by Section \ref{sec:MFCmodel}, formulation of resource allocation problem in Multi-Fog Cloud environment, where user's tasks will have more options to be deployed. 
Considering the model, a heuristic resource allocation strategy is presented in Section \ref{sec:sol} followed by the performance evaluation of the proposed algorithm in Section \ref{sec:perfEval}. 
The concluding remarks and the scope of the future work are presented in Section \ref{sec:concls}. 
The list of acronyms that are used in this article is given in Table \ref{table:acrnm}.


% Table Created with  following online tool
%  https://tableconvert.com/latex-generator
\begin{table}
 	\caption{List of Acronyms}
 	\footnotesize
	\centering
	\begin{tabular}{|l|l||l|l|}
        \hline
            \textbf{Acronym } & \textbf{ Description } & \textbf{Acronym } & \textbf{ Description } \\ \hline
            CPS  &  Cyber Physical System   & QoS  &  Quality of Service  \\ \hline
            DAG  &  Directed Acyclic Graph   & SFC  &  Service Function Chain   \\ \hline
            FCI  &  Fog-Cloud Interface   & SLA  &  Service Level Agreement  \\ \hline
            FN  &  Fog Node  & VM  &  Virtual Machine  \\ \hline
            ILP  &  Integer Linear Programming   & WMD  &  Weighted Multi-Dimensional   \\ \hline
            IoT  &  Internet of Things   & LIFO  &  Last-In-First-Out    \\ \hline
            MFC  &  MultiFog-Cloud  & FCFS  &  First Come, Fist Served  \\ \hline
            QoE  &  Quality of Experience  & ~ & ~ \\ \hline
        \end{tabular}\label{table:acrnm}	
\end{table}


\section{Related Works}\label{sec:relWorks}
In this section, the recent related research articles are reviewed and presented taking different aspects into account. The resource management problem of fog and cloud computing is addressed in general, such as in \cite{tran-dang_frato_2021,kumar_dehury_efficient_2020,MISHRA2019217_2019_j7, 8456519_2018_j4, LI202224, 8014362}. 


\subsection{Resource management in fog-cloud}
    %########## [comp1] [comp2] [comp3]
Authors in \cite{comp1, comp2} address the resource allocation problem with the objective to reduce the computation energy and the delay. The proposed computation offloading scheme in \cite{comp1} allows multiple end-users to transmit the data to the same nearby FN. 
% The scheme allocates the resources without compromising the transmission latency and aims at minimizing the energy cost. 
However, the proposed algorithm may not be suitable when the cloud and fog environments are combined.
Similarly, in \cite{comp2} the game theory approach is followed to allocate the fog-cloud resources among IoT users with the goal to maximize the number of users, energy cost and the overall delay. The same resource allocation problem can also be seen and approached by using priced timed Petri nets method, as in \cite{comp3}, which predicts the task completion time and proactively allocate the resource in a dynamic manner considering both the monetary cost and task completion cost. The major pitfall of above resource allocation mechanisms is their inability to handle multi fog scenario. 
At a higher level of resource management problem, authors in \cite{8449762_2018_c1} proposed a resource identity management strategy considering fog and cloud computing environment. This would help the system administrator to manage not only the cloud resource but also the fog resources that are provided by the end-users. The limitation with this approach lies within single fog to cloud communication, which may not fit to the today's scenarios.
%############# [8449762_2018_c1] [SOUZA20181_2018_j5] [8456519_2018_j4] [MAHMUD2018_2018_j7]:QoE

The proposed placement strategy in \cite{SOUZA20181_2018_j5} uses First-Fit approach to find the suitable FN for each parallel service modules. %This would allow the parallel modules to be scheduled and executed concurrently in different computing environments. 
Extending the proposed scheme to fit the multi-fog scenario could further increase the concurrency of the service execution. Similarly, in \cite{MAHMUD2018_2018_j7}, authors proposed the application placement strategy in fog computing environment taking the Quality of Experience (QoE) into account. The proposed mechanism prioritizes the users' applications based on their requirements, intention,  execution platform and other users' expectation parameters, which may slightly deviate from the QoS parameters. Authors in \cite{8456519_2018_j4} investigated the resource allocation problem from the security point of view and proposed privacy-preserving resource allocation mechanisms for fog environments. The proposed mechanism mainly revolves around the fog computing environment and may not give efficient result in case of cloud environment. 

        %######      [8399557_2018_j3] [8556474_2019_j3] Task scheduling
Introducing the fog computing environment between the industrial cloud and the terminal devices, authors in \cite{8399557_2018_j3} proposed a task scheduling algorithm, that ensures the task completion time and optimizes the execution concurrency. However, the proposed algorithm lacks the ability to utilize the computing resource available at the terminal of the nearby fog devices. % As a result, this leaves a room for potentially further improvement. 
Similarly, considering the heterogeneous FNs, a scalable and decentralized scheduling algorithm is presented by authors in \cite{8556474_2019_j3} with the goal to minimize the service delay by efficiently managing computation and communication resources of FNs. 


\subsection{Energy optimization}
     %####       [8673721_2019_j1] [ADHIKARI2019100053_2019_j4] energy specific
In order to offload the computation, while minimizing the energy consumption, authors in \cite{8673721_2019_j1, ADHIKARI2019100053_2019_j4} proposed different strategies, that mitigate and handle the problem of growing resource demand of IoT users. In \cite{8673721_2019_j1}, authors optimize the three major parameters: transmission power of the diverse application, computation resource distribution among those application and offloading decision. Similarly, authors in \cite{ADHIKARI2019100053_2019_j4} optimize two QoS parameters: energy consumption and computational time in sustainable fog-cloud infrastructure. To meet the requirement of multi-fog and cloud environment the above proposed mechanisms need to be extended significantly.

    %######       [8437204_2018_j1] [8240666_2018_j2] Cost efficiency
Based on the cost efficiency, authors in \cite{8437204_2018_j1} investigated the cloud and fog resource management problem. The problem is formulated and presented in a three-layer computing environment. A double two-sided matching optimization model is designed keeping the high cost-efficiency performance in mind. Similarly, with the min-max fairness guarantee, authors in \cite{8240666_2018_j2} proposed a sub-optimal resource allocation strategy that offloads the computational task to minimize the energy consumption and delay cost. The strategy is to divide the entire problem into offloading decision-making problem and resource allocation problem, which are solved by semidefinite relaxation and randomization method and Lagrangian dual decomposition method, respectively. 










%############## [KIM201876_2018_j6] [SUN2017687_2017_j1] : UserIncentive
%Authors in \cite{KIM201876_2018_j6} investigated the possibility of making fog computing infrastructure more flexible and open to others to be a part of fog. In the proposed user incentive model, users are allowed to invest their computing resources and make the fog computing more powerful. Based on tendencies, conditions, types of incentive and other parameters, users are classified into four categories. Container technology is used as the underlined virtualization technology for efficient resource utilization. Similarly, authors in \cite{SUN2017687_2017_j1} proposed the model that allows the users to integrate the spare computing resources to the existing nearby fog computing environment. Authors included an incentive mechanism in the resource sharing crowd-funding algorithm, that would not only reduce the service latency of the fog environment but also reduce the SLA violation. \rone{However, to fit the real world requirement, the proposed algorithm may need significant improvement to be applicable to the multi-fog scenario.}


%############## [LUO201950_2019_j6]: energy-scheduling [MISHRA2019217_2019_j7] [ZHAO2019591_2019_j8]

%In the environment of multiple fogs and multiple clouds, authors in \cite{LUO201950_2019_j6} proposed container-based scheduling algorithm that maximizes the resource utilization and reduces the service delay. The proposed real-time task scheduling algorithm considers and balances the transmission energy consumption of the terminal devices. The multi-criteria decision-making problem on resource selection and allocation in fog computing environment are investigated by authors in \cite{MISHRA2019217_2019_j7}. The problem of migrating the Service Function Chain (SFC) from one fog to other fog when the user moves is investigated in \cite{ZHAO2019591_2019_j8}. Authors proposed the mobile-aware SFC migration strategies that focus on minimizing the number of virtual network function migration, reconfiguration cost, migration time without affecting much on the on-going service delivery process. Upon achieving the above-mentioned goal, fog-cloud service provider would be able to reduce the workload on the network and minimize the utilization of network bandwidth due to service migration. 


\subsection{Real-world application specific}
        %#####      APPLICATION SPECIFIC
 The resource management problem is also addressed considering diverse practical real-life applications such as vehicular network \cite{07448886_2019_j2, comp1}, smart grid \cite{8616989_2018_c2, aranda_context-aware_2022}, smart Buildings \cite{8450331_2018_c3, 8450422_2018_c4}, smart manufacturing \cite{8399557_2018_j3, ROSENDO202271}, smart city \cite{WANG201911_2019_j5, ALAMGIRHOSSAIN2018226}. Authors in \cite{07448886_2019_j2} presented an adaptive resource management algorithm for vehicular networks with the goal to minimize the transmission rate, delay-jitter and the upper-bound of delay. A model for the integration of fog and cloud with smart grid is presented in \cite{8616989_2018_c2}, where the data flow and the request forwarding for electricity to micro-grid are handled by FNs. Fog and cloud computing environments are used for management of the smart building resources through different load balancing algorithms in \cite{8450331_2018_c3, 8450422_2018_c4, 8450410_2018_c5,8449762_2018_c1}. From the application point of view, authors in \cite{WANG201911_2019_j5} show how the smart city resources can be managed by taking advantage of fog computing. The fog computing model acts as the buffer and controller between cyber-physical world and the cloud computing environment with the goal to reduce the coupling in computing and maximize the utilization of resources. 



\section{System Model}
\label{sec:symodel}

The users' request consists of multiple inter-connected tasks. An example of user's request consisting of six tasks is shown in Figure \ref{fig:userApp}. Each task is further associated with a certain amount of resource demand, fulfilled by the servers at fog and cloud environments. Figure \ref{fig:mfcenv} shows the  proposed hierarchy of users base at the bottom, FNs and cloud environment. 
% In the proposed hierarchy, users at the bottom of the tree requesting for the service from the fog and the cloud environment. 
Users access the services from the nearby FNs at the fog layer. For example, \emph{User1} and \emph{User2} access the requested service from the FN \emph{F1}. Between the fog layer and the cloud environment, a Fog-Cloud Interface (FCI) is introduced. FCI is responsible for (1) establishing a communication bridge between FNs and cloud and (2) establishing communication interface among FNs. It is also assumed that the FCIs may be interconnected among themselves. The communication among the FCIs enables the communication among FNs. As presented in Figure \ref{fig:mfcenv}, FNs that are connected to \emph{FCI1} may communicate with the FNs that are connected to \emph{FCI4} through \emph{FCI2}. This allows the users, connected to FN \emph{F1} under \emph{FCI4}, to access the cloud resource through \emph{FCI2}.

\begin{table}[htb]
\caption{List of key notations and description}
\resizebox{\textwidth}{!}{
% \begin{tabular}{l p{0.8\linewidth}}
\footnotesize
\begin{tabular}{l l}
	\hline 
	\textbf{Notation} & \textbf{Description} \\ 
	\hline
	$G$& $=\left( V, E\right) $ Resource graph  \\ 
	
	$V$ & $=\{C, F_1, F_2, \dots \}$ in MFC environment \\ 
	
	$C$ & Set of servers in cloud environment  $ = \{s_c^1, s_c^2, s_c^3, .., s_c^{n_c}\}$ \\
	
	$F_i$ & FN $i$ in MFC environment\\
	$s^i_{f_j}$ & Server $i$ in FN $F_j$ \\
%	$F$ & Set of servers at all Fog nodes $ = \{s_f^1, s_f^2, s_f^3, .., s_f^{n_f}\}$  \\
	
%	$E$& \{Set of edges among all servers\} \\ 
%	\hline 
%	$M$ & $\#$ of fog nodes \\ 
%	\hline 
%	$s_c^i$ & $i^{th}$ server in cloud environment\\ 
%	\hline 
%	$s_f^j$ & $j^{th}$ server in fog environment \\ 
	
%	$n_c$& Number of servers in cloud environment \\ 
%	$n_f$ & Number of servers in fog environment\\ 
	$\vec{P}_{ij}$ & Physical path between server $s^{m_i} \in F_k$ and $s^{m_j} \in F_l, k \ne l$ \\
	
	$\vec{H}_{ij}$ & Number of hops present in between the server $s^{m_i} \in F_k$ and $s^{m_j} \in F_l, k \ne l$ \\
%	$f_i$ & $i^{th}$ FN \\ 
%	\hline 
	$R^x(s^{m_1})$&  Remaining resource of type $x$ available at server $s^{m_1} \in V$  \\ 
	
	$\alpha(s^{m_1},s^{m_2})$ & Available bandwidth available between server $s^{m_1} \in V$ and $s^{m_2} \in V$ \\
	
	$\beta(s^{m_1},s^{m_2})$ & Network latency on the physical link between server  $s^{m_1} \in V$ and $s^{m_2} \in V$ \\
	
	$G'$& $=\left( V', E'\right) $ User's application \\ 
	$V'$ & Set of tasks in user's application. $V'= \{v_1,v_2, ..., v_{n'}\}$ \\
	$E'$ & Set of edges between users' tasks \\
	$\hat{e}(v_i,v_j)$ & The boolean variable indicating if there is an edge existing from task $v_i$ to task $v_j$ \\
%	$\gamma_{c,j}^i$ & The boolean variable indicating if task $v_i$ is placed onto cloud server $s_c^j$ \\
%	$\gamma_{f_j,k}^i$ & The boolean variable indicating if task $v_i$ is placed onto fog server $s_{f_j}^{k}$ in MFC environment\\
%	$\hat{\gamma}_{f_j,k}^i$ & The boolean variable indicating if task $v_i$ is placed onto the nearby fog server $s_{f_j}^{k}$ in MFC environment \\
	$\bar{R}^x(v_i)$& Resource demand of type $x$ by task $v_i \in V'$  \\ 
	$\alpha'(v_i,v_j)$ & Bandwidth demand between  task $v_i$ and $v_j$ \\
	$\beta'(v_i,v_j)$ & Maximum network latency allowed from task $v_i$ to $v_j$ \\
	$\vec{\beta}(s^{m_i}, s^{m_j})$ & Network latency between the server $s^{m_i} \in F_k$ and $s^{m_j} \in F_l, k\ne l$ in MFC environment\\
	$\kappa$ & The mean network latency between the cloud environment and the fog environment \\ 
	 $P_i$ & Priority of a task $v_i$ \\
	 $M_i$ & Makespan of a task $v_i$\\
	 $\hat{M}_i$ & Normalized makespan of a task $v_i$ \\
	 $\hat{P}_i$ & Normalized Priority of a task $v_i$ \\
	 $\hat{R}_i$ & Normalized computing resource demand of a task $v_i$ \\
	\hline 
\end{tabular} \label{table:notation}
}
\end{table}

The placement and resource allocation strategy follows a bottom-up approach. As a result, the users' tasks are first forwarded to the fog environment. %, which comprises of limited number of high-end servers. 
If the fog resources are not enough to fulfill the demand, the users' tasks are further forwarded to cloud. %, which comprises of a large number of high-end physical servers. 
%Since the fog environment is very near to the users' location,
Due to the proximity the network latency between user and fog server is very less compared to the network latency between user and cloud. As a result, user can experience the real-time performance of different services, for instance services such as online gaming service, online video analysis service which depends on huge data exchange between user device and cloud or FN, etc.

The relationship among the set of cloud servers, fog servers, and the users can be represented as a graph structure. The list of notations that are used through-out the paper including the problem models in Section \ref{sec:probFormulation} and \ref{sec:MFCmodel} is given in Table \ref{table:notation}.
%, where users act as the leaf nodes, fog servers act as parent of leaf nodes and cloud act as the root of the tree.  





\subsection{Resource Graph}

\begin{figure}[t]
	\centering
	\subfloat[Resource graph consisting of one FN and one cloud environments.]
	{
		\includegraphics[width=0.30\linewidth]{ResourceGraph-1Fog-1Cloud.pdf}
		\label{fig:resource_graph:1fog-1cloud}
	}\hspace{10mm} %\hfill
	\subfloat[Resource graph consisting of two FNs and one cloud environments]
	{
		\includegraphics[width=0.30\linewidth]{ResourceGraph-2Fog-1Cloud.pdf}
		\label{fig:resource_graph:2fogs-1cloud}
	}
	\caption{An example of resource graph.} \vspace{-4mm}
	\label{fig:resource_graph}
\end{figure}

The resources of both cloud and fog servers are represented in a graph structure, known as resource graph. Let $G=\left( V, E\right) $ be the resource graph, where $V = \{C, F\}$ be the set of servers available in both Cloud and Fog environment and $E$ be the set of edges among those servers. An example of such resource graph can be seen in Figure \ref{fig:resource_graph:1fog-1cloud}. Further, $C=\{ s_c^1, s_c^2, s_c^3, .., s_c^{n_c} \}$ be the set of servers available in cloud environment and 
$F = \{s_f^1, s_f^2, s_f^3, .., s_f^{n_f}\}$
%$F=\{ s_f^{1 1}, s_f^{2 1}, .., s_f^{n_{f_1} 1}, s_f^{1 2}, s_f^{2 2}, .., s_f^{n_{f_2} 2}, .., s_f^{1 M}, s_f^{2 M}, .., s_f^{n_{f_M} M} \}$ 
be the servers available in nearby fog environment. In Figure \ref{fig:resource_graph:1fog-1cloud}, the cloud environment consisting of five servers, $C=\{s_c^1, s_c^2, s_c^3, s_c^4, s_c^5\}$, and the fog environment consisting of three servers, $F = \{s_f^1, s_f^2, s_f^3\}$. For each server $s^{m_1} \in V$, $R^x(s^{m_1})$ represents the remaining resource available of type $x \in \{CPU, Memory\}$.  
%% about edges and network latency
%$e(s^{m_1},s^{m_2})$ indicates if the physical edge between server $s^{m_1}$ and server $s^{m_2}$ exists, as in Equation \ref{eq:phyEdge}.
%\begin{equation} \label{eq:phyEdge}
%e(s^{m_1},s^{m_2}) = \left\lbrace 
%\begin{array}{ll}
% 1& \text{if physical edge exists between server}\\
% & s^{m_1} \text{and } s^{m_2},\quad s^{m_1},s^{m_2} \in C\text{ or }s^{m_1},s^{m_2} \in F  \\ 
%0& \text{Otherwise}
%\end{array} \right.
%\end{equation} 
Available bandwidth and network latency are the two parameters that are associated with each physical edge $e(s^{m_1},s^{m_2})$. Let $\alpha(s^{m_1},s^{m_2})$ and $\beta(s^{m_1},s^{m_2})$ be the available bandwidth and network latency, respectively, on physical link between server $s^{m_1}$ and $s^{m_2}$, $(s^{m_1},s^{m_2}) \in C$ or $(s^{m_1},s^{m_2}) \in F$. The mean network latency between the cloud and the fog environment is represented by $\kappa$. Since the network latency is assumed to be time-variant, the mean value is always considered during the placement of the tasks. 
Further, it is assumed that there exists a physical link between each pair of fog servers. For example the servers in Figure \ref{fig:resource_graph:1fog-1cloud} are connected directly without any FCI in-between. Mathematically, $e(s^{m_1},s^{m_2}) = 1, \forall (s^{m_1},s^{m_2}) \in F$. In other words, the fog servers are connected to each other in a complete graph. The other form of connection i.e. incomplete graph fog servers is discussed in Section \ref{sec:MFCmodel}, where it is considered as each pair of fog servers may not be connected directly, as the servers may belong to different fog environments in different geographical locations (e.g. in Figure \ref{fig:resource_graph:2fogs-1cloud}). 





\subsection{Task Graph}

The users' requests are assumed to be comprised of multiple inter-connected tasks, represented as Directed Acyclic Graph (DAG), $G'=(V', E')$. An example of user's application is presented in Figure \ref{fig:userApp}, which consists of $6$ tasks, $V1 - V6$. Let $V'=\{v_1, v_2, ..., v_{n'}\}$ be the set of tasks and $E'$ be the set of edges among tasks. Each task $v_i$ is associated with resource demand of type CPU and memory, denoted by $\bar{R}^x(v_i), x \in \{CPU, Memory\}$. Let $\hat{e}(v_i,v_j)$ be the boolean variable indicating if there is an edge existing from task $v_i$ to task $v_j$. The direction of the edge also represents the dependencies of task $v_j$ upon the task $v_i$. This dependency indicates that the dependent task $v_j$ cannot start its execution before task $v_i$, as task $v_j$ needs input from task $v_i$. Since the tasks graph is modelled as DAG, the value of $\hat{e}(v_i,v_j)+\hat{e}(v_j,v_i) \le 1$. 
% \begin{equation}
% \hat{e}(v_i,v_j) = \left\lbrace \begin{array}{ll}
% 1 & \text{if edge from task }v_i\text{ to }v_j\text{ exist}  \\ 
% 0 & \text{Otherwise}
% \end{array} \right.
% \end{equation}

Taking the network resource demand into account, let $\alpha'(v_i,v_j)$ be the bandwidth demand between the task $v_i$ and $v_j$. In order to establish the smooth communication among the tasks, let $\beta'(v_i,v_j)$ be the maximum network latency allowed from task $v_i$ to $v_j$.



\section{Problem formulation } 
\label{sec:probFormulation}
Considering the resource graph and the task graph, the main goal is to map or place the interconnected tasks onto both cloud and fog servers such that the workload distribution among the physical servers can be balanced and the network latency can be minimized. 
% The hardness of this problem, which is NP-hard, can be considered as same to our previous work \cite{MAHMUD2018_2018_j7} on application placement problem in fog-cloud environment, which is also NP-hard in nature. 
The fog servers are given higher preference over the cloud servers while mapping process. The mapping process comprises of two stages: Task mapping and task edge mapping. In task mapping stage, all the tasks are mapped onto multiple servers. Here, we exploit the opportunity to map multiple tasks from same task graph onto the same physical servers. As a result; the number of physical servers involved is less than the number of tasks present in the task graph. In the task edge mapping stage, all the edges between the tasks are mapped onto the physical edges considering the network bandwidth and the network latency. %Both the stages are discussed in detail in following sub-sections.  

%\subsection{Task mapping}
%In order to indicate if a task $v_i \in V'$ is mapped onto a cloud server $s_c^j \in C$, $\gamma_{c,j}^i$ is used as a boolean variable. Mathematically,
%% boolean variable if the task is placed in cloud server
%\begin{equation}
%	\gamma_{c,j}^i = \left\lbrace 
%	\begin{array}{ll}
%	1& \text{if task }v_i\text{ is placed onto cloud server }s_c^j  \\ 
%	0& \text{Otherwise}
%	\end{array} \right.
%\end{equation}
%
%Similarly, $\gamma_{f,k}^i$ be the boolean variable indicating if a task $v_i \in V'$ is mapped onto the fog environment. Mathematically,
%% boolean variable if the task is placed in cloud server
%\begin{equation}
%\gamma_{f,k}^i = \left\lbrace 
%\begin{array}{ll}
%1& \text{if task }v_i\text{ is placed onto fog server }s_f^{k}  \\ 
%0& \text{Otherwise}
%\end{array} \right.
%\end{equation}

\subsection{Task-edge mapping}
The boolean variable $\gamma_{c,j}^i = 1$, if the task $v_i$ is placed onto cloud server $s_c^j$, $0$ otherwise. Similarly, $\gamma_{f,k}^i = 1$ if the task $v_i$ is placed onto fog server $s_f^{k}$, $0$ otherwise. 

The boolean variable $D_{m_1m_2}^{i_1i_2}$ represents if the edge between task $v_{i_1}$ and $v_{i_2}$ is mapped onto the physical edge among cloud and fog servers $s^{m_1} \in V$ and $s^{m_2} \in V$. When a task edge needs to be mapped onto the physical edge between one cloud server and one fog server, an additional network latency $\kappa$ would be considered. In order to differentiate the set of fog servers and the cloud servers, the value of $\beta(s_c^{m_1},s_f^{m_2})= \kappa, \forall s_c^{m_1} \in C, \forall s_f^{m_2} \in F$ must be larger than the network latency between any two cloud or fog servers. Mathematically,

\begin{equation}
\kappa > \max_{0 < m_1, m_2 \le n_c}\left\lbrace \beta(s_c^{m_1},s_c^{m_2})\right\rbrace  
\end{equation}
\begin{equation}
\kappa > \max_{0 < m_1, m_2 \le n_f}\left\lbrace \beta(s_f^{m_1},s_f^{m_2})\right\rbrace 
\end{equation}

%% Assumption paragraph goes here... (( C2.2 ))
A precise summary of the assumptions considered in the context of single FN and cloud environment can be made as follows. A user's request is assumed to be comprised of multiple inter-connected tasks and hence no task is isolated from the other tasks in the same request. The fog servers within a FN are connected and there exists a physical link between each pair of fog servers. It is assumed that FCIs can communicate among themselves either over wired or wireless network connection. The network connection in the whole multiFog-cloud environment is time-variant.
% Key assumptions in System Model section:
% =============================================
% - It is also assumed that the FCIs may be interconnected among themselves.
% - Since the network latency is assumed to be time-variant, the mean value is always considered while placing the tasks onto the cloud and fog environments. 
% - Further, it is assumed that there exists a physical link between each pair of fog servers.
% - The users' request is assumed to be comprised of multiple inter-connected tasks

\subsection{Objective function}\label{sec:obj:1F1C}
% regarding load balancing and network latency
The problem of mapping the users' interconnected tasks to cloud and fog environment can be formulated as mixed integer linear programming problem. The main objectives are to balance the workload by uniform distribution of the tasks and minimize the network latency. 

Hence, the main objective can be divided into two sub-objectives: mapping the tasks to the servers and virtual edges to the physical edges. The following equation represents the assignment of a task $v_{i_1} \in V'$ onto a fog or cloud server.
\begin{equation}\label{eq:obj:task}
	z^{v_{i_1}} = \sum_{\forall s^{m_1} \in V}\left( \gamma_{c,m_1}^{i_1} + \Delta * \gamma_{f,m_1}^{i_1}\right)* \frac{1}{R^x(s^{m_1})}
\end{equation}
Similarly, the mapping of a virtual edge $\hat{e}(v_{i_1},v_{i_2}) \in E'$ onto a physical edge can be represented as follows:
\begin{equation}\label{eq:obj:edge}
	z^{(i_1i_2)} = \sum_{\forall(s^{m_1},s^{m_2}) \in E} D_{m_1m_2}^{i_1i_2} * \hat{e}(v_{i_1},v_{i_2}) * \left[\beta(s^{m_1},s^{m_2}) + \frac{1}{\alpha(s^{m_1},s^{m_2})}\right]
\end{equation}

Using the Equation \ref{eq:obj:task} for task mapping and Equation \ref{eq:obj:edge} for virtual edge mapping, the primary objective function for the user's request $G'$ can be formulated as below.

\textbf{Objective:}
\begin{equation} \label{eq:objfun}
 \min Z = \sum_{\forall v_{i_1} \in V'}  z^{v_{i_1}} + \sum_{\forall (v_{i_1},v_{i_2}) \in E'} z^{(i_1i_2)}
\end{equation}

\textbf{Constraint:}
\begin{equation}\label{const:cloudORfogServer}
\gamma_{c,j}^i + \gamma_{f,k}^i = 1, \quad \forall s_c^j \in C , \forall s_f^{k} \in F
\end{equation}
\begin{equation} \label{const:resrcCnstrnt}
	\bar{R}^x(v_i) < R^x(s_c^j) * \gamma_{c,j}^i + R^x(s_f^{k}) * \gamma_{f,k}^i
\end{equation}
\begin{equation}\label{const:BwCnstrnt}
\alpha(s^{m_1},s^{m_2}) > \alpha'(v_{i_1},v_{i_2})
\end{equation}
\begin{equation}\label{const:latencyCnstrnt}
\beta(s^{m_1},s^{m_2}) < \beta'(v_{i_1},v_{i_2})
\end{equation}
% Multiplied because, we want to give higher preference to fog servers.
% \begin{equation}\label{const:fogServerCnsnt}
% 0 < \Delta < 1
% \end{equation}
\begin{equation}\label{const:fogServerCnsnt} %\label{const:basic}
0 < \Delta < 1; \quad 0 < i_1, i_2 \le n_c; \quad 0< m_1, m_2 \le n_f
\end{equation}

The objective function is of two folds: 
\begin{itemize}
	\item The first part of the function allows all the tasks to be mapped onto the physical servers available at nearby FN and cloud. $\Delta$ is used as a constant to encourage the tasks to be mapped in fog servers instead of cloud servers. Since, the objective is to minimize, the task will be mapped to the servers having maximum remaining resource. Further, if two servers (one at FN and other at cloud) having minimum resources, the task will be mapped to the fog server due the multiplication of $\Delta$ constant. 
	\item The second part of the function (derived from Equation \ref{eq:obj:edge}) minimizes network latency while mapping the virtual edges to the physical edges. $\beta(s^{m_1},s^{m_2})$ in Equation \ref{eq:obj:edge} imposes additional network latency when two servers, $s^{m_1}\text{ and }s^{m_2}$, are in different computing environments. As a result, this discourages using any physical connection between cloud and fog as this would give a larger value of $\beta$. Minimizing the term $\frac{1}{\alpha(s^{m_1},s^{m_2})}$ in Equation \ref{eq:obj:edge} infers that the physical edges with maximum bandwidth available are highly preferred for any virtual edge. 
\end{itemize} 
However, the solution must meet the following constraints.

\begin{itemize}
	\item Constraint (\ref{const:cloudORfogServer}) ensures that all the tasks are mapped onto exactly one server. The physical server must either be cloud server or fog server. 
	\item Constraint (\ref{const:resrcCnstrnt}) and (\ref{const:BwCnstrnt}), ensure that the tasks and the edges between the tasks are mapped onto the physical servers and physical edges with enough remain resources available. 
	\item Similarly, Constraint (\ref{const:latencyCnstrnt}) ensures that the required network latency is fulfilled by the physical edge. On the other hand, Constraint (\ref{const:fogServerCnsnt}) ensures that the value of $\Delta$ must lies between $1$ and $0$ to encourage the tasks to be mapped onto the fog server. 
\end{itemize}



\section{MultiFog-Cloud (MFC) Environment} \label{sec:MFCmodel}

In Section \ref{sec:probFormulation}, the system consists of one cloud and one FN. For each user, the fog servers are connected to each other forming a complete graph. However, in order to fit into the real-scenario, a MultiFog-Cloud (MFC) environment is considered, as shown in Figure \ref{fig:mfcenv}, where more than one FNs are deployed in different geographical locations. All the FNs are directly or indirectly connected to each other. As a result, the fog servers in all FNs are forming an incomplete graph and no fog server is unreachable from any other fog server.  

\subsection{Multi-Fog resource graph}
As the system model, in Section \ref{sec:symodel}, is updated, the resource graph can be represented as $V = \{C, F_1, F_2, ....\}$. For example, in Figure \ref{fig:resource_graph:2fogs-1cloud}, two fog environments: $f_1$ consisting of three servers and $f_2$ consisting of two servers are connected to the cloud environment with five servers. Each fog consists of multiple servers represented as $F_i = \{s_{f_i}^1, s_{f_i}^2, s_{f_i}^3, ...\}$. Each server is represented as $s_{f_i}^j$, which indicates server $j$ in FN $F_i$. $|F_i|$ represents the number of servers present in FN $F_i$. It is assumed that the number of servers in all FNs are not uniform. In order to get the location of a server $s^{m_j}$, the notation $L(s^{m_j})$ is used. As the resource graph is not complete, the term physical edge is different from physical path. Physical edge can be defined as the direct link between two servers. 
%PHYSICAL PATH definition
On the other hand, a physical path is the combination of multiple physical links connected to each other to transfer data from one end to other ends. A physical link can be between FN and FCI, two FCIs, between FCI and cloud. 
%On the other hand, physical path is comprised of multiple connected physical links. Both the end of a physical path can not be same. 
The term physical path is used when establishing a connection between two servers from different FNs. The notation $\vec{P}_{ij}$ is used to indicate the physical path between server $s^{m_i} \in F_k$ and $s^{m_j} \in F_l, k\ne l$. If both the servers belongs to same node, the path $\vec{P}_{ij}$ is same to the notation $e(s^{m_i}, s^{m_j})$. $\vec{H}_{ij}$ indicates the number of hops present in between the server $s^{m_i} \in F_k$ and server $s^{m_j} \in F_l, k\ne l$.  $E$ represents the set of all physical edges and physical paths present in the multi-fog and cloud environment. $\vec{\beta}(s^{m_i}, s^{m_j})$ represents the network latency between the server $s^{m_i} \in F_k$ and $s^{m_j} \in F_l, k\ne l$. It is assumed that network latency of any physical link is smaller than the network latency of any path. Mathematically,
\begin{align}\label{eq:ltncyCnstrnt}
\nonumber \max\{\beta(s^{m_i}, s^{m_j}) | \forall F_k, s^{m_i} \text{and} s^{m_j} \in F_k\} < \\
\min\{\vec{\beta}(s^{m_i}, s^{m_j}) | \forall F_k,F_j, s^{m_i}\in F_k, s^{m_j} \in F_l \}
\end{align}
There is no relationship between network latency of a path between FN and cloud and the network latency of fog paths. 


\subsection{Resource mapping in multi-fog environment}
Considering the new multi-fog cloud model, the complexity of resource mapping problem leverages when the mapping scope stretched from single-fog single-cloud scenario to multi-fog single-cloud scenario. The user's tasks can be mapped onto nearby fog, remote fog or remote cloud computing environment. During mapping, multiple parameters need to be taken into account, such as the number of hops, network latency, workload distribution, etc. 
The boolean variable $\gamma^i_{f_j,k} = 1$, if the task is placed onto the server $k$ in the FN (except the nearby FN) $F_j$, $0$ otherwise.   
%The boolean variable $\gamma^i_{f_j,k}$ represents if the task is placed onto the server $k$ in the FN $F_j$. Mathematically,
%\begin{equation}
%\gamma_{f_j,k}^i = \left\lbrace 
%\begin{array}{ll}
%1& \text{if task }v_i\text{ is placed onto fog server }s_{f_j}^{k}  \\ 
%0& \text{Otherwise}
%\end{array} \right.
%\end{equation}
However, the boolean variable $\hat{\gamma}_{f_j,k}^i = 1$, if task $v_i$ is placed onto a nearby fog server $s_{f_j}^{k}$, $0$ otherwise.
%Similarly, to represent if the task is mapped onto the nearby FN, the notation $\hat{\gamma}_{f_j,k}^i$ is used, where FN $F_j$ is the nearby FN. Mathematically,
%\begin{equation}
%\hat{\gamma}_{f_j,k}^i = \left\lbrace 
%\begin{array}{ll}
%1& \text{if task }v_i\text{ is placed onto the nearby fog server }s_{f_j}^{k}.  \\ 
%0& \text{Otherwise}
%\end{array} \right.
%\end{equation}

The task edges can be mapped onto the physical edge of physical path. In order to achieve the goal of minimizing network latency, the physical edges are given higher preferences over physical path while mapping the task edge. The boolean variable $D_{m_km_l}^{ij}$ represents if the task edge between task $v_i$ and $v_j$ is mapped onto the physical edge $e(s^{m_k}, s^{m_l})$ or the physical path $\vec{P}_{kl}$. 

%% Key Assumption paragraph for MultiFog cloud goes here... (( C2.2 ))
A precise summary of the key assumptions considered in the context of multi-fog and cloud environment (in addition to the key assumptions in Section \ref{sec:symodel}) can be made as follows. The resource configurations/capacities in all FNs are heterogeneous. % This indicates that the resource capacity of FN $FN1$ is different from the resource capacity of FN $FN2$. 
It is also assumed that the network latency of any physical link is smaller than the network latency of any path. Hence, the latency value between any FN and the corresponding FCI is smaller than the latency value among FCIs and between FCIs and the cloud. The proposed work assumed that the user assigns a priority to each task. When tasks are being scheduled, the effect of resource availability fluctuation is negligible.
% MultiFog-Cloud (MFC) Environment:
% ====================
% - It is assumed that the number of servers in all FNs are not uniform.
% - It is assumed that network latency of any physical link is smaller than the network latency of any path.
% - Priority of a task is decided by the user
% - It is assumed that latency value between any FN and the corresponding FCI is small than the latency value among FCIs and between FCIs and cloud.
% - Fluctuation of Resource availability does not affect the algorithm, because the algorithm schedule the applications/tasks taking very negligible amount of time.



\subsection{Objective function}\label{sec:obj:multiF1C}

Extending the objective function in Section \ref{sec:obj:1F1C}, the problem of mapping the user's task graph onto multi-fog and cloud environment can be formulated as mixed integer linear programming problem with the goal to minimize the total network latency and distribute the workload among fog and cloud servers in a balanced manner. The tasks are mapped in such a way that total network latency among tasks is minimum without violating the computing resource demand.

From Equation \ref{eq:objfun}, the mathematical formulation of single task $v_{i_1} \in V'$ mapping onto a physical server available at multi-fog single-cloud environment can be represented as 
\begin{equation}\label{eq:obj:MF1C:task}
	\hat{z}^{v_{i_1}} = \sum_{\forall s^{m_1} \in V} \frac{\left( \gamma_{c,m_1}^{i_1} +  \gamma_{f_j,m_1}^{i_1} + \Delta*\hat{\gamma}_{f_k,m_1}^{i_1} \right) 
	}{R^x(s^{m_1})}
\end{equation}

For further simplification, mapping virtual to physical edges is associated with two objectives: minimizing the network latency and balancing the network workload. While mapping a virtual edge, $(v_{i_1},v_{i_2})$, to a physical edge, the network latency can be calculated as below.
\begin{align}\label{eq:obj:MF1C:edge:latency}
	\nonumber \hat{z}^{(i_1i_2)}_{latency} = \sum_{\forall(s^{m_1},s^{m_2}) \in E} \left\lbrace  \hat{e}(v_{i_1},v_{i_2}) *  \left( D_{m_1m_2}^{i_1i_2} * \right.\right.\\
	\left. \left. \beta(s^{m_1},s^{m_2}) +   D_{m_1m_2}^{i_1i_2} * \vec{\beta}(s^{m_1},s^{m_2}) + \vec{H}_{s^{m_1},s^{m_2}} \right) \right\rbrace
\end{align}
Similarly, the mathematical formulation for mapping a virtual edge, $(v_{i_1},v_{i_2})$, onto a physical edge taking bandwidth availability into account can be represented as, 
\begin{equation}\label{eq:obj:MF1C:edge:bandwidth}
	\hat{z}^{(i_1i_2)}_{bandwidth} = \sum_{\forall(s^{m_1},s^{m_2}) \in E} \frac{D_{m_1m_2}^{i_1i_2} * \hat{e}(v_{i_1},v_{i_2})}{\alpha(s^{m_1},s^{m_2})}
\end{equation}

Considering the Equations \ref{eq:obj:MF1C:task}, \ref{eq:obj:MF1C:edge:latency}, and \ref{eq:obj:MF1C:edge:bandwidth}, the previous objective function in Equation \ref{eq:objfun} can further be modified as follows.\\
\textbf{Objective:}\\
\begin{equation}\label{eq:objfun2}
\min \hat{Z} = \sum_{\forall v_{i_1} \in V'} \hat{z}^{v_{i_1}} + \sum_{\forall (v_{i_1},v_{i_2}) \in E'}   \left[ \hat{z}^{(i_1i_2)}_{latency} + \hat{z}^{(i_1i_2)}_{bandwidth} \right]
\end{equation}
%\begin{align}\label{eq:objfun2}
%\nonumber \min \sum_{\forall v_{i_1} \in V'} \sum_{\forall s^{m_1} \in V} \left( \gamma_{c,m_1}^{i_1} +  \gamma_{f_j,m_1}^{i_1} + \Delta*\hat{\gamma}_{f_k,m_1}^{i_1} \right) * \frac{1}{R^x(s^{m_1})} + \\
%\nonumber \sum_{\forall (v_{i_1},v_{i_2}) \in E'} \sum_{\forall(s^{m_1},s^{m_2}) \in E} \left\lbrace  \hat{e}(v_{i_1},v_{i_2}) *  \left( D_{m_1m_2}^{i_1i_2} *  \beta(s^{m_1},s^{m_2}) + \right. \right.\\
%\nonumber \left. \left.
%  D_{m_1m_2}^{i_1i_2} * \vec{\beta}(s^{m_1},s^{m_2}) + \vec{H}_{s^{m_1},s^{m_2}}   \right) \right\rbrace + \\
%\sum_{\forall (v_{i_1},v_{i_2}) \in E'} \sum_{\forall(s^{m_1},s^{m_2}) \in E} D_{m_1m_2}^{i_1i_2} * \hat{e}(v_{i_1},v_{i_2}) * \frac{1}{\alpha(s^{m_1},s^{m_2})}
%\end{align}


\textbf{Constraint:}\\
\begin{equation}\label{const:obj2:oneTaskOneFog}
\gamma_{f_j,m_1}^i + \gamma_{f_k,m_1}^i \le 1
\end{equation}
\begin{equation}\label{const:obj2:nwltncy}
	\beta'(v_1,v_2) \ge \vec{\beta}(s^{m_1}, s^{m_2})
\end{equation}
\begin{equation}\label{const:obj2:atlstOneTaskinOneFog}
\sum_{\forall v_i \in V'}\hat{\gamma}_{f_j,k}^i \ge 1
\end{equation}

In the first part of the function, servers available at nearby FN, other FNs, and the cloud environments are considered while assigning a task. The second part of the objective function focuses on mapping the virtual edges onto the physical edges. If no suitable physical edge is available, a physical path consisting of multiple physical edges will be chosen (details in Section \ref{sec:sol}). The number of hops and the network latency are collectively considered while selecting a physical path. In addition to the constraints mentioned in Equations \ref{const:cloudORfogServer}-\ref{const:fogServerCnsnt}, the objective function in Equation \ref{eq:objfun2} must satisfy following constraints.

\begin{itemize}
	\item Constraint \ref{const:obj2:oneTaskOneFog} ensures that no task is mapped onto multiple FNs. On the other hand, Constraint \ref{const:obj2:nwltncy} ensures that the maximum allowed network latency of any task edge $\hat{e}(v_{i_1},v_{i_2})$ is smaller than the corresponding physical path $\vec{P}_{m_1m_2}$.
	\item Constraint \ref{const:obj2:atlstOneTaskinOneFog} ensure that at least one task is placed in the nearby FN. This would allow the service provider to deploy the most critical task in the task graph onto the nearby FN. This would also prohibit the service provider to place the entire task graph in a very remote FN
\end{itemize}
 



\section{Proposed Algorithm} \label{sec:sol}

In this section, we discuss the proposed novel Heuristic Resource Allocation and optimization in MultiFog-Cloud (HeRAFC) algorithm, which works in a non-distributed manner and the users requests are handled on FCFS manner. Figure \ref{fig:mfcenv} shows the architecture of multiple fogs and cloud and Figure \ref{fig:userApp} shows an example of users' application. As discussed in earlier section, the job of this algorithm is to distribute the application tasks among nearby FN, the FNs at multi-hop distance and the cloud. Each FN consists of multiple interconnected fog servers and network devices such as switches, routers, etc.

\begin{definition}{\textbf{1-hop distance:}}
	A FN $F_i$ is said to be at 1-hop distance from other FNs $\forall F_j, F_i \ne F_j$, if both the FNs, $F_i$ and $F_j$, share same FCI. Taking Figure \ref{fig:mfcenv} into account, for all the servers located in cloud $C$, all the FNs in $FCI1, FCI2,$ and $FCI3$ are at 1-hop distance, but not the FNs that are connected to $FCI4$. Similarly, FN $F_1$ and $F_2$ under $FCI1$ are at 1-hop distance from each other.
%Some of the examples are given below, taking the Figure \ref{fig:mfcenv} into account.
%	\begin{itemize}
%		\item Example-1: For all the servers located in cloud $C$, all the FNs in $FCI1, FCI2,$ and $FCI3$ are at 1-hop distance, but not the FNs that are connected to $FCI4$.
%		\item Example-2: FN $F_1$ and $F_2$ under $FCI1$ are at 1-hop distance from each other.
%	\end{itemize}
\end{definition}
\begin{definition}{\textbf{2-hop distance:}}
	A FN $F_i$ is said to be at 2-hop distance from other FNs $\forall F_j, F_i \ne F_j$, if the corresponding FCIs can communicate directly with each other without cloud. For example, fog $F_1$ under $FCI1$ is at 2-hop distance from all the FNs that are connected to $FCI2$. Additionally, the servers in cloud $C$ and the FNs connected to $FCI4$ are at 2-hop distance from each other.
%	Considering the Figure \ref{fig:mfcenv}, some examples can be summarized as below.
%	\begin{itemize}
%		\item Example-1: The servers in cloud $C$ and the FNs connected to $FCI4$ are at 2-hop distance from each other.
%		\item Example-2: Fog $F_1$ under $FCI1$ is at 2-hop distance from all the FNs that are connected to $FCI2$.
%		\item Example-3: Similarly, FN $F_1$ connected to $FCI4$ is at 2-hop distance from the FN $F_2$ connected to $FCI2$
%	\end{itemize}
\end{definition}
\begin{definition}{\textbf{n-hop distance:}}
	In general, two FNs are said to at n-hop distance, if $n, n > 0$ number of FCIs are involved in the physical path between corresponding FNs.  For example in Figure \ref{fig:mfcenv}, the FN under $FCI4$ is at 3-hop (here $n=3$) distance from the FN $F_2$ under $FCI1$.
\end{definition}

%COMMENT THIS definition
%\begin{definition}{\textbf{Physical path:}}
%	A physical path is the combination of multiple physical links connected to each other to transfer data from one end to other ends. A physical link can be between FN and FCI, two FCIs, between FCI and cloud. 
%\end{definition}

Three major parameters associated with the users' applications are taken into account: makespan of the task, priority value of each task, and computing resource demand that includes CPU and memory demand.
%  major parameters description
\begin{definition}{\textbf{Task makespan:}}
	Makespan of a task $v_i$, $M_i$ refers to the time required to execute the task, also referred to as the task's execution time. This excluded the response time. The makespan of the tasks can be used to derive the makespan of the application. 
\end{definition}

\begin{definition}{\textbf{Priority value of task:}}
	Priority of a task $v_i$, $P_i$, refers to how important the task is. The value of $P_i$ is decided by the user. Higher the value of priority, more important the task is. Mathematically, a task $v_1$ is said to be higher priority than the task $v_2$, if $P_1 > P_2$.
\end{definition}

%  normalized value calculation
Based on above-mentioned three major parameters, the critical value of each task is calculated. This work considers parameters with a wide range of values. For example, makespan of a task is in time unit and the value may range from 1 to several thousands. Similarly, memory resource demand value is in megabytes and the value may range from hundreds to thousands. The value of CPU resource demand may reach up to 100 and the priority of tasks can be of any discrete integer value. For such a diverse range of values, it is necessary to normalize the values to a range of (0, 1], i.e. greater than 0 and less than or equal to 1. Without normalization, a slight increase in one parameter may have a substantial impact on the final result. On the other hand, a significant change in another parameter may result in a slight change in the final result. The normalized makespan of a task is calculated as:
\begin{equation}\label{eq:normMkspn}
\hat{M}_i = \frac{M_i}{\max\{M_i | \forall v_i \in V\}}
\end{equation}

Similarly the normalized priority value of a task can be calculated as 
\begin{equation}\label{eq:normPriority}
\hat{P}_i = \frac{P_i}{\max\{P_i | \forall v_i \in V\}}
\end{equation}
Since computing resource refers to the CPU and memory demand which are in different units, the normalized value of each type of resource demand is calculated followed by calculating the average of both normalized values. Mathematically, the normalized CPU demand can be calculated as:
\begin{equation}\label{eq:normCPU}
\hat{R}^{CPU}(v_i) = \frac{\bar{R}^{CPU}(v_i)}{ \max \{ R^{CPU}(s^{m_1}) | \forall m_1 \in \{ C, F_1, F_2, \dots \}  \}}
\end{equation}
The normalized value of memory demand can be calculated as:
\begin{equation}\label{eq:normMem}
\hat{R}^{mem}(v_i) = \frac{\bar{R}^{mem}(v_i)}{ \max \{ R^{mem}(s^{m_1}) | \forall m_1 \in \{ C, F_1, F_2, \dots \}  \}}
\end{equation}

Taking the value calculated using Equation \ref{eq:normCPU} and \ref{eq:normMem}, the normalized average weighted computing resource demand can be calculated as follows.
\begin{equation}\label{eq:normCompRsrc}
\hat{R}_i = \frac{\Omega^c*\hat{R}^{CPU}(v_i) + \Omega^m\hat{R}^{mem}(v_i)}{2}
\end{equation}
, where $\Omega^c\text{ and }\Omega^m$ are the constants, $\Omega^c + \Omega^m = 1, 0 < \Omega^c,\Omega^m < 1 $. These constants are used to assign preference values to \textit{CPU} and \textit{mem} resource.

%  Critical values of tasks
Considering the normalized values calculated in Equation \ref{eq:normMkspn}-\ref{eq:normCompRsrc}, the critical value of a task is calculated. Weighted Multi-Dimensional (WMD) approach is followed, where each parameter represents one dimension in a multi-dimensional space. In this context, the makespan, priority value and computing resource demand represent three dimensions of a space. In this 3-Dimentional (3D) space, a task is represented as a 3D rectangular prism object. The volume of such 3D rectangular prism object represents the critical value of that corresponding task. However, in order to allow the service provider to give higher preference to one parameter over other, a weight factor is used in the critical value calculation. The critical value (or the volume of the 3D object) of a task $v_i$ can be calculated as 
\begin{equation}\label{eq:criticalVal}
	WV(v_i) = (w_1*\hat{M}_i)*(w_2*\hat{P}_i)*(w_3*\hat{R}_i),\quad w_1+w_2+w_3 = 1
\end{equation}


\subsection{HeRAFC algorithm}\label{sec:sol:algo}
The whole process of the proposed resource allocation strategy can be divided into two phases. In the first phase, the order of tasks, based on WMD approach, for deployment is decided. In the second phase, suitable locations for each task is chosen. 

\subsubsection{Order of task}\label{sec:sol:algo:taskOrder}
The WMD-based algorithm for deciding the order of tasks is presented in Algorithm \ref{algo:HeRAFC:tskOrdr}. The user's application is taken as the input to this algorithm. The job of this algorithm is to arrange the tasks in queue, which further would be followed to assign or map to the fog-cloud environment. The detail description of this algorithm is presented in Section \ref{sec:sol:desc}. 

% FIRST ALGORITHM
\begin{algorithm}[htb] \small
	 
	\KwIn{User's application; MFC environment}
	
% 	\tcc{Deciding the order of tasks for deloyement}
	$ST = $\{Sort the list of tasks in ascending order based on the number of out-edges\}\;\label{algo:HeRAFC:tskOrdr:sortTask} 
	%TIME complexity: n' is total number of tasks. So the fastest sorting algorithm would take O(n' log n') time.
	
	$ProcessQ=$\{ \} \textit{/* Order of tasks need to be deployed */} \; %\tcc{Order of tasks that need to be deployed}
	
	$TL=$\{Extract the list of tasks from $ST$ with number of out-edges $0$\}\;\label{algo:HeRAFC:tskOrdr:extrctTaskLst}
	%TIME complexity: Constant time
	
	\While{True}{\label{algo:HeRAFC:tskOrdr:LoopStart}
		\ForEach{task $t \in TL$}{\label{algo:HeRAFC:tskOrdr:MCVstart}
			Calculate critical value $WV(t)$ using Equation \ref{eq:criticalVal}\;
			Calculate number of out-edges $OD(t)$\;
			$MCV(t) = WV(t)/(OD(t)+\delta)$ \textit{/*Calculate mean critical value of the task*/} \label{algo:HeRAFC:tskOrdr:MCVend} \;%\tcc{Calculate mean critical value of the task}
		}	%TIME complexity: This would take O(|TL|) time
        
		Sort $TL$ based on their $MCV$ in ascending order\;\label{algo:HeRAFC:tskOrdr:sortExtTskLst}	
		%TIME complexity: The sorting would take O (|TL| log |TL|)
		Append $TL$ to $ProcessQ$ \label{algo:HeRAFC:tskOrdr:append2PrcsQ}\;
		%TIME complexity: Constant
		
		Append $-1$ to $ProcessQ$ \label{algo:HeRAFC:tskOrdr:apnd-1} \textit{/* to mark the tasks in level */} \;%\tcc{to mark the tasks in level}
		
		$tmp = $ \{Extract the list of parent tasks of $TL$ from $ST$\}\;\label{algo:HeRAFC:tskOrdr:extractPrntTask}
		%TIME complexity: with the graph implementation, the time complexity can be reduced to size of TL set
		Remove all the tasks in $TL$\;
		Add the tasks in $tmp$ to $TL$\;\label{algo:HeRAFC:tskOrdr:repeatExtrctTskLst}
	}\label{algo:HeRAFC:tskOrdr:LoopEnd}
\caption{WMD-based Task order selection algorithm}   
\label{algo:HeRAFC:tskOrdr}
\end{algorithm}

\subsubsection{Location selection}\label{sec:sol:algo:locSel}
% Introduce this subsection
In the second phase of the HeRAFC algorithm (as persented in Algorithm \ref{algo:HeRAFC}), the ordered task queue is followed and for each task, a suitable location is searched, known as node mapping. Location here refers to one FN or the cloud. Following this, all the edges among tasks are mapped by finding the suitable path from the corresponding source to destination environment where both the end tasks are already mapped. The detail description of this algorithm is presented in Section \ref{sec:sol:desc}.
\begin{algorithm} \small
	\KwIn{User's application; MFC environment}
	
	$ProcessQ = $Queue of tasks decided by Algorithm \ref{algo:HeRAFC:tskOrdr}\; \label{algo:HeRAFC:callAlgo1}
	% \tcc{Location selection for the tasks in $ProcessQ$}
	Calculated the resource availability matrix $RM$\; \label{algo:HeRAFC:calcAvaMat}
	$mapQ =$ \{\} \; %\tcc{List of tasks that are already mapped}
	% NODE mapping
	\While{True}{ \label{algo:HeRAFC:nodeMap:start}
		$ETL = $\{Extract the list of tasks from $ProcessQ$ until $-1$ in LIFO manner.\}\;\label{algo:HeRAFC:extrctTskSet}
		%$nf = $\{List of nearby FNs\}\;
		\ForEach{(task $t$ in $ETL$) AND ($t$ is not scheduled)}{\label{algo:HeRAFC:nodeMap:innerLoop:start}
			$CFL = $\{get the list of fogs or cloud where the child tasks are deployed\}\;\label{algo:HeRAFC:chldFogLst}
			$nf = CFL$\;\label{algo:HeRAFC:chldNrbyFogLst}
			\If{$CFL == NULL$}{
				$nf = $Nearby FN of user's application\; \label{algo:HeRAFC:tskOrdr:} \label{algo:HeRAFC:appNrbyFN}
			}
			Check if $t$ can be deployed on any one of the fog or cloud in $nf$\; \label{algo:HeRAFC:chckNFLst:start}
			\If{$t$ deployed on $nf$}{
				Append $t$ to $mapQ$ and update $RM$ \;
				Continue with next task from Step \ref{algo:HeRAFC:nodeMap:innerLoop:start} \;\label{algo:HeRAFC:chckNFLst:end}
			}
			$oh = $\{Find the FNs or cloud that are one hop distance to the all fogs in $nf$\}\;\label{algo:HeRAFC:findOH}
			Check if $t$ can be deployed on any one of the fog or cloud in $oh$\;\label{algo:HeRAFC:chckOH:start}
			\If{$t$ deployed on $oh$}{
				Append $t$ to $mapQ$ and update $RM$ \;
				Continue with next task from Step \ref{algo:HeRAFC:nodeMap:innerLoop:start} \;\label{algo:HeRAFC:chckOH:end}
			}
			$th = $\{Find the FNs or cloud that are two hop distance to the all fogs in $nf$\} \;\label{algo:HeRAFC:findTH}
			Check if $t$ can be deployed on any one of the fog or cloud in $th$ \;\label{algo:HeRAFC:chckTH:start}
			\If{$t$ deployed on $th$}{
				Append $t$ to $mapQ$ and update $RM$\;
				Continue with next task from Step \ref{algo:HeRAFC:nodeMap:innerLoop:start} \;\label{algo:HeRAFC:chckTH:end}
			}
		}\label{algo:HeRAFC:nodeMap:innerLoop:end}
		% LINK mapping
		$E'=$\{set of task edges that are connected to the tasks in $ETL$\}\;\label{algo:HeRAFC:formEdgeSet}
		Sort $E'$ in descending order based on bandwidth demand\;\label{algo:HeRAFC:EdgeSorting}
		\ForEach{edge $e \in E'$}{\label{algo:HeRAFC:EdgeMapping:start}
			
			Find source and destination task, $s$ \& $t$\; \label{algo:HeRAFC:getSrcDstTsk}
			$sh = $ Find the FN or cloud where $s$ is deployed\;
			$th = $ Find the FN or cloud where $t$ is deployed\;
			\eIf{$sh \ne NULL$ OR $th \ne NULL$}{
				Find the shortest path from $sh$ to $th$ and map the task edge\; \label{algo:HeRAFC:shrtstPath}
			}{
				Ignore the current edge $e$\; \label{algo:HeRAFC:EdgeMapping:end}
			}
		}
		Reset $RM$ and continue with next set of tasks.\; \label{alog:HeRAFC:resetRsrcMtrx}
	}
	\caption{Resource Allocation in MultiFog-Cloud algorithm}
	\label{algo:HeRAFC}
\end{algorithm}

\subsection{HeRAFC description}\label{sec:sol:desc}
% Detail description of both the algorithms
% Description of First Algorithm
User's application and the information regarding the physical environment are provided to Algorithm \ref{algo:HeRAFC} as the input. The algorithm starts by invoking Algorithm \ref{algo:HeRAFC:tskOrdr} that would decide the order of the tasks that need to be followed in the location selection phase. Algorithm \ref{algo:HeRAFC:tskOrdr} starts by sorting the list of tasks based on the number of child tasks. To calculate the number of child tasks of a parent task, the number of out-edges is calculated as in Line \ref{algo:HeRAFC:tskOrdr:sortTask}. Out-edge refers to the number of outgoing edges from a task. From the sorted task list, the tasks that are having out-edge value 0 or the task having no child are first selected, as in Line \ref{algo:HeRAFC:tskOrdr:extrctTaskLst}. This means, the algorithm gives higher priority to leaf tasks while processing. This way, this algorithm takes the tasks' dependency into consideration.
%out-edges, as in Line \ref{algo:HeRAFC:tskOrdr:sortTask}. Out-edge refers to as the number of outgoing edges from a task. In other words, out-edge of a task can be seen as the number of attached child tasks. From the sorted task list, the tasks that are having out-edge $0$ are first selected, as in Line \ref{algo:HeRAFC:tskOrdr:extrctTaskLst}.} 

For each extracted task,the mean critical value (which eventually follows WMD approach) is calculated, in Line \ref{algo:HeRAFC:tskOrdr:MCVstart}-\ref{algo:HeRAFC:tskOrdr:MCVend}, taking the ratio of critical value to the number of out-edges. 
For the leaf node the number of out edges is $0$. To avoid any \emph{Divide by Zero error}, a small constant $\delta$ is added to $OD(t)$. 
The extracted list is further sorted based on the mean critical value and appended to the process task queue, in Line \ref{algo:HeRAFC:tskOrdr:sortExtTskLst} and \ref{algo:HeRAFC:tskOrdr:append2PrcsQ}, respectively. The same process is applied to the parent task of the currently extracted tasks, as in Line \ref{algo:HeRAFC:tskOrdr:repeatExtrctTskLst}. 
Processing the tasks level-by-level is taken into account to ensure that the dependent tasks (or the child tasks) are executed not before the execution of their predecessor tasks (or the parent tasks). The time complexity of this algorithm is $ \mathcal{O}(|V'| \log{}|V'|)$ , where $V'$ be the set of tasks (proof in Appendix \ref{appen:TimeCompAlgo1}). 



% Descriptioin of second algorithm
% mapping of tasks/node
The output of Algorithm \ref{algo:HeRAFC:tskOrdr}, $ProcessQ$, is further followed to map the tasks and the edges. Each set of tasks are separated by $-1$ in $ProcessQ$, as in Algorithm \ref{algo:HeRAFC:tskOrdr}, Line \ref{algo:HeRAFC:tskOrdr:apnd-1}, can be logically represents as the tasks in one level of the task graph. 
The set of tasks, $ETL$, is first extracted from the $ProcessQ$, as in Line \ref{algo:HeRAFC:extrctTskSet} in a Last-In-First-Out (LIFO) manner. Extracting in LIFO would further ensure that the dependent tasks (or child tasks) are not executed before the execution of predecessor tasks. This indicates that the root task in the DAG will be the first one to get executed. For each task in $ETL$, the set of child tasks is calculated followed by forming a list of FNs and cloud, $CFL$, that are hosting those child tasks, in Line \ref{algo:HeRAFC:chldFogLst}. The set of hosting environment, $CFL$ is considered as nearby fog list, $nf$, in Line \ref{algo:HeRAFC:chldNrbyFogLst}. However, in case of absence of no child task, the FN that is nearer to the user is considered, as in Line \ref{algo:HeRAFC:appNrbyFN}. In Line \ref{algo:HeRAFC:chckNFLst:start} - \ref{algo:HeRAFC:chckNFLst:end}, the current task is checked if this can be assigned to any one of the nearby FNs list. If the amount of resource can be fulfilled by the anyone of the FN or by the cloud, the task will be assigned to the selected FN or cloud and the same process will be applied to next task. However, if no hosting environment is found in Line \ref{algo:HeRAFC:chckNFLst:start}, the list of FNs and cloud that are at one hop distance, $oh$ from the child task is calculated, as in Line \ref{algo:HeRAFC:findOH}. On calculation of the set $oh$, in Line \ref{algo:HeRAFC:chckOH:start} - \ref{algo:HeRAFC:chckOH:end}, all the hosting environments will be checked if the current task can be deployed. In case of availability of the resources on any FN that can fulfill the demand, the current task will be assigned to the selected FN or the cloud. Further, in case of failure of resource availability for the current task, the hosting environments that are at two hops distance will be calculated in Line \ref{algo:HeRAFC:findTH} and the same process will be applied for checking the resource and assigning the task, as mentioned in Line \ref{algo:HeRAFC:chckTH:start} - \ref{algo:HeRAFC:chckTH:end}. This concludes the procedure for mapping the tasks on to different FNs and cloud. 

% task-edge/Link mapping 
Upon mapping the set of tasks, $ETL$, the set of adjacent edges, $E'$ in Line \ref{algo:HeRAFC:formEdgeSet}, will be assigned, in Line \ref{algo:HeRAFC:EdgeMapping:start} - \ref{algo:HeRAFC:EdgeMapping:end}. For each edge, $e$, the information regarding the source task and the destination task are extracted from the task graph, in Line \ref{algo:HeRAFC:getSrcDstTsk}. If both the tasks are already mapped onto the physical environment, the shortest path between the corresponding hosting environment will be calculated and assigned to host the edge $e$, in Line \ref{algo:HeRAFC:shrtstPath}. Upon mapping of both the set of tasks in one level of task graph and the edges among them, the resource matrix will be reset, in Line \ref{alog:HeRAFC:resetRsrcMtrx}. This is due to the fact that one level tasks can not run in parallel to the other level of tasks (parent or child tasks) and need to be scheduled to run in a sequential manner. Hence, upon completion of execution of one level of tasks, the leased resources will be released for the set of tasks in the next level of the task graph. The time complexity of the proposed  algorithm is $\mathcal{O}(|E'|\left[\log{}|E'| + |E|log(|V|)\right])$ , where $E$ and $E'$ are the set of physical edges and task edges, respectively and $V$ is the number of cloud and FNs ( proof in \ref{appen:TimeCompAlgo2}). 

	

\section{Performance evaluation}\label{sec:perfEval}
In this section, the performance of the proposed algorithm is discussed. Liu \emph{et al.} (DRACO) \cite{comp1} and Shah-Mansour \emph{et al.} (HFCCI) \cite{comp2} algorithms are used to compare and present the efficiency in terms of computing and network resource utilization in different hosting environment, such as fog and cloud. FNs nearby can receive data from multiple users in case of DRACO algorithm. By using game theory, HFCCI minimizes the number of users and reduces energy costs while allocating cloud resources. Various performance matrices are employed for evaluating efficiency, including the effect of tasks' order on resource utilization, a comparison of network resource utilization between fog and cloud environments, and tasks' latency. The simulation uses a rule-based approach over a data-driven ML-based approach. Rule-based approaches can be used to generate simulated environments that mimic the real-world. This uses predefined configurations and rules, and does not require any historical data to fit the environment. On the other hand, a data-driven ML-based approach needs historical data that fits the environment and the problem statement. In the context of heterogeneous fog and cloud resource allocation and optimization, the data related to historical performance of the computing and storage cluster, network resource utilization, real requests from the users are difficult to acquire. To the best of our knowledge, there are no such data available that fit the problem addressed in this paper. This is the main reason why most of the research community and we preferred a rule-based approach over a data-driven ML-based approach. In the following subsection, the detail information on simulation environment and the results obtained from the simulation are presented.




\subsection{Simulation environment}\label{sec:perfEval:env}

A Python-based discrete event simulator is used to simulate the proposed resource management algorithm. This simulation does not explicitly configure tasks to run on different CPU cores. This is entirely dependent on the number of cores available and how the native operating system behaves. The resource configuration of the FCIs, fog, cloud and the users' applications are presented in JSON file. 
%The fog and cloud environment is created manually. 
The applications are created dynamically/programmatically. Separate JSON files are created for each user's app, including the tasks list, edges list, resource demand of tasks and edges, and other related information. The detailed configuration of the simulation parameters are given in Table \ref{table:simPar}.
%The detailed simulation parameters related to fog-cloud environment and users' applications are shown in Table \ref{table:simMFCEnv} and \ref{table:simParUsrApp}, respectively.
\ref{appen:simEnv}\revison{, provides a statistical analysis of the entire environment consisting of users' applications, tasks, resource demand, FNs, FCIs, resource availability and priority, connectivity among FNs, FCI and cloud, and network resource availability. } 

%  Description on Simulation parameter table
\revison{The MFC environment consists of $500$ FNs that are connected to $200$ FCIs. Each FCI is connected to at least one FN, whereas a single FN is connected to precisely one FCI. For each FN, the total amount of CPU capacity ranges from 50 through 100 and the values are assigned by following a random distribution. The total memory (RAM) for each FN is assigned randomly ranging from $200 GB$ to $400 GB$, where the unit $GB$ represents GigaByte. The approximate computation power of a FN is calculated as MIPS (Million instructions per  second), which ranges from $3000$ to $5000$.} % 3GhZ processing speed is somewhat around 3000 MIPS or even more

\revison{As discussed before, each FN is connected to exactly one FCI. The maximum network bandwidth available between a FN and the corresponding FCI ranges from $300 Mbps$ through $400 Mbps$. In this paper, it is considered that a FCI may directly communicate with another FCI. In such a situation, there exist a network bandwidth capacity among FCIs, which ranges from $400 Mbps$ through $1000 Mbps$. Similarly, the bandwidth between FCIs and the cloud range from $400 Mbps$ through $1000 Mbps$. The network bandwidth unit is in Mbps (Mega bits Per Second). It is assumed that latency value between any FN and the corresponding FCI is small than the latency among FCIs and between FCIs and cloud. Latency values are assigned to the physical links by following a uniform distribution. The latency (in millisecond) between FN and FCIs ranges from $50 ms$ through $100 ms$. Similarly, the latency values for rest of the physical links range from $100 ms$ through $200 ms$. Technically, a task can be hosted on 1-hop or at a multi-hop distance. However, in the current implementation maximum number of hops is set to $2$. This means there are a maximum of two FCIs and no cloud are present in the physical path between any two FNs that are used to host user's tasks.} 
%Regarding the User's application, the configuration and the resource demand of each application are represented in JSON file format.

\revison{The maximum number of applications is set to $10000$. For each application, random number of tasks are generated ranging from $4$ through $12$. However, the total maximum number of tasks for the entire simulation is set to $100,000$. For each task, the CPU and memory demand range from $1-4 CPUs$ and $100 MB - 1000 MB$, respectively. The average makespan of each task is $350 ms$ with minimum and maximum makespan of $10 ms$ and $1000 ms$, respectively. The network bandwidth demand of edges is ranging from $100 Mbps$ and $200 Mbps$. Similarly, the average latency demand among tasks ranges between $10 ms$ and $50 ms$. The unit of latency is millisecond (ms). The minimum and maximum priority values of a task are $1$ and $5$, respectively. The bandwidth and latency demand of a task ranges between $100-200 Mbps$ and $10-300 ms$, respectively. The number of edges within an application is decided by a link probability of $0.6$. This indicates the connectivity probability among two task.} 



\begin{table}[]
\centering
\scriptsize
\caption{Simulation parameters for Fog-Cloud environment and users application}
    \begin{tabular}{|p{0.33\textwidth} p{0.13\textwidth} || p{0.33\textwidth} p{0.08\textwidth}|}    
    \hline
    \multicolumn{4}{|c|}{Simulation Parameters}                                                                   \\ \hline
    \multicolumn{2}{|c||}{\textbf{Fog-Cloud Environment}}       & \multicolumn{2}{c|}{\textbf{Users Application}} \\ \hline    
    Environment configuration file format & JSON &    Application configuration file format & JSON \\ 
    Total number of FNs &  500 &   Number of applications & 5000-10000  \\ 
	Number of FCIs & 200  &   Number of tasks per application & 4-12  \\  %(Random generation)
                    &       & Maximum number of tasks in the simulation  & 100,000 \\
	CPU capacity per FN & 50-100 &   Required vCPU per task & 1-4  \\  %(Random distribution)
	RAM (in GB) capacity per FN & 100-200 &   Required RAM (in MB) per task & 100-1000       \\  %(Random Distribution)
	Average MIPS per FN & 250-400  &  Makespan of each task (in ms) & 10-1000   \\  %(Random Distribution)
	Bandwidth between of FN and FCI & 300-400 Mbps &   Priority of each task & 1-5  \\  %(Uniform distribution)
	Bandwidth between of FCI and cloud & 400-1000 Mbps &   Edge bandwidth demand (in Mbps) & 100-200  \\  % (Uniform distribution)
	Bandwidth among FCIs & 400-1000 Mbps &  Edge latency demand (in ms) & 10-50   \\  %(Uniform distribution) 
	Latency between FN and FCI & 50-100 ms &   &  \\  %(Uniform distribution) 
	Latency between FCI and Cloud & 101-200 ms  &  &   \\  % (Uniform distribution)
	Latency among FCIs & 101-200 ms &   &  \\  %  (Uniform distribution)
	Maximum number of hops & 2  &   & \\ \hline
    \end{tabular} \vspace{-4mm}
\label{table:simPar}
\end{table}





\subsection{Simulation results}\label{sec:perfEval:result}
Following the discussion of simulation environment in above subsection, this subsection presents the results of the simulation in more details. Figure \ref{fig:sim:compRsrcUtil_F} and Figure \ref{fig:sim:compRsrcUtil_C} provide the resource utilization of fog and cloud environment, respectively. In this simulation implementation, resource refers to CPU, Memory and network bandwidth. Figure \ref{fig:sim:compRsrcUtil_F} shows the comparison of resource utilization of proposed HeRAFC algorithm with DRACO \cite{comp1} and HFCCI \cite{comp2} algorithms only in fog environment. Resource utilization is calculated as the ratio of amount of resources allocated and the total resource capacity. %We have considered the mean of $12$ simulation instances for each set of applications. 

\input{1_comp_CPU_Mem_BW_Util_F.tex} 
\revison{Figures }\ref{fig:sim:compRsrcUtil_F}\subref{fig:sim:compCPUUtil_F}\revison{, }\ref{fig:sim:compRsrcUtil_F}\subref{fig:sim:compMemUtil_F}\revison{, and }\ref{fig:sim:compRsrcUtil_F}\subref{fig:sim:compBWUtil_F}\revison{ show the average CPU, memory, and network bandwidth utilization of FNs, respectively. X-axis represents the total number of applications ranging from $5000$ to $10000$ and Y-axis represents the average resource utilization in percentage. It can be observed that, Upon assigning $5000$ applications using the HeRAFC algorithm, the average CPU utilization of fog servers is approximately $35\%$, similar to the DRACO algorithm. However, it is observed, the CPU utilization is approximately $33\%$ in the case of the HFCCI algorithm, as shown in Figure }\ref{fig:sim:compRsrcUtil_F}\subref{fig:sim:compCPUUtil_F}\revison{. Resource utilization continuously increases when the number of applications increases to $8000$. However, when the number of applications increases beyond $8000$, the rate of increase slows down. It is observed that the average CPU utilization is $66\%, 60\%, $ and $56\%$ in case of HeRAFC, DRACO, and HFCCI algorithms, respectively, when there are a total of $10$ thousand applications. A similar pattern is also observed in memory resource utilization. In case of the proposed HeRAFC algorithm the memory utilization of the fog environment increases from approximately $26\%$ to $67\%$, when the number of applications increases to $5000$ to $10000$. Figure }\ref{fig:sim:compRsrcUtil_F}\subref{fig:sim:compMemUtil_F}\revison{ demonstrates that the proposed HeRAFC algorithm outperforms DRACO and HFCCI algorithms. The memory utilization of FNs is $60\%$ and $58\%$ when DRACO and HFCCI algorithms are applied, respectively, when the number of applications is $10$ thousand. Overall, the proposed HeRAFC algorithm performs better than the others in utilizing the CPU and memory resources. This is due to the underlined multi-fog environment where schedulers can take advantage of the fog resources and hence assign a maximum number of tasks to the fog environment instead of the cloud. However, DRACO and HFCCI do not take advantage of the nearby available fog resources.} 

\revison{Figure }\ref{fig:sim:compRsrcUtil_F}\subref{fig:sim:compBWUtil_F}\revison{, shows the average network bandwidth utilization of different algorithms with the number of applications ranging from $10$ to $100$ within fog environment. As the number of applications increases, the bandwidth utilization of all three methods also increases, but each method has a different rate of increase. It is evident from the chart that HeRAFC and DRACO have similar bandwidth utilization rates, while HFCCI has a lower bandwidth utilization rate. In case of HeRAFC, the average network utilization ranges from $42\%$ to $78\%$ when the number of applications ranges from $5$ to $10$ thousand, which is more than the network utilization obtained using other two related algorithms. This is due to the fact that HeRAFC explores the other FNs that are at multi-hop distance to fulfill the resource demand of application. As a result, the workload on the fog servers and the network among FNs are relatively higher. The average network utilization within FNs in case of DRACO and HFCCI are approximately $62\%$ and $53\%$, respectively, when the number of applications is $10$ thousand. It is to be noted that out of all the applications, some of the applications are hosted on FNs and rest are hosted on cloud environment. The resource utilization of the cloud environment is provided in Figure }\ref{fig:sim:compRsrcUtil_C}.

% This uneven mean resource utilization is due to the randomness in the resource demand of the users' applications, where one application may have maximum resource demand set by the simulation environment and other application may have minimum resource demand set by the simulation parameters.


\input{2_comp_CPU_Mem_BW_Util_C.tex}
\revison{The cloud resource utilization in the case of all three algorithms is presented in Figure }\ref{fig:sim:compRsrcUtil_C}\revison{. Using HeRAFC, the average cloud CPU utilization increases from approximately $28\%$ to $59\%$ when the number of applications increases from $5000$ to $10000$, as in Figure }\ref{fig:sim:compRsrcUtil_C}\subref{fig:sim:compCPUUtil_C}\revison{. However, in the case of DRACO and HFCCI algorithms, the average CPU utilization increases from $28\%$ to $61\%$ and from approx. $27\%$ to $68\%$, respectively, with the same number of applications. Evidently, the cloud CPU utilization using HeRAFC is less than that of the other two algorithms, as the FNs (including those at a multi-hop distance) are given higher priority over the cloud while assigning the applications. A similar trend can be seen in Figure }\ref{fig:sim:compRsrcUtil_C}\subref{fig:sim:compMemUtil_C}\revison{. The average memory utilization in the cloud environment using the HeRAFC algorithm with $5000$ applications is approximately $18\%$. In contrast, the memory utilization values in the case of DRACO and HFCCI algorithms are approximately $20\%$ and $21\%$, respectively. Such a utilization pattern indicates a lesser dependency on the cloud. This is because HeRAFC gives higher priority to the nearby and other FNs, letting fewer tasks be offloaded to the cloud. The impact of such a strategy can also be seen in the significant reduction of cloud network resource utilization, as discussed below.}

\revison{The average network bandwidth utilization, as shown in Figure }\ref{fig:sim:compRsrcUtil_C}\subref{fig:sim:compBWUtil_C}\revison{, refers to the utilization of physical links that are attached to the cloud. When the number of applications increases from $5000$ to $10000$, the average network bandwidth utilization using the HeRAFC algorithm increases from $23\%$ to approximately $40\%$, which is less than that of the other two algorithms. For DRACO and HFCCI, the average network bandwidth utilization increased to $47\%$ and $52\%$ when the number of applications is increased to $10000$. The HeRAFC algorithm gives higher preferences to the nearby FNs. As a result, the maximum number of task edges are assigned to the physical path within FNs,  leading to relatively lesser bandwidth utilization in the cloud environment.}





% \input{latency_of_Priority_tasks.tex}


\revison{The results in Figure }\ref{fig:sim:Ltncy_F}\revison{ and }\ref{fig:sim:Ltncy_C}\revison{ show the effect of weighted critical value ($WV$, as calculated in Equation }\ref{eq:criticalVal})\revison{ and tasks' priority on average network latency in Fog and in cloud environments, respectively. In other words, we observed the average latency of tasks with different priority and $WV$ values. The value of $WV$ depends upon three parameters: makespan of the tasks (Equation }\ref{eq:normMkspn}\revison{), priority (Equation }\ref{eq:normPriority}\revison{), and resource demand (Equation }\ref{eq:normCompRsrc}\revison{). By changing the value of $w_1$ (weight for makespan), $w_2$ (weight for priority), and $w_3$ (weight for resource demand), the value of $WV$ can be changed. Four configurations of $w_1, w_2, $and$ w_3$ are considered in the simulation. Under the first configuration, the same weightage values ($w_1=w_2=w_3$) are given to all the parameters (Figure }\ref{fig:sim:Ltncy_F}\revison{(a)). Under the second configuration, a higher weightage is given to the resource demand of the tasks ($w_1=0.2, w_2=0.3, w_3=0.5$), as shown in Figure }\ref{fig:sim:Ltncy_F}(b)\revison{. In the third and forth configurations, higher importance is given to task's priority (Figure }\ref{fig:sim:Ltncy_F}(c)\revison{) and makespan (}\ref{fig:sim:Ltncy_F}(d)\revison{), respectively. The latency value of a task is calculated by taking the average latency of out-going adjacent edges. So in case of an edge from a task with Priority 1 ($P_i=1$) to a task with priority 2 ($P_i=2$), the value of $P_i=1$ task is calculated by considering the value of latency of edge. In other words, the latency of an edge is assigned to the source task and not the destination task. In this simulation, the tasks with priority value 5 are given higher priority and the minimum priority value of a task is set to 1. The priority values are assigned to the task following a random distribution. Figure }\ref{fig:sim:Ltncy_F}\revison{ shows the latency of different priority tasks within the fog environment. The x-axis represents the total number of tasks, ranging from $50$ thousand to $100$ thousand and Y-axis represents average latency of different tasks in milliseconds (ms). It is observed that the average latency of the high priority tasks is always less than that of the tasks with low priority value, under different configurations.}

\input{latency_weighted_critical_values_DATA.tex}
\input{latency_weighted_critical_values_FIG.tex}

\revison{Under the first configuration, the average latency of tasks with $P_i=5$ and $P_i=1$ in only fog environment is approximately $11 ms$ and $93 ms$ when the number of tasks is $50$ thousand. The latency of those tasks increases to approx. $34 ms$ and $130ms$ when the number of tasks increases to $10000$, as shown in Figure }\ref{fig:sim:Ltncy_F}(a)\revison{. A similar pattern is observed under the forth configuration, where a higher weightage is given to the makespan parameter ($w_1=0.5, w_2=0.3, w_3=0.2$). The average latency of the tasks with priority ($P_i=5$) is approx. $8ms$, whereas the latency of $P_=1$ tasks is approx. $120ms$ when the number of tasks is $50$ thousand. From all the observations (Figure }\ref{fig:sim:Ltncy_F}(a) - \ref{fig:sim:Ltncy_F}(d)\revison{), we can conclude that under different configurations, the proposed HeRAFC algorithm gives a higher preference to the fog environment while fulfilling the resource demand and other objectives.}


\revison{Unlike Figure }\ref{fig:sim:Ltncy_F}\revison{, Figure }\ref{fig:sim:Ltncy_C}\revison{ shows the effect of weighted critical value ($WV$) and the priority of tasks upon the average latency in the cloud environment. Figure }\ref{fig:sim:Ltncy_C}(a)\revison{ represents the average latency when all the parameters are given equal weightage, i.e., $w_1=w_2=w_3$. Figure }\ref{fig:sim:Ltncy_C}(b)\revison{, }\ref{fig:sim:Ltncy_C}(c)\revison{, and }\ref{fig:sim:Ltncy_C}(d)\revison{ represent the average latency when a higher weightage is given to resource demand, priority, and makespan parameters, respectively. Similar to the results in Figure }\ref{fig:sim:Ltncy_F}\revison{, it is observed that the tasks with higher priority have less latency. For instance, as shown in Figure }\ref{fig:sim:Ltncy_C}(a)\revison{, the tasks with $P_i=5$ and that are sent to the cloud environment have an average latency of $55ms$ when the number of tasks is $50$ thousand. The latency increases to $204ms$ when the total number of tasks increases to $100$ thousand. On the contrary, the  have the average latency increases from $1200ms$ and $1810ms$ when the total number of tasks  with $P_i=1$ increases from $50$ thousand to $100$ thousand. The similar patterns can be seen in Figure }\ref{fig:sim:Ltncy_C}(b)\revison{ and }\ref{fig:sim:Ltncy_C}(c)\revison{, where the average latency of $P_i=5$ tasks hosted in cloud environment is approx. $61ms$ and $64ms$ when a higher weightage is given to resource demand and priority parameters, respectively. Moreover, when the $P_i=5$ tasks are increased to $100$ thousand, their average latency increases to approx. $213ms$ and $220ms$ under the same configurations, respectively. Tasks with $P_i=1$ have the maximum latency irrespective of the weightage value of the parameters.} 



%% RESUME HERE


% \input{4_PerofTskInFog_Cloud.tex}


\input{appsTasks_in_fog_and_cloud_DATA.tex}
\input{appsTasks_in_fog_and_cloud_FIG.tex}
\revison{It is essential to investigate the percentage of tasks allocated to fog and cloud environments; the same simulation results are presented in Figure }\ref{fig:sim:PerofTskInFogCld}\revison{. The X-axis represents the number of applications ranging from $5000$ to $10000$. The applications consist of different priority tasks. Y-axis represents the percentage of tasks (with specific priority values) assigned to fog or cloud environments. Figure }\ref{fig:sim:PerofTskInFogCld}\revison{ consists of five sub-figures, each for specific priority values. Figure }\ref{fig:sim:PerofTskInFogCld}(a)\revison{ shows the percentage of $P_i=5$ tasks processed by the FNs that might be in the nearby location or at a multi-hop distance and by the cloud environment. Similarly, Figure }\ref{fig:sim:PerofTskInFogCld}(b)\revison{ and }\ref{fig:sim:PerofTskInFogCld}(c)\revison{ represent the percentage of $P_i=4$ and $P_i=3$ tasks, respectively, that are processed by the FNs and by the cloud environment. The simulation result in all those sub-figures shows the tasks with higher priority are given higher preference to be processed by the FN. For instance, out of all the $P_i=5$ tasks (of $5000$ applications), approximately $80\%$ of the tasks are processed by FNs and rest $20\%$ are processed by the cloud. However, when the number of applications increases to $10000$, a decreasing percentage of the $P_i=5$ tasks are handled by FNs, i.e., $70\%$ of the tasks are processed by fog environment and the rest are handled by cloud, as shown in Figure }\ref{fig:sim:PerofTskInFogCld}(a). 

\revison{Similarly, in the case of $P_i=4$ tasks, it is observed that approx. $70\%$ of the tasks are assigned to the fog environment and the rest $30\%$ are assigned to the cloud when the total number of applications was $5000$. When the total number of applications increases to $10000$ (i.e. number of tasks is $100$ thousand), an approximately equal percentage of $P_i=4$ tasks are processed by FNs and by cloud environment, as shown in Figure }\ref{fig:sim:PerofTskInFogCld}(b)\revison{. Figure }\ref{fig:sim:PerofTskInFogCld}(e)\revison{ gives a result that is opposite to the result in }\ref{fig:sim:PerofTskInFogCld}(a)\revison{. In Figure }\ref{fig:sim:PerofTskInFogCld}(e)\revison{, the maximum number of tasks with $P_i=1$ are handled by the cloud. When the number of applications was $5000$, approx. $25\%$ of the $P_i=1$ tasks are processed by FNs and rest $75\%$ of the $P_i=1$ tasks are processed by the cloud. The percentage of $P_i=1$ tasks that are sent to cloud further increases to more than $85\%$ when the total number of applications increases to $10$ thousand. This is due to the fact that, when the number of applications increases, the number of tasks increases, and their resource demand increases. However, the resource availability in the fog environment is fixed to fulfill those increasing resource demands of all the tasks. As a result, there is a higher chance of tasks being sent to the cloud environment.} 

% Figure \ref{fig:sim:PerofTskInFog} and  \ref{fig:sim:PerofTskInCld} show, what percentage of priority $i$ tasks are allocated to fog and cloud, respectively. The x-Axis represents the number of tasks, which ranges from $100$ through $1000$ and the Y-axis represents percentage of tasks that are allocated to fog or cloud environment. The tasks with higher priority are given first preference to be allocated to the nearby FNs and the same can be observed from the results.

% Approximately $25\%$ and $0\%$ of the priority 5 tasks among 100 tasks are allocated to fog and cloud environment, respectively. For the same number of tasks, approximately $13\%$ and $2\%$ of the low priority tasks (with priority value 1) tasks are allocated to fog and cloud environment, respectively. When the number of tasks increases to $1000$, $16\%$ of total number of tasks (having priority 5) and $8\%$ of total number of tasks (having priority 1) are allocated to FNs, as shown in Figure \ref{fig:sim:PerofTskInFog}. The reason behind such behavior of the HeRAFC is that the task having higher priority values is considered first to be mapped to the fog environment.


% FIGURE 5:
% \usetikzlibrary{patterns}
% \input{5_PrtyDist_task.tex}



%%%%%%  Removed below because of not good clarity
% \begin{figure}[th]
% 	\centering
% 	\includegraphics[width=0.6\linewidth]{5_PrtyDist_task.eps}
% 	\caption{Priority distribution among tasks.} 
% % 	\vspace{-5mm}
% 	\label{fig:sim:PrtyDist_task}
% \end{figure}

% Figure \ref{fig:sim:PrtyDist_task} shows the distribution of tasks under different priority values. X-axis and Y-axis represents the total number of tasks (ranging from 100 through 1000) and the percentage of tasks that belongs to different priority values. In $100$ tasks, it is found that $25\%$ of tasks are of priority 5 and $15\%$ of tasks are of priority 1. However, among $1000$ tasks approximately $21.2\%$ and $19.7\%$ tasks belong to priority 5 and priority 1, respectively. This can be clearly seen that the tasks are almost uniformly distributed and are not biased to any specific priority value. 



% FIGURE 6 (ab): 
% \input{6_tskOrdr_CPUMemBWUtil.tex}
\input{taskOrder_ResrcUtil_DATA}
\input{taskOrder_ResrcUtil_FIG}

\revison{Algorithm }\ref{algo:HeRAFC:tskOrdr}\revison{ decides the order of tasks to be followed while allocating the resources. This paper also investigated the effect of tasks' order on resource utilization, as shown in Figure }\ref{fig:sim:TaskOrderRsrcUtil}\revison{. Along with the WMD (Weighted Multi-Dimensional)  approach, this paper investigated resource utilization when the order of tasks is decided randomly and solely based on the priority value. Figure }\ref{fig:sim:TaskOrderRsrcUtil}\subref{fig:sim:taskOrderCompResrcUtil}\revison{ and }\ref{fig:sim:TaskOrderRsrcUtil}\subref{fig:sim:taskOrderNWResrcUtil}\revison{ shows the computing and network resource utilization, respectively. The X-axis represents the number of applications (ranging from $5000$ to $10$ thousand) and the Y-axis represents the resource utilization in percentage. In the case of random order, the computing resource utilization ranges from approximately $20\%$ to $50\%$ with the number of applications increasing from $5000$ to $10000$ (}Figure \ref{fig:sim:TaskOrderRsrcUtil}\subref{fig:sim:taskOrderCompResrcUtil}\revison{). Similarly, the network resource utilization increases from $20\%$ to $40\%$, when the number of tasks increases from $5000$ to $10000$ (}\ref{fig:sim:TaskOrderRsrcUtil}\subref{fig:sim:taskOrderNWResrcUtil}\revison{). A better result can be observed when the tasks are ordered solely based on their priority values. The computing and network resource utilization increases from $23\%$ to $72\%$ and $26\%$ to $45\%$, when the number of tasks increases from $5000$ to $10000$. However, with the proposed WMD order, resource utilization can further be improved. The computing and network resource utilization increases from $30\%$ to $86\%$ and from $31\%$ to $59\%$, respectively, which is better than the other two approaches. The user decides the tasks' priority values, which do not include tasks' resource demand and makespan values. This affects the final resource utilization, as discussed before. Similarly, in the random order of tasks, none of the parameters are given any priority/importance, resulting in a degraded resource utilization compared to the WMD-based ordering tasks strategy.}   

\subsection{Uncertainty Analysis}
\revison{The impact of fluctuating resource availability at the FNs on the resource utilization is analyzed and the results are presented in }\ref{apndx:uncertainAna}.

\subsection{Simulation time analysis}
\revison{The time taken to handle different number of applications by both the algorithms is observed and the results are analyzed and presented in }\ref{apndx:simTimeAna}.


\section{Conclusions and future works}\label{sec:concls}
In this paper, we investigated the problem of resource allocation to users' applications consisting of multiple dependent tasks in MultiFog-Cloud environment. The proposed HeRAFC algorithm allows one FN to communicate with other FNs through Fog Cloud Interface. As a result, the service providers are allowed to choose a FN that is at multi-hop distance from the nearby FN to host and execute part of the application. Addressing the resource allocation problem in such environment, the aforementioned problem is first formulated as Integer Linear Programming problem and a novel heuristic HeRAFC algorithm is presented that facilitates the users to decide the importance of one task over other through priority values. For each task, the proposed algorithm explores the best hosting environment that is close to the environment where the adjacent tasks are deployed considering the makespan, latency and resource demand as the major parameters. To prove the efficiency, the simulation results of HeRAFC algorithm and other related algorithms are compared and presented. The performance of the proposed algorithm is evaluated in terms of different parameters such as average latency in fog and cloud, resource utilization in fog and cloud etc. 

%Besides the parameters used in the current version of HeRAFC algorithm, 
In a similar manner to our previous work in \cite{MAHMUD2018_2018_j7}, a small experimental testbed will be used to implement the HeRAFC algorithm to make it more applicable to practical scenarios while minimizing energy consumption and improving QoS. We will consider applying the proposed HeRAFC to our proposed Indie Fog \cite{IndieFog} architecture. Additionally, HeRAFC should incorporate both FN mobility and end user applications. The development of a simulated testbed for fog/edge computing is currently underway using an opportunistic network emulator (STEP-ONE \cite{MASS2019100051}), which allows us to use different mobility models. As part of our future work, we will incorporate this dynamic behavior into HeRAFC.

\section*{Acknowledgment}

This research is supported by SERB, India, through grant CRG/2021/003888. We also thank financial support to UoH-IoE by MHRD, India (F11/9/2019-U3(A)).

% REFERENCE SECTION
\bibliographystyle{elsarticle-num}
\bibliography{references}


% \par 
% \noindent
\begin{wrapfigure}{i}{1in}
\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{ckdehury}
\end{wrapfigure}
\textbf{Chinmaya Kumar Dehury} received a Bachelors degree from Sambalpur University, India, in June 2009 and a Masters degree from Biju Pattnaik University of Technology, India, in June 2013. He received a Ph.D. Degree in the Department of Computer Science and Information Engineering, Chang Gung University, Taiwan. He is currently a Lecturer of Distributed System, member of Mobile \& Cloud Lab in the Institute of Computer Science, University of Tartu, Estonia. His research interests include scheduling, resource management and fault tolerance problems of Cloud and fog Computing, the application of artificial intelligence in cloud management, edge intelligence, Internet of Things, and data management frameworks. His research results are published by top-tier journals and transactions such as IEEE TCC, JSAC, TPDS, FGCS, etc. He is a member of IEEE and ACM India. He is also serving as a PC member of several conferences and reviewer to several journals and conferences, such as IEEE TPDS, IEEE JSAC, IEEE TCC, IEEE TNNLS, Wiley Software: Practice and Experience, etc. 
% {\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{ckdehury}}

% \par 
% \noindent
\begin{wrapfigure}{i}{1in}
\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bveeravalli}
\end{wrapfigure}
\textbf{Bharadwaj Veeravalli} received his BSc degree in Physics, from Madurai-Kamaraj University, India, in 1987, the Master's degree in Electrical Communication Engineering from the Indian Institute of Science, Bangalore, India in 1991, and the PhD degree from the Department of Aerospace Engineering, Indian Institute of Science, Bangalore, India, in 1994. He received gold medals for his bachelor degree overall performance and for an outstanding PhD thesis (IISc, Bangalore India) in the years 1987 and 1994, respectively. He is currently with the Department of Electrical and Computer Engineering, Communications and Information Engineering (CIE) division, at The National University of Singapore, Singapore, as a tenured Associate Professor. His main stream research interests include cloud/grid/cluster computing (big data processing, analytics and resource allocation), scheduling in parallel and distributed systems, Cybersecurity, and multimedia computing. He is one of the earliest researchers in the field of Divisible Load Theory (DLT). He is currently serving the editorial board of IEEE Transactions on Parallel and Distributed Systems as an associate editor. He is a senior member of the IEEE and the IEEE-CS.
	
% \par
% \noindent
\begin{wrapfigure}{i}{1in}
\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{srirama}
\end{wrapfigure}
\textbf{Satish Narayana Srirama}
	is an Associate Professor at the School of Computer and Information Sciences, University of Hyderabad, India. He is also a Visiting Professor and the honorary head of the Mobile \& Cloud Lab at the Institute of Computer Science, University of Tartu, Estonia, which he led as a Research Professor until June 2020. He received his PhD in computer science from RWTH Aachen University, Germany in 2008. His current research focuses on cloud computing, mobile web services, mobile cloud, Internet of Things, fog computing, migrating scientific computing and enterprise applications to the cloud and large-scale data analytics on the cloud. He is IEEE Senior Member, an Editor of Wiley Software: Practice and Experience, a 52 year old Journal, was an Associate Editor of IEEE Transactions in Cloud Computing and a program committee member of several international conferences and workshops. Dr. Srirama has co-authored over 170 refereed scientific publications in international conferences and journals. 

% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%
\newpage
\setcounter{page}{1}
%\appendices
\appendix
\section{Time complexity of HeRAFC}
\subsection{Running time of Algorithm \ref{algo:HeRAFC:tskOrdr}}\label{appen:TimeCompAlgo1}
\begin{theorem}
	The time complexity of the Algorithm \ref{algo:HeRAFC:tskOrdr} is $ \mathcal{O}(|V'| \log{}|V'|)$ , where $V'$ be the set of tasks.
\end{theorem}
\begin{proof}
	Algorithm \ref{algo:HeRAFC:tskOrdr} focuses on ordering of the tasks for deployment purpose. $ST$ contains $|V'|$ number of tasks. Sorting of $|V'|$ tasks based on the number of out-edges depends on the sorting algorithm. This sorting time can be reduced to $\mathcal{O}(|V'| \log{}|V'|)$ with QUICKSORT algorithm, as in Line \ref{algo:HeRAFC:tskOrdr:sortTask}. Following this, extraction of the tasks in Line \ref{algo:HeRAFC:tskOrdr:extrctTaskLst} would need a constant time as the list of the tasks is already in the previous step. The looping block from Line \ref{algo:HeRAFC:tskOrdr:MCVstart}-\ref{algo:HeRAFC:tskOrdr:MCVend} would depend on the size of $TL$ set. Furthermore, the sorting process in Line \ref{algo:HeRAFC:tskOrdr:sortExtTskLst} would consume the time-complexity of $\mathcal{O}(|TL| \log{}|TL|)$. The extraction process in Line \ref{algo:HeRAFC:tskOrdr:extractPrntTask} would consume a running time of $\mathcal{O}(|TL|)$. The outer \emph{while} loop from Line \ref{algo:HeRAFC:tskOrdr:LoopStart}-\ref{algo:HeRAFC:tskOrdr:LoopEnd} would depend on the maximum depth of the graph and hence would iterate for $|V'|$ (which is nothing but the size of set $ST$) number of times. This situation would occur if all the tasks run in a sequential manner, resulting only one task in set $TL$ in each iteration. The running time of Algorithm \ref{algo:HeRAFC:tskOrdr} can be calculated as $\mathcal{O}(|V'| \log{}|V'|) + \mathcal{O}(|V'|))$, which can be written as $\mathcal{O}(|V'| \log{}|V'|)$, where $|V'|$ is the number of tasks.
\end{proof}

\subsection{Running time of Algorithm \ref{algo:HeRAFC}}\label{appen:TimeCompAlgo2}
\begin{theorem}
	The time complexity of the proposed HeRAFC algorithm (Algorithm \ref{algo:HeRAFC}) is $\mathcal{O}(|E'|\left[\log{}|E'| + |E|log(|V|)\right]$ , where $E$ and $E'$ are the set of physical edges and task edges, respectively and $V$ is the number of cloud and FNs.
\end{theorem}
\begin{proof}
	Algorithm \ref{algo:HeRAFC} depends on the output from the Algorithm \ref{algo:HeRAFC:tskOrdr} (Line \ref{algo:HeRAFC:callAlgo1}), which is having a running time of $\mathcal{O}(|V'| \log{}|V'|)$, where $|V'|$ is the number of tasks. Calculation of availability matrix takes a constant running time. The \emph{for} loop at Line \ref{algo:HeRAFC:nodeMap:innerLoop:start} iterate for each task in the set $ETL$. For each task, finding the FNs (Line \ref{algo:HeRAFC:nodeMap:innerLoop:start}-\ref{algo:HeRAFC:nodeMap:innerLoop:end}) that are in the communication range of user, at 1-hop distance, and at 2-hop distance would take the running time of $\mathcal{O}(|V|)$, where $|V|$ is the set of cloud and FNs as defined in Section \ref{sec:MFCmodel}. This process will be repeated for $|V'|$ number of times, the total running time can be calculated as $\mathcal{O}(|V'| * |V|)$.
	
	Sorting of edges of the task graph, as in Line \ref{algo:HeRAFC:EdgeSorting}, would take a running time of $\mathcal{O}(|E'| \log{}|E'|)$, where $|E'|$ is the number of task edges. For every task edge, an existing shortest path algorithm can be implemented. In Line \ref{algo:HeRAFC:shrtstPath}, assuming that Dijkstra shortest path algorithm is implemented to find the shortest path from FN $sh$ to $th$, the worst case running time for every task edge would be $\mathcal{O}((|V|+|E|)log(|V|))$, where $|E|$ is the number of physical edges among cloud and FNs. The looping block in Line \ref{algo:HeRAFC:EdgeMapping:start} would iterate for $|E'|$ number of times and hence the total running time can be calculated as $\mathcal{O}(|E'|*(|V|+|E|)log(|V|))$. The total running time of Algorithm \ref{algo:HeRAFC} can be calculated as follows.
	
	\begin{equation}
	\begin{aligned}
	\mathcal{O}(|V'| \log{}|V'|) + \mathcal{O}(|V'|*|V|) + \mathcal{O}(|E'| \log{}|E'|) & \\ \nonumber 
	+ \mathcal{O}(|E'|*(|V|+|E|)log(|V|)) & \\
	= \mathcal{O}(|V'| \log{}|V'|) + \mathcal{O}(|V'| * |V|) + \mathcal{O}(|E'| \log{}|E'|) & \\ 
	+ \mathcal{O}(|E'|*|V|log(|V|)) + \mathcal{O}(|E'|*|E|log(|V|)) \nonumber
	\end{aligned}
	\end{equation}
	In case of a larger task graph, the number of task edges, $|E'|$, increases significantly as compared to the number of tasks, $|V'|$. Similarly, when the number of cloud and FNs increases, relatively the number of physical edges, $|E|$, increases significantly. Considering this scenario, the total running time can be calculated as follows.
	\begin{equation}
	\begin{aligned}
	\mathcal{O}(|E'| \log{}|E'|) + \mathcal{O}(|E'|*|E|log(|V|)) & \\ 
	=  \mathcal{O}(|E'|\left[\log{}|E'| + |E|log(|V|)\right])  & \nonumber
	\end{aligned}
	\end{equation}
\end{proof}


%ELSEVIER : biography section
%\pagebreak

%\section*{ }
%\noindent \textbf{Chinmaya Kumar Dehury} received BCA degree from Sambalpur University, India, in June 2009 and MCA degree from Biju Pattnaik University of Technology, India, in June 2013. He received the PhD Degree in the department of Computer Science and Information Engineering, Chang Gung University, Taiwan. Currently, he is a postdoctoral research fellow in the Mobile \& Cloud Lab, Institute of Computer Science, University of Tartu, Estonia. His research interests include scheduling, resource management and fault tolerance problems of Cloud and fog Computing.
%\subsection*{  } % This subsection (with no heading) is added to give more space between two biographies
%\noindent \textbf{Dr. Satish Narayana Srirama}  is a Research Professor and the head of the Mobile \& Cloud Lab at the Institute of Computer Science, University of Tartu, Estonia. He received his Ph.D. in computer science from RWTH Aachen University, Germany in 2008. His research focuses on cloud computing, mobile web services, mobile cloud, Internet of Things, fog computing, migrating scientific computing and enterprise applications to the cloud and large scale data analytics on the cloud. He is an IEEE member, was an Associate Editor of IEEE Transactions in Cloud Computing, is an Editor of Wiley Software: Practice and Experience, a 49 year old Journal, and a program committee member of several international conferences and workshops. Dr. Srirama has co-authored over 120 refereed scientific publications in several inter-national conferences and journals. For further information of Prof. Srirama, please visit: http://kodu.ut.ee/~srirama/. 




\section{Simulated MFC environment and Applications}\label{appen:simEnv}
\revison{This section provides an in-depth understanding of programmatically generated application and MFC environment that following several distribution functions. The entire simulation environment basically consists of two primary components: Applications and the fog-Cloud environment. The resource configuration of the FCIs, fog, cloud and the users' applications are presented in JSON file. Separate JSON files are created for each user's app, including the tasks list, edges list, resource demand of tasks and edges, and other related information.} % Figure \ref{fig:sim:AppJSON} shows a snippet of fog-cloud JSON file and Figure \ref{fig:sim:FogCloudJSON} shows the snippet of a JSON file for user application.

\subsection{Fog-Cloud Environment}
\input{Env_Analysis_Data.tex}
\input{Env_Analysis_Fig.tex}

\revison{The MFC environment consists of $500$ FNs that are connected to $200$ FCIs. Each FCI is connected to at least one FN, whereas a single FN is connected to precisely one FCI. For each FN, the total amount of CPU capacity ranges from 50 through 100 and the values are assigned by following a random distribution. The total memory (RAM) for each FN is assigned randomly ranging from $200 GB$ to $400 GB$, where the unit $GB$ represents GigaByte. The approximate computation power of a FN is calculated as MIPS (Million instructions per  second), which ranges from $3000$ to $5000$.} % 3GhZ processing speed is somewhat around 3000 MIPS or even more
\revison{As discussed before, each FN is connected to exactly one FCI. The maximum network bandwidth available between a FN and the corresponding FCI ranges from $300 Mbps$ through $400 Mbps$. In this paper, it is considered that a FCI may directly communicate with another FCI. In such a situation, there exist a network bandwidth capacity among FCIs, which ranges from $400 Mbps$ through $1000 Mbps$. Similarly, the bandwidth between FCIs and the cloud range from $400 Mbps$ through $1000 Mbps$. The network bandwidth unit is in Mbps (Mega bits Per Second). It is assumed that latency value between any FN and the corresponding FCI is small than the latency among FCIs and between FCIs and cloud. Latency values are assigned to the physical links by following a uniform distribution. The latency (in millisecond) between FN and FCIs ranges from $50 ms$ through $100 ms$. Similarly, the latency values for rest of the physical links range from $100 ms$ through $200 ms$. Technically, a task can be hosted on 1-hop or at a multi-hop distance. However, in the current implementation maximum number of hops is set to $2$. This means there are a maximum of two FCIs and no cloud are present in the physical path between any two FNs that are used to host user's tasks.} 


\revison{Figure }\ref{fig:sim:envAnalysis}(a)\revison{ and }\ref{fig:sim:envAnalysis}(b)\revison{ present the computing resource and network resource distribution, respectively, among the FNs. Figure }\ref{fig:sim:envAnalysis}(a)\revison{ shows the average CPU and memory resource availability among the FNs. Though the minimum and maximum CPU resource assigned to a FN is $50$ and $100$, respectively, the average CPU resource available is approximately $75$ and $73$ when there are $50$ and $500$ FNs, respectively. Similarly, the average memory resource available ranges between $145GB$ and $150GB$ with the same number of FNs, as shown in Figure }\ref{fig:sim:envAnalysis}(a)\revison{. Figure }\ref{fig:sim:envAnalysis}(b)\revison{ represents the relationship between the number of FNs and the average bandwidth and latency with the connected nearby FCI. The x-axis of the figure represents the number of FNs, while the y-axis represents the average bandwidth and average latency with FCI as two separate series. The figure shows that as the number of FNs increases, the average bandwidth with FCI remains relatively constant, fluctuating around the value between $348Mbps$ and $353Mbps$. However, the average latency with FCI also shows a slightly increasing trend, from $75.4 ms$ for $50$ FNs to $75.6 ms$ for $500$ FNs, with a maximum of $76.2$ for $350$ FNs. This means that as the number of FNs increases, the average latency with FCI slightly increases, but it remains relatively low overall. Figure }\ref{fig:sim:envAnalysis}(b)\revison{ indicates that increasing the number of FNs has little impact on the average bandwidth with FCI, while it slightly increases the average latency with FCI.}

\revison{Figure }\ref{fig:sim:envAnalysis}(c)\revison{ and }\ref{fig:sim:envAnalysis}(d)\revison{ show the relationship of the FCIs with the number of FNs and the network resource availability with the cloud. In Figure }\ref{fig:sim:envAnalysis}(c)\revison{, the x-axis denotes the number of FCIs, while the left y-axis represents the total number of FNs and the right-side y-axis represents the average number of FNs that are connected to the FCI. This shows how the FCIs are distributed and connected with all the $500$ FNs in the simulated environment. The total number of FNs connected to the FCIs is constantly increasing and as expected, $498$ number of FNs are connected to at least one FCI. In the simulated environment, only two FNs are directly connected with cloud. In other words, two FNs have no nearby FCI. The average number of FNs that are connected to the FCIs ranges between $2.4$ and $2.6$, as shown in Figure }\ref{fig:sim:envAnalysis}(c)\revison{. Each connection between an FCI and the cloud is associated with a certain amount of network bandwidth and latency, as shown in Figure }\ref{fig:sim:envAnalysis}(d)\revison{. The minimum and maximum network bandwidth available with any pair of FCI and cloud connection is $400 Mbps$ and $1000 Mbps$, respectively. However, the average bandwidth available among FCIs and the cloud ranges between $720Mbps$ and $740Mbps$. Similarly, in the simulated environment, the average latency between FCI and cloud ranges between $145ms$ and $150ms$, as shown in Figure }\ref{fig:sim:envAnalysis}(d). 

\subsection{Users Application}
\revison{The maximum number of applications is set to $10000$. For each application, random number of tasks are generated ranging from $4$ through $12$. However, the total maximum number of tasks for the entire simulation is set to $100,000$. For each task, the CPU and memory demand range from $1-4 CPUs$ and $100 MB - 1000 MB$, respectively. The average makespan of each task is $350 ms$ with minimum and maximum makespan of $10 ms$ and $1000 ms$, respectively. The network bandwidth demand of edges is ranging from $100 Mbps$ and $200 Mbps$. Similarly, the average latency demand among tasks ranges between $10 ms$ and $50 ms$. The unit of latency is millisecond (ms). The minimum and maximum priority values of a task are $1$ and $5$, respectively. The bandwidth and latency demand of a task ranges between $100-200 Mbps$ and $10-300 ms$, respectively. The number of edges within an application is decided by a link probability of $0.6$. This indicates the connectivity probability among two task. }

\input{App-Tasks_Analysis_Data.tex}
\input{App-Tasks_Analysis_Fig.tex}

\revison{Figure }\ref{fig:sim:appAnalysis}\revison{ gives an insight into the applications and their tasks, including the priorities and the resource demand. Figure B.11a shows the total and average number of tasks with different priority values. The priority values are assigned to the tasks in a programmatic manner. From Figure }\ref{fig:sim:appAnalysis}(a)\revison{, it can be observed that there are a total of $19154$, $20463$, and $21184$ number of tasks having priority $1$, $2$, and $3$. With a priority value $4$ there are a total of $17123$ number of tasks. In the entire simulation, the highest number of tasks have a priority value $5$, i.e. $22076$ tasks. The average number of tasks per application with a specific priority value is also analyzed and presented in the same figure. An average of $1.5$ number of tasks per application are with priority $4$, which is the lowest. On the other hand, an average of $3.75$ number of tasks per application have the priority $5$, which is the maximum, as shown in Figure }\ref{fig:sim:appAnalysis}(a)\revison{. Figure }\ref{fig:sim:appAnalysis}(b)\revison{ gives the tasks' resource configuration and makespan information with their priority. It can be seen that the tasks with priority $3$ are having highest CPU and memory demand, which is $3$ and $550MB$, respectively. Moreover, such tasks have comparatively lower makespan requirement, which is $503 ms$. The tasks with priority $2$ and $4$ have the higher makespan requirement of $505.8 ms$ and $505.9 ms$, respectively.}

\revison{The applications with average priority and the average number of tasks are also analyzed in Figure }\ref{fig:sim:appAnalysis}(c)\revison{. It can be observed that the average priority of the applications ranges between $2.5$ and $3.5$. The average number of tasks of $5000$ applications is $7.5$, whereas this number is reduced to $6.9$ when all the $10$ thousand applications are taken into account. The applications are further analyzed in terms of their resource (CPU, memory) demand and makespan requirement in Figure }\ref{fig:sim:appAnalysis}(d)\revison{. The average CPU demand with a different number of applications ranges between $1.7$ and $3.0$, whereas the memory demand ranges between $540 MB$ and $550 MB$. The average makespan requirement of all $10$  thousand applications is $500.6 ms$. The pattern shows that the distribution of CPU demand, memory demand, and makespan requirements are independent of each other.} 


\section{Uncertainty Analysis}\label{apndx:uncertainAna}
\subsection{Uncertain resource availability impact on utilization}

\revison{Figure }\ref{fig:sim:uncertaintyAnalysis}\revison{ represents the relationship of fluctuation of resource availability, including network bandwidth and latency among FNs, FCIs, and cloud with their utilization. In other simulation instances, the HeRAFC algorithm takes the resource availability values of the entire environment, which remains unchanged until the algorithm handles the last application. For example, suppose the CPU availability of a FN is $75\%$ of its maximum capacity before HeRAFC starts processing the first application. In that case, CPU availability remain unchanged until the last application, provided no task is assigned to that FN.} 

\revison{It is implemented in a way that when the proposed algorithm starts, the resource availability at FNs and at the cloud is not $100\%$ to represent a real-life scenario. In this simulation, the distribution of resource availability follows a random distribution. As shown in Figure }\ref{fig:sim:simulationTime:for_both_algos}\revison{, the total simulation time required to assign $10000$ applications is approximately $230 sec$. However, in this simulation, we randomly modify the resource availability of FNs and cloud at an interval of $20 sec$, as shown in Figure }\ref{fig:sim:uncertaintyAnalysis}\revison{. The X-axis of all subfigures represents the time interval at which the resource availability of the entire environment is modified. }

\revison{Figure }\ref{fig:sim:uncertaintyAnalysis}(a)\revison{ shows the modified CPU and memory resource availability at different time intervals. It can be seen that the initial CPU and memory resource availability at FNs was $45\%$ and $74\%$. After $20 sec$, the CPU and memory configurations of the FNs were changed to $67\%$ and $79\%$, respectively. A similar pattern can be seen in Figure }\ref{fig:sim:uncertaintyAnalysis}(b)\revison{, where bandwidth among FNs and FCIs (Blue solid line) and bandwidth among FCI and cloud (Red solid line) resource availability is modified. Bandwidth availability among FNs and FCI ranges between $35\%$ and $89\%$ across the entire simulation. On the other hand, bandwidth among FCIs and cloud ranges between $45\%$ and $70\%$. As shown in Figure }\ref{fig:sim:uncertaintyAnalysis}(c)\revison{, the latency value, calculated in milliseconds, among FNs and FCI is modified at every $20 sec$, and the value varies between $70 ms$ and $80 ms$. Similarly, the latency among FCIs and cloud, at every $20 sec$, ranges between $140 ms$ and $150 ms$.}  

\input{Uncertainty_Analysis_Data}
\input{Uncertainty_Analysis_Fig}

%% About the utilization performance
\revison{Under the above uncertain resource availability, Figures }\ref{fig:sim:uncertaintyAnalysis}(d)\revison{ - }\ref{fig:sim:uncertaintyAnalysis}(f)\revison{ represent the impact on resource utilization. It is observed that such uncertain conditions and fluctuated resource availability have a higher negative impact on utilization. Figure }\ref{fig:sim:uncertaintyAnalysis}(d)\revison{ shows the CPU and memory utilization of the FNs. In comparison with the results in Figure }\ref{fig:sim:compCPUUtil_F}\revison{ and }\ref{fig:sim:compMemUtil_F}\revison{, it is observed that the CPU and memory resources are being underutilized by approximately $10\%$. This can be observed in the figure where the maximum CPU and memory utilization are less than $60\%$ each. At the same time, this utilization is observed to be more than $65\%$, as in Figure }\ref{fig:sim:compCPUUtil_F}\revison{ and }\ref{fig:sim:compMemUtil_F}. 


\revison{A similar pattern is observed in the case of network bandwidth utilization. The maximum bandwidth utilization among FNs and among FCI and cloud is observed as $78\%$ and $40\%$, in Figure }\ref{fig:sim:compBWUtil_F}\revison{ and }\ref{fig:sim:compBWUtil_C}\revison{, respectively. However, this utilization is reduced by more than $10\%$ when the bandwidth resource availability fluctuates, as shown in Figure }\ref{fig:sim:uncertaintyAnalysis}(e)\revison{. The bandwidth utilization is less than $67\%$ among FNs and FCI, and the utilization among FCI and cloud is less than $30\%$. The uncertain latency among FNs, FCI, and cloud is also analyzed and presented the results in Figure }\ref{fig:sim:uncertaintyAnalysis}(f)\revison{. The latency of the network communications among FNs and FCIs increases from $155 ms$ to $300 ms$ while processing all the applications by HeRAFC algorithm, which is more than $30\%$ increase from the previous result, shown in Figure }\ref{fig:sim:Ltncy_F}\revison{. Similarly, the minimum latency among FCIs and cloud is $1570\%$, which is more than approximately $30\%$ of the previous result, as shown in Figure }\ref{fig:sim:Ltncy_C}\revison{. Overall this observation indicates that latency is the worst-performing parameter among others when the network communication becomes very unstable and fluctuates over time.}



\section{Simulation time analysis}\label{apndx:simTimeAna}
\revison{Figure }\ref{fig:sim:simulationTime}\revison{ helps in visualizing the performance of Algorithm }\ref{algo:HeRAFC:tskOrdr}\revison{ and }\ref{algo:HeRAFC}\revison{ in terms of simulation time taken for different numbers of applications, ranging from $5000$ to $10000$. The number of tasks ranges from $4$ to $12$ per application. However, the maximum number of tasks is set to $100$ thousand. The x-axis represents the number of applications, while the Y-axis in all the sub-figures shows the simulation time. The results suggest that the total simulation time for both algorithms increases as the number of applications increases. Figure }\ref{fig:sim:simulationTime}(a)\revison{ represents the time taken by Algorithm }\ref{algo:HeRAFC:tskOrdr}\revison{ and }\ref{algo:HeRAFC}\revison{ to process a certain number of applications. For instance, as in Figure }\ref{fig:sim:simulationTime:totaltime}\revison{, when there were $5000$ applications, Algorithm }\ref{algo:HeRAFC:tskOrdr}\revison{ takes a total of approximately $5$ seconds, while Algorithm }\ref{algo:HeRAFC}\revison{ takes more than $100$ seconds. Similarly, when the number of applications increases to  $10000$, Algorithm }\ref{algo:HeRAFC:tskOrdr}\revison{ takes approx. $10$ seconds, while Algorithm }\ref{algo:HeRAFC:tskOrdr}\revison{ takes more than $220$ seconds. It is observed (also expected) the total simulation time taken by Algorithm }\ref{algo:HeRAFC}\revison{ is consistently higher than that of Algorithm }\ref{algo:HeRAFC:tskOrdr}\revison{, across all tested scenarios.}   

\input{simulaitonTime_data.tex}
\input{simulaitonTime_fig.tex}

\revison{Similar to the above results, the average time to handle an application by Algorithm }\ref{algo:HeRAFC:tskOrdr}\revison{ and }\ref{algo:HeRAFC}\revison{ is also observed and presented in Figure }\ref{fig:sim:simulationTime}(b)\revison{. Algorithm }\ref{algo:HeRAFC:tskOrdr}\revison{ takes an average of $1.5ms$ when the number of applications is $5000$, whereas Algorithm }\ref{algo:HeRAFC}\revison{ takes an average of more than $20ms$ for the same number of applications, as shown in Figure }\ref{fig:sim:simulationTime}(b)\revison{. Algorithm }\ref{algo:HeRAFC:tskOrdr}\revison{ and }\ref{algo:HeRAFC}\revison{ took an average of $1.25ms$ and $21.9ms$ per application, respectively, when the number of applications increases to $10000$. 
Unlike Figure }\ref{fig:sim:simulationTime}(a)\revison{, presenting the time taken by individuals algorithms, Figure }\ref{fig:sim:simulationTime}(c)\revison{ presents the average and total time taken by both the Algorithms }\ref{algo:HeRAFC:tskOrdr}\revison{ and }\ref{algo:HeRAFC}\revison{. Figure }\ref{fig:sim:simulationTime}(c)\revison{ shows that approximately $114 sec$ is required to process $5000$ applications, with an average of $21ms$ per application. When the number of applications increases to $100000$, a total of $228sec$ (with an average of $23ms$ per application) is required to assign those applications to fog and cloud environments, as shown in Figure }\ref{fig:sim:simulationTime}(c)\revison{. Overall, the Figure }\ref{fig:sim:simulationTime}\revison{ provides a comparison of the simulation time performance of two algorithms for different application scenarios.}






\end{document}


% \begin{figure}[ht]
% 	\centering
% 	\subfigure
% 	{
% 		\includegraphics[width=30mm]{AppJSON.eps}
% 		\label{fig:sim:AppJSON}
% 	}
% 	\subfigure
% 	{
% 		\includegraphics[width=30mm]{FogCloudJSON.eps}
% 		\label{fig:sim:FogCloudJSON}
% 	}
% 	\vspace{-2mm}
% 	\caption{Snippet of configuration file in JSON format.}\vspace{-5mm}
% 	\label{fig:sim:JSON}
% \end{figure}

