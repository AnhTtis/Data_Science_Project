\begin{abstract}
    Building up reliable Out-of-Distribution (OOD) detectors is challenging, often requiring the use of OOD data during training.
    In this work, we develop a data-driven approach which is distinct and complementary to existing works: Instead of using external OOD data, we fully exploit the internal in-distribution (ID) training set by utilizing generative models to produce additional synthetic ID images.
    The classifier is then trained using a novel objective that computes weighted loss on real and synthetic ID samples together.
    Our training framework, which is termed SIO, serves as a ``plug-and-play'' technique that is designed to be compatible with existing and future OOD detection algorithms, including the ones that leverage available OOD training data.
    Our experiments on CIFAR-10, CIFAR-100, and ImageNet variants demonstrate that SIO consistently improves the performance of nearly all state-of-the-art (SOTA) OOD detection algorithms.
    For instance, on the challenging CIFAR-10 v.s. CIFAR-100 detection problem, SIO improves the average OOD detection AUROC of 18 existing methods from 86.25\% to 89.04\% and achieves a new SOTA of 92.94\% according to the OpenOOD benchmark.
    Code is available at \url{https://github.com/zjysteven/SIO}.
    %with the best score improving from 92.26\% to 92.94\%, according to OpenOOD.
\end{abstract}




%%% legacy abstract
\iffalse
\begin{abstract}
   Building up reliable Out-of-Distribution (OOD) detectors is challenging, which often requires dedicated algorithmic design or deliberate inclusion of OOD data for training.
   In this work, perhaps surprisingly, we identify that simply training with more in-distribution (ID) data can also significantly benefit OOD detection.
   We first observe this with real data.
   Then, to translate such observation into a practical approach, we propose a training framework named SIO, which leverages generative models (trained on ID data only) to produce additional synthetic ID samples, and train the classifier using a weighted loss computed on real and synthetic ID samples together.
   SIO is orthogonal to and compatible with existing OOD methodologies.
   In practice, SIO is implemented in a light-weight way which only modifies the batch sampling and does not introduce any extra computational overhead compared with standard training.
   Experimental results on CIFAR-10, CIFAR-100, and ImageNet variants demonstrate that SIO leads to consistent improvements on top of multiple OOD algorithms and yields new state-of-the-art results.
\end{abstract}
\fi


\section{Introduction}
\label{sec:intro}


Being able to identify unknowns is of the utmost importance for intelligent systems to reliably operate in open world settings.
In the domain of image classification, this challenge is known as \textit{Out-of-Distribution (OOD) Detection}. %(or \textit{Open Set Recognition}), 
The goal of OOD detection is to enable the classifier to identify samples that do not belong to one of the known, in-distribution (ID) categories during inference.
However, OOD detection has long been a difficult task due to the implicit closed-world assumption adopted by standard neural network training.
For instance, without certain efforts, a basic CIFAR-10 classifier will confidently predict SVHN digits as one of its classes \cite{whyrelu19cvpr}.


Recent years have seen plenty of OOD detection works, which we roughly divide into three categories.
1) \textit{Inference techniques} study post-hoc scoring rules that best separate ID and OOD data with a pre-trained model(the score indicates each sample's ``OOD-ness'') \cite{openmax16cvpr,msp17iclr,guo2017calibration,odin18iclr,energyood20nips,gradnorm21neurips,react21neurips,species22icml,species22icml,haoqi2022vim,sun2022knnood,sun2021dice}.
2) \textit{Specialized training algorithms} induce more suitable structure \cite{godin20cvpr,wei2022mitigating,cutmix19cvpr} or feature distribution \cite{vos22iclr} inside the model via training to allow for better OOD detection at inference time.
3) \textit{Data-driven methods} utilize additional input data for improvements.
Particularly, existing works incorporate external OOD samples to either let the model learn OOD detection in a supervised way \cite{oe18nips} or mix them with ID images as a data augmentation \cite{hendrycks2021pixmix,mixoe}.
Not surprisingly, data-driven ones are among the most effective methods as they explicitly bring in additional information outside the ID space.



\begin{figure}[t]
\centering
   \includegraphics[width=0.95\linewidth]{figures/intro_new.png}
   \vspace{-1mm}
   \caption{Results on CIFAR-10. SIO is able to yield improvements on top of multiple methods against both near- and far-OOD. See full results in Section \ref{sec:4.1} Table \ref{tab:cifar}.}
   \vspace{-3mm}
\label{fig:intro}
\end{figure}


In this work, we seek to further improve OOD detection by following along the data-driven path.
However, we think in an entirely different and orthogonal direction to existing approaches: While external OOD data could be helpful, \textit{we wonder if the original, internal ID data can be better exploited to benefit OOD detection.}
To answer this question, we propose using generative models (\eg, GANs \cite{stylegan2ada,biggan}, diffusion models \cite{ddpm}) to capture the ID data distribution and to produce additional synthetic ID images to augment the existing real ID set.


On the face of it, one may think that synthetic ID images would not benefit OOD detection and could even have a negative impact for two reasons.
First, since synthetic ID images do not provide any explicit information about the OOD samples, they may not seem useful for improving OOD detection.
Second, prior studies \cite{cas} have shown that naively adding synthetic images into the real training set can lower classification accuracy, which in turn may lead to inferior OOD detection performance according to the correlation between OOD detection rate and ID accuracy \cite{vaze2021open}.


Contrary to initial assumptions, we reveal that under a reasonable weighting scheme, synthetic ID images can indeed be advantageous when integrated with real ID images. 
Concretely, we propose a novel and generic training framework which employs a weighted sum of the loss computed on both real and synthetic ID images to train the classifier. 
We term this framework SIO (\underline{S}ynthetic \underline{I}D data for \underline{O}OD detection).
The proposed SIO framework is distinct and complementary to existing OOD detection methodologies.
%Compared with real data-only training, SIO introduces additional synthetic ID images to the ID training set without modifying other parts of the training pipeline.
It can be seen as a ``plug-and-play'' technique that can be easily integrated into most (if not all) dedicated OOD detection approaches, including those that incorporate OOD samples in the training phase (see Section \ref{sec:3.2} for detailed discussion).
Practically, we design a lightweight implementation of SIO that only modifies the ID batch sampling. 
This guarantees that SIO does not introduce any computational overhead compared to real data-only training, facilitating efficient use and fair comparison.


We conduct a thorough evaluation of SIO in combination with a variety of state-of-the-art OOD detection methods on two widely-used image classification datasets, CIFAR-10 and CIFAR-100, using the OpenOOD benchmark \cite{yang2022openood}. 
Our results indicate that SIO consistently improves OOD detection performance on top of multiple advanced OOD methods (see Figure \ref{fig:intro} for some of the results on CIFAR-10). 
Notably, by applying SIO we achieve new state-of-the-art results in 3 out of the 4 evaluation settings within the OpenOOD benchmark.
To demonstrate scalability to high-resolution images, we further experiment with two ImageNet variants. 
Additionally, we conduct extensive analyses to evaluate the robustness of SIO against hyperparameters, such as the choice of generative models.
Finally, we observe that SIO improves OOD detection rates even if it does not improve ID classification accuracy, providing a more comprehensive view of the OOD detection rate vs. ID accuracy correlation \cite{vaze2021open}.





\iffalse

%In this work, we seek to further improve OOD detection by following along the data-driven path.
%However, we think in a different/orthogonal direction: \textit{Rather than collecting external OOD data, we wonder if more ID data can benefit OOD detection.}
%To answer this question, we perform an initial experiment on CIFAR-10.
%We augment the original training set with another 500K CIFAR-10 images provided by \cite{carmon2019unlabeled}, which are collected from the CIFAR-10's superset Tiny Images.
%We then train two classifiers with a fixed number of gradient steps, one using the original CIFAR-10 training set, and the other using the augmented set.
%The second model immediately outperforms the first one by \textgreater2\% in terms of OOD detection AUROC, demonstrating that training with a larger, more diverse ID set indeed helps with OOD detection.

This result suggests that increasing the diversity of the ID data samples is a 
% This result suggests that ID data augmentation is a
%Such observation points a novel and 
promising direction for improving OOD detection.
However, curating additional real ID training samples requires significant effort which may not always be feasible.
To translate our observation into a practical approach, we propose to leverage generative models (\eg, GANs \cite{stylegan2ada,biggan}, diffusion models \cite{ddpm}) \textemdash{} which are trained with the existing, real ID training set \textemdash{} to generate additional synthetic ID samples and use them to train a better OOD detector.
We term this training framework SIO.
Beyond the high-level idea, there are a few key features and designs about SIO:




\begin{itemize}
    \item The generative models in SIO do \textit{not} have any OOD considerations: It is trained with real ID training data only and is used to produce ID data, unlike many works that attempted to generate OOD data \cite{gen-openmax,counterfactual,confcal18iclr}.
    This allows us to sidestep the difficulty of modeling the OOD data distribution which is much less well-defined.
    
    \item When training the classifier, SIO uses a weighted loss on real and synthetic ID samples together to avoid any bias towards synthetic distribution. 
    For efficiency, in practice SIO is implemented by replacing a certain amount of real samples with synthetic ones within each mini-batch. 
    This ensures that SIO does \textit{not} introduce any computational overhead compared with its real data-only counterpart.
    
    \item By nature SIO is a generic framework that is orthogonal to existing OOD methodologies. 
    Thanks to its light-weight implementation (which only modifies the batch sampling), SIO can be can be seamlessly integrated with most (if not all) dedicated OOD algorithms.
    
\end{itemize}


We comprehensively evaluate SIO by combining it with a wide range of specific OOD methods on CIFAR-10 and CIFAR-100 using the OpenOOD benchmark \cite{yang2022openood}.
See Figure \ref{fig:intro} for some results on CIFAR-10.
We find that SIO consistently improves the detection performance on top of multiple advanced OOD methods.
More importantly, by applying SIO we are able to obtain new state-of-the-art results in 3 out of 4 evaluation settings (according to OpenOOD).
We also confirm that SIO can scale to high-resolution images with experiments on two ImageNet variants.
Lastly, we demonstrate that SIO is fairly robust to its hyperparameters such as the type of generative model to use.


\fi



%However, we think in a different/orthogonal direction: Instead of collecting external OOD data, \textit{we wonder if we can better exploit the original, internal ID data to benefit OOD detection.}
%To this end, we propose to utilize generative models (\eg, GANs \citeph{}, diffusion models \citeph{}) \textemdash{} which are trained using the ID training set \textemdash{} to generate extra synthetic ID samples and use them to train a better OOD detector.
%The intuition is that the additional synthetic samples can introduce higher diversity to the ID training set, which may help the model learn more semantically-meaningful features rather than spurious/shortcut features \citeph{} which can be activated by OOD samples.



