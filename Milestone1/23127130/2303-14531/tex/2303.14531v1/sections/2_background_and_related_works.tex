\section{Background}
\label{sec:background}

\subsection{Problem statement}

In this work we consider OOD detection in the context of multi-class image classification, where the goals are 1) training a base classifier that can accurately classify ID data and 2) building an OOD detector on top of the base classifier that accurately distinguishes OOD from ID samples.
We now formulate this problem to facilitate the discussion.


\noindent \textbf{Training.}
There are in general two types of training schemes adopted by existing works.
The first one trains the base classifier $f$ only on the labeled (real) ID training data sampled from the in-distribution set $\Din$:
\begin{equation}
    \min_{f}\mathbb{E}_{(\bm{x},y)\sim \Din}L(\bm{x},y;f).
\label{eq:id_only_training}
\end{equation}
Here, Equation \ref{eq:id_only_training} is a high-level abstraction of the true optimization objectives used in practice.
For instance, $L$ could be as simple as the standard cross-entropy loss, \ie, $L(\bm{x},y;f)=H(y,\sigma(f(\bm{x})))$, where $f(\bm{x})$ is the logit vector, $\sigma$ is the softmax function, and $y$ is the one-hot representation of the ground-truth label.
$L$ could also involve additional regularization as in \cite{vos22iclr,wei2022mitigating}.


The other type of training assumes the availability of an external set of unlabeled OOD/outlier samples $\Dout$ and trains the classifier with ID and OOD samples together:
\begin{equation}
    \min_{f}\mathbb{E}_{(\bm{x},y)\sim\Din,\hat{\bm{x}}\sim\Dout}L(\bm{x},y,\hat{\bm{x}};f).
\label{eq:id_ood_training}
\end{equation}
For example, the loss function of Outlier Exposure (OE) \cite{oe18nips} is $L(\bm{x},\hat{\bm{x}},y;f)=H(y,\sigma(f(\bm{x}))+\lambda\cdot H(\mathcal{U},\sigma(f(\hat{\bm{x}}))$, where $\mathcal{U}$ is the uniform distribution across the known categories, and $\lambda$ is a weighting term.
Later in Section \ref{sec:3.2}, we will demonstrate the compatibility of our proposed SIO with both of these two training schemes.



\iffalse
Given a labeled ID training set $\Din$, we train a base classifier $f$ with certain loss function $L$:
\begin{equation}
    \min_{f}\mathbb{E}_{(\bm{x},y)\sim \Din}L(\bm{x},y;f).
\label{eq:general_training}
\end{equation}
We remark that this equation and $L$ are just high-level abstractions of the true optimization objectives that are used in practice.
Most basically, $L$ could be as simple as the standard cross-entropy loss, \ie, $L(\bm{x},y;f)=H(y,\sigma(f(\bm{x})))$, where $f(\bm{x})$ is the logit vector, $\sigma$ is the softmax function, and $y$ is the one-hot representation of the ground-truth label.
$L$ could also involve additional regularization terms like in \cite{vos22iclr}.


Later in Section \ref{sec:3.2} we will demonstrate the compatibility of our proposed SIO with existing training methods, including those that incorporate OOD samples.
To this end, here we provide the training objective of Outlier Exposure (OE) \cite{oe18nips} as an example:
\begin{equation}
    \min_{f}\left[\mathbb{E}_{(\bm{x},y)\sim \Din}H(y,\sigma(f(\bm{x}))+\lambda\cdot\mathbb{E}_{\hat{\bm{x}}\sim\DoutOE}H(\mathcal{U},\sigma(f(\hat{\bm{x}}))\right].
\label{eq:oe_training}
\end{equation}
Given the available set of OOD/outlier samples $\DoutOE$, 



Furthermore, Equation \ref{eq:general_training} can be extended to $\min_{f}\mathbb{E}_{(\bm{x},y)\sim \Din,\hat{\bm{x}}\sim\DoutOE}L(\bm{x},\hat{\bm{x}},y;f)$ if an external set of outlier data $\DoutOE$ is available \cite{oe18nips}.
\fi


\noindent \textbf{Inference.}
After training, the OOD detector $g$ is built upon the base classifier $f$ with a scoring module $s$:
\begin{equation}
    g(\bm{x};f,\tau)= 
    \begin{cases}
        \text{OOD,} & \text{if}~ s(\bm{x};f) \geq \tau\\
        \text{ID,}              & \text{otherwise}
    \end{cases}.
\label{eq:detector}
\end{equation}
The scoring module $s$ will assign a score to each sample which indicates its ``OOD-ness''.
Again, $s$ is a high-level representation of the scoring mechanism.
Examples of $s$ include $s(\bm{x};f)=\max_i\sigma(f(\bm{x})_i)$ (MSP \cite{msp17iclr}) and $s(\bm{x};f)=\sum_{i=1}^K e^{f(\bm{x})_i}$ (EBO \cite{energyood20nips}; $K$ is the number of ID categories).
Here $\tau$ is an application-dependent threshold.
The detector $g$ is used to determine whether each incoming sample is ID or OOD.
For OOD samples, the base classifier will refrain from making any predictions.



\subsection{Related works}

\noindent \textbf{1) OOD detection methodologies.}
There have been many works on OOD detection since it emerged as a research problem.
We refer readers to \cite{survey} for a comprehensive survey, while we focus on several top-performing methods that we consider in this work.
As aforementioned, we roughly categorize existing works into three groups.


The first line of works focus on the design of the post-hoc scoring rule $s$ (Equation \ref{eq:detector}), while assuming that the base classifier is pre-trained (usually with the standard cross-entropy loss).
In general, the outputs from the model's decision space (\eg, the final linear layer or the penultimate layer) could derive informative scores.
For example, MSP \cite{msp17iclr} and MLS \cite{species22icml} directly use the (negative) maximum softmax probability and maximum logit value as the score, respectively.
Other works post-process the network's outputs to enlarge the difference between ID and OOD.
Examples include softening softmax probabilities with temperature scaling \cite{guo2017calibration}, applying energy function to the logits \cite{energyood20nips}, rectifying activations with thresholding \cite{react21neurips}, and applying KNN to the penultimate layer's features \cite{sun2022knnood}, etc.


While some researchers focus on inference techniques, others investigate specialized training algorithms that involve developing more advanced $L$ in Equation \ref{eq:id_only_training}.
Notably, G-ODIN \cite{godin20cvpr} trains a dividend/divisor structure to decompose the softmax confidence.
VOS \cite{vos22iclr} encourages the learned representations of the classifier to shape like a mixture of Gaussian distributions.
LogitNorm \cite{wei2022mitigating} trains the model with normalized logit vectors as the unit sphere provides more discriminative information for distinguishing ID and OOD samples.
Meanwhile, using certain tricks such as longer training, stronger data augmentation, and more complex learning rate schedule has also been shown to benefit OOD detection \cite{vaze2021open}.


The final group of works proposes data-driven approaches to improve OOD detection.
In particular, OE \cite{oe18nips} collects real OOD samples and explicitly lets the model learn OOD detection in a supervised manner.
PixMix \cite{hendrycks2021pixmix} applies pixel-level mixing operations between ID images and low-level OOD images (which exhibit certain visual patterns but do not have clear semantics) as an augmentation.
Additionally, pre-training with a huge labeled dataset has been shown to be beneficial for OOD detection in downstream tasks (\eg, using ImageNet pre-training for CIFAR-10) \cite{hendrycks2019using}.
\textit{It is noteworthy that all these data-driven methods require external data beyond the original ID training set.}


While SIO, like other data-driven approaches, introduces additional input images, it stands out from existing methods by utilizing synthetic ID images instead of external OOD images.
This approach is particularly useful in data-scarce scenarios, where external data may not be readily available.
Furthermore, SIO is inherently complementary to all previously discussed methods, including data-driven ones. 
We provide detailed discussions and empirical evidence to support this claim in the subsequent sections.



%We highlight that the key distinction between our work and existing data-driven works is that SIO exploits the internal ID data rather than relying on external data, which we believe is more feasible in certain data-scarce scenarios.
%More importantly, by nature SIO is orthogonal to all discussed methods, including data-driven ones.
%Later we will confirm this with empirical results.



\noindent \textbf{2) Synthetic images for OOD detection.}
Although previous studies in OOD detection research have used synthetic images produced by generative models, they have all focused on synthesizing OOD images \cite{gen-openmax,counterfactual,confcal18iclr}.
However, synthesizing OOD images is a challenging task as there could be a lack of real OOD data for supervision, and the distribution of the open space is too broad to capture. 
As a result, previous attempts have not yielded superior performance, with models trained on synthetic OOD images underperforming those trained on real OOD data \cite{oe18nips} or even simple ID-only training baselines \cite{vaze2021open}.



%Leveraging synthetic images produced by generative models is not entirely new in OOD detection research.
%Yet, previous works all proposed to synthesize OOD images \cite{gen-openmax,counterfactual,confcal18iclr}.
%This is a challenging task itself since there lacks real data as supervision, and the distribution of the open space is too broad to capture.
%As a result, previous attempts did not lead to superior performance, where models trained with synthetic OOD samples underperform the one using real OOD data \cite{oe18nips} or even the simple ID-only training baseline \cite{vaze2021open}.


In sharp contrast, what we propose is using generative models to synthesize ID images, which is much easier a task since the target distribution is well-defined (characterized by the ID training set).
In addition, we demonstrate that incorporating synthetic ID samples can significantly improve the performance of multiple OOD detection methods.



\begin{figure}[t]
\centering
   \includegraphics[width=0.95\linewidth]{figures/method_new.png}
   \caption{Overview of the proposed SIO framework. First, a large number of synthetic ID samples are generated (offline) using a generative model that is trained with real ID images. Then, we combine the real and synthetic ID samples to train the classifier in a weighted fashion described in Equation \ref{eq:SIO_id_only_training}. The specific loss function can be selected from any existing OOD training methods. External OOD samples can also be incorporated into the training depending on their availability (Equation \ref{eq:SIO_id_ood_training}).
   }
   \vspace{-2mm}
\label{fig:method}
\end{figure}




\noindent \textbf{3) Synthetic samples for other performance measures.}
Expanding the real training set with synthetic samples has been demonstrated to help with adversarial training \cite{gowal2021improving}, where overfitting is the primary challenge that hinders better adversarial robustness.
In standard, non-adversarial settings, however, including synthetic samples has not yet led to improvements in, \eg, classification accuracy \cite{cas}.
To the best of our knowledge, we are the first to investigate and demonstrate that synthetic ID images can enhance OOD detection.
Our study provides a new avenue for exploring the use of synthetic data in other areas of machine learning.




%However, it is unclear whether extra (synthetic) ID data is beneficial for OOD detection: At first glance, one may think the opposite way since having more ID data does not introduce any information about OOD (at least not explicitly).
%By confirming that SIO is effective, our work not only obtains performance improvements but may also open up new directions for OOD detection.
