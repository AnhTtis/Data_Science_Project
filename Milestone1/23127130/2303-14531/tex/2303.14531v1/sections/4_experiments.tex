\section{Experiments}

We demonstrate the effectiveness of additional synthetic ID data for OOD detection through extensive experiments and analyses, starting with results in settings where CIFAR-10/100 \cite{cifar} is used as the ID dataset.
We then scale up to high-resolution settings with ImageNet splits.
Finally, we analyze SIO's robustness to hyperparameters.



\subsection{CIFAR}
\label{sec:4.1}
% We adapt OpenOOD's benchmarking setup for generating fair comparisons between state-of-the-art OOD detection methods.
%Benchmarking and comparing OOD detection methods used to be difficult, as there lacked a platform with unified implementations and benchmarks where one can perform apple-to-apple comparison.
%Fortunately, there is a recent work named OpenOOD \cite{yang2022openood} that can serve as such benchmark platform.
%Here we mostly adopt OpenOOD's setup, while applying a few necessary changes.

Benchmarking OOD detection methods used to be challenging because there was no unified platform with standardized implementations and benchmarks, making it difficult to make direct comparisons.
However, the recent work called OpenOOD \cite{yang2022openood} provides such a platform that enables fair and accurate benchmarking.
In this work, we use OpenOOD's setup as a basis for our experiments while making some modifications where necessary.



\noindent \textbf{Baselines.}
Since SIO is orthogonal to existing OOD detection methodologies, we demonstrate the effectiveness of SIO by combining it with multiple OOD detection approaches and comparing the results with the real data-only counterpart in each case.
Specifically, we consider 11 inference techniques \cite{openmax16cvpr,msp17iclr,guo2017calibration,odin18iclr,energyood20nips,react21neurips,species22icml,species22icml,haoqi2022vim,sun2022knnood,sun2021dice}, 5 specialized training algorithms \cite{godin20cvpr,wei2022mitigating,cutmix19cvpr,vos22iclr,vaze2021open}, and 2 data-driven methods \cite{hendrycks2021pixmix,oe18nips}, resulting in a total of 18 OOD methods.
All of them are the top-performing ones according to the OpenOOD benchmark.


\noindent \textbf{Training setup.}
We use ResNet-18 \cite{resnet} as the classifier architecture, following OpenOOD.
Regardless of the training algorithm, we train the model using Nesterov SGD with a momentum of 0.9. 
The initial learning rate is set to 0.1 and is decayed according to the cosine annealing schedule \cite{loshchilov2017sgdr}. 
A weight decay of 0.0005 is applied during training, and the batch size is set to 128. 
The only deviation from the OpenOOD default configuration is that we train each model for 200 epochs instead of 100 epochs, as longer training has been shown to improve OOD performance \cite{vaze2021open}.
For method-specific hyperparameters, we use the default or recommended values provided by OpenOOD.


For our proposed SIO, we generate 1000K synthetic ID samples using StyleGAN2 \cite{stylegan2ada} for both the CIFAR-10 and CIFAR-100 datasets.
The weighting/ratio factor $\alpha$ (Equation \ref{eq:SIO_id_only_training} and \ref{eq:SIO_id_ood_training}) is set to 0.8, \ie, there will be 20\% synthetic ID samples in each ID training batch.
We remark that the SIO model and the real data-only model undergo the exact same number of gradient steps and share the same batch size, ensuring a fair comparison.
The only difference is that the SIO model sees more diverse training samples by leveraging synthetic ID data.


\noindent \textbf{Evaluation setup.}
For CIFAR-10/100, we consider CIFAR-100/10 as near-OOD and MNIST \cite{deng2012mnist}, SVHN \cite{svhn}, Texture \cite{dtd}, and Places365 \cite{zhou2017places} as far-OOD.
Far-OOD samples are semantically far away from the ID samples and meanwhile often exhibit significant low-level, non-semantic shifts as well (due to data collection differences) \cite{ahmed2020detecting, tack2020csi}, making them easier to detect.
Near-OOD samples are more similar to ID samples in both semantic and non-semantic aspects and are more challenging to identify.


We follow the OpenOOD setting with one exception: We remove Tiny ImageNet samples from the near-OOD split since they are used as the training OOD samples for OE \cite{oe18nips}.
If not removed, the training and test OOD distributions would completely overlap, resulting in a trivial case.


We use the area under the receiver operating characteristic curve (AUROC) as the metric for evaluation.
AUROC is a threshold-independent metric for binary classification; the higher the better, and the random-guessing baseline is 50\%.
We report the detection AUROC against near- and far-OOD (averaged over the OOD sets in each split) for each method.
We also report the classification accuracy on ID test data, as a good algorithm should not trade-off ID accuracy for OOD detection performance.
All reported results are averaged over 3 independent training runs.



\input{tables/imagenet_results}



\noindent \textbf{Results.}
See Table \ref{tab:cifar}.
We start with discussing CIFAR-10 results.
The first takeaway is that SIO works well with nearly all OOD methods and helps with both near- and far-OOD detection, leading to noticeable improvements in 35 out of 36 cases (18 methods $\times$ \{near-OOD, far-OOD\}).
On average, SIO improves the near-OOD and far-OOD AUROC from 86.25\% to 89.04\% and from 92.05\% to 94.11\%, respectively.


Interestingly, we find that the performance gains brought by SIO can sometimes surpass those achieved through dedicated algorithmic design.
For example, SIO improves the near-OOD / far-OOD AUROC of the MSP detector from 86.71\% / 91.37\% to 89.86\% / 93.59\%, outperforming the more complex KNN detector on the baseline model, which achieves 89.38\% / 93.51\% at the cost of significantly increased inference latency \cite{timing}.
Furthermore, we confirm that SIO is compatible with data-driven methods that incorporate external OOD data (OE and PixMix), suggesting that the information contained in synthetic ID samples is complementary to that in external OOD samples.


Our second takeaway is that SIO improves the state-of-the-art result on the challenging CIFAR-10 near-OOD detection from 92.26\% to 92.94\%.
On CIFAR-10 far-OOD detection, SIO does not outperform vanilla OE, which incorporates a large set of external OOD samples for training.
We suspect that this is because far-OOD detection on CIFAR-10 is dominated by low-level statistics \cite{ahmed2020detecting,tack2020csi}, and using synthetic ID samples may slightly push the model away from the real low-level statistics of the ID data, causing a shrinked difference between ID and OOD samples.
Nonetheless, SIO improves far-OOD detection in all other cases and effectively closes the gap between non-OE methods and OE. 
Notably, LogitNorm + SIO yields a 97.09\% AUROC without any OOD training data, which is on par with the 97.16\% AUROC achieved by OE.



Lastly, we observe that SIO can also benefit ID classification accuracy.
While this is not the focus of this work, our finding suggests that synthetic samples can be helpful for accuracy if used properly, challenging previous beliefs \cite{cas}.
On the other hand, however, in our later experiments where we vary the hyperparameters, we find that SIO consistently boosts OOD detection performance even if it does not improve ID accuracy.
More discussion on this can be found in Section \ref{sec:analyses}.


On CIFAR-100, we observe similar results to CIFAR-10, with the general observation that SIO can benefit OOD detection performance. 
With SIO, the average near-OOD / far-OOD AUROC is lifted from 76.63\% / 80.29\% to 77.20\% / 83.65\%, and the best numbers are improved from 80.22\% / 87.54\% to 80.34\% / 89.37\%.



\subsection{Scaling to high-resolution images}


We now investigate whether SIO's effect can extend to high-resolution images at the scale of ImageNet.
However, we note that generative modeling on certain ImageNet categories remains a challenging problem due to inherent difficulties in the data \cite{biggan,cas}.
For example, the category \texttt{tench} includes many images depicting the fish being held by human beings, which causes the generative model to learn irrelevant information unrelated to the target object itself.
Consequently, generated images may appear unnatural and deviate significantly from the true distribution (see Appendix \ref{sec:ap_inet} Figure \ref{fig:inet_examples} for visual examples).
In such cases, including unrealistic synthetic images during training is unlikely to be beneficial.
This issue, however, is related to generative modeling rather than our proposed SIO and is expected to be mitigated as generative modeling techniques continue to advance. 
For our experimental purpose, here we utilize two subsets of ImageNet categories where plausible images can be generated. 
This allows us to examine the effectiveness of SIO in high-resolution settings with limited generative modeling challenges.



\begin{figure*}[t]
\centering
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=0.9\linewidth]{figures/gen_model.png}
%\caption{figure caption goes here}
%\label{fig: figure-label}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\centering
%\captionsetup{type=table} %% tell latex to change to table
\resizebox{0.65\linewidth}{!}{
\begin{tabular}{lcc}
     data source & FID$\downarrow$
     & AUROC$\uparrow$ \\ \midrule 
     DDPM* & 3.17 & 90.22 \\
     ReACGAN & 4.40 & 89.29 \\
     MHGAN & 4.18 & 89.84 \\
     ICRGAN & 3.52 & 90.24 \\
     CRGAN & 3.48 & 90.94 \\
     BigGAN & 2.98 & 91.49 \\
     StyleGAN & 2.67 & 91.73 \\ \bottomrule
\end{tabular}
}
\end{minipage}
\vspace{-1mm}
\caption{\textbf{Left:} OOD detection results of using different generative models. SIO is fairly robust to the choice of generative model. \textbf{Right:} We find that the FID metric of synthetic images correlates well with the corresponding OOD detection performance (the average of near- and far-OOD AUROC). *Unlike GAN models, the diffusion model (DDPM) here is unconditional.}
\label{fig:gen_model}
\vspace{-3mm}
\end{figure*}


\begin{figure*}[t]
\centering
   \includegraphics[width=0.95\linewidth]{figures/ratio_new.png}
   \vspace{-1mm}
   \caption{Results of varying the ratio $\alpha$ in Equation \ref{eq:SIO_id_only_training}. Compared with the real data-only baseline ($\alpha$=1.0), SIO consistently leads to improvements in a wide range of $\alpha$ (from 0.2 to 0.9).}
   \vspace{-1mm}
\label{fig:ratio}
\vspace{-3mm}
\end{figure*}




\noindent \textbf{Datasets.}
The first subset is ImageNet-10, which is similar to CIFAR-10 with categories such as \texttt{aircraft}, \texttt{automobile}, and \texttt{bird}.
The other one is ImageNet-dogs \cite{huang2021feature}, which consists of 100 dog categories.
See Appendix \ref{sec:ap_inet} for the complete list of class WordNet IDs.


\noindent \textbf{Training setup.}
We train ResNet-18 models for 60 epochs using SGD with a momentum of 0.9. 
The initial learning rate is 0.1 and decays according to the cosine annealing schedule. 
We use a batch size of 256 and the standard \texttt{RandomResizedCrop} with the final size being 224x224 as data augmentation.
For our SIO, we generate 10K synthetic images per category for both ImageNet-10 and ImageNet-dogs datasets using BigGAN \cite{biggan}.
The weighting/ratio factor $\alpha$ in Equation \ref{eq:SIO_id_only_training} is set to 0.9.





\noindent \textbf{Evaluation setup.}
Due to the high (sometimes unaffordable) computational cost of many specialized training methods \cite{vaze2021open,yang2022openood}, we only evaluate inference techniques with standard cross-entropy training, following OpenOOD. 
However, it should be noted that standard training is in fact a strong baseline on the ImageNet scale \cite{vaze2021open}.


For ImageNet-10, we use Species \cite{species22icml}, iNaturalist \cite{van2018inaturalist}, ImageNet-O \cite{natural_adv_examples}, and OpenImage-O \cite{haoqi2022vim} as near-OOD, as per OpenOOD.
For ImageNet-dogs, we take non-dog ImageNet samples as near-OOD, which is a more challenging problem \cite{huang2021feature}.
For both datasets, Textures \cite{dtd}, MNIST \cite{deng2012mnist}, and SVHN \cite{svhn} are used as far-OOD \cite{yang2022openood}.
In line with our previous experiments, we report the average near-OOD / far-OOD AUROC and ID accuracy from 3 runs.


\noindent \textbf{Results.}
Our results on the ImageNet splits demonstrate that the benefits of SIO observed in CIFAR experiments generalize to high-resolution images.
On ImageNet-10, while SIO does not achieve a higher best AUROC against far-OOD, it does improve the average near-OOD / far-OOD AUROC from 85.35\% / 91.76\% to 87.47\% / 93.63\% and achieves the best near-OOD AUROC of 89.81\% compared to the best score of 87.81\% from real data-only baselines.
On ImageNet-dogs, interestingly, SIO slightly degrades the ID accuracy, but still yields better average and best results than the real data-only baselines.







\subsection{Analyses}
\label{sec:analyses}

To analyze SIO's robustness to its hyperparameters, we conduct several experiments on CIFAR-10 using MSP as the detector.

%We further conduct several experiments on CIFAR-10 to analyze SIO's robustness to its hyperparameters. 
%MSP is used as the detector in these analyses.
%We also discuss why SIO is superior in the end.









\noindent \textbf{Choice of the generative model.}
In our CIFAR experiments, we used StyleGAN to generate synthetic data.
We now investigate whether SIO remains effective when using other generative models by repeating the SIO training with several other class-conditional GANs (using pre-trained models from \cite{kang2022studiogan}). 
We also consider an unconditional diffusion model \cite{ddpm}, where we use pseudo-labels for synthetic images, as discussed in Section \ref{sec:3.1}. 
The results are shown in the left panel of Figure \ref{fig:gen_model}.
We find that in all cases, SIO yields noticeable performance gains over the real data-only baseline, demonstrating that SIO is effective regardless of the specific choice of the generative model.


However, we also observe that certain generative models outperform others when used as the source of synthetic data. 
This leads us to consider metrics that can explain relative performance and guide the selection of the generative model beforehand. 
Intuitively, the \textit{quality} and \textit{diversity} are two important considerations, meaning that we want synthetic samples to be realistic and diverse such that they can provide useful information in addition to the real data.
Since advanced generative models can already provide sufficient diversity on small-scale datasets like CIFAR \cite{stylegan2ada}, we hypothesize that in our experiments the sample quality is the dominant factor.
To measure sample quality, we use the well-established Fr\'echet Inception Distance (FID) \cite{fid}, which measures the distance between the synthetic distribution and the real distribution in the Inception feature space. 
The results are shown in the right panel of Figure \ref{fig:gen_model}.
We find that the FID metric correlates well with OOD detection performance (the average of near- and far-OOD AUROC). 
For example, StyleGAN images exhibit the lowest FID (best sample quality) and lead to the highest OOD detection results. 
The only exception to this correlation is the unconditional diffusion model, where we suspect that the quality of the pseudo-labels may also have an effect.



\begin{figure}[t]
\centering
   \includegraphics[width=0.9\linewidth]{figures/oodvsid_new.png}
   \vspace{-2mm}
   \caption{Near-OOD detection AUROC v.s. ID classification accuracy plot. SIO benefits OOD detection even when there are limited accuracy improvements. It also leads to a greater net gain in OOD detection per unit increase in ID accuracy, compared to the tricks (longer training and RandAug) used in \cite{vaze2021open}.}
   \vspace{-3mm}
\label{fig:oodvsid}
\end{figure}



\begin{figure}[t]
\centering
   \includegraphics[width=0.9\linewidth]{figures/num_syn_images_v0.png}
   \vspace{-2mm}
   \caption{Results of varying the number of synthetic ID images. More samples (higher diversity) in general lead to better performance.}
   \vspace{-5mm}
\label{fig:num_syn_images}
\end{figure}






\noindent \textbf{The ratio $\alpha$.}
Another hyperparameter of SIO is the weighting/ratio $\alpha$ in Equation \ref{eq:SIO_id_only_training} and \ref{eq:SIO_id_ood_training}.
To investigate the effect of $\alpha$, we vary it from 0.0 (using only synthetic samples) to 1.0 (using only real samples) and evaluate the resulting OOD detection performance.
Results are presented in Figure \ref{fig:ratio}.


We identify that SIO consistently provides notable improvements in OOD detection performance across a wide range of $\alpha$. 
The best results are obtained under a large $\alpha$ (\eg, 0.8 or 0.9), with the majority of the samples still being real data.
We think this is because oversampling synthetic samples will exacerbate the distribution shift between the synthetic and real distribution, leading to worse performance.
Such effect can also be observed in the ID classification accuracy, where biasing towards synthetic data ($\alpha$\textless 0.5) reduces the accuracy compared with the real data baseline.
Meanwhile, notice that naively injecting synthetic images into the training set, which corresponds to $\alpha\approx$~0.05 in Figure \ref{fig:ratio} (there are 50K real samples and 1000K synthetic samples), would degrade the performance. 
Our results highlight that the weighting scheme adopted by SIO is the key ingredient of making synthetic images useful.


%, potentially explaining why previous attempts that naively added synthetic samples into the real training set without a proper weighting did not lead to accuracy gains \cite{cas}.




\noindent \textbf{Classifier architecture.}
To demonstrate that SIO can work with other classifier architectures, we repeat the CIFAR-10 experiments with DenseNet-100 \cite{densenet}.
The experimental setup and hyperparameters remain the same as before, and we focus on inference techniques for simplicity.
See Appendix \ref{sec:ap_addtional_results} for full results.
Overall, SIO improves the average scores from 84.58\% / 84.59\% to 86.04\% / 89.50\% and the best scores from 90.12\% / 93.81\% to 90.64\% / 95.43\% against near-OOD / far-OOD, respectively.


\noindent \textbf{Explaining SIO's effects.}
It is possible to assume that SIO's ability to improve OOD detection performance is solely due to its enhancement of ID classification accuracy. 
While this assumption aligns with the previous finding in \cite{vaze2021open} which suggests that there is a correlation between OOD detection performance and ID accuracy, our analysis indicates that this view does \textit{not} fully explain SIO's effects.


To demonstrate this, we reproduce the OOD-ID correlation observed in \cite{vaze2021open} by applying the techniques used in that study, including longer training and RandAug \cite{cubuk2020randaugment}.
The results of this experiment, together with the results of the SIO training, are presented in Figure \ref{fig:oodvsid}. 
The diversity of the blue dots comes from varying SIO's hyperparameters. 
We observe that SIO improves OOD detection performance even when it does not enhance ID classification accuracy. 
Additionally, we find that SIO yields a higher net gain in OOD detection performance per unit increase in ID accuracy than purely chasing a better classifier, as done in \cite{vaze2021open}.


We hypothesize that SIO's effects stem from the increased diversity provided by additional synthetic ID samples.
Our reasoning is based on the observation that neural networks often rely on ``spurious'' or ``shortcut'' features in images that are only superficially correlated with the labels \cite{yin2019fourier,ilyas2019adversarial,gilmer2019a}. 
The challenge of OOD detection, then, is that OOD samples can easily activate these spurious features. 
By training the model on more diverse ID samples, each potentially coming with different spurious features, the model may rely less on such features and become less likely to activate when presented with OOD samples.


To test our hypothesis, we vary the number of synthetic images as a proxy for the diversity of the training set.
The results presented in Figure \ref{fig:num_syn_images} provide supporting evidence for our hypothesis. 
The plot shows a generally increasing trend in OOD detection performance as the number of synthetic images increases.










%We find that the exact reasons for why SIO works can be nuanced.
%On one hand, SIO's effects can be attributed to the improvements in ID classification accuracy, since it is shown that a more accurate classifier also performs better in detecting OOD samples \cite{vaze2021open}.
%Specifically, as we introduce many more synthetic ID samples, we are essentially pushing the model more towards minimizing the \textit{population loss} rather than the \textit{empirical loss}, which in turn reduces the test loss and leads to higher accuracy on test ID data \cite{nakkirandeep}.
%This is indeed the case in some of our experiments.
%For example in Table \ref{tab:cifar} we do observe better ID accuracy on CIFAR datasets.


%On the other hand, however, we find several cases where the classification accuracy cannot fully explain the OOD detection rates.
%In Figure \ref{fig:num_syn_images}, using 100K synthetic images improves the near-OOD AUROC by 1.61\%, despite being marginally better than the real data-only baseline in ID accuracy.
%A similar observation can be seen in Figure \ref{fig:ratio} as well: When $\alpha\in$ \{0.2,0.3,0.4\}, the SIO-trained model degrades the ID accuracy, yet it still yields remarkable gains in detection AUROC especially against near-OOD.
%These observations indicate that SIO may exhibit other advantages that have less to do with ID accuracy but benefit OOD detection.
%We leave as future work a closer investigation on SIO's effects.



























%%%%%%% legacy

\iffalse

\begin{figure*}[t]
\centering
   \includegraphics[width=0.95\linewidth]{figures/cifar10_dn.png}
   \caption{Results of several inference techniques with DenseNet-100 on CIFAR-10. We omit other methods that give significantly worse baseline results for visual clarity; see Appendix \ref{sec:ap_addtional_results} for full results.}
\label{fig:cifar10_dn}
\end{figure*}


\begin{figure*}[t]
\centering
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=0.99\linewidth]{figures/gen_model.png}
%\caption{figure caption goes here}
%\label{fig: figure-label}
\end{minipage}
\begin{minipage}{0.48\textwidth}
\centering
%\captionsetup{type=table} %% tell latex to change to table
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{lccccc}
     \multirow{2}{*}{data source} & \multicolumn{3}{c}{diversity} & \multirow{2}{*}{FID$\downarrow$}
     &\multirow{2}{*}{AUROC} \\ \cmidrule(lr){2-4}
     & train & self & ent.$\uparrow$ & & \\ \midrule 
     DDPM* & 40.55\% & 59.45\% & 0.675 & 3.17 & 90.22 \\
     ReACGAN & 35.53\% & 64.47\% & 0.651 & 4.40 & 89.29 \\
     MHGAN & 38.15\% & 61.85\% & 0.665 & 4.18 & 89.84 \\
     ICRGAN & 37.08\% & 62.92\% & 0.659 & 3.52 & 90.24 \\
     CRGAN & 38.78\% & 61.22\% & 0.668 & 3.48 & 90.94 \\
     BigGAN & 41.17\% & 58.83\% & 0.677 & 2.98 & 91.49 \\
     StyleGAN & 42.39\% & 57.61\% & 0.682 & 2.67 & 91.73 \\ \bottomrule
\end{tabular}
}
\end{minipage}
\caption{\textbf{Left:} OOD detection results of using different generative models. SIO is fairly robust to the choice of generative model. \textbf{Right:} We find that the diversity and FID metric correlate well with the OOD detection performance (the average of near- and far-OOD AUROC). ``ent.'' means entropy; see text for detailed description. *Unlike GAN models, the diffusion model (DDPM) here is unconditional.}
\label{fig:gen_model}
\end{figure*}


\begin{figure*}[t]
\centering
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=0.99\linewidth]{figures/gen_model_ablation.png}
%\caption{figure caption goes here}
%\label{fig: figure-label}
\end{minipage}
\begin{minipage}{0.48\textwidth}
\centering
%\captionsetup{type=table} %% tell latex to change to table
\resizebox{0.98\linewidth}{!}{
\begin{tabular}{lcccccc}
     \multirow{2}{*}{data source} & \multicolumn{4}{c}{diversity} & \multirow{2}{*}{FID$\downarrow$}
     &\multirow{2}{*}{AUROC} \\ \cmidrule(lr){2-5}
     & train & test & self & ent.$\uparrow$ & & \\ \midrule 
     DDPM* & 28.54\% & 27.60\% & 43.86\% & 1.075 & 3.17 & 90.63 \\
     ReACGAN & 25.69\% & 24.06\% & 50.25\% & 1.038 & 4.40 & 89.85 \\
     MHGAN & 26.52\% & 26.18\% & 47.30\% & 1.057 & 4.18 & 90.29 \\
     ICRGAN & 26.52\% & 25.69\% & 47.79\% & 1.054 & 3.52 & 90.67 \\
     CRGAN & 27.33\% & 27.39\% & 45.28\% & 1.068 & 3.48 & 91.38 \\
     BigGAN & 28.65\% & 28.12\% & 43.23\% & 1.077 & 2.98 & 91.87 \\
     StyleGAN & 29.34\% & 28.16\% & 42.50\% & 1.080 & 2.67 & 92.14 \\ \bottomrule
\end{tabular}
}
\end{minipage}
\caption{\textbf{Left:} OOD detection results of using different generative models. SIO is fairly robust to the choice of generative model. \textbf{Right:} We find that the diversity and FID metric correlate well with the OOD detection performance (the average of near- and far-OOD AUROC). ``ent.'' means entropy; see text for detailed description. *Unlike GAN models, the diffusion model (DDPM) here is unconditional.}
\label{fig:gen_model}
\end{figure*}

\fi


%Intuitively, the synthetic data should be \textit{diverse} and be of high \textit{quality} to be helpful.
%Being diverse means that the synthetic images should be sufficiently different from each other and from the real training set, or equivalently, not having mode collapse \citeph{} or just memorizing existing real samples.
%The sample quality is also important: We want the synthetic samples to be as realistic as possible, otherwise they might be too different from the real data distribution and cause learning difficulty for the classifier.


%To measure sample quality, we take the well-established Fr\'echet Inception Distance (FID) \citeph{} as the metric which measures the distance between the synthetic distribution and the real distribution in the Inception feature space \citeph{}.
%For diversity, we use the metric proposed in \citeph{} which finds the nearest neighbor of each synthetic sample in the Inception feature space and examines the proportion of samples with a nearest neighbor in either the real training set, real test set, or the synthetic set itself.
%Ideally we want the nearest neighbors to uniformly distribute across the three sets (\ie, the proportion distribution has high entropy).
%Otherwise, mode collapse might happen if most synthetic samples' nearest neighbors are among themselves; or if the large proportion points to the real training set,  indicates memorization of real data.