\section{Method}
\subsection{Problem Definition}
Few-shot segmentation (FSS) aims to segment novel classes by just a few samples with densely-annotated samples. 
In FSS, the dataset is divided into the training set $\mathbb{D}_\text{train}$, containing the base classes $\mathbb{C}_\text{train}$, and the test set $\mathbb{D}_\text{test}$, containing the novel classes $\mathbb{C}_\text{test}$, where $\mathbb{C}_\text{train}\cap \mathbb{C}_\text{test}= \emptyset$.
To obtain the segmentation model for FSS, the commonly used episode training approach is employed~\cite{wang2019panet}.
Each trainig/testing episode ($S_i, Q_i$) instantiates a $N$-way $K$-shot segmentation learning task.
Specifically, the support set $S_i$ contains $K$ samples of $N$ classes, while the query set $Q_i$ contains one sample from the same class.
The FSS model is trained with episodes to predict the novel class for the query image, guided by the support set.
During inference, the model is evaluated directly on $\mathbb{D}_\text{test}$ without any re-training.
In this paper, we follow the established practice in medical FSS~\cite{hansen2022anomaly,ouyang2022self,shen2022q} that consider the \textbf{1}-way \textbf{1}-shot task.

\input{Figure/Fig_framework}
\subsection{Network Overview}
The Overview of our CAT-Net is illustrated in Fig.~\ref{fig:framework}(a). It consists of three main components:
1) a mask incorporated feature extraction (MIFE) sub-net that extracts initial query and support features as well as query mask; 
2) a cross masked attention Transformer (CMAT) module in which the query and support features boost each other and thus refined the query prediction;
and 3) an iterative refinement framework that sequentially applies the CMAT modules to continually promote the segmentation performance.
The whole framework can be trained in an end-to-end fashion.

\subsection{Mask Incorporated Feature Extraction}
The Mask Incorporate Feature Extraction (MIFE) sub-net takes query and support images as input and generates their respective features, integrated with the support mask. A simple classifier is then used to predict the segmentation for the query image.
Specifically, we first employ a feature extractor network (\ie, ResNet-50) to map the query and support image pair $I^q$ and $I^s$ into the feature space, producing multi-level feature maps $F^q$ and $F^s$ for query and support image, respectively.
Next, the support mask is pooled with $F^s$ and then expanded and concatenated with both $F^q$ and $F^s$.
Additionally, a prior mask is further concatenated with the query feature to strengthen the correlation between query and support features via a pixel-wise similarly map. 
Finally, the query feature is processed by a simple classifier to get the query mask.
Further details of the MIFE architecture can be found in the supplementary material.

\subsection{Cross Masked Attention Transformer}
As shown in Fig.~\ref{fig:framework}(b), the cross masked attention Transformer (CMAT) module comprises three main components: 1) a self-attention module for extracting global information from query and support features; 2)  a cross masked attention module for transferring foreground information between query and support features while eliminating redundant background information, and 3) a prototypical segmentation module for generating the final prediction of the query image.

\noindent\textbf{Self-Attention Module.}
To capture the global context information of every pixel in the query feature $F_0^q$ and support features $F_0^s$, the initial features are first flattened into 1D sequences and fed into two identical self-attention modules.
Each self-attention module consists of a multi-head attention (MHA) layer and a multi-perceptron (MLP) layer. 
Given an input sequence $S$, the MHA layer first projects the sequence into three sequences $K$, $Q$, and $V$ with different weights. The attention matrix $A$ is then calculated as:
\begin{equation}
A(Q, K)=\dfrac{QK^T}{\sqrt{d}}
\end{equation}
where $d$ is the dimension of the input sequence.
The attention matrix is then normalized by a softmax function and multiplied by the value sequence $V$ to get the output sequence $O$:
\begin{equation}
O=\text{softmax}(A)V
\end{equation}
The MLP layer is a simple $1\times 1$ convolution layer that maps the output sequence $O$ to the same dimension as the input sequence $S$.
Finally, the output sequence $O$ is added to the input sequence $S$ and normalized using layer normalization (LN) to obtain the final output sequence $X$.
The output feature sequence of the self-attention alignment encoder is represented by $X^q\in\mathbb{R}^{HW\times D}$ and $X^s\in\mathbb{R}^{HW\times D}$ for query and support features, respectively.

\noindent\textbf{Cross Masked Attention Module.}
We utilize cross masked attention to incorporate query features and support features with respect to their foreground information by constraining the attention region in attention matrix with support and query masks. 
Specifically, given the query feature $X^q$ and support features $X^s$ from the aforementioned self-attention module, we first project the input sequence into three sequences $K$, $Q$, and $V$ using different weights, resulting in $K^q$, $Q^q$, $V^q$, and $K^v$, $Q^v$, $V^v$, respectively. 
Taking the query features as an example, the cross attention matrix is calculated by:
\begin{equation}
\text{A}(K^q,Q^s)=\dfrac{(K^q)^T Q^s}{\sqrt{d}}
\end{equation}
We expend and flatten the binary query mask $M^q$ to limit the foreground region in attention map. 
The masked cross attention (MCA) map is computed as:
\begin{equation}
\text{MCA}(K^q,Q^s,V^s, {M}^q)=M^q \cdot V^s(\text{softmax}(A(K^q,Q^s))
\end{equation}
Similar to self-attention, the query feature is processed by MLP and LN layer to get the final enhanced query features $F^q_1$.
Similarly, the enhanced support feature $F^s_1$ is obtained with foreground information from the query feature.

\noindent\textbf{Prototypical Segmentation Module.}
Once the enhanced query and support features are obtained, the prototypical segmentation is used to obtain the final prediction.
First, a prototype of class $c$ is built by masked average pooling of the support feature $F^s_1$ as follows:
\begin{equation}
p_c = \dfrac{1}{K}\sum^K_{k=1}\dfrac{\sum_{k,x,y} F^s_{1,(k,x,y)}m^s_{(k,x,y,c)}}{\sum_{x,y}m^s_{(k,x,y,c)} }
\end{equation}
where $K$ is the number of support images, and $m^s_{(k,x,y,c)}$ is a binary mask that indicates whether pixel at the location $(x,y)$ in support feature $k$ belongs to class $c$.
Next, we use the non-parametirc metric learning method to perform segmentation. 
The prototype network calculates the distance between the query feature vector and the prototype $P={\{P_c|c\in C\}}$. 
Softmax function is applied to produce probabilistic outputs for all classes, generating the query segmentation:
\begin{equation}
\hat{M}^q_{1,(x,y)} = \text{softmax}\big(\alpha \text{cos}(F^q_{1,(x,y)},p_c)\cdot{\text{softmax}(\alpha \text{cos}(F^q_{1,(x,y)},p_c))}\big)
\end{equation}
where $\text{cos}(\cdot)$ denotes cosine distance, $\alpha$ is a scaling factor that helps gradients to back-propagate in training. In our work, $\alpha$ is set to 20, same as in~\cite{wang2019panet}.

Additionally, we design a double threshold strategy to obtain query segmentation.
Specifically, we set the first threshold $\tau$ to 0.5 to obtain the binary query mask ${M}^q$, which is used to calculate the Dice loss and update the model. 
Then, the second threshold $\hat{\tau}$ is set to 0.4 to obtain the dilated query mask $\hat{M}^q$, which is used to generate the enhanced query feature $F^q_2$ in the next iteration.
The second threshold $\hat{\tau}$ is set lower than the first threshold $\tau$ to prevent some foreground pixels from being mistakenly discarded.
The query segmentation mask ${M}^q$ and dilated mask $\hat{M}^q$ are represented by:
\begin{equation}
{M}^q_1=
\left\{
\begin{aligned}
1, \quad &M^q_{1,(x,y)} > \tau\\
0,\quad & M^q_{1,(x,y)} < \tau \\
\end{aligned}
\right.
\qquad
\hat{M}^q_1=
\left\{
\begin{aligned}
1, \quad &M^q_{1,(x,y)} > \hat{\tau}\\
0,\quad & M^q_{1,(x,y)} < \hat{\tau} \\
\end{aligned}
\right.
\end{equation}

\subsection{Iterative Refinement framework}
As explained above, the CMAT module is designed to refine the query and support features, as well as the query segmentation mask. 
Thus, it's natural to iteratively apply this sub-net to get the enhanced features and refine the mask, resulting in a boosted segmentation result. 
The result after the $i$-th iteration is represented by:
\begin{equation}
(F^s_i,F^q_i,M^q_i,\hat{M}^q_i)=\text{CMAT}(F_{i-1}^{s}, F_{i-1}^{q},\hat{M}_{i-1}^{q},M^s)
\end{equation}
The subdivision of each step can be specifically expressed as:
\begin{equation}
(F^s_i,F^q_i)=\text{CMA}(F_{i-1}^{s}, F_{i-1}^{q},\hat{M}_{i-1}^{q},M^s)
\end{equation}
\begin{equation}
(M^q_i,\hat{M}_{i}^{q}) = \text{Proto}(F_{i}^{s}, F_{i}^{q},M^s, \tau,\hat{\tau})
\end{equation}
where $\text{CMA}(\cdot)$ indicates the self-attention and cross masked attention module, and $\text{Proto}(\cdot)$ represents the prototypical segmentation module.