\section{Introduction}
\label{sec:introduction}
Automatic segmentation of medical images is a fundamental step for a variety of medical image analysis tasks, such as diagnosis, treatment planning, and disease monitoring~\cite{hesamian2019deep}.
The emergence of deep learning (DL) has enabled the development of many medical image segmentation methods, which have achieved remarkable success~\cite{transunet,u-net,hatamizadeh2022unetr,hatamizadeh2022swin}.
 Most of the existing methods follow a fully-supervised learning paradigm, which requires a considerable amount of labeled data for training. 
However, the manual annotation of medical images is time-consuming and labor-intensive, limiting the application of DL in medical image segmentation. 
Specifically for the 3D volumetric medical images (\eg, CT, MRI), the manual annotation is even more challenging which requires the annotators to go through hundreds of 2D slices for each 3D scan.

To address the challenge of manual annotation, various label-efficient techniques have been explored, such as self-supervised learning~\cite{ouyang2022self}, semi-supervised learning~\cite{semi}, and weakly-supervised learning~\cite{Kwak2017WeaklySS}. 
Despite leveraging information from unlabeled or weakly-labeled data, these techniques still require a substantial amount of training data~\cite{14rakelly2018conditional,15shaban2017one,16dong2018few,17siam2019amp}, which may not be practical for novel classes with limited examples in the medical domain.
This limitation encourages the few-shot learning paradigm~\cite{8snell2017prototypical,9sung2018learning,10garcia2017few,11vinyals2016matching} to be applied to medical image segmentation.
Specifically, the few-shot learning paradigm aims to learn a model from a small number of labeled data (denoted as \textit{support}) and then apply it to a new task (denoted as \textit{query}) with only a few labeled data without any retraining.
Considering the hundreds of organs and countless diseases in the human body, FSL brings great potential to the various medical image segmentation tasks where a new task can be easily investigated in a data-efficient manner.

Most few-shot segmentation methods follow the learning-to-learn paradigm, which aims to learn a meta-learner to predict the segmentation of query images based on the knowledge of support images and their respective segmentation labels.
The success of this paradigm depends on how effectively the knowledge can be transferred from the support prototype to the query images.
Existing few-shot segmentation methods mainly focus on the following two aspects: (1) how to learn the meta-learner~\cite{pandey2022adversarially,roy2020squeeze,Lang_2022_CVPR,9761453}; and (2) how to better transfer the knowledge from the support images to the query images~\cite{SUN2022105067,9154595,NEURIPS2021_b8b12f94,protomineing,self-support,Tang_2021_ICCV}.
Despite prototype-based methods having shown success, they typically ignore the interaction between support and query features during training.

In this paper, as shown in Fig.~\ref{fig:framework}(a), we propose \textbf{CAT}-\textbf{Net}, a \textbf{C}ross \textbf{A}ttention \textbf{T}ransformer network for few-shot medical image segmentation, which aims to fully capture intrinsic classes details while eliminating useless pixel information and learn an interdependence between the support and query features.
Different from the existing FSS methods that only focus on the single direction of knowledge transfer (\ie, from the support features to the query features), the proposed CAT-Net can boost the mutual interactions between the support and query features, benefiting the segmentation performance of both the support and query images. 
Additionally, we propose an iterative training framework that feed the prior query segmentation into the attention transformer to effectively enhance and refine the features as well as the segmentation.
Three publicly available datasets are adopted to evaluate our CAT-Net, \ie, Abd-CT~\cite{landman2015miccai}, Abd-MRI~\cite{kavur2021chaos}, and Card-CT~\cite{zhuang2018multivariate}. 
Extensive experiments validate the effectiveness of each component in our CAT-Net, and demonstrate its state-of-the-art performance.
