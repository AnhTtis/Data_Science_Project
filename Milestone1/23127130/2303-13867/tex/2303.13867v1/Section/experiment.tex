\section{Experiment}
\label{sec:experiment}
\subsection{Dataset and Evaluation Metrics}
We evaluate the proposed method on three public datasets, \ie, Abd-CT~\cite{landman2015miccai}, Abd-MRI~\cite{kavur2021chaos}, and Card-MRI~\cite{zhuang2018multivariate}. 
Abd-CT contains 30 abdominal CT scans with annotations of left and right kidney (LK and RK), spleen (Spl), liver (Liv).
Abd-MRI contains 20 abdominal MRI scans with annotations of the same organs as Abd-CT.
Card-MRI includes 35 cardiac MRI scans with annotations of left ventricular blood pool (LV-B), left ventricular myocardium (LV-M), and right ventricle (RV).
We use the Dice score as the evaluation metric following~\cite{ouyang2022self,shen2022q}.

To ensure a fair comparison, all the experiments are conducted under the 1-way 1-shot scenario using 5-fold cross-validation. 
We follow~\cite{ouyang2022self} to remove all slices containing test classes during training to ensure that the test classes are all unseen during validation.
In each fold, we follow~\cite{ouyang2022self,hansen2022anomaly,shen2022q} that takes the last patient as the support image and the remaining patients as the query (setting \Rmnum{1}).
We further propose a new validation setting (setting \Rmnum{2}) that takes every image in each fold as a support image alternately and the other images as the query.
The averaged result of each fold is reported. 
It could evaluate the generalization ability of the model by reducing the affect of support image selection.

\subsection{Implementation Details}
The proposed method is implemented using PyTorch.
Each 3D scan is sliced into 2D slices and reshaped into 256$\times$256 pixels. 
Common 3D image pre-processing techniques, such as intensity normalization and resampling, are applied to the training data.
We apply episode training with 20$k$ iterations. SGD optimizer is adopted with a learning rate of 0.001 and a batch size of 1.
Each episode training takes approximately 4 hours using a single NVIDIA RTX 3090 GPU.

\input{Table/sota}
\subsection{Comparison with State-of-the-Art Methods}
We compare the proposed CAT-Net with state-of-the-art (SOTA) methods, including SE-Net~\cite{roy2020squeeze}, PANet~\cite{wang2019panet}, ALP-Net~\cite{ouyang2022self}, and AD-Net~\cite{hansen2022anomaly}, and Q-Net~\cite{shen2022q}. 
PANet~\cite{wang2019panet} are the typical prototypical FSS method in the natural image domain, SE-Net~\cite{roy2020squeeze}, ALP-Net~\cite{ouyang2022self}, AD-Net~\cite{hansen2022anomaly}, and Q-Net~\cite{shen2022q} are the most representative work in medical FSS task.
Experiment results presented in Table~\ref{tab:sota} demonstrate that the proposed method outperforms SOTAs on all three datasets under both setting \Rmnum{1} and setting \Rmnum{2}.
Under setting \Rmnum{1}, the proposed CAT-Net achieves 66.59\% Dice on Abd-CT, 75.18\% Dice on Abd-MRI, and 79.03\% Dice on Card-MRI in Dice, outperforming SOTAs by 1.76\%, 0.75\%, and 0.45\%, respectively.
Under setting \Rmnum{2}, CAT-Net achieves 70.88\% Dice on Abd-CT, 75.22\% Dice on Abd-MRI, and 79.36\% Dice on Card-MRI, outperforming SOTAs by 2.56\%, 2.02\% and 1.32\%, respectively. 
The consistent superiority of our method to SOTAs on three datasets and under two evaluation settings indicates the effectiveness and generalization ability of the proposed CAT-Net.
In addition, the qualitative results in Fig.~\ref{fig:vision} demonstrate that the proposed method is able to generate more accurate and detailed segmentation results compared to SOTAs.
\input{Figure/vision}

\subsection{Ablation Study}
We conduct an ablation study to investigate the effectiveness of each component in CAT-Net.
All ablation studies are conducted on Abd-MRI under setting \Rmnum{2}.

\subsubsection{Effectiveness of CMAT Block:}
To demonstrate the importance of our proposed CAT-Net in narrowing the information gap between the query and supporting images and obtaining enhanced features, we conducted an ablation study.
Specifically, we compared the results of learning foreground information only from the support (\textit{S$\to$Q}) or query image (\textit{Q$\to$S}) and obtaining a single enhanced feature instead of two (\textit{S$\leftrightarrow$Q}). 
It can be observed that using the enhanced query feature (\textit{S$\to$Q}) achieves 66.72\% in Dice, outperforming only using the enhanced support feature (\textit{Q$\to$S}) by 0.74\%.
With our CMAT block, the mutual boosted support and query feature (\textit{S$\leftrightarrow$Q}) could improve the Dice by 1.90\%.
Moreover, the iteration refinement framework consistently promotes the above three variations by 0.96\%, 0.56\%, and 2.26\% in Dice, respectively.
\input{Figure/fig_abl_iter}

\subsubsection{Influence of Iterative Mask Refinement Block:}
To determine the optimal number of iterative refinement CMAT block, we experiment with different numbers of blocks. 
In Fig.~\ref{fig:abl_iter}, we observe that increasing the number of blocks results in improved performance, with a maximum improvement of 2.26\% in Dice when using 5 blocks.
Considering the performance gain between using 4 and 5 CMAT blocks was insignificant, we hence opt to use four CMAT blocks in our final model to strike a balance between efficiency and performance.