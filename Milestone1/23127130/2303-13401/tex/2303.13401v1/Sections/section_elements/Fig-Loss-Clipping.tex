\begin{figure}[!tb]
\centering
\begingroup 
\setlength{\tabcolsep}{1pt}
\renewcommand{\arraystretch}{0.8}
\begin{tabular}{ccc}
\centering
\textbf{\small{loss}}
&{ }
&\textbf{\small{gradient magnitude}}
\\
\cline{1-1}\cline{3-3}
\vspace{-1em}
\\
\includegraphics[width=0.24\textwidth]{Figures/vis_concept/ce_loss_clip.png}
&{ }
&\includegraphics[width=0.24\textwidth]{Figures/vis_concept/ce_grad_clip.png}
\\
\textbf{\small{(a)}}
&{ }
&\textbf{\small{(b)}}
\\
\includegraphics[width=0.24\textwidth]{Figures/vis_concept/margin_loss_clip.png}
&{ }
&\includegraphics[width=0.24\textwidth]{Figures/vis_concept/margin_grad_clip.png}
\\
\textbf{\small{(c)}}
&{ }
&\textbf{\small{(d)}}
\end{tabular}
\endgroup 
\caption{Visualizations of loss clipping. The cross-entropy loss and its clipped version is shown in \textbf{(a)} and the corresponding norm of gradients is shown in \textbf{(b)}. The clipped version shown in \textbf{(a)} is the one used in the CIFAR-10 experiments. The x-axes in \textbf{(a)} and \textbf{(b)} are the network output value $f_{\mb \theta}^y (\mb x')$ after softmax regularization.The margin loss and its clipped version is shown in \textbf{(c)}, and the corresponding norm of gradients is shown in\textbf{(d)}. The x-axes in \textbf{(c)} and \textbf{(d)} are the value $\max_{i \ne y} f_{\mb \theta}^i (\mb x') - f_{\mb \theta}^y (\mb x')$ before the softmax regularization, which follows the definition of $\ell_{ML}$ in \cref{eq: margin loss}.} 
\label{fig:loss_clipping} 
\end{figure}