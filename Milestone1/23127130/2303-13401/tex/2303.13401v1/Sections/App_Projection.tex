\subsection{Projection onto the intersection of a norm ball and box constraints}
\label{Sec:app_projection}
APGD for solving max-loss form (\ref{eq:robust_loss}) with $\ell_p$ distances entails solving Euclidean projection subproblems of the form: 
\begin{align}
\begin{split}
    & \min_{\mb x' \in \RJU^n} \;  \norm{\mb z - \mb x'}_2^2 \\
    \st\; & \norm{\mb x - \mb x'}_p \le \eps, \quad \mb x' \in [0, 1]^n,
\end{split}
\end{align}
where $\mb z = \mb x + \mb w$ is a one-step update of $\mb x$ toward direction $\mb w$. After a simple reparametrization, we have 
\begin{align}  
\label{eq:app_proj_infty}
\begin{split}
    & \min_{\mb \delta \in \RJU^n} \; \norm{\mb w - \mb \delta}_2^2 \\
    \st \; & \norm{\mb \delta}_p \le \eps, \quad \mb x + \mb \delta \in [0, 1]^n.
\end{split}
\end{align}

We focus on $p = 1, 2, \infty$ which are popular in the AR literature. In early works, a ``lazy'' projection scheme---sequentially projecting onto the $\ell_p$ ball and then to the $[0, 1]^n$ box, is used. \cite{croce2021mind} has recently identified the detrimental effect of lazy projection on the performance for $p=1$, and has derived a closed form solution. Here, we prove the correctness of the sequential projection for $p = \infty$ (\cref{thm:app_proj_inf_lemma}), and discuss problems regarding $p = 2$ (\cref{thm:app_proj_l2_lemma}). 

For $p = \infty$, obviously we only need to consider the individual coordinates. 
\begin{lemma} \label{thm:app_proj_inf_lemma}
Assume $x \in [0, 1]$. The unique solution for the strongly convex problem
\begin{align}
\begin{split}
    & \min_{\delta \in \RJU} \; \paren{w - \delta}^2\\
    \st\; & \abs{\delta} \le \eps, \quad x + \delta \in [0, 1]
\end{split}
\end{align}
is given by
\begin{multline}  \label{eq:app_proj_inf_formula}
    \mc P_{\infty, \mathrm{box}} = \\
        \begin{cases} 
            w,\quad w \in [\max(-x, -\eps), \min (1-x, \eps)]\\
        \max(-x, -\eps), \quad w \le \max(-x, -\eps) \\
        \min (1-x, \eps), \quad w \ge \min (1-x, \eps) 
    \end{cases}, 
\end{multline} 
which agrees with the sequential projectors $\mc P_{\infty} \mc P_{\mathrm{box}}$ and $\mc P_{\mathrm{box}} \mc P_{\infty}$.  
\end{lemma} 
One can easily derive the one-step projection formula \cref{eq:app_proj_inf_formula} once the two box constraints can be combined into one: 
\begin{align} \label{eq:app_proj_inf_equiv_constr}
    \max(-\eps, -x) \le \delta \le \min(\eps, 1-x). 
\end{align} 
To show the equivalence to $\mc P_{\infty} \mc P_{\mathrm{box}}$ and $\mc P_{\mathrm{box}} \mc P_{\infty}$, we could write down all projectors analytically and directly verify the claimed equivalence. But that tends to be cumbersome. Here, we invoke an elegant result due to \cite{yu2013decomposing}. For this, we need to quickly set up the notation. For any function $f: \RJU^n \to \RJU \cup \setJu{+ \infty}$, its proximal mapping $\mathrm{Prox}_f$ is defined as 
\begin{align} 
    \mathrm{Prox}_f(\mb y) = \argmin_{\mb z \in \RJU^n} \frac{1}{2} \norm{\mb y - \mb z}_2^2 + f(\mb z).
\end{align} 
When $f$ is the indicator function $\imath_C$ for a set $C$ defined as 
\begin{align}
    \imath_C(\mb z) = 
    \begin{cases} 
        0   &   \mb z \in C \\
            \infty &   \text{otherwise}   
         \end{cases},
\end{align}
then $\mathrm{Prox}_f(\mb y)$ is the Euclidean projector $\mc P_C(\mb y)$. For two closed proper convex functions $f$ and $g$, the paper \cite{yu2013decomposing} has studied conditions for $\mathrm{Prox}_{f+g} = \mathrm{Prox}_{f} \circ \mathrm{Prox}_{g}$. If $f$ and $g$ are two set indicator functions, this exactly asks when the sequential projector is equivalent to the true projector. 
\begin{theorem}[adapted from Theorem 2 of \cite{yu2013decomposing}]
  If $f = \imath_C$ for a closed convex set $C \subset \RJU$, $\mathrm{Prox}_f \circ \mathrm{Prox}_g = \mathrm{Prox}_{f+g}$ for all closed proper convex functions $g: \RJU \to \RJU \cup \setJu{\pm \infty}$. 
\end{theorem} 
The equivalence of projectors we claim in \cref{thm:app_proj_inf_lemma} follows by setting $f = \imath_{\infty}$ and $g = \imath_{\mathrm{box}}$, and vise versa. 

For $p = 2$, sequential projectors are not equivalent to the true projector in general, although empirically we observe that $\mc P_{2} \mc P_{\mathrm{box}}$ is a much better approximation than $\mc P_{\mathrm{box}} \mc P_{2}$. The former is used in the current APGD algorithm of \texttt{AutoAttack}. 
\begin{lemma} \label{thm:app_proj_l2_lemma}
    Assume $\mb x \in [0, 1]^n$. When $p = 2$, the projector for (\ref{eq:app_proj_infty}) $\mc P_{2, \mathrm{box}}$ does not agree with the sequential projectors $\mc P_{2} \mc P_{\mathrm{box}}$ and $\mc P_{\mathrm{box}} \mc P_{2}$ in general. However, both $\mc P_{2} \mc P_{\mathrm{box}}$ and $\mc P_{\mathrm{box}} \mc P_{2}$ always find feasible points for the projection problem. 
\end{lemma} 
\begin{proof} 
\begin{figure}[!htbp]
    \centering 
    \includegraphics[width=0.6\linewidth]{Figures/L2_box.png}
    \caption{Illustration of the problem with the sequential projectors when $p = 2$. In general, neither of the sequential projectors produces the right projection. }
    \label{fig:l2_box_projection}
\end{figure}
For the non-equivalence, we present a couple of counter-examples in \cref{fig:l2_box_projection}. Note that the point $\mb z$ is inside the normal cone of the bottom right corner point of the intersection: $\setJu{\mb \delta \in \RJU^2: \norm{\mb \delta}_2 \le \eps} \cap \setJu{\mb \delta \in \RJU^2: \mb x + \mb \delta \in [0, 1]^2}$. 

For the feasibility claim, note that for any $\mb y \in \RJU^n$
\begin{align} 
    \mc P_2\paren{\mb y} = 
    \begin{cases}
        \eps \frac{\mb y}{\norm{\mb y}_2} &  \norm{\mb y}_2 \ge \eps \\
        \mb y   &  \text{otherwise}
    \end{cases}
\end{align} 
and for any $y \in \RJU$, 
\begin{align} 
    \mc P_{\mathrm{box}}\paren{y} = 
    \begin{cases}
        1-x &  y \ge 1-x \\
        y  &   -x < y < 1-x \\ 
        -x  & y \le -x 
    \end{cases}
\end{align} 
and $\mc P_{\mathrm{box}}(\mb y)$ acts on any $\mb y \in \RJU^n$ element-wise. For any $\mb y$ inside the $\ell_2$ ball, 
\begin{multline} 
    \norm{\mc P_{\mathrm{box}}\paren{\mb y}}_2 
    = \norm{\mc P_{\mathrm{box}}\paren{\mb y} - \mc P_{\mathrm{box}}\paren{\mb 0}}_2 \\
     \le \norm{\mb y - \mb 0}_2 = \norm{\mb y}_2 \le \eps
\end{multline} 
due to the contraction property of projecting onto convex sets. Therefore, $\mc P_{\mathrm{box}} \mc P_{2}(\mb y)$ is feasible for any $\mb y \in \RJU^n$. Now for any $\mb y$ inside the box: 
\begin{itemize} 
   \item if $\norm{\mb y}_2 < \eps$, $\mc P_2(\mb y) = \mb y$ and $\mc P_2(\mb y)$ remains in the box; 
   \item if $\norm{\mb y}_2 \ge \eps$, $\mc P_2(\mb y) = \eps \frac{\mb y}{\norm{\mb y}_2}$. Since $\eps/\norm{\mb y}_2 \in [0, 1]$, $\mb P_2(\mb y)$ shrinks each component of $\mb y$, but retains their original signs. Thus, $\mb P_2(\mb y)$ remains in the box if $\mb y$ is in the box. 
\end{itemize} 
We conclude that $\mc P_{2} \mc P_{\mathrm{box}}(\mb y)$ is feasible for any $\mb y$, completing the proof. 
\end{proof}


% Let $d_C(\mb x)$ denote the distance function to a closed convex set $C$: 
% \begin{align} 
%     d_C(\mb x) \doteq \inf_{\mb z \in C} \norm{\mb z - \mb x}_2. 
% \end{align} 
% For the set indicator function, 
% \begin{align} 
%     \partial \imath_C(\mb x) = N_C(\mb x) = \ol{\bigcup_{\lambda \ge 0} \lambda \partial d_C(\mb x)} \quad \forall\; \mb x \in C 
% \end{align} 
% where $\ol{\cdot}$ means set closure, and $N_C$ is the normal cone. Now we are ready to prove \cref{thm:app_proj_inf_lemma}. 


% When $p=2$, the projection problem in \cref{app:projection:opt} also has a closed-form solution, which turns out to be the projection onto the $2-$norm ball and the box constraints sequentially. That is, 
% \[
% \text{Proj}_{2,\text{box}} (\delta)= \text{Proj}_{\text{box}} ( \text{Proj}_{2} (\delta)),
% \]
% where $\text{Proj}_{\text{box}}$ and $\text{Proj}_{2} (\delta)$ represent the projection onto the box constraints $\delta + x \in [0,1]^d$ and $\|\delta\|_2\leq \varepsilon$, respectively. This is actually due to the result in \cite[Theorem 1]{yu2013decomposing}. To show the above equation, it suffices to check the condition in the cited results that
% \begin{equation}\label{eq:inclusion}
% \partial {\bf 1}_{2}(\delta) \subseteq \partial {\bf 1}_{2}(\text{Proj}_{\text{box}} (\delta)), 
% \end{equation}
% where ${\bf 1}_{\text{box}}(z) := \left\{\begin{array}{ll}
% 0 & \mbox{if $z + x \in [0,1]^d$}\\ +\infty & \mbox{otherwise}
% \end{array}\RJUight.$
% and $\partial f$ denotes the subdifferential of a convex function $f$.
% In fact, we have
% \[
% \text{Proj}_{2} (\delta) =  \left\{\begin{array}{ll}
% \displaystyle\frac{\varepsilon}{\|\delta\|_2} \delta & \mbox{if $\|\delta\|_2> \varepsilon$}\\[0.15in] \delta & \mbox{otherwise}
% \end{array}\RJUight.
% \]
% and 
% \[
% \partial {\bf 1}_{\text{box}}(\delta_i) = \left\{\begin{array}{ll}
% %(-\infty, 0] & \mbox{if $z_i = -x_i$} \\
% (-\infty,0] & \mbox{if $z_i = -x_i$} \\[0.1in]
% [0, \infty) & \mbox{if $z_i = 1-x_i$} \\[0.1in]
% \{0\} & \mbox{if $-x_i < z_i < 1-x_i$} \\[0.1in]
% \emptyset & \mbox{otherwise}.
% \end{array}\RJUight.
% \]
% Notice that the absolute value of each component of $\delta$ after the projection onto the $2-$norm ball $\text{Proj}_{2} (\delta)$ cannot be larger than its original counterpart. Therefore, for those components $i$ such that $\delta_i = -x_i$, we have $-x_i \leq \left(\text{Proj}_{2} (\delta)\RJUight)_i < 1-x_i$; similarly for those components $i$ such that $\delta_i = 1-x_i$, we have $-x_i < \left(\text{Proj}_{2} (\delta)\RJUight)_i \leq  1-x_i$. The inclusion \RJUef{eq:inclusion} can thus be verified easily.

% For the case $p=1$, the formula to compute \cref{app:projection:opt} has been shown in \cite{croce2021mind}.