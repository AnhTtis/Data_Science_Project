\section{Introduction}
\label{Sec:introduction}
\input{Sections/section_elements/Tab-Acronym}
In visual recognition, deep neural networks (DNNs) are not robust against perturbations that are easily discounted by human perception---either adversarially constructed or naturally occurring~\citep{szegedy2013intriguing,GoodfellowEtAl2015Explaining,hendrycks2018benchmarking,EngstromEtAl2019Exploring,xiao2018spatially,WongEtAl2019Wasserstein,LaidlawFeizi2019Functional,HosseiniPoovendran2018Semantic,BhattadEtAl2019Big}.
\change{A popular way to formalize robustness is by \emph{adversarial loss}, attempting to find the worst-case perturbation(s) of images within a prescribed radius,} by solving the following constrained optimization problem~\cite{HuangEtAl2015Learning, madry2017towards}:
\begin{align}
\label{eq:robust_loss}
   \begin{split}
       \max_{\mb x'}\; & \ell\paren{\mb y, f_{\mb \theta}(\mb x')}\\
   \st \; d\paren{\mb x, \mb x'}& \le \eps\; ,  \quad  \mb x' \in [0, 1]^n   \; \text{,}
   \end{split}
\end{align}
\change{which we will refer to as \emph{max-loss form} in the following context.
Here, $\mb x \in \mathcal{X}$ is a test image from the set $\mathcal{X}$}, $f_{\mb \theta}$ is a DNN parameterized by $\mb \theta$, 
$\mb x'$ is a perturbed version of $\mb x$ with an allowable perturbation radius $\eps$ measured by the distance metric $d$, and $\mb x' \in [0, 1]$ ensures that $\mb x'$ is a valid image ($n$: the number of pixels).
\change{Another popular formalism of robustness is by \emph{robustness radius}, defined as the minimal level of perturbation(s) so that $f_{\mb \theta}(\mb x)$ and $f_{\mb \theta}(\mb x')$ can lead to different predictions, which was first introduced in~\cite{szegedy2013intriguing}, even earlier than max-loss form (\ref{eq:robust_loss})}: 
\begin{align}  
\label{eq:min_distort}
\begin{split}
& \min_{\mb x'}\;  d\paren{\mb x, \mb x'} \\
\st \;  \max_{i \ne y} f_{\mb \theta}^i & (\mb x') \ge f_{\mb \theta}^y (\mb x')\; , \; \mb x' \in [0, 1]^n ~ \text{,} 
\end{split}
\end{align}
which we will refer to as \emph{min-radius form} in the following context.
Here, $y$ is the true label of $\mb x$ and $f_{\mb \theta}^i$ represents the $i^{th}$ logit output of the DNN model.
Early work assumes that the distance metric $d$ in max-loss form and min-radius form is the $\ell_p$ distance, where $p= 1, 2, \infty$ are popular choices~\cite{madry2017towards,GoodfellowEtAl2015Explaining}. Recent work has also modeled non-trivial transformations using metrics other than the $\ell_p$ distances~\cite{hendrycks2018benchmarking,EngstromEtAl2019Exploring,xiao2018spatially,WongEtAl2019Wasserstein,LaidlawFeizi2019Functional,HosseiniPoovendran2018Semantic,BhattadEtAl2019Big,laidlaw2021perceptual} to capture visually realistic perturbations and generate adversarial examples with more varieties.

As for applications, max-loss form is usually associated with an attacker in the adversarial setup, where the attacker tries to find an input that is visually similar to $\mb x$ but can fool $f_{\mb \theta}$ to predict incorrectly, as the solutions to (\ref{eq:robust_loss}) lead to worst-case perturbations to alter the prediction from the desired label.
Thus, it is popular to perform robust evaluation (RE) using max-loss form: generating perturbations over a validation dataset and reporting the classification accuracy (termed \emph{robust accuracy}) using the perturbed samples. 
Max-loss form also motivates \emph{adversarial training} (AT) 
to try to achieve adversarial robustness (AR)~\cite{GoodfellowEtAl2015Explaining,HuangEtAl2015Learning,madry2017towards}: %~\textcolor{red}{(although which seems unlikely; see \cref{subsec: difficult in achieving AR})}:
\begin{align} 
\label{eq:minmax_obj}
    \min_{\mb \theta} \; \bb E_{\paren{\mb x, \mb y} \sim \mc D} \max_{\mb x' \in \Delta(\mb x)} \ell\paren{\mb y, f_{\mb \theta}(\mb x')}
\end{align} where $\mc D$ is the data distribution, and $\Delta(\mb x) = \{\mb x' \in [0, 1]^n: d(\mb x, \mb x') \le \eps\}$, in contrast to the classical supervised learning:
\begin{equation} 
\min_{\mb \theta} \bb E_{\paren{\mb x, \mb y} \sim \mc D}\; \ell\paren{\mb y, f_{\mb \theta}(\mb x)} \; \text{.}
\end{equation}
As for min-radius form, it is also a popular choice to calculate robust accuracy in RE\footnote{One can also perform AT using (\ref{eq:min_distort}) via bi-level optimization; see, e.g., \cite{ZhangEtAl2021Revisiting}.}~\cite{CroceHein2020Minimally, croce2020reliable, PintorEtAl2021Fast}, as solving (\ref{eq:min_distort}) will also produce samples $\mb x'$ to fool the model. But more importantly, solving min-radius form can be used to estimate the sample-wise robustness radius---a quantity that can be used to measure the robustness for every input. However, it is common that existing methods for solving (\ref{eq:min_distort}) emphasize the role of finding $\mb x'$ but overlook the importance of the robustness radius, e.g.,\cite{CroceHein2020Minimally, PintorEtAl2021Fast}.

Despite being popular, solving max-loss form and min-radius form is not easy when DNNs are involved. For max-loss form, the objective is non-concave for typical choices of loss $\ell$ and model $f_{\mb \theta}$; for non-$\ell_p$ metrics $d$, $\mb x'$ often belongs to a complicated non-convex set. In practice, there are two major lines of algorithms: 
\textbf{1) direct numerical maximization} that takes (sub-)differentiable $\ell$ and $f_{\mb \theta}$, and tries direct maximization: for example, using gradient-based methods~\cite{madry2017towards,croce2020reliable}. This often produces sub-optimal solutions and may lead to overoptimistic RE;
\textbf{2) upper-bound maximization} that constructs tractable upper bounds for the margin loss:
\begin{equation}
\label{eq: margin loss}
    \ell_{\mathrm{ML}} = \max_{i \ne y} f_{\mb \theta}^i (\mb x') - f_{\mb \theta}^y (\mb x') ~ \text{,}
\end{equation} 
and then optimizes against the upper bounds~\cite{SinghEtAl2018Fast,SinghEtAl2018Boosting,SinghEtAl2019abstract,SalmanEtAl2019Convex,DathathriEtAl2020Enabling,MuellerEtAl2022PRIMA}. Improving the tightness of the upper bound while maintaining tractability remains an active area of research. It is also worth mentioning that, since $\ell_{\mathrm{ML}} > 0$ implies attack success, upper-bound maximization can also be used for RE by providing an underestimate of robust accuracy~\cite{SinghEtAl2018Fast,SinghEtAl2018Boosting,SinghEtAl2019abstract,SalmanEtAl2019Convex,DathathriEtAl2020Enabling,MuellerEtAl2022PRIMA,WongKolter2017Provable,RaghunathanEtAl2018Certified,WongEtAl2018Scaling,DvijothamEtAl2018Training,LeeEtAl2021Towards}.
As for min-radius form, it can be solved exactly by mixed-integer programming for small-scale cases with certain restrictions on $f_{\mb \theta}$ and some choices of $d$~\cite{TjengEtAl2017Evaluating,KatzEtAl2017Reluplex,BunelEtAl2020Branch}. For general $f_{\mb \theta}$ and some $d$, lower bounds to the robustness radius can be calculated~\cite{WengEtAl2018Towards,ZhangEtAl2018Efficient,WengEtAl2018Evaluating,LyuEtAl2020Fastened}. Whereas in practice, min-radius form is heuristically solved by gradient-based methods or iterative linearization~\cite{szegedy2013intriguing,MoosaviDezfooliEtAl2015DeepFool,Hein2017,CarliniWagner2016Towards,rony2019decoupling,CroceHein2020Minimally,PintorEtAl2021Fast} to obtain upper bounds of the robustness radius; see \cref{sec:background}.

Currently, applying direct numerical methods to solve max-loss form and min-radius form is the most popular strategy for RE in practice, e.g.,~\cite{croce2021robustbench}. However, there are two major limitations:
\begin{itemize}[leftmargin=*]
    \item \textbf{Lack of reliability:} current direct numerical methods are able to obtain solutions but do not assess the solution quality. Therefore, the reliability of the solutions is questionable. \cref{Fig:APGD-FAB-Terminate-Iter} shows that the default stopping point (determined by the maximum allowed number of iterations (MaxIter)) used in \texttt{AutoAttack}~\cite{croce2020reliable} mainly leads to premature termination of the optimization process. In fact, it is challenging to terminate the optimization process by setting the MaxIter, since the actual number of iterations needed can vary from sample to sample.
    
    \item \textbf{Lack of generality:} existing numerical methods are applied mostly to problems where $d$ is $\ell_1$, $\ell_2$ or $\ell_\infty$ distance\footnote{The only exception we have found is~\cite{croce2019sparse}, where perturbations constrained by the $\ell_0$ distance are considered. However, this perturbation model has not been adapted to current empirical RE.} but cannot handle other distance metrics. In fact, the popular RE benchmark, \texttt{robustbench}~\cite{croce2021robustbench}, only has $\ell_2$ and $\ell_\infty$ leaderboards, and the most studied adversarial attack model is the $\ell_\infty$ attack~\cite{bai2021recent}.
\end{itemize}
It is important to address these two limitations, \change{as reliability relates to how much we can trust the evaluation, and the generality of handling more distance metrics can enable RE with more diverse perturbation models for broader applications.}

\input{Sections/section_elements/Fig-Sec3-Ablation-APGD-TerminateIter.tex}
\input{Sections/section_elements/Fig-Sec3-Abaltion-PWCF-Max-TerminateIter.tex}

\vspace{1em}
\noindent \textbf{Our contributions}  \quad
In this paper, we focus on the numerical optimization of max-loss form and min-radius form.\footnote{Both formulations have their targeted versions: i.e., replacing $[\max f_{\mb \theta}^i (\mb x')]$ by $f_{\mb \theta}^i (\mb x')$ in (\ref{eq:min_distort}); using $\ell_{\mathrm{ML}}$ in (\ref{eq:robust_loss}). The targeted versions are indeed simpler than the untargeted (\ref{eq:min_distort}) and (\ref{eq:robust_loss}). Thus, in this paper, we will focus only on untargeted ones.} First, we provide a general solver that can handle both formulations with almost everywhere differentiable distance metrics $d$, which also has a principled stopping criterion to assess the reliability of the solution.
\begin{enumerate}[leftmargin=*]
    \item{We adapt the constrained optimization solver~\texttt{PyGRANSO}~\cite{curtis2017bfgs,BuyunLiangSun2021NCVX}~\textbf{w}ith~\textbf{C}onstraint-\textbf{F}olding (PWCF), which is crucial for boosting the speed and solution quality of \pygranso\, in solving the max-loss form and min-radius form problems; see \cref{pygranso general tricks}.}
    
    \item{\pygranso~is equipped with a rigorous line-search rule and stopping criterion. \change{As a result, the reliability of PWCF's solution can be assessed by the constraint violations and stationarity estimate, and users can determine if further optimization is needed; see \cref{Fig: PWC-Max-Terminate-Iter} and \cref{subsec: PWCF techniques demo}.}}
    
    \item{We show that PWCF not only can perform comparably to state-of-the-art RE packages, such as \texttt{AutoAttack}, on solving max-loss form and min-radius form with $\ell_1$, $\ell_2$, and $\ell_\infty$ distances and provide diverse solutions, but also can handle distance metrics $d$ other than the popular but limited $\ell_1$, $\ell_2$, and $\ell_\infty$}---beyond the reach of existing numerical methods; see \cref{Sec: experiments and results}.
\end{enumerate}
We also investigate the sparsity patterns of the solutions found by solving max-loss form and min-radius form using different combinations of solvers, losses, and distance metrics, and discuss the implications:
\begin{enumerate}[leftmargin=*]
    \item Different combinations of distance metrics $d$, losses $\ell$, and optimization solvers can result in different sparsity patterns in the solutions found. In terms of computing robust accuracy, combining solutions with all possible patterns is important to obtain reliable and accurate results; see \cref{sec:pattern_theory} and \cref{subsec: diversity matters for robust accuracy}. 
    \item \change{The robust accuracy at a preset perturbation level $\eps$ used in current RE is often a bad metric to measure robustness. Instead, solving the min-radius form is more beneficial for RE, as the sample-wise robustness radius contains richer information; see \cref{subsec: robust accuracy is bad metric} and \cref{subsec: min radius is better RE metric}.}
    \item{Due to the pattern difference in solving max-loss form by different combinations of solvers, losses, and distance metrics, the common practice of AT with projected gradient descent (PGD) method on a single distance metric may not be able to achieve generalizable AR---DNNs that are adversarially trained may only be robust to the patterns they have seen during training; see \cref{subsec: difficult in achieving AR}.}
\end{enumerate}
\change{Although previous works, e.g., \cite{CarliniEtAl2019Evaluating, croce2020reliable, gilmer2018motivating}, have mentioned the necessity of involving diverse solvers to achieve more reliable RE, our paper is, to the best of our knowledge, the first to quantify the meaning of diversity in terms of the sparsity patterns. \cite{liang2022optimization} is a preliminary version of this work, which only contains the contents related to solving max-loss form numerically by PWCF. This paper expands~\cite{liang2022optimization} with results on solving min-radius form by PWCF, extensive analysis on the solution patterns, and discussions on the implications.}

Preliminary versions of this paper have been published in \cite{liang2022optimization,liang2023implications}. 