\subsection{Danskin's theorem and min-max optimization}
\label{sec:danskin_minmax}
In this section, we discuss the importance of computing a good solution to the inner maximization problem when applying first-order methods for AT, i.e., solving \formulation (\ref{eq:minmax_obj}). 

Consider the following minimax problem: 
\begin{equation}\label{app:danskin}
\min_{\mb \theta} g(\mb \theta) \doteq  \left[ \max_{\mb x' \in \Delta} \;  h(\mb \theta, \mb x')\right], 
\end{equation}
where we assume that the function $h$ is locally Lipschitz continuous. To apply first-order methods to solve \cref{app:danskin}, one needs to evaluate a (sub)gradient of $g$ at any given $\mb \theta$. If $h(\mb \theta, \mb x')$ is smooth in $\mb \theta$, one can invoke Danskin's theorem for such an evaluation (see, for example, \cite[Appendix A]{madry2017towards}). However, in DL applications with non-smooth activations or losses, $h(\mb \theta, \delta)$ is not differentiable in $\mb \theta$, and hence a general version of Danskin's theorem is needed.

To proceed, we first introduce a few basic concepts in nonsmooth analysis; general background can be found in~\cite{Clarke1990Optimization,BagirovEtAl2014Introduction,CuiPang2021Modern}.  For a locally Lipschitz continuous function $\varphi:\mathbb{R}^n \to \mathbb{R}$, define its Clarke directional derivative at $\bar{\mb z}\in \mathbb{R}^n$ in any direction $\mb d\in \mathbb{R}^n$ as
\[
\varphi^\circ(\bar{\mb z}; \mb d)\doteq \limsup_{t\downarrow 0, \mb z \to \bar{\mb z}} \frac{\varphi(\mb z+t \mb d) - \varphi(\mb z)}{t}
\]
We say $\varphi$ is Clarke regular at $\bar{\mb z}$ if $\varphi^\circ(\bar{\mb z};\mb d) = \varphi^\prime(\bar{\mb z};\mb d)$ for any $\mb d\in \mathbb{R}^n$, where $\varphi^\prime(\bar{\mb z};\mb d) \doteq \displaystyle\lim_{t\downarrow 0} \frac{1}{t} \paren{\varphi(\bar{\mb z} +t \mb d) - \varphi(\bar{\mb z})}$ is the usual one-sided directional derivative. The Clarke subdifferential of $\varphi$ at $\bar{\mb z}$ is defined as
\[
\partial \varphi (\bar{\mb z}) \doteq \left\{\mb v \in \mathbb{R}^n: \varphi^\circ(\bar{\mb z};\mb d) \geq \mb v^\TJU \mb d \right\}
\]
The following result has its source in \cite[Theorem 2.1]{clarke1975generalized}; see also \cite[Section 5.5]{CuiPang2021Modern}.
\begin{theorem}
Assume that $\Delta$ in \cref{app:danskin} is a compact set, and the function $h$ satisfies
\begin{enumerate}
    \item $h$ is jointly upper semicontinuous in $(\mb \theta, \mb x')$;
    \item $h$ is locally Lipschitz continuous in $\mb \theta$, and the Lipschitz constant is uniform in $\mb x' \in \Delta$;
    \item $h$ is directionally differentiable in $\mb \theta$ for all $\mb x' \in \Delta$; 
\end{enumerate}
If $h$ is Clarke regular in $\mb \theta$ for all $\mb \theta$, and $\partial_{\mb \theta} h$ is upper semicontinuous in $(\mb \theta, \mb x')$, we have that for any $\bar{\mb \theta}$ 
\begin{align} 
\partial g(\bar{\mb \theta}) = \mathrm{conv}\{\partial h(\bar{\mb \theta},\mb x'): \mb x' \in \Delta^*(\bar{\mb \theta})\}
\end{align} 
where $\mathrm{conv}(\cdot)$ denotes the convex hull of a set, and $\Delta^*(\bar{\mb \theta})$ is the set of all optimal solutions of the inner maximization problem at $\bar{\mb \theta}$.
\end{theorem}
The above theorem indicates that in order to get an element from the subdifferential set $\partial g(\bar{\mb \theta})$, we need to get at least one {\bf optimal} solution $\mb x' \in \Delta^*(\bar{\mb \theta})$. A suboptimal solution to the inner maximization problem may result in a useless direction for the algorithm to proceed. To illustrate this, let us consider a simple one-dimensional example
\[
\min_\theta  g(\theta) := \left[\, \max_{-1\leq x' \leq 1}  \;\; \max(\theta x', 0)^2\,\right]
\]
which corresponds to a one-layer neural network with one data point $(0,0)$, the ReLU activation function and squared loss. Starting at $\theta_0 = 1$, we get the first inner maximization problem $\max_{-1\leq x' \leq 1} \max(x',0)^2$. Although its global optimal solution is $x'_* = 1$, the point $x' = 0$ is a stationary point satisfying the first-order optimality condition. If the latter point is mistakenly adopted to compute an element in $\partial g(\theta^0)$, it would result in a zero direction so that the overall gradient descent algorithm cannot proceed. 