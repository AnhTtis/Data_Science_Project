\section{MinMax Training for Patterns} 
\label{sec:min max} 

\input{Sections/section_elements/Table-Pattern-Compare}
% It is widely known that robustness does not generalize across even simple $\ell_p$ distance models ~\cite{maini2020adversarial,CroceHein2019Provable}. For example, $\ell_1$ distance seems to be a strong attack model for other distances such as $\ell_2$ and $\ell_\infty$, and even on itself. The patterns we discuss here provide a plausible explanation for why it is expected. Instead of digging deeper into the generalization aspect, here we focus on building reliable RE schemes, especially those non-adaptive ones that provide basic RE in the line of \texttt{AutoAttack}. In \cref{tab: pattern comare}, we quantify how different variants of loss+norm+solver+initialization+iteration combination can complement each other in terms of producing better attacks, and the message is clear: there is almost always room to complement any attack method, sometimes substantially. For example, while M+APGD seems a clear winner over our PWCF on the $\ell_1$ model, the opposite is true for the $\ell_2$ model. There never seems to be a clear winner. This reinforces the guidance on RE in~\cite{CarliniEtAl2019Evaluating,CroceEtAl2020RobustBench}: diversity of solutions matters! Meanwhile, this also highlights the fundamental difficulty of performing reliable RE: the existence of potentially infinite global maximizers with distinct patterns for \cref{eq:robust_loss} may bring back the curse of dimensionality that we try to avoid via maximization. 