\section{Implications from the variety patterns}
\label{sec: implication} 
Now that we have demonstrated the complex interplay of loss $\ell$, distance metric $d$, and the numerical solver for the final solution patterns in \cref{sec:pattern_theory}, we will discuss what this can imply for the reseach of adversarial robustness.

\subsection{Current empirical RE may not be sufficient}
\label{subsec: limiation of current RE}
As introduced in \cref{Sec:introduction}, the most popular empirical RE practice currently is solving max-loss form with a preset level of $\eps$, using a fixed set of algorithms. Then robust accuracy is reported using the perturbed samples found~\cite{croce2021robustbench, papernot2016technical, rauber2017foolbox}. Here, we challenge its validity for measuring robustness.

\subsubsection{Diversity matters for robust accuracy to be trustworthy}
\label{subsec: diversity matters for robust accuracy}
As shown in \cref{sec:pattern_theory}, the perturbations found by different numerical methods can have different sparsity patterns; \cref{{tab: granso_l1_acc}} also shows that combining multiple methods can lead to a lower robust accuracy than any single method. This implies that for robust accuracy to be numerically reliable, including as many solvers to cover as many patterns as possible is necessary. Although works as~\cite{CarliniEtAl2019Evaluating, croce2020reliable, gilmer2018motivating} have mentioned the necessity of diversity in solvers, our paper is the first to quantify such diversity in terms of sparsity patterns from their solutions. However, the existence of infinitely many patterns may be possible, and it is thus possible that faithful robust accuracy may not be able to achieve in practice.

\subsubsection{Robust accuracy is not a good robustness metric}
\label{subsec: robust accuracy is bad metric}
The motivation of using max-loss form for RE is usually associated with the attacker-defender setting, where solutions ($\mb x'$) are viewed as a test bench for all possible future attacks. Ideally, the DNNs must be robust against all of the adversarial samples found. However, it is questionable whether robust accuracy faithfully reflects this notion of robustness: 
\textbf{1)} why the commonly used budget $\eps$ in max-loss form is a reasonable choice needs to be justified. For example, $\eps=0.03$ is commonly used for the $\ell_\infty$ distance, e.g., in~\cite{croce2021robustbench}. We could not find rigorous answers in the previous literature and suspect that the choices are purely empirical. For example, \cite{croce2021robustbench} states the motivation as
\say{...the true label should stay the same for each in-distribution input within the perturbation set...}
but this claim can also support using other values; \textbf{2)} more importantly, Fig. 1. in~\cite{sridhar2022towards} shows that a model having a higher robust accuracy than other models at one $\eps$ level does not imply that such model is also more robust at other levels. The clean-robust accuracy trade-off~\cite{raghunathan2019adversarial, yang2020closer} may also be interpreted similarly\footnote{The `clean-robust accuracy trade-off' refers to the phenomenon where a model that is non-adversarially trained has the best clean accuracy (at level $\eps=0$) and the worst robust accuracy (at the commonly used $\eps$), and vise versa for the models that are adversarially trained.}---they are just most robust to different $\eps$ levels. Thus, robust accuracy is not a complete and trustworthy measure, and conclusions about robustness drawn from robust accuracy based on a single $\eps$ level are misleading.

\subsubsection{Robustness radius is a better robustness measures}
\label{subsec: min radius is better RE metric}
If our goal is indeed to understand the robustness limit of a given DNN model, solving min-loss-form seems more advantageous, especially that:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Robustness radius is more reliable:} unlike that the pattern differences in solving max-loss form can lead to unreliable robust accuracy due to the possibility of multiple solutions, the robustness radius found by solving min-radius form is not sensitive to the existence of multiple solutions.
    \item \textbf{Robustness radius is sample adaptive:} in contrast to the rigid perturbation budget $\eps$ used in max-loss form, the robustness radius is the (sample-wise) distance to the closest decision boundaries.
\end{enumerate}
A clear application of the robustness radius is that we can identify hard (less robust) samples for a given model if the corresponding robustness radii are small.

\input{Sections/section_elements/Fig-PAT-Lp-Pattern-Comparison}

\subsection{Adversarial training may not help to achieve generalizable robustness}
\label{subsec: difficult in achieving AR}

\subsubsection{Solution patterns can explain why $\ell_p$ robustness does not generalize}
\label{subsec: ungeneralizable lp}
Despite the effort of finding ways to achieve generalizable AR, it is widely observed that AR achieved by AT does not generalize across simple $\ell_p$ distances~\cite{maini2020adversarial,CroceHein2019Provable}. For example, models adversarially trained by $\ell_\infty$-attacks do not achieve good robust accuracy with $\ell_2$-attacks; $\ell_1$ seems to be a strong attack for all other $\ell_p$ distances, and even on itself. 
Note that \cite{madry2017towards} has observed that the (approximate) global maximizers are distinct and spatially scattered; the patterns we discussed in \cref{sec:pattern_theory} provide a plausible explanation of why AR achieved by AT is expected not to be generalizable---the model just cannot perform well on an unseen distribution (patterns) from what it has seen during training.

\subsubsection{Adversarial training with perceptual distances does not solve the generalization issue}
\label{subsec: ungeneralizable perceptual metric}
\cite{laidlaw2021perceptual} claims that using PD (\cref{Eq. LPIPS Constraint}) in max-loss form can approximate the universal set of adversarial attacks, and models adversarially trained with PAT can generalize to other unseen attacks. However, we challenge the above conclusion: `unseen attacks' does not necessarily translate to `novel perturbations', especially if we investigate the patterns:
\begin{enumerate}[leftmargin=*]
    \item If we test the models pretrained by $\ell_2$-attack and PAT\footnote{Correspond to $\ell_2$ and PAT-AlexNet in Table 3 of \cite{laidlaw2021perceptual}} in \cite{laidlaw2021perceptual} by APGD-CE-$\ell_{1}~(\eps=1200)$ attack (on ImageNet-100 images), both will achieve $0 \%$ robust accuracy---models pretraiend with PAT do not generalize better to $\ell_1$ attacks compared with others.
    \item By investigating the sparsity patterns similar to \cref{sec:pattern_theory}, the adversarial perturbations generated by solving max-loss form with PD are shown to be similar to the APGD-CE-$\ell_2$ generated ones, see (a)-(d) in~\cref{Fig:pat l2 sparsity}. This may explain why the $\ell_2$ and PAT  pre-trained models in \cite{laidlaw2021perceptual} have comparable robust accuracy against multiple tested attacks.
    \item Substituting the $\ell_2$ distance by $\ell_1$ in \cref{Eq. LPIPS Constraint} as the new PD:
    \begin{align}
        d(\mb x, \mb x') \doteq \norm{\phi(\mb x) - \phi(\mb x')}_{1} ~ \text{,}
    \end{align}
    the solution patterns will change; see (e)-(f) in~\cref{Fig:pat l2 sparsity}. Furthermore, (d)-(e) in \cref{Fig:pat l2 sparsity} also shows that different solvers (LPA and PWCF) will also result in different patterns even for PD---PAT will likely suffer from the pattern differences the same way as popular $\ell_p$-attacks, thus not being `universal'.
\end{enumerate} 
To conclude, we think that it is so far unclear whether using the perceptual distances in the AT pipeline can be beneficial in addressing the generalization issue in robustness.


