\begin{figure}
    \centering
    \includegraphics[width=0.2\textwidth]{Figures/new_pattern_vis/orig.png}
    \caption{A `fish' image example from the Imagenet-100 validation that is used to generate the pattern visualizations in \cref{Fig:max pattern vis} and \cref{Fig:min pattern vis}.}
    \label{fig:dish image example}
\end{figure}

\section{Different combinations of $\ell$, $d$, and the solvers prefer different patterns}
\label{sec:pattern_theory}

\input{Sections/section_elements/Fig-New-Pattern-Max.tex}
\input{Sections/section_elements/Fig-New-Pattern-Min.tex}
\input{Sections/section_elements/Fig-New-Pattern-Histo-Max.tex}
\input{Sections/section_elements/Fig-New-Pattern-Histo-min.tex}

We now demonstrate that using different combinations of \textbf{1)} distance metrics $d$, \textbf{2)} solvers, and \textbf{3)} losses $\ell$ can lead to different sparsity patterns in the following two ways:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Visualization of perturbation images:} we take a `fish' image (\cref{fig:dish image example}) from ImageNet-100 validation set, employ various combinations of losses $\ell$, distance metrices $d$ and solvers to the max-loss form and min-radius form. \cref{Fig:max pattern vis} and \cref{Fig:min pattern vis} visualize the perturbation image $\mb x' - \mb x$, and the histogram of the element-wise error magnitude $\abs{\mb x' - \mb x}$ to display the difference in pattern.
    \item \textbf{Statistics of sparsity levels:} we use the soft sparsity measure $\norm{\mb x' - \mb x}_1 / \norm{\mb x' - \mb x}_2$ to quantify the patterns---the higher the value, the denser the pattern. \cref{Fig:max pattern hist} and \cref{Fig:min pattern hist} display the histograms of the sparsity levels of the error images derived by solving max-loss form and min-radius form, respectively. Here, we used a fixed set of $500$ ImageNet-100 images from the validation set.
\end{enumerate} 

Contrary to the $\ell_1$ distance which induces sparsity, $\ell_\infty$ promotes dense perturbations with comparable entry-wise magnitudes~\cite{StuderEtAl2012Signal} and $\ell_2$ promotes dense perturbations whose entries follow power-law distributions. These varying sparsity patterns due to $d$'s are evident when we compare the solutions with the same solver and loss but with different distances, where \textbf{1)} the shapes of the histograms in \cref{Fig:min pattern vis} and the ranges of the values are very different; \textbf{2)} the sparsity measures show a shift from left to right along the horizontal axis in \cref{Fig:max pattern hist} and \cref{Fig:min pattern hist}. In addition to $d$'s, we also highlight other patterns induced by the loss $\ell$ and the solver:
\begin{itemize}[leftmargin=*]
    \item \textbf{Using margin and cross-entropy losses in solving max-loss form induce different sparsity patterns} \quad 
    Columns `cross-entropy' and `margin' of PWCF in \cref{Fig:max pattern vis} depict the pattern difference with clear divergences in the histograms of error magnitude; for example, the error values of PWCF-$\ell_2$-margin are more concentrated towards $0$ compared to PWCF-$\ell_2$-cross-entropy. The sparsity measures in \cref{Fig:max pattern hist} can further confirm the existence of the difference due to the loss used to solve max-loss form, more for PWCF than APGD.
    \item \textbf{PWCF's solutions have more variety in sparsity than APGD and FAB} \quad
    For the same $d$ and loss used to solve max-loss form, \cref{Fig:max pattern hist} shows that PWCF's solutions have a wider spread in the sparsity measure than APGD. The same observation can be found in \cref{Fig:min pattern hist} as well between PWCF and FAB in solving min-radius form.
\end{itemize} 

\begin{figure}[!tb]
% \vspace{-1em}
  \centering
    \includegraphics[width=0.3\textwidth]{Figures/multi_sol_2.png}
  \caption{Geometry of max-loss form with multiple global maximizers. $\mb u$ and $\mb v$ are the basis vectors of the 2-dimensional coordinate. Here we consider the $\ell_1$-norm ball around $\mb x$, and ignore the box constraint $\mb x' \in [0, 1]^n$. Depending on the loss $\ell$ used, part or the whole of the blue regions becomes the set of global or near-global maximizers.} 
  \label{fig:multi_sol} 
  % \vspace{-1em}
\end{figure}
\begin{figure}
% \vspace{-1em}
  \centering
  \includegraphics[width=0.4\textwidth]{Figures/min_l1_dist_arrow.png}
  \caption{Histogram of the $\ell_1$ robustness radii estimated by solving min-radius form for $88$ CIFAR-10 images. $\eps=12$ (red dashed line) is the typical preset perturbation budget used in max-loss form (\formulation (\ref{eq:robust_loss})).} 
  \label{fig: eps selection} 
  % \vspace{-1em}
\end{figure}

Here, we provide a conceptual explanation of why different sparsity patterns can occur. We take the $\ell_1$ distance (i.e., $\norm{\mb x - \mb x'}_1 \le \eps$) and ignore the box constraint $\mb x' \in [0, 1]$ in max-loss form as an example. For simplicity, we take the loss $\ell$ as $0/1$ classification error $\ell (\mb y, f_{\mb \theta}(\mb x')) = \indicator{\max_{i} f^i_{\mb \theta}(\mb x') \ne y}$. Note that $\ell$ is maximized whenever $f^i_{\mb \theta}(\mb x') > f^y_{\mb \theta}(\mb x')$ for a certain $i \ne y$, so that $\mb x'$ crosses the local decision boundary between the $i$-th and $y$-th classes; see \cref{fig:multi_sol}. In practice, people set a substantially larger perturbation budget in max-loss form than the robustness radius of many samples, which can be estimated by solving min-radius form---see \cref{fig: eps selection}. Thus, there can be infinitely many global maximizers (the shaded blue regions in \cref{fig:multi_sol}). As for the patterns, the solutions in the shaded blue region on the left are denser in pattern than the solutions on the top. For other general losses, such as cross-entropy or margin loss, the set of global maximizers may change, but the patterns can possibly be more complicated due to the typically complicated nonlinear decision boundaries associated with DNNs. As for min-radius form, multiple global optimizers and pattern differences can exist as well, but the optimizers share the same radius.


