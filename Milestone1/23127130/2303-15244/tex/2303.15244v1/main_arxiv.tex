\documentclass[a4paper]{scrartcl}



\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,amsfonts,siunitx,commath}
\usepackage{amsthm}
\usepackage{tikz,url}
\usepackage{geometry}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{comment}
\usepackage{bbm}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\todo}[1]{\textcolor{red}{\textbf{TODO:} #1}}
\newcommand\prox{\mathrm{prox}}
\newcommand\vecmax{\mathrm{vecmax}}
\newcommand\trace{\mathrm{trace}}
\newcommand\Ker{\mathrm{Ker}}
\newcommand\lin{\mathrm{span}}
\newcommand\SPD{\mathrm{SPD}}
\newcommand\dx{\,\mathrm{d}}
\newcommand\tT{\mathrm{T}}
\newcommand\ELBO{\mathrm{ELBO}}


\theoremstyle{plain}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{proposition}[lemma]{Proposition}

\theoremstyle{remark}
\newtheorem{remark}[lemma]{Remark}

%
\theoremstyle{definition}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{example}[lemma]{Example}

\begin{document}
\title{Manifold Learning by Mixture Models of VAEs for Inverse Problems}

\author{Giovanni S. Alberti\footnotemark[1]\and Johannes Hertrich\footnotemark[2] \and Matteo Santacesaria\footnotemark[1] \and Silvia Sciutto\footnotemark[1]}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\date{}
\maketitle
\footnotetext[1]{MaLGa Center, Department of Mathematics, University of Genoa, Italy, giovanni.alberti@unige.it, matteo.santacesaria@unige.it, silvia.sciutto@edu.unige.it} 
\footnotetext[2]{Institute of Mathematics,
  TU Berlin,
  Stra{\ss}e des 17. Juni 136, 
  10623 Berlin, Germany,
  j.hertrich@math.tu-berlin.de
} 

\renewcommand{\thefootnote}{\arabic{footnote}}

\begin{abstract}
Representing a manifold of very high-dimensional data with generative models has been shown to be computationally efficient in practice.
However, this requires that the data manifold admits a global parameterization.
In order to represent manifolds of arbitrary topology, we propose to learn a mixture model of variational autoencoders.
Here, every encoder-decoder pair represents one chart of a manifold.
We propose a loss function for maximum likelihood estimation of the model weights and choose an architecture that provides
us the analytical expression of the charts and of their inverses.
Once the manifold is learned, we use it for solving inverse problems by minimizing a data fidelity term restricted to the learned manifold.
To solve the arising minimization problem we propose a Riemannian gradient descent algorithm on the learned manifold.
We demonstrate the performance of our method for low-dimensional toy examples as well as for deblurring and 
electrical impedance tomography on certain image manifolds.
\end{abstract}

%--------------------------------------------------------------------
\section{Introduction}
%--------------------------------------------------------------------

\paragraph{Manifold learning.}

The treatment of high-dimensional data is often computationally costly and numerically unstable.
Therefore, in many applications, it is important to find a low-dimensional representation of high-dimensional datasets.
Classical methods, like the principal component analysis (PCA) \cite{P1901},  assume that the data is contained in a low-dimensional subspace.
However, for complex datasets this assumption appears to be too restrictive, particularly when working with image datasets. 
Therefore, recent methods rely on the so-called manifold hypothesis \cite{BCV2013}, stating that even complex and high-dimensional datasets are 
contained in a low-dimensional manifold.
Based on this hypothesis, in recent years, many successful approaches have been based on generative models, able to represent high dimensional data in $\R^n$ by a generator $D\colon\R^d\to\R^n$ with $d\ll n$: these include generative adversarial networks (GANs) \cite{GPMX2014},  
variational autoencoders (VAEs) \cite{KW2013}, injective flows \cite{KKHD2021} and score-based diffusion models \cite{song2019generative,ho2020denoising}. For a survey on older approaches to manifold learning, the reader is referred to \cite{ma2011manifold,2012-izenman} and to the references therein.

\paragraph{Learning manifolds with multiple charts.} Under the assumption that $D$ is injective, the set of generated points $\{D(z):z\in\R^d\}$ forms a manifold that approximates the training set.
However, this requires that the data manifold admits a global parameterization. 
In particular, it must not be disconnected or contain holes.
In order to model disconnected manifolds, the authors of \cite{PL2018} propose to model the latent space of a VAE by a Gaussian mixture model.
Similarly, the authors of \cite{DFDKT2018,MLMTT2019,RMP2020} propose latent distributions defined on Riemannian manifolds for representing general topologies. In \cite{2022-massa-garbarino-benvenuto}, the manifold is embedded into a higher-dimensional space, in the spirit of Whitney embedding theorem.
However, these approaches have the drawback that the topology of the manifold has to be known a priori, which is usually not the case in practice.

Here, we focus on the representation of the data manifold by several charts.
A chart provides a parameterization of an open subset from the manifold by defining a mapping from the manifold into a Euclidean space.
Then, the manifold is represented by the collection of all of these charts, which is called atlas.
For finding these charts, the authors of \cite{CDJ2021,FG2021,PRA2013,SDJBS2022} propose the use of clustering algorithms.
By default, these methods do not provide an explicit formulation of the resulting charts.
As a remedy, the authors of \cite{PRA2013} use linear embeddings.
The authors of \cite{SDJBS2022} propose to learn for each chart again a generative model.
However, these approaches often require a large number of charts and are limited to relatively low data dimensions.
The idea of representing the charts by generative models is further elaborated in \cite{K2018,K2021,SCL2019}.
Here, the authors proposes to train at the same time several (non-variational) autoencoders and a classification network that decides
for each point to which chart it belongs.
In contrast to the clustering-based algorithms, the computational effort scales well for large data dimensions.
On the other hand, the numerical examples in the corresponding papers show that the approach already has difficulties to approximate small toy examples like a torus.

In this paper, we propose to approximate the data manifold by a mixture model of VAEs. 
Using Bayes theorem and the ELBO approximation of the likelihood term we derive a loss function for maximum likelihood estimation of the model weights.
Mixture models of generative models for modelling disconnected datasets were already considered in \cite{BGP2017,HNLP2017,LVTRGS2018,SS2022,YB2021}.
However, they are trained in a different way and to the best of our knowledge none of those is used for manifold learning.

\paragraph{Inverse Problems on Manifolds.}

Many problems in applied mathematics and image processing can be formulated as inverse problems.
Here, we consider an observation $y$ which is generated by
\begin{equation}\label{eq:inverse}
y=\mathcal G(x)+\eta,
\end{equation}
where $\mathcal G\colon\R^n\to\R^m$ is an ill-posed or ill-conditioned, possibly nonlinear, forward operator and $\eta$ represents additive noise.
Reconstructing the input $x$ directly from the observation $y$ is usually not possible due to the ill-posed operator and the high dimension of the problem.
As a remedy, the incorporation of prior knowledge is required. 
This is usually achieved by using regularization theory, namely, by minimizing the sum of a data fidelity term $F(x)$ and a regularizer $R(x)$, where $F$ describes the fit of $x$ to $y$ and $R$ incorporates
the prior knowledge.
With the success of deep learning, data-driven regularizers became popular \cite{ADHHMS2022,AMOS2019,GNBDU2022,HHR2022,LOS2018}.

In this paper, we consider a regularizer which constraints the reconstruction $x$ to a learned data manifold $\mathcal M$.
More precisely, we consider the optimization problem
$$
\hat x=\argmin_x F(x)\quad \text{subject to $x\in\mathcal M$,}
$$
where $F(x)=\frac12\|\mathcal G(x)-y\|^2$ is a data-fidelity term.
This corresponds to the regularizer $R(x)$ which is zero for $x\in\mathcal M$ and infinity otherwise.
When the manifold admits a global parameterization given by one single generator $D$, the authors of \cite{ASS2022,CKFB2020,DCE2021,GAT2022} propose to 
reformulate the problem as $\hat x=\hat z$, where $\hat z=\argmin_z F(D(x))$.
Since this is an unconstrained problem, it can be solved by gradient based methods.
However, since we consider manifolds represented by several charts, this reformulation cannot be applied.
As a remedy, we propose to use a Riemannian gradient descent scheme.
In particular, we derive the Riemannian gradient using the decoders and encoders of our manifold
and propose two suitable retractions for applying a descent step into the gradient direction.

To emphasize the advantage of using multiple generators, we demonstrate the performance of our method on numerical examples.
We first consider some two- and three-dimensional toy examples.
Finally, we apply our method to deblurring and to electrical impedance tomography (EIT), a nonlinear inverse problem consisting in the reconstruction of the leading coefficient of a second order elliptic PDE from the knowledge of the boundary values of its solutions  \cite{cheney1999electrical}.
The code of the numerical examples is available online\footnote{\url{https://github.com/johertrich/Manifold_Mixture_VAEs}}.

\paragraph{Outline.}

The paper is organized as follows. In Section~\ref{sec:VAEs}, we revisit VAEs and fix the corresponding notations.
Afterwards, in Section~\ref{sec:chart_learning}, we introduce mixture models of VAEs for learning embedded manifolds of arbitrary dimensions and topologies. Here, we focus particularly on the derivation of the loss function and of the architecture, which allows us to access the charts and their inverses.
For minimizing functions defined on the learned manifold, we propose a Riemannian gradient descent scheme in Section~\ref{sec:opt_prob}.
We provide numerical toy examples for one and two dimensional manifolds in Section~\ref{sec:toy_examples}.
In Section~\ref{sec:inverse_problems}, we discuss the applications to deblurring and to electrical impedance tomography.
Conclusions are drawn in Section~\ref{sec:conclusions}.


%--------------------------------------------------------------------
\section{Variational Autoencoders for Manifold Learning}\label{sec:VAEs}
%--------------------------------------------------------------------

In this paper, we assume that we are given data points $x_1,\dots,x_N\in\R^n$ for a large dimension $n$.
In order to reduce the computational effort and to regularize inverse problems, we assume that these data-points are located in a lower-dimensional manifold.
We aim to learn the underlying manifold from the data points $x_1,\dots,x_N$ with a VAE \cite{KW2013,KW2019}.

A VAE aims to approximate the underlying high-dimensional probability distribution $P_X$ of the random variable $X$ 
with a lower-dimensional latent random variable $Z\sim P_Z$ on $\R^d$ with $d<n$, by using the data points $x_1,\dots,x_N$.
To this end, we define a decoder $D\colon\R^d\to\R^n$ and an encoder $E\colon\R^n\to\R^d$. The decoder approximates $P_X$ by the distribution $P_{\tilde X}$ of a random variable $\tilde X\coloneqq D(Z)+\eta$, where $\eta\sim\mathcal N(0,\sigma_x^2 I_n)$.
Vice versa, the encoder approximates $P_Z$ from $P_X$ by the distribution $P_{\tilde Z}$ of the random variable $\tilde Z\coloneqq E(X)+\xi$ with $\xi\sim\mathcal N(0,\sigma_z^2 I_d)$.
Now, decoder and encoder are trained such that we have $P_X\approx P_{\tilde X}$ and $P_Z\approx P_{\tilde Z}$.
To this end, we aim to maximize the log-likelihood function
$
\ell(\theta)=\sum_{i=1}^N\log(p_{\tilde X}(x_i)),
$
where $\theta$ denotes the parameters  $D$ and $E$ depend upon.

The log-density $\log(p_{\tilde X}(x))$ induced by the model is called the evidence. However, for VAEs the 
computation of the evidence is intractable. Therefore, the authors of \cite{KW2013} suggest to approximate the evidence  by Jensens's inequality as
\begin{align}
\log(p_{\tilde X}(x))&=\log\Big(\int_{\R^d} p_{Z,\tilde X}(z,x) \dx z\Big)
\\&=\log\Big(\int_{\R^d} \frac{p_{Z,\tilde X}(z,x)}{p_{\tilde Z|X=x}(z)}p_{\tilde Z|X=x}(z) \dx z\Big)\\
&\geq \int_{\R^d} \log\Big(\frac{p_{Z,\tilde X}(z,x)}{p_{\tilde Z|X=x}(z)}\Big)p_{\tilde Z|X=x}(z) \dx z\Big)\\
&=\E_{z\sim P_{\tilde Z|X=x}}\Big[\log\Big(\frac{p_{Z}(z)p_{\tilde X|Z=z}(x)}{p_{\tilde Z|X=x}(z)}\Big)\Big]\\
&=\E_{z\sim P_{\tilde Z|X=x}}[\log(p_Z(z))+\log(p_{\tilde X|Z=z}(x))-\log(p_{\tilde Z|X=x}(z))].
\end{align}
Accordingly to the definition of $\tilde Z$ and $\tilde X$, we have that $p_{\tilde X|Z=z}(x)=\mathcal N(x;D(z),\sigma_x^2 I_n)$ and $p_{\tilde Z|X=x}(z)=\mathcal N(z;E(x),\sigma_z^2 I_d)$.
Thus, the above formula is, up to a constant, equal to
\begin{align}
\E_{z\sim P_{\tilde Z|X=x}}[\log(p_Z(z))-\tfrac1{2\sigma_x^2}\|x-D(z)\|^2-\log(\mathcal N(z;E(x),\sigma_z^2 I_d))].
\end{align}
Considering the substitution $\xi=(z-E(x))/\sigma_z$, we obtain
\begin{align}
\E_{\xi\sim\mathcal N(0,I_d)}[\log(p_Z(E(x)+\sigma_z\xi))-\tfrac1{2\sigma_x^2}\|D(E(x)+\sigma_z\xi)-x\|^2
-\log(\mathcal N(\xi;0,I_d))].
\end{align}
Note that also the last summand does not depend on $D$ and $E$. Thus, we obtain, up to a constant, the \emph{evidence lower bound} (ELBO) given by
\begin{equation}\label{eq:ELBO_}
\ELBO(x|\theta)\coloneqq \E_{\xi\sim\mathcal N(0,I_d)}[\log(p_Z(E(x)+\sigma_z\xi))-\tfrac1{2\sigma_x^2}\|D(E(x)+\sigma_z\xi)-x\|^2].
\end{equation}
Finally, a VAE is trained by minimizing the loss function which sums up the negative ELBO values of all data points, i.e.,
$$
\mathcal L_\mathrm{VAE}(\theta)=-\sum_{i=1}^N\ELBO(x_i|\theta).
$$

\paragraph{Learned Latent Space.}
To improve the expressiveness of our VAEs, we use a latent space learned by a normalizing flow.
This was first proposed in \cite{RM2015} and further used, e.g., in \cite{DW2019}. 
Here, we employ the specific loss function from \cite{HHS2022,HHS2023} for training the arising model.
More precisely, we choose the latent distribution
$$
P_Z={\mathcal T}_\#P_{\Xi},
$$
where $\mathcal T\colon\R^d\to\R^d$ is an invertible neural network, called normalizing flow. In this way, $P_Z$ is the push-forward of a fixed (known) distribution $P_{\Xi}$.
Then, the density $p_{Z}$ is given by
$$
p_{Z}(z)=p_{\Xi}(\mathcal T^{-1}(z))|\mathrm{det}(\nabla \mathcal T^{-1}(z))|.
$$
The parameters of $\mathcal T$ are considered as trainable parameters.
Then, the ELBO reads as
\begin{equation}\label{eq:ELBO}
\begin{aligned}
\ELBO(x|\theta)&\coloneqq \E_{\xi\sim\mathcal N(0,I_d)}[\log(p_{\Xi}(\mathcal T^{-1}(E(x)+\sigma_z\xi)))\\
&+\log(|\mathrm{det}(\nabla \mathcal T^{-1}(E(x)+\sigma_z\xi))|)-\tfrac1{2\sigma_x^2}\|D(E(x)+\sigma_z\xi)-x\|^2],
\end{aligned}
\end{equation}
where $\theta$ are the parameters of the decoder, the encoder and of the normalizing flow $\mathcal T$.

In the literature, there exist several invertible neural network architectures based on coupling blocks \cite{DSB2016,KD2018}, residual networks \cite{BGCD2019,CBDJ2029,H2022}, ODE representations \cite{CRBD2018,GCBS2018,OFLR2021} and autoregressive flows \cite{HKLC2018}.
In our numerics, we use the coupling-based architecture from~\cite{AKWR2018}.

\paragraph{Manifold Learning with VAEs.} In order to obtain a lower-dimensional representation of the data points, some papers propose to approximate the
data-manifold by $\mathcal M\coloneqq\{D(z):z\in\R^d\}$, see e.g.~\cite{ASS2022,CKFB2020,DCE2021,GAT2022}. 
However, this is only possible if the data-manifold admits a global parameterization, i.e., it can be approximated by one
generating function. 
This assumption is often violated in practice.
As a toy example, consider the one-dimensional manifold embedded in $\R^2$ that consists of two circles, see Figure~\ref{fig:toy_ex_data}. 
This manifold is disconnected and contains ``holes''. 
Consequently, the topologies of the manifold and of the latent space $\R$ do not coincide,  so that the manifold cannot be approximated by a VAE.
Indeed, this can be verified numerically. 
When we learn a VAE for approximating samples from this manifold, we observe that the two (generated) circles are not closed and that both components are connected, see Figure~\ref{fig:one_gen}.
As a remedy, in the next section, we propose the use of multiple generators to resolve this problem, see Figure~\ref{fig:four_gen}.

\begin{figure}
\begin{subfigure}[t]{0.31\textwidth}
\includegraphics[width=\textwidth]{imgs/data_two_circles.pdf}
\caption{Noisy samples from the manifold.}
\label{fig:toy_ex_data}
\end{subfigure}\hfill
\begin{subfigure}[t]{0.31\textwidth}
\includegraphics[width=\textwidth]{imgs/plot_two_circles_1_gen.pdf}
\caption{Approximation with one chart.}
\label{fig:one_gen}
\end{subfigure}\hfill
\begin{subfigure}[t]{0.31\textwidth}
\includegraphics[width=\textwidth]{imgs/plot_two_circles_4_gen.pdf}
\caption{Approximation with four charts.}
\label{fig:four_gen}
\end{subfigure}
\caption{Example of a one-dimensional manifold that admits no global parameterization.}
\end{figure}

%--------------------------------------------------------------------
\section{Chart Learning by Mixtures of VAEs}
\label{sec:chart_learning}
%--------------------------------------------------------------------

In order to approximate (embedded) manifolds with arbitrary (unknown) topology, we propose to learn several local parameterizations of the manifold instead of a global one. 
To this end, we propose to use mixture models of VAEs.

\paragraph{Embedded Manifolds.}

A subset $\mathcal M\subseteq \R^n$ is called a $d$-dimensional embedded differentiable  manifold if there exists a family $(U_k,\varphi_k)_{k\in I}$ of relatively open sets $U_k\subseteq \mathcal M$ with $\bigcup_{k\in I} U_k=\mathcal M$ and mappings $\varphi_k\colon U_k\to\R^d$
such that it holds for every $k,l\in I$ that
\begin{itemize}
\item[-] $\varphi_k$ is a homeomorphism between $U_k$ and $\varphi_k(U_k)$;
\item[-] the inverse $\varphi_k^{-1}\colon\varphi_k(U_k)\to U_k$ is continuously differentiable;
\item[-] the transition maps $\varphi_k\circ\varphi_l^{-1}\colon \varphi_l(U_k\cap U_l)\to\varphi_k(U_k\cap U_l)$ are continously differentiable;
\item[-] and the Jacobian $\nabla \varphi_k^{-1}(x)$ of $\varphi_k^{-1}$ at $x$ has full column-rank for any $x\in\varphi_k(U_k)$.
\end{itemize}
We call the mappings $\varphi_k$ charts and the family $(U_k,\varphi_k)_{k\in I}$ an atlas.
With an abuse of notation, we sometimes also call the set $U_k$ or the pair $(U_k,\varphi_k)$ a chart.
 Every compact manifold admits an atlas with finitely many charts $(U_k,\varphi_k)_{k=1}^K$, by definition of compactness.


\paragraph{An Atlas as Mixture of VAEs.}

In this paper, we propose to learn the atlas of an embedded manifold $\mathcal M$ by representing it as a mixture model of variational autoencoders with decoders $D_k\colon\R^d\to\R^n$, encoders $E_k\colon\R^n\to\R^d$ and normalizing flows $\mathcal T_k$ in the latent space, for $k=1,\dots,K$.
Then, the inverse of each chart $\varphi_k$ will be represented by  $\varphi_k^{-1}=\mathcal D_k\coloneqq D_k\circ \mathcal T_k$. Similarly, the chart $\varphi_k$ itself is represented by the mapping $\mathcal E_k\coloneqq \mathcal T_k^{-1}\circ E_k$ restricted to the manifold.
Throughout this paper, we denote the parameters of $(D_k,E_k,\mathcal T_k)$ by $\theta_k$.
Now, let $\tilde X_k$, $k=1,\dots,K$, be the random variable generated by the decoder $D_k$ as in the previous section.
Then, we approximate the distribution $P_X$ of the noisy samples from the manifold by the random variable 
$\tilde X\coloneqq\tilde X_J$, where $J$ is a discrete random variable on $\{1,\dots,K\}$ with $P(J=k)=\alpha_k$ with mixing weights $\alpha_k>0$ fulfilling $\sum_{k=1}^K\alpha_k=1$.

%--------------------------------------------------------------------
\subsection{Training of Mixtures of VAEs}
%--------------------------------------------------------------------

\paragraph{Loss function.}
Let $x_1,\dots,x_N$ be the noisy training samples. 
In order to train mixtures of VAEs, we again minimize an approximation of the negative log likelihood function $-\sum_{i=1}^N\log(p_{\tilde X}(x_i))$.
To this end, we employ the law of total probability and obtain 
$$
\log(p_{\tilde X}(x_i))=\sum_{k=1}^K \beta_{ik}\log(p_{\tilde X_k}(x_i)),
$$
where $\beta_{ik}\coloneqq P(J=k|\tilde X=x_i)$ is the probability that the sample $x_i$ 
was generated by the $k$-th random variable $\tilde X_k$.
Using the definition of conditional probabilities, we observe that
\begin{equation}\label{eq_true_betas}
\beta_{ik}=P(J=k|\tilde X=x_i)=\frac{P(J=k)p_{\tilde X_k}(x_i)}{p_{\tilde X}(x_i)}=\tfrac{\alpha_k p_{\tilde X_k}(x_i)}{\sum_{j=1}^K \alpha_j p_{\tilde X_j}(x_i)}.
\end{equation}
As the computation of $p_{\tilde X_k}$ is intractable, we replace it by the ELBO \eqref{eq:ELBO}, i.e., we approximate $\beta_{ik}$ by
\begin{equation}\label{eq_approx_betas}
\tilde \beta_{ik}=\tfrac{\alpha_k \exp(\ELBO(x_i|\theta_k))}{\sum_{j=1}^K \alpha_j \exp(\ELBO(x_i|\theta_j))}.
\end{equation}
Then, we arrive at the approximation
\begin{equation}\label{eq:nll_approx}
\log(p_{\tilde X}(x_i))\approx \ell(x_i|\Theta)=\sum_{k=1}^K\tfrac{\alpha_k \exp(\ELBO(x_i|\theta_k))}{\sum_{j=1}^K \alpha_j \exp(\ELBO(x_i|\theta_j))} \ELBO(x_i|\theta_k).
\end{equation}
By summing up over all $i$, we approximate the negative log likelihood function by the loss function
$$
\mathcal L(\Theta)=-\sum_{i=1}^N\ell(x_i|\Theta),
$$
in order to train the parameters $\Theta=(\theta_k)_{k=1}^K$ of the mixture of VAEs.
Finally, this loss function is then optimized with a stochastic gradient based optimizer like Adam \cite{KB2014}.

\begin{remark}[Lipschitz Regularization]
In order to stabilize the training, we add in the first few training epochs a regularization term which penalizes the Lipschitz constant of the decoders $D_k$ and of the normalizing flows $\mathcal T_k$ in the latent space.
More precisely, for some small $\sigma>0$, we add the regularization term 
$$
\mathcal R(\Theta)\coloneqq \frac1{\sigma^2}\sum_{k=1}^K\E_{z\sim P_\Xi,\eta\sim\mathcal N(0,\sigma^2)}\Big[D_k(\mathcal T_k(z))-D_k(\mathcal T_k(z+\eta))\Big].
$$
\end{remark}

\paragraph{Latent Distribution.}

We choose the distribution $P_{\Xi}$ by using the density $p_{\Xi}(z)\coloneqq \prod_{i=1}^d q(z_i)$, where the density $q$ is up to a multiplicative constant given by
$$
q(z)\propto 
\begin{cases}
1,&|x|<0.8,\\ 
4.8 - 4.75 |x|,&|x|\in[0.8,1],\\
0.05 \exp(-100(|x|-1)),&|x|>1,
\end{cases}
$$
see Figure~\ref{fig:latent} for a plot.
Note that the mass of the distribution $P_{\Xi}$ is mostly concentrated in $(-1,1)^d$.
This allows us to identify the set $U_k$ corresponding to the $k$-th chart. 
More precisely, we define the domain $U_k$ of the $k$-th learned chart as the set $U_k\coloneqq \mathcal D_k((-1,1)^d)$. 

Due to approximation errors and noise, we will have to deal with points $x\in\R^n$ that are not exactly located in one of the sets $U_k$. 
In this case, we cannot be certain to which charts the point $x$ actually belongs.
Therefore, we interpret the conditional probability \eqref{eq_true_betas} as the probability that $x_i$ belongs to the $k$-th chart. Since we cannot compute the $\beta_{ik}$ explicitly, we use the approximations 
$\tilde \beta_{ik}$ from \eqref{eq_approx_betas} instead. 


\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{imgs/latent_density.pdf}
\caption{Plot of the unnormalized latent density $q$.}
\label{fig:latent}
\end{figure}

\paragraph{Overlapping Charts.}

Since the sets $U_k$ of an atlas $(U_k,\varphi_k)_{k\in I}$ are an open covering of the manifold, they have to overlap near their boundaries.
To this end, we propose the following post-processing heuristic.


By the definition of the loss function $\mathcal L$, we have that the $k$-th generator $\mathcal D_k$ is trained such that $U_k$ contains all points $x_i$ from the training set where $\tilde \beta_{ik}$ is non-zero.
The following procedure  modifies the $\tilde \beta_{ik}$ in such a way that the samples $x$ that are close to the boundary of the $k$-th chart will also be assigned to a second chart.

Since the measure $P_{\Xi}$ is mostly concentrated in $(-1,1)^d$, the region close to the boundary of the $k$-th chart can be identified by $D_k(\mathcal T_k(z))$ for all $z$ close to the boundary of $(-1,1)^d$.
For $c>1$, we define the modified ELBO function
\begin{equation}\label{eq:modELBO}
\begin{aligned}
\ELBO_c(x|\theta_k)&\coloneqq \E_{\xi\sim\mathcal N(0,I_d)}[\log(p_{\Xi}(c \mathcal T_k^{-1}(E_k(x)+\sigma_z\xi)))\\
&+\log(|\mathrm{det}(\nabla \mathcal T_k^{-1}(E_k(x)+\sigma_z\xi))|)-\tfrac1{2\sigma_x^2}\|D_k(E_k(x)+\sigma_z\xi)-x\|^2]
\end{aligned}
\end{equation}
which differs from \eqref{eq:ELBO} by the additional scaling factor $c$ within the first summand.
By construction and by the definiton of $p_\Xi$, it holds that $\ELBO_c(x,\theta_k)\approx\ELBO(x|\theta_k)$ whenever $c\|\mathcal T_k^{-1}(E_k(x))\|_\infty< 0.8$ and $0<\sigma_z\ll0.1$ is small. Otherwise, we have $\ELBO_c(x|\theta_k)< \ELBO(x|\theta_k)$
In particular, $\ELBO_c(x|\theta_k)$ is close to $\ELBO(x|\theta_k)$ if $x$ belongs to the interior of the $k$-th chart and is significantly smaller if it is close to the boundary of the $k$-th chart.

As a variation of $\tilde \beta_{ik}$, now we define 
$$
\hat\gamma_{il}^{(l)}=\frac{\alpha_l\exp(\ELBO_c(x_i|\theta_l))}{\alpha_l\exp(\ELBO_c(x_i|\theta_l))+\sum_{j\in\{1,\dots,K\}\setminus\{l\}} \alpha_j\exp(\ELBO(x_i|\theta_j))}
$$
and
$$
\hat\gamma_{ik}^{(l)}=\frac{\alpha_k\exp(\ELBO(x_i|\theta_k))}{\alpha_l\exp(\ELBO_c(x_i|\theta_l))+\sum_{j\in\{1,\dots,K\}\setminus\{l\}} \alpha_j\exp(\ELBO(x_i|\theta_j))}
$$
for $k,l\in\{1,\dots,K\}$.
Similarly as the $\tilde \beta_{ik}$, $\hat \gamma_{ik}^{(l)}$ can be viewed as a classification parameter, which assigns each $x_i$ either to a chart $k\neq l$ or to the interior part of the $l$-th chart.
Consequently, points located near the boundary of the $l$-th chart will also be assigned to another chart.
Finally, we combine and normalize the $\hat\gamma_{ik}^{(l)}$ by
\begin{equation}\label{eq:gamma}
\gamma_{ik}=\frac{\hat\gamma_{ik}}{\sum_{k=1}^K\hat\gamma_{ik}}, \quad\text{where}\quad\hat \gamma_{ik}=\max_{l=1,\dots,K}\hat\gamma_{ik}^{(l)}.
\end{equation}

Once, the $\gamma_{ik}$ are computed, we update the mixing weights $\alpha$ by $\alpha_k=\sum_{i=1}^N\gamma_{ik}$
and minimize the loss function
\begin{equation}\label{eq:loss_k}
\mathcal L_\text{overlap}(\Theta)=-\sum_{i=1}^N\sum_{k=1}^K \gamma_{ik}\mathrm{ELBO}(x_i|\theta_k)
\end{equation}
using a certain number of epochs of the Adam optimizer \cite{KB2014}.
\\

The whole training procedure for a mixture of VAEs representing the charts of an embedded manifold is summarized in Algorithm~\ref{alg:training}.

\begin{algorithm}
\begin{algorithmic}
\State 1. Run the Adam optimizer on $\mathcal L(\Theta)+\lambda \mathcal R(\Theta)$ for $50$ epochs.
\State 2. Run the Adam optimizer on $\mathcal L(\Theta)$ for $150$ epochs.
\State 3. Compute the values $\gamma_{ik}$, $i=1,\dots,N$, $k=1,\dots,K$ from \eqref{eq:gamma}.
\State 4. Compute the mixing weights $\alpha_k=\sum_{i=1}^N\gamma_{ik}$.
\State 5. Run the Adam optimizer on $\mathcal L_\text{overlap}(\Theta)$ from \eqref{eq:loss_k} for $50$ epochs.
\end{algorithmic}
\caption{Training procedure for mixtures of VAEs.}
\label{alg:training}
\end{algorithm}

%--------------------------------------------------------------------
\subsection{Architectures}\label{sec:architectures}
%--------------------------------------------------------------------

In this subsection, we focus on the architecture of the VAEs used in the mixture model representing the manifold $\mathcal M$.
Since $\mathcal D_k =D_k\circ \mathcal T_k$ represent the inverse of our charts $\varphi_k$, the decoders have to be injective. Moreover, since $\mathcal E_k=\mathcal T_k^{-1}\circ E_k$ represents the chart itself, the condition $\mathcal E_k\circ \mathcal D_k=\mathrm{Id}$ must be verified.
Therefore, we choose the encoder $E_k$ as a left-inverse of the corresponding decoder $D_k$.
More precisely, we use the decoders of the form
\begin{equation}\label{eq_generator}
D_k=T_L\circ A_L\circ\cdots \circ T_1\circ A_1,
\end{equation}
where the $T_l\colon\R^{d_l}\to\R^{d_l}$ are invertible neural networks and $A_l\colon\R^{d_{l-1}}\to\R^{d_l}$ are fixed injective linear operators  for $l=1,\dots,L$, $d=d_0<d_1<\cdots<d_L=n$. 
As it is a concatenation of injective mappings, we obtain that $D_k$ is injective.
Finally, the corresponding encoder is given by
\begin{equation}\label{eq_encoder}
E_k=A_1^\dagger \circ T_1^{-1}\circ\cdots\circ A_L^\dagger\circ T_L^{-1},\quad A^\dagger=(A^\tT A)^{-1}A^\tT.
\end{equation}
Then, it holds by construction that $\mathcal E_k\circ \mathcal D_k= \mathrm{Id}$.

In this paper, we build the invertible neural networks $T_l$ and the normalizing flows $\mathcal T_k$ out of coupling blocks as proposed in \cite{AKWR2018} based on the real NVP architecture \cite{DSB2016}.
To this end, we split the input $z\in\R^{d_l}$ into two parts $z=(z_1,z_2)\in\R^{d_{l}^1}\times\R^{d_{l}^2}$ 
with $d_l=d_{l}^1+d_{l}^2$ and define $T_l(z)=(x_1,x_2)$ with 
$$x_1=z_1 \, \mathrm{e}^{s_{2}(z_2)} + t_{2}(z_2)
\quad\text{and}\quad 
x_2=z_2 \, \mathrm{e}^{s_{1}(x_1)}+ t_{1}(x_1),
$$
where $s_{1},t_{1}\colon\R^{d_{l}^1}\to\R^{d_{l}^2}$ and $s_{2},t_{2}\colon\R^{d_{l}^2}\to\R^{d_{l}^1}$ are arbitrary subnetworks (depending on $l$).
Then, the inverse $T_l^{-1}(x_1,x_2)$ can analytically be derived as $z=(z_1,z_2)$ with 
$$
z_2=\big(x_2 - t_{1}(x_1)   \big) \,\mathrm{e}^{-s_{1}(x_1)}
\quad\text{and}\quad
z_1=\big(x_1 - t_{2}(z_2) \big) \,\mathrm{e}^{-s_{2}(z_2)}.
$$


\begin{remark}[Projection onto learned charts]\label{rem_projection}
Consider a decoder $D_k$ and an encoder $E_k$ as defined above.
By construction, the mapping $\pi_k=D_k\circ E_k$ is a (nonlinear) projection onto $\mathrm{range}(D_k)=\mathrm{range}(\pi_k)$, in the sense that $\pi_k\circ\pi_k=\pi_k$ and that $\pi_{k|\mathrm{range}(D_k)}$ is the identity on $\mathrm{range}(D_k)$.
Consequently, the mapping $\pi_k$ is a projection on the range of $D_k$ which represents the $k$-th chart of $\mathcal M$.
In particular, there is an open neighborhood $V\coloneqq\pi_k^{-1}(U_k)\subseteq\R^n$ such that $\pi_{k|V}$ is a projection onto $U_k$.
\end{remark}

%--------------------------------------------------------------------
\section{Optimization on Learned Manifolds}
\label{sec:opt_prob}
%--------------------------------------------------------------------

As motivated in the introduction, we are interested in  optimization problems of the form
\begin{equation}
\label{opt_prob}
\min_{x\in\R^n} F(x)\quad \text{subject to}\quad x\in\mathcal M,   
\end{equation}
where $F\colon\R^n\to\R$ is a differentiable function and $\mathcal M$ is available only through some data points.
In the previous section, we proposed a way to represent the manifold $\mathcal M$ by a mixture model $(D_k,E_k,\mathcal T_k)$ of VAEs.
This section outlines a gradient descent algorithm for the solution of \eqref{opt_prob} once the manifold is learned.

As outlined in the previous section, the inverse charts $\varphi_k^{-1}$ of the manifold $\mathcal M$ are modeled by $\mathcal D_k\coloneqq D_k\circ\mathcal T_k$. The chart $\varphi_k$ itself is given by the mapping $\mathcal E_k\coloneqq \mathcal T_k^{-1}\circ E_k$ restricted to the manifold.
For the special case of a VAE with a single generator $\mathcal D$, the authors of \cite{ASS2022,CKFB2020,DCE2021} propose to solve \eqref{opt_prob} in the latent space.
More precisely, starting with a latent initialization $z_0\in\R^d$ they propose to solve
$$
\min_{z\in\R^d} F(\mathcal D(z))
$$
using a gradient descent scheme.
However, when using multiple charts, such a gradient descent scheme heavily depends on the current chart.
Indeed, the following example shows that the gradient direction can change significantly, if we use a different chart.

\begin{example}
Consider the two-dimensional manifold $\R^2$ and the two learned charts given by the generators 
$$
\mathcal D_1(z_1,z_2)=(10 z_1,z_2),\quad\text{and}\quad\mathcal D_2(z_1,z_2)=(z_1,10 z_2).
$$
Moreover let $F\colon\R^2\to\R$ be given by $(x,y)\mapsto x+y$.
Now, the point $x^{(0)}=(0,0)$ corresponds for  both charts  to $z^{(0)}=(0,0)$.
A gradient descent step with respect to $F\circ \mathcal D_k$, $k=1,2$, using step size $\tau$ yields the 
latent values
\begin{align}
(z_1^{(1)},z_2^{(1)})&=z^{(0)}-\tau\nabla (F\circ \mathcal D_1)(z^{(0)})=-(10 \tau,\tau),\\
(\tilde z_1^{(1)},\tilde z_2^{(1)})&=z^{(0)}-\tau\nabla (F\circ \mathcal D_2)(z^{(0)})=-(\tau,10 \tau).
\end{align}
Thus, one gradient steps with respect to $F\circ\mathcal D_k$ yields the values
$$
x^{(1)}=\mathcal D_1(z^{(1)})=-(100 \tau,\tau),\quad \tilde x^{(1)}=\mathcal D_2(\tilde z^{(1)})=-(\tau,100\tau).
$$
Consequently, the gradient descent steps with respect to two different charts can point into completely different directions, independently of the step size $\tau$.
\end{example}

Therefore, we aim to use a gradient formulation which is independent of the parameterization of the manifold.
Here, we use the concept of the Riemannian gradient with respect to the Riemannian metric, which is inherited 
from the Euclidean space in which the manifold $\mathcal M$ is embedded.
To this end, we first revisit some basic facts about Riemannian gradients on embedded manifolds which can be found, e.g., in \cite{AMS2009}.
Afterwards, we consider suitable retractions in order to perform a descent step in the direction of the negative Riemannian gradient.
Finally, we use these notions in order to derive a gradient descent procedure on a manifold given by mixtures of VAEs.

\paragraph{Riemannian Gradients on Embedded Manifolds.}

Let $x\in\mathcal M\subseteq\R^n$ be a point on the manifold,  let $\varphi\colon U\to\R^d$ be a chart with $x\in U$ and $\varphi^{-1}\colon\varphi(U)\to U$ be its inverse.
Then, the tangent space is given by the set of all directions $\dot\gamma(0)$ of differentiable curves $\gamma\colon(-\epsilon,\epsilon)\to\mathcal M$ with $\gamma(0)=x$.
More precisely, it is given by the linear subspace of $\R^n$ defined as
\begin{equation}\label{eq:tangent_space}
T_x\mathcal M=\{Jy:y\in\R^d\}, \quad \text{ where } J\coloneqq \nabla \varphi^{-1}(\varphi(x))\in\R^{n\times d}.
\end{equation}
The tangent space inherits the Riemannian metric from $\R^n$, i.e., we equip the tangent space with the inner product
$$
\langle u,v\rangle_x=u^\tT v,\quad u,v\in T_x\mathcal M.
$$


A function $f\colon\mathcal M\to\R^m$ is called differentiable if for any differentiable curve $\gamma\colon(-\epsilon,\epsilon)\to\mathcal M$ we have that $f\circ \gamma\colon(-\epsilon,\epsilon)\to\R^m$ is differentiable.
In this case the differential of $f$ is defined by
$$
D f(x)\colon T_x\mathcal M\to\R^m,\qquad D f(x)[h]=\frac{\dx}{\dx t} f(\gamma_h(t))\Big|_{t=0},
$$
where $\gamma_h\colon(-\epsilon,\epsilon)\to\mathcal M$ is a curve with $\gamma_h(0)=x$ and $\dot\gamma_h(0)=h$.
Finally, the Riemannian gradient of a differentiable function $f\colon\mathcal M\to\R$ is given by the unique element $\nabla_{\mathcal M}f\in T_x\mathcal M$ which fulfills
$$
D f(x)[h]=\langle \nabla_{\mathcal M}f,h\rangle_x \quad\text{for all}\quad h\in T_x\mathcal M.
$$
\begin{remark}\label{rem:riem_grad_ex}
In the case that $f$ can be extended to a differentiable function on a neighborhood of $\mathcal M$, these formulas can be simplified.
More precisely, we have that the differential is given by
$
D f(x)[h]=h^\tT \nabla f(x)
$, where $\nabla f$ is the Euclidean gradient of $f$. In other words, $D f(x)$ is the FrÃ©chet derivative of $f$ at $x$ restricted to $T_x\mathcal M$.
Moreover, the Riemannian gradient coincides with the orthogonal projection of $\nabla f$ on the tangent space, i.e.,
$$
\nabla_{\mathcal M}f(x)=P_{T_x\mathcal M}(\nabla f(x)),\quad P_{T_x\mathcal M}(y)=\argmin_{z\in T_x\mathcal M}\|y-z\|^2.
$$
Here the orthogonal projection can be rewritten as
$P_{T_x\mathcal M}=J (J^\tT J)^{-1} J^\tT$, $J=\nabla \varphi^{-1}(\varphi(x))$
such that the Riemannian gradient is given by $\nabla_{\mathcal M}f(x)=J (J^\tT J)^{-1} J^\tT\nabla f(x)$.
\end{remark}

\paragraph{Retractions.}

Once the Riemannian gradient is computed, we aim to perform a descent step in the direction 
of $-\nabla_{\mathcal M}f(x)$ on $\mathcal M$.
To this end, we need the concept of  retraction.
Roughly speaking, a retraction in $x$ maps a tangent vector $\xi$ to the point that is reached by moving
from $x$ in the direction $\xi$.
Formally, it is defined as follows.
\begin{definition}
A differentiable mapping $R_x\colon V_x\to\mathcal M$ for some neighborhood $V_x\subseteq T_x\mathcal M$ of $0$ is called a retraction in $x$, if $R_x(0)=x$ and
$$
D R_x(0)[h]=h\quad\text{for all}\quad h\in T_0(V_x)= T_x\mathcal M,
$$
where we identified $T_0(V_x)$ with $T_x\mathcal M$.
Moreover, a differentiable mapping $R=(R_x)_{x\in\mathcal M}\colon V\to\mathcal M$ on a subset of the tangent bundle $V=(V_x)_{x\in\mathcal M}\subseteq T\mathcal M=(T_x\mathcal M)_{x\in\mathcal M}$ is a retraction on $\mathcal M$, if for all $x\in \mathcal M$ we have that $R_x$ is a retraction in $x$.
\end{definition}

Now, let $R\colon V\to\mathcal M$ be a retraction on $\mathcal M$. 
Then, the Riemannian gradient descent scheme starting at $x_0\in\mathcal M$ with step size $\tau>0$ is defined by
$$
x_{t+1}=R_{x_t}(-\tau\nabla_{\mathcal M} f(x_t)).
$$

In order to apply this gradient scheme for a learned manifold given by the learned mappings $(\mathcal D_k,\mathcal E_k)_{k=1}^K$, 
we consider two types of retractions. 
We introduce them and show that they are indeed retractions in the following lemmas.
The first one generalizes the idea from \cite[Lemma 4, Proposition 5]{AM2012} of moving along the 
tangent vector in $\R^n$ and reprojecting onto the manifold. 
However, the results from \cite{AM2012} are based on the orthogonal projection, which is hard or even impossible to compute.
Thus, we replace it by some more general projection $\pi$.
In our applications,  $\pi$ will be chosen as in Remark~\ref{rem_projection}, i.e., 
we set
$
\pi(x)=\mathcal D_k(\mathcal E_k(x)).
$

\begin{lemma}\label{lem:retr1}
Let $x\in\mathcal M$, $U_x\subseteq\R^n$ be a neighborhood of $x$ in $\R^n$, $\pi\colon U_x\to\mathcal M\cap U_x$ be a differentiable map such that  $\pi\circ\pi=\pi$. Set $V_x=\{h\in T_x\mathcal M\subseteq\R^n: x+h\in U_x\}$. Then
$$
R_x(h)=\pi(x+h),\qquad h\in V_x,
$$
defines a retraction in $x$.
\end{lemma}
\begin{proof}
The property $R_x(0)=x$ is directly clear from the definition of $R_x$.
Now let $h\in T_x\mathcal M\subseteq\R^n$ and $\gamma_h\colon(-\epsilon,\epsilon)\to\mathcal M$ be a differentiable curve with $\gamma_h(0)=x$ and $\dot\gamma_h(0)=h$.
As ${\pi}_{|U}$ is the identity on $\mathcal M$, we have by the chain rule that
$$
h=\dot\gamma_h(t) =\frac{\dx}{\dx t} \pi(\gamma_h(t)) \Big|_{t=0}=\nabla \pi(x)\dot\gamma_h(0)=\nabla \pi(x) h,
$$
where $\nabla \pi(x)$ is the Euclidean Jacobian matrix of $\pi$ at $x$.
Similarly,
\begin{equation*}
DR_x(0)[h]=\frac{\dx}{\dx t}R_x(th)\Big|_{t=0}=\frac{\dx}{\dx t}\pi(x+th)\Big|_{t=0}=\nabla \pi(x) h=h.    \qedhere
\end{equation*}
\end{proof}

The second retraction uses the idea of changing to local coordinates, moving into the gradient direction by using the local coordinates and then going back to the manifold representation. 
Note that similar constructions are considered in \cite[Section 4.1.3]{AMS2009}.
However, as we did not find an explicit proof for the lemma, we give it below for the sake of completeness.

\begin{lemma}\label{lem:retr2}
Let $x\in\mathcal M$ and denote by $\varphi\colon U\to\R^d$ a chart with $x\in U\subseteq \mathcal M$.
Then,
$$
R_x(h)=\varphi^{-1}(\varphi(x)+(J^\tT J)^{-1}J^\tT h), \quad J=\nabla \varphi^{-1}(\varphi(x))
$$
defines a retraction in $x$.
\end{lemma}
\begin{proof}
The property $R_x(0)=x$ is directly clear from the definition of $R_x$.
Now let $h\in T_0(T_x\mathcal M)=T_x\mathcal M\subseteq\R^n$.
By \eqref{eq:tangent_space}, we have that there exists some $y\in\R^d$ such that $h=Jy$.
Then, we have by the chain rule that
$$
DR_x(0)[h]=\frac{\dx}{\dx t}R_x(th)\Big|_{t=0}=(\nabla \varphi^{-1}(\varphi(x))) (J^\tT J)^{-1} J^\tT h=J(J^\tT J)^{-1} J^\tT Jy=Jy=h.
$$
\end{proof}

\paragraph{Gradient Descent on Learned Manifolds.}

By Lemma~\ref{lem:retr1} and \ref{lem:retr2}, we obtain that the mappings
\begin{equation}\label{eq:retractions}
R_{k,x}(h)=\mathcal D_k(\mathcal E_k(x+h))\quad\text{and}\quad \tilde R_{k,x}(h)=\mathcal D_k(\mathcal E_k(x)+(J^\tT J)^{-1}J^\tT h)
\end{equation}
with $J=\nabla \mathcal D_k(\mathcal E_k(x))$
are retractions in all $x\in U_k$.
If we define $R$ such that $R_x$ is given by $R_k$ for some $k$ such that $x\in U_k$,
then the differentiability of $R=(R_x)_{x\in\mathcal M}$ in $x$ might be violated.
Moreover, the charts learned by a mixture of VAEs only overlap approximately and not exactly.
Therefore, these retractions cannot be extended to a retraction on the whole manifold $\mathcal M$ in general.
As a remedy, we propose the following gradient descent step on a learned manifold.

Starting from a point $x_n$, we first compute for $k=1,\dots,K$ the probability, that $x_n$ belongs to the $k$-th chart. By \eqref{eq_true_betas}, this probability can be approximated by
\begin{equation}\label{eq:beta_descent}
\beta_k\coloneqq \tfrac{\alpha_k\exp(\mathrm{ELBO}(x_n|\theta_k))}{\sum_{j=1}^K \alpha_j\exp(\mathrm{ELBO}(x_n|\theta_j))}.
\end{equation}
Afterwards, we project $x_n$ onto the $k$-th chart by applying $\tilde x_{n,k}=\mathcal D_k(\mathcal E_k(x_n))$ (see Remark~\ref{rem_projection}) and compute the Riemannian gradient $g_{n,k}=\nabla_{\mathcal M}F(\tilde x_{n_k})$.
Then, we apply the retraction $R_{k,\tilde x_{n,k}}$ (or $\tilde R_{k,\tilde x_{n,k}}$) to perform a gradient descent step  
$x_{n+1,k}=R_{k,\tilde x_{n,k}}(-\tau_n g_{n,k})$.
Finally, we average the results by $x_{n+1}=\sum_{k=1}^K\beta_{k}x_{n+1,k}$.

The whole gradient descent step is summarized in Algorithm~\ref{alg:gd_manifold}.
Finally, we  compute the sequence $(x_n)_n$ by applying Algorihtm~\ref{alg:gd_manifold} iteratively.

\begin{algorithm}[t]
\begin{algorithmic}
\State Inputs: Function $F\colon\mathcal M\to\R$, point $x_n$, step size $\tau_n>0$.
\For{$k=1,\dots,K$}
\State - Approximate the probability that $x_{n}$ belongs to chart $k$ by computing the
$
\beta_{k}
$
\State \phantom{- }from \eqref{eq:beta_descent}.
\State - Project to the $k$-th chart by $\tilde x_{n,k}=\mathcal D_k(\mathcal E_k(x_n))$.
\State - Compute the Riemannian gradient $g_{n,k}=\nabla_{\mathcal M} F(\tilde x_{n,k})$, e.g., by Remark~\ref{rem:riem_grad_ex}.
\State - Perform a gradient descent with the retraction $R_{k,\tilde x_{n,k}}$, i.e., define
\State \phantom{- }$
x_{n+1,k}=R_{k,\tilde x_{n,k}}(-\tau_n g_{n,k}).$
\EndFor
\State - Average results by computing
$
x_{n+1}=\sum_{k=1}^K\beta_k x_{n+1,k}.
$
\end{algorithmic}
\caption{One gradient descent step on a learned manifold.}
\label{alg:gd_manifold}
\end{algorithm}

For some applications the evaluation of the derivative of $F$ is computationally costly.
Therefore, we aim to take  as large step sizes $\tau_n$ as possible in Algorithm~\ref{alg:gd_manifold}.
On the other hand, large step sizes can lead to numerical instabilities and divergence.
To this end, we use an adaptive step size selection as outlined in Algorithm~\ref{alg:adaptive_steps}.

\begin{algorithm}[t]
\begin{algorithmic}
\State Input: Function $F$, initial point $x_0$, initial step size $\tau_0$.
\For{n=0,1,\dots}
\State Compute $x_{n+1}$ by Algorithm~\ref{alg:gd_manifold} with step size $\tau_n$.
\While{$F(x_{n+1})>F(x_n)$}
\State Update step size by $\tau_n\leftarrow \frac{\tau_n}2$.
\State Update $x_{n+1}$ by Algorithm~\ref{alg:gd_manifold} with the new step size $\tau_n$.
\EndWhile
\State Set step size for the next step $\tau_{n+1}=\frac{3\tau_n}2$.
\EndFor
\end{algorithmic}
\caption{Adaptive step size scheme for gradient descent on learned manifolds}
\label{alg:adaptive_steps}
\end{algorithm}

%--------------------------------------------------------------------
\section{Numerical Examples}\label{sec:toy_examples}
%--------------------------------------------------------------------

\begin{figure}
\centering
\begin{subfigure}[t]{0.19\textwidth}
\includegraphics[width=\textwidth]{imgs/data_two_circles.pdf}
\caption*{Two circles}
\end{subfigure}
\begin{subfigure}[t]{0.19\textwidth}
\includegraphics[width=\textwidth]{imgs/data_ring.pdf}
\caption*{Ring}
\end{subfigure}
\begin{subfigure}[t]{0.19\textwidth}
\includegraphics[width=\textwidth]{imgs/data_sphere.pdf}
\caption*{Sphere}
\end{subfigure}
\begin{subfigure}[t]{0.19\textwidth}
\includegraphics[width=\textwidth]{imgs/data_swiss.pdf}
\caption*{Swiss roll}
\end{subfigure}
\begin{subfigure}[t]{0.19\textwidth}
\includegraphics[width=\textwidth]{imgs/data_torus.pdf}
\caption*{Torus}
\end{subfigure}
\caption{Datasets used for the different manifolds.}
\label{fig:manifolds_data_toy}
\end{figure}

Next, we test the numerical performance of the proposed method. 
In this section, we start with some one- and two-dimensional manifolds embedded in the two- or three-dimensional Euclidean space.
We use the architecture from Section~\ref{sec:architectures} with $L=1$.
That is, for all the manifolds, the decoder is given by $\mathcal T\circ A$ where $A\colon\R^d\to\R^n$ is given by 
$x\mapsto (x,0)$ if $d=n-1$ and by $A=\mathrm{Id}$ if $d=n$ and $\mathcal T$ is an invertible neural network with $5$ invertible coupling blocks where the subnetworks have two hidden layers and $64$ neurons in each layer. 
The normalizing flow modelling the latent space consists of $3$ invertible coupling blocks with the same architecture.
We train the mixture of VAEs for $200$ epochs with the Adam optimizer. Afterwards we apply the overlapping procedure for $50$ epochs, as in Algorithm~\ref{alg:training}.

We consider the manifolds ``two circles'', ``ring'', ``sphere'', ``swiss roll'' and ``torus''.
The (noisy) training data are visualized in Figure~\ref{fig:manifolds_data_toy}.
The number of charts $K$ is given in the following table.
\begin{center}
\begin{tabular}{c|ccccc}
&Two circles&Ring&Sphere&Swiss roll&Torus\\\hline
Number of charts&$4$&$2$&$2$&$4$&$6$
\end{tabular}
\end{center}
We visualize the learned charts in Figure~\ref{fig:learned_charts_toy}. Moreover, additional samples generated by the learned mixture of VAEs are shown in Figure~\ref{fig:generated_samples_toy}.
We observe that our model covers all considered manifolds and provides a reasonable approximation of different charts.
Finally, we apply the gradient descent method from Algorithm~\ref{alg:gd_manifold} to the following functions:
\begin{itemize}
\item $F(x)=x_2$ on the manifold ``two circles'' with initial points $\frac{x_0}{\|x_0\|}\pm(1.5,0)$ for $x_0=(\pm 0.2,1)$;
\item $F(x)=\|x-(-1,0)\|^2$ on the manifold ``ring'' with initial points $(1,\pm0.4)$;
\item $F(x)=\|x-(0,0,-2)\|^2$ on the manifold ``sphere'' with inital points $x_0/\|x_0\|$ for $x_0\in\{(0.3\cos(\tfrac{\pi k}{5}),0.3\sin(\tfrac{\pi k}{5}),1):k=0,\dots,9)\}$;
\item $F(x)=\|x-(-5,0,0)\|^2$ on the manifold ``torus'', where the inital points are drawn randomly from the training set.
\end{itemize}
We use the retraction from Lemma~\ref{lem:retr1} with a step size of $0.01$. 
The resulting trajectories are visualized in Figure~\ref{fig:trajectories_toy}.
We observe that all the trajectories behave as expected and approach the closest minimum of the objective function, even if this is not in the same chart of the initial point.


\begin{figure}
\centering
\begin{subfigure}[t]{0.24\textwidth}
\includegraphics[width=\textwidth]{imgs/plot_two_circles_4_gen.pdf}
\caption*{Two circles}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
\includegraphics[width=\textwidth]{imgs/plot_ring_2_gen.pdf}
\caption*{Ring}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{imgs/plot_sphere_2_gen.pdf}
\caption*{Sphere}
\end{subfigure}
\begin{subfigure}[t]{\textwidth}
\includegraphics[width=\textwidth]{imgs/plot_swiss_4_gen.pdf}
\caption*{Swiss roll}
\end{subfigure}
\begin{subfigure}[t]{\textwidth}
\includegraphics[width=\textwidth]{imgs/plot_torus_6_gen.pdf}
\caption*{Torus}
\end{subfigure}
\caption{Learned charts for the different manifolds. For the manifolds ``two circles'' and ``ring'', each color represents one chart. For the manifolds ``sphere'', ``swiss roll'' and ``torus'' we plot each chart in a separate figure.}
\label{fig:learned_charts_toy}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}[t]{0.19\textwidth}
\includegraphics[width=\textwidth]{imgs/plot_relevant_two_circles_4_gen.pdf}
\caption*{Two circles}
\end{subfigure}
\begin{subfigure}[t]{0.19\textwidth}
\includegraphics[width=\textwidth]{imgs/plot_relevant_ring_2_gen.pdf}
\caption*{Ring}
\end{subfigure}
\begin{subfigure}[t]{0.19\textwidth}
\includegraphics[width=\textwidth]{imgs/plot_relevant_sphere_2_gen.pdf}
\caption*{Sphere}
\end{subfigure}
\begin{subfigure}[t]{0.19\textwidth}
\includegraphics[width=\textwidth]{imgs/plot_relevant_swiss_4_gen.pdf}
\caption*{Swiss roll}
\end{subfigure}
\begin{subfigure}[t]{0.19\textwidth}
\includegraphics[width=\textwidth]{imgs/plot_relevant_torus_6_gen.pdf}
\caption*{Torus}
\end{subfigure}
\caption{Generated samples by the learned mixture of VAEs. The color of a point indicates from which generator the point was sampled.}
\label{fig:generated_samples_toy}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}[t]{0.24\textwidth}
\includegraphics[width=\textwidth]{imgs/trajectory_two_circles.pdf}
\caption*{Two circles}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
\includegraphics[width=\textwidth]{imgs/trajectory_ring.pdf}
\caption*{Ring}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
\includegraphics[width=\textwidth]{imgs/trajectory_sphere.pdf}
\caption*{Sphere}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
\includegraphics[width=\textwidth]{imgs/trajectory_torus.pdf}
\caption*{Torus}
\end{subfigure}
\caption{Trajectories of the gradient descent on the learned manifolds.}
\label{fig:trajectories_toy}
\end{figure}

\begin{remark}[Dimension of the Manifold]
For all our numerical experiments, we assume that the dimension $d$ of the data manifold is known. 
This assumption might be violated for practical applications.
However, there exist several methods in the literature to estimate the dimension of a manifold from data, see e.g., \cite{BSS2022,CS2016,FGQZ2010,LB2004}.
Combining these methods with our mixture of VAEs is not within the scope of this paper and is left for future research.
\end{remark}

%--------------------------------------------------------------------
\section{Mixture of VAEs for Inverse Problems}\label{sec:inverse_problems}
%--------------------------------------------------------------------

In this section we describe how to use mixture of VAEs to solve inverse problems.
We consider an inverse problem of the form
\begin{equation}
\label{IP}
    y = \mathcal{G}(x)+\eta,
\end{equation}
where $\mathcal{G}$ is a possibly nonlinear map between $\R^n$ and $\R^m$, modeling a measurement (forward) operator, $x \in \R^n$ is a quantity to be recovered, $y \in \R^m$ is the noisy data and $\eta$ represents some noise. In particular, we analyze a linear and a nonlinear inverse problem: a deblurring problem and a parameter identification problem for an elliptic PDE arising in  electrical impedance tomography (EIT), respectively.

In many inverse problems, the unknown $x$ can be modeled as an element of a low-dimensional manifold $\mathcal M$ in $\R^n$ \cite{ASS2022,ASA2020,BAPD2017,HBLLS2021,OJMBDW2020,SKJLH2019,2022-massa-garbarino-benvenuto,2023-alberti-santacesaria}, and this manifold can be represented by the mixture of VAEs as explained in Section~\ref{sec:chart_learning}. Thus, the solution of \eqref{IP} can be found by optimizing the function 
\begin{equation}\label{eq:objective_function}
F(x) = \frac{1}{2} \| \mathcal{G}(x) - y \|^2_{\R^m}\quad\text{subject to }x\in\mathcal{M},
\end{equation}
by using the iterative scheme proposed in Section~\ref{sec:opt_prob}.

We would like to emphasize that the main goal of our experiments is not to obtain state-of-the-art results. 
Instead, we want to highlight the advantages of 
using multiple generators via a mixture of VAEs.
All our experiments are designed in such a way that the manifold property of the data is directly clear.
The application to real-world data and the combination with other methods in order to achieve competitive results are not within the scope
of this paper and are left to future research.


\paragraph{Architecture and Training.}
Throughout these experiments we consider images of size $128\times128$ and use the architecture from Section~\ref{sec:architectures} with $L=3$.
Starting with the latent dimension $d$, the mapping $A_1\colon\R^d\to\R^{32^2}$ fills up the input vector $x$ with zeros up to the size $32^2$, i.e., we set $A_1(x)=(x,0)$. The invertible neural network $T_1\colon\R^{32^2}\to\R^{32^2}$ consists of $3$ invertible blocks, where the subnetworks $s_{i}$ and $t_{i}$, $i=1,2$ are dense feed-forward networks with two hidden layers and $64$ neurons.
Afterwards, we reorder the dimensions to obtain an image of size $32\times32$.
The mappings $A_2\colon\R^{32\times 32}\to\R^{32\times32\times4}$ and $A_3\colon\R^{64\times64}\to\R^{64\times64\times 4}$ copy each channel $4$ times.
Then, the generalized inverses $A_2^\dagger\colon\R^{32\times32\times4}\to\R^{32\times32}$ and $A_3^\dagger\colon\R^{64\times64\times4}\to\R^{64\times64}$ from \eqref{eq_encoder} are given by taking the mean of the four channels of the input.
The invertible neural networks $T_{2}\colon\R^{32\times32\times4}\to\R^{64\times 64}$ and $T_3\colon\R^{64\times64\times 4}\to\R^{128\times128}$ consist of $3$ invertible blocks, where the subnetworks $s_i$ and $t_i$, $i=1,2$ are convolutional neural networks with one hidden layer and $64$ channels.
After these three coupling blocks we use an invertible upsampling \cite{EKS2020} to obtain the correct output dimension.
For the normalizing flow in the latent space, we use an invertible neural network with three blocks, where the subnetworks $s_{i}$ and $t_{i}$, $i=1,2$ are dense feed-forward networks with two hidden layers and $64$ neurons.

We train all the models for 200 epochs with the Adam optimizer. Afterwards we apply the overlapping procedure for $50$ epochs. See Algorithm~\ref{alg:training} for the details  of the training algorithm.

%--------------------------------------------------------------------
\subsection{Deblurring}
%--------------------------------------------------------------------

\begin{figure}[t]
\begin{subfigure}[t]{\textwidth}
\includegraphics[width=\textwidth]{imgs/bar_ds_grid}
\caption{Samples from the considered dataset for the deblurring example.}
\label{fig:dataset_deblurring}
\end{subfigure}

\begin{subfigure}[t]{\textwidth}
\includegraphics[width=\textwidth]{imgs/deblurring_chart_1_gen}
\caption{Learned chart with one generator. The figure shows the images $D(x)$ for $20$ values of $x$ equispaced in $[-1,1]$.}
\label{fig:bar_chart_1_gen}
\end{subfigure}

\begin{subfigure}[t]{\textwidth}
\includegraphics[width=\textwidth]{imgs/deblurring_chart_2_gen1}
\includegraphics[width=\textwidth]{imgs/deblurring_chart_2_gen2}
\caption{Learned charts with two generators. The figure shows the images $D_k(x)$ for $20$ values of $x$ equispaced in $[-1,1]$ for $k=1$ (top) and $k=2$ (bottom).}
\label{fig:bar_chart_2_gen}
\end{subfigure}
\begin{subfigure}[t]{\textwidth}
\includegraphics[width=\textwidth]{imgs/bar_ds_grid}
\includegraphics[width=\textwidth]{imgs/bar_obs_grid}
\includegraphics[width=\textwidth]{imgs/bar_res1_grid}
\includegraphics[width=\textwidth]{imgs/bar_res_grid}
\caption{Reconstructions for the deblurring example. From top to bottom: ground truth image, observation, reconstruction with one generator and reconstruction with two generators.}
\label{fig:deblurring_reconstructions}
\end{subfigure}
\caption{Dataset, learned charts and reconstructions for the deblurring example.}
\end{figure}

\begin{figure}
\begin{subfigure}[t]{\textwidth}
\centering
\includegraphics[width=.05\textwidth]{imgs/move_on_bar_gt}
\includegraphics[width=.05\textwidth]{imgs/move_on_bar_observation}
\caption{Ground truth (left) and observation (right).}
\label{fig:traj_gt_obs}
\end{subfigure}
\begin{subfigure}[t]{\textwidth}
\includegraphics[width=\textwidth]{imgs/move_on_bar_grid_1gen}
\caption{Visualization of the trajectories $(x_n)_n$ for different initializations $x_0$ with one generator.
Left column: initialization, right column: reconstruction $x_{250}$, columns in between: images $x_n$ for $n$ approximately equispaced between $0$ and $250$.}
\label{fig:deblurring_one_chart}
\end{subfigure}
\begin{subfigure}[t]{\textwidth}
\includegraphics[width=\textwidth]{imgs/move_on_bar_grid_2gen}
\caption{Visualization of the trajectories $(x_n)_n$ for different initializations $x_0$ with two generators.
Left column: initialization, right column: reconstruction $x_{250}$, columns in between: images $x_n$ for $n$ approximately equispaced between $0$ and $250$.}
\label{fig:deblurring_two_charts}
\end{subfigure}
\caption{Gradient descent for the deblurring example.}
\end{figure}

First, we consider the inverse problem of noisy image deblurring. 
Here, the forward operator $\mathcal G$ in \eqref{IP} is linear and given by the convolution with a $30\times30$ 
Gaussian blur kernel with standard deviation $15$. 
In order to obtain outputs $y$ of the same size as the input $x$, we use constant padding with intensity $1/2$ within the convolution.
Moreover, the image is corrupted by white Gaussian noise $\eta$ with standard deviation $0.1$.
Given an observation $y$ generated by this degradation process, we aim to reconstruct the unknown ground truth image $x$.

\paragraph{Dataset and Manifold Approximation.}

Here, we consider the dataset of $128\times 128$ images showing a bright bar with a gray background which is centered and rotated. 
The intensity of fore- and background as well as the size of the bar are fixed.
Some example images from the dataset are given in Figure~\ref{fig:dataset_deblurring}.
The dataset forms a one-dimensional manifold parameterized by the rotation of the bar.
Therefore, it is homeomorphic to $S^1$ and does not admit a global parameterization since it contains a hole.

We approximate the data manifold by a mixture model of two VAEs and compare the result with the approximation with a single VAE, where the latent dimension is set to $d=1$. The learned charts are visualized in Figure~\ref{fig:bar_chart_1_gen} and \ref{fig:bar_chart_2_gen}.
We observe that the chart learned with a single VAE does not cover all possible rotations of the bar, while the mixture of two VAEs can generate any rotation. Moreover, we can see that the two charts of the mixture of VAEs overlap.



\paragraph{Reconstruction.}

In order to reconstruct the ground truth image, we use our gradient descent scheme for the function \eqref{eq:objective_function} as outlined in Algorithm~\ref{alg:gd_manifold} for 500 iterations.
Since the function $F$ is defined on the whole $\R^{128\times128}$, we compute the Riemannian gradient $\nabla_{\mathcal M}F(x)$ accordingly to Remark~\ref{rem:riem_grad_ex}.
More precisely, for $x\in U_k$, we have $\nabla_{\mathcal M}F(x)= J (J^\tT J)^{-1} J^\tT\nabla F(x)$, where $\nabla F(x)$ is the Euclidean gradient of $F$ and $J=\nabla \mathcal D_k(\mathcal E_k(x))$ is the Jacobian of the $k$-th decoder evaluated at $\mathcal E_k(x)$.
Here, the Euclidean gradient $\nabla F(x)$ and the Jacobian matrix $J$ are computed by algorithmic differentiation.
Moreover, we use the retractions $\tilde R_{k,x}$ from \eqref{eq:retractions}.
As initialization $x_0$ of our gradient descent scheme, we use a random sample from the mixture of VAEs.
The results are visualized in Figure~\ref{fig:deblurring_reconstructions}.
We observe that the reconstructions with two generators always recover the ground truth images very well. 
On the other hand, the reconstructions with one generator often are unrealistic and do not match with the ground truth. 
These unrealistic images appear at exactly those points where the chart of the VAE with one generator does not cover the data manifold.

In order to better understand why the reconstructions with one generator often fail, we consider the trajectories $(x_n)_{n}$ generated by Algorithm~\ref{alg:gd_manifold} more in detail.
We consider a fixed ground truth image showing a horizontal bar and a corresponding observation as given in Figure~\ref{fig:traj_gt_obs}. Then, we run Algorithm~\ref{alg:gd_manifold} for different initializations. 
The results are given in Figure~\ref{fig:deblurring_one_chart} for one generator and in Figure~\ref{fig:deblurring_two_charts} for two generators. 
The left column shows the initialization $x_0$, and in the right column, there are the values $x_{250}$ after 250 gradient descent steps. 
The columns in between show the values $x_n$ for (approximately) equispaced $n$ between $0$ and $250$.
With two generators, the trajectory $(x_n)_n$ are a smooth transition from the initialization to the ground truth.
Only when the initialization is a vertical bar (middle row), the images $x_n$ remain similar to the initialization $x_0$ for all $n$, since this is a critical point of the $F|_{\mathcal M}$ and hence the Riemannian gradient is zero.
With one generator, we observe that some of the trajectories get stuck. 
This emphasizes the need for multiple generators.

%--------------------------------------------------------------------
\subsection{Electrical Impedance Tomography}
%--------------------------------------------------------------------

\begin{figure}[t]
\begin{subfigure}[t]{\textwidth}
\includegraphics[width=\textwidth]{imgs/balls_ds_grid}
\caption{Samples from the considered dataset for the EIT example.}
\label{fig:dataset_EIT}
\end{subfigure}
\begin{subfigure}[t]{\textwidth}
\includegraphics[width=\textwidth]{imgs/balls_grid_1ng}
\caption{Samples from the VAE with one generator.}
\label{fig:balls_chart_1_gen}
\end{subfigure}
\begin{subfigure}[t]{\textwidth}
\includegraphics[width=\textwidth]{imgs/balls_grid_2ng_chart0}
\includegraphics[width=\textwidth]{imgs/balls_grid_2ng_chart1}
\caption{Samples from the mixture of two VAEs. Top: first chart, bottom: second chart.}
\label{fig:balls_chart_2_gen}
\end{subfigure}
\begin{subfigure}[t]{\textwidth}
\includegraphics[width=\textwidth]{imgs/balls_ds_grid}
\includegraphics[width=\textwidth]{imgs/balls_res1_grid}
\includegraphics[width=\textwidth]{imgs/balls_res_grid}
\caption{Reconstructions. From top to bottom: ground truth image, reconstruction with one generator, reconstruction with two generators.}
\label{fig:balls_reconstructions}
\end{subfigure}
\caption{Dataset, synthesized samples and reconstructions for the EIT example.}
\end{figure}

Finally, we consider the highly non-linear and ill-posed inverse problem of electrical impedance tomography (EIT) \cite{cheney1999electrical}, which is also known in the mathematical literature as the Calder\'{o}n problem \cite{AP2006,FSU2019,MS2012}. 
EIT is a non-invasive, radiation-free method to measure the conductivity of a tissue through electrodes placed on the surface of the body. 
More precisely, electrical currents patterns are imposed on some of these electrodes and the resulting voltage differences are measured on the remaining ones. 
Although harmless, the use of this modality in practice is very limited because the standard reconstruction methods provide images with very low spatial resolution. This is an immediate consequence of the severe ill-posedness of the inverse problem \cite{alessandrini1988,mandache2001}.

Classical methods for solving this inverse problem include variational-type methods \cite{CINSG1990}, the Lagrangian method \cite{CZ1999}, the factorization method \cite{BH2000,KG2007}, the D-bar method \cite{SMI2000}, the enclosure method \cite{IS2000}, and the monotonicity method \cite{TR2002}.
Similarly as many other inverse problems,  deep learning methods have had a big impact on EIT.
For example, the authors of \cite{FY2020} propose an end-to-end neural network that learns the forward map $\mathcal G$ and its inverse.
Moreover, deep learning approaches can be combined with classical methods, e.g, by post processing methods \cite{HH2018,HHHK2019} or by variational learning algorithms \cite{SKJLH2019}.

\paragraph{Dataset and Manifold Approximation.}
We consider the manifold consisting of $128\times 128$ images showing two bright non-overlapping balls with a gray background, representing  conductivities with special inclusions.
The radius and the position of the balls vary, while the fore- and background intensities are fixed. 
Some exemplary samples of the dataset are given in Figure~\ref{fig:dataset_EIT}.
The images from the dataset have six degrees of freedom, namely the positions and the radii of both balls in $\R^2$.
Therefore, these images form a six-dimensional manifold.
Since the balls are not allowed to overlap, the manifold has a nontrivial topology and does not admit a global parameterization.

A slightly more general version of this manifold was considered in  \cite{2023-alberti-santacesaria}, where Lipschitz stability is proven for a related inverse boundary value problem restricted to the manifold. Other types of inclusions (with unknown locations), notably polygonal and polyhedral inclusions, have been considered in the literature \cite{2021-beretta-francini-vessella,2022-beretta-francini,2022-aspri-beretta-francini-vessella}. The case of small inclusions is discussed in \cite{2004-ammari-kang}. 

We approximate the data manifold by a mixture of two VAEs and compare the results with the approximation with a single VAE. The latent dimension is set to the manifold dimension, i.e., $d=6$.
Some samples of the learned charts are given in Figure~\ref{fig:balls_chart_1_gen} and \ref{fig:balls_chart_2_gen}.
As in the previous example, both models produce mostly realistic samples.

\paragraph{The Forward Operator and its Derivative.} From a mathematical viewpoint, EIT considers the following PDE with Neumann boundary conditions
\begin{equation}
\label{u_g}
\left \{ \begin{array}{rl}
- \nabla \cdot (\gamma \hspace{0.1cm} \nabla u_g) = 0& \text{in } \Omega, \\
\gamma \hspace{0.1cm} \partial_{\nu} u_g = g & \text{on } \partial \Omega,
\end{array}
\right.
\end{equation}
where $\Omega\subseteq\R^2$ is a bounded domain, $\gamma \in L^{\infty}(\Omega)$ is such that $\gamma(x) \geq \lambda > 0$ and $u_g\in H^1(\Omega)$ is the unique weak solution with zero boundary mean of \eqref{u_g} with Neumann boundary data \smash{$g \in H^{-\frac{1}{2}}_\diamond(\partial \Omega)$}, with $H^s_\diamond(\partial \Omega) = \{ f \in H^s(\partial \Omega) : \int_{\partial \Omega} f ds = 0 \}$.
From the physical point of view, $g$ represents the electric current applied on $\partial \Omega$ (through electrodes placed on the boundary of the body), $u_g$ is the electric potential and  $\gamma$ is the conductivity of the body in the whole domain $\Omega$. The inverse problem consists in the reconstruction of $\gamma$ from the knowledge of all pairs of boundary measurements $(g,u_g|_{\partial\Omega})$, namely, of all injected currents together with the corresponding electric voltages generated at the boundary. In a compact form, the measurements may be modelled by the Neumann-to-Dirichlet map 
\[
\begin{aligned}
\mathcal{G}(\gamma) \colon H^{-\frac{1}{2}}_\diamond(\partial \Omega) &\to H^{\frac{1}{2}}_\diamond(\partial \Omega)\\
g &\mapsto u_g \big|_{\partial \Omega}.
\end{aligned}
\]
Since the PDE \eqref{u_g} is linear,  the map $\mathcal{G}(\gamma) $ is linear. However, the forward map $\gamma\mapsto\mathcal{G}(\gamma) $ is nonlinear in $\gamma$, and so is the corresponding inverse problem.
The map $\mathcal{G}$ is continuously differentiable, and its Fr\'{e}chet derivative (see \cite{H2019}) is given by $[\nabla \mathcal{G}(\gamma)](\sigma)(g) = w_g \big|_{\partial \Omega}$, where $w_g \in H^1(\Omega)$ is the unique weak solution with zero boundary mean of 
\begin{equation*}
\left \{ \begin{array}{rl}
- \nabla \cdot (\gamma \hspace{0.1cm} \nabla w_g) = \nabla \cdot (\sigma \hspace{0.1cm} \nabla u_g)& \text{in } \Omega, \\
- \gamma \hspace{0.1cm} \partial_{\nu} w_g = \sigma \hspace{0.1cm} \partial_{\nu} u_g & \text{on } \partial \Omega,
\end{array}
\right.
\end{equation*}
where $u_g \in H^1(\Omega)$ is the unique weak solution with zero boundary mean that solves \eqref{u_g}. We included the expression of this derivative in the continuous setting for completeness, but, as a matter of fact, we will need only its semi-discrete counterpart given below.


\paragraph{Discretization and Objective Function.}
In our implementations, we discretize the linear mappings $G(\gamma)$ by restricting them to a finite dimensional subspace spanned by 
a-priori fixed boundary functions $g_1,\dots,g_N\in H^{-\frac{1}{2}}_\diamond(\partial \Omega)$.
Then, following \cite[eqt.~(2.2)]{BMPS2018}, we reconstruct the conductivity  by minimizing the semi-discrete functional 
\begin{equation}
\label{J}
    F(\gamma) = \frac{1}{2} \sum_{n = 1}^N \int_{\partial \Omega} | u_{g_n}(s) - (u_{\text{true}})_{g_n}(s)|^2 ds,
\end{equation} 
where $(u_{\text{true}})_{g_n}$ is the observed data.
In our discrete setting, we represent the conductivitiy $\gamma$ by a piecewise constant function $\gamma = \sum_{m=1}^M \gamma_m \mathbbm{1}_{T_m}$ on a triangulation $(T_m)_{m=1,\dots,M}$. Then, following \cite[eqt.~(2.20)]{BMPS2018}, the derivative of \eqref{J} with respect to $\gamma$ is given by \begin{equation}\label{eq:derivative_F_EIT}
\frac{dF}{d\gamma_m}(\gamma) = \sum_{n=1}^N \int_{T_m} \nabla u_{g_n}(x) \cdot \nabla z_{g_n}(x) dx,  
\end{equation}
where $z_{g_n}$ solves 
\begin{equation}
\label{z_g_n}
  \left \{ \begin{array}{rl}
- \nabla \cdot (\gamma \hspace{0.1cm} \nabla z_{g_n}) = 0\phantom{(u_{\text{true}})_{g_n} - u_{g_n}}& \text{in } \Omega, \\
\gamma \hspace{0.1cm} \partial_{\nu} z_{g_n} = (u_{\text{true}})_{g_n} - u_{g_n}\phantom{0}& \text{on } \partial \Omega,
\end{array}
\right.  
\end{equation}
with the normalization $\int_{\partial \Omega} z_{g_n}(s) ds = \int_{\partial \Omega} (u_{\text{true}})_{g_n}(s) ds$.


\paragraph{Implementation Details.} In our experiments the domain $\Omega$ is given by the unit square $[0,1]^2$. For solving the PDEs \eqref{u_g} and \eqref{z_g_n}, we use a finite element solver from the DOLFIN library \cite{LW2010}. 
We employ meshes that are coarser in the middle of $\Omega$ and finer close to the boundary.
To simulate the approximation error of the meshes, and to avoid inverse crimes, we use a fine mesh to generate the observation and a coarser one for the reconstructions.
We use $N=15$ boundary functions, which are chosen as follows.
We divide each of the four edges of the unit square $[0,1]^2$ into 4 segments and denote by $b_1,\dots,b_{16}$ the functions that are equal to 
$1$ on one of these segments and $0$ otherwise. 
Then, we define the boundary functions as $g_n=\sum_{i=1}^{16}a_{n,i}b_i$, where the matrix $A=(a_{n,i})_{n=1,\dots,15,i=1,\dots,16}$ is the $16\times16$ Haar matrix without the first row.
More precisely, the rows of $A$ are given by the rows of the 
matrices $2^{-k/2}(\mathrm{Id}_{2^{4-k}}\otimes (1,-1) \otimes e_{2^{k-1}}^\tT)$ for $k=1,\dots,4$, where $\otimes$ is the Kronecker product and $e_j\in\R^j$ is the vector where all entries are $1$. 


\paragraph{Results.}

We reconstruct the ground truth images from the observations by minimizing the functional $F$ from \eqref{J} subject to $\gamma\in\mathcal M$.
To this end, we apply the gradient descent scheme from Algorithm~\ref{alg:gd_manifold} for 100 steps. Since the evaluation of the forward operator and 
its derivative include the numerical solution of a PDE, it is computationally very costly. Hence, we aim to use as few iterations of Algorithm~\ref{alg:gd_manifold} as possible. To this end, we apply the adaptive step size scheme from Algorithm~\ref{alg:adaptive_steps}.
As retractions we use $\tilde R_{k,x}$ from \eqref{eq:retractions}.
The initialization $\gamma_0$ of the gradient descent scheme is given by a random sample from the mixture of VAEs.

Since $F$ is defined on the whole $\R_+^{128\times128}$, we use again Remark~\ref{rem:riem_grad_ex} for the evaluation of the Riemannian gradient.
More precisely, for $\gamma\in U_k$, we have that $\nabla_{\mathcal M} F(\gamma)=J(J^\tT J)^{-1}J^\tT\nabla F(\gamma)$, where $\nabla F(\gamma)$ is the Euclidean gradient of $F$ and $J=\nabla \mathcal D_k(\mathcal E_k(\gamma))$.  
Here, we compute $\nabla F(\gamma)$  by \eqref{eq:derivative_F_EIT} and $J$ by algorithmic differentiation.

The reconstructions for 20 different ground truths are visualized in Figure~\ref{fig:balls_reconstructions}.
We observe that both models capture the ground truth structure in most cases, but also fail sometimes. 
Nevertheless, the reconstructions with the mixture of two VAEs recover the correct structure more often and more accurately than the single VAE.
To quantify the difference more in detail, we rerun the experiment with $200$ different ground truth images and 
average the resulting PSNR over $200$ reconstructions within the following table.
\begin{center}
\begin{tabular}{c|cc}
&One generator&Two generators\\\hline
PSNR&$23.64\pm3.91$&$24.76\pm3.79$
\end{tabular}
\end{center}
Consequently, the reconstructions with two generators are significantly better than those with one generator.

\section{Conclusions}\label{sec:conclusions}

In this paper we introduced mixture models of VAEs for learning manifolds of arbitrary topology.
The corresponding decoders and encoders of the VAEs provide analytic access to the resulting charts
and are learned by a loss function that approximates the negative log-likelihood function. 
For minimizing functions $F$ defined on the learned manifold we proposed a Riemannian gradient descent scheme.
In the case of inverse problems, $F$ is chosen as a data-fidelity term.
Finally, we demonstrated the advantages of using several generators on numerical examples.

This work can be extended in several directions.
First, gradient descent methods converge only locally and are not necessarily fast. Therefore,
it would be interesting to extend the minimization of the functional $F$ in Section~\ref{opt_prob} to higher-order methods 
or incorporate momentum parameters.
Moreover, a careful choice of the initialization could improve the convergence behavior.
Further, our reconstruction method could be extended to Bayesian inverse problems.
Since the mixture model of VAEs provides us with a probability distribution and an (approximate) density,
 stochastic sampling methods like the Langevin dynamics could be used for quantifying uncertainties within our reconstructions.
Indeed, Langevin dynamics on Riemannian manifolds are still an active area of research.
Finally, recent papers show that diffusion models provide an implicit representation of the data manifold \cite{BSS2022,RLCC2022}.
It would be interesting to investigate optimization models on such manifolds in order to apply them to inverse problems.

\section*{Acknowledgements}
This material is based upon work supported by the Air Force Office of Scientific Research under award number FA8655-20-1-7027. Co-funded by the European Union (ERC, SAMPDE, 101041040). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them. GSA, MS and SS are members of the ``Gruppo Nazionale per lâAnalisi Matematica, la ProbabilitÃ  e le loro Applicazioni'', of the ``Istituto Nazionale di Alta Matematica''.
JH acknowledges funding by the German Research Foundation (DFG) within the project STE 571/16-1.

\bibliographystyle{abbrv}
\bibliography{ref}

\end{document}
