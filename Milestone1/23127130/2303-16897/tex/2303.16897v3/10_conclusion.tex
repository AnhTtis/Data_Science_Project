\section{Conclusion}
We present a physics-driven diffusion model for impact sound synthesis from videos. Our model can effectively generate high fidelity sounds for physical object interactions. We achieve such function by leveraging physics priors as guidance for the diffusion model to generate impact sounds from video input. Experimental results demonstrate that our approach outperforms other methods quantitatively and qualitatively. Ablation studies have demonstrated that physics priors are critical for generating high-fidelity sounds from video inputs. The limitation of our approach naturally becomes that our approach cannot generate impact sounds for unseen physics parameters due to the query process (failure case demonstration is shown in Supplementary material), while we can generate novel sounds given an unseen video.
\label{sec:conclusion}
\paragraph{Acknowledgements.} This work was supported by the MIT-IBM Watson AI Lab, DARPA MCS, DSO grant DSOCO21072, and gift funding from MERL, Cisco, Sony, and Amazon. 