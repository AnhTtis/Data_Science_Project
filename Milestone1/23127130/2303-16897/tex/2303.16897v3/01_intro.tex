\section{Introduction}
\label{sec:intro}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/teaser_new.png}
    \caption{The physics-driven diffusion model takes physics priors and video input as conditions to synthesize high-fidelity impact sound. Please also see the supplementary video and materials with sample results.}
    \label{fig:teaser}
\end{figure}
Automatic sound effect production has become demanding for virtual reality, video games, animation, and movies. Traditional movie production heavily relies on talented Foley artists to record many sound samples in advance and manually perform laborious editing to fit the recorded sounds to visual content. Though we could obtain a satisfactory sound experience at the cinema, it is labor-intensive and challenging to scale up the sound effects generation of various complex physical interactions. 

Recently, much progress has been made in automatic sound synthesis, which can be divided into two main categories. The first category is physics-based modal synthesis methods~\cite{van2001foleyautomatic, o2001synthesizing, o2002synthesizing}, which are often used for simulating sounds triggered by various types of object interactions. Although the synthesized sounds can reflect the differences between various interactions and the geometry property of the objects, such approaches require a sophisticated designed environment to perform physics simulation and compute a set of physics parameters for sound synthesis. It is, therefore, impractical to scale up for a complicated scene because of the time-consuming parameter selection procedure. On the other hand, due to the availability of a significant amount of impact sound videos in the wild, training deep learning models for impact sound synthesis turns out to be a promising direction. Indeed, several works have shown promising results in various audio-visual applications~\cite{zhu2021deep}. Unfortunately, most existing video-driven neural sound synthesis methods~\cite{zhou2018visual, chen2020generating} apply end-to-end black box model training and lack of physics knowledge which plays a significant role in modeling impact sound because a minor change in the impact location could exert a significant difference in the sound generation process. As a result, these methods are prone to learning an average or smooth audio representation that contains artifacts, which usually leads to generating unfaithful sound.

In this work, we aim to address the problem of automatic impact sound synthesis from video input. The main challenge for the learning-based approach is the weak correspondence between visual and audio domains since the impact sounds are sensitive to the undergoing physics. Without further physics knowledge, generating high-fidelity impact sounds from videos alone is insufficient. Inspired by physics-based sound synthesis methods using a set of physics mode parameters to represent and re-synthesize impact sounds~\cite{ren2013example, traer2019perceptually, clarke2022diffimpact}, we design a physics prior that could contain sufficient physics information to serve as a conditional signal to guide the deep generative model synthesizes impact sounds from videos. However, since we could not perform physics simulation on raw video data to acquire precise physics parameters, we explored estimating and predicting physics priors from sounds in videos. We found that such physics priors significantly improve the quality of synthesized impact sounds. For deep generative models, recent successes in image generation such as DALL-E 2 and Imagen~\cite{saharia2022photorealistic} show that Denoising Diffusion Probabilistic Models (DDPM) outperform GANs in terms of fidelity and diversity, and its training process is usually with less instability and mode collapse issues. While the idea of the denoising process is naturally fitted with sound signals, it is unclear how video input and physics priors could jointly condition the DDPM and synthesize impact sounds.

To address all these challenges, we propose a novel system for impact sound synthesis from videos. The system includes two main stages. In the first stage, we encode physics knowledge of the sound using physics priors, including estimated physical parameters using signal processing techniques and learned residual parameters interpreting the sound environment via neural networks. In the second stage, we formulate and design a DDPM model conditioned on visual input and physics priors to generate a spectrogram of impact sounds. Since the physics priors are extracted from the audio samples, they become unavailable at the inference stage. To solve this problem, we propose a novel inference pipeline to use test video features to query a physics latent feature from the training set as guidance to synthesize impact sounds on unseen videos. Since the video input is unseen, we can still generate novel impact sounds from the diffusion model even if we reuse the training set's physics knowledge.
\noindent In summary, our main contributions to this work are:
\vspace{-2mm}
\begin{itemize}[align=right,itemindent=0em,labelsep=2pt,labelwidth=1em,leftmargin=*,itemsep=0em] 

\item We propose novel physics priors to provide physics knowledge to impact sound synthesis, including estimated physics parameters from raw audio and learned residual parameters approximating the sound environment.

\item We design a physics-driven diffusion model with different training and inference pipeline for impact sound synthesis from videos. To the best of our knowledge, we are the first work to synthesize impact sounds from videos using the diffusion model.

\item Our approach outperforms existing methods on both quantitative and qualitative metrics for impact sound synthesis. The transparent and interpretable properties of physics priors unlock the possibility of interesting sound editing applications such as controllable impact sound synthesis.
\end{itemize}

