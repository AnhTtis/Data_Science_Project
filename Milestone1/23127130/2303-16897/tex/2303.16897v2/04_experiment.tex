\section{Experiments}
\subsection{Dataset}
To evaluate our physics-driven diffusion models and make comparison to other approaches, we use the \textit{Greatest Hits} dataset~\cite{owens2016visually} in which people interact with physical objects by hitting and scratching materials with a drumstick, comprising 46,577 total actions in 977 videos. Human annotators labeled the actions with material labels and the time stamps of the impact sound. According to the dataset annotation assumption that the time between two consecutive object sounds is at least $0.25$ second, we segment all audios into $0.25$ second clips based on the data annotation for training and testing. We use the pre-defined train/test split for all experiments.

\subsection{Implementation Details}
We use Pytorch to implement all models in our method. For physics parameter estimation, all audio waveforms are in 44.1Khz and we compute log-scaled spectrograms with 2048 window size and 256 hop size, leading to a $1025 \times 44$ spectrogram for each impact sound. Then we estimate $1025$ modes parameters from the spectrogram as described in the Sec \ref{physics param estim}. For residual parameter prediction, the transformer encoder is a 4-layer transformer encoder with 4 attention heads. The residual weights and decay rate dimensions are both $100$. In the physics-driven diffusion model, we feed $22$ video frames centered at the impact event to the video encoder which is a ResNet-50 model with TSM~\cite{lin2019tsm} to efficiently handle the temporal information. The physics encoder consists of five parallel MLPs which take each of physics priors as input and project into lower-dimension feature vectors. The outputs are concatenated together into a $256$-dim physics latent vector $\mu$. The spectrogram denoiser is an Unet architecture, which is constructed as a spatial downsampling pass followed by a spatial upsampling pass with skip connections to the downsampling pass activation. We use Griffin-Lim algorithm to convert the spectrogram to the final audio waveform~\cite{griffin1984signal}. We use AdamW optimizer to train all models on a A6000 GPU with a batch size of 16 until convergence. The initial learning rate is set to $5e-4$, and it gradually decreases by a factor of 0.95.

\subsection{Baselines}
We compare our physics-driven diffusion model against various state-of-the-art systems. For fair comparison, we use the same video features extracted by TSM~\cite{lin2019tsm}.
\vspace{-2mm}
\begin{itemize}[align=right,itemindent=0em,labelsep=2pt,labelwidth=1em,leftmargin=*,itemsep=-0.5em]
    \item \textbf{ConvNet-based Model}: With a sequence video features, we first up-sampled them to have the same number of frames as the spectrogram. Then we perform a Unet architecture to convert video features to spectrogram. Such a architecture has shown successful results in spectrogram-based music generation~\cite{wang2019performancenet}.
    \item \textbf{Transformer-based Model}: We implement a conditional Transformer Network which has shown promising results in Text-to-Speech~\cite{li2019neural}. Instead of using text as condition, here we use the extracted video features.
    \item \textbf{Video conditioned Diffusion model}: We also compare our approach to two video conditioned spectrogram diffusion model variants. In the first setup, we do not include the physics priors and keep all other settings the same.
    \item \textbf{Video + Class Label conditioned Diffusion model}: In the second variant, we provide a class-label of the impact sound material as an additional condition to the video features. All other settings are the same as ours.
    \item \textbf{Video + Other Audio Features Diffusion model}: To show the importance of physics latents, we replace the physics latent with spectrogram/mfcc latent by extracting spectrogram/mfcc features from the raw audio and pass them to a transformer encoder similar to the one used in physics parameters reconstruction, and then we apply average pooling to obtain the latent vector. During testing, we still use visual features to query the corresponding spectrogram/mfcc latent in the training set and synthesize the final results.
\end{itemize}

\subsection{Evaluation Metrics}
We use four different metrics to automatically assess both the fidelity and relevance of the generated samples. For automatic evaluation purpose, we train an impact sound object material classifier using the labels in Great Hits Dataset. The classifier is a ResNet-50 convolutional-based neural network and we use the spectrogram as input to train the classifier.
\vspace{-2mm}
\begin{itemize}[align=right,itemindent=0em,labelsep=2pt,labelwidth=1em,leftmargin=*,itemsep=-0.5em] 
    \item \textbf{Fr√©chet Inception Distance (FID)} is used for evaluating the quality of generated impact sound spectrograms. The FID score evaluates the distance between the distribution of synthesized spectrograms and the spectrograms in the test set. To build the distribution, we extract the features before the impact sound classification layer.
    \item \textbf{Kernel Inception Distance (KID)} is calculated via maximum mean discrepancy (MMD). Again, we extract features from synthesized and real impact sounds. The MMD is calculated over a number of subsets to both get the mean and standard deviation of KID.
    \item \textbf{KL Divergence} is used to individually compute the distance between output distributions of synthesized and ground truth features since FID and KID mainly rely on the distribution of a collection of samples.
    \item \textbf{Recognition accuracy} is used to evaluate if the quality of generated impact sound samples can fool the classifier.
\end{itemize}

\subsection{Results}
\begin{table*}[]
\centering
\small
{%
\begin{tabular}{|l|cccc|}
\hline
Model\textbackslash{}Metric & FID \textcolor{red}{$\downarrow$} & KID (mean, std)\textcolor{red}{$\downarrow$} & KL Div. \textcolor{red}{$\downarrow$} & Recog. Acc (\%) \textcolor{red}{$\uparrow$}    \\
\hline
ConvNet-based  & 43.50 & 0.053, 0.013 &  4.65 & 51.69    \\ 
Transformer-based  & 34.35 & 0.036, 0.015 & 3.13 & 62.86   \\
Video Diffusion & 54.57 & 0.054, 0.014 & 2.77 & 69.94 \\
Video + Class label Diffusion  & 31.82 & 0.026, 0.021 & 2.38 & 72.02  \\
Video + MFCC Diffusion & 40.21 & 0.037, 0.010 & 2.84 & 67.87 \\
Video + Spec Diffusion & 28.77 & 0.016, 0.009 & 2.55 & 70.46 \\
\cellcolor{mygray-bg}\bf{Video + Physics Diffusion~(Ours)}      & \cellcolor{mygray-bg}\bf 26.20  &   \cellcolor{mygray-bg}\bf 0.010, 0.008 & \cellcolor{mygray-bg}\bf 2.04 & \cellcolor{mygray-bg}\bf 74.09 \\ \hline
\end{tabular}%
}
\caption{Quantitative evaluations for different models. For FID, KID, and KL Divergence, lower is better. For recognition accuracy, higher is better. Bold font indicates the best value.}
\label{tab:quantitative results}
\end{table*}
Quantitative evaluation results are shown in Table~\ref{tab:quantitative results}. Our proposed physics-driven diffusion model outperforms all other methods across all metrics. It is worth noting that without physics priors, using video features alone as condition to the spectrogram denoiser is not sufficient to generate high fidelity sounds. While this could be improved when class labels are available, it is obvious that there is a large gap to reach the performance of the physics-driven method. Fig.~\ref{fig:qualitative_comparison} illustrates a comparison of three examples of generated spectrograms given a video by ConvNet-based, Transformer-based, and our physics-driven approaches to the ground truth. While the ConvNet and Transformer-based approaches could also capture some correspondences between audio and video, it is obvious that a lot of noise appears in the generated spectrogram because these approaches are prone to learn an average smoothed representation, and thus introducing many artifacts in the end. In comparison, our physics-driven diffusion approach does not suffer from this problem and can synthesize high fidelity sound from videos. It is worth noting that the interoperability of our approach could potentially unlock applications such as controllable sound synthesis by manipulating the physics priors.
\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/spec_comparison.png}
    \vspace{-3mm}
    \caption{Qualitative Comparison results on sound spectrogram generated by different methods.}
    \vspace{-3mm}
    \label{fig:qualitative_comparison}
\end{figure*}

\subsection{Human Perceptual Evaluation}
In addition to the objective evaluation of our method, we also perform human perceptual surveys using Amazon Mechanical Turk (AMT). We aim to use perceptual surveys to evaluate the effectiveness of our generated samples to match the video content and the fidelity of the generated samples. For all surveys, we do not request participants with any background on the survey or our approach was given to the participants to avoid perceptual biases. We surveyed $50$ participants individually, where each participant was asked to evaluate $10$ videos along with different generated samples from various methods. A total of $500$ opinions were collected in the end.
\vspace{-2mm}
\begin{itemize}[align=right,itemindent=0em,labelsep=2pt,labelwidth=1em,leftmargin=*,itemsep=-0.5em] 
    \item \textbf{Matching}. In the first survey, we asked people to watch the same video with different synthesized sounds and answer the question: "In which video the sound best matches the video content?". The participants would choose one soundtrack from the ConvNet-based, Transformer-based and Physics-driven diffusion approaches. From results shown in Table.~\ref{tab:human_eval_compare} (Top) (left column), we observe that there exists a clear indication that the sound generated with our method is selected as the best match to the visual content with a higher selected rate. 
    \item \textbf{Quality}. In the second survey, we asked people (non-expert) to choose the video with the highest sound quality, including 3 variations of samples generated by ConvNet-based, Transformer-based and Physics-driven diffusion approaches. Results in Table clearly indicate our approach achieves the best sound quality.
    \item \textbf{Perceptual Ablation Studies}. In the last survey, we performed a perceptual ablation study to test how the physics priors could influence the perception of the generated impact sounds compared to the approaches without it. Survey results shown in Table~\ref{tab:human_eval_compare} (Bottom) and suggest that in comparison to video only model, the physics priors improve the overall perception of synthesized impact sounds.
\end{itemize}

\begin{table}[]
\vspace{-3mm}
\centering
% \resizebox{\columnwidth}{!}
% \small
\footnotesize
%
\begin{tabular}{|l|c|c|}
\hline
Model\textbackslash{}Metric & Matching &  Quality \\
\hline
\multicolumn{3}{|l|}{\textit{Comparison to Baselines}}\\ \hline
ConvNet-based  & 18\% & 17.6\% \\
Transformer-based  & 26.6\% & 28.8\% \\ 
\cellcolor{mygray-bg}\bf{Ours}  & \cellcolor{mygray-bg}\bf 55.4\%  & \cellcolor{mygray-bg}\bf 53.6\% \\  \hline
\multicolumn{3}{|l|}{\textit{Perceptual Ablation Studies}}\\ \hline
Video-only  & 23.6\% & 23.6\% \\
 Video+label  & 37.8\% & 35.8\% \\ 
\cellcolor{mygray-bg}\bf{Ours}  & \cellcolor{mygray-bg}\bf 38.6\%  & \cellcolor{mygray-bg}\bf 40.6\% \\  \hline
\end{tabular}%
\vspace{-1mm}
\caption{(Top) Human perceptual evaluation on matching and quality metrics. (Bottom) Ablation study on human perceptual evaluation. The value indicates the percentage of Amazon Turkers who select the method.}
\vspace{-5mm}
\label{tab:human_eval_compare}

\end{table}

% \begin{table}[]
% \centering
% % \small
% {%
% \begin{tabular}{|l|cc|}
% \hline
% Model\textbackslash{}Metric & Matching &  Quality \\
% \hline
% Video-only  & 23.6\% & 23.6\% \\
% Video+label  & 37.8\% & 35.8\% \\ 
% \cellcolor{mygray-bg}\bf{Ours}  & \cellcolor{mygray-bg}\bf 38.6\%  & \cellcolor{mygray-bg}\bf 40.6\% \\  \hline
% \end{tabular}%
% }
% \caption{Ablation study on human perceptual evaluation. The value indicates the percentage of Amazon Turkers who select the method.}
% \label{tab:human_eval_ablation}
% \end{table}

\subsection{Ablation Studies}
We performed three ablation studies to answer the following questions. \textbf{Q1}: How do residual parameters influence the physics priors? \textbf{Q2}: What is the contribution of each component to our approach? \textbf{Q3}: Is our method better than simple retrieval methods?

\noindent\textbf{Q1}. Since the physics and residual parameters are essential in our approach. We have investigated different physics priors variants to find the most informative physics priors and use the multi-resolution STFT loss of the reconstructed sounds for evaluation. The results in Fig.~\ref{fig:ablation}(a) clearly show that the loss decreases significantly with residual parameters. We also find that using $100$ residual parameters achieves the best performance, while fewer or more residual parameters may damage the performance.

\noindent\textbf{Q2}. We perform extensive experiments to understand the contribution of each component. For all studies, we use the nearest physics parameters/priors retrieved by visual latent to synthesize the sound. Results are shown in Fig.~\ref{fig:ablation}(b). We first observe that without residual components and diffusion models, using estimated physics parameters to perform modal synthesis could not obtain faithful impact sounds. The physics priors could re-synthesize impact sounds with much better quality with learned residual parameters. We further show that using physics priors as the condition input to the diffusion model achieves even better performance. We have also performed an experiment to predict physics latent from video input and use it as the condition for the diffusion model but the quality of generated samples is poor. This is due to the weak correspondence between video inputs and physics behind the impact sounds and indicates the importance of using video inputs to query physics priors of the training set at the inference stage.

\noindent\textbf{Q3}. We consider two retrieval baselines for comparison. The first one is a simple baseline without physics priors and diffusion model. We only use visual features extracted from the ResNet-50 backbone to search the nearest neighbor (NN) in the training set and use the audio as output. In the second experiment, we try our best to reproduce the model in~\cite{owens2016visually} since no official implementation is available. The model predicts sound features (cochleagrams) from images via LSTM. For fair evaluation, a collection-based metric like FID is invalid because the retrieved audios are in the real data distribution. Therefore, we use sample-based metrics, including KL Divergence between predicted and ground truth audio features and Mean Square Error on the spectrogram level. The table~\ref{tab:retrieval_compare} clearly shows that our approach outperforms the retrieval baselines by a large margin.
\begin{table}[h]
\vspace{-3mm}
\centering
% \resizebox{\columnwidth}{!}
\footnotesize
{%
\begin{tabular}{|l|cc|}
\hline
Model\textbackslash{}Metric & KL Div. \textcolor{red}{$\downarrow$} &  Spec. MSE\textcolor{red}{$\downarrow$} \\
\hline
NN via Visual Features & 10.60 & 0.307 \\
NN via Predicted Sound Features~\cite{owens2016visually} & 7.39 & 0.205 \\
\cellcolor{mygray-bg}\bf{Ours}  & \cellcolor{mygray-bg}\bf 2.04  & \cellcolor{mygray-bg}\bf 0.149 \\  \hline
\end{tabular}%
}
\vspace{-3mm}
\caption{Comparison with retrieval methods.}
\label{tab:retrieval_compare}
\vspace{-4mm}%Put here to reduce too much white space after your table 
\end{table}
% \begin{figure}[!t]
%     \begin{subfigure}{.48\linewidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/ablate_physics.png}
%         \caption{Ablation study on the number of residual parameters.}
%         \label{fig:physics_ablation}
%     \end{subfigure}
%     \begin{subfigure}{.48\linewidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/ablate_component.png}
%         \caption{Ablation study on components of our approach.}
%         \label{fig:component_ablation}
%     \end{subfigure}
% \end{figure}

\begin{figure}[!t]
    \vspace{-3mm}
    \centering
    \includegraphics[width=\linewidth]{figs/ablate_component_new.png}
    \vspace{-5mm}
    \caption{(a) Ablation study on the importance and selection for the number of residual parameters by testing multi-resolution STFT loss. (b) Ablation study on the contribution of each component of our approach using FID score, the lower the better.}
    \vspace{-5mm}
    \label{fig:ablation}
\end{figure}