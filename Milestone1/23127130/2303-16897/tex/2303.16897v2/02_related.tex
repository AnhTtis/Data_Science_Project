% //dsadsadsa
\section{Related Work}
\subsection{Sound Synthesis from Videos}
Sound synthesis has been an ongoing research theme with a long history in audio research. Traditional approaches mainly use linear modal synthesis to generate rigid body sounds~\cite{van2001foleyautomatic}. While such methods could produce sounds reflecting the properties of impact sound objects such as the geometry difference, it is often the case that the simulation and engineering tuning on the initial parameters for the virtual sounding materials in the modal analysis is time-consuming and non-intuitive. Suppose we are under a complicated scene consisting of many different sounding materials; the traditional approach can quickly become prohibitively expensive and tedious~\cite{ren2013example}. In recent years, deep learning approaches have been developed for sound synthesis. Owens et al.~\cite{owens2016visually} investigated predicting the sound emitted by interacting in the wild objects using a wood drumstick. However, instead of directly usuing LSTM to generate sound, they first predict sound features and then performed an exemplar-based retrieval algorithm. Instead of performing retrieval, our work directly generates the impact sounds. In addition to impact sound, a conditional generative adversarial network is proposed for cross-modal generation on music performances collected in a lab environment by Chen et al.~\cite{chen2017deep}. Moreover, natural sounds are explored by Zhou et al.~\cite{zhou2018visual} who introduced a SampleRNN-based method to directly predict audio waveform from Youtube videos data but the number of sound categories is limited to ten. Next, several works attempt to generate aligned audio to input videos via a perceptual loss~\cite{chen2018visually} and information bottleneck~\cite{chen2020generating}. More recently, music generation from visual input has also achieved various attentions~\cite{su2020audeo, gan2020foley, su2021does}.

\subsection{Audio-visual learning}
In recent years, methods for multi-modality learning have shown significance in learning joint representation for downstream tasks~\cite{radford2021learning}, and unlocked novel cross-modal applications such as visual captioning~\cite{yu2022coca, liu2021aligning}, visual question answering (VQA)~\cite{yi2018neural, chen2021grounding}, vision language navigation~\cite{anderson2018vision}, spoken question answering (SQA)~\cite{you2021mrd, chen2021self}, healthcare AI~\cite{li2020behrt, liu2021auto, you2022mine, youclass, you2023rethinking}, etc. In this work, we are in the field of audio-visual learning, which deals with exploring and leveraging both audio and video correlation at the same time. For example, earlier work from Owens et al.~\cite{owens2016ambient} tried using clustered sound to learn visual representations from unlabeled video data, and similarly, Aytar et al.~\cite{aytar2016soundnet} leveraged the scene to learn the audio representations. Later, ~\cite{arandjelovic2017look} investigated audio-visual joint learning the visual by training a visual-audio correspondence task. More recently, several works have also explored sound source localization in images or videos in addition to the audio-visual representations~\cite{izadinia2012multimodal, hershey1999audio, senocak2018learning}. Such works include a lot of applications such as biometric matching~\cite{nagrani2018seeing}, visually-guided sound source separation~\cite{zhao2018sound, gan2020music, gao2018learning, xu2019recursive}, understanding physical scene via multi-modal~\cite{gan2020threedworld}, auditory vehicle tracking~\cite{gan2019self}, multi-modal action recognition~\cite{long2018attention, long2018multimodal, gao2020listen}, audio-visual event localization~\cite{tian2018audio}, audio-visual co-segmentation~\cite{rouditchenko2019self}, audio inpainting~\cite{zhou2019vision}, and audio-visual embodied navigation~\cite{gan2020look}.

\subsection{Diffusion Model}
The recently explored diffusion probabilistic models (DPMs)~\cite{sohl2015deep} have served as a powerful generative backbone that achieves promising results in various generative applications~\cite{kong2020diffwave, mittal2021symbolic, lee2021nu, ho2020denoising, nichol2021improved, dhariwal2021diffusion, ho2022cascaded}, outperforming GANs in terms of fidelity and diversity. More intriguing, the training process is usually with less instability and mode collapse issues. Compared to the unconditional case, conditional generation is usually applied in more concrete and practical cross-modality scenarios. Most existing DPM-based conditional synthesis works~\cite{gu2022vector, dhariwal2021diffusion} learn the connection between the conditioning and the generated data implicitly by adding a prior to the variational lower bound. Most of these methods focus on the image domain, while audio data differs in its long-term temporal dependencies. Several works have explored to use of diffusion models for text-to-speech (TTS) synthesis~\cite{jeong2021diff, huang2022prodiff}. Unlike the task of text-to-speech synthesis, which contains a strong correlation between phonemes and speech, the correspondences between impact sounds and videos are weak. Therefore it is non-trivial to directly apply a conditional diffusion model to impact sound synthesis from videos. In this work, we found that only video condition is insufficient to synthesize high-fidelity impact sounds and additionally apply physics priors significantly improve the results. Moreover, due to the difficulty of predicting physics priors from video, we propose different training and testing strategies that could benefit the information of physics priors but also synthesize new impact sounds from the video input.
\label{sec:related}

