\section{Method}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.85\textwidth]{figs/sound_physics_new.png}
    \caption{Reconstruction of physics priors by two components: $1$) We estimate a set of physics parameters (frequency, power, and decay rate) via signal processing techniques. $2$) We predict residual parameters representing the environment by a transformer encoder. A reconstruction loss is applied to optimize all trainable modules.}
    \label{fig:sound_physics}
\end{figure*}
Our method includes two main components: (a) physics priors reconstruction from sound (shown in Fig.~\ref{fig:sound_physics}), and (b) a physics-driven diffusion model for impact sound synthesis (shown in Fig.~\ref{fig:diffusion}). We first show how we can acquire physics priors from sounds In (a). Then in (b), we use reconstructed physics priors as additional information to the video input and guide the diffusion model to learn impact sound synthesis. Since no sound is available during the test time, we use different training and inference strategies to keep the benefit of physics priors and generate novel impact sounds.

\subsection{Reconstruct Physics Priors From Sound}

We aim to reconstruct physics from sound. There are two modules: $1$) the physics parameters estimation extracting modes parameters from audio waveform, and $2$) the residual parameters prediction learning to encode the environment information such as background noise and reverberation using neural networks.

\noindent\textbf{Physics Parameters Estimation}.
The standard linear modal synthesis technique is frequently used for modeling physics-based sound synthesis. The displacement $x$ in such a system can be computed with a linear equation described as follows:
\begin{align}
    M\Ddot{x} + C\Dot{x} + Kx = F, 
\end{align}
where $F$ represents the force, $M$ represents the mass, $C$ represents the damping, and $K$ represents the stiffness. With such a linear system, we can solve the generalized eigenvalue problem $KU = \Lambda MU$ and decouple it into the following form:
\begin{align}
    \Ddot{q} + (\alpha I + \beta \Lambda)\Dot{q} + \Lambda q = U^T F
\end{align}
where $\Lambda$ represents the diagonal matrix that contains eigenvalues of the system, $U$ represents the eigenvectors which can transform $x$ into the bases of decoupled deformation $q$ by matrix multiplication $x = Uq$.
\label{physics param estim}

After solving the decoupled system, we will obtain a set of modes that can be simply expressed as damped sinusoidal waves. The $i$-th mode can be expressed by:
\begin{align}
    q_i = p_i e^{-\lambda_i t} \sin (2\pi f_i t + \theta_i)
\end{align}
where $f_i$ is the frequency of the mode, $\lambda_i$ is the decaying rate, $p_i$ is the excited power, and $\theta_i$ is the initial phase. It is also common to represent $q_i$ under the decibel scale and we have
\begin{align}
    q_i = 10^{({p_i - \lambda_i t})/20} \sin (2\pi f_i t + \theta_i).
    \label{syn}
\end{align}
The frequency, power, and decay rate together define the physics parameter feature $\phi$ of mode $i$: $\phi = (f_i, p_i, \lambda_i)$ and we ignore $\theta_i$ since we assume the object is initially at rest and struck at $t=0$ and therefore it is usually treated as zero in the estimation process~\cite{ren2013example}.

Given a recorded audio waveform $s \in \mathbb{R}^{T}$, from which we first estimate physics parameters including a set of damped sinusoids with constant frequencies, powers, and decay rates. We first compute the log-spectrogram magnitude $S \in \mathbb{R}^{D \times N}$ of the audio by short-time-Fourier-transform (STFT), where $D$ is the number of frequency bins and $N$ is the number of frames. To capture sufficient physics parameters, we set the number of modes to be equal to the number of frequency bins. Within the range of each frequency bin, we identify the peak frequency $f$ from the fast Fourier transform (FFT) magnitude result of the whole audio segment. 
% To we extract the frequency parameters by computing the power spectral density
% \begin{align}
%     X_{\text{psd}} = 10 \log_{10} \frac{1}{N} \sum^N_{n=1} || {X_n} ||^2_2.
% \end{align}
% We perform peak estimation to extract frequency parameters from the power spectral density. 
Next, we extract the magnitude at the first frame in the spectrogram to be the initial power $p$. Finally, we compute the decay time $\lambda$ for the mode according to the temporal bin when it reaches the silence ($-80$dB). At this point, we obtain $D$ modes physics parameters $\{(f_i, p_i, \lambda_i)\}^D_{i=1}$ and we can re-synthesize an audio waveform $\hat{s}$ using equation~\ref{syn}.

\noindent\textbf{Residual Parameters Prediction}. 
While the estimated modes capture most of the components of the impact sound generated by physical object interactions, the recorded audio in the wild has complicated residual components such as background noise and reverberation dependent on the sound environment which is critical for a real and immersive perceptual experience. Here we propose a learning-based approach to model such residual parameters. We approximate the sound environment component with exponentially decaying filtered noise. We first randomly generate a Gaussian white noise $\mathcal{N}(0, 1)$ signal and perform a band-pass filter (BPF) to split it into $M$ bands. Then, for each band $m$, the residual component is formulated as
\begin{align}
    R_m = 10^{(-\gamma t)/20}\text{BPF}(\mathcal{N}(0, 1))_m
\end{align}
The accumulated residual components $R$ is a weighted sum of subband residual components
\begin{align}
    R = \sum^M_{m = 1} w_m R_m,
\end{align}
where $w_m$ is the weight coefficient of band $m$ residual component. Given the log-spectrogram $S\in \mathbb{R}^{D\times N}$ as input, we use a transformer-based encoder to encode each frame of the $S$. The output features are then averaged and two linear projections are used to estimate $\gamma \in \mathbb{R}^M$ and $w \in \mathbb{R}^M$. We minimize the error between $\hat{s}+R$ and $s$ by a multi-resolution STFT loss $L_{\text{mr-stft}}(\hat{s}+R, s)$ which has been shown effective in modeling audio signals in the time domain~\cite{yamamoto2020parallel}.
By estimating physics parameters and predicting residual parameters, we obtain the physics priors and it is ready to be a useful condition to guide the impact sound synthesis model to generate high-fidelity sounds from videos.

\subsection{Physics-Driven Diffusion Models}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{figs/diffusion_new.png}
    \caption{Overview of the physics-driven diffusion model for impact sound synthesis from videos. (left) During training, we reconstruct physics priors from audio samples and encode them into a physics latent. Besides, we use a visual encoder to extract visual latent from the video input. We apply these two latents as conditional inputs to the U-Net spectrogram denoiser. (right) During testing, we extract the visual latent from the test video and use it to query a physics latent from the key-value pairs of visual and physics latents in the training set. Finally, the physics and visual latents are used as conditional inputs to the denoiser and the denoiser iteratively generates the spectrogram.} 
    \label{fig:diffusion}
\end{figure*}
With the physics priors and video inputs, we propose a conditional Denoising Diffusion Probabilistic Model (DDPM) for impact sound synthesis. Our model performs a reverse diffusion process to guide the noise distribution to a spectrogram distribution corresponding to the input physics priors and video content. We encode all physics and residual parameters as a latent feature embedding with multi-layer perceptron (MLPs). The resulting physics latent vector is denoted by $\mu$. For video inputs, given a sequence of RGB frames, we use temporal-shift-module (TSM)~\cite{lin2019tsm} to efficiently extract visual features, which are then average pooled to compute a single visual latent representation $\nu$.

We show our physics-driven diffusion model for sound synthesis in Fig.~\ref{fig:diffusion}. The main component is a diffusion forward process that adds Gaussian noise $\mathcal{N}(0, I)$ at time steps $t=0, ..., T$ to a spectrogram $x$ with variance scale $\beta$. We can use a scheduler to change the variance scale at each time step to have $\beta_1, \beta_2, ..., \beta_T$~\cite{jeong2021diff}. We denote the spectrogram at diffusion time step $t$ as $x_t$. Given the spectrogram at time step $t-1$ as $x_{t-1}$, physics latent $\mu$, and visual latent $\nu$, the explicit diffusion process for spectrogram at time step $t$ can be written as $q(x_t | x_{t-1}, \mu, \nu)$. Since the complete diffusion process that takes $x_0$ to $x_T$ conditioned on $\mu$ and $\nu$ is a Markov process, we can factorize it into a sequence of multiplication $\prod_{t=1}^T q(x_t | x_{t-1})$.
To generate a spectrogram, we need the reverse process that aims to recover a spectrogram from Gaussian noise. The reverse process can be defined as the conditional distribution $p_{\theta} (x_{0:T-1} | x_T, \mu, \nu)$, and according to Markov chain property, it can be factorized into multiple transitions as follows:
\begin{align}
    p_{\theta} (x_0, ..., x_{T-1} | x_T, \mu, \nu) = \prod_{t=1}^T p_{\theta}(x_{t-1} | x_t , \mu, \nu).
\end{align}
Given the diffusion time-step with physics latent and visual latent conditions, a spectrogram is recovered from the latent variables by applying the reverse transitions $p_{\theta} (x_{t-1} | x_t, \mu, \nu)$. Considering the spectrogram distribution as $q(x_0 | \mu, \nu)$, we aim to maximize the log-likelihood of the spectrogram by learning a model distribution $p_{\theta} (x_0 | \mu, \nu)$ obtained from the reverse process to approximate $q(x_0 | \mu, \nu)$. Since it is common that $p_{\theta} (x_0 | \mu, \nu)$ is computationally intractable, we follow the parameterization trick in~\cite{ho2020denoising, jeong2021diff} to calculate the variational lower bound of the log-likelihood. Specifically, the training objective of the diffusion model is L1 loss function between the noise $\epsilon \sim \mathcal{N}(0, I)$ and the diffusion model output $f_{\theta}$ described as follows:
\begin{align}
    \min_{\theta}|| \epsilon - f_{\theta}(h(x_0, \epsilon), t, \mu, \nu) ||_1,
\end{align}
where $h(x_0, \epsilon) = \sqrt{\hat{\beta_t}} x_0 + \sqrt{1 - \hat{\beta_t}}\epsilon$, and $\hat{\beta_t} = \prod^t_{\overline{t}=1} 1 - \beta_{\overline{t}}$.

\subsection{Training and Inference}
During training, we use physics priors extracted from the audio waveform as an additional condition to guide the model to learning correspondence between video inputs and impact sounds. However, since the ground truth sound clip is unavailable during inference, we could not obtain the corresponding physics priors for the video input as we did in the training stage. Therefore, we propose a new inference pipeline to allow us to preserve the benefit of physics priors. To achieve this goal, we construct key-value pairs for visual and physics latents in our training sets. At the inference stage, we feed the testing video input and acquire the visual latent vector $\nu^{\text{test}}$. We then take  $\nu^{\text{test}}$ as a query feature and find the key in training data by computing the Euclidean distance between the test video latent $\nu^{\text{test}}$ and all training video latents $\{\nu^{\text{train}}_j\}^J_{j=1}$. Given the key $\nu^{\text{train}}_j$, we then use the value $\mu^{\text{train}}_j$ as our test physics latent $\hat{\mu}^{\text{test}}$. Once we have both visual latent $\nu^{\text{test}}$ and physics latent $\hat{\mu}^{\text{test}}$, the model reverses the noisy spectrogram by first predicting the added noise at each forward iteration to get model output $f_{\theta} (x_t, t, \hat{\mu}^{\text{test}}, \nu^{\text{test}})$ and then removes the noise by the following:
\begin{align}
    x_{t-1} = \frac{1}{\sqrt{1 - \beta_t}}(x_t - \frac{\beta_t}{\sqrt{1 - \hat{\beta}_t}}f_{\theta}(x_t, t, \hat{\mu}^{\text{test}}, \nu^{\text{test}})) + \eta_t \epsilon_t,
\end{align}
where $\hat{\beta}_t = \prod_{\overline{t}=1}^t 1- \beta_{\overline{t}}$, $\epsilon_t \sim \mathcal{N}(0, I)$, $\eta = \sigma \sqrt{\frac{1-\hat{\beta}_{t-1}}{1-\hat{\beta}_t}\beta_t}$, and $\sigma$ is a temperature scaling factor of the variance~\cite{jeong2021diff}. After iterative sampling over all of the time steps, we can obtain the final spectrogram distribution $p_{\theta}(x_0 | \hat{\mu}^{\text{test}}, \nu^{\text{test}})$. It is worth noting that while we use the physics latent from the training set, we can still generate novel sound since the diffusion model also takes additional visual features as input.
\label{sec:method}

