% //dsadsadsa
\section{Related Work}
\subsection{Sound Synthesis from Videos}
Sound synthesis has been an ongoing research theme with a long history in audio research. Traditional approaches mainly use linear modal synthesis to generate rigid body sounds~\cite{van2001foleyautomatic}. While such methods could produce sounds reflecting the rich variations of interactions and the geometry of the sounding objects, setting up good initial parameters for the virtual sounding materials in the modal analysis is time-consuming and non-intuitive. When faced with
a complicated scene consisting of many different sounding materials, the parameter selection procedure can quickly become prohibitively expensive and tedious~\cite{ren2013example}. In recent years, deep learning approaches have been developed for sound synthesis. Owens et al.~\cite{owens2016visually} investigated predicting the sound emitted by interacting objects with a drumstick. They first used a neural network to predict sound features and then performed an exemplar-based retrieval algorithm instead of directly generating the sound. Instead of performing retrieval, our work directly generates the impact sounds. Chen et al.~\cite{chen2017deep} proposed using conditional generative adversarial networks for cross-modal generation on lab-collected music performance videos. Zhou et al.~\cite{zhou2018visual} introduced a SampleRNN-based method to directly predict a generated waveform from an unconstrained video dataset that contains ten types of sound recorded in the wild. Chen et al.~\cite{chen2018visually} proposed a perceptual loss to improve
the audio-visual semantic alignment. Chen et al.~\cite{chen2020generating} introduced an information
bottleneck to generate visually aligned sound. Several works~\cite{su2020audeo, gan2020foley, su2021does} also attempt to generate music sound from videos.

\subsection{Audio-visual learning}
In recent years, methods for multi-modality learning have shown significance in learning joint representation for downstream tasks~\cite{radford2021learning}, and unlocked novel cross-modal applications such as visual captioning~\cite{yu2022coca, liu2021aligning}, visual question answering (VQA)~\cite{yi2018neural, chen2021grounding}, vision language navigation~\cite{anderson2018vision}, spoken question answering (SQA)~\cite{you2021mrd, chen2021self}, healthcare AI~\cite{li2020behrt, liu2021auto, you2022mine, youclass, you2023rethinking}, etc. In this work, we are in the field of audio-visual learning which deals with exploring and leveraging the correlation of both audio and video simultaneously. For example, given unlabeled training videos, Owens et al.~\cite{owens2016ambient} used sound clusters as supervision to learn visual feature representation, and Aytar et al.~\cite{aytar2016soundnet} utilized the scene to learn the audio representations. Follow-up work~\cite{arandjelovic2017look} further investigated jointly learning the visual and audio representation using a visual-audio correspondence task. Instead of learning feature representations, recent works have also explored localizing sound sources in images or videos~\cite{izadinia2012multimodal, hershey1999audio, senocak2018learning}, biometric matching~\cite{nagrani2018seeing}, visual-guided sound source separation~\cite{zhao2018sound, gan2020music, gao2018learning, xu2019recursive}, auditory vehicle tracking~\cite{gan2019self}, multi-modal action recognition~\cite{long2018attention, long2018multimodal, gao2020listen}, audio inpainting~\cite{zhou2019vision}, emotion recognition~\cite{albanie2018emotion}, audio-visual event localization~\cite{tian2018audio}, multi-modal physical scene understanding~\cite{gan2020threedworld}, audio-visual co-segmentation~\cite{rouditchenko2019self}, and audio-visual embodied navigation~\cite{gan2020look}.

\subsection{Diffusion Model}
The recently explored diffusion probabilistic models (DPMs)~\cite{sohl2015deep} has served as a powerful generative backbone that achieves promising results in both unconditional and conditional generation~\cite{kong2020diffwave, mittal2021symbolic, lee2021nu, ho2020denoising, nichol2021improved, dhariwal2021diffusion, ho2022cascaded}, outperforming GANs in fidelity and diversity, without training instability and mode collapse issues. Compared to the unconditional case, conditional generation is usually applied in more concrete and practical cross-modality scenarios. Most existing DPM-based conditional synthesis works~\cite{gu2022vector, dhariwal2021diffusion} learn the connection between the conditioning and the generated data implicitly by adding a prior to the variational lower bound. Most of these methods focus on the image domain, while audio data differs in its long-term temporal dependencies. Several works have explored to use of diffusion models for text-to-speech (TTS) synthesis~\cite{jeong2021diff, huang2022prodiff}. Unlike the task of text-to-speech synthesis, which contains a strong correlation between phonemes and speech, the correspondences between impact sounds and videos are weak. Therefore it is non-trivial to directly apply a conditional diffusion model to impact sound synthesis from videos. In this work, we found that only video condition is insufficient to synthesize high-fidelity impact sounds and additionally apply physics priors significantly improve the results. Moreover, due to the difficulty of predicting physics priors from video, we propose different training and testing strategies that could benefit the information of physics priors but also synthesize new impact sounds from the video input.
\label{sec:related}

