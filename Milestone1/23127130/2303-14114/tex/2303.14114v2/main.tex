% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{soul}
\usepackage{enumitem}

\usepackage{hyperref}

\usepackage{algorithm}
\usepackage{algpseudocode}



% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).

\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}
\usepackage[textsize=tiny]{todonotes}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
% \def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
% \def\confName{CVPR}
% \def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Object Motion Sensitivity: A Bio-inspired Solution to the Ego-motion Problem for Event-based Cameras}

\author{Shay Snyder\footnotemark[1]\\
\small George Mason University\\
\small Fairfax, VA 22030\\
% {\tt\small ssnyde9@gmu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Hunter Thompson\footnotemark[1]\\
\small Georgia Institute of Technology\\
\small Atlanta, GA 30332\\
% {\tt\small hthompson42@gatech.edu}
\and
Md Abdullah-Al Kaiser\\
\small University of Southern California\\
\small Los Angeles, CA 90007\\
% {\tt\small mdabdull@usc.edu}
\and
Gregory Schwartz\\
\small Northwestern University\\
\small Evanston, IL 60208\\
%{\tt\small greg.schwartz@northwestern.edu}
\and
Akhilesh Jaiswal\\
\small University of Southern California\\
\small Los Angeles, CA 90007\\
% {\tt\small akhilesh@usc.edu}
\and
Maryam Parsa\footnotemark[2]\\
\small George Mason University\\
\small Fairfax, VA 22030\\
% {\tt\small mparsa@gmu.edu}
}

% I'M STILL STRUGGLING WITH 1 AND * :D I RATHER HAVE THREE NAMES PER ROW THAN TWO PER ROW :D 
% \tt\tiny $^1$These authors contributed equally to this work \\
% \tt\tiny  $^*$Corresponding author

\maketitle

% \noindent
% $^1$ These authors contributed equally to this work \\
% $^*$ Corresponding author

%%%%%%%%% ABSTRACT
\begin{abstract}
\noindent

%%% Version 2
\noindent
Neuromorphic (event-based) image sensors draw inspiration from the human-retina to create an electronic device that can process visual stimuli in a way that closely resembles its biological counterpart. These sensors process information significantly different than the traditional RGB sensors. Specifically, the sensory information generated by event-based image sensors are orders of magnitude sparser compared to that of RGB sensors. The first generation of neuromorphic image sensors, Dynamic Vision Sensor (DVS), are inspired by the computations confined to the photoreceptors and the first retinal synapse.  In this work, we highlight the capability of the second generation of neuromorphic image sensors, Integrated Retinal Functionality in CMOS Image Sensors (IRIS), which aims to mimic full retinal computations from photoreceptors to output of the retina (retinal ganglion cells) for targeted feature-extraction. The feature of choice in this work is Object Motion Sensitivity (OMS) that is processed locally in the IRIS sensor. We study the capability of OMS in solving the ego-motion problem of the event-based cameras. Our results show that OMS can accomplish standard computer vision tasks with similar efficiency to conventional RGB and DVS solutions but offers drastic bandwidth reduction. This cuts the wireless and computing power budgets and opens up vast opportunities in high-speed, robust, energy-efficient, and low-bandwidth real-time decision making.


%%% Version 1

% Neuromorphic image sensors draw inspiration from the human-retina to create an electronic device that can process visual stimuli in a way that closely resembles its biological counterpart.
% These sensors process information significantly different than traditional RGB sensors.
% Specifically, the sensory information generated by neuromorphic image sensors is orders of magnitude sparser compared to that of RGB sensors, while retaining feature-rich information.

%Here, we highlight the capability of a new information processing paradigm called Integrated Retinal Functionality in CMOS Image Sensors (IRIS).
%This sensor draws inspiration from the human retina and enables the extraction of feature-rich information that specifically highlights areas with a high Object Motion Sensitivity (OMS) response.

% Our results show that OMS can accomplish standard computer vision tasks with similar efficiency to conventional RGB and Dynamic Vision Sensor (DVS) solutions.
% However, it also offers several key advantages, including a \hl{significant reduction in \textbf{power consumption} and \textbf{latency} by 35\% and 50\%, respectively. }
% We evaluated the performance of OMS on an object detection task with BDD100k and YOLOv5 with each image representation having a resolution of 1280 x 720. When normalizing the accuracy values from each representation by their respective bit rate per frame, we see that an individual bit of OMS contains orders of magnitude more information versus RGB and DVS with \textbf{17,825\%} and \textbf{326\%} more accuracy per bit, respectively.



%%% Version 0
%provide the scientific and academic communities with an electronic device capable of processing visual stimuli in a manner highly representative of the biological counterpart.
%These sensors process information in a drastically different manner than traditional RGB sensors where the resulting sensory information is orders of magnitude more sparse while retaining feature-rich information.
%Here, we highlight the capability of a novel retinal-inspired information processing paradigm, Integrated Retinal Functionality in CMOS Image Sensors (IRIS), which extracts feature-rich information that illuminates areas with a high object motion sensitivity (OMS) response.
%Our results show that OMS is capable of performing standard computer vision tasks with comparable performance to existing RGB and DVS solutions while reducing bandwidth, power consumption, and latency by three orders of magnitude, \textcolor{red}{35\%}, and \textcolor{red}{50\%} respectively.

\end{abstract}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{authors contributed equally}
\footnotetext[2]{corresponding author - mparsa@gmu.edu}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

% introductory paragraph that sets the paper up for further information about
% and the issues with RGB and DVS sensor-based machine learning systems
    
\noindent
% mparsa: I completely disagree with this paragraph :D There are a variety of computer vision application at edge devices or even power restricted domains. I don't agree with putting RGB and DVS in one basket. I also believe we don't need to start introduction with a negative sentence: despite.. we can start with computer vision has immense success in 
%Despite the immense success of computer vision in a variety of fields such as autonomous driving~\cite{yurtsever2020survey, wen2022deep, ma20223d}, robotics \cite{lu2022computer, snyder2021thor, gonzalez2023survey}, and manufacturing \cite{zhou2022computer, wang2023knowledge}, there are multiple issues plaguing the technology's further adoption in portable devices, space applications, and more generally size, weight, and power restricted domains.
%Today's state of the art models are broadly represented through CMOS-based RGB cameras \cite{osti_1639074, doi:10.1179/136821909X12490326247524} and dynamic vision sensors (DVS) \cite{gallego2020event}.

% a brief background of RGB cameras and the specifics of their architecture
\noindent
Digital cameras have become an essential tool for capturing visual information in our environment. Their applications range from smartphones \cite{Hayes:12} and autonomous driving~\cite{ma20223d}, to robotics \cite{snyder2021thor}, and manufacturing \cite{zhou2022computer}. Event-based cameras, also referred to as neuromorphic cameras, represent the next generation of imaging technology, drawing inspiration from biological retinas to extract events from visual stimuli in a faster and more efficient manner compared to traditional cameras \cite{gallego2020event}. 
 The most common event-camera architecture is the Dynamic Vision Sensor (DVS), which is made up of a pixel array that responds asynchronously and independently to brightness changes in the scene \cite{gallego2020event}. 
 
The most common event-camera architecture is the Dynamic Vision Sensor (DVS) which is compromised of a pixel array whose units respond asynchronously and independently to brightness changes in the scene \cite{gallego2020event}. This continuous stream of events is different from the sequential production of frames in traditional active pixel sensor (APS) cameras.
Importantly, events are sparse in space and time and therefore enable a memory and energy efficient representation of spatiotemporal activity including motion.
Therefore, DVS serves as a practical solution to the size, weight, and power (SWaP) constraints of embedded image processing systems, such as in self-driving cars \cite{binas2017ddd17} and autonomous robotics \cite{binas2017ddd17}.

%Digital cameras provide human society with a portable means of capturing visual stimuli from our surroundings. Today, you can find this technology in most areas of modern life such as smartphones \cite{Hayes:12} and automobiles \cite{yurtsever2020survey}.
% \hl{This combined with the ever-increasing amount of digital storage, has led to a situation where amount of information stored in this format is truly unprecedented.
% As such, the greater scientific and academic communities are researching methods of exploiting the information contained within this collection for machine learning models to perform a wide variety of tasks.
% }

% a brief background on DVS cameras and the specifics of their architecture
% Event-based cameras, also referred to as neuromorphic cameras, represent the next generation of imaging technology, drawing inspiration from biological retinas to extract events from visual stimuli in a faster and more efficient manner compared to traditional cameras \cite{gallego2020event}.


%Event-based cameras, also known as neuromorphic cameras, are the the next generation of image technology that take inspiration from biological retinas to provide a faster and lower bandwidth means of extracting events from visual stimuli \cite{gallego2020event}.

% The most common event-camera architecture is the Dynamic Vision Sensor (DVS) which is compromised of a pixel array whose units respond asynchronously and independently to brightness changes in the scene \cite{gallego2020event}.

% This continuous stream of events stands in stark contrast to the discrete production of frames produced by active pixel sensor (APS) cameras.
% Importantly, events are sparse in space and time and therefore enable a memory and energy efficient representation of spatiotemporal activity including motion.
% Therefore, DVS serves as a practical solution to the size, weight, and power (SWaP) constraints of embedded image processing systems, such as in self-driving cars \cite{binas2017ddd17, chen2020event, maqueda2018event} and autonomous robotics \cite{binas2017ddd17, andersen2022event, rodriguez2022free}.

% Discuss the difficulty of distinguishing between ego motion and the motion of other agents

In real-world applications of event-based computer vision systems, distinguishing between events caused by moving objects and those caused by the camera's ego-motion has been a persistent problem \cite{khan2017ego}. Unlike RGB frames, event data provides limited contextual information about the observed scene. Consequently, it is challenging to distinguish between the active foreground object's spikes and the static background events caused by the camera's ego-motion. Numerous methods have been proposed to address the ego-motion problem, ranging from incorporating inertial data to fitting a linear motion model \cite{jia2021self, kinsman2012ego}. While some recent approaches employ neural networks to estimate ego-motion, these methods tend to be computationally demanding.

% \textcolor{red}{revise}Evolution has designed it's own elegant solution and hidden it in the retinal circuits defining animal eyes. A common 

% An important component of vision are 

% the fixational movements without which the retinal neurons would not be sufficiently stimulated to produce visible images \textcolor{red}{cite}. These movements present a similar problem to those faced by DVS cameras on moving platforms.  Optical circuits have evolved to handle these movements by way of object motion sensitive (OMS) ganglion cells}

A biological solution emerges from a computation performed in the neural circuitry of the animal retina--\textit{Object Motion Sensitivity (OMS)} \cite{schwartz2021object, baccus2008retinal}.
OMS is a fundamental computation performed within the animal visual system by the feature-spike activity of Retinal Ganglion Cells (RGCs). The algorithm instantiated in the biological circuit involves subtracting a global temporal contrast signal in the receptive-field surround from a local contrast signal in the receptive-field center.

\begin{figure}
    \centering
    \includegraphics[width=0.37\textwidth]{figs/retinal_circuit_labelled_v2_updated.jpg}
    \caption{Retinal circuit for Object Motion Sensitivity embedded inside hierarchical retinal layers.}
    \label{fig:biological_oms}
\end{figure}

Figure \ref{fig:biological_oms} visualizes the biological retina's architecture that is responsible for extracting the OMS features and highlights that OMS aims to build upon the biological underpinnings of DVS to develop a more biologically plausible sensor.
This activity is used by the brain to discriminate the motion of objects (object motion) from motion caused by motion of the observer (ego-motion) \cite{schwartz2021object}.

% \textcolor{red}{TODO: We need to add information about each of these models on frames per seconds versus power consumption and versus bit rate; I have looked at some papers and it seems that the latency calculation parameters are drastically different between papers so we may have to do this on our own. It may also be beneficial to publish the evaluation code alongside the paper}

% discuss we are aiming to take this a step further by taking more inspiration
% from the biological retina along with some of the lower level architectural
% and feature reduction methods from ganglion cells.
This work takes inspiration from the biological retina to evaluate in-sensor computation methods in real-world environments. Integrated Retinal Functionality in CMOS Image Sensors (IRIS) is a novel retina-inspired approach to vision sensing that uses spike-based processing to filter out the dichotomy between self motion of the camera from the physical movements of objects in the scene \cite{yin2022iris}. Furthermore, IRIS reduces the computational requirements of machine learning models by incorporating data preprocessing and feature extraction within the physical sensor leveraging 3D semiconductor integration technology. This approach stands at the score of the edge computing paradigm where budgets for wireless and computing power can be drastically reduced along with opening up ground breaking opportunities in ultra-fast decision making. Moreover, novel application driven chip solution like IRIS is at the epicenter of United States semiconductor road map (CHIPS act) \cite{Rep_Ryan_2022}. 

% discuss some preview results from our work and state the main axioms
In summary, the major impacts of this paper are as follows:

\begin{enumerate}[noitemsep,itemindent=-1em]
    \item We present an algorithmic model inspired by biological retinal ganglion cell (RGC) computations for extracting OMS features from visual stimuli. Our evaluation focuses on the effectiveness of the model in capturing OMS features compared with RGB and DVS.

    \item We assess the performance characteristics of OMS in a standard computer vision task; object detection using a high-resolution autonomous driving dataset along with a state-of-the-art convolutional neural network (CNN).

    % \item We evaluate the performance characteristics of OMS on a standard computer vision task, object detection using a high resolution autonomous driving dataset along with a state-of-the-art convolutional neural network.
    \item We perform thorough evaluation of the numerous benefits that come from OMS where the resulting representation contains 3.26x more information per bit of transmitted data. 
    
\end{enumerate}

% --------------------------------
% Background and Previous Methods
% --------------------------------
\section{Related Work}

\noindent
To the best of our knowledge, this works serves as a foundational piece at the intersection of end-to-end retinal computations applied to existing computer vision tasks. As such, we will go through two research areas that are directly related to this novel application: ego-motion compensation and optimization for size, weight, and power constrained environments.

\textbf{Ego-Motion Compensation} The ego-motion problem is a persistent issue which has plagued efforts to use DVS cameras mounted on moving platforms. Whenever the platform shifts, the DVS pixels pick up on the reflectance changes and produce output even when all entities in the scene remain static. These events tend to occur around edges in the scene but frequently extend to the object surfaces. This results in the occlusion of salient entities in the scene which complicates the task of moving object detection. Many works have addressed this problem through the parametric modeling of camera motion.

Stoffregen, et al. \cite{stoffregen2019event} leverages an iterative Contrast-Maximization \cite{gallego2018unifying} approach to jointly model the motion parameters defining a set of clusters and predict the event-cluster assignments. While this approach is effective, it requires a slow and compute intensive iterative approach. A later work by Liu, et. al. \cite{liu2020globally} attempts to solve this problem through the use of bounding functions to place constraints on Contrast-Maximization though they still required the use of gradient descent and only evaluated their work on rotational ego-motion.

% ------------------------
% Event-based Moving Object Detection and Tracking
% ------------------------
Similarly, Mitrokhin, et al. \cite{DBLP:journals/corr/abs-1803-04523} introduces an approach to object tracking compatible with event-based cameras using parametric modes. These modes are used to estimate the three-dimensional geometry of the event data and correct for noise from ego-motion. The central experiment within this paper is the ability of the model to segment object motion and compensate for ego motion. Compared to OMS and IRIS, this work has two major limitations: (1) there are no references to biological inspirations, and (2) it cannot be embedded into low-overhead, spatial-temporal computing due to the underlying motion compensation system's need for constant updates of a time-dependent point cloud model.

Several other researchers have explored deep learning methods for compensating for ego-motion during object detection and segmentation tasks. These approaches typically involve discretizing the event stream into a series of frames, which can then be used to train convolutional neural networks (CNNs) for computing the visual odometry of the scene. For example, Nitin et. al. \cite{sanket2020evdodgenet} creates a 3-channel event frame from an event stream where the first and second channels are the positive and negative event counts respectively and the third is the average time between events. This frame is then passed to a series of shallow neural networks which jointly compute the movements of the camera and observed objects. Zhu et. al \cite{zhu2019unsupervised} takes a similar approach though uses a novel method to discretize the time domain though uses bilinear sampling. The resultant frames are given to an encoder-decoder CNN which has another network tied to it's residual block for the purpose of pose estimation when performing ego-motion prediction.

%https://arxiv.org/pdf/1812.08156.pdf

%https://arxiv.org/pdf/2002.05911.pdf

%https://arxiv.org/pdf/1906.02919.pdf EV-DODGE NET 

%https://arxiv.org/pdf/1903.07520.pdf EV-IMO Parametric 

%https://arxiv.org/pdf/1903.07520.pdf

%https://arxiv.org/pdf/1906.02919.pdf

%https://arxiv.org/pdf/2002.10686.pdf

%https://arxiv.org/pdf/1904.01293.pdf

%https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9985998

% ------------------------
% Pseudo-labels for Supervised Learning on Dynamic Vision Sensor Data, Applied to Object Detection under Ego-motion
% ------------------------

Chen, et. al.  \cite{DBLP:journals/corr/abs-1709-09323} introduces a non-biologically inspired method for performing standard computer vision tasks under ego-motion is presented. Their major contribution, as opposed to our work, is their evaluation of the effectiveness of their approach in sub-optimal viewing conditions such as those with motion blur and poor lighting. However, there is no mention of biological inspiration for this method. In contrast, our approach is inspired by biology and our experiments target a broader range of object classes, including cars, pedestrians, buses, trucks, bicycles, riders, and motorcycles.

% In \hl{change format} \cite{DBLP:journals/corr/abs-1709-09323}, a non-biologically inspired method for conducting standard computer vision tasks under ego-motion is introduced. Their major contribution versus this paper is that they evaluated the effectiveness of their approach in sub-optimal viewing conditions such as situations with motion blur and poor lighting. There is no mention of biological inspiration for this specific methods. Compared to our solution, a major limitation of this work is that they only focus on classifying cars where as our experiments look for cars, pedestrians, buses, trucks, bicycles, riders, and motorcycles.


% Most existing works create parametric motion models to track the movement of the camera \textcolor{red}{cite}. These tend to be insuffiecient 

% \hl{TODO: Finish background section}

While the aforementioned studies were the closest to our research, there have been a variety of other methods proposed and investigated in radar applications \cite{lee2020long} and frame-based vision \cite{li2022dynamic}.  Our approach is radically different from previous works because the underpinnings of object motion sensitivity come directly from biology rather than being defined by an arbitrary network architecture or parametric model.

% -------------
% Introduce SWaP as a major area of research
% -------------
\textbf{Size, Weight, and Power (SWaP)} State of the art applications of deep neural networks require the quick and accurate processing of event-based sensory information that is compatible real-time intelligent systems.
% -------------
% Introduce subsections: algorithms and in-sensor
% -------------
Prior works in this area can be broken down into two major areas: algorithm optimization and in-sensor computation.
Algorithm optimization aims to process sensory information from existing technologies such as RGB or DVS and optimize the machine learning pipeline.
These models are responsible for extracting fundamental features from visual perceptions to make better decisions on lower-dimensional data.

% -------------
% Discuss works on algorithm optimization
% -------------
The majority of literature focuses on the algorithmic optimization of computer vision systems to more standard RGB and DVS sensors.
A variety of sub-fields emerge with solutions that aim to mitigate the performance penalties that come with complicated neural network optimization systems such as quantization \cite{choi2016towards}, pruning \cite{blalock2020state}, neuromorphic computing \cite{schuman2022opportunities, patton2021neuromorphic}, and novel architecture design \cite{parsa2021multi, schuman2020automated}. While these works result in intelligent systems that are more capable in edge applications, a significant portion of the learning is dedicated to dealing with noisy and low entropy visual representations.

% -------------
% Discuss works on in-sensor computation
% -------------
Numerous works strive to mitigate the challenges that hinder the wider adoption of intelligent computer vision systems in SWaP (size, weight, and power)-constrained environments by enhancing the sensor's processing capabilities via chip integration methodologies.
A variety of methods are explored with varying success such as 3D monolithic integration \cite{vivet2018monolithic}, 3D heterogeneous integration \cite{zhou2020near}, planar system on chip (SoC) integration \cite{zhou2020near}, and 2.5D chiplet integration \cite{zhou2020near}. These works emphasize on the hardware itself and propose it as a possible solution to a variety of problems. Our work differs in using biologically inspired OMS functionality for algorithmic analysis that can be embedded inside sensor arrays featuring spatio-temporal computations while leveraging 3D integration scheme.  

% W  that can be morphed to variety of computer vision scenarios in real-time applications. The main application area explored in this work was determining if an event occurs within a scene. They highlight that their multi-layer architecture eliminates temporal and spatial redundancy which reduces power consumption by 96.91\%. While this paper presents very interesting results, removing both the spatial and temporal information from visual stimuli makes the application of this technology to existing computer vision applications extremely difficult.
%A major limitation of the broader application of these technologies is 
%For example, state of the art computer vision models such as \cite{DBLP:journals/corr/abs-2104-11892} require anywhere from 219 million floating point operations per second (FLOPS) to 103 billion FLOPS.
% % On an Nvidia A100 this would consume 300 watts of power with an inference speed of 17 frames per second (FPW) to 0.33 FPW.
% This problem is made significantly more difficult when limited to SWaP constrained environments.

% discussion of the latency with RGB cameras
% Despite the prevalence of the RGB image sensor, there are a number of major limitations plaguing the technology's broader adoption in SWaP constrained environments.
% The first of which is the high bandwidth required for real-time data transmission where a 720p RGB camera operating with 8-bits per channel results in a data rate of approximately $2.21e7$ bits per frame.

% % discussion of the lantency with DVS and the limitied processing speed
% % large-scale models on GPUs
% A DVS sensor with a comparable resolution would have a final data rate of $1.96e5$ bits per frame. While this is two orders of magnitude less that the traditional camera, processing incoming data at this rate is a non-trivial task. This example highlights the relevance of the minimizing sensor bandwidth to optimize latency further along the image processing pipeline.

% discussion of the make issue with power and weight that comes from having to run
% these models on large-scale GPU-based computing system

% A plethora of works have been done that aim to create a more biologically inspired method of processing sensory information to try and alleviate many of the issues with ego motion and . 

% % ------------------------
% % Pseudo-labels for Supervised Learning on Dynamic Vision Sensor Data, Applied to Object Detection under Ego-motion
% % ------------------------
% In \cite{DBLP:journals/corr/abs-1709-09323}, a non-biologically inspired method for conducting standard computer vision tasks under ego-motion is introduced. Their major contribution versus this paper is that they evaluated the effectiveness of their approach in sub-optimal viewing conditions such as situations with motion blur and poor lighting. There is no mention of biological inspiration for this specific methods. Compared to our solution, a major limitation of this work is that they only focuses on classifying cars where as our experiments look for cars, pedestrians, buses, trucks, bicycles, riders, and motorcycles.

% % ------------------------
% % Event-based Moving Object Detection and Tracking
% % ------------------------
% A recent article \cite{DBLP:journals/corr/abs-1803-04523} introduced a new approach to object tracking compatible with event-based cameras.
% They use parametric modes to estimate the three-dimensional geometry of the event data and correct for noise from ego-motion. The central experiment within this paper is the ability of the model to segment object motion and compensate for ego motion. There are no references to biological inspirations behind their algorithmic techniques. The major downfall of this paper is the lack of evaluation and experimentation put into evaluating the performance of their system versus state of the art systems.

% % ------------------------
% % Bio-inspired smart vision sensor: toward a reconfigurable hardware modeling of the hierarchical processing in the brain
% % ------------------------
% A series of hierarchical computation layers are proposed in \cite{Bhowmik2021-up} that can be morphed to variety of computer vision scenarios in real-time applications. The main application area explored in this work was determining if an event occurs within a scene. They highlight that their multi-layer architecture eliminates temporal and spatial redundancy which reduces power consumption by 96.91\%. While this paper presents very interesting results, removing both the spatial and temporal information from visual stimuli makes the application of this technology to existing computer vision applications extremely difficult.
 
% -----------
% Conclusion
% -----------
In conclusion, there have been extensive research efforts dedicated to ego-motion detection and compensation, as well as dimensionality reduction for SWaP-constrained environments. While these areas have been investigated separately, there has been limited research at their intersection where the trade-off between ego-motion classification performance, information density, and applicability to existing computer vision applications is explored. This paper seeks to establish a research foundation at the intersection of ego-motion detection and compensation, while taking into account information density and applicability in SWaP-constrained environments through the use of a biologically inspired functionality - Object Motion Sensitivity.

%In summary, there have been many research efforts conducted around ego-motion detection and compensation along with dimensionality reduction for SWaP constrained environments.
%While there have been numerous works done on these areas in their own right, there has been little work done at their intersection where the trade-off between ego-motion classification performance, information density, and applicability to existing computer vision applications is explored.
%This paper aims to build a foundation of research at the intersection of ego-motion detection and compensation along with taking major consideration for information density and applicability in SWaP constrained environments through use of a biologically inspired functionality - Object Motion Sensitivity.

\section{Research Methods}

\noindent
In this section, we discuss the fundamental building blocks of this project. As shown in Figure \ref{fig:methods_flowchart}, we start off with a discussion about object motion sensitivity and Integrated Retinal Functionality in CMOS Image Sensors (IRIS). Next, we take a detailed look at the model architecture, machine learning task, and the various metrics used to evaluate the broader impacts and applications of Object Motion Sensitivity (OMS) on existing computer vision tasks.

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figs/methods_flowchart.png}
    \caption{A flow chart presenting our processing for preprocessing BDD100k, converting it to DVS and OMS representations, fine-tuning YOLOv5, and our performance evaluation.}
    \label{fig:methods_flowchart}
\end{figure}

% ---------------------------
% Research Methods: Dataset
% ---------------------------
\subsection{Dataset}
\noindent
Event-based cameras transmit a continuous stream of events corresponding to reflectance changes in the scene \cite{gallego2020event}. These events are transmitted in an address event representation (AER) unlike traditional active pixel sensors (APS) which output a 3-dimensional image. Due to this difference in output and the limited availability of event-based cameras, the creation of datasets has been limited to static cameras observing single moving objects, such as CIFAR10-DVS \cite{10.3389/fnins.2017.00309} and DHP19 \cite{9025364}. Recently, more complex datasets like DDD20 \cite{DBLP:journals/corr/abs-2005-08605} have been created using event cameras on a moving platform. However, they lack the accurate bounding box and segmentation labels required to compare the detection accuracy of a model trained on OMS data versus the detection accuracy of that same model trained on DVS data.

To overcome this limitation, we used the ViBES retinal computation simulator \cite{vibes} to construct frames that approximate DVS data from the Berkeley Deep Drive 100K Multi-Object Tracking and Segmentation (BDD100K MOTS) dataset \cite{DBLP:journals/corr/abs-1805-04687}.
The BDD100K MOTS dataset contains 90 videos from the original BDD100k dataset. The original videos were recorded at 30 fps and resampled to 5 fps for the MOTS dataset so that the salient objects in each frame could be labeled.
This results in each video having approximately 200 frames per video.
The observed objects fall into one of seven classes: cars, pedestrians, buses, trucks, bicycles, riders, and motorcycles.
We randomly chose 60 of these videos to convert into DVS data and then fed that DVS data into the algorithmic implementation of OMS provided by ViBES.
Our final dataset contains a close approximation of what one could obtain using an OMS inspired sensor such as IRIS \cite{yin2022iris}.


% ---------------------------
% Research Methods: Discuss the details of OMS
% ---------------------------
\subsection{Object Motion Sensitivity}


%Animal eyes%
\noindent
The visual pathways of many organisms have evolved the OMS circuit to suppress stimuli created by arbitrary eye movements and global motion. This circuit is comprised of bipolar, amacrine, and retinal ganglion cell layers, which work together to distinguish between stimuli created from global and local motion.
The ViBES simulator contains an algorithmic approximation of the retinal OMS computation \ref{oms}.
In the case of the BDD100k dataset, the data is initially in the form of a collection of RGB frames.
When executing on this data type, ViBES first computes frames approximating DVS by performing a difference on the frames and determining if the result exceeds the chosen contrast threshold of \(0.1\).

\begin{algorithm}[hbt!]
\caption{OMS algorithm}\label{oms}
\begin{algorithmic}[1]
\Require {radius $r_1$ $<$ radius $r_2$}
\Procedure{OMS}{$dvs\_frames$, $s\_weight$}
\State $center\gets diskfilter(r_1)$
\State $surround \gets s\_weight * diskfilter(r_2)$

\For{$i$ in $len(dvs\_frames)$}{}
    \State $frame \gets dvs\_frames[i]$
    \State $fil\_center \gets applyFilter(center, frame)$
    \State $fil\_surr \gets applyFilter(surround, frame)$
    \State $events \gets fil\_center - fil\_surr$
    \State $OMS[i] \gets events > threshold$
\EndFor

\State \textbf{return} $OMS$
\EndProcedure
\end{algorithmic}
\end{algorithm}

This method is an approximation of that performed by DVS cameras whose events defined as \(e_n=\{x_n, y_n, t_n, p_n\}\) are elicited when a change in the log photocurrent \(L=log(I)\) exceeds the temporal contrast threshold \(\pm C\) at a given pixel \(\varphi_n=\{x_n, y_n\}\) based upon \(E(\varphi_n, t_n)\) such that \cite{gallego2020event}:

\begin{equation}
\Delta L(\varphi_n, t_n)) = L(\varphi_n, t_n) - L(\varphi_{n-1}, t_{n-1})
\end{equation}

\begin{equation}
   E(\varphi_n, t_n)=\begin{cases}
     e_n, & \text{$\Delta L(\varphi_n, t_n) > +C)$} \\
     e_n, & \text{$\Delta L(\varphi_n, t_n) < -C)$} \\
     \varnothing, & otherwise
   \end{cases}
 \end{equation} \\

The resultant DVS frames are then sent to the OMS function \ref{oms}. From a biological perspective, the DVS frames represent the response to photorecptor activation by bipolar cells and the OMS function takes the role of the amacrine and retinal ganglion cell layers. The OMS algorithm is comprised of two circular averaging filters also known as disk filters \cite{MATLAB}. These filters are matrices containing a discrete feathered circle of a chosen radius whose values sum to one. Values in the center of the matrix possess larger values and thus carry more weight. The matrix convolves over the frame by centering itself over each pixel and storing resultant value into said pixel's position. This value is the mean contrast of the region covered by the disk filter.

The smaller of these disk filters is the center filter which represents a retinal ganglion cell (RCG) and the excitatory bipolar cell cluster with which it is connected. We chose a radius of 1 to lower the chance that a single cell cluster covers an entire entity. The larger disk filter server the role of the amacrine cells, which are designed to inhibit the RCGs' response if global motion is observed. For this filter, a radius of 5 was chosen to cover a sufficiently wide region of each frame without significantly diminishing the weights. If the ways are too small then the surround filter will have no impact on the center filter values. In order to simulate the inhibition, the mean contrast values from the amacrine (surround) filter are subtracted from those of the RCG (center) filter. If the resultant values are larger than the threshold, we chose a threshold of 0.1 to match DVS, then a Boolean spike is stored in the OMS frame tensor.


% Input from Greg
% We leveraged the  ViBES simulator's \cite{vibes} algorithmic approximation of the retinal OMS computation \ref{oms} to convert the BDD100k MOTS dataset from RGB to DVS and OMS frames.

% To implement the algorithmic core of the retinal OMS computation, we used the algorithm depicted in algoritm \ref{oms}. The first layer is a differencing circuit akin to current DVS, that computes temporal contrast at each pixel by simply subtracting the previous frame from each frame in the movie. In the retina, these steps occur at the level of photoreceptors and bipolar cells. 

% The second layer of the algorithm captures the action of amacrine cells in the inner retina and integration in RGCs. The contrast (DVS) signals are sampled by spatial disc filters in both a small "center" region and a substantially larger "surround" region (center and surround size are parameters of the algorithm). The mean contrast signal in the surround is then multiplied by a weight factor (another parameter) and subtracted from the mean contrast signal in the center. Finally, the resulting value is compared to a threshold (parameter) to determine whether the model OMS RGC produces a spike. 

% ---------------------------
% Research Methods: Discuss the details of IRIS
% ---------------------------
\subsection{Integrated Retinal Functionality in Images Sensors (IRIS)}

\noindent
IRIS cameras, first proposed in \cite{yin2022iris}, aim to embed retinal computations including OMS functionality inside image sensing platforms. IRIS cameras are new generation of neuromorphic cameras that aim to mimic feature-extraction computations of animal retina from photo-transduction to computations being performed in the inner retinal layers, much of which has only been recently discovered by the retinal neuroscience community \cite{schwartz2021object}. The initial version of the sensor implements two retinal features \cite{schwartz2021retinal}: object motion sensitivity and looming detection. 

In comparison to state-of-the-art active pixel CMOS image sensors (RGB cameras) \cite{feng2019computer, moeslund2001survey}, that use a plethora of sophisticated and computationally complex algorithms to extract images features, retinal computations for IRIS cameras can be embedded inside an image sensor using low-cost, highly-efficient retina-inspired circuits \cite{shah2021review}. Similarly, IRIS cameras go beyond existing DVS cameras that focus on change detection aspect of retina to embed analog spatio-temporal computations for inner retinal layers needed for extracting retinal features by leveraging 3D integration of semiconductor chips \cite{choudhury20103d}. With respect to OMS computations, IRIS cameras distribute retinal computations from photo-transduction to RGC using an interleaved center-surround receptive field distributed through out the camera focal plane \cite{olveczky_Baccus_Meister_2007}. 

IRIS cameras thus form the required underlying hardware substrate that can implement the OMS algorithm of the previous section, inside state-of-the-art camera manufacturing technology, for real-time extraction of OMS spikes in a highly SWaP (size, weight and power) constrained environment. 


%State of the art computer vision systems \cite{DBLP:journals/corr/abs-2104-11892, doi:10.1080/08839514.2022.2032924, feng2019computer, moeslund2001survey, hafiz2023formula, Hussien_2021} use a plethora of sophisticated and computationally complex algorithms to extract images features that are fundamental to a given tasks. Integrated Retinal Functionality in Image Sensors (IRIS) \cite{yin2022iris} aims to mitigate the majority of these notoriously expensive operations by creating a digital analog for computing fundamental images features used within the human retina. 
%\hl{TODO: Need help from Akhilesh on the hardware side of things}

% ---------------------------
% Research Methods: YoloV5
% ---------------------------
\subsection{YOLOv5}

\noindent
Based on the initial version introduced in 2015 \cite{journals/corr/RedmonDGF15}, YOLOv5 (You Only Look Once Version 5) \cite{glenn_jocher_2022_7347926} stands as a major step forward as it introduces a variety of features over the initial version. Throughout the different versions, a variety of features have been added such as focal loss, batch normalization, and Mosaic data augmentation. This model architecture has been deployed in a variety of scenarios such as autonomous vehicles \cite{kasper2021detecting}, medical imaging \cite{mohiyuddin2022breast}, and video surveillance \cite{wu2021application}. We chose YOLOv5 for this study due to its impressive performance and user-friendly interface, which facilitates the replication and extension of results.

% ---------------------------
% Research Methods: Metrics
% ---------------------------
\subsection{Bandwidth Reduction}

\noindent
To gain a quantitative perspective of the capabilities of OMS, we have established a quantitative metric for bandwidth reduction represented by $bit\_rate$. 

\begin{equation}
    bit\_rate=hw\psi/t
    \label{bit_rate}
\end{equation}

\noindent
Our metric for bandwidth, as shown in Equation \ref{bit_rate}, is bit rate which is defined as $bit\_rate=(hw\psi)/t$ where $h,w \in \mathbb{N} \geq 0$ and $\psi,t \in \mathbb{R} \geq 0$. $h$ and $w$ are the height and width of an individual frame in pixels. $t$ is the time in seconds required to process the given frame and $\psi$ is the bit depth of an individual pixel. 

A lower bit-rate indicates a lower data bandwidth requirement while also reducing the total number of bits to be transmitted over a communication channel. Depending on the underlying communication scheme - wired \cite{kempainen2002low}, short-distance \cite{mozaffariahrar2022survey},   or long-distance wireless communication \cite{sinha2017survey}, the lower data bandwidth translates to lower communication energy while also avoiding data congestion in bandwidth constrained environment.

%For energy consumption, we consider the energy consumed in embedding change detection (DVS) as well as  computations for inner retinal layers to achieve OMS functionality inside IRIS cameras. Our energy metrics are based on OMS circuits discussed in \cite{yin2022iris} and are obtained by simulations performed on Globalfoundries 22nm technology node for RGB, DVS and OMS functions. 

%Further, to quantize the energy saved due to reduced bandwidth, we add communication energy i.e. energy consumed in transmitting RGB pixels, DVS spikes and OMS spikes from respective cameras to a back-end processing unit connected to respective cameras using on the the following communication medium (i) on a printed circuit board that uses LVDS (low-voltage differential signalling) pins \cite{kempainen2002low} (ii) on a 2.5D silicon interposer technology \cite{lenihan2013developments} (iii) near distance wireless communication (WiFi) \cite{mozaffariahrar2022survey}  and (iv) long-distance wireless communication (NB-IoT) \cite{sinha2017survey}. These four different communication strategies represent real-life scenarios wherein a feature-extraction camera (RGB, DVS or IRIS (for OMS extraction)) has to communicate with a back-end processor over different communication medium based on target end application and SWaP constrains.

% The concept of entropy comes from information theory where it is defined as the amount of uncertainty with a set of bits \cite{shannon2001mathematical}. This knowledge is built upon the foundation set out by the field of thermodynamics where entropy is defined as the amount of disorder within a thermodynamic system \cite{leff1996thermodynamic}. When the scope of entropy is narrowed to the concept of space time signals, it is also analogous to the amount of information held within the signals.

% \begin{equation}
%    H(X) = - \sum p(x_i) log(p(x_i))
%     \label{entropy-eq}
% \end{equation}

% As shown in Equation \ref{entropy-eq}, Shannon entropy is defined as the negative summation of the probability of $x_i$ times the logarithm of the probability of $x_i$ for all $i$\cite{shannon2001mathematical}. We can gain a lot of context from the values of $H(X)$ where a low value means that the information within a given representation contains less information and can be compressed. Conversely, high entropy values indicate that a given representation contains more information and cannot be compressed as easily.

% Lastly for latency, we have the floating point operations per section required to inference on the different sensor modalities where a lower FLOPS value means that the model can be run at the same speed on slower hardware or must faster on the same hardware. \hl{TODO: Input from Akhilesh about specific numbers from hardware}

% ----------------------------------------------
% - Results ------------------------------------
% ----------------------------------------------
\section{Results}

\noindent
We evaluated the effectiveness of OMS for an object detection task on the Berkeley Deep Drive dataset \cite{DBLP:journals/corr/abs-1805-04687}.
All machine learning tests were conducted with the small configuration of YOLOv5 which has 7.2 million parameters and requires 16.5 billion FLOPS.
This model was pretrained on the COCO dataset \cite{DBLP:journals/corr/LinMBHPRDZ14} for 300 epochs and results in a final mAP value of 37.4. The following subsections contain information and results from each of the performance comparisons used to evaluate the effectiveness of OMS versus RGB and DVS images.
We start off by evaluating the performance delta between RGB, DVS, and OMS in their native state with each image type having a resolution of 1280 by 720.
This resolution will remain static through all of the following tests.
For each image representation, we fine tune the aforementioned pretrained weights for 100 epochs with a batch size of 128 on a computer equipped with an Intel Xeon W-2295 processor, 128GB of system memory, and an Nvidia RTX A5000.
% Next, we evaluate the average information density across the entire dataset among the three image types. This serves as a measure of the feature richness of the given representations. 
We continue with an evaluation of the average bit rate, analogous to spike rate, of each image representation throughout the Berkeley dataset. We end this subsection by normalizing the performance values of each image type by their respective data rate.

% ------------------------------
% - Results: Native Performance
% ------------------------------
\subsection{Native Performance}

\noindent
We begin with a look at the native performance of each image type for object detection with YOLOv5.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{results/comparison_plots/f1.png}
    \caption{A line chart showing the performance deltas between the three image representations at every training epoch.}
    \label{fig:native_performance}
\end{figure}

As shown in Figure \ref{fig:native_performance}, RGB outperforms the other image types by a significant margin where DVS and OMS fall behind by $62.89$\% and $69.83$\%, respectively.
These performance penalties are to be expected as RGB image sensors capture the magnitude of different wavelengths of light at every pixel which results in a drastically higher data rate.
DVS' biologically inspired nature is designed to produce less information per frame.
This explanation is only amplified for the ever-increasing sparsity engineered into OMS sensors.
When applications are not limited by size, weight, and power constraints, it is evident that RGB is the best camera for this particular computer vision task.

% -------------------------------
% - Results: Information Density
% -------------------------------
% \subsection{Information Density}
% Next we investigate the information density and entropy of each representation.
% This takes significant inspiration from information theory \cite{shannon2001mathematical}, which itself takes inspiration from the principles of uncertainty in thermodynamic systems \cite{leff1996thermodynamic}. Given that entropy is analogous to the level of uncertainty within a system, the information density of sparser image frames, when keeping the resolution the same, should result in much lower entropy values.

% \begin{table}[]
%     \centering
%     \begin{tabular}{|l|l|}
%     \hline
%     \textit{Type} & \textit{Entropy} \\ \hline
%     RGB           &                  \\
%     DVS           &  0.7483          \\
%     OMS           &  \textbf{0.2468} \\ \hline
%     \end{tabular}
%     \caption{A comparison of the average entropy, per frame, of each image representation.}
%     \label{table:entropy}
% \end{table}

% Given the high amount of information within RGG images, we expect the average entropy values for this representation should be higher than DVS and OMS. This also comes down to the fact that information from RGB images generally comes from the relationships between multiple pixels rather the value of any single example. This phenomenon is highlighted in Table \ref{table:entropy} where RGB has an average entropy across the Berkeley dataset of \textcolor{red}{0.5} versus DVS and OMS with entropy values of $0.7483$ and $0.2468$ respectively.

% This results highlight the biologically inspired nature where DVS pixels represent more information than simply the color intensity at a given pixels. This reduced the uncertainty within the individual spikes where the information is more sparsely represented. The further biological inspiration behind OMS results in its individual pixels representation more information than the dynamic nature in DVS sensors where the resulting spikes are even more sparse and contain even more information.

% ---------------------
% - Results: Data Rate
% ---------------------
\subsection{Data Rate}

\noindent
Given that OMS is designed to increase the feature-richness of individual spikes along with their sparsity, it is vital that we compare the average data rates of the individual representations across our test dataset.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
    \textit{Type} & \textit{Bit Rate} \\ \hline
    RGB           & 2.21e7            \\
    DVS           & 1.96e5            \\
    OMS           & \textbf{3.77e4}   \\ \hline
    \end{tabular}
    \caption{A comparison of the average per frame bit rate of each image representation.}
    \label{table:data_rate}
\end{table}

Given the low entropy within RGB images, we expect it to have the highest data rate.
This comes in stark contrast to DVS and OMS where they are designed to increase information density while simultaneously decreasing data rates.
Table \ref{table:data_rate} shows the average data rates of these image representations across the entire BDD100K MOTS dataset.
As expected, we see that RGB has the highest data rate with a data rate of $2.21 \times 10^{7}$ bits per frame versus DVS and OMS with data rates of $1.96 \times 10^{5}$ bits per frame and $3.77 \times 10^{4}$ bits per frame, respectively.

% % --------------------------------------
% % - Results: Performance versus Entropy
% % --------------------------------------
% \subsection{Performance versus Entropy}
% Regardless of the higher information densities among the biologically inspired representations, we have to evaluate if the information remaining in these representation is capable of enabling a state of the art model for computer vision tasks.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\textwidth]{latex/figs/placeholder.png}
%     \caption{The F1 scores for the YOLOv5 models fine-tuned on RGB, DVS, and OMS where the final scores are normalized by the average entropy per frame of the given representation.}
%     \label{fig:performance_versus_entropy}
% \end{figure}

% Figure \ref{fig:performance_versus_entropy} shows the object detection accuracy versus entropy of the YOLOv5 model fine tuned on the given representations. We see that RGB versus entropy results in a normalized accuracy of \textcolor{red}{0.5} versus DVS and OMS with normalized accuracy values of \textcolor{red}{0.3} and \textcolor{red}{0.1}, respectively. This highlights the relative importance and feature-richness of the more biological representations where OMS has high ratio of performance versus entropy.

% ----------------------------------------
% - Results: Performance versus Bandwidth
% ----------------------------------------
\subsection{Performance versus Data Rate}

\noindent
Given the drastic reductions in data rate among the more biologically inspired representations, we need to evaluate how much of an impact this will have on the overall accuracy of our computer vision system. Therefore, we evaluate the performance of each image representation where the accuracy values are normalized by the data rates.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{results/comparison_plots/f1_over_bit_rate.png}
    \caption{The F1 scores for the YOLOv5 models fine-tuned on RGB, DVS, and OMS where the final scores are normalized by the average bit rate per frame of the given representation.}
    \label{fig:performance_versus_data_rate}
\end{figure} 

Given the low information density yet high data rate of RGB we expect it to have the lowest coefficient of performance versus data rate. 
The more biologically inspired methods should present ratio increases from RGB to DVS and DVS to OMS.
Figure \ref{fig:performance_versus_data_rate} shows the performance versus data rate for RGB, DVS, and OMS.
The performance versus data rate values for each representation are $1.88 \times 10^{-8}$, $7.91 \times 10^{-7}$, and $3.37 \times 10^{-6}$, respectively.
These results highlight that, compared to RGB, individual DVS bits contribute $41.07$x more accuracy per bit of information. OMS builds upon the impressive foundation of DVS with $187.25$x and $3.26$x more information per bit compared RGB and DVS, respectively.

% \subsection{Performance versus Object Size}
% \textcolor{red}{TODO: add introduction} \\
% \textcolor{red}{TODO: add figure} \\
% \textcolor{red}{TODO: todo add short discussion}

% \subsection{Noise Resiliency}
% \textcolor{red}{TODO: add introduction} \\
% \textcolor{red}{TODO: add figure} \\
% \textcolor{red}{TODO: todo add short discussion}

% ----------------------------------------------
% - Discussion \& Future Works -----------------
% ----------------------------------------------
\section{Discussion \& Future Works}

\noindent
In this work, we conducted a study of the applicability and effectiveness of object motion sensitivity (OMS) versus dynamic vision sensors (DVS), and traditional RGB-based sensors. OMS is a biological computation conducted within animal retinas where a goal is to reduce the dimensionality of visual information from the individual color values perceived at each cell to a more feature-rich and lower dimensional representation.

Rather than fully evaluate the biological plausibility of this mathematical representation of OMS, the focus of this paper was shifted to evaluate its application on more standard computer vision tasks. We choose object detection on the BDD100K MOTS dataset with our deep learning model being represented by YOLOv5. The mathematical representation for OMS is implemented within the Visual Behavioral Environment Simulator (ViBES). While this paper doesn't focus on the physical deployment of this algorithm, the hardware design as been developed and published in \cite{yin2022iris}.

We fine-tuned a pretrained version of YOLOv5 on both DVS and OMS for our performance evaluation on the BDD100K validation set.
We used three metrics to evaluate multiple performance aspects between the various image representations: F1-score, data rate, and F1-score versus data rate.

We began by showing that in their native state, without taking into consideration any of their unique performance characteristics, RGB is the most accurate representation with a final F1-score of $0.4177$.
The other types trailed by a significant margin with DVS and OMS having 62.89\% and 69.83\% less accuracy, respectively.

This story becomes drastically more interesting when taking into consideration the sparsity and lower bandwidths of DVS and OMS versus RGB. For example, the RGB sensor would have a data rate of $2.21 \times 10^7$ bits per frame versus DVS and OMS with an average bits per frame of $1.96 \times 10^5$ and $3.77 \times 10^4$, respectively. When normalizing the accuracy values from each representation by their respective bit rate per frame, we see that an individual bit of OMS contains orders of magnitude more information versus RGB and DVS with $178.25$x and $3.26$x more information per bit, respectively. In other words, this means that a given bit or spike on information contains $178.25$x more information that RGB images and $3.26$x more information per bit that DVS images.

Although we have demonstrated the remarkable performance improvements achieved by OMS compared to RGB and DVS, we have numerous future objectives and ambitions for this project.

% While we have presented the groundbreaking performance gains with OMS over RGB and DVS, we have a plethora of future goals and aspirations for this project:

\begin{enumerate}[noitemsep,itemindent=-1em]
    \item Investigate the impact on the overall effectiveness of OMS versus DVS and RGB when changing algorithmic hyper-parameters through a Bayesian optimization scheme
    \item Compare the operational characteristics of the OMS simulation algorithm against its biological counterpart to gain a more in-depth understanding of how representative it truly is
    \item Implement OMS on a proven DVS simulator such as v2e \cite{hu2021v2e}
    \item Create and incorporate other fundamental retinal computations within this framework to learn the trade-offs between them and how to incorporate multiple features into a holistic computer vision system
\end{enumerate}

% ----------------------------------------------
% - Notes: Introduction ------------------------
% ----------------------------------------------
% \section{Notes - Introduction}

% Event-based cameras

% Events are defined by the difference between image frames in the temporal dimension and thus sample visual information based on the scene dynamics, therefore events position themselves as a natural representation for acquiring motion. 

% Importantly, events are sparse in space and time, and enable an memory and energy efficient representation of spatiotemporal activity. While this not only improves the learning process by reducing image noise (CITE) it also enables a more practical solution to SWaP constrained requirements of embedded image processing systems, such as in self-driving cars (CITE).

% Distinguishing between events caused by moving objects and by the camera's ego-motion has presented a major challenge for the real-world application of event-based cameras. New methods have attempted to solve the ego-motion problem by \textit{doing this this and this}. While the corpus of engineered designs continues to grow, an elegant solution is hard-coded into our eyes designed by millions of years of evolution. This solution emerges from a computation performed in the neural circuitry of the animal retina--\textit{object motion sensitivity (OMS)}. 

% OMS is a fundamental computation produced by the animal visual system represented by the feature-spike activity of Retinal Ganglion Cells (RGCs). EXPLAIN HERE HOW IT WORKS SIMPLY. This activity is used downstream to discriminate the motion of objects (object motion) from motion caused by motion of the observer (ego-motion).


% ----------------------------------------------
% - Notes - Methodology
% ----------------------------------------------
% \section{Notes - Methodology}



% \href{https://kb.iu.edu/d/aetf#:~:text=A%20color's%20RGB%20value%20indicates,CSS%2C%20and%20other%20web%20standards.}{Intensity}

% \href{https://www.sciencedirect.com/science/article/pii/S1319157821001750}{Brightness difference as a mean instensity difference}

% (OMS ALGORITHM HERE)

% This algorithm makes use of (X number) of hyperparemeters which must be tuned towards the provided dataset in order to account for differences in REASON 1, REASON 2, etc.

% Bayesian optimization was used to tune the hyperparamters...where loss was determined through a differencing of OMS frames with the ground truth labels...

% Ground truth labels were obtained by manually filtering all static objects in each frame from the semantically segmented labels of the Berkley DeepDrive MultiObject Tracking and Segmentation (BDD100k MOTS20) dataset. This includes proper labeling of salient entities which changed state from static to in-motion between frames.

% DVS and OMS outputs were each tuned against the datasets to give the best possible results for each for direct comparison. They were evaluated on the following metrics....

% [END ROUGH OUTLINE]

% \section{Notes - References}

% Most information can be obtained from: https://arxiv.org/abs/1904.08405

% https://github.com/bdd100k/bdd100k

% https://arxiv.org/pdf/1906.07165.pdf

% https://arxiv.org/pdf/1811.12039.pdf

% https://arxiv.org/pdf/2005.08605.pdf \\<- 2020 is the latest actually but still doesnt have segmentation data

% https://arxiv.org/pdf/1805.04687.pdf

% https://www.biorxiv.org/content/10.1101/2022.08.14.503909v1.full.pdf

% https://arxiv.org/pdf/1903.07520.pdf

%https://openaccess.thecvf.com/content_CVPRW_2019/papers/EventVision/Calabrese_DHP19_Dynamic_Vision_Sensor_3D_Human_Pose_Dataset_CVPRW_2019_paper.pdf

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}


\end{document}
