% !TEX root = main.tex


\section{Methodology}

The process used to analyze the code quality of the repositories 
is depicted in \fref{fig:extraction}. We evaluated a data set of 24 \ac{RL} Python projects to test our 
hypothesis about their code quality.\footnote{\ac{RL} repositories data set: \url{https://doi.org/10.5281/zenodo.4584135}} 
The data set is split in two subsets to asses the code quality of projects with different characteristics 
in terms of size and developer expertise. Such division allows us to understand if the results 
of the code quality study are attributed to developers or the 
intrinsic complexity of \ac{RL}-based systems. The first 20 projects 
are extracted from open-source GitHub repositories. These correspond to 
the \ac{RL} repositories with the highest number of stars on GitHub 
that are maintained by general GitHub users. To obtain these, we used the GitHub API to filter  
active and popular repositories that implement the Q-learning 
algorithm. We manually filtered the repositories to assure the 
repositories are implemented applications, rather than project 
templates, and do implement the Q-learning algorithm. 
The remaining 4 projects correspond to the ACME 
repositories~\cite{hoffman20}, a research framework developed 
by \ac{RL} engineers,\footnote{\url{https://github.com/deepmind/acme/tree/master/examples}} 
used for the comparison with the \ac{RL} projects found in the wild. We then 
analyze the entire repositories' quality as a whole, using the metrics described in 
\fref{sec:quality}, following the thresholds defined in \fref{tab:thresholds}.

\begin{figure}[htpb]
	\centering
	\includegraphics[width=\columnwidth]{figures/extraction}
	\caption{Repository mining, selection, and analysis process}
	\label{fig:extraction}
\end{figure}

%Additionally we use SonarQube and Prospector on the repositories, to further analyze them with respect to their maintainability, testability, security, and flexibility.

%%
\section{Results and Analysis}
\label{sec:evaluation}

%%%%
\subsection{Results}

The results of our analysis (\fref{tab:qa-results}) show a 
considerable quantity of code smells detected for the 24 \ac{RL} 
projects. The number of code smells identified in the 20 projects 
gathered in the wild significantly differs from the number of code smells identified in the 
ACME projects. This first finding suggests that the projects developed by \ac{RL}
users, rather than by \ac{RL} engineers or developers of purpose could indeed impact negatively 
the quality of \ac{RL}-based systems, supporting our hypothesis.

The results in \fref{tab:qa-results} are ordered from the highest to 
the lowest number of detected code smells. Taking into account the 
first set of repositories, extracted from GitHub, the project with the 
highest number of code smells is Rl\_trading with 300 smells in 35 
files, and the one with the lowest number of code smells is 
Q-trader with 4 code smells in 4 files. This confirms that it is more 
likely for projects with a larger code base
to exhibit more code smells. However, on average, the percentage of 
code smells in the projects is $4.16\%$ (with standard deviation 
$\sigma^2=4.52\%$) of the code base. In proportion, the projects with 
the largest amount of code smells, relative to the code base size, is 
Deep-q-rl in which $22.63\%$ of the code base contains code smells.  
The results obtained from the ACME projects show the 
project with the highest number of code smells is Offline Agents with 
16 smells in 6 files. The project with the lowest number of code 
smells is Continuous control with 0 smells in 7 files. 
The percentage of code smells in the projects is on average $2.90\%$ 
(with standard deviation $\sigma^2=3.02\%$) of the code base. The 
the largest amount of code smells, relative to the code base size, is 
found on Continuous control where $7.14\%$ of the code base 
contains code smells.

We observe that the repositories extracted from GitHub contain 
significantly more code smells than those in the ACME framework. This 
suggest that the code generated by \ac{RL} engineers indeed has a better 
quality than that of the other repositories. Nonetheless, the 
proportion of code smells with respect to the code base is similar. 
This suggest that there is an intrinsic complexity in \ac{RL} 
algorithms that is currently not being addressed.

When we analyze the most recurrent code smells, we observe a similar 
trend between the data sets. The four most recurrent code smells detected 
in the extracted GitHub repositories are \acf{LM} 
(detected 288 times), \acf{LC} (217), \acf{MNC} 
(176), and \acf{LPL} (145).
Similarly, The four most common code smells for the Acme examples are 
\acf{MNC} (detected 8 times), \acf{LLF} (5), \acf{LM} 
(4), and \acf{LPL} (2).

The shared recurrent code smells between the extracted GitHub projects 
and the ACME framework projects again shades lights to an 
intrinsic complexity in the definition and interaction between 
entities of \ac{RL} algorithms that cannot be expressed at the 
appropriate abstraction level at the moment.

\begin{table*}[hptb]
  \centering
  \caption{Results from the quality analysis of the collected repositories}
  \input{tables/qa-results}
  \label{tab:qa-results}
\end{table*}


%%
\subsection{Analysis}

The ACME projects have a lower number of detected 
code smells (\ie 0.8 code smells per file) than the projects extracted 
from GitHub (\ie 3.15 code smells per file). This may be attributed to 
the fact that ACME projects are implemented by \ac{RL} engineers with the 
purpose of being reused by other developers, and therefore have a better 
code quality standard.

Nonetheless, three of the four most recurrent code smells (\ie 
\ac{MNC}, \ac{LM}, and \ac{LPL}) are shared between the two sets of 
projects. The nature of these code smells lies in how programming 
entities access and share information between them. The other common 
code smells \ac{LC} and \ac{LLF} are related to the definition of the 
program entities and their behavior. This finding suggests that there 
is an intrinsic complexity of the systems which is not being captured 
by the code. We observe that the single responsibility principle is 
not being respected according to the number of detected code smells of 
the type \ac{LPL}, \ac{LM}, \ac{LC}, \ac{LLF}. Additionally this 
suggest a low cohesion and a large coupling between program entities. 
The high occurrence of the \ac{MNC} code smell suggests that the 
representation of the state and actions are taking place in 
high-dimensionality structures that are difficult to understand, 
manipulate, and modify. For example, the code of the Deer project to 
accesses the agent's state (\fref{lst:python}) is cumbersome and 
difficult to understand (\eg a \ac{MNC} code smell).

\begin{python}[label={lst:python},
  caption={Deer project array traversing code example}]
  [(player.x_change == 0 and player.y_change
      == -20 and ((list(map(add,player.
      position[-1],[20, 0])) in player.
      position) or
  player.position[ -1][0] + 20 > (game.
      game_width-20)))
\end{python}
      
\ac{MNC} is one of the code smells appearing in most of the 
projects. If we take into account that the ACME projects are more 
concise and focused on the \ac{RL} use cases and applications, the 
fact that \ac{MNC} is the most frequent code smell on those examples, 
suggests that its occurrences are linked to the basic structure of 
\ac{RL} applications. This is reinforced by the fact that the files 
that tend to have this code smell are related to the definition and 
uses of environments, states, rewards, and transition vectors used in 
\ac{RL}. This can be observed for instance in the Deer project, which 
has the highest number of \ac{MNC}. \fref{tab:mnc} shows the \ac{MNC} 
code smells for each file of the Deer project. As can be seen, 29 of 
the 60 \ac{MNC} detected, are part of the environment files.

\begin{table}[hptb]
  \centering
  \caption{\ac{MNC} for individual files in the Deer project}
  \input{tables/mnc}
  \label{tab:mnc}
\end{table}

%%
\subsection{Threats to Validity}

As a preliminary study, a threat to external validity is on the 
generalization of our results from two perspectives. The selection of 
the repositories focuses on the popularity of the repositories, but 
other criteria, as forks or activity may yield a larger set of 
repositories with different characteristics. A similar conclusion can 
be reached from the ACME examples, which are just a small set of 
professional projects.
Another generalization bias may be on the type of projects selected. We keep the evaluated 
repositories to the use of Q-learning. Other \ac{RL} implementations 
(\eg SARSA) may also present different characteristics with respect to 
the code structure and identified smells.

The code smells detected, are the result of using experience-based 
thresholds defined by Python developers~\cite{chen18}. However, these 
thresholds could not be appropriate for the specific case of  
\ac{RL} projects due to the particular features of \ac{RL} itself, 
representing a construct validity threat. 


\endinput


These projects are used as templates for the main four groups of \ac{RL} projects: Offline Agents, Behavior Suite, Continuous control, and Discrete agents.
