% !TEX root = main.tex

\section{Road Map for Higher Quality \ac{RL}}
\label{sec:lang}

The results show a high presence of code smells hindering the quality 
of \ac{RL} projects. We identify two main research avenues to improve 
the quality of \ac{RL} projects. 

\paragraph*{\textbf{Static Analysis Tools}}
First, we recognize that it is necessary to create specific metrics 
and thresholds for \ac{RL} quality analysis. As observed from our 
analysis, the recurrent code smells persist in the two data sets. One 
possible reason for this commonality may be the complexity of \ac{RL} 
algorithms, which have different characteristics than OO 
algorithms. Existing metrics and thresholds may not correctly capture 
the characteristics of \ac{RL}. Therefore, specialized metrics and 
thresholds can offer a more faithful view of \ac{RL} projects quality. 
Additionally, the metrics should come with the associated  
static analysis tools. A full study of software engineering best 
practices with a catalog of \ac{RL}-specific metrics and thresholds is 
required.
  
  
\paragraph*{\textbf{Programming Languages}} 
We observed two characteristics of \ac{RL} projects that influence the presence of code smells. 
\begin{enumerate*}[label=(\arabic*)]
\item Definitions of functions and classes 
are often long in \ac{RL} projects. This affects the maintainability 
and testability of the projects. 
\item The current state of developing \ac{RL} systems relies in basic 
data structures abused to represent the many dimensions required 
to express the environment, actions, and goal of an agent.  
\end{enumerate*}
The definition of a specialized programming language can offer more 
appropriate abstractions for the interaction between the agents and 
the environment can make definitions more concrete.   
Additionally, new programming abstractions can open the possibility to 
capture the ever-changing conditions between the agents and the 
environment. A declarative definition of actions and rewards (\eg by means of a 
predicate or lambda function, to dictate new behavior) could be more easily 
modified at run time to open up the system flexibility. 
For example, such flexibility could open the possibility for lifelong learning~\cite{khetarpal22}, 
adaptation evolution, and agent's goal adaptation~\cite{cardozo21}.


\endinput

We argue that a programming language with the appropriate abstractions 
to express \ac{RL} algorithms could help us in reducing the code 
smells and simplifying programmers' task~\cite{sztwiertnia21}. 
Furthermore, having a flexible programming language for \ac{RL} could 
be helpful in opening new research avenues in \ac{RL} itself, for 
example enabling more flexible interactions between agents.

We now present the three major challenges and benefits to propose such a language.


We now present a first approximation of what such a programming language could entail. To implement our programming language we use Racket,\footnote{\url{http://racket-lang.org/}} as an extensible programming language geared towards the creation of \acp{DSL}.

Para la implementación del lenguaje de dominio específico en Racket que permita crear una solución para el problema planteado previamente, es necesario descomponer el lenguaje en varios elementos. Estos elementos son el reader, lexer, tokenizer, expander y el parser. Cada uno de estos elementos se encarga de una tarea específica en la interpretación del lenguaje y ejecución sobre Racket, estas tareas se explican a continuación.
 Reader: Una de las dos principales partes de un lenguaje implementado en Racket. El reader es el responsable de convertir el código fuente en S-expressions, elemento base de los programas en Racket. Este corre antes que la otra parte principal del lenguaje, el expander.[4]
 Expander: Parte del lenguaje implementado para proporcionar enlaces para el código. Esta parte también es la encargada de evaluar macros y producir un programa totalmente expandido. [4]
 Lexer: Una función auxiliar para un tokenizador que hace coincidir los caracteres de un archivo de origen con una serie de reglas que son similares a las expresiones regulares. [4]
 Tokenizer: Una función que consume un flujo de caracteres de código fuente y lo reduce a las unidades significativas más pequeñas, llamadas tokens, donde estos a su vez sirven como entrada para el parser. [4]
 Parser: Una función que toma un flujo de tokens y los empareja con una gramática para producir un árbol de análisis. [4]
Imagen 4. Composición de un lenguaje de dominio específico en Racket
4.3 Resultados
Como resultados, se obtiene una primera versión de un lenguaje de dominio específico que proporciona una solución para los problemas de reinforcement learning por medio de la técnica de Q Learning. Para la construcción del lenguaje, se adopta una implementación realizada a la técnica de Q Learning realizada en semestres previos [5]. Esta implementación proporciona la implementación de tareas necesarias para el entrenamiento del modelo. Entre estas tareas está obtener una acción, elegir una acción y ejecutar una iteración entre el agente y el ambiente. Así mismo, en esta primera versión del lenguaje, no se abarca la definición del agente y el ambiente. Esta tarea se realiza por medio del lenguaje e implementación existente en Racket.
A continuación, se muestra la implementación del problema de Mountain Car en el lenguaje de dominio específico propuesto.
#lang QL
   ***racket(define (act_velocity state action)
 (define new_vel (+ (cdr state) (- (* (+ action -1) 0.001) ( * (cos
(* 3 (car state))) 0.0025))))
  (set! new_vel (max new_vel -0.07))
 (set! new_vel (min new_vel 0.07))
(cons (car state) new_vel)
  )racket***
***racket(define (act_position state action)
  (define new_pos (+ (car state) (cdr state)))
 (define new_vel (cdr state))
(when (<= new_pos -1.2)
  (set! new_pos -1.2)
 (set! new_vel 0.0)
)
***racket(define (discretize_state state)
 (cons new_pos new_vel)
 )racket***
 ***racket(define (mountain_car_environment state action)
 (define new_state state)
(set! new_state (act_velocity new_state action)) (set! new_state (act_position new_state action))
  (list new_state -1 (>= (car new_state) 0.5)) ; State Reward Done
)racket***
  
  (cdr
(define space (cons (cons -1.2 0.6) (cons -0.07 0.07)))
 (define bucket_size (cons (/ (- (cdr (car space)) (car (car space)))
 10) (/
 (- (cdr (cdr space)) (car (cdr space))) 100)))
(define normalized_state (cons (- (car state) (car (car space))) (-
 state) (car (cdr space)))))
(cons (inexact->exact(round (/ (car normalized_state) (car
  bucket_size))))
 bucket_size)))))
(inexact->exact(round (/ (cdr normalized_state) (cdr
  )racket***
***racket(define (Q_handler Q state action [new_value (void)])
  (define disc_state (discretize_state state))
 (define action_Q (vector-ref (vector-ref Q (car disc_state)) (cdr
 disc_state)))
 (if (void? new_value)
 (vector-ref action_Q action)
 (begin
 (vector-set! action_Q action new_value)
 )
)racket***
Q )
  
***racket(define episodes 10000)racket***
***racket(define max_steps 200)racket***
***racket(define initial_state (cons -0.5 0))racket***
***racket(define epsilon 0.8)racket***
***racket(define epsilon_decay_value 0.00008)racket***
***racket(define learning_rate 0.2)racket***
***racket(define discount 0.9)racket***
***racket(define Q (build-vector 11
                        (lambda (i)
(build-vector 101 (lambda (j)
(make-vector 3
0))))))racket***
run_Q_Learning ( ***racket episodes racket*** ,
***racket max_steps racket*** ,
***racket initial_state racket*** ,
***racket mountain_car_environment racket*** ,
***racket Q racket*** ,
***racket Q_handler racket***,
0.8,
0.00008,
0.2,
0.9)