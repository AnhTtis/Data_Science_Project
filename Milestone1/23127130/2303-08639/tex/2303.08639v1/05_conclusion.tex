\section{Conclusion}

We introduced a method to generate human cinemagraphs from single RGB images. Our main contribution is a cyclic neural network that produces looping video clips. We demonstrated it is possible to train the network with synthetic data and generalize to real data. To do so, we propose working in the image normal space to close the gap between the different data distributions.

While generating plausible results, our method has some limitations. Intrinsic image decomposition which we use as a step to synthesize back RGB images is a challenging problem and often lacks the high-frequency texture details of the original garments, which are lost during the re-shading step. To address this limitation, in the future we would like to tailor a generic solution towards the type of fabric materials and textures that we are interested. Further, we would like to extend our setup to jointly optimize for normal estimation and motion prediction steps, in an end-to-end fashion. %Another interesting possibility is to use material and shape priors to regularize the intrinsic image decomposition and subsequent image generation from the predicted normal fields. 
Finally, a challenging next step would be to add movement on hair strands that can add significant realism to the cinemagraphs. Since hair simulator is far from being a solved problem, it may be worth rethinking the setup to directly learn a neural (hair) simulator from real video footage, thus closely bringing together research in neural simulators and conditional generative models.

\paragraph{Acknowledgements.} This work has been partially supported by the Spanish projects PID2019-105093GB-I00, TED2021-131317B-I00 and PDC2022-133305-I00 (MINECO/FEDER, UE) and CERCA Programme/Generalitat de Catalunya.) This work is partially supported by ICREA under the ICREA Academia programme. We also thank Aniruddha Mahapatra for his valuable assitance and guidance in the development of this work.