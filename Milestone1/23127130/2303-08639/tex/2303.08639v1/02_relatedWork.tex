\section{Related Work}
\subsection{Looping video generation} In this work, we are interested in synthesizing \emph{cinemagraph} style looping animations where only certain parts of a frame are in motion. A typical method for creating such looping clips is to leverage video as input. Many approaches exist that solve an optimization problem to identify segments and transition points in the input video that can be looped seamlessly~\cite{schodl2000video, Bhat2004,Agarwala2005, couture2011panoramic, Tompkin2011,bai2012selectively, Yeh2012, Liao2013, Liao2015}. While we focus on generating such a looping clip from a static single image, we use a video based method~\cite{Liao2015} to ensure our training data is looped properly.

In the context of animating a single image in a looping manner, one approach is to warp regions of the image using Fourier methods in a stochastic manner which amounts to displacing the original texture~\cite{Chuang2005}. Another approach is to transfer the phase patterns from an example video to the given input image~\cite{Prashnani2017}. Okabe et al.~\cite{Okabe2009} also transfer the motion patterns from an example video to an input image of a fluid. Specifically, they map the example video to a constant flow and residual layers, which represent the high frequency motion patterns that are not explained by warping a reference frame using constant flow. Such residual patterns are transferred to the input image. These methods work best for natural phenomena such as water and fire where flow-based texture displacement and warping result in plausible animation. Halperin et al.~\cite{halperin2021endless} present another approach to animating a single image by focusing on repeating patterns. While demonstrating impressive results, such a method is not suitable for our problem since the motion a garment undergoes blowing in the wind is fundamentally different than displacing repeating patterns.

With the recent success of deep learning methods, several learning based approaches have been proposed to create looping animations from single images. While Endo et al.~\cite{Endo2019} predict a flow map to warp the input images directly, Holynski et al.~\cite{Holynski_2021_CVPR} first generate a constant flow map directly from a single image and then warp image features using the generated flow map to synthesize the RGB frames. In a follow-up work, Mahapatra et al.~\cite{mahapatra2022controllable} extend this framework to provide additional control of the motion direction and region of the image to be animated. We compare our method to this state-of-the-art approach and show that the assumption of constant flow is not suitable for garment motion and leads to unsatisfactory results. Recently, Fan et al.~\cite{fan2022simulating} present a method to animate fluids in a still image. Their method uses an additional depth map estimation to generate a surface mesh for the fluid region and thus utilizes physically based simulation priors to predict a motion field. While our approach of incorporating a surface normal map representation is similar, we focus on very different types of motions in our work.



\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{img/pipeline.pdf}
    \caption{Given an input image (top, left) and its predicted surface normal map (bottom, left), we present a network that synthesizes a set of surface normals that resemble the effect of the garment blowing in the wind with a given direction. We ensure a looped animation by encoding the time $t$ with a cyclic positional encoding with respect to a predefined loop duration (150 frames in our experiments). We then synthesize the corresponding RGB images demonstrating plausible garment deformation using an intrinsic image decomposition technique.}
    \label{fig:pipeline}
\end{figure*}


\subsection{Animating single images} With the success of deep learning, several methods have been recently proposed  to animate a given image. One approach is based on using a driving video and focus on synthesizing specific type of content and motion such as time-lapse videos~\cite{cheng2020time,Logacheva_2020_ECCV}, facial and body animation~\cite{Siarohin_2019_NeurIPS,wang2022latent}. We compare our method to the most recent method of Wang et al.~\cite{wang2022latent} and show that it is not suitable to capture the subtle motions observed in a human cinemagraph.

Another line of work directly predicts video or future frames from a given single image~\cite{visualdynamics16,Li2018,Xiong_2018_CVPR,dtvnet} or a semantic map~\cite{pan2019video}. Dorkenwald et al.~\cite{dorkenwald2021stochastic} learn a generative model that encodes a latent residual representation and sample such 
latent code to synthesize a video from a given image. Many of these methods, however, synthesize multiple frames at the same time and hence operate only at low resolution without providing control. To address the latter challenge, Blatmann et al.~\cite{blattmann2021understanding, blattmann2021ipoke} enable the user to provide a poke that determines the final location of a sparse point in the input image. The resulting videos, however, are not looped in contrast to the type of animation we are interested in synthesizing.

Another interesting direction is to train a single image based generator~\cite{rottshaham2019singan}, which can then be utilized to generate animations by providing random walks of the appearance of the object of interest in the latent space. Arora et al.~\cite{arora2021singan} extend this approach to work with an input GIF. While impressive, such approaches do not provide the controllability we aim to achieve with our approach, however.

%\subsection{Video generation} Inspired by the recent success of generative models in high-quality image synthesis, several works have been proposed to extend this to the video generation task. 
%is an example of this. In this work, authors propose animating natural scenes by warping visual features encoded by a neural network, to be later decoded into their new location. Examples of natural scenes animated by this work are: rivers, waterfalls and other masses of water. The methodology takes as input an image, a mask with the image pixels to be animated and sparse cues of motion. Motion cues are extended through the given mask to generate a dense flow. This dense flow is later refined by a neural network to generate more plausible motion. Once the refined dense optical flow is obtained, it can be used to warp visual features encoded by another neural network. To this end, the input image is processed through the neural network, generating multi-scale feature maps. These maps are warped according to the dense optical flow (downsampled for image resolutions lower than the original). Finally, the warped features are decoded back into an image, effectively generating motion. The approach is designed to transition from an initial frame to a final frame, and also backwards, from the final frame to the initial frame. The final result is a smart blend of both transformations. During inference, to generate seamless loops, the initial and final frame are the same image, thus forcing the model to end at its starting point.
%
%For human cinemagraphs, warping based solutions are sub-optimal. This is due mainly to two reasons. First, warping based solutions assume a constant optical flow, extracted from the data. While this assumption works well for rivers and similar natural phenomena, it considerably breaks for cloth deformation which consists of mostly 3D geometric deformations. Second, warping solutions \textit{displaces} the appearance of the image. This works well with waterfalls, rivers, and similar natural phenomena where sense of motion can be achieved by just warping the appearance (\ie, texture). For cloth, this is not the case. For a realistic animation of cloth, wrinkles and folds need to be generated and deformed in 3D. Direct warping of features result in a displacement of the appearance of the cloth, not its underlying geometry.