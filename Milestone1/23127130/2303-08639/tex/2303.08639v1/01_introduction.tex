\section{Introduction}
\label{sec:intro}


%%%%%%% Cinemagraphs
Cinemagraph, a term originally coined by Jamie Beck and Kevin Burg, refers to adding dynamism to still images by adding \textit{minor and repeated movements}, forming a motion loop, to a still image. Such media format is both engaging and intriguing, as adding a simple and subtle motion can bring images to life. Creating such content, however, is challenging as it would require an artist to first set up and capture a suitable video, typically using a tripod, and then carefully mask out most of the movements in a post-processing stage.


%%% what are the challenges
We explore the problem of creating human cinemagraphs directly from a single RGB image of a person.  
%
Given a dataset of images and corresponding animated video pairs, a straightforward solution would be to train a fully supervised network to learn to map an input image to a plausible animated sequence. However, collecting such a dataset is extremely challenging and costly, as it would require capturing hundreds or thousands of videos of people holding a perfectly still pose under the influence of the wind from different known directions. While it is possible to simulate different wind force directions using oscillating fans in a lab setup~\cite{Bouman2013}, capturing the variability of garment geometry and appearance types in such a controlled setting is far from trivial. Hence, we explore the alternative approach of using synthetic data where different wind effects can easily be replicated using physically-based simulation. The challenge, then, is to close the synthetic-to-real gap, both in terms of garment dynamics and appearance variations.


%%% our method -- input/output/assumptions
We propose to address this generalization concern by operating in the gradient domain, i.e., using surface normal maps. Specifically, being robust to lighting or appearance variations, surface normals are arguably easier to generalize from synthetic to real, compared to RGB images. Moreover, surface normals are indicative of the underlying garment geometry (i.e., folds and wrinkles) and hence provide a suitable representation to synthesize geometric and resultant appearance variations~\cite{Lahner_2018_ECCV,zhang2021deep} as the garment interacts with the wind.


%%% technical details
Further, we make the following technical contributions. First, we propose a novel \emph{cyclic} neural network formulation that directly outputs looped videos, with target time periods, without suffering from any temporal jumps. Second, we demonstrate how to condition the model architecture using wind parameters (e.g., direction) to enable control at test time. Finally, we propose a normal-based shading approach that takes the intermediate normals under the target wind attributes to produce RGB image frames. In Figure~\ref{fig:teaser}, we show that our method is applicable to a variety of real test images of different clothing types.

%% evaluation + comaprison + summary
We evaluate our method on both synthetic and real images and discuss ablation results to evaluate the various design choices. We compare our approach against alternative approaches \cite{mahapatra2022controllable,wang2022latent} using various metrics as well as a user study to evaluate the plausibility of the generated methods. Our method achieves superior performance both in terms of quantitative metrics as well as the perceptual user study.

\if0
This document describes a methodology to generate human cinemagraphs from still RGB images. The definition of a cinemagraph is as follows: \textit{Cinemagraphs are still photographs in which a minor and repeated movement occurs, forming a video clip.} The methodology described in this document animates still images of humans. More specifically, it animates the clothes that are draped on the human body, as if waved by the wind, while keeping everything else still. Output cinemagraphs are short looped videos. Images processed with this technology have a more vivid look, making them more appealing and engaging for the user.

Given a suitable dataset of image and corresponding animated video pairs, one straightforward approach would be to train a fully supervised network to learn to map an input image to a plausible animated sequence. However, collecting such a dataset is extremely difficult due to various reasons. First of all, we would require to capture a person holding a still pose under the influence of a wind force with known direction. While it is possible to simulate different wind force directions using oscillating fans in a lab setup~\cite{}, capturing the variability of garment geometry and appearance types in such a controlled setting is far from trivial. Hence, an alternative approach is to utilize synthetic data where different wind force setups can easily be simulated using physically based simulation~\cite{}.
Nevertheless, capturing the variability of realistic garment appearances and lighting conditions that allow to generalize to real images remains as a challenge. We propose to address this challenge by operating on a different modality of surface normal maps. Being invariant to lighting or appearance features, surface normals are arguable easier to generalize from synthetic to real compared to RGB images~\cite{}. Moreover, surface normals are indicative of the underlying garment geometry and hence provide a suitable representation to synthesize folds and wrinkles~\cite{} as the garment interacts with the wind.
\fi

%-------------------------------------------------------------------------
%
%\section{Contributions}
%
%To achieve the goal of cloth animation for human cinemagraphs, we define the following contributions:
%\begin{itemize}
%    \item \textbf{Problem decomposition.} We decompose the problem of creating a cinemagraph based on a single image in three stages: (i) mapping the input RGB frame to a normal map, (ii) animating this normal map into a short looped clip, and (iii) generating the final RGB images which are consistent with the animated normals. This decomposition has various advantages. First of all, normal maps represent the geometry of the objects in a given image, where colors encode the direction of the normal vectors. %Normal vectors are perpendicular to the surface of the object at that given point. 
%    As a wind force is applied to a cloth, new wrinkles and folkds appear that change the 3D geometry of the cloth. Normal maps are a more explicit representation of this changing geometry. Hence, capturing the animation in the normal space helps to generate geometrically plausible cloth deformations. Second of all, this decomposition helps us to utilize synthetic data to train our method more efficiently. Clothing can have a wide range of textures, colors, and materials. Hence capturing real data or simulating synthetic data that can capture this variation is challenging. Additionally, in the RGB space there is often a big gap between synthetic and real data distribution making generalization hard. On the other hand, normal maps are invariant to appearance (cloth color, texture, illumination, etc.) and hence the gap between synthetic and real data in the normal map space is smaller. By decomposing our problem into three stages, we rely mostly on simulated synthetic data to animate normal maps. We propose reshading approach that synthesizes RGB pixels from the original image and animated normals. Hence, our method can be applied to real images.
%    \item \textbf{Cyclic Neural Network.} Cinemagraphs are by definition looped clips. To achieve this behavior, we design a network architecture with a cyclic behaviour with respect to a time parameter. Then, given a chosen cycle length of $T$, this formulation forces the network to generate the same output for $t = 0$, $t = T$, $t = 2T$ and so forth.
%    \item \textbf{Wind Control.} In this methodology, wind direction is a controllable parameter. Hence, the user can define a wind direction, and even change it in real time. This will generate motions consistent with the desired wind direction.
%    \item \textbf{Normal-based Reshading.} The final step of this methodology is to animate the input RGB image in a consistent way with respect to the animated normal maps. We achieve this by modeling the relative change in the shading layer based on the normal maps.
%    \item \textbf{Synthetic dataset.} Real data gathering for this problem is challenging due to the requirements: completely still human bodies and known wind direction. Synthetic data can be obtained with a cloth simulator and a rendering engine. This additionally allows full control and annotation of the wind.
%\end{itemize}