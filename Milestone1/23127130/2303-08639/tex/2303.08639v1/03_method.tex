\section{Methodology}

\begin{figure*}
\centering
    \begin{overpic}[width=.8\textwidth]{img/architecture.pdf}
    \put(16,31){
      \color{atomictangerine} $\Delta t$
      }
      \put(16,4){
      \color{applegreen} $\bm{w}(w_x,w_y)$
      }
        \put(43,30.5){
      \color{black} $\bm{x}_{64}$
      }
       \put(71,35){
      \color{black} $+\bm{x}_{64}$
      }
      \put(43,23.5){
      \color{black} $\bm{x}_{128}$
      }
      \put(71,31){
      \color{black} $+\bm{x}_{128}$
      }
      \put(43,16.5){
      \color{black} $\bm{x}_{256}$
      }
      \put(71,27){
      \color{black} $+\bm{x}_{256}$
      }
      \put(43,10){
      \color{black} $\bm{x}_{512}$
      }
       \put(71,23){
      \color{black} $+\bm{x}_{512}$
      }
      \put(43,4){
      \color{black} $\bm{x}_{1024}$
      }
      \put(71,10){
      \color{black} $+\bm{x}_{1024}$
      }

      \put(49,15){
      \color{black} $\bm{N}_t$
      }
      \put(97,15){
      \color{black} $\bm{N}_{t+\Delta t}$
      }
      
    \end{overpic}
    \caption{Cyclic wind-conditioned UNet. Given an input normal map $N_t$, a delta time increment $\Delta t$, and a wind direction $\bm{w}$; we extend the standard UNet architecture to give it a cyclic behaviour. We encode the time using a cylic positional encoding and concatenate with the wind direction. We pass the concatenated features through different fully convolutional layers to extract features of varying dimensions. The resulting features are provided as skip connections to the UNet architecture which synthesizes the final normal map $N_{t+\Delta t}$.}
    \label{fig:architecture}
\end{figure*}

Given a single input RGB image of a person, $\bm{I}\in\mathbb{R}^{W\times H\times3}$, our goal is to generate a looped video sequence, $\bm{V} := \{\bm{I}_0, \bm{I}_1, ..., \bm{I}_t | \bm{I}_0 = \bm{I}_t\}$, where the loose garments worn by the person exhibit a plausible motion as if blown in the wind. We assume the direction of the wind can be provided by a unit vector $\bm{w}$ in the image plane to control the output animation. Hence, our goal is to learn the mapping $\mathcal{F}(\bm{I},\bm{w}) \longrightarrow \bm{V}^{\bm{w}}$. %Hence, our goal is to learn the mapping $\mathcal{F}(\bm{I},\bm{w}) := \bm{V}^{\bm{w}}$. %$\mathcal{F} : \bm{I}\rightarrow\bm{V}$. Furthermore, given a wind direction parameterization $\bm{w}$ as a unit vector, our methodology is able to generate cinemagraphs consistent with $\bm{w}$ as $\mathcal{F}(\bm{I},\bm{w}) = \bm{V}_{\bm{w}}$.

%Our goal is to generate human cinemagraphs from still images by animating the cloth as if it were under the wind. To this end, we develop a multi-stage approach. In the first stage, a normal map is obtained from the input RGB image. Next, in the second stage, we use a neural network to animate the normal map and generate a video of normal maps. Finally, we go back to RGB domain by re-shading the input image consistently with the animated normal maps. In this section we explain the problem setup and the different stages of our pipeline in detail.
%
%\subsection{Problem definition}
%Given a suitable dataset of (\bm{I}, \bm{V}) pairs, one straightforward approach would be to train a fully supervised network to learn $\mathcal{F}$. However, collecting such a dataset is extremely difficult due to various reasons. First of all, we would require to capture a person holding a still pose under the influence of a wind force with known direction. While it is possible to simulate different wind force directions using oscillating fans in a lab setup~\cite{}, capturing the variability of garment geometry and appearance types in such a controlled setting is far from trivial. Hence, an alternative approach is to utilize synthetic data where different wind force setups can easily be simulated using physically based simulation~\cite{}. Nevertheless, capturing the variability of realistic garment appearances and lighting conditions that allow to generalize to real images remains as a challenge. We propose to address this challenge by operating on a different modality of surface normal maps. Being invariant to lighting or appearance features, surface normals are arguable easier to generalize from synthetic to real compared to RGB images~\cite{}. Moreover, surface normals are indicative of the underlying garment geometry and hence provide a suitable representation to synthesize folds and wrinkles~\cite{} as the garment interacts with the wind.

%On one hand, gathering the necessary data to train in this manner is challenging and costly. It would require capturing in-the-wild videos where the wind generates motion in the cloth while the person is completely still during few seconds, while assuming a constant known wind. On the other hand, the model would need to learn a large variability of lighting conditions and human and cloth appearance. Instead, we propose solving the problem in the normal space. Working with normal maps allows using synthetically generated data without introducing a large domain gap (albeit still there is). Being able to generate data in this way heavily simplifies the data gathering task. Then, normal maps are completely invariant to lighting conditions or appearance, which largely eases the learning task. There is an additional reason to work with normal maps. Cloth motion will appear mainly as moving wrinkles and folds. These are perceived by us through changes in the way light interacts with the cloth. This, in turn, depends on the geometric configuration of the cloth. Normal maps describe the 3D geometry of the garments in the image. Therefore, there is a strong correlation between perceived cloth motion and normal maps. Then, motivated by all of the exposed, we develop our three stage approach: 1) from RGB to normal space, 2) generate normals cinemagraph and 3) going back to RGB space.

To more effectively represent the underlying garment geometry and the changes it undergoes due to the wind force, our method operates on the surface normal map $\bm{N}$ that corresponds to the input image $\bm{I}$. Specifically, given an input image $\bm{I}$, we first predict the surface normal map using an off-the-shelf normal estimator~\cite{Bae2021}. We then propose a novel cyclic network architecture that maps $\bm{N}$ to a sequence of normal maps $\bm{V_N}^{\bm{w}} := \{\bm{N}_0, \bm{N}_1, ..., \bm{N}_t | \bm{N}_0 = \bm{N}_t\}$ that demonstrate plausible motion of the underlying garment under the influence of a wind force with a direction given by $\bm{w}$. Finally, we synthesize back the corresponding RGB images given the original input image and the sequence of animated normal maps using a constrained reshading approach. We provide the overall pipeline in Figure~\ref{fig:pipeline} and next discuss the details of our approach. 

%\subsection{Stage 1: RGB to Normal map}
%
%This stage consists of the mapping of RGB images $\bm{I}$ into normal maps $\bm{N}\in\mathbb{R}^{W\times H\times3}$. To this end, we use an off-the-shelf normal estimator for human images... \textcolor{red}{[CAN WE EXPLAIN Adobe's NormalLab HERE? DO WE CITE A DIFFERENT WORK?]}

\subsection{Cyclic and Controllable Animation}

Given an input normal map $\bm{N}$ and a wind direction $\bm{w}$, our goal is to learn the mapping $\mathcal{F_N}(\bm{N},\bm{w}) \longrightarrow \bm{V_N}^{\bm{w}} = \{\bm{N}_0, \bm{N}_1, ..., \bm{N}_t | \bm{N}_0 = \bm{N}_t\}$ where $\bm{V_N}^{\bm{w}}$ demonstrates plausible garment animation. Our goal is to synthesize a cyclic animation sequence with a predefined period of $T$, so that $t=T-1$. This amounts to synthesizing normal maps that satisfy constraints $\bm{N}_t = \bm{N}_{t+kT}$ $\forall k\in\mathbb{Z}$.

We tackle this problem as an image-to-image translation task where our goal is to learn $f(\bm{N}_t, \Delta t, \bm{w}) \longrightarrow \bm{N}_{t+\Delta t}$ where $\Delta t \in [-T/2, T/2]$. Note that, since we are interested in looped animations, negative values for $\Delta t$ correspond to valid animation samples. We realize the function $f$ as a UNet architecture that is conditioned on both the residual time $\Delta t$ and the wind direction $\bm{w}$, as shown in Figure~\ref{fig:architecture}. To enforce a cyclic behaviour, we first encode $\Delta t$ using sinusoidal functions as:
\begin{equation}
    \label{eq:time_encoding}
    \begin{split}
        \varphi_{\Delta t} &= \frac{2\pi n}{T}\Delta t,  \quad n = 1, 2, 3, 4, 5.\\
        \bm{x}_{\Delta t} &= \{\text{cos}(\varphi_{\Delta t}), \text{sin}(\varphi_{\Delta t})\}.      
    \end{split}
\end{equation}
%
This formulation ensures that $f(\bm{N}_t, \Delta t + kT, \bm{w})$ with $k\in\mathbb{Z}$ gives the same output resulting in a looping animation sequence. Similar to common practice in positional encoding~\cite{vaswani2017attention}, we observe that using multiples of the data frequency ($\omega = 2\pi n/T$) helps to learn higher frequency motions while still enforcing a global cyclic behaviour with period $T$. Note then how the time encoding $\bm{x}_{\Delta t}$ consists of multiple circumferences parameterized by $\Delta t$. 

We represent the wind direction as a unit vector as $\bm{w}$ in the image plane. We concatenate $\bm{w}$ with $\bm{x}_{\Delta t}$ resulting in the final conditioning code $\bm{x} := \bm{x}_{\Delta t} \| \bm{w} = (x_{\Delta t,0}, x_{\Delta t,1}, ..., x_{\Delta t,2n}, w_x, w_y)$. We condition the UNet by introducing $\bm{x}$ at each feature map extracted by the encoder at different scales. To do so, we first linearly transform $\bm{x}$ to the corresponding feature map dimensionality with learnable weights $\{\bm{W}_i\in\mathbb{R}^{F_i\times D}\}$, where $F_i$ is the number of channels of the $i$-th feature map and $D$ is the dimensionality of $\bm{x}$. We apply $1\times 1$ conovolutions to the feature maps before and after combining them with $\bm{x}$.
%
%As it is common, the encoder of the UNet extracts feature maps at different scales to later decode them into the final prediction. We combine these feature maps, prior to the decoding, with $\Delta t$ and $\bm{w}$. 

%\textcolor{red}{[ASSUMING THE MASK HELPS]\\
%In order to ease the task of the network to focus on animating only regions of interest, we optionally make use of an additional mask $\bm{M}\in\mathbb{R}^{W\times H}$ extracted or provided along with the input RGB image $\bm{I}$. $\bm{M}$ is a binary mask where white pixels denote which parts of the input image and the garment should be animated. We concatenate this mask with the input normal map and provide a 4-channel input to our network. Given, the surface normal map $\bn{N_t}$ predicted by the network at time $t$, we obtain a final surface normal map by blending the predicted and original surface normal maps, $\bm{\hat{N}_{t}} = \bm{M}\bm{N_t} + (1 - \bm{M})\bm{N}$. Hence, the final animated surface normal sequence becomes $\bm{\hat{V}_N}^{\bm{w}} = \{\bm{\hat{N}}_0, \bm{\hat{N}}_1, ..., \bm{\hat{N}}_t | \bm{\hat{N}}_0 = \bm{\hat{N}}_t\}$. Please note that our method works in cases where no mask is provided which is equiavalent to having an all-white mask. In this case, the method animates the loose regions of the garments which are most likely to be animated due to the wind force. The network implicitly learns this behaviour from the training data. %This also alleviates the complexity of the learning task for the network, as it does not need to discriminate body from cloth while learning to animate normals.}

%At test time, we provide a normal map estimation $\bm{\hat{N}}$ obtained from our input RGB image $\bm{I}$ during stage 1 of our pipeline. Then, we predict our normal map video with the desired wind direction $\bm{w}$ as $\bm{\hat{V}}^N = f(\bm{\hat{N}}, \bm{\Delta t}, \bm{w})$ with $\bm{\Delta t} = \{i/\nu| 0\leq i<\nu T\}$ where $\nu$ is the desired frames per second of the output video $\bm{\hat{V}}^N$. Predicted frames are independent of each other and thus they can be efficiently parallelized. Thanks for the network formulation, it is guaranteed that $\bm{\hat{V}}^N$ will be seamlessly looped with a cycle period of $T$.

\subsection{Normal Guided Synthesis}
\label{sec:reshading}

The final stage of our approach focuses on computing the final cinemagraph $\bm{V}$ given the original input RGB image $\bm{I}$ and the predicted normal map sequence $\bm{V_N}^{\bm{w}}$. To this end, we rely on the concept of intrinsic image decomposition where we assume images can be decomposed into two layers $\bm{I} = \bm{S}\bm{R}$: (i) the reflectance $\bm{R}\in\mathbb{R}^{W\times H\times3}$, which denotes the albedo invariant color of the materials, and (ii) the shading $\bm{S}\in\mathbb{R}^{W\times H}$ which is the result of the interaction of the light with the underlying geometry of the garment. In particular, the shading layer is crucial in how we perceive the changes in the fold and wrinkle patterns of the garment as it is animated. Given this observation, we synthesize a new shading layer that is consistent with the animated surface normal maps. Then, when composed with the original reflectance map reflects it generates the intended animation.
%
%Formally:
%\begin{equation}
%    \bm{I} = \bm{S}\bm{R}
%    \label{eq:iid}
%\end{equation}
%
%Given the input image $\bm{I}$, we first our proposed solution consists of obtaining first an estimation of the reflectance $\bm{\hat{R}}$, which can be later used to compute the shading layer $\bm{\hat{S}}$. This, combined with the estimated normal map $\bm{\hat{N}}$ allows inferring the lighting conditions of the image. Once we have the reflectance $\bm{\hat{R}}$, the normal map video $\bm{\hat{V}}^N$ and the light $\bm{\hat{L}}$, it is possible to compute a shading \textit{video} as $\bm{V}^S$ and use it to compute $\bm{\hat{V}}$ through Eq.~\ref{eq:iid}.

Given the input image $\bm{I}$, we first run an off-the-shelf intrinsic image decomposition method~\cite{bell14intrinsic} to obtain the reflectance map $\bm{R}$ and the shading map $\bm{S}$. Assuming a simple lighting model composed of a directional and ambient light, we optimize for the light parameters using the predicted surface normal map from the input image:
\begin{equation}
    \bm{S} = \text{max}(0, -\bm{N}\bm{l}) + \delta,
    \label{eq:shading}
\end{equation}

where $\bm{l}\in\mathbb{R}^3$ is the light direction and $\delta\in\mathbb{R}^+$ is the ambient light. Given the predicted animated surface normal map sequence $\bm{\hat{V}_N}$, we generate a new shading map sequence and composite it with the original reflectance map $\bm{R}$ to obtain the final RGB sequence $\bm{\hat{V}}$. At inference time, the user is required to provide a mask to denote the region of interest where motion is desired to be synthesized. Hence, we composite the original image and the synthesized RGB images based on this mask to provide the final output. While this approach changes only the shading without actually warping the texture of the garment, it is sufficient to provide the perception of a plausible animation.

\vspace{-.625cm}
\textcolor{black}{
\paragraph{Local vs Global.} We design this methodology so it leans towards a local solution. The reasons for this are as follows. On one hand, cinemegraphs are characterized by subtle motions (local). On the other hand, \textit{local} solutions generalize better, which is specially important for our approach to handle real test samples from a synthetic training set.
}

%\paragraph{Reflectance estimation.} Intrinsic image decomposition is a complex problem without a closed solution. Nonetheless, we constrain our problem to cloth pixels only, which has a significantly reduced variability compared to in-the-wild unconstrained images. Additionally, a semantic image segmentation can ease the task, when available. We propose a simple iterative algorithm. First, we cluster cloth pixel colors to have an initial rough estimation of the reflectance. Next, we use the reflectance to compute a per-channel shading layer, which we average into a $1$-channel shading. Then, we use this shading layer to compute a new estimation for the reflectance. Finally, using the previous pixel clustering, we average the color of each cluster and assign it to its pixels. We repeat this process a few times to get our reflectance estimation $\bm{\hat{R}}$.
%
%\paragraph{Light estimation.} We assume a simple lighting model composed of a directional light and an ambient light. Then the shading layer can be obtained as a function of the normal map and lighting conditions as:
%\begin{equation}
%    \bm{S} = \text{max}(0, -\bm{N}\bm{l}) + \delta,
%    \label{eq:shading}
%\end{equation}
%where $\bm{l}\in\mathbb{R}^3$ is the light direction and $\delta\in\mathbb{R}^+$ is the ambient light. From the previous process we also obtain an estimation for the shading layer $\bm{\hat{S}}$.  
%
%This methodology consists of three different stages. From an input RGB frame, the first stage consists on normal map estimation. We do not develope specific tools for this stage, we use already existing methodologies. Next stage is normal map animation. From the estimated normal map, the model creates a short looped clip of normal maps with plausible cloth dynamics. The final stage generates an RGB video by reshading the input RGB frame consistently with the normal map clip. This pipeline is depicted in Fig.~\ref{fig:pipeline}.
%
%The key part of this methodology is the prediction of short looped normal map clips. To this end, we propose a novel neural network architecture by extending UNet\footnote{UNet figure derived from the figure found in https://github.com/milesial/Pytorch-UNet}. The network is fed with an input normal map and generates the normal map at time $t$ by using this time as a parameter. The feature maps obtained by the encoder (UNet first half) are conditioned to a cyclic encoding of $t$. This forces the network to generate identical predictions for any two time instants $t_i$ and $t_j$ such that $\Delta t_{ij} = t_i - t_j = kT$ for some integer $k$ and the chosen cycle length $T$. Following a similar fashion, the network is also conditioned to the wind direction, represented as a unit length vector. As explained, during inference, this allows to control the wind direction.
%
%The model is trained with synthetic data generated for this specific task. Gathering real data for this problem is a challenging task because it requires completely still persons and annotated wind data. Computer graphics simulations allows easy capturing and complete annotations. Then, a cloth simulator computes the dynamics of garments draped around humans under the wind. This is done for different body poses and shapes, garments and wind directions. The simulation output is later rendered into RGB images and normal maps. The final data then consists on RGB and normal map frames annotated with the wind direction.
%
%The final stage, re-shading, is based on Intrinsic Image Decomposition. That is, given an RGB image $I\in\mathbf{R}^{H\times W\times 3}$, it can be decomposed into albedo as $A\in\mathbf{R}^{H\times W\times 3}$ that represents the objects colors and a shading layer $S\in\mathbf{R}^{H\times W}$ that describes the intensity of the colors based on illumination. Then, $I = S\cdot A$. The interaction between objects and illumination depends on the 3D geometry of the scene. This is represented by its normal map. We modify the shading layer consistently with each of the frames of the normal map video obtained in the previous stage. This process changes the pixel intensities in $I$ to generate a looped RGB video as a human cinemagraph.