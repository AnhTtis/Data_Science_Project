\section{Experiments}

In the following section, we describe the experimental setup and the qualitative and quantitative results. We detail the data used for training and evaluation, define the metrics, and briefly introduce the state-of-the-art baselines used for comparison. Finally, we provide a discussion of the results.


\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{img/syntheticData.pdf}
    \caption{We train ours on a synthetic dataset that consists of different garment types draped on bodies with varying shape and poses acquired from the Cloth3D dataset~\cite{Bertiche2020}. We simulate the effect of wind and render  the corresponding RGB and surface normal images.}
    \label{fig:synthetic}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{.24\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/realCapture/0.png}
    \end{subfigure}
    \begin{subfigure}{.24\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/realCapture/1.png}
    \end{subfigure}
    \begin{subfigure}{.24\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/realCapture/2.png}
    \end{subfigure}
    \begin{subfigure}{.24\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/realCapture/3.png}
    \end{subfigure}
    \begin{subfigure}{.24\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/realCapture/4.png}
    \end{subfigure}
    \begin{subfigure}{.24\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/realCapture/5.png}
    \end{subfigure}
    \begin{subfigure}{.24\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/realCapture/6.png}
    \end{subfigure}
    \begin{subfigure}{.24\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/realCapture/7.png}
    \end{subfigure}

    \caption{We capture a small real dataset where the subject keeps a still pose during the sequence while a fan generates wind. Different garment types show different dynamics.}
    \label{fig:real_capture} 
\end{figure}


\paragraph{Datasets.}
In order to train our network, we generate a synthetic dataset that consists of different type of garments draped on human bodies with varying shape and pose. Specifically, we sample human body and garment pairs from the Cloth3D dataset~\cite{Bertiche2020}, which is a large-scale dataset of clothed 3D humans. We select $1500$ samples with skirt and dresses and $500$ samples with other clothing types (e.g., trousers, tshirts). Each sample in the original Cloth 3D dataset is a motion sequence. We randomly choose one of the frames in each sequence as a random human body pose. The chosen frame, body and outfit, defines the initial conditions of our cloth simulation. We use Blender\cite{blender} to run the simulations. To this end, we choose a random wind direction in the image plane with constant wind force, and simulate the cloth dynamics while the underlying body remains still. Each simulation output is rendered from a fixed viewpoint with a predefined lighting setup. We apply random checkerboard texture patterns to some garments and assign a uniform color material to others. In addition to RGB output, we also render the corresponding surface normal maps and segmentation masks (body, cloth and background). Figure~\ref{fig:synthetic} shows examples from our dataset. We simulate each sample for $250$ frames at $30$ fps. We observe that the garment drapes on the body in roughly the first $50$ frames of the sequence and later starts blowing in the wind. It is not trivial to guarantee the resulting garment animation is cyclic in such a physically based simulation setup. Hence, we process the resulting animations with the method of Liao et al.~\cite{Liao2015} which detects loops in an input video. After this step, we obtain animation sequences of length $150$ frames which we use as the duration of loops, i.e., $T=150$.

In addition to synthetic data, we test our method on real samples from the Deep Fashion dataset~\cite{liuLQWTcvpr16DeepFashion} as well as additional stock images to test generalization. To evaluate if the predictions obtained on real samples contain plausible cloth dynamics, we capture a small set of real examples. Specifically, we ask a human subject wearing different types of garments to hold a still pose next to an oscillating fan while we record a short video sequence with a fixed camera mounted on a tripod. We record 50 such videos demonstrating 8 different outfit types. Similar to synthetic data, we process each video with the method of Liao et al.~\cite{Liao2015} to obtain looped animations. Figure~\ref{fig:real_capture} shows some real samples.

\paragraph{Evaluation Metrics.}
%
We evaluate our method and baselines on synthetic data where we can access ground truth image and animation pairs. First, we adopt metrics that focus on pixel-level similarity. Specifically, we report per-pixel mean average error~(MAE), mean squared error~(MSE), root of mean squared error~(RMSE), and PSNR. In addition we report metrics that focus on more structural (SSIM~\cite{wang2004image}) and perceptual similarities (LPIPS~\cite{zhang2018perceptual}). For DeepFashion samples we do not have ground truth video data. Hence, in order to evaluate the plausibility of the generated animated sequences we use Frechet Video Distance~(FVD \cite{unterthiner2018towards}) against the real data we have captured.

%\textcolor{red}{[MENTION USER STUDY]}

\paragraph{Baselines.}
We compare our method to two baselines. First, we compare with the work of Mahapatra et al.~\cite{mahapatra2022controllable}, which extends the original Eulerian motion fields approach~\cite{holynski2021animating} to a controllable setup. Since this method is a flow based approach and uses optical flow information to be provided in the dataset, we train it with the looped RGB videos in our synthetic dataset where optical flow can be more reliably estimated using off-the-shelf methods~\cite{teed2020raft}. For each looped sequence, we extract a mask denoting the region where motion is observed and a sparse set of motion directions from the estimated optical flow. We also compare our method to LIA~\cite{wang2022latent}, a state-of-the-art single image-based controllable video generation framework. Since LIA requires a target video sequence to specify the desired animation, we provide the ground truth animation sequences as targets both during training and testing. While it is not possible to use this configuration in a real setup, it provides the best possible results. Outperforming LIA under this configuration means outperforming it under any other.


\begin{figure}[!ht]
    \centering
    \rotatebox{90}{\quad LIA\cite{wang2022latent}}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/LIA_0.png}
    \end{subfigure}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/LIA_1.png}
    \end{subfigure}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/LIA_2.png}
    \end{subfigure}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/LIA_3.png}
    \end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \\
    \hspace{-.5cm}
    \rotatebox{90}{Controllable}
    \rotatebox{90}{\hspace{.075cm}Anim.\cite{mahapatra2022controllable}}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/Kuldeep_0_0.png}
    \end{subfigure}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/Kuldeep_0_1.png}
    \end{subfigure}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/Kuldeep_0_2.png}
    \end{subfigure}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/Kuldeep_0_3.png}
    \end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \\
    \rotatebox{90}{\quad\quad RGB}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/RGB_0.png}
    \end{subfigure}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/RGB_1.png}
    \end{subfigure}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/RGB_2.png}
    \end{subfigure}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/RGB_3.png}
    \end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \\
    \rotatebox{90}{\quad Normals}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/normals_0.png}
    \end{subfigure}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/normals_1.png}
    \end{subfigure}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/normals_2.png}
    \end{subfigure}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/normals_3.png}
    \end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \\
    \hspace{-.5cm}
    \rotatebox{90}{Estimated}
    \rotatebox{90}{\hspace{.0375cm} Normals}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/xnormals_0.png}
    \end{subfigure}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/xnormals_1.png}
    \end{subfigure}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/xnormals_2.png}
    \end{subfigure}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/xnormals_3.png}
    \end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \\
    \rotatebox{90}{\quad\quad GT}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/GT_0.png}
    \end{subfigure}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/GT_1.png}
    \end{subfigure}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/GT_2.png}
    \end{subfigure}
    \begin{subfigure}{.22\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/ablation/GT_3.png}
    \end{subfigure}
    \caption{Ablation study. We evaluate qualitatively the image synthesis capacity of each methodology in our synthetic dataset, namely the state-of-the-art baselines LIA\cite{wang2022latent} and the work on controllable animations of \cite{mahapatra2022controllable} and the different variations of our approach, trained on different data modalities. Normal-based solutions require a re-shading step (Sec.~\ref{sec:reshading}). We observe SOTA solutions (warping-based) are sub-optimal for our setting. Ours is able to generate consistent images with plausible wrinkles.}
    \label{fig:ablation}
\end{figure}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}{.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/generalization/kuldeep_000.png}
    \end{subfigure}
    \begin{subfigure}{.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/generalization/RGB_000.png}
    \end{subfigure}
    \begin{subfigure}{.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/generalization/normals_albedo2014_000.png}
    \end{subfigure}
    \begin{subfigure}{.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/generalization/xnormals_albedo2014_000.png}
    \end{subfigure}
    \begin{subfigure}{.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/generalization/GT_000.png}
    \end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}{.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/generalization/kuldeep_001.png}
    \end{subfigure}
    \begin{subfigure}{.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/generalization/RGB_001.png}
    \end{subfigure}
    \begin{subfigure}{.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/generalization/normals_albedo2014_001.png}
    \end{subfigure}
    \begin{subfigure}{.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/generalization/xnormals_albedo2014_001.png}
    \end{subfigure}
    \begin{subfigure}{.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/generalization/GT_001.png}
    \end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}{.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/generalization/kuldeep_002.png}
        \caption{Contr. Anim.\cite{mahapatra2022controllable}}
    \end{subfigure}
    \begin{subfigure}{.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/generalization/RGB_002.png}
        \caption{RGB}
    \end{subfigure}
    \begin{subfigure}{.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/generalization/normals_albedo2014_002.png}
        \caption{Normals}
    \end{subfigure}
    \begin{subfigure}{.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/generalization/xnormals_albedo2014_002.png}
        \caption{Estimated Normals}
    \end{subfigure}
    \begin{subfigure}{.19\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/generalization/GT_002.png}
        \caption{Input}
    \end{subfigure}
    \caption{We analyze the behaviour of each model during generalization using real samples from DeepFashion\cite{liuLQWTcvpr16DeepFashion}. As can be seen, the work of \cite{mahapatra2022controllable} shows color artifacts. Then, our model shows a tradeoff between reconstruction fidelity and wrinkle generation. The RGB solution generates images where color is faithfully maintained, but shows no wrinkles or motion except for very few specific samples. On the other hand, normal-based solutions are not always able to generate the same color distribution due to limitations in the re-shading algorithm (Sec.~\ref{sec:reshading}). Additionally, we observe only the model trained on estimated normals is able to generalize properly to real samples. We omit LIA\cite{wang2022latent} from this comparison as it is unable to generalize at all to real samples.}
    \label{fig:generalization}
    \vspace{-.35cm}
\end{figure*}

\begin{table}[b!]
\renewcommand{\tabcolsep}{0.065cm}
\footnotesize
\centering
\begin{tabular}{lccccccc}
\toprule
 & \multicolumn{5}{c}{Synthetic Data} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{Real Data} \\
Experiment & MAE & MSE & RMSE & SSIM & PSNR & Perceptual & FVD \\
\midrule
LIA\cite{wang2022latent} & 23.77 & 1131.62 & 32.45 & 0.11 & 25.50 & 400.26 & 873.86 \\
Controllable\\ Anim.\cite{mahapatra2022controllable} & 9.37 & 302.26 & 15.89 & 0.48 & 34.49 & \textbf{220.86} & 625.45 \\
\midrule
 & & & & & & & \\
\midrule
Toy & \textbf{6.52} & 197.73 & 12.36 & \textbf{0.61} & \textbf{40.76} & \textbf{218.39} & 643.42 \\
RGB & 7.64 & 175.24 & \textbf{12.21} & \textbf{0.56} & \textbf{37.64} & 230.40 & 623.32 \\
Normals & 15.75 & 563.76 & 22.63 & 0.41 & 32.11 & 231.29 & 633.12 \\
Estimated\\ normals & 17.32 & 674.62 & 24.31 & 0.34 & 31.81 & 231.74 & \textbf{613.26} \\
\bottomrule
\end{tabular}
\caption{Quantitative evaluation of the different methods tested including state-of-the-art baselines, LIA\cite{wang2022latent} and controllable animations\cite{mahapatra2022controllable}, and the different variants of our method trained using different data modalities. Normal-based solutions require a re-shading step (Sec.~\ref{sec:reshading})}
\label{tab:ablation}
\end{table}




\paragraph{Ablation.}
To analyze the effectiveness of the design choices, we test additional baselines which are variants of our method. On one hand, we train our CycleNet using RGB data directly, as opposed to the proposed pipeline in which we operate in the normal space. Next, we analyze the performance of the proposed methodology trained using ground truth normal maps. Nonetheless, there is still a domain gap between synthetic normal maps and normal maps estimated from real data. For this reason, we also train our method using normal maps estimated from synthetic RGB data as input, while using ground truth normal maps as labels. Since our eventual goal is to generate cinemagraphs from real images, we also assess the effectiveness of each approach on real data. Finally, we add as a reference, a \textit{Toy} baseline, in which we \textit{generate} the output cinemagraph $\bm{V}$ for a given input image $\bm{I}$ as $\bm{V} = \{\bm{I}, \bm{I}, ..., \bm{I}\}$. This baseline generates high quality videos since it uses the original image but lacks any motion. This baseline  evaluates how informative the different metrics we use are for our task. 

Tab.~\ref{tab:ablation} shows the quantitative evaluation of the different baselines. Supervised metrics are computed using the test split of our synthetic dataset. Then, to measure quantitatively the capacity to generalize to real data, we test using DeepFashion samples as input and compare to the small set of real samples we have captured using FVD~\cite{unterthiner2018towards}. We show qualitative results of the image synthesis capacity of each approach in Fig.~\ref{fig:ablation} and~\ref{fig:generalization} but refer to the supplementary material to better show the results in motion. %On one hand, we have state-of-the-art solutions~\cite{wang2022latent, mahapatra2022controllable}. 
LIA\cite{wang2022latent} is a solution tailored for human face animation, which exploits some characteristics of the domain, such as the structural and motion similarities of different faces performing the same action/expression (e.g., smiling, talking, etc.). Due to domain differences, we observe its performance is poor when trained on our task. It is unable to produce meaningful motions and often generates an average image for each synthetic animation sequence (best seen in the 2nd and 3rd column in Fig.~\ref{fig:ablation}). This is reflected in the quantitative metrics as well. The work of \cite{mahapatra2022controllable} is designed to work for images of fluids, under the simplifying assumption that a video sequence has a constant flow. While this assumption works well for natural images of fluids, it does not hold for the domain of garments. This method can generate consistent images, but with unrealistic motion and artifacts due to its warping based architecture. We see this in the second sample of Fig.~\ref{fig:ablation}. Furthermore, as expected, when trained with only synthetic data, neither of these solutions are able to generalize to real data. %They show poor image synthesis capacity for real samples. 

Next, we analyze the different variations of our method. Without a surprise, the RGB solution is the best performing in the synthetic dataset according to the quantitative metrics. Predicting normal map sequences followed by a re-shading step impacts the pixel level reconstruction accuracy as expected. Finally, using estimated normals as input slightly hinders performance w.r.t. using ground truth normals. As observed, the \textit{Toy} baseline performs very well in comparison, while we know it is not generating any motion. This suggests the classical metrics used for image reconstruction have some limitations for evaluating the solutions of our specific problem. The motion we want to generate is localized and subtle. The difference between a plausible motion and a static prediction may be smaller than the reconstruction error of an RGB auto-encoder. 
%Also, reconstruction metrics penalize a plausible motion if it is not exactly the same as the ground truth. A plausible motion (therefore, a valid prediction) with wrinkles placed differently accumulate error due to both missing wrinkles and \textit{misplaced} wrinkles (w.r.t. ground truth), while a static prediction only accounts for errors in the missing wrinkles. 
The solutions based in normal maps require a re-shading step that might produce a slight shift in pixel colors. While this does not hurt the quality of the dynamics, it increases the reconstruction error. The increase in perceptual error from RGB to normal-based solutions is not comparable to that of the other metrics. This suggests that the perceived quality of the generated images is comparable. Next, we evaluate the performance in real samples. For this case, the behaviour we observe is different. While the RGB solution is able to faithfully reproduce the input image with a slight color shift, a large majority of predictions do not show any motion. We observe a similar behaviour with the model trained on ground truth synthetic normals. Due to the domain gap, very few input normal maps produce dynamics in the output. This, added to the color shift due to the re-shading step increases the value of the FVD. Finally, we observe the model trained using predicted normals as input is able to generate plausible dynamics for all the input normals estimated from real samples resulting in the best FVD score. Note how in this case, the \textit{Toy} solution, which \textit{generates} static videos, has the worst FVD score. In, Fig.~\ref{fig:results}, we show additional sequence results for our model trained on estimated normals. For each sequence, we show frames sampled uniformly every $25$ frames. We also add close-up looks of the wrinkles. Our method is able to generate visually appealing results with plausible wrinkles. Finally, we further test the generalization capacity of our method by testing with an image of a hanging garment. This can be seen in Fig.~\ref{fig:test_case}. We refer to the supplementary material for qualitative video results.

\begin{figure}
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/results/0.png}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/results/1.png}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/results/2.png}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\textwidth]{img/results/3.png}
    \end{subfigure}
    \caption{Qualitative results using real samples from DeepFashion\cite{liuLQWTcvpr16DeepFashion}. We obtain this results with the model corresponding to the last row of Tab.~\ref{tab:ablation}. Our approach is able to generate consistent images with plausible wrinkles. We refer to the supplementary material for a video evaluation of our methodology. We omit LIA\cite{wang2022latent} from this comparison since it is unable to generalize at all.}
    \label{fig:results}
    \vspace{-.35cm}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{img/teaser/test_1.png}
    \caption{We further test the generalization capacity of our methodology by testing it with an image of a hanging garment. As observed, our approach can synthesize wrinkles on the cloth.}
    \label{fig:test_case}
    \vspace{-.35cm}
\end{figure}

%To qualitatively analyze the generalization capacity of our models, we show predictions for real samples from DeepFashion~\cite{liuLQWTcvpr16DeepFashion} in Fig.~\ref{fig:generalization}. We omit LIA\cite{wang2022latent} from this comparison as it is unable to generalize at all to real samples. As we can observe, the work of \cite{mahapatra2022controllable} suffers from color artifacts. Next, we see that training our model with different data modalities presents a tradeoff. On one hand, the model trained on synthetic RGB data seems to generalize properly as an auto-encoder. It is able to faithfully reproduce the input image with a slight color shift. Nonetheless, a large majority of predictions are static. On the other hand, training in normal space requires a re-shading algorithm, which has some limitations. The first limitation is related to intrinsic image decomposition. This is a very complex problem without a closed form solution. To address it, the work of \cite{bell14intrinsic} relies on priors. We can see this limitation in the third row of Fig.~\ref{fig:generalization}, where the estimated reflectance does not contain high-frequency texture details of the dress, which are lost during the re-shading step. We find another limitation in the illumination model. This can be seen in the first row of Fig.~\ref{fig:generalization}, where the illumination of the normal-based solutions appear to be different. From the normal-based strategies, we see that training using normals estimated from synthetic RGB is what permits generalization to real samples. As can be seen, only the samples generated with this method show new wrinkles. Next, Fig.~\ref{fig:results} shows sequence results for our model trained on estimated normals. For each sequence, we show frames sampled uniformly every $25$ frames. We also add close-up looks of the wrinkles. Our methodology is able to generate visually appealing results with plausible wrinkles. We refer to the supplementary material for qualitative video results.

Finally, due to the limitations of the quantitative metrics, we complement the evaluation with a qualitative user study. We show random samples of generated animations both on synthetic and real data with the different methods. We ask the users to rate if the animations are plausible or not. Variants of our method are rated as plausible more than $90\%$ of the time on synthetic data. For real images, our method trained on predicted normals is rated as plausible around $60\%$ of the time whereas our method trained on RGB and ground truth normals is rated as plausible around $20\%$ and $30\%$ of the time respectively. The strongest baseline \cite{mahapatra2022controllable} is not perceived as plausible neither on real nor synthetic data. We also ask for an estimation of the perceived wind direction (left or right). Around $\sim70\%$ of users correctly identified the wind direction under which the sequence was generated. We refer to the supplementary material for more details of the user study and additional video evaluation of the wind controllability.

%\begin{table*}[]
%\begin{tabular}{lccccccccc}
%\toprule
% & \multicolumn{7}{c}{Synthetic Data} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{Real Data} \\
%Experiment & MAE & MSE & RMSE & SSIM & PSNR & Perceptual & FVD & & FVD \\
%\midrule
%LIA[] & 23.7736 & 1131.62 & 32.4500 & 0.110538 & 25.5046 & 400.261 & 84.70 & & \\
%Controllable Animation[] & 9.37459 & 302.259 & 15.8852 & 0.481501 & 34.4905 & \textbf{220.855} & 24.03 & & \\
%\midrule
% & & & & & & & & & \\
%\midrule
%Toy & \textbf{6.52096} & 197.73 & 12.3561 & \textbf{0.608148} & \textbf{40.7553} & \textbf{218.393} & 23.97 & & 643.42 \\
%RGB & 7.64237 & 175.243 & \textbf{12.2101} & \textbf{0.558723} & \textbf{37.6388} & 230.397 & \textbf{15.39} & & 623.32 \\
%RGB w/ warp & \textbf{7.44406} & \textbf{173.335} & 12.2357 & 0.548366 & 37.5828 & \textbf{230.327} & 17.32 & & 628.99 \\
%Normals & 16.3458 & 612.53 & 23.6788 & 0.389855 & 31.7348 & 231.817 & 25.08 & & 633.12 \\
%Normals w/ warp & 16.4221 & 611.34 & 23.6857 & 0.386787 & 31.7297 & 231.721 & 25.16 & & 632.86 \\
%Normals w/ mask & 16.4186 & 615.889 & 23.7195 & 0.387198 & 31.7370 & 231.713 & 25.23 & & 636.18 \\
%Estimated Normals & 17.9486 & 717.614 & 25.5064 & 0.317634 & 31.4136 & 232.270 & 29.11 & & \textbf{613.26} \\
%Estimated Normals w/ mask & 17.8134 & 711.838 & 25.4201 & 0.323294 & 31.1309 & 232.549 & 28.52 & & 620.36 \\
%Target Estimated Normals & 15.7654 & 599.793 & 23.0125 & 0.326413 & 31.9411 & 232.451 & 54.47 & & 639.04 \\
%\bottomrule
%\end{tabular}
%\end{table*}

%Ablation:
%\begin{itemize}
%    \item \textbf{Model:}\begin{itemize}
%        \item \textbf{Non-cyclic.} Use standard UNet architecture to learn $f : x_t\rightarrow x_{t+1}$
%        \item \textbf{Cyclic.} Cyclic UNet formulation as $f : (x_0, dt)\rightarrow x_{dt}$
%        \item \textbf{Warping.} Replace time+wind conditioning by feature warping with time+wind dependant optical flow.
%    \end{itemize}
%    \item \textbf{Data:}\begin{itemize}
%        \item \textbf{RGB vs Normals.} Compare generalization to real data for both approaches: train/test on RGB, or three-stage pipeline (RGB$\rightarrow$Normal$\rightarrow$Normal Video$\rightarrow$Normal RGB).
%        \item \textbf{Looped.} Training on pre-processed data vs raw data.
%    \end{itemize}
%\end{itemize}

%Re-shading:
%\begin{itemize}
%    \item \textbf{Stylization.} Generate predictions using this methodology. 
%    \item \textbf{Naive initialization + Stylization.} Address failures of the stylization approach by providing a \textit{soft} guiding with a naive initialization for the RGB frames. This initialization is based on shading differences due to changes in the normals.
%\end{itemize}