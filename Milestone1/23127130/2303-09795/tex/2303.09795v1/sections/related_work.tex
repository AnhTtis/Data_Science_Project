\section{Related Work}
\label{sec:related_work}


\subsection{Challenges and Practices in AI Development}

There has been a series of works investigating the challenges and best practices of various stages in the development of AI software, e.g., data collection, system designing, model testing and deployment, etc.

Whang and Lee~\cite{Whang} discussed the challenges of data collection for AI systems.
Amershi et al.~\cite{amershi2019SE4AI} conducted surveys and interviews to understand the process of developing AI software in Mircosoft teams. 
They organized the response from developers into a set of best practices. 
For example, in the data collection stage, they suggested reusing the data as much as possible to reduce duplicated effort.
Paleyes et al.~\cite{paleyes2021towards} provided suggestions on using flow-based programming to better discover and collect data for AI systems. 
Researchers also investigate the code smells~\cite{10.1145/3522664.3528620} and data smells~\cite{10.1145/3522664.3528621,10.1145/3522664.3528590} of AI software systems.
Serban et al.~\cite{10.1145/3382494.3410681} evaluated the adoption and effects of using conventional software engineering practices in AI software development.
Wan et al.~\cite{8812912} analyzed differences between the development of machine learning systems and the development of non-machine-learning systems, which derives recommendations for AI developers.

Song et al.~\cite{10.1145/3522664.3528596} explored the practices of testing machine learning software via conducting an interactive rapid review with industry practitioners.
Software engineering researchers designed a list of methods to test and improve different AI systems (e.g., speech recognition~\cite{asrdebugger,asdf-paper,crossasrpp}, reinforcement learning~\cite{MDPFuzz}, language models~\cite{checklist}, etc) from various perspectives beyond correctness, e.g., fairness~\cite{CFSA,biasfinder,biasrv,biasheal}, robustness~\cite{acsac2022gong,yang2022revisiting,monash-fairness}, security~\cite{advdoor,baffle}, etc. 
Shneiderman~\cite{shneiderman2020bridging} discussed the guidelines for addressing the ethics and reliability issues in human-centered AI systems.
Paleyes et al.~\cite{10.1145/3533378} presented a survey of case studies of the challenges and practices in deploying AI systems.
Fan et al.~\cite{fan2021makes} mined academic AI repositories and provided some suggestions to improve the quality and popularity of open-source AI repositories.




\subsection{Mining GitHub Issues}

Researchers have investigated the bugs in deep learning systems~\cite{10.1145/3377811.3380395,6405375}.
This paper mainly focuses on the issues raised in AI repositories, which according to our taxonomy in RQ1 covers contents beyond bugs, e.g., paper discussion. 
We also introduce the works on mining GitHub issues.

GitHub Issues~\cite{preston-werner_2009} is an issue-tracking system integrated into GitHub that allows users to track tasks, enhancements, and bugs~\cite{shaowei-icst} for repositories hosted on GitHub. A series of empirical studies have been established to analyze software repositories and their issues. In 2013, Bissyande et al.~\cite{6698918} conducted a large-scale investigation of issue trackers from tens of thousands of GitHub projects and obtained interesting findings, i.e., the correlation between the numbers of issue raisers and the addressing time.
% As time goes by, the number of issues in the GitHub issue tracker increases dramatically, and more studies are conducted to find the best practices for issue management and automate the process of issue management. 
% In 2015, researchers proposed \texttt{GILA} to generate a set of visualizations to facilitate the analysis of issues in a project depending on their label-based categorization~\cite{7081860}. They realized that utilizing labels is a good practice for issue management and aimed to utilize data visualization to assist. Later on, 
\texttt{Ticket Tagger} was presented to automatically assign labels to issues in the GitHub issue tracker~\cite{KALLIS2021102598,8918993}. In this paper, we analyze the influence of labels in managing GitHub issues of AI repositories. Our results show that overall the label function is not frequently used in open-source AI repositories. Moreover, there is a significant difference in label usage distribution between open and closed issues. We believe tools for automatically tagging issues in AI repositories can boost the efficiency of issue resolution~\cite{izadi2022predicting,WANG2022106845}.
Recently, Hata et al.~\cite{hata2022github} analyzed the early usage of GitHub Discussion, a new feature of GitHub that allows users to discuss issues and topics. Their approach, i.e., the sentiment of the content, is also similar to our study. However, their paper does not focus on issue tracking systems. 


% Our empirical study also shows that the average addressing time of issues in open-source AI repositories is over 45 days. The issue lifecycle prediction techniques can help developers prioritize the issues. In 2007, Weiss et al.~\cite{4228638} employed kNN and text similarity to predict the lifecycle of issues in the \texttt{JBoss} project. Panjer et al.~\cite{4228666} utilized various data mining algorithms, i.e., 0-R and 1-R, Decision Trees, Naive Bayes, and Logistic Regression, to predict the bug lifecycle of \texttt{Eclipse}. Panichella et al.~\cite{PANICHELLA2021106665} aimed to automatically identify the GitHub issues that would not be fixed. In this paper, we investigate the correlation between the issue lifecycle, i.e., issue address time, and repository/issue properties, such as the number of contributors and number of issues. We find a weak correlation between the number of issues and average addressing time.

