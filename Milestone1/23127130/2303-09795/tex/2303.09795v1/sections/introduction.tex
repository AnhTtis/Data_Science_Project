\section{Introduction}
\label{sec:intro}

Artificial Intelligence (AI) has revolutionized many critical tasks in our modern lives, such as natural language understanding~\cite{danilevsky2020survey, chowdhary2020natural}, image processing~\cite{tjoa2020survey}, speech recognition~\cite{HuBERT}, and code understanding~\cite{9825884,alsulami_source_2017}.
The success of these AI techniques is heavily dependent on the open-source culture. 
Popular open-source repositories like \texttt{TensorFlow}\footnote{\url{https://www.tensorflow.org/}} and \texttt{PyTorch}\footnote{\url{https://pytorch.org/}} have greatly facilitated the development of AI applications. 
Additionally, research papers published at flagship AI conferences often release their replication packages and models, making it easier for researchers and practitioners to replicate results, run models on other tasks, or conduct further research. Furthermore, open-source benchmarks such as CodeXGLUE\footnote{\url{https://github.com/microsoft/CodeXGLUE}} and ImageNet\footnote{\url{https://www.image-net.org/}} have played a crucial role in the advancement of AI by providing a common platform for researchers to evaluate their models and compare their results.


Despite the benefits of open-source AI repositories, developers often encounter challenges when utilizing them to build AI applications. A recent article~\cite{heaven_2022} points out that many researchers have complained about a `replication crisis' in AI repositories. This issue can be attributed to various factors, such as incomplete instructions for configuring development environments~\cite{gundersen2018reproducible} and a lack of information about the datasets used to train AI models. Additionally, buggy code can also be a significant obstacle for developers. These problems can lead to frustration and hinder the use of open-source AI repositories.



To address the problems above, developers should communicate with repository owners and other developers to discuss and address these problems together. There are several channels for such communication. 
For example, developers can post questions in StackOverflow\footnote{\url{https://stackoverflow.com/}} (SO). However, SO is unsuitable for finding answers to questions specific to a repository as there may be fewer experts on this topic. Another channel is email. Researchers or the repositories' owners usually publish their contact information, e.g., email addresses and affiliations, in their papers or the descriptions of their repositories, respectively. The developers can also inquire about questions via email; however, this communication is often private and inaccessible for analysis. The issue trackers, such as GitHub Issues\footnote{\url{https://github.com/features/issues}} or JIRA,\footnote{\url{https://www.atlassian.com/jira}} can be appropriate resources to analyze the problems faced by the AI repository users. 

 

We obtain a list of papers published between 2013 and 2022 from ten top-tier AI conferences and retrieve their replication packages from the \texttt{PapersWithCode}\footnote{\url{https://paperswithcode.com/}} platform. After cleaning the data (e.g., removing the repositories that are not provided by the authors of papers), we identify 576 open-source software (OSS) AI repositories hosted on GitHub and 24,953 GitHub issues using the GitHub REST APIs. We then conduct open card sorting to develop a taxonomy of the issues in open-source AI repositories, categorizing them into 13 categories. Our findings reveal that the most commonly reported issue is \texttt{Runtime Error}.
The second-largest group of issues falls under \texttt{Unclear Instructions}, primarily caused by inadequate documentation.



Our study examines how developers manage and address issues in their open-source AI repositories. The results indicate that 67.53\% of issues are resolved, with 50\% of issues closed within four days. Additionally, we found a positive correlation between the number of issues in a repository and the average time to address the issues (i.e., the time between the open and close date of issues), as well as a negative correlation between the number of issues and the closed issue rate (i.e., the ratio of closed issues to all the issues in a repository). We also discovered that developers often neglect the features provided by GitHub to manage issues, with only 7.81\% of repositories using labels to categorize issues and only 5.90\% of repositories assigning issues to specific individuals. Further analysis of the relationship between various features and the closure of issues in open-source AI repositories reveals that an issue's label(s) and assignee(s) have a statistically significant impact on the closure of issues. These findings suggest that repository maintainers should actively utilize GitHub issue management tools to effectively manage issues in their repositories. Additionally, issues with longer descriptions and code blocks are more likely to be closed, indicating that issue raisers should provide more details, especially code blocks, to convey information clearly.

We make the following contributions in this paper:
\begin{itemize}
	\item We conduct an empirical study to systematically investigate the issues in open-source AI repositories hosted on GitHub. The study contributes to the community with a dataset of 576 AI repositories that are manually confirmed to be the official implementations of papers from top-tier AI conferences and 24,953 issues in these repositories.
	\item We categorize the issues in open-source AI repositories to help developers better understand the problems users encounter when using these systems.
	\item Our empirical study shows that repository maintainers and developers should follow some good practices to help better address the issues in open-source AI repositories. We encourage the maintainers to actively adopt issue management features, e.g., the labeling and assigning features provided by GitHub. We also suggest that the developers provide detailed information, especially code blocks, and format the issues properly.
\end{itemize}

The rest of this paper is organized as follows. Section~\ref{sec:background} explains the background of open-source AI repositories and their issues.
In Section~\ref{sec:methodology}, we describe the process of collecting and cleaning datasets.
Section~\ref{sec:results} reports research questions and experiment results. 
We discuss the difference between academic and industry repositories, as well as the threats to validity in Section~\ref{sec:discussion}. 
Section~\ref{sec:related_work} reports the works relevant to our work.
We conclude the paper and present future work in Section~\ref{sec:conclusion}.