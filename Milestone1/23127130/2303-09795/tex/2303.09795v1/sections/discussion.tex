\section{Discussion}
\label{sec:discussion}


\begin{table}[]
    \centering
    \caption{The difference between industry and academic repositories.}
    \begin{tabular}{lcccc}
    \hline
    Class            & Academic & Industry & $p$-value          & $\delta$   \\ \hline
    No. Issues       & 22.57    & 257.62   & \textless{}0.01    & Small      \\
    No. Stars        & 232.82   & 2749.10  & \textless{}0.01    & Medium     \\
    No. Contributors & 2.25     & 41.06    & \textless{}0.01    & Small      \\
    Address Time     & 45.47    & 47.90    & \textless{}0.01    & Negligible \\
    Close Rate     & 57\%     & 58\%     & \textgreater{}0.05 & Negligible \\
    Assign Rate      & 1.35\%  & 5.15\%  &  \textless{}0.05  & Small   \\
    Label Rate       & 2.45\%    & 6.45\%  &  \textgreater{}0.05      &  Small \\ \hline
    \end{tabular}
    \label{tab:difference}
\end{table}

% This section discusses the statistical difference between issues in the industry and academic repositories and threats to the validity of our study.

\subsection{Industry vs. Academic Repositories}


Developers from industry and academia work closely to promote the development of AI systems together. 
On the one side, industry practitioners also use academic AI repositories to build their products.
On the other side, academic researchers use popular AI frameworks originating from industry to create their models.
We are also interested in the difference between issues in the industry and academic repositories. 
Similar to the process in~\cite{NICHE}, we ask two annotators to manually split collected repositories into two types: repositories owned by industry and those owned by academia. 
The inter-annotator agreement value is 0.62, indicating substantial agreement among our annotators~\cite{landis1977measurement}.
We invite another annotator to resolve the disagreement between the two annotators. 
In total, we have 63 repositories owned by industry and 513 repositories owned by academia. We compare the two groups of repositories from the perspectives of the number of issues, addressing time, close rate, label distribution, assigning rate, and labeling rate. We conduct a Wilcoxon rank-sum test to check whether the difference is statistically significant. Table~\ref{tab:difference} presents our results.


On average, each industry repository includes 257.62 issues, while each academic repository contains 22.57 issues. 
%We can observe an imbalanced distribution of issues between the two types of repositories, i.e., a small number of industry repositories contribute to a large number of issues. 
However, it does not necessarily indicate that industry repositories have low quality. The average number of stars in industry repositories (2749.10) is one order of magnitude greater than that in academic repositories (232.82), meaning that industry repositories attract much more attention and, therefore, the number of issues in these repositories is larger. Note that the ratio of the number of issues to the number of stars is close: 0.094 for industry repositories and 0.097 for academic ones.

The industry repositories have over 18 times more average contributors than the academic ones (41.06 vs. 2.25). The ratio of the number of issues to the number of contributors is 6.27 for industry repositories and 10.03 for academic repositories. However, a smaller issue-contributor ratio does not lead to faster issue-addressing time; industry repositories even have a slightly (but statistically significant) longer addressing time (47.90 days) than academic repositories (45.47 days). Besides, the issue close rate is also similar between the two types of repositories (0.58 and 0.57). One possible explanation is that the academic repositories are less active after the code is released, meaning that the contributors mainly focus on addressing the issues. 
However, industry repositories require their contributors to put more effort into both developing new features and addressing issues.


The two types of repositories also demonstrate different behaviors in using the issue management tools. Although both the labeling and assigning functions are not widely adopted in the open-source AI repositories, there are more issues in industry repositories that are labeled and assigned. The assign rates are 5.15\% and 1.35\% for industry and academic repositories, respectively; the difference is also statistically significant ($p\text{-value}<0.05$). The label rates are 6.45\% and 2.45\% for industry and academic repositories, respectively. However, the difference is not statistically significant ($p\text{-value}>0.05$). Overall, the industry repositories are more active in using the issue management features than the academic ones.

\subsection{Implications}
Based on the analysis of each research question, we summarize the implications of our study as the suggestions to open-source AI repository developers and users.

In this study, we find that the most prevalent issue among the open-source AI repositories is the \texttt{Runtime Error} category, which can be caused by a variety of factors related to other categories of issues, such as missing datasets or missing models. The second most common category of issues falls under the \texttt{Unclear Instructions}, which is primarily caused by a lack of detailed descriptions. 
To address these issues and improve the quality of open-source AI repositories, we propose two suggestions for the maintainers: (1) ensure that all necessary datasets, models, and other files are provided to allow for accurate replication of results, and (2) provide clear and detailed instructions on how to use the repository, including information on dataset splitting, setting hyperparameters, and managing library dependencies.

Our experiment results show that issues are more likely to be closed if they have assignees and labels. 
Therefore, we recommend that repository maintainers actively leverage the features provided by GitHub to manage issues by (1) tagging issues with labels and (2) assigning specific people to be responsible for addressing the issues.
Besides, the length of issues descriptions and whether the issues have code blocks cause a statistically significant difference to the closure of issues. 
Based on the results, we make two suggestions to the issue raisers: (1) provide more details to precisely describe the issue and (2) use the Markdown syntax to formulate the issue description with code snippets.


\subsection{Threats to Validity}

\noindent \textbf{Threats to Internal Validity.}
We collect a list of repositories from the PaperWithCode platform, which is a community-driven platform where users can link papers with their corresponding repositories. However, there is a risk that some of these repositories may not be official implementations, which can be of low quality and attract less attention from users. To mitigate this threat, we only keep the repositories that are marked as ``official implementations.'' Additionally, we compare the repositories and paper information, such as owner and author information, to ensure that the repositories are official. This process is done manually, with two authors involved, and any disagreements are resolved with the involvement of another annotator.
Furthermore, we manually group the repositories by their ownership, such as industry and academia. However, considering that academia and industry work closely together in AI research, it may be difficult to unanimously agree on the ownership of a repository. In such cases, another annotator is involved in the process to lead the discussion and resolve any disagreements.


\vspace*{0.2cm}
\noindent \textbf{Threats to External Validity.}
Our study collects papers and repositories from the flagship AI conferences within a ten-year timeframe. However, it is important to note that there is a risk that the conclusions drawn from our study may not be generalizable to a broader range of open-source AI repositories. The field of AI research encompasses a wide range of subtopics, some of which are more popular than others. This leads to an imbalanced distribution of repositories tackling various topics in our dataset, which may affect the generalizability of our conclusions to other AI subtopics or interdisciplinary AI research.
Additionally, while we analyze open-source AI repositories hosted on GitHub, which is the most popular platform for open-source AI projects,  other platforms such as GitLab or BitBucket, which employ different issue trackers, such as JIRA, also contain open-source AI repositories. These platforms may have different issue management strategies, which could lead to diverse conclusions. %Furthermore, we analyze the difference between repositories owned by industry and academia, however, the source of these repositories comes from the \texttt{PaperWithCode} platform, meaning that the audience of these repositories is mainly researchers. Thus, our conclusion may not generalize to other types of repositories, such as AI repositories for developing industrial applications.

AI is a fast-evolving field. Future work that replicates our study in a wider scope of open-source AI repositories, e.g., more venues, longer time periods, more AI subtopics, or other hosting platforms, can further validate and generalize our findings. We also make the replication package publicly available to facilitate such replication.
