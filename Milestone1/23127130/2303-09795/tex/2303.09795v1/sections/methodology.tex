\section{Data Collection}
\label{sec:methodology}

This section presents the process of collecting and cleaning the dataset, i.e., GitHub issues of open-source software AI repositories, for our empirical study. 

\subsection{Scope}

Following a previous study on analyzing open-source software (OSS) AI repositories~\cite{fan2021makes}, we include five prestigious AI conferences in our experiments: NeurIPS, ICML, CVPR, ICCV, and ECCV. Out of these five conferences, CVPR, ICCV, and ECCV are computer vision-oriented. We also collect papers and repositories from other highly regarded AI conferences, such as ICLR, ACL, SIGKDD, AAAI, and AAMAS. 
All the selected conferences are categorized as A* in the CORE Ranking.\footnote{https://www.core.edu.au/conference-portal}
In total, our dataset is constructed from ten AI conferences that cover a wide range of AI topics, including natural language processing, reinforcement learning, etc. It is worth noting that frameworks and applications of AI techniques are constantly evolving. Our dataset includes repositories related to AI conferences from 2013 to 2022.


\subsection{Data Curation}



Figure~\ref{fig:workflow} illustrates an overview of the workflow for collecting and cleaning our dataset. We use \texttt{PapersWithCode}\footnote{\url{https://paperswithcode.com}} (\texttt{PwC}), a platform for storing AI-related papers, implementations, and datasets, to obtain AI papers published at conferences and their corresponding repositories. According to the contributing guidelines, \texttt{PwC} users can connect a repository to a paper by adding GitHub, GitLab, or BitBucket URLs. Furthermore, users can monitor all edits on a Slack\footnote{\url{https://slack.com/intl/en-au/}} channel, where everyone is  encouraged to review their contributions to ensure data quality. If contributors acknowledge that a repository is the implementation provided by the authors of the corresponding paper, they mark this repository as \texttt{official}. 


In this paper, we adopt a conservative strategy for selecting repositories. Specifically, we only consider papers whose linked repositories are labeled as \texttt{official}. Intuitively, users tend to ask questions in the official repository maintained by the authors, and such official repositories are more likely to contain more issues from which more actionable suggestions can be derived.
We use \texttt{PwC} APIs\footnote{\url{https://paperswithcode.com/api/v1/docs/}} to obtain papers from different venues and their metadata, including a list of authors and their repository links. We then conduct a series of queries from the \texttt{PwC} APIs with the following constraints:
\begin{itemize}
	\item \texttt{Venue}: Papers from the ten selected conferences. 
	\item \texttt{Year}: The dates of these conferences are between 1/1/2013 and 31/12/2022.
	\item \texttt{Authority}: All the repositories that are labeled as \texttt{official} on \texttt{PapersWithCode}.
\end{itemize}


In our initial collection, we obtain 652 open-source AI repositories and their corresponding papers from \texttt{PapersWithCode}. Among these repositories, 639 (98.01\%) are published on GitHub, nine (1.38\%) are hosted on CodaLab, and only four (0.61\%) are hosted on Bitbucket. Given the prevalence of GitHub as a platform for hosting open-source AI repositories and the convenience of utilizing GitHub APIs, we decided to focus on the repositories hosted on GitHub for our analysis, resulting in a set of 639 open-source AI repositories.


Subsequently, we use the \texttt{git clone} command to download these 639 open-source AI repositories and manually check whether the downloaded repositories are indeed the \texttt{official} implementation of the corresponding papers. To verify the relationship between repositories and their corresponding papers, we utilize three strategies. Firstly, the README.md file of an AI repository should contain the title of the corresponding paper. Secondly, the paper must mention the repository URL. Lastly, the repository owners should be listed as the authors of the corresponding paper. To evaluate the accuracy of these strategies, we asked two annotators to manually examine each pair of a repository and its corresponding paper to evaluate the accuracy of our strategies.
Following prior studies~\cite{biasfinder}, we employ Cohen's Kappa~\cite{mchugh2012interrater} to measure the inter-annotator agreement, which is 0.72. 
According to~\cite{landis1977measurement}, this inter-annotator agreement value indicates substantial agreement among our annotators.
In cases of disagreement, a third annotator was consulted to finalize the decision. The three annotators, who are Ph.D. students with over five years of experience in using GitHub, were able to confirm 576 pairs of repositories and their corresponding papers as \texttt{official} implementations.
%\jh{Maybe we should mention something about annotators. Who are they? Their background knowledge, etc.}



We also employ the GitHub REST API\footnote{\url{https://docs.github.com/en/rest?apiVersion=2022-11-28}} to fetch the metadata and issue information of the collected repositories. The metadata of these repositories includes information such as contributors, the number of stars, the number of forks, the number of watchers, the creation date, and the last update date. We also obtain the issue list of each repository and compute the number of open and closed issues. For each issue, we extract its title, discussion history, status, creation date, close date (if the issue has been closed), assignees, and labels.
Our final dataset consists of 576 pairs of repositories and their corresponding papers. Additionally, we find that 24,953 issues belong to these repositories.
%\jh{Figure 2 is mentioned too late here. Maybe we should mention Figure 2 first as it is an overview of our workflow.}

\begin{figure}[!t]
	\centering
	\includegraphics[width=1\linewidth]{figures/data-collection-workflow.pdf}
	\caption{The workflow of collecting and cleaning the data. We obtain the paper list and the corresponding repository list from \texttt{PapersWithCode}. After manually removing the unofficial repositories, we query the GitHub API to obtain the metadata and issue information.}
	\label{fig:workflow}
\end{figure}
