\section{Results}
\label{sec:results}


In this section, we present our analysis of the issues in open-source software (OSS) AI repositories. 
We conduct experiments to answer the following three research questions:

\begin{itemize}
	\item \textit{RQ1. What do users discuss in the issues of open-source AI repositories?}
	\item \textit{RQ2. How are issues in open-source AI repositories managed and addressed?}
	\item \textit{RQ3. What are the relationships between different features and the closure of issues in open-source AI repositories?}
\end{itemize}

\subsection{RQ1. What do users discuss in the issues of open-source AI repositories?}

\noindent \textbf{Motivation.}
The development of AI systems follows a data-driven paradigm~\cite{ntoutsi2020bias, wolf2020ai}, in which AI models learn knowledge from a large amount of data automatically, as opposed to traditional software systems where developers encode their programming language through control flow and data flow graphs~\cite{allen1970control, kavi1986formal}. Additionally, AI systems often require specialized hardware, such as a GPU or Google TPU, to accelerate the training and testing processes. Therefore, the issues that developers face in open-source AI repositories may differ from those in traditional OSS repositories. In order to better understand the primary issues encountered in the development and usage of AI applications, we aim to construct a taxonomy of the issues in open-source AI repositories.

% \vspace{0.2cm}
\noindent \textbf{Categorization Process.}
We obtain 24,953 issues from the 576 repositories using the GitHub REST API. Among these repositories, 122 (21.18\%) have no GitHub issues. Previous studies~\cite{10.1145/2568225.2568233,10.1145/1718918.1718973} have used the card sorting method, a technique for creating mental models and deriving taxonomies from data, to categorize software engineering-related discussions. As the issues in open-source AI repositories can be unique, we adopt open card sorting, which uses no predefined groups to create the taxonomy for issues in AI repositories. The taxonomy is intended to categorize \textit{the primary reason why developers raise an issue} rather than the \textit{root cause of this issue}, the latter of which is usually difficult to determine without sufficient information. For example, a developer may raise an issue asking why the results are impossible to reproduce, but the root cause can be due to many reasons, such as differences in library versions, hardware, or randomness. For this issue, we place it in the \texttt{Failure to replicate} category. In this paper, we followed three steps to categorize issues in the open-source AI repositories. These steps are described as follows:

\begin{enumerate}[label=(\roman*)]
	\item \textit{Preparation}: We randomly pick 384 issues\footnote{It is statistically representative sample size using a popular sample size calculator (\url{https://www.surveysystem.com/sscalc.htm}) with a confidence level of 99\% and a confidence interval of 10.} from all the collected issues, then we import these issues into a Notion database and create a card for each issue.
	\item \textit{Execution}: Two annotators discuss with each other how to categorize the cards into meaningful groups. We adopt the \textit{open card} sorting method, which lets the groups emerge and evolve during the sorting process. In this step, the two authors are encouraged to create groups. This step takes about seven hours.
	\item \textit{Analysis}: Two annotators who conduct card sorting, as well as a third annotator, discuss together how to merge groups of relevant topics into more general categories. This step takes approximately 1.5 hours.
\end{enumerate}

\pgfmathsetmacro{\MD}{10} % missing dataset
\pgfmathsetmacro{\MM}{9} % missing model
\pgfmathsetmacro{\MCM}{9} % missing code/module
\pgfmathsetmacro{\MCSUM}{int(\MD+\MM+\MCM)} % missing content
\pgfmathsetmacro{\UCUI}{56} % unclear code usage instruction
\pgfmathsetmacro{\UDI}{12} % unclear data information
\pgfmathsetmacro{\UEC}{7} % unclear environment configuration
\pgfmathsetmacro{\UISUM}{int(\UCUI+\UDI+\UEC)} % unclear instructions
\pgfmathsetmacro{\DM}{10} % discuss methodology
\pgfmathsetmacro{\EX}{42} % extension
\pgfmathsetmacro{\RE}{89} % runtime error
\pgfmathsetmacro{\DI}{35} % discuss implementation
\pgfmathsetmacro{\FTR}{13} % fail to replicate
\pgfmathsetmacro{\EN}{16} % enhancement
\pgfmathsetmacro{\AB}{50} % abnormal behavior
\pgfmathsetmacro{\PCM}{5} % paper code misalignment
\pgfmathsetmacro{\SI}{4} % supplementary information
\pgfmathsetmacro{\RI}{15} % re-inplementation
\pgfmathsetmacro{\O}{2} % others
\pgfmathsetmacro{\SUM}{int(\MCSUM+\UISUM+\DM+\EX+\RE+\DI+\FTR+\EN+\AB+\PCM+\SI+\RI+\O)} % sum

\pgfmathsetmacro{\MCSUMpercentage}{\MCSUM/\SUM*100}
\pgfmathsetmacro{\MDpercentage}{\MD/\SUM*100}
\pgfmathsetmacro{\MMpercentage}{\MM/\SUM*100}
\pgfmathsetmacro{\MCMpercentage}{\MCM/\SUM*100}
\pgfmathsetmacro{\UISUMpercentage}{\UISUM/\SUM*100}
\pgfmathsetmacro{\UCUIpercentage}{\UCUI/\SUM*100}
\pgfmathsetmacro{\UDIpercentage}{\UDI/\SUM*100}
\pgfmathsetmacro{\UECpercentage}{\UEC/\SUM*100}
\pgfmathsetmacro{\DMpercentage}{\DM/\SUM*100}
\pgfmathsetmacro{\EXpercentage}{\EX/\SUM*100}
\pgfmathsetmacro{\REpercentage}{\RE/\SUM*100}
\pgfmathsetmacro{\DIpercentage}{\DI/\SUM*100}
\pgfmathsetmacro{\FTRpercentage}{\FTR/\SUM*100}
\pgfmathsetmacro{\ENpercentage}{\EN/\SUM*100}
\pgfmathsetmacro{\ABpercentage}{\AB/\SUM*100}
\pgfmathsetmacro{\PCMpercentage}{\PCM/\SUM*100}
\pgfmathsetmacro{\SIpercentage}{\SI/\SUM*100}
\pgfmathsetmacro{\RIpercentage}{\RI/\SUM*100}
\pgfmathsetmacro{\Opercentage}{\O/\SUM*100}





\begin{table}[]
	\centering
	\caption{The distribution of issues of different categorizes.}
	\begin{tabular}{|c|ll|c|r|}
		\hline
		ID                 & \multicolumn{2}{l|}{Category}                  & No. & Rate \\ \hline
		\multirow{4}{*}{1} & \multicolumn{2}{l|}{Missing Content}           & $\MCSUM$     &  $\pgfmathprintnumber[fixed, fixed zerofill, precision=2]{\MCSUMpercentage}\%$          \\
						   &  & $\cdot$ Missing Dataset                     & $\MD$      &  $\pgfmathprintnumber[fixed, fixed zerofill, precision=2]{\MDpercentage}\%$           \\
						   &        & $\cdot$ Missing Model                         & $\MM$     &  $\pgfmathprintnumber[fixed, fixed zerofill, precision=2]{\MMpercentage}\%$          \\
						   &        & $\cdot$ Missing Code/Module                   & $\MCM$      & $\pgfmathprintnumber[fixed, fixed zerofill, precision=2]{\MCMpercentage}\%$           \\
		\multirow{4}{*}{2} & \multicolumn{2}{l|}{Unclear Instructions}      & $\UISUM$     &  $\pgfmathprintnumber[fixed, fixed zerofill, precision=2]{\UISUMpercentage}\%$          \\
						   &        & $\cdot$ Unclear Code Usage Instruction        & $\UCUI$     &  $\pgfmathprintnumber[fixed, fixed zerofill, precision=2]{\UCUIpercentage}\%$          \\
						   &        & $\cdot$ Unclear Data Information              & $\UDI$     &  $\pgfmathprintnumber[fixed, fixed zerofill, precision=2]{\UDIpercentage}\%$          \\
						   &        & $\cdot$ Unclear Environment Configuration     & $\UEC$      &  $\pgfmathprintnumber[fixed, fixed zerofill, precision=2]{\UECpercentage}\%$          \\
		3                  & \multicolumn{2}{l|}{Discuss Methodology}       & $\DM$     & $\pgfmathprintnumber[fixed, fixed zerofill, precision=2]{\DMpercentage}\%$           \\
		4                  & \multicolumn{2}{l|}{Extension}                 & $\EX$     &  $\pgfmathprintnumber[fixed, fixed zerofill, precision=2]{\EXpercentage}\%$          \\
		5                  & \multicolumn{2}{l|}{Runtime Error}             & $\RE$     &  $\pgfmathprintnumber[fixed, fixed zerofill, precision=2]{\REpercentage}\%$          \\
		6                  & \multicolumn{2}{l|}{Discuss Implementation}    & $\DI$     &  $\pgfmathprintnumber[fixed, fixed zerofill, precision=2]{\DIpercentage}\%$          \\
		7                  & \multicolumn{2}{l|}{Fail to Replicate}         & $\FTR$     &  $\pgfmathprintnumber[fixed, fixed zerofill, precision=2]{\FTRpercentage}\%$          \\
		8                  & \multicolumn{2}{l|}{Enhancement}               & $\EN$     &  $\pgfmathprintnumber[fixed, fixed zerofill, precision=2]{\ENpercentage}\%$          \\
		9                  & \multicolumn{2}{l|}{Abnormal Behavior}        & $\AB$     &  $\pgfmathprintnumber[fixed, fixed zerofill, precision=2]{\ABpercentage}\%$          \\
		10                 & \multicolumn{2}{l|}{Paper-Code Misalignment}   & $\PCM$      &  $\pgfmathprintnumber[fixed, fixed zerofill, precision=2]{\PCMpercentage}\%$          \\
		11                 & \multicolumn{2}{l|}{Supplementary Information} & $\SI$      &  $\pgfmathprintnumber[fixed, fixed zerofill, precision=2]{\SIpercentage}\%$          \\
		12                 & \multicolumn{2}{l|}{Re-implementation}         & $\RI$     &  $\pgfmathprintnumber[fixed, fixed zerofill, precision=2]{\RIpercentage}\%$          \\
		13                 & \multicolumn{2}{l|}{Others}                    & $\O$      &  $\pgfmathprintnumber[fixed, fixed zerofill, precision=2]{\Opercentage}\%$          \\ \hline
		\end{tabular}
	\label{tab:categories}
	\end{table}

\vspace{0.2cm}
\noindent \textbf{Categorization Results.}
We obtain 13 categories; two of them are further divided into three subcategories. Table~\ref{tab:categories} shows the categories and subcategories in the sampled issues. 
In our taxonomy, each issue is assigned to one category, which reflects the primary reason for raising this issue.
We discuss the details of these categories as follows:
\begin{itemize}
	\item \texttt{Missing Content}: 
Users raise issues as they are unable to find the contents used to employ open-source AI repositories. There are 28 issues that fall into this category, which is subsequently divided into three different subcategories, such as \textit{missing dataset} (10 issues), \textit{missing model} (9 issues), and \textit{missing code/module} (9 issues). For example, the issue\footnote{\url{https://github.com/pathak22/context-encoder/issues/24}} titled ``where to download the Paris StreetView Dataset'' requests the Paris StreetView dataset for running the Context Encoders model to generate the contents of an arbitrary image~\cite{pathak2016context}.
	\item \texttt{Unclear Instructions}: 
 The category is mentioned when users are unable to comprehend the instructions used to employ open-source AI repositories. This category contains 75 issues (19.53\%), which is the second-largest category. We define three subcategories based on information from the instructions. The first subcategory is \textit{unclear environment configuration}, e.g., the version of a programming language library in an AI repository.\footnote{https://github.com/ShichenLiu/CondenseNet/issues/22} The second subcategory is \textit{unclear data information}, e.g., how to split a dataset into training and testing sets. The third subcategory is \textit{unclear code usage instruction}, e.g., hyperparameters used to train AI models. 
	\item \texttt{Discuss Methodology}: 
 This category, which includes ten issues, focuses on discussing the methodology of AI models. For example, an issue\footnote{https://github.com/irwinherrmann/stochastic-gates/issues/1} asks the input size of an AI model.
	\item \texttt{Extension}: 
 Users raise their issues to ask AI repositories' owners whether AI models can be extended beyond their original scope. For example, users may want to apply AI models to a new dataset or do some modifications to these models.
	\item \texttt{Runtime Error}: 
 We define this category for users who encounter runtime errors when using AI repositories. The category includes 89 issues (23.18\%) which is the largest population of all categories. There are various reasons for the runtime errors, e.g., the users may not have the required packages installed, the users employ the wrong path, or the hardware is incompatible. We find that it is challenging to further divide these issues into subcategories based on the root cause of the runtime errors. 
	It is difficult to further divide these issues into subcategories based on the cause of the runtime errors. 
	\item \texttt{Discuss Implementation}: 
 This category aims to discuss the implementation details of AI repositories. For example, users may ask whether we should apply dropout techniques to AI models. Some users may also require the AI repositories' owners to clarify the details of their implementation.
	\item \texttt{Failure to Replicate}: 
 Users often submit their issues as they fail to employ an AI repository to replicate results reported in its corresponding paper. We put these issues in this category. 
	\item \texttt{Enhancement}: 
 Issues in this category aim to improve the quality of AI repositories, e.g., by adding new features, fixing bugs, or updating libraries. 
	\item \texttt{Abnormal Behaviour}: 
 This category focuses on users who may encounter performance issues, e.g., the training time of AI models takes too long.\footnote{https://github.com/open-mmlab/mmdetection/issues/5237}
	\item \texttt{Paper-Code Misalignment}: 
Issues related to this category focus on the difference between the code in AI repositories and the descriptions of their corresponding papers. For example, users may ask why the dimensions of hidden layers in an AI repository differ from the descriptions on its paper.\footnote{https://github.com/timmeinhardt/trackformer/issues/58} 
	\item \texttt{Supplementary Information}: 
 These issues request supplementary information for AI repositories, e.g., slides, documents, and videos of AI models.
	\item \texttt{Re-implementation}: 
 Users aim to re-implement AI repositories by employing other programming languages; hence, they may raise issues to discuss with the AI repositories' owners and other developers. 
	\item \texttt{Others}: 
 The remaining issues that are unable to categorize into any of the above categories. For example, some issues have insufficient information to be categorized.\footnote{\url{https://github.com/matterport/Mask_RCNN/issues/2046}} 
\end{itemize}



\begin{tcolorbox}[
	colback=green!10,
	colframe=green!50!black,
	colbacktitle={red!50!white},
	coltitle=white,
	fonttitle=\sffamily\bfseries\large,
	title=Answer to RQ1,
	center title,
	]
	We categorize the issues in open-source AI repositories into 13 classes, including: Missing Content, Unclear Instructions, Discuss Methodology, Extension, Runtime Error, Discuss Implementation, Failure to Replicate, Enhancement, Abnormal Behaviour, Paper-Code Misalignment, Supplementary Information, Re-implementation, and Others.
  \end{tcolorbox}


\subsection{RQ2. How are the issues managed and addressed in open-source AI repositories?}

\noindent \textbf{Motivation.}
The issues raised by users in open-source AI repositories serve as an important indicator of problems encountered during usage and also serve as a means of communication with repository maintainers. Effective management and addressing of these issues can greatly impact the user experience and the overall quality of the AI repositories. As the first research question highlights, users raise issues for a variety of reasons, such as missing content, unclear instructions, or runtime errors. Ignoring these issues can prevent users from successfully replicating the results reported in the repositories. While some issues can be easily resolved by users, such as installing required packages, other issues such as runtime errors or unclear instructions require better management and addressing in order for users to effectively employ the AI repositories for building their software applications. Considering the rapid growth of the AI field, it is crucial to investigate these issues to aid maintainers in effectively managing open-source AI repositories.

\vspace{0.2cm}
\noindent \textbf{How are the issues addressed?}
We aim to understand how to address the issues present in open-source AI repositories. These issues are categorized into two groups, \textit{open} and \textit{closed} issues. Open issues are those that have yet to be addressed, while closed issues are those that have been resolved. Utilizing the GitHub REST API, we collected 24,953 issues from open-source AI repositories. Of these issues, 8,102 (32.47\%) are open issues, and 16,851 (67.53\%) are closed issues. As shown in Figure~\ref{fig:close-rate-dist} (a), the distribution of the \textit{closed issue rate} (CIR) for each repository reveals that 35.76\% of repositories have closed all of their issues. However, it is worth noting that these repositories with a CIR higher than 0.9 have an average of 83.83 issues, indicating that these repositories have a larger number of issues in total.


We discover that 40\% of closed issues are ``\textit{self-closed},'' indicating that the individuals who raised the issues are able to find solutions to their problems through discussion. Additionally, we identify another type of issue referred to as \textit{ignored issues}. These are issues that only the individuals who raised them are interested in and typically receive no response from other users or developers. These ignored issues comprise 11.79\% of all issues. We also define the \textit{ignored issue rate} as the ratio of ignored issues for each repository. This metric allows us to evaluate the level of active engagement by repository maintainers in addressing issues. The results of the ignored issue rate are presented in Figure~\ref{fig:close-rate-dist} (b).

% \jh{We need to discuss Figure 3b first then Figure 3c. Need to recheck the figure number} 
% \yz{I will re-order the figure.}


%\begin{figure}[!t]
%	\centering
%	\includegraphics[width=1\linewidth]{figures/close-rate-dist.jpg}
%	\caption{The distribution of issue close rates in investigated repositories.}
%	\label{fig:close-rate-dist}
%\end{figure}


\begin{figure}[!t]
	\begin{minipage}[t]{0.333333333\linewidth}
	\centering
	\includegraphics[scale=0.35]{figures/issue_close_rate_distribution_exclude_no_issue_repo_box.pdf}
	\label{fig:close-rate-dist}
	\end{minipage}%
	\begin{minipage}[t]{0.333333333\linewidth}
	\centering
	\includegraphics[scale=0.35]{figures/issue_ignore_rate_distribution_exclude_no_issue_repo_box.pdf}
	\label{fig:ignore-rate-dist}
	\end{minipage}%
	\begin{minipage}[t]{0.333333333\linewidth}
	\centering
	\includegraphics[scale=0.35]{figures/issue_resolve_time_distribution_box.pdf}
	\label{fig:address-time-dist}
	\end{minipage}
	\caption{The distribution of closed issue rates (a), addressing time (b), and ignored issue rates (c) in investigated repositories.}
	\end{figure}

Additionally, we investigate the addressing time of issues, i.e., the time between the creation of an issue and its closure. We found that the average addressing time and standard deviation are 46.94 days and 135.34 days, respectively, in the 16,851 closed issues. However, the median addressing time is only four days, while the longest addressing time is over two years. Figure~\ref{fig:close-rate-dist} (c) illustrates the distribution of the addressing time. We analyze what factors affect the addressing time by considering two features at the repository level: (1) the number of contributors and (2) the total number of issues. We hypothesize that more contributors can reduce the addressing time. However, Spearman's rank correlation~\cite{sedgwick2014spearman} shows that the relationship between the number of contributors and the addressing time is not statistically significant ($p$-value $>0.05$). We also hypothesize that more issues can increase the average addressing time; intuitively, more issues mean more workload on the maintainers. Spearman's rank correlation validates this hypothesis by finding a weak correlation between the number of issues and average addressing time ($p$-value $<0.01$). 
% \jh{Recheck the figure number, maybe talk about the addressing time first, then ignore rate}


\vspace{0.2cm}
\noindent \textbf{How are the issues managed?}
GitHub provides several methods to help maintainers manage issues. 
We consider two methods: (1) using labels to tag issues and (2) assigning issues to specific people to address. In the remaining part of this section, we use the terms \texttt{labeling} and \texttt{assigning} to refer to these two methods, respectively. However, our results show that the two methods are not widely adopted to manage issues in open-source AI repositories. Only 7.81\% of the repositories use labels to categorize issues, and only 5.90\% of repositories assign issues to assignees. We find that 11.42\% of issues are tagged with labels and 16.45\% of issues have assignees, which suggests that \textit{repositories with more issues tend to use these two methods to manage issues more often}.

%Top 10 labels:
%bug                      408
%enhancement              305
%awaiting response        220
%community help wanted    213
%help wanted              210
%good first issue         169
%community discussion     163
%reimplementation         154
%question                 151
%Feature Request          104

\begin{figure}[!t]
	\centering
	\includegraphics[width=1\linewidth]{figures/top_10_labels_box.pdf}
	\caption{
		The distribution of the top 10 most frequently used labels. The numbers represent the labels, where 
		1: \texttt{bug}, 2: \texttt{enhancement}, 3: \texttt{awaiting response}, 4: \texttt{community help wanted}, 5: \texttt{help wanted}, 6: \texttt{good first issue}, 7: \texttt{community discussion}, 8: \texttt{reimplementation}, 9: \texttt{question}, 10: \texttt{feature request}.
		}
	\label{fig:label-occurences}
\end{figure}

%\input{tables/diff_labels.tex}


We analyze the use of \texttt{labeling}. GitHub provides a set of nine default labels, such as \textit{bug}, \textit{duplicate}, \textit{enhancement}, \textit{help wanted}, \textit{invalid}, \textit{question}, \textit{wontfix}, and \textit{good first issue}. One sign of actively managing issues is to define new labels beyond just using the default ones. We find that among all the repositories that use labels, 14.41\% of them describe new labels. Including the default labels, we discover 109 unique labels being used in all the repositories. We then count the occurrences of each label. Table~\ref{fig:label-occurences} shows the ranked list of the most frequently used default labels. 
% Table~\ref{tab:diff_labels} compares the percentage of issues that use different labels in open and closed issues. Due to the limited space, we only show the top 7 most frequent labels. 
% We conduct the Fisher's Exact test~\cite{fisher} to analyze whether there is a significant association between the label of issues and their status. 
% \jh{The number of tables should have the same font, i.e., Table IV instead of Table 4}
% \yz{It's defined by the template.}

% \jh{Do we have something to say about the assigning method here?}


\begin{tcolorbox}[
	colback=green!10,
	colframe=green!50!black,
	colbacktitle={red!50!white},
	coltitle=white,
	fonttitle=\sffamily\bfseries\large,
	title=Answer to RQ2,
	center title,
	]
	In the investigated open-source AI repositories, 67.53\% of issues are closed, and 50\% of issues are closed within four days. Only 7.81\% of the repositories use labels to categorize issues, and only 5.9\% of them assign issues to specific assignees.
	We find that repositories with more contributors do not necessarily address issues faster, but repositories with more issues tend to take a longer time to close issues.
  \end{tcolorbox}


\subsection{RQ3. What are the relationships between different features and the closure of issues in open-source AI repositories?}

\noindent \textbf{Motivation.}
This research question aims to explore the relationships between the various features of issues and the closure of those issues. The answer to this research question can provide practical suggestions for both repository maintainers and issue raisers to improve the management and resolution of issues in open-source AI repositories.


\vspace{0.2cm}
\noindent \textbf{Investigated Features.}
We study the relationships between nine features and the closure of issues. The first eight features are related to the issues and the 9$^{th}$ feature, i.e., \texttt{num-contributors} is related to the repositories. We present the nine features as follows: 

\begin{itemize}
	\item \texttt{has-label}: Whether an issue is labeled. In GitHub, anyone with triage access to a repository can tag issues with labels. A labeled issue usually means that it has been noticed and read by the maintainers and, as a result, it may be more likely to be addressed.
	\item \texttt{has-assignee}: 
    Whether an issue is assigned. Users who have a write access permission to a repository can assign issues to developers. If the issue is assigned, the assignee is responsible for addressing the issue, which may lead to a successful resolution.
	\item \texttt{title-length}: The length of the issue title. An informative title may help the repository maintainers better understand the issue at a glance. Usually, a longer title carries more information that can help catch the maintainers' attention, leading to a faster solution.
	\item \texttt{body-length}: The length of the issue body. Similarly, an informative description can help the repository maintainers better understand the issue. In some guidelines, the issue raisers are encouraged to provide a detailed description of the issue, which can help the maintainers better understand the issue.
	\item \texttt{sentiment}: The sentiment of the issue description. If an issue is impolite, e.g., includes negative sentiment, a repository maintainer may be reluctant to address it.
	\item \texttt{is-English}: Whether the issue is written in English. We noticed that 5.94\% of issues are written in a language other than English, e.g., Chinese, leading the repository maintainers to barely understand these issues to provide their solutions.
	\item \texttt{has-code}: Whether the issue description contains code blocks. Code blocks are useful for the repository maintainers to understand the problem. For example, issue raisers may include code snippets corresponding to error tracks in the issue description. 
	\item \texttt{has-url}: Whether the issue description contains URLs. GitHub uses Markdown syntax to format the issue description. An URL can be used to refer to another issue, a website, or an image. This information may help repository maintainers better understand the issue.
	\item \texttt{num-contributors}: The number of contributors in the repository. Intuitively, the more contributors in a repository, the more likely the issue is to be addressed.
\end{itemize}

\vspace{0.2cm}
\noindent \textbf{Feature Measurement and Statistical Test.}
We obtain issues by querying the GitHub API. We compute the length of the issue title and body by converting them into Python strings and using the results returned by the \texttt{len()} function. Following the previous work~\cite{github-disc}, we employ Senti4SD~\cite{Senti4SD}, a sentiment analysis tool specifically tuned for software engineering tasks, to compute the sentiment of the issue description. Senti4SD produces three sentiment scores: \textit{positive}, \textit{negative}, and \textit{neutral}. As we train Senti4SD on texts written in English, we discard the issues that contain non-English characters. For issues that contain code snippets and URLs that are not relevant to analyzing the sentiment of the issue description, we remove them using regular expressions.


We first tokenize the issue title and body into characters. We then check whether the issue contains characters that are in the American Standard Code for Information Interchange\footnote{\url{https://en.wikipedia.org/wiki/ASCII}} (ASCII). If the issue contains non-ASCII characters, we consider it \texttt{non-English}.
The GitHub issues use Markdown syntax, which allows users to format their issue description using code blocks (i.e., code snippet in \texttt{```} and \texttt{'''}) and URLs (i.e., \texttt{[text](URL)}).
We employ regular expressions to detect whether the issue description contains code blocks or URLs using Markdown syntax. 


We group the issues into two groups: \textit{open} and \textit{closed} issues. For each issue, we measure its eight features, such as \texttt{has-label}, \texttt{has-assignee}, or \texttt{title-length}. The number of contributors in a repository is obtained by querying the GitHub API to fetch metadata about the repository. Then, we adopt the Wilcoxon rank-sum test~\cite{Wilcoxon} to compute the significance of the differences between the feature values of open and closed issues. We also estimate the Cohen's effect sizes $\delta$ of the difference between the open and closed issues. According to the guideline for using the effect size, when $|\delta|$ is less than 0.2, between 0.2 and 0.5, between 0.5 and 0.8, and larger than 0.8, the effect size is considered negligible, small, medium, and large, respectively.

\input{tables/relationships.tex}

Table~\ref{tab:features} shows the results of the Wilcoxon rank-sum test and Cohen's effect sizes $\delta$ for the nine features. Five features, such as \texttt{has-label}, \texttt{has-assignee}, \texttt{body-length}, \texttt{has-code}, and \texttt{num-contributors}, show a statistically significant difference between the open and closed issues. The \texttt{num-contributors} feature also has a medium effect size (M), indicating that the more contributors who join the repository, the better we can address the issues. The remaining features, i.e., \texttt{title-length}, \texttt{sentiment}, \texttt{is-English}, and \texttt{has-url}, indicate that there is no significant difference between the open and closed issues. 

% \vspace{0.2cm}
% \noindent \textbf{Suggestions to the repository maintainers and issue raisers.}
We classify the issue-related features into two groups, i.e., features related to the practices of raising issues and features related to the practices of handling issues. Our results show that issues are more likely to be closed if they have assignees. Moreover, issues with labels are more likely to be closed than those without labels. The effect sizes of \texttt{has-label} and \texttt{has-assignee} are small and medium, respectively. 
The two features, i.e., \texttt{body-length} and \texttt{has-code}, are related to how users raise issues. Although their effect sizes are smaller than that of \texttt{has-label} and \texttt{has-assignee}, they still demonstrate statistically significant differences. 



% \jh{I feel that we need to have a specific example to demonstrate our suggestion here.}


\begin{tcolorbox}[
	colback=green!10,
	colframe=green!50!black,
	colbacktitle={red!50!white},
	coltitle=white,
	fonttitle=\sffamily\bfseries\large,
	title=Answer to RQ3,
	center title,
	]
	Our results show that issues are more likely to be closed if they have assignees. Moreover, issues with labels are more likely to be closed than those without labels. Besides, issues with more details (e.g., longer descriptions and code blocks) have a higher chance of being closed.
  \end{tcolorbox}
