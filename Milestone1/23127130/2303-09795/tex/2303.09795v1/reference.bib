@article{ma_tosem,
  author     = {Ma, Wei and Papadakis, Mike and Tsakmalis, Anestis and Cordy, Maxime and Traon, Yves Le},
  title      = {Test Selection for Deep Learning Systems},
  year       = {2021},
  issue_date = {April 2021},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {30},
  number     = {2},
  issn       = {1049-331X},
  url        = {https://doi.org/10.1145/3417330},
  doi        = {10.1145/3417330},
  journal    = {ACM Transactions on Software Engineering and Methodology (TOSEM)},
  month      = {jan},
  articleno  = {13},
  numpages   = {22},
  keywords   = {Deep learning testing, software testing, software engineering}
}

@inproceedings{wei2015submodularity,
  title        = {Submodularity in data subset selection and active learning},
  author       = {Wei, Kai and Iyer, Rishabh and Bilmes, Jeff},
  booktitle    = {International Conference on Machine Learning},
  pages        = {1954--1963},
  year         = {2015},
  organization = {PMLR}
}

@article{T5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1--67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@inproceedings{9825895,
  author    = {Henkel, Jordan and Ramakrishnan, Goutham and Wang, Zi and Albarghouthi, Aws and Jha, Somesh and Reps, Thomas},
  booktitle = {2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)},
  title     = {Semantic Robustness of Models of Source Code},
  year      = {2022},
  volume    = {},
  number    = {},
  pages     = {526-537},
  doi       = {10.1109/SANER53432.2022.00070}
}

@inproceedings{coffee,
  author    = {Nguyen, Phuong T. and Di Sipio, Claudio and Di Rocco, Juri and Di Penta, Massimiliano and Di Ruscio, Davide},
  booktitle = {2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  title     = {Adversarial Attacks to API Recommender Systems: Time to Wake Up and Smell the Coffee?},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {253-265},
  doi       = {10.1109/ASE51524.2021.9678946}
}

@inproceedings{DBLP:conf/sp/PearceA0DK22,
  author    = {Hammond Pearce and
               Baleegh Ahmad and
               Benjamin Tan and
               Brendan Dolan{-}Gavitt and
               Ramesh Karri},
  title     = {Asleep at the Keyboard? Assessing the Security of GitHub Copilot's
               Code Contributions},
  booktitle = {43rd {IEEE} Symposium on Security and Privacy, {SP} 2022, San Francisco,
               CA, USA, May 22-26, 2022},
  pages     = {754--768},
  publisher = {{IEEE}},
  year      = {2022},
  url       = {https://doi.org/10.1109/SP46214.2022.9833571},
  doi       = {10.1109/SP46214.2022.9833571},
  timestamp = {Wed, 07 Dec 2022 23:10:41 +0100},
  biburl    = {https://dblp.org/rec/conf/sp/PearceA0DK22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{9956690,
  author    = {G. Ramakrishnan and A. Albarghouthi},
  booktitle = {2022 26th International Conference on Pattern Recognition (ICPR)},
  title     = {Backdoors in Neural Models of Source Code},
  year      = {2022},
  volume    = {},
  issn      = {},
  pages     = {2892-2899},
  abstract  = {Deep neural networks are vulnerable to a range of adversaries. A particularly pernicious class of vulnerabilities are backdoors, where model predictions diverge in the presence of subtle triggers in inputs. An attacker can implant a backdoor by poisoning the training data to yield a desired target prediction on triggered inputs. We study backdoors in the context of deep-learning for source code. (1) We define a range of backdoor classes for source-code tasks and install backdoors using dataset poisoning. (2) We adapt and improve recent algorithms from robust statistics for our setting, showing that backdoors leave a spectral signature in the learned representation of source code, thus enabling detection of poisoned data. (3) We conduct a thorough evaluation on different architectures and languages, showing the ease of injecting backdoors and our ability to eliminate them.},
  keywords  = {deep learning;codes;source coding;neural networks;training data;implants;predictive models},
  doi       = {10.1109/ICPR56361.2022.9956690},
  url       = {https://doi.ieeecomputersociety.org/10.1109/ICPR56361.2022.9956690},
  publisher = {IEEE Computer Society},
  address   = {Los Alamitos, CA, USA},
  month     = {aug}
}

@inproceedings{7102609,
  author    = {Kochhar, Pavneet Singh and Thung, Ferdian and Nagappan, Nachiappan and Zimmermann, Thomas and Lo, David},
  booktitle = {2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)},
  title     = {Understanding the Test Automation Culture of App Developers},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {1-10},
  doi       = {10.1109/ICST.2015.7102609}
}


@inproceedings{shaowei-icst,
  author    = {Wang, Shaowei and Lo, David},
  title     = {Version History, Similar Report, and Structure: Putting Them Together for Improved Bug Localization},
  year      = {2014},
  isbn      = {9781450328791},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2597008.2597148},
  doi       = {10.1145/2597008.2597148},
  abstract  = {During the evolution of a software system, a large number of bug reports are submitted. Locating the source code files that need to be fixed to resolve the bugs is a challenging problem. Thus, there is a need for a technique that can automatically figure out these buggy files. A number of bug localization solutions that take in a bug report and output a ranked list of files sorted based on their likelihood to be buggy have been proposed in the literature. However, the accuracy of these tools still need to be improved. In this paper, to address this need, we propose AmaLgam, a new method for locating relevant buggy files that puts together version history, similar reports, and structure. To do this, AmaLgam integrates a bug prediction technique used in Google which analyzes version history, with a bug localization technique named BugLocator which analyzes similar reports from bug report system, and the state-of-the-art bug localization technique BLUiR which considers structure. We perform a large-scale experiment on four open source projects, namely AspectJ, Eclipse, SWT and ZXing to localize more than 3,000 bugs. Compared with a history-aware bug localization solution of Sisman and Kak, our approach achieves a 46.1% improvement in terms of mean average precision (MAP). Compared with BugLocator, our approach achieves a 24.4% improvement in terms of MAP. Compared with BLUiR, our approach achieves a 16.4% improvement in terms of MAP.},
  booktitle = {Proceedings of the 22nd International Conference on Program Comprehension},
  pages     = {53–63},
  numpages  = {11},
  keywords  = {Similar Report, Version History, Structure, Bug Localization},
  location  = {Hyderabad, India},
  series    = {ICPC 2014}
}

@inproceedings{CoProtector,
  author    = {Sun, Zhensu and Du, Xiaoning and Song, Fu and Ni, Mingze and Li, Li},
  title     = {CoProtector: Protect Open-Source Code against Unauthorized Training Usage with Data Poisoning},
  year      = {2022},
  isbn      = {9781450390965},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  doi       = {10.1145/3485447.3512225},
  abstract  = { Github Copilot, trained on billions of lines of public code, has recently become the buzzword in the computer science research and practice community. Although it is designed to help developers implement safe and effective code with powerful intelligence, practitioners and researchers raise concerns about its ethical and security problems, e.g., should the copyleft licensed code be freely leveraged or insecure code be considered for training in the first place? These problems pose a significant impact on Copilot and other similar products that aim to learn knowledge from large-scale open-source code through deep learning models, which are inevitably on the rise with the fast development of artificial intelligence. To mitigate such impacts, we argue that there is a need to invent effective mechanisms for protecting open-source code from being exploited by deep learning models. Here, we design and implement a prototype, CoProtector, which utilizes data poisoning techniques to arm source code repositories for defending against such exploits. Our large-scale experiments empirically show that CoProtector is effective in achieving its purpose, significantly reducing the performance of Copilot-like deep learning models while being able to stably reveal the secretly embedded watermark backdoors. },
  booktitle = {Proceedings of the ACM Web Conference 2022},
  pages     = {652–660},
  numpages  = {9},
  keywords  = {open-source code, data poisoning, dataset protection, deep learning},
  location  = {Virtual Event, Lyon, France},
  series    = {WWW '22}
}

@article{landis1977measurement,
  title     = {The measurement of observer agreement for categorical data},
  author    = {Landis, J Richard and Koch, Gary G},
  journal   = {biometrics},
  pages     = {159--174},
  year      = {1977},
  publisher = {JSTOR}
}

@article{husain2019codesearchnet,
  title   = {{CodeSearchNet} challenge: Evaluating the state of semantic code search},
  author  = {Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},
  journal = {arXiv preprint arXiv:1909.09436},
  year    = {2019}
}

@misc{advdoor,
  doi       = {10.48550/ARXIV.2301.02496},
  url       = {https://arxiv.org/abs/2301.02496},
  author    = {Yang, Zhou and Xu, Bowen and Zhang, Jie M. and Kang, Hong Jin and Shi, Jieke and He, Junda and Lo, David},
  title     = {Stealthy Backdoor Attack for Code Models},
  publisher = {arXiv},
  year      = {2023},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{CodePoisoner,
  doi       = {10.48550/ARXIV.2210.17029},
  url       = {https://arxiv.org/abs/2210.17029},
  author    = {Li, Jia and Li, Zhuo and Zhang, Huangzhao and Li, Ge and Jin, Zhi and Hu, Xing and Xia, Xin},
  keywords  = {Software Engineering (cs.SE), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Poison Attack and Defense on Deep Source Code Processing Models},
  publisher = {arXiv},
  year      = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{you-see,
  author    = {Wan, Yao and Zhang, Shijie and Zhang, Hongyu and Sui, Yulei and Xu, Guandong and Yao, Dezhong and Jin, Hai and Sun, Lichao},
  title     = {You See What I Want You to See: Poisoning Vulnerabilities in Neural Code Search},
  year      = {2022},
  isbn      = {9781450394130},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3540250.3549153},
  doi       = {10.1145/3540250.3549153},
  abstract  = {Searching and reusing code snippets from open-source software repositories based on natural-language queries can greatly improve programming productivity.Recently, deep-learning-based approaches have become increasingly popular for code search. Despite substantial progress in training accurate models of code search, the robustness of these models has received little attention so far. In this paper, we aim to study and understand the security and robustness of code search models by answering the following question: Can we inject backdoors into deep-learning-based code search models? If so, can we detect poisoned data and remove these backdoors? This work studies and develops a series of backdoor attacks on the deep-learning-based models for code search, through data poisoning. We first show that existing models are vulnerable to data-poisoning-based backdoor attacks. We then introduce a simple yet effective attack on neural code search models by poisoning their corresponding training dataset. Moreover, we demonstrate that attacks can also influence the ranking of the code search results by adding a few specially-crafted source code files to the training corpus. We show that this type of backdoor attack is effective for several representative deep-learning-based code search systems, and can successfully manipulate the ranking list of searching results. Taking the bidirectional RNN-based code search system as an example, the normalized ranking of the target candidate can be significantly raised from top 50\% to top 4.43\%, given a query containing an attacker targeted word, e.g., file. To defend a model against such attack, we empirically examine an existing popular defense strategy and evaluate its performance. Our results show the explored defense strategy is not yet effective in our proposed backdoor attack for code search systems.},
  booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages     = {1233–1245},
  numpages  = {13},
  keywords  = {Code search, software vulnerability, deep learning, backdoor attack, data poisoning},
  location  = {Singapore, Singapore},
  series    = {ESEC/FSE 2022}
}

@inproceedings{263874,
  author    = {Roei Schuster and Congzheng Song and Eran Tromer and Vitaly Shmatikov},
  title     = {You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion},
  booktitle = {30th USENIX Security Symposium (USENIX Security 21)},
  year      = {2021},
  isbn      = {978-1-939133-24-3},
  pages     = {1559--1575},
  publisher = {USENIX Association},
  month     = aug
}

@inproceedings{wang2021codet5,
  title     = {CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  author    = {Yue Wang and Weishi Wang and Shafiq Joty and Steven C.H. Hoi},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021},
  year      = {2021}
}

@inproceedings{plbart,
  title     = {Unified Pre-training for Program Understanding and Generation},
  author    = {Ahmad, Wasi  and
               Chakraborty, Saikat  and
               Ray, Baishakhi  and
               Chang, Kai-Wei},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = jun,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  pages     = {2655--2668}
}

@inproceedings{9825884,
  author    = {C. Yang and B. Xu and J. Khan and G. Uddin and D. Han and Z. Yang and D. Lo},
  booktitle = {2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)},
  title     = {Aspect-Based API Review Classification: How Far Can Pre-Trained Transformer Model Go?},
  year      = {2022},
  volume    = {},
  issn      = {1534-5351},
  pages     = {385-395},
  abstract  = {APIs (Application Programming Interfaces) are reusable software libraries and are building blocks for modern rapid software development. Previous research shows that programmers frequently share and search for reviews of APIs on the mainstream software question and answer (Q&amp;#x0026;A) platforms like Stack Overflow, which motivates researchers to design tasks and approaches related to process API reviews automatically. Among these tasks, classifying API reviews into different aspects (e.g., performance or security), which is called the aspect-based API review classification, is of great importance. The current state-of-the-art (SOTA) solution to this task is based on the traditional machine learning algorithm. Inspired by the great success achieved by pre-trained models on many software engineering tasks, this study fine-tunes six pre-trained models for the aspect-based API review classification task and compares them with the current SOTA solution on an API review benchmark collected by Uddin et al. The investigated models include four models (BERT, RoBERTa, ALBERT and XLNet) that are pre-trained on natural languages, BERTOverflow that is pre-trained on text corpus extracted from posts on Stack Overflow, and CosSensBERT that is designed for handling imbalanced data. The results show that all the six fine-tuned models outperform the traditional machine learning-based tool. More specifically, the improvement on the F1-score ranges from 21.0&amp;#x0025; to 30.2&amp;#x0025;. We also find that BERTOverflow, a model pre-trained on the corpus from Stack Overflow, does not show better performance than BERT. The result also suggests that CosSensBERT also does not exhibit better performance than BERT in terms of F1, but it is still worthy of being considered as it achieves better performance on MCC and AUC.},
  keywords  = {software libraries;machine learning algorithms;bit error rate;natural languages;transformers;software;data models},
  doi       = {10.1109/SANER53432.2022.00054},
  url       = {https://doi.ieeecomputersociety.org/10.1109/SANER53432.2022.00054},
  publisher = {IEEE Computer Society},
  address   = {Los Alamitos, CA, USA},
  month     = {mar}
}


@inproceedings{PTM4TAG,
  author    = {He, Junda and Xu, Bowen and Yang, Zhou and Han, DongGyun and Yang, Chengran and Lo, David},
  title     = {PTM4Tag: Sharpening Tag Recommendation of Stack Overflow Posts with Pre-Trained Models},
  year      = {2022},
  isbn      = {9781450392983},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3524610.3527897},
  doi       = {10.1145/3524610.3527897},
  abstract  = {Stack Overflow is often viewed as one of the most influential Software Question &amp; Answer (SQA) websites, containing millions of programming-related questions and answers. Tags play a critical role in efficiently structuring the contents in Stack Overflow and are vital to support a range of site operations, e.g., querying relevant contents. Poorly selected tags often introduce extra noise and redundancy, which raises problems like tag synonym and tag explosion. Thus, an automated tag recommendation technique that can accurately recommend high-quality tags is desired to alleviate the problems mentioned above.Inspired by the recent success of pre-trained language models (PTMs) in natural language processing (NLP), we present PTM4Tag, a tag recommendation framework for Stack Overflow posts that utilize PTMs with a triplet architecture, which models the components of a post, i.e., Title, Description, and Code with independent language models. To the best of our knowledge, this is the first work that leverages PTMs in the tag recommendation task of SQA sites. We comparatively evaluate the performance of PTM4Tag based on five popular pre-trained models: BERT, RoBERTa, ALBERT, CodeBERT, and BERTOverflow. Our results show that leveraging CodeBERT, a software engineering (SE) domain-specific PTM in PTM4Tag achieves the best performance among the five considered PTMs and outperforms the state-of-the-art Convolutional Neural Network-based approach by a large margin in terms of average Precision@k, Recall@k, and F1-score@k. We conduct an ablation study to quantify the contribution of a post's constituent components (Title, Description, and Code Snippets) to the performance of PTM4Tag. Our results show that Title is the most important in predicting the most relevant tags, and utilizing all the components achieves the best performance.},
  booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension},
  pages     = {1–11},
  numpages  = {11},
  keywords  = {transformer, tag recommendation, pre-trained models},
  location  = {Virtual Event},
  series    = {ICPC '22}
}

@inproceedings{acsac2022gong,
  author    = {Gong, Chen and Yang, Zhou and Bai, Yunpeng and Shi, Jieke and Sinha, Arunesh and Xu, Bowen and Lo, David and Hou, Xinwen and Fan, Guoliang},
  title     = {Curiosity-Driven and Victim-Aware Adversarial Policies},
  year      = {2022},
  isbn      = {9781450397599},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3564625.3564636},
  doi       = {10.1145/3564625.3564636},
  abstract  = {Recent years have witnessed great potential in applying Deep Reinforcement Learning (DRL) in various challenging applications, such as autonomous driving, nuclear fusion control, complex game playing, etc. However, recently researchers have revealed that deep reinforcement learning models are vulnerable to adversarial attacks: malicious attackers can train adversarial policies to tamper with the observations of a well-trained victim agent, the latter of which fails dramatically when faced with such an attack. Understanding and improving the adversarial robustness of deep reinforcement learning is of great importance in enhancing the quality and reliability of a wide range of DRL-enabled systems. In this paper, we develop curiosity-driven and victim-aware adversarial policy training, a novel method that can more effectively exploit the defects of victim agents. To be victim-aware, we build a surrogate network that can approximate the state-value function of a black-box victim to collect the victim’s information. Then we propose a curiosity-driven approach, which encourages an adversarial policy to utilize the information from the hidden layer of the surrogate network to exploit the vulnerability of victims efficiently. Extensive experiments demonstrate that our proposed method outperforms or achieves a similar level of performance as the current state-of-the-art across multiple environments. We perform an ablation study to emphasize the benefits of utilizing the approximated victim information. Further analysis suggests that our method is harder to defend against a commonly used defensive strategy, which calls attention to more effective protection on the systems using DRL.},
  booktitle = {Proceedings of the 38th Annual Computer Security Applications Conference},
  pages     = {186–200},
  numpages  = {15},
  keywords  = {Adversarial Attack, Reinforcement Learning, Curiosity Mechanism},
  location  = {Austin, TX, USA},
  series    = {ACSAC '22}
}

@inproceedings{CodeBERT,
  title     = {{C}ode{BERT}: A Pre-Trained Model for Programming and Natural Languages},
  author    = {Feng, Zhangyin  and
               Guo, Daya  and
               Tang, Duyu  and
               Duan, Nan  and
               Feng, Xiaocheng  and
               Gong, Ming  and
               Shou, Linjun  and
               Qin, Bing  and
               Liu, Ting  and
               Jiang, Daxin  and
               Zhou, Ming},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
  month     = nov,
  year      = {2020},
  publisher = {Association for Computational Linguistics},
  pages     = {1536--1547},
  abstract  = {We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both {``}bimodal{''} data of NL-PL pairs and {``}unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.}
}

@inproceedings{GraphCodeBERT,
  author    = {Daya Guo and
               Shuo Ren and
               Shuai Lu and
               Zhangyin Feng and
               Duyu Tang and
               Shujie Liu and
               Long Zhou and
               Nan Duan and
               Alexey Svyatkovskiy and
               Shengyu Fu andz
               Michele Tufano and
               Shao Kun Deng and
               Colin B. Clement and
               Dawn Drain and
               Neel Sundaresan and
               Jian Yin and
               Daxin Jiang and
               Ming Zhou},
  title     = {GraphCodeBERT: Pre-training Code Representations with Data Flow},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  year      = {2021}
}


@inproceedings{codebackdoor,
  author    = {G. Ramakrishnan and A. Albarghouthi},
  booktitle = {2022 26th International Conference on Pattern Recognition (ICPR)},
  title     = {Backdoors in Neural Models of Source Code},
  year      = {2022},
  volume    = {},
  issn      = {},
  pages     = {2892-2899},
  abstract  = {Deep neural networks are vulnerable to a range of adversaries. A particularly pernicious class of vulnerabilities are backdoors, where model predictions diverge in the presence of subtle triggers in inputs. An attacker can implant a backdoor by poisoning the training data to yield a desired target prediction on triggered inputs. We study backdoors in the context of deep-learning for source code. (1) We define a range of backdoor classes for source-code tasks and install backdoors using dataset poisoning. (2) We adapt and improve recent algorithms from robust statistics for our setting, showing that backdoors leave a spectral signature in the learned representation of source code, thus enabling detection of poisoned data. (3) We conduct a thorough evaluation on different architectures and languages, showing the ease of injecting backdoors and our ability to eliminate them.},
  keywords  = {deep learning;codes;source coding;neural networks;training data;implants;predictive models},
  doi       = {10.1109/ICPR56361.2022.9956690},
  url       = {https://doi.ieeecomputersociety.org/10.1109/ICPR56361.2022.9956690},
  publisher = {IEEE Computer Society},
  address   = {Los Alamitos, CA, USA},
  month     = {aug}
}


@article{LI2023165,
  title    = {A comparative study of adversarial training methods for neural models of source code},
  journal  = {Future Generation Computer Systems},
  volume   = {142},
  pages    = {165-181},
  year     = {2023},
  issn     = {0167-739X},
  doi      = {https://doi.org/10.1016/j.future.2022.12.030},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167739X22004332},
  author   = {Zhen Li and Xiang Huang and Yangrui Li and Guenevere Chen},
  keywords = {Adversarial training, Robustness, Source code, Comparative study},
  abstract = {Adversarial training has been employed by researchers to protect AI models of source code. However, it is still unknown how adversarial training methods in this field compare to each other in effectiveness and robustness. This study surveys and investigates existing adversarial training methods, and conducts experiments to evaluate these neural models’ performance in the domain of source code. First, we examine the process of adversarial training to identify four dimensions that could be used to classify different adversarial training methods into five categories, which are Mixing Directly, Composite Loss, Adversarial Fine-tuning, Min–max + Composite Loss, and Min–max. Second, we conduct empirical evaluations of these classified adversarial training methods under two tasks (i.e., code summarization and code authorship attribution) to determine their performance of effectiveness and robustness. Experimental results indicate that the performance of certain combinations of adversarial training techniques (i.e., min–max with composite loss, or directly-sample with ordinary loss) would be much better than other combinations or other techniques used alone. Our experiments also reveal that the model’s robustness of defensive methods can be enhanced by using diverse input data for adversarial training, and that the number of fine-tuning epochs has little or no impact on model’s performance.}
}

@article{9916170,
  author  = {Wei, Moshi and Huang, Yuchao and Yang, Jinqiu and Wang, Junjie and Wang, Song},
  journal = {IEEE Transactions on Reliability},
  title   = {CoCoFuzzing: Testing Neural Code Models With Coverage-Guided Fuzzing},
  year    = {2022},
  volume  = {},
  number  = {},
  pages   = {1-14},
  doi     = {10.1109/TR.2022.3208239}
}

@article{Yefet2020,
  archiveprefix = {arXiv},
  arxivid       = {1910.07517},
  author        = {Yefet, Noam and Alon, Uri and Yahav, Eran},
  issn          = {24751421},
  journal       = {Proceedings of the ACM on Programming Languages},
  keywords      = {Adversarial Attacks,Neural Models of Code,Targeted Attacks},
  number        = {OOPSLA},
  title         = {{Adversarial examples for models of code}},
  volume        = {4},
  year          = {2020}
}

@misc{goodname,
  doi       = {10.48550/ARXIV.2211.15844},
  url       = {https://arxiv.org/abs/2211.15844},
  author    = {Yang, Guang and Zhou, Yu and Yang, Wenhua and Yue, Tao and Chen, Xiang and Chen, Taolue},
  keywords  = {Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {How Important are Good Method Names in Neural Code Generation? A Model Robustness Perspective},
  publisher = {arXiv},
  year      = {2022},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}


@article{rabin2021generalizability,
  title     = {On the generalizability of Neural Program Models with respect to semantic-preserving program transformations},
  author    = {Rabin, Md Rafiqul Islam and Bui, Nghi DQ and Wang, Ke and Yu, Yijun and Jiang, Lingxiao and Alipour, Mohammad Amin},
  journal   = {Information and Software Technology},
  volume    = {135},
  pages     = {106552},
  year      = {2021},
  publisher = {Elsevier}
}

@misc{codeattack,
  author    = {Jha, Akshita and Reddy, Chandan K.},
  keywords  = {Computation and Language (cs.CL), Cryptography and Security (cs.CR), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {CodeAttack: Code-based Adversarial Attacks for Pre-Trained Programming Language Models},
  publisher = {arXiv},
  year      = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}


@inproceedings{9438605,
  author    = {Maryam Vahdat Pour and
               Zhuo Li and
               Lei Ma and
               Hadi Hemmati},
  title     = {A Search-Based Testing Framework for Deep Neural Networks of Source
               Code Embedding},
  booktitle = {14th {IEEE} Conference on Software Testing, Verification and Validation,
               {ICST} 2021, Porto de Galinhas, Brazil, April 12-16, 2021},
  pages     = {36--46},
  publisher = {{IEEE}},
  year      = {2021}
}

@inproceedings{NICHE,
  author    = {Widyasari, Ratnadira and Yang, Zhou and Thung, Ferdian and Sim, Sheng Qin and Wee, Fiona and Lok, Camellia and Phan, Jack and Qi, Haodi and Tan, Constance and Tay, Qijin and Lo, David},
  keywords  = {Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {NICHE: A Curated Dataset of Engineered Machine Learning Projects in Python},
  booktitle = {Proceedings of the 20th International Conference on Mining Software Repositories},
  numpages  = {5},
  series    = {MSR '23}
}


@inproceedings{9678706,
  author    = {Applis, Leonhard and Panichella, Annibale and van Deursen, Arie},
  booktitle = {2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  title     = {Assessing Robustness of ML-Based Program Analysis Tools using Metamorphic Program Transformations},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {1377-1381},
  doi       = {10.1109/ASE51524.2021.9678706}
}

@article{MHM,
  title   = {Generating Adversarial Examples for Holding Robustness of Source Code Processing Models},
  volume  = {34},
  number  = {01},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author  = {Zhang, Huangzhao and Li, Zhuo and Li, Ge and Ma, Lei and Liu, Yang and Jin, Zhi},
  year    = {2020},
  month   = {Apr.},
  pages   = {1169-1176}
}

@article{Epresentation2021,
  author   = {Shashank Srikant and
              Sijia Liu and
              Tamara Mitrovska and
              Shiyu Chang and
              Quanfu Fan and
              Gaoyuan Zhang and
              Una{-}May O'Reilly},
  journal  = {ICLR},
  keywords = {minecraft. programming input,programming-learning},
  pages    = {209--226},
  title    = {{Generating Adversarial Computer Programs using Optimized Obfuscations}},
  volume   = {16},
  year     = {2021}
}

@misc{CFSA,
  doi       = {10.48550/ARXIV.2302.08018},
  url       = {https://arxiv.org/abs/2302.08018},
  author    = {Wang, Zichong and Zhou, Yang and Qiu, Meikang and Haque, Israat and Brown, Laura and He, Yi and Wang, Jianwu and Lo, David and Zhang, Wenbin},
  keywords  = {Software Engineering (cs.SE), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Towards Fair Machine Learning Software: Understanding and Addressing Model Bias Through Counterfactual Thinking},
  publisher = {arXiv},
  year      = {2023},
  copyright = {Creative Commons Zero v1.0 Universal}
}



@inproceedings{alert,
  author    = {Yang, Zhou and Shi, Jieke and He, Junda and Lo, David},
  title     = {Natural Attack for Pre-Trained Models of Code},
  year      = {2022},
  isbn      = {9781450392211},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3510003.3510146},
  doi       = {10.1145/3510003.3510146},
  abstract  = {Pre-trained models of code have achieved success in many important software engineering tasks. However, these powerful models are vulnerable to adversarial attacks that slightly perturb model inputs to make a victim model produce wrong outputs. Current works mainly attack models of code with examples that preserve operational program semantics but ignore a fundamental requirement for adversarial example generation: perturbations should be natural to human judges, which we refer to as naturalness requirement.In this paper, we propose ALERT (Naturalness Aware Attack), a black-box attack that adversarially transforms inputs to make victim models produce wrong outputs. Different from prior works, this paper considers the natural semantic of generated examples at the same time as preserving the operational semantic of original inputs. Our user study demonstrates that human developers consistently consider that adversarial examples generated by ALERT are more natural than those generated by the state-of-the-art work by Zhang et al. that ignores the naturalness requirement. On attacking CodeBERT, our approach can achieve attack success rates of 53.62\%, 27.79\%, and 35.78\% across three downstream tasks: vulnerability prediction, clone detection and code authorship attribution. On GraphCodeBERT, our approach can achieve average success rates of 76.95\%, 7.96\% and 61.47\% on the three tasks. The above outperforms the baseline by 14.07\% and 18.56\% on the two pre-trained models on average. Finally, we investigated the value of the generated adversarial examples to harden victim models through an adversarial fine-tuning procedure and demonstrated the accuracy of CodeBERT and GraphCodeBERT against ALERT-generated adversarial examples increased by 87.59\% and 92.32\%, respectively.},
  booktitle = {Proceedings of the 44th International Conference on Software Engineering},
  pages     = {1482–1493},
  numpages  = {12},
  keywords  = {genetic algorithm, pre-trained models, adversarial attack},
  location  = {Pittsburgh, Pennsylvania},
  series    = {ICSE '22}
}

@article{CodeXGLUE,
  author        = {Shuai Lu and
                   Daya Guo and
                   Shuo Ren and
                   Junjie Huang and
                   Alexey Svyatkovskiy and
                   Ambrosio Blanco and
                   Colin B. Clement and
                   Dawn Drain and
                   Daxin Jiang and
                   Duyu Tang and
                   Ge Li and
                   Lidong Zhou and
                   Linjun Shou and
                   Long Zhou and
                   Michele Tufano and
                   Ming Gong and
                   Ming Zhou and
                   Nan Duan and
                   Neel Sundaresan and
                   Shao Kun Deng and
                   Shengyu Fu and
                   Shujie Liu},
  title         = {CodeXGLUE: {A} Machine Learning Benchmark Dataset for Code Understanding
                   and Generation},
  journal       = {CoRR},
  volume        = {abs/2102.04664},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2102.04664}
}

@article{codex,
  author     = {Mark Chen and
                Jerry Tworek and
                Heewoo Jun and
                Qiming Yuan and
                Henrique Ponde de Oliveira Pinto and
                Jared Kaplan and
                Harrison Edwards and
                Yuri Burda and
                Nicholas Joseph and
                Greg Brockman and
                Alex Ray and
                Raul Puri and
                Gretchen Krueger and
                Michael Petrov and
                Heidy Khlaaf and
                Girish Sastry and
                Pamela Mishkin and
                Brooke Chan and
                Scott Gray and
                Nick Ryder and
                Mikhail Pavlov and
                Alethea Power and
                Lukasz Kaiser and
                Mohammad Bavarian and
                Clemens Winter and
                Philippe Tillet and
                Felipe Petroski Such and
                Dave Cummings and
                Matthias Plappert and
                Fotios Chantzis and
                Elizabeth Barnes and
                Ariel Herbert{-}Voss and
                William Hebgen Guss and
                Alex Nichol and
                Alex Paino and
                Nikolas Tezak and
                Jie Tang and
                Igor Babuschkin and
                Suchir Balaji and
                Shantanu Jain and
                William Saunders and
                Christopher Hesse and
                Andrew N. Carr and
                Jan Leike and
                Joshua Achiam and
                Vedant Misra and
                Evan Morikawa and
                Alec Radford and
                Matthew Knight and
                Miles Brundage and
                Mira Murati and
                Katie Mayer and
                Peter Welinder and
                Bob McGrew and
                Dario Amodei and
                Sam McCandlish and
                Ilya Sutskever and
                Wojciech Zaremba},
  title      = {Evaluating Large Language Models Trained on Code},
  journal    = {CoRR},
  volume     = {abs/2107.03374},
  year       = {2021},
  url        = {https://arxiv.org/abs/2107.03374},
  eprinttype = {arXiv},
  eprint     = {2107.03374},
  timestamp  = {Tue, 20 Jul 2021 15:08:33 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2107-03374.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{big_code,
  author    = {R. Karampatsis and H. Babii and R. Robbes and C. Sutton and A. Janes},
  booktitle = {2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)},
  title     = {Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code},
  year      = {2020},
  volume    = {},
  issn      = {},
  pages     = {1073-1085},
  abstract  = {Statistical language modeling techniques have successfully been applied to large source code corpora, yielding a variety of new software development tools, such as tools for code suggestion, improving readability, and API migration. A major issue with these techniques is that code introduces new vocabulary at a far higher rate than natural language, as new identifier names proliferate. Both large vocabularies and out-of-vocabulary issues severely affect Neural Language Models (NLMs) of source code, degrading their performance and rendering them unable to scale. In this paper, we address this issue by: 1) studying how various modelling choices impact the resulting vocabulary on a large-scale corpus of 13,362 projects; 2) presenting an open vocabulary source code NLM that can scale to such a corpus, 100 times larger than in previous work; and 3) showing that such models outperform the state of the art on three distinct code corpora (Java, C, Python). To our knowledge, these are the largest NLMs for code that have been reported. All datasets, code, and trained models used in this work are publicly available.},
  keywords  = {training;vocabulary;adaptation models;computer bugs;predictive models;tools;software engineering},
  doi       = {},
  url       = {https://doi.ieeecomputersociety.org/},
  publisher = {IEEE Computer Society},
  address   = {Los Alamitos, CA, USA},
  month     = {oct}
}


@inproceedings{10.1145/3106237.3106290,
  author    = {Hellendoorn, Vincent J. and Devanbu, Premkumar},
  title     = {Are Deep Neural Networks the Best Choice for Modeling Source Code?},
  year      = {2017},
  isbn      = {9781450351058},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3106237.3106290},
  doi       = {10.1145/3106237.3106290},
  abstract  = {Current statistical language modeling techniques, including deep-learning based models, have proven to be quite effective for source code. We argue here that the special properties of source code can be exploited for further improvements. In this work, we enhance established language modeling approaches to handle the special challenges of modeling source code, such as: frequent changes, larger, changing vocabularies, deeply nested scopes, etc. We present a fast, nested language modeling toolkit specifically designed for software, with the ability to add &amp; remove text, and mix &amp; swap out many models. Specifically, we improve upon prior cache-modeling work and present a model with a much more expansive, multi-level notion of locality that we show to be well-suited for modeling software. We present results on varying corpora in comparison with traditional N-gram, as well as RNN, and LSTM deep-learning language models, and release all our source code for public use. Our evaluations suggest that carefully adapting N-gram models for source code can yield performance that surpasses even RNN and LSTM based deep-learning models.},
  booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
  pages     = {763–773},
  numpages  = {11},
  keywords  = {software tools, naturalness, language models},
  location  = {Paderborn, Germany},
  series    = {ESEC/FSE 2017}
}

@inproceedings{javacorpus,
  author    = {M. Allamanis and C. Sutton},
  booktitle = {2013 10th IEEE Working Conference on Mining Software Repositories (MSR 2013)},
  title     = {Mining source code repositories at massive scale using language modeling},
  year      = {2013},
  volume    = {},
  issn      = {2160-1852},
  pages     = {207-216},
  abstract  = {The tens of thousands of high-quality open source software projects on the Internet raise the exciting possibility of studying software development by finding patterns across truly large source code repositories. This could enable new tools for developing code, encouraging reuse, and navigating large projects. In this paper, we build the first giga-token probabilistic language model of source code, based on 352 million lines of Java. This is 100 times the scale of the pioneering work by Hindle et al. The giga-token model is significantly better at the code suggestion task than previous models. More broadly, our approach provides a new “lens” for analyzing software projects, enabling new complexity metrics based on statistical analysis of large corpora. We call these metrics data-driven complexity metrics. We propose new metrics that measure the complexity of a code module and the topical centrality of a module to a software project. In particular, it is possible to distinguish reusable utility classes from classes that are part of a program&#x27;s core logic based solely on general information theoretic criteria.},
  keywords  = {training;entropy;java;measurement;predictive models;complexity theory;software},
  doi       = {10.1109/MSR.2013.6624029},
  url       = {https://doi.ieeecomputersociety.org/10.1109/MSR.2013.6624029},
  publisher = {IEEE Computer Society},
  address   = {Los Alamitos, CA, USA},
  month     = {may}
}


@inproceedings{10.1145/3236024.3236053,
  author    = {Chen, Junjie and Lou, Yiling and Zhang, Lingming and Zhou, Jianyi and Wang, Xiaoleng and Hao, Dan and Zhang, Lu},
  title     = {Optimizing Test Prioritization via Test Distribution Analysis},
  year      = {2018},
  isbn      = {9781450355735},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3236024.3236053},
  doi       = {10.1145/3236024.3236053},
  abstract  = {Test prioritization aims to detect regression faults faster via reordering test executions, and a large number of test prioritization techniques have been proposed accordingly. However, test prioritization effectiveness is usually measured in terms of the average percentage of faults detected concerned with the number of test executions, rather than the actual regression testing time, making it unclear which technique is optimal in actual regression testing time. To answer this question, this paper first conducts an empirical study to investigate the actual regression testing time of various prioritization techniques. The results reveal a number of practical guidelines. In particular, no prioritization technique can always perform optimal in practice. To achieve the optimal prioritization effectiveness for any given project in practice, based on the findings of this study, we design learning-based Predictive Test Prioritization (PTP). PTP predicts the optimal prioritization technique for a given project based on the test distribution analysis (i.e., the distribution of test coverage, testing time, and coverage per unit time). The results show that PTP correctly predicts the optimal prioritization technique for 46 out of 50 open-source projects from GitHub, outperforming state-of-the-art techniques significantly in regression testing time, e.g., 43.16% to 94.92% improvement in detecting the first regression fault. Furthermore, PTP has been successfully integrated into the practical testing infrastructure of Baidu (a search service provider with over 600M monthly active users), and received positive feedbacks from the testing team of this company, e.g., saving beyond 2X testing costs with negligible overheads.},
  booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages     = {656–667},
  numpages  = {12},
  keywords  = {Machine Learning, Regression Testing, Test Prioritization},
  location  = {Lake Buena Vista, FL, USA},
  series    = {ESEC/FSE 2018}
}

@article{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}

@article{DBLP:journals/corr/abs-2203-09829,
  author     = {Abdul Hameed Azeemi and
                Ihsan Ayyub Qazi and
                Agha Ali Raza},
  title      = {Towards Representative Subset Selection for Self-Supervised Speech
                Recognition},
  journal    = {CoRR},
  volume     = {abs/2203.09829},
  year       = {2022},
  url        = {https://doi.org/10.48550/arXiv.2203.09829},
  doi        = {10.48550/arXiv.2203.09829},
  eprinttype = {arXiv},
  eprint     = {2203.09829},
  timestamp  = {Mon, 28 Mar 2022 17:09:43 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2203-09829.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.1145/3460319.3464810,
  author    = {Cheng, Runxiang and Zhang, Lingming and Marinov, Darko and Xu, Tianyin},
  title     = {Test-Case Prioritization for Configuration Testing},
  year      = {2021},
  isbn      = {9781450384599},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3460319.3464810},
  doi       = {10.1145/3460319.3464810},
  abstract  = {Configuration changes are among the dominant causes of failures of large-scale software system deployment. Given the velocity of configuration changes, typically at the scale of hundreds to thousands of times daily in modern cloud systems, checking these configuration changes is critical to prevent failures due to misconfigurations. Recent work has proposed configuration testing, Ctest, a technique that tests configuration changes together with the code that uses the changed configurations. Ctest can automatically generate a large number of ctests that can effectively detect misconfigurations, including those that are hard to detect by traditional techniques. However, running ctests can take a long time to detect misconfigurations. Inspired by traditional test-case prioritization (TCP) that aims to reorder test executions to speed up detection of regression code faults, we propose to apply TCP to reorder ctests to speed up detection of misconfigurations. We extensively evaluate a total of 84 traditional and novel ctest-specific TCP techniques. The experimental results on five widely used cloud projects demonstrate that TCP can substantially speed up misconfiguration detection. Our study provides guidelines for applying TCP to configuration testing in practice.},
  booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
  pages     = {452–465},
  numpages  = {14},
  keywords  = {software testing, configuration, reliability, Test prioritization},
  location  = {Virtual, Denmark},
  series    = {ISSTA 2021}
}

@inproceedings{919106,
  author    = {Elbaum, S. and Malishevsky, A. and Rothermel, G.},
  booktitle = {Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001},
  title     = {Incorporating varying test costs and fault severities into test case prioritization},
  year      = {2001},
  volume    = {},
  number    = {},
  pages     = {329-338},
  doi       = {10.1109/ICSE.2001.919106}
}

@article{guilford1950fundamental,
  title     = {Fundamental statistics in psychology and education},
  author    = {Guilford, Joy Paul},
  year      = {1950},
  publisher = {McGraw-Hill}
}

@article{testsurvey,
  author     = {Yoo, S. and Harman, M.},
  title      = {Regression Testing Minimization, Selection and Prioritization: A Survey},
  year       = {2012},
  issue_date = {March 2012},
  publisher  = {John Wiley and Sons Ltd.},
  address    = {GBR},
  volume     = {22},
  number     = {2},
  issn       = {0960-0833},
  url        = {https://doi.org/10.1002/stv.430},
  doi        = {10.1002/stv.430},
  abstract   = {Regression testing is a testing activity that is performed to provide confidence that changes do not harm the existing behaviour of the software. Test suites tend to grow in size as software evolves, often making it too costly to execute entire test suites. A number of different approaches have been studied to maximize the value of the accrued test suite: minimization, selection and prioritization. Test suite minimization seeks to eliminate redundant test cases in order to reduce the number of tests to run. Test case selection seeks to identify the test cases that are relevant to some set of recent changes. Test case prioritization seeks to order test cases in such a way that early fault detection is maximized. This paper surveys each area of minimization, selection and prioritization technique and discusses open problems and potential directions for future research. Copyright © 2010 John Wiley &amp; Sons, Ltd.},
  journal    = {Softw. Test. Verif. Reliab.},
  month      = {mar},
  pages      = {67–120},
  numpages   = {54},
  keywords   = {regression testing, regression test selection, test suite minimization, test case prioritization}
}

@inproceedings{PGD,
  author    = {Aleksander Madry and
               Aleksandar Makelov and
               Ludwig Schmidt and
               Dimitris Tsipras and
               Adrian Vladu},
  title     = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
               Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  year      = {2018}
}


@inproceedings{DeepGini,
  author    = {Feng, Yang and Shi, Qingkai and Gao, Xinyu and Wan, Jun and Fang, Chunrong and Chen, Zhenyu},
  title     = {DeepGini: Prioritizing Massive Tests to Enhance the Robustness of Deep Neural Networks},
  year      = {2020},
  isbn      = {9781450380089},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3395363.3397357},
  doi       = {10.1145/3395363.3397357},
  abstract  = {Deep neural networks (DNN) have been deployed in many software systems to assist in various classification tasks. In company with the fantastic effectiveness in classification, DNNs could also exhibit incorrect behaviors and result in accidents and losses. Therefore, testing techniques that can detect incorrect DNN behaviors and improve DNN quality are extremely necessary and critical. However, the testing oracle, which defines the correct output for a given input, is often not available in the automated testing. To obtain the oracle information, the testing tasks of DNN-based systems usually require expensive human efforts to label the testing data, which significantly slows down the process of quality assurance. To mitigate this problem, we propose DeepGini, a test prioritization technique designed based on a statistical perspective of DNN. Such a statistical perspective allows us to reduce the problem of measuring misclassification probability to the problem of measuring set impurity, which allows us to quickly identify possibly-misclassified tests. To evaluate, we conduct an extensive empirical study on popular datasets and prevalent DNN models. The experimental results demonstrate that DeepGini outperforms existing coverage-based techniques in prioritizing tests regarding both effectiveness and efficiency. Meanwhile, we observe that the tests prioritized at the front by DeepGini are more effective in improving the DNN quality in comparison with the coverage-based techniques.},
  booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
  pages     = {177–188},
  numpages  = {12},
  keywords  = {Test Case Prioritization, Deep Learning, Deep Learning Testing},
  location  = {Virtual Event, USA},
  series    = {ISSTA 2020}
}

@inproceedings{ASRTest,
  author    = {Ji, Pin and Feng, Yang and Liu, Jia and Zhao, Zhihong and Chen, Zhenyu},
  title     = {ASRTest: Automated Testing for Deep-Neural-Network-Driven Speech Recognition Systems},
  year      = {2022},
  isbn      = {9781450393799},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3533767.3534391},
  doi       = {10.1145/3533767.3534391},
  abstract  = {With the rapid development of deep neural networks and end-to-end learning techniques, automatic speech recognition (ASR) systems have been deployed into our daily and assist in various tasks. However, despite their tremendous progress, ASR systems could also suffer from software defects and exhibit incorrect behaviors. While the nature of DNN makes conventional software testing techniques inapplicable for ASR systems, lacking diverse tests and oracle information further hinders their testing. In this paper, we propose and implement a testing approach, namely ASR, specifically for the DNN-driven ASR systems. ASRTest is built upon the theory of metamorphic testing. We first design the metamorphic relation for ASR systems and then implement three families of transformation operators that can simulate practical application scenarios to generate speeches. Furthermore, we adopt Gini impurity to guide the generation process and improve the testing efficiency. To validate the effectiveness of ASRTest, we apply ASRTest to four ASR models with four widely-used datasets. The results show that ASRTest can detect erroneous behaviors under different realistic application conditions efficiently and improve 19.1% recognition performance on average via retraining with the generated data. Also, we conduct a case study on an industrial ASR system to investigate the performance of ASRTest under the real usage scenario. The study shows that ASRTest can detect errors and improve the performance of DNN-driven ASR systems effectively.},
  booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
  pages     = {189–201},
  numpages  = {13},
  keywords  = {Metamorphic Testing, Automated Testing, Deep Neural Networks, Automatic Speech Recognition},
  location  = {Virtual, South Korea},
  series    = {ISSTA 2022}
}


@inproceedings{biasrv,
  author    = {Yang, Zhou and Asyrofi, Muhammad Hilmi and Lo, David},
  title     = {BiasRV: Uncovering Biased Sentiment Predictions at Runtime},
  year      = {2021},
  isbn      = {9781450385626},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3468264.3473117},
  doi       = {10.1145/3468264.3473117},
  abstract  = {Sentiment analysis (SA) systems, though widely applied in many domains, have been demonstrated to produce biased results. Some research works have been done in automatically generating test cases to reveal unfairness in SA systems, but the community still lacks tools that can monitor and uncover biased predictions at runtime. This paper fills this gap by proposing BiasRV, the first tool to raise an alarm when a deployed SA system makes a biased prediction on a given input text. To implement this feature, BiasRV dynamically extracts a template from an input text and generates gender-discriminatory mutants (semantically-equivalent texts that only differ in gender information) from the template. Based on popular metrics used to evaluate the overall fairness of an SA system, we define the distributional fairness property for an individual prediction of an SA system. This property specifies a requirement that for one piece of text, mutants from different gender classes should be treated similarly. Verifying the distributional fairness property causes much overhead to the running system. To run more efficiently, BiasRV adopts a two-step heuristic: (1) sampling several mutants from each gender and checking if the system predicts them as of the same sentiment, (2) checking distributional fairness only when sampled mutants have conflicting results. Experiments show that when compared to directly checking the distributional fairness property for each input text, our two-step heuristic can decrease the overhead used for analyzing mutants by 73.81% while only resulting in 6.7% of biased predictions being missed. Besides, BiasRV can be used conveniently without knowing the implementation of SA systems. Future researchers can easily extend BiasRV to detect more types of bias, e.g., race and occupation. The demo video for BiasRV can be viewed at https://youtu.be/WPe4Ml77d3U and the source code can be found at https://github.com/soarsmu/BiasRV.},
  booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages     = {1540–1544},
  numpages  = {5},
  keywords  = {Runtime Verification, Sentiment Analysis, Ethical AI, Fairness},
  location  = {Athens, Greece},
  series    = {ESEC/FSE 2021}
}

@article{biasfinder,
  author  = {Asyrofi, Muhammad Hilmi and Yang, Zhou and Yusuf, Imam Nur Bani and Kang, Hong Jin and Thung, Ferdian and Lo, David},
  journal = {IEEE Transactions on Software Engineering},
  title   = {BiasFinder: Metamorphic Test Generation to Uncover Bias for Sentiment Analysis Systems},
  year    = {2022},
  volume  = {48},
  number  = {12},
  pages   = {5087-5101},
  doi     = {10.1109/TSE.2021.3136169}
}

@inproceedings{europarl,
  title     = {{E}uroparl: A Parallel Corpus for Statistical Machine Translation},
  author    = {Koehn, Philipp},
  booktitle = {Proceedings of Machine Translation Summit X: Papers},
  month     = sep # { 13-15},
  year      = {2005},
  address   = {Phuket, Thailand},
  url       = {https://aclanthology.org/2005.mtsummit-papers.11},
  pages     = {79--86},
  abstract  = {We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web. This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead.}
}

@inproceedings{Libri-Light,
  author    = {Kahn, J. and Rivière, M. and Zheng, W. and Kharitonov, E. and Xu, Q. and Mazaré, P.E. and Karadayi, J. and Liptchinsky, V. and Collobert, R. and Fuegen, C. and Likhomanenko, T. and Synnaeve, G. and Joulin, A. and Mohamed, A. and Dupoux, E.},
  booktitle = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Libri-Light: A Benchmark for ASR with Limited or No Supervision},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {7669-7673},
  doi       = {10.1109/ICASSP40776.2020.9052942}
}

@article{ml-testing-survey,
  author  = {Zhang, Jie M. and Harman, Mark and Ma, Lei and Liu, Yang},
  journal = {IEEE Transactions on Software Engineering},
  title   = {Machine Learning Testing: Survey, Landscapes and Horizons},
  year    = {2022},
  volume  = {48},
  number  = {1},
  pages   = {1-36},
  doi     = {10.1109/TSE.2019.2962027}
}

@article{selection_survey,
  author     = {Kazmi, Rafaqut and Jawawi, Dayang N. A. and Mohamad, Radziah and Ghani, Imran},
  title      = {Effective Regression Test Case Selection: A Systematic Literature Review},
  year       = {2017},
  issue_date = {March 2018},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {50},
  number     = {2},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3057269},
  doi        = {10.1145/3057269},
  abstract   = {Regression test case selection techniques attempt to increase the testing effectiveness based on the measurement capabilities, such as cost, coverage, and fault detection. This systematic literature review presents state-of-the-art research in effective regression test case selection techniques. We examined 47 empirical studies published between 2007 and 2015. The selected studies are categorized according to the selection procedure, empirical study design, and adequacy criteria with respect to their effectiveness measurement capability and methods used to measure the validity of these results.The results showed that mining and learning-based regression test case selection was reported in 39% of the studies, unit level testing was reported in 18% of the studies, and object-oriented environment (Java) was used in 26% of the studies. Structural faults, the most common target, was used in 55% of the studies. Overall, only 39% of the studies conducted followed experimental guidelines and are reproducible.There are 7 different cost measures, 13 different coverage types, and 5 fault-detection metrics reported in these studies. It is also observed that 70% of the studies being analyzed used cost as the effectiveness measure compared to 31% that used fault-detection capability and 16% that used coverage.},
  journal    = {ACM Comput. Surv.},
  month      = {may},
  articleno  = {29},
  numpages   = {32},
  keywords   = {coverage, cost effectiveness, Software testing, fault detection ability, SLR}
}

@inproceedings{common-voice,
  title     = {Common Voice: A Massively-Multilingual Speech Corpus},
  author    = {Ardila, Rosana  and
               Branson, Megan  and
               Davis, Kelly  and
               Kohler, Michael  and
               Meyer, Josh  and
               Henretty, Michael  and
               Morais, Reuben  and
               Saunders, Lindsay  and
               Tyers, Francis  and
               Weber, Gregor},
  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},
  month     = may,
  year      = {2020},
  address   = {Marseille, France},
  publisher = {European Language Resources Association},
  url       = {https://aclanthology.org/2020.lrec-1.520},
  pages     = {4218--4222},
  abstract  = {The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla{'}s DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 {\mbox{$\pm$}} 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.},
  language  = {English},
  isbn      = {979-10-95546-34-4}
}

@inproceedings{biasheal,
  author    = {Yang, Zhou and Jain, Harshit and Shi, Jieke and Asyrofi, Muhammad Hilmi and Lo, David},
  booktitle = {2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  title     = {BiasHeal: On-the-Fly Black-Box Healing of Bias in Sentiment Analysis Systems},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {644-648},
  doi       = {10.1109/ICSME52107.2021.00073}
}

@misc{asrdebugger,
  doi       = {10.48550/ARXIV.2302.00330},
  url       = {https://arxiv.org/abs/2302.00330},
  author    = {Yang, Zhou and Shi, Jieke and Asyrofi, Muhammad Hilmi and Xu, Bowen and Zhou, Xin and Han, DongGyun and Lo, David},
  keywords  = {Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Prioritizing Speech Test Cases},
  publisher = {arXiv},
  year      = {2023},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{hu_tosem,
  author  = {Hu, Qiang and Guo, Yuejun and Cordy, Maxime and Xie, Xiaofei and Ma, Lei and Papadakis, Mike and Le Traon, Yves},
  title   = {An Empirical Study on Data Distribution-Aware Test Selection for Deep Learning Enhancement},
  year    = {2022},
  journal = {ACM Transactions on Software Engineering and Methodology (TOSEM)}
}


@article{DeepCruiser,
  author     = {Xiaoning Du and
                Xiaofei Xie and
                Yi Li and
                Lei Ma and
                Jianjun Zhao and
                Yang Liu},
  title      = {DeepCruiser: Automated Guided Testing for Stateful Deep Learning Systems},
  journal    = {CoRR},
  volume     = {abs/1812.05339},
  year       = {2018},
  url        = {http://arxiv.org/abs/1812.05339},
  eprinttype = {arXiv},
  eprint     = {1812.05339},
  timestamp  = {Fri, 11 Feb 2022 09:12:45 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1812-05339.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mengge-etal-2020-coarse,
  title     = {{C}oarse-to-{F}ine {P}re-training for {N}amed {E}ntity {R}ecognition},
  author    = {Mengge, Xue and Yu, Bowen and Zhang, Zhenyu and Liu, Tingwen and Zhang, Yue and Wang, Bin},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year      = {2020},
  pages     = {6345--6354}
}

@article{hisamoto-etal-2020-membership,
  title     = {Membership Inference Attacks on Sequence-to-Sequence Models: {I}s My Data In Your Machine Translation System?},
  author    = {Hisamoto, Sorami  and
               Post, Matt  and
               Duh, Kevin},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {8},
  year      = {2020},
  address   = {Cambridge, MA},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/2020.tacl-1.4},
  doi       = {10.1162/tacl_a_00299},
  pages     = {49--63},
  abstract  = {Data privacy is an important issue for {``}machine learning as a service{''} providers. We focus on the problem of membership inference attacks: Given a data sample and black-box access to a model{'}s API, determine whether the sample existed in the model{'}s training data. Our contribution is an investigation of this problem in the context of sequence-to-sequence models, which are important in applications such as machine translation and video captioning. We define the membership inference problem for sequence generation, provide an open dataset based on state-of-the-art machine translation models, and report initial results on whether these models leak private information against several kinds of membership inference attacks.}
}

@article{cooper2014mental,
  title  = {Mental workload of common voice-based vehicle interactions across six different vehicle systems},
  author = {Cooper, Joel M and Ingebretsen, Hailey and Strayer, David L},
  year   = {2014}
}

@inproceedings{50459,
  title     = {Automatic Speech Recognition of Disordered Speech: Personalized models outperforming human listeners on short phrases},
  author    = {Jordan R. Green and Bob MacDonald and Pan-Pan Jiang and Julie Cattiau and Rus Heywood and Richard Cave and Katie Seaver and Marilyn Ladewig and Jimmy Tobin and Michael Brenner and Philip Q Nelson and Katrin Tomanek},
  year      = {2021},
  booktitle = {Proc. Interspeech 2021}
}


@inproceedings{personal,
  author    = {Khe Chai Sim and
               Angad Chandorkar and
               Fan Gao and
               Mason Chua and
               Tsendsuren Munkhdalai and
               Fran{\c{c}}oise Beaufays},
  editor    = {Hynek Hermansky and
               Honza Cernock{\'{y}} and
               Luk{\'{a}}s Burget and
               Lori Lamel and
               Odette Scharenborg and
               Petr Motl{\'{\i}}cek},
  title     = {Robust Continuous On-Device Personalization for Automatic Speech Recognition},
  booktitle = {Interspeech 2021, 22nd Annual Conference of the International Speech
               Communication Association, Brno, Czechia, 30 August - 3 September
               2021},
  pages     = {1284--1288},
  publisher = {{ISCA}},
  year      = {2021},
  url       = {https://doi.org/10.21437/Interspeech.2021-318},
  doi       = {10.21437/Interspeech.2021-318},
  timestamp = {Mon, 14 Mar 2022 16:42:12 +0100},
  biburl    = {https://dblp.org/rec/conf/interspeech/SimCGCMB21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{deeproad,
  author    = {Zhang, Mengshi and Zhang, Yuqun and Zhang, Lingming and Liu, Cong and Khurshid, Sarfraz},
  title     = {DeepRoad: GAN-Based Metamorphic Testing and Input Validation Framework for Autonomous Driving Systems},
  year      = {2018},
  isbn      = {9781450359375},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3238147.3238187},
  doi       = {10.1145/3238147.3238187},
  abstract  = {While Deep Neural Networks (DNNs) have established the fundamentals of image-based autonomous driving systems, they may exhibit erroneous behaviors and cause fatal accidents. To address the safety issues in autonomous driving systems, a recent set of testing techniques have been designed to automatically generate artificial driving scenes to enrich test suite, e.g., generating new input images transformed from the original ones. However, these techniques are insufficient due to two limitations: first, many such synthetic images often lack diversity of driving scenes, and hence compromise the resulting efficacy and reliability. Second, for machine-learning-based systems, a mismatch between training and application domain can dramatically degrade system accuracy, such that it is necessary to validate inputs for improving system robustness. In this paper, we propose DeepRoad, an unsupervised DNN-based framework for automatically testing the consistency of DNN-based autonomous driving systems and online validation. First, DeepRoad automatically synthesizes large amounts of diverse driving scenes without using image transformation rules (e.g. scale, shear and rotation). In particular, DeepRoad is able to produce driving scenes with various weather conditions (including those with rather extreme conditions) by applying Generative Adversarial Networks (GANs) along with the corresponding real-world weather scenes. Second, DeepRoad utilizes metamorphic testing techniques to check the consistency of such systems using synthetic images. Third, DeepRoad validates input images for DNN-based systems by measuring the distance of the input and training images using their VGGNet features. We implement DeepRoad to test three well-recognized DNN-based autonomous driving systems in Udacity self-driving car challenge. The experimental results demonstrate that DeepRoad can detect thousands of inconsistent behaviors for these systems, and effectively validate input images to potentially enhance the system robustness as well.},
  booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
  pages     = {132–142},
  numpages  = {11},
  keywords  = {Software testing, Test generation, Deep Neural Networks, Input validation},
  location  = {Montpellier, France},
  series    = {ASE '18}
}


@inproceedings{Fang2020,
  author    = {Fang, Anjie and Filice, Simone and Limsopatham, Nut and Rokhlenko, Oleg},
  year      = {2020},
  month     = {07},
  pages     = {699-708},
  title     = {Using Phoneme Representations to Build Predictive Models Robust to ASR Errors},
  booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '20)},
  doi       = {10.1145/3397271.3401050}
}


@inproceedings{shor19_interspeech,
  author    = {Joel Shor and Dotan Emanuel and Oran Lang and Omry Tuval and Michael Brenner and Julie Cattiau and Fernando Vieira and Maeve McNally and Taylor Charbonneau and Melissa Nollstadt and Avinatan Hassidim and Yossi Matias},
  title     = {{Personalizing ASR for Dysarthric and Accented Speech with Limited Data}},
  year      = 2019,
  booktitle = {Proc. Interspeech 2019},
  pages     = {784--788},
  doi       = {10.21437/Interspeech.2019-1427}
}

@article{strayer2014measuring,
  title  = {Measuring cognitive distraction in the automobile II: Assessing in-vehicle voice-based interactive technologies},
  author = {Strayer, David L and Turrill, Jonna and Coleman, James R and Ortiz, Emily V and Cooper, Joel M},
  year   = {2014}
}

@misc{bitch,
  author    = {Ramesh, Krithika and KhudaBukhsh, Ashiqur R. and Kumar, Sumeet},
  title     = {'Beach' to 'Bitch': Inadvertent Unsafe Transcription of Kids' Content on YouTube},
  publisher = {AAAI},
  year      = {2022}
}


@inproceedings{fast,
  title     = {Fast {W}ord{P}iece Tokenization},
  author    = {Song, Xinying  and
               Salcianu, Alex  and
               Song, Yang  and
               Dopson, Dave  and
               Zhou, Denny},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2021},
  address   = {Online and Punta Cana, Dominican Republic},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.emnlp-main.160},
  doi       = {10.18653/v1/2021.emnlp-main.160},
  pages     = {2089--2103},
  abstract  = {Tokenization is a fundamental preprocessing step for almost all NLP tasks. In this paper, we propose efficient algorithms for the WordPiece tokenization used in BERT, from single-word tokenization to general text (e.g., sentence) tokenization. When tokenizing a single word, WordPiece uses a longest-match-first strategy, known as maximum matching. The best known algorithms so far are O(n{\^{}}2) (where n is the input length) or O(nm) (where m is the maximum vocabulary token length). We propose a novel algorithm whose tokenization complexity is strictly O(n). Our method is inspired by the Aho-Corasick algorithm. We introduce additional linkages on top of the trie built from the vocabulary, allowing smart transitions when the trie matching cannot continue. For general text, we further propose an algorithm that combines pre-tokenization (splitting the text into words) and our linear-time WordPiece method into a single pass. Experimental results show that our method is 8.2x faster than HuggingFace Tokenizers and 5.1x faster than TensorFlow Text on average for general text tokenization.}
}

@inproceedings{shi2022identifier,
  author    = {Jieke Shi and Zhou Yang and Junda He and Bowen Xu and David Lo},
  booktitle = {2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)},
  title     = {Can Identifier Splitting Improve Open-Vocabulary Language Model of Code?},
  year      = {2022},
  volume    = {},
  publisher = {IEEE Computer Society}
}


@inproceedings{monash-fairness,
  title     = {Exploring and Repairing Gender Fairness Violations in Word Embedding-based Sentiment Analysis Model through Adversarial Patches},
  booktitle = {2023 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)},
  author    = {Khoo, Lin Sze and Bay, Jia Qi and Yap, Ming Lee Kimberly and Lim, Mei Kuan and Chong, Chun Yong and Yang, Zhou and Lo, David},
  year      = {2023},
  volume    = {},
  publisher = {IEEE Computer Society}
}


@inproceedings{lin2009select,
  author  = {Lin, Hui and Bilmes, Jeff},
  year    = {2009},
  month   = {09},
  pages   = {2859-2862},
  title   = {How to select a good training-data subset for transcription: Submodular active selection for sequences},
  journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
  doi     = {10.21437/Interspeech.2009-730}
}

@inproceedings{9240704,
  author    = {Zhang, Ting and Xu, Bowen and Thung, Ferdian and Haryono, Stefanus Agus and Lo, David and Jiang, Lingxiao},
  booktitle = {2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  title     = {Sentiment Analysis for Software Engineering: How Far Can Pre-trained Transformer Models Go?},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {70-80},
  doi       = {10.1109/ICSME46990.2020.00017}
}

@article{RoBERTa,
  author     = {Yinhan Liu and
                Myle Ott and
                Naman Goyal and
                Jingfei Du and
                Mandar Joshi and
                Danqi Chen and
                Omer Levy and
                Mike Lewis and
                Luke Zettlemoyer and
                Veselin Stoyanov},
  title      = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal    = {CoRR},
  volume     = {abs/1907.11692},
  year       = {2019},
  url        = {http://arxiv.org/abs/1907.11692},
  eprinttype = {arXiv},
  eprint     = {1907.11692},
  timestamp  = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bert,
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob  and
               Chang, Ming-Wei  and
               Lee, Kenton  and
               Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N19-1423},
  doi       = {10.18653/v1/N19-1423},
  pages     = {4171--4186},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@inproceedings{bert-attack,
  title     = {BERT-ATTACK: Adversarial Attack Against BERT Using BERT},
  author    = {Li, Linyang and Ma, Ruotian and Guo, Qipeng and Xue, Xiangyang and Qiu, Xipeng},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages     = {6193--6202},
  year      = {2020}
}

@inproceedings{simoncini2021seqattack,
  title     = {SeqAttack: On adversarial attacks for named entity recognition},
  author    = {Simoncini, Walter and Spanakis, Gerasimos},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages     = {308--318},
  year      = {2021}
}

@article{mann1947test,
  title     = {On a test of whether one of two random variables is stochastically larger than the other},
  author    = {Mann, Henry B and Whitney, Donald R},
  journal   = {The annals of mathematical statistics},
  pages     = {50--60},
  year      = {1947},
  publisher = {JSTOR}
}

@misc{art,
  title        = {Adversarial Robustness Toolbox (ART)},
  howpublished = {\url{https://github.com/Trusted-AI/adversarial-robustness-toolbox}},
  note         = {Accessed: 2021-11-19}
}



@inproceedings{google_coverage,
  author    = {Ivankovi\'{c}, Marko and Petrovi\'{c}, Goran and Just, Ren\'{e} and Fraser, Gordon},
  title     = {Code Coverage at Google},
  year      = {2019},
  isbn      = {9781450355728},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3338906.3340459},
  doi       = {10.1145/3338906.3340459},
  abstract  = {Code coverage is a measure of the degree to which a test suite exercises a software system. Although coverage is well established in software engineering research, deployment in industry is often inhibited by the perceived usefulness and the computational costs of analyzing coverage at scale. At Google, coverage information is computed for one billion lines of code daily, for seven programming languages. A key aspect of making coverage information actionable is to apply it at the level of changesets and code review. This paper describes Google’s code coverage infrastructure and how the computed code coverage information is visualized and used. It also describes the challenges and solutions for adopting code coverage at scale. To study how code coverage is adopted and perceived by developers, this paper analyzes adoption rates, error rates, and average code coverage ratios over a five-year period, and it reports on 512 responses, received from surveying 3000 developers. Finally, this paper provides concrete suggestions for how to implement and use code coverage in an industrial setting.},
  booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages     = {955–963},
  numpages  = {9},
  keywords  = {coverage, industrial study, test infrastructure},
  location  = {Tallinn, Estonia},
  series    = {ESEC/FSE 2019}
}


@inproceedings{icse_no_correaltion,
  author    = {Inozemtseva, Laura and Holmes, Reid},
  title     = {Coverage is Not Strongly Correlated with Test Suite Effectiveness},
  year      = {2014},
  isbn      = {9781450327565},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2568225.2568271},
  doi       = {10.1145/2568225.2568271},
  abstract  = { The coverage of a test suite is often used as a proxy for its ability to detect faults. However, previous studies that investigated the correlation between code coverage and test suite effectiveness have failed to reach a consensus about the nature and strength of the relationship between these test suite characteristics. Moreover, many of the studies were done with small or synthetic programs, making it unclear whether their results generalize to larger programs, and some of the studies did not account for the confounding influence of test suite size. In addition, most of the studies were done with adequate suites, which are are rare in practice, so the results may not generalize to typical test suites.  We have extended these studies by evaluating the relationship between test suite size, coverage, and effectiveness for large Java programs. Our study is the largest to date in the literature: we generated 31,000 test suites for five systems consisting of up to 724,000 lines of source code. We measured the statement coverage, decision coverage, and modified condition coverage of these suites and used mutation testing to evaluate their fault detection effectiveness.  We found that there is a low to moderate correlation between coverage and effectiveness when the number of test cases in the suite is controlled for. In addition, we found that stronger forms of coverage do not provide greater insight into the effectiveness of the suite. Our results suggest that coverage, while useful for identifying under-tested parts of a program, should not be used as a quality target because it is not a good indicator of test suite effectiveness. },
  booktitle = {Proceedings of the 36th International Conference on Software Engineering},
  pages     = {435–445},
  numpages  = {11},
  keywords  = {Coverage, test suite effectiveness, test suite quality},
  location  = {Hyderabad, India},
  series    = {ICSE 2014}
}

@inproceedings{resnet,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Deep Residual Learning for Image Recognition},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {770-778},
  doi       = {10.1109/CVPR.2016.90},
  url       = {https://doi.org/10.1109/CVPR.2016.90},
  abstract  = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.}
}


@inproceedings{asrevolve,
  author    = {Asyrofi, Muhammad Hilmi and Yang, Zhou and Shi, Jicke and Quan, Chu Wei and Lo, David},
  booktitle = {2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  title     = {Can Differential Testing Improve Automatic Speech Recognition Systems?},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {674-678},
  doi       = {10.1109/ICSME52107.2021.00079}
}

@inproceedings{eurosat,
  author    = {Helber, Patrick and Bischke, Benjamin and Dengel, Andreas and Borth, Damian},
  booktitle = {IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium},
  title     = {Introducing Eurosat: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {204-207},
  doi       = {10.1109/IGARSS.2018.8519248},
  url       = {https://doi.org/10.1109/IGARSS.2018.8519248}
}

@article{ye2021automated,
  title     = {Automated patch assessment for program repair at scale},
  author    = {Ye, He and Martinez, Matias and Monperrus, Martin},
  journal   = {Empirical Software Engineering},
  volume    = {26},
  number    = {2},
  pages     = {1--38},
  year      = {2021},
  publisher = {Springer}
}


@article{align,
  author     = {Nicholas Ruiz and
                Marcello Federico},
  title      = {Phonetically-Oriented Word Error Alignment for Speech Recognition
                Error Analysis in Speech Translation},
  journal    = {CoRR},
  volume     = {abs/1904.11024},
  year       = {2019},
  url        = {http://arxiv.org/abs/1904.11024},
  eprinttype = {arXiv},
  eprint     = {1904.11024},
  timestamp  = {Thu, 02 May 2019 15:13:44 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1904-11024.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{8301638,
  author    = {Këpuska, Veton and Bohouta, Gamal},
  booktitle = {2018 IEEE 8th Annual Computing and Communication Workshop and Conference (CCWC)},
  title     = {Next-generation of virtual personal assistants (Microsoft Cortana, Apple Siri, Amazon Alexa and Google Home)},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {99-103},
  doi       = {10.1109/CCWC.2018.8301638}
}

@inproceedings{healthcare,
  author    = {Kocaballi, A. Baki and Quiroz, Juan C. and Laranjo, Liliana and Rezazadegan, Dana and Kocielnik, Rafal and Clark, Leigh and Liao, Q. Vera and Park, Sun Young and Moore, Robert J. and Miner, Adam},
  title     = {Conversational Agents for Health and Wellbeing},
  year      = {2020},
  isbn      = {9781450368193},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3334480.3375154},
  doi       = {10.1145/3334480.3375154},
  abstract  = {Conversational agents have increasingly been deployed in healthcare applications. However, significant challenges remain in developing this technology. Recent research in this area has highlighted that: i) patient safety was rarely evaluated; ii) health outcomes were poorly measured, and iii) no standardised evaluation methods were employed. The conversational agents in healthcare are lagging behind the developments in other domains. This one-day workshop aims to create a roadmap for healthcare conversational agents to develop standardised design and evaluation frameworks. This will prioritise health outcomes and patient safety while ensuring a high-quality user experience. In doing so, this workshop will bring together researchers and practitioners from HCI, healthcare and related speech and chatbot domains to collaborate on these key challenges.},
  booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
  pages     = {1–8},
  numpages  = {8},
  keywords  = {voice interface, conversational agent, health informatics, healthcare, speech interface, chatbots},
  location  = {Honolulu, HI, USA},
  series    = {CHI EA '20}
}

@inproceedings{robot_control,
  author    = {Yoshimura, Naoya and Yoshida, Hironori and Matulic, Fabrice and Igarashi, Takeo},
  title     = {Extending Discrete Verbal Commands with Continuous Speech for Flexible Robot Control},
  year      = {2019},
  isbn      = {9781450359719},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3290607.3312791},
  doi       = {10.1145/3290607.3312791},
  abstract  = {Speech is a direct and intuitive method to control a robot. While natural speech can capture a rich variety of commands, verbal input is poorly suited to finer grained and real-time control of continuous actions such as short and precise motion commands. For these types of operations, continuous non-verbal speech is more suitable, but it lacks the naturalness and vocabulary breadth of verbal speech. In this work, we propose to combine the two types of vocal input by extending the last vowel of a verbal command to support real-time and smooth control of robot actions. We demonstrate the effectiveness of this novel hybrid speech input method in a beverage-pouring task, where users instruct a robot arm to pour specific quantities of liquid into a cup. A user evaluation reveals that hybrid speech improves on simple verbal-only commands.},
  booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
  pages     = {1–6},
  numpages  = {6},
  keywords  = {human robot interaction, voice inputs, continuous control},
  location  = {Glasgow, Scotland Uk},
  series    = {CHI EA '19}
}

@misc{baffle,
  doi       = {10.48550/ARXIV.2210.04688},
  url       = {https://arxiv.org/abs/2210.04688},
  author    = {Gong, Chen and Yang, Zhou and Bai, Yunpeng and He, Junda and Shi, Jieke and Sinha, Arunesh and Xu, Bowen and Hou, Xinwen and Fan, Guoliang and Lo, David},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Mind Your Data! Hiding Backdoors in Offline Reinforcement Learning Datasets},
  publisher = {arXiv},
  year      = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{6854213,
  author    = {Wei, Kai and Liu, Yuzong and Kirchhoff, Katrin and Bartels, Chris and Bilmes, Jeff},
  booktitle = {2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Submodular subset selection for large-scale speech training data},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {3311-3315},
  doi       = {10.1109/ICASSP.2014.6854213}
}

@inproceedings{7178848,
  author    = {Ni, Chongjia and Wang, Lei and Liu, Haibo and Leung, Cheung-Chi and Lu, Li and Ma, Bin},
  booktitle = {2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Submodular data selection with acoustic and phonetic features for automatic speech recognition},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {4629-4633},
  doi       = {10.1109/ICASSP.2015.7178848}
}

@article{wav2vec2,
  title   = {wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author  = {Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {12449--12460},
  year    = {2020}
}

@article{HuBERT,
  author  = {Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  title   = {HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units},
  year    = {2021},
  volume  = {29},
  number  = {},
  pages   = {3451-3460},
  doi     = {10.1109/TASLP.2021.3122291}
}

@inproceedings{IndicTTS,
  author    = {Vignesh, S. Rupak and Shanmugam, S. Aswin and Murthy, Hema A.},
  booktitle = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Significance of Pseudo-syllables in building better acoustic models for Indian English TTS},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {5620-5624},
  doi       = {10.1109/ICASSP.2016.7472753}
}

@inproceedings{FGSM,
  title     = {Explaining and Harnessing Adversarial Examples},
  author    = {Ian Goodfellow and Jonathon Shlens and Christian Szegedy},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6572},
  booktitle = {International Conference on Learning Representations},
  abstract  = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.}
}

@inproceedings{SSIM,
  author    = {Hor\'{e}, Alain and Ziou, Djemel},
  booktitle = {2010 20th International Conference on Pattern Recognition},
  title     = {Image Quality Metrics: PSNR vs. SSIM},
  year      = {2010},
  volume    = {},
  number    = {},
  pages     = {2366-2369},
  doi       = {10.1109/ICPR.2010.579}
}

@misc{replication-saner-22,
  title        = {Replication Package},
  howpublished = {\url{https://github.com/soarsmu/Revisiting_Neuron_Coverage.git}},
  note         = {Accessed: 2021-11-19}
}

@inproceedings{CONTA,
  author    = {Zhang, Dong and Zhang, Hanwang and Tang, Jinhui and Hua, Xian-Sheng and Sun, Qianru},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {655--666},
  publisher = {Curran Associates, Inc.},
  title     = {Causal Intervention for Weakly-Supervised Semantic Segmentation},
  url       = {https://proceedings.neurips.cc/paper/2020/file/07211688a0869d995947a8fb11b215d6-Paper.pdf},
  volume    = {33},
  year      = {2020}
}



@article{mnih2015human,
  title     = {Human-level control through deep reinforcement learning},
  author    = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal   = {nature},
  volume    = {518},
  number    = {7540},
  pages     = {529--533},
  year      = {2015},
  doi       = {https://doi.org/10.1038/nature14236},
  url       = {https://doi.org/10.1038/nature14236},
  publisher = {Nature Publishing Group},
  abstract  = {The theory of reinforcement learning provides a normative account1, deeply rooted in psychological2 and neuroscientific3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems4,5, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms3. While reinforcement learning agents have achieved some successes in a variety of domains6,7,8, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks9,10,11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games12. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.}
}

@article{abdel2014convolutional,
  author   = {Abdel-Hamid, Ossama and Mohamed, Abdel-rahman and Jiang, Hui and Deng, Li and Penn, Gerald and Yu, Dong},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  title    = {Convolutional Neural Networks for Speech Recognition},
  year     = {2014},
  volume   = {22},
  number   = {10},
  pages    = {1533-1545},
  doi      = {10.1109/TASLP.2014.2339736},
  url      = {https://doi.org/10.1109/TASLP.2014.2339736},
  abstract = {Recently, the hybrid deep neural network (DNN)-hidden Markov model (HMM) has been shown to significantly improve speech recognition performance over the conventional Gaussian mixture model (GMM)-HMM. The performance improvement is partially attributed to the ability of the DNN to model complex correlations in speech features. In this paper, we show that further error rate reduction can be obtained by using convolutional neural networks (CNNs). We first present a concise description of the basic CNN and explain how it can be used for speech recognition. We further propose a limited-weight-sharing scheme that can better model speech features. The special structure such as local connectivity, weight sharing, and pooling in CNNs exhibits some degree of invariance to small shifts of speech features along the frequency axis, which is important to deal with speaker and environment variations. Experimental results show that CNNs reduce the error rate by 6%-10% compared with DNNs on the TIMIT phone recognition and the voice search large vocabulary speech recognition tasks.}
}


@inproceedings{DBLP:journals/corr/BahdanauCB14,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inbook{AdversarialTraining,
  author    = {Shafahi, Ali and Najibi, Mahyar and Ghiasi, Mohammad Amin and Xu, Zheng and Dickerson, John and Studer, Christoph and Davis, Larry S and Taylor, Gavin and Goldstein, Tom},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Adversarial training for free!},
  url       = {https://proceedings.neurips.cc/paper/2019/file/7503cfacd12053d309b6bed5c89de212-Paper.pdf},
  volume    = {32},
  year      = {2019}
}


@inproceedings{icassp2021,
  author    = {Awasthi, Abhijeet and Kansal, Aman and Sarawagi, Sunita and Jyothi, Preethi},
  booktitle = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Error-Driven Fixed-Budget ASR Personalization for Accented Speakers},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {7033-7037},
  doi       = {10.1109/ICASSP39728.2021.9414830}
}

@inproceedings{carlini21extracting,
  author    = {Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and Oprea, Alina and Raffel, Colin},
  title     = {Extracting Training Data from Large Language Models},
  booktitle = {USENIX Security Symposium},
  year      = {2021}
}

@inproceedings{6947957,
  author    = {Mendonça, Gustavo and Candeias, Sara and Perdigão, Fernando and Shulby, Christopher and Toniazzo, Rean and Klautau, Aldebaro and Aluísio, Sandra},
  booktitle = {2014 International Telecommunications Symposium (ITS)},
  title     = {A method for the extraction of phonetically-rich triphone sentences},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {1-5},
  doi       = {10.1109/ITS.2014.6947957}
}

@misc{CovTesting,
  title        = {Yan et al's Replication Package},
  howpublished = {\url{https://github.com/RU-System-Software-and-Security/CovTesting.git}},
  note         = {Accessed: 2021-11-19}
}



@inproceedings{10.1109/ICSE.2017.62,
  author    = {Pearson, Spencer and Campos, Jos\'{e} and Just, Ren\'{e} and Fraser, Gordon and Abreu, Rui and Ernst, Michael D. and Pang, Deric and Keller, Benjamin},
  title     = {Evaluating and Improving Fault Localization},
  year      = {2017},
  isbn      = {9781538638682},
  publisher = {IEEE Press},
  url       = {https://doi.org/10.1109/ICSE.2017.62},
  doi       = {10.1109/ICSE.2017.62},
  abstract  = {Most fault localization techniques take as input a faulty program, and produce as output a ranked list of suspicious code locations at which the program may be defective. When researchers propose a new fault localization technique, they typically evaluate it on programs with known faults. The technique is scored based on where in its output list the defective code appears. This enables the comparison of multiple fault localization techniques to determine which one is better.Previous research has evaluated fault localization techniques using artificial faults, generated either by mutation tools or manually. In other words, previous research has determined which fault localization techniques are best at finding artificial faults. However, it is not known which fault localization techniques are best at finding real faults. It is not obvious that the answer is the same, given previous work showing that artificial faults have both similarities to and differences from real faults.We performed a replication study to evaluate 10 claims in the literature that compared fault localization techniques (from the spectrum-based and mutation-based families). We used 2995 artificial faults in 6 real-world programs. Our results support 7 of the previous claims as statistically significant, but only 3 as having non-negligible effect sizes. Then, we evaluated the same 10 claims, using 310 real faults from the 6 programs. Every previous result was refuted or was statistically and practically insignificant. Our experiments show that artificial faults are not useful for predicting which fault localization techniques perform best on real faults.In light of these results, we identified a design space that includes many previously-studied fault localization techniques as well as hundreds of new techniques. We experimentally determined which factors in the design space are most important, using an overall set of 395 real faults. Then, we extended this design space with new techniques. Several of our novel techniques outperform all existing techniques, notably in terms of ranking defective code in the top-5 or top-10 reports.},
  booktitle = {Proceedings of the 39th International Conference on Software Engineering},
  pages     = {609–620},
  numpages  = {12},
  location  = {Buenos Aires, Argentina},
  series    = {ICSE '17}
}



@inproceedings{10.1145/2597008.2597148,
  author    = {Wang, Shaowei and Lo, David},
  title     = {Version History, Similar Report, and Structure: Putting Them Together for Improved Bug Localization},
  year      = {2014},
  isbn      = {9781450328791},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2597008.2597148},
  doi       = {10.1145/2597008.2597148},
  abstract  = { During the evolution of a software system, a large number of bug reports are submitted. Locating the source code files that need to be fixed to resolve the bugs is a challenging problem. Thus, there is a need for a technique that can automatically figure out these buggy files. A number of bug localization solutions that take in a bug report and output a ranked list of files sorted based on their likelihood to be buggy have been proposed in the literature. However, the accuracy of these tools still need to be improved.  In this paper, to address this need, we propose AmaLgam, a new method for locating relevant buggy files that puts together version history, similar reports, and structure. To do this, AmaLgam integrates a bug prediction technique used in Google which analyzes version history, with a bug localization technique named BugLocator which analyzes similar reports from bug report system, and the state-of-the-art bug localization technique BLUiR which considers structure. We perform a large-scale experiment on four open source projects, namely AspectJ, Eclipse, SWT and ZXing to localize more than 3,000 bugs. Compared with a history-aware bug localization solution of Sisman and Kak, our approach achieves a 46.1% improvement in terms of mean average precision (MAP). Compared with BugLocator, our approach achieves a 24.4% improvement in terms of MAP. Compared with BLUiR, our approach achieves a 16.4% improvement in terms of MAP. },
  booktitle = {Proceedings of the 22nd International Conference on Program Comprehension},
  pages     = {53–63},
  numpages  = {11},
  keywords  = {Structure, Bug Localization, Version History, Similar Report},
  location  = {Hyderabad, India},
  series    = {ICPC 2014}
}

@inproceedings{10.1145/1101908.1101949,
  author    = {Jones, James A. and Harrold, Mary Jean},
  title     = {Empirical Evaluation of the Tarantula Automatic Fault-Localization Technique},
  year      = {2005},
  isbn      = {1581139934},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1101908.1101949},
  doi       = {10.1145/1101908.1101949},
  abstract  = {The high cost of locating faults in programs has motivated the development of techniques that assist in fault localization by automating part of the process of searching for faults. Empirical studies that compare these techniques have reported the relative effectiveness of four existing techniques on a set of subjects. These studies compare the rankings that the techniques compute for statements in the subject programs and the effectiveness of these rankings in locating the faults. However, it is unknown how these four techniques compare with Tarantula, another existing fault-localization technique, although this technique also provides a way to rank statements in terms of their suspiciousness. Thus, we performed a study to compare the Tarantula technique with the four techniques previously compared. This paper presents our study---it overviews the Tarantula technique along with the four other techniques studied, describes our experiment, and reports and discusses the results. Our studies show that, on the same set of subjects, the Tarantula technique consistently outperforms the other four techniques in terms of effectiveness in fault localization, and is comparable in efficiency to the least expensive of the other four techniques.},
  booktitle = {Proceedings of the 20th IEEE/ACM International Conference on Automated Software Engineering},
  pages     = {273–282},
  numpages  = {10},
  keywords  = {empirical study, fault localization, program analysis, automated debugging},
  location  = {Long Beach, CA, USA},
  series    = {ASE '05}
}

@inproceedings{ciregan2012multi,
  author    = {J. Schmidhuber and U. Meier and D. Ciresan},
  booktitle = {2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Multi-column deep neural networks for image classification},
  year      = {2012},
  volume    = {},
  issn      = {1063-6919},
  pages     = {3642-3649},
  keywords  = {neural nets;graphics processing units;handwritten character recognition;image classification;image recognition;learning (artificial intelligence);traffic sign recognition benchmark;multicolumn deep neural networks;image classification;computer vision;machine learning;human performance;handwritten digits recognition;traffic signs;artificial neural network architectures;convolutional winner-take-all neurons;retina;visual cortex;sparsely connected neural layers;graphics cards;fast training;mnist handwriting benchmark;training;error analysis;neurons;computer architecture;benchmark testing;graphics processing unit},
  doi       = {10.1109/CVPR.2012.6248110},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2012.6248110},
  publisher = {IEEE Computer Society},
  address   = {Los Alamitos, CA, USA},
  month     = {jun}
}

@article{kim2021datasets,
  title     = {Are datasets for information retrieval-based bug localization techniques trustworthy?},
  author    = {Kim, Misoo and Lee, Eunseok},
  journal   = {Empirical Software Engineering},
  volume    = {26},
  number    = {3},
  pages     = {1--66},
  year      = {2021},
  publisher = {Springer}
}


@inproceedings{choi2017gram,
  author    = {Choi, Edward and Bahadori, Mohammad Taha and Song, Le and Stewart, Walter F. and Sun, Jimeng},
  title     = {GRAM: Graph-Based Attention Model for Healthcare Representation Learning},
  year      = {2017},
  isbn      = {9781450348874},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3097983.3098126},
  doi       = {10.1145/3097983.3098126},
  abstract  = {Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain: - Data insufficiency: Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results.Interpretation: The representations learned by deep learning methods should align with medical knowledge.To address these challenges, we propose GRaph-based Attention Model (GRAM) that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. Based on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism.We compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task. Compared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.},
  booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages     = {787–795},
  numpages  = {9},
  keywords  = {electronic health records, attention model, predictive healthcare, graph},
  location  = {Halifax, NS, Canada},
  series    = {KDD '17}
}



@inproceedings{gopinath2014code,
  author    = {Gopinath, Rahul and Jensen, Carlos and Groce, Alex},
  title     = {Code Coverage for Suite Evaluation by Developers},
  year      = {2014},
  isbn      = {9781450327565},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2568225.2568278},
  doi       = {10.1145/2568225.2568278},
  abstract  = { One of the key challenges of developers testing code is determining a test suite's quality -- its ability to find faults. The most common approach is to use code coverage as a measure for test suite quality, and diminishing returns in coverage or high absolute coverage as a stopping rule. In testing research, suite quality is often evaluated by a suite's ability to kill mutants (artificially seeded potential faults). Determining which criteria best predict mutation kills is critical to practical estimation of test suite quality. Previous work has only used small sets of programs, and usually compares multiple suites for a single program. Practitioners, however, seldom compare suites --- they evaluate one suite. Using suites (both manual and automatically generated) from a large set of real-world open-source projects shows that evaluation results differ from those for suite-comparison: statement (not block, branch, or path) coverage predicts mutation kills best. },
  booktitle = {Proceedings of the 36th International Conference on Software Engineering},
  pages     = {72–82},
  numpages  = {11},
  keywords  = {evaluation of coverage criteria, statistical analysis, test frameworks},
  location  = {Hyderabad, India},
  series    = {ICSE 2014}
}

@techreport{CIFAR10,
  author      = {Alex Krizhevsky},
  title       = {Learning multiple layers of features from tiny images},
  institution = {CS Toronto},
  year        = {2009},
  url         = {https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf}
}
@inproceedings{surprise,
  author    = {Kim, Jinhan and Feldt, Robert and Yoo, Shin},
  title     = {Guiding Deep Learning System Testing Using Surprise Adequacy},
  year      = {2019},
  publisher = {IEEE Press},
  url       = {https://doi.org/10.1109/ICSE.2019.00108},
  doi       = {10.1109/ICSE.2019.00108},
  abstract  = {Deep Learning (DL) systems are rapidly being adopted in safety and security critical domains, urgently calling for ways to test their correctness and robustness. Testing of DL systems has traditionally relied on manual collection and labelling of data. Recently, a number of coverage criteria based on neuron activation values have been proposed. These criteria essentially count the number of neurons whose activation during the execution of a DL system satisfied certain properties, such as being above predefined thresholds. However, existing coverage criteria are not sufficiently fine grained to capture subtle behaviors exhibited by DL systems. Moreover, evaluations have focused on showing correlation between adversarial examples and proposed criteria rather than evaluating and guiding their use for actual testing of DL systems. We propose a novel test adequacy criterion for testing of DL systems, called Surprise Adequacy for Deep Learning Systems (SADL), which is based on the behaviour of DL systems with respect to their training data. We measure the surprise of an input as the difference in DL system's behaviour between the input and the training data (i.e., what was learnt during training), and subsequently develop this as an adequacy criterion: a good test input should be sufficiently but not overtly surprising compared to training data. Empirical evaluation using a range of DL systems from simple image classifiers to autonomous driving car platforms shows that systematic sampling of inputs based on their surprise can improve classification accuracy of DL systems against adversarial examples by up to 77.5% via retraining.},
  booktitle = {Proceedings of the 41st International Conference on Software Engineering},
  pages     = {1039–1049},
  numpages  = {11},
  keywords  = {deep learning systems, test adequacy},
  location  = {Montreal, Quebec, Canada},
  series    = {ICSE '19}
}

@article{mnist,
  author  = {Deng, Li},
  journal = {IEEE Signal Processing Magazine},
  title   = {The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web]},
  year    = {2012},
  volume  = {29},
  number  = {6},
  pages   = {141-142},
  doi     = {10.1109/MSP.2012.2211477},
  url     = {https://doi.org/10.1109/MSP.2012.2211477}
}


@inproceedings{42503,
  author    = {Christian Szegedy and
               Wojciech Zaremba and
               Ilya Sutskever and
               Joan Bruna and
               Dumitru Erhan and
               Ian J. Goodfellow and
               Rob Fergus},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Intriguing properties of neural networks},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year      = {2014},
  url       = {http://arxiv.org/abs/1312.6199},
  timestamp = {Thu, 25 Jul 2019 14:35:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SzegedyZSBEGF13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{correlation_fse_2020_2,
  author    = {Harel-Canada, Fabrice and Wang, Lingxiao and Gulzar, Muhammad Ali and Gu, Quanquan and Kim, Miryung},
  title     = {Is Neuron Coverage a Meaningful Measure for Testing Deep Neural Networks?},
  year      = {2020},
  isbn      = {9781450370431},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3368089.3409754},
  doi       = {10.1145/3368089.3409754},
  abstract  = {Recent effort to test deep learning systems has produced an intuitive and compelling test criterion called neuron coverage (NC), which resembles the notion of traditional code coverage. NC measures the proportion of neurons activated in a neural network and it is implicitly assumed that increasing NC improves the quality of a test suite. In an attempt to automatically generate a test suite that increases NC, we design a novel diversity promoting regularizer that can be plugged into existing adversarial attack algorithms. We then assess whether such attempts to increase NC could generate a test suite that (1) detects adversarial attacks successfully, (2) produces natural inputs, and (3) is unbiased to particular class predictions. Contrary to expectation, our extensive evaluation finds that increasing NC actually makes it harder to generate an effective test suite: higher neuron coverage leads to fewer defects detected, less natural inputs, and more biased prediction preferences. Our results invoke skepticism that increasing neuron coverage may not be a meaningful objective for generating tests for deep neural networks and call for a new test generation technique that considers defect detection, naturalness, and output impartiality in tandem.},
  booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages     = {851–862},
  numpages  = {12},
  keywords  = {Software Engineering, Machine Learning, Testing, Adversarial Attack, Neuron Coverage},
  location  = {Virtual Event, USA},
  series    = {ESEC/FSE 2020}
}
 

@inproceedings{svhn,
  title     = {Reading Digits in Natural Images with Unsupervised Feature Learning},
  author    = {Yuval Netzer and Tao Wang and Adam Coates and Alessandro Bissacco and Bo Wu and Andrew Y. Ng},
  year      = {2011},
  url       = {http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf},
  booktitle = {NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011}
}


@inproceedings{pmlr-v97-engstrom19a,
  title     = {Exploring the Landscape of Spatial Robustness},
  author    = {Engstrom, Logan and Tran, Brandon and Tsipras, Dimitris and Schmidt, Ludwig and Madry, Aleksander},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  pages     = {1802--1811},
  year      = {2019},
  editor    = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume    = {97},
  series    = {Proceedings of Machine Learning Research},
  address   = {Long Beach, California, USA},
  month     = {09--15 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v97/engstrom19a/engstrom19a.pdf},
  url       = {http://proceedings.mlr.press/v97/engstrom19a.html},
  abstract  = {The study of adversarial robustness has so far largely focused on perturbations bound in $\ell_p$-norms. However, state-of-the-art models turn out to be also vulnerable to other, more natural classes of perturbations such as translations and rotations. In this work, we thoroughly investigate the vulnerability of neural network–based classifiers to rotations and translations. While data augmentation offers relatively small robustness, we use ideas from robust optimization and test-time input aggregation to significantly improve robustness. Finally we find that, in contrast to the $\ell_p$-norm case, first-order methods cannot reliably find worst-case perturbations. This highlights spatial robustness as a fundamentally different setting requiring additional study.}
}

@inproceedings{ACFH2020square,
  title     = {Square Attack: a query-efficient black-box adversarial attack via random search},
  author    = {Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas and Hein, Matthias},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year      = {2020},
  url       = {https://arxiv.org/abs/1912.00049}
}



@inproceedings{ICECCS,
  author    = {Dong, Yizhen and Zhang, Peixin and Wang, Jingyi and Liu, Shuang and Sun, Jun and Hao, Jianye and Wang, Xinyu and Wang, Li and Dong, Jinsong and Dai, Ting},
  booktitle = {2020 25th International Conference on Engineering of Complex Computer Systems (ICECCS)},
  title     = {An Empirical Study on Correlation between Coverage and Robustness for Deep Neural Networks},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {73-82},
  doi       = {10.1109/ICECCS51672.2020.00016},
  url       = {https://doi.org/10.1109/ICECCS51672.2020.00016},
  abstract  = {Deep neural networks (DNN) are increasingly applied in safety-critical systems, e.g., for face recognition, autonomous car control and malware detection. It is also shown that DNNs are subject to attacks such as adversarial perturbation and thus must be properly tested. Many coverage criteria for DNN since have been proposed, inspired by the success of code coverage criteria for software programs. The expectation is that if a DNN is well tested (and retrained) according to such coverage criteria, it is more likely to be robust. In this work, we conduct an empirical study to evaluate the relationship between coverage, robustness and attack/defense metrics for DNN. Our study is the largest to date and systematically done based on 100 DNN models and 25 metrics. One of our findings is that there is limited correlation between coverage and robustness, i.e., improving coverage does not help improve the robustness. Our dataset and implementation have been made available to serve as a benchmark for future studies on testing DNN.}
}

@inproceedings{FSE_Yan,
  author    = {Yan, Shenao and Tao, Guanhong and Liu, Xuwei and Zhai, Juan and Ma, Shiqing and Xu, Lei and Zhang, Xiangyu},
  title     = {Correlations between Deep Neural Network Model Coverage Criteria and Model Quality},
  year      = {2020},
  isbn      = {9781450370431},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3368089.3409671},
  doi       = {10.1145/3368089.3409671},
  abstract  = {Inspired by the great success of using code coverage as guidance in software testing,
               a lot of neural network coverage criteria have been proposed to guide testing of neural
               network models (e.g., model accuracy under adversarial attacks). However, while the
               monotonic relation between code coverage and software quality has been supported by
               many seminal studies in software engineering, it remains largely unclear whether similar
               monotonicity exists between neural network model coverage and model quality. This
               paper sets out to answer this question. Specifically, this paper studies the correlation
               between DNN model quality and coverage criteria, effects of coverage guided adversarial
               example generation compared with gradient decent based methods, effectiveness of coverage
               based retraining compared with existing adversarial training, and the internal relationships
               among coverage criteria.},
  booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages     = {775–787},
  numpages  = {13},
  keywords  = {Deep Neural Networks, Software Testing},
  location  = {Virtual Event, USA},
  series    = {ESEC/FSE 2020}
}

@inproceedings{robot,
  author    = {Wang, Jingyi and Chen, Jialuo and Sun, Youcheng and Ma, Xingjun and Wang, Dongxia and Sun, Jun and Cheng, Peng},
  booktitle = {2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},
  title     = {RobOT: Robustness-Oriented Testing for Deep Learning Systems},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {300-311},
  doi       = {10.1109/ICSE43902.2021.00038}
}

@article{10.1007/s10664-021-10066-6,
  author     = {Pan, Rongqi and Bagherzadeh, Mojtaba and Ghaleb, Taher A. and Briand, Lionel},
  title      = {Test Case Selection and Prioritization Using Machine Learning: A Systematic Literature Review},
  year       = {2022},
  issue_date = {Mar 2022},
  publisher  = {Kluwer Academic Publishers},
  address    = {USA},
  volume     = {27},
  number     = {2},
  issn       = {1382-3256},
  url        = {https://doi.org/10.1007/s10664-021-10066-6},
  doi        = {10.1007/s10664-021-10066-6},
  abstract   = {Regression testing is an essential activity to assure that software code changes do not adversely affect existing functionalities. With the wide adoption of Continuous Integration (CI) in software projects, which increases the frequency of running software builds, running all tests can be time-consuming and resource-intensive. To alleviate that problem, Test case Selection and Prioritization (TSP) techniques have been proposed to improve regression testing by selecting and prioritizing test cases in order to provide early feedback to developers. In recent years, researchers have relied on Machine Learning (ML) techniques to achieve effective TSP (ML-based TSP). Such techniques help combine information about test cases, from partial and imperfect sources, into accurate prediction models. This work conducts a systematic literature review focused on ML-based TSP techniques, aiming to perform an in-depth analysis of the state of the art, thus gaining insights regarding future avenues of research. To that end, we analyze 29 primary studies published from 2006 to 2020, which have been identified through a systematic and documented process. This paper addresses five research questions addressing variations in ML-based TSP techniques and feature sets for training and testing ML models, alternative metrics used for evaluating the techniques, the performance of techniques, and the reproducibility of the published studies. We summarize the results related to our research questions in a high-level summary that can be used as a taxonomy for classifying future TSP studies.},
  journal    = {Empirical Softw. Engg.},
  month      = {mar},
  numpages   = {43},
  keywords   = {Software testing, Test case selection, Test case prioritization, Systematic literature review, Continuous integration, Machine learning}
}

@inproceedings{yang2022revisiting,
  author    = {Zhou Yang and Jieke Shi and Muhammad Hilmi Asyrofi and David Lo},
  booktitle = {2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)},
  title     = {Revisiting Neuron Coverage Metrics and Quality of Deep Neural Networks},
  year      = {2022},
  volume    = {},
  issn      = {1534-5351},
  pages     = {408-419},
  abstract  = {Deep neural networks (DNN) have been widely applied in modern life, including critical domains like autonomous driving, making it essential to ensure the reliability and robustness of DNN-powered systems. As an analogy to code coverage metrics for testing conventional software, researchers have proposed neuron coverage metrics and coverage-driven methods to generate DNN test cases. However, Yan et al. doubt the usefulness of existing coverage criteria in DNN testing. They show that a coverage-driven method is less effective than a gradient-based method in terms of both uncovering defects and improving model robustness. In this paper, we conduct a replication study of the work by Yan et al. and extend the experiments for deeper analysis. A larger model and a dataset of higher resolution images are included to examine the generalizability of the results. We also extend the experiments with more test case generation techniques and adjust the process of improving model robustness to be closer to the practical life cycle of DNN development. Our experiment results confirm the conclusion from Yan et al. that coverage-driven methods are less effective than gradient-based methods. Yan et al. find that using gradient-based methods to retrain cannot repair defects uncovered by coverage-driven methods. They attribute this to the fact that the two types of methods use different perturbation strategies: gradient-based methods perform differentiable transformations while coverage-driven methods can perform additional non-differentiable transformations. We test several hypotheses and further show that even coverage-driven methods are constrained only to perform differentiable transformations, the uncovered defects still cannot be repaired by adversarial training with gradient-based methods. Thus, defensive strategies for coverage-driven methods should be further studied.},
  keywords  = {measurement;deep learning;training;perturbation methods;neurons;neural networks;software},
  publisher = {IEEE Computer Society},
  address   = {Los Alamitos, CA, USA},
  month     = {mar}
}

@inproceedings{harel2020neuron,
  author    = {Harel-Canada, Fabrice and Wang, Lingxiao and Gulzar, Muhammad Ali and Gu, Quanquan and Kim, Miryung},
  title     = {Is Neuron Coverage a Meaningful Measure for Testing Deep Neural Networks?},
  year      = {2020},
  isbn      = {9781450370431},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3368089.3409754},
  doi       = {10.1145/3368089.3409754},
  abstract  = {Recent effort to test deep learning systems has produced an intuitive and compelling test criterion called neuron coverage (NC), which resembles the notion of traditional code coverage. NC measures the proportion of neurons activated in a neural network and it is implicitly assumed that increasing NC improves the quality of a test suite. In an attempt to automatically generate a test suite that increases NC, we design a novel diversity promoting regularizer that can be plugged into existing adversarial attack algorithms. We then assess whether such attempts to increase NC could generate a test suite that (1) detects adversarial attacks successfully, (2) produces natural inputs, and (3) is unbiased to particular class predictions. Contrary to expectation, our extensive evaluation finds that increasing NC actually makes it harder to generate an effective test suite: higher neuron coverage leads to fewer defects detected, less natural inputs, and more biased prediction preferences. Our results invoke skepticism that increasing neuron coverage may not be a meaningful objective for generating tests for deep neural networks and call for a new test generation technique that considers defect detection, naturalness, and output impartiality in tandem.},
  booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages     = {851–862},
  numpages  = {12},
  keywords  = {Software Engineering, Testing, Machine Learning, Neuron Coverage, Adversarial Attack},
  location  = {Virtual Event, USA},
  series    = {ESEC/FSE 2020}
}


@inproceedings{rl_coverage,
  author    = {Trujillo, Miller and Linares-V\'{a}squez, Mario and Escobar-Vel\'{a}squez, Camilo and Dusparic, Ivana and Cardozo, Nicol\'{a}s},
  title     = {Does Neuron Coverage Matter for Deep Reinforcement Learning? A Preliminary Study},
  year      = {2020},
  isbn      = {9781450379632},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3387940.3391462},
  doi       = {10.1145/3387940.3391462},
  abstract  = {Deep Learning (DL) is powerful family of algorithms used for a wide variety of problems and systems, including safety critical systems. As a consequence, analyzing, understanding, and testing DL models is attracting more practitioners and researchers with the purpose of implementing DL systems that are robust, reliable, efficient, and accurate. First software testing approaches for DL systems have focused on black-box testing, white-box testing, and test cases generation, in particular for deep neural networks (CNNs and RNNs). However, Deep Reinforcement Learning (DRL), which is a branch of DL extending reinforcement learning, is still out of the scope of research providing testing techniques for DL systems. In this paper, we present a first step towards testing of DRL systems. In particular, we investigate whether neuron coverage (a widely used metric for white-box testing of DNNs) could be used also for DRL systems, by analyzing coverage evolutionary patterns, and the correlation with RL rewards.},
  booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
  pages     = {215–220},
  numpages  = {6},
  keywords  = {Coverage analysis, Testing, Reinforcement learning, Deep networks},
  location  = {Seoul, Republic of Korea},
  series    = {ICSEW'20}
}

@misc{catchme,
  doi       = {10.48550/ARXIV.2112.01821},
  url       = {https://arxiv.org/abs/2112.01821},
  author    = {Wu, Xiaoliang and Rajan, Ajitha},
  keywords  = {Sound (cs.SD), Computation and Language (cs.CL), Software Engineering (cs.SE), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title     = {Catch Me If You Can: Blackbox Adversarial Attacks on Automatic Speech Recognition using Frequency Masking},
  publisher = {arXiv},
  year      = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{survey,
  author     = {Zhang, He and Babar, Muhammad Ali and Tell, Paolo},
  title      = {Identifying Relevant Studies in Software Engineering},
  year       = {2011},
  issue_date = {June, 2011},
  publisher  = {Butterworth-Heinemann},
  address    = {USA},
  volume     = {53},
  number     = {6},
  issn       = {0950-5849},
  url        = {https://doi.org/10.1016/j.infsof.2010.12.010},
  doi        = {10.1016/j.infsof.2010.12.010},
  abstract   = {Context: Systematic literature review (SLR) has become an important research methodology in software engineering since the introduction of evidence-based software engineering (EBSE) in 2004. One critical step in applying this methodology is to design and execute appropriate and effective search strategy. This is a time-consuming and error-prone step, which needs to be carefully planned and implemented. There is an apparent need for a systematic approach to designing, executing, and evaluating a suitable search strategy for optimally retrieving the target literature from digital libraries. Objective: The main objective of the research reported in this paper is to improve the search step of undertaking SLRs in software engineering (SE) by devising and evaluating systematic and practical approaches to identifying relevant studies in SE. Method: We have systematically selected and analytically studied a large number of papers (SLRs) to understand the state-of-the-practice of search strategies in EBSE. Having identified the limitations of the current ad-hoc nature of search strategies used by SE researchers for SLRs, we have devised a systematic and evidence-based approach to developing and executing optimal search strategies in SLRs. The proposed approach incorporates the concept of 'quasi-gold standard' (QGS), which consists of collection of known studies, and corresponding 'quasi-sensitivity' into the search process for evaluating search performance. Results: We conducted two participant-observer case studies to demonstrate and evaluate the adoption of the proposed QGS-based systematic search approach in support of SLRs in SE research. Conclusion: We report their findings based on the case studies that the approach is able to improve the rigor of search process in an SLR, as well as it can serve as a supplement to the guidelines for SLRs in EBSE. We plan to further evaluate the proposed approach using a series of case studies on varying research topics in SE.},
  journal    = {Inf. Softw. Technol.},
  month      = {jun},
  pages      = {625–637},
  numpages   = {13},
  keywords   = {Quasi-gold standard, Systematic literature review, Evidence-based software engineering, Search strategy}
}



@inproceedings{DeepGauge,
  author    = {Ma, Lei and Juefei-Xu, Felix and Zhang, Fuyuan and Sun, Jiyuan and Xue, Minhui and Li, Bo and Chen, Chunyang and Su, Ting and Li, Li and Liu, Yang and Zhao, Jianjun and Wang, Yadong},
  title     = {DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems},
  year      = {2018},
  isbn      = {9781450359375},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3238147.3238202},
  doi       = {10.1145/3238147.3238202},
  abstract  = {Deep learning (DL) defines a new data-driven programming paradigm that constructs the internal system logic of a crafted neuron network through a set of training data. We have seen wide adoption of DL in many safety-critical scenarios. However, a plethora of studies have shown that the state-of-the-art DL systems suffer from various vulnerabilities which can lead to severe consequences when applied to real-world applications. Currently, the testing adequacy of a DL system is usually measured by the accuracy of test data. Considering the limitation of accessible high quality test data, good accuracy performance on test data can hardly provide confidence to the testing adequacy and generality of DL systems. Unlike traditional software systems that have clear and controllable logic and functionality, the lack of interpretability in a DL system makes system analysis and defect detection difficult, which could potentially hinder its real-world deployment. In this paper, we propose DeepGauge, a set of multi-granularity testing criteria for DL systems, which aims at rendering a multi-faceted portrayal of the testbed. The in-depth evaluation of our proposed testing criteria is demonstrated on two well-known datasets, five DL systems, and with four state-of-the-art adversarial attack techniques against DL. The potential usefulness of DeepGauge sheds light on the construction of more generic and robust DL systems.},
  booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
  pages     = {120–131},
  numpages  = {12},
  keywords  = {Testing criteria, Deep learning, Deep neural networks, Software testing},
  location  = {Montpellier, France},
  series    = {ASE 2018}
}
@inproceedings{DeepTest,
  author    = {Tian, Yuchi and Pei, Kexin and Jana, Suman and Ray, Baishakhi},
  title     = {DeepTest: Automated Testing of Deep-Neural-Network-Driven Autonomous Cars},
  year      = {2018},
  isbn      = {9781450356381},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3180155.3180220},
  doi       = {10.1145/3180155.3180220},
  abstract  = {Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads.However, despite their spectacular progress, DNNs, just like traditional software, often demonstrate incorrect or unexpected corner-case behaviors that can lead to potentially fatal collisions. Several such real-world accidents involving autonomous cars have already happened including one which resulted in a fatality. Most existing testing techniques for DNN-driven vehicles are heavily dependent on the manual collection of test data under different driving conditions which become prohibitively expensive as the number of test conditions increases.In this paper, we design, implement, and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. First, our tool is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc. DeepTest systematically explore different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons. DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.},
  booktitle = {Proceedings of the 40th International Conference on Software Engineering},
  pages     = {303–314},
  numpages  = {12},
  keywords  = {neuron coverage, deep neural networks, deep learning, autonomous vehicle, testing, self-driving cars},
  location  = {Gothenburg, Sweden},
  series    = {ICSE '18}
}


@inproceedings{DeepHunter,
  author    = {Xie, Xiaofei and Ma, Lei and Juefei-Xu, Felix and Xue, Minhui and Chen, Hongxu and Liu, Yang and Zhao, Jianjun and Li, Bo and Yin, Jianxiong and See, Simon},
  title     = {DeepHunter: A Coverage-Guided Fuzz Testing Framework for Deep Neural Networks},
  year      = {2019},
  isbn      = {9781450362245},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3293882.3330579},
  doi       = {10.1145/3293882.3330579},
  abstract  = {The past decade has seen the great potential of applying deep neural network (DNN) based software to safety-critical scenarios, such as autonomous driving. Similar to traditional software, DNNs could exhibit incorrect behaviors, caused by hidden defects, leading to severe accidents and losses. In this paper, we propose DeepHunter, a coverage-guided fuzz testing framework for detecting potential defects of general-purpose DNNs. To this end, we first propose a metamorphic mutation strategy to generate new semantically preserved tests, and leverage multiple extensible coverage criteria as feedback to guide the test generation. We further propose a seed selection strategy that combines both diversity-based and recency-based seed selection. We implement and incorporate 5 existing testing criteria and 4 seed selection strategies in DeepHunter. Large-scale experiments demonstrate that (1) our metamorphic mutation strategy is useful to generate new valid tests with the same semantics as the original seed, by up to a 98% validity ratio; (2) the diversity-based seed selection generally weighs more than recency-based seed selection in boosting the coverage and in detecting defects; (3) DeepHunter outperforms the state of the arts by coverage as well as the quantity and diversity of defects identified; (4) guided by corner-region based criteria, DeepHunter is useful to capture defects during the DNN quantization for platform migration.},
  booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
  pages     = {146–157},
  numpages  = {12},
  keywords  = {Deep learning testing, coverage-guided fuzzing, metamorphic testing},
  location  = {Beijing, China},
  series    = {ISSTA 2019}
}

@article{DeepXplore,
  author     = {Pei, Kexin and Cao, Yinzhi and Yang, Junfeng and Jana, Suman},
  title      = {DeepXplore: Automated Whitebox Testing of Deep Learning Systems},
  year       = {2019},
  issue_date = {November 2019},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {62},
  number     = {11},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/3361566},
  doi        = {10.1145/3361566},
  abstract   = {Deep learning (DL) systems are increasingly deployed in safety- and security-critical domains such as self-driving cars and malware detection, where the correctness and predictability of a system's behavior for corner case inputs are of great importance. Existing DL testing depends heavily on manually labeled data and therefore often fails to expose erroneous behaviors for rare inputs.We design, implement, and evaluate DeepXplore, the first white-box framework for systematically testing real-world DL systems. First, we introduce neuron coverage for measuring the parts of a DL system exercised by test inputs. Next, we leverage multiple DL systems with similar functionality as cross-referencing oracles to avoid manual checking. Finally, we demonstrate how finding inputs for DL systems that both trigger many differential behaviors and achieve high neuron coverage can be represented as a joint optimization problem and solved efficiently using gradient-based search techniques.DeepXplore efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails and malware masquerading as benign software) in state-of-the-art DL models with thousands of neurons trained on five popular datasets such as ImageNet and Udacity self-driving challenge data. For all tested DL models, on average, DeepXplore generated one test input demonstrating incorrect behavior within one second while running only on a commodity laptop. We further show that the test inputs generated by DeepXplore can also be used to retrain the corresponding DL model to improve the model's accuracy by up to 3%.},
  journal    = {Commun. ACM},
  month      = oct,
  pages      = {137–145},
  numpages   = {9}
}


@inproceedings{l2arctic,
  author    = {Guanlong {Zhao} and Sinem {Sonsaat} and Alif {Silpachai} and Ivana {Lucic} and Evgeny {Chukharev-Hudilainen} and John {Levis} and Ricardo {Gutierrez-Osuna}},
  title     = {L2-ARCTIC: A Non-native English Speech Corpus},
  year      = 2018,
  booktitle = {Proc. Interspeech},
  pages     = {2783–2787},
  doi       = {10.21437/Interspeech.2018-1110},
  url       = {http://dx.doi.org/10.21437/Interspeech.2018-1110}
}

@inproceedings{librispeech,
  author    = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle = {2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Librispeech: An ASR corpus based on public domain audio books},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {5206-5210},
  doi       = {10.1109/ICASSP.2015.7178964}
}


@inproceedings{DeepCT,
  author    = {Ma, Lei and Juefei-Xu, Felix and Xue, Minhui and Li, Bo and Li, Li and Liu, Yang and Zhao, Jianjun},
  booktitle = {2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)},
  title     = {DeepCT: Tomographic Combinatorial Testing for Deep Learning Systems},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {614-618},
  abstract  = {Deep learning (DL) has achieved remarkable progress over the past decade and has been widely applied to many industry domains. However, the robustness of DL systems recently becomes great concerns, where minor perturbation on the input might cause the DL malfunction. These robustness issues could potentially result in severe consequences when a DL system is deployed to safety-critical applications and hinder the real-world deployment of DL systems. Testing techniques enable the robustness evaluation and vulnerable issue detection of a DL system at an early stage. The main challenge of testing a DL system attributes to the high dimensionality of its inputs and large internal latent feature space, which makes testing each state almost impossible. For traditional software, combinatorial testing (CT) is an effective testing technique to balance the testing exploration effort and defect detection capabilities. In this paper, we perform an exploratory study of CT on DL systems. We propose a set of combinatorial testing criteria specialized for DL systems, as well as a CT coverage guided test generation technique. Our evaluation demonstrates that CT provides a promising avenue for testing DL systems.},
  keywords  = {},
  doi       = {10.1109/SANER.2019.8668044},
  url       = {https://doi.org/10.1109/SANER.2019.8668044},
  issn      = {1534-5351},
  month     = {Feb}
}

@inproceedings{7958568,
  author    = {Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  booktitle = {2017 IEEE Symposium on Security and Privacy (SP)},
  title     = {Membership Inference Attacks Against Machine Learning Models},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {3-18},
  doi       = {10.1109/SP.2017.41}
}



@inproceedings{malhotra2019active,
  title     = {Active Learning Methods for Low Resource End-to-End Speech Recognition.},
  author    = {Malhotra, Karan and Bansal, Shubham and Ganapathy, Sriram},
  booktitle = {INTERSPEECH},
  pages     = {2215--2219},
  year      = {2019}
}

@inproceedings{4960685,
  author    = {Varadarajan, Balakrishnan and Yu, Dong and Li Deng and Acero, Alex},
  booktitle = {2009 IEEE International Conference on Acoustics, Speech and Signal Processing},
  title     = {Maximizing global entropy reduction for active learning in speech recognition},
  year      = {2009},
  volume    = {},
  number    = {},
  pages     = {4721-4724},
  doi       = {10.1109/ICASSP.2009.4960685}
}

@inproceedings{py150,
  author    = {Raychev, Veselin and Bielik, Pavol and Vechev, Martin},
  title     = {Probabilistic Model for Code with Decision Trees},
  year      = {2016},
  isbn      = {9781450344449},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2983990.2984041},
  doi       = {10.1145/2983990.2984041},
  abstract  = {In this paper we introduce a new approach for learning precise and general probabilistic models of code based on decision tree learning. Our approach directly benefits an emerging class of statistical programming tools which leverage probabilistic models of code learned over large codebases (e.g., GitHub) to make predictions about new programs (e.g., code completion, repair, etc). The key idea is to phrase the problem of learning a probabilistic model of code as learning a decision tree in a domain specific language over abstract syntax trees (called TGen). This allows us to condition the prediction of a program element on a dynamically computed context. Further, our problem formulation enables us to easily instantiate known decision tree learning algorithms such as ID3, but also to obtain new variants we refer to as ID3+ and E13, not previously explored and ones that outperform ID3 in prediction accuracy. Our approach is general and can be used to learn a probabilistic model of any programming language. We implemented our approach in a system called Deep3 and evaluated it for the challenging task of learning probabilistic models of JavaScript and Python. Our experimental results indicate that Deep3 predicts elements of JavaScript and Python code with precision above 82% and 69%, respectively. Further, Deep3 often significantly outperforms state-of-the-art approaches in overall prediction accuracy.},
  booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
  pages     = {731–747},
  numpages  = {17},
  keywords  = {Probabilistic Models of Code, Code Completion, Decision Trees},
  location  = {Amsterdam, Netherlands},
  series    = {OOPSLA 2016}
}

@article{10.1145/3022671.2984041,
  author     = {Raychev, Veselin and Bielik, Pavol and Vechev, Martin},
  title      = {Probabilistic Model for Code with Decision Trees},
  year       = {2016},
  issue_date = {October 2016},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {51},
  number     = {10},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/3022671.2984041},
  doi        = {10.1145/3022671.2984041},
  abstract   = {In this paper we introduce a new approach for learning precise and general probabilistic models of code based on decision tree learning. Our approach directly benefits an emerging class of statistical programming tools which leverage probabilistic models of code learned over large codebases (e.g., GitHub) to make predictions about new programs (e.g., code completion, repair, etc). The key idea is to phrase the problem of learning a probabilistic model of code as learning a decision tree in a domain specific language over abstract syntax trees (called TGen). This allows us to condition the prediction of a program element on a dynamically computed context. Further, our problem formulation enables us to easily instantiate known decision tree learning algorithms such as ID3, but also to obtain new variants we refer to as ID3+ and E13, not previously explored and ones that outperform ID3 in prediction accuracy. Our approach is general and can be used to learn a probabilistic model of any programming language. We implemented our approach in a system called Deep3 and evaluated it for the challenging task of learning probabilistic models of JavaScript and Python. Our experimental results indicate that Deep3 predicts elements of JavaScript and Python code with precision above 82% and 69%, respectively. Further, Deep3 often significantly outperforms state-of-the-art approaches in overall prediction accuracy.},
  journal    = {SIGPLAN Not.},
  month      = {oct},
  pages      = {731–747},
  numpages   = {17},
  keywords   = {Probabilistic Models of Code, Code Completion, Decision Trees}
}



@article{confidence,
  author     = {Hakkani-T\"{u}r, Dilek and Riccardi, Giuseppe and Tur, Gokhan},
  title      = {An Active Approach to Spoken Language Processing},
  year       = {2006},
  issue_date = {October 2006},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {3},
  number     = {3},
  issn       = {1550-4875},
  url        = {https://doi.org/10.1145/1177055.1177056},
  doi        = {10.1145/1177055.1177056},
  abstract   = {State of the art data-driven speech and language processing systems require a large amount of human intervention ranging from data annotation to system prototyping. In the traditional supervised passive approach, the system is trained on a given number of annotated data samples and evaluated using a separate test set. Then more data is collected arbitrarily, annotated, and the whole cycle is repeated. In this article, we propose the active approach where the system itself selects its own training data, evaluates itself and re-trains when necessary. We first employ active learning which aims to automatically select the examples that are likely to be the most informative for a given task. We use active learning for both selecting the examples to label and the examples to re-label in order to correct labeling errors. Furthermore, the system automatically evaluates itself using active evaluation to keep track of the unexpected events and decides on-demand to label more examples. The active approach enables dynamic adaptation of spoken language processing systems to unseen or unexpected events for nonstationary input while reducing the manual annotation effort significantly. We have evaluated the active approach with the AT&amp;T spoken dialog system used for customer care applications. In this article, we present our results for both automatic speech recognition and spoken language understanding.},
  journal    = {ACM Trans. Speech Lang. Process.},
  month      = {oct},
  pages      = {1–31},
  numpages   = {31},
  keywords   = {unsupervised learning, active learning, spoken dialog systems, Passive learning, adaptive learning, spoken language understanding, active evaluation, automatic speech recognition, speech and language processing}
}

@book{kotz2005encyclopedia,
  title     = {Encyclopedia of Statistical Sciences, Volume 1},
  author    = {Kotz, Samuel and Balakrishnan, Narayanaswamy and Read, Campbell B and Vidakovic, Brani},
  year      = {2005},
  publisher = {John Wiley \& Sons}
}

@article{hochreiter1997long,
  title     = {Long short-term memory},
  author    = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal   = {Neural computation},
  volume    = {9},
  number    = {8},
  pages     = {1735--1780},
  year      = {1997},
  publisher = {MIT Press}
}

@inproceedings{sensei,
  author    = {Gao, Xiang and Saha, Ripon K. and Prasad, Mukul R. and Roychoudhury, Abhik},
  title     = {Fuzz Testing Based Data Augmentation to Improve Robustness of Deep Neural Networks},
  year      = {2020},
  isbn      = {9781450371216},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3377811.3380415},
  doi       = {10.1145/3377811.3380415},
  abstract  = {Deep neural networks (DNN) have been shown to be notoriously brittle to small perturbations in their input data. This problem is analogous to the over-fitting problem in test-based program synthesis and automatic program repair, which is a consequence of the incomplete specification, i.e., the limited tests or training examples, that the program synthesis or repair algorithm has to learn from. Recently, test generation techniques have been successfully employed to augment existing specifications of intended program behavior, to improve the generalizability of program synthesis and repair. Inspired by these approaches, in this paper, we propose a technique that re-purposes software testing methods, specifically mutation-based fuzzing, to augment the training data of DNNs, with the objective of enhancing their robustness. Our technique casts the DNN data augmentation problem as an optimization problem. It uses genetic search to generate the most suitable variant of an input data to use for training the DNN, while simultaneously identifying opportunities to accelerate training by skipping augmentation in many instances. We instantiate this technique in two tools, Sensei and Sensei-SA, and evaluate them on 15 DNN models spanning 5 popular image data-sets. Our evaluation shows that Sensei can improve the robust accuracy of the DNN, compared to the state of the art, on each of the 15 models, by upto 11.9\% and 5.5\% on average. Further, Sensei-SA can reduce the average DNN training time by 25\%, while still improving robust accuracy.},
  booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
  pages     = {1147–1158},
  numpages  = {12},
  keywords  = {DNN, data augmentation, genetic algorithm, robustness},
  location  = {Seoul, South Korea},
  series    = {ICSE '20}
}

@inproceedings{LinB09-3,
  title     = {How to select a good training-data subset for transcription: submodular active selection for sequences},
  author    = {Hui Lin and Jeff Bilmes},
  year      = {2009},
  url       = {http://www.isca-speech.org/archive/interspeech_2009/i09_2859.html},
  researchr = {https://researchr.org/publication/LinB09-3},
  cites     = {0},
  citedby   = {0},
  pages     = {2859-2862},
  booktitle = {INTERSPEECH 2009, 10th Annual Conference of the International Speech Communication Association, Brighton, United Kingdom, September 6-10, 2009},
  publisher = {ISCA}
}

@inproceedings{aequevox,
  author    = {Rajan, Sai Sathiesh and Udeshi, Sakshi and Chattopadhyay, Sudipta},
  editor    = {Johnsen, Einar Broch and Wimmer, Manuel},
  title     = {AequeVox: Automated Fairness Testing of Speech Recognition Systems},
  booktitle = {Fundamental Approaches to Software Engineering},
  year      = {2022},
  publisher = {Springer International Publishing},
  pages     = {245--267}
}


@article{ma2019privacy,
  title     = {Privacy-preserving outsourced speech recognition for smart IoT devices},
  author    = {Ma, Zhuo and Liu, Yang and Liu, Ximeng and Ma, Jianfeng and Li, Feifei},
  journal   = {IEEE Internet of Things Journal},
  volume    = {6},
  number    = {5},
  pages     = {8406--8420},
  year      = {2019},
  publisher = {IEEE}
}

@article{wav2vec,
  author     = {Steffen Schneider and
                Alexei Baevski and
                Ronan Collobert and
                Michael Auli},
  title      = {wav2vec: Unsupervised Pre-training for Speech Recognition},
  journal    = {CoRR},
  volume     = {abs/1904.05862},
  year       = {2019},
  url        = {http://arxiv.org/abs/1904.05862},
  eprinttype = {arXiv},
  eprint     = {1904.05862},
  timestamp  = {Thu, 25 Apr 2019 13:55:01 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1904-05862.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{crossasr,
  author    = {Asyrofi, Muhammad Hilmi and Thung, Ferdian and Lo, David and Jiang, Lingxiao},
  booktitle = {2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  title     = {CrossASR: Efficient Differential Testing of Automatic Speech Recognition via Text-To-Speech},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {640-650},
  doi       = {10.1109/ICSME46990.2020.00066}
}

@article{crowdsourcing,
  title   = {Crowdsourcing platform for large-scale speech data collection},
  author  = {Freitas, Joao and Calado, Ant{\'o}nio and Braga, Daniela and Silva, Pedro and Dias, M},
  journal = {Proc. Fala},
  year    = {2010}
}


@article{deepspeech,
  title   = {{Deep Speech}: Scaling up end-to-end speech recognition},
  author  = {Awni Y. Hannun and Carl Case and Jared Casper and Bryan Catanzaro and Greg Diamos and Erich Elsen and Ryan Prenger and Sanjeev Satheesh and Shubho Sengupta and Adam Coates and Andrew Y. Ng},
  journal = {ArXiv},
  year    = {2014},
  volume  = {abs/1412.5567}
}

@article{tjoa2020survey,
  title     = {A survey on explainable artificial intelligence (xai): Toward medical xai},
  author    = {Tjoa, Erico and Guan, Cuntai},
  journal   = {IEEE transactions on neural networks and learning systems},
  volume    = {32},
  number    = {11},
  pages     = {4793--4813},
  year      = {2020},
  publisher = {IEEE}
}

@article{zhuang2022randomness,
  title   = {Randomness in neural network training: Characterizing the impact of tooling},
  author  = {Zhuang, Donglin and Zhang, Xingyao and Song, Shuaiwen and Hooker, Sara},
  journal = {Proceedings of Machine Learning and Systems},
  volume  = {4},
  pages   = {316--336},
  year    = {2022}
}


@inproceedings{quartznet,
  title     = {QuartzNet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions},
  author    = {Samuel Kriman and Stanislav Beliaev and Boris Ginsburg and Jocelyn Huang and Oleksii Kuchaiev and Vitaly Lavrukhin and Ryan Leary and Jason Li and Yang Zhang},
  booktitle = {ICASSP 2022 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {6124–6128}
}


@inproceedings{ai_2,
  author    = {Gehr, Timon and Mirman, Matthew and Drachsler-Cohen, Dana and Tsankov, Petar and Chaudhuri, Swarat and Vechev, Martin},
  booktitle = {2018 IEEE Symposium on Security and Privacy (SP)},
  title     = {AI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {3-18},
  abstract  = {We present AI2, the first sound and scalable analyzer for deep neural networks. Based on overapproximation, AI2 can automatically prove safety properties (e.g., robustness) of realistic neural networks (e.g., convolutional neural networks). The key insight behind AI2 is to phrase reasoning about safety and robustness of neural networks in terms of classic abstract interpretation, enabling us to leverage decades of advances in that area. Concretely, we introduce abstract transformers that capture the behavior of fully connected and convolutional neural network layers with rectified linear unit activations (ReLU), as well as max pooling layers. This allows us to handle real-world neural networks, which are often built out of those types of layers. We present a complete implementation of AI2 together with an extensive evaluation on 20 neural networks. Our results demonstrate that: (i) AI2 is precise enough to prove useful specifications (e.g., robustness), (ii) AI2 can be used to certify the effectiveness of state-of-the-art defenses for neural networks, (iii) AI2 is significantly faster than existing analyzers based on symbolic analysis, which often take hours to verify simple fully connected networks, and (iv) AI2 can handle deep convolutional networks, which are beyond the reach of existing methods.},
  keywords  = {},
  doi       = {10.1109/SP.2018.00058},
  url       = {https://doi.org/10.1109/SP.2018.00058},
  issn      = {2375-1207},
  month     = {May}
}


@inproceedings{DeepConcolic,
  author    = {Sun, Youcheng and Wu, Min and Ruan, Wenjie and Huang, Xiaowei and Kwiatkowska, Marta and Kroening, Daniel},
  title     = {Concolic Testing for Deep Neural Networks},
  year      = {2018},
  isbn      = {9781450359375},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi-org.libproxy.smu.edu.sg/10.1145/3238147.3238172},
  doi       = {10.1145/3238147.3238172},
  abstract  = {Concolic testing combines program execution and symbolic analysis to explore the execution paths of a software program. In this paper, we develop the first concolic testing approach for Deep Neural Networks (DNNs). More specifically, we utilise quantified linear arithmetic over rationals to express test requirements that have been studied in the literature, and then develop a coherent method to perform concolic testing with the aim of better coverage. Our experimental results show the effectiveness of the concolic testing approach in both achieving high coverage and finding adversarial examples.},
  booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
  pages     = {109–119},
  numpages  = {11},
  keywords  = {symbolic execution, concolic testing, neural networks},
  location  = {Montpellier, France},
  series    = {ASE 2018}
}

@inproceedings{crossasrpp,
  author    = {Asyrofi, Muhammad Hilmi and Yang, Zhou and Lo, David},
  title     = {CrossASR++: A Modular Differential Testing Framework for Automatic Speech Recognition},
  year      = {2021},
  isbn      = {9781450385626},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3468264.3473124},
  doi       = {10.1145/3468264.3473124},
  abstract  = {Developers need to perform adequate testing to ensure the quality of Automatic Speech Recognition (ASR) systems. However, manually collecting required test cases is tedious and time-consuming. Our recent work proposes CrossASR, a differential testing method for ASR systems. This method first utilizes Text-to-Speech (TTS) to generate audios from texts automatically and then feed these audios into different ASR systems for cross-referencing to uncover failed test cases. It also leverages a failure estimator to find failing test cases more efficiently. Such a method is inherently self-improvable: the performance can increase by leveraging more advanced TTS and ASR systems. So, in this accompanying tool demo paper, we further engineer CrossASR and propose CrossASR++, an easy-to-use ASR testing tool that can be conveniently extended to incorporate different TTS and ASR systems, and failure estimators. We also make CrossASR++ chunk texts from a given corpus dynamically and enable the estimator to work in a more effective and flexible way. We demonstrate that the new features can help CrossASR++ discover more failed test cases. Using the same TTS and ASR systems, CrossASR++ can uncover 26.2% more failed test cases for 4 ASRs than the original tool. Moreover, by simply adding one more ASR for cross-referencing, we can increase the number of failed test cases uncovered for each of the 4 ASR systems by 25.07%, 39.63%, 20.95% and 8.17% respectively. We also extend CrossASR++ with 5 additional failure estimators. Compared to worst estimator, the best one can discover 10.41% more failed test cases within the same amount of time. The demo video for CrossASR++ can be viewed at https://youtu.be/ddRk-f0QV-g and the source code can be found at https://github.com/soarsmu/CrossASRplus.},
  booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages     = {1575–1579},
  numpages  = {5},
  keywords  = {Cross-Referencing, Automatic Speech Recognition, Text-to-Speech, Test Case Generation},
  location  = {Athens, Greece},
  series    = {ESEC/FSE 2021}
}

@inproceedings{6289079,
  author    = {Schuster, Mike and Nakajima, Kaisuke},
  booktitle = {2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Japanese and Korean voice search},
  year      = {2012},
  volume    = {},
  number    = {},
  pages     = {5149-5152},
  doi       = {10.1109/ICASSP.2012.6289079}
}

@article{wordpiece,
  author     = {Yonghui Wu and
                Mike Schuster and
                Zhifeng Chen and
                Quoc V. Le and
                Mohammad Norouzi and
                Wolfgang Macherey and
                Maxim Krikun and
                Yuan Cao and
                Qin Gao and
                Klaus Macherey and
                Jeff Klingner and
                Apurva Shah and
                Melvin Johnson and
                Xiaobing Liu and
                Lukasz Kaiser and
                Stephan Gouws and
                Yoshikiyo Kato and
                Taku Kudo and
                Hideto Kazawa and
                Keith Stevens and
                George Kurian and
                Nishant Patil and
                Wei Wang and
                Cliff Young and
                Jason Smith and
                Jason Riesa and
                Alex Rudnick and
                Oriol Vinyals and
                Greg Corrado and
                Macduff Hughes and
                Jeffrey Dean},
  title      = {Google's Neural Machine Translation System: Bridging the Gap between
                Human and Machine Translation},
  journal    = {CoRR},
  volume     = {abs/1609.08144},
  year       = {2016},
  url        = {http://arxiv.org/abs/1609.08144},
  eprinttype = {arXiv},
  eprint     = {1609.08144},
  timestamp  = {Thu, 14 Jan 2021 12:12:19 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/WuSCLNMKCGMKSJL16.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{10.1145/375360.375365,
  author     = {Navarro, Gonzalo},
  title      = {A Guided Tour to Approximate String Matching},
  year       = {2001},
  issue_date = {March 2001},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {33},
  number     = {1},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/375360.375365},
  doi        = {10.1145/375360.375365},
  abstract   = {We survey the current techniques to cope with the problem of string matching that allows errors. This is becoming a more and more relevant issue for many fast growing areas such as information retrieval and computational biology. We focus on online searching and mostly on edit distance, explaining the problem and its relevance, its statistical behavior, its history and current developments, and the central ideas of the algorithms and their complexities. We present a number of experiments to compare the performance of the different algorithms and show which are the best choices. We conclude with some directions for future work and open problems.},
  journal    = {ACM Comput. Surv.},
  month      = {mar},
  pages      = {31–88},
  numpages   = {58},
  keywords   = {text searching allowing errors, edit distance, Levenshtein distance, online string matching}
}

@inproceedings{phonme_rich,
  author    = {Shinohara, Yusuke},
  booktitle = {2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {A submodular optimization approach to sentence set selection},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {4112-4115},
  doi       = {10.1109/ICASSP.2014.phonme_rich}
}

@inproceedings{asdf-paper,
  author    = {Yuen, Daniel Hao Xian and Pang, Andrew Yong Chen and Yang, Zhou and Chong, Chun Yong and Lim, Mei Kuan and Lo, David},
  title     = {ASDF: A Differential Testing Framework for Automatic Speech Recognition Systems},
  booktitle = {16th {IEEE} Conference on Software Testing, Verification and Validation,
               {ICST}},
  publisher = {{IEEE}},
  year      = {2023}
}



@electronic{asdf,
  howpublished = {Online},
  month        = jan,
  organization = {GitHub},
  title        = {ASR Differential Testing Framework (ASDF)},
  url          = {https://github.com/danielyuenhx/asdf-differential-testing},
  year         = {2023}
}

@electronic{crossasr++,
  howpublished = {Online},
  month        = dec,
  organization = {GitHub},
  title        = {CrossASR++},
  url          = {https://github.com/soarsmu/CrossASRplus},
  year         = {2021}
}



@electronic{wordhoard,
  author       = {John Bumgarner},
  howpublished = {Online},
  month        = may,
  organization = {GitHub},
  title        = {WordHoard},
  url          = {https://github.com/johnbumgarner/wordhoard},
  year         = {2022}
}


@electronic{wav2letter-repo,
  howpublished = {Online},
  month        = dec,
  organization = {GitHub},
  title        = {wav2letter},
  url          = {https://github.com/flashlight/wav2letter},
  year         = {2022}
}

@electronic{gtts-repo,
  howpublished = {Online},
  month        = dec,
  organization = {GitHub},
  title        = {gTTS},
  url          = {https://github.com/pndurette/gTTS},
  year         = {2022}
}

@article{sedgwick2014spearman,
  title     = {Spearman’s rank correlation coefficient},
  author    = {Sedgwick, Philip},
  journal   = {Bmj},
  volume    = {349},
  year      = {2014},
  publisher = {British Medical Journal Publishing Group}
}

@article{Whang,
  author  = {Whang, Steven and Lee, Jae-Gil},
  year    = {2020},
  month   = {08},
  pages   = {3429-3432},
  title   = {Data collection and quality challenges for deep learning},
  volume  = {13},
  journal = {Proceedings of the VLDB Endowment},
  doi     = {10.14778/3415478.3415562}
}

 @misc{heaven_2022,
  title     = {Ai is wrestling with a replication crisis},
  url       = {shorturl.at/djMN3},
  journal   = {MIT Technology Review},
  publisher = {MIT Technology Review},
  author    = {Heaven, Will Douglas},
  year      = {2022},
  month     = {Apr}
} 

@article{strickland2019ibm,
  title     = {IBM Watson, heal thyself: How IBM overpromised and underdelivered on AI health care},
  author    = {Strickland, Eliza},
  journal   = {IEEE Spectrum},
  volume    = {56},
  number    = {4},
  pages     = {24--31},
  year      = {2019},
  publisher = {IEEE}
}

@article{danilevsky2020survey,
  title   = {A survey of the state of explainable AI for natural language processing},
  author  = {Danilevsky, Marina and Qian, Kun and Aharonov, Ranit and Katsis, Yannis and Kawas, Ban and Sen, Prithviraj},
  journal = {arXiv preprint arXiv:2010.00711},
  year    = {2020}
}

@article{chowdhary2020natural,
  title     = {Natural language processing},
  author    = {Chowdhary, KR1442},
  journal   = {Fundamentals of artificial intelligence},
  pages     = {603--649},
  year      = {2020},
  publisher = {Springer}
}

@inproceedings{7995975,
  author    = {Chen, Zhilu and Huang, Xinming},
  booktitle = {2017 IEEE Intelligent Vehicles Symposium (IV)},
  title     = {End-to-end learning for lane keeping of self-driving cars},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {1856-1860},
  doi       = {10.1109/IVS.2017.7995975}
}


@article{lalwani2018implementation,
  title   = {Implementation of a Chatbot System using AI and NLP},
  author  = {Lalwani, Tarun and Bhalotia, Shashank and Pal, Ashish and Rathod, Vasundhara and Bisen, Shreya},
  journal = {International Journal of Innovative Research in Computer Science \& Technology (IJIRCST) Volume-6, Issue-3},
  year    = {2018}
}

@inproceedings{alsulami_source_2017,
  title     = {Source code authorship attribution using long short-term memory based networks},
  doi       = {10.1007/978-3-319-66402-6_6},
  language  = {English (US)},
  urldate   = {2021-08-21},
  booktitle = {Computer {Security} - {ESORICS} 2017},
  publisher = {Springer Verlag},
  author    = {Alsulami, Bander and Dauber, Edwin and Harang, Richard and Mancoridis, Spiros and Greenstadt, Rachel},
  year      = {2017},
  pages     = {65--82}
}


@article{zhang2019explorative,
  title   = {An explorative study of github repositories of ai papers},
  author  = {Zhang, Boyang},
  journal = {arXiv preprint arXiv:1903.01555},
  year    = {2019}
}

@inproceedings{gonzalez2020state,
  title     = {The state of the ml-universe: 10 years of artificial intelligence \& machine learning software development on github},
  author    = {Gonzalez, Danielle and Zimmermann, Thomas and Nagappan, Nachiappan},
  booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
  pages     = {431--442},
  year      = {2020}
}

@article{tomavsev2020ai,
  title     = {AI for social good: unlocking the opportunity for positive impact},
  author    = {Toma{\v{s}}ev, Nenad and Cornebise, Julien and Hutter, Frank and Mohamed, Shakir and Picciariello, Angela and Connelly, Bec and Belgrave, Danielle and Ezer, Daphne and Haert, Fanny Cachat van der and Mugisha, Frank and others},
  journal   = {Nature Communications},
  volume    = {11},
  number    = {1},
  pages     = {1--6},
  year      = {2020},
  publisher = {Nature Publishing Group}
}


@article{goralski2020artificial,
  title     = {Artificial intelligence and sustainable development},
  author    = {Goralski, Margaret A and Tan, Tay Keong},
  journal   = {The International Journal of Management Education},
  volume    = {18},
  number    = {1},
  pages     = {100330},
  year      = {2020},
  publisher = {Elsevier}
}

@article{carvalho2019off,
  title   = {Off-the-shelf artificial intelligence technologies for sentiment and emotion analysis: a tutorial on using IBM natural language processing},
  author  = {Carvalho, Arthur and Levitt, Adam and Levitt, Seth and Khaddam, Edward and Benamati, John},
  journal = {Communications of the Association for Information Systems},
  volume  = {44},
  number  = {1},
  pages   = {43},
  year    = {2019}
}

@article{izadi2022predicting,
  title     = {Predicting the objective and priority of issue reports in software repositories},
  author    = {Izadi, Maliheh and Akbari, Kiana and Heydarnoori, Abbas},
  journal   = {Empirical Software Engineering},
  volume    = {27},
  number    = {2},
  pages     = {1--37},
  year      = {2022},
  publisher = {Springer}
}

@article{Wilcoxon,
  issn      = {00994987},
  url       = {http://www.jstor.org/stable/3001968},
  author    = {Frank Wilcoxon},
  journal   = {Biometrics Bulletin},
  number    = {6},
  pages     = {80--83},
  publisher = {[International Biometric Society, Wiley]},
  title     = {Individual Comparisons by Ranking Methods},
  urldate   = {2023-01-03},
  volume    = {1},
  year      = {1945}
}

@inproceedings{checklist,
  title     = {Beyond Accuracy: Behavioral Testing of {NLP} Models with {C}heck{L}ist},
  author    = {Ribeiro, Marco Tulio  and
               Wu, Tongshuang  and
               Guestrin, Carlos  and
               Singh, Sameer},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.acl-main.442},
  doi       = {10.18653/v1/2020.acl-main.442},
  pages     = {4902--4912},
  abstract  = {Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.}
}

@inproceedings{6405375,
  author    = {Thung, Ferdian and Wang, Shaowei and Lo, David and Jiang, Lingxiao},
  booktitle = {2012 IEEE 23rd International Symposium on Software Reliability Engineering},
  title     = {An Empirical Study of Bugs in Machine Learning Systems},
  year      = {2012},
  volume    = {},
  number    = {},
  pages     = {271-280},
  doi       = {10.1109/ISSRE.2012.22}
}


@inproceedings{8305957,
  author    = {Sun, Xiaobing and Zhou, Tianchi and Li, Gengjie and Hu, Jiajun and Yang, Hui and Li, Bin},
  booktitle = {2017 24th Asia-Pacific Software Engineering Conference (APSEC)},
  title     = {An Empirical Study on Real Bugs for Machine Learning Programs},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {348-357},
  doi       = {10.1109/APSEC.2017.41}
}


@inproceedings{10.1145/3377811.3380395,
  author    = {Humbatova, Nargiz and Jahangirova, Gunel and Bavota, Gabriele and Riccio, Vincenzo and Stocco, Andrea and Tonella, Paolo},
  title     = {Taxonomy of Real Faults in Deep Learning Systems},
  year      = {2020},
  isbn      = {9781450371216},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  abstract  = {The growing application of deep neural networks in safety-critical domains makes the analysis of faults that occur in such systems of enormous importance. In this paper we introduce a large taxonomy of faults in deep learning (DL) systems. We have manually analysed 1059 artefacts gathered from GitHub commits and issues of projects that use the most popular DL frameworks (TensorFlow, Keras and PyTorch) and from related Stack Overflow posts. Structured interviews with 20 researchers and practitioners describing the problems they have encountered in their experience have enriched our taxonomy with a variety of additional faults that did not emerge from the other two sources. Our final taxonomy was validated with a survey involving an additional set of 21 developers, confirming that almost all fault categories (13/15) were experienced by at least 50% of the survey participants.},
  booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
  pages     = {1110–1121},
  numpages  = {12},
  keywords  = {taxonomy, deep learning, software testing, real faults},
  location  = {Seoul, South Korea},
  series    = {ICSE '20}
}

@article{Spohrer_2021,
  title        = {The Role of Open-Source Software in Artificial Intelligence},
  volume       = {42},
  url          = {https://ojs.aaai.org/index.php/aimagazine/article/view/7488},
  abstractnote = {&lt;div&gt;
                  &lt;p class=&quot;abstract&quot;&gt;&lt;span lang=&quot;EN-IN&quot;&gt;With this publication, we launch a new column for &lt;em&gt;AI Magazine&lt;/em&gt; on the role of open-source software in artificial intelligence. As the column editor, I would like to extend my welcome and invite &lt;em&gt;AI Magazine readers&lt;/em&gt; to send short articles for future columns, which may appear in the traditional print version of &lt;em&gt;AI Magazine&lt;/em&gt;, or on the &lt;em&gt;AI Magazine&lt;/em&gt; interactive site currently under development. This introductory column serves to highlight my interests in open-source software and to propose a few topics for future columns.&lt;/span&gt;&lt;/p&gt;
                  &lt;/div&gt;},
  number       = {1},
  journal      = {AI Magazine},
  author       = {Spohrer, Jim},
  year         = {2021},
  month        = {Apr.},
  pages        = {93-94}
}

@article{gundersen2018reproducible,
  title   = {On reproducible AI: Towards reproducible research, open science, and digital scholarship in AI publications},
  author  = {Gundersen, Odd Erik and Gil, Yolanda and Aha, David W},
  journal = {AI magazine},
  volume  = {39},
  number  = {3},
  pages   = {56--68},
  year    = {2018}
}


@inproceedings{dosovitskiy2017carla,
  title        = {CARLA: An open urban driving simulator},
  author       = {Dosovitskiy, Alexey and Ros, German and Codevilla, Felipe and Lopez, Antonio and Koltun, Vladlen},
  booktitle    = {Conference on robot learning},
  pages        = {1--16},
  year         = {2017},
  organization = {PMLR}
}

@inproceedings{amershi2019SE4AI,
  author    = {Amershi, Saleema and Begel, Andrew and Bird, Christian and DeLine, Robert and Gall, Harald and Kamar, Ece and Nagappan, Nachiappan and Nushi, Besmira and Zimmermann, Thomas},
  booktitle = {2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)},
  title     = {Software Engineering for Machine Learning: A Case Study},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {291-300},
  doi       = {10.1109/ICSE-SEIP.2019.00042}
}

@article{brockman2016openai,
  title   = {Openai gym},
  author  = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal = {arXiv preprint arXiv:1606.01540},
  year    = {2016}
}


@article{mchugh2012interrater,
  title     = {Interrater reliability: the kappa statistic},
  author    = {McHugh, Mary L},
  journal   = {Biochemia medica},
  volume    = {22},
  number    = {3},
  pages     = {276--282},
  year      = {2012},
  publisher = {Medicinska naklada}
}

@inproceedings{10.1145/3522664.3528620,
  author    = {Zhang, Haiyin and Cruz, Lu\'{\i}s and van Deursen, Arie},
  title     = {Code Smells for Machine Learning Applications},
  year      = {2022},
  isbn      = {9781450392754},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3522664.3528620},
  doi       = {10.1145/3522664.3528620},
  abstract  = {The popularity of machine learning has wildly expanded in recent years. Machine learning techniques have been heatedly studied in academia and applied in the industry to create business value. However, there is a lack of guidelines for code quality in machine learning applications. In particular, code smells have rarely been studied in this domain. Although machine learning code is usually integrated as a small part of an overarching system, it usually plays an important role in its core functionality. Hence ensuring code quality is quintessential to avoid issues in the long run. This paper proposes and identifies a list of 22 machine learning-specific code smells collected from various sources, including papers, grey literature, GitHub commits, and Stack Overflow posts. We pinpoint each smell with a description of its context, potential issues in the long run, and proposed solutions. In addition, we link them to their respective pipeline stage and the evidence from both academic and grey literature. The code smell catalog helps data scientists and developers produce and maintain high-quality machine learning application code.},
  booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
  pages     = {217–228},
  numpages  = {12},
  keywords  = {code smell, technical debt, anti-pattern, code quality, machine learning},
  location  = {Pittsburgh, Pennsylvania},
  series    = {CAIN '22}
}

@article{fan2021makes,
  title     = {What makes a popular academic AI repository?},
  author    = {Fan, Yuanrui and Xia, Xin and Lo, David and Hassan, Ahmed E and Li, Shanping},
  journal   = {Empirical Software Engineering},
  volume    = {26},
  number    = {1},
  pages     = {1--35},
  year      = {2021},
  publisher = {Springer}
}


@article{WANG2022106845,
  title    = {Personalizing label prediction for GitHub issues},
  journal  = {Information and Software Technology},
  volume   = {145},
  pages    = {106845},
  year     = {2022},
  issn     = {0950-5849},
  doi      = {https://doi.org/10.1016/j.infsof.2022.106845},
  url      = {https://www.sciencedirect.com/science/article/pii/S0950584922000192},
  author   = {Jun Wang and Xiaofang Zhang and Lin Chen and Xiaoyuan Xie},
  keywords = {Deep learning, Issue labeling, Data analysis, Language model}
}




@article{github-disc,
  author     = {Hata, Hideaki and Novielli, Nicole and Baltes, Sebastian and Kula, Raula Gaikovina and Treude, Christoph},
  title      = {GitHub Discussions: An Exploratory Study of Early Adoption},
  year       = {2022},
  issue_date = {Jan 2022},
  publisher  = {Kluwer Academic Publishers},
  address    = {USA},
  volume     = {27},
  number     = {1},
  issn       = {1382-3256},
  url        = {https://doi.org/10.1007/s10664-021-10058-6},
  doi        = {10.1007/s10664-021-10058-6},
  abstract   = {Discussions is a new feature of GitHub for asking questions or discussing topics outside of specific Issues or Pull Requests. Before being available to all projects in December 2020, it had been tested on selected open source software projects. To understand how developers use this novel feature, how they perceive it, and how it impacts the development processes, we conducted a mixed-methods study based on early adopters of GitHub discussions from January until July 2020. We found that: (1) errors, unexpected behavior, and code reviews are prevalent discussion categories; (2) there is a positive relationship between project member involvement and discussion frequency; (3) developers consider GitHub Discussions useful but face the problem of topic duplication between Discussions and Issues; (4) Discussions play a crucial role in advancing the development of projects; and (5) positive sentiment in Discussions is more frequent than in Stack Overflow posts. Our findings are a first step towards data-informed guidance for using GitHub Discussions, opening up avenues for future work on this novel communication channel.},
  journal    = {Empirical Softw. Engg.},
  month      = {jan},
  numpages   = {32},
  keywords   = {Sentiment, GitHub discussions, Empirical study, Communications, Exploratory study}
}

@article{ntoutsi2020bias,
  title     = {Bias in data-driven artificial intelligence systems—An introductory survey},
  author    = {Ntoutsi, Eirini and Fafalios, Pavlos and Gadiraju, Ujwal and Iosifidis, Vasileios and Nejdl, Wolfgang and Vidal, Maria-Esther and Ruggieri, Salvatore and Turini, Franco and Papadopoulos, Symeon and Krasanakis, Emmanouil and others},
  journal   = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  volume    = {10},
  number    = {3},
  pages     = {e1356},
  year      = {2020},
  publisher = {Wiley Online Library}
}

@inproceedings{wolf2020ai,
  title        = {AI models and their worlds: Investigating data-driven, AI/ML ecosystems through a work practices lens},
  author       = {Wolf, Christine T},
  booktitle    = {International conference on information},
  pages        = {651--664},
  year         = {2020},
  organization = {Springer}
}

@article{jiao2019survey,
  title     = {A survey on the new generation of deep learning in image processing},
  author    = {Jiao, Licheng and Zhao, Jin},
  journal   = {IEEE Access},
  volume    = {7},
  pages     = {172231--172263},
  year      = {2019},
  publisher = {IEEE}
}


@inproceedings{10.1145/1718918.1718973,
  author    = {Breu, Silvia and Premraj, Rahul and Sillito, Jonathan and Zimmermann, Thomas},
  title     = {Information Needs in Bug Reports: Improving Cooperation between Developers and Users},
  year      = {2010},
  isbn      = {9781605587950},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1718918.1718973},
  doi       = {10.1145/1718918.1718973},
  abstract  = {For many software projects, bug tracking systems play a central role in supporting collaboration between the developers and the users of the software. To better understand this collaboration and how tool support can be improved, we have quantitatively and qualitatively analysed the questions asked in a sample of 600 bug reports from the MOZILLA and ECLIPSE projects. We categorised the questions and analysed response rates and times by category and project. Our results show that the role of users goes beyond simply reporting bugs: their active and ongoing participation is important for making progress on the bugs they report. Based on the results, we suggest four ways in which bug tracking systems can be improved.},
  booktitle = {Proceedings of the 2010 ACM Conference on Computer Supported Cooperative Work},
  pages     = {301–310},
  numpages  = {10},
  keywords  = {information needs, question time, response rate, bug reports, questions, response time},
  location  = {Savannah, Georgia, USA},
  series    = {CSCW '10}
}

@inproceedings{10.1145/2568225.2568233,
  author    = {Begel, Andrew and Zimmermann, Thomas},
  title     = {Analyze This! 145 Questions for Data Scientists in Software Engineering},
  year      = {2014},
  isbn      = {9781450327565},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2568225.2568233},
  doi       = {10.1145/2568225.2568233},
  abstract  = {In this paper, we present the results from two surveys related to data science applied to software engineering. The first survey solicited questions that software engineers would like data scientists to investigate about software, about software processes and practices, and about software engineers. Our analyses resulted in a list of 145 questions grouped into 12 categories. The second survey asked a different pool of software engineers to rate these 145 questions and identify the most important ones to work on first. Respondents favored questions that focus on how customers typically use their applications. We also saw opposition to questions that assess the performance of individual employees or compare them with one another. Our categorization and catalog of 145 questions can help researchers, practitioners, and educators to more easily focus their efforts on topics that are important to the software industry.},
  booktitle = {Proceedings of the 36th International Conference on Software Engineering},
  pages     = {12–23},
  numpages  = {12},
  keywords  = {Data Science, Software Engineering, Analytics},
  location  = {Hyderabad, India},
  series    = {ICSE 2014}
}


@article{allen1970control,
  title     = {Control flow analysis},
  author    = {Allen, Frances E},
  journal   = {ACM Sigplan Notices},
  volume    = {5},
  number    = {7},
  pages     = {1--19},
  year      = {1970},
  publisher = {ACM New York, NY, USA}
}

@article{kavi1986formal,
  title     = {A formal definition of data flow graph models},
  author    = {Kavi, Krishna M. and Buckles, Bill P. and Bhat, U. Narayan},
  journal   = {IEEE Transactions on computers},
  volume    = {35},
  number    = {11},
  pages     = {940--948},
  year      = {1986},
  publisher = {IEEE Computer Society}
}

@article{yang2020estimating,
  title     = {Estimating the deep replicability of scientific findings using human and artificial intelligence},
  author    = {Yang, Yang and Youyou, Wu and Uzzi, Brian},
  journal   = {Proceedings of the National Academy of Sciences},
  volume    = {117},
  number    = {20},
  pages     = {10762--10768},
  year      = {2020},
  publisher = {National Acad Sciences}
}

@misc{preston-werner_2009,
  title   = {GitHub issue tracker!},
  url     = {https://github.blog/2009-04-15-github-issue-tracker/},
  journal = {The GitHub Blog},
  author  = {Preston-Werner, Tom},
  year    = {2009},
  month   = {Apr}
}


@inproceedings{10.1145/3180155.3180220,
  author    = {Tian, Yuchi and Pei, Kexin and Jana, Suman and Ray, Baishakhi},
  title     = {DeepTest: Automated Testing of Deep-Neural-Network-Driven Autonomous Cars},
  year      = {2018},
  isbn      = {9781450356381},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3180155.3180220},
  doi       = {10.1145/3180155.3180220},
  abstract  = {Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads.However, despite their spectacular progress, DNNs, just like traditional software, often demonstrate incorrect or unexpected corner-case behaviors that can lead to potentially fatal collisions. Several such real-world accidents involving autonomous cars have already happened including one which resulted in a fatality. Most existing testing techniques for DNN-driven vehicles are heavily dependent on the manual collection of test data under different driving conditions which become prohibitively expensive as the number of test conditions increases.In this paper, we design, implement, and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. First, our tool is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc. DeepTest systematically explore different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons. DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.},
  booktitle = {Proceedings of the 40th International Conference on Software Engineering},
  pages     = {303–314},
  numpages  = {12},
  keywords  = {deep learning, autonomous vehicle, testing, self-driving cars, neuron coverage, deep neural networks},
  location  = {Gothenburg, Sweden},
  series    = {ICSE '18}
}

@inproceedings{pathak2016context,
  title     = {Context encoders: Feature learning by inpainting},
  author    = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {2536--2544},
  year      = {2016}
}
@misc{car,
  doi       = {10.48550/ARXIV.1604.07316},
  url       = {https://arxiv.org/abs/1604.07316},
  author    = {Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {End to End Learning for Self-Driving Cars},
  publisher = {arXiv},
  year      = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{crevier1997knowledge,
  title     = {Knowledge-based image understanding systems: A survey},
  author    = {Crevier, Daniel and Lepage, Richard},
  journal   = {Computer vision and image understanding},
  volume    = {67},
  number    = {2},
  pages     = {161--185},
  year      = {1997},
  publisher = {Elsevier}
}

@inproceedings{Senti4SD,
  author    = {Calefato, Fabio and Lanubile, Filippo and Maiorano, Federico and Novielli, Nicole},
  title     = {Sentiment Polarity Detection for Software Development},
  year      = {2018},
  isbn      = {9781450356381},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3180155.3182519},
  doi       = {10.1145/3180155.3182519},
  booktitle = {Proceedings of the 40th International Conference on Software Engineering},
  pages     = {128},
  numpages  = {1},
  keywords  = {communication channels, sentiment analysis, word embedding, social software engineering, stack overflow},
  location  = {Gothenburg, Sweden},
  series    = {ICSE '18}
}

@article{sonnenburg2007need,
  author  = {Sonnenburg, Soren and Braun, Mikio L and Ong, Cheng Soon and Bengio, Samy and Bottou, Leon and Holmes, Geoffrey and LeCunn, Yann and Muller, Klaus-Robert and Pereira, Fernando and Rasmussen, Carl Edward and others},
  title   = {The Need for Open Source Software in Machine Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2007},
  volume  = {8},
  number  = {81},
  pages   = {2443-2466},
  url     = {http://jmlr.org/papers/v8/sonnenburg07a.html}
}


@article{hata2022github,
  title     = {GitHub Discussions: An exploratory study of early adoption},
  author    = {Hata, Hideaki and Novielli, Nicole and Baltes, Sebastian and Kula, Raula Gaikovina and Treude, Christoph},
  journal   = {Empirical Software Engineering},
  volume    = {27},
  number    = {1},
  pages     = {1--32},
  year      = {2022},
  publisher = {Springer}
}



@inproceedings{10.1145/3522664.3528621,
  author    = {Shome, Arumoy and Cruz, Lu\'{\i}s and van Deursen, Arie},
  title     = {Data Smells in Public Datasets},
  year      = {2022},
  isbn      = {9781450392754},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3522664.3528621},
  doi       = {10.1145/3522664.3528621},
  abstract  = {The adoption of Artificial Intelligence (AI) in high-stakes domains such as healthcare, wildlife preservation, autonomous driving and criminal justice system calls for a data-centric approach to AI. Data scientists spend the majority of their time studying and wrangling the data, yet tools to aid them with data analysis are lacking. This study identifies the recurrent data quality issues in public datasets. Analogous to code smells, we introduce a novel catalogue of data smells that can be used to indicate early signs of problems or technical debt in machine learning systems. To understand the prevalence of data quality issues in datasets, we analyse 25 public datasets and identify 14 data smells.},
  booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
  pages     = {205–216},
  numpages  = {12},
  location  = {Pittsburgh, Pennsylvania},
  series    = {CAIN '22}
}

@article{8812912,
  author  = {Wan, Zhiyuan and Xia, Xin and Lo, David and Murphy, Gail C.},
  journal = {IEEE Transactions on Software Engineering},
  title   = {How does Machine Learning Change Software Development Practices?},
  year    = {2021},
  volume  = {47},
  number  = {9},
  pages   = {1857-1871},
  doi     = {10.1109/TSE.2019.2937083}
}

@article{shneiderman2020bridging,
  title     = {Bridging the gap between ethics and practice: guidelines for reliable, safe, and trustworthy human-centered AI systems},
  author    = {Shneiderman, Ben},
  journal   = {ACM Transactions on Interactive Intelligent Systems (TiiS)},
  volume    = {10},
  number    = {4},
  pages     = {1--31},
  year      = {2020},
  publisher = {ACM New York, NY, USA}
}

@inproceedings{MDPFuzz,
  author    = {Pang, Qi and Yuan, Yuanyuan and Wang, Shuai},
  title     = {MDPFuzz: Testing Models Solving Markov Decision Processes},
  year      = {2022},
  isbn      = {9781450393799},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3533767.3534388},
  doi       = {10.1145/3533767.3534388},
  abstract  = {The Markov decision process (MDP) provides a mathematical frame- work for modeling sequential decision-making problems, many of which are crucial to security and safety, such as autonomous driving and robot control. The rapid development of artificial intelligence research has created efficient methods for solving MDPs, such as deep neural networks (DNNs), reinforcement learning (RL), and imitation learning (IL). However, these popular models solving MDPs are neither thoroughly tested nor rigorously reliable. We present MDPFuzz, the first blackbox fuzz testing framework for models solving MDPs. MDPFuzz forms testing oracles by checking whether the target model enters abnormal and dangerous states. During fuzzing, MDPFuzz decides which mutated state to retain by measuring if it can reduce cumulative rewards or form a new state sequence. We design efficient techniques to quantify the “freshness” of a state sequence using Gaussian mixture models (GMMs) and dynamic expectation-maximization (DynEM). We also prioritize states with high potential of revealing crashes by estimating the local sensitivity of target models over states. MDPFuzz is evaluated on five state-of-the-art models for solving MDPs, including supervised DNN, RL, IL, and multi-agent RL. Our evaluation includes scenarios of autonomous driving, aircraft collision avoidance, and two games that are often used to benchmark RL. During a 12-hour run, we find over 80 crash-triggering state sequences on each model. We show inspiring findings that crash-triggering states, though they look normal, induce distinct neuron activation patterns compared with normal states. We further develop an abnormal behavior detector to harden all the evaluated models and repair them with the findings of MDPFuzz to significantly enhance their robustness without sacrificing accuracy.},
  booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
  pages     = {378–390},
  numpages  = {13},
  keywords  = {Deep learning testing, Markov decision procedure},
  location  = {Virtual, South Korea},
  series    = {ISSTA 2022}
}

@inproceedings{Reluplex,
  author    = {Katz, Guy
               and Barrett, Clark
               and Dill, David L.
               and Julian, Kyle
               and Kochenderfer, Mykel J.},
  editor    = {Majumdar, Rupak
               and Kun{\v{c}}ak, Viktor},
  title     = {Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks},
  booktitle = {Computer Aided Verification},
  year      = {2017},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {97--117},
  abstract  = {Deep neural networks have emerged as a widely used and effective means for tackling complex, real-world problems. However, a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior. We present a novel, scalable, and efficient technique for verifying properties of deep neural networks (or providing counter-examples). The technique is based on the simplex method, extended to handle the non-convex Rectified Linear Unit (ReLU) activation function, which is a crucial ingredient in many modern neural networks. The verification procedure tackles neural networks as a whole, without making any simplifying assumptions. We evaluated our technique on a prototype deep neural network implementation of the next-generation airborne collision avoidance system for unmanned aircraft (ACAS Xu). Results show that our technique can successfully prove properties of networks that are an order of magnitude larger than the largest networks verified using existing methods.},
  isbn      = {978-3-319-63387-9},
  url       = {https://link.springer.com/chapter/10.1007/978-3-319-63387-9_5}
}

@inproceedings{8918993,
  author    = {Kallis, Rafael and Di Sorbo, Andrea and Canfora, Gerardo and Panichella, Sebastiano},
  booktitle = {2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  title     = {Ticket Tagger: Machine Learning Driven Issue Classification},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {406-409},
  doi       = {10.1109/ICSME.2019.00070}
}
@inproceedings{6698918,
  author    = {Bissyandé, Tegawendé F. and Lo, David and Jiang, Lingxiao and Réveillère, Laurent and Klein, Jacques and Traon, Yves Le},
  booktitle = {2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE)},
  title     = {Got issues? Who cares about it? A large scale investigation of issue trackers from GitHub},
  year      = {2013},
  volume    = {},
  number    = {},
  pages     = {188-197},
  doi       = {10.1109/ISSRE.2013.6698918}
}

@article{paleyes2021towards,
  title   = {Towards better data discovery and collection with flow-based programming},
  author  = {Paleyes, Andrei and Cabrera, Christian and Lawrence, Neil D},
  journal = {arXiv preprint arXiv:2108.04105},
  year    = {2021}
}


@inproceedings{10.1145/3522664.3528590,
  author    = {Foidl, Harald and Felderer, Michael and Ramler, Rudolf},
  title     = {Data Smells: Categories, Causes and Consequences, and Detection of Suspicious Data in AI-Based Systems},
  year      = {2022},
  isbn      = {9781450392754},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3522664.3528590},
  doi       = {10.1145/3522664.3528590},
  abstract  = {High data quality is fundamental for today's AI-based systems. However, although data quality has been an object of research for decades, there is a clear lack of research on potential data quality issues (e.g., ambiguous, extraneous values). These kinds of issues are latent in nature and thus often not obvious. Nevertheless, they can be associated with an increased risk of future problems in AI-based systems (e.g., technical debt, data-induced faults). As a counterpart to code smells in software engineering, we refer to such issues as Data Smells. This article conceptualizes data smells and elaborates on their causes, consequences, detection, and use in the context of AI-based systems. In addition, a catalogue of 36 data smells divided into three categories (i.e., Believability Smells, Understandability Smells, Consistency Smells) is presented. Moreover, the article outlines tool support for detecting data smells and presents the result of an initial smell detection on more than 240 real-world datasets.},
  booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
  pages     = {229–239},
  numpages  = {11},
  location  = {Pittsburgh, Pennsylvania},
  series    = {CAIN '22}
}


@inproceedings{7081875,
  author    = {Cabot, Jordi and Cánovas Izquierdo, Javier Luis and Cosentino, Valerio and Rolandi, Belén},
  booktitle = {2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)},
  title     = {Exploring the use of labels to categorize issues in Open-Source Software projects},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {550-554},
  doi       = {10.1109/SANER.2015.7081875}
}

@article{KALLIS2021102598,
  title    = {Predicting issue types on GitHub},
  journal  = {Science of Computer Programming},
  volume   = {205},
  pages    = {102598},
  year     = {2021},
  issn     = {0167-6423},
  doi      = {https://doi.org/10.1016/j.scico.2020.102598},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167642320302069},
  author   = {Rafael Kallis and Andrea {Di Sorbo} and Gerardo Canfora and Sebastiano Panichella},
  keywords = {Software maintenance and evolution, Issue reports management, Labeling unstructured data},
  abstract = {Software maintenance and evolution involves critical activities for the success of software projects. To support such activities and keep code up-to-date and error-free, software communities make use of issue trackers, i.e., tools for signaling, handling, and addressing the issues occurring in software systems. However, in popular projects, tens or hundreds of issue reports are daily submitted. In this context, identifying the type of each submitted report (e.g., bug report, feature request, etc.) would facilitate the management and the prioritization of the issues to address. To support issue handling activities, in this paper, we propose Ticket Tagger, a GitHub app analyzing the issue title and description through machine learning techniques to automatically recognize the types of reports submitted on GitHub and assign labels to each issue accordingly. We empirically evaluated the tool's prediction performance on about 30,000 GitHub issues. Our results show that the Ticket Tagger can identify the correct labels to assign to GitHub issues with reasonably high effectiveness. Considering these results and the fact that the tool is designed to be easily integrated in the GitHub issue management process, Ticket Tagger consists in a useful solution for developers.}
}

@article{10.1145/3533378,
  author     = {Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D.},
  title      = {Challenges in Deploying Machine Learning: A Survey of Case Studies},
  year       = {2022},
  issue_date = {July 2023},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {55},
  number     = {6},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3533378},
  doi        = {10.1145/3533378},
  abstract   = {In recent years, machine learning has transitioned from a field of academic research interest to a field capable of solving real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries, and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. By mapping found challenges to the steps of the machine learning deployment workflow, we show that practitioners face issues at each stage of the deployment process. The goal of this article is to lay out a research agenda to explore approaches addressing these challenges.},
  journal    = {ACM Comput. Surv.},
  month      = {dec},
  articleno  = {114},
  numpages   = {29},
  keywords   = {Machine learning applications, sofware deployment}
}

@inproceedings{10.1145/3382494.3410681,
  author    = {Serban, Alex and van der Blom, Koen and Hoos, Holger and Visser, Joost},
  title     = {Adoption and Effects of Software Engineering Best Practices in Machine Learning},
  year      = {2020},
  isbn      = {9781450375801},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3382494.3410681},
  doi       = {10.1145/3382494.3410681},
  abstract  = {Background. The increasing reliance on applications with machine learning (ML) components calls for mature engineering techniques that ensure these are built in a robust and future-proof manner.Aim. We aim to empirically determine the state of the art in how teams develop, deploy and maintain software with ML components.Method. We mined both academic and grey literature and identified 29 engineering best practices for ML applications. We conducted a survey among 313 practitioners to determine the degree of adoption for these practices and to validate their perceived effects. Using the survey responses, we quantified practice adoption, differentiated along demographic characteristics, such as geography or team size. We also tested correlations and investigated linear and non-linear relationships between practices and their perceived effect using various statistical models.Results. Our findings indicate, for example, that larger teams tend to adopt more practices, and that traditional software engineering practices tend to have lower adoption than ML specific practices. Also, the statistical models can accurately predict perceived effects such as agility, software quality and traceability, from the degree of adoption for specific sets of practices. Combining practice adoption rates with practice importance, as revealed by statistical models, we identify practices that are important but have low adoption, as well as practices that are widely adopted but are less important for the effects we studied.Conclusion. Overall, our survey and the analysis of responses received provide a quantitative basis for assessment and step-wise improvement of practice adoption by ML teams.},
  booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
  articleno = {3},
  numpages  = {12},
  keywords  = {survey, best practices, machine learning engineering},
  location  = {Bari, Italy},
  series    = {ESEM '20}
}

@inproceedings{10.1145/3522664.3528596,
  author    = {Song, Qunying and Borg, Markus and Engstr\"{o}m, Emelie and Ard\"{o}, H\r{a}kan and Rico, Sergio},
  title     = {Exploring ML Testing in Practice: Lessons Learned from an Interactive Rapid Review with Axis Communications},
  year      = {2022},
  isbn      = {9781450392754},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3522664.3528596},
  doi       = {10.1145/3522664.3528596},
  abstract  = {There is a growing interest in industry and academia in machine learning (ML) testing. We believe that industry and academia need to learn together to produce rigorous and relevant knowledge. In this study, we initiate a collaboration between stakeholders from one case company, one research institute, and one university. To establish a common view of the problem domain, we applied an interactive rapid review of the state of the art. Four researchers from Lund University and RISE Research Institutes and four practitioners from Axis Communications reviewed a set of 180 primary studies on ML testing. We developed a taxonomy for the communication around ML testing challenges and results and identified a list of 12 review questions relevant for Axis Communications. The three most important questions (data testing, metrics for assessment, and test generation) were mapped to the literature, and an in-depth analysis of the 35 primary studies matching the most important question (data testing) was made. A final set of the five best matches were analysed and we reflect on the criteria for applicability and relevance for the industry. The taxonomies are helpful for communication but not final. Furthermore, there was no perfect match to the case company's investigated review question (data testing). However, we extracted relevant approaches from the five studies on a conceptual level to support later context-specific improvements. We found the interactive rapid review approach useful for triggering and aligning communication between the different stakeholders.},
  booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
  pages     = {10–21},
  numpages  = {12},
  keywords  = {AI engineering, interactive rapid review, machine learning, taxonomy, testing},
  location  = {Pittsburgh, Pennsylvania},
  series    = {CAIN '22}
}



@inproceedings{levinson2011towards,
  author    = {Levinson, Jesse and Askeland, Jake and Becker, Jan and Dolson, Jennifer and Held, David and Kammel, Soeren and Kolter, J. Zico and Langer, Dirk and Pink, Oliver and Pratt, Vaughan and Sokolsky, Michael and Stanek, Ganymed and Stavens, David and Teichman, Alex and Werling, Moritz and Thrun, Sebastian},
  booktitle = {2011 IEEE Intelligent Vehicles Symposium (IV)},
  title     = {Towards fully autonomous driving: Systems and algorithms},
  year      = {2011},
  volume    = {},
  number    = {},
  pages     = {163-168},
  doi       = {10.1109/IVS.2011.5940562},
  url       = {https://doi.org/10.1109/IVS.2011.5940562},
  abstract  = {In order to achieve autonomous operation of a vehicle in urban situations with unpredictable traffic, several realtime systems must interoperate, including environment perception, localization, planning, and control. In addition, a robust vehicle platform with appropriate sensors, computational hardware, networking, and software infrastructure is essential. We previously published an overview of Junior, Stanford's entry in the 2007 DARPA Urban Challenge. This race was a closed-course competition which, while historic and inciting much progress in the field, was not fully representative of the situations that exist in the real world. In this paper, we present a summary of our recent research towards the goal of enabling safe and robust autonomous operation in more realistic situations. First, a trio of unsupervised algorithms automatically calibrates our 64-beam rotating LIDAR with accuracy superior to tedious hand measurements. We then generate high-resolution maps of the environment which are subsequently used for online localization with centimeter accuracy. Improved perception and recognition algorithms now enable Junior to track and classify obstacles as cyclists, pedestrians, and vehicles; traffic lights are detected as well. A new planning system uses this incoming data to generate thousands of candidate trajectories per second, choosing the optimal path dynamically. The improved controller continuously selects throttle, brake, and steering actuations that maximize comfort and minimize trajectory error. All of these algorithms work in sun or rain and during the day or night. With these systems operating together, Junior has successfully logged hundreds of miles of autonomous operation in a variety of real-life conditions.}
}



@inproceedings{li2019structural,
  author    = {Li, Zenan and Ma, Xiaoxing and Xu, Chang and Cao, Chun},
  booktitle = {2019 IEEE/ACM 41st International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER)},
  title     = {Structural Coverage Criteria for Neural Networks Could Be Misleading},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {89-92},
  doi       = {10.1109/ICSE-NIER.2019.00031},
  url       = {https://doi.org/10.1109/ICSE-NIER.2019.00031},
  abstract  = {There is a dramatically increasing interest in the quality assurance for DNN-based systems in the software engineering community. An emerging hot topic in this direction is structural coverage criteria for testing neural networks, which are inspired by coverage metrics used in conventional software testing. In this short paper, we argue that these criteria could be misleading because of the fundamental differences between neural networks and human written programs. Our preliminary exploration shows that (1) adversarial examples are pervasively distributed in the finely divided space defined by such coverage criteria, while available natural samples are very sparse, and as a consequence, (2) previously reported fault-detection "capabilities" conjectured from high coverage testing are more likely due to the adversary-oriented search but not the real "high" coverage.}
}



@inproceedings{Sun_ICSE,
  author    = {Sun, Zeyu and Zhang, Jie M. and Harman, Mark and Papadakis, Mike and Zhang, Lu},
  title     = {Automatic Testing and Improvement of Machine Translation},
  year      = {2020},
  isbn      = {9781450371216},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3377811.3380420},
  doi       = {10.1145/3377811.3380420},
  abstract  = {This paper presents TransRepair, a fully automatic approach for testing and repairing the consistency of machine translation systems. TransRepair combines mutation with metamorphic testing to detect inconsistency bugs (without access to human oracles). It then adopts probability-reference or cross-reference to post-process the translations, in a grey-box or black-box manner, to repair the inconsistencies. Our evaluation on two state-of-the-art translators, Google Translate and Transformer, indicates that TransRepair has a high precision (99%) on generating input pairs with consistent translations. With these tests, using automatic consistency metrics and manual assessment, we find that Google Translate and Transformer have approximately 36% and 40% inconsistency bugs. Black-box repair fixes 28% and 19% bugs on average for Google Translate and Transformer. Grey-box repair fixes 30% bugs on average for Transformer. Manual inspection indicates that the translations repaired by our approach improve consistency in 87% of cases (degrading it in 2%), and that our repairs have better translation acceptability in 27% of the cases (worse in 8%).},
  booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
  pages     = {974–985},
  numpages  = {12},
  keywords  = {machine translation, testing and repair, translation consistency},
  location  = {Seoul, South Korea},
  series    = {ICSE '20}
}