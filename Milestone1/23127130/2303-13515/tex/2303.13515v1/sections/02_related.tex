
\section{Related Work}

\noindent \textbf{Image and view extrapolation.}
Pioneering work by Kaneva~\etal~\cite{Kaneva_2010} proposed the task of infinite image extrapolation by using a large image database to perform classical 2D image retrieval, stitching, and rendering. 
More recently, various learning-based 2D image inpainting~\cite{hays2007scene,yu2018generative,yu2019free,liu2021pd,zhao2021large, suvorov2022resolution, li2022mat, saharia2022palette} and outpainting~\cite{wang2019wide,yang2019very,teterwak2019boundless,bowen2021oconet,lin2021infinitygan, cheng2022inout} methods have been developed.
These methods fill in missing image regions or expand the field of view by synthesizing realistic image content that is coherent with the partial input image. Beyond 2D, prior work has explored single-view 3D \emph{view extrapolation},
often by applying 2D image synthesis techniques within a 3D representation~\cite{wiles2020synsin, Shih3DP20, rockwell2021pixelsynth, hu2021worldsheet, rombach2021geometry, Li_2021_ICCV, jampani2021slide}.
However, these methods 
can only extrapolate content within a very limited range of viewpoints. 


\smallskip
\noindent \textbf{Video generation.}
Video generation aims to synthesize realistic videos 
from different types of input. 
Unconditional video generation produces long videos often from noise input~\cite{tulyakov2018mocogan,munoz2021temporal,fox2021stylevideogan,skorokhodov2022stylegan, liu2021content, brooks2022generating, ge2022long}, 
while conditional video generation generates sequences by conditioning on one or a few images~\cite{vondrick2016generating,vondrick2017generating,wang2017predrnn,villegas2017decomposing,hsieh2018learning,lee2021revisiting, vondrick2016generating,finn2016unsupervised,denton2018stochastic,ye2019cvp,yu2022generating, koh2021pathdreamer}, or a text prompt~\cite{ho2022imagen, singer2022make}. 
However, applying these ideas in 3D requires supervision  from multi-view training data, and cannot achieve persistent 3D scene content at runtime, since there is no explicit 3D representation. 
Some recent work preserves global scene consistency via extra 3D geometry inputs such as point clouds~\cite{mallya2020world} or voxel grids~\cite{hao2021gancraft}. 
In contrast, our method synthesizes both the geometry and appearance of an entire world from scratch using a global feature representation to achieve consistent generated content.





\input{figures/01_schematic}


\smallskip
\noindent \textbf{Generative view synthesis.}
Novel view synthesis aims to produce new views of a scene from single~\cite{chen2019mono,tulsiani2018layer,niklaus20193d,single_view_mpi,shi2014light,wiles2020synsin,jang2021codenerf,Shih3DP20,Kopf-OneShot-2020,rombach2021geometry, yu2021pixelnerf} or 
multiple image observations~\cite{levoy1996light,zhou2018stereo,mildenhall2019local,flynn2019deepview,extremeview,lombardi2019neural,Riegler2020FVS,mildenhall2020nerf,wang2021ibrnet, muller2022instant, barron2022mip,yu_and_fridovichkeil2021plenoxels,shen2022sgam} by constructing a local or global 3D scene representation. However, most prior methods can only interpolate or extrapolate a limited 
distance from the input views, and do not possess a generative ability.

On the other hand, a number of generative view synthesis methods have been recently proposed utilizing neural volumetric representations~\cite{HoloGAN2019,schwarz2020graf,niemeyer2021giraffe,devries2021unconstrained,niemeyer2021campari,gu2021stylenerf,chan2022eg3D, rebain2022lolnerf,shorokhodov2022epigraf}. 
These methods can learn to generate 3D representations from 2D supervision, and have demonstrated impressive results on generating novel objects~\cite{poole2022dreamfusion}, faces~\cite{gu2021stylenerf, orel2022styleSDF, chan2022eg3D, Deng_2022_CVPR}, or indoor environments~\cite{ren2022look, devries2021unconstrained}. 
However, none of these methods 
can generate unbounded outdoor scenes due to lack of multi-view data for supervision, and due to the larger and more complex scene geometry and appearance that is difficult to model with prior representations. 
In contrast, our approach can generate globally consistent, large-scale nature scenes by training solely from unstructured 2D photo collections.


Our work is particularly inspired by recent perpetual view generation methods, including InfiniteNature~\cite{liu2021infinite} and InfiniteNature-Zero~\cite{li2022infinitenature}, which can generate unbounded fly-through videos of natural scenes, and are trained on nature videos or photo collections. 
However, these methods generate video sequences in an auto-regressive manner, and therefore
cannot achieve globally consistent 3D scene content. 
Our approach instead adopts a global scene representation that can be trained to generate consistent-by-construction and realistic novel views spanning large-scale scenes.
Concurrent works for scene synthesis InfiniCity~\cite{lin2023infinicity} and SceneDreamer\cite{chen2023scenedreamer} leverage birds-eye-view representations, while SceneScape~\cite{fridman2023scenescape} builds a mesh representation from text.







