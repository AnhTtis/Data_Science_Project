
\input{figures/03_models}
\input{figures/04_upsampler}

\section{Experiments}\label{sec:experiment}
Given its persistent scene representation and the extensibility of the its layout grid, our model enables arbitrary motion through a synthesized landscape, including long camera trajectories. 
We show sample outputs from our model under a variety of camera movements (\S~\ref{sec:expt_qualitative}); 
present qualitative and quantitative comparisons with alternate scene representations, including auto-regressive prediction models and unconditional generators defined for bounded or object-centric scenes (\S~\ref{sec:expt_scene_representation}); 
and investigate variations of our model to evaluate design decisions (\S~\ref{sec:expt_model_variations}).

\subsection{Persistent, unbounded scene synthesis} \label{sec:expt_qualitative}
Figure~\ref{fig:qualitative} shows example landscapes generated by our model with various camera motions. 
As the camera moves (by rotating and/or translating) the generated imagery changes in a way that is consistent with the underlying geometry, \eg hills move across the image or become closer. 
Extending the generated aerial feature grid allows us to place the camera \textit{outside} the distribution of training camera poses, while maintaining both geometric and stylistic consistency. 
As illustrated in Figure~\ref{fig:teaser} and our project page, the persistent and extendable layout features enables synthetic `flights' over large distances that can also return to a consistent starting point.

\subsection{Comparing scene representations}\label{sec:expt_scene_representation}
\input{tables/compare_infnatzero}
\input{tables/compare_others}

We compare our model with three state-of-the-art methods.
InfiniteNature-Zero is an auto-regressive method that, given an initial frame, generates successive frames sequentially by warping each image to the next based on depth~\cite{li2022infinitenature}. It allows for unbounded camera trajectories, but has no persistent world model. GSN~\cite{devries2021unconstrained} and EG3D~\cite{chan2022eg3D} are unconditional generative models: GSN uses a layout feature grid which is also the basis of our model, but focuses on bounded indoor scenes with ground-truth camera pose trajectories, while EG3D uses a tri-plane representation and primarily focuses on objects and portraits. These methods have persistent world models (feature grid and tri-plane representation) but do not allow for unbounded trajectories.


\myparagraph{Quantitative comparisons.} We evaluate image quality using FID~\cite{heusel2017gans}, and multi-view consistency using photometric error. To compare with InfiniteNature-Zero (Table~\ref{tab:quant_1}), we initialize with an image and depth map from our model, move the camera forwards using a forward motion trajectory from InfiniteNature-Zero, and evaluate image quality at a distance of 100 forward steps. 
Our model attains better FID, showing that it does not suffer from image degradation due to successive applications of an auto-regressive model. 
To compute one-step consistency error, we generate a new frame at a position equivalent to one forward step of InfiniteNature-Zero, warp it back to the original camera position using depth, and compute L1 error with the original frame in the overlapping region. Because InfiniteNature-Zero uses explicit warping as part of its model, it can achieve better one-step consistency, whereas our 2D upsampling operation is more susceptible to geometric inconsistency. 
We measure cyclic consistency error as the L1 error between the initial frame to the result after a step forward and back.
Because InfiniteNature-Zero lacks a persistent global representation, it has non-zero cyclic consistency error, whereas our model is fully consistent with zero cyclic consistency error.

To compare with the unconditional generative models GSN and EG3D, we compute FID on sets of output images corresponding to different distributions of camera positions: camera poses used in training which are intended to overlap with the layout, camera poses 100 steps forward from these mimicking InfiniteNature-Zero trajectories, and a uniform distribution of randomly oriented cameras 
over the layout grid. As seen in Table~\ref{tab:quant_2}, GSN is the least successful method when applied to this domain. EG3D generates high-quality images at training camera poses, but tends to represent the scene as floating nearby clouds with planar mountains at the edges of the volume (incorrect geometry).
Our method generalizes better to new camera positions. GSN has the highest one-step consistency error, while the consistency error of our model is close to that of EG3D (which relies less on 2D upsampling). In the supplemental, we experiment with an alternative architecture that builds on extendable triplane units with lower consistency error and faster rendering speed.




\myparagraph{Qualitative comparisons.} In Fig.~\ref{fig:models} we show example outputs of each model over forward-moving and rotating trajectories. Due to its auto-regressive nature, the quality of InfiniteNature-Zero's output degrades somewhat as the camera trajectory becomes longer. A more serious limitation is that, trained only on forward movement, it is unable to synthesize plausible views under camera rotation. GSN and EG3D also struggle with long camera trajectories, producing unrealistic outputs as the cameras approach the spatial limits of the training camera distribution. In the case of GSN applied to our setting, the results contain flickering and grid-like artifacts, which our projected noise (\S~\ref{sec:refinement}) mitigates.


\subsection{Model Variations}\label{sec:expt_model_variations}

To investigate individual components of our model, we separately evaluate variations of the layout generator and refinement network. 

\myparagraph{Layout generator.} The resolution of the scene layout grid and the number of samples per ray affect the quality of the volume-rendered output $\ImageLR$. As shown in Table~\ref{tab:layout}, higher resolution and more samples lead to the best image quality (FID computed on 32$\times$32 pixel images for speed, compared to segmented real images with gray sky pixels). To maximize the capacity of layout generation and rendering within computational limits we opt for a 256$\times$256 feature grid with 128 samples per ray.


\input{tables/layout}
\input{tables/upsampler}
\myparagraph{Refinement network.} Next, we investigate the refinement stage, which upsamples and refines the layout generator output. In our full model, the refinement network operates not only on RGB images but also on inverse-depth and sky mask (Eqn.~\ref{eqn:upsampler}), and uses projected noise for spatial consistency of texture detail (Eqn.~\ref{eqn:noise}). As shown in Table~\ref{tab:upsampler}, both help to improve our model's FID and consistency error. 

As shown in Fig.~\ref{fig:upsampler} (second row), upsampling only the RGB image $\ImageLR$ can lead to output that is inconsistent with the generated sky mask, leading to temporally unstable gaps in the final composited image. This figure also shows the effect of our geometric regularization (Eqn.~\ref{eqn:geometry}) in reducing unwanted transparency, especially in distant terrain.







