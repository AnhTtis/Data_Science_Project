

\section{Introduction}



Generative image and video models have achieved remarkable levels of realism, but are still far from presenting a convincing, explorable world.
Moving a virtual camera through these models---either in their latent space~\cite{harkonen2020ganspace,gansteerability,shen2020closed,brooks2022generating} or via explicit conditioning~\cite{drivegan}---is not like walking about in the real world.  
Movement 
is either very limited (for example, in object-centric models~\cite{chan2022eg3D}), or else camera motion is unlimited but quickly reveals the lack of a persistent world model.
Auto-regressive 3D synthesis methods exemplify this lack of persistence~\cite{liu2021infinite,li2022infinitenature}; parts of the scene may change unexpectedly as the camera moves,
and you may find that the scene is entirely different when returning to previous positions. The lack of spatial and temporal consistency can give the output of these models a strange, dream-like quality. 
In contrast, machines that can generate unbounded, persistent 3D worlds could be used to develop agents that plan within a world model~\cite{ha2018recurrent}, or to build virtual reality experiences that feel closer to the natural world, rather than appearing as ephemeral hallucinations~\cite{li2022infinitenature}.


We therefore aim to develop a unconditional generative model capable of generating unbounded 3D scenes with a persistent underlying world representation. We want synthesized content to move in a way that is consistent with camera motion, yet we should also be able to move arbitrarily far and still generate the same scene upon returning to a previous camera location, regardless of the camera trajectory. 

To achieve this goal, we model a 3D world as a \textit{terrain} plus a \textit{skydome}. The terrain is represented by a \textit{scene layout grid}---an extendable 2D array of feature vectors that acts as a map of the landscape. We `lift' these features into 3D and decode them with an MLP into a radiance field for volume rendering. The rendered terrain images are super-resolved and composited with renderings from the skydome model to synthesize final images. We train using a layout grid of limited size, but can extend the scene layout grid 
by any desired amount during inference, enabling unbounded camera trajectories.
Since our underlying representation is persistent over space and time, we can fly around 3D landscapes in a consistent manner. Our method does not require multiview data; each part of our system is trained from an unposed collection of single-view images using GAN objectives.








Our work builds upon two prior threads of research that 
tackle generating immersive worlds: 1) generative models of 3D data, and 2) generative models of infinite videos. Along the first direction are generators of meshes, volumes, radiance fields, etc (e.g., \cite{HoloGAN2019,chan2022eg3D,poole2022dreamfusion}). 
These models represent a consistent 3D world by construction, and excel at rendering isolated objects and bounded indoor scenes. Our work, in contrast, tackles the challenging problem of generating large-scale \emph{unbounded} nature scenes. %
Along the second direction are methods like InfiniteNature~\cite{liu2021infinite,li2022infinitenature}, which can indeed simulate visual worlds of infinite extent. 
These methods enable unbounded scene synthesis by predicting new viewpoints auto-regressively from a starting view. However, they do not ensure a persistent world representation; content may change when revisited. 

Our method aims to combine the best of both worlds,
generating boundless scenes (unlike prior 3D generators) while still representing a persistent 3D world (unlike prior video generative models). In summary:
\begin{packed_item}
    \item We present an unconditional 3D generative model for unbounded nature scenes with a persistent world representation, consisting of a terrain map and skydome.
    \item We augment our generative pipeline to support camera extrapolation beyond the training camera distribution by extending the terrain features. 
    \item Our model is learned entirely from single-view landscape photos with unknown camera poses. 
\end{packed_item}















