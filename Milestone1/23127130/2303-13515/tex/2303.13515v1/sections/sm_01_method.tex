\section{Additional Methodological Details}\label{sec:sm_method}

\subsection{Preprocessing}

\input{figures/sm_01_dataset}

\myparagraph{Dataset Filtering.} 
To remove images in the LHQ~\cite{skorokhodov2021aligning} dataset that contain occluding objects close to the camera, we apply filtering criteria to construct the training dataset. Using the segmentation output of DPT~\cite{ranftl2021vision}, we detect the sky region and boundaries of the resulting binary sky mask. As the segmentation results can include small regions with inconsistent labels (\eg small holes in the sky), we remove all bounded regions with area under 250 pixels to create a more unified sky mask. Next, using this segmentation mask we filter out images for which any of the following hold: (1) there are more than three bounded sky regions, (2) more than 90\% of the scene is not sky pixels, (3) more than 40\% of the upper one-fifth of the image is not sky pixels, and (4) less than 80\% of the lower quarter of the image is not sky pixels. The first three criteria are meant to filter out images that contain occluding structures (such as trees or windows) or images in which there is no sky region present. The fourth criteria is meant to filter out images taken from unusual camera angles (such as from underneath a bridge). Using the monocular depth prediction from DPT, we also remove images containing too many vertical edges: images are removed if the 99th percentile of the pixel-wise finite difference is greater than 0.05, which tends to be indicative of trees or man-made buildings. Fig.~\ref{fig:sm_dataset} shows examples of images that were retained for training, and those that were filtered out.


\myparagraph{Disparity Normalization.} Using the monocular depth prediction from DPT, we normalize the disparity values between 0 and 1 using the 1st and 99th percentile values per image. Next, we clip the minimum disparity for non-sky regions and rescale the disparity values to correspond to the near and far bounds used in volumetric rendering (see \S~\ref{sec:sm_layout}). We use 0.05 for our clip value and 1/16 for the scale factor; this means that after normalization, the disparity values for non-sky pixels range from 1/16 to 1. The disparity for the sky pixels is clamped at zero. 

\input{figures/sm_02_cameras}

\myparagraph{Camera Poses.} We sample training camera poses with a random $(x, z)$ position within the layout grid, and a rotation such that the near half of the view frustrum lies entirely within the training grid. To simulate the forward motion of InfiniteNature-Zero~\cite{li2022infinitenature}, we move the camera forward a distance equivalent to 100 steps of InfiniteNature-Zero, corresponding to roughly half of the scene layout grid. To evaluate view extrapolation, we randomize the position and rotation of the cameras at inference time. These settings are illustrated in Fig.~\ref{fig:sm_cameras}. 

\subsection{Training and Implementation}

\subsubsection{Training objective}
Each stage of our model is trained following the StyleGAN2 objective~\cite{karras2020analyzing}, with a non-saturating GAN loss $\GANCriterion$ and $R_1$ regularization~\cite{mescheder2018training}:
\begin{equation}\label{eqn:gan}
\begin{split}
    &\GANCriterion(\Discriminator, \Generator(\LatentCode), \Image) = \Discriminator(\Image)-\Discriminator(\Generator(\LatentCode)), \\
    &R_1(\Discriminator, \Image) = || \nabla \Discriminator(x) || ^2, \\
    &\Generator = \arg \min_\Generator \max_\Discriminator
    \hspace{1mm} \mathbb{E}_{\LatentCode, \Image \sim \Dataset}
    \hspace{1mm} \GANCriterion(\Discriminator, \Generator(\LatentCode), \Image) + \\
    & \hspace{8mm} \frac{\lambda_{R_1}}{2} R_1(\Discriminator, \Image),
\end{split}
\end{equation}
where $\Generator,\Discriminator$ refer to the corresponding generator and discriminator networks at each training stage, and $x$ refers to real images sampled from dataset $\Dataset$. Additional auxiliary losses for each part of the model are described in the following sections.


\subsubsection{Layout Generator}\label{sec:sm_layout}

Our layout generator is based on the architecture from GSN~\cite{devries2021unconstrained}, which is comprised of two components: $\GeneratorLand$, which synthesizes the scene layout grid, and $\MLP$ which decodes the 2D layout feature into a 3D feature. 

The layout generator $\GeneratorLand$ follows StyleGAN2~\cite{karras2020analyzing}, which generates a $256 \times 256$ grid of features $\FeatureLand \in \mathbb{R}^{32}$. $\GeneratorLand$ contains three mapping layers and the maximum channel dimension is capped at 256; all other parameters are unchanged from StyleGAN2. 

The network $\MLP$ is modeled after the style-modulated MLP from CIPS~\cite{anokhin2021image}, containing eight layers with a hidden channel dimension of 256 and producing features $\FeatureColor \in \mathbb{R}^{128}$. The constant input to $\MLP$ is replaced with the $\Y$-coordinate (height above the ground plane), and the modulation input is the interpolated feature from $\FeatureLand$.

We adapt the rendering procedure of GSN to handle unbounded outdoor scenes. For volumetric rendering, we set the near bound to 1 and the far bound to 16, which corresponds to the scale factor used in disparity normalization during data preprocessing. Each scene layout feature has a unit width of $0.15$, such that the full width of the feature grid is $256 \times 0.15 = 38.4$, which is slightly over twice the far bound distance. We omit positional encoding from $\MLP$, as we found that including positional encoding yielded grid-aligned artifacts in generated images; we also omit the view direction input. Camera rays are sampled using $\mathrm{FOV}=60^{\circ}$ with linearly spaced sampling between the near bound and the far bound. We use inverse-depth (disparity) supervision rather than depth supervision so that we can represent content at infinite distances. This also encourages the terrain generator to create empty space in the sky content, which will be filled with the skydome generator.

We use the volumetric rendering equations from NeRF~\cite{mildenhall2020nerf}, in which the weights $\Weight_i$ of the $i$-th point along a ray depends on densities $\sigma$ which is predicted by multi-layer perceptron $\MLP$ and the distance between samples $\delta$:
\begin{equation}
\alpha_i = 1 - \Exp\left(-\sigma_i\delta_i\right),
\;\;
\Weight_{i} = \alpha_i \; \Exp\big(-\sum_{j=1}^{i-1}\sigma_j\delta_j\big).
\end{equation}

Our training procedure for the layout decoder follows that of GSN~\cite{devries2021unconstrained}, which provides the real RGB image $\Image_\textrm{RGB}$ and disparity $\Depth$ (obtained from DPT) to the discriminator $\Image=\{\Image_\textrm{RGB}, \Depth\}$, and also adds a reconstruction loss on real images using a decoder network $\Reconstruction$ on discriminator features $\DiscriminatorFeature$:
\begin{equation}
    \mathcal{L}_\textrm{rec} = (\Image - \Reconstruction (\DiscriminatorFeature (\Image)))^2.
\end{equation}
The full GAN objective follows Eqn.~\ref{eqn:gan} with weights $\lambda_{R_1} = 0.01$ and $\lambda_\textrm{rec} = 1000$, and we follow the optimizer settings from StyleGAN2 and train for 12M image samples. 

Because the layout decoder tends to generate semi-transparent geometry, which also causes unrealistic sky masks, we regularize the geometry following 
Eqn. \ref{eqn:geometry},
and add the sky mask into the discriminator. We finetune with this additional loss for 400k samples with $\lambda_{\mathrm{transparent}}$ which linearly increases from zero to $\lambda_{\mathrm{transparent}}=80$ over the finetuning procedure.

\subsubsection{Layout Extension}

We use the procedure of SOAT~\cite{chong2021stylegan} in two dimensions to smoothly transition between adjacent feature grids sampled from independent latent codes. SOAT proceeds by operating on 2x2 sub-grids and stitching each layer of intermediate features in the generator (Fig.~\ref{fig:sm_soat}). To start, we simply concatenate the StyleGAN constant tensors, to obtain a feature grid $\Feature_0$ of size $2H_0\times 2W_0$, where $H_0$ and $W_0$ are the height and width of the constant tensor. For each subsequent layer $\Feature_{l+1}$, we modulate the weights $G_l$ with each of four corner latent codes (after applying the mapping network to obtain the style-code) and apply it in a fully convolutional manner to $\Feature_l$, obtaining $\Feature_{k, l+1}$ of size $2H_l\times 2W_l$. Then, we multiply each of $\Feature_{k, l+1}$ with bilinear interpolation weights $\Bilinear$ and take the sum to obtain $\Feature_{l+1}$. This procedure is repeated for each layer of the generator, obtaining a an output feature grid of size $2H\times 2W$. To reduce the effect of padding, these output feature grids are tiled in an overlapping manner, with a 25\% overlap  on each side and with weights that linearly decay to zero away from the center of the tile.

\input{figures/sm_02b_soat}

\input{figures/sm_03_expt_skydome}



\subsubsection{Refinement Network}

The refinement network $\GeneratorUpsample$ uses a truncated StyleGAN2 backbone, which replaces the feature input of the $32\times32$ block with the $32\times32$ rendered feature $\FeatureImage$ and initial image $\ImageLR$, depth $\DepthLR$, and sky mask $\MaskLR$. The skip connection of the upsampler takes in $\ImageLR$, $\DepthLR$, $\MaskLR$ and predicts $\ImageHR$, $\DepthHR$, and $\MaskHR$. Following the noise injection operation in StyleGAN2, we replace the image-space 2D noise tensor with our 3D-consistent projected noise 
(Eqn.~\ref{eqn:noise}).
This network uses two mapping layers, taking as input the style latent vector from $\GeneratorLand$. 

We add an additional objective to encourage consistency between the refined color pixels and the sky mask:
\begin{equation}\label{eqn:reg_upsampler}
\begin{split}
    \mathcal{L}_{\mathrm{consistency}} &= |\DepthHR - {\DepthLR}_\uparrow| + |\MaskHR - {\MaskLR}_\uparrow|, \\
    \mathcal{L}_{\mathrm{sky}} &= \exp(-20*\sum_{c}|\ImageHR[c]|) * \MaskHR. \\
\end{split}
\end{equation}
The loss $\mathcal{L}_{\mathrm{consistency}}$ encourages the high resolution depth and mask outputs to match their upsampled low resolution counterparts (this results in a smoother outcome compared to downsampling the high resolution outputs). The loss $\mathcal{L}_{\mathrm{sky}}$ encourages pixel colors to be nonzero (reserved for the gray sky color) when $\MaskHR=1$, by summing over the three channels $c$ of the predicted image $\ImageHR$; this is meant to encourage the RGB colors produced refinement network to be consistent with the mask and depth outputs.
The refinement network is trained with the GAN objective (Eqn.~\ref{eqn:gan}) with weights $\lambda_{R1}=4$, $\lambda_{\mathrm{consistency}}=5$, and $\lambda_\mathrm{sky}=100$, and the discriminator loss is applied only on the RGB images.  %

Due to the computational costs of volume rendering, we train the refinement network on $32\times32$ inputs to produce $256\times256$ outputs. For 30 fps video visualizations, we supersample the camera rays at 8x spatial density and apply depth-based filtering to the noise input to improve video smoothness; however all metrics in the paper are computed without supersampling for additional smoothness. 

We note that while StyleGAN3~\cite{karras2021aliasfree} is intended to resolve the texture sticking effect caused by the noise input in StyleGAN2, replacing $\GeneratorUpsample$ with a StyleGAN3 backbone resulted in worse image quality in our setting with FID 67.90, compared to FID 21.42 for our final model. 

\subsubsection{Skydome Generator}

The skydome generator takes as input the CLIP~\cite{radford2021learning} embedding of a single terrain image, and predicts a sky output that is consistent with the terrain. The generator architecture follows StyleGAN3~\cite{karras2021aliasfree} adapted with cylindrical coordinates to generate 360$^{\circ}$ panoramas~\cite{chai2022anyresolution}. 

For the terrain input, we take the filtered LHQ dataset and select the non-sky pixels with normalized disparity greater than 1/16 (this leaves some background mountains to be predicted). We follow the training procedure from~\cite{chai2022anyresolution} with a few adaptations. In addition to concatenating the CLIP embedding of the terrain image to the style-code, the generated sky is composited with the terrain input prior to the discriminator with 50\% probability, which is compared to full RGB images from LHQ. The 50\% compositing behavior ensures that the bottom of the generated skydome can still appear realistic (when unmasked), while also matching provided terrain image (when masked). This portion is trained with the $\lambda_{R1}=2$ in the GAN objective (Eqn.~\ref{eqn:gan}), with randomly sampled cylindrical coordinates and a cross-frame discriminator applied to the boundary of two adjacent frames.

\subsection{Extendable Triplane Implementation}

To construct the extendable triplane representation, we modify the triplane model from EG3D~\cite{chan2022eg3D} to generate three planes from independent synthesis networks $\Generator_{XY}$, $\Generator_{XZ}$, and $\Generator_{YZ}$, tied to the same latent code and mapping network. Similar to our layout feature model, we train the terrain generator on sky-segmented images and disparity maps as input into the low-resolution discriminator to help the model learn geometry. The upsampler portion of this model and the training procedure is the same as EG3D, using $\lambda_{R1}=10$. To prevent the model from rendering the segmented sky color (we use white for the sky color, following the background color of NeRF~\cite{mildenhall2020nerf}), we finetune the model penalizing for white pixels when the sky mask is one:
\begin{equation}
\mathcal{L}_{\mathrm{sky}} = \exp(-5*\sum_{c}(\ImageLR[c]-1) * \MaskLR. \\
\end{equation}
The finetuning operation is performed for 400K samples with $\lambda_{\mathrm{sky}}$ increasing from zero to 40 during training.
At inference time, we perform SOAT~\cite{chong2021stylegan} feature stitching to each generator along the appropriate dimensions to obtain the extended triplane representation. As the skydome model does not train on generated images, we use the same skydome model as before. We use 50 randomly sampled camera poses for training, which improves the geometry diversity (more mountainous terrain) the compared to using 1K random training poses.


