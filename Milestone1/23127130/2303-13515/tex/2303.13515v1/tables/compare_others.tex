\begin{table}[t!]
  \centering
  \resizebox{1.0\linewidth}{!}{
  \begin{tabular}{lcccccc}
    \toprule
     \multirow{2}{*}{\bf Model} & 
     \multirow{2}{*}{\bf Persistent} &
     \multirow{2}{*}{\bf Unbounded} & 
     \multicolumn{3}{c}{\bf FID} & 
     \multirow{2}{*}{\bf Consistency} \\
     \cmidrule(lr){4-6}
     & & &  $C_\text{train}$ & $C_\text{forward}$ & $C_\text{random}$ &  \\
    \midrule
    GSN~\cite{devries2021unconstrained} & \cmark & \xmark & 29.95 & 50.22 & 45.48 & 12.80 \\ 
    EG3D~\cite{chan2022eg3D} & \cmark & \xmark & \bf 9.85 & 30.17 & 32.08 & \bf 3.01 \\
    Ours & \cmark & \cmark &  21.42 & \bf 26.67 & \bf 23.39 &  3.56 \\
    \bottomrule
  \end{tabular}
  }
  \vspace{-0.25cm}
\caption{\small \emph{Quantitative comparison to unconditional GANs.}
We evaluate image quality as FID on 5K images on (a) training camera poses $C_\textrm{train}$, (b) forward motion $C_\textrm{forward}$ (See Table~\ref{tab:quant_1}), (c) random camera poses $C_\textrm{random}$. One-step consistency error is measured
as the L1 error when backwards warping the result after one camera step to the initial frame, multiplied by 100. 
Once outside the training pose distribution our model generates better images than other methods, with consistency close to that of EG3D.}
\label{tab:quant_2}
\vspace{-0.5cm}
\end{table}


