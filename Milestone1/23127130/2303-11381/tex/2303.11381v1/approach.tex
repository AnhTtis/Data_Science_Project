\section{\textbf{\modelname}~Prompting}

The goal of \modelname~is to compose numerous vision experts
to empower ChatGPT with visual understanding.
A vision expert is a computer vision model that takes an image as input and 
interprets the content from different perspectives.
For instance, the image captioning expert generates a natural description, the OCR expert extracts the scene text in the image, the celebrity recognition model identifies the celebrity names, and the object detection model extracts the salient object with bounding box locations.
At present, one may have to manually decide which vision experts to employ for specific use cases, and manually compose them. Instead, our target is to automate this process based on the requirements presented by the user query in natural language. 

ChatGPT is an artificial intelligence chatbot with text as both input and output, without visual understanding. However, ChatGPT exhibits strong instruct learning capability, which motivates us to instruct ChatGPT to properly determine which vision expert should be invoked and which image should be processed.

Figure~\ref{fig:arch} shows the flowchart of our \modelname~system. The terms \textit{thought} and \textit{action request} refer to the reasoning and action-oriented texts generated by ChatGPT to break down the problem or invoke vision experts. 
\textit{Observation} refers to the vision expert's responses after the \textit{action execution} requested in the \textit{action request} text.
% \textit{Observation} refers to the vision expert's responses after running the selected vision model (\ie, tools).
Next, we detail each step in the flowchart as follows.

\subsection{User Input}
As ChatGPT only accepts texts as input, the first challenge is how to accommodate non-text inputs, such as multiple images and videos.
Since most vision experts accept the file path or URL, we use the path string to indicate non-text inputs.
The file path itself is meaningless and is essentially a placeholder. 
Although no visual recognition task can be performed directly with file paths, ChatGPT may seek help from different vision experts to understand the image content from different perspectives, \eg, identifying the celebrity names of the detected person. 
By including the provided file path in its text output, ChatGPT can indicate which image should be processed by the vision expert. 

\subsection{ChatGPT Response}
Given the user's input, ChatGPT is expected to provide two kinds of responses. 
The first is to seek help from vision experts, while the second is to respond to the user directly. A key challenge is to set up a protocol such that we know when to invoke the vision expert. 
Inspired by \textsc{ReAct}~\cite{yao2022react}, we instruct ChatGPT to respond with certain watchwords, such as \textit{``Assistant, what objects are there in the image? $<$file path$>$''}, if a specific vision expert is required. In our implementation, we use the keyword  ``Assistant,'' to distinguish whether a vision expert is required.

To further improve the performance, 
we encourage ChatGPT to show the \textit{thought} (reasoning)
process to highlight why an external tool is required. 
It is also shown to be beneficial in NLP studies~\cite{yao2022react} to incorporate such reasoning.

\subsection{Vision Experts}
\label{sec:3.3}
Given the action request from ChatGPT, we use the regular expression matching to parse the expert name and the file path, and invoke the action (vision expert execution).

The expert's output can be in different forms but is standardized into the text format such that ChatGPT can understand it. For certain experts, such as the captioning model or the celebrity model, it is straightforward to represent the output as text. However, the standardization is less intuitive for others. For example, the detection model outputs a list of object names with bounding box locations. In this case, we concatenate all the boxes, each of which is represented as $<$object name, x1, y1, x2, y2$>$, where (x1,y1), (x2,y2) are the coordinates of the top-left and bottom-right corners, respectively. An additional text description is added to explain the meaning of the last four numerical values. In some cases, we find ChatGPT is capable of understanding these coordinates, \eg, identifying which object is on the left.

The text-formed output from vision experts can be interpreted as the \textit{observation} resulting from ChatGPT's action of invoking the vision expert.
Combining observations with the chat history, ChatGPT can further invoke additional experts or return the final answer to the user. We provide examples of full execution flows in Section~\ref{sec:4.2} and Figure~\ref{fig:prompt}.

To inject the knowledge of various vision experts' usages, we add both instructions and in-context examples in the prefix when prompting ChatGPT. 
Each expert is described with the model name, a general description of its capability, the input data format, and the output information. 
After describing each expert, we add a few in-context dialogue examples to enhance the performance.
With the injected knowledge, ChatGPT can effectively select one or multiple vision experts to understand the images or videos from different perspectives.

\subsection{Extensibility} 
Our scheme is motivated by \textsc{ReAct}, which invokes different tools in the NLP field. As only the text is involved, no specific design is required to incorporate other modalities. In this work, we extend \modelname~to the vision domain.
The key is to replace the non-text modality with a path string, enabling ChatGPT to ask specific vision experts to recognize the visual content.
Therefore, we could further extend \modelname~to other modalities, such as speech and audio.
Meanwhile, we can also easily incorporate more tools by formatting their outputs in a text format. 
While ChatGPT serves as the primary LLM in our main implementation, performance could be further enhanced through the simple upgrade to a more powerful LLM, such as GPT-4~\cite{gpt4} discussed in Section~\ref{sec:4.5}.