\begin{abstract}
% \vspace{-2mm}
We propose \modelname, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action. In this paper, we define and explore a comprehensive list of advanced vision tasks that are intriguing to solve, but may exceed the capabilities of existing vision and
vision-language models. To achieve such advanced visual intelligence, \modelname~introduces a textual prompt design that can represent text descriptions, textualized spatial coordinates, and aligned file names for dense visual signals such as images and videos. \modelname's prompt design allows language models to accept, associate, and process multimodal information, thereby facilitating the synergetic combination of ChatGPT and various vision experts. Zero-shot experiments demonstrate \modelname's effectiveness in addressing the specified capabilities of interests and its wide application in different scenarios that require advanced visual understanding. Furthermore, we discuss and compare \modelname's system paradigm with an alternative approach that extends language models for multimodal scenarios through joint finetuning.
Code, demo, video, and visualization are available at \url{https://multimodal-react.github.io/}.
% \vspace{0pt}
\blfootnote{$^*$Equal Contribution\hspace{3mm}$^\spadesuit$Project Lead}
\end{abstract}