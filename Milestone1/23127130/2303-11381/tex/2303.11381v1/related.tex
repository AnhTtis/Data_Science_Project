\section{Related Work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{LLMs Prompting Methods.}
Large language models (LLMs)~\cite{brown2020language,chowdhery2022palm} demonstrate a strong chain-of-thought (CoT) capability~\cite{wei2022chain,kojima2022large} that could break down complex problems into solvable intermediate steps. 
On the other hand, research~\cite{nakano2021webgpt,huang2022language,ahn2022can} shows that LLMs, when equipped with a range of external NLP tools, can effectively serve as action planners to select and utilize tools for problem-solving, such as using search or mathematical tools to address knowledge or math problems.

Nevertheless, LLMs for reasoning~\cite{wei2022chain,kojima2022large} and LLMs for action~\cite{nakano2021webgpt,huang2022language,ahn2022can} 
, when used independently,
fail to solve complex tasks that require breaking down the problem via reasoning and solving sub-steps via planned actions.
Recent studies~\cite{yao2022react,gao2022pal,trivedi2022interleaving,schick2023toolformer} have attempted to merge the action and reasoning phases to enhance LLMs' capabilities in solving complicated tasks that require advanced planning and reasoning. One representative work, \textsc{ReAct}~\cite{yao2022react}, treats reasoning text generation as an executable action and achieves the synergetic combination of reasoning and action for NLP tasks. In this work, we explore how to extend such intriguing properties into multimodal scenarios by modeling thought and invoking vision tools as executable actions.

\begin{figure*}[t]
\centering
\includegraphics[width=.99\textwidth]{iccv2023AuthorKit/figure/flowchart-crop.pdf}
\caption{Flowchart of \modelname~for enhanced visual understanding with ChatGPT. 
The user input can be in the form of text, images, or videos, with the latter two represented as file path strings.
ChatGPT is instructed to say specific watchwords in \textit{action request} if a vision expert is required to interpret the visual inputs. 
Regular expression matching is applied to parse the expert's name and the file path, which are then used to call the vision expert (\textit{action execution}).
The expert's output (\textit{observation}) is serialized as text and combined with the history to further activate ChatGPT.
If no extra experts are needed, \modelname~would return the final response to the user.
The right figure shows a single-round vision expert execution, which is the component that constructs the full execution flow illustrated in Figure~\ref{fig:prompt}.
}
\label{fig:arch}
\end{figure*}


\vspace{1mm}
\noindent\textbf{Vision+LLMs.}
Our \modelname~is related to the previous studies that extend language models to understand visual inputs. The representative framework adds a vision module to project visual inputs into representations that the language model can understand. These representations can be either discrete text words~\cite{yang2022empirical,zeng2022socratic,wanglanguage,hu2022promptcap} or continuous features projected into the textual feature space~\cite{tsimpoukelli2021multimodal,alayrac2022flamingo,li2023blip,huang2023language,driess2023palme}. 
Recent vision-language studies explore the chain-of-thought capability~\cite{wei2022chain,kojima2022large} in multimodal settings. MM-CoT~\cite{zhang2023multimodal} finetunes on the reasoning chain annotated in the ScienceQA~\cite{lu2022learn} dataset to achieve the CoT capability in the science question answering task. KOSMOS-1~\cite{huang2023language} and PaLM-E~\cite{driess2023palme} demonstrate the zero-shot multimodal CoT capabilities with large-scale training.


\vspace{1mm}
\noindent\textbf{Multimodal Reasoning and Action.} 
A key distinction between \modelname~and prior vision+LLM studies discussed above is that \modelname~leverages LLMs' high-level planning abilities to allocate various vision experts, rather than solely using LLMs for text generation conditioned on visual inputs.
\modelname~is closely related to the recent concurrent work of Visual ChatGPT~\cite{wu2023visual} and ViperGPT~\cite{suris2023vipergpt}. In comparison, Visual ChatGPT~\cite{wu2023visual} primarily focuses on image generation and editing, while our \modelname~mainly focuses on visual understanding. ViperGPT~\cite{suris2023vipergpt} instructs LLMs to generate Python code for a one-round query answering. In contrast, \modelname~is a multi-round, dialogue-based system that may integrate the strong QA model as one of its vision experts.