@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


@article{zou2022xdecoder,
  author      = {Zou, Xueyan and Dou, Zi-Yi and Yang, Jianwei and Gan, Zhe and Li, Linjie and Li, Chunyuan and Dai, Xiyang and Wang, Jianfeng and Yuan, Lu and Peng, Nanyun and Wang, Lijuan and Lee, Yong Jae and Gao, Jianfeng},
  title       = {Generalized Decoding for Pixel, Image and Language},
  publisher   = {arXiv:2212.11270v1},
  year        = {2022},
}

@inproceedings{singh2019towards,
  title={Towards vqa models that can read},
  author={Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle={CVPR},
  year={2019}
}
@inproceedings{kamath2021mdetr,
  title={MDETR-modulated detection for end-to-end multi-modal understanding},
  author={Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Synnaeve, Gabriel and Misra, Ishan and Carion, Nicolas},
  booktitle={ICCV},
  year={2021}
}
@inproceedings{shao2019objects365,
  title={Objects365: A large-scale, high-quality dataset for object detection},
  author={Shao, Shuai and Li, Zeming and Zhang, Tianyuan and Peng, Chao and Yu, Gang and Zhang, Xiangyu and Li, Jing and Sun, Jian},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={8430--8439},
  year={2019}
}
@article{kuznetsova2020open,
  title={The open images dataset v4},
  author={Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Kolesnikov, Alexander and others},
  journal={International Journal of Computer Vision},
  volume={128},
  number={7},
  pages={1956--1981},
  year={2020},
  publisher={Springer}
}
@inproceedings{zhou2019grounded,
  title={Grounded video description},
  author={Zhou, Luowei and Kalantidis, Yannis and Chen, Xinlei and Corso, Jason J and Rohrbach, Marcus},
  booktitle={CVPR},
  year={2019}
}
@inproceedings{chen2021pix2seq,
  title={Pix2seq: A Language Modeling Framework for Object Detection},
  author={Chen, Ting and Saxena, Saurabh and Li, Lala and Fleet, David J and Hinton, Geoffrey},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{rohrbach2018object,
  title={Object Hallucination in Image Captioning},
  author={Rohrbach, Anna and Hendricks, Lisa Anne and Burns, Kaylee and Darrell, Trevor and Saenko, Kate},
  booktitle={EMNLP},
  year={2018}
}

@inproceedings{redmon2016you,
  title={You only look once: Unified, real-time object detection},
  author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle={CVPR},
  year={2016}
}
@inproceedings{wang2021simvlm,
  title={SimVLM: Simple Visual Language Model Pretraining with Weak Supervision},
  author={Wang, Zirui and Yu, Jiahui and Yu, Adams Wei and Dai, Zihang and Tsvetkov, Yulia and Cao, Yuan},
  booktitle={ICLR},
  year={2022}
}
@inproceedings{yu2018mattnet,
  title={Mattnet: Modular attention network for referring expression comprehension},
  author={Yu, Licheng and Lin, Zhe and Shen, Xiaohui and Yang, Jimei and Lu, Xin and Bansal, Mohit and Berg, Tamara L},
  booktitle={CVPR},
  year={2018}
}
@article{dou2021empirical,
  title={An Empirical Study of Training End-to-End Vision-and-Language Transformers},
  author={Dou, Zi-Yi and Xu, Yichong and Gan, Zhe and Wang, Jianfeng and Wang, Shuohang and Wang, Lijuan and Zhu, Chenguang and Liu, Zicheng and Zeng, Michael and others},
  journal={arXiv preprint arXiv:2111.02387},
  year={2021}
}
@inproceedings{xue2021probing,
  title={Probing Inter-modality: Visual Parsing with Self-Attention for Vision-and-Language Pre-training},
  author={Xue, Hongwei and Huang, Yupan and Liu, Bei and Peng, Houwen and Fu, Jianlong and Li, Houqiang and Luo, Jiebo},
  booktitle={NeurIPS},
  year={2021}
}
@inproceedings{shen2021much,
  title={How Much Can CLIP Benefit Vision-and-Language Tasks?},
  author={Shen, Sheng and Li, Liunian Harold and Tan, Hao and Bansal, Mohit and Rohrbach, Anna and Chang, Kai-Wei and Yao, Zhewei and Keutzer, Kurt},
  booktitle={ICLR},
  year={2022}
}
@inproceedings{huang2021seeing,
  title={Seeing out of the box: End-to-end pre-training for vision-language representation learning},
  author={Huang, Zhicheng and Zeng, Zhaoyang and Huang, Yupan and Liu, Bei and Fu, Dongmei and Fu, Jianlong},
  booktitle={CVPR},
  year={2021}
}
@inproceedings{vinyals2015show,
  title={Show and tell: A neural image caption generator},
  author={Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  booktitle={CVPR},
  year={2015}
}
@inproceedings{hudson2019gqa,
  title={Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={CVPR},
  year={2019}
}
@inproceedings{hendricks2018women,
  title={Women also snowboard: Overcoming bias in captioning models},
  author={Hendricks, Lisa Anne and Burns, Kaylee and Saenko, Kate and Darrell, Trevor and Rohrbach, Anna},
  booktitle={ECCV},
  year={2018}
}
@inproceedings{zhou2020more,
  title={More grounded image captioning by distilling image-text matching model},
  author={Zhou, Yuanen and Wang, Meng and Liu, Daqing and Hu, Zhenzhen and Zhang, Hanwang},
  booktitle={CVPR},
  year={2020}
}
@inproceedings{ma2019learning,
  title={Learning to Generate Grounded Visual Captions without Localization Supervision},
  author={Ma, Chih-Yao and Kalantidis, Yannis and AlRegib, Ghassan and Vajda, Peter and Rohrbach, Marcus and Kira, Zsolt},
  booktitle={ECCV},
  year={2020}
}
@inproceedings{tarvainen2017mean,
  title={Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
  author={Tarvainen, Antti and Valpola, Harri},
  booktitle={NeurIPS},
  year={2017}
}
@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={ECCV},
  year={2020}
}
@article{Gupta2021TowardsGP,
  title={Towards General Purpose Vision Systems},
  author={Tanmay Gupta and Amita Kamath and Aniruddha Kembhavi and Derek Hoiem},
  journal={arXiv preprint arXiv:2104.00743},
  year={2021}
}
@inproceedings{hu2021unit,
  title={UniT: Multimodal Multitask Learning with a Unified Transformer},
  author={Hu, Ronghang and Singh, Amanpreet},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{cho2021unifying,
  title={Unifying vision-and-language tasks via text generation},
  author={Cho, Jaemin and Lei, Jie and Tan, Hao and Bansal, Mohit},
  booktitle={ICML},
  year={2021}
}

@inproceedings{hu2020iterative,
  title={Iterative answer prediction with pointer-augmented multimodal transformers for textvqa},
  author={Hu, Ronghang and Singh, Amanpreet and Darrell, Trevor and Rohrbach, Marcus},
  booktitle={CVPR},
  year={2020}
}
@inproceedings{sidorov2020textcaps,
  title={Textcaps: a dataset for image captioning with reading comprehension},
  author={Sidorov, Oleksii and Hu, Ronghang and Rohrbach, Marcus and Singh, Amanpreet},
  booktitle={ECCV},
  pages={742--758},
  year={2020},
}
@article{gao2020structured,
  title={Structured Multimodal Attentions for TextVQA},
  author={Gao, Chenyu and Zhu, Qi and Wang, Peng and Li, Hui and Liu, Yuliang and Hengel, Anton van den and Wu, Qi},
  journal={arXiv preprint arXiv:2006.00753},
  year={2020}
}
@inproceedings{biten2019scene,
  title={Scene text visual question answering},
  author={Biten, Ali Furkan and Tito, Ruben and Mafla, Andres and Gomez, Lluis and Rusinol, Mar{\c{c}}al and Valveny, Ernest and Jawahar, CV and Karatzas, Dimosthenis},
  booktitle={ICCV},
  year={2019}
}
@inproceedings{singh2019strings,
  title={From Strings to Things: Knowledge-enabled VQA Model that can Read and Reason},
  author={Singh, Ajeet Kumar and Mishra, Anand and Shekhar, Shashank and Chakraborty, Anirban},
  booktitle={CVPR},
  year={2019}
}
@inproceedings{mishra2019ocr,
  title={Ocr-vqa: Visual question answering by reading text in images},
  author={Mishra, Anand and Shekhar, Shashank and Singh, Ajeet Kumar and Chakraborty, Anirban},
  booktitle={International Conference on Document Analysis and Recognition (ICDAR)},
  year={2019}
}
@inproceedings{wang2020general,
  title={On the general value of evidence, and bilingual scene-text visual question answering},
  author={Wang, Xinyu and Liu, Yuliang and Shen, Chunhua and Ng, Chun Chet and Luo, Canjie and Jin, Lianwen and Chan, Chee Seng and Hengel, Anton van den and Wang, Liangwei},
  booktitle={CVPR},
  year={2020}
}
@inproceedings{li2020oscar,
  title={Oscar: Object-semantics aligned pre-training for vision-language tasks},
  author={Li, Xiujun and Yin, Xi and Li, Chunyuan and Hu, Xiaowei and Zhang, Pengchuan and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others},
  booktitle={ECCV},
  year={2020}
}
@inproceedings{borisyuk2018rosetta,
  title={Rosetta: Large scale system for text detection and recognition in images},
  author={Borisyuk, Fedor and Gordo, Albert and Sivakumar, Viswanath},
  booktitle={SIGKDD},
  year={2018}
}
@inproceedings{bigham2010vizwiz,
  title={VizWiz: nearly real-time answers to visual questions},
  author={Bigham, Jeffrey P and Jayant, Chandrika and Ji, Hanjie and Little, Greg and Miller, Andrew and Miller, Robert C and Miller, Robin and Tatarowicz, Aubrey and White, Brandyn and White, Samual and others},
  booktitle={Proceedings of the 23nd annual ACM symposium on User interface software and technology},
  year={2010}
}
@inproceedings{gao2020multi,
  title={Multi-Modal Graph Neural Network for Joint Reasoning on Vision and Scene Text},
  author={Gao, Difei and Li, Ke and Wang, Ruiping and Shan, Shiguang and Chen, Xilin},
  booktitle={CVPR},
  year={2020}
}
@inproceedings{lu2019vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle={NeurIPS},
  year={2019}
}
@article{li2019visualbert,
  title={Visualbert: A simple and performant baseline for vision and language},
  author={Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:1908.03557},
  year={2019}
}
@misc{enwiki:1068820985,
    author = "{Wikipedia contributors}",
    title = "Code-switching --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2022",
    url = "https://en.wikipedia.org/w/index.php?title=Code-switching&oldid=1068820985"
  }
@inproceedings{aribandi2021ext5,
  title={ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning},
  author={Aribandi, Vamsi and Tay, Yi and Schuster, Tal and Rao, Jinfeng and Zheng, Huaixiu Steven and Mehta, Sanket Vaibhav and Zhuang, Honglei and Tran, Vinh Q and Bahri, Dara and Ni, Jianmo and others},
  booktitle={ICLR},
  year={2022}
}
@inproceedings{chen2021distributed,
  title={Distributed attention for grounded image captioning},
  author={Chen, Nenglun and Pan, Xingjia and Chen, Runnan and Yang, Lei and Lin, Zhiwen and Ren, Yuqiang and Yuan, Haolei and Guo, Xiaowei and Huang, Feiyue and Wang, Wenping},
  booktitle={ACMMM},
  year={2021}
}
@inproceedings{harold_GLIP2021,
      title={Grounded Language-Image Pre-training},
      author={Liunian Harold Li* and Pengchuan Zhang* and Haotian Zhang* and Jianwei Yang and Chunyuan Li and Yiwu Zhong and Lijuan Wang and Lu Yuan and Lei Zhang and Jenq-Neng Hwang and Kai-Wei Chang and Jianfeng Gao},
      year={2021},
      booktitle={arXiv preprint arXiv:2112.03857},
}
@inproceedings{alberti2019fusion,
  title={Fusion of Detected Objects in Text for Visual Question Answering},
  author={Alberti, Chris and Ling, Jeffrey and Collins, Michael and Reitter, David},
  booktitle={EMNLP},
  year={2019}
}
@inproceedings{li2020unicoder,
  title={Unicoder-VL: A Universal Encoder for Vision and Language by Cross-Modal Pre-Training.},
  author={Li, Gen and Duan, Nan and Fang, Yuejian and Gong, Ming and Jiang, Daxin and Zhou, Ming},
  booktitle={AAAI},
  year={2020}
}
@inproceedings{tan2019lxmert,
  title={LXMERT: Learning Cross-Modality Encoder Representations from Transformers},
  author={Tan, Hao and Bansal, Mohit},
  booktitle={EMNLP},
  year={2019}
}
@inproceedings{su2019vl,
  title={VL-BERT: Pre-training of Generic Visual-Linguistic Representations},
  author={Su, Weijie and Zhu, Xizhou and Cao, Yue and Li, Bin and Lu, Lewei and Wei, Furu and Dai, Jifeng},
  booktitle={ICLR},
  year={2019}
}
@inproceedings{zhou2020unified,
  title={Unified Vision-Language Pre-Training for Image Captioning and VQA.},
  author={Zhou, Luowei and Palangi, Hamid and Zhang, Lei and Hu, Houdong and Corso, Jason J and Gao, Jianfeng},
  booktitle={AAAI},
  year={2020}
}
@inproceedings{chen2019uniter,
  title={Uniter: Learning universal image-text representations},
  author={Chen, Yen-Chun and Li, Linjie and Yu, Licheng and Kholy, Ahmed El and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
  booktitle={ECCV},
  year={2020}
}
@inproceedings{lu202012,
  title={12-in-1: Multi-task vision and language representation learning},
  author={Lu, Jiasen and Goswami, Vedanuj and Rohrbach, Marcus and Parikh, Devi and Lee, Stefan},
  booktitle={CVPR},
  year={2020}
}
@article{huang2020pixel,
  title={Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers},
  author={Huang, Zhicheng and Zeng, Zhaoyang and Liu, Bei and Fu, Dongmei and Fu, Jianlong},
  journal={arXiv preprint arXiv:2004.00849},
  year={2020}
}
@InProceedings{VQA_15,
author = {Stanislaw Antol and Aishwarya Agrawal and Jiasen Lu and Margaret Mitchell and Dhruv Batra and C. Lawrence Zitnick and Devi Parikh},
title = {{VQA}: {V}isual {Q}uestion {A}nswering},
booktitle = {ICCV},
year = {2015},
}
@InProceedings{balanced_binary_vqa,
author = {Peng Zhang and Yash Goyal and Douglas Summers{-}Stay and Dhruv Batra and Devi Parikh},
title = {{Y}in and {Y}ang: Balancing and Answering Binary Visual Questions},
booktitle = {CVPR},
year = {2016},
}
@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={ECCV},
  year={2014}
}
@inproceedings{deng2021transvg,
  title={TransVG: End-to-End Visual Grounding with Transformers},
  author={Deng, Jiajun and Yang, Zhengyuan and Chen, Tianlang and Zhou, Wengang and Li, Houqiang},
  booktitle={ICCV},
  year={2021}
}
@inproceedings{hu2021scaling,
  title={Scaling up vision-language pre-training for image captioning},
  author={Hu, Xiaowei and Gan, Zhe and Wang, Jianfeng and Yang, Zhengyuan and Liu, Zicheng and Lu, Yumao and Wang, Lijuan},
  booktitle={CVPR},
  year={2022}
}
@inproceedings{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  booktitle={ICLR},
  year={2020}
}
@inproceedings{aghajanyan2021muppet,
  title={Muppet: Massive Multi-task Representations with Pre-Finetuning},
  author={Aghajanyan, Armen and Gupta, Anchit and Shrivastava, Akshat and Chen, Xilun and Zettlemoyer, Luke and Gupta, Sonal},
  booktitle={EMNLP},
  year={2021}
}
@inproceedings{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  booktitle={ICLR},
  year={2022}
}
@inproceedings{wolf2020transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Chaumond, Julien and Debut, Lysandre and Sanh, Victor and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and others},
  booktitle={EMNLP: System Demonstrations},
  year={2020}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={CVPR},
  year={2016}
}
@inproceedings{lu2018neural,
  title={Neural baby talk},
  author={Lu, Jiasen and Yang, Jianwei and Batra, Dhruv and Parikh, Devi},
  booktitle={CVPR},
  year={2018}
}
@inproceedings{gupta2019lvis,
  title={LVIS: A dataset for large vocabulary instance segmentation},
  author={Gupta, Agrim and Dollar, Piotr and Girshick, Ross},
  booktitle={CVPR},
  year={2019}
}
@inproceedings{karpathy2015deep,
  title={Deep visual-semantic alignments for generating image descriptions},
  author={Karpathy, Andrej and Fei-Fei, Li},
  booktitle={CVPR},
  year={2015}
}
@article{raffel2019exploring,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={JMLR},
  year={2020}
}
@inproceedings{donahue2015long,
  title={Long-term recurrent convolutional networks for visual recognition and description},
  author={Donahue, Jeffrey and Anne Hendricks, Lisa and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor},
  booktitle={CVPR},
  year={2015}
}
@InProceedings{jiang2020defense,
  title={In Defense of Grid Features for Visual Question Answering},
  author={Jiang, Huaizu and Misra, Ishan and Rohrbach, Marcus and Learned-Miller, Erik and Chen, Xinlei},
  booktitle={CVPR},
  year={2020}
}
@inproceedings{pan2020x,
  title={X-Linear Attention Networks for Image Captioning},
  author={Pan, Yingwei and Yao, Ting and Li, Yehao and Mei, Tao},
  booktitle={CVPR},
  year={2020}
}
@inproceedings{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={NAACL-HLT},
  year={2019}
}
@inproceedings{kant2020spatially,
  title={Spatially Aware Multimodal Transformers for TextVQA},
  author={Kant, Yash and Batra, Dhruv and Anderson, Peter and Schwing, Alex and Parikh, Devi and Lu, Jiasen and Agrawal, Harsh},
  booktitle={ECCV},
  year={2020}
}
@inproceedings{cao2020behind,
  title={Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models},
  author={Cao, Jize and Gan, Zhe and Cheng, Yu and Yu, Licheng and Chen, Yen-Chun and Liu, Jingjing},
  booktitle={ECCV},
  year={2020}
}
@inproceedings{kong2019mutual,
  title={A Mutual Information Maximization Perspective of Language Representation Learning},
  author={Kong, Lingpeng and d'Autume, Cyprien de Masson and Ling, Wang and Yu, Lei and Dai, Zihang and Yogatama, Dani},
  booktitle={ICLR},
  year={2020}
}
@inproceedings{sharma2018conceptual,
  title={Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},
  author={Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
  booktitle={ACL},
  year={2018}
}
@inproceedings{wang2020multimodal,
  title={Multimodal Attention with Image Text Spatial Relationship for OCR-Based Image Captioning},
  author={Wang, Jing and Jinhui, Tang and Jiebo, Luo},
  booktitle={ACMMM},
  year={2020}
}
@inproceedings{karatzas2013icdar,
  title={ICDAR 2013 robust reading competition},
  author={Karatzas, Dimosthenis and Shafait, Faisal and Uchida, Seiichi and Iwamura, Masakazu and i Bigorda, Lluis Gomez and Mestre, Sergi Robles and Mas, Joan and Mota, David Fernandez and Almazan, Jon Almazan and De Las Heras, Lluis Pere},
  booktitle={12th International Conference on Document Analysis and Recognition},
  year={2013}
}
@inproceedings{karatzas2015icdar,
  title={ICDAR 2015 competition on robust reading},
  author={Karatzas, Dimosthenis and Gomez-Bigorda, Lluis and Nicolaou, Anguelos and Ghosh, Suman and Bagdanov, Andrew and Iwamura, Masakazu and Matas, Jiri and Neumann, Lukas and Chandrasekhar, Vijay Ramaseshan and Lu, Shijian and others},
  booktitle={2015 13th International Conference on Document Analysis and Recognition (ICDAR)},
  year={2015}
}
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={CVPR},
  year={2009}
}
@inproceedings{gurari2018vizwiz,
  title={Vizwiz grand challenge: Answering visual questions from blind people},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
  booktitle={CVPR},
  year={2018}
}
@inproceedings{mishra2013image,
  title={Image retrieval using textual cues},
  author={Mishra, Anand and Alahari, Karteek and Jawahar, CV},
  booktitle={ICCV},
  year={2013}
}
@article{krishna2017visual,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal={IJCV},
  year={2017}
}
@article{veit2016coco,
  title={Coco-text: Dataset and benchmark for text detection and recognition in natural images},
  author={Veit, Andreas and Matera, Tomas and Neumann, Lukas and Matas, Jiri and Belongie, Serge},
  journal={arXiv preprint arXiv:1601.07140},
  year={2016}
}
@inproceedings{anderson2018bottom,
  title={Bottom-up and top-down attention for image captioning and visual question answering},
  author={Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
  booktitle={CVPR},
  year={2018}
}
@inproceedings{huang2019attention,
  title={Attention on attention for image captioning},
  author={Huang, Lun and Wang, Wenmin and Chen, Jie and Wei, Xiao-Yong},
  booktitle={ICCV},
  year={2019}
}
@inproceedings{cnmt,
  title={Confidence-aware Non-repetitive Multimodal Transformers for TextCaps},
  author={Wang, Zhaokai and Bao, Renda and Wu, Qi and Liu, Si},
  booktitle={AAAI},
  year={2021}
}
%   title={CNMT, TextCaps Challenge 2020 top entry.},
%   author={submission, Anonymous},
%   booktitle={https://evalai.cloudcv.org/web/challenges/ challenge-page/573/overview},
%   month={nov},
%   year={2020}
% }
@inproceedings{yao2018exploring,
  title={Exploring visual relationship for image captioning},
  author={Yao, Ting and Pan, Yingwei and Li, Yehao and Mei, Tao},
  booktitle={ECCV},
  year={2018}
}
@inproceedings{yang2019fast,
  title={A fast and accurate one-stage approach to visual grounding},
  author={Yang, Zhengyuan and Gong, Boqing and Wang, Liwei and Huang, Wenbing and Yu, Dong and Luo, Jiebo},
  booktitle={ICCV},
  year={2019}
}
@inproceedings{papineni2002bleu,
  title={BLEU: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={ACL},
  year={2002}
}
@inproceedings{denkowski2014meteor,
  title={Meteor universal: Language specific translation evaluation for any target language},
  author={Denkowski, Michael and Lavie, Alon},
  booktitle={Proceedings of the ninth workshop on statistical machine translation},
  year={2014}
}
@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}
@inproceedings{anderson2016spice,
  title={Spice: Semantic propositional image caption evaluation},
  author={Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
  booktitle={ECCV},
  year={2016}
}
@inproceedings{vedantam2015cider,
  title={Cider: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={CVPR},
  year={2015}
}
@inproceedings{ren2015faster,
  title={Faster r-cnn: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  booktitle={NeurIPS},
  year={2015}
}
@inproceedings{girshick2014rich,
  title={Rich feature hierarchies for accurate object detection and semantic segmentation},
  author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle={CVPR},
  year={2014}
}
@article{almazan2014word,
  title={Word spotting and recognition with embedded attributes},
  author={Almaz{\'a}n, Jon and Gordo, Albert and Forn{\'e}s, Alicia and Valveny, Ernest},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={36},
  number={12},
  pages={2552--2566},
  year={2014},
  publisher={IEEE}
}
@article{bojanowski2017enriching,
  title={Enriching word vectors with subword information},
  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal={Transactions of the Association for Computational Linguistics},
  volume={5},
  pages={135--146},
  year={2017},
  publisher={MIT Press}
}
@inproceedings{ordonez2011im2text,
  title={Im2text: Describing images using 1 million captioned photographs},
  author={Ordonez, Vicente and Kulkarni, Girish and Berg, Tamara L},
  booktitle={NeurIPS},
  year={2011}
}
@inproceedings{yu2016modeling,
  title={Modeling context in referring expressions},
  author={Yu, Licheng and Poirson, Patrick and Yang, Shan and Berg, Alexander C and Berg, Tamara L},
  booktitle={ECCV},
  year={2016},
  }
  @inproceedings{mao2016generation,
  title={Generation and comprehension of unambiguous object descriptions},
  author={Mao, Junhua and Huang, Jonathan and Toshev, Alexander and Camburu, Oana and Yuille, Alan L and Murphy, Kevin},
  booktitle={CVPR},
  year={2016}
}
@inproceedings{plummer2015flickr30k,
  title={Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models},
  author={Plummer, Bryan A and Wang, Liwei and Cervantes, Chris M and Caicedo, Juan C and Hockenmaier, Julia and Lazebnik, Svetlana},
  booktitle={ICCV},
  year={2015}
}
@article{wang2018learning,
  title={Learning two-branch neural networks for image-text matching tasks},
  author={Wang, Liwei and Li, Yin and Huang, Jing and Lazebnik, Svetlana},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={41},
  number={2},
  pages={394--407},
  year={2018},
  publisher={IEEE}
}
@inproceedings{anderson2018vision,
  title={Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments},
  author={Anderson, Peter and Wu, Qi and Teney, Damien and Bruce, Jake and Johnson, Mark and S{\"u}nderhauf, Niko and Reid, Ian and Gould, Stephen and van den Hengel, Anton},
  booktitle={CVPR},
  pages={3674--3683},
  year={2018}
}
@article{kuznetsova2018open,
  title={The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale},
  author={Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Duerig, Tom and others},
  journal={arXiv preprint arXiv:1811.00982},
  year={2018}
}
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NeurIPS},
  year={2017}
}
@inproceedings{li2020does,
  title={What Does BERT with Vision Look At?},
  author={Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={5265--5275},
  year={2020}
}
@article{jiang2018pythia,
  title={Pythia v0. 1: the winning entry to the vqa challenge 2018},
  author={Jiang, Yu and Natarajan, Vivek and Chen, Xinlei and Rohrbach, Marcus and Batra, Dhruv and Parikh, Devi},
  journal={arXiv preprint arXiv:1807.09956},
  year={2018}
}
@article{han2020finding,
  title={Finding the Evidence: Localization-aware Answer Prediction for Text Visual Question Answering},
  author={Han, Wei and Huang, Hantao and Han, Tao},
  journal={arXiv preprint arXiv:2010.02582},
  year={2020}
}
@inproceedings{liu2020cascade,
  title={Cascade Reasoning Network for Text-based Visual Question Answering},
  author={Liu, Fen and Xu, Guanghui and Wu, Qi and Du, Qing and Jia, Wei and Tan, Mingkui},
  booktitle={Proceedings of the 28th ACM International Conference on Multimedia},
  pages={4060--4069},
  year={2020}
}
@article{young2014image,
  title={From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
  author={Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
  journal={Transactions of the Association for Computational Linguistics},
  volume={2},
  pages={67--78},
  year={2014},
  publisher={MIT Press}
}
@inproceedings{kazemzadeh2014referitgame,
  title={Referitgame: Referring to objects in photographs of natural scenes},
  author={Kazemzadeh, Sahar and Ordonez, Vicente and Matten, Mark and Berg, Tamara},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={787--798},
  year={2014}
}
@article{hu2020vivo,
  title={VIVO: Surpassing Human Performance in Novel Object Captioning with Visual Vocabulary Pre-Training},
  author={Hu, Xiaowei and Yin, Xi and Lin, Kevin and Wang, Lijuan and Zhang, Lei and Gao, Jianfeng and Liu, Zicheng},
  journal={arXiv preprint arXiv:2009.13682},
  year={2020}
}
@inproceedings{xu2020layoutlm,
  title={Layoutlm: Pre-training of text and layout for document image understanding},
  author={Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={1192--1200},
  year={2020}
}
@inproceedings{singh2018pythia,
  title={Pythia-a platform for vision \& language research},
  author={Singh, Amanpreet and Natarajan, Vivek and Jiang, Yu and Chen, Xinlei and Shah, Meet and Rohrbach, Marcus and Batra, Dhruv and Parikh, Devi},
  booktitle={SysML Workshop, NeurIPS},
  volume={2018},
  year={2018}
}
@misc{singh2020mmf,
  title={MMF: A multimodal framework for vision and language research},
  author={Singh, Amanpreet and Goswami, Vedanuj and Natarajan, Vivek and Jiang, Yu and Chen, Xinlei and Shah, Meet and Rohrbach, Marcus and Batra, Dhruv and Parikh, Devi},
  howpublished ={\url{https://github.com/facebookresearch/mmf}},
  year={2020}
}
@article{lin2021m6,
  title={M6: A chinese multimodal pretrainer},
  author={Lin, Junyang and Men, Rui and Yang, An and Zhou, Chang and Ding, Ming and Zhang, Yichang and Wang, Peng and Wang, Ang and Jiang, Le and Jia, Xianyan and others},
  journal={arXiv preprint arXiv:2103.00823},
  year={2021}
}
@article{jia2021scaling,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc V and Sung, Yunhsuan and Li, Zhen and Duerig, Tom},
  journal={arXiv preprint arXiv:2102.05918},
  year={2021}
}
@article{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  journal={arXiv preprint arXiv:2103.00020},
  year={2021}
}
@article{huo2021wenlan,
  title={WenLan: Bridging vision and language by large-scale multi-modal pre-training},
  author={Huo, Yuqi and Zhang, Manli and Liu, Guangzhen and Lu, Haoyu and Gao, Yizhao and Yang, Guoxing and Wen, Jingyuan and Zhang, Heng and Xu, Baogui and Zheng, Weihao and others},
  journal={arXiv preprint arXiv:2103.06561},
  year={2021}
}
@article{wang2020language,
  title={Language models are open knowledge graphs},
  author={Wang, Chenguang and Liu, Xiao and Song, Dawn},
  journal={arXiv preprint arXiv:2010.11967},
  year={2020}
}
@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={NeurIPS},
  year={2020}
}
@article{tsimpoukelli2021multimodal,
  title={Multimodal Few-Shot Learning with Frozen Language Models},
  author={Tsimpoukelli, Maria and Menick, Jacob and Cabi, Serkan and Eslami, SM and Vinyals, Oriol and Hill, Felix},
  journal={arXiv preprint arXiv:2106.13884},
  year={2021}
}
@inproceedings{antol2015vqa,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  booktitle={ICCV},
  year={2015}
}
@inproceedings{goyal2017making,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={CVPR},
  pages={6904--6913},
  year={2017}
}
@inproceedings{marino2019ok,
  title={Ok-vqa: A visual question answering benchmark requiring external knowledge},
  author={Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
  booktitle={CVPR},
  pages={3195--3204},
  year={2019}
}
@inproceedings{kim2021vilt,
  title={Vilt: Vision-and-language transformer without convolution or region supervision},
  author={Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
  booktitle={ICML},
  year={2021}
}
@inproceedings{li2021align,
  title={Align before fuse: Vision and language representation learning with momentum distillation},
  author={Li, Junnan and Selvaraju, Ramprasaath R and Gotmare, Akhilesh Deepak and Joty, Shafiq and Xiong, Caiming and Hoi, Steven},
  booktitle={NeurIPS},
  year={2021}
}
@inproceedings{choe2019attention,
  title={Attention-based dropout layer for weakly supervised object localization},
  author={Choe, Junsuk and Shim, Hyunjung},
  booktitle={CVPR},
  year={2019}
}
@inproceedings{xu2021e2e,
  title={E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning},
  author={Xu, Haiyang and Yan, Ming and Li, Chenliang and Bi, Bin and Huang, Songfang and Xiao, Wenming and Huang, Fei},
  booktitle={ACL},
  year={2021}
}
@inproceedings{zhao2018weakly,
  title={Weakly supervised phrase localization with multi-scale anchored transformer network},
  author={Zhao, Fang and Li, Jianshu and Zhao, Jian and Feng, Jiashi},
  booktitle={CVPR},
  pages={5696--5705},
  year={2018}
}
@article{yeh2018interpretable,
  title={Interpretable and globally optimal prediction for textual grounding using image concepts},
  author={Yeh, Raymond A and Xiong, Jinjun and Hwu, Wen-Mei W and Do, Minh N and Schwing, Alexander G},
  journal={arXiv preprint arXiv:1803.11209},
  year={2018}
}
@inproceedings{zhou2016learning,
  title={Learning deep features for discriminative localization},
  author={Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  booktitle={CVPR},
  year={2016}
}
@inproceedings{yun2019cutmix,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle={CVPR},
  year={2019}
}
@inproceedings{singh2017hide,
  title={Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization},
  author={Singh, Krishna Kumar and Lee, Yong Jae},
  booktitle={ICCV},
  year={2017}
}
@inproceedings{zhang2018adversarial,
  title={Adversarial complementary learning for weakly supervised object localization},
  author={Zhang, Xiaolin and Wei, Yunchao and Feng, Jiashi and Yang, Yi and Huang, Thomas S},
  booktitle={CVPR},
  year={2018}
}
@inproceedings{zhang2018self,
  title={Self-produced guidance for weakly-supervised object localization},
  author={Zhang, Xiaolin and Wei, Yunchao and Kang, Guoliang and Yang, Yi and Huang, Thomas},
  booktitle={ECCV},
  year={2018}
}
@inproceedings{tan2019efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={ICML},
  year={2019}
}
@inproceedings{choe2020evaluating,
  title={Evaluating weakly supervised object localization methods right},
  author={Choe, Junsuk and Oh, Seong Joon and Lee, Seungho and Chun, Sanghyuk and Akata, Zeynep and Shim, Hyunjung},
  booktitle={CVPR},
  year={2020}
}
@article{wang2021minmaxcam,
  title={MinMaxCAM: Improving object coverage for CAM-basedWeakly Supervised Object Localization},
  author={Wang, Kaili and Oramas, Jose and Tuytelaars, Tinne},
  journal={arXiv preprint arXiv:2104.14375},
  year={2021}
}
@article{wu2021multi,
  title={Multi-Modal Answer Validation for Knowledge-Based VQA},
  author={Wu, Jialin and Lu, Jiasen and Sabharwal, Ashish and Mottaghi, Roozbeh},
  journal={arXiv preprint arXiv:2103.12248},
  year={2021}
}
@inproceedings{rennie2017self,
  title={Self-critical sequence training for image captioning},
  author={Rennie, Steven J and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
  booktitle={CVPR},
  year={2017}
}
@inproceedings{gan2020large,
  title={Large-scale adversarial training for vision-and-language representation learning},
  author={Gan, Zhe and Chen, Yen-Chun and Li, Linjie and Zhu, Chen and Cheng, Yu and Liu, Jingjing},
  booktitle={NeurIPS},
  year={2020}
}
@article{fu2017advances,
  title={Advances in deep learning approaches for image tagging},
  author={Fu, Jianlong and Rui, Yong},
  journal={APSIPA Transactions on Signal and Information Processing},
  volume={6},
  year={2017},
  publisher={Cambridge University Press}
}
@article{chen2015microsoft,
  title={Microsoft coco captions: Data collection and evaluation server},
  author={Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  journal={arXiv preprint arXiv:1504.00325},
  year={2015}
}
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}@inproceedings{dancette2021beyond,
  title={Beyond question-based biases: Assessing multimodal shortcut learning in visual question answering},
  author={Dancette, Corentin and Cadene, Remi and Teney, Damien and Cord, Matthieu},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1574--1583},
  year={2021}
}
@article{wang2022git,
  title={GIT: A Generative Image-to-text Transformer for Vision and Language},
  author={Wang, Jianfeng and Yang, Zhengyuan and Hu, Xiaowei and Li, Linjie and Lin, Kevin and Gan, Zhe and Liu, Zicheng and Liu, Ce and Wang, Lijuan},
  journal={arXiv preprint arXiv:2205.14100},
  year={2022}
}
@article{schick2020exploiting,
  title={Exploiting cloze questions for few shot text classification and natural language inference},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2001.07676},
  year={2020}
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{gafni2022make,
  title={Make-a-scene: Scene-based text-to-image generation with human priors},
  author={Gafni, Oran and Polyak, Adam and Ashual, Oron and Sheynin, Shelly and Parikh, Devi and Taigman, Yaniv},
  journal={arXiv preprint arXiv:2203.13131},
  year={2022}
}

@article{li2020image,
  title={Image-to-image translation with text guidance},
  author={Li, Bowen and Qi, Xiaojuan and Torr, Philip HS and Lukasiewicz, Thomas},
  journal={arXiv preprint arXiv:2002.05235},
  year={2020}
}

@inproceedings{DETRRes50,
  title={DETR Res50 Checkpoint from the Official DETR Repo.},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth},
  year={2020}
}

@inproceedings{pont2020connecting,
  title={Connecting vision and language with localized narratives},
  author={Pont-Tuset, Jordi and Uijlings, Jasper and Changpinyo, Soravit and Soricut, Radu and Ferrari, Vittorio},
  booktitle={European conference on computer vision},
  pages={647--664},
  year={2020},
  organization={Springer}
}

@inproceedings{hong2018inferring,
  title={Inferring semantic layout for hierarchical text-to-image synthesis},
  author={Hong, Seunghoon and Yang, Dingdong and Choi, Jongwook and Lee, Honglak},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7986--7994},
  year={2018}
}

@inproceedings{li2019object,
  title={Object-driven text-to-image synthesis via adversarial training},
  author={Li, Wenbo and Zhang, Pengchuan and Zhang, Lei and Huang, Qiuyuan and He, Xiaodong and Lyu, Siwei and Gao, Jianfeng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12174--12182},
  year={2019}
}

@article{hinz2019generating,
  title={Generating multiple objects at spatially distinct locations},
  author={Hinz, Tobias and Heinrich, Stefan and Wermter, Stefan},
  journal={arXiv preprint arXiv:1901.00686},
  year={2019}
}

@inproceedings{koh2021text,
  title={Text-to-image generation grounded by fine-grained user attention},
  author={Koh, Jing Yu and Baldridge, Jason and Lee, Honglak and Yang, Yinfei},
  booktitle={WACV},
  pages={237--246},
  year={2021}
}

@inproceedings{pavllo2020controlling,
  title={Controlling style and semantics in weakly-supervised image generation},
  author={Pavllo, Dario and Lucchi, Aurelien and Hofmann, Thomas},
  booktitle={European conference on computer vision},
  pages={482--499},
  year={2020},
  organization={Springer}
}

@article{ding2022cogview2,
  title={CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers},
  author={Ding, Ming and Zheng, Wendi and Hong, Wenyi and Tang, Jie},
  journal={arXiv preprint arXiv:2204.14217},
  year={2022}
}

@inproceedings{reed2016generative,
  title={Generative adversarial text to image synthesis},
  author={Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},
  booktitle={International conference on machine learning},
  pages={1060--1069},
  year={2016},
  organization={PMLR}
}

@inproceedings{zhang2017stackgan,
  title={Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks},
  author={Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris N},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5907--5915},
  year={2017}
}

@article{zhang2018stackgan++,
  title={Stackgan++: Realistic image synthesis with stacked generative adversarial networks},
  author={Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris N},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={8},
  pages={1947--1962},
  year={2018},
  publisher={IEEE}
}

@inproceedings{xu2018attngan,
  title={Attngan: Fine-grained text to image generation with attentional generative adversarial networks},
  author={Xu, Tao and Zhang, Pengchuan and Huang, Qiuyuan and Zhang, Han and Gan, Zhe and Huang, Xiaolei and He, Xiaodong},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1316--1324},
  year={2018}
}

@article{ho2022classifier,
  title={Classifier-free diffusion guidance},
  author={Ho, Jonathan and Salimans, Tim},
  journal={arXiv preprint arXiv:2207.12598},
  year={2022}
}

@inproceedings{zhang2021cross,
  title={Cross-modal contrastive learning for text-to-image generation},
  author={Zhang, Han and Koh, Jing Yu and Baldridge, Jason and Lee, Honglak and Yang, Yinfei},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={833--842},
  year={2021}
}

@inproceedings{zhou2022towards,
  title={Towards Language-Free Training for Text-to-Image Generation},
  author={Zhou, Yufan and Zhang, Ruiyi and Chen, Changyou and Li, Chunyuan and Tensmeyer, Chris and Yu, Tong and Gu, Jiuxiang and Xu, Jinhui and Sun, Tong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={17907--17917},
  year={2022}
}

@article{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-3?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  journal={arXiv preprint arXiv:2101.06804},
  year={2021}
}
@inproceedings{marino2021krisp,
  title={Krisp: Integrating implicit and symbolic knowledge for open-domain knowledge-based vqa},
  author={Marino, Kenneth and Chen, Xinlei and Parikh, Devi and Gupta, Abhinav and Rohrbach, Marcus},
  booktitle={CVPR},
  pages={14111--14121},
  year={2021}
}
@inproceedings{garderes2020conceptbert,
  title={Conceptbert: Concept-aware representation for visual question answering},
  author={Garderes, Fran{\c{c}}ois and Ziaeefard, Maryam and Abeloos, Baptiste and Lecue, Freddy},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings},
  pages={489--498},
  year={2020}
}
@inproceedings{ben2017mutan,
  title={Mutan: Multimodal tucker fusion for visual question answering},
  author={Ben-Younes, Hedi and Cadene, R{\'e}mi and Cord, Matthieu and Thome, Nicolas},
  booktitle={ICCV},
  pages={2612--2620},
  year={2017}
}
@inproceedings{bosselut2021dynamic,
  title={Dynamic neuro-symbolic knowledge graph construction for zero-shot commonsense question answering},
  author={Bosselut, Antoine and Le Bras, Ronan and Choi, Yejin},
  booktitle={Proceedings of the 35th AAAI Conference on Artificial Intelligence (AAAI)},
  year={2021}
}
@article{zhu2020mucko,
  title={Mucko: multi-Layer cross-modal knowledge reasoning for fact-based visual question answering},
  author={Zhu, Zihao and Yu, Jing and Wang, Yujing and Sun, Yajing and Hu, Yue and Wu, Qi},
  journal={arXiv preprint arXiv:2006.09073},
  year={2020}
}
@inproceedings{speer2017conceptnet,
  title={Conceptnet 5.5: An open multilingual graph of general knowledge},
  author={Speer, Robyn and Chin, Joshua and Havasi, Catherine},
  booktitle={Thirty-first AAAI conference on artificial intelligence},
  year={2017}
}
@article{shin2020autoprompt,
  title={Autoprompt: Eliciting knowledge from language models with automatically generated prompts},
  author={Shin, Taylor and Razeghi, Yasaman and Logan IV, Robert L and Wallace, Eric and Singh, Sameer},
  journal={arXiv preprint arXiv:2010.15980},
  year={2020}
}
@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}
@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}
@article{liu2021gpt,
  title={GPT Understands, Too},
  author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2103.10385},
  year={2021}
}
@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}
@article{logan2021cutting,
  title={Cutting down on prompts and parameters: Simple few-shot learning with language models},
  author={Logan IV, Robert L and Bala{\v{z}}evi{\'c}, Ivana and Wallace, Eric and Petroni, Fabio and Singh, Sameer and Riedel, Sebastian},
  journal={arXiv preprint arXiv:2106.13353},
  year={2021}
}
@article{hu2021lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
@inproceedings{zhang2021vinvl,
  title={Vinvl: Revisiting visual representations in vision-language models},
  author={Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng},
  booktitle={CVPR},
  year={2021}
}
@inproceedings{PontTuset_eccv2020,
  author    = {Jordi Pont-Tuset and Jasper Uijlings and Soravit Changpinyo and Radu Soricut and Vittorio Ferrari},
  title     = {Connecting Vision and Language with Localized Narratives},
  booktitle = {ECCV},
  year      = {2020}
}
@article{chen2021evaluating,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Ponde, Henrique and Kaplan, Jared and Edwards, Harri and Burda, Yura and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}
@inproceedings{yang2021tap,
  title={TAP: Text-Aware Pre-training for Text-VQA and Text-Caption},
  author={Yang, Zhengyuan and Lu, Yijuan and Wang, Jianfeng and Yin, Xi and Florencio, Dinei and Wang, Lijuan and Zhang, Cha and Zhang, Lei and Luo, Jiebo},
  booktitle={CVPR},
  pages={8751--8761},
  year={2021}
}
@article{wu2019generating,
  title={Generating question relevant captions to aid visual question answering},
  author={Wu, Jialin and Hu, Zeyuan and Mooney, Raymond J},
  journal={arXiv preprint arXiv:1906.00513},
  year={2019}
}
@inproceedings{li2018vqa,
  title={Vqa-e: Explaining, elaborating, and enhancing your answers for visual questions},
  author={Li, Qing and Tao, Qingyi and Joty, Shafiq and Cai, Jianfei and Luo, Jiebo},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={552--567},
  year={2018}
}
@inproceedings{park2018multimodal,
  title={Multimodal explanations: Justifying decisions and pointing to the evidence},
  author={Park, Dong Huk and Hendricks, Lisa Anne and Akata, Zeynep and Rohrbach, Anna and Schiele, Bernt and Darrell, Trevor and Rohrbach, Marcus},
  booktitle={CVPR},
  pages={8779--8788},
  year={2018}
}
@inproceedings{zellers2019recognition,
  title={From recognition to cognition: Visual commonsense reasoning},
  author={Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={CVPR},
  pages={6720--6731},
  year={2019}
}
@article{wang2017fvqa,
  title={Fvqa: Fact-based visual question answering},
  author={Wang, Peng and Wu, Qi and Shen, Chunhua and Dick, Anthony and Van Den Hengel, Anton},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={10},
  pages={2413--2427},
  year={2017},
  publisher={IEEE}
}
@article{wang2015explicit,
  title={Explicit knowledge-based reasoning for visual question answering},
  author={Wang, Peng and Wu, Qi and Shen, Chunhua and Hengel, Anton van den and Dick, Anthony},
  journal={arXiv preprint arXiv:1511.02570},
  year={2015}
}
@inproceedings{narasimhan2018straight,
  title={Straight to the facts: Learning knowledge base retrieval for factual visual question answering},
  author={Narasimhan, Medhini and Schwing, Alexander G},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={451--468},
  year={2018}
}
@article{narasimhan2018out,
  title={Out of the box: Reasoning with graph convolution nets for factual visual question answering},
  author={Narasimhan, Medhini and Lazebnik, Svetlana and Schwing, Alexander G},
  journal={arXiv preprint arXiv:1811.00538},
  year={2018}
}
@inproceedings{li2020boosting,
  title={Boosting visual question answering with context-aware knowledge aggregation},
  author={Li, Guohao and Wang, Xin and Zhu, Wenwu},
  booktitle={Proceedings of the 28th ACM International Conference on Multimedia},
  pages={1227--1235},
  year={2020}
}
@inproceedings{zhu2015aligning,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={ICCV},
  pages={19--27},
  year={2015}
}

@article{petroni2019language,
  title={Language models as knowledge bases?},
  author={Petroni, Fabio and Rockt{\"a}schel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H and Riedel, Sebastian},
  journal={arXiv preprint arXiv:1909.01066},
  year={2019}
}

@article{ramesh2022hierarchical,
  title={Hierarchical text-conditional image generation with clip latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal={arXiv preprint arXiv:2204.06125},
  year={2022}
}

@article{saharia2022photorealistic,
  title={Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  author={Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S Sara and Lopes, Rapha Gontijo and others},
  journal={arXiv preprint arXiv:2205.11487},
  year={2022}
}

@article{yu2022scaling,
  title={Scaling Autoregressive Models for Content-Rich Text-to-Image Generation},
  author={Yu, Jiahui and Xu, Yuanzhong and Koh, Jing Yu and Luong, Thang and Baid, Gunjan and Wang, Zirui and Vasudevan, Vijay and Ku, Alexander and Yang, Yinfei and Ayan, Burcu Karagol and others},
  journal={Transactions on Machine Learning Research}
}

@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={8821--8831},
  year={2021},
  organization={PMLR}
}

@inproceedings{wang2022ofa,
  title={Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework},
  author={Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  booktitle={International Conference on Machine Learning},
  pages={23318--23340},
  year={2022},
  organization={PMLR}
}

@inproceedings{yang2022unitab,
  title={Unitab: Unifying text and box outputs for grounded vision-language modeling},
  author={Yang, Zhengyuan and Gan, Zhe and Wang, Jianfeng and Hu, Xiaowei and Ahmed, Faisal and Liu, Zicheng and Lu, Yumao and Wang, Lijuan},
  booktitle={European Conference on Computer Vision},
  pages={521--539},
  year={2022},
  organization={Springer}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10684--10695},
  year={2022}
}

@inproceedings{zhao2019image,
  title={Image generation from layout},
  author={Zhao, Bo and Meng, Lili and Yin, Weidong and Sigal, Leonid},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8584--8593},
  year={2019}
}

@inproceedings{sun2019image,
  title={Image synthesis from reconfigurable layout and style},
  author={Sun, Wei and Wu, Tianfu},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10531--10540},
  year={2019}
}

@inproceedings{li2020bachgan,
  title={Bachgan: High-resolution image synthesis from salient object layout},
  author={Li, Yandong and Cheng, Yu and Gan, Zhe and Yu, Licheng and Wang, Liqiang and Liu, Jingjing},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8365--8374},
  year={2020}
}

@inproceedings{li2021image,
  title={Image Synthesis from Layout with Locality-Aware Mask Adaption},
  author={Li, Zejian and Wu, Jingyu and Koh, Immanuel and Tang, Yongchuan and Sun, Lingyun},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={13819--13828},
  year={2021}
}

@inproceedings{frolov2021attrlostgan,
  title={AttrLostGAN: attribute controlled image synthesis from reconfigurable layout and style},
  author={Frolov, Stanislav and Sharma, Avneesh and Hees, J{\"o}rn and Karayil, Tushar and Raue, Federico and Dengel, Andreas},
  booktitle={DAGM German Conference on Pattern Recognition},
  pages={361--375},
  year={2021},
  organization={Springer}
}

@inproceedings{yang2022modeling,
  title={Modeling Image Composition for Complex Scene Generation},
  author={Yang, Zuopeng and Liu, Daqing and Wang, Chaoyue and Yang, Jie and Tao, Dacheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7764--7773},
  year={2022}
}

@article{cho2022dall,
  title={Dall-eval: Probing the reasoning skills and social biases of text-to-image generative transformers},
  author={Cho, Jaemin and Zala, Abhay and Bansal, Mohit},
  journal={arXiv preprint arXiv:2202.04053},
  year={2022}
}

@inproceedings{johnson2018image,
  title={Image generation from scene graphs},
  author={Johnson, Justin and Gupta, Agrim and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1219--1228},
  year={2018}
}

@inproceedings{schuhmann2022laion,
  title={LAION-5B: An open large-scale dataset for training next generation image-text models},
  author={Schuhmann, Christoph and Beaumont, Romain and Gordon, Cade W and Wightman, Ross and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Schramowski, Patrick and Kundurthy, Srivatsa R and Crowson, Katherine and others},
  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2022}
}

@inproceedings{zhou2022detecting,
  title={Detecting twenty-thousand classes using image-level supervision},
  author={Zhou, Xingyi and Girdhar, Rohit and Joulin, Armand and Kr{\"a}henb{\"u}hl, Philipp and Misra, Ishan},
  booktitle={European Conference on Computer Vision},
  pages={350--368},
  year={2022},
  organization={Springer}
}

@inproceedings{huang2022multimodal,
  title={Multimodal conditional image synthesis with product-of-experts gans},
  author={Huang, Xun and Mallya, Arun and Wang, Ting-Chun and Liu, Ming-Yu},
  booktitle={European Conference on Computer Vision},
  pages={91--109},
  year={2022},
  organization={Springer}
}

@article{salimans2016improved,
  title={Improved techniques for training gans},
  author={Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{heusel2017gans,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{sylvain2021object,
  title={Object-centric image generation from layouts},
  author={Sylvain, Tristan and Zhang, Pengchuan and Bengio, Yoshua and Hjelm, R Devon and Sharma, Shikhar},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2021}
}

@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={International Conference on Medical image computing and computer-assisted intervention},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@inproceedings{parmar2022aliased,
  title={On aliased resizing and surprising subtleties in gan evaluation},
  author={Parmar, Gaurav and Zhang, Richard and Zhu, Jun-Yan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11410--11420},
  year={2022}
}

@article{fan2022frido,
  title={Frido: Feature Pyramid Diffusion for Complex Scene Image Synthesis},
  author={Fan, Wan-Cyuan and Chen, Yen-Chun and Chen, DongDong and Cheng, Yu and Yuan, Lu and Wang, Yu-Chiang Frank},
  journal={arXiv preprint arXiv:2208.13753},
  year={2022}
}

@article{yuan2021florence,
  title={Florence: A new foundation model for computer vision},
  author={Yuan, Lu and Chen, Dongdong and Chen, Yi-Ling and Codella, Noel and Dai, Xiyang and Gao, Jianfeng and Hu, Houdong and Huang, Xuedong and Li, Boxin and Li, Chunyuan and others},
  journal={arXiv preprint arXiv:2111.11432},
  year={2021}
}

@article{liu2022pseudo,
  title={Pseudo numerical methods for diffusion models on manifolds},
  author={Liu, Luping and Ren, Yi and Lin, Zhijie and Zhao, Zhou},
  journal={arXiv preprint arXiv:2202.09778},
  year={2022}
}

@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={arXiv preprint arXiv:2205.11916},
  year={2022}
}

@article{nakano2021webgpt,
  title={Webgpt: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}

@inproceedings{huang2022language,
  title={Language models as zero-shot planners: Extracting actionable knowledge for embodied agents},
  author={Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle={International Conference on Machine Learning},
  pages={9118--9147},
  year={2022},
  organization={PMLR}
}

@article{ahn2022can,
  title={Do as i can, not as i say: Grounding language in robotic affordances},
  author={Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and others},
  journal={arXiv preprint arXiv:2204.01691},
  year={2022}
}

@article{yao2022react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2022}
}

@article{gao2022pal,
  title={PAL: Program-aided Language Models},
  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  journal={arXiv preprint arXiv:2211.10435},
  year={2022}
}

@article{trivedi2022interleaving,
  title={Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions},
  author={Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2212.10509},
  year={2022}
}

@article{schick2023toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={arXiv preprint arXiv:2302.04761},
  year={2023}
}

@inproceedings{yang2022empirical,
  title={An empirical study of gpt-3 for few-shot knowledge-based vqa},
  author={Yang, Zhengyuan and Gan, Zhe and Wang, Jianfeng and Hu, Xiaowei and Lu, Yumao and Liu, Zicheng and Wang, Lijuan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={3},
  pages={3081--3089},
  year={2022}
}

@inproceedings{driess2023palme,
    title={PaLM-E: An Embodied Multimodal Language Model},
    author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
    booktitle={arXiv preprint arXiv:2303.03378},
    year={2023}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and others},
  journal={arXiv preprint arXiv:2204.14198},
  year={2022}
}

@article{yu2022coca,
  title={Coca: Contrastive captioners are image-text foundation models},
  author={Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
  journal={arXiv preprint arXiv:2205.01917},
  year={2022}
}

@article{wu2023visual,
  title={Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models},
  author={Wu, Chenfei and Yin, Shengming and Qi, Weizhen and Wang, Xiaodong and Tang, Zecheng and Duan, Nan},
  journal={arXiv preprint arXiv:2303.04671},
  year={2023}
}

@article{zeng2022socratic,
  title={Socratic models: Composing zero-shot multimodal reasoning with language},
  author={Zeng, Andy and Wong, Adrian and Welker, Stefan and Choromanski, Krzysztof and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas and Lee, Johnny and Vanhoucke, Vincent and others},
  journal={arXiv preprint arXiv:2204.00598},
  year={2022}
}

@article{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal={arXiv preprint arXiv:2301.12597},
  year={2023}
}

@article{huang2023language,
  title={Language Is Not All You Need: Aligning Perception with Language Models},
  author={Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Liu, Qiang and others},
  journal={arXiv preprint arXiv:2302.14045},
  year={2023}
}

@article{hu2022promptcap,
  title={PromptCap: Prompt-Guided Task-Aware Image Captioning},
  author={Hu, Yushi and Hua, Hang and Yang, Zhengyuan and Shi, Weijia and Smith, Noah A and Luo, Jiebo},
  journal={arXiv preprint arXiv:2211.09699},
  year={2022}
}

@inproceedings{wanglanguage,
  title={Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners},
  author={Wang, Zhenhailong and Li, Manling and Xu, Ruochen and Zhou, Luowei and Lei, Jie and Lin, Xudong and Wang, Shuohang and Yang, Ziyi and Zhu, Chenguang and Hoiem, Derek and others},
  booktitle={Advances in Neural Information Processing Systems}
}

@article{zhang2023multimodal,
  title={Multimodal chain-of-thought reasoning in language models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
  journal={arXiv preprint arXiv:2302.00923},
  year={2023}
}

@article{lu2022learn,
  title={Learn to explain: Multimodal reasoning via thought chains for science question answering},
  author={Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  journal={arXiv preprint arXiv:2209.09513},
  year={2022}
}

@article{vemprala2023chatgpt,
  title={ChatGPT for Robotics: Design Principles and Model Abilities},
  author={Vemprala, Sai and Bonatti, Rogerio and Bucker, Arthur and Kapoor, Ashish},
  year={2023}
}

@misc{langchain,
  title={LangChain},
  author={Chase, Harrison},
  howpublished ={\url{https://langchain.readthedocs.io/}},
  year={2023}
}

@article{gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  year={2023}
}

@article{suris2023vipergpt,
  title={ViperGPT: Visual Inference via Python Execution for Reasoning},
  author={Surs, Ddac and Menon, Sachit and Vondrick, Carl},
  journal={arXiv preprint arXiv:2303.08128},
  year={2023}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{dehghani2023scaling,
  title={Scaling vision transformers to 22 billion parameters},
  author={Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas and Caron, Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and others},
  journal={arXiv preprint arXiv:2302.05442},
  year={2023}
}

@inproceedings{liu2015deep,
  title={Deep learning face attributes in the wild},
  author={Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={3730--3738},
  year={2015}
}

@inproceedings{huiskes2008mir,
  title={The mir flickr retrieval evaluation},
  author={Huiskes, Mark J and Lew, Michael S},
  booktitle={Proceedings of the 1st ACM international conference on Multimedia information retrieval},
  pages={39--43},
  year={2008}
}

@inproceedings{chua2009nus,
  title={Nus-wide: a real-world web image database from national university of singapore},
  author={Chua, Tat-Seng and Tang, Jinhui and Hong, Richang and Li, Haojie and Luo, Zhiping and Zheng, Yantao},
  booktitle={Proceedings of the ACM international conference on image and video retrieval},
  pages={1--9},
  year={2009}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{wu2022grit,
  title={GRiT: A Generative Region-to-text Transformer for Object Understanding},
  author={Wu, Jialian and Wang, Jianfeng and Yang, Zhengyuan and Gan, Zhe and Liu, Zicheng and Yuan, Junsong and Wang, Lijuan},
  journal={arXiv preprint arXiv:2212.00280},
  year={2022}
}