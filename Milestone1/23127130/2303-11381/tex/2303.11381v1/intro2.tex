\section{Introduction}
Recent years have seen significant advancement for computer vision, thanks to 
improved network architecture~\cite{he2016deep,vaswani2017attention,dosovitskiy2020image,carion2020end}, large-scale model training~\cite{yuan2021florence,dehghani2023scaling,wang2022git}, and other factors.
However, different vision problems typically require different models, 
which often require manual selection and composition of individual models for each use case.
For example, when determining if an image contains ``people'', we may choose the image tagging model~\cite{huiskes2008mir,chua2009nus,lin2014microsoft} and check if the predicted tag list contains \textit{``people''}. 
If \textit{``people''} exists, we may select the celebrity model~\cite{liu2015deep} to further understand 
whether a celebrity appears and who he/she is.

One research direction is to combine the vision and language modules as one end-to-end model,
such as Flamingo~\cite{alayrac2022flamingo}, PaLM-E~\cite{driess2023palme},
to provide a dialogue-based experience to the end user. 
That is, the user can use natural language to interact with the model around the image content.
The vision module encodes vision signals into special text tokens or features that the language module can understand, enabling the system to utilize the language module for understanding user queries and providing responses.
However, these joint finetuning approaches require a large amount of computing resources and annotated data to enable specific capabilities.
In this work, we aim to combine existing individual vision models with the language model in a more flexible manner to tackle complicated visual understanding problems, \eg, the ones illustrated in Figure~\ref{fig:teaser}. 

Large language models (LLMs)~\cite{brown2020language,chowdhery2022palm}, such as ChatGPT, have shown impressive dialogue capability with text as both input and output. Recent NLP research~\cite{yao2022react,gao2022pal,trivedi2022interleaving,schick2023toolformer} (\eg, \textsc{ReAct}~\cite{yao2022react}) demonstrates the effectiveness of integrating external NLP tools, such as search engines and math calculators, with LLMs by proper instruction. 
Specifically, \textsc{ReAct}~\cite{yao2022react} prompts an LLM to generate \textit{reasoning} texts that break down complex problems into intermediate steps, and \textit{action} texts that allocate NLP tools for solving these steps.
One example is that the LLM can suggest a text query to a modern search engine to grab the latest internet information, and return the user with the information that is not in the pre-training corpus. 
Inspired by the efficacy of reasoning and acting with LLMs and NLP tools, we explore the integration of vision expert tools with LLMs.

To this end, we present \modelname, a system paradigm that composes numerous vision experts with ChatGPT for multimodal reasoning and action.
To enable images and videos as inputs, we use their file path as the input to ChatGPT.
The file path functions as a placeholder, allowing ChatGPT to treat it as a black box.
Whenever a specific property such as celebrity names or box coordinates is required, ChatGPT is expected to seek help from a specific vision expert to identify the desired information. 
To inject the knowledge of vision experts' usages into ChatGPT, we add instructions to ChatGPT prompts about each expert's capability, input argument type, and output type, along with a few in-context examples for each expert. 
Additionally, a special watchword is instructed such that we can use regex expression matching to invoke the expert accordingly.

We show \modelname's representative visual understanding capabilities in Figure~\ref{fig:teaser}. For example, \modelname~could associate information from multiple uploaded receipts and calculate the total
travel cost (``Multi-Image Reasoning''), recognize and answer questions about the ``morel mushrooms'' (``Open-World Concept Understanding''), and condense a long video into representative thumbnails (``Video Summarization and Event Localization''). These visual intelligence features are similar to those found in recent models, such as multimodal GPT-4~\cite{gpt4} and PaLM-E~\cite{driess2023palme}, but achieved via prompting instead of additional multimodal training. The \modelname~system may provide extra flexibility in module upgrades, and may be effective in certain visual understanding tasks by better utilizing existing specialized vision experts, such as celebrity recognition and dense captioning.