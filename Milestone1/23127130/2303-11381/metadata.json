{
    "arxiv_id": "2303.11381",
    "paper_title": "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action",
    "authors": [
        "Zhengyuan Yang",
        "Linjie Li",
        "Jianfeng Wang",
        "Kevin Lin",
        "Ehsan Azarnasab",
        "Faisal Ahmed",
        "Zicheng Liu",
        "Ce Liu",
        "Michael Zeng",
        "Lijuan Wang"
    ],
    "submission_date": "2023-03-20",
    "revised_dates": [
        "2023-03-22"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.CL",
        "cs.LG"
    ],
    "abstract": "We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action. In this paper, we define and explore a comprehensive list of advanced vision tasks that are intriguing to solve, but may exceed the capabilities of existing vision and vision-language models. To achieve such advanced visual intelligence, MM-REACT introduces a textual prompt design that can represent text descriptions, textualized spatial coordinates, and aligned file names for dense visual signals such as images and videos. MM-REACT's prompt design allows language models to accept, associate, and process multimodal information, thereby facilitating the synergetic combination of ChatGPT and various vision experts. Zero-shot experiments demonstrate MM-REACT's effectiveness in addressing the specified capabilities of interests and its wide application in different scenarios that require advanced visual understanding. Furthermore, we discuss and compare MM-REACT's system paradigm with an alternative approach that extends language models for multimodal scenarios through joint finetuning. Code, demo, video, and visualization are available at https://multimodal-react.github.io/",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.11381v1"
    ],
    "publication_venue": null
}