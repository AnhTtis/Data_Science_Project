\section{Experiments}
\subsection{Datasets}

{\noindent \textbf{YoucookII}} contains $2,000$ long untrimmed videos from $89$ cooking recipes, including $1,333$ videos for training, $457$ videos for testing. The average length of videos is $5.3$ minutes and each video is divided into $7.7$ video clips on average with the manually labeled caption sentences.

{\noindent \textbf{ActivityNet Captions}} is a large-scale challenging dataset for video captioning, with $10,024$ videos for training, $4,926$ for validation, and $5,044$ for testing. The average length of each video is $2.1$ minutes and is divided into $3.65$ clips on average. Each clip is assigned with a manually labeled caption sentence. Since the testing set is not publicly available, following MART \cite{Mart}, we split the validation set into two subsets, \ie, ae-val with $2,460$ videos for validation and ae-test with $2,457$ videos for testing. 

{\noindent \textbf{MSR-VTT}} includes $10,000$ video clips with $15$ seconds on average. Each video clip contains $20$ human annotated captions and the average length of each sentence is $9.28$ words. Following \cite{HMN,MGRMP,SGN}, $6,513$, $497$, and $2,990$ videos are used for training, validation, and testing, respectively. 

{\noindent \textbf{MSVD}} consists of $1970$ video clips collected from YouTube and each clip is approximately $10$ seconds long. Each clip is labeled with $40$ sentences. Following \cite{HMN,MGRMP,SGN}, we split the dataset into $1,200$, $100$, and $670$ video clips for training, validation and testing, respectively.

\subsection{Evaluation metrics}
To compare the performance of the proposed method with other methods, five model-free automatic evaluation metrics are used, including BLEU@4 \cite{BLEU} (abbreviated to B) that are precision-based metric, METEOR \cite{METEOR} (abbreviated to M) that computes sentence-level scores, CIDEr \cite{CIDEr} (abbreviated to C) that is consensus-based metric, ROUGE \cite{ROUGE} (abbreviated to R) that uses longest common subsequence to compute the similarities between sentences, and Rep@4 \cite{Re} (abbreviated to Rep) that computes the redundancy score for every captions. Lower Rep indicates better model. The CIDEr score is used as the primary metric in evaluation. Notably, both micro-level and paragraph-level evaluation modes are used. The micro-level mode evaluates the predicted captions of each video clip independently, while the paragraph-level mode considers the relations between captions of different clips. Following \cite{Mart,COOT}, the paragraph-level evaluation mode is used in the YouCookII and ActivityNet datasets, which contains multiple clips in each video.

\subsection{Implementation Details}
We sample every $2$ frames per second in each video and use the pre-trained models to extract video features. For the YouCookII and ActivityNet Captions datasets, similar to \cite{Mart}, the ResNet-200 and BN-Inception models are used to extract the appearance and motion features, respectively. Following \cite{HMN,ORG-TRL,SAAT}, the InceptionResNetV2 and C3D models are used to extract the appearance and motion features for the MSR-VTT and MSVD dataset, respectively. Meanwhile, the Faster R-CNN method using ResNet-101 as the backbone is adopted to detect object in each video frame, which is trained on the Visual Genome dataset \cite{VG}. Notably, only $N_{\text{r}}=6$ classes of the detected objects with top scores are used to form region features in each frame. 

The caption sentences, speech transcripts, and retrieved knowledge in knowledge graph are split into individual words using the NLTK toolkit\footnote{https://github.com/nltk/nltk}, and the GloVe \cite{Glove} algorithm is used to extract the word embeddings for each word to construct the transcript, caption, and knowledge tokens. The dimension of word embeddings is set to $300$. The captions are truncated to $20$ words and the maximum text length of transcripts is set to $300$ words. For each detected object, we retrieve up to $N_{k}=5$ pieces of knowledge items in knowledge graph depending on the relevant scores.

Notably, the proposed method is implemented using Pytorch. $3$ blocks of multi-modality self-attention and cross-attention modules are used in the MSR-VTT \cite{DBLP:conf/cvpr/XuMYR16} and MSVD \cite{DBLP:conf/acl/ChenD11} datasets, and $2$ are used in the YouCookII \cite{DBLP:conf/aaai/ZhouXC18} and ActivityNet Captions \cite{DBLP:conf/iccv/KrishnaHRFN17} datasets, to form the two-stream transformer, respectively. The feature dimension in each block is set to $768$, and the number of heads in multi-head architecture is set to $12$. The proposed method is trained using the Adam algorithm \cite{DBLP:journals/corr/KingmaB14} with the initial learning rate of $1e-4$. The batch size in training phase is set to $6$. The hyper-parameters $\omega_{1}$, $\omega_{2}$, $\lambda_1$, and $\lambda_2$ are set to $0.8$, $0.2$, $0.5$ and $0.5$, empirically. 


\subsection{Comparison to the State of the Art Methods}
{\noindent \textbf{YouCookII dataset}.} To validate the effectiveness of the proposed method, we compare it to the state-of-the-art (SOTA) methods using the paragraph-level evaluation mode, reported in Table \ref{tab-youcookii-paragraph}. As shown in Table \ref{tab-youcookii-paragraph}, our TextKG method achieves the best results on the YouCookII dataset. Specifically, TextKG improves $18.7\%$ absolute CIDEr score compared to the SOTA method COOT \cite{COOT}. COOT \cite{COOT} focuses on leveraging the hierarchy information and modeling the interactions between different levels granularity and different modalities, such as frames and words, clip and sentences, and videos and paragraphs, which is pre-trained on the large-scale HowTo100M \cite{howto100} dataset. In contrast, our TextKG method aims to exploit additional knowledge in knowledge graph and multi-modality information in videos to improve the caption results.

Beside the paragraph-level evaluation mode, we also report the evaluation results based on the micro-level mode on the YouCookII dataset in Table \ref{tab-youcookii-micro}. As shown in Table \ref{tab-youcookii-micro}, TextKG significantly outperforms other methods, including Univl \cite{univl} and AT \cite{AT}, both of which use transcript information to enhance model performance by directly concatenating visual and transcript embedding. We argue that with the use of knowledge graphs, TextKG gives a better understanding of the content included in the speech transcripts and thus yields favorable results. 


\begin{table}[t]
\centering
\setlength{\tabcolsep}{9.5pt}
\small{
\begin{tabular}{c|cccc}
\toprule
Method &B &M &C &Rep \\
\midrule
Van-Trans \cite{MaskedTrans} & 7.6 & 15.7 & 32.3 & 7.8 \\
Trans-XL \cite{Transformer-XL} & 6.6 & 14.8 & 26.4 & 6.3 \\
Trans-XLRG \cite{Transformer-XLRG} & 6.6 & 14.7 & 25.9 & 6.0 \\
MART \cite{Mart} & 8.0 & 16.0 & 35.7 & 4.4 \\
Van-Trans+COOT \cite{COOT} & 11.1 & 19.8 & 55.6 & 5.7\\
COOT \cite{COOT} & 11.3 & 19.9 & 57.2 & 6.7 \\ \hline
TextKG & \textbf{14.0} & \textbf{22.1} & \textbf{75.9} & \textbf{2.8} \\ \hline
Human & - & - & - & 1.0 \\
\bottomrule
\end{tabular}}
\caption{Evaluation results on the YouCookII val subset in the paragraph-level evaluation mode.}
\label{tab-youcookii-paragraph}
\vspace{-4mm}
\end{table}


\begin{table}[t]
\centering
\small{
\begin{tabular}{c|c|ccccc}
\toprule
Method & Input &B &M &R &C \\ \hline
Masked Trans \cite{MaskedTrans} & V & 3.8 & 11.6 & 27.4 & 38 \\
S3D \cite{S3D} & V & 3.2 & 9.5 & 26.1 & 31 \\
VideoAsMT \cite{VideoASMT} & V & 5.3 & 13.4 & - & - \\
SwinBERT \cite{Swinbert} & V & 9 & 15.6 & 37.3 & 109 \\
{\color[HTML]{C0C0C0}VideoBERT \cite{VideoBERT}}  & {\color[HTML]{C0C0C0}V} & {\color[HTML]{C0C0C0}4.0} & {\color[HTML]{C0C0C0}11.0} & {\color[HTML]{C0C0C0}27.5} & {\color[HTML]{C0C0C0}49} \\
{\color[HTML]{C0C0C0}VideoBERT+S3D \cite{VideoBERT}} & {\color[HTML]{C0C0C0}V} & {\color[HTML]{C0C0C0}4.3} & {\color[HTML]{C0C0C0}11.9} & {\color[HTML]{C0C0C0}28.8} & {\color[HTML]{C0C0C0}50} \\
{\color[HTML]{C0C0C0}ActBERT \cite{Actbert}} & {\color[HTML]{C0C0C0}V} & {\color[HTML]{C0C0C0}5.4} & {\color[HTML]{C0C0C0}14.3} & {\color[HTML]{C0C0C0}30.6} & {\color[HTML]{C0C0C0}65} \\
AT \cite{AT}& V+S & 9.0 & 17.8 & 36.7 & 112 \\
DPC \cite{DPC}& V+S & 2.8 & 18.1 & - & - \\
{\color[HTML]{C0C0C0}VALUE \cite{value}} & {\color[HTML]{C0C0C0} V+S} & {\color[HTML]{C0C0C0} 12.4} & {\color[HTML]{C0C0C0} 18.8} & {\color[HTML]{C0C0C0} 40.4} & {\color[HTML]{C0C0C0} 130} \\
Univl \cite{univl} & V+S & 9.5 & 16.3 & 37.4 & 115 \\
{\color[HTML]{C0C0C0} Univl \cite{univl}}& {\color[HTML]{C0C0C0} V+S} & {\color[HTML]{C0C0C0} 17.4} & {\color[HTML]{C0C0C0} 22.4} & {\color[HTML]{C0C0C0} 46.5} &{\color[HTML]{C0C0C0}  181} \\ 
MV-GPT \cite{MVC}& V+S & \textbf{13.3} & 17.6  & 35.5 & 103 \\ 
{\color[HTML]{C0C0C0} MV-GPT \cite{MVC}}& {\color[HTML]{C0C0C0} V+S} &{\color[HTML]{C0C0C0}  21.9} & {\color[HTML]{C0C0C0} 27.1} &{\color[HTML]{C0C0C0}  49.4} &{\color[HTML]{C0C0C0}  221} \\ \hline
TextKG & V+S & 11.7 &\textbf{18.4} &\textbf{40.2}&\textbf{133} \\
\bottomrule
\end{tabular}}
\caption{Evaluation results on the YouCookII val subset in the micro-level evaluation mode. ‘V’ indicates the methods use video appearance information, and ‘S’ indicates the methods use the speech information. We gray out models that pre-training on large-scale datasets for a fair comparison.}
\label{tab-youcookii-micro}
\end{table}

{\noindent \textbf{ActivityNet Caption dataset.}} We also evaluate our TextKG method on the challenging ActivityNet Caption dataset in Table \ref{tab-activity-paragraph}. As shown in Table \ref{tab-activity-paragraph}, TextKG achieves the best results on $3$ out of $4$ metrics, \ie, BLEU, METOR, and CIDEr. The ActivityNet Caption dataset \cite{DBLP:conf/iccv/KrishnaHRFN17} contains a series of complex events with annotated caption sentences describing the events that occur. These events may occur over various periods of time and may co-occur. In contrast to existing methods \cite{HMN,MGRMP,SGN,SAAT}, our TextKG understands complex events more comprehensively with the help of exploiting information from transcripts and integrating the relevant knowledge in knowledge graph to enhance the common-sense information for video captioning. 


\begin{table}[t]
\centering
\small{
\setlength{\tabcolsep}{11.0pt}
\begin{tabular}{c|cccc}
\toprule
Method &B &M &C & Rep \\
\midrule
HSE \cite{HSE} & 9.8 & 13.8 & 18.8 & 13.2 \\
GVD \cite{GVD} & 11.0 & 15.7 & 22.0 & 8.8 \\
GVDsup \cite{GVD} & \textbf{11.3} & 16.4 & 22.9 & 7.0 \\
Van-Trans \cite{MaskedTrans} & 9.8 & 15.6 & 22.2 & 7.8 \\
Trans-XL \cite{Transformer-XL} & 10.4 & 15.1 & 21.7 & 8.5 \\
Trans-XLRG \cite{Transformer-XLRG} & 10.2 & 14.8 & 20.4 & 8.9 \\
MART \cite{Mart} & 10.3 & 15.7 & 23.4 & \textbf{5.2} \\ \hline
TextKG & \textbf{11.3} & \textbf{16.5} & \textbf{26.6} & 6.3 \\
\bottomrule
\end{tabular}}
\caption{Evaluation results on the Activity-Net Captions \textit{ae-val} subset in the paragraph-level evaluation mode.}
\label{tab-activity-paragraph}
\vspace{-4mm}
\end{table}


{\noindent \textbf{MSR-VTT dataset}.} The evaluation results on the MSR-VTT dataset are reported in Table \ref{tab-comparison-msrvtt-micro}. For the fair comparison, we use the same visual features as the existing methods \cite{HMN, MGRMP,ORG-TRL,SAAT}. In contrast to existing methods focusing on network architecture design to exploit visual information, our TextKG aims to exploit mutli-modality information in external knowledge graph and original videos, achieving the state-of-the-art performance on $3$ out of $4$ metrics, \ie, BLEU, METEOR, and CIDEr.  

Meanwhile, we also compare the TextKG method to the SOTA methods focusing on multi-modality pretraining models in Table \ref{tab-comparison-mrsvtt-micro}. As shown in Table \ref{tab-comparison-mrsvtt-micro}, our TextKG method performs favorably against existing methods, such as DECEM \cite{DECEM}, UniVL \cite{univl} and MV-GPT \cite{MVC}, that are pretrained on the large-scale datasets (\eg, Howto100M \cite{howto100}), \ie, improve $0.7$ absolute CIDEr score compare to the SOTA method LAVENDER \cite{lavender}. 


\begin{table}[t]
\centering
\setlength{\tabcolsep}{11.0pt}
\small{
\begin{tabular}{c|cccc}
\toprule
Method &B &M &R &C \\ \hline
OA-BTG \cite{OA-BTG} & 41.4 & 28.2 & - & 46.9 \\
POS-CG \cite{POS-CG} & 42 & 28.2 & 61.6 & 48.7 \\
MGSA \cite{MGSA} & 42.4 & 27.6 & - & 47.5 \\
STG-KD \cite{STG-KD} & 40.5 & 28.3 & 60.9 & 47.1 \\
ORG-TRL \cite{ORG-TRL} & 43.6 & 28.8 & 62.1 & 50.9 \\
SGN \cite{SGN} & 40.8 & 28.3 & 60.8 & 49.5 \\
MGRMP \cite{MGRMP} & 41.7 & 28.9 & 62.1 & 51.4 \\
HMN \cite{HMN} & 43.5 & 29 & \textbf{62.7} & 51.5 \\ \hline
TextKG & \textbf{43.7} & \textbf{29.6} & 62.4 & \textbf{52.4}  \\
\bottomrule
\end{tabular}}
\caption{Evaluation results on the MSR-VTT test subset in the micro-level evaluation mode.}
\label{tab-comparison-msrvtt-micro}
\vspace{-3mm}
\end{table}


\begin{table}[t]
\small
\centering
\setlength{\tabcolsep}{3.0pt}
\small{
\begin{tabular}{c|c|c|cccc}
\toprule
Method & Input & Features &B &M &R &C \\ \hline
SWINBERT \cite{Swinbert} & V & VidSwin & 45.4 & 30.6 & 64.1 & 55.9 \\
CLIP4C \cite{Clip4caption} & V & CLIP & 46.1 & 30.7 & 63.7 & 57.7 \\
CMVC \cite{cmvc} & V & CLIP & \textbf{48.2} & \textbf{31.3} & \textbf{64.8} & 58.7 \\ 
{\color[HTML]{C0C0C0}LAVENDER \cite{lavender}} & {\color[HTML]{C0C0C0}V} & {\color[HTML]{C0C0C0}VidSwin} & {\color[HTML]{C0C0C0}-} & {\color[HTML]{C0C0C0}-} & {\color[HTML]{C0C0C0}-} & {\color[HTML]{C0C0C0}60.1} \\
{\color[HTML]{C0C0C0}DeCEM \cite{DECEM}} & {\color[HTML]{C0C0C0}V+S} & {\color[HTML]{C0C0C0}BERT} & {\color[HTML]{C0C0C0}45.2} & {\color[HTML]{C0C0C0}29.7} & {\color[HTML]{C0C0C0}64.7} & {\color[HTML]{C0C0C0}52.3} \\
{\color[HTML]{C0C0C0}UniVL \cite{univl}} & {\color[HTML]{C0C0C0}V+S} & {\color[HTML]{C0C0C0}S3D} & {\color[HTML]{C0C0C0}41.8} & {\color[HTML]{C0C0C0}28.9} & {\color[HTML]{C0C0C0}60.8} & {\color[HTML]{C0C0C0}50.0} \\
{\color[HTML]{C0C0C0}MV-GPT \cite{MVC}} & {\color[HTML]{C0C0C0}V+S} & {\color[HTML]{C0C0C0}ViViT} & {\color[HTML]{C0C0C0}48.9$^*$} & {\color[HTML]{C0C0C0}38.7$^*$} & {\color[HTML]{C0C0C0}64.0} & {\color[HTML]{C0C0C0}60.0}\\ \hline
TextKG (CLIP) & V+S & CLIP & 46.6 & 30.5 & \textbf{64.8} & \textbf{60.8} \\ 
\bottomrule
\end{tabular}}
\caption{Comparison to the methods focusing on model pretraining on the MSR-VTT dataset in the micro-level evaluation mode. Notably, following CLIP4C, we use the pre-trained CLIP model on LAION-400M \cite{DBLP:journals/corr/abs-2111-02114} to extract video appearance features. $\ast$ The authors use a different library to compute BLEU and METOR. Thus, the results on BLEU and METOR are not directly comparable to other methods.}
\label{tab-comparison-mrsvtt-micro}
\end{table}


{\noindent \textbf{MSVD dataset}.} We also evaluate the proposed TextKG method on the MSVD dataset \cite{DBLP:conf/acl/ChenD11} that doesn't have speech and repoprt the results in Table \ref{tab-comparison-msvd-micro}. As shown in Table \ref{tab-comparison-msvd-micro}, our TextKG achieves the best results without speech transcripts by improving $1.2\%$ and $3.8\%$ CIDEr scores compared to the SOTA method HMN \cite{HMN} and the JCRR method \cite{bad} exploiting knowledge graph to model the relations between objects, demonstrating the effectiveness of the knowledge graph usage in our method for video captioning. 

\begin{table}[t]
\centering
\setlength{\tabcolsep}{11.0pt}
\small{
\begin{tabular}{c|cccc}
\toprule
Method &B &M &R &C \\ \hline
OA-BTG \cite{OA-BTG} & 56.9 & 36.2 & - & 90.6 \\
POS-CG \cite{POS-CG} & 52.5 & 34.1 & 71.3 & 88.7 \\
MGSA \cite{MGSA} & 53.4 & 35 & - & 86.7 \\
STG-KD \cite{STG-KD} & 52.2 & 36.9 & 73.9 & 93.0 \\
ORG-TRL \cite{ORG-TRL} & 54.3 & 36.4 & 73.9 & 95.2 \\
SGN \cite{SGN} & 52.8 & 35.5 & 72.9 & 94.3 \\
MGRMP \cite{MGRMP} & 55.8 & 36.9 & 74.5 & 98.5 \\
JCRR \cite{bad} & 57.0 & 36.8 & - & 96.8 \\
HMN \cite{HMN} & 59.2 & 37.7 & \textbf{75.1} & 104.0 \\ \hline
TextKG & \textbf{60.8} & \textbf{38.5} & \textbf{75.1} & \textbf{105.2}  \\
\bottomrule
\end{tabular}}
\caption{Evaluation results on the MSVD test subset in the micro-level evaluation mode.}
\label{tab-comparison-msvd-micro}
\end{table}

 
\begin{table}[h]
\centering
\setlength{\tabcolsep}{3.5pt}
\small{
\begin{tabular}{ccc|ccc|cccc}
\hline
\begin{tabular}[c]{@{}c@{}}V-F\end{tabular} & \begin{tabular}[c]{@{}c@{}}R-F\end{tabular} & Text & 
\begin{tabular}[c]{@{}c@{}}G-KG\end{tabular} & 
\begin{tabular}[c]{@{}c@{}}S-KG\end{tabular} & \begin{tabular}[c]{@{}c@{}}K-S\end{tabular} &B &M &C &Rep \\ \hline
\checkmark & & & & & & 7.4 & 15.7 & 32.1 & 4.1 \\
\checkmark & \checkmark & & & & & 9.5 & 17.7 & 45.9 & 5.2 \\
\checkmark & \checkmark & & \checkmark & & \checkmark & 9.7 & 17.8 & 48.9 & 4.2 \\
\checkmark & \checkmark & & & \checkmark & \checkmark & 9.7 & 18.0 & 48.5 & 3.3 \\
\checkmark & \checkmark & & \checkmark & \checkmark & \checkmark & 9.6 & 17.7 & 49.8 & 5.5 \\\hline
\checkmark & & \checkmark & & & & 13.0 & 21.2 & 62.5 & 2.7 \\
\checkmark & \checkmark & \checkmark & & & & 13.9 & 22.1 & 71.3 & 2.9 \\
\checkmark & \checkmark & \checkmark & \checkmark & & \checkmark & 13.7 & 22.0 & 73.5 & 2.0 \\
\checkmark & \checkmark & \checkmark & & \checkmark & \checkmark & 13.5 & 21.9 & 74.8 & 2.1\\
\checkmark & \checkmark & \checkmark & \checkmark & \checkmark &  & 13.7 & 21.8 & 72.0 & 2.8\\
\checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & {\bf 14.0} & {\bf 22.1} & {\bf 75.9} & {\bf 2.8} \\
\hline
\end{tabular}}
\caption{Ablation study on the YouCookII val subset in the paragraph-level evaluation mode. ``V-F'' and ``R-F'' indicate the video and region features, ``Text'' indicates the speech transcripts features, ``G-KG'' and ``S-KG'' indicate general and specific knowledge graph, and ``K-S'' indicates the knowledge selection mechanism.}
\vspace{-5mm}
\label{tab-ablation-youcookii-paragraph}

\end{table}

\begin{figure*}[t]
 \centering
 \includegraphics[trim=42 210 90 40, clip, width=0.90\linewidth]{figures/Figure3.pdf}
 \caption{Qualitative results of the proposed method on the YouCookII dataset. The information in the speech transcripts and knowledge graph is not considered in the baseline.}
 \label{qualitative-results}
\end{figure*}

\subsection{Ablation study}
{\noindent {\bf Influence of different modules.}}
To validate the effectiveness of different modules in TextKG, we conduct several experiments on the YouCookII dataset in Table \ref{tab-ablation-youcookii-paragraph}. As shown in Table \ref{tab-ablation-youcookii-paragraph}, the baseline method considers the information of video features and predicted captions, achieving $32.1$ CIDEr score. After integrating the speech transcripts, the CIDEr score is improved to $62.5$, which ensures the key information of videos is considered in generating video captions. After integrating the region features, TextKG achieves $71.3$ CIDEr score. The CIDEr score can be further improved $4.6$ CIDEr score to $75.9$ by exploiting additional knowledge in knowledge graph. Notably, even without the information of speech transcripts, the knowledge graphs are still crucial for video captioning, \ie, improving $3.9$ CIDEr score ($49.8$ {\em vs.} $45.9$). Meanwhile, we also demonstrate the effectiveness of knowledge selection mechanism  in Table \ref{tab-ablation-youcookii-paragraph}. As shown in Table \ref{tab-ablation-youcookii-paragraph}, without the knowledge selection mechanism, the CIDEr score significantly drops $3.9$ to $72.0$. We believe that the noisy knowledge is harmful to the accuracy of video captions. 

{\noindent {\bf Influence of transcript quality.}}
To explore the effect of transcript quality on the knowledge graph, we add some noises to the speech transcripts (\ie, mask some amount of speech transcripts) and evaluate the performance of our models on the YouCookII dataset in Figure \ref{influence-transcripts}. We find that the method using both the general and specific knowledge graphs performs favorable against other methods with different degrees of masked speech transcripts. Meanwhile, as the amount of masked speech transcripts increasing, the accuracy of the method using specific knowledge graph drops sharply, while the accuracy of the method using general knowledge graph drops at a much slower pace. The aforementioned results demonstrate the importance of general knowledge graph for the video captioning task.

\begin{figure}[t]
\centering
 \includegraphics[trim=20 0 40 30, clip, width=0.95\linewidth]{figures/masking_ratio.pdf}
 \caption{Influence of the transcripts qualities. ``w/o KG'' indicates that the method does not use the general and specific knowledge graphs. ``with G-KG'' and ``with S-KG'' indicate that only the general and specific knowledge graphs are used, respectively. ``with G- and S-KG'' indicates that both the general and specific knowledge graphs are used. The CIDEr differences between the ``with G-KG'', ``with S-KG'', ``with G- and S-KG'' methods and the ``w/o KG'' method are shown at the bottom corner.}
 \label{influence-transcripts}
 \vspace{-4mm}
\end{figure}

\subsection{Qualitative Results}
We present the qualitative results of the proposed method on the YouCookII dataset in Figure \ref{qualitative-results}, including the key frames, detected salient objects, speech transcripts, retrieved knowledge items, caption results predicted by our model, ground-truth captions, and key words distributions in the training set. As shown in Figure \ref{qualitative-results}, we observe that the transcript contains key information in the video, \eg, egg, salt, pepper, com starch and flour, to ensure the quality of the generated video captions. However, the model with only the speech transcripts fails to predict the long-tail phrases, such as sesame oil and black pepper. With the help of knowledge graph providing the relations among these phrases, \eg, sesame oil, black pepper and corn starch, our TextKG model can predict the phrases more accurately. This indicates that the knowledge graph is crucial to mitigate the long-tail word challenges in video captioning.
