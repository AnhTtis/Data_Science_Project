\begin{figure*}[t]
 \centering
 \includegraphics[trim=53 126 175 40, clip, width=0.95\linewidth]{figures/Figure1.pdf}
 \caption{The architecture of our TextKG method, which is formed the external and internal streams. Each stream is stacked with $N$ sets of multi-modality self-attention and cross-attention modules.The cross attention modules are designed to align tokens in different modalities.}
 \label{fig:framework}
\vspace{-3mm} 
\end{figure*}

\section{Our Approach}
As mentioned above, our method aims to integrate additional knowledge using knowledge graph and exploits the multi-modality information in videos to mitigate the long-tail words challenge in the field of video captioning. We design a text with knowledge graph augmented transformer, \ie, a two-stream transformer formed by the external and internal streams. Both streams are constructed by $N$ sets of alternately stacked self-attention and cross-attention blocks. Specifically, we first use a detector to generate the salient object regions in each video frame, and use the automatic speech recognition (ASR) method \cite{asr} to generate the speech transcripts in videos. After that, the class labels of detected salient objects are used to retrieve the prior knowledge in knowledge graphs. Then, the appearance embeddings of detected salient objects and video frames, and embeddings of retrieved prior knowledge, speech transcripts, and predicted captions are fed into the two-stream transformer to generate subsequent words of the predicted captions. The overall architecture of the proposed TextKG is shown in Figure \ref{fig:framework}.


\subsection{Two-Stream Transformer}
As described above, our two-stream transformer is formed by the external stream and internal stream. Several self-attention and cross-attention blocks are interleaved to construct the two streams.

Let $\{ {\it y}_{1}, {\it y}_{2}, \cdots, {\it y}_{l} \}$ be the predicted captions, where ${\it y}_{i}$ is the index of the $i$-th word in the dictionary, $\{{\it p}_{1}, {\it p}_{2}, \cdots, {\it p}_{l}\}$ to be the predicted probabilities of words in the dictionary, where ${\it p}_{i}$ is the probability of the $i$-th word. The index of the $i$-th word ${\it y}_{i}$ is computed as ${\it y}_{i}=\mathop{\arg\max}_{i}{\it p}_{i}$. Meanwhile, let $z^{\text{ext}}_{i}$ and $z^{\text{int}}_{i}$ to be the output probabilities of the external and internal streams of the $i$-th word. Thus, we have ${\it p}_{i} = \omega_1 z^{\text{ext}}_{i} + \omega_2 z^{\text{int}}_i$, where $\omega_1$ and $\omega_2$ are the hyperparameters used to balance the outputs of the two-streams. In this way, ${\it y}_{i}$ is computed as 
\begin{equation}
    \begin{array}{ll}
{\it y}_{i}=\mathop{\arg\max}_{i}(\omega_1 z^{\text{ext}}_{i} + \omega_2 z^{\text{int}}_i). 
    \end{array}
\end{equation}
In the following sections, we will describe each module in our two-stream transformer in more details.


{\noindent {\bf Multi-modality Self-Attention Module}.}
As shown in Figure \ref{fig:framework}, we use the self-attention module \cite{DBLP:conf/nips/VaswaniSPUJGKP17} in both the external and internal streams to model the interactions among multi-modality information. Specifically, for the external stream, the concatenation of feature embeddings of detected objects ${\cal F}_{\text{r}}$, retrieved prior knowledge ${\cal F}_{\text{k}}$, speech transcripts ${\cal F}_{\text{s}}$, and predicted video captions ${\cal F}_{\text{c}}$ are fed into the self-attention module to model the interactions, \ie, ${\cal X}_{\text{ext}}=\textnormal{Concat}({\cal F}_{\text{r}}, {\cal F}_{\text{k}}, {\cal F}_{\text{s}}, {\cal F}_{\text{c}})$. Meanwhile, for the internal stream, the concatenation of feature embeddings of speech transcripts ${\cal F}_{\text{s}}$, predicted video captions ${\cal F}_{\text{c}}$, and video frame ${\cal F}_{\text{v}}$ are fed into the self-attention model to exploit the interactions, \ie, ${\cal X}_{\text{int}}=\textnormal{Concat}({\cal F}_{\text{s}}, {\cal F}_{\text{c}}, {\cal F}_{\text{v}})$. The self-attention module computes the interactions as follows,
\begin{equation}
\begin{array}{ll}
\Phi(Q,K,V)={\textnormal{softmax}}({QK^T \over \sqrt{d}} + {\it M})V,
\end{array}
\label{equ:self-attention}
\end{equation}
where $d$ is the feature dimension of queries $Q$ and keys $K$. We have $Q={\cal X}_{(\cdot)}{\it W}^{\textnormal{Q}}$, $K={\cal X}_{(\cdot)}{\it W}^{\textnormal{K}}$, $V={\cal X}_{(\cdot)}{\it W}^{\textnormal{V}}$, where ${\it W}^{\textnormal{Q}}$, ${\it W}^{\textnormal{K}}$, and ${\it W}^{\textnormal{V}}$ are learnable parameters, and ${\cal X}_{(\cdot)}\in\{{\cal X}_{\text{ext}}, {\cal X}_{\text{int}}\}$ are the concatenated feature embeddings for the external and internal streams, respectively. 

Following \cite{Mart}, we introduce a mask matrix ${\it M}$ in attention function \eqref{equ:self-attention} of both the external and internal streams to prevent the model from seeing future words. That is, for the $i$-th caption token, we set ${\it M}_{i, j} = 0$ for $j=1,\cdots, i$, and set ${\it M}_{i, j}=-\inf$ for $j>i$. Meanwhile, for the external stream, we set ${\it M}_{i, j}=-\inf$ to prevent the associations between irrelevant retrieved prior knowledge and the detected salient object regions. 


{\noindent {\bf Multi-modality Cross-Attention Module.}}
Besides self-attention module, we use the cross-attention module to align the interactions modeled by both the external and internal streams. Specifically, we compute the affinity matrix to guide the alignments between the modeled interactions by injecting the information. Similar to the self-attention module, we also introduce the mask matrix into the attention function to compute the cross-attention. Intuitively, the retrieved prior knowledge should not have direct effect on the predicted captions, but through the detected salient objects. Thus, we set the corresponding elements to $-\inf$ to prevent the retrieved prior knowledge directly influencing the predicted captions. 

\subsection{Optimization}
The cross entropy loss function is used to guide the training of our TextKG method. Given the ground-truth indices of previous $(i-1)$ words and the concatenated of two-streams ${\cal X}_{\text{ext}}$ and ${\cal X}_{\text{int}}$, we can get the predictions of the current $i$-th word. After that, the training loss of our method is computed as 
\begin{equation}
    \begin{array}{ll}
    {\cal L}=-\sum_{i=1}^{l}(\lambda_1\log{\it z}^{\text{ext}}_{i}+
 \lambda_2\log{\it z}^{\text{int}}_{i}),
    \end{array}
\end{equation}
where ${\it z}^{\text{ext}}_{i}$ and ${\it z}^{\text{int}}_{i}$ are the $i$-th output word of the external and internal streams, $\lambda_1$ and $\lambda_2$ are the preset parameters used to balance the two streams, and $l$ is the total length of predicted captions. The Adam optimization algorithm \cite{DBLP:journals/corr/KingmaB14} with an initial learning rate of $1e-4$, $\beta_1=0.9$, $\beta_2=0.999$ and $L2$ weight decay of $0.01$ is used to train the model. The Warmup strategy is adopted in training phase by linearly increasing the learning rate from $0$ to the initial learning rate for the first $10\%$ of training epochs, and linearly decreasing to $0$ for the remaining $90\%$ of epochs.

 
\subsection{Multimodal Tokens}
The appearance embeddings of detected salient objects and video frames, and embeddings of retrieved prior, speech transcripts, and predicted captions are fed into the two-stream transformer to generate subsequent captions after tokenization. We will describe the feature extraction and tokenization processes of the aforementioned embeddings. 


{\noindent \textbf{Video tokens}} encode the appearance and motion information of video frames. Specifically, the aligned appearance and motion features form the video tokens. In each second of the video, we sample $2$ frames to extract features. Following \cite{Mart}, we use the ``Flatten-673'' layer in ResNet-200 \cite{resnet} to extract appearance features, and the ``global pool'' layer in BN-Inception \cite{BN} to extract the motion features on the YouCookII \cite{DBLP:conf/aaai/ZhouXC18} and ActivityNet Captions \cite{DBLP:conf/iccv/KrishnaHRFN17} datasets. Meanwhile, for fair comparison, we use the InceptionResNetV2 \cite{incev2} and C3D \cite{c3d} models to extract appearance and motion features on the MSR-VTT \cite{DBLP:conf/cvpr/XuMYR16} and MSVD \cite{DBLP:conf/acl/ChenD11} datasets, following the methods \cite{HMN,ORG-TRL,SAAT}.


{\noindent \textbf{Region tokens}} are used to describe the visual information of the detected salient objects. We use the Faster R-CNN method \cite{fasterrcnn} to extract visual features for detected object regions, which is pretrained on the Visual Genome dataset \cite{VG}, and the ${\it N}_{\text{r}}$ detected objects with highest confidence score in each frame are reserved for video captioning. 


{\noindent \textbf{Transcript tokens}} are used to encode the content of speech transcripts, which contains some key information in videos. The GloVe model \cite{Glove} is used to extract features of each word in transcripts to form the transcript tokens.  


{\noindent \textbf{Caption tokens}} encode the information of predicted captions. Similar to transcript tokens, GloVe \cite{Glove} is adopted to extract features of each word in the generated captions. 


{\noindent \textbf{Knowledge tokens}} model the content of retrieved knowledge, which is important to mitigate the long-tail words challenge in video captioning. Our TextKG method uses a triple structure to encode the retrieved knowledge, formed by two object items and the relations between them, \ie, $(\alpha_{\text{head}}, \alpha_{\text{tail}}, \rho_{\text{rel}})$, where $\alpha_{\text{head}}$ and $\alpha_{\text{tail}}$ are the two nodes in knowledge graph, and $\rho_{\text{rel}}$ indicates the edge encoding the relations between them. For example, $\alpha_{\text{head}}=$``\texttt{knife}'', $\alpha_{\text{tail}}=$``\texttt{hard}'', and $\rho_{\text{rel}}=$``\texttt{has property}''. For each detected object $\alpha_{\text{head}}$, we aim to integrate prior context information for more accurate caption results. That is, we use the GloVe model to extract linguistic features of the node $\alpha_{\text{tail}}$ and set the learnable embeddings for the edge $\rho_{\text{rel}}$. After that, we compute the summation of them to get the knowledge embeddings of the knowledge tokens.  

For each detected object, we use its category name to retrieve knowledge (\ie, triples containing the category name of the detected object) from the knowledge graph. Notably, only a portion of the retrieved knowledge are highly related to the video. The rests are potentially harmful to the performance of caption generation as it may distract the training and inference and also increase computational cost. For example, to generate caption of a cooking tutorial video, the knowledge ``\texttt{knife - used to -cut}'' is more useful rather than ``\texttt{knife - has property - hard}''. Because cutting food may be an important procedure for food preparation. Thus, we design a primary rank mechanism to sort the retrieved knowledge based on their semantic similarity to the video. The cosine similarity between the emebeddings of knowledge tokens and video transcripts is computed as the semantic similarity for ranking. The pretrained SBERT \cite{sbert} model is used to extract feature embeddings of transcripts. Finally, ${\it N}_{\text{k}}$ knowledge items with the highest similarity scores are reserved for video captioning. 


\subsection{Knowledge Graphs Construction}
As mentioned above, we use the knowledge graph to retain the prior knowledge. The nodes in the graph could corresponding to a noun, an adjective, a verb or an adverb, etc. The edges in the graph are used to encode the relations between different nodes. Apparently, it is important to include all key information of videos in knowledge graphs for accurate caption results. Thus, besides general knowledge graph covering most key knowledge in general scenarios, we also introduce the specific knowledge graph for each dataset covering the key knowledge in specific scenarios. 


{\noindent \textbf{General knowledge graph}} is designed to include most key information in general scenarios that we are interested, such as cooking  and activity. Specifically, the general knowledge graph is exported from the public available giant knowledge graph ConceptNet \cite{ConceptNet} by extracting key words\footnote{The key words are exported from the speech transcripts and annotated ground-truth captions in the training sets of the caption datasets.} in ConceptNet with the connected edges and neighboring nodes. 


{\noindent \textbf{Specific knowledge graph.}} Besides the general knowledge graph, we also construct the specific knowledge graph to cover key information in specific scenarios. We believe that the speech transcripts of videos contain most of the crucial information and use the speech transcripts as the source to construct the specific knowledge graph. We first use the automatic speech recognition (ASR) model \cite{asr} to convert the speech in all videos to transcripts. After that, we use the Stanford NLP library\footnote{https://stanfordnlp.github.io/CoreNLP/} to analyze the components in each sentence in transcripts to form a structured grammer tree. We collect the ``adjective and noun'', ``noun and noun'' and ``adverb and verb'' phrases from the grammar trees to construct the specific knowledge graph. 




