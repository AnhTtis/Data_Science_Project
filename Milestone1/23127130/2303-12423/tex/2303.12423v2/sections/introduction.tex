\section{Introduction}
Video captioning aims to generate a complete and natural sentence to describe video content, which attracts much attention in recent years. Generally, most existing methods \cite{HMN,SGN,Mart,DPC} require a large amount of paired video and description data for model training. Several datasets, such as YouCookII \cite{DBLP:conf/aaai/ZhouXC18}, and ActivityNet Captions \cite{DBLP:conf/iccv/KrishnaHRFN17} are constructed to promote the development of video captioning field. Meanwhile, some methods \cite{MVC,univl,Actbert,DECEM} also use the large-scale narrated video dataset HowTo100M \cite{DBLP:conf/iccv/MiechZATLS19} to pretrain the captioning model to further improve the accuracy. 

Although significant progress has been witnessed, it is still a challenge for video captioning methods to be applied in real applications, mainly due to the long-tail issues of words. Most existing methods \cite{DECEM,MVC,Actbert,univl} attempt to design powerful neural networks, trained on the large-scale video-text datasets to make the network learn the relations between video appearances and descriptions. However, it is pretty tough for the networks to accurately predict the objects, properties, or behaviors that are infrequently or never appearing in training data. Some methods \cite{image,bad} attempt to use knowledge graph to exploit the relations between objects for long-tail challenge in image or video captioning, which produces promising results. 

In this paper, we present a text with knowledge graph augmented transformer (TextKG), which integrates additional knowledge in knowledge graph and exploits the multi-modality information in videos to mitigate the long-tail words challenge. TextKG is a two-stream transformer, formed by the external stream and internal stream. The external stream is used to absorb additional knowledge to help mitigate long-tail words challenge by modeling the interactions between the additional knowledge in pre-built knowledge graph, and the built-in information of videos, such as the salient object regions in each frame, speech transcripts, and video captions. Specifically, the information is first retrieved from the pre-built knowledge graphs based on the detected salient objects. 
After that, we combine the features of the retrieved information, the appearance features of detected salient objects,  the features of speech transcripts and captions, then feed them into the external stream of TextKG to model the interactions. The internal stream is designed to exploit the multi-modality information in videos, such as the appearance of video frames, speech transcripts and video captions, which can ensure the quality of caption results. To share information between two streams, the cross attention mechanism is introduced. In this way, the two streams can obtain the required modal information from each other for generating more accurate results. The architecture of the proposed method is shown in Figure \ref{fig:framework}.


Several experiments conducted on four challenging datasets, \ie, YouCookII \cite{DBLP:conf/aaai/ZhouXC18}, ActivityNet Captions \cite{DBLP:conf/iccv/KrishnaHRFN17}, MSR-VTT \cite{DBLP:conf/cvpr/XuMYR16}, and MSVD \cite{DBLP:conf/acl/ChenD11} demonstrate that the proposed method performs favorably against the state-of-the-art methods. Notably, our TextKG method outperforms the best published results by improving $18.7\%$ and  $3.2\%$ absolute CIDEr scores in the paragraph-level evalution mode on the YouCookII and Activity-Net Captions datasets.