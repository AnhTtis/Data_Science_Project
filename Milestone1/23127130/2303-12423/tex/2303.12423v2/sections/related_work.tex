\section{Related Work}
{\noindent {\bf Video captioning}} attracts much attention of researchers in recent years. The best practice has been achieved by attention-based methods, which attempts to associate visual components with sentences in videos. Some of them focus on designing powerful network architectures. VLM \cite{VLM} and VideoBERT \cite{VideoBERT} take the visual and text modalities as input, and use a shared transformer to construct a task-agnostic video-language model. ViLBERT \cite{Vilbert} processes visual and linguistic information separately with two parallel streams, and then use the attention mechanism to model the interactions between visual and language features. Instead of using the separate encoder-decoder architecture, MART \cite{Mart} designs a shared encoder-decoder network and augments it with the memory module. ActBert \cite{Actbert} uses local regional features to learn better visual-language alignment. WLT \cite{wlt} takes audio features as an additional input, and uses context fusion to generate multimodal features. 

Meanwhile, some methods \cite{image,bad,tkg, BEIC, ICiek, DST} focus on exploiting prior knowledge to provide semantic correlations and constraints between objects for image or video captioning, producing promising results. ORG-TRL \cite{ORG-TRL} uses the knowledge information in the language model (BERT) to provide
candidate words for video captioning. In contrast, we propose a two-stream transformer for video captioning, with the internal stream used to exploit multi-modality information in videos, and the external stream used to model the interactions between the additional knowledge and the built-in information of videos. These two streams use the cross-attention mechanism to share information in different modalities for generating more accurate results. 


{\noindent {\bf Vision-and-language representation learning}} is a hot topic in recent years. ViLBERT \cite{Vilbert}, LXMERT \cite{LXMERT}, UNITER \cite{UNITER}, UNIMO \cite{UNIMO} and Unified-VL \cite{Unified-VL} learn the representations between image and text, while Univl \cite{univl}, VideoBERT \cite{VideoBERT}, ActBERT \cite{Actbert} and MV-GPT \cite{MVC} learn the representations between videos and transcripts. Notably, most of these methods attempt to learn powerful vision-and-language representations by pre-training the models on the large-scale datasets, \eg, Howto100M \cite{howto100} and WebVid-2M \cite{Frozen}, and then finetune them on downstream tasks such as video captioning, video-text retrieval and visual question answering. In contrast, our TextKG method uses the speech transcripts as the text to model the visual and linguistic representations and integrate the additional knowledge in knowledge graph to mitigate long-tail words challenge in video captioning.


{\noindent {\bf Knowledge graph in NLP.}}
Knowledge graph is an useful tool to indicate the real-world entities and their relations, which provides rich structured knowledge facts for language modeling. Large-scale knowledge graphs are used to train knowledge enhanced language models for various natural language processing (NLP) tasks. CoLAKE \cite{CoLake} proposes to inject the knowledge context of an entity, and to jointly learn the contextualized representation for both language and knowledge by a unified structure. ERNIE \cite{ERNIE} enhances BERT architecture to better integrate the knowledge information and textual information. KEPLER \cite{KEPLER} not only improves language models by integrating factual knowledge, but also generates text-enhanced knowledge representation. JAKET \cite{JAKET} proposes a joint pre-training framework to model knowledge graph and language simultaneously. Inspired by CoLAKE, our method jointly learns the representations of vision, language and knowledge, and enhances the joint visual-language representations by retrieving relevant knowledge in knowledge graphs.