\section{Conclusion}
In this paper, we present a text with knowledge graph augmented transformer for video captioning, which aims to integrate external knowledge in knowledge graph and exploit the multi-modality information in video to mitigate long-tail words challenge. Externsive experiments conducted on four challenging datasets demonstrate the effectiveness of the proposed method. 

In the future, we plan to improve the proposed method in two directions, \ie, (1) optimizing the knowledge retrieve strategy by considering semantic context information of detected objects and corresponding actions in videos; (2) constructing multi-modal knowledge graph (\eg, the nodes in knowledge graph formed by text, speech, and images or videos) to improve the accuracy of video captioning. 