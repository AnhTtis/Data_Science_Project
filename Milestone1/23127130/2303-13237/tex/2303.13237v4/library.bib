@article{Huser2019,
   abstract = {Many environmental processes exhibit weakening spatial dependence as events become more extreme. Well-known limiting models, such as max-stable or generalized Pareto processes, cannot capture this, which can lead to a preference for models that exhibit a property known as asymptotic independence. However, weakening dependence does not automatically imply asymptotic independence, and whether the process is truly asymptotically (in)dependent is usually far from clear. The distinction is key as it can have a large impact upon extrapolation, that is, the estimated probabilities of events more extreme than those observed. In this work, we present a single spatial model that is able to capture both dependence classes in a parsimonious manner, and with a smooth transition between the two cases. The model covers a wide range of possibilities from asymptotic independence through to complete dependence, and permits weakening dependence of extremes even under asymptotic dependence. Censored likelihood-based inference for the implied copula is feasible in moderate dimensions due to closed-form margins. The model is applied to oceanographic datasets with ambiguous true limiting dependence structure. Supplementary materials for this article are available online.},
   author = {Raphaël Huser and Jennifer L. Wadsworth},
   doi = {10.1080/01621459.2017.1411813},
   issn = {1537274X},
   issue = {525},
   journal = {Journal of the American Statistical Association},
   keywords = {Asymptotic dependence and independence,Censored likelihood inference,Copula,Spatial extremes,Threshold exceedance},
   pages = {434-444},
   title = {Modeling Spatial Processes with Unknown Extremal Dependence Class},
   volume = {114},
   year = {2019},
}
@article{Heffernan2004,
   abstract = {Multivariate extreme value theory and methods concern the characterization, estimation and extrapolation of the joint tail of the distribution of a d-dimensional random variable. Existing approaches are based on limiting arguments in which all components of the variable become large at the same rate. This limit approach is inappropriate when the extreme values of all the variables are unlikely to occur together or when interest is in regions of the support of the joint distribution where only a subset of components is extreme. In practice this restricts existing methods to applications where d is typically 2 or 3. Under an assumption about the asymptotic form of the joint distribution of a d-dimensional random variable conditional on its having an extreme component, we develop an entirely new semiparametric approach which overcomes these existing restrictions and can be applied to problems of any dimension. We demonstrate the performance of our approach and its advantages over existing methods by using theoretical examples and simulation studies. The approach is used to analyse air pollution data and reveals complex extremal dependence behaviour that is consistent with scientific understanding of the process. We find that the dependence structure exhibits marked seasonality, with extremal dependence between some pollutants being significantly greater than the dependence at non-extreme levels.},
   author = {Janet E. Heffernan and Jonathan A. Tawn},
   doi = {10.1111/j.1467-9868.2004.02050.x},
   issn = {13697412},
   issue = {3},
   journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
   keywords = {Air pollution,Asymptotic independence,Bootstrap,Conditional distribution,Gaussian estimation,Multivariate extreme value theory,Semiparametric modelling},
   pages = {497-546},
   title = {A conditional approach for multivariate extreme values},
   volume = {66},
   year = {2004},
}
@article{Engelke2020a,
   abstract = {Conditional independence, graphical models and sparsity are key notions for parsimonious statistical models and for understanding the structural relationships in the data. The theory of multivariate and spatial extremes describes the risk of rare events through asymptotically justified limit models such as max-stable and multivariate Pareto distributions. Statistical modelling in this field has been limited to moderate dimensions so far, partly owing to complicated likelihoods and a lack of understanding of the underlying probabilistic structures. We introduce a general theory of conditional independence for multivariate Pareto distributions that enables the definition of graphical models and sparsity for extremes. A Hammersley–Clifford theorem links this new notion to the factorization of densities of extreme value models on graphs. For the popular class of Hüsler–Reiss distributions we show that, similarly to the Gaussian case, the sparsity pattern of a general extremal graphical model can be read off from suitable inverse covariance matrices. New parametric models can be built in a modular way and statistical inference can be simplified to lower dimensional marginals. We discuss learning of minimum spanning trees and model selection for extremal graph structures, and we illustrate their use with an application to flood risk assessment on the Danube river.},
   author = {Sebastian Engelke and Adrien S. Hitz},
   doi = {10.1111/rssb.12355},
   issn = {14679868},
   issue = {4},
   journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
   keywords = {Conditional independence,Extreme value theory,Graphical models,Multivariate Pareto distribution,Sparsity},
   pages = {871-932},
   title = {Graphical models for extremes},
   volume = {82},
   year = {2020},
}
@article{Castro-Camilo2020,
   abstract = {To disentangle the complex nonstationary dependence structure of precipitation extremes over the entire contiguous United States (U.S.), we propose a flexible local approach based on factor copula models. Our subasymptotic spatial modeling framework yields nontrivial tail dependence structures, with a weakening dependence strength as events become more extreme; a feature commonly observed with precipitation data but not accounted for in classical asymptotic extreme-value models. To estimate the local extremal behavior, we fit the proposed model in small regional neighborhoods to high threshold exceedances, under the assumption of local stationarity, which allows us to gain in flexibility. By adopting a local censored likelihood approach, we make inference on a fine spatial grid, and we perform local estimation by taking advantage of distributed computing resources and the embarrassingly parallel nature of this estimation procedure. The local model is efficiently fitted at all grid points, and uncertainty is measured using a block bootstrap procedure. We carry out an extensive simulation study to show that our approach can adequately capture complex, nonstationary dependencies, in addition, our study of U.S. winter precipitation data reveals interesting differences in local tail structures over space, which has important implications on regional risk assessment of extreme precipitation events. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
   author = {Daniela Castro-Camilo and Raphaël Huser},
   doi = {10.1080/01621459.2019.1647842},
   issn = {1537274X},
   issue = {531},
   journal = {Journal of the American Statistical Association},
   keywords = {Factor copula model,Local likelihood,Nonstationarity,Spatial extremes,Threshold exceedance},
   pages = {1037-1054},
   publisher = {Taylor & Francis},
   title = {Local Likelihood Estimation of Complex Tail Dependence Structures, Applied to U.S. Precipitation Extremes},
   volume = {115},
   url = {https://doi.org/10.1080/01621459.2019.1647842},
   year = {2020},
}
@article{Eastoe2009,
   abstract = {Statistical methods for modelling extremes of stationary sequences have received much attention. The most common method is to model the rate and size of exceedances of some high constant threshold; the size of exceedances is modelled by using a generalized Pareto distribution. Frequently, data sets display non-stationarity; this is especially common in environmental applications. The ozone data set that is presented here is an example of such a data set. Surface level ozone levels display complex seasonal patterns and trends due to the mechanisms that are involved in ozone formation. The standard methods of modelling the extremes of a non-stationary process focus on retaining a constant threshold but using covariate models in the rate and generalized Pareto distribution parameters. We suggest an alternative approach that uses preprocessing methods to model the non-stationarity in the body of the process and then uses standard methods to model the extremes of the preprocessed data. We illustrate both the standard and the preprocessing methods by using a simulation study and a study of the ozone data. We suggest that the preprocessing method gives a model that better incorporates the underlying mechanisms that generate the process, produces a simpler and more efficient fit and allows easier computation. © 2009 Royal Statistical Society.},
   author = {Emma F. Eastoe and Jonathan A. Tawn},
   doi = {10.1111/j.1467-9876.2008.00638.x},
   issn = {00359254},
   issue = {1},
   journal = {Journal of the Royal Statistical Society. Series C: Applied Statistics},
   keywords = {Generalized Pareto distribution,Non-stationary process,Ozone,Preprocessing,Return levels,Threshold exceedances},
   pages = {25-45},
   title = {Modelling non-stationary extremes with application to surface level ozone},
   volume = {58},
   year = {2009},
}
@article{DeFondeville2020,
   abstract = {Peaks-over-threshold analysis using the generalized Pareto distribution is widely applied in modelling tails of univariate random variables, but much information may be lost when complex extreme events are studied using univariate results. In this paper, we extend peaks-over-threshold analysis to extremes of functional data. Threshold exceedances defined using a functional $r$ are modelled by the generalized $r$-Pareto process, a functional generalization of the generalized Pareto distribution that covers the three classical regimes for the decay of tail probabilities. This process is the only possible limit for the distribution of $r$-exceedances of a properly rescaled process. We give construction rules, simulation algorithms and inference procedures for generalized $r$-Pareto processes, discuss model validation, and use the new methodology to study extreme European windstorms and heavy spatial rainfall.},
   author = {Raphaël de Fondeville and Anthony C. Davison},
   keywords = {functional regular variation,pareto process,peaks-over-threshold analysis,r -,rainfall,spatial statistics,statistics of extremes,windstorm},
   pages = {1-68},
   title = {Functional Peaks-over-threshold Analysis},
   url = {http://arxiv.org/abs/2002.02711},
   year = {2020},
}
@article{Goncalves2011,
   author = {Sílvia Gonçalves and Dimitris Politis},
   doi = {10.1016/j.jkss.2011.07.003},
   issn = {12263192},
   issue = {4},
   journal = {Journal of the Korean Statistical Society},
   pages = {383-386},
   publisher = {Elsevier B.V.},
   title = {Discussion: Bootstrap methods for dependent data: A review},
   volume = {40},
   url = {http://dx.doi.org/10.1016/j.jkss.2011.07.003},
   year = {2011},
}
@article{Eastoe2012,
   abstract = {A standard approach to model the extreme values of a stationary process is the peaks over threshold method, which consists of imposing a high threshold, identifying clusters of exceedances of this threshold and fitting the maximum value from each cluster using the generalized Pareto distribution. This approach is strongly justified by underlying asymptotic theory. We propose an alternative model for the distribution of the cluster maxima that accounts for the subasymptotic theory of extremes of a stationary process. This new distribution is a product of two terms, one for the marginal distribution of exceedances and the other for the dependence structure of the exceedance values within a cluster. We illustrate the improvement in fit, measured by the root mean square error of the estimated quantiles, offered by the new distribution over the peaks over thresholds analysis using simulated and hydrological data, and we suggest a diagnostic tool to help identify when the proposed model is likely to lead to an improved fit. © 2011 Biometrika Trust.},
   author = {Emma F. Eastoe and Jonathan A. Tawn},
   doi = {10.1093/biomet/asr078},
   issn = {00063444},
   issue = {1},
   journal = {Biometrika},
   keywords = {Cluster maxima,Extremal index,Generalized Pareto distribution,L-moment,Peaks over thresholds},
   pages = {43-55},
   title = {Modelling the distribution of the cluster maxima of exceedances of subasymptotic thresholds},
   volume = {99},
   year = {2012},
}
@article{Ramos2009,
   abstract = {A fundamental issue in applied multivariate extreme value analysis is modelling dependence within joint tail regions. The primary focus of this work is to extend the classical pseudopolar treatment of multivariate extremes to develop an asymptotically motivated representation of extremal dependence that also encompasses asymptotic independence. Starting with the usual mild bivariate regular variation assumptions that underpin the coefficient of tail dependence as a measure of extremal dependence, our main result is a characterization of the limiting structure of the joint survivor function in terms of an essentially arbitrary non-negative measure that must satisfy some mild constraints. We then construct parametric models from this new class and study in detail one example that accommodates asymptotic dependence, asymptotic independence and asymmetry within a straightforward parsimonious parameterization. We provide a fast simulation algorithm for this example and detail likelihood-based inference including tests for asymptotic dependence and symmetry which are useful for submodel selection. We illustrate this model by application to both simulated and real data. In contrast with the classical multivariate extreme value approach, which concentrates on the limiting distribution of normalized componentwise maxima, our framework focuses directly on the structure of the limiting joint survivor function and provides significant extensions of both the theoretical and the practical tools that are available for joint tail modelling. © 2009 Royal Statistical Society.},
   author = {Alexandra Ramos and Anthony Ledford},
   doi = {10.1111/j.1467-9868.2008.00684.x},
   issn = {13697412},
   issue = {1},
   journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
   keywords = {Asymptotic independence,Coefficient of tail dependence,Hidden regular variation,Joint tail modelling,Maximum likelihood,Multivariate extreme values},
   pages = {219-241},
   title = {A new class of models for bivariate joint tails},
   volume = {71},
   year = {2009},
}
@article{Eastoe2019,
   abstract = {Under the influence of local- and large-scale climatological processes, extreme river flow events often show long-term trends, seasonality, interyear variability, and other characteristics of temporal nonstationarity. Properly accounting for this nonstationarity is vital for making accurate predictions of future floods. In this paper, a regional model based on the generalised Pareto distribution is developed for peaks-over-threshold river flow data sets when the event sizes are nonstationary. If observations are nonstationary and covariates are available, then extreme-value (semi)parametric regression models may be used. Unfortunately, the necessary covariates are rarely observed, and if they are, it is often not clear which process, or combination of processes, to include in the model. Within the statistical literature, latent process (or random effects) models are often used in such scenarios. We develop a regional time-varying random effects model that allows identification of temporal nonstationarity in event sizes by pooling information across all sites in a spatially homogeneous region. The proposed model, which is an instance of a Bayesian hierarchical model, can be used to predict both unconditional extreme events such as the m-year maximum and extreme events that condition on being in a given year. The estimated random effects may also tell us about likely candidates for the climatological processes that cause nonstationarity in the flood process. The model is applied to UK flood data from 817 stations spread across 81 hydrometric regions.},
   author = {E. F. Eastoe},
   doi = {10.1002/env.2560},
   issn = {1099095X},
   issue = {5},
   journal = {Environmetrics},
   keywords = {Bayesian hierarchical models,flooding,nonstationarity,return levels,spatial pooling,time-varying random effects},
   pages = {1-18},
   title = {Nonstationarity in peaks-over-threshold river flows: A regional random effects model},
   volume = {30},
   year = {2019},
}
@article{Eastoe2010,
   abstract = {In a peaks over threshold analysis of a series of river flows, a sufficiently high threshold is used to extract the peaks of independent flood events. This paper reviews existing, and proposes new, statistical models for both the annual counts of such events and the process of event peak times. The most common existing model for the process of event times is a homogeneous Poisson process. This model is motivated by asymptotic theory. However, empirical evidence suggests that it is not the most appropriate model, since it implies that the mean and variance of the annual counts are the same, whereas the counts appear to be overdispersed, i.e., have a larger variance than mean. This paper describes how the homogeneous Poisson process can be extended to incorporate time variation in the rate at which events occur and so help to account for overdispersion in annual counts through the use of regression and mixed models. The implications of these new models on the implied probability distribution of the annual maxima are also discussed. The models are illustrated using a historical flow series from the River Thames at Kingston. Copyright 2010 by the American Geophysical Union.},
   author = {Emma F. Eastoe and Jonathan A. Tawn},
   doi = {10.1029/2009WR007757},
   issn = {00431397},
   issue = {2},
   journal = {Water Resources Research},
   keywords = {http://dx.doi.org/10.1029/2009WR007757, doi:10.102},
   pages = {1-12},
   title = {Statistical models for overdispersion in the frequency of peaks over threshold data for a flow series},
   volume = {46},
   year = {2010},
}
@article{Papastathopoulos2013,
   abstract = {The most popular approach in extreme value statistics is the modelling of threshold exceedances using the asymptotically motivated generalised Pareto distribution. This approach involves the selection of a high threshold above which the model fits the data well. Sometimes, few observations of a measurement process might be recorded in applications and so selecting a high quantile of the sample as the threshold leads to almost no exceedances. In this paper we propose extensions of the generalised Pareto distribution that incorporate an additional shape parameter while keeping the tail behaviour unaffected. The inclusion of this parameter offers additional structure for the main body of the distribution, improves the stability of the modified scale, tail index and return level estimates to threshold choice and allows a lower threshold to be selected. We illustrate the benefits of the proposed models with a simulation study and two case studies. © 2012 Elsevier B.V.},
   author = {Ioannis Papastathopoulos and Jonathan A. Tawn},
   doi = {10.1016/j.jspi.2012.07.001},
   issn = {03783758},
   issue = {1},
   journal = {Journal of Statistical Planning and Inference},
   keywords = {Extended generalised Pareto distribution,Extreme value theory,Liver toxicity,Tail estimation,Threshold selection},
   pages = {131-143},
   publisher = {Elsevier},
   title = {Extended generalised Pareto models for tail estimation},
   volume = {143},
   url = {http://dx.doi.org/10.1016/j.jspi.2012.07.001},
   year = {2013},
}
@article{Ramos2011,
   abstract = {An alternative limiting point process to that of de Haan (1985) is studied that holds regardless of whether the underlying data generation mechanism is asymptotically dependent or asymptotically independent. We characterize its intensity function in terms of the coefficient of tail dependence and an angular measure which satisfies a normalisation condition. We use this point process to derive a generalisation of standard componentwise maxima results that holds for both asymptotic dependence and asymptotic independence. We illustrate our results using a flexible parametric example and provide methods for simulating from both the limiting point process and the limiting componentwise maxima distribution. © Taylor & Francis Group, LLC.},
   author = {Alexandra Ramos and Anthony Ledford},
   doi = {10.1080/03610921003764233},
   isbn = {0361092100376},
   issn = {03610926},
   issue = {12},
   journal = {Communications in Statistics - Theory and Methods},
   keywords = {Asymptotic dependence,Asymptotic independence,Coefficient of tail dependence,Componentwise maxima,Extreme value theory,Joint survivor function,Multivariate extreme values,Point processes,Simulation},
   pages = {2205-2224},
   title = {An alternative point process framework for modeling multivariate extreme values},
   volume = {40},
   year = {2011},
}
@article{Mhalla2019,
   abstract = {The probability and structure of co-occurrences of extreme values in multivariate data may critically depend on auxiliary information provided by covariates. In this contribution, we develop a flexible generalized additive modeling framework based on high threshold exceedances for estimating covariate-dependent joint tail characteristics for regimes of asymptotic dependence and asymptotic independence. The framework is based on suitably defined marginal pretransformations and projections of the random vector along the directions of the unit simplex, which lead to convenient univariate representations of multivariate exceedances based on the exponential distribution. Good performance of our estimators of a nonparametrically designed influence of covariates on extremal coefficients and tail dependence coefficients are shown through a simulation study. We illustrate the usefulness of our modeling framework on a large dataset of nitrogen dioxide measurements recorded in France between 1999 and 2012, where we use the generalized additive framework for modeling marginal distributions and tail dependence in large concentrations observed at pairs of stations. Our results imply asymptotic independence of data observed at different stations, and we find that the estimated coefficients of tail dependence decrease as a function of spatial distance and show distinct patterns for different years and for different types of stations (traffic vs. background).},
   author = {Linda Mhalla and Thomas Opitz and Valérie Chavez-Demoulin},
   doi = {10.1007/s10687-019-00342-6},
   isbn = {1068701900},
   issn = {1572915X},
   issue = {3},
   journal = {Extremes},
   keywords = {Asymptotic independence,Extreme value theory,Generalized additive models,Penalized likelihood,Tail dependence},
   pages = {523-552},
   title = {Exceedance-based nonlinear regression of tail dependence},
   volume = {22},
   year = {2019},
}
@article{Marcon2017,
   abstract = {The analysis of multiple extreme values aims to describe the stochastic behaviour of observations in the joint upper tail of a distribution function. For instance, being able to simulate multivariate extreme events is convenient for end users who need a large number of random replications of extremes as input of a given complex system to test its sensitivity. The simulation of multivariate extremes is often based on the assumption that the dependence structure, the so-called extremal dependence function, is described by a specific parametric model. We propose a simulation method for sampling bivariate extremes, under the assumption that the extremal dependence function is semiparametric. This yields a flexible tool that can be broadly applied in real-data analyses. With the aim of estimating the probability of belonging to some extreme sets, our methodology is examined via simulation and illustrated by an analysis of strong wind gusts in France.},
   author = {Giulia Marcon and Philippe Naveau and Simone Padoan},
   doi = {10.1002/sta4.145},
   issn = {20491573},
   issue = {1},
   journal = {Stat},
   keywords = {Angular measure,Bernstein polynomials,Bivariate extreme-value distribution,Extremal dependence,Generalized extreme-value distribution,Pickands dependence function,Poisson point process,Wind speed},
   pages = {184-201},
   title = {A semi-parametric stochastic generator for bivariate extreme events},
   volume = {6},
   year = {2017},
}
@article{Mhalla2019a,
   abstract = {We propose a vector generalized additive modeling framework for taking into account the effect of covariates on angular density functions in a multivariate extreme value context. The proposed methods are tailored for settings where the dependence between extreme values may change according to covariates. We devise a maximum penalized log-likelihood estimator, discuss details of the estimation procedure, and derive its consistency and asymptotic normality. The simulation study suggests that the proposed methods perform well in a wealth of simulation scenarios by accurately recovering the true covariate-adjusted angular density. Our empirical analysis reveals relevant dynamics of the dependence between extreme air temperatures in two alpine resorts during the winter season.},
   author = {L. Mhalla and M. de Carvalho and V. Chavez-Demoulin},
   doi = {10.1111/sjos.12388},
   issn = {14679469},
   issue = {4},
   journal = {Scandinavian Journal of Statistics},
   keywords = {VGAM,angular density,covariate adjustment,penalized log-likelihood,statistics of multivariate extremes},
   pages = {1141-1167},
   title = {Regression-type models for extremal dependence},
   volume = {46},
   year = {2019},
}
@article{Cooley2019,
   abstract = {We present a method for drawing isolines indicating regions of equal joint exceedance probability for bivariate data. The method relies on bivariate regular variation, a dependence framework widely used for extremes. The method we utilize for characterizing dependence in the tail is largely nonparametric. The extremes framework enables drawing isolines corresponding to very low exceedance probabilities and may even lie beyond the range of the data; such cases would be problematic for standard nonparametric methods. Furthermore, we extend this method to the case of asymptotic independence and propose a procedure which smooths the transition from hidden regular variation in the interior to the first-order behavior on the axes. We propose a diagnostic plot for assessing the isoline estimate and choice of smoothing, and a bootstrap procedure to visually assess uncertainty.},
   author = {Daniel Cooley and Emeric Thibaud and Federico Castillo and Michael F. Wehner},
   doi = {10.1007/s10687-019-00348-0},
   isbn = {1068701900},
   issn = {1572915X},
   issue = {3},
   journal = {Extremes},
   keywords = {62G32,62H99,Asymptotic independence,Extreme values,Hidden regular variation,Multivariate,Regular variation},
   pages = {373-390},
   publisher = {Extremes},
   title = {A nonparametric method for producing isolines of bivariate exceedance probabilities},
   volume = {22},
   year = {2019},
}
@article{Belzunce2007,
   abstract = {Within the context of a general bivariate distribution an intuitive method is presented in order to study the dependence structure of the two distributions. A set of points-level curve-which accumulate the same probability for a fixed quadrant is considered. This procedure provides four level curves which can be considered as the boundary of a generalization of the real interquantile interval. It is shown that the accumulated probability among the level curves depends on the dependence structure of the distribution function where the dependence structure is given by the notion of copula. Furthermore, the case when the marginal distributions are independent is investigated. This result is used to find out positive or negative dependence properties for the variables. Finally, a nonparametric test for independence with a local dependence meaning is performed and applied to different data sets. © 2006 Elsevier B.V. All rights reserved.},
   author = {F. Belzunce and A. Castaño and A. Olvera-Cervantes and A. Suárez-Llorens},
   doi = {10.1016/j.csda.2006.08.017},
   issn = {01679473},
   issue = {10},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Bivariate quantile,Central region,Copula,Positive or negative dependence,Test for independence},
   pages = {5112-5129},
   title = {Quantile curves and dependence structure for bivariate distributions},
   volume = {51},
   year = {2007},
}
@article{Wadsworth2017,
   abstract = {Different dependence scenarios can arise in multivariate extremes, entailing careful selection of an appropriate class of models. In bivariate extremes, the variables are either asymptotically dependent or are asymptotically independent. Most available statistical models suit one or other of these cases, but not both, resulting in a stage in the inference that is unaccounted for but can substantially impact subsequent extrapolation. Existing modelling solutions to this problem are either applicable only on subdomains or appeal to multiple limit theories. We introduce a unified representation for bivariate extremes that encompasses a wide variety of dependence scenarios and applies when at least one variable is large. Our representation motivates a parametric model that encompasses both dependence classes. We implement a simple version of this model and show that it performs well in a range of settings.},
   author = {J. L. Wadsworth and J. A. Tawn and A. C. Davison and D. M. Elton},
   doi = {10.1111/rssb.12157},
   issn = {14679868},
   issue = {1},
   journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
   keywords = {Asymptotic independence,Censored likelihood,Conditional extremes,Dependence modelling,Extreme value theory,Multivariate regular variation},
   pages = {149-175},
   title = {Modelling across extremal dependence classes},
   volume = {79},
   year = {2017},
}
@article{Coles1999,
   abstract = {Quantifying dependence is a central theme in probabilistic and statistical methods for multivariate extreme values. Two situations are possible: one where, in a limiting sense, the extremes are dependent; the other where, in the same sense, the extremes are independent. This paper comprises an overview of the principal issues through a uni®ed approach which encompasses both these situations. Novel diagnostic measures for dependence are also developed which provide complementary information about different aspects of extremal dependence. The paper is written in an elementary style, with the methodology illustrated by application to theoretical examples and typical data-sets. These data-sets and the S-plus functions used for the analyses are available online.},
   author = {Stuart Coles and Janet Heffernan and Jonathan Tawn},
   issue = {4},
   journal = {Extremes},
   keywords = {asymptotic independence,bivariate extreme value distribution,copula,point processes},
   pages = {339-365},
   title = {Dependence measures for multivariate extremes},
   volume = {2},
   url = {http://www.math.lancs.ac.uk./~coless.},
   year = {1999},
}
@article{Wadsworth2013,
   abstract = {Existing theory for multivariate extreme values focuses upon characterizations of the distributional tails when all components of a random vector, standardized to identical margins, grow at the same rate. In this paper, we consider the effect of allowing the components to grow at different rates, and characterize the link between these marginal growth rates and the multivariate tail probability decay rate. Our approach leads to a whole class of univariate regular variation conditions, in place of the single but multivariate regular variation conditions that underpin the current theories. These conditions are indexed by a homogeneous function and an angular dependence function, which, for asymptotically independent random vectors, mirror the role played by the exponent measure and Pickands' dependence function in classical multivariate extremes. We additionally offer an inferential approach to joint survivor probability estimation. The key feature of our methodology is that extreme set probabilities can be estimated by extrapolating upon rays emanating from the origin when the margins of the variables are exponential. This offers an appreciable improvement over existing techniques where extrapolation in exponential margins is upon lines parallel to the diagonal. © 2013 ISI/BS.},
   author = {J. L. Wadsworth and J. A. Tawn},
   doi = {10.3150/12-BEJ471},
   issn = {13507265},
   issue = {5 B},
   journal = {Bernoulli},
   keywords = {Asymptotic independence,Coefficient of tail dependence,Multivariate extreme value theory,Pickands' dependence function,Regular variation},
   pages = {2689-2714},
   title = {A new representation for multivariate tail probabilities},
   volume = {19},
   year = {2013},
}
@misc{,
   title = {Useful_jon_MV_Notes.pdf},
}
@article{Salvadori2004,
   abstract = {In this paper we provide a general theoretical framework exploiting copulas for studying the return periods of hydrological events; in particular, we consider events depending upon the joint behavior of two nonindependent random variables, an approach which can easily be generalized to the multivariate case. We show that using copulas may greatly simplify the calculations and may even yield analytical expressions for the isolines of the return periods, both in the unconditional and in the conditional case. In addition, we show how a new probability distribution may be associated with the return period of specific events and introduce the definitions of sub-, super-, and critical events as well as those of primary and secondary return periods. An illustration of the techniques proposed is provided by analyzing some case studies already examined in literature.},
   author = {G. Salvadori and C. De Michele},
   doi = {10.1029/2004WR003133},
   issn = {00431397},
   issue = {12},
   journal = {Water Resources Research},
   keywords = {2-Copulas,Return period},
   pages = {1-17},
   title = {Frequency analysis via copulas: Theoretical aspects and applications to hydrological events},
   volume = {40},
   year = {2004},
}
@article{Rootzen2013,
   abstract = {In the past, the concepts of return levels and return periods have been standard and important tools for engineering design. However, these concepts are based on the assumption of a stationary climate and do not apply to a changing climate, whether local or global. In this paper, we propose a refined concept, Design Life Level, which quantifies risk in a nonstationary climate and can serve as the basis for communication. In current practice, typical hydrologic risk management focuses on a standard (e.g., in terms of a high quantile corresponding to the specified probability of failure for a single year). Nevertheless, the basic information needed for engineering design should consist of (i) the design life period (e.g., the next 50 years, say 2015-2064); and (ii) the probability (e.g., 5% chance) of a hazardous event (typically, in the form of the hydrologic variable exceeding a high level) occurring during the design life period. Capturing both of these design characteristics, the Design Life Level is defined as an upper quantile (e.g., 5%) of the distribution of the maximum value of the hydrologic variable (e.g., water level) over the design life period. We relate this concept and variants of it to existing literature and illustrate how they, and some useful complementary plots, may be computed and used. One practically important consideration concerns quantifying the statistical uncertainty in estimating a high quantile under nonstationarity. Key Points The concepts return level and return period do not apply to a changing climate We propose a concept, Design Life Level, which gives key information for design We illustrate by rainfall in Western Australia and warm winters in Fort Collins ©2013. American Geophysical Union. All Rights Reserved.},
   author = {Holger Rootzén and Richard W. Katz},
   doi = {10.1002/wrcr.20425},
   issn = {00431397},
   issue = {9},
   journal = {Water Resources Research},
   keywords = {climate change,design criteria,exceedance risk,extreme value statistics,nonstationary,return level,return period},
   pages = {5964-5972},
   title = {Design Life Level: Quantifying risk in a changing climate},
   volume = {49},
   year = {2013},
}
@article{Keef2013,
   abstract = {A number of different approaches to study multivariate extremes have been developed. Arguably the most useful and flexible is the theory for the distribution of a vector variable given that one of its components is large. We build on the conditional approach of Heffernan and Tawn (2004). [13] for estimating this type of multivariate extreme property. Specifically we propose additional constraints for, and slight changes in, their model formulation. These changes in the method are aimed at overcoming complications that have been experienced with using the approach in terms of their modelling of negatively associated variables, parameter identifiability problems and drawing conditional inferences which are inconsistent with the marginal distributions. The benefits of the methods are illustrated using river flow data from two tributaries of the River Thames in the UK. © 2012 Elsevier Inc.},
   author = {Caroline Keef and Ioannis Papastathopoulos and Jonathan A. Tawn},
   doi = {10.1016/j.jmva.2012.10.012},
   issn = {0047259X},
   journal = {Journal of Multivariate Analysis},
   pages = {396-404},
   publisher = {Elsevier Inc.},
   title = {Estimation of the conditional distribution of a multivariate variable given that one of its components is large: Additional constraints for the Heffernan and Tawn model},
   volume = {115},
   url = {http://dx.doi.org/10.1016/j.jmva.2012.10.012},
   year = {2013},
}
@article{Ledford1997,
   abstract = {Standard approaches for modelling dependence within joint tail regions are based on extreme value methods which assume max-stability, a particular form of joint tail dependence. We develop joint tail models based on a broader class of dependence structure which provides a natural link between max-stable models and weaker forms of dependence including independence and negative association. This approach overcomes many of the problems that are encountered with standard methods and is the basis for a Poisson process representation that generalizes existing bivariate results. We apply the new techniques to simulated and environmental data, and demonstrate the marked advantage that the new approach offers for joint tail extrapolation. © 1997 Royal Statistical Society.},
   author = {Anthony W. Ledford and Jonathan A. Tawn},
   doi = {10.1111/1467-9868.00080},
   issn = {13697412},
   issue = {2},
   journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
   keywords = {Asymptotic independence,Coefficient of tail dependence,Componentwise maxima,Extreme value theory,Maximum likelihood,Non-homogeneous poisson process,Rates of convergence,Slowly varying functions},
   pages = {475-499},
   title = {Modelling dependence within joint tail regions},
   volume = {59},
   year = {1997},
}
@article{Kiriliouk2019,
   abstract = {When assessing the impact of extreme events, it is often not just a single component, but the combined behavior of several components which is important. Statistical modeling using multivariate generalized Pareto (GP) distributions constitutes the multivariate analogue of univariate peaks over thresholds modeling, which is widely used in finance and engineering. We develop general methods for construction of multivariate GP distributions and use them to create a variety of new statistical models. A censored likelihood procedure is proposed to make inference on these models, together with a threshold selection procedure, goodness-of-fit diagnostics, and a computationally tractable strategy for model selection. The models are fitted to returns of stock prices of four UK-based banks and to rainfall data in the context of landslide risk estimation. Supplementary materials and codes are available online.},
   author = {Anna Kiriliouk and Holger Rootzén and Johan Segers and Jennifer L. Wadsworth},
   doi = {10.1080/00401706.2018.1462738},
   issn = {15372723},
   issue = {1},
   journal = {Technometrics},
   keywords = {Financial risk,Landslides,Multivariate extremes,Tail dependence},
   pages = {123-135},
   title = {Peaks Over Thresholds Modeling With Multivariate Generalized Pareto Distributions},
   volume = {61},
   year = {2019},
}
@article{Ledford1996,
   abstract = {We propose a multivariate extreme value threshold model for joint tail estimation which overcomes the problems encountered with existing techniques when the variables are near independence. We examine inference under the model and develop tests for independence of extremes of the marginal variables, both when the thresholds are fixed, and when they increase with the sample size. Motivated by results obtained from this model, we give a new and widely applicable characterisation of dependence in the joint tail which includes existing models as special cases. A new parameter which governs the form of dependence is of fundamental importance to this characterisation. By estimating this parameter, we develop a diagnostic test which assesses the applicability of bivariate extreme value joint tail models. The methods are demonstrated through simulation and by analysing two previously published data sets.},
   author = {Anthony W. Ledford and Jonathan A. Tawn},
   doi = {10.1093/biomet/83.1.169},
   issn = {00063444},
   issue = {1},
   journal = {Biometrika},
   keywords = {Asymptotic independence,Coefficient of tail dependence,Extreme value theory,Generalised Pareto distribution,Maximum likelihood,Multivariate extreme value distribution,Nonregular estimation,Poisson process,Threshold exceedance},
   pages = {169-187},
   title = {Statistics for near independence in multivariate extreme values},
   volume = {83},
   year = {1996},
}
@article{Huser2020,
   abstract = {The classical modeling of spatial extremes relies on asymptotic models (i.e., max-stable processes or $r$-Pareto processes) for block maxima or peaks over high thresholds, respectively. However, at finite levels, empirical evidence often suggests that such asymptotic models are too rigidly constrained, and that they do not adequately capture the frequent situation where more severe events tend to be spatially more localized. In other words, these asymptotic models have a strong tail dependence that persists at increasingly high levels, while data usually suggest that it should weaken instead. Another well-known limitation of classical spatial extremes models is that they are either computationally prohibitive to fit in high dimensions, or they need to be fitted using less efficient techniques. In this review paper, we describe recent progress in the modeling and inference for spatial extremes, focusing on new models that have more flexible tail structures that can bridge asymptotic dependence classes, and that are more easily amenable to likelihood-based inference for large datasets. In particular, we discuss various types of random scale constructions, as well as the conditional spatial extremes model, which have recently been getting increasing attention within the statistics of extremes community. We illustrate some of these new spatial models on two different environmental applications.},
   author = {Raphaël Huser and Jennifer L. Wadsworth},
   keywords = {and engineering,and technology,asymptotic independence and dependence,cemse,computer,conditional spatial extremes model,division,dullah university of science,e-mail,electrical and mathematical sciences,kaust,king ab-,max-stable process,pareto process,random scale mixtures,saudi arabia,sub-asymptotic tail modeling,thuwal 23955-6900},
   pages = {1-59},
   title = {Advances in Statistical Modeling of Spatial Extremes},
   url = {http://arxiv.org/abs/2007.00774},
   year = {2020},
}
@article{Davison1990,
   author = {A. C. Davison and R. L. Smith},
   doi = {10.1111/j.2517-6161.1990.tb01796.x},
   issn = {00359246},
   issue = {3},
   journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
   month = {7},
   pages = {393-425},
   title = {Models for Exceedances Over High Thresholds},
   volume = {52},
   url = {http://doi.wiley.com/10.1111/j.2517-6161.1990.tb01796.x},
   year = {1990},
}
@article{Tawn1992,
   author = {Jonathan A. Tawn},
   doi = {10.2307/2347619},
   issn = {00359254},
   issue = {1},
   journal = {Applied Statistics},
   keywords = {extremal index,extreme value theory,generalized extreme value distribution,joint,maximum likelihood estimation,non-stationarity,probabilities method,sea-level,tides},
   pages = {77},
   title = {Estimating Probabilities of Extreme Sea-Levels},
   volume = {41},
   url = {https://www.jstor.org/stable/10.2307/2347619?origin=crossref},
   year = {1992},
}
@article{Naveau2016,
   author = {Philippe Naveau and Raphael Huser and Pierre Ribereau and Alexis Hannart},
   doi = {10.1002/2015WR018552},
   issn = {00431397},
   issue = {4},
   journal = {Water Resources Research},
   month = {4},
   pages = {2753-2769},
   title = {Modeling jointly low, moderate, and heavy rainfall intensities without a threshold selection},
   volume = {52},
   url = {http://doi.wiley.com/10.1002/2015WR018552},
   year = {2016},
}
@article{Dupuis2019,
   author = {Debbie J. Dupuis and Luca Trapin},
   doi = {10.1214/18-AOAS1183},
   issn = {1932-6157},
   issue = {1},
   journal = {The Annals of Applied Statistics},
   month = {3},
   pages = {34-59},
   title = {Ground-level ozone: Evidence of increasing serial dependence in the extremes},
   volume = {13},
   url = {https://projecteuclid.org/euclid.aoas/1554861640},
   year = {2019},
}
@article{Coles2002,
   author = {Stuart Coles and Francesco. Pauli},
   doi = {10.1093/biomet/89.1.183},
   issn = {0006-3444},
   issue = {1},
   journal = {Biometrika},
   month = {3},
   pages = {183-196},
   title = {Models and inference for uncertainty in extremal dependence},
   volume = {89},
   url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/89.1.183},
   year = {2002},
}
@article{Vignotto2020,
   abstract = {Classification tasks usually assume that all possible classes are present during the training phase. This is restrictive if the algorithm is used over a long time and possibly encounters samples from unknown new classes. It is therefore fundamental to develop algorithms able to distinguish between normal and abnormal test data. In the last few years, extreme value theory has become an important tool in multivariate statistics and machine learning. The recently introduced extreme value machine, a classifier motivated by extreme value theory, addresses this problem and achieves competitive performance in specific cases. We show that this algorithm has some theoretical and practical drawbacks and can fail even if the recognition task is fairly simple. To overcome these limitations, we propose two new algorithms for anomaly detection relying on approximations from extreme value theory that are more robust in such cases. We exploit the intuition that test points that are extremely far from the training classes are more likely to be abnormal objects. We derive asymptotic results motivated by univariate extreme value theory that make this intuition precise. We show the effectiveness of our classifiers in simulations and on real data sets.},
   author = {Edoardo Vignotto and Sebastian Engelke},
   doi = {10.1007/s10687-020-00393-0},
   issn = {1572915X},
   journal = {Extremes},
   keywords = {Clustering,Machine learning,Novelty detection,Open set classification,Statistical methods},
   pages = {501-520},
   publisher = {Extremes},
   title = {Extreme value theory for anomaly detection – the GPD classifier},
   volume = {1},
   year = {2020},
}
@article{Hill1975,
   author = {Bruce M. Hill},
   doi = {10.1214/aos/1176343247},
   issn = {0090-5364},
   issue = {5},
   journal = {The Annals of Statistics},
   month = {9},
   pages = {1163-1174},
   title = {A Simple General Approach to Inference About the Tail of a Distribution},
   volume = {3},
   url = {http://projecteuclid.org/euclid.aos/1176343247},
   year = {1975},
}
@article{Siffer2017,
   abstract = {Anomaly detection in time series has attracted considerable attention due to its importance in many real-world applications including intrusion detection, energy management and finance. Most approaches for detecting outliers rely on either manually set thresholds or assumptions on the distribution of data according to Chandola, Banerjee and Kumar. Here, we propose a new approach to detect outliers in streaming univariate time series based on Extreme Value Theory that does not require to hand-set thresholds and makes no assumption on the distribution: the main parameter is only the risk, controlling the number of false positives. Our approach can be used for outlier detection, but more generally for automatically setting thresholds, making it useful in wide number of situations. We also experiment our algorithms on various real-world datasets which confirm its soundness and efficiency.},
   author = {Alban Siffer and Pierre Alain Fouque and Alexandre Termier and Christine Largouet},
   doi = {10.1145/3097983.3098144},
   isbn = {9781450348874},
   journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {Extreme value theory,Outliers in time series,Streaming},
   pages = {1067-1075},
   title = {Anomaly detection in streams with extreme value theory},
   volume = {Part F1296},
   year = {2017},
}
@article{Scarrott2012,
   abstract = {The last decade has seen development of a plethora of approaches for threshold estimation in extreme value applications. From a statistical perspective, the threshold is loosely defined such that the population tail can be well approximated by an extreme value model (e.g., the generalised Pareto distribution), obtaining a balance between the bias due to the asymptotic tail approximation and parameter estimation uncertainty due to the inherent sparsity of threshold excess data. This paper reviews recent advances and some traditional approaches, focusing on those that provide quantification of the associated uncertainty on inferences (e.g., return level estimation).},
   author = {Carl Scarrott and Anna MacDonald},
   issn = {16456726},
   issue = {1},
   journal = {Revstat Statistical Journal},
   keywords = {Extreme value threshold selection,Graphical diagnostics,Mixture modelling,Rule of thumb,Threshold uncertainty},
   pages = {33-60},
   title = {A review of extreme value threshold estimation and uncertainty quantification},
   volume = {10},
   year = {2012},
}
@article{MacDonald2011,
   abstract = {Extreme value theory is used to derive asymptotically motivated models for unusual or rare events, e.g. the upper or lower tails of a distribution. A new flexible extreme value mixture model is proposed combining a non-parametric kernel density estimator for the bulk of the distribution with an appropriate tail model. The complex uncertainties associated with threshold choice are accounted for and new insights into the impact of threshold choice on density and quantile estimates are obtained. Bayesian inference is used to account for all uncertainties and enables inclusion of expert prior information, potentially overcoming the inherent sparsity of extremal data. A simulation study and empirical application for determining normal ranges for physiological measurements for pre-term infants is used to demonstrate the performance of the proposed mixture model. The potential of the proposed model for overcoming the lack of consistency of likelihood based kernel bandwidth estimators when faced with heavy tailed distributions is also demonstrated. © 2011 Elsevier B.V. All rights reserved.},
   author = {A. MacDonald and C. J. Scarrott and D. Lee and B. Darlow and M. Reale and G. Russell},
   doi = {10.1016/j.csda.2011.01.005},
   issn = {01679473},
   issue = {6},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Extreme values,Kernel density,Mixture model,Threshold selection},
   pages = {2137-2157},
   publisher = {Elsevier B.V.},
   title = {A flexible extreme value mixture model},
   volume = {55},
   url = {http://dx.doi.org/10.1016/j.csda.2011.01.005},
   year = {2011},
}
@book{Beirlant2004,
   abstract = {Research in the statistical analysis of extreme values has flourished over the past decade: new probability models, inference and data analysis techniques have been introduced; and new application areas have been explored. Statistics of Extremes comprehensively covers a wide range of models and application areas, including risk and insurance: a major area of interest and relevance to extreme value theory. Case studies are introduced providing a good balance of theory and application of each model discussed, incorporating many illustrated examples and plots of data. The last part of the book covers some interesting advanced topics, including time series, regression, multivariate and Bayesian modelling of extremes, the use of which has huge potential.},
   author = {Jan Beirlant and Yuri Goegebeur and Jozef Teugels and Johan Segers},
   doi = {10.1002/0470012382},
   isbn = {9780471976479},
   journal = {Statistics of Extremes: Theory and Applications},
   month = {8},
   pages = {1-490},
   publisher = {Wiley},
   title = {Statistics of Extremes},
   url = {https://onlinelibrary.wiley.com/doi/book/10.1002/0470012382},
   year = {2004},
}
@book{Rosenthal2006a,
   abstract = {2nd ed. The need for measure theory -- Probability triples -- Further probabilistic foundations -- Expected values -- Inequalities and convergence -- Distributions of random variables -- Stochastic processes and gambling games -- Discrete Markov chains -- More probability theorems -- Weak convergence -- Characteristic functions -- Decomposition of probability laws -- Conditional probability and expectation -- Martingales -- General stochastic processes -- Mathematical background.},
   author = {Jeffrey S. (Jeffrey Seth) Rosenthal},
   isbn = {9812703713},
   pages = {219},
   title = {Exercise solutions--A first look at rigorous probability theory},
   year = {2006},
}
@book{Wood2017,
   abstract = {This book imparts a thorough understanding of the theory and practical applications of GAMs and related advanced models, enabling informed use of these very flexible tools. The author bases his approach on a framework of penalized regression splines, and builds a well- grounded foundation through motivating chapters on linear and generalized linear models. While firmly focused on the practical aspects of GAMs, discussions include fairly full explanations of the theory underlying the methods. The treatment is rich with practical examples, and it includes an entire chapter on the analysis of real data sets using R and the author's add-on package mgcv. Each chapter includes exercises, for which complete solutions are provided in an appendix.},
   author = {Simon N. Wood},
   doi = {10.1201/9781315370279},
   isbn = {9781315370279},
   issn = {0964-1998},
   month = {5},
   publisher = {Chapman and Hall/CRC},
   title = {Generalized Additive Models},
   url = {https://www.taylorfrancis.com/books/9781498728348},
   year = {2017},
}
@book{Coles2001,
   abstract = {An Introduction to Statistical Modeling of Extreme Values},
   author = {Stuart Coles},
   city = {London},
   doi = {10.1007/978-1-4471-3675-0},
   isbn = {978-1-84996-874-4},
   publisher = {Springer London},
   title = {An Introduction to Statistical Modeling of Extreme Values},
   url = {http://link.springer.com/10.1007/978-1-4471-3675-0},
   year = {2001},
}
@book{Rosenthal2006,
   author = {Jeffrey S Rosenthal},
   doi = {10.1142/6300},
   isbn = {978-981-270-370-5},
   month = {11},
   publisher = {WORLD SCIENTIFIC},
   title = {A First Look at Rigorous Probability Theory},
   url = {https://www.worldscientific.com/worldscibooks/10.1142/6300},
   year = {2006},
}
@book{Leadbetter1983,
   author = {M. R. Leadbetter and Georg Lindgren and Holger Rootzén},
   city = {New York, NY},
   doi = {10.1007/978-1-4612-5449-2},
   isbn = {978-1-4612-5451-5},
   publisher = {Springer New York},
   title = {Extremes and Related Properties of Random Sequences and Processes},
   url = {http://link.springer.com/10.1007/978-1-4612-5449-2},
   year = {1983},
}
@article{Pickands1975,
   author = {James Pickands},
   doi = {10.1214/aos/1176343003},
   issn = {0090-5364},
   issue = {1},
   journal = {The Annals of Statistics},
   month = {1},
   pages = {119-131},
   title = {Statistical Inference Using Extreme Order Statistics},
   volume = {3},
   url = {http://projecteuclid.org/euclid.aos/1176343003},
   year = {1975},
}
@article{Sklar1959,
   author = {Abe Sklar},
   journal = {Publ. Inst. Statist. Univ. Paris},
   pages = {229-231},
   title = {Fonctions de repartition a n dimensions et leurs marges},
   volume = {8},
   url = {http://ci.nii.ac.jp/naid/10011938360/en/},
   year = {1959},
}
@book{Ruschendorf2013,
   author = {Ludger Rüschendorf},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-33590-7},
   isbn = {978-3-642-33589-1},
   publisher = {Springer Berlin Heidelberg},
   title = {Mathematical Risk Analysis},
   url = {http://link.springer.com/10.1007/978-3-642-33590-7},
   year = {2013},
}
@article{Barnett1976,
   author = {V. Barnett},
   doi = {10.2307/2344839},
   issn = {00359238},
   issue = {3},
   journal = {Journal of the Royal Statistical Society. Series A (General)},
   pages = {318-344},
   title = {The Ordering of Multivariate Data},
   volume = {139},
   url = {https://www.jstor.org/stable/10.2307/2344839?origin=crossref},
   year = {1976},
}
@article{Balkema1977,
   abstract = {Necessary and sufficient conditions are given for a distribution function in ℝ 2 to be max-infinitely divisible. The d.f. F is max i.d. if F t is a d.f. for every t > 0. This property is essential in defining multivariate extremal processes and arises in an approach to the study of the range of an i.i.d. sample.},
   author = {A. A. Balkema and S. I. Resnick},
   doi = {10.2307/3213001},
   issn = {0021-9002},
   issue = {2},
   journal = {Journal of Applied Probability},
   month = {6},
   pages = {309-319},
   title = {Max-infinite divisibility},
   volume = {14},
   url = {https://www.cambridge.org/core/product/identifier/S002190020010498X/type/journal_article},
   year = {1977},
}
@article{Pickands1981,
   author = {James Pickands},
   journal = {Proceedings 43th, Session of International Statistical Institution, 1981},
   title = {Multivariate extreme value distribution},
   url = {http://ci.nii.ac.jp/naid/10022049959/en/},
   year = {1981},
}
@book{Resnick1987,
   author = {Sidney I. Resnick},
   city = {New York},
   doi = {10.1007/978-0-387-75953-1},
   isbn = {978-0-387-75952-4},
   publisher = {Springer New York},
   title = {Extreme Values, Regular Variation and Point Processes},
   url = {http://link.springer.com/10.1007/978-0-387-75953-1},
   year = {1987},
}
@article{Stephenson2002,
   author = {Alec Stephenson},
   issue = {2},
   journal = {R News},
   pages = {31-32},
   title = {evd: Extreme Value Distributions},
   volume = {2},
   url = {https://cran.r-project.org/doc/Rnews/},
   year = {2002},
}
@article{Nicolet2018,
   abstract = {Modeling extreme snow depths in space is important for water storage, tourism industry, mountain ecosystems, collapse of buildings, and avalanche prevention. However, studies modeling the spatial dependence structure of extremes generally assume temporal stationarity which is clearly questionable in a climate change context. We model climatic trends within the spatial dependence structure of extremes, with application to a data set of snow depth winter maxima. From 82 stations spanning the 1970–2012 period in the French Alps, we infer a strong decrease in the range of spatial extremal dependence. This finding is related to a strong decrease in both the snow precipitation ratio and the winter cumulated snowfall, due to increasing temperatures. Hence, we demonstrate that the spatial dependence of extreme snow depths is impacted by climate change in a similar way as has been observed for extreme snowfalls. Furthermore, snow depths maxima are more spatially dependent than snowfalls. The space-time approach that we introduce may be very useful for assessing past and future evolutions under ongoing climate change in various hydrological quantities.},
   author = {Gilles Nicolet and Nicolas Eckert and Samuel Morin and Juliette Blanchet},
   doi = {10.1029/2018WR022763},
   issn = {19447973},
   issue = {10},
   journal = {Water Resources Research},
   keywords = {French Alps,climate change,extreme value statistic,max-stable processes,snow depth,spatial dependence},
   pages = {7820-7840},
   title = {Assessing Climate Change Impact on the Spatial Dependence of Extreme Snow Depth Maxima in the French Alps},
   volume = {54},
   year = {2018},
}
@article{Tawn1990,
   abstract = {Multivariate extreme value distributions arise as the limiting joint distribution of normalized componentwise maxima/minima. No parametric family exists for the dependence between the margins. This paper extends to more than two variables the models and results for the bivariate case obtained by Tawn (1988). Two new families of physically motivated parametric models for the dependence structure are presented and are illustrated with an application to trivariate extreme sea level data. © 1990 Biometrika Trust.},
   author = {Jonathan A. Tawn},
   doi = {10.1093/biomet/77.2.245},
   issn = {00063444},
   issue = {2},
   journal = {Biometrika},
   keywords = {Extreme value theory,Generalized Pareto distribution,Multivariate exponential distribution,Nonregular estimation},
   pages = {245-253},
   title = {Modelling multivariate extreme value distributions},
   volume = {77},
   year = {1990},
}
@misc{Barltrop2019,
   author = {Callum Barltrop},
   institution = {Lancaster University},
   title = {Multivariate Extremes for Nuclear Regulation - PhD Proposal},
   year = {2019},
}
@misc{Barltrop2020,
   author = {Callum Barltrop},
   institution = {Lancaster University},
   title = {Multivariate Extremes for Nuclear Regulation - 10 Month Review},
   year = {2020},
}
@article{Leonelli2020,
   abstract = {Inference over multivariate tails often requires a number of assumptions which may affect the assessment of the extreme dependence structure. Models are usually constructed in such a way that extreme components can either be asymptotically dependent or be independent of each other. Recently, there has been an increasing interest on modelling multivariate extremes more flexibly, by allowing models to bridge both asymptotic dependence regimes. Here we propose a novel semiparametric approach which allows for a variety of dependence patterns, be them extremal or not, by using in a model-based fashion the full dataset. We build on previous work for inference on marginal exceedances over a high, unknown threshold, by combining it with flexible, semiparametric copula specifications to investigate extreme dependence, thus separately modelling marginals and dependence structure. Because of the generality of our approach, bivariate problems are investigated here due to computational challenges, but multivariate extensions are readily available. Empirical results suggest that our approach can provide sound uncertainty statements about the possibility of asymptotic independence, and we propose a criterion to quantify the presence of either extreme regime which performs well in our applications when compared to others available. Estimation of functions of interest for extremes is performed via MCMC algorithms. Attention is also devoted to the prediction of new extreme observations. Our approach is evaluated through simulations, applied to real data and assessed against competing approaches. Evidence demonstrates that the bulk of the data do not bias and improve the inferential process for extremal dependence in our applications.},
   author = {Manuele Leonelli and Dani Gamerman},
   doi = {10.1007/s11222-019-09878-w},
   issn = {15731375},
   issue = {2},
   journal = {Statistics and Computing},
   keywords = {Asymptotic dependence,Copulae,GPD distribution,High quantiles,Prediction,Threshold estimation},
   pages = {221-236},
   publisher = {Springer US},
   title = {Semiparametric bivariate modelling with flexible extremal dependence},
   volume = {30},
   url = {https://doi.org/10.1007/s11222-019-09878-w},
   year = {2020},
}
@article{Chavez-Demoulin2012,
   abstract = {The need to model rare events of univariate time series has led to many recent advances in theory and methods. In this paper, we review telegraphically the literature on extremes of dependent time series and list some remaining challenges.},
   author = {V Chavez-Demoulin and A C Davison},
   issue = {1},
   journal = {REVSTAT-Statistical Journal},
   keywords = {62,Box-Cox transformation,Hill estimator,Key-Words: • Bayesian statistics,clustering,dependence,extremal index,extremogram,generalized Pareto distribution,generalized extreme-value distribution,non-stationarity,nonparametric smoothing,regression,tail index AMS Subject Classification: • 62E20},
   pages = {109-133},
   title = {Modelling time series extremes},
   volume = {10},
   url = {http://infoscience.epfl.ch/record/180506},
   year = {2012},
}
@article{Xi2019,
   abstract = {Fire danger systems have evolved from qualitative indices, to process-driven deterministic models of fire behavior and growth, to data-driven stochastic models of fire occurrence and simulation systems. However, there has often been little overlap or connectivity in these frameworks, and validation has not been common in deterministic models. Yet, marked increases in annual fire costs, losses, and fatality costs over the past decade draw attention to the need for better understanding of fire risk to support fire management decision making through the use of science-backed, data-driven tools. Contemporary risk modeling systems provide a useful integrative framework. This article discusses a variety of important contributions for modeling fire risk components over recent decades, certain key fire characteristics that have been overlooked, and areas of recent research that may enhance risk models.},
   author = {Dexen D.Z. Xi and Stephen W. Taylor and Douglas G. Woolford and C. B. Dean},
   doi = {10.1146/annurev-statistics-031017-100450},
   issn = {2326831X},
   journal = {Annual Review of Statistics and Its Application},
   keywords = {fire duration,fire load,fire occurrence,fire survival,joint modeling,wildfire management,wildfire science,wildland fire},
   pages = {197-222},
   title = {Statistical models of key components of wildfire risk},
   volume = {6},
   year = {2019},
}
@article{Castro-Camilo2018,
   abstract = {Extremal dependence between international stock markets is of particular interest in today’s global financial landscape. However, previous studies have shown this dependence is not necessarily stationary over time. We concern ourselves with modeling extreme value dependence when that dependence is changing over time, or other suitable covariate. Working within a framework of asymptotic dependence, we introduce a regression model for the angular density of a bivariate extreme value distribution that allows us to assess how extremal dependence evolves over a covariate. We apply the proposed model to assess the dynamics governing extremal dependence of some leading European stock markets over the last three decades, and find evidence of an increase in extremal dependence over recent years.},
   author = {Daniela Castro-Camilo and Miguel de Carvalho and Jennifer Wadsworth},
   doi = {10.1214/17-AOAS1089},
   issn = {19417330},
   issue = {1},
   journal = {Annals of Applied Statistics},
   keywords = {Angular measure,Bivariate extreme values,European stock market integration,Risk,Statistics of extremes},
   pages = {283-309},
   title = {Time-varying extreme value dependence with application to leading European stock markets},
   volume = {12},
   year = {2018},
}
@article{Manuel2018,
   abstract = {It is necessary to evaluate site-specific extreme environmental conditions in the design of wave energy converters (WECs) as well as other offshore structures. As WECs are generally resonance-driven devices, critical metocean parameters associated with a target return period of interest (e.g., 50 years) must generally be established using combinations, say, of significant wave height and spectral peak period, as opposed to identifying single-valued wave height levels alone. We present several methods for developing so-called “environmental contours” for any target return period. The environmental contour (EC) method has been widely acknowledged as an efficient way to derive design loads for offshore oil and gas platforms and for land-based as well as offshore wind turbines. The use of this method for WECs is also being considered. A challenge associated with its use relates to the need to accurately characterize the uncertainties in metocean variables that define the “environment”. The joint occurrence frequency of values of two or more random variables needs to be defined formally. There are many ways this can be done—the most thorough and complete of these is to define a multivariate joint probability distribution of the random variables. However, challenges arise when data from the site where the WEC device is to be deployed are limited, making it difficult to estimate the joint probability distribution. A more easily estimated set of inputs consists of marginal distribution functions for each random variable and pairwise correlation coefficients. Pearson correlation coefficients convey information that rely on up to the second moment of each variable and on the expected value of the product of the paired variables. Kendall’s rank correlation coefficients, on the other hand, convey information on similarity in the “rank” of two variables and are useful especially in dealing with extreme values. The EC method is easily used with Rosenblatt transformations when joint distributions are available. In cases where Pearson’s correlation coefficients have been estimated along with marginal distributions, a Nataf transformation can be used, and if Kendall’s rank coefficients have been estimated and are available, a copula-based transformation can be used. We demonstrate the derivation of 50-year sea state parameters using the EC method with all three approaches where we consider data from the National Data Buoy Center Station 46022 (which can be considered the site for potential WEC deployment). A comparison of the derived environmental contours using the three approaches is presented. The focus of this study is on investigating differences between the derived environmental contours and, thus, on associated sea states arising from the different dependence structure assumptions for the metocean random variables. Both parametric and non-parametric approaches are used to define the probability distributions.},
   author = {Lance Manuel and Phong T.T. Nguyen and Jarred Canning and Ryan G. Coe and Aubrey C. Eckert-Gallup and Nevin Martin},
   doi = {10.1007/s40722-018-0123-0},
   issn = {21986452},
   issue = {4},
   journal = {Journal of Ocean Engineering and Marine Energy},
   keywords = {Environmental contour method,Extremes,Metocean data,copula},
   pages = {293-310},
   publisher = {Springer International Publishing},
   title = {Alternative approaches to develop environmental contours from metocean data},
   volume = {4},
   url = {https://doi.org/10.1007/s40722-018-0123-0},
   year = {2018},
}
@article{Gouldby2017,
   abstract = {It is widely recognised that coastal flood events can arise from combinations of extreme waves and sea levels. For flood risk analysis and the design of coastal structures it is therefore necessary to assess the joint probability of the occurrence of these variables. Traditional methods have involved the application of joint probability contours, defined in terms of extremes of sea conditions that can, if applied without correction factors, lead to the underestimation of flood risk and under-design of coastal structures. This paper describes the application of a robust multivariate statistical model to analyse extreme offshore waves, wind and sea levels around the coast of England. The approach described here is risk based in that it seeks to define extremes of response variables directly, rather than the joint extremes of sea conditions. The output of the statistical model comprises a Monte Carlo simulation of extreme events. These distributions of extreme events have been transformed from offshore to nearshore using a statistical emulator of a wave transformation model. The resulting nearshore extreme sea condition distributions have the potential to be applied for a range of purposes. The application is demonstrated using two structures located on the south coast of England.},
   author = {Ben Gouldby and David Wyncoll and Mike Panzeri and Mark Franklin and Tim Hunt and Dominic Hames and Nigel Tozer and Peter Hawkes and Uwe Dornbusch and Tim Pullen},
   doi = {10.1680/jmaen.2016.16},
   issn = {17517737},
   issue = {1},
   journal = {Proceedings of the Institution of Civil Engineers: Maritime Engineering},
   keywords = {Floods & floodworks,Maritime engineering,Risk & probability analysis},
   pages = {3-20},
   title = {Multivariate extreme value modelling of sea conditions around the coast of England},
   volume = {170},
   year = {2017},
}
@article{DNV2011,
   abstract = {A three-dimensional rotational failure mechanism for earth slope is extended from toe failure to include face failure and base failure. An efficient optimisation method is simultaneously employed to find the least upper bounds to the critical height in order to avoid missing the global minimum. Compared with the results from analysis based on toe failure alone, best estimates of the critical height and the critical failure mechanism are obtained. The calculated results are given in the form of graphs and tables for a wide range of parameters. The critical failure surfaces are also investigated to assess the influences of geometrical constraint and soil property on failure mechanism.},
   author = {DNV},
   isbn = {DNV-OS-C101},
   issue = {April},
   journal = {Det Norske Veritas},
   pages = {49},
   title = {Design of Offshore Steel Structures , General - LRFD Method},
   volume = {2018 Ed.},
   year = {2011},
}
@article{Valamanesh2015,
   abstract = {Most offshore wind turbines (OWTs) are designed according to the international standard IEC 61400-3 which requires consideration of several design load cases under 50-year extreme storm conditions during which the wind turbine is not operational (i.e. the rotor is parked and blades are feathered). Each of these load cases depends on combinations of at least three jointly distributed metocean parameters, the mean wind speed, the significant wave height, and the peak spectral period. In practice, these variables are commonly estimated for the 50-year extreme storm using a simple but coarse method, wherein 50-year values of wind speed and wave height are calculated independently and combined with a range of peak spectral period conditioned on the 50-year wave height. The IEC Standard does not provide detailed guidance on how to calculate the appropriate range of peak spectral period. Given the varying correlation of these parameters from site-to-site, this approach is clearly an approximation which is assumed to overestimate structural loads since wind and wave are combined without regard to their correlation. In this paper, we introduce an alternative multivariate method for assessing extreme storm conditions. The method is based on the Nataf model and the Inverse First Order Reliability Method (IFORM) and uses measurements or hindcasts of wind speed, wave height and peak spectral period to estimate an environmental surface which defines combinations of these parameters with a particular recurrence period. The method is illustrated using three sites along the U.S. Atlantic coast near Maine, Delaware and Georgia. Mudline moments are calculated using this new multivariate method for a hypothetical 5. MW OWT supported by a monopile and compared with mudline moments calculated using simpler univariate approaches. The results of the comparison highlight the importance of selecting an appropriate range of the peak spectral period when using the simpler univariate approaches.},
   author = {V. Valamanesh and A. T. Myers and S. R. Arwade},
   doi = {10.1016/j.strusafe.2015.03.002},
   issn = {01674730},
   journal = {Structural Safety},
   keywords = {Extreme value analysis,Inverse First Order Reliability Method,Multivariate Metocean Hazard,Offshore wind turbine},
   pages = {60-69},
   title = {Multivariate analysis of extreme metocean conditions for offshore wind turbines},
   volume = {55},
   year = {2015},
}
@article{Eckert-Gallup2016,
   abstract = {The estimation of environmental contours of extreme sea states characterized by significant wave height and energy period for the purposes of reliability-based offshore design is a problem that has been tackled in many different ways. Many of the methods used to generate such contours rely on parametric approaches that require an a priori assumption of the relationship between the variables of interest. These relationships, often given in the form of assumed probability distributions or joint probability structures, may not be flexible across a wide variety of global observation sites. We propose the use of bivariate kernel density estimation (KDE) with adaptive bandwidth selection for generating the joint probability distribution of significant wave height and energy period. This method is nonparametric, straightforward to apply, and lends itself to a characterization of contour uncertainty that is an important aspect when contours are used to generate inputs for numerical or physical simulations of offshore structures. The joint probability distribution of significant wave height and energy period can be queried using a return period of interest to determine environmental contours of extreme sea states. This paper demonstrates that this method provides a robust and flexible characterization when compared to other approaches.},
   author = {Aubrey Eckert-Gallup and Nevin Martin},
   doi = {10.1109/OCEANS.2016.7761150},
   isbn = {978-1-5090-1537-5},
   journal = {In: OCEANS 2016 MTS/IEEE. IEEE, Monterey, CA, USA},
   keywords = {Adaptive bandwidth selection,Environmental contours,Extreme sea state characterization,Kernel density estimation},
   month = {9},
   pages = {1-5},
   publisher = {IEEE},
   title = {Kernel density estimation (KDE) with adaptive bandwidth selection for environmental contours of extreme sea states},
   url = {http://ieeexplore.ieee.org/document/7761150/},
   year = {2016},
}
@article{Velarde2019,
   abstract = {Offshore wind turbines can exhibit dynamic resonant behavior due to sea states with wave excitation frequencies coinciding with the structural eigenfrequencies. In addition to significant contributions to fatigue actions, dynamic load amplification can govern extreme wind turbine responses. However, current design requirements lack specifications for assessment of resonant loads, particularly during parked or idling conditions where aerodynamic damping contributions are significantly reduced. This study demonstrates a probabilistic approach for assessment of offshore wind turbines under extreme resonant responses during parked situations. Based on in-situ metocean observations on the North Sea, the environmental contour method is used to establish relevant design conditions. A case study on a feasible large monopile design showed that resonant loads can govern the design loads. The presented framework can be applied to assess the reliability of wave-sensitive offshore wind turbine structures for a given site-specific metocean conditions and support structure design.},
   author = {Joey Velarde and Erik Vanem and Claus Kramhøft and John Dalsgaard Sørensen},
   doi = {10.1016/j.apor.2019.101947},
   issn = {01411187},
   journal = {Applied Ocean Research},
   keywords = {Dynamic response,Environmental contour method,Marine structures,Offshore wind turbines,Probabilistic design,Reliability analysis},
   pages = {1-16},
   publisher = {Elsevier},
   title = {Probabilistic analysis of offshore wind turbines under extreme resonant response: Application of environmental contour method},
   volume = {93},
   url = {https://doi.org/10.1016/j.apor.2019.101947},
   year = {2019},
}
@article{Eckert2021,
   abstract = {Environmental contours of extreme sea states are often utilized for the purposes of reliability-based offshore design. Many methods have been proposed to estimate environmental contours of extreme sea states, including, but not limited to, the traditional inverse first-order reliability method (I-FORM) and subsequent modifications, copula methods, and Monte Carlo methods. These methods differ in terms of both the methodology selected for defining the joint distribution of sea state parameters and in the method used to construct the environmental contour from the joint distribution. It is often difficult to compare the results of proposed methods to determine which method should be used for a particular application or geographical region. The comparison of the predictions from various contour methods at a single site and across many sites is important to making environmental contours of extreme sea states useful in practice. The goal of this paper is to develop a comparison framework for evaluating methods for developing environmental contours of extreme sea states. This paper develops generalized metrics for comparing the performance of contour methods to one another across a collection of study sites, and applies these metrics and methods to develop conclusions about trends in the wave resource across geographic locations, as demonstrated for a pilot dataset. These proposed metrics and methods are intended to judge the environmental contours themselves relative to other contour methods, and are thus agnostic to a specific device, structure, or field of application. The metrics developed and applied in this paper include measures of predictive accuracy, physical validity, and aggregated temporal performance that can be used to both assess contour methods and provide recommendations for the use of certain methods in various geographical regions. The application and aggregation of the metrics proposed in this paper outline a comparison framework for environmental contour methods that can be applied to support design analysis workflows for offshore structures. This comparison framework could be extended in future work to include additional metrics of interest, potentially including those to address issues pertinent to a specific application area or analysis discipline, such as metrics related to structural response across contour methods or additional physics-based metrics based on wave dynamics.},
   author = {Aubrey Eckert and Nevin Martin and Ryan G. Coe and Bibiana Seng and Zacharia Stuart and Zachary Morrell},
   doi = {10.3390/jmse9010016},
   issn = {20771312},
   issue = {1},
   journal = {Journal of Marine Science and Engineering},
   keywords = {Comparison framework,Environmental contours,Extreme sea state characterization,Physical validity,Predictive accuracy},
   pages = {1-24},
   title = {Development of a comparison framework for evaluating environmental contours of extreme sea states},
   volume = {9},
   year = {2021},
}
@book{Efron1994,
   author = {Bradley Efron and R.J. Tibshirani},
   pages = {456},
   publisher = {Chapman and Hall/CRC},
   title = {An Introduction to the Bootstrap - 1st Edition - Bradley Efron - R.J.},
   url = {https://www.routledge.com/An-Introduction-to-the-Bootstrap/Efron-Tibshirani/p/book/9780412042317},
   year = {1994},
}
@article{Serinaldi2015,
   abstract = {The concept of return period in stationary univariate frequency analysis is prone to misconceptions and misuses that are well known but still widespread. In this study we highlight how nonstationary and multivariate extensions of such a concept are affected by additional misconceptions, thus easily resulting in further ill-posed procedures and misleading conclusions. We also show that the concepts of probability of exceedance and risk of failure over a given design life period provide more coherent, general and well devised tools for risk assessment and communication.},
   author = {Francesco Serinaldi},
   doi = {10.1007/s00477-014-0916-1},
   issn = {14363259},
   issue = {4},
   journal = {Stochastic Environmental Research and Risk Assessment},
   keywords = {Copulas,Design life,Design values,Multivariate frequency analysis,Nonstationary frequency analysis,Return period,Risk of failure},
   pages = {1179-1189},
   publisher = {Springer Berlin Heidelberg},
   title = {Dismissing return periods!},
   volume = {29},
   url = {http://dx.doi.org/10.1007/s00477-014-0916-1},
   year = {2015},
}
@article{Vanem2020,
   abstract = {Environmental contours are often applied in probabilistic structural reliability analysis to identify extreme environmental conditions that may give rise to extreme loads and responses. They facilitate approximate long term analysis of critical structural responses in situations where computationally heavy and time-consuming response calculations makes full long-term analysis infeasible. The environmental contour method identifies extreme environmental conditions that are expected to give rise to extreme structural response of marine structures. The extreme responses can then be estimated by performing response calculations for environmental conditions along the contours. Response-based analysis is an alternative, where extreme value analysis is performed on the actual response rather than on the environmental conditions. For complex structures, this is often not practical due to computationally heavy response calculations. However, by establishing statistical emulators of the response, using machine learning techniques, one may obtain long time-series of the structural response and use this to estimate extreme responses. In this paper, various contour methods will be compared to response-based estimation of extreme vertical bending moment for a tanker. A response emulator based on Gaussian processes regression with adaptive sampling has been established based on response calculations from a hydrodynamic model. Long time-series of sea-state parameters such as significant wave height and wave period are used to construct N-year environmental contours and the extreme N-year response is estimated from numerical calculations for identified sea states. At the same time, the response emulator is applied on the time series to provide long time-series of structural response, in this case vertical bending moment of a tanker. Extreme value analysis is then performed directly on the responses to estimate the N-year extreme response. The results from either method will then be compared, and it is possible to evaluate the accuracy of the environmental contour method in estimating the response. Moreover, different contour methods will be compared.},
   author = {Erik Vanem and Bingjie Guo and Emma Ross and Philip Jonathan},
   doi = {10.1016/j.marstruc.2019.102680},
   issn = {09518339},
   journal = {Marine Structures},
   keywords = {Environmental contours,Environmental loads,Extreme ship response analysis,Marine structures,Ocean environment,Response-based methods,Structural reliability},
   pages = {1-19},
   publisher = {Elsevier Ltd},
   title = {Comparing different contour methods with response-based methods for extreme ship response analysis},
   volume = {69},
   url = {https://doi.org/10.1016/j.marstruc.2019.102680},
   year = {2020},
}
@article{Guerrero2021,
   abstract = {Epilepsy is a chronic neurological disorder affecting more than 50 million people globally. An epileptic seizure acts like a temporary shock to the neuronal system, disrupting normal electrical activity in the brain. Epilepsy is frequently diagnosed with electroencephalograms (EEGs). Current methods study the time-varying spectra and coherence but do not directly model changes in extreme behavior. Thus, we propose a new approach to characterize brain connectivity based on the joint tail behavior of the EEGs. Our proposed method, the conditional extremal dependence for brain connectivity (Conex-Connect), is a pioneering approach that links the association between extreme values of higher oscillations at a reference channel with the other brain network channels. Using the Conex-Connect method, we discover changes in the extremal dependence driven by the activity at the foci of the epileptic seizure. Our model-based approach reveals that, pre-seizure, the dependence is notably stable for all channels when conditioning on extreme values of the focal seizure area. Post-seizure, by contrast, the dependence between channels is weaker, and dependence patterns are more "chaotic". Moreover, in terms of spectral decomposition, we find that high values of the high-frequency Gamma-band are the most relevant features to explain the conditional extremal dependence of brain connectivity.},
   author = {Matheus B. Guerrero and Raphaël Huser and Hernando Ombao},
   doi = {10.1214/22-AOAS1621},
   issn = {1932-6157},
   issue = {1},
   journal = {The Annals of Applied Statistics},
   month = {3},
   pages = {178-198},
   title = {Conex–Connect: Learning patterns in extremal brain connectivity from MultiChannel EEG data},
   volume = {17},
   url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-17/issue-1/ConexConnect--Learning-patterns-in-extremal-brain-connectivity-from-MultiChannel/10.1214/22-AOAS1621.full},
   year = {2023},
}
@article{Eckert-gallup2017,
   author = {Aubrey Eckert-gallup and Nevin Martin},
   title = {New Methods for Calculating Environmental Contours of Extreme Sea States The purpose of this work is to develop a new methodology for determining values of parameters describing extreme sea states that can be used in survivability models for wave energy P},
   year = {2017},
}
@article{Mackay2021,
   abstract = {Various methods have been proposed for defining an environmental contour, based on different concepts of exceedance probability. In the inverse first-order reliability method (IFORM) and the direct sampling (DS) method, contours are defined in terms of exceedances within a region bounded by a hyperplane in either standard normal space or the original parameter space, corresponding to marginal exceedance probabilities under rotations of the coordinate system. In contrast, the more recent inverse second-order reliability method (ISORM) and highest density (HD) contours are defined in terms of an isodensity contour of the joint density function in either standard normal space or the original parameter space, where an exceedance is defined to be anywhere outside the contour. Contours defined in terms of the total probability outside the contour are significantly more conservative than contours defined in terms of marginal exceedance probabilities. In this work we study the relationship between the marginal exceedance probability of the maximum value of each variable along an environmental contour and the total probability outside the contour. The marginal exceedance probability of the contour maximum can be orders of magnitude lower than the total exceedance probability of the contour, with the differences increasing with the number of variables. For example, a 50-year ISORM contour for two variables at 3-h time steps, passes through points with marginal return periods of 635 years, and the marginal return periods increase to 10,950 years for contours of four variables. It is shown that the ratios of marginal to total exceedance probabilities for DS contours are similar to those for IFORM contours. However, the marginal exceedance probabilities of the maximum values of each variable along an HD contour are not in fixed relation to the contour exceedance probability, but depend on the shape of the joint density function. Examples are presented to illustrate the impact of the choice of contour on simple structural reliability problems for cases where the use of contours defined in terms of either marginal or total exceedance probabilities may be appropriate. The examples highlight that to choose an appropriate contour method, some understanding about the shape of a structure's failure surface is required.},
   author = {Ed Mackay and Andreas F. Haselsteiner},
   doi = {10.1016/j.marstruc.2020.102863},
   issn = {09518339},
   issue = {September 2020},
   journal = {Marine Structures},
   keywords = {Direct sampling contour,Directional design criteria,Environmental contour,Extremes,Highest density contour,IFORM,ISORM,Joint distribution,Return value},
   month = {1},
   pages = {1-24},
   publisher = {Elsevier Ltd},
   title = {Marginal and total exceedance probabilities of environmental contours},
   volume = {75},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0951833920301568},
   year = {2021},
}
@article{Jonathan2020,
   author = {Philip Jonathan},
   keywords = {extremes,joint distribution,metocean},
   title = {OMAE2020-18308 ESTIMATION OF ENVIRONMENTAL CONTOURS USING A BLOCK RESAMPLING},
   year = {2020},
}
@article{VanDeLindt2000,
   abstract = {In many engineering problems the uncertainty in the design and analysis process is dominated by uncertainty concerning the environmentally induced loading on the system. One such type of system is a structure located in high seismic zones, where there is a high level of uncertainty associated with the ground acceleration. In this study the demand caused by the environmental loads is statistically characterized in terms of magnitude, site-to-source distance and attenuation error at a specified structural period. Through the use of a reliability-based procedure known as the Inverse-First Order Reliability Method all of the combinations of the random loading variables that produce a response spectrum with the specified return period may be identified. These infinite number of combinations produce an Environmental Contour that may be derived for the two, three, or four-dimensional case. Because these contours represent an infinite number of combinations of environmental loading random variables, and in turn, a family of response spectra with the same return period, one need only search the Environmental Contour for the response spectrum producing the peak spectral acceleration at the structural period of interest. This study present a general multi-variate framework focusing on the derivation of these two, three, and four-dimensional Environmental Contours. Initially, the magnitude and site-to-source distance are assumed to be statistically independent. Two and three-dimensional Environmental Contours are derived for this case and critical response spectra for three different, commonly used return periods are examined. Then a hypothetical example in which the site-to-source distance is assumed LogNormal and dependent on magnitude, is used as a basis for discussion for a generic California site. A qualitative discussion between the assumption of independence and dependence between magnitude and site-to-source distance is addressed. Finally, the peak spectral accelerations for the dependent two, three, and four-dimensional cases are compared to one another and the possible consequences associated with increasing the complexity of the environmental contour model analyzed and discussed. (C) 2000 Elsevier Science Ltd. All rights reserved.},
   author = {J. W. Van De Lindt and J. M. Niedzwecki},
   doi = {10.1016/S0141-0296(99)00114-5},
   issn = {01410296},
   issue = {12},
   journal = {Engineering Structures},
   keywords = {Attenuation relations,Earthquake engineering,Environmental contours,Magnitude,Reliability,Response spectrum,Site-to-source distance},
   pages = {1661-1676},
   title = {Environmental contour analysis in earthquake engineering},
   volume = {22},
   year = {2000},
}
@article{Jonathan2014,
   abstract = {Understanding extreme ocean environments and their interaction with fixed and floating structures is critical for offshore and coastal design. Design contours are useful to describe the joint behavior of environmental, structural loading, and response variables. We compare different forms of design contours, using theory and simulation, and present a new method for joint estimation of contours of constant exceedance probability for a general set of variables. The method is based on a conditional extremes model from the statistics literature, motivated by asymptotic considerations. We simulate under the conditional extremes model to estimate contours of constant exceedance probability. We also use the estimated conditional extremes model to estimate other forms of design contours, including those based on the first-order reliability method (FORM), without needing to specify the functional forms of conditional dependence between variables. We demonstrate the application of new method in estimation of contours of constant exceedance probability using measured and hindcast data from the Northern North Sea, the Gulf of Mexico, and the North West Shelf of Australia, and quantify their uncertainties using a bootstrap analysis.},
   author = {Philip Jonathan and Kevin Ewans and Jan Flynn},
   doi = {10.1115/1.4027645},
   issn = {1528896X},
   issue = {4},
   journal = {Journal of Offshore Mechanics and Arctic Engineering},
   pages = {1-8},
   title = {On the estimation of ocean engineering design contours},
   volume = {136},
   year = {2014},
}
@article{Haver2004,
   abstract = {Methods of prediction of structural loads corresponding to a required target annual exceedance probability are reviewed. Particular attention is given to utilization of environmental contour lines for such a purpose. This approach is based on using short term methods for predicting adequate estimates of the q-probability response. The environmental contour line approach is a very convenient approach if complicated structural problems are considered. For such problems one will often have to involve numerical time domain analyses or model tests to reveal the short term probabilistic structure of the response maxima, making a full long term response analysis impossible for most practical problems.},
   author = {Sverre Haver and Gudmund Kleiven},
   doi = {10.1115/OMAE2004-51157},
   issue = {2},
   journal = {Proceedings of the International Conference on Offshore Mechanics and Arctic Engineering - OMAE},
   keywords = {Design values,Environmental contour lines,Extreme values,Long term analysis,Short term analysis},
   pages = {337-345},
   title = {Environmental contour lines for design purposes - Why and when?},
   volume = {1 A},
   year = {2004},
}
@article{Ecsades,
   author = {Philip Jonathan},
   title = {Environmental contours : fundamentals and recommended practice},
}
@article{Haselsteiner2019,
   abstract = {A wide range of methods have been proposed for the derivation of environmental contours for marine structures that must meet reliability targets. An environmental contour is a set of joint extremes of environmental conditions associated with a target return period. In general, environmental contour methods help with the prediction of some future critical combinations of environmental conditions (e.g., wind, waves, current) at a location of interest based on a limited dataset, thus allowing designers to ensure a prescribed structural reliability. In fact, some of these contour methods are specifically recommended by technical specifications and standards as part of a design process. This paper outlines the rules and procedures for a collaborative benchmarking exercise - focused on open comparison - in which researchers are invited to develop and present their own contour derivation approaches based on common datasets that will be available to all. Hindcast and observational datasets are considered and two exercises are planned: One focuses on applying environmental contour methods to a wide range of datasets and the other focuses on uncertainty characterization. Besides describing the benchmark's methodology, this paper presents baseline results of computed contours following current recommendations. The overall goals of this endeavor are: (i) to work towards the development of more robust statistical models and contour construction methods, (ii) to encourage increased discussion in the international research community and among practitioners, and (iii) to support ongoing efforts to improve technical specifications and standards.},
   author = {Andreas F. Haselsteiner and Phong T.T. Nguyen and Ryan G. Coe and Nevin Martin and Lance Manuel and Aubrey Eckert-Gallup},
   doi = {10.1115/OMAE2019-96523},
   isbn = {9780791858783},
   journal = {Proceedings of the International Conference on Offshore Mechanics and Arctic Engineering - OMAE},
   keywords = {OMAE2019-96523},
   pages = {1-10},
   title = {A benchmarking exercise on estimating extreme environmental conditions: Methodology and Baseline results},
   volume = {3},
   year = {2019},
}
@article{Boldi2007,
   abstract = {The spectral density function plays a key role in fitting the tail of multivariate extre-mal data and so in estimating probabilities of rare events. This function satisfies moment con-straints but unlike the univariate extreme value distributions has no simple parametric form. Parameterized subfamilies of spectral densities have been suggested for use in applications, and non-parametric estimation procedures have been proposed, but semiparametric models for multivariate extremes have hitherto received little attention. We show that mixtures of Dirichlet distributions satisfying the moment constraints are weakly dense in the class of all non-parametric spectral densities, and discuss frequentist and Bayesian inference in this class based on the EM algorithm and reversible jump Markov chain Monte Carlo simulation. We illustrate the ideas using simulated and real data. © 2007 Royal Statistical Society.},
   author = {M. O. Boldi and A. C. Davison},
   doi = {10.1111/j.1467-9868.2007.00585.x},
   issn = {13697412},
   issue = {2},
   journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
   keywords = {Adequacy,Air pollution data,Dirichlet distribution,EM algorithm,Multivariate extreme values,Oceanographic data,Reversible jump Markov chain Monte Carlo simulatio,Spectral distribution},
   pages = {217-229},
   title = {A mixture model for multivariate extremes},
   volume = {69},
   year = {2007},
}
@article{Einmahl2009,
   abstract = {Consider a random sample from a bivariate distribution function F in the max-domain of attraction of an extreme-value distribution function G.This G is characterized by two extreme-value indices and a spectral measure, the latter determining the tail dependence structure of F. A major issue in multivariate extreme-value theory is the estimation of the spectral measure &p with respect to the Lp norm. For every p [1, oo], a nonparametric maximum empirical likelihood estimator is proposed for &p. The main novelty is that these estimators are guaranteed to satisfy the moment constraints by which spectral measures are characterized. Asymptotic normality of the estimators is proved under conditions that allow for tail independence. Moreover, the conditions are easily verifiable as we demonstrate through a number of theoretical examples. A simulation study shows a substantially improved performance of the new estimators. Two case studies illustrate how to implement the methods in practice. © Institute of Mathematical Statistics, 2009.},
   author = {John H.J. Einmahl and Johan Segers},
   doi = {10.1214/08-AOS677},
   issn = {00905364},
   issue = {5 B},
   journal = {Annals of Statistics},
   keywords = {Functional central limit theorem,Local empirical process,Moment constraint,Multivariate extremes,National Health and Nutrition Examination Survey,Nonparametric maximum likelihood estimator,Tail dependence},
   pages = {2953-2989},
   title = {Maximum empirical likelihood estimation of the spectral measure of an extreme-value distribution},
   volume = {37},
   year = {2009},
}
@article{DeHaan1977,
   abstract = {Let \{Mathematical expression\} be k-dimensional iid random vectors. Necessary and sufficient conditions are found for the weak convergence of the maxima \{Mathematical expression\} suitably normed to a non-degenerate limit df. The class of such limits is specified and conditions stated for the limit joint df to be a product of marginal df's. Some results are presented concerning extremal processes generated by multivariate df's. © 1977 Springer-Verlag.},
   author = {Laurens de Haan and Sidney I. Resnick},
   doi = {10.1007/BF00533086},
   issn = {00443719},
   issue = {4},
   journal = {Zeitschrift für Wahrscheinlichkeitstheorie und Verwandte Gebiete},
   pages = {317-337},
   title = {Limit theory for multivariate sample extremes},
   volume = {40},
   year = {1977},
}
@article{Coles1991,
   author = {Stuart G Coles and Jonathan A. Tawn},
   issue = {2},
   journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
   keywords = {extreme value theory,generalized extreme value distribution},
   pages = {377-392},
   title = {Modelling Extreme Multivariate Events},
   volume = {53},
   year = {1991},
}
@article{Resnick2002,
   abstract = {We survey the related asymptotic properties of multivariate distributions; (i) asymptotic independence, (ii) hidden regular variation, and (iii) multivariate second order regular variation. Connections and implications are discussed. The point of view of convergence of measures is emphasized in formulations because we are interested in the concepts being coordinate system free, whenever possible.},
   author = {Sidney Resnick},
   doi = {10.1023/A:1025148622954},
   issn = {1386-1999},
   issue = {4},
   journal = {Extremes},
   keywords = {Analyse multivariable,Approximation asymptotique,Asymptotic approximation,Asymptotic behavior,Asymptotic distribution,Asymptotic independence,Asymptotic property,Comportement asymptotique,Convergence méthode numérique,Convergence of numerical methods,Distribution function,Distribution statistique,Independence,Multivariate analysis,Multivariate distribution,Regular variation,Sample survey,Statistical distribution,Statistics,Variations},
   pages = {303-336},
   title = {Hidden Regular Variation, Second Order Regular Variation and Asymptotic Independence},
   volume = {5},
   year = {2002},
}
@article{Mackay2020,
   abstract = {This article compares the accuracy of return value estimates from stationary and non-stationary extreme value models when the data exhibits covariate dependence. The non-stationary covariate representation used is a penalised piecewise-constant (PPC) model, in which the data are partitioned into bins defined by covariates and the extreme value distribution is assumed to be homogeneous within each bin. A generalised Pareto model is assumed, where the scale parameter can vary between bins but is penalised for the variance across bins, and the shape parameter is assumed constant over all covariate bins. The number and sizes of covariate bins must be defined by the user based on physical considerations. Numerical simulations are conducted to compare the performance of stationary and non-stationary models for various case studies, in terms of quality of estimation of the T-year return value over the full covariate domain. It is shown that a non-stationary model can give improved estimates of return values, provided that model assumptions are consistent with the data. When the data exhibits non-stationarity in the generalised Pareto tail shape, the use of non-stationary model assuming a constant shape parameter can produce biases in return values. In such cases, a stationary model can give a more accurate estimate of return value over the full covariate domain as only the most extreme observations (regardless of covariate) are used to estimate tail shape. In other cases, the assumption of a stationary model will ignore key features of the data and be less reliable than a non-stationary model. For example, if a relatively benign covariate interval exhibits a long (or heavy) tail, extreme values from this interval may influence the T-year return value for very large T. However the sample of peaks over threshold, with high threshold, used to estimate a stationary model in this case may not include sufficient observations from this interval to estimate the return value adequately.},
   author = {Ed Mackay and Philip Jonathan},
   doi = {10.1016/j.oceaneng.2020.107406},
   issn = {00298018},
   issue = {April},
   journal = {Ocean Engineering},
   keywords = {Covariate,Extreme,Generalised Pareto,Metocean,Non-stationary,Significant wave height},
   pages = {107406},
   publisher = {Elsevier Ltd},
   title = {Assessment of return value estimates from stationary and non-stationary extreme value models},
   volume = {207},
   url = {https://doi.org/10.1016/j.oceaneng.2020.107406},
   year = {2020},
}
@article{Simpson2021,
   abstract = {Vine copulas are a type of multivariate dependence model, composed of a collection of bivariate copulas that are combined according to a specific underlying graphical structure. Their flexibility and practicality in moderate and high dimensions have contributed to the popularity of vine copulas, but relatively little attention has been paid to their extremal properties. To address this issue, we present results on the tail dependence properties of some of the most widely studied vine copula classes. We focus our study on the coefficient of tail dependence and the asymptotic shape of the sample cloud, which we calculate using the geometric approach of Nolde (2014). We offer new insights by presenting results for trivariate vine copulas constructed from asymptotically dependent and asymptotically independent bivariate copulas, focusing on bivariate extreme value and inverted extreme value copulas, with additional detail provided for logistic and inverted logistic examples. We also present new theory for a class of higher dimensional vine copulas, constructed from bivariate inverted extreme value copulas.},
   author = {Emma S. Simpson and Jennifer L. Wadsworth and Jonathan A. Tawn},
   doi = {10.1016/j.jmva.2021.104736},
   issn = {10957243},
   journal = {Journal of Multivariate Analysis},
   keywords = {Coefficient of tail dependence,Gauge function,Multivariate extremes,Vine copula},
   pages = {104736},
   publisher = {Elsevier Inc.},
   title = {A geometric investigation into the tail dependence of vine copulas},
   volume = {184},
   url = {https://doi.org/10.1016/j.jmva.2021.104736},
   year = {2021},
}
@article{Tawn1988,
   abstract = {Bivariate extreme value distributions arise as the limiting distributions of renormalized componentwise maxima. No natural parametric family exists for the dependence between the marginal distributions, but there are considerable restrictions on the dependence structure. We consider modelling the dependence function with parametric models, for which two new models are presented. Tests for independence, and discriminating between models, are also given. The estimation procedure, and the flexibility of the new models, are illustrated with an application to sea level data. © 1988 Biometrika Trust.},
   author = {Jonathan A. Tawn},
   doi = {10.1093/biomet/75.3.397},
   issn = {00063444},
   issue = {3},
   journal = {Biometrika},
   keywords = {Bivariate exponential distribution,Extreme value theory,Maximum likelihood,Nonregular estimation,Stable distribution,Survival data},
   pages = {397-415},
   title = {Bivariate extreme value theory: Models and estimation},
   volume = {75},
   year = {1988},
}
@book{Pickles1985,
   author = {Andrew Pickles},
   doi = {10.1007/0-387-22764-4_3},
   isbn = {0860941906},
   publisher = {W. H. Hutchins \& Sons},
   title = {An Introduction to Likelihood Inference},
   url = {https://archive.org/details/introductiontoli0000pick/page/21/mode/2up},
   year = {1985},
}
@article{Rootzen2006,
   abstract = {Statistical inference for extremes has been a subject of intensive research over the past couple of decades. One approach is based on modelling exceedances of a random variable over a high threshold with the generalized Pareto (GP) distribution. This has proved to be an important way to apply extreme value theory in practice and is widely used. We introduce a multivariate analogue of the GP distribution and show that it is characterized by each of following two properties: first, exceedances asymptotically have a multivariate GP distribution if and only if maxima asymptotically are extreme value distributed; and second, the multivariate GP distribution is the only one which is preserved under change of exceedance levels. We also discuss a bivariate example and lower-dimensional marginal distributions. © 2006 ISI/BS. © 2006 Applied Probability Trust.},
   author = {Holger Rootzén and Nader Tajvidi},
   doi = {10.3150/bj/1161614952},
   issn = {13507265},
   issue = {5},
   journal = {Bernoulli},
   keywords = {Generalized Pareto distribution,Multivariate Pareto distribution,Multivariate extreme value theory,Non-homogeneous Poisson process,Peaks-over-threshold method},
   month = {1},
   pages = {917-930},
   publisher = {Bernoulli Society for Mathematical Statistics and Probability},
   title = {Multivariate generalized Pareto distributions},
   volume = {12},
   url = {https://projecteuclid.org/journals/bernoulli/volume-12/issue-5/Multivariate-generalized-Pareto-distributions/10.3150/bj/1161614952.full https://projecteuclid.org/journals/bernoulli/volume-12/issue-5/Multivariate-generalized-Pareto-distributions/10.3150/bj},
   year = {2006},
}
@article{Belzile2017,
   abstract = {Liouville copulas introduced in McNeil and Nešlehová (2010) are asymmetric generalizations of the ubiquitous Archimedean copula class. They are the dependence structures of scale mixtures of Dirichlet distributions, also called Liouville distributions. In this paper, the limiting extreme-value attractors of Liouville copulas and of their survival counterparts are derived. The limiting max-stable models, termed here the scaled extremal Dirichlet, are new and encompass several existing classes of multivariate max-stable distributions, including the logistic, negative logistic and extremal Dirichlet. As shown herein, the stable tail dependence function and angular density of the scaled extremal Dirichlet model have a tractable form, which in turn leads to a simple de Haan representation. The latter is used to design efficient algorithms for unconditional simulation based on the work of Dombry et al. (2016) and to derive tractable formulas for maximum-likelihood inference. The scaled extremal Dirichlet model is illustrated on river flow data of the river Isar in southern Germany.},
   author = {Léo R. Belzile and Johanna G. Nešlehová},
   doi = {10.1016/j.jmva.2017.05.008},
   issn = {10957243},
   issue = {5},
   journal = {Journal of Multivariate Analysis},
   keywords = {Extremal attractor,Extremal function,Liouville copula,Scaled extremal Dirichlet model,Stable tail dependence function,de Haan decomposition},
   pages = {68-92},
   title = {Extremal attractors of Liouville copulas},
   volume = {160},
   year = {2017},
}
@book{Chernozhukov2017,
   abstract = {In 1895, the Italian econometrician Vilfredo Pareto discovered that the power law describes well the tails of income and wealth data. This simple observation stimulated further applications of the power law to economic data, including Zipf (1949), Mandelbrot (1963), Fama (1965), Praetz (1972), Sen (1973), and Longin (1996), among many others. It also led to a theory to analyze the properties of the tails of the distributions, so-called extreme value (EV) theory, which was developed by Gnedenko (1943) and deHaan (1970). Jansen and de Vries (1991) applied this theory to analyze the tail properties of US financial returns and concluded that the 1987 market crash was not an outlier; rather, it was a rare event whose magnitude could have been predicted by prior data. This work stimulated numerous other studies that rigorously documented the tail properties of economic data (Embrechts et al., 1997).},
   author = {Victor Chernozhukov and Iván Fernández-Val and Tetsuya Kaji},
   doi = {10.1201/9781315120256},
   isbn = {9781315120256},
   issue = {2005},
   journal = {Handbook of Quantile Regression},
   month = {10},
   pages = {333-362},
   publisher = {Chapman and Hall/CRC},
   title = {Handbook of Quantile Regression Chapter 18 - Extremal Quantile Regression},
   url = {https://www.taylorfrancis.com/books/9781498725293},
   year = {2017},
}
@article{Mackay2020a,
   abstract = {A new method for estimating joint distributions of environmental variables is presented. The key difference to previous methods is that the joint distribution of only storm-peak parameters is modelled, rather than fitting a model to all observations. This provides a stronger justification for the use of asymptotic extreme value models, as the data considered are approximately independent. The joint distribution of all data is recovered by resampling and rescaling storm histories, conditional on the peak values. This simplifies the analysis as much of the complex dependence structure is resampled, rather than modelled explicitly. The storm histories are defined by splitting the time series into discrete blocks, with the dividing points defined as the minimum value of a variable between adjacent maxima. Storms are characterised in terms of the peak values of each parameter within each discrete block, which need not coincide in time. The key assumption is that rescaling a measured storm history results in an equally realistic time series, provided that the change in peak values is not large. Two examples of bivariate distribution are considered: the joint distribution of significant wave height (Hs) and zero up-crossing period (Tz) and the joint distribution of Hs and wind speed. It is shown that the storm resampling method gives estimates of environmental contours that agree well with the observations and provides a rigorous method for estimating extreme values.},
   author = {Ed B.L. Mackay and Philip Jonathan},
   doi = {10.1115/omae2020-18308},
   isbn = {9780791884324},
   issue = {March},
   journal = {Proceedings of the International Conference on Offshore Mechanics and Arctic Engineering - OMAE},
   keywords = {Extremes,Joint distribution,Metocean},
   title = {Estimation of environmental contours using a block resampling method},
   volume = {2A-2020},
   year = {2020},
}
@article{Aitkin1982,
   author = {Murray Aitkin},
   doi = {10.1007/978-1-4612-5771-4_8},
   journal = {GLIM 82: Proceedings of the International Conference on Generalised Linear Models},
   pages = {76-86},
   publisher = {Springer, New York, NY},
   title = {Direct Likelihood Inference},
   url = {https://link.springer.com/chapter/10.1007/978-1-4612-5771-4_8},
   year = {1982},
}
@article{Smith1985,
   abstract = {We consider maximum likelihood estimation of the parameters of a probability density which is zero for x < θ and asymptotically αc(x-θ)α-1 as x ↓ θ. Here θ and other parameters, which may or may not include α and c1 are unknown. The classical regularity conditions for the asymptotic properties of maximum likelihood estimators are not satisfied but it is shown that, when α> 2, the information matrix is finite and the classical asymptotic properties continue to hold. For α= 2 the maximum likelihood estimators are asymptotically efficient and normally distributed, but with a different rate of convergence. For 1 < α < 2, the maximum likelihood estimators exist in general, but are not asymptotically normal, while the question of asymptotic efficiency is still unsolved. For α < 1, the maximum likelihood estimators may not exist at all, but alternatives are proposed. All these results are already known for the case of a single unknown location parameter θ, but are here extended to the case in which there are additional unknown parameters. The paper concludes with a discussion of the applications in extreme value theory. © 1985 Biometrika Trust.},
   author = {Richard L. Smith},
   doi = {10.1093/biomet/72.1.67},
   issn = {00063444},
   issue = {1},
   journal = {Biometrika},
   keywords = {Extreme value theory,Maximum likelihood,Nonregular estimation,Stable distribution,Weibull distribution},
   pages = {67-90},
   title = {Maximum likelihood estimation in a class of nonregular cases},
   volume = {72},
   year = {1985},
}
@article{Chernozhukov2004,
   author = {V Chernozhukov},
   keywords = {2002,2005,c13,c14,c21,c41,c51,c53,data and software in,date,jel,may,preliminary revision,quantile regression,r available by request,revised january,this is still a},
   title = {Inference for Extremal Conditional Quantile Models},
   year = {2004},
}
@article{Chernozhukov2011,
   abstract = {Quantile regression (QR) is an increasingly important empirical tool in economics and other sciences for analysing the impact a set of regressors has on the conditional distribution of an outcome. Extremal QR, or QR applied to the tails, is of interest in many economic and financial applications, such as conditional value at risk, production efficiency, and adjustment bands in (S, s) models. This paper provides feasible inference tools for extremal conditional quantile models that rely on extreme value approximations to the distribution of self-normalized QR statistics. The methods are simple to implement and can be of independent interest even in the univariate (non-regression) case. We illustrate the results with two empirical examples analysing extreme fluctuations of a stock return and extremely low percentiles of live infant birthweight in the range between 250 and 1500 g. © The Author 2011. Published by Oxford University Press on behalf of The Review of Economic Studies Limited.},
   author = {Victor Chernozhukov and Iván Fernández-Val},
   doi = {10.1093/restud/rdq020},
   issn = {1467937X},
   issue = {2},
   journal = {Review of Economic Studies},
   keywords = {Birthweights,Extreme value theory,Feasible inference,Market risk,Quantile regression,Stress testing,Systemic risk},
   pages = {559-589},
   title = {Inference for extremal conditional quantile models, with an application to market and birthweight risks},
   volume = {78},
   year = {2011},
}
@article{Society2020,
   author = {Royal Statistical Society},
   issue = {1},
   keywords = {extremal index,extreme value theory,generalized extreme value distribution,joint,maximum likelihood estimation,non-stationarity,probabilities method,sea-level,tides},
   pages = {77-93},
   title = {Estimating Probabilities of Extreme Sea-Levels Author ( s ): Jonathan A . Tawn Source : Journal of the Royal Statistical Society . Series C ( Applied Statistics ), Vol . 41 , No . 1 Published by : Wiley for the Royal Statistical Society Stable URL : https},
   volume = {41},
   year = {2020},
}
@article{Huser2016,
   abstract = {Max-stable processes are natural models for spatial extremes because they provide suitable asymptotic approximations to the distribution of maxima of random fields. In the recent past, several parametric families of stationary max-stable models have been developed, and fitted to various types of data. However, a recurrent problem is the modeling of non-stationarity. In this paper, we develop non-stationary max-stable dependence structures in which covariates can be easily incorporated. Inference is performed using pairwise likelihoods, and its performance is assessed by an extensive simulation study based on a non-stationary locally isotropic extremal t model. Evidence that unknown parameters are well estimated is provided, and estimation of spatial return level curves is discussed. The methodology is demonstrated with temperature maxima recorded over a complex topography. Models are shown to satisfactorily capture extremal dependence.},
   author = {Raphaël Huser and Marc G. Genton},
   doi = {10.1007/s13253-016-0247-4},
   issn = {15372693},
   issue = {3},
   journal = {Journal of Agricultural, Biological, and Environmental Statistics},
   keywords = {Covariate,Extremal t model,Extreme event,Max-stable process,Non-stationarity},
   pages = {470-491},
   publisher = {Springer US},
   title = {Non-Stationary Dependence Structures for Spatial Extremes},
   volume = {21},
   year = {2016},
}
@article{Marcon2017a,
   abstract = {Many applications in risk analysis require the estimation of the dependence among multivariate maxima, especially in environmental sciences. Such dependence can be described by the Pickands dependence function of the underlying extreme-value copula. Here, a nonparametric estimator is constructed as the sample equivalent of a multivariate extension of the madogram. Shape constraints on the family of Pickands dependence functions are taken into account by means of a representation in terms of Bernstein polynomials. The large-sample theory of the estimator is developed and its finite-sample performance is evaluated with a simulation study. The approach is illustrated with a dataset of weekly maxima of hourly rainfall in France recorded from 1993 to 2011 at various weather stations all over the country. The stations are grouped into clusters of seven stations, where our interest is in the extremal dependence within each cluster.},
   author = {G. Marcon and S. A. Padoan and P. Naveau and P. Muliere and J. Segers},
   doi = {10.1016/j.jspi.2016.10.004},
   issn = {03783758},
   journal = {Journal of Statistical Planning and Inference},
   keywords = {Bernstein polynomials,Extremal dependence,Extreme-value copula,Heavy rainfall,Multivariate max-stable distribution,Nonparametric estimation,Pickands dependence function},
   pages = {1-17},
   publisher = {Elsevier B.V.},
   title = {Multivariate nonparametric estimation of the Pickands dependence function using Bernstein polynomials},
   volume = {183},
   url = {http://dx.doi.org/10.1016/j.jspi.2016.10.004},
   year = {2017},
}
@article{Saunders2021,
   abstract = {To mitigate the risk posed by extreme rainfall events, we require statistical models that reliably capture extremes in continuous space with dependence. However, assuming a stationary dependence structure in such models is often erroneous, particularly over large geographical domains. Furthermore, there are limitations on the ability to fit existing models, such as max-stable processes, to a large number of locations. To address these modelling challenges, we present a regionalisation method that partitions stations into regions of similar extremal dependence using clustering. To demonstrate our regionalisation approach, we consider a study region of Australia and discuss the results with respect to known climate and topographic features. To visualise and evaluate the effectiveness of the partitioning, we fit max-stable models to each of the regions. This work serves as a prelude to how one might consider undertaking a project where spatial dependence is non-stationary and is modelled on a large geographical scale.},
   author = {K. R. Saunders and A. G. Stephenson and D. J. Karoly},
   doi = {10.1007/s10687-020-00395-y},
   issn = {1572915X},
   issue = {2},
   journal = {Extremes},
   keywords = {60G70,62D05,62G32,62P12,Climate extremes,Clustering,Extremal dependence,Spatial dependence},
   pages = {215-240},
   publisher = {Extremes},
   title = {A regionalisation approach for rainfall based on extremal dependence},
   volume = {24},
   year = {2021},
}
@book{DeValk2016,
   abstract = {This article discusses modelling of the tail of a multivariate distribution function by means of a large deviation principle (LDP), and its application to the estimation of the probability pn of a multivariate extreme event from a sample of niid random vectors, with pn?[n-t2,n-t1] for some t1>1 and t2>t1. One way to view the classical tail limits is as limits of probability ratios. In contrast, the tail LDP provides asymptotic bounds or limits for log-probability ratios. After standardising the marginals to standard exponential, tail dependence is represented by a homogeneous rate function I. Furthermore, the tail LDP can be extended to represent both dependence and marginals, the latter implying marginal log-Generalised Weibull tail limits. A connection is established between the tail LDP and residual tail dependence (or hidden regular variation) and a recent extension of it. Under a smoothness assumption, they are implied by the tail LDP. Based on the tail LDP, a simple estimator for very small probabilities of extreme events is formulated. It avoids estimation of I by making use of its homogeneity. Strong consistency in the sense of convergence of log-probability ratios is proven. Simulations and an application illustrate the difference between the classical approach and the LDP-based approach.},
   author = {Cees de Valk},
   doi = {10.1007/s10687-016-0252-6},
   isbn = {1068701602526},
   issn = {1572915X},
   issue = {4},
   journal = {Extremes},
   keywords = {Generalised Weibull tail limit,Hidden regular variation,Large deviation principle,Log-GW tail limit,Multivariate extremes,Residual tail dependence},
   pages = {687-717},
   publisher = {Extremes},
   title = {Approximation and estimation of very small probabilities of multivariate extreme events},
   volume = {19},
   url = {http://dx.doi.org/10.1007/s10687-016-0252-6},
   year = {2016},
}
@article{Zhang2021,
   abstract = {Quantile regression is a popular and powerful method for studying the effect of regressors on quantiles of a response distribution. However, existing results on quantile regression were mainly developed for cases in which the quantile level is fixed, and the data are often assumed to be independent. Motivated by recent applications, we consider the situation where (i) the quantile level is not fixed and can grow with the sample size to capture the tail phenomena, and (ii) the data are no longer independent, but collected as a time series that can exhibit serial dependence in both tail and non-tail regions. To study the asymptotic theory for high-quantile regression estimators in the time series setting, we introduce a tail adversarial stability condition, which had not previously been described, and show that it leads to an interpretable and convenient framework for obtaining limit theorems for time series that exhibit serial dependence in the tail region, but are not necessarily strongly mixing. Numerical experiments are conducted to illustrate the effect of tail dependence on high-quantile regression estimators, for which simply ignoring the tail dependence may yield misleading $p$-values.},
   author = {Ting Zhang},
   doi = {10.1093/biomet/asaa046},
   issn = {14643510},
   issue = {1},
   journal = {Biometrika},
   keywords = {Adversarial innovation,Double asymptotics,High-quantile regression,Limit theorem,Tail-dependent time series},
   pages = {113-126},
   title = {High-quantile regression for tail-dependent time series},
   volume = {108},
   year = {2021},
}
@article{Koenker2020,
   author = {Roger Koenker},
   title = {Extremal quantile regression inference: an r vinaigrette},
   year = {2020},
}
@article{Cho2017,
   abstract = {The objective of this paper is two-fold: to propose efficient estimation of multiple quantile regression analysis of longitudinal data and to develop a new test for the homogeneity of independent variable effects across multiple quantiles. Estimating multiple regression quantile coefficients simultaneously entails accommodating both association among the multiple quantiles and association among the repeated measurements of the response within subjects. We formulate simultaneous estimating equations using basis matrix expansion which accounts for the above-mentioned associations. The empirical likelihood method is adopted to estimate multiple regression quantile coefficients. Theoretical results show that the proposed simultaneous estimation is asymptotically more efficient than separate estimation of individual regression quantiles or ignoring the within-subject dependency. The proposed method also offers an empirical likelihood ratio test examining the homogeneity of the independent variable effects across the multiple quantiles. The Wilk's theorem holds for the test statistic, and thus the test is easy to implement. Simulation studies and real data example of a multi-center AIDS cohort study are included to illustrate the proposed estimation and testing methods, and empirically examine their properties.},
   author = {Hyunkeun Cho and Seonjin Kim and Mi Ok Kim},
   doi = {10.1016/j.jmva.2017.01.009},
   issn = {10957243},
   journal = {Journal of Multivariate Analysis},
   keywords = {Asymptotic efficiency,Empirical likelihood,Heteroscedasticity test,Longitudinal data,Multiple quantiles},
   pages = {334-343},
   publisher = {Elsevier Inc.},
   title = {Multiple quantile regression analysis of longitudinal data: Heteroscedasticity and efficient estimation},
   volume = {155},
   url = {http://dx.doi.org/10.1016/j.jmva.2017.01.009},
   year = {2017},
}
@article{Ziel2020,
   abstract = {Research Highlights: Flammability of wildland fuels is a key factor influencing risk-based decisions related to preparedness, response, and safety in Alaska. However, without effective measures of current and expected flammability, the expected likelihood of active and problematic wildfires in the future is difficult to assess and prepare for. This study evaluates the effectiveness of diverse indices to capture high-risk fires. Indicators of drought and atmospheric drivers are assessed along with the operational Canadian Forest Fire Danger Rating System (CFFDRS). Background and Objectives: In this study, 13 different indicators of atmospheric conditions, fuel moisture, and flammability are compared to determine how effective each is at identifying thresholds and trends for significant wildfire activity. Materials and Methods: Flammability indices are compared with remote sensing characterizations that identify where and when fire activity has occurred. Results: Among these flammability indicators, conventional tools calibrated to wildfire thresholds (DuffMoisture Code (DMC) and Buildup Index (BUI), as well as measures of atmospheric forcing (Vapor Pressure Deficit (VPD), performed best at representing the conditions favoring initiation and size of significant wildfire events. Conventional assessments of seasonal severity and overall landscape flammability using DMC and BUI can be continued with confidence. Fire models that incorporate BUI in overall fire potential and fire behavior assessments are likely to produce effective results throughout boreal landscapes in Alaska. One novel result is the effectiveness of VPD throughout the state, making it a potential alternative to FFMC among the short-lag/1-day indices. Conclusions: This study demonstrates the societal value of research that joins new academic research results with operational needs. Developing the framework to do this more effectively will bring science to action with a shorter lag time, which is critical as we face growing challenges from a changing climate.},
   author = {Robert H. Ziel and Peter A. Bieniek and Uma S. Bhatt and Heidi Strader and T. Scott Rupp and Alison York},
   doi = {10.3390/F11050516},
   issn = {19994907},
   issue = {5},
   journal = {Forests},
   keywords = {Boreal wildland fire,Canadian forest fire danger rating system,Evaporative demand drought index,Standardized precipitation evapotranspiration inde,Vapor pressure deficit},
   title = {A comparison of fire weather indices with MODIS fire days for the natural regions of alaska},
   volume = {11},
   year = {2020},
}
@article{Fern,
   author = {Javier Fern},
   issue = {1},
   keywords = {automatic differentiation,bayes space,bivariate copula,els,extreme-value copula,gini coefficient,integral splines,pickands dependence function,python,s transform,semiparametric mod-,williamson,zero-},
   pages = {1-53},
   title = {Semiparametric bivariate extreme-value copulas},
}
@article{Nissan2019,
   abstract = {Climate resilience is increasingly prioritized by international development agencies and national governments. However, current approaches to informing communities of future climate risk are problematic. The predominant focus on end-of-century projections neglects more pressing development concerns, which relate to the management of shorter-term risks and climate variability, and constitutes a substantial opportunity cost for the limited financial and human resources available to tackle development challenges. When a long-term view genuinely is relevant to decision-making, much of the information available is not fit for purpose. Climate model projections are able to capture many aspects of the climate system and so can be relied upon to guide mitigation plans and broad adaptation strategies, but the use of these models to guide local, practical adaptation actions is unwarranted. Climate models are unable to represent future conditions at the degree of spatial, temporal, and probabilistic precision with which projections are often provided, which gives a false impression of confidence to users of climate change information. In this article, we outline these issues, review their history, and provide a set of practical steps for both the development and climate scientist communities to consider. Solutions to mobilize the best available science include a focus on decision-relevant timescales, an increased role for model evaluation and expert judgment and the integration of climate variability into climate change services. This article is categorized under: Climate and Development > Knowledge and Action in Development.},
   author = {Hannah Nissan and Lisa Goddard and Erin Coughlan de Perez and John Furlow and Walter Baethgen and Madeleine C. Thomson and Simon J. Mason},
   doi = {10.1002/wcc.579},
   issn = {17577799},
   issue = {3},
   journal = {Wiley Interdisciplinary Reviews: Climate Change},
   keywords = {climate change adaptation,climate change projections,climate resilience,climate services,international development},
   pages = {1-16},
   title = {On the use and misuse of climate change projections in international development},
   volume = {10},
   year = {2019},
}
@article{Dupuis1999,
   abstract = {In this paper, we consider the modeling of exceedances over high thresholds.The natural distribution for such exceedances, the generalized Pareto distribution (GPD), is used and the problematic issue of threshold selection is addressed. We fit the GPD robustly to the data using techniques based on optimal bias-robust estimates. The robust procedure will assign weights between 0 and 1 to each data point. These weights are used to assess the validity of the GPD model for exceedances of the proposed threshold and thus can guide threshold selection. That is, we can initially consider a low threshold and increase it (thus reducing the number of data points) until all weights are close to one. The new approach is used to analyze two of the NERC data sets.},
   author = {D.J. Dupuis},
   doi = {10.1023/A:1009914915709},
   issn = {1386-1999},
   issue = {3},
   journal = {Extremes},
   keywords = {data point,data set,econometrics,generalized pareto distribution,mathematics,robust statistics,statistics},
   pages = {251-261},
   title = {Exceedances over High Thresholds: A Guide to Threshold Selection},
   volume = {1},
   year = {1999},
}
@article{Ozari2019,
   abstract = {The Block Maxima method divides sample data into equal blocks. Predictions are based on the maximum values of the observations. Choosing an efficient and proper block size for the Block Maxima method is an important issue and varies across fields (e.g., flood, rainfall, finance). However, the main problem is deciding which block size is suitable or optimal for the prediction. In the literature, it is a known fact that the selection of a small block size leads to bias, while the selection of a large block size leads to a variance problem. In one respect, this issue is any trade off problem between the bias and the variance. This paper proposes simple and easy computational method to specify the optimal block size selection process for the Block Maxima method.},
   author = {Çiğdem Özari and Özge Eren and Hasan Saygin},
   doi = {10.17559/TV-20180529125449},
   issn = {18486339},
   issue = {5},
   journal = {Tehnicki Vjesnik},
   keywords = {Block maxima,Extreme value theory,Maximum likelihood},
   pages = {1292-1296},
   title = {A new methodology for the block maxima approach in selecting the optimal block size},
   volume = {26},
   year = {2019},
}
@article{Kysely2010,
   abstract = {The paper presents a methodology for estimating high quantiles of distributions of daily temperature in a non-stationary context, based on peaks-over-threshold analysis with a time-dependent threshold expressed in terms of regression quantiles. The extreme value models are applied to estimate 20-yr return values of maximum daily temperature over Europe in transient global climate model (GCM) simulations for the 21st century. A comparison of scenarios of changes in the 20-yr return temperatures based on the non-stationary peaks-over-threshold models with conventional stationary models is performed. It is demonstrated that the application of the stationary extreme value models in temperature data from GCM scenarios yields results that may be to a large extent biased, while the non-stationary models lead to spatial patterns that are robust and enable one to detect areas where the projected warming in the tail of the distribution of daily temperatures is largest. The method also allows splitting the projected warming of extremely high quantiles into two parts that reflect change in the location and scale of the distribution of extremes, respectively. Spatial patterns of the two components differ significantly in the examined climate change projections over Europe. © 2010 Elsevier B.V.},
   author = {Jan Kyselý and Jan Picek and Romana Beranová},
   doi = {10.1016/J.GLOPLACHA.2010.03.006},
   issn = {0921-8181},
   issue = {1-2},
   journal = {Global and Planetary Change},
   keywords = {Climate change,Extreme temperatures,Extreme value analysis,Global climate models,Peaks-over-threshold method,Poisson process,Quantile regression},
   month = {5},
   pages = {55-68},
   publisher = {Elsevier},
   title = {Estimating extremes in climate change simulations using the peaks-over-threshold method with a non-stationary threshold},
   volume = {72},
   year = {2010},
}
@article{Northrop2011,
   abstract = {In environmental applications it is common for the extremes of a variable to be non-stationary, varying systematically in space, time or with the values of covariates. Multi-site datasets are common, and in such cases there is likely to be non-negligible inter-site dependence. We consider applications in which multi-site data are used to infer the marginal behaviour of the extremes at individual sites, while adjusting for inter-site dependence. For reasons of statistical efficiency, it is standard to model exceedances of a high threshold. Choosing an appropriate threshold can be problematic, particularly if the extremes are non-stationary. We propose a method for setting a covariate-dependent threshold using quantile regression. We consider how the quantile regression model and extreme value models fitted to threshold exceedances should be parameterized, in order that they are compatible. We adjust estimates of uncertainty for spatial dependence using methodology proposed recently. These methods are illustrated using time series of storm peak significant wave heights from 72 sites in the Gulf of Mexico. A simulation study illustrates the applicability of the proposed methodology more generally. © 2011 John Wiley & Sons, Ltd.},
   author = {Paul J. Northrop and Philip Jonathan},
   doi = {10.1002/ENV.1106},
   issn = {1099-095X},
   issue = {7},
   journal = {Environmetrics},
   keywords = {dependent data,extreme value regression modelling,quantile regression,threshold selection,wave heights},
   month = {11},
   pages = {799-809},
   publisher = {John Wiley & Sons, Ltd},
   title = {Threshold modelling of spatially dependent non-stationary extremes with application to hurricane-induced wave heights},
   volume = {22},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/env.1106 https://onlinelibrary.wiley.com/doi/abs/10.1002/env.1106 https://onlinelibrary.wiley.com/doi/10.1002/env.1106},
   year = {2011},
}
@article{Sigauke2017,
   abstract = {Long term peak electricity demand forecasting is a crucial step in the process of planning for power transmission and new generation capacity. This paper discusses an application of the Generalized Pareto Distribution to the modelling of daily peak electricity demand using South African data for the period 2000 to 2010. The main contribution of this paper is in the use of a cubic smoothing spline with a constant shift factor as a time varying threshold. An intervals estimator method is then used to decluster the observations above the threshold. We explore the influence of temperature by including it as a covariate in the Generalized Pareto Distribution parameters. A comparative analysis is done using the block maxima approach. The GPD model showed a better fit to the data compared to the GEVD model. Key findings from this study are that the Weibull class of distributions best fits the data which is bounded from above for both stationary and non-stationary models. Another key finding is that for different values of the temperature covariate the shape parameter is invariant and the scale parameter changes for different values of heating degree days.},
   author = {Caston Sigauke and Alphonce Bere},
   doi = {10.1016/J.ENERGY.2016.12.027},
   issn = {0360-5442},
   journal = {Energy},
   keywords = {Extreme value theory,Non stationary time series,Peak electricity demand,Penalized smoothing splines,Time varying threshold},
   month = {1},
   pages = {152-166},
   publisher = {Pergamon},
   title = {Modelling non-stationary time series using a peaks over threshold distribution with time varying covariates and threshold: An application to peak electricity demand},
   volume = {119},
   year = {2017},
}
@article{Coles1990,
   abstract = {For the design of sea defences the main statistical issue is to estimate quantiles of the distribution of annual maximum sea levels for all coastal sites. Traditional procedures independently analyse data from each individual site; thus known spatial properties of the meteorological and astronomical tidal components of sea level are not exploited. By spatial modelling of the marginal behaviour and inter-site dependence of sea level annual maxima around the British coast we are able to examine risk assessment for coastlines and the issue of sensitivity to climatic change.},
   author = {G Coles and J. A. Tawn},
   doi = {10.1098/rsta.1990.0126},
   issn = {0962-8428},
   issue = {1627},
   journal = {Philosophical Transactions of the Royal Society of London. Series A: Physical and Engineering Sciences},
   month = {9},
   pages = {457-476},
   publisher = {The Royal Society London},
   title = {Statistics of coastal flood prevention},
   volume = {332},
   url = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.1990.0126},
   year = {1990},
}
@article{Dixon1999,
   abstract = {The sea-level is the composition of astronomical tidal and meteorological surge processes. It exhibits temporal non-stationarity due to a combination of long-term trend in the mean level, the deterministic tidal component, surge seasonality and interactions between the tide and surge. We assess the effect of these non-stationarities on the estimation of the distribution of extreme sea-levels. This is important for coastal flood assessment as the traditional method of analysis assumes that, once the trend has been removed, extreme sea-levels are from a stationary sequence. We compare the traditional approach with a recently proposed alternative that incorporates the knowledge of the tidal component and its associated interactions, by applying them to 22 UK data sites and through a simulation study. Our main finding is that if the tidal non-stationarity is ignored then a substantial underestimation of extreme sea-levels results for most sites. In contrast, if surge seasonality and the tide-surge interaction are not modelled the traditional approach produces little additional bias. The alternative method is found to perform well but requires substantially more statistical modelling and better data quality.},
   author = {Mark J. Dixon and Jonathan A. Tawn},
   doi = {10.1111/1467-9876.00145},
   issn = {1467-9876},
   issue = {2},
   journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
   keywords = {Annual maximum method,Extreme sea,Extreme value theory,Joint probabilities method,Return level,levels},
   month = {1},
   pages = {135-151},
   publisher = {John Wiley & Sons, Ltd},
   title = {The Effect of Non-Stationarity on Extreme Sea-Level Estimation},
   volume = {48},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1111/1467-9876.00145 https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9876.00145 https://rss.onlinelibrary.wiley.com/doi/10.1111/1467-9876.00145},
   year = {1999},
}
@article{Casson1999,
   abstract = {Meteorological data are often recorded at a number of spatial locations. This gives rise to the possibility of pooling data through a spatial model to overcome some of the limitations imposed on an extreme value analysis by a lack of information. In this paper we develop a spatial model for extremes based on a standard representation for site-wise extremal behavior, combined with a spatial latent process for parameter variation over the region. A smooth, but possibly non-linear, spatial structure is an intrinsic feature of the model, and difficulties in computation are solved using Markov chain Monte Carlo inference. A simulation study is carried out to illustrate the potential gain in efficiency achieved by the spatial model. Finally, the model is applied to data generated from a climatological model in order to characterize the hurricane climate of the Gulf and Atlantic coasts of the United States.},
   author = {Edward Casson and Stuart Coles},
   doi = {10.1023/A:1009931222386},
   issn = {1572-915X},
   journal = {Extremes},
   keywords = {Civil Engineering,Economics,Environmental Management,Finance,Hydrogeology,Insurance,Management,Quality Control,Reliability,Safety and Risk,Statistics,Statistics for Business,general},
   pages = {449-468},
   publisher = {Springer},
   title = {Spatial Regression Models for Extremes},
   volume = {1},
   url = {https://link.springer.com/article/10.1023/A:1009931222386},
   year = {1999},
}
@article{Gilleland2006,
   author = {Eric Gilleland and Douglas Nychka and Uli Schneider},
   doi = {e249602fc8245c7cf041ed4d21fcd140f7526cc8},
   journal = {Hierarchical Modelling for the Environmental Sciences: Statistical Methods and Applications},
   pages = {170-183},
   title = {Spatial models for the distribution of extremes},
   volume = {216},
   url = {www.cgd.ucar.edu/stats},
   year = {2006},
}
@article{Cooley2012,
   abstract = {Quantification of precipitation extremes is important for flood planning purposes, and a common measure of extreme events is the r-year return level. We present a method for producing maps of precipitation return levels and uncertainty measures and apply it to a region in Colorado. Separate hierarchical models are constructed for the intensity and the frequency of extreme precipitation events. For intensity, we model daily precipitation above a high threshold at 56 weather stations with the generalized Pareto distribution. For frequency, we model the number of exceedances at the stations as binomial random variables. Both models assume that the regional extreme precipitation is driven by a latent spatial process characterized by geographical and climatological covariates. Effects not fully described by the covariates are captured by spatial structure in the hierarchies. Spatial methods were improved by working in a space with climatological coordinates. Inference is provided by a Markov chain Monte Carlo algorithm and spatial interpolation method, which provide a natural method for estimating uncertainty. © 2007 American Statistical Association.},
   author = {Daniel Cooley and Douglas Nychka and Philippe Naveau},
   doi = {10.1198/016214506000000780},
   issn = {01621459},
   issue = {479},
   journal = {Journal of the American Statistical Association},
   keywords = {Colorado,Extreme value theory,Generalized pareto distribution,Hierarchical model,Latent process},
   month = {9},
   pages = {824-840},
   publisher = {Taylor & Francis},
   title = {Bayesian spatial modeling of extreme precipitation return levels},
   volume = {102},
   url = {https://www.tandfonline.com/doi/abs/10.1198/016214506000000780},
   year = {2007},
}
@article{Mhalla2017,
   abstract = {The dependence structure of max-stable random vectors can be characterized by their Pickands dependence function. In many applications, the extremal dependence measure varies with covariates. We develop a flexible, semi-parametric method for the estimation of non-stationary multivariate Pickands dependence functions. The proposed construction is based on an accurate max-projection allowing to pass from the multivariate to the univariate setting and to rely on the generalized additive modeling framework. In the bivariate case, the resulting estimator of the Pickands function is regularized using constrained median smoothing B-splines, and bootstrap variability bands are constructed. In higher dimensions, we tailor our approach to the estimation of the extremal coefficient. An extended simulation study suggests that our estimator performs well and is competitive with the standard estimators in the absence of covariates. We apply the new methodology to a temperature dataset in the US where the extremal dependence is linked to time and altitude.},
   author = {Linda Mhalla and Valérie Chavez-Demoulin and Philippe Naveau},
   doi = {10.1016/J.JMVA.2017.04.006},
   issn = {0047-259X},
   journal = {Journal of Multivariate Analysis},
   keywords = {Extreme value theory,Generalized additive models,Max-stable random vectors,Non-stationarity,Pickands function,Semi-parametric models,Temperature data},
   month = {7},
   pages = {49-66},
   publisher = {Academic Press},
   title = {Non-linear models for extremal dependence},
   volume = {159},
   year = {2017},
}
@article{Jonathan2014a,
   abstract = {Characterising the joint structure of extremes of environmental variables is important for improved understanding of those environments. Yet, many applications of multivariate extreme value analysis adopt models that assume a particular form of extremal dependence between variables without justification, or restrict attention to regions in which all variables are extreme. The conditional extremes model of Heffernan and Tawn provides one approach to avoiding these particular restrictions. Extremal marginal and dependence characteristics of environmental variables typically vary with covariates. Reliable descriptions of extreme environments should also therefore characterise any non-stationarity. A recent article by the current authors extends the conditional extremes model of Heffernan and Tawn to include covariate effects, using Fourier representations of model parameters for single periodic covariates. Here, we further extend our recent work, introducing a general purpose spline representation for model parameters as functions of multidimensional covariates, common to all inference steps. We use a non-crossing quantile regression to estimate appropriate non-stationary marginal quantiles simultaneously as functions of covariate; these are necessary as thresholds for extreme value modelling and for standardisation of marginal distributions prior to application of the conditional extremes model. Then, we perform marginal extreme value and conditional extremes modelling within a roughness-penalised likelihood framework, with cross-validation to estimate suitable model parameter roughness. Finally, we use a bootstrap re-sampling procedure, encompassing all inference steps, to quantify uncertainties in, and dependence structure of, parameter estimates and estimates of conditional extremes of one variate given large values of another. We validate the approach using simulations from known joint distributions, the extremal dependence structures of which change with covariate. We apply the approach to joint modelling of storm peak significant wave height and associated storm peak period for extra-tropical storms at a northern North Sea location, with storm direction as covariate. We evaluate the impact of incorporating directional effects on estimates for conditional return values. © 2014 John Wiley & Sons, Ltd.},
   author = {P. Jonathan and K. Ewans and D. Randell},
   doi = {10.1002/ENV.2262},
   issn = {1099-095X},
   issue = {3},
   journal = {Environmetrics},
   keywords = {bootstrap,conditional extremes,covariate,cross,crossing quantile regression,non,spline,stationarity,validation},
   month = {5},
   pages = {172-188},
   publisher = {John Wiley & Sons, Ltd},
   title = {Non-stationary conditional extremes of northern North Sea storm characteristics},
   volume = {25},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/env.2262 https://onlinelibrary.wiley.com/doi/abs/10.1002/env.2262 https://onlinelibrary.wiley.com/doi/10.1002/env.2262},
   year = {2014},
}
@article{Heffernan2007,
   abstract = {Models based on assumptions of multivariate regular variation and hidden regular variation provide ways to describe a broad range of extremal dependence structures when marginal distributions are heavy tailed. Multivariate regular variation provides a rich description of extremal dependence in the case of asymptotic dependence, but fails to distinguish between exact independence and asymptotic independence. Hidden regular variation addresses this problem by requiring components of the random vector to be simultaneously large but on a smaller scale than the scale for the marginal distributions. In doing so, hidden regular variation typically restricts attention to that part of the probability space where all variables are simultaneously large. However, since under asymptotic independence the largest values do not occur in the same observation, the region where variables are simultaneously large may not be of primary interest. A different philosophy was offered in the paper of Heffernan and Tawn [J. R. Stat. Soc. Ser. B Stat. Methodol. 66 (2004) 497–546] which allows examination of distributional tails other than the joint tail. This approach used an asymptotic argument which conditions on one component of the random vector and finds the limiting conditional distribution of the remaining components as the conditioning variable becomes large. In this paper, we provide a thorough mathematical examination of the limiting arguments building on the orientation of Heffernan and Tawn [J. R. Stat. Soc. Ser. B Stat. Methodol. 66 (2004) 497–546]. We examine the conditions required for the assumptions made by the conditioning approach to hold, and highlight simililarities and differences between the new and established methods.},
   author = {Janet E. Heffernan and Sidney I. Resnick},
   doi = {10.1214/105051606000000835},
   issn = {1050-5164},
   issue = {2},
   journal = {The Annals of Applied Probability},
   keywords = {60G70,62G32,Asymptotic independence,Conditional models,coefficient of tail dependence,heavy tails,hidden regular variation,regular variation},
   month = {4},
   pages = {537-571},
   publisher = {Institute of Mathematical Statistics},
   title = {Limit laws for random vectors with an extreme component},
   volume = {17},
   url = {https://projecteuclid.org/journals/annals-of-applied-probability/volume-17/issue-2/Limit-laws-for-random-vectors-with-an-extreme-component/10.1214/105051606000000835.full},
   year = {2007},
}
@article{Mentaschi2016,
   abstract = {Statistical approaches to study extreme events require, by definition, long time series of data. In many scientific disciplines, these series are often subject to variations at different temporal scales that affect the frequency and intensity of their extremes. Therefore, the assumption of stationarity is violated and alternative methods to conventional stationary extreme value analysis (EVA) must be adopted. Using the example of environmental variables subject to climate change, in this study we introduce the transformed-stationary (TS) methodology for non-stationary EVA. This approach consists of (i) transforming a non-stationary time series into a stationary one, to which the stationary EVA theory can be applied, and (ii) reverse transforming the result into a non-stationary extreme value distribution. As a transformation, we propose and discuss a simple time-varying normalization of the signal and show that it enables a comprehensive formulation of non-stationary generalized extreme value (GEV) and generalized Pareto distribution (GPD) models with a constant shape parameter. A validation of the methodology is carried out on time series of significant wave height, residual water level, and river discharge, which show varying degrees of long-term and seasonal variability. The results from the proposed approach are comparable with the results from (a) a stationary EVA on quasi-stationary slices of non-stationary series and (b) the established method for non-stationary EVA. However, the proposed technique comes with advantages in both cases. For example, in contrast to (a), the proposed technique uses the whole time horizon of the series for the estimation of the extremes, allowing for a more accurate estimation of large return levels. Furthermore, with respect to (b), it decouples the detection of non-stationary patterns from the fitting of the extreme value distribution. As a result, the steps of the analysis are simplified and intermediate diagnostics are possible. In particular, the transformation can be carried out by means of simple statistical techniques such as low-pass filters based on the running mean and the standard deviation, and the fitting procedure is a stationary one with a few degrees of freedom and is easy to implement and control. An open-source MATLAB toolbox has been developed to cover this methodology, which is available at <a hrefCombining double low line"https://github.com/menta78/tsEva/" targetCombining double low line"-blank">https://github.com/menta78/tsEva/</a> (Mentaschi et al., 2016).},
   author = {Lorenzo Mentaschi and Michalis Vousdoukas and Evangelos Voukouvalas and Ludovica Sartini and Luc Feyen and Giovanni Besio and Lorenzo Alfieri},
   doi = {10.5194/HESS-20-3527-2016},
   issue = {9},
   journal = {Hydrology and Earth System Sciences},
   month = {9},
   pages = {3527-3547},
   publisher = {Copernicus GmbH},
   title = {The transformed-stationary approach: A generic and simplified methodology for non-stationary extreme value analysis},
   volume = {20},
   year = {2016},
}
@article{Nogaj2007,
   abstract = {In this paper, we study extreme values of non-stationary climatic phenomena. In the usually considered stationary case, the modelling of extremes is only based on the behaviour of the tails of the distribution of the remainder of the data set. In the non-stationary case though, it seems reasonable to assume that the temporal dynamics of the entire data set and that of extremes are closely related and thus all the available information about this link should be used in statistical studies of these events. We try to study how centered and normalized data which are closer to stationary data than the observation allows easier statistical analysis and to understand if we are very far from a hypothesis stating that the extreme events of centered and normed data follow a stationary distribution. The location and scale parameters used for this transformation (the central field), as well as extreme parameters obtained for the transformed data enable us to retrieve the trends in extreme events of the initial data set. Through non-parametric statistical methods, we thus compare a model directly built on the extreme events and a model reconstructed from estimations of the trends of the location and scale parameters of the entire data set and stationary extremes obtained from the centered and normed data set. In case of a correct reconstruction, we can clearly state that variations of the characteristics of extremes are well explained by the central field. Through these analyses we bring arguments to choose constant shape parameters of extreme distributions. We show that for the frequency of the moments of high threshold excesses (or for the mean of annual extremes), the general dynamics explains a large part of the trends on frequency of extreme events. The conclusion is less obvious for the amplitudes of threshold exceedances (or the variance of annual extremes) - especially for cold temperatures, partly justified by the statistical tools used, which require further analyses on the variability definition.},
   author = {M. Nogaj and S. Parey and D. Dacunha-Castelle},
   doi = {10.5194/NPG-14-305-2007},
   issue = {3},
   journal = {Nonlinear Processes in Geophysics},
   pages = {305-316},
   publisher = {European Geosciences Union},
   title = {Non-stationary extreme models and a climatic application},
   volume = {14},
   year = {2007},
}
@article{Stein2021,
   abstract = {The two sentences below Proposition 2 presently read: The parameters in (Formula presented.) all have distinct and interpretable roles in satisfying (5) and (6). From the proof in Section 7, (Formula presented.), regardless of the values of the other parameters. The expression for (Formula presented.) in the second sentence is incorrect. These two sentences should instead read: The parameters in (Formula presented.) all have distinct and interpretable roles in satisfying (5) and (6). From the proof in Section 7, (Formula presented.), regardless of the values of the other parameters.},
   author = {Michael L. Stein},
   doi = {10.1002/env.2704},
   issn = {1180-4009},
   issue = {8},
   journal = {Environmetrics},
   keywords = {NIL},
   month = {12},
   pages = {1-24},
   publisher = {John Wiley & Sons, Ltd},
   title = {A parametric model for distributions with flexible behaviour in both tails},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/10.1002/env.2704},
   year = {2021},
}
@article{Stein2021a,
   abstract = {For many problems of inference about a marginal distribution function, while the entire distribution is important, extreme quantiles are of particular interest because rare outcomes may have large consequences. In some applications, only the extreme upper quantiles require extra attention, but in, for example, climatological applications, extremes in both tails of the distribution can be impactful. A possible approach in this setting is to use parametric families of distributions that have flexible behavior in both tails. One way to quantify this property is to require that, for any two generalized Pareto distributions, there is a member of the parametric family that behaves like one of the generalized Pareto distributions in the upper tail and like the negative of the other generalized Pareto distribution in the lower tail. This work proposes some specific quantifications of this notion and describes parametric families of distributions that satisfy these specifications. The proposed families all have closed form expressions for their densities and, hence, are convenient for use in practice. A simulation study shows how one of the proposed families can work well for estimating all quantiles when both tails of a distribution are heavy tailed. An application to climate model output shows this family can also work well when applied to daily average January temperature near Calgary, for which the evolving distribution over time due to climate change is difficult to model accurately by any standard parametric family.},
   author = {Michael L. Stein},
   doi = {10.1002/ENV.2658},
   issn = {1099-095X},
   issue = {2},
   journal = {Environmetrics},
   keywords = {climate model,generalized Pareto distribution,temperature extremes},
   month = {3},
   pages = {e2658},
   publisher = {John Wiley & Sons, Ltd},
   title = {A parametric model for distributions with flexible behavior in both tails},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/env.2658 https://onlinelibrary.wiley.com/doi/abs/10.1002/env.2658 https://onlinelibrary.wiley.com/doi/10.1002/env.2658},
   year = {2021},
}
@article{Carvalho2014,
   abstract = {The modeling of multivariate extremes has received increasing recent attention because of its importance in risk assessment. In classical statistics of extremes, the joint distribution of two or more extremes has a nonparametric form, subject to moment constraints. This article develops a semiparametric model for the situation where several multivariate extremal distributions are linked through the action of a covariate on an unspecified baseline distribution, through a so-called density ratio model. Theoretical and numerical aspects of empirical likelihood inference for this model are discussed, and an application is given to pairs of extreme forest temperatures. Supplementarymaterials for this article are available online.},
   author = {Miguel de Carvalho and Anthony C. Davison},
   doi = {10.1080/01621459.2013.872651},
   issn = {1537274X},
   issue = {506},
   journal = {Journal of the American Statistical Association},
   keywords = {Air temperature,Empirical likelihood,Exponential tilting,Forest microclimate,Multivariate extreme values,Semiparametric modeling,Spectral distribution},
   pages = {764-776},
   publisher = {Taylor & Francis},
   title = {Spectral density ratio models for multivariate extremes},
   volume = {109},
   url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.2013.872651},
   year = {2014},
}
@article{Chavez-Demoulin2005,
   abstract = {We describe smooth non-stationary generalized additive modelling for sample extremes, in which spline smoothers are incorporated into models for exceedances over high thresholds. Fitting is by maximum penalized likelihood estimation, with uncertainty assessed by using differences of deviances and bootstrap simulation. The approach is illustrated by using data on extreme winter temperatures in the Swiss Alps, analysis of which shows strong influence of the north Atlantic oscillation. Benefits of the new approach are flexible and appropriate modelling of extremes, more realistic assessment of estimation uncertainty and the accommodation of complex dependence patterns.},
   author = {V. Chavez-Demoulin and A. C. Davison},
   doi = {10.1111/J.1467-9876.2005.00479.X},
   issn = {1467-9876},
   issue = {1},
   journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
   keywords = {Bootstrap,Generalized Pareto distribution,Generalized additive model,Natural cubic spline,North Atlantic oscillation,Parameter orthogonality,Peaks over threshold,Penalized likelihood,Statistics of extremes,Temperature data},
   month = {1},
   pages = {207-222},
   publisher = {John Wiley & Sons, Ltd},
   title = {Generalized additive modelling of sample extremes},
   volume = {54},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9876.2005.00479.x https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9876.2005.00479.x https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1467-9876.2005.00479.x},
   year = {2005},
}
@article{Frahm2006,
   abstract = {A measure called 'extremal dependence coefficient' (EDC) is introduced for studying the asymptotic dependence structure of the minimum and the maximum of a random vector. Some general properties of the EDC are derived and its relation to the tail dependence coefficient is examined. The extremal dependence structure of regularly varying elliptical random vectors is investigated and it is shown that the EDC is only determined by the tail index and by the pseudo-correlation coefficients of the elliptical distribution. © 2006 Elsevier B.V. All rights reserved.},
   author = {Gabriel Frahm},
   doi = {10.1016/J.SPL.2006.03.006},
   issn = {0167-7152},
   issue = {14},
   journal = {Statistics and Probability Letters},
   keywords = {Asymptotic dependence,Copula,Elliptical distribution,Extremal dependence,Tail dependence coefficient,Tail index},
   month = {8},
   pages = {1470-1481},
   publisher = {North-Holland},
   title = {On the extremal dependence coefficient of multivariate distributions},
   volume = {76},
   year = {2006},
}
@article{Beranger2019,
   abstract = {Estimation of extreme quantile regions, spaces in which future extreme events can occur with a given low probability, even beyond the range of the observed data, is an important task in the analysis of extremes. Existing methods to estimate such regions are available, but do not provide any measures of estimation uncertainty. We develop univariate and bivariate schemes for estimating extreme quantile regions under the Bayesian paradigm that outperforms existing approaches and provides natural measures of quantile region estimate uncertainty. We examine the method’s performance in controlled simulation studies. We illustrate the applicability of the proposed method by analysing high bivariate quantiles for pairs of pollutants, conditionally on different temperature gradations, recorded in Milan, Italy.},
   author = {Boris Beranger and Simone A Padoan and Scott A Sisson and Boris Beranger BBeranger and unsweduau A Simone Padoan and Scott A Sisson ScottSisson},
   doi = {10.1007/S10687-019-00364-0},
   issn = {1572-915X},
   issue = {2},
   journal = {Extremes 2019 24:2},
   keywords = {Civil Engineering,Economics,Environmental Management,Finance,Hydrogeology,Insurance,Management,Quality Control,Reliability,Safety and Risk,Statistics,Statistics for Business,general},
   month = {12},
   pages = {349-375},
   publisher = {Springer},
   title = {Estimation and uncertainty quantification for extreme quantile regions},
   volume = {24},
   url = {https://link.springer.com/article/10.1007/s10687-019-00364-0},
   year = {2019},
}
@article{Wei2006,
   abstract = {Estimation of reference growth curves for children's height and weight has traditionally relied on normal theory to construct families of quantile curves based on samples from the reference population. Age-specific parametric transformation has been used to significantly broaden the applicability of these normal theory methods. Non-parametric quantile regression methods offer a complementary strategy for estimating conditional quantile functions. We compare estimated reference curves for height using the penalized likelihood approach of Cole and Green (Statistics in Medicine 1992; 11:1305-1319) with quantile regression curves based on data used for modern Finnish reference charts. An advantage of the quantile regression approach is that it is relatively easy to incorporate prior growth and other covariates into the analysis of longitudinal growth data. Quantile specific autoregressive models for unequally spaced measurements are introduced and their application to diagnostic screening is illustrated. Copyright © 2005 John Wiley & Sons, Ltd.},
   author = {Ying Wei and Anneli Pere and Roger Koenker and Xuming He},
   doi = {10.1002/SIM.2271},
   issn = {1097-0258},
   issue = {8},
   journal = {Statistics in Medicine},
   keywords = {growth curves,height,longitudinal data,quantile regression},
   month = {4},
   pages = {1369-1382},
   pmid = {16143984},
   publisher = {John Wiley & Sons, Ltd},
   title = {Quantile regression methods for reference growth charts},
   volume = {25},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/sim.2271 https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.2271 https://onlinelibrary.wiley.com/doi/10.1002/sim.2271},
   year = {2006},
}
@article{Cade2003,
   author = {Brian S. Cade and Barry R. Noon},
   doi = {10.2307/3868138},
   issn = {15409295},
   issue = {8},
   journal = {Frontiers in Ecology and the Environment},
   month = {10},
   pages = {412-420},
   title = {A Gentle Introduction to Quantile Regression for Ecologists},
   volume = {1},
   url = {http://doi.wiley.com/10.2307/3868138},
   year = {2003},
}
@book{Koenker2017,
   author = {Roger Koenker and Victor Chernozhukov and Xuming He and Limin Peng},
   doi = {10.1201/9781315120256},
   isbn = {9781315120256},
   month = {10},
   publisher = {Chapman and Hall/CRC},
   title = {Handbook of Quantile Regression},
   url = {https://www.taylorfrancis.com/books/9781498725293},
   year = {2017},
}
@article{Marcon2016,
   abstract = {A simple approach for modeling multivariate extremes is to consider the vector of component-wise maxima and their max-stable distributions. The extremal dependence can be inferred by estimating the angular measure or, alternatively, the Pickands dependence function. We propose a nonparametric Bayesian model that allows, in the bivariate case, the simultaneous estimation of both functional representations through the use of polynomials in the Bernstein form. The constraints required to provide a valid extremal dependence are addressed in a straightforward manner, by placing a prior on the coefficients of the Bernstein polynomials which gives probability one to the set of valid functions. The prior is extended to the polynomial degree, making our approach nonparametric. Although the analytical expression of the posterior is unknown, inference is possible via a trans-dimensional MCMC scheme. We show the efficiency of the proposed methodology by means of a simulation study. The extremal behaviour of log-returns of daily exchange rates between the Pound Sterling vs the U.S. Dollar and the Pound Sterling vs the Japanese Yen is analysed for illustrative purposes.},
   author = {Giulia Marcon and Simone A. Padoan and Isadora Antoniano-Villalobos},
   doi = {10.1214/16-EJS1162},
   issn = {19357524},
   issue = {2},
   journal = {Electronic Journal of Statistics},
   keywords = {Angular measure,Bayesian nonparametrics,Bernstein polynomials,Exchange rates,Extremal dependence,Generalised extreme value distribution,Max-stable distribution,Trans-dimensional MCMC},
   month = {1},
   pages = {3310-3337},
   publisher = {Institute of Mathematical Statistics and Bernoulli Society},
   title = {Bayesian inference for the extremal dependence},
   volume = {10},
   url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-10/issue-2/Bayesian-inference-for-the-extremal-dependence/10.1214/16-EJS1162.full https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-10/issue-2/Bayesian-inf},
   year = {2016},
}
@article{Guillotte2016,
   abstract = {Pickands dependence functions characterize bivariate extreme value copulas. In this paper, we study the class of polynomial Pickands functions. We provide a solution for the characterization of such polynomials of degree at most m + 2, m ≥ 0, and show that these can be parameterized by a vector in Rm+1 belonging to the intersection of two ellipsoids. We also study the class of Bernstein approximations of order m+2 of Pickands functions which are shown to be (polynomial) Pickands functions and parameterized by a vector in Rm+1 belonging to a polytope. We give necessary and sufficient conditions for which a polynomial Pickands function is in fact a Bernstein approximation of some Pickands function. Approximation results of Pickands dependence functions by polynomials are given. Finally, inferential methodology is discussed and comparisons based on simulated data are provided.},
   author = {Simon Guillotte and François Perron},
   doi = {10.3150/14-BEJ656},
   issn = {13507265},
   issue = {1},
   journal = {Bernoulli},
   keywords = {Bernstein's theorem,Extreme value copulas,Lorentz degree,Pickands dependence function,Polynomials,Spectral measure},
   pages = {213-241},
   title = {Polynomial Pickands functions},
   volume = {22},
   year = {2016},
}
@article{Tran2021,
   abstract = {The Latent River Problem has emerged as a flagship problem for causal discovery in extreme value statistics. This paper gives QTree, a simple and efficient algorithm to solve the Latent River Problem that outperforms existing methods. QTree returns a directed graph and achieves almost perfect recovery on the Upper Danube, the existing benchmark dataset, as well as on new data from the Lower Colorado River in Texas. It can handle missing data, has an automated parameter tuning procedure, and runs in time $O(n |V|^2)$, where $n$ is the number of observations and $|V|$ the number of nodes in the graph. In addition, under a Bayesian network model for extreme values with propagating noise, we show that the QTree estimator returns for $n\to\infty$ a.s. the correct tree.},
   author = {Ngoc Mai Tran and Johannes Buck and Claudia Klüppelberg},
   keywords = {Bayesian networks,causal inference,di-rected graphical models,extreme values statistics,max-linear},
   month = {2},
   title = {Estimating a Latent Tree for Extremes},
   url = {http://arxiv.org/abs/2102.06197},
   year = {2021},
}
@article{Beranger2021,
   abstract = {Estimation of extreme quantile regions, spaces in which future extreme events can occur with a given low probability, even beyond the range of the observed data, is an important task in the analysis of extremes. Existing methods to estimate such regions are available, but do not provide any measures of estimation uncertainty. We develop univariate and bivariate schemes for estimating extreme quantile regions under the Bayesian paradigm that outperforms existing approaches and provides natural measures of quantile region estimate uncertainty. We examine the method’s performance in controlled simulation studies. We illustrate the applicability of the proposed method by analysing high bivariate quantiles for pairs of pollutants, conditionally on different temperature gradations, recorded in Milan, Italy.},
   author = {Boris Beranger and Simone A. Padoan and Scott A. Sisson},
   doi = {10.1007/s10687-019-00364-0},
   isbn = {1068701900},
   issn = {1572915X},
   issue = {2},
   journal = {Extremes},
   keywords = {Air pollution,Bayesian nonparametrics,Bernstein polynomials,Extremal dependence,Extreme quantile regions,Max-stable distributions},
   pages = {349-375},
   publisher = {Extremes},
   title = {Estimation and uncertainty quantification for extreme quantile regions},
   volume = {24},
   year = {2021},
}
@article{Balkema1974,
   author = {A. A. Balkema and L. de Haan},
   doi = {10.1214/aop/1176996548},
   issn = {0091-1798},
   issue = {5},
   journal = {The Annals of Probability},
   month = {10},
   pages = {792-804},
   title = {Residual Life Time at Great Age},
   volume = {2},
   url = {https://projecteuclid.org/journals/annals-of-probability/volume-2/issue-5/Residual-Life-Time-at-Great-Age/10.1214/aop/1176996548.full},
   year = {1974},
}
@article{Wood2021,
   abstract = {Generalized additive (mixed) models, some of their extensions and other generalized ridge regression with multiple smoothing parameter estimation by (Restricted) Marginal Likelihood, Generalized Cross Validation and similar, or using iterated nested Laplace approximation for fully Bayesian inference. See Wood (2017) <doi:10.1201/9781315370279> for an overview. Includes a gam() function, a wide variety of smoothers, 'JAGS' support and distributions beyond the exponential family.},
   author = {Simon Wood},
   journal = {R Package},
   title = {Mixed GAM Computation Vehicle with Automatic Smoothness Estimation},
   year = {2021},
}
@article{Wadsworth2016,
   abstract = {To model the tail of a distribution, one has to define the threshold above or below which an extreme value model produces a suitable fit. Parameter stability plots, whereby one plots maximum likelihood estimates of supposedly threshold-independent parameters against threshold, form one of the main tools for threshold selection by practitioners, principally due to their simplicity. However, one repeated criticism of these plots is their lack of interpretability, with pointwise confidence intervals being strongly dependent across the range of thresholds. In this article, we exploit the independent-increments structure of maximum likelihood estimators to produce complementary plots with greater interpretability, and suggest a simple likelihood-based procedure that allows for automated threshold selection. Supplementary materials for this article are available online.},
   author = {J. L. Wadsworth},
   doi = {10.1080/00401706.2014.998345/SUPPL_FILE/UTCH_A_998345_SM1169.ZIP},
   issn = {15372723},
   issue = {1},
   journal = {Technometrics},
   keywords = {Diagnostic plots,Extreme value modeling,Maximum likelihood,Threshold selection},
   month = {1},
   pages = {116-126},
   publisher = {American Statistical Association},
   title = {Exploiting structure of maximum likelihood estimators for extreme value threshold selection},
   volume = {58},
   url = {https://amstat.tandfonline.com/doi/abs/10.1080/00401706.2014.998345},
   year = {2016},
}
@article{Velthoen2021,
   abstract = {Extreme quantile regression provides estimates of conditional quantiles outside the range of the data. Classical methods such as quantile random forests perform poorly in such cases since data in the tail region are too scarce. Extreme value theory motivates to approximate the conditional distribution above a high threshold by a generalized Pareto distribution with covariate dependent parameters. This model allows for extrapolation beyond the range of observed values and estimation of conditional extreme quantiles. We propose a gradient boosting procedure to estimate a conditional generalized Pareto distribution by minimizing its deviance. Cross-validation is used for the choice of tuning parameters such as the number of trees and the tree depths. We discuss diagnostic plots such as variable importance and partial dependence plots, which help to interpret the fitted models. In simulation studies we show that our gradient boosting procedure outperforms classical methods from quantile regression and extreme value theory, especially for high-dimensional predictor spaces and complex parameter response surfaces. An application to statistical post-processing of weather forecasts with precipitation data in the Netherlands is proposed.},
   author = {Jasper Velthoen and Clément Dombry and Juan-Juan Cai and Sebastian Engelke},
   isbn = {2103.00808v1},
   journal = {arXiv},
   keywords = {extreme quantile regression,extreme value theory,generalized Pareto distribution,gradient boosting,tree-based methods},
   month = {3},
   title = {Gradient boosting for extreme quantile regression},
   volume = {2103.00808},
   url = {https://arxiv.org/abs/2103.00808v1},
   year = {2021},
}
@article{Youngman2020,
   abstract = {R package version 0.1.4},
   author = {Ben Youngman},
   journal = {R Package},
   title = {evgam: Generalised Additive Extreme Value Models},
   url = {https://cran.r-project.org/package=evgam},
   year = {2020},
}
@misc{Koenker2017a,
   abstract = {Since Quetelet's work in the nineteenth century, social science has iconified the average man, that hypothetical man without qualities who is comfortable with his head in the oven and his feet in a bucket of ice. Conventional statistical methods since Quetelet have sought to estimate the effects of policy treatments for this average man. However, such effects are often quite heterogeneous: Medical treatments may improve life expectancy but also impose serious short-term risks; reducing class sizes may improve the performance of good students but not help weaker ones, or vice versa. Quantile regression methods can help to explore these heterogeneous effects. Some recent developments in quantile regression methods are surveyed in this review.},
   author = {Roger Koenker},
   doi = {10.1146/annurev-economics-063016-103651},
   issn = {19411391},
   journal = {Annual Review of Economics},
   keywords = {Causal inference,Heterogeneity,Quantile regression,Treatment effects},
   month = {8},
   pages = {155-176},
   publisher = {Annual Reviews},
   title = {Quantile regression: 40 years on},
   volume = {9},
   url = {https://www.annualreviews.org/doi/abs/10.1146/annurev-economics-063016-103651},
   year = {2017},
}
@article{Knochenhauer2004,
   author = {Michael Knochenhauer and Pekka Louko},
   city = {London},
   doi = {10.1007/978-0-85729-410-4_241},
   journal = {Probabilistic Safety Assessment and Management},
   pages = {1498-1503},
   publisher = {Springer London},
   title = {Guidance for External Events Analysis},
   url = {http://link.springer.com/10.1007/978-0-85729-410-4_241},
   year = {2004},
}
@article{,
   title = {NARSIS New Approach to Reactor Safety ImprovementS WP1: Characterization of potential physical threats due to different external hazards and scenarios Del 1.1 Review of state-of-the art for hazard and multi-hazard characterisation},
}
@article{Hansen2020,
   abstract = {Design and re-analysis of offshore structures requires the joint estimation of extreme values for a set of environmental variables, representing so-called long-term and short-term characteristics of the environment, subject to sources of systematic variation including directionality and seasonality. Estimation is complicated by numerous sources of uncertainty, typically including limited sample size and the specification of a number of analysis parameters (such as thresholds for peaks over threshold analysis). In this work, we present a model to estimate joint extremal characteristics of the ocean environment incorporating non-stationary marginal and conditional extreme value analysis, and thorough uncertainty quantification, within a Bayesian framework. The model is used to quantify the joint directional–seasonal structure of extremes waves, winds and currents at a location in the Danish sector of the North Sea.},
   author = {Hans Fabricius Hansen and David Randell and Allan Rod Zeeberg and Philip Jonathan},
   doi = {10.1016/J.OCEANENG.2019.106665},
   issn = {0029-8018},
   journal = {Ocean Engineering},
   keywords = {Bayesian,Conditional extremes,Extremes,Non-stationary,Offshore design,Uncertainty},
   month = {1},
   pages = {106665},
   publisher = {Pergamon},
   title = {Directional–seasonal extreme value analysis of North Sea storm conditions},
   volume = {195},
   year = {2020},
}
@article{DeHaan1998,
   abstract = {Multivariate extreme value theory is used to estimate the probability of failure of a sea-wall near the town of Petten in Noord Holland, The Netherlands. The sample consists of 828 observations of still water levels and wave heights collected during storm events over a 13-year period. The paper sketches the probabilistic and statistical theory behind the estimation procedures used.},
   author = {Laurens de Haan and John de Ronde},
   doi = {10.1023/A:1009909800311},
   issn = {1572-915X},
   issue = {1},
   journal = {Extremes},
   keywords = {Civil Engineering,Economics,Environmental Management,Finance,Hydrogeology,Insurance,Management,Quality Control,Reliability,Safety and Risk,Statistics,Statistics for Business,general},
   pages = {7-45},
   publisher = {Springer},
   title = {Sea and Wind: Multivariate Extremes at Work},
   volume = {1},
   url = {https://link.springer.com/article/10.1023/A:1009909800311},
   year = {1998},
}
@article{Wadsworth2019,
   abstract = {Currently available models for spatial extremes suffer either from
inflexibility in the dependence structures that they can capture, lack of
scalability to high dimensions, or in most cases, both of these. We present an
approach to spatial extreme value theory based on the conditional multivariate
extreme value model, whereby the limit theory is formed through conditioning
upon the value at a particular site being extreme. The ensuing methodology
allows for a flexible class of dependence structures, as well as models that
can be fitted in high dimensions. To overcome issues of conditioning on a
single site, we suggest a joint inference scheme based on all observation
locations, and implement an importance sampling algorithm to provide spatial
realizations and estimates of quantities conditioning upon the process being
extreme at any of one of an arbitrary set of locations. The modelling approach
is applied to Australian summer temperature extremes, permitting assessment the
spatial extent of high temperature events over the continent.},
   author = {Jennifer L. Wadsworth and Jonathan Tawn},
   doi = {10.48550/arxiv.1912.06560},
   month = {12},
   title = {Higher-dimensional spatial extremes via single-site conditioning},
   url = {https://arxiv.org/abs/1912.06560},
   year = {2019},
}
@article{Diederen2019,
   abstract = {We present a new method to generate spatially coherent river discharge peaks over multiple river basins, which can be used for continental event-based probabilistic flood risk assessment. We first extract extreme events from river discharge time series data over a large set of locations by applying new peak identification and peak-matching methods. Then we describe these events using the discharge peak at each location while accounting for the fact that the events do not affect all locations. Lastly we fit the state-of-the-art multivariate extreme value distribution to the discharge peaks and generate from the fitted model a large catalogue of spatially coherent synthetic event descriptors. We demonstrate the capability of this approach in capturing the statistical dependence over all considered locations. We also discuss the limitations of this approach and investigate the sensitivity of the outcome to various model parameters.},
   author = {Dirk Diederen and Ye Liu and Ben Gouldby and Ferdinand Diermanse and Sergiy Vorogushyn},
   doi = {10.5194/nhess-19-1041-2019},
   issn = {16849981},
   issue = {5},
   journal = {Natural Hazards and Earth System Sciences},
   pages = {1041-1053},
   title = {Stochastic generation of spatially coherent river discharge peaks for continental event-based flood risk assessment},
   volume = {19},
   year = {2019},
}
@article{Shooter2022,
   abstract = {The joint extremal spatial dependence of wind speed and significant wave height in the North East Atlantic is quantified using Metop satellite scatterometer and hindcast observations for the period 2007–2018, and a multivariate spatial conditional extremes (MSCE) model, ultimately motivated by the work of Heffernan and Tawn (2004). The analysis involves (a) registering individual satellite swaths and corresponding hindcast data onto a template transect (running approximately north-east to south-west, between the British Isles and Iceland), (b) non-stationary directional-seasonal marginal extreme value analysis at a set of registration locations on the transect, (c) transformation from physical to standard Laplace scale using the fitted marginal model, (d) estimation of the MSCE model on the set of registration locations, and assessment of quality of model fit. A joint model is estimated for three spatial quantities: Metop wind speed, hindcast wind speed and hindcast significant wave height. Results suggest that, when conditioning on extreme Metop wind speed, extremal spatial dependence for all three quantities decays over approximately 600–800 km.},
   author = {Rob Shooter and Emma Ross and Agustinus Ribal and Ian R. Young and Philip Jonathan},
   doi = {10.1016/j.oceaneng.2022.110647},
   issn = {00298018},
   issue = {September 2021},
   journal = {Ocean Engineering},
   keywords = {Conditional extremes,Covariate effects,Extremal spatial dependence,Joint extremes,Metop},
   pages = {110647},
   publisher = {Elsevier Ltd},
   title = {Multivariate spatial conditional extremes for extreme ocean environments},
   volume = {247},
   url = {https://doi.org/10.1016/j.oceaneng.2022.110647},
   year = {2022},
}
@article{Quinn2019,
   abstract = {In this paper we seek to understand the nature of flood spatial dependence over the conterminous United States. We extend an existing conditional multivariate statistical model to enable its application to this large and heterogenous region and apply it to a 40-year data set of ~2,400 U.S. Geological Survey gauge series records to simulate 1,000 years of U.S. flooding comprising more than 63,000 individual events with realistic spatial dependence. A continental-scale hydrodynamic model at 30 m resolution is then used to calculate the economic loss arising from each of these events. From this we are able to compute the probability that different values of U.S. annual total economic loss due to flooding are exceeded (i.e., a loss-exceedance curve). Comparing these data to an observed flood loss-exceedance curve for the period 1988–2017 shows a reasonable match for annual losses with probability below 10% (e.g., >1 in 10-year return period). This analysis suggests that there is a 1% chance of U.S. annual fluvial flood losses exceeding $78Bn in any given year, and a 0.1% chance of them exceeding $136Bn. Analysis of the set of stochastic events and losses yields new insights into the nature of flooding and flood risk in the United States. In particular, we confirm the strong relationship between flood affected area and event peak magnitude, but show considerable variability in this relationship between adjacent U.S. regions. The analysis provides a significant advance over previous national flood risk analyses as it gives the full loss-exceedance curve instead of simply the average annual loss.},
   author = {Niall Quinn and Paul D. Bates and Jeff Neal and Andy Smith and Oliver Wing and Chris Sampson and James Smith and Janet Heffernan},
   doi = {10.1029/2018WR024205},
   issn = {19447973},
   issue = {3},
   journal = {Water Resources Research},
   keywords = {Flood risk,Flooding,Hydrodynamic modelling,Spatial dependence},
   pages = {1890-1911},
   title = {The Spatial Dependence of Flood Hazard and Risk in the United States},
   volume = {55},
   year = {2019},
}
@article{Wing2020,
   abstract = {Global flood models integrate flood maps of constant probability in space, ignoring the correlation between sites and thus potentially misestimating the risk posed by extreme events. Stochastic flood models alleviate this issue through the simulation of flood events with a realistic spatial structure, yet their proliferation at large scales has historically been inhibited by data quality and computer availability. In this paper, we show, for the first time, the efficacy of modeled river discharge reanalyses in the characterization of flood spatial dependence in the absence of a dense stream gauge network. While global hydrological models may show poor correspondence with absolute observed river flows, we find that the rate at which they can simulate the joint occurrence of relative flow exceedances at two given locations is broadly similar to when a gauge-based statistical model is used. Evidenced over the United States, flood events simulated using observed gauge data from the U.S. Geological Survey versus those generated using modeled streamflows have similar (i) distributions of site-to-site correlation strength, (ii) relationships between event size and return period, and, importantly, (iii) loss distributions when incorporated into a continental-scale flood risk model. Extremal dependence is generally quantified less accurately on larger rivers, in arid climates, in mountainous terrain, and for the rarest high-magnitude events. However, local-scale errors are shown to broadly cancel each other out when combined, producing an unbiased flood spatial dependence model. These findings suggest that building accurate stochastic flood models worldwide may no longer be a distant aspiration.},
   author = {Oliver E.J. Wing and Niall Quinn and Paul D. Bates and Jeffrey C. Neal and Andrew M. Smith and Christopher C. Sampson and Gemma Coxon and Dai Yamazaki and Edwin H. Sutanudjaja and Lorenzo Alfieri},
   doi = {10.1029/2020WR027692},
   issn = {19447973},
   issue = {8},
   journal = {Water Resources Research},
   keywords = {flood risk,flooding,hydraulic modelling,hydrological modelling,spatial dependence},
   title = {Toward Global Stochastic River Flood Modeling},
   volume = {56},
   year = {2020},
}
@article{Keef2013a,
   abstract = {Flooding is a natural phenomenon that regularly causes financial and human devastation around the world. In many countries the risk of flooding is managed by society through a combination of governmental agencies and the insurance industry. For both these types of organisation an estimate of the largest, or most widespread, events that can be expected to occur is useful. Such estimates can be used to help in preparing or co-ordinating flood mitigation activities and by the insurance and re-insurance industries to assess financial risk. In this paper we develop a method to simulate a set of synthetic flood events that can be used to estimate the probability of widespread floods. We demonstrate this method using data from a set of UK river flow gauges. The model used in this simulation process is based on the conditional exceedance model of Heffernan and Tawn, extended to incorporate features typically found in the data for extreme river floods. We also present an improved estimation method for the model parameters and demonstrate its advantages through the results of a simulation study. The benefits of the method over previous models used are that it provides a theoretical basis for extrapolation and is flexible enough to account for varying strengths of extremal dependence that are observed in flood data. © 2012 John Wiley & Sons, Ltd.},
   author = {Caroline Keef and Jonathan A. Tawn and Rob Lamb},
   doi = {10.1002/env.2190},
   issn = {11804009},
   issue = {1},
   journal = {Environmetrics},
   keywords = {Conditional exceedance model,Extremal dependence,Multivariate extreme value theory,Spatial flood risk assessment},
   pages = {13-21},
   title = {Estimating the probability of widespread flood events},
   volume = {24},
   year = {2013},
}
@article{Tendijck2021,
   abstract = {There currently exist a variety of statistical methods for modeling bivariate extremes. However, when the dependence between variables is driven by more than one latent process, these methods are likely to fail to give reliable inferences. We consider situations in which the observed dependence at extreme levels is a mixture of a possibly unknown number of much simpler bivariate distributions. For such structures, we demonstrate the limitations of existing methods and propose two new methods: an extension of the Heffernan–Tawn conditional extreme value model to allow for mixtures and an extremal quantile-regression approach. The two methods are examined in a simulation study and then applied to oceanographic data. Finally, we discuss extensions including a subasymptotic version of the proposed model, which has the potential to give more efficient results by incorporating data that are less extreme. Both new methods outperform existing approaches when mixtures are present.},
   author = {Stan Tendijck and Emma Eastoe and Jonathan Tawn and David Randell and Philip Jonathan},
   doi = {10.1080/01621459.2021.1996379},
   issn = {0162-1459},
   issue = {0},
   journal = {Journal of the American Statistical Association},
   keywords = {Conditional extremes,Mixture distributions,Multivariate extremes,Offshore wave extremes,Quantile-regression},
   month = {12},
   pages = {1-12},
   publisher = {American Statistical Association},
   title = {Modeling the Extremes of Bivariate Mixture Distributions With Application to Oceanographic Data},
   volume = {0},
   url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2021.1996379},
   year = {2021},
}
@article{Curceac2020,
   abstract = {This study investigated core components of an extreme value methodology for the estimation of high-flow frequencies from agricultural surface water run-off. The Generalized Pareto distribution (GPD) was used to model excesses in time-series data that resulted from the ‘Peaks Over Threshold’ (POT) method. First, the performance of eight different GPD parameter estimators was evaluated through a Monte Carlo experiment. Second, building on the estimator comparison, two existing automated GPD threshold selection methods were evaluated against a proposed approach that automates the threshold stability plots. For this second experiment, methods were applied to discharge measured at a highly-instrumented agricultural research facility in the UK. By averaging fine-resolution 15-minute data to hourly, 6-hourly and daily scales, we were also able to determine the effect of scale on threshold selection, as well as the performance of each method. The results demonstrate the advantages of the proposed threshold selection method over two commonly applied methods, while at the same time providing useful insights into the effect of the choice of the scale of measurement on threshold selection. The results can be generalised to similar water monitoring schemes and are important for improved characterisations of flood events and the design of associated disaster management protocols.},
   author = {Stelian Curceac and Peter M. Atkinson and Alice Milne and Lianhai Wu and Paul Harris},
   doi = {10.1016/J.JHYDROL.2020.124845},
   issn = {00221694},
   journal = {Journal of Hydrology},
   keywords = {Flood frequency analysis,Generalized pareto distribution,Grassland agriculture,Peaks over threshold,Scale effects,Threshold selection},
   month = {6},
   publisher = {Elsevier B.V.},
   title = {An evaluation of automated GPD threshold selection methods for hydrological extremes across different scales},
   volume = {585},
   url = {https://doi.org/10.1016/j.jhydrol.2020.124845},
   year = {2020},
}
@article{Lamb2010,
   abstract = {To date, national- and regional-scale flood risk assessments have provided valuable information about the annual expected consequences of flooding, but not the exposure to widespread concurrent flooding that could have damaging consequences for people and the economy. We present a new method for flood risk assessment that accommodates the risk of widespread flooding. It is based on a statistical conditional exceedance model, which is fitted to gauged data and describes the joint probability of extreme river flows or sea levels at multiple locations. The method can be applied together with data from models for flood defence systems and economic damages to calculate a risk profile describing the probability distribution of economic losses or other consequences aggregated over a region. The method has the potential to augment national or regional risk assessments of expected annual damage with new information about the likelihoods, extent and impacts of events that could contribute to the risk. © The Authors. Journal of Flood Risk Management © 2010 The Chartered Institution of Water and Environmental Management.},
   author = {R. Lamb and C. Keef and J. Tawn and S. Laeger and I. Meadowcroft and S. Surendran and P. Dunning and C. Batstone},
   doi = {10.1111/j.1753-318X.2010.01081.x},
   issn = {1753318X},
   issue = {4},
   journal = {Journal of Flood Risk Management},
   keywords = {Economic damages,Flood risk,Joint probability,Spatial dependence},
   pages = {323-336},
   title = {A new method to assess the risk of local and widespread flooding on rivers and coasts},
   volume = {3},
   year = {2010},
}
@article{Towe2019,
   abstract = {Multivariate extreme value models are used to estimate joint risk in a number of applications, with a particular focus on environmental fields ranging from climatology and hydrology to oceanography and seismic hazards. The semi-parametric conditional extreme value model of Heffernan and Tawn involving a multivariate regression provides the most suitable of current statistical models in terms of its flexibility to handle a range of extremal dependence classes. However, the standard inference for the joint distribution of the residuals of this model suffers from the curse of dimensionality because, in a d-dimensional application, it involves a d−1-dimensional nonparametric density estimator, which requires, for accuracy, a number points and commensurate effort that is exponential in d. Furthermore, it does not allow for any partially missing observations to be included, and a previous proposal to address this is extremely computationally intensive, making its use prohibitive if the proportion of missing data is nontrivial. We propose to replace the d−1-dimensional nonparametric density estimator with a model-based copula with univariate marginal densities estimated using kernel methods. This approach provides statistically and computationally efficient estimates whatever the dimension, d, or the degree of missing data. Evidence is presented to show that the benefits of this approach substantially outweigh potential misspecification errors. The methods are illustrated through the analysis of UK river flow data at a network of 46 sites and assessing the rarity of the 2015 floods in North West England.},
   author = {R. P. Towe and J. A. Tawn and R. Lamb and C. G. Sherlock},
   doi = {10.1002/ENV.2575},
   issn = {1099095X},
   issue = {8},
   journal = {Environmetrics},
   keywords = {copula,dependence modelling,missing values,multivariate extreme value theory,spatial flood risk assessment},
   month = {12},
   publisher = {John Wiley and Sons Ltd},
   title = {Model-based inference of conditional extreme value distributions with hydrological applications},
   volume = {30},
   year = {2019},
}
@article{Northrop2017,
   abstract = {Design conditions for marine structures are typically informed by threshold-based extreme value analyses of oceanographic variables, in which excesses of a high threshold are modelled by a generalized Pareto distribution. Too low a threshold leads to bias from model misspecification, and raising the threshold increases the variance of estimators: a bias–variance trade-off. Many existing threshold selection methods do not address this trade-off directly but rather aim to select the lowest threshold above which the generalized Pareto model is judged to hold approximately. In the paper Bayesian cross-validation is used to address the trade-off by comparing thresholds based on predictive ability at extreme levels. Extremal inferences can be sensitive to the choice of a single threshold. We use Bayesian model averaging to combine inferences from many thresholds, thereby reducing sensitivity to the choice of a single threshold. The methodology is applied to significant wave height data sets from the northern North Sea and the Gulf of Mexico.},
   author = {Paul J. Northrop and Nicolas Attalides and Philip Jonathan},
   doi = {10.1111/RSSC.12159},
   issn = {14679876},
   issue = {1},
   journal = {Journal of the Royal Statistical Society. Series C: Applied Statistics},
   keywords = {Cross-validation,Extreme value theory,Generalized Pareto distribution,Predictive inference,Threshold},
   month = {1},
   pages = {93-120},
   publisher = {Blackwell Publishing Ltd},
   title = {Cross-validatory extreme value threshold selection and uncertainty with application to ocean storm severity},
   volume = {66},
   year = {2017},
}
@article{Richards2021,
   abstract = {Inference on the extremal behaviour of spatial aggregates of precipitation is important for quantifying river flood risk. There are two classes of previous approach, with one failing to ensure self-consistency in inference across different regions of aggregation and the other imposing highly restrictive assumptions. To overcome these issues, we propose a model for high-resolution precipitation data, from which we can simulate realistic fields and explore the behaviour of spatial aggregates. Recent developments have seen spatial extensions of the Heffernan and Tawn (2004) model for conditional multivariate extremes, which can handle a wide range of dependence structures. Our contribution is twofold: extensions and improvements of this approach and its model inference for high-dimensional data; and a novel framework for deriving aggregates addressing edge effects and sub-regions without rain. We apply our modelling approach to gridded East-Anglia, UK precipitation data. Return-level curves for spatial aggregates over different regions of various sizes are estimated and shown to fit very well to the data.},
   author = {Jordan Richards and Jonathan A. Tawn and Simon Brown},
   doi = {10.48550/arxiv.2102.10906},
   journal = {Preprint},
   keywords = {extremal dependence,extreme precipitation,spatial aggregates,spatial conditional extremes},
   month = {2},
   title = {Modelling Extremes of Spatial Aggregates of Precipitation using Conditional Methods},
   url = {https://arxiv.org/abs/2102.10906v3},
   year = {2021},
}
@article{Jane2020,
   abstract = {Miami-Dade County (south-east Florida) is among the most vulnerable regions to sea level rise in the United States, due to a variety of natural and human factors. The co-occurrence of multiple, often statistically dependent flooding drivers-termed compound events-typically exacerbates impacts compared with their isolated occurrence. Ignoring dependencies between the drivers will potentially lead to underestimation of flood risk and under-design of flood defence structures. In Miami-Dade County water control structures were designed assuming full dependence between rainfall and Ocean-side Water Level (O-sWL), a conservative assumption inducing large safety factors. Here, an analysis of the dependence between the principal flooding drivers over a range of lags at three locations across the county is carried out. A two-dimensional analysis of rainfall and O-sWL showed that the magnitude of the conservative assumption in the original design is highly sensitive to the regional sea level rise projection considered. Finally, the vine copula and Heffernan and Tawn (2004) models are shown to outperform five standard higher-dimensional copulas in capturing the dependence between the principal drivers of compound flooding: Rainfall, O-sWL, and groundwater level. The work represents a first step towards the development of a new framework capable of capturing dependencies between different flood drivers that could potentially be incorporated into future Flood Protection Level of Service (FPLOS) assessments for coastal water control structures.},
   author = {Robert Jane and Luis Cadavid and Jayantha Obeysekera and Thomas Wahl},
   doi = {10.5194/NHESS-20-2681-2020},
   issn = {16849981},
   issue = {10},
   journal = {Natural Hazards and Earth System Sciences},
   month = {10},
   pages = {2681-2699},
   publisher = {Copernicus GmbH},
   title = {Multivariate statistical modelling of the drivers of compound flood events in south Florida},
   volume = {20},
   year = {2020},
}
@article{Salvadori2011,
   abstract = {Calculating return periods and design quantiles in a multivariate environment is a difficult problem: this paper tries to make the issue clear. First, we outline a possible way to introduce a consistent theoretical framework for the calculation of the return period in a multi-dimensional environment, based on Copulas and the Kendall's measure. Secondly, we introduce several approaches for the identification of suitable design events: these latter quantities are of utmost importance in practical applications, but their calculation is yet limited, due to the lack of an adequate theoretical environment where to embed the problem. Throughout the paper, a case study involving the behavior of a dam is used to illustrate the new concepts outlined in this work. © Author(s) 2011. CC Attribution 3.0 License.},
   author = {G. Salvadori and C. De Michele and F. Durante},
   doi = {10.5194/HESS-15-3293-2011},
   issn = {10275606},
   issue = {11},
   journal = {Hydrology and Earth System Sciences},
   pages = {3293-3305},
   title = {On the return period and design in a multivariate framework},
   volume = {15},
   year = {2011},
}
@article{Rojas2011,
   abstract = {In this work we asses the benefits of removing bias in climate forcing data used for hydrological climate change impact assessment at pan-European scale, with emphasis on floods. Climate simulations from the HIRHAM5-ECHAM5 model driven by the SRES-A1B emission scenario are corrected for bias using a histogram equalization method. As target for the bias correction we employ gridded interpolated observations of precipitation, average, minimum, and maximum temperature from the E-OBS data set. Bias removal transfer functions are derived for the control period 1961-1990. These are subsequently used to correct the climate simulations for the control period, and, under the assumption of a stationary error model, for the future time window 2071-2100. Validation against E-OBS climatology in the control period shows that the correction method performs successfully in removing bias in average and extreme statistics relevant for flood simulation over the majority of the European domain in all seasons. This translates into considerably improved simulations with the hydrological model of observed average and extreme river discharges at a majority of 554 validation river stations across Europe. Probabilities of extreme events derived employing extreme value techniques are also more closely reproduced. Results indicate that projections of future flood hazard in Europe based on uncorrected climate simulations, both in terms of their magnitude and recurrence interval, are likely subject to large errors. Notwithstanding the inherent limitations of the large-Correspondence to: R. Rojas scale approach used herein, this study strongly advocates the removal of bias in climate simulations prior to their use in hydrological impact assessment.},
   author = {R Rojas and L Feyen and A Dosio and D Bavera},
   doi = {10.5194/hess-15-2599-2011},
   journal = {Hydrol. Earth Syst. Sci},
   pages = {2599-2620},
   title = {Improving pan-European hydrological simulation of extreme events through statistical bias correction of RCM-driven climate simulations},
   volume = {15},
   url = {www.hydrol-earth-syst-sci.net/15/2599/2011/},
   year = {2011},
}
@article{Enayati2021,
   abstract = {This study aims to conduct a thorough investigation to compare the abilities of quantile mapping (QM) techniques as a bias correction method for the raw outputs from general circulation model (GCM)/ regional climate model (RCM) combinations. The Karkheh River basin in Iran was selected as a case study, due to its diverse topographic features, to test the performances of the bias correction methods under different conditions. The outputs of two GCM/RCM combinations (ICHEC and NOAA-ESM) were acquired from the coordinated regional climate downscaling experiment (CORDEX) dataset for this study. The results indicated that the performances of the QMs varied, depending on the transformation functions, parameter sets, and topographic conditions. In some cases, the QMs’ adjustments even made the GCM/RCM combinations’ raw outputs worse. The result of this study suggested that apart from DIST, PTF:scale, and SSPLIN, the rest of the considered QM methods can provide relatively improved results for both rainfall and temperature variables. It should be noted that, according to the results obtained from the diverse topographic conditions of the sub-basins, the empirical quantiles (QUANT) and robust empirical quantiles (RQUANT) methods proved to be excellent options to correct the bias of rainfall data, while all bias correction methods, with the notable exceptions of performed PTF:scale and SSPLIN, performed relatively well for the temperature variable.},
   author = {Maedeh Enayati and Omid Bozorg-Haddad and Javad Bazrafshan and Somayeh Hejabi and Xuefeng Chu},
   doi = {10.2166/WCC.2020.261},
   issn = {2040-2244},
   issue = {2},
   journal = {Journal of Water and Climate Change},
   keywords = {COREDEX,Climate change,Quantile mapping,RCM,| bias correction},
   month = {3},
   pages = {401-419},
   publisher = {IWA Publishing},
   title = {Bias correction capabilities of quantile mapping methods for rainfall and temperature variables},
   volume = {12},
   url = {http://creativecommons.org/licenses/by/4.0/},
   year = {2021},
}
@article{Osuch2016,
   abstract = {The aim of this study is to estimate likely changes in flood indices under a future climate and to assess the uncertainty in these estimates for selected catchments in Poland. Precipitation and temperature time series from climate simulations from the EURO-CORDEX initiative for the periods 1971-2000, 2021-2050 and 2071-2100 following the RCP4.5 and RCP8.5 emission scenarios have been used to produce hydrological simulations based on the HBV hydrological model. As the climate model outputs for Poland are highly biased, post processing in the form of bias correction was first performed so that the climate time series could be applied in hydrological simulations at a catchment-scale. The results indicate that bias correction significantly improves flow simulations and estimated flood indices based on comparisons with simulations from observed climate data for the control period. The estimated changes in the mean annual flood and in flood quantiles under a future climate indicate a large spread in the estimates both within and between the catchments. An ANOVA analysis was used to assess the relative contributions of the 2 emission scenarios, the 7 climate models and the 4 bias correction methods to the total spread in the projected changes in extreme river flow indices for each catchment. The analysis indicates that the differences between climate models generally make the largest contribution to the spread in the ensemble of the three factors considered. The results for bias corrected data show small differences between the four bias correction methods considered, and, in contrast with the results for uncorrected simulations, project increases in flood indices for most catchments under a future climate.},
   author = {Marzena Osuch and Deborah Lawrence and Hadush K Meresa and J J Napiorkowski and Renata J Romanowicz},
   doi = {10.1007/s00477-016-1296-5},
   isbn = {0047701612965},
   journal = {Stochastic Environmental Research and Risk Assessment},
   keywords = {ANOVA,Bias correction,Climate change,Floods,Poland},
   title = {Projected changes in flood indices in selected catchments in Poland in the 21st century},
   volume = {31},
   year = {2016},
}
@article{Haselsteiner2021,
   abstract = {Environmental contours are used to simplify the process of design response analysis. A wide variety of contour methods exist; however, there have been a very limited number of comparisons of these methods to date. This paper is the output of an open benchmarking exercise, in which contributors developed contours based on their preferred methods and submitted them for a blind comparison study. The exercise had two components—one, focusing on the robustness of contour methods across different offshore sites and, the other, focusing on characterizing sampling uncertainty. Nine teams of researchers contributed to the benchmark. The analysis of the submitted contours highlighted significant differences between contours derived via different methods. For example, the highest wave height value along a contour varied by as much as a factor of two between some submissions while the number of metocean data points or observations that fell outside a contour deviated by an order of magnitude between the contributions (even for contours with a return period shorter than the duration of the record). These differences arose from both different joint distribution models and different contour construction methods, however, variability from joint distribution models appeared to be higher than variability from contour construction methods.},
   author = {Andreas F. Haselsteiner and Ryan G. Coe and Lance Manuel and Wei Chai and Bernt Leira and Guilherme Clarindo and C. Guedes Soares and Ásta Hannesdóttir and Nikolay Dimitrov and Aljoscha Sander and Jan Hendrik Ohlendorf and Klaus Dieter Thoben and Guillaume de Hauteclocque and Ed Mackay and Philip Jonathan and Chi Qiao and Andrew Myers and Anna Rode and Arndt Hildebrandt and Boso Schmidt and Erik Vanem and Arne Bang Huseby},
   doi = {10.1016/J.OCEANENG.2021.109504},
   issn = {0029-8018},
   journal = {Ocean Engineering},
   keywords = {Environmental contour,Extreme response,Joint distribution,Metocean extremes,Structural reliability},
   month = {9},
   pages = {1-29},
   publisher = {Pergamon},
   title = {A benchmarking exercise for environmental contours},
   volume = {236},
   year = {2021},
}
@article{Moftakhari2019,
   abstract = {A method to link bivariate statistical analysis and hydrodynamic modeling for flood hazard estimation in tidal channels and estuaries is presented and discussed for the general case where flood hazards are linked to upstream riverine discharge Q and downstream ocean level, H. Using a bivariate approach, there are many possible combinations of Q and H that jointly reflect a specific return period, T, raising questions about the best choice as boundary forcing in a hydrodynamic model. We show, first of all, how possible Q and H values depend on whether the definition of T corresponds to the probability of exceedance of “H OR Q” or “H AND Q”. We also show that flood hazards defined by “OR” return periods are more conservative than “AND” return periods. Finally, we introduce a new composite water surface profile to represent the spatially distributed hazard for return period T. The composite profile synthesizes hydrodynamic model results from the “AND” hazard scenario and two scenarios based on traditional univariate analysis, a “Marginal Q” scenario and a “Marginal H” scenario.},
   author = {Hamed Moftakhari and Jochen E. Schubert and Amir AghaKouchak and Richard A. Matthew and Brett F. Sanders},
   doi = {10.1016/j.advwatres.2019.04.009},
   issn = {03091708},
   issue = {September 2018},
   journal = {Advances in Water Resources},
   keywords = {Bivariate statistical analysis,Coastal flood hazards,Compound flooding,Hydrodynamic modeling},
   pages = {28-38},
   publisher = {Elsevier Ltd},
   title = {Linking statistical and hydrodynamic modeling for compound flood hazard assessment in tidal channels and estuaries},
   volume = {128},
   url = {https://doi.org/10.1016/j.advwatres.2019.04.009},
   year = {2019},
}
@article{Youngman2019,
   abstract = {Generalized additive model (GAM) forms offer a flexible approach to capturing marginal variation. Such forms are used here to represent distributional variation in extreme values and presented in terms of spatio-temporal variation, which is often evident in environmental processes. A two-stage procedure is proposed that identifies extreme values as exceedances of a high threshold, which is defined as a fixed quantile and estimated by quantile regression. Excesses of the threshold are modelled with the generalized Pareto distribution (GPD). GAM forms are adopted for the threshold and GPD parameters, and directly estimated—in particular smoothing parameters—by restricted maximum likelihood, which provides an objective and relatively fast method of inference. The GAM models are used to produce return level maps for extreme wind gust speeds over the United States, which show extreme quantiles of the distribution of annual maximum gust speeds. Supplementary materials for this article are available online.},
   author = {Benjamin D. Youngman},
   doi = {10.1080/01621459.2018.1529596/SUPPL_FILE/UASA_A_1529596_SM9576.ZIP},
   issn = {1537274X},
   issue = {528},
   journal = {Journal of the American Statistical Association},
   keywords = {Extremal index,Generalized Pareto distribution,Quantile regression,Restricted maximum likelihood,Semiparametric model,U.S. wind gusts},
   month = {10},
   pages = {1865-1879},
   publisher = {American Statistical Association},
   title = {Generalized Additive Models for Exceedances of High Thresholds With an Application to Return Level Estimation for U.S. Wind Gusts},
   volume = {114},
   url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.2018.1529596},
   year = {2019},
}
@article{Simpson2022,
   abstract = {The key to successful statistical analysis of bivariate extreme events lies in flexible modelling of the tail dependence relationship between the two variables. In the extreme value theory literature, various techniques are available to model separate aspects of tail dependence, based on different asymptotic limits. Results from Balkema and Nolde (2010) and Nolde (2014) highlight the importance of studying the limiting shape of an appropriately-scaled sample cloud when characterising the whole joint tail. We now develop the first statistical inference for this limit set, which has considerable practical importance for a unified inference framework across different aspects of the joint tail. Moreover, Nolde and Wadsworth (2022) link this limit set to various existing extremal dependence frameworks. Hence, a by-product of our new limit set inference is the first set of self-consistent estimators for several extremal dependence measures, avoiding the current possibility of contradictory conclusions. In simulations, our limit set estimator is successful across a range of distributions, and the corresponding extremal dependence estimators provide a major joint improvement and small marginal improvements over existing techniques. We consider an application to sea wave heights, where our estimates successfully capture the expected weakening extremal dependence as the distance between locations increases.},
   author = {Emma S. Simpson and Jonathan A. Tawn},
   journal = {arXiv},
   keywords = {bivariate extremes,coefficient of asymptotic independence,conditional extremes,environ-,extremal dependence structure,gauge function,mental contours,sample cloud},
   month = {7},
   title = {Estimating the limiting shape of bivariate scaled sample clouds: with additional benefits of self-consistent inference for existing extremal dependence properties},
   volume = {2207.02626},
   url = {http://arxiv.org/abs/2207.02626},
   year = {2022},
}
@article{Rohrbeck2021a,
   abstract = {Hazard event sets, which correspond to a collection of synthetic flood events, are an important tool for practitioners to analyse and manage future flood risks. In this paper, we address the issue of generating hazard event sets for northern England and southern Scotland, a region which has been particularly affected by flooding over the past years. We start by analysing extreme river flow across 45 gauges in the region using recently introduced ideas from extreme value theory. This results in a set of extremal principal components, with the first components describing the large-scale structure of the observed flood events, and we find interesting connections to the region's topography and climate. We then introduce a framework to approximate the distribution of the extremal principal components which is dimension reducing in that it distinctly handles the large-scale and local extremal behavior. Synthetic flood events are subsequently generated efficiently by sampling from the fitted distribution. Our approach for generating hazard event sets can be easily implemented by practitioners and our results indicate good agreement between the observed and simulated extreme river flow dynamics.},
   author = {Christian Rohrbeck and Daniel Cooley},
   journal = {arXiv},
   keywords = {multivariate extreme value theory,nonparametric bootstrapping,principal com-},
   title = {Simulating flood event sets using extremal principal components},
   volume = {2106.00630},
   url = {http://arxiv.org/abs/2106.00630},
   year = {2021},
}
@article{Brunner2016,
   abstract = {Estimates of flood event magnitudes with a certain return period are required for the design of hydraulic structures. While the return period is clearly defined in a univariate context, its definition is more challenging when the problem at hand requires considering the dependence between two or more variables in a multivariate framework. Several ways of defining a multivariate return period have been proposed in the literature, which all rely on different probability concepts. Definitions use the conditional probability, the joint probability, or can be based on the Kendall's distribution or survival function. In this study, we give a comprehensive overview on the tools that are available to define a return period in a multivariate context. We especially address engineers, practitioners, and people who are new to the topic and provide them with an accessible introduction to the topic. We outline the theoretical background that is needed when one is in a multivariate setting and present the reader with different definitions for a bivariate return period. Here, we focus on flood events and the different probability concepts are explained with a pedagogical, illustrative example of a flood event characterized by the two variables peak discharge and flood volume. The choice of the return period has an important effect on the magnitude of the design variable quantiles, which is illustrated with a case study in Switzerland. However, this choice is not arbitrary and depends on the problem at hand. WIREs Water 2016, 3:819–833. doi: 10.1002/wat2.1173. This article is categorized under: Engineering Water > Methods Engineering Water > Planning Water Science of Water > Water Extremes.},
   author = {Manuela Irene Brunner and Jan Seibert and Anne Catherine Favre},
   doi = {10.1002/wat2.1173},
   issn = {20491948},
   issue = {6},
   journal = {Wiley Interdisciplinary Reviews: Water},
   pages = {819-833},
   title = {Bivariate return periods and their importance for flood peak and volume estimation},
   volume = {3},
   year = {2016},
}
@article{Engelke2021,
   abstract = {Extremal graphical models encode the conditional independence structure of multivariate extremes. For the popular class of H\"usler--Reiss models, we propose a majority voting algorithm for learning the underlying graph from data through $L^1$ regularized optimization. We derive explicit conditions that ensure consistent graph recovery for general connected graphs. A key statistic in our method is the empirical extremal variogram. We prove non-asymptotic concentration bounds for this quantity that hold for general multivariate Pareto distributions and are of independent interest.},
   author = {Sebastian Engelke and Michaël Lalancette and Stanislav Volgushev},
   title = {Learning extremal graphical structures in high dimensions},
   url = {http://arxiv.org/abs/2111.00840},
   year = {2021},
}
@article{Barlow2022,
   abstract = {Metocean extremes often vary systematically with covariates such as direction and season. In this work, we present non-stationary models for the size and rate of occurrence of peaks over threshold of metocean variables with respect to one- or two-dimensional covariates. The variation of model parameters with covariate is described using a piecewise-linear function in one or two dimensions defined with respect to pre-specified node locations on the covariate domain. Parameter roughness is regulated to provide optimal predictive performance, assessed using cross-validation, within a penalised likelihood framework for inference. Parameter uncertainty is quantified using bootstrap resampling. The models are used to estimate extremes of storm peak significant wave height with respect to direction and season for a site in the northern North Sea. A covariate representation based on a triangulation of the direction-season domain with six nodes gives good predictive performance. The penalised piecewise-linear framework provides a flexible representation of covariate effects at reasonable computational cost.},
   author = {Anna Maria Barlow and Ed Mackay and Emma Eastoe and Philip Jonathan},
   issue = {1},
   keywords = {covariate,extreme,non-stationary,penalised likelihood,significant wave height},
   pages = {1-19},
   title = {A penalised piecewise-linear model for non-stationary extreme value analysis of peaks over threshold},
   url = {http://arxiv.org/abs/2201.03915},
   year = {2022},
}
@article{Lee2021,
   abstract = {We consider the problem of performing prediction when observed values are at their highest levels. We construct an inner product space of nonnegative random variables from transformed-linear combinations of independent regularly varying random variables. The matrix of inner products corresponds to the tail pairwise dependence matrix, which summarizes tail dependence. The projection theorem yields the optimal transformed-linear predictor, which has the same form as the best linear unbiased predictor in non-extreme prediction. We also construct prediction intervals based on the geometry of regular variation. We show that these intervals have good coverage in a simulation study as well as in two applications; prediction of high pollution levels, and prediction of large financial losses.},
   author = {Jeongjin Lee and Daniel Cooley},
   keywords = {air pollution,by us national science,cooley were partially supported,financial risk,foundation grant,jeongjin lee and daniel,matrix,multivariate regular variation,projection theorem,tail pairwise dependence},
   pages = {1-24},
   title = {Transformed-linear prediction for extremes},
   url = {http://arxiv.org/abs/2111.03754},
   year = {2021},
}
@article{Rohrbeck2021,
   abstract = {To address the need for efficient inference for a range of hydrological extreme value problems, spatial pooling of information is the standard approach for marginal tail estimation. We propose the first extreme value spatial clustering methods which account for both the similarity of the marginal tails and the spatial dependence structure of the data to determine the appropriate level of pooling. Spatial dependence is incorporated in two ways: to determine the cluster selection and to account for dependence of the data over sites within a cluster when making the marginal inference. We introduce a statistical model for the pairwise extremal dependence which incorporates distance between sites, and accommodates our belief that sites within the same cluster tend to exhibit a higher degree of dependence than sites in different clusters. By combining the models for the marginal tails and the dependence structure, we obtain a composite likelihood for the joint spatial distribution. We use a Bayesian framework which learns about both the number of clusters and their spatial structure, and that enables the inference of site-specific marginal distributions of extremes to incorporate uncertainty in the clustering allocation. The approach is illustrated using simulations, the analysis of daily precipitation levels in Norway and daily river flow levels in the UK. Code and data for the simulation study and river flow example are available in the online supplementary materials.},
   author = {Christian Rohrbeck and Jonathan A. Tawn},
   doi = {10.1080/10618600.2020.1777139/SUPPL_FILE/UCGS_A_1777139_SM3181.ZIP},
   issn = {15372715},
   issue = {1},
   journal = {Journal of Computational and Graphical Statistics},
   keywords = {Bayesian clustering,Composite likelihood,Extreme value analysis,Reversible jump Markov chain Monte Carlo,Spatio-temporal modeling},
   pages = {91-105},
   publisher = {American Statistical Association},
   title = {Bayesian Spatial Clustering of Extremal Behavior for Hydrological Variables},
   volume = {30},
   url = {https://www.tandfonline.com/doi/abs/10.1080/10618600.2020.1777139},
   year = {2021},
}
@article{Zhao2021,
   abstract = {Design flood estimation is a fundamental task in hydrology. In this research, we propose a machine-learning-based approach to estimate design floods globally. This approach involves three stages: (i) estimating at-site flood frequency curves for global gauging stations using the Anderson-Darling test and a Bayesian Markov chain Monte Carlo (MCMC) method; (ii) clustering these stations into subgroups using a K-means model based on 12 globally available catchment descriptors; and (iii) developing a regression model in each subgroup for regional design flood estimation using the same descriptors. A total of 11ĝ€¯793 stations globally were selected for model development, and three widely used regression models were compared for design flood estimation. The results showed that (1) the proposed approach achieved the highest accuracy for design flood estimation when using all 12 descriptors for clustering; and the performance of the regression was improved by considering more descriptors during training and validation; (2) a support vector machine regression provided the highest prediction performance amongst all regression models tested, with a root mean square normalised error of 0.708 for 100-year return period flood estimation; (3) 100-year design floods in tropical, arid, temperate, cold and polar climate zones could be reliably estimated (i.e. <±25ĝ€¯% error), with relative mean bias (RBIAS) values of -0.199, -0.233, -0.169, 0.179 and -0.091 respectively; (4) the machine-learning-based approach developed in this paper showed considerable improvement over the index-flood-based method introduced by Smith et al. (2015, 10.1002/2014WR015814) for design flood estimation at global scales; and (5) the average RBIAS in estimation is less than 18ĝ€¯% for 10-, 20-, 50- and 100-year design floods. We conclude that the proposed approach is a valid method to estimate design floods anywhere on the global river network, improving our prediction of the flood hazard, especially in ungauged areas.},
   author = {Gang Zhao and Paul Bates and Jeffrey Neal and Bo Pang},
   doi = {10.5194/hess-25-5981-2021},
   issn = {16077938},
   issue = {11},
   journal = {Hydrology and Earth System Sciences},
   pages = {5981-5999},
   title = {Design flood estimation for global river networks based on machine learning models},
   volume = {25},
   year = {2021},
}
@article{Sharkey2019,
   abstract = {Intense precipitation events are commonly known to be associated with an increased risk of flooding. As a result of the societal and infrastructural risks linked with flooding, extremes of precipitation require careful modeling. Extreme value analysis is typically used to model large precipitation events, although an independent analysis of neighboring sites can produce very different estimates of risk. In reality, one would expect neighboring locations to exhibit similar extremal behavior. A common method of inducing spatial similarity of extremal behavior is to define a spatial structure on the parameters of a generalized Pareto distribution in a Bayesian hierarchical modeling framework. These methods are often implemented under the assumption of conditional independence in time and space, with the consequence that standard errors of parameter estimates are too small. We present an approach for accounting for spatial and temporal dependence when quantifying the uncertainty in Bayesian hierarchical models when the misspecification of conditional independence has been made. We also present comparisons of performance between this class of models and alternative approaches, applied to precipitation data in Great Britain.},
   author = {Paul Sharkey and Hugo C. Winter},
   doi = {10.1002/ENV.2529},
   issn = {1099-095X},
   issue = {1},
   journal = {Environmetrics},
   keywords = {Bayesian inference,climate extremes,covariate modelling,extreme value analysis,spatial modeling},
   month = {2},
   pages = {e2529},
   publisher = {John Wiley & Sons, Ltd},
   title = {A Bayesian spatial hierarchical model for extreme precipitation in Great Britain},
   volume = {30},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/env.2529 https://onlinelibrary.wiley.com/doi/abs/10.1002/env.2529 https://onlinelibrary.wiley.com/doi/10.1002/env.2529},
   year = {2019},
}
@article{RezaNajafi2013,
   abstract = {A spatial hierarchical Bayesian method is developed to model the extreme runoffs over two spatial domains in Columbia River Basin, USA. This method combines the limited number of data from different locations. The two spatial domains contain 31 and 20 gage stations, respectively, with daily streamflow records ranging from 30 to over 130 years. The generalized Pareto distribution (GPD) is employed for the analysis of extremes. Temporally independent data are generated using declustering procedure, where runoff extremes are first grouped into clusters and then the maximum of each cluster is retained. The GPD scale parameter is modeled based on a Gaussian geostatistical process and additional variables including the latitude, longitude, elevation, and drainage area are incorporated by means of a hierarchy. Metropolis-Hasting within Gibbs Sampler is used to infer the parameters of the GPD and the geostatistical process to estimate the return levels across the basins. The performance of the hierarchical Bayesian model is evaluated by comparing the estimates of 100 year return level floods with the maximum likelihood estimates at sites that are not used during the parameter inference process. Various prior distributions are used to assess the sensitivity of the posterior distributions. The selected model is then employed to estimate floods with different return levels in time slices of 15 years in order to detect possible trends in runoff extremes. The results show cyclic variations in the spatial average of the 100 year return level floods across the basins with consistent increasing trends distinguishable in some areas. Key Points Hierarchical model significantly reduces flood estimation uncertainty. GPD Scale not sensitive to covariance latent parameter prior distribution. Increasing trend of extreme runoffs in recent fifteen year period. ©2013. American Geophysical Union. All Rights Reserved.},
   author = {Mohammad Reza Najafi and Hamid Moradkhani},
   doi = {10.1002/WRCR.20381},
   issn = {1944-7973},
   issue = {10},
   journal = {Water Resources Research},
   keywords = {climate change,extreme,flood,runoff,spatial hierarchical Bayesian,uncertainty},
   month = {10},
   pages = {6656-6670},
   publisher = {John Wiley & Sons, Ltd},
   title = {Analysis of runoff extremes using spatial hierarchical Bayesian modeling},
   volume = {49},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/wrcr.20381 https://onlinelibrary.wiley.com/doi/abs/10.1002/wrcr.20381 https://agupubs.onlinelibrary.wiley.com/doi/10.1002/wrcr.20381},
   year = {2013},
}
@article{Liu2014,
   abstract = {Analysing the extremes of multi-dimensional data is a difficult task for many reasons, e.g.the wide range of extremal dependence structures and the scarcity of the data. Some popular approaches that account for various extremal dependence types are based on asymptotically motivated models so that there is a probabilistic underpinning basis for extrapolating beyond observed levels. Among these efforts, Heffernan and Tawn developed a methodology for modelling the distribution of a d-dimensional variable when at least one of its components is extreme. Their approach is based on a series (i = 1, . , d) of conditional distributions, in which the distribution of the rest of the vector is modelled given that the ith component is large. This model captures a wide range of dependence structures and is applicable to cases of large d. However their model suffers from a lack of self-consistency between these conditional distributions and so does not uniquely determine probabilities when more than one component is large. This paper looks at these unsolved issues and makes proposals which aim to improve the efficiency of the Heffernan-Tawn model in practice. Tests based on simulated and financial data suggest that the proposed estimation method increases the self-consistency and reduces the RMSE of the estimated coefficient of tail dependence. © 2014 Elsevier Inc.},
   author = {Y. Liu and J. A. Tawn},
   doi = {10.1016/J.JMVA.2014.02.003},
   issn = {0047259X},
   journal = {Journal of Multivariate Analysis},
   keywords = {Coefficient of tail dependence,Conditional analysis,Multivariate extreme value theory,Quantile regression,Self-consistency,The Heffernan-Tawn model},
   month = {5},
   pages = {19-35},
   title = {Self-consistent estimation of conditional multivariate extreme value distributions},
   volume = {127},
   year = {2014},
}
@article{Varty2021,
   abstract = {Investment in measuring a process more completely or accurately is only useful if these improvements can be utilised during modelling and inference. We consider how improvements to data quality over time can be incorporated when selecting a modelling threshold and in the subsequent inference of an extreme value analysis. Motivated by earthquake catalogues, we consider variable data quality in the form of rounded and incompletely observed data. We develop an approach to select a time-varying modelling threshold that makes best use of the available data, accounting for uncertainty in the magnitude model and for the rounding of observations. We show the benefits of the proposed approach on simulated data and apply the method to a catalogue of earthquakes induced by gas extraction in the Netherlands. This more than doubles the usable catalogue size and greatly increases the precision of high magnitude quantile estimates. This has important consequences for the design and cost of earthquake defences. For the first time, we find compelling data-driven evidence against the applicability of the Gutenberg-Richer law to these earthquakes. Furthermore, our approach to automated threshold selection appears to have much potential for generic applications of extreme value methods.},
   author = {Zak Varty and Jonathan A. Tawn and Peter M. Atkinson and Stijn Bierman},
   journal = {arXiv},
   title = {Inference for extreme earthquake magnitudes accounting for a time-varying measurement process},
   volume = {2102.00884},
   url = {http://arxiv.org/abs/2102.00884},
   year = {2021},
}
@article{Tran2022,
   abstract = {Extreme value statistics is the max analogue of classical statistics, while tropical geometry is the max analogue of classical geometry. In this paper, we review recent work where insights from tropical geometry were used to develop new, efficient learning algorithms with leading performance on benchmark datasets in extreme value statistics. We give intuition, backed by performances on benchmark datasets, for why and when causal inference for extremes should be employed over classical methods. Finally, we list some open problems at the intersection of causal inference, tropical geometry and deep learning.},
   author = {Ngoc M Tran},
   journal = {arXiv},
   pages = {1-19},
   title = {The tropical geometry of causal inference for extremes},
   url = {http://arxiv.org/abs/2207.10227},
   year = {2022},
}
@article{Deuber2021,
   abstract = {Causal inference for extreme events has many potential applications in fields such as medicine, climate science and finance. We study the extremal quantile treatment effect of a binary treatment on a continuous, heavy-tailed outcome. Existing methods are limited to the case where the quantile of interest is within the range of the observations. For applications in risk assessment, however, the most relevant cases relate to extremal quantiles that go beyond the data range. We introduce an estimator of the extremal quantile treatment effect that relies on asymptotic tail approximations and uses a new causal Hill estimator for the extreme value indices of potential outcome distributions. We establish asymptotic normality of the estimators even in the setting of extremal quantiles, and we propose a consistent variance estimator to achieve valid statistical inference. In simulation studies we illustrate the advantages of our methodology over competitors, and we apply it to a real data set.},
   author = {David Deuber and Jinzhou Li and Sebastian Engelke and Marloes H. Maathuis},
   issue = {1},
   journal = {arXiv},
   title = {Estimation and Inference of Extremal Quantile Treatment Effects for Heavy-Tailed Distributions},
   volume = {1},
   url = {http://arxiv.org/abs/2110.06627},
   year = {2021},
}
@article{Contzen2022,
   abstract = {We present a new approach to modeling the future development of extreme temperatures globally and on a long time-scale by using non-stationary generalized extreme value distributions in combination with logistic functions. This approach is applied to data from the fully coupled climate model AWI-ESM. It enables us to investigate how extremes will change depending on the geographic location not only in terms of the magnitude, but also in terms of the timing of the changes. We observe that in general, changes in extremes are stronger and more rapid over land masses than over oceans. In addition, our models differentiate between changes in mean, in variability and in distributional shape, allowing for developments in these statistics to take place independently and at different times. Different models are presented and the Bayesian Information Criterion is used for model selection. It turns out that in most regions, changes in mean and variance take place simultaneously while the shape parameter of the distribution is predicted to stay constant. In the Arctic region, however, a different picture emerges: There, climate variability drastically and abruptly increases around 2050 due to the melting of ice, whereas changes in the mean values take longer and come into effect later.},
   author = {Justus Contzen and Thorsten Dickhaus and Gerrit Lohmann},
   journal = {arXiv},
   pages = {1-23},
   title = {Long-term temporal evolution of extreme temperature in a warming Earth},
   url = {http://arxiv.org/abs/2207.13512},
   year = {2022},
}
@article{Belzile2022,
   abstract = {This review paper surveys recent development in software implementations for extreme value analyses since the publication of Stephenson and Gilleland (2006) and Gilleland et al. (2013), here with a focus on numerical challenges. We provide a comparative review by topic and highlight differences in existing routines, along with listing areas where software development is lacking. The online supplement contains two vignettes providing a comparison of implementations of frequentist and Bayesian estimation of univariate extreme value models.},
   author = {Léo R. Belzile and Christophe Dutang and Paul J. Northrop and Thomas Opitz},
   journal = {arXiv},
   pages = {1-35},
   title = {A modeler's guide to extreme value software},
   url = {http://arxiv.org/abs/2205.07714},
   year = {2022},
}
@article{DArcy2022a,
   abstract = {Reliable estimates of sea level return levels are crucial for coastal flooding risk assessments and for coastal flood defence design. We describe a novel method for estimating extreme sea levels that is the first to capture seasonality, interannual variations and longer term changes. We use a joint probabilities method, with skew surge and peak tide as two sea level components. The tidal regime is predictable but skew surges are stochastic. We present a statistical model for skew surges, where the main body of the distribution is modelled empirically whilst a non-stationary generalised Pareto distribution (GPD) is used for the upper tail. We capture within-year seasonality by introducing a daily covariate to the GPD model and allowing the distribution of peak tides to change over months and years. Skew surge-peak tide dependence is accounted for via a tidal covariate in the GPD model and we adjust for skew surge temporal dependence through the subasymptotic extremal index. We incorporate spatial prior information in our GPD model to reduce the uncertainty associated with the highest return level estimates. Our results are an improvement on current return level estimates, with previous methods typically underestimating. We illustrate our method at four UK tide gauges.},
   author = {Eleanor D'Arcy and Jonathan A. Tawn and Amélie Joly and Dafni E. Sifnioti},
   journal = {arXiv},
   keywords = {extreme sea levels,generalised pareto distribution,non-stationarity,skew surge},
   pages = {1-21},
   title = {Accounting for Seasonality in Extreme Sea Level Estimation},
   url = {http://arxiv.org/abs/2207.09870},
   year = {2022},
}
@article{Bernard2013,
   abstract = {One of the main objectives of statistical climatology is to extract relevant information hidden in complex spatial-temporal climatological datasets. To identify spatial patterns, most well-known statistical techniques are based on the concept of intra- and intercluster variances (like the k-means algorithm or EOFs). As analyzing quantitative extremes like heavy rainfall has become more and more prevalent for climatologists and hydrologists during these last decades, finding spatial patterns with methods based on deviations from the mean (i.e., variances) may not be the most appropriate strategy in this context of studying such extremes. For practitioners, simple and fast clustering tools tailored for extremes have been lacking. A possible avenue to bridging this methodological gap resides in taking advantage of multivariate extreme value theory, a welldeveloped research field in probability, and to adapt it to the context of spatial clustering. In this paper, a novel algorithm based on this plan is proposed and studied. The approach is compared and discussed with respect to the classical k-means algorithm throughout the analysis of weekly maxima of hourly precipitation recorded in France (fall season, 92 stations, 1993-2011). © 2013 American Meteorological Society.},
   author = {Elsa Bernard and Philippe Naveau and Mathieu Vrac and Olivier Mestre},
   doi = {10.1175/JCLI-D-12-00836.1},
   issn = {08948755},
   issue = {20},
   journal = {Journal of Climate},
   pages = {7929-7937},
   title = {Clustering of maxima: Spatial dependencies among heavy rainfall in france},
   volume = {26},
   year = {2013},
}
@article{Hu2022,
   abstract = {Multivariate extreme value distributions are a common choice for modelling mul- tivariate extremes. In high dimensions, however, the construction of flexible and par- simonious models is challenging. We propose to combine bivariate extreme value dis- tributions into a Markov random field with respect to a tree. Although in general not an extreme value distribution itself, this Markov tree is attracted by a multivari- ate extreme value distribution. The latter serves as a tree-based approximation to an unknown extreme value distribution with the given bivariate distributions as margins. Given data, we learn an appropriate tree structure by Prim’s algorithm with estimated pairwise upper tail dependence coefficients or Kendall’s tau values as edge weights. The distributions of pairs of connected variables can be fitted in various ways. The resulting tree-structured extreme value distribution allows for inference on rare event probabili- ties, as illustrated on river discharge data from the upper Danube basin.},
   author = {Shuang Hu and Zuoxiang Peng and Johan Segers},
   doi = {10.1093/biomet/77.2.245},
   issn = {00063444},
   journal = {arXiv},
   keywords = {Extreme value theory,Generalized Pareto distribution,Multivariate exponential distribution,Nonregular estimation},
   title = {Modelling multivariate extreme value distributions via Markov trees},
   year = {2022},
}
@article{Richards2022,
   abstract = {Risk management in many environmental settings requires an understanding of the mechanisms that drive extreme events. Useful metrics for quantifying such risk are extreme quantiles of response variables conditioned on predictor variables that describe e.g., climate, biosphere and environmental states. Typically these quantiles lie outside the range of observable data and so, for estimation, require specification of parametric extreme value models within a regression framework. Classical approaches in this context utilise linear or additive relationships between predictor and response variables and suffer in either their predictive capabilities or computational efficiency; moreover, their simplicity is unlikely to capture the truly complex structures that lead to the creation of extreme wildfires. In this paper, we propose a new methodological framework for performing extreme quantile regression using artificial neutral networks, which are able to capture complex non-linear relationships and scale well to high-dimensional data. The "black box" nature of neural networks means that they lack the desirable trait of interpretability often favoured by practitioners; thus, we combine aspects of linear, and additive, models with deep learning to create partially interpretable neural networks that can be used for statistical inference but retain high prediction accuracy. To complement this methodology, we further propose a novel point process model for extreme values which overcomes the finite lower-endpoint problem associated with the generalised extreme value class of distributions. Efficacy of our unified framework is illustrated on U.S. wildfire data with a high-dimensional predictor set and we illustrate vast improvements in predictive performance over linear and spline-based regression techniques.},
   author = {Jordan Richards and Raphaël Huser},
   journal = {arXiv},
   keywords = {deep learning,explainable ai,extreme quantile regression,neural networks,spatio-temporal extremes},
   pages = {1-50},
   title = {A unifying partially-interpretable framework for neural network-based extreme quantile regression},
   url = {http://arxiv.org/abs/2208.07581},
   year = {2022},
}
@article{Pasche2022,
   abstract = {Risk assessment for extreme events requires accurate estimation of high quantiles that go beyond the range of historical observations. When the risk depends on the values of observed predictors, regression techniques are used to interpolate in the predictor space. We propose the EQRN model that combines tools from neural networks and extreme value theory into a method capable of extrapolation in the presence of complex predictor dependence. Neural networks can naturally incorporate additional structure in the data. We develop a recurrent version of EQRN that is able to capture complex sequential dependence in time series. We apply this method to forecasting of flood risk in the Swiss Aare catchment. It exploits information from multiple covariates in space and time to provide one-day-ahead predictions of return levels and exceedances probabilities. This output complements the static return level from a traditional extreme value analysis and the predictions are able to adapt to distributional shifts as experienced in a changing climate. Our model can help authorities to manage flooding more effectively and to minimize their disastrous impacts through early warning systems.},
   author = {Olivier C. Pasche and Sebastian Engelke},
   journal = {arXiv},
   keywords = {extreme value theory,generalized pareto distribution,machine learning,prediction,recurrent neural network},
   pages = {1-27},
   title = {Neural Networks for Extreme Quantile Regression with an Application to Forecasting of Flood Risk},
   url = {http://arxiv.org/abs/2208.07590},
   year = {2022},
}
@article{Dietz2022,
   abstract = {Bayesian hierarchical models are proposed for modeling tropical cyclone characteristics and their damage potential in the Atlantic basin. We model the joint probability distribution of tropical cyclone characteristics and their damage potential at two different temporal scales, while taking several climate indices into account. First, a predictive model for an entire season is developed that forecasts the number of cyclone events that will take place, the probability of each cyclone causing some amount of damage, and the monetized value of damages. Then, specific characteristics of individual cyclones are considered to predict the monetized value of the damage it will cause. Robustness studies are conducted and excellent prediction power is demonstrated across different data science models and evaluation techniques.},
   author = {Lindsey Dietz and Sakshi Arya and Snigdhansu Chatterjee},
   journal = {arXiv},
   pages = {34-36},
   title = {Predictions of damages from Atlantic tropical cyclones: a hierarchical Bayesian study on extremes},
   url = {http://arxiv.org/abs/2208.07899},
   year = {2022},
}
@article{Groll2017,
   abstract = {Long and consistent wave data are important for analysing wave climate variability and change. Moreover, such wave data are also needed in coastal and offshore design and for addressing safety-related issues at sea. Using the third-generation spectral wave model WAM a multi-decadal wind-wave hindcast for the North Sea covering the period 1949-2014 was produced. The hindcast is part of the coastDat database representing a consistent and homogeneous met-ocean data set. It is shown that despite not being perfect, data from the wave hindcast are generally suitable for wave climate analysis. In particular, comparisons of hindcast data with in situ and satellite observations show on average a reasonable agreement, while a tendency towards overestimation of the highest waves could be inferred. Despite these limitations, the wave hindcast still provides useful data for assessing wave climate variability and change as well as for risk analysis, in particular when conservative estimates are needed. Hindcast data are stored at the World Data Center for Climate (WDCC) and can be freely accessed using the <a hrefCombining double low line"https://doi.org/10.1594/WDCC/coastDat-2-WAM-North-Sea" targetCombining double low line"-blank">doi:10.1594/WDCC/coastDat-2-WAM-North-Sea</a> Groll and Weisse(2016) or via the coastDat web-page <a hrefCombining double low line"http://www.coastdat.de" targetCombining double low line"-blank">http://www.coastdat.de</a>.},
   author = {Nikolaus Groll and Ralf Weisse},
   doi = {10.5194/ESSD-9-955-2017},
   issn = {18663516},
   issue = {2},
   journal = {Earth System Science Data},
   month = {12},
   pages = {955-968},
   publisher = {Copernicus GmbH},
   title = {A multi-decadal wind-wave hindcast for the North Sea 1949-2014: CoastDat2},
   volume = {9},
   year = {2017},
}
@article{Nguyen2022,
   abstract = {Extreme events such as natural and economic disasters leave lasting impacts on society and motivate the analysis of extremes from data. While classical statistical tools based on Gaussian distributions focus on average behaviour and can lead to persistent biases when estimating extremes, extreme value theory (EVT) provides the mathematical foundations to accurately characterise extremes. In this paper, we adapt a dynamic extreme value model recently introduced to forecast financial risk from high frequency data to the context of natural hazard forecasting. We demonstrate its wide applicability and flexibility using a case study of the Piton de la Fournaise volcano. The value of using EVT-informed thresholds to identify and model extreme events is shown through forecast performance.},
   author = {Michele Nguyen and Almut E. D. Veraart and Benoit Taisne and Tan Chiou Ting and David Lallemant},
   journal = {arXiv},
   title = {A dynamic extreme value model with applications to volcanic eruption forecasting},
   url = {http://arxiv.org/abs/2208.10724},
   year = {2022},
}
@article{Berghuijs2019,
   abstract = {River flooding is a common hazard, causing billions of dollars in annual losses. Flood impacts are shaped by the spatial scale over which different rivers flood simultaneously, but this dimension of flood risk remains largely unknown. Using annual flood data from several thousand European rivers, we demonstrate that the flood synchrony scale—the distance over which multiple rivers flood near synchronously—far exceeds the size of individual drainage basins and varies regionally by more than an order of magnitude. These data also show that flood synchrony scales have grown by about 50% over the period 1960–2010. Detrended flood synchrony values are serially correlated, implying that years with spatially extensive floods tend to follow one another. These findings reveal that flood risks are correlated well beyond the individual drainage basins for which flood hazards are typically assessed and managed.},
   author = {Wouter R. Berghuijs and Scott T. Allen and Shaun Harrigan and James W. Kirchner},
   doi = {10.1029/2018GL081883},
   issn = {19448007},
   issue = {3},
   journal = {Geophysical Research Letters},
   keywords = {catchment,flood,hazard},
   pages = {1423-1428},
   title = {Growing Spatial Scales of Synchronous River Flooding in Europe},
   volume = {46},
   year = {2019},
}
@article{Krock2022,
   abstract = {Current models for spatial extremes are concerned with the joint upper (or lower) tail of the distribution at two or more locations. Such models cannot account for teleconnection patterns of two-meter surface air temperature ($T_\{2m\}$) in North America, where very low temperatures in the contiguous Unites States (CONUS) may coincide with very high temperatures in Alaska in the wintertime. This dependence between warm and cold extremes motivates the need for a model with opposite-tail dependence in spatial extremes. This work develops a statistical modeling framework which has flexible behavior in all four pairings of high and low extremes at pairs of locations. In particular, we use a mixture of rotations of common Archimedean copulas to capture various combinations of four-corner tail dependence. We study teleconnected $T_\{2m\}$ extremes using ERA5 reanalysis of daily average two-meter temperature during the boreal winter. The estimated mixture model quantifies the strength of opposite-tail dependence between warm temperatures in Alaska and cold temperatures in the midlatitudes of North America, as well as the reverse pattern. These dependence patterns are shown to correspond to blocked and zonal patterns of mid-tropospheric flow. This analysis extends the classical notion of correlation-based teleconnections to considering dependence in higher quantiles.},
   author = {Mitchell L. Krock and Adam H. Monahan and Michael L. Stein},
   keywords = {blocking,copula,extremes,reanalysis,spatial data,tail dependence,teleconnec-,tion},
   title = {Teleconnected warm and cold extremes of North American wintertime temperatures},
   url = {http://arxiv.org/abs/2208.12092},
   year = {2022},
}
@article{Agovino2021,
   abstract = {Wildfires constitute a serious threat for both the environment and human well-being. The US fire policy aims to tackle this problem, devoting a sizeable amount of resources and resorting extensively to fire suppression strategies. The theoretical literature has established a link between climate conditions and wildfire incidence. Using state-level data from 2002 to 2013 for the USA, this work proposes a wildfire incidence indicator and runs a generalized spatial ordered probit model in order to test the findings of the previous literature empirically. Moreover, this article investigates the extent of spatial spillovers in the climatic covariates. The results highlight a significant impact of precipitation and temperature on fire incidence and provide some evidence of the role of spatial spillovers. In particular, transitions from lower to higher wildfire incidence levels are significantly encouraged by increases in local temperature and significantly discouraged by increases in both local precipitation and lagged precipitation. The present analysis complements the recent literature, confirming the previous findings with a solid empirical investigation and offering a policy-oriented picture of wildfire risks all over the USA.},
   author = {Massimiliano Agovino and Massimiliano Cerciello and Aniello Ferraro and Antonio Garofalo},
   doi = {10.1007/s10668-020-00863-2},
   issn = {15732975},
   issue = {4},
   journal = {Environment, Development and Sustainability},
   keywords = {Fire policy,Generalized spatial ordered probit,Spatial spillovers,Wildfires},
   month = {4},
   pages = {6084-6105},
   publisher = {Springer Science and Business Media B.V.},
   title = {Spatial analysis of wildfire incidence in the USA: the role of climatic spillovers},
   volume = {23},
   year = {2021},
}
@article{Bodik2021,
   abstract = {Consider two stationary time series with heavy-tailed marginal distributions. We aim to detect whether they have a causal relation, that is, if a change in one of them causes a change in the other. Usual methods for causality detection are not well suited if the causal mechanisms only manifest themselves in extremes. This paper aims to detect the causal relations in extremes between time series. We define the so-called causal tail coefficient for time series, which, under some assumptions, correctly detects the asymmetrical causal relations between extremes of the time series. The advantage is that this method works even if nonlinear relations and common ancestors are present. Moreover, we mention how our method can help detect a time delay between the two time series. We describe some of its asymptotic properties and show how it performs on some simulations. Finally, we show how this method works on space-weather and hydro-meteorological data sets.},
   author = {Juraj Bodik and Milan Paluš and Zbyněk Pawlas},
   journal = {arXiv},
   month = {12},
   title = {Causality in extremes of time series},
   url = {http://arxiv.org/abs/2112.10858},
   year = {2021},
}
@article{,
   abstract = {In situations where both extreme and non-extreme data are of interest, modelling the whole data set accurately is important. In a univariate framework, modelling the bulk and tail of a distribution has been studied before, but little work has been done when more than one variable is of concern. We propose a dependence model that blends two copulas with different characteristics over the whole range of the data support. One copula is tailored to the bulk and the other to the tail, with a dynamic weighting function employed to transition smoothly between them. We investigate tail dependence properties numerically and use simulation to confirm that the blended model is flexible enough to capture a wide variety of structures. We apply our model to study the dependence between temperature and ozone concentration at two sites in the UK and compare with a single copula fit. The proposed model provides a better, more flexible, fit to the data, and is also capable of capturing complex dependence structures.},
   author = {Lídia M. André and Jennifer L. Wadsworth and Adrian O'Hagan},
   month = {9},
   title = {Joint modelling of the body and tail of bivariate data},
   url = {http://arxiv.org/abs/2209.05795},
   year = {2022},
}
@article{Berghuijs2014,
   abstract = {Recent hydrologic synthesis efforts have presented evidence that the seasonal water balance is at the core of overall catchment responses, and understanding it will assist in predicting signatures of streamflow variability at other time scales, including interannual variability, the flow duration curve, low flows, and floods. In this study, we group 321 catchments located across the continental U.S. into several clusters with similar seasonal water balance behavior. We then delineate the boundaries between these clusters on the basis of a similarity framework based on three hydroclimatic indices that represent aridity, precipitation timing, and snowiness. The clustering of catchments based on the seasonal water balance has a strong relationship not only with regional patterns of the three climate indices but also with regional ecosystem, soil, and vegetation classes, which point to the strong dependence of these physiographic characteristics on seasonal climate variations and the hydrologic regimes. Building on these catchment clusters, we demonstrate that the seasonal water balance does have an imprint on signatures of streamflow variability over a wide range of time scales (daily to decadal) and a wide range of states (low flows to floods). The seasonal water balance is well integrated into variability at seasonal and longer time scales, but is only partly reflected in the signatures at shorter time scales, including flooding responses. Overall, the seasonal water balance has proven to be a similarity measure that serves as a link between both short-term hydrologic responses and long-term adaptation of the landscape with climate. Key Points Framework for the seasonal water balance is developed based on climatic controls The 321 catchments are grouped into 10 clusters with similar seasonal water balances The seasonal water balance has an imprint both on streamflow and landscape © 2014. American Geophysical Union. All Rights Reserved.},
   author = {Wouter R. Berghuijs and Murugesu Sivapalan and Ross A. Woods and Hubert H.G. Savenije},
   doi = {10.1002/2014WR015692},
   issn = {19447973},
   issue = {7},
   journal = {Water Resources Research},
   keywords = {classification,hydrologic similarity,seasonal water balance,streamflow variability,vegetation adaptation},
   pages = {5638-5661},
   publisher = {Blackwell Publishing Ltd},
   title = {Patterns of similarity of seasonal water balances: A window into streamflow variability over a range of time scales},
   volume = {50},
   year = {2014},
}
@article{Knoben2018,
   abstract = {Classification is essential in the study of natural systems, yet hydrology has no formal way to structure the climatic forcing that underlies hydrologic response. Various climate classification systems can be borrowed from other disciplines but these are based on different organizing principles than a hydrological classification might need. This work presents a hydrologically informed way to quantify global climates, explicitly addressing the shortcomings in earlier climate classifications. In this work, causal factors (climate) and hydrologic response (streamflow) are separated, meaning that our classification scheme is based only on climatic information and can be evaluated with independent streamflow data. Using gridded global climate data, we calculate three dimensionless indices per grid cell, describing annual aridity, aridity seasonality, and precipitation-as-snow. We use these indices to create several climate groups and define the membership degree of 1,103 catchments to each of the climate groups, based on each catchment's climate. Streamflow patterns within each group tend to be similar, and tend to be different between groups. Visual comparison of flow regimes and Wilcoxon two-sample statistical tests on 16 streamflow signatures show that this index-based approach is more effective than the often-used Köppen-Geiger classification for grouping hydrologically similar catchments. Climate forcing exerts a strong control on typical hydrologic response and we show that at the global scale both change gradually in space. We argue that hydrologists should consider the hydroclimate as a continuous spectrum defined by the three climate indices, on which all catchments are positioned and show examples of this in a regionalization context.},
   author = {Wouter J.M. Knoben and Ross A. Woods and Jim E. Freer},
   doi = {10.1029/2018WR022913},
   issn = {19447973},
   issue = {7},
   journal = {Water Resources Research},
   keywords = {climate classification,hydroclimatic indices,hydrologic regimes},
   month = {7},
   pages = {5088-5109},
   publisher = {Blackwell Publishing Ltd},
   title = {A Quantitative Hydrological Climate Classification Evaluated With Independent Streamflow Data},
   volume = {54},
   year = {2018},
}
@article{Diawara2022,
   abstract = {According to the Chinese Health Statistics Yearbook, in 2005, the number of traffic accidents was 187781 with total direct property losses of 103691.7 (10000 Yuan). This research aims to fill the gap in the literature by investigating the extreme claim sizes not only for the entire portfolio. This empirical study investigates the behavior of the upper tail of the claim size by class of policyholders.},
   author = {Daouda Diawara and Ladji Kane and Soumaila Dembele and Gane Samb Lo},
   month = {9},
   title = {Applying of the Extreme Value Theory for determining extreme claims in the automobile insurance sector: Case of a China car insurance},
   url = {http://arxiv.org/abs/2209.10194},
   year = {2022},
}
@article{Rohrbeck2022,
   abstract = {A key aspect where extreme values methods differ from standard statistical models is through having asymptotic theory to provide a theoretical justification for the nature of the models used for extrapolation. In multivariate extremes many different asymptotic theories have been proposed, partly as a consequence of the lack of ordering property with vector random variables. One class of multivariate models, based on conditional limit theory as one variable becomes extreme, developed by Heffernan and Tawn (2004), has developed wide practical usage. The underpinning value of this approach has been supported by further theoretical characterisations of the limiting relationships by Heffernan and Resnick (2007) and Resnick and Zeber (2014). However Drees and Jan\{\ss\}en (2017) provided a number of counterexamples of their results, which potentially undermine the trust in these statistical methods. Here we show that in the Heffernan and Tawn (2004) framework, which involves marginal standardisation to a common exponentially decaying tailed marginal distribution, the problems in these examples are removed.},
   author = {Christian Rohrbeck and Jonathan A Tawn},
   month = {9},
   title = {Standardisation overcomes counter-examples of conditional extremes},
   url = {http://arxiv.org/abs/2209.10936},
   year = {2022},
}
@article{Segers2022,
   abstract = {Graphical models with heavy-tailed factors can be used to model extremal dependence or causality between extreme events. In a Bayesian network, variables are recursively defined in terms of their parents according to a directed acyclic graph (DAG). We focus on max-linear graphical models with respect to a special type of graphs, which we call a \emph\{tree of transitive tournaments\}. The latter are block graphs combining in a tree-like structure a finite number of transitive tournaments, each of which is a DAG in which every two nodes are connected. We study the limit of the joint tails of the max-linear model conditionally on the event that a given variable exceeds a high threshold. Under a suitable condition, the limiting distribution involves the factorization into independent increments along the shortest trail between two variables, thereby imitating the behavior of a Markov random field. We are also interested in the identifiability of the model parameters in case some variables are latent and only a subvector is observed. It turns out that the parameters are identifiable under a criterion on the nodes carrying the latent variables which is easy and quick to check.},
   author = {Johan Segers and Stefka Asenova},
   month = {9},
   title = {Max-linear graphical models with heavy-tailed factors on trees of transitive tournaments},
   url = {http://arxiv.org/abs/2209.14938},
   year = {2022},
}
@article{Lee2022,
   abstract = {We develop a method for investigating conditional extremal relationships between variables at their extreme levels. We consider an inner product space constructed from transformed-linear combinations of independent regularly varying random variables. By developing the projection theorem for the inner product space, we derive the concept of partial tail correlation via projection theorem. We show that the partial tail correlation can be understood as the inner product of the prediction errors associated with the best transformed-linear prediction. Similar to Gaussian cases, we connect partial tail correlation to the inverse of the inner product matrix and show that a zero in this inverse implies a partial tail correlation of zero. We develop a hypothesis test for the partial tail correlation of zero and demonstrate the performance in a simulation study as well as in two applications: high nitrogen dioxide levels in Washington DC and extreme river discharges in the upper Danube basin.},
   author = {Jeongjin Lee and Daniel Cooley},
   month = {10},
   title = {Partial Tail Correlation for Extremes},
   url = {http://arxiv.org/abs/2210.02048},
   year = {2022},
}
@article{Varin2011,
   author = {Cristiano Varin and Nancy Reid and David Firth},
   journal = {Statistica Sinica},
   pages = {5-42},
   title = {An overview of composite likelihood methods},
   volume = {21},
   year = {2011},
}
@article{Vettori2018,
   abstract = {Various nonparametric and parametric estimators of extremal dependence have been proposed in the literature. Nonparametric methods commonly suffer from the curse of dimensionality and have been mostly implemented in extreme-value studies up to three dimensions, whereas parametric models can tackle higher-dimensional settings. In this paper, we assess, through a vast and systematic simulation study, the performance of classical and recently proposed estimators in multivariate settings. In particular, we first investigate the performance of nonparametric methods and then compare them with classical parametric approaches under symmetric and asymmetric dependence structures within the commonly used logistic family. We also explore two different ways to make nonparametric estimators satisfy the necessary dependence function shape constraints, finding a general improvement in estimator performance either (i) by substituting the estimator with its greatest convex minorant, developing a computational tool to implement this method for dimensions D≥ 2 or (ii) by projecting the estimator onto a subspace of dependence functions satisfying such constraints and taking advantage of Bernstein–Bézier polynomials. Implementing the convex minorant method leads to better estimator performance as the dimensionality increases.},
   author = {Sabrina Vettori and Raphaël Huser and Marc G. Genton},
   doi = {10.1007/s11222-017-9745-7},
   issn = {15731375},
   issue = {3},
   journal = {Statistics and Computing},
   keywords = {Asymmetric logistic model,Componentwise maxima,Convexity,Copula,Greatest convex minorant,Nonparametric and parametric estimators,Pickands dependence function},
   month = {5},
   pages = {525-538},
   publisher = {Springer New York LLC},
   title = {A comparison of dependence function estimators in multivariate extremes},
   volume = {28},
   year = {2018},
}
@article{Eastoe2014,
   abstract = {The traditional approach to multivariate extreme values has been through the multivariate extreme value distribution G, characterised by its spectral measure H and associated Pickands' dependence function A. More generally, for all asymptotically dependent variables, H determines the probability of all multivariate extreme events. When the variables are asymptotically dependent and under the assumption of unit Fréchet margins, several methods exist for the estimation of G, H and A which use variables with radial component exceeding some high threshold. For each of these characteristics, we propose new asymptotically consistent nonparametric estimators which arise from Heffernan and Tawn's approach to multivariate extremes that conditions on variables with marginal values exceeding some high marginal threshold. The proposed estimators improve on existing estimators in three ways. First, under asymptotic dependence, they give self-consistent estimators of G, H and A; existing estimators are not self-consistent. Second, these existing estimators focus on the bivariate case, whereas our estimators extend easily to describe dependence in the multivariate case. Finally, for asymptotically independent cases, our estimators can model the level of asymptotic independence; whereas existing estimators for the spectral measure treat the variables as either being independent, or asymptotically dependent. For asymptotically dependent bivariate random variables, the new estimators are found to compare favourably with existing estimators, particularly for weak dependence. The method is illustrated with an application to finance data. © 2013 Springer Science+Business Media New York.},
   author = {Emma F. Eastoe and Janet E. Heffernan and Jonathan A. Tawn},
   doi = {10.1007/s10687-013-0173-6},
   issn = {1572915X},
   issue = {1},
   journal = {Extremes},
   keywords = {Asymptotic dependence,Conditional distribution,Economic monetary union,Multivariate extreme value theory,Nonparametric modelling,Poisson process},
   pages = {25-43},
   publisher = {Kluwer Academic Publishers},
   title = {Nonparametric estimation of the spectral measure, and associated dependence measures, for multivariate extreme values using a limiting conditional representation},
   volume = {17},
   year = {2014},
}
@article{Moins2022,
   abstract = {Combining extreme value theory with Bayesian methods offers several advantages, such as a quantification of uncertainty on parameter estimation or the ability to study irregular models that cannot be handled by frequentist statistics. However, it comes with many options that are left to the user concerning model building, computational algorithms, and even inference itself. Among them, the parameterization of the model induces a geometry that can alter the efficiency of computational algorithms, in addition to making calculations involved. We focus on the Poisson process characterization of extremes and outline two key benefits of an orthogonal parameterization addressing both issues. First, several diagnostics show that Markov chain Monte Carlo convergence is improved compared with the original parameterization. Second, orthogonalization also helps deriving Jeffreys and penalized complexity priors, and establishing posterior propriety. The analysis is supported by simulations, and our framework is then applied to extreme level estimation on river flow data.},
   author = {Théo Moins and Julyan Arbel and Stéphane Girard and Anne Dutfoy},
   month = {10},
   title = {Reparameterization of extreme value framework for improved Bayesian workflow},
   url = {http://arxiv.org/abs/2210.05224},
   year = {2022},
}
@article{Nolde2014,
   abstract = {The residual dependence coefficient was originally introduced by Ledford and Tawn (1996) [25] as a measure of residual dependence between extreme values in the presence of asymptotic independence. We present a geometric interpretation of this coefficient with the additional assumptions that the random samples from a given distribution can be scaled to converge onto a limit set and that the marginal distributions have Weibull-type tails. This result leads to simple and intuitive computations of the residual dependence coefficient for a variety of distributions. © 2013 Elsevier Inc.},
   author = {Natalia Nolde},
   doi = {10.1016/j.jmva.2013.08.018},
   issn = {0047259X},
   journal = {Journal of Multivariate Analysis},
   keywords = {Asymptotic independence,Geometric approach,Limit set,Multivariate density,Residual dependence coefficient,Sample clouds},
   month = {1},
   pages = {85-95},
   title = {Geometric interpretation of the residual dependence coefficient},
   volume = {123},
   year = {2014},
}
@article{Ross2020,
   abstract = {Environmental contours are used in structural reliability analysis of marine and coastal structures as an approximate means to locate the boundary of the distribution of environmental variables, and hence sets of environmental conditions giving rise to extreme structural loads and responses. Outline guidance concerning the application of environmental contour methods is given in recent design guidelines from many organisations. However there is lack of clarity concerning (a) the differences between approaches to environmental contour estimation reported in the literature, and (b) the relationship between the environmental contour, corresponding to some return period, and the extreme structural response for the same period. Hence there is uncertainty about precisely when environmental contours should be used, and how they should be used well. This article seeks to provide some assistance in understanding the fundamental issues regarding environmental contours and their use in structural reliability analysis. Approaches to estimating the joint distribution of environmental variables, and to estimating environmental contours based on that distribution, are described. Simple freely-available software for estimation of the joint distribution, and hence environmental contours, is illustrated. Extra assumptions required to relate the characteristics of environmental contours to structural failure are outlined. Alternative response-based methods not requiring environmental contours are summarised. The results of an informal survey of the metocean user community regarding environmental contours are presented. Finally, recommendations about when and how environmental contour methods should be used are made.},
   author = {Emma Ross and Ole Christian Astrup and Elzbieta Bitner-Gregersen and Nigel Bunn and Graham Feld and Ben Gouldby and Arne Huseby and Ye Liu and David Randell and Erik Vanem and Philip Jonathan},
   doi = {10.1016/J.OCEANENG.2019.106194},
   issn = {0029-8018},
   journal = {Ocean Engineering},
   keywords = {Environmental contour,Extreme,IFORM,Joint probability,Return value,Structural reliability,Structural response},
   month = {1},
   pages = {106194},
   publisher = {Pergamon},
   title = {On environmental contours for marine and coastal design},
   volume = {195},
   year = {2020},
}
@misc{,
   author = {Aubrey C Eckert-Gallup and Cédric J Sallaberry and Ann R Dallman and Vincent S Neary},
   title = {SANDIA REPORT Modified Inverse First Order Reliability Method (I-FORM) for Predicting Extreme Sea States},
   url = {http://www.ntis.gov/help/ordermethods.asp?loc=7-4-0#online},
}
@article{Hall2000,
   abstract = {Two new methods are suggested for estimating the dependence function of a bivariate extreme-value distribution. One is based on a multiplicative modification of an earlier technique proposed by Pickands, and the other employs spline smoothing under constraints. Both produce estimators that satisfy all the conditions that define a dependence function, including convexity and the restriction that its curve lie within a certain triangular region. The first approach does not require selection of smoothing parameters; the second does, and for that purpose we suggest explicit tuning methods, one of them based on cross-validation. © 2000 ISI/BS.},
   author = {Peter Hall and Nader Tajvidi},
   doi = {10.2307/3318758},
   issn = {13507265},
   issue = {5},
   journal = {Bernoulli},
   keywords = {Convex hull,Cross-validation,Marginal distribution,Multivariate extreme-value distribution,Nonparametric curve estimation,Smoothing parameter,Spline},
   pages = {835-844},
   publisher = {International Statistical Institute},
   title = {Distribution and dependence-function estimation for bivariate extreme-value distributions},
   volume = {6},
   year = {2000},
}
@article{Cormier2014,
   abstract = {A visual tool is proposed for detecting the presence of extreme-value dependence or extremal tail behavior in bivariate data. The points appearing on the plot stem from rank-based transformations of the observations and can serve to estimate the unknown Pickands dependence function of the underlying extreme-value copula or its attractor. Quadratic constrained B-spline smoothing is used to derive an intrinsic estimator, which naturally leads to a test of extremeness. Both the estimator and the test are seen to perform well in simulations. The proposed methodology is illustrated with real data and the treatment of ties is briefly discussed.},
   author = {Eric Cormier and Christian Genest and Johanna G. Nešlehová},
   doi = {10.1007/S10687-014-0199-4},
   issn = {1572-915X},
   issue = {4},
   journal = {Extremes },
   keywords = {Civil Engineering,Economics,Environmental Management,Finance,Hydrogeology,Insurance,Management,Quality Control,Reliability,Safety and Risk,Statistics,Statistics for Business,general},
   month = {8},
   pages = {633-659},
   publisher = {Springer},
   title = {Using B-splines for nonparametric inference on bivariate extreme-value copulas},
   volume = {17},
   url = {https://link.springer.com/article/10.1007/s10687-014-0199-4},
   year = {2014},
}
@article{Kiriliouk2022,
   abstract = {An important problem in extreme-value theory is the estimation of the probability that a high-dimensional random vector falls into a given extreme failure set. This paper provides a parametric approach to this problem, based on a generalization of the tail pairwise dependence matrix (TPDM). The TPDM gives a partial summary of tail dependence for all pairs of components of the random vector. We propose an algorithm to obtain an approximate completely positive decomposition of the TPDM. The decomposition is easy to compute and applicable to moderate to high dimensions. Based on the decomposition, we obtain parameters estimates of a max-linear model whose TPDM is equal to that of the original random vector. We apply the proposed decomposition algorithm to industry portfolio returns and maximal wind speeds to illustrate its applicability.},
   author = {Anna Kiriliouk and Chen Zhou},
   month = {10},
   title = {Estimating probabilities of multivariate failure sets based on pairwise tail dependence coefficients},
   url = {http://arxiv.org/abs/2210.12618},
   year = {2022},
}
@article{Fasiolo2021,
   abstract = {We propose a novel framework for fitting additive quantile regression models, which provides well-calibrated inference about the conditional quantiles and fast automatic estimation of the smoothing parameters, for model structures as diverse as those usable with distributional generalized additive models, while maintaining equivalent numerical efficiency and stability. The proposed methods are at once statistically rigorous and computationally efficient, because they are based on the general belief updating framework of Bissiri, Holmes, and Walker to loss based inference, but compute by adapting the stable fitting methods of Wood, Pya, and Säfken. We show how the pinball loss is statistically suboptimal relative to a novel smooth generalization, which also gives access to fast estimation methods. Further, we provide a novel calibration method for efficiently selecting the “learning rate” balancing the loss with the smoothing priors during inference, thereby obtaining reliable quantile uncertainty estimates. Our work was motivated by a probabilistic electricity load forecasting application, used here to demonstrate the proposed approach. The methods described here are implemented by the qgam R package, available on the Comprehensive R Archive Network (CRAN). Supplementary materials for this article are available online.},
   author = {Matteo Fasiolo and Simon N. Wood and Margaux Zaffran and Raphaël Nedellec and Yannig Goude},
   doi = {10.1080/01621459.2020.1725521/SUPPL_FILE/UASA_A_1725521_SM0399.ZIP},
   issn = {1537274X},
   issue = {535},
   journal = {Journal of the American Statistical Association},
   keywords = {Calibrated Bayes,Electricity load forecasting,Generalized additive models,Penalized regression splines,Quantile regression},
   pages = {1402-1412},
   publisher = {American Statistical Association},
   title = {Fast Calibrated Additive Quantile Regression},
   volume = {116},
   url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.2020.1725521},
   year = {2021},
}
@article{Hentschel2022,
   abstract = {The severity of multivariate extreme events is driven by the dependence between the largest marginal observations. The H\"usler-Reiss distribution is a versatile model for this extremal dependence, and it is usually parameterized by a variogram matrix. In order to represent conditional independence relations and obtain sparse parameterizations, we introduce the novel H\"usler-Reiss precision matrix. Similarly to the Gaussian case, this matrix appears naturally in density representations of the H\"usler-Reiss Pareto distribution and encodes the extremal graphical structure through its zero pattern. For a given, arbitrary graph we prove the existence and uniqueness of the completion of a partially specified H\"usler-Reiss variogram matrix so that its precision matrix has zeros on non-edges in the graph. Using suitable estimators for the parameters on the edges, our theory provides the first consistent estimator of graph structured H\"usler-Reiss distributions. If the graph is unknown, our method can be combined with recent structure learning algorithms to jointly infer the graph and the corresponding parameter matrix. Based on our methodology, we propose new tools for statistical inference of sparse H\"usler-Reiss models and illustrate them on large flight delay data in the U.S.},
   author = {Manuel Hentschel and Sebastian Engelke and Johan Segers},
   month = {10},
   title = {Statistical Inference for H\"usler-Reiss Graphical Models Through Matrix Completions},
   url = {http://arxiv.org/abs/2210.14292},
   year = {2022},
}
@article{Ahmad2022,
   abstract = {The statistical modelling of integer-valued extremes such as large avalanche counts has received less attention than their continuous counterparts in the extreme value theory (EVT) literature. One approach to moving from continuous to discrete extremes is to model threshold exceedances of integer random variables by the discrete version of the generalized Pareto distribution. Still, the optimal threshold selection that defines exceedances remains a problematic issue. Moreover, within a regression framework, the treatment of the many data points (those below the chosen threshold) is either ignored or decoupled from extremes. Considering these issues, we extend the idea of using a smooth transition between the two tails (lower and upper) to force large and small discrete extreme values to comply with EVT. In the case of zero inflation, we also develop models with an additional parameter. To incorporate covariates, we extend the Generalized Additive Models (GAM) framework to discrete extreme responses. In the GAM forms, the parameters of our proposed models are quantified as a function of covariates. The maximum likelihood estimation procedure is implemented for estimation purposes. With the advantage of bypassing the threshold selection step, our findings indicate that the proposed models are more flexible and robust than competing models (i.e. discrete generalized Pareto distribution and Poisson distribution).},
   author = {Touqeer Ahmad and Carlo Gaetan and Philippe Naveau},
   month = {10},
   title = {Modelling of discrete extremes through extended versions of discrete generalized Pareto distribution},
   url = {http://arxiv.org/abs/2210.15253},
   year = {2022},
}
@article{Johnston2022,
   abstract = {In this study, we examine a Bayesian approach to analyze extreme daily rainfall amounts and forecast return-levels. Estimating the probability of occurrence and quantiles of future extreme events is important in many applications, including civil engineering and the design of public infrastructure. In contrast to traditional analysis, which use point estimates to accomplish this goal, the Bayesian method utilizes the complete posterior density derived from the observations. The Bayesian approach offers the benefit of well defined credible (confidence) intervals, improved forecasting, and the ability to defend rigorous probabilistic assessments. We illustrate the Bayesian approach using extreme precipitation data from Long Island, NY, USA and show that current return levels, or precipitation risk, may be understated.},
   author = {Douglas E. Johnston},
   journal = {arXiv},
   keywords = {extreme value theory,quantile forecasting},
   pages = {1-8},
   title = {Bayesian Analysis of Extreme Precipitation Events and Forecasting Return Levels},
   url = {http://arxiv.org/abs/2208.12316},
   year = {2022},
}
@article{Mackay2022,
   author = {Ed Mackay and Guillaume de Hauteclocque},
   journal = {arXiv},
   title = {Model-free environmental contours in higher dimensions},
   year = {2022},
}
@article{,
   abstract = {We propose kernel PCA as a method for analyzing the dependence structure of multivariate extremes and demonstrate that it can be a powerful tool for clustering and dimension reduction. Our work provides some theoretical insight into the preimages obtained by kernel PCA, demonstrating that under certain conditions they can effectively identify clusters in the data. We build on these new insights to characterize rigorously the performance of kernel PCA based on an extremal sample, i.e., the angular part of random vectors for which the radius exceeds a large threshold. More specifically, we focus on the asymptotic dependence of multivariate extremes characterized by the angular or spectral measure in extreme value theory and provide a careful analysis in the case where the extremes are generated from a linear factor model. We give theoretical guarantees on the performance of kernel PCA preimages of such extremes by leveraging their asymptotic distribution together with Davis-Kahan perturbation bounds. Our theoretical findings are complemented with numerical experiments illustrating the finite sample performance of our methods.},
   author = {Marco Avella-Medina and Richard A. Davis and Gennady Samorodnitsky},
   month = {11},
   title = {Kernel PCA for multivariate extremes},
   url = {http://arxiv.org/abs/2211.13172},
   year = {2022},
}
@article{Gomes2015,
   abstract = {Statistical issues arising in modelling univariate extremes of a random sample have been successfully used in the most diverse fields, such as biometrics, finance, insurance and risk theory. Statistics of univariate extremes (SUE), the subject to be dealt with in this review paper, has recently faced a huge development, partially because rare events can have catastrophic consequences for human activities, through their impact on the natural and constructed environments. In the last decades, there has been a shift from the area of parametric SUE, based on probabilistic asymptotic results in extreme value theory, towards semi-parametric approaches. After a brief reference to Gumbel's block methodology and more recent improvements in the parametric framework, we present an overview of the developments on the estimation of parameters of extreme events and on the testing of extreme value conditions under a semi-parametric framework. We further discuss a few challenging topics in the area of SUE.},
   author = {M. Ivette Gomes and Armelle Guillou},
   doi = {10.1111/INSR.12058},
   issn = {17515823},
   issue = {2},
   journal = {International Statistical Review},
   keywords = {Extreme value index and tail parameters,Parametric and semi-parametric approaches,Statistics of univariate extremes,Testing issues},
   month = {8},
   pages = {263-292},
   publisher = {International Statistical Institute},
   title = {Extreme Value Theory and Statistics of Univariate Extremes: A Review},
   volume = {83},
   year = {2015},
}
@article{Nolde2022,
   abstract = {The study of multivariate extremes is dominated by multivariate regular variation, although it is well known that this approach does not provide adequate distinction between random vectors whose components are not always simultaneously large. Various alternative dependence measures and representations have been proposed, with the most well-known being hidden regular variation and the conditional extreme value model. These varying depictions of extremal dependence arise through consideration of different parts of the multivariate domain, and particularly through exploring what happens when extremes of one variable may grow at different rates from other variables. Thus far, these alternative representations have come from distinct sources, and links between them are limited. In this work we elucidate many of the relevant connections through a geometrical approach. In particular, the shape of the limit set of scaled sample clouds in light-tailed margins is shown to provide a description of several different extremal dependence representations.},
   author = {Natalia Nolde and Jennifer L. Wadsworth},
   doi = {10.1017/APR.2021.51},
   issn = {0001-8678},
   issue = {3},
   journal = {Advances in Applied Probability},
   keywords = {Multivariate extreme value theory,asymptotic (in)dependence,conditional extremes,hidden regular variation,limit set},
   month = {9},
   pages = {688-717},
   publisher = {Cambridge University Press},
   title = {Linking representations for multivariate extremes via a limit set},
   volume = {54},
   url = {https://www.cambridge.org/core/journals/advances-in-applied-probability/article/linking-representations-for-multivariate-extremes-via-a-limit-set/9A77F8E10602DC13EA26E429CFB5FD21},
   year = {2022},
}
@book{Gentle2002,
   author = {James E. Gentle},
   city = {New York},
   doi = {10.1007/b97337},
   isbn = {0-387-95489-9},
   publisher = {Springer-Verlag},
   title = {Elements of Computational Statistics},
   year = {2002},
}
@article{Marron1992,
   author = {J. S. Marron and M. P. Wand},
   doi = {10.1214/aos/1176348653},
   issn = {0090-5364},
   issue = {2},
   journal = {The Annals of Statistics},
   month = {6},
   title = {Exact Mean Integrated Squared Error},
   volume = {20},
   year = {1992},
}
@article{Engelke2022,
   abstract = {Conditional independence and graphical models are well studied for probability distributions on product spaces. We propose a new notion of conditional independence for any measure $\Lambda$ on the punctured Euclidean space $\mathbb R^d\setminus \\{0\\}$ that explodes at the origin. The importance of such measures stems from their connection to infinitely divisible and max-infinitely divisible distributions, where they appear as L\'evy measures and exponent measures, respectively. We characterize independence and conditional independence for $\Lambda$ in various ways through kernels and factorization of a modified density, including a Hammersley-Clifford type theorem for undirected graphical models. As opposed to the classical conditional independence, our notion is intimately connected to the support of the measure $\Lambda$. Our general theory unifies and extends recent approaches to graphical modeling in the fields of extreme value analysis and L\'evy processes. Our results for the corresponding undirected and directed graphical models lay the foundation for new statistical methodology in these areas.},
   author = {Sebastian Engelke and Jevgenijs Ivanovs and Kirstin Strokorb},
   month = {11},
   title = {Graphical models for infinite measures with applications to extremes and L\'evy processes},
   url = {http://arxiv.org/abs/2211.15769},
   year = {2022},
}
@article{Perperoglou2019,
   abstract = {Background: With progress on both the theoretical and the computational fronts the use of spline modelling has become an established tool in statistical regression analysis. An important issue in spline modelling is the availability of user friendly, well documented software packages. Following the idea of the STRengthening Analytical Thinking for Observational Studies initiative to provide users with guidance documents on the application of statistical methods in observational research, the aim of this article is to provide an overview of the most widely used spline-based techniques and their implementation in R. Methods: In this work, we focus on the R Language for Statistical Computing which has become a hugely popular statistics software. We identified a set of packages that include functions for spline modelling within a regression framework. Using simulated and real data we provide an introduction to spline modelling and an overview of the most popular spline functions. Results: We present a series of simple scenarios of univariate data, where different basis functions are used to identify the correct functional form of an independent variable. Even in simple data, using routines from different packages would lead to different results. Conclusions: This work illustrate challenges that an analyst faces when working with data. Most differences can be attributed to the choice of hyper-parameters rather than the basis used. In fact an experienced user will know how to obtain a reasonable outcome, regardless of the type of spline used. However, many analysts do not have sufficient knowledge to use these powerful tools adequately and will need more guidance.},
   author = {Aris Perperoglou and Willi Sauerbrei and Michal Abrahamowicz and Matthias Schmid},
   doi = {10.1186/s12874-019-0666-3},
   issn = {1471-2288},
   issue = {1},
   journal = {BMC Medical Research Methodology},
   keywords = {Functional form of continuous covariates,Multivariable modelling},
   month = {12},
   pages = {1-16},
   pmid = {30841848},
   publisher = {BioMed Central Ltd.},
   title = {A review of spline function procedures in R},
   volume = {19},
   url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-019-0666-3},
   year = {2019},
}
@article{,
   abstract = {We investigate conditions for the existence of the limiting conditional distribution of a bivariate random vector when one component becomes large. We revisit the existing literature on the topic, and present some new sufficient conditions. We concentrate on the case where the conditioning variable belongs to the maximum domain of attraction of the Gumbel law, and we study geometric conditions on the joint distribution of the vector. We show that these conditions are of a local nature and imply asymptotic independence when both variables belong to the domain of attraction of an extreme value distribution. The new model we introduce can also be useful to simulate bivariate random vectors with a given limiting conditional distribution. Copyright © Taylor & Francis Group, LLC.},
   author = {Anne Laure Fougères and Philippe Soulier},
   doi = {10.1080/15326340903291362},
   issn = {15324214},
   issue = {1},
   journal = {Stochastic Models},
   keywords = {Asymptotic independence,Conditional excess probability,Conditional extreme-value model,Elliptic distributions,Second-order correction,τvarying tail},
   pages = {54-77},
   publisher = {Taylor and Francis Inc.},
   title = {Limit conditional distributions for bivariate vectors with polar representation},
   volume = {26},
   year = {2010},
}
@article{Richards2022,
   abstract = {Extreme wildfires continue to be a significant cause of human death and biodiversity destruction within countries that encompass the Mediterranean Basin. Recent worrying trends in wildfire activity (i.e., occurrence and spread) suggest that wildfires are likely to be highly impacted by climate change. In order to facilitate appropriate risk mitigation, it is imperative to identify the main drivers of extreme wildfires and assess their spatio-temporal trends, with a view to understanding the impacts of global warming on fire activity. To this end, we analyse the monthly burnt area due to wildfires over a region encompassing most of Europe and the Mediterranean Basin from 2001 to 2020, and identify high fire activity during this period in eastern Europe, Algeria, Italy and Portugal. We build an extreme quantile regression model with a high-dimensional predictor set describing meteorological conditions, land cover usage, and orography, for the domain. To model the complex relationships between the predictor variables and wildfires, we make use of a hybrid statistical deep-learning framework that allows us to disentangle the effects of vapour-pressure deficit (VPD), air temperature, and drought on wildfire activity. Our results highlight that whilst VPD, air temperature, and drought significantly affect wildfire occurrence, only VPD affects extreme wildfire spread. Furthermore, to gain insights into the effect of climate change on wildfire activity in the near future, we perturb VPD and temperature according to their observed trends and find evidence that global warming may lead to spatially non-uniform changes in wildfire activity.},
   author = {Jordan Richards and Raphaël Huser and Emanuele Bevacqua and Jakob Zscheischler},
   month = {12},
   title = {Insights into the drivers and spatio-temporal trends of extreme Mediterranean wildfires with statistical deep-learning},
   url = {http://arxiv.org/abs/2212.01796},
   year = {2022},
}
@article{Vandeskog2022,
   abstract = {A successful model for high-dimensional spatial extremes should, in principle, be able to describe both weakening extremal dependence at increasing levels and changes in the type of extremal dependence class as a function of the distance between locations. Furthermore, the model should allow for computationally tractable inference using inference methods that efficiently extract information from data and that are robust to model misspecification. In this paper, we demonstrate how to fulfil all these requirements by developing a comprehensive methodological workflow for efficient Bayesian modelling of high-dimensional spatial extremes using the spatial conditional extremes model while performing fast inference with R-INLA. We then propose a post hoc adjustment method that results in more robust inference by properly accounting for possible model misspecification. The developed methodology is applied for modelling extreme hourly precipitation from high-resolution radar data in Norway. Inference is computationally efficient, and the resulting model fit successfully captures the main trends in the extremal dependence structure of the data. Robustifying the model fit by adjusting for possible misspecification further improves model performance.},
   author = {Silius M. Vandeskog and Sara Martino and Raphaël Huser},
   month = {10},
   title = {An Efficient Workflow for Modelling High-Dimensional Spatial Extremes},
   url = {http://arxiv.org/abs/2210.00760},
   year = {2022},
}
@article{Majumder2022,
   abstract = {Extreme streamflow is a key indicator of flood risk, and quantifying the changes in its distribution under non-stationary climate conditions is key to mitigating the impact of flooding events. We propose a non-stationary process mixture model (NPMM) for annual streamflow maxima over the central US (CUS) which uses downscaled climate model precipitation projections to forecast extremal streamflow. Spatial dependence for the model is specified as a convex combination of transformed Gaussian and max-stable processes, indexed by a weight parameter which identifies the asymptotic regime of the process. The weight parameter is modeled as a function of region and of regional precipitation, introducing spatio-temporal non-stationarity within the model. The NPMM is flexible with desirable tail dependence properties, but yields an intractable likelihood. To address this, we embed a neural network within a density regression model which is used to learn a synthetic likelihood function using simulations from the NPMM with different parameter settings. Our model is fitted using observational data for 1972-2021, and inference carried out in a Bayesian framework. Annual streamflow maxima forecasts for 2021-2035 estimate an increase in the frequency and magnitude of extreme streamflow, with changes being more pronounced in the largest quantiles of the projected annual streamflow maxima.},
   author = {Reetam Majumder and Brian Reich},
   month = {12},
   title = {A Deep Learning Synthetic Likelihood Approximation of a Non-stationary Spatial Model for Extreme Streamflow Forecasting},
   url = {http://arxiv.org/abs/2212.07267},
   year = {2022},
}
@article{Zou2022,
   abstract = {The stable tail dependence function provides a full characterization of the extremal dependence structures. Unfortunately, the estimation of the stable tail dependence function often suffers from significant bias, whose scale relates to the Peaks-Over-Threshold (POT) second-order parameter. For this second-order parameter, this paper introduces a penalized estimator that discourages it from being too close to zero. This paper then establishes this estimator's asymptotic consistency, uses it to correct the bias in the estimation of the stable tail dependence function, and illustrates its desirable empirical properties in the estimation of the extremal dependence structures.},
   author = {Nan Zou},
   month = {12},
   title = {Estimating POT Second-order Parameter for Bias Correction},
   url = {http://arxiv.org/abs/2212.08331},
   year = {2022},
}
@article{Majumder2022,
   abstract = {Quantifying changes in the probability and magnitude of extreme flooding events is key to mitigating their impacts. While hydrodynamic data are inherently spatially dependent, traditional spatial models such as Gaussian processes are poorly suited for modeling extreme events. Spatial extreme value models with more realistic tail dependence characteristics are under active development. They are theoretically justified, but give intractable likelihoods, making computation challenging for small datasets and prohibitive for continental-scale studies. We propose a process mixture model which specifies spatial dependence in extreme values as a convex combination of a Gaussian process and a max-stable process, yielding desirable tail dependence properties but intractable likelihoods. To address this, we employ a unique computational strategy where a feed-forward neural network is embedded in a density regression model to approximate the conditional distribution at one spatial location given a set of neighbors. We then use this univariate density function to approximate the joint likelihood for all locations by way of a Vecchia approximation. The process mixture model is used to analyze changes in annual maximum streamflow within the US over the last 50 years, and is able to detect areas which show increases in extreme streamflow over time.},
   author = {Reetam Majumder and Brian J. Reich and Benjamin A. Shaby},
   month = {8},
   title = {Modeling Extremal Streamflow using Deep Learning Approximations and a Flexible Spatial Process},
   url = {http://arxiv.org/abs/2208.03344},
   year = {2022},
}
@article{,
   abstract = {Many multivariate data sets exhibit a form of positive dependence, which can either appear globally between all variables or only locally within particular subgroups. In models in multivariate extremes arising from threshold exceedances, a natural notion of positive dependence is the recently introduced extremal multivariate total positivity of order 2 ($ \text\{EMTP\}_2 $). While $ \text\{EMTP\}_2 $ has nice theoretical properties, it is by construction a global property and therefore not suitable for applications with only local positive dependence. We introduce extremal association as a weaker form of extremal positive dependence and show that it generalizes extremal tree models. This follows from a sufficient condition for extremal association, which for H\"usler--Reiss distributions permits a parametric description that we call the metric property. As the parameter of a H\"usler--Reiss distribution is a Euclidean distance matrix, the metric property relates to research in electric network theory and Euclidean geometry. We show that the metric property can be localized with respect to a graph and study surrogate likelihood inference. This gives rise to a two-step estimation procedure for locally metrical H\"usler--Reiss graphical models. The second step allows for a simple dual problem, which is implemented via a gradient descent algorithm. Finally, we demonstrate our results on simulated and real data.},
   author = {Frank Röttger and Quentin Schmitz},
   month = {12},
   title = {On the local metric property in multivariate extremes},
   url = {http://arxiv.org/abs/2212.10350},
   year = {2022},
}
@article{Bodik2022,
   abstract = {Determining the causes of extreme events is a fundamental question in many scientific fields. An important aspect when modelling multivariate extremes is the tail dependence. In application, the extreme dependence structure may significantly depend on covariates. As for the general case of modelling including covariates, only some of the covariates are causal. In this paper, we propose a methodology to discover the causal covariates explaining the tail dependence structure between two variables. The proposed methodology for discovering causal variables is based on comparing observations from different environments or perturbations. It is a desired methodology for predicting extremal behaviour in a new, unobserved environment. The methodology is applied to a dataset of $\text\{NO\}_2$ concentration in the UK. Extreme $\text\{NO\}_2$ levels can cause severe health problems, and understanding the behaviour of concurrent severe levels is an important question. We focus on revealing causal predictors for the dependence between extreme $\text\{NO\}_2$ observations at different sites.},
   author = {Juraj Bodik and Linda Mhalla and Valérie Chavez-Demoulin},
   journal = {arXiv},
   month = {12},
   title = {Detecting causal covariates for extreme dependence structures},
   volume = {2212.09831},
   url = {http://arxiv.org/abs/2212.09831},
   year = {2022},
}
@article{Quintos2001,
   author = {Carmela Quintos and Zhenhong Fan and Peter C. B. Philips},
   doi = {10.1111/1467-937X.00184},
   issn = {0034-6527},
   issue = {3},
   journal = {Review of Economic Studies},
   month = {7},
   pages = {633-663},
   title = {Structural Change Tests in Tail Behaviour and the Asian Crisis},
   volume = {68},
   url = {https://academic.oup.com/restud/article-lookup/doi/10.1111/1467-937X.00184},
   year = {2001},
}
@article{Kesemen2020,
   abstract = {In this study, some bivariate distribution functions are defined in the polar coordinate system and random numbers are generated from these distribution functions. In these definitions, the angular change of the probability density function is taken as constant, and the distance change is performed based on the univariate probability density function. Also, the chi-square goodness-of-fit test is proposed for random numbers generated in the polar coordinates. Four different distribution functions are selected to evaluate the success of the proposed chi-square goodness of fit test for polar distribution functions. Lastly, the validity and the success of the proposed method is shown in the simulation study and the real-life example.},
   author = {Orhan Kesemen and Bugra Kaan Tiryaki and Ozge Tezel and Ayse Pak},
   doi = {10.35378/gujs.610086},
   issn = {21471762},
   issue = {3},
   journal = {Gazi University Journal of Science},
   keywords = {Goodness of fit test,Polar distribution,Polar histogram,Random number},
   month = {9},
   pages = {846-882},
   publisher = {Gazi Universitesi},
   title = {Some statistics in polar coordinate system with uniform angles},
   volume = {33},
   year = {2020},
}
@article{Solman2014,
   abstract = {Search outside the laboratory involves tradeoffs among a variety of internal and external exploratory processes. Here we examine the conditions under which item specific memory from prior exposures to a search array is used to guide attention during search. We extend the hypothesis that memory use increases as perceptual search becomes more difficult by turning to an ecologically important type of search difficulty - energetic cost. Using optical motion tracking, we introduce a novel head-contingent display system, which enables the direct comparison of search using head movements and search using eye movements. Consistent with the increased energetic cost of turning the head to orient attention, we discover greater use of memory in head-contingent versus eye-contingent search, as reflected in both timing and orienting metrics. Our results extend theories of memory use in search to encompass embodied factors, and highlight the importance of accounting for the costs and constraints of the specific motor groups used in a given task when evaluating cognitive effects. © 2014 Elsevier B.V.},
   author = {Grayden J.F. Solman and Alan Kingstone},
   doi = {10.1016/J.COGNITION.2014.05.005},
   issn = {0010-0277},
   issue = {3},
   journal = {Cognition},
   keywords = {Embodied cognition,Memory,Search},
   month = {9},
   pages = {443-454},
   pmid = {24946208},
   publisher = {Elsevier},
   title = {Balancing energetic and cognitive resources: Memory use during search depends on the orienting effector},
   volume = {132},
   year = {2014},
}
@article{Forster2022,
   abstract = {In recent years, parametric models for max-stable processes have become a popular choice for modeling spatial extremes because they arise as the asymptotic limit of rescaled maxima of independent and identically distributed random processes. Apart from few exceptions for the class of extremal-t processes, existing literature mainly focuses on models with stationary dependence structures. In this paper, we propose a novel non-stationary approach that can be used for both Brown-Resnick and extremal-t processes - two of the most popular classes of max-stable processes - by including covariates in the corresponding variogram and correlation functions, respectively. We apply our new approach to extreme precipitation data in two regions in Southern and Northern Germany and compare the results to existing stationary models in terms of Takeuchi's information criterion (TIC). Our results indicate that, for this case study, non-stationary models are more appropriate than stationary ones for the region in Southern Germany. In addition, we investigate theoretical properties of max-stable processes conditional on random covariates. We show that these can result in both asymptotically dependent and asymptotically independent processes. Thus, conditional models are more flexible than classical max-stable models.},
   author = {Carolin Forster and Marco Oesting},
   month = {12},
   title = {Non-stationary max-stable models with an application to heavy rainfall data},
   url = {http://arxiv.org/abs/2212.11598},
   year = {2022},
}
@article{,
   abstract = {This paper devises a regression-type model for the situation where both the response and covariates are extreme. The proposed approach is designed for the setting where the response and covariates are modeled as multivariate extreme values, and thus contrarily to standard regression methods it takes into account the key fact that the limiting distribution of suitably standardized componentwise maxima is an extreme value copula. An important target in the proposed framework is the regression manifold, which consists of a family of regression lines obeying the latter asymptotic result. To learn about the proposed model from data, we employ a Bernstein polynomial prior on the space of angular densities which leads to an induced prior on the space of regression manifolds. Numerical studies suggest a good performance of the proposed methods, and a finance real-data illustration reveals interesting aspects on the conditional risk of extreme losses in two leading international stock markets.},
   author = {Miguel De Carvalho and Alina Kumukova and · Gonçalo and Dos Reis},
   journal = {arXiv},
   keywords = {Angular measure,Bernstein polynomials,Extreme value copula,Joint extremes,Multivariate extreme value distribution,Quantile regression,Statistics of extremes},
   title = {Regression-type analysis for multivariate extreme values},
}
@article{Bai2023,
   abstract = {Conventional methods for extreme event estimation rely on well-chosen parametric models asymptotically justified from extreme value theory (EVT). These methods, while powerful and theoretically grounded, could however encounter a difficult bias-variance tradeoff that exacerbates especially when data size is too small, deteriorating the reliability of the tail estimation. In this paper, we study a framework based on the recently surging literature of distributionally robust optimization. This approach can be viewed as a nonparametric alternative to conventional EVT, by imposing general shape belief on the tail instead of parametric assumption and using worst-case optimization as a resolution to handle the nonparametric uncertainty. We explain how this approach bypasses the bias-variance tradeoff in EVT. On the other hand, we face a conservativeness-variance tradeoff which we describe how to tackle. We also demonstrate computational tools for the involved optimization problems and compare our performance with conventional EVT across a range of numerical examples.},
   author = {Yuanlu Bai and Henry Lam and Xinyu Zhang},
   month = {1},
   title = {A Distributionally Robust Optimization Framework for Extreme Event Estimation},
   url = {http://arxiv.org/abs/2301.01360},
   year = {2023},
}
@article{Hüsler1989,
   abstract = {A new approach to the asymptotic treatment of multivariate sample maxima is suggested and exemplified in the particular case of maxima of normal random vectors. In the limit one obtains a class of multivariate maxstable distributions not considered in literature so far. © 1989.},
   author = {Jürg Hüsler and Rolf Dieter Reiss},
   doi = {10.1016/0167-7152(89)90106-5},
   issn = {0167-7152},
   issue = {4},
   journal = {Statistics \& Probability Letters},
   keywords = {Multivariate sample maxima,correlation coefficient,max-stability,normal random vectors},
   month = {2},
   pages = {283-286},
   publisher = {North-Holland},
   title = {Maxima of normal random vectors: Between independence and complete dependence},
   volume = {7},
   year = {1989},
}
@article{Gumbel1960,
   abstract = {A bivariate distribution is not determined by the knowledge of the margins. Two bivariate distributions with exponential margins are analyzed and another is briefly mentioned. In the first distribution (2.1) the conditional expectation of one variable decreases to zero with increasing values of the other one. The coefficient of correlation is never positive and lies in the interval –.40≤ρ≤0, and the correlation ratio varies from –.48 to zero. In the second distribution (3.4) the conditional expectation of one variable increases or decreases with increasing values of the other variable depending on the sign of the correlation. The coefficient of correlation lies in the interval –.25≤ρ≤.25, and the correlation ratio is proportional to the coefficient. © Taylor & Francis Group, LLC.},
   author = {E. J. Gumbel},
   doi = {10.1080/01621459.1960.10483368},
   issn = {1537274X},
   issue = {292},
   journal = {Journal of the American Statistical Association},
   pages = {698-707},
   title = {Bivariate Exponential Distributions},
   volume = {55},
   year = {1960},
}
@article{Meyer2020,
   abstract = {Identifying directions where extreme events occur is a major challenge in multivariate extreme value analysis. In this paper, we use the concept of sparse regular variation introduced by Meyer and Wintenberger (2021) to infer the tail dependence of a random vector X. This approach relies on the Euclidean projection onto the simplex which better exhibits the sparsity structure of the tail of X than the standard methods. Our procedure based on a rigorous methodology aims at capturing clusters of extremal coordinates of X. It also includes the identification of the threshold above which the values taken by X are considered as extreme. We provide an efficient and scalable algorithm called MUSCLE and apply it on numerical examples to highlight the relevance of our findings. Finally we illustrate our approach with financial return data.},
   author = {Nicolas Meyer and Olivier Wintenberger},
   journal = {arXiv},
   month = {7},
   title = {Multivariate sparse clustering for extremes},
   url = {http://arxiv.org/abs/2007.11848},
   year = {2020},
}
@article{Draisma2004,
   abstract = {In the classical setting of bivariate extreme value theory, the procedures for estimating the probability of an extreme event are not applicable if the componentwise maxima of the observations are asymptotically independent. To cope with this problem, Ledford and Tawn proposed a submodel in which the penultimate dependence is characterized by an additional parameter. We discuss the asymptotic properties of two estimators for this parameter in an extended model. Moreover, we develop an estimator for the probability of an extreme event that works in the case of asymptotic independence as well as in the case of asymptotic dependence, and prove its consistency.},
   author = {Gerrit Draisma and Holger Drees and Ana Ferreira and Laurens De Haan},
   doi = {10.3150/bj/1082380219},
   issn = {1350-7265},
   issue = {2},
   journal = {Bernoulli},
   keywords = {Hill estimator,asymptotic normality,bivariate extreme value distribution,coefficient of tail dependence,copula,failure probability,moment estimator},
   month = {4},
   pages = {251-280},
   title = {Bivariate tail estimation: dependence in asymptotic independence},
   volume = {10},
   url = {https://projecteuclid.org/journals/bernoulli/volume-10/issue-2/Bivariate-tail-estimation-dependence-in-asymptotic-independence/10.3150/bj/1082380219.full},
   year = {2004},
}
@article{Smith1989,
   author = {Richard L. Smith},
   doi = {10.1214/ss/1177012400},
   issn = {0883-4237},
   issue = {4},
   journal = {Statistical Science},
   month = {11},
   pages = {367-377},
   title = {Extreme Value Analysis of Environmental Time Series: An Application to Trend Detection in Ground-Level Ozone},
   volume = {4},
   url = {https://projecteuclid.org/journals/statistical-science/volume-4/issue-4/Extreme-Value-Analysis-of-Environmental-Time-Series--An-Application/10.1214/ss/1177012400.full},
   year = {1989},
}
@article{Yee2007,
   abstract = {Over recent years parametric and nonparametric regression has slowly been adopted into extreme value data analysis. Its introduction has been characterized by piecemeal additions and embellishments, which has had a negative effect on software development and usage. The purpose of this article is to convey the classes of vector generalized linear and additive models (VGLMs and VGAMs) as offering significant advantages for extreme value data analysis, providing flexible smoothing within a unifying framework. In particular, VGLMs and VGAMs allow all parameters of extreme value distributions to be modelled as linear or smooth functions of covariates. We implement new auxiliary methodology by incorporating a quasi-Newton update for the working weight matrices within an iteratively reweighted least squares (IRLS) algorithm. A software implementation by the first author, called the vgam package for [InlineMediaObject not available: see fulltext.], is used to illustrate the potential of VGLMs and VGAMs. © 2007 Springer Science+Business Media, LLC.},
   author = {Thomas W. Yee and Alec G. Stephenson},
   doi = {10.1007/S10687-007-0032-4/METRICS},
   issn = {13861999},
   issue = {1-2},
   journal = {Extremes},
   keywords = {Extreme value modelling,Fisher scoring,Iteratively reweighted least squares,Maximum likelihood estimation,Penalized likelihood,Smoothing,Vector splines},
   month = {6},
   pages = {1-19},
   publisher = {Springer},
   title = {Vector generalized linear and additive extreme value models},
   volume = {10},
   url = {https://link.springer.com/article/10.1007/s10687-007-0032-4},
   year = {2007},
}
@article{Escobar2018,
   abstract = {We consider the robust estimation of the Pickands dependence function in the random covariate framework. Our estimator is based on local estimation with the minimum density power divergence criterion. We provide the main asymptotic properties, in particular the convergence of the stochastic process, correctly normalized, towards a tight centered Gaussian process. The finite sample performance of our estimator is evaluated with a simulation study involving both uncontaminated and contaminated samples. The method is illustrated on a dataset of air pollution measurements.},
   author = {Mikael Escobar-Bach and Yuri Goegebeur and Armelle Guillou},
   doi = {10.1214/17-AOS1640},
   issn = {0090-5364},
   issue = {6A},
   journal = {The Annals of Statistics},
   keywords = {60F05,60G70,62G05,62G20,62G32,Conditional Pickands dependence function,Stochastic convergence,robustness},
   month = {12},
   pages = {2806-2843},
   publisher = {Institute of Mathematical Statistics},
   title = {Local robust estimation of the Pickands dependence function},
   volume = {46},
   url = {https://projecteuclid.org/journals/annals-of-statistics/volume-46/issue-6A/Local-robust-estimation-of-the-Pickands-dependence-function/10.1214/17-AOS1640.full},
   year = {2018},
}
@article{Jonathan2014b,
   abstract = {Careful modelling of non-stationarity is critical to reliable specification of marine and coastal design criteria. We present a spline based methodology to incorporate spatial, directional, temporal and other covariate effects in extreme value models for environmental variables such as storm severity. For storm peak significant wave height events, the approach uses quantile regression to estimate a suitable extremal threshold, a Poisson process model for the rate of occurrence of threshold exceedances, and a generalised Pareto model for size of threshold exceedances. Multidimensional covariate effects are incorporated at each stage using penalised (tensor products of) B-splines to give smooth model parameter variation as a function of multiple covariates. Optimal smoothing penalties are selected using cross-validation, and model uncertainty is quantified using a bootstrap re-sampling procedure. The method is applied to estimate return values for large spatial neighbourhoods of locations, incorporating spatial and directional effects. Extensions to joint modelling of multivariate extremes, incorporating extremal spatial dependence (using max-stable processes) or more general extremal dependence (using the conditional extremes approach) are outlined. © 2014 Elsevier Ltd.},
   author = {Philip Jonathan and David Randell and Yanyun Wu and Kevin Ewans},
   doi = {10.1016/J.OCEANENG.2014.07.007},
   issn = {0029-8018},
   journal = {Ocean Engineering},
   keywords = {B-spline,Covariate,Extreme,Offshore design,Return value,Storm severity},
   month = {9},
   pages = {520-532},
   publisher = {Pergamon},
   title = {Return level estimation from non-stationary spatial data exhibiting multidimensional covariate effects},
   volume = {88},
   year = {2014},
}
@article{Blanchet2011,
   abstract = {The spatial modeling of extreme snow is important for adequate risk management in Alpine and high altitude countries. A natural approach to such modeling is through the theory of max-stable processes, an infinite-dimensional extension of multivariate extreme value theory. In this paper we describe the application of such processes in modeling the spatial dependence of extreme snow depth in Switzerland, based on data for the winters 1966–2008 at 101 stations. The models we propose rely on a climate transformation that allows us to account for the presence of climate regions and for directional effects, resulting from synoptic weather patterns. Estimation is performed through pairwise likelihood inference and the models are compared using penalized likelihood criteria. The max-stable models provide a much better fit to the joint behavior of the extremes than do independence or full dependence models.},
   author = {Juliette Blanchet and Anthony C. Davison},
   doi = {10.1214/11-AOAS464},
   issn = {1932-6157},
   issue = {3},
   journal = {The Annals of Applied Statistics},
   keywords = {Climate space,Extreme value theory,Max-stable process,extremal coefficient,pairwise likelihood,snow depth data},
   month = {9},
   pages = {1699-1725},
   publisher = {Institute of Mathematical Statistics},
   title = {Spatial modeling of extreme snow depth},
   volume = {5},
   url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-5/issue-3/Spatial-modeling-of-extreme-snow-depth/10.1214/11-AOAS464.full},
   year = {2011},
}
@article{Youngman2020a,
   abstract = {There are many situations when modelling environmental phenomena for which it is not appropriate to assume a stationary dependence structure. \cite\{sampson1992\} proposed an approach to allowing nonstationarity in dependence based on a deformed space: coordinates from original geographic "$G$" space are mapped to a new dispersion "$D$" space in which stationary dependence is a reasonable assumption. \cite\{sampson1992\} achieve this with two deformation functions, which are chosen as thin plate splines, each representing how one of the two coordinates in $D$-space relates to the original $G$-space coordinates. This works extends the deformation approach, and the dimension expansion approach of \cite\{bornn2012\}, to a regression-based framework in which all dimensions in $D$-space are treated as "smooths" as found, for example, in generalized additive models. The framework offers an intuitive and user-friendly approach to specifying $D$-space, allows different levels of smoothing for dimensions in $D$-space, and allows objective inference for all model parameters. Furthermore, a numerical approach is proposed to avoid non-bijective deformations, should they occur, which applies to any deformation. The proposed framework is demonstrated on the solar radiation data studied in \cite\{sampson1992\}, and then on an example related to risk analysis, which culminates in producing simulations of extreme rainfall for part of Colorado, US.},
   author = {Benjamin D. Youngman},
   journal = {arXiv},
   month = {1},
   title = {Flexible models for nonstationary dependence: Methodology and examples},
   url = {http://arxiv.org/abs/2001.06642},
   year = {2020},
}
@article{Tilloy2020,
   abstract = {Modelling multiple hazard interrelations remains a challenge for practitioners. This article primarily focuses on the interrelations between pairs of hazards. The efficacy of six distinct bivariate extreme models is evaluated through their fitting capabilities to 60 synthetic datasets. The properties of the synthetic datasets (marginal distributions, tail dependence structure) are chosen to match bivariate time series of environmental variables. The six models are copulas (one non-parametric, one semi-parametric, four parametric). We build 60 distinct synthetic datasets based on different parameters of log-normal margins and two different copulas. The systematic framework developed contrasts the model strengths (model flexibility) and weaknesses (poorer fits to the data). We find that no one model fits our synthetic data for all parameters but rather a range of models depending on the characteristics of the data. To highlight the benefits of the systematic modelling framework developed, we consider the following environmental data: (i) daily precipitation and maximum wind gusts for 1971 to 2018 in London, UK, and (ii) daily mean temperature and wildfire numbers for 1980 to 2005 in Porto District, Portugal. In both cases there is good agreement in the estimation of bivariate return periods between models selected from the systematic framework developed in this study. Within this framework, we have explored a way to model multi-hazard events and identify the most efficient models for a given set of synthetic data and hazard sets.},
   author = {Aloïs Tilloy and Bruce D. Malamud and Hugo Winter and Amélie Joly-Laugel},
   doi = {10.5194/NHESS-20-2091-2020},
   issn = {16849981},
   issue = {8},
   journal = {Natural Hazards and Earth System Sciences},
   month = {8},
   pages = {2091-2117},
   publisher = {Copernicus GmbH},
   title = {Evaluating the efficacy of bivariate extreme modelling approaches for multi-hazard scenarios},
   volume = {20},
   year = {2020},
}
@article{Berkowitz2000,
   author = {Jeremy Berkowitz and Lutz Kilian},
   doi = {10.1080/07474930008800457},
   issn = {15324168},
   issue = {1},
   journal = {Econometric Reviews},
   keywords = {ARLIA,Blocks,Bootstrap,Frequency domain},
   pages = {1-48},
   title = {Recent developments in bootstrapping time series},
   volume = {19},
   year = {2000},
}
@article{Baker2018,
   abstract = {The Paris Agreement 1 aims to 'pursue efforts to limit the temperature increase to 1.5 °C above pre-industrial levels.' However, it has been suggested that temperature targets alone are insufficient to limit the risks associated with anthropogenic emissions 2,3. Here, using an ensemble of model simulations, we show that atmospheric CO2 increase-an even more predictable consequence of emissions than global temperature increase-has a significant direct impact on Northern Hemisphere summer temperature, heat stress, and tropical precipitation extremes. Hence in an iterative climate mitigation regime aiming solely for a specific temperature goal, an unexpectedly low climate response may have corresponding 'dangerous' changes in extreme events. The direct impact of higher CO2 concentrations on climate extremes therefore substantially reduces the upper bound of the carbon budget, and highlights the need to explicitly limit atmospheric CO2 concentration when formulating allowable emissions. Thus, complementing global mean temperature goals with explicit limits on atmospheric CO2 concentrations in future climate policy would limit the adverse effects of high-impact weather extremes.},
   author = {Hugh S. Baker and Richard J. Millar and David J. Karoly and Urs Beyerle and Benoit P. Guillod and Dann Mitchell and Hideo Shiogama and Sarah Sparrow and Tim Woollings and Myles R. Allen},
   doi = {10.1038/s41558-018-0190-1},
   issn = {17586798},
   issue = {7},
   journal = {Nature Climate Change},
   month = {7},
   pages = {604-608},
   publisher = {Nature Publishing Group},
   title = {Higher CO2 concentrations increase extreme event risk in a 1.5 °c world},
   volume = {8},
   year = {2018},
}
@article{Yu2022,
   abstract = {For any given city, on any calendar day, there will be record high and low temperatures. Which record occurred earlier? If there is a trend towards warming then, intuitively, there should be a preponderance of record highs occurring more recently than the record lows for each of the 365 calendar days. We are interested in modeling the joint distribution of appearances of the extremes but not these values themselves. We develop a bivariate discrete distribution modeling the joint indices of maximum and minimum in a sequence of independent random variables sampled from different distributions. We assume these distributions share a proportional hazard rate and develop regression methods for these paired values. This approach has reasonable power to detect a small mean change over a decade. Using readily available public data, we examine the daily calendar extreme values of five US cities for the decade 2011–2020. We develop linear regression models for these data, describe models to account for calendar-date dependence, and use diagnostic measures to detect remarkable observations.},
   author = {Chang Yu and Ondrej Blaha and Michael Kane and Wei Wei and Denise Esserman and Daniel Zelterman},
   doi = {10.1002/ENV.2764},
   issn = {1099095X},
   issue = {7},
   journal = {Environmetrics},
   keywords = {bivariate discrete distribution,climate change,proportional hazards},
   month = {11},
   pages = {1-14},
   publisher = {John Wiley and Sons Ltd},
   title = {Regression methods for the appearances of extremes in climate data},
   volume = {33},
   year = {2022},
}
@article{Worms2022,
   abstract = {Within the statistical climatology literature, inferring the contributions of potential causes with regard to climate change has become a recurrent research theme during this last decade. In particular, disentangling human induced (anthropogenic) forcings from natural causes represents a nontrivial statistical task, especially when the focal point moves away from mean behaviors and goes towards extreme events with high societal impacts. Most studies found in the field of extreme event attributions (EEA) rely on extreme value theory. Under this theoretical umbrella, it is often assumed that, for a given location, temporal changes in extremes can be detected in both location and scale parameters of an extreme value distribution, while its shape parameter remains unchanged over time. This assumption of constant tail shape parameters between a so-called factual world (all forcings) and a counterfactual one (without anthropogenic forcing) can be challenged due to the fact that important forcing changes could impact large scale atmospheric and oceanic circulation patterns, and consequently, the latter can reshape the full distribution, including its shape parameter that drives extremal behavior. In this article, we study how allowing different tail shape parameters between the factual and counterfactual worlds can affect the analysis of records. In particular, we extend the work of Naveau et al. in which this case was not treated. We also add properties and theoretical inferential results about records in EEA and propose a procedure for model validation. A simulation study of our approach is detailed. Our method is applied to records of yearly maxima of daily maxima of near-surface air temperature issued from the numerical climate model CNRM-CM6-1 of Météo-France.},
   author = {Julien Worms and Philippe Naveau},
   doi = {10.1002/env.2777},
   issn = {1180-4009},
   issue = {8},
   journal = {Environmetrics},
   keywords = {Weibull distribution,causality theory,climate detection and attribution,generalized extreme value},
   month = {12},
   pages = {1-16},
   publisher = {John Wiley and Sons Ltd},
   title = {Record events attribution in climate studies},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/10.1002/env.2777},
   year = {2022},
}
@article{Shooter2021,
   abstract = {The extremal spatial dependence of significant wave height in the North East Atlantic is explored using Joint Altimetry Satellite Oceanography Network satellite altimeter observations for the period 2002–2018, and a spatial conditional extremes model motivated by the work of Heffernan and Tawn. The analysis involves (a) registering individual satellite passes onto a template transect, (b) marginal extreme value analysis at a set of locations on the template transect and transformation from physical to standard Laplace scale, (c) estimation of the spatial conditional extremes model for a set of locations on a template transect, and (d) comparison of extreme spatial dependence for different template transects. Inferences for two transects considered are qualitatively similar; however, for the “normal ascending” transect running approximately south-west to north-east lying between Iceland and the United Kingdom, extremal spatial dependence is found to decay more quickly than for the second “opposite descending” transect running approximately north-west to south-east to the west of Ireland.},
   author = {R. Shooter and E. Ross and A. Ribal and I. R. Young and P. Jonathan},
   doi = {10.1002/env.2674},
   issn = {1180-4009},
   issue = {4},
   journal = {Environmetrics},
   keywords = {Atlantic,altimeter,extreme,satellite,significant wave height,spatial dependence},
   month = {6},
   pages = {1-15},
   publisher = {John Wiley and Sons Ltd},
   title = {Spatial dependence of extreme seas in the North East Atlantic from satellite altimeter measurements},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/10.1002/env.2674},
   year = {2021},
}
@article{Baran2021,
   abstract = {In recent years, ensemble weather forecasting has become a routine at all major weather prediction centers. These forecasts are obtained from multiple runs of numerical weather prediction models with different initial conditions or model parametrizations. However, ensemble forecasts can often be underdispersive and also biased, so some kind of postprocessing is needed to account for these deficiencies. One of the most popular state of the art statistical postprocessing techniques is the ensemble model output statistics (EMOS), which provides a full predictive distribution of the studied weather quantity. We propose a novel EMOS model for calibrating wind speed ensemble forecasts, where the predictive distribution is a generalized extreme value (GEV) distribution left truncated at zero (TGEV). The truncation corrects the disadvantage of the GEV distribution-based EMOS models of occasionally predicting negative wind speed values, without affecting its favorable properties. The new model is tested on four datasets of wind speed ensemble forecasts provided by three different ensemble prediction systems, covering various geographical domains and time periods. The forecast skill of the TGEV EMOS model is compared with the predictive performance of the truncated normal, log-normal and GEV methods and the raw and climatological forecasts as well. The results verify the advantageous properties of the novel TGEV EMOS approach.},
   author = {Sándor Baran and Patrícia Szokol and Marianna Szabó},
   doi = {10.1002/env.2678},
   issn = {1180-4009},
   issue = {6},
   journal = {Environmetrics},
   keywords = {continuous ranked probability score,ensemble calibration,ensemble model output statistics,truncated generalized extreme value distribution},
   month = {9},
   pages = {1-24},
   publisher = {John Wiley and Sons Ltd},
   title = {Truncated generalized extreme value distribution‐based ensemble model output statistics model for calibration of wind speed ensemble forecasts},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/10.1002/env.2678},
   year = {2021},
}
@article{Kunsch1989,
   abstract = {We extend the jackknife and the bootstrap method of estimating standard errors to the case where the observations form a general stationary sequence. We do not attempt a reduction to i.i.d. values. The jackknife calculates the sample variance of replicates of the statistic obtained by omitting each block of $l$ consecutive data once. In the case of the arithmetic mean this is shown to be equivalent to a weighted covariance estimate of the spectral density of the observations at zero. Under appropriate conditions consistency is obtained if $l = l(n) \rightarrow \infty$ and $l(n)/n \rightarrow 0$. General statistics are approximated by an arithmetic mean. In regular cases this approximation determines the asymptotic behavior. Bootstrap replicates are constructed by selecting blocks of length $l$ randomly with replacement among the blocks of observations. The procedures are illustrated by using the sunspot numbers and some simulated data.},
   author = {Hans R. Kunsch},
   doi = {10.1214/AOS/1176347265},
   issn = {0090-5364},
   issue = {3},
   journal = {The Annals of Statistics},
   keywords = {62G05,62G15,62M10,bootstrap,influence function,jackknife,statistics defined by functionals,time series,variance estimation},
   month = {9},
   pages = {1217-1241},
   publisher = {Institute of Mathematical Statistics},
   title = {The Jackknife and the Bootstrap for General Stationary Observations},
   volume = {17},
   url = {https://projecteuclid.org/journals/annals-of-statistics/volume-17/issue-3/The-Jackknife-and-the-Bootstrap-for-General-Stationary-Observations/10.1214/aos/1176347265.full https://projecteuclid.org/journals/annals-of-statistics/volume-17/issue-3/The-Jackknife-and-the-Bootstrap-for-General-Stationary-Observations/10.1214/aos/1176347265.short},
   year = {1989},
}
@article{Deidda2023,
   abstract = {Extremal dependence describes the strength of correlation between the largest observations of two variables. It is usually measured with symmetric dependence coefficients that do not depend on the order of the variables. In many cases, there is a natural asymmetry between extreme observations that can not be captured by such coefficients. An example for such asymmetry are large discharges at an upstream and a downstream stations on a river network: an extreme discharge at the upstream station will directly influence the discharge at the downstream station, but not vice versa. Simple measures for asymmetric dependence in extreme events have not yet been investigated. We propose the asymmetric tail Kendall's $\tau$ as a measure for extremal dependence that is sensitive to asymmetric behaviour in the largest observations. It essentially computes the classical Kendall's $\tau$ but conditioned on the extreme observations of one of the two variables. We show theoretical properties of this new coefficient and derive a formula to compute it for existing copula models. We further study its effectiveness and connections to causality in simulation experiments. We apply our methodology to a case study on river networks in the United Kingdom to illustrate the importance of measuring asymmetric extremal dependence in hydrology. Our results show that there is important structural information in the asymmetry that would have been missed by a symmetric measure. Our methodology is an easy but effective tool that can be applied in exploratory analysis for understanding the connections among variables and to detect possible asymmetric dependencies.},
   author = {Cristina Deidda and Sebastian Engelke and Carlo De Michele},
   month = {1},
   title = {Asymmetric dependence in hydrological extremes},
   url = {http://arxiv.org/abs/2301.08764},
   year = {2023},
}
@article{Chebana2011,
   abstract = {Several hydrological phenomena are described by two or more correlated characteristics. These dependent characteristics should be considered jointly to be more representative of the multivariate nature of the phenomenon. Consequently, probabilities of occurrence cannot be estimated on the basis of univariate frequency analysis (FA). The quantile, representing the value of the variable(s) corresponding to a given risk, is one of the most important notions in FA. The estimation of multivariate quantiles has not been specifically treated in the hydrological FA literature. In the present paper, we present a new and general framework for local FA based on a multivariate quantile version. The multivariate quantile offers several combinations of the variable values that lead to the same risk. A simulation study is carried out to evaluate the performance of the proposed estimation procedure and a case study is conducted. Results show that the bivariate estimation procedure has an analogous behaviour to the univariate one with respect to the risk and the sample size. However, the dependence structure between variables is ignored in the univariate case. The univariate estimates are obtained as special combinations by the multivariate procedure and with equivalent accuracy. Copyright © 2009 John Wiley & Sons, Ltd.},
   author = {F. Chebana and T. B.M.J. Ouarda},
   doi = {10.1002/ENV.1027},
   issn = {11804009},
   issue = {1},
   journal = {Environmetrics},
   keywords = {Estimation,Floods,Frequency analysis,Hydrology,Multivariate quantile},
   month = {2},
   pages = {63-78},
   title = {Multivariate quantiles in hydrological frequency analysis},
   volume = {22},
   year = {2011},
}
@article{Konzen2021,
   abstract = {<p> This article compares the modeling of nonstationary extreme events using parametric models with local parametric and semiparametric approaches also motivated by extreme value theory. Specifically, three estimators are compared based on (a) (local) semiparametric moment estimation, (b) (local) maximum likelihood estimation, and (c) spline‐based maximum likelihood estimation. Inference is performed in a sequential manner, highlighting the synergies between the different approaches to estimating extreme quantiles, including the <italic>T</italic> ‐year level and right endpoint when finite. We present a novel heuristic to estimate nonstationary extreme value threshold with exceedances varying on a circular domain, and hypothesis‐testing procedures for identifying max‐domain of attraction in the nonstationary setting. Bootstrapping is used to estimate nonstationary confidence bounds throughout. We provide step‐by‐step guides for estimation, and explore the different inference strategies in application to directional modeling of hindcast storm peak significant wave heights recorded in the North Sea. </p>},
   author = {Evandro Konzen and Cláudia Neves and Philip Jonathan},
   doi = {10.1002/env.2667},
   issn = {1180-4009},
   issue = {4},
   journal = {Environmetrics},
   keywords = {circular,direction,endpoint,extreme quantile,kernel smoothing,peaks over threshold,periodic,significant wave height,spline,threshold selection},
   month = {6},
   pages = {1-23},
   publisher = {John Wiley and Sons Ltd},
   title = {Modeling nonstationary extremes of storm severity: Comparing parametric and semiparametric inference},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/10.1002/env.2667},
   year = {2021},
}
@misc{,
   abstract = {The design and reanalysis of offshore and coastal structures usually requires the estimation of return values for dominant metocean variables (such as significant wave height) and associated values for other variables (such as peak spectral period or wind speed) from a finite sample of data; these are typically estimated using extreme value analysis. Yet the parameters of extreme value models can only be estimated with error from finite data. Different choices available to summarise uncertain information about the characteristics of the tail of a multivariate distribution in a small number of summary statistics (such as return values and associated values) complicates their estimation, especially for small sample sizes: choices regarding the ordering of mathematical operations lead to estimators of return values and associated values with different finite sample bias and variance characteristics. The current work extends a previous study (Jonathan et al. 2021) into the performance of estimators for marginal return values in the presence of sampling uncertainty, to estimators of associated values based on the bivariate conditional extremes model (Heffernan and Tawn 2004) and competitors. Using a large designed simulation experiment, we explore the performance of combinations of 12 different estimators and three bivariate model candidates. The rich set of results from the simulation experiment are reported and explained in detail. Briefly: (a) calculation of associated values is only always feasible from small samples using two of the 12 estimators, which should be preferred; (b) estimators exploiting the median rather than the mean to summarise a distribution are more robust, and should also be preferred, especially for small sample sizes; (c) extreme value models incorporating appropriate descriptions of marginal and dependence provide better estimation of associated values for larger sample size; and (d) summarising the joint tail of metocean variables (in terms of return values and associated values) should be avoided where possible, in favour of probabilistic risk analysis of structural failure incorporating full uncertainty propagation.},
   author = {Ross Towe and David Randell and Jennifer Kensler and Graham Feld and Philip Jonathan},
   keywords = {associated value,conditioning,extreme,metocean,multivariate,return value},
   title = {Estimation of associated values from conditional extreme value models},
   url = {https://ssrn.com/abstract=4247199},
}
@article{Politis1994,
   author = {Dimitris N. Politis and Joseph P. Romano},
   doi = {10.1080/01621459.1994.10476870},
   issn = {0162-1459},
   issue = {428},
   journal = {Journal of the American Statistical Association},
   month = {12},
   pages = {1303-1313},
   title = {The Stationary Bootstrap},
   volume = {89},
   year = {1994},
}
@article{Simpson2021a,
   abstract = {Recent extreme value theory literature has seen significant emphasis on the modelling of spatial extremes, with comparatively little consideration of spatio-temporal extensions. This neglects an important feature of extreme events: their evolution over time. Many existing models for the spatial case are limited by the number of locations they can handle; this impedes extension to space–time settings, where models for higher dimensions are required. Moreover, the spatio-temporal models that do exist are restrictive in terms of the range of extremal dependence types they can capture. Recently, conditional approaches for studying multivariate and spatial extremes have been proposed, which enjoy benefits in terms of computational efficiency and an ability to capture both asymptotic dependence and asymptotic independence. We extend this class of models to a spatio-temporal setting, conditioning on the occurrence of an extreme value at a single space–time location. We adopt a composite likelihood approach for inference, which combines information from full likelihoods across multiple space–time conditioning locations. We apply our model to Red Sea surface temperatures, show that it fits well using a range of diagnostic plots, and demonstrate how it can be used to assess the risk of coral bleaching attributed to high water temperatures over consecutive days.},
   author = {Emma S. Simpson and Jennifer L. Wadsworth},
   doi = {10.1016/J.SPASTA.2020.100482},
   issn = {22116753},
   journal = {Spatial Statistics},
   keywords = {Conditional extremes,Environmental extremes,Extremal dependence modelling,Spatio-temporal modelling},
   month = {3},
   pages = {1-17},
   publisher = {Elsevier B.V.},
   title = {Conditional modelling of spatio-temporal extremes for Red Sea surface temperatures},
   volume = {41},
   year = {2021},
}
@article{Coles1994,
   abstract = {For many structural design problems univariate extreme value theory is applied to quantify the risk of failure due to extreme levels of some environmental process. In practice, many forms of structure fail owing to a combination of various processes at extreme levels. Recent developments in statistical methodology for multivariate extremes enable the modelling of such behaviour. The aim of this paper is to demonstrate how these ideas can be exploited as part of the design process},
   author = {Stuart G. Coles and Jonathan A. Tawn},
   doi = {10.2307/2986112},
   issn = {00359254},
   issue = {1},
   journal = {Applied Statistics},
   pages = {1-48},
   publisher = {JSTOR},
   title = {Statistical Methods for Multivariate Extremes: An Application to Structural Design},
   volume = {43},
   url = {https://www.jstor.org/stable/2986112?origin=crossref},
   year = {1994},
}
@article{Chatterjee1990,
   abstract = {Extreme points in multivariate space have many applications in statistics and can be studied as vertices of a convex polytope (hulls). Finding such vertices is a major topic of study in computational geometry but has received very little attention in applied statistics. Three new methods, based on random and restricted random sampling, is introduced for finding vertices of convex hulls of a sample of N points in k space. Simple projections are used iteratively to find the extreme points. Compared to existing methods, the new method utilizes raw computing power over theoretical analysis and is efficient for practical problems.},
   author = {Sangit Chatterjee and Samprit Chatterjee},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Computational complexity,Convex hulls,Data analysis,Polytopes,Random sampling,Vertices},
   pages = {87-92},
   title = {A note on finding extreme points in multivariate space},
   volume = {10},
   year = {1990},
}
@article{Peng1999,
   abstract = {In this paper we shall give an alternative derivation of the coefficient of tail dependence introduced by Ledford and Tawn [1996, Biometrika 83, 169-187] and propose a consistent estimator, which is asymptotically normal. © 1999 Elsevier Science B.V. All rights reserved.},
   author = {L. Peng},
   doi = {10.1016/S0167-7152(98)00280-6},
   issn = {01677152},
   issue = {4},
   journal = {Statistics and Probability Letters},
   keywords = {Bivariate extremes,Coefficient of tail dependence,Extreme value theory,Regular variation},
   month = {7},
   pages = {399-409},
   publisher = {Elsevier},
   title = {Estimation of the coefficient of tail dependence in bivariate extremes},
   volume = {43},
   year = {1999},
}
@article{Frahm2005,
   abstract = {The concept of tail dependence describes the amount of dependence in the lower-left-quadrant tail or upper-right-quadrant tail of a bivariate distribution. A common measure of tail dependence is given by the so-called tail-dependence coefficient. This paper surveys various estimators for the tail-dependence coefficient within a parametric, semiparametric, and nonparametric framework. Further, a detailed simulation study is provided which compares and illustrates the advantages and disadvantages of the estimators. © 2005 Elsevier B.V. All rights reserved.},
   author = {Gabriel Frahm and Markus Junker and Rafael Schmidt},
   doi = {10.1016/J.INSMATHECO.2005.05.008},
   issn = {01676687},
   issue = {1 SPEC. ISS.},
   journal = {Insurance: Mathematics and Economics},
   keywords = {Copula,Estimation,Extreme value theory,Simulation,Tail dependence,Tail-dependence coefficient},
   month = {8},
   pages = {80-100},
   publisher = {Elsevier},
   title = {Estimating the tail-dependence coefficient: Properties and pitfalls},
   volume = {37},
   year = {2005},
}
@article{Aghakouchak2010,
   abstract = {Extreme rainfall events are of particular importance due to their severe impacts on the economy, the environment and the society. Characterization and quantification of extremes and their spatial dependence structure may lead to a better understanding of extreme events. An important concept in statistical modeling is the tail dependence coefficient (TDC) that describes the degree of association between concurrent rainfall extremes at different locations. Accurate knowledge of the spatial characteristics of the TDC can help improve on the existing models of the occurrence probability of extreme storms. In this study, efficient estimation of the TDC in rainfall is investigated using a dense network of rain gauges located in south Louisiana, USA. The inter-gauge distances in this network range from about 1. km to 9. km. Four different nonparametric TDC estimators are implemented on samples of the rain gauge data and their advantages and disadvantages are discussed. Three averaging time-scales are considered: 1. h, 2. h and 3. h. The results indicate that a significant tail dependency may exist that cannot be ignored for realistic modeling of multivariate rainfall fields. Presence of a strong dependence among extremes contradicts with the assumption of joint normality, commonly used in hydrologic applications. © 2010 Elsevier Ltd.},
   author = {Amir Aghakouchak and Grzegorz Ciach and Emad Habib},
   doi = {10.1016/J.ADVWATRES.2010.07.003},
   issn = {03091708},
   issue = {9},
   journal = {Advances in Water Resources},
   keywords = {Empirical copula,Extreme values,Nonparametric estimators,Rainfall,Tail dependence coefficient (TDC)},
   month = {9},
   pages = {1142-1149},
   title = {Estimation of tail dependence coefficient in rainfall accumulation fields},
   volume = {33},
   year = {2010},
}
@article{Taillardat2019,
   abstract = {Verifying probabilistic forecasts for extreme events is a highly active research area because popular media and public opinions are naturally focused on extreme events, and biased conclusions are readily made. In this context, classical verification methods tailored for extreme events, such as thresholded and weighted scoring rules, have undesirable properties that cannot be mitigated, and the well-known continuous ranked probability score (CRPS) is no exception. In this paper, we define a formal framework for assessing the behavior of forecast evaluation procedures with respect to extreme events, which we use to demonstrate that assessment based on the expectation of a proper score is not suitable for extremes. Alternatively, we propose studying the properties of the CRPS as a random variable by using extreme value theory to address extreme event verification. An index is introduced to compare calibrated forecasts, which summarizes the ability of probabilistic forecasts for predicting extremes. The strengths and limitations of this method are discussed using both theoretical arguments and simulations.},
   author = {Maxime Taillardat and Anne-Laure Fougères and Philippe Naveau and Raphaël de Fondeville},
   doi = {10.1016/j.ijforecast.2022.07.003},
   month = {5},
   title = {Evaluating probabilistic forecasts of extremes using continuous ranked probability score distributions},
   url = {http://arxiv.org/abs/1905.04022 http://dx.doi.org/10.1016/j.ijforecast.2022.07.003},
   year = {2019},
}
@article{Casey2023,
   abstract = {We develop an asymptotic theory for extremes in decomposable graphical models by presenting results applicable to a range of extremal dependence types. Specifically, we investigate the weak limit of the distribution of suitably normalised random vectors, conditioning on an extreme component, where the conditional independence relationships of the random vector are described by a chordal graph. Under mild assumptions, the random vector corresponding to the distribution in the weak limit, termed the tail graphical model, inherits the graphical structure of the original chordal graph. Our theory is applicable to a wide range of decomposable graphical models including asymptotically dependent and asymptotically independent graphical models. Additionally, we analyze combinations of copula classes with differing extremal dependence in cases where a normalization in terms of the conditioning variable is not guaranteed by our assumptions. We show that, in a block graph, the distribution of the random vector normalized in terms of the random variables associated with the separators converges weakly to a distribution we term tail noise. In particular, we investigate the limit of the normalized random vectors where the clique distributions belong to two widely used copula classes, the Gaussian copula and the max-stable copula.},
   author = {Adrian Casey and Ioannis Papastathopoulos},
   journal = {arXiv},
   month = {2},
   title = {Decomposable Tail Graphical Models},
   volume = {2302.05182},
   url = {http://arxiv.org/abs/2302.05182},
   year = {2023},
}
@article{Caires2005,
   abstract = {We prove the strong consistency of estimators of the conditional distribution function and conditional expectation of a future observation of a discrete time stochastic process given a fixed number of past observations. The results apply to conditionally stationary processes (a class of processes including Markov and stationary processes) satisfying a strong mixing condition, and they extend and bring together the work of several authors in the area of non-parametric estimation. One of our goals is to provide further justification for the growing practical application of non-parametric estimators in non-stationary time series and in other 'non-i.i.d.' settings. Some arguments as to why such estimators should work very generally in practice, often in a nearly 'optimal' way, are given. Two numerical illustrations are included, one with simulated data and the other with oceanographic data.},
   author = {S. Caires and J. A. Ferreira},
   doi = {10.1007/s11203-004-0383-2},
   issn = {1387-0874},
   issue = {2},
   journal = {Statistical Inference for Stochastic Processes},
   keywords = {conditional distribution function,conditional expectation,data analysis,non-parametric prediction,time series},
   month = {9},
   pages = {151-184},
   title = {On the Non-parametric Prediction of Conditionally Stationary Sequences},
   volume = {8},
   url = {http://link.springer.com/10.1007/s11203-004-0383-2},
   year = {2005},
}
@article{Drees2022,
   abstract = {We analyze the extreme value dependence of independent, not necessarily identically distributed multivariate regularly varying random vectors. More specifically, we propose estimators of the spectral measure locally at some time point and of the spectral measures integrated over time. The uniform asymptotic normality of these estimators is proved under suitable nonparametric smoothness and regularity assumptions. We then use the process convergence of the integrated spectral measure to devise consistent tests for the null hypothesis that the spectral measure does not change over time. The finite sample performance of these tests is investigated in Monte Carlo simulations.},
   author = {Holger Drees},
   month = {1},
   title = {Statistical Inference on a Changing Extremal Dependence Structure},
   url = {http://arxiv.org/abs/2201.06389},
   year = {2022},
}
@article{Nutt2010,
   abstract = {Background Proper assessment of the harms caused by the misuse of drugs can inform policy makers in health, policing, and social care. We aimed to apply multicriteria decision analysis (MCDA) modelling to a range of drug harms in the UK.},
   author = {David J Nutt and Leslie A King and Lawrence D Phillips},
   doi = {10.1016/S0140-6736(10)61462-6},
   issn = {01406736},
   issue = {9752},
   journal = {The Lancet},
   month = {11},
   pages = {1558-1565},
   title = {Drug harms in the UK: a multicriteria decision analysis},
   volume = {376},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673610614626},
   year = {2010},
}
@article{Murphy-Barltrop2023,
   author = {C. J. R. Murphy-Barltrop and J. L. Wadsworth and E. F. Eastoe},
   doi = {10.1002/env.2797},
   issn = {1180-4009},
   journal = {Environmetrics},
   month = {2},
   pages = {1-22},
   title = {New estimation methods for extremal bivariate return curves},
   volume = {e2797},
   url = {https://onlinelibrary.wiley.com/doi/10.1002/env.2797},
   year = {2023},
}
@article{Bernard2013,
   abstract = {One of the main objectives of statistical climatology is to extract relevant information hidden in complex spatial-temporal climatological datasets. To identify spatial patterns, most well-known statistical techniques are based on the concept of intra- and intercluster variances (like the k-means algorithm or EOFs). As analyzing quantitative extremes like heavy rainfall has become more and more prevalent for climatologists and hydrologists during these last decades, finding spatial patterns with methods based on deviations from the mean (i.e., variances) may not be the most appropriate strategy in this context of studying such extremes. For practitioners, simple and fast clustering tools tailored for extremes have been lacking. A possible avenue to bridging this methodological gap resides in taking advantage of multivariate extreme value theory, a welldeveloped research field in probability, and to adapt it to the context of spatial clustering. In this paper, a novel algorithm based on this plan is proposed and studied. The approach is compared and discussed with respect to the classical k-means algorithm throughout the analysis of weekly maxima of hourly precipitation recorded in France (fall season, 92 stations, 1993-2011). © 2013 American Meteorological Society.},
   author = {Elsa Bernard and Philippe Naveau and Mathieu Vrac and Olivier Mestre},
   doi = {10.1175/JCLI-D-12-00836.1},
   issn = {08948755},
   issue = {20},
   journal = {Journal of Climate},
   pages = {7929-7937},
   title = {Clustering of maxima: Spatial dependencies among heavy rainfall in france},
   volume = {26},
   year = {2013},
}
@article{,
   abstract = {Background: The present paper describes the results of a rating study performed by a group of European Union (EU) drug experts using the multicriteria decision analysis model for evaluating drug harms. Methods: Forty drug experts from throughout the EU scored 20 drugs on 16 harm criteria. The expert group also assessed criteria weights that would apply, on average, across the EU. Weighted averages of the scores provided a single, overall weighted harm score (range: 0-100) for each drug. Results: Alcohol, heroin and crack emerged as the most harmful drugs (overall weighted harm score 72, 55 and 50, respectively). The remaining drugs had an overall weighted harm score of 38 or less, making them much less harmful than alcohol. The overall weighted harm scores of the EU experts correlated well with those previously given by the UK panel. Conclusion: The outcome of this study shows that the previous national rankings based on the relative harms of different drugs are endorsed throughout the EU. The results indicates that EU and national drug policy measures should focus on drugs with the highest overall harm, including alcohol and tobacco, whereas drugs such as cannabis and ecstasy should be given lower priority including a lower legal classification.},
   author = {Jan Van Amsterdam and David Nutt and Lawrence Phillips and Wim Van Den Brink},
   doi = {10.1177/0269881115581980},
   issn = {14617285},
   issue = {6},
   journal = {Journal of Psychopharmacology},
   keywords = {Illicit drugs,alcohol,ranking,recreational drugs,risk assessment,tobacco},
   month = {6},
   pages = {655-660},
   pmid = {25922421},
   publisher = {SAGE Publications Ltd},
   title = {European rating of drug harms},
   volume = {29},
   year = {2015},
}
@book{Joe1997,
   author = {Harry Joe},
   doi = {10.1201/9780367803896},
   isbn = {9780367803896},
   month = {5},
   publisher = {Chapman and Hall/CRC},
   title = {Multivariate Models and Multivariate Dependence Concepts},
   url = {https://www.taylorfrancis.com/books/9781466581432},
   year = {1997},
}
@article{Aghbalou2021,
   abstract = {We consider the problem of supervised dimension reduction with a particular focus on extreme values of the target $Y\in\mathbb\{R\}$ to be explained by a covariate vector $X \in \mathbb\{R\}^p$. The general purpose is to define and estimate a projection on a lower dimensional subspace of the covariate space which is sufficient for predicting exceedances of the target above high thresholds. We propose an original definition of Tail Conditional Independence which matches this purpose. Inspired by Sliced Inverse Regression (SIR) methods, we develop a novel framework (TIREX, Tail Inverse Regression for EXtreme response) in order to estimate an extreme sufficient dimension reduction (SDR) space of potentially smaller dimension than that of a classical SDR space. We prove the weak convergence of tail empirical processes involved in the estimation procedure and we illustrate the relevance of the proposed approach on simulated and real world data.},
   author = {Anass Aghbalou and François Portier and Anne Sabourin and Chen Zhou},
   journal = {arXiv},
   month = {7},
   title = {Tail inverse regression for dimension reduction with extreme response},
   volume = {2108.01432},
   url = {http://arxiv.org/abs/2108.01432},
   year = {2021},
}
@article{Yu2001,
   abstract = {The paper introduces the idea of Bayesian quantile regression employing a likelihood function that is based on the asymmetric Laplace distribution. It is shown that irrespective of the original distribution of the data, the use of the asymmetric Laplace distribution is a very natural and effective way for modelling Bayesian quantile regression. The paper also demonstrates that improper uniform priors for the unknown model parameters yield a proper joint posterior. The approach is illustrated via a simulated and two real data sets. © 2001 Elsevier Science B.V.},
   author = {Keming Yu and Rana A. Moyeed},
   doi = {10.1016/S0167-7152(01)00124-9},
   issn = {01677152},
   issue = {4},
   journal = {Statistics \& Probability Letters},
   keywords = {Asymmetric Laplace distribution,Bayesian inference,Markov chain Monte Carlo methods,Quantile regression},
   month = {10},
   pages = {437-447},
   publisher = {North-Holland},
   title = {Bayesian quantile regression},
   volume = {54},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0167715201001249},
   year = {2001},
}
@article{Oh2011,
   abstract = {The calculation of nonparametric quantile regression curve estimates is often computationally intensive, as typically an expensive nonlinear optimization problem is involved. This article proposes ...},
   author = {Hee-Seok Oh and Thomas C. M. Lee and Douglas W. Nychka},
   doi = {10.1198/jcgs.2010.10063},
   issn = {1061-8600},
   issue = {2},
   journal = {Journal of Computational and Graphical Statistics},
   keywords = {Bivariate quantile regression,Nonparametric regression,Pseudo data,Regression quantile,Wavelets},
   month = {1},
   pages = {510-526},
   publisher = {Taylor & Francis},
   title = {Fast Nonparametric Quantile Regression With Arbitrary Smoothing Methods},
   volume = {20},
   url = {http://www.tandfonline.com/doi/abs/10.1198/jcgs.2010.10063},
   year = {2011},
}
@article{Auld2023,
   abstract = {Accurate modelling of the joint extremal dependence structure within a stationary time series is a challenging problem that is important in many applications.\ Several previous approaches to this problem are only applicable to certain types of extremal dependence in the time series such as asymptotic dependence, or Markov time series of finite order.\ In this paper, we develop statistical methodology for time series extremes based on recent probabilistic results that allow us to flexibly model the decay of a stationary time series after witnessing an extreme event.\ While Markov sequences of finite order are naturally accommodated by our approach, we consider a broader setup, based on the conditional extreme value model, which allows for a wide range of possible dependence structures in the time series.\ We consider inference based on Monte Carlo simulation and derive an upper bound for the variance of a commonly used importance sampler.\ Our methodology is illustrated via estimation of cluster functionals in simulated data and in a time series of daily maximum temperatures from Orleans, France.},
   author = {Graeme Auld and Ioannis Papastathopoulos},
   month = {3},
   title = {Time series conditional extremes},
   url = {http://arxiv.org/abs/2303.04447},
   year = {2023},
}
@article{Lederer2023,
   abstract = {Extreme-value theory has been explored in considerable detail for univariate and low-dimensional observations, but the field is still in an early stage regarding high-dimensional multivariate observations. In this paper, we focus on H\"usler-Reiss models and their domain of attraction, a popular class of models for multivariate extremes that exhibit some similarities to multivariate Gaussian distributions. We devise novel estimators for the parameters of this model based on score matching and equip these estimators with state-of-the-art theories for high-dimensional settings and with exceptionally scalable algorithms. We perform a simulation study to demonstrate that the estimators can estimate a large number of parameters reliably and fast; for example, we show that H\"usler-Reiss models with thousands of parameters can be fitted within a couple of minutes on a standard laptop.},
   author = {Johannes Lederer and Marco Oesting},
   month = {3},
   title = {Extremes in High Dimensions: Methods and Scalable Algorithms},
   url = {http://arxiv.org/abs/2303.04258},
   year = {2023},
}
@article{,
   abstract = {Estimating the density function of a random vector taking values on the d-dimensional unit sphere is considered. Also the estimation of the Laplacian of the density and estimation of other types of derivatives is considered. Fast convergence rate theory is developed for pointwise, L 1 , and L 2 error, extending some results of Hall, Watson and Cabrera (1987). It is also proved that asymptotically the plug-in method is as good as using the asymptotically optimal deterministic smoothing parameter sequence.},
   author = {Jussi Klemelä},
   doi = {10.1006/jmva.1999.1861},
   issn = {0047259X},
   issue = {1},
   journal = {Journal of Multivariate Analysis},
   keywords = {Laplace operator,Lp error,kernel estimator,nonparametric density estimation,plug-in method,spherical data},
   month = {4},
   pages = {18-40},
   title = {Estimation of Densities and Derivatives of Densities with Directional Data},
   volume = {73},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0047259X99918614},
   year = {2000},
}
@article{Prayag1990,
   abstract = {Nearest neighbour methods traditionally used to estimate density of a sessile biological population treat individuals as points. The present paper suggests distance-based density estimators which treat individuals as circles with variable areas. Distribution of distance between a sample point and the k-th (k = 1, 2, 3, ...) nearest circle is derived. Maximum likelihood estimator of density is obtained from a random sample of point to k-th order distances. Assuming a skewed distribution for the circle radius, moment estimators of density and mean circle area are derived.},
   author = {V. R. Prayag and A. P. Gore},
   doi = {10.1007/BF02613507},
   issn = {0026-1335},
   issue = {1},
   journal = {Metrika},
   keywords = {Density estimation,Nearest neighbour methods,Random configuration},
   month = {12},
   pages = {63-69},
   title = {Density estimation for randomly distributed circular objects},
   volume = {37},
   url = {http://link.springer.com/10.1007/BF02613507},
   year = {1990},
}
@article{Fisher1989,
   abstract = {The purpose of this note is to demonstrate certain pitfalls associated with the use of circular histograms or rose diagrams for displaying two-dimensional orientation data, and to recommend an alternative method of summarizing and displaying distributional features of the data. © 1989.},
   author = {N. I. Fisher},
   doi = {10.1016/0191-8141(89)90012-6},
   issn = {01918141},
   issue = {6},
   journal = {Journal of Structural Geology},
   pages = {775-778},
   title = {Smoothing a sample of circular data},
   volume = {11},
   year = {1989},
}
@article{Taylor2008,
   abstract = {Given angular data θ1, ..., θn ∈ [0, 2 π) a common objective is to estimate the density. In case that a kernel estimator is used, bandwidth selection is crucial to the performance. A "plug-in rule" for the bandwidth, which is based on the concentration of a reference density, namely, the von Mises distribution is obtained. It is seen that this is equivalent to the usual Euclidean plug-in rule in the case where the concentration becomes large. In case that the concentration parameter is unknown, alternative methods are explored which are intended to be robust to departures from the reference density. Simulations indicate that "wrapped estimators" can perform well in this context. The methods are applied to a real bivariate dataset concerning protein structure. © 2007 Elsevier Ltd. All rights reserved.},
   author = {Charles C. Taylor},
   doi = {10.1016/J.CSDA.2007.11.003},
   issn = {01679473},
   issue = {7},
   journal = {Computational Statistics and Data Analysis},
   month = {3},
   pages = {3493-3500},
   title = {Automatic bandwidth selection for circular density estimation},
   volume = {52},
   year = {2008},
}
@article{Schorlepp2023,
   abstract = {We introduce and compare computational techniques for sharp extreme event probability estimates in stochastic differential equations with small additive Gaussian noise. In particular, we focus on strategies that are scalable, i.e. their efficiency does not degrade upon spatial and temporal refinement. For that purpose, we extend algorithms based on the Laplace method for estimating the probability of an extreme event to infinite dimensions. The method estimates the limiting exponential scaling using a single realization of the random variable, the large deviation minimizer. Finding this minimizer amounts to solving an optimization problem governed by a differential equation. The probability estimate becomes sharp when it additionally includes prefactor information, which necessitates computing the determinant of a second derivative operator to evaluate a Gaussian integral around the minimizer. We present an approach in infinite dimensions based on Fredholm determinants, and develop numerical algorithms to compute these determinants efficiently for the high-dimensional systems that arise upon discretization. We also give an interpretation of this approach using Gaussian process covariances and transition tubes. An example model problem, for which we also provide an open-source python implementation, is used throughout the paper to illustrate all methods discussed. To study the performance of the methods, we consider examples of stochastic differential and stochastic partial differential equations, including the randomly forced incompressible three-dimensional Navier-Stokes equations.},
   author = {Timo Schorlepp and Shanyin Tong and Tobias Grafke and Georg Stadler},
   journal = {arXiv},
   month = {3},
   title = {Scalable Methods for Computing Sharp Extreme Event Probabilities in Infinite-Dimensional Stochastic Systems},
   volume = {2303.11919},
   url = {http://arxiv.org/abs/2303.11919},
   year = {2023},
}
@article{Murphy-Barltrop2023a,
   abstract = {Modelling the extremal dependence of bivariate variables is important in a wide variety of practical applications, including environmental planning, catastrophe modelling and hy-drology. The majority of these approaches are based on the framework of bivariate regular variation, and a wide range of literature is available for estimating the dependence structure in this setting. However, this framework is only applicable to variables exhibiting asymptotic dependence, even though asymptotic independence is often observed in practice. In this paper , we consider the so-called 'angular dependence function'; this quantity summarises the extremal dependence structure for asymptotically independent variables. Until recently, only pointwise estimators of the angular dependence function have been available. We introduce a range of global estimators and compare them to another recently introduced technique for global estimation through a systematic simulation study, and a case study on river flow data from the north of England, UK.},
   author = {C J R Murphy-Barltrop and J L Wadsworth and E F Eastoe},
   journal = {arXiv},
   keywords = {Angular De-pendence Function 1,Asymptotic Independence,Bivariate Extremes,Dependence Modelling},
   title = {Improving estimation for asymptotically independent bivariate extremes via global estimators for the angular dependence function},
   volume = {2303.13237},
   year = {2023},
}
@article{Zanini2020,
   abstract = {Environmental extremes often show systematic variation with covariates. Three different nonparametric descriptions (penalized B-splines, Bayesian adaptive regression splines, and Voronoi partition) for the dependence of extreme value model parameters on covariates are considered. These descriptions take the generic form of a linear combination of basis functions on the covariate domain, but differ (i) in the way that basis functions are constructed and possibly modified, and potentially (ii) by additional penalization of the variability (e.g., variance or roughness) of basis coefficients, for a given sample, to improve inference. The three representations are used to characterize variation of parameters in a nonstationary generalized Pareto model for the magnitude of threshold exceedances with respect to covariates. Computationally efficient schemes for Bayesian inference are used, including Riemann manifold Metropolis-adjusted Langevin algorithm and reversible jump. A simulation study assesses relative performance of the three descriptions in estimating the distribution of the T-year maximum event (for arbitrary T greater than the period of the sample) from a peaks over threshold extreme value analysis with respect to a single periodic covariate. The three descriptions are also used to estimate a directional tail model for peaks over threshold of storm peak significant wave height at a location in the northern North Sea.},
   author = {E. Zanini and E. Eastoe and M. J. Jones and D. Randell and P. Jonathan},
   doi = {10.1002/ENV.2624},
   issn = {1099095X},
   issue = {5},
   journal = {Environmetrics},
   keywords = {Bayesian,MCMC,Poisson,Voronoi,covariate,extreme,generalized Pareto,mMALA,nonstationarity,ocean wave,return value,reversible jump,spline,storm severity},
   month = {8},
   pages = {1-28},
   publisher = {John Wiley and Sons Ltd},
   title = {Flexible covariate representations for extremes},
   volume = {31},
   year = {2020},
}
@article{Murphy-Barltrop2022,
   abstract = {In many practical applications, evaluating the joint impact of combinations of environmental variables is important for risk management and structural design analysis. When such variables are considered simultaneously, non-stationarity can exist within both the marginal distributions and dependence structure, resulting in complex data structures. In the context of extremes, few methods have been proposed for modelling trends in extremal dependence, even though capturing this feature is important for quantifying joint impact. Motivated by the increasing dependence of data from the UK Climate Projections, we propose a novel semi-parametric modelling framework for bivariate extremal dependence structures. This framework allows us to capture a wide variety of dependence trends for data exhibiting asymptotic independence. When applied to the climate projection dataset, our model is able to capture observed dependence trends and, in combination with models for marginal non-stationarity, can be used to produce estimates of bivariate risk measures at future time points.},
   author = {C. J. R. Murphy-Barltrop and J. L. Wadsworth},
   journal = {arXiv},
   month = {3},
   title = {Modelling non-stationarity in asymptotically independent extremes},
   volume = {2203.05860},
   url = {http://arxiv.org/abs/2203.05860},
   year = {2024},
}
@article{Simpson2020,
   abstract = {In multivariate extreme value analysis, the nature of the extremal dependence between variables should be considered when selecting appropriate statistical models. Interest often lies in determining which subsets of variables can take their largest values simultaneously while the others are of smaller order. Our approach to this problem exploits hidden regular variation properties on a collection of nonstandard cones, and provides a new set of indices that reveal aspects of the extremal dependence structure not available through existing measures of dependence. We derive theoretical properties of these indices, demonstrate their utility through a series of examples, and develop methods of inference that also estimate the proportion of extremal mass associated with each cone. We apply the methods to river flows in the U.K., estimating the probabilities of different subsets of sites being large simultaneously.},
   author = {E S Simpson and J L Wadsworth and J A Tawn},
   doi = {10.1093/biomet/asaa018},
   issn = {0006-3444},
   issue = {3},
   journal = {Biometrika},
   keywords = {Asymptotic independence,Extremal dependence structure,Hidden regular variation,Multivariate regular variation},
   month = {9},
   pages = {513-532},
   publisher = {Oxford University Press},
   title = {Determining the dependence structure of multivariate extremes},
   volume = {107},
   url = {https://academic.oup.com/biomet/article/107/3/513/5831922},
   year = {2020},
}
@inbook{Chaubeyor2022,
   abstract = {Fisher (1989: J. Structural Geology 11, 775-778) outlined an adaptation of the linear kernel estima-tor for density estimation that is commonly used in applications. However, better alternatives are now available based on circular kernels; see e.g. Di Marzio, Panzera, and Taylor, 2009: Statistics & Probability Letters 79, 2066-2075. This paper provides a short review on modern smoothing methods for density and distribution functions dealing with the circular data. We highlight the usefulness of circular kernels for smooth density estimation in this context and contrast it with smooth density estimation based on orthogonal series. It is seen that the wrapped Cauchy kernel as a choice of circular kernel appears as a natural candidate as it has a close connection to orthogonal series density estimation on a unit circle. In the literature the use of von Mises circular kernel is investigated (see Taylor, 2008: Computational Statistics & Data Analysis 52, 3493-3500), that requires numerical computation of Bessel function. On the other hand, the wrapped Cauchy kernel is much simpler to use. This adds further weight to the considerable role of the wrapped Cauchy distribution in circular statistics.},
   author = {Yogendra P. Chaubey},
   doi = {10.1007/978-981-19-1044-9_19},
   keywords = {circular kernels,kernel density estimator,orthogonal polynomials,orthogonal series density},
   pages = {351-378},
   title = {On Nonparametric Density Estimation for Circular Data: An Overview},
   url = {https://link.springer.com/10.1007/978-981-19-1044-9_19},
   year = {2022},
}
@article{Pewsey2021,
   abstract = {Mainstream statistical methodology is generally applicable to data observed in Euclidean space. There are, however, numerous contexts of considerable scientific interest in which the natural supports for the data under consideration are Riemannian manifolds like the unit circle, torus, sphere, and their extensions. Typically, such data can be represented using one or more directions, and directional statistics is the branch of statistics that deals with their analysis. In this paper, we provide a review of the many recent developments in the field since the publication of Mardia and Jupp (Wiley 1999), still the most comprehensive text on directional statistics. Many of those developments have been stimulated by interesting applications in fields as diverse as astronomy, medicine, genetics, neurology, space situational awareness, acoustics, image analysis, text mining, environmetrics, and machine learning. We begin by considering developments for the exploratory analysis of directional data before progressing to distributional models, general approaches to inference, hypothesis testing, regression, nonparametric curve estimation, methods for dimension reduction, classification and clustering, and the modelling of time series, spatial and spatio-temporal data. An overview of currently available software for analysing directional data is also provided, and potential future developments are discussed.},
   author = {Arthur Pewsey and Eduardo García-Portugués},
   doi = {10.1007/s11749-021-00759-x},
   issn = {1133-0686},
   issue = {1},
   journal = {TEST},
   keywords = {Classification,Clustering,Dimension reduction,Distributional models,Exploratory data analysis,Hypothesis tests,Nonparametric methods,Regression,Serial dependence,Software,Spatial statistics},
   month = {3},
   pages = {1-58},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Recent advances in directional statistics},
   volume = {30},
   url = {https://link.springer.com/10.1007/s11749-021-00759-x},
   year = {2021},
}
@article{Geraci2007,
   abstract = {In longitudinal studies, measurements of the same individuals are taken repeatedly through time. Often, the primary goal is to characterize the change in response over time and the factors that influence change. Factors can affect not only the location but also more generally the shape of the distribution of the response over time. To make inference about the shape of a population distribution, the widely popular mixed-effects regression, for example, would be inadequate, if the distribution is not approximately Gaussian. We propose a novel linear model for quantile regression (QR) that includes random effects in order to account for the dependence between serial observations on the same subject. The notion of QR is synonymous with robust analysis of the conditional distribution of the response variable. We present a likelihood-based approach to the estimation of the regression quantiles that uses the asymmetric Laplace density. In a simulation study, the proposed method had an advantage in terms of mean squared error of the QR estimator, when compared with the approach that considers penalized fixed effects. Following our strategy, a nearly optimal degree of shrinkage of the individual effects is automatically selected by the data and their likelihood. Also, our model appears to be a robust alternative to the mean regression with random effects when the location parameter of the conditional distribution of the response is of interest. We apply our model to a real data set which consists of self-reported amount of labor pain measurements taken on women repeatedly over time, whose distribution is characterized by skewness, and the significance of the parameters is evaluated by the likelihood ratio statistic.},
   author = {Marco Geraci and Matteo Bottai},
   doi = {10.1093/BIOSTATISTICS/KXJ039},
   issn = {1465-4644},
   issue = {1},
   journal = {Biostatistics},
   keywords = {Asymmetric Laplace distribution,Clinical trials,Markov Chain Monte Carlo,Quantile regression,Random effects},
   month = {1},
   pages = {140-154},
   pmid = {16636139},
   publisher = {Oxford Academic},
   title = {Quantile regression for longitudinal data using the asymmetric Laplace distribution},
   volume = {8},
   url = {https://academic.oup.com/biostatistics/article/8/1/140/252234},
   year = {2007},
}
@article{Wood2016,
   abstract = {This article discusses a general framework for smoothing parameter estimation for models with regular likelihoods constructed in terms of unknown smooth functions of covariates. Gaussian random effects and parametric terms may also be present. By construction the method is numerically stable and convergent, and enables smoothing parameter uncertainty to be quantified. The latter enables us to fix a well known problem with AIC for such models, thereby improving the range of model selection tools available. The smooth functions are represented by reduced rank spline like smoothers, with associated quadratic penalties measuring function smoothness. Model estimation is by penalized likelihood maximization, where the smoothing parameters controlling the extent of penalization are estimated by Laplace approximate marginal likelihood. The methods cover, for example, generalized additive models for nonexponential family responses (e.g., beta, ordered categorical, scaled t distribution, negative binomial and Tweedie distributions), generalized additive models for location scale and shape (e.g., two stage zero inflation models, and Gaussian location-scale models), Cox proportional hazards models and multivariate additive models. The framework reduces the implementation of new model classes to the coding of some standard derivatives of the log-likelihood. Supplementary materials for this article are available online.},
   author = {Simon N. Wood and Natalya Pya and Benjamin Säfken},
   doi = {10.1080/01621459.2016.1180986},
   issn = {1537274X},
   issue = {516},
   journal = {Journal of the American Statistical Association},
   keywords = {AIC,Additive model,Distributional regression,GAM,Location scale and shape model,Ordered categorical regression,Penalized regression spline,REML,Smooth Cox model,Smoothing parameter uncertainty,Statistical algorithm,Tweedie distribution},
   month = {10},
   pages = {1548-1563},
   publisher = {American Statistical Association},
   title = {Smoothing Parameter and Model Selection for General Smooth Models},
   volume = {111},
   year = {2016},
}
@article{Padoan2023,
   abstract = {Marginal expected shortfall is unquestionably one of the most popular systemic risk measures. Studying its extreme behaviour is particularly relevant for risk protection against severe global financial market downturns. In this context, results of statistical inference rely on the bivariate extreme values approach, disregarding the extremal dependence among a large number of financial institutions that make up the market. In order to take it into account we propose an inferential procedure based on the multivariate regular variation theory. We derive an approximating formula for the extreme marginal expected shortfall and obtain from it an estimator and its bias-corrected version. Then, we show their asymptotic normality, which allows in turn the confidence intervals derivation. Simulations show that the new estimators greatly improve upon the performance of existing ones and confidence intervals are very accurate. An application to financial returns shows the utility of the proposed inferential procedure. Statistical results are extended to a general $\beta$-mixing context that allows to work with popular time series models with heavy-tailed innovations.},
   author = {Simone A. Padoan and Stefano Rizzelli and Matteo Schiavone},
   month = {4},
   title = {Marginal expected shortfall inference under multivariate regular variation},
   url = {http://arxiv.org/abs/2304.07578},
   year = {2023},
}
@article{Parey2013,
   abstract = {The important role of the evolution of mean temperature in the changes of extremes has been recently documented in the literature, and variability is known to play a role in the occurrence of extremes, too. This paper aims at further investigating the role of their evolutions in the observed changes of temperature extremes. Analyses are based on temperature time series for Eurasia and the United States and concern absolute minima in winter and absolute maxima in summer of daily minimum and maximum temperatures. A test is designed to check whether the extremes of the residuals after accounting for a time-varying mean and standard deviation can be considered stationary. This hypothesis is generally true for all extremes, seasons, and locations. Then, the comparison between the directly fitted parameters and the retrieved ones from those of the residuals compares favorably. Finally, a method is proposed to compute future return levels from the stationary return levels of the residuals and the projected mean and variance at the desired time horizon. Comparisons with return levels obtained through the extrapolation of significant linear trends identified in the parameters of the generalized extreme value (GEV) distribution show that the proposed method gives relevant results. It allows taking mean and/or variance trends into account in the estimation of extremes even though no significant trends in the GEV parameters can be identified. Moreover, the role of trends in variance cannot be neglected. Lastly, first results based on two CMIP5 climate models show that the identified link between mean and variance trends and trends in extremes is correctly reproduced by the models and is maintained in the future. Key Points The paper analyses the evolution of temperature extremes To do so, a statioanrity test is designed This helps computing future return levels ©2013. American Geophysical Union. All Rights Reserved.},
   author = {Sylvie Parey and Thi Thu Huong Hoang and Didier Dacunha-Castelle},
   doi = {10.1002/JGRD.50629},
   issn = {2169-8996},
   issue = {15},
   journal = {Journal of Geophysical Research: Atmospheres},
   keywords = {air temperature,climate change,extremes,trends,variability},
   month = {8},
   pages = {8285-8296},
   publisher = {John Wiley & Sons, Ltd},
   title = {The importance of mean and variance in predicting changes in temperature extremes},
   volume = {118},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/jgrd.50629 https://onlinelibrary.wiley.com/doi/abs/10.1002/jgrd.50629 https://agupubs.onlinelibrary.wiley.com/doi/10.1002/jgrd.50629},
   year = {2013},
}
@article{Parey2019,
   abstract = {Designing some long-lasting industrial assets necessitates an estimation of far future extremes. Extreme value estimation is commonly based on an application of the statistical extreme value theory (EVT), which requires that the studied variable is independent and identically distributed, or, at least, stationary. Climate variables exhibit different behaviors which potentially violate this assumption: Seasonality is generally the easiest to handle, and interannual variability is more complicated. Now, as far as temperature is concerned, an additional source of non-stationarity appears: the warming trend, whose interactions with interannual variability add another range of complexity. The approach proposed here is based on the construction of a standardized variable, whose extremes can be considered as stationary. This allows an application of EVT in better accordance with its assumptions. Recent works (Parey et al. in J Geophys Res Atmos 118:8285–8296, 2013. https://doi.org/10.1002/jgrd.50629) have shown that if optimized smooth trends in mean and standard deviation are removed from the temperature time series, then the extremes of the residuals can be considered as stationary. A statistical test has been designed to check this assumption. Here, the inference of high-temperature extremes from the extremes of this standardized variable and future mean and standard deviation projected at the desired time horizon, and given by climate model simulations, is further analyzed and justified.},
   author = {S. Parey and T. T.H. Hoang and D. Dacunha-Castelle},
   doi = {10.1007/S11069-018-3499-1/TABLES/7},
   issn = {15730840},
   issue = {3},
   journal = {Natural Hazards},
   keywords = {Climate change,Extreme value theory,Temperature extremes},
   month = {9},
   pages = {1115-1134},
   publisher = {Springer Netherlands},
   title = {Future high-temperature extremes and stationarity},
   volume = {98},
   url = {https://link.springer.com/article/10.1007/s11069-018-3499-1},
   year = {2019},
}
@article{Parey2007,
   abstract = {The existence of an increasing trend in average temperatures during the last 50 years is widely acknowledged. Furthermore, there is compelling evidence of the variability of extremes, and rapid strides are made in studies of these events. Indeed, by extending the results of the "extreme value theory" (EVT) to the non-stationary case, analyses can examine the presence of trends in extreme values of stochastic processes. Definition of extreme events, their statistical significance as well as their interpretations have to be handled with great care when used for environmental concerns and public safety. Thus, we will discuss the validity of the hypothesis allowing the use of mathematical theories for these problems. To answer safety requirements, respect installation norms and reduce public risk, return levels are a major operational goal, obtained with the EVT. In this paper, we give quantitative results for observations of high temperatures over the 1950-2003 period in 47 stations in France. We examined the validity of the non-stationary EVT and introduced the notion of return levels (RL) in a time-varying context. Our analysis puts particular accent on the difference between methods used to describe extremes, to perform advanced fits and tests (climatic science), and those estimating the probability of rare future events (security problems in an evolving climate). After enouncing the method used for trend identification of extremes in term of easily interpretable parameters of distribution laws, we apply the procedure to long series of temperature measurements and check the influence of data length on trend estimation. We also address the problem of choosing the part of observations allowing appropriate extrapolation. In our analysis, we determined the influence of the 2003 heat wave on trend and return-level estimation comparing it to the RL in a stationary context. The application of the procedure to 47 stations spread over France is a first step for a refined spatial analysis. Working on the behavior of distribution parameters while assessing trend identification is a primary tool in order to classify climatic change with respect to the location of the station and open a systematic work using the same methodology for other variables and multivariate studies. © 2006 Springer Science+Business Media B.V.},
   author = {Sylvie Parey and Farida Malek and Carine Laurent and Didier Dacunha-Castelle},
   doi = {10.1007/S10584-006-9116-4/METRICS},
   issn = {01650009},
   issue = {3-4},
   journal = {Climatic Change},
   keywords = {Atmospheric Sciences,Climate Change/Climate Change Impacts},
   month = {4},
   pages = {331-352},
   publisher = {Springer},
   title = {Trends and climate evolution: Statistical approach for very high temperatures in France},
   volume = {81},
   url = {https://link.springer.com/article/10.1007/s10584-006-9116-4},
   year = {2007},
}
@article{Parey2010,
   abstract = {The climate change context has raised new problems in the computation of temperature return levels (RLs) in using the statistical extreme value theory. This arises since it is not yet possible to accept the hypothesis that the series of maxima or of high level values are stationary, without at least verifying the assumption. Thus, in this paper, different approaches are tested and compared to derive high order RLs in the nonstationary context. These RLs are computed by extrapolating identified trends, and a bootstrap method is used to estimate confidence intervals. The identification of trends can be made either in the parameters of the extreme value distributions or in the mean and variance of the whole series. Then, a methodology is proposed to test if the trends in extremes can be explained by the trends in mean and variance of the whole dataset. If this is the case, the future extremes can be derived from the stationary extremes of the centered and normalized variable and the changes in mean and variance of the whole dataset. The RL can then be estimated as nonstationary or as stationary for fixed future periods. The work is done for both extreme value methods: block maxima and peak over threshold, and will be illustrated with the example of a long observation time series for daily maximum temperature in France. © 2010 John Wiley & Sons, Ltd.},
   author = {Sylvie Parey and Thi Thu Huong Hoang and Didier Dacunha-Castelle},
   doi = {10.1002/ENV.1060},
   issn = {1099-095X},
   issue = {7-8},
   journal = {Environmetrics},
   keywords = {climate change,extrapolation,extreme value theory,return level,trends},
   month = {11},
   pages = {698-718},
   publisher = {John Wiley & Sons, Ltd},
   title = {Different ways to compute temperature return levels in the climate change context},
   volume = {21},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/env.1060 https://onlinelibrary.wiley.com/doi/abs/10.1002/env.1060 https://onlinelibrary.wiley.com/doi/10.1002/env.1060},
   year = {2010},
}
@article{Momoki2023,
   abstract = {Extreme value theory (EVT) provides an elegant mathematical tool for the statistical analysis of rare events. Typically, when data consists of multiple clusters, analysts want to preserve cluster information such as region, period, and group. To take into account the large sized cluster information in extreme value analysis, we incorporate the mixed effects model (MEM) into the regression technique in EVT instead of the traditional approach such as multivariate extreme value distribution. The MEM has been recognized not only as a model for clustered data, but also as a tool for providing reliable estimates of large sized clusters with small sample sizes. In EVT for rare event analysis, the effective sample size for each cluster is often small. Therefore, the MEM may also contribute to improving the predictive accuracy of extreme value analysis. However, to the best of our knowledge, the MEM has not yet been developed in the context of EVT. This motivates us to verify the effectiveness of the MEM in EVT through theoretical studies and numerical experiments, including application to real data for risk assessment of heavy rainfall in Japan.},
   author = {Koki Momoki and Takuma Yoshida},
   journal = {arXiv},
   month = {5},
   title = {Mixed effects models for large sized clustered extremes},
   volume = {2305.05106},
   url = {http://arxiv.org/abs/2305.05106},
   year = {2023},
}
@article{Wand2000,
   abstract = {Regression spline smoothing involves modelling a regression function as a piecewise polynomial with a high number of pieces relative to the sample size. Because the number of possible models is so large, efficient strategies for choosing among them are required. In this paper we review approaches to this problem and compare them through a simulation study. For simplicity and conciseness we restrict attention to the univariate smoothing setting with Gaussian noise and the truncated polynomial regression spline basis.},
   author = {M. P. Wand},
   doi = {10.1007/s001800000047},
   issn = {0943-4062},
   issue = {4},
   journal = {Computational Statistics},
   keywords = {B-spline,Bayesian variable selection,Gibbs sampling,Non-parametric regression,Polynomial spline,Roughness penalty,Stepwise re-gression},
   month = {12},
   pages = {443-462},
   title = {A Comparison of Regression Spline Smoothing Procedures},
   volume = {15},
   url = {http://link.springer.com/10.1007/s001800000047},
   year = {2000},
}
@article{Kauermann2011,
   abstract = {A number of criteria exist to select the penalty in penalized spline regression, but the selection of the number of spline basis functions has received much less attention in the literature. We propose a likelihood-based criterion to select the number of basis functions in penalized spline regression. The criterion is easy to apply and we describe its theoretical and practical properties.},
   author = {G. Kauermann and J. D. Opsomer},
   doi = {10.1093/biomet/asq081},
   issn = {0006-3444},
   issue = {1},
   journal = {Biometrika},
   keywords = {Maximum likelihood,Mixed model,Nonparametric regression,Some key words: Knot selection},
   month = {3},
   pages = {225-230},
   title = {Data-driven selection of the spline dimension in penalized spline regression},
   volume = {98},
   url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/asq081},
   year = {2011},
}
@article{Wood2003,
   abstract = {<p>I discuss the production of low rank smoothers for d ≥ 1 dimensional data, which can be fitted by regression or penalized regression methods. The smoothers are constructed by a simple transformation and truncation of the basis that arises from the solution of the thin plate spline smoothing problem and are optimal in the sense that the truncation is designed to result in the minimum possible perturbation of the thin plate spline smoothing problem given the dimension of the basis used to construct the smoother. By making use of Lanczos iteration the basis change and truncation are computationally efficient. The smoothers allow the use of approximate thin plate spline models with large data sets, avoid the problems that are associated with ‘knot placement’ that usually complicate modelling with regression splines or penalized regression splines, provide a sensible way of modelling interaction terms in generalized additive models, provide low rank approximations to generalized smoothing spline models, appropriate for use with large data sets, provide a means for incorporating smooth functions of more than one variable into non-linear models and improve the computational efficiency of penalized likelihood models incorporating thin plate splines. Given that the approach produces spline-like models with a sparse basis, it also provides a natural way of incorporating unpenalized spline-like terms in linear and generalized linear models, and these can be treated just like any other model terms from the point of view of model selection, inference and diagnostics.</p>},
   author = {Simon N. Wood},
   doi = {10.1111/1467-9868.00374},
   issn = {1369-7412},
   issue = {1},
   journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
   keywords = {Generalized additive model,Regression spline,Thin plate spline},
   month = {2},
   pages = {95-114},
   title = {Thin Plate Regression Splines},
   volume = {65},
   url = {https://academic.oup.com/jrsssb/article/65/1/95/7110632},
   year = {2003},
}
@book{Durrett2012,
   author = {Richard Durrett},
   city = {New York, NY},
   doi = {10.1007/978-1-4614-3615-7},
   isbn = {978-1-4614-3614-0},
   publisher = {Springer New York},
   title = {Essentials of Stochastic Processes},
   url = {http://link.springer.com/10.1007/978-1-4614-3615-7},
   year = {2012},
}
@article{Oliveira2012,
   abstract = {A new plug-in rule procedure for bandwidth selection in kernel circular density estimation is introduced. The performance of this proposal is checked throughout a simulation study considering a variety of circular distributions exhibiting multimodality, peakedness and/or skewness. The plug-in rule behavior is also compared with other existing bandwidth selectors. The method is illustrated with some classical datasets. © 2012 Elsevier B.V. All rights reserved.},
   author = {M. Oliveira and R.M. Crujeiras and A. Rodríguez-Casal},
   doi = {10.1016/j.csda.2012.05.021},
   issn = {01679473},
   issue = {12},
   journal = {Computational Statistics \& Data Analysis},
   keywords = {Bandwidth selection,Circular density,Kernel estimator,Plug-in rule,von Mises distribution},
   month = {12},
   pages = {3898-3908},
   title = {A plug-in rule for bandwidth selection in circular density estimation},
   volume = {56},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0167947312002204},
   year = {2012},
}
@article{García–Portugués2013,
   abstract = {New bandwidth selectors for kernel density estimation with directional data are presented in this work. These selectors are based on asymptotic and exact error expressions for the kernel density estimator combined with mixtures of von Mises distributions. The performance of the proposed selectors is investigated in a simulation study and compared with other existing rules for a large variety of directional scenarios, sample sizes and dimensions. The selector based on the exact error expression turns out to have the best behaviour of the studied selectors for almost all the situations. This selector is illustrated with real data for the circular and spherical cases.},
   author = {Eduardo García–Portugués},
   doi = {10.1214/13-EJS821},
   issn = {1935-7524},
   journal = {Electronic Journal of Statistics},
   keywords = {Bandwidth selection,Directional data,Kernel density estimator,Mixtures,Von Mises},
   month = {1},
   pages = {1655-1685},
   title = {Exact risk improvement of bandwidth selectors for kernel density estimation with directional data},
   volume = {7},
   url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-7/issue-none/Exact-risk-improvement-of-bandwidth-selectors-for-kernel-density-estimation/10.1214/13-EJS821.full},
   year = {2013},
}
@article{Chen2017,
   abstract = {This tutorial provides a gentle introduction to kernel density estimation (KDE) and recent advances regarding confidence bands and geometric/topological features. We begin with a discussion of basic properties of KDE: the convergence rate under various metrics, density derivative estimation, and bandwidth selection. Then, we introduce common approaches to the construction of confidence intervals/bands, and we discuss how to handle bias. Next, we talk about recent advances in the inference of geometric and topological features of a density function using KDE. Finally, we illustrate how one can use KDE to estimate a cumulative distribution function and a receiver operating characteristic curve. We provide R implementations related to this tutorial at the end.},
   author = {Yen-Chi Chen},
   doi = {10.1080/24709360.2017.1396742},
   issn = {2470-9360},
   issue = {1},
   journal = {Biostatistics \& Epidemiology},
   keywords = {Kernel density estimation,bootstrap,confidence bands,nonparametric statistics},
   month = {1},
   pages = {161-187},
   publisher = {Taylor and Francis Ltd.},
   title = {A tutorial on kernel density estimation and recent advances},
   volume = {1},
   url = {https://www.tandfonline.com/doi/full/10.1080/24709360.2017.1396742},
   year = {2017},
}
@article{Heidenreich2013,
   abstract = {On the one hand, kernel density estimation has become a common tool for empirical studies in any research area. This goes hand in hand with the fact that this kind of estimator is now provided by many software packages. On the other hand, since about three decades the discussion on bandwidth selection has been going on. Although a good part of the discussion is about nonparametric regression, this parameter choice is by no means less problematic for density estimation. This becomes obvious when reading empirical studies in which practitioners have made use of kernel densities. New contributions typically provide simulations only to show that the own selector outperforms some of the existing methods. We review existing methods and compare them on a set of designs that exhibit few bumps and exponentially falling tails. We concentrate on small and moderate sample sizes because for large ones the differences between consistent methods are often negligible, at least for practitioners. As a byproduct we find that a mixture of simple plug-in and cross-validation methods produces bandwidths with a quite stable performance. © 2013 Springer-Verlag Berlin Heidelberg.},
   author = {Nils Bastian Heidenreich and Anja Schindler and Stefan Sperlich},
   doi = {10.1007/S10182-013-0216-Y/TABLES/2},
   issn = {18638171},
   issue = {4},
   journal = {AStA Advances in Statistical Analysis},
   keywords = {Bandwidth selection,Kernel density estimation},
   month = {10},
   pages = {403-433},
   publisher = {Springer},
   title = {Bandwidth selection for kernel density estimation: A review of fully automatic selectors},
   volume = {97},
   url = {https://link.springer.com/article/10.1007/s10182-013-0216-y},
   year = {2013},
}
@article{Hall1987,
   author = {Peter Hall and G. S. Watson and Javier Cabrera},
   doi = {10.2307/2336469},
   issn = {00063444},
   issue = {4},
   journal = {Biometrika},
   month = {12},
   pages = {751},
   title = {Kernel Density Estimation with Spherical Data},
   volume = {74},
   url = {https://www.jstor.org/stable/2336469?origin=crossref},
   year = {1987},
}
@article{DiMarzia2011,
   abstract = {Kernel density estimation for multivariate, circular data has been formulated only when the sample space is the sphere, but theory for the torus would also be useful. For data lying on a d-dimensional torus ðd ! 1Þ, we discuss kernel estimation of a density, its mixed partial derivatives, and their squared functionals. We introduce a specific class of product kernels whose order is suitably defined in such a way to obtain L 2-risk formulas whose structure can be compared to their Euclidean counterparts. Our kernels are based on circular densities; however, we also discuss smaller bias estimation involving negative kernels which are functions of circular densities. Practical rules for selecting the smoothing degree, based on cross-validation, bootstrap and plug-in ideas are derived. Moreover, we provide specific results on the use of kernels based on the von Mises density. Finally, real-data examples and simulation studies illustrate the findings.},
   author = {Marco Di Marzio and Agnese Panzera and Charles C. Taylor},
   doi = {10.1016/j.jspi.2011.01.002},
   issn = {03783758},
   issue = {6},
   journal = {Journal of Statistical Planning and Inference},
   keywords = {Circular symmetric unimodal families,Conformation angles,Density functionals,Efficiency,Minimax bounds,Mises density,Mixed derivatives,Sin-order,Toroidal kernels,Twicing von},
   month = {6},
   pages = {2156-2173},
   title = {Kernel density estimation on the torus},
   volume = {141},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S037837581100019X},
   year = {2011},
}
@article{Gnecco2022,
   abstract = {Classical methods for quantile regression fail in cases where the quantile of interest is extreme and only few or no training data points exceed it. Asymptotic results from extreme value theory can be used to extrapolate beyond the range of the data, and several approaches exist that use linear regression, kernel methods or generalized additive models. Most of these methods break down if the predictor space has more than a few dimensions or if the regression function of extreme quantiles is complex. We propose a method for extreme quantile regression that combines the flexibility of random forests with the theory of extrapolation. Our extremal random forest (ERF) estimates the parameters of a generalized Pareto distribution, conditional on the predictor vector, by maximizing a local likelihood with weights extracted from a quantile random forest. We penalize the shape parameter in this likelihood to regularize its variability in the predictor space. Under general domain of attraction conditions, we show consistency of the estimated parameters in both the unpenalized and penalized case. Simulation studies show that our ERF outperforms both classical quantile regression methods and existing regression approaches from extreme value theory. We apply our methodology to extreme quantile prediction for U.S. wage data.},
   author = {Nicola Gnecco and Edossa Merga Terefe and Sebastian Engelke},
   month = {1},
   title = {Extremal Random Forests},
   url = {http://arxiv.org/abs/2201.12865},
   year = {2022},
}
@article{Richards2023,
   abstract = {Making inference with spatial extremal dependence models can be computationally burdensome since they involve intractable and/or censored likelihoods. Building on recent advances in likelihood-free inference with neural Bayes estimators, that is, neural networks that approximate Bayes estimators, we develop highly efficient estimators for censored peaks-over-threshold models that encode censoring information in the neural network architecture. Our new method provides a paradigm shift that challenges traditional censored likelihood-based inference methods for spatial extremal dependence models. Our simulation studies highlight significant gains in both computational and statistical efficiency, relative to competing likelihood-based approaches, when applying our novel estimators to make inference with popular extremal dependence models, such as max-stable, $r$-Pareto, and random scale mixture process models. We also illustrate that it is possible to train a single neural Bayes estimator for a general censoring level, precluding the need to retrain the network when the censoring level is changed. We illustrate the efficacy of our estimators by making fast inference on hundreds-of-thousands of high-dimensional spatial extremal dependence models to assess extreme particulate matter 2.5 microns or less in diameter (PM2.5) concentration over the whole of Saudi Arabia.},
   author = {Jordan Richards and Matthew Sainsbury-Dale and Andrew Zammit-Mangion and Raphaël Huser},
   month = {6},
   title = {Neural Bayes estimators for censored inference with peaks-over-threshold models},
   url = {http://arxiv.org/abs/2306.15642},
   year = {2023},
}
@article{,
   author = {Matthew Sainsbury-Dale and Andrew Zammit-Mangion and Raphaël Huser},
   doi = {10.1080/00031305.2023.2249522},
   issn = {0003-1305},
   journal = {The American Statistician},
   month = {8},
   pages = {1-23},
   title = {Likelihood-Free Parameter Estimation with Neural Bayes Estimators},
   url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2023.2249522},
   year = {2023},
}
@misc{Blumenson1960,
   abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.},
   author = {L E Blumenson},
   isbn = {141.30.70.140},
   issue = {1},
   journal = {Source: The American Mathematical Monthly},
   pages = {63-66},
   title = {A Derivation of n-Dimensional Spherical Coordinates},
   volume = {67},
   year = {1960},
}
@article{Cannon2018,
   abstract = {The goal of quantile regression is to estimate conditional quantiles for specified values of quantile probability using linear or nonlinear regression equations. These estimates are prone to “quantile crossing”, where regression predictions for different quantile probabilities do not increase as probability increases. In the context of the environmental sciences, this could, for example, lead to estimates of the magnitude of a 10-year return period rainstorm that exceed the 20-year storm, or similar nonphysical results. This problem, as well as the potential for overfitting, is exacerbated for small to moderate sample sizes and for nonlinear quantile regression models. As a remedy, this study introduces a novel nonlinear quantile regression model, the monotone composite quantile regression neural network (MCQRNN), that (1) simultaneously estimates multiple non-crossing, nonlinear conditional quantile functions; (2) allows for optional monotonicity, positivity/non-negativity, and generalized additive model constraints; and (3) can be adapted to estimate standard least-squares regression and non-crossing expectile regression functions. First, the MCQRNN model is evaluated on synthetic data from multiple functions and error distributions using Monte Carlo simulations. MCQRNN outperforms the benchmark models, especially for non-normal error distributions. Next, the MCQRNN model is applied to real-world climate data by estimating rainfall Intensity–Duration–Frequency (IDF) curves at locations in Canada. IDF curves summarize the relationship between the intensity and occurrence frequency of extreme rainfall over storm durations ranging from minutes to a day. Because annual maximum rainfall intensity is a non-negative quantity that should increase monotonically as the occurrence frequency and storm duration decrease, monotonicity and non-negativity constraints are key constraints in IDF curve estimation. In comparison to standard QRNN models, the ability of the MCQRNN model to incorporate these constraints, in addition to non-crossing, leads to more robust and realistic estimates of extreme rainfall.},
   author = {Alex J. Cannon},
   doi = {10.1007/s00477-018-1573-6},
   issn = {14363259},
   issue = {11},
   journal = {Stochastic Environmental Research and Risk Assessment},
   month = {11},
   pages = {3207-3225},
   publisher = {Springer New York LLC},
   title = {Non-crossing nonlinear regression quantiles by monotone composite quantile regression neural network, with application to rainfall extremes},
   volume = {32},
   year = {2018},
}
@article{Cannon2011,
   abstract = {The qrnn package for R implements the quantile regression neural network, which is an artificial neural network extension of linear quantile regression. The model formulation follows from previous work on the estimation of censored regression quantiles. The result is a nonparametric, nonlinear model suitable for making probabilistic predictions of mixed discrete-continuous variables like precipitation amounts, wind speeds, or pollutant concentrations, as well as continuous variables. A differentiable approximation to the quantile regression error function is adopted so that gradient-based optimization algorithms can be used to estimate model parameters. Weight penalty and bootstrap aggregation methods are used to avoid overfitting. For convenience, functions for quantile-based probability density, cumulative distribution, and inverse cumulative distribution functions are also provided. Package functions are demonstrated on a simple precipitation downscaling task. © 2010.},
   author = {Alex J. Cannon},
   doi = {10.1016/j.cageo.2010.07.005},
   issn = {00983004},
   issue = {9},
   journal = {Computers and Geosciences},
   keywords = {Artificial neural network,Downscaling,Nonlinear,Nonparametric,Precipitation,Prediction interval,Probabilistic,Quantile,R programming language},
   month = {9},
   pages = {1277-1284},
   title = {Quantile regression neural networks: Implementation in R and application to precipitation downscaling},
   volume = {37},
   year = {2011},
}
@article{Rohrbeck2023,
   author = {Christian Rohrbeck and Emma S Simpson and Jonathan A Tawn},
   journal = {Extremes},
   title = {Editorial: EVA 2023 Data Challenge},
   volume = {(to appear)},
   year = {2023},
}
@article{Opitz2018,
   abstract = {This work is motivated by the challenge organized for the 10th International Conference on Extreme-Value Analysis (EVA2017) to predict daily precipitation quantiles at the 99.8 % level for each month at observed and unobserved locations. Our approach is based on a Bayesian generalized additive modeling framework that is designed to estimate complex trends in marginal extremes over space and time. First, we estimate a high non-stationary threshold using a gamma distribution for precipitation intensities that incorporates spatial and temporal random effects. Then, we use the Bernoulli and generalized Pareto (GP) distributions to model the rate and size of threshold exceedances, respectively, which we also assume to vary in space and time. The latent random effects are modeled additively using Gaussian process priors, which provide high flexibility and interpretability. We develop a penalized complexity (PC) prior specification for the tail index that shrinks the GP model towards the exponential distribution, thus preventing unrealistically heavy tails. Fast and accurate estimation of the posterior distributions is performed thanks to the integrated nested Laplace approximation (INLA). We illustrate this methodology by modeling the daily precipitation data provided by the EVA2017 challenge, which consist of observations from 40 stations in the Netherlands recorded during the period 1972–2016. Capitalizing on INLA’s fast computational capacity and powerful distributed computing resources, we conduct an extensive cross-validation study to select the model parameters that govern the smoothness of trends. Our results clearly outperform simple benchmarks and are comparable to the best-scoring approaches of the other teams.},
   author = {Thomas Opitz and Raphaël Huser and Haakon Bakka and Håvard Rue},
   doi = {10.1007/s10687-018-0324-x},
   issn = {1572915X},
   issue = {3},
   journal = {Extremes},
   keywords = {62E20,62M30,62P12,Bayesian hierarchical modeling,Extreme-Value Analysis Conference challenge,Extreme-Value Theory,Generalized Pareto distribution,High quantile estimation,Integrated nested Laplace approximation (INLA)},
   month = {9},
   pages = {441-462},
   publisher = {Springer Science+Business Media B.V.},
   title = {INLA goes extreme: Bayesian tail regression for the estimation of high spatio-temporal quantiles},
   volume = {21},
   year = {2018},
}
@article{Krock2022b,
   abstract = {In traditional extreme value analysis, the bulk of the data is ignored, and only the tails of the distribution are used for inference. Extreme observations are specified as values that exceed a threshold or as maximum values over distinct blocks of time, and subsequent estimation procedures are motivated by asymptotic theory for extremes of random processes. For environmental data, nonstationary behavior in the bulk of the distribution, such as seasonality or climate change, will also be observed in the tails. To accurately model such nonstationarity, it seems natural to use the entire dataset rather than just the most extreme values. It is also common to observe different types of nonstationarity in each tail of a distribution. Most work on extremes only focuses on one tail of a distribution, but for temperature, both tails are of interest. This paper builds on a recently proposed parametric model for the entire probability distribution that has flexible behavior in both tails. We apply an extension of this model to historical records of daily mean temperature at several locations across the United States with different climates and local conditions. We highlight the ability of the method to quantify changes in the bulk and tails across the year over the past decades and under different geographic and climatic conditions. The proposed model shows good performance when compared to several benchmark models that are typically used in extreme value analysis of temperature.},
   author = {Mitchell Krock and Julie Bessac and Michael L. Stein and Adam H. Monahan},
   doi = {10.1016/j.wace.2022.100438},
   issn = {22120947},
   journal = {Weather and Climate Extremes},
   keywords = {Bulk and tails,Climate change,Nonstationary,Temperature extremes},
   month = {6},
   publisher = {Elsevier B.V.},
   title = {Nonstationary seasonal model for daily mean temperature distribution bridging bulk and tails},
   volume = {36},
   year = {2022},
}
@article{Carrer2022,
   abstract = {The Extended Generalized Pareto Distribution (EGPD) (Naveau et al. 2016) is a family of distribution that has been introduced to model the full range of a positive random variable but with the lower and the upper tails distributed according to the peaks-over-threshold methodology. The aim of this article is to augment the scope of application of EGPD allowing the analyst to incorporate the effect of covariates on the model. In particular we introduce a specification where the parameters of EGPD can be modeled as additive functions of the covariates, e.g. space or time. As a related product we provide an add-on code written in R that it is flexible enough to implement the EGPD in a generic way, allowing to introduce new parametric forms. We show the potential of our add-on on the modeling of hourly rainfalls over the North-West region of France and discuss modeling strategies.},
   author = {Noémie Le Carrer and Carlo Gaetan},
   journal = {arXiv},
   month = {9},
   title = {Distributional regression models for Extended Generalized Pareto distributions},
   volume = {2209.04660},
   url = {http://arxiv.org/abs/2209.04660},
   year = {2022},
}
@article{deHaan2021,
   abstract = {We consider extreme value analysis for independent but nonidentically distributed observations. In particular, the observations do not share the same extreme value index. Assuming continuously changing extreme value indices, we provide a nonparametric estimate for the functional extreme value index. Besides estimating the extreme value index locally, we also provide a global estimator for the trend and its joint asymptotic theory. The asymptotic theory for the global estimator can be used for testing a prespecified parametric trend in the extreme value indices. In particular, it can be applied to test whether the extreme value index remains at a constant level across all observations.},
   author = {Laurens de Haan and Chen Zhou},
   doi = {10.1080/01621459.2019.1705307},
   issn = {1537274X},
   issue = {535},
   journal = {Journal of the American Statistical Association},
   keywords = {Hill estimator,Local and global estimation,Peak over threshold,Stochastic integral},
   pages = {1265-1279},
   publisher = {American Statistical Association},
   title = {Trends in Extreme Value Indices},
   volume = {116},
   year = {2021},
}
@article{,
   abstract = {Neural Bayes estimators are neural networks that approximate Bayes estimators in a fast and likelihood-free manner. They are appealing to use with spatial models and data, where estimation is often a computational bottleneck. However, neural Bayes estimators in spatial applications have, to date, been restricted to data collected over a regular grid. These estimators are also currently dependent on a prescribed set of spatial locations, which means that the neural network needs to be re-trained for new data sets; this renders them impractical in many applications and impedes their widespread adoption. In this work, we employ graph neural networks to tackle the important problem of parameter estimation from data collected over arbitrary spatial locations. In addition to extending neural Bayes estimation to irregular spatial data, our architecture leads to substantial computational benefits, since the estimator can be used with any arrangement or number of locations and independent replicates, thus amortising the cost of training for a given spatial model. We also facilitate fast uncertainty quantification by training an accompanying neural Bayes estimator that approximates a set of marginal posterior quantiles. We illustrate our methodology on Gaussian and max-stable processes. Finally, we showcase our methodology in a global sea-surface temperature application, where we estimate the parameters of a Gaussian process model in 2,161 regions, each containing thousands of irregularly-spaced data points, in just a few minutes with a single graphics processing unit.},
   author = {Matthew Sainsbury-Dale and Jordan Richards and Andrew Zammit-Mangion and Raphaël Huser},
   month = {10},
   title = {Neural Bayes Estimators for Irregular Spatial Data using Graph Neural Networks},
   url = {http://arxiv.org/abs/2310.02600},
   year = {2023},
}
@book{Khoshnevisan2016,
   author = {Davar Khoshnevisan and René Schilling},
   city = {Cham},
   doi = {10.1007/978-3-319-34120-0},
   editor = {Frederic Utzet and Lluis Quer-Sardanyons},
   isbn = {978-3-319-34119-4},
   publisher = {Springer International Publishing},
   title = {From Lévy-Type Processes to Parabolic SPDEs},
   url = {http://link.springer.com/10.1007/978-3-319-34120-0},
   year = {2016},
}
@article{Cotsakis2023,
   abstract = {We introduce the extremal range, a local statistic for studying the spatial extent of extreme events in random fields on $\mathbb\{R\}^2$. Conditioned on exceedance of a high threshold at a location $s$, the extremal range at $s$ is the random variable defined as the smallest distance from $s$ to a location where there is a non-exceedance. We leverage tools from excursion-set theory to study distributional properties of the extremal range, propose parametric models and predict the median extremal range at extreme threshold levels. The extremal range captures the rate at which the spatial extent of conditional extreme events scales for increasingly high thresholds, and we relate its distributional properties with the bivariate tail dependence coefficient and the extremal index of time series in classical Extreme-Value Theory. Consistent estimation of the distribution function of the extremal range for stationary random fields is proven. For non-stationary random fields, we implement generalized additive median regression to predict extremal-range maps at very high threshold levels. An application to two large daily temperature datasets, namely reanalyses and climate-model simulations for France, highlights decreasing extremal dependence for increasing threshold levels and reveals strong differences in joint tail decay rates between reanalyses and simulations.},
   author = {Ryan Cotsakis and Elena Di Bernardino and Thomas Opitz},
   month = {10},
   title = {A local statistic for the spatial extent of extreme threshold exceedances},
   url = {http://arxiv.org/abs/2310.09075},
   year = {2023},
}
@article{Behme2021,
   abstract = {In this article, a special case of two coupled M/G/1-queues is considered, where two servers are exposed to two types of jobs that are distributed among the servers via a random switch. In this model, the asymptotic behavior of the workload buffer exceedance probabilities for the two single servers/both servers together/one (unspecified) server is determined. Hereby, one has to distinguish between jobs that are either heavy-tailed or light-tailed. The results are derived via the dual risk model of the studied coupled M/G/1-queues for which the asymptotic behavior of different ruin probabilities is determined.},
   author = {Anita Behme and Philipp Lukas Strietzel},
   doi = {10.1007/s11134-021-09697-9},
   issn = {15729443},
   issue = {1-2},
   journal = {Queueing Systems},
   keywords = {Bipartite network,Bivariate compound Poisson process,Coupled M/G/1-queues,Hitting probability,Queueing theory,Random switch,Regular variation,Ruin theory},
   month = {10},
   pages = {27-64},
   publisher = {Springer},
   title = {A 2×2 random switching model and its dual risk model},
   volume = {99},
   year = {2021},
}
@misc{Watson2016,
   abstract = {This course is an introduction to the theory of Lévy processes. It covers definitions, the Lévy-Itô decomposition, and some early results in the direction of fluctuation theory. 1 Lévy processes Let us begin by recalling the definition of two familiar processes, a Brownian motion and a Poisson process. A real-valued process B = \{B t : t ≥ 0\} defined on a probability space (Ω, F, P) is said to be a Brownian motion if the following hold: • The paths of B are P-almost surely continuous. • P(B 0 = 0) = 1. • For 0 ≤ s ≤ t, B t − B s is equal in distribution to B t−s. • For 0 ≤ s ≤ t, B t − B s is independent of \{B u : u ≤ s\}. • For each t > 0, B t is equal in distribution to a normal random variable with variance t. A process valued on the non-negative integers N = \{N t : t ≥ 0\}, defined on a probability space (Ω, F, P), is said to be a Poisson process with intensity λ > 0 if the following hold: • The paths of N are P-almost surely right continuous with left limits. • P(N 0 = 0) = 1. • For 0 ≤ s ≤ t, N t − N s is equal in distribution to N t−s. • For 0 ≤ s ≤ t, N t − N s is independent of \{N u : u ≤ s\}.},
   author = {Alex Watson},
   title = {Introduction to Lévy processes},
   year = {2016},
}
@article{Samorodnitsky2016,
   abstract = {We propose a new definition of a multivariate subexponential distribution. We compare this definition with the two existing notions of multivariate subexponentiality, and compute the asymptotic behaviour of the ruin probability in the context of an insurance portfolio, when multivariate subexponentiality holds. Previously such results were available only in the case of multivariate regularly varying claims.},
   author = {Gennady Samorodnitsky and Julian Sun},
   doi = {10.1007/s10687-016-0242-8},
   issn = {1386-1999},
   issue = {2},
   journal = {Extremes},
   keywords = {Heavy tails,Insurance portfolio,Multivariate,Regular variation,Ruin probability,Subexponential distribution},
   month = {6},
   pages = {171-196},
   publisher = {Springer Science+Business Media B.V.},
   title = {Multivariate subexponential distributions and their applications},
   volume = {19},
   url = {http://link.springer.com/10.1007/s10687-016-0242-8},
   year = {2016},
}
@book{LaRiccia2009,
   author = {Vincent N. LaRiccia and Paul P. Eggermont},
   city = {New York, NY},
   doi = {10.1007/b12285},
   isbn = {978-0-387-40267-3},
   publisher = {Springer New York},
   title = {Maximum Penalized Likelihood Estimation},
   year = {2009},
}
@article{Mackay2023,
   abstract = {We present a new framework for modelling multivariate extremes, based on an angular-radial representation of the probability density function. Under this representation, the problem of modelling multivariate extremes is transformed to that of modelling an angular density and the tail of the radial variable, conditional on angle. Motivated by univariate theory, we assume that the tail of the conditional radial distribution converges to a generalised Pareto (GP) distribution. To simplify inference, we also assume that the angular density is continuous and finite and the GP parameter functions are continuous with angle. We refer to the resulting model as the semi-parametric angular-radial (SPAR) model for multivariate extremes. We consider the effect of the choice of polar coordinate system and introduce generalised concepts of angular-radial coordinate systems and generalised scalar angles in two dimensions. We show that under certain conditions, the choice of polar coordinate system does not affect the validity of the SPAR assumptions. However, some choices of coordinate system lead to simpler representations. In contrast, we show that the choice of margin does affect whether the model assumptions are satisfied. In particular, the use of Laplace margins results in a form of the density function for which the SPAR assumptions are satisfied for many common families of copula, with various dependence classes. We show that the SPAR model provides a more versatile framework for characterising multivariate extremes than provided by existing approaches, and that several commonly-used approaches are special cases of the SPAR model. Moreover, the SPAR framework provides a means of characterising all `extreme regions' of a joint distribution using a single inference. Applications in which this is useful are discussed.},
   author = {Ed Mackay and Philip Jonathan},
   journal = {arXiv},
   month = {10},
   title = {Modelling multivariate extremes through angular-radial decomposition of the density function},
   volume = {2310.12711},
   url = {http://arxiv.org/abs/2310.12711},
   year = {2023},
}
@article{Murphy2023,
   abstract = {Threshold selection is a fundamental problem in any threshold-based extreme value analysis. While models are asymptotically motivated, selecting an appropriate threshold for finite samples can be difficult through standard methods. Inference can also be highly sensitive to the choice of threshold. Too low a threshold choice leads to bias in the fit of the extreme value model, while too high a choice leads to unnecessary additional uncertainty in the estimation of model parameters. In this paper, we develop a novel methodology for automated threshold selection that directly tackles this bias-variance trade-off. We also develop a method to account for the uncertainty in this threshold choice and propagate this uncertainty through to high quantile inference. Through a simulation study, we demonstrate the effectiveness of our method for threshold selection and subsequent extreme quantile estimation. We apply our method to the well-known, troublesome example of the River Nidd dataset.},
   author = {Conor Murphy and Jonathan A. Tawn and Zak Varty},
   journal = {arXiv},
   month = {10},
   title = {Automated threshold selection and associated inference uncertainty for univariate extremes},
   volume = {2310.17999},
   url = {http://arxiv.org/abs/2310.17999},
   year = {2024},
}
@article{,
   abstract = {<p>This paper details a methodology proposed for the EVA 2021 conference data challenge. The aim of this challenge was to predict the number and size of wildfires over the contiguous US between 1993 and 2015, with more importance placed on extreme events. In the data set provided, over 14% of both wildfire count and burnt area observations are missing; the objective of the data challenge was to estimate a range of marginal probabilities from the distribution functions of these missing observations. To enable this prediction, we make the assumption that the marginal distribution of a missing observation can be informed using non-missing data from neighbouring locations. In our method, we select spatial neighbourhoods for each missing observation and fit marginal models to non-missing observations in these regions. For the wildfire counts, we assume the compiled data sets follow a zero-inflated negative binomial distribution, while for burnt area values, we model the bulk and tail of each compiled data set using non-parametric and parametric techniques, respectively. Cross validation is used to select tuning parameters, and the resulting predictions are shown to significantly outperform the benchmark method proposed in the challenge outline. We conclude with a discussion of our modelling framework, and evaluate ways in which it could be extended.</p>},
   author = {Eleanor D’Arcy and Callum J. R. Murphy-Barltrop and Rob Shooter and Emma S. Simpson},
   doi = {10.1007/s10687-023-00469-7},
   issn = {1386-1999},
   issue = {2},
   journal = {Extremes},
   keywords = {Extreme value theory,Semi-parametric modelling,Wildfire prediction},
   month = {6},
   pages = {381-398},
   publisher = {Springer},
   title = {A marginal modelling approach for predicting wildfire extremes across the contiguous United States},
   volume = {26},
   url = {https://link.springer.com/10.1007/s10687-023-00469-7},
   year = {2023},
}
@article{Zhong2023,
   abstract = {Significant advancements in the development of machine learning (ML) models for weather forecasting have produced remarkable results. State-of-the-art ML-based weather forecast models, such as FuXi, have demonstrated superior statistical forecast performance in comparison to the high-resolution forecasts (HRES) of the European Centre for Medium-Range Weather Forecasts (ECMWF). However, ML models face a common challenge: as forecast lead times increase, they tend to generate increasingly smooth predictions, leading to an underestimation of the intensity of extreme weather events. To address this challenge, we developed the FuXi-Extreme model, which employs a denoising diffusion probabilistic model (DDPM) to restore finer-scale details in the surface forecast data generated by the FuXi model in 5-day forecasts. An evaluation of extreme total precipitation ($\textrm\{TP\}$), 10-meter wind speed ($\textrm\{WS10\}$), and 2-meter temperature ($\textrm\{T2M\}$) illustrates the superior performance of FuXi-Extreme over both FuXi and HRES. Moreover, when evaluating tropical cyclone (TC) forecasts based on International Best Track Archive for Climate Stewardship (IBTrACS) dataset, both FuXi and FuXi-Extreme shows superior performance in TC track forecasts compared to HRES, but they show inferior performance in TC intensity forecasts in comparison to HRES.},
   author = {Xiaohui Zhong and Lei Chen and Jun Liu and Chensen Lin and Yuan Qi and Hao Li},
   month = {10},
   title = {FuXi-Extreme: Improving extreme rainfall and wind forecasts with diffusion model},
   url = {http://arxiv.org/abs/2310.19822},
   year = {2023},
}
@article{Zhang2023,
   abstract = {Moving average processes driven by exponential-tailed L\'evy noise are important extensions of their Gaussian counterparts in order to capture deviations from Gaussianity, more flexible dependence structures, and sample paths with jumps. Popular examples include non-Gaussian Ornstein--Uhlenbeck processes and type G Mat\'ern stochastic partial differential equation random fields. This paper is concerned with the open problem of determining their extremal dependence structure. We leverage the fact that such processes admit approximations on grids or triangulations that are used in practice for efficient simulations and inference. These approximations can be expressed as special cases of a class of linear transformations of independent, exponential-tailed random variables, that bridge asymptotic dependence and independence in a novel, tractable way. This result is of independent interest since models that can capture both extremal dependence regimes are scarce and the construction of such flexible models is an active area of research. This new fundamental result allows us to show that the integral approximation of general moving average processes with exponential-tailed L\'evy noise is asymptotically independent when the mesh is fine enough. Under mild assumptions on the kernel function we also derive the limiting residual tail dependence function. For the popular exponential-tailed Ornstein--Uhlenbeck process we prove that it is asymptotically independent, but with a different residual tail dependence function than its Gaussian counterpart. Our results are illustrated through simulation studies.},
   author = {Zhongwei Zhang and David Bolin and Sebastian Engelke and Raphaël Huser},
   month = {7},
   title = {Extremal Dependence of Moving Average Processes Driven by Exponential-Tailed L\'evy Noise},
   url = {http://arxiv.org/abs/2307.15796},
   year = {2023},
}
@article{Hickling2023,
   abstract = {We propose a transformation capable of altering the tail properties of a distribution, motivated by extreme value theory, which can be used as a layer in a normalizing flow to approximate multivariate heavy tailed distributions. We apply this approach to model financial returns, capturing potentially extreme shocks that arise in such data. The trained models can be used directly to generate new synthetic sets of potentially extreme returns},
   author = {Tennessee Hickling and Dennis Prangle},
   month = {11},
   title = {Flexible Tails for Normalising Flows, with Application to the Modelling of Financial Return Data},
   url = {http://arxiv.org/abs/2311.00580},
   year = {2023},
}
@article{Padoan2023,
   abstract = {In many applied fields it is desired to make predictions with the aim of assessing the plausibility of more severe events than those already recorded to safeguard against calamities that have not yet occurred. This problem can be analysed using extreme value theory. We consider the popular peaks over a threshold method and show that the generalised Pareto approximation of the true predictive densities of both a future unobservable excess or peak random variable can be very accurate. We propose both a frequentist and a Bayesian approach for the estimation of such predictive densities. We show the asymptotic accuracy of the corresponding estimators and, more importantly, prove that the resulting predictive inference is asymptotically reliable. We show the utility of the proposed predictive tools analysing extreme temperatures in Milan in Italy.},
   author = {S. A. Padoan and Stefano Rizzelli},
   month = {11},
   title = {Statistical Prediction of Peaks Over a Threshold},
   url = {http://arxiv.org/abs/2311.11852},
   year = {2023},
}
@article{Boulin2023,
   abstract = {The task of simplifying the complex spatio-temporal variables associated with climate modeling is of utmost importance and comes with significant challenges. In this research, our primary objective is to tailor clustering techniques to handle compound extreme events within gridded climate data across Europe. Specifically, we intend to identify subregions that display asymptotic independence concerning compound precipitation and wind speed extremes. To achieve this, we utilise daily precipitation sums and daily maximum wind speed data derived from the ERA5 reanalysis dataset spanning from 1979 to 2022. Our approach hinges on a tuning parameter and the application of a divergence measure to spotlight disparities in extremal dependence structures without relying on specific parametric assumptions. We propose a data-driven approach to determine the tuning parameter. This enables us to generate clusters that are spatially concentrated, which can provide more insightful information about the regional distribution of compound precipitation and wind speed extremes. In the process, we aim to elucidate the respective roles of extreme precipitation and wind speed in the resulting clusters. The proposed method is able to extract valuable information about extreme compound events while also significantly reducing the size of the dataset within reasonable computational timeframes.},
   author = {Alexis Boulin and Elena Di Bernardino and Thomas Laloë and Gwladys Toulemonde},
   month = {11},
   title = {Identifying regions of concomitant compound precipitation and wind speed extremes over Europe},
   url = {http://arxiv.org/abs/2311.11292},
   year = {2023},
}
@article{Nolde2021,
   abstract = {<p>This article reviews methods from extreme value analysis with applications to risk assessment in finance. It covers three main methodological paradigms: the classical framework for independent and identically distributed data with application to risk estimation for market and operational loss data, the multivariate framework for cross-sectional dependent data with application to systemic risk, and the methods for stationary serially dependent data applied to dynamic risk management. The article is addressed to statisticians with interest and possibly experience in financial risk management who are not familiar with extreme value analysis.</p>},
   author = {Natalia Nolde and Chen Zhou},
   doi = {10.1146/annurev-statistics-042720-015705},
   issn = {2326-8298},
   issue = {1},
   journal = {Annual Review of Statistics and Its Application},
   keywords = {asymptotic dependence,heavy tails,market risk,operational risk,systemic risk},
   month = {3},
   pages = {217-240},
   title = {Extreme Value Analysis for Financial Risk Management},
   volume = {8},
   url = {https://www.annualreviews.org/doi/10.1146/annurev-statistics-042720-015705},
   year = {2021},
}
@article{Majumder2023,
   abstract = {We propose a model to flexibly estimate joint tail properties by exploiting the convergence of an appropriately scaled point cloud onto a compact limit set. Characteristics of the shape of the limit set correspond to key tail dependence properties. We directly model the shape of the limit set using Bezier splines, which allow flexible and parsimonious specification of shapes in two dimensions. We then fit the Bezier splines to data in pseudo-polar coordinates using Markov chain Monte Carlo, utilizing a limiting approximation to the conditional likelihood of the radii given angles. By imposing appropriate constraints on the parameters of the Bezier splines, we guarantee that each posterior sample is a valid limit set boundary, allowing direct posterior analysis of any quantity derived from the shape of the curve. Furthermore, we obtain interpretable inference on the asymptotic dependence class by using mixture priors with point masses on the corner of the unit box. Finally, we apply our model to bivariate datasets of extremes of variables related to fire risk and air pollution.},
   author = {Reetam Majumder and Benjamin A. Shaby and Brian J. Reich and Daniel Cooley},
   journal = {arXiv},
   month = {6},
   title = {Semiparametric Estimation of the Shape of the Limiting Multivariate Point Cloud},
   volume = {2306.13257},
   url = {http://arxiv.org/abs/2306.13257},
   year = {2023},
}
@article{Engelke2021,
   abstract = {<p>Extreme value statistics provides accurate estimates for the small occurrence probabilities of rare events. While theory and statistical tools for univariate extremes are well developed, methods for high-dimensional and complex data sets are still scarce. Appropriate notions of sparsity and connections to other fields such as machine learning, graphical models, and high-dimensional statistics have only recently been established. This article reviews the new domain of research concerned with the detection and modeling of sparse patterns in rare events. We first describe the different forms of extremal dependence that can arise between the largest observations of a multivariate random vector. We then discuss the current research topics, including clustering, principal component analysis, and graphical modeling for extremes. Identification of groups of variables that can be concomitantly extreme is also addressed. The methods are illustrated with an application to flood risk assessment.</p>},
   author = {Sebastian Engelke and Jevgenijs Ivanovs},
   doi = {10.1146/annurev-statistics-040620-041554},
   issn = {2326-8298},
   issue = {1},
   journal = {Annual Review of Statistics and Its Application},
   keywords = {conditional independence,dimension reduction,extremal graphical models,extreme value theory,sparsity},
   month = {3},
   pages = {241-270},
   title = {Sparse Structures for Multivariate Extremes},
   volume = {8},
   url = {https://www.annualreviews.org/doi/10.1146/annurev-statistics-040620-041554},
   year = {2021},
}
@article{Schorlepp2023,
   abstract = {We introduce and compare computational techniques for sharp extreme event probability estimates in stochastic differential equations with small additive Gaussian noise. In particular, we focus on strategies that are scalable, i.e. their efficiency does not degrade upon temporal and possibly spatial refinement. For that purpose, we extend algorithms based on the Laplace method for estimating the probability of an extreme event to infinite dimensional path space. The method estimates the limiting exponential scaling using a single realization of the random variable, the large deviation minimizer. Finding this minimizer amounts to solving an optimization problem governed by a differential equation. The probability estimate becomes sharp when it additionally includes prefactor information, which necessitates computing the determinant of a second derivative operator to evaluate a Gaussian integral around the minimizer. We present an approach in infinite dimensions based on Fredholm determinants, and develop numerical algorithms to compute these determinants efficiently for the high-dimensional systems that arise upon discretization. We also give an interpretation of this approach using Gaussian process covariances and transition tubes. An example model problem, for which we provide an open-source python implementation, is used throughout the paper to illustrate all methods discussed. To study the performance of the methods, we consider examples of stochastic differential and stochastic partial differential equations, including the randomly forced incompressible three-dimensional Navier-Stokes equations.},
   author = {Timo Schorlepp and Shanyin Tong and Tobias Grafke and Georg Stadler},
   doi = {10.1007/s11222-023-10307-2},
   month = {3},
   title = {Scalable Methods for Computing Sharp Extreme Event Probabilities in Infinite-Dimensional Stochastic Systems},
   url = {http://arxiv.org/abs/2303.11919 http://dx.doi.org/10.1007/s11222-023-10307-2},
   year = {2023},
}
@article{Li2023,
   abstract = {Extreme quantiles are critical for understanding the behavior of data in the tail region of a distribution. It is challenging to estimate extreme quantiles, particularly when dealing with limited data in the tail. In such cases, extreme value theory offers a solution by approximating the tail distribution using the Generalized Pareto Distribution (GPD). This allows for the extrapolation beyond the range of observed data, making it a valuable tool for various applications. However, when it comes to conditional cases, where estimation relies on covariates, existing methods may require computationally expensive GPD fitting for different observations. This computational burden becomes even more problematic as the volume of observations increases, sometimes approaching infinity. To address this issue, we propose an interpolation-based algorithm named EMI. EMI facilitates the online prediction of extreme conditional quantiles with finite offline observations. Combining quantile regression and GPD-based extrapolation, EMI formulates as a bilevel programming problem, efficiently solvable using classic optimization methods. Once estimates for offline observations are obtained, EMI employs B-spline interpolation for covariate-dependent variables, enabling estimation for online observations with finite GPD fitting. Simulations and real data analysis demonstrate the effectiveness of EMI across various scenarios.},
   author = {Zhengpin Li and Jian Wang and Yanxi Hou},
   month = {11},
   title = {Online Prediction of Extreme Conditional Quantiles via B-Spline Interpolation},
   url = {http://arxiv.org/abs/2311.13825},
   year = {2023},
}
@article{Marsaglia1972,
   author = {George Marsaglia},
   doi = {10.1214/aoms/1177692644},
   issn = {0003-4851},
   issue = {2},
   journal = {The Annals of Mathematical Statistics},
   month = {4},
   pages = {645-646},
   title = {Choosing a Point from the Surface of a Sphere},
   volume = {43},
   url = {http://projecteuclid.org/euclid.aoms/1177692644},
   year = {1972},
}
@article{Wadsworth2023,
    author = {Wadsworth, Jennifer L and Campbell, Ryan},
    title = "{Statistical inference for multivariate extremes via a geometric approach}",
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    pages = {qkae030},
    year = {2024},
    month = {03},
    abstract = "{A geometric representation for multivariate extremes, based on the shapes of scaled sample clouds in light-tailed margins and their so-called limit sets, has recently been shown to connect several existing extremal dependence concepts. However, these results are purely probabilistic, and the geometric approach itself has not been fully exploited for statistical inference. We outline a method for parametric estimation of the limit set shape, which includes a useful non-/semi-parametric estimate as a pre-processing step. More fundamentally, our approach provides a new class of asymptotically motivated statistical models for the tails of multivariate distributions, and such models can accommodate any combination of simultaneous or non-simultaneous extremes through appropriate parametric forms for the limit set shape. Extrapolation further into the tail of the distribution is possible via simulation from the fitted model. A simulation study confirms that our methodology is very competitive with existing approaches and can successfully allow estimation of small probabilities in regions where other methods struggle. We apply the methodology to two environmental datasets, with diagnostics demonstrating a good fit.}",
    issn = {1369-7412},
    doi = {10.1093/jrsssb/qkae030},
    url = {https://doi.org/10.1093/jrsssb/qkae030},
    eprint = {https://academic.oup.com/jrsssb/advance-article-pdf/doi/10.1093/jrsssb/qkae030/57115817/qkae030.pdf},
}




@article{Gu1993,
   author = {Chong Gu},
   doi = {10.1080/01621459.1993.10476300},
   issn = {0162-1459},
   issue = {422},
   journal = {Journal of the American Statistical Association},
   month = {6},
   pages = {495-504},
   title = {Smoothing Spline Density Estimation: A Dimensionless Automatic Algorithm},
   volume = {88},
   year = {1993},
}
@article{Kiriliouk2023,
   abstract = {Regular vine sequences permit the organisation of variables in a random vector along a sequence of trees. Regular vine models have become greatly popular in dependence modelling as a way to combine arbitrary bivariate copulas into higher-dimensional ones, offering flexibility, parsimony, and tractability. In this project, we use regular vine structures to decompose and construct the exponent measure density of a multivariate extreme value distribution, or, equivalently, the tail copula density. Although these densities pose theoretical challenges due to their infinite mass, their homogeneity property offers simplifications. The theory sheds new light on existing parametric families and facilitates the construction of new ones, called X-vines. Computations proceed via recursive formulas in terms of bivariate model components. We develop simulation algorithms for X-vine multivariate Pareto distributions as well as methods for parameter estimation and model selection on the basis of threshold exceedances. The methods are illustrated by Monte Carlo experiments and a case study on US flight delay data.},
   author = {Anna Kiriliouk and Jeongjin Lee and Johan Segers},
   month = {12},
   title = {X-Vine Models for Multivariate Extremes},
   url = {http://arxiv.org/abs/2312.15205},
   year = {2023},
}
@article{Randell2016,
   abstract = {<p>We propose a simple piecewise model for a sample of peaks‐over‐threshold, nonstationary with respect to multidimensional covariates, and estimate it using a carefully designed and computationally efficient Bayesian inference. Model parameters are themselves parameterized as functions of covariates using penalized B‐spline representations. This allows detailed characterization of non‐stationarity extreme environments. The approach gives similar inferences to a comparable frequentist penalized maximum likelihood method, but is computationally considerably more efficient and allows a more complete characterization of uncertainty in a single modelling step. We use the model to quantify the joint directional and seasonal variation of storm peak significant wave height at a northern North Sea location and estimate predictive directional–seasonal return value distributions necessary for the design and reliability assessment of marine and coastal structures.</p>},
   author = {D. Randell and K. Turnbull and K. Ewans and P. Jonathan},
   doi = {10.1002/env.2403},
   issn = {1180-4009},
   issue = {7},
   journal = {Environmetrics},
   month = {11},
   pages = {439-450},
   title = {Bayesian inference for nonstationary marginal extremes},
   volume = {27},
   year = {2016},
}
@article{Higham1988,
   abstract = {The nearest symmetric positive senidefbite matrix in the Frobenius norm to an arbitrary real matrix A is shown to be (B + H)/2, where H is the symmetric p&r factor of B = (A + AT)/% In the e-norm a nearest symmetric positive semidefinite matrix, and its distance &s(A) from A, are given by a cc$mputationaUy &akuging formuia due to Halmos. We show how the bisection method can be applied to this bml.4a to compute upper and lower bout& for 8s(A) d&ring by no more tk a given amount. A key @rexBent is a stable and efficient test for positive definiteness, based on an attempted Choleslci decomposition. For accurate computation of &(A) we fommlate the problem as one of zero fin&g aud apply a hybrid Newton-bisection algorithm. Some numerical diffkukes are dkussed and inustrated by example.},
   author = {Nicholas J. Higham},
   doi = {10.1016/0024-3795(88)90223-6},
   issn = {00243795},
   journal = {Linear Algebra and its Applications},
   month = {5},
   pages = {103-118},
   title = {Computing a nearest symmetric positive semidefinite matrix},
   volume = {103},
   url = {https://linkinghub.elsevier.com/retrieve/pii/0024379588902236},
   year = {1988},
}
@article{Jonathan2013,
   abstract = {We review aspects of extreme value modelling relevant to characterisation of ocean environments and the design of marine structures, summarising basic concepts, modelling with covariates and multivariate modelling (including conditional and spatial extremes). We outline Bayesian inference for extremes and reference software resources for extreme value modelling. Extreme value analysis is inherently different to other empirical modelling, in that estimating the tail (rather than the body) of a distribution from a sample of data, and extrapolation beyond the sample (rather than interpolation within) is demanded. Intuition accumulated from other areas of empirical modelling can be misleading. Careful consideration of the effects of sample size, measurement scale, threshold selection and serial dependence, associated uncertainties and implications of choices made is essential. Incorporation of covariate effects when necessary improves inference. Suitable tools (e.g. based on additive models, splines, random fields, spatial processes) have been developed, but their use is restricted in general to academia. Effective modelling of multivariate extremes will improve the specification of design conditions for systems whose response cannot be easily characterised in terms of one variable. Approaches such as the conditional extremes model are easily implemented, and provide generalisations of existing marine design approaches (e.g. for primary and associated variables). Software is available, but again generally only for academic use. Modelling spatial dependence rigourously will provide single extreme value models applicable to spatial neighbourhoods including complete ocean basins, avoiding the need for procedures such as site pooling. Indeed, once the model is established, the metocean engineer may not ever need to perform further extreme value analysis for that basin in principle. Spatial extremes is an area of active research in the statistics community. A limited number of appropriate models have been deployed (e.g. for precipitation, temperature and metocean applications). Software is available, but again for specialist use. Bayesian inference provides a consistent framework for inference and is rapidly becoming the standard approach in academia. It appears inevitable that, in time, Bayesian inference will also be regarded as the standard in ocean engineering applications. Implementation of Bayesian methods requires some expertise. Software is available, but again generally only used by statistical specialists. © 2013 Elsevier Ltd.},
   author = {Philip Jonathan and Kevin Ewans},
   doi = {10.1016/j.oceaneng.2013.01.004},
   issn = {00298018},
   journal = {Ocean Engineering},
   keywords = {Bayesian,Covariate,Extreme,Multivariate,Return value,Review},
   month = {4},
   pages = {91-109},
   title = {Statistical modelling of extreme ocean environments for marine design: A review},
   volume = {62},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S002980181300019X},
   year = {2013},
}
@article{Towe2023,
   abstract = {The design and reanalysis of offshore and coastal structures usually requires the estimation of return values for dominant metocean variables (such as significant wave height) and associated values for other variables (such as peak spectral period or wind speed) from a finite sample of data; these are typically estimated using extreme value analysis. Yet the parameters of extreme value models can only be estimated with error from finite data. Different choices available to summarise uncertain information about the characteristics of the tail of a multivariate distribution in a small number of summary statistics (such as return values and associated values) complicates their estimation, especially for small sample sizes: choices regarding the ordering of mathematical operations lead to estimators of return values and associated values with different finite sample bias and variance characteristics. The current work extends a previous study (Jonathan et al.,2021) into the performance of estimators for marginal return values in the presence of sampling uncertainty, to estimators of associated values based on the bivariate conditional extremes model (Heffernan and Tawn, 2004) and competitors. Using a large designed simulation experiment, we explore the performance of combinations of 12 different estimators and three bivariate model candidates. The rich set of results from the simulation experiment are reported and explained in detail. Briefly: (a) calculation of associated values is only always feasible from small samples using two of the 12 estimators, which should be preferred; (b) estimators exploiting the median rather than the mean to summarise a distribution are more robust, and should also be preferred, especially for small sample sizes; (c) extreme value models incorporating appropriate descriptions of marginal and dependence provide better estimation of associated values for larger sample size; and (d) summarising the joint tail of metocean variables (in terms of return values and associated values) should be avoided where possible, in favour of probabilistic risk analysis of structural failure incorporating full uncertainty propagation.},
   author = {Ross Towe and David Randell and Jennifer Kensler and Graham Feld and Philip Jonathan},
   doi = {10.1016/j.oceaneng.2023.113808},
   issn = {00298018},
   journal = {Ocean Engineering},
   keywords = {Associated value,Conditioning,Extreme,Metocean,Multivariate,Return value},
   month = {3},
   pages = {113808},
   publisher = {Elsevier Ltd},
   title = {Estimation of associated values from conditional extreme value models},
   volume = {272},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0029801823001920},
   year = {2023},
}
@article{Cisneros2023,
   abstract = {Recent wildfires in Australia have led to considerable economic loss and property destruction, and there is increasing concern that climate change may exacerbate their intensity, duration, and frequency. Hazard quantification for extreme wildfires is an important component of wildfire management, as it facilitates efficient resource distribution, adverse effect mitigation, and recovery efforts. However, although extreme wildfires are typically the most impactful, both small and moderate fires can still be devastating to local communities and ecosystems. Therefore, it is imperative to develop robust statistical methods to reliably model the full distribution of wildfire spread. We do so for a novel dataset of Australian wildfires from 1999 to 2019, and analyse monthly spread over areas approximately corresponding to Statistical Areas Level~1 and~2 (SA1/SA2) regions. Given the complex nature of wildfire ignition and spread, we exploit recent advances in statistical deep learning and extreme value theory to construct a parametric regression model using graph convolutional neural networks and the extended generalized Pareto distribution, which allows us to model wildfire spread observed on an irregular spatial domain. We highlight the efficacy of our newly proposed model and perform a wildfire hazard assessment for Australia and population-dense communities, namely Tasmania, Sydney, Melbourne, and Perth.},
   author = {Daniela Cisneros and Jordan Richards and Ashok Dahal and Luigi Lombardo and Raphaël Huser},
   journal = {arXiv},
   month = {8},
   title = {Deep graphical regression for jointly moderate and extreme Australian wildfires},
   volume = {2308.14547},
   url = {http://arxiv.org/abs/2308.14547},
   year = {2023},
}
@article{Belzile2023,
   abstract = {The EVA 2023 data competition consisted of four challenges, ranging from interval estimation for very high quantiles of univariate extremes conditional on covariates, point estimation of unconditional return levels under a custom loss function, to estimation of the probabilities of tail events for low and high-dimensional multivariate data. We tackle these tasks by revisiting the current and existing literature on conditional univariate and multivariate extremes. We propose new cross-validation methods for covariate-dependent models, validation metrics for exchangeable multivariate models, formulae for the joint probability of exceedance for multivariate generalized Pareto vectors and a composition sampling algorithm for generating multivariate tail events for the latter. We highlight overarching themes ranging from model validation at extremely high quantile levels to building custom estimation strategies that leverage model assumptions.},
   author = {Léo R Belzile and Arnab Hazra and Rishikesh Yadav},
   journal = {arXiv},
   keywords = {conditional extremes modelling,cross-validation,data challenge,generalized additive models,missing data imputation,multivariate generalized Pareto distributions},
   title = {An utopic adventure in the modelling of conditional univariate and multivariate extremes},
   volume = {2312.13517},
   year = {2023},
}
@article{Lee2024,
   abstract = {The power law is useful in describing count phenomena such as network degrees and word frequencies. With a single parameter, it captures the main feature that the frequencies are linear on the log-log scale. Nevertheless, there have been criticisms of the power law, for example that a threshold needs to be pre-selected without its uncertainty quantified, that the power law is simply inadequate, and that subsequent hypothesis tests are required to determine whether the data could have come from the power law. We propose a modelling framework that combines two different generalisations of the power law, namely the generalised Pareto distribution and the Zipf-polylog distribution, to resolve these issues. The proposed mixture distributions are shown to fit the data well and quantify the threshold uncertainty in a natural way. A model selection step embedded in the Bayesian inference algorithm further answers the question whether the power law is adequate.},
   author = {Clement Lee and Emma Eastoe and Aiden Farrell},
   journal = {arXiv},
   month = {8},
   title = {From the power law to extreme value mixture distributions},
   volume = {2008.03073},
   url = {http://arxiv.org/abs/2008.03073},
   year = {2024},
}
@article{Papastathopoulos2024,
   abstract = {We use a functional analogue of the quantile function for probability measures admitting a continuous Lebesgue density on $\mathbb\{R\}^d$ to characterise the class of non-trivial limit distributions of radially recentered and rescaled multivariate exceedances. A new class of multivariate distributions is identified, termed radially-stable generalised Pareto distributions, and is shown to admit certain stability properties that permit extrapolation to extremal sets along any direction in cones such as $\mathbb\{R\}^d$ and $\mathbb\{R\}_+^d$. Leveraging the limit Poisson point process likelihood of the point process of radially renormalised exceedances, we develop parsimonious statistical models that exploit theoretical links between structural star-bodies and are amenable to Bayesian inference. Our framework sharpens statistical inference by suitably including additional information from the angular directions of the geometric exceedances and facilitates efficient computations in dimensions $d=2$ and $d=3$. Additionally, it naturally leads to the notion of return level-set, which is a canonical quantile set expressed in terms of its average recurrence interval, and a geometric analogue of the uni-dimensional return level. We illustrate our methods with a simulation study showing superior predictive performance of probabilities of rare events, and with two case studies, one associated with river flow extremes, and the other with oceanographic extremes.},
   author = {Ioannis Papastathopoulos and Lambert de Monte and Ryan Campbell and Haavard Rue},
   journal = {arXiv},
   month = {10},
   title = {Statistical inference for radially-stable generalized Pareto distributions and return level-sets in geometric extremes},
   volume = {2310.06130},
   url = {http://arxiv.org/abs/2310.06130},
   year = {2024},
}
@article{Brezger2008,
   abstract = {In many practical situations, it is desirable to restrict the flexibility of nonparametric estimation to accommodate a presumed monotonic relationship between a covariate and the response variable. We follow a Bayesian approach using penalized B-splines and incorporate the assumption of monotonicity in a natural way by an appropriate specification of the respective prior distributions. We illustrate the methodology in an empirical application modeling demand for brands of orange juice and show that imposing monotonicity constraints for own- and cross-item price effects improves the predictive validity of the estimated sales response functions considerably. © 2008 American Statistical Association.},
   author = {Andreas Brezger and Winfried J Steiner},
   doi = {10.1198/073500107000000223},
   issn = {0735-0015},
   issue = {1},
   journal = {Journal of Business & Economic Statistics},
   keywords = {Asymmetric quality tier competition,Generalized additive model,Markov chain Monte Carlo,Own- and cross-item price effects,Sales promotion},
   month = {1},
   pages = {90-104},
   title = {Monotonic Regression Based on Bayesian P–Splines},
   volume = {26},
   url = {http://www.tandfonline.com/doi/abs/10.1198/073500107000000223},
   year = {2008},
}
@book{Fahrmeir2011,
   abstract = {Several recent advances in smoothing and semiparametric regression are presented in this book from a unifying, Bayesian perspective. Simulation-based full Bayesian Markov chain Monte Carlo (MCMC) inference, as well as empirical Bayes procedures closely related to penalized likelihood estimation and mixed models, are considered here. Throughout, the focus is on semiparametric regression and smoothing based on basis expansions of unknown functions and effects in combination with smoothness priors for the basis coefficients. Beginning with a review of basic methods for smoothing and mixed models, longitudinal data, spatial data, and event history data are treated in separate chapters. Worked examples from various fields such as forestry, development economics, medicine, and marketing are used to illustrate the statistical methods covered in this book. Most of these examples have been analysed using implementations in the Bayesian software, BayesX, and some with R Codes.},
   author = {Ludwig Fahrmeir and Thomas Kneib},
   doi = {10.1093/acprof:oso/9780199533022.001.0001},
   isbn = {9780199533022},
   journal = {Bayesian Smoothing and Regression for Longitudinal, Spatial and Event History Data},
   keywords = {Bayesian perspective,Bayesx,Developmental economics,Forestry,MCMC,Marketing,Markov chain monte carlo,Medicine,Semiparametric regression,Smoothing},
   month = {4},
   pages = {1-544},
   publisher = {Oxford University Press},
   title = {Bayesian Smoothing and Regression for Longitudinal, Spatial and Event History Data},
   volume = {9780199533022},
   url = {https://academic.oup.com/book/11531},
   year = {2011},
}
@article{Dahal2024,
   abstract = {The most adopted definition of landslide hazard combines spatial information about landslide location (susceptibility), threat (intensity), and frequency (return period). Only the first two elements are usually considered and estimated when working over vast areas. Even then, separate models constitute the standard, with frequency being rarely investigated. Frequency and intensity are intertwined and depend on each other because larger events occur less frequently and vice versa. However, due to the lack of multi-temporal inventories and joint statistical models, modelling such properties via a unified hazard model has always been challenging and has yet to be attempted. Here, we develop a unified model to estimate landslide hazard at the slope unit level to address such gaps. We employed deep learning, combined with a model motivated by extreme-value theory to analyse an inventory of 30 years of observed rainfall-triggered landslides in Nepal and assess landslide hazard for multiple return periods. We also use our model to further explore landslide hazard for the same return periods under different climate change scenarios up to the end of the century. Our results show that the proposed model performs excellently and can be used to model landslide hazard in a unified manner. Geomorphologically, we find that under both climate change scenarios (SSP245 and SSP885), landslide hazard is likely to increase up to two times on average in the lower Himalayan regions while remaining the same in the middle Himalayan region whilst decreasing slightly in the upper Himalayan region areas.},
   author = {Ashok Dahal and Raphaël Huser and Luigi Lombardo},
   journal = {arXiv},
   month = {1},
   title = {At the junction between deep learning and statistics of extremes: formalizing the landslide hazard definition},
   volume = {2401.14210},
   url = {http://arxiv.org/abs/2401.14210},
   year = {2024},
}
@article{Gnecco2024,
   author = {Nicola Gnecco and Edossa Merga Terefe and Sebastian Engelke},
   doi = {10.1080/01621459.2023.2300522},
   issn = {0162-1459},
   journal = {Journal of the American Statistical Association},
   month = {1},
   pages = {1-24},
   title = {Extremal Random Forests},
   url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2023.2300522},
   year = {2024},
}
@article{,
   abstract = {Modern inference in extreme value theory faces numerous complications, such as missing data, hidden covariates or design problems. Some of those complications were exemplified in the EVA 2023 data challenge. The challenge comprises multiple individual problems which cover a variety of univariate and multivariate settings. This note presents the contribution of team genEVA in said competition, with particular focus on a detailed presentation of methodology and inference.},
   author = {Gloria Buriticá and Manuel Hentschel and Olivier C. Pasche and Frank Röttger and Zhongwei Zhang},
   month = {1},
   title = {Modeling Extreme Events: Univariate and Multivariate Data-Driven Approaches},
   url = {http://arxiv.org/abs/2401.14910},
   year = {2024},
}
@article{Huser2024,
   abstract = {Environmental data science for spatial extremes has traditionally relied heavily on max-stable processes. Even though the popularity of these models has perhaps peaked with statisticians, they are still perceived and considered as the `state-of-the-art' in many applied fields. However, while the asymptotic theory supporting the use of max-stable processes is mathematically rigorous and comprehensive, we think that it has also been overused, if not misused, in environmental applications, to the detriment of more purposeful and meticulously validated models. In this paper, we review the main limitations of max-stable process models, and strongly argue against their systematic use in environmental studies. Alternative solutions based on more flexible frameworks using the exceedances of variables above appropriately chosen high thresholds are discussed, and an outlook on future research is given, highlighting recommendations moving forward and the opportunities offered by hybridizing machine learning with extreme-value statistics.},
   author = {Raphaël Huser and Thomas Opitz and Jennifer Wadsworth},
   journal = {arXiv},
   month = {1},
   title = {Modeling of spatial extremes in environmental data science: Time to move away from max-stable processes},
   volume = {2401.17430},
   url = {http://arxiv.org/abs/2401.17430},
   year = {2024},
}
@inproceedings{Jurenaite2022,
   abstract = {In oncology, Deep Learning has shown great potential to personalise tasks such as tumour type classification, based on per-patient omics data-sets. Being high dimensional, incorporation of such data in one model is a challenge, often leading to one-dimensional studies and, therefore, information loss. Instead, we first propose relying on non-fixed sets of mutated genome sequences, which can be used for supervised learning of oncology-relevant tasks by our Transformer-based Deep Neural Network, SETQUENCE. Second, we extend the model to incorporate these representations as well as multiple sources of omics data in a flexible way with SETOMIC. Evaluation, using these representations, shows improved robustness and reduced information loss compared to previous approaches, while still being computationally tractable. By means of Explainable Artificial Intelligence methods, our models are shown to be able to recapitulate the biological contribution of several features in cancer, such as individual expression loci. This validation opens the door to novel directions in multi-faceted genome-wide biomarker discovery and personalised treatment among other presently clinically relevant tasks.},
   author = {Neringa Jurenaite and Daniel Leon-Perinan and Veronika Donath and Sunna Torge and Rene Jakel},
   doi = {10.1109/CIBCB55180.2022.9863058},
   isbn = {9781665484626},
   journal = {2022 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology, CIBCB 2022},
   keywords = {Deep Neural Network,Set Representations,gene expression,genome,molecular sequence analysis,multi-omics,mutome,natural language processing},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {SetQuence & SetOmic: Deep Set Transformer-based Representations of Cancer Multi-Omics},
   year = {2022},
}
@book{Linton2019,
   author = {Oliver Linton},
   doi = {10.1017/9781316819302},
   isbn = {9781316819302},
   journal = {Financial Econometrics},
   month = {2},
   pages = {358-421},
   publisher = {Cambridge University Press},
   title = {Financial Econometrics},
   url = {https://www.cambridge.org/highereducation/books/financial-econometrics/09CA0F5E949EB8F516EE4BB4E45F393E#contents},
   year = {2019},
}
@misc{,
   abstract = {Environmental contours are often used in engineering applications to describe risky combinations of variables according to some definition of an exceedance probability. These contours can be used to both understand multivariate extreme events in environmental processes and mitigate against their effects, e.g., in the design of structures. Such ideas are also useful in other disciplines, with the types of extreme events of interest depending on the context. Despite clear connections with extreme value modelling, much of this methodology has so far not been exploited in the estimation of environmental contours; in this work, we provide a way to unify these areas. We focus on the bivariate case, introducing two new definitions of environmental contours. We develop techniques for their inference which exploit a non-standard radial and angular decomposition of the variables, building on previous work for estimating limit sets. Specifically, we model the upper tails of the radial distribution using a generalised Pareto distribution, with adaptable smoothing of the parameters of this distribution. Our methods work equally well for asymptotically independent and asymptotically dependent variables, so do not require us to distinguish between different joint tail forms. Simulations demonstrate reasonable success of the estimation procedure, and we apply our approach to an air pollution data set, which is of interest in the context of environmental impacts on health.},
   author = {Emma S Simpson and Jonathan A Tawn},
   keywords = {bivariate extremes,environmental contours,generalised Pareto distribution,generalised additive model,structure function 1},
   title = {Inference for new environmental contours using extreme value analysis},
}
@article{Wood2017a,
   abstract = {The P-splines of Eilers and Marx (Stat Sci 11:89–121, 1996) combine a B-spline basis with a discrete quadratic penalty on the basis coefficients, to produce a reduced rank spline like smoother. P-splines have three properties that make them very popular as reduced rank smoothers: (i) the basis and the penalty are sparse, enabling efficient computation, especially for Bayesian stochastic simulation; (ii) it is possible to flexibly ‘mix-and-match’ the order of B-spline basis and penalty, rather than the order of penalty controlling the order of the basis as in spline smoothing; (iii) it is very easy to set up the B-spline basis functions and penalties. The discrete penalties are somewhat less interpretable in terms of function shape than the traditional derivative based spline penalties, but tend towards penalties proportional to traditional spline penalties in the limit of large basis size. However part of the point of P-splines is not to use a large basis size. In addition the spline basis functions arise from solving functional optimization problems involving derivative based penalties, so moving to discrete penalties for smoothing may not always be desirable. The purpose of this note is to point out that the three properties of basis-penalty sparsity, mix-and-match penalization and ease of setup are readily obtainable with B-splines subject to derivative based penalization. The penalty setup typically requires a few lines of code, rather than the two lines typically required for P-splines, but this one off disadvantage seems to be the only one associated with using derivative based penalties. As an example application, it is shown how basis-penalty sparsity enables efficient computation with tensor product smoothers of scattered data.},
   author = {Simon N. Wood},
   doi = {10.1007/s11222-016-9666-x},
   issn = {15731375},
   issue = {4},
   journal = {Statistics and Computing},
   keywords = {Derivative penalty,P-spline,Reduced rank spline,Smoothing spline,Tensor product smooth},
   month = {7},
   pages = {985-989},
   publisher = {Springer New York LLC},
   title = {P-splines with derivative based penalties and tensor product smoothing of unevenly distributed data},
   volume = {27},
   year = {2017},
}
@article{Youngman2022,
   author = {Benjamin D. Youngman},
   doi = {10.18637/jss.v103.i03},
   issn = {1548-7660},
   issue = {3},
   journal = {Journal of Statistical Software},
   title = {evgam: An R Package for Generalized Additive Extreme Value Models},
   volume = {103},
   url = {https://www.jstatsoft.org/v103/i03/},
   year = {2022},
}
@article{Yoshida2023,
   abstract = {The classical approach to analyzing extreme value data is the generalized Pareto distribution (GPD). When the GPD is used to explain a target variable with the large dimension of covariates, the shape and scale function of covariates included in GPD are sometimes modeled using the generalized additive models (GAM). In contrast to many results of application, there are no theoretical results on the hybrid technique of GAM and GPD, which motivates us to develop its asymptotic theory. We provide the rate of convergence of the estimator of shape and scale functions, as well as its local asymptotic normality.},
   author = {Takuma Yoshida},
   journal = {arXiv},
   month = {3},
   title = {Asymptotic theory for extreme value generalized additive models},
   volume = {2303.02402},
   url = {http://arxiv.org/abs/2303.02402},
   year = {2023},
}
@article{Andre2023,
   abstract = {To capture the extremal behaviour of complex environmental phenomena in practice, flexible techniques for modelling tail behaviour are required. In this paper, we introduce a variety of such methods, which were used by the Lancopula Utopiversity team to tackle the data challenge of the 2023 Extreme Value Analysis Conference. This data challenge was split into four sections, labelled C1-C4. Challenges C1 and C2 comprise univariate problems, where the goal is to estimate extreme quantiles for a non-stationary time series exhibiting several complex features. We propose a flexible modelling technique, based on generalised additive models, with diagnostics indicating generally good performance for the observed data. Challenges C3 and C4 concern multivariate problems where the focus is on estimating joint extremal probabilities. For challenge C3, we propose an extension of available models in the multivariate literature and use this framework to estimate extreme probabilities in the presence of non-stationary dependence. Finally, for challenge C4, which concerns a 50 dimensional random vector, we employ a clustering technique to achieve dimension reduction and use a conditional modelling approach to estimate extremal probabilities across independent groups of variables.},
   author = {L. M. André and R. Campbell and E. D'Arcy and A. Farrell and D. Healy and L. Kakampakou and C. Murphy and C. J. R. Murphy-Barltrop and M. Speers},
   journal = {arXiv},
   month = {12},
   title = {Extreme value methods for estimating rare events in Utopia},
   volume = {2312.09825},
   url = {http://arxiv.org/abs/2312.09825},
   year = {2023},
}
@article{,
   abstract = {In this work, we summarize the state-of-the-art methods in causal inference for extremes. In a non-exhaustive way, we start by describing an extremal approach to quantile treatment effect where the treatment has an impact on the tail of the outcome. Then, we delve into two primary causal structures for extremes, offering in-depth insights into their identifiability. Additionally, we discuss causal structure learning in relation to these two models as well as in a model-agnostic framework. To illustrate the practicality of the approaches, we apply and compare these different methods using a Seine network dataset. This work concludes with a summary and outlines potential directions for future research.},
   author = {Valérie Chavez-Demoulin and Linda Mhalla},
   journal = {arXiv},
   month = {3},
   title = {Causality and extremes},
   volume = {2403.05331},
   url = {http://arxiv.org/abs/2403.05331},
   year = {2024},
}
@article{Legrand2021,
   abstract = {Machine learning classification methods usually assume that all possible classes are sufficiently present within the training set. Due to their inherent rarities, extreme events are always under-represented and classifiers tailored for predicting extremes need to be carefully designed to handle this under-representation. In this paper, we address the question of how to assess and compare classifiers with respect to their capacity to capture extreme occurrences. This is also related to the topic of scoring rules used in forecasting literature. In this context, we propose and study a risk function adapted to extremal classifiers. The inferential properties of our empirical risk estimator are derived under the framework of multivariate regular variation and hidden regular variation. A simulation study compares different classifiers and indicates their performance with respect to our risk function. To conclude, we apply our framework to the analysis of extreme river discharges in the Danube river basin. The application compares different predictive algorithms and test their capacity at forecasting river discharges from other river stations.},
   author = {Juliette Legrand and Philippe Naveau and Marco Oesting},
   journal = {arXiv},
   month = {12},
   title = {Evaluation of binary classifiers for asymptotically dependent and independent extremes},
   volume = {2112.13738},
   url = {http://arxiv.org/abs/2112.13738},
   year = {2021},
}
@article{Haugh2021,
   abstract = {This tutorial 1 provides an introduction to Bayesian modeling and Markov Chain Monte-Carlo (MCMC) algorithms including the Metropolis-Hastings and Gibbs Sampling algorithms. We discuss some of the challenges associated with running MCMC including the important question of determining when convergence to stationarity has been achieved. Several applications of Bayesian modeling are also provided including the MRP approach to modeling election outcomes, topic modeling in machine learning, and large-scale optimization and code breaking. We also discuss the important problems of Bayesian model checking and selection and provide an introduction to empirical Bayesian models. In the appendices we briefly discuss several topics including: (i) Hamiltonian Monte-Carlo (an MCMC algorithm that forms the basis for the statistical modeling language STAN [10]) (ii) the EM algorithm and (iii) other problems in Bayesian statistics including Bayesian approaches to constructing confidence intervals, hypothesis testing and decision theory. 1 This tutorial grew out of a shorter set of lectures notes on MCMC / Bayesian modeling that I'd previously developed for M.Sc. level Machine Learning and Monte Carlo Simulation courses that I taught at the IE&OR Department at Columbia University. The focus on Monte-Carlo Simulation helps to explain my choice of getting quickly to the MCMC material rather than focusing first on Bayesian modeling. Some of the figures in the tutorial are taken from the excellent textbooks of Barber [2], Bishop [5], Efron and Hastie [15] and Gelman et al. [19]. The textbooks by MacKay [30] and Robert and Casella [39] were also particularly helpful in putting this material together. Comments are welcome and unfortunately all errors and omissions are mine alone.},
   author = {Martin B. Haugh},
   doi = {10.2139/ssrn.3759243},
   issn = {1556-5068},
   journal = {SSRN Electronic Journal},
   title = {A Tutorial on Markov Chain Monte-Carlo and Bayesian Modeling},
   url = {https://www.ssrn.com/abstract=3759243},
   year = {2021},
}
@article{Jespersen2010,
   abstract = {This paper introduces the method of Markov Chain Monte Carlo (MCMC). An outline of the methods is given together with some preliminary tools. The Bayesian approach to statistics is introduced, and the necessary continuous state space Markov chain theory is summarized. Two common algorithms for generating random draws from complex joint distribution are presented; The Gibbs sampler and the Metropolis-Hastings algorithm. We discuss implementational issues and demonstrate the method by a simple empirical example on a generalized linear mixed model. The reader is assumed to have background in probability theory and to be familiar with discrete time Markov chains on a finite state space.},
   author = {Nicolai Schipper Jespersen},
   doi = {10.2139/ssrn.1594971},
   issn = {1556-5068},
   journal = {SSRN Electronic Journal},
   keywords = {Bayesian,GLMM,Gibbs sampler,MCMC,Markov chain Monte Carlo,Metropolis-Hastings algorithm,generalized linear mixed model,simulation},
   title = {An Introduction to Markov Chain Monte Carlo},
   url = {http://www.ssrn.com/abstract=1594971},
   year = {2010},
}
@book{Ntzoufras2009,
   author = {Ioannis Ntzoufras},
   doi = {10.1002/9780470434567},
   isbn = {9780470141144},
   month = {1},
   publisher = {Wiley},
   title = {Bayesian Modeling Using WinBUGS},
   year = {2009},
}
@inbook{Fahrmeir2011,
   abstract = {Several recent advances in smoothing and semiparametric regression are presented in this book from a unifying, Bayesian perspective. Simulation-based full Bayesian Markov chain Monte Carlo (MCMC) inference, as well as empirical Bayes procedures closely related to penalized likelihood estimation and mixed models, are considered here. Throughout, the focus is on semiparametric regression and smoothing based on basis expansions of unknown functions and effects in combination with smoothness priors for the basis coefficients. Beginning with a review of basic methods for smoothing and mixed models, longitudinal data, spatial data, and event history data are treated in separate chapters. Worked examples from various fields such as forestry, development economics, medicine, and marketing are used to illustrate the statistical methods covered in this book. Most of these examples have been analysed using implementations in the Bayesian software, BayesX, and some with R Codes.},
   author = {Ludwig Fahrmeir and Thomas Kneib},
   doi = {10.1093/acprof:oso/9780199533022.003.0002},
   journal = {Bayesian Smoothing and Regression for Longitudinal, Spatial and Event History Data},
   keywords = {Bayesian perspective,Bayesx,Developmental economics,Forestry,MCMC,Marketing,Markov chain monte carlo,Medicine,Semiparametric regression,Smoothing},
   month = {4},
   pages = {18-106},
   publisher = {Oxford University Press},
   title = {Basic Concepts for Smoothing and Semiparametric Regression},
   volume = {9780199533022},
   url = {https://academic.oup.com/book/11531/chapter/160298905},
   year = {2011},
}
@article{Wood2017,
   abstract = {We develop scalable methods for fitting penalized regression spline based generalized additive models with of the order of 104 coefficients to up to 108 data. Computational feasibility rests on: (i) a new iteration scheme for estimation of model coefficients and smoothing parameters, avoiding poorly scaling matrix operations; (ii) parallelization of the iteration’s pivoted block Cholesky and basic matrix operations; (iii) the marginal discretization of model covariates to reduce memory footprint, with efficient scalable methods for computing required crossproducts directly from the discrete representation. Marginal discretization enables much finer discretization than joint discretization would permit. We were motivated by the need to model four decades worth of daily particulate data from the U.K. Black Smoke and Sulphur Dioxide Monitoring Network. Although reduced in size recently, over 2000 stations have at some time been part of the network, resulting in some 10 million measurements. Modeling at a daily scale is desirable for accurate trend estimation and mapping, and to provide daily exposure estimates for epidemiological cohort studies. Because of the dataset size, previous work has focused on modeling time or space averaged pollution levels, but this is unsatisfactory from a health perspective, since it is often acute exposure locally and on the time scale of days that is of most importance in driving adverse health outcomes. If computed by conventional means our black smoke model would require a half terabyte of storage just for the model matrix, whereas we are able to compute with it on a desktop workstation. The best previously available reduced memory footprint method would have required three orders of magnitude more computing time than our new method. Supplementary materials for this article are available online.},
   author = {Simon N. Wood and Zheyuan Li and Gavin Shaddick and Nicole H. Augustin},
   doi = {10.1080/01621459.2016.1195744},
   issn = {0162-1459},
   issue = {519},
   journal = {Journal of the American Statistical Association},
   keywords = {Air pollution,Big data,Parallel computing,Regression,Smoothing},
   month = {7},
   pages = {1199-1210},
   publisher = {American Statistical Association},
   title = {Generalized Additive Models for Gigadata: Modeling the U.K. Black Smoke Network Daily Data},
   volume = {112},
   url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2016.1195744},
   year = {2017},
}
@article{Bodik2024,
   abstract = {The potential outcomes framework serves as a fundamental tool for quantifying the causal effects. When the treatment variable (exposure) is continuous, one is typically interested in the estimation of the effect curve (also called the average dose-response function), denoted as \(mu(t)\). In this work, we explore the ``extreme causal effect,'' where our focus lies in determining the impact of an extreme level of treatment, potentially beyond the range of observed values--that is, estimating \(mu(t)\) for very large \(t\). Our framework is grounded in the field of statistics known as extreme value theory. We establish the foundation for our approach, outlining key assumptions that enable the estimation of the extremal causal effect. Additionally, we present a novel and consistent estimation procedure that utilizes extreme value theory in order to potentially reduce the dimension of the confounders to at most 3. In practical applications, our framework proves valuable when assessing the effects of scenarios such as drug overdoses, extreme river discharges, or extremely high temperatures on a variable of interest.},
   author = {Juraj Bodik},
   journal = {arXiv},
   month = {3},
   title = {Extreme Treatment Effect: Extrapolating Causal Effects Into Extreme Treatment Domain},
   volume = {2403.11003},
   url = {http://arxiv.org/abs/2403.11003},
   year = {2024},
}
@misc{Georges2001,
   abstract = {In this paper, we review the use of copulas for multivariate survival modelling. In particular, we study properties of survival copulas and discuss the dependence measures associated to this construction. Then, we consider the problem of competing risks. We derive the distribution of the failure time and order statistics. After having presented statistical inference, we finally provide financial applications which concern the life time value, the link between default, prepayment and credit lifetime, the measure of risk for a credit portfolio and the pricing of credit derivatives.},
   author = {P Georges and A-G Lamy and E Nicolas and G Quibel and T Roncalli},
   title = {Multivariate survival modelling: a unified approach with copulas},
   year = {2001},
}
@article{,
   abstract = {Max-autogressive moving average (Max-ARMA) processes are powerful tools for modelling time series data with heavy-tailed behaviour; these are a non-linear version of the popular autoregressive moving average models. River flow data typically have features of heavy tails and non-linearity, as large precipitation events cause sudden spikes in the data that then exponentially decay. Therefore, stationary Max-ARMA models are a suitable candidate for capturing the unique temporal dependence structure exhibited by river flows. This paper contributes to advancing our understanding of the extremal properties of stationary Max-ARMA processes. We detail the first approach for deriving the extremal index, the lagged asymptotic dependence coefficient, and an efficient simulation for a general Max-ARMA process. We use the extremal properties, coupled with the belief that Max-ARMA processes provide only an approximation to extreme river flow, to fit such a model which can broadly capture river flow behaviour over a high threshold. We make our inference under a reparametrisation which gives a simpler parameter space that excludes cases where any parameter is non-identifiable. We illustrate results for river flow data from the UK River Thames.},
   author = {Eleanor D'Arcy and Jonathan A Tawn},
   journal = {arXiv},
   month = {3},
   title = {Extremal properties of max-autoregressive moving average processes for modelling extreme river flows},
   volume = {2403.16590},
   url = {http://arxiv.org/abs/2403.16590},
   year = {2024},
}
@article{Reistad2011,
   abstract = {A combined high-resolution atmospheric downscaling and wave hindcast based on the ERA-40 reanalysis covering the Norwegian Sea, the North Sea, and the Barents Sea is presented. The period covered is from September 1957 to August 2002. The dynamic atmospheric downscaling is performed as a series of short prognostic runs initialized from a blend of ERA-40 and the previous prognostic run to preserve the fine-scale surface features from the high-resolution model while maintaining the large-scale synoptic field from ERA-40. The nested WAM wave model hindcast consists of a coarse 50 km model covering the North Atlantic forced with ERA-40 winds and a nested 10-11 km resolution model forced with downscaled winds. A comparison against in situ and satellite observations of wind and sea state reveals significant improvement in mean values and upper percentiles of wind vectors and the significant wave height over ERA-40. Improvement is also found in the mean wave period. ERA-40 is biased low in wind speed and significant wave height, a bias which is not reproduced by the downscaling. The atmospheric downscaling also reproduces polar lows, which cannot be resolved by ERA-40, but the lows are too weak and short-lived as the downscaling is not capable of capturing their full life cycle. Copyright 2011 by the American Geophysical Union.},
   author = {Magnar Reistad and Øyvind Breivik and Hilde Haakenstad and Ole Johan Aarnes and Birgitte R. Furevik and Jean-Raymond Bidlot},
   doi = {10.1029/2010JC006402},
   issn = {0148-0227},
   issue = {C5},
   journal = {Journal of Geophysical Research},
   month = {5},
   pages = {C05019},
   publisher = {Blackwell Publishing Ltd},
   title = {A high-resolution hindcast of wind and waves for the North Sea, the Norwegian Sea, and the Barents Sea},
   volume = {116},
   url = {http://doi.wiley.com/10.1029/2010JC006402},
   year = {2011},
}
@article{Murphy-Barltrop2024,
   abstract = {The modelling of multivariate extreme events is important in a wide variety of applications, including flood risk analysis, metocean engineering and financial modelling. A wide variety of statistical techniques have been proposed in the literature; however, many such methods are limited in the forms of dependence they can capture, or make strong parametric assumptions about data structures. In this article, we introduce a novel inference framework for multivariate extremes based on a semi-parametric angular-radial model. This model overcomes the limitations of many existing approaches and provides a unified paradigm for assessing joint tail behaviour. Alongside inferential tools, we also introduce techniques for assessing uncertainty and goodness of fit. Our proposed technique is tested on simulated data sets alongside observed metocean time series', with results indicating generally good performance.},
   author = {Callum John Rowlandson Murphy-Barltrop and Ed Mackay and Philip Jonathan},
   journal = {arXiv},
   month = {1},
   title = {Inference for multivariate extremes via a semi-parametric angular-radial model},
   volume = {2401.07259},
   url = {http://arxiv.org/abs/2401.07259},
   year = {2024},
}
