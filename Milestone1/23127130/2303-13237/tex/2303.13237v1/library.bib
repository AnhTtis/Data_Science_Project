@article{Huser2019,
   abstract = {Many environmental processes exhibit weakening spatial dependence as events become more extreme. Well-known limiting models, such as max-stable or generalized Pareto processes, cannot capture this, which can lead to a preference for models that exhibit a property known as asymptotic independence. However, weakening dependence does not automatically imply asymptotic independence, and whether the process is truly asymptotically (in)dependent is usually far from clear. The distinction is key as it can have a large impact upon extrapolation, that is, the estimated probabilities of events more extreme than those observed. In this work, we present a single spatial model that is able to capture both dependence classes in a parsimonious manner, and with a smooth transition between the two cases. The model covers a wide range of possibilities from asymptotic independence through to complete dependence, and permits weakening dependence of extremes even under asymptotic dependence. Censored likelihood-based inference for the implied copula is feasible in moderate dimensions due to closed-form margins. The model is applied to oceanographic datasets with ambiguous true limiting dependence structure. Supplementary materials for this article are available online.},
   author = {Raphaël Huser and Jennifer L. Wadsworth},
   doi = {10.1080/01621459.2017.1411813},
   issn = {1537274X},
   issue = {525},
   journal = {Journal of the American Statistical Association},
   keywords = {Asymptotic dependence and independence,Censored likelihood inference,Copula,Spatial extremes,Threshold exceedance},
   pages = {434-444},
   title = {Modeling Spatial Processes with Unknown Extremal Dependence Class},
   volume = {114},
   year = {2019},
}
@article{Heffernan2004,
   abstract = {Multivariate extreme value theory and methods concern the characterization, estimation and extrapolation of the joint tail of the distribution of a d-dimensional random variable. Existing approaches are based on limiting arguments in which all components of the variable become large at the same rate. This limit approach is inappropriate when the extreme values of all the variables are unlikely to occur together or when interest is in regions of the support of the joint distribution where only a subset of components is extreme. In practice this restricts existing methods to applications where d is typically 2 or 3. Under an assumption about the asymptotic form of the joint distribution of a d-dimensional random variable conditional on its having an extreme component, we develop an entirely new semiparametric approach which overcomes these existing restrictions and can be applied to problems of any dimension. We demonstrate the performance of our approach and its advantages over existing methods by using theoretical examples and simulation studies. The approach is used to analyse air pollution data and reveals complex extremal dependence behaviour that is consistent with scientific understanding of the process. We find that the dependence structure exhibits marked seasonality, with extremal dependence between some pollutants being significantly greater than the dependence at non-extreme levels.},
   author = {Janet E. Heffernan and Jonathan A. Tawn},
   doi = {10.1111/j.1467-9868.2004.02050.x},
   issn = {13697412},
   issue = {3},
   journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
   keywords = {Air pollution,Asymptotic independence,Bootstrap,Conditional distribution,Gaussian estimation,Multivariate extreme value theory,Semiparametric modelling},
   pages = {497-546},
   title = {A conditional approach for multivariate extreme values},
   volume = {66},
   year = {2004},
}
@article{Engelke2020a,
   abstract = {Conditional independence, graphical models and sparsity are key notions for parsimonious statistical models and for understanding the structural relationships in the data. The theory of multivariate and spatial extremes describes the risk of rare events through asymptotically justified limit models such as max-stable and multivariate Pareto distributions. Statistical modelling in this field has been limited to moderate dimensions so far, partly owing to complicated likelihoods and a lack of understanding of the underlying probabilistic structures. We introduce a general theory of conditional independence for multivariate Pareto distributions that enables the definition of graphical models and sparsity for extremes. A Hammersley–Clifford theorem links this new notion to the factorization of densities of extreme value models on graphs. For the popular class of Hüsler–Reiss distributions we show that, similarly to the Gaussian case, the sparsity pattern of a general extremal graphical model can be read off from suitable inverse covariance matrices. New parametric models can be built in a modular way and statistical inference can be simplified to lower dimensional marginals. We discuss learning of minimum spanning trees and model selection for extremal graph structures, and we illustrate their use with an application to flood risk assessment on the Danube river.},
   author = {Sebastian Engelke and Adrien S. Hitz},
   doi = {10.1111/rssb.12355},
   issn = {14679868},
   issue = {4},
   journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
   keywords = {Conditional independence,Extreme value theory,Graphical models,Multivariate Pareto distribution,Sparsity},
   pages = {871-932},
   title = {Graphical models for extremes},
   volume = {82},
   year = {2020},
}
@article{Engelke2020,
   abstract = {Extreme value statistics provides accurate estimates for the small occurrence probabilities of rare events. While theory and statistical tools for univariate extremes are well-developed, methods for high-dimensional and complex data sets are still scarce. Appropriate notions of sparsity and connections to other fields such as machine learning, graphical models and high-dimensional statistics have only recently been established. This article reviews the new domain of research concerned with the detection and modeling of sparse patterns in rare events. We first describe the different forms of extremal dependence that can arise between the largest observations of a multivariate random vector. We then discuss the current research topics including clustering, principal component analysis and graphical modeling for extremes. Identification of groups of variables which can be concomitantly extreme is also addressed. The methods are illustrated with an application to flood risk assessment.},
   author = {Sebastian Engelke and Jevgenijs Ivanovs},
   keywords = {conditional independence,dimension reduction,extremal graphical models,extreme value theory,sparsity},
   pages = {1-32},
   title = {Sparse Structures for Multivariate Extremes},
   url = {http://arxiv.org/abs/2004.12182},
   year = {2020},
}
@article{Castro-Camilo2020,
   abstract = {To disentangle the complex nonstationary dependence structure of precipitation extremes over the entire contiguous United States (U.S.), we propose a flexible local approach based on factor copula models. Our subasymptotic spatial modeling framework yields nontrivial tail dependence structures, with a weakening dependence strength as events become more extreme; a feature commonly observed with precipitation data but not accounted for in classical asymptotic extreme-value models. To estimate the local extremal behavior, we fit the proposed model in small regional neighborhoods to high threshold exceedances, under the assumption of local stationarity, which allows us to gain in flexibility. By adopting a local censored likelihood approach, we make inference on a fine spatial grid, and we perform local estimation by taking advantage of distributed computing resources and the embarrassingly parallel nature of this estimation procedure. The local model is efficiently fitted at all grid points, and uncertainty is measured using a block bootstrap procedure. We carry out an extensive simulation study to show that our approach can adequately capture complex, nonstationary dependencies, in addition, our study of U.S. winter precipitation data reveals interesting differences in local tail structures over space, which has important implications on regional risk assessment of extreme precipitation events. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
   author = {Daniela Castro-Camilo and Raphaël Huser},
   doi = {10.1080/01621459.2019.1647842},
   issn = {1537274X},
   issue = {531},
   journal = {Journal of the American Statistical Association},
   keywords = {Factor copula model,Local likelihood,Nonstationarity,Spatial extremes,Threshold exceedance},
   pages = {1037-1054},
   publisher = {Taylor & Francis},
   title = {Local Likelihood Estimation of Complex Tail Dependence Structures, Applied to U.S. Precipitation Extremes},
   volume = {115},
   url = {https://doi.org/10.1080/01621459.2019.1647842},
   year = {2020},
}
@article{Eastoe2009,
   abstract = {Statistical methods for modelling extremes of stationary sequences have received much attention. The most common method is to model the rate and size of exceedances of some high constant threshold; the size of exceedances is modelled by using a generalized Pareto distribution. Frequently, data sets display non-stationarity; this is especially common in environmental applications. The ozone data set that is presented here is an example of such a data set. Surface level ozone levels display complex seasonal patterns and trends due to the mechanisms that are involved in ozone formation. The standard methods of modelling the extremes of a non-stationary process focus on retaining a constant threshold but using covariate models in the rate and generalized Pareto distribution parameters. We suggest an alternative approach that uses preprocessing methods to model the non-stationarity in the body of the process and then uses standard methods to model the extremes of the preprocessed data. We illustrate both the standard and the preprocessing methods by using a simulation study and a study of the ozone data. We suggest that the preprocessing method gives a model that better incorporates the underlying mechanisms that generate the process, produces a simpler and more efficient fit and allows easier computation. © 2009 Royal Statistical Society.},
   author = {Emma F. Eastoe and Jonathan A. Tawn},
   doi = {10.1111/j.1467-9876.2008.00638.x},
   issn = {00359254},
   issue = {1},
   journal = {Journal of the Royal Statistical Society. Series C: Applied Statistics},
   keywords = {Generalized Pareto distribution,Non-stationary process,Ozone,Preprocessing,Return levels,Threshold exceedances},
   pages = {25-45},
   title = {Modelling non-stationary extremes with application to surface level ozone},
   volume = {58},
   year = {2009},
}
@article{DeFondeville2020,
   abstract = {Peaks-over-threshold analysis using the generalized Pareto distribution is widely applied in modelling tails of univariate random variables, but much information may be lost when complex extreme events are studied using univariate results. In this paper, we extend peaks-over-threshold analysis to extremes of functional data. Threshold exceedances defined using a functional $r$ are modelled by the generalized $r$-Pareto process, a functional generalization of the generalized Pareto distribution that covers the three classical regimes for the decay of tail probabilities. This process is the only possible limit for the distribution of $r$-exceedances of a properly rescaled process. We give construction rules, simulation algorithms and inference procedures for generalized $r$-Pareto processes, discuss model validation, and use the new methodology to study extreme European windstorms and heavy spatial rainfall.},
   author = {Raphaël de Fondeville and Anthony C. Davison},
   keywords = {functional regular variation,pareto process,peaks-over-threshold analysis,r -,rainfall,spatial statistics,statistics of extremes,windstorm},
   pages = {1-68},
   title = {Functional Peaks-over-threshold Analysis},
   url = {http://arxiv.org/abs/2002.02711},
   year = {2020},
}
@article{Goncalves2011,
   author = {Sílvia Gonçalves and Dimitris Politis},
   doi = {10.1016/j.jkss.2011.07.003},
   issn = {12263192},
   issue = {4},
   journal = {Journal of the Korean Statistical Society},
   pages = {383-386},
   publisher = {Elsevier B.V.},
   title = {Discussion: Bootstrap methods for dependent data: A review},
   volume = {40},
   url = {http://dx.doi.org/10.1016/j.jkss.2011.07.003},
   year = {2011},
}
@article{Eastoe2012,
   abstract = {A standard approach to model the extreme values of a stationary process is the peaks over threshold method, which consists of imposing a high threshold, identifying clusters of exceedances of this threshold and fitting the maximum value from each cluster using the generalized Pareto distribution. This approach is strongly justified by underlying asymptotic theory. We propose an alternative model for the distribution of the cluster maxima that accounts for the subasymptotic theory of extremes of a stationary process. This new distribution is a product of two terms, one for the marginal distribution of exceedances and the other for the dependence structure of the exceedance values within a cluster. We illustrate the improvement in fit, measured by the root mean square error of the estimated quantiles, offered by the new distribution over the peaks over thresholds analysis using simulated and hydrological data, and we suggest a diagnostic tool to help identify when the proposed model is likely to lead to an improved fit. © 2011 Biometrika Trust.},
   author = {Emma F. Eastoe and Jonathan A. Tawn},
   doi = {10.1093/biomet/asr078},
   issn = {00063444},
   issue = {1},
   journal = {Biometrika},
   keywords = {Cluster maxima,Extremal index,Generalized Pareto distribution,L-moment,Peaks over thresholds},
   pages = {43-55},
   title = {Modelling the distribution of the cluster maxima of exceedances of subasymptotic thresholds},
   volume = {99},
   year = {2012},
}
@article{Ramos2009,
   abstract = {A fundamental issue in applied multivariate extreme value analysis is modelling dependence within joint tail regions. The primary focus of this work is to extend the classical pseudopolar treatment of multivariate extremes to develop an asymptotically motivated representation of extremal dependence that also encompasses asymptotic independence. Starting with the usual mild bivariate regular variation assumptions that underpin the coefficient of tail dependence as a measure of extremal dependence, our main result is a characterization of the limiting structure of the joint survivor function in terms of an essentially arbitrary non-negative measure that must satisfy some mild constraints. We then construct parametric models from this new class and study in detail one example that accommodates asymptotic dependence, asymptotic independence and asymmetry within a straightforward parsimonious parameterization. We provide a fast simulation algorithm for this example and detail likelihood-based inference including tests for asymptotic dependence and symmetry which are useful for submodel selection. We illustrate this model by application to both simulated and real data. In contrast with the classical multivariate extreme value approach, which concentrates on the limiting distribution of normalized componentwise maxima, our framework focuses directly on the structure of the limiting joint survivor function and provides significant extensions of both the theoretical and the practical tools that are available for joint tail modelling. © 2009 Royal Statistical Society.},
   author = {Alexandra Ramos and Anthony Ledford},
   doi = {10.1111/j.1467-9868.2008.00684.x},
   issn = {13697412},
   issue = {1},
   journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
   keywords = {Asymptotic independence,Coefficient of tail dependence,Hidden regular variation,Joint tail modelling,Maximum likelihood,Multivariate extreme values},
   pages = {219-241},
   title = {A new class of models for bivariate joint tails},
   volume = {71},
   year = {2009},
}
@article{Eastoe2019,
   abstract = {Under the influence of local- and large-scale climatological processes, extreme river flow events often show long-term trends, seasonality, interyear variability, and other characteristics of temporal nonstationarity. Properly accounting for this nonstationarity is vital for making accurate predictions of future floods. In this paper, a regional model based on the generalised Pareto distribution is developed for peaks-over-threshold river flow data sets when the event sizes are nonstationary. If observations are nonstationary and covariates are available, then extreme-value (semi)parametric regression models may be used. Unfortunately, the necessary covariates are rarely observed, and if they are, it is often not clear which process, or combination of processes, to include in the model. Within the statistical literature, latent process (or random effects) models are often used in such scenarios. We develop a regional time-varying random effects model that allows identification of temporal nonstationarity in event sizes by pooling information across all sites in a spatially homogeneous region. The proposed model, which is an instance of a Bayesian hierarchical model, can be used to predict both unconditional extreme events such as the m-year maximum and extreme events that condition on being in a given year. The estimated random effects may also tell us about likely candidates for the climatological processes that cause nonstationarity in the flood process. The model is applied to UK flood data from 817 stations spread across 81 hydrometric regions.},
   author = {E. F. Eastoe},
   doi = {10.1002/env.2560},
   issn = {1099095X},
   issue = {5},
   journal = {Environmetrics},
   keywords = {Bayesian hierarchical models,flooding,nonstationarity,return levels,spatial pooling,time-varying random effects},
   pages = {1-18},
   title = {Nonstationarity in peaks-over-threshold river flows: A regional random effects model},
   volume = {30},
   year = {2019},
}
@article{Eastoe2010,
   abstract = {In a peaks over threshold analysis of a series of river flows, a sufficiently high threshold is used to extract the peaks of independent flood events. This paper reviews existing, and proposes new, statistical models for both the annual counts of such events and the process of event peak times. The most common existing model for the process of event times is a homogeneous Poisson process. This model is motivated by asymptotic theory. However, empirical evidence suggests that it is not the most appropriate model, since it implies that the mean and variance of the annual counts are the same, whereas the counts appear to be overdispersed, i.e., have a larger variance than mean. This paper describes how the homogeneous Poisson process can be extended to incorporate time variation in the rate at which events occur and so help to account for overdispersion in annual counts through the use of regression and mixed models. The implications of these new models on the implied probability distribution of the annual maxima are also discussed. The models are illustrated using a historical flow series from the River Thames at Kingston. Copyright 2010 by the American Geophysical Union.},
   author = {Emma F. Eastoe and Jonathan A. Tawn},
   doi = {10.1029/2009WR007757},
   issn = {00431397},
   issue = {2},
   journal = {Water Resources Research},
   keywords = {http://dx.doi.org/10.1029/2009WR007757, doi:10.102},
   pages = {1-12},
   title = {Statistical models for overdispersion in the frequency of peaks over threshold data for a flow series},
   volume = {46},
   year = {2010},
}
@article{Papastathopoulos2013,
   abstract = {The most popular approach in extreme value statistics is the modelling of threshold exceedances using the asymptotically motivated generalised Pareto distribution. This approach involves the selection of a high threshold above which the model fits the data well. Sometimes, few observations of a measurement process might be recorded in applications and so selecting a high quantile of the sample as the threshold leads to almost no exceedances. In this paper we propose extensions of the generalised Pareto distribution that incorporate an additional shape parameter while keeping the tail behaviour unaffected. The inclusion of this parameter offers additional structure for the main body of the distribution, improves the stability of the modified scale, tail index and return level estimates to threshold choice and allows a lower threshold to be selected. We illustrate the benefits of the proposed models with a simulation study and two case studies. © 2012 Elsevier B.V.},
   author = {Ioannis Papastathopoulos and Jonathan A. Tawn},
   doi = {10.1016/j.jspi.2012.07.001},
   issn = {03783758},
   issue = {1},
   journal = {Journal of Statistical Planning and Inference},
   keywords = {Extended generalised Pareto distribution,Extreme value theory,Liver toxicity,Tail estimation,Threshold selection},
   pages = {131-143},
   publisher = {Elsevier},
   title = {Extended generalised Pareto models for tail estimation},
   volume = {143},
   url = {http://dx.doi.org/10.1016/j.jspi.2012.07.001},
   year = {2013},
}
@article{Ramos2011,
   abstract = {An alternative limiting point process to that of de Haan (1985) is studied that holds regardless of whether the underlying data generation mechanism is asymptotically dependent or asymptotically independent. We characterize its intensity function in terms of the coefficient of tail dependence and an angular measure which satisfies a normalisation condition. We use this point process to derive a generalisation of standard componentwise maxima results that holds for both asymptotic dependence and asymptotic independence. We illustrate our results using a flexible parametric example and provide methods for simulating from both the limiting point process and the limiting componentwise maxima distribution. © Taylor & Francis Group, LLC.},
   author = {Alexandra Ramos and Anthony Ledford},
   doi = {10.1080/03610921003764233},
   isbn = {0361092100376},
   issn = {03610926},
   issue = {12},
   journal = {Communications in Statistics - Theory and Methods},
   keywords = {Asymptotic dependence,Asymptotic independence,Coefficient of tail dependence,Componentwise maxima,Extreme value theory,Joint survivor function,Multivariate extreme values,Point processes,Simulation},
   pages = {2205-2224},
   title = {An alternative point process framework for modeling multivariate extreme values},
   volume = {40},
   year = {2011},
}
@article{Mhalla2019,
   abstract = {The probability and structure of co-occurrences of extreme values in multivariate data may critically depend on auxiliary information provided by covariates. In this contribution, we develop a flexible generalized additive modeling framework based on high threshold exceedances for estimating covariate-dependent joint tail characteristics for regimes of asymptotic dependence and asymptotic independence. The framework is based on suitably defined marginal pretransformations and projections of the random vector along the directions of the unit simplex, which lead to convenient univariate representations of multivariate exceedances based on the exponential distribution. Good performance of our estimators of a nonparametrically designed influence of covariates on extremal coefficients and tail dependence coefficients are shown through a simulation study. We illustrate the usefulness of our modeling framework on a large dataset of nitrogen dioxide measurements recorded in France between 1999 and 2012, where we use the generalized additive framework for modeling marginal distributions and tail dependence in large concentrations observed at pairs of stations. Our results imply asymptotic independence of data observed at different stations, and we find that the estimated coefficients of tail dependence decrease as a function of spatial distance and show distinct patterns for different years and for different types of stations (traffic vs. background).},
   author = {Linda Mhalla and Thomas Opitz and Valérie Chavez-Demoulin},
   doi = {10.1007/s10687-019-00342-6},
   isbn = {1068701900},
   issn = {1572915X},
   issue = {3},
   journal = {Extremes},
   keywords = {Asymptotic independence,Extreme value theory,Generalized additive models,Penalized likelihood,Tail dependence},
   pages = {523-552},
   title = {Exceedance-based nonlinear regression of tail dependence},
   volume = {22},
   year = {2019},
}
@article{Marcon2017,
   abstract = {The analysis of multiple extreme values aims to describe the stochastic behaviour of observations in the joint upper tail of a distribution function. For instance, being able to simulate multivariate extreme events is convenient for end users who need a large number of random replications of extremes as input of a given complex system to test its sensitivity. The simulation of multivariate extremes is often based on the assumption that the dependence structure, the so-called extremal dependence function, is described by a specific parametric model. We propose a simulation method for sampling bivariate extremes, under the assumption that the extremal dependence function is semiparametric. This yields a flexible tool that can be broadly applied in real-data analyses. With the aim of estimating the probability of belonging to some extreme sets, our methodology is examined via simulation and illustrated by an analysis of strong wind gusts in France.},
   author = {Giulia Marcon and Philippe Naveau and Simone Padoan},
   doi = {10.1002/sta4.145},
   issn = {20491573},
   issue = {1},
   journal = {Stat},
   keywords = {Angular measure,Bernstein polynomials,Bivariate extreme-value distribution,Extremal dependence,Generalized extreme-value distribution,Pickands dependence function,Poisson point process,Wind speed},
   pages = {184-201},
   title = {A semi-parametric stochastic generator for bivariate extreme events},
   volume = {6},
   year = {2017},
}
@article{Mhalla2019a,
   abstract = {We propose a vector generalized additive modeling framework for taking into account the effect of covariates on angular density functions in a multivariate extreme value context. The proposed methods are tailored for settings where the dependence between extreme values may change according to covariates. We devise a maximum penalized log-likelihood estimator, discuss details of the estimation procedure, and derive its consistency and asymptotic normality. The simulation study suggests that the proposed methods perform well in a wealth of simulation scenarios by accurately recovering the true covariate-adjusted angular density. Our empirical analysis reveals relevant dynamics of the dependence between extreme air temperatures in two alpine resorts during the winter season.},
   author = {L. Mhalla and M. de Carvalho and V. Chavez-Demoulin},
   doi = {10.1111/sjos.12388},
   issn = {14679469},
   issue = {4},
   journal = {Scandinavian Journal of Statistics},
   keywords = {VGAM,angular density,covariate adjustment,penalized log-likelihood,statistics of multivariate extremes},
   pages = {1141-1167},
   title = {Regression-type models for extremal dependence},
   volume = {46},
   year = {2019},
}
@article{Cooley2019,
   abstract = {We present a method for drawing isolines indicating regions of equal joint exceedance probability for bivariate data. The method relies on bivariate regular variation, a dependence framework widely used for extremes. The method we utilize for characterizing dependence in the tail is largely nonparametric. The extremes framework enables drawing isolines corresponding to very low exceedance probabilities and may even lie beyond the range of the data; such cases would be problematic for standard nonparametric methods. Furthermore, we extend this method to the case of asymptotic independence and propose a procedure which smooths the transition from hidden regular variation in the interior to the first-order behavior on the axes. We propose a diagnostic plot for assessing the isoline estimate and choice of smoothing, and a bootstrap procedure to visually assess uncertainty.},
   author = {Daniel Cooley and Emeric Thibaud and Federico Castillo and Michael F. Wehner},
   doi = {10.1007/s10687-019-00348-0},
   isbn = {1068701900},
   issn = {1572915X},
   issue = {3},
   journal = {Extremes},
   keywords = {62G32,62H99,Asymptotic independence,Extreme values,Hidden regular variation,Multivariate,Regular variation},
   pages = {373-390},
   publisher = {Extremes},
   title = {A nonparametric method for producing isolines of bivariate exceedance probabilities},
   volume = {22},
   year = {2019},
}
@article{Belzunce2007,
   abstract = {Within the context of a general bivariate distribution an intuitive method is presented in order to study the dependence structure of the two distributions. A set of points-level curve-which accumulate the same probability for a fixed quadrant is considered. This procedure provides four level curves which can be considered as the boundary of a generalization of the real interquantile interval. It is shown that the accumulated probability among the level curves depends on the dependence structure of the distribution function where the dependence structure is given by the notion of copula. Furthermore, the case when the marginal distributions are independent is investigated. This result is used to find out positive or negative dependence properties for the variables. Finally, a nonparametric test for independence with a local dependence meaning is performed and applied to different data sets. © 2006 Elsevier B.V. All rights reserved.},
   author = {F. Belzunce and A. Castaño and A. Olvera-Cervantes and A. Suárez-Llorens},
   doi = {10.1016/j.csda.2006.08.017},
   issn = {01679473},
   issue = {10},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Bivariate quantile,Central region,Copula,Positive or negative dependence,Test for independence},
   pages = {5112-5129},
   title = {Quantile curves and dependence structure for bivariate distributions},
   volume = {51},
   year = {2007},
}
@article{Wadsworth2017,
   abstract = {Different dependence scenarios can arise in multivariate extremes, entailing careful selection of an appropriate class of models. In bivariate extremes, the variables are either asymptotically dependent or are asymptotically independent. Most available statistical models suit one or other of these cases, but not both, resulting in a stage in the inference that is unaccounted for but can substantially impact subsequent extrapolation. Existing modelling solutions to this problem are either applicable only on subdomains or appeal to multiple limit theories. We introduce a unified representation for bivariate extremes that encompasses a wide variety of dependence scenarios and applies when at least one variable is large. Our representation motivates a parametric model that encompasses both dependence classes. We implement a simple version of this model and show that it performs well in a range of settings.},
   author = {J. L. Wadsworth and J. A. Tawn and A. C. Davison and D. M. Elton},
   doi = {10.1111/rssb.12157},
   issn = {14679868},
   issue = {1},
   journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
   keywords = {Asymptotic independence,Censored likelihood,Conditional extremes,Dependence modelling,Extreme value theory,Multivariate regular variation},
   pages = {149-175},
   title = {Modelling across extremal dependence classes},
   volume = {79},
   year = {2017},
}
@article{Coles1999,
   abstract = {Quantifying dependence is a central theme in probabilistic and statistical methods for multivariate extreme values. Two situations are possible: one where, in a limiting sense, the extremes are dependent; the other where, in the same sense, the extremes are independent. This paper comprises an overview of the principal issues through a uni®ed approach which encompasses both these situations. Novel diagnostic measures for dependence are also developed which provide complementary information about different aspects of extremal dependence. The paper is written in an elementary style, with the methodology illustrated by application to theoretical examples and typical data-sets. These data-sets and the S-plus functions used for the analyses are available online.},
   author = {Stuart Coles and Janet Heffernan and Jonathan Tawn},
   issue = {4},
   journal = {Extremes},
   keywords = {asymptotic independence,bivariate extreme value distribution,copula,point processes},
   pages = {339-365},
   title = {Dependence measures for multivariate extremes},
   volume = {2},
   url = {http://www.math.lancs.ac.uk./~coless.},
   year = {1999},
}
@report{Simpson2017,
   author = {Emma S. Simpson and Jennifer L. Wadsworth},
   institution = {https://www.onr.org.uk/documents/2017/onr-rrr-054.pdf},
   pages = {1-25},
   title = {Introduction to Extreme Value Theory and Constructing Hazard Curves},
   url = {https://www.onr.org.uk/documents/2017/onr-rrr-054.pdf},
   year = {2017},
}
@article{Wadsworth2013,
   abstract = {Existing theory for multivariate extreme values focuses upon characterizations of the distributional tails when all components of a random vector, standardized to identical margins, grow at the same rate. In this paper, we consider the effect of allowing the components to grow at different rates, and characterize the link between these marginal growth rates and the multivariate tail probability decay rate. Our approach leads to a whole class of univariate regular variation conditions, in place of the single but multivariate regular variation conditions that underpin the current theories. These conditions are indexed by a homogeneous function and an angular dependence function, which, for asymptotically independent random vectors, mirror the role played by the exponent measure and Pickands' dependence function in classical multivariate extremes. We additionally offer an inferential approach to joint survivor probability estimation. The key feature of our methodology is that extreme set probabilities can be estimated by extrapolating upon rays emanating from the origin when the margins of the variables are exponential. This offers an appreciable improvement over existing techniques where extrapolation in exponential margins is upon lines parallel to the diagonal. © 2013 ISI/BS.},
   author = {J. L. Wadsworth and J. A. Tawn},
   doi = {10.3150/12-BEJ471},
   issn = {13507265},
   issue = {5 B},
   journal = {Bernoulli},
   keywords = {Asymptotic independence,Coefficient of tail dependence,Multivariate extreme value theory,Pickands' dependence function,Regular variation},
   pages = {2689-2714},
   title = {A new representation for multivariate tail probabilities},
   volume = {19},
   year = {2013},
}
@generic{,
   title = {Useful_jon_MV_Notes.pdf},
}
@article{Salvadori2004,
   abstract = {In this paper we provide a general theoretical framework exploiting copulas for studying the return periods of hydrological events; in particular, we consider events depending upon the joint behavior of two nonindependent random variables, an approach which can easily be generalized to the multivariate case. We show that using copulas may greatly simplify the calculations and may even yield analytical expressions for the isolines of the return periods, both in the unconditional and in the conditional case. In addition, we show how a new probability distribution may be associated with the return period of specific events and introduce the definitions of sub-, super-, and critical events as well as those of primary and secondary return periods. An illustration of the techniques proposed is provided by analyzing some case studies already examined in literature.},
   author = {G. Salvadori and C. De Michele},
   doi = {10.1029/2004WR003133},
   issn = {00431397},
   issue = {12},
   journal = {Water Resources Research},
   keywords = {2-Copulas,Return period},
   pages = {1-17},
   title = {Frequency analysis via copulas: Theoretical aspects and applications to hydrological events},
   volume = {40},
   year = {2004},
}
@article{Rootzen2013,
   abstract = {In the past, the concepts of return levels and return periods have been standard and important tools for engineering design. However, these concepts are based on the assumption of a stationary climate and do not apply to a changing climate, whether local or global. In this paper, we propose a refined concept, Design Life Level, which quantifies risk in a nonstationary climate and can serve as the basis for communication. In current practice, typical hydrologic risk management focuses on a standard (e.g., in terms of a high quantile corresponding to the specified probability of failure for a single year). Nevertheless, the basic information needed for engineering design should consist of (i) the design life period (e.g., the next 50 years, say 2015-2064); and (ii) the probability (e.g., 5% chance) of a hazardous event (typically, in the form of the hydrologic variable exceeding a high level) occurring during the design life period. Capturing both of these design characteristics, the Design Life Level is defined as an upper quantile (e.g., 5%) of the distribution of the maximum value of the hydrologic variable (e.g., water level) over the design life period. We relate this concept and variants of it to existing literature and illustrate how they, and some useful complementary plots, may be computed and used. One practically important consideration concerns quantifying the statistical uncertainty in estimating a high quantile under nonstationarity. Key Points The concepts return level and return period do not apply to a changing climate We propose a concept, Design Life Level, which gives key information for design We illustrate by rainfall in Western Australia and warm winters in Fort Collins ©2013. American Geophysical Union. All Rights Reserved.},
   author = {Holger Rootzén and Richard W. Katz},
   doi = {10.1002/wrcr.20425},
   issn = {00431397},
   issue = {9},
   journal = {Water Resources Research},
   keywords = {climate change,design criteria,exceedance risk,extreme value statistics,nonstationary,return level,return period},
   pages = {5964-5972},
   title = {Design Life Level: Quantifying risk in a changing climate},
   volume = {49},
   year = {2013},
}
@article{Keef2013,
   abstract = {A number of different approaches to study multivariate extremes have been developed. Arguably the most useful and flexible is the theory for the distribution of a vector variable given that one of its components is large. We build on the conditional approach of Heffernan and Tawn (2004). [13] for estimating this type of multivariate extreme property. Specifically we propose additional constraints for, and slight changes in, their model formulation. These changes in the method are aimed at overcoming complications that have been experienced with using the approach in terms of their modelling of negatively associated variables, parameter identifiability problems and drawing conditional inferences which are inconsistent with the marginal distributions. The benefits of the methods are illustrated using river flow data from two tributaries of the River Thames in the UK. © 2012 Elsevier Inc.},
   author = {Caroline Keef and Ioannis Papastathopoulos and Jonathan A. Tawn},
   doi = {10.1016/j.jmva.2012.10.012},
   issn = {0047259X},
   journal = {Journal of Multivariate Analysis},
   pages = {396-404},
   publisher = {Elsevier Inc.},
   title = {Estimation of the conditional distribution of a multivariate variable given that one of its components is large: Additional constraints for the Heffernan and Tawn model},
   volume = {115},
   url = {http://dx.doi.org/10.1016/j.jmva.2012.10.012},
   year = {2013},
}
@article{Ledford1997,
   abstract = {Standard approaches for modelling dependence within joint tail regions are based on extreme value methods which assume max-stability, a particular form of joint tail dependence. We develop joint tail models based on a broader class of dependence structure which provides a natural link between max-stable models and weaker forms of dependence including independence and negative association. This approach overcomes many of the problems that are encountered with standard methods and is the basis for a Poisson process representation that generalizes existing bivariate results. We apply the new techniques to simulated and environmental data, and demonstrate the marked advantage that the new approach offers for joint tail extrapolation. © 1997 Royal Statistical Society.},
   author = {Anthony W. Ledford and Jonathan A. Tawn},
   doi = {10.1111/1467-9868.00080},
   issn = {13697412},
   issue = {2},
   journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
   keywords = {Asymptotic independence,Coefficient of tail dependence,Componentwise maxima,Extreme value theory,Maximum likelihood,Non-homogeneous poisson process,Rates of convergence,Slowly varying functions},
   pages = {475-499},
   title = {Modelling dependence within joint tail regions},
   volume = {59},
   year = {1997},
}
@article{Kiriliouk2019,
   abstract = {When assessing the impact of extreme events, it is often not just a single component, but the combined behavior of several components which is important. Statistical modeling using multivariate generalized Pareto (GP) distributions constitutes the multivariate analogue of univariate peaks over thresholds modeling, which is widely used in finance and engineering. We develop general methods for construction of multivariate GP distributions and use them to create a variety of new statistical models. A censored likelihood procedure is proposed to make inference on these models, together with a threshold selection procedure, goodness-of-fit diagnostics, and a computationally tractable strategy for model selection. The models are fitted to returns of stock prices of four UK-based banks and to rainfall data in the context of landslide risk estimation. Supplementary materials and codes are available online.},
   author = {Anna Kiriliouk and Holger Rootzén and Johan Segers and Jennifer L. Wadsworth},
   doi = {10.1080/00401706.2018.1462738},
   issn = {15372723},
   issue = {1},
   journal = {Technometrics},
   keywords = {Financial risk,Landslides,Multivariate extremes,Tail dependence},
   pages = {123-135},
   title = {Peaks Over Thresholds Modeling With Multivariate Generalized Pareto Distributions},
   volume = {61},
   year = {2019},
}
@article{Ledford1996,
   abstract = {We propose a multivariate extreme value threshold model for joint tail estimation which overcomes the problems encountered with existing techniques when the variables are near independence. We examine inference under the model and develop tests for independence of extremes of the marginal variables, both when the thresholds are fixed, and when they increase with the sample size. Motivated by results obtained from this model, we give a new and widely applicable characterisation of dependence in the joint tail which includes existing models as special cases. A new parameter which governs the form of dependence is of fundamental importance to this characterisation. By estimating this parameter, we develop a diagnostic test which assesses the applicability of bivariate extreme value joint tail models. The methods are demonstrated through simulation and by analysing two previously published data sets.},
   author = {Anthony W. Ledford and Jonathan A. Tawn},
   doi = {10.1093/biomet/83.1.169},
   issn = {00063444},
   issue = {1},
   journal = {Biometrika},
   keywords = {Asymptotic independence,Coefficient of tail dependence,Extreme value theory,Generalised Pareto distribution,Maximum likelihood,Multivariate extreme value distribution,Nonregular estimation,Poisson process,Threshold exceedance},
   pages = {169-187},
   title = {Statistics for near independence in multivariate extreme values},
   volume = {83},
   year = {1996},
}
@article{Huser2020,
   abstract = {The classical modeling of spatial extremes relies on asymptotic models (i.e., max-stable processes or $r$-Pareto processes) for block maxima or peaks over high thresholds, respectively. However, at finite levels, empirical evidence often suggests that such asymptotic models are too rigidly constrained, and that they do not adequately capture the frequent situation where more severe events tend to be spatially more localized. In other words, these asymptotic models have a strong tail dependence that persists at increasingly high levels, while data usually suggest that it should weaken instead. Another well-known limitation of classical spatial extremes models is that they are either computationally prohibitive to fit in high dimensions, or they need to be fitted using less efficient techniques. In this review paper, we describe recent progress in the modeling and inference for spatial extremes, focusing on new models that have more flexible tail structures that can bridge asymptotic dependence classes, and that are more easily amenable to likelihood-based inference for large datasets. In particular, we discuss various types of random scale constructions, as well as the conditional spatial extremes model, which have recently been getting increasing attention within the statistics of extremes community. We illustrate some of these new spatial models on two different environmental applications.},
   author = {Raphaël Huser and Jennifer L. Wadsworth},
   keywords = {and engineering,and technology,asymptotic independence and dependence,cemse,computer,conditional spatial extremes model,division,dullah university of science,e-mail,electrical and mathematical sciences,kaust,king ab-,max-stable process,pareto process,random scale mixtures,saudi arabia,sub-asymptotic tail modeling,thuwal 23955-6900},
   pages = {1-59},
   title = {Advances in Statistical Modeling of Spatial Extremes},
   url = {http://arxiv.org/abs/2007.00774},
   year = {2020},
}
@article{Davison1990,
   author = {A. C. Davison and R. L. Smith},
   doi = {10.1111/j.2517-6161.1990.tb01796.x},
   issn = {00359246},
   issue = {3},
   journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
   month = {7},
   pages = {393-425},
   title = {Models for Exceedances Over High Thresholds},
   volume = {52},
   url = {http://doi.wiley.com/10.1111/j.2517-6161.1990.tb01796.x},
   year = {1990},
}
@article{Tawn1992,
   author = {Jonathan A. Tawn},
   doi = {10.2307/2347619},
   issn = {00359254},
   issue = {1},
   journal = {Applied Statistics},
   keywords = {extremal index,extreme value theory,generalized extreme value distribution,joint,maximum likelihood estimation,non-stationarity,probabilities method,sea-level,tides},
   pages = {77},
   title = {Estimating Probabilities of Extreme Sea-Levels},
   volume = {41},
   url = {https://www.jstor.org/stable/10.2307/2347619?origin=crossref},
   year = {1992},
}
@article{Naveau2016,
   author = {Philippe Naveau and Raphael Huser and Pierre Ribereau and Alexis Hannart},
   doi = {10.1002/2015WR018552},
   issn = {00431397},
   issue = {4},
   journal = {Water Resources Research},
   month = {4},
   pages = {2753-2769},
   title = {Modeling jointly low, moderate, and heavy rainfall intensities without a threshold selection},
   volume = {52},
   url = {http://doi.wiley.com/10.1002/2015WR018552},
   year = {2016},
}
@article{Dupuis2019,
   author = {Debbie J. Dupuis and Luca Trapin},
   doi = {10.1214/18-AOAS1183},
   issn = {1932-6157},
   issue = {1},
   journal = {The Annals of Applied Statistics},
   month = {3},
   pages = {34-59},
   title = {Ground-level ozone: Evidence of increasing serial dependence in the extremes},
   volume = {13},
   url = {https://projecteuclid.org/euclid.aoas/1554861640},
   year = {2019},
}
@article{Coles2002,
   author = {Stuart Coles and Francesco. Pauli},
   doi = {10.1093/biomet/89.1.183},
   issn = {0006-3444},
   issue = {1},
   journal = {Biometrika},
   month = {3},
   pages = {183-196},
   title = {Models and inference for uncertainty in extremal dependence},
   volume = {89},
   url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/89.1.183},
   year = {2002},
}
@article{Vignotto2020,
   abstract = {Classification tasks usually assume that all possible classes are present during the training phase. This is restrictive if the algorithm is used over a long time and possibly encounters samples from unknown new classes. It is therefore fundamental to develop algorithms able to distinguish between normal and abnormal test data. In the last few years, extreme value theory has become an important tool in multivariate statistics and machine learning. The recently introduced extreme value machine, a classifier motivated by extreme value theory, addresses this problem and achieves competitive performance in specific cases. We show that this algorithm has some theoretical and practical drawbacks and can fail even if the recognition task is fairly simple. To overcome these limitations, we propose two new algorithms for anomaly detection relying on approximations from extreme value theory that are more robust in such cases. We exploit the intuition that test points that are extremely far from the training classes are more likely to be abnormal objects. We derive asymptotic results motivated by univariate extreme value theory that make this intuition precise. We show the effectiveness of our classifiers in simulations and on real data sets.},
   author = {Edoardo Vignotto and Sebastian Engelke},
   doi = {10.1007/s10687-020-00393-0},
   issn = {1572915X},
   journal = {Extremes},
   keywords = {Clustering,Machine learning,Novelty detection,Open set classification,Statistical methods},
   pages = {501-520},
   publisher = {Extremes},
   title = {Extreme value theory for anomaly detection – the GPD classifier},
   volume = {1},
   year = {2020},
}
@article{Hill1975,
   author = {Bruce M. Hill},
   doi = {10.1214/aos/1176343247},
   issn = {0090-5364},
   issue = {5},
   journal = {The Annals of Statistics},
   month = {9},
   pages = {1163-1174},
   title = {A Simple General Approach to Inference About the Tail of a Distribution},
   volume = {3},
   url = {http://projecteuclid.org/euclid.aos/1176343247},
   year = {1975},
}
@article{Siffer2017,
   abstract = {Anomaly detection in time series has attracted considerable attention due to its importance in many real-world applications including intrusion detection, energy management and finance. Most approaches for detecting outliers rely on either manually set thresholds or assumptions on the distribution of data according to Chandola, Banerjee and Kumar. Here, we propose a new approach to detect outliers in streaming univariate time series based on Extreme Value Theory that does not require to hand-set thresholds and makes no assumption on the distribution: the main parameter is only the risk, controlling the number of false positives. Our approach can be used for outlier detection, but more generally for automatically setting thresholds, making it useful in wide number of situations. We also experiment our algorithms on various real-world datasets which confirm its soundness and efficiency.},
   author = {Alban Siffer and Pierre Alain Fouque and Alexandre Termier and Christine Largouet},
   doi = {10.1145/3097983.3098144},
   isbn = {9781450348874},
   journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {Extreme value theory,Outliers in time series,Streaming},
   pages = {1067-1075},
   title = {Anomaly detection in streams with extreme value theory},
   volume = {Part F1296},
   year = {2017},
}
@article{Scarrott2012,
   abstract = {The last decade has seen development of a plethora of approaches for threshold estimation in extreme value applications. From a statistical perspective, the threshold is loosely defined such that the population tail can be well approximated by an extreme value model (e.g., the generalised Pareto distribution), obtaining a balance between the bias due to the asymptotic tail approximation and parameter estimation uncertainty due to the inherent sparsity of threshold excess data. This paper reviews recent advances and some traditional approaches, focusing on those that provide quantification of the associated uncertainty on inferences (e.g., return level estimation).},
   author = {Carl Scarrott and Anna MacDonald},
   issn = {16456726},
   issue = {1},
   journal = {Revstat Statistical Journal},
   keywords = {Extreme value threshold selection,Graphical diagnostics,Mixture modelling,Rule of thumb,Threshold uncertainty},
   pages = {33-60},
   title = {A review of extreme value threshold estimation and uncertainty quantification},
   volume = {10},
   year = {2012},
}
@article{MacDonald2011,
   abstract = {Extreme value theory is used to derive asymptotically motivated models for unusual or rare events, e.g. the upper or lower tails of a distribution. A new flexible extreme value mixture model is proposed combining a non-parametric kernel density estimator for the bulk of the distribution with an appropriate tail model. The complex uncertainties associated with threshold choice are accounted for and new insights into the impact of threshold choice on density and quantile estimates are obtained. Bayesian inference is used to account for all uncertainties and enables inclusion of expert prior information, potentially overcoming the inherent sparsity of extremal data. A simulation study and empirical application for determining normal ranges for physiological measurements for pre-term infants is used to demonstrate the performance of the proposed mixture model. The potential of the proposed model for overcoming the lack of consistency of likelihood based kernel bandwidth estimators when faced with heavy tailed distributions is also demonstrated. © 2011 Elsevier B.V. All rights reserved.},
   author = {A. MacDonald and C. J. Scarrott and D. Lee and B. Darlow and M. Reale and G. Russell},
   doi = {10.1016/j.csda.2011.01.005},
   issn = {01679473},
   issue = {6},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Extreme values,Kernel density,Mixture model,Threshold selection},
   pages = {2137-2157},
   publisher = {Elsevier B.V.},
   title = {A flexible extreme value mixture model},
   volume = {55},
   url = {http://dx.doi.org/10.1016/j.csda.2011.01.005},
   year = {2011},
}
@book{Beirlant2004,
   abstract = {Research in the statistical analysis of extreme values has flourished over the past decade: new probability models, inference and data analysis techniques have been introduced; and new application areas have been explored. Statistics of Extremes comprehensively covers a wide range of models and application areas, including risk and insurance: a major area of interest and relevance to extreme value theory. Case studies are introduced providing a good balance of theory and application of each model discussed, incorporating many illustrated examples and plots of data. The last part of the book covers some interesting advanced topics, including time series, regression, multivariate and Bayesian modelling of extremes, the use of which has huge potential.},
   author = {Jan Beirlant and Yuri Goegebeur and Jozef Teugels and Johan Segers and Daniel De Waal and Chris Ferro},
   doi = {10.1002/0470012382},
   isbn = {9780470012383},
   journal = {Statistics of Extremes: Theory and Applications},
   pages = {1-490},
   publisher = {John Wiley \& Sons, Inc.},
   title = {Statistics of extremes: Theory and applications},
   year = {2004},
}
@book{Rosenthal2006a,
   abstract = {2nd ed. The need for measure theory -- Probability triples -- Further probabilistic foundations -- Expected values -- Inequalities and convergence -- Distributions of random variables -- Stochastic processes and gambling games -- Discrete Markov chains -- More probability theorems -- Weak convergence -- Characteristic functions -- Decomposition of probability laws -- Conditional probability and expectation -- Martingales -- General stochastic processes -- Mathematical background.},
   author = {Jeffrey S. (Jeffrey Seth) Rosenthal},
   isbn = {9812703713},
   pages = {219},
   title = {Exercise solutions--A first look at rigorous probability theory},
   year = {2006},
}
@book{Wood2017,
   abstract = {This book imparts a thorough understanding of the theory and practical applications of GAMs and related advanced models, enabling informed use of these very flexible tools. The author bases his approach on a framework of penalized regression splines, and builds a well- grounded foundation through motivating chapters on linear and generalized linear models. While firmly focused on the practical aspects of GAMs, discussions include fairly full explanations of the theory underlying the methods. The treatment is rich with practical examples, and it includes an entire chapter on the analysis of real data sets using R and the author's add-on package mgcv. Each chapter includes exercises, for which complete solutions are provided in an appendix.},
   author = {Simon N. Wood},
   doi = {10.1201/9781315370279},
   isbn = {9781315370279},
   issn = {0964-1998},
   month = {5},
   publisher = {Chapman and Hall/CRC},
   title = {Generalized Additive Models},
   url = {https://www.taylorfrancis.com/books/9781498728348},
   year = {2017},
}
@book{Coles2001,
   abstract = {An Introduction to Statistical Modeling of Extreme Values},
   author = {Stuart Coles},
   city = {London},
   doi = {10.1007/978-1-4471-3675-0},
   isbn = {978-1-84996-874-4},
   publisher = {Springer London},
   title = {An Introduction to Statistical Modeling of Extreme Values},
   url = {http://link.springer.com/10.1007/978-1-4471-3675-0},
   year = {2001},
}
@book{Rosenthal2006,
   author = {Jeffrey S Rosenthal},
   doi = {10.1142/6300},
   isbn = {978-981-270-370-5},
   month = {11},
   publisher = {WORLD SCIENTIFIC},
   title = {A First Look at Rigorous Probability Theory},
   url = {https://www.worldscientific.com/worldscibooks/10.1142/6300},
   year = {2006},
}
@book{Leadbetter1983,
   author = {M. R. Leadbetter and Georg Lindgren and Holger Rootzén},
   city = {New York, NY},
   doi = {10.1007/978-1-4612-5449-2},
   isbn = {978-1-4612-5451-5},
   publisher = {Springer New York},
   title = {Extremes and Related Properties of Random Sequences and Processes},
   url = {http://link.springer.com/10.1007/978-1-4612-5449-2},
   year = {1983},
}
@article{Pickands1975,
   author = {James Pickands},
   doi = {10.1214/aos/1176343003},
   issn = {0090-5364},
   issue = {1},
   journal = {The Annals of Statistics},
   month = {1},
   pages = {119-131},
   title = {Statistical Inference Using Extreme Order Statistics},
   volume = {3},
   url = {http://projecteuclid.org/euclid.aos/1176343003},
   year = {1975},
}
@article{Sklar1959,
   author = {Abe Sklar},
   journal = {Publ. Inst. Statist. Univ. Paris},
   pages = {229-231},
   title = {Fonctions de repartition a n dimensions et leurs marges},
   volume = {8},
   url = {http://ci.nii.ac.jp/naid/10011938360/en/},
   year = {1959},
}
@book{Ruschendorf2013,
   author = {Ludger Rüschendorf},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-33590-7},
   isbn = {978-3-642-33589-1},
   publisher = {Springer Berlin Heidelberg},
   title = {Mathematical Risk Analysis},
   url = {http://link.springer.com/10.1007/978-3-642-33590-7},
   year = {2013},
}
@article{Barnett1976,
   author = {V. Barnett},
   doi = {10.2307/2344839},
   issn = {00359238},
   issue = {3},
   journal = {Journal of the Royal Statistical Society. Series A (General)},
   pages = {318},
   title = {The Ordering of Multivariate Data},
   volume = {139},
   url = {https://www.jstor.org/stable/10.2307/2344839?origin=crossref},
   year = {1976},
}
@article{Balkema1977,
   abstract = {Necessary and sufficient conditions are given for a distribution function in ℝ 2 to be max-infinitely divisible. The d.f. F is max i.d. if F t is a d.f. for every t > 0. This property is essential in defining multivariate extremal processes and arises in an approach to the study of the range of an i.i.d. sample.},
   author = {A. A. Balkema and S. I. Resnick},
   doi = {10.2307/3213001},
   issn = {0021-9002},
   issue = {2},
   journal = {Journal of Applied Probability},
   month = {6},
   pages = {309-319},
   title = {Max-infinite divisibility},
   volume = {14},
   url = {https://www.cambridge.org/core/product/identifier/S002190020010498X/type/journal_article},
   year = {1977},
}
@article{Pickands1981,
   author = {James Pickands},
   journal = {Proceedings 43th, Session of International Statistical Institution, 1981},
   title = {Multivariate extreme value distribution},
   url = {http://ci.nii.ac.jp/naid/10022049959/en/},
   year = {1981},
}
@book{Resnick1987,
   author = {Sidney I. Resnick},
   city = {New York},
   doi = {10.1007/978-0-387-75953-1},
   isbn = {978-0-387-75952-4},
   publisher = {Springer New York},
   title = {Extreme Values, Regular Variation and Point Processes},
   url = {http://link.springer.com/10.1007/978-0-387-75953-1},
   year = {1987},
}
@article{Stephenson2002,
   author = {Alec Stephenson},
   issue = {2},
   journal = {R News},
   title = {evd: Extreme Value Distributions},
   volume = {2},
   url = {https://cran.r-project.org/doc/Rnews/},
   year = {2002},
}
@article{Nicolet2018,
   abstract = {Modeling extreme snow depths in space is important for water storage, tourism industry, mountain ecosystems, collapse of buildings, and avalanche prevention. However, studies modeling the spatial dependence structure of extremes generally assume temporal stationarity which is clearly questionable in a climate change context. We model climatic trends within the spatial dependence structure of extremes, with application to a data set of snow depth winter maxima. From 82 stations spanning the 1970–2012 period in the French Alps, we infer a strong decrease in the range of spatial extremal dependence. This finding is related to a strong decrease in both the snow precipitation ratio and the winter cumulated snowfall, due to increasing temperatures. Hence, we demonstrate that the spatial dependence of extreme snow depths is impacted by climate change in a similar way as has been observed for extreme snowfalls. Furthermore, snow depths maxima are more spatially dependent than snowfalls. The space-time approach that we introduce may be very useful for assessing past and future evolutions under ongoing climate change in various hydrological quantities.},
   author = {Gilles Nicolet and Nicolas Eckert and Samuel Morin and Juliette Blanchet},
   doi = {10.1029/2018WR022763},
   issn = {19447973},
   issue = {10},
   journal = {Water Resources Research},
   keywords = {French Alps,climate change,extreme value statistic,max-stable processes,snow depth,spatial dependence},
   pages = {7820-7840},
   title = {Assessing Climate Change Impact on the Spatial Dependence of Extreme Snow Depth Maxima in the French Alps},
   volume = {54},
   year = {2018},
}
@article{Tawn1990,
   abstract = {Multivariate extreme value distributions arise as the limiting joint distribution of normalized componentwise maxima/minima. No parametric family exists for the dependence between the margins. This paper extends to more than two variables the models and results for the bivariate case obtained by Tawn (1988). Two new families of physically motivated parametric models for the dependence structure are presented and are illustrated with an application to trivariate extreme sea level data. © 1990 Biometrika Trust.},
   author = {Jonathan A. Tawn},
   doi = {10.1093/biomet/77.2.245},
   issn = {00063444},
   issue = {2},
   journal = {Biometrika},
   keywords = {Extreme value theory,Generalized Pareto distribution,Multivariate exponential distribution,Nonregular estimation},
   pages = {245-253},
   title = {Modelling multivariate extreme value distributions},
   volume = {77},
   year = {1990},
}
@report{Barltrop2019,
   author = {Callum Barltrop},
   institution = {Lancaster University},
   title = {Multivariate Extremes for Nuclear Regulation - PhD Proposal},
   year = {2019},
}
@report{Barltrop2020,
   author = {Callum Barltrop},
   institution = {Lancaster University},
   title = {Multivariate Extremes for Nuclear Regulation - 10 Month Review},
   year = {2020},
}
@article{Leonelli2020,
   abstract = {Inference over multivariate tails often requires a number of assumptions which may affect the assessment of the extreme dependence structure. Models are usually constructed in such a way that extreme components can either be asymptotically dependent or be independent of each other. Recently, there has been an increasing interest on modelling multivariate extremes more flexibly, by allowing models to bridge both asymptotic dependence regimes. Here we propose a novel semiparametric approach which allows for a variety of dependence patterns, be them extremal or not, by using in a model-based fashion the full dataset. We build on previous work for inference on marginal exceedances over a high, unknown threshold, by combining it with flexible, semiparametric copula specifications to investigate extreme dependence, thus separately modelling marginals and dependence structure. Because of the generality of our approach, bivariate problems are investigated here due to computational challenges, but multivariate extensions are readily available. Empirical results suggest that our approach can provide sound uncertainty statements about the possibility of asymptotic independence, and we propose a criterion to quantify the presence of either extreme regime which performs well in our applications when compared to others available. Estimation of functions of interest for extremes is performed via MCMC algorithms. Attention is also devoted to the prediction of new extreme observations. Our approach is evaluated through simulations, applied to real data and assessed against competing approaches. Evidence demonstrates that the bulk of the data do not bias and improve the inferential process for extremal dependence in our applications.},
   author = {Manuele Leonelli and Dani Gamerman},
   doi = {10.1007/s11222-019-09878-w},
   issn = {15731375},
   issue = {2},
   journal = {Statistics and Computing},
   keywords = {Asymptotic dependence,Copulae,GPD distribution,High quantiles,Prediction,Threshold estimation},
   pages = {221-236},
   publisher = {Springer US},
   title = {Semiparametric bivariate modelling with flexible extremal dependence},
   volume = {30},
   url = {https://doi.org/10.1007/s11222-019-09878-w},
   year = {2020},
}
@article{Chavez-Demoulin2012,
   abstract = {The need to model rare events of univariate time series has led to many recent advances in theory and methods. In this paper, we review telegraphically the literature on extremes of dependent time series and list some remaining challenges.},
   author = {V Chavez-Demoulin and A C Davison},
   issue = {1},
   journal = {REVSTAT-Statistical Journal},
   keywords = {62,Box-Cox transformation,Hill estimator,Key-Words: • Bayesian statistics,clustering,dependence,extremal index,extremogram,generalized Pareto distribution,generalized extreme-value distribution,non-stationarity,nonparametric smoothing,regression,tail index AMS Subject Classification: • 62E20},
   pages = {109-133},
   title = {Modelling time series extremes},
   volume = {10},
   url = {http://infoscience.epfl.ch/record/180506},
   year = {2012},
}
@article{Xi2019,
   abstract = {Fire danger systems have evolved from qualitative indices, to process-driven deterministic models of fire behavior and growth, to data-driven stochastic models of fire occurrence and simulation systems. However, there has often been little overlap or connectivity in these frameworks, and validation has not been common in deterministic models. Yet, marked increases in annual fire costs, losses, and fatality costs over the past decade draw attention to the need for better understanding of fire risk to support fire management decision making through the use of science-backed, data-driven tools. Contemporary risk modeling systems provide a useful integrative framework. This article discusses a variety of important contributions for modeling fire risk components over recent decades, certain key fire characteristics that have been overlooked, and areas of recent research that may enhance risk models.},
   author = {Dexen D.Z. Xi and Stephen W. Taylor and Douglas G. Woolford and C. B. Dean},
   doi = {10.1146/annurev-statistics-031017-100450},
   issn = {2326831X},
   journal = {Annual Review of Statistics and Its Application},
   keywords = {fire duration,fire load,fire occurrence,fire survival,joint modeling,wildfire management,wildfire science,wildland fire},
   pages = {197-222},
   title = {Statistical models of key components of wildfire risk},
   volume = {6},
   year = {2019},
}
@article{Castro-Camilo2018,
   abstract = {Extremal dependence between international stock markets is of particular interest in today’s global financial landscape. However, previous studies have shown this dependence is not necessarily stationary over time. We concern ourselves with modeling extreme value dependence when that dependence is changing over time, or other suitable covariate. Working within a framework of asymptotic dependence, we introduce a regression model for the angular density of a bivariate extreme value distribution that allows us to assess how extremal dependence evolves over a covariate. We apply the proposed model to assess the dynamics governing extremal dependence of some leading European stock markets over the last three decades, and find evidence of an increase in extremal dependence over recent years.},
   author = {Daniela Castro-Camilo and Miguel de Carvalho and Jennifer Wadsworth},
   doi = {10.1214/17-AOAS1089},
   issn = {19417330},
   issue = {1},
   journal = {Annals of Applied Statistics},
   keywords = {Angular measure,Bivariate extreme values,European stock market integration,Risk,Statistics of extremes},
   pages = {283-309},
   title = {Time-varying extreme value dependence with application to leading european stock markets},
   volume = {12},
   year = {2018},
}
@article{Manuel2018,
   abstract = {It is necessary to evaluate site-specific extreme environmental conditions in the design of wave energy converters (WECs) as well as other offshore structures. As WECs are generally resonance-driven devices, critical metocean parameters associated with a target return period of interest (e.g., 50 years) must generally be established using combinations, say, of significant wave height and spectral peak period, as opposed to identifying single-valued wave height levels alone. We present several methods for developing so-called “environmental contours” for any target return period. The environmental contour (EC) method has been widely acknowledged as an efficient way to derive design loads for offshore oil and gas platforms and for land-based as well as offshore wind turbines. The use of this method for WECs is also being considered. A challenge associated with its use relates to the need to accurately characterize the uncertainties in metocean variables that define the “environment”. The joint occurrence frequency of values of two or more random variables needs to be defined formally. There are many ways this can be done—the most thorough and complete of these is to define a multivariate joint probability distribution of the random variables. However, challenges arise when data from the site where the WEC device is to be deployed are limited, making it difficult to estimate the joint probability distribution. A more easily estimated set of inputs consists of marginal distribution functions for each random variable and pairwise correlation coefficients. Pearson correlation coefficients convey information that rely on up to the second moment of each variable and on the expected value of the product of the paired variables. Kendall’s rank correlation coefficients, on the other hand, convey information on similarity in the “rank” of two variables and are useful especially in dealing with extreme values. The EC method is easily used with Rosenblatt transformations when joint distributions are available. In cases where Pearson’s correlation coefficients have been estimated along with marginal distributions, a Nataf transformation can be used, and if Kendall’s rank coefficients have been estimated and are available, a copula-based transformation can be used. We demonstrate the derivation of 50-year sea state parameters using the EC method with all three approaches where we consider data from the National Data Buoy Center Station 46022 (which can be considered the site for potential WEC deployment). A comparison of the derived environmental contours using the three approaches is presented. The focus of this study is on investigating differences between the derived environmental contours and, thus, on associated sea states arising from the different dependence structure assumptions for the metocean random variables. Both parametric and non-parametric approaches are used to define the probability distributions.},
   author = {Lance Manuel and Phong T.T. Nguyen and Jarred Canning and Ryan G. Coe and Aubrey C. Eckert-Gallup and Nevin Martin},
   doi = {10.1007/s40722-018-0123-0},
   issn = {21986452},
   issue = {4},
   journal = {Journal of Ocean Engineering and Marine Energy},
   keywords = {Environmental contour method,Extremes,Metocean data,copula},
   pages = {293-310},
   publisher = {Springer International Publishing},
   title = {Alternative approaches to develop environmental contours from metocean data},
   volume = {4},
   url = {https://doi.org/10.1007/s40722-018-0123-0},
   year = {2018},
}
@article{Gouldby2017a,
   abstract = {It is widely recognised that coastal flood events can arise from combinations of extreme waves and sea levels. For flood risk analysis and the design of coastal structures it is therefore necessary to assess the joint probability of the occurrence of these variables. Traditional methods have involved the application of joint probability contours, defined in terms of extremes of sea conditions that can, if applied without correction factors, lead to the underestimation of flood risk and under-design of coastal structures. This paper describes the application of a robust multivariate statistical model to analyse extreme offshore waves, wind and sea levels around the coast of England. The approach described here is risk based in that it seeks to define extremes of response variables directly, rather than the joint extremes of sea conditions. The output of the statistical model comprises a Monte Carlo simulation of extreme events. These distributions of extreme events have been transformed from offshore to nearshore using a statistical emulator of a wave transformation model. The resulting nearshore extreme sea condition distributions have the potential to be applied for a range of purposes. The application is demonstrated using two structures located on the south coast of England.},
   author = {Ben Gouldby and David Wyncoll and Mike Panzeri and Mark Franklin and Tim Hunt and Dominic Hames and Nigel Tozer and Peter Hawkes and Uwe Dornbusch and Tim Pullen},
   doi = {10.1680/jmaen.2016.16},
   issn = {17517737},
   issue = {1},
   journal = {Proceedings of the Institution of Civil Engineers: Maritime Engineering},
   keywords = {Floods & floodworks,Maritime engineering,Risk & probability analysis},
   pages = {3-20},
   title = {Multivariate extreme value modelling of sea conditions around the coast of England},
   volume = {170},
   year = {2017},
}
@article{DNV2011,
   abstract = {A three-dimensional rotational failure mechanism for earth slope is extended from toe failure to include face failure and base failure. An efficient optimisation method is simultaneously employed to find the least upper bounds to the critical height in order to avoid missing the global minimum. Compared with the results from analysis based on toe failure alone, best estimates of the critical height and the critical failure mechanism are obtained. The calculated results are given in the form of graphs and tables for a wide range of parameters. The critical failure surfaces are also investigated to assess the influences of geometrical constraint and soil property on failure mechanism.},
   author = {DNV},
   isbn = {DNV-OS-C101},
   issue = {April},
   journal = {Det Norske Veritas},
   pages = {49},
   title = {Design of Offshore Steel Structures , General - LRFD Method},
   volume = {2018 Ed.},
   year = {2011},
}
@article{Valamanesh2015,
   abstract = {Most offshore wind turbines (OWTs) are designed according to the international standard IEC 61400-3 which requires consideration of several design load cases under 50-year extreme storm conditions during which the wind turbine is not operational (i.e. the rotor is parked and blades are feathered). Each of these load cases depends on combinations of at least three jointly distributed metocean parameters, the mean wind speed, the significant wave height, and the peak spectral period. In practice, these variables are commonly estimated for the 50-year extreme storm using a simple but coarse method, wherein 50-year values of wind speed and wave height are calculated independently and combined with a range of peak spectral period conditioned on the 50-year wave height. The IEC Standard does not provide detailed guidance on how to calculate the appropriate range of peak spectral period. Given the varying correlation of these parameters from site-to-site, this approach is clearly an approximation which is assumed to overestimate structural loads since wind and wave are combined without regard to their correlation. In this paper, we introduce an alternative multivariate method for assessing extreme storm conditions. The method is based on the Nataf model and the Inverse First Order Reliability Method (IFORM) and uses measurements or hindcasts of wind speed, wave height and peak spectral period to estimate an environmental surface which defines combinations of these parameters with a particular recurrence period. The method is illustrated using three sites along the U.S. Atlantic coast near Maine, Delaware and Georgia. Mudline moments are calculated using this new multivariate method for a hypothetical 5. MW OWT supported by a monopile and compared with mudline moments calculated using simpler univariate approaches. The results of the comparison highlight the importance of selecting an appropriate range of the peak spectral period when using the simpler univariate approaches.},
   author = {V. Valamanesh and A. T. Myers and S. R. Arwade},
   doi = {10.1016/j.strusafe.2015.03.002},
   issn = {01674730},
   journal = {Structural Safety},
   keywords = {Extreme value analysis,Inverse First Order Reliability Method,Multivariate Metocean Hazard,Offshore wind turbine},
   pages = {60-69},
   title = {Multivariate analysis of extreme metocean conditions for offshore wind turbines},
   volume = {55},
   year = {2015},
}
@article{Eckert-Gallup2016,
   abstract = {The estimation of environmental contours of extreme sea states characterized by significant wave height and energy period for the purposes of reliability-based offshore design is a problem that has been tackled in many different ways. Many of the methods used to generate such contours rely on parametric approaches that require an a priori assumption of the relationship between the variables of interest. These relationships, often given in the form of assumed probability distributions or joint probability structures, may not be flexible across a wide variety of global observation sites. We propose the use of bivariate kernel density estimation (KDE) with adaptive bandwidth selection for generating the joint probability distribution of significant wave height and energy period. This method is nonparametric, straightforward to apply, and lends itself to a characterization of contour uncertainty that is an important aspect when contours are used to generate inputs for numerical or physical simulations of offshore structures. The joint probability distribution of significant wave height and energy period can be queried using a return period of interest to determine environmental contours of extreme sea states. This paper demonstrates that this method provides a robust and flexible characterization when compared to other approaches.},
   author = {Aubrey Eckert-Gallup and Nevin Martin},
   doi = {10.1109/OCEANS.2016.7761150},
   isbn = {9781509015375},
   journal = {OCEANS 2016 MTS/IEEE Monterey, OCE 2016},
   keywords = {Adaptive bandwidth selection,Environmental contours,Extreme sea state characterization,Kernel density estimation},
   pages = {1-5},
   title = {Kernel density estimation (KDE) with adaptive bandwidth selection for environmental contours of extreme sea states},
   year = {2016},
}
@article{Velarde2019,
   abstract = {Offshore wind turbines can exhibit dynamic resonant behavior due to sea states with wave excitation frequencies coinciding with the structural eigenfrequencies. In addition to significant contributions to fatigue actions, dynamic load amplification can govern extreme wind turbine responses. However, current design requirements lack specifications for assessment of resonant loads, particularly during parked or idling conditions where aerodynamic damping contributions are significantly reduced. This study demonstrates a probabilistic approach for assessment of offshore wind turbines under extreme resonant responses during parked situations. Based on in-situ metocean observations on the North Sea, the environmental contour method is used to establish relevant design conditions. A case study on a feasible large monopile design showed that resonant loads can govern the design loads. The presented framework can be applied to assess the reliability of wave-sensitive offshore wind turbine structures for a given site-specific metocean conditions and support structure design.},
   author = {Joey Velarde and Erik Vanem and Claus Kramhøft and John Dalsgaard Sørensen},
   doi = {10.1016/j.apor.2019.101947},
   issn = {01411187},
   journal = {Applied Ocean Research},
   keywords = {Dynamic response,Environmental contour method,Marine structures,Offshore wind turbines,Probabilistic design,Reliability analysis},
   pages = {101947},
   publisher = {Elsevier},
   title = {Probabilistic analysis of offshore wind turbines under extreme resonant response: Application of environmental contour method},
   volume = {93},
   url = {https://doi.org/10.1016/j.apor.2019.101947},
   year = {2019},
}
@article{Eckert2021,
   abstract = {Environmental contours of extreme sea states are often utilized for the purposes of reliability-based offshore design. Many methods have been proposed to estimate environmental contours of extreme sea states, including, but not limited to, the traditional inverse first-order reliability method (I-FORM) and subsequent modifications, copula methods, and Monte Carlo methods. These methods differ in terms of both the methodology selected for defining the joint distribution of sea state parameters and in the method used to construct the environmental contour from the joint distribution. It is often difficult to compare the results of proposed methods to determine which method should be used for a particular application or geographical region. The comparison of the predictions from various contour methods at a single site and across many sites is important to making environmental contours of extreme sea states useful in practice. The goal of this paper is to develop a comparison framework for evaluating methods for developing environmental contours of extreme sea states. This paper develops generalized metrics for comparing the performance of contour methods to one another across a collection of study sites, and applies these metrics and methods to develop conclusions about trends in the wave resource across geographic locations, as demonstrated for a pilot dataset. These proposed metrics and methods are intended to judge the environmental contours themselves relative to other contour methods, and are thus agnostic to a specific device, structure, or field of application. The metrics developed and applied in this paper include measures of predictive accuracy, physical validity, and aggregated temporal performance that can be used to both assess contour methods and provide recommendations for the use of certain methods in various geographical regions. The application and aggregation of the metrics proposed in this paper outline a comparison framework for environmental contour methods that can be applied to support design analysis workflows for offshore structures. This comparison framework could be extended in future work to include additional metrics of interest, potentially including those to address issues pertinent to a specific application area or analysis discipline, such as metrics related to structural response across contour methods or additional physics-based metrics based on wave dynamics.},
   author = {Aubrey Eckert and Nevin Martin and Ryan G. Coe and Bibiana Seng and Zacharia Stuart and Zachary Morrell},
   doi = {10.3390/jmse9010016},
   issn = {20771312},
   issue = {1},
   journal = {Journal of Marine Science and Engineering},
   keywords = {Comparison framework,Environmental contours,Extreme sea state characterization,Physical validity,Predictive accuracy},
   pages = {1-24},
   title = {Development of a comparison framework for evaluating environmental contours of extreme sea states},
   volume = {9},
   year = {2021},
}
@book{Efron1994,
   author = {Bradley Efron and R.J. Tibshirani},
   pages = {456},
   publisher = {Chapman and Hall/CRC},
   title = {An Introduction to the Bootstrap - 1st Edition - Bradley Efron - R.J.},
   url = {https://www.routledge.com/An-Introduction-to-the-Bootstrap/Efron-Tibshirani/p/book/9780412042317},
   year = {1994},
}
@report{Agency2005,
   abstract = {The effects on benthic communities and sediments of anthropogenic influences such as those arising from oil and gas installations, aggregate extraction and dredged material disposal were not identifiable on the relatively coarse scale of the present sampling grid, indicating that, as might be expected, the consequences of these activities when considered on a case-by-case basis are much more localised. However, it may also be concluded that there is no evidence of a cumulative footprint associated with concentrations of these activities, e.g., gas platforms and aggregate extraction sites off the English east coast. Similarly, there was no evidence of deleterious effects associated with demersal fishing activity, which operates on a much wider spatial scale. As with the other sources of anthropogenic activities investigated, the disposition and size of areas of relatively intense fishing effort are, individually, beyond the resolving power of the present survey design, and so this is not to imply the absence of any effects within the survey area.},
   author = {Environmental Agency},
   institution = {Department for Environment, Food and Rural Affairs},
   journal = {Research Policy},
   pages = {1-11},
   title = {CSG 15 Final Project Report},
   url = {randd.defra.gov.uk/Document.aspx?Document=AE1143_3886_FRP.pdf},
   year = {2005},
}
@article{Serinaldi2015,
   abstract = {The concept of return period in stationary univariate frequency analysis is prone to misconceptions and misuses that are well known but still widespread. In this study we highlight how nonstationary and multivariate extensions of such a concept are affected by additional misconceptions, thus easily resulting in further ill-posed procedures and misleading conclusions. We also show that the concepts of probability of exceedance and risk of failure over a given design life period provide more coherent, general and well devised tools for risk assessment and communication.},
   author = {Francesco Serinaldi},
   doi = {10.1007/s00477-014-0916-1},
   issn = {14363259},
   issue = {4},
   journal = {Stochastic Environmental Research and Risk Assessment},
   keywords = {Copulas,Design life,Design values,Multivariate frequency analysis,Nonstationary frequency analysis,Return period,Risk of failure},
   pages = {1179-1189},
   publisher = {Springer Berlin Heidelberg},
   title = {Dismissing return periods!},
   volume = {29},
   url = {http://dx.doi.org/10.1007/s00477-014-0916-1},
   year = {2015},
}
@article{Vanem2020,
   abstract = {Environmental contours are often applied in probabilistic structural reliability analysis to identify extreme environmental conditions that may give rise to extreme loads and responses. They facilitate approximate long term analysis of critical structural responses in situations where computationally heavy and time-consuming response calculations makes full long-term analysis infeasible. The environmental contour method identifies extreme environmental conditions that are expected to give rise to extreme structural response of marine structures. The extreme responses can then be estimated by performing response calculations for environmental conditions along the contours. Response-based analysis is an alternative, where extreme value analysis is performed on the actual response rather than on the environmental conditions. For complex structures, this is often not practical due to computationally heavy response calculations. However, by establishing statistical emulators of the response, using machine learning techniques, one may obtain long time-series of the structural response and use this to estimate extreme responses. In this paper, various contour methods will be compared to response-based estimation of extreme vertical bending moment for a tanker. A response emulator based on Gaussian processes regression with adaptive sampling has been established based on response calculations from a hydrodynamic model. Long time-series of sea-state parameters such as significant wave height and wave period are used to construct N-year environmental contours and the extreme N-year response is estimated from numerical calculations for identified sea states. At the same time, the response emulator is applied on the time series to provide long time-series of structural response, in this case vertical bending moment of a tanker. Extreme value analysis is then performed directly on the responses to estimate the N-year extreme response. The results from either method will then be compared, and it is possible to evaluate the accuracy of the environmental contour method in estimating the response. Moreover, different contour methods will be compared.},
   author = {Erik Vanem and Bingjie Guo and Emma Ross and Philip Jonathan},
   doi = {10.1016/j.marstruc.2019.102680},
   issn = {09518339},
   issue = {August 2019},
   journal = {Marine Structures},
   keywords = {Environmental contours,Environmental loads,Extreme ship response analysis,Marine structures,Ocean environment,Response-based methods,Structural reliability},
   pages = {102680},
   publisher = {Elsevier Ltd},
   title = {Comparing different contour methods with response-based methods for extreme ship response analysis},
   volume = {69},
   url = {https://doi.org/10.1016/j.marstruc.2019.102680},
   year = {2020},
}
@article{Guerrero2021,
   abstract = {Epilepsy is a chronic neurological disorder affecting more than 50 million people globally. An epileptic seizure acts like a temporary shock to the neuronal system, disrupting normal electrical activity in the brain. Epilepsy is frequently diagnosed with electroencephalograms (EEGs). Current methods study the time-varying spectra and coherence but do not directly model changes in extreme behavior. Thus, we propose a new approach to characterize brain connectivity based on the joint tail behavior of the EEGs. Our proposed method, the conditional extremal dependence for brain connectivity (Conex-Connect), is a pioneering approach that links the association between extreme values of higher oscillations at a reference channel with the other brain network channels. Using the Conex-Connect method, we discover changes in the extremal dependence driven by the activity at the foci of the epileptic seizure. Our model-based approach reveals that, pre-seizure, the dependence is notably stable for all channels when conditioning on extreme values of the focal seizure area. Post-seizure, by contrast, the dependence between channels is weaker, and dependence patterns are more "chaotic". Moreover, in terms of spectral decomposition, we find that high values of the high-frequency Gamma-band are the most relevant features to explain the conditional extremal dependence of brain connectivity.},
   author = {Matheus B. Guerrero and Raphaël Huser and Hernando Ombao},
   journal = {Preprint},
   title = {Conex-Connect: Learning Patterns in Extremal Brain Connectivity From Multi-Channel EEG Data},
   url = {http://arxiv.org/abs/2101.09352},
   year = {2021},
}
@article{Eckert-gallup2017,
   author = {Aubrey Eckert-gallup and Nevin Martin},
   title = {New Methods for Calculating Environmental Contours of Extreme Sea States The purpose of this work is to develop a new methodology for determining values of parameters describing extreme sea states that can be used in survivability models for wave energy P},
   year = {2017},
}
@article{Mackay2021,
   abstract = {Various methods have been proposed for defining an environmental contour, based on different concepts of exceedance probability. In the inverse first-order reliability method (IFORM) and the direct sampling (DS) method, contours are defined in terms of exceedances within a region bounded by a hyperplane in either standard normal space or the original parameter space, corresponding to marginal exceedance probabilities under rotations of the coordinate system. In contrast, the more recent inverse second-order reliability method (ISORM) and highest density (HD) contours are defined in terms of an isodensity contour of the joint density function in either standard normal space or the original parameter space, where an exceedance is defined to be anywhere outside the contour. Contours defined in terms of the total probability outside the contour are significantly more conservative than contours defined in terms of marginal exceedance probabilities. In this work we study the relationship between the marginal exceedance probability of the maximum value of each variable along an environmental contour and the total probability outside the contour. The marginal exceedance probability of the contour maximum can be orders of magnitude lower than the total exceedance probability of the contour, with the differences increasing with the number of variables. For example, a 50-year ISORM contour for two variables at 3-h time steps, passes through points with marginal return periods of 635 years, and the marginal return periods increase to 10,950 years for contours of four variables. It is shown that the ratios of marginal to total exceedance probabilities for DS contours are similar to those for IFORM contours. However, the marginal exceedance probabilities of the maximum values of each variable along an HD contour are not in fixed relation to the contour exceedance probability, but depend on the shape of the joint density function. Examples are presented to illustrate the impact of the choice of contour on simple structural reliability problems for cases where the use of contours defined in terms of either marginal or total exceedance probabilities may be appropriate. The examples highlight that to choose an appropriate contour method, some understanding about the shape of a structure's failure surface is required.},
   author = {Ed Mackay and Andreas F. Haselsteiner},
   doi = {10.1016/j.marstruc.2020.102863},
   issn = {09518339},
   issue = {September 2020},
   journal = {Marine Structures},
   keywords = {Direct sampling contour,Directional design criteria,Environmental contour,Extremes,Highest density contour,IFORM,ISORM,Joint distribution,Return value},
   pages = {102863},
   publisher = {Elsevier Ltd},
   title = {Marginal and total exceedance probabilities of environmental contours},
   volume = {75},
   url = {https://doi.org/10.1016/j.marstruc.2020.102863},
   year = {2021},
}
@article{Jonathan2020,
   author = {Philip Jonathan},
   keywords = {extremes,joint distribution,metocean},
   title = {OMAE2020-18308 ESTIMATION OF ENVIRONMENTAL CONTOURS USING A BLOCK RESAMPLING},
   year = {2020},
}
@article{VanDeLindt2000,
   abstract = {In many engineering problems the uncertainty in the design and analysis process is dominated by uncertainty concerning the environmentally induced loading on the system. One such type of system is a structure located in high seismic zones, where there is a high level of uncertainty associated with the ground acceleration. In this study the demand caused by the environmental loads is statistically characterized in terms of magnitude, site-to-source distance and attenuation error at a specified structural period. Through the use of a reliability-based procedure known as the Inverse-First Order Reliability Method all of the combinations of the random loading variables that produce a response spectrum with the specified return period may be identified. These infinite number of combinations produce an Environmental Contour that may be derived for the two, three, or four-dimensional case. Because these contours represent an infinite number of combinations of environmental loading random variables, and in turn, a family of response spectra with the same return period, one need only search the Environmental Contour for the response spectrum producing the peak spectral acceleration at the structural period of interest. This study present a general multi-variate framework focusing on the derivation of these two, three, and four-dimensional Environmental Contours. Initially, the magnitude and site-to-source distance are assumed to be statistically independent. Two and three-dimensional Environmental Contours are derived for this case and critical response spectra for three different, commonly used return periods are examined. Then a hypothetical example in which the site-to-source distance is assumed LogNormal and dependent on magnitude, is used as a basis for discussion for a generic California site. A qualitative discussion between the assumption of independence and dependence between magnitude and site-to-source distance is addressed. Finally, the peak spectral accelerations for the dependent two, three, and four-dimensional cases are compared to one another and the possible consequences associated with increasing the complexity of the environmental contour model analyzed and discussed. (C) 2000 Elsevier Science Ltd. All rights reserved.},
   author = {J. W. Van De Lindt and J. M. Niedzwecki},
   doi = {10.1016/S0141-0296(99)00114-5},
   issn = {01410296},
   issue = {12},
   journal = {Engineering Structures},
   keywords = {Attenuation relations,Earthquake engineering,Environmental contours,Magnitude,Reliability,Response spectrum,Site-to-source distance},
   pages = {1661-1676},
   title = {Environmental contour analysis in earthquake engineering},
   volume = {22},
   year = {2000},
}
@article{Jonathan2014,
   abstract = {Understanding extreme ocean environments and their interaction with fixed and floating structures is critical for offshore and coastal design. Design contours are useful to describe the joint behavior of environmental, structural loading, and response variables. We compare different forms of design contours, using theory and simulation, and present a new method for joint estimation of contours of constant exceedance probability for a general set of variables. The method is based on a conditional extremes model from the statistics literature, motivated by asymptotic considerations. We simulate under the conditional extremes model to estimate contours of constant exceedance probability. We also use the estimated conditional extremes model to estimate other forms of design contours, including those based on the first-order reliability method (FORM), without needing to specify the functional forms of conditional dependence between variables. We demonstrate the application of new method in estimation of contours of constant exceedance probability using measured and hindcast data from the Northern North Sea, the Gulf of Mexico, and the North West Shelf of Australia, and quantify their uncertainties using a bootstrap analysis.},
   author = {Philip Jonathan and Kevin Ewans and Jan Flynn},
   doi = {10.1115/1.4027645},
   issn = {1528896X},
   issue = {4},
   journal = {Journal of Offshore Mechanics and Arctic Engineering},
   pages = {1-8},
   title = {On the estimation of ocean engineering design contours},
   volume = {136},
   year = {2014},
}
@article{Haver2004,
   abstract = {Methods of prediction of structural loads corresponding to a required target annual exceedance probability are reviewed. Particular attention is given to utilization of environmental contour lines for such a purpose. This approach is based on using short term methods for predicting adequate estimates of the q-probability response. The environmental contour line approach is a very convenient approach if complicated structural problems are considered. For such problems one will often have to involve numerical time domain analyses or model tests to reveal the short term probabilistic structure of the response maxima, making a full long term response analysis impossible for most practical problems.},
   author = {Sverre Haver and Gudmund Kleiven},
   doi = {10.1115/OMAE2004-51157},
   issue = {2},
   journal = {Proceedings of the International Conference on Offshore Mechanics and Arctic Engineering - OMAE},
   keywords = {Design values,Environmental contour lines,Extreme values,Long term analysis,Short term analysis},
   pages = {337-345},
   title = {Environmental contour lines for design purposes - Why and when?},
   volume = {1 A},
   year = {2004},
}
@article{Ecsades,
   author = {Philip Jonathan},
   title = {Environmental contours : fundamentals and recommended practice},
}
@article{Haselsteiner2019,
   abstract = {A wide range of methods have been proposed for the derivation of environmental contours for marine structures that must meet reliability targets. An environmental contour is a set of joint extremes of environmental conditions associated with a target return period. In general, environmental contour methods help with the prediction of some future critical combinations of environmental conditions (e.g., wind, waves, current) at a location of interest based on a limited dataset, thus allowing designers to ensure a prescribed structural reliability. In fact, some of these contour methods are specifically recommended by technical specifications and standards as part of a design process. This paper outlines the rules and procedures for a collaborative benchmarking exercise - focused on open comparison - in which researchers are invited to develop and present their own contour derivation approaches based on common datasets that will be available to all. Hindcast and observational datasets are considered and two exercises are planned: One focuses on applying environmental contour methods to a wide range of datasets and the other focuses on uncertainty characterization. Besides describing the benchmark's methodology, this paper presents baseline results of computed contours following current recommendations. The overall goals of this endeavor are: (i) to work towards the development of more robust statistical models and contour construction methods, (ii) to encourage increased discussion in the international research community and among practitioners, and (iii) to support ongoing efforts to improve technical specifications and standards.},
   author = {Andreas F. Haselsteiner and Phong T.T. Nguyen and Ryan G. Coe and Nevin Martin and Lance Manuel and Aubrey Eckert-Gallup},
   doi = {10.1115/OMAE2019-96523},
   isbn = {9780791858783},
   journal = {Proceedings of the International Conference on Offshore Mechanics and Arctic Engineering - OMAE},
   keywords = {OMAE2019-96523},
   pages = {1-10},
   title = {A benchmarking exercise on estimating extreme environmental conditions: Methodology and Baseline results},
   volume = {3},
   year = {2019},
}
@article{Boldi2007,
   abstract = {The spectral density function plays a key role in fitting the tail of multivariate extre-mal data and so in estimating probabilities of rare events. This function satisfies moment con-straints but unlike the univariate extreme value distributions has no simple parametric form. Parameterized subfamilies of spectral densities have been suggested for use in applications, and non-parametric estimation procedures have been proposed, but semiparametric models for multivariate extremes have hitherto received little attention. We show that mixtures of Dirichlet distributions satisfying the moment constraints are weakly dense in the class of all non-parametric spectral densities, and discuss frequentist and Bayesian inference in this class based on the EM algorithm and reversible jump Markov chain Monte Carlo simulation. We illustrate the ideas using simulated and real data. © 2007 Royal Statistical Society.},
   author = {M. O. Boldi and A. C. Davison},
   doi = {10.1111/j.1467-9868.2007.00585.x},
   issn = {13697412},
   issue = {2},
   journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
   keywords = {Adequacy,Air pollution data,Dirichlet distribution,EM algorithm,Multivariate extreme values,Oceanographic data,Reversible jump Markov chain Monte Carlo simulatio,Spectral distribution},
   pages = {217-229},
   title = {A mixture model for multivariate extremes},
   volume = {69},
   year = {2007},
}
@article{Einmahl2009,
   abstract = {Consider a random sample from a bivariate distribution function F in the max-domain of attraction of an extreme-value distribution function G.This G is characterized by two extreme-value indices and a spectral measure, the latter determining the tail dependence structure of F. A major issue in multivariate extreme-value theory is the estimation of the spectral measure &p with respect to the Lp norm. For every p [1, oo], a nonparametric maximum empirical likelihood estimator is proposed for &p. The main novelty is that these estimators are guaranteed to satisfy the moment constraints by which spectral measures are characterized. Asymptotic normality of the estimators is proved under conditions that allow for tail independence. Moreover, the conditions are easily verifiable as we demonstrate through a number of theoretical examples. A simulation study shows a substantially improved performance of the new estimators. Two case studies illustrate how to implement the methods in practice. © Institute of Mathematical Statistics, 2009.},
   author = {John H.J. Einmahl and Johan Segers},
   doi = {10.1214/08-AOS677},
   issn = {00905364},
   issue = {5 B},
   journal = {Annals of Statistics},
   keywords = {Functional central limit theorem,Local empirical process,Moment constraint,Multivariate extremes,National Health and Nutrition Examination Survey,Nonparametric maximum likelihood estimator,Tail dependence},
   pages = {2953-2989},
   title = {Maximum empirical likelihood estimation of the spectral measure of an extreme-value distribution},
   volume = {37},
   year = {2009},
}
@article{DeHaan1977,
   abstract = {Let \{Mathematical expression\} be k-dimensional iid random vectors. Necessary and sufficient conditions are found for the weak convergence of the maxima \{Mathematical expression\} suitably normed to a non-degenerate limit df. The class of such limits is specified and conditions stated for the limit joint df to be a product of marginal df's. Some results are presented concerning extremal processes generated by multivariate df's. © 1977 Springer-Verlag.},
   author = {Laurens de Haan and Sidney I. Resnick},
   doi = {10.1007/BF00533086},
   issn = {00443719},
   issue = {4},
   journal = {Zeitschrift für Wahrscheinlichkeitstheorie und Verwandte Gebiete},
   pages = {317-337},
   title = {Limit theory for multivariate sample extremes},
   volume = {40},
   year = {1977},
}
@article{Coles1991,
   author = {Stuart G Coles and Jonathan A. Tawn},
   issue = {2},
   journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
   keywords = {extreme value theory,generalized extreme value distribution},
   pages = {377-392},
   title = {Modelling Extreme Multivariate Events},
   volume = {53},
   year = {1991},
}
@article{Resnick2002,
   abstract = {We survey the related asymptotic properties of multivariate distributions; (i) asymptotic independence, (ii) hidden regular variation, and (iii) multivariate second order regular variation. Connections and implications are discussed. The point of view of convergence of measures is emphasized in formulations because we are interested in the concepts being coordinate system free, whenever possible.},
   author = {Sidney Resnick},
   doi = {10.1023/A:1025148622954},
   issn = {1386-1999},
   issue = {4},
   journal = {Extremes},
   keywords = {Analyse multivariable,Approximation asymptotique,Asymptotic approximation,Asymptotic behavior,Asymptotic distribution,Asymptotic independence,Asymptotic property,Comportement asymptotique,Convergence méthode numérique,Convergence of numerical methods,Distribution function,Distribution statistique,Independence,Multivariate analysis,Multivariate distribution,Regular variation,Sample survey,Statistical distribution,Statistics,Variations},
   pages = {303-336},
   title = {Hidden Regular Variation, Second Order Regular Variation and Asymptotic Independence},
   volume = {5},
   year = {2002},
}
@article{Mackay2020,
   abstract = {This article compares the accuracy of return value estimates from stationary and non-stationary extreme value models when the data exhibits covariate dependence. The non-stationary covariate representation used is a penalised piecewise-constant (PPC) model, in which the data are partitioned into bins defined by covariates and the extreme value distribution is assumed to be homogeneous within each bin. A generalised Pareto model is assumed, where the scale parameter can vary between bins but is penalised for the variance across bins, and the shape parameter is assumed constant over all covariate bins. The number and sizes of covariate bins must be defined by the user based on physical considerations. Numerical simulations are conducted to compare the performance of stationary and non-stationary models for various case studies, in terms of quality of estimation of the T-year return value over the full covariate domain. It is shown that a non-stationary model can give improved estimates of return values, provided that model assumptions are consistent with the data. When the data exhibits non-stationarity in the generalised Pareto tail shape, the use of non-stationary model assuming a constant shape parameter can produce biases in return values. In such cases, a stationary model can give a more accurate estimate of return value over the full covariate domain as only the most extreme observations (regardless of covariate) are used to estimate tail shape. In other cases, the assumption of a stationary model will ignore key features of the data and be less reliable than a non-stationary model. For example, if a relatively benign covariate interval exhibits a long (or heavy) tail, extreme values from this interval may influence the T-year return value for very large T. However the sample of peaks over threshold, with high threshold, used to estimate a stationary model in this case may not include sufficient observations from this interval to estimate the return value adequately.},
   author = {Ed Mackay and Philip Jonathan},
   doi = {10.1016/j.oceaneng.2020.107406},
   issn = {00298018},
   issue = {April},
   journal = {Ocean Engineering},
   keywords = {Covariate,Extreme,Generalised Pareto,Metocean,Non-stationary,Significant wave height},
   pages = {107406},
   publisher = {Elsevier Ltd},
   title = {Assessment of return value estimates from stationary and non-stationary extreme value models},
   volume = {207},
   url = {https://doi.org/10.1016/j.oceaneng.2020.107406},
   year = {2020},
}
@article{Simpson2021,
   abstract = {Vine copulas are a type of multivariate dependence model, composed of a collection of bivariate copulas that are combined according to a specific underlying graphical structure. Their flexibility and practicality in moderate and high dimensions have contributed to the popularity of vine copulas, but relatively little attention has been paid to their extremal properties. To address this issue, we present results on the tail dependence properties of some of the most widely studied vine copula classes. We focus our study on the coefficient of tail dependence and the asymptotic shape of the sample cloud, which we calculate using the geometric approach of Nolde (2014). We offer new insights by presenting results for trivariate vine copulas constructed from asymptotically dependent and asymptotically independent bivariate copulas, focusing on bivariate extreme value and inverted extreme value copulas, with additional detail provided for logistic and inverted logistic examples. We also present new theory for a class of higher dimensional vine copulas, constructed from bivariate inverted extreme value copulas.},
   author = {Emma S. Simpson and Jennifer L. Wadsworth and Jonathan A. Tawn},
   doi = {10.1016/j.jmva.2021.104736},
   issn = {10957243},
   journal = {Journal of Multivariate Analysis},
   keywords = {Coefficient of tail dependence,Gauge function,Multivariate extremes,Vine copula},
   pages = {104736},
   publisher = {Elsevier Inc.},
   title = {A geometric investigation into the tail dependence of vine copulas},
   volume = {184},
   url = {https://doi.org/10.1016/j.jmva.2021.104736},
   year = {2021},
}
@article{Tawn1988,
   abstract = {Bivariate extreme value distributions arise as the limiting distributions of renormalized componentwise maxima. No natural parametric family exists for the dependence between the marginal distributions, but there are considerable restrictions on the dependence structure. We consider modelling the dependence function with parametric models, for which two new models are presented. Tests for independence, and discriminating between models, are also given. The estimation procedure, and the flexibility of the new models, are illustrated with an application to sea level data. © 1988 Biometrika Trust.},
   author = {Jonathan A. Tawn},
   doi = {10.1093/biomet/75.3.397},
   issn = {00063444},
   issue = {3},
   journal = {Biometrika},
   keywords = {Bivariate exponential distribution,Extreme value theory,Maximum likelihood,Nonregular estimation,Stable distribution,Survival data},
   pages = {397-415},
   title = {Bivariate extreme value theory: Models and estimation},
   volume = {75},
   year = {1988},
}
@book{Pickles1985,
   author = {Andrew Pickles},
   doi = {10.1007/0-387-22764-4_3},
   isbn = {0860941906},
   publisher = {W. H. Hutchins \& Sons},
   title = {An Introduction to Likelihood Inference},
   url = {https://archive.org/details/introductiontoli0000pick/page/21/mode/2up},
   year = {1985},
}
@article{Rootzen2006,
   abstract = {Statistical inference for extremes has been a subject of intensive research over the past couple of decades. One approach is based on modelling exceedances of a random variable over a high threshold with the generalized Pareto (GP) distribution. This has proved to be an important way to apply extreme value theory in practice and is widely used. We introduce a multivariate analogue of the GP distribution and show that it is characterized by each of following two properties: first, exceedances asymptotically have a multivariate GP distribution if and only if maxima asymptotically are extreme value distributed; and second, the multivariate GP distribution is the only one which is preserved under change of exceedance levels. We also discuss a bivariate example and lower-dimensional marginal distributions. © 2006 ISI/BS. © 2006 Applied Probability Trust.},
   author = {Holger Rootzén and Nader Tajvidi},
   doi = {10.3150/bj/1161614952},
   issn = {13507265},
   issue = {5},
   journal = {Bernoulli},
   keywords = {Generalized Pareto distribution,Multivariate Pareto distribution,Multivariate extreme value theory,Non-homogeneous Poisson process,Peaks-over-threshold method},
   month = {1},
   pages = {917-930},
   publisher = {Bernoulli Society for Mathematical Statistics and Probability},
   title = {Multivariate generalized Pareto distributions},
   volume = {12},
   url = {https://projecteuclid.org/journals/bernoulli/volume-12/issue-5/Multivariate-generalized-Pareto-distributions/10.3150/bj/1161614952.full https://projecteuclid.org/journals/bernoulli/volume-12/issue-5/Multivariate-generalized-Pareto-distributions/10.3150/bj},
   year = {2006},
}
@article{Belzile2017,
   abstract = {Liouville copulas introduced in McNeil and Nešlehová (2010) are asymmetric generalizations of the ubiquitous Archimedean copula class. They are the dependence structures of scale mixtures of Dirichlet distributions, also called Liouville distributions. In this paper, the limiting extreme-value attractors of Liouville copulas and of their survival counterparts are derived. The limiting max-stable models, termed here the scaled extremal Dirichlet, are new and encompass several existing classes of multivariate max-stable distributions, including the logistic, negative logistic and extremal Dirichlet. As shown herein, the stable tail dependence function and angular density of the scaled extremal Dirichlet model have a tractable form, which in turn leads to a simple de Haan representation. The latter is used to design efficient algorithms for unconditional simulation based on the work of Dombry et al. (2016) and to derive tractable formulas for maximum-likelihood inference. The scaled extremal Dirichlet model is illustrated on river flow data of the river Isar in southern Germany.},
   author = {Léo R. Belzile and Johanna G. Nešlehová},
   doi = {10.1016/j.jmva.2017.05.008},
   issn = {10957243},
   issue = {5},
   journal = {Journal of Multivariate Analysis},
   keywords = {Extremal attractor,Extremal function,Liouville copula,Scaled extremal Dirichlet model,Stable tail dependence function,de Haan decomposition},
   pages = {68-92},
   title = {Extremal attractors of Liouville copulas},
   volume = {160},
   year = {2017},
}
@book{Chernozhukov2017,
   abstract = {In 1895, the Italian econometrician Vilfredo Pareto discovered that the power law describes well the tails of income and wealth data. This simple observation stimulated further applications of the power law to economic data, including Zipf (1949), Mandelbrot (1963), Fama (1965), Praetz (1972), Sen (1973), and Longin (1996), among many others. It also led to a theory to analyze the properties of the tails of the distributions, so-called extreme value (EV) theory, which was developed by Gnedenko (1943) and deHaan (1970). Jansen and de Vries (1991) applied this theory to analyze the tail properties of US financial returns and concluded that the 1987 market crash was not an outlier; rather, it was a rare event whose magnitude could have been predicted by prior data. This work stimulated numerous other studies that rigorously documented the tail properties of economic data (Embrechts et al., 1997).},
   author = {Victor Chernozhukov and Iván Fernández-Val and Tetsuya Kaji},
   doi = {10.1201/9781315120256},
   isbn = {9781315120256},
   issue = {2005},
   journal = {Handbook of Quantile Regression},
   month = {10},
   pages = {333-362},
   publisher = {Chapman and Hall/CRC},
   title = {Handbook of Quantile Regression},
   url = {https://www.taylorfrancis.com/books/9781498725293},
   year = {2017},
}
@article{Mackay2020a,
   abstract = {A new method for estimating joint distributions of environmental variables is presented. The key difference to previous methods is that the joint distribution of only storm-peak parameters is modelled, rather than fitting a model to all observations. This provides a stronger justification for the use of asymptotic extreme value models, as the data considered are approximately independent. The joint distribution of all data is recovered by resampling and rescaling storm histories, conditional on the peak values. This simplifies the analysis as much of the complex dependence structure is resampled, rather than modelled explicitly. The storm histories are defined by splitting the time series into discrete blocks, with the dividing points defined as the minimum value of a variable between adjacent maxima. Storms are characterised in terms of the peak values of each parameter within each discrete block, which need not coincide in time. The key assumption is that rescaling a measured storm history results in an equally realistic time series, provided that the change in peak values is not large. Two examples of bivariate distribution are considered: the joint distribution of significant wave height (Hs) and zero up-crossing period (Tz) and the joint distribution of Hs and wind speed. It is shown that the storm resampling method gives estimates of environmental contours that agree well with the observations and provides a rigorous method for estimating extreme values.},
   author = {Ed B.L. Mackay and Philip Jonathan},
   doi = {10.1115/omae2020-18308},
   isbn = {9780791884324},
   issue = {March},
   journal = {Proceedings of the International Conference on Offshore Mechanics and Arctic Engineering - OMAE},
   keywords = {Extremes,Joint distribution,Metocean},
   title = {Estimation of environmental contours using a block resampling method},
   volume = {2A-2020},
   year = {2020},
}
@article{Aitkin1982,
   author = {Murray Aitkin},
   doi = {10.1007/978-1-4612-5771-4_8},
   journal = {GLIM 82: Proceedings of the International Conference on Generalised Linear Models},
   pages = {76-86},
   publisher = {Springer, New York, NY},
   title = {Direct Likelihood Inference},
   url = {https://link.springer.com/chapter/10.1007/978-1-4612-5771-4_8},
   year = {1982},
}
@article{Smith1985,
   abstract = {We consider maximum likelihood estimation of the parameters of a probability density which is zero for x < θ and asymptotically αc(x-θ)α-1 as x ↓ θ. Here θ and other parameters, which may or may not include α and c1 are unknown. The classical regularity conditions for the asymptotic properties of maximum likelihood estimators are not satisfied but it is shown that, when α> 2, the information matrix is finite and the classical asymptotic properties continue to hold. For α= 2 the maximum likelihood estimators are asymptotically efficient and normally distributed, but with a different rate of convergence. For 1 < α < 2, the maximum likelihood estimators exist in general, but are not asymptotically normal, while the question of asymptotic efficiency is still unsolved. For α < 1, the maximum likelihood estimators may not exist at all, but alternatives are proposed. All these results are already known for the case of a single unknown location parameter θ, but are here extended to the case in which there are additional unknown parameters. The paper concludes with a discussion of the applications in extreme value theory. © 1985 Biometrika Trust.},
   author = {Richard L. Smith},
   doi = {10.1093/biomet/72.1.67},
   issn = {00063444},
   issue = {1},
   journal = {Biometrika},
   keywords = {Extreme value theory,Maximum likelihood,Nonregular estimation,Stable distribution,Weibull distribution},
   pages = {67-90},
   title = {Maximum likelihood estimation in a class of nonregular cases},
   volume = {72},
   year = {1985},
}
@article{Mattei2001,
   author = {J.M. Mattei and E Vial and V Rebour and H. Liemersdorf and M. Turschmann},
   journal = {Eurosafe},
   title = {Generic results and conclusions of re-evaluating the flooding protection in French and German nuclear power plants},
   volume = {1999},
   year = {2001},
}
@article{Chernozhukov2004,
   author = {V Chernozhukov},
   keywords = {2002,2005,c13,c14,c21,c41,c51,c53,data and software in,date,jel,may,preliminary revision,quantile regression,r available by request,revised january,this is still a},
   title = {Inference for Extremal Conditional Quantile Models},
   year = {2004},
}
@article{Chernozhukov2011,
   abstract = {Quantile regression (QR) is an increasingly important empirical tool in economics and other sciences for analysing the impact a set of regressors has on the conditional distribution of an outcome. Extremal QR, or QR applied to the tails, is of interest in many economic and financial applications, such as conditional value at risk, production efficiency, and adjustment bands in (S, s) models. This paper provides feasible inference tools for extremal conditional quantile models that rely on extreme value approximations to the distribution of self-normalized QR statistics. The methods are simple to implement and can be of independent interest even in the univariate (non-regression) case. We illustrate the results with two empirical examples analysing extreme fluctuations of a stock return and extremely low percentiles of live infant birthweight in the range between 250 and 1500 g. © The Author 2011. Published by Oxford University Press on behalf of The Review of Economic Studies Limited.},
   author = {Victor Chernozhukov and Iván Fernández-Val},
   doi = {10.1093/restud/rdq020},
   issn = {1467937X},
   issue = {2},
   journal = {Review of Economic Studies},
   keywords = {Birthweights,Extreme value theory,Feasible inference,Market risk,Quantile regression,Stress testing,Systemic risk},
   pages = {559-589},
   title = {Inference for extremal conditional quantile models, with an application to market and birthweight risks},
   volume = {78},
   year = {2011},
}
@article{Society2020,
   author = {Royal Statistical Society},
   issue = {1},
   keywords = {extremal index,extreme value theory,generalized extreme value distribution,joint,maximum likelihood estimation,non-stationarity,probabilities method,sea-level,tides},
   pages = {77-93},
   title = {Estimating Probabilities of Extreme Sea-Levels Author ( s ): Jonathan A . Tawn Source : Journal of the Royal Statistical Society . Series C ( Applied Statistics ), Vol . 41 , No . 1 Published by : Wiley for the Royal Statistical Society Stable URL : https},
   volume = {41},
   year = {2020},
}
@article{Huser2016,
   abstract = {Max-stable processes are natural models for spatial extremes because they provide suitable asymptotic approximations to the distribution of maxima of random fields. In the recent past, several parametric families of stationary max-stable models have been developed, and fitted to various types of data. However, a recurrent problem is the modeling of non-stationarity. In this paper, we develop non-stationary max-stable dependence structures in which covariates can be easily incorporated. Inference is performed using pairwise likelihoods, and its performance is assessed by an extensive simulation study based on a non-stationary locally isotropic extremal t model. Evidence that unknown parameters are well estimated is provided, and estimation of spatial return level curves is discussed. The methodology is demonstrated with temperature maxima recorded over a complex topography. Models are shown to satisfactorily capture extremal dependence.},
   author = {Raphaël Huser and Marc G. Genton},
   doi = {10.1007/s13253-016-0247-4},
   issn = {15372693},
   issue = {3},
   journal = {Journal of Agricultural, Biological, and Environmental Statistics},
   keywords = {Covariate,Extremal t model,Extreme event,Max-stable process,Non-stationarity},
   pages = {470-491},
   publisher = {Springer US},
   title = {Non-Stationary Dependence Structures for Spatial Extremes},
   volume = {21},
   year = {2016},
}
@article{Marcon2017a,
   abstract = {Many applications in risk analysis require the estimation of the dependence among multivariate maxima, especially in environmental sciences. Such dependence can be described by the Pickands dependence function of the underlying extreme-value copula. Here, a nonparametric estimator is constructed as the sample equivalent of a multivariate extension of the madogram. Shape constraints on the family of Pickands dependence functions are taken into account by means of a representation in terms of Bernstein polynomials. The large-sample theory of the estimator is developed and its finite-sample performance is evaluated with a simulation study. The approach is illustrated with a dataset of weekly maxima of hourly rainfall in France recorded from 1993 to 2011 at various weather stations all over the country. The stations are grouped into clusters of seven stations, where our interest is in the extremal dependence within each cluster.},
   author = {G. Marcon and S. A. Padoan and P. Naveau and P. Muliere and J. Segers},
   doi = {10.1016/j.jspi.2016.10.004},
   issn = {03783758},
   journal = {Journal of Statistical Planning and Inference},
   keywords = {Bernstein polynomials,Extremal dependence,Extreme-value copula,Heavy rainfall,Multivariate max-stable distribution,Nonparametric estimation,Pickands dependence function},
   pages = {1-17},
   publisher = {Elsevier B.V.},
   title = {Multivariate nonparametric estimation of the Pickands dependence function using Bernstein polynomials},
   volume = {183},
   url = {http://dx.doi.org/10.1016/j.jspi.2016.10.004},
   year = {2017},
}
@article{Saunders2021,
   abstract = {To mitigate the risk posed by extreme rainfall events, we require statistical models that reliably capture extremes in continuous space with dependence. However, assuming a stationary dependence structure in such models is often erroneous, particularly over large geographical domains. Furthermore, there are limitations on the ability to fit existing models, such as max-stable processes, to a large number of locations. To address these modelling challenges, we present a regionalisation method that partitions stations into regions of similar extremal dependence using clustering. To demonstrate our regionalisation approach, we consider a study region of Australia and discuss the results with respect to known climate and topographic features. To visualise and evaluate the effectiveness of the partitioning, we fit max-stable models to each of the regions. This work serves as a prelude to how one might consider undertaking a project where spatial dependence is non-stationary and is modelled on a large geographical scale.},
   author = {K. R. Saunders and A. G. Stephenson and D. J. Karoly},
   doi = {10.1007/s10687-020-00395-y},
   issn = {1572915X},
   issue = {2},
   journal = {Extremes},
   keywords = {60G70,62D05,62G32,62P12,Climate extremes,Clustering,Extremal dependence,Spatial dependence},
   pages = {215-240},
   publisher = {Extremes},
   title = {A regionalisation approach for rainfall based on extremal dependence},
   volume = {24},
   year = {2021},
}
@book{DeValk2016,
   abstract = {This article discusses modelling of the tail of a multivariate distribution function by means of a large deviation principle (LDP), and its application to the estimation of the probability pn of a multivariate extreme event from a sample of niid random vectors, with pn?[n-t2,n-t1] for some t1>1 and t2>t1. One way to view the classical tail limits is as limits of probability ratios. In contrast, the tail LDP provides asymptotic bounds or limits for log-probability ratios. After standardising the marginals to standard exponential, tail dependence is represented by a homogeneous rate function I. Furthermore, the tail LDP can be extended to represent both dependence and marginals, the latter implying marginal log-Generalised Weibull tail limits. A connection is established between the tail LDP and residual tail dependence (or hidden regular variation) and a recent extension of it. Under a smoothness assumption, they are implied by the tail LDP. Based on the tail LDP, a simple estimator for very small probabilities of extreme events is formulated. It avoids estimation of I by making use of its homogeneity. Strong consistency in the sense of convergence of log-probability ratios is proven. Simulations and an application illustrate the difference between the classical approach and the LDP-based approach.},
   author = {Cees de Valk},
   doi = {10.1007/s10687-016-0252-6},
   isbn = {1068701602526},
   issn = {1572915X},
   issue = {4},
   journal = {Extremes},
   keywords = {Generalised Weibull tail limit,Hidden regular variation,Large deviation principle,Log-GW tail limit,Multivariate extremes,Residual tail dependence},
   pages = {687-717},
   publisher = {Extremes},
   title = {Approximation and estimation of very small probabilities of multivariate extreme events},
   volume = {19},
   url = {http://dx.doi.org/10.1007/s10687-016-0252-6},
   year = {2016},
}
@article{Zhang2021,
   abstract = {Quantile regression is a popular and powerful method for studying the effect of regressors on quantiles of a response distribution. However, existing results on quantile regression were mainly developed for cases in which the quantile level is fixed, and the data are often assumed to be independent. Motivated by recent applications, we consider the situation where (i) the quantile level is not fixed and can grow with the sample size to capture the tail phenomena, and (ii) the data are no longer independent, but collected as a time series that can exhibit serial dependence in both tail and non-tail regions. To study the asymptotic theory for high-quantile regression estimators in the time series setting, we introduce a tail adversarial stability condition, which had not previously been described, and show that it leads to an interpretable and convenient framework for obtaining limit theorems for time series that exhibit serial dependence in the tail region, but are not necessarily strongly mixing. Numerical experiments are conducted to illustrate the effect of tail dependence on high-quantile regression estimators, for which simply ignoring the tail dependence may yield misleading $p$-values.},
   author = {Ting Zhang},
   doi = {10.1093/biomet/asaa046},
   issn = {14643510},
   issue = {1},
   journal = {Biometrika},
   keywords = {Adversarial innovation,Double asymptotics,High-quantile regression,Limit theorem,Tail-dependent time series},
   pages = {113-126},
   title = {High-quantile regression for tail-dependent time series},
   volume = {108},
   year = {2021},
}
@article{Koenker2020,
   author = {Roger Koenker},
   title = {Extremal quantile regression inference: an r vinaigrette},
   year = {2020},
}
@article{Cho2017,
   abstract = {The objective of this paper is two-fold: to propose efficient estimation of multiple quantile regression analysis of longitudinal data and to develop a new test for the homogeneity of independent variable effects across multiple quantiles. Estimating multiple regression quantile coefficients simultaneously entails accommodating both association among the multiple quantiles and association among the repeated measurements of the response within subjects. We formulate simultaneous estimating equations using basis matrix expansion which accounts for the above-mentioned associations. The empirical likelihood method is adopted to estimate multiple regression quantile coefficients. Theoretical results show that the proposed simultaneous estimation is asymptotically more efficient than separate estimation of individual regression quantiles or ignoring the within-subject dependency. The proposed method also offers an empirical likelihood ratio test examining the homogeneity of the independent variable effects across the multiple quantiles. The Wilk's theorem holds for the test statistic, and thus the test is easy to implement. Simulation studies and real data example of a multi-center AIDS cohort study are included to illustrate the proposed estimation and testing methods, and empirically examine their properties.},
   author = {Hyunkeun Cho and Seonjin Kim and Mi Ok Kim},
   doi = {10.1016/j.jmva.2017.01.009},
   issn = {10957243},
   journal = {Journal of Multivariate Analysis},
   keywords = {Asymptotic efficiency,Empirical likelihood,Heteroscedasticity test,Longitudinal data,Multiple quantiles},
   pages = {334-343},
   publisher = {Elsevier Inc.},
   title = {Multiple quantile regression analysis of longitudinal data: Heteroscedasticity and efficient estimation},
   volume = {155},
   url = {http://dx.doi.org/10.1016/j.jmva.2017.01.009},
   year = {2017},
}
@article{Ziel2020,
   abstract = {Research Highlights: Flammability of wildland fuels is a key factor influencing risk-based decisions related to preparedness, response, and safety in Alaska. However, without effective measures of current and expected flammability, the expected likelihood of active and problematic wildfires in the future is difficult to assess and prepare for. This study evaluates the effectiveness of diverse indices to capture high-risk fires. Indicators of drought and atmospheric drivers are assessed along with the operational Canadian Forest Fire Danger Rating System (CFFDRS). Background and Objectives: In this study, 13 different indicators of atmospheric conditions, fuel moisture, and flammability are compared to determine how effective each is at identifying thresholds and trends for significant wildfire activity. Materials and Methods: Flammability indices are compared with remote sensing characterizations that identify where and when fire activity has occurred. Results: Among these flammability indicators, conventional tools calibrated to wildfire thresholds (DuffMoisture Code (DMC) and Buildup Index (BUI), as well as measures of atmospheric forcing (Vapor Pressure Deficit (VPD), performed best at representing the conditions favoring initiation and size of significant wildfire events. Conventional assessments of seasonal severity and overall landscape flammability using DMC and BUI can be continued with confidence. Fire models that incorporate BUI in overall fire potential and fire behavior assessments are likely to produce effective results throughout boreal landscapes in Alaska. One novel result is the effectiveness of VPD throughout the state, making it a potential alternative to FFMC among the short-lag/1-day indices. Conclusions: This study demonstrates the societal value of research that joins new academic research results with operational needs. Developing the framework to do this more effectively will bring science to action with a shorter lag time, which is critical as we face growing challenges from a changing climate.},
   author = {Robert H. Ziel and Peter A. Bieniek and Uma S. Bhatt and Heidi Strader and T. Scott Rupp and Alison York},
   doi = {10.3390/F11050516},
   issn = {19994907},
   issue = {5},
   journal = {Forests},
   keywords = {Boreal wildland fire,Canadian forest fire danger rating system,Evaporative demand drought index,Standardized precipitation evapotranspiration inde,Vapor pressure deficit},
   title = {A comparison of fire weather indices with MODIS fire days for the natural regions of alaska},
   volume = {11},
   year = {2020},
}
@article{Fern,
   author = {Javier Fern},
   issue = {1},
   keywords = {automatic differentiation,bayes space,bivariate copula,els,extreme-value copula,gini coefficient,integral splines,pickands dependence function,python,s transform,semiparametric mod-,williamson,zero-},
   pages = {1-53},
   title = {Semiparametric bivariate extreme-value copulas},
}
@article{Nissan2019,
   abstract = {Climate resilience is increasingly prioritized by international development agencies and national governments. However, current approaches to informing communities of future climate risk are problematic. The predominant focus on end-of-century projections neglects more pressing development concerns, which relate to the management of shorter-term risks and climate variability, and constitutes a substantial opportunity cost for the limited financial and human resources available to tackle development challenges. When a long-term view genuinely is relevant to decision-making, much of the information available is not fit for purpose. Climate model projections are able to capture many aspects of the climate system and so can be relied upon to guide mitigation plans and broad adaptation strategies, but the use of these models to guide local, practical adaptation actions is unwarranted. Climate models are unable to represent future conditions at the degree of spatial, temporal, and probabilistic precision with which projections are often provided, which gives a false impression of confidence to users of climate change information. In this article, we outline these issues, review their history, and provide a set of practical steps for both the development and climate scientist communities to consider. Solutions to mobilize the best available science include a focus on decision-relevant timescales, an increased role for model evaluation and expert judgment and the integration of climate variability into climate change services. This article is categorized under: Climate and Development > Knowledge and Action in Development.},
   author = {Hannah Nissan and Lisa Goddard and Erin Coughlan de Perez and John Furlow and Walter Baethgen and Madeleine C. Thomson and Simon J. Mason},
   doi = {10.1002/wcc.579},
   issn = {17577799},
   issue = {3},
   journal = {Wiley Interdisciplinary Reviews: Climate Change},
   keywords = {climate change adaptation,climate change projections,climate resilience,climate services,international development},
   pages = {1-16},
   title = {On the use and misuse of climate change projections in international development},
   volume = {10},
   year = {2019},
}
@article{Dupuis1999,
   abstract = {In this paper, we consider the modeling of exceedances over high thresholds.The natural distribution for such exceedances, the generalized Pareto distribution (GPD), is used and the problematic issue of threshold selection is addressed. We fit the GPD robustly to the data using techniques based on optimal bias-robust estimates. The robust procedure will assign weights between 0 and 1 to each data point. These weights are used to assess the validity of the GPD model for exceedances of the proposed threshold and thus can guide threshold selection. That is, we can initially consider a low threshold and increase it (thus reducing the number of data points) until all weights are close to one. The new approach is used to analyze two of the NERC data sets.},
   author = {D.J. Dupuis},
   doi = {10.1023/A:1009914915709},
   issn = {1386-1999},
   issue = {3},
   journal = {Extremes},
   keywords = {data point,data set,econometrics,generalized pareto distribution,mathematics,robust statistics,statistics},
   pages = {251-261},
   title = {Exceedances over High Thresholds: A Guide to Threshold Selection},
   volume = {1},
   year = {1999},
}
@article{Ozari2019,
   abstract = {The Block Maxima method divides sample data into equal blocks. Predictions are based on the maximum values of the observations. Choosing an efficient and proper block size for the Block Maxima method is an important issue and varies across fields (e.g., flood, rainfall, finance). However, the main problem is deciding which block size is suitable or optimal for the prediction. In the literature, it is a known fact that the selection of a small block size leads to bias, while the selection of a large block size leads to a variance problem. In one respect, this issue is any trade off problem between the bias and the variance. This paper proposes simple and easy computational method to specify the optimal block size selection process for the Block Maxima method.},
   author = {Çiğdem Özari and Özge Eren and Hasan Saygin},
   doi = {10.17559/TV-20180529125449},
   issn = {18486339},
   issue = {5},
   journal = {Tehnicki Vjesnik},
   keywords = {Block maxima,Extreme value theory,Maximum likelihood},
   pages = {1292-1296},
   title = {A new methodology for the block maxima approach in selecting the optimal block size},
   volume = {26},
   year = {2019},
}
@article{Kysely2010,
   abstract = {The paper presents a methodology for estimating high quantiles of distributions of daily temperature in a non-stationary context, based on peaks-over-threshold analysis with a time-dependent threshold expressed in terms of regression quantiles. The extreme value models are applied to estimate 20-yr return values of maximum daily temperature over Europe in transient global climate model (GCM) simulations for the 21st century. A comparison of scenarios of changes in the 20-yr return temperatures based on the non-stationary peaks-over-threshold models with conventional stationary models is performed. It is demonstrated that the application of the stationary extreme value models in temperature data from GCM scenarios yields results that may be to a large extent biased, while the non-stationary models lead to spatial patterns that are robust and enable one to detect areas where the projected warming in the tail of the distribution of daily temperatures is largest. The method also allows splitting the projected warming of extremely high quantiles into two parts that reflect change in the location and scale of the distribution of extremes, respectively. Spatial patterns of the two components differ significantly in the examined climate change projections over Europe. © 2010 Elsevier B.V.},
   author = {Jan Kyselý and Jan Picek and Romana Beranová},
   doi = {10.1016/J.GLOPLACHA.2010.03.006},
   issn = {0921-8181},
   issue = {1-2},
   journal = {Global and Planetary Change},
   keywords = {Climate change,Extreme temperatures,Extreme value analysis,Global climate models,Peaks-over-threshold method,Poisson process,Quantile regression},
   month = {5},
   pages = {55-68},
   publisher = {Elsevier},
   title = {Estimating extremes in climate change simulations using the peaks-over-threshold method with a non-stationary threshold},
   volume = {72},
   year = {2010},
}
@article{Northrop2011,
   abstract = {In environmental applications it is common for the extremes of a variable to be non-stationary, varying systematically in space, time or with the values of covariates. Multi-site datasets are common, and in such cases there is likely to be non-negligible inter-site dependence. We consider applications in which multi-site data are used to infer the marginal behaviour of the extremes at individual sites, while adjusting for inter-site dependence. For reasons of statistical efficiency, it is standard to model exceedances of a high threshold. Choosing an appropriate threshold can be problematic, particularly if the extremes are non-stationary. We propose a method for setting a covariate-dependent threshold using quantile regression. We consider how the quantile regression model and extreme value models fitted to threshold exceedances should be parameterized, in order that they are compatible. We adjust estimates of uncertainty for spatial dependence using methodology proposed recently. These methods are illustrated using time series of storm peak significant wave heights from 72 sites in the Gulf of Mexico. A simulation study illustrates the applicability of the proposed methodology more generally. © 2011 John Wiley & Sons, Ltd.},
   author = {Paul J. Northrop and Philip Jonathan},
   doi = {10.1002/ENV.1106},
   issn = {1099-095X},
   issue = {7},
   journal = {Environmetrics},
   keywords = {dependent data,extreme value regression modelling,quantile regression,threshold selection,wave heights},
   month = {11},
   pages = {799-809},
   publisher = {John Wiley & Sons, Ltd},
   title = {Threshold modelling of spatially dependent non-stationary extremes with application to hurricane-induced wave heights},
   volume = {22},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/env.1106 https://onlinelibrary.wiley.com/doi/abs/10.1002/env.1106 https://onlinelibrary.wiley.com/doi/10.1002/env.1106},
   year = {2011},
}
@article{Sigauke2017,
   abstract = {Long term peak electricity demand forecasting is a crucial step in the process of planning for power transmission and new generation capacity. This paper discusses an application of the Generalized Pareto Distribution to the modelling of daily peak electricity demand using South African data for the period 2000 to 2010. The main contribution of this paper is in the use of a cubic smoothing spline with a constant shift factor as a time varying threshold. An intervals estimator method is then used to decluster the observations above the threshold. We explore the influence of temperature by including it as a covariate in the Generalized Pareto Distribution parameters. A comparative analysis is done using the block maxima approach. The GPD model showed a better fit to the data compared to the GEVD model. Key findings from this study are that the Weibull class of distributions best fits the data which is bounded from above for both stationary and non-stationary models. Another key finding is that for different values of the temperature covariate the shape parameter is invariant and the scale parameter changes for different values of heating degree days.},
   author = {Caston Sigauke and Alphonce Bere},
   doi = {10.1016/J.ENERGY.2016.12.027},
   issn = {0360-5442},
   journal = {Energy},
   keywords = {Extreme value theory,Non stationary time series,Peak electricity demand,Penalized smoothing splines,Time varying threshold},
   month = {1},
   pages = {152-166},
   publisher = {Pergamon},
   title = {Modelling non-stationary time series using a peaks over threshold distribution with time varying covariates and threshold: An application to peak electricity demand},
   volume = {119},
   year = {2017},
}
@article{Coles1990,
   abstract = {For the design of sea defences the main statistical issue is to estimate quantiles of the distribution of annual maximum sea levels for all coastal sites. Traditional procedures independently analyse data from each individual site; thus known spatial properties of the meteorological and astronomical tidal components of sea level are not exploited. By spatial modelling of the marginal behaviour and inter-site dependence of sea level annual maxima around the British coast we are able to examine risk assessment for coastlines and the issue of sensitivity to climatic change.},
   author = {G Coles and J. A. Tawn},
   doi = {10.1098/rsta.1990.0126},
   issn = {0962-8428},
   issue = {1627},
   journal = {Philosophical Transactions of the Royal Society of London. Series A: Physical and Engineering Sciences},
   month = {9},
   pages = {457-476},
   publisher = {The Royal Society London},
   title = {Statistics of coastal flood prevention},
   volume = {332},
   url = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.1990.0126},
   year = {1990},
}
@article{Dixon1999,
   abstract = {The sea-level is the composition of astronomical tidal and meteorological surge processes. It exhibits temporal non-stationarity due to a combination of long-term trend in the mean level, the deterministic tidal component, surge seasonality and interactions between the tide and surge. We assess the effect of these non-stationarities on the estimation of the distribution of extreme sea-levels. This is important for coastal flood assessment as the traditional method of analysis assumes that, once the trend has been removed, extreme sea-levels are from a stationary sequence. We compare the traditional approach with a recently proposed alternative that incorporates the knowledge of the tidal component and its associated interactions, by applying them to 22 UK data sites and through a simulation study. Our main finding is that if the tidal non-stationarity is ignored then a substantial underestimation of extreme sea-levels results for most sites. In contrast, if surge seasonality and the tide-surge interaction are not modelled the traditional approach produces little additional bias. The alternative method is found to perform well but requires substantially more statistical modelling and better data quality.},
   author = {Mark J. Dixon and Jonathan A. Tawn},
   doi = {10.1111/1467-9876.00145},
   issn = {1467-9876},
   issue = {2},
   journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
   keywords = {Annual maximum method,Extreme sea,Extreme value theory,Joint probabilities method,Return level,levels},
   month = {1},
   pages = {135-151},
   publisher = {John Wiley & Sons, Ltd},
   title = {The Effect of Non-Stationarity on Extreme Sea-Level Estimation},
   volume = {48},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1111/1467-9876.00145 https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9876.00145 https://rss.onlinelibrary.wiley.com/doi/10.1111/1467-9876.00145},
   year = {1999},
}
@article{Casson1999,
   abstract = {Meteorological data are often recorded at a number of spatial locations. This gives rise to the possibility of pooling data through a spatial model to overcome some of the limitations imposed on an extreme value analysis by a lack of information. In this paper we develop a spatial model for extremes based on a standard representation for site-wise extremal behavior, combined with a spatial latent process for parameter variation over the region. A smooth, but possibly non-linear, spatial structure is an intrinsic feature of the model, and difficulties in computation are solved using Markov chain Monte Carlo inference. A simulation study is carried out to illustrate the potential gain in efficiency achieved by the spatial model. Finally, the model is applied to data generated from a climatological model in order to characterize the hurricane climate of the Gulf and Atlantic coasts of the United States.},
   author = {Edward Casson and Stuart Coles},
   doi = {10.1023/A:1009931222386},
   issn = {1572-915X},
   issue = {4},
   journal = {Extremes 1999 1:4},
   keywords = {Civil Engineering,Economics,Environmental Management,Finance,Hydrogeology,Insurance,Management,Quality Control,Reliability,Safety and Risk,Statistics,Statistics for Business,general},
   pages = {449-468},
   publisher = {Springer},
   title = {Spatial Regression Models for Extremes},
   volume = {1},
   url = {https://link.springer.com/article/10.1023/A:1009931222386},
   year = {1999},
}
@article{Gilleland2014,
   author = {Doug Nychka and Eric Gilleland and Douglas Nychka and Uli Schneider},
   title = {Spatial models for the distribution of extremes},
   url = {www.cgd.ucar.edu/stats},
   year = {2014},
}
@article{Cooley2012,
   abstract = {Quantification of precipitation extremes is important for flood planning purposes, and a common measure of extreme events is the r-year return level. We present a method for producing maps of precipitation return levels and uncertainty measures and apply it to a region in Colorado. Separate hierarchical models are constructed for the intensity and the frequency of extreme precipitation events. For intensity, we model daily precipitation above a high threshold at 56 weather stations with the generalized Pareto distribution. For frequency, we model the number of exceedances at the stations as binomial random variables. Both models assume that the regional extreme precipitation is driven by a latent spatial process characterized by geographical and climatological covariates. Effects not fully described by the covariates are captured by spatial structure in the hierarchies. Spatial methods were improved by working in a space with climatological coordinates. Inference is provided by a Markov chain Monte Carlo algorithm and spatial interpolation method, which provide a natural method for estimating uncertainty. © 2007 American Statistical Association.},
   author = {Daniel Cooley and Douglas Nychka and Philippe Naveau},
   doi = {10.1198/016214506000000780},
   issn = {01621459},
   issue = {479},
   journal = {Journal of the American Statistical Association},
   keywords = {Colorado,Extreme value theory,Generalized pareto distribution,Hierarchical model,Latent process},
   month = {9},
   pages = {824-840},
   publisher = {Taylor & Francis},
   title = {Bayesian spatial modeling of extreme precipitation return levels},
   volume = {102},
   url = {https://www.tandfonline.com/doi/abs/10.1198/016214506000000780},
   year = {2007},
}
@article{Mhalla2017,
   abstract = {The dependence structure of max-stable random vectors can be characterized by their Pickands dependence function. In many applications, the extremal dependence measure varies with covariates. We develop a flexible, semi-parametric method for the estimation of non-stationary multivariate Pickands dependence functions. The proposed construction is based on an accurate max-projection allowing to pass from the multivariate to the univariate setting and to rely on the generalized additive modeling framework. In the bivariate case, the resulting estimator of the Pickands function is regularized using constrained median smoothing B-splines, and bootstrap variability bands are constructed. In higher dimensions, we tailor our approach to the estimation of the extremal coefficient. An extended simulation study suggests that our estimator performs well and is competitive with the standard estimators in the absence of covariates. We apply the new methodology to a temperature dataset in the US where the extremal dependence is linked to time and altitude.},
   author = {Linda Mhalla and Valérie Chavez-Demoulin and Philippe Naveau},
   doi = {10.1016/J.JMVA.2017.04.006},
   issn = {0047-259X},
   journal = {Journal of Multivariate Analysis},
   keywords = {Extreme value theory,Generalized additive models,Max-stable random vectors,Non-stationarity,Pickands function,Semi-parametric models,Temperature data},
   month = {7},
   pages = {49-66},
   publisher = {Academic Press},
   title = {Non-linear models for extremal dependence},
   volume = {159},
   year = {2017},
}
@article{Jonathan2014a,
   abstract = {Characterising the joint structure of extremes of environmental variables is important for improved understanding of those environments. Yet, many applications of multivariate extreme value analysis adopt models that assume a particular form of extremal dependence between variables without justification, or restrict attention to regions in which all variables are extreme. The conditional extremes model of Heffernan and Tawn provides one approach to avoiding these particular restrictions. Extremal marginal and dependence characteristics of environmental variables typically vary with covariates. Reliable descriptions of extreme environments should also therefore characterise any non-stationarity. A recent article by the current authors extends the conditional extremes model of Heffernan and Tawn to include covariate effects, using Fourier representations of model parameters for single periodic covariates. Here, we further extend our recent work, introducing a general purpose spline representation for model parameters as functions of multidimensional covariates, common to all inference steps. We use a non-crossing quantile regression to estimate appropriate non-stationary marginal quantiles simultaneously as functions of covariate; these are necessary as thresholds for extreme value modelling and for standardisation of marginal distributions prior to application of the conditional extremes model. Then, we perform marginal extreme value and conditional extremes modelling within a roughness-penalised likelihood framework, with cross-validation to estimate suitable model parameter roughness. Finally, we use a bootstrap re-sampling procedure, encompassing all inference steps, to quantify uncertainties in, and dependence structure of, parameter estimates and estimates of conditional extremes of one variate given large values of another. We validate the approach using simulations from known joint distributions, the extremal dependence structures of which change with covariate. We apply the approach to joint modelling of storm peak significant wave height and associated storm peak period for extra-tropical storms at a northern North Sea location, with storm direction as covariate. We evaluate the impact of incorporating directional effects on estimates for conditional return values. © 2014 John Wiley & Sons, Ltd.},
   author = {P. Jonathan and K. Ewans and D. Randell},
   doi = {10.1002/ENV.2262},
   issn = {1099-095X},
   issue = {3},
   journal = {Environmetrics},
   keywords = {bootstrap,conditional extremes,covariate,cross,crossing quantile regression,non,spline,stationarity,validation},
   month = {5},
   pages = {172-188},
   publisher = {John Wiley & Sons, Ltd},
   title = {Non-stationary conditional extremes of northern North Sea storm characteristics},
   volume = {25},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/env.2262 https://onlinelibrary.wiley.com/doi/abs/10.1002/env.2262 https://onlinelibrary.wiley.com/doi/10.1002/env.2262},
   year = {2014},
}
@article{Krock2021,
   abstract = {In traditional extreme value analysis, the bulk of the data is ignored, and only the tails of the distribution are used for inference. Extreme observations are specified as values that exceed a threshold or as maximum values over distinct blocks of time, and subsequent estimation procedures are motivated by asymptotic theory for extremes of random processes. For environmental data, nonstationary behavior in the bulk of the distribution, such as seasonality or climate change, will also be observed in the tails. To accurately model such nonstationarity, it seems natural to use the entire dataset rather than just the most extreme values. It is also common to observe different types of nonstationarity in each tail of a distribution. Most work on extremes only focuses on one tail of a distribution, but for temperature, both tails are of interest. This paper builds on a recently proposed parametric model for the entire probability distribution that has flexible behavior in both tails. We apply an extension of this model to historical records of daily mean temperature at several locations across the United States with different climates and local conditions. We highlight the ability of the method to quantify changes in the bulk and tails across the year over the past decades and under different geographic and climatic conditions. The proposed model shows good performance when compared to several benchmark models that are typically used in extreme value analysis of temperature.},
   author = {Mitchell Krock and Julie Bessac and Michael L. Stein and Adam H. Monahan},
   journal = {Preprint},
   keywords = {bulk and tails,climate change,nonstationary,temperature extremes},
   pages = {1-19},
   title = {Nonstationary seasonal model for daily mean temperature distribution bridging bulk and tails},
   url = {http://arxiv.org/abs/2110.10046},
   year = {2021},
}
@article{Heffernan2007,
   abstract = {Models based on assumptions of multivariate regular variation and hidden regular variation provide ways to describe a broad range of extremal dependence structures when marginal distributions are heavy tailed. Multivariate regular variation provides a rich description of extremal dependence in the case of asymptotic dependence, but fails to distinguish between exact independence and asymptotic independence. Hidden regular variation addresses this problem by requiring components of the random vector to be simultaneously large but on a smaller scale than the scale for the marginal distributions. In doing so, hidden regular variation typically restricts attention to that part of the probability space where all variables are simultaneously large. However, since under asymptotic independence the largest values do not occur in the same observation, the region where variables are simultaneously large may not be of primary interest. A different philosophy was offered in the paper of Heffernan and Tawn [J. R. Stat. Soc. Ser. B Stat. Methodol. 66 (2004) 497–546] which allows examination of distributional tails other than the joint tail. This approach used an asymptotic argument which conditions on one component of the random vector and finds the limiting conditional distribution of the remaining components as the conditioning variable becomes large. In this paper, we provide a thorough mathematical examination of the limiting arguments building on the orientation of Heffernan and Tawn [J. R. Stat. Soc. Ser. B Stat. Methodol. 66 (2004) 497–546]. We examine the conditions required for the assumptions made by the conditioning approach to hold, and highlight simililarities and differences between the new and established methods.},
   author = {Janet E. Heffernan and Sidney I. Resnick},
   doi = {10.1214/105051606000000835},
   issn = {1050-5164},
   issue = {2},
   journal = {https://doi.org/10.1214/105051606000000835},
   keywords = {60G70,62G32,Asymptotic independence,Conditional models,coefficient of tail dependence,heavy tails,hidden regular variation,regular variation},
   month = {4},
   pages = {537-571},
   publisher = {Institute of Mathematical Statistics},
   title = {Limit laws for random vectors with an extreme component},
   volume = {17},
   url = {https://projecteuclid.org/journals/annals-of-applied-probability/volume-17/issue-2/Limit-laws-for-random-vectors-with-an-extreme-component/10.1214/105051606000000835.full https://projecteuclid.org/journals/annals-of-applied-probability/volume-17/issue-2/L},
   year = {2007},
}
@article{Mentaschi2016,
   abstract = {Statistical approaches to study extreme events require, by definition, long time series of data. In many scientific disciplines, these series are often subject to variations at different temporal scales that affect the frequency and intensity of their extremes. Therefore, the assumption of stationarity is violated and alternative methods to conventional stationary extreme value analysis (EVA) must be adopted. Using the example of environmental variables subject to climate change, in this study we introduce the transformed-stationary (TS) methodology for non-stationary EVA. This approach consists of (i) transforming a non-stationary time series into a stationary one, to which the stationary EVA theory can be applied, and (ii) reverse transforming the result into a non-stationary extreme value distribution. As a transformation, we propose and discuss a simple time-varying normalization of the signal and show that it enables a comprehensive formulation of non-stationary generalized extreme value (GEV) and generalized Pareto distribution (GPD) models with a constant shape parameter. A validation of the methodology is carried out on time series of significant wave height, residual water level, and river discharge, which show varying degrees of long-term and seasonal variability. The results from the proposed approach are comparable with the results from (a) a stationary EVA on quasi-stationary slices of non-stationary series and (b) the established method for non-stationary EVA. However, the proposed technique comes with advantages in both cases. For example, in contrast to (a), the proposed technique uses the whole time horizon of the series for the estimation of the extremes, allowing for a more accurate estimation of large return levels. Furthermore, with respect to (b), it decouples the detection of non-stationary patterns from the fitting of the extreme value distribution. As a result, the steps of the analysis are simplified and intermediate diagnostics are possible. In particular, the transformation can be carried out by means of simple statistical techniques such as low-pass filters based on the running mean and the standard deviation, and the fitting procedure is a stationary one with a few degrees of freedom and is easy to implement and control. An open-source MATLAB toolbox has been developed to cover this methodology, which is available at <a hrefCombining double low line"https://github.com/menta78/tsEva/" targetCombining double low line"-blank">https://github.com/menta78/tsEva/</a> (Mentaschi et al., 2016).},
   author = {Lorenzo Mentaschi and Michalis Vousdoukas and Evangelos Voukouvalas and Ludovica Sartini and Luc Feyen and Giovanni Besio and Lorenzo Alfieri},
   doi = {10.5194/HESS-20-3527-2016},
   issue = {9},
   journal = {Hydrology and Earth System Sciences},
   month = {9},
   pages = {3527-3547},
   publisher = {Copernicus GmbH},
   title = {The transformed-stationary approach: A generic and simplified methodology for non-stationary extreme value analysis},
   volume = {20},
   year = {2016},
}
@article{Nogaj2007,
   abstract = {In this paper, we study extreme values of non-stationary climatic phenomena. In the usually considered stationary case, the modelling of extremes is only based on the behaviour of the tails of the distribution of the remainder of the data set. In the non-stationary case though, it seems reasonable to assume that the temporal dynamics of the entire data set and that of extremes are closely related and thus all the available information about this link should be used in statistical studies of these events. We try to study how centered and normalized data which are closer to stationary data than the observation allows easier statistical analysis and to understand if we are very far from a hypothesis stating that the extreme events of centered and normed data follow a stationary distribution. The location and scale parameters used for this transformation (the central field), as well as extreme parameters obtained for the transformed data enable us to retrieve the trends in extreme events of the initial data set. Through non-parametric statistical methods, we thus compare a model directly built on the extreme events and a model reconstructed from estimations of the trends of the location and scale parameters of the entire data set and stationary extremes obtained from the centered and normed data set. In case of a correct reconstruction, we can clearly state that variations of the characteristics of extremes are well explained by the central field. Through these analyses we bring arguments to choose constant shape parameters of extreme distributions. We show that for the frequency of the moments of high threshold excesses (or for the mean of annual extremes), the general dynamics explains a large part of the trends on frequency of extreme events. The conclusion is less obvious for the amplitudes of threshold exceedances (or the variance of annual extremes) - especially for cold temperatures, partly justified by the statistical tools used, which require further analyses on the variability definition.},
   author = {M. Nogaj and S. Parey and D. Dacunha-Castelle},
   doi = {10.5194/NPG-14-305-2007},
   issue = {3},
   journal = {Nonlinear Processes in Geophysics},
   pages = {305-316},
   publisher = {European Geosciences Union},
   title = {Non-stationary extreme models and a climatic application},
   volume = {14},
   year = {2007},
}
@article{Stein2021,
   abstract = {The two sentences below Proposition 2 presently read: The parameters in (Formula presented.) all have distinct and interpretable roles in satisfying (5) and (6). From the proof in Section 7, (Formula presented.), regardless of the values of the other parameters. The expression for (Formula presented.) in the second sentence is incorrect. These two sentences should instead read: The parameters in (Formula presented.) all have distinct and interpretable roles in satisfying (5) and (6). From the proof in Section 7, (Formula presented.), regardless of the values of the other parameters.},
   author = {Michael L. Stein},
   doi = {10.1002/ENV.2704},
   issn = {1099-095X},
   journal = {Environmetrics},
   keywords = {NIL},
   pages = {e2704},
   publisher = {John Wiley & Sons, Ltd},
   title = {A parametric model for distributions with flexible behaviour in both tails},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/env.2704 https://onlinelibrary.wiley.com/doi/abs/10.1002/env.2704 https://onlinelibrary.wiley.com/doi/10.1002/env.2704},
   year = {2021},
}
@article{Stein2021a,
   abstract = {For many problems of inference about a marginal distribution function, while the entire distribution is important, extreme quantiles are of particular interest because rare outcomes may have large consequences. In some applications, only the extreme upper quantiles require extra attention, but in, for example, climatological applications, extremes in both tails of the distribution can be impactful. A possible approach in this setting is to use parametric families of distributions that have flexible behavior in both tails. One way to quantify this property is to require that, for any two generalized Pareto distributions, there is a member of the parametric family that behaves like one of the generalized Pareto distributions in the upper tail and like the negative of the other generalized Pareto distribution in the lower tail. This work proposes some specific quantifications of this notion and describes parametric families of distributions that satisfy these specifications. The proposed families all have closed form expressions for their densities and, hence, are convenient for use in practice. A simulation study shows how one of the proposed families can work well for estimating all quantiles when both tails of a distribution are heavy tailed. An application to climate model output shows this family can also work well when applied to daily average January temperature near Calgary, for which the evolving distribution over time due to climate change is difficult to model accurately by any standard parametric family.},
   author = {Michael L. Stein},
   doi = {10.1002/ENV.2658},
   issn = {1099-095X},
   issue = {2},
   journal = {Environmetrics},
   keywords = {climate model,generalized Pareto distribution,temperature extremes},
   month = {3},
   pages = {e2658},
   publisher = {John Wiley & Sons, Ltd},
   title = {A parametric model for distributions with flexible behavior in both tails},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/env.2658 https://onlinelibrary.wiley.com/doi/abs/10.1002/env.2658 https://onlinelibrary.wiley.com/doi/10.1002/env.2658},
   year = {2021},
}
@article{Carvalho2014,
   abstract = {The modeling of multivariate extremes has received increasing recent attention because of its importance in risk assessment. In classical statistics of extremes, the joint distribution of two or more extremes has a nonparametric form, subject to moment constraints. This article develops a semiparametric model for the situation where several multivariate extremal distributions are linked through the action of a covariate on an unspecified baseline distribution, through a so-called density ratio model. Theoretical and numerical aspects of empirical likelihood inference for this model are discussed, and an application is given to pairs of extreme forest temperatures. Supplementarymaterials for this article are available online.},
   author = {Miguel de Carvalho and Anthony C. Davison},
   doi = {10.1080/01621459.2013.872651},
   issn = {1537274X},
   issue = {506},
   journal = {Journal of the American Statistical Association},
   keywords = {Air temperature,Empirical likelihood,Exponential tilting,Forest microclimate,Multivariate extreme values,Semiparametric modeling,Spectral distribution},
   pages = {764-776},
   publisher = {Taylor & Francis},
   title = {Spectral density ratio models for multivariate extremes},
   volume = {109},
   url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.2013.872651},
   year = {2014},
}
@article{Chavez-Demoulin2005,
   abstract = {We describe smooth non-stationary generalized additive modelling for sample extremes, in which spline smoothers are incorporated into models for exceedances over high thresholds. Fitting is by maximum penalized likelihood estimation, with uncertainty assessed by using differences of deviances and bootstrap simulation. The approach is illustrated by using data on extreme winter temperatures in the Swiss Alps, analysis of which shows strong influence of the north Atlantic oscillation. Benefits of the new approach are flexible and appropriate modelling of extremes, more realistic assessment of estimation uncertainty and the accommodation of complex dependence patterns.},
   author = {V. Chavez-Demoulin and A. C. Davison},
   doi = {10.1111/J.1467-9876.2005.00479.X},
   issn = {1467-9876},
   issue = {1},
   journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
   keywords = {Bootstrap,Generalized Pareto distribution,Generalized additive model,Natural cubic spline,North Atlantic oscillation,Parameter orthogonality,Peaks over threshold,Penalized likelihood,Statistics of extremes,Temperature data},
   month = {1},
   pages = {207-222},
   publisher = {John Wiley & Sons, Ltd},
   title = {Generalized additive modelling of sample extremes},
   volume = {54},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9876.2005.00479.x https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9876.2005.00479.x https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1467-9876.2005.00479.x},
   year = {2005},
}
@article{Frahm2006,
   abstract = {A measure called 'extremal dependence coefficient' (EDC) is introduced for studying the asymptotic dependence structure of the minimum and the maximum of a random vector. Some general properties of the EDC are derived and its relation to the tail dependence coefficient is examined. The extremal dependence structure of regularly varying elliptical random vectors is investigated and it is shown that the EDC is only determined by the tail index and by the pseudo-correlation coefficients of the elliptical distribution. © 2006 Elsevier B.V. All rights reserved.},
   author = {Gabriel Frahm},
   doi = {10.1016/J.SPL.2006.03.006},
   issn = {0167-7152},
   issue = {14},
   journal = {Statistics and Probability Letters},
   keywords = {Asymptotic dependence,Copula,Elliptical distribution,Extremal dependence,Tail dependence coefficient,Tail index},
   month = {8},
   pages = {1470-1481},
   publisher = {North-Holland},
   title = {On the extremal dependence coefficient of multivariate distributions},
   volume = {76},
   year = {2006},
}
@article{Beranger2019,
   abstract = {Estimation of extreme quantile regions, spaces in which future extreme events can occur with a given low probability, even beyond the range of the observed data, is an important task in the analysis of extremes. Existing methods to estimate such regions are available, but do not provide any measures of estimation uncertainty. We develop univariate and bivariate schemes for estimating extreme quantile regions under the Bayesian paradigm that outperforms existing approaches and provides natural measures of quantile region estimate uncertainty. We examine the method’s performance in controlled simulation studies. We illustrate the applicability of the proposed method by analysing high bivariate quantiles for pairs of pollutants, conditionally on different temperature gradations, recorded in Milan, Italy.},
   author = {Boris Beranger and Simone A Padoan and Scott A Sisson and Boris Beranger BBeranger and unsweduau A Simone Padoan and Scott A Sisson ScottSisson},
   doi = {10.1007/S10687-019-00364-0},
   issn = {1572-915X},
   issue = {2},
   journal = {Extremes 2019 24:2},
   keywords = {Civil Engineering,Economics,Environmental Management,Finance,Hydrogeology,Insurance,Management,Quality Control,Reliability,Safety and Risk,Statistics,Statistics for Business,general},
   month = {12},
   pages = {349-375},
   publisher = {Springer},
   title = {Estimation and uncertainty quantification for extreme quantile regions},
   volume = {24},
   url = {https://link.springer.com/article/10.1007/s10687-019-00364-0},
   year = {2019},
}
@article{Wei2006,
   abstract = {Estimation of reference growth curves for children's height and weight has traditionally relied on normal theory to construct families of quantile curves based on samples from the reference population. Age-specific parametric transformation has been used to significantly broaden the applicability of these normal theory methods. Non-parametric quantile regression methods offer a complementary strategy for estimating conditional quantile functions. We compare estimated reference curves for height using the penalized likelihood approach of Cole and Green (Statistics in Medicine 1992; 11:1305-1319) with quantile regression curves based on data used for modern Finnish reference charts. An advantage of the quantile regression approach is that it is relatively easy to incorporate prior growth and other covariates into the analysis of longitudinal growth data. Quantile specific autoregressive models for unequally spaced measurements are introduced and their application to diagnostic screening is illustrated. Copyright © 2005 John Wiley & Sons, Ltd.},
   author = {Ying Wei and Anneli Pere and Roger Koenker and Xuming He},
   doi = {10.1002/SIM.2271},
   issn = {1097-0258},
   issue = {8},
   journal = {Statistics in Medicine},
   keywords = {growth curves,height,longitudinal data,quantile regression},
   month = {4},
   pages = {1369-1382},
   pmid = {16143984},
   publisher = {John Wiley & Sons, Ltd},
   title = {Quantile regression methods for reference growth charts},
   volume = {25},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/sim.2271 https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.2271 https://onlinelibrary.wiley.com/doi/10.1002/sim.2271},
   year = {2006},
}
@article{Cade2003,
   author = {Brian S. Cade and Barry R. Noon},
   doi = {10.2307/3868138},
   issn = {15409295},
   issue = {8},
   journal = {Frontiers in Ecology and the Environment},
   month = {10},
   pages = {412},
   title = {A Gentle Introduction to Quantile Regression for Ecologists},
   volume = {1},
   url = {http://doi.wiley.com/10.2307/3868138},
   year = {2003},
}
@book{Koenker2017,
   author = {Roger Koenker and Victor Chernozhukov and Xuming He and Limin Peng},
   doi = {10.1201/9781315120256},
   isbn = {9781315120256},
   month = {10},
   publisher = {Chapman and Hall/CRC},
   title = {Handbook of Quantile Regression},
   url = {https://www.taylorfrancis.com/books/9781498725293},
   year = {2017},
}
@article{Marcon2016,
   abstract = {A simple approach for modeling multivariate extremes is to consider the vector of component-wise maxima and their max-stable distributions. The extremal dependence can be inferred by estimating the angular measure or, alternatively, the Pickands dependence function. We propose a nonparametric Bayesian model that allows, in the bivariate case, the simultaneous estimation of both functional representations through the use of polynomials in the Bernstein form. The constraints required to provide a valid extremal dependence are addressed in a straightforward manner, by placing a prior on the coefficients of the Bernstein polynomials which gives probability one to the set of valid functions. The prior is extended to the polynomial degree, making our approach nonparametric. Although the analytical expression of the posterior is unknown, inference is possible via a trans-dimensional MCMC scheme. We show the efficiency of the proposed methodology by means of a simulation study. The extremal behaviour of log-returns of daily exchange rates between the Pound Sterling vs the U.S. Dollar and the Pound Sterling vs the Japanese Yen is analysed for illustrative purposes.},
   author = {Giulia Marcon and Simone A. Padoan and Isadora Antoniano-Villalobos},
   doi = {10.1214/16-EJS1162},
   issn = {19357524},
   issue = {2},
   journal = {Electronic Journal of Statistics},
   keywords = {Angular measure,Bayesian nonparametrics,Bernstein polynomials,Exchange rates,Extremal dependence,Generalised extreme value distribution,Max-stable distribution,Trans-dimensional MCMC},
   month = {1},
   pages = {3310-3337},
   publisher = {Institute of Mathematical Statistics and Bernoulli Society},
   title = {Bayesian inference for the extremal dependence},
   volume = {10},
   url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-10/issue-2/Bayesian-inference-for-the-extremal-dependence/10.1214/16-EJS1162.full https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-10/issue-2/Bayesian-inf},
   year = {2016},
}
@article{Guillotte2016,
   abstract = {Pickands dependence functions characterize bivariate extreme value copulas. In this paper, we study the class of polynomial Pickands functions. We provide a solution for the characterization of such polynomials of degree at most m + 2, m ≥ 0, and show that these can be parameterized by a vector in Rm+1 belonging to the intersection of two ellipsoids. We also study the class of Bernstein approximations of order m+2 of Pickands functions which are shown to be (polynomial) Pickands functions and parameterized by a vector in Rm+1 belonging to a polytope. We give necessary and sufficient conditions for which a polynomial Pickands function is in fact a Bernstein approximation of some Pickands function. Approximation results of Pickands dependence functions by polynomials are given. Finally, inferential methodology is discussed and comparisons based on simulated data are provided.},
   author = {Simon Guillotte and François Perron},
   doi = {10.3150/14-BEJ656},
   issn = {13507265},
   issue = {1},
   journal = {Bernoulli},
   keywords = {Bernstein's theorem,Extreme value copulas,Lorentz degree,Pickands dependence function,Polynomials,Spectral measure},
   pages = {213-241},
   title = {Polynomial Pickands functions},
   volume = {22},
   year = {2016},
}
@article{Tran2021,
   abstract = {The Latent River Problem has emerged as a flagship problem for causal discovery in extreme value statistics. This paper gives QTree, a simple and efficient algorithm to solve the Latent River Problem that outperforms existing methods. QTree returns a directed graph and achieves almost perfect recovery on the Upper Danube, the existing benchmark dataset, as well as on new data from the Lower Colorado River in Texas. It can handle missing data, has an automated parameter tuning procedure, and runs in time $O(n |V|^2)$, where $n$ is the number of observations and $|V|$ the number of nodes in the graph. In addition, under a Bayesian network model for extreme values with propagating noise, we show that the QTree estimator returns for $n\to\infty$ a.s. the correct tree.},
   author = {Ngoc Mai Tran and Johannes Buck and Claudia Klüppelberg},
   keywords = {Bayesian networks,causal inference,di-rected graphical models,extreme values statistics,max-linear},
   month = {2},
   title = {Estimating a Latent Tree for Extremes},
   url = {http://arxiv.org/abs/2102.06197},
   year = {2021},
}
@article{Beranger2021,
   abstract = {Estimation of extreme quantile regions, spaces in which future extreme events can occur with a given low probability, even beyond the range of the observed data, is an important task in the analysis of extremes. Existing methods to estimate such regions are available, but do not provide any measures of estimation uncertainty. We develop univariate and bivariate schemes for estimating extreme quantile regions under the Bayesian paradigm that outperforms existing approaches and provides natural measures of quantile region estimate uncertainty. We examine the method’s performance in controlled simulation studies. We illustrate the applicability of the proposed method by analysing high bivariate quantiles for pairs of pollutants, conditionally on different temperature gradations, recorded in Milan, Italy.},
   author = {Boris Beranger and Simone A. Padoan and Scott A. Sisson},
   doi = {10.1007/s10687-019-00364-0},
   isbn = {1068701900},
   issn = {1572915X},
   issue = {2},
   journal = {Extremes},
   keywords = {Air pollution,Bayesian nonparametrics,Bernstein polynomials,Extremal dependence,Extreme quantile regions,Max-stable distributions},
   pages = {349-375},
   publisher = {Extremes},
   title = {Estimation and uncertainty quantification for extreme quantile regions},
   volume = {24},
   year = {2021},
}
@article{Balkema1974,
   author = {A. A. Balkema and L. de Haan},
   doi = {10.1214/aop/1176996548},
   issn = {0091-1798},
   issue = {5},
   journal = {The Annals of Probability},
   month = {10},
   title = {Residual Life Time at Great Age},
   volume = {2},
   url = {https://www.jstor.org/stable/2959306?seq=1#metadata_info_tab_contents https://projecteuclid.org/journals/annals-of-probability/volume-2/issue-5/Residual-Life-Time-at-Great-Age/10.1214/aop/1176996548.full},
   year = {1974},
}
@article{Wood2021,
   abstract = {Generalized additive (mixed) models, some of their extensions and other generalized ridge regression with multiple smoothing parameter estimation by (Restricted) Marginal Likelihood, Generalized Cross Validation and similar, or using iterated nested Laplace approximation for fully Bayesian inference. See Wood (2017) <doi:10.1201/9781315370279> for an overview. Includes a gam() function, a wide variety of smoothers, 'JAGS' support and distributions beyond the exponential family.},
   author = {Simon Wood},
   journal = {R Package},
   title = {Mixed GAM Computation Vehicle with Automatic Smoothness Estimation},
   year = {2021},
}
@article{Wadsworth2016,
   abstract = {To model the tail of a distribution, one has to define the threshold above or below which an extreme value model produces a suitable fit. Parameter stability plots, whereby one plots maximum likelihood estimates of supposedly threshold-independent parameters against threshold, form one of the main tools for threshold selection by practitioners, principally due to their simplicity. However, one repeated criticism of these plots is their lack of interpretability, with pointwise confidence intervals being strongly dependent across the range of thresholds. In this article, we exploit the independent-increments structure of maximum likelihood estimators to produce complementary plots with greater interpretability, and suggest a simple likelihood-based procedure that allows for automated threshold selection. Supplementary materials for this article are available online.},
   author = {J. L. Wadsworth},
   doi = {10.1080/00401706.2014.998345/SUPPL_FILE/UTCH_A_998345_SM1169.ZIP},
   issn = {15372723},
   issue = {1},
   journal = {Technometrics},
   keywords = {Diagnostic plots,Extreme value modeling,Maximum likelihood,Threshold selection},
   month = {1},
   pages = {116-126},
   publisher = {American Statistical Association},
   title = {Exploiting structure of maximum likelihood estimators for extreme value threshold selection},
   volume = {58},
   url = {https://amstat.tandfonline.com/doi/abs/10.1080/00401706.2014.998345},
   year = {2016},
}
@article{Velthoen2021,
   abstract = {Extreme quantile regression provides estimates of conditional quantiles outside the range of the data. Classical methods such as quantile random forests perform poorly in such cases since data in the tail region are too scarce. Extreme value theory motivates to approximate the conditional distribution above a high threshold by a generalized Pareto distribution with covariate dependent parameters. This model allows for extrapolation beyond the range of observed values and estimation of conditional extreme quantiles. We propose a gradient boosting procedure to estimate a conditional generalized Pareto distribution by minimizing its deviance. Cross-validation is used for the choice of tuning parameters such as the number of trees and the tree depths. We discuss diagnostic plots such as variable importance and partial dependence plots, which help to interpret the fitted models. In simulation studies we show that our gradient boosting procedure outperforms classical methods from quantile regression and extreme value theory, especially for high-dimensional predictor spaces and complex parameter response surfaces. An application to statistical post-processing of weather forecasts with precipitation data in the Netherlands is proposed.},
   author = {Jasper Velthoen and Clément Dombry and Juan-Juan Cai and Sebastian Engelke},
   isbn = {2103.00808v1},
   journal = {Preprint},
   keywords = {extreme quantile regression,extreme value theory,generalized Pareto distribution,gradient boosting,tree-based methods},
   month = {3},
   title = {Gradient boosting for extreme quantile regression},
   url = {https://arxiv.org/abs/2103.00808v1},
   year = {2021},
}
@article{Youngman2020,
   abstract = {R package version 0.1.4},
   author = {Ben Youngman},
   journal = {R Package},
   title = {evgam: Generalised Additive Extreme Value Models},
   url = {https://cran.r-project.org/package=evgam},
   year = {2020},
}
@generic{Koenker2017a,
   abstract = {Since Quetelet's work in the nineteenth century, social science has iconified the average man, that hypothetical man without qualities who is comfortable with his head in the oven and his feet in a bucket of ice. Conventional statistical methods since Quetelet have sought to estimate the effects of policy treatments for this average man. However, such effects are often quite heterogeneous: Medical treatments may improve life expectancy but also impose serious short-term risks; reducing class sizes may improve the performance of good students but not help weaker ones, or vice versa. Quantile regression methods can help to explore these heterogeneous effects. Some recent developments in quantile regression methods are surveyed in this review.},
   author = {Roger Koenker},
   doi = {10.1146/annurev-economics-063016-103651},
   issn = {19411391},
   journal = {Annual Review of Economics},
   keywords = {Causal inference,Heterogeneity,Quantile regression,Treatment effects},
   month = {8},
   pages = {155-176},
   publisher = {Annual Reviews},
   title = {Quantile regression: 40 years on},
   volume = {9},
   url = {https://www.annualreviews.org/doi/abs/10.1146/annurev-economics-063016-103651},
   year = {2017},
}
@article{Knochenhauer2004,
   author = {Michael Knochenhauer and Pekka Louko},
   city = {London},
   doi = {10.1007/978-0-85729-410-4_241},
   journal = {Probabilistic Safety Assessment and Management},
   pages = {1498-1503},
   publisher = {Springer London},
   title = {Guidance for External Events Analysis},
   url = {http://link.springer.com/10.1007/978-0-85729-410-4_241},
   year = {2004},
}
@article{,
   title = {NARSIS New Approach to Reactor Safety ImprovementS WP1: Characterization of potential physical threats due to different external hazards and scenarios Del 1.1 Review of state-of-the art for hazard and multi-hazard characterisation},
}
@article{Hansen2020,
   abstract = {Design and re-analysis of offshore structures requires the joint estimation of extreme values for a set of environmental variables, representing so-called long-term and short-term characteristics of the environment, subject to sources of systematic variation including directionality and seasonality. Estimation is complicated by numerous sources of uncertainty, typically including limited sample size and the specification of a number of analysis parameters (such as thresholds for peaks over threshold analysis). In this work, we present a model to estimate joint extremal characteristics of the ocean environment incorporating non-stationary marginal and conditional extreme value analysis, and thorough uncertainty quantification, within a Bayesian framework. The model is used to quantify the joint directional–seasonal structure of extremes waves, winds and currents at a location in the Danish sector of the North Sea.},
   author = {Hans Fabricius Hansen and David Randell and Allan Rod Zeeberg and Philip Jonathan},
   doi = {10.1016/J.OCEANENG.2019.106665},
   issn = {0029-8018},
   journal = {Ocean Engineering},
   keywords = {Bayesian,Conditional extremes,Extremes,Non-stationary,Offshore design,Uncertainty},
   month = {1},
   pages = {106665},
   publisher = {Pergamon},
   title = {Directional–seasonal extreme value analysis of North Sea storm conditions},
   volume = {195},
   year = {2020},
}
@article{DeHaan1998,
   abstract = {Multivariate extreme value theory is used to estimate the probability of failure of a sea-wall near the town of Petten in Noord Holland, The Netherlands. The sample consists of 828 observations of still water levels and wave heights collected during storm events over a 13-year period. The paper sketches the probabilistic and statistical theory behind the estimation procedures used.},
   author = {Laurens de Haan and John de Ronde},
   doi = {10.1023/A:1009909800311},
   issn = {1572-915X},
   issue = {1},
   journal = {Extremes 1998 1:1},
   keywords = {Civil Engineering,Economics,Environmental Management,Finance,Hydrogeology,Insurance,Management,Quality Control,Reliability,Safety and Risk,Statistics,Statistics for Business,general},
   pages = {7-45},
   publisher = {Springer},
   title = {Sea and Wind: Multivariate Extremes at Work},
   volume = {1},
   url = {https://link.springer.com/article/10.1023/A:1009909800311},
   year = {1998},
}
@article{Murphy-Barltrop2022,
   abstract = {In many practical applications, evaluating the joint impact of combinations of environmental variables is important for risk management and structural design analysis. When such variables are considered simultaneously, non-stationarity can exist within both the marginal distributions and dependence structure, resulting in complex data structures. In the context of extremes, few methods have been proposed for modelling trends in extremal dependence, even though capturing this feature is important for quantifying joint impact. Motivated by the increasing dependence of data from the UK Climate Projections, we propose a novel semi-parametric modelling framework for bivariate extremal dependence structures. This framework allows us to capture a wide variety of dependence trends for data exhibiting asymptotic independence. When applied to the aforementioned dataset, our model is able to capture observed dependence trends and, in combination with models for marginal non-stationarity, can be used to produce estimates of bivariate risk measures at future time points.},
   author = {C. J. R. Murphy-Barltrop and J. L. Wadsworth},
   journal = {arXiv},
   keywords = {and phrases,extremal dependence,multivariate extremes,non-stationary processes},
   title = {Modelling non-stationarity in asymptotically independent extremes},
   volume = {2203.05860},
   url = {https://arxiv.org/abs/2203.05860},
   year = {2022},
}
@article{Wadsworth2019,
   abstract = {Currently available models for spatial extremes suffer either from
inflexibility in the dependence structures that they can capture, lack of
scalability to high dimensions, or in most cases, both of these. We present an
approach to spatial extreme value theory based on the conditional multivariate
extreme value model, whereby the limit theory is formed through conditioning
upon the value at a particular site being extreme. The ensuing methodology
allows for a flexible class of dependence structures, as well as models that
can be fitted in high dimensions. To overcome issues of conditioning on a
single site, we suggest a joint inference scheme based on all observation
locations, and implement an importance sampling algorithm to provide spatial
realizations and estimates of quantities conditioning upon the process being
extreme at any of one of an arbitrary set of locations. The modelling approach
is applied to Australian summer temperature extremes, permitting assessment the
spatial extent of high temperature events over the continent.},
   author = {Jennifer L. Wadsworth and Jonathan Tawn},
   doi = {10.48550/arxiv.1912.06560},
   month = {12},
   title = {Higher-dimensional spatial extremes via single-site conditioning},
   url = {https://arxiv.org/abs/1912.06560},
   year = {2019},
}
@article{Diederen2019,
   abstract = {We present a new method to generate spatially coherent river discharge peaks over multiple river basins, which can be used for continental event-based probabilistic flood risk assessment. We first extract extreme events from river discharge time series data over a large set of locations by applying new peak identification and peak-matching methods. Then we describe these events using the discharge peak at each location while accounting for the fact that the events do not affect all locations. Lastly we fit the state-of-the-art multivariate extreme value distribution to the discharge peaks and generate from the fitted model a large catalogue of spatially coherent synthetic event descriptors. We demonstrate the capability of this approach in capturing the statistical dependence over all considered locations. We also discuss the limitations of this approach and investigate the sensitivity of the outcome to various model parameters.},
   author = {Dirk Diederen and Ye Liu and Ben Gouldby and Ferdinand Diermanse and Sergiy Vorogushyn},
   doi = {10.5194/nhess-19-1041-2019},
   issn = {16849981},
   issue = {5},
   journal = {Natural Hazards and Earth System Sciences},
   pages = {1041-1053},
   title = {Stochastic generation of spatially coherent river discharge peaks for continental event-based flood risk assessment},
   volume = {19},
   year = {2019},
}
@article{Shooter2022,
   abstract = {The joint extremal spatial dependence of wind speed and significant wave height in the North East Atlantic is quantified using Metop satellite scatterometer and hindcast observations for the period 2007–2018, and a multivariate spatial conditional extremes (MSCE) model, ultimately motivated by the work of Heffernan and Tawn (2004). The analysis involves (a) registering individual satellite swaths and corresponding hindcast data onto a template transect (running approximately north-east to south-west, between the British Isles and Iceland), (b) non-stationary directional-seasonal marginal extreme value analysis at a set of registration locations on the transect, (c) transformation from physical to standard Laplace scale using the fitted marginal model, (d) estimation of the MSCE model on the set of registration locations, and assessment of quality of model fit. A joint model is estimated for three spatial quantities: Metop wind speed, hindcast wind speed and hindcast significant wave height. Results suggest that, when conditioning on extreme Metop wind speed, extremal spatial dependence for all three quantities decays over approximately 600–800 km.},
   author = {Rob Shooter and Emma Ross and Agustinus Ribal and Ian R. Young and Philip Jonathan},
   doi = {10.1016/j.oceaneng.2022.110647},
   issn = {00298018},
   issue = {September 2021},
   journal = {Ocean Engineering},
   keywords = {Conditional extremes,Covariate effects,Extremal spatial dependence,Joint extremes,Metop},
   pages = {110647},
   publisher = {Elsevier Ltd},
   title = {Multivariate spatial conditional extremes for extreme ocean environments},
   volume = {247},
   url = {https://doi.org/10.1016/j.oceaneng.2022.110647},
   year = {2022},
}
@article{Quinn2019,
   abstract = {In this paper we seek to understand the nature of flood spatial dependence over the conterminous United States. We extend an existing conditional multivariate statistical model to enable its application to this large and heterogenous region and apply it to a 40-year data set of ~2,400 U.S. Geological Survey gauge series records to simulate 1,000 years of U.S. flooding comprising more than 63,000 individual events with realistic spatial dependence. A continental-scale hydrodynamic model at 30 m resolution is then used to calculate the economic loss arising from each of these events. From this we are able to compute the probability that different values of U.S. annual total economic loss due to flooding are exceeded (i.e., a loss-exceedance curve). Comparing these data to an observed flood loss-exceedance curve for the period 1988–2017 shows a reasonable match for annual losses with probability below 10% (e.g., >1 in 10-year return period). This analysis suggests that there is a 1% chance of U.S. annual fluvial flood losses exceeding $78Bn in any given year, and a 0.1% chance of them exceeding $136Bn. Analysis of the set of stochastic events and losses yields new insights into the nature of flooding and flood risk in the United States. In particular, we confirm the strong relationship between flood affected area and event peak magnitude, but show considerable variability in this relationship between adjacent U.S. regions. The analysis provides a significant advance over previous national flood risk analyses as it gives the full loss-exceedance curve instead of simply the average annual loss.},
   author = {Niall Quinn and Paul D. Bates and Jeff Neal and Andy Smith and Oliver Wing and Chris Sampson and James Smith and Janet Heffernan},
   doi = {10.1029/2018WR024205},
   issn = {19447973},
   issue = {3},
   journal = {Water Resources Research},
   keywords = {Flood risk,Flooding,Hydrodynamic modelling,Spatial dependence},
   pages = {1890-1911},
   title = {The Spatial Dependence of Flood Hazard and Risk in the United States},
   volume = {55},
   year = {2019},
}
@article{Wing2020,
   abstract = {Global flood models integrate flood maps of constant probability in space, ignoring the correlation between sites and thus potentially misestimating the risk posed by extreme events. Stochastic flood models alleviate this issue through the simulation of flood events with a realistic spatial structure, yet their proliferation at large scales has historically been inhibited by data quality and computer availability. In this paper, we show, for the first time, the efficacy of modeled river discharge reanalyses in the characterization of flood spatial dependence in the absence of a dense stream gauge network. While global hydrological models may show poor correspondence with absolute observed river flows, we find that the rate at which they can simulate the joint occurrence of relative flow exceedances at two given locations is broadly similar to when a gauge-based statistical model is used. Evidenced over the United States, flood events simulated using observed gauge data from the U.S. Geological Survey versus those generated using modeled streamflows have similar (i) distributions of site-to-site correlation strength, (ii) relationships between event size and return period, and, importantly, (iii) loss distributions when incorporated into a continental-scale flood risk model. Extremal dependence is generally quantified less accurately on larger rivers, in arid climates, in mountainous terrain, and for the rarest high-magnitude events. However, local-scale errors are shown to broadly cancel each other out when combined, producing an unbiased flood spatial dependence model. These findings suggest that building accurate stochastic flood models worldwide may no longer be a distant aspiration.},
   author = {Oliver E.J. Wing and Niall Quinn and Paul D. Bates and Jeffrey C. Neal and Andrew M. Smith and Christopher C. Sampson and Gemma Coxon and Dai Yamazaki and Edwin H. Sutanudjaja and Lorenzo Alfieri},
   doi = {10.1029/2020WR027692},
   issn = {19447973},
   issue = {8},
   journal = {Water Resources Research},
   keywords = {flood risk,flooding,hydraulic modelling,hydrological modelling,spatial dependence},
   title = {Toward Global Stochastic River Flood Modeling},
   volume = {56},
   year = {2020},
}
@article{Keef2013a,
   abstract = {Flooding is a natural phenomenon that regularly causes financial and human devastation around the world. In many countries the risk of flooding is managed by society through a combination of governmental agencies and the insurance industry. For both these types of organisation an estimate of the largest, or most widespread, events that can be expected to occur is useful. Such estimates can be used to help in preparing or co-ordinating flood mitigation activities and by the insurance and re-insurance industries to assess financial risk. In this paper we develop a method to simulate a set of synthetic flood events that can be used to estimate the probability of widespread floods. We demonstrate this method using data from a set of UK river flow gauges. The model used in this simulation process is based on the conditional exceedance model of Heffernan and Tawn, extended to incorporate features typically found in the data for extreme river floods. We also present an improved estimation method for the model parameters and demonstrate its advantages through the results of a simulation study. The benefits of the method over previous models used are that it provides a theoretical basis for extrapolation and is flexible enough to account for varying strengths of extremal dependence that are observed in flood data. © 2012 John Wiley & Sons, Ltd.},
   author = {Caroline Keef and Jonathan A. Tawn and Rob Lamb},
   doi = {10.1002/env.2190},
   issn = {11804009},
   issue = {1},
   journal = {Environmetrics},
   keywords = {Conditional exceedance model,Extremal dependence,Multivariate extreme value theory,Spatial flood risk assessment},
   pages = {13-21},
   title = {Estimating the probability of widespread flood events},
   volume = {24},
   year = {2013},
}
@article{Tendijck2021,
   abstract = {There currently exist a variety of statistical methods for modeling bivariate extremes. However, when the dependence between variables is driven by more than one latent process, these methods are likely to fail to give reliable inferences. We consider situations in which the observed dependence at extreme levels is a mixture of a possibly unknown number of much simpler bivariate distributions. For such structures, we demonstrate the limitations of existing methods and propose two new methods: an extension of the Heffernan–Tawn conditional extreme value model to allow for mixtures and an extremal quantile-regression approach. The two methods are examined in a simulation study and then applied to oceanographic data. Finally, we discuss extensions including a subasymptotic version of the proposed model, which has the potential to give more efficient results by incorporating data that are less extreme. Both new methods outperform existing approaches when mixtures are present.},
   author = {Stan Tendijck and Emma Eastoe and Jonathan Tawn and David Randell and Philip Jonathan},
   doi = {10.1080/01621459.2021.1996379},
   issn = {0162-1459},
   issue = {0},
   journal = {Journal of the American Statistical Association},
   keywords = {Conditional extremes,Mixture distributions,Multivariate extremes,Offshore wave extremes,Quantile-regression},
   month = {12},
   pages = {1-12},
   publisher = {American Statistical Association},
   title = {Modeling the Extremes of Bivariate Mixture Distributions With Application to Oceanographic Data},
   volume = {0},
   url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2021.1996379},
   year = {2021},
}
@article{Curceac2020,
   abstract = {This study investigated core components of an extreme value methodology for the estimation of high-flow frequencies from agricultural surface water run-off. The Generalized Pareto distribution (GPD) was used to model excesses in time-series data that resulted from the ‘Peaks Over Threshold’ (POT) method. First, the performance of eight different GPD parameter estimators was evaluated through a Monte Carlo experiment. Second, building on the estimator comparison, two existing automated GPD threshold selection methods were evaluated against a proposed approach that automates the threshold stability plots. For this second experiment, methods were applied to discharge measured at a highly-instrumented agricultural research facility in the UK. By averaging fine-resolution 15-minute data to hourly, 6-hourly and daily scales, we were also able to determine the effect of scale on threshold selection, as well as the performance of each method. The results demonstrate the advantages of the proposed threshold selection method over two commonly applied methods, while at the same time providing useful insights into the effect of the choice of the scale of measurement on threshold selection. The results can be generalised to similar water monitoring schemes and are important for improved characterisations of flood events and the design of associated disaster management protocols.},
   author = {Stelian Curceac and Peter M. Atkinson and Alice Milne and Lianhai Wu and Paul Harris},
   doi = {10.1016/J.JHYDROL.2020.124845},
   issn = {00221694},
   journal = {Journal of Hydrology},
   keywords = {Flood frequency analysis,Generalized pareto distribution,Grassland agriculture,Peaks over threshold,Scale effects,Threshold selection},
   month = {6},
   publisher = {Elsevier B.V.},
   title = {An evaluation of automated GPD threshold selection methods for hydrological extremes across different scales},
   volume = {585},
   url = {https://doi.org/10.1016/j.jhydrol.2020.124845},
   year = {2020},
}
@article{Lamb2010,
   abstract = {To date, national- and regional-scale flood risk assessments have provided valuable information about the annual expected consequences of flooding, but not the exposure to widespread concurrent flooding that could have damaging consequences for people and the economy. We present a new method for flood risk assessment that accommodates the risk of widespread flooding. It is based on a statistical conditional exceedance model, which is fitted to gauged data and describes the joint probability of extreme river flows or sea levels at multiple locations. The method can be applied together with data from models for flood defence systems and economic damages to calculate a risk profile describing the probability distribution of economic losses or other consequences aggregated over a region. The method has the potential to augment national or regional risk assessments of expected annual damage with new information about the likelihoods, extent and impacts of events that could contribute to the risk. © The Authors. Journal of Flood Risk Management © 2010 The Chartered Institution of Water and Environmental Management.},
   author = {R. Lamb and C. Keef and J. Tawn and S. Laeger and I. Meadowcroft and S. Surendran and P. Dunning and C. Batstone},
   doi = {10.1111/j.1753-318X.2010.01081.x},
   issn = {1753318X},
   issue = {4},
   journal = {Journal of Flood Risk Management},
   keywords = {Economic damages,Flood risk,Joint probability,Spatial dependence},
   pages = {323-336},
   title = {A new method to assess the risk of local and widespread flooding on rivers and coasts},
   volume = {3},
   year = {2010},
}
@article{Towe2019,
   abstract = {Multivariate extreme value models are used to estimate joint risk in a number of applications, with a particular focus on environmental fields ranging from climatology and hydrology to oceanography and seismic hazards. The semi-parametric conditional extreme value model of Heffernan and Tawn involving a multivariate regression provides the most suitable of current statistical models in terms of its flexibility to handle a range of extremal dependence classes. However, the standard inference for the joint distribution of the residuals of this model suffers from the curse of dimensionality because, in a d-dimensional application, it involves a d−1-dimensional nonparametric density estimator, which requires, for accuracy, a number points and commensurate effort that is exponential in d. Furthermore, it does not allow for any partially missing observations to be included, and a previous proposal to address this is extremely computationally intensive, making its use prohibitive if the proportion of missing data is nontrivial. We propose to replace the d−1-dimensional nonparametric density estimator with a model-based copula with univariate marginal densities estimated using kernel methods. This approach provides statistically and computationally efficient estimates whatever the dimension, d, or the degree of missing data. Evidence is presented to show that the benefits of this approach substantially outweigh potential misspecification errors. The methods are illustrated through the analysis of UK river flow data at a network of 46 sites and assessing the rarity of the 2015 floods in North West England.},
   author = {R. P. Towe and J. A. Tawn and R. Lamb and C. G. Sherlock},
   doi = {10.1002/ENV.2575},
   issn = {1099095X},
   issue = {8},
   journal = {Environmetrics},
   keywords = {copula,dependence modelling,missing values,multivariate extreme value theory,spatial flood risk assessment},
   month = {12},
   publisher = {John Wiley and Sons Ltd},
   title = {Model-based inference of conditional extreme value distributions with hydrological applications},
   volume = {30},
   year = {2019},
}
@article{Northrop2017,
   abstract = {Design conditions for marine structures are typically informed by threshold-based extreme value analyses of oceanographic variables, in which excesses of a high threshold are modelled by a generalized Pareto distribution. Too low a threshold leads to bias from model misspecification, and raising the threshold increases the variance of estimators: a bias–variance trade-off. Many existing threshold selection methods do not address this trade-off directly but rather aim to select the lowest threshold above which the generalized Pareto model is judged to hold approximately. In the paper Bayesian cross-validation is used to address the trade-off by comparing thresholds based on predictive ability at extreme levels. Extremal inferences can be sensitive to the choice of a single threshold. We use Bayesian model averaging to combine inferences from many thresholds, thereby reducing sensitivity to the choice of a single threshold. The methodology is applied to significant wave height data sets from the northern North Sea and the Gulf of Mexico.},
   author = {Paul J. Northrop and Nicolas Attalides and Philip Jonathan},
   doi = {10.1111/RSSC.12159},
   issn = {14679876},
   issue = {1},
   journal = {Journal of the Royal Statistical Society. Series C: Applied Statistics},
   keywords = {Cross-validation,Extreme value theory,Generalized Pareto distribution,Predictive inference,Threshold},
   month = {1},
   pages = {93-120},
   publisher = {Blackwell Publishing Ltd},
   title = {Cross-validatory extreme value threshold selection and uncertainty with application to ocean storm severity},
   volume = {66},
   year = {2017},
}
@article{Richards2021,
   abstract = {Inference on the extremal behaviour of spatial aggregates of precipitation is important for quantifying river flood risk. There are two classes of previous approach, with one failing to ensure self-consistency in inference across different regions of aggregation and the other imposing highly restrictive assumptions. To overcome these issues, we propose a model for high-resolution precipitation data, from which we can simulate realistic fields and explore the behaviour of spatial aggregates. Recent developments have seen spatial extensions of the Heffernan and Tawn (2004) model for conditional multivariate extremes, which can handle a wide range of dependence structures. Our contribution is twofold: extensions and improvements of this approach and its model inference for high-dimensional data; and a novel framework for deriving aggregates addressing edge effects and sub-regions without rain. We apply our modelling approach to gridded East-Anglia, UK precipitation data. Return-level curves for spatial aggregates over different regions of various sizes are estimated and shown to fit very well to the data.},
   author = {Jordan Richards and Jonathan A. Tawn and Simon Brown},
   doi = {10.48550/arxiv.2102.10906},
   journal = {Preprint},
   keywords = {extremal dependence,extreme precipitation,spatial aggregates,spatial conditional extremes},
   month = {2},
   title = {Modelling Extremes of Spatial Aggregates of Precipitation using Conditional Methods},
   url = {https://arxiv.org/abs/2102.10906v3},
   year = {2021},
}
@article{Jane2020,
   abstract = {Miami-Dade County (south-east Florida) is among the most vulnerable regions to sea level rise in the United States, due to a variety of natural and human factors. The co-occurrence of multiple, often statistically dependent flooding drivers-termed compound events-typically exacerbates impacts compared with their isolated occurrence. Ignoring dependencies between the drivers will potentially lead to underestimation of flood risk and under-design of flood defence structures. In Miami-Dade County water control structures were designed assuming full dependence between rainfall and Ocean-side Water Level (O-sWL), a conservative assumption inducing large safety factors. Here, an analysis of the dependence between the principal flooding drivers over a range of lags at three locations across the county is carried out. A two-dimensional analysis of rainfall and O-sWL showed that the magnitude of the conservative assumption in the original design is highly sensitive to the regional sea level rise projection considered. Finally, the vine copula and Heffernan and Tawn (2004) models are shown to outperform five standard higher-dimensional copulas in capturing the dependence between the principal drivers of compound flooding: Rainfall, O-sWL, and groundwater level. The work represents a first step towards the development of a new framework capable of capturing dependencies between different flood drivers that could potentially be incorporated into future Flood Protection Level of Service (FPLOS) assessments for coastal water control structures.},
   author = {Robert Jane and Luis Cadavid and Jayantha Obeysekera and Thomas Wahl},
   doi = {10.5194/NHESS-20-2681-2020},
   issn = {16849981},
   issue = {10},
   journal = {Natural Hazards and Earth System Sciences},
   month = {10},
   pages = {2681-2699},
   publisher = {Copernicus GmbH},
   title = {Multivariate statistical modelling of the drivers of compound flood events in south Florida},
   volume = {20},
   year = {2020},
}
@article{Salvadori2011,
   abstract = {Calculating return periods and design quantiles in a multivariate environment is a difficult problem: this paper tries to make the issue clear. First, we outline a possible way to introduce a consistent theoretical framework for the calculation of the return period in a multi-dimensional environment, based on Copulas and the Kendall's measure. Secondly, we introduce several approaches for the identification of suitable design events: these latter quantities are of utmost importance in practical applications, but their calculation is yet limited, due to the lack of an adequate theoretical environment where to embed the problem. Throughout the paper, a case study involving the behavior of a dam is used to illustrate the new concepts outlined in this work. © Author(s) 2011. CC Attribution 3.0 License.},
   author = {G. Salvadori and C. De Michele and F. Durante},
   doi = {10.5194/HESS-15-3293-2011},
   issn = {10275606},
   issue = {11},
   journal = {Hydrology and Earth System Sciences},
   pages = {3293-3305},
   title = {On the return period and design in a multivariate framework},
   volume = {15},
   year = {2011},
}
@article{Rojas2011,
   abstract = {In this work we asses the benefits of removing bias in climate forcing data used for hydrological climate change impact assessment at pan-European scale, with emphasis on floods. Climate simulations from the HIRHAM5-ECHAM5 model driven by the SRES-A1B emission scenario are corrected for bias using a histogram equalization method. As target for the bias correction we employ gridded interpolated observations of precipitation, average, minimum, and maximum temperature from the E-OBS data set. Bias removal transfer functions are derived for the control period 1961-1990. These are subsequently used to correct the climate simulations for the control period, and, under the assumption of a stationary error model, for the future time window 2071-2100. Validation against E-OBS climatology in the control period shows that the correction method performs successfully in removing bias in average and extreme statistics relevant for flood simulation over the majority of the European domain in all seasons. This translates into considerably improved simulations with the hydrological model of observed average and extreme river discharges at a majority of 554 validation river stations across Europe. Probabilities of extreme events derived employing extreme value techniques are also more closely reproduced. Results indicate that projections of future flood hazard in Europe based on uncorrected climate simulations, both in terms of their magnitude and recurrence interval, are likely subject to large errors. Notwithstanding the inherent limitations of the large-Correspondence to: R. Rojas scale approach used herein, this study strongly advocates the removal of bias in climate simulations prior to their use in hydrological impact assessment.},
   author = {R Rojas and L Feyen and A Dosio and D Bavera},
   doi = {10.5194/hess-15-2599-2011},
   journal = {Hydrol. Earth Syst. Sci},
   pages = {2599-2620},
   title = {Improving pan-European hydrological simulation of extreme events through statistical bias correction of RCM-driven climate simulations},
   volume = {15},
   url = {www.hydrol-earth-syst-sci.net/15/2599/2011/},
   year = {2011},
}
@article{Enayati2021,
   abstract = {This study aims to conduct a thorough investigation to compare the abilities of quantile mapping (QM) techniques as a bias correction method for the raw outputs from general circulation model (GCM)/ regional climate model (RCM) combinations. The Karkheh River basin in Iran was selected as a case study, due to its diverse topographic features, to test the performances of the bias correction methods under different conditions. The outputs of two GCM/RCM combinations (ICHEC and NOAA-ESM) were acquired from the coordinated regional climate downscaling experiment (CORDEX) dataset for this study. The results indicated that the performances of the QMs varied, depending on the transformation functions, parameter sets, and topographic conditions. In some cases, the QMs’ adjustments even made the GCM/RCM combinations’ raw outputs worse. The result of this study suggested that apart from DIST, PTF:scale, and SSPLIN, the rest of the considered QM methods can provide relatively improved results for both rainfall and temperature variables. It should be noted that, according to the results obtained from the diverse topographic conditions of the sub-basins, the empirical quantiles (QUANT) and robust empirical quantiles (RQUANT) methods proved to be excellent options to correct the bias of rainfall data, while all bias correction methods, with the notable exceptions of performed PTF:scale and SSPLIN, performed relatively well for the temperature variable.},
   author = {Maedeh Enayati and Omid Bozorg-Haddad and Javad Bazrafshan and Somayeh Hejabi and Xuefeng Chu},
   doi = {10.2166/WCC.2020.261},
   issn = {2040-2244},
   issue = {2},
   journal = {Journal of Water and Climate Change},
   keywords = {COREDEX,Climate change,Quantile mapping,RCM,| bias correction},
   month = {3},
   pages = {401-419},
   publisher = {IWA Publishing},
   title = {Bias correction capabilities of quantile mapping methods for rainfall and temperature variables},
   volume = {12},
   url = {http://creativecommons.org/licenses/by/4.0/},
   year = {2021},
}
@article{Osuch2016,
   abstract = {The aim of this study is to estimate likely changes in flood indices under a future climate and to assess the uncertainty in these estimates for selected catchments in Poland. Precipitation and temperature time series from climate simulations from the EURO-CORDEX initiative for the periods 1971-2000, 2021-2050 and 2071-2100 following the RCP4.5 and RCP8.5 emission scenarios have been used to produce hydrological simulations based on the HBV hydrological model. As the climate model outputs for Poland are highly biased, post processing in the form of bias correction was first performed so that the climate time series could be applied in hydrological simulations at a catchment-scale. The results indicate that bias correction significantly improves flow simulations and estimated flood indices based on comparisons with simulations from observed climate data for the control period. The estimated changes in the mean annual flood and in flood quantiles under a future climate indicate a large spread in the estimates both within and between the catchments. An ANOVA analysis was used to assess the relative contributions of the 2 emission scenarios, the 7 climate models and the 4 bias correction methods to the total spread in the projected changes in extreme river flow indices for each catchment. The analysis indicates that the differences between climate models generally make the largest contribution to the spread in the ensemble of the three factors considered. The results for bias corrected data show small differences between the four bias correction methods considered, and, in contrast with the results for uncorrected simulations, project increases in flood indices for most catchments under a future climate.},
   author = {Marzena Osuch and Deborah Lawrence and Hadush K Meresa and J J Napiorkowski and Renata J Romanowicz},
   doi = {10.1007/s00477-016-1296-5},
   isbn = {0047701612965},
   journal = {Stochastic Environmental Research and Risk Assessment},
   keywords = {ANOVA,Bias correction,Climate change,Floods,Poland},
   title = {Projected changes in flood indices in selected catchments in Poland in the 21st century},
   volume = {31},
   year = {2016},
}
@article{Haselsteiner2021,
   abstract = {Environmental contours are used to simplify the process of design response analysis. A wide variety of contour methods exist; however, there have been a very limited number of comparisons of these methods to date. This paper is the output of an open benchmarking exercise, in which contributors developed contours based on their preferred methods and submitted them for a blind comparison study. The exercise had two components—one, focusing on the robustness of contour methods across different offshore sites and, the other, focusing on characterizing sampling uncertainty. Nine teams of researchers contributed to the benchmark. The analysis of the submitted contours highlighted significant differences between contours derived via different methods. For example, the highest wave height value along a contour varied by as much as a factor of two between some submissions while the number of metocean data points or observations that fell outside a contour deviated by an order of magnitude between the contributions (even for contours with a return period shorter than the duration of the record). These differences arose from both different joint distribution models and different contour construction methods, however, variability from joint distribution models appeared to be higher than variability from contour construction methods.},
   author = {Andreas F. Haselsteiner and Ryan G. Coe and Lance Manuel and Wei Chai and Bernt Leira and Guilherme Clarindo and C. Guedes Soares and Ásta Hannesdóttir and Nikolay Dimitrov and Aljoscha Sander and Jan Hendrik Ohlendorf and Klaus Dieter Thoben and Guillaume de Hauteclocque and Ed Mackay and Philip Jonathan and Chi Qiao and Andrew Myers and Anna Rode and Arndt Hildebrandt and Boso Schmidt and Erik Vanem and Arne Bang Huseby},
   doi = {10.1016/J.OCEANENG.2021.109504},
   issn = {0029-8018},
   journal = {Ocean Engineering},
   keywords = {Environmental contour,Extreme response,Joint distribution,Metocean extremes,Structural reliability},
   month = {9},
   pages = {109504},
   publisher = {Pergamon},
   title = {A benchmarking exercise for environmental contours},
   volume = {236},
   year = {2021},
}
@article{Moftakhari2019,
   abstract = {A method to link bivariate statistical analysis and hydrodynamic modeling for flood hazard estimation in tidal channels and estuaries is presented and discussed for the general case where flood hazards are linked to upstream riverine discharge Q and downstream ocean level, H. Using a bivariate approach, there are many possible combinations of Q and H that jointly reflect a specific return period, T, raising questions about the best choice as boundary forcing in a hydrodynamic model. We show, first of all, how possible Q and H values depend on whether the definition of T corresponds to the probability of exceedance of “H OR Q” or “H AND Q”. We also show that flood hazards defined by “OR” return periods are more conservative than “AND” return periods. Finally, we introduce a new composite water surface profile to represent the spatially distributed hazard for return period T. The composite profile synthesizes hydrodynamic model results from the “AND” hazard scenario and two scenarios based on traditional univariate analysis, a “Marginal Q” scenario and a “Marginal H” scenario.},
   author = {Hamed Moftakhari and Jochen E. Schubert and Amir AghaKouchak and Richard A. Matthew and Brett F. Sanders},
   doi = {10.1016/j.advwatres.2019.04.009},
   issn = {03091708},
   issue = {September 2018},
   journal = {Advances in Water Resources},
   keywords = {Bivariate statistical analysis,Coastal flood hazards,Compound flooding,Hydrodynamic modeling},
   pages = {28-38},
   publisher = {Elsevier Ltd},
   title = {Linking statistical and hydrodynamic modeling for compound flood hazard assessment in tidal channels and estuaries},
   volume = {128},
   url = {https://doi.org/10.1016/j.advwatres.2019.04.009},
   year = {2019},
}
@article{Youngman2019a,
   abstract = {Generalized additive model (GAM) forms offer a flexible approach to capturing marginal variation. Such forms are used here to represent distributional variation in extreme values and presented in terms of spatio-temporal variation, which is often evident in environmental processes. A two-stage procedure is proposed that identifies extreme values as exceedances of a high threshold, which is defined as a fixed quantile and estimated by quantile regression. Excesses of the threshold are modelled with the generalized Pareto distribution (GPD). GAM forms are adopted for the threshold and GPD parameters, and directly estimated—in particular smoothing parameters—by restricted maximum likelihood, which provides an objective and relatively fast method of inference. The GAM models are used to produce return level maps for extreme wind gust speeds over the United States, which show extreme quantiles of the distribution of annual maximum gust speeds. Supplementary materials for this article are available online.},
   author = {Benjamin D. Youngman},
   doi = {10.1080/01621459.2018.1529596/SUPPL_FILE/UASA_A_1529596_SM9576.ZIP},
   issn = {1537274X},
   issue = {528},
   journal = {Journal of the American Statistical Association},
   keywords = {Extremal index,Generalized Pareto distribution,Quantile regression,Restricted maximum likelihood,Semiparametric model,U.S. wind gusts},
   month = {10},
   pages = {1865-1879},
   publisher = {American Statistical Association},
   title = {Generalized Additive Models for Exceedances of High Thresholds With an Application to Return Level Estimation for U.S. Wind Gusts},
   volume = {114},
   url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.2018.1529596},
   year = {2019},
}
@article{Simpson2022,
   abstract = {An integral part of carrying out statistical analysis for bivariate extreme events is characterising the tail dependence relationship between the two variables. In the extreme value theory literature, various techniques are available to assess or model different aspects of tail dependence; currently, inference must be carried out separately for each of these, with the possibility of contradictory conclusions. Recent developments by Nolde and Wadsworth (2022) have established theoretical links between different characterisations of extremal dependence, through studying the limiting shape of an appropriately-scaled sample cloud. We exploit these results for inferential purposes, by first developing an estimator for the sample limit set and then using this to deduce self-consistent estimates for the extremal dependence properties of interest. In simulations, the limit set estimates are shown to be successful across a range of distributions, and the estimates of dependence features are individually competitive with existing estimation techniques, and jointly provide a major improvement. We apply the approach to a data set of sea wave heights at pairs of locations, where the estimates successfully capture changes in the limiting shape of the sample cloud as the distance between the locations increases, including the weakening extremal dependence that is expected in environmental applications.},
   author = {Emma S. Simpson and Jonathan A. Tawn},
   journal = {arXiv},
   keywords = {bivariate extremes,coefficient of asymptotic independence,conditional extremes,environ-,extremal dependence structure,gauge function,mental contours,sample cloud},
   title = {Estimating the limiting shape of bivariate scaled sample clouds for self-consistent inference of extremal dependence properties},
   volume = {2207.02626},
   url = {http://arxiv.org/abs/2207.02626},
   year = {2022},
}
@article{Rohrbeck2021a,
   abstract = {Hazard event sets, which correspond to a collection of synthetic flood events, are an important tool for practitioners to analyse and manage future flood risks. In this paper, we address the issue of generating hazard event sets for northern England and southern Scotland, a region which has been particularly affected by flooding over the past years. We start by analysing extreme river flow across 45 gauges in the region using recently introduced ideas from extreme value theory. This results in a set of extremal principal components, with the first components describing the large-scale structure of the observed flood events, and we find interesting connections to the region's topography and climate. We then introduce a framework to approximate the distribution of the extremal principal components which is dimension reducing in that it distinctly handles the large-scale and local extremal behavior. Synthetic flood events are subsequently generated efficiently by sampling from the fitted distribution. Our approach for generating hazard event sets can be easily implemented by practitioners and our results indicate good agreement between the observed and simulated extreme river flow dynamics.},
   author = {Christian Rohrbeck and Daniel Cooley},
   journal = {arXiv},
   keywords = {multivariate extreme value theory,nonparametric bootstrapping,principal com-},
   title = {Simulating flood event sets using extremal principal components},
   volume = {2106.00630},
   url = {http://arxiv.org/abs/2106.00630},
   year = {2021},
}
@article{Brunner2016,
   abstract = {Estimates of flood event magnitudes with a certain return period are required for the design of hydraulic structures. While the return period is clearly defined in a univariate context, its definition is more challenging when the problem at hand requires considering the dependence between two or more variables in a multivariate framework. Several ways of defining a multivariate return period have been proposed in the literature, which all rely on different probability concepts. Definitions use the conditional probability, the joint probability, or can be based on the Kendall's distribution or survival function. In this study, we give a comprehensive overview on the tools that are available to define a return period in a multivariate context. We especially address engineers, practitioners, and people who are new to the topic and provide them with an accessible introduction to the topic. We outline the theoretical background that is needed when one is in a multivariate setting and present the reader with different definitions for a bivariate return period. Here, we focus on flood events and the different probability concepts are explained with a pedagogical, illustrative example of a flood event characterized by the two variables peak discharge and flood volume. The choice of the return period has an important effect on the magnitude of the design variable quantiles, which is illustrated with a case study in Switzerland. However, this choice is not arbitrary and depends on the problem at hand. WIREs Water 2016, 3:819–833. doi: 10.1002/wat2.1173. This article is categorized under: Engineering Water > Methods Engineering Water > Planning Water Science of Water > Water Extremes.},
   author = {Manuela Irene Brunner and Jan Seibert and Anne Catherine Favre},
   doi = {10.1002/wat2.1173},
   issn = {20491948},
   issue = {6},
   journal = {Wiley Interdisciplinary Reviews: Water},
   pages = {819-833},
   title = {Bivariate return periods and their importance for flood peak and volume estimation},
   volume = {3},
   year = {2016},
}
@article{Engelke2021,
   abstract = {Extremal graphical models encode the conditional independence structure of multivariate extremes. For the popular class of H\"usler--Reiss models, we propose a majority voting algorithm for learning the underlying graph from data through $L^1$ regularized optimization. We derive explicit conditions that ensure consistent graph recovery for general connected graphs. A key statistic in our method is the empirical extremal variogram. We prove non-asymptotic concentration bounds for this quantity that hold for general multivariate Pareto distributions and are of independent interest.},
   author = {Sebastian Engelke and Michaël Lalancette and Stanislav Volgushev},
   title = {Learning extremal graphical structures in high dimensions},
   url = {http://arxiv.org/abs/2111.00840},
   year = {2021},
}
@article{Barlow2022,
   abstract = {Metocean extremes often vary systematically with covariates such as direction and season. In this work, we present non-stationary models for the size and rate of occurrence of peaks over threshold of metocean variables with respect to one- or two-dimensional covariates. The variation of model parameters with covariate is described using a piecewise-linear function in one or two dimensions defined with respect to pre-specified node locations on the covariate domain. Parameter roughness is regulated to provide optimal predictive performance, assessed using cross-validation, within a penalised likelihood framework for inference. Parameter uncertainty is quantified using bootstrap resampling. The models are used to estimate extremes of storm peak significant wave height with respect to direction and season for a site in the northern North Sea. A covariate representation based on a triangulation of the direction-season domain with six nodes gives good predictive performance. The penalised piecewise-linear framework provides a flexible representation of covariate effects at reasonable computational cost.},
   author = {Anna Maria Barlow and Ed Mackay and Emma Eastoe and Philip Jonathan},
   issue = {1},
   keywords = {covariate,extreme,non-stationary,penalised likelihood,significant wave height},
   pages = {1-19},
   title = {A penalised piecewise-linear model for non-stationary extreme value analysis of peaks over threshold},
   url = {http://arxiv.org/abs/2201.03915},
   year = {2022},
}
@article{Lee2021,
   abstract = {We consider the problem of performing prediction when observed values are at their highest levels. We construct an inner product space of nonnegative random variables from transformed-linear combinations of independent regularly varying random variables. The matrix of inner products corresponds to the tail pairwise dependence matrix, which summarizes tail dependence. The projection theorem yields the optimal transformed-linear predictor, which has the same form as the best linear unbiased predictor in non-extreme prediction. We also construct prediction intervals based on the geometry of regular variation. We show that these intervals have good coverage in a simulation study as well as in two applications; prediction of high pollution levels, and prediction of large financial losses.},
   author = {Jeongjin Lee and Daniel Cooley},
   keywords = {air pollution,by us national science,cooley were partially supported,financial risk,foundation grant,jeongjin lee and daniel,matrix,multivariate regular variation,projection theorem,tail pairwise dependence},
   pages = {1-24},
   title = {Transformed-linear prediction for extremes},
   url = {http://arxiv.org/abs/2111.03754},
   year = {2021},
}
@article{Rohrbeck2021,
   abstract = {To address the need for efficient inference for a range of hydrological extreme value problems, spatial pooling of information is the standard approach for marginal tail estimation. We propose the first extreme value spatial clustering methods which account for both the similarity of the marginal tails and the spatial dependence structure of the data to determine the appropriate level of pooling. Spatial dependence is incorporated in two ways: to determine the cluster selection and to account for dependence of the data over sites within a cluster when making the marginal inference. We introduce a statistical model for the pairwise extremal dependence which incorporates distance between sites, and accommodates our belief that sites within the same cluster tend to exhibit a higher degree of dependence than sites in different clusters. By combining the models for the marginal tails and the dependence structure, we obtain a composite likelihood for the joint spatial distribution. We use a Bayesian framework which learns about both the number of clusters and their spatial structure, and that enables the inference of site-specific marginal distributions of extremes to incorporate uncertainty in the clustering allocation. The approach is illustrated using simulations, the analysis of daily precipitation levels in Norway and daily river flow levels in the UK. Code and data for the simulation study and river flow example are available in the online supplementary materials.},
   author = {Christian Rohrbeck and Jonathan A. Tawn},
   doi = {10.1080/10618600.2020.1777139/SUPPL_FILE/UCGS_A_1777139_SM3181.ZIP},
   issn = {15372715},
   issue = {1},
   journal = {Journal of Computational and Graphical Statistics},
   keywords = {Bayesian clustering,Composite likelihood,Extreme value analysis,Reversible jump Markov chain Monte Carlo,Spatio-temporal modeling},
   pages = {91-105},
   publisher = {American Statistical Association},
   title = {Bayesian Spatial Clustering of Extremal Behavior for Hydrological Variables},
   volume = {30},
   url = {https://www.tandfonline.com/doi/abs/10.1080/10618600.2020.1777139},
   year = {2021},
}
@article{Zhao2021,
   abstract = {Design flood estimation is a fundamental task in hydrology. In this research, we propose a machine-learning-based approach to estimate design floods globally. This approach involves three stages: (i) estimating at-site flood frequency curves for global gauging stations using the Anderson-Darling test and a Bayesian Markov chain Monte Carlo (MCMC) method; (ii) clustering these stations into subgroups using a K-means model based on 12 globally available catchment descriptors; and (iii) developing a regression model in each subgroup for regional design flood estimation using the same descriptors. A total of 11ĝ€¯793 stations globally were selected for model development, and three widely used regression models were compared for design flood estimation. The results showed that (1) the proposed approach achieved the highest accuracy for design flood estimation when using all 12 descriptors for clustering; and the performance of the regression was improved by considering more descriptors during training and validation; (2) a support vector machine regression provided the highest prediction performance amongst all regression models tested, with a root mean square normalised error of 0.708 for 100-year return period flood estimation; (3) 100-year design floods in tropical, arid, temperate, cold and polar climate zones could be reliably estimated (i.e. <±25ĝ€¯% error), with relative mean bias (RBIAS) values of -0.199, -0.233, -0.169, 0.179 and -0.091 respectively; (4) the machine-learning-based approach developed in this paper showed considerable improvement over the index-flood-based method introduced by Smith et al. (2015, 10.1002/2014WR015814) for design flood estimation at global scales; and (5) the average RBIAS in estimation is less than 18ĝ€¯% for 10-, 20-, 50- and 100-year design floods. We conclude that the proposed approach is a valid method to estimate design floods anywhere on the global river network, improving our prediction of the flood hazard, especially in ungauged areas.},
   author = {Gang Zhao and Paul Bates and Jeffrey Neal and Bo Pang},
   doi = {10.5194/hess-25-5981-2021},
   issn = {16077938},
   issue = {11},
   journal = {Hydrology and Earth System Sciences},
   pages = {5981-5999},
   title = {Design flood estimation for global river networks based on machine learning models},
   volume = {25},
   year = {2021},
}
@article{Sharkey2019,
   abstract = {Intense precipitation events are commonly known to be associated with an increased risk of flooding. As a result of the societal and infrastructural risks linked with flooding, extremes of precipitation require careful modeling. Extreme value analysis is typically used to model large precipitation events, although an independent analysis of neighboring sites can produce very different estimates of risk. In reality, one would expect neighboring locations to exhibit similar extremal behavior. A common method of inducing spatial similarity of extremal behavior is to define a spatial structure on the parameters of a generalized Pareto distribution in a Bayesian hierarchical modeling framework. These methods are often implemented under the assumption of conditional independence in time and space, with the consequence that standard errors of parameter estimates are too small. We present an approach for accounting for spatial and temporal dependence when quantifying the uncertainty in Bayesian hierarchical models when the misspecification of conditional independence has been made. We also present comparisons of performance between this class of models and alternative approaches, applied to precipitation data in Great Britain.},
   author = {Paul Sharkey and Hugo C. Winter},
   doi = {10.1002/ENV.2529},
   issn = {1099-095X},
   issue = {1},
   journal = {Environmetrics},
   keywords = {Bayesian inference,climate extremes,covariate modelling,extreme value analysis,spatial modeling},
   month = {2},
   pages = {e2529},
   publisher = {John Wiley & Sons, Ltd},
   title = {A Bayesian spatial hierarchical model for extreme precipitation in Great Britain},
   volume = {30},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/env.2529 https://onlinelibrary.wiley.com/doi/abs/10.1002/env.2529 https://onlinelibrary.wiley.com/doi/10.1002/env.2529},
   year = {2019},
}
@article{RezaNajafi2013,
   abstract = {A spatial hierarchical Bayesian method is developed to model the extreme runoffs over two spatial domains in Columbia River Basin, USA. This method combines the limited number of data from different locations. The two spatial domains contain 31 and 20 gage stations, respectively, with daily streamflow records ranging from 30 to over 130 years. The generalized Pareto distribution (GPD) is employed for the analysis of extremes. Temporally independent data are generated using declustering procedure, where runoff extremes are first grouped into clusters and then the maximum of each cluster is retained. The GPD scale parameter is modeled based on a Gaussian geostatistical process and additional variables including the latitude, longitude, elevation, and drainage area are incorporated by means of a hierarchy. Metropolis-Hasting within Gibbs Sampler is used to infer the parameters of the GPD and the geostatistical process to estimate the return levels across the basins. The performance of the hierarchical Bayesian model is evaluated by comparing the estimates of 100 year return level floods with the maximum likelihood estimates at sites that are not used during the parameter inference process. Various prior distributions are used to assess the sensitivity of the posterior distributions. The selected model is then employed to estimate floods with different return levels in time slices of 15 years in order to detect possible trends in runoff extremes. The results show cyclic variations in the spatial average of the 100 year return level floods across the basins with consistent increasing trends distinguishable in some areas. Key Points Hierarchical model significantly reduces flood estimation uncertainty. GPD Scale not sensitive to covariance latent parameter prior distribution. Increasing trend of extreme runoffs in recent fifteen year period. ©2013. American Geophysical Union. All Rights Reserved.},
   author = {Mohammad Reza Najafi and Hamid Moradkhani},
   doi = {10.1002/WRCR.20381},
   issn = {1944-7973},
   issue = {10},
   journal = {Water Resources Research},
   keywords = {climate change,extreme,flood,runoff,spatial hierarchical Bayesian,uncertainty},
   month = {10},
   pages = {6656-6670},
   publisher = {John Wiley & Sons, Ltd},
   title = {Analysis of runoff extremes using spatial hierarchical Bayesian modeling},
   volume = {49},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/wrcr.20381 https://onlinelibrary.wiley.com/doi/abs/10.1002/wrcr.20381 https://agupubs.onlinelibrary.wiley.com/doi/10.1002/wrcr.20381},
   year = {2013},
}
@article{Liu2014,
   abstract = {Analysing the extremes of multi-dimensional data is a difficult task for many reasons, e.g.the wide range of extremal dependence structures and the scarcity of the data. Some popular approaches that account for various extremal dependence types are based on asymptotically motivated models so that there is a probabilistic underpinning basis for extrapolating beyond observed levels. Among these efforts, Heffernan and Tawn developed a methodology for modelling the distribution of a d-dimensional variable when at least one of its components is extreme. Their approach is based on a series (i = 1, . , d) of conditional distributions, in which the distribution of the rest of the vector is modelled given that the ith component is large. This model captures a wide range of dependence structures and is applicable to cases of large d. However their model suffers from a lack of self-consistency between these conditional distributions and so does not uniquely determine probabilities when more than one component is large. This paper looks at these unsolved issues and makes proposals which aim to improve the efficiency of the Heffernan-Tawn model in practice. Tests based on simulated and financial data suggest that the proposed estimation method increases the self-consistency and reduces the RMSE of the estimated coefficient of tail dependence. © 2014 Elsevier Inc.},
   author = {Y. Liu and J. A. Tawn},
   doi = {10.1016/J.JMVA.2014.02.003},
   issn = {0047259X},
   journal = {Journal of Multivariate Analysis},
   keywords = {Coefficient of tail dependence,Conditional analysis,Multivariate extreme value theory,Quantile regression,Self-consistency,The Heffernan-Tawn model},
   month = {5},
   pages = {19-35},
   title = {Self-consistent estimation of conditional multivariate extreme value distributions},
   volume = {127},
   year = {2014},
}
@article{Varty2021,
   abstract = {Investment in measuring a process more completely or accurately is only useful if these improvements can be utilised during modelling and inference. We consider how improvements to data quality over time can be incorporated when selecting a modelling threshold and in the subsequent inference of an extreme value analysis. Motivated by earthquake catalogues, we consider variable data quality in the form of rounded and incompletely observed data. We develop an approach to select a time-varying modelling threshold that makes best use of the available data, accounting for uncertainty in the magnitude model and for the rounding of observations. We show the benefits of the proposed approach on simulated data and apply the method to a catalogue of earthquakes induced by gas extraction in the Netherlands. This more than doubles the usable catalogue size and greatly increases the precision of high magnitude quantile estimates. This has important consequences for the design and cost of earthquake defences. For the first time, we find compelling data-driven evidence against the applicability of the Gutenberg-Richer law to these earthquakes. Furthermore, our approach to automated threshold selection appears to have much potential for generic applications of extreme value methods.},
   author = {Zak Varty and Jonathan A. Tawn and Peter M. Atkinson and Stijn Bierman},
   journal = {arXiv},
   title = {Inference for extreme earthquake magnitudes accounting for a time-varying measurement process},
   volume = {2102.00884},
   url = {http://arxiv.org/abs/2102.00884},
   year = {2021},
}
@article{DArcy2022,
   abstract = {Reliable estimates of sea level return levels are crucial for coastal flooding risk assessments and for coastal flood defence design. We describe a novel method for estimating extreme sea levels that is the first to capture seasonality, interannual variations and longer term changes. We use a joint probabilities method, with skew surge and peak tide as two sea level components. The tidal regime is predictable but skew surges are stochastic. We present a statistical model for skew surges, where the main body of the distribution is modelled empirically whilst a non-stationary generalised Pareto distribution (GPD) is used for the upper tail. We capture within-year seasonality by introducing a daily covariate to the GPD model and allowing the distribution of peak tides to change over months and years. Skew surge-peak tide dependence is accounted for via a tidal covariate in the GPD model and we adjust for skew surge temporal dependence through the subasymptotic extremal index. We incorporate spatial prior information in our GPD model to reduce the uncertainty associated with the highest return level estimates. Our results are an improvement on current return level estimates, with previous methods typically underestimating. We illustrate our method at four UK tide gauges.},
   author = {Eleanor D'Arcy and Jonathan A. Tawn and Amélie Joly and Dafni E. Sifnioti},
   journal = {arXiv},
   keywords = {extreme sea levels,generalised pareto distribution,joint probabilities,method,non-stationarity,skew surge},
   title = {Accounting for Seasonality in Extreme Sea Level Estimation},
   url = {http://arxiv.org/abs/2207.09870},
   year = {2022},
}
@article{Tran2022,
   abstract = {Extreme value statistics is the max analogue of classical statistics, while tropical geometry is the max analogue of classical geometry. In this paper, we review recent work where insights from tropical geometry were used to develop new, efficient learning algorithms with leading performance on benchmark datasets in extreme value statistics. We give intuition, backed by performances on benchmark datasets, for why and when causal inference for extremes should be employed over classical methods. Finally, we list some open problems at the intersection of causal inference, tropical geometry and deep learning.},
   author = {Ngoc M Tran},
   journal = {arXiv},
   pages = {1-19},
   title = {The tropical geometry of causal inference for extremes},
   url = {http://arxiv.org/abs/2207.10227},
   year = {2022},
}
@article{Deuber2021,
   abstract = {Causal inference for extreme events has many potential applications in fields such as medicine, climate science and finance. We study the extremal quantile treatment effect of a binary treatment on a continuous, heavy-tailed outcome. Existing methods are limited to the case where the quantile of interest is within the range of the observations. For applications in risk assessment, however, the most relevant cases relate to extremal quantiles that go beyond the data range. We introduce an estimator of the extremal quantile treatment effect that relies on asymptotic tail approximations and uses a new causal Hill estimator for the extreme value indices of potential outcome distributions. We establish asymptotic normality of the estimators even in the setting of extremal quantiles, and we propose a consistent variance estimator to achieve valid statistical inference. In simulation studies we illustrate the advantages of our methodology over competitors, and we apply it to a real data set.},
   author = {David Deuber and Jinzhou Li and Sebastian Engelke and Marloes H. Maathuis},
   issue = {1},
   journal = {arXiv},
   title = {Estimation and Inference of Extremal Quantile Treatment Effects for Heavy-Tailed Distributions},
   volume = {1},
   url = {http://arxiv.org/abs/2110.06627},
   year = {2021},
}
@article{Contzen2022,
   abstract = {We present a new approach to modeling the future development of extreme temperatures globally and on a long time-scale by using non-stationary generalized extreme value distributions in combination with logistic functions. This approach is applied to data from the fully coupled climate model AWI-ESM. It enables us to investigate how extremes will change depending on the geographic location not only in terms of the magnitude, but also in terms of the timing of the changes. We observe that in general, changes in extremes are stronger and more rapid over land masses than over oceans. In addition, our models differentiate between changes in mean, in variability and in distributional shape, allowing for developments in these statistics to take place independently and at different times. Different models are presented and the Bayesian Information Criterion is used for model selection. It turns out that in most regions, changes in mean and variance take place simultaneously while the shape parameter of the distribution is predicted to stay constant. In the Arctic region, however, a different picture emerges: There, climate variability drastically and abruptly increases around 2050 due to the melting of ice, whereas changes in the mean values take longer and come into effect later.},
   author = {Justus Contzen and Thorsten Dickhaus and Gerrit Lohmann},
   journal = {arXiv},
   pages = {1-23},
   title = {Long-term temporal evolution of extreme temperature in a warming Earth},
   url = {http://arxiv.org/abs/2207.13512},
   year = {2022},
}
@article{Belzile2022,
   abstract = {This review paper surveys recent development in software implementations for extreme value analyses since the publication of Stephenson and Gilleland (2006) and Gilleland et al. (2013), here with a focus on numerical challenges. We provide a comparative review by topic and highlight differences in existing routines, along with listing areas where software development is lacking. The online supplement contains two vignettes providing a comparison of implementations of frequentist and Bayesian estimation of univariate extreme value models.},
   author = {Léo R. Belzile and Christophe Dutang and Paul J. Northrop and Thomas Opitz},
   journal = {arXiv},
   pages = {1-35},
   title = {A modeler's guide to extreme value software},
   url = {http://arxiv.org/abs/2205.07714},
   year = {2022},
}
@article{DArcy2022a,
   abstract = {Reliable estimates of sea level return levels are crucial for coastal flooding risk assessments and for coastal flood defence design. We describe a novel method for estimating extreme sea levels that is the first to capture seasonality, interannual variations and longer term changes. We use a joint probabilities method, with skew surge and peak tide as two sea level components. The tidal regime is predictable but skew surges are stochastic. We present a statistical model for skew surges, where the main body of the distribution is modelled empirically whilst a non-stationary generalised Pareto distribution (GPD) is used for the upper tail. We capture within-year seasonality by introducing a daily covariate to the GPD model and allowing the distribution of peak tides to change over months and years. Skew surge-peak tide dependence is accounted for via a tidal covariate in the GPD model and we adjust for skew surge temporal dependence through the subasymptotic extremal index. We incorporate spatial prior information in our GPD model to reduce the uncertainty associated with the highest return level estimates. Our results are an improvement on current return level estimates, with previous methods typically underestimating. We illustrate our method at four UK tide gauges.},
   author = {Eleanor D'Arcy and Jonathan A. Tawn and Amélie Joly and Dafni E. Sifnioti},
   journal = {arXiv},
   keywords = {extreme sea levels,generalised pareto distribution,non-stationarity,skew surge},
   pages = {1-21},
   title = {Accounting for Seasonality in Extreme Sea Level Estimation},
   url = {http://arxiv.org/abs/2207.09870},
   year = {2022},
}
@article{Bernard2013,
   abstract = {One of the main objectives of statistical climatology is to extract relevant information hidden in complex spatial-temporal climatological datasets. To identify spatial patterns, most well-known statistical techniques are based on the concept of intra- and intercluster variances (like the k-means algorithm or EOFs). As analyzing quantitative extremes like heavy rainfall has become more and more prevalent for climatologists and hydrologists during these last decades, finding spatial patterns with methods based on deviations from the mean (i.e., variances) may not be the most appropriate strategy in this context of studying such extremes. For practitioners, simple and fast clustering tools tailored for extremes have been lacking. A possible avenue to bridging this methodological gap resides in taking advantage of multivariate extreme value theory, a welldeveloped research field in probability, and to adapt it to the context of spatial clustering. In this paper, a novel algorithm based on this plan is proposed and studied. The approach is compared and discussed with respect to the classical k-means algorithm throughout the analysis of weekly maxima of hourly precipitation recorded in France (fall season, 92 stations, 1993-2011). © 2013 American Meteorological Society.},
   author = {Elsa Bernard and Philippe Naveau and Mathieu Vrac and Olivier Mestre},
   doi = {10.1175/JCLI-D-12-00836.1},
   issn = {08948755},
   issue = {20},
   journal = {Journal of Climate},
   pages = {7929-7937},
   title = {Clustering of maxima: Spatial dependencies among heavy rainfall in france},
   volume = {26},
   year = {2013},
}
@article{Hu2022,
   abstract = {Multivariate extreme value distributions are a common choice for modelling mul- tivariate extremes. In high dimensions, however, the construction of flexible and par- simonious models is challenging. We propose to combine bivariate extreme value dis- tributions into a Markov random field with respect to a tree. Although in general not an extreme value distribution itself, this Markov tree is attracted by a multivari- ate extreme value distribution. The latter serves as a tree-based approximation to an unknown extreme value distribution with the given bivariate distributions as margins. Given data, we learn an appropriate tree structure by Prim’s algorithm with estimated pairwise upper tail dependence coefficients or Kendall’s tau values as edge weights. The distributions of pairs of connected variables can be fitted in various ways. The resulting tree-structured extreme value distribution allows for inference on rare event probabili- ties, as illustrated on river discharge data from the upper Danube basin.},
   author = {Shuang Hu and Zuoxiang Peng and Johan Segers},
   doi = {10.1093/biomet/77.2.245},
   issn = {00063444},
   journal = {arXiv},
   keywords = {Extreme value theory,Generalized Pareto distribution,Multivariate exponential distribution,Nonregular estimation},
   title = {Modelling multivariate extreme value distributions via Markov trees},
   year = {2022},
}
@article{Majumder2022,
   abstract = {Quantifying changes in the probability and magnitude of extreme flooding events is key to mitigating their impacts. While hydrodynamic data are inherently spatially dependent, traditional spatial models such as Gaussian processes are poorly suited for modeling extreme events. Spatial extreme value models with more realistic tail dependence characteristics are under active development. They are theoretically justified, but give intractable likelihoods, making computation challenging for small datasets and prohibitive for continental-scale studies. We propose a process mixture model which specifies spatial dependence in extreme values as a convex combination of a Gaussian process and a max-stable process, yielding desirable tail dependence properties but intractable likelihoods. To address this, we employ a unique computational strategy where a feed-forward neural network is embedded in a density regression model to approximate the conditional distribution at one spatial location given a set of neighbors. We then use this univariate density function to approximate the joint likelihood for all locations by way of a Vecchia approximation. The process mixture model is used to analyze changes in annual maximum streamflow within the US over the last 50 years, and is able to detect areas which show increases in extreme streamflow over time.},
   author = {Reetam Majumder and Brian J. Reich and Benjamin A. Shaby},
   journal = {arXiv},
   keywords = {gaussian process,max-,neural networks,spatial extremes,stable process,vecchia approximation},
   title = {Modeling Extremal Streamflow using Deep Learning Approximations and a Flexible Spatial Process},
   url = {http://arxiv.org/abs/2208.03344},
   year = {2022},
}
@article{Richards2022,
   abstract = {Risk management in many environmental settings requires an understanding of the mechanisms that drive extreme events. Useful metrics for quantifying such risk are extreme quantiles of response variables conditioned on predictor variables that describe e.g., climate, biosphere and environmental states. Typically these quantiles lie outside the range of observable data and so, for estimation, require specification of parametric extreme value models within a regression framework. Classical approaches in this context utilise linear or additive relationships between predictor and response variables and suffer in either their predictive capabilities or computational efficiency; moreover, their simplicity is unlikely to capture the truly complex structures that lead to the creation of extreme wildfires. In this paper, we propose a new methodological framework for performing extreme quantile regression using artificial neutral networks, which are able to capture complex non-linear relationships and scale well to high-dimensional data. The "black box" nature of neural networks means that they lack the desirable trait of interpretability often favoured by practitioners; thus, we combine aspects of linear, and additive, models with deep learning to create partially interpretable neural networks that can be used for statistical inference but retain high prediction accuracy. To complement this methodology, we further propose a novel point process model for extreme values which overcomes the finite lower-endpoint problem associated with the generalised extreme value class of distributions. Efficacy of our unified framework is illustrated on U.S. wildfire data with a high-dimensional predictor set and we illustrate vast improvements in predictive performance over linear and spline-based regression techniques.},
   author = {Jordan Richards and Raphaël Huser},
   journal = {arXiv},
   keywords = {deep learning,explainable ai,extreme quantile regression,neural networks,spatio-temporal extremes},
   pages = {1-50},
   title = {A unifying partially-interpretable framework for neural network-based extreme quantile regression},
   url = {http://arxiv.org/abs/2208.07581},
   year = {2022},
}
@article{Pasche2022,
   abstract = {Risk assessment for extreme events requires accurate estimation of high quantiles that go beyond the range of historical observations. When the risk depends on the values of observed predictors, regression techniques are used to interpolate in the predictor space. We propose the EQRN model that combines tools from neural networks and extreme value theory into a method capable of extrapolation in the presence of complex predictor dependence. Neural networks can naturally incorporate additional structure in the data. We develop a recurrent version of EQRN that is able to capture complex sequential dependence in time series. We apply this method to forecasting of flood risk in the Swiss Aare catchment. It exploits information from multiple covariates in space and time to provide one-day-ahead predictions of return levels and exceedances probabilities. This output complements the static return level from a traditional extreme value analysis and the predictions are able to adapt to distributional shifts as experienced in a changing climate. Our model can help authorities to manage flooding more effectively and to minimize their disastrous impacts through early warning systems.},
   author = {Olivier C. Pasche and Sebastian Engelke},
   journal = {arXiv},
   keywords = {extreme value theory,generalized pareto distribution,machine learning,prediction,recurrent neural network},
   pages = {1-27},
   title = {Neural Networks for Extreme Quantile Regression with an Application to Forecasting of Flood Risk},
   url = {http://arxiv.org/abs/2208.07590},
   year = {2022},
}
@article{Dietz2022,
   abstract = {Bayesian hierarchical models are proposed for modeling tropical cyclone characteristics and their damage potential in the Atlantic basin. We model the joint probability distribution of tropical cyclone characteristics and their damage potential at two different temporal scales, while taking several climate indices into account. First, a predictive model for an entire season is developed that forecasts the number of cyclone events that will take place, the probability of each cyclone causing some amount of damage, and the monetized value of damages. Then, specific characteristics of individual cyclones are considered to predict the monetized value of the damage it will cause. Robustness studies are conducted and excellent prediction power is demonstrated across different data science models and evaluation techniques.},
   author = {Lindsey Dietz and Sakshi Arya and Snigdhansu Chatterjee},
   journal = {arXiv},
   pages = {34-36},
   title = {Predictions of damages from Atlantic tropical cyclones: a hierarchical Bayesian study on extremes},
   url = {http://arxiv.org/abs/2208.07899},
   year = {2022},
}
@article{Groll2017,
   abstract = {Long and consistent wave data are important for analysing wave climate variability and change. Moreover, such wave data are also needed in coastal and offshore design and for addressing safety-related issues at sea. Using the third-generation spectral wave model WAM a multi-decadal wind-wave hindcast for the North Sea covering the period 1949-2014 was produced. The hindcast is part of the coastDat database representing a consistent and homogeneous met-ocean data set. It is shown that despite not being perfect, data from the wave hindcast are generally suitable for wave climate analysis. In particular, comparisons of hindcast data with in situ and satellite observations show on average a reasonable agreement, while a tendency towards overestimation of the highest waves could be inferred. Despite these limitations, the wave hindcast still provides useful data for assessing wave climate variability and change as well as for risk analysis, in particular when conservative estimates are needed. Hindcast data are stored at the World Data Center for Climate (WDCC) and can be freely accessed using the <a hrefCombining double low line"https://doi.org/10.1594/WDCC/coastDat-2-WAM-North-Sea" targetCombining double low line"-blank">doi:10.1594/WDCC/coastDat-2-WAM-North-Sea</a> Groll and Weisse(2016) or via the coastDat web-page <a hrefCombining double low line"http://www.coastdat.de" targetCombining double low line"-blank">http://www.coastdat.de</a>.},
   author = {Nikolaus Groll and Ralf Weisse},
   doi = {10.5194/ESSD-9-955-2017},
   issn = {18663516},
   issue = {2},
   journal = {Earth System Science Data},
   month = {12},
   pages = {955-968},
   publisher = {Copernicus GmbH},
   title = {A multi-decadal wind-wave hindcast for the North Sea 1949-2014: CoastDat2},
   volume = {9},
   year = {2017},
}
@article{Nguyen2022,
   abstract = {Extreme events such as natural and economic disasters leave lasting impacts on society and motivate the analysis of extremes from data. While classical statistical tools based on Gaussian distributions focus on average behaviour and can lead to persistent biases when estimating extremes, extreme value theory (EVT) provides the mathematical foundations to accurately characterise extremes. In this paper, we adapt a dynamic extreme value model recently introduced to forecast financial risk from high frequency data to the context of natural hazard forecasting. We demonstrate its wide applicability and flexibility using a case study of the Piton de la Fournaise volcano. The value of using EVT-informed thresholds to identify and model extreme events is shown through forecast performance.},
   author = {Michele Nguyen and Almut E. D. Veraart and Benoit Taisne and Tan Chiou Ting and David Lallemant},
   journal = {arXiv},
   title = {A dynamic extreme value model with applications to volcanic eruption forecasting},
   url = {http://arxiv.org/abs/2208.10724},
   year = {2022},
}
@article{Berghuijs2019,
   abstract = {River flooding is a common hazard, causing billions of dollars in annual losses. Flood impacts are shaped by the spatial scale over which different rivers flood simultaneously, but this dimension of flood risk remains largely unknown. Using annual flood data from several thousand European rivers, we demonstrate that the flood synchrony scale—the distance over which multiple rivers flood near synchronously—far exceeds the size of individual drainage basins and varies regionally by more than an order of magnitude. These data also show that flood synchrony scales have grown by about 50% over the period 1960–2010. Detrended flood synchrony values are serially correlated, implying that years with spatially extensive floods tend to follow one another. These findings reveal that flood risks are correlated well beyond the individual drainage basins for which flood hazards are typically assessed and managed.},
   author = {Wouter R. Berghuijs and Scott T. Allen and Shaun Harrigan and James W. Kirchner},
   doi = {10.1029/2018GL081883},
   issn = {19448007},
   issue = {3},
   journal = {Geophysical Research Letters},
   keywords = {catchment,flood,hazard},
   pages = {1423-1428},
   title = {Growing Spatial Scales of Synchronous River Flooding in Europe},
   volume = {46},
   year = {2019},
}
@article{Krock2022,
   abstract = {Current models for spatial extremes are concerned with the joint upper (or lower) tail of the distribution at two or more locations. Such models cannot account for teleconnection patterns of two-meter surface air temperature ($T_\{2m\}$) in North America, where very low temperatures in the contiguous Unites States (CONUS) may coincide with very high temperatures in Alaska in the wintertime. This dependence between warm and cold extremes motivates the need for a model with opposite-tail dependence in spatial extremes. This work develops a statistical modeling framework which has flexible behavior in all four pairings of high and low extremes at pairs of locations. In particular, we use a mixture of rotations of common Archimedean copulas to capture various combinations of four-corner tail dependence. We study teleconnected $T_\{2m\}$ extremes using ERA5 reanalysis of daily average two-meter temperature during the boreal winter. The estimated mixture model quantifies the strength of opposite-tail dependence between warm temperatures in Alaska and cold temperatures in the midlatitudes of North America, as well as the reverse pattern. These dependence patterns are shown to correspond to blocked and zonal patterns of mid-tropospheric flow. This analysis extends the classical notion of correlation-based teleconnections to considering dependence in higher quantiles.},
   author = {Mitchell L. Krock and Adam H. Monahan and Michael L. Stein},
   keywords = {blocking,copula,extremes,reanalysis,spatial data,tail dependence,teleconnec-,tion},
   title = {Teleconnected warm and cold extremes of North American wintertime temperatures},
   url = {http://arxiv.org/abs/2208.12092},
   year = {2022},
}
@article{Agovino2021,
   abstract = {Wildfires constitute a serious threat for both the environment and human well-being. The US fire policy aims to tackle this problem, devoting a sizeable amount of resources and resorting extensively to fire suppression strategies. The theoretical literature has established a link between climate conditions and wildfire incidence. Using state-level data from 2002 to 2013 for the USA, this work proposes a wildfire incidence indicator and runs a generalized spatial ordered probit model in order to test the findings of the previous literature empirically. Moreover, this article investigates the extent of spatial spillovers in the climatic covariates. The results highlight a significant impact of precipitation and temperature on fire incidence and provide some evidence of the role of spatial spillovers. In particular, transitions from lower to higher wildfire incidence levels are significantly encouraged by increases in local temperature and significantly discouraged by increases in both local precipitation and lagged precipitation. The present analysis complements the recent literature, confirming the previous findings with a solid empirical investigation and offering a policy-oriented picture of wildfire risks all over the USA.},
   author = {Massimiliano Agovino and Massimiliano Cerciello and Aniello Ferraro and Antonio Garofalo},
   doi = {10.1007/s10668-020-00863-2},
   issn = {15732975},
   issue = {4},
   journal = {Environment, Development and Sustainability},
   keywords = {Fire policy,Generalized spatial ordered probit,Spatial spillovers,Wildfires},
   month = {4},
   pages = {6084-6105},
   publisher = {Springer Science and Business Media B.V.},
   title = {Spatial analysis of wildfire incidence in the USA: the role of climatic spillovers},
   volume = {23},
   year = {2021},
}
@article{Wadsworth2022,
   abstract = {A geometric representation for multivariate extremes, based on the shapes of scaled sample clouds in light-tailed margins and their so-called limit sets, has recently been shown to connect several existing extremal dependence concepts. However, these results are purely probabilistic, and the geometric approach itself has not been fully exploited for statistical inference. We outline a method for parametric estimation of the limit set shape, which includes a useful non/semi-parametric estimate as a pre-processing step. More fundamentally, our approach provides a new class of asymptotically-motivated statistical models for the tails of multivariate distributions, and such models can accommodate any combination of simultaneous or non-simultaneous extremes through appropriate parametric forms for the limit set shape. Extrapolation further into the tail of the distribution is possible via simulation from the fitted model. A simulation study confirms that our methodology is very competitive with existing approaches, and can successfully allow estimation of small probabilities in regions where other methods struggle. We apply the methodology to two environmental datasets, with diagnostics demonstrating a good fit.},
   author = {Jennifer Wadsworth and Ryan Campbell},
   journal = {arXiv},
   month = {8},
   title = {Statistical inference for multivariate extremes via a geometric approach},
   volume = {2208.14951},
   url = {http://arxiv.org/abs/2208.14951},
   year = {2022},
}
@article{Bodik2021,
   abstract = {Consider two stationary time series with heavy-tailed marginal distributions. We aim to detect whether they have a causal relation, that is, if a change in one of them causes a change in the other. Usual methods for causality detection are not well suited if the causal mechanisms only manifest themselves in extremes. This paper aims to detect the causal relations in extremes between time series. We define the so-called causal tail coefficient for time series, which, under some assumptions, correctly detects the asymmetrical causal relations between extremes of the time series. The advantage is that this method works even if nonlinear relations and common ancestors are present. Moreover, we mention how our method can help detect a time delay between the two time series. We describe some of its asymptotic properties and show how it performs on some simulations. Finally, we show how this method works on space-weather and hydro-meteorological data sets.},
   author = {Juraj Bodik and Milan Paluš and Zbyněk Pawlas},
   journal = {arXiv},
   month = {12},
   title = {Causality in extremes of time series},
   url = {http://arxiv.org/abs/2112.10858},
   year = {2021},
}
@article{,
   abstract = {In situations where both extreme and non-extreme data are of interest, modelling the whole data set accurately is important. In a univariate framework, modelling the bulk and tail of a distribution has been studied before, but little work has been done when more than one variable is of concern. We propose a dependence model that blends two copulas with different characteristics over the whole range of the data support. One copula is tailored to the bulk and the other to the tail, with a dynamic weighting function employed to transition smoothly between them. We investigate tail dependence properties numerically and use simulation to confirm that the blended model is flexible enough to capture a wide variety of structures. We apply our model to study the dependence between temperature and ozone concentration at two sites in the UK and compare with a single copula fit. The proposed model provides a better, more flexible, fit to the data, and is also capable of capturing complex dependence structures.},
   author = {Lídia M. André and Jennifer L. Wadsworth and Adrian O'Hagan},
   month = {9},
   title = {Joint modelling of the body and tail of bivariate data},
   url = {http://arxiv.org/abs/2209.05795},
   year = {2022},
}
@article{Berghuijs2014,
   abstract = {Recent hydrologic synthesis efforts have presented evidence that the seasonal water balance is at the core of overall catchment responses, and understanding it will assist in predicting signatures of streamflow variability at other time scales, including interannual variability, the flow duration curve, low flows, and floods. In this study, we group 321 catchments located across the continental U.S. into several clusters with similar seasonal water balance behavior. We then delineate the boundaries between these clusters on the basis of a similarity framework based on three hydroclimatic indices that represent aridity, precipitation timing, and snowiness. The clustering of catchments based on the seasonal water balance has a strong relationship not only with regional patterns of the three climate indices but also with regional ecosystem, soil, and vegetation classes, which point to the strong dependence of these physiographic characteristics on seasonal climate variations and the hydrologic regimes. Building on these catchment clusters, we demonstrate that the seasonal water balance does have an imprint on signatures of streamflow variability over a wide range of time scales (daily to decadal) and a wide range of states (low flows to floods). The seasonal water balance is well integrated into variability at seasonal and longer time scales, but is only partly reflected in the signatures at shorter time scales, including flooding responses. Overall, the seasonal water balance has proven to be a similarity measure that serves as a link between both short-term hydrologic responses and long-term adaptation of the landscape with climate. Key Points Framework for the seasonal water balance is developed based on climatic controls The 321 catchments are grouped into 10 clusters with similar seasonal water balances The seasonal water balance has an imprint both on streamflow and landscape © 2014. American Geophysical Union. All Rights Reserved.},
   author = {Wouter R. Berghuijs and Murugesu Sivapalan and Ross A. Woods and Hubert H.G. Savenije},
   doi = {10.1002/2014WR015692},
   issn = {19447973},
   issue = {7},
   journal = {Water Resources Research},
   keywords = {classification,hydrologic similarity,seasonal water balance,streamflow variability,vegetation adaptation},
   pages = {5638-5661},
   publisher = {Blackwell Publishing Ltd},
   title = {Patterns of similarity of seasonal water balances: A window into streamflow variability over a range of time scales},
   volume = {50},
   year = {2014},
}
@article{Knoben2018,
   abstract = {Classification is essential in the study of natural systems, yet hydrology has no formal way to structure the climatic forcing that underlies hydrologic response. Various climate classification systems can be borrowed from other disciplines but these are based on different organizing principles than a hydrological classification might need. This work presents a hydrologically informed way to quantify global climates, explicitly addressing the shortcomings in earlier climate classifications. In this work, causal factors (climate) and hydrologic response (streamflow) are separated, meaning that our classification scheme is based only on climatic information and can be evaluated with independent streamflow data. Using gridded global climate data, we calculate three dimensionless indices per grid cell, describing annual aridity, aridity seasonality, and precipitation-as-snow. We use these indices to create several climate groups and define the membership degree of 1,103 catchments to each of the climate groups, based on each catchment's climate. Streamflow patterns within each group tend to be similar, and tend to be different between groups. Visual comparison of flow regimes and Wilcoxon two-sample statistical tests on 16 streamflow signatures show that this index-based approach is more effective than the often-used Köppen-Geiger classification for grouping hydrologically similar catchments. Climate forcing exerts a strong control on typical hydrologic response and we show that at the global scale both change gradually in space. We argue that hydrologists should consider the hydroclimate as a continuous spectrum defined by the three climate indices, on which all catchments are positioned and show examples of this in a regionalization context.},
   author = {Wouter J.M. Knoben and Ross A. Woods and Jim E. Freer},
   doi = {10.1029/2018WR022913},
   issn = {19447973},
   issue = {7},
   journal = {Water Resources Research},
   keywords = {climate classification,hydroclimatic indices,hydrologic regimes},
   month = {7},
   pages = {5088-5109},
   publisher = {Blackwell Publishing Ltd},
   title = {A Quantitative Hydrological Climate Classification Evaluated With Independent Streamflow Data},
   volume = {54},
   year = {2018},
}
@article{Diawara2022,
   abstract = {According to the Chinese Health Statistics Yearbook, in 2005, the number of traffic accidents was 187781 with total direct property losses of 103691.7 (10000 Yuan). This research aims to fill the gap in the literature by investigating the extreme claim sizes not only for the entire portfolio. This empirical study investigates the behavior of the upper tail of the claim size by class of policyholders.},
   author = {Daouda Diawara and Ladji Kane and Soumaila Dembele and Gane Samb Lo},
   month = {9},
   title = {Applying of the Extreme Value Theory for determining extreme claims in the automobile insurance sector: Case of a China car insurance},
   url = {http://arxiv.org/abs/2209.10194},
   year = {2022},
}
@article{Rohrbeck2022,
   abstract = {A key aspect where extreme values methods differ from standard statistical models is through having asymptotic theory to provide a theoretical justification for the nature of the models used for extrapolation. In multivariate extremes many different asymptotic theories have been proposed, partly as a consequence of the lack of ordering property with vector random variables. One class of multivariate models, based on conditional limit theory as one variable becomes extreme, developed by Heffernan and Tawn (2004), has developed wide practical usage. The underpinning value of this approach has been supported by further theoretical characterisations of the limiting relationships by Heffernan and Resnick (2007) and Resnick and Zeber (2014). However Drees and Jan\{\ss\}en (2017) provided a number of counterexamples of their results, which potentially undermine the trust in these statistical methods. Here we show that in the Heffernan and Tawn (2004) framework, which involves marginal standardisation to a common exponentially decaying tailed marginal distribution, the problems in these examples are removed.},
   author = {Christian Rohrbeck and Jonathan A Tawn},
   month = {9},
   title = {Standardisation overcomes counter-examples of conditional extremes},
   url = {http://arxiv.org/abs/2209.10936},
   year = {2022},
}
@article{Segers2022,
   abstract = {Graphical models with heavy-tailed factors can be used to model extremal dependence or causality between extreme events. In a Bayesian network, variables are recursively defined in terms of their parents according to a directed acyclic graph (DAG). We focus on max-linear graphical models with respect to a special type of graphs, which we call a \emph\{tree of transitive tournaments\}. The latter are block graphs combining in a tree-like structure a finite number of transitive tournaments, each of which is a DAG in which every two nodes are connected. We study the limit of the joint tails of the max-linear model conditionally on the event that a given variable exceeds a high threshold. Under a suitable condition, the limiting distribution involves the factorization into independent increments along the shortest trail between two variables, thereby imitating the behavior of a Markov random field. We are also interested in the identifiability of the model parameters in case some variables are latent and only a subvector is observed. It turns out that the parameters are identifiable under a criterion on the nodes carrying the latent variables which is easy and quick to check.},
   author = {Johan Segers and Stefka Asenova},
   month = {9},
   title = {Max-linear graphical models with heavy-tailed factors on trees of transitive tournaments},
   url = {http://arxiv.org/abs/2209.14938},
   year = {2022},
}
@article{Lee2022,
   abstract = {We develop a method for investigating conditional extremal relationships between variables at their extreme levels. We consider an inner product space constructed from transformed-linear combinations of independent regularly varying random variables. By developing the projection theorem for the inner product space, we derive the concept of partial tail correlation via projection theorem. We show that the partial tail correlation can be understood as the inner product of the prediction errors associated with the best transformed-linear prediction. Similar to Gaussian cases, we connect partial tail correlation to the inverse of the inner product matrix and show that a zero in this inverse implies a partial tail correlation of zero. We develop a hypothesis test for the partial tail correlation of zero and demonstrate the performance in a simulation study as well as in two applications: high nitrogen dioxide levels in Washington DC and extreme river discharges in the upper Danube basin.},
   author = {Jeongjin Lee and Daniel Cooley},
   month = {10},
   title = {Partial Tail Correlation for Extremes},
   url = {http://arxiv.org/abs/2210.02048},
   year = {2022},
}
@article{Varin2011,
   author = {Cristiano Varin and Nancy Reid and David Firth},
   journal = {Statistica Sinica},
   pages = {5-42},
   title = {An overview of composite likelihood methods},
   year = {2011},
}
@article{Vettori2018,
   abstract = {Various nonparametric and parametric estimators of extremal dependence have been proposed in the literature. Nonparametric methods commonly suffer from the curse of dimensionality and have been mostly implemented in extreme-value studies up to three dimensions, whereas parametric models can tackle higher-dimensional settings. In this paper, we assess, through a vast and systematic simulation study, the performance of classical and recently proposed estimators in multivariate settings. In particular, we first investigate the performance of nonparametric methods and then compare them with classical parametric approaches under symmetric and asymmetric dependence structures within the commonly used logistic family. We also explore two different ways to make nonparametric estimators satisfy the necessary dependence function shape constraints, finding a general improvement in estimator performance either (i) by substituting the estimator with its greatest convex minorant, developing a computational tool to implement this method for dimensions D≥ 2 or (ii) by projecting the estimator onto a subspace of dependence functions satisfying such constraints and taking advantage of Bernstein–Bézier polynomials. Implementing the convex minorant method leads to better estimator performance as the dimensionality increases.},
   author = {Sabrina Vettori and Raphaël Huser and Marc G. Genton},
   doi = {10.1007/s11222-017-9745-7},
   issn = {15731375},
   issue = {3},
   journal = {Statistics and Computing},
   keywords = {Asymmetric logistic model,Componentwise maxima,Convexity,Copula,Greatest convex minorant,Nonparametric and parametric estimators,Pickands dependence function},
   month = {5},
   pages = {525-538},
   publisher = {Springer New York LLC},
   title = {A comparison of dependence function estimators in multivariate extremes},
   volume = {28},
   year = {2018},
}
@article{Eastoe2014,
   abstract = {The traditional approach to multivariate extreme values has been through the multivariate extreme value distribution G, characterised by its spectral measure H and associated Pickands' dependence function A. More generally, for all asymptotically dependent variables, H determines the probability of all multivariate extreme events. When the variables are asymptotically dependent and under the assumption of unit Fréchet margins, several methods exist for the estimation of G, H and A which use variables with radial component exceeding some high threshold. For each of these characteristics, we propose new asymptotically consistent nonparametric estimators which arise from Heffernan and Tawn's approach to multivariate extremes that conditions on variables with marginal values exceeding some high marginal threshold. The proposed estimators improve on existing estimators in three ways. First, under asymptotic dependence, they give self-consistent estimators of G, H and A; existing estimators are not self-consistent. Second, these existing estimators focus on the bivariate case, whereas our estimators extend easily to describe dependence in the multivariate case. Finally, for asymptotically independent cases, our estimators can model the level of asymptotic independence; whereas existing estimators for the spectral measure treat the variables as either being independent, or asymptotically dependent. For asymptotically dependent bivariate random variables, the new estimators are found to compare favourably with existing estimators, particularly for weak dependence. The method is illustrated with an application to finance data. © 2013 Springer Science+Business Media New York.},
   author = {Emma F. Eastoe and Janet E. Heffernan and Jonathan A. Tawn},
   doi = {10.1007/s10687-013-0173-6},
   issn = {1572915X},
   issue = {1},
   journal = {Extremes},
   keywords = {Asymptotic dependence,Conditional distribution,Economic monetary union,Multivariate extreme value theory,Nonparametric modelling,Poisson process},
   pages = {25-43},
   publisher = {Kluwer Academic Publishers},
   title = {Nonparametric estimation of the spectral measure, and associated dependence measures, for multivariate extreme values using a limiting conditional representation},
   volume = {17},
   year = {2014},
}
@article{Moins2022,
   abstract = {Combining extreme value theory with Bayesian methods offers several advantages, such as a quantification of uncertainty on parameter estimation or the ability to study irregular models that cannot be handled by frequentist statistics. However, it comes with many options that are left to the user concerning model building, computational algorithms, and even inference itself. Among them, the parameterization of the model induces a geometry that can alter the efficiency of computational algorithms, in addition to making calculations involved. We focus on the Poisson process characterization of extremes and outline two key benefits of an orthogonal parameterization addressing both issues. First, several diagnostics show that Markov chain Monte Carlo convergence is improved compared with the original parameterization. Second, orthogonalization also helps deriving Jeffreys and penalized complexity priors, and establishing posterior propriety. The analysis is supported by simulations, and our framework is then applied to extreme level estimation on river flow data.},
   author = {Théo Moins and Julyan Arbel and Stéphane Girard and Anne Dutfoy},
   month = {10},
   title = {Reparameterization of extreme value framework for improved Bayesian workflow},
   url = {http://arxiv.org/abs/2210.05224},
   year = {2022},
}
@article{Nolde2014,
   abstract = {The residual dependence coefficient was originally introduced by Ledford and Tawn (1996) [25] as a measure of residual dependence between extreme values in the presence of asymptotic independence. We present a geometric interpretation of this coefficient with the additional assumptions that the random samples from a given distribution can be scaled to converge onto a limit set and that the marginal distributions have Weibull-type tails. This result leads to simple and intuitive computations of the residual dependence coefficient for a variety of distributions. © 2013 Elsevier Inc.},
   author = {Natalia Nolde},
   doi = {10.1016/j.jmva.2013.08.018},
   issn = {0047259X},
   journal = {Journal of Multivariate Analysis},
   keywords = {Asymptotic independence,Geometric approach,Limit set,Multivariate density,Residual dependence coefficient,Sample clouds},
   month = {1},
   pages = {85-95},
   title = {Geometric interpretation of the residual dependence coefficient},
   volume = {123},
   year = {2014},
}
@article{Ross2020,
   abstract = {Environmental contours are used in structural reliability analysis of marine and coastal structures as an approximate means to locate the boundary of the distribution of environmental variables, and hence sets of environmental conditions giving rise to extreme structural loads and responses. Outline guidance concerning the application of environmental contour methods is given in recent design guidelines from many organisations. However there is lack of clarity concerning (a) the differences between approaches to environmental contour estimation reported in the literature, and (b) the relationship between the environmental contour, corresponding to some return period, and the extreme structural response for the same period. Hence there is uncertainty about precisely when environmental contours should be used, and how they should be used well. This article seeks to provide some assistance in understanding the fundamental issues regarding environmental contours and their use in structural reliability analysis. Approaches to estimating the joint distribution of environmental variables, and to estimating environmental contours based on that distribution, are described. Simple freely-available software for estimation of the joint distribution, and hence environmental contours, is illustrated. Extra assumptions required to relate the characteristics of environmental contours to structural failure are outlined. Alternative response-based methods not requiring environmental contours are summarised. The results of an informal survey of the metocean user community regarding environmental contours are presented. Finally, recommendations about when and how environmental contour methods should be used are made.},
   author = {Emma Ross and Ole Christian Astrup and Elzbieta Bitner-Gregersen and Nigel Bunn and Graham Feld and Ben Gouldby and Arne Huseby and Ye Liu and David Randell and Erik Vanem and Philip Jonathan},
   doi = {10.1016/J.OCEANENG.2019.106194},
   issn = {0029-8018},
   journal = {Ocean Engineering},
   keywords = {Environmental contour,Extreme,IFORM,Joint probability,Return value,Structural reliability,Structural response},
   month = {1},
   pages = {106194},
   publisher = {Pergamon},
   title = {On environmental contours for marine and coastal design},
   volume = {195},
   year = {2020},
}
@report{,
   author = {Aubrey C Eckert-Gallup and Cédric J Sallaberry and Ann R Dallman and Vincent S Neary},
   title = {SANDIA REPORT Modified Inverse First Order Reliability Method (I-FORM) for Predicting Extreme Sea States},
   url = {http://www.ntis.gov/help/ordermethods.asp?loc=7-4-0#online},
}
@article{Hall2000,
   abstract = {Two new methods are suggested for estimating the dependence function of a bivariate extreme-value distribution. One is based on a multiplicative modification of an earlier technique proposed by Pickands, and the other employs spline smoothing under constraints. Both produce estimators that satisfy all the conditions that define a dependence function, including convexity and the restriction that its curve lie within a certain triangular region. The first approach does not require selection of smoothing parameters; the second does, and for that purpose we suggest explicit tuning methods, one of them based on cross-validation. © 2000 ISI/BS.},
   author = {Peter Hall and Nader Tajvidi},
   doi = {10.2307/3318758},
   issn = {13507265},
   issue = {5},
   journal = {Bernoulli},
   keywords = {Convex hull,Cross-validation,Marginal distribution,Multivariate extreme-value distribution,Nonparametric curve estimation,Smoothing parameter,Spline},
   pages = {835-844},
   publisher = {International Statistical Institute},
   title = {Distribution and dependence-function estimation for bivariate extreme-value distributions},
   volume = {6},
   year = {2000},
}
@article{Cormier2014,
   abstract = {A visual tool is proposed for detecting the presence of extreme-value dependence or extremal tail behavior in bivariate data. The points appearing on the plot stem from rank-based transformations of the observations and can serve to estimate the unknown Pickands dependence function of the underlying extreme-value copula or its attractor. Quadratic constrained B-spline smoothing is used to derive an intrinsic estimator, which naturally leads to a test of extremeness. Both the estimator and the test are seen to perform well in simulations. The proposed methodology is illustrated with real data and the treatment of ties is briefly discussed.},
   author = {Eric Cormier and Christian Genest and Johanna G. Nešlehová},
   doi = {10.1007/S10687-014-0199-4},
   issn = {1572-915X},
   issue = {4},
   journal = {Extremes },
   keywords = {Civil Engineering,Economics,Environmental Management,Finance,Hydrogeology,Insurance,Management,Quality Control,Reliability,Safety and Risk,Statistics,Statistics for Business,general},
   month = {8},
   pages = {633-659},
   publisher = {Springer},
   title = {Using B-splines for nonparametric inference on bivariate extreme-value copulas},
   volume = {17},
   url = {https://link.springer.com/article/10.1007/s10687-014-0199-4},
   year = {2014},
}
@article{Kiriliouk2022,
   abstract = {An important problem in extreme-value theory is the estimation of the probability that a high-dimensional random vector falls into a given extreme failure set. This paper provides a parametric approach to this problem, based on a generalization of the tail pairwise dependence matrix (TPDM). The TPDM gives a partial summary of tail dependence for all pairs of components of the random vector. We propose an algorithm to obtain an approximate completely positive decomposition of the TPDM. The decomposition is easy to compute and applicable to moderate to high dimensions. Based on the decomposition, we obtain parameters estimates of a max-linear model whose TPDM is equal to that of the original random vector. We apply the proposed decomposition algorithm to industry portfolio returns and maximal wind speeds to illustrate its applicability.},
   author = {Anna Kiriliouk and Chen Zhou},
   month = {10},
   title = {Estimating probabilities of multivariate failure sets based on pairwise tail dependence coefficients},
   url = {http://arxiv.org/abs/2210.12618},
   year = {2022},
}
@article{Fasiolo2021,
   abstract = {We propose a novel framework for fitting additive quantile regression models, which provides well-calibrated inference about the conditional quantiles and fast automatic estimation of the smoothing parameters, for model structures as diverse as those usable with distributional generalized additive models, while maintaining equivalent numerical efficiency and stability. The proposed methods are at once statistically rigorous and computationally efficient, because they are based on the general belief updating framework of Bissiri, Holmes, and Walker to loss based inference, but compute by adapting the stable fitting methods of Wood, Pya, and Säfken. We show how the pinball loss is statistically suboptimal relative to a novel smooth generalization, which also gives access to fast estimation methods. Further, we provide a novel calibration method for efficiently selecting the “learning rate” balancing the loss with the smoothing priors during inference, thereby obtaining reliable quantile uncertainty estimates. Our work was motivated by a probabilistic electricity load forecasting application, used here to demonstrate the proposed approach. The methods described here are implemented by the qgam R package, available on the Comprehensive R Archive Network (CRAN). Supplementary materials for this article are available online.},
   author = {Matteo Fasiolo and Simon N. Wood and Margaux Zaffran and Raphaël Nedellec and Yannig Goude},
   doi = {10.1080/01621459.2020.1725521/SUPPL_FILE/UASA_A_1725521_SM0399.ZIP},
   issn = {1537274X},
   issue = {535},
   journal = {Journal of the American Statistical Association},
   keywords = {Calibrated Bayes,Electricity load forecasting,Generalized additive models,Penalized regression splines,Quantile regression},
   pages = {1402-1412},
   publisher = {American Statistical Association},
   title = {Fast Calibrated Additive Quantile Regression},
   volume = {116},
   url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.2020.1725521},
   year = {2021},
}
@article{Hentschel2022,
   abstract = {The severity of multivariate extreme events is driven by the dependence between the largest marginal observations. The H\"usler-Reiss distribution is a versatile model for this extremal dependence, and it is usually parameterized by a variogram matrix. In order to represent conditional independence relations and obtain sparse parameterizations, we introduce the novel H\"usler-Reiss precision matrix. Similarly to the Gaussian case, this matrix appears naturally in density representations of the H\"usler-Reiss Pareto distribution and encodes the extremal graphical structure through its zero pattern. For a given, arbitrary graph we prove the existence and uniqueness of the completion of a partially specified H\"usler-Reiss variogram matrix so that its precision matrix has zeros on non-edges in the graph. Using suitable estimators for the parameters on the edges, our theory provides the first consistent estimator of graph structured H\"usler-Reiss distributions. If the graph is unknown, our method can be combined with recent structure learning algorithms to jointly infer the graph and the corresponding parameter matrix. Based on our methodology, we propose new tools for statistical inference of sparse H\"usler-Reiss models and illustrate them on large flight delay data in the U.S.},
   author = {Manuel Hentschel and Sebastian Engelke and Johan Segers},
   month = {10},
   title = {Statistical Inference for H\"usler-Reiss Graphical Models Through Matrix Completions},
   url = {http://arxiv.org/abs/2210.14292},
   year = {2022},
}
@article{Ahmad2022,
   abstract = {The statistical modelling of integer-valued extremes such as large avalanche counts has received less attention than their continuous counterparts in the extreme value theory (EVT) literature. One approach to moving from continuous to discrete extremes is to model threshold exceedances of integer random variables by the discrete version of the generalized Pareto distribution. Still, the optimal threshold selection that defines exceedances remains a problematic issue. Moreover, within a regression framework, the treatment of the many data points (those below the chosen threshold) is either ignored or decoupled from extremes. Considering these issues, we extend the idea of using a smooth transition between the two tails (lower and upper) to force large and small discrete extreme values to comply with EVT. In the case of zero inflation, we also develop models with an additional parameter. To incorporate covariates, we extend the Generalized Additive Models (GAM) framework to discrete extreme responses. In the GAM forms, the parameters of our proposed models are quantified as a function of covariates. The maximum likelihood estimation procedure is implemented for estimation purposes. With the advantage of bypassing the threshold selection step, our findings indicate that the proposed models are more flexible and robust than competing models (i.e. discrete generalized Pareto distribution and Poisson distribution).},
   author = {Touqeer Ahmad and Carlo Gaetan and Philippe Naveau},
   month = {10},
   title = {Modelling of discrete extremes through extended versions of discrete generalized Pareto distribution},
   url = {http://arxiv.org/abs/2210.15253},
   year = {2022},
}
@article{Johnston2022,
   abstract = {In this study, we examine a Bayesian approach to analyze extreme daily rainfall amounts and forecast return-levels. Estimating the probability of occurrence and quantiles of future extreme events is important in many applications, including civil engineering and the design of public infrastructure. In contrast to traditional analysis, which use point estimates to accomplish this goal, the Bayesian method utilizes the complete posterior density derived from the observations. The Bayesian approach offers the benefit of well defined credible (confidence) intervals, improved forecasting, and the ability to defend rigorous probabilistic assessments. We illustrate the Bayesian approach using extreme precipitation data from Long Island, NY, USA and show that current return levels, or precipitation risk, may be understated.},
   author = {Douglas E. Johnston},
   journal = {arXiv},
   keywords = {extreme value theory,quantile forecasting},
   pages = {1-8},
   title = {Bayesian Analysis of Extreme Precipitation Events and Forecasting Return Levels},
   url = {http://arxiv.org/abs/2208.12316},
   year = {2022},
}
@article{Mackay2022,
   author = {Ed Mackay and Guillaume de Hauteclocque},
   journal = {arXiv},
   title = {Model-free environmental contours in higher dimensions},
   year = {2022},
}
@article{,
   abstract = {We propose kernel PCA as a method for analyzing the dependence structure of multivariate extremes and demonstrate that it can be a powerful tool for clustering and dimension reduction. Our work provides some theoretical insight into the preimages obtained by kernel PCA, demonstrating that under certain conditions they can effectively identify clusters in the data. We build on these new insights to characterize rigorously the performance of kernel PCA based on an extremal sample, i.e., the angular part of random vectors for which the radius exceeds a large threshold. More specifically, we focus on the asymptotic dependence of multivariate extremes characterized by the angular or spectral measure in extreme value theory and provide a careful analysis in the case where the extremes are generated from a linear factor model. We give theoretical guarantees on the performance of kernel PCA preimages of such extremes by leveraging their asymptotic distribution together with Davis-Kahan perturbation bounds. Our theoretical findings are complemented with numerical experiments illustrating the finite sample performance of our methods.},
   author = {Marco Avella-Medina and Richard A. Davis and Gennady Samorodnitsky},
   month = {11},
   title = {Kernel PCA for multivariate extremes},
   url = {http://arxiv.org/abs/2211.13172},
   year = {2022},
}
@article{Simpson2020,
   abstract = {In multivariate extreme value analysis, the nature of the extremal dependence between variables should be considered when selecting appropriate statistical models. Interest often lies in determining which subsets of variables can take their largest values simultaneously while the others are of smaller order. Our approach to this problem exploits hidden regular variation properties on a collection of nonstandard cones, and provides a new set of indices that reveal aspects of the extremal dependence structure not available through existing measures of dependence. We derive theoretical properties of these indices, demonstrate their utility through a series of examples, and develop methods of inference that also estimate the proportion of extremal mass associated with each cone. We apply the methods to river flows in the U.K., estimating the probabilities of different subsets of sites being large simultaneously.},
   author = {E. S. Simpson and J. L. Wadsworth and J. A. Tawn},
   doi = {10.1093/BIOMET/ASAA018},
   issn = {0006-3444},
   issue = {3},
   journal = {Biometrika},
   keywords = {Asymptotic independence,Extremal dependence structure,Hidden regular variation,Multivariate regular variation},
   month = {9},
   pages = {513-532},
   publisher = {Oxford Academic},
   title = {Determining the dependence structure of multivariate extremes},
   volume = {107},
   url = {https://academic.oup.com/biomet/article/107/3/513/5831922},
   year = {2020},
}
@article{Gomes2015,
   abstract = {Statistical issues arising in modelling univariate extremes of a random sample have been successfully used in the most diverse fields, such as biometrics, finance, insurance and risk theory. Statistics of univariate extremes (SUE), the subject to be dealt with in this review paper, has recently faced a huge development, partially because rare events can have catastrophic consequences for human activities, through their impact on the natural and constructed environments. In the last decades, there has been a shift from the area of parametric SUE, based on probabilistic asymptotic results in extreme value theory, towards semi-parametric approaches. After a brief reference to Gumbel's block methodology and more recent improvements in the parametric framework, we present an overview of the developments on the estimation of parameters of extreme events and on the testing of extreme value conditions under a semi-parametric framework. We further discuss a few challenging topics in the area of SUE.},
   author = {M. Ivette Gomes and Armelle Guillou},
   doi = {10.1111/INSR.12058},
   issn = {17515823},
   issue = {2},
   journal = {International Statistical Review},
   keywords = {Extreme value index and tail parameters,Parametric and semi-parametric approaches,Statistics of univariate extremes,Testing issues},
   month = {8},
   pages = {263-292},
   publisher = {International Statistical Institute},
   title = {Extreme Value Theory and Statistics of Univariate Extremes: A Review},
   volume = {83},
   year = {2015},
}
@article{Nolde2022,
   abstract = {The study of multivariate extremes is dominated by multivariate regular variation, although it is well known that this approach does not provide adequate distinction between random vectors whose components are not always simultaneously large. Various alternative dependence measures and representations have been proposed, with the most well-known being hidden regular variation and the conditional extreme value model. These varying depictions of extremal dependence arise through consideration of different parts of the multivariate domain, and particularly through exploring what happens when extremes of one variable may grow at different rates from other variables. Thus far, these alternative representations have come from distinct sources, and links between them are limited. In this work we elucidate many of the relevant connections through a geometrical approach. In particular, the shape of the limit set of scaled sample clouds in light-tailed margins is shown to provide a description of several different extremal dependence representations.},
   author = {Natalia Nolde and Jennifer L. Wadsworth},
   doi = {10.1017/APR.2021.51},
   issn = {0001-8678},
   issue = {3},
   journal = {Advances in Applied Probability},
   keywords = {Multivariate extreme value theory,asymptotic (in)dependence,conditional extremes,hidden regular variation,limit set},
   month = {9},
   pages = {688-717},
   publisher = {Cambridge University Press},
   title = {Linking representations for multivariate extremes via a limit set},
   volume = {54},
   url = {https://www.cambridge.org/core/journals/advances-in-applied-probability/article/linking-representations-for-multivariate-extremes-via-a-limit-set/9A77F8E10602DC13EA26E429CFB5FD21},
   year = {2022},
}
@book{Gentle2002,
   author = {James E. Gentle},
   city = {New York},
   doi = {10.1007/b97337},
   isbn = {0-387-95489-9},
   publisher = {Springer-Verlag},
   title = {Elements of Computational Statistics},
   year = {2002},
}
@article{Marron1992,
   author = {J. S. Marron and M. P. Wand},
   doi = {10.1214/aos/1176348653},
   issn = {0090-5364},
   issue = {2},
   journal = {The Annals of Statistics},
   month = {6},
   title = {Exact Mean Integrated Squared Error},
   volume = {20},
   year = {1992},
}
@article{Engelke2022,
   abstract = {Conditional independence and graphical models are well studied for probability distributions on product spaces. We propose a new notion of conditional independence for any measure $\Lambda$ on the punctured Euclidean space $\mathbb R^d\setminus \\{0\\}$ that explodes at the origin. The importance of such measures stems from their connection to infinitely divisible and max-infinitely divisible distributions, where they appear as L\'evy measures and exponent measures, respectively. We characterize independence and conditional independence for $\Lambda$ in various ways through kernels and factorization of a modified density, including a Hammersley-Clifford type theorem for undirected graphical models. As opposed to the classical conditional independence, our notion is intimately connected to the support of the measure $\Lambda$. Our general theory unifies and extends recent approaches to graphical modeling in the fields of extreme value analysis and L\'evy processes. Our results for the corresponding undirected and directed graphical models lay the foundation for new statistical methodology in these areas.},
   author = {Sebastian Engelke and Jevgenijs Ivanovs and Kirstin Strokorb},
   month = {11},
   title = {Graphical models for infinite measures with applications to extremes and L\'evy processes},
   url = {http://arxiv.org/abs/2211.15769},
   year = {2022},
}
@generic{Perperoglou2019,
   abstract = {Background: With progress on both the theoretical and the computational fronts the use of spline modelling has become an established tool in statistical regression analysis. An important issue in spline modelling is the availability of user friendly, well documented software packages. Following the idea of the STRengthening Analytical Thinking for Observational Studies initiative to provide users with guidance documents on the application of statistical methods in observational research, the aim of this article is to provide an overview of the most widely used spline-based techniques and their implementation in R. Methods: In this work, we focus on the R Language for Statistical Computing which has become a hugely popular statistics software. We identified a set of packages that include functions for spline modelling within a regression framework. Using simulated and real data we provide an introduction to spline modelling and an overview of the most popular spline functions. Results: We present a series of simple scenarios of univariate data, where different basis functions are used to identify the correct functional form of an independent variable. Even in simple data, using routines from different packages would lead to different results. Conclusions: This work illustrate challenges that an analyst faces when working with data. Most differences can be attributed to the choice of hyper-parameters rather than the basis used. In fact an experienced user will know how to obtain a reasonable outcome, regardless of the type of spline used. However, many analysts do not have sufficient knowledge to use these powerful tools adequately and will need more guidance.},
   author = {Aris Perperoglou and Willi Sauerbrei and Michal Abrahamowicz and Matthias Schmid},
   doi = {10.1186/s12874-019-0666-3},
   issn = {14712288},
   issue = {1},
   journal = {BMC Medical Research Methodology},
   keywords = {Functional form of continuous covariates,Multivariable modelling},
   month = {3},
   pmid = {30841848},
   publisher = {BioMed Central Ltd.},
   title = {A review of spline function procedures in R},
   volume = {19},
   year = {2019},
}
@article{,
   abstract = {We investigate conditions for the existence of the limiting conditional distribution of a bivariate random vector when one component becomes large. We revisit the existing literature on the topic, and present some new sufficient conditions. We concentrate on the case where the conditioning variable belongs to the maximum domain of attraction of the Gumbel law, and we study geometric conditions on the joint distribution of the vector. We show that these conditions are of a local nature and imply asymptotic independence when both variables belong to the domain of attraction of an extreme value distribution. The new model we introduce can also be useful to simulate bivariate random vectors with a given limiting conditional distribution. Copyright © Taylor & Francis Group, LLC.},
   author = {Anne Laure Fougères and Philippe Soulier},
   doi = {10.1080/15326340903291362},
   issn = {15324214},
   issue = {1},
   journal = {Stochastic Models},
   keywords = {Asymptotic independence,Conditional excess probability,Conditional extreme-value model,Elliptic distributions,Second-order correction,τvarying tail},
   pages = {54-77},
   publisher = {Taylor and Francis Inc.},
   title = {Limit conditional distributions for bivariate vectors with polar representation},
   volume = {26},
   year = {2010},
}
@article{Richards2022,
   abstract = {Extreme wildfires continue to be a significant cause of human death and biodiversity destruction within countries that encompass the Mediterranean Basin. Recent worrying trends in wildfire activity (i.e., occurrence and spread) suggest that wildfires are likely to be highly impacted by climate change. In order to facilitate appropriate risk mitigation, it is imperative to identify the main drivers of extreme wildfires and assess their spatio-temporal trends, with a view to understanding the impacts of global warming on fire activity. To this end, we analyse the monthly burnt area due to wildfires over a region encompassing most of Europe and the Mediterranean Basin from 2001 to 2020, and identify high fire activity during this period in eastern Europe, Algeria, Italy and Portugal. We build an extreme quantile regression model with a high-dimensional predictor set describing meteorological conditions, land cover usage, and orography, for the domain. To model the complex relationships between the predictor variables and wildfires, we make use of a hybrid statistical deep-learning framework that allows us to disentangle the effects of vapour-pressure deficit (VPD), air temperature, and drought on wildfire activity. Our results highlight that whilst VPD, air temperature, and drought significantly affect wildfire occurrence, only VPD affects extreme wildfire spread. Furthermore, to gain insights into the effect of climate change on wildfire activity in the near future, we perturb VPD and temperature according to their observed trends and find evidence that global warming may lead to spatially non-uniform changes in wildfire activity.},
   author = {Jordan Richards and Raphaël Huser and Emanuele Bevacqua and Jakob Zscheischler},
   month = {12},
   title = {Insights into the drivers and spatio-temporal trends of extreme Mediterranean wildfires with statistical deep-learning},
   url = {http://arxiv.org/abs/2212.01796},
   year = {2022},
}
@article{Vandeskog2022,
   abstract = {A successful model for high-dimensional spatial extremes should, in principle, be able to describe both weakening extremal dependence at increasing levels and changes in the type of extremal dependence class as a function of the distance between locations. Furthermore, the model should allow for computationally tractable inference using inference methods that efficiently extract information from data and that are robust to model misspecification. In this paper, we demonstrate how to fulfil all these requirements by developing a comprehensive methodological workflow for efficient Bayesian modelling of high-dimensional spatial extremes using the spatial conditional extremes model while performing fast inference with R-INLA. We then propose a post hoc adjustment method that results in more robust inference by properly accounting for possible model misspecification. The developed methodology is applied for modelling extreme hourly precipitation from high-resolution radar data in Norway. Inference is computationally efficient, and the resulting model fit successfully captures the main trends in the extremal dependence structure of the data. Robustifying the model fit by adjusting for possible misspecification further improves model performance.},
   author = {Silius M. Vandeskog and Sara Martino and Raphaël Huser},
   month = {10},
   title = {An Efficient Workflow for Modelling High-Dimensional Spatial Extremes},
   url = {http://arxiv.org/abs/2210.00760},
   year = {2022},
}
@article{Majumder2022,
   abstract = {Extreme streamflow is a key indicator of flood risk, and quantifying the changes in its distribution under non-stationary climate conditions is key to mitigating the impact of flooding events. We propose a non-stationary process mixture model (NPMM) for annual streamflow maxima over the central US (CUS) which uses downscaled climate model precipitation projections to forecast extremal streamflow. Spatial dependence for the model is specified as a convex combination of transformed Gaussian and max-stable processes, indexed by a weight parameter which identifies the asymptotic regime of the process. The weight parameter is modeled as a function of region and of regional precipitation, introducing spatio-temporal non-stationarity within the model. The NPMM is flexible with desirable tail dependence properties, but yields an intractable likelihood. To address this, we embed a neural network within a density regression model which is used to learn a synthetic likelihood function using simulations from the NPMM with different parameter settings. Our model is fitted using observational data for 1972-2021, and inference carried out in a Bayesian framework. Annual streamflow maxima forecasts for 2021-2035 estimate an increase in the frequency and magnitude of extreme streamflow, with changes being more pronounced in the largest quantiles of the projected annual streamflow maxima.},
   author = {Reetam Majumder and Brian Reich},
   month = {12},
   title = {A Deep Learning Synthetic Likelihood Approximation of a Non-stationary Spatial Model for Extreme Streamflow Forecasting},
   url = {http://arxiv.org/abs/2212.07267},
   year = {2022},
}
@article{Zou2022,
   abstract = {The stable tail dependence function provides a full characterization of the extremal dependence structures. Unfortunately, the estimation of the stable tail dependence function often suffers from significant bias, whose scale relates to the Peaks-Over-Threshold (POT) second-order parameter. For this second-order parameter, this paper introduces a penalized estimator that discourages it from being too close to zero. This paper then establishes this estimator's asymptotic consistency, uses it to correct the bias in the estimation of the stable tail dependence function, and illustrates its desirable empirical properties in the estimation of the extremal dependence structures.},
   author = {Nan Zou},
   month = {12},
   title = {Estimating POT Second-order Parameter for Bias Correction},
   url = {http://arxiv.org/abs/2212.08331},
   year = {2022},
}
@article{Majumder2022,
   abstract = {Quantifying changes in the probability and magnitude of extreme flooding events is key to mitigating their impacts. While hydrodynamic data are inherently spatially dependent, traditional spatial models such as Gaussian processes are poorly suited for modeling extreme events. Spatial extreme value models with more realistic tail dependence characteristics are under active development. They are theoretically justified, but give intractable likelihoods, making computation challenging for small datasets and prohibitive for continental-scale studies. We propose a process mixture model which specifies spatial dependence in extreme values as a convex combination of a Gaussian process and a max-stable process, yielding desirable tail dependence properties but intractable likelihoods. To address this, we employ a unique computational strategy where a feed-forward neural network is embedded in a density regression model to approximate the conditional distribution at one spatial location given a set of neighbors. We then use this univariate density function to approximate the joint likelihood for all locations by way of a Vecchia approximation. The process mixture model is used to analyze changes in annual maximum streamflow within the US over the last 50 years, and is able to detect areas which show increases in extreme streamflow over time.},
   author = {Reetam Majumder and Brian J. Reich and Benjamin A. Shaby},
   month = {8},
   title = {Modeling Extremal Streamflow using Deep Learning Approximations and a Flexible Spatial Process},
   url = {http://arxiv.org/abs/2208.03344},
   year = {2022},
}
@article{,
   abstract = {Many multivariate data sets exhibit a form of positive dependence, which can either appear globally between all variables or only locally within particular subgroups. In models in multivariate extremes arising from threshold exceedances, a natural notion of positive dependence is the recently introduced extremal multivariate total positivity of order 2 ($ \text\{EMTP\}_2 $). While $ \text\{EMTP\}_2 $ has nice theoretical properties, it is by construction a global property and therefore not suitable for applications with only local positive dependence. We introduce extremal association as a weaker form of extremal positive dependence and show that it generalizes extremal tree models. This follows from a sufficient condition for extremal association, which for H\"usler--Reiss distributions permits a parametric description that we call the metric property. As the parameter of a H\"usler--Reiss distribution is a Euclidean distance matrix, the metric property relates to research in electric network theory and Euclidean geometry. We show that the metric property can be localized with respect to a graph and study surrogate likelihood inference. This gives rise to a two-step estimation procedure for locally metrical H\"usler--Reiss graphical models. The second step allows for a simple dual problem, which is implemented via a gradient descent algorithm. Finally, we demonstrate our results on simulated and real data.},
   author = {Frank Röttger and Quentin Schmitz},
   month = {12},
   title = {On the local metric property in multivariate extremes},
   url = {http://arxiv.org/abs/2212.10350},
   year = {2022},
}
@article{Bodik2022,
   abstract = {Determining the causes of extreme events is a fundamental question in many scientific fields. An important aspect when modelling multivariate extremes is the tail dependence. In application, the extreme dependence structure may significantly depend on covariates. As for the general case of modelling including covariates, only some of the covariates are causal. In this paper, we propose a methodology to discover the causal covariates explaining the tail dependence structure between two variables. The proposed methodology for discovering causal variables is based on comparing observations from different environments or perturbations. It is a desired methodology for predicting extremal behaviour in a new, unobserved environment. The methodology is applied to a dataset of $\text\{NO\}_2$ concentration in the UK. Extreme $\text\{NO\}_2$ levels can cause severe health problems, and understanding the behaviour of concurrent severe levels is an important question. We focus on revealing causal predictors for the dependence between extreme $\text\{NO\}_2$ observations at different sites.},
   author = {Juraj Bodik and Linda Mhalla and Valérie Chavez-Demoulin},
   month = {12},
   title = {Detecting causal covariates for extreme dependence structures},
   url = {http://arxiv.org/abs/2212.09831},
   year = {2022},
}
@article{Quintos2001,
   author = {Carmela Quintos and Zhenhong Fan and Peter C. B. Philips},
   doi = {10.1111/1467-937X.00184},
   issn = {0034-6527},
   issue = {3},
   journal = {Review of Economic Studies},
   month = {7},
   pages = {633-663},
   title = {Structural Change Tests in Tail Behaviour and the Asian Crisis},
   volume = {68},
   url = {https://academic.oup.com/restud/article-lookup/doi/10.1111/1467-937X.00184},
   year = {2001},
}
@article{Kesemen2020,
   abstract = {In this study, some bivariate distribution functions are defined in the polar coordinate system and random numbers are generated from these distribution functions. In these definitions, the angular change of the probability density function is taken as constant, and the distance change is performed based on the univariate probability density function. Also, the chi-square goodness-of-fit test is proposed for random numbers generated in the polar coordinates. Four different distribution functions are selected to evaluate the success of the proposed chi-square goodness of fit test for polar distribution functions. Lastly, the validity and the success of the proposed method is shown in the simulation study and the real-life example.},
   author = {Orhan Kesemen and Bugra Kaan Tiryaki and Ozge Tezel and Ayse Pak},
   doi = {10.35378/gujs.610086},
   issn = {21471762},
   issue = {3},
   journal = {Gazi University Journal of Science},
   keywords = {Goodness of fit test,Polar distribution,Polar histogram,Random number},
   month = {9},
   pages = {846-882},
   publisher = {Gazi Universitesi},
   title = {Some statistics in polar coordinate system with uniform angles},
   volume = {33},
   year = {2020},
}
@article{Solman2014,
   abstract = {Search outside the laboratory involves tradeoffs among a variety of internal and external exploratory processes. Here we examine the conditions under which item specific memory from prior exposures to a search array is used to guide attention during search. We extend the hypothesis that memory use increases as perceptual search becomes more difficult by turning to an ecologically important type of search difficulty - energetic cost. Using optical motion tracking, we introduce a novel head-contingent display system, which enables the direct comparison of search using head movements and search using eye movements. Consistent with the increased energetic cost of turning the head to orient attention, we discover greater use of memory in head-contingent versus eye-contingent search, as reflected in both timing and orienting metrics. Our results extend theories of memory use in search to encompass embodied factors, and highlight the importance of accounting for the costs and constraints of the specific motor groups used in a given task when evaluating cognitive effects. © 2014 Elsevier B.V.},
   author = {Grayden J.F. Solman and Alan Kingstone},
   doi = {10.1016/J.COGNITION.2014.05.005},
   issn = {0010-0277},
   issue = {3},
   journal = {Cognition},
   keywords = {Embodied cognition,Memory,Search},
   month = {9},
   pages = {443-454},
   pmid = {24946208},
   publisher = {Elsevier},
   title = {Balancing energetic and cognitive resources: Memory use during search depends on the orienting effector},
   volume = {132},
   year = {2014},
}
@article{Forster2022,
   abstract = {In recent years, parametric models for max-stable processes have become a popular choice for modeling spatial extremes because they arise as the asymptotic limit of rescaled maxima of independent and identically distributed random processes. Apart from few exceptions for the class of extremal-t processes, existing literature mainly focuses on models with stationary dependence structures. In this paper, we propose a novel non-stationary approach that can be used for both Brown-Resnick and extremal-t processes - two of the most popular classes of max-stable processes - by including covariates in the corresponding variogram and correlation functions, respectively. We apply our new approach to extreme precipitation data in two regions in Southern and Northern Germany and compare the results to existing stationary models in terms of Takeuchi's information criterion (TIC). Our results indicate that, for this case study, non-stationary models are more appropriate than stationary ones for the region in Southern Germany. In addition, we investigate theoretical properties of max-stable processes conditional on random covariates. We show that these can result in both asymptotically dependent and asymptotically independent processes. Thus, conditional models are more flexible than classical max-stable models.},
   author = {Carolin Forster and Marco Oesting},
   month = {12},
   title = {Non-stationary max-stable models with an application to heavy rainfall data},
   url = {http://arxiv.org/abs/2212.11598},
   year = {2022},
}
@article{,
   abstract = {This paper devises a regression-type model for the situation where both the response and covariates are extreme. The proposed approach is designed for the setting where the response and covariates are modeled as multivariate extreme values, and thus contrarily to standard regression methods it takes into account the key fact that the limiting distribution of suitably standardized componentwise maxima is an extreme value copula. An important target in the proposed framework is the regression manifold, which consists of a family of regression lines obeying the latter asymptotic result. To learn about the proposed model from data, we employ a Bernstein polynomial prior on the space of angular densities which leads to an induced prior on the space of regression manifolds. Numerical studies suggest a good performance of the proposed methods, and a finance real-data illustration reveals interesting aspects on the conditional risk of extreme losses in two leading international stock markets.},
   author = {Miguel De Carvalho and Alina Kumukova and · Gonçalo and Dos Reis},
   journal = {arXiv},
   keywords = {Angular measure,Bernstein polynomials,Extreme value copula,Joint extremes,Multivariate extreme value distribution,Quantile regression,Statistics of extremes},
   title = {Regression-type analysis for multivariate extreme values},
}
@article{Bai2023,
   abstract = {Conventional methods for extreme event estimation rely on well-chosen parametric models asymptotically justified from extreme value theory (EVT). These methods, while powerful and theoretically grounded, could however encounter a difficult bias-variance tradeoff that exacerbates especially when data size is too small, deteriorating the reliability of the tail estimation. In this paper, we study a framework based on the recently surging literature of distributionally robust optimization. This approach can be viewed as a nonparametric alternative to conventional EVT, by imposing general shape belief on the tail instead of parametric assumption and using worst-case optimization as a resolution to handle the nonparametric uncertainty. We explain how this approach bypasses the bias-variance tradeoff in EVT. On the other hand, we face a conservativeness-variance tradeoff which we describe how to tackle. We also demonstrate computational tools for the involved optimization problems and compare our performance with conventional EVT across a range of numerical examples.},
   author = {Yuanlu Bai and Henry Lam and Xinyu Zhang},
   month = {1},
   title = {A Distributionally Robust Optimization Framework for Extreme Event Estimation},
   url = {http://arxiv.org/abs/2301.01360},
   year = {2023},
}
@article{Hüsler1989,
   abstract = {A new approach to the asymptotic treatment of multivariate sample maxima is suggested and exemplified in the particular case of maxima of normal random vectors. In the limit one obtains a class of multivariate maxstable distributions not considered in literature so far. © 1989.},
   author = {Jürg Hüsler and Rolf Dieter Reiss},
   doi = {10.1016/0167-7152(89)90106-5},
   issn = {0167-7152},
   issue = {4},
   journal = {Statistics \& Probability Letters},
   keywords = {Multivariate sample maxima,correlation coefficient,max-stability,normal random vectors},
   month = {2},
   pages = {283-286},
   publisher = {North-Holland},
   title = {Maxima of normal random vectors: Between independence and complete dependence},
   volume = {7},
   year = {1989},
}
@article{Gumbel1960,
   abstract = {A bivariate distribution is not determined by the knowledge of the margins. Two bivariate distributions with exponential margins are analyzed and another is briefly mentioned. In the first distribution (2.1) the conditional expectation of one variable decreases to zero with increasing values of the other one. The coefficient of correlation is never positive and lies in the interval –.40≤ρ≤0, and the correlation ratio varies from –.48 to zero. In the second distribution (3.4) the conditional expectation of one variable increases or decreases with increasing values of the other variable depending on the sign of the correlation. The coefficient of correlation lies in the interval –.25≤ρ≤.25, and the correlation ratio is proportional to the coefficient. © Taylor & Francis Group, LLC.},
   author = {E. J. Gumbel},
   doi = {10.1080/01621459.1960.10483368},
   issn = {1537274X},
   issue = {292},
   journal = {Journal of the American Statistical Association},
   pages = {698-707},
   title = {Bivariate Exponential Distributions},
   volume = {55},
   year = {1960},
}
@article{Meyer2020,
   abstract = {Identifying directions where extreme events occur is a major challenge in multivariate extreme value analysis. In this paper, we use the concept of sparse regular variation introduced by Meyer and Wintenberger (2021) to infer the tail dependence of a random vector X. This approach relies on the Euclidean projection onto the simplex which better exhibits the sparsity structure of the tail of X than the standard methods. Our procedure based on a rigorous methodology aims at capturing clusters of extremal coordinates of X. It also includes the identification of the threshold above which the values taken by X are considered as extreme. We provide an efficient and scalable algorithm called MUSCLE and apply it on numerical examples to highlight the relevance of our findings. Finally we illustrate our approach with financial return data.},
   author = {Nicolas Meyer and Olivier Wintenberger},
   journal = {arXiv},
   month = {7},
   title = {Multivariate sparse clustering for extremes},
   url = {http://arxiv.org/abs/2007.11848},
   year = {2020},
}
@article{Draisma2004,
   abstract = {In the classical setting of bivariate extreme value theory, the procedures for estimating the probability of an extreme event are not applicable if the componentwise maxima of the observations are asymptotically independent. To cope with this problem, Ledford and Tawn proposed a submodel in which the penultimate dependence is characterized by an additional parameter. We discuss the asymptotic properties of two estimators for this parameter in an extended model. Moreover, we develop an estimator for the probability of an extreme event that works in the case of asymptotic independence as well as in the case of asymptotic dependence, and prove its consistency.},
   author = {Gerrit Draisma and Holger Drees and Ana Ferreira and Laurens De Haan},
   doi = {10.3150/bj/1082380219},
   issn = {1350-7265},
   issue = {2},
   journal = {Bernoulli},
   keywords = {Hill estimator,asymptotic normality,bivariate extreme value distribution,coefficient of tail dependence,copula,failure probability,moment estimator},
   month = {4},
   title = {Bivariate tail estimation: dependence in asymptotic independence},
   volume = {10},
   url = {https://projecteuclid.org/journals/bernoulli/volume-10/issue-2/Bivariate-tail-estimation-dependence-in-asymptotic-independence/10.3150/bj/1082380219.full},
   year = {2004},
}
@article{Smith1989,
   author = {Richard L. Smith},
   doi = {10.1214/ss/1177012400},
   issn = {0883-4237},
   issue = {4},
   journal = {Statistical Science},
   month = {11},
   pages = {367-377},
   title = {Extreme Value Analysis of Environmental Time Series: An Application to Trend Detection in Ground-Level Ozone},
   volume = {4},
   url = {https://projecteuclid.org/journals/statistical-science/volume-4/issue-4/Extreme-Value-Analysis-of-Environmental-Time-Series--An-Application/10.1214/ss/1177012400.full},
   year = {1989},
}
@article{Yee2007,
   abstract = {Over recent years parametric and nonparametric regression has slowly been adopted into extreme value data analysis. Its introduction has been characterized by piecemeal additions and embellishments, which has had a negative effect on software development and usage. The purpose of this article is to convey the classes of vector generalized linear and additive models (VGLMs and VGAMs) as offering significant advantages for extreme value data analysis, providing flexible smoothing within a unifying framework. In particular, VGLMs and VGAMs allow all parameters of extreme value distributions to be modelled as linear or smooth functions of covariates. We implement new auxiliary methodology by incorporating a quasi-Newton update for the working weight matrices within an iteratively reweighted least squares (IRLS) algorithm. A software implementation by the first author, called the vgam package for [InlineMediaObject not available: see fulltext.], is used to illustrate the potential of VGLMs and VGAMs. © 2007 Springer Science+Business Media, LLC.},
   author = {Thomas W. Yee and Alec G. Stephenson},
   doi = {10.1007/S10687-007-0032-4/METRICS},
   issn = {13861999},
   issue = {1-2},
   journal = {Extremes},
   keywords = {Extreme value modelling,Fisher scoring,Iteratively reweighted least squares,Maximum likelihood estimation,Penalized likelihood,Smoothing,Vector splines},
   month = {6},
   pages = {1-19},
   publisher = {Springer},
   title = {Vector generalized linear and additive extreme value models},
   volume = {10},
   url = {https://link.springer.com/article/10.1007/s10687-007-0032-4},
   year = {2007},
}
@article{Escobar2018,
   abstract = {We consider the robust estimation of the Pickands dependence function in the random covariate framework. Our estimator is based on local estimation with the minimum density power divergence criterion. We provide the main asymptotic properties, in particular the convergence of the stochastic process, correctly normalized, towards a tight centered Gaussian process. The finite sample performance of our estimator is evaluated with a simulation study involving both uncontaminated and contaminated samples. The method is illustrated on a dataset of air pollution measurements.},
   author = {Mikael Escobar-Bach and Yuri Goegebeur and Armelle Guillou},
   doi = {10.1214/17-AOS1640},
   issn = {0090-5364},
   issue = {6A},
   journal = {https://doi.org/10.1214/17-AOS1640},
   keywords = {60F05,60G70,62G05,62G20,62G32,Conditional Pickands dependence function,Stochastic convergence,robustness},
   month = {12},
   pages = {2806-2843},
   publisher = {Institute of Mathematical Statistics},
   title = {Local robust estimation of the Pickands dependence function},
   volume = {46},
   url = {https://projecteuclid.org/journals/annals-of-statistics/volume-46/issue-6A/Local-robust-estimation-of-the-Pickands-dependence-function/10.1214/17-AOS1640.full https://projecteuclid.org/journals/annals-of-statistics/volume-46/issue-6A/Local-robust-estimation-of-the-Pickands-dependence-function/10.1214/17-AOS1640.short},
   year = {2018},
}
@article{Jonathan2014b,
   abstract = {Careful modelling of non-stationarity is critical to reliable specification of marine and coastal design criteria. We present a spline based methodology to incorporate spatial, directional, temporal and other covariate effects in extreme value models for environmental variables such as storm severity. For storm peak significant wave height events, the approach uses quantile regression to estimate a suitable extremal threshold, a Poisson process model for the rate of occurrence of threshold exceedances, and a generalised Pareto model for size of threshold exceedances. Multidimensional covariate effects are incorporated at each stage using penalised (tensor products of) B-splines to give smooth model parameter variation as a function of multiple covariates. Optimal smoothing penalties are selected using cross-validation, and model uncertainty is quantified using a bootstrap re-sampling procedure. The method is applied to estimate return values for large spatial neighbourhoods of locations, incorporating spatial and directional effects. Extensions to joint modelling of multivariate extremes, incorporating extremal spatial dependence (using max-stable processes) or more general extremal dependence (using the conditional extremes approach) are outlined. © 2014 Elsevier Ltd.},
   author = {Philip Jonathan and David Randell and Yanyun Wu and Kevin Ewans},
   doi = {10.1016/J.OCEANENG.2014.07.007},
   issn = {0029-8018},
   journal = {Ocean Engineering},
   keywords = {B-spline,Covariate,Extreme,Offshore design,Return value,Storm severity},
   month = {9},
   pages = {520-532},
   publisher = {Pergamon},
   title = {Return level estimation from non-stationary spatial data exhibiting multidimensional covariate effects},
   volume = {88},
   year = {2014},
}
@article{Blanchet2011,
   abstract = {The spatial modeling of extreme snow is important for adequate risk management in Alpine and high altitude countries. A natural approach to such modeling is through the theory of max-stable processes, an infinite-dimensional extension of multivariate extreme value theory. In this paper we describe the application of such processes in modeling the spatial dependence of extreme snow depth in Switzerland, based on data for the winters 1966–2008 at 101 stations. The models we propose rely on a climate transformation that allows us to account for the presence of climate regions and for directional effects, resulting from synoptic weather patterns. Estimation is performed through pairwise likelihood inference and the models are compared using penalized likelihood criteria. The max-stable models provide a much better fit to the joint behavior of the extremes than do independence or full dependence models.},
   author = {Juliette Blanchet and Anthony C. Davison},
   doi = {10.1214/11-AOAS464},
   issn = {1932-6157},
   issue = {3},
   journal = {The Annals of Applied Statistics},
   keywords = {Climate space,Extreme value theory,Max-stable process,extremal coefficient,pairwise likelihood,snow depth data},
   month = {9},
   pages = {1699-1725},
   publisher = {Institute of Mathematical Statistics},
   title = {Spatial modeling of extreme snow depth},
   volume = {5},
   url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-5/issue-3/Spatial-modeling-of-extreme-snow-depth/10.1214/11-AOAS464.full},
   year = {2011},
}
@article{Youngman2020a,
   abstract = {There are many situations when modelling environmental phenomena for which it is not appropriate to assume a stationary dependence structure. \cite\{sampson1992\} proposed an approach to allowing nonstationarity in dependence based on a deformed space: coordinates from original geographic "$G$" space are mapped to a new dispersion "$D$" space in which stationary dependence is a reasonable assumption. \cite\{sampson1992\} achieve this with two deformation functions, which are chosen as thin plate splines, each representing how one of the two coordinates in $D$-space relates to the original $G$-space coordinates. This works extends the deformation approach, and the dimension expansion approach of \cite\{bornn2012\}, to a regression-based framework in which all dimensions in $D$-space are treated as "smooths" as found, for example, in generalized additive models. The framework offers an intuitive and user-friendly approach to specifying $D$-space, allows different levels of smoothing for dimensions in $D$-space, and allows objective inference for all model parameters. Furthermore, a numerical approach is proposed to avoid non-bijective deformations, should they occur, which applies to any deformation. The proposed framework is demonstrated on the solar radiation data studied in \cite\{sampson1992\}, and then on an example related to risk analysis, which culminates in producing simulations of extreme rainfall for part of Colorado, US.},
   author = {Benjamin D. Youngman},
   journal = {arXiv},
   month = {1},
   title = {Flexible models for nonstationary dependence: Methodology and examples},
   url = {http://arxiv.org/abs/2001.06642},
   year = {2020},
}
@article{Tilloy2020,
   abstract = {Modelling multiple hazard interrelations remains a challenge for practitioners. This article primarily focuses on the interrelations between pairs of hazards. The efficacy of six distinct bivariate extreme models is evaluated through their fitting capabilities to 60 synthetic datasets. The properties of the synthetic datasets (marginal distributions, tail dependence structure) are chosen to match bivariate time series of environmental variables. The six models are copulas (one non-parametric, one semi-parametric, four parametric). We build 60 distinct synthetic datasets based on different parameters of log-normal margins and two different copulas. The systematic framework developed contrasts the model strengths (model flexibility) and weaknesses (poorer fits to the data). We find that no one model fits our synthetic data for all parameters but rather a range of models depending on the characteristics of the data. To highlight the benefits of the systematic modelling framework developed, we consider the following environmental data: (i) daily precipitation and maximum wind gusts for 1971 to 2018 in London, UK, and (ii) daily mean temperature and wildfire numbers for 1980 to 2005 in Porto District, Portugal. In both cases there is good agreement in the estimation of bivariate return periods between models selected from the systematic framework developed in this study. Within this framework, we have explored a way to model multi-hazard events and identify the most efficient models for a given set of synthetic data and hazard sets.},
   author = {Aloïs Tilloy and Bruce D. Malamud and Hugo Winter and Amélie Joly-Laugel},
   doi = {10.5194/NHESS-20-2091-2020},
   issn = {16849981},
   issue = {8},
   journal = {Natural Hazards and Earth System Sciences},
   month = {8},
   pages = {2091-2117},
   publisher = {Copernicus GmbH},
   title = {Evaluating the efficacy of bivariate extreme modelling approaches for multi-hazard scenarios},
   volume = {20},
   year = {2020},
}
@article{Berkowitz2000,
   author = {Jeremy Berkowitz and Lutz Kilian},
   doi = {10.1080/07474930008800457},
   issn = {15324168},
   issue = {1},
   journal = {Econometric Reviews},
   keywords = {ARLIA,Blocks,Bootstrap,Frequency domain},
   pages = {1-48},
   title = {Recent developments in bootstrapping time series},
   volume = {19},
   year = {2000},
}
@article{Baker2018,
   abstract = {The Paris Agreement 1 aims to 'pursue efforts to limit the temperature increase to 1.5 °C above pre-industrial levels.' However, it has been suggested that temperature targets alone are insufficient to limit the risks associated with anthropogenic emissions 2,3. Here, using an ensemble of model simulations, we show that atmospheric CO2 increase-an even more predictable consequence of emissions than global temperature increase-has a significant direct impact on Northern Hemisphere summer temperature, heat stress, and tropical precipitation extremes. Hence in an iterative climate mitigation regime aiming solely for a specific temperature goal, an unexpectedly low climate response may have corresponding 'dangerous' changes in extreme events. The direct impact of higher CO2 concentrations on climate extremes therefore substantially reduces the upper bound of the carbon budget, and highlights the need to explicitly limit atmospheric CO2 concentration when formulating allowable emissions. Thus, complementing global mean temperature goals with explicit limits on atmospheric CO2 concentrations in future climate policy would limit the adverse effects of high-impact weather extremes.},
   author = {Hugh S. Baker and Richard J. Millar and David J. Karoly and Urs Beyerle and Benoit P. Guillod and Dann Mitchell and Hideo Shiogama and Sarah Sparrow and Tim Woollings and Myles R. Allen},
   doi = {10.1038/s41558-018-0190-1},
   issn = {17586798},
   issue = {7},
   journal = {Nature Climate Change},
   month = {7},
   pages = {604-608},
   publisher = {Nature Publishing Group},
   title = {Higher CO2 concentrations increase extreme event risk in a 1.5 °c world},
   volume = {8},
   year = {2018},
}
@article{Yu2022,
   abstract = {For any given city, on any calendar day, there will be record high and low temperatures. Which record occurred earlier? If there is a trend towards warming then, intuitively, there should be a preponderance of record highs occurring more recently than the record lows for each of the 365 calendar days. We are interested in modeling the joint distribution of appearances of the extremes but not these values themselves. We develop a bivariate discrete distribution modeling the joint indices of maximum and minimum in a sequence of independent random variables sampled from different distributions. We assume these distributions share a proportional hazard rate and develop regression methods for these paired values. This approach has reasonable power to detect a small mean change over a decade. Using readily available public data, we examine the daily calendar extreme values of five US cities for the decade 2011–2020. We develop linear regression models for these data, describe models to account for calendar-date dependence, and use diagnostic measures to detect remarkable observations.},
   author = {Chang Yu and Ondrej Blaha and Michael Kane and Wei Wei and Denise Esserman and Daniel Zelterman},
   doi = {10.1002/ENV.2764},
   issn = {1099095X},
   issue = {7},
   journal = {Environmetrics},
   keywords = {bivariate discrete distribution,climate change,proportional hazards},
   month = {11},
   publisher = {John Wiley and Sons Ltd},
   title = {Regression methods for the appearances of extremes in climate data},
   volume = {33},
   year = {2022},
}
@article{Worms2022,
   abstract = {Within the statistical climatology literature, inferring the contributions of potential causes with regard to climate change has become a recurrent research theme during this last decade. In particular, disentangling human induced (anthropogenic) forcings from natural causes represents a nontrivial statistical task, especially when the focal point moves away from mean behaviors and goes towards extreme events with high societal impacts. Most studies found in the field of extreme event attributions (EEA) rely on extreme value theory. Under this theoretical umbrella, it is often assumed that, for a given location, temporal changes in extremes can be detected in both location and scale parameters of an extreme value distribution, while its shape parameter remains unchanged over time. This assumption of constant tail shape parameters between a so-called factual world (all forcings) and a counterfactual one (without anthropogenic forcing) can be challenged due to the fact that important forcing changes could impact large scale atmospheric and oceanic circulation patterns, and consequently, the latter can reshape the full distribution, including its shape parameter that drives extremal behavior. In this article, we study how allowing different tail shape parameters between the factual and counterfactual worlds can affect the analysis of records. In particular, we extend the work of Naveau et al. in which this case was not treated. We also add properties and theoretical inferential results about records in EEA and propose a procedure for model validation. A simulation study of our approach is detailed. Our method is applied to records of yearly maxima of daily maxima of near-surface air temperature issued from the numerical climate model CNRM-CM6-1 of Météo-France.},
   author = {Julien Worms and Philippe Naveau},
   doi = {10.1002/env.2777},
   issn = {1180-4009},
   issue = {8},
   journal = {Environmetrics},
   keywords = {Weibull distribution,causality theory,climate detection and attribution,generalized extreme value},
   month = {12},
   publisher = {John Wiley and Sons Ltd},
   title = {Record events attribution in climate studies},
   volume = {33},
   url = {https://onlinelibrary.wiley.com/doi/10.1002/env.2777},
   year = {2022},
}
@article{Shooter2021,
   abstract = {The extremal spatial dependence of significant wave height in the North East Atlantic is explored using Joint Altimetry Satellite Oceanography Network satellite altimeter observations for the period 2002–2018, and a spatial conditional extremes model motivated by the work of Heffernan and Tawn. The analysis involves (a) registering individual satellite passes onto a template transect, (b) marginal extreme value analysis at a set of locations on the template transect and transformation from physical to standard Laplace scale, (c) estimation of the spatial conditional extremes model for a set of locations on a template transect, and (d) comparison of extreme spatial dependence for different template transects. Inferences for two transects considered are qualitatively similar; however, for the “normal ascending” transect running approximately south-west to north-east lying between Iceland and the United Kingdom, extremal spatial dependence is found to decay more quickly than for the second “opposite descending” transect running approximately north-west to south-east to the west of Ireland.},
   author = {R. Shooter and E. Ross and A. Ribal and I. R. Young and P. Jonathan},
   doi = {10.1002/ENV.2674},
   issn = {1099095X},
   issue = {4},
   journal = {Environmetrics},
   keywords = {Atlantic,altimeter,extreme,satellite,significant wave height,spatial dependence},
   month = {6},
   publisher = {John Wiley and Sons Ltd},
   title = {Spatial dependence of extreme seas in the North East Atlantic from satellite altimeter measurements},
   volume = {32},
   year = {2021},
}
@article{Baran2021,
   abstract = {In recent years, ensemble weather forecasting has become a routine at all major weather prediction centers. These forecasts are obtained from multiple runs of numerical weather prediction models with different initial conditions or model parametrizations. However, ensemble forecasts can often be underdispersive and also biased, so some kind of postprocessing is needed to account for these deficiencies. One of the most popular state of the art statistical postprocessing techniques is the ensemble model output statistics (EMOS), which provides a full predictive distribution of the studied weather quantity. We propose a novel EMOS model for calibrating wind speed ensemble forecasts, where the predictive distribution is a generalized extreme value (GEV) distribution left truncated at zero (TGEV). The truncation corrects the disadvantage of the GEV distribution-based EMOS models of occasionally predicting negative wind speed values, without affecting its favorable properties. The new model is tested on four datasets of wind speed ensemble forecasts provided by three different ensemble prediction systems, covering various geographical domains and time periods. The forecast skill of the TGEV EMOS model is compared with the predictive performance of the truncated normal, log-normal and GEV methods and the raw and climatological forecasts as well. The results verify the advantageous properties of the novel TGEV EMOS approach.},
   author = {Sándor Baran and Patrícia Szokol and Marianna Szabó},
   doi = {10.1002/ENV.2678},
   issn = {1099095X},
   issue = {6},
   journal = {Environmetrics},
   keywords = {continuous ranked probability score,ensemble calibration,ensemble model output statistics,truncated generalized extreme value distribution},
   month = {9},
   publisher = {John Wiley and Sons Ltd},
   title = {Truncated generalized extreme value distribution-based ensemble model output statistics model for calibration of wind speed ensemble forecasts},
   volume = {32},
   year = {2021},
}
@article{Kunsch1989,
   abstract = {We extend the jackknife and the bootstrap method of estimating standard errors to the case where the observations form a general stationary sequence. We do not attempt a reduction to i.i.d. values. The jackknife calculates the sample variance of replicates of the statistic obtained by omitting each block of $l$ consecutive data once. In the case of the arithmetic mean this is shown to be equivalent to a weighted covariance estimate of the spectral density of the observations at zero. Under appropriate conditions consistency is obtained if $l = l(n) \rightarrow \infty$ and $l(n)/n \rightarrow 0$. General statistics are approximated by an arithmetic mean. In regular cases this approximation determines the asymptotic behavior. Bootstrap replicates are constructed by selecting blocks of length $l$ randomly with replacement among the blocks of observations. The procedures are illustrated by using the sunspot numbers and some simulated data.},
   author = {Hans R. Kunsch},
   doi = {10.1214/AOS/1176347265},
   issn = {0090-5364},
   issue = {3},
   journal = {https://doi.org/10.1214/aos/1176347265},
   keywords = {62G05,62G15,62M10,bootstrap,influence function,jackknife,statistics defined by functionals,time series,variance estimation},
   month = {9},
   pages = {1217-1241},
   publisher = {Institute of Mathematical Statistics},
   title = {The Jackknife and the Bootstrap for General Stationary Observations},
   volume = {17},
   url = {https://projecteuclid.org/journals/annals-of-statistics/volume-17/issue-3/The-Jackknife-and-the-Bootstrap-for-General-Stationary-Observations/10.1214/aos/1176347265.full https://projecteuclid.org/journals/annals-of-statistics/volume-17/issue-3/The-Jackknife-and-the-Bootstrap-for-General-Stationary-Observations/10.1214/aos/1176347265.short},
   year = {1989},
}
@article{Deidda2023,
   abstract = {Extremal dependence describes the strength of correlation between the largest observations of two variables. It is usually measured with symmetric dependence coefficients that do not depend on the order of the variables. In many cases, there is a natural asymmetry between extreme observations that can not be captured by such coefficients. An example for such asymmetry are large discharges at an upstream and a downstream stations on a river network: an extreme discharge at the upstream station will directly influence the discharge at the downstream station, but not vice versa. Simple measures for asymmetric dependence in extreme events have not yet been investigated. We propose the asymmetric tail Kendall's $\tau$ as a measure for extremal dependence that is sensitive to asymmetric behaviour in the largest observations. It essentially computes the classical Kendall's $\tau$ but conditioned on the extreme observations of one of the two variables. We show theoretical properties of this new coefficient and derive a formula to compute it for existing copula models. We further study its effectiveness and connections to causality in simulation experiments. We apply our methodology to a case study on river networks in the United Kingdom to illustrate the importance of measuring asymmetric extremal dependence in hydrology. Our results show that there is important structural information in the asymmetry that would have been missed by a symmetric measure. Our methodology is an easy but effective tool that can be applied in exploratory analysis for understanding the connections among variables and to detect possible asymmetric dependencies.},
   author = {Cristina Deidda and Sebastian Engelke and Carlo De Michele},
   month = {1},
   title = {Asymmetric dependence in hydrological extremes},
   url = {http://arxiv.org/abs/2301.08764},
   year = {2023},
}
@article{Chebana2011,
   abstract = {Several hydrological phenomena are described by two or more correlated characteristics. These dependent characteristics should be considered jointly to be more representative of the multivariate nature of the phenomenon. Consequently, probabilities of occurrence cannot be estimated on the basis of univariate frequency analysis (FA). The quantile, representing the value of the variable(s) corresponding to a given risk, is one of the most important notions in FA. The estimation of multivariate quantiles has not been specifically treated in the hydrological FA literature. In the present paper, we present a new and general framework for local FA based on a multivariate quantile version. The multivariate quantile offers several combinations of the variable values that lead to the same risk. A simulation study is carried out to evaluate the performance of the proposed estimation procedure and a case study is conducted. Results show that the bivariate estimation procedure has an analogous behaviour to the univariate one with respect to the risk and the sample size. However, the dependence structure between variables is ignored in the univariate case. The univariate estimates are obtained as special combinations by the multivariate procedure and with equivalent accuracy. Copyright © 2009 John Wiley & Sons, Ltd.},
   author = {F. Chebana and T. B.M.J. Ouarda},
   doi = {10.1002/ENV.1027},
   issn = {11804009},
   issue = {1},
   journal = {Environmetrics},
   keywords = {Estimation,Floods,Frequency analysis,Hydrology,Multivariate quantile},
   month = {2},
   pages = {63-78},
   title = {Multivariate quantiles in hydrological frequency analysis},
   volume = {22},
   year = {2011},
}
@article{Konzen2021,
   abstract = {This article compares the modeling of nonstationary extreme events using parametric models with local parametric and semiparametric approaches also motivated by extreme value theory. Specifically, three estimators are compared based on (a) (local) semiparametric moment estimation, (b) (local) maximum likelihood estimation, and (c) spline-based maximum likelihood estimation. Inference is performed in a sequential manner, highlighting the synergies between the different approaches to estimating extreme quantiles, including the T-year level and right endpoint when finite. We present a novel heuristic to estimate nonstationary extreme value threshold with exceedances varying on a circular domain, and hypothesis-testing procedures for identifying max-domain of attraction in the nonstationary setting. Bootstrapping is used to estimate nonstationary confidence bounds throughout. We provide step-by-step guides for estimation, and explore the different inference strategies in application to directional modeling of hindcast storm peak significant wave heights recorded in the North Sea.},
   author = {Evandro Konzen and Cláudia Neves and Philip Jonathan},
   doi = {10.1002/ENV.2667},
   issn = {1099095X},
   issue = {4},
   journal = {Environmetrics},
   keywords = {circular,direction,endpoint,extreme quantile,kernel smoothing,peaks over threshold,periodic,significant wave height,spline,threshold selection},
   month = {6},
   publisher = {John Wiley and Sons Ltd},
   title = {Modeling nonstationary extremes of storm severity: Comparing parametric and semiparametric inference},
   volume = {32},
   year = {2021},
}
@report{,
   abstract = {The design and reanalysis of offshore and coastal structures usually requires the estimation of return values for dominant metocean variables (such as significant wave height) and associated values for other variables (such as peak spectral period or wind speed) from a finite sample of data; these are typically estimated using extreme value analysis. Yet the parameters of extreme value models can only be estimated with error from finite data. Different choices available to summarise uncertain information about the characteristics of the tail of a multivariate distribution in a small number of summary statistics (such as return values and associated values) complicates their estimation, especially for small sample sizes: choices regarding the ordering of mathematical operations lead to estimators of return values and associated values with different finite sample bias and variance characteristics. The current work extends a previous study (Jonathan et al. 2021) into the performance of estimators for marginal return values in the presence of sampling uncertainty, to estimators of associated values based on the bivariate conditional extremes model (Heffernan and Tawn 2004) and competitors. Using a large designed simulation experiment, we explore the performance of combinations of 12 different estimators and three bivariate model candidates. The rich set of results from the simulation experiment are reported and explained in detail. Briefly: (a) calculation of associated values is only always feasible from small samples using two of the 12 estimators, which should be preferred; (b) estimators exploiting the median rather than the mean to summarise a distribution are more robust, and should also be preferred, especially for small sample sizes; (c) extreme value models incorporating appropriate descriptions of marginal and dependence provide better estimation of associated values for larger sample size; and (d) summarising the joint tail of metocean variables (in terms of return values and associated values) should be avoided where possible, in favour of probabilistic risk analysis of structural failure incorporating full uncertainty propagation.},
   author = {Ross Towe and David Randell and Jennifer Kensler and Graham Feld and Philip Jonathan},
   keywords = {associated value,conditioning,extreme,metocean,multivariate,return value},
   title = {Estimation of associated values from conditional extreme value models},
   url = {https://ssrn.com/abstract=4247199},
}
@article{Politis1994,
   author = {Dimitris N. Politis and Joseph P. Romano},
   doi = {10.1080/01621459.1994.10476870},
   issn = {0162-1459},
   issue = {428},
   journal = {Journal of the American Statistical Association},
   month = {12},
   pages = {1303-1313},
   title = {The Stationary Bootstrap},
   volume = {89},
   year = {1994},
}
@article{Simpson2021a,
   abstract = {Recent extreme value theory literature has seen significant emphasis on the modelling of spatial extremes, with comparatively little consideration of spatio-temporal extensions. This neglects an important feature of extreme events: their evolution over time. Many existing models for the spatial case are limited by the number of locations they can handle; this impedes extension to space–time settings, where models for higher dimensions are required. Moreover, the spatio-temporal models that do exist are restrictive in terms of the range of extremal dependence types they can capture. Recently, conditional approaches for studying multivariate and spatial extremes have been proposed, which enjoy benefits in terms of computational efficiency and an ability to capture both asymptotic dependence and asymptotic independence. We extend this class of models to a spatio-temporal setting, conditioning on the occurrence of an extreme value at a single space–time location. We adopt a composite likelihood approach for inference, which combines information from full likelihoods across multiple space–time conditioning locations. We apply our model to Red Sea surface temperatures, show that it fits well using a range of diagnostic plots, and demonstrate how it can be used to assess the risk of coral bleaching attributed to high water temperatures over consecutive days.},
   author = {Emma S. Simpson and Jennifer L. Wadsworth},
   doi = {10.1016/J.SPASTA.2020.100482},
   issn = {22116753},
   journal = {Spatial Statistics},
   keywords = {Conditional extremes,Environmental extremes,Extremal dependence modelling,Spatio-temporal modelling},
   month = {3},
   publisher = {Elsevier B.V.},
   title = {Conditional modelling of spatio-temporal extremes for Red Sea surface temperatures},
   volume = {41},
   year = {2021},
}
@article{Coles1994,
   abstract = {For many structural design problems univariate extreme value theory is applied to quantify the risk of failure due to extreme levels of some environmental process. In practice, many forms of structure fail owing to a combination of various processes at extreme levels. Recent developments in statistical methodology for multivariate extremes enable the modelling of such behaviour. The aim of this paper is to demonstrate how these ideas can be exploited as part of the design process},
   author = {Stuart G. Coles and Jonathan A. Tawn},
   doi = {10.2307/2986112},
   issn = {00359254},
   issue = {1},
   journal = {Applied Statistics},
   pages = {1},
   publisher = {JSTOR},
   title = {Statistical Methods for Multivariate Extremes: An Application to Structural Design},
   volume = {43},
   url = {https://www.jstor.org/stable/2986112?origin=crossref},
   year = {1994},
}
@article{Chatterjee1990,
   abstract = {Extreme points in multivariate space have many applications in statistics and can be studied as vertices of a convex polytope (hulls). Finding such vertices is a major topic of study in computational geometry but has received very little attention in applied statistics. Three new methods, based on random and restricted random sampling, is introduced for finding vertices of convex hulls of a sample of N points in k space. Simple projections are used iteratively to find the extreme points. Compared to existing methods, the new method utilizes raw computing power over theoretical analysis and is efficient for practical problems.},
   author = {Sangit Chatterjee and Samprit Chatterjee},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Computational complexity,Convex hulls,Data analysis,Polytopes,Random sampling,Vertices},
   pages = {87-92},
   title = {A note on finding extreme points in multivariate space},
   volume = {10},
   year = {1990},
}
@article{Peng1999,
   abstract = {In this paper we shall give an alternative derivation of the coefficient of tail dependence introduced by Ledford and Tawn [1996, Biometrika 83, 169-187] and propose a consistent estimator, which is asymptotically normal. © 1999 Elsevier Science B.V. All rights reserved.},
   author = {L. Peng},
   doi = {10.1016/S0167-7152(98)00280-6},
   issn = {01677152},
   issue = {4},
   journal = {Statistics and Probability Letters},
   keywords = {Bivariate extremes,Coefficient of tail dependence,Extreme value theory,Regular variation},
   month = {7},
   pages = {399-409},
   publisher = {Elsevier},
   title = {Estimation of the coefficient of tail dependence in bivariate extremes},
   volume = {43},
   year = {1999},
}
@article{Frahm2005,
   abstract = {The concept of tail dependence describes the amount of dependence in the lower-left-quadrant tail or upper-right-quadrant tail of a bivariate distribution. A common measure of tail dependence is given by the so-called tail-dependence coefficient. This paper surveys various estimators for the tail-dependence coefficient within a parametric, semiparametric, and nonparametric framework. Further, a detailed simulation study is provided which compares and illustrates the advantages and disadvantages of the estimators. © 2005 Elsevier B.V. All rights reserved.},
   author = {Gabriel Frahm and Markus Junker and Rafael Schmidt},
   doi = {10.1016/J.INSMATHECO.2005.05.008},
   issn = {01676687},
   issue = {1 SPEC. ISS.},
   journal = {Insurance: Mathematics and Economics},
   keywords = {Copula,Estimation,Extreme value theory,Simulation,Tail dependence,Tail-dependence coefficient},
   month = {8},
   pages = {80-100},
   publisher = {Elsevier},
   title = {Estimating the tail-dependence coefficient: Properties and pitfalls},
   volume = {37},
   year = {2005},
}
@article{Aghakouchak2010,
   abstract = {Extreme rainfall events are of particular importance due to their severe impacts on the economy, the environment and the society. Characterization and quantification of extremes and their spatial dependence structure may lead to a better understanding of extreme events. An important concept in statistical modeling is the tail dependence coefficient (TDC) that describes the degree of association between concurrent rainfall extremes at different locations. Accurate knowledge of the spatial characteristics of the TDC can help improve on the existing models of the occurrence probability of extreme storms. In this study, efficient estimation of the TDC in rainfall is investigated using a dense network of rain gauges located in south Louisiana, USA. The inter-gauge distances in this network range from about 1. km to 9. km. Four different nonparametric TDC estimators are implemented on samples of the rain gauge data and their advantages and disadvantages are discussed. Three averaging time-scales are considered: 1. h, 2. h and 3. h. The results indicate that a significant tail dependency may exist that cannot be ignored for realistic modeling of multivariate rainfall fields. Presence of a strong dependence among extremes contradicts with the assumption of joint normality, commonly used in hydrologic applications. © 2010 Elsevier Ltd.},
   author = {Amir Aghakouchak and Grzegorz Ciach and Emad Habib},
   doi = {10.1016/J.ADVWATRES.2010.07.003},
   issn = {03091708},
   issue = {9},
   journal = {Advances in Water Resources},
   keywords = {Empirical copula,Extreme values,Nonparametric estimators,Rainfall,Tail dependence coefficient (TDC)},
   month = {9},
   pages = {1142-1149},
   title = {Estimation of tail dependence coefficient in rainfall accumulation fields},
   volume = {33},
   year = {2010},
}
@article{Taillardat2019,
   abstract = {Verifying probabilistic forecasts for extreme events is a highly active research area because popular media and public opinions are naturally focused on extreme events, and biased conclusions are readily made. In this context, classical verification methods tailored for extreme events, such as thresholded and weighted scoring rules, have undesirable properties that cannot be mitigated, and the well-known continuous ranked probability score (CRPS) is no exception. In this paper, we define a formal framework for assessing the behavior of forecast evaluation procedures with respect to extreme events, which we use to demonstrate that assessment based on the expectation of a proper score is not suitable for extremes. Alternatively, we propose studying the properties of the CRPS as a random variable by using extreme value theory to address extreme event verification. An index is introduced to compare calibrated forecasts, which summarizes the ability of probabilistic forecasts for predicting extremes. The strengths and limitations of this method are discussed using both theoretical arguments and simulations.},
   author = {Maxime Taillardat and Anne-Laure Fougères and Philippe Naveau and Raphaël de Fondeville},
   doi = {10.1016/j.ijforecast.2022.07.003},
   month = {5},
   title = {Evaluating probabilistic forecasts of extremes using continuous ranked probability score distributions},
   url = {http://arxiv.org/abs/1905.04022 http://dx.doi.org/10.1016/j.ijforecast.2022.07.003},
   year = {2019},
}
@article{Casey2023,
   abstract = {We develop an asymptotic theory for extremes in decomposable graphical models by presenting results applicable to a range of extremal dependence types. Specifically, we investigate the weak limit of the distribution of suitably normalised random vectors, conditioning on an extreme component, where the conditional independence relationships of the random vector are described by a chordal graph. Under mild assumptions, the random vector corresponding to the distribution in the weak limit, termed the tail graphical model, inherits the graphical structure of the original chordal graph. Our theory is applicable to a wide range of decomposable graphical models including asymptotically dependent and asymptotically independent graphical models. Additionally, we analyze combinations of copula classes with differing extremal dependence in cases where a normalization in terms of the conditioning variable is not guaranteed by our assumptions. We show that, in a block graph, the distribution of the random vector normalized in terms of the random variables associated with the separators converges weakly to a distribution we term tail noise. In particular, we investigate the limit of the normalized random vectors where the clique distributions belong to two widely used copula classes, the Gaussian copula and the max-stable copula.},
   author = {Adrian Casey and Ioannis Papastathopoulos},
   journal = {arXiv},
   month = {2},
   title = {Decomposable Tail Graphical Models},
   volume = {2302.05182},
   url = {http://arxiv.org/abs/2302.05182},
   year = {2023},
}
@article{Caires2005,
   abstract = {We prove the strong consistency of estimators of the conditional distribution function and conditional expectation of a future observation of a discrete time stochastic process given a fixed number of past observations. The results apply to conditionally stationary processes (a class of processes including Markov and stationary processes) satisfying a strong mixing condition, and they extend and bring together the work of several authors in the area of non-parametric estimation. One of our goals is to provide further justification for the growing practical application of non-parametric estimators in non-stationary time series and in other 'non-i.i.d.' settings. Some arguments as to why such estimators should work very generally in practice, often in a nearly 'optimal' way, are given. Two numerical illustrations are included, one with simulated data and the other with oceanographic data.},
   author = {S. Caires and J. A. Ferreira},
   doi = {10.1007/s11203-004-0383-2},
   issn = {1387-0874},
   issue = {2},
   journal = {Statistical Inference for Stochastic Processes},
   keywords = {conditional distribution function,conditional expectation,data analysis,non-parametric prediction,time series},
   month = {9},
   pages = {151-184},
   title = {On the Non-parametric Prediction of Conditionally Stationary Sequences},
   volume = {8},
   url = {http://link.springer.com/10.1007/s11203-004-0383-2},
   year = {2005},
}
@article{Drees2022,
   abstract = {We analyze the extreme value dependence of independent, not necessarily identically distributed multivariate regularly varying random vectors. More specifically, we propose estimators of the spectral measure locally at some time point and of the spectral measures integrated over time. The uniform asymptotic normality of these estimators is proved under suitable nonparametric smoothness and regularity assumptions. We then use the process convergence of the integrated spectral measure to devise consistent tests for the null hypothesis that the spectral measure does not change over time. The finite sample performance of these tests is investigated in Monte Carlo simulations.},
   author = {Holger Drees},
   month = {1},
   title = {Statistical Inference on a Changing Extremal Dependence Structure},
   url = {http://arxiv.org/abs/2201.06389},
   year = {2022},
}
@article{Nutt2010,
   abstract = {Background Proper assessment of the harms caused by the misuse of drugs can inform policy makers in health, policing, and social care. We aimed to apply multicriteria decision analysis (MCDA) modelling to a range of drug harms in the UK.},
   author = {David J Nutt and Leslie A King and Lawrence D Phillips},
   doi = {10.1016/S0140-6736(10)61462-6},
   issn = {01406736},
   issue = {9752},
   journal = {The Lancet},
   month = {11},
   pages = {1558-1565},
   title = {Drug harms in the UK: a multicriteria decision analysis},
   volume = {376},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673610614626},
   year = {2010},
}
@article{Murphy‐Barltrop2023,
   author = {C. J. R. Murphy‐Barltrop and J. L. Wadsworth and E. F. Eastoe},
   doi = {10.1002/env.2797},
   issn = {1180-4009},
   journal = {Environmetrics},
   month = {2},
   title = {New estimation methods for extremal bivariate return curves},
   url = {https://onlinelibrary.wiley.com/doi/10.1002/env.2797},
   year = {2023},
}
@article{Bernard2013,
   abstract = {One of the main objectives of statistical climatology is to extract relevant information hidden in complex spatial-temporal climatological datasets. To identify spatial patterns, most well-known statistical techniques are based on the concept of intra- and intercluster variances (like the k-means algorithm or EOFs). As analyzing quantitative extremes like heavy rainfall has become more and more prevalent for climatologists and hydrologists during these last decades, finding spatial patterns with methods based on deviations from the mean (i.e., variances) may not be the most appropriate strategy in this context of studying such extremes. For practitioners, simple and fast clustering tools tailored for extremes have been lacking. A possible avenue to bridging this methodological gap resides in taking advantage of multivariate extreme value theory, a welldeveloped research field in probability, and to adapt it to the context of spatial clustering. In this paper, a novel algorithm based on this plan is proposed and studied. The approach is compared and discussed with respect to the classical k-means algorithm throughout the analysis of weekly maxima of hourly precipitation recorded in France (fall season, 92 stations, 1993-2011). © 2013 American Meteorological Society.},
   author = {Elsa Bernard and Philippe Naveau and Mathieu Vrac and Olivier Mestre},
   doi = {10.1175/JCLI-D-12-00836.1},
   issn = {08948755},
   issue = {20},
   journal = {Journal of Climate},
   pages = {7929-7937},
   title = {Clustering of maxima: Spatial dependencies among heavy rainfall in france},
   volume = {26},
   year = {2013},
}
@article{,
   abstract = {Background: The present paper describes the results of a rating study performed by a group of European Union (EU) drug experts using the multicriteria decision analysis model for evaluating drug harms. Methods: Forty drug experts from throughout the EU scored 20 drugs on 16 harm criteria. The expert group also assessed criteria weights that would apply, on average, across the EU. Weighted averages of the scores provided a single, overall weighted harm score (range: 0-100) for each drug. Results: Alcohol, heroin and crack emerged as the most harmful drugs (overall weighted harm score 72, 55 and 50, respectively). The remaining drugs had an overall weighted harm score of 38 or less, making them much less harmful than alcohol. The overall weighted harm scores of the EU experts correlated well with those previously given by the UK panel. Conclusion: The outcome of this study shows that the previous national rankings based on the relative harms of different drugs are endorsed throughout the EU. The results indicates that EU and national drug policy measures should focus on drugs with the highest overall harm, including alcohol and tobacco, whereas drugs such as cannabis and ecstasy should be given lower priority including a lower legal classification.},
   author = {Jan Van Amsterdam and David Nutt and Lawrence Phillips and Wim Van Den Brink},
   doi = {10.1177/0269881115581980},
   issn = {14617285},
   issue = {6},
   journal = {Journal of Psychopharmacology},
   keywords = {Illicit drugs,alcohol,ranking,recreational drugs,risk assessment,tobacco},
   month = {6},
   pages = {655-660},
   pmid = {25922421},
   publisher = {SAGE Publications Ltd},
   title = {European rating of drug harms},
   volume = {29},
   year = {2015},
}
@book{Joe1997,
   author = {Harry Joe},
   doi = {10.1201/9780367803896},
   isbn = {9780367803896},
   month = {5},
   publisher = {Chapman and Hall/CRC},
   title = {Multivariate Models and Multivariate Dependence Concepts},
   year = {1997},
}
