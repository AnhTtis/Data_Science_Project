% \section{Mathematical Notations}



\section{Experimental and Implementation Details}

\begin{table}[!h]
\centering
\caption{The size of different splits for each classification data set.}
\label{tab:data_details}
\begin{tabular}{|c|c|c|c|c|}
\hline
Dataset & Calibration data & Scaling data & Validation data & Testing data\\ 
 & &  & &\\ \hline
ImageNet & 5000 & 5000 & 15000 & 25000\\ \hline
CIFAR100 & 3000 & 1000 & 3000 & 3000\\ \hline
CIFAR10 & 3000 & 1000 & 3000 & 3000\\ \hline

\end{tabular}
%\vspace{-1.0ex}
\end{table}






\section{Neighborhood Conformal Prediction (NCP) vs. Conformal Prediction (CP)}

\vspace{2.0ex}

\subsection{Proof of Theorem \ref{theorem:improved_LCP_over_CP} }
\label{section:proof_improved_LCP_over_CP}



Before proving Theorem \ref{theorem:improved_LCP_over_CP}, we present the following technical lemma.
\begin{lemma}
\label{lemma:decreased_quantile}
(Increase of quantile in decreasing probability function of threshold $t$)
Suppose $G_0, G_1$ are monotonically non-decreasing functions.
Given $t$, if $G_0(t) \geq G_1(t)$ and $\alpha \in (0, 1)$,
we have
\begin{align*}
\min\{ t : G_0(t) \geq 1 - \alpha \}
\leq
\min\{ t : G_1(t) \geq 1 - \alpha \}.
\end{align*}
\end{lemma}

\begin{proof}
(of Lemma \ref{lemma:decreased_quantile})

\begin{align*}
\min\{ t : G_0(t) \geq 1 - \alpha \}
= &
\min\{ t : G_1(t) \geq 1 - \alpha - \underbrace{ ( G_0(t) - G_1(t) ) }_{ \geq 0 } \}
\\
\leq &
\min\{ t : G_1(t) \geq 1 - \alpha \},
\end{align*}
where the inequality is due to the monotonic increase of $G_1$ for achieving a larger minimum value, from $1-\alpha - (G_0(t)-G_1(t))$ to $1-\alpha$.

\end{proof}

Lemma \ref{lemma:decreased_quantile} is simple and used very frequently in the following analysis. 
The main functionality of Lemma \ref{lemma:decreased_quantile} is to track the increase of quantile (the threshold found by taking $\min$ as shown above), as the decrease of probability function, which is generalized by $G_0(t)$ and $G_1(t)$.







\begin{proof} (Proof of Theorem \ref{theorem:improved_LCP_over_CP}, improved efficiency of NCP over CP)

In this proof, we use the notation narrowed down in Section 4 for the multi-class classification setting.
We begin with an artificial conformal prediction algorithm that is locally built in a class-wise manner, which is named by class-wise NCP and is different from NCP that locally builds quantile on each data sample.
Subsequently, we use this new algorithm as a proxy to build the connection between NCP and CP.
Specifically, we build expected quantile functions of NCP, class-wise NCP, and CP in population, respectively, all of which take the desired significance level $\alpha$ as input.
If we fix the desired significance level for all algorithms and derive the expected quantile values on the entire data distribution, then it is easy to compare and identify under what conditions, NCP can be more efficient (i.e., smaller prediction set/interval size) than CP.



Recall that NCP algorithm aims to find the following $\tilde \alpha$ given $\alpha$:
\begin{align*}
\alpha^\NCP(\alpha)
\triangleq
\max\{ \tilde \alpha : \P_{X} \{ X \leq Q^\NCP(\tilde \alpha; X) \} \geq 1 - \alpha \} ,
\end{align*}
where the quantile for $X$ with significance $\tilde \alpha$ is defined as follows.
\begin{align*}
Q^\NCP(\tilde \alpha; X)
\triangleq
\min\{ t : \underbrace{ \P_{X'} \{ V(X', F^*(X')) \leq t, X' \in \calN_\calB(X) \} }_{ \triangleq G^\NCP(t; X) } \geq 1 - \tilde \alpha \}
\end{align*}
is the quantile of $X$ derived from its neighborhood region $\calN_\calB$.
Note that the above definitions are in population, which is different from the empirical definition in (\ref{eq:LCP_tilde_alpha}).
Above $G^\NCP(t; X)$ is the coverage probability function given a threshold $t$ (like the function in Lemma \ref{lemma:decreased_quantile}).






Given $t$, for $G(t; X)$ and $F^*(X) = c \in [C]$, we have the following development:

\begin{align}\label{eq:probability_LCP_to_classLCP}
G^\NCP(t; X) =
&
\P_{X'} \{ V(X') \leq t, X' \in \calN_\calB(X) \}
\nonumber\\
= &
\P_{X} \{ X \in \calR_\calB^* \cap \calX_c \} \cdot \P_{X'} \{ V(X', F^*(X') ) \leq t, X' \in \calN_\calB(X) | X \in \calR_\calB^* \cap \calX_c \}
\nonumber\\
&
+ \underbrace{
\P_{X} \{ X \notin \calR_\calB^* \cap \calX_c \} \cdot \P_{X'} \{ V(X', F^*(X')) \leq t, X' \in \calN_\calB(X) | X \notin \calR_\calB^* \cap \calX_c \}
}_{ \geq 0 }
\nonumber\\
\stackrel{(a)}{\geq} &
\P_{X} \{ X \in \calR_\calB^* \cap \calX_c \} \cdot \P_{X'} \{ V(X', F^*(X')) \leq t, X' \in \calN_\calB(X) | X \in \calR_\calB^* \cap \calX_c \}
\nonumber\\
\stackrel{(b)}{\geq} &
( 1 - \mu_\calB ) \cdot \P_{X'} \{ V(X', F^*(X')) \leq t, X' \in \calN_\calB(X) | X \in \calR_\calB^* \cap \calX_c \}
\nonumber\\
\stackrel{(c)}{\geq} &
\underbrace{ ( 1 - \mu_\calB ) \cdot \sigma }_{ \triangleq \hat \sigma } \cdot \underbrace{ \P_{X'} \{ V(X', F^*(X')) \leq t | F^*(X') = c \} }_{ \triangleq G^\NCP_\class(t; c) },
\end{align}


where the above inequality $(a)$ is due to $\P\{ \cdot \} \geq 0$ for any event.
Inequality $(b)$ is due to $\mu_\calB$-separation of $\calD_\calX$ assumed in Assumption \ref{assumption:LCP}.
Inequality $(c)$ is due to $\sigma$-concentration of any quantile on $\calR_\calB^*$ assumed in Assumption \ref{assumption:LCP}.




As mentioned in Section 4, this concentration condition shows that data samples satisfying the certain quantile $t$ (i.e., for $X$ such that $V(X, F^*(X)) \leq t$) are significantly densely distributed on the robust set $\calR_\calB^*$ (or its class-wise partition $\calR_\calB^* \cap \calX_c$ for the class $c$).
This significance of dense distribution is captured by the condition number $\sigma$, which is explicitly shown as follows
\begin{align*}
\sigma 
\leq &
\frac{ \P_{X'} \{ V(X', F^*(X')) \leq t, X' \in \calN_\calB(X) | X \in \calR_\calB^* \cap \calX_c \} }{ \P_{X'} \{ V(X', F^*(X')) \leq t | F^*(X') = c \} }.
\end{align*}


Now we can design an artificial conformal prediction algorithm, i.e., {\it class-wise NCP}, to solve the following problem:
\begin{align*}
\alpha^\NCP_\class(\alpha)
\triangleq &
\min\{ \tilde \alpha : \P_{X} \{ X \leq Q_\class^\NCP(\tilde \alpha; F^*(X)) \} \geq 1 - \alpha \},
\end{align*}
where the {\it class-wise quantile} $Q_\class^\NCP(\tilde\alpha; c)$ for the class $c$ can be determined as follows
\begin{align}\label{eq:quantile_classLCP}
Q^\NCP_\class(\tilde \alpha; c)
\triangleq
\min\{ t : \underbrace{ \P_{X'}\{ V(X', F^*(X')) \leq t | F^*(X') = c \} }_{ = G^\NCP_\class(t; c) } \geq 1 - \tilde \alpha \}.
\end{align}


In the following three steps, we use this artificial class-wise NCP algorithm as a proxy to build connection between NCP and CP, respectively, from which we show the condition that makes NCP more efficient than CP.


{\bf Step 1. Connection between NCP and class-wise NCP}

In the first step, we show 
from inequalities in (\ref{eq:probability_LCP_to_classLCP}), we have
\begin{align*}
G^\NCP(t; X)
\geq
\underbrace{ (1-\mu_\calB) \sigma }_{ = \hat \sigma } \cdot G_\class^\NCP(t; F^*(X))
\end{align*}



Therefore, by Lemma \ref{lemma:decreased_quantile}, we have the following relation between the quantiles of NCP and class-wise NCP:
\begin{align}\label{eq:quantile_LCP_to_classLCP}
Q^\NCP(\tilde \alpha; X)
= &
\min\{ t : G(t; X) \geq 1 - \tilde \alpha \}
\nonumber\\
\leq &
\min\{ t : \hat \sigma \cdot G_\class^\NCP(t; Y) \geq 1 - \tilde \alpha \}
\nonumber\\
= &
\min\{ t : G_\class^\NCP(t; Y) \geq \frac{ 1 - \tilde \alpha }{ \hat \sigma } \}
\nonumber\\
= &
Q_\class^\NCP \Big( \frac{ \hat \sigma - 1 + \tilde \alpha }{ \hat \sigma } ; F^*(X) \Big) ,
\end{align}
where the last equality is due to 
\begin{align*}
\frac{ 1 - \tilde \alpha }{ \hat \sigma }
=
\frac{ \hat \sigma  - \hat \sigma + 1 - \tilde \alpha }{ \hat \sigma }
=
1 - \frac{ \hat \sigma - 1 + \tilde \alpha }{ \hat \sigma }.
\end{align*}

Note that 
\begin{align*}
\frac{ \hat \sigma - 1 + \tilde \alpha }{ \hat \sigma } - \tilde \alpha
= 
\frac{\hat \sigma - 1}{\hat \sigma} + \tilde \alpha \Big( \frac{1}{\hat \sigma} - 1 \Big)
=
1 - \frac{1}{\hat \sigma} + \tilde \alpha \Big( \frac{1}{\hat \sigma} - 1 \Big)
=
\Big( 1 - \frac{1}{\hat \sigma} \Big) ( 1 - \tilde \alpha )
\geq 0,
\end{align*}
so we have $\frac{ \hat \sigma - 1 + \tilde \alpha }{ \hat \sigma } \geq \tilde \alpha$.
This shows that, to achieve more certain prediction, i.e., smaller significance level $\tilde \alpha$, NCP requires smaller quantile than class-wise NCP, as shown in (\ref{eq:quantile_LCP_to_classLCP}).



Before we finish this step, we can have a further investigation of the expected quantile determined by NCP and its link to class-wise NCP in (\ref{eq:quantile_LCP_to_classLCP}) as follows.
\begin{align}\label{eq:class_conditional_quantile_LCP_to_classLCP}
\bar Q^\NCP(\tilde \alpha)
\triangleq
\sum_{c=1}^C \P\{ X \in \calX_c \} \cdot Q^\NCP(\tilde \alpha; X | X \in \calX_c )
\nonumber\\
\leq 
\sum_{c=1}^C \P\{ X \in \calX_c \} \cdot Q^\NCP_\class\Big( \frac{ \hat\sigma - 1 + \tilde \alpha }{ \hat \sigma } ; c \Big) .
\end{align}













{\bf Step 2. Connection between class-wise NCP and CP}

In this step, we show that $\alpha^\NCP_\class(\alpha) \leq \alpha$, which implies that to achieve the same significance level with CP, class-wise NCP can simply find the quantiles using $\alpha$ directly on each class.
To show this equality, we have
\begin{align*}
\alpha^\NCP_\class(\alpha)
= &
\max\{ \tilde \alpha : \sum_{c=1}^C \P_{X}\{ F^*(X) = c \} \cdot \P_{ X }\{ X \leq Q_\class^\NCP(\tilde \alpha; c) | F^*(X) = c\} \geq 1 - \alpha \}
\\
\leq &
\max\{ \tilde \alpha : \sum_{c=1}^C \P_{X}\{ F^*(X) = c \} \cdot ( 1 - \tilde \alpha ) \geq 1 - \alpha \}
\\
= &
\max\{ ( \tilde \alpha : 1 \cdot ( 1 - \tilde \alpha ) \geq 1 - \alpha \}
= \alpha .
\end{align*}
The above result states that to achieve $\alpha$ mis-coverage probability, class-wise NCP requires to find the quantile to give $\alpha$ mis-coverage for each of the classes, by which the final coverage probability can be lower bounded by $1-\alpha$.
However, only deriving the above significance level is not sufficient to investigate the efficiency of class-wise NCP.
Below, we try to lower bound the coverage probability function of class-wise NCP with CP quantile replacing class-wise NCP quantile.


We now use a somewhat similar idea as in (\ref{eq:probability_LCP_to_classLCP}), where we derive the coverage probability from NCP by using the presentation of class-wise NCP.
We make use of the assumptions of the robust set on the data sample distribution and the alignment between the quantile of this distribution property.
To build connection from class-wise NCP to CP, we now treat classes in a similar way: we group all classes into two categories, i.e., the robust class set and non-robust class set.



Specifically, let $R_\class(\tilde \alpha) = \{ c \in [C] : Q_\class^\NCP(\tilde \alpha; c) \leq Q^\CP(\tilde \alpha) \}$ denote the robust set of class labels, for which the quantile determined by class-wise NCP is smaller than that determined by CP given the same $\tilde \alpha$.
As a result, we can verify that
\begin{align*}
&
\P_X \{ V(X, F^*(X)) \leq Q_\class^\NCP(\tilde \alpha; F^*(X)) | F^*(X) \notin R_\class \}
\\
\geq &
\P_X \{ V(X, F^*(X)) \leq Q^\CP(\tilde \alpha) | F^*(X) \notin R_\class \}
\geq 
1 - \tilde \alpha .
\end{align*}
For the other group of classes, we can consider the worst-case, i.e., the minimum class distribution probability $P_\class^{min}$, to bound.
It is easy to see that $| R_\class(\tilde \alpha)| \geq 1$.


We now provide details of the relation of actual coverage probability for using different quantiles determined by class-wise NCP and CP with the same significance level $\tilde \alpha$.
For the class conditional coverage probability for class $c$, we can plug $Q_\class^\NCP(\tilde \alpha; c)$ in (\ref{eq:quantile_classLCP}) into $G_\class^\NCP(t; c)$ and replace $t$ as follows.
\begin{align*}
&
G_\class^\NCP( Q_\class^\NCP(\tilde \alpha; c) ; c )
\\
= &
\P_X \{ V(X, F^*(X)) \leq Q_\class^\NCP(\tilde \alpha; c) | F^*(X) = c \}
\\
= &
\P\{ c \in R_\class(\tilde \alpha) \} \cdot \P_X \{ V(X, F^*(X)) \leq Q_\class^\NCP(\tilde \alpha; c) | F^*(X) = c \in R_\class(\tilde \alpha) \}
\\
&
+ \P\{ c \notin R_\class(\tilde \alpha) \} \cdot \P_X \{ V(X, F^*(X)) \leq Q_\class^\NCP(\tilde \alpha; c) | F^*(X) = c \notin R_\class(\tilde \alpha) \}
\\
\stackrel{(a)}{\geq} & 
\P\{ c \in R_\class(\tilde \alpha) \} \cdot \P_X \{ V(X, F^*(X)) \leq Q_\class^\NCP(\tilde \alpha; c) | F^*(X) = c \in R_\class(\tilde \alpha) \}
\\
&
+ \P\{ c \notin R_\class(\tilde \alpha) \} \cdot \P_X \{ V(X, F^*(X)) \leq Q^\CP(\tilde \alpha) | F^*(X) = c \notin R_\class(\tilde \alpha) \}
\\
= &
\P_X \{ V(X, F^*(X)) \leq Q^\CP(\tilde \alpha) | F^*(X) = c \}
\\
&
+ \P\{ c \in R_\class(\tilde \alpha) \} \cdot 
\underbrace{
\P_X \{ V(X, F^*(X)) \leq Q_\class^\NCP(\tilde \alpha; c) | F^*(X) = c \in R_\class(\tilde \alpha) \}
}_{ \geq 1-\tilde \alpha }
\\
&
- \P\{ c \in R_\class(\tilde \alpha) \} \cdot \P_X \{ V(X, F^*(X)) \leq Q^\CP(\tilde \alpha) | F^*(X) = c \in R_\class(\tilde \alpha) \}
\\
\stackrel{(b)}{\geq} &
\P_X \{ V(X, F^*(X)) \leq Q^\CP(\tilde \alpha) | F^*(X) = c \}
\\
&
+ \P\{ c \in R_\class(\tilde \alpha) \} \cdot (1 - \tilde \alpha)
\\
&
- \P\{ c \in R_\class(\tilde \alpha) \} \cdot \P_X \{ V(X, F^*(X)) \leq Q^\CP(\tilde \alpha) | F^*(X) = c \in R_\class(\tilde \alpha) \}
\\
= &
\P_X \{ V(X, F^*(X)) \leq Q^\CP(\tilde \alpha) | F^*(X) = c \}
\\
&
% + \underbrace{ 
+ \P\{ c \in R_\class(\tilde \alpha) \} \cdot \Big( 1 - \tilde \alpha 
- \underbrace{ 
\P_X \{ V(X, F^*(X)) \leq Q^\CP(\tilde \alpha) | F^*(X) = c \in R_\class(\tilde \alpha) \}
}_{ \leq 1 }
\Big)
% }_{ \triangleq G_0(\tilde \alpha; c) \leq 0 }
% \\
% \geq &
% \P_X \{ V(X, F^*(X)) \leq Q^\CP(\tilde \alpha) | F^*(X) = c \}
% \\
% &
% + \Big( 1 - \tilde \alpha - \P_X \{ V(X, F^*(X)) \leq Q^\CP(\tilde \alpha) | F^*(X) = c \} \Big)
\\
\stackrel{(c)}{\geq} &
\P_X \{ V(X, F^*(X)) \leq Q^\CP(\tilde \alpha) | F^*(X) = c \}
- \tilde \alpha ( 1 - P_\class^{min} ) 
,
\end{align*}
where the first inequality $(a)$ is due to the definition of $R_\class(\tilde \alpha)$, i.e., we can replace $Q_\class^\NCP(\tilde \alpha; c)$ with $Q^\CP(\tilde \alpha)$ for $c \notin R_\class(\tilde \alpha)$.
The second inequality $(b)$ is due to 
$$\P_X\{ V(X, F^*(X)) \leq Q_\class^\NCP(\tilde \alpha; c) | F^*(X) = c \in R_\class(\tilde \alpha) \} \geq 1 - \tilde \alpha.$$
The last inequality $(c)$ is due to 
$$\P_X \{ V(X, F^*(X)) \leq Q^\CP(\tilde \alpha) | F^*(X) = c \in R_\class(\tilde \alpha) \} \leq 1 ,$$ 
and note that if $| R_\class | \neq C$ (at least one class not in $R_\class$), then
\begin{align*}
&
\P\{ c \notin R_\class(\tilde \alpha) \} \geq P_\class^{min}
\\
\Rightarrow & \\
&
\P\{ c \in R_\class(\tilde \alpha) \} \leq 1 -  P_\class^{min} .
\end{align*}


From above, after inequality $(a)$, we can view it as the class-conditional coverage of a new conformal prediction algorithm, i.e., using $Q^\NCP_\class(\tilde \alpha; c)$ for $c \in R_\class(\tilde \alpha)$ and using $Q^\CP(\tilde \alpha)$ for $c \notin R_\class(\tilde \alpha)$, which definitely has smaller expected quantile than CP across all classes.





Then we have the following full coverage probability using class conditional ones:
\begin{align*}
&
\sum_{c=1}^C \P\{ X \in \calX_c \} \cdot \underbrace{ \P_X\{ V(X, F^*(X)) \leq Q_\class^\NCP(\tilde \alpha; c) \} | F^*(X) = c }_{ = G_\class^\NCP(\tilde \alpha; c) } \}
\\
\geq &
\sum_{c=1}^C \P\{ X \in \calX_c \} \cdot \P_X\{ V(X, F^*(X)) \leq Q^\CP(\tilde \alpha) | F^*(X) = c \} 
% + \sum_{c=1}^C \P\{ X \in \calX_c\} G_0(\tilde \alpha; c)
- \tilde \alpha ( 1 - P_\class^{min} )
\\
= &
\P_X\{ V(X, F^*(X)) \leq Q^\CP(\tilde \alpha) \}
- \tilde \alpha (1 - P_\class^{min} )
\geq 
1 - \tilde \alpha - \tilde \alpha ( 1 - P_\class^{min} )
\\
= &
1 - \tilde \alpha ( 2 - P_\class^{min} ) .
\end{align*}
The above inequality shows that if using a constant quantile (determined by CP using target $\tilde \alpha$ significance level) in class-wise NCP setting, we only achieve $1 - \tilde \alpha ( 2 - P_\class^{min} )$ coverage.
To achieve the same level of $1 - \tilde \alpha$ as CP using this constant quantile, then we need to reduce the mis-coverage parameter to $\tilde \alpha / ( 2 - P_\class^{min})$ from $\tilde \alpha$.


Then we can compute the upper bound of the expected quantile of class-wise NCP as follows:
\begin{align}\label{eq:quantile_classLCP_to_CP}
\bar Q_\class^\NCP(\tilde \alpha)
\triangleq
\sum_{c=1}^C \P\{ X \in \calX_c \} \cdot Q_\class^\NCP(\tilde \alpha; c)
\leq 
\sum_{c=1}^C \P\{ X \in \calX_c \} \cdot Q^\CP (\tilde \alpha / ( 2 - P_\class^{min} ) )
=
Q^\CP(\tilde \alpha / ( 2 - P_\class^{min} ) ) .
\end{align}




{\bf Step 3: Connection between the quantiles determined by NCP and CP via class-wise NCP}

Now we have by (\ref{eq:quantile_classLCP_to_CP}):
\begin{align*}
\bar Q_\class^\NCP(\tilde \alpha)
\leq 
Q^\CP( \tilde \alpha / ( 2 - P_\class^{min} ) ),
\end{align*}
and by (\ref{eq:quantile_LCP_to_classLCP}) and (\ref{eq:class_conditional_quantile_LCP_to_classLCP}):
\begin{align*}
\bar Q^\NCP(\tilde \alpha)
\leq 
\sum_{c=1}^C \P\{ X \in \calX_c \} \cdot Q^\NCP_\class\Big( \frac{ \hat \sigma - 1 + \tilde \alpha }{ \hat \sigma } ; c \Big)
=
\bar Q_\class^\NCP\Big( \frac{ \hat \sigma - 1 + \tilde \alpha }{ \hat \sigma } \Big) .
\end{align*}


If we would like to make NCP more efficient (i.e., smaller predicted set/interval size) than CP, then the following inequality is required to hold:
\begin{align*}
\bar Q^\NCP(\tilde \alpha)
\leq 
\bar Q_\class^\NCP\Big( \frac{ \hat \sigma - 1 + \tilde \alpha }{ \hat \sigma } \Big)
\leq 
Q^\CP \Big( \frac{ \hat \sigma - 1 + \tilde \alpha }{ \hat \sigma ( 2 - P_\class^{min} ) } \Big),
\end{align*}
or equivalently by letting $\alpha = \frac{ \hat \sigma - 1 + \tilde \alpha }{ \hat \sigma ( 2 - P_\class^{min} ) }$:
\begin{align*}
\bar Q^\NCP( 1 - \hat \sigma ( 1 - ( 2 - P^{min}_\class ) \alpha ) )
\leq 
Q^\CP( \alpha ) .
\end{align*}

It suffices to make sure the mis-coverage has the following relation:
\begin{align*}
& 1 - \hat \sigma ( 1 - ( 2 - P^{min}_\class ) \alpha
\leq 
\alpha
\\
\Leftrightarrow & \\
&
1 - \alpha 
\leq 
\hat \sigma ( 1 - ( 2 - P^{min}_\class) \alpha )
\\
\Leftrightarrow & ~~(\text{due to } \alpha \leq 1 / 2 \text{ according to Assumption \ref{assumption:LCP}}) \\
&
\hat \sigma
\geq 
\frac{ 1 - \alpha }{ 1 - ( 2 - P_\class^{min}) \alpha } ,
\end{align*}
where the last inequality holds under Assumption \ref{assumption:LCP}.

\end{proof}


\subsection{Additional Experiments}

\begin{table}[!h]
\centering
\caption{Clustering coefficient for ImageNet-val data. Silhouette score can have values between -1 to 1. Higher silhouette score means better clustering property. These results justify the use of the learned representation to define the neighborhood and weighting function for NCP algorithm.}
\label{tab:cvg}

\begin{tabular}{|c|c|c|c|c|}

    \hline
     Models & silhouette score(raw data) & silhouette score(representation)\\
     \hline
     ResNetXt101&-0.379&0.076 \\
     \hline
     ResNet152&-0.510&0.052\\
     \hline
     ResNet101&-0.477&0.045\\
     \hline
     ResNet50&-0.444&0.034\\
     \hline
     ResNet18&-0.372&0.001\\
     \hline
     DenseNet161&-0.336&0.021\\
     \hline
     VGG16&-0.183&0.012\\
     \hline
     Inception&-0.483&0.060\\
     \hline
     ShuffleNet&-0.346&-0.003\\
     \hline
\end{tabular}

\end{table}

\begin{table}[!h]
\centering
\caption{Ablation results comparing NCP and NCP variant using all calibration examples as neighborhood (NCP-All). Both NCP and NCP-All use the conformity scoring function of APS. NCP reduces the mean prediction set size as shown in table \ref{tab:LCP_NCP} by using a relatively smaller neighborhood. Both NCP and NCP-All achieve $1 - \alpha$ coverage.}
\label{tab:LCP_NCP_cvg}
\begin{tabular}{|c|cc|cc|cc|}
\hline
\multicolumn{1}{|c|}{Dataset}   & \multicolumn{2}{c|}{ImageNet}             & \multicolumn{2}{c|}{CIFAR100}             & \multicolumn{2}{c|}{CIFAR10}              \\ \hline
             $1 - \alpha$                   & \multicolumn{2}{c|}{$0.900$} & \multicolumn{2}{c|}{$0.900$} & \multicolumn{2}{c|}{$ 0.960$} \\ \hline
\multicolumn{1}{|c|}{Models}     & \multicolumn{1}{c|}{NCP-All}       & NCP      & \multicolumn{1}{c|}{NCP-All}       & NCP      & \multicolumn{1}{c|}{NCP-All}       & NCP      \\ \hline
\multicolumn{1}{|c|}{ResNet18}  & \multicolumn{1}{c|}{0.905}     & 0.901    & \multicolumn{1}{c|}{0.928}     & 0.908    & \multicolumn{1}{c|}{0.980}     & 0.962    \\ \hline
\multicolumn{1}{|c|}{ResNet50}  & \multicolumn{1}{c|}{0.921}     & 0.902    & \multicolumn{1}{c|}{0.939}     & 0.906    & \multicolumn{1}{c|}{0.982}     & 0.965    \\ \hline
\multicolumn{1}{|c|}{ResNet101} & \multicolumn{1}{c|}{0.920}     & 0.903    & \multicolumn{1}{c|}{0.936}     & 0.907    & \multicolumn{1}{c|}{0.983}     & 0.966    \\ \hline
\end{tabular}
\end{table}



\begin{table}[!h]
\centering
\caption{The average percentage of calibration examples used by \textbf{NCP(APS)} as nearest neighbors  to produce prediction sets. RN denotes ResNet.}
\label{tab:K_fraction}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}

\hline
Model->     & RN152 & RN101 &  RN50 & RN18 & ResNeXt101 & DenseNet161 & VGG16 & Inception & ShuffleNet\\ \hline
ImageNet  &  28.0 & 33.2 & 12.8 & 24.8 & 30.0 & 15.6 & 46.0 & 30.0 & 24.0\\ \hline
CIFAR100  & - & 12.0 & 08.3 & 12.3 & -  & - & - & - & -  \\ \hline
CIFAR10 &  - & 13.0 & 15.0 & 09.0 & - & - & - & - & -  \\ \hline
\end{tabular}
\end{table}


\begin{figure}[!h]
    \centering
    \includegraphics[scale = 0.4]{figures/RN18.png}
    \caption{Results showing coverage and prediction set size of d prediction set size of APS, NCP(APS), and NCP(APS)-All for classification as a function of the localization parameter for ResNet18 model on CIFAR100 dataset. A very high value of localization parameter degenerates to APS.}
    \label{fig:classi_loc_RN18}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[scale = 0.4]{figures/RN50.png}
    \caption{Results showing coverage and prediction set size of d prediction set size of APS, NCP(APS), and NCP(APS)-All for classification as a function of the localization parameter for ResNet50 model on CIFAR100 dataset. A very high value of localization parameter degenerates to APS.}
    \label{fig:classi_loc_RN50}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[scale = 0.4]{figures/RN101.png}
    \caption{Results showing coverage and prediction set size of APS, NCP(APS), and NCP(APS)-All for classification as a function of the localization parameter for ResNet101 model on CIFAR100 dataset. A very high value of localization parameter degenerates to APS.}
    \label{fig:classi_loc_RN101}
\end{figure}




\begin{table}[]
\centering
\begin{tabular}{|ccccc|}
\hline
\multicolumn{1}{|c|}{Model}       & \multicolumn{1}{c|}{Inference} & \multicolumn{1}{c|}{RAPS} & \multicolumn{1}{c|}{NCP(RAPS)} & NCP(RAPS) with LSH\\ \hline
\multicolumn{5}{|c|}{ImageNet}                                                                                                                                                      \\ \hline
\multicolumn{1}{|c|}{ResNet152}   & \multicolumn{1}{c|}{1.215}        & \multicolumn{1}{c|}{1.043}        & \multicolumn{1}{c|}{1.154}             & 1.046                          \\ \hline
\multicolumn{1}{|c|}{ResNet101}   & \multicolumn{1}{c|}{1.106}        & \multicolumn{1}{c|}{1.022}        & \multicolumn{1}{c|}{1.188}             & 1.037                          \\ \hline
\multicolumn{1}{|c|}{ResNet50}    & \multicolumn{1}{c|}{1.013}        & \multicolumn{1}{c|}{1.062}        & \multicolumn{1}{c|}{1.162}             & 1.071                          \\ \hline
\multicolumn{1}{|c|}{ResNet18}    & \multicolumn{1}{c|}{0.963}        & \multicolumn{1}{c|}{1.019}        & \multicolumn{1}{c|}{1.171}             & 1.028                          \\ \hline
\multicolumn{1}{|c|}{ResNeXt101}  & \multicolumn{1}{c|}{1.748}        & \multicolumn{1}{c|}{1.070}        & \multicolumn{1}{c|}{1.121}             & 1.071                          \\ \hline
\multicolumn{1}{|c|}{DenseNet161} & \multicolumn{1}{c|}{1.690}        & \multicolumn{1}{c|}{1.030}        & \multicolumn{1}{c|}{1.065}             & 1.044                          \\ \hline
\multicolumn{1}{|c|}{VGG16}       & \multicolumn{1}{c|}{1.109}        & \multicolumn{1}{c|}{1.027}        & \multicolumn{1}{c|}{1.153}             & 1.046                          \\ \hline
\multicolumn{1}{|c|}{Inception}   & \multicolumn{1}{c|}{1.434}        & \multicolumn{1}{c|}{1.050}        & \multicolumn{1}{c|}{1.178}             & 1.051                          \\ \hline
\multicolumn{1}{|c|}{ShuffleNet}  & \multicolumn{1}{c|}{0.914}        & \multicolumn{1}{c|}{1.042}        & \multicolumn{1}{c|}{1.101}             & 1.043                          \\ \hline
\multicolumn{5}{|c|}{CIFAR100}                                                                                                                                                      \\ \hline
\multicolumn{1}{|c|}{ResNet18}    & \multicolumn{1}{c|}{0.039}        & \multicolumn{1}{c|}{0.331}        & \multicolumn{1}{c|}{0.388}             & 0.332                          \\ \hline
\multicolumn{1}{|c|}{ResNet50}    & \multicolumn{1}{c|}{0.143}        & \multicolumn{1}{c|}{0.333}        & \multicolumn{1}{c|}{0.367}             & 0.336                          \\ \hline
\multicolumn{1}{|c|}{ResNet101}   & \multicolumn{1}{c|}{0.248}        & \multicolumn{1}{c|}{0.330}        & \multicolumn{1}{c|}{0.361}             & 0.331                          \\ \hline
\multicolumn{5}{|c|}{CIFAR10}                                                                                                                                                       \\ \hline
\multicolumn{1}{|c|}{ResNet18}    & \multicolumn{1}{c|}{0.049}        & \multicolumn{1}{c|}{0.291}        & \multicolumn{1}{c|}{0.311}             & 0.292                          \\ \hline
\multicolumn{1}{|c|}{ResNet50}    & \multicolumn{1}{c|}{0.119}        & \multicolumn{1}{c|}{0.295}        & \multicolumn{1}{c|}{0.311}             & 0.298                          \\ \hline
\multicolumn{1}{|c|}{ResNet101}   & \multicolumn{1}{c|}{0.162}        & \multicolumn{1}{c|}{0.298}        & \multicolumn{1}{c|}{0.313}             & 0.311                          \\ \hline
\end{tabular}
\caption{Runtime comparison (in seconds) during testing time for using different settings. The inference time is shown to provide a frame of reference for the additional CP runtime overhead. We show two variants of NCP algorithm: NCP with standard KNN search algorithm and NCP using Locality Sensitive Hashing (LSH) based KNN search.}
\label{Time_comparison}
\end{table}





