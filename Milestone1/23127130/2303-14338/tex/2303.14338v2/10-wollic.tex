% !TEX root = 00-wollic.tex

%\input{10-wollic}
Logic as the theory of theories was originally developed to prove true statements. Here we study developments in the opposite direction: modifying interpretations to make true some previously false statements. In modal logic, such logical processes have been modeled as instances of \emph{belief update}\/ \cite{Baltag-Sadrzadeh-Coecke:DEL,Baltag-Moss:logic,Ditmarsch:DEL}. In the practice of science, such processes arise when theories are updated to explain new observations \cite[Ch.~4]{Osherson:sci-inquiry}. In public life, the goal of such processes is to influence some public perceptions to better suit some private preferences \cite[Part~V]{Easley-Kleinberg:book}. This range of applications gave rise to a gamut of techniques for  influence and belief engineering, covering the space from unsupervised learning to conditioning. 

\paragraph{From incomplete theories to complete beliefs.} The idea to incrementally complete incomplete theories \cite{DavisM:undecidable} arose soon after G\"odel proved his Incompleteness Theorem \cite{GoedelK:ueber}. Alan Turing wrote a  thesis about ordinal towers of completions and discovered the hierarchy of unsolvability degrees \cite{TuringA:thesis}. The core idea was to keep recognizing and adding true but unprovable statements to theories. In the meantime, interests shifted from making true statements provable to making false statements true. Many toy examples of belief updates and revisions have been formalized and studied in dynamic-epistemic logic \cite{Baltag-Sadrzadeh:DEL,BenthemJ:interaction}, but the advances in belief engineering and the resulting industry of influence overtook the theory at great speed, and turned several corners of market and political monetizations. The theory remained   fragmented even on its own. While modal presentations of G\"odel's theorems appeared early on \cite{RosserB:extensions}, the computational ideas, that made his self-referential constructions  possible \cite{SmorynskiC:self-reference}, never transpired back into modal logic. \emph{The point of the present paper is that combining belief updates with universal languages and self-reference leads to a curious new logical capability, whereby theories and models can be steered to assure consistency and completeness of future updates.} This capability precludes disproving current beliefs and the framework becomes \emph{belief-complete}\/ in a suitable formal sense, discussed below.

The logical framework combining belief updates and universal languages may seem unfamiliar. The main body of this paper is devoted to an attempt to describe how it arises from familiar logical frameworks. Here we try to clarify the underlying ideas. 

\paragraph{Universality.} Just like G\"odel's incompleteness theorems, our constructions of unfalsifiable beliefs are based on a \emph{universal language $\DP$}. The abstract characterization of universality, which we borrow from \cite[Ch.~2]{PavlovicD:MonCom}, is that $\DP$ comes equipped with a family of \emph{interpreters}\/ $\universal \colon \DP\times A\to B$, one for each pair of types\footnote{Each pair carries a different interpreter $\universal^{AB}$ but we elide the superscripts.} $A,B$, such that every function  $f\colon A\to B$ has a description\footnote{There may be many descriptions for each $f$ and $\enco f$ refers to an arbitrary one.} $\enco f$ in $\DP$, satisfying\footnote{The curly bracket notation allows abbreviating $\lambda a. \universal\left(p, a\right)$ to $\uev p$.} 
\bea \label{eq:univ}
f & = &  \uev{\enco f}
\eea  
This is spelled out in Sec.~\ref{Sec:moncom}. The construction in Sec.~\ref{Sec:self-fulfill} will imply that every $g\colon \DP\times A\to B$ has a fixpoint $\Gamma$, satisfying 
\bea\label{eq:fixpoint}
g(\Gamma,a) & = & \uev\Gamma (a)
\eea
Any complete programming language can be used as $\DP$. Its  interpreters support  \eqref{eq:univ} and its specializers induce \eqref{eq:fixpoint}. A sufficiently expressive software specification framework  \cite{PavlovicD:ASE01} would also fit the bill, as would a general scientific formalism \cite{Osherson:sci-inquiry}. 

\paragraph{G\"odel's incompleteness: true but unprovable statement.} G\"odel used the set of natural numbers $\NNn$ as $\LLl$, with arithmetic making it into a programming language. The concept of a programming language did not yet exist, but it came into existence through G\"odel's construction. An arithmetic expression specifying a function $f$ was encoded as a number $\enco f$ and decoded by an arithmetic function $\universal\colon\NNn\times \NNn\to \NNn$ as in \eqref{eq:univ}. A restriction of \eqref{eq:fixpoint} was proved for arithmetic predicates $p\colon \NNn\to \Bool$, where $\Bool =\{0,1\}\subset \NNn$, and a fixpoint of a predicate $g\colon \DP\times A\to \Bool$ was constructed as a predicate encoding $\enco\gamma$ satisfying\footnote{Although this discussion is semi-formal, it may be helpful to bear in mind that the equality $\{\enco\gamma\} (a) = \gamma(a)$ is \emph{extensional}: it just says that interpreting the description $\enco\gamma$ on a value $a$ always outputs the value $\gamma(a)$. But the process whereby $\{\enco\gamma\} (a)$ arrives at this value may be different from a given direct evaluation of $\gamma(a)$.}
\bea\label{eq:fixpred}
g\left(\enco\gamma,a\right) & = & \uev{\enco\gamma} (a)\ =\ \gamma(a)
\eea
To complete the incompleteness proof,  G\"odel constructed a predicate $\Prov\colon \NNn\to \Bool$ characterizing provability in formal arithmetic:
\bea\label{eq:provability}
\Prov\left(\enco p,a\right) \ \ &\iff &\ \ \vdash p(a)  
\eea
for all arithmetic predicates $p:\NNn\to \Bool$. Although proofs may be arbitrarily large, they are always finite, and if $p(a)$ has a proof, $\Prov$ will eventually find it. On the other hand, since arithmetic predicates, like all arithmetic functions, satisfy $p = \uev{\enco p}$, we also have
\bea\label{eq:provdef}
\Prov\left(\enco p,a\right) & = &  \uev{\enco p}(a) \ =\ p(a)
\eea
Setting $g(p, a) = \Prov(\enco{\neg p}, a)$  in \eqref{eq:fixpoint} induces a fixpoint $\gamma$ with
\beq
\Prov(\enco{\neg \gamma}, a) \ \ \stackrel{\eqref{eq:fixpred}}=\ \ \uev{\enco\gamma}(a)\ \ \stackrel{\eqref{eq:provdef}}= \ \ \Prov\left(\enco \gamma, a\right) 
\eeq
But \eqref{eq:provability} then implies 
\bea
\vdash \neg \gamma(a)\ \  &\iff &\ \  \vdash \gamma(a)
\eea
which means that neither $\gamma$ nor $\neg \gamma$ can be provable. On the other hand, the disjunction $\gamma \vee \neg \gamma$ is classically true. The statement $\gamma \vee \neg \gamma$ is thus true but not provable, and arithmetic is therefore incomplete. 

\paragraph{Belief completeness: universal updating.} Remarkably, the same encoding-fixpoint conundrum (\ref{eq:univ}--\ref{eq:fixpoint}), which leads to the incompleteness of static theories, also leads to the completeness of  dynamically updated theories. Updating is presented as state dependency. The function $f$ in \eqref{eq:univ} is now in the form $f:X\times A\to X\times B$ where $X$ is the state space. It may be more intuitive to think of $f$ as a \emph{process}, since it captures state changes\footnote{In automata theory, such functions are called the \emph{Mealy machines}.}. We conveniently present it as a pair $f=\left<\sta f, \out f\right>$, where $\sta f\colon X\times A\to X$ is the \emph{next state}\/ update, whereas $\out f \colon X\times A \to B$ is an $X$-indexed family of functions $\out f_{x}\colon A\to B$. The elements of the universal language $\DP$ are now construed as  \emph{belief states}. Its universality means that every observable state $x$ from any state space $X$ is expressible as a belief. The interpreters $\uuniversal\colon \DP \times A\to \DP \times B$ are also presented as pairs $\uuniversal = \left<\sta \uuniversal, \out \uuniversal\right>$, where $\sta {\uuniversal} \colon \DP\times A\to \DP$ updates the belief states whereas $\out \uuniversal \colon \DP \times A \to B$ evaluates beliefs to functions. Just like every state $x$ in $X$ determines a function $\out f_{x}\colon A\to B$, every belief $\ell$ in $\DP$ determines a function $\out{\uev\ell}\colon A\to B$, which makes predictions based on the current belief. Generalizing the fixpoint construction \eqref{eq:fixpoint}, every process $f=\left<\sta f, \out f\right>\colon X\times A\to X\times B$ now induces an assignment $\ana f\colon X\to \DP$ of beliefs to states such that
\beq \label{eq:ana}
\sta{\uev{\ana f(x)}}  =  \ana f\left(\sta f _{x}\right)\qquad \qquad \qquad \out{\uev{\ana f (x)}} = \out f_{x}
\eeq
The construction of $\ana f$ is presented in Sec.~\ref{Sec:unfalse}. Here we propose an interpretation. The second equation says that the output component of $\uuniversal$ behaves as it did in \eqref{eq:univ}: it interprets the description $\ana f(x)$ and recovers the function $\out f_{x}$ executed by the process $f$ at the state $x$. The first equation says that the interpreter $\uuniversal$ maps the $\ana f$-description of the state $x$ to the $\ana f$-description of the updated state $\sta f_{x}$:
\beq\label{eq:simul}
\prooftree
\sta f \colon x \longmapsto \sta f_{x}
\justifies
\sta\universal \colon \ana f(x) \longmapsto \ana f\left(\sta f_{x}\right)
\endprooftree
\eeq
Any state change caused by the process $f$ is thus explained by a belief update of $\ana f$ along $\uuniversal$. Interpreting the belief states $\ana f$ by the interpreter $\uuniversal$ provides belief updates that can be construed as \emph{explanations}\/ in the language $\DP$ of any state changes in the process $f$. All that can be learned about $f$ is already expressed in $\ana f$ and all state changes that may be observed will be explained by the updates anticipated by the current belief, as indicated in \eqref{eq:simul}. The belief is complete.

\paragraph{Remark.} In coalgebra and process calculus, the universal interpreters $\uuniversal \colon \DP\times A\to \DP\times B$ would be characterized as weakly final simulators \cite{PavlovicD:MonCom3}. They are universal in the sense that the same state space $\DP$ works for all types $A,B$. See \cite[Sec.~7.2]{PavlovicD:MonCom} for details and references.

\paragraph{The logic of going dynamic.} When $\DP$ is a programming language, the interpreter $\uuniversal$ interprets programs as computable functions $A\to B$, where $A$ and $B$ are types, usually predicates that allow type checking. When $\DP$ is a language of  software specifications or scientific theories construed as beliefs about the state of the world, the interpreter $\uuniversal$ updates beliefs to explain the state changes observed in explainable  processes $X\times A\to X\times B$, where $A, B$ and $X$ are state spaces. States are usually also defined by some predicates, but their purpose is not to be easy to check but to define the state changes as semantical reassignments. This is spelled out in Sec.~\ref{Sec:state}. Dynamic reassignments of meaning bring us into the realm of \emph{dynamic logic}. If the propositions from a lattice $\TTT$ are used as assertions about the states of the world or the states of our beliefs about the world, then the dynamic changes of these assertions under the influence of events from a lattice $\EEE$ can be expressed in terms of \emph{Hoare triples}
\beq\label{eq:triple} A\uev e B\eeq
saying that the event $e\in \EEE$ after the precondition $A\in \TTT$ leads to the postcondition $B\in \TTT$. The \emph{Hoare logic}\/ of such statements was developed in the late 1960s as a method for reasoning about programs. The algebra of events $\EEE$ was generated by program expressions, whereas the propositional lattice $\TTT$ was generated by formal versions of  the comments inserted by programmers into their code, to clarify the intended meanings of blocks of code \cite{FloydR:meaning,Hoare-logic}. A triple \eqref{eq:triple} would thus correspond to a block of code $e$, a comment $A$ describing the assumed state before $e$ is executed, as its precondition, and a comment $B$ describing the guaranteed state after $e$  is executed, as its postcondition. By formalizing the \emph{``assume-guarantee''}\/ reasoning of software developers, the Hoare triples provided a stepping stone into the logic of state transitions in general. The propositional algebra of dynamic logic can be viewed as a monotone map
\[ \TTT^{o}\times \EEE\times \TTT\tto{ - \uev - -} \OOO\]
where $\OOO$ is a lattice of truth values, whereas $\TTT$ and $\EEE$ are as above, and $\TTT^{o}$ is $\TTT$ with the opposite order. If the lattice $\TTT$ is complete, then each event $e\in \EEE$ induces a Galois connection
\[ A\rtimes e \vdash B \ \ \iff\ \ A\uev e B\ \ \iff\ \ A\vdash [e]B\]
determining a \emph{dynamic modality}\/ $[e]\colon\TTT\to\TTT$ for every $e\in \EEE$ \cite{PrattV:dynamic}. The induced interior operation $\left([e]B\right)\rtimes E\vdash B$ says that $[e]B$ is the weakest precondition that guarantees $B$ after $e$. The induced closure $A\vdash [e]\left(A\rtimes e\right)$ says that $A\rtimes e$ is the strongest postcondition that can be guaranteed by the assumption $A$ before $e$. In addition to formal program annotations, dynamic logic found many other uses and interpretations \cite{BenthemJ:lambdas,Ditmarsch:DEL,Kozen:dynamic-book}. Here we use it as a backdrop for the coevolution of theories and their interpretations.

\paragraph{Updating completeness.} In static logic, a theory is complete when all statements true in a reference model are provable in the theory. In dynamic logic, the model changes dynamically and the true statements vary. There are different ways in which the notion of completeness can be generalized for dynamic situations. The notion of completeness that seems to be of greatest practical interest is the requirement that the theory and the model can be dynamically adapted to each other: the theory can be updated to make provable some true statements or the model can be updated to make true some false statements.   This requirement covers both the theory updates in science and the model updates by self-fulfilling  and belief-building announcements  in various non-sciences. The logical frameworks satisfying such completeness requirements allow for matching current beliefs and future states.