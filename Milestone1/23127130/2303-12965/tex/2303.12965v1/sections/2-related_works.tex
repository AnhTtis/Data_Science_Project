\section{Related Works}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/pipeline.fig-2.png}
    \caption{\textbf{The pipeline of EMA}. EMA jointly optimizes canonical shapes, materials, lights, and motions via efficient differentiable inverse rendering. The canonical shapes are attained firstly through the differentiable marching tetrahedra~\cite{gaoLearningDeformableTetrahedral2020,shenDeepMarchingTetrahedra2021,munkbergExtractingTriangular3D2022}, which converts SDF fields into meshes. 
    Next, it queries PBR materials, including diffuse colors, roughness, and specularity on the mesh surface.
    Meanwhile, the skinning weights and per-vertices offsets are predicted on the surface as well, which are then applied to the canonical meshes with the guide of input skeletons. 
    Finally, a rasterization-based differentiable renderer takes in the posed meshes, materials, and environment lights, and renders the final avatars efficiently.}
    \label{fig:my_label}
\end{figure*}




\noindent \textbf{Explicit Representations for Human Modeling:}
It is intuitive to model the surfaces of humans with mesh.
However, humans are highly varied in both shape and appearance and have a large pose space, which all contribute to a high dimensional space.
Researchers first model humans with limited clothes.
One of the prevalent methods is parametric models~\cite{anguelovSCAPEShapeCompletion,loperSMPLSkinnedMultiperson2015,pavlakosExpressiveBodyCapture2019,romeroEmbodiedHandsModeling2022,suANeRFArticulatedNeural2021}.
Fitting humans from scans is inapplicable in real-world applications. 
Thus,~\cite{kanazawaEndtoendRecoveryHuman2018,bogoKeepItSMPL2016,kocabasPAREPartAttention2021,zhangPyMAF3DHuman2021,zhangPyMAFXWellalignedFullbody2022,kocabasVIBEVideoInference2020,sunMonocularOnestageRegression2021} estimate the human surface from images or videos.
To model the clothed human,~\cite{prokudinSMPLpixNeuralAvatars2020,alldieckPhotorealisticMonocular3D2022,alldieckVideoBasedReconstruction2018} deform the template human vertices in canonical T-pose.
However, these methods are prone to capturing coarse geometry due to the limited capacity of the deformation layer.
Besides, the textures are modeled with sphere harmonics which are far from photo-realistic.
Our method takes the mesh as our core representation to enable efficient training and rendering and realize the topological change of shape and photo-realistic texture via neural fields.


\looseness=-1
\noindent \textbf{Implicit Representations for Human Modeling:}
Implicit representations~\cite{parkDeepSDFLearningContinuous2019,meschederOccupancyNetworksLearning2019,mildenhallNeRFRepresentingScenes2020} model the objects in a continuous manner, whose explicit entity cannot be attained directly.
Specifically, Signed Distance Function~\cite{parkDeepSDFLearningContinuous2019}, Occupancy Field~\cite{meschederOccupancyNetworksLearning2019} and Radiance Field~\cite{mildenhallNeRFRepresentingScenes2020} are all parametrized by neural networks.
Given full-body scans as 3D supervision, \cite{saitoPIFuPixelAlignedImplicit2019,saitoPIFuHDMultiLevelPixelAligned2020,heARCHAnimationReadyClothed2022,huangARCHAnimatableReconstruction2020, alldieckPhotorealisticMonocular3D2022} learned the SDFs or occupancy fields directly from images, which could predict photo-realistic human avatars in inference phrase.
\cite{pengNeuralBodyImplicit2021,suANeRFArticulatedNeural2021,liuNeuralActorNeural2022,pengAnimatableNeuralRadiance2021,liTAVATemplatefreeAnimatable2022,jiangNeuManNeuralHuman2022,chenGeometryGuidedProgressiveNeRF2022,wangARAHAnimatableVolume,zhangNDFNeuralDeformable2022,noguchiNeuralArticulatedRadiance2021,zhengStructuredLocalRadiance2022} leveraged the radiance field for more photo-realistic human avatars from multi-view images or single-view videos without any 3D supervision.
Although implicit representations improve reconstruction quality against explicit ones, they still have drawbacks, \eg, large computation burden or poor geometry.s
Besides, volume rendering is incompatible with graphics hardware, 
thus the outputs are inapplicable in downstream applications without further post-processing.
Our method absorbs the merits of implicit representations by using neural networks to predict photo-realistic textures and shape fields, leveraging~\cite{shenDeepMarchingTetrahedra2021} to convert SDFs to explicit meshes, which are fully compatible with the graphic pipeline.



\looseness=-1
\noindent \textbf{Hybrid Representations for Human Modeling:}
There are two tracks of literature modeling humans with explicit geometry representations and implicit texture representations.
One track of literature~\cite{khakhulinRealisticOneshotMeshbased2022,zhaoHighFidelityHumanAvatars2022} leveraged neural rendering techniques~\cite{thiesDeferredNeuralRendering2019}.
Meshes~\cite{prokudinSMPLpixNeuralAvatars2020,zhaoHighFidelityHumanAvatars2022,alldieckVideoBasedReconstruction2018,alldieckDetailedHumanAvatars2018} or point clouds~\cite{wuMultiViewNeuralHuman2020} are commonly chosen explicit representations. Moreover, fine-grained geometry and textures are learned by neural networks.
However, these methods are either only applicable for novel view synthesis~\cite{wuMultiViewNeuralHuman2020} or restricted to self-rotation video captures~\cite{alldieckVideoBasedReconstruction2018,alldieckDetailedHumanAvatars2018}.
Besides, the neural renderers have limitations, \eg, stitching texture~\cite{karrasStyleBasedGeneratorArchitecture2019,karrasAnalyzingImprovingImage2020}, and baked textures into the renderer.
In contrast, the human avatars learned by our method are compatible with graphics pipeline, which means they are \textbf{applicable in downstream tasks}, \eg, re-posing, editing in design software.
The other track of literature leveraged neural networks to learn both geometry and textures based on differentiable rendering~\cite{laineModularPrimitivesHighPerformance2020,chenDIBRLearningPredict2021,chenLearningPredict3D2019}.
It~\cite{liuSoftRasterizerDifferentiable2019,blanzMorphableModelSynthesis,laineModularPrimitivesHighPerformance2020,raviAccelerating3DDeep2020,lassnerPulsarEfficientSpherebased2020,rajANRArticulatedNeural2020} equips traditional graphics pipeline with the ability of error backpropagation, which make scene properties (\textit{i.e,} assets, lights, cameras poses, \textit{etc.}) optimizable through gradient descent \textit{w.r.t} photo-metric loss.
Thus, we can learn both geometry and textures that are compatible with existing graphics hardware.
However, the geometry optimization process is non-convex and highly unstable~\cite{grassalNeuralHeadAvatars2022}, so it is hard to give fine-grained geometry details.
Besides, the topology of the mesh is fixed leading to limited shape modeling.
We leverage~\cite{munkbergExtractingTriangular3D2022,shenDeepMarchingTetrahedra2021} to convert SDFs to meshes with differentiable marching tets, and model the motion dynamics of humans with additional neural fields. 
Our method enjoys flexibility from implicit representations and efficiency brought by explicit meshes, yet still maintains high-fidelity reconstructions.
