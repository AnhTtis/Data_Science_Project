
\input{tables/main-table}

\section{Experiments}

\subsection{Dataset and Metrics}


\noindent \textbf{H36M} consists of 4 multi-view cameras and uses \textbf{marker-based} motion capture to collect human poses.
Each video contains a single subject performing a complex action.
We follow~\cite{pengAnimatableNeuralRadiance2021} data protocol which includes subject S1, S5-S9, and S11.
The videos are split into two parts: ``training poses'' for novel view synthesis and ``Unseen poses'' for novel pose synthesis.
Among the video frames, 3 views are used for training, and the rest views are for evaluation.
The novel view and novel pose metrics are computed on rest views.
We use the same data preprocessing as~\cite{pengAnimatableNeuralRadiance2021}.

\noindent \textbf{ZJU-MoCap} records 9 subjects performing complex actions with 23 cameras.
The human poses are obtained with a markerless motion capture system.
Thus the pose tracking is rather noisier compared with H36M.
Likewise, there are two sets of video frames, ``training poses'' for novel view synthesis and ``Unseen poses'' for novel pose synthesis.
4 evenly distributed camera views are chosen for training, and the rest 19 views are for evaluation.
Again, the evaluation metrics are computed on rest views.
The same data protocol and processing approaches are adopted following~\cite{pengNeuralBodyImplicit2021, pengAnimatableNeuralRadiance2021}.

\noindent \textbf{Metrics}. We follow the typical protocol in ~\cite{pengNeuralBodyImplicit2021, pengAnimatableNeuralRadiance2021} using
two metrics to measure image quality: peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM).

\subsection{Evaluation and Comparison}





\noindent \textbf{Baselines}. We compare our method with template-based methods~\cite{pengNeuralBodyImplicit2021, xuSurfaceAlignedNeuralRadiance2022a} and template-free methods~\cite{pengAnimatableNeuralRadiance2021, wangARAHAnimatableVolume}. Here we list the average metric values with different training times to illustrate our very competitive performance and significant speed boost. 1) Tempelate-based methods. 
Neural Body (NB)~\cite{pengNeuralBodyImplicit2021} learns a set of latent codes anchored to a deformable template mesh to provide geometry guidance.
Surface-Aligned NeRF (SA-NeRF)~\cite{xuSurfaceAlignedNeuralRadiance2022a} proposes projecting a point onto a mesh surface to align surface points and signed height to the surface. 
2) Template-free methods.
Animatable NeRF (Ani-NeRF)~\cite{pengAnimatableNeuralRadiance2021} introduces neural blend weight fields to produce the deformation fields instead of explicit template control.
ARAH~\cite{wangARAHAnimatableVolume} combines an articulated implicit surface representation with volume rendering and proposes a novel joint root-finding algorithm.


\looseness=-1
\noindent \textbf{Comparisons with state-of-the-arts}.
Table~\ref{tab:main-table} illustrates the quantitative comparisons with previous arts.
Notably, our method achieves very competitive performance within much less training time.
The previous volume rendering-based counterparts spend tens of hours of optimization time, while our method only takes an hour of training (for previous SOTA method ARAH~\cite{wangARAHAnimatableVolume}, it takes about 2 days of training).
On the marker-based H36M dataset, our method reaches the SOTA performance in terms of novel view synthesis on training poses and outperforms previous SOTA (ARAH~\cite{wangARAHAnimatableVolume}) for novel view synthesis on novel poses, which indicates that our method can generalize better on novel poses.
The significant boost in training speeds lies in, on the one hand, the core mesh representation which can be rendered efficiently with the current graphic pipeline~\cite{laineModularPrimitivesHighPerformance2020}.
On the other hand, the triangular renderer uses less memory.
Thus we can compute losses over the whole image to learn the representations holistically.
In contrast, previous methods are limited to much fewer sampled pixels in each optimization step.

On the markerless ZJU-Mocap dataset, our method falls behind for training poses novel view synthesis and ranks 3rd place in terms of unseen poses novel view synthesis among the competitors.
We argue that the quality of pose tracking results in the performance gaps between the two datasets.
The markerless pose tracking data \textbf{are much noisier} than the marker-based ones (\eg, the tracked skeleton sequence is jittering, and the naked human~\cite{loperSMPLSkinnedMultiperson2015} rendering is misaligned with human parsings), which makes our performance saturated by harming the multi-view consistency.
The problem is even amplified with the holistic loss computation over the whole pixels.
We conduct additional ablation on pose tracking quality on H36M in Sec.~\ref{exp:data:pose_tracking_type}.
Besides, our non-rigid modeling is only over the surface (no topology change), which is less powerful than the volume rendering-based ones (with topology change).

 We further each method under \textbf{the same optimization duration} in Table~\ref{tab:main-table}.
For the extremely low inference speed of our competitor, we only evaluate at most 10 frames in each subject, and for ZJU MoCap we only choose another 4 evenly spaced cameras as the evaluation views.
For both 1 hour and 10 minutes optimization time, our method outperforms other methods for both training poses and unseen poses novel view synthesis on the marker-based H36M dataset.
On the markerless ZJU-Mocap dataset, our method is comparable with previous SOTA in terms of PSNR and SSIM for both evaluation splits.

\looseness=-1
Figure~\ref{fig:h36m} illustrates the qualitative comparisons between our method and previous arts under the same optimization duration.
It is worth noting that on both H36M and ZJU-MoCap datasets, our method can synthesize clearer and more fine-grained images against competitors, which raises the misalignment of the quantitative metrics for measuring image similarity.


\setlength{\columnsep}{6pt}
\begin{wrapfigure}{r}{0.625\linewidth}
\centering
\vspace{-1em}
\includegraphics[width=1\linewidth]{figs/inference.png}
\vspace{-2em}
\caption{\textbf{Rendering Efficiency}}
\vspace{-1em}
\label{fig:inference}
\end{wrapfigure}


\noindent \textbf{Rendering Efficiency}:
We provide the rendering speed of our method against previous methods in Figure~\ref{fig:inference}. Our method reaches real-time inference speed (100+ FPS for rendering 512Ã—512 images), which is hundreds of times faster than the previous ones. 
Our method takes considerably less memory than the previous ones.


\subsection{Ablation Studies}


\input{tables/ablation-method}

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figs/ablate-method.s9.1x6}
\caption{\textbf{Qualitative ablation on each module}. The SDF MLP improves the mesh smoothness; non-rigid modeling proves the texture quality by solving the multi-view consistency of cloth dynamics; The PBR materials have a larger capacity for modeling complex materials and lighting against the only-RGB and the no-specular counterparts, which further facilitates both mesh and material learning.}
\label{fig:ablation:method}
\end{figure}

We conduct ablation studies on the H36M S9 subject.


\noindent \textbf{The parametrization type for SDF field.}
The SDF fields can either be parameterized as either MLPs or value fields.
Table~\ref{table:ablation:method} and Figure~\ref{fig:ablation:method} show that using MLP to predict SDF values results in a smoother mesh surface that is watertight.
MLP offers extrapolation ability to predict invisible parts and keep the mesh watertight.
While directly optimizing SDF value fields leads to a jiggling mesh surface and holes in invisible parts during training (\eg, underarm).


\noindent \textbf{The shading model type in geometry module.}
We compare PBR shading models with directly predicting RGB colors and PBR without shading specular.
Table~\ref{table:ablation:method} and Figure~\ref{fig:ablation:method} show that PBR shading models lead to higher metrics against RGB predications,
which indicates that PBR materials can better model complex textures and lights for dynamic humans.
Removing the specular term in PBR does not affect the performance much. 
We conjecture that there is less specularity in human skin and clothes materials.


\noindent \textbf{The impact of the non-rigid net in motion module.}
As shown in Table~\ref{table:ablation:method} and Figures~\ref{fig:ablation:method}, modeling pose-dependent non-rigid dynamics of clothes improves the overall reconstruction quality. It facilitates the aggregation of shading information for multi-view inputs during training.

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figs/pose_track_type.pdf}
\vspace{-1em}
\caption{\textbf{Qualitative results of models trained on poses} from marker-less and marker-based systems.}
\label{fig:ablation:data:pose_track_type}
\vspace{-1em}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figs/num_views.pdf}
\vspace{-1em}
\caption{\textbf{Comparison of models trained with different numbers of camera views} on the subject ``S9".}
\label{fig:ablation:data:num_views}
\vspace{-1em}
\end{figure}

\input{tables/ablation-data}




\noindent \textbf{The impact of human tracking quality.}
\label{exp:data:pose_tracking_type}
Table~\ref{table:ablation:data}~(a) and Figure~\ref{fig:ablation:data:pose_track_type} show that using marker-based pose-tracking data can give better results. 
The same phenomenon has been stated in~\cite{pengAnimatableNeuralRadiance2021}.
Noisy marker-less pose-tracking harms the optimization process by damaging the multi-view consistency and the exact pose for shading optimization,
which leads to blurry textures.

\noindent \textbf{The impact of training view amount.}
Table~\ref{table:ablation:data}~(b) and Figure~\ref{fig:ablation:data:num_views} reveal that giving one camera of view degrades the overall reconstruction quality, and multi-view consistency improves the final results.
The model can aggregate multi-view information for better shading optimization, thus leading to clearer surface materials.

\noindent \textbf{The impact of training frame amount.}
As the number of training frames increases, the rendering quality on novel view and novel pose increases as well (Table~\ref{table:ablation:data}~(c) and Figure~\ref{fig:ablation:data:num_frames}). Notice that the reconstruction quality saturated after using a certain amount of training frames, the same results can be observed in~\cite{pengAnimatableNeuralRadiance2021} as well.

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figs/num_frames.pdf}
\vspace{-1em}
\caption{\textbf{Comparison of models trained with different numbers of video frames} on the subject ``S9".}
\label{fig:ablation:data:num_frames}
\vspace{-1em}
\end{figure}


\subsection{Applications}
\label{exp:application}
After training, we can export mesh representations, which enables instant downstream applications.
We showcase two examples of novel pose synthesis, material editing, and human relighting in Figure~\ref{fig:teaser}. For more examples, please refer to our supplementary.

