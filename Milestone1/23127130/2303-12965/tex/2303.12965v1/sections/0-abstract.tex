\begin{abstract}
\looseness=-1
Efficiently digitizing high-fidelity animatable human avatars from videos is a challenging and active research topic.
Recent volume rendering-based neural representations open a new way for human digitization with their friendly usability and photo-realistic reconstruction quality.
However, they are inefficient for long optimization times and slow inference speed;
their implicit nature results in entangled geometry, materials, and dynamics of humans,
which are hard to edit afterward.
Such drawbacks prevent their direct applicability to downstream applications, especially the prominent rasterization-based graphic ones.
We present \textbf{EMA}, a method that \textbf{E}fficiently learns \textbf{M}eshy neural fields to reconstruct animatable human \textbf{A}vatars. 
It jointly optimizes explicit triangular canonical mesh, spatial-varying material, and motion dynamics, via inverse rendering in an end-to-end fashion.
Each above component is derived from separate neural fields, relaxing the requirement of a template, or rigging.
The mesh representation is highly compatible with the efficient rasterization-based renderer, thus our method only takes about an hour of training and can render in real-time.
Moreover, only minutes of optimization are enough for plausible reconstruction results.
The disentanglement of meshes enables direct downstream applications.
Extensive experiments illustrate the very competitive performance and significant speed boost against previous methods.
We also showcase applications including novel pose synthesis, material editing, and relighting.
The project page: \url{https://xk-huang.github.io/ema/}.
\end{abstract}
