\section{Introduction}


Recent years have witnessed the rise of human digitization~\cite{habermannDeepCapMonocularHuman2020,alexanderCREATINGPHOTOREALDIGITAL,pengNeuralBodyImplicit2021,alldieckDetailedHumanAvatars2018, rajANRArticulatedNeural2020}. This technology greatly impacts the entertainment, education, design, and engineering industry.
There is a well-developed industry solution for this task.
High-fidelity reconstruction of humans can be achieved either with full-body laser scans~\cite{saitoSCANimateWeaklySupervised2021}, dense synchronized multi-view cameras~\cite{xiangModelingClothingSeparate2021a,xiangDressingAvatarsDeep2022a}, or light stages~\cite{alexanderCREATINGPHOTOREALDIGITAL}.
However, these settings are expensive and tedious to deploy and consist of a complex processing pipeline, preventing the technology's democratization.

Another solution is to view the problem as inverse rendering and learn digital humans directly from custom-collected data.
Traditional approaches directly optimize explicit mesh representation~\cite{loperSMPLSkinnedMultiperson2015, fangRMPERegionalMultiperson2018, pavlakosExpressiveBodyCapture2019} which suffers from the problems of smooth geometry and coarse textures~\cite{prokudinSMPLpixNeuralAvatars2020,alldieckVideoBasedReconstruction2018}. Besides, they require professional artists to design human templates, rigging, and unwrapped UV coordinates.
Recently, with the help of volumetric-based implicit representations~\cite{mildenhallNeRFRepresentingScenes2020, parkDeepSDFLearningContinuous2019, meschederOccupancyNetworksLearning2019} and neural rendering~\cite{laineModularPrimitivesHighPerformance2020, liuSoftRasterizerDifferentiable2019, thiesDeferredNeuralRendering2019}, 
one can easily digitize a quality-plausible human avatar from video footage~\cite{jiangNeuManNeuralHuman2022,wengHumanNeRFFreeviewpointRendering}.
Particularly, volumetric-based implicit representations~\cite{mildenhallNeRFRepresentingScenes2020, pengNeuralBodyImplicit2021} can reconstruct scenes or objects with much higher fidelity against previous neural renderer~\cite{thiesDeferredNeuralRendering2019,prokudinSMPLpixNeuralAvatars2020}, and is more user-friendly as it does not need any human templates, pre-set rigging, or UV coordinates.
Captured visual footage and corresponding skeleton tracking are enough for training.
However, better reconstructions and more friendly usability are at the expense of the following factors.
1) \textbf{Inefficiency:}
They require longer optimization times (typically tens of hours or days) and inference slowly.
Volume rendering~\cite{mildenhallNeRFRepresentingScenes2020,lombardiNeuralVolumesLearning2019} formulates images by querying the densities and colors of millions of spatial coordinates. 
In the training stage, due to memory constraints, only a small fraction of points are sampled which leads to slow convergence speed.
2) \textbf{Entangled representations}:
The geometry, materials, and motion dynamics are entangled in the neural networks. 
Due to the implicit nature of neural nets, one can hardly edit one property without touching the others~\cite{yuanNeRFEditingGeometryEditing2022a,liuEditingConditionalRadiance2021}.
3) \textbf{Graphics incompatibility}:
Volume rendering is incompatible with the current popular graphic pipeline,
which renders triangular/quadrilateral meshes efficiently with the rasterization technique.
Many downstream applications require mesh rasterization in their workflow (\eg, editing~\cite{foundationBlenderOrgHome}, simulation~\cite{benderPositionBasedSimulationMethods2015}, real-time rendering~\cite{akenine2019real}, ray-tracing~\cite{waldRTXRayTracing}).
Although there are approaches~\cite{lorensenMarchingCubesHigh,labelleIsosurfaceStuffingFast2007} can convert volumetric fields into meshes, the gaps from discrete sampling degrade the output quality in terms of both meshes and textures.


To address these issues, we present \textbf{EMA}, a method based on \textbf{E}fficient \textbf{M}eshy neural fields to reconstruct animatable human \textbf{A}vatars.
Our method enjoys flexibility from implicit representations and efficiency from explicit meshes, yet still maintains high-fidelity reconstruction quality.
Given video sequences and the corresponding pose tracking, our method digitizes humans in terms of canonical triangular meshes, physically-based rendering (PBR) materials, and skinning weights \textit{w.r.t.} skeletons.
We jointly learn the above components via inverse rendering~\cite{laineModularPrimitivesHighPerformance2020,chenDIBRLearningPredict2021,chenLearningPredict3D2019} in an end-to-end manner.
Each of them is derived from a separate neural field, which relaxes the requirements of a preset human template, rigging, or UV coordinates.
Specifically, we predict a canonical mesh out of a signed distance field (SDF) by differentiable marching tetrahedra~\cite{shenDeepMarchingTetrahedra2021,gaoGET3DGenerativeModel,gaoLearningDeformableTetrahedral2020,munkbergExtractingTriangular3D2022}, then we extend the marching tetrahedra~\cite{shenDeepMarchingTetrahedra2021} for spatial-varying materials by utilizing a neural field to predict PBR materials \textit{on the mesh surfaces} after rasterization~\cite{munkbergExtractingTriangular3D2022,hasselgrenShapeLightMaterial2022,laineModularPrimitivesHighPerformance2020}.
To make the canonical mesh animatable, we take another neural field to model the forward linear blend skinning for the meshes. 
Given a posed skeleton, the canonical mesh is then transformed into the corresponding poses.
Finally, we shade the mesh with a rasterization-based differentiable renderer~\cite{laineModularPrimitivesHighPerformance2020} and train our models with a photo-metric loss.
After training, we export the mesh with materials and discard the neural fields.

\looseness=-1
There are several merits of our method design.
1) \textbf{Efficiency}:
Powered by efficient mesh rendering, our method can render in real-time.
Besides, the training speed is boosted as well, 
since we compute loss holistically on the whole image and the gradients only flow on the mesh surface. In contrast, volume rendering takes limited pixels for loss computation and back-propagates the gradients in the whole space.
Our method only needs about an hour of training and minutes of optimization are enough for plausible avatar reconstruction.
2) \textbf{Disentangled representations}:
Our shape, materials, and motion modules are disentangled naturally by design, which facilitates editing. 
Besides, Canonical meshes with forward skinning modeling handle the out-of-distribution poses better.
3) \textbf{Graphics compatibility}:
Our derived mesh representation is compatible with 
the prominent graphic pipeline, which leads to instant downstream applications (\eg, the shape and materials can be edited directly in design software~\cite{foundationBlenderOrgHome}).
To further improve reconstruction quality, we additionally optimize image-based environment lights and non-rigid motions.


We conduct extensive experiments on standards benchmarks H36M~\cite{ionescuHuman36MLarge2014b} and ZJU-MoCap~\cite{pengNeuralBodyImplicit2021}.
Our method achieves very competitive performance for novel view synthesis, generalizes better for novel poses, 
and significantly improves both training time and inference speed against previous arts.
Our research-oriented code reaches real-time inference speed (100+ FPS for rendering $512\times512$ images).
We in addition showcase applications including novel pose synthesis, material editing, and relighting.