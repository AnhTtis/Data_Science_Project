\section{Implementation Details}

\label{supp:implementation_details}



\noindent \textbf{SDF network}. We parametrize the SDF field with an MLP to increase surface water-tight and smoothness.
We choose the MLP architecture from \cite{mildenhallNeRFRepresentingScenes2020}, which consists of 6 frequency bands for positional encoding, and 8 linear layers, each having 256 neurons, followed by ReLU activations.
We implicitly regularize the smoothness by increasing the Lipschitz property in the SDF field\cite{liuLearningSmoothNeural2022a}.

\noindent \textbf{Material network}. The material model is a small MLP with hash-encoding~\cite{mullerInstantNeuralGraphics2022} as the materials query is computationally extensive.
The MLP consists of two linear layers, each having 32 neurons, followed by ReLU activations.
The hash-encoding has a spatial resolution of 4096 and the rest configures are the same as~\cite{munkbergExtractingTriangular3D2022}.
To reduce computation, we predict all material channels at once with one backbone network.
Besides, we introduce inductive bias of materials of clothed humans in the real world,
by providing minimum and maximum values for each materials channel.
We follow~\cite{Zhang2021NeRFactorNF} to limit the albedo $\mathbf{k}_d \in [0.03, 0.8]$, and the roughness $\mathbf{k}_r \in [0.08, 1]$.
The texels in the environment light are randomly initialized between $[0.25, 0.75]$.

\noindent \textbf{Motion networks}. For the motion module, we use the same MLP architecture as~\cite{chenSNARFDifferentiableForward2021,wangARAHAnimatableVolume}, which is similar to our SDF MLP.
To resolve the problem where the training pose variation is too limited for skinning field learning (\eg, self-rotation video without any limbs movements), 
we initialize the MLP with the pre-trained skinning model provided by~\cite{wangARAHAnimatableVolume},
and impose $\ell_2$ norm for the skinning weights logits between our predictions and the ground truth from SMPL~\cite{loperSMPLSkinnedMultiperson2015}.
We ablate the design choices in Sec.~\ref{supp:additional_ablations:skinning}.
For the non-rigid modeling, we use another 4-layer ReLU MLP with a 4-frequency-band positional encoding.
We also progressively anneal its encoding for 5k iterations as~\cite{parkNerfiesDeformableNeural2021}.
The weights of the last layer are initialized with a uniform distribution $\mathcal{U}(-10^{-5}, 10^{-5})$, \ie initializing the non-rigid offsets to be close to zero and not interfering with the major optimizations of geometry and materials.

\noindent \textbf{Optimization}. 
We use Adam~\cite{kingmaAdamMethodStochastic2017} as our default optimizer.
We optimize the subject for 5k steps for 1024$\times$1024 images or 10k steps 512$\times$512 images.
We disable the perturbed normal map during optimization as it leads to SDF collapsing abruptly at a certain step (\ie, all SDF values are positive or negative where marching tetrahedra fails).
The optimization process takes about an hour on a single NVIDIA GTX3090 GPU.
The indicative results with plausible quality appear after a few minutes, which is quite faster than our counterparts~\cite{pengNeuralBodyImplicit2021,pengAnimatableNeuralRadiance2021,wangARAHAnimatableVolume,xuSurfaceAlignedNeuralRadiance2022a}.
Such superior efficiency could largely accelerate downstream applications.
The training visualization is presented in the supplemental video.

\noindent \textbf{Tetrahedra grids}. We start with a tetrahedra grid with $128 \times 128$ resolution, including 192k tetrahedra and 37k vertices.
Each tetrahedron can produce at most 2 triangles by marching tetrahedra algorithm~\cite{munkbergExtractingTriangular3D2022,shenDeepMarchingTetrahedra2021,gaoLearningDeformableTetrahedral2020}.
To increase the resolution of the tetrahedra grid, we subdivide the grid at the 500th step.
To avoid the out-of-memory problem caused by the vast amount of floating meshes in the void space at the beginning of training, we pre-train the SDF network to \textbf{match a visual hull} of humans in canonical space.
The hull could be derived from either the skeleton capsules or the SMPL~\cite{loperSMPLSkinnedMultiperson2015} mesh.
Note that we only pre-train for 500 iterations, which leads to \textbf{a very coarse shape} akin to the visual hull rather than the given ground truth mesh.
The initialized mesh is presented in the training visualization part of the supplemental video.