{
    "arxiv_id": "2303.15081",
    "paper_title": "Exemplar-based Video Colorization with Long-term Spatiotemporal Dependency",
    "authors": [
        "Siqi Chen",
        "Xueming Li",
        "Xianlin Zhang",
        "Mingdao Wang",
        "Yu Zhang",
        "Jiatong Han",
        "Yue Zhang"
    ],
    "submission_date": "2023-03-27",
    "revised_dates": [
        "2023-03-28"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV"
    ],
    "abstract": "Exemplar-based video colorization is an essential technique for applications like old movie restoration. Although recent methods perform well in still scenes or scenes with regular movement, they always lack robustness in moving scenes due to their weak ability in modeling long-term dependency both spatially and temporally, leading to color fading, color discontinuity or other artifacts. To solve this problem, we propose an exemplar-based video colorization framework with long-term spatiotemporal dependency. To enhance the long-term spatial dependency, a parallelized CNN-Transformer block and a double head non-local operation are designed. The proposed CNN-Transformer block can better incorporate long-term spatial dependency with local texture and structural features, and the double head non-local operation further leverages the performance of augmented feature. While for long-term temporal dependency enhancement, we further introduce the novel linkage subnet. The linkage subnet propagate motion information across adjacent frame blocks and help to maintain temporal continuity. Experiments demonstrate that our model outperforms recent state-of-the-art methods both quantitatively and qualitatively. Also, our model can generate more colorful, realistic and stabilized results, especially for scenes where objects change greatly and irregularly.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.15081v1"
    ],
    "publication_venue": null
}