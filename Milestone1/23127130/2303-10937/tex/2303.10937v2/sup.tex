\documentclass[10pt,twocolumn,letterpaper]{article}
%\usepackage[rebuttal,applications]{wacv}  % use this for an Applications Track rebuttal
%\usepackage[rebuttal,algorithms]{wacv}  % use this for an Algorithms Track rebuttal
\usepackage{wacv}
% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{afterpage}
\usepackage{multicol}

\usepackage{blindtext}
\usepackage{appendix}
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\wacvPaperID{818} % *** Enter the CVPR Paper ID here
\def\confName{WACV}
\def\confYear{2024}

\begin{document}

\begin{appendices}

\renewcommand{\thesection}{S.\arabic{section}}
%\renewcommand{\figurename}{Figure A.\arabic{figure}}
\renewcommand{\thetable}{S.\arabic{table}}
\renewcommand{\thefigure}{S.\arabic{figure}}

%\title{Appendix to “Boosting Weakly Supervised Object Detection using Fusion and Priors from Hallucinated Depth”}
\title{Boosting Weakly Supervised Object Detection using Fusion and Priors from Hallucinated Depth\\(Supplementary File)}

\maketitle

\section{Alternative approach for depth priors}
\label{sec:alternate_approach}

\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{ lcccccc}
\toprule
 & \multicolumn{3}{c}{Avg. Precision, IoU}  & \multicolumn{3}{c}{Avg. Precision, Area} \\
 \cmidrule{2-4} \cmidrule{5-7}
Methods on COCO & 0.5:0.95 & 0.5 & 0.75 & S & M & L  \\
\midrule 
\textsc{MIST \cite{ren2020instance}} w/ EM &8.5&17.9&7.3 & 3.0 & 9.4 & 14.9 \\
\textsc{+ Siamese-Only} & 8.8 & 18.7 & 7.3 & 2.9 & 9.6 & 15.4  \\
\textsc{+ Depth-Oicr-Alt} & \underline{8.9} & \underline{19.0} & \underline{7.3} & \underline{3.0} & \underline{9.6} & \underline{15.6} \\
\textsc{+ Depth-Oicr} & \underline{9.0} & \underline{19.4} & \underline{7.3} & \underline{3.1} & \underline{9.6} & \underline{15.9} \\
\textsc{+ Depth-Attention-Alt} & \underline{9.0} & \underline{18.8} & \underline{7.7} & \underline{3.0} & 9.5 & \underline{16.0}  \\
\textsc{+ Depth-Attention} & \underline{9.1} & \underline{19.0} & \underline{7.9} & \underline{3.0} & 9.5 & \underline{16.2}  \\

\bottomrule
\hline \\
\end{tabular}}
\caption{This table introduces the comparison of the proposed and alternate approaches (\textsc{Depth-Oicr-Alt} and \textsc{Depth-Attention-Alt}) for modeling depth priors. All proposed methods that outperform the \textsc{Siamese-Only} are \underline{underlined}.}
\label{Table:alternate}
\end{table}

In Sec. 3.3, we described our method to obtain depth priors leveraging generated
bounding box predictions and associated captions to extract knowledge about the relative depths of objects. As an alternative, we use only bounding box predictions (without captions) to obtain depth priors.

Similar to Sec. 3.3, let $C$ be the set of object categories, and $B$ be the set of predicted bounding boxes. Let $d_{c,b} \in [0, 1]$ denote the depth value for object $c \in C$ and box $b \in B$. Further, $d_{c}$ represents a set of depth values for each $c$. The depth range $r_{c} = [mean - std, mean + std]$ is obtained by utilizing the mean and standard deviation (std) of this set of depth values in $d_{c}$. 

Table \ref{Table:alternate} demonstrates that the \textsc{Depth-Oicr-Alt} and \textsc{Depth-Attention-Alt} techniques, employed from the alternate approach, yield superior results compared to \textsc{Siamese-Only}. Nevertheless, the \textsc{Depth-Oicr} and \textsc{Depth-Attention} methods, derived from the proposed approach, outperform both \textsc{Depth-Oicr-Alt} and \textsc{Depth-Attention-Alt}. Note that the depth range $r_{c}$ remains consistent across all images in this alternative method. Conversely, in the proposed approach, the depth range $dr_{c}$ is computed individually for each image by taking into account the corresponding caption, as visualized in Figure 3 of the paper. As a result, the proposed approach demonstrates enhanced capacity in modeling depth priors through the utilization of captions. 

\section{Depth modality in inference}

As detailed in Section 3.2, fusion scores ($f^{det}$ and $f^{cls}$) are calculated by adding together RGB scores ($v^{det}$ and $v^{cls}$) and depth scores ($d^{det}$ and $d^{cls}$) in Eq. 5. Then, fusion scores are used to compute multiple instance learning loss $\mathcal{L}_{mil}$ in Eq. 9 to derive region-level scores during training. However, our proposed method uses only RGB detection $v^{det}$ and classification $v^{cls}$ scores during inference.

\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{ lcccccc}
\toprule
 & \multicolumn{3}{c}{Avg. Precision, IoU}  & \multicolumn{3}{c}{Avg. Precision, Area} \\
 \cmidrule{2-4} \cmidrule{5-7}

Methods on COCO & 0.5:0.95 & 0.5 & 0.75 & S & M & L  \\
\midrule 

\textsc{MIST \cite{ren2020instance}} & 11.8 & 24.3 & 10.7 & 3.6 & 13.2 & 18.9\\

\textsc{+ Wsod-Amplifier} (Ours) & \bf{13.8}&\bf{27.8}&\bf{12.5}&\bf{4.6}&\bf{14.8}&\bf{22.6}\\
\textsc{+ Wsod-Amplifier-Depth} & 4.4 & 8.7 & 3.8 & 0.6 & 3.5 & 8.8\\
\textsc{+ Wsod-Amplifier-Fusion} & 13.1 & 27.5 & 11.9 & 4.3 & 14.3 & 22.2\\

\bottomrule
\hline \\
\end{tabular}}
\caption{This table compares the different variations of our \textsc{Wsod-Amplifier} method during inference. The best performer per column is in \bf{bold}.}
\label{Table:inference}
\end{table}

The performance of \textsc{Wsod-Amplifier-Depth}, which relies solely on depth scores \emph{during inference}, is poorer due to the relatively lower informativeness of depth images compared to RGB images for detection in Table \ref{Table:inference}. On the other hand, \textsc{Wsod-Amplifier-Fusion}, which employs fusion scores, exhibits lower performance than our proposed approach, \textsc{Wsod-Amplifier}, which solely employs RGB scores during inference. The rationale behind the enhancement of results through fusion during training, while not during inference, stems from the nature of the MIL loss $\mathcal{L}_{mil}$. This loss function is formulated to guide the model in the classification of objects in an image, indirectly yielding region-level detection scores. While training, inaccuracies in depth for certain regions can be compensated by other regions to classify objects in MIL loss. However, during inference, each region operates independently, and errors on a per-region basis more directly impact detection results. Further, \cite{meyer2020improving} demonstrates that integrating the depth modality with RGB aids in enhancing the model's performance in classification tasks. The improved classification capacity of our model consequently leads to enhanced benefits from the MIL loss, contributing to improved representation learning. This improvement is highlighted by the enhanced performance of RGB scores during inference.

\input{table_per_class}

\section{Class-wise comparison on COCO}
Figure ~\ref{fig:coco_depth_ranges} demonstrates the effectiveness of depth priors in capturing the depth characteristics of different object classes in the COCO dataset. On average across classes, a substantial portion of the training data (80.75\%), resides within the constant ranges $r_{c}$ in the figure. Note the depth ranges $dr_{c}$ in our proposed approach for depth priors are computed separately for each image according to the corresponding caption. This image-specific calculation achieves more comprehensive coverage of depth variations within the training data compared to constant ranges $r_{c}$. Diverse object classes exhibit distinct depth ranges. The "fire hydrant" object displays the smallest average depth mean at 19\%, whereas the "kite" object displays the highest (54\%). 



%Furthermore, we mention in Sec. 4.3 that applying the depth priors calculated from COCO to Conceptual Captions (CC) results in a more significant performance improvement on the noisy CC compared to COCO, even though priors are computed from COCO. 


\section{Scalability and generalization}
Our demonstration reveals that a substantial amount of data is \emph{not} necessary for estimating depth. 
%Depth priors improve the performance of objects based on their characteristics, rather than relying on their frequency in the training dataset. 
The depth priors calculated from the least frequent classes contribute even more to performance improvement than those from the most frequent classes. 
Referring to the data presented in Table \ref{Table:class-wise}, the average increase in $mAP_{50:95}$ for the least frequent 20\% classes is 3.4, while the increase for the most frequent 20\% classes is 0.8. 

To understand the impact of diverse depth ranges on performance, Table \ref{Table:class-wise} includes per-class standard deviations (STD). Notably, the depth priors computed from classes with smaller STDs make a more significant contribution to performance enhancement compared to those with larger STDs. On average, the $mAP_{50:95}$ increase for the 20\% of classes with the smallest STD is 4.8, whereas the increase for the 20\% of classes with larger STDs is 1.2. The observation suggests that a narrower depth distribution of an object corresponds to more informative depth information. Certainly, objects like "bear," "train," and "giraffe" exhibit smaller STDs, thereby leading to a more significant improvement in detection performance. Additionally, respective percentages of objects in Figure ~\ref{fig:coco_depth_ranges}, which depict the proportion of training data within the specified depth range, are notably higher at 89\%, 90\%, and 88\%, respectively.

Our analysis demonstrates that depth priors calculated using COCO exhibit similarities to those computed using PASCAL, as evident from Figure \ref{fig:voc_prior_two_plot}. On an average basis across various classes, approximately 84.4\% of data aligns within the constant ranges derived from PASCAL, whereas 82.3\% aligns within the ranges from COCO. The minimal differences in percentage and the visual resemblance of depth ranges in Figure \ref{fig:voc_prior_two_plot} underscore the generalizability of our depth priors. Furthermore, we mention in Sec. 4.3 that applying the depth priors calculated from COCO to Conceptual Captions (CC) results in a more significant performance improvement on the noisy CC compared to COCO, even though priors are computed from COCO. 

\input{table_domain_shift}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{word_stat_fig.png}
    \caption{This figure provides an intuitive understanding of how the utilization of captions enhances the estimation of the depth range $dr_c$ of class c for a specific image. Here, $r_{c,w}$ signifies the depth range of class c when it is associated with the word w.}
    \label{fig:word_stat}
\end{figure*}

\section{Generalization to appearance changes}

By relying on depth information, our method builds some robustness to overfitting to appearance, which may not be the same across datasets. To test this hypothesis, we conduct experiments with domain shift datasets \cite{inoue2018cross}. Table ~\ref{Table:3} shows that our \textsc{Wsod-Amplifier} boosts the performance of MIST baseline in $mAP_{50}$ by $4-10\%$, even though no training is performed on these datasets.

\section{Illustration to depth ranges with caption}

The depth range $dr_c$ for class $c$ is individually computed for each image by leveraging the corresponding caption. In this context, $r_{c,w}$ denotes the depth range of class $c$ when linked to the word $w$. In Eq. 10, the calculation of $dr_c$ involves deriving the average of depth ranges $r_{c,w}$ across each word present within the caption. Figure \ref{fig:word_stat} illustrates three object examples: "kite", "TV", and "zebra". The top row of images depicts instances where objects are situated farther away from the camera, while the bottom row showcases examples where objects are positioned closer to the camera. As an illustration, consider the "kite" object in the upper image, where the caption features the word "ocean." Due to the influence of $r_{kite, ocean}$, the depth range $dr_{kite}$ becomes larger.  It's reasonable to expect that a kite would be positioned at a greater distance from the camera if it's flying over an ocean. Worth noting is that while $dr_{kite}$ is enlarged, it remains smaller than $r_{kite, ocean}$ due to the averaging effect with less descriptive words like ``is."  In a similar scenario, the depth range $dr_{kite}$ in the lower image becomes smaller due to the impact of $r_{kite, holding}$. When a person holds a kite, the likelihood is that the kite is positioned closer to the camera.

\section{Failure cases}
In Figure \ref{fig:failure}, the initial three cases depict objects with smaller depth values lying outside the depth range, while the last two cases feature objects with larger depth values. In the second scenario, the car is positioned closer to the camera, causing it to exceed the depth range limits. In contrast, in the fourth scenario, the broccoli is situated in the background with a larger depth value.

\begin{figure*}[t]
    \includegraphics[width=\textwidth]{coco_depth_ranges.png}
    \caption{This figure illustrates how depth priors effectively capture depth characteristics across various object classes within the COCO dataset. In this visualization, points colored in blue indicate the depths of samples in the training set associated with the designated class.  Points colored in red denote the mean of depth range, and green points mark the boundaries of the range. The percentages located on the right side of class names provide insight into the proportion of training data falling within the defined depth range.}
 
    \label{fig:coco_depth_ranges}
\end{figure*}

\begin{figure*}[t]
    \includegraphics[width=\textwidth]{voc_prior_two_plot.drawio.png}
    \caption{This figure demonstrates the effectiveness of depth priors, computed using the COCO dataset on the left and computed using the PASCAL dataset on the right, in capturing the depth characteristics across different object classes in the PASCAL dataset. }
 
    \label{fig:voc_prior_two_plot}
\end{figure*}




\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{failure_case_depth.png}
    \caption{This figure illustrates the failure cases where the depth value $p_{d}$ of objects lies outside the depth range $dr_{c}$.}
    \label{fig:failure}
\end{figure*}

\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


\end{appendices}

\end{document}