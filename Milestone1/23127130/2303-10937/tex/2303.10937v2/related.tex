\section{Related Work}
\textbf{Weakly-supervised object detection (WSOD)} is the task of learning to detect the location and type of objects given only image-level labels during training. The multi-instance learning (MIL) framework is commonly utilized in WSOD methods such as
%, with early works such as 
WSDDN \cite{bilen2016weakly}.
%integrating MIL into an end-to-end WSOD system. 
OICR \cite{tang2017multiple} improved upon this by proposing pseudo-ground-truth mining and an online instance refinement, which was further refined by proposal clustering \cite{tang2018pcl}. C-MIL \cite{wan2019cmil} and MIST \cite{ren2020instance} introduced modifications to the MIL loss and pseudo-ground-truth mining, respectively. 
%UWSOD \cite{shen2020uwsod} addresses the scale variation challenge by using a multi-rate resampling pyramid, which aggregates contextual information at multiple scales. CASD \cite{huang2020comprehensive}, have proposed self-distillation along with attention to improving WSOD performance. 
SoS-WSOD \cite{sui2022salvage} proposed a method that produces pseudo boxes for FSOD 
%by using their improved WSOD module, then they 
and splits noisy data for semi-supervised object detection.
Additionally, there have been efforts to bypass the need for image-level labels by utilizing noisy labels extracted from caption or subtitle data \cite{ ye2019cap2det,Chen_2017_CVPR,zareian2021open,unal2022learning,gao}.
Additionally, \cite{gungor2023complementary} leverages audio to improve WSOD performance and reduce noise from text-based label extraction.
In contrast to these works, our method leverages depth information as an additional modality, leading to improved performance in WSOD and a reduction of the noise in labels extracted from text.

\textbf{RGB-D detection.} 
The integration of RGB and depth information to derive complementary features has been previously studied for fully-supervised indoor analysis \cite{ying2022uctnet,zhou2022scale,jiao2019geometry,li2019mapnet, zhang2021depth} and object detection \cite{lee2022spsn,hussain2022pyramidal,feng2022encoder,fu2020jl, li2021joint, hoffman2016learning, huang2022good}. %Fusing the information contained in the RGB and depth modalities is crucial, as they provide complementary information. 
The strategies for merging the two modalities can be classified into three groups, depending on the point in the processing pipeline where the fusion occurs: early fusion \cite{fan2020rethinking,qu2017rgbd}, middle fusion \cite{fu2020jl,fu2021siamese, zhu2019pdnet, chen2018progressively}, and late fusion \cite{han2017cnns,piao2019depth}. Early fusion techniques involve combining the RGB and depth images into a single four-channel matrix at the earliest stage of the process. %This integrated matrix is then treated as a single input. 
Middle fusion provides a balance between early and late fusion by utilizing CNNs for both feature extraction and subsequent merging. 
%A subsidiary network was utilized in \cite{zhu2019pdnet} to extract depth features, which were then used to improve the intermediate representation in an encoder-decoder architecture. 
In late fusion, individual saliency prediction maps are produced from the RGB and depth channels to be combined through post-processing operations.  %These two predicted maps are then combined through post-processing operations such as element-wise summation or multiplication. 
%Han et al. \cite{han2017cnns} modified a deep neural network to work with both RGB and depth views and combined the output of these views using a fully-connected layer. 
In contrast to the majority of aforementioned methods, which use separate networks to extract features from RGB and depth images, several studies \cite{fu2020jl, fu2021siamese, song2021exploiting, meyer2020improving} employ Siamese networks to learn hierarchical features from both RGB and depth inputs by utilizing shared parameters.
%, with varying goals. 
%The studies in \cite{fu2020jl, fu2021siamese} utilize the middle-fusion strategy, while \cite{song2021exploiting} tackles the problem of increased model complexity and \cite{meyer2020improving} enhances the representation learning process through Siamese networks.
%In our study, we employ a Siamese network (shared backbone) to enhance the representation learning process through the use of contrastive learning between RGB and depth features. Additionally, this approach addresses the issue of increased model complexity. The Siamese network design allows us to make use of depth information during training while incurring little extra cost, while the depth is not used during inference. Additionally, we merge the detection and classification scores obtained from both RGB and depth modalities in a weakly supervised object detection network, which can be classified as a late fusion approach.
However, \emph{we are the first to leverage depth data in weakly-supervised object detection.} Our approach is not specific to a particular method, as it can be applied to different MIL-based WSOD methods to improve their performance without incurring any extra annotation expenses and with minimal computational overhead during training. Although the depth modality is not used during the inference stage, incorporating it during training enhances the performance of the inference.

\textbf{Monocular depth estimation} involves predicting the depth map of a scene from a single RGB image \cite{miangoleh2021boosting,ranftl2020towards,ranftl2021vision,yin2021learning}.
%has focused on developing deep networks that can effectively leverage the rich visual information in RGB images to estimate depth. 
We utilize the method in \cite{miangoleh2021boosting} to estimate depth on the training set due to its strong performance. 
%that boosts the results of \cite{yin2021learning} in order to estimate the depth information from RGB images. 
This estimated (``hallucinated'') depth information is utilized to improve the performance of weakly supervised object detection.
