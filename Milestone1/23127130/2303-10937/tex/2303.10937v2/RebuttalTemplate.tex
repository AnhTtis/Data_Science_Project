\documentclass[10pt,twocolumn,letterpaper]{article}
%\usepackage[rebuttal,applications]{wacv}  % use this for an Applications Track rebuttal
\usepackage[rebuttal,algorithms]{wacv}  % use this for an Algorithms Track rebuttal

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}



%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\wacvPaperID{818} % *** Enter the CVPR Paper ID here
\def\confName{WACV}
\def\confYear{2024}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Boosting Weakly Supervised Object Detection using Fusion and Priors from Hallucinated Depth}  % **** Enter the paper title here

\maketitle
%\thispagestyle{empty}
%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
We thank reviewers for encouragement and positive feedback (\textcolor{red}{R3}: “novel approach, detailed comparisons”, \textcolor{blue}{R4}: “interesting method, promising improvement, well-written”, \textcolor{green}{R5}: “easy to implement, considerable  improvements, excels in noisy situations, well-structured”, \textcolor{cyan}{R7}: “interesting and effective, comprehensive analysis, clear formulations”). \textbf{Key changes in the paper are in \textcolor{blue}{blue}, and we provide a new, detailed supplementary file.}

\noindent 
\textcolor{red}{(R3-1)}, \textcolor{red}{(R3-2)}, \textcolor{green}{(R5-2)} \textbf{More insights on depth priors:} We \textbf{include new comprehensive analysis in the paper}: alternative approach for modeling priors (L504-513), a result on PASCAL to show depth ranges are consistent across datasets (L827-834); and \textbf{in supplementary}: scatter plots of each class (Sec. S.3), per-class mAP results (Sec S.4),  and failure cases (Sec. S.7). 
%per-class mAP results (L*-*), scatter plots of each class (L*-*),  and failure cases (L*-*).

\noindent 
\textcolor{red}{(R3-3)} \textbf{Using depth modality in inference:} We \textbf{include the new result} in the paper (L698-701) and further analysis in supplementary (Sec. S.2).
%to show how depth affects the results during inference. 
Briefly, depth improves the RGB representation and multiple-instance model by helping discount incorrect regions from the MIL loss \emph{during training}, but it is less effective directly \emph{during inference}.


\noindent 
\textcolor{red}{(R3-4)} \textbf{Presentation:} We revised text to clarify.


\noindent 
\textcolor{blue}{(R4-1)} \textbf{Motivation to use captions, alternative:} 
We \textbf{elaborate on motivation} in revision (L82, L122, L421, L473).
Since we utilize captions only for training and many such datasets e.g. [22, 30, RedCaps, LAION] are widely available, freely-available text on the web is cost-effective when compared to image labels which require dedicated crowdsourcing effort. See L703-710. As image labels represent the strongest supervision in WSOD, captions offer weaker, coarser supervision that aligns with the WSOD setting. We did experiment with an \textbf{alternative approach} for modeling depth priors which \textbf{does not use captions, see L504-513, L786-799, Tab.~2}. It \textbf{still gives a boost} in performance.

\noindent 
\textcolor{blue}{(R4-2)} \textbf{SOTA performance:} \textbf{\textcolor{blue}{R4}'s claim is incorrect.} In our Table 1, we show we achieve 27.8 mAP$_{50}$ on COCO, higher than the 24.8 reported in the reference [1] that \textcolor{blue}{R4} gives.  %assertion that current experiments do not convincingly show an enhancement in existing WSOD methods, we effectively demonstrate that our proposed approach enhances the results of SOTA methods, as evidenced by our attainment of the new SOTA result in Table 1. They also highlight that [1] achieves SOTA performance with just image-level labels. Note that noisy captions are easier to provide than exhaustive image-level labels so they provide weaker supervision as discussed in the preceding paragraph. 
We'd also like to \textbf{point out a mistake in \textcolor{blue}{R4}'s review}, namely that [1] and [2] in the review refer to the same paper. We would like to clearly state that, \textbf{based on \textcolor{blue}{R4}'s review text, we do not think that SR is justified.} This is particularly true considering \textbf{\textcolor{blue}{R4}'s acknowledgment of the interesting method, quality of writing, and promising advancements.}


\noindent 
\textcolor{green}{(R5-1)}, \textcolor{green}{(R5-2)} \textbf{Scalability and generalization, how depth ranges across datasets:} We include additional analysis in the supplementary (Sec S.4). Our demonstration reveals that a substantial amount of data is \emph{not} necessary for estimating depth. 
%Depth priors improve the performance of objects based on their characteristics, rather than relying on their frequency in the training dataset. We illustrate that while 
\textbf{The depth priors calculated from the least frequent classes contribute even more to performance improvement than those from the most frequent classes.}
The average increase in $mAP_{50:95}$ for the least frequent 20\% classes is 3.4, while the increase for the most frequent 20\% classes is 0.8. 
Further, applying the depth priors calculated from COCO to Conceptual Captions (CC) results in a significant performance improvement when training on CC (L801-809), indicating \textbf{cross-dataset generalization.}
We also show in supp that \textbf{depth priors calculated on COCO are similar to those calculated on PASCAL}.
%This finding underscores the generalization capability of our method.

\noindent 
\textcolor{green}{(R5-3)} \textbf{Ad-hoc steps:} Some OICR-based modules in the WSOD literature [29, 32] serve the purpose of enhancing performance by eliminating or refining regions that are less likely to contain objects during training. Following the literature, we employ a binary mask of 0 to refine regions. Consequently, we believe this approach is not ad-hoc. 
There exists a trade-off between estimating broader and narrower depth ranges; the former could prevent identifying less relevant regions, while the latter may prevent identifying relevant regions. We argue that utilizing the mean and standard deviation presents a straightforward yet efficient method to provide a good balance in this trade-off, particularly given the significant performance enhancement in DEPTH-OICR and DEPTH-ATTENTION attributed to the depth priors. Figure S.1 in supplementary visually depicts how the mean and standard deviation effectively estimate depth.

\noindent 
\textcolor{green}{(R5-4)} \textbf{Halving probabilities in DEPTH-ATTENTION:} We aim to reduce the probabilities of regions that are less likely to contain objects. Initially, we experimented with a binary mask of 0, but this led to worse performance compared to the baseline. This outcome was likely due to the overly stringent approach of disregarding the potential of these regions entirely.  Subsequently, we decided on a straightforward method of halving the probabilities, which yielded favorable results. This could be considered a hyperparameter that offers room for better tuning and exhibits variability across distinct datasets. 
%It is clarified in the revised paper (L*-*).


\noindent 
\textcolor{green}{(R5-5)} \textbf{Prior conditioned on every word in caption:} Thanks for the suggestion. We are not able to try it given the limited time between reviews and revision, but we will try it for the final version if the paper is accepted. Note that non-meaningful words will not affect the priors greatly because they will be close to some dataset average of depths, thus will be similar for different categories, and the effect of more important words will dominate. We add \textbf{additional illustration in supp (Sec S.6). Also see resp. to \textcolor{blue}{(R4-1)}.}


\noindent 
\textcolor{cyan}{(R7-1)} \textbf{Depth estimation accuracy:} Upon examining Table 4 in reference [31], we observe the difference between using estimated and GT depth is small (59.7 vs 60.9).
%that an estimated depth yields a 5\% enhancement (56.9 vs. 59.7), whereas employing a GT depth results in a 7\% improvement (56.9 vs. 60.9). 
It is conceivable that employing GT depth could yield a \emph{slight} advancement in our results, but the majority of object detection datasets lack GT depth information, and it is not possible to retroactively annotate (post-data collection).




%{\small
%\bibliographystyle{ieee}
%\bibliography{egbibrebuttal}
%}

\end{document}
