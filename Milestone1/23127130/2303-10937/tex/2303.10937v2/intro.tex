 \section{Introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{concept.png}
    \caption{{Object from the same category may be at different depth depending on the context/setting. We use captions to capture context-conditioned depth ranges for each object class and co-occurring word: a bird may be closer when co-occurring with the word ``feed'' than the word ``ocean''.
    We use these ranges to spot relevant proposals that may contain target objects, and prune irrelevant ones, in weakly supervised object detection training. }}
    \label{fig:concept}
\end{figure}

Weakly-supervised object detection (WSOD) is a challenging task since it is unclear which instances have the label that was provided at the image level. Traditional methods only use appearance information in RGB images. However, appearance information is insufficient to localize objects in complex, cluttered environments. On the other hand, humans are capable of finding useful information in 
complex environments because they rely on object function, not just appearance.
For example, they might reason about which objects are within reach, which can be captured with depth from stereo vision \cite{cammack2016depth}. 
The depth modality provides additional cues about the spatial relationships and geometrical structure of objects in a scene and is invariant to appearance variations (e.g. in texture), making it complementary to the RGB modality. However, weakly-supervised object detection methods do not use depth information.

We equip WSOD methods with the ability to reason about functional information (depth). Importantly, our method does so without requiring additional annotations or suffering significant computational costs. We propose an amplifier method that can enhance the performance of different weakly supervised object detection methods based on multiple-instance learning. Since traditional WSOD datasets do not contain ground-truth depth information, the proposed method utilizes hallucinated (predicted) depth information obtained through a monocular depth estimation technique. During training, the method incorporates depth information to improve representation learning and to prune or down-weight predictions at the box level, which leads to improved object detection performance during inference.

First, depth can directly be used as a feature to aid representation learning, or to produce predictions which can be fused with those computed from RGB. While simple, this technique has not been used for WSOD, and we show that it is very effective: it boosts the performance of appearance-only methods by up to 2.6 mAP@0.5 (11\% relative gain).

Further, depth can provide strong priors about which of the bounding box proposals in the noisy WSOD setting contain an object of interest. We examine the rough depth at which objects of particular categories occur, by computing the depth range of an object using the predictions of a WSOD network. To make this range more precise, we examine the relationship between language context in captions and depth, by keeping track of depth range statistics conditioned on co-mentioned objects.
%and averaging across the most common co-occurring objects. 
{The use of captions allows us to cope with the variable depth at which an object may occur depending on the context, as shown in Fig.~\ref{fig:concept}.}
We then use this range to prune the pseudo ground-truth bounding boxes used to iteratively update weakly-supervised detection methods, or to down-weight predictions at the box level.
This approach boosts WSOD performance further for a total up to 14\% mAP@0.5 gain.

Our method is simple and can boost multiple WSOD methods that rely on iterative improvement. We test it using two state-of-art WSOD baselines, MIST \cite{ren2020instance} and SoS-WSOD \cite{sui2022salvage}, on COCO and PASCAL VOC.
%, Clipart1k, Watercolor2k, and Comic2k. 
Inspired by recent work that trains object detection methods with language supervision \cite{zareian2021open,unal2022learning,gao}, we further test our method in a setting where labels at the image level are not ground-truth but estimated. In this setting, our method boosts the basic WSOD performance even more, by 18\% when labels for training are extracted from COCO, and 63\% when they are extracted from Conceptual Captions.

To summarize, our contributions are: 
%\begin{itemize}[nolistsep,noitemsep]
    (1) We examine for the first time the use of depth in weakly-supervised object detection.
    (2) In addition to depth fusion, we propose a technique specific to WSOD, which estimates depth priors with the help of language, and uses them to refine pseudo boxes and box predictions. 
    (3) We show large performance gains in a large variety of settings, with the biggest boost from depth refinement when supervision is least expensive.
%\end{itemize}



% ----- OLD TEXT

% with overlapping objects varying illumination conditions, primarily because it depends solely on color intensity. 
%%illumination or color variations, making it complementary to the RGB modality to overcome the limitations.

% COULD TALK ABOUT MORE MODALITIES IN RELATED WORK IF ROOM
% To address this challenge, recent research has focused on incorporating other modalities, including audio, language, thermal, and depth, to improve the performance and robustness of object detection. 

%This research paper investigates the effectiveness of the depth modality in weakly supervised object detection. We propose an amplifier method that can enhance the performance of any weakly supervised object detection method based on multiple instance learning, without requiring additional annotations or suffering significant computational costs. Since traditional WSOD datasets do not contain ground truth depth information, the proposed method utilizes hallucinated depth information obtained through a monocular depth estimation technique. During training, the method incorporates depth information to improve representation learning, which subsequently leads to improved object detection performance during inference.
