\section{Approach}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{main_fig.png}
    \caption{This figure illustrates the design of our proposed amplifier technique that takes advantage of depth information to enhance the performance of other weakly-supervised object detection methods. During inference, we only use the RGB branch (shown in orange).}
 
    \label{fig:main_fig}
\end{figure*}

We propose an amplifier approach that incorporates a depth modality to improve the effectiveness of WSOD methods.
%that apply the MIL principle. 
Our method can be used with different MIL-based WSOD methods to boost their performance by incurring little extra cost during training.
It does not use the depth modality during inference to avoid any slow-downs and reliance on additional data (depth estimation or captions). The proposed approach comprises three main steps (Sec.~\ref{sec:siamese_module}, \ref{sec:fusion}, \ref{sec:depth}, respectively). First, a Siamese network with a shared backbone is employed to improve representation learning through contrastive learning between RGB and depth features (referred to as \textsc{Siamese-Only} in the experiments). Second, we combine detection and classification scores obtained from both the RGB and depth modalities, which can be categorized as late fusion (\textsc{Fusion}). Third, we use captions and bounding box predictions of traditional WSOD to calculate depth priors.
%, which provide insight into the objects and their depth. 
These depth priors are then used to improve the OICR-style \cite{tang2017multiple} module in two WSOD methods (named \textsc{Depth-OICR}) and create attention with combined score probabilities (\textsc{Depth-Attention}).
Note that \textsc{Siamese-Only} is always applied, while \textsc{Fusion}, \textsc{Depth-OICR} and \textsc{Depth-Attention} build on top of it, and can be used alone or combined.

\subsection{The Siamese WSOD Network}
\label{sec:siamese_module}

\textbf{WSOD.}
Following Bilen et al. \cite{bilen2016weakly}, let $\mathbf{I} \in \mathbb{R}^{h\times w \times 3}$ denote an RGB image, and $y_c \in \{0, 1\}$ (where $c \in \{1, . . . , C\}$ and $C$ is the total number of object categories) be its corresponding ground-truth class labels.
%, and $\mathbf{R} \in \mathbb{R}^{4\times N }$ as the precomputed object proposals. 
Let $v_i$, $i \in \{1, . . . , R\}$ (where $R$ is the number of proposals), 
%denote the visual regions $\mathbf{v}$  
denote the visual proposals in image $\mathbf{I}$. 
%Proposal feature vectors are extracted by an 
%and a box feature extractor are applied to extract a
RoI pooling is applied and a fixed-length feature vector $\phi(v_i)$ extracted for each visual region. The proposal features $\phi(v_i)$ are fed into two parallel fully-connected layers to compute the visual detection score $v^{det}_{i,c} \in \mathbb{R}^1$ and classification score $v^{cls}_{i,c} \in \mathbb{R}^1$: 
%which are dependent to particular class c and visual region $v_i$:

\begin{equation}
   v^{det}_{i,c} = w^{det\intercal}_{c} \phi(v_i) + b^{det}_{c}, \quad v^{cls}_{i,c} = w^{cls\intercal}_{c} \phi(v_i) + b^{cls}_{c}
   \label{eq:1}
\end{equation}
where $w$ and $b$ are weights and bias, % parameters, 
respectively.



\textbf{Estimating the depth images.} To extract depth information from RGB images, we employ the monocular depth estimation technique by Mahdi et al. \cite{miangoleh2021boosting}. This enables us to use existing RGB-only object detection datasets without the need for additional annotations. Although the extracted depth images are initially grayscale, we use a color map to convert them to RGB images with three channels.
%, allowing them to share the Siamese backbone as the RGB images. 

\textbf{Siamese design.}
Our approach utilizes a Siamese network with contrastive learning to incorporate depth information in the weakly-supervised object detection network during training. 
%This approach is effective, as we show in experiments, and efficient, because...
This design allows us to use a backbone pre-trained with RGB images to extract features from both RGB and depth images, without adding extra complexity to the model's parameters. We enhance the representation learning of the backbone by defining contrastive loss between RGB and depth features similar to \cite{meyer2020improving}.
%, which enables the backbone to extract features of depth. 
Utilizing a Siamese network provides the advantage of using only RGB images during inference similar to other WSOD methods. This ensures that our contribution does not introduce any additional overhead on the inference time.

With the help of a pre-trained backbone model, the feature map of RGB image $\psi(\mathbf{I})$ is extracted. Let $\mathbf{D} \in \mathbb{R}^{h\times w \times 3}$ denote a depth image associated with the RGB image $\mathbf{I}$ and let $\psi(\mathbf{D})$ be the feature map of the depth image $\mathbf{D}$ extracted by the Siamese backbone. The RGB feature map $\psi(\mathbf{I})$ and depth feature map $\psi(\mathbf{D})$ are fed into adaptive pooling and fully connected layers to obtain $d$-dimensional projected feature vectors $\psi_{proj}(\mathbf{I})$ and $\psi_{proj}(\mathbf{D})$.
%for $\mathbf{I}$ and $\mathbf{D}$, respectively. 
The only extra parameters we add to the traditional MIL-based WSOD network come from the fully connected layer for projection with 8 percent overhead (13M parameters for the projection layer, vs 154M total). %13M (proj layer) vs 154M (total)
%However, these additional parameters are relatively insignificant when compared to the total number of parameters.
If no late fusion is performed in the experiments, we train as described in Sec.~\ref{sec:fusion}, but excluding the $d_{i, c}$ variables in Eq.~\ref{eq:fusion}. 

\textbf{Contrastive learning.}
We L2-normalize the RGB and depth feature vectors $\psi_{proj}(\mathbf{I})$ and $\psi_{proj}(\mathbf{D})$ vectors, and compute their cosine similarity:
\begin{equation}
    S(\mathbf{I},\mathbf{D}) = 	\langle \psi_{proj}(\mathbf{I}), \psi_{proj}(\mathbf{D}) \rangle / \rho
    \label{eq:b}
\end{equation} where $\rho$, is a learnable temperature parameter.
We use noise contrastive estimation (NCE) \cite{gutmann2010noise} to define the contrastive learning by considering RGB image and depth image pairs $(\mathbf{I}, \mathbf{D}) \in \mathcal{B}$ where $\mathcal{B}$ is an RGB-depth pair batch. The first component of the NCE loss contrasts an RGB image with negative depth images to measure how closely the RGB image matches with its paired depth among others in the batch:

\begin{equation}
\resizebox{\columnwidth}{!}{$
\mathcal{L}_{{D}\rightarrow{I}} = -\dfrac{1}{|\mathcal{B}|} \sum_{(\mathbf{I},\mathbf{D}) \in \mathcal{B}} log \dfrac{exp(S(\mathbf{I},\mathbf{D}))}{exp(S(\mathbf{I},\mathbf{D})) + \sum_{(\mathbf{I}',\mathbf{D}') \in \mathcal{B}} exp(S(\mathbf{I},\mathbf{D}'))}$}
\end{equation} 

The second component of the NCE loss, $\mathcal{L}_{{I} \rightarrow {D}}$, is analogously defined to contrast a depth image with negative RGB image samples, and
%to measure how closely the depth image matches with its RGB image pair among the others in the batch:
% \begin{equation}
% \resizebox{\columnwidth}{!}{
%     \mathcal{L}_{{I} \rightarrow {D}} = -\dfrac{1}{|\mathcal{B}|} \sum_{(\mathbf{I},\mathbf{D}) \in \mathcal{B}} log \dfrac{exp(S(\mathbf{I},\mathbf{D}))}{exp(S(\mathbf{I},\mathbf{D})) + \sum_{(\mathbf{I}',\mathbf{D}') \in \mathcal{B}} exp(S(\mathbf{I}',\mathbf{D}))}}
% \end{equation} 
the two components are averaged:
\begin{equation}
    \mathcal{L}_{NCE} = (\mathcal{L}_{{D} \rightarrow {I}} + \mathcal{L}_{{I} \rightarrow {D}}) / 2
\end{equation}

\subsection{Late Fusion of the Modalities}
\label{sec:fusion}

The detection and classification scores computed from RGB and depth modalities are imbued with disparate and complementary details that jointly enrich our understanding of the target objects. Therefore, we combine these scores to amplify the performance of object detection.

As the depth images are derived from the RGB images, the spatial arrangement of the objects is equivalent in both modalities. Hence, we utilize the same visual region proposals for both RGB and depth modalities. Following the application of the RoI pooling layer and the Siamese box feature extractor to the depth feature map $\psi(\mathbf{D})$, we obtain the feature vector $\phi(d_i)$ for each depth region. Thereafter, we employ the approach presented in Eq.~\ref{eq:1} to derive the depth detection score $d^{det}_{i,c} \in \mathbb{R}^1$ and the depth classification score $d^{cls}_{i,c} \in \mathbb{R}^1$. Subsequently, we fuse (sum) the scores from the RGB and depth modalities:
\begin{equation}
  f^{det}_{i,c} = v^{det}_{i,c} + d^{det}_{i,c}, \quad f^{cls}_{i,c} = v^{cls}_{i,c} + d^{cls}_{i,c}
\label{eq:fusion}
\end{equation}
where $f^{det}_{i,c}$ and $f^{cls}_{i,c}$ are fusion detection and classification scores, respectively.

Following the WSDDN \cite{bilen2016weakly} architecture, these classification and detection scores are converted to probabilities such that $p^{cls}_{i,c}$ is the probability that class $c$ is in present proposal $f_i$, and $p^{det}_{i,c}$ is the probability that $f_i$ is important for predicting image-level label $y_c$.

\begin{equation}
   p^{det}_{i,c} = \dfrac{exp(f^{det}_{i,c})} {\sum_{k=1}^{R} exp(f^{det}_{k,c})}, \quad p^{cls}_{i,c} = \dfrac{exp(f^{cls}_{i,c})} {\sum_{k=1}^{C} exp(f^{cls}_{i,k})}
   \label{eq:7}
\end{equation}

We element-wise multiply the classification and detection scores to obtain the combined score $p^{comb}_{i,c}$:
%Finally, visual aggregated image-level predictions $\hat{vp}_c$ are computed as follows, where greater values of $\hat{vp}_c\in [0, 1]$  mean higher likelihood that $c$ is present in the image.
\begin{equation}
  p^{comb}_{i,c} = p^{det}_{i,c} p^{cls}_{i,c}
  %, \quad  \hat{vp}_c = \sigma\left(\sum_{i=1}^{M}vp^{comb}_{i,c} \right)
  \label{eq:8}
\end{equation}

Finally, image-level predictions $\hat{p}_c$ are computed as follows, where greater values of $\hat{p}_c\in [0, 1]$ mean a higher likelihood that $c$ is present in the image.
\begin{equation}
  \hat{p}_c = \sigma\left(\sum_{i=1}^{R}p^{comb}_{i,c} \right)
  \label{eq:confidence}
\end{equation}
Assuming the label $y_c$ = 1 if and only if class $c$ is present, the classification loss used for training the model is defined as follows. Since no region-level labels are provided, we must derive region-level scores indirectly, by optimizing this loss.
\begin{equation}
    \mathcal{L}_{mil} = -\sum_{c=1}^{C}\left[ y_c\log\hat{p}_c + (1-y_c)\log(1-\hat{p}_c) \right]
    \label{eq:visual_loss}
\end{equation}

\subsection{Depth Priors}
\label{sec:depth}

We utilize the baseline WSOD methods, which we aim to improve, to generate bounding box object predictions in the training set. Further, we leverage both the generated bounding box predictions and associated captions to extract knowledge about the relative depths of objects. We note that our proposed methodology adheres to the WSOD setting, deriving benefits from the predicted bounding boxes, as opposed to ground truth bounding box annotations to calculate depth priors. We subsequently exploit these depth priors to guide the identification of the relevant visual regions that may contain the target objects.
%We note that the depth priors we estimate vary very little whether we use 1\%, 5\%, 10\%, or 50\% of the available COCO training data; see Table \ref{Table:4}. 
Further, we show that even though we estimate the depth priors from COCO, they generalize to Conceptual Captions (Table \ref{Table:2}).

We use the notation $pd_i \in [0, 1]$, $i \in {1, . . . , R}$, where $R$ is the number of pre-computed region proposals for depth image $\mathbf{D}$, to represent the average depth value in the $i$-th region proposal. Each region proposal contains pixels with values ranging from 0 to 1, which correspond to the smallest and largest depth values, respectively. %The average depth values in the pixels across each region proposal are then calculated to obtain $pd_i$.

\begin{figure}[t]
\includegraphics[width= \columnwidth]{prior.png} 
\caption{The figure displays a row of images that are accompanied by their respective depth and caption data, as well as proposal depth value of different regions and estimated depth prior range.}
\label{fig:prior}
\end{figure}

We employ bounding box predictions $B$ to approximate the depth value of objects using the caption that describes the image in which the objects are present. 
{We also use co-occurring captions to capture the context in which an object occurs, and condition depth priors on this context which varies across images.}
Let $C$ be the set of object categories, $W$ be the set of distinct words in the vocabulary that includes every word in the captions, and $B$ be the set of predicted bounding boxes. Let $d_{c,w,b} \in \{[0, 1], \varnothing\}$ denote the depth value for object $c \in C$, word $w \in W$ and box $b \in B$, which is calculated by averaging the depth values in the pixels of $b$ similar to the calculation of $pd_i$.  As an example, $d_{\text{bird},\text{ocean}, b}$ represents the depth value of the ``bird" object box $b$ of a depth image that has a caption that includes the ``ocean" word. In the absence of ``ocean" in the caption or when annotation $b$ does not correspond to the ``bird" object, the depth value $d_{\text{bird},\text{ocean}, b}$ is set to null $\varnothing$. Further, $d_{c,w}$ represents a set of depth values calculated {by averaging $d_{c,w,b}$ over predicted boxes $b \in B$,} excluding $\varnothing$ values. The depth range $r_{c,w} = [mean - std, mean + std]$ for class $c$ and word $w$ is obtained by utilizing the mean and standard deviation (std) of this set of depth values in $d_{c,w}$. 
Once these depth ranges $r_{c, w}$ are computed, they can be applied to estimate an allowable depth range for a class $c$ in a new image, without any boxes on that image.

%Suppose we have $r_{\text{bird},\text{ocean}} = [0.25, 0.83]$, it implies that if the depth image caption includes the "ocean" word and the image is supposed to include "bird" object, then the expected range of depth value in a region proposal for the "bird" object is between $0.25$ (closest to the camera) and $0.83$ (farthest from the camera). Any region proposal depth value $rd_i$ that falls outside this range is less likely to contain the "bird" object.

For any new depth image $\mathbf{D}$, the range of estimated depth priors for an object $c$ is $dr_c$:
%where $c \in C$. The computation of $dr_c$ is as follows:
\begin{equation}
dr_{c} = \dfrac{\sum_{s \in S}{r_{c,s}}}{|S|}
\label{eq:prior}
\end{equation}
where $S$ denotes the set of words in the caption corresponding to $\mathbf{D}$. {Thus the depth at which we expect to find objects in a particular image varies depending on the context provided by words in the corresponding caption.}
We \textbf{only require captions at training time.}
%Fig.~\ref{fig:prior} presents two images featuring a "bird" object, with different depths. In the first image, the object appears closer to the camera and has a proposal depth value of $rd_1 = 0.23$, while in the second image, the object appears farther from the camera and has a proposal depth value of $rd_1 = 0.51$. Images have different captions that describe the images so estimated depth priors $p_\text{bird}$ are calculated differently using these captions using in Eq.~\ref{eq:prior}. The caption of the first image includes "feeding" and "hand" words which are the cues that the "bird" object is likely to have a smaller depth value so the estimated depth prior range $p_\text{bird}$ is relatively smaller with the range of $[0.15,0.48]$. The caption of the second image includes "flying" and "ocean" words which are the cues that the "bird" object is likely to have a bigger depth value so the estimated depth prior range $p_\text{bird}$ is relatively larger with the range of $[0.35,0.68]$. Note that while the regions having proposal depth value of $rd_1$ on the images are in the estimated depth prior range because they include "bird" object, the regions having proposal depth value of $rd_2$ are out of the range because they are unrelated regions for "bird" object.

We utilize the estimated depth prior range $dr_c$ to identify potentially important regions in $pd$ for each class. We define a depth mask indicator variable $m_{i,c} \in {0,1}$ for each region $i$ $\in$ $R$ and class $c$ $\in$ $C$, which indicates the likelihood of a particular region in an image containing an object of a certain class. The computation of this variable is as follows:
\begin{equation}
    m_{i,c}= 
\begin{cases}
    1,& \text{if } pd_i \in dr_c\\
    0,              & \text{otherwise}
\end{cases}
\label{eq:mask}
\end{equation}
If the proposal depth value $pd_i$ falls within the estimated depth prior range $dr_c$ for class $c$, it is considered as a relevant region for that class, and the corresponding mask variable $m_{i,c}$ is set to 1; otherwise, it is set to 0. Subsequently, we utilize the mask variable $m_{i,c}$ in combination with our end-to-end network to improve its performance.

As an example, Fig.~\ref{fig:prior} presents two images featuring a ``bird" object, with different depths. The estimated depth prior ranges $dr_\text{bird}$
%$p_\text{bird}$ 
are calculated using Eq.~\ref{eq:prior} for each image based on the words in the caption. The caption of the first image includes ``feeding" and ``hand" which suggest the ``bird" is likely to have a smaller depth, while the caption of the second image includes ``flying" and ``ocean" that suggest the ``bird" is likely to have a bigger depth. The regions on the images having a proposal depth value of $pd_1$ are in the estimated depth prior range $dr_\text{bird}$; we observe that they truly include the ``bird" object. 
The range allows us to rule out regions with values $pd_2$, which do not contain ``bird''.


{ 
\textbf{Alternative method for estimating depth priors.}
As an alternative, we use only bounding box predictions (without captions) to obtain depth priors. 
%Let $C$ be the set of object categories, and $B$ be the set of predicted bounding boxes. 
Let $d_{c,b} \in [0, 1]$ denote the depth value for object $c \in C$ and box $b \in B$. Further, 
let $d_{c}$ represent a set of depth values for each $c$. The depth range $r_{c} = [mean - std, mean + std]$ is obtained by utilizing the mean and standard deviation (std) of this set of depth values in $d_{c}$. Then we set $dr_c = r_c$ as we do not use caption information (compared to Eq.~\ref{eq:prior}); $dr_c$ is used in Eq.~\ref{eq:mask}.
}


\subsubsection{Depth Priors: Updated OICR}
\label{sec:depth_oicr}
\newcommand\sForAll[2]{ \ForAll{#1}#2\EndFor} % snappy version of \ForAll...\EndFor
\newcommand\sIf[2]{ \If{#1}#2\EndIf}
\begin{algorithm}
	\caption{OICR Mining with Depth Priors} 
        Input: Proposals $R$, Depth Mask Indicator Variable $m$\\
        Output: Pseudo boxes $\hat{R}$
	\begin{algorithmic}[1]
        \State $\hat{R} = \varnothing$ 
        \sForAll {$c=1:C$}{
            \sForAll {$i=1:|R|$}{
                \State $\hat{R_c} = \hat{R_c} \cup R_i $ \textbf{if} $m_{i,c} = 1$
            }
        }
        \State \textbf{return} $\hat{R}$
	\end{algorithmic} 
\label{algorithm:algo}
\end{algorithm}

 Online Instance Classifier Refinement (OICR) \cite{tang2017multiple} is a weakly supervised object detection algorithm that iteratively refines object proposals.  Recent studies \cite{tang2018pcl,ren2020instance,sui2022salvage} have highlighted the importance of more effective proposal mining strategies for achieving better recall and precision of objects in WSOD detectors. We propose an algorithm that incorporates the depth priors during the proposal mining provided in Alg.~\ref{algorithm:algo}. As our proposed method aims to enhance MIL-based WSOD methods, we utilize our algorithm in conjunction with recent OICR-style/self-training/mining strategies, subject to the depth prior condition specified in the fourth line of Alg.~\ref{algorithm:algo}. After using the depth prior condition, OICR-style mining selects fewer but more relevant proposals so our contribution increases mining precision.\footnote{In early experiments, we verified our method's gains persist if the baseline %is allowed to 
 drops the lowest-scoring pseudo boxes without using depth.} 

 \subsubsection{Depth Priors: Attention}
 \label{sec:depth_attention}
The depth mask variable $m_{i,c}$ indicates the potentially important proposal regions for each class. We use this variable to employ an attention mechanism with combined score probabilities $p^{comb}_{i,c}$ provided in Eq. ~\ref{eq:8} as follows:
\begin{equation}
    %p^{attention}_{i,c}= 
    p^{comb}_{i,c}= 
%\begin{cases}
    p^{comb}_{i,c} * 0.5, \text{if } m_{i,c} = 0
%    p^{comb}_{i,c},              & \text{otherwise}
%\end{cases}
\end{equation}
This mechanism reduces the probability of a region for class $c$ by half if the region is determined as less likely to be important by $m_{i,c}$. These scores are then used in Eq.~\ref{eq:confidence}.


 