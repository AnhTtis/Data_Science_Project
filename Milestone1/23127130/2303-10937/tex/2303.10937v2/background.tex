\section{Background}

The following section will provide an overview of MIL-based WSOD, which will serve as the background for introducing our proposed method in the subsequent sections.

Specifically, let denote $\mathbf{I} \in \mathbb{R}^{h\times w \times 3}$ as an RGB image, $y_c \in \{0, 1\}$ where $c \in \{1, . . . , C\}$ as its corresponding ground-truth class labels
%, and $\mathbf{R} \in \mathbb{R}^{4\times N }$ as the precomputed object proposals. 
where C is the total number of object categories. Let $v_i$ where $i \in \{1, . . . , R\}$ to denote the visual regions $\mathbf{v}$ of a given image $\mathbf{I}$ where R is the number of proposals. With the help of a pre-trained backbone model, the feature map $\psi(\mathbf{I})$ is extracted, and proposal feature vectors are extracted by an RoI pooling layer. Finally, a box feature extractor is applied to extract a fixed-length feature vector $\phi(v_i)$ for each visual region. The proposal features $\phi(v_i)$ are fed into two parallel fully-connected layers to compute the visual detection score $v^{det}_{i,c} \in \mathbb{R}^1$ and classification score $v^{cls}_{i,c} \in \mathbb{R}^1$: %which are dependent to particular class c and visual region $v_i$:

\begin{equation}
   v^{det}_{i,c} = w^{det\intercal}_{c} \phi(v_i) + b^{det}_{c}, \quad v^{cls}_{i,c} = w^{cls\intercal}_{c} \phi(v_i) + b^{cls}_{c}
   \label{eq:1}
\end{equation}

 where $w$ and $b$ are weight and bias parameters, respectively.


 
 %These classification and detection scores are converted to probabilities such that $vp^{cls}_{i,c}$ is the probability that class $c$ is in present proposal $v_i$, and $vp^{det}_{i,c}$ is the probability that $v_i$ is important for predicting image-level label $y_c$.

%\begin{equation}
%   vp^{det}_{i,c} = \dfrac{exp(v^{det}_{i,c})} {\sum_{k=1}^{N} exp(v^{det}_{k,c})}, \quad vp^{cls}_{i,c} = \dfrac{exp(v^{cls}_{i,c})} {\sum_{k=1}^{C} exp(v^{cls}_{i,k})}
%   \label{eq:2}
%\end{equation}

%Element-wise multiplication of classification and detection score probabilities results in $vp^{comb}_{i,c}$. Finally, visual aggregated image-level predictions $\hat{vp}_c$ are computed as follows, where greater values of $\hat{vp}_c\in [0, 1]$  mean higher likelihood that $c$ is present in the image.

%\begin{equation}
%  vp^{comb}_{i,c} = vp^{det}_{i,c} vp^{cls}_{i,c}, \quad  \hat{vp}_c = \sigma\left(\sum_{i=1}^{M}vp^{comb}_{i,c} \right)
%  \label{eq:confidence}
%\end{equation}

%Assuming the label $y_c$ = 1 if and only if class $c$ is present, the visual classification loss used for training the model is defined as follows. Again, since no region-level labels are provided, region-level scores are derived indirectly, by optimizing this loss.

%\begin{equation}
%    \mathcal{L}_{mil} = -\sum_{c=1}^{C}\left[ y_c\log\hat{vp}_c + (1-y_c)\log(1-\hat{vp}_c) \right]
%    \label{eq:visual_loss}
%\end{equation}