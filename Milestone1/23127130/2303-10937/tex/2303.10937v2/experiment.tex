\section{Experiments}

We test our method on top of two weakly-supervised detection techniques, and verify the contributions of  the constituents of our approach:

\begin{itemize}[nolistsep,noitemsep]
\item Siamese WSOD Network (\textsc{Siamese-Only}, Sec.~\ref{sec:siamese_module});
\item Late Fusion of the Modalities (\textsc{Fusion}, Sec.~\ref{sec:fusion}) which combines classification/detection from RGB and depth, and builds on top of the Siamese WSOD Network (Sec.~\ref{sec:siamese_module});
%an improvement upon \textsc{Siamese-Only};
\item Depth Priors are utilized to enhance the OICR-style module (\textsc{Depth-Oicr}, Sec.~\ref{sec:depth_oicr}) and construct attention (\textsc{Depth-Attention}, Sec.~\ref{sec:depth_attention}) with visual-only score probabilities, both building upon the Siamese WSOD Network (Sec.~\ref{sec:siamese_module}); 
%Both of these methods are improvements upon the \textsc{Siamese-Only};
\item Finally, we use all components of our method (\textsc{Wsod-Amplifier, Sec.~\ref{sec:siamese_module}, \ref{sec:fusion}, \ref{sec:depth_oicr}, \ref{sec:depth_attention}}).
\item {\textsc{Depth-Oicr-Alt} and \textsc{Depth-Attention-Alt} estimate depth priors without captions.}
\item {\textsc{Wsod-Amplifier-Inf} fuses RGB and depth at inference time, unlike our proposed method.}
\end{itemize}

\subsection{Experimental Setup}
%\subsubsection{Dataset and Metrics}

%We conduct experiments using three datasets, namely PASCAL VOC, COCO, and Conceptual Captions. 

\textbf{PASCAL Visual Object Classes 2007 (VOC-07)} \cite{everingham2009pascal} contains 20 classes. For training, we use 2501 images from the train set and 2510 images from the validation set. We evaluate using 4952 images from the test set.

\textbf{Common Objects in Context (COCO)} \cite{lin2014microsoft} consists of 80 classes. 
%To train our models with ground truth labels, 
We utilize approximately 118k images from the train set and use the labels provided at the image level. Additionally, to test how well our method works when labels are obtained from noisy language supervision (in captions), we train our models using labels obtained through an exact match (EM) method following \cite{unal2022learning}, also referred to as substring matching in \cite{fang2022data}. Due to the unavailability of any labels for around 15k images extracted from captions, we excluded them from the training set and use 103k images. We evaluate using 5k images from the validation set.

\textbf{Conceptual Captions (CC)} \cite{sharma2018conceptual} is a large-scale image captioning dataset containing over 3 million images annotated with only captions. We use around 30k images and their corresponding captions and the labels are extracted for the 80 COCO dataset classes using an exact match method from the captions. During the evaluation, we used 5k images from the COCO validation set.

\textbf{Domain shift datasets.} 
{In the supplementary}, we also evaluate our method in a domain shift setting \cite{inoue2018cross}, using three datasets. Clipart1k has the same 20 classes as VOC with 1,000 images, while Watercolor2k and Comic2k share 6 classes with VOC and have 2,000 images each. %We use these datasets for evaluation only. 

\textbf{Evaluation protocols.} We utilize mean Average Precision (mAP) considering various IoU thresholds as the common evaluation metric for COCO and VOC datasets. Additionally, we report mAP for objects of different sizes during COCO evaluation and we report the results of Correct Localization (CorLoc) for VOC evaluation.

\textbf{Implementation details.}
We employ the official PyTorch implementations of SoS-WSOD \cite{sui2022salvage} and MIST \cite{ren2020instance} methods to apply our amplifier technique. SoS-WSOD uses four images per GPU as two augmented images and their flipped versions with a total of 4 GPUs, whereas MIST uses only one image per GPU with a total of 8 GPUs. However, we use one image per GPU for SoS-WSOD due to VRAM limitation in our GPUs, as we also utilized depth images for each corresponding RGB image. Therefore, the baseline results of SoS-WSOD reported in Table ~\ref{Table:1} are slightly lower than those reported in the original paper. Moreover, we solely use the first stage of SoS-WSOD since it includes the MIL-based WSOD module which is convenient to implement our method on top of. The other settings are kept the same as the official implementations with the VGG16 backbone. The inference is done on the training set by using baseline MIST and SoS-WSOD methods to obtain box predictions having confidence scores higher than 0.5. These box predictions are then used to calculate depth priors. Furthermore, we utilize the same depth range $r_{c,w}$ from the COCO annotations for the \textsc{Wsod-Amplifier} method on the Conceptual Captions dataset. 

\subsection{Comparing our amplifier to state of the art}

\input{table_gt_coco_voc}

We evaluate our proposed methods, \textsc{Fusion} and \textsc{Wsod-Amplifier}, using two state-of-the-art WSOD approaches, SoS-WSOD \cite{sui2022salvage} and MIST \cite{ren2020instance}, and the COCO and VOC-07 datasets. The performance of our proposed methods are compared with the baseline methods in Table ~\ref{Table:1}. 
When our \textsc{Wsod-Amplifier} method is applied to MIST, it improves the baseline performance by $17\%$ in $mAP_{50:95}$ (relative gain, 13.8/11.8-1) and $14\%$ in $mAP_{50}$. Similarly, when our \textsc{Wsod-Amplifier} method is applied to SoS-WSOD, it improves the baseline performance by $2\%$ in $mAP_{50:95}$, $1.5\%$ in $mAP_{50}$, and $6\%$ in $mAP_{75}$. As the VOC-07 dataset does not have 
%ground truth box annotations and 
captions, we are only able to apply the \textsc{Siamese-Only} and \textsc{Fusion} methods on SoS-WSOD but not the \textsc{Depth-Oicr} and \textsc{Depth-Attention}. On this dataset, our improvements outperformed the baseline SoS-WSOD by $5\%$ in $mAP_{50:95}$, $2\%$ in $mAP_{50}$, and $9\%$ in $mAP_{75}$. %Moreover, our methods  enhance the $mAP$ results for objects with different sizes.
{\textsc{Wsod-Amplifier-Inf} performs worse than \textsc{Wsod-Amplifier}. We argue depth is useful as a soft guide to balance region information during MIL training, but less so when directly used in the strict detection setting.}

\input{table_em_coco_cc}


\subsection{Ablation studies and visualization}
\textbf{Experiments with labels from captions.} Several attempts \cite{ye2019cap2det, unal2022learning,fang2022data} have been made to eliminate the requirement for image-level labels by leveraging noisy label information obtained from captions or subtitles. Although it is cost-effective to use text information for label extraction, it results in a decrease in the performance of weakly supervised object detection. \cite{unal2022learning} propose a text classifier approach to extract labels more effectively than the simple exact match (EM) and reduce the noise between text and ground truth (GT) labels. In contrast to previous studies, our research employs the depth modality to reduce the noise in labels extracted from captions. Our approach improves the model's detection capability and employs captions during the calculation of depth priors. We conducted experiments with MIST \cite{ren2020instance} using both GT and EM labels and observed that, as expected, training with GT labels leads to significantly better performance than training with EM labels in Table ~\ref{Table:2} due to the noise in labels extracted from captions. However, our proposed \textsc{Wsod-Amplifier} method applied on MIST with EM labels surpasses the baseline and MIST with GT labels. These findings demonstrate that our method \emph{effectively reduces noise} and enables the model trained with EM labels to achieve better performance than those trained with GT labels. It is worth noting that the text classifier approach proposed by \cite{unal2022learning} also performs better than EM-labeled training data, but falls short of the performance achieved by GT-labeled data.

\textbf{Results on noisy datasets.}
We also extract labels from captions on the Conceptual Captions dataset, which lacks labels at the image level. We observe that our \textsc{Wsod-Amplifier} boosts results by an impressive 63\% relative gain using $mAP_{50}$. 
Conceptual Captions is a noisier dataset than COCO, since captions were not collected through crowdsourcing, but were crawled as alt-text for web search results. 
Thus, it is noteworthy that \emph{the benefit of our approach becomes more pronounced as the cost of supervision decreases, and the noise in the supervision increases.}





\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{vis.png}
    \caption{Qualitative comparison of MIST \cite{ren2020instance} (top) and our proposed \textsc{Wsod-Amplifier} method (bottom),  %implemented on MIST. 
    on COCO val. The ground-truth objects are vase, zebra, bicycle and car, sink and toilet, couch in this order. Confidence scores and names of objects shown. }
    \label{fig:qualitative}
\end{figure*}


%\input{table_anno_percent}


\textbf{Analysing the components of our approach.} To understand the impact of each component of our approach on the overall performance, we conducted experiments with MIST \cite{ren2020instance} using EM labels as a baseline and applied each component of our method on top of the baseline in Table ~\ref{Table:2}. Our \textsc{Siamese-Only} method, which incorporates the depth modality in the Siamese network using contrastive learning, improves feature extraction and results in a $4\%$ increase in $mAP_{50:95}$ and $mAP_{50}$. Our \textsc{Depth-Oicr} method, which utilizes depth priors in the OICR module to improve the mining strategy, increases $mAP_{50:95}$ and $mAP_{50}$ over \textsc{Siamese-Only} by $6-8\%$ on COCO and $42-47\%$ on Conceptual Captions (CC). Our \textsc{Depth-Attention} method, which incorporates depth priors to use potentially important regions in an attention mechanism with combined score probabilities, increases $AP_{50:95}$ and $mAP_{75}$ by $6-7\%$. Our \textsc{Fusion} method, which combines RGB and depth image scores, improves by $14-16\%$ on COCO and $7-21\%$ on CC. 
Comparing \textsc{Fusion} and \textsc{Depth-Oicr}, the bigger gain using $mAP_{50}$ is achieved by \textsc{Fusion} on COCO, and \textsc{Depth-Oicr} on CC. \emph{Thus, the benefit of our WSOD-specific method component increases as the noise in the dataset increases, which is appealing due to its real-world applicability.} 
Finally, our \textsc{Wsod-Amplifier} method, which includes all components of our approach, achieves the highest performance increase over MIST w/ EM baseline, with \emph{improvements in all $mAP$ metrics by $16-20\%$ on COCO and $53-65\%$ on CC.} 
%It is notable that our \textsc{Wsod-Amplifier} method has shown better performance in improving the MIST with EM labels in Table ~\ref{Table:2}, with an improvement of $18-20\%$, compared to the improvement of $14-17\%$ in the GT labels setting shown in Table ~\ref{Table:1}. Furthermore, 

{
\textbf{Alternative depth priors.} 
%In Sec.~\ref{sec:depth}, we describe a method for estimating depth priors without captions. 
At the bottom of Table \ref{Table:2} (COCO), we see that \textsc{Depth-Oicr-Alt} and \textsc{Depth-Attention-Alt}, derived from the alternative approach, yield superior results compared to \textsc{Siamese-Only}. However, the \textsc{Depth-Oicr} and \textsc{Depth-Attention} methods, derived from our full (caption-based) approach, outperform both \textsc{Depth-Oicr-Alt} and \textsc{Depth-Attention-Alt}. Note that the depth range $r_{c}$ remains consistent across all images in this alternative method. Conversely, in the proposed approach, the depth range $dr_{c}$ is computed individually for each image by taking into account the corresponding caption, as visualized in Fig.~\ref{fig:prior}. As a result, the proposed approach demonstrates enhanced capacity in modeling depth priors through the utilization of captions. }

\textbf{Generalization of depth priors.} {Even though on CC we use the depth priors calculated from COCO}, our proposed method exhibits a more substantial enhancement in CC performance compared to COCO, achieving an improvement of $63\%$ $mAP_{50}$ over the MIST baseline (50\% improvement from \textsc{Depth-Oicr} alone). 
%This is due to the fact that CC has more noisy captions. 
{Thus, our \textsc{Depth-Oicr} demonstrates generalization, as it has a higher impact than \textsc{Fusion} on CC (without recomputing priors), in contrast to COCO.}
%, owing to the presence of noisier captions in CC. 
Given the recent interest in learning from vision-language data, our approach has the potential to be highly impactful.
{Further, we compared the priors estimated from different datasets, and found them to be similar. In particular, 82.3\% of PASCAL objects fit within the $[mean-stdev, mean+stdev]$ range computed from COCO, and 84.4\% when the range is computed on PASCAL itself; the cross-domain gap in the range is small.}


%\textbf{Analyzing the depth priors calculated with varying percentages of GT annotations.} In this section, we examine how the proportion of ground truth (GT) annotations affects the ability to estimate the relative depth range of objects.

%\textbf{Analyzing the depth priors calculated with varying percentages of GT annotations.} In this section, we examine how the proportion of ground truth (GT) annotations affects the ability to estimate the relative depth range of objects. Table ~\ref{Table:4} presents the average depth ranges and standard deviations for ten objects at various annotation percentages. Across all 80 objects, the average difference between the upper and lower bounds is $0.26$, while the average standard deviation is a mere $0.01$. This indicates that the standard deviation is significantly lower than the average range difference. Although we utilize $5\%$ percent of annotations as default in our experiments, our findings suggest that even utilizing only $1\%$ of GT annotations is sufficient to approximate depth priors successfully.

\textbf{Qualitative analysis.} We visualize the object detection performance of our proposed \textsc{Wsod-Amplifier} compared to MIST \cite{ren2020instance} in Fig.~~\ref{fig:qualitative}. 
%Because we only use RGB images during inference, 
The confidence scores are calculated using visual detection $v^{det}$ and classification scores $v^{cls}$. We show boxes with scores higher than 0.5. In the first image, the baseline struggles to accurately identify multiple instances of the same ``vase" objects, instead grouping them together in a single box. %with a high score. 
Our method overcomes this challenge, precisely detecting each individual ``vase". In the second image, the baseline faces the problem of part domination due to some discriminative parts of a ``zebra". Our method overcomes this issue by utilizing depth modality during training, which emphasizes the geometric variations of objects, while comparatively ignoring the complex background.
%, unlike RGB modality that focuses on color intensity. 
In other images, unlike our method, the baseline misses objects entirely, or produces large and imprecise bounding boxes.
%, whereas our method performs better in detecting objects. 
Moreover, the boxes detected by our method tend to have higher prediction scores.






% ----- OLD TEXT

%\begin{table}[t]
%\centering
%\resizebox{\columnwidth}{!}{
%\begin{tabular}{ lcccccc}
%\toprule
% & \multicolumn{3}{c}{Avg. Precision, IoU}  & \multicolumn{3}{c}{Avg. Precision, Area} \\
% \cmidrule{2-4} \cmidrule{5-7}

%Methods on CC & 0.5:0.95 & 0.5 & 0.75 & S & M & L  \\
%\midrule 
%\textsc{MIST \cite{ren2020instance}}  &1.7&3.8&1.4 & %0.3 & 1.7 & 3.4 \\

%\textsc{+ Fusion} & 2.0 & {4.1} & {1.7} & {0.3} & {1.9} & {4.0} \\

%\textsc{+ Depth-Oicr} & {2.5} & {5.7} & {{2.0}} & {0.4} & {2.1} & {5.3} \\
%\textsc{+ Wsod-Amplifier} & \bf{{2.6}} & \bf{{6.2}} & \bf{2.0} & \bf{{0.4}} & \bf{{2.7}} & \bf{{5.6}}  \\
%\bottomrule
%\hline \\
%\end{tabular}}
%\caption{This table introduces the effect of each component of our method implemented on MIST \cite{ren2020instance} with exact match (EM) labels. The best performer per column is in \textbf{bold}, and all of our proposed methods that outperform the \textsc{Siamese-Only} are \underline{underlined}.} % for visual methods.} 
%\label{Table:3}
%\end{table}

%\textbf{Analysing the components of our approach.} To understand the impact of each component of our approach on the overall performance, we conducted experiments with MIST \cite{ren2020instance} using EM labels as a baseline and applied each component of our method on top of the baseline. Our \textsc{Siamese-Only} method, which incorporates the depth modality in the siamese network using contrastive learning, improves feature extraction and results in a $4\%$ increase in $AP_{50:95}$ and a $4\%$ increase in $AP_{50}$. Our \textsc{Depth-Oicr} method, which utilizes depth priors in the OICR module to improve the mining strategy, increases $AP_{50:95}$ and $AP_{50}$ by $7\%$ and $8\%$, respectively. Our \textsc{Depth-Attention} method, which incorporates depth priors to use potentially important regions in an attention mechanism with combined score probabilities, increases $AP_{50:95}$ and $AP_{75}$ by $7\%$ and $6\%$, respectively. Our \textsc{Fusion} method, which combines RGB and depth image scores, achieves the highest performance increase among the other components over baseline, with improvements of $16\%$ in $AP_{50:95}$, $14\%$ in $AP_{50}$, and $16\%$ in $AP_{75}$. Finally, our \textsc{Wsod-Amplifier} method, which includes all components of our approach, achieves the highest performance increase over MIST w/ EM baseline, with improvements of $20\%$ in $AP_{50:95}$ and $18\%$ in $AP_{50}$. It is worth noting that while our \textsc{Wsod-Amplifier} method improves the performance of MIST with GT labels setting by $14-17\%$ in Table ~\ref{Table:1}, the same method improves the performance of MIST with EM labels setting by $18-20\%$ in Table ~\ref{Table:2}. Our approach could be impactful given the recent interest in learning from Visual-Languge data.

