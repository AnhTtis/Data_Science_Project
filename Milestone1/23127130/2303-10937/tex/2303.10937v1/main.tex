\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm} 
\usepackage{algorithmicx}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage[noend]{algpseudocode}
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%
%\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
%\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

% Include other packages here, before hyperref.
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{2213} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Boosting Weakly Supervised Object Detection using Fusion and Priors from Hallucinated Depth}

\newcommand*{\affaddr}[1]{#1} % No op here. Customize it for different styles.
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}



\author{%
Cagri Gungor\affmark[1] and Adriana Kovashka\affmark[1,2]\\
\affaddr{\affmark[1]Intelligent Systems Program}, \affaddr{\affmark[2]Department of Computer Science}\\
\affaddr{University of Pittsburgh}\\
{\tt\small cagri.gungor@pitt.edu}, {\tt\small kovashka@cs.pitt.edu}
%\affaddr{ \small \url{https://cagrigungor.github.io/AudioVisualWSOD/}}%
}

\maketitle
% Remove page # from the first page of camera-ready.
%\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
Despite recent attention and exploration of depth for various tasks, it is still an unexplored modality for weakly-supervised object detection (WSOD). We propose an amplifier method for enhancing the performance of WSOD by integrating depth information. Our approach can be applied to any WSOD method based on multiple-instance learning, without necessitating additional annotations or inducing large computational expenses. Our proposed method employs a monocular depth estimation technique to obtain hallucinated depth information, which is then incorporated into a Siamese WSOD network using contrastive loss and fusion. By analyzing the relationship between language context and depth, we calculate depth priors to identify the bounding box proposals that may contain an object of interest. These depth priors are then utilized to update the list of pseudo ground-truth boxes, or adjust the confidence of per-box predictions. Our proposed method is evaluated on six datasets (COCO, PASCAL VOC, Conceptual Captions, Clipart1k, Watercolor2k, and Comic2k) by implementing it on top of two state-of-the-art WSOD methods, and we demonstrate a substantial enhancement in performance.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{concept.png}
    \caption{Depth ranges of objects are estimated using depth-language relationship before training. These ranges are further used to spot relevant visual proposals that may contain target objects to enhance weakly supervised object detection. }
    %detector trained with noisy labels.}
    \label{fig:concept}
\end{figure}

Weakly-supervised object detection (WSOD) is a challenging task since it is unclear which instances have the label that was provided at the image level. Traditional methods only use appearance information in RGB images. However, appearance information is insufficient to localize objects in complex, cluttered environments. On the other hand, humans are capable of finding useful information in 
complex environments because they rely on object function, not just appearance.
For example, they might reason about which objects are within reach, which can be captured with depth from stereo vision \cite{cammack2016depth}. 
The depth modality provides additional cues about the spatial relationships and geometrical structure of objects in a scene and is invariant to appearance variations (e.g. in texture), making it complementary to the RGB modality. However, weakly-supervised object detection methods do not use depth information.

We equip WSOD methods with the ability to reason about functional information (depth). Importantly, our method does so without requiring additional annotations or suffering significant computational costs. We propose an amplifier method that can enhance the performance of any weakly supervised object detection method based on multiple-instance learning. Since traditional WSOD datasets do not contain ground-truth depth information, the proposed method utilizes hallucinated (predicted) depth information obtained through a monocular depth estimation technique. During training, the method incorporates depth information to improve representation learning and to prune or down-weight predictions at the box level, which leads to improved object detection performance during inference.

First, depth can directly be used as a feature to aid in representation learning, or to produce predictions which can be fused with those computed from RGB. While simple, this technique has not been used for WSOD, and we show that it is very effective: it boosts the performance of appearance-only methods by up to 2.6 mAP@0.5 (11\% relative gain).



Further, depth can provide strong priors about which of the bounding box proposals in the noisy WSOD setting contain an object of interest. We examine the rough depth at which objects of particular categories occur, by computing the depth range of an object in 1-5\% of annotated images. To make this range more precise, we examine the relationship between language context and depth, by keeping track of depth range statistics conditioned on co-mentioned objects and averaging across the most common co-occurring objects. We then use this range to prune the pseudo ground-truth bounding boxes used to iteratively update weakly-supervised detection methods, or to down-weight predictions on the box level.
This approach boosts WSOD performance further for a total of 14\% mAP@0.5 relative gain.

Our method is simple and can boost multiple WSOD methods that rely on iterative improvement. We test it in a variety of settings, using two state-of-art WSOD baselines, MIST \cite{ren2020instance} and SoS-WSOD \cite{sui2022salvage}, and five datasets, COCO, PASCAL VOC, Clipart1k, Watercolor2k, and Comic2k. 

Inspired by recent work that trains object detection methods with language supervision \cite{zareian2021open,unal2022learning,gao}, we further test our method in a setting where labels at the image level are not ground-truth but estimated. In this setting, our method boosts the basic WSOD performance even more, by 18\% when labels for training are extracted from COCO, and 63\% when they are extracted from Conceptual Captions.

To summarize, our contributions are: 
%\begin{itemize}[nolistsep,noitemsep]
    (1) We examine for the first time the use of depth in weakly-supervised object detection.
    (2) In addition to depth fusion, we propose a technique specific to WSOD, which estimates depth priors with the help of language, and uses them to refine pseudo boxes and box predictions. 
    (3) We show large performance gains in a large variety of settings, with the biggest boost from depth refinement when supervision is least expensive.

\section{Related Work}
\textbf{Weakly-supervised object detection (WSOD).} 
WSOD is the task of learning to detect the location and type of multiple objects given only image-level labels during training. The multi-instance learning (MIL) framework is commonly utilized in WSOD methods, with early works such as WSDDN \cite{bilen2016weakly} integrating MIL into an end-to-end WSOD system. OICR \cite{tang2017multiple} improved upon this by proposing pseudo-ground-truth mining and an online instance refinement branch, which was further refined by PCL \cite{tang2018pcl} through proposal clustering. C-MIL \cite{wan2019cmil} and MIST \cite{ren2020instance} introduced modifications to the MIL loss and pseudo-ground-truth mining, respectively. 
%UWSOD \cite{shen2020uwsod} addresses the scale variation challenge by using a multi-rate resampling pyramid, which aggregates contextual information at multiple scales. CASD \cite{huang2020comprehensive}, have proposed self-distillation along with attention to improving WSOD performance. 
More recent work, SoS-WSOD \cite{sui2022salvage}, proposes a method that produces pseudo boxes for FSOD by using their improved WSOD module, then they split noisy data for semi-supervised object detection.
Additionally, there have been efforts to bypass the need for image-level labels by utilizing noisy label information extracted from caption or subtitle data \cite{ ye2019cap2det,Chen_2017_CVPR,zareian2021open,unal2022learning,gao}.
%Furthermore, Gungor et al. \cite{gungor2023complementary} employed the audio modality to enhance the performance of WSOD while mitigating the noise generated from the label extraction process from text sources. 
In contrast to these works, our methodology leverages the incorporation of depth information as an additional modality, leading to improved performance in WSOD and a reduction of the noise between text and label information. 

\textbf{RGB-D detection.} 
The integration of RGB and depth information to derive complementary features has been previously studied for fully-supervised indoor analysis \cite{ying2022uctnet,zhou2022scale,jiao2019geometry,li2019mapnet} and salient object detection \cite{lee2022spsn,hussain2022pyramidal,feng2022encoder,fu2020jl, li2021joint}. Fusing the information contained in the RGB and depth modalities is crucial, as they provide complementary information. The strategies for merging the two modalities can be classified into three groups, depending on the point in the processing pipeline where the fusion occurs: early fusion \cite{fan2020rethinking,qu2017rgbd}, middle fusion \cite{fu2020jl,fu2021siamese, zhu2019pdnet, chen2018progressively}, and late fusion \cite{han2017cnns,piao2019depth}. Early fusion techniques involve combining the RGB and depth images into a single four-channel matrix at the earliest stage of the process. This integrated matrix is then treated as a single input. Middle fusion provides a balance between early and late fusion by utilizing CNNs for both feature extraction and subsequent merging. 
%A subsidiary network was utilized in \cite{zhu2019pdnet} to extract depth features, which were then used to improve the intermediate representation in an encoder-decoder architecture. 
In late fusion, individual saliency prediction maps are produced from the RGB and depth channels. These two predicted maps are then combined through post-processing operations such as element-wise summation or multiplication. 
%Han et al. \cite{han2017cnns} modified a deep neural network to work with both RGB and depth views and combined the output of these views using a fully-connected layer. 
In contrast to the majority of aforementioned methods, which use separate networks to extract features from RGB and depth images, several studies \cite{fu2020jl, fu2021siamese, song2021exploiting, meyer2020improving} employ Siamese networks to learn hierarchical features from both RGB and depth inputs by utilizing shared parameters.
%, with varying goals. 
%The studies in \cite{fu2020jl, fu2021siamese} utilize the middle-fusion strategy, while \cite{song2021exploiting} tackles the problem of increased model complexity and \cite{meyer2020improving} enhances the representation learning process through Siamese networks.
%In our study, we employ a Siamese network (shared backbone) to enhance the representation learning process through the use of contrastive learning between RGB and depth features. Additionally, this approach addresses the issue of increased model complexity. The Siamese network design allows us to make use of depth information during training while incurring little extra cost, while the depth is not used during inference. Additionally, we merge the detection and classification scores obtained from both RGB and depth modalities in a weakly supervised object detection network, which can be classified as a late fusion approach.
However, \emph{we are the first to leverage depth data in weakly-supervised object detection.} Our approach is not specific to a particular method, as it can be applied to any MIL-based WSOD method to improve its performance without incurring any extra annotation expenses and with minimal computational overhead during training. Although the depth modality is not used during the inference stage, incorporating it during training enhances the performance of the inference.

\textbf{Monocular depth estimation} involves predicting the depth map of a scene from a single RGB image \cite{miangoleh2021boosting,ranftl2020towards,ranftl2021vision,yin2021learning}.
%has focused on developing deep networks that can effectively leverage the rich visual information in RGB images to estimate depth. 
We utilize the method described in \cite{miangoleh2021boosting} due to its strong performance. 
%that boosts the results of \cite{yin2021learning} in order to estimate the depth information from RGB images. 
This estimated (``hallucinated'') depth information is then utilized to improve the performance of weakly supervised object detection.

\section{Approach}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{main_fig.png}
    \caption{This figure illustrates the design of our proposed amplifier technique that takes advantage of depth information to enhance the performance of other weakly-supervised object detection methods. During inference, we only use the RGB branch (shown in orange).}
 
    \label{fig:main_fig}
\end{figure*}

We propose an amplifier approach that incorporates a depth modality to improve the effectiveness of WSOD methods.
%that apply the MIL principle. 
Our method can be used with all MIL-based WSOD methods to boost their performance by incurring little extra cost during training.
It does not use the depth modality during inference to avoid any slow-downs and reliance on additional data (depth estimation or captions). The proposed approach comprises three main steps (Sec.~\ref{sec:siamese_module}, \ref{sec:fusion}, \ref{sec:depth}, respectively). First, a Siamese network with a shared backbone is employed to improve representation learning through contrastive learning between RGB and depth features (referred to as \textsc{Siamese-Only} in the experiments). Second, we combine detection and classification scores obtained from both RGB and depth modalities, which can be categorized as late fusion (\textsc{Fusion}). Third, we use captions and a small number of ground truth bounding box annotations to calculate depth priors.
%, which provide insight into the objects and their depth. 
These depth priors are then used to improve the OICR-style \cite{tang2017multiple} module in two WSOD methods (\textsc{Depth-OICR}) and create attention with combined score probabilities (\textsc{Depth-Attention}).
Note that \textsc{Siamese-Only} is always applied, while \textsc{Fusion}, \textsc{Depth-OICR} and \textsc{Depth-Attention} build on top of it, and can be used alone or combined.

\subsection{The Siamese WSOD Network}
\label{sec:siamese_module}

\textbf{WSOD.}
Following Bilen et al. \cite{bilen2016weakly}, let $\mathbf{I} \in \mathbb{R}^{h\times w \times 3}$ denote an RGB image, and $y_c \in \{0, 1\}$ (where $c \in \{1, . . . , C\}$ and $C$ is the total number of object categories) be its corresponding ground-truth class labels.
%, and $\mathbf{R} \in \mathbb{R}^{4\times N }$ as the precomputed object proposals. 
Let $v_i$, $i \in \{1, . . . , R\}$ (where $R$ is the number of proposals), 
%denote the visual regions $\mathbf{v}$  
denote the visual proposals in image $\mathbf{I}$. 
%Proposal feature vectors are extracted by an 
%and a box feature extractor are applied to extract a
RoI pooling is applied and a fixed-length feature vector $\phi(v_i)$ extracted for each visual region. The proposal features $\phi(v_i)$ are fed into two parallel fully-connected layers to compute the visual detection score $v^{det}_{i,c} \in \mathbb{R}^1$ and classification score $v^{cls}_{i,c} \in \mathbb{R}^1$: 
%which are dependent to particular class c and visual region $v_i$:

\begin{equation}
   v^{det}_{i,c} = w^{det\intercal}_{c} \phi(v_i) + b^{det}_{c}, \quad v^{cls}_{i,c} = w^{cls\intercal}_{c} \phi(v_i) + b^{cls}_{c}
   \label{eq:1}
\end{equation}
where $w$ and $b$ are weights and bias, % parameters, 
respectively.



\textbf{Estimating the depth images.} To extract depth information from RGB images, we employ the monocular depth estimation technique by Mahdi et al. \cite{miangoleh2021boosting}. This enables us to use existing RGB-only object detection datasets without the need for additional annotations. Although the extracted depth images are initially grayscale, we use a color map to convert them to RGB images with three channels.
%, allowing them to share the Siamese backbone as the RGB images. 

\textbf{Siamese design.}
Our approach utilizes a Siamese network with contrastive learning to incorporate depth information in the weakly-supervised object detection network during training. 
%This approach is effective, as we show in experiments, and efficient, because...
This design allows us to use a backbone pre-trained with RGB images to extract features from both RGB and depth images, without adding extra complexity to the model's parameters. We enhance the representation learning of the backbone by defining contrastive loss between RGB and depth features similar to \cite{meyer2020improving}.
%, which enables the backbone to extract features of depth. 
Utilizing a Siamese network provides the advantage of using only RGB images during inference similar to other WSOD methods. This ensures that our contribution does not introduce any additional overhead on the inference time.

With the help of a pre-trained backbone model, the feature map of RGB image $\psi(\mathbf{I})$ is extracted. Let $\mathbf{D} \in \mathbb{R}^{h\times w \times 3}$ denote a depth image associated with the RGB image $\mathbf{I}$ and let $\psi(\mathbf{D})$ be the feature map of the depth image $\mathbf{D}$ extracted by the Siamese backbone. The RGB feature map $\psi(\mathbf{I})$ and depth feature map $\psi(\mathbf{D})$ are fed into adaptive pooling and the Siamese fully connected layer to obtain $d$-dimensional projected feature vectors $\psi_{proj}(\mathbf{I})$ and $\psi_{proj}(\mathbf{D})$.
%for $\mathbf{I}$ and $\mathbf{D}$, respectively. 
The only extra parameters we add to the traditional MIL-based WSOD network come from the fully connected layer for projection with 8 percent overhead (13M parameters for the projection layer, vs 154M total). %13M (proj layer) vs 154M (total)
%However, these additional parameters are relatively insignificant when compared to the total number of parameters.
If no late fusion is performed in the experiments, we train as described in Sec.~\ref{sec:fusion}, but excluding the $d_{i, c}$ variables in Eq.~\ref{eq:fusion}. 

\textbf{Contrastive learning.}
We L2-normalize the RGB and depth feature vectors $\psi_{proj}(\mathbf{I})$ and $\psi_{proj}(\mathbf{D})$ vectors, and compute their cosine similarity:
\begin{equation}
    S(\mathbf{I},\mathbf{D}) = 	\langle \psi_{proj}(\mathbf{I}), \psi_{proj}(\mathbf{D}) \rangle / \rho
    \label{eq:b}
\end{equation} where $\rho$, is a learnable temperature parameter.
We use noise contrastive estimation (NCE) \cite{gutmann2010noise} to define the contrastive learning by considering RGB image and depth image pairs $(\mathbf{I}, \mathbf{D}) \in \mathcal{B}$ where $\mathcal{B}$ is an RGB-depth pair batch. The first component of the NCE loss contrasts an RGB image with negative depth images to measure how closely the RGB image matches with its paired depth among others in the batch:

\begin{equation}
\resizebox{\columnwidth}{!}{$
\mathcal{L}_{{D}\rightarrow{I}} = -\dfrac{1}{|\mathcal{B}|} \sum_{(\mathbf{I},\mathbf{D}) \in \mathcal{B}} log \dfrac{exp(S(\mathbf{I},\mathbf{D}))}{exp(S(\mathbf{I},\mathbf{D})) + \sum_{(\mathbf{I}',\mathbf{D}') \in \mathcal{B}} exp(S(\mathbf{I},\mathbf{D}'))}$}
\end{equation} 

The second component of the NCE loss, $\mathcal{L}_{{I} \rightarrow {D}}$, is analogously defined to contrast a depth image with negative RGB image samples, and
%to measure how closely the depth image matches with its RGB image pair among the others in the batch:
% \begin{equation}
% \resizebox{\columnwidth}{!}{
%     \mathcal{L}_{{I} \rightarrow {D}} = -\dfrac{1}{|\mathcal{B}|} \sum_{(\mathbf{I},\mathbf{D}) \in \mathcal{B}} log \dfrac{exp(S(\mathbf{I},\mathbf{D}))}{exp(S(\mathbf{I},\mathbf{D})) + \sum_{(\mathbf{I}',\mathbf{D}') \in \mathcal{B}} exp(S(\mathbf{I}',\mathbf{D}))}}
% \end{equation} 
the two components are averaged:
\begin{equation}
    \mathcal{L}_{NCE} = (\mathcal{L}_{{D} \rightarrow {I}} + \mathcal{L}_{{I} \rightarrow {D}}) / 2
\end{equation}

\subsection{Late Fusion of the Modalities}
\label{sec:fusion}

The detection and classification scores computed from RGB and depth modalities are imbued with disparate and complementary details that jointly enrich our understanding of the target objects. Therefore, we combine these scores to amplify the performance of object detection.

As the depth images are derived from the RGB images, the spatial arrangement of the objects is equivalent in both modalities. Hence, we utilize the same visual region proposals for both RGB and depth modalities. Following the application of the RoI pooling layer and the Siamese box feature extractor to the depth feature map $\psi(\mathbf{D})$, we obtain the feature vector $\phi(d_i)$ for each depth region. Thereafter, we employ the approach presented in Eq.~\ref{eq:1} to derive the depth detection score $d^{det}_{i,c} \in \mathbb{R}^1$ and the depth classification score $d^{cls}_{i,c} \in \mathbb{R}^1$. Subsequently, we fuse (sum) the scores from the RGB and depth modalities:
\begin{equation}
  f^{det}_{i,c} = v^{det}_{i,c} + d^{det}_{i,c}, \quad f^{cls}_{i,c} = v^{cls}_{i,c} + d^{cls}_{i,c}
\label{eq:fusion}
\end{equation}
where $f^{det}_{i,c}$ and $f^{cls}_{i,c}$ are fusion detection and classification scores, respectively.

Following the WSDDN \cite{bilen2016weakly} architecture, these classification and detection scores are converted to probabilities such that $p^{cls}_{i,c}$ is the probability that class $c$ is in present proposal $f_i$, and $p^{det}_{i,c}$ is the probability that $f_i$ is important for predicting image-level label $y_c$.

\begin{equation}
   p^{det}_{i,c} = \dfrac{exp(f^{det}_{i,c})} {\sum_{k=1}^{R} exp(f^{det}_{k,c})}, \quad p^{cls}_{i,c} = \dfrac{exp(f^{cls}_{i,c})} {\sum_{k=1}^{C} exp(f^{cls}_{i,k})}
   \label{eq:7}
\end{equation}

We element-wise multiply the classification and detection scores to obtain the combined score $p^{comb}_{i,c}$:
%Finally, visual aggregated image-level predictions $\hat{vp}_c$ are computed as follows, where greater values of $\hat{vp}_c\in [0, 1]$  mean higher likelihood that $c$ is present in the image.
\begin{equation}
  p^{comb}_{i,c} = p^{det}_{i,c} p^{cls}_{i,c}
  %, \quad  \hat{vp}_c = \sigma\left(\sum_{i=1}^{M}vp^{comb}_{i,c} \right)
  \label{eq:8}
\end{equation}

Finally, image-level predictions $\hat{p}_c$ are computed as follows, where greater values of $\hat{p}_c\in [0, 1]$ mean a higher likelihood that $c$ is present in the image.
\begin{equation}
  \hat{p}_c = \sigma\left(\sum_{i=1}^{R}p^{comb}_{i,c} \right)
  \label{eq:confidence}
\end{equation}
Assuming the label $y_c$ = 1 if and only if class $c$ is present, the classification loss used for training the model is defined as follows. Since no region-level labels are provided, we must derive region-level scores indirectly, by optimizing this loss.
\begin{equation}
    \mathcal{L}_{mil} = -\sum_{c=1}^{C}\left[ y_c\log\hat{p}_c + (1-y_c)\log(1-\hat{p}_c) \right]
    \label{eq:visual_loss}
\end{equation}

\subsection{Depth Priors}
\label{sec:depth}

By leveraging a small amount of caption and ground-truth bounding box annotations, we extract prior knowledge about the relative depths of objects. We subsequently exploit these depth priors to guide the identification of the relevant visual regions that may contain the target objects.
We note that the depth priors we estimate vary very little whether we use 1\%, 5\%, 10\%, or 50\% of the available COCO training data; see Table \ref{Table:4}. Further, we show that even though we estimate the depth priors from COCO, they generalize to Conceptual Captions (Table \ref{Table:2}).

We use the notation $pd_i \in [0, 1]$, $i \in {1, . . . , R}$, where $R$ is the number of pre-computed region proposals for depth image $\mathbf{D}$, to represent the average depth value in the $i$-th region proposal. Each region proposal contains pixels with values ranging from 0 to 1, which correspond to the smallest and largest depth values, respectively. %The average depth values in the pixels across each region proposal are then calculated to obtain $pd_i$.

\begin{figure}[t]
\includegraphics[width= \columnwidth]{prior} 
\caption{The figure displays a row of images that are accompanied by their respective depth and caption data, as well as proposal depth value of different regions and estimated depth prior range.}
\label{fig:prior}
\end{figure}

We employ a limited set of ground-truth bounding box annotations $B$ to approximate the depth value of objects using the caption that describes the image in which the objects are present. Let $C$ be the set of object categories, $W$ be the set of distinct words in the vocabulary that includes every word in the captions, and $B$ be the set of ground-truth bounding box annotations. Let $d_{c,w,b} \in \{[0, 1], \varnothing\}$ denote the depth value for object $c \in C$, word $w \in W$ and box $b \in B$, which is calculated by averaging the depth values in the pixels of $b$ similar to the calculation of $pd_i$.  As an example, $d_{\text{bird},\text{ocean}, b}$ represents the depth value of the ``bird" object corresponding box $b$ of a depth image that has a caption that includes the ``ocean" word. In the absence of ``ocean" in the caption or when annotation $b$ does not correspond to the ``bird" object, the depth value $d_{\text{bird},\text{ocean}, b}$ is set to null $\varnothing$. Further, $d_{c,w}$ represents a set of depth values calculated from annotations $B$, excluding $\varnothing$ values. The depth range $r_{c,w} = [mean - std, mean + std]$ for class $c$ and word $w$ is obtained by utilizing the mean and standard deviation (std) of this set of depth values in $d_{c,w}$. 
Once these depth ranges $r_{c, w}$ are computed, they can be applied to estimate an allowable depth range for a class $c$ in a new image, without any boxes on that image.

%Suppose we have $r_{\text{bird},\text{ocean}} = [0.25, 0.83]$, it implies that if the depth image caption includes the "ocean" word and the image is supposed to include "bird" object, then the expected range of depth value in a region proposal for the "bird" object is between $0.25$ (closest to the camera) and $0.83$ (farthest from the camera). Any region proposal depth value $rd_i$ that falls outside this range is less likely to contain the "bird" object.

For any new depth image $\mathbf{D}$, the range of estimated depth priors for an object $c$ is $dr_c$:
%where $c \in C$. The computation of $dr_c$ is as follows:
\begin{equation}
dr_{c} = \dfrac{\sum_{s \in S}{r_{c,s}}}{|S|}
\label{eq:prior}
\end{equation}
where $S$ denotes the set of words in the caption corresponding to $\mathbf{D}$. 
We \textbf{only require captions at training time.}
%Fig.~\ref{fig:prior} presents two images featuring a "bird" object, with different depths. In the first image, the object appears closer to the camera and has a proposal depth value of $rd_1 = 0.23$, while in the second image, the object appears farther from the camera and has a proposal depth value of $rd_1 = 0.51$. Images have different captions that describe the images so estimated depth priors $p_\text{bird}$ are calculated differently using these captions using in Eq.~\ref{eq:prior}. The caption of the first image includes "feeding" and "hand" words which are the cues that the "bird" object is likely to have a smaller depth value so the estimated depth prior range $p_\text{bird}$ is relatively smaller with the range of $[0.15,0.48]$. The caption of the second image includes "flying" and "ocean" words which are the cues that the "bird" object is likely to have a bigger depth value so the estimated depth prior range $p_\text{bird}$ is relatively larger with the range of $[0.35,0.68]$. Note that while the regions having proposal depth value of $rd_1$ on the images are in the estimated depth prior range because they include "bird" object, the regions having proposal depth value of $rd_2$ are out of the range because they are unrelated regions for "bird" object.

We utilize the estimated depth prior range $dr_c$ to identify potentially important regions in $pd$ for each class. We define a depth mask indicator variable $m_{i,c} \in {0,1}$ for each region $i$ $\in$ $R$ and class $c$ $\in$ $C$, which indicates the likelihood of a particular region in an image containing an object of a certain class. The computation of this variable is as follows:
\begin{equation}
    m_{i,c}= 
\begin{cases}
    1,& \text{if } pd_i \in dr_c\\
    0,              & \text{otherwise}
\end{cases}
\end{equation}
If the proposal depth value $pd_i$ falls within the estimated depth prior range $dr_c$ for class $c$, it is considered as a relevant region for that class, and the corresponding mask variable $m_{i,c}$ is set to 1; otherwise, it is set to 0. Subsequently, we utilize the mask variable $m_{i,c}$ in combination with our end-to-end network to improve its performance.

As an example, Fig.~\ref{fig:prior} presents two images featuring a ``bird" object, with different depths. The estimated depth prior ranges $dr_\text{bird}$
%$p_\text{bird}$ 
are calculated using Eq.~\ref{eq:prior} for each image based on the words present in the caption. The caption of the first image includes ``feeding" and ``hand" words that suggest the ``bird" object is likely to have a smaller depth value, while the caption of the second image includes ``flying" and ``ocean" words that suggest the ``bird" object is likely to have a bigger depth value. The regions on the images having a proposal depth value of $pd_1$ are in the estimated depth prior range $dr_\text{bird}$; we observe that they truly include the ``bird" object. 
The range allows us to rule regions with depth values $pd_2$, which do not contain ``bird''.



\subsubsection{Depth Priors: Updated OICR}
\label{sec:depth_oicr}
\newcommand\sForAll[2]{ \ForAll{#1}#2\EndFor} % snappy version of \ForAll...\EndFor
\newcommand\sIf[2]{ \If{#1}#2\EndIf}
\begin{algorithm}
	\caption{OICR Mining with Depth Priors} 
        Input: Proposals $R$, Depth Mask Indicator Variable $m$\\
        Output: Pseudo boxes $\hat{R}$
	\begin{algorithmic}[1]
        \State $\hat{R} = \varnothing$ 
        \sForAll {$c=1:C$}{
            \sForAll {$i=1:|R|$}{
                \State $\hat{R_c} = \hat{R_c} \cup R_i $ \textbf{if} $m_{i,c} = 1$
            }
        }
        \State \textbf{return} $\hat{R}$
	\end{algorithmic} 
\label{algorithm:algo}
\end{algorithm}

 Online Instance Classifier Refinement (OICR) \cite{tang2017multiple} is a weakly supervised object detection algorithm that iteratively refines object proposals.  Recent studies \cite{tang2018pcl,ren2020instance,sui2022salvage} have highlighted the importance of more effective proposal mining strategies for achieving better recall and precision of objects in WSOD detectors. We propose an algorithm that incorporates the depth priors during the proposal mining provided in Alg.~\ref{algorithm:algo}. As our proposed method aims to enhance MIL-based WSOD methods, we utilize our algorithm in conjunction with recent OICR-style/self-training/mining strategies, subject to the depth prior condition specified in the fourth line of Alg.~\ref{algorithm:algo}. After using the depth prior condition, OICR-style mining selects fewer but more relevant proposals so our contribution increases mining precision.\footnote{In early experiments, we verified that our method's gains persist if the baseline is allowed to drop the lowest-scoring pseudo boxes but without using depth.} 

 \subsubsection{Depth Priors: Attention}
 \label{sec:depth_attention}
The depth mask variable $m_{i,c}$ indicates the potentially important proposal regions for each class. We use this variable to employ an attention mechanism with combined score probabilities $p^{comb}_{i,c}$ provided in Eq. ~\ref{eq:8} as follows:
\begin{equation}
    %p^{attention}_{i,c}= 
    p^{comb}_{i,c}= 
%\begin{cases}
    p^{comb}_{i,c} * 0.5, \text{if } m_{i,c} = 0
%    p^{comb}_{i,c},              & \text{otherwise}
%\end{cases}
\end{equation}
This mechanism reduces the probability of a region for class $c$ by half if the region is determined as less likely to be important by $m_{i,c}$. These scores are then used in Eq.~\ref{eq:confidence}.


 


\section{Experiments}

We test our method on top of two weakly-supervised detection techniques, and verify the contributions of  the constituents of our approach:

\begin{itemize}[nolistsep,noitemsep]
\item Siamese WSOD Network (\textsc{Siamese-Only}, Sec.~\ref{sec:siamese_module});
\item Late Fusion of the Modalities (\textsc{Fusion}, Sec.~\ref{sec:fusion}) which combines classification/detection from RGB and depth, and builds on top of the Siamese WSOD Network (Sec.~\ref{sec:siamese_module});
%an improvement upon \textsc{Siamese-Only};
\item Depth Priors are utilized to enhance the OICR-style module (\textsc{Depth-Oicr}, Sec.~\ref{sec:depth_oicr}) and construct attention (\textsc{Depth-Attention}, Sec.~\ref{sec:depth_attention}) with visual-only score probabilities, both building upon the Siamese WSOD Network (Sec.~\ref{sec:siamese_module}); 
%Both of these methods are improvements upon the \textsc{Siamese-Only};
\item Finally, we use all components of our method (\textsc{Wsod-Amplifier, Sec.~\ref{sec:siamese_module}, \ref{sec:fusion}, \ref{sec:depth_oicr}, \ref{sec:depth_attention}}).
\end{itemize}

\subsection{Experimental Setup}
\subsubsection{Dataset and Metrics}

%We conduct experiments using three datasets, namely PASCAL VOC, COCO, and Conceptual Captions. 

\textbf{PASCAL Visual Object Classes 2007 (VOC-07)} \cite{everingham2009pascal} contains 20 classes. For training, we use 2501 images from the train set and 2510 images from the validation set. We evaluate using 4952 images from the test set.

\textbf{Common Objects in Context (COCO)} \cite{lin2014microsoft} consists of 80 classes. 
%To train our models with ground truth labels, 
We utilize approximately 118k images from the train set and use the labels provided at the image level. Additionally, to test how well our method works when labels are obtained from noisy language supervision (in captions), we train our models using labels obtained through an exact match (EM) method following \cite{unal2022learning}, also referred to as substring matching in \cite{fang2022data}. Due to the unavailability of any labels for around 15k images extracted from captions, we excluded them from the training set and use 103k images. We evaluate using 5k images from the validation set.

\textbf{Conceptual Captions (CC)} \cite{sharma2018conceptual} is a large-scale image captioning dataset containing over 3 million images annotated with only captions. We use around 30k images and their corresponding captions and the labels are extracted for the 80 COCO dataset classes using an exact match method from the captions. During the evaluation, we used 5k images from the COCO validation set.

\textbf{Domain shift datasets} \cite{inoue2018cross}. Clipart1k has the same 20 classes as VOC with 1,000 images, while Watercolor2k and Comic2k share 6 classes with VOC and have 2,000 images each. We use these datasets for evaluation only. 

\textbf{Evaluation protocols.} We utilize mean Average Precision (mAP) considering various IoU thresholds as the common evaluation metric for COCO and VOC datasets. Additionally, we report mAP for objects of different sizes during COCO evaluation and we report the results of Correct Localization (CorLoc) for VOC evaluation.

\subsubsection{Implementation details}

We employ the official PyTorch implementations of SoS-WSOD \cite{sui2022salvage} and MIST \cite{ren2020instance} methods to apply our amplifier technique. SoS-WSOD uses four images per GPU as two augmented images and their flipped versions with a total of 4 GPUs, whereas MIST uses only one image per GPU with a total of 8 GPUs. However, we use one image per GPU for SoS-WSOD due to VRAM limitation in our GPUs, as we also utilized depth images for each corresponding RGB image. Therefore, the baseline results of SoS-WSOD reported in Table ~\ref{Table:1} are slightly lower than those reported in the original paper. Moreover, we solely use the first stage of SoS-WSOD since it includes the MIL-based WSOD module which is convenient to implement our method on top of. The other settings are kept the same as the official implementations with the VGG16 backbone. To compute the depth prior, we use only 5 percent of the total annotations from the COCO dataset as the ground-truth bounding box annotations $B$. Furthermore, we utilize the same depth range $r_{c,w}$ from the COCO annotations for the \textsc{Wsod-Amplifier} method on the Conceptual Captions dataset, as the latter lacks ground-truth box annotations.

\subsection{Comparing our amplifier to state of the art}

\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{ lcccccc}
\toprule
 & \multicolumn{3}{c}{Avg. Precision, IoU}  & \multicolumn{3}{c}{Avg. Precision, Area} \\
 \cmidrule{2-4} \cmidrule{5-7}
Methods on COCO & 0.5:0.95 & 0.5  & 0.75 & S & M  & L  \\
% & & &   & & &  \\

\midrule 

\textsc{MIST \cite{ren2020instance}} & 11.8 & 24.3 & 10.7 & 3.6 & 13.2 & 18.9\\
\textsc{+ Fusion} & 13.5&26.9&\bf{12.4}&4.0&14.7&21.6\\
\textsc{+ Wsod-Amplifier} & \bf{13.8}&\bf{27.7}&12.3&\bf{4.6}&\bf{14.7}&\bf{22.4}\\

\midrule 
\textsc{SoS-WSOD \cite{sui2022salvage}} & 10.2 & 21.5 & 8.6 & \bf{2.5} & 10.6 & 17.7\\
\textsc{+ Fusion} & 10.3 & 21.6 & 8.9 & 2.3 & 10.8 & 18.4\\
\textsc{+ Wsod-Amplifier} & \bf{10.4} & \bf{21.8} & \bf{9.1} &  2.4 & \bf{11.0} & \bf{18.7} \\

\midrule
\midrule
 & \multicolumn{3}{c}{Avg. Precision, IoU}  & \multicolumn{3}{c}{CorLoc} \\
 \cmidrule{2-4} \cmidrule{5-7}
Methods on VOC-07 &  0.5:0.95 & 0.5  & 0.75  & 0.5:0.95 & 0.5  & 0.75 \\
\midrule 
\textsc{SoS-WSOD \cite{sui2022salvage}} & 24.8&52.2&20.4&38.7&71.7&36.9\\
\textsc{+ Fusion} & \bf{26.0}&\bf{53.1}&\bf{22.3}&\bf{39.6}&\bf{72.1}&\bf{38.5}\\

\bottomrule
\hline \\
\end{tabular}}
\caption{This table compares the performance enhancement of our methods, to their baseline results SoS-WSOD \cite{sui2022salvage} and MIST \cite{ren2020instance}, on COCO and VOC-07. The best performer per column is in \textbf{bold}.} % for visual methods.} 
\label{Table:1}
\end{table}

We evaluate our proposed methods, \textsc{Fusion} and \textsc{Wsod-Amplifier}, on two state-of-the-art WSOD approaches, SoS-WSOD \cite{sui2022salvage} and MIST \cite{ren2020instance}, and conduct experiments on COCO and VOC-07 datasets. The performance of our proposed methods are compared with the baseline methods in Table ~\ref{Table:1}. When our \textsc{Wsod-Amplifier} method is applied to MIST, it improves the baseline performance by $17\%$ in $mAP_{50:95}$ (relative gain, 13.8/11.8-1) and $14\%$ in $mAP_{50}$. Similarly, when our \textsc{Wsod-Amplifier} method is applied to SoS-WSOD, it improves the baseline performance by $2\%$ in $mAP_{50:95}$, $1.5\%$ in $mAP_{50}$, and $6\%$ in $mAP_{75}$. As the VOC-07 dataset does not have 
%ground truth box annotations and 
captions, we are only able to apply the \textsc{Siamese-Only} and \textsc{Fusion} methods on SoS-WSOD but not the \textsc{Depth-Oicr} and \textsc{Depth-Attention}. On this dataset, our improvements outperformed the baseline SoS-WSOD by $5\%$ in $mAP_{50:95}$, $2\%$ in $mAP_{50}$, and $9\%$ in $mAP_{75}$. %Moreover, our methods  enhance the $mAP$ results for objects with different sizes.


\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{ lcccccc}
\toprule
 & \multicolumn{3}{c}{Avg. Precision, IoU}  & \multicolumn{3}{c}{Avg. Precision, Area} \\
 \cmidrule{2-4} \cmidrule{5-7}

 %& \multicolumn{3}{c}{Avg. Precision, IoU}\\
 %\cmidrule{2-4}
Methods on COCO & 0.5:0.95 & 0.5 & 0.75 & S & M & L  \\
%Methods on COCO & 0.5:0.95 & 0.5 & 0.75  \\
% & & &   & & &  \\

\midrule 
\textsc{MIST \cite{ren2020instance}} w/ GT &9.7&21.1&8.0 & 3.0 & 10.4 & 15.1 \\
\midrule[0.1pt]
\textsc{MIST \cite{ren2020instance}} w/ EM &8.5&17.9&7.3 & 3.0 & 9.4 & 14.9 \\
\textsc{+ Siamese-Only} & 8.8 & 18.7 & 7.3 & 2.9 & 9.6 & 15.4  \\
\textsc{+ Depth-Oicr} & \underline{9.1} & \underline{19.4} & \underline{7.4} & \underline{3.0} & \underline{9.8} & \underline{16.0} \\
\textsc{+ Depth-Attention} & \underline{9.1} & \underline{18.8} & \underline{7.8} & 2.8 & 9.4 & \underline{16.1}  \\
\textsc{+ Fusion} & \underline{9.9} & \underline{20.4} & \bf{\underline{8.5}} & \underline{3.0} & \underline{10.1} & \underline{17.1} \\
\textsc{+ Wsod-Amplifier} & \bf{\underline{10.2}} & \bf{\underline{21.1}} & \underline{8.4} & \bf{\underline{3.2}} & \bf{\underline{10.3}} & \bf{\underline{17.4}}  \\

\midrule
\midrule
 & \multicolumn{3}{c}{Avg. Precision, IoU}  & \multicolumn{3}{c}{Avg. Precision, Area} \\
 \cmidrule{2-4} \cmidrule{5-7}

Methods on CC & 0.5:0.95 & 0.5 & 0.75 & S & M & L  \\
\midrule 
\textsc{MIST \cite{ren2020instance}} w/ EM  &1.7&3.8&1.4 & 0.3 & 1.7 & 3.4 \\

\textsc{+ Fusion} & 2.0 & {4.1} & {1.7} & {0.3} & {1.9} & {4.0} \\

\textsc{+ Depth-Oicr} & {2.5} & {5.7} & {{2.0}} & {0.4} & {2.1} & {5.3} \\
\textsc{+ Wsod-Amplifier} & \bf{{2.6}} & \bf{{6.2}} & \bf{2.0} & \bf{{0.4}} & \bf{{2.7}} & \bf{{5.6}}  \\

\bottomrule
\hline \\
\end{tabular}}
\caption{This table introduces the effect of each component of our method implemented on MIST \cite{ren2020instance} with exact match (EM) labels on COCO (top) and Conceptual Captions (CC) (bottom). The best performer per column is in \textbf{bold}. On top, all proposed methods that outperform the \textsc{Siamese-Only} are \underline{underlined}.} % for visual methods.} 
\label{Table:2}
\end{table}


\subsection{Ablation studies and visualization}
\textbf{Experiments with labels from captions.} Several attempts \cite{ye2019cap2det, unal2022learning,fang2022data} have been made to eliminate the requirement for image-level labels by leveraging noisy label information obtained from captions or subtitles. Although it is cost-effective to use text information for label extraction, it results in a decrease in the performance of weakly supervised object detection. \cite{unal2022learning} propose a text classifier approach to extract labels more effectively than the simple exact match (EM) and reduce the noise between text and ground truth (GT) labels. In contrast to previous studies, our research employs the depth modality to reduce the noise in labels extracted from captions. Our approach improves the model's detection capability and employs captions during the calculation of depth priors. We conducted experiments with MIST \cite{ren2020instance} using both GT and EM labels and observed that, as expected, training with GT labels leads to significantly better performance than training with EM labels in Table ~\ref{Table:2} due to the noise in labels extracted from captions. However, our proposed \textsc{Wsod-Amplifier} method applied on MIST with EM labels surpasses the baseline and MIST with GT labels. These findings demonstrate that our method effectively reduces noise and enables the model trained with EM labels to achieve better performance than those trained with GT labels. It is worth noting that the text classifier approach proposed by \cite{unal2022learning} also performs better than EM-labeled training data, but falls short of the performance achieved by GT-labeled data.

\textbf{Results on noisy datasets.}
We also extract labels from captions on the Conceptual Captions dataset, which lacks labels at the image level. We also observe that our \textsc{Wsod-Amplifier} boosts results by an impressive 63\% relative gain using $mAP_{50}$. 
Conceptual Captions is a noisier dataset than COCO, since captions were not collected through crowdsourcing, but were crawled as alt-text for web search results. 
Thus, it is noteworthy that \emph{the benefit of our approach becomes more pronounced as the cost of supervision decreases, and the noise in the supervision increases.}





\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{vis.png}
    \caption{Qualitative comparison of MIST \cite{ren2020instance} (top) and our proposed \textsc{Wsod-Amplifier} method (bottom) implemented on MIST.}
    \label{fig:qualitative}
\end{figure*}


\begin{table*}[t]
\centering
\resizebox{2.05\columnwidth}{!}{
\begin{tabular}{ lcc|cc|cc|cc|cc|cc|cc|cc|cc|cc}
\toprule
 %& \multicolumn{3}{c}{Avg. Precision, IoU}  & \multicolumn{3}{c}{Avg. Precision, Area} \\
 %\cmidrule{2-4} \cmidrule{5-7}

 %& \multicolumn{3}{c}{Avg. Precision, IoU}\\
 %\cmidrule{2-4}
\% of GT anno. & \multicolumn{2}{c}{person}&\multicolumn{2}{c}{car}&\multicolumn{2}{c}{motorcycle}&\multicolumn{2}{c}{handbag}&\multicolumn{2}{c}{bowl}&\multicolumn{2}{c}{hair drier}&\multicolumn{2}{c}{toaster}&\multicolumn{2}{c}{scissors}&\multicolumn{2}{c}{parking meter}&\multicolumn{2}{c}{tennis racket}  \\

\midrule 

\textsc{1\%}  &0.23&0.49&0.26&0.53&0.17&0.43&0.13&0.41&0.13&0.43&0.39&0.52&0.32&0.42&0.08&0.42&0.11&0.39&0.23&0.48\\

\textsc{5\%} 
&0.23&0.49&0.28&0.54&0.15&0.41&0.14&0.41&0.13&0.43&0.22&0.46&0.27&0.55&0.15&0.45&0.12&0.35&0.16&0.44\\

\textsc{10\%}
&0.23&0.49&0.28&0.54&0.16&0.41&0.14&0.42&0.13&0.43&0.27&0.47&0.26&0.54&0.15&0.46&0.16&0.44&0.16&0.43\\

\textsc{50\%}
&0.23&0.49&0.27&0.54&0.16&0.42&0.15&0.42&0.14&0.44&0.24&0.50&0.26&0.54&0.16&0.45&0.15&0.44&0.17&0.44\\
\midrule 
\textsc{Average}
&0.230&0.490&0.272&0.537&0.160&0.417&0.140&0.415&0.132&0.432&0.280&0.487&0.277&0.512&0.135&0.445&0.135&0.405&0.180&0.447\\
\textsc{Stdev}
&0&0&0.009&0.005&0.008&0.009&0.008&0.005&0.005&0.005&0.070&0.027&0.028&0.061&0.036&0.017&0.023&0.043&0.033&0.022\\

\bottomrule
\hline \\
\end{tabular}}
\caption{The table displays the lower and upper depth range values, denoted as $r_c = [\text{lower}, \text{upper}]$, for each object class $c$, which are extracted using varying percentages of ground truth bounding box annotations.
%, such as $1\%$, $5\%$, $10\%$, and $50\%$. 
Among the ten object classes considered, the first 5 %, namely person, car, motorcycle, handbag, and bowl, 
exhibit the lowest standard deviations across the different percentages, and the last 5 %, including hair drier, toaster, scissors, parking meters, and tennis racket, 
display the highest deviations out of the 80 COCO classes.} % for visual methods.} 
\label{Table:4}
\end{table*}



\textbf{Analysing the components of our approach.} To understand the impact of each component of our approach on the overall performance, we conducted experiments with MIST \cite{ren2020instance} using EM labels as a baseline and applied each component of our method on top of the baseline in Table ~\ref{Table:2}. Our \textsc{Siamese-Only} method, which incorporates the depth modality in the Siamese network using contrastive learning, improves feature extraction and results in a $4\%$ increase in $mAP_{50:95}$ and $mAP_{50}$. Our \textsc{Depth-Oicr} method, which utilizes depth priors in the OICR module to improve the mining strategy, increases $mAP_{50:95}$ and $mAP_{50}$ over \textsc{Siamese-Only} by $7-8\%$ on COCO and $47-50\%$ on Conceptual Captions (CC). Our \textsc{Depth-Attention} method, which incorporates depth priors to use potentially important regions in an attention mechanism with combined score probabilities, increases $AP_{50:95}$ and $mAP_{75}$ by $6-7\%$. Our \textsc{Fusion} method, which combines RGB and depth image scores, improves by $14-16\%$ on COCO and $7-21\%$ on CC. 
Comparing \textsc{Fusion} and \textsc{Depth-Oicr}, the bigger gain using $mAP_{50}$ is achieved by \textsc{Fusion} on COCO, and \textsc{Depth-Oicr} on CC. \emph{Thus, the benefit of our WSOD-specific method component increases as the noise in the dataset increases, which is appealing due to its real-world applicability.} 
Finally, our \textsc{Wsod-Amplifier} method, which includes all components of our approach, achieves the highest performance increase over MIST w/ EM baseline, with \emph{improvements in all $mAP$ metrics by $18-20\%$ on COCO and $42-63\%$ on CC.} 
%It is notable that our \textsc{Wsod-Amplifier} method has shown better performance in improving the MIST with EM labels in Table ~\ref{Table:2}, with an improvement of $18-20\%$, compared to the improvement of $14-17\%$ in the GT labels setting shown in Table ~\ref{Table:1}. Furthermore, 


\begin{table}[t]
\centering
\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{ lccc}
\toprule

Methods & Clipart & Watercolor & Comic  \\
\midrule 
\textsc{MIST \cite{ren2020instance} w/ GT}  &9.4&13.3&9.2  \\

\textsc{+ Wsod-Amplifier} & \bf{{10.2}} & \bf{{14.7}} & \bf{9.6}   \\
\bottomrule
\hline \\
\end{tabular}}
\caption{This table introduces the improvement of our \textsc{Wsod-Amplifier} over MIST on domain shift datasets. The results are in $mAP_{50}$ metric. The best performer per column is in \textbf{bold}.} % for visual methods.} 
\label{Table:3}
\end{table}



\textbf{Generalization of depth priors.} Even though we apply the depth priors calculated from COCO to CC, without any bounding boxes available in CC, our proposed method exhibits a more substantial enhancement in CC performance compared to COCO, achieving an improvement of $63\%$ $mAP_{50}$ over the MIST baseline (50\% improvement from \textsc{Depth-Oicr} alone). 
%This is due to the fact that CC has more noisy captions. 
Thus, our \textsc{Depth-Oicr} method has a higher impact than \textsc{Fusion} on CC, in contrast to COCO.
%, owing to the presence of noisier captions in CC. 
Given the recent interest in learning from vision-language data, our approach has the potential to be highly impactful.

\textbf{Generalization to appearance changes.}
By relying on depth information, our method builds some robustness to overfitting to appearance, which may not be the same across datasets. To test this hypothesis, we conduct experiments with domain shift datasets \cite{inoue2018cross}. Table ~\ref{Table:3} shows that our \textsc{Wsod-Amplifier} boosts the performance of MIST baseline in $mAP_{50}$ by $4-10\%$, even though no training was performed on these datasets.



\textbf{Analyzing the depth priors calculated with varying percentages of GT annotations.} In this section, we examine how the proportion of ground truth (GT) annotations affects the ability to estimate the relative depth range of objects. Table ~\ref{Table:4} presents the average depth ranges and standard deviations for ten objects at various annotation percentages. Across all 80 objects, the average difference between the upper and lower bounds is $0.26$, while the average standard deviation is a mere $0.01$. This indicates that the standard deviation is significantly lower than the average range difference. Although we utilize $5\%$ percent of annotations as default in our experiments, our findings suggest that even utilizing only $1\%$ of GT annotations is sufficient to approximate depth priors successfully.

\textbf{Qualitative analysis.} We visualize the object detection performance improvement of our proposed \textsc{Wsod-Amplifier} method in comparison to MIST \cite{ren2020instance} in Figure ~\ref{fig:qualitative}. In the first image, the baseline struggles to accurately identify multiple instances of the same ``vase" objects, instead grouping them together in a single bounding box with a high score. Our method overcomes this challenge, precisely detecting each individual ``vase" instance. In the second image, the baseline faces the problem of part domination due to some discriminative parts of a ``zebra" object. Our proposed method helps to overcome this issue by utilizing depth modality, which emphasizes the geometric variations of objects, while comparatively ignoring the complex background.
%, unlike RGB modality that focuses on color intensity. 
In other images, unlike our method, the baseline misses objects entirely, or produces large and imprecise bounding boxes.
%, whereas our method performs better in detecting objects. 
Moreover, the bounding boxes detected by our method tend to have higher prediction scores.

\section{Conclusion}
We have demonstrated how the depth modality can be useful for weakly-supervised object detection, without incurring annotation or significant computation costs. Our Siamese WSOD network efficiently incorporates RGB and depth modality, along with contrastive loss and fusion. With the relationship of language and depth, depth priors estimate the bounding box proposals that may contain an object of interest.
%by improving OICR and attending with score probabilities. 
We have implemented our boosting approach on two WSOD methods, SoS-WSOD, and MIST and significantly increased their performance.

%-------------------------------------------------------------------------


{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}

\end{document}
