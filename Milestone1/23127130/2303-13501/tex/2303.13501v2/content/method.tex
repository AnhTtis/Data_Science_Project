\vspace{-1mm}\section{Chordal Centroids on Flag Manifolds}\vspace{-1mm}\label{sec:method}
We begin by providing the necessary definitions related to flag manifolds before presenting our chordal flag-mean and -median algorithms. 
%We then show how rotation and motion averaging problems can all be addressed within our framework.
% This is a very rough interpretation of how to navigate between $SE(3)$, $SO(4)$ and $FL(1,2,3;4)^+$. I think I have some idea of what is going on, but I still need to understand the saletan contraction and the specifics of translating between quaternion representations for these spaces. \tolga{\href{https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w35/Busam_Camera_Pose_Filtering_ICCV_2017_paper.pdf}{Here is one paper} we had on dual quaternions, which could be easy to read.}

% Let $[\{v_1,v_2,v_3\}]$ denote the $3$-dimensional subspace spanned by $\{v_1,v_2,v_3\}$. Let $[v_1,v_2,v_3]$ denote the matrix with columns $\{v_1,v_2,v_3\}$.



\begin{dfn}[Matrix groups] The \textbf{orthogonal group} $O(d)$ denotes the group of distance-preserving transformations of a Euclidean space of dimension $d$. $SO(d)$ is the \textbf{special orthogonal group} containing matrices in $O(d)$ determinant $1$. The \textbf{Stiefel manifold} $St(k,d)$, a.k.a. the set of all orthonormal $k$-frames in $\R^d$, can be represented as the quotient group: $St(k,d) = O(d)/O(d-k)$. A point on the Stiefel manifold is parameterized by a tall-skinny $d \times k$ real matrix with orthonormal columns.
%\begin{dfn}[Grassmannian]
The \textbf{Grassmannian}, $Gr(k, d)$, represents the collection of points parameterizing the $k$-dimensional subspaces of a fixed $d$-dimensional vector space, \eg $\R^d$. For our purposes, $Gr(k, d)$ is a \emph{real matrix manifold}, where each point is identified with an \emph{equivalence class of orthogonal matrices}, \ie $Gr(k, d)=O(d)/O(k)\times O(d-k)$. \\
\textbf{Notation:} We represent $[\X] \in Gr(k,d)$ using the truncated orthogonal matrix $\X \in \R^{d \times k}$. For this paper $[\X]$ is used to denote the subspace spanned by the columns of $\X$.
%\end{dfn}
\end{dfn}


\begin{dfn}[Flag]\label{def:flagobj}
A \emph{flag} in a finite dimensional vector space $\mathcal{V}$ over a field is a sequence of nested subspaces with \emph{increasing} dimension, each containing its predecessor, \ie the filtration: $\{\emptyset \}=\mathcal{V}_0 \subset \mathcal{V}_1 \subset \dots \subset \mathcal{V}_{k} \subset \mathcal{V}$ with $0=d_0<d_1<\dots<d_k<d_{k+1} = d$ where $\mathrm{dim} \mathcal{V}_i=d_i$ and $\mathrm{dim} \mathcal{V}=d$. We say this flag is of \emph{type} or \emph{signature} $(d_1, \dots ,d_k,d)$. A flag is called \emph{complete} if $d_i = i,\,\forall i$. Otherwise the flag is \emph{incomplete} or \emph{partial}. 

\noindent\textbf{\emph{Notation}:} A flag, $[\![\X]\!]$ of type $(d_1, \dots ,d_k, d)$, is represented by a truncated orthogonal matrix $\X \in \R^{d \times d_k}$. Let $m_j = d_j-d_{j-1}$ for $j=1,2,\dots, k+1$, and $\X_j  \in \R^{d \times m_j}$ for $j=1,2,\dots, k$ whose columns are the $d_{j-1}+1$ to $d_{j}$ columns of $\X$. $[\![\X]\!]$ is
\begin{equation}
     [ \X_1 ]  \subset [\X_1, \X_2] \subset \cdots \subset  [\X_1, \dots, \X_k] = [\X] \subset \R^d. \nonumber
\end{equation}
\end{dfn}

\begin{dfn}[Flag manifold]
The aggregate of all flags of the same type, \ie a certain collection of ordered sets of vector subspaces, admit the structure of manifolds. We refer to this \emph{flag manifold} %with its dimension (type) 
as $\flag(d_1, ..., d_k; d)$ or equivalently as $\flag(d+1)$\footnote{Note that we will use $\flag(d_1, ..., d_k; d)$ and $\flag(d+1)$ interchangeably in the rest of the manuscript.}. The points of $\flag(d+1)$
%$\flag(d_1, ..., d_k; d)$ 
parameterize all flags of type $(d_1, ..., d_k,d)$. 
%In other words, each point in $\flag(d+1)$
%$\flag(d_1, ..., d_k; d)$ 
%is a nested sequence of subspaces. 
Flag manifolds generalize Grassmannians because $\flag(k; d)=\Gr(k,d)$. $\flag(d+1)$ can be thought of as a quotient of groups ~\cite{ma2022self}:
\begin{equation*}
\flag(d+1) = SO(d)/S(O(m_1) \times O(m_2) \times \cdots \times O(m_{k+1})).
\end{equation*}
%where $S(G)$ denotes the matrices of $G$ with determinant $1$. 
\end{dfn}

\begin{dfn}[Chordal distance on the flag manifold~\cite{pitaval2013flag}]\label{def:flagdist}
For $[\![\X]\!], [\![\Y]\!] \in \flag(d+1)$, the \emph{chordal distance} is a map $d_c: \flag(d+1) \times \flag(d+1) \to \R$:
\begin{equation}\label{eq: chordal distance}
    d_c([\![\X]\!], [\![\Y]\!]) := \sqrt{\sum_{j=1}^k m_j - \tr(\X_j^{\top} \Y_j \Y_j^{\top} \X_j)}.
\end{equation}
\end{dfn}
%\begin{dfn}[Oriented vector space~\cite{alberti2005geometric}]
% Two subspaces $[\X]$, $[\Y] \in Gr(k,n)$, are said to have the same orientation if $\prod_{i=1}^k \lambda_i(\X \Y^{\top}) > 0$, where $\lambda_i(\cdot)$ extracts the the $i^\mathrm{th}$ eigenvalue, \ie, 
%Two vector spaces have the same orientation if the determinant of the unique linear transformation between them is positive.
%\textcolor{blue}{Nate: Should we just remove this definition and reference the definition for oriented vector spaces as a sentence in oriented flag?}
%\end{dfn}
We now endow flags with \emph{orientation}, which is required in certain applications such as motion averaging. 
\begin{dfn}[Oriented flag manifold~\cite{selig2005study, ma2022self}]
An \emph{oriented flag manifold}, $\flag^+(d+1)$, contains only flags with subspaces with compatible orientations. Algebraically:
\begin{equation*}
    \flag^+(d+1) = SO(d)/(SO(m_1) \times \cdots \times SO(m_{k+1})). 
\end{equation*}
Two oriented vector spaces have the same orientation if the determinant of the unique linear transformation between them is positive~\cite{alberti2005geometric}.
\end{dfn}

% \begin{comment}
% \begin{dfn}[Orientable Flag]
% \nate{I am not sure if this is enough citations for my two steps... what do you think?}
% % \nate{ I think this is the definition of two spaces having the same orientation: ``Two spaces have the same orientation if the determinant of the unique linear transformation between their basis vectors is positive.'' So, I think this means that two subspaces, $[\X], [\Y] \in Gr(k,n)$, have the same orientation when $\prod_{i=1}^k \lambda_i(\X \Y^{\top}) > 0 $. But I can't find a citation for this...}
% Two vector spaces with have the same orientation if the determinant of the linear transformation between them is positive. An oriented flag has nested subspaces, each of which are oriented \cite{selig2005study}. In practice, we can determine whether two subspaces, $[\X], [\Y] \in Gr(k,n)$, have the same orientation in two steps \cite{alberti2005geometric}. 
% \begin{enumerate}
% \item Compute the set of positive eigenvalues of $\X \Y^{\top}$: $\{\lambda_i(\X \Y^{\top})\}_{i=1}^k$.
% \item Verify that $\prod_{i=1}^k \lambda_i(\X \Y^{\top}) > 0$.
% \end{enumerate}
% % We can require flags to be orientable by representing any element in $\flag(d_1, ..., d_k; d)^+$ as $[\{ v_1 \}]\subset [\{ v_1,v_2 \}] \subset [\{ v_1,v_2,\dots ,v_k \}] \subset \mathbb{R}^d \in \flag(d_1, ..., d_k; d)^+$ \cite{selig2005study}. In this paper we will use orthonormal orientable flag representations, that is the set of vectors $\{ v_1,v_2,\dots ,v_k \}$ is an orthonormal set.
% \end{dfn}
% \end{comment}

% \begin{dfn}[Quaternions $\QH$]\label{dfn:quaternion}
% A \emph{quaternion} $\q$ is an element of Hamilton algebra $\QH$, extending the complex numbers with three imaginary units $\textbf{i}$, $\textbf{j}$, $\textbf{k}$ in the form:
% $\q
% 		= q_1 \textbf{1} + q_2 \textbf{i} + q_3 \textbf{j} + q_4 \textbf{k}
%     = \left(q_1, q_2, q_3, q_4\right)^{\text{T}}$,
% with $\left(q_1, q_2, q_3, q_4\right)^{\text{T}} \in \mathbb{R}^4$ and
% $\textbf{i}^2 = \textbf{j}^2 = \textbf{k}^2 = \textbf{i}\textbf{j}\textbf{k} = - \textbf{1}$. $q_1 \in \mathbb{R}$ denotes the scalar part and $\textbf{v} = \left(q_2, q_3, q_ 4\right)^{\text{T}} \in \mathbb{R}^3$, the vector part.
% The \emph{conjugate} $\bar{\q}$ of the quaternion $\q$ is given by $
% \qc := q_1 - q_2 \textbf{i} - q_3 \textbf{j} - q_4 \textbf{k}$.
% A \emph{unit quaternion} $\q \in \mathbb{H}_1$ with $1 \stackrel{\text{!}}{=} \left\|\q\right\|
% 	:= \q \cdot \qc$ and $\q^{-1}= \qc$,
% gives a compact and numerically stable parametrization to represent orientation of objects on the unit sphere $\Sphere^3$, avoiding gimbal lock and singularities~\cite{busam2016_iccvw}.  Identifying antipodal points $\q$ and $-\q$ with the same element, the unit quaternions form a double covering group of $SO\left(3\right)$. 
% % \textnormal{-- Quaternion product: }
% % \label{sec:quatsProd}\noindent
% $\QH$ is closed under the non-commutative multiplication or the Hamilton product:
% \begin{align}
%     (\q) \circ (\textbf{r}) =
% [{q}_1{r}_1-\mathbf{v}_q\cdot \mathbf{v}_r\, ;\,{q}_1\mathbf{v}_r+{r}_1 \mathbf{v}_q+\mathbf{v}_q \times \mathbf{v}_r].\nonumber
% \end{align}
% %For simplicity we use $\textbf{p}\circ\textbf{r}:=\textbf{p}\cdot\textbf{r}:=\textbf{p}\textbf{r}$.
% \end{dfn}

% \begin{dfn}[Study Quadric]
% $\{(a0 : a1 : a2 : a3 : c0 : c1 : c2 : c3)\}\subset \R \mathbb{P}^7$ that satisfy the relation
% \begin{equation}
% a_0c_0 + a_1c_1 + a_2c_2 + a_3c_3 = 0.    
% \end{equation}
% \end{dfn}


% \begin{dfn}[Dual Quaternions ($\DQH$)]
% A dual quaternion is an ordered set of quaternions $\DQH \ni \q = \q_1 + \epsilon \q_2 $ where $\q_1\in\QH$ and $\q_2\in\QH$ and $\epsilon$ has the property that $\epsilon^2 = 0$.
% We can represent a point on $SE(3)$ using the \emph{Study quadric}~\cite{selig2005study} as the unit dual quaternion $h_0 + \epsilon h_1$ where $h_0 = a_0 + a_1 i+ a_2 j + a_3 k$ and $h_1 = c_0 + c_1 i + c_2 j + c_3 k$. The coefficients for this unit dual unit quaternion can be thought of as a point in $(a_0:a_1:a_2:a_3:c_0:c_1:c_2:c_3) \in \mathbb{R}P^{7}$ that satisfy the relation $h_0h_1^* + h_1h_0^* = 0$ or  $a_0c_0+a_1c_1+a_2c_2+a_3c_3 = 0$. 
% Unit dual quaternions can be used to represent rotations and translations as $q_r + \epsilon q_d$. Let $q_r = r$ and $q_d = \frac{1}{2}tr$ where $t$ is a pure quaternion. We can represent $r$ as a unit quaternion: $r = \cos(\theta/2) + u \sin(\theta/2)$ where $u$ is a unit pure quaternion. (This is from \url{http://www.roboticsproceedings.org/rss12/p13.pdf})

% We rotate about an axis defined by $u$ through an angle $\theta$. The translation is defined by the coordinates in $t$. We can generate the rotation matrix from $u$ and $\theta$ by the graphic below

% \begin{figure}[H]
%     \centering
%     \includegraphics[width = \linewidth]{figures/rotation_matrix.png}
%     % \caption{Caption}
%     % \label{fig:my_label}
% \end{figure}


% This rotation, then translation mapping can also be thought of as taking some pure quaterion $x_1 i + x_2 j + x_3 k$ that corresponds with $x = [x_1,x_2,x_3] \in \R^3$. Then this is represented by the dual quaternion $v = 1 + \epsilon x$. I think the SE(3) mapping $x \mapsto t + Rx$ corresponds to
% \[
% 1 + \epsilon x \mapsto (q_r + \epsilon q_d)v(q_r - \epsilon q_d)^*.
% \]

% This results in
% \begin{align}
%     &(q_r + \epsilon q_d)v(q_r - \epsilon q_d)^* \\
%     &= q_r q_r^* - \epsilon q_r q_d^* + \epsilon q_d q_r^* + \epsilon q_r x q_r^*  \\
%     &= 1 - \epsilon q_r q_d^* + \epsilon q_d q_r^* + \epsilon q_r x q_r^*   \\
%     &= 1 + \epsilon (q_d q_r^* - q_r q_d^*)   + \epsilon q_r x q_r^*   \\
%     &= 1 + \epsilon (q_d q_r^* - q_r q_d^*)   + \epsilon (q_r x q_r^*   - q_d x q_d^*)\\
%     &= 1 + \epsilon( t + (q_r x q_r^*))
% \end{align}

% Another link that discusses this correspondence is \url{http://faculty.mae.carleton.ca/John_Hayes/5507Notes/Ch5JH_Geometry.pdf}.
% \end{dfn}

% \begin{prop}[$SO(4) \to \DQH$]
% % \textbf{Nate will fix this}
% We can get $SO(4)$ by identifying unit double quaternions with opposite sign with one another. We do this by actually just considering unit double quaterions.

% % Let us assume that $t t^* = 1$ which results in $q_d q_d^* = 1$. Now let $q = q_1 + \sigma q_2 = \frac{1}{2} (q_r + q_d) + \sigma\frac{1}{2}(q_r - q_d)$

% % Then we have 
% % \begin{equation}
% %     q_1 q_2^* +  q_2 q_1^* = \frac{1}{2}(q_r q_r^* - q_d q_d^*) = 0
% % \end{equation}


%  Take $(x_0,x_1,x_2,x_3) \in \mathbb{R}^4$ and let $x = x_0 + \sigma(x_1i + x_2j +x_3k)$. Then $q$ represents a rotation in $SO(4)$ by $x \mapsto (q_r + \sigma q_d) x (q_r^* - \sigma q_d^*)$. 
 
% %  Now the question is: how do we pull a matrix out of this operation?
 

% % \begin{align}
% %     (q_1 + \sigma q_2) x (q_1^* - \sigma q_2^*) &= q_1 x q_1^* - q_2 x q_2^* - \sigma q_1 x q_2^* + \sigma q_2 x q_1^*\\
% %     &= q_r x q_d^*+q_d x q_r^* +\sigma (-q_1 x q_2^* + q_2 x q_1^*)\\
% %     &= q_r x q_d^*+q_d x q_r^* +\sigma (q_r x q_d^* - q_d x q_r^*)\\
% %     &=(1+\sigma)q_r x q_d^* + (1-\sigma) q_d x q_r^*\\
% %     &= \sigma (q_r(x_1i + x_2j +x_3k)q_d^* + q_d(x_1i + x_2j +x_3k)q_r^*)+ \sigma (q_r x q_d^* - q_d x q_r^*)\\
% %     &= q_r(x_1i + x_2j +x_3k)q_d^* - q_d(x_1i + x_2j +x_3k)q_r^*\\ &+\sigma (q_r(x_1i + x_2j +x_3k)q_d^* + q_d(x_1i + x_2j +x_3k)q_r^* + x_0(q_r q_d^*-q_d q_r^*))\\
% %     &= q_r(x_1i + x_2j +x_3k)q_d^* - q_d(x_1i + x_2j +x_3k)q_r^*\\ &+\sigma (q_r (x_0 + x_1i + x_2j +x_3k) q_d^* + q_d(-x_0 + x_1i + x_2j +x_3k)q_r^*) \\
% % \end{align}
% % Ew. this is getting gross...
% \end{prop}



\subsection{The Chordal Flag-mean}
% \nate{maybe the representation of point on a flag belongs in the flag manifold definition?}
% We will represent the point $[\![\X]\!] \in\flag(d_1,d_2,\dots, d_k; d)$ using the truncated unitary matrix $\X \in \R^{n \times d_k}$. 
% Let $\X \in \R^{n \times d_k}$ be a truncated unitary matrix that denotes 
% %a point on 
% $[\![\X]\!] \in \flag(d+1)$. 
% Let $m_1 = d_1$, $m_j = d_j-d_{j-1}$ for $j=2,\dots, p$, and $\X_j \in \R^{d \times m_j}$ for $j=1,2,\dots, p$. Then
% \begin{equation}
%     [\![\X]\!] = [ \X_1 ]  \subset [\X_1, \X_2] \subset \dots \subset  [ \X] \subset \R^d.
% \end{equation}
%\textcolor{blue}{Nate: point representation has been moved to to the definition of the flag manifold.} \textcolor{blue}{Nate: chordal distance definition has been moved to the flag manifold definition.}
% Having defined the distance, we are now ready to state the chordal flag-mean estimation problem formally.
Armed with notation for flags (Dfn. \ref{def:flagobj}) and ways to measure distance between them (Dfn. \ref{def:flagdist}), we are prepared to state the chordal flag-mean estimation problem formally.
\begin{dfn}[Weighted chordal flag-mean]
Let $\{ [\![\X^{(i)}]\!] \}_{i=1}^p \subseteq \flag(d+1)$ be a set of points on a flag manifold with weights $\{\alpha_i\}_{i=1}^p \subset \R$ where $\alpha_i \geq 0$. The chordal flag-mean $[\![\bm{\mu}]\!]$ of these points solves:
%the points on the flag manifold then solves: 
%in \ref{eq: chordal flag mean opt}.
\begin{equation} \label{eq: chordal flag mean opt}
    \argmin_{[\![\Y]\!]\in \flag(d+1)} \sum_{i=1}^p \alpha_i d_c([\![\X^{(i)}]\!], [\![\Y]\!])^2.
\end{equation}
% where $[\![\Y]\!] \in \flag(d+1)$.
\end{dfn}
\textit{Note: for $\flag(k;n)$, this amounts to the Grassmannian-mean by Draper~\etal~\cite{draper2014flag}.}
\begin{prop}\label{prop: flag mean}
The chordal flag-mean optimization problem in Eq. \ref{eq: chordal flag mean opt} can be phrased as the Stiefel manifold optimization problem: %a Stiefel manifold optimization problem as follows:
\begin{equation} \label{eq: stiefel opt}
    \argmin_{\Y \in St(d_k,d)} \sum_{j=1}^k m_j -  \tr \left( \I_j \Y^{\top} \mathbf{P}_j  \Y \right). 
\end{equation}
where the matrices $\I_j$ and $\mathbf{P_j}$ are given below
% in \cref{eq: Ij} and \cref{eq: Pj} respectively.
%and one scalar $\beta_j$ in \cref{eq: betaj}.
\begin{equation}\label{eq: Ij}
    (\I_j)_{i,l} = 
    \begin{cases}
        1, & i = l \in \{ d_{j-1} + 1, 
 d_{j-1} + 2, \dots, d_j\} \\
        0, &\text{ otherwise}\nonumber \\
    \end{cases},
\end{equation}
% For example, if we are averaging on $\flag(1,3;4)$ we have  
% %\tolga{I also added $\I_1$. Is this correct?}
% \begin{equation*}
% \I_1 = 
% \begin{bmatrix}
%     1 & 0 & 0\\
%     0 & 0 & 0\\
%     0 & 0 & 0
% \end{bmatrix} \qquad
% \I_2 = 
% \begin{bmatrix}
%     0 & 0 & 0\\
%     0 & 1 & 0\\
%     0 & 0 & 1
% \end{bmatrix}.
% \end{equation*}
% Using $\I_j$ allows us to write $\Y_j \Y_j^{\top} = \Y \I_j \Y^{\top}$ and define:
\begin{equation}\label{eq: Pj}
    \mathbf{P}_j =  \sum_{i=1}^p \alpha_j \X_j^{(i)} {\X_j^{(i)}}^{\top}.
\end{equation}
% $\beta_j$ is defined as
% \begin{equation}\label{eq: betaj}
%     \beta_j = \frac{\alpha_j}{\sum_{i=1}^p \alpha_i}
% \end{equation}
\end{prop}
\begin{proof}[Proof sketch]
We use truncated orthogonal representations for points on the Stiefel and flag manifolds.
% $\Y$ representing a point on $\flag(d+1)$ is the same as to $[\Y] \in St(d,d_k)$.
By the equivalence of minimization problems we write Eq.~\ref{eq: chordal flag mean opt} as
\begin{equation*}
 \argmin_{\Y \in St(d_k,d)}\sum_{j=1}^k  m_j -  \sum_{j=1}^k \sum_{i=1}^p \alpha_i \tr\left( \Y_j^{\top}\X_j^{(i)} {\X_j^{(i)}}^{\top}\Y_j \right).
\end{equation*}
$\I_j$ allows us to write $\Y_j \Y_j^{\top} = \Y \I_j \Y^{\top}$. Using this, properties of trace, and our definition of $\mathbf{P}_j$ we write Eq.~\ref{eq: chordal flag mean opt} as Eq.~\ref{eq: stiefel opt}.
% \begin{equation*}
% \argmin_{\Y \in St(d_k,d)} \sum_{j=1}^k m_j - \tr \left( \I_j \Y^{\top}\mathbf{P}_j\Y \right).
% \end{equation*}

% We begin by realizing \cref{eq: chordal flag mean opt} as an optimization problem over $\Y \in \R^{n \times d_k}$.
% \begin{equation}
%     \min_{\Y \in \R^{n \times d_k}} \sum_{i=1}^p  \alpha_i \sum_{j=1}^k m_j - \tr\left( {\X_j^{(i)}}^{\top} \Y_j \Y_j^{\top} \X_j^{(i)} \right) .
% \end{equation}
% There are two constraints for $[\![\Y]\!] \in \flag(d+1)$. The first constraint is $\Y_j^{\top} \Y_j = \I$. The second constraint is $[\Y_j] \cap [\Y_k] = 0$. These constraints are satisfied when $\Y^{\top}\Y = \I$, e.g. $[\Y] \in St(d_k,d)$. 
% Introducing $a = \sum_{i=1}^p \alpha_i$ and using our definitions for $\I_j$ and $\mathbf{P}_j$, we write:
% \begin{equation}
% f(\Y) = \sum_{j=1}^k\sum_{i=1}^p \alpha_i \tr\left( \Y_j^{\top}\X_j^{(i)} {\X_j^{(i)}}^{\top}\Y_j \right).
% \end{equation}
% and show that the minimization in~\cref{eq: chordal flag mean opt} is identical to $\argmin_{\Y} \sum_{j=1}^k m_j - f(\Y)$.
% We provide the complete proof in our supplementary material.
\end{proof}
We provide the full proof in 
%our supplementary material.
the appendix. We now extend the chordal mean to the case of a certain family of \emph{complete} and \emph{oriented} \emph{flags}.

%%%%%%%%%%%%%%%%%%%%
%OLD PROPOSITION 2

% \nate{ This is useful! keep it!}
% \tolga{is it true or is it a hypothesis?}
% \nate{it's a hypothesis. we need something that says that computing the euclidean mean of a set of subspace representatives doesn't change the orientation of the subspaces if their close enough...}
% % \nate{can you add in the citation from \url{https://poisson.phc.dm.unipi.it/~inversi/GMT.pdf}}
% \begin{prop}\label{prop: not useful}
% Suppose each of $\{ \X_j^{(i)}\}_{i=1}^p$ are sufficiently close. We calculate an oriented chordal flag-mean $[\![\Y^+]\!] \in \flag(d+1)^+$ in the following steps:
% \begin{enumerate}
% \item Compute Euclidean means of $\{ \X_j^{(i)}\}_{i=1}^p$: 
% \begin{equation*}
% \mu_j = \frac{1}{n}\sum_{i=1}^p\X_j^{(i)}
% \end{equation*}
% \item Project $\mu_j$ to a representative for a point on $Gr(m_j,d)$ using the QR decomposition: $\Q \mathbf{R} = \mu_j$. $\widehat{\mu_j} = \Q_{:,:m_j}$. 
% \item Use the eigenvalues of $\Y_j\widehat{\mu_j}^{\top}$ to force $\Y_j^+$ to have the same orientation as $\widehat{\mu_j}$ by changing the sign of it's first column vector. Thus defining $\Y^+_j$ as $\Y^+_j = \Y_j$ if $\prod_{i=1}^{m_j} \lambda_i$ and $\Y^+_j = [-(\Y_j)_1, (\Y_j)_2, \dots, (\Y_j)_{m_j}]$ otherwise.
% \end{enumerate}
% \end{prop}

%%%%%%%%%%%%%%%%%%%%

\begin{prop}\label{prop:eucmean}
% \nate{look at this again}
Let $ \{ \x^{(i)} \}_{i=1}^p \subset \R^d$. Then suppose ${\x^{(i)}}^{\top} \x^{(j)} > 0$ for all $i,j$. Then the naive Euclidean mean $\bm{z} = \frac{1}{n}\sum_{i=1}^p\x^{(i)}$ has the same orientation as each $\x^{(i)}$. 
% \tolga{or the majority of $\x^{(i)}$s?}
\end{prop}
\begin{proof}
    The proof follows from the simple derivation:
\begin{align*}
{\x^{(j)}}^{\top} \bm{z} = {\x^{(j)}}^{\top} \frac{1}{n}\sum_{i=1}^p\x^{(i)}
= \frac{1}{n}\sum_{i=1}^p {\x^{(j)}}^{\top} \x^{(i)} > 0.
\end{align*}
\end{proof}
\begin{dfn}[$\flag^+(1,\dots,d-1;d)$ chordal flag-mean]\label{def:reorient}
% \nate{only true when subspaces are close enough}
%Let $\flag(d+1)^+$ contain only complete flags. 
Let $\{[\![\X^{(i)}]\!]\}_{i=1}^p \subset \flag(1,2,\dots,d-1;d)$  where for each $j$ and any $i$ and $k$, ${\X_j^{(i)}}^{\top}{\X_j^{(k)}} > 0$. Let $[\![\bm{\mu}]\!]$ be the chordal flag-mean (e.g., Eq.~\ref{eq: chordal flag mean opt}) and $\bm{z}_j$ be the Euclidean mean of $\{ \X_j^{(i)}\}_{i=1}^p \in \R^d$. Then the oriented chordal flag-mean is defined as $[\![\bm{\mu}^+]\!] \in \flag^+(1,\dots,d-1;d)^+$:
\begin{equation}\label{eq: orientation ex}
    \bm{\mu}^+_j =
    \begin{cases}
        \Y_j, & \bm{z}_j^{\top} \Y_j \geq 0\\
        -\Y_j, & \text{otherwise}.
    \end{cases}
\end{equation}
\end{dfn}
\begin{remark}
The ordering of the columns of $\bm{\mu}$ is the same as that of each $\X^{(i)}$ because the chordal distance on the flag manifold respects the ordering of the vectors in the flag representation by only comparing $\bm{\mu}_j$ to $\X_j^{(i)}$. So, we only need to correct for the sign of the columns of $\bm{\mu}$.
By Prop.~\ref{prop:eucmean}, we know that the Euclidean mean, $\bm{z}$, 
%$\bm{z}_j =\sum_{i=1}^p \X_j^{(i)}/p$
%\begin{equation}
%\bm{\mu}_j = \frac{1}{p} \sum_{i=1}^p \X_j^{(i)}
%\end{equation}
has the same orientation as each of $\X_j^{(i)}$. We use Eq.~\ref{eq: orientation ex} to force $\bm{\bm{z}}_j^{\top}\bm{\mu}_j^* \geq 0$.
Dfn. \ref{def:reorient} gives us a way to choose which chordal flag-mean representatives are best for averaging representations of motions in $\flag^+(1,2,3;4)$ in Sec.~\ref{sec:motavg}.
\end{remark}
% \begin{proof}
% We begin by considering the Euclidean case. Let $ \{ \x^{(i)} \}_{i=1}^p \subset \R^d$ where ${\x^{(i)}}^{\top} \x^{(j)} \geq 0$ for all $i,j$. The Euclidean mean of these vectors is $\mu = \frac{1}{n}\sum_{i=1}^p\x^{(i)}$ satisfies $\mu^{\top}{\x^{(i)}} \geq 0$ for each $i$. So $\mu$ has the same orientation as each $\x^{(i)}$.

% Using this fact, one can approximate the chordal flag mean on the orientable flag $\flag(1,2,3;4)^+$ by computing the chordal flag-mean, $[\![\Y]\!] \in \flag(1,2,3;4)$, using \ref{alg:chordalavg}. 

% Then we check that each $\Y_j \in \R^d$ is pointing in the same direction as the Euclidean mean of $\{ \X_j^{(i)}\}_{i=1}^p$: $\mu_j$. We find the oriented chordal flag mean $\Y^+ \in \flag(1,2,3;4)^+$ by defining $\Y^+_j$ using \cref{eq: orientation ex}.

% \end{proof}


\input{content/alg_chordalavg.tex}




% This problem can also be formulated as 
% \begin{align}
%     &\argmax_{\y \in \R^{3n}} \y^{\top} \X \y \,\,\text{ s.t. }\,\,\y^{\top} \I'_s \y = 0 \text{, } \y^{\top} \I'_i \y = 1 \text{ for }i=1..3\nonumber\\
%     &\text{where }\quad\y = [\Y_1^\top \Y_2^\top \Y_3^\top]^\top \text{\, and \,} \\
%     &\X = \begin{bmatrix} \sum\limits_{i=1}^p \X_1^{(i)} {\X_1^{(i)}}^{\top}  & 0 & 0\\ 0 & \sum\limits_{i=1}^p \X_2^{(i)} {\X_2^{(i)}}^{\top} & 0 \\ 0 & 0 & \sum\limits_{i=1}^p \X_3^{(i)} {\X_3^{(i)}}^{\top} \end{bmatrix}\nonumber\\
%     %&\text{subject to } \y^{\top} \I'_s \y = 0 \text{, } \y^{\top} \I'_i \y = 1 \text{ for }i=1,2,3\\
%     &\text{and } \I'_s = 
%     \begin{bmatrix}
%     0 & \I & 0 \\
%     0 & 0 & \I \\
%     \I & 0 & 0 \\
%     \end{bmatrix}\text{, }
%     \I'_1 = 
%     \begin{bmatrix}
%     \I & 0 & 0 \\
%     0 & 0 & 0 \\
%     0 & 0 & 0 \\
%     \end{bmatrix}\text{, }
%     \I'_2 = 
%     \begin{bmatrix}
%     0 & 0 & 0 \\
%     0 & \I & 0 \\
%     0 & 0 & 0 \\
%     \end{bmatrix}\text{, }
%     \I'_3 = 
%     \begin{bmatrix}
%     0 & 0 & 0 \\
%     0 & 0 & 0 \\
%     0 & 0 & \I \\
%     \end{bmatrix}\nonumber
% \end{align}

% We can use this formulation to write down the Lagrangian of this optimization problem.

% \begin{align}
%     \mathcal{L}(\y, \lambda_s, \lambda_1, \lambda_2, \lambda_3) &= \y^{\top} \X \y + \lambda_s \y^{\top} \I_s \y \nonumber\\
%     &+\lambda_1(\y^{\top} \I_1 \y-1) + \lambda_2(\y^{\top} \I_2 \y-1)\nonumber \\
%     &+ \lambda_3(\y^{\top} \I_3 \y-1)
% \end{align}

% Then its gradient results in the system of linear equations for $\y,\lambda_s,\lambda_1,\lambda_2, \lambda_3$

% \begin{align}
%     \left( \X+ \begin{bmatrix}
%     \lambda_1 \I & \lambda_s \I & 0 \\
%     0 & \lambda_2 \I & \lambda_s \I \\
%     \lambda_s \I & 0 & \lambda_3 \I \\
%     \end{bmatrix} \right) \y  &= 0 \\
%     \y^{\top} \I_s \y &= 0\\
%     \y^{\top} \I_i \y &= 1 \text{ for } i=1,2,3
% \end{align}

% Using this system of equations, we see that $\y^{\top} \X \y = \lambda_s$.


% Another realization of the same optimization problem involves tensors. Define the tensor $\X\in \R^{4\times 3 \times 4}$:
% \begin{equation}\nonumber
% \X = \begin{bmatrix} \sum\limits_{i=1}^p \X_1^{(i)} {\X_1^{(i)}}^{\top} & \sum\limits_{i=1}^p \X_1^{(i)} {\X_1^{(i)}}^{\top} & \sum\limits_{i=1}^p \X_1^{(i)} {\X_1^{(i)}}^{\top}\end{bmatrix}\nonumber
% \end{equation}
% and the matrix $\Y = [\Y_1 \Y_2 \Y_3] \in \R^{4 \times 3}$. Let's define the trace of a $3$-tensor as the sum of it's super-diagonal. Then the optimization problem amounts to 
% \begin{equation}
%     \max_{\Y^{\top} \Y = \I} \tr (\Y^{\top} \X \Y).
% \end{equation}

% We propose a solution to this problem by first finding each $[\Y_j]$ separately, then imposing the orthogonality constraint $\Y_{j+1}^{\top} \Y_j = \mathbf{0}$.

% To find $[\Y_j]$, we solve
% \begin{equation}
%     \min_{[\Y_j] \in \Gr(n_j,n)} \sum_{i=1}^p \alpha_i d_c([\X_j^{(i)}], [\Y_j])^2.
% \end{equation}

% The solution to this problem is $[\Y_j] = \text{flag-mean}\{[\X_1^{(i)}]\}$.


% Now we propose the following algorithm.

% \begin{itemize}
%     \item For each $n_j$ calculate the flag mean of $\{[\X_j^{(i)}]\}_{i=1}^p$ with weights $\{\alpha_i\}_{i=1}^p$. Call this $[\Z_j]$
%     \item Construct the matrix $\Z = [ \Z_1,  \Z_2,\dots, \Z_d]$
%     \item Compute the QR decomposition $\mathbf{Q} \mathbf{R} = \Z$.
%     \item The real flag-mean of these data is the first $\sum_{j=1}^d n_j$ columns of $\mathbf{Q}$. $\Y_j$ is the first $k_{j-1}$ to $k_j$ columns of $\mathbf{Q}$, where we take $k_0 = 0$.
% \end{itemize}
To compute the proposed mean, we optimize Eq.~\ref{eq: stiefel opt} via RTR methods~\cite{absil2007trust,boumal2014manopt} and re-orient the mean using Dfn.~\ref{def:reorient}.
%\tolga{so where is the final opjective that we solve with Riemannian trust regions (RTR)? Why is it easier than solving eq. 1 directly with RTR?}
%\nate{we solve \cref{eq: stiefel opt} with RTR}
\begin{remark}
%\tolga{nate will comment on this : )}
%\nate{done.}
    % Mention that the flag mean is non-convex (or convex when?). Mention known results about grassmanians and geodesic distances. If not known, mention that we leave it as a future work. :)
    The geodesic distance averages on the Grassmannian (e.g. $\ell_2$-median and Karcher mean) are known to be unique only for certain subsets of the Grassmannian \cite{afsari2011riemannian}. The proof of this revolves around finding the region of convexity of the geodesic distance function and its square.
    Uniqueness for Grassmannian chordal distance averages (\eg the GR-mean \cite{draper2014flag} and -median \cite{mankovich2022flag}) is largely unstudied. It is known that the chordal distance on the Grassmannian approximates the geodesic distance, but its region of convexity %under the chordal metric and its square 
    is an open problem to the best of our knowledge.
    Determining the convexity of our chordal flag-mean and -median would boil down to finding the region of convexity of the chordal distance function and its square on the flag manifold. Additionally, one could generalize geodesic distance averages to the flag manifold using Riemannian operators on flags~\cite{ye2022optimization}, find an algorithm to compute them and their region of convexity. We leave these projects to future work.
\end{remark}

\subsection{The Chordal Flag-median}
We are now ready to provide our iterative algorithm for robust centroid estimation.
% \tolga{Does this algorithm converge?}
% \nate{It seems to converge in most of our experiments. I do not have a proof of convergence yet}
\begin{dfn}[Weighted chordal flag-median]
Let $\{ [\![\X^{(i)}]\!] \}_{i=1}^p \subseteq \flag(d+1)$ be a set of points on a flag manifold with weights $\{\alpha_i\}_{i=1}^p \subset \R$ where $\alpha_i \geq 0$. The chordal flag-median, $[\![\bm{\eta}]\!]$, of these points solves
\begin{equation} \label{eq: chordal flag median opt}
    \argmin_{[\![\Y]\!] \in \flag(d+1)} \sum_{i=1}^p \alpha_i d_c([\![\X^{(i)}]\!], [\![\Y]\!]).
\end{equation}
\end{dfn}
\textit{Note: for $\flag(k;n)$, this amounts to the Grassmannian-median by Mankovich~\etal~\cite{mankovich2022flag}.}

% \begin{prop}
% The flag median optimization problem in \cref{eq: chordal flag median opt} can be phrased as weighted flag mean problem with 
% weights $w_i$: %\cref{eq: irls weights} with $\epsilon = 0$ as long as $d_c([\![\X^{(i)}]\!], [\![\Y]\!]) \neq 0$ for all $i$.
% \begin{equation}\label{eq: irls weights}
%     w_i = \sum_{j=1}^k \frac{\alpha_i}{\max\{d_c([\![\X^{(i)}]\!], [\![\Y]\!]), \epsilon\}}
% \end{equation}
% where $\epsilon = 0$ as long as $d_c([\![\X^{(i)}]\!], [\![\Y]\!]) \neq 0$ for all $i$.
% \end{prop}

\begin{prop}\label{prop: flag median}
The flag-median optimization problem in Eq.~\ref{eq: chordal flag median opt} can be phrased with weights $w_i([\![\Y]\!])$ in: 
\begin{equation}\label{eq: irls weights}
    w_i([\![\Y]\!]) =  \frac{\alpha_i}{\max\{d_c([\![\X^{(i)}]\!], [\![\Y]\!]), \epsilon\}},
\end{equation}
\begin{equation}\label{eq: median equiv}
 \argmin_{[\![\Y]\!] \in \flag(d+1)}\sum_{i=1}^p \sum_{j=1}^k m_j - w_i([\![\Y]\!]) \tr\left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right).
\end{equation}
where $\epsilon = 0$ as long as $d_c([\![\X^{(i)}]\!], [\![\Y]\!]) \neq 0$ for all $i$.
\end{prop}

\begin{proof}[Proof sketch] 
We can encode the constraints and our optimization problem into the Lagrangian:
\begin{align}
\begin{aligned}%\label{eq: lagrangian grad}
    \nabla_{\Y_j} \mathcal{L}
    &=  -2 \sum_{i=1}^p \frac{\alpha_i \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j}{\sqrt{\sum_{j=1}^k m_j - \tr \left( {\X_j^{(i)}}^{\top} \Y_j \Y_j^{\top} \X_j^{(i)} \right) } } \nonumber\\
    &+2\sum_{j=1}^k \lambda_{i,j}\Y_i\Y_i^{\top} \Y_j,\\
    % &+2 \sum\limits{i=1}^k \lambda_{i,j} \Y_i\Y_i^{\top} \Y_j\\[1ex]
     \nabla_{\lambda_{i,j}} \mathcal{L} &=  m_j \delta_{i,j} - \tr\left( \Y_i^{\top}\Y_j\Y_j^{\top} \Y_i \right).
\end{aligned}
\end{align}
Then we take the gradient of the Lagrangian with respect to $\Y_j$ and $\lambda_{i,j}$ and set it equal to zero. So, for each $j$, we have 
\begin{equation*}
4m_j \lambda_{j,j} =  \sum_{i=1}^p \frac{\alpha_i \tr \left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right) }{d_c([\![\X^{(i)}]\!], [\![\Y]\!]) }.
\end{equation*}
Maximizing each $4m_j \lambda_{j,j}$ will minimize the objective function in Eq.~\ref{eq: chordal flag median opt}. We use equivalences of optimization problems to reformulate this maximization as Eq.~\ref{eq: median equiv}.
\end{proof}
% \begin{equation*}
%  \argmin_{[\![\Y]\!] \in \flag(d+1)} \sum_{i=1}^p \frac{\alpha_i}{ d_c([\![\X^{(i)}]\!], [\![\Y]\!])} d_c([\![\X^{(i)}]\!], [\![\Y]\!]).   
% \end{equation*}

% \tolga{Let's shorten this.}
% \nate{look at this naate!!}
% Ignoring our constraints, we can write this optimization problem for $\Y \in \R^{n \times d_k}$.
% \begin{equation}
%     \min_{\Y \in \R^{n \times d_k}} \sum_{i=1}^p \alpha_i \sqrt{\sum_{j=1}^k m_j - \tr\left( {\X_j^{(i)}}^{\top} \Y_j \Y_j^{\top} \X_j^{(i)} \right) }.
% \end{equation}

% Let $\Lambda$ be a symmetric matrix encoding the orthogonality constraints. \tolga{what is the structure of $\Lambda$?} Denote the entry in the $i$th row and $j$th column of $\Lambda$ as $\lambda_{i,j}$. Also, let 
% \begin{equation*}
%     \delta_{i,j} = 
%     \begin{cases}
%         1, & i = j\\
%         0, & i \neq j.
%     \end{cases}
% \end{equation*}
% With the constraints added to the objective, we define the Lagrangian and its gradient w.r.t. ${\Y_j}$ and ${\lambda_{i,j}}$:
% \begin{align*}
%     \mathcal{L}(\Y, \Lambda) &=  \sum_{i=1}^p \alpha_i \sqrt{\sum_{j=1}^k m_j - \tr\left( {\X_j^{(i)}}^{\top} \Y_j \Y_j^{\top} \X_j^{(i)} \right)} \\
%     &- \sum_{i=j}^k \sum_{j=1}^k \lambda_{i,j} (m_j \delta_{i,j} - \tr(\Y_i^{\top}\Y_j\Y_j^{\top} \Y_i)), \\
%     %&\nabla_{\Y_j} \mathcal{L}(\Y, \Lambda) \\
%     \nabla_{\Y_j} \mathcal{L}
%     &=  -2 \sum_{i=1}^p \frac{\alpha_i \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j}{\sqrt{\sum_{j=1}^k m_j - \tr \left( {\X_j^{(i)}}^{\top} \Y_j \Y_j^{\top} \X_j^{(i)} \right) } } \\
%     &+2 \sum\limits{i=1}^k \lambda_{i,j} \Y_i\Y_i^{\top} \Y_j\\[1ex]
%      \nabla_{\lambda_{i,j}} \mathcal{L} &=  m_j \delta_{i,j} - \tr(\Y_i^{\top}\Y_j\Y_j^{\top} \Y_i).
% \end{align*}
% Notice we are not dividing by zero because $d_c([\![\X^{(i)}]\!], [\![\Y]\!]) \neq 0$ for all $i$.
% Now we use $\nabla_{\Y_j} \mathcal{L} = 0$ and $\nabla_{\lambda_{i,j}} \mathcal{L} = 0$ to solve for $\lambda_{j,j}$:
% \begin{align*}
% \sum_{i=1}^k \lambda_{i,j} \Y_i\Y_i^{\top} \Y_j &=  \sum_{i=1}^p \frac{\alpha_i \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j}{(d_c([\![\X^{(i)}]\!], [\![\Y]\!])  }\\
% \sum_{i=1}^k \lambda_{i,j} \Y_j^{\top} \Y_i\Y_i^{\top} \Y_j &=  \sum_{i=1}^p \frac{\alpha_i \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j}{d_c([\![\X^{(i)}]\!], [\![\Y]\!]) } \\
% \sum_{i=1}^k \lambda_{i,j} \tr (\Y_j^{\top} \Y_i\Y_i^{\top} \Y_j) &=  \sum_{i=1}^p \frac{\alpha_i \tr \left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right) }{d_c([\![\X^{(i)}]\!], [\![\Y]\!]) } \\
% \lambda_{j,j} \tr (\Y_j^{\top} \Y_j\Y_j^{\top} \Y_j) &=  \sum_{i=1}^p \frac{\alpha_i \tr \left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right) }{d_c([\![\X^{(i)}]\!], [\![\Y]\!]) } \\
% m_j \lambda_{j,j} &=  \sum_{i=1}^p \frac{\alpha_i \tr \left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right) }{d_c([\![\X^{(i)}]\!], [\![\Y]\!]) } \\
% \end{align*}

% For $[\![\Y]\!]$ to minimize \cref{eq: chordal flag median opt}, we would want to maximize $d_j \lambda_{j,j}$ for each $j$. That is to say, we wish to maximize $\sum_{j=1}^k d_j \lambda_{j,j}$:
% %\begin{equation}
% %\sum_{j=1}^k  \sum_{i=1}^p \frac{\alpha_i \tr\left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right) }{d_c([\![\X^{(i)}]\!], [\![\Y]\!])}.
% %\end{equation}
% \begin{equation}\label{eq: chord flag median max}
% \sum_{i=1}^p \sum_{j=1}^k \frac{\alpha_i}{d_c([\![\X^{(i)}]\!], [\![\Y]\!])} \tr\left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right).
% \end{equation}
% Notice that the maximizing \cref{eq: chord flag median max} is the same as the minimizing \cref{eq: chord flag median min}:
% %\begin{equation}\label{eq: chord flag median max}
% %\sum_{i=1}^p \sum_{j=1}^k \frac{\alpha_i}{d_c([\![\X^{(i)}]\!], [\![\Y]\!])} \tr\left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right).
% %\end{equation}
% \begin{equation}\label{eq: chord flag median min}
% \sum_{i=1}^p \sum_{j=1}^k m_j - \frac{\alpha_i}{ d_c([\![\X^{(i)}]\!], [\![\Y]\!])} \tr\left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right).
% \end{equation}
% % \cref{eq: chord flag median min} can be translated to the Stiefel manifold optimization problem in \cref{eq: stiefel opt median} using a similar method for proving \ref{prop: flag mean}.
% which is minimized at the same point as:
% %The argmin of \cref{eq: chord flag median min} is the same as the argmin of 
% % \begin{equation*}
% %   \sum_{j=1}^k \sum_{i=1}^p m_j -  \tr\left( \Y_j^{\top} \left( \frac{\alpha_i}{ d_c([\![\X^{(i)}]\!], [\![\Y]\!])} \X_j^{(i)}{\X_j^{(i)}}^{\top} \right) \Y_j \right).
% % \end{equation*}
% % Which is the same as the argmin of
% \begin{align*}
% &\sum_{j=1}^k \left( \sum_{i=1}^p \frac{\alpha_i}{ d_c([\![\X^{(i)}]\!], [\![\Y]\!])} \right) m_j \\
% &-  \sum_{i=1}^p\frac{\alpha_i}{ d_c([\![\X^{(i)}]\!], [\![\Y]\!])}\tr\left( \Y_j^{\top}  \X_j^{(i)}{\X_j^{(i)}}^{\top}  \Y_j \right)\\
% &=  \sum_{i=1}^p \frac{\alpha_i}{ d_c([\![\X^{(i)}]\!], [\![\Y]\!])} \sum_{j=1}^k m_j -  \tr\left( \Y_j^{\top}  \X_j^{(i)}{\X_j^{(i)}}^{\top}  \Y_j \right)\\
% &=  \sum_{i=1}^p \frac{\alpha_i}{ d_c([\![\X^{(i)}]\!], [\![\Y]\!])}\sum_{j=1}^k d_c([\![\X^{(i)}]\!], [\![\Y]\!]).
% \end{align*}

% % \begin{align*}
% %     & \sum_{j=1}^k m_j -  \tr\left( \Y_j^{\top} \left( \frac{\alpha_i}{ d_c([\![\X^{(i)}]\!], [\![\Y]\!])} \X_j^{(i)}{\X_j^{(i)}}^{\top} \right) \Y_j \right)\\
% %      &=   \sum_{j=1}^k m_j -  \tr\left( \Y_j^{\top} \mathbf{P}_j' \Y_j \right)\\
% %      &=  \sum_{j=1}^k m_j -  \tr\left( \I_j \Y_j^{\top} \mathbf{P}_j' \Y_j \right)\\
% % \end{align*}


% % We define $\I_j$ using \cref{eq: Ij}. $\mathbf{P}_j'$ is defined in \cref{eq: Pj prime}.

% % When we add in our constraints for $[\![\Y]\!] \in \flag(d_1,d_2,\dots, d_k; d)$, this is the weighted chordal flag mean problem \cref{eq: weighted flag mean}  with weights $w^{(i)}$ \cref{eq: IRLS weights}.


\begin{prop}\label{prop: irls iteration}
 Fixing $[\![\Z]\!] \in \flag(d+1)$, Eq.~\ref{eq: median equiv}, with $w_i([\![\Z]\!])$, becomes
 \begin{equation*}
  \argmin_{[\![\Y]\!] \in \flag(d+1)}\sum_{i=1}^p \sum_{j=1}^k m_j - w_i([\![\Z]\!]) \tr\left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right)
 \end{equation*}
 and is equivalent to a chordal flag-mean with weights $w_i([\![\Z]\!])$.
 Note: $\epsilon = 0$ as long as $d_c([\![\X^{(i)}]\!], [\![\Z]\!]) \neq 0$ for all $i$.
\end{prop}
\begin{proof}[Proof sketch] 
This follows from the proof of Prop.~\ref{prop: flag mean}.
\end{proof}

\input{content/alg_chordalmedian.tex}
Prop.~\ref{prop: flag median} simplifies our optimization problem to Eq.~\ref{eq: median equiv}. Given an estimate for the chordal flag-median, $[\![\Z]\!]$, Prop.~\ref{prop: irls iteration} shows that solving a weighted chordal flag mean problem will approximate the solution to Eq.~\ref{eq: median equiv}. Using the propositions, we are now ready to present our iterative algorithm for flag-median estimation in Alg.~\ref{alg:chordalmedian}.
% Note that, while this algorithm seems to inherit convergence properties of IRLS~\cite{aftab2015convergence} and appears to converge in our experiments, we leave a rigorous proof for a future study.

The convergence of Weiszfeld-type algorithms are well studied in the literature~\cite{aftab2015convergence,beck2015weiszfeld,zhao2020quaternion} and our IRLS algorithm for the chordal flag-median can be proven to  decrease its respective objective function value over iterations. This is what we establish next in Prop.~\ref{prop: flagirls decreasing}, inspired by the proof methods given in~\cite{beck2015weiszfeld}.%$ by leveraging recent results in~\cite{beck2015weiszfeld}:
%\textcolor{blue}{Nate: The proof is inspired by a proof method in~\cite{beck2015weiszfeld}, but does not follow from any results there.}
% This algorithm seems to inherit convergence properties of IRLS~\cite{aftab2015convergence,beck2015weiszfeld} and appears to converge in our experiments. In fact, we find that iterations of our IRLS algorithm for the flag-median decrease its respective objective function value in~\cref{prop: flagirls decreasing}.

\begin{prop}\label{prop: flagirls decreasing}
Let $[\![\Y]\!] \in \flag(d+1)$. Suppose $d([\![\Y]\!],[\![\X^{(i)}]\!]) > \epsilon$ for $i = 1,2, \dots, p$. Also define the maps: $T:\flag(d+1) \rightarrow \flag(d+1)$ as an iteration of Alg.~\ref{alg:chordalmedian} and $f:\flag(d+1) \rightarrow \R$ as the chordal flag-median objective function value. Then
\begin{equation}
f(T([\![\Y]\!])) \leq f([\![\Y]\!]).
\end{equation}
\end{prop}
\begin{proof}[Proof sketch] 
We define the function 
\begin{equation}
h([\![\Z]\!],[\![\Y]\!]) = \sum_{i=1}^p w_i([\![\Z]\!]) d_c([\![\X^{(i)}]\!],[\![\Y]\!])^2.
\end{equation}
By definition of $h$, $T$, and $f$, we have
\begin{equation}
    h(T([\![\Y]\!]),  [\![\Y]\!]) \leq h([\![\Y]\!], [\![\Y]\!]) \leq f([\![\Y]\!]).\nonumber
\end{equation}
We use $h$ and $2a- b < \frac{a^2}{b}$ for $a,b \in \R$, $b > 0$ to find 
\begin{equation}
2f(T([\![\Y]\!]) - f([\![\Y]\!]) \leq h(T([\![\Y]\!]),  [\![\Y]\!]).\nonumber
\end{equation}
From our string of inequalities, we have the desired result. We leave the full proof to our supplementary material.
\end{proof}
\begin{remark}
The distance vanishes when $[\![\Y]\!] = [\![\X^{(i)}]\!]$ (e.g., $d_c([\![\Y]\!],[\![\X^{(i)}]\!]) =0$). In this case, Alg.~\ref{alg:chordalmedian} gets stuck at $[\![\X^{(i)}]\!]$ and the result in Prop.~\ref{prop: flagirls decreasing} becomes
\begin{equation}
f(T([\![\Y]\!])) \leq f([\![\Y]\!]) + {p \epsilon}/{2}.
\end{equation}
This singularity can be removed even for a general Weiszfeld iteration, simply by replacing the weights~\cite{aftab2014generalized}.
\end{remark}

% We use~\cref{prop: flagirls decreasing} to prove that the objective function values of the flag-median converge in~\cref{alg:chordalmedian}.

\begin{prop}\label{cor: flagirls obj converges}
Let $[\![\Y_k]\!] \in \flag(d+1)$ be an iterate of Alg.~\ref{alg:chordalmedian} and $f:\flag(d+1) \rightarrow \R$ denote the chordal flag-median objective value. $f([\![\Y_k]\!])$ converges as $k \rightarrow \infty$ as long as $d_c([\![\Y]\!],[\![\X_i]\!]) >\epsilon$ for $i=1,2,\dots,p$ and each $k$.
\end{prop}

\begin{proof}
 Notice that the real sequence with terms $f([\![\Y_k]\!]) \in \R$ is bounded below by $0$ and is decreasing by Prop.~\ref{prop: flagirls decreasing}. So it converges as $k \rightarrow \infty$.
\end{proof}



% \nate{say that an iteration of the algorithm is derived in the proof...}

%\nate{This can be formalized with a convergence parameter and put into an algorithm environment.}
%Initialize $[\![\Y]\!]$. Then repeat the following until convergence.
%\begin{itemize}
%    \item Assign weights $w_i$ using \cref{eq: irls weights} (with $\epsilon > 0$ to avoid dividing by $0$)
%    \item Re-assign $[\![\Y]\!]$ as weighted chordal flag mean of $\{ [\![\X_i]\!] \}_{i=1}^p$
%\end{itemize}


% There is also a canonical way to represent $q$ as an element of $SO(4)$ using matrices. so perhaps this is another option. But, $\epsilon$ will get in the way of this mapping.

% Now take 
% \[
% \begin{bmatrix}
% x_1 & y_1 & z_1 & w_1 \\
% x_2 & y_2 & z_2 & w_2 \\
% x_3 & y_3 & z_3 & w_3 \\
% x_4 & y_4 & z_4 & w_4
% \end{bmatrix}\in SO(4)
% \]
% Then we use the stereographic projection to get $3$ vectors that represent $X \in FL(1,2,3;4)$ as
% \[
% (x_1,y_1,z_1,w_1) \mapsto \left( \frac{2x}{1+w}, \frac{2y}{1+w}, \frac{2z}{1+w}, 1 \right)
% \]