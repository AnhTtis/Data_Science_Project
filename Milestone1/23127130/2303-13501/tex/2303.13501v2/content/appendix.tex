%\newpage
%\appendix
% \onecolumn

\section{Flag Representations}
% \tolga{Explain flag representations. (\cf~\cref{fig:flag})}
A flag is a nested collection of subspaces of increasing dimension. An illustration of a $\flag(1,2;3)$ is in Fig.~\ref{fig:flag}). 


Flags are a natural representation for time series data as nested ``time subspaces.'' Suppose we data at three times: $\x_{t=1},\x_{t=2},\x_{t=3} \in \R^d$. We can group these data based on their ``effect over time'' in the sense that time $t=1$ stands alone, $t=1$ affects $t=2$, and $t=1$ and $t=2$ affect $t=3$. This grouping gives us the flag of type $\flag(1,2,3:d)$:\vspace{-2.5mm}
\begin{equation}
    \text{span}\{\x_1\}  \subset \text{span}\{\x_1,  \x_2 \} \subset \{ \x_1, \x_2, \x_3\} \subset \R^d.\vspace{-2.5mm}
\end{equation}
% Flags can also model hierarchical data or represent short time series data, \emph{i.e.} the length of the time series is smaller than $d$. 
Flags can also model some hierarchical data using hierarchically nested subspaces. For a nice list of flags in mathematics, see~\cite{ye2022optimization}.

Recall $\flag(d+1) = \flag(d_1,d_2,\dots, d_k;d_{k+1}=d)$, we take $m_1=1$ and $m_j = d_j - d_{j-1}$. There are number of representations for flag manifolds involving quotients ~\cite{ye2022optimization}. We mention the most popular representation in the manuscript. A number works~\cite{ma2021flag, ma2022self, ye2022optimization} use
\begin{equation}\label{eq:sorep}
    \frac{SO(d)}{S(O(m_1) \times O(m_2) \times \cdots \times O(m_{k+1}))}
\end{equation}
where $S(O(m_1)\times \dots \times O(m_k))$ is\vspace{-2.15mm}
\begin{equation}
    \{ (\mathbf{M}_1, \dots, \mathbf{M}_k) \: : \: \prod_{i=1}^k \det(\mathbf{M}_i) = 1\}.\vspace{-1.8mm}
\end{equation}
Other works~\cite{pitaval2013flag, nguyen2022closed} represent flag manifolds using the quotient
\begin{equation}\label{eq:strep}
    \frac{St(d_k, d)}{O(m_1) \times O(m_2) \times \cdots \times O(m_{k})}.
\end{equation}
In this representation, $\X \in St(d_k, d)$ is used to represent the equivalence class
\begin{equation*}
[\![\X]\!] = \left\{ \X \mathbf{O} \: : \: \mathbf{O}_i \in O(m_i) \right\} \in \flag(d+1)
\end{equation*}
where  $\mathbf{O} = \text{diag}(\mathbf{O}_{m_1}, \dots, \mathbf{O}_{m_{k}})$. We use this Stiefel quotient representation in this manuscript.

\begin{figure}[t]
        \includegraphics[width=\columnwidth]{figures/flags.pdf}
	\caption{Illustration of a nested sequence of subspaces corresponding to
a point on the flag manifold. \vspace{-2.5mm}}
	\label{fig:flag}
\end{figure}
Ye~\etal prove that $\flag(d+1)$ is diffeomorphic to Eqs.~\ref{eq:sorep} and \ref{eq:strep} (see Prop. 4 and 12 in~\cite{ye2022optimization}). Additionally, Ye~\etal prove that flags are a closed submanifold of
\begin{equation}
    \Gr(m_1, d) \times \Gr(m_2, d) \times \cdots \Gr(m_k, d).
\end{equation}
Our chordal distance on flag manifolds leverages this product-of-Grassmannians since it is the $2$-norm of the chordal distances between each $\Gr(m_1, d)$.

\section{Proof of Proposition I}
Before providing the full proof, let us recall Prop. I:
\begin{prop}\label{prop: flag mean app}
The chordal flag-mean of $\left \{ [\![\X_i ]\!] \right \}_{i=1}^p \subset \flag(d+1)$ is
\begin{equation}\label{eq:suppflagmeanopt} 
    \left[\![\bm{\mu}\right]\!] := \argmin_{[\![\Y]\!]\in \flag(d+1)} \sum_{i=1}^p \alpha_i d_c([\![\X^{(i)}]\!], [\![\Y]\!])^2
\end{equation}
and can be phrased into a Stiefel manifold optimization problem as
\begin{equation}\label{eq:suppstiefelopt}
    \left[\bm{\mu}\right] = \argmin_{\Y \in St(d_k,d)} \sum_{j=1}^k m_j -  \tr \left( \I_j \Y^{\top} \mathbf{P}_j  \Y \right)
\end{equation}
where the matrices $\I_j$ and $\mathbf{P_j}$ are given in Eq. \ref{eq: Ij2} and Eq. \ref{eq: Pj2} respectively.
%and one scalar $\beta_j$ in \cref{eq: betaj}.
\begin{equation}\label{eq: Ij2}
    (\I_j)_{i,l} = 
    \begin{cases}
        1, & i = l \in \{ d_{j-1} + 1, 
 d_{j-1} + 2, \dots, d_j\} \\
        0, &\text{ otherwise} \\
    \end{cases}
\end{equation}
and define:
\begin{equation}\label{eq: Pj2}
    \mathbf{P}_j =  \sum_{i=1}^p \alpha_j \X_j^{(i)} {\X_j^{(i)}}^{\top}
\end{equation}
% $\beta_j$ is defined as
% \begin{equation}\label{eq: betaj}
%     \beta_j = \frac{\alpha_j}{\sum_{i=1}^p \alpha_i}
% \end{equation}
\end{prop}
For example, if we are averaging on $\flag(1,3;4)$ we have
\begin{equation*}
\I_1 = 
\begin{bmatrix}
    1 & 0 & 0\\
    0 & 0 & 0\\
    0 & 0 & 0
\end{bmatrix} \mathrm{ \quad and \quad}
\I_2 = 
\begin{bmatrix}
    0 & 0 & 0\\
    0 & 1 & 0\\
    0 & 0 & 1
\end{bmatrix}.
\end{equation*}
\begin{proof}
We begin by realizing Eq. \ref{eq:suppflagmeanopt} as an optimization problem using the definition of chordal distance:
\begin{equation*}
    \argmin_{[\![\Y]\!]\in \flag(d+1)} \sum_{i=1}^p  \alpha_i \left( \sum_{j=1}^k m_j - \tr\left( {\X_j^{(i)}}^{\top} \Y_j \Y_j^{\top} \X_j^{(i)} \right) \right).
\end{equation*}
Then we move our summations around to simplify our objective function:
\begin{align*}
    & \sum_{i=1}^p \alpha_i \left( \sum_{j=1}^k m_j - \tr\left( \Y_j^{\top}\X_j^{(i)} {\X_j^{(i)}}^{\top}\Y_j \right) \right)\\
    &=  \sum_{j=1}^k  \left( \sum_{i=1}^p \alpha_i\right) m_j -  \sum_{i=1}^p \alpha_i \tr\left( \Y_j^{\top}\X_j^{(i)} {\X_j^{(i)}}^{\top}\Y_j \right), \\
    &=  \sum_{j=1}^k  \left( \sum_{i=1}^p \alpha_i\right) m_j -  \sum_{j=1}^k \sum_{i=1}^p \alpha_i \tr\left( \Y_j^{\top}\X_j^{(i)} {\X_j^{(i)}}^{\top}\Y_j \right).
\end{align*}

% Let $a = \sum_{i=1}^p \alpha_i$ and define
% \begin{equation*}
% f(\Y) = \sum_{j=1}^k\sum_{i=1}^p \alpha_i \tr\left( \Y_j^{\top}\X_j^{(i)} {\X_j^{(i)}}^{\top}\Y_j \right).
% \end{equation*}
% Notice that $\argmin_{\Y} \sum_{j=1}^k a m_j - f(\Y)$ is the same as $\argmin_{\Y} \sum_{j=1}^k m_j - f(\Y)$. 
% %This means %\tolga{what is $am_j$?} \nate{$am_j$ is a scalar. Its the sum of the weights $a$ times $m_j = d_j - d_{j-1}$}
% Then minimizing 
% \begin{equation*}
% \sum_{j=1}^k \left( \sum_{i=1}^p \alpha_i\right) m_j -  \sum_{j=1}^k \sum_{i=1}^p \alpha_i \tr\left( \Y_j^{\top}\X_j^{(i)} {\X_j^{(i)}}^{\top}\Y_j \right) 
% \end{equation*}
Since $\sum_{i=1}^p \alpha_i$ is constant with respect to $[\![\Y]\!]$, Eq.~\ref{eq:suppflagmeanopt} is equivalent to 
\begin{equation*}
 \argmin_{[\![\Y]\!]\in \flag(d+1)} \sum_{j=1}^k  m_j -  \sum_{j=1}^k \sum_{i=1}^p \alpha_i \tr\left( \Y_j^{\top}\X_j^{(i)} {\X_j^{(i)}}^{\top}\Y_j \right).
\end{equation*}
Using our definitions for $\I_j$ and $\mathbf{P}_j$, we can write the objective function in terms of $\Y$:
\begin{align*}
    &\sum_{j=1}^k  m_j - \tr \left( \Y_j^{\top}\left( \sum_{i=1}^p\alpha_i\X_j^{(i)} {\X_j^{(i)}}^{\top} \right) \Y_j \right), \\
    &= \sum_{j=1}^k m_j - \tr \left( \Y_j^{\top}\mathbf{P}_j \Y_j \right),\\
    &= \sum_{j=1}^k m_j - \tr \left( \Y_j \Y_j^{\top} \mathbf{P}_j  \right),\\
    &= \sum_{j=1}^k m_j - \tr \left( \Y \I_j \Y^{\top} \mathbf{P}_j  \right).%\\
    % &= \sum_{j=1}^k m_j - \tr \left( \I_j \Y^{\top}\mathbf{P}_j\Y \right)
\end{align*}
The third equality is true because $\Y_j \Y_j^{\top} = \Y \I_j \Y^{\top}$.

% \nate{Should I prove equivalence of the optimization problems here or add it to an appendix section?} \tolga{Let's give the proof here first. If it's too long, we can move it.}

There are two constraints for $[\![\Y]\!] \in \flag(d+1)$ according to our representation for points on the flag manifold. The first constraint is $\Y_j^{\top} \Y_j = \I$ for $j=1,2,\dots,p$. The second constraint is $[\Y_j] \cap [\Y_i] = \emptyset$ for all $i \neq j$. These constraints are satisfied when $\Y^{\top}\Y = \I$, e.g. $\Y \in St(d_k,d)$.

Using trace invariance to cyclic permutations, the chordal flag mean optimization problem Eq.~\ref{eq:suppflagmeanopt} is equivalent to the Stiefel optimization problem Eq.~\ref{eq:suppstiefelopt}.
\end{proof}

% \section{Proof of Proposition II}
% Should there be a proposition for this?

\section{Proof of Proposition III}
\begin{prop}
% The flag median optimization problem in \cref{eq: chordal flag median opt} can be phrased as weighted flag mean problem with 
The chordal flag-median of $\left \{ [\![\X_i ]\!] \right \}_{i=1}^p \subset \flag(d+1)$,
\begin{equation} \label{eq:suppflagmedian}
    \left[\![\bm{\eta}]\right] = \argmin_{[\![\Y]\!] \in \flag(d+1)} \sum_{i=1}^p \alpha_i d_c([\![\X^{(i)}]\!], [\![\Y]\!]),
\end{equation}
can be phrased with weights %\cref{eq: irls weights} with $\epsilon = 0$ as long as $d_c([\![\X^{(i)}]\!], [\![\Y]\!]) \neq 0$ for all $i$.
\begin{equation*}
    w_i([\![\Y]\!]) = \sum_{j=1}^k \frac{\alpha_i}{\max\{d_c([\![\X^{(i)}]\!], [\![\Y]\!]), \epsilon\}}
\end{equation*}
as the optimization problem
\begin{equation*}
 \argmin_{[\![\Y]\!] \in \flag(d+1)}\sum_{i=1}^p \sum_{j=1}^k m_j - w_i([\![\Y]\!]) \tr\left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right)
\end{equation*}
with $\epsilon = 0$ as long as $d_c([\![\X^{(i)}]\!], [\![\Y]\!]) \neq 0$ for all $i$.
\end{prop}

\begin{proof}
We can write Eq.~\ref{eq:suppflagmedian} using the definition of chordal distance as
\begin{equation*}
     \argmin_{[\![\Y]\!] \in \flag(d+1)} \sum_{i=1}^p \alpha_i \sqrt{\sum_{j=1}^k m_j - \tr\left( {\X_j^{(i)}}^{\top} \Y_j \Y_j^{\top} \X_j^{(i)} \right) }.
\end{equation*}

The orthogonality constraints for $\Y \in \R^{d \times d_k}$ to represent a point on $\flag(d+1)$ are: (i) $[\Y_j] \cap [\Y_i]  = \emptyset$ for all $i \neq j$ and $\Y_j^{\top} \Y_j = \I$ for all $j$. Let $\theta([\Y_i],[\Y_j])$ denote the vector of principal angles between $[\Y_i]$ and $[\Y_j]$. Using $\tr(\Y_i^{\top}\Y_j\Y_j^{\top} \Y_i) = \|\cos \theta([\Y_i],[\Y_j])\|_2^2$, we encode our orthogonality constraints as
\begin{equation*}
\tr(\Y_i^{\top}\Y_j\Y_j^{\top} \Y_i) = \begin{cases}
                               0 & i \neq j\\
                               d_j & i = j
                              \end{cases}.
\end{equation*}
We will now use 
\begin{equation*}
    \delta_{i,j} = 
    \begin{cases}
        1, & i = j\\
        0, & i \neq j.
    \end{cases}
\end{equation*}
to put these constraints into the Lagrangian. 

Let $\bm{\Lambda}$ be a symmetric matrix of Lagrange multipliers corresponding to the orthogonality constraints. Denote the entry in the $i$th row and $j$th column of $\bm{\Lambda}$ as $\lambda_{i,j}$. With the constraints added to the objective, we define the Lagrangian in Eq.~\ref{eq: lagrangian}.
\begin{align}
\label{eq: lagrangian}
    \mathcal{L}(\Y, \Lambda) &=  \sum_{i=1}^p \alpha_i \sqrt{\sum_{j=1}^k m_j - \tr\left( {\X_j^{(i)}}^{\top} \Y_j \Y_j^{\top} \X_j^{(i)} \right)}\nonumber \\
    &-\sum_{i=j}^k \sum_{j=1}^k \lambda_{i,j} (m_j \delta_{i,j} - \tr(\Y_i^{\top}\Y_j\Y_j^{\top} \Y_i)).
\end{align}


%$[\Y_j] \cap [\Y_i]  = \emptyset$ is equivalent to $\|\cos \theta([\Y_i],[\Y_j])\|_2^2 = 0$ and $\Y_j^{\top} \Y_j = \I$ is approximately $\|\cos \theta([\Y_j],[\Y_j])\|_2^2 = d_j$. 
The gradient of Eq.~\ref{eq: lagrangian} w.r.t. ${\Y_j}$ and ${\lambda_{i,j}}$ is
\begin{align*}
\begin{aligned}\label{eq: lagrangian grad}
    \nabla_{\Y_j} \mathcal{L}
    &=  - \sum_{i=1}^p \frac{\alpha_i \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j}{\sqrt{\sum_{j=1}^k m_j - \tr \left( {\X_j^{(i)}}^{\top} \Y_j \Y_j^{\top} \X_j^{(i)} \right) } } \\
    &+2\sum_{\substack{i=1 \\ i \neq j}}^k \lambda_{i,j}\Y_i\Y_i^{\top} \Y_j + 4\lambda_{j,j}\Y_j\Y_j^{\top} \Y_j,\\
    % &+2 \sum\limits{i=1}^k \lambda_{i,j} \Y_i\Y_i^{\top} \Y_j\\[1ex]
     \nabla_{\lambda_{i,j}} \mathcal{L} &=  m_j \delta_{i,j} - \tr\left(\Y_i^{\top}\Y_j\Y_j^{\top} \Y_i\right).
\end{aligned}
\end{align*}
Notice we are not dividing by zero because $d_c([\![\X^{(i)}]\!], [\![\Y]\!]) \neq 0$ for all $i$.
Now we use $\nabla_{\Y_j} \mathcal{L} = \bm{0}$ and $\nabla_{\lambda_{i,j}} \mathcal{L} = 0$ to solve for $\lambda_{j,j}$. 

First we will work with $\nabla_{\Y_j} \mathcal{L} = \bm{0}$.
\begin{align*}
    \begin{aligned}
        \mathbf{0} &=  - \sum_{i=1}^p \frac{\alpha_i \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j}{d_c([\![\X^{(i)}]\!], [\![\Y]\!])} \\
    &+2\sum_{\substack{i=1 \\ i \neq j}}^k \lambda_{i,j}\Y_i\Y_i^{\top} \Y_j + 4\lambda_{j,j}\Y_j\Y_j^{\top} \Y_j,\\
    &=  - \sum_{i=1}^p \frac{\alpha_i \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j}{d_c([\![\X^{(i)}]\!], [\![\Y]\!])} \\
    &+2\sum_{\substack{i=1 \\ i \neq j}}^k \lambda_{i,j}\Y_j^{\top}\Y_i\Y_i^{\top} \Y_j + 4\lambda_{j,j}\Y_j^{\top}\Y_j\Y_j^{\top} \Y_j,\\
    0 &=  - \sum_{i=1}^p \frac{\alpha_i \tr \left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right)}{d_c([\![\X^{(i)}]\!], [\![\Y]\!])} \\
    &+2\sum_{\substack{i=1 \\ i \neq j}}^k \lambda_{i,j}\tr \left( \Y_j^{\top}\Y_i\Y_i^{\top} \Y_j\right) + 4\lambda_{j,j}\tr \left(\Y_j^{\top}\Y_j\Y_j^{\top} \Y_j\right).\\
    \end{aligned}
\end{align*}
% \begin{align*}
% \sum_{i=1}^k \lambda_{i,j} \Y_i\Y_i^{\top} \Y_j &=  \sum_{i=1}^p \frac{\alpha_i \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j}{(  }\\
% \sum_{i=1}^k \lambda_{i,j} \Y_j^{\top} \Y_i\Y_i^{\top} \Y_j &=  \sum_{i=1}^p \frac{\alpha_i \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j}{d_c([\![\X^{(i)}]\!], [\![\Y]\!]) } \\
% \sum_{i=1}^k \lambda_{i,j} \tr (\Y_j^{\top} \Y_i\Y_i^{\top} \Y_j) &=  \sum_{i=1}^p \frac{\alpha_i \tr \left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right) }{d_c([\![\X^{(i)}]\!], [\![\Y]\!]) } \\
% \end{align*}
% \nate{now we use the other part of the lagrangian}
Using $\nabla_{\lambda_{i,j}} \mathcal{L} = 0$ simplifies our equation to
\begin{align*}
4\lambda_{j,j} \tr (\Y_j^{\top} \Y_j\Y_j^{\top} \Y_j) &=  \sum_{i=1}^p \frac{\alpha_i \tr \left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right) }{d_c([\![\X^{(i)}]\!], [\![\Y]\!]) }, \\
4m_j \lambda_{j,j} &=  \sum_{i=1}^p \frac{\alpha_i \tr \left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right) }{d_c([\![\X^{(i)}]\!], [\![\Y]\!]) }. \\
\end{align*}

% \begin{equation}
%   \sum_{i=1}^p \frac{\alpha_i \tr \left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right) }{d_c([\![\X^{(i)}]\!], [\![\Y]\!])}  = 
%  d_j\lambda_{j,j} 
% \end{equation}

For $[\![\Y]\!]$ to minimize Eq.~\ref{eq:suppflagmedian}, we would want to maximize $m_j \lambda_{j,j}$ for each $j$. That is to say, we wish to maximize $\sum_{j=1}^k m_j \lambda_{j,j}$:
%\begin{equation}
%\sum_{j=1}^k  \sum_{i=1}^p \frac{\alpha_i \tr\left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right) }{d_c([\![\X^{(i)}]\!], [\![\Y]\!])}.
%\end{equation}
\begin{equation}\label{eq:flagmedianmax}
\sum_{i=1}^p \sum_{j=1}^k \frac{\alpha_i}{d_c([\![\X^{(i)}]\!], [\![\Y]\!])} \tr\left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right).
\end{equation}
Maximizing Eq.~\ref{eq:flagmedianmax} is the same as minimizing 
%\begin{equation}\label{eq: chord flag median max}
%\sum_{i=1}^p \sum_{j=1}^k \frac{\alpha_i}{d_c([\![\X^{(i)}]\!], [\![\Y]\!])} \tr\left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right).
%\end{equation}
\begin{equation*}%\label{eq: chord flag median min}
\sum_{i=1}^p \sum_{j=1}^k m_j - \frac{\alpha_i}{ d_c([\![\X^{(i)}]\!], [\![\Y]\!])} \tr\left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right).
\end{equation*}
Using the definition of $w_i([\![\Y]\!])$, this minimization is
\begin{equation*}
 \argmin_{[\![\Y]\!] \in \flag(d+1)}\sum_{i=1}^p \sum_{j=1}^k m_j - w_i([\![\Y]\!]) \tr\left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right)
\end{equation*}

% \cref{eq: chord flag median min} can be translated to the Stiefel manifold optimization problem in \cref{eq: stiefel opt median} using a similar method for proving \ref{prop: flag mean app}.
% which is minimized at the same point as:
%The argmin of \cref{eq: chord flag median min} is the same as the argmin of 
% \begin{equation*}
%   \sum_{j=1}^k \sum_{i=1}^p m_j -  \tr\left( \Y_j^{\top} \left( \frac{\alpha_i}{ d_c([\![\X^{(i)}]\!], [\![\Y]\!])} \X_j^{(i)}{\X_j^{(i)}}^{\top} \right) \Y_j \right).
% \end{equation*}
% Which is the same as the argmin of
% \begin{align*}
% &\sum_{j=1}^k \left( \sum_{i=1}^p \frac{\alpha_i}{ d_c([\![\X^{(i)}]\!], [\![\Y]\!])} \right) m_j \\
% &-  \sum_{i=1}^p\frac{\alpha_i}{ d_c([\![\X^{(i)}]\!], [\![\Y]\!])}\tr\left( \Y_j^{\top}  \X_j^{(i)}{\X_j^{(i)}}^{\top}  \Y_j \right)\\
% &=  \sum_{i=1}^p \frac{\alpha_i}{ d_c([\![\X^{(i)}]\!], [\![\Y]\!])} \sum_{j=1}^k m_j -  \tr\left( \Y_j^{\top}  \X_j^{(i)}{\X_j^{(i)}}^{\top}  \Y_j \right)\\
% &=  \sum_{i=1}^p \frac{\alpha_i}{ d_c([\![\X^{(i)}]\!], [\![\Y]\!])} d_c([\![\X^{(i)}]\!], [\![\Y]\!]).
% \end{align*}
% \begin{align*}
%     & \sum_{j=1}^k m_j -  \tr\left( \Y_j^{\top} \left( \frac{\alpha_i}{ d_c([\![\X^{(i)}]\!], [\![\Y]\!])} \X_j^{(i)}{\X_j^{(i)}}^{\top} \right) \Y_j \right)\\
%      &=   \sum_{j=1}^k m_j -  \tr\left( \Y_j^{\top} \mathbf{P}_j' \Y_j \right)\\
%      &=  \sum_{j=1}^k m_j -  \tr\left( \I_j \Y_j^{\top} \mathbf{P}_j' \Y_j \right)\\
% \end{align*}
% We define $\I_j$ using \cref{eq: Ij}. $\mathbf{P}_j'$ is defined in \cref{eq: Pj prime}.
% When we add in our constraints for $[\![\Y]\!] \in \flag(d_1,d_2,\dots, d_k; d)$, this is the weighted chordal flag mean problem \cref{eq: weighted flag mean}  with weights $w^{(i)}$ \cref{eq: IRLS weights}.
\end{proof}

\begin{prop}
 Fix $[\![\Z]\!] \in \flag(d+1)$. Then the minimizer of 
 \begin{equation}\label{eq: median equiv app}
  \sum_{i=1}^p \sum_{j=1}^k\left( m_j - w_i(\Z) \tr\left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right)\right)
 \end{equation}
 over $[\![\Y]\!] \in \flag(d+1)$ is the weighted chordal flag mean of $\{ [\![ \X^{(i)}]\!]\}_{i=1}^p \in \flag(d+1)$ with weights $w_i(\Z)$.
 Note: $\epsilon = 0$ as long as $d_c([\![\X^{(i)}]\!], [\![\Z]\!]) \neq 0$ for all $i$.
\end{prop}

\begin{proof}
By re-arranging the summations in Eq.~\ref{eq: median equiv app}, we see its minimizer is also
 \begin{equation*}
 \argmin_{[\![\Y]\!] \in \flag(d+1)}\sum_{j=1}^k  m_j - \sum_{j=1}^k \sum_{i=1}^p w_i(\Z) \tr\left( \Y_j^{\top} \X_j^{(i)}{\X_j^{(i)}}^{\top} \Y_j \right).
\end{equation*}
We showed that this is the same as the chordal flag-mean optimization problem with weights $w_i(\Z)$ in the proof of Prop.~\ref{prop: flag mean app}.
\end{proof}

\section{Proof of Proposition VI}
%%%%%%%%%%%%%%%%%%%%%%
%Potential convergence results!
\begin{prop}
Let $[\![\Y]\!] \in \flag(d+1)$ and $\epsilon > 0$. Assume that $d([\![\Y]\!],[\![\X^{(i)}]\!]) > \epsilon$ for $i = 1,2, \dots, p$. Denote the flag median objective function value as $f:\flag(d+1) \rightarrow \R$ and an iteration of our chordal flag-median IRLS algorithm as $T:\flag(d+1) \rightarrow \flag(d+1)$. Then
\[
f(T([\![\Y]\!])) \leq f([\![\Y]\!]).
\]
\end{prop}

\begin{proof}
Assuming that $d([\![\Y]\!],[\![\X^{(i)}]\!]) > \epsilon$ for $i = 1,2, \dots, p$, we define the function $h: \flag(d+1) \times \flag(d+1) \rightarrow \R$ as
\begin{align*}\label{eq: h flagirls def}
\begin{aligned}
h([\![\Z]\!], [\![\Y]\!]) &= \sum_{i=1}^p w_i([\![\Y]\!]) d_c([\![\Z]\!],[\![\X^{(i)}]\!])^2,\\  
w_i([\![\Y]\!]) &=  \frac{1}{\max \left\{ d_c([\![\Y]\!],[\![\X^{(i)}]\!]), \epsilon \right \} } \\
&= \frac{1}{ d_c([\![\Y]\!],[\![\X^{(i)}]\!]) }.
\end{aligned}
\end{align*}

Some algebra reduces $h([\![\Z]\!], [\![\Y]\!])$ to
% \njm{$w_i^2$ is not what was written below. I think we need to change it to $w_i^2 = \frac{1}{\max \{ \sqrt{m_i - \tr{\Y^{\top} \X_i \X_i^{\top} \Y}, \epsilon}}$}
\begin{align*}
    h([\![\Z]\!], [\![\Y]\!]) &= \sum_{i=1}^p w_i([\![\Y]\!]) d_c([\![\Z]\!],[\![\X^{(i)}]\!])^2,\\
    &= \sum_{i=1}^p \frac{d_c([\![\Z]\!],[\![\X^{(i)}]\!])^2}{ d_c([\![\Y]\!],[\![\X^{(i)}]\!]) }.\\
\end{align*}

$h([\![\Z]\!], [\![\Y]\!])$ is the weighted flag-mean objective function (of $\{[\![\X^{(i)}]\!]\}_i$) with weights $w_i([\![\Y]\!])$. So minimizing $h([\![\Z]\!], [\![\Y]\!])$ over $[\![\Z]\!]$ is an iteration of our IRLS algorithm to compute the flag-median. In other words,

%\njm{is this true?} Also, $h(\Z,\Y)$ is a convex function in $\Z$. So, if we fix $\Y$,we have 
\begin{equation}\label{eq: h flagirls min}
    T([\![\Y]\!]) = \argmin_{[\![\Z]\!] \in \flag(d+1)} h([\![\Z]\!], [\![\Y]\!]).
\end{equation}
Using Eq.~\ref{eq: h flagirls min}, we have
\begin{equation*}
    h(T([\![\Y]\!]), [\![\Y]\!]) \leq h([\![\Y]\!], [\![\Y]\!]).
\end{equation*}

By the definition of $h$
\begin{align*}
    h([\![\Y]\!], [\![\Y]\!]) &= \sum_{i=1}^p \frac{d_c([\![\Y]\!],[\![\X^{(i)}]\!])^2}{d_c([\![\Y]\!],[\![\X^{(i)}]\!])}, \\
    &= \sum_{i=1}^p d_c([\![\Y]\!],[\![\X^{(i)}]\!]),\\
    &= f([\![\Y]\!]).
\end{align*}
This means, we have
\begin{equation}\label{eq: h flagirls less}
    h(T([\![\Y]\!]),  [\![\Y]\!]) \leq f([\![\Y]\!]).
\end{equation}






Now we use the identity from algebra: $\frac{a^2}{b} \geq 2a-b$ for any $a,b \in \R$ and $b > 0$. Let 
\begin{equation*}
    a = d_c([\![\Z]\!],[\![\X^{(i)}]\!]) \text{ and } b =  d_c([\![\Y]\!],[\![\X^{(i)}]\!]) .
\end{equation*} 
Then
\begin{align*}
h([\![\Z]\!], [\![\Y]\!]) &\geq 2\sum_{i=1}^p d_c([\![\Z]\!],[\![\X^{(i)}]\!])\\
&- \sum_{i=1}^p  d_c([\![\Y]\!],[\![\X^{(i)}]\!]),  \\
&= 2f([\![\Z]\!]) - f([\![\Y]\!]).
\end{align*}
% In summary, we have
% \begin{equation}\label{eq: h inequality}
%     h([\Z], [\Y]) \geq 2f_{fmed}(\Z)- f_{fmed}(\Y)
% \end{equation}
Now, take $[\![\Z]\!] = T([\![\Y]\!])$. This gives us
\begin{equation}\label{eg: h flagirls greater}
    h(T([\![\Y]\!]), [\![\Y]\!]) \geq 2f(T([\![\Y]\!]))-  f([\![\Y]\!]).
\end{equation}

Then, combining Eq.~\ref{eg: h flagirls greater} with Eq.~\ref{eq: h flagirls less}, we have

\begin{align*}
    2f(T([\![\Y]\!]))- f([\![\Y]\!]) &\leq f([\![\Y]\!]), \\
    f(T([\![\Y]\!])) &\leq f([\![\Y]\!]) . 
\end{align*}
\end{proof}







\section{Further Experimental Evaluation}
\subsection{Further Qualitative Results on Faces Dataset}
We now show in Fig.~\ref{fig:sra12} further visualizations of Flag and Grassmann averages of faces.  
\begin{figure}[t]
        \includegraphics[width=\columnwidth]{figures/faces2.pdf}
	\caption{Averaging a collection of faces belonging to three different identities, captured under varying illumination: center, left and right. Notice that the first dimension of the flag representations is center illuminated, better representing the mean compared to Grassmannian.\vspace{-3mm}}
	\label{fig:sra12}
\end{figure}

\subsection{Further Qualitative Results on MNIST}
We use $20$ examples (e.g., points on $\flag(1,2;748)$) of $6$s and add $10$ examples of $7$s. We use the same workflow from the manuscript to represent the MNIST digits on $\Gr(2,748)$ and $\flag(1,2;748)$. We compute the averages on the Grassmannian~\cite{draper2014flag, mankovich2022flag} and flag (ours). The reshaped first dimension of each of these averages is in Fig.~\ref{fig:mnist_qual}. The brightness of the bottom left corner of each image is brighter the more present the $7$s digit (outlier class) is in the image. Notice the bottom left corner of each image, boxed in red, becomes darker as we move from left to right. So, our averaging on the flag is more robust to outliers than Grassmannian averaging. In fact, the bottom left corner of the flag-median is the darkest. Therefore, our flag-median is the least affected by the outlier examples of $7$s.
\begin{figure}[t]
    \includegraphics[width=\columnwidth]{figures/mnist_qual_20x6_10x7.pdf}
	\caption{The first dimension of Grassmannian (``GR-'') and flag (``FL-'') averages of a data set with $20$ representations of $6$s and $10$ representations of $7$s. The bottom red boxes are the enlarged version of the upper image. Our flag-median is the least affected by the outlier examples of $7$s.}
	\label{fig:mnist_qual}
\end{figure}

\subsection{LBG Clustering on UFC YouTube}
 We use a subset of the UCF YouTube Action dataset~\cite{liu2009recognizing} to run a similar experiment to what was done by Mankovich~\etal \cite{mankovich2022flag}. The dataset contains labeled RGB video clips of people performing actions. Within each labeled action, the videos are grouped into subsets of clips with common features. We take approximately one example from each subset from each class. This results in $23$ examples of basketball shooting, $23$ of biking/cycling, $25$ of diving, $25$ of golf swinging, $24$ of horse back riding, $25$ of soccer juggling, $23$ of swinging, $24$ of tennis swinging, $24$ of trampoline jumping, 24 of volleyball spiking, and $24$ of walking with a dog. We convert these frames to gray scale, then we use INTER\_AREA interpolation from the OpenCV package~\cite{opencv_library} to resize the frames to have only $450$ pixels. This is, on average, only $1\%$ of the number of pixels in the original frame. We vectorize and horizontally stack each video, then use the first $10$ columns of $\mathbf{Q}$ from the QR decomposition to realize each video as a point on $\Gr(10,450)$ and $\flag(1,2,\dots,10;450)$.

 We run Linde-Buzo-Gray (LBG) clustering on these videos and report the resulting cluster purities in Fig.~\ref{fig:lbgres}. Clustering on the flag manifold with our flag averages (blue boxes) improves cluster purities over Grassmannian methods. We also see higher variance in cluster purities for flag methods. Even though we are only working with approximately $1\%$ of the total number of pixels in each frame, we are able to produce cluster purities that are competitive with those reported in~\cite{mankovich2022flag} using a similar set of videos. Specifically, our flag-LBG clustering is well within $0.1$ of the highest cluster purities reported in~\cite{mankovich2022flag}. Overall, our flag methods improve cluster purities in a head-to-head experiment while remaining competitive with Grassmannian LBG with only using approximately 1\% of pixels per frame.
\begin{figure}[t]
        \includegraphics[width=\columnwidth]{figures/youtube_lbg_10trials_small1.pdf}
	\caption{LBG cluster purities of YouTube videos with $10$ experiments with different numbers of centers, codebook sizes. The Grassmannian, \eg ``GR-'', boxes are results from LBG clustering using chordal distance and Grassmannian averages from~\cite{draper2014flag,mankovich2022flag}. The ``FL-" boxes are results from using the flag chordal distance and our flag-mean and -median.\vspace{-3mm}}
	\label{fig:lbgres}
\end{figure}

\subsection{Ablation Studies}
\paragraph{Robustness to initialization}
For Fig.~\ref{fig:init_ablation}, we fix a single-cluster dataset of $100$ points on $\flag(1,2,3;10)$ then run our IRLS algorithm for the flag-median and Stiefel RTR~\cite{absil2007trust,boumal2014manopt} for the flag-mean with initial points that are further and further away from the center of the dataset. Our dataset is computed the same way we compute synthetic datasets for the manuscript: compute a center, $[\![\mathbf{C}]\!] \in \flag(1,2,3;10)$, and then add noise to the center using the parameter $\delta$. For this experiment we use $\delta = .2$. Our initial point for our IRLS algorithm and RTR is computed as the first $3$ columns of the QR decomposition of $\mathbf{C}  + \Z \delta_{init}$ where $\Z \in \R^{10 \times 3}$ has entries sampled from $\mathcal{U}[-.5,.5)$. We call $\delta_{init}$ the noise added to the initial point and plot it on the $x$-axis of Fig.~\ref{fig:init_ablation}. The ``Error" is the chordal distance on $\flag(1,2,3;10)$ between the center and the algorithm output. ``Iterations" is the number of iterations of RTR for the flag-mean and IRLS for the flag-median until convergence. ``Cost" is the objective function values of the algorithm output. Our IRLS algorithm estimates the flag-median is further away from the center, $[\![\mathbf{C}]\!]$ than the flag-mean estimate. Also, the number of iterations of Stiefel RTR increases as we move the center further away from our dataset whereas our IRLS algorithm number of iterations remains constant. Finally, the cost value for the flag-median estimate is higher than the flag-mean estimate because the flag-mean estimate objective function likely contains squares of values less than $1$.

\begin{figure}[t]
    \includegraphics[width=\linewidth]{figures/init_ablation.pdf}
    \caption{A plot of the robustness of our IRLS algorithm for the flag-median and Stiefel RTR for the flag-mean to initialization. For the median, we report the IRLS-iterations whereas for the mean, we report the RTR-iterations. Note that, even in large noise variances, both of the algorithms converge to a reasonable point regardless of initialization.}
    \label{fig:init_ablation}
\end{figure}

\paragraph{Computation time}
% We conducted further experiments with ambient dimension and ``dimension gap'' and plot the runtime in~\cref{fig:timed,fig:timedk}, respectively. As shown, the runtime increases linearly with dimension and decreases linearly with dimension gap. The flag-mean is less affected than the median when changing $d$ or $d-k$. \tolga{Write more details on this.}

We conducted the further experiments with ambient dimension and ``dimension gap'' and plot the runtime of Alg. 1 (Fl-Mean) and Alg. 2 (Fl-Median) in Fig.~\ref{fig:times}. As shown, the runtime increases linearly with dimension and decreases linearly with dimension gap. The FL-Mean is less affected than the FL-Median when changing $d$ or $d-k$. For high $d$, the runtime for the FL-Median is unstable with high standard deviation and high changes in the mean runtime across small changes in $d$. In contrast, the FL-Mean is relatively stable in run-time to increasing $d$. When we vary dimension gap ($d-k$), there is a negligible standard deviation in runtime for the FL-Mean and FL-Median. The FL-Median algorithm is very slow for low $d-k$ and as fast as the FL-Mean for high $d-k$. The FL-Mean algorithm runtime is more stable to changes in dimension gap than the FL-Median.

\begin{figure}[t]
        \includegraphics[width=\columnwidth]{figures/time_plots.pdf}\vspace{-5mm}
	\caption{Time to compute the chordal flag-mean and -median of $10$ points over $20$ random trials. The shaded region is the standard deviation. We vary $d$ in $\flag(1,2;d)$ (left) and vary $d-k$ in $\flag(1,k;d=50)$ (right).\vspace{-4.5mm}}
	\label{fig:times}
\end{figure}

% \begin{figure}[t]
%         \includegraphics[width=\columnwidth]{figures/time_d.pdf}\vspace{-3mm}
% 	\caption{Time to compute the chordal flag-mean and -median of $10$ points in $\flag(1,2;d)$ over $20$ random trials. The shaded region is the standard deviation.\vspace{-3mm}}
% 	\label{fig:timed}
% \end{figure}
% \begin{figure}[t]
%         \includegraphics[width=\columnwidth]{figures/time_gap.pdf}\vspace{-3mm}
% 	\caption{Same plots as above but for $\flag(1,k;d=50)$.\vspace{-3mm}}
% 	\label{fig:timedk}
% \end{figure}





\subsection{Motion Averaging}

\paragraph{On error metrics}
We score the quality of our averages using the geodesic distance on the pose manifold $SE(3)$ (or equivalently the geodesic distance on dual quaternions):
\begin{equation}\label{eq:se3error}
    \epsilon(\mathbf{T}_1,\mathbf{T}_2) = {\frac{1}{\pi}\|\log(\mathbf{R_1}^\top\mathbf{R}_2)\|_2+\lambda_T\|\mathbf{t}_1-\mathbf{t}_1\|_2}
\end{equation}
where $(\mathbf{R}_i,\mathbf{t}_i)$ are extracted from $\mathbf{T}_i$ as rotational and translational components, respectively. $\lambda_T$ is a scene dependent strictly positive scaling factor. Note that, as discussed in the paper, this is very much related to the $\lambda$ used in motion contraction. $\log(\cdot):SO(3)\to \mathfrak{so}(3)$ denotes the \emph{logarithmic map} of the $SO(3)$-manifold. As such, this residual defined in Fig.~\ref{eq:se3error} is equivalent to:
\begin{equation}\label{eq:tracedist}
    \epsilon(\mathbf{T}_1,\mathbf{T}_2) = {\frac{1}{\pi}\arccos\left(\mathrm{tr} \frac{\mathbf{R}_1^\top\mathbf{R}_2 -1}{2} \right)+\lambda_s\|\mathbf{t}_1-\mathbf{t}_2\|_2}.\nonumber
\end{equation}

\paragraph{Single rotation averaging}
Single rotation averaging where a set of rotation matrices are averaged, is a special case of motion averaging where the translational components are set to zero. Due to the compactness of the manifold, additional $SO(3)$-specific averaging algorithms can be employed for the case of pure rotations. To compare our method against a larger class of well established, rotation-specific averaging algorithms we opt for zeroing the translational components, performing averages and reporting only the angular errors. Figs.~\ref{fig:rotavg1},\ref{fig:rotAvgRobust} present our results with increasing noise and increasing outliers respectively. For the case of outliers, we further include the recent robust methods of Rie \& Civera~\cite{lee2020robust}. \emph{Naive} refers to the Euclidean averages of rotation matrices (with a subsequent projection). 

\paragraph{Impact of $\lambda$}
As we have discussed in the paper, the scene-dependent scaling $\lambda$ is a hyper-parameter in our $SE(3)$-averaging. Note that, other distance metrics such as the ones dependent on 3D point distances exist~\cite{bregier2018defining}. These metrics exploit the action of 3D transformations on an auxiliary point set to measure distances in the 3D configuration of points. However, even those are somehow dependent upon a hyper-parameter such as the diameter of the point set or the point configurations. This is why we evaluate the impact of $\lambda$ in our averaging. In particular, we design multiple experiments to average $250$ random points on $SE(3)$ generated with an angular noise level of $0.075$. The radius of this scene is set to $1$, up to a translational noise level of $0.15$. This also means that the optimal $\lambda^\star$ (unknown during test) is $1$. We then vary $\lambda\in [0.002, 0.025, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.8, 2, 2.25,\\2.5]$ and average $250$ random points, over $50$ runs. In each run, the point sets differ randomly. We compute the errors using Eq.~\ref{eq:tracedist} with $\lambda=1$ and accumulate them over all runs. Fig.~\ref{fig:lambdaexp} plots the average errors per each $\lambda$. In this outlier-free regime, our flag-mean and flag-median are almost aligned when $\lambda=1$. Flag-median shows slight advantage over the mean for smaller values of $\lambda$.
\begin{figure}[t]
        \includegraphics[width=\columnwidth]{figures/perturb_axis_change.pdf}
	\caption{Single rotation averaging results for increasing levels of axial noise on synthetic, outlier-free data.}
	\label{fig:rotavg1}
\end{figure}
\begin{figure}[t]
        \includegraphics[width=\columnwidth]{figures/robust_averaging.pdf}
	\caption{Single rotation averaging results for increasing levels of outliers on synthetic data with $<5^\circ$ of noise.}
	\label{fig:rotAvgRobust}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=\linewidth]{figures/lambda_exp.pdf}
    \caption{Impact of $\lambda$ our flag-mean and flag-median. During data generation we use $\lambda^\star=1$, whereas our algorithms use varying $\lambda$ as plotted in the $x$-axis. We then compare the resulting averages to the ground truth average and report the deviation. This is an outlier-free regime and as expected, median \& mean prototypes overlap when we are at the optimal value, $\lambda=1$. Though, we also see that our algorithms are not too sensitive to the exact choice of this parameter.\vspace{-3mm}}
    \label{fig:lambdaexp}
\end{figure}



\section{On Motion \& Rotation Averaging}
Motion averaging lies at the heart of structure from motion and 3D reconstruction in multi-view settings. Typically, the problem of recovering individual motions for a set of cameras when we are given a number of relative motion estimates between camera pairs is known as \emph{multiple motion averaging} or \emph{transformation / motion synchronization}~\cite{huang2019learning,arrigoni2016spectral,birdal2018bayesian,dellaert2020shonan,eriksson2019rotation,chatterjee2017robust,govindu2001combining,govindu2004lie}. This problem is foundational for 3D structure recovery. Synchronization algorithms usually solve multiple \emph{single averaging} sub-problems robustly~\cite{birdal2020synchronizing,li2020hybrid,hartley2011l1}, hence the name multiple motion averaging. These sub-problems involving the computation of a robust-barycenter of a set of points on $SE(3)$, are commonly known as \emph{robust single motion averaging} and is the focus of our paper. Although our method directly operates on the product manifold, it is a de-facto standard to decompose the problem into \emph{single translation averaging} and \emph{single rotation averaging}. The lattter is particularly well studied due to the interesting mathematical structure of the problem~\cite{hartley2013rotation,hartley2011l1,lee2020robust,govindu2004lie}. Nevertheless, our method is general enough to solve all of these variants, as we have experimented with in Figs.~\ref{fig:rotavg1},\ref{fig:rotAvgRobust}.


