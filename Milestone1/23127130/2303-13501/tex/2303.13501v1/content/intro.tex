\section{Introduction}\label{sec:intro}
Subspace analysis is a key workhorse of machine learning since various forms of data and parameter sets admit a compact representation as a subspace of a high-dimensional vector space. Diffusion imaging data~\cite{fletcher2007riemannian} or 
appearance variations of objects (\eg human faces) under variable lighting can be effectively modeled by low dimensional linear spaces~\cite{beveridge2008principal}, while a video as a whole can be modeled as the subspace that spans the observed frames~\cite{marrinan2014finding}. 

A large body of the aforementioned approaches leverage the mathematical framework of Grassmanian manifolds thanks to the ease in dealing with the confounding variability in observations~\cite{harandi2011graph,he2012incremental,hong2014geodesic,kumar2016robust}.  As such, they rely on statistical analysis tools inherently requiring mean or variance estimations on matrix manifolds~\cite{chakraborty2017intrinsic,chakraborty2015recursive,marrinan2014finding}. Yet, (i) they have been found to be susceptible to outliers, and (ii) while Grassmanians were suitable for analyzing \emph{tall} data where the ambient dimension is much larger than the number of data points, they become less effective when it comes to \emph{wide} data where the data dimension is relatively small~\cite{ma2021flag}. In such cases, the more structured \emph{flag manifolds} have been found to be more effective.

A flag manifold is a nested series of subspaces geometrically generalizing Grassmanians. Any multilevel, multiresolution, or multiscale phenomena is likely to involve flags, whether implicitly or explicitly. This makes flag manifolds instrumental in dimensionality reduction, clustering, learning deep feature embeddings, visual domain adaptation, deep neural network compression and dataset analysis~\cite{minnehan2019deep,ma2021flag,zhang2018grassmannian}. Thus, computing statistics on flag manifolds becomes an essential prerequisite powering several downstream applications. %
In this paper, we propose an approach for computing first order statistics on (\emph{oriented}) flag manifolds.\footnote{While our averages are for general flag-manifolds, we do provide oriented averages for flag manifolds of type $1,2,3,\dots,d-1$ in $d$-D space.} In particular, endowing flag manifolds with the non-canonical chordal metric, we first transform the (weighted) \emph{flag-mean} problem into an equivalent minimization on the Stiefel manifold, the space of orthonormal frames, via the method of Lagrange multipliers. We then leverage Riemannian Trust-Region (RTR) optimizers~\cite{boumal2014manopt,birdal2019probabilistic} to obtain the solution. Subsequently, we introduce an iteratively reweighted least squares (IRLS) scheme to estimate the more robust \emph{flag-median} as an $L_1$ flag-mean. Finally, we show how several common problems in computer vision such as motion averaging, can be translated onto averages on flag manifolds using group contraction operators~\cite{ozyesil2018synchronization}. In particular, our contributions are:
\begin{itemize}[itemsep=0.5pt,leftmargin=*,topsep=1pt]
    \item We introduce a new algorithm for computing flag-prototypes (\eg flag-mean and -median) of a set of points lying on the flag-manifold.
    \item Analogous to our flag-mean, we introduce an IRLS minimization to estimate the flag-median.
    \item We prove the convergence of the proposed IRLS algorithm for the flag-median.
    \item We show how rigid motions can be embedded into flags and thus provide a new way to robustly average motions.
\end{itemize}
 Our diverse experiments reveal that flag averages are more robust, usually yield more reliable estimates, and are more general, \ie, generalize Grassmannian averages. We will release our implementations upon publication. 
 
 
 
 


