
\section{Proof of Proposition I}
Before providing the full proof, let us recall Prop. I:
\begin{prop}\label{prop: flag mean app}
The chordal flag-mean of $\left \{ [[\X_i ]] \right \}_{i=1}^p \subset \flag(d+1)$ is
\begin{equation}\label{eq:suppflagmeanopt} 
    \left[[\bm{\mu}]\right] := \argmin_{[[\Y]]\in \flag(d+1)} \sum_{i=1}^p \alpha_i d_c([[\X^{(i)}]], [[\Y]])^2
\end{equation}
and can be phrased into a Stiefel manifold optimization problem as
\begin{equation}\label{eq:suppstiefelopt}
    \left[\bm{\mu}\right] = \argmin_{[\Y] \in St(d_k,d)} \sum_{j=1}^k m_j -  \tr \left( \I_j \Y^T \mathbf{P}_j  \Y \right)
\end{equation}
where the matrices $\I_j$ and $\mathbf{P_j}$ are given in \cref{eq: Ij2} and \cref{eq: Pj2} respectively.
\begin{equation}\label{eq: Ij2}
    (\I_j)_{i,l} = 
    \begin{cases}
        1, & i = l \in \{ d_{j-1} + 1, 
 d_{j-1} + 2, \dots, d_j\} \\
        0, &\text{ otherwise} \\
    \end{cases}
\end{equation}
and define:
\begin{equation}\label{eq: Pj2}
    \mathbf{P}_j =  \sum_{i=1}^p \alpha_j \X_j^{(i)} {\X_j^{(i)}}^T
\end{equation}
\end{prop}
For example, if we are averaging on $\flag(1,3;4)$ we have
\begin{equation*}
\I_1 = 
\begin{bmatrix}
    1 & 0 & 0\\
    0 & 0 & 0\\
    0 & 0 & 0
\end{bmatrix} \mathrm{ \quad and \quad}
\I_2 = 
\begin{bmatrix}
    0 & 0 & 0\\
    0 & 1 & 0\\
    0 & 0 & 1
\end{bmatrix}.
\end{equation*}
\begin{proof}
We begin by realizing \cref{eq:suppflagmeanopt} as an optimization problem using the definition of chordal distance:
\begin{equation*}
    \argmin_{[[\Y]]\in \flag(d+1)} \sum_{i=1}^p  \alpha_i \left( \sum_{j=1}^k m_j - \tr\left( {\X_j^{(i)}}^T \Y_j \Y_j^T \X_j^{(i)} \right) \right).
\end{equation*}
Then we move our summations around to simplify our objective function:
\begin{align*}
    & \sum_{i=1}^p \alpha_i \left( \sum_{j=1}^k m_j - \tr\left( \Y_j^T\X_j^{(i)} {\X_j^{(i)}}^T\Y_j \right) \right)\\
    &=  \sum_{j=1}^k  \left( \sum_{i=1}^p \alpha_i\right) m_j -  \sum_{i=1}^p \alpha_i \tr\left( \Y_j^T\X_j^{(i)} {\X_j^{(i)}}^T\Y_j \right), \\
    &=  \sum_{j=1}^k  \left( \sum_{i=1}^p \alpha_i\right) m_j -  \sum_{j=1}^k \sum_{i=1}^p \alpha_i \tr\left( \Y_j^T\X_j^{(i)} {\X_j^{(i)}}^T\Y_j \right).
\end{align*}

Since $\sum_{i=1}^p \alpha_i$ is constant with respect to $[[\Y]]$,~\cref{eq:suppflagmeanopt} is equivalent to 
\begin{equation*}
 \argmin_{[[\Y]]\in \flag(d+1)} \sum_{j=1}^k  m_j -  \sum_{j=1}^k \sum_{i=1}^p \alpha_i \tr\left( \Y_j^T\X_j^{(i)} {\X_j^{(i)}}^T\Y_j \right).
\end{equation*}
Using our definitions for $\I_j$ and $\mathbf{P}_j$, we can write the objective function in terms of $\Y$:
\begin{align*}
    &\sum_{j=1}^k  m_j - \tr \left( \Y_j^T\left( \sum_{i=1}^p\alpha_i\X_j^{(i)} {\X_j^{(i)}}^T \right) \Y_j \right), \\
    &= \sum_{j=1}^k m_j - \tr \left( \Y_j^T\mathbf{P}_j \Y_j \right),\\
    &= \sum_{j=1}^k m_j - \tr \left( \Y_j \Y_j^T \mathbf{P}_j  \right),\\
    &= \sum_{j=1}^k m_j - \tr \left( \Y \I_j \Y^T \mathbf{P}_j  \right).%
\end{align*}
The third equality is true because $\Y_j \Y_j^T = \Y \I_j \Y^T$.


There are two constraints for $[[\Y]] \in \flag(d+1)$ according to our representation for points on the flag manifold. The first constraint is $\Y_j^T \Y_j = \I$ for $j=1,2,\dots,p$. The second constraint is $[\Y_j] \cap [\Y_i] = \emptyset$ for all $i \neq j$. These constraints are satisfied when $\Y^T\Y = \I$, e.g. $[\Y] \in St(d_k,d)$.

Using trace invariance to cyclic permutations, the chordal flag mean optimization problem \cref{eq:suppflagmeanopt} is equivalent to the Stiefel optimization problem \cref{eq:suppstiefelopt}.
\end{proof}


\section{Proof of Proposition III}
\begin{prop}
The chordal flag-median of $\left \{ [[\X_i ]] \right \}_{i=1}^p \subset \flag(d+1)$,
\begin{equation} \label{eq:suppflagmedian}
    \left[[\bm{\eta}]\right] = \argmin_{[[\Y]] \in \flag(d+1)} \sum_{i=1}^p \alpha_i d_c([[\X^{(i)}]], [[\Y]]),
\end{equation}
can be phrased with weights %
\begin{equation*}
    w_i([[\Y]]) = \sum_{j=1}^k \frac{\alpha_i}{\max\{d_c([[\X^{(i)}]], [[\Y]]), \epsilon\}}
\end{equation*}
as the optimization problem
\begin{equation*}
 \argmin_{[[\Y]] \in \flag(d+1)}\sum_{i=1}^p \sum_{j=1}^k m_j - w_i([[\Y]]) \tr\left( \Y_j^T \X_j^{(i)}{\X_j^{(i)}}^T \Y_j \right)
\end{equation*}
with $\epsilon = 0$ as long as $d_c([[\X^{(i)}]], [[\Y]]) \neq 0$ for all $i$.
\end{prop}

\begin{proof}
We can write~\cref{eq:suppflagmedian} using the definition of chordal distance as
\begin{equation*}
     \argmin_{[[\Y]] \in \flag(d+1)} \sum_{i=1}^p \alpha_i \sqrt{\sum_{j=1}^k m_j - \tr\left( {\X_j^{(i)}}^T \Y_j \Y_j^T \X_j^{(i)} \right) }.
\end{equation*}

The orthogonality constraints for $\Y \in \R^{d \times d_k}$ to represent a point on $\flag(d+1)$ are: (i) $[\Y_j] \cap [\Y_i]  = \emptyset$ for all $i \neq j$ and $\Y_j^T \Y_j = \I$ for all $j$. Let $\theta([\Y_i],[\Y_j])$ denote the vector of principal angles between $[\Y_i]$ and $[\Y_j]$. Using $\tr(\Y_i^T\Y_j\Y_j^T \Y_i) = \|\cos \theta([\Y_i],[\Y_j])\|_2^2$, we encode our orthogonality constraints as
\begin{equation*}
\tr(\Y_i^T\Y_j\Y_j^T \Y_i) = \begin{cases}
                               0 & i \neq j\\
                               d_j & i = j
                              \end{cases}.
\end{equation*}
We will now use 
\begin{equation*}
    \delta_{i,j} = 
    \begin{cases}
        1, & i = j\\
        0, & i \neq j.
    \end{cases}
\end{equation*}
to put these constraints into the Lagrangian. 

Let $\bm{\Lambda}$ be a symmetric matrix of Lagrange multipliers corresponding to the orthogonality constraints. Denote the entry in the $i$th row and $j$th column of $\bm{\Lambda}$ as $\lambda_{i,j}$. With the constraints added to the objective, we define the Lagrangian in~\cref{eq: lagrangian}.
\begin{align}
\begin{aligned}\label{eq: lagrangian}
    \mathcal{L}(\Y, \Lambda) &=  \sum_{i=1}^p \alpha_i \sqrt{\sum_{j=1}^k m_j - \tr\left( {\X_j^{(i)}}^T \Y_j \Y_j^T \X_j^{(i)} \right)} \\
    &- \sum_{i=j}^k \sum_{j=1}^k \lambda_{i,j} (m_j \delta_{i,j} - \tr(\Y_i^T\Y_j\Y_j^T \Y_i)). \\
\end{aligned}
\end{align}


The gradient of~\cref{eq: lagrangian} w.r.t. ${\Y_j}$ and ${\lambda_{i,j}}$ is
\begin{align*}
\begin{aligned}\label{eq: lagrangian grad}
    \nabla_{\Y_j} \mathcal{L}
    &=  - \sum_{i=1}^p \frac{\alpha_i \X_j^{(i)}{\X_j^{(i)}}^T \Y_j}{\sqrt{\sum_{j=1}^k m_j - \tr \left( {\X_j^{(i)}}^T \Y_j \Y_j^T \X_j^{(i)} \right) } } \\
    &+2\sum_{\substack{i=1 \\ i \neq j}}^k \lambda_{i,j}\Y_i\Y_i^T \Y_j + 4\lambda_{j,j}\Y_j\Y_j^T \Y_j,\\
     \nabla_{\lambda_{i,j}} \mathcal{L} &=  m_j \delta_{i,j} - \tr\left(\Y_i^T\Y_j\Y_j^T \Y_i\right).
\end{aligned}
\end{align*}
Notice we are not dividing by zero because $d_c([[\X^{(i)}]], [[\Y]]) \neq 0$ for all $i$.
Now we use $\nabla_{\Y_j} \mathcal{L} = \bm{0}$ and $\nabla_{\lambda_{i,j}} \mathcal{L} = 0$ to solve for $\lambda_{j,j}$. 

First we will work with $\nabla_{\Y_j} \mathcal{L} = \bm{0}$.
\begin{align*}
    \begin{aligned}
        \mathbf{0} &=  - \sum_{i=1}^p \frac{\alpha_i \X_j^{(i)}{\X_j^{(i)}}^T \Y_j}{d_c([[\X^{(i)}]], [[\Y]])} \\
    &+2\sum_{\substack{i=1 \\ i \neq j}}^k \lambda_{i,j}\Y_i\Y_i^T \Y_j + 4\lambda_{j,j}\Y_j\Y_j^T \Y_j,\\
    &=  - \sum_{i=1}^p \frac{\alpha_i \Y_j^T \X_j^{(i)}{\X_j^{(i)}}^T \Y_j}{d_c([[\X^{(i)}]], [[\Y]])} \\
    &+2\sum_{\substack{i=1 \\ i \neq j}}^k \lambda_{i,j}\Y_j^T\Y_i\Y_i^T \Y_j + 4\lambda_{j,j}\Y_j^T\Y_j\Y_j^T \Y_j,\\
    0 &=  - \sum_{i=1}^p \frac{\alpha_i \tr \left( \Y_j^T \X_j^{(i)}{\X_j^{(i)}}^T \Y_j \right)}{d_c([[\X^{(i)}]], [[\Y]])} \\
    &+2\sum_{\substack{i=1 \\ i \neq j}}^k \lambda_{i,j}\tr \left( \Y_j^T\Y_i\Y_i^T \Y_j\right) + 4\lambda_{j,j}\tr \left(\Y_j^T\Y_j\Y_j^T \Y_j\right).\\
    \end{aligned}
\end{align*}
Using $\nabla_{\lambda_{i,j}} \mathcal{L} = 0$ simplifies our equation to
\begin{align*}
4\lambda_{j,j} \tr (\Y_j^T \Y_j\Y_j^T \Y_j) &=  \sum_{i=1}^p \frac{\alpha_i \tr \left( \Y_j^T \X_j^{(i)}{\X_j^{(i)}}^T \Y_j \right) }{d_c([[\X^{(i)}]], [[\Y]]) }, \\
4m_j \lambda_{j,j} &=  \sum_{i=1}^p \frac{\alpha_i \tr \left( \Y_j^T \X_j^{(i)}{\X_j^{(i)}}^T \Y_j \right) }{d_c([[\X^{(i)}]], [[\Y]]) }. \\
\end{align*}


For $[[\Y]]$ to minimize \cref{eq:suppflagmedian}, we would want to maximize $m_j \lambda_{j,j}$ for each $j$. That is to say, we wish to maximize $\sum_{j=1}^k m_j \lambda_{j,j}$:
\begin{equation}\label{eq:flagmedianmax}
\sum_{i=1}^p \sum_{j=1}^k \frac{\alpha_i}{d_c([[\X^{(i)}]], [[\Y]])} \tr\left( \Y_j^T \X_j^{(i)}{\X_j^{(i)}}^T \Y_j \right).
\end{equation}
Maximizing \cref{eq:flagmedianmax} is the same as minimizing 
\begin{equation*}%
\sum_{i=1}^p \sum_{j=1}^k m_j - \frac{\alpha_i}{ d_c([[\X^{(i)}]], [[\Y]])} \tr\left( \Y_j^T \X_j^{(i)}{\X_j^{(i)}}^T \Y_j \right).
\end{equation*}
Using the definition of $w_i([[\Y]])$, this minimization is
\begin{equation*}
 \argmin_{[[\Y]] \in \flag(d+1)}\sum_{i=1}^p \sum_{j=1}^k m_j - w_i([[\Y]]) \tr\left( \Y_j^T \X_j^{(i)}{\X_j^{(i)}}^T \Y_j \right)
\end{equation*}

\end{proof}

\begin{prop}
 Fix $[[\Z]] \in \flag(d+1)$. Then the minimizer of 
 \begin{equation}\label{eq: median equiv app}
  \sum_{i=1}^p \sum_{j=1}^k\left( m_j - w_i(\Z) \tr\left( \Y_j^T \X_j^{(i)}{\X_j^{(i)}}^T \Y_j \right)\right)
 \end{equation}
 over $[[\Y]] \in \flag(d+1)$ is the weighted chordal flag mean of $\{ [[ \X^{(i)}]]\}_{i=1}^p \in \flag(d+1)$ with weights $w_i(\Z)$.
 Note: $\epsilon = 0$ as long as $d_c([[\X^{(i)}]], [[\Z]]) \neq 0$ for all $i$.
\end{prop}

\begin{proof}
By re-arranging the summations in~\cref{eq: median equiv app}, we see its minimizer is also
 \begin{equation*}
 \argmin_{[[\Y]] \in \flag(d+1)}\sum_{j=1}^k  m_j - \sum_{j=1}^k \sum_{i=1}^p w_i(\Z) \tr\left( \Y_j^T \X_j^{(i)}{\X_j^{(i)}}^T \Y_j \right).
\end{equation*}
We showed that this is the same as the chordal flag-mean optimization problem with weights $w_i(\Z)$ in the proof of~\cref{prop: flag mean app}.
\end{proof}

\section{Proof of Proposition VI}
\begin{prop}
Let $[[\Y]] \in \flag(d+1)$ and $\epsilon > 0$. Assume that $d([[\Y]],[[\X^{(i)}]]) > \epsilon$ for $i = 1,2, \dots, p$. Denote the flag median objective function value as $f:\flag(d+1) \rightarrow \R$ and an iteration of our chordal flag-median IRLS algorithm as $T:\flag(d+1) \rightarrow \flag(d+1)$. Then
\[
f(T([[\Y]])) \leq f([[\Y]]).
\]
\end{prop}

\begin{proof}
Assuming that $d([[\Y]],[[\X^{(i)}]]) > \epsilon$ for $i = 1,2, \dots, p$, we define the function $h: \flag(d+1) \times \flag(d+1) \rightarrow \R$ as
\begin{align*}\label{eq: h flagirls def}
\begin{aligned}
h([[\Z]], [[\Y]]) &= \sum_{i=1}^p w_i([[\Y]]) d_c([[\Z]],[[\X^{(i)}]])^2,\\  
w_i([[\Y]]) &=  \frac{1}{\max \left\{ d_c([[\Y]],[[\X^{(i)}]]), \epsilon \right \} } \\
&= \frac{1}{ d_c([[\Y]],[[\X^{(i)}]]) }.
\end{aligned}
\end{align*}

Some algebra reduces $h([[\Z]], [[\Y]])$ to
\begin{align*}
    h([[\Z]], [[\Y]]) &= \sum_{i=1}^p w_i([[\Y]]) d_c([[\Z]],[[\X^{(i)}]])^2,\\
    &= \sum_{i=1}^p \frac{d_c([[\Z]],[[\X^{(i)}]])^2}{ d_c([[\Y]],[[\X^{(i)}]]) }.\\
\end{align*}

$h([[\Z]], [[\Y]])$ is the weighted flag-mean objective function (of $\{[[\X^{(i)}]]\}_i$) with weights $w_i([[\Y]])$. So minimizing $h([[\Z]], [[\Y]])$ over $[[\Z]]$ is an iteration of our IRLS algorithm to compute the flag-median. In other words,

\begin{equation}\label{eq: h flagirls min}
    T([[\Y]]) = \argmin_{[[\Z]] \in \flag(d+1)} h([[\Z]], [[\Y]]).
\end{equation}
Using~\cref{eq: h flagirls min}, we have
\begin{equation*}
    h(T([[\Y]]), [[\Y]]) \leq h([[\Y]], [[\Y]]).
\end{equation*}

By the definition of $h$
\begin{align*}
    h([[\Y]], [[\Y]]) &= \sum_{i=1}^p \frac{d_c([[\Y]],[[\X^{(i)}]])^2}{d_c([[\Y]],[[\X^{(i)}]])}, \\
    &= \sum_{i=1}^p d_c([[\Y]],[[\X^{(i)}]]),\\
    &= f([[\Y]]).
\end{align*}
This means, we have
\begin{equation}\label{eq: h flagirls less}
    h(T([[\Y]]),  [[\Y]]) \leq f([[\Y]]).
\end{equation}






Now we use the identity from algebra: $\frac{a^2}{b} \geq 2a-b$ for any $a,b \in \R$ and $b > 0$. Let 
\begin{equation*}
    a = d_c([[\Z]],[[\X^{(i)}]]) \text{ and } b =  d_c([[\Y]],[[\X^{(i)}]]) .
\end{equation*} 
Then
\begin{align*}
h([[\Z]], [[\Y]]) &\geq 2\sum_{i=1}^p d_c([[\Z]],[[\X^{(i)}]])\\
&- \sum_{i=1}^p  d_c([[\Y]],[[\X^{(i)}]]),  \\
&= 2f([[\Z]]) - f([[\Y]]).
\end{align*}
Now, take $[[\Z]] = T([[\Y]])$. This gives us
\begin{equation}\label{eg: h flagirls greater}
    h(T([[\Y]]), [[\Y]]) \geq 2f(T([[\Y]]))-  f([[\Y]]).
\end{equation}

Then, combining~\cref{eg: h flagirls greater} with~\cref{eq: h flagirls less}, we have

\begin{align*}
    2f(T([[\Y]]))- f([[\Y]]) &\leq f([[\Y]]), \\
    f(T([[\Y]])) &\leq f([[\Y]]) . 
\end{align*}
\end{proof}







\section{Further Experimental Evaluation}
\subsection{Further Qualitative Results on Faces Dataset}
We now show in~\cref{fig:sra12} further visualizations of Flag and Grassmann averages of faces.  
\begin{figure}[t]
        \includegraphics[width=\columnwidth]{figures/faces2.pdf}
	\caption{Averaging a collection of faces belonging to three different identities, captured under varying illumination: center, left and right. Notice that the first dimension of the flag representations is center illuminated, better representing the mean compared to Grassmannian.\vspace{-3mm}}
	\label{fig:sra12}
\end{figure}

\subsection{Further Qualitative Results on MNIST}
We use $20$ examples (e.g., points on $\flag(1,2;748)$) of $6$s and add $10$ examples of $7$s. We use the same workflow from the manuscript to represent the MNIST digits on $\Gr(2,748)$ and $\flag(1,2;748)$. We compute the averages on the Grassmannian~\cite{draper2014flag, mankovich2022flag} and flag (ours). The reshaped first dimension of each of these averages is in~\cref{fig:mnist_qual}. The brightness of the bottom left corner of each image is brighter the more present the $7$s digit (outlier class) is in the image. Notice the bottom left corner of each image, boxed in red, becomes darker as we move from left to right. So, our averaging on the flag is more robust to outliers than Grassmannian averaging. In fact, the bottom left corner of the flag-median is the darkest. Therefore, our flag-median is the least affected by the outlier examples of $7$s.
\begin{figure}[t]
    \includegraphics[width=\columnwidth]{figures/mnist_qual_20x6_10x7.pdf}
	\caption{The first dimension of Grassmannian (``GR-'') and flag (``FL-'') averages of a data set with $20$ representations of $6$s and $10$ representations of $7$s. The bottom red boxes are the enlarged version of the upper image. Our flag-median is the least affected by the outlier examples of $7$s.}
	\label{fig:mnist_qual}
\end{figure}

\subsection{LBG Clustering on UFC YouTube}
 We use a subset of the UCF YouTube Action dataset~\cite{liu2009recognizing} to run a similar experiment to what was done by Mankovich~\etal \cite{mankovich2022flag}. The dataset contains labeled RGB video clips of people performing actions. Within each labeled action, the videos are grouped into subsets of clips with common features. We take approximately one example from each subset from each class. This results in $23$ examples of basketball shooting, $23$ of biking/cycling, $25$ of diving, $25$ of golf swinging, $24$ of horse back riding, $25$ of soccer juggling, $23$ of swinging, $24$ of tennis swinging, $24$ of trampoline jumping, 24 of volleyball spiking, and $24$ of walking with a dog. We convert these frames to gray scale, then we use INTER\_AREA interpolation from the OpenCV package~\cite{opencv_library} to resize the frames to have only $450$ pixels. This is, on average, only $1\%$ of the number of pixels in the original frame. We vectorize and horizontally stack each video, then use the first $10$ columns of $\mathbf{Q}$ from the QR decomposition to realize each video as a point on $\Gr(10,450)$ and $\flag(1,2,\dots,10;450)$.

 We run Linde-Buzo-Gray (LBG) clustering on these videos and report the resulting cluster purities in~\cref{fig:lbgres}. Clustering on the flag manifold with our flag averages (blue boxes) improves cluster purities over Grassmannian methods. We also see higher variance in cluster purities for flag methods. Even though we are only working with approximately $1\%$ of the total number of pixels in each frame, we are able to produce cluster purities that are competitive with those reported in~\cite{mankovich2022flag} using a similar set of videos. Specifically, our flag-LBG clustering is well within $0.1$ of the highest cluster purities reported in~\cite{mankovich2022flag}. Overall, our flag methods improve cluster purities in a head-to-head experiment while remaining competitive with Grassmannian LBG with only using approximately 1\% of pixels per frame.
\begin{figure}[t]
        \includegraphics[width=\columnwidth]{figures/youtube_lbg_10trials_small1.pdf}
	\caption{LBG cluster purities of YouTube videos with $10$ experiments with different numbers of centers, codebook sizes. The Grassmannian, \eg ``GR-'', boxes are results from LBG clustering using chordal distance and Grassmannian averages from~\cite{draper2014flag,mankovich2022flag}. The ``FL-" boxes are results from using the flag chordal distance and our flag-mean and -median.\vspace{-3mm}}
	\label{fig:lbgres}
\end{figure}

\subsection{Ablation Studies}
For~\cref{fig:init_ablation}, we fix a single-cluster dataset of $100$ points on $\flag(1,2,3;10)$ then run our IRLS algorithm for the flag-median and Stiefel RTR~\cite{absil2007trust,boumal2014manopt} for the flag-mean with initial points that are further and further away from the center of the dataset. Our dataset is computed the same way we compute synthetic datasets for the manuscript: compute a center, $[[\mathbf{C}]] \in \flag(1,2,3;10)$, and then add noise to the center using the parameter $\delta$. For this experiment we use $\delta = .2$. Our initial point for our IRLS algorithm and RTR is computed as the first $3$ columns of the QR decomposition of $\mathbf{C}  + \Z \delta_{init}$ where $\Z \in \R^{10 \times 3}$ has entries sampled from $\mathcal{U}[-.5,.5)$. We call $\delta_{init}$ the noise added to the initial point and plot it on the $x$-axis of~\cref{fig:init_ablation}. The ``Error" is the chordal distance on $\flag(1,2,3;10)$ between the center and the algorithm output. ``Iterations" is the number of iterations of RTR for the flag-mean and IRLS for the flag-median until convergence. ``Cost" is the objective function values of the algorithm output. Our IRLS algorithm estimates the flag-median is further away from the center, $[[\mathbf{C}]]$ than the flag-mean estimate. Also, the number of iterations of Stiefel RTR increases as we move the center further away from our dataset whereas our IRLS algorithm number of iterations remains constant. Finally, the cost value for the flag-median estimate is higher than the flag-mean estimate because the flag-mean estimate objective function likely contains squares of values less than $1$.


\begin{figure}[t]
    \includegraphics[width=\linewidth]{figures/init_ablation.pdf}
    \caption{A plot of the robustness of our IRLS algorithm for the flag-median and Stiefel RTR for the flag-mean to initialization. For the median, we report the IRLS-iterations whereas for the mean, we report the RTR-iterations. Note that, even in large noise variances, both of the algorithms converge to a reasonable point regardless of initialization.}
    \label{fig:init_ablation}
\end{figure}



\subsection{Motion Averaging}

\paragraph{On error metrics}
We score the quality of our averages using the geodesic distance on the pose manifold $SE(3)$ (or equivalently the geodesic distance on dual quaternions):
\begin{equation}\label{eq:se3error}
    \epsilon(\mathbf{T}_1,\mathbf{T}_2) = {\frac{1}{\pi}\|\log(\mathbf{R_1}^\top\mathbf{R}_2)\|_2+\lambda_T\|\mathbf{t}_1-\mathbf{t}_1\|_2}
\end{equation}
where $(\mathbf{R}_i,\mathbf{t}_i)$ are extracted from $\mathbf{T}_i$ as rotational and translational components, respectively. $\lambda_T$ is a scene dependent strictly positive scaling factor. Note that, as discussed in the paper, this is very much related to the $\lambda$ used in motion contraction. $\log(\cdot):SO(3)\to \mathfrak{so}(3)$ denotes the \emph{logarithmic map} of the $SO(3)$-manifold. As such, this residual defined in~\cref{eq:se3error} is equivalent to:
\begin{equation}\label{eq:tracedist}
    \epsilon(\mathbf{T}_1,\mathbf{T}_2) = {\frac{1}{\pi}\arccos\left(\mathrm{tr} \frac{\mathbf{R}_1^\top\mathbf{R}_2 -1}{2} \right)+\lambda_s\|\mathbf{t}_1-\mathbf{t}_2\|_2}.\nonumber
\end{equation}

\paragraph{Single rotation averaging}
Single rotation averaging where a set of rotation matrices are averaged, is a special case of motion averaging where the translational components are set to zero. Due to the compactness of the manifold, additional $SO(3)$-specific averaging algorithms can be employed for the case of pure rotations. To compare our method against a larger class of well established, rotation-specific averaging algorithms we opt for zeroing the translational components, performing averages and reporting only the angular errors.~\cref{fig:rotavg1,fig:rotAvgRobust} present our results with increasing noise and increasing outliers respectively. For the case of outliers, we further include the recent robust methods of Rie \& Civera~\cite{lee2020robust}. \emph{Naive} refers to the Euclidean averages of rotation matrices (with a subsequent projection). 

\paragraph{Impact of $\lambda$}
As we have discussed in the paper, the scene-dependent scaling $\lambda$ is a hyper-parameter in our $SE(3)$-averaging. Note that, other distance metrics such as the ones dependent on 3D point distances exist~\cite{bregier2018defining}. These metrics exploit the action of 3D transformations on an auxiliary point set to measure distances in the 3D configuration of points. However, even those are somehow dependent upon a hyper-parameter such as the diameter of the point set or the point configurations. This is why we evaluate the impact of $\lambda$ in our averaging. In particular, we design multiple experiments to average $250$ random points on $SE(3)$ generated with an angular noise level of $0.075$. The radius of this scene is set to $1$, up to a translational noise level of $0.15$. This also means that the optimal $\lambda^\star$ (unknown during test) is $1$. We then vary $\lambda\in [0.002, 0.025, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.8, 2, 2.25,\\2.5]$ and average $250$ random points, over $50$ runs. In each run, the point sets differ randomly. We compute the errors using~\cref{eq:tracedist} with $\lambda=1$ and accumulate them over all runs.~\cref{fig:lambdaexp} plots the average errors per each $\lambda$. In this outlier-free regime, our flag-mean and flag-median are almost aligned when $\lambda=1$. Flag-median shows slight advantage over the mean for smaller values of $\lambda$.
\begin{figure}[t]
        \includegraphics[width=\columnwidth]{figures/perturb_axis_change.pdf}
	\caption{Single rotation averaging results for increasing levels of axial noise on synthetic, outlier-free data.}
	\label{fig:rotavg1}
\end{figure}
\begin{figure}[t]
        \includegraphics[width=\columnwidth]{figures/robust_averaging.pdf}
	\caption{Single rotation averaging results for increasing levels of outliers on synthetic data with $<5^\circ$ of noise.}
	\label{fig:rotAvgRobust}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=\linewidth]{figures/lambda_exp.pdf}
    \caption{Impact of $\lambda$ our flag-mean and flag-median. During data generation we use $\lambda^\star=1$, whereas our algorithms use varying $\lambda$ as plotted in the $x$-axis. We then compare the resulting averages to the ground truth average and report the deviation. This is an outlier-free regime and as expected, median \& mean prototypes overlap when we are at the optimal value, $\lambda=1$. Though, we also see that our algorithms are not too sensitive to the exact choice of this parameter.\vspace{-3mm}}
    \label{fig:lambdaexp}
\end{figure}



\section{On Motion \& Rotation Averaging}
Motion averaging lies at the heart of structure from motion and 3D reconstruction in multi-view settings. Typically, the problem of recovering individual motions for a set of cameras when we are given a number of relative motion estimates between camera pairs is known as \emph{multiple motion averaging} or \emph{transformation / motion synchronization}~\cite{huang2019learning,arrigoni2016spectral,birdal2018bayesian,dellaert2020shonan,eriksson2019rotation,chatterjee2017robust,govindu2001combining,govindu2004lie}. This problem is foundational for 3D structure recovery. Synchronization algorithms usually solve multiple \emph{single averaging} sub-problems robustly~\cite{birdal2020synchronizing,li2020hybrid,hartley2011l1}, hence the name multiple motion averaging. These sub-problems involving the computation of a robust-barycenter of a set of points on $SE(3)$, are commonly known as \emph{robust single motion averaging} and is the focus of our paper. Although our method directly operates on the product manifold, it is a de-facto standard to decompose the problem into \emph{single translation averaging} and \emph{single rotation averaging}. The lattter is particularly well studied due to the interesting mathematical structure of the problem~\cite{hartley2013rotation,hartley2011l1,lee2020robust,govindu2004lie}. Nevertheless, our method is general enough to solve all of these variants, as we have experimented with in~\cref{fig:rotavg1,fig:rotAvgRobust}.


