{
    "arxiv_id": "2303.15671",
    "paper_title": "Colo-SCRL: Self-Supervised Contrastive Representation Learning for Colonoscopic Video Retrieval",
    "authors": [
        "Qingzhong Chen",
        "Shilun Cai",
        "Crystal Cai",
        "Zefang Yu",
        "Dahong Qian",
        "Suncheng Xiang"
    ],
    "submission_date": "2023-03-28",
    "revised_dates": [
        "2023-03-29"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV"
    ],
    "abstract": "Colonoscopic video retrieval, which is a critical part of polyp treatment, has great clinical significance for the prevention and treatment of colorectal cancer. However, retrieval models trained on action recognition datasets usually produce unsatisfactory retrieval results on colonoscopic datasets due to the large domain gap between them. To seek a solution to this problem, we construct a large-scale colonoscopic dataset named Colo-Pair for medical practice. Based on this dataset, a simple yet effective training method called Colo-SCRL is proposed for more robust representation learning. It aims to refine general knowledge from colonoscopies through masked autoencoder-based reconstruction and momentum contrast to improve retrieval performance. To the best of our knowledge, this is the first attempt to employ the contrastive learning paradigm for medical video retrieval. Empirical results show that our method significantly outperforms current state-of-the-art methods in the colonoscopic video retrieval task.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.15671v1"
    ],
    "publication_venue": "Accepted by ICME 2023"
}