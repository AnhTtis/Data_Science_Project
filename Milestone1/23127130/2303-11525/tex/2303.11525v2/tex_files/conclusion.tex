\section{Conclusion}
We introduce a new family of Sparse Iso-FLOP Transformations (Sparse-IFT) to
improve the training efficiency of DNNs. These transformations can be used as
drop-in replacements for dense layers and increase the representational capacity
while using sparsity to maintain training FLOPs. This increase in capacity also
translates to a larger search space allowing sparse training methods to explore
better and identify optimal sparse subnetworks. For the same computational cost
as the original dense model, Sparse-IFT improves the training efficiency across
multiple model families in the CV and NLP domains for various tasks. We hope our
work will open new investigations into improving the accuracy of DNNs via
sparsity, especially as new hardware accelerators build better support for
weight sparsity during training.
