\section{Additional Methodology Details}

\subsection{Sparse-IFT for Convolutional Layers \label{app:sift_conv}} In this
section, we detail the straightforward extension of the Sparse-IFT family for
convolutional layers.

\paragraph{Sparse Wide}
Similar to the setup for fully connected layers, in the case of convolutional
layers, we widen the number of input and output channels.

\paragraph{Sparse Parallel}
Similar to the setup for fully connected layers, in the case of convolutional
layers, we can implement this transformation with the use of convolutional
branches in parallel.

\paragraph{Sparse Factorized and Sparse Doped}
Let $\theta_l \in \mathbb{R}^{c_{in}\times c_{out} \times k_h \times k_w}$
represent the weight matrix of a convolutional layer, where $c_{in}, c_{out},
k_h, k_w$ denote the input channels, output channels, kernel height, and kernel
width, respectively. We apply low-rank or matrix factorization to the weight
matrix by first converting the 4D tensor into a 2D matrix with shape:
$(c_{in}\cdot k_h \cdot k_w)\times c_{out}$. In this setup, we can express
$\theta_l = UV^T$, where $U \in \mathbb{R}^{c_{in}\cdot k_h \cdot k_w \times
d}$, $V \in \mathbb{R}^{c_{out} \times d}$. In this factorization, $U$ learns a
lower-dimensional set of features and is implemented as a convolutional layer
with $d$ output channels and $k_h \times k_w$ filter. $V$ matrix expands this
low-dimensional set of features and is implemented as a convolutional layer with
$1\times1$ filter.

\subsubsection{Sparse-IFT for Depthwise Convolution Layers}

For a normal convolution layer, all inputs are convolved to all outputs.
However, for depthwise convolutions, each input channel is convolved with its
own set of filters. Let $\theta_l \in \mathbb{R}^{c_{in}\times c_{out} \times
k_h \times k_w}$ represent the weight matrix of a normal convolution layer,
where $c_{in}, c_{out}, k_h, k_w$ denote the input channels, output channels,
kernel height, and kernel width, respectively. An equivalent depthwise
convolution layer will have weights $\theta_{dw,l} \in \mathbb{R}^{ 1\times
c_{out} \times k_h \times k_w}$.

\paragraph{Sparse Wide} A Sparse Wide depthwise convolution will have weights
$\theta_{dw,l}^{sw} \in \mathbb{R}^{ 1\times k_{sw}{\cdot}c_{out} \times k_h
\times k_w}$. Since the fraction of non-sparse weights is given by $1-s$, the
FLOPs required by this transformation are
$B{\cdot}(k_{sw}{\cdot}c_{out}){\cdot}k_h{\cdot}k_w{\cdot}(1-s)$. Setting these
equal to the FLOPs of the original dense $\theta_{dw,l}$, we obtain the widening
factor $k_{sw} = \frac{1}{(1-s)}$. In this case, we do not scale the input
channels as it converts the depthwise convolution to a grouped convolution
without an equivalent scaling in the number of groups.

\paragraph{Other Sparse-IFTs} The Sparse Wide transformation generally changes a
layer's input and output channels, subsequently scaling the following layers in
a CNN. However, the other Sparse-IFTs (e.g., Sparse Parallel, Sparse Factorized,
and Sparse Doped) do not modify a convolution layer's input or output channels
(as seen in Figure~\ref{fig:diff_members_of_sift}). This allows for fine-grained
control of what layers to apply the Sparse-IFT transformations. Since depthwise
convolutions are an extreme form of structured sparsity, where some filters
interact with only specific input channels, we opt not to sparsify them when
using the other Sparse-IFTs and leave the layer unchanged while still
maintaining FLOPs equivalent to the dense baseline. Note that the different
convolution layers surrounding the depthwise convolution are still transformed
with Sparse-IFT to increase their representational capacity.

\section{Computer Vision: Experimental Settings \label{app:cv-exp-details}}

\subsection{Importance of Non-linearity}\label{app:nonlinear-importance} We use
BatchNorm~\cite{ioffe2015batch} followed by ReLU~\cite{nair2010rectified} as a
non-linearity. We provide an extended set of empirical results in
Table~\ref{tab:non_linearity_importance} to help validate the importance of
training with and without non-linearity by training configurations of the Sparse
Parallel, Factorized, and Doped IFTs at different levels of sparsity. The
results without non-linear activation functions are often worse than the dense
accuracy (77\%) across all Sparse-IFT family transformations. We omit Sparse
Wide in Table~\ref{tab:non_linearity_importance} because here we increase the
number of channels in the convolutional layers while maintaining the existing
architecture.

\begin{table}[h]
    \caption{Evaluation on the importance of utilizing the non-linear activation
across different members of Sparse-IFT with ResNet-18 on CIFAR100 across
different values of sparsity (columns). Non-linear activations enhance the
representational capacity of Sparse-IFT, leading to higher accuracy. All
reported results are the average over 3 random seeds.} \vskip 2pt
    \centering
    \begin{small}
    \begin{sc}
    \begin{tabular}{c|cccc}
        \toprule
        Transformation & Non-linear activation & 0.50  & 0.75 & 0.90  \\
	\midrule
    \multirow{2}{*}{Sparse Factorized} & \xmark    &  75.9 $\pm$ 0.3 & 76.6
    $\pm$ 0.4 & 76.5 $\pm$ 0.4  \\
       &  \cmark

&  \textbf{77.8 $\pm$ 0.4} &  \textbf{78.4 $\pm$ 0.5} & \textbf{78.9  $\pm$ 0.5}
\\
       \midrule \multirow{2}{*}{Sparse Parallel} & \xmark            	      &
       77.1 $\pm$ 0.1 &  77.2 $\pm$ 0.2 & 77.6 $\pm$ 0.1  \\
       &  \cmark         	      &  \textbf{77.9 $\pm$ 0.2 } &  \textbf{79.1
       $\pm$ 0.2 } & \textbf{78.2 $\pm$ 0.2} \\
       \midrule \multirow{2}{*}{Sparse Doped} & \xmark            	      & 77.3
       $\pm$ 0.2 &  77.1 $\pm$ 0.1 & 76.5 $\pm$ 0.2  \\
       &  \cmark         	      &  \textbf{78.2 $\pm$ 0.1} &  \textbf{77.8
       $\pm$ 0.1} & \textbf{76.9 $\pm$ 0.2} \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \label{tab:non_linearity_importance}
\end{table}


\subsection{Computer Vision: Pre-Training Settings \label{app:pt-implement}}
\paragraph{CIFAR-100} Our implementation of CIFAR-100 follows the setup
from~\cite{devries2017improved} for ResNets. We train the models for 200 epochs
with batches of 128 using SGD, Nesterov momentum of 0.9, and weight-decay of
5$\times 10^{-4}$. The learning rate is initially set to 0.1 and is scheduled to
decay to decrease by a factor of 5x after each of the 60th, 120th, and 160th
epochs. Following recent advances in improving ResNets, we initialize the
network with Kaiming He initialization~\cite{he2016identity}, zero-init
residuals~\cite{he2019bag}, and disable weight-decay in biases and
BatchNorm~\cite{ioffe2015batch} layers. For CIFAR-100 experiments with
MobileNetV2, MobileViT-S, and BotNet-50, we follow the same training setup used
for ResNet, but the learning rate is scheduled via cosine annealing.

\paragraph{ImageNet} Our implementation of ImageNet follows the standard setup
from~\cite{krizhevsky2017imagenet, simonyan2014very}. The image is resized with
its shorter side randomly sampled in [256, 480] for scale
augmentation~\cite{simonyan2014very}. A 224 $\times$ 224 crop is randomly
sampled from an image or its horizontal flop, and then normalized. For
evaluation, the image is first resized to 256 $\times$ 256, followed by a 224
$\times$ 224 center crop, and then normalized. Following recent advances in
improving ResNets, we initialize the network with Kaiming He
initialization~\cite{he2016identity} and zero-init residuals~\cite{he2019bag}.

For ResNets, we replicate the settings recommended by Nvidia~\cite{resnetv15},
which uses the SGD optimizer with a momentum of 0.875 and weight decay of
3.0517578125$\times 10^{-5}$. We disable weight-decay for biases and BatchNorm
layers. The model is trained with label smoothing~\cite{szegedy2016rethinking}
of 0.1 and mixed precision~\cite{micikevicius2017mixed} for the standard 90
epochs using a cosine-decay learning rate schedule with an initial learning rate
of 0.256 for a batch size of 256.~\citet{srinivas2021bottleneck} follow the same
setup as ResNet for training BotNet-50 on ImageNet, therefore we maintain the
same hyperparameter settings as~\citet{resnetv15} for our BotNet-50 ImageNet
experiments.

\paragraph{Sparsity Setup} For enabling the Sparse-IFTs, we use the
RigL~\cite{evci2020rigging} algorithm in its default hyperparameter settings
($\alpha = 0.3, \Delta T = 100$), with the drop-fraction ($\alpha$) annealed
using a cosine decay schedule for 75\% of the training run. We keep the first
and last layers (input convolution and output linear layer) dense to prevent a
significant degradation in model quality during pre-training, which is standard
practice. We account for these additional dense FLOPs by increasing the sparsity
in the remaining layers, similar to~\citet{gale2019state}
and~\citet{liu2022unreasonable}.


\subsection{Computer Vision \label{app:ft-implement}}

\subsubsection{Sparse-IFT on Efficient Computer Vision
Architectures}\label{app:efficientcv} Here, we provide an extended set of
results on MobileNetV2, MobileViT-S, and BotNet-50 on CIFAR-100. In particular,
we enable Sparse Wide and Sparse Parallel IFT at 50\% and 75\% sparsity values
(see Table~\ref{tab:mbv2-cifar-app}).

\begin{table}[!ht]
    \caption{Evaluation of Sparse Wide and Sparse Parallel IFT with various
compute efficient architectures on CIFAR-100 across different values of sparsity
(columns). Using Sparse Parallel IFT, all architectures outperform the dense
baseline by a significant margin.} \vskip 2pt
    \centering
    \begin{small}
    \begin{sc}
    \begin{tabular}{c|c|ccc}
        \toprule
        	    & Dense  & Transformation           & 0.50 & 0.75 \\ \midrule
                \multirow{2}{*}{MobileNetV2} & \multirow{2}{*}{72.4 $\pm$ 0.2 }
                & Sparse Wide  & 73.4 & \textbf{73.7} \\

                & & Sparse Parallel & 72.9 & \textbf{73.3} \\ \midrule
                \multirow{2}{*}{MobileViT-S} & \multirow{2}{*}{73.5 $\pm$ 0.1} &
                Sparse Wide & 74.6 & \textbf{74.8} \\
                & & Sparse Parallel & 73.7 & \textbf{74.4} \\ \midrule
\multirow{2}{*}{BotNet-50}  & \multirow{2}{*}{79.8 $\pm$ 0.2} & Sparse Wide &
80.3 & \textbf{80.6} \\
        & & Sparse Parallel & 79.7 & \textbf{80.5} \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
\label{tab:mbv2-cifar-app}
\end{table}

\subsubsection{Evaluation of Sparse-IFT with Structured Sparsity}
\label{app:structured_sparsity}

\paragraph{Block Sparsity} For all of our N:M transposable sparsity experiments,
we use the official code from Habana
Labs~\footnote{\url{https://github.com/papers-submission/structured_transposable_masks}}.
To derive Iso-FLOP configurations with block sparsity, we reuse the analysis
done previously with unstructured sparsity (see
Section~\ref{subsec:members_of_sift}) and express the width scaling as a
function of sparsity. However, we will search for a block sparse mask during
training instead of an unstructured sparsity mask. We use the method proposed
by~\citet{hubara2021accel} to search N:M transposable sparsity, which can
accelerate both the forward and backward pass during training on NVIDIA GPUs
with Tensor Cores. We use 4:8-T, 2:8-T, and 1:8-T block patterns to obtain 50\%,
75\%, and 87.5\% sparsity, respectively. Note the 1:8-T block is the closest
approximation to a 90\% sparsity pattern attainable with a block size of 8. We
also set up and experimented using the method proposed
by~\citet{jiang2022exposing} to train with fine-grained sparse block structures
dynamically. However, the algorithm uses agglomerative clustering which led to a
much slower runtime and quickly ran out of memory even at 50\% sparsity using
the Sparse Wide transformation on a single Nvidia V100 (16 GB).
\paragraph{Low Rank}
Let $k_{lr}$ be the factor with which we widen all layers' input and output
 dimensions for low-rank factorization. We replace all dense layers with
 low-rank factorization, i.e. $\theta_l^{lr} = U_lV_{l}^{T}$, where $U_l \in
 \mathbb{R}^{(k_{lr}.D_{in}) \times d}$ and $V_l \in \mathbb{R}^{
 (k_{lr}.D_{out}) \times d}$. Given a widening factor and equating the FLOPs of
 this transformation to that of a dense transformation $f_{\theta}$, we obtain
 the following expression for rank $d$: $\frac{D_{in}.D_{out}.k_{lr}}{(D_{in} +
 D_{out}}$. We evaluate this factorization across different values of
 width-scaling $k_{lr}$ in Table~\ref{tab:struct-cifar-lowrank}.

\begin{table*}[]
    \caption{Comparison of structured sparse and unstructured sparse methods on
    CIFAR-100 test accuracy on ResNet-18.} \vskip 2pt
    \centering
    \begin{small}
    \begin{sc}
    \begin{tabular}{c|cccccc}
        \toprule
        &               &          & \multicolumn{4}{l}{Width Scaling Factor} \\
        Transformation & Sparsity Type & Sparsity & 1x      & 1.41x     & 2x &
        3.16x     \\ \midrule Low Rank, Linear &  Structured & 0\% & 74.1 & 74.3
        & 74.3 & 73.4 \\
        Low Rank, Non-Linear & Structured  &  0\% &  76.8 & 76.5 & 76.0 & 75.3
        \\
\midrule
        \multirow{6}{*}{Sparse Wide} &
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}N:M Block Sparse\\
\citep{hubara2021accel}\end{tabular}} & 4:8-T    &         &    77.1       & &
 \\
        &                                       & 2:8-T    &         & &
        \textbf{78.4} &           \\
        &                                       & 1:8-T    &         & & & 78.1
        \\
   & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Unstructured Sparse\\
\citep{evci2020rigging}\end{tabular} } & 50\%    &         &    79.1       & &
\\
        &                                       & 75\%   &         &           &
        79.5     &           \\
        &                                       & 90\%    &         & &        &
        \textbf{80.1}  \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
\label{tab:struct-cifar-lowrank}
\end{table*}


\subsubsection{Evaluation on downstream tasks \label{app:eval_downstream}}
\subsubsection*{COCO Object Detection \label{app:coco}} This dataset contains
118K training, 5K validation (\texttt{minival}), and 20K test-dev images. We
adopt the standard single-scale training setting~\cite{lin2017feature} where
there is no additional data augmentation beyond standard horizontal flipping.
For training and testing, the input images are resized so that the shorter edge
is 800 pixels~\cite{lin2017feature}. The model is trained with a batch size of
16, using the SGD optimizer with a momentum of 0.9 and weight decay of 1$\times
10^{-4}$. We follow the standard 1x schedule (12 epochs) using a step learning
rate schedule, with a 10x decrease at epochs 8 and 11, an initial learning rate
warmup of 500 steps starting from a learning rate of 2$\times 10^{-5}$, and a
peak learning rate of 0.01.


\begin{table*}[h]
    \caption{ Object detection results on COCO \texttt{minival} in the RetinaNet
        framework. Sparse Wide IFT configurations of RetinaNet outperform the
        dense baseline by a large margin on all metrics while using similar
        FLOPs. } \vskip 2pt
    \centering
    \begin{small}
    \begin{sc}
    \begin{tabular}{c|cccccccc}
        \toprule
        Backbone   &      AP   & AP$_{50}$ & AP$_{75}$ & AP$_{S}$ & AP$_{M}$ &
        AP$_{L}$ \\ \midrule Dense           &   29.3 & 46.2 & 30.9 & 14.7 &
        31.5 & 39.6 \\
        Sparse Wide (50\%)  & 31.3 & 49.0 & 33.0 & 16.6 & 34.0 & 42.0 \\
        Sparse Wide (75\%) & 32.8 & 51.0 & 34.8 & 17.3 & 35.8 & 43.3 \\
        Sparse Wide (90\%) & \textbf{34.5} & \textbf{53.5} & \textbf{36.5} &
        \textbf{18.6} & \textbf{37.6} & \textbf{45.3} \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \label{tab:app-cocoretinanet}
\end{table*}


\subsubsection*{CityScapes Semantic Segmenation \label{app:cityscapes}}

\paragraph{Setup} We follow the same training protocol
as~\cite{zhao2017pyramid}, where the data is augmented by random cropping (from
1024 $\times$ 2048 to 512 $\times$ 1024), random scaling in the range [0.5, 2],
and random horizontal flipping. The model is trained with a batch size of 16,
using the SGD optimizer with a momentum of 0.9 and weight decay of 5$\times
10^{-4}$. We follow the 80K iterations setup from MMSegmentation with an initial
learning rate of 0.01 annealed using a poly learning rate schedule to a minimum
of 1$\times 10^{-4}$. Similar to most setups that tune
hyperparameters~\cite{zhao2017pyramid, liu2021swin, wang2020deep} for reporting
the best results, we tune the learning rate for all our models. All our results
are reported using a learning rate of 0.03 for the sparse backbones and 0.01 for
the dense baseline.

\begin{table*}[h]
    \caption{ Semantic segmentation results on the Cityscapes \texttt{val} set
        using DeepLabV3+. Sparse Wide IFT configurations ResNet-18 backbones
        outperform the dense baseline on all metrics while using similar FLOPs.
        }
    \centering
    \vskip 2pt
    \begin{small}
    \begin{sc}
    \begin{tabular}{c|cc}
        \toprule
        Backbone  &  mIoU & mAcc \\ \midrule Dense          & 76.72 & 84.40 \\
        Sparse Wide (50\%)      &  77.90 & 85.12 \\
        Sparse Wide  (75\%)      &   78.92 & 85.68 \\
        Sparse Wide  (90\%)      &  \textbf{79.10} & \textbf{86.01} \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \label{tab:app-cityscapes}
\end{table*}

\section{Natural Language Processing: Experimental
Settings~\label{app:llm-setup}}

\subsection{Details for GPT End-to-End Training}
\label{app:gpt_e2e}

Our end-to-end training setup for GPT-3 on WikiText-103 follows a similar
procedure to~\citet{dao2022monarch}. We use a batch size of 512 and train with
the AdamW optimizer for 100 epochs. Also, we use a learning rate warmup for 10
epochs and a weight decay of 0.1. To discover good hyperparameters, we perform a
grid search to discover an appropriate learning rate among \{8e-3, 6e-3, 5.4e-3,
1.8e-3, 6e-4, 2e-4, 6e-5\} that led to the best perplexity for a given compute
budget on the validation set. In Table~\ref{tab:app-gpt-pt-sift}, we outline the
architecture configurations for the original dense model and its Sparse Wide IFT
50\% and 75\% variants.

\begin{table*}[!ht]
    \caption{ Sizes and architecture definitions of the dense GPT-3 Small model
        and its Sparse Wide IFT variants. } \vskip 2pt
    \centering
    \begin{small}
    \begin{sc}
    \begin{tabular}{c|c|c|ccccc}
        \toprule
        Model & Transformation & Sparsity & $n_{layers}$ & $d_{model}$ &
        $d_{\text{ff}}$ & $n_{heads}$  & $d_{head}$  \\ \midrule GPT-3 Small &
        Dense   & 0\% & 12 & 768 & 3072 & 12 & 64 \\
        GPT-3 Small & Sparse Wide   & 50\% & 12 & 1092 & 4344 & 12 & 64 \\
        GPT-3 Small & Sparse Wide   & 75\% & 12 & 1536 & 6144 & 12 & 64 \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \label{tab:app-gpt-pt-sift}
\end{table*}

\paragraph{WikiText-103 End-to-End Training Results} We highlight that in
Table~\ref{tab:gpt3_scratch_flops}, the Sparse Wide IFT GPT-3 Small at 50\%
sparsity attains a better perplexity on WikiText-103 while using 2.4x fewer
training FLOPs than the GPT-3 Medium dense model. In this setup, using Sparse
Wide transformation does not change the FLOP of the dense layer, but this leads
to a slight increase in the attention FLOPs. This explains the 1.17x increase in
FLOPs between the GPT-3 Small Sparse Wide at 50\% sparsity and the dense GPT-3
Small model. Note, out of all the Sparse-IFTs, this increase only occurs in the
Sparse Wide transformation.

\begin{table}[ht]
    \caption{Details on the total training FLOPs for each GPT-3 model tested. We
        note that the reported FLOPs per sequence (seq) include both forward and
        backward passes. The reported perplexity (lower is better) is on the
        WikiText-103 test set over 3 random seeds.}
    \begin{small}
    \begin{sc}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{cccccccc}
    \toprule
    Model       & Transformation & Sparsity & \begin{tabular}[c]{@{}c@{}}Total\\
    Seqs\end{tabular} & \begin{tabular}[c]{@{}c@{}}Total FLOPs/\\
    Seq\end{tabular} & \begin{tabular}[c]{@{}c@{}}Total\\ FLOPs\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Total\\ exaFLOPs\end{tabular} & Perplexity \\
    \midrule GPT-3 Small & Dense          & 0\%      &     2.28e6 & 8.763e11 &
    2.0011e18 & 2.00                                       & 20.8 $\pm$ 0.3 \\
    GPT-3 Small & Sparse Wide             & 50\%     &     2.28e6 & 1.029e12 &
    2.3498e18 & 2.35                                         & \textbf{20.4
    $\pm$ 0.2} \\
    \midrule GPT-3 Medium & Dense          & 0\%      &     2.28e6 & 2.4845e12 &
    5.6734e18 &                5.67 &                      20.5 $\pm$ 0.2 \\
    \bottomrule      
    \end{tabular}
    }
    \end{sc}
    \end{small}
    \label{tab:gpt3_scratch_flops}
\end{table}


\subsection{Details for Sparse Pre-training and Dense
Fine-tuning~\citep{thangarasa2023spdf}}
\label{app:spdf}

We provide an extended set of results that showcase the added benefit of using
Sparse-IFTs. Here, we apply the Sparse Pre-training and Dense Fine-tuning (SPDF)
framework introduced by~\citet{thangarasa2023spdf}. In this setup, all models
are pre-trained under a similar FLOP budget. However, during the fine-tuning
stage, Sparse-IFT models have extra representational capacity which can be
enabled by allowing the zeroed weights to learn (i.e., dense fine-tuning). Even
though the fine-tuning FLOPs are more than the original dense model, we leverage
Sparse-IFT method's extra capacity to obtain accuracy gains on the downstream
task. To ensure a fair baseline, we also compare dense fine-tuning to sparse
fine-tuning (i.e., pre-trained model remains as-is) similar
to~\citet{thangarasa2023spdf}.

\subsubsection{SPDF on BERT}
\paragraph{Experimental Setup}
We train BERT models using the open-source LAMB~\citep{you2020large}
implementation provided by~\citet{nvidiabert2019}. In this setup, BERT is
pre-trained on the BookCorpus~\citep{zhu2015bookc} and Wikipedia datasets in two
phases. In the first phase, models are trained for 82\% of total iterations with
a sequence length of 128. In the second phase, models are trained for the
remaining 18\% of iterations with sequence length 512. We use a batch size of
8192 and 4096 in phase 1 and phase 2, respectively. Table~\ref{tab:app-bert-pt}
shows details of the size and architecture of the BERT Small model. For
finetuning models on SQuADv1.1~\citep{rajpurkar2016squad}, we train for two
epochs with AdamW optimizer and use a grid search to tune the learning rate and
batch size.

\begin{table*}[h]
    \caption{ Size and architecture of the BERT Small model, which is trained
    using the setup from~\citet{nvidiabert2019} } \vskip 2pt
    \centering
    \begin{small}
    \begin{sc}
    \begin{tabular}{c|ccccc}
        \toprule
        Model & $n_{params}$ & $n_{layers}$ & $d_{model}$ & $n_{heads}$ &
        $d_{head}$ \\ \midrule BERT Small & 29.1M & 4 & 512 & 8 & 64 \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \label{tab:app-bert-pt}
\end{table*}

\paragraph{SPDF on SQuADv1.1 Results}
We evaluate BERT Small with Sparse Wide, Sparse Parallel, and Sparse Factorized
members of the Sparse-IFT family. All transformations, except Sparse Parallel,
perform comparably to the dense baseline on SQuAD. Unlike CV architectures, BERT
initializes the layers with a normal distribution, which has an adverse effect
when layers undergo shape transformations (e.g., changes in depth
\cite{zhang2019improving}, or width \cite{yang2022tensor}). In our initial
experiments, we found changing the initialization of BERT enables other families
to outperform the dense baseline. In addition to initialization, BERT training
has over six hyperparameters. We leave optimizing and analyzing the effect of
these hyperparameters on Sparse-IFT for future work and restrict our current
scope to demonstrating gains without tuning any hyperparameters. Using the
Sparse Parallel transformation with 50\% sparsity leads to a 0.7\% improvement
in the exact match (EM) accuracy over the dense baseline (see
Table~\ref{tab:bert_wikitext_ft}).


\begin{table}[ht]
    \caption{ Evaluation of Sparse Parallel IFT for pre-training BERT Small. We
report EM (higher is better) obtained by sparse fine-tuning and dense
fine-tuning BERT models on SQuADv1.1, respectively. } \vskip 1pt
    \centering
    % \resizebox{0.\columnwidth}{!}{
    \begin{small}
    \begin{sc}
    \begin{tabular}{c|ccccc}
        \toprule
        Dense	    & Transformation & Fine-Tuning Method & 0.50 & 0.75 \\
	\midrule
    \multirow{2}{*}{70.6} & \multirow{2}{*}{Sparse Parallel}  & Sparse &
    \textbf{70.7} & 69.9  \\
        & &  Dense &   \textbf{71.3}      &  70.8   \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    % }
    \label{tab:bert_wikitext_ft}
\vspace{-0.1in}
\end{table}


\subsubsection{SPDF on GPT}
\paragraph{Pre-training Experimental Setup}
Here, we pre-train the models on the Pile~\cite{gao2020pile} dataset. To train
all GPT models, we use AdamW optimizer \cite{loshchilov2017decoupled} with
$\beta_1 = 0.9$, $\beta_2 = 0.999$ and $\epsilon = 10^{-8}$. The global norm is
clipped at 1.0, and a weight decay of 0.1 is used. There is a learning rate
warmup over the first 375M tokens, followed by a cosine decay to 10\% of the
peak learning rate. We follow the recently published Chinchilla
\cite{hoffmann2022an} recommendations for obtaining loss-optimal pre-trained
baseline configurations of models. The context window size is 2048 following
\cite{brown2020language}. Table~\ref{tab:app-gpt-pt} shows a detailed breakdown
of the model architectures, learning rate, and training settings. In
Table~\ref{tab:app-gpt-pt-sift}, we outline the architecture configurations for
Sparse Wide IFT 50\% and 75\% variants.

\begin{table*}[!ht]
    \caption{ Size, architecture, and learning hyperparameters (batch size and
        learning rate) of the GPT-3 Small model, which is trained using
        Chinchilla optimal configurations ($\approx$ 20 tokens per parameter) }
        \vskip 2pt
    \centering
    \resizebox{\textwidth}{!}{
    \begin{small}
    \begin{sc}
    \begin{tabular}{c|ccccc|cc|c}
        \toprule
        Model & $n_{params}$ & $n_{layers}$ & $d_{model}$ & $n_{heads}$  &
        $d_{head}$ & Batch Size & Learning Rate & Training Tokens \\ \midrule
        GPT-3 Small           &  125M & 12 & 768 & 12 & 64 & 256 & 6$\times
        10^{-4}$ & 2.5B \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    }
    \label{tab:app-gpt-pt}
\end{table*}


\paragraph{Fine-tuning Experimental Setup}
\label{app:wikitext103} 
We finetune the Sparse Wide IFT variants of GPT-3 Small on the
WikiText-103~\citep{merity2017pointer} dataset following the setup presented
in~\citep{rae2021scaling}. We finetune for ten epochs and perform early stopping
once the models overfit. We performed a grid search to discover an appropriate
learning rate that led to the best perplexity for a given compute budget. More
specifically, on the dense baseline and Sparse Wide IFT variants, we use a batch
size of 32 and select the best learning rate among \{5e-3, 3e-3, 1e-3, 3e-4,
1e-4, 3e-5, 1e-5\} on the validation set.

In Tables~~\ref{tab:app-gpt-pt-sift},~\ref{tab:app-bert-pt},
and~\ref{tab:app-gpt-pt}, $n_{params}$ is the total number of trainable
parameters, $n_{layers}$ is the number of decoder layers, and $d_{model}$ is the
base size of the model. The feedforward bottleneck is four times the base size,
i.e., $d_{\text{ff}} = 4\times d_{model}$. Finally, $n_{heads}$ is the number of
attention heads, and $d_{head}$ is the dimension of each attention head.

\paragraph{SPDF on WikiText-103 Results}
Here, we pre-train a GPT-3 Small architecture with Sparse Wide transformations
at 50\% and 75\% sparsity. Post pre-training, we finetune our models on
WikiText-103. The GPT-3 Small 75\% Sparse Wide model reduces the perplexity
(PPL) by a noticeable 1.3 points compared to dense (refer to
Table~\ref{tab:gpt_wikitext_ft}).

\begin{table}[ht]
    \caption{ Evaluation of Sparse Wide IFT for pre-training GPT-3 Small. We
report perplexity (lower is better) obtained by sparse fine-tuning and dense
fine-tuning GPT models on Wikitext-103, respectively. } \vskip 1pt
    \centering
    % \resizebox{0.\columnwidth}{!}{
    \begin{small}
    \begin{sc}
    \begin{tabular}{c|ccccc}
        \toprule
        Dense	    & Transformation & Fine-Tuning Method & 0.50 & 0.75 \\
	\midrule
    \multirow{2}{*}{15.9} & \multirow{2}{*}{Sparse Wide}  & Sparse &
    \textbf{15.6} & 16.0  \\
        & &  Dense &   15.1      & \textbf{14.6}  \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    % }
\label{tab:gpt_wikitext_ft}
\vspace{-0.1in}
\end{table}


\section{Wall Clock Acceleration with Sparsity \label{app:benchmark}}

\paragraph{Inference}
We use Neural Magic's DeepSparse~\citep{neuralmagic2021, pmlrv119kurtz20a} tool
for benchmarking Sparse-IFT variants. The benchmarking is conducted on G4dn
instances available on the AWS cloud. These instances support the AVX-512
instruction set, which is used by the DeepSparse inference runtime to accelerate
unstructured sparsity. We report runtime for batch-inference of 64 images at 224
$\times$ 224 resolution.

\paragraph{Training}
We benchmark the training speed measured in seconds/iteration on a custom
hardware accelerator, which supports and accelerates training using unstructured
sparsity. Note that the overall FLOPs of models in the GPT family are comprised
of matrix multiplication FLOPs and attention FLOPs. Attention FLOPs (i.e., spent
in multi-head attention) scale quadratically with sequence length and are
invariant to weight sparsity. To demonstrate the efficacy of sparse kernels for
unstructured weight sparsity, we report our results for dense and Sparse Wide
variants of the GPT-3 1.3B model with a sequence length of 256 and batch size of
528. 

\section{Author Contributions}
\label{app:authorcontrib}

We provide a summary of each authorâ€™s contributions:

\begin{itemize}
    \item Shreyas Saxena conceived the key idea of matching the FLOPs of Sparse
    Wide transformation to a compact dense model, extended the idea to other
    members of the Sparse-IFT family, helped with the implementation,
    established cardinality of Sparse-IFT members to explain the results,
    conducted experiments for BERT, benchmarked Sparse-IFT for inference, and
    wrote majority of the manuscript.

    \item Vithursan Thangarasa was an integral part of the project by
    participating in discussions with Shreyas Saxena and contributing to the
    method. He also implemented all Sparse-IFTs in PyTorch, proposed using
    non-linearity in Sparse-IFTs, conducted experiments for the entire study on
    CIFAR-100 and its ablations, obtained initial results on ImageNet, extended
    Sparse-IFT to efficient architectures (e.g., BotNet, MobileViT), conducted
    the entire study with GPT on Cerebras CS-2, and contributed to writing parts
    of the manuscript.
    
    \item Abhay Gupta validated sparse optimizers in PyTorch, conducted
    experiments with Sparse-IFT ResNet variants on ImageNet, obtained results
    with MobileNetV2 architecture, helped with pre-training of Sparse-IFT
    variants of GPT on Cerebras CS-2, conducted all experiments of Sparse-IFT on
    downstream CV tasks, and contributed to writing parts of the manuscript.
    
    \item Sean helped with the bring-up of sparsity support on Cerebras CS-2
    which was crucial for benchmarking and training Sparse-IFT variants of GPT
    models, and provided feedback to improve the structuring and presentation of
    the manuscript.
    
\end{itemize}
