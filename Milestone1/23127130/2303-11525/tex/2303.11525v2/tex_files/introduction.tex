\section{Introduction}

\begin{figure}
\centering
\includegraphics[keepaspectratio=true, width=7.5cm]{./figures/SIFT_ResNet_Improvement.pdf}
\vspace{-0.1in}
\caption{ Accuracy vs. Training FLOPs for different variants of ResNet on
ImageNet. Sparse Iso-FLOP Transformation (Sparse-IFT) provides significant
accuracy gains across different models and sparsity levels while using the same
FLOP budget as its dense counterpart. In particular, the best Sparse-IFT
variants of ResNet-18 and ResNet-34 achieve 3.5\% and 2.7\% improvements over
their dense baselines, respectively. }
\vspace{-0.25in}
\label{fig:sift_resnet_improvement}
\end{figure}

Increases in model size and training data have led to many breakthroughs in deep
learning (e.g., AlexNet~\citep{krizhevsky2012imagenet},
ResNet~\citep{he2016identity}, Transformers~\citep{vaswani2017attention},
GPT~\citep{radford2018improving, radford2019language},
AlphaGo~\citep{silver2017mastering}, etc.). Consequently, the computational and
memory footprint of training and deploying deep neural networks (DNNs) has grown
exponentially. To enable the deployment of large models, multiple techniques
(e.g., distillation (Hinton et al., 2015), quantization (Han et al., 2015a),
pruning (Han et al., 2015b)) have been introduced to reduce inference FLOPs and
memory requirements. While these techniques improve inference efficiency (test
accuracy w.r.t inference FLOPs), the associated training costs are still
prohibitive. In this work, we focus on improving the training efficiency
(test-accuracy w.r.t training FLOPs) of DNNs.

Recent works~\citep{evci2020rigging, jayakumar2020top} have explored using
weight sparsity to reduce the FLOPs spent in training.
\citet{frankle2018lottery} demonstrate that sparse subnetworks (termed “lottery
tickets”) exist at initialization and can be trained to match the accuracy of
their original dense network. Inspired by this result, various dynamic sparse
training (DST) methods~\citep{ma2022effective,evci2020rigging, liu2021selfish,
jayakumar2020top} attempt to find optimal sparse subnetworks in a single
training run.  While these methods primarily aim to improve training efficiency
by reaching dense accuracy with fewer FLOPs, they often perform worse than their
dense baselines or rely on longer training schedules (up to 2-5$\times$ training
iterations) to close the gap. As a result, these techniques can sometimes even
require more FLOPs than training the dense
model~\citep{ma2022effective,evci2020rigging, jayakumar2020top}. In contrast to
prior work, we focus on showing training efficiency gains by using sparsity to
increase accuracy while consuming the same training FLOPs as the dense model.
Specifically, we introduce a family of Sparse Iso-FLOP Transformations
(Sparse-IFT) that can be used as drop-in replacements for dense layers in DNNs.
These transformations increase the representational capacity of layers and
facilitate the discovery of optimal sparse subnetworks without changing the
layer’s underlying FLOPs (i.e., Iso-FLOP). For example, making a layer wider but
sparser increases dimensionality while still maintaining FLOPs due to sparsity.
All Sparse-IFT members are parameterized by a single hyperparameter, the
sparsity level. Figure~\ref{fig:sift_resnet_improvement} summarizes the ImageNet
performance with ResNet models, where our Sparse Wide IFT variants significantly
increase the accuracy of matching Iso-FLOP dense models. In particular, Sparse
Wide ResNet-18 at 90\% sparsity improves the top-1 accuracy from 70.9\% to
74.4\% (+3.5\%), and outperforms a dense ResNet-34 (74.2\%) while using 2x fewer
FLOPs. We emphasize that these gains were obtained by replacing dense layers
with Sparse-IFTs and required no changes to training hyperparameters. The main
contributions of our work are: 

\vspace{-0.1in}

\begin{enumerate}%[noitemsep]
\item We introduce a family of Sparse Iso-FLOP Transformations to improve the
training efficiency of DNNs by improving accuracy while holding FLOPs constant.
These transformations are parameterized by a single hyperparameter (sparsity
level) and can be used as drop-in replacements for dense layers without changing
the overall FLOPs of the model.

\item In the CV domain, using Sparse-IFT increases the top-1 accuracy of
ResNet-18 and ResNet-34 by 3.5\% and 2.6\% respectively on ImageNet. Finetuning
these pre-trained models for object detection (MS COCO) and segmentation
(CityScapes) leads to an improvement of 5.2\% mAP and 2.4\% mIoU, respectively.

\item In the NLP domain, using Sparse-IFT with GPT-3 Small leads to a 0.4
perplexity improvement on the WikiText-103 language modeling task.

\item We report wall-clock speed-ups for both training on the Cerebras
CS-2~\citep{cerebrasHarnessingPower, liehotchips} and inference on a CPU with
unstructured sparsity, highlighting the practical value of Sparse-IFT.
\end{enumerate}
