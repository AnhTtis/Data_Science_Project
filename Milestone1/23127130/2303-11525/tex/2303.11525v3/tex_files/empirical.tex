\section{Empirical Evaluation}\label{sec:empirical_res} Building on insights
gained from our ablations discussed in Section~\ref{sec:sift_ablations}, we
apply Sparse-IFTs to ImageNet (Section~\ref{subsec:imagenet}), also
demonstrating its advantages for transfer learning in various computer vision
tasks. Additionally, we highlight the benefits of Sparse-IFT in the domain of
NLP by presenting results on GPT~\citep{brown2020language} in
Section~\ref{subsec:nlp_results}.

\begin{table}
    \caption{Sparse Wide IFT with various efficient architectures on CIFAR-100
across different levels of sparsity (columns).} 
    \centering
    \begin{sc}
    \begin{small}
    \resizebox{0.70\linewidth}{!}{
    \begin{tabular}{c|ccc}
        \toprule
        	    & Dense             & 0.50 & 0.75 \\ \midrule MobileNetV2 & 72.4
        $\pm$ 0.2    & 73.4 & \textbf{73.7} \\
        MobileViT-S & 73.5 $\pm$ 0.1    & 74.6 & \textbf{74.8} \\
        BotNet-26   & 78.0 $\pm$ 0.2    & 78.1 & \textbf{78.7} \\
        BotNet-50   & 79.8 $\pm$ 0.2     & 80.3 & \textbf{80.9} \\
        \bottomrule
    \end{tabular}
    }
    \end{small}
    \end{sc}
\label{tab:mbv2-cifar}
\end{table}

\begin{table}
        \caption{Sparse-IFT on ImageNet. Best result for each transformation and
    architecture is highlighted in bold.}
        \begin{center}
        \begin{small}
        \begin{sc}
        \resizebox{\linewidth}{!}{
        \begin{tabular}{c|c|c|ccc}
            \toprule
            Model & Dense & Transformation & 0.50 & 0.75 & 0.90 \\
    \midrule
            \multirow{2}{*}{ResNet-18} &  \multirow{2}{*}{70.9 $\pm$ 0.1} &
    Sparse Wide & 72.7 & 73.8 & \textbf{74.4} \\
            &   & Sparse Parallel & 72.7 & 73.2 & \textbf{74.0} \\ \midrule
            ResNet-34 & 74.2 $\pm$ 0.1 & Sparse Wide &  75.6 & 76.4 &
            \textbf{76.8} \\ \midrule BotNet-50 & 77.5 $\pm$ 0.1 &  Sparse Wide
            & 77.9 & 78.3 & \textbf{78.5} \\
            \bottomrule
        \end{tabular}
        }
        \end{sc}
        \end{small}
    \end{center}
    \label{tab:resnet-i1k}
\end{table}

\subsection{ImageNet}
\label{subsec:imagenet}
We apply the best-performing Sparse-IFT transformations (Sparse Wide IFT and
Sparse Parallel IFT) from CIFAR-100 to ImageNet using ResNet-18. We follow
published training settings for ImageNet~\citep{nvidia2023gpuperf}. Both
Sparse-IFT families achieve significantly higher accuracy compared to the dense
baseline (see Table~\ref{tab:resnet-i1k}). Specifically, Sparse Wide IFT
ResNet-18 at 90\% sparsity improves over the dense baseline by 3.5\% and matches
the accuracy of a dense ResNet-34 with 2Ã— fewer training FLOPs (refer to
Figure~\ref{fig:sift_resnet_improvement}). We also apply the best-performing
transformation (Sparse Wide IFT) to ResNet-34 and BotNet-50. Increasing sparsity
consistently improves accuracy, indicating enhanced training efficiency at
higher sparsities. On BotNet-50, a hybrid ViT model, there is a 1.1\%
improvement at 90\% sparsity.

\subsection{Transfer Learning on Downstream Tasks}
\label{subsec:transfer_learning}
To show the effectiveness of pre-training our Sparse-IFT classification
backbones, we evaluate them on 1) object detection on MS COCO
2017~\cite{lin2014microsoft}, and 2) semantic segmentation on
CityScapes~\cite{cordts2016cityscapes}. For object detection, we adopt
RetinaNet~\cite{lin2017focal} from the MMDetection open-source
toolbox~\cite{mmdetection} and report results in the standardized training
setting. For semantic segmentation, we utilize DeepLabV3+~\cite{chen2018encoder}
in the MMSegmenation open-source toolbox~\cite{mmseg2020}. We evaluate ResNet-18
with Sparse Wide IFT and to ensure FLOP-equivalent comparisons with the dense
backbone, the Sparse-IFT backbones remain sparse during fine-tuning.
Appendix~\ref{app:eval_downstream} provides more details on the training setup.
We summarize our findings in Table \ref{tab:down_stream}, where using Sparse
Wide IFT ResNet-18 backbone leads to significant accuracy gains across all
metrics on both tasks.

\begin{table}
    \caption{Sparse Wide IFT variants of ResNet-18 as backbones for: (a) object
detection on MS COCO, (b) semantic segmentation on Cityscapes.} 
    \centering
    \begin{small}
    \begin{sc}
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{c|c|cccc}
        \toprule
      & Metric & Dense & 0.50   & 0.75  & 0.90 \\
    \midrule
 \multirow{3}{*}{MS COCO}     &  AP      &  29.3  & 31.3 & 32.8 & \textbf{34.5}
 \\
      &  AP$_{50}$    &  46.2  & 49.0 & 51.0 & \textbf{53.5} \\
      &  AP$_{75}$    &  30.9  & 33.0 & 34.8 & \textbf{36.5} \\
    \midrule
  \multirow{2}{*}{CityScapes}     &  \text{mIoU}      &   76.7   &  77.9   &
  78.9 & \textbf{79.1} \\
      &  \text{mAcc}      &   84.4   &  85.1   & 85.7 & \textbf{86.0} \\
        \bottomrule
    \end{tabular}
    }
    \end{sc}
    \end{small}
\label{tab:down_stream}
\end{table}

\begin{table}
    \caption{Average accuracy of Sparse Wide IFT with GPT-3 Small across ARC,
     HellaSwag, TruthfulQA, MMLU and Winogrande tasks on the Open LLM
     Leaderboard.} 
    \begin{center}
    \begin{small} 
    \begin{sc}
    \resizebox{0.7\linewidth}{!}{
    \begin{tabular}{cc|cc}
        \toprule
       	Model	     & Dense & 0.50 & 0.75 \\

	\midrule
        GPT-3 Small &   33.8 $\pm$ 0.1  &  34.1      &  \textbf{34.7}  \\
        \bottomrule
    \end{tabular}
    }
    \end{sc}
    \end{small}
    \end{center}
\label{tab:nlp_scratch_result}
\end{table}

\subsection{Language Modeling}
\label{subsec:nlp_results} 
We pre-train the Sparse Wide IFT GPT-3 Small model at $s \in \{50\%, 75\%\}$
from scratch on the Pile~\citep{gao2020pile} dataset using the
SET~\citep{mocanu2018}, and compare against the standard dense model. All models
were trained on the Cerebras CS-2~\citep{lie_2023} following
Chinchilla~\citep{hoffmann2022an} for obtaining loss-optimal pre-trained
baseline configurations of models. We evaluate the models on 5 tasks from the
Open LLM leaderboard~\citep{open-llm-leaderboard} (i.e.,
ARC~\citep{clark2018think}, HellaSwag~\citep{zellers2019hellaswag},
MMLU~\citep{hendrycks2021measuring}, TruthfulQA~\citep{lin2022truthfulqa} and
Winogrande~\citep{DBLP:journals/corr/abs-1907-10641}), and show that the Sparse
Wide IFT GPT-3 Small at 75\% sparsity improves the average accuracy by a
noticeable 0.9\% (see Table~\ref{tab:nlp_scratch_result}). In
Appendix~\ref{app:gpt_e2e}, we provide details on the models and
hyperparameters.