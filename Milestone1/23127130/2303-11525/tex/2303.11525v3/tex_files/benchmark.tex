\section{Wall-Clock Acceleration with Sparse-IFT}
\label{sec:wall_clock}
Empirical studies in Section~\ref{sec:empirical_res} show noticeably improved
training efficiency (test accuracy~w.r.t training FLOPs) for Sparse-IFT models.
Despite theoretical significance, this improvement may not directly benefit
hardware unable to accelerate unstructured sparsity (e.g., Nvidia GPU, Google
TPU). Recent developments, like specialized software kernels and hardware (e.g.,
DeepSparse~\citep{neural_magic_2021} and Cerebras
CS-2~\citep{cerebrasHarnessingPower}) indicate promising gains in realizing
unstructured sparsity benefits during training and
inference~\citep{thangarasa2023spdf,lasby2023dynamic}. As sparse methods
co-designed with hardware evolve, FLOP reduction is expected to translate into
wall-clock speedups.

\begin{figure}[!ht]
    \centering
    \subfigure{\includegraphics[width=0.23\textwidth]{figures/neural_magic_inference_relative.pdf}}
    \subfigure{\includegraphics[width=0.23\textwidth]{figures/cerebras_training_speed_relative.pdf}}
    \caption{Benchmarking unstructured sparsity during (left) inference on
    Neural Magic's DeepSparse runtime and (right) training acceleration on the
    Cerebras CS-2. In both setups, we measure the relative increase in latency
    or training speed for Sparse-IFT variants against the dense model.}
    \label{fig:sparsity_benchmark}
\end{figure}

\paragraph{Real-World Acceleration} 
We assess Sparse-IFT's inference efficiency using
DeepSparse\footnote{\href{https://github.com/neuralmagic/deepsparse}{Neural
Magic DeepSparse}}. Evaluations include benchmarking Sparse Wide ResNet-18
models with sparsity $\in \{50\%, 75\%, 90\%\}$ for batched ImageNet inference.
We also evaluate the training efficiency of Sparse-IFT on the Cerebras
CS-2\footnote{\href{https://docs.cerebras.net/en/2.1.1/wsc/how_to_guides/sparsity.html}{Cerebras
CS-2 (R2.1.1): Train a Model with Weight Sparsity}} which supports unstructured
sparse training of LLMs. Sparse Wide GPT-3 20B configurations at sparsity $\in
\{50\%, 75\%, 90\%\}$ are benchmarked, reporting secs/iteration. Results,
detailed in Figure \ref{fig:sparsity_benchmark}, reveal that up to 75\%
sparsity, Sparse-IFT shows minimal compute overhead. At 90\% sparsity, there is
some overhead, yet a notable speed-up of 5.2x and 4.1x over GPU during inference
and training, respectively. More details on the benchmarking setups are outlined
in Appendix~\ref{app:benchmark}. The interaction of layer dimensions, sparsity,
and overhead is influenced by hardware architecture, emphasizing the importance
of co-designing sparse techniques with hardware for optimal performance. Our
work aims to showcase algorithmic advancements over previous sparse methods, and
our results suggest training consistently in a sparse manner and emphasize the
benefits of winning ``the hardware lottery''~\citep{2020shooker}.

