\section{Related Work}

Our work aligns with research on overparameterization and sparsity in DNN
training. The required modeling capacity for a given task is often unknown,
leading to training overparameterized models to fully exploit learning
capabilities before compressing them into smaller subnetworks.

\paragraph{Overparameterization}~\citet{nakkiran2021deep} show that DNNs benefit
from overparameterization. Following this, there have been many works that
leverage overparameterization by scaling the size of models
\cite{rae2021scaling, goyal2022vision} and augmenting existing DNNs to increase
modeling capacity and the accuracy of trained networks
~\cite{shuxuan2020expandnet, ding2019acnet, ding2021repvgg, jinming2022doconv,
mobileone2022, liu2022more}. These methods use linear parameterizations of the
model, making them highly inefficient to train, and are focused on improving
inference throughput (reduced latency). In contrast, our work is focused on
improving the modeling capacity using sparse non-linear parameterizations. Our
approach enhances accuracy without increasing training and inference FLOPs
compared to the baseline model.

\paragraph{Sparse Training} The Lottery Ticket
Hypothesis~\cite{frankle2018lottery, frankle2020linear} shows that accurate
sparse subnetworks exist in overparameterized dense networks but require
training a dense baseline to find. Other approaches have proposed frameworks for
identifying lottery tickets~\citep{hattie2019supermask, ma2022effective} but
still require a lot of compute resources. Following this, various attempts have
been made to find the optimal sparse subnetwork in a single shot. These methods
either try to find the subnetworks at initialization~\cite{tanaka2020pruning,
wang2020picking, de2020progressive, lee2018snip} or dynamically during
training~\cite{mocanu2018, evci2020rigging, jayakumar2020top, raihan2020sparse}.
However, given a fixed model capacity, these methods tradeoff accuracy relative
to the dense baseline to save training FLOPs. ~\citet{stosic2021search}
and~\citet{ramanujan2020s} increase the search space during sparse training to
retain accuracy; however, do not guarantee FLOPs savings. In contrast to these
methods, our work introduces a set of non-linear sparse transformations, which
increase the representational capacity of the network. This approach does not
introduce a new sparse training algorithm, but instead improves the search space
of existing methods, leading to improved generalization while being efficient to
train.

\paragraph{Iso-Parameter vs. Iso-FLOP} Recent works have focused on improving
generalization at high sparsity levels. Hence, layer-wise sparsity distributions
such as the Erd\"os-R\'enyi-Kernel~\cite{evci2020rigging}, Ideal Gas
Quota~\cite{chen2022sparsity}, and parameter leveling~\cite{golubeva2021are} are
often used with sparse training to boost accuracies. However, these works target
the setting where the models being compared have a fixed parameter budget (i.e.,
Iso-Parameter), which does not translate to similar training FLOPs to the
original dense model (especially in CNNs). In contrast to methods that
necessitate varied memory or computational resources per layer when training
models with diverse distributions, our approach prioritizes a uniform sparsity
distribution. This ensures consistent sparsity levels for every layer, leading
to uniform FLOP reductions across the network. By employing Iso-FLOP
transformations and sparsity, we achieve equivalent computational FLOPs to a
dense network.
