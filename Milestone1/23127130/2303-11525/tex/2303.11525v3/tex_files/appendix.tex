\section{Additional Methodology Details}

\subsection{Sparse-IFT for Convolutional Layers \label{app:sift_conv}} In this
section, we detail the straightforward extension of the Sparse-IFT family for
convolutional layers.

\paragraph{Sparse Wide}
Similar to the setup for fully connected layers, in the case of convolutional
layers, we widen the number of input and output channels.

\paragraph{Sparse Parallel}
Similar to the setup for fully connected layers, in the case of convolutional
layers, we can implement this transformation with the use of convolutional
branches in parallel.

\paragraph{Sparse Factorized and Sparse Doped}
Let $\theta_l \in \mathbb{R}^{c_{in}\times c_{out} \times k_h \times k_w}$
represent the weight matrix of a convolutional layer, where $c_{in}, c_{out},
k_h, k_w$ denote the input channels, output channels, kernel height, and kernel
width, respectively. We apply low-rank or matrix factorization to the weight
matrix by first converting the 4D tensor into a 2D matrix with shape:
$(c_{in}\cdot k_h \cdot k_w)\times c_{out}$. In this setup, we can express
$\theta_l = UV^T$, where $U \in \mathbb{R}^{c_{in}\cdot k_h \cdot k_w \times
d}$, $V \in \mathbb{R}^{c_{out} \times d}$. In this factorization, $U$ learns a
lower-dimensional set of features and is implemented as a convolutional layer
with $d$ output channels and $k_h \times k_w$ filter. $V$ matrix expands this
low-dimensional set of features and is implemented as a convolutional layer with
$1\times1$ filter.

\subsubsection{Sparse-IFT for Depthwise Convolution Layers}

For a normal convolution layer, all inputs are convolved to all outputs.
However, for depthwise convolutions, each input channel is convolved with its
own set of filters. Let $\theta_l \in \mathbb{R}^{c_{in}\times c_{out} \times
k_h \times k_w}$ represent the weight matrix of a normal convolution layer,
where $c_{in}, c_{out}, k_h, k_w$ denote the input channels, output channels,
kernel height, and kernel width, respectively. An equivalent depthwise
convolution layer will have weights $\theta_{dw,l} \in \mathbb{R}^{ 1\times
c_{out} \times k_h \times k_w}$.

\paragraph{Sparse Wide} A Sparse Wide depthwise convolution will have weights
$\theta_{dw,l}^{sw} \in \mathbb{R}^{ 1\times k_{sw}{\cdot}c_{out} \times k_h
\times k_w}$. Since the fraction of non-sparse weights is given by $1-s$, the
FLOPs required by this transformation are
$B{\cdot}(k_{sw}{\cdot}c_{out}){\cdot}k_h{\cdot}k_w{\cdot}(1-s)$. Setting these
equal to the FLOPs of the original dense $\theta_{dw,l}$, we obtain the widening
factor $k_{sw} = \frac{1}{(1-s)}$. In this case, we do not scale the input
channels as it converts the depthwise convolution to a grouped convolution
without an equivalent scaling in the number of groups.

\paragraph{Other Sparse-IFT Transformations} The Sparse Wide IFT generally
changes a layer's input and output channels, subsequently scaling the following
layers in a CNN. However, the other Sparse-IFT transforms (Sparse Parallel,
Sparse Factorized, and Sparse Doped) do not modify a convolution layer's input
or output channels (as seen in Figure~\ref{fig:diff_members_of_sift}). This
allows for fine-grained control of what layers to apply the Sparse-IFT
transformations. Since depthwise convolutions are an extreme form of structured
sparsity, where some filters interact with only specific input channels, we opt
not to sparsify them when using the other Sparse-IFT transformations and leave
the layer unchanged while still maintaining FLOPs equivalent to the dense
baseline. Note that the different convolution layers surrounding the depthwise
convolution are still transformed with Sparse-IFT to increase their
representational capacity.

\subsection{Controlling for Iso-FLOP}
\label{app:control_flops}
As mentioned before, in our work, we mainly apply a uniform sparsity
distribution to the model, which essentially means each layer is allocated the
same level of sparsity.  Let $\mathcal{N}$ denote a $L$ layered DNN
parameterized by $\Theta_{\mathcal{N}}$. Let $\Theta_{\mathcal{N}} \in
\{\theta_1, \ldots, \theta_L\}$ denote the parameters of the DNN. Now, let $M_l$
be the binary mask for layer $l \in \{1,\ldots,L\}$ with dimensions
corresponding to the parameters of that layer. The binary mask $m_l$ has values
of 1 for active weights and 0 for non-active weights. Let $\theta_l$ be the
total number of parameters in $l$, hence, the sparsity level per layer, $s_l$,
is defined as $\frac{\sum_{i,j} \mathbb{I} (m_l(i,j) \neq 0)}{\theta_l}$. The
average sparsity level in the network, $s$, is then defined as the ratio of the
total number of zero parameters to the total number of parameters. This is
expressed as $s = \frac{\sum_{l=1}^{L}\sum_{i,j}\mathbb{I}(m_l(i,j) \neq
0)}{\Theta_{\mathcal{N}}}$. Below, we characterize the different scenarios when
training with different sparse training methods:

 \begin{itemize}
    \item \textbf{Random Static Sparsity:} In this case, the sparsity
    distribution is uniform, ensuring that the sparsity in each layer matches
    the target sparsity level. Consequently, the application of Sparse-IFT,
    parameterized by the sparsity level, maintains Iso-FLOP equivalence to the
    original dense model. However, adhering to common practice for computer
    vision networks (e.g., ResNet), we retain the first and last layers (input
    convolution and output linear layer) as dense to prevent a significant
    decline in model quality during pre-training. Consequently, the Sparse-IFT
    network deviates from Iso-FLOP to the dense model, introducing additional
    FLOPs that need consideration.
    \item \textbf{Pruning at Initialization:} The algorithms, such as
    SNIP~\citep{lee2018snip}, GraSP~\citep{wang2020picking},
    FORCE~\citep{de2020progressive}, etc., introduce distinct criteria or
    methods for determining which weights to prune at initialization,
    influencing the sparsity distribution. Consequently, the inherent
    characteristics of these algorithms result in changes to the sparsity
    distribution. In the context of Sparse-IFT, despite having an identical
    total sparse parameter count to the original dense model, the Sparse-IFT
    network no longer maintains Iso-FLOP equivalence.
    \item \textbf{Dense-to-Sparse Training:} Sparse training methods, such as
    GraNet~\citep{liu2021sparse}, employ dense-to-sparse training, initiating
    training from either a fully dense state or a state less sparse than the
    target sparsity level. For instance, GraNet utilizes gradual magnitude
    pruning~\citep{Zhu2017ToPO} at the beginning of training to systematically
    reduce the network's density to the target sparsity level. Consequently, in
    the context of Sparse-IFT networks, this configuration no longer maintains
    Iso-FLOP equivalence to the dense model, as the average training FLOPs
    surpass those of the original dense model.
 \end{itemize}

 To address the FLOPs discrepancy between the Sparse-IFT network trained with
 non-uniform sparsity distributions (e.g., PaI methods or densifying certain
 layers) and dense-to-sparse training (e.g., GraNet), we employ a binary search
 to fine-tune the target sparsity of the network prior to any training. In this
 process, we set the maximum and minimum values for the target sparsity level.
 At each iteration, we profile the FLOPs used by the Sparse-IFT network and
 compare it to the original dense model FLOPs. The target sparsity level is
 adjusted through the binary search, ensuring that the total FLOPs of the
 Sparse-IFT network are within 0.0001\% of the dense model FLOPs.

\section{Graph Analysis of Sparse-IFT with DST}
\label{app:graph_analysis_sift_dst}
In our analysis, we interpret the Sparse-IFT ResNet-18 models as a series of
bipartite compute graphs, where each layer, $\{\theta_1,\ldots, \theta_L\}$ in
an $L$ layered sparse DNN,  takes the form of a square adjacency matrix $A$. The
Ramanujan gap is defined as $\Delta r = 2 \ast \sqrt{d-1} -
\hat{\mu}(A)$~\citep{hoang2023revisiting,hoang2023dont}, where $d$ is the
average edge per node, and $\hat{\mu}(A)$ is the non-trivial eigenvalue of $A$.
Also, we analyze the Iterative Mean Difference Bound, $\Delta r_{imdb} =
\frac{1}{\abs{K}} \sum_{i=1}^{\abs{K}}(2\sqrt{d_i - 1} -
\hat{\mu}(A_{K_i}))$~\citep{hoang2023revisiting}. We train a ResNet-18 model
with all members of the Sparse-IFT family using a dynamic sparse training
algorithm (i.e., RigL~\citep{evci2020rigging}). 

\textbf{$\vb*{\Delta r}$ Analysis:} In Figure~\ref{fig:graph_analysis_detailed},
we observe that $\Delta r$ decreases over the course of training and then
maximizes at later stages, which suggests that the spectral properties of the
adjacency matrices are changing dynamically during training. The fact that
$\Delta r$ maximizes at later stages and correlates with the Sparse-IFT
ResNet-18 model achieving the highest test accuracy indicates a potential
connection between the spectral properties of the adjacency matrices and the
model's performance. The dynamic changes in $\Delta r$ might indicate that the
neural network is adapting its structure during training. The network might be
pruning less important connections and reinforcing more important ones, leading
to an optimized structure. Moreover, the increase in $\Delta r$ could be related
to implicit regularization effects. The spectral properties of the adjacency
matrices may play a role in controlling the model's capacity, preventing
overfitting, and enhancing generalization. The correlation between the
maximization of $\Delta r$ at the later stages of training and the highest test
accuracy suggests that there is a relationship between the identified spectral
properties and the performance of the Sparse-IFT ResNet-18 model. The
maximization of $\Delta r$ could represent an optimal point in the trade-off
between sparsity and model accuracy for the given task.

\textbf{$\vb*{\Delta r_{imdb}}$ Analysis:} The increasing trend of $\Delta
r_{imdb}$ during training suggests that the overall connectivity boundary across
subgraphs is progressively being enhanced. This could imply that the network is
learning to establish more meaningful and relevant connections within its
structure as training progresses. The DST algorithm may be facilitating an
adaptive refinement of connectivity within the network. The observed increase in
$\Delta r_{imdb}$ could indicate that the model is iteratively adjusting its
connectivity boundaries to improve information flow. $\Delta r_{imdb}$ evaluates
the average connectivity boundary across all subgraphs, providing a more
comprehensive measure of the network's overall connectivity changes. The
correlation with the highest performing models at the final stage of training
suggests that the average connectivity enhancements captured by $\Delta
r_{imdb}$ are beneficial for the model's performance.

\begin{figure*}
    \vspace{-0.1in}
    \centering
    \subfigure{\includegraphics[width=0.30\textwidth]{figures/resnet18_sw_50_iter_vs_epochs.pdf}}
    \subfigure{\includegraphics[width=0.30\textwidth]{figures/resnet18_sp_50_iter_vs_epochs.pdf}}
    \subfigure{\includegraphics[width=0.30\textwidth]{figures/resnet18_sf_50_iter_vs_epochs.pdf}}
    \\
    \subfigure{\includegraphics[width=0.30\textwidth]{figures/resnet18_sw_50_deltar_vs_epochs.pdf}}
    \subfigure{\includegraphics[width=0.30\textwidth]{figures/resnet18_sp_50_deltar_vs_epochs.pdf}}
    \subfigure{\includegraphics[width=0.30\textwidth]{figures/resnet18_sf_50_deltar_vs_epochs.pdf}}
    \vspace{-15pt}
    \caption{Illustrates the dynamic interplay between the (top row) Iterative
    Mean Difference Bound, $\Delta r_{imdb}$ and test accuracy, as well as the
    correlation between the Ramanujan Gap, $\Delta r$ and test accuracy
    throughout the training process. This illustrates the evolving relationship
    between spectral graph properties and network performance, shedding light on
    the connectivity dynamics of the Sparse-IFT networks trained with DST.}
    \vspace{-5pt}
    \label{fig:graph_analysis_detailed}
    \vspace{-10pt}
\end{figure*}

\section{Computer Vision: Experimental Settings \label{app:cv-exp-details}}

\subsection{Computer Vision: Pre-Training Settings \label{app:pt-implement}}
\paragraph{CIFAR-100} Our implementation of CIFAR-100 follows the setup
from~\cite{devries2017improved} for ResNets. We train the models for 200 epochs
with batches of 128 using SGD, Nesterov momentum of 0.9, and weight-decay of
5$\times 10^{-4}$. The learning rate is initially set to 0.1 and is scheduled to
decay to decrease by a factor of 5x after each of the 60th, 120th, and 160th
epochs. Following recent advances in improving ResNets, we initialize the
network with Kaiming He initialization~\cite{he2016identity}, zero-init
residuals~\cite{he2019bag}, and disable weight-decay in biases and
BatchNorm~\cite{ioffe2015batch} layers. For CIFAR-100 experiments with
MobileNetV2, MobileViT-S, and BotNet-50, we follow the same training setup used
for ResNet, but the learning rate is scheduled via cosine annealing.

\paragraph{ImageNet} Our implementation of ImageNet follows the standard setup
from~\cite{krizhevsky2017imagenet, simonyan2014very}. The image is resized with
its shorter side randomly sampled in [256, 480] for scale
augmentation~\cite{simonyan2014very}. A 224 $\times$ 224 crop is randomly
sampled from an image or its horizontal flop, and then normalized. For
evaluation, the image is first resized to 256 $\times$ 256, followed by a 224
$\times$ 224 center crop, and then normalized. Following recent advances in
improving ResNets, we initialize the network with Kaiming He
initialization~\cite{he2016identity} and zero-init residuals~\cite{he2019bag}.

For ResNets, we replicate the settings recommended by Nvidia~\cite{resnetv15},
which uses the SGD optimizer with a momentum of 0.875 and weight decay of
3.0517578125$\times 10^{-5}$. We disable weight-decay for biases and BatchNorm
layers. The model is trained with label smoothing~\cite{szegedy2016rethinking}
of 0.1 and mixed precision~\cite{micikevicius2017mixed} for the standard 90
epochs using a cosine-decay learning rate schedule with an initial learning rate
of 0.256 for a batch size of 256.~\citet{srinivas2021bottleneck} follow the same
setup as ResNet for training BotNet-50 on ImageNet, therefore we maintain the
same hyperparameter settings as~\citet{resnetv15} for our BotNet-50 ImageNet
experiments.

\paragraph{Sparsity Setup} For enabling the Sparse-IFT transformations, we use
the RigL~\cite{evci2020rigging} algorithm in its default hyperparameter settings
($\alpha = 0.3, \Delta T = 100$), with the drop-fraction ($\alpha$) annealed
using a cosine decay schedule for 75\% of the training run. We keep the first
and last layers (input convolution and output linear layer) dense to prevent a
significant degradation in model quality during pre-training, which is standard
practice. We account for these additional dense FLOPs by increasing the sparsity
in the remaining layers, similar to~\citet{gale2019state}
and~\citet{liu2022unreasonable}.

\subsection{Importance of Non-linearity}\label{app:nonlinear-importance} We use
BatchNorm~\cite{ioffe2015batch} followed by ReLU~\cite{nair2010rectified} as a
non-linearity. We provide an extended set of empirical results in
Table~\ref{tab:non_linearity_importance} to help validate the importance of
training with and without non-linearity by training configurations of the Sparse
Parallel, Factorized, and Doped IFT families at different levels of sparsity.
The results without non-linear activation functions are often worse than the
dense accuracy (77\%) across all Sparse-IFT family transformations. We omit
Sparse Wide in Table~\ref{tab:non_linearity_importance} because here we increase
the number of channels in the convolutional layers while maintaining the
existing architecture.

\begin{table}[h]
    \caption{Evaluation on the importance of utilizing the non-linear activation
across different members of Sparse-IFT with ResNet-18 on CIFAR100 across
different values of sparsity (columns). Non-linear activations enhance the
representational capacity of Sparse-IFT, leading to higher accuracy. All
reported results are the average over 3 random seeds.} 
    \centering
    \begin{small}
    \begin{tabular}{c|cccc}
        \toprule
        Transformation & Non-linear activation & 0.50  & 0.75 & 0.90  \\
	\midrule
    \multirow{2}{*}{Sparse Factorized} & \xmark    &  75.9 $\pm$ 0.3 & 76.6
    $\pm$ 0.4 & 76.5 $\pm$ 0.4  \\
       &  \cmark

&  \textbf{77.8 $\pm$ 0.4} &  \textbf{78.4 $\pm$ 0.5} & \textbf{78.9  $\pm$ 0.5}
\\
       \midrule \multirow{2}{*}{Sparse Parallel} & \xmark            	      &
       77.1 $\pm$ 0.1 &  77.2 $\pm$ 0.2 & 77.6 $\pm$ 0.1  \\
       &  \cmark         	      &  \textbf{77.9 $\pm$ 0.2 } &  \textbf{79.1
       $\pm$ 0.2 } & \textbf{78.2 $\pm$ 0.2} \\
       \midrule \multirow{2}{*}{Sparse Doped} & \xmark            	      & 77.3
       $\pm$ 0.2 &  77.1 $\pm$ 0.1 & 76.5 $\pm$ 0.2  \\
       &  \cmark         	      &  \textbf{78.2 $\pm$ 0.1} &  \textbf{77.8
       $\pm$ 0.1} & \textbf{76.9 $\pm$ 0.2} \\
        \bottomrule
    \end{tabular}
    \end{small}
    \label{tab:non_linearity_importance}
\end{table}

\subsection{Computer Vision \label{app:ft-implement}}

\subsubsection{Sparse-IFT vs. Extended Sparse Training Schedules}
\label{app:sift_vs_extendedsparse}
We provide a direct comparison with sparse training methods (e.g., RigL and SET)
in the Iso-FLOP setting (i.e., training with a longer schedule) to demonstrate
the significance of our results with respect to this standard sparse baselines.
As shown in the Table~\ref{tab:sift_vs_extendedsparse}, Sparse-IFTs outperform
dynamic sparse training methods by a significant margin across all levels of
sparsity. Note, at higher levels of sparsity (e.g., 90\%), sparse training
methods obtain worse accuracy compared to the FLOP equivalent dense baseline. In
contrast, with Sparse-IFT, we observe higher accuracy across all levels of
sparsity evaluated.

\begin{table}
    \caption{Results with ResNet-18 on CIFAR-100 across different values of
    sparsity (columns). Best accuracy for each sparse training method is
    highlighted in bold. The original dense ResNet-18 model obtains an accuracy
    of 77.0$\pm$0.2. All reported results are over 3 random seeds. } 
    \centering
    \begin{small}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{c|cccccc}
        \toprule
        Dense 		& Transformation & Sparse Training Method & Epochs & 0.50  &
        0.75 & 0.90  \\
	\midrule
 \multirow{4}{*}{77.0 $\pm$ 0.2}
		& Sparse Wide &  SET & 200 $\cdot$ $\frac{1}{1-s}$ & \textbf{78.7 $\pm$
		0.2} & 78.4 $\pm$ 0.1 & 76.8 $\pm$ 0.1  \\
		&  Sparse Wide  &  RigL & 200 $\cdot$ $\frac{1}{1-s}$ & \textbf{78.9
		$\pm$ 0.1} & 78.8 $\pm$ 0.1 & 76.4 $\pm$ 0.2 \\
		&  Sparse Wide  &  RigL  & 200 &  79.1 $\pm$ 0.2 & 79.5 $\pm$ 0.1 &
		\textbf{80.1 $\pm$ 0.2} \\
        \bottomrule
    \end{tabular}
    }
\end{small}
\label{tab:sift_vs_extendedsparse}
\end{table}

\subsubsection{Sparse-IFT on Efficient Computer Vision
Architectures}\label{app:efficientcv} Here, we provide an extended set of
results on MobileNetV2, MobileViT-S, and BotNet-50 on CIFAR-100. In particular,
we enable Sparse Wide and Sparse Parallel IFT at 50\% and 75\% sparsity values
(see Table~\ref{tab:mbv2-cifar-app}).

\begin{table}[!ht]
    \caption{Evaluation of Sparse Wide and Sparse Parallel IFT with various
compute efficient architectures on CIFAR-100 across different values of sparsity
(columns). Using Sparse Parallel IFT, all architectures outperform the dense
baseline by a significant margin.} \vskip 2pt
    \centering
    \begin{small}
    \begin{tabular}{c|c|ccc}
        \toprule
        	    & Dense  & Transformation           & 0.50 & 0.75 \\ \midrule
                \multirow{2}{*}{MobileNetV2} & \multirow{2}{*}{72.4 $\pm$ 0.2 }
                & Sparse Wide  & 73.4 & \textbf{73.7} \\

                & & Sparse Parallel & 72.9 & \textbf{73.3} \\ \midrule
                \multirow{2}{*}{MobileViT-S} & \multirow{2}{*}{73.5 $\pm$ 0.1} &
                Sparse Wide & 74.6 & \textbf{74.8} \\
                & & Sparse Parallel & 73.7 & \textbf{74.4} \\ \midrule
\multirow{2}{*}{BotNet-50}  & \multirow{2}{*}{79.8 $\pm$ 0.2} & Sparse Wide &
80.3 & \textbf{80.6} \\
        & & Sparse Parallel & 79.7 & \textbf{80.5} \\
        \bottomrule
    \end{tabular}
    \end{small}
\label{tab:mbv2-cifar-app}
\end{table}

\subsubsection{Evaluation of Sparse-IFT with Structured Sparsity}
\label{app:structured_sparsity}

\paragraph{Block Sparsity}
To derive Iso-FLOP configurations with block sparsity, we reuse the analysis
done previously with unstructured sparsity (see
Section~\ref{subsec:members_of_sift}) and express the width scaling as a
function of sparsity. However, we will search for a block sparse mask during
training instead of an unstructured sparsity mask. We use the method proposed
by~\citet{hubara2021accel} to search N:M transposable sparsity, which can
accelerate both the forward and backward pass during training on NVIDIA GPUs
with Tensor Cores. We use 4:8-T, 2:8-T, and 1:8-T block patterns to obtain 50\%,
75\%, and 87.5\% sparsity, respectively. Note the 1:8-T block is the closest
approximation to a 90\% sparsity pattern attainable with a block size of 8. We
also set up and experimented using the method proposed
by~\citet{jiang2022exposing} to train with fine-grained sparse block structures
dynamically. However, the algorithm uses agglomerative clustering which led to a
much slower runtime and quickly ran out of memory even at 50\% sparsity using
the Sparse Wide IFT on a single Nvidia V100 (16 GB).

\paragraph{Low Rank}
Let $k_{lr}$ be the factor with which we widen all layers' input and output
 dimensions for low-rank factorization. We replace all dense layers with
 low-rank factorization, i.e. $\theta_l^{lr} = U_lV_{l}^{T}$, where $U_l \in
 \mathbb{R}^{(k_{lr}.D_{in}) \times d}$ and $V_l \in \mathbb{R}^{
 (k_{lr}.D_{out}) \times d}$. Given a widening factor and equating the FLOPs of
 this transformation to that of a dense transformation $f_{\theta}$, we obtain
 the following expression for rank $d$: $\frac{D_{in}.D_{out}.k_{lr}}{(D_{in} +
 D_{out}}$. We evaluate this factorization across different values of
 width-scaling $k_{lr}$ in Table~\ref{tab:struct-cifar-lowrank}.

\begin{table*}[]
    \caption{Comparison of structured sparse and unstructured sparse methods on
    CIFAR-100 test accuracy on ResNet-18.} 
    \centering
    \begin{small}
    \begin{tabular}{c|cccccc}
        \toprule
        &               &          & \multicolumn{4}{l}{Width Scaling Factor} \\
        Transformation & Sparsity Type & Sparsity & 1x      & 1.41x     & 2x &
        3.16x     \\ \midrule Low Rank, Linear &  Structured & 0\% & 74.1 & 74.3
        & 74.3 & 73.4 \\
        Low Rank, Non-Linear & Structured  &  0\% &  76.8 & 76.5 & 76.0 & 75.3
        \\
\midrule
        \multirow{6}{*}{Sparse Wide} &
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}N:M Block Sparse\\
\citep{hubara2021accel}\end{tabular}} & 4:8-T    &         &    77.1       & &
 \\
        &                                       & 2:8-T    &         & &
        \textbf{78.4} &           \\
        &                                       & 1:8-T    &         & & & 78.1
        \\
   & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Unstructured Sparse\\
\citep{evci2020rigging}\end{tabular} } & 50\%    &         &    79.1       & &
\\
        &                                       & 75\%   &         &           &
        79.5     &           \\
        &                                       & 90\%    &         & &        &
        \textbf{80.1}  \\
        \bottomrule
    \end{tabular}
    \end{small}
\label{tab:struct-cifar-lowrank}
\end{table*}

\subsubsection{Evaluation on downstream tasks \label{app:eval_downstream}}
\subsubsection*{COCO Object Detection \label{app:coco}} This dataset contains
118K training, 5K validation (\texttt{minival}), and 20K test-dev images. We
adopt the standard single-scale training setting~\cite{lin2017feature} where
there is no additional data augmentation beyond standard horizontal flipping.
For training and testing, the input images are resized so that the shorter edge
is 800 pixels~\cite{lin2017feature}. The model is trained with a batch size of
16, using the SGD optimizer with a momentum of 0.9 and weight decay of 1$\times
10^{-4}$. We follow the standard 1x schedule (12 epochs) using a step learning
rate schedule, with a 10x decrease at epochs 8 and 11, an initial learning rate
warmup of 500 steps starting from a learning rate of 2$\times 10^{-5}$, and a
peak learning rate of 0.01.

\begin{table*}[h]
    \caption{ Object detection results on COCO \texttt{minival} in the RetinaNet
        framework. Sparse Wide IFT configurations of RetinaNet outperform the
        dense baseline by a large margin on all metrics while using similar
        FLOPs. } 
    \centering
    \begin{small}
    \begin{tabular}{c|cccccccc}
        \toprule
        Backbone   &      AP   & AP$_{50}$ & AP$_{75}$ & AP$_{S}$ & AP$_{M}$ &
        AP$_{L}$ \\ \midrule Dense           &   29.3 & 46.2 & 30.9 & 14.7 &
        31.5 & 39.6 \\
        Sparse Wide (50\%)  & 31.3 & 49.0 & 33.0 & 16.6 & 34.0 & 42.0 \\
        Sparse Wide (75\%) & 32.8 & 51.0 & 34.8 & 17.3 & 35.8 & 43.3 \\
        Sparse Wide (90\%) & \textbf{34.5} & \textbf{53.5} & \textbf{36.5} &
        \textbf{18.6} & \textbf{37.6} & \textbf{45.3} \\
        \bottomrule
    \end{tabular}
    \end{small}
    \label{tab:app-cocoretinanet}
\end{table*}

\subsubsection*{CityScapes Semantic Segmenation \label{app:cityscapes}}

\paragraph{Setup} We follow the same training protocol
as~\cite{zhao2017pyramid}, where the data is augmented by random cropping (from
1024 $\times$ 2048 to 512 $\times$ 1024), random scaling in the range [0.5, 2],
and random horizontal flipping. The model is trained with a batch size of 16,
using the SGD optimizer with a momentum of 0.9 and weight decay of 5$\times
10^{-4}$. We follow the 80K iterations setup from MMSegmentation with an initial
learning rate of 0.01 annealed using a poly learning rate schedule to a minimum
of 1$\times 10^{-4}$. Similar to most setups that tune
hyperparameters~\cite{zhao2017pyramid, liu2021swin, wang2020deep} for reporting
the best results, we tune the learning rate for all our models. All our results
are reported using a learning rate of 0.03 for the sparse backbones and 0.01 for
the dense baseline.

\begin{table*}[h]
    \caption{ Semantic segmentation results on the Cityscapes \texttt{val} set
        using DeepLabV3+. Sparse Wide IFT configurations ResNet-18 backbones
        outperform the dense baseline on all metrics while using similar FLOPs.
        }
    \centering
    \begin{small}
    \begin{tabular}{c|cc}
        \toprule
        Backbone  &  mIoU & mAcc \\ \midrule Dense          & 76.72 & 84.40 \\
        Sparse Wide (50\%)      &  77.90 & 85.12 \\
        Sparse Wide  (75\%)      &   78.92 & 85.68 \\
        Sparse Wide  (90\%)      &  \textbf{79.10} & \textbf{86.01} \\
        \bottomrule
    \end{tabular}
    \end{small}
    \label{tab:app-cityscapes}
\end{table*}

\section{Natural Language Processing: Experimental
Settings~\label{app:llm-setup}}

\subsection{Details for GPT End-to-End Training}
\label{app:gpt_e2e}
We demonstrate the benefits of using Sparse-IFT transformations in the NLP
domain by pre-training GPT-3 models and performing zero-shot eval on downstream
tasks from the HuggingFace Open LLM leaderboard. Here, we pre-train the models
on the Pile~\cite{gao2020pile} dataset. To train all GPT models, we use the
AdamW optimizer \cite{loshchilov2017decoupled} with $\beta_1 = 0.9$, $\beta_2 =
0.999$ and $\epsilon = 10^{-8}$. The global norm is clipped at 1.0, and a weight
decay of 0.1 is used. There is a learning rate warmup over the first 375M
tokens, followed by a cosine decay to 10\% of the peak learning rate. We follow
the recently published Chinchilla \cite{hoffmann2022an} recommendations for
obtaining loss-optimal pre-trained baseline configurations of models. The
context window size is 2048 following \cite{brown2020language}.
Table~\ref{tab:app-gpt-pt} shows a detailed breakdown of the model
architectures, learning rate, and training settings. 

In Tables~\ref{tab:app-gpt-pt} and~\ref{tab:app-gpt-pt-sift}, we outline the
architecture configurations for Sparse Wide IFT 50\% and 75\% variants. We train
the Sparse Wide GPT-3 models using the dynamic sparse training algorithm,
SET~\citep{mocanu2018} on the Cerebras CS-2 to realize the acceleration from
unstructured sparsity. Currently, Cerebras CS-2’s specialized kernels support
training with dynamic unstructured sparsity via SET; therefore, results in this
section are reported with SET. In Tables~\ref{tab:app-gpt-pt}
and~\ref{tab:app-gpt-pt-sift}, $n_{params}$ is the total number of trainable
parameters, $n_{layers}$ is the number of decoder layers, and $d_{model}$ is the
base size of the model. The feedforward bottleneck is four times the base size,
i.e., $d_{\text{ff}} = 4\times d_{model}$. Finally, $n_{heads}$ is the number of
attention heads, and $d_{head}$ is the dimension of each attention head. 

\begin{table*}[]
    \caption{ Size, architecture, and learning hyperparameters (batch size and
        learning rate) of the GPT-3 Small model, which is trained using
        Chinchilla optimal configurations ($\approx$ 20 tokens per parameter) }
    \centering
    \resizebox{\textwidth}{!}{
    \begin{small}
    \begin{tabular}{c|ccccc|cc|c}
        \toprule
        Model & $n_{params}$ & $n_{layers}$ & $d_{model}$ & $n_{heads}$  &
        $d_{head}$ & Batch Size & Learning Rate & Training Tokens \\ \midrule
        GPT-3 Small           &  125M & 12 & 768 & 12 & 64 & 256 & 6$\times
        10^{-4}$ & 2.5B \\
        \bottomrule
    \end{tabular}
    \end{small}
    }
    \label{tab:app-gpt-pt}
\end{table*}

\begin{table*}[]
    \caption{ Sizes and architecture definitions of the dense GPT-3 Small model
        and its Sparse Wide IFT variants. } 
    \centering
    \begin{small}
    \begin{sc}
    \begin{tabular}{c|c|c|ccccc}
        \toprule
        Model & Transformation & Sparsity & $n_{layers}$ & $d_{model}$ &
        $d_{\text{ff}}$ & $n_{heads}$  & $d_{head}$  \\ \midrule GPT-3 Small &
        Dense   & 0\% & 12 & 768 & 3072 & 12 & 64 \\
        GPT-3 Small & Sparse Wide   & 50\% & 12 & 1092 & 4344 & 12 & 64 \\
        GPT-3 Small & Sparse Wide   & 75\% & 12 & 1536 & 6144 & 12 & 64 \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \label{tab:app-gpt-pt-sift}
\end{table*}

\paragraph{Evaluation} We conducted a comprehensive evaluation of both dense and
Sparse Wide IFT GPT-3 Small models, assessing their performance at 50\% and 75\%
sparsity levels across five distinct tasks on the Open LLM
leaderboard~\citep{open-llm-leaderboard} using the
LM-eval-harness~\citep{eval-harness}. The tasks encompassed
ARC~\citep{clark2018think}, HellaSwag~\citep{zellers2019hellaswag},
TruthfulQA~\citep{lin2022truthfulqa}, MMLU~\citep{hendrycks2021measuring}, and
Winogrande~\citep{DBLP:journals/corr/abs-1907-10641}. In
Table~\ref{tab:eleuther}, our results reveal that the Sparse IFT GPT-3 Small
model at 75\% sparsity achieved a notable 0.9\% improvement over the dense
baseline, underscoring the efficacy of Sparse Wide IFT in enhancing model
performance across a diverse range of language understanding tasks.

\begin{table}[!h]
    \caption{Performance Evaluation of Dense and Sparse Wide IFT GPT-3 Small
    Models at 50\% and 75\% sparsity levels across five tasks (i.e., ARC,
    HellaSwag, TruthfulQA, MMLU, and Winogrande) on the Open LLM Leaderboard}
    \begin{center}
    \begin{small} 
    \begin{sc}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|c|c|cccccc}
        \toprule
    \multirow{2}{*}{Model} & \multirow{2}{*}{Transformation} &
    \multirow{2}{*}{Sparsity} & \multirow{2}{*}{Sparse Method} &
    \multicolumn{6}{c}{Open LLM Leaderboard}                    \\
                           &                                 & & & ARC  &
                           HellaSwag & TruthfulQA & MMLU & Winogrande & Average
                           \\ \midrule \multirow{3}{*}{GPT-3 Small} & Dense &
                           0\% &                -                & 20.8 & 27.2 &
                           47.0         & 24.6 & 49.4       & 33.8    \\
                           & Sparse Wide                     & 50\% & SET & 20.6
                           & 27.4      & 47.4      & 25.6 & 49.6 & 34.1    \\
                           & Sparse Wide                     & 75\% & SET &
                           \textbf{22.1} & \textbf{27.8}      & \textbf{47.5} &
                           \textbf{25.6} & \textbf{50.4}       & \textbf{34.7}
                           \\ \bottomrule
    \end{tabular}
    }
    \end{sc}
    \end{small}
    \end{center}
    \label{tab:eleuther}
\end{table}

\section{Benchmarking Efficiency w.r.t Wall-clock \label{app:benchmark}} In this
section we provide additional details on the benchmarking setups for inference
on Neural Magic
DeepSparse~\citep{neural_magic_2021,neuralmagic2021,pmlrv119kurtz20a}
sparsity-aware runtime and training on the Cerebras
CS-2~\citep{cerebrasHarnessingPower,lie_2023} for evaluating the efficiency of
Sparse-IFT with respect to the wall-clock time. 

\paragraph{Inference Setup}
We use Neural Magic's DeepSparse tool for benchmarking Sparse-IFT variants. The
benchmarking is conducted on G4dn instances available on the AWS cloud. These
instances support the AVX-512 instruction set, which is used by the DeepSparse
inference runtime to accelerate unstructured sparsity. We benchmark different
configurations of the Sparse Wide ResNet-18 model with sparsity $\in$ \{50\%,
75\%, 90\%\} for batched inference on ImageNet. We report runtime for
batch-inference of 64 images at 224 $\times$ 224 resolution.

\paragraph{Training Setup}
We evaluate the training efficiency of Sparse-IFT on the Cerebras CS-2 which
supports and accelerates training with unstructured sparsity (both forward and
backward passes). We benchmark the training speed measured in seconds/iteration.
Note that the overall FLOPs of models in the GPT family are comprised of matrix
multiplication FLOPs and attention FLOPs. Attention FLOPs (i.e., spent in
multi-head attention) scale quadratically with sequence length and are invariant
to weight sparsity. To demonstrate the efficacy of sparse kernels for
unstructured weight sparsity, we report our results for dense and Sparse Wide
variants of the GPT-3 20B model with a sequence length of 256 and batch size of
256. We benchmark different configurations of Sparse Wide GPT-3 20B with
sparsity $\in$ \{50\%, 75\%, 90\%\} and report seconds/ iteration. 

\paragraph{Benchmarking Analysis} Figure~\ref{fig:sparsity_benchmark} in
Section~\ref{sec:wall_clock} presents the results of benchmarking inference and
training of Sparse-IFT Sparse Wide family. In both setups, we measure the
relative increase in latency or training speed for Sparse-IFT variants against
the dense model. Note that configurations of Sparse-IFT at different values of
sparsity do not incur a significant change in the FLOPs compared to the dense
model. On ideal hardware, FLOPs should translate directly to wall clock time,
and hence, the inference latency or training time for all configurations of
Sparse-IFT should be the same as that of the dense model (dotted black line).
Conversely, when hardware does not support unstructured sparsity, the latency or
training time of Sparse-IFT variants increases with sparsity (blue line). 

The results in Figure~\ref{fig:sparsity_benchmark} of
Section~\ref{sec:wall_clock} show that up 75\%, there is minimal computational
overhead compared to training the original dense baseline model. At 90\%
sparsity, our results lie between these two spectrums (green line). Using Neural
Magic's sparse inference runtime, we observe a significant reduction in
inference latency, bringing down the relative increase in latency from 19.5x to
3.5x. Similiarly, in the case of training on the Cerebras CS-2, we observe a
significant reduction in training-time, bringing down the relative increase from
10.6x to 2.8x.

In Figure~\ref{fig:matmulspeedup}, we illustrate the achievable benefits of
unstructured weight sparsity when utilizing specialized hardware designed for
deep learning, such as the Cerebras CS-2. This figure was regenerated based on
the plot in~\citep{lie_2021}.

\begin{figure*}[!ht]
    \centering
    \includegraphics[keepaspectratio=true, width=0.45\linewidth]{./figures/gpt_speedup.pdf}
    \caption{Measured speedup versus theoretical speedup at varying sparsity
    levels for a GPT-3 layer 12k $\times$ 12k matrix multiplication
    (MatMul)~\citep{lie_2021}.}
    \label{fig:matmulspeedup}
\end{figure*}

\section{Broader Impact and Ethical Considerations}

The landscape of machine learning (ML) has witnessed an exponential growth in
models, particularly in domains such as natural language processing and computer
vision. However, this surge in model size has come with a considerable cost in
terms of compute, memory, and energy requirements. Our approach, Sparse Iso-FLOP
Transformations (Sparse-IFT), represents a significant stride toward mitigating
these resource-intensive demands.

Sparse-IFT introduces a novel approach that enhances the efficiency of training
large neural networks. Remarkably, it achieves improved accuracy while
maintaining the same FLOPs as the original dense baseline model. Our method
holds promise for positive environmental impacts, given the substantial
computational resources typically associated with training large neural
networks.

The potential sustainability contribution lies in the fact that, as sparse ML
software and hardware co-design continues to evolve, we may be able to train
more accurate "larger sparse" networks within the confines of the same
computational budget as a smaller dense model. This paradigm shift could usher
in a more environmentally conscious approach to deep learning, addressing the
concerns associated with the escalating resource requirements of ever-expanding
models.

The key lies in achieving sparsity acceleration, which necessitates a harmonious
collaboration between hardware support for sparsity and the development of
sparse ML techniques. The seamless integration of these elements is crucial for
realizing the full potential of Sparse-IFT and similar innovations. This
holistic co-design approach ensures that hardware architectures are optimized to
complement sparse techniques, fostering a sustainable and efficient trajectory
for the future of deep learning.

As we continue to explore the intersection of hardware support for sparsity and
the evolution of sparse ML techniques, our benchmarking analysis in
Section~\ref{sec:wall_clock} and Appendix~\ref{app:benchmark} serve as a
practical illustration of the transformative potential of Sparse-IFT. It not
only substantiates the theoretical promises but also offers a roadmap for future
developments in the pursuit of sustainable and efficient deep learning
practices.

\section{Author Contributions} \label{app:authorcontrib}

We provide a summary of each author’s contributions:

\begin{itemize} 
    \item Vithursan Thangarasa was an integral part of the project
    by participating in discussions with Shreyas Saxena and contributing to
    the method. He also implemented all Sparse-IFT transformations in PyTorch,
    proposed using non-linearity in Sparse-IFT, conducted experiments for the
    entire study on CIFAR-100 and its ablations, obtained initial results on
    ImageNet, extended Sparse-IFT to efficient architectures (e.g., BotNet,
    MobileViT), conducted the entire study with GPT on Cerebras CS-2, and
    contributed to writing parts of the manuscript.

    \item Shreyas Saxena conceived the key idea of matching the FLOPs of
    Sparse Wide transformation to a compact dense model, extended the idea to
    other members of the Sparse-IFT family, helped with the implementation,
    established cardinality of Sparse-IFT members to explain the results,
    conducted experiments for BERT, benchmarked Sparse-IFT for inference, and
    wrote majority of the manuscript.

    \item Abhay Gupta validated sparse optimizers in PyTorch, conducted
    experiments with Sparse-IFT ResNet variants on ImageNet, obtained results
    with MobileNetV2 architecture, helped with pre-training of Sparse-IFT
    variants of GPT on Cerebras CS-2, conducted all experiments of Sparse-IFT
    on downstream CV tasks, and contributed to writing parts of the
    manuscript.
    
    \item Sean helped with the bring-up of sparsity support on Cerebras CS-2
    which was crucial for benchmarking and training Sparse-IFT variants of GPT
    models, and provided feedback to improve the structuring and presentation
    of the manuscript.
\end{itemize}
