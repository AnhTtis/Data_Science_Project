\begin{table}
    \caption{Sparse Wide IFT with ResNet-18 trained using various sparse
training methods on CIFAR-100 across different sparsity levels (columns). Best
accuracy for each sparse training method is highlighted in bold. } 
    \centering
    \begin{small}
    \begin{sc}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{ccccc}
            \hline
            \multicolumn{1}{c|}{Dense}                           & Sparse Method
            & 0.50                                     & 0.75           & 0.90
            \\ \hline
            \multicolumn{1}{c|}{\multirow{6}{*}{77.0 $\pm$ 0.2}} & Static &
            \textbf{78.5 $\pm$ 0.3} & 78.3 $\pm$ 0.1 & 78.2 $\pm$ 0.3 \\
            \multicolumn{1}{c|}{}                                & SNIP &
            \textbf{77.8 $\pm$ 0.3}                  & 77.0 $\pm$ 0.2 & 75.8
            $\pm$ 0.2                           \\
            \multicolumn{1}{c|}{}                                & GraSP &
            \textbf{77.7 $\pm$ 0.3}                  & 76.5 $\pm$ 0.3 & 76.5
            $\pm$ 0.3                           \\
            \multicolumn{1}{c|}{}                                & FORCE &
            \textbf{77.2 $\pm$ 0.3}                  & 76.9 $\pm$ 0.3 & 75.4
            $\pm$ 0.4                           \\
            \multicolumn{1}{c|}{}                                & SET & 78.8
            $\pm$ 0.1                           & 79.2 $\pm$ 0.2 & \textbf{79.8
            $\pm$ 0.2} \\
            \multicolumn{1}{c|}{}                                & RigL & 79.1
            $\pm$ 0.2                           & 79.5 $\pm$ 0.1 & \textbf{80.1
            $\pm$ 0.2} \\  
            \multicolumn{1}{c|}{}                                 & GraNet &
            79.2 $\pm$ 0.2                           & 79.6 $\pm$ 0.2 &
            \textbf{80.0 $\pm$ 0.2}  \\ \bottomrule
            \end{tabular}
    }
    \end{sc}
    \end{small}
\label{tab:dynamic_sparsity_importance}
\end{table}

\section{Sparse-IFT and Dynamic Sparse Training~\label{sec:dst}} This section
explores Sparse-IFT networks trained with DST, showcasing DST's effectiveness in
navigating larger parameter spaces~\citep{huang2023dynamic,tai2022spartan}. We
empirically establish DST's consistent superiority over static sparse training
and then examine the behavior of DST-trained Sparse-IFT models through the lens
of Ramanujan graphs. All experiments use the ResNet-18 architecture on CIFAR-100
with published settings~\citep{devries2017improved}. Model details and
hyperparameters are in Appendix \ref{app:pt-implement}, and results are averaged
over 3 seeds.

\paragraph{Importance of DST} 
Sparse-IFTs employ unstructured sparsity in its transformations. This study
investigates the impact of sparse training methods on various Sparse-IFT
configurations, focusing on Sparse Wide IFT with sparsity $\in \{50\%, 75\%,
90\%\}$. We evaluate: random static sparsity, SNIP~\citep{lee2018snip},
GraSP~\citep{wang2020picking}, FORCE\citep{de2020progressive},
SET~\cite{mocanu2018}, RigL~\cite{evci2020rigging} and
GraNet~\citep{liu2021sparse}. SET, RigL, and GraNet are DST methods, with SET
updating the mask randomly, RigL updating it with gradient information and
GraNet incorporating gradual magnitude pruning~\citep{Zhu2017ToPO} with RigL.
Pruning at Initialization (PaI) methods (e.g., SNIP, GraSP, FORCE) and GraNet
increase training FLOPs due to non-uniform sparsity and dense-to-sparse
training. We address this by adjusting target sparsity levels to align
Sparse-IFT training FLOPs with the dense baseline (see
Appendix~\ref{app:control_flops}). Results in
Table~\ref{tab:dynamic_sparsity_importance} indicate that DST methods
consistently outperform static sparsity, with improvements persisting at higher
sparsity levels. However, in Iso-FLOP scenarios, PaI methods underperform,
likely due to disruptive sparsity imbalances affecting information propagation.
Sparse-IFTs expand the sparse mask-weight space $\propto$ sparsity, benefiting
DST methods in thorough exploration and exploitation within this space. RigL is
chosen as the sparse training method for simplicity in all experiments, despite
GraNet performing the best.

\begin{figure}
    \centering
    \subfigure{\includegraphics[width=0.23\textwidth]{figures/resnet18_sw_50_iter.pdf}}
    \subfigure{\includegraphics[width=0.23\textwidth]{figures/resnet18_sp_50_iter.pdf}}
    \\ 
    \subfigure{\includegraphics[width=0.23\textwidth]{figures/resnet18_sw_50_std.pdf}}
    \subfigure{\includegraphics[width=0.23\textwidth]{figures/resnet18_sp_50_std.pdf}}
    \caption{The relationship between the structure and weights of Sparse-IFT
    ResNet-18 networks are analyzed through a graph perspective in terms of
    performance. Top row: we assess the relationship between $\Delta r_{imdb}$
    and $\lambda_{imsg}$. Bottom row: investigates the correlation between
    $\Delta r$ and $\lambda$. The Pareto curvature heatmap visually represents
    the classification performance, with varying color gradients symbolizing the
    spectrum from low to high test accuracy on CIFAR-100.}
    \label{fig:graph_analysis}
\end{figure}
\begin{figure*}[]
    \centering
    \subfigure{\includegraphics[width=0.33\textwidth]{figures/resnet18_us_vs_s_cifar100.pdf}}
    \subfigure{\includegraphics[width=0.33\textwidth]{figures/resnet18_sift_cifar100.pdf}}
    \subfigure{\includegraphics[width=0.33\textwidth]{figures/resnet18_dense_vs_sift_cifar100.pdf}}
    \caption{Ablation studies with Sparse-IFT on the ResNet-18 model for
    CIFAR-100 across sparsity $\in \{50\%, 75\%, 90\%\}$. (left) Sparse Wide IFT
    trained with dynamic unstructured and structured sparsity. (middle)
    Sparse-IFT family members trained with RigL, where Sparse Wide performs the
    best. (right) Sparse Wide IFT trained in a sparse and dense manner.}
    \label{fig:cifar100_ablations}
\end{figure*}

\subsection{Spectral Analysis of Sparse-IFT trained with DST}
We explore the benefits of Sparse-IFT networks using DST by analyzing the
Ramanujan Gap and Spectral Gap characteristics. Ramanujan graph structures which
are known to exhibit sparsity and high connectivity like expander graphs, are
investigated to reveal their correlation with the final performance of sparse
networks. Our analysis evaluates the impact of model parameters and graph
connectivity on the effectiveness of DNNs with Sparse-IFTs, aiming to provide
insights into the training dynamics of Sparse-IFT models. Inspired
by~\citet{hoang2023revisiting, hoang2023dont}, in this analysis, we interpret
the ResNet-18 model as a series of bipartite compute graphs, where each layer,
$\{\theta_1,\ldots, \theta_L\}$ in an $L$ layered sparse DNN, takes the form of
a square adjacency matrix $A$. \citet{hoang2023revisiting} proposed several
graph metrics inspired by Ramanujan properties for characterizing sparse
networks, via: 1) \textbf{Ramanujan Gap}: $\Delta r = 2 \ast \sqrt{d-1} -
\hat{\mu}(A)$, and $\Delta r_{imdb} = \frac{1}{\abs{K}}
\sum_{i=1}^{\abs{K}}(2\sqrt{d_i - 1} - \hat{\mu}(A_{K_i}))$, where $d$ is the
average edge per node, and $\hat{\mu}(A)$ is the non-trivial eigenvalue of $A$.
Here, $\Delta r$ is the conventional view of measuring gap between Ramanujan's
upper bound $2 \ast \sqrt{d-1}$ and $\hat{\mu}(A)$. $\Delta r$ measures the
network's degree of connectivity to reveal the flow of information propagation.
$\Delta r_{imdb}$~\citep{hoang2023revisiting}, the Iterative Mean Difference
Bound (imdb), evaluates the average connectivity boundary across all subgraphs
$K$ within $A$. A higher $\Delta r$ in sparse networks signifies efficient
information flow, gradient propagation, and a well-separated spectrum in the
adjacency matrix of sparse weights; indicating robust and efficient
representation. In addition, an increasing $\Delta r_{imdb}$ indicates more
extensive connectivity boundaries within subgraphs, enhancing communication
among nodes and fostering stronger connections. 2)~\textbf{Weighted Spectral
Gap}: $\lambda = \mu_0(\abs{\vb*{W}}) - \hat{\mu}(\abs{\vb*{W}})$, and
$\lambda_{imsg} =
\frac{1}{\abs{K}}\sum_{i=1}^{\abs{K}}(\mu_{0}(\abs{\vb*{W}_{K_i}}) -
\hat{\mu}(\abs{\vb*{W}_{K_i}}))$. Here, the gap between $\mu_0$, the trivial
parameters, and $\hat{\mu}$, the non-trivial eigenvalues of $\vb*{W}$, the
weighted adjacency matrix, is denoted as $\lambda$, the weighted spectral gap.
Then, $\lambda_{imsg}$~\citep{hoang2023dont} is the iterative version which
takes into account all subgraphs $K$ within $\vb*{W}$. A higher $\lambda_{imsg}$
indicates enhanced spectral separation between $\mu_0$ and $\hat{\mu}$ of
$\vb*{W}$, implying a more distinct and well-defined spectral structure within
subgraphs. This improved separation in the spectrum, represented by a higher
$\lambda$, facilitates better isolation of meaningful signals. We train Sparse
Wide and Sparse Parallel ResNet-18 models at 50\% sparsity on CIFAR-100. Then,
we computed the metrics mentioned above using the official
code~\footnote{\href{https://github.com/VITA-Group/ramanujan-on-pai}{https://github.com/VITA-Group/ramanujan-on-pai}}
from~\citet{hoang2023revisiting}. Last, we generated a Pareto curvature heatmap,
considering weight magnitudes and graph topological structure details (see
Figure~\ref{fig:graph_analysis}). See Appendix~\ref{app:graph_analysis_sift_dst}
for a detailed analysis.

\paragraph{$\vb*{\Delta r_{imdb}}$ and $\vb*{\lambda_{imsg}}$ Analysis:} In the
initial to middle stages of training, RigL's dynamic pruning and regrowth
increases $\Delta r_{imdb}$ for the network to \textit{explore} diverse
connectivity patterns (see top row, Figure~\ref{fig:graph_analysis}). Subsequent
pruning removes less critical connections, diversifying subgraphs in the
adjacency matrix $A$. Later stages witness a decrease in $\Delta r_{imdb}$ as
the network converges to more focused and organized connectivity patterns. RigL
prioritizes crucial connections, \textit{exploiting} an efficient subgraph
structure linked with highly accuracy regions of Sparse-IFT models. Early in
training, increasing $\lambda_{imsg}$ suggests successful isolation of different
modes. Pruning leads to a distinct separation between dominant and less dominant
modes. The subsequent $\lambda_{imsg}$ decrease signals the network's
convergence to a more specialized representation, emphasizing key spectral
components and diminishing the influence of less critical modes. While both
Sparse Wide and Sparse Parallel IFTs show increasing $\Delta r_{imdb}$, the
larger search space cardinality in Sparse Wide facilitates the emergence of
diverse subgraph structures within each layer, allowing for a richer set of
connections between nodes; resulting in a higher maximum $\Delta r_{imdb}$.
Similarly, Sparse Wide has a higher maximum $\lambda_{imsg}$ compared to Sparse
Parallel, indicating the emergence of subgraphs with more distinct spectral
properties.

\textbf{$\vb*{\Delta r}$ and $\vb*{\lambda}$ Analysis:}
Figure~\ref{fig:graph_analysis}'s bottom row reveals a strong correlation
between $\Delta r$ and $\lambda$. $\Delta r$ initially decreases, indicating a
temporary relaxation of spectral constraints during dynamic pruning with RigL.
Subsequently, it maximizes in the final training stages, signifying RigL's
ability to guide the network to reorganize its connectivity, promoting more
structured and favorable spectral characteristics. Similarly, $\lambda$ follows
a pattern of initial decrease and later maximization. This implies that RigL's
dynamic sparsity initially results in less optimal weight organization
concerning spectral properties. However, RigL's iterative pruning and rewiring
dynamically adapts the network, aligning weights to enhance spectral
characteristics and increase the spectral gap.  Our analysis demonstrates that
DST, as exemplified by RigL, outperforms static sparse training by optimizing
spectral characteristics for Sparse-IFT; fostering improved connectivity
patterns and a more favorable spectral profile.