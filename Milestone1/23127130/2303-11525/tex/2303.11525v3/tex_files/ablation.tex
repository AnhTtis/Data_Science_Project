\section{Sparse-IFT Architecture Ablation Studies} 
\label{sec:sift_ablations}
This section delves into design considerations for Sparse-IFT by first,
exploring the role of non-linearities in enhancing representational capacity.
Then, the advantage of training with dynamic unstructured sparsity over
structured is investigated. Next, we compare between densely and sparsely
trained Sparse-IFT models. Finally, by applying top-performing Sparse-IFTs to
efficient vision models, these insights contribute to a synthesized framework.

\subsection{Importance of Using Non-Linear Activations}
For some of the Sparse-IFT members,, we draw inspiration from linear
overparameterization methods, which fold the feedforward function into a dense
matrix post-training~\cite{ding2021repvgg, ding2021diverse, guo2020expandnets,
ding2019acnet}. Our method enhances feedforward function capacity through an
Iso-FLOP transformation without increasing training FLOPs. Maintaining original
dense FLOP levels eliminates the need for post-training modifications, enabling
efficient inference and incorporation of non-linearities like ReLU in
Sparse-IFT. Experiments on ResNet-18 on CIFAR-100 show notable accuracy gains
across all sparsity levels with non-linear activations.
For example, at 90\% sparsity, using non-linearities in Sparse Factorized IFT
yields a 1.8\% accuracy increase over the dense baseline, in contrast to a 0.5\%
decrease without non-linearities. These findings extend to all Sparse-IFT
members (see Appendix~\ref{app:nonlinear-importance} for details).

\subsection{Unstructured vs. Structured Sparsity}
We compare dynamic unstructured and structured sparsity using Sparse-IFT.
Unstructured sparsity explores all mask variations but is less
hardware-compatible. Prior works have investigated structured sparsity, such as
low-rank and block-sparse matrices, for wall-clock
speed-ups~\cite{khodak2020initialization, chen2021drone, hubara2021accel,
dao2022monarch}. We explore structured sparsity through Iso-FLOP configurations
with Sparse Wide IFT, employing low-rank factorization and N:M sparsity for GPU
acceleration. In Figure~\ref{fig:cifar100_ablations}, we compare dynamic
unstructured sparsity with N:M transposable structured
sparsity~\citep{hubara2021accel} using Sparse-IFT. The latter demonstrates
improvements over the dense baseline, with unstructured sparsity yielding the
highest gains due to reduced mask diversity in block-sparse
matrices~\citep{hubara2021accel}. Results also indicate that N:M block sparsity
outperforms low-rank factorization (see Appendix~\ref{app:structured_sparsity}).

\subsection{Sparse-IFT ResNet-18} 
We assess all Sparse-IFT family members with ResNet-18 on CIFAR-100 across
different sparsity levels. The middle plot of
Figure~\ref{fig:cifar100_ablations}, highlights the best accuracy achieved by
each Sparse-IFT member. All members exhibit substantial accuracy improvements
compared to the dense baseline (77\%), using the same FLOPs. Sparse Wide
consistently performs the best, while Sparse Doped is the only member not
gaining accuracy at higher sparsity. This is attributed to limited search space
growth and a decrease in active weights in the unstructured matrix. In
Appendix~\ref{app:sift_vs_extendedsparse}, we compare Sparse-IFT against other
DST baselines under the same training efficiency setup by extending the training
steps, showing Sparse-IFT outperforms them significantly at $s \in \{50\%, 75\%,
90\%\}$.

\subsection{Sparse-IFT vs. Dense Overparametrization}
Sparse-IFT members excel in exploring a large search space with DST, achieving
accuracy comparable to their dense counterparts. Despite dense training
incurring higher FLOPs than the baseline, it establishes an upper bound on the
accuracy of Sparse-IFT models. In Figure~\ref{fig:cifar100_ablations}, the right
plot compares the sparse and dense versions of Sparse Wide IFT, the
top-performing member. Both approaches achieve similar accuracy across sparsity
levels, showcasing efficient exploration and exploitation of the larger
parameter space without the compute cost of dense training. For instance, at
90\% sparsity, dense runs require 10x more FLOPs than sparse runs.

\subsection{Efficient Architectures}
\label{subsec:efficient_archs}
To assess Sparse-IFT's robustness across diverse set of models, we evaluate it
on architectures optimized for efficient inference
(MobileNetV2~\citep{sandler2018mobilenetv2} and
MobileViT~\citep{mehta2021mobilevit}) and efficient training
(BotNet~\citep{srinivas2021bottleneck}). Applying Sparse Wide IFT to dense
layers significantly improves test accuracy across all architectures (refer to
Table~\ref{tab:mbv2-cifar}). Similarly, utilizing the Sparse Parallel IFT
consistently enhances performance across all architectures (see
Appendix~\ref{app:efficientcv}). We evaluate the best-performing model,
BotNet-50, on ImageNet (see Section~\ref{subsec:imagenet}), and provide
additional experimental setup details in Appendix~\ref{app:pt-implement}.
