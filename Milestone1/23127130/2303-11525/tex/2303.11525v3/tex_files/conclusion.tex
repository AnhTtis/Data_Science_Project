\section{Conclusion}
We introduced Sparse-IFT as a drop-in replacement for dense layers in DNNs,
enhancing test accuracy~w.r.t training FLOPs by increasing representational
capacity through sparsity. The expanded weight space enables effective
exploration and exploitation by DST algorithms, facilitating the discovery of
optimal sparse subnetworks. Our spectral analysis of Sparse-IFT models trained
with DST reveals efficient connectivity and information propagation, correlated
with high-performance networks. Notably, Sparse-IFT consistently outperforms
dense models in vision and NLP domains. Despite current hardware limitations,
promising benchmarks on Cerebras CS-2 and Neural Magic DeepSparse runtime
highlight the need for improved support for unstructured weight sparsity. We
hope our findings encourage the community to explore unstructured sparsity for
improved model efficiency and performance across applications.