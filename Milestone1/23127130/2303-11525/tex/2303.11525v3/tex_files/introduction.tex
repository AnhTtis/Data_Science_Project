\section{Introduction}

\begin{figure}[ht]
    \begin{center}
    \centerline{\includegraphics[width=1.0\columnwidth]{./figures/SIFT_ResNet_improvement_vithu2.pdf}}
    \caption{Top-1 Accuracy vs. Training FLOPs for variants of ResNet on
    ImageNet. Sparse-IFT provides significant accuracy gains across different
    models and sparsity levels while using the same FLOP budget as its dense
    counterpart.}
    \label{fig:sift_resnet_improvement}
    \end{center}
    \vspace{-10pt}
\end{figure}

Increases in model size and training data have led to many breakthroughs in deep
learning (e.g., AlexNet~\citep{krizhevsky2012imagenet},
ResNet~\citep{he2016identity}, Transformers~\citep{vaswani2017attention},
GPT~\citep{radford2018improving, radford2019language},
AlphaGo~\citep{silver2017mastering}, etc.). Consequently, computational and
memory demands for training and deploying deep neural networks (DNNs) have
surged dramatically. To enable the deployment of large models, multiple
techniques (e.g., distillation~\citep{hinton2015distilling},
quantization~\citep{han2015deep}, pruning~\citep{han2015learning}) have been
introduced to reduce inference FLOPs and memory requirements. While these
techniques improve inference efficiency (test accuracy w.r.t inference FLOPs),
the associated training costs are still prohibitive. Our work focuses on
improving the training efficiency (test-accuracy w.r.t training FLOPs) of DNNs
through weight sparsity. In recent years, we have witnessed progress in using
weight sparsity to reduce training FLOPs~\citep{evci2020rigging, liu2021sparse,
jayakumar2020top}. \citet{frankle2018lottery} show that sparse subnetworks
("lottery tickets") exist at initialization and can be trained to match dense
network accuracy. Dynamic sparse training (DST) methods~\citep{ma2022effective,
evci2020rigging, liu2021selfish, jayakumar2020top} iteratively adjust sparsity
patterns to facilitate the discovery of optimal sparse subnetworks within a
single training run. However, they often lag behind dense baselines or require
longer training schedules (e.g., 2-5x training steps) to close the
gap~\citep{yuan2021mest, tai2022spartan, liu2021sparse}. Our unique contribution
focuses on using sparsity to improve a given dense model's accuracy. We
introduce the Sparse Iso-FLOP Transformations (Sparse-IFT), a family of
techniques serving as drop-in replacements for dense layers in DNNs.

Sparse-IFTs increase layer representational capacity, facilitating the discovery
of optimal sparse subnetworks while maintaining constant FLOPs (i.e., Iso-FLOP).
For example, widening a layer with maintained sparsity increases dimensionality
without impacting FLOPs; expanding the sparse mask-weight space for more diverse
configurations. This enables DST methods to navigate the solution space
effectively, potentially finding improved sparse subnetworks for higher
accuracy. Drawing inspiration from prior works~\citep{hoang2023revisiting,
hoang2023dont}, we analyze the connectivity of Sparse-IFT models as Ramanujan
graphs and their impact on performance when trained with DST. All Sparse-IFTs
are parameterized by a single hyperparameter, the sparsity level.
Figure~\ref{fig:sift_resnet_improvement} summarizes ImageNet performance,
showing significant accuracy gains with Sparse Wide IFT ResNet variants. Sparse
Wide ResNet-18 achieves +3.5\% top-1 accuracy at 90\% sparsity, surpassing a
dense ResNet-34 (74.2\%) with 2x fewer FLOPs. These gains result from replacing
dense layers with Sparse-IFTs, requiring no changes to training hyperparameters.
The main contributions of our work are:

\begin{enumerate}%[noitemsep]
    \item We introduce Sparse Iso-FLOP Transformations (Sparse-IFTs), a family
    of techniques aimed at enhancing DNN training efficiency. These
    transformations boost accuracy while maintaining a constant FLOP count.
    Sparse-IFTs are parameterized by a \textit{single hyperparameter, sparsity
    level}, and can be seamlessly used as drop-in replacements for dense layers.

    \item We empirically validate the consistent advantage of DST over static
    sparse training for Sparse-IFT networks. Our investigation into the dynamic
    evolution of sparse topologies in DST via Ramanujan graph spectral analysis
    highlights optimized connectivity patterns and improved spectral
    characteristics.

    \item We show consistent benefits of Sparse-IFT across computer vision and
    natural language processing domains. Sparse-IFT enhances ResNet-18 and
    ResNet-34 top-1 accuracy on ImageNet by 3.5\% and 2.6\%, respectively.
    Fine-tuning for object detection (MS COCO) and segmentation (CityScapes)
    yields improvements of 5.2\% mAP and 2.4\% mIoU. Sparse-IFT with GPT-3
    results in a 0.9\% improvement on the Open LLM leaderboard.

    \item We showcase the practical value of Sparse-IFT with real-world timings
    for training on the Cerebras CS-2~\citep{cerebrasHarnessingPower, lie_2023}
    and inference with Neural Magic DeepSparse~\citep{neural_magic_2021} using
    unstructured sparsity. Despite being 2x wider at 75\% sparsity with Sparse
    Wide IFT, we observe minimal compute overhead on both platforms compared to
    GPUs.

\end{enumerate}
