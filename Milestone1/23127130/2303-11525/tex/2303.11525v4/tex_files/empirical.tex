\section{Empirical Evaluation}\label{sec:empirical_res} Building on insights
gained from our ablations discussed in Section~\ref{sec:sift_ablations}, we
apply Sparse-IFTs to ImageNet, also demonstrating its advantages for transfer
learning in various computer vision tasks. Additionally, we highlight the
benefits of Sparse-IFT in the domain of NLP by presenting results on
pre-training GPT~\citep{brown2020language}.
\vspace{-5pt}
\subsection{ImageNet and Transfer Learning}
\label{subsec:imagenet}
We apply the best-performing Sparse-IFT transformations (Sparse Wide IFT and
Sparse Parallel IFT) from CIFAR-100 to ImageNet using ResNet-18. We follow
published training settings for ImageNet~\citep{nvidia2023gpuperf}. Both
Sparse-IFT families achieve significantly higher accuracy compared to the dense
baseline (see Table~\ref{tab:resnet-i1k}). Specifically, Sparse Wide IFT
ResNet-18 at 90\% sparsity improves over the dense baseline by 3.5\% and matches
the accuracy of a dense ResNet-34 with 2Ã— fewer training FLOPs (refer to
Figure~\ref{fig:sift_resnet_improvement}). We also apply the best-performing
transformation (Sparse Wide IFT) to ResNet-34 and BotNet-50. Increasing sparsity
consistently improves accuracy, indicating enhanced training efficiency at
higher sparsities. On BotNet-50, a hybrid ViT model, there is a 1.1\%
improvement at 90\% sparsity.
\vspace{-10pt}
\paragraph{Transfer Learning on Downstream}
\label{subsec:transfer_learning}
To show the effectiveness of pre-training our Sparse-IFT classification
backbones, we evaluate them on 1) object detection on MS COCO
2017~\cite{lin2014microsoft}, and 2) semantic segmentation on
CityScapes~\cite{cordts2016cityscapes}. For object detection, we adopt
RetinaNet~\cite{lin2017focal} from the MMDetection open-source
toolbox~\cite{mmdetection} and report results in the standardized training
setting. For semantic segmentation, we utilize DeepLabV3+~\cite{chen2018encoder}
in the MMSegmenation open-source toolbox~\cite{mmseg2020}. We evaluate ResNet-18
with Sparse Wide IFT and to ensure FLOP-equivalent comparisons with the dense
backbone, the Sparse-IFT backbones remain sparse during fine-tuning.
Appendix~\ref{app:eval_downstream} provides more details on the training setup.
We summarize our findings in Table \ref{tab:down_stream}, where using Sparse
Wide IFT ResNet-18 backbone leads to significant accuracy gains across all
metrics on both tasks.

\begin{table}
    \caption{Sparse-IFT on ImageNet. Best result for each transformation and
architecture is highlighted in bold.}
    \vspace{-5pt}
    \begin{center}
    \begin{small}
    \begin{sc}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|c|ccc}
        \toprule
        Model & Dense & Transformation & 0.50 & 0.75 & 0.90 \\
\midrule
        \multirow{2}{*}{ResNet-18} &  \multirow{2}{*}{70.9 $\pm$ 0.1} & Sparse
Wide & 72.7 $\pm$ 0.1 & 73.8 $\pm$ 0.2 & \textbf{74.4 $\pm$ 0.2} \\
        &   & Sparse Parallel & 72.7 $\pm$ 0.2 & 73.2 $\pm$ 0.2 & \textbf{74.0
        $\pm$ 0.2} \\ \midrule ResNet-34 & 74.2 $\pm$ 0.1 & Sparse Wide &  75.6
        $\pm$ 0.2 & 76.4 $\pm$ 0.1 & \textbf{76.8 $\pm$ 0.3} \\ \midrule
        BotNet-50 & 77.5 $\pm$ 0.1 &  Sparse Wide & 77.9 $\pm$ 0.2 & 78.3 $\pm$
        0.2 & \textbf{78.6 $\pm$ 0.3} \\
        \bottomrule
    \end{tabular}
    }
    \end{sc}
    \end{small}
\end{center}
\vspace{-5pt}
\label{tab:resnet-i1k}
\end{table}

\begin{table}
    \caption{Sparse Wide IFT variants of ResNet-18 as backbones for: (a) object
detection on MS COCO, (b) semantic segmentation on Cityscapes.} 
    \vspace{-5pt}
    \centering
    \begin{small}
    \begin{sc}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|cccc}
        \toprule
      & Metric & Dense & 0.50   & 0.75  & 0.90 \\
    \midrule
 \multirow{3}{*}{MS COCO}     &  AP      &  29.3 $\pm$ 0.1  & 31.3 $\pm$ 0.1 &
 32.8  $\pm$ 0.2  & \textbf{34.5 $\pm$ 0.2}   
 \\
      &  AP$_{50}$    &  46.2 $\pm$ 0.2  & 49.0 $\pm$ 0.2 & 51.0 $\pm$ 0.2 &
      \textbf{53.5 $\pm$ 0.2}   \\
      &  AP$_{75}$    &  30.9 $\pm$ 0.2  & 33.0 $\pm$ 0.2 & 34.8 $\pm$ 0.2 &
      \textbf{36.5 $\pm$ 0.3}   \\
    \midrule
  \multirow{2}{*}{CityScapes}     &  \text{mIoU}      &   76.7 $\pm$ 0.2   &
  77.9 $\pm$ 0.2    & 78.9 $\pm$ 0.2  & \textbf{79.1 $\pm$ 0.2}    \\
      &  \text{mAcc}      &   84.4  $\pm$ 0.2   &  85.1 $\pm$ 0.2    & 85.7
      $\pm$ 0.2  & \textbf{86.0 $\pm$ 0.2}   \\
        \bottomrule
    \end{tabular}
    }
    \end{sc}
    \end{small}
\label{tab:down_stream}
\vspace{-5pt}
\end{table}

\vspace{-5pt}
\subsection{Language Modeling}
\label{subsec:nlp_results} 
We pre-train the Sparse Wide IFT GPT-3 Small model at $s \in \{50\%, 75\%\}$
from scratch on the Pile~\citep{gao2020pile} dataset using
SET~\citep{mocanu2018}, and compare against the standard dense model. All models
were trained on the Cerebras CS-2~\citep{lie_2023} following
Chinchilla~\citep{hoffmann2022an} for obtaining loss-optimal pre-trained
baseline configurations of models. We evaluate the models on 5 tasks from the
Open LLM leaderboard~\citep{open-llm-leaderboard} (i.e.,
ARC~\citep{clark2018think}, HellaSwag~\citep{zellers2019hellaswag},
MMLU~\citep{hendrycks2021measuring}, TruthfulQA~\citep{lin2022truthfulqa} and
Winogrande~\citep{DBLP:journals/corr/abs-1907-10641}), and show that the Sparse
Wide IFT GPT-3 Small at 75\% sparsity improves the average accuracy by a
noticeable 0.9\% (see Table~\ref{tab:nlp_scratch_result}). In
Appendix~\ref{app:gpt_e2e}, we provide details on the models and
hyperparameters.
\begin{table}
    \caption{Average accuracy of Sparse Wide IFT with GPT-3 Small across ARC,
     HellaSwag, TruthfulQA, MMLU and Winogrande tasks on the Open LLM
     Leaderboard.} 
    \vspace{-5pt}
    \begin{center}
    \begin{small} 
    \begin{sc}
    \resizebox{0.85\linewidth}{!}{
    \begin{tabular}{cc|cc}
        \toprule
       	Model	     & Dense & 0.50 & 0.75 \\

	\midrule
        GPT-3 Small &   33.8 $\pm$ 0.1  &  34.1 $\pm$ 0.2      &  \textbf{34.7
        $\pm$ 0.2}  \\
        \bottomrule
    \end{tabular}
    }
    \end{sc}
    \end{small}
    \end{center}
\label{tab:nlp_scratch_result}
\vspace{-15pt}
\end{table}
\vspace{-5pt}