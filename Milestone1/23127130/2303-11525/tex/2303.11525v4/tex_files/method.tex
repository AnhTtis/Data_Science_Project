\section{Method}
\label{sec:method}

\begin{figure*}[h]
\includegraphics[width=\textwidth]{./figures/sift_family.pdf}

\vspace{-0.05in}
\caption{Different members of the Sparse-IFT family, each parameterized by a
single hyperparameter (i.e., sparsity level, $s$). Black and white squares
denote non-active and active weights, respectively. Green block indicates a
non-linear activation function (e.g., ReLU). Derived with sparsity set at $50\%$
as an example, all transformations are Iso-FLOP to the dense feedforward
function $f_{\theta_l}$, making them suitable drop-in replacements for
$f_{\theta_l}$. Details about each member are in
Section~\ref{subsec:members_of_sift}.}
\label{fig:diff_members_of_sift}
% \vspace{-0.1in} \vspace{-10pt}
\vspace{-10pt}
\end{figure*}

In this section, we first explain our intuition and hypotheses, followed by our
methodology to improve training efficiency.

% \vspace{-5pt}

\vspace{-10pt}
\paragraph{Training with Dense Matrices is FLOP Inefficient}
Modern DNNs are often overparameterized, showing sparsity in features and
weights across layers. The Lottery Ticket Hypothesis~\cite{frankle2018lottery,
chen2020lottery} suggests sparse DNNs, initialized with an effective sparsity
mask (``lottery ticket''), can achieve the same accuracy as dense counterparts.
Sparse training methods theoretically enhance efficiency but often yield lower
accuracy than dense baselines. This discrepancy may stem from challenges in
identifying optimal masks within a single training run. Existing sparse training
methods~\cite{jayakumar2020top, evci2020rigging, yuan2021mest, tai2022spartan,
liu2021sparse} invest these FLOP savings into longer training schedules to
bridge accuracy gaps, inefficiently requiring more FLOPs than dense baselines
for the same target accuracy.

In our work, we take an orthogonal approach and invest these FLOP savings to (1)
enhance a layer's representational capacity and (2) expand its search space,
aiming to discover an optimal sparse mask~\cite{ramanujan2020s,
stosic2021search}. Larger sparse models show potential for improved accuracy,
but designing an appropriate architecture is challenging. For instance,
achieving performance surpassing ResNet-18 on ImageNet requires careful balance
of sparsity and network size. Existing studies explore diverse combinations but
often lack FLOP efficiency, requiring multiple iterations for optimal settings
and hyperparameter tuning. To address this, we propose the Sparse Iso-FLOP
Transformation (Sparse-IFT) family, replacing dense layers with FLOP-equivalent
sparse transformations. Notably, Sparse-IFT is parameterized by a single
hyperparameterâ€”the sparsity level, simplifying the tuning process.

\vspace{-5pt}
\subsection{Sparse Iso-FLOP Transformations}
% \subsection{Setup}
\paragraph{Setup} For clarity, we explain our method in the context of a fully
connected network.
%$f(x; \theta)$
Let $\mathcal{N}$ denote a $L$ layered DNN parameterized by
$\Theta_{\mathcal{N}}$. Let $\Theta_{\mathcal N} \in \{\theta_1, \ldots,
\theta_L\}$ denote the parameters of the DNN. The output of the $l$-th layer is
defined as: $z_l = \sigma (f_{\theta_l}(z_{l-1}))$ for some activation function
$\sigma$ (e.g., ReLU~\cite{nair2010rectified}) and feedforward function
$f_{\theta_l}$. Specifically, let $f_{\theta_l}(z_{l-1}) = \theta_l^T z_{l-1}$,
where $\theta_l \in \mathbb{R}^{D_{in} \times D_{out}}$,  $z_{l-1} \in
\mathbb{R}^{D_{in} \times B}$ and $B$, $D_{in}$, $D_{out}$ denote the
batch-size, input, and output dimensionality of features respectively. The total
FLOPs needed for $f_{\theta_l}$ are given by $B{\cdot}D_{in}{\cdot}D_{out}$.  In
Appendix~\ref{app:sift_conv}, we detail a straightforward extension to
convolutional layers. In the standard setup, the feedforward function
$f_{\theta_l}$ computes output features through a linear transformation of input
features. While theoretically, arbitrary non-linear transformations can be
applied, practical implementations often resort to expressing transformations as
dense matrix multiplications for efficient GPU
support~\citep{nvidia2023gpuperf}. We aim to boost DNN training efficiency by
enhancing the representational capacity of the feedforward function. Unlike
conventional methods that increase capacity by stacking more
layers~\cite{lin2014network}, widening~\cite{zagoruyko2016wide}, or
ensembling~\citep{etai2020colleg}, our approach introduces unstructured sparsity
in weight matrices, achieving the same FLOPs as a dense feedforward function. 

Let $\Psi_l$ denote the set of Sparse Iso-FLOP Transformations (Sparse-IFT) for
a particular layer $l$: \setlength{\belowdisplayskip}{2pt}
\setlength{\belowdisplayshortskip}{2pt} \setlength{\abovedisplayskip}{2pt}
\setlength{\abovedisplayshortskip}{2pt}
\begin{equation*}
\Psi_l: \{ \psi_l(s),  0 \leq s < 1,  g(\psi_l) \approx g(f_{\theta_l}) \},
\end{equation*}
where $\psi_l$ is a transformation, $s$ represents the sparsity level, and
$g(\cdot)$ returns the computational FLOPs. Each transformation in this set
satisfies the following properties: (1) the computational FLOPs of the
transformation $\psi_l$ are same as that of dense transformation $f_{\theta_l}$,
and (2) the transformation is parameterized by a single hyperparameter - the
sparsity level. These Iso-FLOP transformations serve as drop-in replacements for
dense feedforward functions, preserving layer FLOPs. While there may be other
FLOP-invariant transformations, in this work, we explore: Sparse Wide, Sparse
Parallel, Sparse Factorized, and Sparse Doped.

\vspace{-5pt}
\subsection{Members of Sparse-IFT}
\label{subsec:members_of_sift}
% $k_{sw} = \sqrt{1/(1-s)}$
\paragraph{Sparse Wide}
This transformation augments the representational capacity of a layer by
increasing the number of output features while keeping $s$ fraction of weights
sparse. Hence, it widens the input and output features for all $L$ layers of the
network with the same widening factor, $k_{sw}$, to avoid mismatch in feature
dimensionality across layers. Let $\theta_l^{sw} \in
\mathbb{R}^{k_{sw}{\cdot}D_{in} \times k_{sw}{\cdot}D_{out}}$ denote the
transformation matrix, with $s$ fraction of weights being sparse. Since the
fraction of non-sparse weights is given by $1-s$, the FLOPs required by this
transformation are
$B{\cdot}(k_{sw}{\cdot}D_{in}){\cdot}(k_{sw}{\cdot}D_{out}){\cdot}(1-s)$.
Setting these equal to the FLOPs of the original dense $f_{\theta_l}$, we obtain
the widening factor $k_{sw} = \sqrt{1/(1-s)}$. If we set the sparsity $s$ to
$0$, we obtain $k_{sw}$ as $1$ and recover the  dense feedforward function.
\vspace{-15pt}
\paragraph{Sparse Parallel} The sparse parallel transformation replaces the
feedforward function with a sum of $k_{sp}$ non-linear functions. Let
$\theta_l^{sp} \in \{ \theta_l^{sp, 1},\ldots, \theta_l^{sp, k_{sp}}\}$ denote
the parameters of this transformation, where $\theta_l^{sp, j} \in
\mathbb{R}^{D_{in}\times D_{out}}$ denotes the transformation matrix of $j^{th}$
function, where $s$ fraction of weights are sparse. The sparse parallel
transformation in this case is $\psi^{sp}_l =
\sum^{k_{sp}}_{j=1}\sigma((\theta_l^{sp, j})^{T} z_{l})$, where $\sigma$ is a
non linear function. In practice, $\psi^{sp}_l$ is implemented as a layer with
$k_{sp}$ parallel branches. The computational FLOPs of this transformation is
$k_{sp}{\cdot}B{\cdot}D_{in}{\cdot}D_{out}{\cdot}(1-s)$. Setting these FLOPs
equal to FLOPs of $f_{\theta}$, we obtain $k_{sp} = 1/(1-s)$. Note, at $s=0$,
the number of parallel branches $k_{sp}$ is $1$. If we replace $\sigma$ with
Identity, we can recover the original dense feedforward function.

\vspace{-15pt}
\paragraph{Sparse Factorized} The transformation matrix of the feedforward
function $f_{\theta_l}$ is denoted by $\theta_l \in \mathbb{R}^{D_{in} \times
D_{out}}$. Multiple works have explored matrix factorization techniques to
express the transformation matrix $\theta_l$ as a product of two matrices
$\theta_l = UV^T$, where $U \in \mathbb{R}^{D_{in} \times d}$, $V \in
\mathbb{R}^{D_{out} \times d}$.~\citet{khodak2020initialization,
tai2016convolutional} and~\citet{chen2021drone} have explored low-rank
factorization ($d << D_{out}$) as a form of structured sparsity to improve
training and inference efficiency, while~\citet{arora2018optimization}
and~\citet{guo2020expandnets} have explored overparameterized factorizations for
better generalization and faster convergence. In contrast, we use factorization
to augment the representational capacity without decreasing or increasing the
FLOPs. More precisely, let $\theta_l^{sf} \in \{ U_l, V_l \}$ denote the
parameters of this transformation, where $U_l \in \mathbb{R}^{D_{in} \times
d_{sf}}$, $V_l \in \mathbb{R}^{d_{sf} \times D_{out}}$ are sparse matrices with
$s$ fraction of their weights being sparse. The functional transformation in
this case is $\psi^{sf}_l = V_l^T\sigma(U_l^Tz_l)$. The computational FLOPs of
this transformation is $d_{sf}{\cdot}B{\cdot}(D_{in} + D_{out}){\cdot}(1-s)$.
Setting these FLOPs equal to FLOPs of $f_{\theta_l}$, we obtain $d_{sf} =
\frac{D_{in}{\cdot}D_{out}}{(D_{in} + D_{out}){\cdot}(1-s)}$. Note, setting
sparsity $s=0$, we recover a non-linear low-rank factorization with dense
matrices.
% \vspace{-5pt}
\begin{table}
    % \vspace{-5pt}
    \caption{Cardinality of search space for sparsity masks of different members
    of the Sparse-IFT family.}
    % \vskip 0.15in
    \vspace{-5pt}
    \begin{center}
    \begin{small}
    \begin{sc}
    \resizebox{0.65\linewidth}{!}{
    \begin{tabular}{cc}
        \toprule
        Transformation & \begin{tabular}[c]{@{}c@{}}Cardinality of\\ Search
        Space\end{tabular} \\ \midrule Sparse Wide &
        $(k_{sw})^2{\cdot}(D_{in}{\cdot}D_{out})$ \\
        Sparse Parallel    &  $k_{sp}{\cdot}(D_{in}{\cdot}D_{out})$    \\
        Sparse Factorized  & $d_{sf}{\cdot}(D_{in} + D_{out})$   \\
        Sparse Doped       & $D_{in}{\cdot}D_{out}$            \\
        \bottomrule
    \end{tabular}
    }
    \end{sc}
    \end{small}
    \end{center}
    \label{tab:nature_of_sift_transformations}
    \vspace{-10pt}
    % \vskip -0.1in
\end{table}
\vspace{-10pt}
\paragraph{Sparse Doped} is inspired by previous works which approximate a dense
matrix with a combination of low-rank factorization and sparse
matrix~\citep{chen2021scatterbrain, thakker2021doping, udell2019big,
candes2011robust}. In our approach, we replace the feedforward function with
low-rank factorization (with rank $d_{sd}$) and an unstructured sparse weight
matrix (with sparsity $s$). Let $U_l \in \mathbb{R}^{D_{in} \times d_{sd}}, V_l
\in \mathbb{R}^{d_{sd} \times D_{out}}$ denote the low-rank matrices, and
$\theta_l^{sd} \in \mathbb{R}^{D_{in} \times D_{out}}$ denote the matrix with
unstructured sparsity. The functional transformation, in this case, is given by
$\psi^{sd}_l = V_l^T(U_l^Tz_l) + \sigma((\theta_l^{sd})^Tz_l)$. The
computational FLOPs associated with this transformation are
$B{\cdot}d_{sd}{\cdot}(D_{in} + D_{out}) +
(1-s){\cdot}B{\cdot}D_{in}{\cdot}D_{out}$. Setting these FLOPs equal to FLOPs of
$f_{\theta_l}$, we obtain $d_{sd} = \frac{s{\cdot}D_{in}{\cdot}D_{out}}{(D_{in}
+ D_{out})}$. Note, as $s \to 0$ and $d_{sd} \to 0$, the low-rank component of
disappears, and we can recover the dense feedforward function as a special case
by setting $\sigma$ to Identity.

\vspace{-10pt}
% \subsection{Cardinality of Search Space}
\paragraph{Cardinality of Search Space}
\label{subsec:cardinality} 
Increasing the sparsity mask search space with Sparse-IFT is anticipated to
enhance training efficiency, as indicated by prior works~\citep{ramanujan2020s,
liu2022unreasonable, stosic2021search}. The likelihood of finding a lottery
ticket in a randomly initialized network increases with network
width~\citep{ramanujan2020s}. Both \citet{liu2022the} and
\citet{stosic2021search} show that expanding the search space through increased
width or depth improves accuracy. The search space cardinality, defined as the
weights a sparse training method can explore, is detailed in
Table~\ref{tab:nature_of_sift_transformations}. Sparse Wide, Sparse Parallel,
and Sparse Factorized scale with width, parallel branches, and hidden dimension
size, respectively. Sparse Doped maintains a constant search space by allocating
FLOPs between a low-rank and an unstructured sparse weight matrix. Therefore,
DST becomes crucial for effectively traversing this larger parameter subspace,
as discussed in Section~\ref{sec:dst}.
\vspace{-5pt}