% \paragraph{Sparse-IFT Performance with ResNet-18} 
\section{Sparse-IFT Ablation Studies} 

\begin{figure*}[]
    \vspace{-10pt}
    \centering
    \subfigure{\includegraphics[width=0.33\textwidth]{figures/resnet18_us_vs_s_cifar100.pdf}}
    \subfigure{\includegraphics[width=0.33\textwidth]{figures/resnet18_sift_cifar100.pdf}}
    \subfigure{\includegraphics[width=0.33\textwidth]{figures/resnet18_dense_vs_sift_cifar100.pdf}}
    \vspace{-20pt}
    \caption{Ablation studies with Sparse-IFT on the ResNet-18 model for
    CIFAR-100 across sparsity $\in \{50\%, 75\%, 90\%\}$. (left) Sparse Wide IFT
    trained with dynamic unstructured and structured sparsity. (middle)
    Sparse-IFT family members trained with RigL, where Sparse Wide performs the
    best. (right) Sparse Wide IFT trained in a sparse and dense manner.}
    \label{fig:cifar100_ablations}
    \vspace{-10pt}
\end{figure*}

In this section, we present a comprehensive analysis of Sparse-IFT networks,
focusing on their training methodologies and design considerations. First, we
compare static sparse training with DST, highlighting DST's superior performance
in handling larger parameter spaces through empirical results using the
ResNet-18 architecture on CIFAR-100. Then, we explore critical design aspects of
Sparse-IFT, including the role of non-linearities, and the benefits of dynamic
unstructured sparsity over structured sparsity. Finally, we evaluate the
efficacy of DST by comparing it against densely trained Sparse-IFT models.
\vspace{-5pt}
\subsection{Impact of Sparse Training Techniques}
\label{sec:dst}
\begin{table}
    \caption{Sparse Wide IFT with ResNet-18 trained using various sparse
training methods on CIFAR-100 across different sparsity levels (columns). Best
accuracy for each sparse training method is highlighted in bold. } 
    \vspace{-2pt}
    \centering
    \begin{small}
    \begin{sc}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{ccccc}
            \hline
            \multicolumn{1}{c|}{Dense}                           & Sparse Method
            & 0.50                                     & 0.75           & 0.90
            \\ \hline
            \multicolumn{1}{c|}{\multirow{7}{*}{77.0 $\pm$ 0.2}} & Static &
            \textbf{78.5 $\pm$ 0.3} & 78.3 $\pm$ 0.1 & 78.2 $\pm$ 0.3 \\
            \multicolumn{1}{c|}{}                                & SNIP &
            \textbf{77.8 $\pm$ 0.3}                  & 77.0 $\pm$ 0.2 & 75.8
            $\pm$ 0.2                           \\
            \multicolumn{1}{c|}{}                                & GraSP &
            \textbf{77.7 $\pm$ 0.3}                  & 76.5 $\pm$ 0.3 & 76.5
            $\pm$ 0.3                           \\
            \multicolumn{1}{c|}{}                                & FORCE &
            \textbf{77.2 $\pm$ 0.3}                  & 76.9 $\pm$ 0.3 & 75.4
            $\pm$ 0.4                           \\
            \multicolumn{1}{c|}{}                                & SET & 78.8
            $\pm$ 0.1                           & 79.2 $\pm$ 0.2 & \textbf{79.8
            $\pm$ 0.2} \\
            \multicolumn{1}{c|}{}                                & RigL & 79.1
            $\pm$ 0.2                           & 79.5 $\pm$ 0.1 & \textbf{80.1
            $\pm$ 0.2} \\  
            \multicolumn{1}{c|}{}                                 & GraNet &
            79.2 $\pm$ 0.2                           & 79.6 $\pm$ 0.2 &
            \textbf{80.0 $\pm$ 0.2}  \\ \bottomrule
            \end{tabular}
    }
    \end{sc}
    \end{small}
\label{tab:dynamic_sparsity_importance}
\vspace{-15pt}
\end{table}

This section provides a comparative analysis of Sparse-IFT networks trained with
two classes of methods: static sparse training and DST. The focus is on
demonstrating DST's effectiveness in navigating larger parameter spaces, as
evidenced by previous research~\citep{huang2023dynamic,tai2022spartan}. Our
empirical results consistently show DST's superiority over static sparse
training. All experiments utilize the ResNet-18 architecture on CIFAR-100 with
published settings~\citep{devries2017improved}. Detailed model information and
hyperparameters are available in Appendix \ref{app:pt-implement}, and all
results are averaged over 3 seeds.

Sparse-IFTs employ unstructured sparsity in its transformations. This study
investigates the impact of sparse training methods on various Sparse-IFT
configurations, focusing on Sparse Wide IFT with sparsity $\in \{50\%, 75\%,
90\%\}$. In Table~\ref{tab:dynamic_sparsity_importance}, we evaluate: random
static sparsity, SNIP~\citep{lee2018snip}, GraSP~\citep{wang2020picking},
FORCE~\citep{de2020progressive}, SET~\cite{mocanu2018},
RigL~\cite{evci2020rigging} and GraNet~\citep{liu2021sparse}. SET, RigL, and
GraNet are DST methods, with SET updating the mask randomly, RigL updating it
with gradient information and GraNet incorporating gradual magnitude
pruning~\citep{Zhu2017ToPO} with RigL. Pruning at Initialization (PaI) methods
(e.g., SNIP, GraSP, FORCE) and GraNet increase training FLOPs due to non-uniform
sparsity and dense-to-sparse training. We address this by adjusting target
sparsity levels to align Sparse-IFT training FLOPs with the dense baseline (see
Appendix~\ref{app:control_flops}). In Iso-FLOP scenarios, PaI methods
underperform because they heavily prune parameter-rich layers to match target
sparsity levels, leading to layer-collapse and poor gradient flow. Furthermore,
DST methods consistently outperform static sparsity, with improvements
persisting at higher sparsity levels. Sparse-IFTs expand the sparse mask-weight
space $\propto$ sparsity, benefiting DST in thorough exploration and
exploitation within this space. While RigL and GraNet attain similar
performance, RigL is chosen as the sparse training method for simplicity in all
experiments.
\vspace{-15pt}
\subsection{Assessing the Effects of Architecture Variations}
\label{sec:sift_ablations}
% \vspace{-5pt}
This section analyzes different design considerations for Sparse-IFT by first,
exploring the role of non-linearities in enhancing representational capacity.
Then, the advantage of training with dynamic unstructured sparsity over
structured is investigated. Next, we compare between densely and sparsely
trained Sparse-IFT models. Finally, by applying top-performing Sparse-IFTs to
efficient vision models, these insights contribute to a synthesized framework.
\vspace{-15pt}
\paragraph{Importance of Using Non-Linear Activations}
For some of the Sparse-IFT members, we draw inspiration from linear
overparameterization methods, which fold the feedforward function into a dense
matrix post-training~\cite{ding2021repvgg, ding2021diverse, guo2020expandnets,
ding2019acnet}. Our method enhances representational capacity through an
Iso-FLOP transformation without increasing training FLOPs. Maintaining original
dense FLOP levels eliminates the need for post-training modifications, enabling
efficient inference and incorporation of non-linearities (i.e., ReLU) in
Sparse-IFT. Experiments on ResNet-18 on CIFAR-100 show notable accuracy gains
across all sparsity levels with non-linear activations. For example, at 90\%
sparsity, using non-linearities in Sparse Factorized IFT yields a 1.8\% accuracy
increase over the dense baseline, in contrast to a 0.5\% decrease without
non-linearities. These findings extend to all Sparse-IFT members (see
Appendix~\ref{app:nonlinear-importance} for details). The accuracy improvements
at all sparsity levels highlight the effectiveness of incorporating non-linear
activations in Sparse-IFT.
\vspace{-15pt}
\paragraph{Unstructured vs. Structured Sparsity}
We compare dynamic unstructured and structured sparsity using Sparse-IFT.
Unstructured sparsity explores all mask variations, but most hardware
accelerators do not support unstructured sparse acceleration. Prior works have
investigated structured sparsity, such as low-rank and block-sparse matrices,
for wall-clock speed-ups~\cite{khodak2020initialization, chen2021drone,
hubara2021accel, dao2022monarch}. We explore structured sparsity through
Iso-FLOP configurations with Sparse Wide IFT, employing low-rank factorization
and N:M sparsity for GPU acceleration. In Figure~\ref{fig:cifar100_ablations}
(left plot), we compare dynamic unstructured sparsity with N:M transposable
structured sparsity~\citep{hubara2021accel} using Sparse-IFT. The latter
demonstrates improvements over the dense baseline at 75\% and 90\% sparsity
levels. Results also indicate that N:M block sparsity outperforms low-rank
factorization (see Appendix~\ref{app:structured_sparsity}). However,
unstructured sparsity still gives the highest gains, as N:M sparsity has reduced
mask diversity in block-sparse matrices~\citep{hubara2021accel},  therefore, we
adopt unstructured sparsity in all subsequent experiments.

\vspace{-15pt}
\paragraph{Sparse-IFT ResNet-18} 
We assess all Sparse-IFT family members with ResNet-18 on CIFAR-100 across
different sparsity levels. The middle plot of
Figure~\ref{fig:cifar100_ablations}, highlights the best accuracy achieved by
each Sparse-IFT member. All members exhibit substantial accuracy improvements
compared to the dense baseline (77\%), using the same FLOPs. Sparse Wide
consistently performs the best, while Sparse Doped is the only member not
gaining accuracy at higher sparsity. This is attributed to Sparse Doped
maintaining constant search space by distributing FLOPs between low-rank and
unstructured sparse matrices (see
Table~\ref{tab:nature_of_sift_transformations}), leading to a decrease in active
weights in the unstructured matrix. In
Appendix~\ref{app:sift_vs_extendedsparse}, we compare Sparse-IFT against other
DST baselines under the same training efficiency setup by extending the training
steps, showing Sparse-IFT outperforms them significantly at $s \in \{50\%, 75\%,
90\%\}$. Since, Sparse Parallel and Sparse Wide perform the best across
ablations, we use these two IFTs for the main experiments.
\vspace{-15pt}
\paragraph{Sparse-IFT vs. Dense Overparametrization}
A crucial element in the success of Sparse-IFT lies in its efficient exploration
of the search space. In this section, to benchmark this exploration, we
establish an upper bound by training the Sparse-IFT architectures in a dense
manner (with sparsity levels $s \in \text{50\%, 75\%, 90\%}$). In
Figure~\ref{fig:cifar100_ablations}, the right plot compares the sparse and
dense versions of Sparse Wide and Sparse Parallel IFTs. Both Sparse-IFT members
excel in exploring a large search space with DST, achieving accuracy comparable
to their dense counterparts without the computational overhead. These results
highlight that the sparsity search in DST approaches optimality and can achieve
accuracy comparable to that of densely trained models. This efficiency does not
compromise accuracy and offers substantial computational benefits, especially on
hardware optimized for sparsity (further discussed in Section 6).
\vspace{-5pt}
\begin{table}
    \caption{Sparse Wide IFT with various efficient architectures on CIFAR-100
across different levels of sparsity (columns).} 
    \vspace{-5pt}
    \centering
    \begin{sc}
    \begin{small}
    \resizebox{0.85\linewidth}{!}{
    \begin{tabular}{c|ccc}
        \toprule
        	    & Dense             & 0.50 & 0.75 \\ \midrule MobileNetV2 & 72.4
        $\pm$ 0.2    & 73.4 $\pm$ 0.2 & \textbf{73.7 $\pm$ 0.2}  \\
        MobileViT-S & 73.5 $\pm$ 0.1    & 74.6 $\pm$ 0.2 & \textbf{74.8 $\pm$
        0.2} \\
%        BotNet-26   & 78.0              & 78.1 & \textbf{78.7} \\
        BotNet-50   & 79.8 $\pm$ 0.2     & 80.3 $\pm$ 0.3 & \textbf{80.9 $\pm$
        0.3} \\
        \bottomrule
    \end{tabular}
    }
    \end{small}
    \end{sc}
\label{tab:mbv2-cifar}
\vspace{-5pt}
\end{table}
\vspace{-20pt}
\paragraph{Efficient Architectures}
\label{subsec:efficient_archs}
To assess Sparse-IFT's robustness across diverse set of models, we evaluate it
on architectures optimized for efficient inference
(MobileNetV2~\citep{sandler2018mobilenetv2} and
MobileViT~\citep{mehta2021mobilevit}) and efficient training
(BotNet~\citep{srinivas2021bottleneck}). Applying Sparse Wide IFT to dense
layers significantly improves test accuracy across all architectures (refer to
Table~\ref{tab:mbv2-cifar}). Similarly, utilizing the Sparse Parallel IFT
consistently enhances performance across all architectures (see
Appendix~\ref{app:efficientcv}). We evaluate the best-performing model,
BotNet-50, on ImageNet, where the Sparse-IFT variant outperforms dense by 1\%
(see Section~\ref{subsec:imagenet}). We provide additional experimental setup
details in Appendix~\ref{app:pt-implement}. In summary, Sparse-IFT significantly
improves test accuracy across all efficient architectures, demonstrating its
robustness and effectiveness.
\vspace{-15pt}