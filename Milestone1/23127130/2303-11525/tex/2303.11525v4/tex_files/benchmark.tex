\section{Wall-Clock Acceleration with Sparse-IFT}
\label{sec:wall_clock}
Our studies in Section~\ref{sec:empirical_res} show noticeably improved training
efficiency (test accuracy~w.r.t training FLOPs) for Sparse-IFT models. In this
section, we aim to showcase the practicality of Sparse-IFT models, providing
unique hardware insights for accelerating DNNs with unstructured sparsity, a
perspective notably absent in most existing works. Recent developments, like
specialized software kernels and hardware (e.g.,
DeepSparse~\citep{neural_magic_2021} and Cerebras CS-2~\citep{lie2023cerebras})
indicate promising gains in realizing unstructured sparsity benefits during
training and inference~\citep{thangarasa2023spdf}. This sets the stage for
examining the impact on inference and training acceleration.

\vspace{-15pt}
% \vspace{-0.2in}
\paragraph{Real-World Inference Acceleration} 
We assess Sparse-IFT's inference efficiency using
DeepSparse\footnote{\href{https://github.com/neuralmagic/deepsparse}{Neural
Magic DeepSparse}}. Our setup employs a ResNet-18 model and performs batched
inference of 64 images from ImageNet at 224 Ã— 224 resolution on Intel Cascade
Lake CPUs, known for their AVX-512 support. The latency (i.e., seconds per
batch) is compared between the dense ResNet-18 model and the Sparse Wide IFT
variants at $s \in \text{50\%, 75\%, 90\%}\}$. On an ideal hardware, FLOPs
should directly translate to wall clock time. Therefore, the inference latency
or training time for all Sparse-IFT models should match that of the dense model,
as all models are Iso-FLOP. This baseline is illustrated by the black dashed
line in the left plot of Figure~\ref{fig:sparsity_benchmark}. However, the blue
line shows the expected increases in latency on hardware \textit{without}
unstructured sparse acceleration support, like the CPUs we benchmarked on,  with
a notable 19.5x increase at $s = \text{90\%}$. In contrast, the green line
demonstrates a significant reduction in latency using DeepSparse, decreasing the
latency increase from 19.5x to 3.5x, and showing minimal overhead up to 75\%
sparsity. This emphasizes the benefits of optimized kernel support for sparse
inference acceleration, showcasing the potential for practical deployment of
Sparse-IFT models.

\begin{figure}[!t]
    \centering
    \vspace{-5pt}
    \subfigure{\includegraphics[width=0.23\textwidth]{figures/neural_magic_inference_relative.pdf}}
    \subfigure{\includegraphics[width=0.23\textwidth]{figures/cerebras_training_speed_relative.pdf}}
    \vspace{-10pt}
    \caption{Benchmarking unstructured sparsity during (left) inference on
    Neural Magic's DeepSparse runtime and (right) training acceleration on the
    Cerebras CS-2. In both setups, we measure the relative increase in latency
    or training speed for Sparse-IFT variants against the dense model.}
    \label{fig:sparsity_benchmark}
    \vspace{-10pt}
\end{figure}

% \vspace{-10pt}
\paragraph{Real-World Training Acceleration} 
In the right plot of Figure~\ref{fig:sparsity_benchmark}, we evaluate the
training efficiency of Sparse-IFT on the Cerebras CS-2 system, which supports
unstructured sparse training for
LLMs\footnote{\href{https://docs.cerebras.net/en/2.1.1/wsc/how_to_guides/sparsity.html}{Cerebras
CS-2 (R2.1.1): Train a Model with Weight Sparsity}}. Our experimental setup
involves pre-training a GPT-3 model on the CS-2. We measured and compared
throughput (i.e., iterations per second), between the dense GPT-3 model and
Sparse Wide IFT variants at sparsity levels of 50\%, 75\%, and 90\%. As
previously mentioned, the theoretical baseline (black dashed line) suggests that
since both the dense model and Sparse Wide IFT configurations are Iso-FLOP,
training time should not increase with increasing sparsity. The blue line shows
the throughput for Sparse Wide IFT variants run \textit{without} any
unstructured sparse acceleration support on the Cerebras CS-2, indicating a
$\sim$10x increase in training time for the model at 90\% sparsity. A similar
degradation in performance would be expected on traditional Nvidia GPU or Google
TPU hardware as well. In contrast, the green line demonstrates the effect of
utilizing the Cerebras CS-2's unstructured sparse training support. Here, we
observe a significant reduction in relative training time, bringing down the
increase from $\sim$10x to 2.82x at 90\% sparsity. Additionally, for sparsity
levels up to 75\%, we note minimal overhead compared to the dense model.
Detailed benchmarking setups are in Appendix~\ref{app:benchmark}. 

While we do not achieve perfect FLOPs translation with Sparse-IFT models, our
promising results highlight the importance of ML software and hardware co-design
for leveraging sparsity. The interaction of layer dimensions, sparsity, and
overhead, influenced by hardware architecture, necessitates co-designed sparse
techniques for optimal performance. Our work showcases algorithmic advancements
over prior sparse methods, emphasizing the benefits of sparse training and
winning \textit{the hardware lottery}~\citep{2020shooker}.
\vspace{-10pt}
