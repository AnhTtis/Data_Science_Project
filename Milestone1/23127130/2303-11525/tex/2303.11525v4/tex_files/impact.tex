\section*{Impact Statement}

The landscape of machine learning (ML) has witnessed an exponential growth in
models, particularly in domains such as natural language processing and computer
vision. However, this surge in model size has come with a considerable cost in
terms of compute, memory, and energy requirements. Our approach, Sparse Iso-FLOP
Transformations (Sparse-IFT), represents a significant stride toward mitigating
these resource-intensive demands. Sparse-IFT introduces a novel approach that
enhances the efficiency of training large neural networks. Remarkably, it
achieves improved accuracy while maintaining the same FLOPs as the original
dense baseline model. Our method holds promise for positive environmental
impacts, given the substantial computational resources typically associated with
training large neural networks.

Models trained with Sparse-IFT require less computing resources and energy to
achieve higher model quality, directly translating to lower deployment costs for
real-world applications. Furthermore, training models with sparsity has been
shown to lead to better generalization~\cite{chen2022sparsity}, a benefit
supported by our transfer learning results on computer vision tasks. An
additional advantage is the enhanced efficiency in training larger sparse
models, facilitated by the widespread adoption of AI hardware, such as the
Cerebras CS-2, which accelerates unstructured sparsity. The key here is
achieving sparsity acceleration through a harmonious collaboration between
hardware support and the development of sparse ML techniques.

The potential sustainability contribution lies in the fact that, as sparse ML
software and hardware co-design continues to evolve, we may be able to train
more accurate ``larger sparse'' networks within the confines of the same
computational budget as a smaller dense model. This paradigm shift could usher
in a more environmentally conscious approach to deep learning, addressing the
concerns associated with the escalating resource requirements of ever-expanding
models. The seamless integration of these elements ensures that hardware
architectures are optimized to complement sparse techniques, fostering a
sustainable and efficient trajectory for the future of deep learning.

As we continue to explore the intersection of hardware support for sparsity and
the evolution of sparse ML techniques, our benchmarking analysis in
Section~\ref{sec:wall_clock} and Appendix~\ref{app:benchmark} serves as a
practical illustration of the transformative potential of Sparse-IFT. It not
only substantiates the theoretical promises but also offers a roadmap for future
developments in the pursuit of sustainable and efficient deep learning
practices.
