\section{Related Work}
Our work aligns with research on overparameterization and sparsity in DNN
training. The required modeling capacity for a given task is often unknown,
leading to training overparameterized models to fully exploit learning
capabilities before compressing them into smaller subnetworks.
\vspace{-10pt}
\paragraph{Overparameterization}~\citet{nakkiran2021deep} show that DNNs benefit
from overparameterization. Subsequently, several studies have capitalized on
overparameterization by scaling the size of models~\cite{rae2021scaling,
goyal2022vision} and augmenting existing DNNs to boost modeling capacity and the
accuracy of trained networks~\cite{shuxuan2020expandnet, ding2019acnet,
ding2021repvgg, jinming2022doconv, mobileone2022, liu2022more}. These methods
use linear parameterizations of the model, making them highly inefficient to
train, and are focused on improving inference throughput. In contrast, our work
is focused on improving the modeling capacity using sparse non-linear
parameterizations. Sparse-IFT enhances accuracy without increasing training and
inference FLOPs compared to the baseline dense model.
\vspace{-25pt}
\paragraph{Sparse Network Training} The Lottery Ticket
Hypothesis~\cite{frankle2018lottery, frankle2020linear} shows that accurate
sparse subnetworks exist in overparameterized dense networks but require
training a dense baseline to find. Other approaches have proposed frameworks for
identifying lottery tickets~\citep{hattie2019supermask, ma2022effective} but
still require a lot of compute resources. Following this, various attempts have
been made to find the optimal sparse subnetwork in a single shot. These methods
either try to find the subnetworks at initialization~\cite{tanaka2020pruning,
wang2020picking, de2020progressive, lee2018snip} or dynamically during
training~\cite{mocanu2018, evci2020rigging, jayakumar2020top, raihan2020sparse}.
However, given a fixed model capacity, these methods tradeoff accuracy relative
to the dense baseline to save training FLOPs. ~\citet{stosic2021search}
and~\citet{ramanujan2020s} increase the search space during sparse training to
retain accuracy; however, do not guarantee FLOPs savings. In contrast to these
methods, our work introduces a set of non-linear sparse transformations, which
increase the representational capacity of the network. This approach does not
introduce a new sparse training algorithm, but instead improves the search space
of existing methods, leading to improved generalization while being efficient to
train.

\vspace{-10pt}
\paragraph{Iso-Parameter vs. Iso-FLOP} Recent works have focused on improving
generalization at high sparsity levels. Techniques such as the
Erd\"{o}s-R\'{e}nyi-Kernel~\cite{evci2020rigging}, Ideal Gas
Quota~\cite{chen2022sparsity}, and parameter leveling~\cite{golubeva2021are}
employ layer-wise sparsity distributions in sparse training to boost accuracies.
These methods, however, target scenarios where models have a fixed parameter
budget (i.e., Iso-Parameter), which does not equate to similar training FLOPs as
the original dense model. Our work highlights that while transformer-based NLP
networks may not show significant differences between Iso-Parameter and Iso-FLOP
optimization, this distinction becomes critical in CV networks. In CNNs and
heterogeneous ViTs~\cite{pyramidvit2021, scalablevit2021, cvt2021}, the uneven
distribution of parameters and computational costs across layers necessitates a
distinct approach. Optimizing for Iso-Parameter typically involves pruning
later, parameter-rich layers, thus maintaining performance but not significantly
reducing computational costs. Conversely, optimizing for Iso-FLOP shifts pruning
to early, FLOP-intensive layers, enhancing performance by addressing both
computational demands and pruning needs. Unlike variable sparsity techniques
that adapt to different computational and memory demands across layers, our
method employs a uniform sparsity approach, ensuring consistent FLOP reductions
across all layers. This aligns computational costs closely with those of a fully
dense model, achieving significant computational efficiencies without
compromising performance.

\vspace{-10pt}
\paragraph{Sparse-IFT and Scaling Laws}
Recent advances in deep learning highlight the importance of scaling laws, which
provide a systematic framework for optimizing model performance as model size
increases. Pioneering scaling laws work such as ConvNeXt~\citep{convnext2023},
EfficientNet~\citep{pmlr-v97-tan19a}, large language
models~\citep{kaplan2020scaling}, and vision
transformers~\citep{alabdulmohsin2023getting} demonstrate that achieving optimal
performance typically involves tuning multiple training (e.g., learning rate,
batch sizes, etc.) and architectural (e.g., depth, width, resolution, etc.)
hyperparameters. This intricate balance necessitates extensive experimentation
and hyperparameter tuning. Sparse-IFT introduces a streamlined approach for
scaling DNNs, leveraging a single hyperparameter, the sparsity level, to enhance
model efficiency and accuracy. This method simplifies the optimization process
by eliminating the need to tune multiple factors concurrently. Future research
will explore the integration of Sparse-IFT with scaling laws to address the
challenges of scaling large models. This involves examining the interplay
between Sparse-IFT and various architectural elements, such as depth and width,
while maintaining a constant computational FLOP budget.
\vspace{-10pt} %\raggedbottom
