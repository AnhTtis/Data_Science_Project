\section{Spectral Analysis of DST in Sparse-IFT}
In this study, we investigate the intricate properties of Sparse-IFT networks
and their training dynamics. We analyze the benefits of Sparse-IFT networks
trained with DST by analyzing the Ramanujan Gap and Spectral Gap
characteristics. Ramanujan graph structures which are known to exhibit sparsity
and high connectivity like expander graphs, are investigated to reveal their
correlation with the final performance of sparse networks. Our analysis
evaluates the impact of model parameters and graph connectivity on the
effectiveness of DNNs with Sparse-IFTs, aiming to provide insights into the
training dynamics of Sparse-IFT models. Inspired by~\citet{hoang2023revisiting,
hoang2023dont}, in this analysis, we interpret the ResNet-18 model as a series
of bipartite compute graphs, where each layer, $\{\theta_1,\ldots, \theta_L\}$
in an $L$ layered sparse DNN, takes the form of a square adjacency matrix $A$.
\citet{hoang2023revisiting} proposed several graph metrics inspired by Ramanujan
properties for characterizing sparse networks, via: 1) \textbf{Ramanujan Gap}:
$\Delta r = 2 \ast \sqrt{d-1} - \hat{\mu}(A)$, and $\Delta r_{imdb} =
\frac{1}{\abs{K}} \sum_{i=1}^{\abs{K}}(2\sqrt{d_i - 1} - \hat{\mu}(A_{K_i}))$,
where $d$ is the average edge per node, and $\hat{\mu}(A)$ is the non-trivial
eigenvalue of $A$. Here, $\Delta r$ is the conventional view of measuring gap
between Ramanujan's upper bound $2 \ast \sqrt{d-1}$ and $\hat{\mu}(A)$. $\Delta
r$ measures the network's degree of connectivity to reveal the flow of
information propagation. $\Delta r_{imdb}$~\citep{hoang2023revisiting}, the
Iterative Mean Difference Bound (imdb), evaluates the average connectivity
boundary across all subgraphs $K$ within $A$. A higher $\Delta r$ in sparse networks signifies efficient information flow,
gradient propagation, and a well-separated spectrum in the adjacency matrix of
sparse weights; indicating robust and efficient representation. In addition, an
increasing $\Delta r_{imdb}$ indicates more extensive connectivity boundaries
within subgraphs, enhancing communication among nodes and promoting stronger
connections. 2)~\textbf{Weighted Spectral Gap}: $\lambda = \mu_0(\abs{\vb*{W}})
- \hat{\mu}(\abs{\vb*{W}})$, and $\lambda_{imsg} =
\frac{1}{\abs{K}}\sum_{i=1}^{\abs{K}}(\mu_{0}(\abs{\vb*{W}_{K_i}}) -
\hat{\mu}(\abs{\vb*{W}_{K_i}}))$. Here, the gap between $\mu_0$, the trivial
parameters, and $\hat{\mu}$, the non-trivial eigenvalues of $\vb*{W}$, the
weighted adjacency matrix, is denoted as $\lambda$, the weighted spectral gap.
Then, $\lambda_{imsg}$~\citep{hoang2023dont} is the iterative version which
takes into account all subgraphs $K$ within $\vb*{W}$. A higher $\lambda_{imsg}$
indicates enhanced spectral separation between $\mu_0$ and $\hat{\mu}$ of
$\vb*{W}$, implying a more distinct and well-defined spectral structure within
subgraphs. This improved separation in the spectrum, represented by a higher
$\lambda$, facilitates better isolation of meaningful signals. We train Sparse
Wide and Sparse Parallel ResNet-18 models at 50\% sparsity on CIFAR-100. Then,
we generated a Pareto curvature heatmap, considering weight magnitudes and graph
topological structure details (see Figure~\ref{fig:graph_analysis}). See
Appendix~\ref{app:graph_analysis_sift_dst} for a detailed analysis. 
% \newline
\begin{figure}
    \vspace{-0.1in}
    \centering
    \subfigure{\includegraphics[width=0.23\textwidth]{figures/resnet18_sw_50_iter.pdf}}
    \subfigure{\includegraphics[width=0.23\textwidth]{figures/resnet18_sp_50_iter.pdf}}
    \\ \vspace{-10pt}
    \subfigure{\includegraphics[width=0.23\textwidth]{figures/resnet18_sw_50_std.pdf}}
    \subfigure{\includegraphics[width=0.233\textwidth]{figures/resnet18_sp_50_std_fixed.pdf}}
    \vspace{-15pt}
    \caption{The relationship between the structure and weights of Sparse-IFT
    ResNet-18 networks are analyzed through a graph perspective in terms of
    performance. Top row: we assess the relationship between $\Delta r_{imdb}$
    and $\lambda_{imsg}$. Bottom row: investigates the correlation between
    $\Delta r$ and $\lambda$. The Pareto curvature heatmap visually represents
    the classification performance, with varying color gradients symbolizing the
    spectrum from low to high test accuracy on CIFAR-100.}
    \vspace{-5pt}
    \label{fig:graph_analysis}
    \vspace{-10pt}
\end{figure}

\textbf{$\vb*{\Delta r_{imdb}}$ and $\vb*{\lambda_{imsg}}$ Analysis:} In the
initial to middle stages of training, RigL's dynamic pruning and regrowth
increases $\Delta r_{imdb}$ for the network to \textit{explore} diverse
connectivity patterns (see top row, Figure~\ref{fig:graph_analysis}). Subsequent
pruning removes less critical connections, diversifying subgraphs in the
adjacency matrix $A$. Later stages witness a decrease in $\Delta r_{imdb}$ as
the network converges to more focused and organized connectivity patterns. RigL
prioritizes crucial connections, \textit{exploiting} an efficient subgraph
structure linked with highly accuracy regions of Sparse-IFT models. Early in
training, increasing $\lambda_{imsg}$ suggests successful isolation of different
modes. Pruning leads to a distinct separation between dominant and less dominant
modes. The subsequent $\lambda_{imsg}$ decrease signals the network's
convergence to a more specialized representation, emphasizing key spectral
components and diminishing the influence of less critical modes. While both
Sparse Wide and Sparse Parallel IFTs show increasing $\Delta r_{imdb}$, the
larger search space cardinality in Sparse Wide facilitates the emergence of
diverse subgraph structures within each layer, allowing for a richer set of
connections between nodes; resulting in a higher maximum $\Delta r_{imdb}$.
Similarly, Sparse Wide has a higher maximum $\lambda_{imsg}$ compared to Sparse
Parallel, indicating the emergence of subgraphs with more distinct spectral
properties. \\
% \newline
\textbf{$\vb*{\Delta r}$ and $\vb*{\lambda}$ Analysis:}
Figure~\ref{fig:graph_analysis}'s bottom row reveals a strong correlation
between $\Delta r$ and $\lambda$. $\Delta r$ initially decreases, indicating a
temporary relaxation of spectral constraints during dynamic pruning with RigL.
Subsequently, it maximizes in the final training stages, signifying RigL's
ability to guide the network to reorganize its connectivity, promoting more
structured and favorable spectral characteristics. Similarly, $\lambda$ follows
a pattern of initial decrease and later maximization. This implies that RigL's
dynamic sparsity initially results in less optimal weight organization
concerning spectral properties. However, RigL's iterative pruning and rewiring
dynamically adapts the network, aligning weights to enhance spectral
characteristics and increase the spectral gap.  Our analysis demonstrates that
DST, as exemplified by RigL, outperforms static sparse training by optimizing
spectral characteristics for Sparse-IFT; facilitating improved connectivity
patterns and a more favorable spectral profile.
\vspace{-5pt}