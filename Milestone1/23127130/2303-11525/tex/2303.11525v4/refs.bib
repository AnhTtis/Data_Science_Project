@inproceedings{yang2022tensor,
  title={Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer},
  author={Yang, Greg and Hu, Edward J and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{zhang2019improving,
  title={Improving deep transformer with depth-scaled initialization and merged attention},
  author={Zhang, Biao and Titov, Ivan and Sennrich, Rico},
  booktitle={EMNLP},
  year={2019}
}


@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  year={2017},
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv},
  year={2015}
}

@inproceedings{you2020large,
  title={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  booktitle={ICLR},
  year={2020},
}

@inproceedings{
hoang2023revisiting,
title={Revisiting Pruning At Initialization Through The Lens of Ramanujan Graph},
author={Duc N.M Hoang and Shiwei Liu and Radu Marculescu and Zhangyang Wang},
booktitle={ICLR},
year={2023},
}

@inproceedings{hoang2023dont,
title={Don{\textquoteright}t just prune by magnitude! Your mask topology is a secret weapon},
author={Duc N.M Hoang and Souvik Kundu and Shiwei Liu and Zhangyang Wang},
booktitle={NeurIPS},
year={2023}
}

@inproceedings{yu2017scalpel,
  title={Scalpel: Customizing dnn pruning to the underlying hardware parallelism},
  author={Yu, Jiecao and Lukefahr, Andrew and Palframan, David and Dasika, Ganesh and Das, Reetuparna and Mahlke, Scott},
  journal={ACM SIGARCH Computer Architecture},
  year={2017},
}

@inproceedings{han2017ese,
  title={ESE: Efficient speech recognition engine with sparse lstm on fpga},
  author={Han, Song and Kang, Junlong and Mao, Huizi and Hu, Yiming and Li, Xin and Li, Yubin and Xie, Dongliang and Luo, Hong and Yao, Song and Wang, Yu and others},
  booktitle={ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
  year={2017}
}


@inproceedings{sanh2020movement,
  title={Movement pruning: Adaptive sparsity by fine-tuning},
  author={Sanh, Victor and Wolf, Thomas and Rush, Alexander},
  booktitle={NeurIPS},
  year={2020}
}

@InProceedings{thangarasa2023spdf,
  title = 	 {{SPDF}: Sparse Pre-training and Dense Fine-tuning for Large Language Models},
  author =   {Thangarasa, Vithursan and Gupta, Abhay and Marshall, William and Li, Tianda and Leong, Kevin and DeCoste, Dennis and Lie, Sean and Saxena, Shreyas},
  booktitle = {UAI},
  year = 	 {2023},
}


@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on
imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{srinivas2021bottleneck,
  title={Bottleneck transformers for visual recognition},
  author={Srinivas, Aravind and Lin, Tsung-Yi and Parmar, Niki and Shlens,
Jonathon and Abbeel, Pieter and Vaswani, Ashish},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{mehta2021mobilevit,
  title={MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision
Transformer},
  author={Mehta, Sachin and Rastegari, Mohammad},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{chen2022coarsening,
  title={Coarsening the granularity: Towards structurally sparse lottery
tickets},
  author={Chen, Tianlong and Chen, Xuxi and Ma, Xiaolong and Wang, Yanzhi and
Wang, Zhangyang},
  booktitle={ICML},
  year={2022}
}


@inproceedings{
jiang2022exposing,
title={Exposing and Exploiting Fine-Grained Block Structures for Fast and
Accurate Sparse Training},
author={Peng Jiang and Lihan Hu and Shihui Song},
booktitle={NeurIPS},
year={2022},
}


@inproceedings{
chen2022pixelated,
title={Pixelated Butterfly: Simple and Efficient Sparse training for Neural
Network Models},
author={Beidi Chen and Tri Dao and Kaizhao Liang and Jiaming Yang and Zhao Song
and Atri Rudra and Christopher Re},
booktitle={ICLR},
year={2022},
}

@inproceedings{nair2010rectified,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={ICML},
  year={2010}
}

@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing
internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={ICML},
  year={2015}
}

@article{stosic2021search,
  title={Search spaces for neural model training},
  author={Stosic, Darko and Stosic, Dusan},
  journal={arXiv},
  year={2021}
}

@inproceedings{ramanujan2020s,
  title={What's hidden in a randomly weighted neural network?},
  author={Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and
Farhadi, Ali and Rastegari, Mohammad},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{
liu2022the,
title={The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training},
author={Shiwei Liu and Tianlong Chen and Xiaohan Chen and Li Shen and Decebal Constantin Mocanu and Zhangyang Wang and Mykola Pechenizkiy},
booktitle={ICLR},
year={2022},
}

@inproceedings{
merity2017pointer,
title={Pointer Sentinel Mixture Models},
author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
booktitle={ICLR},
year={2017},
}

@inproceedings{ding2019acnet,
  title={Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks},
  author={Ding, Xiaohan and Guo, Yuchen and Ding, Guiguang and Han, Jungong},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{ding2021diverse,
  title={Diverse branch block: Building a convolution as an inception-like unit},
  author={Ding, Xiaohan and Zhang, Xiangyu and Han, Jungong and Ding, Guiguang},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{thakker2021doping,
  title={Doping: A technique for efficient compression of LSTM models using sparse structured additive matrices},
  author={Thakker, Urmish and Whatmough, Paul N and Liu, Zhigang and Mattina, Matthew and Beu, Jesse},
  booktitle={MLSys},
  year={2021}
}

@inproceedings{chen2021scatterbrain,
  title={Scatterbrain: Unifying Sparse and Low-rank Attention Approximation},
  author={Chen, Beidi and Dao, Tri and Winsor, Eric and Song, Zhao and Rudra, Atri and R{\'e}, Christopher},
  booktitle={NeurIPS},
  year={2021}
}

@article{udell2019big,
  title={Why are big data matrices approximately low rank?},
  author={Udell, Madeleine and Townsend, Alex},
  journal={SIAM Journal on Mathematics of Data Science},
  year={2019}
}

@article{candes2011robust,
  title={Robust principal component analysis?},
  author={Cand{\`e}s, Emmanuel J and Li, Xiaodong and Ma, Yi and Wright, John},
  journal={Journal of the ACM},
  year={2011},
}

@inproceedings{ding2021repvgg,
  title={Repvgg: Making vgg-style convnets great again},
  author={Ding, Xiaohan and Zhang, Xiangyu and Ma, Ningning and Han, Jungong and Ding, Guiguang and Sun, Jian},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{dosovitskiy2020image,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{arora2018optimization,
  title={On the optimization of deep networks: Implicit acceleration by overparameterization},
  author={Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  booktitle={ICML},
  year={2018}
}

@inproceedings{chen2021drone,
  title={Drone: Data-aware low-rank compression for large nlp models},
  author={Chen, Patrick and Yu, Hsiang-Fu and Dhillon, Inderjit and Hsieh, Cho-Jui},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{tai2016convolutional,
  title={Convolutional neural networks with low-rank regularization},
  author={Tai, Cheng and Xiao, Tong and Zhang, Yi and Wang, Xiaogang and Weinan, E},
  booktitle={ICLR},
  year={2016}
}
@inproceedings{khodak2020initialization,
  title={Initialization and Regularization of Factorized Neural Layers},
  author={Khodak, Mikhail and Tenenholtz, Neil A and Mackey, Lester and Fusi, Nicolo},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{evci2020rigging,
  title={Rigging the lottery: Making all tickets winners},
  author={Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  booktitle={ICML},
  year={2020}
}

@inproceedings{guo2020expandnets,
  title={Expandnets: Linear over-parameterization to train compact convolutional networks},
  author={Guo, Shuxuan and Alvarez, Jose M and Salzmann, Mathieu},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{shazeer2016outrageously,
  title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  booktitle={ICLR},
  year={2016}
}

@article{fedus2021switch,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={arXiv},
  year={2021}
}


@inproceedings{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  booktitle={ICLR},
  year={2018}
}

@inproceedings{jayakumar2020top,
  title={Top-kast: Top-k always sparse training},
  author={Jayakumar, Siddhant and Pascanu, Razvan and Rae, Jack and Osindero, Simon and Elsen, Erich},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{krizhevsky2012imagenet,
  title = {ImageNet Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  booktitle={NeurIPS},
  year={2012}
}

@inproceedings{lin2014network,
  title={Network in network},
  author={Lin, Min and Chen, Qiang and Yan, Shuicheng},
  booktitle={ICLR},
  year={2014}
}

@inproceedings{liu2021selfish,
  title={Selfish sparse RNN training},
  author={Liu, Shiwei and Mocanu, Decebal Constantin and Pei, Yulong and Pechenizkiy, Mykola},
  booktitle={ICML},
  year={2021},
}

@inproceedings{liu2021we,
  title={Do we actually need dense over-parameterization? in-time over-parameterization in sparse training},
  author={Liu, Shiwei and Yin, Lu and Mocanu, Decebal Constantin and Pechenizkiy, Mykola},
  booktitle={ICML},
  year={2021},
}

@article{mocanu2018,
title = "Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science",
author = "Mocanu, {Decebal Constantin} and Elena Mocanu and Peter Stone and Nguyen, {Phuong H.} and Madeleine Gibescu and Antonio Liotta",
year = "2018",
journal = "Nature Communications",
}


@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  journal={OpenAI Blog}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NeurIPS},
  year={2017}
}


@inproceedings{zagoruyko2016wide,
  title={Wide Residual Networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  booktitle={BMVC},
  year={2016}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={IJCV},
  year={2015}
}

@article{torchvision2016,
    title        = {TorchVision: PyTorch's Computer Vision library},
    author       = {PyTorch},
    year         = 2016,
    publisher    = {GitHub},
    url = {https://pytorch.org/vision/stable/models.html}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv},
  year={2014}
}


@inproceedings{he2019bag,
  title={Bag of tricks for image classification with convolutional neural networks},
  author={He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{he2016identity,
  title={Identity mappings in deep residual networks},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={ECCV},
  year={2016}
}

@article{resnetv15,
  title = {ResNet v1.5 for PyTorch},
  author = {Nvidia},
  year={2019},
  url={https://catalog.ngc.nvidia.com/orgs/nvidia/resources/resnet_50_v1_5_for_pytorch}
}

@article{nvidiabert2019,
  title = {Deep Learning Examples, Language Modeling using BERT},
  author = {Nvidia},
  year={2019},
  url={https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT}
}

@inproceedings{szegedy2016rethinking,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{
micikevicius2017mixed,
title={Mixed Precision Training},
author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
booktitle={ICLR},
year={2018}
}

@inproceedings{graesser2022state,
  title={The State of Sparse Training in Deep Reinforcement Learning},
  author={Graesser, Laura and Evci, Utku and Elsen, Erich and Castro, Pablo Samuel},
  booktitle={ICML},
  year={2022}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={ECCV},
  year={2014}
}

@inproceedings{cordts2016cityscapes,
  title={The cityscapes dataset for semantic urban scene understanding},
  author={Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{lin2017focal,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{lin2017feature,
  title={{Feature Pyramid Networks for Object Detection}},
  author={Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{chen2018encoder,
  title={Encoder-decoder with atrous separable convolution for semantic image segmentation},
  author={Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{zhao2017pyramid,
  title={Pyramid scene parsing network},
  author={Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{dao2022monarch,
  title={Monarch: Expressive structured matrices for efficient and accurate training},
  author={Dao, Tri and Chen, Beidi and Sohoni, Nimit S and Desai, Arjun and Poli, Michael and Grogan, Jessica and Liu, Alexander and Rao, Aniruddh and Rudra, Atri and R{\'e}, Christopher},
  booktitle={ICML},
  year={2022}
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv},
  year={2020}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv},
  year={2021}
}

@inproceedings{rajpurkar2016squad,
  title={SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  booktitle={EMNLP},
  year={2016}
}

@article{curationcorpusbase2020,
  title={Curation Corpus Base},
  author={Curation},
  year={2020},
  url={https://github.com/CurationCorp/curation-corpus}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv},
  year={2017}
}

@inproceedings{convnext2023,
  author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle = {CVPR},
  title = {A ConvNet for the 2020s},
  year = 2022
}


@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  journal={Master's thesis, Department of Computer Science, University of Toronto},
  year={2009}
}

@article{devries2017improved,
  title={Improved regularization of convolutional neural networks with cutout},
  author={DeVries, Terrance and Taylor, Graham W},
  journal={arXiv},
  year={2017}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{sandler2018mobilenetv2,
  title={Mobilenetv2: Inverted residuals and linear bottlenecks},
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle={CVPR},
  year={2018}
}

@article{mmdetection,
  title   = {{MMDetection}: Open MMLab Detection Toolbox and Benchmark},
  author  = {Chen, Kai and Wang, Jiaqi and Pang, Jiangmiao and Cao, Yuhang and
             Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and
             Liu, Ziwei and Xu, Jiarui and Zhang, Zheng and Cheng, Dazhi and
             Zhu, Chenchen and Cheng, Tianheng and Zhao, Qijie and Li, Buyu and
             Lu, Xin and Zhu, Rui and Wu, Yue and Dai, Jifeng and Wang, Jingdong
             and Shi, Jianping and Ouyang, Wanli and Loy, Chen Change and Lin, Dahua},
  journal= {arXiv},
  year={2019}
}

@misc{mmseg2020,
    title={{MMSegmentation}: OpenMMLab Semantic Segmentation Toolbox and Benchmark},
    author={MMSegmentation Contributors},
    howpublished = {\url{https://github.com/open-mmlab/mmsegmentation}},
    year={2020}
}

@inproceedings{chen2021distilling,
  title={Distilling knowledge via knowledge review},
  author={Chen, Pengguang and Liu, Shu and Zhao, Hengshuang and Jia, Jiaya},
  booktitle={CVPR},
  year={2021}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv},
  year={2018}
}


@InProceedings{pmlr-v97-tan19a,
  title = 	 {{E}fficient{N}et: Rethinking Model Scaling for Convolutional Neural Networks},
  author =       {Tan, Mingxing and Le, Quoc},
  booktitle = 	 {ICML},
  year = 	 {2019},
}


@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI Blog},
  year={2019}
}

@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{
alabdulmohsin2023getting,
title={Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design},
author={Ibrahim Alabdulmohsin and Xiaohua Zhai and Alexander Kolesnikov and Lucas Beyer},
booktitle={NeurIPS},
year={2023}
}

@inproceedings{hubara2021accel,
 author = {Hubara, Itay and Chmiel, Brian and Island, Moshe and Banner, Ron and Naor, Joseph and Soudry, Daniel},
 booktitle = {NeurIPS},
 title = {Accelerated Sparse Neural Training: A Provable and Efficient Method to Find N:M Transposable Masks},
 year = {2021}
}


@InProceedings{Li_2019_ICCV,
author = {Li, Duo and Zhou, Aojun and Yao, Anbang},
title = {HBONet: Harmonious Bottleneck on Two Orthogonal Dimensions},
booktitle = {ICCV},
year = {2019}
}

@article{gale2019state,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv},
  year={2019}
}

@article{liu2022unreasonable,
  title={The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training},
  author={Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Shen, Li and Mocanu, Decebal Constantin and Wang, Zhangyang and Pechenizkiy, Mykola},
  journal={arXiv},
  year={2022}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{wang2020deep,
  title={Deep high-resolution representation learning for visual recognition},
  author={Wang, Jingdong and Sun, Ke and Cheng, Tianheng and Jiang, Borui and Deng, Chaorui and Zhao, Yang and Liu, Dong and Mu, Yadong and Tan, Mingkui and Wang, Xinggang and others},
  booktitle={TPAMI},
  year={2020}
}

@article{2020shooker,
  author = {{Hooker}, Sara},
  title = "{The Hardware Lottery}",
  journal={arXiv},
  year = 2020,
}   
              

@inproceedings{matsubara2021torchdistill,
  title={{torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation}},
  author={Matsubara, Yoshitomo},
  booktitle={International Workshop on Reproducible Research in Pattern Recognition},
  year={2021}
}


@article{kaplan2020scaling,
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Scaling Laws for Neural Language Models},
  journal = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{
ruiz2021scaling,
title={Scaling Vision with Sparse Mixture of Experts},
author={Carlos Riquelme Ruiz and Joan Puigcerver and Basil Mustafa and Maxim Neumann and Rodolphe Jenatton and Andr{\'e} Susano Pinto and Daniel Keysers and Neil Houlsby},
booktitle={NeurIPS},
year={2021}
}

@inproceedings{
micikevicius2018mixed,
title={Mixed Precision Training},
author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
booktitle={ICLR},
year={2018}
}

@inproceedings{
das2018mixed,
title={Mixed Precision Training of Convolutional Neural Networks using Integer Operations},
author={Dipankar Das and Naveen Mellempudi and Dheevatsa Mudigere and Dhiraj Kalamkar and Sasikanth Avancha and Kunal Banerjee and Srinivas Sridharan and Karthik Vaidyanathan and Bharat Kaul and Evangelos Georganas and Alexander Heinecke and Pradeep Dubey and Jesus Corbal and Nikita Shustrov and Roma Dubtsov and Evarist Fomenko and Vadim Pirogov},
booktitle={ICLR},
year={2018}
}

@article{barret2022stmoe,
  author = {Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus, William},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {ST-MoE: Designing Stable and Transferable Sparse Expert Models},
  journal = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{
hoffmann2022an,
title={An empirical analysis of compute-optimal large language model training},
author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katherine Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Oriol Vinyals and Jack William Rae and Laurent Sifre and et al.},
booktitle={NeurIPS},
year={2022}
}

@inproceedings{xiao20198bit,
 author = {Sun, Xiao and Choi, Jungwook and Chen, Chia-Yu and Wang, Naigang and Venkataramani, Swagath and Srinivasan, Vijayalakshmi (Viji) and Cui, Xiaodong and Zhang, Wei and Gopalakrishnan, Kailash},
 booktitle = {NeurIPS},
 title = {Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks},
 year = {2019}
}


@article{hoefler2022sparsity,
author = {Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
title = {Sparsity in Deep Learning: Pruning and Growth for Efficient Inference and Training in Neural Networks},
year = {2022},
abstract = {The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, sometimes even better than, the original dense networks. Sparsity promises to reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks. In this paper, we survey prior work on sparsity in deep learning and provide an extensive tutorial of sparsification for both inference and training. We describe approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. Our work distills ideas from more than 300 research papers and provides guidance to practitioners who wish to utilize sparsity today, as well as to researchers whose goal is to push the frontier forward. We include the necessary background on mathematical methods in sparsification, describe phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and show techniques for achieving acceleration on real hardware. We also define a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks. We close by speculating on how sparsity can improve future workloads and outline major open problems in the field.},
journal = {JMLR},
}

@inproceedings{
golubeva2021are,
title={Are wider nets better given the same number of parameters?},
author={Anna Golubeva and Guy Gur-Ari and Behnam Neyshabur},
booktitle={ICLR},
year={2021},
}


@InProceedings{zhu2019theory,
  title = 	 {A Convergence Theory for Deep Learning via Over-Parameterization},
  author =       {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle = 	 {ICML},
  year = 	 {2019},
}


@inproceedings{
neyshabur2018the,
title={The role of over-parametrization in generalization of neural networks},
author={Behnam Neyshabur and Zhiyuan Li and Srinadh Bhojanapalli and Yann LeCun and Nathan Srebro},
booktitle={ICLR},
year={2019},
}

@inproceedings{orseau2020log,
author = {Orseau, Laurent and Hutter, Marcus and Rivasplata, Omar},
title = {Logarithmic Pruning is All You Need},
year = {2020},
abstract = {The Lottery Ticket Hypothesis is a conjecture that every large neural network contains a subnetwork that, when trained in isolation, achieves comparable performance to the large network. An even stronger conjecture has been proven recently: Every sufficiently overparameterized network contains a subnetwork that, at random initialization, but without training, achieves comparable accuracy to the trained large network. This latter result, however, relies on a number of strong assumptions and guarantees a polynomial factor on the size of the large network compared to the target function. In this work, we remove the most limiting assumptions of this previous work while providing significantly tighter bounds: the overparameterized network only needs a logarithmic factor (in all variables but depth) number of neurons per weight of the target subnetwork.},
booktitle = {NeurIPS},
}

@inproceedings{eran2020proof,
author = {Malach, Eran and Yehudai, Gilad and Shalev-shwartz, Shai and Shamir, Ohad},
title = {Proving the Lottery Ticket Hypothesis: Pruning is All You Need},
year = {2020},
abstract = {The lottery ticket hypothesis (Frankle and Carbin, 2018), states that a randomly-initialized network contains a small subnetwork such that, when trained in isolation, can compete with the performance of the original network. We prove an even stronger hypothesis (as was also conjectured in Ramanujan et al., 2019), showing that for every bounded distribution and every target network with bounded weights, a sufficiently over-parameterized neural network with random weights contains a subnetwork with roughly the same accuracy as the target network, without any further training.},
booktitle = {ICML}
}


@inproceedings{mostafa2019dsr,
  title = 	 {Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization},
  author =       {Mostafa, Hesham and Wang, Xin},
  booktitle = 	 {ICML},
  abstract = 	 {Modern deep neural networks are typically highly overparameterized. Pruning techniques are able to remove a significant fraction of network parameters with little loss in accuracy. Recently, techniques based on dynamic reallocation of non-zero parameters have emerged, allowing direct training of sparse networks without having to pre-train a large dense model. Here we present a novel dynamic sparse reparameterization method that addresses the limitations of previous techniques such as high computational cost and the need for manual configuration of the number of free parameters allocated to each layer. We evaluate the performance of dynamic reallocation methods in training deep convolutional networks and show that our method outperforms previous static and dynamic reparameterization methods, yielding the best accuracy for a fixed parameter budget, on par with accuracies obtained by iteratively pruning a pre-trained dense model. We further investigated the mechanisms underlying the superior generalization performance of the resultant sparse networks. We found that neither the structure, nor the initialization of the non-zero parameters were sufficient to explain the superior performance. Rather, effective learning crucially depended on the continuous exploration of the sparse network structure space during training. Our work suggests that exploring structural degrees of freedom during training is more effective than adding extra parameters to the network.}
}


@article{tim2019snfs,
  author = {Dettmers, Tim and Zettlemoyer, Luke},
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Sparse Networks from Scratch: Faster Training without Losing Performance},
  journal = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{bellec2018deepr,
  author    = {Guillaume Bellec and
               David Kappel and
               Wolfgang Maass and
               Robert Legenstein},
  title     = {Deep Rewiring: Training very sparse deep networks},
  booktitle = {ICLR},
  year      = {2018}
}

@inproceedings{etai2020colleg,
title = {Collegial ensembles},
abstract = {Modern neural network performance typically improves as model size increases. A recent line of research on the Neural Tangent Kernel (NTK) of over-parameterized networks indicates that the improvement with size increase is a product of a better conditioned loss landscape. In this work, we investigate a form of over-parameterization achieved through ensembling, where we define collegial ensembles (CE) as the aggregation of multiple independent models with identical architectures, trained as a single model. We show that the optimization dynamics of CE simplify dramatically when the number of models in the ensemble is large, resembling the dynamics of wide models, yet scale much more favorably. We use recent theoretical results on the finite width corrections of the NTK to perform efficient architecture search in a space of finite width CE that aims to either minimize capacity, or maximize trainability under a set of constraints. The resulting ensembles can be efficiently implemented in practical architectures using group convolutions and block diagonal layers. Finally, we show how our framework can be used to analytically derive optimal group convolution modules originally found using expensive grid searches, without having to train a single model.},
author = {Etai Littwin and Ben Myara and Sima Sabah and Joshua Susskind and Shuangfei Zhai and Oren Golan},
year = {2020},
booktitle ={NeurIPS}
}

@article{lasby2023dynamic,
      title={Dynamic Sparse Training with Structured Sparsity}, 
      author={Mike Lasby and Anna Golubeva and Utku Evci and Mihai Nica and Yani Ioannou},
      year={2023},
      journal={arXiv}
}

@inproceedings{chen2020lottery,
    title={The Lottery Ticket Hypothesis for Pre-trained BERT Networks},
    author={Tianlong Chen and Jonathan Frankle and Shiyu Chang and Sijia Liu and Yang Zhang and Zhangyang Wang and Michael Carbin},
    year = {2020},
    booktitle ={NeurIPS}
}

@inproceedings{shuxuan2020expandnet,
author = {Guo, Shuxuan and Alvarez, Jose M. and Salzmann, Mathieu},
title = {ExpandNets: Linear over-Parameterization to Train Compact Convolutional Networks},
year = {2020},
abstract = {We introduce an approach to training a given compact network. To this end, we leverage over-parameterization, which typically improves both neural network optimization and generalization. Specifically, we propose to expand each linear layer of the compact network into multiple consecutive linear layers, without adding any nonlinearity. As such, the resulting expanded network, or ExpandNet, can be contracted back to the compact one algebraically at inference. In particular, we introduce two convolutional expansion strategies and demonstrate their benefits on several tasks, including image classification, object detection, and semantic segmentation. As evidenced by our experiments, our approach outperforms both training the compact network from scratch and performing knowledge distillation from a teacher. Furthermore, our linear over-parameterization empirically reduces gradient confusion during training and improves the network generalization.},
booktitle = {NeurIPS}
}

@inproceedings{fuzhao2022gowide,
  author    = {Fuzhao Xue and
               Ziji Shi and
               Futao Wei and
               Yuxuan Lou and
               Yong Liu and
               Yang You},
  title     = {Go Wider Instead of Deeper},
  booktitle = {AAAI},
  year      = {2022},
}


@inproceedings{
chen2022sparsity,
title={Sparsity Winning Twice: Better Robust Generalization from More Efficient Training},
author={Tianlong Chen and Zhenyu Zhang and pengjun wang and Santosh Balachandra and Haoyu Ma and Zehao Wang and Zhangyang Wang},
booktitle={ICLR},
year={2022}
}

@ARTICLE{jinming2022doconv,
  author={Cao, Jinming and Li, Yangyan and Sun, Mingchao and Chen, Ying and Lischinski, Dani and Cohen-Or, Daniel and Chen, Baoquan and Tu, Changhe},
  journal={IEEE Transactions on Image Processing},
  title={DO-Conv: Depthwise Over-Parameterized Convolutional Layer},
  year={2022}
}


@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv},
  year={2015}
}

@inproceedings{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={NeurIPS},
  year={2015}
}

@inproceedings{tanaka2020pruning,
  title={Pruning neural networks without any data by iteratively conserving synaptic flow},
  author={Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel L and Ganguli, Surya},
  booktitle={NeurIPS},
  year={2020}
}

@article{de2020progressive,
  title={Progressive skeletonization: Trimming more fat from a network at initialization},
  author={de Jorge, Pau and Sanyal, Amartya and Behl, Harkirat S and Torr, Philip HS and Rogez, Gregory and Dokania, Puneet K},
  journal={arXiv},
  year={2020}
}

@article{wang2020picking,
  title={Picking winning tickets before training by preserving gradient flow},
  author={Wang, Chaoqi and Zhang, Guodong and Grosse, Roger},
  journal={arXiv},
  year={2020}
}

@article{lee2018snip,
  title={Snip: Single-shot network pruning based on connection sensitivity},
  author={Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip HS},
  journal={arXiv},
  year={2018}
}

@inproceedings{raihan2020sparse,
  title={Sparse weight activation training},
  author={Raihan, Md Aamir and Aamodt, Tor},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{frankle2020linear,
  title={Linear mode connectivity and the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle={ICML},
  year={2020}
}

@article{nakkiran2021deep,
  title={Deep double descent: Where bigger models and more data hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  year={2021}
}

@article{goyal2022vision,
  title={Vision models are more robust and fair when pretrained on uncurated images without supervision},
  author={Goyal, Priya and Duval, Quentin and Seessel, Isaac and Caron, Mathilde and Singh, Mannat and Misra, Ishan and Sagun, Levent and Joulin, Armand and Bojanowski, Piotr},
  journal={arXiv},
  year={2022}
}

@article{mobileone2022,
  title={An Improved One millisecond Mobile Backbone},
  author={Vasu, Pavan Kumar Anasosalu and Gabriel, James and Zhu, Jeff and Tuzel, Oncel and Ranjan, Anurag},
  journal={arXiv},
  year={2022}
}

@inproceedings{du2022glam,
  title={Glam: Efficient scaling of language models with mixture-of-experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  booktitle={ICML},
  year={2022}
}

@article{lepikhin2020gshard,
  title={Gshard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv},
  year={2020}
}


@article{liu2022more,
  title={More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity},
  author={Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Chen, Xuxi and Xiao, Qiao and Wu, Boqian and Pechenizkiy, Mykola and Mocanu, Decebal and Wang, Zhangyang},
  journal={arXiv},
  year={2022}
}

@article{iulia2019bertsmall,
  author = {Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},
  journal = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{zhu2015bookc,
author = {Y. Zhu and R. Kiros and R. Zemel and R. Salakhutdinov and R. Urtasun and A. Torralba and S. Fidler},
booktitle = {ICCV},
title = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
year = {2015},
abstract = {Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.}
}

@article{Zhu2017ToPO,
  title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author={Michael Zhu and Suyog Gupta},
  journal={arXiv},
  year={2017},
}

@inproceedings{
ma2022effective,
title={Effective Model Sparsification by Scheduled Grow-and-Prune Methods},
author={Xiaolong Ma and Minghai Qin and Fei Sun and Zejiang Hou and Kun Yuan and Yi Xu and Yanzhi Wang and Yen-Kuang Chen and Rong Jin and Yuan Xie},
booktitle={ICLR},
year={2022}
}

@inproceedings{hattie2019supermask,
 author = {Zhou, Hattie and Lan, Janice and Liu, Rosanne and Yosinski, Jason},
 booktitle = {NeurIPS},
 title = {Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask},
 year = {2019}
}

@article{nvidia2023gpuperf,
  author = {Nvidia},
  title = {Nvidia Performance Documentation},
  url = {https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html},
  year = {2023}
}


@misc{lie_2021,
	author = {Sean Lie},
	title = {Thinking Outside the Die: Architecting the ML Accelerator of the Future},
  publisher={MICRO 2021 Keynote}, 
	howpublished = {\url{https://www.microarch.org/micro54/media/lie-keynote.pdf}},
	year = {2021}
}

@inproceedings{pyramidvit2021,
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  booktitle={ICCV}, 
  title={Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions}, 
  year={2021}
}

@inproceedings{scalablevit2021,
  author={Pan, Zizheng and Zhuang, Bohan and Liu, Jing and He, Haoyu and Cai, Jianfei},
  booktitle={ICCV}, 
  title={Scalable Vision Transformers with Hierarchical Pooling}, 
  year={2021}
}

@inproceedings{cvt2021,
  author={Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
  booktitle={ICCV}, 
  title={CvT: Introducing Convolutions to Vision Transformers}, 
  year={2021},
}


@article{lie_2022, 
 title={Cerebras Architecture Deep Dive: First look inside the HW/SW co-design for Deep Learning}, 
 url={https://www.cerebras.net/blog/cerebras-architecture-deep-dive-first-look-inside-the-hw/sw-co-design-for-deep-learning}, 
 journal={Cerebras Systems Blog}, publisher={Cerebras Systems Inc.}, 
 author={Lie, Sean}, 
 year={2022}, 
 month={Dec}
 } 

@article{huang2023dynamic,
      title={Dynamic Sparse Training via Balancing the Exploration-Exploitation Trade-off}, 
      author={Shaoyi Huang and Bowen Lei and Dongkuan Xu and Hongwu Peng and Yue Sun and Mimi Xie and Caiwen Ding},
      year={2023},
      journal={arXiv}
}

@article{lie_2023, 
 title={Train a Model with Weight Sparsity}, 
 url={https://docs.cerebras.net/en/2.1.1/wsc/how_to_guides/sparsity.html}, 
 journal={Cerebras Wafer-Scale cluster (R2.1.1) Documentation}, publisher={Cerebras Systems Inc.}, 
 author={Cerebras}, 
 year={2023}, 
 month={Jan}
 } 


 @article{elsen2019sparse,
  author = {Elsen, Erich and Dukhan, Marat and Gale, Trevor and Simonyan, Karen},
  title = {Fast Sparse ConvNets},
  journal = {arXiv},
  year = {2019}
}

@inproceedings{Ashby2019ExploitingUS,
  title={Exploiting Unstructured Sparsity on Next-Generation Datacenter Hardware},
  author={Mike Ashby and Christiaan Baaij and Peter Baldwin and Martijn Bastiaan and Oliver Bunting and Aiken Cairncross and Christopher Chalmers and Liz Corrigan and Sam Davis and Nathan van Doorn and Jon Fowler and Graham Hazel and Basile Henry and David Page and Jonny Shipton and Shaun. Steenkamp},
  year={2019}
}

@misc{neural_magic_2021, 
 title={DeepSparse }, 
 url={https://github.com/neuralmagic/deepsparse}, 
 journal={GitHub}, 
 author={NeuralMagic}, 
 year={2021}} 

@InProceedings{
    pmlrv119kurtz20a,
    title = {Inducing and Exploiting Activation Sparsity for Fast Inference on Deep Neural Networks},
    author = {Kurtz, Mark and Kopinsky, Justin and Gelashvili, Rati and Matveev, Alexander and Carr, John and Goin, Michael and Leiserson, William and Moore, Sage and Nell, Bill and Shavit, Nir and Alistarh, Dan},
    booktitle = {ICML},
    year = {2020},
    pdf = {http://proceedings.mlr.press/v119/kurtz20a/kurtz20a.pdf},
}

@article{neuralmagic2021,
  author    = {Eugenia Iofinova and
               Alexandra Peste and
               Mark Kurtz and
               Dan Alistarh},
  title     = {How Well Do Sparse Imagenet Models Transfer?},
  journal   = {CoRR},
  volume    = {abs/2111.13445},
  year      = {2021},
  eprinttype = {arXiv},
  eprint    = {2111.13445},
  timestamp = {Wed, 01 Dec 2021 15:16:43 +0100},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{liehotchips,
  author={Lie, Sean},
  booktitle={2022 IEEE Hot Chips 34 Symposium (HCS)}, 
  title={Cerebras Architecture Deep Dive: First Look Inside the HW/SW Co-Design for Deep Learning : Cerebras Systems}, 
  year={2022},
  volume={},
  number={},
}

@article{lie2023cerebras,
  title={Cerebras architecture deep dive: First look inside the hardware/software co-design for deep learning},
  author={Lie, Sean},
  journal={IEEE Micro},
  year={2023},
  publisher={IEEE}
}

@inproceedings{yuan2021mest,
  title={MEST: Accurate and Fast Memory-Economic Sparse Training Framework on the Edge},
  author={Yuan, Geng and Ma, Xiaolong and Niu, Wei and Li, Zhengang and Kong, Zhenglun and Liu, Ning and Gong, Yifan and Zhan, Zheng and He, Chaoyang and Jin, Qing and others},
  journal={NeurIPS},
  volume={34},
  year={2021}
}

@inproceedings{tai2022spartan,
    title={{Spartan: Differentiable Sparsity via Regularized Transportation}},
    author={Tai, Kai Sheng and Tian, Taipeng and Lim, Ser-Nam},
    booktitle={NeurIPS},
    year={2022}
}

@inproceedings{liu2021sparse, 
title={Sparse Training via Boosting Pruning Plasticity with Neuroregeneration}, 
author={Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Atashgahi, Zahra and Yin, Lu and Kou, Huanyu and Shen, Li and Pechenizkiy, Mykola and Wang, Zhangyang and Mocanu, Decebal Constantin}, 
journal={NeurIPS},
year={2021}
}


@inproceedings{LiuLSHYZ17,
  author = {Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
  booktitle = {ICCV},
  title = {Learning Efficient Convolutional Networks through Network Slimming.},
  year = 2017
}

@inproceedings{MolchanovTKAK17,
  author       = {Pavlo Molchanov and
                  Stephen Tyree and
                  Tero Karras and
                  Timo Aila and
                  Jan Kautz},
  title        = {Pruning Convolutional Neural Networks for Resource Efficient Inference},
  booktitle    = {ICLR},
  year         = {2017},
}

@misc{open-llm-leaderboard,
  author = {Edward Beeching and Cl√©mentine Fourrier and Nathan Habib and Sheon Han and Nathan Lambert and Nazneen Rajani and Omar Sanseviero and Lewis Tunstall and Thomas Wolf},
  title = {Open LLM Leaderboard},
  year = {2023},
  publisher = {Hugging Face},
  howpublished = "\url{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard}"
}
@software{eval-harness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = sep,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628}
}
@misc{clark2018think,
      title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
      author={Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
      year={2018},
      eprint={1803.05457},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@misc{zellers2019hellaswag,
      title={HellaSwag: Can a Machine Really Finish Your Sentence?},
      author={Rowan Zellers and Ari Holtzman and Yonatan Bisk and Ali Farhadi and Yejin Choi},
      year={2019},
      eprint={1905.07830},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{hendrycks2021measuring,
      title={Measuring Massive Multitask Language Understanding},
      author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2009.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}
@misc{lin2022truthfulqa,
      title={TruthfulQA: Measuring How Models Mimic Human Falsehoods},
      author={Stephanie Lin and Jacob Hilton and Owain Evans},
      year={2022},
      eprint={2109.07958},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{DBLP:journals/corr/abs-1907-10641,
      title={{WINOGRANDE:} An Adversarial Winograd Schema Challenge at Scale},
      author={Keisuke Sakaguchi and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
      year={2019},
      eprint={1907.10641},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{DBLP:journals/corr/abs-2110-14168,
      title={Training Verifiers to Solve Math Word Problems},
      author={Karl Cobbe and
                  Vineet Kosaraju and
                  Mohammad Bavarian and
                  Mark Chen and
                  Heewoo Jun and
                  Lukasz Kaiser and
                  Matthias Plappert and
                  Jerry Tworek and
                  Jacob Hilton and
                  Reiichiro Nakano and
                  Christopher Hesse and
                  John Schulman},
      year={2021},
      eprint={2110.14168},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
