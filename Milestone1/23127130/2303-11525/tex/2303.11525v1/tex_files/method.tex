\section{Method}
\label{sec:method}

\begin{figure*}
\includegraphics[width=\textwidth]{./figures/sift_family.pdf}

\vspace{-0.05in}
\caption{Different members of the SIFT family. Transformation of all members is
parameterized by a single hyperparameter (i.e., sparsity level ($s$)). Black and
white squares denote sparse and active weights, respectively. Green block
indicates a non-linear activation function (e.g., BatchNorm, ReLU, LayerNorm).
All transformations are derived with sparsity set to $50\%$ as an example, are
Iso-FLOP to the dense feedforward function $f_{\theta_l}$, and hence can be used
as a drop-in replacement of $f_{\theta_l}$. As shown in the figure, FLOPs spent
in a dense matrix multiplication can be utilized to enhance the representational
capacity of the feedforward function using unstructured sparsity. See
Section~\ref{subsec:members_of_sift} for more details about each member.}
\label{fig:diff_members_of_sift}
\vspace{-0.1in}
\end{figure*}


In this section, we present our method to improve training efficiency. We first
explain our intuition and hypotheses, followed by our methodology.


\subsection{Training with Dense Matrices is FLOP Inefficient}
Prior works have shown that modern DNNs are overparameterized and that the
features and weights learned at each layer are sparse. Recent work of Lottery
Ticket Hypothesis (LTH)~\cite{frankle2018lottery} demonstrates that sparse DNNs
can be trained to the same accuracy as their dense counterparts, as long as one
seeds the training with a good sparsity mask (termed as ``lottery ticket'').
These works indicate that the optimal set of weights in a DNN is sparse.
Therefore, representing these weights as dense matrices throughout training is
FLOP inefficient, and training with sparse matrices should be more efficient.
However, in practice, most sparse training methods obtain worse accuracy than
dense baseline. We hypothesize this is due to the inefficiency of searching for
``lottery tickets'' in single-shot training.


While sparse models reduce the FLOPs needed per step, we hypothesize that
existing sparse training methods make sub-optimal use of these computational
savings. For example, SOTA sparse training methods~\cite{jayakumar2020top,
evci2020rigging} invest these FLOP savings into longer training schedules to
close the accuracy gap and compensate for the inability to discover an optimal
mask earlier in training. This setup is inefficient since it ultimately requires
more training FLOPs than the dense baseline to reach the same target accuracy.
In our work, we take an orthogonal approach and invest these FLOP savings into
(a) increasing the representational capacity of a layer and (b) increasing its
search space, which we hypothesize can facilitate the discovery of an optimal
sparse mask~\cite{ramanujan2020s, stosic2021search}. We do this by replacing
dense transformations with FLOP-equivalent sparse transformations. We denote
these transformations as the Sparse Iso-FLOP Transformation (SIFT) family.


\subsection{Setup}
For clarity, we will explain our method for a fully connected neural network. In
Appendix~\ref{app:sift_conv}, we detail the straightforward extension of our
method to convolutional layers.
%$f(x; \theta)$
Let $\mathcal{N}$ denote a $L$ layered DNN parameterized by
$\Theta_{\mathcal{N}}$. Let $\Theta_{\mathcal N} \in \{\theta_1, ...,
\theta_L\}$ denote the parameters of the DNN. The output of the $l$-th layer is
defined as: $z_l = \sigma (f_{\theta_l}(z_{l-1}))$ for some activation function
$\sigma$ (e.g., ReLU~\cite{nair2010rectified}) and feedforward function
$f_{\theta_l}$. Specifically, let $f_{\theta_l}(z_{l-1}) = \theta_l^T z_{l-1}$,
where $\theta_l \in \mathbb{R}^{D_{in} \times D_{out}}$,  $z_{l-1} \in
\mathbb{R}^{D_{in} \times B}$ and $B$, $D_{in}$, $D_{out}$ denote the
batch-size, input, and output dimensionality of features respectively. The total
FLOPs needed for $f_{\theta_l}$ are given by $B{\cdot}D_{in}{\cdot}D_{out}$.


%% Version 1
\subsection{Sparse Iso-FLOP Transformations}
In the standard setup, the feedforward function $f_{\theta_l}$ computes the
output features as a linear transformation of input features. From a theoretical
perspective, the feedforward function can make use of arbitrary non-linear
transformations. However, in practice, most transformations are expressed as
dense matrix multiplications due to widespread support on
GPUs~\citep{nvidia2023gpuperf}.

As stated before, we are interested in improving the training efficiency of
DNNs, by enhancing the representational capacity of the feedforward function.
Naively increasing the representational capacity by stacking more
layers~\cite{lin2014network}, increasing width~\cite{zagoruyko2016wide}, mixture
of experts~\cite{shazeer2016outrageously}, etc. increases the computational
FLOPs. In our work, we use unstructured sparsity in weight matrices and ensure
that the FLOPs of the transformation are the same as that of a dense feedforward
function.
%% Version 1
Let $\Psi_l$ denote the set of Sparse Iso-FLOP Transformations (SIFT) for a
particular layer $l$:
\begin{equation*}
\Psi_l: \{ \psi_l(s),  0 \leq s < 1,  g(\psi_l) \approx g(f_{\theta_l}) \},
\end{equation*}
where $\psi_l$ is a transformation, $s$ represents the sparsity level, and
$g(.)$ returns the computational FLOPs. Each transformation in this set
satisfies the following properties: (1) the computational FLOPs of the
transformation $\psi_l$ are same as that of dense transformation $f_{\theta_l}$,
and (2) the transformation is parameterized by a single hyperparameter - the
sparsity level. Since these transformations are Iso-FLOP to the dense
feedforward function, we can use them as drop-in replacements without affecting
the FLOPs of a layer. While many FLOP-equivalent transformations fall under the
SIFT family, in this work, we detail four different members: Sparse Wide, Sparse
Parallel, Sparse Factorized, and Sparse Doped.
%, followed by experimental results demonstrating their usefulness in practice.


\subsection{Members of SIFT}
\label{subsec:members_of_sift}

\paragraph{Sparse Wide}

The sparse wide transformation augments the representational capacity of a layer
 by increasing the number of output features while keeping $s$ fraction of
 weights sparse. When using this transformation, we widen the input and output
 features for all the $L$ layers of the network with the same widening factor,
 $k_{sw}$, to avoid a mismatch in feature dimensionality across layers. Let
 $\theta_l^{sw} \in \mathbb{R}^{k_{sw}{\cdot}D_{in} \times
 k_{sw}{\cdot}D_{out}}$ denote the transformation matrix, with $s$ fraction of
 weights being sparse. Since the fraction of non-sparse weights is given by
 $1-s$, the FLOPs required by this transformation are
 $B{\cdot}(k_{sw}{\cdot}D_{in}){\cdot}(k_{sw}{\cdot}D_{out}){\cdot}(1-s)$.
 Setting these equal to the FLOPs of the original dense $f_{\theta_l}$, we
 obtain the widening factor $k_{sw} = \sqrt{\frac{1}{(1-s)}}$. If we set the
 sparsity $s$ to $0$, we obtain $k_{sw}$ as $1$ and recover the original dense
 feedforward function.


\paragraph{Sparse Parallel} The sparse parallel transformation replaces the
feedforward function with a sum of $k_{sp}$ non-linear functions. Let
$\theta_l^{sp} \in \{ \theta_l^{sp, 1}, ..., \theta_l^{sp, k_{sp}}\}$ denote the
parameters of this transformation, where $\theta_l^{sp, j} \in
\mathbb{R}^{D_{in}\times D_{out}}$ denotes the transformation matrix of $j^{th}$
function, where $s$ fraction of weights are sparse. The sparse parallel
transformation in this case is $\psi^{sp}_l =
\sum^{k_{sp}}_{j=1}\sigma((\theta_l^{sp, j})^{T} z_{l})$, where $\sigma$ is a
non linear function. In practice, $\psi^{sp}_l$ is implemented as a layer with
$k_{sp}$ parallel branches. The computational FLOPs of this transformation is
$k_{sp}{\cdot}B{\cdot}D_{in}{\cdot}D_{out}{\cdot}(1-s)$. Setting these FLOPs
equal to FLOPs of $f_{\theta}$, we obtain $k_{sp} = \frac{1}{(1-s)}$. Note, at
$s=0$, the number of parallel branches $k_{sp}$ is $1$. If we replace the
non-linear function $\sigma$ with Identity, we can recover the original dense
feedforward transformation.


\paragraph{Sparse Factorized}
The transformation matrix of the feedforward function $f_{\theta_l}$ is denoted
by $\theta_l \in \mathbb{R}^{D_{in} \times D_{out}}$. Multiple works have
explored matrix factorization techniques to express the transformation matrix
$\theta_l$ as a product of two matrices $\theta_l = UV^T$, where $U \in
\mathbb{R}^{D_{in} \times d}$, $V \in \mathbb{R}^{D_{out} \times
d}$.~\citet{khodak2020initialization, tai2016convolutional}
and~\citet{chen2021drone} have explored low-rank factorization ($d << D_{out}$)
as a form of structured sparsity to improve training and inference efficiency,
while~\citet{arora2018optimization} and~\citet{guo2020expandnets} have explored
overparameterized factorizations for better generalization and faster
convergence. In contrast, we use factorization to augment the representational
capacity without decreasing or increasing the FLOPs. More precisely, let
$\theta_l^{sf} \in \{ U_l, V_l \}$ denote the parameters of this transformation,
where $U_l \in \mathbb{R}^{D_{in} \times d_{sf}}$, $V_l \in \mathbb{R}^{d_{sf}
\times D_{out}}$ are sparse matrices with $s$ fraction of their weights being
sparse. The functional transformation in this case is $\psi^{sf}_l =
V_l^T\sigma(U_l^Tz_l)$. The computational FLOPs of this transformation is
$d_{sf}{\cdot}B{\cdot}(D_{in} + D_{out}){\cdot}(1-s)$. Setting these FLOPs equal
to FLOPs of $f_{\theta_l}$, we obtain $d_{sf} =
\frac{D_{in}{\cdot}D_{out}}{(D_{in} + D_{out}){\cdot}(1-s)}$. Note, setting
sparsity $s=0$, we recover a non-linear low-rank factorization with dense
matrices.


\paragraph{Sparse Doped} family of transformation is inspired by works
\cite{chen2021scatterbrain, thakker2021doping, udell2019big, candes2011robust}
which approximate a dense matrix with a combination of low-rank factorization
and sparse matrix. In our work, we replace the feedforward function with
low-rank factorization (with rank $d_{sd}$) and an unstructured sparse weight
matrix (with sparsity $s$). Let $U_l \in \mathbb{R}^{D_{in} \times d_{sd}}, V_l
\in \mathbb{R}^{d_{sd} \times D_{out}}$ denote the low-rank matrices, and
$\theta_l^{sd} \in \mathbb{R}^{D_{in} \times D_{out}}$ denote the matrix with
unstructured sparsity. The functional transformation, in this case, is given by
$\psi^{sd}_l = V_l^T(U_l^Tz_l) + \sigma((\theta_l^{sd})^Tz_l)$. The
computational FLOPs associated with this transformation are
$B{\cdot}d_{sd}{\cdot}(D_{in} + D_{out}) +
(1-s){\cdot}B{\cdot}D_{in}{\cdot}D_{out}$. Setting these FLOPs equal to FLOPs of
$f_{\theta_l}$, we obtain $d_{sd} = \frac{s{\cdot}D_{in}{\cdot}D_{out}}{(D_{in}
+ D_{out})}$. Note, as $s \to 0$ and $d_{sd} \to 0$, the low-rank component of
the transformation disappears, and we can recover the dense feedforward function
as a special case by setting $\sigma$ to Identity.


\subsection{Cardinality of Search Space}\label{subsec:cardinality} One of our
hypotheses is that increasing the search space of the sparsity mask via SIFT can
make training more efficient. Results from past work support this
hypothesis.~\citet{ramanujan2020s} demonstrate that the odds of finding a
lottery ticket in a randomly initialized network increase with the width of a
network.~\citet{liu2022unreasonable} and~\citet{stosic2021search} show that
increasing the search space by increasing width or depth improves accuracy. In
our work, we define the cardinality of a search space as the number of weights a
sparse training method can explore.
Table~\ref{tab:nature_of_sift_transformations} characterizes the cardinality of
search space for each member of SIFT family. The search space for Sparse Wide,
Sparse Parallel, and Sparse Factorized transformations increase proportional to
the width scaling factor, number of parallel branches, and size of intermediate
hidden dimension, respectively. Sparse Doped transformation splits its
computational FLOPs between low-rank factorization and unstructured sparse
weight matrix. The size of the unstructured weight matrix is invariant to
sparsity; thus cardinality of search space for this transformation is constant.


\begin{table}[ht]
    \caption{Cardinality of search space of sparsity mask for different members
of SIFT family.}\vskip 1pt
    \centering
    \resizebox{0.98\columnwidth}{!}{
    \begin{small}
    \begin{sc}
    \begin{tabular}{cc}
        \toprule
        Transformation & Cardinality of Search Space \\ \midrule
        Sparse Wide        & $(k_{sw})^2{\cdot}(D_{in}{\cdot}D_{out})$ \\
        Sparse Parallel    &  $k_{sp}{\cdot}(D_{in}{\cdot}D_{out})$    \\
        Sparse Factorized  & $d_{sf}{\cdot}(D_{in} + D_{out})$   \\
        Sparse Doped       & $D_{in}{\cdot}D_{out}$            \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    }
% \vspace{-0.2in}
\label{tab:nature_of_sift_transformations}
\end{table}
