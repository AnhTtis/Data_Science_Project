\section{Experiments~\label{sec:experiments}}

In this section, we demonstrate how transformations from the SIFT Family lead to
improvements across a variety of different tasks in the CV and NLP domains.
First, in section~\ref{subsec:results_ablations}, we describe the experimental
setups and validate the design choices through multiple ablation studies on
CIFAR-100~\cite{krizhevsky2009learning}, followed by results on
ImageNet~\cite{krizhevsky2012imagenet}. Then, in
section~\ref{subsec:transfer_learning}, we highlight the advantages of
pre-training with SIFT through gains on downstream tasks. Next, we present the
benefits of SIFT in the NLP domain by demonstrating results on
BERT~\cite{devlin2018bert} and GPT~\cite{brown2020language} in
section~\ref{subsec:nlp_results}. Finally in section~\ref{subsec:wall_clock}, we
show speed-ups during training and inference with unstructured sparsity,
measured in wall clock time. Unless stated otherwise, the results presented
below are obtained by replacing all dense layers with a given SIFT
transformation while only tuning the sparsity level. All sparse models are
trained using a uniform sparsity distribution (i.e., all layers have the same
sparsity level). We adopt the default hyperparameters from
RigL~\cite{evci2020rigging} for dynamic sparsity. More details about the setup
can be found in Appendix~\ref{app:pt-implement}.


\subsection{CV Implementation Details}

We evaluate our method on CIFAR-100 and ImageNet using convolutional networks
and hybrid Vision Transformer (ViT) networks. We follow published training
settings for CIFAR-100~\cite{devries2017improved} and ImageNet~\cite{resnetv15}.
For both datasets, we follow the standard evaluation procedures and report the
top-1 accuracy. Details for model architectures, datasets, and training
hyperparameters are given in Appendix \ref{app:pt-implement}.


\subsection{Results and Ablations on CIFAR-100}
\label{subsec:results_ablations}
In this section, we conduct various ablations to validate our design choices.
Unless stated otherwise, all experiments below are with ResNet-18 architecture
on CIFAR-100.


\begin{table}[]
    \caption{Evaluation of SIFT Sparse Wide family using various sparse training
methods with ResNet-18 on CIFAR-100 across different values of sparsity
(columns). Best accuracy for each sparse training method is highlighted in bold.
} \vskip 1pt
    \centering

\resizebox{\columnwidth}{!}{
    \begin{small}
    \begin{sc}
    \begin{tabular}{c|cccc}
        \toprule
        Dense 		&
\begin{tabular}[c]{@{}c@{}} Sparse \\ Training Method\end{tabular} & 0.50  &
0.75 & 0.90  \\
	\midrule
 \multirow{3}{*}{77.0 $\pm$ 0.2}
		& Static    & \textbf{78.5}  & 78.3 &  78.2 \\
		&  SET      & 78.8  & 79.2 &  \textbf{79.8} \\
		&  RigL     & 79.1  & 79.5 &  \textbf{80.1} \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
}

\label{tab:dynamic_sparsity_importance}
% \vspace{-0.1in}
\end{table}

\paragraph{Importance of Dynamic Sparsity}
All members of the SIFT family utilize transformations with unstructured
sparsity. This study investigates the importance of the sparse training method
when training SIFT transformed architectures. For this analysis, we focus on the
Sparse Wide transformation and evaluate it with transformations obtained with
sparsity $\in $ \{50\%, 75\%, 90\%\} using three sparse training methods: static
sparsity, SET~\cite{mocanu2018} and RigL~\cite{evci2020rigging}. RigL and SET
are dynamic sparse training methods in which the sparsity mask evolves during
training. The key difference is that RigL updates the mask based on gradient
information, whereas SET updates the mask randomly. Results of our ablation are
documented in Table~\ref{tab:dynamic_sparsity_importance}. Here, the following
trends can be observed: 1) the Sparse Wide transformation outperforms dense
baselines across all operating points (sparsity and sparse training method), 2)
dynamic sparse training methods (RigL and SET) obtain higher accuracies compared
to training with static sparsity, and 3) gains with static sparsity plateau at
lower levels of sparsity, while dynamic sparse training methods gain accuracy at
higher sparsities. As mentioned in Section~\ref{subsec:cardinality}, SIFT
transformations increase the search space $\propto$ sparsity. Dynamic sparse
training methods can explore and exploit this increased search
space~\cite{stosic2021search} and therefore outperform training with static
sparsity. Out of the two dynamic sparse training methods evaluated in our study,
RigL consistently outperforms SET. Therefore, we use RigL as our sparse training
method for all the experiments reported below.


\paragraph{Importance of Using Non-Linear Activations}
Some members of the SIFT family are inspired by recent works which
overparameterize the feedforward function during training and fold it back into
a single dense matrix post training~\cite{ding2021repvgg, ding2021diverse,
guo2020expandnets, ding2019acnet}. Although these works show the benefits of
linear overparameterization, this comes at the cost of a significant increase in
training FLOPs. In contrast, while we also increase the representational
capacity of the feedforward function, we do so with an Iso-FLOP transformation.
Since we remain Iso-FLOP to the original dense model, we do not require
post-training modifications to collapse weight matrices for inference
efficiency. This uniquely allows us to use non-linearities (e.g., ReLU) in our
SIFT transformations to enhance the representational capacity of the network
further. We validate the importance of this design choice by training ResNet-18
with SIFT Sparse Factorized transformations with and without non-linearities,
and observe significant accuracy gains across all sparsity levels when using
non-linear activations. For example, at 90\% Sparse Factorized, using
non-linearity, we see a 1.8\% gain in test accuracy over the ResNet-18 CIFAR-100
dense baseline, compared to a drop of 0.5\% without it. These findings hold for
other members of SIFT family as well (see
Appendix~\ref{app:nonlinear-importance} for more details).

\begin{table}
    \caption{Evaluation of SIFT families on CIFAR-100 with ResNet-18 model
across different values of sparsity (columns). Best accuracy of each
transformation is highlighted in bold. All members of SIFT outperform the dense
baseline by a significant margin.} \vskip 1pt
    \centering
\resizebox{\columnwidth}{!}{
    \begin{small}
    \begin{sc}
    \begin{tabular}{c|cccc}
        \toprule
	Dense	    &  Transformation     & 0.50   & 0.75  & 0.90 \\
	\midrule
  \multirow{4}{*}{77.0 $\pm$ 0.2} & Sparse Wide        & 79.1 & 79.5 &
  \textbf{80.1} \\
        			  & Sparse Factorized  & 77.8 & 78.4 & \textbf{78.9} \\
        			  & Sparse Parallel    & 77.9 & \textbf{79.1} & 78.2 \\
        			  & Sparse Doped       & \textbf{78.2} & 77.8 & 76.9 \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
}
\label{tab:cifar_sift_results}
% \vspace{-0.1in}
\end{table}

\paragraph{SIFT with ResNet-18} In the preceding paragraphs, we validate the
design choices for our method (i.e., the importance of dynamic sparsity and
non-linearity). Now, we evaluate different members of the SIFT family on
ResNet-18 and CIFAR-100 across different sparsity levels.
Table~\ref{tab:cifar_sift_results} highlights the best accuracy achieved by each
member of the SIFT family. Compared to the accuracy of the dense baseline
(77\%), all SIFT members obtain significant accuracy improvements using the same
FLOPs as the dense model. We note that the Sparse Doped transformation is the
only member of the SIFT family which does not gain accuracy at higher levels of
sparsity. We hypothesize that this phenomenon occurs due to two reasons: 1)
cardinality of the search space of the sparsity mask does not increase with
sparsity level (see Table~\ref{tab:nature_of_sift_transformations}), and 2) the
number of active weights in the unstructured matrix decreases $\propto$
sparsity.


\paragraph{Comparison with Structured Sparsity}
In this section, we compare structured sparsity to unstructured sparsity with
SIFT. In theory, for a fixed number of non-zero elements in a sparse mask, the
use of unstructured sparsity can search over all the possible variations of the
mask. However, since most hardware accelerators are not able to accelerate
computations with unstructured sparsity, multiple works have investigated
training with structured sparsity (e.g., low-rank and block-sparse matrices) to
obtain wall clock speed-ups~\cite{khodak2020initialization,
tai2016convolutional, chen2021drone, hubara2021accel, dao2022monarch,
chen2022pixelated}. We study structured sparsity by deriving Iso-FLOP
configurations using low-rank and block sparsity with Sparse Wide
transformation. We use the method proposed in~\citet{hubara2021accel} to search
N:M transposable sparsity, which can accelerate training on GPUs with Tensor
Cores. In our evaluation, the low-rank factorization results were worse than
block sparsity (see more details in Appendix~\ref{app:structured_sparsity}).
Table~\ref{tab:cifar_struct} compares unstructured sparsity to block sparsity.
Although using SIFT with block sparse matrices lead to improvements over the
dense baseline, unstructured sparsity achieves the highest gains. This result
can be explained by the fact that block-sparse matrices have reduced mask
diversity~\citep{hubara2021accel} compared to unstructured sparse matrices.

\begin{table}[!t]
    \caption{Evaluation of Sparse Wide SIFT family with unstructured and
structured sparsity across different values of sparsity (columns) on CIFAR-100
with ResNet-18. } \vskip 1pt
    \centering
\resizebox{0.95\columnwidth}{!}{
    \begin{small}
    \begin{sc}
    \begin{tabular}{c|cccc}
        \toprule
        Dense &	Sparsity Pattern    & 0.50   & 0.75  & 0.90 \\
	\midrule
 \multirow{2}{*}{77.0 $\pm$ 0.2}    & Unstructured        & 79.0 & 79.5 &
 \textbf{80.1} \\
              & N:M Block Sparse    & 77.1 & \textbf{78.4} & 78.1 \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
}
% \vspace{-0.1in}
\label{tab:cifar_struct}
\end{table}

\begin{table}[!t]
    \caption{Evaluation of Sparse Wide SIFT family with various compute
efficient architectures on CIFAR-100 across different values of sparsity
(columns). Using SIFT, all architectures outperform the dense by a significant
margin.} \vskip 1pt
    \centering
    \resizebox{0.8\columnwidth}{!}{

    \begin{small}
    \begin{sc}
    \begin{tabular}{cccc}
        \toprule
        	    & Dense             & 0.50 & 0.75 \\ \midrule MobileNetV2 & 72.4
        $\pm$ 0.2    & 73.4 & \textbf{73.7} \\
        MobileViT-S & 73.5 $\pm$ 0.1    & 74.6 & \textbf{74.8} \\
%        BotNet-26   & 78.0              & 78.1 & \textbf{78.7} \\
        BotNet-50   & 79.8 $\pm$ 0.2     & 80.3 & \textbf{80.6} \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    }
\label{tab:mbv2-cifar}
% \vspace{-0.1in}
\end{table}

\subsection{Results with Efficient Architectures}

To further understand the robustness of SIFT across different model families, we
evaluate SIFT on architectures that are optimized for efficient inference
(MobileNetV2~\citep{sandler2018mobilenetv2} and
MobileViT~\citep{mehta2021mobilevit}) and efficient training
(BotNet~\citep{srinivas2021bottleneck}). We transform the dense layers in these
architectures with the Sparse Wide family and evaluate them at different
sparsity levels. We observe a noticeable increase in test accuracy across all
architectures (see Table~\ref{tab:mbv2-cifar}). In addition, we demonstrate the
robustness of the SIFT family by also applying the Sparse Parallel
transformation and show consistent improvement across all architectures (see
Appendix~\ref{app:efficientcv}). We evaluate the best-performing architecture
(BotNet-50) on ImageNet (see Section~\ref{sub:imagenet_result}). The details of
the experimental setup can be found in Appendix~\ref{app:pt-implement}.


\subsection{Results on ImageNet}
\label{sub:imagenet_result}

We take the best-performing SIFT transformations (i.e., Sparse Wide and Sparse
Parallel) on CIFAR-100, and evaluate them on ImageNet using ResNet-18. Both
families of SIFT obtain significantly higher accuracy compared to the dense
baseline (refer to Table~\ref{tab:resnet-i1k}). Note, SIFT Sparse Wide ResNet-18
at 90\% sparsity improves over the dense baseline by 3.5\%, and is able to match
accuracy of dense ResNet34 with 2$\times$ fewer training FLOPs (see
Figure~\ref{fig:sift_resnet_improvement}). We take the best-performing
transformation (Sparse Wide) and apply it to ResNet-34 and BotNet-50. Increasing
sparsity leads to a consistent increase in accuracy, indicating improved
training efficiency at higher sparsities across all architectures. On BotNet-50,
a hybrid ViT model, we see a 1\% improvement at 90\% sparsity.

\begin{table}[!t]
    \caption{Evaluation of SIFT on ImageNet. Best result for each transformation
and architecture is highlighted in bold.}
    \centering
\resizebox{\columnwidth}{!}{
    \begin{small}
    \begin{sc}
    \begin{tabular}{cc|cccc}
        \toprule
     & \multirow{2}{*}{Dense} & \multirow{2}{*}{Transformation}
     &\multicolumn{3}{c}{Sparsity} \\
  	& &  & 0.50 & 0.75 & 0.90 \\
\midrule
        \multirow{2}{*}{ResNet-18} &  \multirow{2}{*}{70.9 $\pm$ 0.1} & Sparse
Wide & 72.7 & 73.8 & \textbf{74.4} \\
        &   & Sparse Parallel & 72.7 & 73.2 & \textbf{74.0} \\ \midrule
        ResNet-34 & 74.2 $\pm$ 0.1 & Sparse Wide &  75.6 & 76.4 & \textbf{76.8}
        \\ \midrule BotNet-50 & 77.5 $\pm$ 0.1 &  Sparse Wide & 77.9 & 78.3 &
        \textbf{78.5} \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
}
\label{tab:resnet-i1k}
% \vspace{-0.1in}
\end{table}


\subsection{Transfer Learning with SIFT}
\label{subsec:transfer_learning}
To show the effectiveness of pre-training our SIFT classification backbones, we
evaluate them on 1) object detection on MS COCO 2017~\cite{lin2014microsoft},
and 2) semantic segmentation on CityScapes~\cite{cordts2016cityscapes}. For
object detection, we adopt the RetinaNet~\cite{lin2017focal} framework from the
MMDetection open-source toolbox~\cite{mmdetection} and report results in the
standardized training setting. For semantic segmentation, we utilize
DeepLabV3+~\cite{chen2018encoder} in the MMSegmenation open-source
toolbox~\cite{mmseg2020}. We evaluate ResNet-18 with Sparse Wide transformation
(best-performing transformation on ImageNet). To ensure FLOP-equivalent
comparisons with the dense backbone, we ensure that SIFT backbones remain sparse
during fine-tuning. Appendix~\ref{app:eval_downstream} provides more details
regarding the training setup. We summarize our findings in Table
\ref{tab:down_stream}. Using SIFT Sparse Wide ResNet-18 backbone leads to
significant accuracy gains across all metrics on both downstream tasks.

% \vspace{-0.25cm}
\begin{table}[!t]
    \caption{Evaluation of SIFT variants of ResNet-18 as backbones on downstream
tasks : (a) Object detection on MS COCO, (b) Semantic segmentation on
Cityscapes. SIFT Sparse Wide ResNet-18 backbones outperform dense baseline by a
significant margin across all metrics on both tasks.} \vskip 1pt
    \centering
\resizebox{0.97\columnwidth}{!}{
    \begin{small}
    \begin{sc}
    \begin{tabular}{ccc|ccc}
        \toprule
      & \multirow{2}{*}{Metric} & \multirow{2}{*}{Dense}
      &\multicolumn{3}{c}{Sparsity} \\
	&	     &	      & 0.50   & 0.75  & 0.90 \\
	\midrule
 \multirow{3}{*}{MS COCO}     &  AP      &  29.3  & 31.3 & 32.8 & \textbf{34.5}
 \\
      &  AP$_{50}$    &  46.2  & 49.0 & 51.0 & \textbf{53.5} \\
      &  AP$_{75}$    &  30.9  & 33.0 & 34.8 & \textbf{36.5} \\
	\midrule
  \multirow{2}{*}{CityScapes}     &  \text{mIoU}      &   76.7   &  77.9   &
  78.9 & \textbf{79.1} \\
      &  \text{mAcc}      &   84.4   &  85.1   & 85.7 & \textbf{86.0} \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
}
\label{tab:down_stream}
\end{table}

\begin{table}[t]
    \caption{ Evaluation of SIFT for pre-training GPT-3 Small from scratch on
the WikiText-103 dataset and report the test perplexity (lower is better) over 3
random seeds. } \vskip 1pt
    \centering
    \resizebox{0.80\columnwidth}{!}{
    \begin{small}
    \begin{sc}
    \begin{tabular}{cc|cc}
        \toprule
       		     & Dense & 0.50 & 0.75 \\

	\midrule
        GPT-3 Small &   20.8 $\pm$ 0.3  &  \textbf{20.4}      &  22.1  \\
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    }
\label{tab:nlp_scratch_result}
% \vspace{-0.1in}
\end{table}

\subsection{NLP Implementation Details}
\label{subsec:nlp_results}

We evaluate SIFT by training GPT-3 Small~\citep{brown2020language} from scratch
on the WikiText-103~\citep{merity2017pointer} language modeling task, a commonly
used NLP benchmark dataset. Training large GPT models is very costly and compute
intensive. Although SIFT does not increase the training FLOPs, in practice,
since GPUs do not accelerate unstructured sparsity, the wall clock time to train
with SIFT increases $\propto \frac{1}{1-s}$. For example, training with 75\%
sparsity leads to 4x longer wall clock training time on GPUs. The compute cost
and resources for training quickly become prohibitive when transforming GPT
models with SIFT. Therefore, we believe SIFT is well suited for emerging sparse
deep learning hardware accelerators like the Cerebras
CS-2~\citep{cerebrasHarnessingPower, liehotchips}. Hence, we train our GPT
models on the CS-2 and leverage its ability to accelerate training with
unstructured sparsity. We provide more details about performance and wall clock
speed-ups in Section~\ref{subsec:wall_clock}. The current implementation of
Cerebras CS-2's specialized kernels support training with static unstructured
sparsity; therefore, results in this section are reported without DST methods.

\subsection{Results on GPT End-to-End Training}
We train the SIFT Sparse Wide GPT-3 Small models at 50\% and 75\% sparsity
levels, and compare against the standard dense GPT-3 Small and GPT-3 Medium
models. Following~\citet{dao2022monarch}, we train all models from scratch on
the WikiText-103 dataset and report the average test perplexity (PPL) over 3
random seeds in Table~\ref{tab:nlp_scratch_result}. We show that SIFT Sparse
Wide GPT-3 Small at 50\% sparsity improves the perplexity by 0.4 over its dense
counterpart. This result is inline with dense GPT-3 Medium (20.5 Â± 0.2 PPL)
while our SIFT Sparse Wide model uses 2.4x fewer training FLOPs. In
Appendix~\ref{app:gpt_e2e}, we provide details on the hyperparameters and how
the total training FLOPs for the models in Table~\ref{tab:nlp_scratch_result}
were calculated.

\paragraph{GPT Pre-training and Fine-tuning} 

While not the primary focus of our method, we note that SIFT can also be applied
in a fine-tuning setup for NLP models. After pre-training sparse, the SIFT model
can be fine-tuned as-is (i.e., remains sparse) or after densifying (i.e., allow
the zeroed weights to learn) using a technique such as
SPDF~\citep{thangarasa2023spdf}. We perform some preliminary fine-tuning studies
on BERT and GPT and those results can be found in Appendix~\ref{app:spdf}.

% In this study, we show the efficacy of SIFT within the pre-training followed
% by fine-tuning paradigm. Inspired by Thangarasa et al. (2023), we apply their
% SPDF framework to SIFT Sparse Wide BERT~\citep{iulia2019bertsmall} and GPT-3
% models, and demonstrate an additional benefit of SIFT, which allows us to take
% advantage of the additional representational capacity during the fine-tuning
% stage. Here, we first pre-train a language model and its SIFT variants at 50\%
% and 75\% on an unsupervised pre-training dataset. Then, we perform dense
% fine-tuning on the downstream tasks (i.e., allow the zeroed weights to learn).
% More details on the training hyperparameters for pre-training and fine-tuning
% with BERT and GPT, along with their results can be found in
% Appendix~\ref{app:spdf}.


\section{Wall Clock Acceleration with Sparsity}
\label{subsec:wall_clock}

Results presented in Section~\ref{sec:experiments} help validate our hypothesis,
i.e., training DNNs with dense matrices is FLOP inefficient. Replacing dense
layers with SIFT increases the training efficiency by providing significantly
higher accuracy using the same amount of training FLOPS. This result is
significant from a theoretical perspective but does not translate to direct
practical value on hardware that can not accelerate unstructured sparsity (e.g.,
Nvidia GPUs, Google TPUs). However, there has recently been a renewed interest
in hardware software co-design for accelerating unstructured sparsity. Here, we
benchmark SIFT on these platforms to demonstrate its practical value. We hope
these results motivate the broader machine learning community to explore and
exploit the benefits of unstructured sparsity for training and inference.

%\paragraph{Inference}
\vspace{-0.1in}
\paragraph{Setup}
We evaluate the inference efficiency of SIFT using Neural Magic's sparsity-aware
runtime\footnote{\url{https://github.com/neuralmagic/deepsparse}}. We benchmark
different configurations of the Sparse Wide ResNet-18 model with sparsity $\in$
\{50\%, 75\%, 90\%\} for batched inference on ImageNet. We also evaluate the
training efficiency of SIFT on the Cerebras CS-2 which supports and accelerates
training with unstructured sparsity. Technical details regarding the
implementation of the specialized sparse kernels are beyond the scope of this
paper. We plan to release our code and details about the hardware. We benchmark
different configurations of Sparse Wide GPT-3 1.3B with sparsity $\in$ \{50\%,
75\%, 90\%\} and report seconds/ iteration. More details about our setup can be
found in Appendix~\ref{app:benchmark}. Our benchmarking results are detailed in
Figure \ref{fig:sparsity_benchmark}. We note that configurations of SIFT at
different values of sparsity do not incur a significant change in the FLOPs
compared to the dense model. On ideal hardware, FLOPs should translate directly
to wall clock time, and the inference latency or training time for all
configurations of SIFT should be the same as that of the dense model (dotted
black line). Conversely, when hardware does not support unstructured sparsity,
the latency or training time of SIFT variants increases with sparsity (blue
line). Our results lie between these two spectrums (green line). Using Neural
Magic's inference runtime, we observe significant speed-up with unstructured
sparsity (5.2x at 90\% sparsity). Similarly, we observe significant training
speed-up  (3.8x at 90\% sparsity) on the Cerebras CS-2.


\begin{figure}[htp]
  \centering
\resizebox{1.0\columnwidth}{!}{
  \subfigure{\includegraphics[scale=0.13]{./figures/neural_magic_inference.pdf}}\quad
  \subfigure{\includegraphics[scale=0.13]{./figures/cerebras_training_speed.pdf}}
  }
% \vspace{-0.5cm}
\caption{Benchmarking (left) inference on Neural Magic's DeepSparse runtime and
(right) training acceleration with unstructured sparsity on the Cerebras CS-2.}
\label{fig:sparsity_benchmark}
% \vspace{-0.5cm}
\end{figure}
