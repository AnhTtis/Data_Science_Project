\section{Related Work}

Our work is similar to the body of work studying the role of
overparameterization and sparsity for training DNNs. The modeling capacity
needed to learn a task is often unknown. Hence, we often solve this by training
overparameterized models to fully exploit the learning capability and then
compress them into a smaller subnetwork.

\paragraph{Overparameterization}~\citet{nakkiran2021deep} show that DNNs benefit
from overparameterization. Following this, there have been many works that
leverage overparameterization by scaling the size of models
\cite{rae2021scaling, goyal2022vision} and augmenting existing DNNs to increase
modeling capacity and the accuracy of trained networks
~\cite{shuxuan2020expandnet, ding2019acnet, ding2021repvgg, jinming2022doconv,
mobileone2022, liu2022more}. These methods use linear parameterizations of the
model, making them highly inefficient to train, and are focused on improving
inference throughput (reduced latency). In contrast, our work is focused on
improving the modeling capacity using sparse non-linear parameterizations, which
do not increase training FLOPs compared to the baseline model. While both
approaches have the same inference FLOPs, our approach improves accuracy without
increasing the training FLOPs.

\paragraph{Sparse Training} The Lottery Ticket
Hypothesis~\cite{frankle2018lottery, frankle2020linear} shows that accurate
sparse subnetworks exist in overparameterized dense networks but require
training a dense baseline to find. Other approaches have proposed frameworks for
identifying lottery tickets~\citep{hattie2019supermask, ma2022effective} but
still require a tremendous amount of compute resources. Following this, various
attempts have been made to find the optimal sparse subnetwork in a single shot.
These methods either try to find the subnetworks at
initialization~\cite{tanaka2020pruning, wang2020picking, de2020progressive,
lee2018snip} or dynamically during training~\cite{mocanu2018, evci2020rigging,
jayakumar2020top, raihan2020sparse}. However, given a fixed model capacity,
these methods tradeoff accuracy relative to the dense baseline to save training
FLOPs. ~\citet{stosic2021search} and~\citet{ramanujan2020s} increase the search
space during sparse training to retain accuracy; however, do not guarantee FLOPs
savings. In contrast to these methods, our work introduces a set of non-linear
sparse transformations, which increase the representational capacity of the
network. This approach does not introduce a new sparse training algorithm, but
instead improves the search space of existing methods, leading to improved
generalization while being efficient to train.

\paragraph{Iso-Parameter vs. Iso-FLOP} Recent sparsity literature is focused on
improving generalization at high sparsity levels. Hence, layer-wise sparsity
distributions such as the Erd\"os-R\'enyi-Kernel~\cite{evci2020rigging}, Ideal
Gas Quota~\cite{chen2022sparsity}, and parameter leveling~\cite{golubeva2021are}
are often used with sparse training to boost accuracies. However, these works
target the setting where the models being compared have a fixed parameter budget
(i.e., Iso-Parameter), which does not translate to similar training FLOPs to the
original dense model (especially in CNNs). As a result, training models with
these distributions often require different memory or computational resources
per layer. Our approach does not focus on this Iso-Parameter setting but instead
adopts the uniform sparsity distribution (i.e., every layer gets the same
sparsity level), ensuring uniform FLOP reductions across the network. We also
ensure the same computational FLOPs of a dense network by leveraging sparsity
along with our Iso-FLOP transformations.

\raggedbottom
