\section{Introduction}

\begin{figure}
\centering
\includegraphics[keepaspectratio=true, width=7.5cm]{./figures/SIFT_ResNet_Improvement.pdf}
\vspace{-0.1in}
\caption{ Accuracy vs. Training FLOPs for different variants of ResNet on
ImageNet. SIFT provides significant accuracy gains across different models and
sparsity levels while using the same FLOP budget as its dense counterpart. In
particular, the best SIFT variants of ResNet-18 and ResNet-34 achieve 3.5\% and
2.7\% improvements over their dense baselines, respectively. }
\vspace{-0.25in}
\label{fig:sift_resnet_improvement}
\end{figure}

Increases in model size and training data have led to many breakthroughs in deep
learning (e.g., AlexNet~\citep{krizhevsky2012imagenet},
ResNet~\citep{he2016identity}, Transformers~\citep{vaswani2017attention},
GPT~\citep{radford2018improving, radford2019language},
AlphaGo~\citep{silver2017mastering}, etc.). Consequently, the computational and
memory footprint of training and deploying deep neural networks (DNNs) has grown
exponentially. To enable the deployment of large models, multiple techniques
(e.g., distillation (Hinton et al., 2015), quantization (Han et al., 2015a),
pruning (Han et al., 2015b)) have been introduced to reduce inference FLOPs and
memory requirements. While these techniques improve inference efficiency (test
accuracy w.r.t inference FLOPs), the associated training costs are still
prohibitive. In this work, we focus on improving the training efficiency
(test-accuracy w.r.t training FLOPs) of DNNs.

Recent works~\citep{evci2020rigging, jayakumar2020top} have explored using
weight sparsity to reduce the FLOPs spent in training.
\citet{frankle2018lottery} demonstrate that sparse subnetworks (termed “lottery
tickets”) exist at initialization and can be trained to match the accuracy of
their original dense network. Inspired by this result, various dynamic sparse
training (DST) methods~\citep{ma2022effective,evci2020rigging, liu2021selfish,
jayakumar2020top} attempt to find optimal sparse subnetworks in a single
training run.  While these methods primarily aim to improve training efficiency
by reaching dense accuracy with fewer FLOPs, they often perform worse than their
dense baselines or rely on longer training schedules (up to 2-5$\times$ training
iterations) to close the gap. As a result, these techniques can sometimes even
require more FLOPs than training the dense
model~\citep{ma2022effective,evci2020rigging, jayakumar2020top}. In contrast to
prior work, we focus on showing training efficiency gains by using sparsity to
increase accuracy while consuming the same training FLOPs as the dense model.
Specifically, we introduce a family of Sparse Iso-FLOP Transformations (SIFT)
that can be used as drop-in replacements for dense layers in DNNs. These
transformations increase the representational capacity of layers and facilitate
the discovery of optimal sparse subnetworks without changing the layer’s
underlying FLOPs (i.e., Iso-FLOP). For example, making a layer wider but sparser
increases dimensionality while still maintaining FLOPs due to sparsity. All SIFT
transformations are parameterized by a single hyperparameter, the sparsity
level. Figure~\ref{fig:sift_resnet_improvement} summarizes the ImageNet
performance with ResNet models, where our SIFT Sparse Wide variants
significantly increase the accuracy of matching Iso-FLOP dense models. In
particular, SIFT Sparse Wide ResNet-18 at 90\% sparsity improves the top-1
accuracy from 70.9\% to 74.4\% (+3.5\%), and outperforms a dense ResNet-34
(74.2\%) while using 2x fewer FLOPs. We emphasize that these gains were obtained
by replacing dense layers with SIFT transformations and required no changes to
training hyperparameters. The main contributions of our work are: 

% Pre-Sean feedback Increases in model size and training data have led to many
%breakthroughs in deep learning (e.g., AlexNet~\citep{krizhevsky2012imagenet},
%ResNet~\citep{he2016identity}, Transformers~\citep{vaswani2017attention},
%GPT~\citep{radford2018improving, radford2019language},
%AlphaGo~\citep{silver2017mastering}, etc.). Consequently, the computational and
%memory footprint of training and deploying deep neural networks (DNNs) has
%grown exponentially. To enable the deployment of large models, multiple
%techniques (e.g., distillation~\citep{hinton2015distilling},
%quantization~\citep{han2015deep}, pruning~\citep{han2015learning}) have been
%introduced to reduce inference FLOPs and memory requirements. While these
%techniques improve inference efficiency (test accuracy w.r.t inference FLOPs),
%the associated training costs are still prohibitive. In this work, we focus on
%improving the training efficiency (test-accuracy w.r.t training FLOPs) of DNNs.
%Recent works~\citep{evci2020rigging, jayakumar2020top} have explored using
%weight sparsity to reduce the FLOPs spent in training.
%\citet{frankle2018lottery} demonstrate that sparse subnetworks (termed
%``lottery tickets’’) exist at initialization and can be trained to match the
%accuracy of their original dense network. Inspired by this result, various
%dynamic sparse training (DST) methods~\citep{ma2022effective, evci2020rigging,
%liu2021selfish, jayakumar2020top} attempt to find optimal sparse subnetworks in
%a single training run. While these methods aim to improve training efficiency
%by reaching dense accuracy with fewer FLOPs, they often perform worse than
%their dense baselines and rely on extended training schedules (2-5$\times$
%training iterations) to close the gap. This ultimately requires more FLOPs than
%training the dense model~\citep{ma2022effective,evci2020rigging,
%jayakumar2020top} and reduces training efficiency. In contrast to prior work,
%we improve training efficiency by using sparsity to increase accuracy while
%using the same training FLOPs as the dense model. Specifically, we introduce a
%family of \textbf{S}parse \textbf{I}so-\textbf{F}LOP \textbf{T}ransformations
%(SIFT) that can be used as drop-in replacements for dense layers in DNNs. These
%transformations increase the representational capacity of layers and facilitate
%the discovery of optimal sparse subnetworks without changing the layer’s
%underlying FLOPs (i.e., Iso-FLOP). For example, making a layer wider but
%sparser increases dimensionality while still maintaining FLOPs due to sparsity.
%All SIFT transformations are parameterized by a single hyperparameter, the
%sparsity level. Figure~\ref{fig:sift_resnet_improvement} shows the accuracy
%improvements obtained by applying SIFT to ResNet model variants on ImageNet. We
%emphasize these gains were obtained by replacing dense layers with SIFT
%transformations and required no changes to training hyperparameters. The main
%contributions of our work are:
\vspace{-0.1in}

\begin{enumerate}%[noitemsep]
\item We introduce a family of Sparse Iso-FLOP Transformations to improve the
training efficiency of DNNs by improving accuracy while holding FLOPs constant.
These transformations are parameterized by a single hyperparameter (sparsity
level) and can be used as drop-in replacements for dense layers without changing
the overall FLOPs of the model.

\item In the CV domain, using SIFT increases the top-1 accuracy of ResNet-18 and
ResNet-34 by 3.5\% and 2.6\% respectively on ImageNet. Finetuning these
pre-trained models for object detection (MS COCO) and segmentation (CityScapes)
leads to an improvement of 5.2\% mAP and 2.4\% mIoU, respectively.

\item In the NLP domain, using SIFT with GPT-3 Small leads to a 0.4 perplexity
improvement on the WikiText-103 language modeling task.

\item We report wall-clock speed-ups for both training and inference with
unstructured sparsity, highlighting the practical value of SIFT.
\end{enumerate}
