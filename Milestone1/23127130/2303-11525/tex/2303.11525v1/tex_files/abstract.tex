\begin{abstract}

% Pre-Sean's feedback Recent works have explored using weight sparsity to
%improve the training efficiency (test accuracy w.r.t training FLOPs) of dense
%neural networks (DNNs). These works aim to reduce training FLOPs without
%sacrificing accuracy. However, training with sparsity leads to a drop in
%accuracy. This gap can be closed by extending the training schedule, but this
%ultimately requires more training FLOPs than the dense model and therefore does
%not improve training efficiency compared to the dense model. In contrast, we
%use sparsity to increase accuracy while using the same FLOPS as the dense
%model. We introduce SIFT, a family of Sparse Iso-FLOP Transformations that can
%be used as drop-in replacements for dense layers to improve their
%representational capacity and FLOP efficiency. Each transformation is
%parameterized by a single parameter (sparsity value) and provides a larger
%search space to find optimal sparse masks. Without changing any training
%hyperparameters, replacing dense layers with SIFT leads to significant
%improvements across Computer Vision (CV) and Natural Language Processing (NLP)
%tasks, including ResNet-18 on ImageNet (+3.5\%) and GPT-3 Small on WikiText-103
%(-1.3 PPL). To the best of our knowledge, this is the first time sparsity has
%improved accuracy while using the same FLOPs as the dense baseline.

Recent works have explored the use of weight sparsity to improve the training
efficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs).
These works aim to reduce training FLOPs but training with sparse weights often
leads to accuracy loss or requires longer train schedules, making the resulting
training efficiency less clear. In contrast, we focus on using sparsity to
increase accuracy while using the same FLOPS as the dense model and show
training efficiency gains through higher accuracy. In this work, we introduce
SIFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in
replacements for dense layers to improve their representational capacity and
FLOP efficiency. Each transformation is parameterized by a single parameter
(sparsity level) and provides a larger search space to find optimal sparse
masks. Without changing any training hyperparameters, replacing dense layers
with SIFT leads to significant improvements across computer vision (CV) and
natural language processing (NLP) tasks, including ResNet-18 on ImageNet
(+3.5\%) and GPT-3 Small on WikiText-103 (-0.4 PPL), both matching larger dense
model variants with 2x or more FLOPs. To the best of our knowledge, this is the
first work to demonstrate the use of sparsity for improving accuracy of dense
models via a simple-to-use set of sparse transformations. Code is available
at:~\url{https://github.com/CerebrasResearch/SIFT}.

\end{abstract}
