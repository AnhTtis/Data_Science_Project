% arara: pdflatex
% arara: bibtex
% arara: pdflatex
% arara: pdflatex
% This paper contains the submission to the CDC 2023 on the convergence of
% the Heavy Ball and Nesterov methods with approximate gradients.
% The date is 18.03.2023.

\documentclass[11pt]{article}

% \documentclass[journal,final]{IEEEtran}
% \documentclass[conference]{ieeeconf}
% \IEEEoverridecommandlockouts
% \overrideIEEEmargins

\setlength{\textheight}{225mm}
\setlength{\textwidth}{165mm}
\setlength{\oddsidemargin}{-5mm}
\setlength{\topmargin}{-5mm}

% \setlength{\textheight}{237mm}

% \usepackage{multirow}
% \usepackage{amssymb,amsmath,amsthm}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{latexsym,amsfonts}
\usepackage{color}
\usepackage{graphicx}
  % \graphicspath{{./Figures/}}
  \DeclareGraphicsExtensions{.jpeg,.png,.jpg,.eps,.pdf}
\usepackage{tikz}
\usepackage{epstopdf}
\usepackage{hyperref}

\input{notation.tex}

% Special notation for this paper.

\definecolor{verm}{rgb}{0.6,0.2,0.2}
\definecolor{purp}{rgb}{0.3,0.1,0.6}
\definecolor{purple}{rgb}{0.4,0.0,0.6}
\definecolor{bggreen}{rgb}{0.1,0.3,0.1}
\definecolor{dgreen}{rgb}{0.1,0.6,0.1}
\definecolor{black}{rgb}{0.0,0.0,0.0}
\definecolor{crim}{rgb}{0.3,0.1,0.1}
\definecolor{dred}{rgb}{0.5,0.1,0.1}

\newtheorem{corollary}{Corollary}{\bf}{\it}
\newtheorem{definition}{Definition}{\bf}{\it}
\newtheorem{example}{Example}{\bf}{\rm}
\newtheorem{lemma}{Lemma}{\bf}{\it}
\newtheorem{theorem}{Theorem}{\bf}{\it}
\newtheorem{proposition}{Proposition}{\bf}{\it}
\newtheorem{conjecture}{Conjecture}{\bf}{\it}
\newtheorem{problem}{Problem}{\bf}{\rm}

\begin{document}

\title{
Convergence of Momentum-Based Heavy Ball Method with \\
Batch Updating and/or Approximate Gradients 
% (Dedicated to the Memory of Boris Teodorovich Polyak)
}

\author{Tadipatri Uday Kiran Reddy and M.\ Vidyasagar
\thanks{
TUKR is a final year undergraduate in the Electrical Engineering Department
at the Indian Institute of Technology Hyderabad, Kandi, Talangana 502284, INDIA.
Email: ee19btech11038@iith.ac.in.
MV is SERB National Science Chair and Distinguished Professor,
Indian Institute of Technology Hyderabad, Kandi, Talangana 502284, INDIA.
Email: m.vidyasagar@iith.ac.in.
This research is supported by the Science and Engineering Research Board (SERB),
India. We thank Speech Information Processing (SIP) Lab, Indian Institute of 
Technology Hyderabad for providing their computational resources.}
}

\date{\today}

\maketitle

\begin{abstract}

In this paper, we study the well-known ``Heavy Ball''
method for convex and nonconvex optimization introduced by
Polyak in 1964, and establish its convergence under
a variety of situations.
Traditionally, most algorthms use ``full-coordinate update,''
that is, at each step, \textit{every component} of the argument is updated.
However, when the dimension of the argument is very high,
it is more efficient to update \textit{some but not all} components of 
the argument at each iteration.
We refer to this as ``batch updating'' in this paper.

When gradient-based algorithms are used together with batch updating,
in principle it is sufficient to compute only those components of the
gradient for which the argument is to be updated.
However, if a method such as back propagation is used to compute these
components, computing only \textit{some components}
of gradient does not offer much savings over computing
the entire gradient.
Therefore, to achieve a noticeable reduction in CPU usage at each step,
one can use first-order differences to approximate the
gradient.
The resulting estimates are \textit{biased}, and also have unbounded variance.
Thus some delicate analysis is required to ensure that the HB
algorithm converge when batch updating is used instead of full-coordinate
updating, and/or approximate gradients are used instead of true gradients.
% Stating and proving such theorems is the objective of the present paper.
In this paper, we not only
establish the almost sure convergence of the iterations to the
stationary point(s) of the objective function, but also derive upper
bounds on the \textit{rate of convergence.}
To the best of our knowledge, there is no other paper that combines
all of these features.

\textbf{This paper is dedicated to the memory of Boris Teodorovich Polyak}

\end{abstract}

\section{Introduction}\label{sec:Intro}

In this paper, we study the well-known and widely-used ``Heavy Ball'' (HB)
method for convex and nonconvex optimization introduced by 
Polyak in \cite{Polyak-CMMP64}, and establish its convergence under
a variety of situations.
% ``momentum-based''
% optimization algorithms, namely the Heavy Ball (HB) method due to Polyak
% and the Accelerated Gradient method due to Nesterov \cite{Nesterov-Dokl83}.
% The latter is often referred to as the Nesterov Accelerated Gradient (NAG)
% method.

Let $J : \R^d \ap \R$ denote the $\C^1$ objective function to be
minimized, and let $\bth_t$ denote the estimate for the minimizer
of $J(\cdot)$ at step $t$.
Traditionally, most algorthms use ``full-coordinate update,''
that is, at each step $t$, \textit{every component} of $\bth_t$ is updated.
However, when the dimension $d$ of $\bth_t$ is very high, as it usually is
in problems of deep neural network training and deep reinforcement learning,
it is more efficient to update \textit{some but not all} components of $\bth_t$
at each $t$.
We refer to this as ``batch updating'' in this paper.\footnote{Other 
commonly used phrases are coordinate update, block update, etc.}
When gradient-based algorithms are used together with batch updating,
in principle it is sufficient
to compute the components $[\gJ(\bth_t)]_i$ for only those indices $i$
for which the $i$-th component of $\bth_t$ is to be updated.
However, if a method such as back propagation is used to compute these
components, it turns out that computing only \textit{some components}
of gradient does not offer much savings over computing
the entire gradient $\gJ(\bth_t)$.
% is not any more CPU-intensive than computing just some components of
% $\gJ(\bth_t)$.
Therefore, to achieve a noticeable reduction in CPU usage at each step,
one should find a way to estimate the values of $[\gJ(\bth_t)]_i$
for only a few specified indices $i$.
This can be done by using first-order differences to approximate the
gradient.
However, from a mathematical standpoint, the resulting estimates are
\textit{biased}.
Moreover, if the function values used in estimating $[\gJ(\bth_t)]_i$
are corrupted by additive noise, then the estimates of $[\gJ(\bth_t)]_i$
also have \textit{unbounded variance}, in the sense that
the variance grows without bound as $\tai$.
Thus some delicate analysis is required to ensure that the HB 
algorithm converges when batch updating is used instead of full-coordinate
updating, and/or approximate gradients are used instead of true gradients.
% Stating and proving such theorems is the objective of the present paper.

We are now in a position to state the contributions of the present paper.
We study both convex as well as a class of nonconvex objective functions.
We apply stochastic versions of the HB algorithm with batch updating,
and the measurements of $\gJ(\cdot)$ are either noisy or approximate.
We adapt some ideas from \cite{Liu-Yuan-arxiv22}, and not only
establish the almost sure convergence of the iterations to the
stationary point(s) of the objective function, but also derive upper
bounds on the \textit{rate of convergence.}
To the best of our knowledge, there is no other paper that combines
all of these features.

\subsection{General Form of the Algorithm}\label{ssec:32}

In this subsection we state the general form of the Stochastic Heavy Ball
(SHB) algorithm.
% and Stochastic NAG (SNAG) algorithms that are analyzed here.
For this purpose, it is assumed that the objective function $J(\cdot)$
is $C^1$.
Additional assumptions are given in Section \ref{ssec:41}.
The general form of this algorithm is
% Both algorithms have the same general form, namely
\be\label{eq:321}
\bth_{t+1} = \bth_t + \al_t \bphi_{t+1} + \mu (\bth_t - \bth_{t-1}) ,
\ee
where $\bphi_{t+1}$ is an
$\R^d$-valued random variable, denoting the search direction at time $t$,
and $\mu \in [0,1)$ is called the ``momentum coefficient.''
In the usual case of full coordinate updating, it is customary to assume that
\be\label{eq:322}
\bphH = - \gJ(\bth_t) +\bzeta_{t+1} ,
\ee
% \be\label{eq:322a}
% \bphN = - \gJ(\bth_t + \mu( \bth_t - \bth_{t-1} ) ) +\bxi_{t+1} ,
% \ee
where $\bzeta_{t+1}$ is a measurement error with zero conditional expectation.
Thus, in the usual case,
$\bphH$ is a noise-corrupted  but unbiased measurement of $- \gJ(\bth_t)$.
% while $\bphN$ is a noise-corrupted  measurement of
% $- \gJ(\bth_t + \mu( \bth_t - \bth_{t-1} )$.

However, when one applies batch updating and/or uses approximate
gradients instead of the (noise-corrupted) true gradient,
\eqref{eq:322} does not hold in general.
% and \eqref{eq:322a} do 
To cater this more general situation, we proceed as follows:
Let $\bth_0^t := ( \th_0 , \cdots , \th_t )$, and similarly
$\bphi_1^t := ( \phi_1 , \cdots , \phi_t )$.
(Note that there is no $\bphi_0$.)
% Also, for the moment we omit the superscripts ``HB'' and ``NAG'' for clarity.)
Suppose that all of these are random variables on some underlying
probability space $(\OM , \SI , P)$.\footnote{The reader is referred to
\cite{Durrett19} for all concepts related to stochastic processes,
conditional expectations, etc.}
Let $\{ \F_t \}$ denote any \textbf{filtration}, that is,
an increasing sequence of $\s$-algebras satisfying $\F_t \seq
\F_{t+1} \seq \SI$, such that $\bth_0^t, \bphi_1^t$ are measurable
with respect to $\F_t$, denoted by $\bth_1^t , \bphi_1^t \in \M(\F_t)$.
Further, let $E_t(X)$ be a shorthand for
$E( X | \F_t )$, the conditional expectation of $X$ with respect to $\F_t$.

With this notational convention, our stochastic
algorithm is once again given by
\eqref{eq:321}, where $\bphi_{t+1}$ is as yet unspecified.
Define
\be\label{eq:324}
\bzeta_{t+1} := \bphi_{t+1} - E_t ( \bphi_{t+1} ) .
\ee
Then, from the ``tower'' property of conditional expectations
(see \cite{Williams91}), it follows that\footnote{Hereafter
we omit the phrase ``almost surely'' almost everywhere.}
\be\label{eq:325}
E_t(\bzeta_{t+1}) = \bz \as, \fa t .
\ee
% To differentiate between SHB and SNAG, we define
Next, define
\be\label{eq:326}
\phH := E_t ( \bphi_{t+1} ) + \gJ(\bth_t) ,
\ee
% \be\label{eq:327}
% \phN := E_t ( \bphN ) + \gJ(\bth_t + \mu( \bth_t - \bth_{t-1} )) .
% \ee
Thus, in the usual HB algorithm we would have $\phH = \bz$ almost surely.
% whereas in the usual NAG algorithm we would have $\phN = \bz$
% almost surely.
However, no such assumption is made here, to accommodate the use of
approximate gradients in \eqref{eq:321}.
Note that $\phH \in \M(\F_t)$.
% Note that both and $\phH$ and $\phN$ are
% measurable with respect to $\F_t$.

\subsection{Types of Batch Updating Considered}\label{ssec:33}

In batch updating, the rule \eqref{eq:321} is applied to \textit{some
but not necessarily all} components of $\bth_{t+1}$.
Unless this is done with some care, technical difficulties can arise.
With this in mind,
we describe various options for batch updating that are studied in the
paper.
These include most if not all of the widely used batch updating methods.
In addition, we also describe variants of each of these options, when
the noise-corrupted
true gradient is replaced by a first-order approximation, commonly
referred to as Simultaneous Perturbation Stochastic Approximation (SPSA)
\cite{Spall-JHU98}. We refer to these variations of the basic HB algorithm
as Stochastic Heavy Ball (SHB).
Throughout, the symbol $\bphi_{t+1}$ denotes the search direction
in \eqref{eq:321}.
% By choosing the search direction appropriately, we can recover batch
% updating versions of SHB and SNAG.
Four different options are prescribed, as denoted by superscripts.
% Some of the content here, except for SPSA, is taken from
% \cite{MV-TUKR-ICML23}, and the reader is referred to that source
% for further details.

\textbf{Option 1: Full Coordinate Update:}
Let
\be\label{eq:331}
\bphi^{(1)}_{t+1} = \bphi_{t+1} .
\ee

\textbf{Option 2: Single Coordinate Update:}
This option is also known as ``coordinate gradient descent'' as defined in
\cite{Wright15} and studied further in \cite{Bach-et-al-aisats19}.
(However, those papers study only the steepest descent method and not 
its variants such as SHB).
At time $t$, choose an index $\kappa_{t+1} \in [d]$ at random with a
uniform probability, and independently of previous choices.
Let $\eb_{\kappa_{t+1}}$ denote the elementary unit vector with a $1$
as the $\kappa_{t+1}$-th component and zeros elsewhere,
and let $\bxi_{t+1}$ denote the measurement noise.
Then define
\be\label{eq:332}
\bphi^{(2)}_{t+1} = d \eb_{\kappa_{t+1} } \circ \bphi_{t+1} ,
\ee
where $\circ$ denotes the Hadamard, or componentwise, product of two
vectors of equal dimension.
The factor of $d$ arises because the likelihood that $\kappa_{t+1}$
equalling any one index $i \in [d]$ is $1/d$.
% Thus \eqref{eq:332} ensures that, in a sense made precise below,
% the search direction $\bphi^{(2)}_{t+1}$ equals $\bphi_{t+1}$ ``on average.''
% Note that, though $\kappa_{t+1}$ is used at time $t+1$,
% it is \textit{chosen} at time $t$, that is, \textit{prior}
% to the update at time $t+1$.
% Thus the filtration $\{ \F_t \}$ is chosen such that
% $\kappa_{t+1} \in \M(\F_t)$, i.e., $\kappa_{t+1}$ is measurable
% with respect to $\F_t$>.

\textbf{Option 3: Multiple Coordinate Update:}
This is option is just coordinate update along multiple
coordinates chosen at random.
At time $t$, choose $N$ different indices $\kappa_{t+1}^n$ from $[d]$
\textit{with replacement}, with each choice being independent of the rest,
and also of past choices.
Moreover, each $\kappa_{t+1}^n$ is chosen from $[d]$ with uniform probability.
Then define
\be\label{eq:333}
\bphi^{(3)}_{t+1} := \frac{d}{N} \sum_{n=1}^N \eb_{\kappa_{t+1}^n}
\circ \bphi_{t+1} .
\ee
In this option, $\bphi^{(3)}_{t+1}$ can have \textit{up to} $N$ nonzero
components.
Because the sampling is \textit{with replacement}, there might
be some duplicated samples.
In such a case, the corresponding component of $\bphi_{t+1}$ simply
gets counted multiple times in \eqref{eq:333}.

\textbf{Option 4: Bernoulli Update:}
At time $t+1$, let $\{ B_{t+1,i}, i \in [d] \}$  be independent Bernoulli
processes with success rate $\r_{t+1}$.
Thus
\be\label{eq:334}
\Pr \{ B_{t+1,i} = 1 \} = \r_{t+1} , \fa i \in [d] .
\ee
It is permissible for the success probability $\r_{t+1}$ to vary with time.
However, at any one time, all components must have the same success
probability.
Then define
\be\label{eq:335}
\v_{t+1} := \sum_{i=1}^d \eb_i I_{ \{ B_{t+1,i} = 1 \} }  \in \bi^d .
\ee
Thus $\v_{t+1}$ is a random vector, and
$v_{t+1,i}$ equals $1$ if $ B_{t+1,i} = 1$, and equals $0$ otherwise.
Now define
\be\label{eq:336}
\bphi^{(4)}_{t+1} = \frac{1}{\r_{t+1}}  \v_{t+1} \circ \bphi_{t+1} .
\ee
In this option,
the search direction $\bphi^{(4)}_{t+1}$ can have up to $d$ nonzero components.
However, the \textit{expected} number of nonzero components is $\r_{t+1} d$.

The key point is this: In all four options, ``on average'' $\bphi^{(n)}_{t+1}$
equals $\bphi_{t+1}$, for $n = 1 , \cdots , 4$.
Also, when the choice of the batch update direction involves some
random choices (such as $\kappa_{t+1}$ or $B_{t+1}$), the filtration
$\{ \F_t \}$ is chosen in such a way that all of these random variables
are measurable with respect to $\F_t$.
Note that, the subscript is $t+1$ because the choice is \textit{implemented}
at time $t+1$.
However, the random variables are measurable with respect to $\F_t$.

\textbf{Options 1A through 4A:}
Next we introduce Options 1A through 4A.
These are the same as Options 1 through 4, except that the true gradient
% $\gJ(\bth_t)$ in the case of SHB and
% $\gJ(\bth_t + \mu ( \bth_t - \bth_{t-1}) )$ in the case of SNAG
is approximated by a vector of first-order differences.
This approach goes back at least to \cite{Kief-Wolf-AOMS52} for
the scalar case, and to \cite{Blum54} for the case $d \geq 2$.
% The approach suggested in \cite{Blum54} is to evaluate $J(\bth_t)$,
% and then $J(\bth_t + c_t \eb_i)$ for $i \in [d]$, where $\eb_i$
% is the elementary basis vector with a $1$ in the $i$-component and zeros
% elsewhere, and $c_t$ is called the ``increment'' (not to be confused with
% the ``step size'' $\al_t$).
% Then
% \bd
% [\gJ(\bth_t)]_i \approx \frac{J(\bth_t + c_t \eb_i) - J(\bth_t) }{c_t} ,
% i \in [d] .
% \ed
% The above approach requires $d+1$ function evaluations, which is
% impractical when $d$ is large, as it often is in neural network training
% for example.
In \cite{Spall-JHU98}, the idea of Simultaneous Perturbation Stochastic
Approximation (SPSA) is proposed, wherein an approximation to $\gJ(\bth_t)$
is constructed using \textit{only two} function evaluations, irrespective
of the value of $d$.
The method consists of choosing, at each time $t$,
$d$ different \textbf{Rademacher random variables}
$\D_{t,1} , \cdots , \D_{t,d}$\footnote{Recall that Rademacher
random variables assume values in $\bp$ and are independent of each other.}
Define $\bD_{t+1}$ to be the corresponding random vector in $\bp^d$.
Then, for SHB we define
\beq
\phi^A_{t+1, i} & = & \frac{J(\bth_t - c_t \bD_{t+1}) + \xi_{t+1,i}^- }
{2 c_t \D_{t+1, i}} \nonumber \\
& - & \frac{J(\bth_t + c_t \bD_{t+1}) - \xi_{t+1,i}^+ } {2 c_t \D_{t+1, i}} ,
i \in [d] .
\label{eq:337}
\eeq
% For the SNAG we define
% \beq
% \phi^{A,NAG}_{t+1, i} & = &
% \frac{J(\bth_t + \mu( \bth_t - \bth_{t-1} ) - c_t \bD_{t+1}) + \xi_{t+1,i}^- }
% {2 c_t \D_{t+1, i}} \nonumber \\
% & - & \frac{J(\bth_t + \mu( \bth_t - \bth_{t-1} ) + c_t \bD_{t+1}) - \xi_{t+1,i}^+ } {2 c_t \D_{t+1, i}} ,
% i \in [d] .
% \label{eq:338}
% \eeq
where $\bxi_{t+1}^- , \bxi_{t+1}^+$ are measurement errors.
Once we define the above ``approximate'' search directions, we can
go ahead and do batch updating as in Options 1 through 4.
We denote these as Options 1A through 4A.
% Strictly speaking, we should also differentiate between \eqref{eq:337}
% \eqref{eq:338}.
% But we do not do so to avoid notational clutter.
% {\color{verm}Revisit this sentence once the paper is nearly complete.
% Instead, we analyze Options 1A through 4A for SHB (\eqref{eq:337}),
% and indicate how the analysis can be modified for SNAG (\eqref{eq:338}).}
% 
It can be seen that, to first order in $c_t$ (and ignoring the errors
$\bxi^-_{t+1}$ and $\bxi^-_{t+1}$),
the right side of \eqref{eq:337}
is an approximation to $-[\gJ(\bth_t)]_i$ for each $i \in [d]$.
% Similarly for \eqref{eq:338}.

In the definition \eqref{eq:337},
% and \eqref{eq:338},
it is possible to use other probability distributions for $\bD$.
For instance, in \cite{Nesterov-Spokoiny-FCM15}, the perturbations
$\D_{t+1,i}$ are assumed to have Gaussian distributions.
Note that, if $\D_{t+1,i}$ are binary, then $\nmeu{\bD_{t+1}} = \sqrt{d}$,
and as a result,
\be\label{eq:339}
\frac{ \nmeu{\bD_{t+1} } }{ | \D_{t+1,i} | } = \sqrt{d} .
\ee
With Gaussian variables, this ratio can be artibrarily large, but with
ever lower probability.
This is an unnecessary complication that we wish to avoid.
A further generalization of SPSA is given in \cite{Bhat-Pras-arxiv22}
which requires $2k$ function evaluations instead of $2$, 
where $k$ is order of G-SPSA. We do not study this extension in this paper.


\subsection{Analysis of Various Options}\label{ssec:34}

The objective of Lemma \ref{lemma:31} below is to relate the conditional
variance of Options 2, 3 and 4 to that of Option 1.
Similarly, Lemma \ref{lemma:32} relates the conditional variance of
Options 1A through 4A to those of $\bphi_{t+1}$.
% Basically, this lemmas allows us to show that, once we establish the
% convergence of either SHB or SNAG using full coordinate update,
% the convergence of all the other options follows readily.

% \begin{lemma}\label{lemma:31}
% Define $\w_t := E_t( \bphi^{(1)}_{t+1} )$.
% Then
% \be\label{eq:341}
% E_t ( \nmeusq{ \bphi^{(2)}_{t+1} - \w_t } ) = (d-1) \nmeusq {\w_t} ,
% \ee
% \be\label{eq:342}
% E_t ( \nmeusq{ \bphi^{(3)}_{t+1} - \w_t } ) = (d-1) \nmeusq {\w_t} ,
% \ee
% \be\label{eq:343}
% E_t ( \nmeusq{ \bphi^{(4)}_{t+1} - \w_t } ) = \frac{d(1 - \r_t)}
% {\r_t} \nmeusq {\w_t} .
% \ee
% \end{lemma}

\begin{lemma}\label{lemma:31}
Define $\w_t := E_t( \bphi^{(1)}_{t+1} )$.
Then
\be\label{eq:341}
E_t ( \nmeusq{ \bphi^{(2)}_{t+1} - \w_t } ) = (d-1) \nmeusq {\w_t} ,
\ee
\be\label{eq:342}
E_t ( \nmeusq{ \bphi^{(3)}_{t+1} - \w_t } ) = (d-1) \nmeusq {\w_t} ,
\ee
\be\label{eq:343}
E_t ( \nmeusq{ \bphi^{(4)}_{t+1} - \w_t } ) = \frac{d(1 - \r_t)}
{\r_t} \nmeusq {\w_t} .
\ee
\end{lemma}

\begin{lemma}\label{lemma:32}
Define $\w_t := E_t( \bphi^{(1)}_{t+1} )$.
The
\be\label{eq:344}
E_t ( \nmeusq{ \bphi^{(1A)}_{t+1} - \w_t } )
= \frac{(d-1)}{2c_t^2} \nmeusq {\w_t} ,
\ee
\be\label{eq:345}
E_t ( \nmeusq{ \bphi^{(2A)}_{t+1} - \w_t } )
= \frac{(d-1)}{2c_t^2} \nmeusq {\w_t} ,
\ee
\be\label{eq:346}
E_t ( \nmeusq{ \bphi^{(3A)}_{t+1} - \w_t } )
= \frac{(d-1)}{2c_t^2} \nmeusq {\w_t} ,
\ee
\be\label{eq:347}
E_t ( \nmeusq{ \bphi^{(4A)}_{t+1} - \w_t } ) = \frac{d(1 - \r_t)}
{2\r_tc_t^2} \nmeusq {\w_t} .
\ee
Additionally,
\be\label{eq:348}
\nmeu{\bphit_{t+1}^{xA}} \le \frac{dL}{2}c_t , \forall x \in [1, 2, 3, 4].
\ee
\end{lemma}

\section{Statement of Main Theorems}\label{sec:Main}

\subsection{Assumptions on the Objective Function and Search Direction}
\label{ssec:41}

In this paper, we study functions $J : \R^d \ap \R$ that satisfy the
following \textbf{Standing Assumptions:}
\ben
\item[(A1)] $J(\cdot)$ is $\C^1$.
Moreover, $\gJ(\cdot)$ is globally Lipschitz-continuous with constant $L$.
\item[(A2)] $J(\cdot)$ has a global infimum $J^*$, which is attained.
\item[(A3)] $J(\cdot)$ has compact level sets.
Thus, for every fixed constant $r$, the set
\bd
L_J(r) := \{ \bth : J(\bth) \leq r \}
\ed
is compact.
\een
Note that an equivalent way of stating Assumption (A3) is that
\bd
\lim_{\nmeu{\bth} \ap \infty} J(\bth) = \infty .
\ed
Such a function is called \textbf{radially unbounded}, or
\textbf{coercive}.
We do not use either term in this paper.

As stated above, (A1)--(A3) are the standing assumptions.
Now we state other assumptions.
Note that different assumptions are used in different theorems.
For this purpose, we define
\bd
\Jb(\bth) := J(\bth) - J^* .
\ed
We also make use of the following concept 
from \cite[Definition 1]{MV-MCSS-arxiv23}:
A function $\eta : \R_+ \ap \R_+$ is
said to \textbf{belong to Class $\B$} if $\eta(0) = 0$, and in addition
\bd
\inf_{\e \leq r \leq M} \eta(r) > 0 , \fa 0 < \e < M < \infty .
\ed
Note that if $\eta : \R_+ \ap \R_+$ is continuous, then
$\eta(\cdot)$ belongs to class $\B$ if and only if (i) $\eta(0) = 0$,
and (ii) $\eta(r) > 0$ for all $r > 0$.
\ben
\item[(J1)] There exists a constant $H$ such that
\be\label{eq:411}
\nmeusq{ \gJ(\bth) } \leq H \Jb(\bth) , \fa \bth \in \R^d .
\ee
\item[(J2)]
There is a function $\mu(\cdot)$ belonging to Class $\B$ such that
\be\label{eq:412}
\mu(\Jb(\bth)) \leq \nmeu{\gJ(\bth)} , \fa \bth \in \R^d .
\ee
\item[(J3)] Define
\be\label{eq:413}
S(J) := \{ \bth : \gJ(\bth) = \bz \} .
\ee
The assumption is:
There is a function $\nu(\cdot)$ belonging to Class $\B$
such that
\be\label{eq:414}
\r(\bth,S(J)) \leq \nu(\Jb(\bth) )  , \fa \bth \in \R^d .
\ee
where
\bd
\r(\bth,S(J)) := \inf_{\bpsi \in S(J)} \nmeu{\bth - \bpsi } 
\ed
is the distance between $\bth$ and the set $S(J)$.
\een

Now we discuss the significance of these assumptions.
First it is shown that these assumptions hold readily in the case
where $J(\cdot)$ is convex with some additional conditions.
Then an example is presented where these assumptions hold even though
$J(\cdot)$ is not convex.

If $J(\cdot)$ is convex, then it is known
that \eqref{eq:411} is satisfied with $H = 2L$, where $L$ is the Lipschitz
constant of $\gJ(\cdot)$; see \cite[Theorem 2.1.5]{Nesterov04}.
If in addition $J(\cdot)$ is also $M$-strongly convex, in the sense that
\bd
J(\bpsi) \geq J(\bth) + \IP{\gJ(\bth)}{\bpsi - \bth} + \frac{M}{2}
\nmeusq{ \bpsi - \bth } ,
\ed
then it follows from \cite[Theorem 2.1.10]{Nesterov04} that
\bd
\Jb(\bth) \leq \frac{1}{2M} \nmeusq{\gJ(\bth} , \fa \bth \in \R^d .
\ed
This is the same as (J2) with $\mu(r) = \sqrt{2Mr}$.
In this case, $S(J)$ is a singleton set, say $\{ \bths \}$, so that
$\r(\bth,S(J)) = \nmeu{\bth - \bths}$, and
$J(\bth) \leq (L/2) \nmeusq{\bth - \bths}$.
Therefore (J3) is also satisfied.

\begin{example}\label{exam:31}
Consider the following function $J : \R \ap \R$:
\bd
J(\th) := \left\{ \ba{ll} \sin[(\pi/2)(\th-1)] + 1 , & -5 \leq \th \leq 5 , \\
0.5 + \sqrt{(\pi/2)(\th-5) + 0.25}, & 5 \leq \th , \\
J(-\th), & \th \leq -5 , \ea \right.
\ed
which is depicted in Figure \ref{fig:exam-1}
It has multiple local minima, but each of them is also a global minimum.
This function satisfies Assumptions (J1)--(J3).
However, if a function $J(\cdot)$ had some local minima that are not also
global minima, then (J2) and (J3) would fail to be satisfied.
\bfig[htb]
\bc
\includegraphics[width=80mm]{Example-1_v2.eps}
\ec
\caption{Depiction of a function with multiple minima satisfying
Assumptions (J4) and (J5)}
\label{fig:exam-1}
\efig
\end{example}

Next we state the assumptions on $\phH$, which equals 
$E_t ( \bphH) + \gJ(\bth_t)$, as defined in \eqref{eq:326}.
Note that these assumptions are the same as in \cite{MV-TUKR-ICML23}.

\ben
\item[(D1).] There is a sequence of deterministic constants $\{ b_t \}$ such
that
\be\label{eq:415}
\nmeusq{ \phH} \leq b_t^2 , \fa t .
\ee
\item[(D2).] There is a sequence of deterministic constants $\{ M_t \}$ such
that
\be\label{eq:416}
E_t( \nmeusq{\bzeta_{t+1} } ) \leq M_t^2 (1 + \nmeusq{\gJ(\bth_t) } ) , \fa t .
\ee
\een

Now we discuss the significance of these assumptions.
Assumption (D1) is less restrictive than requiring that 
$E_t(\bphH) = - \gJ(\bth_t)$ (or equivalently $\phH = \bz$),
which is the prevailing assumption in almost all existing papers.
% In this case $\phH = \bz$ for all $t$.
When approximate gradients are used, it becomes necessary to permit
$\phH$ to be nonzero.
Next we come to Assumption (D2).
In \cite{Khaled-Rich-arxiv20}, the following condition is stated as
the weakest condition for analyzing stochastic gradient descent for
nonconvex objective functions:
There exist constants $A, B, C$ such that
\be\label{eq:417}
E_t( \nmeusq{\bzeta_{t+1} } ) \leq A \Jb(\bth_t) + B \nmeusq{\gJ(\bth_t)} + C .
\ee
However, if Assumption (J1) holds, then \eqref{eq:417} can be recast
in the form \eqref{eq:416}, with $M_t$ being uniformly bounded with
respect to $t$.
As we shall see below, when approximate gradients are used, \eqref{eq:416}
does not hold with bounded $M_t$.
It becomes necessary to permit $M_t$ to grow without bound.

\subsection{Main Theorems}

In this subsection we state the main theorems.
Note that, throughout all statements hold almost surely.
The first theorem pertains to the boundedness and convergence of
the iterations $\{ \bth_t \}$.

\begin{theorem}\label{thm:SHB}
Suppose the Standing Assumptions (A1)--(A3)
about $J(\cdot)$ hold, as well as Assumptions (D1)--(D2) about $\phH$.
Suppose any one of Options (1)--(4) is applied in the SHB algorithm.
\ben
\item Suppose Assumption (J1) holds, and in addition
\be\label{eq:421}
\sum_{t=0}^\infty \al_t^2 < \infty ,
\sum_{t=0}^\infty \al_t b_t  < \infty ,
\sum_{t=0}^\infty \al_t^2 M_t^2 < \infty .
\ee
Then $\{ J(\bth_t) \}$ and $\{ \bth_t \}$ are bounded.\footnote{In this
theorem, all statements hold almost surely.}
\item Suppose that, in addition to \eqref{eq:421}, we also have that
\be\label{eq:422}
\sum_{t=0}^\infty \al_t = \infty .
\ee
Then
\be\label{eq:423}
\liminf_{\tai} \nmeu{ \gJ(\bth_t)} = 0 .
\ee
\item If in addition to the assumptions in Item 2, we also add Assumption (J2),
then $\gJ(\bth_t) \ap \bz$ as $\tai$.
\item If in addition to the assumptions in Item 3, we also add Assumption (J3),
then $\r(S(J),\bth_t) \ap 0$ as $\tai$.
\een
\end{theorem}

The above theorem is applicable to Options 1 through 4.
If we now replace these by their A counterparts (that is, replacing
the true gradient by the approximate gradient), we get the following theorem:

\begin{theorem}\label{thm:SHB-A}
Suppose the Standing Assumptions (A1)--(A3) about $J(\cdot)$ hold, as well as
Assumptions (D1)--(D2) about $\phH$.
Suppose any one of Options (1A)--(4A) are applied in the SHB algorithm.
\ben
\item Suppose Assumption (J1) holds, and in addition
\be\label{eq:424}
\sum_{t=0}^\infty \al_t^2 < \infty ,
\sum_{t=0}^\infty \al_t c_t  < \infty ,
\sum_{t=0}^\infty ( \al_t / c_t )^2 < \infty .
\ee
Then $\{ J(\bth_t) \}$ and $\{ \bth_t \}$ are bounded.
\item Suppose that, in addition to \eqref{eq:421}, we also have that
\be\label{eq:422}
\sum_{t=0}^\infty \al_t = \infty .
\ee
Then
\be\label{eq:425}
\liminf_{\tai} \nmeu{ \gJ(\bth_t)} = 0 .
\ee
\item If in addition to the assumptions in Item 2, we also add Assumption (J2),
then $\gJ(\bth_t) \ap \bz$ as $\tai$.
\item If in addition to the assumptions in Item 3, we also add Assumption (J3),
then $\r(S(J),\bth_t) \ap 0$ as $\tai$.
\een
\end{theorem}

The next theorem is about the \textit{rate} of convergence in Theorem
\ref{thm:SHB}.
Before stating it, we specify what ``rate of convergence'' means for a
stochastic process.

\begin{definition}\label{def:Rate}
Suppose $\{ X_t \}$ is a stochastic process on a probability space
$(\OM , \SI , P)$, and that $Y: \R_+ \ap \R_+$ is a measurable function.
Then we say that $X_t = O(Y_t)$ if, for each $\om \in \OM$, there exists
a constant $C(\om)$ such that
\be\label{eq:426}
|X_t(\om)| \leq C(\om) Y(t) , \fa t .
\ee
\end{definition}

Thus, the bounding constant $C(\om)$ could vary from one sample path to
another, and there is no presumption that there is a uniform bound across
sample paths.
%\begin{theorem}\label{thm:SHB-Rate}
%Analogous to the results of Liu-Yuan \cite{Liu-Yuan-arxiv22}.
%\end{theorem}

\section{Proofs}\label{sec:Proofs}

Recall the SHB algorithm defined in \eqref{eq:321}.
We now define
\be\label{eq:501}
\v_t := \bth_t - \bth_{t-1} , \z_t = \bth_t - \frac{\mu}{1 - \mu} \v_t .
\ee
Then, as shown in \cite{Liu-Yuan-arxiv22}, it is possible to rearrange 
the three equations \eqref{eq:321} and \eqref{eq:501} as
\be\label{eq:502}
\v_{t+1} = \mu \v_t + \al_t \bphH ,
\ee
\be\label{eq:502a}
\z_{t+1} = \z_t + \frac{\al_t}{1 - \mu} \bphH .
\ee
The advantage of the above reformulation is that the original
updating formula for $\bth_{t+1}$ is divided into two separate
updates, for $\v_{t+1}$ and $\z_{t+1}$ respectively,
in such a way that neither update contains a ``delay'' term.
Moreover, the variable of optimization $\bth_t$ can be expressed
from \eqref{eq:501} as
\be\label{eq:503}
\bth_t = \z_t + \frac{\mu}{1 - \mu} \v_t .
\ee

The proof of Theorem \ref {thm:SHB}
is based on the ``almost supermartingale lemma'' of Robbins \&
Siegmund Theorem \cite{Robb-Sieg71}.
That paper is rather difficult to locate.
However, the same theorem is stated as \cite[Lemma 2, Section 5.2]{BMP92}.
A recent survey of many results along similar lines is found in
\cite{Fran-Gram22}, where Lemma \ref{lemma:51} below is stated as Lemma 4.1.
The result states the following:

\begin{lemma}\label{lemma:51}
Suppose $\{ z_t \} , \{ \d_t \} , \{ \g_t \} , \{ \psi_t \}$ are
stochastic processes defined on some probability space $(\OM,\SI,P)$,
taking values in $[0,\infty)$, adapted to some
filtration $\{ \F_t \}$, satisfying
\be\label{eq:504}
E(z_{t+1} | \F_t ) \leq (1 + \d_t) z_t + \g_t - \psi_t \as, \fa t .
\ee
Define
\be\label{eq:505}
\OM_0 := \{ \om \in \OM : \sum_{t=0}^\infty \d_t(\om) < \infty \}
\cap \{ \om : \sum_{t=0}^\infty \g_t(\om) < \infty \} .
\ee
Then for all $\om \in \OM_0$, $\lim_{\tai} z_t(\om)$ exists, and in addition,
\be\label{eq:506}
\sum_{t=0}^\infty \psi_t(\om) < \infty , \fa \om \in \OM_0 .
\ee
In particular, if $P(\OM_0) = 1$, then $\{ z_t \}$ is bounded almost surely.
\end{lemma}

The proof consists of showing that the combined quantity
$J(\z_{t+1}) + \nmeusq{\v_{t+1} }$ satisfies the conditions of 
Lemma \ref{lemma:51}.
For this purpose, we find bounds for $E_t(J(\z_{t+1}))$ and for
$E_t( \nmeusq{\v_{t+1} } )$ and add them.
Since there is a lot of computation involved, it is a good idea to
keep the overall objective in mind.

Throughout various proofs, we make repeated use of the following
simple identity:
For $x,y \in \R, C > 0$, we have
\be\label{eq:51}
2Cxy \leq \e x^2 + \frac{C^2}{\e} y^2 , \fa \e > 0 .
\ee
To see this, note that
\bd
0 \leq \left( \sqrt{\e} x - \frac{C}{\sqrt{\e}} y \right)^2 
= \e x^2 + \frac{C^2}{\e} y^2 - 2Cxy .
\ed

We begin by providing an upper bound on $E_t( \nmeusq{ \bphH } )$.

\begin{lemma}\label{lemma:52}
Suppose Assumptions (D1), (D2) and (J1) hold.
Then
\be\label{eq:52}
E_t( \nmeusq{ \bphH } ) \leq (b_t^2 + b_t + M_t^2) 
+ H ( b_t +1 + M_t^2 ) \Jb(\bth_t) .
\ee
\end{lemma}

\begin{proof}
Recall from \eqref{eq:326} and \eqref{eq:324} that
\bd
\bphH = \phH - \gJ(\bth_t) + \bzeta_{t+1} .
\ed
Since $E_t(\bzeta_{t+1}) = \bz$, it follows that
\beq
E_t ( \nmeusq{ \bphH } ) & = & \nmeusq{ \phH - \gJ(\bth_t) }
+ E_t( \nmeusq{ \bzeta_{t+1} } ) \label{eq:53a}
\eeq
Define
\be\label{eq:53}
S_1 := \nmeusq{ \phH - \gJ(\bth_t) } , S_2 = E_t( \nmeusq{ \bzeta_{t+1} } ) .
\ee
% & =: & S_1 + S_2 . \label{eq:53}
% \eeq
Now Assumptions (D1) and (J1) imply that
\beq
% \nmeusq{ \phH - \gJ(\bth_t) } & \leq & \nmeusq{ \phH }
S_1 & \leq & \nmeusq{ \phH }
+ 2 \nmeu{ \phH } \cdot \nmeu{ \gJ( \bth_t ) } 
+ \nmeusq{\gJ(\bth_t)} \nonumber \\
& \leq & b_t^2 + b_t ( 1 + \nmeusq{ \gJ(\bth_t) })
+ \nmeusq{\gJ(\bth_t)} \nonumber \\
& \leq & b_t^2 + b_t ( 1 + H \Jb(\bth_t) ) + H \Jb(\bth_t ) ,
\label{eq:54}
\eeq
while Assumption (D2) implies that
\be\label{eq:55}
E_t( \nmeusq{ \bzeta_{t+1} } ) \leq M_t^2 ( 1 + H \Jb(\bth) )
\ee
Substituting from \eqref{eq:54} and \eqref{eq:55} into \eqref{eq:53a}
gives \eqref{eq:52}.
\end{proof}

\begin{lemma}\label{lemma:53}
Define $S$ denote the right side of the bound in \eqref{eq:52}.
Then, for every $\e_1 > 0$, we have that
\be\label{eq:56}
E_t( \nmeusq{v_{t+1} } ) \leq \mu^2 \nmeusq{\v_t} 
+ \al_t^2 S \nonumber \\
+ \mu \al_t b_t (1 + \nmeusq{\v_t} ) \nonumber \\
+ \mu \left( \e_1 \nmeusq{\v_t} + \frac{\al_t^2}{\e_1} 
H \Jb(\bth_t) \right) .  
\ee
\end{lemma}

\begin{proof}
It follows from \eqref{eq:502} that
\be\label{eq:57}
\nmeusq{\v_{t+1} } = \mu^2 \nmeusq{\v_t} +\al_t^2
\nmeusq{\bphH } + 2 \mu \al_t \IP{\bphH }{\v_t}.
\ee
Lemma \ref{lemma:52} already shows that
\bd
E_t( \nmeusq{\bphH } ) \leq S ,
\ed
while $\nmeusq{\v_t} \in \M(\F_t)$.
These are the first two terms on the right side of \eqref{eq:56}.
So it remains only to bound the last term.
Define
\be\label{eq:58}
S_3 = E_t ( 2 \mu \al_t \IP{\bphH }{\v_t} ) .
\ee
Then
\beq
S_3 & = & 2 \mu \al_t \IP{E_t(\bphH)}{\v_t} \nonumber \\
& = & 2 \mu \al_t \IP{ \phH - \gJ(\bth_t) }{\v_t} \nonumber \\
& \leq & 2 \mu \al_t b_t \nmeu{\v_t} + 2 \mu \al_t 
\nmeu{\gJ(\bth_t)} \cdot \nmeu{\v_t} \nonumber \\
& \leq & \mu \al_t b_t (1 + \nmeusq{\v_t}) 
+ \mu \left( \e_1 \nmeusq{\v_t} + \frac{\al_t^2}{\e_1}H \Jb(\bth_t)
\right) , \label{eq:59}
\eeq
where in the last step we use Assumption (J1).
Substituting from \eqref{eq:59} into \eqref{eq:57} gives the desired conclusion.
\end{proof}

\begin{lemma}\label{lemma:54}
We have that, for every $\e_2 > 0$,
\be\label{eq:510}
E_t(\Jb(\z_{t+1})) \leq \left(1 + \frac{H\al_tb_t}{2(1-\mu)}\right)\Jb(\z_t) 
+ \frac{\al_t^2 L}{2(1 - \mu)^2}S
+ \e_2 \nmeusq{\v_t} 
- \left[ \frac{\al_t}{ 1 - \mu}
- \frac{\al_t^2 \mu^2 L^2}{(1-\mu)^4} \right]
\nmeusq{\gJ(\bth_t)} + \frac{\al_tb_t}{2(1-\mu)} . 
\ee
\end{lemma}

\begin{proof}
Recall from \eqref{eq:502a} that
\bd
\z_{t+1} = \z_t + \frac{\al_t}{1 - \mu} \bphH .
\ed
From \cite[Eq.\ (2.40)]{Ber-Tsi-SIAM00}, we know that
\bd
\Jb(\z_{t+1}) \leq \Jb(\z_t) + \frac{\al_t}{1 - \mu}
\IP{ \gJ(\z_t)}{\bphH } 
+ \frac{L}{2}\left( \frac{\al_t}{1 - \mu} \right)^2
\nmeusq{ \bphH } .
\ed
Therefore
\bd
E_t(\Jb(\z_{t+1}) ) \leq \Jb(\z_t) 
+ \frac{L}{2}\left( \frac{\al_t}{1 - \mu} \right)^2 S 
+ \frac{\al_t}{1 - \mu} S_4 ,
\ed
where
\beq
S_4 & = & E_t(\IP{ \gJ(\z_t)}{\bphH }) \nonumber \\
& = & \IP{ \gJ(\z_t)}{\phH - \gJ(\bth_t)} \nonumber \\
& \leq & b_t \nmeu{\gJ(\z_t)} - \IP{ \gJ(\z_t)}{\gJ(\bth_t)} \nonumber \\
& \leq & \frac{b_t}{2} (1 + \nmeusq{\gJ(\z_t)})
- \IP{ \gJ(\z_t)}{\gJ(\bth_t)} \nonumber \\
& \leq & \frac{b_t}{2} (1 + H \Jb(\z_t))
- \IP{ \gJ(\z_t)}{\gJ(\bth_t)} \label{eq:511}
\eeq
So now we bound the inner product.
Note that
\bd
- \IP{ \gJ(\z_t)}{\gJ(\bth_t)} = - \nmeusq{\gJ(\bth_t)} 
+ \IP{\gJ(\bth_t) - \gJ(\z_{t})}{\gJ(\bth_t)} .
\ed
Next, from the $L$-Lipschitz-continuity of $\gJ(\cdot)$, we have
\bd
\nmeu{\gJ(\bth_t) - \gJ(\z_{t})} \leq L \nmeu{ \bth_t - \z_{t} } 
= \frac{\mu L}{1 - \mu} \nmeu{\v_t} .
\ed
We conclude that
\bd
- \IP{ \gJ(\z_t)}{\gJ(\bth_t)} \leq - \nmeusq{\gJ(\bth_t)} 
+ \frac{\mu L}{1 - \mu} \nmeu{\gJ(\bth_t))} \cdot \nmeu{\v_t} .
\ed
Hence, for all $\e_2 > 0$, we have
\begin{eqnarray*}
\frac{\al_t}{1 - \mu} S_4 & \leq & - \frac{\al_t}{1 - \mu}
\nmeusq{\gJ(\bth_t)} 
+  \frac{\al_t \mu L}{(1 - \mu)^2} \nmeu{\gJ(\bth_t))} \cdot \nmeu{\v_t}
+ \frac{\al_tb_t}{2(1-\mu)} (1 + H \Jb(\z_t))\\
& \leq & - \frac{\al_t}{1 - \mu}
\nmeusq{\gJ(\bth_t)} 
+ \e_2 \nmeusq{\v_t}
+ \frac{\al_t^2 \mu^2 L^2}{2\e_2 (1- \mu)^4} \nmeusq{\gJ(\bth_t)} 
+ \frac{\al_tb_t}{2(1-\mu)} (1 + H \Jb(\z_t)).
\end{eqnarray*}
Substituting all of these bounds into
\be\label{eq:512}
E_t(\Jb(\z_{t+1}) \leq \Jb(\z_t) + \frac{ \al_t^2 L}{2(1-\mu)^2} S
+ \frac{\al_t}{1 - \mu} S_4 
\ee
and collecting the coefficients of
$\nmeusq{\gJ(\bth_t)}$ in one place gives the desired upper bound 
\eqref{eq:510}.
\end{proof}

We can now add the bounds for $\Jb(\z_{t+1})$ and $\nmeusq{\v_{t+1}}$.
However, these bounds involve $\Jb(\bth_t)$ or $\nmeusq{\gJ(\bth_t)}$.
We now show how to replace these by bounds involving
$\Jb(\z_t)$ and $\nmeusq{\gJ(\z_t)}$.

\begin{lemma}\label{lemma:55}
If Assumption (J1) holds, then
\be\label{eq:513}
\Jb(\bth_t) \leq \left( 1 + \frac{H}{2} \right) \Jb(\z_t)
+ \frac{\mu^2 (1+L)} {2(1-\mu)^2} \nmeusq{\v_t} ,
\ee
%\be\label{eq:514}
%\nmeusq{\gJ(\bth_t)} \leq 2H\Jb(\z_t)
%+ \frac{2 \mu^2 L^2}{(1-\mu)^2} \nmeusq{\v_t} .
%\ee
\end{lemma}

\begin{proof}
Observe that
\begin{eqnarray*}
\Jb(\bth_t) & \leq & \Jb(\z_t) + \frac{\mu}{1 - \mu}
\IP{\gJ(\z_t)}{\v_t} 
+ \frac{\mu^2 L}{(1 - \mu)^2} \nmeusq{\v_t} \\
& \leq & \Jb(\z_t) + \half \nmeusq{\gJ(\z_t)} 
+ \frac{\mu^2}{2 (1 - \mu)^2 } \nmeusq{\v_t}  
+ \frac{\mu^2 L}{(1 - \mu)^2} \nmeusq{\v_t} .
\end{eqnarray*}
After substituting $\nmeusq{\gJ(\z_t)} \leq H \Jb(\z_t)$, this gives
\eqref{eq:513}.
%Next,
%\begin{eqnarray*}
%\nmeusq{\gJ(\bth_t)} & = & \nmeusq{ \gJ(\z_t) + \gJ(\bth_t) - \gJ(\z_t)  } \\
%& \leq & \nmeusq{ \gJ(\z_t) } + \nmeusq{\gJ(\bth_t) - \gJ(\z_t)  }
%+ 2 \nmeu{ \gJ(\z_t) } \cdot \nmeu{\gJ(\bth_t) - \gJ(\z_t)  } \\
%& \leq & 2 \nmeusq{ \gJ(\z_t) } + 2 \nmeusq{\gJ(\bth_t) - \gJ(\z_t)  } \\
%& \leq & 2 \nmeusq{\gJ(\z_t)}
%+ \frac{2 \mu^2 L^2}{(1-\mu)^2} \nmeusq{\v_t} .
%\end{eqnarray*}
%By invoking Assumption (J1) we obtain \eqref{eq:514}.
\end{proof}

(Of Theorem \ref{thm:SHB}:)
Now we can put Lemma \ref{lemma:53}, \ref{lemma:54}, \ref{lemma:55} together
and obtain an upper bound of the form
\bd
E_t( \Jb(\z_{t+1}) + \nmeusq{\v_{t+1} } ) \leq \Jb(\z_t)
+ C_1 + HC_2 \Jb(\z_t) 
+ C_3 \nmeusq{\v_t} - C_{4,t} \nmeusq{\gJ(\bth_t)} ,
\ed
where $C_1, C_2$ are messy but constant,
\bd
C_1 = \mu\al_tb_t + 
\al_t^2\left(b_t^2 + b_t + M_t^2 + \frac{L}{2(1-\mu)}+ \frac{\mu}{\e_1}\right)
\ed
\bd
C_2 =  \frac{\al_tb_t}{2(1-\mu)} + \al_t^2\left(1+\frac{H}{2}\right)
\left(b_t + 1 + \frac{\mu}{\e_1} + M_t^2\right)
\ed
\bd
C_3  = \mu^2 + \mu \al_t b_t + \mu \e_1 + \e_2 
+ \al_t^2 \frac{\mu^2 (1+L)}{2(1-\mu)^2}
\left(b_t + 1 + \frac{\mu}{\e_1} + M_t^2\right),
\ed
\bd
C_{4,t} = \frac{\al_t}{ 1 - \mu}
- \frac{\al_t^2 \mu^2 L^2}{(1-\mu)^4} .
\ed
Suppose the hypotheses \eqref{eq:421} holds.
Then, and by suitably choosing $\e_1$
and $\e_2$, we can make the ``constant'' coefficient of $\nmeusq{\v_t}$
no larger than one, and the time-varying coefficient is a summable sequence.
Then the Robbins-Siegmund theorem leads to the conclusion that 
$\Jb(\z_t) + \nmeusq{\v_t}$ is bounded almost surely.
Thus $\Jb(\z_t)$ and $\nmeusq{\v_t}$ are both bounded.
Now Lemma \ref{lemma:55} shows that $\Jb(\bth_t)$ is bounded.
This is the first part of the conclusions.
Second, suppose \eqref{eq:422} also holds.
Then
\bd
\sum_{t=0}^\infty C_{4,t} = \infty .
\ed
By arguments in \cite{MV-TUKR-ICML23}, the other conclusions follow.

\section{Numerical Examples}

In this section we present the outcomes of several numerical experiments.
Specifically, we applied batch updating in
% This section outlines the implementation and evaluation of batch updating for
various standard optimization algorithms, such as SGD, SHB, SNAG, ADAM, NADAM,
and RMSPROP optimizers \footnote{The author is referred to \cite{Ruder-arxiv16}
for the brief description of these optimization algorithms.}.
In the case of SNAG, we studied two variants: NAG\_F where the step size
$\al_t$ is varied and the momentum term $\mu$ is fixed, and NAG\_S
where the momentum term $\mu_t$ is scheduled according to the Nesterov's
sequence \cite{Nesterov-Dokl83}, and $\al_t = \al_0$, a fixed constant.
While applying these algorithms,
we used both noise-corrupted actual gradients, as well as approximate gradients.

We implemented four different options, namely Options 1, 1A, 4 and 4A.
That is, we implemented full updating with both noisy and approximate
gradients, and also batch updating where
the components to be updated at each time were chosen via
independent Bernoulli processes with a success rate of $\r$, which was
varied over a range of values.
To simulate noisy conditions, additive white Gaussian noise (AWGN) was added
with various signal-to-noise ratios (SNRs).
We used step and increment sequences of the form
\bd
\al_t = \frac{\al_0}{(1 + (t/\tau))^p},
c_t = \frac{c_0}{(1 + (t/\tau))^q}, \mu = 0.9 , \fa t .
\ed
where $\tau$ = 200, $\al_0 = 10^{-6}$, $c_0 = 10^{-4}$, $p = 1$,
and $q = 0.01$. For comparison, we set $\alpha_0$ to be the same
in ADAM, NADAM, and RMSPROP.
For SGD \&  NAG\_F, we used the same $\al_t$ and $\mu$(=0 in case of SGD),
while for NAG\_S, we chose $\al_t = 10^{-6}$ and 
$\mu_t$ to be the Nesterov sequence.

To evaluate the performance of batch updating, we chose the convex
objective function
\bd
\J(\bth_t) = \bth_t^TA\bth_t + \log(\sum_{i=0}^{d-1}e^{\th_{t, i}}) ,
\ed
where $\bth_t$ is a vector of 1 million parameters,
and $A$ is a block-diagonal matrix of size ($10^6$, $10^6$) consisting
of 100 Hilbert matrices, each of dimension $10^4 \times 10^4$.
The Hilbert matrix is known to be notoriously ill-conditioned,
and as its eigenvectors are not aligned with elementary basis vectors
\cite{Fettis-MC67}.
Therefore this represents a good choice for testing the
robustness of batch updating.

Here are the implementation details.
We implemented the
algorithms in \textit{Python} using \textit{PyTorch} framework.
The experiments were conducted on a workstation equipped with an
Intel Xeon Silver 4114 CPU @ 2.20GHz, 256 GB RAM,
and 4 NVIDIA GeForce RTX 2080 Ti GPUs, each with 12GB of memory.

\begin{figure*}[ht]
\includegraphics[width=80mm]{exact_final.eps}
\includegraphics[width=80mm]{exact_bcd_final.eps}
\caption{[LEFT] Option 1; [RIGHT] Option 4}
\label{fig:exact}
\end{figure*}

The results, as shown in Figure \ref{fig:exact}, demonstrate that
when noisy gradients are used, ADAM, NADAM and RMSPROP comfortably
outperform the other four methods.
Within those four, NAG\_S outperforms SHB, which in turn outperforms NAG\_F.
As expected, SGD performs the worst of all.
Moreover, the convergence of ADAM, NADAM and RMSPROP with Option 4 and
$\r = 0.2$ (only 20\% of components updated at each iteration) is comparable
to that of full update, after accounting for the reduced updating.
However, when approximate gradients are used in the place of noisy gradients,
as shown in Figure \ref{fig:approx}, NAG\_S diverges almost immediately, 
while ADAM, NADAM and RMSPROP neither converge nor diverge.
In fact these three methods perform worse that even SGD.
Among the rest, SGD converges least slowly, NAG\_F is intermediate,
and SHB performs the best.

Thus, the use of approximate gradients apparently makes it infeasible to use
NAG\_S, ADAM, NADAM and RMSPROP, whether with full update or batch update.
As pointed out earlier, implementing batch update with noisy gradient
does not lead to much savings in CPU time, because computing only some
components of the gradient vector is almost as CPU-intensive as computing
the entire gradient.
In contrast, with approximate gradients, the amount of computation is
proportional to $\r$ when Option 4A is used.
The fact that all three methods (SGD, NAG\_F and SHB) all converge
despite using approximate gradients is very encouraging.

\begin{figure*}[ht]
\includegraphics[width=80mm]{approx_final.eps}
\includegraphics[width=80mm]{approx_bcd_final.eps}
\caption{[LEFT] Option 1A; [RIGHT] Option 4A}
\label{fig:approx}
\end{figure*}

\begin{figure*}[ht]
\includegraphics[width=140mm]{approx_bcd_all_diff_delta_final.eps}
\caption{Option 4A at different $\rho$}
\label{fig:varyrho}
\end{figure*}

Figure \ref{fig:varyrho} shows that , as expected,
reducing $\rho$ results in slower
convergence because parameters are updated less frequently.
However, the iterations still converge for values of $\r$ as small as $0.1$,
that is, only 10\% of the components of $\bth_t$ are
updated on average at each iteration.

Finally, we also tested the robustness
of the batch updating schemes at various noise levels.
The results are shown in Figure \ref{fig:varysnr}. The convergence
rates of these algorithms are comaparable at all SNR levels.%
%this is because
%of implicit noise arising due to approximate gradients, which are computed
%using SPSA.

\begin{figure*}[ht]
\includegraphics[width=140mm]{approx_bcd_all_diff_snrs_final.eps}
\caption{Option 4A at different SNR levels}
\label{fig:varysnr}
\end{figure*}

\section{Conclusions and Future Work}\label{sec:Conc}

In this paper, we have analyzed the well-known Heavy Ball method 
under batch updating, using either noisy gradients or approximate gradients.
We have established some useful relationships on the conditional variance
of the resulting measurement errors in each case.
Using these bounds, we are able to prove the convergence of the Stochastic
Heavy Ball method in a variety of situations.

We then carried out a series of numerical computations to demonstrate the
following tentative conclusions:
\bit

\item When batch updating is applied to noisy gradients, methods such as
ADAM, NADAM and RMSPROP outperform Stochastic versions of pure gradient
descent, Heavy Ball, and two variants of Nesterov's method.
\item However, when batch updating is applied to approximate gradients,
Nesterov's original method diverges, while ADAM, NADAM and RMSPROP
barely show any reduction in the objective function.
In fact they perform worse than plain steepest descent.
On the other hand, Stochastic Heavy Ball performs the best.
\eit
Therefore further theoretical analysis is required to explore
why this is so.

% \bibliographystyle{IEEEtran}
% \bibliography{ML,Opt}

\input{Heavy-Ball-arxiv.bbl}

\end{document}
