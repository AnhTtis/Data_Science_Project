\documentclass[10pt,twocolumn,letterpaper]{article}



% Include other packages here, before hyperref.
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{float}
\usepackage{overpic}
\usepackage{caption}

%\everypar{\looseness=-1}
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{cleveref}
\Crefname{equation}{Eq.}{Eqs.}
\Crefname{section}{Sec.}{Sec.}



\iccvfinalcopy % *** Uncomment this line for the final submission

%\def\iccvPaperID{11702} % *** Enter the ICCV Paper ID here
%\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}


% Custom commands
% Each author should define their comment command
\newcommand{\authorcomment}[3]{{\color{#1} \textbf{#2}: {#3}}}
\newcommand{\ste}[1]{{\color{brown} \textbf{Stefano}: {#1}}}
\newcommand{\daniele}[1]{\authorcomment{teal}{Daniele}{#1}}
\newcommand{\filippo}[1]{\authorcomment{orange}{Filippo}{#1}}
\newcommand{\neural}[2]{#1_{\bm{#2}}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\expnumber}[2]{{#1}\mathrm{e}{#2}}
\newcommand{\loss}[1]{\mathcal{L}_{\text{#1}}}


% Math operators
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%% Teaser
\makeatletter
\let\@oldmaketitle\@maketitle% Store \@maketitle
\renewcommand{\@maketitle}{\@oldmaketitle% Update \@maketitle to insert...
  \input{imgs/new_teaser/figure}\bigskip}% ... an image
\makeatother

%%%%%%%%% TITLE
% options:
%%% Neural Fluid Dynamics: Learning from Fluid Simulations to Reconstruct General 3D Deformations
%%% Neural Fluid Dynamics: General 4D Reconstruction from Fluid Simulation Priors
%%% Neural Fluid Dynamics: Learning Fluid Simulations as a 3D Deformation Model
%%% Fluid Dynamics Network: Exploiting Fluid Dynamics Priors for Topology-Agnostic 4D Reconstruction
\title{Fluid Dynamics Network: \\ Topology-Agnostic 4D Reconstruction via Fluid Dynamics Priors}

\author{Daniele Baieri\\
Sapienza University of Rome\\
{\tt\small baieri@di.uniroma1.it}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Stefano Esposito\\
Hochschule Bonn-Rhein-Sieg\\
{\tt\small stefano.esposito@h-brs.de}
\and
Filippo Maggioli\\
Sapienza University of Rome\\
{\tt\small maggioli@di.uniroma1.it}
\and
Emanuele Rodol\`a\\
Sapienza University of Rome\\
{\tt\small rodola@di.uniroma1.it}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
    Representing 3D surfaces as level sets of continuous functions over $\real^3$ is the common denominator of neural implicit representations, which recently enabled remarkable progress in geometric deep learning and computer vision tasks. 
    In order to represent 3D motion within this framework, it is often assumed (either explicitly or implicitly) that the transformations which a surface may undergo are homeomorphic: this is not necessarily true, for instance, in the case of fluid dynamics. 
    In order to represent more general classes of deformations, we propose to apply this theoretical framework as regularizers for the optimization of simple 4D implicit functions (such as signed distance fields). We show that our representation is capable of capturing both homeomorphic and topology-changing deformations, while also defining correspondences over the continuously-reconstructed surfaces.
    
    %Adding one coordinate to the input space provides a simple yet general representation of 3D deformations which, however, does not define correspondences along the time dimension.
    %4D geometry reconstruction is one among several tasks which greatly benefited from the introduction of neural implicit representations as a geometric deep learning tool. While various solutions have been proposed in recent years, we identify in their high computational burden and \daniele{/or?} lack of generality two possible directions for improvement. 
    %Our proposal aims to improve under both aspects, by leveraging a general theoretical framework for dynamics and removing all assumptions on the topology of input observations. We prove that our method performs well both inside and outside the domain of fluid simulations, and it is \daniele{some order of magnitude} faster than recent proposals.
   
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Representing the motion of 3D objects is a crucial aspect in several areas of 3D graphics. For instance, skinning models are an established method for mesh animation, which provides a plausible and efficient way of deforming a surface given a sequence of skeleton poses~\cite{jacobson:2014:skinning}. However, various applications such as fluid simulation require representing topology-changing (formally, non-homeomorphic) deformations, which are notoriously difficult to approach with mesh-based approaches.

%\begin{figure}[H]
%    \input{imgs/teaser/figure}
%\end{figure}

Implicit representations are becoming an increasingly important topic in computer graphics, as they enable or improve a variety of game-changing technologies, such as shape modeling~\cite{malladi:1995:sdfmodeling}, accurate shape and scene reconstruction~\cite{icml2020_2086,mildenhall2020nerf} and fast rendering~\cite{esposito:2022:kiloneus,hart1993sphere,mueller2022instant}. Yet, the representation of a 3D object with a signed distance field introduces many challenges since many established pipelines from classical computer graphics cannot be applied anymore. Between these challenges, the non-rigid animation of a body is one of the most obvious, as standard approaches like skinning models do not have straightforward conversions for working with implicit representations.


\begin{figure}[h]
\input{imgs/torus-water/figure}
\end{figure}


% Add also Fast-Snarf https://arxiv.org/abs/2211.15601 to skeletal based methods
Recently, some works presented significant progresses in this direction. Taking advantage of deep learning techniques, it has been shown that implicit surfaces can be successfully rigged and animated using skeletons, with a particular focus on humanoid characters~\cite{chen2021snarf,tiwari21neuralgif}. Other approaches got rid of skeletal animations and focused on reconstructing 3D geometries that are continuously deformed over time from a set of discrete observations~\cite{Lei2022CaDeX,OccupancyFlow}. However, these techniques represent deformation as continuous bijective functions with continuous inverse, and as a consequence they are limited to representing fixed-topology deformations.

We fill this gap by introducing a new computational model that can reconstruct continuously deformable 3D objects from a discrete sample of observations without any assumptions on the topology changes. Our approach is shown to be effective in settings where the geometry deformation is homeomorphic (such as character animations), as well as applications where the topology changes drastically over time (like fluid dynamics scenes). We also show that the 4D reconstruction produced by our model can be used to track dense correspondences in the evolving surface, making it also suitable for shape-matching applications.

% \begin{figure}[h]
% \input{imgs/fluid-dfaust-examples/figure}
% \end{figure}

\section{Related work}

%\daniele{We have a ton of work we can cite. Some useful lines of work could be: a) computational fluid dynamics (classic stuff) b) neural representations (nerf as well but not a full discussion of all nerf-like methods) c) ML and fluid simulations (PiNF, any work from Thuerey), d) 4D reconstruction (CaDeX, Occupancy Flow, other stuff I can't think of rn)}

\paragraph{Neural implicit representations}

Representing 3D surfaces as continuous functions over ambient space is a recent trend in geometric deep learning. Manipulating continuous surfaces offers new perspectives in geometry processing, and led to significant advances in various complex tasks. Occupancy fields \cite{Occupancy_Networks} and signed distance fields \cite{Park_2019_CVPR} were applied in several different settings, such as 3D surface reconstruction \cite{Atzmon_2020_CVPR,icml2020_2086,Peng2020ECCV,sitzmann2019siren,wang2021neus}, animation \cite{chen2021snarf,tiwari21neuralgif}, and inverse rendering \cite{DVR,oechsle2021unisurf,yariv2020multiview}. This last application was revolutionized by neural radiance fields \cite{mildenhall2020nerf}, which inspired an entire line of research. Successively, NeRFs were applied to deforming scenes (4D reconstruction from multi-view videos) \cite{chu2022physics,park2021nerfies,dnerf,tretschk2020nonrigid}, MRI scans \cite{coronafigueroa2022mednerf,nerfmrisupersample}, and for scene relighting \cite{boss2021nerd,nerv2021,zhang2021nerfactor}. Instant-NGP \cite{mueller2022instant} further enabled application of neural implicit representations by showing how to optimize them in a matter of seconds.



\begin{figure}[h]
    \input{imgs/fluid-dfaust-examples/figure}
\end{figure}

%\begin{figure}[h]
%    \input{imgs/teaser/figure}
%\end{figure}

\paragraph{4D shape reconstruction}

Continuous 4D reconstruction methods tackle the reconstruction of dynamic scenes both in space and time from a sequence of sparse observations, typically point-clouds or 2D images. While 3D reconstruction allows for several algorithmic baselines (such as \cite{ssdrecon,poisson}), the complexity introduced by the requirement of reconstructing motion steered this task towards deep learning methods, and most recently, neural implicit representations.

PSGN \cite{psgn} is a 3D reconstruction method for multi-view images which can be easily generalized to predict a 4D point cloud, i.e. the point cloud trajectory instead of a single point set. Similarly, implicit representation networks such as occupancy net \cite{Occupancy_Networks} can be generalized by adding an input coordinate (usually called ONet4D), in order to define the occupancy field in the spatio-temporal domain by predicting time-varying occupancy values. By assigning a velocity vector to every point in space and time, Niemeyer \etal \cite{OccupancyFlow} offered a method to represent continuous 4D shapes by trasporting a static occupancy field, while also implicitly modeling dense correspondences. Tang \etal \cite{tang2021learning} proposed a novel 4D point cloud encoder design that performs efficient spatio-temporal shape properties aggregation from 4D point cloud sequences, improving upon Occupancy Flow's reconstructed geometry. CaDeX \cite{Lei2022CaDeX} explicitly represents the deformation as an homeomorphism, by factorizing it into a continuous invertible function and its continuous inverse, mapping the source geometry frame into a common 3D coordinate space and back to the destination frame, respectively; its prior allows for topology preservation by construction and the recovery of consistent correspondences across frames.

All aforementioned methods are data driven, where at test time reconstructions are conducted from an image or a partial observations leveraging prior knowledge learned from large-scale datasets containing both complete surfaces (\ie meshes) and their sparse observations. This improves the accuracy and consistency of the results, especially in situations where the input data is noisy or incomplete. However, prior-based methods can be computationally expensive and their large amounts of training data requirement can limit their practical applicability. 

Furthermore, a common assumption among these methods is preservation of topology; for instance, the integration of points through the velocity field learned by Occupancy Flow is also implicitly an homeomorphism. Our method will make no such assumption, thus retaining an additional level of generality, and can be optimized from a sequence of dense oriented point clouds, without data priors. 


%Unlike traditional 3D reconstruction methods, which typically work on static scenes, 4D reconstruction methods use deep learning to infer the 3D shape and motion of objects in a scene over time, allowing for the reconstruction of accurate and detailed representations of dynamic geometries.

%State-of-the-arts for 4D reconstruction from point cloud sequences.

%\daniele{citations to \cite{chen2021snarf,tiwari21neuralgif} should probably be moved to this section: they are data-driven models for implicit surface animation, which is a form of 4D reconstruction.}

% https://arxiv.org/pdf/1612.00603.pdf


% https://arxiv.org/pdf/1612.00603.pdf


\begin{figure}[h]
\input{imgs/cold-gas-obstacles/figure}
\end{figure}





\paragraph{Computational fluid dynamics}


The simulation of fluid dynamics is a fundamental topic in computer graphics, and much research has been devoted to this area in the last decades. Researchers have approached this problem from a variety of points of view, spanning from the direct simulation of Navier-Stokes equations~\cite{bridson:2007:fluidsiggraph} to non physics-based algorithm for credible visual results~\cite{fournier:1986:oceanwaves,hisinger:2002:oceanwaves}. For a detailed survey on fluid solvers, we refer to Bridson's textbook~\cite{bridson:2018:fluid} and Koshier \emph{et al.}~\cite{koshier:2019:fluid}. Despite the amount of work dedicated to this problem, the room for improvement is still significant, and new studies on fluid simulation are continuously coming out. In recent years, many studies tried to achieve better quality in simulation by focusing on specific problems, like certain kinds of waves~\cite{Huang:2021:VastOcean} and particular environmental conditions~\cite{lan:2022:smokesubway}. Simultaneously, new techniques for speeding up simulations have been developed, with particular attention to the interaction between solid obstacles and turbulent fluids~\cite{li:2020:turbulentflow,lyu:2021:fluidsolid} and to efficient space partitioning~\cite{Shao:2022:Multigrid}.

\paragraph{Machine learning and fluid simulation}
Recently, more innovative approaches tried to combine the descriptive power of machine learning with the simulation of fluid dynamics. Um \etal showed a general method to improve PDE solvers (such as fluid simulators) solutions using statistical ML models \cite{solverintheloop}. Vinuesa \emph{et al.}~\cite{vinuesa:2022:fluidlearning} tried to apply learning paradigms to different stages of fluid simulation, showing the potential improvement in quality and performance. Following this direction, data-driven models have been developed for upsampling turbulent flows and obtaining fine-grained details from coarse simulations~\cite{bai:2020:upsamplingsmoke,bai:2021:predictingturbulence}. Fluid super-resolution has also been tackled in a GAN paradigm, as in \cite{fluidsuperresgan,xie2018tempoGAN}. A completely different approach has been proposed in Li \emph{et al.}~\cite{li:2022:neuralnetfluid}, where particle-based fluid models have successfully been related to graph neural networks obtaining performance improvement at basically no quality cost. Ummenhofer \etal \cite{Ummenhofer2020Lagrangian} later showed that the same can be achieved without graph structure, by performing continuous convolution over sets of points. More similar in spirit to our work is Chu \emph{et al.}~\cite{chu2022physics}: the authors apply fluid simulation priors to inverse rendering of dynamic fluid scenes from sparse multi-view videos, without geometric information. 


%However, differently from our approach, they rely on data priors and pre-trained models. In contrast, our goal is to infer the reconstruction from geometric information completely data-freely.



\begin{figure}[h]
\input{imgs/dfaust-reconstruction/figure}
\end{figure}


\section{Method}

\subsection{Preliminaries}

Simulating fluid dynamics typically involves computing a velocity field $\bm{u}$, by integrating the Navier-Stokes PDE:
\begin{align}
    \dfrac{\partial \bm{u}}{\partial t} + (\bm{u}\cdot\nabla)\bm{u} - \nu\nabla^2\bm{u} &= -\dfrac{1}{\rho}\nabla p +\bm{g}\label{eq:navier} \\
    \nabla\cdot\bm{u} &= 0 \label{eq:div}
\end{align}
And applying the resulting velocity field $\bm{u}$ to transport some type of geometric representation. For our purposes, we represent geometry as a scalar field $f$, which can be transported via the advection equation (assuming \cref{eq:div} holds):
\begin{equation}
    \dfrac{\partial f}{\partial t} + \bm{u}\cdot\nabla f = 0\label{eq:advect}
\end{equation}

%In our setting, we aim to infer dynamics from a data prior; the only structural priors we wish to maintain for our velocity field are self-advection and null divergence. Eq.\ref{eq:navier} simplifies to:
Typically, simulators will integrate a simpler form of Eq.~\ref{eq:navier}, only enforcing self-advection:
\begin{equation}
    \dfrac{\partial \bm{u}}{\partial t} + (\bm{u}\cdot\nabla)\bm{u} = 0\label{eq:selfadvect}
\end{equation}
and add on top of this partial result the effects of pressure ($p$), external forces ($\bm{g}$) and viscosity ($\nabla^2\bm{u}$), which are solved for independently. This approximation mitigates the complexity of integrating \Cref{eq:navier} while retaining high precision. In a similar spirit, we use \Cref{eq:selfadvect} as a structural prior and allow our model to estimate the influence of the aforementioned components from the data.


\begin{figure}[h]
\input{imgs/viscous-vortex/figure}
\end{figure}


%\begin{figure}[h]
%    \centering
%    \includegraphics[width=.24\columnwidth]{imgs/placeholders/1x1.png}
%    \includegraphics[width=.475\columnwidth]{imgs/placeholders/1x2.png}
%    \includegraphics[width=.24\columnwidth]{imgs/placeholders/1x1.png}
%    \caption{Super-sampling of an animation from the D-FAUST dataset. The two frames in the middle are non-supervised and lies in between the $i$-th (left) and the $(i+1)$-th (right) supervised frames. The fluid dynamics priors helps in keeping the animation smooth.}
%    \label{fig:dfaust-midframes}
%\end{figure}


\subsection{Regularizing 4D reconstruction by CFD priors}

Our method leverages fluid simulation priors as a regularizer for the optimization of a neural representation of the 4D geometry and dynamics. Let $\neural{f}{\theta}\colon\real^4\to\real^d$, $\neural{v}{\phi}\colon\real^4\to\real^3$ be our (temporal) geometry and velocity functions, represented as neural networks. As outlined by Niemeyer \etal in \cite{OccupancyFlow}, a trivial solution to the 4D reconstruction task can be found by optimizing the general loss:
\begin{equation}
    %\mathcal{L}_{\text{recon}} = \sum_{t=1}^T \mathbb{E}_{x}\left[L\left(f_{\theta}(x, \tau(t)), \mathcal{X}_t(x)\right)\right]
    \loss{recon} = \sum_{t\in T} L\left(\neural{f}{\theta}, \mathcal{X}_t; \tau(t)\right)\label{eq:lrecon}
\end{equation}
Where $\{\mathcal{X}_t\}_{t\in T}$ are time-labeled sparse geometry observations, $T$ is the set of available supervised time steps, $\tau$ maps a timestep label to a real time value, and $L$ is an error function comparing the optimized function and the ground truth data. 

This model (DeepSDF4D in the following), while technically correct, does not learn any information about the dynamics underlying the observations; thus, it is similar to applying a 3D reconstruction technique for each frame. As a consequence, no type of correspondence among reconstructed surfaces is established during training. Despite these shortcomings, however, it allows to represent general deformations of 3D geometry, including topological and volume changes. Most proposals in 4D reconstruction literature (such as \cite{Lei2022CaDeX,OccupancyFlow}) rule this possibility out by assuming to only work with isometries (either implicitly or explicitly); we discuss how to improve DeepSDF4D in order to model general deformations (up to volume changes), while establishing correspondences between the reconstructed surfaces.

Similarly to Chu \etal \cite{chu2022physics}, we train $\neural{v}{\phi}$ jointly with $\neural{f}{\theta}$ using the following losses, adapted from \Cref{eq:selfadvect,eq:div,eq:advect}:
\begin{align}
    \loss{div} =& \int_{0}^{\tau}\int_{\real^3} \nabla\cdot\neural{v}{\phi} \,d\bm{x}\,dt \label{eq:ldiv} \\
    \loss{advect} =& \int_{0}^{\tau}\int_{\real^3} \dfrac{\partial \neural{f}{\theta}}{\partial t} + \neural{v}{\phi}\cdot\nabla\neural{f}{\theta}  \,d\bm{x}\,dt\label{eq:ladvect}  \\
    \loss{NS} =& \int_{0}^{\tau}\int_{\real^3} \dfrac{\partial \neural{v}{\phi}}{\partial t} + (\neural{v}{\phi}\cdot\nabla)\neural{v}{\phi} \,d\bm{x}\,dt  \label{eq:lns}
\end{align}
Where $\mathcal{L}_{\text{advect}}$ is only used to train $\neural{v}{\phi}$ (otherwise it is trivially minimized by zeroing $\neural{f}{\theta}$). We discuss how we approximate these integrals in \Cref{sec:implement:training}.


While \Cref{eq:ladvect} is enough to optimize $\neural{v}{\phi}$ with respect to a given set of observations (assuming $\neural{f}{\theta}$ is also trained via \Cref{eq:lrecon}), we still need to regularize our geometry function based on our learned velocity field. An option is to perform random linear warping, as in \cite{chu2022physics,tretschk2020nonrigid}: that is, we query our geometry network for points $\bm{x}$ at a given timestep $t_i$ as 
\begin{equation}
\hat{\neural{f}{\theta}}(\bm{x}, t_i) =  \neural{f}{\theta}(\bm{x} + \delta_t\neural{v}{\phi}(\bm{x}, t_i), t_i + \delta_t)\label{eq:linwarp}
\end{equation}
For randomly sampled $\delta_t$ (further discussion about sampling in \Cref{sec:implement:training}). In combination with \Cref{eq:lrecon}, linear warping allows to transport supervision signals across time through $\neural{v}{\phi}$. Finally, we instantiate $\loss{recon}$ as a SDF reconstruction loss, similar to Park \etal \cite{Park_2019_CVPR}: 
\begin{equation}
    L\left(\neural{f}{\theta}, \mathcal{X}_t; \tau(t)\right) = \int_{\real^3} \left\lVert \neural{f}{\theta}(\bm{x}, \tau(t)) - d(\bm{x}, \mathcal{X}_t)  \right\rVert \,d\bm{x} \label{eq:lsdf}
\end{equation}
Where $\{\mathcal{X}_t\}_{t\in T}$ are oriented point clouds, and $d$ computes the ground-truth signed distance value for $\bm{x}$ by mapping it to the closest point $\bm{p}$ on $\mathcal{X}_t$ and determining its sign (inside/outside) via the winding number of $\bm{p}$ \cite{winding}.







\subsection{Implementation details}\label{sec:implement}

\subsubsection{Model}
\looseness=-1

We implement $\neural{f}{\theta}$ and $\neural{v}{\phi}$ as two Siren \cite{sitzmann2019siren} networks with 5 layers and 256 hidden units. We find that the high-frequency bias induced by this architecture is really beneficial in learning erratic and unpredictable dynamics such as fluid behaviour, allowing to considerably speed up convergence and properly model surface details. \Cref{fig:fluid-siren-vs-prev} highlights this aspect by comparing Siren with a simple MLP with positional encoding \cite{tancik2020fourfeat}. Our networks are trained with the loss function
\begin{equation}
    \mathcal{L} = \loss{recon} + \lambda_1\loss{div} + \lambda_2\loss{advect} + \lambda_3\loss{NS}
\end{equation}
Where we set $\lambda_1=\lambda_2=10, \lambda_3=0.1$.


\begin{figure}[h]
    \input{imgs/ball-siren-compare/figure}
\end{figure}



\subsubsection{Training}\label{sec:implement:training}
\looseness=-1

Our networks are trained for 500 epochs using the Adam optimizer~\cite{adam} with a learning rate of $\expnumber{1}{-4}$. During each training iteration, we need to approximate the integrals in our loss functions (\Cref{eq:lsdf,eq:ldiv,eq:ladvect,eq:lns}). We approximate the time integral via stratified sampling, similarly to how Mildenhall \etal \cite{mildenhall2020nerf} integrate radiance and density to compute light transport on a single ray. Over multiple epochs, this helps to retain continuity of the representation over time (as opposed to, \eg, always integrating over a fixed linear space of time steps). Therefore, we evaluate our loss functions independently for each time step and sum all the loss gradients before updating network parameters. For each timestep, the space integral is approximated by uniformly sampling in a bounding box $D$.

Training occurs in three phases: first, we train $\neural{f}{\theta}$ to near convergence over the input data. Then, based on the partial solution provided by $\neural{f}{\theta}$, we train the velocity network $\neural{v}{\phi}$ by the fluid dynamics priors encoded in \Cref{eq:ladvect,eq:lns,eq:ldiv}. During the last phase, we introduce linear warping with a linear schedule, \ie, the interval from which we uniformly sample $\delta_t$ to compute \Cref{eq:linwarp} grows linearly with training time, up to the entire space between adjacent supervised frames. We find that running each phase for 200 epochs yields satisfying results. 



%\daniele{Why this is not good: 1. Only as good as data, both in terms of quality and quantity. 2. No cognition of dynamics, no priors, no nothing. 3. No form of correspondence among reconstructed surfaces.}

%A trivial baseline for 4D reconstruction can be obtained by optimizing a continuous temporal geometry function $f_{\theta}\colon \mathbb{R}^4\to\mathbb{R}^d$ to represent the observed frames . We define the general reconstruction loss:

%

%This simple formulation can be improved as in, \eg, \daniele{Occupancy Flow}: that is, you may optimize a temporal velocity function $v_{\phi}\colon\mathbb{R}^4\to\mathbb{R}^3$ in order to model the motion, and use it to regularize $f_{\theta}$ in some fashion. 





%\begin{figure}
%    \centering
%    \includegraphics[width=\columnwidth]{imgs/placeholders/2x3.png}
%    \caption{Comparison of \textless some-metric-for-surface-accuracy\textgreater~on DeformingThings4D between our method (in blue) against OFlow (orange) and CaDeX (yellow). \filippo{Short analysis to be added after the results are available. Other combinations of methods/metrics to be considered given the time allowance.}}
%    \label{fig:compare-dataset-dt4d}
%\end{figure}


%\begin{figure}
%    \centering
%    \includegraphics[width=\columnwidth]{imgs/placeholders/1x3.png}
%    \caption{Evaluation of our method on various fluid simulation scenes. We compare the reconstructed surface with the simulated one using \textless some-metric-for-surface-accuracy\textgreater. \filippo{Short analysis to be added after the results are available.}}
%    \label{fig:evaluation-fluid-scenes}
%\end{figure}


%\begin{figure}
%    \centering
%    \includegraphics[width=\columnwidth]{imgs/placeholders/1x3.png}
%    \caption{An animation from the D-FAUST dataset reconstructed with our method. We use the velocity field to obtain a correspondence of the shape across the frames.}
%    \label{fig:dfaust-correspondence}
%\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{imgs/chamfer-distance.pdf}
    \caption{Evaluation of reconstruction quality of our method on a subset of the DFAUST dataset (lower is better). \texttt{OFlow} and \texttt{Ours-17f} are evaluated on the first 17 frames of each sequence. While the high frequency of our geometry function  may hinder reconstruction quality to a small degree, our method manages to reconstruct dynamics more faithfully than Occupancy Flow.}
    \label{fig:recon-quality}
\end{figure}






\section{Experiments}
\looseness=-1

\subsection{4D surface reconstruction}\label{sec:exp1}
\looseness=-1
In order to measure the representational power of our method, we compare meshes extracted from our method using marching cubes \cite{mcubes} with resolution 128 against ground-truth meshes, using the chamfer distance as metric. We run the evaluation over a small subset of the DFAUST dataset \cite{dfaust:CVPR:2017}, where we select one sequence type for each subject, so that most of the sequence types are represented. The results are showed in \Cref{fig:recon-quality}.



\begin{figure}[b]
    \centering
    \includegraphics[width=\columnwidth]{imgs/compare-dataset-dfaust.pdf}
    \caption{Quantitative comparison of correspondences on DFAUST between our method and Occupancy Flow \cite{OccupancyFlow}. Our method manages to match most points with low error (80\% of points have error percentage below 10\% for the 17 frames evaluation). Saturation seems comparable with the Occupancy Flow pretrained model.}
    \label{fig:compare-dataset-dfaust}
\end{figure}



\subsection{Shape correspondence}

Our method defines a temporal velocity field, which we may leverage in order to define correspondences among the reconstructed surfaces. Furthermore, by relaxing assumptions on the deformation function, we model correspondences with changes of topology (\eg \Cref{fig:non-homeo-matching}) in a fully unsupervised fashion. Formally, for a single sequence of observations $\{\mathcal{X}_i\}_{i=1}^n$, we define the correspondence between the vertices of $\mathcal{X}_1$ and $\mathcal{X}_n$ via the initial value problem: 


\begin{align}
\begin{split}
    \Phi(\mathcal{X}_1, 0) &= \mathcal{X}_1 \\
    \dot{\Phi}(\mathcal{X}_1, t) &= \neural{v}{\phi}(\mathcal{X}_1, t)
\end{split}
\end{align}

We integrate $\Phi$ via Euler using $[\tau(1); \tau(n)]$ as time interval. After flowing a point $x$ through the entire time interval via $\Phi(x, \tau(n))$, we select the result on the target set of vertices as the nearest neighbor

\begin{equation}
    \Pi(x) = \argmin_{z\in\mathcal{X}_n} \lVert \Phi(x, \tau(n)) - z \rVert_2
\end{equation}


In order to compare our matching method quantitatively to other 4D reconstruction algorithms with correspondences, we plot a comparison in cumulative matching curves evaluated on the same subset of DFAUST described in \Cref{sec:exp1}. For each analyzed sequence, we match the first and last frames; then, we take as point-wise error metric the geodesic distance between the ground-truth match and the predicted match. The results are showed in \Cref{fig:compare-dataset-dfaust}. We evaluate our method with subsequences of 150 frames and 17 frames, in order to compare it to the pretrained Occupancy Flow model, which was trained only on 17 frames sequences.





%For each $x \in V_t$, correspondences between $V_{t-1}$ and $V_t$ are defined as:
%\begin{equation}
%    \left(x, \argmin_{z \in V_{t-1}} \lVert z + V(z, t-1) dt, x\rVert\right)
%\end{equation}

%\ste{or maybe}

%Given two point clouds $V_{t-1}$ and $V_{t}$ at subsequent timestamps, for each  $\bm{x} \in V_{t}$ we define a correspondences with $V_{t-1}$ as:  
%\begin{equation}
%    \left(\bm{x}, \argmin_{\bm{z} \in V_{t-1}} \lVert \bm{z} + \neural{v}{\phi}(\bm{z}, t-1) dt, \bm{x}\rVert\right)
%\end{equation}

%\filippo{I agree the second is better, maybe let's also give an intuitive explanation of what this equation is describing.}

%\daniele{say no more lol}


%\daniele{
%Lista esperimenti sicuri:
%\begin{itemize}
%    \item reconstruction quality/quantitative results
%    \begin{itemize}
%        \item metrics: Chamfer, Average absolute SDF for surface points, IoU
%        \item datasets: Fluid Simulations, DeformingThings4D
%    \end{itemize}
%    \item qualitative results:
%    \begin{itemize}
%        \item simple/basic scene
%        \item variable viscosity
%        \item multiple obstacles
%        \item external forces (vortices, pumps, etc.)
%        \item inflow/outflow in the same scene (with and without volume conservation)
%        \item animation reconstruction (with comparison to previous work)
%        \begin{itemize}
%            \item rigid deformations
%            \item non-rigid deformations
%            \item variation of scale
%        \end{itemize}
%        \item variable time sparsity
%    \end{itemize}
%\end{itemize}

%Lista esperimenti da provare:
%\begin{itemize}
%    \item shape matching
%    \item results for temporal geometry function VS integrated SDF from t=0
%    \item complex fluid scenes
%\end{itemize}
%}

%\daniele{

%To-do for March 8th deadline:

%\begin{itemize}
%    \item Qualitative results:
%    \begin{itemize}
%        \item OFlow single scene reconstruction
%        \item (if time allows) CaDeX single scene reconstruction
%        \item Ours single scene reconstruction
%        \item Ours correspondence preservation
%    \end{itemize}
%    \item Quantitative results:
%    \begin{itemize}
%        \item Mesh reconstruction quality wrt training set
%        \item Correspondence quality
%    \end{itemize}
%\end{itemize}

%}




\begin{figure}[b]
\input{imgs/non-homeo-matching/figure}
\end{figure}


\section{Conclusions}

\subsection{Discussion and limitations}

We presented a non-data-driven method for 4D reconstruction which is capable of capturing a vast range of geometric deformations and define dense, continuous correspondences among the reconstructed surfaces. 
One main limitation of our method is its requirement for dense input point clouds; we propose to investigate solutions in the future (see \Cref{sec:future}). 

Furthermore, the simplified version of the Navier-Stokes equation which we use in our loss function (\Cref{eq:lns}) is not enough to capture any fluid behaviour: imagine a cylinder rotating on its ``up'' axis. The ground-truth SDF signal for the sequence of shapes will be constant, and therefore the velocity field regressed by our method will be close to zero everywhere. This setting is also not unlikely in fluid simulation, as it is a close analogy for a vortex of high-viscosity fluid. This issue may be addressed by employing the complete Navier-Stokes form, at the cost of having to regress additional functions (such as pressure and external forces) and to optimize second derivatives of the velocity field. One last limitation would be the case of handling non-volume-preserving deformations; however, the entire fluid simulations framework is unsuited for this purpose, thus we consider it to be outside of our scope.


\subsection{Future work}\label{sec:future}

In the near future, we intend to gather the experimental fluid data which we employed for this work into a proper, multimodal (including multi-view renders) 4D geometry dataset. This should provide for a useful benchmark for topology-agnostic methods in geometry processing.

Furthermore, we intend to generalize our presented formulation to a data-driven model: this should allow us to relieve our need for dense geometric input at inference time, albeit constraining the learned model to the distribution presented in the employed dataset.

Lastly, we intend to investigate the capabilities of our formulation in point cloud segmentation, in particular, we believe our method has promise in separating static and dynamic content in 4D point clouds.



{\small
\bibliographystyle{ieee_fullname}
\bibliography{4dfluid_review}
}


\end{document}