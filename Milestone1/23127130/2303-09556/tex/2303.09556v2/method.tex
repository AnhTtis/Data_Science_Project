% We introduce our method here.

\subsection{Preliminary}\label{sec:preliminary}

% Diffusion based generative models, inspired by non-equilibrium thermodynamics~\cite{sohl2015nonequ}, are widely used in image synthesis~\cite{ho2020ddpm,ramesh2022dalle2,saharia2022imagen}, video synthesis~\cite{Ho2022imagenvideo}, and even text generation~\cite{}. 
Diffusion models consist of two processes: a forward noising process and a reverse denoising process. 
We denote the distribution of training data as $p(\mathbf{x}_0)$.
The forward process is a Gaussian transition, gradually adds noise with different scales to a real data point $\mathbf{x}_0\sim p(\mathbf{x}_0)$ to obtain a series of noisy latent variables $\{ \mathbf{x}_1,\mathbf{x}_2, \ldots, \mathbf{x}_T \}$:
% The process could be formulated as
% \begin{align}
%     % q(\mathbf{x}_t | \mathbf{x}_{t-1}) &= \mathcal{N}(\mathbf{x}_t;\sqrt{\alpha}_t\mathbf{x}_{t-1}, (1 - \alpha_t)\mathbf{I}), \\
%     q(\mathbf{x}_t | \mathbf{x}_0) &= \mathcal{N}(\mathbf{x}_t;\sqrt{\Bar{\alpha}_t}\mathbf{x}_0, (1 - \Bar{\alpha}_t)\mathbf{I}),
% \end{align}
% \begin{align}
%     \mathbf{x}_t = \sqrt{\Bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \Bar{\alpha}_t} \boldsymbol{\epsilon}
% \end{align}
%% ------------------------------
% \begin{align}
%     \mathbf{x}_t = \alpha_t \mathbf{x}_0 + \sigma_t \boldsymbol{\epsilon},~t=1,2,\ldots,T,
% \end{align}
\begin{align}
    q(\mathbf{x}_t | \mathbf{x}_0) &= \mathcal{N}( \mathbf{x}_t; \alpha_t \mathbf{x}_0, \sigma_t^2 \mathbf{I}) \\
    \mathbf{x}_t &= \alpha_t \mathbf{x}_0 + \sigma_t \boldsymbol{\epsilon} \label{eq:diffusion}
\end{align}
%% ------------------------------
% where $1 - \alpha_t = \beta_t $ is the magnitude of the added Gaussian noise and $\Bar{\alpha}_t$ is equal to $\alpha_1\alpha_2\ldots\alpha_t$.  After $T$ steps (usually $T\geq 1000$), $\alpha_1\alpha_2\ldots\alpha_T$ gets small enough and the real distribution is transformed to Gaussian Noise $\mathcal{N}(0, \mathbf{I})$.
where $\boldsymbol{\epsilon}$ is the noise sampled from Gaussian distribution $\mathcal{N}(0, \mathbf{I})$.
The noise schedule $\sigma_t$ denotes the magnitude of noise added to the clean data at $t$ timestep. It increases monotonically with $t$. In this paper, we adopt the standard variance-preserving diffusion process, where $\alpha_t = \sqrt{1 - \sigma_t^2}$.
%Furthermore, we follow VDM~\cite{kingma2021vdm} to define the \textit{signal-to-noise ratio} (SNR):
%\begin{equation}
%    \text{SNR}(t) = \alpha_t^2 / \sigma_t^2
%\end{equation}

% The reverse process starts from a Gaussian noise input, gradually reduces the noise, and generates the true sample. 
% The core is to learn a model to estimate the posterior
% \begin{equation}
%     p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \mathbf{\hat{\mu}}_\theta (\mathbf{x}_{t}), \Sigma_\theta(\mathbf{x}_{t})).
% \end{equation}
% Ho et al.~\cite{ho2020ddpm} set $\Sigma_\theta(\mathbf{x}_{t})$ to the constant $\sigma_t^2\mathbf{I}$.

% To maximize the log-likelihood $\log p_\theta (\mathbf{x}_0)$, the variational bound can be derived into the optimization between real noise and predicted noise 
% \begin{equation}
%     \mathbb{E}_{\mathbf{x}_0,\mathbf{\epsilon},t}\left[
%     \frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\Bar{\alpha}_t)}\lVert \mathbf{\epsilon} - \mathbf{\epsilon}_\theta (\sqrt{\Bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \Bar{\alpha}_t}\epsilon) \rVert ^2
%     \right]
% \end{equation}
% where $t\in \{ 1, 2, \ldots, T\}$, $\mathbf{x}_0$ is sampled from the real data distribution $p(\mathbf{x}_0)$, and $\epsilon$ is sampled from Gaussian distribution.
The reverse process is parameterized by another Gaussian transition, gradually denoises the latent variables and restores the real data $\mathbf{x}_0$ from a Gaussian noise:
% To maximize the likelihood $p_\theta (\mathbf{x}_0)$, we use a parametric model to estimate the posterior
% Diffusion models are trained to recover from the corruptions in forward process: 
\begin{equation}
    p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \mathbf{\hat{\mu}}_\theta (\mathbf{x}_{t}), \hat{\Sigma}_\theta(\mathbf{x}_{t})).
\end{equation}
$\mathbf{\hat{\mu}}_\theta$ and $\hat{\Sigma}_\theta$ are predicted statistics. 
Ho et al.~\cite{ho2020ddpm} set $\hat{\Sigma}_\theta(\mathbf{x}_{t})$ to the constant $\sigma_t^2\mathbf{I}$, and $\hat{\mu}_\theta$ can be decomposed into the linear combination of $\mathbf{x}_t$ and a noise approximation model $\hat{\epsilon}_\theta$. They find using a network to predict noise $\mathbf{\epsilon}$ works well, especially when combined with a simple re-weighted loss function:
% and parameterize mean prediction $\hat{\mu}_\theta$ to noise prediction $\hat{\epsilon}_\theta$.
% They derive the optimization target from variantional lower bound (VLB) but find the simple re-weighted loss works well:
\begin{equation}\label{equ:simple}
    \mathcal{L}_{\text{simple}}^t (\theta) = \mathbb{E}_{\mathbf{x}_0,\mathbf{\epsilon}}\left[
    \lVert \mathbf{\epsilon} - \hat{\epsilon}_\theta (\alpha_t \mathbf{x}_0 + \sigma_t \epsilon) \rVert_2^2
    \right].
    % \mathcal{L}_{\text{simple}}^t (\theta) = \mathbb{E}_{\mathbf{x}_0,\mathbf{\epsilon}}\left[
    % \lVert \mathbf{\epsilon} - \hat{\epsilon}_\theta  \rVert ^2
    % \right]
\end{equation}
%Combined with Eq.~\ref{eq:diffusion}, We can derive it into predicting $\mathbf{x}_0$ from the network:
% The training loss can also be defined in the $\mathbf{x}$-\text{space},
%\begin{equation}\label{equ:simple-x0}
%    \mathcal{L}_{\text{simple}}^t (\theta) = \mathbb{E}_{\mathbf{x}_0,\mathbf{\epsilon}}\left[
%    \frac{\alpha_t^2}{\sigma_t^2} \lVert \mathbf{x}_0 - \hat{\mathbf{x}}_\theta (\alpha_t \mathbf{x}_0 + \sigma_t \epsilon) \rVert ^2
%    \right]
%\end{equation}
%These two variants can be transformed into each other by adding a coefficient $\text{SNR}(t)$ on the loss weight. 
Most previous works~\cite{nichol2021iddpm,dhariwal2021adm,Nichol2021glide} follow this strategy and predict the noise. Later works~\cite{gu2022vqdiffusion,salimans2022distillprogressive} use another re-parameterization that predicts the noiseless state $x_0$:
\begin{equation}\label{equ:simple-x0}
    \mathcal{L}_{\text{simple}}^t (\theta) = \mathbb{E}_{\mathbf{x}_0,\mathbf{\epsilon}}\left[
     \lVert \mathbf{x}_0 - \hat{\mathbf{x}}_\theta (\alpha_t \mathbf{x}_0 + \sigma_t \epsilon) \rVert _2^2
    \right].
\end{equation}
And some other works~\cite{salimans2022distillprogressive,rombach2022ldm} even employ the network to directly predict velocity $v$. Despite their prediction targets being different, we can derive that they are mathematically equivalent by modifying their loss weights.

% Some other works~\cite{gu2022vqdiffusion} even ignored the loss weight coefficient and achieved good performance. Although the specific form of loss is different, the loss used at different timesteps is the same for most previous works.

% \cite{ho2020ddpm} also conducts experiments to directly predict $\mathbf{x}_0$ but finds this results in worse results.
% Much of the later work follow DDPM to predict noise.

% \begin{equation}
%     q(\mathbf{x}_t | \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t;\sqrt{\Bar{\alpha}_t}\mathbf{x}_0, (1 - \Bar{\alpha}_t)\mathbf{I}),
% \end{equation}
% where $\mathbf{z}_0$ is one data point sampled from real data distribution.
% However, Ho et al.~\cite{ho2020ddpm} find that a simple re-weighted loss could help achieve better results
% \begin{equation}\label{equ:simple}
%     \mathcal{L}_{\text{simple}}^t (\theta) = \mathbb{E}_{\mathbf{x}_0,\mathbf{\epsilon}}\left[
%     \lVert \mathbf{\epsilon} - \mathbf{\epsilon}_\theta (\sqrt{\Bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \Bar{\alpha}_t}\epsilon) \rVert ^2
%     \right]
% \end{equation}

% To train the DDPM in $\epsilon$-space, we sample a real data point, add noise to it, estimate the noise with the model, and minimize the mean-squared error between real noise and predicted noise. For more details, you could refer to DDPM~\cite{ho2020ddpm}.
% To train the DDPM in $\mathbf{x}_0$-space,~\cite{salimans2022distillprogressive} estimated the raw data from the noisy input with the model and then used SNR weight as in Eq.~\ref{equ:simple-x0}.

\subsection{Diffusion Training as Multi-Task Learning}
To reduce the number of parameters, previous studies~\cite{ho2020ddpm,nichol2021iddpm,dhariwal2021adm} often share the parameters of the denoising models across all steps.
However, it's important to keep in mind that different steps may have vastly different requirements. At each step of a diffusion model, the strength of the denoising varies. 
For example, easier denoising tasks (when $t\to 0$) may require simple reconstructions of the input in order to achieve lower denoising loss. This strategy, unfortunately, does not work as well for noisier tasks (when $t\to T$). 
Thus, it's extremely important to analyze the correlation between different timesteps.

% Several works~\cite{balaji2022eDiff-I,feng2022ernie2} have noticed the phenomenon and propose mixture of experts (MoE) to alleviate the issue. 
% In other words, they adopt several models with the same architecture and assign each model with different ranges of steps. 
% Sharing parameters across different timestamps may hinder model's ability to handle easy and hard tasks, especially when the loss weight for each $t$ is the same.

In this regard, we conduct a simple experiment. We begin by clustering the denoising process into several separate bins. Then we finetune the diffusion model by sampling timesteps in each bin. Lastly, we evaluate its effectiveness by looking at how it impacted the loss of other bins. As shown in Figure~\ref{fig:ft-loss-curve}, we can observe that finetuning specific steps benefited those surrounding steps. However, it's often detrimental for other steps that are far away. This inspires us to consider whether \emph{we can find a more efficient solution that benefits all timesteps simultaneously}.

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figs/loss_100k_ft10k.pdf}
    \vspace{-0.4cm}
    \caption{
    We finetune the diffusion model in specific ranges of timesteps:[100, 200), [200, 300), and [300, 400), then we investigate how it affects the loss in different timesteps. The surrounding timesteps may derive benefit from it, while others may experience adverse effects.
    % Further fine-tuned loss curve. {\color{magenta} (others - baseline - (others - baseline).min()) - log scale.}
    % Differences between the loss calculated from a model trained on all timesteps and the loss calculated from a model that has undergone further fine-tuning. Fine-tuning a specific range of timesteps results in lower loss over these timesteps and those close to them, while loss over distant timesteps increases.
    % The losses are plotted in log scale for better visualization.
    }
    % \vspace{-0.3cm}
    \label{fig:ft-loss-curve}
\end{figure}




% [{\color{red} Some experiments here if possible ...}]
% \begin{figure}[t]
% \begin{center}
% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    %\includegraphics[width=0.8\linewidth]{egfigure.eps}
% \end{center}
%    \caption{Further fine-tuned results based on the baseline. We fine-tune the model on specific range of timestamps and replace the baseline model with it during sampling.}
% \label{fig:long}
% \label{fig:onecol}
% \end{figure}




We re-organized our goal from the perspective of multitask learning. The training process of denoising diffusion models contains $T$ different tasks, each task represents an individual timestep. We denote the model parameters as $\theta$ and the corresponding training loss is $\mathcal{L}^t(\theta), t\in \{ 1,2,\ldots,T\}$. Our goal is to find a update direction $\delta \neq 0$, that satisfies:
\begin{equation}
    \mathcal{L}^t(\theta+\delta) \leq \mathcal{L}^t(\theta), \forall t \in \{1,2,\ldots,T\}.
\end{equation}
We consider the first-order Taylor expansion:
\begin{equation}
\mathcal{L}^t(\theta+\delta) \approx \mathcal{L}^t(\theta) + \left \langle\delta, \nabla_{\theta}\mathcal{L}^t(\theta)\right \rangle.
\end{equation}
% Thus, the optimization objective is equivalent to satisfy:
Thus, the ideal update direction is equivalent to satisfy:
\begin{equation}
    \left \langle\delta, \nabla_{\theta}\mathcal{L}^t(\theta)\right \rangle \leq 0, \forall t \in \{1,2,\ldots,T\}.
    \label{eqn:optimization_objective}
\end{equation}

\subsection{Pareto optimality of diffusion models}
% If Equation~\ref{eqn:optimization_objective} does not exist an optimal solution, it means that we must sacrifice a certain task in exchange for the loss decrease of other tasks. In other word, we have reached Pareto Stationary and the training has converged. Otherwise, we have the following theorem:

\begin{theorem}
Consider a update direction $\delta^*$:
\begin{equation}
\delta^* = -\sum_{t=1}^T w_t \nabla_{\theta} \mathcal{L}^t(\theta),
\end{equation}
of which $w_t$ is the solution to the optimization problem:
\begin{equation}
    \label{eq:opt}
    \min_{w^t} \left\{   \lVert \sum_{t=1}^T w^t \nabla_{\theta} \mathcal{L}^t(\theta)\rVert^2  | \sum_{t=1}^T w^t=1, w^t \geq 0\right\}
\end{equation}
If the optimal solution to the Equation~\ref{eqn:optimization_objective} exists, then $\delta^*$ should satisfy it. Otherwise, it means that we must sacrifice a certain task in exchange for the loss decrease of other tasks. In other words, we have reached the Pareto Stationary and the training has converged.
\end{theorem}

% Here we assume that there exists an optimal solution to the Equation~\ref{eqn:optimization_objective}, which is $\delta^*$. If it does not exist, it means that we must sacrifice a certain task in exchange for the loss decrease of other tasks. We have reached Pareto Stationary and the training has converged.


A more general form of this theorem was first proposed in~\cite{desideri2012mgda} and we leave a succinct proof in the appendix. 
% In our scenario, diffusion models are required to go through all the timesteps when generating images. So any timestep should not be ignored during training. 
% Therefore, we propose a constraint that penalizes when loss weight is smaller than a threshold $\delta$. 
Since diffusion models are required to go through all the timesteps when generating images. So any timestep should not be ignored during training. Consequently, a regularization term is included to prevent the loss weights from becoming excessively small. The optimization goal in Equation~\ref{eq:opt} becomes:
\begin{equation}
    \label{eq:reg-opt}
    % \mathcal{S} = \lVert \sum_{t=1}^T w_t \nabla_{\theta} \mathcal{L}^t(\theta)\rVert_2^2 + \lambda \sum_{t=1}^T \max(0, \delta-w_t)
    \min_{w_t} \left \{ \lVert \sum_{t=1}^T w_t \nabla_{\theta} \mathcal{L}^t(\theta)\rVert_2^2 + \lambda \sum_{t=1}^T \lVert w_t \rVert_2^2 \right \}
\end{equation}
where $\lambda$ controls the regularization strength. % We set $\lambda$ and $\beta$ to $0$ and $0$ as our default setting.
% {
% \color{blue} 
% However, sparse solutions for loss weight will hinder the learning of diffusion models because zero or near-zero weights can result in some steps being insufficiently trained.
% We incorporate soft constraints to reduce sparsity in the solutions. Concretely, we add penalty for weights that are smaller than some threshold $\Delta$.
% }

% To solve Eq.~\ref{eq:opt},~\cite{} leverages the Frank-Wolfe~\cite{} algorithm to obtain the weight $\{ w_1, w_2, \ldots, w_T \}$ through iteratively optimization. Another approach is to adopt Unconstrained Gradient Descent(UGD). Specifically, we re-parameterize $w_t$ through $\beta_t$:

To solve Equation~\ref{eq:reg-opt},~\cite{sener2018mto} leverages the Frank-Wolfe~\cite{frank1956frank} algorithm to obtain the weight $\{ w_t \}$ through iterative optimization. Another approach is to adopt Unconstrained Gradient Descent(UGD). Specifically, we re-parameterize $w_t$ through $\beta_t$:
\begin{equation}
    w_t = \frac{e^{\beta_t}}{Z}, Z=\sum_{t} e^{\beta_t}, \beta_t \in \mathbb{R}.
\end{equation}
Combined with Equation~\ref{eq:reg-opt}, we can use gradient descent to optimize each term independently:
\begin{equation}
    \label{eq:ugd-opt}
    % \min_{\beta_t} \frac{1}{Z^2} \lVert \sum_{t=1}^T e^{\beta_t} \nabla_{\theta} \mathcal{L}_t(\theta)\rVert_2^2 + \lambda \sum_{t=1}^T \max(0, \delta-\frac{e^{\beta_t}}{Z})
    \min_{\beta_t} \frac{1}{Z^2} \lVert \sum_{t=1}^T e^{\beta_t} \nabla_{\theta} \mathcal{L}_t(\theta)\rVert_2^2 + \frac{\lambda}{Z^2} \sum_{t=1}^T \lVert e^{\beta_t} \rVert_2^2
\end{equation}



% \begin{equation}
%     \min_{\beta_t} \frac{1}{2 Z^2} \lVert \sum_{t=1}^T w_t \nabla_{\theta} \mathcal{L}_t(\theta)\rVert_2^2 + \lambda \sum_{t=1}^T \max \{0, \Delta - w_t\}
% \end{equation}
% where the second item represents a soft weight constraint, with $\lambda$ denoting the regularization coefficient and $\Delta$ denoting the threshold. We take some solutions as input, calculate relative objective values, and show the results in Fig.~\ref{fig:object-value}.


% Taming diffusion models 
% Our goal is to make each $\mathcal{L}_t, t\in \{ 1,2,\ldots,T\}$ relative low to help model the whole diffusion process. 
% The whole diffusion training process can be divided into several tasks. Each task takes in charge of one of more timestamps.
% Our goal to learn every task well can also be formulated as multi-objective optimization: 
% \begin{equation}
%     \min~[\mathcal{L}_{\text{simple}}^1,\mathcal{L}_{\text{simple}}^2,\ldots,\mathcal{L}_{\text{simple}}^T]^\top.
% \end{equation}
% Ho et al.~\cite{ho2020ddpm} treat each task with equal importance. Consider two sets of parameters $\theta_1$ and $\theta_2$, it could be possible $\mathcal{L}_{\text{simple}}^i (\theta_1) > \mathcal{L}_{\text{simple}}^i (\theta_2)$ while $\mathcal{L}_{\text{simple}}^j (\theta_1) > \mathcal{L}_{\text{simple}}^j (\theta_2) (i \neq j)$.
% Multi-objective optimization aims to achieve Pareto optimality~\cite{gunantara2018mtoreview}, i.e., find $\theta^*$ that satisfies
% \begin{equation*}
%     \forall \theta, \mathcal{L}_{\text{simple}}^t (\theta^*) \leq \mathcal{L}_{\text{simple}}^t (\theta), t=1,2,\ldots,T.
% \end{equation*}


% In multi-task learning literature, several works~\cite{desideri2012mgda,sener2018mto} have studied the multi-objective optimization and adopt Karush-Kuhn-Tucker (KKT) conditions~\cite{boyd2004convex,rockafellar1970convex} to achieve optimality.
% For KKT conditions, we assume 
% there exist $w^1, w^2, \ldots, w_t$ such that $\sum_{t=1}^Tw_t = 1$ and $\sum_{t=1}^T w_t \nabla_{\theta} \mathcal{L}_{\text{simple}}^t(\theta) = 0 $.
% Directly solving the conditions is intractable. But it can be transformed to the optimization problem
% \begin{equation}\label{eq:kkt}
%     \min_{w_t,t=1,2,\ldots,T} \left\{  	\lVert \sum_{t=1}^T w_t \nabla_{\theta} \mathcal{L}_{\text{simple}}^t(\theta)\rVert_2^2  | \sum_{t=1}^T w_t=1, w_t \geq 0\right\}
% \end{equation}
% To achieve the Pareto stationary point which minimizes Eq.~\ref{eq:kkt}, we need to get the gradient for each task first. 
% Then we obtain the solution $\{ w^1, w^2, \ldots, w_t \}$ through an optimized-based method, e.g., Frank-Wolfe~\cite{sener2018mto}.
% The solution can be used to calculate $\sum_{t=1}^T w_t \nabla_{\theta} \mathcal{L}_{\text{simple}}^t(\theta)$ as 
%  the gradient to update the model.

However, whether leveraging the Frank-Wolfe or the UGD algorithm, there are two disadvantages:
% However, there are two main drawbacks of such optimization-based method.
1) \textit{Inefficiency}. Both of these two methods need additional optimization at each training iteration, it greatly increases the training cost.
% We need to calculate the loss and backpropagate to get the gradient for each task. 
% For diffusion models, tasks are far more than traditional multi-task learning.
% During training, it requires to calculate the weight through optimization.
2) \textit{Instability}. In practice, by using a limited number of samples to calculate the gradient term $\nabla_{\theta} \mathcal{L}^t(\theta)$, the optimization results are unstable(as shown in Figure~\ref{fig:viz-alpha-instablity}). In other words, the loss weights for each denoising task vary greatly during training, making the entire diffusion training inefficient.

% [{\color{red} A figure here to demonstrate the instability on number of samples}]

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{figs/solution-over-samples.pdf}
    \vspace{-0.3cm}
    \caption{Demonstration of the instability of optimization-based weighting strategy. As the number of samples increases, the loss weight becomes stable, while the computation cost increases.
    % Visualization to demonstrate the instability of $w^2$ solved from Eq.~\ref{eq:ugd-opt}. 
    % {\color{magenta} Still have bugs here ...}
    }
    \vspace{-0.3cm}
    \label{fig:viz-alpha-instablity}
\end{figure}

% \begin{figure*}[!h]
%     \centering
%     \includegraphics[width=\textwidth]{figs/abl-viz-lossv1.pdf}
%     \caption{
%     TO ADD CAPTION, ...
%     }
%     \label{fig:viz-alpha-instablity}
% \end{figure*}



\subsection{Min-SNR-$\gamma$ Loss Weight Strategy}\label{sec:different-loss-strategy}
In order to avoid the inefficiency and instability caused by the iterative optimization in each iteration, one possible attempt is to adopt a stationery loss weight strategy. 

To simplify the discussion, we assume that the network is reparametered to predict the noiseless state $\mathbf{x}_0$. However, it's worth noting that different prediction objectives can be transformed into one another, we will delve into it in Section~\ref{sec:abalation}. Now, we consider the following alternative training loss weights:

% To simplify the discussion, we predict the noiseless state $\mathbf{x}_0$ from the network:
% The training loss can also be defined in the $\mathbf{x}$-\text{space},
% \begin{equation}\label{equ:simple-x0}
%     \mathcal{L}^t (\theta) = \mathbb{E}_{\mathbf{x}_0,\mathbf{\epsilon}}\left[
 %     \lVert \mathbf{x}_0 - \hat{\mathbf{x}}_\theta (\alpha_t \mathbf{x}_0 + \sigma_t \epsilon) \rVert _2^2
 %    \right]
% \end{equation}
% We consider the following alternative training loss weights:
%of term $\lVert \mathbf{x}_0 - \mathbf{\hat{x}_{\theta}} \rVert_2^2$. 


\begin{itemize}
    \item Constant weighting. 
    %$w_t = \frac{1}{T}$. 
    $w_t = 1$. 
    Which treats different tasks as equally weighted and has been used in both discrete diffusion models~\cite{gu2022vqdiffusion, tang2022improved} and continuous diffusion models~\cite{cao2022survey}.
    \item SNR weighting. $w_t=\text{SNR}(t)$, where $\text{SNR}(t)=\alpha_t^2 / \sigma_t^2$. It's the most widely used weighting strategy~\cite{meng2022ondistillation,ho2022video,dhariwal2021adm,rombach2022ldm}. By combining with Equation~\ref{eq:diffusion}, we can find it's numerically equivalent to the constant weighting strategy when the predicting target is noise.
    % \item ``SNR+1'' weighting. $w_t=\text{SNR}(t)+1$. This modification of SNR weighting is first proposed in ~\cite{salimans2022distillprogressive} to avoid a weight of zero with zero SNR steps.
    \item Max-SNR-$\gamma$ weighting. $w_t=\max\{ \text{SNR}(t), \gamma \}$. This modification of SNR weighting is first proposed in ~\cite{salimans2022distillprogressive} to avoid a weight of zero with zero SNR steps. They set $\gamma=1$ as their default setting. However, the weights still concentrate on small noise levels.

    % \item SNR loss weight proposed in Progressive Distillation~\cite{salimans2022distillprogressive}. Salimans et al. find that predicting $\mathbf{x}_0$ is better for distilling diffusion models than noise $\epsilon$. To match the predicting target, they deduce the simple loss for $\mathbf{x}_0$: $\mathcal{L}_{\text{simple}}^t = \frac{\alpha_t^2}{\sigma_t^2} \lVert \mathbf{x} - \hat{\mathbf{x}}_\theta (\mathbf{x}_t) \rVert_2^2$. $\text{SNR}=\frac{\alpha_t^2}{\sigma_t^2}$ is the signal-to-noise-ratio.
    \item Min-SNR-$\gamma$ weighting. $w_t=\min\{ \text{SNR}(t), \gamma \}$. We propose this weighting strategy to avoid the model focusing too much on small noise levels.
    % \item $\max\{ \text{SNR}, 1 \}$. It is also referred as ``truncated SNR'' in~\cite{salimans2022distillprogressive}. This design avoids a weight of zero to data with zero SNR.
    % \item Loss weights solved through Frank-Wolfe~\cite{sener2018mto} to minimize Eq.~\ref{eq:kkt}.
    % \item Other optimization based methods to achieve Pareto optimality, e.g., refer to Jianlian Su's blog.
    % \item UGD optimization weighting. $w_t$ is calculated by optimizing Equation~\ref{eq:ugd-opt} in each timestep. Compared with the previous setting, this strategy is changeable during training.
    \item UGD optimization weighting. $w_t$ is optimized from Equation~\ref{eq:ugd-opt} in each timestep. Compared with the previous setting, this strategy changes during training.
\end{itemize}

First, we combine these weighting strategies into Equation~\ref{eq:reg-opt} to validate whether they are approach to the Pareto optimality state. As shown in Figure~\ref{fig:object-value}, the UGD optimization weighting strategy can achieve the lowest score on our optimization target. In addition, the Min-SNR-$\gamma$ weighting strategy is the closest to the optimum, demonstrating it has the property to optimize different timesteps simultaneously.

% In the next section, we conduct experiments to verify our Min-SNR-$\gamma$ weighting strategy can effectively balance different noise levels, so as to obtain the fastest convergence speed and strong performance.
In the following section, we present experimental results to demonstrate the effectiveness of our Min-SNR-$\gamma$ weighting strategy in balancing diverse noise levels.
Our approach aims to achieve faster convergence and strong performance. 


% \begin{figure}[!ht]
%     \centering
%     \includegraphics{figs/loss-weight.pdf}
%     \caption{Visualization of difference loss strategies (in log scale). }
%     \label{fig:viz-SNR}
% \end{figure}

% We calculate the norm of weighted gradient in Eq.~\ref{eq:kkt}.
% The calculated norms and their related FID score is shown in Tab.~\ref{tab:min-norm}.


\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figs/objective-values.pdf}
    \vspace{-0.4cm}
    \caption{Comparison of the objective values in Equation~\ref{eq:reg-opt} on different weighting strategies.
    % {\color{magenta} To improve if having time ...}
    % Objective values on different setting. The regularization weight is $4\times 10^{3}$ and the threshold is $10^{-3}$.{\color{magenta} To replace the figure}
    }
    \vspace{-0.3cm}
    \label{fig:object-value}
\end{figure}



% template table
% \begin{table}[!h]
%     \centering
%     \begin{tabular}{|c|c|c|}
%     \hline
%         EXP-ID & MIN NORM & FID score \\
%     \hline\hline
%         x & x & x \\
%     \hline
%     x & x & x \\
%     \hline
%     \end{tabular}
%     \caption{Calculated minimum norm in Eq.~\ref{eq:kkt} and their related FID score. When xx is lower, the FID score is getting better.}
%     \label{tab:min-norm}
% \end{table}

% \begin{table*}[!ht]
%     \centering
%     \begin{tabular}{|l|l|l|l|l|l|l|l|}
%     \hline
%         {} & Optimized $\alpha$ & Const 0.1 & SNR & min1 & min5 & min10 & max1 \\ \hline
%         100k & 0.00256 & 0.010273 & 0.002893 & 0.003221 & 0.002727 & 0.002683 & 0.003177 \\ \hline
%         200k & 0.002233 & 0.00945 & 0.002265 & 0.002679 & 0.002315 & 0.002248 & 0.002711 \\ \hline
%         400k & 0.001777 & 0.010072 & 0.001885 & 0.002356 & 0.001916 & 0.001842 & 0.002617 \\ \hline
%         600k & 0.001836 & 0.004184 & 0.00186 & 0.002095 & 0.001864 & 0.001825 & 0.001795 \\ \hline
%         800k & 0.001438 & 0.006539 & 0.00144 & 0.002032 & 0.001607 & 0.001506 & 0.001795 \\ \hline
%         1M & 0.002236 & 0.007674 & 0.002395 & 0.00267 & 0.002326 & 0.002308 & 0.002798 \\ \hline
%     \end{tabular}
%     \caption{Calculated minimum norm in Eq.~\ref{eq:kkt} and their related FID score. When xx is lower, the FID score is getting better.}
%     \label{tab:min-norm}
% \end{table*}

% \subsection{Placeholer}
% refer to Jianlin Su's blog ...

% \subsection{ViT~\cite{vit}}

% There have been several works adopting Vision Transformer~\cite{vit} as the backbone.
% Bao et al.~\cite{bao2022uvit} borrow the skip-connection design from UNet~\cite{ho2020ddpm,2021Scoresde,dhariwal2021adm}.
% They also add one convolution layer after linear projection.
% Peebles et al.~\cite{peebles2022dit} also adopt the Adaptive LayerNorm (AdaLN) design from Adm~\cite{dhariwal2021adm} and find that zero-initialization is important for fast convergence and better performance.

% We find that there is no need to change the design of ViT~\cite{vit} and simple loss weight design could help the model converge faster and achieve superior performance than previous ViT diffusion models.