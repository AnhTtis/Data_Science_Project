
% \subsubsection{INet64}

% \subsection{INet256}
In this section, we first provide an overview of the experimental setup. Subsequently, we conduct comprehensive ablation studies to show that our method is versatile and suitable for various prediction targets and network architectures. Finally, we compare our approach to the state-of-the-art methods across multiple image generation benchmarks, demonstrating not only its accelerated convergence but also its superior capability in generating high-quality images.
% demonstrating its faster convergence and superior generation quality.

% We evaluate our proposed method on several image generation benchmarks.
% We list the implementation details in Sec.~\ref{sec:implementation}, including used datasets, training and evaluation settings.
% In Sec.~\ref{sec:abalation}, we perform an ablation study on our loss design to showcase the effectiveness, fast convergence, and robustness of our method.
% Sec.~\ref{sec:compare-with-sota} presents a comparison of our method with state-of-the-art methods, highlighting its superiority.

% ------------------------------------------
% Implementation Details
% ------------------------------------------

% \subsection{Implementation Details and Evaluation}
\subsection{Setup}\label{sec:implementation}

% We evaluate our proposed method on several common datasets.
% We show the details of the datasets and training details.
% \subsubsection{Test Benchmarks}
\noindent \textbf{Datasets.}
We perform experiments on both unconditional and conditional image generation using the CelebA dataset~\cite{liu2015CelebA} and the ImageNet dataset~\cite{deng2009imagenet}. 
The CelebA dataset, which comprises 162,770 human faces, is a widely-used resource for unconditional image generation studies. 
We follow ScoreSDE~\cite{2021Scoresde} for data pre-processing, which involves center cropping each image to a resolution of $140\times 140$ and then resizing it to $64\times 64$. 
% To preprocess our images, we adopt the same approach as ScoreSDE~\cite{2021Scoresde}. Specifically, we first perform a center crop on each image, reducing its resolution to $140\times 140$ pixels. We then resize the cropped image to a final resolution of $64\times 64$ pixels.
For the class conditional image generation, we adopt the ImageNet dataset~\cite{deng2009imagenet} with a total of 1.3 million images from 1000 different classes. We test the performance on both $64\times 64$ and $256\times 256$ resolutions.
% with a total of 1.3 million images from 1000 classes.
% using {\color{magenta} what's here?}

\noindent \textbf{Training Details.}\label{sec:training-detail}
% {\color{blue} pixel space/latent space, network structure, parameter settings.}
For low resolution ($64\times 64$) image generation, we follow ADM~\cite{dhariwal2021adm} and directly train the diffusion model on the pixel-level. 
For high-resolution image generation, we utilize LDM~\cite{rombach2022ldm} approach by first compressing the images into latent space, then training a diffusion model to model the latent distributions. 
% Our compression utilizes the VQ-VAE from Stable Diffusion\footnote{https://huggingface.co/stabilityai/sd-vae-ft-mse-original}, which encodes an image of $256\times 256\times 3$ resolution into $32\times 32 \times 4$ latent codes.
To obtain the latent for images, we employ VQ-VAE from Stable Diffusion\footnote{https://huggingface.co/stabilityai/sd-vae-ft-mse-original}, which encodes a high-resolution image ($256\times 256\times 3$) into $32\times 32 \times 4$ latent codes.

In our experiments, we employ both ViT and UNet as our diffusion model backbones. We adopt a vanilla ViT structure without any modifications~\cite{vit} as our default setting. we incorporate the timestep $t$ and class condition $\mathbf{c}$ as learnable input tokens to the model. 
Although further customization of the network structure may improve performance, our focus in this paper is to analyze the general properties of diffusion models. For the UNet structure, we follow ADM~\cite{dhariwal2021adm} and keep the FLOPs similar to the ViT-B model, which has $1.5\times$ parameters. Additional details can be found in the appendix.

For the diffusion settings, we use a cosine noise scheduler following the approach in~\cite{nichol2021iddpm,dhariwal2021adm}. The total number of timesteps is standardized to $T=1000$ across all datasets. We adopt AdamW~\cite{2014Adam,2018adamw} as our optimizer. 
For the CelebA dataset, we train our model for 500K iterations with a batch size of 128. During the first 5,000 iterations, we implement a linear warm-up and keep the learning rate at $1\times 10^{-4}$ for the remaining training. 
For the ImageNet dataset, the default learning rate is fixed at $1\times 10^{-4}$. 
% The batch size is set to $1024$ for $64\times 64$ resolution and $256$ for $256\times 256$ resolution. 
The batch size is set to $1024$ for $64^2$ resolution and $256$ for $256^2$ resolution. 
% They are trained for 800K and 7M iterations, respectively.
%
% We train all the models with AdamW~\cite{2014Adam,2018adamw}.
% The exponential moving average (EMA) is of great signifiance to generative models.
% During training, we save the model with EMA rate 0.9999 and use it for later evaluation.

% \noindent \textbf{Hyperparameters.~}
% To train our model for the CelebA $64\times 64$, we conducted 500$K$ iterations with batch size 128 and applied a linear warmup of 5,000 iterations at the beginning. Subsequently, we maintained a constant learning rate of $1\times 10^{-4}$ for the remaining iterations. 
% For ImageNet, we use a constant learning rate of $8\times 10^{-5}$. The training batch size of $64\times 64$ is set to 1024, and that of $256\times 256$ is set to 256.
% We train on ImageNet $64\times 64$ for 800K iterations and ImageNet $256\times 256$ for 7M iterations.


% \noindent \textbf{High Resolution Image Generation.~}%\textbf{Pixel/Latent Space.} 
% We directly train the diffusion model on raw pixels for images at resolution $64\times 64$.
% However, it is difficult for diffusion models to directly learn the distribution of high-resolution images in pixel space ($256\times 256$ or higher).
% In ADM~\cite{dhariwal2021adm} and CDM~\cite{ho2022cdm}, the authors train several upsamplers (super-resolution modules) to obtain high resolution images.
% However, it costs a lot to train the base generator and upsamplers.
% We follow LDM~\cite{rombach2022ldm} to first compress the images into latent space (an image of resolution $256\times 256\times 3$ is mapped to a latent code $32\times 32 \times 4$) and train a diffusion model to learn the distribution of latent codes. 
% The autoencoder to compress the images is from Stable Diffusion\footnote{https://huggingface.co/stabilityai/sd-vae-ft-mse-original}.
% The encoder is from the original kl-f8 autoencoder while the decoder is further finetuned 

% \noindent \textbf{Backbone Choices.~}
% Our method works well both for ViT~\cite{vit} and UNet~\cite{dhariwal2021adm}.
% We conduct extensive ablation study using these two backbones.
% Due to its \textit{scalability} and potential in diffusion models~\cite{peebles2022dit}, we utilize ViT~\cite{vit} as a benchmark model to compare with state-of-the-art methods on challenging datasets. 
% The architecture setting for UNet is from ADM~\cite{dhariwal2021adm}.
% We keep the parameters of the UNet close to ViT-B and the FLOPs $1.5\times$ of ViT-B.
% We use standard initialization techniques in ViT~\cite{vit,bao2021beit} for our Transformer backbone.
% The timestep $t$ and the class condition $c$ are taken as input tokens to the model. We list the basic setting for ViTs in the appendix.
% \begin{table}[!h]
% \begin{center}
% \setlength{\tabcolsep}{1.5mm}
% \begin{tabular}{l c c c c}
% \toprule
% Model & Layers & Hidden Size &  Heads & Params \\
% \midrule
% ViT-Small & 13 & 512 & 6 & 43M \\
% ViT-Base & 12 & 768 & 12 & 88M \\
% % ViT-Mid   & - & - & - & - \\
% ViT-Large   & 21 & 1024 & 16 & 269M \\
% ViT-XL   & 28 & 1152 & 16 & 451M \\
% \bottomrule
% \end{tabular}
% \end{center}
% \caption{
% Configurations of our used ViTs.{\color{magenta} remove later ...}
% }
% \label{tab:abl-pred-noise}
% \end{table}



\noindent \textbf{Evaluation Settings.}\label{sec:evaluation}
To evaluate the performance of our models, we utilize an Exponential Moving Average (EMA) model with a rate of 0.9999. 
During the evaluation phase, we generate images with the Heun sampler from EDM~\cite{karras2022edm}. For conditional image generation, we also implement the classifier-free sampling strategy~\cite{ho2021classifierfree} to achieve better results. Finally, we measure the quality of the generated images using the FID score calculated on 50K images.

% During training, we save the model with EMA rate 0.9999 and use it for later evaluation.

% classifier-free sampling strategy~\cite{}

% We adopt the widely used FID~\cite{heusel2017fid} metric to evaluate the quality of generated images.
% We generate 50,000 samples to calculate the FID score as in~\cite{dhariwal2021adm,nichol2021iddpm}.
% For ImageNet $64\times 64$, we adopt the pre-trained noisy classifier as guidance~\cite{2014Adam} and use DPM Solver~\cite{Lu2022DPMSolverAF} following U-ViT~\cite{bao2022uvit}.
% For CelebA $64\times 64$, we use EDM's Heun sampler~\cite{karras2022edm} to generate the images. 
% For ImageNet $256\times 256$, we use EDM's Heun sampler to obatain the latent codes and decode them to images by fine-tuned VAE decoder.

% A summary of the detailed settings mentioned above will be included in the appendix.


% ------------------------------------------
% Ablation Study
% ------------------------------------------

% \subsection{Analysis (Method Extension)}
\subsection{Analysis of the Proposed Min-SNR-$\gamma$ }\label{sec:abalation}

% {\color{blue} compare different loss weight, highlight the fast convergence and better performance, only x0+different loss weight, provide visualization results to demonstrate the fast convergence!}

\begin{figure*}[!h]
    \centering
    \includegraphics[width=0.33\textwidth]{figs/abl-x0-vit.pdf}
    \includegraphics[width=0.33\textwidth]{figs/abl-noise-vit.pdf}
    \includegraphics[width=0.33\textwidth]{figs/abl-v-vit.pdf}
    \vspace{-0.8cm}
    \caption{Comparing different loss weighting designs on predicting $\mathbf{x}_0$, $\mathbf{\epsilon}$, $\mathbf{v}$. Taking the neural network output as noise with const or Max-SNR-$\gamma$ strategy lead to divergence. Min-SNR-$\gamma$ strategy converges the fastest under all these settings.
    % Ablate loss weighting design choices on predicting $\mathbf{x}_0$, $\mathbf{\epsilon}$, $\mathbf{v}$. We adopt Heun sampler proposed in EDM~\cite{karras2022edm} to sample $50K$ images to calculate the FID score. 
    % Predicting $\epsilon$ with loss weight ``const'' and ``$\max \{ \text{SNR}, 1\}$'' failed.
    }
    \label{fig:abl-in256}
\end{figure*}


\begin{figure*}[!h]
    \centering
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/loss_0-100.pdf}
    \end{subfigure}
    \hspace{-2mm}
    % \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/loss_200-300.pdf}
    \end{subfigure}
    % \hfill
    \hspace{-2mm}
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/loss_600-700.pdf}
    \end{subfigure}
    \hspace{-2mm}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/loss_800-900.pdf}
    \end{subfigure}
    \vspace{-0.3cm}
    \caption{
    Unweighted loss in different ranges of timesteps. From left to right, each figure represents a specific range of timesteps: $[0, 100), [200, 300), [600, 700), [800, 900)$. The $y$-axis represents the Mean Squared Error (MSE), averaged over each range of timesteps.}
    \label{fig:viz-loss-diff-schedule}
\end{figure*}


\noindent \textbf{Comparison of Different Weighting Strategies.}
To demonstrate the significance of the loss weighting strategy, we conduct experiments with different loss weight settings for predicting $\mathbf{x}_0$. 
These settings include: 
1) constant weighting, where $w_t = 1$, 
2) SNR weighting, with $w_t = \text{SNR}(t)$, 
3) truncated SNR weighting, with $w_t = \max\{ \text{SNR}(t), \gamma \}$ (following~\cite{salimans2022distillprogressive} with a set value of $\gamma=1$), and 4) our proposed Min-SNR-$\gamma$ weighting strategy, with $w_t = \min\{ \text{SNR}(t), \gamma \}$, we set $\gamma=5$ as the default value.

The ViT-B serves as our default backbone and experiments are performed on ImageNet $256\times 256$. As illustrated in Figure~\ref{fig:abl-in256}, we observe that all results improve as the number of training iterations increases. However, our method demonstrates a significantly faster convergence compared to other methods. Specifically, it exhibits a $3.4\times$ speedup in reaching an FID score of $10$. It is worth mentioning that the SNR weighting strategy performed the worst, which could be due to its disproportionate focus on less noisy stages.


For a deeper understanding of the reasons behind the varying convergence rates, we analyzed their training loss at different noise levels. For a fair comparison, we exclude the loss weight term by only calculating $\lVert \mathbf{x}_0 - \mathbf{\hat{x}_{\theta}} \rVert_2^2$. 
% Considering that the loss of different noise levels varies greatly, we follow~\ref{} and calculate the loss in different bins and present in Fig~\ref{}. The results show that the constant weighting strategy is effective for high noise intensities, but struggles with low noise intensities. Conversely, the SNR weighting strategy exhibits the opposite behavior. In contrast, our proposed Min-SNR-$\gamma$ strategy achieves a lower training loss for all cases, indicates quicker convergence through the FID metric.
Considering that the loss of different noise levels varies greatly, we calculate the loss in different bins and present the results in Figure~\ref{fig:viz-loss-diff-schedule}. 
The results show that while the constant weighting strategy is effective for high noise intensities, it performs poorly at low noise intensities. Conversely, the SNR weighting strategy exhibits the opposite behavior. 
In contrast, our proposed Min-SNR-$\gamma$ strategy achieves a lower training loss across all cases, and indicates quicker convergence through the FID metric.

% Here we ablate on different loss weighting strategies.
% Our model takes noisy latent variables as input and predicts $\mathbf{x}_0$.
% We compare our proposed Min-SNR-$k$ with 3 baselines: 1) simple mean-squred error (MSE) loss on $\mathbf{x}_0$; 2) MSE loss with SNR as loss weight~\cite{salimans2022distillprogressive}; 3) MSE loss with $\max \{ \text{SNR}, 1 \}$ (truncated SNR) as loss weight.

% We present the FID score plotted against the training progress in Fig.~\ref{fig:abl-in256}.
% We can see that SNR and $\max \{ \text{SNR}, 1 \}$ perform worse than MSE loss with constant loss weight ($=1$).
% Our method converge faster than three baseline methods and shows $3.4\times$ speedup to achieve the FID score of 10.
% We train 1M iterations for ablation study and our method achieve 4.98 FID, which is better than $\mathbf{x}_0 + \text{const}$'s 9.25.
% The results in the figure demonstrate our method's faster convergence and better performance. {\color{magenta} Add analysis here.}

% The visual results are generated from a ViT-B backbone.
% We train the diffusion ViT-B using different loss weighting strategies and generate samples at different training iterations.
% Our visual results are along with above FID score presented above but provide more visual insights.

Furthermore, we present visual results in Figure~\ref{fig:abl-loss-viz} to demonstrate the fast convergence of the Min-SNR-$\gamma$ strategy. 
We apply the same random seed for noise to sample images from training iteration 50K, 200K, 400K, and 1M with different loss weight settings. Our results show that the Min-SNR-$\gamma$ strategy generates a clear object with only 200K iterations, which is significantly better in quality than the results obtained by other methods.




\begin{figure*}[!h]
    \centering
    \includegraphics[width=\textwidth]{figs/abl-viz.pdf}
    \caption{Qualitative comparison of the generation results from different weighting strategies on ImageNet-256 dataset. Images in each column are sampled from 50K, 200K, 400K, and 1M iterations. Our Min-SNR-5 strategy yields significant improvements in visual fidelity from the same iteration.}
    \label{fig:abl-loss-viz}
    \vspace{-5mm}
\end{figure*}

% \begin{figure}
%     \centering
%     \includegraphics{figs/viz-loss-for-ablation.pdf}
%     \caption{loss related to figure 7.a.}
%     \label{fig:viz-loss-for-ablation}
% \end{figure}

\noindent \textbf{Min-SNR-$\gamma$ for Different Prediction Targets.}
Instead of predicting the original signal $\mathbf{x}_0$ from the network, some recent works have employed alternative re-parameterizations, such as predicting noise $\epsilon$, or velocity $\mathbf{v}$~\cite{salimans2022distillprogressive}. To verify the applicability of our weighting strategy to these prediction targets, we conduct experiments comparing the four aforementioned weighting strategies across these different re-parameterizations. 

As we discussed in Section~\ref{sec:different-loss-strategy}, predicting noise $\epsilon$ is mathematically equivalent to predicting $\mathbf{x}_0$ by intrinsically involving Signal-to-Noise Ratio as a weight factor, thus we divide the SNR term in practice. For example, the Min-SNR-$\gamma$ strategy in predicting noise can be expressed as $w_t = \frac{\min\{ \text{SNR}(t), \gamma \}}{\text{SNR}(t)} = \min\{ \frac{\gamma}{\text{SNR}(t)}, 1 \}$. And the SNR strategy in predicting noise is equivalent to a ``constant strategy''. For simplicity and consistency, we still refer to them as Min-SNR-$\gamma$ and SNR strategies. Similarly, we can derive that when predicting velocity $\mathbf{v}$, the loss weight factor must be divided by $(\text{SNR}+1)$. These strategies are still referred to by their original names for ease of reference.

We conduct experiments on these two variants and present the results in Figure~\ref{fig:abl-in256}. Taking the neural network output as noise with const or Max-SNR-$\gamma$ setting leads to divergence. Meanwhile, our proposed Min-SNR-$\gamma$ strategy converges faster than other loss weighting strategies for both prediction noise and predicting velocity. These demonstrate that balancing the loss weights for different timesteps is intrinsic, independent of any re-parameterization.

% Wait for results.

% Our method not only works for predicting $\mathbf{x}_0$, but also works well for predicting noise $\epsilon$ and predicting velocity $\mathbf{v}$~\cite{salimans2022distillprogressive}.
% We conduct the experiments predicting $\epsilon$ and present the results in Tab.~\ref{tab:abl-arch-design}.
% From the table, we can see that, predicting $\epsilon$ with our loss weight also converges faster and achieve better preformance.


% \begin{table}[!h]
% \begin{center}
% \setlength{\tabcolsep}{1.5mm}
% \begin{tabular}{l c c c c c}
% \toprule
% Training Iterations & 200K & 400K & 600K & 800K & 1M \\
% \midrule
% Baseline ($\epsilon$) & 17.28 & 9.56 & 7.55 & 6.63 & 6.10 \\
% + Min-SNR-$5$ & 13.01 & 7.13 & 5.72 & 5.22 & 4.96 \\
% \bottomrule
% \end{tabular}
% \end{center}
% \caption{
% Ablation study on predicting noise.
% Our simple design also works well for predicting noise $\epsilon$.
% It has $4.27$ FID improvement at $200K$ iterations and still has $1.14$ improvement at $1M$ iterations.
% }
% \label{tab:abl-pred-noise}
% \end{table}


\noindent \textbf{Min-SNR-$\gamma$ on Different Network Architectures.}
% {\color{blue} validate Min-SNR-$k$ on unet structure}
The Min-SNR-$\gamma$ strategy is versatile and robust for different prediction targets and network structures. We conduct experiments on the widely used UNet and keep the number of parameters close to the ViT-B model. For each experiment, models were trained for 1 million iterations and their FID scores were calculated at multiple intervals. The results in Table~\ref{tab:abl-unet} indicate that the Min-SNR-$\gamma$ strategy converges significantly faster than the baseline and provides better performance for both predicting $\mathbf{x}_0$ and predicting noise.

% Wait for results.
% The UNet follows the design from ADM~\cite{dhariwal2021adm} and we keep the parameters close to ViT-B.
% From the table, we can see that with our loss weighting design, diffusion UNet obtain the FID 7.99, which is significantly superior better than the baseline, only after $200K$ training iterations. 

\begin{table}[t]
\begin{center}
\setlength{\tabcolsep}{1.5mm}
\begin{tabular}{l c c c c c}
\toprule
Training Iterations & 200K & 400K & 600K & 800K & 1M \\
\midrule
Baseline ($\mathbf{x}_0$) & 25.93 & 15.41 & 11.54 & 9.52 & 8.33 \\
+ Min-SNR-$5$ & \textbf{7.99} & \textbf{5.34} & \textbf{4.69} & \textbf{4.41} & \textbf{4.28} \\
\midrule
Baseline ($\epsilon$) & 8.55 & 5.43 & 4.64 & 4.35 & 4.21 \\
+ Min-SNR-$5$ & \textbf{7.32} & \textbf{4.98} & \textbf{4.48} & \textbf{4.24} & \textbf{4.14} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-5mm}
\caption{
Ablation studies on the UNet backbone. Whether the network predicts $\mathbf{x}_0$ or $\epsilon$, the Min-SNR-5 weighting design converges faster and achieves better FID score.
}
\label{tab:abl-unet}
\vspace{-5mm}
\end{table}
% \begin{figure}[!h]
%     \centering
%     \includegraphics{figs/abl-in256-x0-unet.pdf}
%     \caption{Ablate loss weight design choices on predicting $\mathbf{x}_0$. We adopt Heun sampler proposed in EDM~\cite{karras2022edm} to sample $50K$ images to calculate the FID score.}
%     \label{fig:abl-in256}
% \end{figure}

% \begin{table}[!h]
% \begin{center}
% \setlength{\tabcolsep}{1.5mm}
% \begin{tabular}{l c c c c c}
% \toprule
% Training Iterations & 200K & 400K & 600K & 800K & 1M \\
% \midrule
% Baseline & 30.15 & 17.18 & 12.55 & 10.44 & 9.25 \\
% B + SC & 24.4 & 12.67 & 9.24 & 7.92 & 7.28 \\
% B + min$5$ & 12.22 & 6.73 & 5.60 & 5.21 & 4.98 \\
% B + SC + min$5$ & 9.44 & 6.10 & 5.52 & 5.33 & 5.31 \\
% \bottomrule
% \end{tabular}
% \end{center}
% \caption{
% Comparison between loss weighting design and architecture modification. 
% We take ViT-B and $\mathbf{x}_0 + \text{const}$ as baseline.
% Our method without specific architecture design performs better than that with skip connection as in~\cite{bao2022uvit}.
% SC denotes skip connection.
% }
% \label{tab:abl-arch-design}
% \end{table}

% Recent works on diffusion Transformers~\cite{peebles2022dit,bao2022uvit} modify the architecture to achieve competitive performance as in UNet~\cite{dhariwal2021adm}.
% Bao et al.~\cite{bao2022uvit} borrow the skip-connection design from UNet~\cite{ho2020ddpm,2021Scoresde,dhariwal2021adm}.
% They also add one convolution layer after linear projection.
% Peebles et al.~\cite{peebles2022dit} adopt the Adaptive LayerNorm (AdaLN) design from ADM~\cite{dhariwal2021adm} and find that zero-initialization is important for better performance. Do we really need to modify the ViT architecture to learn a better diffusion model?

% We find that there is no need to change the design of ViT~\cite{vit} and simple loss weight design could help the model converge faster and achieve superior performance than previous ViT diffusion models.

% Here we compare the results of our method with those results from architecture design choices, especially the skip-connection and Zero-AdaLN.
% We can find that even though these designs improve the performance but our simple loss design is more effective.
% {\color{magenta} Results on AdaLN-Zero are to be reported ... Still have bugs ...}

\noindent \textbf{Robustness Analysis.}
% {\color{blue} analysis if k is robustness, maybe this sections can merge to 4.2.1}
Our approach utilizes a single hyperparameter, $\gamma$, as the truncate value. To assess its robustness, we conducted a thorough robustness analysis in various settings. Our experiments were performed on the ImageNet-256 dataset using the ViT-B model and the prediction target of the network is $\mathbf{x}_0$. We varied the truncate value $\gamma$ by setting it to 1, 5, 10, and 20 and evaluated their performance. The results are shown in Table~\ref{tab:abl-diff-k}. We find there are only minor variations in the FID score when $\gamma$ is smaller than 20. Additionally, we conducted more experiments by modifying the predicting target to the noise $\epsilon$, and modifying the network structure to UNet. We find that the results were also consistently stable. Our results indicate that good performance can usually be achieved when $\gamma$ is set to 5, making it the established default setting.

\begin{table}[t]
\begin{center}
\setlength{\tabcolsep}{2mm}
\begin{tabular}{l c c c c }
\toprule
$\gamma$ & 1 & 5 & 10 & 20  \\
\midrule
ViT ($\mathbf{x}_0$) & 4.98 & \textbf{4.92} & 5.34 & 5.45  \\
ViT ($\epsilon$)  & 4.89 & \textbf{4.84} & 4.94 & 5.41  \\
UNet ($\mathbf{x}_0$) & 4.49 & \textbf{4.28} & 4.32 & 4.37  \\
UNet ($\epsilon$) & 4.30 & 4.14 & 4.14 & \textbf{4.12}  \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-5mm}
\caption{ 
Ablation study on $\gamma$.
The results are robust to the hyper-parameter $\gamma$ in different settings.}
\label{tab:abl-diff-k}
\vspace{-3mm}
\end{table}


\begin{table}[t]
\begin{center}
\begin{tabular}{l c c}
\toprule
Method & \#Params & FID \\
\midrule
DDIM~\cite{ddim} & 79M & 3.26 \\
Soft Truncation~\cite{ddim} & 62M & 1.90 \\
\textbf{Our UNet} & 59M & \textbf{1.60} \\
\midrule
U-ViT-Small~\cite{bao2022uvit} & 44M &   2.87 \\
\textbf{ViT-Small (ours)} & 43M &   \textbf{2.14} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-5mm}
\caption{FID results of unconditional image generation on CelebA $64\times 64$~\cite{liu2015CelebA}. 
We conduct experiments with both UNet and ViT backbone.
% ``\#Params'' denotes the number of parameters.
}
\label{tab:CelebA64}
\vspace{-3mm}
\end{table}

%setting for CelebA-unet: feb10_CelebA_unet_64_1222_lr1e-4_09_0999_warm5k_pred_x0__min_SNR_60__fp16_bs4x32

% ---------------------------------
% imagenet 64
% ---------------------------------


% \begin{figure}[t]
% \begin{center}
% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    %\includegraphics[width=0.8\linewidth]{egfigure.eps}
% \end{center}
%    \caption{Comparison with the designs proposed in DiT~\cite{peebles2022dit} and U-ViT~\cite{bao2022uvit}.}
% \label{fig:compare-arch-design}
% \end{figure}


% template table
% \begin{table}[!h]
%     \centering
%     \begin{tabular}{|c|c|}
%     \hline
%         x & x \\
%     \hline\hline
%         x & x \\
%     \hline
%     x & x \\
%     \hline
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table}

\subsection{Comparison with state-of-the-art Methods}\label{sec:compare-with-sota}
% For unconditional image generation, our method with ViT~\cite{vit} could achieve the FID score of 2.14, which is better than previous ViT methods and comparable to previous UNet methods.
\noindent\textbf{CelebA-64.}
% The FID results of unconditional image generation on CelebA $64\times 64$ are shown in Tab.~\ref{tab:CelebA64}. It shows that our method with ViT~\cite{vit} could achieve the FID score of 2.14, which is better than previous ViT methods.
% Our method with UNet~\cite{dhariwal2021adm} could achieve the FID score of 1.60, which outperforms previous baseline UNet methods.
We conduct experiments on the CelebA $64\times 64$ dataset for unconditional image generation. Both UNet and ViT are used as our backbones and are trained for 500K iterations. During the evaluation, we use the EDM sampler~\cite{karras2022edm} to generate 50K samples and calculate the FID score. The results are summarized in Table~\ref{tab:CelebA64}. Our ViT-Small~\cite{vit} model outperforms previous ViT-based models with an FID score of 2.14. It is worth mentioning that no modifications are made to the naive network structure, demonstrating that the results could still be improved further. Meanwhile, our method using the UNet~\cite{dhariwal2021adm} structure achieves an even better FID score of 1.60, outperforming previous UNet methods.

\noindent\textbf{ImageNet-64.}
% The results of conditional image generation on ImageNet $64\times 64$ are presented in Tab.~\ref{tab:in64}. 
% During sampling process, a pre-trained noisy classifier~\cite{dhariwal2021adm} is used for guidance.
% Even with almost the same model parameters and no addtional architecture designs, our method with 21-layer ViT-Large has a FID score of 2.40 and an improvement of \textbf{1.74} than U-ViT-Large~\cite{bao2022uvit}. 
% The SoTA method EDM~\cite{karras2022edm} is with advanced training designs.
We also validate our method on class-conditional image generation on the ImageNet $64\times 64$ dataset. 
During training, the class label is dropped with the probability $0.15$ for classifier-free inference~\cite{ho2021classifierfree}. 
The model is trained for 800K iterations and images are synthesized using classifier-free guidance with a scale of $\text{cfg}=1.5$ and the EDM sampler for image generation. 
For a fair comparison, we adopt a 21-layer ViT-Large model without additional architecture designs, which has a similar number of parameters to U-ViT-Large~\cite{bao2022uvit}. 
The results presented in Table~\ref{tab:in64} show that our method achieves an FID score of 2.28, significantly improving upon the U-ViT-Large model. 
% Meanwhile, we also test our model with UNet backbone from~\cite{dhariwal2021adm}, {\color{magenta} we are able to obtain FID results that are comparable to those of previous strong baselines.}

% Even with almost the same model parameters and no addtional architecture designs, our method with 21-layer ViT-Large has a FID score of 2.40 and an improvement of \textbf{1.74} than U-ViT-Large~\cite{bao2022uvit}. 


\begin{table}[t]
\begin{center}
\begin{tabular}{l c c }
\toprule
Method & \#Params & FID \\
\midrule
BigGAN-deep~\cite{brock2018biggan} & {} & 4.06 \\
StyleGAN-XL~\cite{sauer2022styleganxl} & {} & 1.51 \\
\midrule
IDDPM (small)~\cite{nichol2021iddpm} & 100M & 6.92 \\
IDDPM (large)~\cite{nichol2021iddpm} & 270M &  2.92 \\
CDM~\cite{ho2022cdm} & {} &  1.48 \\
ADM~\cite{dhariwal2021adm} & 296M &  2.61 \\
% ADM (dropout)~\cite{dhariwal2021adm} & 296M &  2.07 \\
% EDM~\cite{karras2022edm} & 296M &   1.36 \\
% \textbf{Our UNet} & 296M &  2.33 \\
% \textbf{Our UNet} & 296M &  3.55 \\
\midrule
U-ViT-Mid~\cite{bao2022uvit} & 131M &   5.85 \\
U-ViT-Large~\cite{bao2022uvit} & 287M &   4.26 \\
\textbf{ViT-L (ours)} & 269M &   \textbf{2.28} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-5mm}
\caption{
FID results on ImageNet $64\times 64$. We conduct experiments using the ViT-L backbone which significantly improves upon previous methods.}
\label{tab:in64}
\end{table}


% ---------------------------------
% imagenet 64
% ---------------------------------
\begin{table}[t]
\begin{center}
\begin{tabular}{ l c c }
\toprule
Method & \#Params & FID \\
\midrule
BigGAN-deep~\cite{brock2018biggan} & 340M & 6.95 \\
StyleGAN-XL~\cite{sauer2022styleganxl} & {} & 2.30 \\
\midrule
Improved VQ-Diffusion~\cite{gu2022vqdiffusion} & 460M & 4.83 \\
\midrule
IDDPM~\cite{nichol2021iddpm} & 270M &  12.26 \\
CDM~\cite{ho2022cdm} & {} &  4.88 \\
ADM~\cite{dhariwal2021adm} & 554M &  10.94 \\
ADM-U~\cite{dhariwal2021adm} & 608M &  7.49 \\
ADM-G~\cite{dhariwal2021adm} & 554M &  4.59 \\
ADM-U, ADM-G~\cite{dhariwal2021adm} & 608M &  3.94 \\
LDM~\cite{rombach2022ldm} & 400M & 3.60 \\
\textbf{UNet (ours)} & 395M & \textbf{2.81}$^\dagger$ \\
% \textbf{Our UNet} & 296M & 4.46 (640K) \\
\midrule
U-ViT-L~\cite{bao2022uvit} & 287M &   3.40 \\
DiT-XL-2~\cite{peebles2022dit} & 675M &   2.27 \\
% \textbf{Our ViT-L} & 271M &   2.14 \\
% Our ViT-XL & x &   \textbf{2.0559} \\
\textbf{ViT-XL (ours)} & 451M &   \textbf{2.06} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-5mm}
\caption{
FID results on ImageNet $256\times 256$. $^\dagger$ denotes only train 1.4M iterations. Our model with a ViT-XL backbone achieves a new record FID score of 2.06. 
% {
% \color{magenta} Notice: ADM's diffusion model consists of base generator and upsampeler, but only \#Params for base generator is reported.
% }
}
\label{tab:in256}
\vspace{-3mm}
\end{table}


\noindent\textbf{ImageNet-256.}
% We compare our results on ImageNet $256\times 256$ in Tab.~\ref{tab:in256}.
% We train the diffusion models in latent space with size $32\times 32 \times 4$. 
% We sample the latent codes using classifier-free guidance~\cite{ho2021classifierfree}.
% The sampled codes are later decoded to images.
% Our XL model with patch size $p=2$ achieves the FID score of 2.06, which is \textbf{0.21} better than previous SoTA method DiT~\cite{peebles2022dit}.
We also apply diffusion models for higher-resolution image generation on the ImageNet $256\times 256$ benchmark. To enhance training efficiency, we first compress $256\times 256 \times 3$ images into $32\times 32 \times 4$ latent codes using the encoder from LDM~\cite{rombach2022ldm}. During the sampling process, we employ the EDM sampler and the classifier-free guidance to generate images. 
The FID comparison is presented in Table~\ref{tab:in256}. 
Under the setting of predicting $\epsilon$ with Min-SNR-5, our ViT-XL model achieves the FID of $2.08$ for only 2.1M iterations, which is $3.3\times$ faster than DiT and outperforms the previous state-of-the-art FID record of $2.27$. 
Moreover, with longer training (about 7M iterations as in~\cite{peebles2022dit}), we are able to achieve the FID score of 2.06 by predicting $\mathbf{x}_0$ with Min-SNR-5.
% Our UNet-based model with 395M parameters is also significantly improved to previous works~\cite{rombach2022ldm}.
Our UNet-based model with 395M parameters is trained for about 1.4M iterations and achieves FID score of 2.81.
% The diffusion Transformers are then trained in the latent space for $7M$ iterations. 
% {\color{magenta} Predicting noise + training for 2.1M ($3.3\times$ faster than DiT) iterations could achieve the FID 2.08.}
 % We find that
 % our method with ViT-XL backbone achieves a new FID record 2.06, which is an improvement of $0.21$ than previous SoTA 2.27~\cite{peebles2022dit}. We also conduct experiments with a UNet backbone, 
% {\color{magenta} to add ...}


% Qualitative results are shown in Fig.~\ref{fig:visual-results-on-different-datasets}. The samples on CelebA $64\times 64$, ImageNet $64\times 64$, and ImageNet $256\times 256$ are randomly chosen without cherry-pick.
% We also train diffusion models on ImageNet $512\times 512$ (latent codes with size $64\times 64 \times 4$). 
% Due to the limit of compute resource, we only train it with $p=4$. The samples in Fig.~\ref{fig:abl-in256} are generated with a classifier-free guidance scale {\color{red} [XXX]}.
% {\color{magenta}Whether to add visual results on other dataset? Or just put them in the appendix.}

% \begin{figure*}[!h]
%     \centering
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/CelebA64.png}
%         \caption{CelebA $64\times 64$}
%         \label{fig:CelebA64}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/in64.png}
%         \caption{ImageNet $64\times 64$}
%         \label{fig:in64}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/in256.png}
%         \caption{ImageNet $256\times 256$}
%         \label{fig:in64}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/in512.png}
%         \caption{ImageNet $512\times 512$}
%         \label{fig:in64}
%     \end{subfigure}
%     \caption{Generated samples from different datasets: CelebA-$64\times 64$ and ImageNet~\cite{deng2009imagenet} at different resolutions. Samples for ImageNet $512\times 512$ are selected while others are randomly presented.}
%     \label{fig:visual-results-on-different-datasets}
% \end{figure*}


% ---------------------------------
% CelebA 64
% ---------------------------------



% ----------------
% best large model: https://ml.azure.com/experiments/id/1638a26d-3efa-4da0-a276-5e6a44ee21c4/runs/generation-324105c3-diffusion-eval_jan18_ldm32_beit_large_layer24_lr1e-4_099_099_img64_pred_x0_cosine_droplabel015_nowd__min_SNR_5__fp16_bs16x16_ft1M_0_4M_bs8x32_3_79M-8f0465d8?wsid=/subscriptions/db9fc1d1-b44e-45a8-902d-8c766c255568/resourcegroups/ussclowpriv100/workspaces/ussclowpriv100ws&tid=72f988bf-86f1-41af-91ab-2d7cd011db47#outputsAndLogs
% ----------------