

In this section, we first provide an overview of the experimental setup. Subsequently, we conduct comprehensive ablation studies to show that our method is versatile and suitable for various prediction targets and network architectures. Finally, we compare our approach to the state-of-the-art methods across multiple image generation benchmarks, demonstrating not only its accelerated convergence but also its superior capability in generating high-quality images.



\subsection{Setup}\label{sec:implementation}

\noindent \textbf{Datasets.}
We perform experiments on both unconditional and conditional image generation using the CelebA dataset~\cite{liu2015CelebA} and the ImageNet dataset~\cite{deng2009imagenet}. 
The CelebA dataset, which comprises 162,770 human faces, is a widely-used resource for unconditional image generation studies. 
We follow ScoreSDE~\cite{2021Scoresde} for data pre-processing, which involves center cropping each image to a resolution of $140\times 140$ and then resizing it to $64\times 64$. 
For the class conditional image generation, we adopt the ImageNet dataset~\cite{deng2009imagenet} with a total of 1.3 million images from 1000 different classes. We test the performance on both $64\times 64$ and $256\times 256$ resolutions.

\noindent \textbf{Training Details.}\label{sec:training-detail}
For low resolution ($64\times 64$) image generation, we follow ADM~\cite{dhariwal2021adm} and directly train the diffusion model on the pixel-level. 
For high-resolution image generation, we utilize LDM~\cite{rombach2022ldm} approach by first compressing the images into latent space, then training a diffusion model to model the latent distributions. 
To obtain the latent for images, we employ VQ-VAE from Stable Diffusion\footnote{https://huggingface.co/stabilityai/sd-vae-ft-mse-original}, which encodes a high-resolution image ($256\times 256\times 3$) into $32\times 32 \times 4$ latent codes.

In our experiments, we employ both ViT and UNet as our diffusion model backbones. We adopt a vanilla ViT structure without any modifications~\cite{vit} as our default setting. we incorporate the timestep $t$ and class condition $\mathbf{c}$ as learnable input tokens to the model. 
Although further customization of the network structure may improve performance, our focus in this paper is to analyze the general properties of diffusion models. For the UNet structure, we follow ADM~\cite{dhariwal2021adm} and keep the FLOPs similar to the ViT-B model, which has $1.5\times$ parameters. Additional details can be found in the appendix.

For the diffusion settings, we use a cosine noise scheduler following the approach in~\cite{nichol2021iddpm,dhariwal2021adm}. The total number of timesteps is standardized to $T=1000$ across all datasets. We adopt AdamW~\cite{2014Adam,2018adamw} as our optimizer. 
For the CelebA dataset, we train our model for 500K iterations with a batch size of 128. During the first 5,000 iterations, we implement a linear warm-up and keep the learning rate at $1\times 10^{-4}$ for the remaining training. 
For the ImageNet dataset, the default learning rate is fixed at $1\times 10^{-4}$. 
The batch size is set to $1024$ for $64^2$ resolution and $256$ for $256^2$ resolution. 







\noindent \textbf{Evaluation Settings.}\label{sec:evaluation}
To evaluate the performance of our models, we utilize an Exponential Moving Average (EMA) model with a rate of 0.9999. 
During the evaluation phase, we generate images with the Heun sampler from EDM~\cite{karras2022edm}. For conditional image generation, we also implement the classifier-free sampling strategy~\cite{ho2021classifierfree} to achieve better results. Finally, we measure the quality of the generated images using the FID score calculated on 50K images.







\subsection{Analysis of the Proposed Min-SNR-$\gamma$ }\label{sec:abalation}


\begin{figure*}[!h]
    \centering
    \includegraphics[width=0.33\textwidth]{figs/abl-x0-vit.pdf}
    \includegraphics[width=0.33\textwidth]{figs/abl-noise-vit.pdf}
    \includegraphics[width=0.33\textwidth]{figs/abl-v-vit.pdf}
    \vspace{-0.8cm}
    \caption{Comparing different loss weighting designs on predicting $\mathbf{x}_0$, $\mathbf{\epsilon}$, $\mathbf{v}$. Taking the neural network output as noise with const or Max-SNR-$\gamma$ strategy lead to divergence. Min-SNR-$\gamma$ strategy converges the fastest under all these settings.
    }
    \label{fig:abl-in256}
\end{figure*}


\begin{figure*}[!h]
    \centering
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/loss_0-100.pdf}
    \end{subfigure}
    \hspace{-2mm}
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/loss_200-300.pdf}
    \end{subfigure}
    \hspace{-2mm}
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/loss_600-700.pdf}
    \end{subfigure}
    \hspace{-2mm}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/loss_800-900.pdf}
    \end{subfigure}
    \vspace{-0.3cm}
    \caption{
    Unweighted loss in different ranges of timesteps. From left to right, each figure represents a specific range of timesteps: $[0, 100), [200, 300), [600, 700), [800, 900)$. The $y$-axis represents the Mean Squared Error (MSE), averaged over each range of timesteps.}
    \label{fig:viz-loss-diff-schedule}
\end{figure*}


\noindent \textbf{Comparison of Different Weighting Strategies.}
To demonstrate the significance of the loss weighting strategy, we conduct experiments with different loss weight settings for predicting $\mathbf{x}_0$. 
These settings include: 
1) constant weighting, where $w_t = 1$, 
2) SNR weighting, with $w_t = \text{SNR}(t)$, 
3) truncated SNR weighting, with $w_t = \max\{ \text{SNR}(t), \gamma \}$ (following~\cite{salimans2022distillprogressive} with a set value of $\gamma=1$), and 4) our proposed Min-SNR-$\gamma$ weighting strategy, with $w_t = \min\{ \text{SNR}(t), \gamma \}$, we set $\gamma=5$ as the default value.

The ViT-B serves as our default backbone and experiments are performed on ImageNet $256\times 256$. As illustrated in Figure~\ref{fig:abl-in256}, we observe that all results improve as the number of training iterations increases. However, our method demonstrates a significantly faster convergence compared to other methods. Specifically, it exhibits a $3.4\times$ speedup in reaching an FID score of $10$. It is worth mentioning that the SNR weighting strategy performed the worst, which could be due to its disproportionate focus on less noisy stages.


For a deeper understanding of the reasons behind the varying convergence rates, we analyzed their training loss at different noise levels. For a fair comparison, we exclude the loss weight term by only calculating $\lVert \mathbf{x}_0 - \mathbf{\hat{x}_{\theta}} \rVert_2^2$. 
Considering that the loss of different noise levels varies greatly, we calculate the loss in different bins and present the results in Figure~\ref{fig:viz-loss-diff-schedule}. 
The results show that while the constant weighting strategy is effective for high noise intensities, it performs poorly at low noise intensities. Conversely, the SNR weighting strategy exhibits the opposite behavior. 
In contrast, our proposed Min-SNR-$\gamma$ strategy achieves a lower training loss across all cases, and indicates quicker convergence through the FID metric.




Furthermore, we present visual results in Figure~\ref{fig:abl-loss-viz} to demonstrate the fast convergence of the Min-SNR-$\gamma$ strategy. 
We apply the same random seed for noise to sample images from training iteration 50K, 200K, 400K, and 1M with different loss weight settings. Our results show that the Min-SNR-$\gamma$ strategy generates a clear object with only 200K iterations, which is significantly better in quality than the results obtained by other methods.




\begin{figure*}[!h]
    \centering
    \includegraphics[width=\textwidth]{figs/abl-viz.pdf}
    \caption{Qualitative comparison of the generation results from different weighting strategies on ImageNet-256 dataset. Images in each column are sampled from 50K, 200K, 400K, and 1M iterations. Our Min-SNR-5 strategy yields significant improvements in visual fidelity from the same iteration.}
    \label{fig:abl-loss-viz}
    \vspace{-5mm}
\end{figure*}


\noindent \textbf{Min-SNR-$\gamma$ for Different Prediction Targets.}
Instead of predicting the original signal $\mathbf{x}_0$ from the network, some recent works have employed alternative re-parameterizations, such as predicting noise $\epsilon$, or velocity $\mathbf{v}$~\cite{salimans2022distillprogressive}. To verify the applicability of our weighting strategy to these prediction targets, we conduct experiments comparing the four aforementioned weighting strategies across these different re-parameterizations. 

As we discussed in Section~\ref{sec:different-loss-strategy}, predicting noise $\epsilon$ is mathematically equivalent to predicting $\mathbf{x}_0$ by intrinsically involving Signal-to-Noise Ratio as a weight factor, thus we divide the SNR term in practice. For example, the Min-SNR-$\gamma$ strategy in predicting noise can be expressed as $w_t = \frac{\min\{ \text{SNR}(t), \gamma \}}{\text{SNR}(t)} = \min\{ \frac{\gamma}{\text{SNR}(t)}, 1 \}$. And the SNR strategy in predicting noise is equivalent to a ``constant strategy''. For simplicity and consistency, we still refer to them as Min-SNR-$\gamma$ and SNR strategies. Similarly, we can derive that when predicting velocity $\mathbf{v}$, the loss weight factor must be divided by $(\text{SNR}+1)$. These strategies are still referred to by their original names for ease of reference.

We conduct experiments on these two variants and present the results in Figure~\ref{fig:abl-in256}. Taking the neural network output as noise with const or Max-SNR-$\gamma$ setting leads to divergence. Meanwhile, our proposed Min-SNR-$\gamma$ strategy converges faster than other loss weighting strategies for both prediction noise and predicting velocity. These demonstrate that balancing the loss weights for different timesteps is intrinsic, independent of any re-parameterization.






\noindent \textbf{Min-SNR-$\gamma$ on Different Network Architectures.}
The Min-SNR-$\gamma$ strategy is versatile and robust for different prediction targets and network structures. We conduct experiments on the widely used UNet and keep the number of parameters close to the ViT-B model. For each experiment, models were trained for 1 million iterations and their FID scores were calculated at multiple intervals. The results in Table~\ref{tab:abl-unet} indicate that the Min-SNR-$\gamma$ strategy converges significantly faster than the baseline and provides better performance for both predicting $\mathbf{x}_0$ and predicting noise.


\begin{table}[t]
\begin{center}
\setlength{\tabcolsep}{1.5mm}
\begin{tabular}{l c c c c c}
\toprule
Training Iterations & 200K & 400K & 600K & 800K & 1M \\
\midrule
Baseline ($\mathbf{x}_0$) & 25.93 & 15.41 & 11.54 & 9.52 & 8.33 \\
+ Min-SNR-$5$ & \textbf{7.99} & \textbf{5.34} & \textbf{4.69} & \textbf{4.41} & \textbf{4.28} \\
\midrule
Baseline ($\epsilon$) & 8.55 & 5.43 & 4.64 & 4.35 & 4.21 \\
+ Min-SNR-$5$ & \textbf{7.32} & \textbf{4.98} & \textbf{4.48} & \textbf{4.24} & \textbf{4.14} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-5mm}
\caption{
Ablation studies on the UNet backbone. Whether the network predicts $\mathbf{x}_0$ or $\epsilon$, the Min-SNR-5 weighting design converges faster and achieves better FID score.
}
\label{tab:abl-unet}
\vspace{-5mm}
\end{table}





\noindent \textbf{Robustness Analysis.}
Our approach utilizes a single hyperparameter, $\gamma$, as the truncate value. To assess its robustness, we conducted a thorough robustness analysis in various settings. Our experiments were performed on the ImageNet-256 dataset using the ViT-B model and the prediction target of the network is $\mathbf{x}_0$. We varied the truncate value $\gamma$ by setting it to 1, 5, 10, and 20 and evaluated their performance. The results are shown in Table~\ref{tab:abl-diff-k}. We find there are only minor variations in the FID score when $\gamma$ is smaller than 20. Additionally, we conducted more experiments by modifying the predicting target to the noise $\epsilon$, and modifying the network structure to UNet. We find that the results were also consistently stable. Our results indicate that good performance can usually be achieved when $\gamma$ is set to 5, making it the established default setting.

\begin{table}[t]
\begin{center}
\setlength{\tabcolsep}{2mm}
\begin{tabular}{l c c c c }
\toprule
$\gamma$ & 1 & 5 & 10 & 20  \\
\midrule
ViT ($\mathbf{x}_0$) & 4.98 & \textbf{4.92} & 5.34 & 5.45  \\
ViT ($\epsilon$)  & 4.89 & \textbf{4.84} & 4.94 & 5.41  \\
UNet ($\mathbf{x}_0$) & 4.49 & \textbf{4.28} & 4.32 & 4.37  \\
UNet ($\epsilon$) & 4.30 & 4.14 & 4.14 & \textbf{4.12}  \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-5mm}
\caption{ 
Ablation study on $\gamma$.
The results are robust to the hyper-parameter $\gamma$ in different settings.}
\label{tab:abl-diff-k}
\vspace{-3mm}
\end{table}


\begin{table}[t]
\begin{center}
\begin{tabular}{l c c}
\toprule
Method & \#Params & FID \\
\midrule
DDIM~\cite{ddim} & 79M & 3.26 \\
Soft Truncation~\cite{ddim} & 62M & 1.90 \\
\textbf{Our UNet} & 59M & \textbf{1.60} \\
\midrule
U-ViT-Small~\cite{bao2022uvit} & 44M &   2.87 \\
\textbf{ViT-Small (ours)} & 43M &   \textbf{2.14} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-5mm}
\caption{FID results of unconditional image generation on CelebA $64\times 64$~\cite{liu2015CelebA}. 
We conduct experiments with both UNet and ViT backbone.
}
\label{tab:CelebA64}
\vspace{-3mm}
\end{table}







\subsection{Comparison with state-of-the-art Methods}\label{sec:compare-with-sota}
\noindent\textbf{CelebA-64.}
We conduct experiments on the CelebA $64\times 64$ dataset for unconditional image generation. Both UNet and ViT are used as our backbones and are trained for 500K iterations. During the evaluation, we use the EDM sampler~\cite{karras2022edm} to generate 50K samples and calculate the FID score. The results are summarized in Table~\ref{tab:CelebA64}. Our ViT-Small~\cite{vit} model outperforms previous ViT-based models with an FID score of 2.14. It is worth mentioning that no modifications are made to the naive network structure, demonstrating that the results could still be improved further. Meanwhile, our method using the UNet~\cite{dhariwal2021adm} structure achieves an even better FID score of 1.60, outperforming previous UNet methods.

\noindent\textbf{ImageNet-64.}
We also validate our method on class-conditional image generation on the ImageNet $64\times 64$ dataset. 
During training, the class label is dropped with the probability $0.15$ for classifier-free inference~\cite{ho2021classifierfree}. 
The model is trained for 800K iterations and images are synthesized using classifier-free guidance with a scale of $\text{cfg}=1.5$ and the EDM sampler for image generation. 
For a fair comparison, we adopt a 21-layer ViT-Large model without additional architecture designs, which has a similar number of parameters to U-ViT-Large~\cite{bao2022uvit}. 
The results presented in Table~\ref{tab:in64} show that our method achieves an FID score of 2.28, significantly improving upon the U-ViT-Large model. 



\begin{table}[t]
\begin{center}
\begin{tabular}{l c c }
\toprule
Method & \#Params & FID \\
\midrule
BigGAN-deep~\cite{brock2018biggan} & {} & 4.06 \\
StyleGAN-XL~\cite{sauer2022styleganxl} & {} & 1.51 \\
\midrule
IDDPM (small)~\cite{nichol2021iddpm} & 100M & 6.92 \\
IDDPM (large)~\cite{nichol2021iddpm} & 270M &  2.92 \\
CDM~\cite{ho2022cdm} & {} &  1.48 \\
ADM~\cite{dhariwal2021adm} & 296M &  2.61 \\
\midrule
U-ViT-Mid~\cite{bao2022uvit} & 131M &   5.85 \\
U-ViT-Large~\cite{bao2022uvit} & 287M &   4.26 \\
\textbf{ViT-L (ours)} & 269M &   \textbf{2.28} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-5mm}
\caption{
FID results on ImageNet $64\times 64$. We conduct experiments using the ViT-L backbone which significantly improves upon previous methods.}
\label{tab:in64}
\end{table}


\begin{table}[t]
\begin{center}
\begin{tabular}{ l c c }
\toprule
Method & \#Params & FID \\
\midrule
BigGAN-deep~\cite{brock2018biggan} & 340M & 6.95 \\
StyleGAN-XL~\cite{sauer2022styleganxl} & {} & 2.30 \\
\midrule
Improved VQ-Diffusion~\cite{gu2022vqdiffusion} & 460M & 4.83 \\
\midrule
IDDPM~\cite{nichol2021iddpm} & 270M &  12.26 \\
CDM~\cite{ho2022cdm} & {} &  4.88 \\
ADM~\cite{dhariwal2021adm} & 554M &  10.94 \\
ADM-U~\cite{dhariwal2021adm} & 608M &  7.49 \\
ADM-G~\cite{dhariwal2021adm} & 554M &  4.59 \\
ADM-U, ADM-G~\cite{dhariwal2021adm} & 608M &  3.94 \\
LDM~\cite{rombach2022ldm} & 400M & 3.60 \\
\textbf{UNet (ours)} & 395M & \textbf{2.81}$^\dagger$ \\
\midrule
U-ViT-L~\cite{bao2022uvit} & 287M &   3.40 \\
DiT-XL-2~\cite{peebles2022dit} & 675M &   9.62 \\
DiT-XL-2 (cfg=1.50)~\cite{peebles2022dit}  & 675M &   2.27 \\
{ViT-XL (ours)} & 451M &   8.10 \\
\textbf{ViT-XL (ours, cfg=1.50)} & 451M &   \textbf{2.06} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-5mm}
\caption{
FID results on ImageNet $256\times 256$. $^\dagger$ denotes only train 1.4M iterations. Our model with a ViT-XL backbone achieves a new record FID score of 2.06. 
}
\label{tab:in256}
\vspace{-3mm}
\end{table}


\noindent\textbf{ImageNet-256.}
We also apply diffusion models for higher-resolution image generation on the ImageNet $256\times 256$ benchmark. To enhance training efficiency, we first compress $256\times 256 \times 3$ images into $32\times 32 \times 4$ latent codes using the encoder from LDM~\cite{rombach2022ldm}. During the sampling process, we employ the EDM sampler and the classifier-free guidance to generate images. 
The FID comparison is presented in Table~\ref{tab:in256}. 
Under the setting of predicting $\epsilon$ with Min-SNR-5, our ViT-XL model achieves the FID of $2.08$ for only 2.1M iterations, which is $3.3\times$ faster than DiT and outperforms the previous state-of-the-art FID record of $2.27$. 
Moreover, with longer training (about 7M iterations as in~\cite{peebles2022dit}), we are able to achieve the FID score of 2.06 by predicting $\mathbf{x}_0$ with Min-SNR-5.
Our UNet-based model with 395M parameters is trained for about 1.4M iterations and achieves FID score of 2.81.








