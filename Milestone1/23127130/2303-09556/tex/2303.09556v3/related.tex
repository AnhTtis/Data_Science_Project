\noindent\textbf{Denoising Diffusion Models.}
 Diffusion models~\cite{ho2020ddpm, song2019generative,dhariwal2021adm} are strong generative models, particularly in the field of image generation, due to their ability to model complex distributions. This advantage has led to superiority over previous GAN models in terms of both high-fidelity and diversity of generated images~\cite{dhariwal2021adm, karras2022edm, Nichol2021glide, ramesh2022dalle2, rombach2022ldm, saharia2022imagen}. Besides, diffusion models also show great success in text-to-video generation~\cite{Ho2022imagenvideo,Singer2022makeavideo,villegas2022phenaki}, 3D Avatar generation~\cite{poole2022dreamfusion,wang2022rodin}, image to image translation~\cite{parmar2023zeroI2I}, image manipulation~\cite{brooks2022instructpix2pix,kim2021diffusionclip}, music generation~\cite{huang2023noise2music}, and even drug discovery~\cite{xu2022geodiff}. The most widely used network structure for diffusion models in the field of image generation is UNet~\cite{ho2020ddpm, dhariwal2021adm, Nichol2021glide, nichol2021iddpm}. Recently, researchers have also explored the use of Vision Transformers~\cite{vit} as an alternative, with U-ViT~\cite{bao2022uvit} borrowing the skip connection design from UNet~\cite{unet} and DiT~\cite{peebles2022dit} leveraging Adaptive LayerNorm and discovering that the zero initialization strategy is critical for achieving state-of-the-art class-conditional ImageNet generation results.







\noindent\textbf{Improved Diffusion Models.} 
Recent studies have tried to improve the diffusion models from different perspectives. Some works aim to improve the quality of generated images by guiding the sampling process~\cite{dhariwal2021diffusion,ho2022classifier}. Other studies propose fast sampling methods that require only a dozen steps~\cite{ddim, Liu2022PNMD, Lu2022DPMSolverAF, karras2022edm} to generating high-quality images. Some works have further distilled the diffusion models for even fewer steps in the sampling process~\cite{salimans2022distillprogressive,meng2022ondistillation}. Meanwhile, some researchers~\cite{ho2020ddpm,karras2022edm,chen2023importance} have noticed that the noise schedule is important for diffusion models. Other works~\cite{nichol2021iddpm, salimans2022distillprogressive} have found that different predicting targets from denoising networks affect the training stability and final performance. Finally, some works~\cite{feng2022ernie2,balaji2022eDiff-I} have proposed using the Mixture of Experts (MoE) approach to handle noise from different levels, which can boost the performance of diffusion models, but require a larger number of parameters and longer training time.






\noindent\textbf{Multi-task Learning.} 
The goal of Multi-task learning (MTL) is to learn multiple related tasks jointly so that the knowledge contained in a task can be leveraged by other tasks. One of the main challenges in MTL is negative transfer~\cite{crawshaw2020multi}, means the joint training of tasks hurts learning instead of helping it. From an optimization perspective, it manifests as the presence of conflicting task gradients. To address this issue, some previous works~\cite{yu2020pcgrad,wang2020gradvac,chen2020graddrop} try to modulate the gradient to prevent conflicts. Meanwhile, other works attempt to balance different tasks through carefully design the loss weights~\cite{chen2018gradnorm,kendall2018multi}. GradNorm~\cite{chen2018gradnorm} considers loss weight as learnable parameters and updates them through gradient descent. Another approach MTO~\cite{desideri2012mgda,sener2018mto} regards the multi-task learning problem as a multi-objective optimization problem and obtains the loss weights by solving a quadratic programming problem. 
