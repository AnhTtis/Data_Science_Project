In this paper, we point out that the conflicting optimization directions between different timesteps may cause slow convergence in diffusion training. To address it, we regard the diffusion training process as a multi-task learning problem and introduce a novel weighting strategy, named Min-SNR-$\gamma$, to effectively balance different timesteps. Experiments demonstrate our method can boost diffusion training several times faster, and achieves the state-of-the-art FID score on ImageNet-256 dataset.

% we analyze the training loss and gradient norm of diffusion from the perspective of multi-task learning.
% We find that the solutions that better approximate the Pareto optimality could obtain better convergence and performance.
% Based on the analysis, we present a simple yet effective loss weight design for training diffusion models.
% It can help diffusion models converge faster and achieve better results.
% We conduct extensive experiments to validate the effectiveness of the design and we achieve the state-of-the-art FID score on ImageNet $256\times 256$.
% We hope our analysis and simple loss weighting design could shed light on training of diffusion models.
