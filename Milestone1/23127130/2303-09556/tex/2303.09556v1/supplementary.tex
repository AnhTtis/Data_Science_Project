% \documentclass[10pt,twocolumn,letterpaper]{article}

% \usepackage{iccv}
% \usepackage{times}
% \usepackage{epsfig}
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}

% \usepackage{caption}
% \usepackage{subcaption}
% \usepackage{booktabs}

% % Include other packages here, before hyperref.

% % If you comment hyperref and then uncomment it, you should delete
% % egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% % run, let it finish, and you should be clear).
% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
% \newtheorem{proposition}{Proposition}
% \newtheorem{definition}{Definition}
% \newtheorem{theorem}{Theorem}
% % \iccvfinalcopy % *** Uncomment this line for the final submission

% \def\iccvPaperID{5819} % *** Enter the ICCV Paper ID here
% \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% % Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

% \begin{document}
% \title{Efficient Diffusion Training via Min-SNR Weighting Strategy \\
% appendix
% }

% \maketitle

In the appendix, we first provide the proof of Theorem 1 in Section~\ref{sec:ugd}.
Then we derive the relationship between loss weights of different predicting targets in Section~\ref{sec:transformation}. 
% We list the licenses of all used datasets in Section~\ref{sec:datasets}.
In Section~\ref{sec:hyper-parameter}, we provide more details on the network architecture, training and sampling settings. Finally, we present more visual results in Section~\ref{sec:addtional-results}.


\section{Proof for Theorem 1}~\label{sec:ugd}
% Our goal is avoid conflicts between different tasks.
First, we introduce the Pareto Optimality mentioned in the paper.
Assume the loss for each task is $\mathcal{L}^t(\theta), t \in \{ 1, 2,\ldots, T\}$ and the respective gradient to $\theta$ is $\nabla_\theta \mathcal{L}^t(\theta)$.
For simplicity, we denote $\mathcal{L}^t(\theta)$ as $\mathcal{L}^t$.
%% translate from Jianlin Su' blog.
If we treat each task with equal importance, we assume each loss item $\mathcal{L}^1, \mathcal{L}^2, \ldots, \mathcal{L}^T$ is decreasing or kept the same.
There exists one point $\theta^*$ where any change of the point will leads to the increase of one loss item.
We call the point $\theta^*$ ``Pareto Optimality''.
In other words, we cannot sacrifice one task for another task's improvement. 
% If the Taylor expansion in Equation {\color{red} 7} is satisfied,
To reach Pareto Optimality, we need to find an update direction $\delta$ which meet:
\begin{align}
    \left\{ 
    \begin{array}{cc}
         \left\langle \nabla_\theta \mathcal{L}_{\theta}^1, \delta \right\rangle & \leq 0  \\
         \left\langle \nabla_\theta \mathcal{L}_{\theta}^2, \delta \right\rangle & \leq 0  \\
         \vdots & {}  \\
         \left\langle \nabla_\theta \mathcal{L}_{\theta}^T, \delta \right\rangle & \leq 0  \\
    \end{array}
    \right.
\end{align}
$\left\langle \cdot,\cdot \right\rangle$ denotes the inner product of two vectors.
It is worth noting that $\delta=0$ satisfies all the above inequalities.
We care more about the non-zero solution and adopt it for updating the network parameter $\theta$. 
If the non-zero point does not exist, it may already achieve the ``Pareto Optimality'', which is referred as ``Pareto Stationary''.

For simplicity, we denote the gradient for each loss item $\nabla_\theta\mathcal{L}^t$ as $\mathbf{g}_t$. Suppose we have a gradient vector $\mathbf{u}$ to satisfy that all $\left\langle \mathbf{g}_t, \mathbf{u} \right\rangle \geq 0, t\in \{1,2,\ldots,T \}$. 
Then $-\mathbf{u}$ is the updating direction ensuring a lower loss for each task.

% Next, we demonstrate how to formalize and solve this problem.
% As proposed in~\cite{kexuefm-8896}, we formulate the problem as the optimization of a weighted sum of gradients and solve it using unconstrained gradient descent.
% Here we denote the gradient for each loss item $\nabla_\theta\mathcal{L}^t$ as $\mathbf{g}_t$.
% We want to find one gradient vector $\mathbf{u}$ to ensure all $\left\langle \mathbf{g}_t, \mathbf{u} \right\rangle \geq 0, t\in \{1,2,\ldots,T \}$. 
% Then we can update the parameters through the direction $-\mathbf{u}$.

As proposed in~\cite{kexuefm8896}, $\left\langle \mathbf{g}_t, \mathbf{u} \right\rangle \geq 0, \forall t \in \{ 1, 2, \ldots, T \}$ is equivalent to $\min_t \left\langle \mathbf{g}_t, \mathbf{u} \right\rangle \geq 0$.
% Our goal is to find the solution that maximizes the minimal $\left\langle \mathbf{g}_t, \mathbf{u} \right\rangle$.
% Then the problem is transformed to a ``maxmin'' problem.
And it could be achieved when the minimal value of $\left\langle \mathbf{g}_t, \mathbf{u} \right\rangle$ is maximized.
Thus the problem is further converted to:
\begin{align*}
    \max_{\mathbf{u}} \min_t \left\langle \mathbf{g}_t, \mathbf{u} \right\rangle
\end{align*}
There is no constraint for the vector $\mathbf{u}$, so it may become infinity and make the updating unstable.
To avoid it, we add a regularization term to it
\begin{align}\label{eq:objective-function}
    \max_{\mathbf{u}} \min_t \left\langle \mathbf{g}_t, \mathbf{u} \right\rangle - \frac{1}{2}\lVert \mathbf{u}  \rVert_2^2.
\end{align}
And notice that the $\max$ function ensures the value is always greater than or equal to a specific value $\mathbf{u}=0$. 
\begin{align*}
 &\max_{\mathbf{u}} \min_t \left\langle \mathbf{g}_t, \mathbf{u} \right\rangle - \frac{1}{2}\lVert \mathbf{u}  \rVert_2^2 \\
 \geq   &\left.  \min_t \left\langle \mathbf{g}_t, \mathbf{u}
 \right\rangle - \frac{1}{2}\lVert \mathbf{u}  \rVert_2^2 \right| _{\mathbf{u}=0} \\
 =&~ 0,
 \end{align*}
 which also means $\max_{\mathbf{u}} \min_t \left\langle \mathbf{g}_t, \mathbf{u} \right\rangle \geq \frac{1}{2}\lVert \mathbf{u}  \rVert_2^2 \geq 0 $. Therefore, the solution of Equation~\ref{eq:objective-function} satisfies our optimization goal of $\left\langle \mathbf{g}_t, \mathbf{u} \right\rangle \geq 0, \forall t \in \{ 1, 2, \ldots, T \}$.
 %The equality is achieved when $\mathbf{u}$ equals 0.

We define $\mathcal{C}^T$ as a set of $n$-dimensional variables
\begin{align}
    \mathcal{C}^T = \left\{ (w_1, w_2, \ldots, w_T)  | w_1, w_2, \ldots, w_T \geq 0, \sum_{t=1}^T w_t = 1 \right\},
\end{align}
% which is a convex set. 
It is easy to verify that 
\begin{align}
    \min_t \left\langle \mathbf{g}_t,  \mathbf{u}\right\rangle =  \min_{w \in \mathcal{C}^T} \left\langle \sum_t w_t  \mathbf{g}_t, \mathbf{u} \right\rangle.
\end{align}



% We can find that
% \begin{align*}
%     \left\langle \sum_t \alpha_t \mathbf{g}_t, \mathbf{u} \right\rangle &=  \sum_t \alpha_t \left\langle \mathbf{g}_t, \mathbf{u} \right\rangle \\
%     &\geq  \sum_t \alpha_t \left(\min_j \left\langle \mathbf{g}_j,  \mathbf{u}\right\rangle \right) \\
%     &= \left(\min_j \left\langle \mathbf{g}_j,  \mathbf{u}\right\rangle \right) \sum_t \alpha_t \\
%     &= \min_j \left\langle \mathbf{g}_j,  \mathbf{u}\right\rangle.
% \end{align*}
% The equality can be achieved when 
% \begin{align*}
% \sum_t \alpha_t \left\langle \mathbf{g}_t, \mathbf{u} \right\rangle - \sum_t \alpha_t \left(\min_j \left\langle \mathbf{g}_j,  \mathbf{u}\right\rangle \right) &= 0, \\
% \sum_t \alpha_t \left(\left\langle \mathbf{g}_t, \mathbf{u} \right\rangle - \min_j \left\langle \mathbf{g}_j,  \mathbf{u}\right\rangle \right) &= 0,
% \end{align*}
% because $\forall i \in \{ 1,2,\ldots,n \}, \alpha \geq 0$ and $\left\langle \mathbf{g}_t, \mathbf{u} \right\rangle - \min_j \left\langle \mathbf{g}_j,  \mathbf{u}\right\rangle \geq 0$.
% The above equation holds when $\forall i \in \{ 1, 2, \ldots, n\}, \alpha_t \left( \left\langle \mathbf{g}_t, \mathbf{u} \right\rangle - \min_j \left\langle \mathbf{g}_j,  \mathbf{u}\right\rangle = 0 \right)$.
% % ``Pareto Optimality'' means a solution $\theta^*$

% So we can get 
% \begin{align*}
%     \min_t \left\langle \mathbf{g}_t,  \mathbf{u}\right\rangle =  \min_{w \in \mathcal{C}^T} \left\langle \sum_t w_t  \mathbf{g}_t, \mathbf{u} \right\rangle
% \end{align*}

We can also verify the above function is concave with respect to $\mathbf{u}$ and $\alpha$. According to Von Neumann's Minmax theorem~\cite{von1947theory}, the objective with regularization in Equation~\ref{eq:objective-function} is equivalent to 
\begin{align}
    & \max_{\mathbf{u}} \min_{w \in \mathcal{C}^T} \left\{\left\langle \sum_t w_t  \mathbf{g}_t, \mathbf{u} \right\rangle - \frac{1}{2}\lVert \mathbf{u}  \rVert_2^2\right\} \\
    = & \min_{w \in \mathcal{C}^T} \max_{\mathbf{u}} \left\{ \left\langle \sum_t w_t  \mathbf{g}_t, \mathbf{u} \right\rangle - \frac{1}{2}\lVert \mathbf{u}  \rVert_2^2 \right\} \\
    % = & \left. \min_{w \in \mathcal{C}^T} \left\{  \left\langle \sum_t w_t  \mathbf{g}_t, \mathbf{u} \right\rangle - \frac{1}{2}\lVert \mathbf{u}  \rVert_2^2 \right\} \right|_{\frac{\partial \left\langle \sum_t w_t  \mathbf{g}_t, \mathbf{u} \right\rangle - \frac{1}{2}\lVert \mathbf{u}  \rVert_2^2}{\partial \mathbf{u}}=0 } \\
    = & \left. \min_{w \in \mathcal{C}^T} \left\{  \left\langle \sum_t w_t  \mathbf{g}_t, \mathbf{u} \right\rangle - \frac{1}{2}\lVert \mathbf{u}  \rVert_2^2 \right\} \right|_{\mathbf{u}=\frac{1}{2} \sum_t w_t  \mathbf{g}_t} \\
    = & \min_{w \in \mathcal{C}^T} \frac{1}{2} \left\lVert \sum_t w_t  \mathbf{g}_t \right\rVert_2^2.
\end{align}
Finally, we achieved Theorem 1 in the main paper.
% Then the problem is formulated as the optimization of a weighted sum of gradients.
% To optimize the above objective with constraint $\sum_t \alpha_t = 1, \alpha_t \geq 0$, we re-parameterize the variable $\alpha_t$ through
% \begin{align*}
%     \alpha_t = \frac{e^{\beta_t}}{Z}, Z = \sum_t e^{\beta_t}, \beta_t \in \mathbb{R},
% \end{align*}
% Then the constrained optimization problem becomes the unconstrained one as in the paper.
% We obtain the value through simple gradient descent.

% \cite{kexuefm-8896}

\section{Relationship between Different Targets}\label{sec:transformation}
The most common predicting target is in $\epsilon$-space.
Loss for prediction in $\mathbf{x}_0$-space and $\epsilon$-space can be transformed by the SNR loss weight.
\begin{align*}
    \mathcal{L}_\theta &= \left\lVert 
\epsilon - \hat{\epsilon}_\theta (\mathbf{x}_t)  \right\rVert_2^2 \\
&= \left\lVert \frac{1}{\sigma_t} (\mathbf{x}_t - \alpha_t \mathbf{x}_0 ) - \frac{1}{\sigma_t} (\mathbf{x}_t - \alpha_t \hat{\mathbf{x}}_\theta (\mathbf{x}_t)) \right\rVert_2^2 \\
&= \frac{\alpha_t^2}{\sigma_t^2} \left\lVert 
\mathbf{x}_0 - \hat{\mathbf{x}}_\theta (\mathbf{x}_t))  \right\rVert_2^2 \\
&= \text{SNR}(t) \left\lVert 
\mathbf{x}_0 - \hat{\mathbf{x}}_\theta (\mathbf{x}_t))  \right\rVert_2^2,
\end{align*}
where $\hat{\epsilon}_\theta$ is the network to predict the noise and $ \hat{\mathbf{x}}_\theta$ is to predict the clean data.

Prediction target $\mathbf{v} = \alpha_t \epsilon - \sigma_t \mathbf{x}_0$ is proposed in~\cite{salimans2022distillprogressive}, we can derive the related loss
% \begin{align*}
%     \mathcal{L}_\theta &= \left\lVert 
% \epsilon - \hat{\epsilon}_\theta (\mathbf{x}_t)  \right\rVert_2^2 \\
% &= \left\lVert \frac{1}{\alpha}_t(\mathbf{v}_t + \sigma_t \mathbf{x}_0) - \frac{1}{\alpha}_t(\mathbf{v}_\theta (\mathbf{x}_t) + \sigma_t \mathbf{x}_0) \right\rVert_2^2 \\
% &= \frac{1}{\alpha_t^2}\left\lVert \mathbf{v} - \mathbf{v}_\theta (\mathbf{x}_t)\right\rVert_2^2 \\
% &= \frac{\alpha_t^2 + \sigma_t^2}{\alpha_t^2}\left\lVert \mathbf{v} - \mathbf{v}_\theta (\mathbf{x}_t)\right\rVert_2^2 \\
% &= \frac{\text{SNR}(t) + 1}{\text{SNR}(t)}\left\lVert \mathbf{v} - \mathbf{v}_\theta (\mathbf{x}_t)\right\rVert_2^2\\
% \left\lVert \mathbf{v} - \mathbf{v}_\theta (\mathbf{x}_t)\right\rVert_2^2 &= \frac{\text{SNR}(t)}{\text{SNR}(t) + 1} \mathcal{L}_\theta \\
% &= \frac{\text{SNR}^2(t)}{\text{SNR}(t) + 1} \left\lVert 
% \mathbf{x}_0 - \hat{\mathbf{x}}_\theta (\mathbf{x}_t))  \right\rVert_2^2
% \end{align*}
\begin{align*}
    \mathcal{L}_\theta &= \left\lVert \mathbf{v}_t - \mathbf{v}_\theta (\mathbf{x}_t) \right\rVert_2^2 \\
    &= \left\lVert\left( \alpha_t \epsilon - \sigma_t \mathbf{x}_0\right) - \left( \alpha_t \hat{\epsilon}_\theta (\mathbf{x}_t) - \sigma_t \hat{\mathbf{x}}_\theta(\mathbf{x}_t)\right) \right\rVert_2^2 \\
    &= \left\lVert\alpha_t\left(  \epsilon - \hat{\epsilon}_\theta (\mathbf{x}_t) \right)  - \sigma_t \left(   \mathbf{x}_0 -  \hat{\mathbf{x}}_\theta(\mathbf{x}_t)\right) \right\rVert_2^2 \\
    &= \left\lVert \alpha_t \frac{\alpha_t}{\sigma_t}\left( \hat{\mathbf{x}}_\theta(\mathbf{x}_t)   - \mathbf{x}_0 \right)- \sigma_t \left(   \mathbf{x}_0 -  \hat{\mathbf{x}}_\theta(\mathbf{x}_t)\right) \right\rVert_2^2 \\
    &= \left\lVert \frac{\alpha_t^2 + \sigma_t^2}{\sigma_t} \left(   \mathbf{x}_0 -  \hat{\mathbf{x}}_\theta(\mathbf{x}_t)\right) \right\rVert_2^2 \\
    &= \frac{1}{\sigma_t^2} \left\lVert  \left(   \mathbf{x}_0 -  \hat{\mathbf{x}}_\theta(\mathbf{x}_t)\right) \right\rVert_2^2 \\
    &= \frac{\alpha_t^2 + \sigma_t^2}{\sigma_t^2} \left\lVert  \left(   \mathbf{x}_0 -  \hat{\mathbf{x}}_\theta(\mathbf{x}_t)\right) \right\rVert_2^2 \\
    &= (\text{SNR}(t)+1) \left\lVert  \left(   \mathbf{x}_0 -  \hat{\mathbf{x}}_\theta(\mathbf{x}_t)\right) \right\rVert_2^2 \\
    % &= \sigma_t^2 \left\lVert \mathbf{x}_0 - \hat{\mathbf{x}}_\theta (\mathbf{x}_t) \right\rVert_2^2 \\
    % &= \frac{\sigma_t^2}{\alpha_t^2 + \sigma_t^2}\left\lVert \mathbf{x}_0 - \hat{\mathbf{x}}_\theta (\mathbf{x}_t) \right\rVert_2^2 \\
    % &= \frac{1}{1 + \text{SNR}(t)} \left\lVert \mathbf{x}_0 - \hat{\mathbf{x}}_\theta (\mathbf{x}_t) \right\rVert_2^2 
\end{align*}


% \section{Datasets}\label{sec:datasets}
% We list all the licenses of all used datasets here.
% \subsection{Licenses}
% CelebA dataset is licensed under CC BY 4.0 and can be used for research purpose.

% ImageNet is licensed under the BSD 3-Clause License. We can redistribute and modify the data for research purpose. 


\section{Hyper-parameter}\label{sec:hyper-parameter}
Here we list more details about the architecture, training and evaluation setting.

\subsection{Architecture Settings}
The ViT setting adopted in the paper are as follows,
\begin{table}[!h]
\begin{center}
\setlength{\tabcolsep}{1.5mm}
\begin{tabular}{l c c c c}
\toprule
Model & Layers & Hidden Size &  Heads & Params \\
\midrule
ViT-Small & 13 & 512 & 6 & 43M \\
ViT-Base & 12 & 768 & 12 & 88M \\
% ViT-Mid   & - & - & - & - \\
ViT-Large   & 21 & 1024 & 16 & 269M \\
ViT-XL   & 28 & 1152 & 16 & 451M \\
\bottomrule
\end{tabular}
\end{center}
\caption{
Configurations of our used ViTs.
}
\label{tab:config-vit}
\end{table}

We use ViT-Small for face generation on CelebA $64\times 64$. Besides, we adopt ViT-Base as the default backbone for the ablation study.
To make relative fair comparison with U-ViT, we use a 21-layer ViT-Large for ImageNet $64\times 64$ benchmark.
To compare with former state-of-the-art method DiT~\cite{peebles2022dit} on ImageNet $256\times 256$, we adopt the similar setting ViT-XL with the same depth, hidden size, and patch size.

In the paper, we also evaluate our method's robustness to model architectures using the UNet backbone.
For ablation study, we adjust the setting based on ADM~\cite{dhariwal2021adm} to make the parameters and FLOPs close to ViT-B. The setting is 
\begin{itemize}
    \item Base channels: 192
    \item Channel multipliers: 1, 2, 2, 2
    \item Residual blocks per resolution: 3
    \item Attention resolutions: 8, 16
    \item Attention heads: 4
\end{itemize}

We also conduct experiments with the same architecture (296M) in ADM~\cite{dhariwal2021adm} on ImageNet $64\times 64$.
After 900K training iterations with batch size 1024, it could achieve an FID score of 2.11.

For high resolution generation on ImageNet $256\times 256$.
We use the 395M setting from LDM~\cite{rombach2022ldm}, which operates on the $32\times 32 \times 4$ latent space.

\subsection{Training Settings}
The training iterations and learning rate have been reported in the paper.
We use AdamW~\cite{2018adamw,2014Adam} as our default optimizer.
$(\beta_1, \beta_2)$ is set to $(0.9,0.999)$ for UNet backbone.
Following~\cite{bao2022uvit}, we set $(\beta_1, \beta_2)$ to $(0.99,0.99)$ for ViT backbone.

\subsection{Sampling Settings}
If not otherwise specified, we only use EDM's~\cite{karras2022edm} Heun sampler.
We only adjust the sampling steps for better results.
For ablation study with ViT-B and UNet, we set the number of steps to 30.
For ImageNet $64 \times 64$ in Table~4, the number of steps is set to 20.
For ImageNet $256\times 256$ in Table~5, the number of sampling steps is set to 50.


\section{Additional Results}\label{sec:addtional-results}

\subsection{Ablation Study on Pixel Space}
In the paper, most of the ablation study is conducted on ImageNet $256\times 256$'s latent space.
Here, we present the results on ImageNet $64\times 64$ pixel space.
We adopt a ViT-B model as our backbone and train the diffusion model for 800K iterations with batch size 512. 
Our predicting targets are $\mathbf{x}_0$ and $\epsilon$ and they are equipped with our proposed simple Min-SNR-$\gamma$ loss weight ($\gamma=5$).
We adopt the pre-trained noisy classifier at $64\times 64$ from ADM~\cite{dhariwal2021adm} as conditional guidance.
We can see that the loss weighting strategy contributes to the faster convergence for both $\mathbf{x}_0$ and $\epsilon$.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.42\textwidth]{figs/abl-in64.pdf}
    \caption{Ablate loss weight design in pixel space (ImageNet $64\times 64$). We adopt DPM Solver~\cite{Lu2022DPMSolverAF} to sample $50k$ images to calculate the FID score with classifier guidance.}
    \label{fig:abl-in64}
\end{figure}

\subsection{Visual Results on Different Datasets}
We provide additional generated results in Figure~\ref{fig:CelebA64}-\ref{fig:vit_in256}.
Figure~\ref{fig:CelebA64} shows the generated samples with UNet backbone on CelebA $64\times 64$.
Figure~\ref{fig:vit_tn64} and Figure~\ref{fig:unet_in64} demonstrate the generated samples on conditional ImageNet $64\times 64$ benchmark with ViT-Large and UNet backbone respectively.
The visual results on CelebA $64\times 64$ and ImageNet $64\times 64$ are randomly synthesized without cherry-pick.

We also present some visual results on ImageNet $256\times 256$ with our model which can achieve the FID 2.06 in Figure~\ref{fig:vit_in256}.

\begin{figure*}
    \centering
    \includegraphics[width=0.95\textwidth]{supp_figs/celeba64.png}
    \caption{Additional generated samples on CelebA $64\times 64$. The samples are from UNet backbone with 1.60 FID.}
    \label{fig:CelebA64}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.95\textwidth]{supp_figs/vit_in64.png}
    \caption{Additional generated samples on ImageNet $64\times 64$. The samples are from ViT backbone with 2.28 FID.}
    \label{fig:vit_tn64}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.95\textwidth]{supp_figs/unet_in64.png}
    \caption{Additional generated samples on ImageNet $64\times 64$. The samples are from UNet backbone with 2.14 FID.}
    \label{fig:unet_in64}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.95\textwidth]{supp_figs/tmp.png}
    \caption{Additional generated samples on ImageNet $256\times 256$. The samples are from ViT backbone with 2.06 FID.}
    \label{fig:vit_in256}
\end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=\textwidth]{supp_figs/in256_scale4.0.png}
% \end{figure*}

% \begin{figure*}[!h]
%     \centering
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/CelebA64.png}
%         \caption{CelebA $64\times 64$}
%         \label{fig:CelebA64}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/in64.png}
%         \caption{ImageNet $64\times 64$}
%         \label{fig:in64}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/in256.png}
%         \caption{ImageNet $256\times 256$}
%         \label{fig:in64}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/in512.png}
%         \caption{ImageNet $512\times 512$}
%         \label{fig:in64}
%     \end{subfigure}
%     \caption{Generated samples from different datasets: CelebA-$64\times 64$ and ImageNet~\cite{deng2009imagenet} at different resolutions. Samples for ImageNet $512\times 512$ are selected while others are randomly presented.}
%     \label{fig:visual-results-on-different-datasets}
% \end{figure*}





% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }

% \end{document}