@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@ARTICLE{Kratzert2021-ui,
  title     = "{Niederschlags-Abfluss-Modellierung} mit Long {Short-Term}
               Memory ({LSTM})",
  author    = "Kratzert, Frederik and Gauch, Martin and Nearing, Grey and
               Hochreiter, Sepp and Klotz, Daniel",
  abstract  = "ZusammenfassungMethoden der k{\"u}nstlichen Intelligenz haben
               sich in den letzten Jahren zu essenziellen Bestandteilen fast
               aller Bereiche von Wissenschaft und Technik entwickelt. Dies
               gilt auch f{\"u}r die Hydrologie: Vielschichtige neuronale
               Netzwerke -- auch bekannt als Modelle des Deep Learning --
               erm{\"o}glichen hier Vorhersagen von Niederschlagsabflussmengen
               in zuvor unerreichter Pr{\"a}zision.Dieser Beitrag beleuchtet
               das Potenzial von Deep Learning f{\"u}r wasserwirtschaftliche
               Anwendungen. Der erste Teil des Artikels zeigt, wie sogenannte
               Long Short-Term Memory-Netzwerke -- eine spezifisch f{\"u}r
               Zeitreihen entwickelte Methode des Deep Learnings -- f{\"u}r die
               Niederschlags-Abfluss-Modellierung verwendet werden, und wie
               diese f{\"u}r eine Reihe hydrologischer Probleme bessere
               Ergebnisse als jedes andere bekannte hydrologische Modell
               erzielen. Der zweite Teil demonstriert wesentliche Eigenschaften
               der Long Short-Term Memory-Netzwerke. Zum einen zeigen wir, dass
               diese Netzwerke beliebige Daten verarbeiten k{\"o}nnen. Dies
               erlaubt es, m{\"o}gliche synergetische Effekte aus
               unterschiedlichen meteorologischen Datens{\"a}tzen zu
               extrahieren und damit die Modellg{\"u}te zu verbessern. Zum
               anderen stellen wir dar, wie relevante hydrologische Prozesse
               (wie z. B. das Akkumulieren und Schmelzen von Schnee) innerhalb
               der Modelle abgebildet werden, ohne dass diese spezifisch darauf
               trainiert wurden.",
  journal   = "{\"O}sterr. Wasser- Abfallwirtsch.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  73,
  number    = "7-8",
  pages     = "270--280",
  month     =  aug,
  year      =  2021,
  copyright = "https://creativecommons.org/licenses/by/4.0",
  language  = "de"
}


@ARTICLE{Kollias2023-ur,
  title         = "{ABAW}: {Valence-Arousal} Estimation, expression
                   recognition, Action Unit Detection \& Emotional Reaction
                   Intensity Estimation Challenges",
  author        = "Kollias, Dimitrios and Tzirakis, Panagiotis and Baird, Alice
                   and Cowen, Alan and Zafeiriou, Stefanos",
  abstract      = "The fifth Affective Behavior Analysis in-the-wild (ABAW)
                   Competition is part of the respective ABAW Workshop which
                   will be held in conjunction with IEEE Computer Vision and
                   Pattern Recognition Conference (CVPR), 2023. The 5th ABAW
                   Competition is a continuation of the Competitions held at
                   ECCV 2022, IEEE CVPR 2022, ICCV 2021, IEEE FG 2020 and CVPR
                   2017 Conferences, and is dedicated at automatically
                   analyzing affect. For this year's Competition, we feature
                   two corpora: i) an extended version of the Aff-Wild2
                   database and ii) the Hume-Reaction dataset. The former
                   database is an audiovisual one of around 600 videos of
                   around 3M frames and is annotated with respect to:a) two
                   continuous affect dimensions -valence (how positive/negative
                   a person is) and arousal (how active/passive a person is)-;
                   b) basic expressions (e.g. happiness, sadness, neutral
                   state); and c) atomic facial muscle actions (i.e., action
                   units). The latter dataset is an audiovisual one in which
                   reactions of individuals to emotional stimuli have been
                   annotated with respect to seven emotional expression
                   intensities. Thus the 5th ABAW Competition encompasses four
                   Challenges: i) uni-task Valence-Arousal Estimation, ii)
                   uni-task Expression Classification, iii) uni-task Action
                   Unit Detection, and iv) Emotional Reaction Intensity
                   Estimation. In this paper, we present these Challenges,
                   along with their corpora, we outline the evaluation metrics,
                   we present the baseline systems and illustrate their
                   obtained performance.",
  month         =  mar,
  year          =  2023,
  copyright     = "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2303.01498"
}


@ARTICLE{Du2015-pm,
  title     = "Compound facial expressions of emotion: from basic research to
               clinical applications",
  author    = "Du, Shichuan and Martinez, Aleix M",
  abstract  = "Emotions are sometimes revealed through facial expressions. When
               these natural facial articulations involve the contraction of
               the same muscle groups in people of distinct cultural
               upbringings, this is taken as evidence of a biological origin of
               these emotions. While past research had identified facial
               expressions associated with a single internally felt category
               (eg, the facial expression of happiness when we feel joyful), we
               have recently studied facial expressions observed when people
               experience compound emotions (eg, the facial expression of happy
               surprise when we feel joyful in a surprised way, as, for
               example, at a surprise birthday party). Our research has
               identified 17 compound expressions consistently produced across
               cultures, suggesting that the number of facial expressions of
               emotion of biological origin is much larger than previously
               believed. The present paper provides an overview of these
               findings and shows evidence supporting the view that spontaneous
               expressions are produced using the same facial articulations
               previously identified in laboratory experiments. We also discuss
               the implications of our results in the study of
               psychopathologies, and consider several open research questions.",
  journal   = "Dialogues Clin. Neurosci.",
  publisher = "Informa UK Limited",
  volume    =  17,
  number    =  4,
  pages     = "443--455",
  month     =  dec,
  year      =  2015,
  keywords  = "action unit; computer vision; emotion category; psychopathology;
               spontaneous expression",
  copyright = "http://creativecommons.org/licenses/by-nc-nd/3.0/",
  language  = "en"
}

@article{kollias2022abaweccv, title={ABAW: Learning from Synthetic Data \& Multi-Task Learning Challenges}, author={Kollias, Dimitrios}, journal={arXiv preprint arXiv:2207.01138}, year={2022} }

@inproceedings{kollias2022abawcvpr, title={Abaw: Valence-arousal estimation, expression recognition, action unit detection \& multi-task learning challenges}, author={Kollias, Dimitrios}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={2328--2336}, year={2022} }

@article{kollias2021distribution, title={Distribution Matching for Heterogeneous Multi-Task Learning: a Large-scale Face Study}, author={Kollias, Dimitrios and Sharmanska, Viktoriia and Zafeiriou, Stefanos}, journal={arXiv preprint arXiv:2105.03790}, year={2021} }

@inproceedings{kollias2021analysing, title={Analysing affective behavior in the second abaw2 competition}, author={Kollias, Dimitrios and Zafeiriou, Stefanos}, booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages={3652--3660}, year={2021}}

@article{kollias2021affect, title={Affect Analysis in-the-wild: Valence-Arousal, Expressions, Action Units and a Unified Framework}, author={Kollias, Dimitrios and Zafeiriou, Stefanos}, journal={arXiv preprint arXiv:2103.15792}, year={2021}}

@inproceedings{kollias2020analysing, title={Analysing Affective Behavior in the First ABAW 2020 Competition}, author={Kollias, D and Schulc, A and Hajiyev, E and Zafeiriou, S}, booktitle={2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)}, pages={794--800}}

@article{kollias2019face,title={Face Behavior a la carte: Expressions, Affect and Action Units in a Single Network}, author={Kollias, Dimitrios and Sharmanska, Viktoriia and Zafeiriou, Stefanos}, journal={arXiv preprint arXiv:1910.11111}, year={2019}}

@INCOLLECTION{Kim2022-av,
  title     = "Emotion-aware multi-view contrastive learning for facial emotion
               recognition",
  booktitle = "Lecture Notes in Computer Science",
  author    = "Kim, Daeha and Song, Byung Cheol",
  publisher = "Springer Nature Switzerland",
  pages     = "178--195",
  series    = "Lecture notes in computer science",
  year      =  2022,
  address   = "Cham"
}

@INPROCEEDINGS{Jeong2022-th,
  title           = "Classification of facial expression in-the-wild based on
                     ensemble of multi-head cross attention networks",
  booktitle       = "2022 {IEEE/CVF} Conference on Computer Vision and Pattern
                     Recognition Workshops ({CVPRW})",
  author          = "Jeong, Jae-Yeop and Hong, Yeong-Gi and Kim, Daun and
                     Jeong, Jin-Woo and Jung, Yuchul and Kim, Sang-Ho",
  publisher       = "IEEE",
  month           =  jun,
  year            =  2022,
  conference      = "2022 IEEE/CVF Conference on Computer Vision and Pattern
                     Recognition Workshops (CVPRW)",
  location        = "New Orleans, LA, USA"
}

@ARTICLE{Khalil2019-bw,
  title     = "Speech emotion recognition using deep learning techniques: A
               review",
  author    = "Khalil, Ruhul Amin and Jones, Edward and Babar, Mohammad
               Inayatullah and Jan, Tariqullah and Zafar, Mohammad Haseeb and
               Alhussain, Thamer",
  journal   = "IEEE Access",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  7,
  pages     = "117327--117345",
  year      =  2019
}

@ARTICLE{Revina2018-lc,
  title     = "A survey on human face expression recognition techniques",
  author    = "Revina, I Michael and Emmanuel, W R Sam",
  journal   = "J. King Saud Univ. - Comput. Inf. Sci.",
  publisher = "Elsevier BV",
  month     =  sep,
  year      =  2018,
  copyright = "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  language  = "en"
}

@ARTICLE{Tian2022-uz,
  title   = "Emotion-aware {Human-Robot} Interaction and Social Robots",
  author  = "Tian, Leimin",
  journal = "Applied Affective Computing",
  year    =  2022
}

@ARTICLE{Khanna2022-rw,
  title     = "Affective computing in psychotherapy",
  author    = "Khanna, Rahul and Robinson, Nicole and O'Donnell, Meaghan and
               Eyre, Harris and Smith, Erin",
  journal   = "Advances in Psychiatry and Behavioral Health",
  publisher = "Elsevier BV",
  volume    =  2,
  number    =  1,
  pages     = "95--105",
  month     =  sep,
  year      =  2022,
  language  = "en"
}

@ARTICLE{Setiono2021-hb,
  title     = "Enhancing player experience in game with affective computing",
  author    = "Setiono, Doyo and Saputra, David and Putra, Kaleb and Moniaga, Jurike V and Chowanda, Andry",
  journal   = "Procedia Comput. Sci.",
  publisher = "Elsevier BV",
  volume    =  179,
  pages     = "781--788",
  year      =  2021,
  copyright = "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  language  = "en"
}




@article{kollias2020deep,
  title={Deep neural network augmentation: Generating faces for affect analysis},
  author={Kollias, Dimitrios and Cheng, Shiyang and Ververas, Evangelos and Kotsia, Irene and Zafeiriou, Stefanos},
  journal={International Journal of Computer Vision},
  volume={128},
  number={5},
  pages={1455--1484},
  year={2020},
  publisher={Springer}
}

@inproceedings{kollias2020va,
  title={Va-stargan: Continuous affect generation},
  author={Kollias, Dimitrios and Zafeiriou, Stefanos},
  booktitle={International Conference on Advanced Concepts for Intelligent Vision Systems},
  pages={227--238},
  year={2020},
  organization={Springer}
}

@article{kollias2019expression,
  title={Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface},
  author={Kollias, Dimitrios and Zafeiriou, Stefanos},
  journal={arXiv preprint arXiv:1910.04855},
  year={2019}
}

@article{kollias2019deep,
  title={Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond},
  author={Kollias, Dimitrios and Tzirakis, Panagiotis and Nicolaou, Mihalis A and Papaioannou, Athanasios and Zhao, Guoying and Schuller, Bj{\"o}rn and Kotsia, Irene and Zafeiriou, Stefanos},
  journal={International Journal of Computer Vision},
  volume={127},
  number={6},
  pages={907--929},
  year={2019},
  publisher={Springer}
}


@inproceedings{zafeiriou2017aff,
  title={Aff-wild: valence and arousal'In-the-Wild'challenge},
  author={Zafeiriou, Stefanos and Kollias, Dimitrios and Nicolaou, Mihalis A and Papaioannou, Athanasios and Zhao, Guoying and Kotsia, Irene},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition workshops},
  pages={34--41},
  year={2017}
}


@article{wen2021distract,
  title={Distract your attention: multi-head cross attention network for facial expression recognition},
  author={Wen, Zhengyao and Lin, Wenzhong and Wang, Tao and Xu, Ge},
  journal={arXiv preprint arXiv:2109.07270},
  year={2021}
}


@article{mollahosseini2017affectnet,
  title={Affectnet: A database for facial expression, valence, and arousal computing in the wild},
  author={Mollahosseini, Ali and Hasani, Behzad and Mahoor, Mohammad H},
  journal={IEEE Transactions on Affective Computing},
  volume={10},
  number={1},
  pages={18--31},
  year={2017},
  publisher={IEEE}
}













@inproceedings{christ2022muse,
  title={The muse 2022 multimodal sentiment analysis challenge: humor, emotional reactions, and stress},
  author={Christ, Lukas and Amiriparian, Shahin and Baird, Alice and Tzirakis, Panagiotis and Kathan, Alexander and M{\"u}ller, Niklas and Stappen, Lukas and Me{\ss}ner, Eva-Maria and K{\"o}nig, Andreas and Cowen, Alan and others},
  booktitle={Proceedings of the 3rd International on Multimodal Sentiment Analysis Workshop and Challenge},
  pages={5--14},
  year={2022}
}



@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}