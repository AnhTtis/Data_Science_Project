\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
\usepackage[preprint,nonatbib]{neurips_2021}
\usepackage{multirow}
% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%\usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\title{Human Reaction Intensity Estimation with Ensemble of Multi-task Networks}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  JiYeon Oh${^2}$, Daun Kim${^1}$, Jae-Yeop Jeong${^1}$, Yeong-Gi Hong${^1}$, and Jin-Woo Jeong${^1}$\\%\thanks{Corresponding author} \\
  Department of Data Science${^1}$, Division of IISE${^2}$ \\
  Seoul National University of Science and Technology \\
  Seoul, Korea \\
  \texttt{\{dhwldus0906, daun, jaey.jeong, yghong, jinw.jeong\}@seoultech.ac.kr} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
Facial expression in-the-wild is essential for various interactive computing domains. Especially, "Emotional Reaction Intensity" (ERI) is an important topic in the facial expression recognition task. In this paper, we propose a multi-emotional task learning-based approach and present preliminary results for the ERI challenge introduced in the 5th affective behavior analysis in-the-wild (ABAW) competition. Our method achieved the mean PCC score of 0.3254.
  
\end{abstract}

\section{Introduction}
%one of the most natural and explicit mediums for human beings to show their internal states. Automatic facial expression recognition can be used as important role in various human-computer interaction (HCI) systems such as social robots, driver's drowsiness detection, and consumer satisfaction survey \cite{li2020deep}.
Affective computing is a long-established area of interactive computing and one of the most active research areas in human computer interaction field, such as psychotherapy \cite{Khanna2022-rw}, game \cite{Setiono2021-hb}, and social robots \cite{Tian2022-uz}. In particular, accurately recognizing human response is crucial for HCI applications. Generally, the human response can be represented as emotion, which is explicit/implicit feedback on their internal states. Therefore, numerous studies have explored tracking emotion with various modalities, such as facial images \cite{Revina2018-lc}, speeches \cite{Khalil2019-bw}, and so on. Previous studies have shown that deep learning models perform well for relatively simple emotion recognition tasks, such as categorical emotion classification and valence/arousal regression \cite{Jeong2022-th, Kim2022-av, kollias2019face, kollias2021affect, kollias2021distribution}. However, these results do not necessarily imply that they reflect the overall state of human reactions. Therefore, more fine-grained tasks and solutions have been required to capture the wide range of internal human states and their corresponding responses. To address this challenge, more complex problems must be tackled, such as recognizing compound expressions \cite{Du2015-pm} and human reactions \cite{christ2022muse}. 
 
 As one of many attempts to address this issue, the 5th competition on Affective Behavior Analysis in-the-wild (ABAW)is held in conjunction with the Conference on Computer Vision and Pattern Recognition (CVPR) 2023 \cite{Kollias2023-ur}. 
 %a continuation of the 4th Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW) in ECCV 2022 \cite{kollias2022abaweccv}
 The ABAW competition \cite{kollias2020analysing, kollias2021analysing, kollias2022abawcvpr, kollias2022abaweccv} aims to ensure the entire feasibility of in-the-wild affective behavior analysis systems that can withstand video recording conditions, different situations, and display timing, regardless of human age, gender, race, and status. The 5th ABAW competition has the following tracks: 1) Valence-Arousal (VA)  2) Expression recognition (EXPR) 3) Action Unit detection (AU) 4) Emotional Reaction Intensity (ERI). The first three tracks are based on the Aff-Wild2 database \cite{kollias2019expression}, which is an extension of the Aff-wild database \cite{zafeiriou2017aff, kollias2019deep}. The last track is based on the Hume-Reaction dataset.
  
In this work, we report our methods for the ERI Estimation challenge and early results. For this challenge, the Hume-Reaction dataset \cite{christ2022muse} is employed for model train and validation. It consists of subjects responding to a more fine-grained range of various emotional video-based stimuli. It is multi-modal (i.e., facial images and audio) and consists of approximately 75 hours of video recordings recorded via a webcam inside subjectsâ€™ homes. A total of 2,222 subjects were recorded across two cultures, South Africa and the United States. Each sample within the dataset has been self-annotated by the subjects themselves for the intensity of 7 emotional experiences in a range from 1-100: Adoration, Amusement, Anxiety, Disgust, Empathic Pain, Fear, and Surprise.


\section{Method}
\subsection{Overview}
The Hume-Reaction dataset contains approximately 75 hours of video recordings. In other words, leveraging the time-series image frames and audio characteristics of these videos is the key breakthrough for ERI. 
% However, we deemed that audio is not crucial and can even negatively affect performance, as the baseline audio modality showed a low correlation (0.0741\%) \cite{Kollias2023-ur}. Therefore, 
In this work, we use only a set of facial images extracted from videos in the Hume-Reaction dataset. To derive high-level performance, the following points were carefully taken into account in particular: better feature representation and handling time series context. Hence, we utilize a deep-learning model called MTL-DAN (refer to \cite{wen2021distract} for original DAN), which is a modified architecture to embrace robust feature representation in a multi-emotional task learning manner. In addition, to exploit the context of time-series data, we feed the final feature representation of MTL-DAN to a recurrent neural network, LSTM, \cite{Kratzert2021-ui} for the final estimation of emotional reaction intensity. For the ERI challenge, the mean Pearson Correlation Coefficient(PCC) across all 7 reaction categories was used as a metric. More details about our framework can be found in Section \ref{sec:emotion Feature Extractor} and \ref{sec:lstm}.



\begin{figure}[t]
    \centering
    %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
    \includegraphics[width=\columnwidth]{figure/MTL_DAN.png}
    \caption{Architecture of MTL-DAN}
    \label{fig:MTL}
    %\Description{}
\end{figure}

\subsection{Training data}
Figure \ref{fig:dataset} represents facial images extracted from videos in the Hume-Reaction dataset. We segment each video by extracting the first frame from every 30 frames. On average, each video is represented using 11.62 frames. 
%The database is presented by the large-scale and in-the-wild Hume Reaction dataset \cite{christ2022muse}. The participants of this database explore a multi-output regression task, utilizing seven, self-annotated, nuanced classes of emotion. There are two modalities available in the database: Sound and Video. To apply the model, we only use video data for which the producer gives an ERI estimation challenge. In order to train our model using video data, we split the video data into b frames and created almost x images for each video. Furthermore, we assigned a sequence length of x to each video, where x is the number of images in the video sequence.
%First, the affective state label consists of 6 basic facial expressions (anger, disgust, fear, happiness, sadness, surprise), which were offered by the 4th ABAW competition organizers. 
%Next, the landmark annotations were created through DECA \cite{feng2021learning} framework which is a state-of-the-art 3D face shape reconstruction method that reconstructs both face landmark and 3D mesh from a single facial image. Figure \ref{fig:landmark} shows our example of landmark annotation which is composed of 68 coordinate points with (x, y) of the face.

\begin{figure}[t]
    \centering
    %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
    \includegraphics[width=\columnwidth]{figure/reaction_arch.png}
    \caption{Overview of the architecture used in this study}
    \label{fig:architecture}
    %\Description{}
\end{figure}

\begin{figure}[t]
    \centering
    %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
    \includegraphics[width=\columnwidth]{figure/reaction.png}
    \caption{Training data used in our study}
    \label{fig:dataset}
    %\Description{}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.5]{figure/MTL_DAN_dataset.png}
    \caption{Example of training images for MTL-DAN}
    \label{fig:MTLDAN}
    %\Description{}
\end{figure}
 

%\subsection{Model Architecture}
%\label{modelarch}
%As depicted in Figure \ref{fig:architecture}, our framework consists of two steps: 1) feature extractor and 2) regression head. 
%First, we employ a feature extractor as MTL-DAN, which enables the use of DAN for multi-emotional task learning. Second, outputs from MTL-DAN are fed into the regression head, which includes a series of LSTM/FC layers to estimate the emotional reaction intensity.
%composed of two branches: 1) emotion and 2) appearance. In each branch, we utilized a pre-trained backbone for a robust and generalized feature extraction. Finally, we employed a series of shared fully connected layers right after two branches, to exploit all knowledge extracted from the backbone model of each branch.



\subsubsection{DAN Architecture for Multi-emotional task Learning}
\label{sec:emotion Feature Extractor}
As depicted in Figure \ref{fig:architecture}, we adopt a deep learning-based facial expression recognition approach called "DAN" \cite{wen2021distract} which is a high-performance method for the AffectNet database \cite{mollahosseini2017affectnet}. The DAN architecture has two phases: feature extractor and attention parts. In the attention phase, there are multi-head cross-attention units which consist of a combination of spatial and channel attention units. The DAN architecture used in our framework is a modified version, called MTL-DAN. We extend the original DAN architecture to MTL-DAN that can jointly optimize categorical expression recognition (EXPR), action unit detection(AU), and valence/arousal regression(VA) in a multi-task learning manner. More details of MTL-DAN are as follows. As shown in Fig. \ref{fig:MTL}, Resnet18 \cite{he2016deep} is a feature extractor in the original DAN that creates an image feature containing emotional attributes. 
In DAN-MTL, the features from ResNet18 is utilized differently to handle different emotional recognition tasks. First, feature representation from ResNet18 is not fed into an attention block in the case of VA task, while EXPR and AU go through an independent attention block. After that, we concatenate feature representation from the attention blocks of EXPR and AU, which is represented as a shared feature in Fig.\ref{fig:MTL} Finally, we feed together the shared feature and each independent feature into each classification/detection/regression(VA) head for robust feature representation. As a result, MTL-DAN generates the outputs of three types: $V_{EXPR} \in  R^{8}$, $V_{AU} \in  R^{12}$, and $V_{VA}\in  R^{2}$. 
In our experiments, we initialize the parameters of MTL-DAN with pretrained MTL-DAN using Aff-wild2 multi-task learning challenge dataset \cite{kollias2022abawcvpr}. Also, we do not update the parameters of MTL-DAN to prevent overfitting to emotional reaction labels.   

 
\subsubsection{Regression Head}
\label{sec:lstm}
In this section, we describe how the estimation of emotional reaction intensity using a regression head (RH), which consists of a series of LSTM and fully connected layers, is made. Prior to feeding the outputs of MTL-DAN into LSTM, we concatenate three outputs from EXPR, AU, and VA heads.
%and stack the different feature representations based on the time with pre-defined sequential length(i.e., $RH_{input} \in  R^{12 \times 22}$). 
Finally, our framework produces the final prediction of ERI estimation with the sigmoid function.
%We need to combine the features and convert them to features including sequence lengths ($V_{video} \in  R^{S \times 22}$). in which $S$ is x of sequence length.
%To estimate emotional reaction Using all those features, we use an LSTM layer that can take sequence data as input. Then, The LSTM layer converts the sequence of features to emotional reaction features using a fully connected layer. The emotional feature dimension is $V_{out} \in R^{7}$.


%The goal of the appearance branch is to extract the robust feature in terms of visual appearance. For this, we employed various backbone models pre-trained on large scale data sets, ResNet50 \cite{he2016deep} pre-trained on VGGFace2 \cite{cao2018vggface2}, DINO ResNet50 \cite{caron2021emerging} pre-trained on VGGFace \cite{parkhi2015deep}, and MobileVITv2 \cite{mehta2022separable} pre-trained on ImageNet \cite{krizhevsky2012imagenet}, as a feature extractor for the appearance branch. 
%As shown in the appearance branch of Figure \ref{fig:architecture}, we trained only a single backbone for landmark detection during the multi-task learning phase, rather than utilizing multiple backbones together.
%Similar to the emotion branch, we also used a data augmentation strategy in the appearance branch. Due to the characteristics of landmark data, we applied Colorjitter only to prevent unnecessary spatial transformations. After feature extraction, all the landmark-related features are passed to the shared fully connected layers. 

%In summary, our multi-task learning model is configured with DAN on the emotion branch and the appearance branch with one of the following backbones: a) ResNet50, b) DINO ResNet50, c) MobileViT-v2.








\section{Results}
All the experiments were conducted using a GPU server with six NVIDIA RTX 3090 GPUs, 128 GB RAM, Intel i9-10940X CPU, and Pytorch framework. 

The goal of the experiments is to measure the performance of the model in estimating the emotional reaction intensity. To improve the performance, we experimented with two loss functions: the Concordance Correlation Coefficient (CCC) loss and the Pearson Correlation Coefficient (PCC) loss. We selected the loss function that resulted in the best performance.
Our preliminary results on the official validation set for the ERI Estimation challenge was 0.3254 in terms of the mean PCC score, which outperforms baseline methods, such as ResNet50-FAU (0.2840) and ResNet50-VGGFace2 (0.2488) \cite{Kollias2023-ur}.
%The ensemble configuration that resulted in the best performance is as follows: two strong models and five best resulting weak models on each sub-sampled data sets, two strong models of a) and b), two weak models of a), two weak models of b), and single weak model of c).

% 

%two strong models of a) and b), and weak models a), b), and c) as follows: {2, 2, 1}.
%\medskip


\section{Conclusion}

In this paper, we proposed a multi-emotional task learning-based architecture, MTL-DAN, and presented the preliminary results for the ERI estimation challenge in the 5th ABAW competition. Our method produced a mean PCC score of 0.3254 on the validation set for the ERI estimation challenge. The implementation details and validation results may be updated after the submission of this paper to arxiv.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{Checklist}

% %%% BEGIN INSTRUCTIONS %%%
% The checklist follows the references.  Please
% read the checklist guidelines carefully for information on how to answer these
% questions.  For each question, change the default \answerTODO{} to \answerYes{},
% \answerNo{}, or \answerNA{}.  You are strongly encouraged to include a {\bf
% justification to your answer}, either by referencing the appropriate section of
% your paper or providing a brief inline description.  For example:
% \begin{itemize}
%   \item Did you include the license to the code and datasets? \answerYes{See Section~\ref{gen_inst}.}
%   \item Did you include the license to the code and datasets? \answerNo{The code and the data are proprietary.}
%   \item Did you include the license to the code and datasets? \answerNA{}
% \end{itemize}
% Please do not modify the questions and only use the provided macros for your
% answers.  Note that the Checklist section does not count towards the page
% limit.  In your paper, please delete this instructions block and only keep the
% Checklist section heading above along with the questions/answers below.
% %%% END INSTRUCTIONS %%%

% \begin{enumerate}

% \item For all authors...
% \begin{enumerate}
%   \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
%     \answerTODO{}
%   \item Did you describe the limitations of your work?
%     \answerTODO{}
%   \item Did you discuss any potential negative societal impacts of your work?
%     \answerTODO{}
%   \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
%     \answerTODO{}
% \end{enumerate}

% \item If you are including theoretical results...
% \begin{enumerate}
%   \item Did you state the full set of assumptions of all theoretical results?
%     \answerTODO{}
% 	\item Did you include complete proofs of all theoretical results?
%     \answerTODO{}
% \end{enumerate}

% \item If you ran experiments...
% \begin{enumerate}
%   \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
%     \answerTODO{}
%   \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
%     \answerTODO{}
% 	\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
%     \answerTODO{}
% 	\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
%     \answerTODO{}
% \end{enumerate}

% \item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
% \begin{enumerate}
%   \item If your work uses existing assets, did you cite the creators?
%     \answerTODO{}
%   \item Did you mention the license of the assets?
%     \answerTODO{}
%   \item Did you include any new assets either in the supplemental material or as a URL?
%     \answerTODO{}
%   \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
%     \answerTODO{}
%   \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
%     \answerTODO{}
% \end{enumerate}

% \item If you used crowdsourcing or conducted research with human subjects...
% \begin{enumerate}
%   \item Did you include the full text of instructions given to participants and screenshots, if applicable?
%     \answerTODO{}
%   \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
%     \answerTODO{}
%   \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
%     \answerTODO{}
% \end{enumerate}

% \end{enumerate}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \appendix

% \section{Appendix}

% Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.
% This section will often be part of the supplemental material.
\nocite{*}
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\end{document}