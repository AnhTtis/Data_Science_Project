\section{Supplementary Material}

% ------------------------------------------------------------------------------
\subsection{Training and sampling with diffusion models.}

The following two algorithms show the training and sampling strategy for a diffusion model adapted from \cite{ho_denoising_2020}.

\begin{algorithm}[H]
\SetKwInput{kwReq}{Required}

\caption{Training of a vanilla diffusion model}\label{alg:train}
\kwReq{Denoising network $\bm \epsilon_\theta$ with parameters $\theta$, forward variances $\beta_i$ with $i \in \{1, ..., T\}$}

\Repeat{converged}{
\tcp{Sample uncorrupted image from data distribution}
$\x_0 \sim q(\x_o)$

\tcp{Sample uniformly distributed timestep}
$t \sim U({1, ..., T})$

\tcp{Sample normal distributed noise}
$\bm \epsilon \sim \mathcal{N}(\bm 0, \bm I)$

\tcp{Compute alpha products}
$\bar{\alpha}_t = \prod_{s=1}^t (1-\beta_s)$

\tcp{Obtain corrupted $\x_0$ at timestep $t$}
$\x_t = \sqrt{\bar{\alpha}_t}\x_0 + \sqrt{1-\bar{\alpha}_t}\bm \epsilon$

\tcp{Update weights with loss gradients}
$\theta \gets \theta - \nabla_\theta \Vert \bm \epsilon_t - \bm \epsilon_{\theta}(\x_t, t) \Vert^2_2$
}
\end{algorithm}
%
\begin{algorithm}[H]
\SetKwInput{kwReq}{Required}


\caption{Sampling from a vanilla diffusion model}\label{alg:samp}
\kwReq{Denoising network $\bm \epsilon_\theta$, forward variances $\beta_i$ with $i \in \{1, ..., T\}$, reverse variances $\sigma_i$ with $i \in \{1, ..., T\}$}

 \tcp{Sample start as isotropic noise}
$\x_T \sim \mathcal{N}(\bm{0}, \bm{I})$

 \For{$t \gets T$ \KwTo $1$}{

\tcp{Compute alpha products}
$\bar{\alpha}_t = \prod_{s=1}^t (1-\beta_s)$

\tcp{Do not set any noising for the last step}
\lIf{$t > 1$}
{
    $\bm \epsilon \sim \mathcal{N}(\bm 0, \bm I)$
     \textbf{else} $\epsilon = \bm 0$
}

\tcp{Compute state of \x\ on previous timestep}
$\x_{t-1} \gets \frac{1}{\sqrt{1-\beta_t}} \left( \x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \bm \epsilon_\theta(\x_t, t) \right) + \sigma_t \epsilon$

}
\Return $\x_0$
\end{algorithm}
%
Here, $\sigma_t$ is the variance of the reverse process at timestep $t$. 
While this parameter is learnable in general, \cite{ho_denoising_2020} presents two static versions of $\sigma_t$: Setting it directly to $\beta_t$ or $\frac{1 - \bar{\alpha}_{t - 1}}{1 - \bar{\alpha}_t - } \cdot \beta_t$.
We chose the latter option in our experiments.
We refer to \cite{ho_denoising_2020} for extended derivations and proofs for the above algorithms and a deeper understanding of diffusion modeling.

% ------------------------------------------------------------------------------

\subsection{MaCheX Collection Details}

In the following, more details about the components of \textit{MaCheX} are given.

\begin{itemize}
    \item \textbf{ChestX-ray14} \cite{wang_chestx-ray8_2017} contains 108,948 frontal-view X-ray images from the NIH clinical center in Bethesda between 1992 to 2015. Labels for over 14 pathologies are sourced via NLP methods from radiological reports.
    \item \textbf{CheXpert} \cite{irvin_chexpert_2019} consists of 224,316 frontal- and lateral-view scans from the Stanford Hospital in Palo Alto. The studies were performed between 2002 and 2017. While text reports are also the basis for the provided labels, the extraction process offers the option of marking a pathology as \textit{not mentioned} or \textit{uncertain}. The so-called \textit{CheXpert labeler} is also open-sourced, but the text reports are not.
    \item \textbf{MIMIC-CXR} \cite{johnson_mimic-cxr_2019} provides 377,110 images from the Beth Israel Deaconess Medical Center Emergency Department recorded between 2011 and 2016. In addition to labels provided in the same style as in CheXpert, the dataset also releases full-text radiological reports.
    \item \textbf{PadChest} \cite{bustos_padchest_2020} contains studies with 160,868 images from the San Juan hospital in Alicante from 2009 to 2017. The dataset promotes radiological texts in Spanish and inferred labels from the reports.
    \item \textbf{BRAX} \cite{reis_eduardo_pontes_brax_nodate} is a Brazilian dataset from the Hospital Israelita Albert Einstein, housing 40,967 chest x-rays with supplied labels in the style of CheXpert.
    \item \textbf{VinDr-CXR} \cite{nguyen_vindr-cxr_2022} consists of 18,000 thoracic scans from 2018 to 2020 recorded at Hospital 108 and Hanoi Medical University Hospital in Vietnam. It provides hand-labeled annotations with coordinate bounding boxes for various pathologies.
\end{itemize}

% ------------------------------------------------------------------------------

\subsection{Training details of Cheff}

All model training was conducted on an NVIDIA DGX system with 8 A100 @ 40GB VRAM.
To reduce computational complexity, we do not conduct a hyperparameter search but rely on the recommended and subsequently elaborated settings proposed in \cite{rombach_high-resolution_2022, saharia_image_2021}.
To accelerate the sampling procedure in the diffusion models, we utilize DDIM sampling with 150 steps.
All models were trained with the Adam optimizer.

\subsubsection{Autoencoder (AE)}

For AE we use the KL-regularized continuous variational AE (VAE) with a downsampling factor 4 as in \cite{rombach_high-resolution_2022} and train it 3 epochs with a batch size of 64 on \textit{MaCheX}.
We did not see an improvement in validation errors in longer training sessions.
The convolutional embedding has a dimension of $3 \times 64 \times 64 = 12288$.
The architecture consists of 3 blocks with 128, 256, and 512 channels respectively.
Each block contains two residual layers.
No self-attention or dropout was applied.
The decoder is the mirrored version of the encoder.
To limit the normal prior in the latent space and focus on reconstruction quality, a KL-divergence weighting of \num{1e-6} was set.
We stick to the default learning rate of \num{4.5e-6}.
We experimented with larger downsampling factors as well as discretized embeddings and found that factor 4 offers the highest, most fine-grained reconstruction quality.

\subsubsection{Semantic diffusion model (SDM)}

\paragraph{Unconditional SDM}
%
The model is trained on the full \textit{MaCheX} collection for 500,000 steps with a batch size of 256.
The time-conditional U-net has 4 downsampling blocks with a base filter number of 224 and the multipliers $(1, 2, 4, 4)$ respectively.
Each downsampling block has 2 residual layers.
Attention is applied to the spatial resolution of 8, 16, and 32.
The learning rate is set to \num{5.0e-05}.
The timestep is encoded over sinusoidal position embeddings.

We noticed that the default linear variance schedule with a maximum variance of 0.0195 does not induce enough noise into the process to guarantee an approximate isotropic  Gaussian in $\x_T$, which proves to be extremely destructive for the synthesis process.
The number of used timesteps is 1000.
Raising the maximum variance to 0.0295 eliminates this issue.

\paragraph{Report-conditioned SDM}
%
The structure and training procedure of the base U-net stays the same, but we train for a longer period of time, namely for 750,000 steps.
We apply the BERT-tokenizer to preprocess the text sequences and limit the length of the tokenized input to a maximum of 150. 
The conditioning $\bm y$ is fed to the model over cross-attention, where $\tau_\phi$ is a transformer with a depth of 32 and an embedding dimension of 1280.
As it is described in \cite{rombach_high-resolution_2022, vaswani_attention_2017}, an embedding $\tau_\phi(\bm y) \in \mathbb{R}^{d_{\tau}}$ is induced to the model over cross-attention.
Thus, an attention layer with $\text{Attention}(Q,K,V) = \text{softmax} \left( \frac{QK^T}{\sqrt{d}} \right) V$ has keys $K$, queries $Q$ and values $V$ defined as
%
\begin{align*}
Q &= W_Q \cdot \varepsilon  &  K &= W_K \cdot \tau_\phi(\bm y)  &  V &= W_V \cdot \tau_\phi(\bm y)    \eqendp\\
\end{align*}
%
$W_Q \in \mathbb{R}^{d_q \times d_\varepsilon}, W_K \in \mathbb{R}^{d_k \times d_\tau}, W_V \in \mathbb{R}^{d_v \times d_\tau}$. $\varepsilon$ is an intermediate vectorized representation in $\bm \epsilon_\theta$, i.e., the input to the respective attention layer in the time-conditioned denoising U-net.
In other words, cross-attention works by providing keys $K$ and values $V$ with information over the targeted conditioning and querying (via $Q$) with the concept that is being processed, i.e., the image to be denoised. 

\subsubsection{Super-resolution (SR)}
%
SR training amounts to 1 million steps on \textit{MaCheX} with a batch size of 32.
The conditioning is done by simply concatenating the low-resolution image to the target input on the channel dimension.
To obtain \xlr\ for training, we resize \xhr\ to the low resolution with bicubic downsampling.
To align SR with the synthesis pipeline, we fine-tune SR for another 500,000 steps on $D(\z)$.
The learning rate for normal training was \num{5.0e-05} and \num{2.0e-05} during finetuning.
A cosine variance schedule with 2000 timesteps is used instead of the piece-wise distribution in \cite{saharia_image_2021}.

The model itself is a U-net with sinusoidal position embeddings for the timestep.
It has a base number of filters of 16 and 8 downsampling blocks following the filter multiplication schedule of $(1, 2, 4, 8, 16, 32, 32, 32)$.
Each block has 2 residual layers.
Self-attention is applied in the lowest 5 downsampling levels.

% ------------------------------------------------------------------------------

\subsection{Sparse latent space}

While the basic formulation of a VAE often promises the existence of a generative model in itself, we observe that this is not the case in the AE component here.
As the AE is trained with an extremely low weighting of KL-divergence, the influence of the Gaussian prior is minimal in the structure of the posterior latent space $\mathcal{Z}$.
As a result, $\mathcal{Z}$ is not smooth, and sampling $\z \sim \mathcal{N}(\bm{0}, \sigma \bm{I})$ does not yield any valid samples of the dataspace even for small values of $\sigma$.
This can also be easily explained, as the convolutional AE fosters a spatial embedding, and isotropic sampled noise does not resemble anything lung shape-related.
Yet, simple linear interpolation between two valid samples in latent space still seems rather smooth, but when investigated in detail, the transition is more a blending of the two images instead of merging on a conceptual level.
%
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{supplement/noise_samples.png}
    \caption{Reconstructions for samples $\z \sim \mathcal{N}(\bm{0}, \bm{I})$ showing missing generative capacity in a non-smooth latent space for the AE without a functioning prior like a diffusion model.}
    \label{fig:noise}
\end{figure}
%
\begin{figure}
     \centering
    \begin{tabular}{c}
  \includegraphics[width=\textwidth]{supplement/interpol_1.png} \\
  \includegraphics[width=\textwidth]{supplement/interpol_2.png} \\
  \includegraphics[width=\textwidth]{supplement/interpol_3.png} \\
    \end{tabular}
\caption{Linear interpolation in the latent space of the AE.}
\label{fig:interpol}
\end{figure}
%

\clearpage

\subsection{Experiments with VQ-VAE}

We also experimented with a vector-quantized version of AE, but found a dip in reconstruction quality and especially in annotations, and favored the classic VAE formulation.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{supplement/inputs_gs-060000_e-000006_b-001260.png}
         \caption{Original images}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{supplement/reconstructions_gs-060000_e-000006_b-001260.png}
         \caption{Reconstructed images}
     \end{subfigure}
     \hfill
        \caption{Comparison of original and reconstructed chest x-ray samples using VQ-VAE.}
\end{figure}

% ------------------------------------------------------------------------------

\subsection{Effects of insufficient noising}

When training with the full default settings of LDM, we were only able to produce corrupted and solarized synthetic samples (Figure~\ref{fig:noise_rec}).
As it turns out, this was due to a too-small maximal $\beta$ value of $0.0195$ in the linear variance schedule.
%
\begin{figure}[b]
    \centering
    \includegraphics[width=0.9\textwidth]{supplement/samples_gs-630000_e-000016_b-003519.png}
    \caption{Insufficient noise in the forward process leads to corrupted synthesis.}
    \label{fig:noise_rec}
\end{figure}
%
By investigating the forward diffusion process, we observed that the maximal corrupted latent space sample $\z_T$ still encodes some structural information of its original image $\x_0$ (see Figure~\ref{fig:low-beta} right column) when decoding $\z_T$ with $D$.
During synthesis, where $\z_T \sim \mathcal{N}(\bm{0}, \bm{I})$ is chosen as a starting point, the model does not know what to infer from this unseen unconditional information, which results in low-quality samples.
Increasing the maximal $\beta$ to $0.0295$ resolves this issue.
%
\begin{figure}
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{supplement/diffusion_row_gs-640000_e-000016_b-013519.jpg}
         \caption{Low maximal $\beta$}
         \label{fig:low-beta}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{supplement/diffusion_row_gs-085000_e-000008_b-006687.jpg}
         \caption{Increased maximal $\beta$}
          \label{fig:high-beta}
     \end{subfigure}
     \hfill
        \caption{Comparison the forward diffusion process for the default $\beta$ (\textbf{right}) and our increased value (\textbf{left}).}
\end{figure}
%

% ------------------------------------------------------------------------------

\subsection{More inpainting examples using Cheff}

The below figure displays two more examples of inpainting in chest radiographs. The first image shows the removal of a chest port, the second image shows the successful completion of separated edges.
%
\begin{figure}
    \centering
    \begin{tabular}{cccc}
     \includegraphics[width=0.24\textwidth]{supplement/00071_overlay.png} &   \includegraphics[width=0.24\textwidth]{supplement/00071_new_sr.png} &
  \includegraphics[width=0.24\textwidth]{supplement/01264_overlay.png} &   \includegraphics[width=0.24\textwidth]{supplement/01264_new_sr.png} \\
    \end{tabular}
    \caption{More examples of image inpainting with \textit{Cheff}.}
    \label{fig:ex-inpaint2}
\end{figure}
%

% ------------------------------------------------------------------------------

\subsection{More synthetic samples of Cheff}

On the following pages more random synthetic Chest X-rays, which are generated by \textit{Cheff}, are showcased.

\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}

%
\begin{figure}
    \centering
    \begin{tabular}{c}
    \includegraphics[width=0.75\textwidth]{supplement/03515.png} \\
    \includegraphics[width=0.75\textwidth]{supplement/03514.png} \\
    \end{tabular}
    \caption{Synthetic Chest X-rays generated by \textit{Cheff} (\textbf{\RNum{1}})}
\end{figure}
%
%
\begin{figure}
    \centering
    \begin{tabular}{c}
    \includegraphics[width=0.75\textwidth]{supplement/00853.png} \\
    \includegraphics[width=0.75\textwidth]{supplement/09382.png} \\
    \end{tabular}
    \caption{Synthetic Chest X-rays generated by \textit{Cheff} (\textbf{\RNum{1}})}
\end{figure}
%
%
\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{supplement/medium_grid_2.png}
    \caption{Synthetic Chest X-rays generated by \textit{Cheff} (\textbf{\RNum{3}})}
    \label{fig:ex-samples-l}
\end{figure}
%
%
\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{supplement/medium_grid.png}
    \caption{Synthetic Chest X-rays generated by \textit{Cheff} (\textbf{\RNum{4}})}
\end{figure}
%
%
\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{supplement/small_grid.png}
    \caption{Synthetic Chest X-rays generated by \textit{Cheff} (\textbf{\RNum{5}})}
\end{figure}
%