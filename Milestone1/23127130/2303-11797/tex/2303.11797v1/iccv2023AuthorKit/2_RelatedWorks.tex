
 \input{iccv2023AuthorKit/figure/tex/refinement.tex}

\input{iccv2023AuthorKit/figure/tex/main_architecture.tex}

\section{Related Work}

\paragraph{Open-vocabulary semantic segmentation.} Classical approaches to the task~\cite{zhao2017open, bucher2019zero, xian2019semantic} attempt to learn visual embeddings that align with pre-defined text embeddings~\cite{miller1998wordnet, mikolov2013efficient}. However, the domain difference, as well as the limited vocabulary of the words have been the major bottlenecks. To address this, LSeg~\cite{li2022language} leveraged CLIP for learning pixel-level visual embeddings aligned with the text embeddings of CLIP. Alternatively, OpenSeg~\cite{ghiasi2022scaling} proposed to identify local regions within the image and correlate with the text embeddings with class-agnostic region proposals.

More recently, ZegFormer~\cite{ding2022decoupling} and ZSseg~\cite{xu2022simple} proposed two-stage frameworks. Typically, they first learn to predict class-agnostic region proposals similar to~\cite{ghiasi2022scaling}, and feed them to CLIP for final predictions. To better recognize these regions, OVSeg~\cite{liang2022open} collects region-text pairs to fine-tune the CLIP encoder. A notable work, MaskCLIP~\cite{zhou2022extract} attempts to obtain pixel-level embeddings from CLIP that can be directly utilized for segmentation. However, based on their findings~\cite{zhou2022extract} that conventional fine-tuning impedes the open-vocabulary capability of CLIP, they instead propose to freeze the CLIP encoder and apply non-learnable methods to mitigate its noise. Unlike the aforementioned works, we introduce a cost aggregation method to benefit from fine-tuning the CLIP encoder and obtain accurate pixel-level predictions.



\vspace{-10pt}
\paragraph{Cost aggregation.} Cost aggregation is a popular technique adopted for the process of establishing correspondence between visually or semantically similar images~\cite{kendall2017end,guo2019group,yang2019hierarchical,cho2021cats,hong2022cost} by reducing the impact of errors and inconsistencies in the matching process. A matching cost, an input to cost aggregation, is typically constructed between dense features extracted from a pair of images~\cite{rocco2017convolutional}, and often cosine-similarity~\cite{liu2022graftnet,rocco2017convolutional} is used. In matching literature, numerous works~\cite{kendall2017end, chang2018pyramid, guo2019group, yang2019hierarchical, song2021adastereo,hong2022neural,huang2022flowformer,cho2022cats++} have proposed cost aggregation modules and demonstrated its importance, owing to its favorable generalization ability~\cite{song2021adastereo,liu2022graftnet}. In this work, we leverage the cost volume constructed between image and text embeddings from CLIP encoders to promote accurate segmentation through cost aggregation.

