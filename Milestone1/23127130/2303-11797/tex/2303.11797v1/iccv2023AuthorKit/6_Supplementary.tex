\input{iccv2023AuthorKit/figure/tex/supple_architecture}
\renewcommand{\thesection}{\Alph{section}}
\setcounter{section}{0}
\section*{Appendix}
In the following, we first provide more implementation details in Section~\ref{A}. We then provide additional experimental results and ablation study in Section~\ref{B}. Finally, we present qualitative results for all the benchmarks and human part segmentation in Section~\ref{C} and a discussion of limitations in Section~\ref{D}.


\section{More Details}\label{A}
\subsection{Architectural Details}

In the following, we provide more architectural details. Our overall architecture is first illustrated in Fig.~\ref{fig:guidance-architecture} (a).

\smallbreak
\noindent\textbf{Embedding guidance.}
In this paragraph, we provide more details of embedding guidance, which is designed to facilitate the cost aggregation process by exploiting its rich semantics for a guidance.  We first extract visual and text embeddings from feature backbone and frozen CLIP text encoder~\cite{radford2021learning}, respectively. The embeddings then undergo linear projection and concatenated to the cost volume before query and key projections in aggregation layer. The design is illustrated in Fig.~\ref{fig:guidance-architecture}~(b). In each case, for employing Swin Transformer~\cite{liu2021swin} as a feature backbone, the stage 3 output is used, whereas for ResNet-101~\cite{he2016deep}, the output from the last conv layer of \texttt{conv4\_x} is used as our guidance.

\smallbreak
\noindent\textbf{Upsampling decoder.}
The detailed architecture is illustrated in Fig.~\ref{fig:guidance-architecture}~(c). For multi-level features $E^V_{Dec}$, we leverage the features of last layers from \texttt{conv2\_x} and \texttt{conv3\_x} when ResNet-101~\cite{he2016deep} is used, whereas the output features of stages 1 and 2 are leveraged when Swin Transformer~\cite{liu2021swin} is used. 
%\input{iccv2023AuthorKit/figure/tex/supple_architecture}

% \vspace{-10pt}
% \paragraph{Motivation for embedding guidance.}
% As previously stated, direct optimization of the CLIP image encoder can harm open-vocabulary capability of CLIP and cost aggregation can serve an effective way to transfer the knowledge of CLIP from image-level to pixel-level. However, during the cost computation stage, the projection of high-dimensional embeddings into restricted low-dimensional space inevitably results in the loss of semantic information related to class or fine-grained details of the input image. One solution for the loss of information is to aid the cost aggregation with the readily available embedding used for cost computation, extracted from the CLIP image and text encoder. However, as Table.~\ref{tab:feature-vs-cost} implies, direct utilization of such embeddings can lead to performance degradation. To detour this utilization, 

\input{iccv2023AuthorKit/figure/tex/fine-tuning}

\subsection{Other Implementation Details}
\smallbreak
\noindent\textbf{Training details.}
In Fig.~\ref{fig:fine-tune}, we visualize the proposed approach to fine-tune the CLIP image encoder, which was introduced in Section 3.5 in the main paper. A cost volume resolution of $H=W=24$ is used for training. The position embeddings of the CLIP image encoder is initialized with bicubic interpolation~\cite{touvron2021training}, and we set training resolution as $384\times 384$. For ViT-B and ViT-L variants, we initialize CLIP~\cite{radford2021learning} with official weights of ViT-B/16 and ViT-L/14@336px respectively. For ViT-H and ViT-G variants, we initialize with OpenCLIP~\cite{cherti2022reproducible} weights trained with LAION-2B~\cite{schuhmann2022laion}. The CLIP text encoder remains frozen across all experiments. The feature backbone networks are pre-trained on ImageNet-1k~\cite{deng2009imagenet} in $224\times224$ resolution for ResNet-101~\cite{he2016deep}, or on ImageNet-21k in $384\times384$ resolution for Swin Transformer~\cite{liu2021swin}. All hyperparameters are kept constant across the evaluation datasets.  

\input{iccv2023AuthorKit/figure/tex/baseline}
\smallbreak
\noindent\textbf{Text prompt templates.}
To obtain text embeddings from the text encoder, we follow CLIP~\cite{radford2021learning} and form sentences with the class names, such as \texttt{"a photo of a \{class\}"}. We ensemble 80 text prompts originally used in CLIP for ImageNet classification, without additionally curating text prompts or synonyms.

\smallbreak
\noindent\textbf{Feature and cost aggregation baselines.}
In this paragraph, we provide more details of the architecture of two models introduced in Fig.~\ref{fig:conceptual_qual}: One is feature aggregation method and the other is cost aggregation method. As shown in Fig.~\ref{fig:baseline} (a), the feature aggregation method directly leverages the features extracted from CLIP image encoder by feeding the concatenated image and text embeddings into the upsampling decoder. Fig.~\ref{fig:baseline} (b) shows the cost aggregation approach that constructs cost volume instead, and subsequent embedding layer processes it to feed into upsampling decoder. 
%

\subsection{Patch Inference}
The practicality of Vision Transformer (ViT)~\cite{dosovitskiy2020image} for high-resolution image processing has been limited due to its quadratic complexity with respect to the sequence length. As our model leverages ViT to extract image embeddings, \ours may struggle to output to the conventional image resolutions commonly employed in semantic segmentation literature, such as $640\times 640$~\cite{cheng2021per,ghiasi2022scaling}, without sacrificing some accuracy made by losing some fine-details. Although we can adopt the same approach proposed in~\cite{zhou2022extract} to upsample the positional embedding~\cite{zhou2022extract}, we ought to avoid introducing excessive computational burdens, and thus adopt an effective inference strategy without requiring additional training which is illustrated in Fig.~\ref{fig:patch-inference}.

To this end, we begin by partitioning the input image into overlapping patches of size $\frac{H}{N_P} \times \frac{W}{N_P}$. Intuitively, given an image size of $640 \times 640$, we partition the image to sub-images of size $384 \times 384$, which matches the image resolution at training phase, and each sub-images has overlapping regions $128 \times 128$. Subsequently, we feed these sub-images and the original image that is resized to $384 \times 384$ into the model. Given the results for each patches and the image, we merge the obtained prediction, while the overlapping regions are averaged to obtain the final prediction. In practice, we employ $N_P=2$,  while adjusting the overlapping region to match the effective resolution of $640\times 640$.

\input{iccv2023AuthorKit/figure/tex/patch_inference}

\section{Additional Ablation Study}\label{B}
\subsection{Comparison of Aggregation Baselines}\vspace{-10pt}
\input{iccv2023AuthorKit/table/feature_vs_cost}
In this ablation study, we provide a quantitative comparison of two aggregation baselines, feature aggregation and cost aggregation, in Table~\ref{tab:feature-vs-cost}. We freeze the CLIP image encoder and only optimize the upsampling decoder, and the results are summarized in \textbf{(I)} and \textbf{(III)}. Subsequently, in \textbf{(II)} and \textbf{(IV)}, we fine-tune the CLIP image encoder on top of \textbf{(I)} and \textbf{(III)}. Our results show that feature aggregation  can benefit from fine-tuning, but the gain is only marginal. On the other hand, cost aggregation benefits significantly from fine-tuning, highlighting the effectiveness of cost aggregation to transfer knowledge in CLIP encoder. 

\subsection{Ablation Study of the Number of Layers $N_B$ }\vspace{-10pt}
\input{iccv2023AuthorKit/table/layer_ablation}
Table~\ref{tab:layer-ablation} summarizes the effects of varying $N_B$. From the results, we find that choosing higher $N_B$ does not always lead to performance improvements.
% ; rather, as the model gets heavier, it risks overfitting, which is confirmed by the results when $N_B = 4$. 
Note that we chose $N_B = 2$ to balance between performance and model capacity.

\subsection{Ablation Study of Inference Strategy}\vspace{-10pt}
\input{iccv2023AuthorKit/table/inference_ablation}
Table~\ref{tab:inference-ablation} presents effects of different inference strategies for our model. The first row shows the results using the training resolution at inference time. The second row represents a variant that upsamples the positional embedding within the CLIP image encoder, allowing the encoder to process higher resolution images. Finally, the last row adopts the proposed patch inference strategy. It is shown that increasing the positional embedding introduces substantial memory overhead. Moreover, for some datasets, using training resolution yields better performance. On the other hand, our proposed approach can bring large performance gains, while also ensuring high efficiency.

\subsection{Effects of Upsampling Decoder}\vspace{-10pt}
\input{iccv2023AuthorKit/table/conv_decoder}
We provide an quantitative results of adopting the proposed upsampling decoder in Table~\ref{tab:conv-decoder}. The results show consistent improvements across all the benchmarks.

\subsection{Comparison of Inference Time}\vspace{-10pt}
\input{iccv2023AuthorKit/table/inference_time}
In Table~\ref{tab:inference-time}, we show run-time comparison at inference phase to the recent two-stage approach~\cite{ding2022decoupling}. We report the results with different VLM, \textit{e.g.,} ViT-B, L, H and G.   Note that the inference speed of the proposed method differs depending on the number of categories, resulting in different run-times across the evaluation datasets. In comparison to~\cite{ding2022decoupling}, we report the mean run-time, as its performance demonstrates minimal variation with respect to the number of categories.

When utilizing ViT-B for vision-language models, our method exhibits a relatively slower inference time for some scenarios, including A-847 and PC-459.  However, as we scale the VLM to larger variants, such as ViT-L, H, and G, our approach enjoys significantly faster inference by almost over 20 times faster, highlighting the efficiency of the proposed method.

% \vspace{-10pt}
% \paragraph{Result with additional prompt engineering.}
% In Table~\ref{tab:prompt-engineering}, we study the effect of prompt engineering introduced in~\cite{ghiasi2022scaling}. This includes augmenting the class description with synonyms or adding detailed description for solving ambiguity of polysemy. During inference, .As the result shows, our method also can benefit from manual prompt engineering.

\section{More Qualitative Results}\label{C}
We provide more qualitative results on A-847~\cite{zhou2019semantic} in Fig.~\ref{fig:ade847}, PC-459~\cite{mottaghi2014role} in Fig.~\ref{fig:pc459}, A-150~\cite{zhou2019semantic} in Fig.~\ref{fig:ade150}, and PC-59~\cite{mottaghi2014role} in Fig.~\ref{fig:pc59}. We also further compare the results in A-847~\cite{zhou2019semantic} with other methods~\cite{ding2022decoupling, xu2022simple, liang2022open} in Fig.~\ref{fig:ade847-comparison}.

\smallbreak
\noindent\textbf{Qualitative results on part segmentation.}
We further compare human part segmentation results with \cite{liang2022open} in Fig.~\ref{fig:partseg-supple}. 
% Specifically, we provide 5 classes for part segmentation as follows: \{``background", ``head of person", ``torso of person", ``arm of person", ``leg of person"\}.

\section{Limitations}\label{D}
To evaluate open-vocabulary semantic segmentation results, we follow ~\cite{ghiasi2022scaling, liang2022open} and compute the metrics using the other segmentation datasets. However, since the ground-truth segmentation maps involve some ambiguities, the reliability of  the evaluation dataset is somewhat questionable. For example, the last row of Fig.~\ref{fig:qualitative}~(e) exemplifies how our predictions in the mirror, ``sky" and ``car", as well as ``plant" in between the ``fence", are classified as wrong segmentation as the ground truth classes are ``mirror" and ``fence". Constructing a more reliable dataset including ground-truths accounting for above issue for accurate evaluation is an intriguing topic. 
%but this is beyond the scope of our work.

\input{iccv2023AuthorKit/figure/tex/qualitative_supp/ade847}
\input{iccv2023AuthorKit/figure/tex/qualitative_supp/pc459}
\input{iccv2023AuthorKit/figure/tex/qualitative_supp/ade150}
\input{iccv2023AuthorKit/figure/tex/qualitative_supp/pc59}
\input{iccv2023AuthorKit/figure/tex/qualitative_supp/supp_comparison}
\input{iccv2023AuthorKit/figure/tex/qualitative_supp/part}

\clearpage
