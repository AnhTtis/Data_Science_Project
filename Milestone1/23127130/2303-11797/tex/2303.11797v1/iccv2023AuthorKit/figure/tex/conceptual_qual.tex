\begin{figure*}[t]
  \centering
  \renewcommand{\thesubfigure}{}

     \subfigure[(a) mIoU scores of \textbf{seen} classes]
{\includegraphics[width=0.28\linewidth]{iccv2023AuthorKit/figure/pdf/plot1.pdf}}\hfill
     \subfigure[(b) mIoU scores of \textbf{unseen} classes]
{\includegraphics[width=0.28\linewidth]{iccv2023AuthorKit/figure/pdf/plot2.pdf}}\hfill
     \subfigure[(c) Qualitative comparisons]
{\includegraphics[width=0.440\linewidth]{iccv2023AuthorKit/figure/pdf/fig2.pdf}}\hfill\\


\vspace{-10pt}
\caption{\textbf{Comparison between feature and cost aggregation.} To validate our framework, we consider two approaches: direct optimization of CLIP embeddings through feature aggregation and indirect optimization through \textbf{cost aggregation}. 
(a): Both approaches achieves performance gains for \textbf{seen} classes from fine-tuning the CLIP image encoder. (b): Feature aggregation fails to generalize to \textbf{unseen} classes, while cost aggregation achieves a large performance gains, highlighting the effectiveness of this approach for open-vocabulary segmentation. (c): Qualitative results where \textbf{(IV)} successfully segments the previously unseen class, e.g., `\textit{birdcage}', whereas \textbf{(III)} fails.
}
\vspace{-5pt}
  \label{fig:conceptual_qual}
\end{figure*}