\begin{figure*}[t]
  \centering
  \renewcommand{\thesubfigure}{}
    \subfigure[(a) Input]
{\includegraphics[width=0.166\linewidth]{iccv2023AuthorKit/figure/pdf/image.pdf}}\hfill
    \subfigure[(b) ZegFormer~\cite{ding2022decoupling}]
{\includegraphics[width=0.166\linewidth]{iccv2023AuthorKit/figure/pdf/zegformer.pdf}}\hfill
     \subfigure[(c) ZSSeg~\cite{xu2022simple}]
 {\includegraphics[width=0.166\linewidth]{iccv2023AuthorKit/figure/pdf/zsseg.pdf}}\hfill
    \subfigure[(d) OVSeg~\cite{liang2022open}]
{\includegraphics[width=0.166\linewidth]{iccv2023AuthorKit/figure/pdf/ovseg.pdf}}\hfill
    \subfigure[(e) \textbf{\ours (ours)}]
{\includegraphics[width=0.166\linewidth]{iccv2023AuthorKit/figure/pdf/ours.pdf}}\hfill
    \subfigure[(f) Ground-truth]
{\includegraphics[width=0.166\linewidth]{iccv2023AuthorKit/figure/pdf/gt.pdf}}\hfill
\\
\vspace{-10pt}
\caption{\textbf{Qualitative results on PASCAL-Context with 459 categories.} In comparison to the two-stage approach~\cite{liang2022open}, where each region of an image is independently classified using CLIP, our model excels at capturing the context of an image, as the CLIP image encoder can process the image as a whole, thereby allowing the CLIP to better capture the relationships and contextual information between objects.} 
\vspace{-10pt}

  \label{fig:qualitative}
\end{figure*}