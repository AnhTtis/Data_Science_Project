
\input{iccv2023AuthorKit/figure/tex/conceptual_qual}
\section{Introduction}

Open-vocabulary semantic segmentation aims to label each pixel in an image with class  categories given as textual descriptions, which may include classes that were not seen during training. To address this task, recent works~\cite{li2022language,ghiasi2022scaling,ding2022decoupling,liang2022open,xu2022simple} typically attempted to leverage large-scale pre-trained vision-language models, e.g., CLIP~\cite{radford2021learning} and ALIGN~\cite{jia2021scaling}, for their remarkable open-vocabulary recognition capabilities achieved from extensive training process involving large image-caption pairs scaling from millions to billions~\cite{schuhmann2022laion}. However, transferring the knowledge of these models trained with image-level supervision to the pixel-level prediction task is highly challenging~\cite{xu2022simple} due to the difference in granularity between image and pixel.


To address this challenge, recent methods~\cite{liang2022open,ding2022decoupling,xu2022simple} generate class-agnostic region proposals and use these regions as inputs to the CLIP encoder. Despite their appreciable performance, these proposals not only exhibit bias towards the training dataset~\cite{liang2022open} but are also processed independently by CLIP disregarding the global context within the image. Alternatively, MaskCLIP~\cite{zhou2022extract} modifies the last pooling layer within the CLIP encoder~\cite{radford2021learning} to obtain dense image embeddings. However, this approach can introduce substantial noise without a means for refinement, which may lead to erroneous predictions, as shown in Fig.~\ref{fig:intuition}. While it is a common practice to fine-tune the encoder to obtain task-specific features~\cite{long2015fully, he2017mask, chen2017deeplab} for mitigating this issue, it has been shown that using the conventional fine-tuning approach harms the capabilities of CLIP~\cite{zhou2022extract}. 

Moreover, existing methods~\cite{li2022language, ding2022decoupling, ghiasi2022scaling, liang2022open} often do not account for the additional challenges presented by open-vocabulary setups. In contrast to conventional segmentation settings~\cite{long2015fully, he2017mask, he2019adaptive, yu2020context,zhou2022rethinking}, where relational information among predefined classes can be used to better understand the scene during inference~\cite{zhou2022rethinking, he2019adaptive, jin2021mining, yuan2020object}, open-vocabulary setups assume arbitrary classes at inference time, preventing the models from learning such relational information without an explicit means for consideration.


To this end, given that the encoders of CLIP have been trained on a large-scale data that enables remarkable open-vocabulary capabilities~\cite{li2022language, ghiasi2022scaling, zhou2022extract,xu2022simple,liang2022open,ding2022decoupling}, we aim to effectively transfer their knowledge for the pixel-level prediction task, and attentively relate objects within the image to given class categories provided as textual descriptions by joint aggregation of both modalities. Furthermore, we aim to fine-tune the CLIP image encoder to enable the model to adapt to the pixel-level prediction task at hand.
However, due to the aforementioned issue that direct optimization degrades the open-vocabulary capabilities~\cite{zhou2022extract}, it is important to explore an alternative to optimize the image encoder of CLIP without hurting its pre-trained knowledge. 

Upon revisiting the training process of CLIP, we find that the cost volume, often defined as cosine-similarity map,
constructed between the image and text embeddings~\cite{radford2021learning} is used for loss computation. From this, based on an insight that preserving the use of cost volume could serve as an alternative approach, we propose to construct an image-text matching cost to explicitly represent their relations, and introduce a cost aggregation-based framework to process the cost volume as shown in Fig.~\ref{fig:intuition}. We demonstrate that this approach can effectively transfer the knowledge of CLIP without diminishing its open-vocabulary capabilities as shown in Fig.~\ref{fig:conceptual_qual}. 

Our network, namely \ours, consists of three components: first, we construct a cost volume using image and text embeddings; second, we employ a Transformer ~\cite{vaswani2017attention} based module that decomposes the aggregation process into spatial and class aggregation, aided by an additional technique called embedding guidance, to effectively aggregate the cost volume; and finally, we use a decoder to process the aggregated cost volume while capturing fine details. In addition, we introduce an efficient approach for training our network, where we only fine-tune the attention layers within the Transformer architecture. Our approach is not only efficient, but also more effective compared to the conventional method of fine-tuning all parameters.

In experiments, we evaluate \ours on various datasets, including ADE20K~\cite{zhou2019semantic}, PASCAL VOC~\cite{everingham2009pascal} and PASCAL-Context~\cite{mottaghi2014role}. \ours achieves state-of-the-art results, with a 22\% improvement in mIoU over the previous best result on ADE20K with 150 categories and a 73\% improvement on PASCAL-Context with 459 categories. We also provide extensive ablation studies and analyses.

