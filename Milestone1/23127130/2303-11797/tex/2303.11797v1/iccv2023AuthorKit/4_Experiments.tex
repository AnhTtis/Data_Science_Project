\input{iccv2023AuthorKit/table/main_table.tex}
\input{iccv2023AuthorKit/table/clipcats_ablation.tex}
\input{iccv2023AuthorKit/table/finetuning_ablation.tex}
\input{iccv2023AuthorKit/table/affinity_backbone_ablation}

\section{Experiments}
\subsection{Experimental Setup}
\paragraph{Datasets and evaluation metric.} 
We train our model on the COCO-Stuff dataset, which has 118k densely annotated training images with 171 categories, following \cite{liang2022open}. We evaluate our model on ADE20K~\cite{zhou2019semantic}, PASCAL VOC~\cite{everingham2009pascal}, and PASCAL-Context~\cite{mottaghi2014role} datasets. ADE20K has 20k training and 2k validation images, with two sets of categories: A-150 with 150 frequent classes and A-847 with 847 classes. PASCAL-Context contains 5k training and validation images, with 459 classes in the full version (PC-459) and the most frequent 59 classes in the PC-59 version. PASCAL VOC has 20 object classes and a background class, with 1.5k training and validation images. We report PAS-20 using 20 object classes. We also report the score for PAS-$20^b$, which defines the ``background" as classes present in PC-59 but not in PAS-20, as in \cite{ghiasi2022scaling}. We adopt mean Intersection over Union (mIoU) as evaluation metric for all experiments.

\vspace{-10pt}
\paragraph{Implementation details.}
We train the CLIP image encoder and the cost aggregation module with per-pixel binary cross-entropy loss and freeze the CLIP text encoder for all settings.
Feature embeddings $E^V$ are obtained from ResNet-101~\cite{he2016deep} pre-trained on the ImageNet-1k~\cite{deng2009imagenet} for model that uses CLIP with ViT-B~\cite{dosovitskiy2020image} and Swin-B~\cite{liu2021swin} pre-trained on the ImageNet-21k for models that use CLIP with either ViT-L, H or G~\cite{zhai2022scaling}.
We set $d_F=128$, $N_B=2$, $N_U=2$ for all of our models.
We implement our work using PyTorch~\cite{paszke2019pytorch} and Detectron2~\cite{wu2019detectron2}. AdamW~\cite{loshchilov2017decoupled} 
optimizer is used with a learning rate of $2\cdot10^{-4}$ for our model  and $2\cdot10^{-6}$ for the CLIP image encoder, with weight decay set to $10^{-4}$. The batch size is set to 4. We use 4 NVIDIA RTX 3090 GPUs for the model variant that utilizes ViT-B, L, H~\cite{dosovitskiy2020image} as the CLIP encoder, and use a single NVIDIA A100 for ViT-G~\cite{zhai2022scaling}. All of the models are trained for 80k iterations, which takes about 8 hours for model using ViT-L. Further details can be found in supplementary material. Our code and pre-trained weights will be made publicly available.




\subsection{Main Results}

In Table~\ref{tab:main_table}, we provide quantitative comparisons to competitors. For a fair comparison, we note the type of the vision-language model (VLM), the feature backbone, and the training dataset. We also specify the additional dataset if it is used. We first compare our method with those that employ ResNet-101 as the feature backbone and ViT-B as vision-language models. 

Overall, our method significantly outperforms all the other competitors, including  those~\cite{ghiasi2022scaling,liang2022open} that leverage additional dataset~\cite{chen2015microsoft,pont2020connecting} for further performance improvements. For methods that employ Swin-B as the feature backbone and ViT-L as vision-language models, our method surpasses the closest competitor by 20\% in A-847 and 65\% in PC-459.
Furthermore, if we choose to scale our VLM model to a larger variant, performance is further boosted. For CLIP variants introduced in~\cite{cherti2022reproducible}, \ie ViT-H and ViT-G, which are trained on LAION-2B~\cite{schuhmann2022laion}, our method enjoys further performance gains. These results confirm the validity of our approach to leverage cost aggregation, which offers an effective means of transferring knowledge from large-scale vision-language models.

We also present qualitative results of PASCAL-Context with 459 categories in Fig.~\ref{fig:qualitative}, demonstrating the efficacy of our proposed approach in comparison to the current state-of-the-art methods~\cite{ding2022decoupling, xu2022simple,liang2022open}. 

\subsection{Analysis and Ablation Study}
In this section, we provide ablation study and analysis to validate our choices. For all the experiments, we employ ViT-L variant for the CLIP encoder~\cite{radford2021learning} and Swin-B~\cite{liu2021swin} for the feature backbone if not mentioned.

\vspace{-10pt}
\paragraph{Component analysis.}
Table~\ref{tab:clipcats-ablation} shows the effectiveness of the main components within our architecture through quantitative results. 
First, we introduce the baseline models in \textbf{(I)} and \textbf{(II)}, which simply feed the feature embeddings or the cost volume to the proposed upsampling decoder. We then progressively add each proposed components from \textbf{(IV)} to \textbf{(VI}) to validate our approach to factorize the aggregation process into spatial and class aggregations, and the proposed embedding guidance. Note that for the design of \textbf{(III)}, we employ a Linear Transformer~\cite{katharopoulos2020transformers} that aggregates the entire input without separately processing spatial and class dimensions. For this analysis, we control the number of parameters to be similar across all the variants from \textbf{(III)} to \textbf{(VI}). 

As shown, we stress the gap between \textbf{(I)} and  \textbf{(II)}, which supports the findings presented in Fig.~\ref{fig:conceptual_qual}. 
We also highlight that as we gradually incorporate the proposed spatial and class aggregation techniques, our approach \textbf{(V)} outperforms \textbf{(III)}, demonstrating the effectiveness of our design.
Finally, \textbf{(VI)} shows that our embedding guidance further improves performance across most of the benchmarks, with the exception of PAS-20, where the error levels have become saturated as its class categories mostly overlap with the training dataset.


\input{iccv2023AuthorKit/figure/tex/inference_time}
\input{iccv2023AuthorKit/table/scaling_ablation.tex}
\input{iccv2023AuthorKit/table/cross_dataset_ablation.tex}

\vspace{-10pt}
\label{finetune}
\paragraph{Comparison among different fine-tuning approaches.}
In this ablation study, we examine both effectiveness and efficiency of the proposed approach to fine-tune the CLIP image encoder. In Table~\ref{tab:finetuning-ablation}, we report the results of different approaches, which include the variant without fine-tuning, adopting VPT~\cite{jia2022visual}, fine-tuning the entire network and our approach. We additionally show the comparison of time taken for training in Fig.~\ref{fig:inference-time}~(a). In short, we find that our proposed approach not only yields the best performance, but also requires less memory and training time than full fine-tuning, which highlights the effectiveness and efficiency of our approach.


\vspace{-10pt}
\paragraph{Feature backbone.}
Table~\ref{tab:affinity-backbone} presents a comparison of variants with different feature backbones for providing feature embeddings $E^V$. The first row reports the results using the CLIP image encoder~\cite{radford2021learning} whereas the second row reports the results using Swin-B~\cite{liu2021swin} as the feature backbone. The results indicate that the use of the CLIP image encoder generally reduces performance across all benchmarks, although it still outperforms other approaches~\cite{li2022language,xu2022simple, ding2022decoupling, liang2022open, ghiasi2022scaling}. In contrast, using a separate network, Swin-B~\cite{liu2021swin}, leads to the best performance. We attribute this trend to the same observation shown in Fig.~\ref{fig:conceptual_qual}, where the direct optimization of the CLIP image embeddings harms its open-vocabulary capability. However, we find that optimizing the CLIP image embedding through the cost volume can mitigate the performance degradation. To verify this, we include additional results in the last row that employ the CLIP image encoder as feature backbone, but prohibit gradient flow through the cost volume, allowing the gradient to flow only through the feature embedding to the encoder. The degradation in performance is found to be significant, indicating that using the cost volume can serve as an effective means to prevent such degradation.

\input{iccv2023AuthorKit/figure/tex/qualitative}
\input{iccv2023AuthorKit/figure/tex/partseg}

\vspace{-10pt}
\paragraph{Scaling comparison.}
In this analysis, we examine the scaling property of our method and compare with other recent methods~\cite{ding2022decoupling,xu2022simple} in Table~\ref{tab:scaling-ablation}. We use four variants of CLIP model, ViT-B, L, H and ViT-G, where ViT-B model has the least number of learnable parameters and ViT-G model has the most. As shown, we observe only marginal performance gains or even severe drops for competitors when we progressively increase the model capacity. In contrast, our proposed framework benefits and shows apparent improvements on almost all the datasets, which indicate the favorable scaling capacity of our work. Notably, on A-150 and A-847, our work achieves mIoU improvements of +9.0 and +4.9 over the ViT-B model, respectively.
\vspace{-10pt}
\paragraph{Training with various datasets.}
In this experiment, we further examine the generalization power of our method in comparison to other methods~\cite{ding2022decoupling, xu2022simple} by training our model on smaller-scale datasets, which include A-150 and PC-59, that poses additional challenges to achieve good performance.  The results are shown in Table~\ref{tab:cross-dataset-ablation}. As shown, we find that although we observe some performance drops, which seems quite natural when smaller dataset is used, our work significantly outperforms other competitors. These results highlight the strong generalization power of our framework, which is a favorable characteristic that suggests practicality of our approach.

\vspace{-10pt}
\paragraph{Inference speed comparison.}
In Fig.~\ref{fig:inference-time}~(b), we visualize the run-time comparison of different CLIP ViT variants~\cite{radford2021learning}, and compare our results with a recent  two-stage framework~\cite{ding2022decoupling}. Note that as the run-time of our method resort on the number of classes at inference, which results in different run-time for each dataset, we report the mean value of the run-time. As illustrated,  our approach generally runs faster than the competitor. We highlight that as we scale the model size, our approach increasingly benefits from fast inference time, which is favorable for practicality.


\subsection{Generalization to Part Segmentation} In Fig.~\ref{fig:partseg},  we show that our framework can generalize to part segmentation task, even when fine-grained categories that were not observed during training are given as inputs.
We compare with state-of-the-art two-stage approach~\cite{liang2022open}.
Interestingly, \cite{liang2022open} fails to account for fine-grained, unseen categories, as its mask proposals exhibit bias towards its training dataset. For example, as ``person" seen during training, it can only identify person as a whole, failing to segment fine-grained categories such as ``arm" and ``leg". In contrast, our framework does not suffer from such bias, enabling robust recognition of unseen objects of different granularity.