\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
   \begin{tabular}{ll|cccccc|cc}
        \toprule
        &\multirow{2}{*}{Methods} & \multirow{2}{*}{A-847} & \multirow{2}{*}{PC-459} & \multirow{2}{*}{A-150} & \multirow{2}{*}{PC-59}& \multirow{2}{*}{PAS-20} & \multirow{2}{*}{$\textnormal{PAS-20}^b$} &\#param.  & Memory
         \\
         &&&&&&&&(M)&(GiB)
         \\
        \midrule\midrule
        \textbf{(I)} &Freeze & 4.8& 8.7& 20.8& 52.5& 84.5& 76.7& 0 & 1.9\\
        \textbf{(II)} &VPT~\cite{jia2022visual}  & 8.3& 13.7& 29.4& 59.5 & 90.8 & 81.0 &2.3 & 2.0\\
        \textbf{(III)} &Full F.T.  & \underline{9.4}& \underline{18.5}& \underline{30.2}& \underline{61.0}& \textbf{96.8}& \underline{81.5} & 290.0 & 5.4\\
        \hlrow \textbf{(IV)} &Ours  & \textbf{10.8}& \textbf{20.4}& \textbf{31.5}& \textbf{62.0}& \underline{96.6}& \textbf{81.8} & 96.7 & 3.1\\
        \bottomrule
\end{tabular}}%
\vspace{-5pt}
        \caption{\textbf{Ablation study of fine-tuning methods for CLIP image encoder.} We additionally note the number of learnable parameters of CLIP and memory consumption during training. Our method not only outperforms full fine-tuning, but also requires a smaller computational footprint. \textit{F.T.: Fine-Tune}}
    \label{tab:finetuning-ablation}
    \vspace{-5pt}
\end{table}
