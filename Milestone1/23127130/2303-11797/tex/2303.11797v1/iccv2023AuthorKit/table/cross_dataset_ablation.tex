\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|c|ccccccc}
    \toprule
        Methods & Training dataset & A-847 & PC-459 & A-150 & PC-59 & PAS-20 & $\textnormal{PAS-20}^b$
        \\
        \midrule\midrule
        ZegFormer~\cite{ding2022decoupling} & COCO-Stuff & 5.6 & \underline{10.4} & 18.0 & 45.5 & \underline{89.5} & 65.5\\
        ZSseg~\cite{xu2022simple} & COCO-Stuff & \underline{7.0} & 9.0 & \underline{20.5} & \underline{47.7} & 88.4 & \underline{67.9}\\
        \hlrow \ours (ours) & COCO-Stuff & \textbf{8.4} & \textbf{16.6} & \textbf{27.2} & \textbf{57.5} & \textbf{93.7} & \textbf{78.3}\\
        \midrule
        ZegFormer~\cite{ding2022decoupling} & A-150 & 6.8 & \underline{7.1} & \color{gray}{33.1} & 34.7 & 77.2 & 53.6 \\
        ZSseg~\cite{xu2022simple} & A-150 & \underline{7.6} & \underline{7.1} & \color{gray}{40.3} & \underline{39.7} & \underline{80.9} & \underline{61.1}\\
        \hlrow \ours (ours) & A-150 & \textbf{10.6} & \textbf{14.5} & \color{gray}{46.8} & \textbf{46.7} & \textbf{85.5} & \textbf{70.3} \\
        \midrule
        ZegFormer~\cite{ding2022decoupling} & PC-59 & \underline{3.8} & \underline{8.2} & \underline{13.1} & \color{gray}{48.7} & 86.5 & 66.8 \\
        ZSseg~\cite{xu2022simple} & PC-59 & 3.0 & 7.6 & 11.9 & \color{gray}{54.7} & \underline{87.7} & \underline{71.7}\\
        \hlrow \ours (ours) & PC-59 & \textbf{5.6} & \textbf{12.9} & \textbf{23.0} & \color{gray}{62.4} & \textbf{87.3} & \textbf{79.0} \\
        \bottomrule
    \end{tabular}
    }
    
    \vspace{-5pt}
    \caption{\textbf{Training on various datasets.} CLIP with ViT-B is used for all methods. Our model demonstrates remarkable generalization capabilities even on relatively smaller datasets. The scores evaluated on the same dataset used for training are colored in \textcolor{gray}{gray} for clarity.}
    \label{tab:cross-dataset-ablation}
    \vspace{-5pt}
\end{table}
