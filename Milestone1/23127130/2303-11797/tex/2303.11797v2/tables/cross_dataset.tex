\begin{table}[!t]
    \centering
    
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|c|ccccccc}
    \toprule
        Methods & Training dataset & A-847 & PC-459 & A-150 & PC-59 & PAS-20 & $\textnormal{PAS-20}^b$
        \\
        \midrule\midrule
        ZegFormer & COCO-Stuff & 5.6 & \underline{10.4} & 18.0 & 45.5 & \underline{89.5} & 65.5\\
        ZSseg & COCO-Stuff & \underline{7.0} & 9.0 & \underline{20.5} & \underline{47.7} & 88.4 & \underline{67.9}\\
        \hlrow \ours (ours) & COCO-Stuff & \textbf{12.0} & \textbf{19.0} & \textbf{31.8} & \textbf{57.5} & \textbf{94.6} & \textbf{77.3}\\
        \midrule
        ZegFormer & A-150 & 6.8 & \underline{7.1} & \color{gray}{33.1} & 34.7 & 77.2 & 53.6 \\
        ZSseg & A-150 & \underline{7.6} & \underline{7.1} & \color{gray}{40.3} & \underline{39.7} & \underline{80.9} & \underline{61.1}\\
        \hlrow \ours (ours) & A-150 & \textbf{14.4} & \textbf{16.2} & \color{gray}{47.7} & \textbf{49.9} & \textbf{91.1} & \textbf{73.4} \\
        \midrule
        ZegFormer & PC-59 & \underline{3.8} & \underline{8.2} & \underline{13.1} & \color{gray}{48.7} & 86.5 & 66.8 \\
        ZSseg & PC-59 & 3.0 & 7.6 & 11.9 & \color{gray}{54.7} & \underline{87.7} & \underline{71.7}\\
        \hlrow \ours (ours) & PC-59 & \textbf{9.6} & \textbf{16.7} & \textbf{27.4} & \color{gray}{63.7} & \textbf{93.5} & \textbf{79.9} \\
        \bottomrule
    \end{tabular}
    }

    
    \vspace{-5pt}
    \caption{\textbf{Training on various datasets.} CLIP with ViT-B is used for all methods. Our model demonstrates remarkable generalization capabilities even on relatively smaller datasets. The scores evaluated on the same dataset used for training are colored in \textcolor{gray}{gray}.}
    \vspace{-10pt}
    \label{tab:cross-dataset-ablation}
\end{table}
