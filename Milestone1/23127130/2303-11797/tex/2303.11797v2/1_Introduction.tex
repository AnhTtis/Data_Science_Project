\section{Introduction}

Open-vocabulary semantic segmentation aims to assign each pixel in an image to a class label from an unbounded range, defined by text descriptions. To handle the challenge of associating an image with a wide variety of text descriptions, pre-trained vision-language foundation models, \eg, CLIP~\cite{radford2021learning} and ALIGN~\cite{jia2021scaling}, have drawn attention as they exerted strong open-vocabulary recognition capabilities achieved through training on extensive image-text datasets. Nonetheless, these foundation models primarily receive image-level supervision during training, which introduces a notable disparity when applying them to the pixel-level segmentation tasks~\cite{zhou2022extract}. 

To address this gap, recent works~\cite{ding2022decoupling,ghiasi2022scaling,xu2022simple, liang2022open, xu2023open, xu2023side, yu2023convolutions} have reformulated the task into a region-level problem by utilizing mask proposal generators. While this partially bridges the discrepancy between the pre-training and the downstream task, a discernible gap persists between the conceptualization of regions and the entire image for CLIP. 

\input{figures/fig_motivation/motivation}

In this work, we investigate methods to transfer the holistic understanding capability of images to the pixel-level task of segmentation. 
While a straightforward approach would be to fine-tune the encoders of CLIP, existing methods struggle in such attempt~\cite{zhou2022extract,yu2023convolutions,xu2023side} as they encounter significant overfitting problems to the seen classes.
This results in the misalignment of the joint embedding space for unseen classes, as the CLIP features undergo decoder modules for aggregating them into segmentation masks, hence losing their alignment.
Consequently, most methods~\cite{ding2022decoupling,ghiasi2022scaling,xu2022simple, liang2022open, xu2023open, xu2023side, yu2023convolutions} opt for freezing the encoders of CLIP instead, remaining the challenge underexplored.

In this regard, we extend the exploration of adapting CLIP for open-vocabulary semantic segmentation and introduce a novel cost-based framework. We propose to aggregate the cosine similarity between image and text embeddings of CLIP, \textit{i.e.}, the matching cost, drawing parallels to the visual correspondence literature~\cite{kendall2017end}. Surprisingly, we find that fine-tuning CLIP upon this framework effectively adapts CLIP to the downstream task of segmentation for both seen and unseen classes, as shown in Fig.~\ref{fig:motivation}. Noticing this, we delve into better aggregating the cost volume between image and text for segmentation.

Intuitively, the cost volume can be viewed as rough semantic masks grounded to their respective classes, as illustrated in Fig.~\ref{fig:intuition}. Subsequently, these rough masks can be further refined to obtain accurate predictions, being the cost aggregation process. In light of this, we aim to effectively aggregate the cost volume and configure the process into spatial and class aggregation, regarding its multi-modal nature from being established between image and text. Furthermore, by observing the effectiveness of fine-tuning CLIP for its adaptation to semantic segmentation, we explore various methods to facilitate this process efficiently.

We analyze our cost aggregation framework to be advantageous in two aspects for adapting CLIP to dense prediction: \textit{i}) the robustness of cost aggregation against overfitting, and \textit{ii}) the direct construction of the cost volume from image and text embeddings of CLIP. For cost aggregation, the aggregation layers operate upon similarity scores, preventing them from overfitting to the features~\cite{cai2020matching,song2021adastereo,liu2022graftnet}. Moreover, as opposed to existing methods where they often employ decoder layers upon the image embeddings of CLIP~\cite{zhou2022extract, yu2023convolutions}, we do not introduce additional layers that can potentially project the embeddings to a different embedding space. 

Our framework, dubbed \ours, combines our cost aggregation-based framework consisting of spatial and class aggregation, with our optimal approach for fine-tuning the encoders of CLIP. We achieve state-of-the-art results on every standard open-vocabulary benchmark with large margins, gaining +3.6 mIoU in A-847 and +8.1 mIoU in PC-459 compared to the recent state-of-the-art. Not only \ours it is effective, but is also efficient both for training and inference compared to region-text methods, being over $\times$3.7 faster for inference. Furthermore, even in the extreme scenario~\citep{blumenstiel2023mess} where the domain of the image and text description differs significantly from the training dataset, our model outperforms existing state-of-the-art methods with a large margin, paving the way for various domain-specific applications. 

We summarize our contribution as follows:
\begin{itemize}
  \item We propose a cost aggregation-based framework for open-vocabulary semantic segmentation, effectively adapting CLIP to the downstream task of segmentation by fine-tuning its encoders. 
  \item To aggregate the image-text cost volume, we consist of our framework with spatial and class aggregation to reason the multi-modal cost volume and explore various methods to enhance our cost aggregation framework.
  \item Our framework, named \ours, establishes state-of-the-art performance for standard open-vocabulary benchmarks, as well as for extreme case scenarios~\cite{blumenstiel2023mess}, demonstrating versatility and practicality.
\end{itemize}

\input{figures/fig_cost/fig_cost}
