
\begin{figure*}[t]
    \centering
    \Large \textbf{CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation}\\[5pt]
    \large \textbf{-- Supplementary Material --}

    
    \hsize=1.0\textwidth
    \includegraphics[width=\textwidth]{supple/architecture.pdf}
    \vspace{-10pt}
    \caption{\textbf{More architectural details of CAT-Seg:} (a) overall architecture. (b) embedding guidance. Note that a generalized embedding guidance is illustrated to include different attention designs, \ie, shifted window attention~\cite{liu2021swin} or linear attention~\cite{katharopoulos2020transformers}. (c) upsampling decoder layer. GN: Group Normalization~\cite{wu2018group}. LN: Layer Normalization~\cite{ba2016layer}.}
    \label{fig:guidance-architecture}
    \vspace{-10pt}
\end{figure*}

\renewcommand{\thesection}{\Alph{section}}
\setcounter{section}{0}


In the following, we provide the full results from MESS~\cite{blumenstiel2023mess} in Section~A. We further provide implementation details in Section~B. We then provide additional experimental results and ablation study in Section~C. Finally, we present qualitative results for the benchmarks in Section~D and a discussion of limitations in Section~E.





\section{More Results}\label{A}
\input{supple/mess_full}
\paragraph{Full quantitative results on MESS benchmark.}
In Table~\ref{tab:mess_full}, we provide the results of all 22 datasets within MESS~\cite{blumenstiel2023mess}, including results from Grounded-SAM~\cite{Grounded-SAM_Contributors_Grounded-Segment-Anything_2023}.


\section{More Details}\label{B}
\subsection{Architectural Details}

In the following, we provide more architectural details. Our detailed overall architecture is illustrated in Fig.~\ref{fig:guidance-architecture} (a).

\smallbreak
\noindent\textbf{Embedding guidance.}
In this paragraph, we provide more details of embedding guidance, which is designed to facilitate the cost aggregation process by exploiting its rich semantics for a guidance.  We first extract visual and text embeddings from CLIP encoders~\cite{radford2021learning}. The embeddings then undergo linear projection and concatenated to the cost volume before query and key projections in aggregation layer. The design is illustrated in Fig.~\ref{fig:guidance-architecture}~(b).

\smallbreak
\noindent\textbf{Upsampling decoder.}
The detailed architecture is illustrated in Fig.~\ref{fig:guidance-architecture}(c). In our upsampling decoder, we start by taking high-resolution features from the CLIP ViT model~\cite{dosovitskiy2020image}. We then apply a single transposed convolution layer to these extracted features to generate an upsampled feature map. Initially, the extracted feature maps have a resolution of $24\times 24$ pixels. However, after processing them with the transposed convolution operation, we increase their resolution to $48\times 48$ pixels for the first feature map, denoted as $E^V_{Dec,1}$, and to $96\times 96$ pixels for the second feature map, denoted as $E^V_{Dec,2}$.

To obtain $E^V_{Dec,1}$, we utilize the output of the 8th layer for the ViT-B/16 model, and for the ViT-L/14 model, we use the output of the 16th layer. For the extraction of $E^V_{Dec,2}$, we employ shallower features: the output of the 4th layer for the ViT-B/16 model as a VLM, and the output of the 8th layer for the ViT-L/14 model. These features are employed to enhance cost embeddings with fine details using a U-Net-like architecture~\cite{ronneberger2015u}.


\subsection{Other Implementation Details}
\smallbreak
\noindent\textbf{Training details.}
A resolution of $H=W=24$ is used during training for constructing cost volume. The position embeddings of the CLIP image encoder is initialized with bicubic interpolation~\cite{touvron2021training}, and we set training resolution as $384\times 384$. For ViT-B and ViT-L variants, we initialize CLIP~\cite{radford2021learning} with official weights of ViT-B/16 and ViT-L/14@336px respectively. All hyperparameters are kept constant across the evaluation datasets.  

\smallbreak
\noindent\textbf{Text prompt templates.}
To obtain text embeddings from the text encoder, we form sentences with the class names, such as \texttt{"A photo of a \{class\}"}. We do not explore handcrafted prompts in this work, but it is open for future investigation. %

\input{supple/mess_details}

\input{supple/patch_inference}
\subsection{Patch Inference}
The practicality of Vision Transformer (ViT)~\cite{dosovitskiy2020image} for high-resolution image processing has been limited due to its quadratic complexity with respect to the sequence length. As our model leverages ViT to extract image embeddings, \ours may struggle to output to the conventional image resolutions commonly employed in semantic segmentation literature, such as $640\times 640$~\cite{cheng2021per,ghiasi2022scaling}, without sacrificing some accuracy made by losing some fine-details. Although we can adopt the same approach proposed in~\cite{zhou2022extract} to upsample the positional embedding~\cite{zhou2022extract}, we ought to avoid introducing excessive computational burdens, and thus adopt an effective inference strategy without requiring additional training which is illustrated in Fig.~\ref{fig:patch-inference}.

To this end, we begin by partitioning the input image into overlapping patches of size $\frac{H}{N_P} \times \frac{W}{N_P}$. Intuitively, given an image size of $640 \times 640$, we partition the image to sub-images of size $384 \times 384$, which matches the image resolution at training phase, and each sub-images has overlapping regions $128 \times 128$. Subsequently, we feed these sub-images and the original image that is resized to $384 \times 384$ into the model. Given the results for each patches and the image, we merge the obtained prediction, while the overlapping regions are averaged to obtain the final prediction. In practice, we employ $N_P=2$,  while adjusting the overlapping region to match the effective resolution of $640\times 640$.

\subsection{More Details of MESS Benchmark}

In Table~\ref{tab:mess-detail}, we provide details of the datasets in the MESS benchmark~\cite{blumenstiel2023mess}.

\section{Additional Ablation Study}\label{C}

\subsection{Ablation Study of Inference Strategy}\vspace{-10pt}
\input{supple/inference_ablation}
Table~\ref{tab:inference-ablation} presents effects of different inference strategies for our model. The first row shows the results using the training resolution at inference time. The last row adopts the proposed patch inference strategy. It is shown that our proposed approach can bring large performance gains, compared to using the training resolution.

\subsection{Ablation on VLM}
\input{supple/vlm}
Table~\ref{tab:vlms} shows the results with various VLMs. We found that \ours can be applied to various VLMs, and better results can be obtained when a more powerful model is applied.

\section{More Qualitative Results}\label{D}
We provide more qualitative results on A-847~\cite{zhou2019semantic} in Fig.~\ref{fig:ade847}, PC-459~\cite{mottaghi2014role} in Fig.~\ref{fig:pc459}, A-150~\cite{zhou2019semantic} in Fig.~\ref{fig:ade150}, and PC-59~\cite{mottaghi2014role} in Fig.~\ref{fig:pc59}. We also further compare the results in A-847~\cite{zhou2019semantic} with other methods~\cite{ding2022decoupling, xu2022simple, liang2022open} in Fig.~\ref{fig:ade847-comparison}.


\section{Limitations}\label{E}
To evaluate open-vocabulary semantic segmentation results, we follow ~\cite{ghiasi2022scaling, liang2022open} and compute the metrics using the other segmentation datasets. However, since the ground-truth segmentation maps involve some ambiguities, the reliability of  the evaluation dataset is somewhat questionable. %
Constructing a more reliable dataset including ground-truths accounting for above issue for accurate evaluation is an intriguing topic. 


\input{supple/ade847}
\input{supple/pc459}
\input{supple/ade150}
\input{supple/pc59}
\input{supple/supp_comparison}

\clearpage
\clearpage
