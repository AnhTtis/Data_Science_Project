\section{Methodology}

\input{figures/fig_overall/overall}
Given an image $I$ and a set of candidate class categories $\mathcal{C}=\{T(n)\} \text{ for } n=1,\dots ,N_{\mathcal{C}}$, where $T(n)$ denotes textual description of $n$-th category and $N_\mathcal{C}$ is the number of classes, open-vocabulary semantic segmentation assigns a class label for each pixel in image $I$.
Different from classical semantic segmentation tasks~\cite{long2015fully, he2017mask, zhou2022rethinking, he2019adaptive, jin2021mining, yu2020context, yuan2020object}, open-vocabulary segmentation is additionally challenged by varying $\mathcal{C}$, given as free-form text description.

In this section, we describe our cost-based approach for open-vocabulary semantic segmentation. In specific, we refine the cosine-similarity scores from image and text embedding of CLIP, as illustrated in Fig.~\ref{fig:intuition}. The process of refining the cosine-similarity scores, or cost aggregation~\cite{kendall2017end}, was initially developed for the image correspondence problem and specifically designed to process an image-to-image cost volume. Consequently, traditional cost aggregation methods leverage image-specific priors, such as the assumption of local smoothness of images~\cite{min2021convolutional, min2021hypercorrelation, lee2021deep} for aggregating the cost volume.

On the other hand, we aim to aggregate the image-to-text cost volume, hence need to consider the multi-modality of the cost volume and the respective characteristics of each modality. 
In this regard, as shown in Fig.~\ref{fig:overall}, we break down the aggregation stage into two separate modules, \ie, spatial and class aggregation, reasonably addressing the unique challenges presented by the task of open-vocabulary semantic segmentation. This includes aspects such as handling varying numbers of classes during inference and guaranteeing the permutation invariance between classes. Specifically, we perform spatial aggregation followed by class aggregation and alternate both aggregations. In the following section, we describe the cost aggregation process in detail, as well as introduce additional techniques for enhancing the cost aggregation framework.


\subsection{Cost Computation and Embedding}\label{sec:cost-computation}
Given an image $I$ and a set of classes $\mathcal{C}$, we extract the dense image embeddings $D^V = \Phi^V(I) \in \mathbb{R}^{(H \times W )\times d}$ and the text embeddings $D^L = \Phi^L(T) \in \mathbb{R}^{N_{\mathcal{C}}\times d}$, where $\Phi^V(\cdot)$ and  $\Phi^L(\cdot)$, denotes the image and text encoders of CLIP respectively. For extracting dense CLIP image embeddings, we follow the method described in \cite{zhou2022extract}, wherein we modify the last attention layer of the image encoder to eliminate the pooling effect.
We use the image and text embeddings $D^V(i)$ and $D^L(n)$, where $i$ denotes 2D spatial positions of the image embedding and $n$ denotes an index for a class, to compute a cost volume $C\in\mathbb{R}^{(H\times W)\times N_\mathcal{C}}$ by cosine similarity~\cite{rocco2017convolutional}. 
Formally, this is defined as:
\begin{equation}
  {C}(i,n)=
    \frac{D^V(i)\cdot D^L(n)}{\|D^V(i)\|\|D^L(n)\|}.
\end{equation}
To enhance the processing of cost in high dimensional feature space, we feed the cost volume to a single convolution layer that processes each cost slice $C(:, n)\in \mathbb{R}^{(H\times W)\times 1}$ independently to obtain initial cost volume embedding $F\in \mathbb{R}^{(H\times W)\times N_\mathcal{C} \times d_F}$, where $d_F$ is the cost embedding dimension, as shown in Fig.~\ref{fig:overall}. 

\subsection{Spatial Cost Aggregation} \label{sec:spatial-aggregation}
For spatial aggregation, we aim to consider the characteristics of images within the image-text cost volume, such as spatial smoothness within the image. Specifically, we apply spatial aggregation for each class, respectively. Considering that we pursue the holistic understanding of images of CLIP to effectively transfer to segmentation, we adopt Transformer~\cite{vaswani2017attention,liu2021swin} over CNNs for its global~\cite{vaswani2017attention} or semi-global~\cite{liu2021swin,hong2022cost} receptive fields. In practice, we employ Swin Transformer~\cite{liu2021swin} for computational efficiency. We define this process as follows:
\begin{equation}
    {F}'(:, n) = \mathcal{T}^\mathrm{sa}(F(:, n)), 
    \label{eq:spatial-aggregation}
\end{equation}
where $F(:,n)\in \mathbb{R}^{(H\times W)\times d_F}$, and $\mathcal{T}^\mathrm{sa}(\cdot)$ denotes a pair of two consecutive Swin transformer block for spatial aggregation, where the first block features self-attention within a local window, followed by the second block with self-attention within shifted window. Note that we treat $d_F$ as channel dimensions for each token, and attention is computed within individual classes separately. Intuitively, we can roughly relate the process of spatial aggregation to the bottom row of Fig.~\ref{fig:intuition}, where the cost volume for ``sofa" is well-refined after aggregation, and the noise in the background region is suppressed.

\subsection{Class Cost Aggregation} \label{sec:class-aggregation}
Subsequent to spatial aggregation, class aggregation is applied to consider the text modality, explicitly capturing relationships between different class categories. We also consider the unique challenges of open-vocabulary semantic segmentation of handling varying numbers of categories $\mathcal{C}$ while being invariant to their ordering. To address these challenges, we employ a Transformer~\cite{vaswani2017attention} layer without position embedding for aggregation, as this can achieve both of the aforementioned criteria. This process is defined as: 
\begin{align}
    {F}''(i, :) = \mathcal{T}^\mathrm{ca}({F}'(i, :)),
    \label{eq:class-aggregation}
\end{align}
where $F'(i,:)\in \mathbb{R}^{N_{\mathcal{C}}\times d_F}$, and $\mathcal{T}^\mathrm{ca}(\cdot)$ denotes a transformer block for class aggregation. 
In contrast to spatial aggregation, we instead employ a linear transformer~\cite{katharopoulos2020transformers} as we do not need to consider spatial structure of the input tokens in this aggregation, as well as benefitting from the linear computational complexity with respect to the number of the tokens. The class aggregation process can be related to the top row of Fig.~\ref{fig:intuition}, where the aggregated cost volume depicts its prediction to only chairs and excluding the sofa, as both classes are given together for reasoning.


\subsection{\ours Framework} \label{upsampling}
Upon the aggregated cost volume through spatial and class aggregation, we further enhance our methodology by incorporating an upsampling and aggregation process to derive semantic segmentation predictions. Additionally, drawing insights from state-of-the-art cost aggregation techniques~\cite{cho2021cats, cho2022cats++, hong2022cost}, we refine our cost aggregation strategy by leveraging guidance derived from the embeddings of CLIP. Finally, we examine various methods to fine-tune the encoders of CLIP, in pursuit of effectively, yet efficiently adapting CLIP for open-vocabulary semantic segmentation. Altogether, we introduce \textbf{C}ost \textbf{A}ggrega\textbf{T}ion approach for open-vocabulary semantic \textbf{Seg}mentation (\ours). 
We describe the upsampling decoder, embedding guidance, and our fine-tuning approach in detail in the subsequent sections. For detailed illustrations of the architecture for each component, please refer to the supplementary materials.

\vspace{-10pt}
\paragraph{Upsampling decoder.} 
Similar to FPN~\cite{lin2017feature}, we employ bilinear upsampling on the aggregated cost volume and concatenate it with the corresponding level of feature map extracted from CLIP, followed by a convolutional layer with a 3$\times$3 kernel of fixed size. We iterate this process $N_U$ times, generating a high-resolution output which is fed into the prediction head for final inference.
To extract the high-resolution feature map, we avoid using an additional feature backbone that would introduce a heavy computational burden. Instead, similarly to~\cite{li2022exploring}, we extract these maps from the middle layers of the CLIP image encoder. Specifically, we extract the feature map from the output of intermediate layers of CLIP ViT~\cite{dosovitskiy2020image} and then upsample them using a single learnable transposed convolution layer. This approach allows us to efficiently leverage the well-learned representations of CLIP for obtaining detailed predictions. For additional details, refer to the supplementary materials.

\vspace{-10pt}
\paragraph{Embedding guidance.}
As a means to enhance the cost aggregation process, we additionally leverage the embeddings $D_L\text{ and }D_V$ to provide spatial structure or contextual information of the inputs. Intuitively, we aim to guide the process with embeddings, based on the assumption that visually or semantically similar input tokens, e.g., color or category, have similar matching costs, inspired by cost volume filtering~\cite{hosni2012fast,sun2018pwc} in stereo matching literature~\cite{scharstein2002taxonomy}.
Accordingly, we redefine Eq.~\ref{eq:spatial-aggregation} and Eq.~\ref{eq:class-aggregation} as:  
\begin{equation}
\begin{split}
    &{F}'(:, n) = \mathcal{T}^\mathrm{sa}([F(:, n);\mathcal{P}^V(D^V)]),\\
    &{F}''(i,:) = \mathcal{T}^\mathrm{ca}([F'(i,:);\mathcal{P}^L(D^L)]),
\end{split}
\end{equation}
where $[\cdot]$ denotes concatenation, $\mathcal{P}^V$ and $\mathcal{P}^L$ denote linear projection layer, $D^V\in \mathbb{R}^{(H\times W)\times d}$, and $D^L\in \mathbb{R}^{N_{\mathcal{C}}\times d}$, where $d$ denotes the feature dimension. Notably, we only provide the embeddings to query and key as we find this is sufficient for embedding guidance. 

\vspace{-10pt}
\paragraph{Efficient fine-tuning of CLIP} 
While we aim to fully adapt CLIP to the downstream task through fine-tuning its image and text encoders, fine-tuning such foundation models can scale up to hundreds of millions of parameters, being computationally expensive and memory-intensive. On the other hand, freezing some of its layers, not only would be more efficient but also can help CLIP preserve its original embedding space, allowing it to be more robust to overfitting. To this end, we extensively investigate which layers should be frozen within CLIP~\cite{dosovitskiy2020image}, among examining various approaches for fine-tuning pre-trained models. We provide a detailed analysis of our exploration in Sec.~\ref{sec:ablation}. 

