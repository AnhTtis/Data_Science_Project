\input{tables/main_table}
\input{tables/mess_table}

\section{Experiments}
\subsection{Datasets and Evaluation}
We train our model on the COCO-Stuff~\cite{caesar2018coco}, which has 118k densely annotated training images with 171 categories, following \cite{liang2022open}. We employ the mean Intersection-over-Union (mIoU) as the evaluation metric for all experiments. For the evaluation, we conducted experiments on two different sets of datasets~\cite{zhou2019semantic,everingham2009pascal,mottaghi2014role}: a commonly used in-domain datasets~\cite{ghiasi2022scaling}, and a multi-domain evaluation set~\cite{blumenstiel2023mess} containing domain-specific images and class labels. 

\vspace{-10pt}
\paragraph{Datasets for standard benchmarks.} 
For in-domain evaluation, we evaluate our model on ADE20K~\cite{zhou2019semantic}, PASCAL VOC~\cite{everingham2009pascal}, and PASCAL-Context~\cite{mottaghi2014role} datasets. ADE20K has 20k training and 2k validation images, with two sets of categories: A-150 with 150 frequent classes and A-847 with 847 classes~\cite{ding2022decoupling}. PASCAL-Context contains 5k training and validation images, with 459 classes in the full version (PC-459) and the most frequent 59 classes in the PC-59 version. PASCAL VOC has 20 object classes and a background class, with 1.5k training and validation images. We report PAS-20 using 20 object classes. We also report the score for PAS-$20^b$, which defines the ``background" as classes present in PC-59 but not in PAS-20, as in \citet{ghiasi2022scaling}.

\vspace{-10pt}
\paragraph{Datasets for multi-domain evaluation.}
We conducted a multi-domain evaluation on the MESS benchmark~\cite{blumenstiel2023mess}, specifically designed to stress-test the real-world applicability of open-vocabulary models with 22 datasets. The benchmark includes a wide range of domain-specific datasets from fields such as earth monitoring, medical sciences, engineering, agriculture, and biology. Additionally, the benchmark contains a diverse set of general domains, encompassing driving scenes, maritime scenes, paintings, and body parts. We report the average scores for each domain in the main text for brevity. For the complete results and details of the 22 datasets, please refer to the supplementary material.

\subsection{Implementation Details}
We train the CLIP image encoder and the cost aggregation module with per-pixel binary cross-entropy loss. We set $d_F=128$, $N_B=2$, $N_U=2$ for all of our models. We implement our work using PyTorch~\cite{paszke2019pytorch} and Detectron2~\cite{wu2019detectron2}. AdamW~\cite{loshchilov2017decoupled} 
optimizer is used with a learning rate of $2\cdot10^{-4}$ for our model  and $2\cdot10^{-6}$ for the CLIP, with weight decay set to $10^{-4}$. The batch size is set to 4. We use 4 NVIDIA RTX 3090 GPUs for training. All of the models are trained for 80k iterations. 

\subsection{Main Results}
\paragraph{Results of standard benchmarks.}
The evaluation of standard open-vocabulary semantic segmentation benchmarks is shown in Table~\ref{tab:main_table}. Overall, our method significantly outperforms all competing methods, including those~\cite{ghiasi2022scaling,liang2022open} that leverage additional datasets~\cite{chen2015microsoft,pont2020connecting} for further performance improvements. To ensure a fair comparison, we categorize the models based on the scale of the vision-language models (VLMs) they employ. First, we present results for models that use VLMs of comparable scale to ViT-B/16~\cite{dosovitskiy2020image}, and our model surpasses all previous methods, even achieving performance that matches or surpasses those using the ViT-L/14 model as their VLM~\cite{xu2023side}.
For models employing the ViT-L/14 model as their VLM, our model demonstrates remarkable results, achieving a 16.0 mIoU in the challenging A-847 dataset and a 23.8 mIoU in PC-459. These results represent a 29\% and 52\% increase, respectively, compared to the previous state-of-the-art.
We also present qualitative results of PASCAL-Context with 459 categories in Fig.~\ref{fig:qualitative}, demonstrating the efficacy of our proposed approach in comparison to the current state-of-the-art methods~\cite{ding2022decoupling, xu2022simple,liang2022open}. 


\input{figures/fig4/fig4}
\input{tables/feature_cost}


\vspace{-10pt}
\paragraph{Results of multi-domain evaluation.}
In Table~\ref{tab:mess}, we present the qualitative results obtained from the MESS benchmark~\cite{blumenstiel2023mess}. This benchmark assesses the real-world performance of a model across a wide range of domains. Notably, our model demonstrates a significant performance boost over other models, achieving the highest mean score. It particularly excels in the general domain as well as in agriculture and biology, showing its strong generalization ability. However, in the domains of medical sciences and engineering, the results exhibit inconsistencies with respect to the size of the VLM. Additionally, the scores for medical sciences are comparable to random predictions. We speculate that CLIP may have limited knowledge in these particular domains~\cite{radford2021learning}.


\subsection{Analysis and Ablation Study}\label{sec:ablation}

\paragraph{Comparison between feature and cost aggregation.} We provide quantitative and qualitative comparison of two aggregation baselines, feature aggregation, and cost aggregation, in Table~\ref{tab:feature-vs-cost}. For both of baseline architectures, we simply apply the upsampling decoder and note that both methods share most of the architecture, but differ in whether they aggregate the concatenated features or aggregate the cosine similarity between image and text embeddings of CLIP.
\input{figures/fig_featurecost/feature_cost}

For \textbf{(I)} and \textbf{(III)}, we freeze the encoders of CLIP and only optimize the upsampling decoder. Subsequently, in \textbf{(II)} and \textbf{(IV)}, we fine-tune the encoders of CLIP on top of \textbf{(I)} and \textbf{(III)}. Our results show that feature aggregation can benefit from fine-tuning, but the gain is only marginal. On the other hand, cost aggregation benefits significantly from fine-tuning, highlighting the effectiveness of cost aggregation for adapting CLIP to the task of segmentation. 

For the qualitative results in Fig.~\ref{fig:feature_cost}, we show the prediction results from \textbf{(II)} and \textbf{(IV)}. As seen in Fig.~\ref{fig:feature_cost}(c-d), we observe that feature aggregation shows overfitting to the seen class of ``bucket," while cost aggregation successfully identifies the unseen class ``birdcage." 

\input{tables/components}
\input{tables/ablations}
\vspace{-10pt}
\paragraph{Component analysis.}
Table~\ref{tab:ablation} shows the effectiveness of the main components within our architecture through quantitative results. 
First, we introduce the baseline models in \textbf{(I)} and \textbf{(II)}, identical to the fine-tuned baseline models from Table~\ref{tab:feature-vs-cost}.
We first add the proposed spatial and class aggregations to the cost aggregation baseline in \textbf{(III)} and \textbf{(IV)}, respectively. In \textbf{(V)}, we interleave the spatial and class aggregations. Lastly, we add the proposed embedding guidance to \textbf{(V)}, which becomes our final model.

As shown, we stress the gap between \textbf{(I)} and  \textbf{(II)}, which supports the findings presented in Fig.~\ref{fig:feature_cost}. Given that PAS-20 shares most of its classes with the training datasets\cite{xu2022simple}, the performance gap between \textbf{(I)} and \textbf{(II)} is minor. However, for challenging datasets such as A-847 or PC-459, the difference is notably significant, validating our cost aggregation framework for its generalizability.
We also highlight that as we incorporate the proposed spatial and class aggregation techniques, our approach \textbf{(V)} outperforms \textbf{(II)}, demonstrating the effectiveness of our design.
Finally, \textbf{(VI)} shows that our embedding guidance further improves performance across all the benchmarks.
Furthermore, we provide quantitative results of adopting the upsampling decoder in Table ~\ref{tab:conv-decoder}. The results show consistent improvements across all the benchmarks.


\input{tables/finetune}
\input{figures/fig_embedding/fig_embedding}

\vspace{-10pt}
\label{finetune}
\paragraph{Analysis on fine-tuning of CLIP.}
In this section, we analyze the effects and methods of fine-tuning of the encoders of CLIP. In Table~\ref{tab:finetuning-ablation}, we report the results of different approaches, which include the variant \textbf{(I)}:~without fine-tuning, \textbf{(II)}:~adopting Prompt Tuning~\cite{zhou2022learning, jia2022visual}, \textbf{(III)}:~fine-tuning the entire CLIP, \textbf{(IV)}:~fine-tuning the attention layer only~\cite{touvron2022three}, \textbf{(V)}:~fine-tuning query and key projections only, \textbf{(VI)}:~fine-tuning key and value projections only, \textbf{(VII)}:~our approach for CLIP image encoder only, \textbf{(VIII)}:~our approach for text encoder only, and  \textbf{(IX)}:~our approach for both encoders. Note that both image and text encoders are fine-tuned in \textbf{(I-VI)}. Overall, we observed that fine-tuning enhances the performance of our framework. Among the various fine-tuning methods, fine-tuning only the query and value projection yields the best performance improvement while also demonstrating high efficiency. Additionally, as can be seen in \textbf{(VII-IX)}, fine-tuning both encoders leads to better performance compared to fine-tuning only one of them in our framework.

In Fig.~\ref{fig:embedding_space}, we show the t-SNE~\cite{van2008visualizing} visualization of the dense image embeddings of CLIP within the A-150~\cite{zhou2019semantic} dataset. We color the embeddings based on the prediction with text classes. From (a), we can observe that the clusters are not well-formed for each classes, due to the image-level training of CLIP. In contrast, we observe well-formed clusters in (b) for both seen and unseen classes, showing the adaptation of CLIP for the downstream task.

\vspace{-10pt}
\paragraph{Training with various datasets.}
In this experiment, we further examine the generalization power of our method in comparison to other methods~\cite{ding2022decoupling, xu2022simple} by training our model on smaller-scale datasets, which include A-150 and PC-59, that poses additional challenges to achieve good performance.  The results are shown in Table~\ref{tab:cross-dataset-ablation}. As shown, we find that although we observe some performance drops, which seem quite natural when a smaller dataset is used, our work significantly outperforms other competitors. These results highlight the strong generalization power of our framework, a favorable characteristic that suggests the practicality of our approach.

\input{tables/cross_dataset}
\input{tables/efficiency}

\vspace{-10pt}
\paragraph{Efficiency comparison.}
In Table~\ref{tab:efficiency}, we thoroughly compare the efficiency of our method to recent methods~\cite{ding2022decoupling,xu2022simple,liang2022open}. We measure the number of learnable parameters, the total number of parameters, training time, inference time, and inference GFLOPs. Our model demonstrates strong efficiency in terms of both training and inference. This efficiency is achieved because our framework does not require an additional mask generator~\cite{ding2022decoupling}.
