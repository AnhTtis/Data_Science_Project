\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\input{preamble}

\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}




\def\paperID{14262} %
\def\confName{CVPR}
\def\confYear{2024}



\newcommand{\ANURAG}[1]{\textcolor{blue}{[Anurag: #1]}}
\newcommand{\paragraphni}[1]{\noindent\textbf{#1}}

\begin{document}
\newcommand{\one}{\textcolor{red}{ufwP}}
\newcommand{\two}{\textcolor{green}{RpZD}}
\newcommand{\three}{\textcolor{blue}{RC4p}}



\renewcommand{\thetable}{\Alph{table}}
\renewcommand{\thefigure}{\Alph{figure}}

\paragraphni{General response. (\one, \two, \three)}
We thank the reviewers for their insightful comments, acknowledging our main idea of cost aggregation to be significant, as well as our experiments to be solid with strong performance and extensive ablations.
We also appreciate the detailed comments on figures and errors, and will revise the paper accordingly.


\begin{table}[h]
\centering
\vspace{-8pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{ll|cccccc|cc}
        \toprule
        &Components & A-847 & PC-459 & A-150 & PC-59 & PAS-20 & $\textnormal{PAS-20}^b$ & $_{seen}^{\text{A-150}}$ & $_{unseen}^{\text{A-150}}$ \\
        \midrule\midrule
        \textbf{(I)} & CAT-Seg & 16.0 & 23.8 & 37.9 & 63.3 & 97.0 & 82.5 & 43.4 & 32.4\\
        \textbf{(II)} & \textbf{(I)} + stop gradient & 15.8 & 23.9 & 37.9 & 63.0 & 97.0 & 82.3 & 43.3 & 32.6\\ \midrule
        \textbf{(III)} & CAT-Seg + Full F.T. &  13.6 & 22.2 & 34.0 & 61.1 & 97.3 & 79.7 & 41.3 & 26.6\\
        \textbf{(IV)} & \textbf{(III)} + stop gradient &  15.5 & 23.5 & 35.8 & 62.4 & 97.2 & 81.0 & 42.2 & 29.5\\ \midrule
        \textbf{(V)} & CAT-Seg + Feature agg. & 6.2 & 13.9 & 25.9 & 60.0 & 95.6 & 80.4 & 38.5 & 13.4\\
        \textbf{(VI)} & \textbf{(V)} + stop gradient & 6.2 & 14.0 & 26.3 & 60.5 & 95.6 & 81.0 & 39.1 & 13.6\\
        \bottomrule
\end{tabular}}
\vspace{-10pt}
\caption{Analysis on effects from the usage of CLIP features.}
    \vspace{-13pt}
    \label{tab:overfitting}
\end{table}
\paragraphni{Concerns about the potential of overfitting from using CLIP features. (\textcolor{red}{ufwP})}
We greatly thank the reviewer for raising a valuable concern. To ablate the effects that may be caused from using CLIP features, we apply stop-gradient to the embedding guidance and the upsampling decoder, which are the components indicated by the reviewer. In Tab.~\ref{tab:overfitting} \textbf{(I-II)},
we do not observe much difference as concerned; we find this is likely to be the effects of our fine-tuning method, where we only partially fine-tune CLIP, further contributing to prevent overfitting.
We also ablate this in \textbf{(III-IV)} by fine-tuning all parameters, and observe a more noticeable drop in $\text{A-150}_{unseen}$ than $\text{A-150}_{seen}$ without the stop-gradient. 
As the reviewer concerned, this suggests overfitting to be of impact. %
However, we still find cost aggregation to be crucial to prevent overfitting, where as feature aggregation shows diminishing results in \textbf{(V-VI)}, especially for $\text{A-150}_{unseen}$ when compared to \textbf{(I-IV)}.


\begin{table}[h]
\centering
\vspace{-10pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccccc|c}
        \toprule
        Number of classes $k$ & 16 & 32 & 64 & 128 & 256 & 512 & 847\\
        \midrule\midrule
         Inference time (s) &0.30 & 0.33 & 0.39 & 0.51 & 0.73 & 1.25 & 1.88\\
        Inference GFLOPs & 1096.84 & 1212.97 & 1444.23 & 1909.05 & 2838.25 & 4696.94 & 7126.59\\
        \midrule
        mIoU on A-847 & 14.8 & 15.6 & 15.9 & 15.9 & 16.0 & 16.1 & 15.9\\
        \bottomrule
\end{tabular}}
    \vspace{-10pt}
\caption{Analysis on varying number of classes.} %
    \vspace{-13pt}
    \label{tab:number_classes}
\end{table}
\paragraphni{Discussion on the impact of number of classes on computation. (\textcolor{red}{ufwP})}
In Table~\ref{tab:number_classes}, we show that the computation does scale with larger number of classes, as pointed out by the reviewer.
Although we can process 847 classes with less computation than other methods in Tab.~8, we may experience having infeasible computation in extreme scenarios.
In practice, we note that we select top-$k$ classes for aggregation, based on the cosine-similarity scores. We report the mIoU on A-847 with varying $k$ in Tab.~\ref{tab:number_classes}, and demonstrate that the computation can be effectively moderated while having minimal impact to the performance. We clarify that we use $k=256$ for our experiments, and promise to add further analysis and discussions to our main paper.


\paragraphni{Details on efficiency assessment.} We clarify that the results in Tab.~8 are averaged over all 6 standard benchmarks. 

\paragraphni{Concerns on the analysis on results of MESS.~(\textcolor{red}{ufwP}, \textcolor{green}{RpZD})} 
We appreciate the reviewer \two, for pointing out that CAT-Seg underperforms compared to lower-bound score (LB) on some medical datasets. Although some methods (e.g., ZSSeg or OpenSeeD) appear to show strong performance as indicated by reviewer \one, we find such cases to underperform the LB for the \textit{foreground} classes. This can still result in high mIoU score when having only few classes, \ie predicting all pixels as background, which is a concern also raised in MESS~[1]. We will revise our claims accordingly.

\paragraphni{Discussions on datasets with few classes.~(\two)} 
For datasets with few classes, we also observe that they mostly include a background class, which is given as ``others" in default. In this regard, we find that the selection of prompt for the background class can significantly influence the IoU, especially for the medical domain. As shown in Tab.~\ref{tab:background}, ``background" generally performs better overall, surpassing ``others." We also note that these gains are from both foreground and background classes, suggesting that better representing the background class could further improve CAT-Seg to deal with datasets with few classes. We find such explorations as intriguing direction for future research.

\begin{table}[t]
\centering
\vspace{5pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccccc|c}
        \toprule
        Background classname & General & Earth M. & Medical & Engin. & Agr.\&Bio. & Mean
         \\
        \midrule\midrule
        ``others" (default) & \underline{44.69} & \textbf{39.99} & 24.70 & 20.20 & \textbf{38.61} & \underline{34.70}\\
        ``background" & \textbf{44.91} & \underline{39.52} & \textbf{31.49} & \textbf{21.86} & \underline{36.47} & \textbf{35.90}\\
        ``void" & 43.11 & 38.67 & \underline{30.18} & \underline{21.59} & 33.65 & 34.55\\
        \bottomrule
\end{tabular}}
\vspace{-10pt}
\caption{Analysis on different classnames in MESS.}
    \vspace{-20pt}
    \label{tab:background}
\end{table}

\begin{table}[h]
\centering
\vspace{-10pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccccc}
        \toprule
        VLM & A-847 & PC-459 & A-150 & PC-59 & PAS-20 & $\textnormal{PAS-20}^b$
         \\
        \midrule\midrule
        EVA-02-CLIP-L/14 (Sun et al., 2023) & \underline{16.4} & \underline{24.5} & 37.8 & \underline{62.7} & \textbf{97.9} & \textbf{83.7}\\
        SigLIP-ViT-L/16 (Zhai et al., 2023) & \textbf{18.0}& \textbf{26.1} & \textbf{39.1} & 60.9& \underline{97.2}&80.8\\
        \midrule
        CLIP-ViT-L/14 & 16.0 & 23.8 & \underline{37.9} & \textbf{63.3} & 97.0 & \underline{82.5} \\
        \bottomrule
\end{tabular}}
    \vspace{-10pt}
\caption{Results on various VLMs.}
    \vspace{-13pt}
    \label{tab:vlms}
\end{table}
\paragraphni{More results on various VLMs. (\textcolor{blue}{RC4p})}
We provide results with additional VLMs in Tab.~\ref{tab:vlms}, showing that CAT-Seg can be effectively applied to other VLMs asides CLIP. 

\paragraphni{Clarification on cost and feature aggregation. (\textcolor{blue}{RC4p})} 
We refer feature aggregation as processing concatenated image and text \textit{features} of CLIP, where as cost aggregation as processing the \textit{cosine-similarity} between these features. Although both are from the same image and text features of CLIP, we demonstrate the significance of cost aggregation in Fig.~1 and Tab.~4, as well as in Tab.~\ref{tab:overfitting} \textbf{(I)} and \textbf{(V)}. 

\paragraphni{Concern on the novelty of cost aggregation.  (\textcolor{blue}{RC4p})} 
We wish to emphasize that our novelty lies in its application to the \textit{open-vocabulary} task, and its subsequent findings. Through effectively aggregating the multi-modal cost volume, in contrast to other segmentation tasks~[18, 38], we discover that this allows to fine-tune CLIP for the pixel-level task, demonstrating its significance over various datasets.

\begin{table}[h]
\centering
\vspace{-10pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{ll|cccccc}
        \toprule
        Spatial Agg. & Class Agg. & A-847 & PC-459 & A-150 & PC-59 & PAS-20 & $\textnormal{PAS-20}^b$
         \\
        \midrule\midrule
        Swin Trans. & Swin Trans. & 15.5 & 23.1 & \underline{37.0} & 61.6 & 96.3 & 81.0 \\
        Lin. Trans. & Lin. Trans. & \underline{15.6} & \underline{23.3} & 36.9 & \underline{62.3} & \underline{96.7} & \underline{82.1}\\
        \midrule
        Swin Trans. & Lin. Trans. & \textbf{16.0} & \textbf{23.8} & \textbf{37.9} & \textbf{63.3} & \textbf{97.0} & \textbf{82.5} \\
        \bottomrule
\end{tabular}}
    \vspace{-10pt}
\caption{Ablation on architectural choices for aggregation.}
    \vspace{-13pt}
    \label{tab:attention}
\end{table}
\paragraphni{
Justifications for architectural choices. (\textcolor{blue}{RC4p})} 
We did not employ Swin Transformer for class aggregation as it is not permutation invariant, which we explicitly consider in Sec.~3.3. We note that it would not be problematic in case of applying Transformer for spatial aggregation, and
provide a complete ablation study on this design choice in Tab.~\ref{tab:attention}. 
  

\end{document}
