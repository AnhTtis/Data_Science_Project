\begin{table*}[!t]
    \begin{center}
   
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|cccc|cccccc}
    \toprule

        Model & VLM & Additional Backbone & Training Dataset & Additional Dataset & A-847 & PC-459 & A-150 & PC-59 & PAS-20 & $\textnormal{PAS-20}^b$\\
        \midrule\midrule
        SPNet~\citep{xian2019semantic}  & - & ResNet-101 & PASCAL VOC & \xmark & - & - & - & 24.3 & 18.3 & - \\
        ZS3Net~\citep{bucher2019zero}  & - & ResNet-101 & PASCAL VOC & \xmark  & - & - & - & 19.4 & 38.3 & - \\
        LSeg~\citep{li2022language} & CLIP ViT-B/32 & ResNet-101 & PASCAL VOC-15  & \xmark & - & - & - & - & 47.4 & - \\
        LSeg+~\citep{ghiasi2022scaling} & ALIGN & ResNet-101 & COCO-Stuff & \xmark  & 2.5 & 5.2 & 13.0 & 36.0 & - & 59.0 \\
        ZegFormer~\citep{ding2022decoupling} & CLIP ViT-B/16 & ResNet-101 & COCO-Stuff-156 & \xmark   & 4.9 & 9.1 & 16.9 & 42.8 & 86.2 & 62.7 \\
        ZegFormer{$\dagger$}~\citep{ding2022decoupling} & CLIP ViT-B/16 & ResNet-101 & COCO-Stuff & \xmark & 5.6 & 10.4 & 18.0 & 45.5 & 89.5 & \underline{65.5} \\
        ZSseg~\citep{xu2022simple} & CLIP ViT-B/16 & ResNet-101 & COCO-Stuff & \xmark   & 7.0 & - & 20.5 & 47.7 & 88.4 & - \\
        OpenSeg~\citep{ghiasi2022scaling} & ALIGN & ResNet-101 & COCO Panoptic & \cmark  &  4.4 & 7.9 & 17.5 & 40.1 & - & 63.8 \\
        OVSeg~\citep{liang2022open} & CLIP ViT-B/16 & ResNet-101c & COCO-Stuff & \cmark  &  7.1 & 11.0 & 24.8 & 53.3 & 92.6 & - \\
        ZegCLIP~\citep{zhou2022zegclip} & CLIP ViT-B/16 & - & COCO-Stuff-156 & \xmark  & -&-&-&41.2& 93.6 & - \\
        SAN~\citep{xu2023side} & CLIP ViT-B/16 & - & COCO-Stuff & \xmark  & \underline{10.1} & \underline{12.6} & \underline{27.5} & \underline{53.8} & \underline{94.0} & - \\
        \hlrow 
        & & & & &  \textbf{12.0} &\textbf{19.0} &\textbf{31.8} &\textbf{57.5} & \textbf{94.6} & \textbf{77.3} \\
        \hlrow\multirow{-2}{*}{\ours (ours)} & \multirow{-2}{*}{CLIP ViT-B/16} & \multirow{-2}{*}{-} & \multirow{-2}{*}{COCO-Stuff} & \multirow{-2}{*}{\xmark}  & \textcolor{ForestGreen}{(+1.9)} & \textcolor{ForestGreen}{(+6.4)} & \textcolor{ForestGreen}{(+4.3)} & \textcolor{ForestGreen}{(+3.7)} & \textcolor{ForestGreen}{(+0.6)} & \textcolor{ForestGreen}{(+11.8)} \\
        \midrule
        LSeg~\citep{li2022language} & CLIP ViT-B/32 & ViT-L/16 & PASCAL VOC-15 & \xmark & - & - & - & - & 52.3 & - \\
        OpenSeg~\citep{ghiasi2022scaling} & ALIGN & Eff-B7 & COCO Panoptic & \cmark & 8.1 & 11.5 & 26.4 & 44.8 & - & \underline{70.2}\\
        OVSeg~\citep{liang2022open} & CLIP ViT-L/14 & Swin-B & COCO-Stuff & \cmark & 9.0 & 12.4 & 29.6 & 55.7 & 94.5 & - \\
        SAN~\citep{xu2023side} & CLIP ViT-L/14 & - & COCO-Stuff & \xmark & \underline{12.4} & \underline{15.7} & \underline{32.1} & \underline{57.7} & \underline{94.6} & - \\
        ODISE~\citep{xu2023open} & CLIP ViT-L/14 & Stable Diffusion & COCO-Stuff & \xmark & 11.1 & 14.5 & 29.9 & 57.3 & - & - \\
        \hlrow 
        &  & & & & \textbf{16.0} & \textbf{23.8} & \textbf{37.9} & \textbf{63.3} & \textbf{97.0} & \textbf{82.5}\\
        \hlrow\multirow{-2}{*}{\ours (ours)} & \multirow{-2}{*}{CLIP ViT-L/14} & \multirow{-2}{*}{-} & \multirow{-2}{*}{COCO-Stuff} & \multirow{-2}{*}{\xmark} & \textcolor{ForestGreen}{(+3.6)} & \textcolor{ForestGreen}{(+8.1)} & \textcolor{ForestGreen}{(+5.8)} & \textcolor{ForestGreen}{(+5.6)} & \textcolor{ForestGreen}{(+2.4)} & \textcolor{ForestGreen}{(+12.3)}\\

        \bottomrule
    \end{tabular}
    }
    \vspace{-5pt}
        \caption{\textbf{Quantitative evaluation on standard benchmarks.} The best-performing results are presented in bold, while the second-best results are underlined. Improvements over the second-best are highlighted in \textcolor{ForestGreen}{green}. $\dagger$: Re-implementation trained on full COCO-Stuff.}
    \vspace{-15pt}
    \label{tab:main_table}
    \end{center}

\end{table*}
