\section{Related Work}
\paragraph{Open-vocabulary semantic segmentation.} Classical approaches to the task~\cite{zhao2017open, bucher2019zero, xian2019semantic} attempt to learn visual embeddings that align with pre-defined text embeddings~\cite{miller1998wordnet, mikolov2013efficient}. However, the limited vocabulary of the words has been the major bottlenecks. To address this, LSeg~\cite{li2022language} leveraged CLIP for learning pixel-level visual embeddings aligned with the text embeddings of CLIP. Alternatively, OpenSeg~\cite{ghiasi2022scaling} proposed to identify local regions within the image and correlate with the text embeddings with class-agnostic region proposals. Similarly, ZegFormer~\cite{ding2022decoupling} and ZSseg~\cite{xu2022simple} proposed two-stage frameworks for dealing with the task. Typically, they first learn to predict class-agnostic region proposals similar to~\cite{ghiasi2022scaling}, and feed them to CLIP for final predictions. To better recognize these regions, OVSeg~\cite{liang2022open} collects region-text pairs to fine-tune the CLIP encoder, while MaskCLIP~\cite{ding2022open} leverages the self-attention map from CLIP to refine the region proposals. Alternatively, ODISE~\cite{xu2023open} leverages pre-trained Stable Diffusion~\cite{Rombach_2022_CVPR} model for generating high-quality class-agnostic masks. However, these region-to-text matching methods~\cite{ding2022decoupling,ghiasi2022scaling,xu2022simple, liang2022open, xu2023open, xu2023side, yu2023convolutions} require a region generator, which is trained on a limited scale of annotated datasets.

More recently, ZegCLIP~\cite{zhou2022zegclip} and SAN~\cite{xu2023side} proposed one-stage frameworks, where they attempt to leverage the embeddings from CLIP to predict masks instead of having class-agnostic mask generators parallel to CLIP. Although these methods can better leverage the pre-trained knowledge from CLIP, they introduce learnable tokens or adapter layers to the CLIP image encoder, which can be only trained on the seen classes. FC-CLIP~\cite{yu2023convolutions} implements CLIP as the visual backbone for the segmentation model but opts for a frozen image encoder as they find fine-tuning the image encoder hinders performance for unseen classes. In contrast, we refrain from adding external layers to CLIP and achieve fine-tuning of its encoders by aggregating the cost volume, which is obtained solely from the embeddings of CLIP.


\vspace{-10pt}
\paragraph{Fine-tuning vision-language models.} Along with the advance of large-scale vision-language models, \eg CLIP, numerous attempts have been made to adapt CLIP to various downstream tasks~\cite{wortsman2022robust}. CoOp~\cite{zhou2022learning} and CoCoOp~\cite{zhou2022conditional} learn prompt tokens instead of optimizing the full model. Another stream of work is CLIP-Adapter~\cite{gao2023clip} and TIP-Adapter~\cite{zhang2021tip}, where they aggregate the image and text embeddings from CLIP through adapter layers instead of tuning the encoder itself. 
However, such methods mainly focus on few-shot settings rather than zero-shot evaluation. We explore end-to-end fine-tuning of CLIP for zero-shot pixel-level prediction, which has failed in numerous attempts~\cite{zhou2022extract, xu2023side, yu2023convolutions}.

\vspace{-10pt}
\paragraph{Cost aggregation.} Cost aggregation~\cite{kendall2017end, chang2018pyramid, guo2019group, yang2019hierarchical, song2021adastereo,hong2022neural,huang2022flowformer,cho2022cats++} is a popular technique adopted for the process of establishing correspondence between visually or semantically similar images~\cite{kendall2017end,guo2019group,yang2019hierarchical,cho2021cats,hong2022cost}
by reducing the impact of errors and inconsistencies in the matching process. A matching cost, an input to cost aggregation, is typically constructed between dense features extracted from a pair of images~\cite{rocco2017convolutional}, and often cosine-similarity~\cite{liu2022graftnet,rocco2017convolutional} is used. 
In this work, we view the cosine-similarity score between image and text embeddings of CLIP from the viewpoint of establishing the matching cost volume. Especially, we find the robustness of the cost aggregation layers to be favorable to open-vocabulary semantic segmentation, as these layers operate upon the similarity scores rather than the embeddings itself~\cite{song2021adastereo,liu2022graftnet}. However, our approach diverges from traditional methods as the cost volume obtained from CLIP is inherently multi-modal, originating from both image and text modalities. This contrasts with conventional cost aggregation techniques~\cite{kendall2017end,guo2019group,yang2019hierarchical,cho2021cats,hong2022cost}. Consequently, we explore methods to effectively aggregate the multi-modal cost volume.
