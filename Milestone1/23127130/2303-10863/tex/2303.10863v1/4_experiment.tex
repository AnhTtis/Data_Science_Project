
\begin{table*}
  \centering
  \footnotesize
  \setlength{\tabcolsep}{0.3em}
  \adjustbox{width=\linewidth}
  % \scalebox{1.0}
  {
  \begin{tabular}{|c | l| c| c |c | c |c| c |c|}
  \hline
  % \multicolumn{9}{c}{Predicate Classification} \\
  % \specialrule{0em}{1pt}{1pt}
  % % \cmidrule{3-11}
  % \cline{3-11}
  % \specialrule{0em}{1pt}{1pt}
  & \multirow{3}{*}{Method} & \multirow{3}{*}{Encoder} &  \multicolumn{2}{c|}{1-shot (mR)} & \multicolumn{2}{c|}{5-shot (mR)} & \multicolumn{2}{c|}{10-shot (mR)}\\
  % \specialrule{0em}{1pt}{1pt}
  % \cmidrule{3-11}
  \cline{4-9}
    & &  & Base & Novel &  Base & Novel  & Base  & Novel  \\ 
  &  & & @20~/~50~/~100  &  @20~/~50~/~100 &  @20~/~50~/~100  & @20~/~50~/~100 &  @20~/~50~/~100 &  @20~/~50~/~100  \\
  \hline
  \multirow{12}{*}{\rotatebox{90}{Predcls}}& \textcolor{gray}{Baseline$^\dagger$} & \multirow{6}{*}{Motifs}&  \textcolor{gray}{4.7~/~8.9~/~12.0} & \textcolor{gray}{0.0~/~0.1~/~0.2} & \textcolor{gray}{4.2~/~7.7~/~11.3} & \textcolor{gray}{0.4~/~0.7~/~1.1} &  \textcolor{gray}{4.3~/~7.8~/~11.4}  & \textcolor{gray}{1.6~/~2.4~/~2.9}    \\ 
    & ProtoNet~\cite{snell2017prototypical}  &  & 4.5~/~7.0~/~9.1 & 1.7~/~2.7~/~3.3  & 8.6~/~12.1~/~15.0 & 3.0~/~3.9~/~4.4  & 8.6~/~12.2~/~15.3  & 3.6~/~4.6~/~5.3   \\ 
   & RelationNet~\cite{sung2018learning}  &  & 2.8~/~5.6~/~8.4  & 1.4~/~2.0~/~2.4& 6.3~/~10.7~/~14.4 &  2.0~/~2.8~/~3.6  & 7.8~/~11.8~/~14.9 & 2.3~/~3.3~/~4.3 \\
   & CoOp~\cite{zhou2022learning}   & & 0.2~/~0.5~/~1.0 & 0.8~/~1.3~/~1.8  &  0.2~/~0.5~/~1.0 &  1.6~/~2.6~/~3.8 & 0.3~/~0.7~/~1.3 & 2.3~/~3.8~/~4.9    \\
   & CSC~\cite{zhou2022learning}   &  & 0.2~/~0.6~/~1.1 & 1.8~/~2.7~/~3.4  & 0.2~/~0.6~/~1.2  &  2.9~/~4.7~/~6.0 & 0.3~/~0.7~/~1.3 & 3.4~/~5.1~/~6.3 \\
   & \cellcolor{mygray-bg}{\textbf{DPL (Ours)}}  &  & \cellcolor{mygray-bg}{\textbf{6.9}~/~\textbf{10.2}~/~\textbf{12.8}} & \cellcolor{mygray-bg}{\textbf{2.6}~/~\textbf{3.2}~/~\textbf{3.9}}   & \cellcolor{mygray-bg}{\textbf{10.2}~/~\textbf{15.4}~/~\textbf{18.6}} & \cellcolor{mygray-bg}{\textbf{4.8}~/~\textbf{5.9}~/~\textbf{6.6}}  & \cellcolor{mygray-bg}{\textbf{10.9}~/~\textbf{15.9}~/~\textbf{19.5}} & \cellcolor{mygray-bg}{\textbf{5.1}~/~\textbf{6.3}~/~\textbf{6.9}}  \\ 
  %  \specialrule{0em}{1pt}{1pt}
  \cline{2-9}
   & \textcolor{gray}{Baseline$^\dagger$} & \multirow{6}{*}{VCTree} & \textcolor{gray}{4.4~/~8.1~/~12.1} & \textcolor{gray}{0.1~/~0.2~/~0.3}  & \textcolor{gray}{4.4~/~8.1~/~12.1} & \textcolor{gray}{0.5~/~0.8~/~1.2}  & \textcolor{gray}{4.4~/~8.3~/~12.4} & \textcolor{gray}{1.4~/~2.1~/~2.6}   \\  
   & ProtoNet~\cite{snell2017prototypical}  &  & 4.1~/~6.4~/~9.0 & 1.5~/~2.2~/~2.8 & 9.3~/~12.6~/~15.4 &  3.2~/~4.4~/~5.1  & 8.9~/~12.6~/~15.6 &  3.5~/~4.8~/~5.6  \\ 
   & RelationNet~\cite{sung2018learning}  &  & 4.6~/~7.9~/~11.1 & 2.0~/~2.7~/~3.0  & 7.6~/~11.6~/~14.7   & 2.1~/~2.7~/~3.1 & 7.3~/~10.4~/~13.1 & 2.2~/~3.3~/~4.1\\
   & CoOp~\cite{zhou2022learning}   &  & 0.2~/~0.5~/~1.1  & 0.6~/~1.1~/~1.6   & 0.4~/~0.7~/~1.5  & 1.5~/~2.9~/~3.9   & 0.2~/~0.6~/~1.5  & 2.4~/~3.7~/~5.0  \\
   & CSC~\cite{zhou2022learning}   &   & 0.2~/~0.6~/~1.2 & 2.4~/~3.3~/~4.2   & 0.3~/~0.6~/~1.1 & 3.3~/~4.6~/~5.9 & 0.2~/~0.6~/~1.7 & 4.8~/~6.3~/~\textbf{7.5}  \\
   & \cellcolor{mygray-bg}{\textbf{DPL (Ours)}}  &   & \cellcolor{mygray-bg}{\textbf{7.5}~/~\textbf{10.3}~/~\textbf{12.2}} & \cellcolor{mygray-bg}{\textbf{3.0}~/~\textbf{3.8}~/~\textbf{4.3}}  & \cellcolor{mygray-bg}{~\textbf{11.2}~/~\textbf{16.6}~/~\textbf{20.0}} & \cellcolor{mygray-bg}{~\textbf{5.4}~/~\textbf{6.6}~/~\textbf{7.2}} & \cellcolor{mygray-bg}{~\textbf{10.8}~/~\textbf{16.3}~/~\textbf{20.0}} & \cellcolor{mygray-bg}{\textbf{5.2}~/~\textbf{6.4}~/~7.1}   \\ 
  
   \hline
   \multirow{6}{*}{\rotatebox{90}{SGCls}} & \textcolor{gray}{Baseline$^\dagger$} & \multirow{6}{*}{Motifs}&  \textcolor{gray}{3.0~/~4.4~/~5.8}  & \textcolor{gray}{0.0~/~0.0~/~0.1}  & \textcolor{gray}{2.9~/~4.4~/~5.7} & \textcolor{gray}{0.1~/~0.2~/~0.3}  & \textcolor{gray}{3.0~/~4.5~/~5.7}   &\textcolor{gray}{ 0.6~/~0.8~/~1.0 }    \\ 
    & ProtoNet~\cite{snell2017prototypical}  &  & 2.1~/~3.1~/~3.8 & 0.5~/~0.8~/~1.0  & 4.1~/~5.2~/~5.9   & 1.1~/~1.3~/~1.5 & 3.9~/~5.2~/~6.1  & 1.4~/~1.7~/~1.9  \\ 
   & RelationNet~\cite{sung2018learning}  &  & 1.9~/~2.9~/~3.5 & 0.7~/~0.9~/~1.1  & 3.4~/~4.5~/~5.3 & 1.0~/~1.2~/~1.3 & 3.7~/~5.0~/~5.7& 1.1~/~1.4~/~1.5 \\
   & CoOp~\cite{zhou2022learning}   &  & 0.2~/~0.4~/~0.6  & 0.2~/~0.2~/~0.3 & 0.3~/~0.6~/~1.0  & 0.5~/~0.8~/~0.9 &0.4~/~0.7~/~1.3  & 0.9~/~1.2~/~1.6   \\
   & CSC~\cite{zhou2022learning}   &  & 0.1~/~0.2~/~0.3  & 0.2~/~0.2~/~0.3   &  0.3~/~0.7~/~1.0 & 0.8~/~1.2~/~1.5 &  0.4~/~0.8~/~1.3  & 1.1~/~1.6~/~1.9 \\
   & \cellcolor{mygray-bg}{\textbf{DPL (Ours)}}  & & \cellcolor{mygray-bg}{\textbf{2.8}~/~\textbf{3.7}~/~\textbf{4.4}} & \cellcolor{mygray-bg}{\textbf{0.9}~/~\textbf{1.2}~/~\textbf{1.3}}  &    \cellcolor{mygray-bg}{\textbf{4.5}~/~\textbf{5.9}~/~\textbf{7.0}} & \cellcolor{mygray-bg}{\textbf{1.6}~/~\textbf{1.9}~/~\textbf{2.1}} & \cellcolor{mygray-bg}{\textbf{4.8}~/~\textbf{6.5}~/~\textbf{7.4}}  & \cellcolor{mygray-bg}{\textbf{1.6}~/~\textbf{1.9}~/~\textbf{2.2}}  \\ 

      \hline
   \multirow{6}{*}{\rotatebox{90}{SGDet}}& \textcolor{gray}{Baseline$^\dagger$} & \multirow{6}{*}{Motifs}&  \textcolor{gray}{1.4~/~1.8~/~2.1} & \textcolor{gray}{0.0~/~0.0~/~0.1}   & \textcolor{gray}{1.5~/~1.9~/~2.1}  & \textcolor{gray}{0.2~/~0.3~/~0.4}   & \textcolor{gray}{1.5~/~1.9~/~2.1} & \textcolor{gray}{0.2~/~0.4~/~0.4}     \\ 
    & ProtoNet~\cite{snell2017prototypical}  &  & 0.7~/~0.9~/~1.1   & 0.4~/~0.4~/~0.5  & 1.3~/~1.5~/~2.0 &  0.5~/~\textbf{0.7}~/~0.7 & 1.5~/~1.9~/~2.2 & 0.5~/~0.6~/~0.7  \\ 
   & RelationNet~\cite{sung2018learning}  &  & 0.8~/~1.0~/~1.1  & 0.4~/~0.5~/~0.5 & 1.3~/~1.6~/~2.0  &  0.4~/~0.4~/~0.5 &  1.3~/~1.6~/~1.9 & 0.5~/~0.6~/~0.6\\
   & CoOp~\cite{zhou2022learning}   & & 0.2~/~0.3~/~0.4  & 0.0~/~0.1~/~0.1  & 0.3~/~0.5~/~0.7  & 0.2~/~0.3~/~0.4 & 0.6~/~0.8~/~1.0   &  0.4~/~0.4~/~0.5  \\
   & CSC~\cite{zhou2022learning}   & & 0.1~/~0.2~/~0.4  & 0.2~/~0.3~/~0.4 &  0.4~/~0.6~/~0.7 & 0.3~/~0.4~/~0.5 & 0.7~/~0.9~/~1.1   & 0.5~/~0.6~/~0.7\\
   & \cellcolor{mygray-bg}{\textbf{DPL (Ours)}}  & & \cellcolor{mygray-bg}{\textbf{0.9}~/~\textbf{1.1}~/~\textbf{1.2} } & \cellcolor{mygray-bg}{ \textbf{0.4}~/~\textbf{0.5}~/~\textbf{0.6}}  &  \cellcolor{mygray-bg}{\textbf{1.5}~/~\textbf{1.7}~/~\textbf{2.1}} & \cellcolor{mygray-bg}{\textbf{0.5}~/~0.6~/~\textbf{0.7}}  & \cellcolor{mygray-bg}{\textbf{1.6}~/~\textbf{2.1}~/~\textbf{2.3}}   & \cellcolor{mygray-bg}{ \textbf{0.5}~/~\textbf{0.7}~/~\textbf{0.8}} \\ 
   \hline

  % \bottomrule
  \end{tabular}
  }
  \hspace{3em}
  \caption{FSSGG performance (\%) comparison on mR@K. We mainly report the mR@K scores of all the methods on both Base and Novel splits. VCTree~\cite{tang2019learning} and Motifs~\cite{zellers2018neural} are two popular SGG models exploited as context encoders. \textcolor{gray}{$^\dagger$} For the baseline results, we added the support samples for Novel (unknown in the training stage for all other baselines) and Base categories into training data and train models in a conventional fully-supervised way.}
  \vspace{-1em}
  \label{tab:maincompare}
\end{table*}

\section{Experiments}

\subsection{Experimental Settings}
\noindent{\textbf{Datasets.}} We conducted all experiments on the most prevalent SGG dataset: \textbf{Visual Genome} (VG)~\cite{krishna2017visual}.
To benchmark FSSGG, we re-split VG, and set $80$ common predicate categories as base categories and $60$ rare predicate categories as novel categories. As for the object categories, we selected the most common $247$ object categories, and they are shared in both training and test sets.
For each predicate category, we randomly split $K$-shot from its samples as its support samples in the test set. 
The triplet samples annotated with novel predicate in the test set are called Novel split, and the triplet samples annotated with base predicate are called Base split.
In experiment, we set $K$ as $1$, $5$ and $10$, respectively. In summary, there are 11K test images and 78K training images.


\noindent{\textbf{Tasks.}}
Following SGG works, We evaluated FSSGG on three tasks:
1) \emph{Predicate Classification} (\textbf{PredCls}): Given ground-truth object bboxes and class labels, the model is required to predict the predicate class of pairwise objects. 2) \emph{Scene Graph Classification} (\textbf{SGCls}): Given ground-truth object bboxes, the model is required to predict both object classes and predicate classes of pairwise objects. 3) \emph{Scene Graph Detection} (\textbf{SGDet}): Given an image, the model is required to detect object bboxes, and predict object classes and predicate classes of pairwise objects.

\noindent{\textbf{Evaluation Metrics.}}
To avoid the major influence of common categories on performance and evaluate each category equally, following recent works~\cite{tang2020unbiased}, we mainly reported the performance on mean Recall@K (\textbf{mR@K}) which is the average of recall scores computed for each category separately.
Following previous FSL works~\cite{gidaris2018dynamic,kukleva2021generalized}, we reported mR@K performance on both \textbf{Novel}  and \textbf{Base} splits. 

\noindent{\textbf{Training Details.}}
To mimic the evaluation setting, we randomly generated the support set and query set from training data in a batch. For each batch, we selected $N_b$ categories, and split its positive samples into support samples and query samples randomly. 
The number of sampled categories $N_b$, query, and support samples for each predicate are set differently according to the batch data.
We set the ratio of foreground samples and background samples as 1:2. 
In the experiment, we set the number of tokens for each prompt as $24$ for all prompt-based experiments.
We exploited pretrained CLIP~\cite{radford2021learning} as our VL model in all our prompt-based experiments.
In the SGDet task, we exploited a Faster R-CNN detector pretrained on OpenImage~\cite{kuznetsova2020open} to extract the general proposals in the images as weakly-supervised SGG~\cite{li2022integrating}.

\subsection{Model Comparison}

\indent\textbf{Baselines.} To better evaluate our DPL, we compared it with a set of state-of-the-art FSL methods: 1) \emph{Metric-base Methods}: \textbf{ProtoNet}~\cite{snell2017prototypical} and \textbf{RelationNet}~\cite{sung2018learning}. They both generate the latent embeddings of the query sample and support sample with the same network, and estimate their similarity by computing Euclidean distance and a trainable metric net, respectively.  2) \emph{Prompt-based Methods}: \textbf{CoOp}~\cite{zhou2022learning} and \textbf{CSC}~\cite{zhou2022learning}. They both introduce learnable prompts and exploit CLIP to extract text embeddings of the target category, and then compute the cosine similarity scores with the visual embeddings of each sample.
The difference is that \textbf{CoOp} sets global prompts shared across all categories while \textbf{CSC} set specific prompts for each category.
Following~\cite{zhou2022learning}, we fine-tuned models \textbf{CoOp} and \textbf{CSC} on the $K$-shot support set.
For fair comparisons, we also pretrained them on base training data in advance.
For \textbf{ProtoNet} and \textbf{RelationNet}, we applied the same sampling strategy with DPL. 
We exploited SGG models VCTree~\cite{tang2019learning} and Motifs~\cite{zellers2018neural} as context encoders and reported results in Table~\ref{tab:maincompare}.


\begin{table}
  \centering
 
  % \vspace{-1em}
  \scalebox{0.9}
  {
  \begin{tabular}{ |l | c c  c|}
  \hline
  & 1-shot & 5-shot & 10-shot  \\
  & @50~/~100  & @50~/~100 & @50~/~100 \\
  \hline
  Baseline & 2.7~/~3.3   & 3.9~/~4.4 &  4.6~/~5.3 \\ 
  ~~+Fixed & 1.6~/~2.4  &  2.6~/~3.5  &  2.8~/~4.0 \\         
  ~~+Learnable & \textbf{3.2}~/~\textbf{3.9}  & \textbf{5.9}~/~\textbf{6.6} & \textbf{6.3}~/~\textbf{6.9} \\
  \hline
  \end{tabular}
  }
  \hspace{3em}
  \caption{MR@K performance (\%) of decomposed prototype representation with the fixed and learnable prompts on Novel split. }
  \label{tab:proto}
\end{table}


\textbf{Results Analysis.}
From Table~\ref{tab:maincompare}, we can find the following observations: 1) Our method achieves state-of-the-art performance on both Novel and Base splits under almost all the settings. Even in the challenging $1$-shot setting, it performs excellently.
2) The two prompt-based methods perform well on the Novel split but fail on the Base split. That may be due to the fact that the VL models like CLIP are mostly pretrained on image-caption pairs based on contrastive learning paradigm and they mainly focus on more concrete visual concepts (\eg, object) or fine-grained semantic concepts (\eg, action relationships). They are not sensitive to spatial relationships (\eg \texttt{on} and \texttt{in}), which are mostly split into the Base categories.
Compared to \textbf{CoOp}, \textbf{CSC} performs better on novel splits which is consistent with the conclusion in~\cite{zhou2022learning} that \textbf{CSC} works better on some fine-grained categories with specific prompts for each category. 
While different from \textbf{CSC}, our DPL devises a compositional prompt learning paradigm and exploits a small number of prompts shared across categories which can uncover the hidden semantic knowledge of relationship triplets in VL models, providing great improvements on both novel and base splits.
3) The performance of two metric-based methods is excellent on both Base and Novel splits and even exceeds the Base split performance of traditional classification paradigm (\ie Baseline in Table~\ref{tab:maincompare}) on $5$-shot and $10$-shot. 
However, they are still inferior to our DPL method. 
That is due to that the previous metric-based methods are not tailored to relationship triplets learning and ignore the compositional structure of the relationship triplets.
In contrast, our decomposable prototype space and adaptive metric learner explore this compositional knowledge in subjects and objects.


\begin{figure*}
  \centering
  \includegraphics[width=0.97\linewidth]{figures/visualization.pdf}
  \caption{Visualization of assigned weights on $5$-shot of support samples. The first column displays the query triplets and the target predicates (highlighted in \textcolor{green}{green}). The other columns display the support triplets and their assigned weights by our metric learner. The subjects and objects are drawn in \textcolor{red}{red} and \textcolor{blue}{blue} boxes respectively.}
  \label{fig:visualize}
\end{figure*}

\subsection{Ablation Study}
In this section, we conducted a set of ablation studies on the two proposed modules: decomposed prototype representation and adaptive metric learner.


\textbf{Study of Decomposed Prototype Representation.}
In this part, we mainly analyze the effectiveness of our decomposed prototype representation. 
We first removed the decomposed prototype representation to observe the performance (\ie, Baseline in Table~\ref{tab:proto}). We also devised an experiment with fixed prompts (removing the learnable prompt tokens) to extract the semantic embeddings to show the superiority of our learnable prompts.
We reported the mR@K performance on Novel split in Table~\ref{tab:proto} and set $K$ as $1$, $5$, and $10$, respectively.
From the table, we can see that our learnable prompts bring great improvement compared to the baseline, which verifies the effectiveness of our DPL module. 
In contrast, exploiting semantic knowledge with fixed prompts harms the model's performance. This verifies the difficulty of semantic prototype modeling and directly exploiting hand-crafted prompts is non-trivial. In comparison, our devised learnable prompts are able to induce useful knowledge of the VL model and bring benefits to the model.


\begin{table}
  \centering
 
  % \vspace{-1em}
  \scalebox{0.9}
  {
  \begin{tabular}{ |l | c c  c|}
  \hline
  & 1-shot & 5-shot & 10-shot \\
  & @50~/~100  & @50~/~100 & @50~/~100 \\
  \hline
  Baseline & 2.7~/~3.3   & 3.9~/~4.4 &  4.6~/~5.3 \\ 
  ~~+Average &   \textbf{3.6}~/~\textbf{4.0}  &   4.7~/~5.3   & 5.3~/~5.9 \\
  ~~+Re-weight &3.2~/~3.9  & \textbf{5.9}~/~\textbf{6.6} & \textbf{6.3}~/~\textbf{6.9} \\
  \hline
  \end{tabular}
  }
  \hspace{3em}
  \caption{Ablation study of the metric learner on the Novel split.}
  \label{tab:reweight}
\end{table}



\textbf{Study of Metric Learner.} % ~\label{sec:metric_exp}
To verify the effectiveness of our metric learner, we removed the re-weighting strategy and averaged the distance from each support sample to make final predictions (\ie, Average in Table~\ref{tab:reweight}). Benefiting from our decomposed prototype representation module, the average strategy achieves better performance compared to the baseline, as shown in Table~\ref{tab:reweight}. However, its performance drops a lot compared to the re-weighting strategy under the settings of $5$-shot and $10$-shot. This is because that the average strategy ignores the difference among support samples when estimating their distance with query samples.


\emph{Case Study.}
To show how our metric learner works, we randomly selected some query triplets and visualized their weights assigned to the support samples during the inference in Figure~\ref{fig:visualize}. We have the following findings: 1) The visual appearances of support samples for one predicate vary greatly under different subject-object compositions. 2) For each query triplet, our metric learner tends to assign higher attention weights to more reliable support samples which have relevant subject-object pairs with the query sample, thus helping our model make more accurate predictions.
