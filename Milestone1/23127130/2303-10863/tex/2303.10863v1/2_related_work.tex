\section{Related Work}
\textbf{Scene Graph Generation (SGG).}
SGG aims to predict visual relationship triplets between object pairs in an image. 
Recent SGG works mainly focus on three directions: 1) \emph{Model Architectures}: Current mainstream SGG methods are mostly based on two-stage: they first detect all localized objects and subsequently predict their entity categories and pair-wise relationships. Usually, they focus on modeling global context information~\cite{zellers2018neural,tang2019learning,yang2018graph,chen2019counterfactual}, language priors~\cite{lu2016visual,zellers2018neural} and so on.
Recently, several one-stage SGG works~\cite{li2022sgtr,teng2022structured} based on Transformer~\cite{vaswani2017attention} have been proposed and achieved impressive performance. They generate the pairs of localized objects and their relationships simultaneously. 2) \emph{Unbiased SGG}: Due to the long-tailed predicate distribution in the prevalent SGG datasets, unbiased SGG which aims to improve the performance of tail informative predicates has attracted considerable attention~\cite{tang2020unbiased,li2022devil}. 3) \emph{Weakly-supervised SGG}: To overcome the limitation of expensive annotations, researchers proposed to solve SGG under weakly-supervised setting, \ie, using image-level annotations as supervision~\cite{zhong2021learning,li2022integrating}. In this work, we focus on transferring previous knowledge for novel predicate categories and propose a new promising task: FSSGG.

\textbf{Few-Shot Learning (FSL).}
FSL has been widely explored in various scene understanding tasks to get rid of costly manual labeling efforts.
It aims to learn valid knowledge about novel categories quickly with only a few reference samples. 
To achieve this goal, lots of creative solutions have been proposed.
One of the most well-known paradigm of FSL is meta-learning, which can be further categorized into three groups: 1) \emph{Metric-based}: Learning a projection function and measuring the similarity between query and support samples on the learned space~\cite{snell2017prototypical,sung2018learning,li2019distribution,li2019revisiting}, which always assumes that each class has a prototype representation at the center of support points. 2) \emph{Memory-based}: Designing models capable of quick adaptation to novel tasks with an external memory~\cite{cai2018memory,santoro2016meta}. 3) \emph{Optimization-based}: Learning a good model initialization that enables the model to be adapted to novel tasks using a few optimization steps~\cite{lee2019meta,finn2017model}.
Recently, some works begin to focus on prompt learning methods that design proper prompts for few-shot or zero-shot learning using large pretrained models~\cite {tsimpoukelli2021multimodal,alayrac2022flamingo,he2022towards,gu2021ppt}.
We share the core idea of metric-based meta-learning algorithms while focusing on SGG, \ie, the proposed learnable prototype space for learning compositional patterns across different predicate categories.


\textbf{Prompt Learning.}
Prompt learning is an emerging technique to allow the large pre-trained models to be able to perform few-shot or even zero-shot learning on new scenarios by defining a new prompting function.
With the emergence of large VL models like CLIP~\cite{radford2021learning}, prompt learning has been widely used in various tasks in the vision community, such as zero-shot image classification~\cite{radford2021learning}, object detection~\cite{du2022learning}, and captioning~\cite{chen2022visualgpt}.
Early works~\cite{nag2022zero} just exploit fixed prompts to exploit the knowledge of VL models, and researchers found that model performance is highly related to the input prompts. 
However, the engineering of designing hand-crafted prompts is extremely time-consuming.
To make it more efficient, recent works~\cite{zhou2022learning} attempt to model the prompts with a set of learnable vectors in continuous space and achieved great success.
Currently, many excellent works~\cite{li2021prefix, zhou2022learning,zhou2022conditional,gao2023compositional} based on task-specific prompt learning have been proposed.
In this work, we devise learnable prompts to help us generate semantic prototypes (for both subject and object) of predicates. 
