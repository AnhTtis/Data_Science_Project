\section{Introduction}
Scene Graph Generation (SGG) is a challenging scene understanding task that aims to detect visual relationships between object pairs in one image and represent them in the form of a triplet: $\left \langle  \texttt{sub}, \texttt{pred}, \texttt{obj} \right \rangle $. Recently, increasing works have been committed to this task and achieved great success.
However, current SGG models typically rely on abundant manual annotations. 
Compared to other scene understanding tasks (\eg, object detection), labeling a high-quality SGG dataset is much more difficult 
due to the expensive annotation form ($N^{2}$ annotation cost)~\cite{li2022integrating} and the notorious multi-label noisy annotation issue~\cite{li2022devil}.
Therefore, it is difficult to apply these models to real-world scenarios involving vast amounts of rare predicates (or relationships) whose annotations are hard to collect.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/intro.pdf}
    \caption{The illustration of FSSGG with 3-shots. For each predicate category, 3 support samples are provided with annotations (\ie the bounding boxes and categories of subjects and objects, and predicate category). The target of FSSGG models is to detect the relation triplets in the query images. }
    \label{fig:intro}
\end{figure}

 
In contrast, we humans are able to generalize the previous knowledge on new visual relationships quickly with only a few examples, and recognize them well.
In this paper, we aim to imitate this human ability and equip SGG models with the transfer ability. Inspired by few-shot object detection~\cite{snell2017prototypical,sung2018learning} and segmentation~\cite{wang2019panet,li2021adaptive}, we propose a promising and important task: few-shot SGG (FSSGG).
As shown in Figure~\ref{fig:intro}, referring to a few annotated examples (\ie, \textbf{support samples}) of a novel predicate category, an ideal FSSGG model is expected to quickly capture the crucial clue for this predicate, recognize the relation triplets of it, and localize them in the target image (\ie, \textbf{query image}). 


\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/intro2.pdf}
    \caption{(a) The traditional metric-based method learns one prototype representation for each class in few-shot image classification. (b) Due to the multiple semantics of predicates in FSSGG, the predicate may have multiple prototypes in the latent space. }
    \label{fig:intro2}
\end{figure}

Currently, many approaches have been proposed to solve few-shot learning (FSL) tasks, such as object detection~\cite{fan2020few} and image classification~\cite{zhou2022learning}.
And one of the most popular solutions is metric-based methods~\cite{kim2019variational,snell2017prototypical,sung2018learning}.
As shown in Figure~\ref{fig:intro2}(a), its basic idea is to project visual samples (\eg, $\boldsymbol{x}$) as points in a latent semantic space and devise a metric to calculate their distances to different category prototype representations, which are always assumed to be the centers of corresponding support points (\ie, one prototype for each category~\cite{snell2017prototypical,sung2018learning,liu2020prototype}).
Although metric-based methods have achieved great success,
naively applying them to FSSGG is non-trivial due to the intrinsic characteristics of visual relation concepts in SGG:

\textbf{Firstly}, polysemy is very common in the predicate categories, and each predicate category may have diverse semantics under different contexts.
For example, as shown in Figure~\ref{fig:intro}, the predicate \texttt{has} may not only mean \emph{possession relation} between main body and individuals (\eg, $\left \langle  \texttt{chair}, \texttt{has}, \texttt{leg} \right \rangle $) but also mean \emph{eating relation} between human and food (\eg, $\left \langle  \texttt{boy}, \texttt{has}, \texttt{food} \right \rangle $).
This will lead to multiple semantic centers when all samples of one predicate are projected on the latent space (\cf, Figure~\ref{fig:intro2}(b)).
Therefore, it is inadequate to represent each predicate category with only one prototype representation.
\textbf{Secondly}, the relation triplets of the same predicate sometimes have totally distinct visual appearance under different subject-object pairs. For instance, the relation triplets of $\left \langle \texttt{man}, \texttt{has}, \texttt{arm} \right \rangle $ in $S_2$ and $\left \langle  \texttt{chair}, \texttt{has}, \texttt{leg} \right \rangle $ in $S_3$ look irrelevant visually. 
This great variance of input signals increases the difficulty to build the projection of support samples into a concentrated point on the latent space, resulting in considerable confusion during inference as their distances from the query point may vary greatly.


In this paper, we propose a novel Decomposed Prototype Learning (\textbf{DPL}) for FSSGG, which can mitigate the mentioned issues. Specifically, \textbf{for the first issue}, in order to model multiple semantics of predicates, we construct a decomposable semantic space and represent each predicate as multiple semantic prototypes in this space.  
As the semantic meaning of each predicate is highly relevant to the subject-object pairs, 
we exploit the subject-object pair context of support samples to help us construct the semantic space for each predicate.
However, due to the small number of support samples, it is difficult to catch enough information to build high-quality semantic representations.
Inspired by recent prompt-tuning works, we devise learnable prompts to help us induce the knowledge of powerful pretrained VL models for deeper exploration of the predicate semantic space. 
Besides, in order to explore more novel compositions of subject-object
context, we decompose the subject-object pairs and generate prototypes for subjects and objects separately (\eg, \texttt{boy}-\texttt{has} and \texttt{has}- \texttt{food} are regarded as two prototypes to be composable with other subject or object prototypes).
We assign soft scores to each prototype for the input sample, 
and then exploit the attended prototype representations to enhance the final latent embedding of the sample.
\textbf{For the second issue}, we devise an intelligent metric learner to estimate the confidence of each support sample for each query sample.
Since each composition looks different, the metric learner will assign adaptive weights to each support sample by considering the relevance of their subject-object pairs to make more reliable predictions.

To better benchmark the new proposed Few-Shot SGG, we re-split the prevalent Visual Genome (VG) dataset~\cite{krishna2017visual} into 80 base predicate categories and 60 novel predicate categories to evaluate the transfer ability of the model on novel predicate categories. Meanwhile, the object categories are shared in both base and novel relation triplets.

We summarize our contributions as follows:
\begin{itemize}
    \item We propose a new promising research task: FSSGG, which requires SGG models to transfer the knowledge quickly to novel predicates with only a few examples.
    \item We devise a decomposable prototype space to model multiple semantics of predicates, and equip it with the knowledge of powerful VL models. Besides, we propose an intelligent metric learner to adaptively assign confidence to each support for different query samples. 
    \item To better benchmark FSSGG, we re-split VG dataset and evaluate several state-of-the-art FSL methods on the dataset for future research.
\end{itemize}

