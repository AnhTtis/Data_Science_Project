



\section{Model-based Optimization Algorithms for RIS-aided Wireless Networks}
\label{sec-model}

This section introduces model-based algorithms and applications for optimizing RIS-aided wireless networks, including AO, MM, SCA, BCD, SDR, SOCP, FP, and BnB. In addition, we summarize the features, advantages, drawbacks, difficulties, and applications of these techniques.   

\iffalse
\begin{figure}[!t]
\centering
\includegraphics[width=0.9\linewidth]{Image/fig-AO.jpg}
\caption{The alternating optimization procedure for joint active and passive beamforming.}
\label{fig-AO}
\setlength{\abovecaptionskip}{-2pt} 
\vspace{-10pt}
\end{figure}
\fi

\subsection{Alternating Optimization} 

AO has been widely applied for RIS-related control and optimizations. The main reason is the high complexity of joint optimization problems that include multiple control variables such as the BS beamforming matrix and RIS phase shifts. \blue{For instance, the joint active and passive beamforming is usually decoupled into an active BS beamforming sub-problem and passive RIS beamforming sub-problem, and then each sub-problem is alternatively optimized.}
\blue{In particular,} for an optimization problem with $I$ control variables $\Vec{x} = (x_1,x_2,...,x_i,...,x_I) $ and  $x_i \in X_{i}$, to minimize objective function $f(\Vec{x})$, AO method is summarized by
\blue{
\begin{itemize}
    \item Step 1: Initializing the control variables by setting \qquad $\Vec{x^{0}} = (x^0_1,x^0_2,...,x^0_i,...,x^0_I)$. Defining control variable number $i=1$, iteration number $l=1$, maximum iteration number $L$, and termination threshold $\delta$. 
    \item Step 2: In the $l^{th}$ iteration, for control variable $x^{l}_{i}$, 
    optimizing $f(\Vec{x})$ by finding $x_{i}$ that satisfies 
    \begin{equation} \label{eq-ao}
     x^{l}_i \leftarrow \argmin\limits_{x_{i} \in X_i} f(\underbrace{x^{l}_1,x^{l}_2,...,x^{l}_{i-1}}_{\textbf{done}}\underbrace{,x_{i},}_{\textbf{current}}\underbrace{x^{l-1}_{i+1},...,x^{l-1}_{I}}_{\textbf{todo}}),    
    \end{equation}
    while holding all the other control variables $(x^{l}_1,x^{l}_2,...,x^{l}_{i-1},x^{l-1}_{i+1},...,x^{l-1}_{I})$ constant. 
    \item Step 3: $i=i+1$ and repeating from Step 2 until $i\leq I$. 
    \item Step 4: If $f(\Vec{x_{l}})-f(\Vec{x_{l-1}}) \leq \delta $, stopping all iterations and $\Vec{x_{l}}$ is the optimal solution; if not, moving to step 5. 
    \item Step 5: If $l\leq L$, then $l=l+1$, and setting control variable number $i=1$ and repeat from Step 2; if not, outputting $\Vec{x_{L}}$ as the optimal solution.
\end{itemize}} 
In equation (\ref{eq-ao}), AO simplifies joint optimization problems by optimizing single control variables alternatively while holding other variables unchanged\cite{jcbe}. Each iteration is time-efficient by optimizing one individual variable, which is easily implemented. In addition, it does not require step size parameter tuning and extra storage vectors. AO provides an iterative optimization scheme, but it still relies on other techniques to solve each sub-problem. Also, having each variable monotonically decrease at each iteration does not guarantee the algorithm will converge to a global minimum, and moreover, the convergence may slow down near an optimum point\cite{jcbe}.

The RIS is often combined with other techniques for joint optimization, such as joint active and passive beamforming, RIS-related resource allocation, RIS-NOMA, and RIS-MEC, leading to coupled control variables and large solution spaces.    
AO is particularly useful in solving such joint optimization problems. For example, 
the RIS-MEC system can be decoupled into RIS phase-shift control sub-problem and task offloading sub-problem, and these two sub-problems will be iteratively optimized to reduce the overall complexity. Joint active and passive beamforming is another example that has been widely investigated, which applies AO to generate BS active beamforming and RIS passive beamforming sub-problems\cite{qingqing,qingqing2}. 






\subsection{Block Coordinate Descent}  
Coordinate descent is a very useful method to solve large-scale optimization problems, and BCD is considered a generalized version to improve computation efficiency.
Compared with AO, each block in the BCD algorithm may include several control variables, enabling dynamic block generation, selection and updating. Therefore, BCD method is more suitable than AO for optimizing a large number of control variables simultaneously, which has been widely applied to RIS-related optimization problems. 

BCD method sequentially minimizes the objective function $F(\Vec{x})$ in each block $x_{i}$ while the other blocks are held fixed. Specifically, it minimizes $ x^{l}_{i} \leftarrow \argmin\limits_{x_i \in \mathscr{X_{i}}} (f(x_{i})+f_{i}(x_i))$ while holding other blocks $x_{1},x_{2},...,x_{i-1},x_{i+1},...x_{I}$ fixed. However, it is worth noting that each block consists of multiple control variables, and the block selection and updating method will affect the BCD performance. An ideal block selection method is expected to maximize the improvement by choosing the blocks that decrease $F(\Vec{x})$ by the largest amount\cite{Julie}. On the other hand, there are many alternatives for the block updating method such as block proximal updating
     \begin{equation} \label{eq-bcd2}
        x^{l}_{i} \leftarrow \argmin\limits_{x_i \in \mathscr{X_{i}}} (f(x_{i})+f_{i}(x_i)+\frac{L_{i}^{l-1}}{2}\|x_{i}-x^{l-1}_{i}\|^2),
    \end{equation}
where $L_{i}^{l-1}>0$. Equation (\ref{eq-bcd2}) is more stable than conventional BCD by including $\frac{L_{i}^{l-1}}{2}\|x_{i}-x^{l-1}_{i}\|^2$. 
The BCD algorithm is easily deployed with low memory requirements and iteration costs, allowing parallel or distributed implementations. But the block selection may affect the algorithm performance, and block updating is difficult in some cases. 

Similar to AO, BCD is considered as an iteration-based scheme to reduce problem-solving complexity. BCD has been applied to sum-rate maximization \cite{cunhua,jiey,huayan}, user fairness maximization \cite{gang}, and power minimization \cite{zhiyang}. 
As an example, a two-block BCD is used to maximize the sum-rate in \cite{huayan}, in which the first block is for BS active beamforming and the second is for RIS passive beamforming, then these blocks are iteratively optimized. 





\begin{figure}[!t]
\centering
\setlength{\abovecaptionskip}{-2pt} 
\includegraphics[width=0.95\linewidth]{Image/fig-MM.jpg}
\caption{MM method for RIS-related optimizations.}
\label{fig-mm}
\vspace{-10pt}
\end{figure}



\subsection{Majorization-Minimization Method} 

MM is an iterative optimization method that has been applied for RIS control and optimizations. Consider an optimization problem $\min\limits_{x}  f(x) $ and $x \in \mathscr{X}$, where $f(x)$ is a continuous objective function and $\mathscr{X}$ is a convex closed set. In RIS-related control problems, the $f(x)$ is usually complicated to solve directly due to fractional and logarithmic terms.
As shown in Fig. \ref{fig-mm}, the main idea of the MM algorithm is to construct a surrogate function $g(x)$ that can locally approximate the objective function $f(x)$, e.g., power minimization or sum-rate maximization. $g(x)$ is considered an upper bound of $f(x)$, which is easier to be optimized. Therefore, optimizing $g(x)$ can either improve the objective function value or leave it unchanged with $g(x) \geq f(x)$\cite{ying}.   

Constructing a surrogate function $g(x)$ is the first step of applying the MM algorithm, since $g(x)$ will be optimized directly instead of the original objective $f(x)$. The $g(x)$ construction rules include:\\
    A1): $g(x^{l-1}|x^{l-1})=f(x)$; \quad A2): $g(x|x^{l-1}) \geq f(x)$;\\
    A3): $ g'(x|x^{l-1};d)|_{x=x^{l-1}}=f'(x^{l-1};d)|_{x=x^{l-1}}$;\\
    A4): $g(x|x^{l-1})$ is continuous in $x$ and $x^{l-1}$, \\
where $x^{l-1}$ is the produced point at iteration $l-1$. 
$g(x|x^{l-1})$ is an approximation function of $f(x)$ at the iteration $l$, and "$|$" in $g(x|x^{l-1})$ means that the point $x^{l-1}$ is already on this function. 
$d$ indicates the distance from a point $x$ to a set $\mathscr{X}$ and $d=\inf\limits_{x'\in \mathscr{X}}||(x-x')||$. $f'(x;d)$ is the directional derivative of $f(x)$ in direction $d$.
Assumptions (A1) and (A2) indicate that $g(x|x^{l-1})$ is a tight upper bound of the original objective $f(x)$. It guarantees that optimizing $g(x|x^{l-1})$ can meanwhile find an improved objective value for $f(x)$. 
Note that surrogate function may be defined in various ways, e.g. Jensen's inequality, Convexity inequality, Cauchy–Schwarz inequality.     
Then, the surrogate function $g(x|x^{l-1})$ is iteratively minimized and updated by
$ x^{l} \leftarrow \argmin\limits_{x \in \mathscr{X}} g(x|x^{l-1})$ until convergence. 
 
As an estimation-based method, MM is considered a low-complexity solution for many RIS-related optimizations, including sum-rate maximization\cite{cunhua}, fairness maximization\cite{menghua,guiz}, secure transmission\cite{huiming2} and so on.
For example, the joint active and passive beamforming problem is decoupled into BS transmit power control and RIS phase-shift optimization in both \cite{cunhua} and \cite{menghua}. Then, the RIS phase-shift optimization problem is first converted into a non-convex quadratically constrained quadratic program (QCQP) problem\footnote{A QCQP problem example is given by equation (\ref{eq-qcqp}) in Section \ref{section_sdr}, which is frequently formulated in wireless networks.}, and a MM algorithm is applied to obtain locally optimal solutions by $\min_{\theta} g(\theta|\theta^l)$ with constraint $|\theta_n|=1$. After that, the optimal phase shift $\theta$ in current iteration $l$ is obtained as $\hat{\theta}^l$, and then $l=l+1$ and $\hat{\theta}^l$ becomes a new $\theta^l$ in $\min\limits_{\theta} g(\theta|\theta^l)$.  

The MM applies surrogate functions to avoid the complexity of optimizing the non-convex objective function directly, transforming non-differentiable problems into smooth optimizations. The MM method requires that the surrogate function $g(x)$ must be a global upper bound for $f(x)$, which is a fundamental assumption for using MM. However, defining such a tight upper bound can be impractical in some cases, which may prevent the application of the MM method. 










\subsection{Successive Convex Approximation} 

\begin{figure}[!t]
\centering
\setlength{\abovecaptionskip}{-2pt} 
\includegraphics[width=1.02\linewidth]{Image/fig-SCA.jpg}
\caption{ Using SCA algorithm for RIS-related optimization.}
\label{fig-SCA}
\vspace{-10pt}
\end{figure}

Similar to the MM algorithm, SCA applies a surrogate function $g(x)$ to approximate the original objective function $f(x)$, which is shown in Fig. \ref{fig-SCA}. However, the $g(x)$ in the SCA algorithm does not have to be a tight upper bound for $f(x)$, reducing the complexity of the surrogate function design \cite{palomar}. Therefore, SCA is more flexible and easier to be implemented for RIS-related optimization problems.

The SCA method first constructs a surrogate function $g(x)$, and the assumptions are similar to the MM algorithm:  \\ 
A1): $g(x|x^{l-1})$ is continuous in $\mathscr{X}$;\\
A2): $g(x^{l-1}|x^{l-1})=f(x)$;\\
A3): $g(x)$ is differentiable with $\nabla_{x} g(x|x^{l-1})|_{x=x^{l-1}}=\nabla_{x} f(x)|_{x=x^{l-1}}$. \\
SCA relaxes the upper bound condition for the surrogate function, but $g(x|x^{l-1})$ must be strongly convex in $\mathscr{X}$.
Then, solving the constructed surrogate problem 
$\mathbf{\hat{x}}(x^{l}) \leftarrow \argmin\limits_{x \in \mathscr{X}} g(x|x^{l-1})$,
and smoothing the next point by 
\begin{equation}
    x^{l}=x^{l-1}+\beta^{l-1}(\mathbf{\hat{x}}(x^{l})-x^{l-1}),
\end{equation} 
where $\beta^{l-1}$ is the step size for value updating.
Finally,  $g(x)$ construction and solving are repeated until meeting the convergence criteria. 
In SCA, the surrogate function $g(x)$ does not have to be a tight upper bound for $f(x)$. Therefore, the step size in each iteration requires dedicated designs to guarantee an accurate approximation. The factor $\beta^{l-1}$ is used to control the $x^{l}$ updating step size. Meanwhile, the MM algorithm updates the whole control variable $x$ at each iteration, but SCA can be naturally implemented in a distributed manner when the constraints are separable. 

Compared with MM, SCA is more frequently applied in RIS-related optimizations due to the relaxed upper bound, e.g., sum-rate maximization in \cite{ming,yuanbin,xidong} and power minimization in \cite{huimei,jianyue}.
Defining a surrogate function is the key to using the SCA method, which depends on specific objective functions and constraints in RIS-related applications.
For instance, the non-convex BS transmit power constraint in \cite{huimei} is replaced by a first-order Taylor approximation to apply the SCA algorithm. By contrast, Pan \textit{et al.} in \cite{cunh} claim that the unit modulus constraint of the RIS phase shift $|\theta_n|=1$ can be relaxed as a series of convex constraints, e.g., $1 \leq 2 {\rm Re}\{\theta^*_n\theta_n^l\}-|\theta_n^l|^2$, where ${\rm Re}\{\cdot\}$ denotes the real part of a complex argument and $\theta^*$ is the conjugate of $\theta$. 








\subsection{Semidefinite Relaxation}  
\label{section_sdr}

Many RIS-related signal processing problems can be described by QCQP formulations, and SDR is an efficient solution to solve QCQP problems\cite{shuzhong}. The QCQP problem is defined by
\begin{equation}\label{eq-qcqp}
\begin{aligned}
\min\limits_{x\in \mathscr{X}}  \qquad & x^{T}C x  \\
 \text{s.t.}  \qquad & x^{T}D_{i} x \geq b_{i}, i=1,2,3,,,n,  
\end{aligned}
\end{equation} 
where the "$\geq$" in the constraint can also be replaced by "$\leq$". Note that $x^{T}C x$ produces an $1\times 1$ matrix, and therefore $x^{T}Cx=Cx^{T}x=Tr(Cx^{T}x)$. Similarly, $x^{T}D_{i}x=D_{i}x^{T}x=Tr(D_{i}x^{T}x)$ is achieved. By introducing $X=xx^T$, then  
\begin{equation}\label{eq-qcqp2}
\begin{aligned}
\min\limits_{x\in \mathscr{X}}  \qquad & Tr(CX)  \\
 \text{s.t.}  \qquad & Tr(DX_{i}) \geq b_{i}, i=1,2,3,,,I,  \\
 \qquad & X\succeq 0,     \\
  \qquad & rank(X)=1,   
\end{aligned}
\end{equation}
where $Tr$ indicates the trace operation, and $X\succeq 0$ indicates that $X$ is positive semidefinite with $X=xx^T$. Then, the non-convex constraint $rank(X)=1$ is relaxed and achieve 
\begin{equation}\label{eq-qcqp3}
\begin{aligned}
\min\limits_{x\in \mathscr{X}}  \qquad & Tr(CX)    \\
 \text{s.t.}  \qquad & Tr(DX_{i}) \geq b_{i}, i=1,2,3,,,I,   \\
 \qquad & X\succeq 0.  
\end{aligned}
\end{equation}

Equation (\ref{eq-qcqp3}) is an SDR of (\ref{eq-qcqp2}), which can be efficiently solved by semidefinite programming (SDP)\cite{zhiquan}. 
SDR has been very generally applied to RIS-related optimization problems, since the $rank(x)=1$ is frequently formulated for phase control. Specifically, the RIS phase shift constraint $|\theta_n|=1$ is non-convex with $\theta\theta^T=1$. 
Then we can define $\mathcal{V}=\theta\theta^T$ with $\mathcal{V}\succeq1$ and $rank(\mathcal{V})=1$, which can be then transformed and relaxed as shown by equations (\ref{eq-qcqp2}) and (\ref{eq-qcqp3}). 

However, the main obstacle to applying SDR is to transform a globally optimal solution $\hat{\mathcal{V}}$ into a feasible solution $\hat{\theta}$. An ideal solution is that $\hat{\mathcal{V}}$ is rank-one, and then $\hat{\theta}$ is easily obtained by solving $\hat{\mathcal{V}}=\hat{\theta}\hat{\theta}^T$. Otherwise, if $rank(\hat{\theta})>1$, a rank-one approximation may be used to obtain a sub-optimal solution $\widetilde{\theta}$. There are multiple methods to find a feasible $\widetilde{\theta}$ from $\hat{\mathcal{V}}$, leading to various solution qualities. 
For instance, Mu \textit{et al.} propose a penalty-based method to relax the rank-one constraint, finding a sub-optimal solution by introducing penalties if $rank(\hat{x})>1$ \cite{mu2021simultaneously}. 
SDR has been used for sum-rate maximization \cite{boya, peilan, ni2021resource}, power minimization \cite{guizhou2,huimei,guiz3,jianyue}, fairness maximization \cite{gang,hailiang,zaid,menghua}, and secure transmission \cite{zheng,wei,xianghao,biqian,zijie}. 



\begin{figure}[!t]
\centering
\setlength{\abovecaptionskip}{-3pt} 
\includegraphics[width=0.7\linewidth]{Image/fig-socp.jpg}
\caption{An example of a second-order cone in 3D space.}
\label{fig-socp}
\vspace{-15pt}
\end{figure}






\subsection{Second-order Cone Programming}  
SOCP is another method that is used to efficiently solve optimization problems in wireless networks, especially for QCQP and fractional problems. 
Fig. \ref{fig-socp} presents a second-order cone example in 3D space. SOCP is a generalization of linear and quadratic programming that allows for affine combinations of variables to be constrained inside a second-order cone
\begin{equation}\label{eq-scop}
\begin{aligned}
\min\limits_{x\in \mathscr{X}}  \qquad & C^{T} x   \\
 \text{s.t.}  \qquad & ||A_{i}x+b_{i}||\leq c_{i}^{T}x+d_{i}, i=1,2,3,,,I, 
\end{aligned}
\end{equation} 
where $A \in \mathbb{R}^{n_i\times n}$, $b_i \in \mathbb{R}^{n_i}$, $c_i \in \mathbb{R}^n$, and $d_i \in \mathbb{R}$. The $x$ in equation (\ref{eq-scop}) may be RIS phase shifts, BS beamforming vectors, and so on, which depends on specific application scenarios. Consider the inverse image of the unit second-order cone with an affine mapping
\begin{equation} \label{eq-scop3}
 ||A_{i}x+b_{i}||\leq c_{i}^{T}x+d_{i} \leftrightarrow 
 \left[ \begin{array}{c}  
    A_{i} \\  
    c_{i}^{T} \\ 
  \end{array} \right] x +  
\left[ \begin{array}{c}  
    b_{i} \\  
    d_{i} \\ 
  \end{array}
\right] \in \mathscr{C}_{n_i+1}.       
\end{equation}
Therefore, SOCP is a convex optimization problem with a convex objective function and convex constraints. Equation (\ref{eq-scop3}) indicates the core properties of SOCP problems, and hence many problems are converted into SOCPs and solved efficiently\cite{miguso}. 

For instance, sum and fractional problems are frequently defined in RIS-related problems to maximize the sum-rate or total throughput regarding the SINR
\begin{equation}\label{eq-scop7}
\begin{aligned}
\min\limits_{x\in \mathscr{X}}  \qquad & \sum_{i=1}^{I}\frac{||C_{i}^T+D_{i}||^2}{A_{i}^Tx+B_{i}}  \\
 \text{s.t.}  \qquad & A_{i}^Tx+B_{i} \geq 0, i=1,2,3,,,I, 
\end{aligned}
\end{equation} 
which is converted into a SOCP by
\begin{equation}\label{eq-scop8}
\begin{aligned}
\min\limits_{x\in \mathscr{X}}  \qquad & \sum_{i=1}^{I}t_{i} \\
 \text{s.t.}  \qquad & (C_{i}^T+D_{i})^T(C_{i}^T+D_{i}) \leq t_{i}(A_{i}^Tx+B_{i}),    \\
  \qquad & A_{i}^Tx+B_{i} > 0, i=1,2,3,,,I.  
\end{aligned}
\end{equation} 

SOCP can be efficiently solved by the interior point method. Meanwhile, SOCP is less general than SDP since equation (\ref{eq-scop}) may be transformed into an SDP problem. However, the complexity of solving SOCP is $O(n^2\sum_{i}{n_i})$, while the complexity for SDP is $O(n^2\sum_{i}{n_i}^2)$\cite{nest}. Such complexity difference is crucial for large-dimension problems. 

Finally, to apply SOCP for RIS-aided optimizations, the first step is to utilize AO or BCD scheme to decouple the control variables into multiple sub-problems, e.g., BS precoding matrix and RIS passive beamforming\cite{guiz,yiqing,menghua}, coordinated transmit beamforming and RIS passive beamforming\cite{hailiang}. For example, the max-min data rate problem in \cite{menghua} is decoupled into SOCP-based BS beamforming and SDR-based RIS phase-shift control, and the data rate maximization problem in \cite{jiey} is converted into a SOCP-based BS active beamforming and SDR-based RIS passive beamforming.         






\subsection{Fractional Programming} 
FP refers to optimization problems involving ratios or fractional terms.
FP is particularly useful for wireless network optimizations due to the fractional terms in communication systems, especially for SINR and energy efficiency\cite{zappone}. 

Consider a single-ratio FP problem to maximize the SINR of single UE by $\max\limits_{x\in \mathscr{X}} {f(x)}/{g(x)}$, where $f(x)$ is the signal strength and $g(x)$ is the interference and noise. There are many classic methods to solve FP problems, such as Charnes-Cooper transform and Dinkelbach’s transform\cite{Dinke}. Dinkelbach’s method reformulates the problem into $\max\limits_{x\in \mathscr{X},y\in \mathbb{R}}  f(x)-yg(x)$, where $y$ is the auxiliary variable that is updated iteratively $y^{(l+1)}={f(x)^l}/{g(x)^l}$, 
and $l$ is the iteration number. Then, alternatively updating $y$ and $x$ will lead to a converged solution with non-decreasing $y^l$. However, instead of the single-ratio problem, sum-ratio FP problems are more frequently involved in wireless networks, i.e., maximizing sum-rate or total channel capacity as 
$\max\limits_{x\in \mathscr{X}}  \ \sum_{i=1}^{I} f_{i}(x)/g_{i}(x) $.

However, classic methods can not be directly generalized to sum-ratio cases, since maximizing single ratios cannot guarantee the convergence and maximization for sum-ratio cases. An equivalent transform proposed by \cite{shenk} is 
\begin{equation}\label{eq-fp5}
\max\limits_{x\in \mathscr{X},y\in \mathbb{R}}  \quad 2yf(x)^{0.5}-y^{2}g(x),  
\end{equation} 
which can be readily converted into sum-ratio problems.
In addition, equation (\ref{eq-fp5}) is further generalized to sum-ratio problems as
\begin{equation}\label{eq-fp7}
\max\limits_{x\in \mathscr{X},y\in \mathbb{R}}  \quad \sum_{i=1}^{I}F_{i}(2y_{i}C_{i}(x)^{0.5}-y_{i}^{2}D_{i}(x)),  
\end{equation}
where $F_{i}$ is a non-decreasing function. Equation (\ref{eq-fp7}) is particularly useful given the frequently used term $\sum log(1+SINR)$ in wireless communications. 

The FP method can significantly lower the problem-solving complexity by eliminating fractional items. This transformation is very useful for RIS-related optimization problems, especially considering that RIS phase shifts will affect the received signal strength and interference simultaneously.
In addition, the FP method can be particularly useful for RIS-related max-min fairness problems, which are usually formulated as
$\max\limits_{x\in \mathscr{X}}  \  \min\limits_{1\leq i \leq I} \ {f_{i}(x)}/{g_{i}(x)}$, where $x$ indicates the control variables, e.g., RIS phase shifts and BS transmit power. $f_{i}(x)$ can be the signal strength of user $i$, and $g_{i}(x)$ indicates the interference and noise. Then the max-min fairness problems can be reformulated as 
\begin{equation}\label{eq-fp9}
\begin{aligned}
\max\limits_{x\in \mathscr{X}, y,z\in \mathbb{R}}  &  \quad z \\
 \text{s.t.}  \quad & 2y_{i}f_{i}(x)^{0.5}-y_{i}^{2}g_{i}(x) \geq z; i=1,2,3,,,I.
\end{aligned}
\end{equation}
where $z$ is an intermediate objective function that is included in the constraint. A detailed proof of obtaining equation (\ref{eq-fp9}) can be found in \cite{shenk}.

The FP method significantly reduces the optimization complexity by decoupling the fractional terms. Therefore, it has been widely used in wireless network optimizations, including power control, beamforming, energy efficiency, and so on\cite{huayan,shuaiqi}. However, note that FP is usually used for transformation, and then the reformulated problems still need to be solved by other techniques. A widely considered method is first to apply FP to eliminate the fractional terms in objective functions, e.g., throughput and power consumption for energy efficiency maximization, received signal strength and interference for SINR maximization. And then, AO is used to separate the coupled control variables, e.g., RIS phase-shift design and BS transmit power control, and optimize each sub-problem iteratively.





\begin{figure}[!t]
\centering
\setlength{\abovecaptionskip}{-3pt} 
\includegraphics[width=0.8\linewidth]{Image/fig-bb.jpg}
\caption{Using BnB for RIS control with discrete phase shifts.}
\label{fig-bb}
\vspace{-15pt}
\end{figure}








\begin{table*}[!t]
\caption{Summary of model-based optimization algorithms for RIS-aided wireless networks }
\centering
\setstretch{1.15}
\small
\resizebox{1\textwidth}{!}{%
\begin{tabular}{|m{1cm}<{\centering}|m{3.7cm}<{\centering}|m{3.3cm}<{\centering}|m{3cm}<{\centering}|m{3cm}<{\centering}|m{4.9cm}<{\centering}|}
\hline 
Methods &  Main features   &     Advantage    &  Drawbacks    &  Difficulties   &  Application scenarios  \\
\hline
AO & Decoupling the joint optimization into multiple sub-problems, and alternatively optimizing each sub-problem.   &  The problem-solving complexity is greatly reduced. Each sub-problem may be easier to solve.  & Iterative optimization may lead to sub-optimal results; the convergence must be proved. &  The complexity is high when each sub-problem is still complicated.  & AO is the most widely applied optimization scheme for RIS-aided networks. It decouples the joint optimization into multiple sub-problems and then optimizes them iteratively \cite{chongwen}.  \\
\hline
BCD &  The control variables are divided into multiple blocks. Minimizing one block in each iteration and keeping other blocks fixed.  & Cheap iteration costs; low memory requirements; potential for parallel implementation   &  Block selection may affect the BCD performance, and block updating is difficult in some cases.  &  Block selection and updating methods are complicated.  & BCD employs alternating schemes to reduce joint optimization complexity, e.g., sum-rate maximization \cite{cunhua} and power minimization\cite{zhiyang}, and one block in BCD may include multiple control variables.     \\
\hline
MM & Iteratively constructing and optimizing an upper bound surrogate function that can locally approximate objective functions. &  Avoiding the complexity of optimizing non-convex objective functions directly.  &  The surrogate function must be a strict tight upper bound for objective functions, which is hard to achieve in practice.   &  The surrogate function must follow the shape of objective functions and meanwhile be easy to optimize.  &  The MM applies surrogate functions as low-complexity solutions for many RIS-aided optimizations, i.e., sum-rate maximization\cite{cunhua}, fairness maximization\cite{menghua,guiz}, secure transmission\cite{huiming2}.    \\
\hline
SCA & Constructing and optimizing surrogate functions iteratively to estimate the objective function.  & Low computational complexity; the tight upper bound is not required for the surrogate function; naturally implemented in a distributed manner.  &  The step size selection is critical for an accurate approximation.  &  Surrogate function and step size selection.  &   SCA relaxes the tight upper bound constraint on surrogate function design. Such an estimation-based approach is easier to be implemented in RIS-related optimization, e.g., sum-rate maximization \cite{ming,yuanbin} and power minimization\cite{huimei,jianyue}.     \\
\hline
SDR & SDR is used to solve QCQP problems by relaxing the rank constraint. Then the reformulated problem is efficiently solved by SDP. & Given the objective problem, SDR is easily implemented without extra parameters or settings. & Approximation is required if the relaxed solution is not rank one.  &  The reformulated problem is complicated if the achieved solution is not rank one. & SDR is particularly useful to solve $rank(x)=1$ constraints, such as RIS phase-shift constraint $|\theta_{n}|=1$ \cite{boya} \cite{peilan,qingqing}.\\
\hline
SOCP & SOCP utilizes the property of the second-order cone, and many problems are reformulated into SOCP, which is much easier to be solved.  & SOCP can be efficiently solved by many existing algorithms. It has a lower complexity $O(n^2\sum_{i}{n_i})$ than SDR.  &  Problem reformulation into SOCP is complicated.  &  The main difficulty lies in how to reformulate the original problem into SOCP.  & SOCP can be very useful if the RIS-related problems can be easily formulated as a second-order cone, which has been used for power minimization \cite{yiqing} and user fairness \cite{guiz,hailiang,menghua}.\\
\hline
FP & FP refers to optimization problems that involve fractional terms, which is very useful for wireless communications considering the form of SINR and energy efficiency.  & FP is easily implemented without extra parameters or problem formulation requirements.  &  The reformulated problems generally require iterative optimization to approximate the solution of original FP problems.   & Compared with single-ratio problems, wireless networks are more related to sum-ratio problems, which are more complicated to solve.   & FP is particularly useful when decoupling the fractional terms in RIS-related problems, e.g., SINR and energy efficiency. It is widely applied for optimizing RIS-aided networks \cite{huayan,jianyue,chongwen}.    \\
\hline
BnB & BnB is mainly designed for combinational optimization problems. It applies a tree to enumerate all possible subsets and sub-problems.  &  Lower complexity than direct optimizations. The solution quality is controlled by customized search, branching, and pruning rules.  &  The algorithm is slow when constantly searching or branching in the worst case.   &  The algorithm performance relies on the searching and pruning method, which is hard to select in some cases.  &  Different from aforementioned technique, BnB is mainly applied for discrete and combinational optimization problems, i.e., RIS on/off and discrete phase shift\cite{shiqi,qingqing2,boya}.     \\
\hline
\end{tabular}}
\label{tab-comparison}
\vspace{-13pt}
\end{table*}



\subsection{Branch-and-Bound } 

BnB is a classic scheme for combinatorial and discrete optimization problems\cite{clau}. To minimize $f(x)$ with $x\in \mathscr{X}$, BnB applies a tree scheme to enumerate all possible subsets $X_{i} \subseteq \mathscr{X}$, and each subset $X_{i}$ indicates a sub-problem $f_{i}(x)$. Solving sub-problems $f_{i}(x)$ will generate and prune branches based on the estimated lower and upper bounds.  

A BnB algorithm consists of three basic operations: branching, bounding, and pruning.
Considering a non-linear integer programming problem, and the BnB scheme is summarized as Fig. \ref{fig-bb}, including the search method, branching strategies, and pruning rules. In particular, the search method indicates the order of sub-problem exploration in the tree, e.g., which RIS phase-shift combination is first explored. The branching strategy specifies how to generate new sub-problems from the solution space, e.g., how to generate a new set of phase-shift designs. Finally, pruning rules can prevent exploring specific regions of the tree, which will eliminate sub-optimal RIS phase-shift solutions.
BnB produces a series of sub-problems $f_{i}(x)$ that are equivalent to the original $f(x)$, which is much more efficient than brute-force enumeration.  It provides an alternative solution for challenging problems that cannot be solved directly. An important advantage is that the quality of the solution is controlled by customized searching, branching, and pruning rules. 

BnB is mainly applied for RIS control with discrete phase shift, including sum-rate maximization \cite{boya}, power minimization \cite{qingqing2}, and max-min SINR \cite{shiqi}. The main reason is that the problem formulations are usually MINLP problems, which are NP-hard and intractable. As shown in Fig. \ref{fig-bb}, the MINLP is converted into an 0-1 integer linear programming using the special ordered set of type 1 (SOS1) transformation \cite{qingqing2} and reformulation-linearization \cite{shiqi}. BnB performance is very dependent on search and pruning rules, and defining these rules can be difficult in some cases. In addition, the algorithm may converge slowly when constantly searching and branching for new solutions, which may be caused by the considerable number of RIS elements.



\subsection{Discussions and Numerical Results}

Table \ref{tab-comparison} summarizes model-based algorithms for RIS-aided wireless networks, including main features, advantages, disadvantages, difficulties, and application scenarios. 

Firstly, considering the high complexity of RIS-related optimization, AO is regarded as the primary scheme to decouple the joint optimization problem into several sub-problems. Then, each sub-problem is alternatively solved by using different algorithms, e.g., SCA, SDR, and BnB. Compared with AO, the BCD algorithm applies a similar iterative optimization scheme, but one block may include multiple control variables. When there are a large number of control variables, the BCD algorithm can be more efficient, e.g., coupled optimization problem with a considerable number of control variables.

MM and SCA are two estimation-based algorithms that avoid the complexity of direct optimizations. However, the MM algorithm requires a tight upper bound when designing the surrogate function. Such requirements can be impractical in some cases, especially considering non-convex and highly non-linear RIS phase-shift design problems.
By contrast, the SCA method relaxes the upper bound requirement for surrogate functions, which is more flexible and easier for design and implementation.
However, without the upper bound constraint, the updating step size in SCA may affect the solution quality, which should be carefully selected. MM and SCA are usually considered low-complexity solutions for RIS-aided wireless network optimizations. 

SDR and FP are usually combined with other techniques for optimizations. In particular, SDR is mainly used to relax the RIS phase constraints, while FP can decouple the numerator and denominator for SINR and energy efficiency terms. These two techniques reformulate the original problems into low-complexity or even convex forms, then other techniques can be applied. Meanwhile, SOCP takes advantage of the property of the second-order cone, which is efficiently solved by many existing methods. But the main difficulty is how to transform the problem with logarithm and fractional terms into a second-order cone. BnB is mainly designed for combinational and discrete optimization problems, e.g., RIS control with discrete phase shifts and elements on/off.


\begin{figure}[!t]
\centering
\subfigure[Average throughput comparison under various peak traffic loads. AOFP: combining AO and the FP algorithm; surrogate method: using a surrogate function to approximate the objective function in a black-box manner. ]{ \label{fig_result1}
\includegraphics[width=7.2cm,height=5.2cm]{Image/fig-results2.jpg}
}
\,
\subfigure[Convergence performance of AOFP under various numbers of RIS elements.]{ \label{fig_result2}
\includegraphics[width=7.2cm,height=5.2cm]{Image/fig-results1.jpg}
}
\setlength{\abovecaptionskip}{0pt} 
\caption{Simulation results by combining AO and FP to maximize the channel throughput. We consider a MISO system with one BS and multiple UEs, and the daily traffic load pattern is shown in \ref{fig_result1}. Detailed simulation parameters and algorithms can be found in \cite{zhou2023cooperative}.}
\vspace{-15pt}
\label{fig-re-model}
\end{figure}


Finally, it is worth noting that these algorithms are not independent, and multiple algorithms are usually combined for transformation and optimizations. The main objective of Table \ref{tab-comparison} is to analyze the feasibility of these problems for various RIS-related optimizations, and the most efficient solution for specific scenarios requires case-by-case analyses. For instance, Fig. \ref{fig-re-model} shows an example of combining AO and FP for RIS phase-shift control in an MISO system with multiple UEs. Specifically, it applies FP to decouple the received signal strength with interference and noise, and then uses AO to optimize multiple control variables alternatively\cite{zhou2023cooperative}. Fig. \ref{fig_result1} presents the average throughput under various peak traffic loads, which involves a daily traffic load pattern as shown by the blue shade in Fig. \ref{fig_result1}. Meanwhile, we consider surrogate optimization as a baseline, which applies surrogate functions to approximate the objective function in a black-box manner. When the peak traffic load is light, one can observe that AOFP and surrogate function have comparable performance, which means that the channel capacity can already satisfy the traffic demand. However, when peak traffic load increases, AOFP attains higher throughput than baselines, which demonstrates that RIS control and deployment strategy should consider dynamic UE traffic demand. Additionally, Fig. \ref{fig_result2} presents the convergence performance of AOFP, in which the objective function is improved with increasing iterations and finally converges. This reveals the basic features of AO, which is to guarantee the objective function will be improved iteration-by-iteration, and such a scheme has been widely used in RIS-related optimization studies. 


