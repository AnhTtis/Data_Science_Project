



\section{ML-enabled Optimization for RIS-aided Wireless Networks }
\label{sec-ml}

ML has achieved great success in various fields, and this section investigates ML applications for the control and optimization of RIS-aided wireless networks, including supervised learning, unsupervised learning, RL, FL, graph learning, transfer learning, hierarchical learning, and meta-learning.

A variety of algorithms have been developed to optimize RIS-aided wireless networks. Early studies mainly considered model-based methods, and some heuristic algorithms are deployed as low-complexity solutions. However, there are several challenges for these conventional optimization techniques:

1) \textbf{Highly dynamic wireless environment}: Wireless networks are highly dynamic due to frequently changing channel conditions, traffic demands, and user conditions. These dynamics lead to great difficulty for conventional optimization schemes. As an example,  model-based methods need full knowledge of the formulated problem, but some sensitive information, e.g., real-time user locations, may be unknown in practice.

2) \textbf{Evolving network architecture}: The wireless network architecture is constantly evolving from RAN to cloud RAN, virtual RAN, and Open RAN. Consequently, these new architectures increase the complexity of network management, and conventional algorithms may have difficulty modelling and optimizing such complicated systems.

3) \textbf{Diverse user requirements}: Wireless network user types are not limited to enhanced Mobile Broad Band, Ultra Reliable Low Latency Communications, and massive Machine Type Communications. Some newly emerged applications, such as virtual and augmented reality, have more stringent requirements on network metrics, leading to a great burden for conventional optimization methods. 

Given these challenges, ML-enabled control and optimization techniques have become appealing approaches for wireless communications in general, as well as for  RIS-aided wireless networks. In the following, we will introduce the fundamentals and applications of various ML techniques. \blue{It is worth noting that ML algorithms can be applied to optimize RIS-aided networks in various ways, e.g., controlling RIS elements directly or jointly optimizing the whole RIS-aided network scenario. Here we focus on the application of using ML algorithms to optimize RIS elements directly, e.g., supervised learning-based sum-rate prediction, unsupervised learning-enabled RIS phase-shift optimization, RL-enabled RIS phase-shift control, and so on. }   








\begin{table*}[!tbhp]
\caption{Summary of supervised learning for RIS-aided wireless networks}
\centering
\small
\setstretch{1.1}
\resizebox{1\textwidth}{!}{%
\begin{tabular}{|m{0.6cm}<{\centering}|m{2cm}<{\centering}|m{1.5cm}<{\centering}|m{2cm}<{\centering}|m{1.1cm}<{\centering}|m{1.7cm}<{\centering}|m{1cm}<{\centering}|m{1.5cm}<{\centering}|m{2.5cm}<{\centering}|m{2.2cm}<{\centering}|m{2.7cm}<{\centering}|}
\hline 
Ref. &  Scenario  & Phase-shift resolution & Channel settings & CSI & Objectives  &  Model &  Layer  &  Data acquisition & Input data  & Output data \\
\hline
\cite{zhang2021deep} & \multirow{2}*{\makecell{Point-to-point\\ SISO}}  &  \multirow{2}*{Discrete} & \multirow{2}*{\makecell{DeepMIMO \\ dataset}}  &  \multirow{2}*{Predicted} & Maximizing data rate  &  CNN  & 9 Conv2D layers  & Collected from fully active model & Estimated partial channels   &  Full channel information  \\
\cline{6-10}
 &   &  & &  &  &  FNN  & 5 Layers  &  Codebook &  CSI  & RIS phase shifts  \\
\hline
\cite{stylianopoulos2022deep} &  Rich-scattering Point-to-point &  Binary &  Generated by simulator    &  Predicted  & Maximizing data rate  &  DNN  &  4 layers  & Obtained from simulators & RIS phases   &  Second-order moments of CSI. \\
\hline
\cite{taha2021enabling} & Point-to-point SISO  & Continuous &  Wideband geometric & Partially  &  Maximizing data rate  &   DNN  &  6 layers   &  Exhaustive generation  &  CSI  & Estimated data rate  \\
\hline
\cite{aygul2021deep} & Point-to-point SISO   & Continuous  &   Wideband geometric   &  Perfect    & Maximizing data rate    & DNN   & 5 layers & Exhaustive search beamforming  &  Pilot signals   &  RIS phase shifts  \\
\hline
\cite{alexandropoulos2020phase} & Point-to-point SISO  &  Discrete &  Rician fading   &  Perfect & Maximizing data rate & DNN  &   5 layers  &  Simulation generated   & Transmit power, and positions    & RIS phase shifts   \\
\hline
\cite{ozdougan2020deep} & MISO-DL-SU  & Continuous &   Quasi-static flat-fading   &   Estimated  &  Maximize energy efficiency  &  DNN  &  5 layers; 6 layers  &  Separately generated  & Pilot signal  & RIS phase shifts and BS beamforming vector  \\
\hline
\cite{huang2019indoor} &  MISO-DL-SU  & Continuous &   Rayleigh fading   & Perfect     &  Maximizing data rate  & DNN    & 5 layers      &  Generated by estimation   &  User positions  & RIS phase shifts  \\
\hline
\cite{yang2021intelligent} & SISO-UL-MU   & Continuous &   Quasi-static  &  Perfect  & Maximizing SINR   &   CNN    &  5 layers   &  Collected using by USRP2 testbed  &  Incident RF signal  & Interfering user set  \\
\hline
\cite{song2021truly} & MISO-DL-MU  &  Continuous &  Rician fading  &  Perfect  & Maximizing secrecy rate  &  DNN    & 6 layers    &  Generated by AO algorithm     & Channel coefficients   & RIS phase shifts   \\
\hline
\cite{hu2021reconfigurable} & \multirow{2}*{\makecell{ MISO-DL-MU \\MEC }}  &  \multirow{2}*{Continuous} &   \multirow{2}*{\makecell{ Rician fading }}   &   \multirow{2}*{Predicted} &  \multirow{2}*{\makecell{Maximizing \\ data rate}}  & DNN   & 7 layers  & \multirow{2}*{\makecell{Generated by \\ BCD algorithm}} & CSI  &  \multirow{2}*{\makecell{RIS phase shifts; \\ offloading decision}}   \\
\cline{7-8} \cline{10-10}
 &   &  & &  &  &  DNN  & 7 layers  &   &  UE positions  &  \\
\hline
\end{tabular}}
\label{tab-supervised}
\vspace{0pt}
\end{table*}




\begin{figure*}[!t]
\centering
\setlength{\abovecaptionskip}{-5pt} 
\includegraphics[width=0.95\linewidth]{Image/fig-nn.jpg}
\caption{Supervised learning for RIS-aided wireless networks.}
\label{fig-nn}
\vspace{0pt}
\end{figure*}



\subsection{Supervised Learning-enabled Optimization}

Supervised learning is designed to find the hidden relationships between inputs and labeled outputs. Supervised learning algorithms adjust their parameters to map the input to the expected output, and this relationship is used for the prediction and classification of unseen data. Table \ref{tab-supervised} summarizes supervised learning-based control and optimization studies for RIS-aided wireless networks. 
It shows that most studies consider partial CSI or pilot signals as input to predict full CSI or RIS phase shifts, and then utilize the prediction results to maximize the data rate. Meanwhile, there are various approaches for neural network model selection, dataset acquisition, input and output data definitions, etc.
This subsection will discuss how to apply supervised learning for optimizing RIS-aided wireless networks, including data acquisition, neural network architecture, loss functions and algorithm training. 

\subsubsection{Dataset Acquisition in RIS-aided Environments}
\label{sec-dataset}
A fine-grained dataset is the prerequisite for deploying supervised learning, since it relies on the labeled output for validation. Table \ref{tab-supervised} indicates that the dataset is generated in various ways: simulators, exhaustive searches, codebook, model-based optimization algorithms or live networks. For example, the exhaustive searches mean trying different solutions and then collecting the corresponding output to form labeled datasets\cite{taha2021enabling}\cite{aygul2021deep}. By contrast, a more efficient method is to reuse the data produced by AO\cite{song2021truly} and BCD\cite{hu2021reconfigurable} as model-based optimization algorithms. 

In addition, the algorithm performance also depends on the dataset size, ranging from 5000 \cite{alexandropoulos2020phase} and 30000\cite{aygul2021deep,taha2021enabling} to 200000 \cite{hu2021reconfigurable} in several studies. The simulation results in \cite{aygul2021deep,taha2021enabling} demonstrate that the achievable data rate is significantly improved when the number of training samples increases from 5000 to 30000. Note that the complex entry of the input data, especially the channel coefficient, is usually split into real and imaginary parts, increasing the dimension of the neural network input. 
Although there are several ways to generate the data for supervised learning, most existing datasets are simulation-based. Realistic datasets that are produced in real-world RIS-aided environments are still very rare. 

\subsubsection{Loss Functions and Algorithm Training}
Given the huge number of training samples, supervised learning models are trained to produce the expected output. Suppose that the prediction output is the RIS phase shifts \cite{zhang2021deep,aygul2021deep,alexandropoulos2020phase}, and   
the loss function is defined to minimize the mean square error (MSE) of algorithm training
\begin{equation}\label{ml-nnloss}
Loss (\omega)=\frac{1}{N}\sum^N_{i=1}(\theta_i-\hat{\theta}_{i}(\omega))^2,    
\end{equation}
where $N$ is the total number of outputs, i.e., the number of RIS elements, $\theta_i$ is the desired phase shift given by the dataset, $\omega$ is the neural network weight, and $\hat{\theta}_{i}(w)$ indicates the RIS phase shifts predicted by neural networks. The desired phase shift $\theta_i$ can be obtained in various ways, such as \blue{exhaustive} search or model-based approaches\cite{taha2021enabling,song2021truly}, which have been introduced in Section \ref{sec-dataset}. For example, Song \textit{et al.} apply AO to produce a dataset with desired targets for DNN training\cite{song2021truly}, and Hu \textit{et al.} apply BCD algorithm to generate target phase shift to train DNN models\cite{hu2021reconfigurable}. Meanwhile, note that the dataset must be divided into training and validation samples, since the objective of algorithm training is to predict unseen data. For example, the authors in \cite{song2021truly} include 10000 samples to predict the RIS phase shifts, of which 90\% is used for training and the remaining 10\% for testing purposes. 





\subsubsection{Neural Network Architecture and Overfitting}
Table \ref{tab-supervised} shows that DNN is used in most studies to predict CSI or RIS phase shifts, and the network architecture ranges from 4 to 9 layers. It is known that more hidden layers may provide a better performance, but the computational complexity and training time will increase. Hence, the network architecture selection should consider the trade-off between performance and training costs.  

Overfitting is another important issue for neural network training. It means that the algorithm fits exactly to the current training data, but cannot achieve satisfactory prediction for unseen data, which should be carefully prevented. One solution is to add a random dropout layer with probabilities, ignoring the contribution of some neurons \cite{hu2021reconfigurable}. Multiple methods are provided by \cite{song2021truly} to suppress overfitting in predicting RIS phase shifts, including larger datasets (CSI and RIS phase shift pairs), decreasing hidden layers, and early stopping. 

Fig. \ref{fig-nn} summarizes how to apply supervised learning for RIS-aided wireless networks. Firstly, the datasets can be produced by various methods, including simulators, exhaustive searches, testbed, and model-based methods. The collected dataset may include UE positions, data rates, and pilot signals received at the transmitter and receiver, which mainly depends on the designed prediction algorithms. Then, one specific model will be selected, i.e., FNN, convolutional neural networks (CNNs), and recurrent neural networks (RNNs). Note that each neural network model has unique features and advantages, e.g., RNNs are suitable for handling sequential data, and CNNs can better handle spatial data. The selection of neural network models requires case-by-case analyses of the dataset size, quality, and data-processing demands.  The number of nodes and hidden layers of neural networks should be carefully designed, which will affect the network training time and accuracy. Finally, selected models are trained and implemented, and the algorithm output includes RIS phase shifts, achieved data rate, BS beamforming vectors and so on, which are further used to optimize network performance.
Supervised learning has been widely used for wireless networks. However, note that it relies on high-quality labeled datasets for model training, which may be inaccessible in practice. In addition, the algorithm performance is sensitive to hyperparameters, and the fine-tuning of parameters requires considerable experience. 


\begin{table*}[!t]
\caption{Summary of unsupervised learning for RIS-aided wireless networks }
\centering
\small
\setstretch{1.05}
\resizebox{1\textwidth}{!}{%
\begin{tabular}{|m{0.7cm}<{\centering}|m{2cm}<{\centering}|m{1.5cm}<{\centering}|m{1.9cm}<{\centering}|m{1.3cm}<{\centering}|m{1.7cm}<{\centering}|m{1cm}<{\centering}|m{1.5cm}<{\centering}|m{2.7cm}<{\centering}|m{2.4cm}<{\centering}|m{2.2cm}<{\centering}|}
\hline 
Ref. &  Scenario  & Phase-shift resolution & Channel settings & CSI & Objectives  &  Model &  Layer  &  Data generation & Input data  & Output data \\
\hline
\multirow{3}*{\cite{song2020unsupervised}} & \multirow{3}*{\makecell{MISO-DL-MU}} &   \multirow{3}*{Continuous}  & \multirow{3}*{\makecell{Rician \\ fading}}  &\multirow{3}*{Perfect} &  \multirow{3}*{\makecell{Maximizing \\ sum-rate}}  &  CNN  &  6 layers &  \multirow{3}*{\makecell{Generated by \cite{huayan}}}  &  CSI  &  RIS phase shifts  \\
\cline{7-8} \cline{10-11}
 &   &  & & &  &  FNN & 5 layers  &   & Effective channel matrix   &  BS beamforming vector  \\
\hline
\cite{nguyen2021machine} & MIMO-DL-SU  &  Continuous &   Rician fading   &  Perfect   &  Maximizing spectral efficiency  & DNN  &  4 layers  & Generated by random exploration  & CSI   & RIS phase shifts   \\
\hline
\cite{dinh2022unsupervised} & Broadcasting
Communications for IoTs  &  Continuous &  Rician fading   & Statistical  &  Maximizing spectral efficiency & DNN  & 4 layers   &  Generated by random exploration  &  CSI  & RIS phase shifts  \\
\hline
\cite{gao2020unsupervised} & MISO-DL-SU  &  Continuous &  Rayleigh fading   &  Perfect & Maximizing data rate  & FNN   &   7 layers    & Generated as \cite{huang2019indoor}   &  CSI   & RIS phase shifts  \\
\hline
\multirow{4}*{\cite{lopez2022deep}} & \multirow{4}*{\makecell{MISO-DL-MU}}  &  \multirow{4}*{\makecell{Continuous/  \\ discrete}} &  \multirow{4}*{\makecell{ Geometry-based \\ clustered\\ delay line}} &  \multirow{4}*{Estimated} &  \multirow{4}*{\makecell{Maximizing \\ sum-rate}} &  FNN &  6 layers  &  Obtained from \cite{bjornson2021configuring}  &  CSI  &  RIS phase shifts \\  
\cline{7-11} 
&   &  &  &  & &  \multicolumn{3}{c|}{ \makecell{k-means is used to cluster RIS elements \\ based on their estimated cascaded channel \\ coefficient without dataset.}}  & Estimated cascaded channel of RIS elements.  &  RIS element clusters.  \\
\hline
\end{tabular}}
\label{tab-unsuper}
\vspace{-10pt}
\end{table*}


\subsection{Unsupervised Learning-based Optimization}

Supervised learning is data-demanding, but fine-grained labeled datasets may be inaccessible in practice, preventing the application of supervised learning algorithms. On the contrary, unsupervised learning can find hidden patterns of unlabeled data without predefined targets or human intervention. Table \ref{tab-unsuper} summarizes unsupervised learning algorithms for RIS-aided wireless networks. It shows that neural networks are used in unsupervised manners for RIS phase-shift configuration. Meanwhile, some classic unsupervised learning methods, such as k-means, can also be applied for clustering RIS elements. This subsection will introduce unsupervised neural networks and clustering algorithms.

\begin{figure}[!t]
\centering
\setlength{\abovecaptionskip}{-5pt} 
\includegraphics[width=0.8\linewidth]{Image/fig-unsupnn.jpg}
\caption{Unsupervised neural networks for optimizing RIS phase shifts.}
\label{fig-unsupnn}
\vspace{0pt}
\end{figure}

\subsubsection{Algorithm Training and Network Architecture of Unsupervised Neural Networks} \label{s-ml-un} 
Supervised neural networks aim to minimize the loss between predicted results and desired target, i.e., predicted and target data rate in the dataset. However, in unsupervised neural networks, the loss function is directly related to optimization objectives. 
Specifically, we consider a single-user scenario as an example, and the user SNR is
\begin{equation}\label{eq-unsnr}
\eta_{k}=\frac{|(\bm{h}^{R}\bm{\Theta}\bm{G}+\bm{h}^{D})p|^2}{N_{0}^2},
\end{equation}
where $p$ is the transmit power at the BS, $\bm{G}$ indicates the channel gain from BS antennas to RIS elements, $h^{R}$ indicates the channel gain from RIS elements to the user, $h^{D}$ indicates the channel gain from BS antennas to the user, $N_{0}^2$ is the noise power, and $\bm{\Theta}$ is the matrix of RIS phase shifts. As shown in Fig. \ref{fig-unsupnn}, the neural network considers the channel state information as input, including $\bm{G}$, $\bm{h}^{R}$ and $\bm{h}^{D}$. Then, the output is the predicted RIS phase shifts $\bm{\Theta}$. The loss function is defined by
\begin{equation} \label{eq-unsup}
Loss (w)=-\frac{1}{\mathcal{T}}\sum^{\mathcal{T}}_{i=1}(\bm{h}^{R}\bm{\Theta}\bm{G}+\bm{h}^{D}),     
\end{equation}
where $\mathcal{T}$ is the minibatch size. To minimize the loss function equation (\ref{eq-unsup}), $\bm{h}^{R}\bm{\Theta}\bm{G}+\bm{h}^{D}$ must be maximized. This means that the neural network must predict appropriate RIS phase shifts $\bm{\Theta}$ to maximize $\bm{h}^{R}\bm{\Theta}\bm{G}+\bm{h}^{D}$, and the SNR will be maximized accordingly. 

Table \ref{tab-unsuper} shows that most existing works apply 2 to 5 hidden layers. In particular, the hidden layer numbers are related to the problem's complexity. The authors in \cite{nguyen2021machine} used 1 hidden layer with 40 nodes for $8 \times 2$ MIMO, and 2 hidden layers for $16 \times 2$ MIMO, achieving satisfactory simulation results without overfitting or underfitting. In addition, similar to supervised learning, early stop is applied in \cite{song2020unsupervised} to prevent overfitting.   

\subsubsection{Clustering Algorithms} 
Clustering algorithms are usually unsupervised ML algorithms, i.e., k-means and Density-based spatial clustering of applications with noise (DBSCAN). These algorithms are designed to partition objects into multiple sets to minimize the within-cluster sum of squares. Specifically, it aggregates objects with the same hidden patterns. For instance, k-means is used in \cite{lopez2022deep} to group RIS elements according to estimated channel coefficients, and then each group has the same RIS configurations to reduce the computational complexity. 

The main advantage of unsupervised learning is that it has no requirement on predefined targets, which is more practical in real-world applications. However, the absence of targets means that the model output is hard to validate or verify, and the solution quality cannot be guaranteed. 




\begin{table*}[!t]
\caption{Summary of reinforcement learning for RIS-aided wireless networks }
\centering
\small
\setstretch{1.05}
\resizebox{1\textwidth}{!}{%
\begin{tabular}{|m{0.6cm}<{\centering}|m{2.1cm}<{\centering}|m{1.5cm}<{\centering}|m{1.7cm}<{\centering}|m{1.3cm}<{\centering}|m{1.7cm}<{\centering}|m{1.8cm}<{\centering}|m{2.9cm}<{\centering}|m{3.1cm}<{\centering}|m{3.3cm}<{\centering}|}
\hline 
Ref. &  Scenario  & Phase-shift resolution & Channel settings   & CSI & Objectives &  Algorithm  &   State definition  &  Action definition  & Reward function  \\
\hline
\cite{yang2020deep} & MISO-DL-MU NOMA  & Discrete &  Rayleigh fading  & Perfect  & Maximizing sum-rate  &  DDPG  & Current RIS phases &  RIS phase shifts    &  Sum-rate    \\
\hline
\cite{taha2020deep} & Point-to-point communications  &  Discrete   & Wideband geometric   & Estimated   &  Maximizing data-rate &  DRL & CSI  &  RIS phase shifts   & Data rate   \\
\hline
\cite{guo2021learning} &  MISO-DL-MU UAV  &  Continuous  & Saleh-Valenzuela  &  Imperfect  & Maximizing secrecy rate  &  DDPG  &  CSI  & RIS phase shifts and BS beamforming vector  &  Secrecy rate with penalty   \\
\hline
\cite{yang2020intelligent} & MISO-DL-MU with jammer & Continuous  &  Quasi-static flat-fading &  Perfect    & Maximizing sum-rate  &  Fast-policy hill-climbing learning  &  Previous jammer power and SINR, current CSI  & BS transmit power and RIS phase shifts   &   Maximizing data-rate, decreasing BS power and SINR penalty\\
\hline
\cite{yang2020deep2} & MISO-DL-MU  &  Continuous   & Rayleigh fading  &  Delayed &  Maximizing secrecy rate  &  DRL  &  CSI, previous secrecy rate and transmission rate, QoS level  & BS beamforming vector and RIS phase shifts   &  Maximize the system secrecy rate, guaranteeing QoS requirements.  \\
\hline
\cite{feng2020deep} & MISO-DL-SU  & Continuous  &  Rayleigh fading  &  Perfect  & Maximizing SNR  &  DDPG  &  SNR and current RIS phases   &  RIS phase shifts     &  Received SNR  \\
\hline
\cite{huang2020hybrid} & MISO-DL-MU THz &  Continuous  &  Rayleigh distribution  &  Perfect     &    Maximizing sum-rate  &  DDPG  &  Current BS and RIS beamforming vectors, CSI   &  BS beamforming vectors and RIS phase shifts   &  Throughput and the penalty of adjusting the beamforming direction.  \\
\hline
\cite{lee2020deep} & MISO-DL-MU  & Continuous  & Quasi-static flat-fading  &  Perfect  & Maximizing energy efficiency &  DRL  & CSI and energy level of RIS   & BS beamforming vector, RIS phase shifts and on/off   &  Energy efficiency  \\
\hline
\cite{lin2020deep} & MISO-DL-SU  &  Continuous  &  Quasi-static flat-fading   &  Perfect  & Power minimization  & DDPG   & CSI, previous outage events  & BS beamforming vector and RIS configurations  &  Energy efficiency  \\
\hline
\cite{huang2020reconfigurable} & MISO-DL-MU  & Continuous  &  Frequency flat fading   & Perfect & Maximizing channel capacity & DDPG & Transmit and received power, previous action, and CSI    & BS beamforming vector and RIS phase shifts  & Channel capacity    \\
\hline
\cite{zhang2021millimeter} & MISO-DL-MU mmWave &  Continuous  & 3GPP model  & Perfect/ Imperfect  &   Maximizing sum-rate   &   Distributed RL   &  CSI   &  RIS phase shifts  &  Data rate   \\
\hline
\cite{liu2020ris} &  MISO-DL-MU NOMA  & Continous & Rayleigh fading   &  Perfect  &  Maximizing energy efficiency  & Decaying DDQN   & RIS phases and positions, UE positions, and current BS power allocations & RIS phase and position and BS beamforming changes   &   Energy efficiency with penalty  \\
\hline
\cite{kim2021multi} &  Multi-cell communications  & Continuous  & Rayleigh fading   & Imperfect  &  Maximizing sum-rate &  Multi-agent DRL  &  Local and neighbor CSI, local sum-rate   & RIS phase shifts, BS beamforming vector, and UE power changes     & Sum-rate with interference penalties \\
\hline
\cite{samir2021optimizing} & MISO-UL-MU IoT UAV  &  Continuous  & Rician fading   & Perfect  & Minimizing sum AoI   &   DRL    & SNR and UAV height  & UAV  altitude changes  & Negative summation of age of information   \\
\hline
\multirow{2}*{\cite{yang2021machine}} & \multirow{2}*{\makecell{MISO-DL-MU \\ NOMA}}  &  \multirow{2}*{Continuous}  & \multirow{2}*{ \makecell{Rayleigh \\ fading}}  & \multirow{2}*{Perfect} &  \multirow{2}*{\makecell{Maximizing \\ sum-rate}}  & Object migration automation   &  Current RIS phase  &  Power allocation coefficient  &  Sum-rate  \\
\cline{7-10}
 &   &  &  & &   & DDPG  & Current RIS phase  &  RIS Phase changes  & Sum-rate difference \\
\hline
\end{tabular}}
\label{tab-rl}
\vspace{0pt}
\end{table*}



\subsection{Reinforcement Learning-based Optimization}


\begin{figure*}[!t]
\centering
\includegraphics[width=0.85\linewidth]{Image/fig-rl2.jpg}
\caption{\blue{\textbf{DRL-empowered RIS-aided wireless networks}}}
\label{fig-rl}
\setlength{\abovecaptionskip}{-2pt} 
\vspace{-10pt}
\end{figure*}

\blue{ RL is the most widely applied ML technique for optimization, including model-free (e.g., Q-learning, DQN, actor-critic learning) and model-based (i.e., dynamic programming) algorithms. However, defining the Markov decision process (MDP) is fundamental to applying model-based or model-free RL algorithms\cite{moerland2023model}\cite{sutton2018reinforcement}, and the RL agent interacts with the environment under an MDP scheme to learn the best long-term policy.} Given the current system state $s$, the agent selects an action $a$ for implementation and receives a reward $r$, and then the environment will move to the next state $s'$. An MDP model is critical to transforming the optimization problem into an RL context. Specifically, environmental status, control variables, and optimization objectives are defined as states, actions, and rewards, respectively. Then RL algorithms can be used subsequently to maximize the reward and improve the objective function.
Table \ref{tab-rl} summarizes existing studies that apply RL to RIS-aided wireless networks. This subsection will first analyze the state, action, and reward function definitions of these existing studies, and then present the algorithm architecture and training methods. 

\subsubsection{State Definition}
Table \ref{tab-rl} shows that the state may be defined in various ways, e.g., CSI\cite{taha2020deep,guo2021learning,yang2020intelligent}, current RIS phase\cite{yang2020deep,huang2020hybrid,liu2020ris}, position\cite{liu2020ris,samir2021optimizing}, energy level\cite{lee2020deep}, previous transmission rate\cite{yang2020deep2}.    
Specifically, the state refers to the environment status that should be considered for decision-making. For example, the CSI has a great effect on the RIS phase shifts, and therefore CSI is involved in the state definition of many studies \cite{taha2020deep,guo2021learning,yang2020intelligent}. Similarly, RIS-aided UAVs are investigated in \cite{samir2021optimizing}, and the UAV altitude is included in the state definitions because the height will directly affect the channel conditions.


\subsubsection{Action Definition}
In the context of MDP, the action indicates control variables that will change the state, such as RIS phase shifts\cite{yang2020deep}-\cite{yang2021machine}, 
BS beamforming \cite{guo2021learning,yang2020intelligent,yang2020deep2,huang2020hybrid,lee2020deep}, RIS positions\cite{liu2020ris} and elements on/off\cite{lee2020deep}. The control variables in problem formulations are easily converted into actions. However, note that many RL algorithms require discrete action spaces, but the control variables in problem formulations are usually continuous as shown in Section \ref{sec-bac}. The first solution is to quantize the control variables. For instance, the BS transmit power is quantized with an interval of 1 W\cite{lee2020deep}, and the RIS phase changes $\bigtriangleup \theta \in \{-\frac{\pi}{10},0,\frac{\pi}{10}\}$ in \cite{liu2020machine}. Another solution is to apply the deep deterministic policy gradient (DDPG) algorithm, which can handle continuous action-space problems\cite{yang2020deep,guo2021learning,huang2020hybrid,lin2020deep,yang2021machine}.         




\subsubsection{Reward Functions}
The reward function is a crucial part of RL. As shown in Table \ref{tab-rl}, the reward function definition mainly depends on the optimization objectives, including data rate\cite{yang2020deep,taha2020deep,zhang2021millimeter,kim2021multi}, energy efficiency \cite{lee2020deep,lin2020deep,liu2020ris}, channel capacity\cite{huang2020reconfigurable}, and SNR\cite{feng2020deep}. Moreover, the reward function can include multiple objectives and constraints to balance the overall performance. As an example, the reward function in \cite{yang2020intelligent} has data rate as a positive term to maximize the data rate, while BS power consumption is a negative term to reduce power consumption. RL focuses on the long-term accumulated reward, which means it can better adapt to highly dynamic wireless environments without requiring full knowledge of the defined problem. 



\subsubsection{Algorithm Architecture and Training}
\label{ss}

\blue{In Q-learning, the state-action values are updated by}
\begin{equation} \label{eq-qvalue}
Q^{new}(s,a)= Q^{old}(s,a)+\alpha(r+\eta \max\limits_{a} Q(s',a) -Q^{old}(s,a)),
\end{equation}
where $Q^{old}(s,a)$ and $Q^{new}(s,a)$ are old and new Q-values, respectively. $\alpha$ is the learning rate ($0< \alpha < 1 $), \blue{and $\eta$ is the discount factor $(0<\eta<1)$}. 

Equation (\ref{eq-qvalue}) indicates that a Q-table is used to record all the state-action values, leading to slow convergence for problems with large state-action space. To this end, DQN is proposed to use neural networks for Q-value estimation: 
\begin{equation} \label{eq-dqn}
Loss(\omega)=\mathscr{E}(r+\eta \max\limits_{a} Q(s',a,\omega')-Q(s,a,\omega)),
\end{equation}
where $\mathscr{E}$ represents the error between the predicted Q-value $Q(s_,a,\omega)$ and target Q-value $r+\eta \max\limits_{a} Q(s',a,\omega')$. $\omega$ and $\omega'$ are the weight of the main and target networks, respectively. The main network is used to predict current Q-values by $Q(s_,a,\omega)$, and the target network estimates target Q-values by $Q(s',a,\omega')$. 

In DQN, $\max\limits_{a} Q(s',a,\omega')$ indicates that the target network will select the action and meanwhile evaluate the action, and the maximizing operator will result in over-optimistic Q-value estimation. Then double deep Q-learning (DDQN) is proposed to mitigate Q-value over-estimation by
\begin{equation} \label{eq-ddqn}
\resizebox{0.88\hsize}{!}{$Loss(w)=\mathscr{E}(r+\eta Q(s',\arg \max\limits_{a}Q(s',a,\omega),\omega') - Q(s,a,\omega))$},
\end{equation}
\blue{where} $\arg \max\limits_{a}Q(s',a,\omega)$ means action selection of the main network, and $Q(s',\arg \max\limits_{a}Q(s',a,\omega),\omega')$ indicates the action evaluation of the target network. Decoupling the action selection and evaluation can provide more accurate Q-value prediction and prevent over-estimation.
 
DRL has been used for RIS phase-shift optimization in \cite{taha2020deep,feng2020deep, yang2020deep2}. In these studies, continuous phase shifts are quantized to form discrete action spaces for DQN or DDQN. On the contrary, DDPG can handle continuous action spaces directly without quantization, which has been used for continuous RIS phase-shift control in \cite{yang2020deep,guo2021learning,huang2020hybrid}. 




DDPG is considered a combination of actor-critic learning and DQN, in which the actor network selects actions, and the critic network evaluates the state-action values. The loss function of the critic network is defined as
\begin{equation} \label{eq-ddpg1}
\resizebox{0.88\hsize}{!}{$Loss(w^C)=\mathscr{E}(r+\eta Q(s',a(s',\omega^{A'}),\omega^{C'}) - Q(s,a,\omega^{C}))$},
\end{equation}
where $a(s',\omega^{A'})$ indicates that action $a$ is selected by the target actor network with weight $\omega^{A'}$, and $Q(s',a(s',\omega^{A'}),\omega^{C'})$ means the state-action value is evaluated by the target critic network with weight $\omega^{C'}$. For the actor network, the policy gradient is 
\begin{equation} \label{eq-ddpg2}
\begin{aligned}
\nabla_{\omega^{A}} J \approx \frac{1}{\mathcal{T}}\sum^{\mathcal{T}}_{i=1} (\nabla_{a}Q(s,a,\omega^{C})&|_{s=s_{i},a=a(s_i,\omega^{A})} \\ 
& \cdot\nabla_{\omega^{A}}a(s_i,\omega^{A})|_{s=s_i}),
\end{aligned}
\end{equation}
In equation (\ref{eq-ddpg2}), the critic network provides the Q-value  $Q(s,a,\omega^{C})$, and it represents the expected accumulated reward for a given pair $(s,a)$. 
The actor network is trained to produce actions that can result in the maximum state-action value as predicted by the critic network. Therefore, a common approach to calculate the loss function of the actor network is
\begin{equation} \label{eq-ddpg3}
Loss(w^A)=-\frac{1}{\mathcal{T}}\sum^{\mathcal{T}}_{i=1}(Q(s_i,a_i,w^C)),
\end{equation}
which is computed by using the negative mean of the Q-values predicted by the critic network.  


\begin{figure*}[!t]
\centering
\includegraphics[width=0.95\linewidth]{Image/fig-fl.jpg}
\caption{Comparison between conventional Air-FL and Air-FL with RISs}
\label{fig-fl}
\setlength{\abovecaptionskip}{-2pt} 
\vspace{-10pt}
\end{figure*}

Fig. \ref{fig-rl} shows DRL-empowered RIS-aided wireless networks, which include DDQN and DDPG as two DRL examples. Based on the current state $s$, the agent selects BS beamforming vectors and RIS phase shifts as the action $a$. Then the action $a$ is implemented and rewards $r$ are collected, e.g., sum-rate, energy efficiency, or power consumption. The system will arrive at a new state $s'$ that is indicated by CSI, user positions, or SNR. The experience tuple $<s,a,r,s'>$ is saved in the experience pool, and a mini-batch is sampled for network training. For the DDQN algorithm, the main network is trained as equation (\ref{eq-ddqn}), and the target network will copy the weight of the main network, providing a stable reference. By contrast, the actor and critic networks are trained by equations (\ref{eq-ddpg1}) and (\ref{eq-ddpg2}) in DDPG, and it applies slow update strategies for target networks. 

\blue{Fig. \ref{fig-rl} presents the application of DDQN and DDPG to joint active and passive beamforming problems. Note that here the DDQN and DDPG algorithms can be easily generalized to many other RL algorithms without loss of generality.
This scheme can also be applied to other RIS-related scenarios. For instance, for the UAV-RIS joint optimization problem, one can include the UAV control variables in the action definition, and add UAV altitude in the state.} 
Finally, there have been various reinforcement learning algorithms, but one common deficiency is the low sampling efficiency. It requires substantial numbers of interactions for agent training, leading to large costs in real-world applications, e.g., hundreds of millions of samples.









\begin{table*}[!t]
\caption{Summary of federated learning and RIS-aided wireless networks }
\centering
\small
\setstretch{1}
\begin{threeparttable} 
\resizebox{1\textwidth}{!}{%
\begin{tabular}{|m{0.8cm}<{\centering}|m{1.7cm}<{\centering}|m{1.5cm}<{\centering}|m{1.8cm}<{\centering}|m{1cm}<{\centering}|m{3.3cm}<{\centering}|m{3.4cm}<{\centering}|m{3.8cm}<{\centering}|m{2.2cm}<{\centering}|}
\hline 
Ref. &  Scenario  & Phase-shift resolution  &  Channel settings & CSI &  FL-related objectives &   Control variables  &  Constraints & Algorithms   \\
\hline
\cite{liu2021reconfigurable} &  AirFL with NOMA  &  Continuous & Rician channel   & Perfect   & Minimizing the gap between converged and optimal  training loss.     & FL device selection, receiver beamforming and RIS phase shifts.  & Device selection, receiver beamform, and phase shifts constraints  & AO, Gibbs sampling, SCA  \\
\hline
\cite{yang2021reconfigurable} & AirFL with RISs  & Continuous  & Rician fading  &   Perfect &   Minimizing global loss  &  Transmit power and RIS phase shifts  & Power and dual constraints  & AO, QCQP, SDP   \\
\hline
\cite{ni2022star} & AirFL with NOMA  & Continuous & Rayleigh fading/  Rician fading     & Perfect   & Minimizing FL training gap  &  BS transmit power and RIS configurations    &  Transmit power, target rate, MSE tolerance, and RIS configuration constraints. & AO, SCA, SDR   \\
\hline
\cite{battiloro2022dynamic} & RIS-enhanced FL   & Discrete  & Generated by simulator   & Perfect   & Minimizing average system power consumption  &  RIS phase shifts and bits, bandwidth, and CPU frequency & Training latency, convergence rate, and learning performance constraints  & Stochastic Lyapunov optimization, greedy algorithm   \\
\hline
\cite{zheng2022balancing} & AirFL with RISs  & Continuous &  Empirical channel fading  & Perfect/ Imperfect   & Minimizing the MSE of the aggregated AirFL model   & Receive and transmit beamformer, RIS phase shifts   &  Total transmit power, phase shifts, and target rate constraints  &  AO, SCA  \\
\hline
\cite{ni2021over} &  AirFL with RISs and NOMA  &  Continuous & Rayleigh fading  &  Perfect & Maximizing the achievable hybrid rate of FL and NOMA   & User transmit power, BS receive scalar, and RIS phase shifts  & Target rate and MSE, phase configuration and total transmit power constraints  &   AO, SCA, SDR   \\
\hline
\cite{ni2021federated} &  AirFL with RISs and NOMA  & Continuous & Rayleigh fading   &  Perfect  &  Minimizing the FL MSE and cardinality  &  Transmit power, receive scalar, reflection coefficients, and learning participants  & Total transmit power, phase configuration, target MSE, and the number of learning devices   &  AO, SDR, SCA  \\
\hline
\cite{liu2021joint} &  AirFL with RISs   & Continuous  & Obtained from \cite{tang2020wireless}  & Perfect   &   Minimizing the effect of device selection and the communication error on the convergence rate  & Device selection, over-the-air transceivers, and RIS phase shifts   & Device selection, receiver beamforming, and phase configurations    &  Gibbs-sampling, SCA   \\
\hline
\cite{yang2022federated} & AirFL with RISs   &  Continuous  & Obtained from testbed   & Perfect     &  Maximizing FL utility  &  RIS phase shifts, user-RIS association, and bandwidth allocation        & Bandwidth allocation, RIS phase configurations and association, target SNR constraints  & Matching game, bisection search      \\
\hline
\cite{zhang2021energy} &  AirFL with RISs     &    Continuous & Rayleigh fading   & Perfect   &  Power minimization   & CPU frequency, power and bandwidth allocation, RIS configurations and accuracy design   &  Task completion time, maximum transmit power, phase configuration, total bandwidth       & AO, SDP, MM     \\
\hline
\cite{li2020enhanced} & FL-aided RIS optimization   &   Continuous  
& Wideband geometric/ Rayleigh fading  &  Predicted    &   Average rate maximization    &\multicolumn{3}{c|}{ \makecell{Local models: local devices train local DNNs to predict channel \\ rate using sampled channel vectors; \\Global model: edge server aggregates local DNN models and average.\tnote{1}\\
}} \\
\hline
\cite{zhong2022mobile} & FL-aided mobile RIS optimization   &   Continuous & Rician fading   &  Predicted    &   Sum-rate maximization    &  \multicolumn{3}{c|}{ \makecell{FL-DDPG is applied. Neural networks are trained at local agents\\ and then aggregated to predict Q-values. Control variables \\ include RIS positions, phase shifts, and AP power allocation.  }} \\
\hline
\end{tabular}}
 \begin{tablenotes}    
        \footnotesize       
        \item[1] The columns are combined because \cite{li2020enhanced} \cite{zhong2022mobile} are different from other studies by using FL as an optimization approach, while FL in other works \\ of Table \ref{tab-fl} is part of the optimization objectives. Therefore, instead of showing control variables and constraints, it is essential to present the local \\ and global models of FL-based optimization algorithms.   
\end{tablenotes} 
      
\end{threeparttable} 
\label{tab-fl}
\vspace{-12pt}
\end{table*}








\subsection{ Federated Learning and RISs}

Different from conventional centralized ML algorithms, FL trains the model across multiple decentralized edge devices or servers that hold local datasets without exchanging data. In FL, each edge device will train a local model using local samples, and then a global model is formed by aggregating local model parameters. Afterwards, edge devices download the global model to update local models. 
Table \ref{tab-fl} summarizes existing works focusing on FL and RIS-aided wireless communications. This subsection first discusses RIS-enhanced over-the-air FL (AirFL), and then introduces how to use FL optimization in RIS-aided environments. 


\begin{figure*}[!t]
\centering
\includegraphics[width=1\linewidth]{Image/fig-gl.jpg}
\caption{Graph learning for RIS-aided wireless networks}
\label{fig-gl}
\setlength{\abovecaptionskip}{-2pt} 
\vspace{-10pt}
\end{figure*}


\subsubsection{RIS-enhanced Over-the-air FL}
The main advantage of FL is that it helps preserve data security and privacy, and the distributed property makes wireless networks an ideal platform for FL training. Therefore AirFL is proposed to combine FL with wireless communications. In particular, AirFL implements FL in wireless networks, using edge devices for local model training and edge servers for model aggregation.

However, the information exchange between local and global servers may be affected by unreliable wireless links, limited bandwidth, signal distortion, dynamic channel conditions, and so on. The uncontrollable signal propagation path can degrade the FL performance, e.g., slow uploading of local models due to low data rate. Therefore, RISs are combined with AirFL to realize the full potential of FL. 

As shown in Fig. \ref{fig-fl}, in conventional Air-FL, obstacles may lead to high penetration loss between edge devices and edge servers, and then the low channel capacity will result in slow model uploading and downloading. Finally, the slow parameter exchange efficiency may degrade the convergence rate and lower the accuracy of Air-FL. By contrast, in Air-FL with RISs, the indirect transmission between UE-RIS-BS provides an alternative transmission path for local model uploading or global model downloading. RISs improve the channel capacity by manipulating the signal propagation path. Therefore, efficient model uploading and downloading will improve the convergence rate and precision of Air-FL.

There are a few works that investigate how to enhance FL performance in RIS-aided wireless networks by minimizing global training loss\cite{liu2021reconfigurable, yang2021reconfigurable},  MSE\cite{ni2022star,ni2021federated}, power consumption\cite{zhang2021energy,battiloro2022dynamic}, maximizing the FL utility\cite{yang2022federated}. In \cite{yang2021reconfigurable}, Yang \textit{et al.} aim to minimize the global training loss of FL by controlling transmit power and RIS phase shifts, and the optimization problem is solved by AO-based QCQP and SDP. 
\cite{yang2022federated} proves that RISs can improve more than 30\% prediction accuracy of AirFL, and a 10 times lower AirFL test error is reported in \cite{ni2021federated} by using multi-RIS. In these works, the FL performance is improved by optimizing the resource allocation and user-RIS association, and then edge users can efficiently upload the local models.
Meanwhile, it is worth noting that these works still rely on model-based optimization algorithms, such as AO, QCQP\cite{yang2021reconfigurable}, SCA\cite{zheng2022balancing,ni2021over,ni2021federated}, and MM\cite{zhang2021energy}. 





\subsubsection{ FL for RIS-aided Wireless Communications} 
FL can also be used to optimize the performance of RIS-aided wireless communications. 
For example, deploying a local FL model in RISs may reduce the communication overhead between RISs and the BS, since only local model parameters are shared instead of sharing the whole dataset. In addition, FL can better protect private information such as user CSI, which may be used to infer user locations.
Specifically, FL is used in \cite{li2020enhanced} and \cite{zhong2022mobile} for average rate maximization, in which local models are deployed in user devices and the global model is aggregated by edge servers. In \cite{li2020enhanced}, federated neural networks consider sampled channel vectors as input to predict achievable rates. FL and DDPG are combined in \cite{zhong2022mobile}, and the local neural networks used in DDPG will be aggregated and updated.   

FL is an appealing technique for wireless networks as a distributed ML algorithm. However, the distributed implementation also leads to high communication overhead due to frequent parameter sharing. Meanwhile, the local devices may have different computational capabilities and storage capacities, and such heterogeneity may affect model aggregation and update in FL.








\subsection{Graph Learning }
Graph learning refers to a group of ML techniques in the graph domain, including graph neural networks (GNN), graph attention networks (GAN) and graph convolution networks (GCN). Compared with CNN, which operates on regular Euclidean data like images (2D grid) and text (1D sequence), graph learning is more efficient in describing graphs and structures. Graph learning aims to transform nodes, edges, and their features into low-dimension vector spaces by preserving properties such as graph structure\cite{cui2018survey}. 

Wireless networks are highly dynamic, and wireless data may be collected from non-Euclidean domains, which is represented by graph structure with high dependency on network topology.
The conventional approach of data processing is to convert the data with graph structure into Euclidean domain, but such transformation leads to high complexity and extra overhead. 
By contrast, graph learning enables the graph-structured data to be processed effectively, and transforming the wireless network topology into graphs can better describe the association and interference between network devices\cite{he2021overview}.
Therefore, graph learning has been applied to power control and interference management \cite{naderializadeh2020wireless}, resource allocation \cite{eisen2020optimal,jiang2020dynamic}, network slicing \cite{wang2020graph}, and so on. In the following, GNN is used as an example to introduce graph learning fundamentals, and then we explain how to apply graph learning for RIS control and optimizations. 






\subsubsection{GNN Fundamentals} 
The primary motivation for developing GNN is to extend the existing neural network architecture into graph-related data processing capabilities\cite{zhou2020graph}. In a graph, each node is described by its features and related nodes. Suppose that $z_{v}$ is a state vector to describe the features of node $v$, and it is defined by
\begin{equation}\label{eq-gnn1}
 z_v=f(y_v,y_{v}^{ed},y_{v}^{ne},z_{v}^{ne}),    
\end{equation}
where $y_v$ and $y_{v}^{ed}$ are the features of node $v$ and its edge, and $z_{v}^{ne}$ and $y_{v}^{ne}$ are the state and features of neighbour nodes, respectively.
Then, $z_{v}$ and $y_v$ are used to produce an output $o_v$ by
\begin{equation} \label{eq-gnn2}
o_v=g(z_v,y_v),    
\end{equation}
where $g$ is the output function to map the relationship between states, features, and outputs.  

Similarly, by collecting all the states and features, we have
\begin{equation} \label{eq-gnn3}
Z^{l+1}=f(Z^{l+1},Y),    
\end{equation}
\begin{equation}
O=g(Z,Y_N),    
\end{equation}
where $Z^{l+1}$ indicates all the states at $l^{th}$ iteration, $Y$ indicates all the features, $Y_N$ means the node features, and $O$ is the overall output. Equation (\ref{eq-gnn3}) shows that the system state is updated in an iterative manner, which is inspired by Banachâ€™s fixed point theorem \cite{khamsi2011introduction}. Finally, similar to conventional neural networks, GNN aims to minimize the loss function.



\subsubsection{Graph Learning for RIS Control and Optimizations} 
Interference control is an important technique for multi-user environments to maximize the system sum-rate, and the interactions between RISs and UEs are easily described by a graph. The graph in Fig. \ref{fig-gl} includes $K+1$ nodes, in which one node represents the RIS and the rest are $K$ UEs. Given this scheme, GNN is applied to user scheduling and RIS configurations in \cite{zhang2022learning} and \cite{zhang2022user}. In particular, GNN is trained in an unsupervised manner, and the inputs are user weights and pilot sub-frames of the scheduled users, and the outputs are RIS configurations and beamformers. Similarly, unsupervised GNN is applied in \cite{jiang2021learning} for network utility maximization, which takes pilot signals as input to optimize the BS beamforming and RIS configurations.  

In \cite{zhang2022learning,zhang2022user,jiang2021learning}, a useful feature of GNN is used to reduce the interference between users. Specifically, when updating one node in the GNN, all the neighbour nodes will be included in the updating function, which means GNN can better capture the mutual interference between users.  
Meanwhile, RIS node updating is a function of all the user nodes, enabling GNN to configure RIS elements to improve the channel capacity of all users.   
In addition, the authors in \cite{jiang2021learning} note that 
another key advantage of GNN is the generalization capability. 
For instance, when the number of cell users constantly changes, conventional FNN must be re-trained to handle various user numbers. 
In contrast, a GNN can generalize to different numbers of users by simply adding and removing components in its feature extraction and information exchange stages. Such generalization capability can considerably alleviate ML model training efforts.

Graph learning is one of the most state-of-the-art ML techniques. However, the application to wireless networks is still in a very early stage. The real-time wireless environment can produce dynamic and generative changing graphs, which may prevent the application of graph learning.



\subsection{Transfer Learning}
Long training time and slow convergence are common issues of most ML algorithms, and one of the main reasons is that the model must explore the task from scratch. Fast decision-making is critical in wireless communications, but the low sampling efficiency may prevent applying ML to RIS-aided wireless networks. This subsection will introduce transfer learning fundamentals and explain how transfer learning can improve ML-enabled wireless networks with RISs.

\subsubsection{Transfer Learning Fundamentals}
Transfer learning can be combined with many ML algorithms, and here we consider transfer reinforcement learning (TRL) as an example\cite{zhou2022learning}. In conventional RL, the decision-making $\mathcal{D}_{RL}$ of one agent is described by 
\begin{equation} \label{eq-trl}
\mathcal{D}_{RL}:s \times \mathscr{K}\rightarrow a, r ,
\end{equation}
where $\mathscr{K}$ represents the agent's knowledge, $s$, $a$, and $r$ are the current state, selected action, and received reward, respectively. In equation (\ref{eq-trl}), the agent utilizes the collected knowledge $\mathscr{K}$  for decision-making and action selection.  

By contrast, the decision-making in TRL is
\begin{equation} \label{eq-trl2}
\mathcal{D}_{TRL}:s \times \mathcal{M}(\mathscr{K}_{expert}) \times \mathscr{K}_{learner} \rightarrow a,r,
\end{equation}
where $\mathscr{K}_{expert}$ and $\mathscr{K}_{learner}$ are the knowledge of the expert and learner agents, respectively. The learner is designed to solve the target task, and the expert has some existing knowledge of related source tasks.
Considering the similarities between the source and target tasks, the expert's experience may be reused by the learner as prior knowledge.
The $\mathcal{M}$ in equation (\ref{eq-trl2}) defines a mapping function. $\mathcal{M}(\mathscr{K}_{expert})$ indicates that the expert's experience will be transformed into digestible knowledge, boosting the learning process of the learner. With existing prior knowledge, the learner can achieve a jump-start at the exploration phase, achieving a higher exploration efficiency and average reward with faster convergence\cite{zhou2022knowledge}.     





\begin{figure}[!t]
\centering
\includegraphics[width=0.95\linewidth]{Image/fig-trl.jpg}
\caption{\blue{\textbf{Transfer reinforcement learning for RIS-aided wireless networks}}}
\label{fig-trl}
\setlength{\abovecaptionskip}{-2pt} 
\vspace{-10pt}
\end{figure}




\subsubsection{Transfer Learning-boosted Wireless Networks with RISs} 

\blue{Wireless networks can be highly dynamic. For example, user numbers and CSI patterns may change quickly in a short period of time, and then the RIS control policy may need to be retrained to handle these dramatic changes. However, ML algorithms usually require many training iterations, preventing the application to dynamic wireless networks. To this end, TRL may become a promising solution.}
Fig. \ref{fig-trl} illustrates how TRL is used for RIS-aided wireless networks, which includes source and target tasks. We assume that the expert agent has existing knowledge of the source task, BS beamforming, and the learner agent is designed for the target task, joint active and passive beamforming. Due to the potential task similarities, the learner may reuse the expert's experience to better handle target tasks. 
However, note that the expert's knowledge may exist in various ways, e.g., state-action values and action selections, and then the mapping function may be defined in different manners. Fig. \ref{fig-trl} provides an example by finding similar states and actions, and a Q-value-based mapping function can be defined by 
\begin{equation} \label{eq-map}
\resizebox{0.89\hsize}{!}{$\begin{aligned}
Q^{new}(s^{L},a^{L})=  &Q^{E}(\mathcal{M}(s^{L}),\mathcal{M'}(a^{L}))+Q^{old}(s^{L},a^{L})+\\
&\alpha(r+\eta \max\limits_{a} Q(s',a)-Q^{old}(s^{L},a^{L})),
\end{aligned}$}
\end{equation}
where $s^{L}$ and $a^{L}$ are the learner's state and action, $\mathcal{M}$ and $\mathcal{M'}$ are the state and action map functions, respectively, and $Q^{E}$ indicates the state-action value of the expert. 
Compared with conventional RL, the main difference is that $Q^{E}(\mathcal{M}(s^{L}),\mathcal{M'}(a^{L}))$ is involved as an extra reward for selecting $a^{L}$ under $s^{L}$. 
\blue{In particular, Fig. \ref{fig-trl} shows the steps of defining mapping functions for active and passive beamforming tasks. Firstly, the state mapping function $\mathcal{M}$ is defined to find $s^E=\mathcal{M}(s^{L})$, finding similar environment states such as CSI or current BS beamforming vectors between the learner and expert agents. Similarly, the action mapping function $\mathcal{M'}$ aims to find similar beamforming decisions between the learner and expert action spaces.   
Finally, by finding these similar network states and beamforming decisions, as shown in equation (\ref{eq-map}), good actions with high Q-values in the expert can provide extra rewards for the learner. Then the learner is encouraged to select better actions to achieve a higher sum-rate or energy efficiency.}   

With transfer learning, the RL agent can achieve higher exploration efficiency and faster convergence, enhancing the efficiency of RIS-aided wireless networks. Transfer learning has been used in \cite{zhou2022learning} for joint resource allocation of network slicing, and \cite{elsayed2020transfer} for mmWave networks, achieving faster convergence and better network performance. Similarly, transfer learning can be applied to ML-enabled RIS optimization for faster convergence and achieving prompt phase-shift responses. 
Transfer learning is a very useful technique to mitigate ML model training effort. However, note that transfer learning relies on existing experts to reuse prior knowledge, and the mapping function definition may be difficult due to the inherent task difference between experts and learners.







\subsection{Hierarchical Learning }

\begin{figure}[!t]
\centering
\includegraphics[width=1\linewidth]{Image/fig-hrl.jpg}
\caption{Hierarchical reinforcement learning for RIS-aided wireless networks}
\label{fig-hrl}
\setlength{\abovecaptionskip}{-2pt} 
\vspace{-10pt}
\end{figure}









Hierarchical learning is another technique that can be used for optimizing RIS-aided wireless networks. The main idea of hierarchical learning is to decouple the long-term task into multiple achievable goals to increase exploration efficiency\cite{pateria2021hierarchical}. In particular, it defines a meta-controller to select goals and a sub-controller to achieve these goals.
Based on the short-term performance of the sub-controller, the meta-controller can adjust the goal dynamically to guarantee the long-term performance of the whole system. Hierarchical learning can also be applied to optimization problems that include multiple control variables with different time scales\cite{zhou2023hierarchical}.  
For instance, in \cite{zhou2023hierarchical}, Zhou \textit{et al.} consider a meta-controller for sleep control, and sub-controllers for transmission power and RIS control, enabling control variables with different time scales. 

Fig. \ref{fig-hrl} shows how hierarchical reinforcement learning is applied to RIS-aided wireless networks, and the agent consists of a meta-controller and a sub-controller. 
Specifically, the sub-controller can generate long-term policy instructions for the sub-controller, such as the maximum number of active RIS elements that is available. Then, as shown in Fig. \ref{fig-hrl}, given high-level goals, the sub-controllers can select short-term decisions for RIS phase shifts. Meanwhile, the meta-controller focuses on average power consumption in a period as long-term network performance, and the sub-controller accounts for delay or data rate as instant metrics. This scheme can coordinate control variables with different time scales, balancing instant and long-term network metrics. More specifically, the state-action value of the meta-controller is updated by:
\begin{equation} \label{eq-hrl1}
\resizebox{0.89\hsize}{!}{$\begin{aligned}
&Q_{meta}^{new}(s_{meta},g_{meta}) = Q_{meta}^{old}(s_{meta},g_{meta})+\\
&\alpha(r_{ex}+\eta \max\limits_{g} Q_{meta}(s_{meta}',g)-Q_{meta}^{old}(s_{meta},g_{meta})),
\end{aligned}$}
\end{equation}
where $s_{meta}$ and $s_{meta}'$ is the current and next meta-states, $g_{meta}$ is the goal, and $r_{ex}$ is the extrinsic reward, respectively. $Q^{old}_{meta}$ and $Q^{new}_{meta}$ are old and new state-action values for the meta-controller, indicating the accumulated reward by selecting $g_{meta}$ under state $s_{meta}$.  

Similarly, the Q-value of the sub-controller is updated by
\begin{equation} \label{eq-hrl2}
\resizebox{0.89\hsize}{!}{$\begin{aligned}
Q&_{sub}^{new}(s_{sub},g_{meta},a_{sub}) = Q_{sub}^{old}(s_{sub},g_{meta},a_{sub})+\\
&\alpha(r_{in}+\eta \max\limits_{a} Q_{sub}(s_{sub}',g_{meta},a)-Q_{sub}^{old}(s_{sub},g_{meta},a_{sub})),
\end{aligned}$}
\end{equation}
where $s_{sub}$ and $s_{sub}'$ are current and the next sub-states, $a_{sub}$ is the action, and $r_{in}$ is the intrinsic reward. $Q^{new}_{sub}$ and $Q^{old}_{sub}$ are defined similarly as the meta-controller, indicating the expected reward of selecting $a_{sub}$ under state $s_{sub}$ and goal $g_{meta}$. Equation (\ref{eq-hrl2}) shows that the sub-controller is under the policy control of the meta-controller.

Hierarchical learning is a promising technology to enable hierarchical autonomy in RIS-aided wireless networks. However, one key challenge is to define the relationship between different hierarchies, e.g., meta-controller and sub-controllers. In addition, decoupling one task into multiple sub-tasks can be difficult in highly-dynamic wireless networks, which may prevent the application of hierarchical learning.



\begin{figure}[!t]
\centering
\includegraphics[width=1\linewidth]{Image/fig-meta.jpg}
\caption{Meta-learning for RIS-aided wireless networks}
\label{fig-meta}
\setlength{\abovecaptionskip}{-2pt} 
\vspace{-10pt}
\end{figure}


\begin{table*}[!t]
\caption{Summary of ML-based control and optimization algorithms for RIS-aided wireless networks. }
\centering
\small
\setstretch{1.05}
\resizebox{1\textwidth}{!}{%
\begin{tabular}{|m{2cm}<{\centering}|m{2.5cm}<{\centering}|m{5.5cm}<{\centering}|m{4.2cm}<{\centering}|m{4.5cm}<{\centering}|}
\hline
ML \quad techniques & Typical algorithms & Main features & Difficulties & RIS-related applications\\
\hline
\multirow{1}*{\makecell{Supervised\\ learning}} & Supervised DNN, CNN, decision trees, and support vector machine. & The algorithm is trained to map the relationship between the given input and labeled output for classification and prediction. The input data is fed into the model, and then the model parameters are adjusted until the output is properly fitted.  
& 1) Supervised learning relies on fine-grained datasets to train the algorithm; 2) The algorithm training may be time-consuming; 3) The model is easy to be overfitted.   & Supervised learning is a promising technique if there exist fine-grained datasets, and then various neural networks may be used to predict full channel states \cite{zhang2021deep} or optimal RIS phase shifts\cite{alexandropoulos2020phase}- \cite{hu2021reconfigurable}. \\
\hline
Unsupervised learning  & k-means, DBSCAN, and unsupervised neural networks.  & Unsupervised learning algorithms aim to unveil hidden patterns of unlabeled datasets.  & The result performance is hard to be testified or explain. & Unsupervised DNN can be directly used for optimization problems in RIS-aided networks without involving datasets, which designs RIS phase-shift by defining objectives as loss functions\cite{nguyen2021machine}-\cite{gao2020unsupervised}. \\
\hline
\multirow{20}*{\makecell{Reinforcement \\ learning}}  & Q-learning & The agent interacts with the environment under an MDP framework, recording experience by a Q-table.  & 1)Long convergence time for large state-action problems; 2) Discrete states and actions only.  &\multirow{18}*{\makecell{RL is the most widely \\ applied ML technique for \\ the control and optimization \\  of RIS-aided wireless networks, \\ e.g., power minimization \cite{lin2020deep}, \\ sum-rate\cite{yang2020deep,taha2020deep,samir2021optimizing,yang2021machine},\\ secrecy rate \cite{guo2021learning,yang2020deep2},\\ and energy efficiency\cite{lee2020deep,liu2020ris}.\\ DDPG is especially useful \\ considering the continuous \\ RIS phase-shift control \\ requirements \cite{yang2020deep, guo2021learning}.\\ RL can also be combined with\\ other ML techniques, e.g., \\ transfer reinforcement learning,\\ federated reinforcement learning, \\and meta reinforcement learning. }}\\
\cline{2-4}
& Actor-critic learning & The actor is defined to select actions, while the critic evaluates the actions. & 1) Long convergence iterations; 2) Unstable performance due to the interaction between actor and critic. & \\
\cline{2-4}
& Deep reinforcement learning & DRL applies neural networks to predict state-action values, solving the large state-action issue of tabular Q-learning. 
& \multirow{3}*{\makecell{1) Hyperparameter tuning can \\ be difficult when lacking \\ experience. 2) The sampling \\ efficiency is low. }}  &\\
\cline{2-3}
& Double deep Q-learning & DDQN provides a more accurate Q-value estimation by decoupling the action selection and evaluation.  & &\\
\cline{2-4}
& Multi-agent reinforcement learning & Each agent applies RL or DRL independently to optimize its performance or achieve an overall goal. & The coordination mechanism of multiple agents must be carefully designed.  &\\
\cline{2-4}
& DDPG & DDPG combines actor-critic with policy gradients, optimizing problems with continuous action space.  &  1) Unstable and heavily dependent on appropriate hyperparameters; 2) Overestimation in critic network.   &\\
\hline 
Federated learning & Federated deep learning, federated DRL & Local models are first trained using local datasets, and then the parameters are aggregated to form a global model. Local devices will download the global model to update local models. User privacy is well protected in FL.
& 1) High communication overhead due to parameter exchange; 2) The local device heterogeneity will affect the system performance. & On the one hand, RISs can improve the AirFL performance by improving the channel capacity; on the other hand, FL is used to optimize RIS-aided network performance \cite{li2020enhanced, zhong2022mobile}.    \\
\hline
Graph learning & Graph neural networks, and graph attention networks. & Graph learning refers to ML on graphs. It maps the graph features to vectors with the same dimensions in the embedding space, which is used for link prediction, matching and classification.  &  1) Dynamic and generative changing graph; 2)Interpretability of graph learning.  &  GNN is used for user schedule and RIS configurations in \cite{zhang2022learning, zhang2022user, jiang2021learning} to maximize network utility and sum-rate. The general application of graph learning is still an open issue.   \\
\hline
Transfer learning & Transfer reinforcement learning, transfer supervised learning  & Transfer learning aims to reuse the existing knowledge of experts to accelerate the learning process on target tasks, achieving faster convergence and less training efforts. & 1) The mapping function is hard to design, changing with different algorithms; 2) Transfer learning is vulnerable to adversarial attacks.  & When there are existing experts or source tasks, transfer learning may be used to accelerate ML algorithm training in RIS-aided wireless networks\cite{zhou2022learning}.  \\
\hline
Hierarchical learning & Hierarchical reinforcement learning, hierarchical deep learning  &   Hierarchical learning decouples the task into multiple sub-tasks and goals, increasing the task exploration efficiency.  &  1) The goal and sub-task selection require case-by-case analyses; 2) The relationship between the meta-controller and the sub-controller may be unstable.  & Hierarchical learning is used for optimizing RIS-aided networks with control variables that have different time scales or sparse rewards \cite{zhou2023hierarchical,zhou2023cooperative}.     \\
\hline
 Meta-learning & Meta reinforcement learning, supervised meta-learning & Using experience of former learning tasks to improve the performance on target tasks. The ML model will learn how to learn across tasks.     &  1) Source task distribution must be carefully designed; 2) How to prevent overfitting and underfitting. & Pre-training ML models at the BS for RIS channel estimation\cite{jung2021meta};  Model-agnostic meta-learning for joint RIS phase control and power allocation\cite{zou2021meta}.     \\
\hline
\end{tabular}}
\label{tab-mlsummary}
\vspace{-10pt}
\end{table*}


\subsection{Meta-Learning}

Meta-learning refers to ML algorithms that extract the experience of multiple learning episodes, e.g., a distribution of related tasks, and then use such prior training to improve the performance on target tasks\cite{vanschoren2018meta}. 
In particular, meta-learning is designed to learn how to learn across tasks, 
and this learning-to-learn design can bring several benefits, such as improved training and learning efficiency. In addition, it is better aligned with human learning features, where learning skills are constantly improved on a lifetime timescale and evolutionary policy\cite{hospedales2021meta}. 

RIS-aided networks may include diverse elements, such as RISs, BSs, UAVs, etc, and it can be difficult to train ML models from scratch and meanwhile jointly consider all these network elements. Fig. \ref{fig-meta} shows an example of using meta-learning schemes for UAV-aided joint active and passive beamforming, in which RISs are deployed on the UAV for location flexibility. The ML model is first pre-trained by three existing tasks such as RIS passive beamforming, BS beamforming, and UAV trajectory design. Then, using prior experience, the ML model is expected to learn quickly on the target task, which will jointly consider RISs, BSs, and UAVs. Additionally, such a constant learning scheme can be more useful when other future tasks are expected, and incoming new tasks are always trained based on plenty of former knowledge. There are few works on applying meta-learning to RIS-aided networks. For instance, Jung \textit{et al.} apply meta-learning for RIS channel estimation, and the ML model is pre-trained at the BS by using pilot signals to rapidly estimate RIS channels\cite{jung2021meta}. In \cite{zou2021meta}, model-agnostic meta-learning is used for joint RIS phase-shift control and power allocation, which has a faster convergence rate than baseline ML algorithms.

However, meta-learning must balance the meta-training and self-learning phases. Specifically, meta-training with a wide variety of tasks may lead to underfitting, which means that the agent is unable to specialize to the target task when self-learning. By contrast, if the meta-training tasks are too specific, the knowledge learned on the source tasks may have difficulty in generalizing to target tasks\cite{hospedales2021meta}. Therefore, the source task distribution in the meta-training phase has to be carefully selected.
















\subsection{Discussions and Numerical Results}

ML offers promising opportunities for optimizing RIS-aided wireless communications. Table \ref{tab-mlsummary} overviews various ML techniques \footnote{Note that there are many ML algorithms applied to wireless communications. Instead of collecting all the existing ML algorithms, Table \ref{tab-mlsummary} provides a compressed taxonomy to understand the feature of each technique along with RIS control applications.}.  

Supervised learning trains ML models to best map the input to output, e.g., CSI and user position to RIS phase shifts. However, the model training relies on fine-grained labeled datasets, which may be inaccessible in practice. 
By contrast, unsupervised learning has no need for labeled datasets, and it involves the objective function in the loss function for improvement.
Such unsupervised learning approaches can reduce the dependence on labeled datasets, but the generated results are hard to validate due to the absence of labeled data in most circumstances. 

RL is the most widely applied ML technique for optimization problems, and each RL algorithm has its own features and difficulties. For example, DDPG can handle continuous action space of RIS but can be unstable\cite{yang2020deep, guo2021learning, huang2020hybrid,lin2020deep}, and DDQN can prevent overestimation but sampling efficiency is low \cite{liu2020ris}.
FL and graph learning are newly emerging ML techniques. Most existing works consider RIS-enhanced AirFL, demonstrating that RISs can improve the training efficiency and performance of FL \cite{liu2021reconfigurable, yang2021reconfigurable, ni2022star,ni2021federated,zhang2021energy,battiloro2022dynamic}. 
Graph learning has shown great potential in many other fields, and wireless network applications include power control and interference management \cite{naderializadeh2020wireless}, resource allocation \cite{eisen2020optimal,jiang2020dynamic}, and network slicing \cite{wang2020graph}. Despite the advantages, applying graph learning to wireless networks is still an open issue that requires more effort.



\begin{figure}[!t]
\centering
\setlength{\abovecaptionskip}{-2pt} 
\includegraphics[width=7.2cm,height=5.2cm]{Image/fig-results5.jpg}
\caption{Convergence performance of transfer deep reinforcement learning (TDRL) and DRL. 1) TDRL: We assume there is an existing DRL agent that has been trained under a limited number of UEs. Then a TDRL agent will reuse the expert's prior knowledge to adapt to the environment with more diverse UEs. 2) DRL: conventional DQN-based RIS phase-shift control.}
\label{fig-result-tdrl}
%\vspace{-10pt}
\end{figure}


\begin{figure}[!t]
\centering
\setlength{\abovecaptionskip}{-2pt} 
\includegraphics[width=7.2cm,height=5.2cm]{Image/fig-results6.jpg}
\caption{Energy efficiency performance by joint sleep control and RIS phase-shift design.
We consider a multi-BS and multi-RIS heterogeneous network. Sleep control is a long-term decision to decide the BS on/off status, while RIS phase-shift control is a short-term optimization based on dynamic channel status. Detailed parameters can be found in \cite{zhou2023cooperative}. }
\label{fig-result-hrl}
\vspace{-10pt}
\end{figure}



Transfer learning and hierarchical learning are both promising ML techniques for RIS-aided wireless networks. Transfer learning can reduce the model training efforts, while hierarchical learning provides a novel architecture for applying ML to wireless communications with hierarchical intelligence, especially when optimization parameters have different timescales. However, more research is needed on these techniques as they are used for RIS-aided wireless networks. 
 Both transfer learning and meta-learning involve source tasks and prior experience.  
The core feature of meta-learning is learn-to-learn, which is an appealing advantage for enabling rapid adaptation to dynamic wireless environments. Compared with transfer learning, meta-learning provides a scheme that can be used to facilitate transfer learning as well as other techniques. In transfer learning, the prior knowledge is usually extracted from the source task without defining a meta-objective. By contrast, the prior experience in meta-learning is usually defined by an outer optimization that evaluates the potential benefit of handling new tasks. Meanwhile, meta-learning involves a wider range of meta-representation problems than transfer learning.

Instead of applying one specific ML algorithm solely, note that these ML algorithms may be jointly used. For instance, federated deep reinforcement learning deploys DRL in each local server for decision-making, and then uses a global server to aggregate the main networks for overall estimation and coordination. Such integration can make the most of each algorithm's advantages, achieving better overall performance.




Finally, Fig. \ref{fig-result-tdrl} and \ref{fig-result-hrl} present examples of using transfer learning and hierarchical learning for RIS-related optimization, respectively. In particular, Fig. \ref{fig-result-tdrl} compares the convergence of transfer deep reinforcement learning (TDRL) and DRL, and TDRL achieves faster convergence with higher average reward. The main reason is that TDRL can reuse the former knowledge of existing experts, which will considerably improve the exploration efficiency of ML algorithms. Such improvement becomes more obvious when the number of RIS elements increases, which indicates higher exploration difficulty for conventional DRL algorithms.
Meanwhile, Fig. \ref{fig-result-hrl} shows the energy efficiency of hierarchical reinforcement learning-enabled joint sleep control and RIS phase-shift optimization\cite{zhou2023cooperative}. It includes a multi-BS and multi-RIS scenario, and sleep control can decide the on/off status of BSs to reduce energy consumption, while RISs can improve the channel capacity. Fig. \ref{fig-result-hrl} demonstrates that combining sleep control with RISs can bring higher energy efficiency than using each technique solely, and hierarchical reinforcement learning can well coordinate different decisions with various time scales. 





\begin{table*}[!t]
\caption{Summary of control and optimization techniques for RIS-aided wireless networks }
\centering
\small
\setstretch{1}
\resizebox{1\textwidth}{!}{%
\begin{tabular}{|m{1.5cm}<{\centering}|m{3.5cm}<{\centering}|m{3.3cm}<{\centering}|m{3.5cm}<{\centering}|m{3cm}<{\centering}|m{4cm}<{\centering}|}
\hline 
Optimization approaches  &  Main features   &     Advantage    &  Drawbacks    &  Difficulties   &  Application scenarios for RISs  \\
\hline
Model-based algorithms &  Model-based algorithms aim to find global optimal or at least sub-optimal results for target problems. They usually require full knowledge of the problem to find near-optimal solutions by using transformation, relaxation, and approximation.  &  Model-based algorithms, i.e., SCA, MM, can provide detailed proofs and explanations for the optimality. Target problems are efficiently solved with guaranteed optimality once the closed-form solution is achieved.  & Model-based solutions are usually problem-specific with certain requirements such as convexity and continuity, indicating case-by-case analyses and design. It has difficulty adapting to dynamically changing environments.  &  It has to apply transformations, division, and relaxation to convert the problem to specific forms. These transformations need a dedicated design for each problem.  & Numerous algorithms have been developed to solve RIS-aided optimization problems, e.g., AO to decouple the active and passive beamforming, SDR to relax the rank constraints, and SCA to estimate the sub-optimal results.    \\
\hline
Heuristic algorithms & It applies heuristic rules to find a trade-off between optimality and computational complexity. Heuristic algorithms focus on local optima and low-complexity solutions.  & Heuristic algorithms have much lower computational complexity. It has few requirements for the properties of target problems.  &  It only presents local optima in the current stage, indicating a bad performance in some cases.   & Heuristic rules should be carefully selected and designed, directly affecting the algorithm performance.  &  Considering the high complexity of RIS control problems, heuristic algorithms can provide low-complexity alternatives, i.e., sequential phase shift and on/off control using greedy rule, phase-shift optimization using GA.     \\
\hline
ML techniques & ML techniques are usually data-driven, providing unified control and optimization algorithms for certain types of problems. Most algorithms are easily applied without requiring dedicated design.    &  Data-driven approaches avoid the complexity of building dedicated optimization models. It can better adapt to the dynamic wireless environment given the learning capability.  & It may require many iterations for the algorithm training. ML optimization techniques do not guarantee optimality.   &  Algorithm training is the main difficulty of applying ML, which is data and computation-demanding.  & Various ML techniques have been applied for RIS-related optimizations, e.g., neural networks for CSI prediction and RIS phase control, and DDPG for continuous RIS phase-shift optimization.   \\
\hline
\end{tabular}}
\label{tab-overallcom}
\vspace{-10pt}
\end{table*}





\begin{figure*}[!t]
\centering
\includegraphics[width=0.88\linewidth]{Image/fig-test.jpg}
\caption{Algorithm selection and relationship of model-based, heuristic and ML approaches.}
\label{fig-compar}
\setlength{\abovecaptionskip}{-2pt} 
\vspace{-14pt}
\end{figure*}










\section{Comparison and Relationship between Model-based, Heuristic and ML Approaches}
\label{sec-compa}


This work has introduced three types of optimization techniques: model-based, heuristic, and ML approaches. One intuitive question is how to evaluate the advantages and difficulties of these techniques as well as their relationships. To answer this question, we compare these approaches in Table \ref{tab-overallcom}, including main features, advantages, drawbacks, difficulties, and applications for RISs. In addition, Fig. \ref{fig-compar} summarizes algorithm selection of applying various methods and their relationships. Note that Fig. \ref{fig-compar} provides a general overview for optimizing RIS-aided wireless networks, but the algorithm selection and design should be combined with specific application scenarios.

1) \textbf{Model-based method:} Table \ref{tab-overallcom} shows that model-based approaches can provide efficient and stable solutions once the problem is properly reformulated, especially when closed-form expressions are obtained. 
However, model-based algorithms are usually complicated to design, indicating a series of transformations and relaxations, e.g., decoupling the denominator and numerator in SINR terms and relaxing integer constraints. As a result, the approximation and relaxation can undermine the quality of solutions. 
Additionally, environmental uncertainties can significantly affect the performance of model-based algorithms, since they require full knowledge of the optimization parameters. One possible solution is to assume environment changes follow some specific distributions, but the optimization over distributions will further increase the complexity. Another solution is to use Monte Carlo sampling and repeat the optimization to achieve average results, which is time-consuming.
As illustrated in Fig. \ref{fig-compar}, given a joint optimization problem with coupled control variables, one may use AO or BCD to decouple the joint optimization into multiple sub-problems. Specifically, FP can be used to eliminate fractional terms, e.g., SINR and energy efficiency, and SDR is applied to relax non-convex constraints. Then, various techniques may be applied, such as MM, SCA, SOCP and BnB, to solve each sub-problem under the alternating framework.





2) \textbf{Heuristic algorithms:} The primary benefit of heuristic methods is the low implementation complexity, i.e., optimizing RIS control in an element-by-element manner, achieving a trade-off between optimality and computational complexity. Heuristic methods also show a high generalization capability, e.g., genetic algorithm and PSO apply unified fitness functions to represent the optimization objectives. 
 However, meta-heuristic algorithms are sensitive to key parameters, e.g., population numbers and inertia weight in PSO, which may require find-tuning efforts. But other heuristic methods, especially greedy algorithms and matching theory, can be easily applied with little tuning requirement. 
Meanwhile, Fig. \ref{fig-compar} shows that heuristic algorithms can also be used to solve sub-problems that are defined under an AO scheme, indicating possible combinations between model-based and heuristic algorithms. For instance, to maximize energy efficiency, Yang \textit{et al.} define three sub-problems, and SCA is deployed for active and passive beamforming, while a greedy algorithm is used for RIS on/off control\cite{zhaohui}. Such a combined scheme demonstrates the capability of integrating model-based algorithms with heuristic algorithms.



3) \textbf{ML algorithms:} 
Wireless networks are highly dynamic, and hence optimization techniques must be robust to environmental uncertainties. With the learning capability, ML algorithms can adapt well to dynamic environments.
In particular, ML algorithms present unified optimization schemes, which are applied to diverse problems with few design requirements. For instance, most optimization problems can be converted into unified MDPs that include state, action, transition probability and rewards, and then reinforcement learning is utilized to maximize the reward for a higher sum-rate or energy efficiency. 
Meanwhile, as summarized in Fig. \ref{fig-compar}, reinforcement learning can be integrated with other ML techniques to develop diverse optimization algorithms, such as federated deep reinforcement learning, transfer reinforcement learning, and hierarchical reinforcement learning. For example, transfer reinforcement learning can achieve faster convergence and higher average reward than conventional reinforcement learning algorithms.
However, ML algorithm training is usually computation-demanding, requiring a large number of computational resources, e.g., iterative exploration of RL and back-propagation for neural network training. Finally, datasets are crucial to applying data-driven ML algorithms, especially for supervised learning. Model-based methods provide a useful approach for labeled dataset generation, which indicates the potential to combine model-based and ML algorithms. For example, Hu \textit{et al.} first apply the BCD method for RIS-aided mobile edge computing, and then the produced results serve as datasets for location-based supervised learning algorithms \cite{hu2021reconfigurable}. This reveals the potential benefit of integrating ML techniques with model-based algorithms.







%Additionally, Fig. \ref{fig-compar} compares the advantage and disadvantages of these methods, including stability, optimality, generalization capability, robustness for the environment uncertainty, algorithm design/tuning difficulty, and computational resources requirements\footnote{Note that the objective of Fig. \ref{fig-compar} is not to demonstrate which technique is the best, but to provide insights into the features of diverse optimization techniques.}.




%1) \textbf{Optimality:} Optimality indicates the quality of solutions, which is one of the most important metrics for optimization problems. Given full knowledge of the problem, model-based methods can usually achieve the best solution with detailed proofs. Meanwhile, ML algorithms can explore the solution space efficiently for optimization. RL takes $\epsilon$-greedy policy for action selection, and neural networks can predict the performance for unseen data. Finally, heuristic methods may find local optima instead of global optimality. 

%2) \textbf{Generalization capability:} Generalization capability indicates how the optimization model can adapt to new problems. ML algorithms apply unified control and optimization models with good generalization capability. For instance, given labeled or unlabeled datasets, neural networks are easily deployed in supervised or unsupervised learning manners. In contrast, model-based methods have low generalization capability, since the optimization model requires case-by-case design. Some. 

%3) \textbf{Robustness for the environment uncertainty:}  Meanwhile, some heuristic algorithms can also adapt to environment changes by applying heuristic rules.   

%4) \textbf{Algorithm design/tuning difficulty:} This metric refers to the difficulty of designing or tuning algorithms. ML methods apply unified frameworks with low design difficulty. However, some key parameters, such as the number of neural network layers and learning rates, increase the difficulty of algorithm tuning.  
%Model-based methods are usually complicated to design because it has stringent requirements for problem forms. Therefore, diverse transformation, relaxation and approximation techniques must be applied, increasing the algorithm design difficulty. 

%5) \textbf{Computational resources requirements:} This metric evaluates the number of computing resources (time and space) that are consumed when running a particular algorithm.  Many model-based methods need iterations to solve the problem, and the matrix operations also contribute to storage space requirements. Heuristic methods have low computational resource requirements because heuristic rules already simplify the difficulty of problem-solving. 

%6) \textbf{Stability:} Stability is another critical metric to evaluate whether the optimization algorithm can provide a stable output. ML algorithms can usually achieve good performance after long exploration, but the tedious training iterations will undermine the stability of the results. Most heuristic algorithms have low stability because heuristic rules do not guarantee a stable output.

  

