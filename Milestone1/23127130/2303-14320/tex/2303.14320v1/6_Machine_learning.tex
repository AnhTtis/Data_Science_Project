









\section{ML-enabled Optimization for RIS-aided Wireless Networks }
\label{sec-ml}

ML has achieved great success in various fields, and this section investigates ML applications for the control and optimization of RIS-aided wireless networks, including supervised learning, unsupervised learning, RL, FL, graph learning, transfer learning, and hierarchical learning.

A variety of algorithms have been developed to optimize RIS-aided wireless networks. Early studies mainly considered model-based methods, and some heuristic algorithms are deployed as low-complexity solutions. However, there are several challenges for these conventional optimization techniques:

1) \textbf{Highly dynamic wireless environment}: Wireless networks are highly dynamic due to frequently changing channel conditions, traffic demands, and user conditions. These dynamics lead to great difficulty for conventional optimization schemes. As an example,  model-based methods need full knowledge of the formulated problem, but some sensitive information, e.g., real-time user locations, may be unknown in practice.

2) \textbf{Evolving network architecture}: The wireless network architecture is constantly evolving from RAN to cloud RAN, virtual RAN, and Open RAN. Consequently, these new architectures increase the complexity of network management, and conventional algorithms may have difficulty modelling and optimizing such complicated systems.

3) \textbf{Diverse user requirements}: Wireless network user types are not limited to enhanced Mobile Broad Band, Ultra Reliable Low Latency Communications, and massive Machine Type Communications. Some newly emerged applications, such as virtual and augmented reality, have more stringent requirements on network metrics, leading to a great burden for conventional optimization methods. 

Given these challenges, ML-enabled control and optimization techniques have become appealing approaches for wireless communications in general, as well as for  RIS-aided wireless networks. In the following, we will introduce the fundamentals and applications of various ML techniques.   





\begin{figure*}[!t]
\centering
\setlength{\abovecaptionskip}{-5pt} 
\includegraphics[width=0.95\linewidth]{Image/fig-nn.jpg}
\caption{Supervised learning for RIS-aided wireless networks.}
\label{fig-nn}
\vspace{0pt}
\end{figure*}


\subsection{Supervised Learning-enabled Optimization}

Supervised learning is designed to find the hidden relationships between inputs and labelled outputs. Supervised learning algorithms adjust their parameters to map the input to the expected output, and this relationship is used for the prediction and classification of unseen data. Table \ref{tab-supervised} summarizes supervised learning-based control and optimizations for RIS-aided wireless networks. 
It shows that most studies consider partial CSI or pilot signals as input to predict full CSI, RIS phase shifts, and data rate, and then utilize the prediction results to maximize the data rate.
This subsection will discuss how to apply supervised learning for optimizing RIS-aided wireless networks, including data acquisition, neural network architecture, loss functions and algorithm training. 

\subsubsection{Dataset Acquisition in RIS-aided Environments}
A fine-grained dataset is the prerequisite for deploying supervised learning, since it relies on the labelled output for validation. Table \ref{tab-supervised} indicates that the dataset is generated in various ways: simulators, exhaustive search, codebook, model-based optimization algorithms or live networks. For example, the exhaustive search method means trying different solutions randomly and then collecting the corresponding output to form labelled datasets\cite{taha2021enabling}\cite{aygul2021deep}. By contrast, a more efficient method is to reuse the data produced by AO\cite{song2021truly} and BCD\cite{hu2021reconfigurable} as model-based optimization algorithms. 

In addition, the algorithm performance also depends on the dataset size, ranging from 5000 \cite{alexandropoulos2020phase} and 30000\cite{aygul2021deep,taha2021enabling} to 200000 \cite{hu2021reconfigurable} in several studies. The simulation results in \cite{aygul2021deep,taha2021enabling} demonstrate that the achievable data rate is significantly improved when the number of training samples increases from 5000 to 30000. Note that the complex entry of the input data, especially the channel coefficient, is usually split into real and imaginary parts, increasing the dimension of the neural network input. 
Although there are several ways to generate the data for supervised learning, most existing datasets are simulation-based. Realistic datasets that are produced in real-world RIS-aided environments are still very rare. 

\subsubsection{Loss Functions and Algorithm Training}
Given the huge number of training samples, supervised learning models are trained to produce the expected output. Suppose that the prediction output is the RIS phase shifts \cite{zhang2021deep,aygul2021deep,alexandropoulos2020phase}, and   
the loss function is defined to minimize the mean square error (MSE) of algorithm training
\begin{equation}\label{ml-nnloss}
Loss (\omega)=\frac{1}{N}\sum^N_{i=1}(\theta_i-\hat{\theta}_{i}(\omega))^2,    
\end{equation}
where $N$ is the total number of outputs, i.e., the number of RIS elements, $\theta_i$ is the desired phase shift given by the dataset, $\omega$ is the neural network weight, and $\hat{\theta}_{i}(w)$ indicates the RIS phase shift predicted by neural networks. Meanwhile, note that the dataset must be divided into training and validation samples, since the objective of algorithm training is to predict unseen data. For example, the authors in \cite{song2021truly} includes 10000 samples to predict the RIS phase shifts, of which 90\% is used for training and the remaining 10\% for testing purposes. 





\subsubsection{Neural Network Architecture and Overfitting}
Table \ref{tab-supervised} shows that DNN is used in most studies to predict CSI or RIS phase shifts, and the network architecture ranges from 4 to 9 layers. It is known that more hidden layers may provide a better performance, but the computational complexity and training time will increase. Hence, the network architecture selection should consider the trade-off between performance and training costs.  

Overfitting is another important issue for neural network training. It means that the algorithm fits exactly to the current training data, but cannot achieve satisfying prediction for unseen data, which should be carefully prevented. One solution is to add a random dropout layer with probabilities, ignoring the contribution of some neurons \cite{hu2021reconfigurable}. Multiple methods are provided by \cite{song2021truly} to suppress overfitting in predicting RIS phase shifts, including larger datasets (CSI and RIS phase shift pairs), decreasing hidden layers, and early stopping. 

Finally, Fig. \ref{fig-nn} summarizes how to apply supervised learning for RIS-aided wireless networks. The datasets are produced by simulators, exhaustive searching, testbed, and model-based methods. Then, one specific model will be selected, i.e., FNN, CNN, and RNN. Note that each neural network model has unique features and advantages, e.g., RNNs are suitable for handling sequential data, and CNNs can better handle spatial data. The selection of neural network models require case-by-case analyses on the dataset size, quality, and data-processing demands.  The number of nodes and hidden layers of neural networks should be carefully designed, which will affect the network training time and accuracy. Finally, selected models are trained and implemented, and the algorithm output includes RIS phase shifts, achieved data rate, BS beamforming vectors and so on, which are further used to optimize network performance.




\begin{table*}[!t]
\caption{Summary of unsupervised learning for RIS-aided wireless networks }
\centering
\small
\setstretch{1.05}
\resizebox{1\textwidth}{!}{%
\begin{tabular}{|m{0.7cm}<{\centering}|m{2cm}<{\centering}|m{1.5cm}<{\centering}|m{1.9cm}<{\centering}|m{1.3cm}<{\centering}|m{1.7cm}<{\centering}|m{1cm}<{\centering}|m{1.5cm}<{\centering}|m{2.7cm}<{\centering}|m{2.4cm}<{\centering}|m{2.2cm}<{\centering}|}
\hline 
Ref. &  Scenario  & Phase-shift resolution & Channel settings & CSI & Objectives  &  Model &  Layer  &  Data generation & Input data  & Output data \\
\hline
\multirow{3}*{\cite{song2020unsupervised}} & \multirow{3}*{\makecell{MISO-DL-MU}} &   \multirow{3}*{Continuous}  & \multirow{3}*{\makecell{Rician \\ fading}}  &\multirow{3}*{Perfect} &  \multirow{3}*{\makecell{Maximizing \\ sum-rate}}  &  CNN  &  6 layers &  \multirow{3}*{\makecell{Generated by \cite{huayan}}}  &  CSI  &  RIS phase shifts  \\
\cline{7-8} \cline{10-11}
 &   &  & & &  &  FNN & 5 layers  &   & Effective channel matrix   &  BS beamforming vector  \\
\hline
\cite{nguyen2021machine} & MIMO-DL-SU  &  Continuous &   Rician fading   &  Perfect   &  Maximizing spectral efficiency  & DNN  &  4 layers  & Generated by random exploration  & CSI   & RIS phase shifts   \\
\hline
\cite{dinh2022unsupervised} & Broadcasting
Communications for IoTs  &  Continuous &  Rician fading   & Statistical  &  Maximizing spectral efficiency & DNN  & 4 layers   &  Generated by random exploration  &  CSI  & RIS phase shifts  \\
\hline
\cite{gao2020unsupervised} & MISO-DL-SU  &  Continuous &  Rayleigh fading   &  Perfect & Maximizing data rate  & FNN   &   7 layers    & Generated as \cite{huang2019indoor}   &  CSI   & RIS phase shifts  \\
\hline
\multirow{4}*{\cite{lopez2022deep}} & \multirow{4}*{\makecell{MISO-DL-MU}}  &  \multirow{4}*{\makecell{Continuous/  \\ discrete}} &  \multirow{4}*{\makecell{ Geometry-based \\ clustered\\ delay line}} &  \multirow{4}*{Estimated} &  \multirow{4}*{\makecell{Maximizing \\ sum-rate}} &  FNN &  6 layers  &  Obtained from \cite{bjornson2021configuring}  &  CSI  &  RIS phase shifts \\  
\cline{7-11} 
&   &  &  &  & &  \multicolumn{3}{c|}{ \makecell{K-means is used to cluster RIS elements \\ based on their estimated cascaded channel \\ coefficient without dataset.}}  & Estimated cascaded channel of RIS elements.  &  RIS element clusters.  \\
\hline
\end{tabular}}
\label{tab-unsuper}
\vspace{-10pt}
\end{table*}


\subsection{Unsupervised Learning-based Optimization}

Supervised learning is data-demanding, and fine-grained datasets may be inaccessible in practice and labeling might not be straightforward, preventing the application of supervised learning algorithms. On the contrary, unsupervised learning can find hidden patterns of unlabelled data without predefined targets or human intervention. Table \ref{tab-unsuper} summarizes unsupervised learning algorithms for RIS-aided wireless networks, including unsupervised DNN, FNN, CNN and K-means. This subsection will introduce unsupervised neural networks and clustering algorithms.

\subsubsection{Algorithm Training and Network Architecture of Unsupervised Neural Networks} \label{s-ml-un} 
Supervised neural networks aim to minimize the loss between predicted results and desired target, i.e., predicted and target data rate in the dataset. However, in unsupervised neural networks, the loss function is directly related to optimization objectives. For instance, to maximize the sum-rate, the loss function is defined by
\begin{equation} \label{eq-unsup}
Loss (w)=-\frac{1}{\mathcal{T}}\sum^{\mathcal{T}}_{i=1}(\sum_{k=1}^{K}w_{k} \log(1+\gamma_{k})),     
\end{equation}
where $\sum_{k=1}^{K}w_{k} \log(1+\gamma_{k})$ is the sum-rate of $K$ users, and $\mathcal{T}$ is the minibatch size. Given the loss function as equation (\ref{eq-unsup}), the weight and bias of neural networks will be optimized to minimize the loss function and meanwhile maximize the data rate. 
Table \ref{tab-unsuper} shows that most existing works apply 2 to 5 hidden layers. In particular, the hidden layer numbers are related to the problem's complexity. The authors in \cite{nguyen2021machine} used 1 hidden layer with 40 nodes for $8 \times 2$ MIMO, and 2 hidden layers for $16 \times 2$ MIMO, achieving satisfying simulation results without overfitting or underfitting. In addition, similar to supervised learning, early stop is applied in \cite{song2020unsupervised} to prevent overfitting.   

\subsubsection{Clustering Algorithms} 
Clustering algorithms are usually unsupervised ML algorithms, i.e., K-means and Density-based spatial clustering of applications with noise (DBSCAN). These algorithms are designed to partition objects into multiple sets to minimize the within-cluster sum of squares. Specifically, it aggregates objects with the same hidden patterns. For instance, K-means is used in \cite{lopez2022deep} to group RIS elements according to estimated channel coefficients, and then each group has the same RIS configurations to reduce the computational complexity. 



\begin{table*}[!t]
\caption{Summary of reinforcement learning for RIS-aided wireless networks }
\centering
\small
\setstretch{1.05}
\resizebox{1\textwidth}{!}{%
\begin{tabular}{|m{0.6cm}<{\centering}|m{2.1cm}<{\centering}|m{1.5cm}<{\centering}|m{1.7cm}<{\centering}|m{1.3cm}<{\centering}|m{1.7cm}<{\centering}|m{1.8cm}<{\centering}|m{2.9cm}<{\centering}|m{3.1cm}<{\centering}|m{3.3cm}<{\centering}|}
\hline 
Ref. &  Scenario  & Phase-shift resolution & Channel settings   & CSI & Objectives &  Algorithm  &   State definition  &  Action definition  & Reward function  \\
\hline
\cite{yang2020deep} & MISO-DL-MU NOMA  & Discrete &  Rayleigh fading  & Perfect  & Maximizing sum-rate  &  DDPG  & Current RIS phases &  RIS phase shifts    &  Sum-rate    \\
\hline
\cite{taha2020deep} & Point-to-point communications  &  Discrete   & Wideband geometric   & Estimated   &  Maximizing data-rate &  DRL & CSI  &  RIS phase shifts   & Data rate   \\
\hline
\cite{guo2021learning} &  MISO-DL-MU UAV  &  Continuous  & Saleh-Valenzuela  &  Imperfect  & Maximizing secrecy rate  &  DDPG  &  CSI  & RIS phase shifts and BS beamforming vector  &  Secrecy rate with penalty   \\
\hline
\cite{yang2020intelligent} & MISO-DL-MU with jammer & Continuous  &  Quasi-static flat-fading &  Perfect    & Maximizing sum-rate  &  Fast-policy hill-climbing learning  &  Previous jammer power and SINR, current CSI  & BS transmit power and RIS phase shifts   &   Maximizing data-rate, decreasing BS power and SINR penalty\\
\hline
\cite{yang2020deep2} & MISO-DL-MU  &  Continuous   & Rayleigh fading  &  Delayed &  Maximizing secrecy rate  &  DRL  &  CSI, previous secrecy rate and transmission rate, QoS level  & BS beamforming vector and RIS phase shifts   &  Maximize the system secrecy rate, guaranteeing QoS requirements.  \\
\hline
\cite{feng2020deep} & MISO-DL-SU  & Continuous  &  Rayleigh fading  &  Perfect  & Maximizing SNR  &  DDPG  &  SNR and current RIS phases   &  RIS phase shifts     &  Received SNR  \\
\hline
\cite{huang2020hybrid} & MISO-DL-MU THz &  Continuous  &  Rayleigh distribution  &  Perfect     &    Maximizing sum-rate  &  DDPG  &  Current BS and RIS beamforming vectors, CSI   &  BS beamforming vectors and RIS phase shifts   &  Throughput and the penalty of adjusting the beamforming direction.  \\
\hline
\cite{lee2020deep} & MISO-DL-MU  & Continuous  & Quasi-static flat-fading  &  Perfect  & Maximizing energy efficiency &  DRL  & CSI and energy level of RIS   & BS beamforming vector, RIS phase shifts and on/off   &  Energy efficiency  \\
\hline
\cite{lin2020deep} & MISO-DL-SU  &  Continuous  &  Quasi-static flat-fading   &  Perfect  & Power minimization  & DDPG   & CSI, previous outage events  & BS beamforming vector and RIS configurations  &  Energy efficiency  \\
\hline
\cite{huang2020reconfigurable} & MISO-DL-MU  & Continuous  &  Frequency flat fading   & Perfect & Maximizing channel capacity & DDPG & Transmit and received power, previous action, and CSI    & BS beamforming vector and RIS phase shifts  & Channel capacity    \\
\hline
\cite{zhang2021millimeter} & MISO-DL-MU mmWave &  Continuous  & 3GPP model  & Perfect/ Imperfect  &   Maximizing sum-rate   &   Distributed RL   &  CSI   &  RIS phase shifts  &  Data rate   \\
\hline
\cite{liu2020ris} &  MISO-DL-MU NOMA  & Continous & Rayleigh fading   &  Perfect  &  Maximizing energy efficiency  & Decaying DDQN   & RIS phases and positions, UE positions, and current BS power allocations & RIS phase and position and BS beamforming changes   &   Energy efficiency with penalty  \\
\hline
\cite{kim2021multi} &  Multi-cell communications  & Continuous  & Rayleigh fading   & Imperfect  &  Maximizing sum-rate &  Multi-agent DRL  &  Local and neighbor CSI, local sum-rate   & RIS phase shifts, BS beamforming vector, and UE power changes     & Sum-rate with interference penalties \\
\hline
\cite{samir2021optimizing} & MISO-UL-MU IoT UAV  &  Continuous  & Rician fading   & Perfect  & Minimizing sum AoI   &   DRL    & SNR and UAV height  & UAV  altitude changes  & Negative summation of age of information   \\
\hline
\multirow{2}*{\cite{yang2021machine}} & \multirow{2}*{\makecell{MISO-DL-MU \\ NOMA}}  &  \multirow{2}*{Continuous}  & \multirow{2}*{ \makecell{Rayleigh \\ fading}}  & \multirow{2}*{Perfect} &  \multirow{2}*{\makecell{Maximizing \\ sum-rate}}  & Object migration automation   &  Current RIS phase  &  Power allocation coefficient  &  Sum-rate  \\
\cline{7-10}
 &   &  &  & &   & DDPG  & Current RIS phase  &  RIS Phase changes  & Sum-rate difference \\
\hline
\end{tabular}}
\label{tab-rl}
\vspace{0pt}
\end{table*}



\subsection{Reinforcement Learning-based Optimization}


\begin{figure*}[!t]
\centering
\includegraphics[width=0.75\linewidth]{Image/fig-rl.jpg}
\caption{DRL-empowered RIS-aided wireless networks}
\label{fig-rl}
\setlength{\abovecaptionskip}{-2pt} 
\vspace{-10pt}
\end{figure*}

RL is the most widely applied ML technique for optimization. The RL agent interacts with the environment under the Markov decision process (MDP) to learn the best long-term policy. In particular, given the current system state $s$, the agent selects an action $a$ for implementation and receives a reward $r$, and then the environment will move to the next state $s'$.
Table \ref{tab-rl} summarizes existing studies that apply RL to RIS-aided wireless networks. This subsection will first analyze the state, action, and reward function definitions of these existing studies, and then present the algorithm architecture and training methods. 

\subsubsection{State Definition}
Table \ref{tab-rl} shows that the state may be defined in various ways, e.g., CSI\cite{taha2020deep,guo2021learning,yang2020intelligent}, current RIS phase\cite{yang2020deep,huang2020hybrid,liu2020ris}, position\cite{liu2020ris,samir2021optimizing}, energy level\cite{lee2020deep}, previous transmission rate\cite{yang2020deep2}.    
Specifically, the state refers to the environment status that should be considered for decision-making. For example, the CSI has a great effect on the RIS phase shifts, and therefore CSI is involved in the state definition of many studies \cite{taha2020deep,guo2021learning,yang2020intelligent}. Similarly, RIS-aided UAVs are investigated in \cite{samir2021optimizing}, and the UAV altitude is included in the state definitions because the height will directly affect the channel conditions.


\subsubsection{Action Definition}
In the context of MDP, the action indicates control variables that will change the state, such as RIS phase shifts\cite{yang2020deep}-\cite{yang2021machine}, 
BS beamforming \cite{guo2021learning,yang2020intelligent,yang2020deep2,huang2020hybrid,lee2020deep}, RIS positions\cite{liu2020ris} and elements on/off\cite{lee2020deep}. The control variables in problem formulations are easily converted into actions. However, note that many RL algorithms require discrete action spaces, but the control variables in problem formulations are usually continuous as shown in Section \ref{sec-bac}. The first solution is to quantize the control variables. For instance, the BS transmit power is quantized with an interval of 1 W\cite{lee2020deep}, and the RIS phase changes $\bigtriangleup \theta \in \{-\frac{\pi}{10},0,\frac{\pi}{10}\}$ in \cite{liu2020machine}. Another solution is to apply the DDPG algorithm, which can handle continuous action-space problems\cite{yang2020deep,guo2021learning,huang2020hybrid,lin2020deep,yang2021machine}.         




\subsubsection{Reward Functions}
The reward function is a crucial part of RL. As shown in Table \ref{tab-rl}, the reward function definition mainly depends on the optimization objectives, including data rate\cite{yang2020deep,taha2020deep,zhang2021millimeter,kim2021multi}, energy efficiency \cite{lee2020deep,lin2020deep,liu2020ris}, channel capacity\cite{huang2020reconfigurable}, and SNR\cite{feng2020deep}. Moreover, the reward function can include multiple objectives and constraints to balance the overall performance. As an example, the reward function in \cite{yang2020intelligent} has data rate as a positive term to maximize the data rate, while BS power consumption is a negative term to reduce power consumption. RL focuses on the long-term accumulated reward, which means it can better adapt to highly dynamic wireless environments without requiring full knowledge of the defined problem. 







\subsubsection{Algorithm Architecture and Training}
RL aims to maximize the long-term expected reward by
\begin{equation} \label{eq-rlmax}
V(s) =\mathbb{E}(\sum_{l=0}^{\infty}\gamma^{l} r(s_{l},a_{l})|s=s_{0}),
\end{equation}
where $V(s)$ indicates the expected accumulated reward at state $s$, $s_{0}$ is the initial state, $r(s_{l},a_{l})$ is the reward of choosing action $a_{l}$ at state $s_{l}$ in $l^{th}$ episode, and $\gamma$ is the discount factor $(0<\gamma<1)$ to balance the instant and future reward.

Then the state-action values are defined to represent the potential reward of selecting action $a$ under state $s$. The state-action value is updated by
\begin{equation} \label{eq-qvalue}
\resizebox{0.87\hsize}{!}{$Q^{new}(s,a)= Q^{old}(s,a)+\alpha(r+\gamma \max\limits_{a} Q(s',a) -Q^{old}(s,a))$},
\end{equation}
where $Q^{old}(s,a)$ and $Q^{new}(s,a)$ are old and new Q-values, respectively. $\alpha$ is the learning rate ($0< \alpha < 1 $). Equation (\ref{eq-qvalue}) indicates that a Q-table is used to record all the state-action values, leading to slow convergence for problems with large state-action space. To this end, DQN is proposed to use neural networks for Q-value estimation, and the loss function is defined by
\begin{equation} \label{eq-dqn}
\resizebox{0.87\hsize}{!}{$Loss(\omega)=Er(r+\gamma \max\limits_{a} Q(s',a,\omega')-Q(s,a,\omega))$},
\end{equation}
where $Er$ represents the error between the predicted Q-value $Q(s_,a,\omega)$ and target Q-value $r+\gamma \max\limits_{a} Q(s',a,\omega')$. $\omega$ and $\omega'$ are the weight of the main and target networks, respectively. 

In DQN, $\max\limits_{a} Q(s',a,\omega')$ indicates that the target network will select the action and meanwhile evaluate the action, and the maximizing operator will result in over-optimistic Q-value estimation. Then DDQN is proposed to mitigate Q-value over-estimation by
\begin{equation} \label{eq-ddqn}
\resizebox{0.88\hsize}{!}{$Loss(w)=Er(r+\gamma Q(s',\arg \max\limits_{a}Q(s',a,\omega),\omega') - Q(s,a,\omega))$},
\end{equation}
where $\arg \max\limits_{a}Q(s',a,\omega)$ means action selection of the main network, and $Q(s',\arg \max\limits_{a}Q(s',a,\omega),\omega')$ indicates the action evaluation of the target network.
 
DRL has been used for RIS phase-shift optimization in \cite{taha2020deep,feng2020deep, yang2020deep2}. In these studies, continuous phase shifts are quantized to form discrete action spaces for DQN or DDQN. On the contrary, DDPG can handle continuous action spaces directly without quantization, which has been used for continuous RIS phase-shift control in \cite{yang2020deep,guo2021learning,huang2020hybrid}. 





DDPG is considered a combination of actor-critic learning and DQN, in which the actor network selects actions, and the critic network evaluates the state-action values. The loss function of the critic network is defined as
\begin{equation} \label{eq-ddpg1}
\resizebox{0.88\hsize}{!}{$Loss(w)=Er(r+\gamma Q(s',a(s',\omega^{A'}),\omega^{C'}) - Q(s,a,\omega^{C}))$},
\end{equation}
where $a(s',\omega^{A'})$ indicates that action $a$ is selected by the target actor network with weight $\omega^{A'}$, and $Q(s',a(s',\omega^{A'}),\omega^{C'})$ means the state-action value is evaluated by the target critic network with weight $\omega^{C'}$. For the actor network, the policy gradient is 
\begin{equation} \label{eq-ddpg2}
\begin{aligned}
\nabla_{\omega^{A}} J \approx \frac{1}{\mathcal{T}}\sum^{\mathcal{T}}_{i=1} (\nabla_{a}Q(s,a,\omega^{C})&|_{s=s_{i},a=a(s_i,\omega^{A})} \\ 
& \cdot\nabla_{\omega^{A}}a(s_i,\omega^{A})|_{s=s_i}),
\end{aligned}
\end{equation}
In addition, similar to DQN, DDPG uses the experience replay technique to produce a minibatch for training. DDPG also uses the soft update method for target network updating.  

Finally, Fig. \ref{fig-rl} shows DRL-empowered RIS-aided wireless networks. Based on the current state $s$, the agent selects BS beamforming vector and RIS phase shifts as action $a$. Then the action $a$ is implemented and rewards $r$ are collected, e.g., sum-rate, energy efficiency, and power consumption. The system will arrive at a new state $s'$ that is indicated by CSI, user positions, and SNR. The experience tuple $<s,a,r,s'>$ is saved in the experience pool, and a mini-batch is sampled to train the main network. Finally, the target network will copy the weight of the main network, providing a stable reference for the main network training. The framework in Fig. \ref{fig-rl} may be easily generalized to many other RL algorithms without loss of generality.  





\begin{figure*}[!t]
\centering
\includegraphics[width=0.95\linewidth]{Image/fig-fl.jpg}
\caption{Federated learning and RIS-aided wireless networks}
\label{fig-fl}
\setlength{\abovecaptionskip}{-2pt} 
\vspace{-10pt}
\end{figure*}


\begin{table*}[!t]
\caption{Summary of federated learning and RIS-aided wireless networks }
\centering
\small
\setstretch{1}
\begin{threeparttable} 
\resizebox{1\textwidth}{!}{%
\begin{tabular}{|m{0.8cm}<{\centering}|m{1.7cm}<{\centering}|m{1.5cm}<{\centering}|m{1.8cm}<{\centering}|m{1cm}<{\centering}|m{3.3cm}<{\centering}|m{3.4cm}<{\centering}|m{3.8cm}<{\centering}|m{2.2cm}<{\centering}|}
\hline 
Ref. &  Scenario  & Phase-shift resolution  &  Channel settings & CSI &  FL-related objectives &   Control variables  &  Constraints & Algorithms   \\
\hline
\cite{liu2021reconfigurable} &  AirFL with NOMA  &  Continuous & Rician channel   & Perfect   & Minimizing the gap between converged and optimal  training loss.     & FL device selection, receiver beamforming and RIS phase shifts.  & Device selection, receiver beamform, and phase shifts constraints  & AO, Gibbs sampling, SCA  \\
\hline
\cite{yang2021reconfigurable} & AirFL with RISs  & Continuous  & Rician fading  &   Perfect &   Minimizing global loss  &  Transmit power and RIS phase shifts  & Power and dual constraints  & AO, QCQP, SDP   \\
\hline
\cite{ni2022star} & AirFL with NOMA  & Continuous & Rayleigh fading/  Rician fading     & Perfect   & Minimizing FL training gap  &  BS transmit power and RIS configurations    &  Transmit power, target rate, MSE tolerance, and RIS configuration constraints. & AO, SCA, SDR   \\
\hline
\cite{battiloro2022dynamic} & RIS-enhanced FL   & Discrete  & Generated by simulator   & Perfect   & Minimizing average system power consumption  &  RIS phase shifts and bits, bandwidth, and CPU frequency & Training latency, convergence rate, and learning performance constraints  & Stochastic Lyapunov optimization, greedy algorithm   \\
\hline
\cite{zheng2022balancing} & AirFL with RISs  & Continuous &  Empirical channel fading  & Perfect/ Imperfect   & Minimizing the MSE of the aggregated AirFL model   & Receive and transmit beamformer, RIS phase shifts   &  Total transmit power, phase shifts, and target rate constraints  &  AO, SCA  \\
\hline
\cite{ni2021over} &  AirFL with RISs and NOMA  &  Continuous & Rayleigh fading  &  Perfect & Maximizing the achievable hybrid rate of FL and NOMA   & User transmit power, BS receive scalar, and RIS phase shifts  & Target rate and MSE, phase configuration and total transmit power constraints  &   AO, SCA, SDR   \\
\hline
\cite{ni2021federated} &  AirFL with RISs and NOMA  & Continuous & Rayleigh fading   &  Perfect  &  Minimizing the FL MSE and cardinality  &  Transmit power, receive scalar, reflection coefficients, and learning participants  & Total transmit power, phase configuration, target MSE, and the number of learning devices   &  AO, SDR, SCA  \\
\hline
\cite{liu2021joint} &  AirFL with RISs   & Continuous  & Obtained from \cite{tang2020wireless}  & Perfect   &   Minimizing the effect of device selection and the communication error on the convergence rate  & Device selection, over-the-air transceivers, and RIS phase shifts   & Device selection, receiver beamforming, and phase configurations    &  Gibbs-sampling, SCA   \\
\hline
\cite{yang2022federated} & AirFL with RISs   &  Continuous  & Obtained from testbed   & Perfect     &  Maximizing FL utility  &  RIS phase shifts, user-RIS association, and bandwidth allocation        & Bandwidth allocation, RIS phase configurations and association, target SNR constraints  & Matching game, bisection search      \\
\hline
\cite{zhang2021energy} &  AirFL with RISs     &    Continuous & Rayleigh fading   & Perfect   &  Power minimization   & CPU frequency, power and bandwidth allocation, RIS configurations and accuracy design   &  Task completion time, maximum transmit power, phase configuration, total bandwidth       & AO, SDP, MM     \\
\hline
\cite{li2020enhanced} & FL-aided RIS optimization   &   Continuous  
& Wideband geometric/ Rayleigh fading  &  Predicted    &   Average rate maximization    &\multicolumn{3}{c|}{ \makecell{Local models: local devices train local DNNs to predict channel \\ rate using sampled channel vectors; \\Global model: edge server aggregates local DNN models and average.\tnote{1}\\
}} \\
\hline
\cite{zhong2022mobile} & FL-aided mobile RIS optimization   &   Continuous & Rician fading   &  Predicted    &   Sum-rate maximization    &  \multicolumn{3}{c|}{ \makecell{FL-DDPG is applied. Neural networks are trained at local agents\\ and then aggregated to predict Q-values. Control variables \\ include RIS positions and phase shifts, and AP power allocation .  }} \\
\hline
\end{tabular}}
 \begin{tablenotes}    
        \footnotesize       
        \item[1] The columns are combined because \cite{li2020enhanced} \cite{zhong2022mobile} are different than other studies by using FL as an optimization approach, while FL in other works \\ of Table \ref{tab-fl} is part of the optimization objectives. Therefore, instead of showing control variables and constraints, it is essential to present the local \\ and global models of FL-based optimization algorithms.   
\end{tablenotes} 
      
\end{threeparttable} 
\label{tab-fl}
\vspace{-12pt}
\end{table*}





\begin{figure*}[!t]
\centering
\includegraphics[width=1\linewidth]{Image/fig-gl.jpg}
\caption{Graph learning for RIS-aided wireless networks}
\label{fig-gl}
\setlength{\abovecaptionskip}{-2pt} 
\vspace{-10pt}
\end{figure*}


\subsection{ Federated Learning and RISs}

Different with conventional centralized ML algorithms, FL trains the model across multiple decentralized edge devices or servers that hold local datasets without exchanging data. In FL, each edge device will train a local model using local samples, and then a global model is formed by aggregating local model parameters. Afterwards, edge devices download the global model to update local models. 
Table \ref{tab-fl} summarizes existing works focusing on FL and RIS-aided wireless communications. This subsection first discusses RIS-enhanced over-the-air FL (AirFL), then introduces how to use FL optimization in RIS-aided environments. In particular, AirFL implements FL in wireless networks, using edge devices for local model training and edge server for model aggregation.

\subsubsection{RIS-enhanced Over-the-air FL}
The main advantage of FL is that it preserves data security and privacy, and the distributed property makes wireless networks an ideal platform for FL training. Therefore AirFL is proposed to combine FL with wireless communications. Specifically, it investigates how to enhance FL performance in RIS-aided wireless networks by minimizing global training loss\cite{liu2021reconfigurable, yang2021reconfigurable},  MSE\cite{ni2022star,ni2021federated}, power consumption\cite{zhang2021energy,battiloro2022dynamic}, maximizing the FL utility\cite{yang2022federated}. In \cite{yang2021reconfigurable}, Yang \textit{et al.} aim to minimize the global training loss of FL by controlling transmit power and RIS phase shifts, and the optimization problem is solved by AO-based QCQP and SDP. 
\cite{yang2022federated} proves that RISs can improve more than 30\% prediction accuracy of AirFL, and a 10 times lower AirFL test error is reported in \cite{ni2021federated} by using multi-RIS. In these works, the FL performance is improved by optimizing the resource allocation and user-RIS association, and then edge users can efficiently upload the local models.
Meanwhile, it is worth noting that these works still rely on model-based optimization algorithms, such as AO, QCQP\cite{yang2021reconfigurable}, SCA\cite{ni2022star,zheng2022balancing,ni2021over,ni2021federated}, and MM\cite{zhang2021energy}. 





\subsubsection{ FL for RIS-aided Wireless Communications} 
On the other hand, FL can also be used to optimize the performance of RIS-aided wireless communications. For example, FL is used in \cite{li2020enhanced} and \cite{zhong2022mobile} for average rate maximization, in which local models are deployed in user devices and the global model is aggregated by edge servers. In \cite{li2020enhanced}, federated neural networks consider sampled channel vectors as input to predict achievable rates. FL and DDPG are combined in \cite{zhong2022mobile}, and the local neural networks used in DDPG will be aggregated and updated.   

Fig. \ref{fig-fl} shows the interaction between FL and RIS-aided wireless communications. On the one hand, RIS can enhance the AirFL performance, i.e., increasing prediction accuracy and minimizing training loss. On the other hand, FL can improve the RIS-aided wireless network performance, e.g., maximizing data rate and minimizing power consumption. 








\subsection{Graph Learning }
Graph learning refers to a group of ML techniques in the graph domain, including graph neural networks (GNN), graph attention networks (GAN) and graph convolution networks (GCN). Compared with CNN, which operates on regular Euclidean data like images (2D grid) and text (1D sequence), graph learning is more efficient in describing graphs and structures. Graph learning aims to transform nodes, edges, and their features into low-dimension vector spaces by preserving properties such as graph structure\cite{cui2018survey}. 

 Wireless networks are highly dynamic, and the wireless data may be collected from non-Euclidean domains, which is represented by graph structure with high dependency on network topology.
The conventional approach of data processing is to convert the data with graph structure into Euclidean domain, but such transformation leads to high complexity and extra overhead. 
By contrast, graph learning enables the graph-structured data to be processed effectively, and transforming the wireless network topology into graphs can better describe the association and interference between network devices\cite{he2021overview}.
Therefore, graph learning has been applied to power control and interference management \cite{naderializadeh2020wireless}, resource allocation \cite{eisen2020optimal,jiang2020dynamic}, network slicing \cite{wang2020graph}, and so on. In the following, GNN is used as an example to introduce graph learning fundamentals, and then we explain how to apply graph learning for RIS control and optimizations. 






\subsubsection{GNN Fundamentals} 
The primary motivation for developing GNN is to extend the existing neural network architecture into graph-related data processing capabilities\cite{zhou2020graph}. In a graph, each node is described by its features and related nodes. Suppose that $z_{v}$ is a state vector to describe the features of node $v$, and it is defined by
\begin{equation}\label{eq-gnn1}
 z_v=f(y_v,y_{v}^{ed},y_{v}^{ne},z_{v}^{ne}),    
\end{equation}
where $y_v$ and $y_{v}^{ed}$ are the features of node $v$ and its edge, and $z_{v}^{ne}$ and $y_{v}^{ne}$ are the state and features of neighbour nodes, respectively.
Then, $z_{v}$ and $y_v$ are used to produce an output $o_v$ by
\begin{equation} \label{eq-gnn2}
o_v=g(z_v,y_v),    
\end{equation}
where $g$ is the output function to map the relationship between states, features, and outputs.  

Similarly, by collecting all the states and features, we have
\begin{equation} \label{eq-gnn3}
Z^{l+1}=f(Z^{l+1},Y),    
\end{equation}
\begin{equation}
O=g(Z,Y_N),    
\end{equation}
where $Z^{l+1}$ indicates all the states at $l^{th}$ iteration, $Y$ indicates all the features, $Y_N$ means the node features, and $O$ is the overall output. Equation (\ref{eq-gnn3}) shows that the system state is updated in an iterative manner, which is inspired by Banachâ€™s fixed point theorem \cite{khamsi2011introduction}. Finally, similar to conventional neural networks, GNN aims to minimize the loss function.



\subsubsection{Graph Learning for RIS Control and Optimizations} 
Interference control is an important technique for multi-user environments to maximize the system sum-rate, and the interactions between RISs and UEs are easily described by a graph. The graph in Fig. \ref{fig-gl} includes $K+1$ nodes, in which one node represents the RIS and the rest are $K$ UEs. Given this scheme, GNN is applied to user scheduling and RIS configurations in \cite{zhang2022learning} and \cite{zhang2022user}. In particular, the GNN is trained in an unsupervised manner, and the inputs are user weights and pilot sub-frames of the scheduled users, and the outputs are RIS configurations and beamformers. Similarly, unsupervised GNN is applied in \cite{jiang2021learning} for network utility maximization, which takes pilot signals as input to optimize the BS beamforming and RIS configurations.     

In \cite{zhang2022learning,zhang2022user,jiang2021learning}, a useful feature of GNN is used to reduce the interference between users. Specifically, when updating one node in the GNN, all the neighbour nodes will be included in the updating function, which means GNN can better capture the mutual interference between users.  
In addition, RIS node updating is a function of all the user nodes, enabling GNN to configure RIS elements to improve the channel capacity of all users.   








\subsection{Transfer Learning}
Long training time and slow convergence are common issues of most ML algorithms, and one of the main reasons is that the model must explore the task from scratch. Fast decision-making is critical in wireless communications, but the low sampling efficiency may prevent applying ML to RIS-aided wireless networks. This subsection will introduce transfer learning fundamentals and explain how transfer learning can improve ML-enabled wireless networks with RISs.

\subsubsection{Transfer Learning Fundamentals}
Transfer learning can be combined with many ML algorithms, and here we consider transfer reinforcement learning (TRL) as an example\cite{zhou2022learning}. In conventional RL, the decision-making $\mathcal{D}_{RL}$ of one agent is described by 
\begin{equation} \label{eq-trl}
\mathcal{D}_{RL}:s \times \mathscr{K}\rightarrow a, r ,
\end{equation}
where $\mathscr{K}$ represents the agent's knowledge, $s$, $a$, and $r$ are the current state, selected action, and received reward, respectively. In equation (\ref{eq-trl}), the agent utilizes the collected knowledge $\mathscr{K}$  for decision-making and action selection.  

By contrast, the decision-making in TRL is
\begin{equation} \label{eq-trl2}
\mathcal{D}_{TRL}:s \times \mathcal{M}(\mathscr{K}_{expert}) \times \mathscr{K}_{learner} \rightarrow a,r,
\end{equation}
where $\mathscr{K}_{expert}$ and $\mathscr{K}_{learner}$ are the knowledge of the expert and learner agents, respectively. The learner is designed to solve the target task, and the expert has some existing knowledge of related source tasks.
Considering the similarities between the source and target tasks, the expert's experience may be reused by the learner as prior knowledge.
The $\mathcal{M}$ in equation (\ref{eq-trl2}) defines a mapping function. $\mathcal{M}(\mathscr{K}_{expert})$ indicates that the expert's experience will be transformed into digestible knowledge, boosting the learning process of the learner. With existing prior knowledge, the learner can achieve a jump-start at the exploration phase, achieving a higher exploration efficiency and average reward with faster convergence\cite{zhou2022knowledge}.     





\begin{figure}[!t]
\centering
\includegraphics[width=0.85\linewidth]{Image/fig-trl.jpg}
\caption{Transfer reinforcement learning for RIS-aided wireless networks}
\label{fig-trl}
\setlength{\abovecaptionskip}{-2pt} 
\vspace{-10pt}
\end{figure}



\subsubsection{Transfer Learning-boosted Wireless Networks with RIS} 
Fig. \ref{fig-trl} illustrates how to use TRL for RIS-aided wireless networks. We assume that the expert agent has existing knowledge of the source task, BS beamforming, and the learner agent is designed for the target task, joint active and passive beamforming. Then the task similarities mean that the learner can reuse the expert's experience to better handle target tasks. 
Specifically, the expert's knowledge may exist in various ways, e.g., state-action values, action selections, and reward definitions. Then the mapping function may be defined in different manners, and we present a Q-value-based mapping function as an example
\begin{equation} \label{eq-map}
\resizebox{0.89\hsize}{!}{$\begin{aligned}
Q^{new}(s^{L},a^{L})=  &Q^{E}(\mathcal{M}(s^{L}),\mathcal{M'}(a^{L}))+Q^{old}(s^{L},a^{L})+\\
&\alpha(r+\gamma \max\limits_{a} Q(s',a)-Q^{old}(s^{L},a^{L})),
\end{aligned}$}
\end{equation}
where $s^{L}$ and $a^{L}$ are the learner's state and action, $\mathcal{M}$ and $\mathcal{M'}$ are the state and action map functions, respectively, and $Q^{E}$ indicates the state-action value of the expert. 
Compared with conventional RL, the main difference is that $Q^{E}(\mathcal{M}(s^{L}),\mathcal{M'}(a^{L}))$ is involved as an extra reward for selecting $a^{L}$ under $s^{L}$. $\mathcal{M}$ and $\mathcal{M'}$ aim to obtain $s^E=\mathcal{M}(s^{L})$ and $a^E=\mathcal{M'}(a^{L})$, where $s^{E}$ and $a^{E}$ indicate the expert's state and action that are the closest to $s^{L}$ and $a^{L}$, respectively. By finding these similar states and actions,  good actions with high Q-values in the expert could also bring satisfying rewards for the learner, which will further guide the exploration of the learner. Note that both the control variables of the expert and learner include BS beamformer, which are used to define the action mapping function $\mathcal{M'}$.  

With transfer learning, the RL agent can achieve higher exploration efficiency and faster convergence, enhancing the efficiency of RIS-aided wireless networks. Transfer learning has been used in \cite{zhou2022learning,zhou2022knowledge2} for joint resource allocation of network slicing, and \cite{elsayed2020transfer} for mmWave networks, achieving faster convergence and better network performance. Similarly, transfer learning can be applied to ML-enabled RIS optimization for faster convergence and achieving prompt phase-shift responses. 



\subsection{Hierarchical Learning }

\begin{figure}[!t]
\centering
\includegraphics[width=1\linewidth]{Image/fig-hrl.jpg}
\caption{Hierarchical reinforcement learning for RIS-aided wireless networks}
\label{fig-hrl}
\setlength{\abovecaptionskip}{-2pt} 
\vspace{-10pt}
\end{figure}



\begin{table*}[!t]
\caption{Summary of ML-based control and optimization algorithms for RIS-aided wireless networks. }
\centering
\small
\setstretch{1.05}
\resizebox{1\textwidth}{!}{%
\begin{tabular}{|m{2cm}<{\centering}|m{2.5cm}<{\centering}|m{5.5cm}<{\centering}|m{4.2cm}<{\centering}|m{4.5cm}<{\centering}|}
\hline
ML \quad techniques & Typical algorithms & Main features & Difficulties & RIS-related applications\\
\hline
\multirow{1}*{\makecell{Supervised\\ learning}} & Supervised DNN, CNN, decision trees, and support vector machine. & The algorithm is trained to map the relationship between the given input and labelled output for classification and prediction. The input data is fed into the model, and then the model parameters are adjusted until the output is properly fitted.  
& 1) Supervised learning relies on fine-grained datasets to train the algorithm; 2) The algorithm training may be time-consuming; 3) The model is easy to be overfitted.   & Given the dataset input, DNN is applied to predict full channel states \cite{zhang2021deep} or optimal RIS phase shifts\cite{alexandropoulos2020phase}- \cite{hu2021reconfigurable}.  \\
\hline
Unsupervised learning  & K-means, DBSCAN, and unsupervised neural networks.  & Unsupervised learning algorithms aim to unveil hidden patterns of unlabelled datasets.  & The result performance is hard to be testified or explain. & Unsupervised DNN is used for RIS phase-shift control by defining objectives as loss functions\cite{nguyen2021machine}-\cite{gao2020unsupervised}. \\
\hline
\multirow{20}*{\makecell{Reinforcement \\ learning}}  & Q-learning & The agent interacts with the environment under the MDP framework, recording experience by a Q-table.  & 1)Long convergence time for large state-action problems; 2) Discrete states and actions only.  &\multirow{18}*{\makecell{RL is the most widely \\ applied ML technique for \\ the control and optimization \\  of RIS-aided wireless networks, \\ e.g., power minimization \cite{lin2020deep}, \\ sum-rate\cite{yang2020deep,taha2020deep,samir2021optimizing,yang2021machine},\\ secrecy rate \cite{guo2021learning,yang2020deep2},\\ and energy efficiency\cite{lee2020deep,liu2020ris}.\\ DDPG is especially useful \\ considering the continuous \\ RIS phase-shift control \\ requirements\\ \cite{yang2020deep, guo2021learning, huang2020hybrid,lin2020deep}. }}\\
\cline{2-4}
& Actor-critic learning & The actor is defined to select actions, while the critic evaluates the actions. & 1) Long convergence iterations; 2) Unstable performance due to the interaction between actor and critic. & \\
\cline{2-4}
& Deep reinforcement learning & DRL applies neural networks to predict state-action values, solving the large state-action issue of tabular Q-learning. 
& \multirow{3}*{\makecell{1) Hyperparameter tuning can \\ be difficult when lacking \\ experience. 2) The sampling \\ efficiency is low. }}  &\\
\cline{2-3}
& Double deep Q-learning & DDQN provides a more accurate Q-value estimation by decoupling the action selection and evaluation.  & &\\
\cline{2-4}
& Multi-agent reinforcement learning & Each agent applies RL or DRL independently to optimize its performance or achieve an overall goal. & The coordination mechanism of multiple agents must be carefully designed.  &\\
\cline{2-4}
& DDPG & DDPG combines actor-critic with policy gradients, optimizing problems with continuous action space.  &  1) Unstable and heavily dependent on appropriate hyperparameters; 2) overestimation in critic network.   &\\
\hline 
Federated learning & Federated deep learning, federated DRL & Local models are first trained using local datasets, and then the parameters are aggregated to form a global model. Local devices will download the global model to update local models. User privacy is well protected in FL.
& 1) High communication overhead due to parameter exchange; 2) the local device heterogeneity will affect the system performance. & On one hand, RISs can improve the AirFL performance by minimizing global loss and training gap; on the other hand, FL is used to optimize network performance \cite{li2020enhanced, zhong2022mobile}.    \\
\hline
Graph learning & Graph neural networks, and graph attention networks. & Graph learning refers to ML on graphs. It maps the graph features to vectors with the same dimensions in the embedding space, which is used for link prediction, matching and classification.  &  1) Dynamic and generative changing graph; 2)Interpretability of Graph Learning.  & GNN is used for user schedule and RIS configurations in \cite{zhang2022learning, zhang2022user, jiang2021learning} to maximize network utility and sum-rate.    \\
\hline
Transfer learning & Transfer reinforcement learning, transfer supervised learning  & Transfer learning aims to reuse the existing knowledge of experts to accelerate the learning process on target tasks, achieving faster convergence and less training efforts. & 1) The mapping function is hard to design, changing with different algorithms; 2) Transfer learning is vulnerable to adversarial attacks.  & Transfer learning may be used to accelerate the ML algorithm training for optimizing RIS-aided wireless networks.  \\
\hline
Hierarchical learning & Hierarchical reinforcement learning, hierarchical deep learning  &   Hierarchical learning decouples the task into multiple sub-tasks and goals, increasing the task exploration efficiency.  &  1) The goal and sub-task selection require case-by-case analyses; 2) The relationship between the meta-controller and the sub-controller may be unstable.  & It is used for optimizing RIS-aided networks with control variables that have different time scales.     \\
\hline
\end{tabular}}
\label{tab-mlsummary}
\vspace{-10pt}
\end{table*}


\begin{table*}[!t]
\caption{Summary of control and optimization techniques for RIS-aided wireless networks }
\centering
\small
\setstretch{1}
\resizebox{1\textwidth}{!}{%
\begin{tabular}{|m{1.5cm}<{\centering}|m{3.5cm}<{\centering}|m{3.3cm}<{\centering}|m{3.5cm}<{\centering}|m{3cm}<{\centering}|m{4cm}<{\centering}|}
\hline 
Optimization approaches  &  Main features   &     Advantage    &  Drawbacks    &  Difficulties   &  Application scenarios for RISs  \\
\hline
Model-based algorithms &  Model-based algorithms aim to find global optimal or at least sub-optimal results for target problems. They usually require full knowledge of the problem to find near-optimal solutions by using transformation, relaxation, and approximation.  &  Model-based algorithms, i.e., SCA, MM, can provide detailed proofs and explanations for the optimality. Target problems are efficiently solved with guaranteed optimality once the closed-form solution is achieved.  & Model-based solutions are usually problem-specific with certain requirements such as convexity and continuity, indicating case-by-case analyses and design. It has difficulty adapting to dynamically changing environments.  &  It has to apply transformations, division, and relaxation to convert the problem to specific forms. These transformations need a dedicated design for each problem.  & Numerous algorithms have been developed to solve RIS-aided optimization problems, e.g., AO to decouple the active and passive beamforming, SDR to relax the rank constraints, and SCA to estimate the sub-optimal results.    \\
\hline
Heuristic algorithms & It applies heuristic rules to find a trade-off between optimality and computational complexity. Heuristic algorithms focus on local optima and low-complexity solutions.  & Heuristic algorithms have much lower computational complexity. It has few requirements for the properties of target problems.  &  It only presents local optima in the current stage, indicating a bad performance in some cases.   & Heuristic rules should be carefully selected and designed, directly affecting the algorithm performance.  &  Considering the high complexity of RIS control problems, heuristic algorithms can provide low-complexity alternatives, i.e., sequential phase shift and on/off control using greedy rule, phase-shift optimization using GA.     \\
\hline
ML techniques & ML techniques are usually data-driven, providing unified control and optimization algorithms for certain types of problems. Most algorithms are easily applied without requiring dedicated design.    &  Data-driven approaches avoid the complexity of building dedicated optimization models. It can better adapt to the dynamic wireless environment given the learning capability.  & It may require long iteration numbers for the algorithm training. ML optimization techniques do not guarantee optimality.   &  Algorithm training is the main difficulty of applying ML, which is data and computation-demanding.  & Various ML techniques have been applied for RIS-related optimizations, e.g., neural networks for CSI prediction and RIS phase control, and DDPG for continuous RIS phase-shift optimization.   \\
\hline
\end{tabular}}
\label{tab-overallcom}
\vspace{0pt}
\end{table*}


Hierarchical learning is another technique that can be used for optimizing RIS-aided wireless networks. The main idea of hierarchical learning is to decouple the long-term task into multiple achievable goals to increase exploration efficiency\cite{pateria2021hierarchical}. In particular, it defines a meta-controller to select goals and a sub-controller to achieve these goals.
Based on the short-term performance of the sub-controller, the meta-controller can adjust the goal dynamically to guarantee the long-term performance of the whole system. Hierarchical learning can also be applied to optimization problems that include multiple control variables with different time scales\cite{zhou2023hierarchical}.  
For instance, in \cite{zhou2023hierarchical}, Zhou \textit{et al.} considered a meta-controller for sleep control, and sub-controllers for transmission power and RIS control, enabling control variables with different time scales. 

Fig. \ref{fig-hrl} shows how to apply hierarchical reinforcement learning to RIS-aided wireless networks. The agent consists of a meta-controller and a sub-controller. 
The sub-controller can produce long-term policy instructions for the sub-controller, such as the maximum number of active RIS elements that is available. Then, given high-level goals, the sub-controllers can select short-term decisions for RIS phase shifts. Meanwhile, the meta-controller focuses on average power consumption in a period as long-term network performance, and the sub-controller accounts for delay or data rate as instant metrics. This scheme can coordinate control variables with different time scales, balancing instant and long-term network metrics. More specifically, the state-action value of the meta-controller is updated by:
\begin{equation} \label{eq-hrl1}
\resizebox{0.89\hsize}{!}{$\begin{aligned}
&Q_{meta}^{new}(s_{meta},g_{meta}) = Q_{meta}^{old}(s_{meta},g_{meta})+\\
&\alpha(r_{ex}+\gamma \max\limits_{g} Q_{meta}(s_{meta}',g)-Q_{meta}^{old}(s_{meta},g_{meta})),
\end{aligned}$}
\end{equation}
where $s_{meta}$ and $s_{meta}'$ is the current and next meta-states, $g_{meta}$ is the goal, and $r_{ex}$ is the extrinsic reward, respectively. $Q^{old}_{meta}$ and $Q^{new}_{meta}$ are old and new state-action values for the meta-controller, indicating the accumulated reward by selecting $g_{meta}$ under state $s_{meta}$.  



Similarly, the Q-value of the sub-controller is updated by
\begin{equation} \label{eq-hrl2}
\resizebox{0.89\hsize}{!}{$\begin{aligned}
Q&_{sub}^{new}(s_{sub},g_{meta},a_{sub}) = Q_{sub}^{old}(s_{sub},g_{meta},a_{sub})+\\
&\alpha(r_{in}+\gamma \max\limits_{a} Q_{sub}(s_{sub}',g_{meta},a)-Q_{sub}^{old}(s_{sub},g_{meta},a_{sub})),
\end{aligned}$}
\end{equation}
where $s_{sub}$ and $s_{sub}'$ are current and the next sub-states, $a_{sub}$ is the action, and $r_{in}$ is the intrinsic reward. $Q^{new}_{sub}$ and $Q^{old}_{sub}$ are defined similarly as the meta-controller, indicating the expected reward of selecting $a_{sub}$ under state $s_{sub}$ and goal $g_{meta}$. Equation (\ref{eq-hrl2}) shows that the sub-controller is under the policy control of the meta-controller.





\subsection{Analyses and Discussion }

ML offers promising opportunities for optimizing RIS-aided wireless communications. Table \ref{tab-mlsummary} overviews various ML techniques \footnote{Note that there are many ML algorithms applied to wireless communications. Instead of collecting all the existing ML algorithms, Table \ref{tab-mlsummary} provides a compressed taxonomy to understand the feature of each technique along with RIS control applications.}.  
Supervised learning applies neural networks for prediction-based control, utilizing CSI to predict achievable data rate, but it relies on fine-grained labelled datasets\cite{alexandropoulos2020phase}- \cite{hu2021reconfigurable}. Meanwhile, neural networks may be used in an unsupervised manner by involving the objective function in the loss function\cite{nguyen2021machine}-\cite{gao2020unsupervised}. Such an unsupervised learning approach can reduce the dependence on labelled datasets. 

RL is the most widely applied ML technique for optimization problems, and each RL algorithm has its own features and difficulties. For example, DDPG can handle continuous action space of RIS but can be unstable\cite{yang2020deep, guo2021learning, huang2020hybrid,lin2020deep}, and DDQN can prevent overestimation but sampling efficiency is low \cite{liu2020ris}.
FL and graph learning are newly emerging ML techniques. Most existing works consider RIS-enhanced AirFL, demonstrating that RISs can improve the training efficiency and performance of FL \cite{liu2021reconfigurable, yang2021reconfigurable, ni2022star,ni2021federated,zhang2021energy,battiloro2022dynamic}. 
Graph learning has shown great potential in many other fields, and wireless network applications include power control and interference management \cite{naderializadeh2020wireless}, resource allocation \cite{eisen2020optimal,jiang2020dynamic}, and network slicing \cite{wang2020graph}.
Finally, transfer learning and hierarchical learning are promising ML techniques for RIS-aided wireless networks. Transfer learning can reduce the model training efforts, while hierarchical learning provides a novel architecture for applying ML to wireless communications, especially when optimization parameters have different timescales. However, more research is needed on both techniques as they are used for RIS-aided wireless networks.    



\begin{figure*}[!t]
\centering
\includegraphics[width=1\linewidth]{Image/fig-compar.jpg}
\caption{Comparison between model-based algorithms, heuristic methods, and ML-based algorithms.}
\label{fig-compar}
\setlength{\abovecaptionskip}{-2pt} 
\vspace{-10pt}
\end{figure*}



\section{Comparison between Model-based, Heuristic and ML Approaches}
\label{sec-compa}


This work has introduced three types of optimization techniques: model-based, heuristic, and ML approaches. One intuitive question is how to evaluate the advantages and difficulties of these techniques, which can help to select the most efficient algorithm for specific problems. To answer this question, we compare these approaches in Table \ref{tab-overallcom}, including main features, advantages, drawbacks, difficulties, and applications for RIS.

1) \textbf{Model-based method:} Table \ref{tab-overallcom} shows that model-based methods usually require full knowledge of the defined problem for transformation and approximation. Model-based approaches can provide efficient and stable solutions once the problem is properly reformulated. However, model-based algorithms are usually complicated to design, indicating a series of transformations, e.g., decoupling the denominator and numerator in SINR terms and relaxing integer constraints.

2) \textbf{Heuristic algorithms:} The primary benefit of the heuristic method is the low implementation complexity, decoupling the problem into multiple stages and solving sequentially, i.e., optimizing RIS control in an element-by-element manner. Heuristic algorithms achieve a trade-off between optimality and low computational complexity. 

3) \textbf{ML algorithms:} Data-driven ML algorithms present unified optimization schemes, applying to diverse problems without dedicated design. Most optimization problems can be converted into unified MDPs that include state, action, transition probability and rewards, and then RL is utilized to maximize the reward for a higher sum-rate or energy efficiency.  





Additionally, Fig. \ref{fig-compar} compares the advantage and disadvantages of these methods, including stability, optimality, generalization capability, robustness for the environment uncertainty, algorithm design/tuning difficulty, and computational resources requirements\footnote{Note that the objective of Fig. \ref{fig-compar} is not to demonstrate which technique is the best, but to provide insights into the features of diverse optimization techniques.}.

1) \textbf{Optimality:} Optimality indicates the quality of solutions, which is one of the most important metrics for optimization problems. Given full knowledge of the problem, model-based methods can usually achieve the best solution with detailed proofs. Meanwhile, ML algorithms can explore the solution space efficiently for optimization. RL takes $\epsilon$-greedy policy for action selection, and neural networks can predict the performance for unseen data. Finally, heuristic methods may find local optima instead of global optimality. 

2) \textbf{Generalization capability:} Generalization capability indicates how the optimization model can adapt to new problems. ML algorithms apply unified control and optimization models with good generalization capability. For instance, given labelled or unlabelled datasets, neural networks are easily deployed in supervised or unsupervised learning manners. In contrast, model-based methods have low generalization capability, since the optimization model requires case-by-case design. Some heuristic methods also show a high generalization capability. GA and PSO are considered examples by applying unified fitness functions to represent the optimization objective. 

3) \textbf{Robustness for the environment uncertainty:} Wireless networks are highly dynamic, and hence optimization techniques must be robust to environment uncertainties. With the learning capability, ML algorithms can adapt well to dynamic environments. Meanwhile, some heuristic algorithms can also adapt to environment changes by applying heuristic rules. However, such uncertainty may greatly affect the performance of model-based algorithms, since they require full knowledge of the optimization problem. One possible solution is to assume environment changes follow some specific distributions, but the optimization over distributions will further increase the complexity. Another solution is to use Monte Carlo sampling and repeat the optimization to achieve average results, which is time-consuming.   





4) \textbf{Algorithm design/tuning difficulty:} This metric refers to the difficulty of designing or tuning algorithms. ML methods apply unified frameworks with low design difficulty. However, some key parameters, such as the number of neural network layers and learning rates, increase the difficulty of algorithm tuning. Similarly, meta-heuristic algorithms are also sensitive to key parameters, e.g., population numbers and inertia weight. But other heuristic methods, especially greedy algorithms and matching theory, can be easily designed with little tuning effort.  
Model-based methods are usually complicated to design because it has stringent requirements for problem forms. Therefore, diverse transformation, relaxation and approximation techniques must be applied, increasing the algorithm design difficulty. 

5) \textbf{Computational resources requirements:} This metric evaluates the number of computing resources (time and space) that are consumed when running a particular algorithm. ML algorithms are usually computation-demanding, requiring a large number of computational resources for model training. For instance, iterative exploration of RL and backpropagation for neural network training are time-consuming. Many model-based methods need iterations to solve the problem, and the matrix operations also contribute to storage space requirements. Heuristic methods have low computational resource requirements because heuristic rules already simplify the difficulty of problem-solving. 

6) \textbf{Stability:} Stability is another critical metric to evaluate whether the optimization algorithm can provide a stable output. With full knowledge of the problem, model-based methods can present a very stable result, especially when closed-form expressions are obtained. ML algorithms can usually achieve good performance after long exploration, but the tedious training iterations will undermine the stability of the results. Most heuristic algorithms have low stability because heuristic rules do not guarantee a stable output.

In summary, model-based solutions have high optimality and stability, but their generalization capability is low. Such algorithms are hard to design and vulnerable to environment uncertainties. Heuristic algorithms focus on feasible solutions with low design difficulty and computational resource requirements, but optimality and stability cannot be guaranteed. Finally, ML algorithms present high optimality, generalization capability, and robustness for dynamic environments. But ML techniques are computation-demanding with moderate algorithm training and tuning challenges.      

