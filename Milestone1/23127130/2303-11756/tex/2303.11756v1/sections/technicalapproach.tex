\vspace{-0.1cm}
\section{TECHNICAL APPROACH}
\label{drift:sec:approach}
In our work, we employ a dynamics model that predicts a potential future state given an input action. Subsequently, the dynamics model is used to control a vehicle along pre-defined trajectories following a model predictive scheme.
Our approach does not model explicit parameters of a physical model nor does it take factors such as tire pressure or temperature into account. 
One could combine these aspects with out method but this is beyond the scope of this work.

In contrast to previous approaches, we propose a method that considers potential changes of the road material along the track, which may influence the dynamic behavior of the vehicle. 
Therefore, we first create a grid map that is defined in global coordinates and equally divides the space into quadratic cells. 
Once the vehicle traverses a cell of the grid map, a mapping neural network infers a latent vector that describes the local road surface. 
Thereby, the mapping network leverages various modalities that were recorded during the traversal of the cell, such as RGB images, acoustic spectrograms, history states, and history actions. 
Our dynamics model takes, aside from low-level state and action information, the latent vector corresponding to the current vehicle location as an input, such that it can leverage the additional mapped cues from multiple modalities to optimize the predictions of future states, making it surface-aware.
As the training of the mapping model is guided by the loss of the dynamics model predictions, we do not require any labels that associate the input modalities with specific surface characteristics.
Note, that in contrast to previous approaches, our work models the whole surface-aware dynamics as a neural network, which avoids assumptions or inductive biases for the created surface map.
Next, we unroll multiple trajectories using the dynamics model and sampling from the action space. 
Our system then employs a reward function and the cross entropy~\cite{de2005tutorial} method to score and select the trajectories that follow a reference path as fast and close as possible. 
Following a typical model predictive scheme, we execute the first action of the resulting plan and restart the process.
In the following we describe each component of our system in more detail, followed by an explanation of the loss functions and training procedure.



\vspace{-0.1cm}
\subsection{Surface-Aware Probabilistic Dynamics Model Ensemble}
\vspace{-0.1cm}
\label{drift:sec:dyn_model}
Our dynamics model predicts the next output state $s^{\mathrm{out}}_{t+1}$, given the current input state $s^{\mathrm{in}}_{t}$ and the current action $a_t$. To this end, we distinguish between input states, which are the representation of the input to the dynamics model, and output states, which are the prediction of the dynamics model.
We define the input state as the concatenation of 3D linear velocities $\mathrm{vel}_l$, angular velocities $\mathrm{vel}_a$, linear accelerations $\mathrm{acc}_l$, angular accelerations $\mathrm{acc}_a$, and motor rpm : $s^{\mathrm{in}} = [\mathrm{vel}_l^T, \mathrm{vel}_a^T, \mathrm{acc}_l^T, \mathrm{acc}_a^T, \mathrm{rpm}]^T$. 
Additionally, we define the predicted output state of the dynamics model as estimated local changes in the x-position $\Delta p_x$, y-position $\Delta p_y$ and yaw angle $\Delta \gamma$, all velocities, and all accelerations as  $s^{\mathrm{out}} = [\Delta p_x, \Delta p_y, \Delta \gamma, \mathrm{vel}_l^T, \mathrm{vel}_a^T, \mathrm{acc}_l^T, \mathrm{acc}_a^T, \mathrm{rpm}]^T$.
The action is composed of the throttle $a_\mathrm{th}$ and steering command $a_\mathrm{st}$, such that $a=[a_\mathrm{th}, a_\mathrm{st}]$.
We make our model surface-aware by using additional latent vectors as input to the dynamics model. These latent vectors describe the local learned properties of the road.
Therefore, we propose to learn a latent map $L$ that is represented as a grid map where each quadratic cell $c$ holds a distribution over the latent vector. Here, we assume each entry of the map to be a $k_l$-dimensional multivariate normal distribution that is parametrized by $l_{\theta}^c$, corresponding to cell $c$. As the vector is learned implicitly, the number of dimensions $k_l$ represents a hyperparameter. 
To predict the next output state, we first sample a latent vector $l^c$ from the latent distribution $\mathcal{N}(l_{\theta_\mu}^c, {l_{\theta_{\sigma^2}}^c})$ at the cell corresponding to the current vehicle position. 
Following, we employ an ensemble of probabilistic dynamics models~\cite{chua2018deep} with parameters $\psi$ to predict the next state, while capturing model and data uncertainties. The input of this ensemble comprises the current input state, the current action, and the sampled latent vector, which we feed by simple concatenation. 
Thus, we model the Gaussian distribution of the next state as:
\vspace{-0.1cm}
\begin{equation}
    f_{\psi}(s_{t+1} \mid s_{t}, a_t) = \mathrm{Pr}(s_{t+1} \mid s_{t}, a_t, l^c;\psi).
    \vspace{-0.1cm}
\end{equation}
For clarity, we omitted the differentiation between the input and the output state.



\vspace{-0.1cm}
\subsection{Mapping Network}
\vspace{-0.1cm}
\label{drift:sec:mapping_network}
To estimate the latent map, we propose a novel neural network architecture that takes a variety of modalities as input.
In more detail, when the car traverses a cell $c$, we leverage an RGB image $I^c$, an acoustic spectrogram $S^c$, the history of states $H_s^c$, the history of actions $H_a^c$, and the previous estimate of the mean and variance of the latent vector $l_{\theta}^c$ that were recorded in the same cell $c$. The cues are then encoded into high-level features using respective encoders.
These features are then concatenated and passed to another MLP with two output heads, which predicts the mean and variance of the latent vector respectively.
As mentioned in Sec.~\ref{drift:sec:dyn_model}, the means and variances are then aggregated to a latent map $L$.
More formally, let $\phi$ be the parameters of the mapping model and $\Tilde{l_{\theta}^{c}}$ the updated parametrization of the Gaussian distribution of the latent vector in cell $c$. We define a latent update for the cell $c$ as: 
\vspace{-0.1cm}
\begin{equation}
\Tilde{l_{\theta}^{c}} = M_{\phi}(I^c, S^c, H_{s}^c, H_{a}^c, l_{\theta}^c),
\vspace{-0.1cm}
\end{equation}
Note, that since we input previous latent estimates $l_{\theta}^c$, the latent representation of the road surface is updated iteratively. 

While the RGB image may entail visual information of the road, the acoustic spectrograms capture direct tire-road interactions that are characteristic for specific materials. Additionally, from learning about the history of states and actions, our mapping model can infer ground patterns that lead to specific state sequences given the respective actions.

Particularly at inference time, estimating the road characteristics from only the low-level state and action history would most likely fail when the vehicle is at slow speeds or stands still.
To argue about the road surface, the dynamic behavior needs to differ across different road materials due to distinct friction characteristics. This, however, is only given in cases where the maximum frictional force~\cite{mu2003estimation} that is achieved is smaller than the force that is needed to sustain the vehicle track. Thus, to enable arguing about the road material, situations are required in which the car starts slipping.
While, for training purposes of the dynamics model, this data can be collected by an expert driver, uncontrolled slipping should be avoided during inference. 
To this end, our multimodal approach allows learning a mapping that associates visual or acoustic cues to a latent representation of the road without the requirement of slipping. As an example, our network can learn to associate a slippery surface with the visually shiny appearance of the road. Further, acoustic spectrograms can contain information about the road even under low velocities.

Consequently, we require examples of aggressive driving only during training, while during inference the road representation can be estimated under all conditions.
Furthermore, the employed modalities can complement each other if one modality lacks information due to visual occlusions, low lighting,  or when external acoustic events drown out important acoustic tire-road interactions.
We show in Sec.~\ref{drift:sec:results} that leveraging multimodal data yields high gains in state prediction performance.

\vspace{-0.1cm}
\subsection{Training of the models}
\vspace{-0.1cm}
\label{drift:sec:training}
We first collect a training dataset with dynamic examples of random driving in environments with spatially changing road materials. These driving examples include situations where the car slips.
To train our networks we propose a loss function that fulfills two requirements:
\begin{itemize}
    \item Unrolling of future states may lead to querying cells that have not been observed yet. In these cases, it should be possible to inform the dynamics model of zero knowledge of the surface material.
    \item The outputs of the mapping model should represent a spatial property of the road surface that is valid for any state prediction in the respective cell. 
\end{itemize}
To accomplish these requirements, we first group the individual recorded ground-truth state transitions according to the cell $c$ in which the transition was captured. Here, we denote the $n$-th state-transition in our dataset that occurred in the cell $c$ as $s^{c^n}_{t} \rightarrow  s^{c^n}_{t+1}$. Now, having a list of all state transitions that occurred in the same cell, we select $N$ random state transitions within a cell and define the loss for training the dynamics model as:
\vspace{-0.1cm}
\begin{equation}
\label{drift:eq:main_loss}
\mathcal{L}_d = \sum_{n=[0, 1, ... N]}(\mathcal{L}_g( f_{\psi}(s_{t+1}^{c^n} \mid s_{t}^{c^n}, a_t^{c^n}, l^{c^n} ),  \Bar{s^{c^n}_{t+1}})),
\vspace{-0.1cm}
\end{equation}
where $l^{c^0} = 0$ and for $n>0$:
\vspace{-0.1cm}
\begin{equation}
% l^{n+1} = M_{\phi}\left(I^{c^{n}}, S^{c^{n}}, H_{s}^{c^{n}}, H_{a}^{c^{n}}, l_{\theta}^{c^{n}}\right),
l^{c^{n+1}} = M_{\phi}\big(I^{c^{n}}, S^{c^{n}}, H_{s}^{c^{n}}, H_{a}^{c^{n}}, l_{\theta}^{c^{n}}\big),
\vspace{-0.05cm}
\end{equation}
and where $\mathcal{L}_g(\theta, \mathrm{target}) = \frac{1}{2} (\log \theta_{\sigma}^2 + \frac{(\theta_\mu - \mathrm{target})^2}{\theta_{\sigma}^2})$ is the Gaussian negative log-likelihood loss, and $\Bar{s^{c^n}_{t+1}}$ is the ground truth target state. The order of the traversals is irrelevant to our loss. Our loss represents the update scheme of the latent vectors, in which in the first iteration no knowledge of the surface is assumed. Thus, in the first iteration, a latent vector for the dynamics model is defined as a zero-vector, which forces the dynamics model to predict a future state distribution that is broad enough to cover all road materials properties that appear during training. This is particularly useful when unrolling state sequences over cells that have not been observed yet. In these cases, a conservative estimate of the state distribution is required as unobserved cells may entail any material or surface condition. 
In the following iterations of our loss function ($n >0$), the latent vector fed to the dynamics model is updated using our mapping model. In our loss function, the latent update is calculated based on the observed data of the previous traversal $n-1$, while the dynamics model predicts the state transition for the current traversal $n$ in the same cell $c$.  Thus, we ensure that the latent vectors are independent of the currently predicted state transition and represent a joint representation that improves prediction accuracy for all transitions in the respective cell.
Note that if the dynamics model would receive a latent vector generated by the mapping network using data recorded at the same time as the inputs of the dynamics model, the mapping model could directly contribute to the prediction of the next state rather than representing a spatial property of the track.
For our experiments, we set the number of selected state transitions to $N=3$.

\subsubsection{Three Stage Training}
We optimize our model using a three-stage approach. In the first stage, we optimize the dynamics model as well as the latent vectors but without training the mapping network. Instead, we optimize the latent vectors $l^c$ directly by backpropagating into them, treating the map as model parameters. We denote the directly optimized latent parameters as $\Bar{l^{c}} \in \Bar{L}$. In contrast to the limited information of the input of the mapping network at inference time, this has the advantage that the latent vectors can be thoroughly optimized over all batches of the dataset. 
We denote the resulting loss as:
\vspace{-0.1cm}
\begin{equation}
    \mathcal{L}_{d_{\mathrm{s1}}} = \sum_{n=[0, 1, ... N]}(\mathcal{L}_g( f_{\psi}(s_{t+1}^{c^n} \mid s_{t}^{c^n}, a_t^{c^n}, \Bar{l^{c}}),  \Bar{s^{c^n}_{t+1}}))
\vspace{-0.1cm}
\end{equation}
In contrast to the later training stages, we do not inject zero-vectors, while optimizing the latent vectors directly as we presented in Eq. \ref{drift:eq:main_loss}. Experiments have shown that the training becomes instable otherwise.

% To avoid agitated latent maps, we additionally add a smoothness term that minimizes the local gradient of the latent maps:
To avoid agitated latent maps, we add a smoothness term minimizing the local gradient of the latent maps:
\vspace{-0.15cm}
\begin{equation}
\mathcal{L}_s = \; \mid \mid \nabla \Bar{L} \mid \mid^2    
\vspace{-0.15cm}
\end{equation}
The overall loss being optimized in the first stage is simply a weighted sum of both loss functions:
\vspace{-0.15cm}
\begin{equation}
    \mathcal{L}_{\mathrm{s1}} = \mathcal{L}_{d_{\mathrm{s1}}} + \lambda \mathcal{L}_s,
\vspace{-0.15cm}
\end{equation}
where $\lambda$ denotes a weighting hyper-parameter that defines the strength of the smoothness term.

However, as the parameter map $\Bar{L}$ is optimized offline, it can not be employed in practical applications as the vehicle should be capable of driving through previously unseen environments. By leveraging our multimodal mapper, new environments should be observed on-the-fly avoiding this limitation.
Thus, in the second stage, we freeze the learned latent parameters $\Bar{L}$ and the dynamics model while optimizing the mapping network. In this stage, we guide the latent predictions from our mapping model, by optimizing the negative log-likelihood of the predicted latent vector distribution $l_{\theta}^{c}$ given the learned parameter corresponding to the same cell $\Bar{l^{c}}$.
Overall, the loss for the second stage of our training scheme is defined as:
\vspace{-0.15cm}
\begin{equation}
    \mathcal{L}_{\mathrm{s2}} = \sum_{n=[1, ... N]} \mathcal{L}_g(l_{\theta}^{c^n}, \Bar{l^{c^n}}).
\vspace{-0.15cm}
\end{equation}
In the last stage, we then freeze the mapping model and refine the dynamics model by optimizing Eq.~\ref{drift:eq:main_loss} and feed the estimate $l^c$ from the mapping network into the dynamics model instead of the previously used learned parameter $\Bar{l^{c}}$.

\begin{figure*}
\centering
\includegraphics[width=0.9\linewidth]{images/scheme2.png}
\caption{In our approach the vehicle estimates a distribution of a latent representation of the road surface using different modalities, such as history state/action information, RGB images, and acoustic spectrograms. Thereby, all local latent distributions are collected in a global grid map $L$.  Once the vehicle traverses a cell $c$ of the grid map, the underlying latent distribution $l_{\theta}^c$ gets updated. For planning purposes, an ensemble of probabilistic dynamics models estimates the distribution of the future state $s_{t+1}$ given the actions $a_t$, the current input state $s_t$ and a sampled latent vector from the latent distribution of the grid-map-cell that corresponds to the input state $s_t$. As the dynamics model can be successively applied, a distribution of future trajectories can be obtained. As both mechanisms, latent mapping and state prediction can run independently from each other, the mapping is conducted asynchronously.}
\label{fig:architecture}
\vspace{-0.5cm}
\end{figure*}



\vspace{-0.1cm}
\subsection{Planning and Control}
\vspace{-0.1cm}
In order to follow a reference trajectory $T_r$, we follow a model predictive control approach. In detail, we use iCEM~\cite{pinneri2020sample}, which generates multiple future state sequences by sampling over the actions. 
The best sequences are selected using a pre-defined reward function and are refined for a specific amount of iterations. In contrast to the vanilla CEM~\cite{de2005tutorial}, iCEM provides significantly better sample efficiency and generates smoother trajectories due to enforced temporal consistency along the state sequences. These properties make the sampling-based planning real-time capable, which is a crucial requirement for high-speed autonomous driving.
\subsubsection{Trajectory Unrolling and Latent Sampling}
To generate candidate trajectories for the planning module, we start from the initial current state of the vehicle and unroll future state sequences by successively applying our dynamics model given a sequence of actions. We then accumulate all predicted local changes of the x-position $\Delta p_x$, y-position $\Delta p_y$, and yaw angle $\Delta \gamma$ to convert the state sequences into trajectories that are defined in the coordinate system of the initial state. 
Finally, we add the initial position of the vehicle to convert them into the global coordinate system. To sample multiple trajectories from our ensemble of dynamics models, we employ the \textit{TS1}-strategy \cite{chua2018deep}. In each iteration of iCEM we generate trajectories for $N_a$ distinct action sequences. As our ensemble of dynamics models predicts a single state transition at a time, we apply our dynamics model $h$ times for a trajectory with the length of $h$ time steps and the same number of actions. Further, we sample $k$ different state hypotheses for each individual action, effectively resulting in $N_a*k*h$ inferences passes of our dynamics model and $k*N_a$ trajectories. During trajectory generation, the latent vectors are sampled from the respective latent distribution for each individual state of the trajectories and leveraged for the next update using the dynamics model. Thus, intra-trajectory changes of the road materials are considered.
\subsubsection{Map Update}
We start with a map in which the parametrization of each cell is set to zero. As discussed in Sec.~\ref{drift:sec:training} this indicates zero knowledge about the map.
As the latent distribution in each cell can be updated asynchronously with respect to the prediction of the dynamics model, we run the mapping model and the controller including the dynamics model in separate threads.
This ensures that the dynamics model does not need to wait for the mapping inference to finish and allows for parallelization as we run the dynamics model on CPU and the mapping model on GPU.
\subsubsection{Reward Function}
To score all trajectory candidates, we propose a reward function that evaluates these in terms of different metrics.
The target trajectory is given by $x-y$ coordinate waypoints.
We measure the deviation of a given trajectory to the target with the cross-track error $R_{cte}$.
As we want the car to move as fast as possible we define a progress reward $R_p$ by adding up the length of all the line segments between waypoints the trajectory passes.
Further, to punish risky maneuvers and prevent shortcuts we define a binary boundary violation reward $R_b$ which is equal to $1$ if a specified boundary around the waypoints is exceeded and $0$ otherwise.
Lastly, to encourage smooth driving we define the reward $R_a$ as the absolute difference between the last executed throttle command and that of the first action of the trajectory.
The final reward for a trajectory is defined as
\vspace{-0.1cm}
\begin{equation}
    % R = w_p \cdot R_p - w_{cte} \cdot R_{cte} - w_a \cdot R_a + w_t \cdot R_t - w_b \cdot R_b,
    R = w_p \cdot R_p - w_{cte} \cdot R_{cte} - w_a \cdot R_a - w_b \cdot R_b,
\vspace{-0.1cm}
\end{equation}
where we set the weighting parameters to $w_p=40, w_{cte}=10,w_a=20, w_b=20000$.
