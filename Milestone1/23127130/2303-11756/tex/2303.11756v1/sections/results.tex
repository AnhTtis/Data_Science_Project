\vspace{-0.1cm}
\section{Implementation Details}
\vspace{-0.1cm}
We implement our approach on top of MBRL \cite{Pineda2021MBRL}, a pytorch-based~\cite{NEURIPS2019_9015} general framework for model-based reinforcement learning. We employ a ResNet-18 architecture for the image and spectrogram encoders respectively. For the base architecture of the dynamics model, we leverage the implementation of the Gaussian ensemble from MBRL.

We use a learning rate of $0.001$ for optimizing the parameters of the dynamics model and the map parameters, and a different learning rate of $0.0001$ for the mapping model. 
We train all models with a batch size of 96 across four \textit{RTX 2080 ti} GPUs for 500 epochs. For inference and running the model predictive controller, we use a Ryzen 5950X processor.

To extract the spectrograms from the acoustic data, we use an FFT with 257 bins and a hop length of 128. We extract the spectrogram over the last second of data and convert it to decibels afterward. The images and spectrograms are resized to $336$x$188$ before passing them to their respective encoders.

We further define the time span of a single state transition as $0.1$s and set the map resolution to $50$cm. As for the iCEM optimizer, we set $\beta = 4$ and $\gamma=1.3$. Furthermore, we use a planning horizon of $h=8$ steps, $32$ samples in each iteration, and set the number of iCEM iterations to 2.

\section{Experimental Results}
\label{drift:sec:results}
To assess the efficacy of our method we train on the training set of \textit{Dynamic FreiCar}.
\vspace{-0.05cm}
\subsection{Evaluation of the Dynamics Model}
\vspace{-0.05cm}
We evaluate the prediction accuracy of the dynamics model on the test set of our dataset. To generate a test set that represents practical use-cases, we leverage our method to drive 10 laps autonomously on track 3 (see Sec.~\ref{drift::planning_control}). We record all state transitions during this run and employ them as the test set for the following evaluation.
As unrolling long trajectories are practically important for planning and control, we introduce a metric that describes how well the predicted states align with the ground truth while considering uncertainty.
Thus, for all time steps in our dataset, we unroll a sequence of $N_s$ states using the future actions that have been carried out following the current state. To capture multiple hypotheses we repeat the unrolling process $100$ times for the same action sequence. Now, given $100$ hypotheses of the future trajectory, we compute the euclidean distance between each point of every trajectory hypothesis and the observed ground-truth trajectory that was driven. Finally, we compute the mean euclidean error.
More formally, let $D_s$ be the set of all observed states in the dataset, $H(s)$ all unrolled hypotheses starting in state $s$, and $\overline{T(s)_n}$ the n-th point of the observed ground-truth trajectory that starts in $s$. Then we define the metric as:
\vspace{-0.1cm}
\begin{equation}
    L2_{N_s} =\frac{1}{N_D N_H N_s}\sum_{s \in D_s} \sum_{h \in H(s)} \sum_{n=0}^{N_s} \mid \mid h_n - \overline{T(s)_n} \mid \mid_2 .
    \vspace{-0.1cm}
\end{equation}
where $N_D$ is the number of starting states in our dataset and $N_H=100$ is the number of rolled-out hypotheses.
We compute our evaluation metrics in the chronological order of the test dataset by iteratively updating the latent cues of the grid map as more data about the surface can be observed over time. This evaluation scheme strictly represents the practical deployment of the model, since the full map can not be leveraged at the beginning but builds up progressively.

We compare our approach against a baseline that does not employ any latent mapping (Ours-w/o map) similar to PETS~\cite{chua2018deep} and additionally present the performance of our approach when only a subset of the modalities are leveraged for the mapping model.
We denote a model that uses only images, spectrograms or history state/action information for latent mapping as \textit{Ours (I)}, \textit{Ours (A)} or \textit{Ours (S)} respectively. Furthermore, we show the results of a dynamics model that takes the ground-truth terrain types instead of the estimated latent vectors of the map as input. To encode the ground-truth surface we simply provide a scalar value to the model indicating on which terrain type it is operating. We denote this model as \textit{Ours (GT)}. We argue that this model should provide the highest accuracy as the terrain is known at all times.
As described in Sec.~\ref{sec:relatedworks}, the settings tackled in previous works deviate considerably from ours in terms of surface map representation and hence we refrain from comparing to them. In our work, we set the focus on evaluating the benefits of latent surface maps for dynamics models.

The results in Table \ref{drift:tab:quantitative} show that our multimodal latent mapper (\textit{Ours (AIS)}) significantly boosts prediction performance. 
By learning and mapping multimodal cues about the surface material, we reduce the error for $N_s=30$ from $0.462$ to $0.374$ corresponding to a reduction of $19\% $ over a model without the mapping model. In comparison to the model with the ground-truth terrain types as input (\textit{Ours (GT))}, our model is on par with respect to the $L2_{10}$ metric and only $1.9\%$ and $4.2\%$ worse for the $L2_{20}$ and $L2_{30}$ metrics respectively. As the performance of the model with ground-truth terrain types is expected to be an upper limit, one can argue that our model effectively predicts the surface information needed to improve prediction performance. 

Further, one can observe that the dynamics model performs best when using a mapping model that takes all modalities as input. 
Leveraging only the history of state/action information yields an $L2_{30}$ error of $0.422$, 
only acoustic yields $0.397$, and only visual cues yields $0.383$.
An interesting observation is that the combination of the history of states/actions and acoustic cues reaches a very low $L2_{30}$ error of $0.393$, even though images are not employed in this case. In contrast to RGB images, acoustic spectrograms do not contain information on the spatial location of the car. Thus, the image-free version of our mapping model is less prone to overfitting. Although we could not observe overfitting of the mapping model during our experiments, this could ease training in more large-scale scenarios.
Further, some of the improvement of using all modalities over just using the image might come from the fact that the front-facing camera only captures the surface farther in front of the current position, which might result in poor image-terrain associations at surface transitions. 


While we used a ten-dimensional latent vector ($k_l=10$) for the previously explained models, we additionally evaluate models that leverage all modalities and employ $1$ and $5$ elements. These models are denoted as \textit{Ours (AIS,$k_l=1$)} and \textit{Ours (AIS,$k_l=5$)} respectively. Here, one can note that estimating ten-dimensional latent vectors additionally improves the $L2_{30}$ metric by $10\%$ over a model that employs a single dimension. We explain this effect by the thesis that overparametrization of the surface information eases the training procedure and helps avoiding local minima.  Estimating more than ten latent dimensions did not show further performance gains during our experiments. 


\begin{table}
\footnotesize  % original
% \scriptsize  % smaller
\centering
% \renewcommand{\arraystretch}{0.2}  % TABLE ROW HEIGHT
\begin{tabular}{c|c|c|c}
Model & $L2_{10}$ $\downarrow$ & $L2_{20}$ $\downarrow$ & $L2_{30}$ $\downarrow$\\
\noalign{\smallskip}\hline\hline\noalign{\smallskip}
Ours-w/o map & $0.094$ & $0.262$ & $0.462$ \\
Ours (S) & $0.090$ & $0.242$ & $0.422$ \\
Ours (A) & $0.085$ & $0.228$ & $0.397$ \\
Ours (I) & $0.081$ & $0.218$ & $0.383$ \\
Ours (AS) & $0.082$ & $0.225$ & $0.393$ \\
Ours (AIS-$k_l$=1) & $0.087$ & $0.237$ & $0.418$ \\
Ours (AIS-$k_l$=5) & $0.080$ & $0.223$ & $0.400$ \\
Ours (AIS) & $\mathbf{0.079}$ & $\mathbf{0.212}$ & $\mathbf{0.374}$ \\
\noalign{\smallskip}\hline\noalign{\smallskip}
Ours (GT) & $\mathbf{0.079}$ & $\mathbf{0.208}$ & $\mathbf{0.359}$\\
\hline\noalign{\smallskip}
\end{tabular}
\caption{Quantitative evaluation of the prediction accuracy of our proposed dynamics network. We present comparisons to a model that is not using a latent map as input and to a model that leverages a map that is optimized offline as parameters.}
\label{drift:tab:quantitative}
\vspace{-0.3cm}
\end{table}

\vspace{-0.05cm}
\subsection{Qualitative Latent Maps}
\vspace{-0.05cm}
To investigate the learned latent maps in more detail, we visualize the learned map using a color-coding and $k_l=10$.
A principal component analysis (PCA) projects the multi-dimensional latent vector to a single scalar. We create the latent vectors using all available data of the test-split ($10$ laps) of our dataset. 
The updates of the latent vectors are conducted in chronological order of the split. 
Fig.~\ref{drift:fig:qualitative_maps} shows the learned map, the predicted variance of the latent estimate, and the corresponding ground-truth layout. %The offline optimized parameter map is not shown, as it is only optimized in the training set with different environments.
The experiment clearly reveals that our latent embedding correlates with the real-world layout of the test environment. Thus, we can validate that our mapping model predicts spatial cues of the track surface. Further, we observe that the predicted variance of the latent cues correlates with the locations of transitions between different materials. This follows our intuition as the observed data corresponding to a single cell of the grid map can contain information about multiple materials, since the cell can potentially stretch over material transitions. As the test split entails a distinct arrangement of the surface materials not seen during training, we can confirm that our mapping model generalizes over the map structure.

\begin{figure}
\centering
\footnotesize
\setlength{\tabcolsep}{0.1cm}% for the horiz padding
    \begin{tabular}{P{2.5cm}P{2.5cm}P{2.5cm}}
        GT & Ours-Mean(AIS) & Ours-Var(AIS) \\
        \begin{overpic}[width=1.3\linewidth,tics=10, angle =90]{images/gt_2.png}
 \put (5,2) {\textcolor{red}{\small \textbf{gt-layout}}}
\end{overpic}  & \begin{overpic}[width=1.3\linewidth,tics=10, angle =90]{images/map_mean.png}
 \put (5,2) {\textcolor{red}{\small \textbf{mean}}}
\end{overpic}  & \begin{overpic}[width=1.3\linewidth,tics=10, angle =90]{images/map_var.png}
 \put (5,2) {\textcolor{red}{\small \textbf{variance}}}
\end{overpic}  \\
    \end{tabular}
    \scriptsize  % smaller
    \caption{Qualitative results of aggregated predicted latent vectors. It can be observed that the predicted maps are representing different materials well.}
    \label{drift:fig:qualitative_maps} 
    \vspace{-0.6cm}
\end{figure}

\subsection{Real-World Planning and Control}
\vspace{-0.03cm}
\label{drift::planning_control}
To evaluate our surface-aware dynamics model for practical applications, we employ our model predictive controller for autonomous racing.
The experiment is designed to investigate the effect of the dynamics model being surface-aware or not under otherwise same conditions. The dynamics model could be further improved by taking other features such as tire pressure and temperature into account. However, the car does not have sensors for this and these are orthogonal improvements that are beyond the scope of this work.
We conduct the experiments on three diverse single-lane maps varying significantly in curvature.
Track 1 and 3 have three different surfaces with highly varying friction values while Track 2 is completely on a slippery surface.
The accompanying video shows all maps as well as the resulting driving of our approach.
% The maps as well as the resulting driving of our approach can be seen in the accompanying video.
We quantify the driving performance in terms of three metrics computed over 30 laps.
In more detail, we present the average lap time, the average cross-track error (CTE), and the average lane-boundary violation score.
The lap time is a useful performance indicator as wrong dynamics predictions either lead to overly careful driving or to overly aggressive maneuvers on slippery surfaces which results in deviations from the racing line or emergency braking. The other two performance indicators focus on measuring wrong predictions in the context of overly aggressive driving.
The results in Tab.~\ref{drift:tab:quantitative_driving} suggest, that our surface-aware dynamics model significantly improves all stated metrics.
Our surface-aware model achieves significant lower lap times.
Furthermore, it yields reduced cross-track error and significantly reduces the violation of lane boundaries. Thus, our approach increases driving safety in challenging environments.

\begin{table}
\footnotesize  % original
% \scriptsize  % smaller
\centering
% \renewcommand{\arraystretch}{0.2}  % TABLE ROW HEIGHT
\begin{tabular}{c|c|c|c}
Model &  Lap Time $\downarrow$  & CTE $\downarrow$ & Bd. Violations $\downarrow$ \\
\noalign{\smallskip}\hline\hline\noalign{\smallskip}
Track 1 & \multicolumn{3}{c}{\raisebox{-.5\height}{\includegraphics[height=1.3cm]{images/map_6_crop.png}}}\\
\hline\hline\noalign{\smallskip}
Ours w/o map & $8.17 \pm 0.59$ & $27.63 \pm 7.95$ & $4.82 \pm 8.64$ \\
Ours (AIS)  & $\mathbf{7.35 \pm 0.42}$ & $\mathbf{26.34 \pm 6.12}$ & $\mathbf{2.79 \pm 5.98}$ \\
\hline\noalign{\smallskip}
Track 2 & \multicolumn{3}{c}{\raisebox{-.5\height}{\includegraphics[height=1.3cm]{images/map_wood_crop.png}}}\\
\hline\hline\noalign{\smallskip}
Ours w/o map & $5.47 \pm 0.34$ & $\mathbf{14.57 \pm 4.92}$ & $1.39 \pm 4.89$ \\
Ours (AIS) & $\mathbf{4.89 \pm 0.26}$ & $17.33 \pm 3.48$ & $\mathbf{0.30 \pm 1.29}$ \\
\hline\noalign{\smallskip}
Track 3 & \multicolumn{3}{c}{\raisebox{-.5\height}{\includegraphics[height=1.3cm]{images/map_1_crop.png}}}\\
\hline\hline\noalign{\smallskip}
Ours w/o map  & $20.47 \pm 0.91$ & $61.32 \pm 14.04$ & $10.09 \pm 14.06$ \\
Ours (AIS)  & $\mathbf{19.08 \pm 0.89}$ & $\mathbf{56.14 \pm 8.17}$ & $\mathbf{8.78 \pm 10.82}$ \\
\end{tabular}
\caption{Quantitative evaluation of the driving performance of our approach. The results show the efficiency of our approach, which significantly reduces the cross-track error while achieving faster lap times in comparison to a model without a mapping network.}
\label{drift:tab:quantitative_driving}
\vspace{-0.5cm}
\end{table}



We observe that latent mapping helps in particular to improve the violation of lane boundaries as our model prevents unexpected drifting that leads off the racing track.
We illustrate such a scenario in Fig.~\ref{drift:fig:path_overlay}.
Further, actions obtained from optimizing erroneous trajectory estimates sometimes led to failure cases on slippery surfaces. In these cases, we had to manually intervene to prevent damage to the car. This happened three times for the model without map updates, while we never observed it for the model with map updates.
This behavior indicates that on slippery surfaces the model with latent surface information predicts the dynamics more accurately while the model without overestimates the speed at which certain corners can be taken as it is trained to predict well averaged over all surfaces including those for which such maneuvers are possible.


\vspace{-0.03cm}
\subsection{Progressive Map Building}
\vspace{-0.03cm}
We further investigate the quality of the latent map with respect to the number of driven laps. 
The mapping model can improve its estimate of the underlying latent vector and variance with every new lap.
This experiment is conducted on Track $3$ while driving ten laps. 
Tab.~\ref{drift:tab:quantitative_laps} shows the lap times and the L2 error that is computed by considering the state transitions of all laps but only using the observations of the first ten laps to build the latent map respectively. The results show that with each additional lap our mapping model is consistently improving the latent estimates to improve the state predictions as well as the driving performance.



\begin{table}
\setlength{\tabcolsep}{3pt}
\footnotesize  % original
% \scriptsize  % smaller
\centering
% \renewcommand{\arraystretch}{0.2}  % TABLE ROW HEIGHT
\begin{tabular}{l|c|c|c|c|c|c}
Lap number &  1 &  2 & 3 & 4 & 5 & 10\\
\noalign{\smallskip}\hline\hline\noalign{\smallskip}
Lap time [s] $\downarrow$  & $20.89$ & $20.62$ & $20.08$ & $19.44$ & $19.32$ & $\mathbf{19.10}$ \\
\noalign{\smallskip}\hline\noalign{\smallskip}
$L2_{10}$ $\downarrow$ & $0.0857$  & $0.0816$ & $0.0807$ & $0.0805$ & $0.0804$  & $\mathbf{0.0801}$\\
\noalign{\smallskip}\hline\noalign{\smallskip}
\end{tabular}
\caption{Quantitative evaluation of the influence of completed laps with respect to the dynamics prediction accuracy and lap time. In this experiment, we only consider map updates during a specified number of consecutive laps. The respective L2 error is calculated over the state transitions of all laps. }
\label{drift:tab:quantitative_laps}
\vspace{-0.5cm}
\end{table}


