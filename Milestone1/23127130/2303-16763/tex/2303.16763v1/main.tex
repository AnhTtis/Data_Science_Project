% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{soul}
\usepackage{color,xcolor}
\usepackage{hyperref}
\usepackage{enumitem}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

\definecolor{local}{RGB}{222,64,44}
\definecolor{global}{RGB}{246,173,83}

% Title.
% ------
%\title{ACTION ITEM DETECTION WITH CONTEXT BASED ON MEETING TRANSCRIPTS}
\title{Meeting Action Item Detection with Regularized Context Modeling}
%
% Single address.
% ---------------
% \name{Author(s) Name(s)\thanks{Thanks to XYZ agency for funding.}}
% \name{Jiaqing Liu, Chong Deng, Qinglin Zhang, Qian Chen, Wen Wang}
% \address{Speech Lab, Alibaba Group.}

\name{Jiaqing Liu, Chong Deng, Qinglin Zhang, Qian Chen, Wen Wang}
\address{Speech Lab of DAMO Academy, Alibaba Group \\
\tt \normalsize \{mingzhai.ljq,dengchong.d,qinglin.zql,tanqing.cq,w.wang\}@alibaba-inc.com}

%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
% \twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
% 	{School A-B\\
% 	Department A-B\\
% 	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
% 	while at ...}}
% 	{School C-D\\
% 	Department C-D\\
% 	Address C-D}
%

% -- added for copyright box
\usepackage[pscoord]{eso-pic}
\newcommand{\placetextbox}[3]{% \placetextbox{<horizontal pos>}{<vertical pos>}{<stuff>}
\setbox0=\hbox{#3}% Put <stuff> in a box
\AddToShipoutPictureFG*{% Add <stuff> to current page foreground
\put(\LenToUnit{#1\paperwidth},\LenToUnit{#2\paperheight}){\vtop{{\null}\makebox[0pt][c]{#3}}}%
}%
}%

\begin{document}
% \topmargin=0mm

\ninept
%
\maketitle
%
\begin{abstract}
Meetings are increasingly important for collaborations. 
%With the support of automatic speech recognition, meetings could be transcribed into transcripts. 
Action items in meeting transcripts are crucial for managing post-meeting to-do tasks, which usually are summarized laboriously. 
The Action Item Detection task aims to automatically detect meeting content associated with action items. However, datasets manually annotated with action item detection labels are scarce and in small scale.
We construct and release the first Chinese meeting corpus with manual action item annotations\footnote{\url{https://www.modelscope.cn/datasets/modelscope/Alimeeting4MUG/summary}}.
In addition, we propose a Context-Drop approach to utilize both local and global contexts by contrastive learning, and achieve better accuracy and robustness for action item detection.
We also propose a Lightweight Model Ensemble method to exploit different pre-trained models\footnote{\url{https://github.com/alibaba-damo-academy/SpokenNLP/tree/main/action-item-detection}}.  Experimental results on our Chinese meeting corpus and the English AMI corpus demonstrate the effectiveness of the proposed approaches.
%Nowadays, with the support of automatic speech recognition, it is convenient to convert the audio of meetings into transcripts. Based on meeting transcripts, many tasks have been proposed to obtain important information automatically. Among them, the action item detection task is aimed to detect sentences that contain action items.  For this text classification task, many previous studies introduced context information to improve performance based on the public meeting corpus. However, these corpora usually lack high-quality annotations of action items.  Therefore, we build and plan to publish the Chinese meeting corpus with annotations of action items to prompt related research.  We tried many methods to utilize contextual information and achieved better performance. 
\end{abstract}
%
\begin{keywords}
Action item detection, text classification, public meeting corpus, contextual information, model ensemble
\end{keywords}
%
\section{Introduction}
\label{sec:intro}
Due to technological advances and the pandemic, online meetings become more and more common for collaboration and information sharing. Automatic Speech Recognition (ASR) systems can convert audio recordings of meetings into transcripts. Many Natural Language Processing (NLP) tasks are conducted on meeting transcripts to automatically extract or generate important information such as summaries, decisions, and action items.  Action item refers to a task discussed in the meeting and assigned to participant(s) and expected to complete \emph{within a short time window} after the meeting~\cite{DBLP:conf/sigdial/GruensteinNP05}. The action item detection task aims to detect sentences containing information about actionable tasks in meeting transcripts. Action item detection could help users easily summarize meeting minutes, view and follow up on post-meeting to-do tasks.

%%%%Nowadays, online meetings have become a common and important way to collaborate on work, due to the COVID-19 pandemic and technological advances.  With the support of Automatic Speech Recognition (ASR), it is convenient to convert the audio of meetings into transcripts. Based on the meeting transcripts, many tasks are proposed to automatically detect or generate important information such as decisions, questions, and action items.  Among them, the action item detection task is aimed to detect sentences or discussions related to action items from the meeting transcripts.  The action item detection task could help users to summarize meeting minutes, and view and follow up on post-meeting to-do tasks conveniently, especially for people who are absent from the meeting.


Action item detection is usually modeled as a sentence-level binary classification task, to determine whether a sentence contains action items or not.
Many previous works~\cite{morgan2006automatically} explore machine learning methods and feature engineering on publicly available meeting corpora such as ICSI~\cite{janin2003icsi} and AMI~\cite{carletta2005ami}. 
Recently, with the success of the pretraining-finetuning paradigm and the revival of meeting-related research, approaches have been proposed based on pre-trained models~\cite{sachdeva2021action}, such as BERT~\cite{DBLP:conf/naacl/DevlinCLT19} and ETC~\cite{DBLP:conf/emnlp/AinslieOACFPRSW20}. 
In addition, some works~\cite{DBLP:conf/mlmi/PurverEN06} focus on detecting each element of action items independently, including task description, ownership, timeframe, and agreement.
%%%%The action item detection task is usually modeled as a sentence binary classification task, to determine whether a sentence involves action items. Many previous works~\cite{morgan2006automatically} applied feature engineering and machine learning methods based on public meeting corpora such as ICSI~\cite{janin2003icsi} and AMI~\cite{carletta2005ami}. With the success of pre-training framework and revival of meeting-related research, many approaches~\cite{sachdeva2021action} have been proposed based on pre-trained models, such as BERT~\cite{DBLP:conf/naacl/DevlinCLT19} and ETC~\cite{DBLP:conf/emnlp/AinslieOACFPRSW20} in recent years. In addition, there are some papers~\cite{purver2007detecting} focused on each element of action items independently, including task description, ownership, timeframe, and agreement.


For action item detection, existing public meeting corpora, such as the AMI meeting corpus and the ICSI meeting corpus,  are far from adequate to evaluate advanced deep learning models. We obtain 101 annotated AMI meetings with 381 action items following previous works~\cite{sachdeva2021action}. The ICSI meeting corpus comprises only 75 meetings without publicly available action item annotations. Therefore, we construct and make available a Chinese meeting corpus of 424 meetings with manual action item annotations on manual transcripts of meeting recordings (Table~\ref{tab:data}), to prompt research on action item detection.


%%%%For action item detection, current common datasets are not adequate to evaluate advanced deep learning based models. For instance, the AMI corpus contains only 171 meetings and has no direct annotations for action items. We have to map the action-related abstractive summary to corresponding dialogue acts to construct 101 annotated meetings with 381 action items. The ICSI corpus contains only 75 meetings without available action item annotations. Therefore, we construct and plan to make available a large-scale Mandarin meeting corpus with action item annotations. As shown in Table~\ref{tab:data}, the corpus contains ASR transcripts and annotations of 424 meetings. These meetings are simulated meetings with multiple participants interacting on a topic according to a script. The action items are annotated by three annotators independently and decided by an expert for inconsistent samples. 


% One of the important reasons limiting the development of this task is the dataset. 
% There is no doubt that public meeting corpora facilitated meeting-related research greatly.
% However, the data size of these corpora is not enough to meet the need for the training of models with an increasingly large number of parameters. 
% In addition, these meeting corpora lack high-quality annotations for action items.
% For instance, the AMI corpus contains only 171 meetings and has no direct annotations for action items. We have to map the action-related abstractive summary to corresponding dialogue acts to get the indirect annotations for 101 meetings. 
% The ICSI corpus contains only 75 meetings and it is difficult to access annotations for action items. 

\sethlcolor{yellow}
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{example.pdf}
    \caption{\small{An example of action item detection. We show the speaker and sentence id, mark the \hl{action item}, \textcolor{local}{local context} and \textcolor{global}{global context}. The \textcolor{local}{local context} provides the \underline{\textbf{timeframe}} information. And the \textcolor{global}{global context} provides the \underline{\emph{task description}} information.}
    }
    \label{fig:example}
    \vspace{-7mm}
\end{figure}


% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=\linewidth]{example.pdf}
%     \caption{\small{An example of action item detection. We show the speaker and sentence id, mark the \sethlcolor{yellow}\hl{action item}, \textcolor[RGB]{222,64,44}{local context} and \textcolor[RGB]{246,173,83}{global context}. The \textcolor[RGB]{222,64,44}{local context} provides the \underline{\textbf{timeframe}} information. And the \textcolor[RGB]{246,173,83}{global context} provides the \underline{\emph{task description}} information.}
%     }
%     \label{fig:example}
%     \vspace{-7mm}
% \end{figure}


% Therefore, we construct and plan to publish the Chinese meeting corpus with the annotation of action items. 
% As shown in Table~\ref{tab:data}, the corpus contains ASR transcripts and annotations of 424 meetings. 
% These meetings are simulated meetings with multiple participants interacting on a topic according to a script.
% To reduce the annotation cost, we have applied some heuristic methods to select candidate sentences that have both time words and action words. 
% These candidate sentences were annotated by multiple annotators independently, according to the corresponding context and the annotation specification.

Context understanding plays a critical role in various tasks on meeting transcripts. Prior works~\cite{sachdeva2021action, mullenbach2021clip} also explore context to improve action item detection performance. 
However, most methods concatenate the focus sentence with adjacent sentences (\emph{local context}) and only achieve limited gains. As shown in Figure~\ref{fig:example}, relevant but non-contiguous sentences (\emph{global context}) also provide useful information for action items.
On the other hand, both local and global contexts may contain irrelevant information, which may distract the classifier.  We propose a novel \textbf{Context-Drop} approach to improve context modeling with regularization so that the model could focus more on the current sentence, to better exploit relevant information, and be less distracted by irrelevant information in context.
%%%%Many previous works~\cite{sachdeva2021action, mullenbach2021clip} introduce context to improve the performance of action item detection.  However, we observe their method of concatenating adjacent sentences (\emph{local context}) with the sentence can only bring limited improvement. On the one hand, as shown in Figure~\ref{fig:example}, some relevant but non-contiguous sentences (\emph{global context}) also provide useful auxiliary information. On the other hand, both local and global contexts have irrelevant information, which may distract the classifier.  Therefore, we propose a novel context-drop approach to improve context modeling. Inspired by contrastive learning, context-drop forces the prediction probability distributions of a single sentence and the sentence with its context to be consistent with each other. In this way, the model could focus more on the current sentence, to better exploit relevant information and be less distracted by irrelevant information in context.


In addition, we observe that the majority voting labels are usually correct during action item annotations. Inspired by this observation, we propose a \textbf{Lightweight Model Ensemble} method to improve performance by exploiting different pre-trained models while preserving inference latency.


\placetextbox{0.5}{0.08}{\fbox{\parbox{\dimexpr\textwidth-2\fboxsep-2\fboxrule\relax}{\footnotesize \centering Accepted paper. \copyright  2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.}}}


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{model.pdf}
    \caption{\small{Illustration of proposed Context-Drop  (Section~\ref{ssec:context}) and Lightweight Model Ensemble (Section~\ref{ssec:ensemble}) methods. Based on the pre-trained models, we propose the Context-Drop method to employ contextual information for action item detection. We utilize both local and global contexts to exploit as much relevant context as possible within the max sequence length of transformers. We also propose the Lightweight Model Ensemble to improve performance using different pre-trained models.}
    }
    \label{fig:model}
    \vspace{-4mm}
\end{figure*}


The contributions of our work are as follows:
\vspace{-2mm}
\begin{itemize}[leftmargin=*,noitemsep]
    \item We construct and make available a Chinese meeting corpus with action item annotations, to alleviate scarcity of resources and prompt related research. To the best of our knowledge, this is so far the largest meeting action item detection corpus.
    \item We propose a novel Context-Drop approach to improve context modeling of both local and global contexts with regularization, and achieve improvement in accuracy and robustness of action item detection for both Chinese and English meeting corpora. 
    \item We propose a Lightweight Model Ensemble approach to integrate knowledge from different pre-trained models. We achieve improvement in accuracy while preserving inference latency.
    
\end{itemize}


% In addition, previous works lack comparative experiments on classifiers. 
% We found that the classifier initialized from another pre-trained model usually tends to achieve better performance.
% Namely, the pre-trained model to initialize the classifier is different from the pre-trained model to initialize the encoder, which can be regarded as a lightweight model ensemble approach. 
% This method can utilize different pre-trained models to bring improvements without increasing the number of parameters.











% \section{related work}
% \label{sec:prior}

% related word TBD


\section{Datasets}
\label{sec:dataset}

\vspace{-2mm}
\subsection{AMI Meeting Corpus}
\label{ssec:ami}
The AMI meeting corpus~\cite{carletta2005ami} has played an essential role in various meeting-related research. It contains 171 meeting transcripts and various types of annotations. Among 171 meetings, 145 meetings are scenario-based meetings and 26 are naturally occurring meetings. The AMI meeting corpus is a common dataset for benchmarking action item detection systems. Although there are no direct annotations for action items for this corpus, indirect annotations can be generated based on annotations of the summary. Following previous works~\cite{sachdeva2021action}, we consider dialogue acts linked to the action-related abstractive summary as positive samples for action item detection and otherwise negative samples. In this way, we obtain 101 annotated meetings with 381 action items.
%The AMI meeting corpus was published by the IDIAP research institute in 2006. It contains a total of 171 meeting transcripts and various types of annotations. Among them, there are 145 scenario-based meetings and 26 naturally occurring meetings. There is no doubt that the AMI meeting corpus plays an essential role in meeting-related research. The AMI meeting corpus is also a common dataset in action item detection. Although there is no direct annotation of action items, we can get indirect annotations based on the annotations of the summary. Following the method of previous papers, we regard the dialogue acts linked to the action-related abstractive summary as positive samples. In this way, there are 101 annotated meetings with 381 action items. 


\vspace{-2mm}
\subsection{Building A Large-scale Chinese Meeting Corpus}
\label{ssec:corpus}
The two common datasets for action item detection, namely the AMI meeting corpus and ICSI meeting corpus, are both far from adequate for evaluating advanced deep learning models on action item detection.
As described above, there are only 101 annotated meetings with 381 action items in the AMI meeting corpus. Another public meeting corpus, the ICSI meeting corpus, has action item annotations for 18 meetings~\cite{purver2007detecting} and is much smaller for action item detection research. Also, these annotations are no longer publicly available. 
%\cite{DBLP:conf/mlmi/PurverEN06} defined 4 action item dialog acts (AIDA), i.e., \emph{Task Description}, \emph{timeframe}, \emph{ownership}, and \emph{agreement}, and annotated 18 ICSI meetings this way.  However, these AIDA annotations are no longer publicly available.  ~\cite{DBLP:conf/lrec/ChenH16} defined 10 intents/actions and annotated 22 ICSI meetings. 
Scarce and small-scale meeting datasets have hindered research on action item detection. To address this issue and prompt research on this topic, we construct and make available a Chinese meeting corpus, \emph{the AliMeeting-Action Corpus (denoted as AMC-A)}, with manual action item annotations on manual transcripts of meeting recordings.  We extend 224 meetings previously published in ~\cite{yu2022m2met} with additional 200 meetings. Each meeting session consists of a 15-minute to 30-minute discussion by 2-4 participants covering certain topics from a diverse set, biased towards work meetings in various industries.
%All these meetings are simulated meetings, with interactions among multiple participants discussing. 
All 424 meeting recordings are manually transcribed with punctuation inserted.  Semantic units ended with a manually labeled period, question mark, and exclamation are treated as \textbf{sentences} for action item annotations and modeling.

We formulate action item detection as a binary classification task and conduct sentence-level action item annotations, i.e., sentences containing action item information (task description, time frame, owner) as positive samples (labeled as 1) and otherwise negative samples (labeled as 0). As found in previous research and our experience, annotations of action items have high subjectivity and low consistency, e.g., only a Kappa coefficient of 0.36 on the ICSI corpus~\cite{purver2007detecting}.
To ease the task, we provide detailed annotation guidelines with sufficient examples.  To reduce the annotation cost, we first select candidate sentences containing both temporal expressions (e.g., ``tomorrow'') and action-related verbs (e.g., ``finish''), and highlight them in different colors.  Candidate sentences are then annotated by three annotators independently. 
During annotation, candidate sentences are presented with their context so that annotators can easily exploit context information. 
With these quality control methods, the average Kappa coefficient on AMC-A between pairs of annotators is \textbf{0.47}. For inconsistent labels from three annotators, an expert reviews the majority voting results and decides on final labels. Table~\ref{tab:data} shows that AMC-A has much more meeting sessions, total utterances, and total action items than the AMI meeting corpus and comparable avg. action items per meeting.  To the best of our knowledge, AMC-A is so far the first Chinese meeting corpus and the largest meeting corpus in any language labeled for action item detection.
%The corpus is one of the important reasons to limit the research and application of action item detection.  These public meeting corpora are not enough to meet the need of models with an increasingly large number of parameters.  Therefore, we construct and plan to publish the Chinese meeting corpus with the annotation of action items.  As shown in Table~\ref{tab:data}, the Chinese meeting corpus contains ASR transcripts and annotations of 424 meetings.  All of these meetings are simulated meetings with multiple participants interacting on a given topic according to a script. Among them, there are 224 meetings published from the Alimeeting dataset~\cite{yu2022m2met} and we provided the annotations of action items.  Moreover, we added another 200 meetings with annotations of action items.  We conducted sentence-level action item annotation.  To reduce the annotation cost, we have applied some heuristic methods to select candidate sentences, such as containing both time words and action words.  These candidate sentences were annotated by multiple annotators independently. The average kappa value for two-by-two annotation is 0.47. For inconsistent samples, a final label will be annotated by one expert. According to previous papers and our experience, the annotation of action items has a high subjectivity and low consistency. To improve the annotation quality, we attached the corresponding context to each sentence, as well as the sentence id in the meeting.  We highlighted the time words and action words in different colors.  And we provided detailed annotation specifications and annotation examples. To the best of our knowledge, this is the first public Chinese corpus in action item detection. We hope this corpus can promote related research and applications. 

\begin{table}[t!]
    \centering
     \scalebox{0.8}{
    \begin{tabular}{lrrrrr}
    \toprule
    & \multicolumn{4}{c}{\textbf{AMC-A (ours)}}  & \multirow{2}{*}{\textbf{AMI}} \\
    \cmidrule(lr){2-5}
    & \textbf{All} & \textbf{Train} & \textbf{Dev} & \textbf{Test} & \\
    
    \midrule
    \textbf{Total \# Meetings} & \textbf{424} & 295 & 65 & 64 & 101 \\
    \textbf{Total \# Utterances} & \textbf{306,846} & 213,235 & 45,869 & 47,742 & 80,298 \\
    \textbf{Total \# Action} & \textbf{1506} & 1014 & 222 & 270 & 381  \\
    \textbf{Kappa Coefficient} & 0.47 & 0.46 & 0.49 & 0.50 & /  \\
    \textbf{Avg. \# Action per Meeting} & 3.55 & 3.44 & 3.42 & 4.22 & 3.77 \\
    \textbf{Std. \# Action per Meeting} & 3.97 & 3.98 & 3.35 & 4.41 & 1.95 \\
    \bottomrule
    \end{tabular}
    }
    \caption{\small{Statistics of our Chinese AMC-A corpus and the English AMI meeting corpus studied in this work.}}
    \label{tab:data}
    \vspace{-5mm}
\end{table}





\vspace{-3mm}
\section{Method}
\label{sec:method}
\vspace{-1mm}
We formulate action item detection as a binary classification task.  Given an utterance $X$ with its context $C$, the model predicts the label $\hat{y}$, i.e., whether $X$ contains action items or not. Figure~\ref{fig:model} illustrates the two proposed approaches, \textbf{Context-Drop} (Fixed and Dynamic) and \textbf{Lightweight Model Ensemble}.  Context-Drop explores local and global contexts together with regularization. Lightweight Model Ensemble is an efficient approach for improving performance using different pre-trained models while preserving inference latency.

%The definition of action item detection is as follows: Given the utterance $X$ with the context $C$ (optional), the model needs to predict the label $\hat{y}$, i.e., whether the utterance $X$ contains an action item or not. As shown in Figure~\ref{fig:model}, based on the pre-trained models, we propose the context-drop method to employ contextual information to action item detection. We utilize both local and global contexts in order to exploit as much relevant context as possible within the max length limit of transformers. In addition, we experimented with lightweight model ensemble method to improve performance using different pre-trained models.


\subsection{Context-Drop}
\label{ssec:context}
\noindent \textbf{Local and Global Context}
Coreferences and omission of information are quite common in multi-party meetings. Relevant and supporting information may appear in adjacent sentences or non-contiguous sentences. Context understanding has played a critical role in various understanding tasks in meetings, including sentence-level action item detection. 
%Relevant context can provide some auxiliary information. 
Relevant contexts are not limited to adjacent sentences (\emph{local context}). In real meeting scenarios, topics are usually mixed, hence discussions of a certain action item may spread in the session. We denote these relevant but non-contiguous sentences by \emph{global context} for action item detection.

% Many previous works~\cite{sachdeva2021action, mullenbach2021clip} explore context information in action item detection. These methods usually concatenate the focus sentence with local context as input to the model.  However, we find these concatenation-based methods only bring limited improvement. The classifier may be distracted by irrelevant information in the context. In addition, these works only utilize local context and lack exploration of the global context.

Since the global context may be distant from the focus sentence, including all sentences between the global context and the focus sentence may exceed the max sequence length mandated by Transformer-based pre-trained language models (PLMs), such as BERT~\cite{DBLP:conf/naacl/DevlinCLT19} and RoBERTa~\cite{DBLP:journals/corr/abs-1907-11692}, due to their quadratic time and memory complexity to the input sequence length~\cite{DBLP:conf/nips/VaswaniSPUJGKP17}. Hence, we employ a \textit{context selection} method to retrieve the global context for each sentence. We use the cosine similarity of n-grams to measure the similarity between sentences in a document, following the n-gram overlap method~\cite{han2021modeling}.  For each sentence, we select the top-k sentences with the highest similarity scores as its global context.

%For sentence-level action item detection, there is no doubt that context plays a very important role. In multi-person oral interaction scenarios, it is very common for omissions or coreference in a single sentence, and there is relevant supporting information in the context.  In addition, adjacent sentences tend to have the same label, because action items tend to appear frequently in task-related discussions.

%Many previous works~\cite{sachdeva2021action, mullenbach2021clip} introduced contextual information in action item detection. These methods usually concatenate the sentence with context as input to the model.  However, we have observed these concatenating methods can only bring limited improvement. The classifier may be distracted by the irrelevant information of the context. In addition, these works only utilize the local context and lack exploration of the global context.

%For sentence-level action item detection, relevant context can provide some auxiliary information.  However, relevant contexts sometimes are not only adjacent contexts (i.e., local contexts).   In real meeting scenarios, the topics are usually mixed.  For example, we can discuss action items related to the previously mentioned topics at the end of the meeting. In this case, there may be a long distance between the relevant context and the single sentence. Limited by the computational complexity, many pre-trained models can only model a max sequence length of 512.  Therefore, we retrieve the relevant sentences for each sentence in the document, to utilize the global relevant contexts as much as possible. We use the cosine similarity of n-grams to measure the similarity between sentences followed the n-gram overlap method~\cite{han2021modeling}.  And we select the top-k sentence with the highest similarity score as the global context.

\noindent \textbf{Context-Drop}
We propose a novel Context-Drop approach to improve context modeling for action item detection. 
% We observe that adjacent sentences tend to have the same labels for action items. 
Inspired by Contrastive Learning and R-drop~\cite{wu2021r}, Context-Drop forces the prediction probability distributions of a single sentence and the sentence with its context to be consistent with each other. We hypothesize that Context-Drop could help the model to focus more on the current sentence, to better exploit relevant information and be less distracted by irrelevant information in context, which in turn could improve the robustness and performance of the model. 


We propose two types of Context-Drop, namely, \emph{Context-Drop (Fixed)} and \emph{Context-Drop (Dynamic)}. As shown in Figure~\ref{fig:model}, for Context-Drop (Fixed), $input_1$ is the focus sentence, $input_2$ is the sentence with its local/global context. For Context-Drop (Dynamic), the local/global context of the focus sentence is selected dynamically. Each sentence in context has a certain probability to be kept; otherwise, the sentence is dropped from context.
Both Context-Drop variants force the prediction probability distributions for $input_1$ (denoted $x$) and $input_2$ (denoted $x^{\prime}$) to be as close as possible, by minimizing the bidirectional Kullback-Leibler divergence as in Eqn.~\ref{equ:kl}. The overall loss is calculated as Eqn.~\ref{equ:overall}, where $\alpha$ is a hyperparameter:

\begin{align}
    \mathcal{L}^i_{\text{CE}} = & - \frac{1}{2} \log \left( P_1(y_i|x_i) \cdot P_2(y_i|x^{\prime}_i) \right) \\
    \begin{split}
        \mathcal{L}^i_{\text{KL}} = & \frac{1}{2} \left( \mathcal{D}_{\text{KL}} \left( P_1(y_i|x_i) || P_2(y_i|x^{\prime}_i) \right) \right. \\ 
        & + \left. \mathcal{D}_{\text{KL}} \left( P_2(y_i|x^{\prime}_i) || P_1(y_i|x_i) \right) \right) \label{equ:kl}
    \end{split} \\
    \mathcal{L}^i = & \mathcal{L}^i_{\text{CE}}  + \alpha \cdot \mathcal{L}^i_{\text{KL}} \label{equ:overall}
\end{align}


Context-Drop (Dynamic) makes the contrast between samples more flexible.   When all contexts are dropped for both $input_1$ and $input_2$, Context-Drop (Dynamic) works equivalently to the \emph{R-Drop (Sentence)} method in Figure~\ref{fig:model}.  When all contexts are kept for both $input_1$ and $input_2$, the approach works equivalently to \emph{R-Drop (Context)}. When all contexts are dropped for $input_1$ and all contexts are kept for $input_2$, the approach works equivalently to Context-Drop (Fixed). Hence, Context-Drop (Dynamic) could be considered as a generalization of the other three methods in Figure~\ref{fig:model}.

%Therefore, we propose the context-drop approach to utilize context to improve the performance of action item detection.  We note that context does not change whether a single sentence contains an action item in the majority of cases. For sentence-level action item detection, the model prediction results should be consistent whether a single sentence as input or a sentence with context as input.  Inspired by the contrastive learning, we want the prediction probability distributions of single sentences and sentences with contexts as close as possible. By this context-drop method, the model would be more robust.  Because the model would focus more on the current single sentence and avoid being disturbed by some irrelevant information in the context.  And the model can achieve better performance using the auxiliary information from context. 

%In detail, we propose two types of context-drop methods, that is the context-drop (fixed) method and the context-drop (dynamic) method.  For the context-drop (fixed) method, the $input_1$ is single sentence, the $input_2$ is sentence with context.  We want the predicted probability distributions to be as close as possible. The distance of the predicted probability distribution is measured by the Kullback-Leibler divergence. For the context-drop (dynamic) method, the context of input can be selected dynamically.  For each sentence of context, there is a 50\% probability to be dropped, and a 50\% probability to be used as input.  This dynamic selection method makes the contrast of samples more flexible.  When all contexts are dropped for both $input_1$ and $input_2$, this input form is equivalent to R-Drop (sentence) method. When all contexts are used for both $input_1$ and $input_2$, this input form is equivalent to R-Drop (context) method. When all contexts are dropped for$input_1$, and all contexts are used for $input_2$, this input form is equivalent to the context-drop (fixed) method.


% \subsection{Global Context}
% \label{ssec:global}



\begin{table}[t!]
    \centering
    \begin{tabular}{llc}
    \toprule
    \textbf{Model} & \textbf{Modeling Task} & \textbf{AMC-A F$_1$} \\
    \midrule
    BERT & sentence classification & 64.76\footnotesize{$\pm$0.98} \\
    Longformer & sequence labeling & 65.35\footnotesize{$\pm$1.33} \\
    StructBERT & sentence classification & \textbf{67.84}\footnotesize{$\pm$1.20} \\
    \bottomrule
    \end{tabular}
    \caption{\small{Positive F$_1$ on the \textbf{Test} set of our AMC-A corpus using different pre-trained language models with different modeling tasks.}}
    \label{tab:baseline}
\end{table}


\begin{table}[t!]
    \centering
    \begin{tabular}{lcc}
    \toprule
    \textbf{Input Method} & \textbf{AMC-A F$_1$} & \textbf{AMI F$_1$} \\
    
    \midrule
    sentence & 67.84\footnotesize{$\pm$1.20} & 38.67\footnotesize{$\pm$1.25} \\
    w/\ R-Drop & 68.77\footnotesize{$\pm$0.82} & 39.26\footnotesize{$\pm$1.70} \\
    
    \midrule
    + local context & 68.50\footnotesize{$\pm$1.21} & 41.03\footnotesize{$\pm$1.42} \\
    w/\ R-Drop & 68.79\footnotesize{$\pm$0.42} & \underline{42.72}\footnotesize{$\pm$0.74} \\
    w/\ $\text{Context-Drop}_{\text{fixed}}$ & 69.15\footnotesize{$\pm$0.91} & \textbf{43.12}\footnotesize{$\pm$0.74} \\
    \quad\;w/o KL loss & 68.23\footnotesize{$\pm$1.11} & 40.71\footnotesize{$\pm$1.78} \\
    w/\ $\text{Context-Drop}_{\text{dynamic}}$ & 69.53\footnotesize{$\pm$0.75} & 42.05\footnotesize{$\pm$0.31} \\
    \quad\;w/o KL loss  & 67.97\footnotesize{$\pm$0.53} & 41.44\footnotesize{$\pm$2.29} \\
    
    \midrule
    + global context & 67.99\footnotesize{$\pm$1.86} & 35.82\footnotesize{$\pm$1.11} \\
    w/\ R-Drop & 69.80\footnotesize{$\pm$1.14} & 37.88\footnotesize{$\pm$1.04} \\
    w/\ $\text{Context-Drop}_{\text{fixed}}$ & 69.07\footnotesize{$\pm$0.57} & 39.23\footnotesize{$\pm$0.73} \\
    w/\ $\text{Context-Drop}_{\text{dynamic}}$ & \underline{70.48}\footnotesize{$\pm$0.63} & 41.25\footnotesize{$\pm$1.76} \\
    
    \midrule
    + local \& global context & 69.09\footnotesize{$\pm$1.23} & 41.31\footnotesize{$\pm$1.51} \\
    w/\ R-Drop & 68.72\footnotesize{$\pm$1.04} & 40.75\footnotesize{$\pm$1.28} \\
    w/\ $\text{Context-Drop}_{\text{fixed}}$ & 69.28\footnotesize{$\pm$0.95} & 38.66\footnotesize{$\pm$0.77} \\
    w/\ $\text{Context-Drop}_{\text{dynamic}}$ & \textbf{70.82}\footnotesize{$\pm$1.33} & 41.50\footnotesize{$\pm$1.52} \\
    
    \bottomrule
    \end{tabular}
    \caption{\small{Positive F$_1$ on the \textbf{Test} sets of our AMC-A corpus and the AMI meeting corpus. All experiments fine-tune the pre-trained Chinese and English StructBERT models respectively. We compare the performance of different input methods (the single focus sentence or the focus sentence with its local/global context) and different training loss, including the standard CE loss by default, with R-Drop, and with the two variations of Context-Drop (Section~\ref{ssec:context}).}}
    \label{tab:metric}
\end{table}


\begin{table}[t!]
    \centering
    \begin{tabular}{llc}
    \toprule
    \textbf{Model Layers} & \textbf{Pooler Layer} & \textbf{AMC-A F$_1$} \\
    
    \midrule
    \multirow{2}{*}{StructBERT} & StructBERT & 67.84\footnotesize{$\pm$1.20} \\
    & RoBERTa & 68.36\footnotesize{$\pm$0.93} \\
    
    \midrule
    \multirow{2}{*}{RoBERTa} & RoBERTa & 66.87\footnotesize{$\pm$0.44} \\
    & StructBERT & 67.25\footnotesize{$\pm$0.93} \\
    
    \bottomrule
    \end{tabular}
    \caption{\small{Positive F$_1$ on the \textbf{Test} set of our AMC-A corpus, from fine-tuning pre-trained StructBERT and RoBERTa models and the hybrid model using Lightweight Model Ensemble (Section~\ref{ssec:ensemble}).}}
    \label{tab:ensemble}
    \vspace{-6mm}
\end{table}



% \begin{table}[t!]
%     \centering
%     \caption{The ablation experiment on the meeting corpus.}
%     \label{tab:ablation}
%     \begin{tabular}{lcc}
%     \toprule
%     \textbf{Input Method} & \textbf{AMC-A F1} & \textbf{AMI F1} \\
    
%     \midrule
%     $\text{Context-Drop}_{\text{fixed}}$ & 69.15\footnotesize{$\pm$0.91} & 43.12\footnotesize{$\pm$0.74} \\
%     \quad w/o\ KL divergence & 68.23\footnotesize{$\pm$1.11} & 40.71\footnotesize{$\pm$1.78} \\
    
%     \midrule
%     $\text{Context-Drop}_{\text{dynamic}}$ & 69.53\footnotesize{$\pm$0.75} & 42.05\footnotesize{$\pm$0.31} \\
%     \quad w/o\ KL divergence & 67.97\footnotesize{$\pm$0.53} & 41.44\footnotesize{$\pm$2.29} \\
    
%     \bottomrule
%     \end{tabular}
% \end{table}




\subsection{Lightweight Model Ensemble}
\label{ssec:ensemble}
During action item annotation, we observe that for inconsistent labels from three annotators, the majority voting results are usually correct despite the relatively low inter-annotator agreement, as the expert only modifies 5\%-10\% of the majority voting labels. Inspired by this observation, we explore model ensemble, a common approach for improving performance. In this work, we propose a Lightweight Model Ensemble approach, which improves accuracy while preserving inference latency.  Conventionally, we initialize each layer of a classification model with parameters from the same pre-trained model. In our Lightweight Model Ensemble approach, we initialize encoder layers of the action item detection model $\theta_C$ using parameters from one pre-trained model $\theta_A$ and initialize the pooler layer of $\theta_C$ using the pooler layer parameters from another pre-trained model $\theta_B$. We then fine-tune $\theta_C$ with the cross-entropy loss on the meeting corpus. In this way, we integrate knowledge from different pre-trained models efficiently, without increasing the overall number of parameters and slowing down inference.


%During the data annotation, we observed that the majority voting label were usually correct, despite the low consistency among different annotators. This inspired us to explore the model ensemble method, which is a common method to improve performance. Moreover, we do not want to increase the computation and damage the inference speed. For classification model, each layer is usually initialized from the same pre-trained model. However, different layers have different roles. For instance, the pooler layer after [CLS] token is closely related to the task instead of the general language knowledge. Thus, different layers could be initialized using parameters from different pre-trained models. Therefore, we propose a lightweight model ensemble approach.  We initialized model layers from one pre-trained model, and initialized pooler layer from another pre-trained model. In this way, we can integrate knowledge from different models to bring improvements. And we do not increase the number of parameters and inference speed.


% In addition, we propose a lightweight model ensemble approach. 
% In the pre-training fine-tuning paradigm, the encoder and classifier are usually initialized from the same pre-trained model for classification task. 
% While we found that the classifier initialized from another pre-trained model sometimes tends to achieve better performance. 
% This method can utilize different pre-trained models to bring improvements without increasing the number of parameters.







\vspace{-2mm}
\section{Experiments}
\label{sec:experiments}
\vspace{-2mm}
\subsection{Datasets and Metrics}
\label{ssec:dataset}
\vspace{-2mm}
We use both the AMI meeting corpus and our AMC-A corpus. 
We partition the AMI meeting corpus following the official scenario-only dataset partitioning\footnote{\url{https://groups.inf.ed.ac.uk/ami/corpus/datasets.shtml}}.
We partition AMC-A into train/dev/test sets with a ratio of 70:15:15, respectively. Considering the sparsity of positive samples, we report positive F$_1$ as the evaluation metric.

%%%%We use both the AMI meeting corpus and our Chinese meeting corpus (AMC-A).  For the AMI dataset, we apply the officially scenario-only dataset partitioning. For our Chinese meeting corpus, we partition the dataset in the ratio of 70:15:15. Considering the sparsity of positive samples, we use the F1 as an metric to evaluate the performance of the models.

\vspace{-4mm}
\subsection{Baseline and Implementation Details}
\label{ssec:implement}
\vspace{-2mm}

To evaluate our proposed methods on the AMC-A and AMI datasets, we use the following strong baseline pre-trained models, namely, BERT~\cite{DBLP:conf/naacl/DevlinCLT19}\footnote{\url{https://github.com/google-research/bert}}, RoBERTa~\cite{DBLP:journals/corr/abs-1907-11692}, StructBERT~\cite{wang2019structbert}\footnote{\url{https://modelscope.cn/models/damo/nlp\_structbert\_backbone\_base\_std}}, and Longformer~\cite{beltagy2020longformer} which provides efficient long-sequence modeling.  For RoBERTa, We use the pre-trained Chinese RoBERTa-wwm-ext model\cite{cui-etal-2020-revisiting}\footnote{\url{https://github.com/ymcui/Chinese-BERT-wwm}}.  For Longformer, we use the pre-trained Erlangshen-Longformer-110M\cite{fengshenbang}\footnote{\url{https://github.com/IDEA-CCNL/Fengshenbang-LM}} to model action item detection as a sequence labeling task and use a fixed sliding window with size 4096 and allow one sentence overlap. The sentence labeling task takes multiple sentences as input and outputs the probabilities for every sentence.
For BERT, StructBERT, and RoBERTa, we model action item detection as a sentence classification task and truncate input to 128 tokens. The sentence classification task takes a sentence as input and outputs the probabilities for the sentence.
We compare our Context-Drop approach to R-Drop~\cite{wu2021r}. R-Drop forces the predicted probability distribution of the same sample after two dropouts to be as close as possible.
We compare the performance of  R-Drop with sentence-level inputs and context-level inputs (Figure~\ref{fig:model}).

We use TensorFlow and PyTorch to implement all models.
All PLMs used are of BERT base size.
The batch size is 32 and the dropout rate is 0.3. For each experiment in this paper, we run 5 times with different random seeds; for each run, we conduct a grid search among $\{1e-5,2e-5\}$ learning rate and $\{2, 3\}$ epochs on the dev set. We then report the mean and standard deviation of the best results from 5 runs. The weight $\alpha$ of KL divergence loss is set to 4.0 for R-Drop and 1.0 for Context-Drop by optimizing positive F$_1$ on the dev set.
For each sentence, we use its preceding sentence and following sentence as \emph{local context}, and select the top-2 most similar sentences to this sentence as \emph{global context} (see Section~\ref{ssec:context} for details).
The probability to keep contextual sentences is 50\% for \textit{local} or \textit{global contexts}, and 70\% for \textit{local \& global contexts}.
Following setups in prior works, no sampling methods are applied.
%%%%To test the performance of our model on the meeting corpora, we use the following baseline models for comparison: \textbf{BERT}\footnote{https://github.com/google-research/bert} is proposed by Devlin et al.~\cite{DBLP:conf/naacl/DevlinCLT19}, which is the classical pre-trained model for text classification. \textbf{RoBERTa} is propose by Liu et al.~\cite{DBLP:journals/corr/abs-1907-11692}, which is a strong pre-trained baseline model, and we used the pre-training parameters of Chinese RoBERTa-wwm-ext-large model\footnote{https://github.com/ymcui/Chinese-BERT-wwm} \cite{cui-etal-2020-revisiting}. \textbf{StructBERT}\footnote{https://modelscope.cn/models/damo/nlp\_structbert\_backbone\_base\_std} is proposed by Wang et al.~\cite{wang2019structbert}, which is our base pre-trained model.  We observed its advantages in the spoken language domain because of the pre-training task. \textbf{Longformer}~\cite{beltagy2020longformer} is employed for long sequence modeling, and we used the pre-training parameters of Erlangshen model\footnote{https://github.com/IDEA-CCNL/Fengshenbang-LM} \cite{fengshenbang}. \textbf{R-Drop}~\cite{wu2021r}  make the predicted probability distribution of the same sample after two dropouts as close as possible. We set different granularity of input for comparison, including sentence-level input and context-level input.

% \textbf{Context-Drop-Aug} is the same as method of context-drop method, except driving the probability distributions of the two inputs as close as possible. It can be regared as a data augmentation method. 


%%%%\subsection{Implementation Details}
%%%%\label{ssec:implement}


\vspace{-4mm}
\subsection{Results and Analysis}
\label{ssec:result}
\vspace{-2mm}
As shown in Table~\ref{tab:baseline}, we compare different PLMs with different modeling tasks. When modeling action item detection as a sentence classification task, StructBERT outperforms BERT with a remarkable gain of \textbf{+3.08} on positive F$_1$. The word structural pre-training objective of StructBERT reconstructs tokens in the correct order from the shuffled trigrams. This could improve its robustness to disordered sentences, which is quite common in spoken languages, and in turn improve its performance of meeting action item detection. We formulate action item detection as a sequence labeling task to exploit the advantage of Longformer in long-sequence modeling. However, we only observe limited improvement from Longformer over BERT, 0.59 gain on positive F$_1$. Therefore, we formulate action item detection as a sentence classification task and use StructBERT as the pre-trained model for evaluating Context-Drop.

As shown in Table~\ref{tab:metric}, based on the baseline StructBERT, we compare various contrastive learning methods using different contexts (Figure~\ref{fig:model}). On AMC-A, when not using contrastive learning methods, i.e., no w/ R-Drop nor w/ Context-Drop, the baseline using both local and global context performs the best (69.09), followed by the baseline using the local context (68.50). We observe the same trend on AMI.
This indicates that adjacent contextual sentences do provide useful information. Global context provides complementary information and a combination of global and local context achieves further improvement. 
% which is why this approach has been used in previous papers.
% With the support of both local and global context, the baseline model achieves the best performance.
On AMC-A, when using different contrastive learning methods, the configuration of using the focus sentence and local \& global context as input with $\text{Context-Drop}_{\text{dynamic}}$ achieves the best performance (70.82), outperforming the baseline using the sentence as input without contrastive learning (67.84) by \textbf{+2.98} absolute on positive F$_1$, and also outperforming R-Drop (68.72) by \textbf{+2.10} absolute gain. 
On AMI, sentence+local context w/ $\text{Context-Drop}_{\text{fixed}}$ (43.12) also outperforms the baseline sentence input (38.67) and R-Drop (42.72).
These results confirm our hypothesis that Context-Drop could help the model to focus more on the current sentence, exploit relevant information in context and be less distracted by irrelevant information.  Moreover, a reduction in the standard deviations shows that Context-Drop improves the stability and robustness of the model. For different contexts, $\text{Context-Drop}_{\text{dynamic}}$ outperforms $\text{Context-Drop}_{\text{fixed}}$ in most cases, which suggests that the flexible and dynamic contrastive learning method can achieve better performance.

We also conduct ablation analysis on Context-Drop, as in the second group of Table~\ref{tab:metric}.  Without the regularization loss of KL divergence (denoted KL loss), Context-Drop can be regarded as a data augmentation method using fixed or dynamically selected context. On AMC-A and AMI, for both $\text{Context-Drop}_{\text{fixed}}$ and  $\text{Context-Drop}_{\text{dynamic}}$, w/o KL loss degrades the performance, which indicates contrastive learning is important for gains. 
With the regularization loss, the model could better focus on the current sentence and be less distracted by irrelevant information in context.

As shown in Table~\ref{tab:ensemble}, we compare the performance of applying  Lightweight Model Ensemble integrating various pre-trained models using the sentence input. StructBERT encoder with RoBERTa pooler layer parameters achieves \textbf{+0.52} absolute gain and RoBERTa encoder with StructBERT pooler layer parameters achieves \textbf{+0.38} absolute gain. These results show that Lightweight Model Ensemble could integrate knowledge from different models and achieve better performance without increasing the overall number of parameters.

%%%%As shown in Table~\ref{tab:baseline}, we compare various modeling tasks and pre-trained models. When formulating action item detection as a sequence labeling task, we observe limited improvement from a longer range of contexts. Therefore, we still formulate action item detection as a sentence classification task and explore methods to utilize context within the max length effectively. In addition, StructBERT shows a noticeable advantage in the spoken language domain.StructBERT uses word structural objective as a pre-training task rather than the masked language model, which expects the model can reconstruct tokens in the correct order from the shuffled tri-grams.This task may strengthen the ability to understand the disordered sentence, which is very common in the spoken language domain.Therefore, we selected StructBERT as our base model and explored some efficient methods to utilize context.

%%%%As shown in Table~\ref{tab:metric}, based on the StructBERT model, we compare various contrastive learning methods for different contexts.Without the contrastive learning method, the baseline with both local and global context performs the best, followed by the baseline with the local context. This indicates that adjacent contextual sentences do provide auxiliary information to improve performance.
% which is why this approach has been used in previous papers.
%%%%And the global contexts show their irreplaceable role to provide complementary auxiliary information. 
% With the support of both local and global context, the baseline model achieves the best performance.
%%%%In different contrastive learning methods, our context-drop method basically outperforms the baseline and the R-Drop method. This confirms our hypothesis that the context-drop method could help the model to focus more on the current sentence. By exploiting relevant information and being less distracted by irrelevant information in context, the context-drop method achieves the best performance. Moreover, the reduction of the standard deviation shows an improvement in the model's robustness. For different contexts, the context-drop (dynamic) method outperforms context-drop (fixed) method in the vast majority of cases.  This suggests that this flexible and dynamic contrastive learning method can achieve better performance in a variety of situations.

%%%%In addition, we conduct the ablation experiment as shown in Table~\ref{tab:metric}. We remove the regularization loss of KL divergence to verify whether our contrastive learning is necessary. Without the KL loss, it can be regarded as a data augmentation method using fixed or dynamic selected context.  The loss of performance reflects the necessity of contrastive learning. With the regularization loss, the model could better focus on the current sentence and be less distracted by irrelevant information in context.

%%%%As shown in Table~\ref{tab:ensemble}, we compare the lightweight model ensemble based on various pre-trained models in sentence input. For both StructBERT and RoBERTa model, the performance is improved after replacing the pooler layer parameters from another pre-trained model.  This illustrates that this lightweight model ensemble approach could integrate model knowledge to achieve better performance without increasing the overall number of parameters.


\vspace{-3mm}
\section{Conclusion}
\label{sec:conclusion}
\vspace{-2mm}
We construct and make available the first Chinese meeting corpus with action item annotations, to alleviate the scarcity of resources and prompt research on meeting action item detection. We propose Context-Drop to exploit both local and global contexts with regularization. On both our meeting corpus and English AMI meeting corpus, Context-Drop improves the accuracy and robustness of action item detection. We also propose Lightweight Model Ensemble and achieve improvement. In future work, we plan to refine Lightweight Model Ensemble and investigate its efficacy on other tasks as well as combining Context-Drop and Lightweight Model Ensemble.
% Given that the majority voting labels are pretty reliable






\vfill\pagebreak

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{main}

\end{document}
