\begin{table*}[thb]
\begin{center}
\scalebox{0.9}{
\begin{tabular}{lcccccccccc}
% \toprule
 & \multicolumn{2}{c}{SituatedQA} & \multicolumn{2}{c}{MC-TACO} & \multicolumn{2}{c}{TimeDIAL} & \multicolumn{2}{c}{TimeDIAL-0} & &  \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule{8-9}
Model & F1 & EM & F1 & EM & 1-Best & 2-Best & 1-Best & 2-Best & Overall & Overall-0  \\
\midrule
% \textsc{ALICE} \cite{pereira-etal-2020-adversarial} & -- & -- & 78.64 & 58.02 & -- & -- & -- \\
% \textsc{DeBERTa} \cite{he-etal-2020-deberta} & -- & -- & 82.92 & 63.81 & -- & -- & -- \\ 
% \textsc{BART} \cite{lewis-etal-2020-bart} & -- & 18.3 & -- & -- & -- & -- \\
% \textsc{ALBERT}  \cite{lan2019albert}  & -- & -- & -- & -- & -- & 76.10 & -- \\
\textsc{Best Reported} & -- & 18.53 & 82.92 & 63.81 & -- &  76.10 & -- & 50.60 & 52.74 & 44.31 \\
\midrule
\textsc{T5} & 25.75 & 19.78 & 84.00 & 64.56 & 99.91 & \textbf{84.50} & 90.85 & 37.59 & 56.28 & 40.64 \\
\textsc{T5-LM} & 25.38 & 19.63 & 81.99 & 59.83 & 99.91 & 80.60 & 86.87 & 32.16 & 53.35 & 37.21  \\
\textsc{SSM} & 29.92 & 23.12 & 85.88 & 68.39 & 99.73 & 84.06 & 96.74 & 67.21 & 58.52 & 52.91 \\
\midrule
\textsc{Entities} & 29.42 & 22.82 & 85.47 & 66.59 & 99.91 & 83.06 & 97.64 & 67.93 & 57.49 & 52.45 \\
\textsc{TSM} & 27.42 & 21.18 & 84.89 & 65.92 & 99.91 & 83.88 & \textbf{99.82} & \textbf{77.54} & 56.99 & 54.88  \\
\textsc{TSM+SSM} & 29.33 & 22.76 & \textbf{86.20} & 67.64 & \textbf{100.0} & 83.78 & 98.19 & 73.10 & 58.03 & 54.5  \\
\textsc{Entities+TSM} & \textbf{30.78} & \textbf{24.60} & 85.32 & \textbf{68.47} & 99.91 & 84.24 & 98.91 & 76.09 & \textbf{59.09} & \textbf{56.39} \\
\bottomrule
\end{tabular}
}
\caption{Aggregate metrics across the three datasets. \textit{Overall} performance is the simple arithmetic average of the harder metric for each approach (EM, EM, 2B); \textit{Overall-0} uses TimeDIAL-0 instead of TimeDIAL. The second section contains our runs of earlier models; the Best Reported uses best known published numbers. The third section represents our models. Note that all models (in the second and third sections) are based on T5-1.1-XXL models. Best Reported results are ALBERT \citep{lan2019albert} from \citet{abramson2022application} for TimeDIAL, BART results from \citet{zhang-choi-2021-situatedqa}, and DeBERTa \citep{he-etal-2020-deberta} results from the leaderboard for MC-TACO. Note that F1 for MC-TACO is based on the precision/recall over answers and EM is based on labeling every answer for a question correctly, while the F1 for SituatedQA is based on the token-level F1 of the answer span.}
\label{tab:overall}

\end{center}
\end{table*}