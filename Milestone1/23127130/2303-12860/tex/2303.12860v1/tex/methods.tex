\section{Methods}
Following \citet{roberts-etal-2020-much}, we utilize intermediate training to improve pretrained models' generalization to downstream tasks. 
%
All models are initialized from the encoder-decoder T5-1.1-XXL language model \citep{raffel-etal-2020-exploring}, which was shown to have the best closed-book QA performance in \citet{roberts-etal-2020-much}. 

\subsection{Background: Salient Span Masking}
Salient Span Masking (SSM) was first introduced by \citet{guu-etal-2020-realm} and is designed to specifically mask named entities and dates or \textit{salient spans}.
These salient spans are automatically identified from English Wikipedia using a Named Entity Recognition model to find entities as well as a regular expression to find dates. The authors mask one such span per sentence during training: the model must maximize the probability of the masked entity or date given the corrupted input sentence. \citet{guu-etal-2020-realm} designed the task to improve downstream performance on tasks requiring world knowledge in order to improve their retrieval-augmented model's ability to use retrieved texts. \citet{roberts-etal-2020-much} then

\subsection{Proposed: Temporal Span Masking}
\label{sec:tsm}
Inspired from the success of SSM, TSM is designed to address problems requiring temporal knowledge.
To create training examples for TSM, we automatically identify temporal expressions in a large corpus using \textsc{sutime} \cite{chang-manning-2012-sutime}, a rule-based temporal parser, that identifies temporal expressions from raw text. 
Given an input sentence, \textsc{sutime} is built to identify expressions of the following four types: \textbf{Time} which indicates a particular point in time such as \emph{next Monday}, \textbf{Duration} such as \emph{3 days}, \textbf{Set} which indicates periodic set of time that occur with some frequency such as \emph{every 4 years} and \textbf{Date} such as \emph{January 1}.

We run \textsc{sutime} on all of English Wikipedia\footnote{We use the 2020 snapshot of English Wikipedia (TFDS datadump \emph{wikipedia20201201en})}.
Specifically, we divide the articles into sentences, and apply \textsc{sutime}\footnote{\url{https://github.com/FraBle/python-sutime}} on each sentence.
For our TSM training data, we ensure that exactly one temporal span is masked per example.
So, if a sentence contains four temporal spans, we create four training examples with exactly one temporal span masked per example.
Details of the temporal distribution are in  \autoref{tab:tsmdata}. 
Each example is created by masking the tokens belonging to the temporal expression, as shown in  \autoref{fig:tsm_overview}, corrupting the input sentence by replacing the span with (\textunderscore X\textunderscore) and having the model predict the masked tokens.
The training objective is to maximize the probability of the target span given the corrupted input sentence, similar to T5's span corruption training objective \citep{raffel-etal-2020-exploring} and the SSM training objective \citep{roberts-etal-2020-much}.


\subsection{Model Variants}
All newly reported results are based on T5-1.1-XXL models. Proposed models are named for their intermediate training objective: TSM is trained solely on the masked temporal spans described above; SSM is trained solely on the training objective described in \citet{roberts-etal-2020-much}.\footnote{We note that the SSM-spans are derived from the 2018 snapshot of English Wikipedia (same as \citet{guu-etal-2020-realm}) while TSM-spans from the 2020 snapshot.}

We also investigate a version of the SSM pretraining data that only uses the \textit{named entity} spans identified by the NER model; in other words, all of the \textit{date} spans identified by the regular expression are removed, but the task is otherwise the same. We call this objective \textsc{Entities}. Finally, we compare against models that are trained proportional mixtures of both TSM+SSM and TSM+\textsc{Entities}. 

For baseline models, we use the same pretrained model (T5-1.1-XXL) with no intermediate training (T5), as well as a T5-1.1-XXL model which has been trained for an additional 100K steps on the prefix LM task (T5-LM; \citealt{lester-etal-2021-power}).

\subsection{Downstream Temporal Tasks}
We evaluate on three downstream temporal tasks for evaluation, finetuning a model on each task separately. 
Below, we briefly describe the tasks and datasets, with additional training details in \autoref{sec:training_appendix}.

\paragraph{MC-TACO} \citet{zhou-etal-2019-going} release a human-annotated dataset to measure temporal commonsense understanding.
It consists of 13k tuples of \emph{(sentence, question, candidate answer)} covering five types of common sense problems such as event frequency, event duration, event ordering, stationarity and event typical time.
Given a sentence context, a temporal question about that context, and a possible commonsense answer, the task is to determine whether the provided answer is reasonable for the given context.
For instance, for the event of \textit{taking a shower} with four possible answer choices \textit{five minutes, fifteen minutes, fifteen hours}, and \textit{fifteen years}, the first two are plausible and will have the \textit{yes} label while the latter two choices would be \textit{no}.
There is no training data released for this task, so we finetune the model on the provided validation set and evaluate on the test set.

\paragraph{TimeDIAL} \citet{qin-etal-2021-timedial} release a human-annotated multi-turn dialog dataset for measuring temporal commonsense understanding in a dialog setting. 
The dataset comprises of challenge test set with 1.1k dialog instances derived from the Daily Dialog dataset described in \citet{qin-etal-2021-timedial}.
TimeDIAL dialogs  mostly comprise of common sense instances where the answers generally consist of one temporal span. 
For instance, in the following dialog ``I'll just be a minute'., the span ``a minute'' may be masked out and the model is required to predict the masked span based on the dialog turns.
Given a dialog with a temporal expression masked out, the task is to correctly predict which two of the four provided answers are valid in the given context.
We report results without finetuning (styled TimeDIAL-0) as well as results from finetuning the model on the Daily Dialog dataset.
 
\paragraph{SituatedQA} \citet{zhang-choi-2021-situatedqa} release an open-domain QA dataset derived from existing question answering datasets with additional annotations that resolve temporal and geographic ambiguities.
Each example consists of a disambiguated question: for instance, ``Which COVID-19 vaccines have been authorized in the US [as of 2020]?'' or ``What was the first COVID-19 vaccine to be authorized [in the US]?''. 
For the purpose of this work, we focus on the temporal questions. These consist of 9K additional questions, with a training set of about 4.5K questions. 
We finetune on the training set and evaluate on the test set.
 
\subsection{Natural Questions}
While the focus of our method is improving temporal question answering performance, we also wanted to ensure that our method does not degrade performance on non-temporal question answering tasks. Thus, we also evaluate our model variants on Natural Questions \cite{nq}, using the ``open'' variant popularized by \citet{lee-etal-2019-latent}. These examples discard those questions without short answers or that require an evidence document to answer. These consist of about 87K questions for training and an additional 3.6k questions for validation, which we use for evaluation.