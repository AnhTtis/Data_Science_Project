\section{Further Discussion}
\label{app:discussion_appendix}
\subsection{Other Experiments}
We previously experimented with the T5~Large and T5-XL models, as well the as 1.0 versions of the T5 models that were first described in \citet{raffel-etal-2020-exploring}. In general, larger models and the 1.1 versions worked better. While we refrain from reporting results due to inconsistent setups, in general the smaller models were notably worse, such that distinguishing between two similar setups (such as TSM and SSM) was difficult on many tasks. While we know of no work testing salient span masking on extremely large models, it is possible it would actually show a larger impact, based on this trend. While left-to-right decoding serves as an awkward fit for the paradigm, if our hypothesis on the reason why SSM works is correct, then it should not prove to be a substantial hurdle. See also below for more discussion on said hypothesis.

\input{tab/tsmdata}
\input{tab/ssmdata}
\input{tab/ssmdatabreakdownsutime}

\subsection{Span Distribution vs. Text Distribution}
Our hypothesis for SSM's effectiveness is due to it oversampling difficult sentences. This is based on the performance gain for the \textsc{Entities} intermediate training as well as the number of temporal spans that occur in the SSM training data.
\autoref{tab:ssmdatatsmsutime} shows the results of \textsc{SUTIME} parser on the SSM training data, and as we can see, significant portion of the SSM data (45\%) has temporal spans.
 \autoref{tab:ssmdatatsmsutime} shows the breakdown of different temporal spans for each SSM salient span type.
 We leave an exact test of this for future work, but if this is true, then we might expect left-to-right decoding models to also benefit from the sampling procedure of SSM, even though they do not use a masked language modeling paradigm for training. 


\subsection{Span Types in Table \ref{tab:tsmdata}}\label{sec:table2}
Mapping MC-TACO's span types is somewhat helpful to see the performance breakdown. Note that these are now based on individual answers, while MC-TACO's strict match metric is based on correctly labeling all answers for a given question.

\paragraph{Entities} While MC-TACO is a common sense dataset, it frequently relies on reasoning about relatively complicated phenomena. While it is common sense to know that a dynasty does not rule in China for only a few minutes, it is still required to know more about China and dynasties to answer the question correctly. TimeDIAL on the other hand is normally ordinary conversations that are not very entity-centric. SituatedQA is derived from Natural Questions, which is an information seeking dataset that frequently features entities.

\paragraph{Duration} MC-TACO's event duration maps well to the Duration type in \textsc{sutime}. While there may be some SituatedQA examples that include durations, we do not filter for them.

\paragraph{Set} MC-TACO's Frequency type asks question of the "How often" nature while sets frequently have answer types of that nature e.g., "every third sunday", but this is not a perfect mapping.

\paragraph{Date} MC-TACO's typical time sometimes includes dates, but it is less likely to be a specific date and more likely to be a generic date like Sunday, rather than a specific knowledge-based date. SituatedQA questions always include dates that decontextualize Natural Questions. 

\paragraph{Time} MC-TACO's typical time sometimes corresponds with times as well, but they are again less likely to be specific. Unfortunately, Date and Time are not separated in MC-TACO. 

\paragraph{Other MC-TACO Types} Note that we did not include the ``Stationarity'' or the ``Event Ordering'' MC-TACO types in the breakdown, as they do not correspond well to any \textsc{sutime} type. 

