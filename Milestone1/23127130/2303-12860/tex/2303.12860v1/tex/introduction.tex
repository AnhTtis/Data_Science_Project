\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{fig/tsm.pdf}
    \caption{Overview of the TSM and SSM tasks: While SSM \cite{guu-etal-2020-realm} identifies named entities and dates as salient spans, for TSM we use \textsc{sutime} parser that captures other temporal expressions such as durations and intervals.
    \textsc{sutime} is first applied on raw sentences to identify any temporal expressions. Next, training data for TSM  is created from those sentences which have at least one temporal expression identified. In SSM, the masked spans comprise of named entity or date spans. Input to each task consists of the sentence with the selected span dropped out where the model is trained to predict the dropped tokens.
    }
    \label{fig:tsm_overview} 
\end{figure*}

\section{Introduction}
Salient Span Masking (SSM), first introduced by \citet{guu-etal-2020-realm} for retrieval-based language modeling, has shown performance gains for closed-book question answering (CBQA) \cite{roberts-etal-2020-much,ye2020studying}. 
SSM is a form of intermediate pretraining \cite{ye-etal-2021-influence}, where a pretrained model such as a BERT \cite{devlin-etal-2019-bert} or T5 \cite{raffel-etal-2020-exploring} is trained further before task-specific finetuning, generally on more specialized data that does not require expensive annotations.
Specifically, SSM uses the masked language modeling objective but only masks named entities and dates in sentences from English Wikipedia articles; these ``salient'' spans likely contain more facts, so the language model
must memorize more facts in order to do the task successfully \cite{petroni-etal-2019-language}.
The authors use a named entity recognition model to identify entity spans and a regular expression to identify date spans. 
While this works well for knowledge intensive downstream tasks,
such as entity-centric question answering, it remains unclear whether it is helpful for tasks
that are less aligned with the data, such as common sense  or temporal reasoning.
Moreover, is it possible to select spans that are more related to a downstream task in order to get further performance gains?

In this work, we investigate SSM for tasks that require understanding \emph{temporal} expressions.
While SSM does include dates, the tasks we investigate include other complex temporal expressions such as durations and intervals. 
To that end, we introduce Temporal Span Masking (TSM): an intermediate pretraining strategy for predicting spans that are likely temporal expressions
(\autoref{fig:tsm_overview}). 
Similar to SSM, TSM is automatically generated from English Wikipedia articles.
We compare models trained on TSM and SSM on three  temporal tasks, namely MC-TACO \cite{zhou-etal-2019-going}, TimeDIAL \cite{qin-etal-2021-timedial} and SituatedQA \cite{zhang-choi-2021-situatedqa}, and for one general-purpose question answering (QA) task of Natural Questions (NQ) \cite{nq}.
We summarize our contributions as follows:
\begin{itemize}
    \item We propose TSM Intermediate Training, which automatically selects temporal spans for masking.
    \item The new best reported results on the three temporal tasks: the best average performance is from a mixture of TSM and SSM. This mixture also does slightly better than SSM on Natural Questions. 
    \item Experiments investigating the role of different TSM and SSM span types, showing entity spans alone are helpful, which implies that difficult examples help improve representations of the unmasked spans as well.
\end{itemize}

