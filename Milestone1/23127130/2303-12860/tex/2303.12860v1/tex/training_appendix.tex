\section{Training Details}
\label{sec:training_appendix}
All models are initialized from the public T5-1.1-XXL checkpoints.\footnote{https://github.com/google-research/text-to-text-transfer-transformer}

\subsection{Intermediate Training}
\label{sec:intermediate}
We use 256 Cloud TPU v3 cores for the intermediate training procedure using a batch size of 2048 and the fixed default learning rate of 0.001. Training generally proceeds for one epoch, which is between 100-150K steps depending on the precise task, though we used early stopping for the TSM and TSM+SSM models based on MC-TACO performance, as they seem to overfit.

\subsection{Finetuning}
\label{sec:finetuning}
We use 64 Cloud TPU v3 cores for finetuning and inference on all tasks. For MC-TACO, Natural Questions, and SituatedQA, we use the same fixed learning rate of 0.001 and train for 10K steps with batch size 128. For TimeDIAL, we attempt to follow their training setup more closely, and use a lower learning rate (0.0001) and train for up to 100K steps, still with batch size 128 and use early stopping on the validation set to inform when to stop. For most of the models that had intermediate training, the early stopping point was for 10K steps. However, for the basic T5 model, it was after 20K steps (improving from ($82.97 \rightarrow 84.51$)), implying that it can overcome its lack of intermediate training with additional finetuning data. Note that in the zero-shot variant, no finetuning is done.
