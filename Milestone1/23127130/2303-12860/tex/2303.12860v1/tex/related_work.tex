\section{Related Work}
\paragraph{Span Masking and Intermediate Training}
Salient Span Masking \citep{guu-etal-2020-realm} came out of a series of efforts like SpanBERT \citep{joshi-etal-2020-spanbert} to select more difficult examples to improve models memorization of the text.

Most similar to us, \citet{ye-etal-2021-influence} explore a similar paradigm of choosing better spans for a downstream task (e.g., entity linking or relation extraction) where they experiment with both a heuristic masking policy similar to SSM and also a learned masking policy.
They similarly find that masking spans that resemble  downstream tasks improve performance, however, they also note that learned masking policies suffer from overfitting.
\citet{yang-etal-2020-improving} and \citet{zhou-etal-2020-temporal} explore intermediate training by designing heuristics to identify sentences containing temporal expressions and then adding additional tasks and losses, rather than using span masking. TSM differs in more closely resembling the pretraining task.

\citet{levine2021pmimasking} use pointwise mutual information to jointly mask highly correlated spans to avoid the model relying on local signals but rather learning from the broad context.
They find this leads to faster and better pretraining.
In the future, it might be interesting to see how PMI-spans can combine with knowledge-oriented span techniques such as SSM, TSM, and whether they can help in the intermediate training paradigm.

\paragraph{Temporal Understanding}
There has been a surge of interest in probing models' temporal awareness. While we evaluate on a three tasks, it is far from an exhaustive evaluation and we leave further evaluations of our method to future work.

Recently,
\citet{thukral-etal-2021-probing} and \citet{vashishtha-etal-2020-temporal} construct NLI datasets to test whether pretrained models understand certain types of common sense temporal expressions, such as containment. 
%
To probe common sense, we use TimeDIAL \citep{qin-etal-2021-timedial} for its naturalistic dialogues as well as MC-TACO \citep{zhou-etal-2020-temporal}, which uses a diverse set of situations and temporal expressions. 

For factual questions, 
open-response temporal questions are closely aligned with our work (e.g., TimeQA; \citealt{chen2021dataset}; TempLAMA; \citealt{dhingra2022time}). 
All of TempLAMA, TimeQA, and SituatedQA \citep{zhang-choi-2021-situatedqa} rely primarily on the year as the main temporal expression being tested, where facts are scoped to the provided years. To probe temporally scoped facts, we use SituatedQA for its more naturalistic questions.
