\section{Dataset}
\label{sec:dataset}

In this section, we introduce the proposed VideoXum dataset. The dataset is reannotated by a limited number of workers, including 14,001 long videos with 140,010 video and text summaries pairs. We describe the process of dataset collection and annotation strategy. We also provide several quantitative and qualitative analyses of the proposed dataset. Finally, we compare the VideoXum with existing single-modal video summarization datasets.


\subsection{Dataset Curation}


\noindent \textbf{Dataset Collection}.
The VideoXum dataset is built based on ActivityNet Captions~\cite{krishna2017dense}, a high-quality public video captioning benchmark. There are three primary reasons to build upon ActivityNet Captions.
First, the dataset contains 20K real-life Youtube videos with diverse content, in terms of rich topics, different photographic devices, multiple view angles, and so on. Each video in this dataset is annotated with a series of dense temporal segments, and each segment corresponds to a concrete sentence description, offering diverse patterns essential for video understanding and generation tasks.
Second, the dataset contains numerous lengthy videos in Fig.~\ref{fig:video_len}, which introduces more challenges to the cross-modal video summarization task.
Third, as described in Section \ref{sec:intro}, the well-annotated sentence narratives are natural summaries of the source videos.
Therefore, the content and length of videos in the ActivityNet Captions dataset largely meet our requirements and provide an ideal foundation for constructing our cross-modal video summarization benchmark. To maintain our focus on long videos, we filter out videos shorter than 10 seconds. 

\noindent \textbf{Dataset Reannotation}.
For each video, we expect the total length of its video summary to be bound to 15\% of the source video, along with a semantically aligned text summary. ActivityNet Captions~\cite{krishna2017dense}  already contains video captions with temporal segments for long videos. Therefore, we concatenate the caption sentences as a text summary for the long source video. 
However, the annotated video spans, which cover an average of 94.6\% of the source videos, are too long to be regarded as a video summary by themselves since video summaries need to be much more concise. Therefore, we reannotate the video spans and obtain an abridged version of video segments for better aligning with the sentence captions.

Due to the inherently subjective nature of summarizing a long video (this conclusion is also reflected by the human performance on V2V-SUM in TABLE \ref{tab:videoxum_table}), it is hard to obtain perfect ground truth labels for this task.
Following previous works~\cite{song2015tvsum,lei2021detecting,sun2014ranking}, we required ten different workers to annotate video summary spans corresponding to a same text description. For each given caption, we obtained ten shortened spans. During the evaluation, we compared the prediction with all ten annotations and then obtain the average score for each video.
To further ensure consistent annotations, we hired 40 workers in total to reannotate all 140,010 summarised video spans over a period of two months. On average, each worker reannotated about 15 videos per hour. To maintain high-quality annotations, we regularly reviewed the reannotated video spans and provided feedback to workers. Every 24 hours, we randomly evaluated 15\% of an annotation batch for accuracy. If the acceptance rate of the sampled annotations reached 90\%, we considered the entire annotation batch as passed; otherwise, we asked workers to reannotate the batch.

This reannotation pipeline aims to obtain an abridged version (ideally bounded to 15\%) of videos for better aligning with the sentence captions. Therefore, we filter the initial ActivityNet dataset using the length compression ratio of video with 20\% as the threshold. The video length compression ratio is calculated as $\text{Ratio}(S, V) = \frac{|S|}
{|C|}$, where $|S|$ denotes the length of summary, $|C|$ denotes the length of source video. Finally, 14,001 long videos remain in our dataset.


% Figure 2
\input{figures/data_stat}


\noindent \textbf{Dataset Split}.
We split the dataset into training, validation, and test sets. The split strategy also guarantees that all three data splits preserve the same distribution of video length. In particular, the dataset is divided into 8,000, 2,001, and 4,000 videos in the training, validation, and test sets, respectively.

\vspace{-2mm}
\subsection{Dataset Statistics}
Fig.~\ref{fig:dataset_hist} presents the statistical information of the VideoXum dataset. As shown in Fig.~\ref{fig:video_len}, it shows that the length of the videos ranges from 10 to 755 seconds, with 99.9\% of them under 300 seconds. The average length is 124.2 seconds, and the median length is 121.6 seconds.
For the video summarization task, most video summary lengths are shorter than 15\% of the source video length. Fig.~\ref{fig:comp_ratio} shows that the average length compression ratio is 13.6\%, with a median ratio of 13.7\%, and a maximum ratio of 20\%.
Moreover, we investigate the distribution of the center timestamps of important clips. All the center timestamps are normalized to fall within the range of $[0, 1.0]$ according to the original video length. Fig.~\ref{fig:norm_center_hist} suggests that the important clips are generally uniformly distributed throughout the video, with a mild peak at the beginning. Therefore, the VideoXum dataset does not suffer from temporal bias issues~\cite{lei2021detecting}.
For the text summarization task, each video is summarized into a narrative paragraph that describes multiple events. On average, each narrative paragraph contains 49.9 words. Fig.~\ref{fig:tsum_len} indicates that most (98\%) text summaries are shorter than 128 words, which guides us to set the maximum text generation length as 128.



% Table 2
\input{tables/human_eval}

\subsection{Comparison with Existing Single-modal Video Summarization Datasets}
In TABLE~\ref{tab:datasets}, we compare the proposed VideoXum dataset with existing {\it single-modal} video-to-video and video-to-text\footnote{In this paper, we regard the video captioning task as video-to-text summarization task} summarization datasets.
The main difference between VideoXum and other existing datasets is that VideoXum contains aligned human-annotated video and text summaries, while others only have single-modal summaries for source videos.
Compared with the existing video summarization benchmarks (e.g., SumMe~\cite{gygli2014creating} and TVSum~\cite{song2015tvsum}), the amount of data in the VideoXum dataset is significantly larger. In addition, VideoXum contains open-domain videos with more diverse scenarios than other datasets. To ensure the quality of human annotation, we evaluate annotated data using a leave-one-out strategy~\cite{otani2019rethinking}. TABLE~\ref{tab:human_eval} shows that our annotation quality is comparable with existing benchmarks.