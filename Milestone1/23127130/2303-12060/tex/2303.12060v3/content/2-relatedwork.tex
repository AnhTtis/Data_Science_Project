\section{Related Work}

\subsection{Video Summarization}
 Video summarization datasets (\eg,  SumMe~\cite{gygli2014creating}, TVSum~\cite{song2015tvsum}, and YouTube~\cite{de2011vsumm}) have enabled the development of state-of-the-art video summarization methods~\cite{narasimhan2021clip,zhang2016video,zhou2018deep,park2020sumgraph,saquil2021multiple}. Among these models, vsLSTM \cite{zhang2016video} first attempted to learn frame importance by modeling the temporal dependency among frames using LSTM \cite{graves2012long} units. The model can be combined with a determinantal point process (DPP) to improve the diversity of generated video summary. Following vsLSTM, several other approaches were proposed to model the temporal dependency, e.g., H-RNN~\cite{zhao2017hierarchical}, HSA-RNN\cite{zhao2018hsa}, DASP~\cite{ji2020deep}. Another solution models the spatiotemporal structure of the video to learn frame importance, such as MerryGoRoundNet~\cite{lal2019online}, and CRSum \cite{yuan2019spatiotemporal}. Adversarial learning-based methods~\cite{fu2019attentive,zhang2019dtr} can also perform well. Recently, multimodal-based video summarization method~\cite{narasimhan2021clip} leverages generated text summaries to promote predictions of frame-level scores for video summaries. Different from multimodal-based video summarization, the cross-modal video summarization task requires simultaneously producing both visual and textual summaries from a source video, which goes beyond generic video summarization. Moreover, it ensures semantic coherence between these two modalities.


\vspace{-2mm}
\subsection{Video Captioning}
% Video Captioning aims to automatically generate short descriptions for a video by understanding the action and event in a video, which can help retrieve videos efficiently through text queries.
Video Captioning aims to describe a video with text, which requires the capability of understanding actions and events.
Existing benchmarks (e.g., MSVD~\cite{chen2011collecting}, YouCook~\cite{zhou2018towards}, MSR-VTT~\cite{xu2016msr}, and ActivityNet Captions \cite{krishna2017dense}) have helped to promote the ability of language models to generate reasonable captions for video. Benefiting from these human-annotated datasets, many novel approaches are proposed. 
Attention-based methods~\cite{yao2015describing,yan2019stat} employ attention mechanisms to help the model in associating relevant frames since not every frame in a video is equally important.
%Graph-based video captioning~\cite{zhang2019object,zhang2020video} leverage inter-frame and intra-frame object interactions in both time and space degree. 
DENSE~\cite{krishna2017dense} is an early attempt at dense video captioning, which detects events with an event proposal module and associates them with LSTM. Wang et al.~\cite{wang2018bidirectional} develop a bidirectional process to encode context for detecting event proposals. Moreover, Masked Transformer~\cite{zhou2018end} proposes a differentiable masking scheme to ensure consistency between event proposal and caption generation modules.




\vspace{-2mm}
\subsection{Multimodal Pretraining}
Large language models (LLMs) \cite{brown2020language,devlin2018bert,lewis2019bart,raffel2020exploring} have revolutionized NLP research in recent years. Following the large-scale pretraining models in the field of NLP, numerous works \cite{ju2022prompting,kim2021vilt,wang2020minilm,xue2021probing,zhang2021vinvl,hu2022promptcap} on exploring the combination of vision and language (VL) pretraining have achieved great success. Since then, image-text pretraining has become
a default approach to tackling VL tasks \cite{biten2019scene,lin2014microsoft,regneri2013grounding,singh2019towards}. In addition, the introduction of Vision Transformers \cite{dosovitskiy2020image} enables vision and language modalities to be jointly modeled by Transformers in a more scalable fashion \cite{alayrac2022flamingo,wang2022git,yu2022coca,yuan2021florence}. According to the encoding strategies for image and language modalities, VL models can be categorized into fusion encoder \cite{li2019visualbert,lu2019vilbert,su2019vl,tan2019lxmert}, dual encoder \cite{radford2021learning}, and a combination of both \cite{bao2021vlmo,du2022survey,singh2022flava}. Several video-language pretrained models have also shown strong performance on video captioning and other video tasks, such as HERO \cite{li2020hero}, VideoBERT \cite{sun2019videobert}, and UniVL \cite{luo2020univl}.
In this work, cross-modal video summarization requires models with strong video understanding and language modeling capabilities. Therefore, this new task provides a practical scenario to assess the superiority of multimodal pretrained models.


\input{tables/dataset}