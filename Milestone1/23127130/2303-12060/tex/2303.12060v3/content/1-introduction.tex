\section{Introduction}
\label{sec:intro}


\IEEEPARstart{V}{ideo} summarization, which is known as generating a concise summary that conveys the primary parts of a full-length video, is a profound challenge for video analysis. Practical automatic video summarization systems have a great potential impact on numerous applications, \eg, movie trailer generation~\cite{irie2010autotrailer}
and narrative generation~\cite{grabska2021application}. Typical approaches of video summarization extract essential clips or frames from a given long video \cite{song2015tvsum,gygli2014creating,de2011vsumm}. Alternatively, the principal video content can also be summarized in natural language, e.g., video captioning~\cite{zhou2018towards,xu2016msr,krishna2017dense}. However, previous works treat either visual or textual summarization as separate tasks and thus ignore the semantic correlation between these two modalities of summarization. Therefore, these methods lack the ability to generate aligned visual textual summaries. An earlier attempt \cite{chen2017video} seeks to simultaneously generate visual and textual summaries from long videos. Still, the generated visual textual summaries in this work are not guaranteed to be semantically aligned since the two tasks were treated as separate, and there were no paired video and text summarization data for training or testing.


% Figure 1
\input{figures/v2x_sum}


In this study, we first introduce a novel cross-modal video summarization task, which involves generating visual and textual summaries with semantic coherence. To facilitate this new task, we propose \textbf{VideoXum}, an enriched large-scale dataset for cross-modal video summarization. The dataset is built on ActivityNet Captions \cite{krishna2017dense}, a large-scale public video captioning benchmark consisting of 200 distinct activity categories. These activity classes belong to 5 different top-level video topics: ``Eating and Drinking", ``Sports, Exercises, and Recreation", ``Socializing, Relaxing, and Leisure", ``Personal Care", and ``Household". To ensure consistent annotations, we hire workers to annotate ten shortened video summaries for each long source video according to the corresponding captions. Consequently, VideoXum contains 14K long videos with 140K pairs of aligned video and text summaries. Our goal is to extend the traditional single-modal video summarization task to a cross-modal video summarization task. Fig.~\ref{fig:v2xsum} presents this novel task termed V2X-SUM (Video-to-X Summarization), where X denotes the modality of generated summaries. According to the target modality, we categorize the V2X-SUM task into three subtasks:


\noindent \textbf{Video-to-Video Summarization (V2V-SUM)}. 
This task requires models to identify key segments from a source video and produce an abridged version.


\noindent \textbf{Video-to-Text Summarization (V2T-SUM)}.
In this task, models need to summarize the main content of the source video into a brief text description.

\noindent \textbf{Video-to-Video{\&}Text Summarization (V2VT-SUM)}. This task requires models to achieve V2V-SUM and V2T-SUM tasks simultaneously. Moreover, the semantics of these two modalities of summaries should be well aligned.


Compared with single-modal summarization tasks, cross-modal video summarization comes with its own challenges.
We summarize three primary challenges for this new task.
First, 
the scarcity of large-scale, diverse, and well-annotated cross-model video summarization benchmarks presents a significant hurdle for researchers in promoting the corresponding techniques.
Second, from the perspective of optimization, it is nontrivial to ensure the stability of the training process that accommodates both tasks concurrently. Specifically, a stable training process could facilitate learning of each single-modal task, thereby improving the overall performance.
Third, either assuring or evaluating the semantic coherence between the generated video and text summaries is challenging.


To establish strong baseline models for this emerging task, we propose VTSUM-BLIP, an end-to-end cross-modal video summarization model. 
To leverage the strong capability of vision-language pretrained (VLP) models for vision understanding and language modeling, we employ BLIP \cite{li2022blip} as our foundational backbone. This VLP encoder-decoder architecture provides a superior initialization, which is crucial for stable and effective optimization in machine learning models~\cite{zoph2020rethinking}.
Inspired by efficient video encoding techniques \cite{li2020hero,beltagy2020longformer,ju2022prompting,ni2022expanding}, we design an efficient hierarchical video encoding strategy, incorporating a frozen encoder, a temporal modeling module, and a context aggregation module to encode long videos. The video encoder is followed by different task-specific decoders for video and text summarization. The modularized design enables us to perform more complex downstream tasks without changing the structure of the pretrained backbone. 
Existing multimodal-based video summarization works~\cite{chen2017video,narasimhan2021clip} follow a pipeline where a summary is first generated in one modality, and then this generated summary is served as a prompt to improve the summary in another modality. Such methods may suffer from bias accumulation issues since they do not consider the semantic coherence of summaries in two modalities.
In contrast, our proposed VTSUM-BLIP enables joint training of the video and text summarization decoders in parallel. 
In other words, the predictions of these two decoders avoid sequential dependency between the two modalities of summaries.
Furthermore, the video and text summarization decoders collaboratively influence the shared parameters during training, allowing the framework to learn the semantic coherence between two tasks.


Our proposed framework achieves promising performance on VideoXum, as well as other existing single-modal video summarization datasets (\ie, TVSum~\cite{song2015tvsum}, SumMe~\cite{gygli2014creating}, and ActivityNet Captions~\cite{krishna2017dense}).
Inspired by the CLIPScore~\cite{hessel2021clipscore} and its video-text variant~\cite{wu2021godiva}, we adapt these metrics for the VideoXum and propose VT-CLIPScore for evaluating the semantic coherence of cross-modal summaries. The empirical results show the consistency of the proposed metric with human evaluation.



Our main contributions can be summarized as follows: 

\begin{itemize}
    \item We introduce \textbf{VideoXum}, an enriched large-scale dataset, to bridge the modality gap between the video and text summarization. The dataset contains 14K long videos with corresponding human-annotated video and text summaries. We conduct comprehensive experimental analyses to verify the rationality of our proposed new dataset. 
    \item Based on VLP encoder-decoder architecture, we propose an end-to-end cross-modal video summarization framework -- VTSUM-BLIP to establish strong baseline models for this novel task. The models achieve promising results on VideoXum and the new state of the art on several existing single-modal video summarization datasets.
    \item We propose an evaluation metric VT-CLIPScore on the VideoXum benchmark to evaluate cross-modal semantic consistency. The empirical results show the high consistency of our proposed metric with human evaluation.
\end{itemize}
\vspace{-2mm}
