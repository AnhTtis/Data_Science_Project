

\section{Conclusion}
In this study, we first propose a new video and text summarization task along with an enriched dataset VideoXum. In this task, we jointly consider the generic video summarization task and video-to-text summarization task. Furthermore, we propose a strong baseline framework VTSUM-BLIP to address the challenges for our proposed task. The empirical results show that our proposed framework achieves promising performance on VideoXum. In addition, we adapt CLIPScore on the VideoXum benchmark and introduce a new metric VT-CLIPScore to evaluate cross-modal video summarization semantic consistency, which shows high consistency with human evaluation on multiple experimental results.
For future studies, there are several promising directions on this benchmark.
There is plenty of room to explore the strategy of associating V2V and V2T summarization tasks for better performance and efficiency.
The proposed VideoXum dataset provides a foundation that could be significantly expanded through GPT-4~\cite{openai2023gpt4}, for generating video instruction-following data and thereby promoting the development of a general-purpose video assistant. 
For the evaluation metric, the adapted CLIP model for measuring video-text similarity is a practical compromise for the lack of large video-text pretrained models. It also suggests the need for a more reliable metric for video-text coherence measurement.
In addition, more advanced visual/video encoders and large language models (LLMs)~\cite{openai2023gpt4,touvron2023llama} could be integrated into the proposed framework to benefit the results of cross-modal summarization.