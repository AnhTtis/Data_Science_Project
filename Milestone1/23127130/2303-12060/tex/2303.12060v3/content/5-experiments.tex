\section{Experiments}
In this section, we first introduce the baseline models and experimental setup for the proposed VideoXum dataset. Then, we present the evaluation metrics and human evaluation strategy. In addition, we report several baseline models' performances under different settings and present a comprehensive experimental analysis to prove the effectiveness of our proposed method. 


\subsection{Baseline Models}

We introduce all the baseline models listed in TABLE~\ref{tab:videoxum_table}:

\noindent {\bf Frozen-BLIP} refers to inference over the test set using a frozen BLIP model without training. We take this zero-shot setting performance as a lower bound for our benchmark. 

\noindent {\bf VSUM-BLIP (Base)} is a baseline model to perform video-to-video summary. It consists of a frozen BLIP encoder and a learnable video-sum decoder.

\noindent {\bf TSUM-BLIP (Base)} is a video-to-text summary baseline. We employ the vanilla BLIP model with a frozen encoder.

\noindent {\bf VTSUM-BLIP (Base)} combines the VSUM-BLIP and TSUM-BLIP modules of the model. It is comprised of a shared frozen encoder and two task-specific decoders. 

\noindent {\bf Temporal Transformer (TT)} is a crucial module to achieve the hierarchical encoding for videos while incorporating temporal information into a video sequence. Specifically, we use several Transformer layers combined with temporal positional embedding to model the temporal information.

\noindent {\bf Context Aggregation (CA)} is a plug-and-play module to model the video frame representations for the V2VSum task. Compared with the baseline models, this mechanism enhances the local context information for video representations and could help reduce the redundancy of video summaries.

\vspace{-3mm}
\subsection{Experimental Setup and Implementation Details} \label{sec:exp_setup}

% Data preprocess
\noindent \textbf{Data Preprocessing}. Video frames of all \textit{train/val/test} sets are first resized using bi-linear resampling to 224 pixels along the shorter side. Next, a $224 \times 224$ center crop is applied to the resized frames. This is a common preprocessing method. For each training batch, we add padding to all video sequences to make them the same length, enabling the videos to be processed in parallel and speeding up the training process. In addition, the padding tokens are masked out during the self-attention calculation. Based on data statistics in Fig.~\ref{fig:video_len}, we set the maximum video length to 512, and frames exceeding the maximum length are truncated.
For each text summary, we concatenate (dense) sentence captions in a video to construct a narrative paragraph~\cite{gabeur2020multi,patrick2021supportset,zhang2018cross}. According to data statistics in Fig.~\ref{fig:tsum_len}, we set the maximum generation length to 128 in the text summarization task.



% Table 3
\input{tables/videoxum_table}

% Model Architecture
\noindent \textbf{Model Architecture}. We employ ViT-B/16~\cite{dosovitskiy2020image} as the image encoder backbone with $N_\text{vis}=12$ layers.
The $N_\text{tem}$-layer Temporal Transformer (TT) follows the image encoder, where $N_\text{tem}$ is 1. The temporal positional embeddings $\varepsilon_\text{temp}$ in Eq.(\ref{eq:vis_feat}) are also learnable.
The video-sum decoder contains a Context Aggregation (CA) module capturing local context and a binary linear classifier. The CA module constructs a binary local attention map with window size $\epsilon=5$.
For the text-sum decoder, we adopt a variant of Transformer with $N_\text{tex}=12$ layers, which replaces the bidirectional self-attention module with a causal self-attention module~\cite{dosovitskiy2020image}. In addition, the prompt $\mathcal{T}_\text{prompt}$ of the text-sum decoder in Eq.(\ref{eq:tsum}) is set as ``[\texttt{DEC}] a video of''.



% Table 4
\input{tables/v2vsum_table}

% Weight initialization
\noindent \textbf{Weight Initialization}. To initialize the weights of our model, we employ a state-of-the-art VLP model called BLIP~\cite{li2022blip}. The image encoder and the text-sum decoder are initialized by pretrained BLIP$_\text{CapFilt-L}$. Additionally, the Temporal Transformer and video-sum decoder are randomly initialized.

% Fine-tuning
\noindent \textbf{Optimization}. Due to limited computational resources, we finetuned all of the parameters in our proposed VTSUM-BLIP model, except for the image encoder. We adopt the AdamW~\cite{loshchilov2018decoupled} optimizer with an initial learning rate of $2\times10^{-5}$ to optimize the model, and the $\beta_1=0.9, \beta_2=0.999$. The batch size is 64, and weight decay is $5\times10^{-2}$. The learning rate follows a cosine decay schedule~\cite{loshchilov2016sgdr} with the minimum learning rate of $0.0$. We train the VTSUM-BLIP framework for 56 epochs with a batch size of 64 on 4 A100 GPUs. In addition, the weights of video-sum loss $\mathcal{L}_v$ and text-sum loss $\mathcal{L}_t$ are $\lambda_v=15.0$ and $\lambda_t=1.0$, respectively.


\subsection{Evaluation}


\noindent \textbf{Video Summary Evaluation}. Following previous works~\cite{narasimhan2021clip,otani2019rethinking,saquil2021multiple} for video summarization evaluation, we adopt the F1 score, Kendall's $\tau$~\cite{kendall1945treatment}, and Spearman's $\rho$~\cite{zwillinger1999crc} as our automatic evaluation metrics.


\noindent \textbf{Text Summary Evaluation}. To evaluate the quality of generated text summaries for video {\bf text summary}, we adopt several metrics for video captioning evaluation \cite{xu2023mplug} including: BLEU~\cite{papineni2002bleu}, METEOR~\cite{banerjee2005meteor}, ROUGE-L~\cite{lin2004rouge}, CIDEr~\cite{vedantam2015cider}.

\noindent \textbf{Video-text Semantic Consistency Evaluation}. Apart from independently evaluating single-modal summaries, we also evaluate \textit{semantic consistency} of text and video summaries.
Inspired by previous works~\cite{hessel2021clipscore,wu2021godiva,singer2022make}, we adapt CLIPScore for VideoXum benchmark and introduce a new evaluation metric -- VT-CLIPScore for evaluating the text and video semantic consistency. Specifically, we finetune the vanilla CLIP model~\cite{radford2021learning} on VideoXum dataset with contrastive learning strategies.
It is worth noting that adapting CLIPScore to our proposed benchmark is necessary since there is a domain gap between the CLIP pretraining data and our VideoXum data. Therefore, finetuning the CLIP model on our data makes the evaluation score more reliable. Similar attempts of finetuning evaluation models (\ie, BERTScore~\cite{Zhang2020BERTScore} and Sentence-BERT~\cite{reimers2019sentence}) also support the necessity of the VT-CLIPScore.

To facilitate reimplementation, we use the AdamW optimizer with an initial learning rate of $2\times10^{-6}$ and a weight decay of $5\times10^{-2}$. We finetune the CLIP model for 50 epochs with a batch size of 16 on 4 GPUs. The empirical results in TABLE~\ref{tab:clipscore} show that our proposed VT-CLIPScore is sensitive enough to the semantic change of video and text. Moreover, the results in TABLE~\ref{tab:he} indicate the high consistency of our proposed automatic evaluation metric with human evaluation. 





% Table 5
\input{tables/v2tsum_table}

\vspace{-2mm}
\subsection{Results on VideoXum}
We conduct experiments on VideoXum using different baseline models. TABLE~\ref{tab:videoxum_table} shows the empirical results of the models on VideoXum. By comparing VSUM-BLIP (Base), TSUM-BLIP (Base), and VTSUM-BLIP (Base) with Frozen-BLIP, BLIP models show better results after finetuning on specific tasks.
The comparison between the end-to-end VTSUM-BLIP (Base) and the Two-stage Manner~\cite{chen2017video} (\ie, first V2V-Sum and then V2T-Sum) demonstrates the superiority of the end-to-end framework since errors originating in the V2V-SUM stage could negatively influence the V2T-SUM stage.
In all three tasks, the model with TT can help model the video sequence better, indicating that temporal information is necessary. The CA module can enhance the local information awareness of the model, which can help improve the performance of the V2V-SUM task. The performance gains of TT on V2V-SUM are more significant than that on V2T-SUM, one of the possible reasons is that the text decoder is a well-generalized model trained on a large corpus and is insensitive to the subtle changes of the input features \cite{arora2018stronger,hua2021noise,hua2022fine}. A Base BLIP model combined with TT and CA achieves the SOTA results in our proposed three tasks. From the overall performance,  our proposed multitask framework can benefit both V2V and V2T-SUM tasks. In addition, TABLE~\ref{tab:videoxum_table} shows the human performance on VideoXum. The result is obtained on human-annotated reference summaries using a leave-one-out strategy~\cite{otani2019rethinking}, which measures the average consistency of human annotators. Although humans outperform all baseline models in most evaluation metrics on different tasks (except for BLEU@4), our proposed VTSUM-BLIP archives quite competitive results, especially on the V2T-SUM task. Fig.~\ref{fig:vis} visualizes some examples of generated video and text summaries, showing the effectiveness of TT and CA modules. Additionally, more visualization results are present in Fig.~\ref{fig:add_vis}.



% Figure 4
\input{figures/visualize}

\vspace{-5mm}
\subsection{Experimental Analysis}

\noindent \textbf{Method Comparisons on Existing Benchmarks}. To further evaluate the effectiveness of our proposed model, we conduct experiments on two well-known video summarization datasets, \ie, TVSum and SumMe. The results in TABLE~\ref{tab:v2vsum_table} show that the BLIP-VSUM (Base) achieves competitive results against several strong baselines. Moreover, our proposed mechanisms of Temporal Transformer and Context Aggregation can further improve video summarization performance. We also verify the ability of video-to-text summarization of our model on ActivityNet Captions. As we can see from TABLE~\ref{tab:v2tsum_table}, our proposed model outperforms all the strong baseline models by a large margin in multiple evaluation metrics (2.9 in BLEU@4, 1.0 in METEOR, 7.4 in ROUGE-L, and 19.0 in CIDEr).

\noindent \textbf{Human Evaluation}.
We conducted a human evaluation of video/text summaries on 50 random samples, assessed by workers for quality and consistency across video and text summaries, scoring 1-5 (5 best). We report the average score in TABLE~\ref{tab:he}. We can conclude from the table that our proposed model can generate more fluent and accurate text summaries for long videos. The proposed temporal Transformer and Context Aggregation can help generate accurate and consistent video summaries. Following \cite{wang2019controllable}, we compute the Kappa coefficient of different workers, and the value is 0.49 $\in$ (0.41, 0.60),
which means that the consistency is moderate.


\input{tables/human}

\input{tables/clipscore}

\noindent \textbf{Comparison between CLIPScore and VT-CLIPScore}.
Although we can apply a pretrained CLIP model without any adaptation on our dataset to evaluate the semantic consistency of the video and text summaries, the similarity score may be insensitive to the semantic change of generated cross-modal summaries.
In TABLE~\ref{tab:clipscore}, we compare the vanilla CLIPScore~\cite{hessel2021clipscore} and the finetuned VT-CLIPScore to measure the similarity of different video and text summarization pairs.
The positive pairs refer to the paired video and text summaries. The negative pair includes unpaired video and text summaries.
From the results, we can observe that CLIPScore provides a solid foundation for the finetuned VT-CLIPScore.
Moreover, adapting the vanilla CLIP model to our proposed task is necessary since there is a domain gap between the CLIP pretraining data and our VideoXum data. The finetuned CLIP model on our data makes the evaluation score more reliable.
TABLE~\ref{tab:clipscore} shows that finetuning on VideoXum makes the similarity scores more reflective or informative in measuring the semantic consistency of cross-modal summaries.




% Figure 7
\input{figures/win_size}

% Table 9
\input{tables/tt}

\vspace{-2mm}
\subsection{Extended Discussion}
{
\noindent\textbf{Complexity analysis}.
In terms of computational complexity, the V2V-Sum, V2T-Sum, and V2VT-Sum models require 0.06, 1.15, and 1.18 GPU hours for training on an A100 GPU, respectively. Regarding the model complexity, they have parameter sizes of 435.6 MB, 564.8 MB, and 567.1 MB.
}


{
\noindent\textbf{Impact of multi-task weights in Eq.(\ref{eq:loss})}.
To determine the impact of multi-task weights (\ie, $\lambda_v$ and $\lambda_t$), we perform an ablation study on the $\lambda_v$ and $\lambda_t$ using the VideoXum \textit{val} set. Fig.~\ref{fig:multitask_weight} shows that the peak point appears when $\lambda_v = 15.0$ and $\lambda_t = 1.0$ as mentioned in Section~\ref{sec:exp_setup}.
}

\noindent\textbf{Impact of Local Window Size $\varepsilon$}.
For the V2V-SUM task, the local window size $\varepsilon$ controls the context range of local self-attention. For $\varepsilon = 1$, the local self-attention module degrades to a multilayer perceptron (MLP). As $\varepsilon$ increases to $T$ ($>1$), the local self-attention module upgrades to a global/regular self-attention module.
%To determine the impact of local window size $\varepsilon$, we perform an ablation study on the VSUM-BLIP model using the VideoXum \textit{val} set.
Fig.~\ref{fig:win_size} shows the optimal performance occurs at $\varepsilon=5$; below this, limited context hinders performance, while above it, excess context introduces irrelevant frames, reducing efficacy. Therefore, the performance of the V2V-SUM task is improved by carefully selecting appropriate local context information.

\noindent\textbf{Impact of Temporal Transformer Layers}. We conduct an ablation study on the number of Temporal Transformer layers $N_\text{tem}$ using VideoXum \textit{val} set. TABLE~\ref{tab:tt_layer} indicates that altering $N_\text{tem}$ does not significantly affect the performance for all three tasks. Therefore, we set $N_\text{tem}$ to 1.

\noindent\textbf{Analysis for Human Performance on V2T-SUM}.
Human performance on our proposed three tasks can be regarded as an upper bound of each task.
TABLE~\ref{tab:videoxum_table} presents human performance outperforms our proposed model by a large margin on V2V-SUM and V2VT-SUM. However, on V2T-SUM, human performance does not exhibit a significant advantage over our model (especially on BLEU@4).
To better understand this phenomenon, we examine human-annotated captions and their corresponding references, where ``Human'' indicates the human predictions on VideoXum \textit{test} set and ``Reference'' denotes the corresponding ground truth. Both ``Human'' and ``Reference'' are human-annotated text summaries from ActivityNet Captions~\cite{krishna2017dense} validation sets. In particular, we present a representative example below:

\vspace{-2mm}

{\scriptsize
\begin{verbatim}
============================================================
*Human*: Two children stand in front of a mat. They throw
something onto the mat. They take turns jumping across the
mat. They pick up the item they threw on it.
============================================================
*Reference*: Two young children are standing in line indoors
in what looks like a living room. The little girl is stand-
ing closest to the hopscotch mat and she throws her toy onto
the mat and then begins jumping until she meets the end of
the mat then turns around and heads back to the point she
started and her turn is over. The little boy goes next, and
he throws the toy onto the mat and begins jumping to the end
of the mat, then turns around and jumps back towards his
starting point.The little girl steps in front of the boy and
gets into motion to start another turn on the hopscotch mat.
============================================================
\end{verbatim}
}

\vspace{-2mm}

\noindent Both captions describe two children playfully interacting on a mat, but ``Reference'' provides a more vivid and detailed picture of the scene. The comparison shows that summarizing a long video is inherently subjective, leading to varying text descriptions of the same content among different individuals. Therefore, it explains why human performance does not exhibit a significant advantage
over VTSUM-BLIP model.