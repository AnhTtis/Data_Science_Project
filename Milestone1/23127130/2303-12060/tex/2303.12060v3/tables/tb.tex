\begin{table*}[h]
\small
\centering
\caption{In-context learning results on VQAv2, TextVQA, and OK-VQA. 
Both our systems and Flamingo~\cite{alayrac2022flamingo} uses $n=32$ examples in the prompt.
Frozen \cite{tsimpoukelli2021multimodal} uses $n=4$ examples.
The upper part shows the state of the art by supervised/fine-tuned methods.
%\nascomment{maybe label these with how many labeled examples they each use?}\yushi{we used synthesized data from these datasets to train, so maybe it's better not to bring that up}
}
% \begin{tabular}{P{1.7cm}P{1.2cm}P{1.2cm}P{1.0cm}P{1.2cm}P{1.2cm}P{1.2cm}P{1.1cm}P{1.1cm}P{1.1cm}}
\begin{tabular}{lccccccc}
\shline
Method & In-Context & Image & \multicolumn{2}{c}{VQAv2} & \multicolumn{2}{c}{TextVQA} & OK-VQA\\
& Example Retrieval & Representation & test-dev & test-std & val & test & val \\
\shline
\multicolumn{8}{l}{\textbf{Fine-tuned E2E}} \\
%Oscar  &  Feature  & 73.6 & 73.8 \\
%VinVL  & Feature & 75.5 & 75.6 \\
OFA-large \cite{wang2022ofa} & - & Feature  & 80.3 & 80.5 & -& -&-\\
KAT \cite{gui2022kat} & - & Caption + Tags + Feature & - & -& -&- & 54.4\\
GIT \cite{wang2022git} & - & Feature & 78.6 & 78.8 & 59.9 & 59.8 & - \\
%Florence & Feature & 80.2 & 80.4 \\
\shline
\multicolumn{8}{l}{\textbf{In-Context Learning}} \\
Frozen\cite{tsimpoukelli2021multimodal}  & Random & Feature &  38.2 & - & -  & - & 12.6 \\
Flamingo (80B)\cite{alayrac2022flamingo} & Random & Feature & 67.6 & - & 37.9 & - & 57.8 \\
OFA Cap + GPT-3 & Random & Caption & 61.2 & - & 16.5 & -  & 51.2\\
$\ \ $ + Example Selection & CLIP & Caption & 64.9 & - & 22.6 & - & 55.4 \\
PromptCap + GPT-3 & Random & Caption & 72.1 & - & 48.9 & - & 55.3\\
$\ \ $ + Example Selection & CLIP & Caption & \textbf{74.1} & \textbf{74.1} & \textbf{50.9} & \textbf{51.8} & \textbf{
58.8}\\
% GT + GPT-3 & GT-Caption-5 &  59.7 \\
\shline

\end{tabular}
\label{tab:icl}

% \bottomrule
 \vspace{-3mm}
\end{table*}