\section{Conclusion}
In this study, we first propose a new video and text summarization task along with an enriched dataset VideoXum. In this task, we jointly consider the traditional video summarization task and video-to-text summarization task. Furthermore, we propose a novel model VTSUM-BLIP to address the challenges for our proposed task. The empirical results show that our proposed framework achieves promising performance on VideoXum. In addition, we propose a new metric VT-CLIPScore to evaluate cross-modal video summarization semantic consistency, which shows high consistency with human evaluation on multiple experimental results. 
We plan to investigate more sophisticated models to exploit the aligned video and text summaries to improve the performance of each task. We may also employ in-context learning of language models \cite{hu2022promptcap} to perform cross-modal summarization. 