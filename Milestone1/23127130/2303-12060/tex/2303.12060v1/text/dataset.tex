\section{Dataset}
\label{sec:dataset}

In this section, we introduce the proposed VideoXum dataset. The dataset is annotated by workers, including 14,001 long videos with video and text summaries pairs. We describe the process of dataset collection and annotation strategy. In addition, we provide several quantitative and qualitative analyses of the proposed dataset.


\subsection{Dataset Curation}
\subsubsection{Dataset Collection} The VideoXum dataset is built based on ActivityNet Captions~\cite{krishna2017dense}, a high-quality public video captioning benchmark. The dataset contains 20K real-life Youtube videos with diverse content, in terms of abundant topics, different photographic devices, multiple view angles, and so on. Each video in this dataset is annotated with a series of dense temporal segments, and each segment corresponds to a concrete sentence description. As described in Section \ref{sec:intro}, the well-annotated sentence narratives are natural summaries of the source videos. The content and length of videos in the ActivityNet Captions dataset largely meet our requirements for building the cross-modal summarization dataset and provide an ideal foundation for constructing our cross-modal summarization benchmark. We filter out videos shorter than 10 seconds.

\vspace{-3mm}
\subsubsection{Dataset Reannotation} In our video and text summarization task, each cross-modal video summary contains multiple summarized video spans and the corresponding sentence descriptions. For each video, we expect the total length of its video summary to be bounded to 15\% of the source video. ActivityNet Captions~\cite{krishna2017dense}  already contains  video captions with temporal segments for long videos. So we concatenate the caption sentences as a text summary for the long source video. However, the annotated video spans, which cover an average of 94.6\% of the source videos, are too long to be regarded as a video summary by themselves since video summaries need to be much more concise. Therefore, we reannotate the video spans and obtain an abridged version of video segments for better aligning with the sentence captions. Due to the inherently subjective nature of summarizing a long video (this conclusion is also reflected by the human performance on V2V-SUM in Table \ref{tab:videoxum_table}), it is hard to obtain perfect ground truth labels for this task. Following previous works~\cite{lei2021detecting,song2015tvsum,sun2014ranking}, we employ ten different workers to annotate the video summary spans for each text description. During the evaluation of video summarization, we will compare the prediction with all ten annotations and then obtain the average score for each video. We then filter the initial ActivityNet dataset using the length compression ratio of video summaries with 20\% as the threshold. The compression ratio is calculated as $\text{Ratio}(S, V) = \frac{|S|}
{|C|}$, where $|S|$ denotes the length of summary, $|C|$ denotes the length of source video. Finally, 14,001 long videos remain in our dataset.


\subsubsection{Dataset Split}
We split the dataset into training, validation, and test sets. The split strategy also guarantees that all three data splits preserve the same distribution of video length. In particular, the whole dataset is divided into 8,000, 2,001, and 4,000 videos in the training, validation, and test sets, respectively.


%\vspace{-2mm}
\subsection{Dataset Statistics}
The VideoXum dataset consists of 14,001 videos in total. The length of the videos ranges from 10 to 755 seconds, with most of them under 300 seconds. Each long video has 10 corresponding video and text summary pairs. The distributions of video length compression ratio and text summary length among the dataset are shown in Figure~\ref{fig:dataset_hist}. For the video summarization task, most video summary lengths are shorter than 15\% of the source video length. The average length compression ratio is 13.6\%, with a median ratio of 13.7\%, and a maximum ratio of 20\%. For the text summarization task, each video is summarized into a narrative paragraph that describes multiple events. On average, each narrative paragraph contains 49.9 words. Figure~\ref{fig:dataset_hist} indicates that most (98\%) text summaries are shorter than 128 words.


% Figure 2
\input{figures/data_stat}


\subsection{Comparison with Existing Single-modal Video Summarization Datasets}
In Table~\ref{tab:datasets}, we compare the proposed VideoXum dataset with existing {\it single-modal} video-to-video and video-to-text\footnote{In this paper, we regard the video captioning task as video-to-text summarization task} summarization datasets.
The main difference between VideoXum and other existing datasets is that VideoXsum contains aligned video and text summaries, while others only have single-modal summaries for source videos.
Compared with the existing video summarization benchmarks (e.g., SumMe~\cite{gygli2014creating} and TVSum~\cite{song2015tvsum}), the amount of data in the VideoXum dataset is significantly larger.% than others. 
In addition, VideoXum contains open-domain videos with more diverse scenarios than other datasets. To ensure the quality of human annotation, we evaluate the annotated data using a leave-one-out strategy~\cite{otani2019rethinking}. Table~\ref{tab:human_eval} shows that our annotation quality is comparable with existing benchmarks.

\input{tables/human_eval}