\setcounter{figure}{4}
\setcounter{table}{7}
\section{Supplementary Overview} \label{sec:supp}
In the supplementary material, we first offer additional details on the proposed VideoXum dataset. Next, we elaborate on additional implementation details of the proposed framework. Furthermore, we provide an extended empirical analysis. Finally, we present more qualitative results on the VideoXum dataset.


\subsection{Dataset Details}

\subsubsection{Detailed Data Statistics}

This section provides additional details on the dataset.
Figure~\ref{fig:data_stat_supp} shows the distributions of video lengths and normalized center timestamps of summarized video spans. As shown in Figure~\ref{fig:data_stat_supp} (\textit{left}), the video lengths vary greatly, ranging from 10 to 755 seconds, with 99.9\% of them under 300 seconds. The average length is 124.2 seconds, and the median length is 121.6 seconds.
Figure~\ref{fig:data_stat_supp} (\textit{right}) shows that the important clips are generally uniformly distributed throughout the video, with a mild peak at the beginning. Therefore, the VideoXum dataset does not suffer from temporal bias issues~\cite{lei2021detecting}.


\subsubsection{Reannotation Pipeline}
This annotation pipeline aims to reannotate the video spans of ActivityNet Captions~\cite{krishna2017dense} and obtain an abridged version (ideally bounded to 15\%) of video segments for better aligning with the sentence captions.
In particular, crowd workers are given a video with several captions and the corresponding temporal annotations (i.e., pairs of start and end timestamps). For each caption, ten workers are assigned to reannotate the corresponding video segment and obtain ten shortened spans. 

To ensure consistent annotations, we hired \textbf{40 crowd workers} to reannotate all \textbf{140,000 summarised video spans} over a period of \textbf{two months}. On average, each worker reannotated about \textbf{15 videos per hour}. To maintain high-quality annotations, we regularly reviewed the reannotated video spans and provided feedback to workers. Every 24 hours, we randomly evaluated 15\% of an annotation batch for accuracy. If the acceptance rate of the sampled annotations reached 90\%, we considered the entire annotation batch as passed; otherwise, we asked the workers to reannotate the batch.

\subsection{Implementation Details}

\subsubsection{Additional Training Details}
In this section, We provide additional training details about reproduction for reproducibility purposes.


% Figure 5
\input{figures/data_stat_supp}

% Data preprocess
\noindent \textbf{Data Preprocessing}. All video frames are first resized using bi-linear resampling to 224 pixels along the shorter side. Next, a $224 \times 224$ center crop is applied to the resized frames. For each training batch, we add padding to all video sequences to make them the same length, enabling the videos to be processed in parallel and speeding up the training process. In addition, the padding tokens are masked out during the self-attention calculation. Based on data statistics shown in Figure~\ref{fig:data_stat_supp} (\textit{left}), we set the maximum video length to 512, and frames exceeding the maximum length are truncated.
For each text summary, we concatenate (dense) sentence captions in a video to construct a narrative paragraph~\cite{gabeur2020multi,patrick2021supportset,zhang2018cross}. According to data statistics shown in Figure 2 (\textit{right}), we set the maximum generation length to 128 in the text summarization task.

% Model Architecture
\noindent \textbf{Model Architecture}. We employ ViT-B/16~\cite{dosovitskiy2020image} as the image encoder backbone with $N_\text{vis}=12$ layers.
The $N_\text{tem}$-layer Temporal Transformer (TT) follows the image encoder, where $N_\text{tem}$ is 1. The temporal positional embeddings $\varepsilon_\text{temp}$ in Eq.(1) are also learnable.
The video-sum decoder contains a Context Aggregation (CA) module capturing local context and a binary linear classifier. The CA module constructs a binary local attention map with window size $\epsilon=5$.
For the text-sum decoder, we adopt a variant of Transformer with $N_\text{tex}=12$ layers, which replaces the bidirectional self-attention module with a causal self-attention module~\cite{dosovitskiy2020image}. In addition, the prompt $\mathcal{T}_\text{prompt}$ of the text-sum decoder in Eq.(3) is set as ``[\texttt{DEC}] a video of''.



% Weight initialization
\noindent \textbf{Weight Initialization}. To initialize the weights of our model, we employ a state-of-the-art pretrained vision-language model called BLIP~\cite{li2022blip}. The image encoder and the text-sum decoder of VTSUM-BLIP are initialized by pretrained BLIP$_\text{CapFilt-L}$~\cite{li2022blip}. Additionally, the Temporal Transformer and video-sum decoder are randomly initialized.

% Fine-tuning
\noindent \textbf{Finetuning}. Due to limited computational resources, we finetuned all of the parameters in our proposed VTSum-BLIP model, except for the image encoder. We adopt the AdamW~\cite{loshchilov2018decoupled} optimizer with an initial learning rate of $2\times10^{-5}$ to optimize the model, and the weight decay is $5\times10^{-2}$. We train the VTSum-BLIP model for 56 epochs with a batch size of 64 on 4 A100 GPUs. In addition, the weights of video-sum loss $\mathcal{L}_v$ and text-sum loss $\mathcal{L}_t$ are $\lambda_v=15.0$ and $\lambda_t=1.0$, respectively.


% Figure 6
\input{figures/ref_human}


\subsubsection{VT-CLIPScore}
VT-CLIPScore aims to evaluate the text and video semantic consistency. We first finetune all of the parameters of the vanilla CLIP model~\cite{radford2021learning} on our proposed VideoXum dataset, which enables the pretrained CLIP model to adapt to the VideoXum. In detail, we use the AdamW~\cite{loshchilov2018decoupled} optimizer with an initial learning rate of $2\times10^{-6}$ and a weight decay of $5\times10^{-2}$. We finetune the CLIP model for 50 epochs with a batch size of 16 on 4 A100 GPUs. Based on the finetuned CLIP model, we calculate averaged CLIP similarity between video frames and text to derive the VT-CLIPScore. 


% Figure 7
\input{figures/win_size}

% Table 8
\input{tables/ref_human}

% Table 9
\input{tables/tt}


\subsection{Extended Empirical Analysis}

\subsubsection{Analysis for Human Performance on V2T-SUM}
Human performance on our proposed three tasks can be regarded as an upper bound of each task.
In Table \ref{tab:videoxum_table}, we can see that human performance outperforms our proposed model by a large margin on V2V-SUM and V2VT-SUM. However, on V2T-SUM, human performance does not exhibit a significant advantage over our model (especially on BLEU@4).
To better understand this phenomenon, we examine several human-annotated examples and their corresponding references in Table~\ref{tab:v2texp}, where ``Human'' indicates the human predictions on VideoXum \textit{test} set and ``Reference'' denotes the corresponding ground truth.
Both ``Human'' and ``Reference'' are human-annotated text summaries from ActivityNet Captions~\cite{krishna2017dense} validation set.
As shown in Table~\ref{tab:v2texp}, the examples demonstrate that summarizing a long video is inherently subjective, leading to varying text descriptions of the same content among different individuals. Figure \ref{fig:ref_human} shows the statistical distribution of summary length differences between the reference and human performance, highlighting the significant variability in annotators' performance on the V2T-SUM task.


\subsubsection{Impact of Local Window Size $\varepsilon$}
For the V2V-SUM task, the local window size $\varepsilon$ controls the context range of local self-attention. For $\varepsilon = 1$, the local self-attention module degrades to a multilayer perceptron (MLP). As $\varepsilon$ increases to $T$ ($>1$), the local self-attention module upgrades to a global/regular self-attention module. To determine the impact of local window size $\varepsilon$, we perform an ablation study on the VSUM-BLIP model using the VideoXum \textit{val} set. As shown in Figure~\ref{fig:win_size}, the optimal performance is obtained when $\varepsilon$ is set to $5$. The limited context would hamper the performance as the value of $\varepsilon$ decreases below $5$. On the other hand, when $\varepsilon$ becomes larger than $5$, the large context range would introduce irrelevant frames, resulting in suboptimal performance. Therefore, the performance of the V2V-SUM task is improved by carefully selecting appropriate local context information.

\subsubsection{Impact of Temporal Transformer Layers}
In this section, we conduct an ablation study on the number of Temporal Transformer layers $N_\text{tem}$ using VideoXum \textit{val} set. Table~\ref{tab:tt_layer} indicates that altering $N_\text{tem}$ does not significantly affect the performance for all three tasks. Therefore, we set $N_\text{tem}$ to 1.

\subsection{Additional Qualitative Results}
In this section, we present additional qualitative results in Figure~\ref{fig:visualize} ({\bf on Page 15}).

% Figure 8
\input{figures/visualize_supp}