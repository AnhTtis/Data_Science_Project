\section{Methodology}

% Figure 3
\input{figures/vtsum_blip}


\subsection{Problem Formulation}
 We formulate the problem of cross-modal video summarization as a multi-task learning problem, including V2V-SUM and V2T-SUM. Given a video $\mathcal{V}=\{\bv_{i}\}_{i=1}^{T}$,  $T$ is the number of frames in the video and $\bv_i$ denotes the $i\text{-th}$ frame in the temporal order.
 Our goal is to learn a shared video encoder $f(\cdot; \theta)$ followed by two task-specific decoders, including a video summarization decoder $g_v(\cdot; \theta_{v})$ and a text summarization decoder $g_t(\cdot; \theta_t)$. In particular, the notations of $\theta$, $\theta_v$, and $\theta_t$ represent the learnable parameters of the shared video encoder, video summarization decoder, and text summarization decoder, respectively. We first feed the input video $\mathcal{V}$ into the shared video encoder to produce the video features $\tilde{\mathcal{Z}}$:
\begin{align}
    \tilde{\mathcal{Z}} &= f(\mathcal{V}, \mathcal{E}_\text{temp}; \theta), \label{eq:vis_feat}
\end{align}
where $\mathcal{E}_\text{temp}$ is the temporal position embedding for the video frames. Given the video features $\tilde{\mathcal{Z}}$, the model generates a video summary $\mathcal{V}_\text{sum}$ and a text summary $\mathcal{T}_\text{sum}$. In particular, we can formulate visual and narrative outcomes as follows:
\begin{align}
    \mathcal{V}_\text{sum} &= g_v(\tilde{\mathcal{Z}}; \theta_v),  \label{eq:vsum} \\
    \mathcal{T}_\text{sum} &= g_t(\tilde{\mathcal{Z}}, \mathcal{T}_\text{prompt}; \theta_t),  \label{eq:tsum}
\end{align}
where $\mathcal{T}_\text{prompt}$ denotes a prompt sequence.


\subsection{Cross-modal Video Summarization}
Our proposed VideoXum benchmark requires models with strong video understanding and language modeling capabilities. To this end, we employ the large pretrained vision-language model BLIP~\cite{li2022blip} as our backbone. The framework of our model is shown in Figure~\ref{fig:v2xsum-framework}.


\subsubsection{Hierarchical Video Encoder} 
The hierarchical video encoder $f(\cdot; \theta)$ aims to address the challenge of efficiently extracting temporal-aware visual features from a long video. Drawing the inspiration from long document summarization \cite{ruan2022histruct+}, we formulate the BILP image encoder into a hierarchical architecture for long video encoding without changing the structure of the encoder. This enables us to efficiently obtain rich video features at the frame and image patch levels. Specifically, given a video $\mathcal{V}=\{\bv_{i}\}_{i=1}^{T}$ with $T$-frame, the frozen image encoder projects each video frame $\bv_i$ into the representation space and produce $T$ visual tokens $\mathcal{Z}=\{\bz_{i}\}_{i=1}^{T}$. Next, we use temporal position embedding  $\mathcal{E}_\text{temp}=\{\bolde_i \}^T_{i=1}$ with the shared \textbf{T}emporal \textbf{T}ransformer (TT) to model the temporal information for the video sequence. In this way, we can obtain temporally-aware visual features $\tilde{\mathcal{Z}}=\{\tilde{\bz_{i}}\}_{i=1}^{T}$.


\noindent \textbf{Frozen Image Encoder}.
Following the previous works~\cite{chen2020uniter,zhang2021vinvl}, we freeze the parameters of the pretrained BLIP encoder, which can help to improve the training time and GPU memory efficiency for encoding long videos. In detail, we first convert input images into several patches as the input tokens for the $N_\text{vis}$-layer BLIP encoder. The patch embedding is prepended with a [\texttt{CLS}] token in the representation space. Next, we take all output of the [\texttt{CLS}] tokens as the representation of the input frames. We can compress the input video at the frame level through the hierarchical encoding strategy and generate the representation $\mathcal{Z}$.


\noindent \textbf{Shared Temporal Transformer}.
After obtaining the representation of the sequence of the video frame $\mathcal{Z}=\{\bz_i\}^T_{i=1}$, we add these temporal position embeddings $\mathcal{E}_\text{temp}=\{\bolde_i \}^T_{i=1}$ to $\mathcal{Z}$, and feed them into the shared temporal Transformer (TT) for temporal modeling and get the temporal-aware visual features $\tilde{\mathcal{Z}}=\{\tilde{\bz _i}\}^T_{i=1}$ in Eq.(\ref{eq:vis_feat}):
\vspace{-0.3em}
\begin{align}
    \nonumber &\bz_i^{(0)} = \bz_i+\bolde_i, \\ 
    \nonumber &\bz_i^{(l)} = \text{TT}^{(l)}(\bz_1^{(l-1)},\dots,\bz_T^{(l-1)}), \ l=1,\dots,N_{tem},\\
    &\tilde{\bz_i} = \bz_i^{(N_{tem})},
\end{align}
where $l$ indicates the $l$-th block of the temporal Transformer, and $N_{tem}$ denotes total block number of the temporal Transformer.


\vspace{-2mm}
\subsubsection{Video-Sum Decoder}
The video-sum decoder $g_v(\cdot; \theta_{v})$ contains a \textbf{C}ontext \textbf{A}ggregation (CA) module that captures context from neighboring frames with local self-attention. 
In particular, we first define a fixed-size slice window at each temporal position and then construct a binary local attention map ${M}^{LA} \in \{0,1\}^{T \times T}$ with a given window size $\varepsilon$. For example,  Figure~\ref{fig:v2xsum-framework} ({\em right}) presents a local attention map with a window size $\varepsilon=7$. Next, we compute the local attention features $\mathcal{A}_{loc}$:
\vspace{-1mm}
\begin{align}
    \mathcal{A}_{loc} = \left(\text{softmax}(\frac{QK^T}{\sqrt{d}}) \odot M^{LA}\right)V,
\end{align}
where $\odot$ is element-wise multiplication and queries $Q$, keys $K$, and values $V$ are $d$-dimensional features generated from temporal-aware visual features $\tilde{\mathcal{Z}}$. Finally, we feed local attention-enhanced features into a linear classifier to obtain the predictions of the frame-level importance scores $\{p_i\}_{i=1}^T$. 
Our training objective for video summarization is an averaged binary cross-entropy loss, as following:
\vspace{-0.3em}
\begin{equation}
\small
    \mathcal{L}_{v} = -\frac{1}{T}\sum^{T}_{i=1} (\hat{y}_i\cdot \log(p_i) + (1 - \hat{y}_i)\cdot \log(1- p_i)),
\end{equation}
where $\hat{y_i} \in \{0,1\}$ denotes whether the $i$-th frame is a key frame, and $p_i$ indicates the predicted importance score of the $i$-th frame. Finally, we select the top 15\% of frames to attain a video-sum result $\mathcal{V}_\text{sum}$ in Eq.(\ref{eq:vsum}) from a long video. 



\input{tables/videoxum_table}

\vspace{-2mm}
\subsubsection{Text-Sum Decoder}
\vspace{-1mm}
The pretrained BLIP text decoder is a strong baseline for
text generation. The text summarization decoder $g_t(\cdot; \theta_{t})$ contains $N_{tex}$ stacked Transformer decoder blocks with cross-attention modules. During the decoding process, the text decoder takes a prompt sequence $\mathcal{T}_\text{prompt}$, 
 and the video features $\tilde{\mathcal{Z}}=\{\tilde{\bz_i}\}^T_{i=1}$ from the video encoder as inputs and then generate the final text summary $\mathcal{T}_\text{sum}$ in Eq.(\ref{eq:tsum}).
The training objective of text summarization is negative log-likelihood (NLL), which can be expressed in the equation as:
\vspace{-3mm}
\begin{equation}
    \mathcal{L}_{t} = - \sum^{N_{tex}}_{i=1} \log P(w_i\vert w_0, w_1,\dots, w_{i-1}, \tilde{\mathcal{Z}}).
\vspace{-2mm}
\end{equation}
where $w_i$ denotes the $i$-th word in the sentence, $N_{tex}$ is the length of output sequence.

\subsection{Overall Objective}
\vspace{-1mm}
Following the multi-task learning paradigm, the overall objective of our proposed framework is calculated as the integration of video-sum loss $\mathcal{L}_v$ and text-sum loss $\mathcal{L}_t$:
\vspace{-1mm}
\begin{equation}
    \mathcal{L} = \lambda_v \mathcal{L}_v + \lambda_t\mathcal{L}_t,
\vspace{-1mm}
\end{equation}
where $\lambda_v$ and $\lambda_t$ are the weight of different summary tasks.