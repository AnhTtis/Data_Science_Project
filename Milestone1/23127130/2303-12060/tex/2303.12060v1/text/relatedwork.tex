\section{Related Work}

\input{tables/dataset} % Table 1

\subsection{Video Summarization}
 Video summarization datasets, e.g., SumMe \cite{gygli2014creating}, TVSum~\cite{song2015tvsum}, and VSUMM \cite{de2011vsumm} have enabled the development of state-of-the-art video summarization models. Among these models, vsLSTM \cite{zhang2016video} first attempted to learn frame importance by modeling the temporal dependency among frames using LSTM \cite{graves2012long} units. The model can be combined with a determinantal point process (DPP) to improve the diversity of generated video summary. Following vsLSTM, several other approaches were proposed  to model the temporal dependency, e.g., H-RNN \cite{zhao2017hierarchical}, Hsa-RNN\cite{zhao2018hsa}, TTH-RNN \cite{zhao2020tth}, DASP \cite{ji2020deep}. Another solution models the spatiotemporal structure of the video to learn frame importance, such as MerryGoRoundNet \cite{lal2019online}, and CRSum \cite{yuan2019spatiotemporal}. Adversarial learning-based methods~\cite{fu2019attentive,zhang2019dtr} can also perform well in video summarization.


\subsection{Video Captioning}
Video Captioning aims to automatically generate short descriptions for a video by understanding the action and event in a video, which can help retrieve videos efficiently through text queries. Existing benchmarks (e.g., YouCook2 \cite{zhou2018towards}, DiDeMo \cite{hendricks2018localizing}, MSR-VTT \cite{xu2016msr} ActivityNet Captions \cite{krishna2017dense}, VATEX \cite{wang2019vatex}, and MSVD\cite{chen-dolan-2011-collecting}) have helped to promote the ability of language models to generate reasonable captions for video. Benefiting from these large-scale datasets, many novel approaches are proposed and achieve state-of-the-art performance. MDVC \cite{iashin2020multi} utilizes audio and
speech modalities to improve dense video captioning.


\subsection{Multimodal Pretraining}
Large language models (LLMs) \cite{brown2020language,devlin2018bert,lewis2019bart,raffel2020exploring} have revolutionized NLP research in recent years. Following the large-scale pretraining models in the field of NLP, numerous works \cite{kim2021vilt,wang2020minilm,xue2021probing,zhang2021vinvl} on exploring the combination of vision and language (VL) pretraining have achieved great success. Since then, image-text pretraining has become
a default approach to tackling VL tasks \cite{biten2019scene,lin2014microsoft,regneri2013grounding,singh2019towards}. In addition, the introduction of Vision Transformers \cite{dosovitskiy2020image} enables vision and language modalities to be jointly modeled by Transformers in a more scalable fashion \cite{alayrac2022flamingo,wang2022git,yu2022coca,yuan2021florence}. According to the encoding strategies for image and language modalities, VL models can be categorized into fusion encoder \cite{li2019visualbert,lu2019vilbert,su2019vl,tan2019lxmert}, dual encoder \cite{radford2021learning}, and a combination of both \cite{bao2021vlmo,du2022survey,singh2022flava}. Several pretrained vision and language models have also shown strong performance on video captioning and other tasks, such as HERO \cite{li2020hero}, VideoBERT \cite{sun2019videobert}, and UniVL \cite{luo2020univl}.