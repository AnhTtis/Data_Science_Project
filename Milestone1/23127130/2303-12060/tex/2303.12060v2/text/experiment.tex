\section{Experiments}
\vspace{-1mm}
In this section, we first introduce the baseline models and experimental setup for the proposed VideoXum dataset. Then we introduce the evaluation metrics and human evaluation strategy. In addition, we report several baseline models' performances under different settings and present a comprehensive experimental analysis to prove the effectiveness of our proposed method. 

\subsection{Baseline Models}
\vspace{-1mm}

We introduce all the baseline models listed in Table~\ref{tab:videoxum_table}:

\noindent {\bf Frozen-BLIP} refers to inference over the test set using a frozen BLIP model without training. We take this zero-shot setting performance as a lower bound for our benchmark. 

\noindent {\bf VSUM-BLIP (Base)} is a baseline model to perform video-to-video summary. It consists of a frozen BLIP encoder and a learnable video-sum decoder (i.e., a task-specific head over the encoder).

\noindent {\bf TSUM-BLIP (Base)} is a video-to-text summary baseline. We employ the vanilla BLIP model with a frozen encoder.

\noindent {\bf VTSUM-BLIP (Base)} combines the VSUM-BLIP and TSUM-BLIP modules of the model. It is comprised of a shared frozen encoder and two task-specific decoders. 

\noindent {\bf Temporal Transformer (TT)} is a crucial module to achieve the hierarchical encoding for videos while incorporating temporal information into a video sequence. Specifically, we use several Transformer layers combined with temporal positional embedding to model the temporal information.

\noindent {\bf Context Aggregation (CA)} is a plug-and-play module to model the video frame representations for the V2VSum task. Compared with the baseline models, this mechanism enhances the local context information for video representations and could help reduce the redundancy of video summaries.


\subsection{Experimental Setup and Implementation}

 Our model is implemented in PyTorch. We use the AdamW~\cite{loshchilov2018decoupled} optimizer with an initial learning rate of $\{1\times10^{-5}, 2\times10^{-5}, 5\times10^{-5}\}$ to optimize the model, and the $\beta_1=0.9, \beta_2=0.999$. The batch size is \{16, 32, 64\}, and weight decay is $5\times10^{-2}$. The learning rate follows a cosine decay schedule~\cite{loshchilov2016sgdr} with the minimum learning rate of $0.0$. We train all baseline models for 56 epochs on 4 A100 GPUs. We sample frames from the input videos at 1 fps with a uniform sampling schema for all the experimental settings. The various lengths of videos in a mini-batch are padded to equal in length. According to the average length of text summaries in Section~\ref{sec:dataset}, we set the maximum generation length to 128 in the text summarization task. Following previous works~\cite{gygli2014creating,song2015tvsum}, we take the video frames with predicted scores in the top 15\% as the video summary. 
More details are described in Appendix~\ref{sec:supp}.

\subsection{Evaluation}

\vspace{-1mm}
\noindent \textbf{Video Summary Evaluation}. Following previous works~\cite{narasimhan2021clip,otani2019rethinking,saquil2021multiple} for video summarization evaluation, we adopt the F1 score, Kendall's $\tau$~\cite{kendall1945treatment}, and Spearman's $\rho$~\cite{zwillinger1999crc} as our automatic evaluation metrics.


\noindent \textbf{Text Summary Evaluation}. To evaluate the quality of generated text summaries for video {\bf text summary}, we adopt several metrics for video captioning evaluation \cite{xu2023mplug} including: BLEU~\cite{papineni2002bleu}, METEOR~\cite{banerjee2005meteor}, ROUGE-L~\cite{lin2004rouge}, CIDEr~\cite{vedantam2015cider}.


\noindent \textbf{Video-text Semantic Consistency Evaluation}. Apart from independently evaluating the single-modal summaries, we also evaluate the {\it semantic consistency} of text and video summaries. Inspired by the previous work \cite{radford2021learning,singer2022make,wu2021godiva}, we design an evaluation metric -- VT-CLIPScore for evaluating the text and video semantic consistency. Specifically, we finetune the vanilla CLIP model on our dataset with contrastive learning strategies that can better fit our task. The empirical results in Table \ref{tab:clipscore} show that our proposed VT-CLIPScore is sensitive enough to the semantic change of video and text. Moreover, the results in Table \ref{tab:he}  indicate the high consistency of our proposed automatic evaluation metric with human evaluation. More details are described in Appendix~\ref{sec:supp}.


\input{tables/v2vsum_table}

\subsection{Results on VideoXum}
We conduct experiments on VideoXum using different baseline models. Table~\ref{tab:videoxum_table} shows the empirical results of the models on VideoXum. By comparing VSUM-BLIP (Base), TSUM-BLIP (Base), and VTSUM-BLIP (Base) with Frozen-BLIP, BLIP models show better results after finetuning on specific tasks.
In all three tasks, the model with TT can help model the video sequence better, indicating that temporal information is necessary. The CA module can enhance the local information awareness of the model, which can help improve the performance of the V2V-SUM task. The performance gains of TT on V2V-SUM are more significant than that on V2T-SUM, one of the possible reasons is that the text decoder is a well-generalized model trained on a large corpus and is insensitive to the subtle changes of the input features \cite{arora2018stronger,hua2021noise,hua2022fine}. A Base BLIP model combined with TT and CA achieves the state-of-the-art in our proposed three tasks. From the overall performance,  our proposed multitask framework can benefit both the video and the text summarization tasks. In addition, Table~\ref{tab:videoxum_table} also shows the human performance on VideoXum. The result is obtained on human-annotated reference summaries using a leave-one-out strategy~\cite{otani2019rethinking}, which measures the average consistency of human annotators. Although humans outperform all baseline models in most evaluation metrics on different tasks (except for BLEU@4), our proposed VTSUM-BLIP archives quite competitive results, especially on the V2T-SUM task. Figure \ref{fig:vis} visualizes some examples of generated video and text summaries.


\input{tables/v2tsum_table}

% Figure 4
\input{figures/visualize}

\subsection{Experimental Analysis}

\subsubsection{Method Comparisons on Existing Benchmarks}

\textbf{TVSum and SumMe.}
To further evaluate the effectiveness of our proposed model, we conduct experiments on two well-known video summarization datasets TVSum and SumMe. The results in Table \ref{tab:v2vsum_table} show that the BLIP-VSUM (Base) achieves competitive results against several strong baselines. Moreover, our proposed mechanisms of Temporal Transformer and Context Aggregation can further improve video summarization performance.



\paragraph{ActivityNet Captions.}
We also verify the ability of video-to-text summarization of our model on ActivityNet Captions. As we can see from Table \ref{tab:v2tsum_table}, our proposed model outperforms all the strong baseline models by a large margin in multiple evaluation metrics (2.9 in BLEU@4, 1.0 in METEOR, 7.4 in ROUGE-L, and 19.0 in CIDEr).



\vspace{-2mm}
\subsubsection{Human Evaluation}
\vspace{-2mm}
We conduct the human evaluation for each task to further evaluate  the quality of generated video and text summaries. Specifically, we randomly sample 50 examples from the generated summaries and then hire crowd workers to evaluate the quality of generated summaries. The crowd workers are instructed to compare the predicted video and text summaries with the source video and rate the multimodal summaries from three aspects: the quality of video summaries, the quality of text summaries, and the consistency of video and text summaries. The scores range from 1 to 5, and 5 is the best. We report the average score in Table \ref{tab:he}. We can conclude from the table that our proposed model can generate more fluent and accurate text summaries for long videos. The proposed temporal Transformer and Context Aggregation can help generate accurate and consistent video summaries. Following \cite{wang2019controllable}, we compute the Kappa coefficient of different workers, and the value is 0.49 $\in$ (0.41, 0.60),
which means that the consistency is moderate.


\input{tables/human}

\input{tables/clipscore}

\subsubsection{Adapted VT-CLIPScore}
Although we can apply a pretrained CLIP model without any adaptation on our dataset to evaluate the semantic consistency of the video and text summaries, the similarity score may be insensitive to the semantic change of generated cross-modal summaries. In Table \ref{tab:clipscore}, we compare the vanilla CLIP model and the finetuned CLIP model to measure the similarity of different video and text summarization pairs. The positive pairs refer to the paired video and text summaries. The negative pair includes unpaired video and text summaries. The Shuffle-words refer to the pairs of video summary and shuffled text summary. From the results, we can conclude that fine-tuning CLIP models on our dataset is necessary and makes the similarity scores more reflective or informative in measuring the semantic consistency of cross-modal summaries.



