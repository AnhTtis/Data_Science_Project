\vspace{-2mm}
\section{Introduction}
\label{sec:intro}

Traditional video summarization aims to distill the most important information from a source video to produce an abridged version for particular users and tasks. Typical approaches to video summarization  extract essential clips or frames from a given long video \cite{apostolidis2021combining,perrett2019ddlstm, zhu2022relational,zhu2020dsnet}. Alternatively, the principal video content can also be summarized in natural language, e.g., video captioning~\cite{de2011vsumm, gygli2014creating,song2015tvsum}. However, previous works treat either video or textual summarization as separate tasks and thus ignore bridging the two modalities of summarization tasks together. There is an earlier attempt \cite{chen2017video} to simultaneously generate video and text summaries from long videos. However, the generated video and text summaries in this work are not guaranteed to be semantically aligned because the two tasks were treated as separate, and there were no paired video and text summarization data for training or testing.

% Figure 1
\input{figures/v2x_sum}

In this study, we first propose \textbf{VideoXum}, an enriched large-scale dataset for cross-modal video summarization. The dataset is built on ActivityNet Captions \cite{krishna2017dense}, a large-scale public video captioning benchmark. We hire workers to annotate ten shortened video summaries for each long source video according to the corresponding captions. VideoXum contains 14K long videos with 140K pairs of aligned video and text summaries. Next, our goal is to extend the traditional single-modal video summarization task to a cross-modal video summarization task, referred to as V2X-SUM, to meet the demands of broader application scenarios (e.g., movie trailer generation and narrative generation). According to the target modality of the generated summaries, we categorize our proposed V2X-SUM task into three subtasks:


\noindent \textbf{Video-to-Video Summarization (V2V-SUM)}. This task requires models to identify the most important segments from the source video and generate an abridged version of the source video.


\noindent \textbf{Video-to-Text Summarization (V2T-SUM)}.
In this task, models need to summarize the main content of the source video and generate a short text description.


\noindent \textbf{Video-to-Video{\&}Text Summarization (V2VT-SUM)}. This task requires models to summarize a short video and the corresponding narrative from a source video simultaneously. Moreover, the semantics of these two modalities of summaries should be well aligned.

There are three potential challenges for the V2X-SUM task from the perspective of language model learning: First, simultaneously generating video and text summaries for a vision and language model could be quite challenging due to the time and GPU memory efficiency requirement for video encoding and the stability requirement for language model fine-tuning; Second, from the perspective of model design and optimization, it is nontrivial for the combined two tasks to learn and benefit from each other; Third, it is hard to guarantee the generated different modalities of the cross-modal summary are semantically aligned.

We propose VTSUM-BLIP, a novel end-to-end cross-modal video summarization model, to tackle the above challenges. To leverage the strong capability of vision understanding and language modeling of pretrained language models, we employ BLIP \cite{li2022blip} as our backbone. Inspired by HERO \cite{li2020hero}, we design an efficient hierarchical video encoding strategy with a frozen encoder and a temporal modeling module to encode long videos. We also design different task-specific decoders for video and text summarization. The modularized design enables us to perform more complex downstream tasks without changing the structure of the pretrained model. Our proposed framework achieves promising performance on VideoXum, as well as other existing single-modal video summarization datasets (e.g., TVSum~\cite{song2015tvsum}, SumMe~\cite{gygli2014creating}, and ActivityNet Captions~\cite{krishna2017dense}).

Several multimodal-based video summarization works \cite{ji2019video,narasimhan2021clip,perrett2019ddlstm,rochan2018video,zhu2020dsnet} exploit video captions to guide models to predict video summaries. Such methods usually design a pipeline that generates video captions using pretrained language models and then uses the generated caption as a prompt to improve the models' performance. However, these methods may suffer from bias accumulation issues since the hallucination \cite{ji2022survey} of pretrained language models may lead to the misalignment of semantics between the generated captions and video. In contrast, our proposed new dataset enables joint training of the video and text summarization tasks using an end-to-end model. The experiments show that our multitask training framework can improve the model's performance on both visual and textual summarization of videos.

Furthermore, we design a new metric -- VT-CLIPScore for evaluating the semantic consistency of cross-modal summaries. The empirical results show the consistency of the proposed new metric with human evaluation. 

Our main contributions can be summarized as follows: 
\vspace{-2mm}
\begin{itemize}
    \item We propose \textbf{VideoXum}, an enriched large-scale dataset, to bridge the modality gap between the video and text summarization. The dataset contains 14,001 long videos with corresponding human-annotated video and text summaries. We conduct comprehensive experimental analyses to verify the rationality of our proposed new dataset. 
    \vspace{-2mm}

    \item We propose a novel end-to-end video and text summarization model -- VTSUM-BLIP to address the  challenges of our proposed task of cross-modal summarization. The model achieves promising results on VideoXum and the new state of the art on several existing single-modal video summarization datasets. 
    \vspace{-2mm}

    \item We propose an evaluation metric VT-CLIPScore to evaluate cross-modal semantic consistency. The empirical results show the high consistency of our proposed metric with human evaluation.
\end{itemize}

