\appendix

In this document, we collect all the results and discussions, which, due to the page limit, could not find space in the main manuscript.
This supplementary material consists of two parts.
First, in \cref{suppsec:implementation_details}, we describe more implementation details mainly regarding our pilot study, local feature pre-training, and experiments on downstream deformable shape data.
Next, in \cref{suppsec:additional_results}, we present additional experimental results and analysis of our local feature pre-training strategy and its generalization in downstream tasks, including deformable shape matching and segmentation. 


\section{Implementation Details}
\label{suppsec:implementation_details}


\subsection{Feature Locality vs. Transferability}
\label{suppsubsec:feature_locality_vs_transferability}
In Sec.~3 of the main text, we conducted a pilot study on feature locality vs. transferability on deformable shapes.
We tested three different architectures for pre-training a \textit{local} feature extractor, and their details are as follows.

\mypara{SparseConv.}
We used the \texttt{ResNet14} architecture introduced in \cite{choy20194d}.
During pre-training, given a 3D point cloud $P$, a fixed-size local patch with a radius of 0.15 is cropped at point $\mathbf{p} \in P$ and then reoriented with a local reference frame (LRF) computed by the method in \cite{gojcic2019perfect} for rotation invariance.
The resulting local patch is fed to the sparse convolution network, which extracts a 32-dimensional feature vector for point $\mathbf{p}$.

\mypara{PCPNet.}
It is a variant of PointNet \cite{qi2017pointnet} endowed with a quaternion spatial transformer.
We used the single-scale architecture proposed by \cite{pcpnet2018}.
PCPNet is designed to be a local network requiring input patches to have a fixed number of points.
Thus during pre-training, a fixed-size local patch (radius = 0.15) is cropped at point $\mathbf{p}$ and reoriented by an LRF. 
The local patch is then resampled to 1,024 points and fed to the network, resulting in a 32-dimensional feature vector for point $\mathbf{p}$.

\mypara{3DCNN.}
We used the architecture from \cite{li2021updesc} with a learnable receptive field size and differentiable voxelization, the same as our \OurMethodName{} in Sec.~4.1 of the main text.
More details can be found in \cref{suppsubsec:local_feature_pretraining}.

\mypara{Dataset.}
We pre-trained the above local networks on the 3DMatch dataset, which is a collection of RGB-D scan datasets with 62 indoor scenes and 4,142 point cloud fragments. 
There are 13K points on average in a fragment after downsampling.

\mypara{Loss.}
We used the PointInfoNCE loss, in which 300 point correspondences were randomly sampled for a pair of point clouds for faster training and the temperature parameter $\tau$ was set to 0.07. 

We also used the cycle consistency loss $\mathcal{L}_c$. During pre-training, we use the extracted features to build correspondences for rigid alignment between shapes $P$ and $Q$.
The intuition for $\mathcal{L}_c$ is that the estimated transformation $(\mathbf{R}, \mathbf{t})$ aligning $P$ to $Q$ should be the inverse of the transformation $(\mathbf{R}', \mathbf{t}')$ aligning $Q$ to $P$.
Mathematically, this can be expressed as:

\begin{equation}
\begin{bmatrix}
\mathbf{R} & \mathbf{t}\\
\mathbf{0} & 1
\end{bmatrix}
\begin{bmatrix}
\mathbf{R'} & \mathbf{t'}\\
\mathbf{0} & 1 
\end{bmatrix}
=
\begin{bmatrix}
\mathbf{R}\mathbf{R'} & \mathbf{R}\mathbf{t'} + \mathbf{t}\\
\mathbf{0} & 1
\end{bmatrix}
= \mathbf{I}
\end{equation}

\mypara{Application to deformable shape matching.}
In Fig. 3 of the main text, we have shown the results of shape matching on the Faust Remeshed dataset, directly using the pre-trained feature extractors. Given two shapes $S_1$, and $S_2$, we compute their respective point-wise features $F_1$ and $F_2$ using a specific pre-trained model. We first produce an estimate of the point-to-point maps $T_{21}^{nn}$ and $T_{12}^{nn}$ using nearest neighbor search between $F_1$ and $F_2$. We then filter the correspondences by mutual check: a pair of points $x \in S_1, y \in S_2$ is considered to be in correspondence, if and only if in the feature space, $x$ is the nearest neighbor of $y$, and $y$ is the nearest neighbor of $x$. This results in two filtered maps $T_{21}^{mf}$ and $T_{12}^{mf}$. Finally, we further refine these two maps using the ZoomOut method \cite{Melzi_2019}, which is based on navigating between the spectral and spatial domains while progressively increasing the number of spectral basis functions. We emphasize that if the initial point-to-point map is noisy or contains strong ambiguities like symmetry ambiguities, ZoomOut is not able to remedy these errors, thus leading to final correspondences of bad quality. We perform 10 iterations of ZoomOut, starting from 30 eigenfunctions up to 100 eigenfunctions.



\subsection{Local Feature Pre-training}
\label{suppsubsec:local_feature_pretraining}
In Sec.~4.1 of the main text, we introduced our local feature pre-training strategy.

\mypara{Feature extraction.} We use $r_{\text{LRF}}=0.3$ and $\sigma=10^{-3}$ for differentiable voxelization \cite{li2021updesc}, and the voxel grid resolution is set to $16^3$.
We pre-trained on the 3DMatch dataset introduced in \cref{suppsubsec:feature_locality_vs_transferability}.

\mypara{Pre-training loss.} 
For the PointInfoNCE loss $\mathcal{L}_{\text{nce}}$, its settings are described in \cref{suppsubsec:feature_locality_vs_transferability}.
For the cycle consistency loss $\mathcal{L}_{\text{c}}$, 300 points were randomly sampled on each point cloud for feature extraction and alignment estimation.
A relaxation-based solver is used in $\mathcal{L}_{\text{c}}$ for estimating a 3D transformation between two point clouds, and its details can be found in \cite{li2021updesc}.
 
In the main text, we investigated the performance difference between the cycle consistency loss and PointInfoNCE loss w.r.t learned feature smoothness.
Suppose that $F \in \mathbb{R}^{m \times n}$ is the matrix of extracted $n$-dimensional point-wise features for a shape of $m$ vertices, we measure the Dirichlet energy as follows: 
\begin{equation}
    % E_{Dirichlet}(F) = \frac{1}{n} \sum_{i=1}^n \frac{F_i^{\top} W F_i}{F_i^{\top} A F_i},
    E_{Dirichlet}(F) = \frac{1}{n} \sum_{i=1}^n F_i^{\top} W F_i,
\end{equation}
where $F_i$ is the $i^{\text{th}}$ column of $F$, and $W$ is the standard stiffness matrix computed using the classical cotangent discretization scheme of the Laplace-Beltrami operator \cite{Pinkall1993}.



\subsection{Baselines}
In Sec.~5 of the main text, we tested our proposed \OurMethodName{} features against a wide spectrum of competitors, including both hand-crafted and learned features.

Specifically, the Heat Kernel Signature (HKS) and Wave Kernel Signature (WKS) features are both sampled at 100 values of energy \textit{t}, logarithmically spaced in the range proposed in their respective original papers.
SHOT descriptors are 352-dimensional, and we used the implementation from the PCL library \cite{Rusu_ICRA2011_PCL}.
PointContrast features are 32-dimensional, and we used the publicly available implementation and the pre-trained weights released by the authors\footnote{\url{https://github.com/facebookresearch/PointContrast}}.


\subsection{Downstream Shape Analysis Training}
In Sec.~5 of the main text, we used DiffusionNet on top of the baselines features and our \OurMethodName{} respectively, in both the shape matching and segmentation tasks. We employed the publicly available implementation of DiffusionNet released by the authors\footnote{\url{https://github.com/nmwsharp/diffusion-net}}.
Unless specified otherwise, in our experiments, we used four DiffusionNet blocks of width = 128. 
The DiffusionNet is trained by an ADAM optimizer \cite{kingma2017adam} with an initial learning rate of $10^{-3}$.

In Sec.~5.1 of the main text, we also used a point-wise MLP network on top of the baselines features and our \OurMethodName{} respectively for supervised shape matching.
For this, we use the same MLP architecture as in FMNet \cite{litany2017deep}. 
After computing the point features with the MLP, we use them to compute the predicted functional map $C_{pred}$ as in \cite{donati2020deep} and penalize its deviation from the ground-truth map $C_{gt}$ using the L2 loss: $L = \|C_{pred} - C_{gt}\|_2^2$.


\subsection{Computational Specifications}
All our experiments were executed using Pytorch \cite{NEURIPS2019_9015}, on a 64-bit machine, equipped with an Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz and an RTX 2080 Ti Graphics Card.

In terms of computational time, pertaining our method takes about 12 hours on a single RTX 2080 Ti Graphics Card, in contrast to the 64 hours required for PointContrast. The receptive field optimization takes about 20 minutes per dataset. For feature extraction, our method takes 3 seconds to extract local features for a 5000-vertex shape, which is on par with other local features like SHOT~\cite{tombari2010unique}, but slower than PointContrast (0.1s). Finally, the forward pass using \OurMethodName{} takes the same time as for all baseline features, e.g., 0.2 seconds per iteration for the unsupervised shape-matching experiment in Sec 5.1 of the main text.


\section{Additional Results and Analysis}
\label{suppsec:additional_results}

\subsection{Size of the learned receptive field}
Fig.~5 of our paper provides an illustration of the optimized receptive field in downstream tasks. In \cref{fig:receptive_field}, we include more visualizations for shape \textit{pairs} for both humans and animals.
Observe that the optimized receptive field indeed corresponds to interpretable concepts, such as the head or foot of a human, and is consistent across shape pairs.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.99\linewidth]{figures/patch_viz.pdf}
   \caption{Visualizing the optimized receptive field for shape pairs.}
   % \vspace{-0.6cm}
   \label{fig:receptive_field}
\end{figure}


\subsection{Human Shape Matching} 
\label{suppsubsec:human_matching}
In Sec.~5.1 of the main text, we performed unsupervised shape matching on the FAUST-Remeshed (FR), SCAPE-Remeshed (SR), and SHRECâ€™19 datasets (SH) and reported the matching performance in Tab.~1.
We provide additional quantitative results of the FR-SR and SR-FR settings in \cref{tab:unaligned_unsup_supp}.
Compared with the baseline features, our \OurMethodName{} has the best and most consistent performance in both settings.

\input{body/table_unsup_human_matching_supp}

% \input{body/table_sup_human_matching}


% \rev{In the main text, we also tested the robustness of our \OurMethodName{} features to the change of remeshing.
% To complement Fig.~6 of the main text, we provide additional experiments using the same training setup, by training on the FO dataset, and testing on the FR and SR datasets, respectively.
% \cref{tab:robust_connectivity} shows the evaluation results.
% We observe that our method consistently outperforms the competing features.
% This indicates that our \OurMethodName{} features are robust and descriptive under change of sampling and can generalize well across datasets (FO-SR setting).}

\subsection{Molecular Surface Segmentation} 
\label{suppsubsec:qualitative_evaluation}
In \cref{fig:rna_seg_qual}, we show qualitative results of RNA segmentation using DiffusionNet + \OurMethodName{}.
It can be seen that the challenging RNA molecules can be robustly segmented into functional components with our pre-trained features.

\input{body/figure_qualitative_rna}


\subsection{Human Shape Segmentation}
\label{suppsubsec:human_segmentation}

We performed an additional experiment on the human shape segmentation task.
We used the dataset introduced in \cite{maron2017convolutional}, which combines segmented human models taken from a variety of existing datasets.
We used the same train/test split of 380 training and 18 test shapes as in prior works.
We compared our \OurMethodName{} only with methods that used the original evaluation protocol as in \cite{maron2017convolutional}, i.e., without using post-processing and evaluating the results on the full shape resolution (techniques such as Mesh Walker \cite{lahav2020meshwalker} are thus excluded). 

We ran each experiment five times and report the mean and standard deviation of the accuracy in \cref{tab:human-segmentation}.  
Our \OurMethodName{} features achieve an accuracy of $92.4 \pm 0.25\%$, the state-of-the-art result on this dataset.
In \cref{fig:human_seg_qual}, we present qualitative results of human segmentation using DiffusionNet + \OurMethodName{}.
Note that the segmentation results are simply the network predictions, and we do not perform any complex post-processing to the segmentation.

\input{body/table_human_seg}

\input{body/figure_qualitative_human_supp}


% \subsection{Training with Limited Data}
% Train on small dataset to show how informative the features are

% we can obtain better results by training on a small dataset
% see table

% not worth it, experiment dropped

\subsection{Robustness to Noise}
We performed an additional experiment to evaluate the robustness of our features to noise. For this, we followed the same setup as in Sec.~5.1 of the main text and in \cref{suppsubsec:human_matching}, by performing unsupervised learning on FR and testing on SR with an increasing amount of noise as input.
We compared our method to the best three competing features. The results are shown in \cref{fig:noise_robust} - left. It can be seen that our features are more robust to noise, i.e., the performance does not vary much with different noise levels (the intensity of the noise can be seen in \cref{fig:noise_robust} - right), which is not the case with other features, such as SHOT, whose performance degrades very quickly.


\begin{figure}[t]
    \centering
    %\includegraphics[width=0.9\columnwidth]{figures/noise_robust.pdf}
    \includegraphics[width=1.0\columnwidth]{figures/noise_levels.pdf}
    \caption{Left: Evolution of the geodetic error as a function of different
input noise levels. Right: Qualitative visualization of noise levels.}
    \label{fig:noise_robust}
\end{figure}



\subsection{Convergence Speed}

In our experiments, we observed that our \OurMethodName{} descriptors take less time to train and facilitate learning. To demonstrate this, we show in \cref{fig:convergence_speed} the evolution of validation accuracy during learning of the RNA segmentation task (Sec. 5.2 of the main text). It can be seen that compared to the other features, VADER requires far fewer training iterations to achieve similar performance. This clearly indicates the better descriptiveness and generalizability of our features.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/eval_acc_seg_new.pdf}
    \caption{Evolution of the RNA segmentation accuracy on the validation set, during the training of DiffusionNet with different features.}
    \label{fig:convergence_speed}
\end{figure}





% \subsection{Ablation Study}
% \label{suppsubsec:ablation_study}

% In Sec.~5.5 of the main text, we investigated different 3D datasets for local feature pre-training and showed that local geometries in 3DMatch are richer than those in DFAUST.
% We performed PCA on the local patches of 3DMatch and DFAUST.
% For each dataset, we first randomly extracted 200K local patches. 
% We then encode each patch as a high dimensional vector by first orienting it using a local reference frame and then voxelizing it to a small 3D grid of resolution = $16^3$ using the method of \cite{gojcic2019perfect}. 
% The resulting vectors are 4096-dimensional and fed as input to PCA to analyze the internal richness and complexity of each dataset.
% In \cref{fig:pca_unexplained_supp}, we report the unexplained variance as a function of the number of principal components. It further confirms that 3DMatch is significantly more diverse than DFAUST, since more principal components are needed to explain its full variance. 

% \input{body/figure_pca_supp}

