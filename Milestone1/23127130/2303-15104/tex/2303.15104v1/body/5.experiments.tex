\section{Experiments}
\label{sec:experiments}

In this section, we provide extensive experiments to highlight the generalization power of our local feature pre-training and receptive field size optimization methods in a suite of downstream shape analysis tasks especially involving highly deformable, organic shapes.
We consider diverse benchmarks including human and partial animal shape matching as well as molecular surface segmentation.
We reuse the same pre-trained local feature extractor $\mathcal{F}_{s, \Theta}$ and then perform our receptive field size optimization once individually on each deformable shape dataset (\cref{subsec:local_feature_transfer}).

\mypara{Implementation.}
We denote our features as \OurMethodName{} for Voxelized Alignment-based DEscriptoR.
%We implemented our \OurMethodName{} pre-training and transferring pipeline with PyTorch \cite{NEURIPS2019_9015}.
In the pre-training stage, we use the 3DMatch dataset \cite{zeng20173dmatch} and train the local feature extractor $\mathcal{F}_{s, \Theta}$ for 16K steps.
We use the Adam optimizer with a learning rate of $10^{-3}$  for network weight update.
For receptive field size optimization, we use the same learning rate for Adam, and we take $n_s = 10^4$  extracted patches from the pre-training dataset. 
%Our code and data will be released after publication.

\mypara{Baselines.}
We compare a wide spectrum of hand-crafted and pre-trained features in deformable shape tasks.
For the hand-crafted features, we consider the Heat Kernel Signature (HKS)  \cite{sun2009concise}, Wave Kernel Signature (WKS) \cite{aubry2011wave}, and SHOT descriptors \cite{tombari2010unique}, as well as the straightforward vertex positions (XYZ).
For the pre-trained features, we use the PointContrast features learned with the PointInfoNCE loss (PCN) or a hardest-contrastive loss (PCH) \cite{xie2020pointcontrast}.

%\rev{TODO: comparison of Vader with NCE vs cycle-consistency loss in some downstream task?}

\subsection{Human Shape Matching}
\label{subsec:human_matching}

\input{body/table_unsup_human_matching}

\mypara{Unsupervised matching.}
We perform unsupervised shape matching \cite{roufosse2019unsupervised,eisenberger2020deep} on the FAUST-Remeshed (FR), SCAPE-Remeshed (SR), and SHREC'19 datasets (SH) \cite{Ren2019,Bogo2014,Anguelov2005,shrec19}, consisting of 100, 71, and 44 \emph{unaligned} human shapes in different poses, respectively. 
The same train/test splits in prior works \cite{sharma2020weakly,eisenberger2020deep} are adopted.
We feed the above baseline features and our \OurMethodName{} respectively as input to a surface learning backbone DiffusionNet \cite{sharp2020diffusionnet}, which produces a functional map \cite{ovsjanikov2012functional,litany2017deep} as output for a given pair of shapes.
We leverage the unsupervised functional map losses \cite{sharma2020weakly} for training the backbone.
The evaluation metric is the mean geodesic error of predicted maps with respect to the ground truth on unit-area shapes \cite{Kim2011}. We use X-Y to denote training on dataset X and testing on dataset Y.

%\maks{we don't mention what  FR-SH stands for. Also, we should highlight that this is a very difficult problem and existing unsuprevised methods have only been used on aligned data. this explains why most of the numbers in Table 1 are so bad.}

As shown in \cref{tab:unaligned_unsup}, our approach yields the best results for unsupervised shape correspondence in both FR-SH and SR-SH settings, while the other tested features fail to achieve reasonable matching performance.
We stress that this is a challenging test case as most existing unsupervised methods rely on aligned shapes, e.g.,  \cite{sharma2020weakly,eisenberger2021neuromorph}.
Note that the PointContrast features perform worse than the hand-crafted features, indicating the overfitting to the pre-training data distribution, as discussed in \cref{sec:motivation}, and thus the limited transferability.
Our approach also outperforms several recent unsupervised approaches in \cref{tab:unaligned_unsup}, including SURFMNET \cite{roufosse2019unsupervised}, Cyclic FMaps \cite{ginzburg2019cyclic}, WSupFMNet \cite{sharma2020weakly}, and Deep Shells \cite{eisenberger2020deep}. 
The comparisons clearly demonstrate the utility of incorporating generalizable pre-trained features, which is missing in prior works. 
We provide qualitative comparisons in \cref{fig:qual_all_human} (Top), showing that only our approach leads to visually plausible results.

\input{body/figure_qualitative_human}


\mypara{Robustness to meshing.}
We evaluate the performance of supervised shape matching \cite{donati2020deep} on the original FAUST (FO) dataset \cite{Bogo2014} and its remeshed version by quadratic error simplification (FQ) \cite{sharp2020diffusionnet} to demonstrate the robustness and generalization of our approach against significant mesh connectivity changes across datasets. 
We build a point-wise MLP network \cite{litany2017deep} (to reduce the dependence on the backbone architecture) on top of the baseline features and our \OurMethodName{} respectively, and then predict functional maps for shape pairs.
The MLP backbones are trained on FO with predictions supervised by the ground-truth maps with a simple $L_2$ loss.
We report the mean geodesic error metric on FQ.

\cref{fig:super_fo_fq} shows the correspondence quality with a varying error threshold. 
It can be seen that hand-crafted features such as SHOT degrade rapidly under remeshing. 
Although WKS and HKS are intrinsic features and do not depend on the meshing connectivity, they are not expressive enough by themselves and need to be combined with a powerful network backbone such as DiffusionNet, instead of the point-wise MLPs used in this experiment.
Differently, our approach achieves superior generalization performance in this challenging setting, showing that \OurMethodName{} is highly robust to remeshing and resampling, and effectively captures local geometric structures in deformable shapes.
\cref{fig:qual_all_human} (bottom) presents a qualitative comparison of the computed maps with the three best-performing competitors.

\input{body/figure_human_acc}

\subsection{Molecular Surface Segmentation}
\label{subsec:molecular_segmentation}

\input{body/table_RNA_segmentation}

Next, we conduct experiments in the molecular surface segmentation task, which aims to segment RNA molecules into functional components.
We use the dataset introduced in \cite{poulenard2019effective}, consisting of 640 RNA triangle meshes, where
each vertex is labeled into one of 259 atomic categories.
The dataset has an 80/20\% split for training and test sets.
We feed the baseline features and our \OurMethodName{} respectively as input to DiffusionNet and train it to predict a label at each vertex as output.
% Each experiment is run five times with different random initialization.
% Mean accuracy and its standard deviation are reported.

As shown in \cref{tab:rna_seg}, our approach achieves state-of-the-art segmentation performance when used with the full training set, outperforming both hand-crafted and pre-trained PointContrast features as well as several recent shape segmentation networks, such as \cite{kostrikov2018surface}.
We also perform experiments in more challenging settings, where only a fraction of the training set, with respectively 50 and 100 shapes (corresponding to 9\% and 18\% of the training set), is used.
We observe in \cref{tab:rna_seg} that our method consistently outperforms the competitors by a significant margin when given limited training data.
The results highlight that our pre-training and receptive field size optimization strategies bring significant improvement to downstream organic shape analysis tasks.

\subsection{Partial Animal Matching}
\label{subsec:animal_matching}
We also evaluate how well different geometric features perform on deformable shapes in the presence of significant partiality.
For this, we test on the challenging SHREC16' Cuts dataset \cite{cosmo2016shrec}, where the animal classes (cat, centaur, dog, horse, and wolf) are used for partial shape matching.
We follow the setup of DiffusionNet described in \cref{subsec:human_matching} for correspondence prediction. 

We compare our approach to XYZ and SHOT as they are widely used in partial matching pipelines, in addition to full-fledged methods PFM \cite{Rodol2016} and FSP \cite{Litany2017}, specifically tailored toward partial shape matching. 
The results are summarized in \cref{tab:sup_animal}. Qualitative results are visualized in \cref{fig:qual_sup_animal}. Observe that our approach outperforms the competitors in this setting by a significant margin, including specially-tailored partial matching methods PFM and FSP.

\input{body/table_sup_animal}

\input{body/figure_qualitative_animal}

\subsection{Shape Classification}
\label{subsec:shape_classification}

% Finally, to show the utility of our pre-trained feature extractor on man-made objects, we adopt the ShapeNet \cite{chang2015shapenet} classification setup from PointContrast \cite{xie2020pointcontrast}, where pre-trained weights are used as initialization for fine-tuning a classification network.
% For comparison with PointContrast, we conduct experiments on the ShapeNetCore v2 dataset with the same train/test split, and our feature extractor pre-trained with the PointInfoNCE loss is used.
% In addition to fine-tuning on the full training set, a limited training data setup (i.e., 1\% or 10\%) is also considered.

% \cref{table-shapenet_classification_accuracy_retrain} shows the classification accuracy comparison.
% We observe that feature pre-training generally improves the performance across different training setups, compared to training from scratch for the classification networks.
% In particular, our network has better classification accuracy than PointContrast when using 1\%, 10\%, or 100\% training data.

We use the ShapeNet dataset \cite{chang2015shapenet} to demonstrate the effectiveness of our pre-trained feature extractor on man-made objects. We follow the classification setup from PointContrast \cite{xie2020pointcontrast}, using pre-trained weights as initialization for fine-tuning a classification network. Here our feature extractor is pre-trained with the PointInfoNCE loss. We conduct experiments on the ShapeNetCore v2 dataset with the same train/test split as PointContrast. We also consider a limited fine-tuning data setup, using a fraction of the fine-tuning data (1\% or 10\%). The classification accuracy comparison is summarized in \cref{table-shapenet_classification_accuracy_retrain}. It can be seen that feature pre-training improves performance compared to training from scratch. Also, our network achieves higher classification accuracy than PointContrast in all training setups.

\input{body/table_shapenet_classification}




\subsection{Ablation Study}
\label{subsec:ablation_study}

\input{body/table_dataset_ablation}

We also evaluate the role of the pretraining dataset for local feature learning. 
For this, we compare 3DMatch used in our experiments to DFAUST \cite{dfaust:CVPR:2017}, a large-scale dataset of human subjects in motion.
We use the unsupervised shape matching task and the evaluation protocol introduced in \cref{subsec:human_matching} to test the generalization of a pre-trained feature extractor to the FR and SR datasets.



% \begin{wrapfigure}[8]{r}{0.4\columnwidth}
%     \centering
%     \includegraphics[width=0.4\columnwidth]{figures/pca_projection.png}
%     \vspace{-10pt}
% \end{wrapfigure}

The comparisons in \cref{tab:dataset_ablation} show that pre-training on 3DMatch leads to more generalizable features and consistent matching performance, even though DFAUST has greater similarity to the downstream human shape datasets FR and SR.
We attribute this to the fact that \textit{local geometries }in 3DMatch, which consists of real-world scans, are richer and more complex than those in template-fitted DFAUST, leading to a more universally useful pre-training signal.
% To validate this, we perform PCA \cite{pca01} on the local patches in 
% % here inset
% each dataset and visualize the projections to principal 
% components in the inset figure, showing that the local patches in DFAUST (red dots) are included in 3DMatch (blue dots).
% Please see the supplementary for more analysis and experiments.

To validate this, we perform PCA analysis \cite{pca01} on the local patches of 3DMatch and DFAUST.
For each dataset, we first randomly extract 200K local patches. We then encode each patch as a high dimensional vector by first orienting it using a local reference frame and then voxelizing it to a small 3D grid of resolution = $16^3$ using the method of \cite{gojcic2019perfect}. The resulting vectors are 4096-dimensional and are fed as input to PCA. In \cref{fig:ds_abla} (a), we report the unexplained variance as a function of the number of principal components. It can be seen that 3DMatch is significantly more diverse than DFAUST since more principal components are needed to explain its full variance. In \cref{fig:ds_abla} (b), we visualize the projection of patches in the first two principal components and observe that local patches in DFAUST (red dots) are included in 3DMatch (blue dots), demonstrating the diversity and richness of 3DMatch once more.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{figures/pretrain_ds_ablation.pdf}
    \caption{Comparing richness of local geometries in 3DMatch and DFAUST via PCA. (a) We perform PCA on sampled 3D local patches and plot the unexplained variance \wrt the number of principal components. (b) We project the local geometries onto the first two principal components.}
    \label{fig:ds_abla}
    \vspace{\figmargin}
\end{figure}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% We performed PCA on the local patches of 3DMatch and DFAUST.
% For each dataset, we first randomly extracted 200K local patches. 
% We then encode each patch as a high dimensional vector by first orienting it using a local reference frame and then voxelizing it to a small 3D grid of resolution = $16^3$ using the method of \cite{gojcic2019perfect}. 
% The resulting vectors are 4096-dimensional and fed as input to PCA to analyze the internal richness and complexity of each dataset.
% In \cref{fig:pca_unexplained_supp}, we report the unexplained variance as a function of the number of principal components. It further confirms that 3DMatch is significantly more diverse than DFAUST, since more principal components are needed to explain its full variance. 
