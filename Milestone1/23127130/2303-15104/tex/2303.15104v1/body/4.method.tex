\section{Methodology}
\label{sec:method}

In this section, we first briefly introduce our chosen local feature pre-training method in \cref{subsec:local_feature_pretrain}.
We then describe our novel differentiable method for optimizing the receptive field size of local features for transfer learning across datasets in \cref{subsec:local_feature_transfer}.

In the following, we represent our local feature extractor as $\mathcal{F}_{s, \Theta}$, where $s \in \mathbb{R}$ is a \emph{learnable} receptive field size (\cref{sec:motivation}) and $\Theta$ denotes the parameters of the network backbone.
Given a shape $P$ represented as either a triangle mesh or a point cloud and a point $\mathbf{p} \in P$, the feature extractor $\mathcal{F}_{s, \Theta}$ associates a feature vector to $\mathbf{p}$, which summarizes the local 3D geometry around $\mathbf{p}$.
The feature extractor is composed of two stages:
local patch extraction, denoted by $\mathcal{G}_{s}(\mathbf{p}, P)$,
and geometric feature extraction, denoted by $\mathcal{H}_{\Theta}(\mathcal{G}_{s}(\mathbf{p}, P)) = \mathcal{F}_{s, \Theta} (\mathbf{p}, P)$.

\subsection{Local Feature Pre-training}
\label{subsec:local_feature_pretrain}

\mypara{Feature extraction.}
Following the transferability study in \cref{fig:locality_vs_transferability}, we build $\mathcal{F}_{s, \Theta}$ by adapting the local feature extractor architecture from WSDesc \cite{li2021updesc}, which is based on a voxel-based representation \cite{zeng20173dmatch,gojcic2019perfect} and a 3D CNN for geometric learning.
\cref{fig:input_parameterization} illustrates the local patch extraction procedure.
A voxel grid $V_{\mathbf{p}}=\mathcal{G}_{s}(\mathbf{p}, P)$ is anchored at $\mathbf{p}$, whose size is determined by the optimizable $s$, to capture the local geometric structure.
$V_{\mathbf{p}}$ is reoriented by a local reference frame (LRF) for rotation invariance in local features \cite{gojcic2019perfect,li2021updesc}.
The voxel values in $V_{\mathbf{p}}$ are computed by a differentiable voxelization layer based on probabilistic aggregation \cite{li2021updesc} for back-propagating gradients to $s$.
Next, for geometric feature extraction, $V_{\mathbf{p}}$ is fed to a 3D CNN $\mathcal{H}_{\Theta}$, which consists of six convolutional layers with ReLU and normalization in-between \cite{gojcic2019perfect}.
The resulting feature vector $\mathcal{H}_{\Theta}(V_{\mathbf{p}})$ is 32-dimensional.

\input{body/figure_input_parameterization}

\mypara{Pre-training loss.}
We adopt the pretext task of matching local features for 3D scene alignment, as advocated in PointContrast \cite{xie2020pointcontrast}, to pre-train the feature extractor $\mathcal{F}_{s, \Theta}$.
This allows us to gain access to large-scale 3D training data, which has been absent thus far for deformable shape analysis.
In this work, we examine two different loss functions for pre-training: one is the PointInfoNCE loss $\mathcal{L}_{\text{nce}}$ (\cref{sec:motivation}), and the other is an unsupervised loss $\mathcal{L}_{\text{c}}$ based on cycle consistency, introduced in \cite{li2021updesc} and briefly described below for completeness.

Cycle consistency evaluates the extracted features in a rigid registration pipeline without using ground truth labels.
Given two shapes $P$ and $Q$ and their per-point features extracted by $\mathcal{F}_{s, \Theta}$, putative correspondences between $P$ and $Q$ are first constructed by differentiable nearest neighbor search \cite{plotz2018neural} in the learned feature space.
Then a 3D transformation aligning $P$ to $Q$ is estimated from the correspondences and represented as a rotation $\mathbf{R} \in \mathbb{R}^{3 \times 3}$ and a translation $\mathbf{t} \in \mathbb{R}^{3}$.
Let $\mathbf{R}'$ and $\mathbf{t}'$ denote the computed alignment, in the reverse direction, from $Q$ to $P$.
The cycle consistency loss enforces the composition of $(\mathbf{R}, \mathbf{t})$ and $(\mathbf{R}', \mathbf{t}')$ to be an identity matrix as follows:
\begin{equation}
    \label{eq:CycleConsistencyLoss}
    \mathcal{L}_{\text{c}} = \| \mathbf{R}\mathbf{R}' - \mathbf{I} \|_1 + \| \mathbf{R}\mathbf{t}' +  \mathbf{t}\|_1,
\end{equation}
where $\mathbf{I}$ is an identity matrix.
We refer the reader to \cite{li2021updesc} for more details.

As shown in \cref{fig:locality_vs_transferability}, pre-training with $\mathcal{L}_{\text{c}}$ leads to local features having significantly better transferability on FR, compared to pre-training with $\mathcal{L}_{\text{nce}}$.

This is remarkable as the PointInfoNCE loss is a go-to choice for representation learning across many domains and application scenarios \cite{Feyetal2020,chen20214dcontrast,li2022srfeat}. We ascribe this performance difference partly to the fact the PointInfoNCE loss focuses on whether \textit{individual} point correspondences are correct or not, and does not consider local or global feature consistency \cite{li2022srfeat}.
In contrast, the cycle consistency loss jointly assesses all the extracted local features and thus imposes correspondence consistency between $P$ and $Q$ for successful global alignment. As a result, $\mathcal{L}_{\text{c}}$ promotes more consistent (and smoother) local features to be learned for nearby points than $\mathcal{L}_{\text{nce}}$.

To validate this hypothesis, we measure the Dirichlet energy \cite{pinkall1993computing} of the pre-trained features (the lower, the smoother) on the human shapes of FR, resulting in 86.8 for $\mathcal{L}_{\text{nce}}$ and 75.4 for $\mathcal{L}_{\text{c}}$.
These measurements confirm that the local features pre-trained by the cycle consistency loss are smoother (and thus more consistent) than those by the PointInfoNCE loss, which can potentially benefit downstream applications, especially across different datasets. 

\subsection{Local Feature Transfer}
\label{subsec:local_feature_transfer}

As we demonstrated in \cref{sec:motivation}, the size of the receptive field is of crucial importance for downstream tasks. In this section, we introduce a receptive field optimization method, which automatically determines the best receptive field for each downstream dataset.
To this end, we take advantage of the full differentiability of our network design and optimize the learnable receptive field size $s$, so that the distribution of features extracted on the target dataset is similar to that of features extracted by the pre-trained network on the pre-training dataset.

We draw inspiration from the domain adaptation literature \cite{kundu_cvpr_2020,liu2021sourcefree,liang2020really,huang2021model}, where the main goal is to transfer knowledge from a network trained in a domain with abundant labeled data to a domain of interest with minimal or unlabeled data. This is achieved by minimizing the \textit{domain shift} between the feature distribution of the two domains using some distance in the probability measure space. Thus we propose to  minimize the discrepancy between the distribution of features extracted from the pre-training dataset, and the distribution of features extracted from the target (downstream) dataset. For this, we base our approach on the Maximum Mean Discrepancy (MMD) used in previous works \cite{liu2021sourcefree,qin2019pointdan,6751384,Borgwardt2006IntegratingSB}.
Intuitively, networks are pre-trained to have a strong response signal under the distribution of training local patches, and thus our goal is to optimize the local patch sizes, given \textit{unlabeled} downstream data, so that the extracted features have a similar distribution.

In practice, after pre-training the feature extractor, we sample $n_s$ points from the pre-training (source) dataset and extract the features from these points with the pre-trained network. We denote these features as $F^s = \{f_i^s\}_{i=1..n_s}$.
For each new downstream (target) dataset, we formulate an optimization problem, where we freeze the network backbone weights $\Theta$ but optimize the receptive field size $s$, so that the network produces features as close to $F^s$ as possible, using the MMD metric. Our network is fully differentiable, and we solve this problem via gradient descent to find the optimal receptive field $s$. This optimized receptive field is then used for all downstream tasks involving this dataset. At each iteration during optimization, we randomly sample $n_t$ points from the target dataset and compute features for them with the pre-trained network. We denote these features as $\{f_i^t\}_{i=1..n_t}$ and formulate the loss as follows:
\begin{align}
	\nonumber
E_{mmd} =& \frac{1}{n_s n_s} \sum_{i, j=1}^{n_s} \kappa(f_i^s, f_j^s) + \frac{1}{n_s n_t} \sum_{i, j=1}^{n_s, n_t} \kappa(f_i^s, f_j^t) \\
&+ \frac{1}{n_t n_t} \sum_{i, j=1}^{n_t} \kappa(f_i^t, f_j^t),
	\label{eq:loss_mmd}
\end{align}
where $\kappa$ is the Radial Basis Function (RBF) kernel.
The Adam optimizer \cite{kingma2017adam} is used in gradient descent.
 
% Since our chosen network is fully differentiable, it allows to optimize the scale of input shapes and thus the receptive field of local patches. This would be difficult to achieve with point-based networks, where receptive field changes introduce new points into the patch in a non-differentiable way. We demonstrate this property in the case of RNA segmentation in \cref{subsec:rna_seg}.

\input{body/figure_patch_size}
