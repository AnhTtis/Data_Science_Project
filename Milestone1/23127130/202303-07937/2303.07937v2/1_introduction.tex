\section{Introduction}     

Text-to-3D generation, the task of generating realistic 3D models from texts~\cite{Sanghi_2022_CVPR, liu2022iss, jain2022zero}, has rapidly grown in recent years to become an essential problem in computer vision and graphics. Recent approaches~\cite{poole2022dreamfusion,wang2022score,lin2022magic3d,or2022stylesdf} incorporate generative models with neural radiance field (NeRF)~\cite{mildenhall2020nerf} to recover more plausible 3D scenes. Recent works along this line, such as DreamFusion~\cite{poole2022dreamfusion} and Score Jacobian Chaining (SJC)~\cite{wang2022score}, utilize large-scale diffusion models~\cite{rombach2022high,saharia2022photorealistic} as a prior for optimizing NeRF. This methodology, named score distillation~\cite{poole2022dreamfusion}, enables successful generation of a realistic 3D scene from a given text prompt.          

However, 3D scenes generated by score distillation~\cite{poole2022dreamfusion} are often prone to suffer from distortions and artifacts and sensitive to text prompts and random seeds, as shown in ``Baseline" results of \textit{``a photo of cute hippo"} in Fig.~\ref{fig:fig1}. One such failure is a 3D-incoherence problem, in which the rendered 3D scenes produce geometric features that belong to the frontal view (such as a face) multiple times at various viewpoints, as described in (a) of Fig.~\ref{fig:fig2}. This failure occurs due to the 2D diffusion model's lack of awareness regarding 3D information, especially the camera pose. Since the diffusion model does not know from which direction the object is viewed, it is easily biased to produce frontal geometric features at all viewpoints, including side and back views, resulting in heavy distortions to the 3D scene.       

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig/arxiv_fig1.pdf}\vspace{-5pt}
\caption{\textbf{Teaser.} \ours enables view-consistent 3D generation given text prompt, while baseline (SJC~\cite{wang2022score}) often fails to recover plausible 3D.}
\label{fig:fig1}
\vspace{-5pt}
\end{figure}

An ideal solution to address this issue would be using a 3D aware diffusion model that can generate 3D-consistent images for any given scene viewpoint. However, naively training such a model from scratch would require extensive 3D data, whose size is minuscule compared to vast amounts of 2D data~\cite{schuhmann2022laion}. In addition, this approach cannot benefit from the powerful generalization capabilities of diffusion models trained on large 2D data. In light of this, a practical solution would be a middle-ground approach that combines the best of both worlds - a pretrained 2D diffusion model imbued with 3D awareness suitable for 3D-consistent NeRF optimization.

In this paper, we propose a novel framework, named \ours, that effectively injects 3D awareness into pre-trained 2D diffusion models. Given a text prompt, we first sample semantic code to fasten the semantic identity of the generated scene. The semantic code consists of a generated 2D image and a prompt embedding optimized from the pretrained diffusion model. Our consistency injection module takes this semantic code and obtains a viewpoint-specific depth map: a coarse 3D geometry constructed by an off-the-shelf model~\cite{nichol2022point,ranftl2021vision,wu2023multiview} from the generated image is projected to given viewpoint to build the depth map. The module then leverages the coarse depth map and the semantic code to inject 3D information into the diffusion model. As the predicted 3D geometry is bound to have errors, our module is able to handle the errors and coarseness within the depth maps. To this end, we introduce a sparse depth injector to implicitly correct erroneous depth information and LoRA \cite{hu2021lora} adaptation to preserve the semantics consistently. Our framework achieves significant improvement over previous works in generation quality and geometric consistency.

We summarize our contributions as follows: \vspace{-5pt}
\begin{itemize}
    \item We propose a novel framework, called \ours, that infuses 3D awareness into a pretrained 2D diffusion model, preserving the original generalization capability.\vspace{-5pt}
    \item By distilling the score of the diffusion model that produces 3D-consistent image, \ours stably optimizes NeRF for view-consistent text-to-3D generation. \vspace{-5pt}
    \item We demonstrate the effectiveness of our framework qualitatively, and introduce a new metric for quantitative evaluation of 3D-consistency. 
\end{itemize}

% by incorporating Stable Diffusion~\cite{rombach2022high} into our 3DFuse framework for text-to-3D generation by score distillation. Our method generates high-quality 3D content with consistency and fidelity compared that surpasses previous methods. 