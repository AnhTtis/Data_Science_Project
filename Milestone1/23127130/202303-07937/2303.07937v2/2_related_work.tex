\section{Related work}

\paragraph{Diffusion models.}
Diffusion models~\cite{ho2020denoising,song2021denoising,song2020score} have gained much attention as generative models due to their stability, diversity, and scalability. Given these advantages, diffusion models have been applied in various fields, such as image translation~\cite{rombach2022high,su2022dual,seo2022midms}, image editing~\cite{meng2021sdedit, Kim_2022_CVPR}, and conditional generation~\cite{kanizo2013palette,rombach2022high,zhang2023adding}. Especially, text-to-image generation has been highlighted with the introduction of various guidance techniques~\cite{ho2021classifier,dhariwal2021diffusion,hong2022improving}. GLIDE~\cite{nichol2021glide} utilizes CLIP~\cite{radford2021learning} guidance to enable text-to-image generation, followed by large-scale text-to-image diffusion models such as Imagen~\cite{saharia2022photorealistic}, DALL-E2~\cite{ramesh2022hierarchical}, and Stable Diffusion~\cite{rombach2022high}. The emergence of such models has led to the widespread utilization of pretrained text-to-image models for the tasks such as endowing additional conditions~\cite{wang2022pretraining,zhang2023adding} or performing manipulations~\cite{gal2022image,kwon2022diffusion,ruiz2022dreambooth}. 


\paragraph{3D shape synthesis.} 3D shape synthesis is a task of constructing a 3D representation (\eg, voxel grid~\cite{Tatarchenko_2017_ICCV, li2017grass}, mesh~\cite{gao2019sdm, nash2020polygen, Henderson_2020_CVPR, NEURIPS2020_1349b36b, gao2020tmnet}, point cloud~\cite{Luo_2021_CVPR, Zhou_2021_ICCV, zeng2022lion}, implicit fields~\cite{Wu_2020_CVPR, Luo_2021_ICCV, zheng2022sdfstylegan, wu2022learning, Cheng2022-edit3d}) from a given input, usually a single image. Early approaches such as PrGAN~\cite{gadelha20173d} and PointFlow~\cite{yang2019pointflow} reconstruct single images into voxel and point cloud, while some works address specific tasks such as human mesh reconstruction~\cite{kolotouros2019convolutional, hmrKanazawa17, kolotouros2019spin, joo2020eft}. Recent works~\cite{nichol2022point, wu2023multiview} employ large generative models trained on large-scale data to achieve breakthroughs in 3D shape generation: Point-E~\cite{nichol2022point} directly trains a diffusion model on point cloud data for a conditional point cloud generation. Another work, MCC~\cite{wu2023multiview}, trains a masked autoencoding architecture on a large-scale 3D dataset Co3D~\cite{reizenstein2021common} for point cloud construction from single images.

\paragraph{Text-to-3D generation.} Existing text-to-3D generation methods generally employ pretrained vision-and-language models, such as CLIP~\cite{radford2021learning} to generate 3D shapes and scenes from text prompts. DreamFields~\cite{jain2022zero} incorporates CLIP with neural radiance fields (NeRF)~\cite{mildenhall2020nerf} demonstrating the potential for zero-shot NeRF optimization using only CLIP as guidance. Recently, Dreamfusion~\cite{poole2022dreamfusion} and SJC~\cite{wang2022score} have demonstrated an impressive ability to generate NeRF with frozen diffusion models instead of CLIP. Among its follow-up works, Latent-NeRF~\cite{metzer2022latent} and Dream3D~\cite{xu2022dream3d} are concurrent works and the most comparable to our work because they both utilize explicit 3D shapes to provide additional training signals in NeRF optimization. Dream3D directly initializes NeRF using generated SDF~\cite{park2019deepsdf}, while Latent-NeRF utilizes a user-provided mesh to give NeRF direct occupancy loss~\cite{mescheder2019occupancy} for geometry optimization. Our approach differs from theirs in that we leverage the 3D priors to implicitly imbue the diffusion model itself with 3D awareness for consistent and robust NeRF generation, while they apply 3D priors directly to facilitate NeRF optimization.