


\subsection{Implementation details}

\begin{wrapfigure}{r}{0.5\textwidth}\vspace{-50pt}
  \begin{center}
      \vspace{14pt}
      \begin{tabular}{@{\hspace{1pt}}c@{\hspace{1pt}}c@{\hspace{1pt}}c@{\hspace{1pt}}c@{}}
      \includegraphics[width=0.114\textwidth]{fig/mcc_1_1.png} &
      \includegraphics[width=0.114\textwidth]{fig/mcc_3_1.png} &
      \includegraphics[width=0.114\textwidth]{fig/mcc_2_1.png} &
      \includegraphics[width=0.114\textwidth]{fig/mcc_4_1.png}\vspace{-2pt} \\ 
      \includegraphics[width=0.114\textwidth]{fig/mcc_1_2.png} &
      \includegraphics[width=0.114\textwidth]{fig/mcc_3_2.png} &
      \includegraphics[width=0.114\textwidth]{fig/mcc_2_2.png} &
      \includegraphics[width=0.114\textwidth]{fig/mcc_4_2.png} \\
    \end{tabular} 
    \vspace{-10pt}
  \end{center}
  % \vspace{-20pt}
  \caption{\textbf{Qualitative results of \ours with MCC~\citep{wu2023multiview}}. Our \ours framework yields high-fidelity results with MCC model.}
\label{fig:qual_mcc}
\vspace{-15pt}
\end{wrapfigure}

We conduct all experiments with a Stable Diffusion model based on LDM~\citep{rombach2022high}, and train the sparse depth injector using a subset of the Co3D dataset from the weights~\citep{zhang2023adding} trained image-text pairs along with depth maps predicted by MiDaS. We conduct experiments with two off-the-shelf modules, mainly using Point-E and in certain experiments also leveraging MCC~\citep{wu2023multiview} with MiDaS. All details regarding score distillation and 3D representation follow the settings of the respective baseline models we use for each experiment. For semantic code sampling, we adopt Karlo~\citep{kakaobrain2022karlo-v1-alpha} based on unCLIP~\citep{ramesh2022hierarchical}, as we find that it tends to follow user prompt more closely. 

\subsection{Text-to-3D generation}

We apply \ours's methodology to previous SDS-based text-to-3D frameworks DreamFusion, Score Jacobian Chaining (SJC) and ProlificDreamer. All experiments utilize the Stable Diffusion as its diffusion model, which is a publicly available large-scale text-to-image diffusion model. Because DreamFusion's official implementation uses publicly unavailable Imagen~\citep{saharia2022photorealistic}, we instead resort to using Stable Diffusion for implementation.\vspace{-3pt}

\paragrapht{Qualitative evaluation.} We present our qualitative evaluation in Fig.~\ref{fig:qual_one} and Fig.~\ref{fig:qual_two}, which clearly demonstrates the effectiveness of our method in ensuring the geometric consistency and high fidelity of generated 3D scenes. While the baseline methods produce inconsistent, distorted geometry in multiple directions, combined with \ours they are empowered to generate robust geometry at every viewpoint, as evidenced by the figure as well as video results provided in the supplementary materials. In Fig.~\ref{fig:qual_mcc}, we also present the qualitative results of our approach when utilizing MCC~\citep{wu2023multiview} for the 3D prior, which also show consistent high-fidelity 3D scene generation results.

\begin{wrapfigure}{r}{0.5\textwidth}\vspace{-18pt}
  \begin{center}
  \includegraphics[width=\linewidth]{fig/fig_2dgen_new.pdf}\vspace{-5pt}
  \end{center}
  \vspace{-5pt}
  \caption{\textbf{View-dependent image generation comparison.} (a) are the results of naive view augmented prompting, and (b) our results with \ours framework.}
\label{fig:qual_2dgen}
\vspace{-15pt}
\end{wrapfigure}

\paragrapht{Quantitative evaluation.}
It is difficult to conduct a quantitative evaluation on a zero-shot text-to-3D generative model due to the absence of ground truth 3D scenes corresponding to the text prompts. In this light, we propose a new metric that utilizes COLMAP \citep{schonberger2016structure} to measure the 3D consistency of a generated scene. In Table~\ref{tab_col}, we report the average of variance scores over 42 generated 3D scenes. Our \ours framework outperforms baseline SJC by a large margin, demonstrating that our framework achieves more geometrically consistent text-to-3D generation. The details of our metric are given in Appendix~\ref{appendix:colmap}. 

\paragrapht{User study.}
We have conducted a user study with 102 participants, which is shown in Table.~\ref{tab_userstudy}. We have asked the participants to choose their preferred result in terms of 3D coherence, prompt adherence, and overall quality between Stable-DreamFusion, SJC, and our \ours-combined SJC. The results show that \ours generates 3D scenes that the majority of people judge as having higher fidelity and better geometric consistency than previous methods. Further details are described in Appendix~\ref{spp:userstudy}.
\input{table/quantitative_tables}
\vspace{-5pt}

\subsection{View-dependent text-to-image generation}

We conduct text-to-image generation experiments with \ours to verify our framework's capability to inject 3D awareness into 2D diffusion models. We observe whether the images are generated in a 3D-aware manner when viewpoints are given as conditions through our \ours framework. Fig.~\ref{fig:qual_2dgen} demonstrates the effectiveness of our framework: it allows for highly precise control of the camera pose in 2D images, with even small changes in viewpoint being reflected well on the generated images. Our approach shows superior performance to previous prompt-based methods (\eg, ``\textit{A front view of}'') regarding both precision and controllability of injected 3D awareness.

\vspace{-3pt}

\begin{figure}[t]
    \centering
    \begin{minipage}{0.47\textwidth}
    %%
      \begin{center}
        % \includegraphics[width=\linewidth]{fig/imagecon/Qual_img_cond_3D_gen.pdf}\vspace{-5pt}
        \vspace{-13pt}
        \includegraphics[width=\linewidth]{fig/3dfuse_zero123.pdf}\vspace{-5pt}
      \end{center}
      % \vspace{-20pt}
      \caption{\textbf{Qualitative result for comparison with Zero-1-to-3~\citep{liu2023zero}.} With reference image directly given as input, \ours generates more expressive, high-fidelity 3D scenes than Zero-1-to-3.}
    \label{fig:zero123}
    \vspace{-20pt}
    %%
    \end{minipage}
    \hfill
    \begin{minipage}{0.47\textwidth}\vspace{-13pt}
    %%
        \centering
        \small
        \setlength\tabcolsep{0.8pt}
        {
        \begin{tabular}{@{\hspace{1pt}}c@{\hspace{1pt}}c@{\hspace{1pt}}c@{}}
        
        \includegraphics[width=0.32\linewidth]{fig/ablation_geo/chair_points_d} &
        \includegraphics[width=0.32\linewidth]{fig/ablation_geo/afancychair} &
        \includegraphics[width=0.32\linewidth]{fig/ablation_geo/abeachchair} \\
        Condition & ``\textit{a fancy chair}'' & ``\textit{a beach chair}'' \\
        \includegraphics[width=0.32\linewidth]{fig/ablation_geo/mug_points_d} &
        \includegraphics[width=0.32\linewidth]{fig/ablation_geo/whitemug} &
        \includegraphics[width=0.32\linewidth]{fig/ablation_geo/rustymug} \\
        Condition & ``\textit{a simple mug}'' & ``\textit{a rusty mug}'' \\
        \end{tabular} }
        \vspace{-5pt}
        \caption{\textbf{3D reconstruction with different prompts given a single 3D structure.} The first column represents the given point clouds as conditions, and the second and third columns show synthesized results.  }
        \label{fig:ablation_different}\vspace{-5pt}
    %%
    \vspace{-20pt}
    \end{minipage}
\end{figure}




\subsection{Ablation study}
\label{experiments:ablation}


\paragrapht{Comparison with Zero-123.}
\label{abl:zero123}
We conduct qualitative comparison with concurrent work Zero-1-to-3, which bears similarity to our work in its aim to inject 3D awareness into diffusion models through view conditioning. We compare the two methods by conducting SDS-based 3D scene generation, one with Zero-1-to-3 as its diffusion model and the other with our \ours framework as its diffusion backbone model, with all other settings identical. As shown in Fig.~\ref{fig:zero123}, we notice that our \ours results generally more expressive and high-fidelity results. We hypothesize this phenomenon is related to the fact that unlike Zero-1-to-3 which requires finetuning of the diffusion model itself for incorporation of 3D pose awareness, our framework only finetunes an external conditioning module in a manner similar to ControlNet~\citep{zhang2023adding}, leaving the diffusion model untouched. This allows our framework to leverage the pretrained diffusion model's generative capability to the fullest, resulting more expressive and high-fidelity scene generation. Note also that our model only uses image as loose condition for general scene geometry and does not apply direct losses to align 3D scene with the image, which is the reason our generated results do not fit completely to input image unlike Zero-123. We provide additional experiment results applying Zero-123 for text-to-3D generation, in a manner similar to our methodology, in our Appendix.

\paragrapht{Different prompts with single 3D representation.}
3D representation, \ie\xspace point cloud obtained from off-the-shelf models~\citep{nichol2022point, wu2023multiview} is typically coarse and sparse.
To investigate how the diffusion model implicitly refines the coarse 3D structure, we conduct an ablation study using a fixed point cloud and different text prompts instead of inferring a point cloud from the initial image $\hat{x}$ of semantic code $s$.
Fig.~\ref{fig:ablation_different} shows that our \ours is flexible in responding to the error and sparsity inherent in the point cloud, depending on the semantics of text input.

% \vspace{-10pt}
\begin{wrapfigure}{r}{0.45\textwidth}\vspace{-20pt}
  \begin{center}
    \includegraphics[width=\linewidth]{fig/ablation_semantic/Abl_semantic.pdf}
  \end{center}
  \vspace{-12pt}
  \caption{\textbf{Ablation study on semantic code sampling.} The given prompts are ``\textit{a round orange pumpkin with a stem and a textured surface}'' (top) and ``\textit{a product photo of a toy tank}'' (bottom).}
\label{fig:ablation_semantic}
\vspace{-30pt}
\end{wrapfigure}

\paragrapht{Semantic code sampling.}
\label{abl:semcode}
We conduct an ablation study on semantic code sampling in our \ours framework. The results in the first two columns of Fig.~\ref{fig:ablation_semantic} show significant differences in geometry (pumpkin's smooth left surface and bumpy right surface) and semantic details (the tank's absence of wheel on the left) without semantic code sampling, depending on viewpoints. In contrast, using semantic code sampling ensures both geometric and semantic consistency across viewpoints, demonstrating its prominence in preserving the semantic identity of the 3D scene.

% \vspace{-10pt}