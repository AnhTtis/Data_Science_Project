\section{Introduction}     

Text-to-3D generation, the task of generating 3D content through given text prompts~\citep{Sanghi_2022_CVPR, liu2022iss, jain2022zero}, has seen a rapid growth surge thanks to the advancements in diffusion models~\citep{ho2020denoising,rombach2022high}. This innovation allows people to easily create 3D content without dealing with professional modeling tools, gaining significant attention across a wide range of industries, such as gaming, graphics, VR/AR, and animation. 

One of the most popular methodologies currently used for text-to-3D generation tasks is score distillation sampling (SDS)~\citep{poole2022dreamfusion}, which employs the gradients of 2D diffusion models to optimize a 3D representation, \ie, a Neural radiance field (NeRF)~\citep{mildenhall2020nerf}. This approach has gained prominence due to its leveraging the generation capability of pretrained 2D diffusion models to create expressive and detailed 3D scenes. However, various works~\citep{wang2023prolificdreamer} show that 3D scenes generated by SDS often display distortions and artifacts depending on text prompts. One notable failure case is a 3D inconsistency problem (dubbed the ``Janus problem"~\citep{wang2023prolificdreamer, shi2023mvdream}), where a frontal feature (such as a frontal face) pertaining to the text prompt is replicated at various sides of the generated scene. As shown in (a) of Fig.~\ref{fig:fig2}, this problem arises from the inherent limitation of the 2D diffusion model -- specifically, it lacks awareness of the camera pose from which it views the scene. Therefore, it is prone to produce frontal geometric features across all directions of the scene, generating distorted and unrealistic 3D scenes.   

A possible approach to solve this issue would be training a diffusion model that can be conditioned explicitly on camera pose values to produce plausible images of novel viewpoints~\citep{liu2023zero}. However, there are inherent hurdles to this approach. As the pose-conditioned diffusion model operates solely on 2D domain and has no explicit knowledge of 3D space, it has difficulty in modeling 3D transformations as well as subsequent self-occlusions, resulting in geometrically inconsistent novel view inferences~\citep{liu2023one2345}. The training also requires ground-truth 3D data, whose creation require either human effort or expensive 3D sensors, leading to smaller data size and inferior quality in comparison to the vast 2D data~\citep{schuhmann2022laion} available. This limits the generative capability of this model from reaching that of ordinary 2D diffusion models. We solve this dilemma between 2D diffusion models and explicit 3D geometry with a middle-ground approach that combines the best of both worlds -- a pretrained 2D diffusion model imbued with 3D awareness, conditioned on coarse yet explicit 3D geometry.

We propose a novel methodology, dubbed \ours, to improve the geometric consistency of
generated 3D scenes. Central to our method is a consistency injection module, which effectively imbues pretrained 2D diffusion models with 3D awareness and is easily adaptable to existing SDS-based text-to-3D baselines. Specifically, our consistency injection module creates a 3D coarse geometry of given text through an off-the-shelf point cloud generation model~\citep{nichol2022point,wu2023multiview}, which can be projected to arbitrary views to generate coarse depth maps. As this coarse depth map is explicitly 3D consistent and outlines the overall structure of the scene expected at the viewpoint, the diffusion model conditioned on it can optimize the given viewpoint in a 3D-aware manner. To this end, we add a conditioning module to the diffusion model and finetune it, leveraging the generative capability of pretrained diffusion model toward inferring dense structures of the scene despite the sparsity of the depth map. We also introduce semantic coding to add controllability and specificity in the generation process and show how it can be leveraged with Low-rank adaptation (LoRA)~\citep{hu2021lora} for more semantically consistent results. 

% the generative capability of the pretrained 2D diffusion model allows it to successfully infer dense structures despite the sparsity of depth maps. 

To demonstrate our method's adaptability and its universally powerful performance at various baselines, we employ our framework across a range of text-to-3D baseline models, \ie\space Dreamfusion~\citep{poole2022dreamfusion}, SJC~\citep{wang2022score}, and ProlificDreamer~\citep{wang2023prolificdreamer}. The results show significant improvements in both generation quality and geometric consistency on all baselines. We extensively demonstrate the effectiveness of our framework with qualitative analyses and ablation studies.
Moreover, we introduce an innovative metric for quantitatively assessing the 3D consistency of the generated scenes.       
\vspace{-10pt}



