\paragraph{Diffusion models.}
Diffusion models are generative models that learn data distribution from a Gaussian distribution by a gradual denoising process~\cite{ho2020denoising}. Diffusion models define a deterministic forward process $q(\cdot)$ that adds noise such that $q({x}_t|{x}_0) :=\mathcal{N}({x}_{t};\alpha_t {x}_0, \sigma^2_t {I})$, 
where $x_t$ is a noised sample with noise level $t$ and ${x}_0$ is a clean sample, \eg, original image, and $\alpha_t$ and $\sigma_t$ are pre-defined variables that control a noise schedule. The reverse process consists of denoising steps that progressively remove noise by modeling a neural network $\mu_\theta({x}_t,t)$ with parameters $\theta$ that predicts ${x}_0$ given ${x}_t$, and sampling from a posterior function derived from the forward process such that $p_{\theta}({x}_{t-1}|{x}_t):=q({x}_{t-1}|{x}_t,{x}_0={\mu}_{\theta}({x}_t,t))$. 
As shown in DDPMs~\cite{ho2020denoising}, noise approximation model $\epsilon_{\theta}({x}_t, t)$ can be used instead of ${\mu}_{\theta} ({x}_t,t)$ as follows:
\begin{equation}
\epsilon_{\theta} (x_t, t) = \frac{x_t-\alpha_t\mu_{\theta} (x_t, t)}{\sigma_t}.
\label{equation:ddpm}
\end{equation}

For a conditional generation, for instance, text-to-image diffusion models such as Stable Diffusion~\cite{rombach2022high} receive a text prompt as an additional condition. Specifically, when a text prompt $c$ is given, a mapping model $T(\cdot)$ maps the prompt $c$ into the embedding $e=T(c)$. Then, the embedding $e$ is injected into the diffusion model. Formally, we denote the text-to-image diffusion model as $\epsilon_\theta(x_t, t, T(c))$. For the sake of brevity, we shall omit the variable $t$ and refer to the function $\epsilon_{\theta}(x_t, T(c))$. In this case, the loss function for training the diffusion model is defined as follows:
\begin{equation}
    \mathcal{L}_\mathrm{diff}(\theta,x) = \mathbb{E}_{t,\epsilon}\Bigl[w(t)\|\epsilon_\theta\bigl(x_t, T(c)\bigr) - \epsilon \|^2_2 \Bigr],
\label{equation:diffloss}
\end{equation}
where $\epsilon$ is a Gaussian noise and $w(t)$ is a weighting function. Intuitively, this loss trains the model to predict the added Gaussian noise in the data.

\paragraph{Score distillation to NeRF.}
Optimizing NeRF with score distillation was first proposed in DreamFusion~\cite{poole2022dreamfusion}, which optimizes NeRF parameters to follow the direction of the score predicted by the diffusion model, \ie, the direction of higher-density regions~\cite{poole2022dreamfusion}. Concurrent works such as SJC~\cite{wang2022score} and subsequent studies~\cite{lin2022magic3d, metzer2022latent} have been proposed based on similar intuition. 

 Specifically, let us denote ${\Theta}$ as parameters of NeRF, and $\mathcal{R}_\Theta(\pi)$ as a rendering function given a camera pose $\pi$. In DreamFusion and SJC, random camera pose $\pi$ is sampled, and the diffusion model is utilized to infer the 2D score of the rendered image, \ie, $x=\mathcal{R}_\Theta(\pi)$. This score is used to optimize the NeRF parameters $\Theta$ by letting the rendered image move to the higher-density regions, \ie, to be realistic. This can be explained as minimizing the loss function introduced in Eq.~\ref{equation:diffloss} with respect to the NeRF parameters $\Theta$ instead of the diffusion model's parameters $\theta$ such that
\begin{equation}
\Theta^* = \underset{\Theta}{\mathrm{argmin}}\,\mathbb{E}_{\pi}\Bigl[\mathcal{L}_\mathrm{diff}\bigl(\theta, x=\mathcal{R}_\Theta(\pi) \bigr)\Bigr].
\label{equation:sds1}
\end{equation}
In addition, the Jacobian term of the diffusion U-net $\partial\epsilon_\theta\bigl(x_t,T(c)\bigr)/\partial x_t$ from the gradient of the loss function $\nabla_\Theta \mathcal{L}_\mathrm{diff}$ can be omitted for efficiency, and the new gradient can be formulated  such that
\begin{equation}
\begin{split}
     \nabla_\Theta &\mathcal{L}_\mathrm{SDS}(\theta, x=\mathcal{R}_\Theta(\pi)) \\
    &\triangleq \mathbb{E}_{t,\epsilon}\Bigl[\Tilde{w}\bigl(t\bigr)\bigl(\epsilon_\theta \bigl(x_t,T(c) \bigr)-\epsilon\bigr)\frac{\partial x}{\partial \Theta}\Bigr],
\end{split}
\label{equation:sds2}
\end{equation}
where $\nabla_\Theta \mathcal{L}_\mathrm{SDS}$ is a gradient of $\mathcal{L}_\mathrm{diff}$ with the U-net Jacobian term omitted, and $\Tilde{w}(t)$ is a weighting function. 