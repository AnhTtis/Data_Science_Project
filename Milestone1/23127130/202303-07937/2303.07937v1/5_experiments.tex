

\input{table/user_study}
\subsection{Implementation details}
We conduct all experiments with the Stable Diffusion~\cite{rombach2022high} based on LDM~\cite{rombach2022high}, and train the sparse depth injector using a subset of the Co3D dataset~\cite{reizenstein2021common} from the weights~\cite{zhang2023adding} trained image-text pairs along with depth maps predicted by MiDaS~\cite{ranftl2020towards}. We conduct experiments with two off-the-shelf modules, mainly using Point-E~\cite{nichol2022point} and in certain experiments also leveraging MCC~\cite{wu2023multiview} with MiDaS.
The details of score distillation and neural radiance field representation follow the settings of SJC~\cite{wang2022score}, our baseline. In semantic code sampling, we adopt Karlo~\cite{kakaobrain2022karlo-v1-alpha} based on unCLIP~\cite{ramesh2022hierarchical} because we find that it tends to follow user prompt more closely.


\subsection{Text-to-3D generation}
We compare \ours to previous score distillation text-to-3D frameworks, DreamFusion~\cite{poole2022dreamfusion} and Score Jacobian Chaining (SJC)~\cite{wang2022score}. All experiments including \ours use Stable Diffusion~\cite{rombach2022high} as a backbone, which is a publicly available large-scale text-to-image diffusion model. Because DreamFusion~\cite{poole2022dreamfusion}'s official implementation uses publicly unavailable  Imagen~\cite{saharia2022photorealistic} model as its baseline, we instead resort to using Stable Diffusion~\cite{rombach2022high} for implementation, which we call Stable-DreamFusion~\cite{stable-dreamfusion}.

\paragraph{Qualitative evaluation.}
We present our qualitative evaluation in Fig.~\ref{qual_textto3d}, which demonstrates that the neural radiance fields generated by our framework surpass previous methods in both fidelity and geometric consistency. While previous methods produce inconsistent, distorted geometry in multiple regions, our method generates robust geometry at every viewpoint, as evidenced by the figure as well as video results provided in the project page. Furthermore, we present the qualitative results of our approach when utilizing MCC~\cite{wu2023multiview} for the 3D prior in Fig.~\ref{fig:qual_mcc}, which shows generating high-quality 3D content as well.

\begin{figure}[t]
    \centering
    \small
    \setlength\tabcolsep{0.8pt}

    \begin{tabular}{cccc}
      \includegraphics[width=0.24\linewidth]{fig/mcc_1_1.png} &
      \includegraphics[width=0.24\linewidth]{fig/mcc_3_1.png} &
      \includegraphics[width=0.24\linewidth]{fig/mcc_2_1.png} &
      \includegraphics[width=0.24\linewidth]{fig/mcc_4_1.png}\vspace{-2pt} \\ 
      \includegraphics[width=0.24\linewidth]{fig/mcc_1_2.png} &
      \includegraphics[width=0.24\linewidth]{fig/mcc_3_2.png} &
      \includegraphics[width=0.24\linewidth]{fig/mcc_2_2.png} &
      \includegraphics[width=0.24\linewidth]{fig/mcc_4_2.png} \\
       ``\textit{a toy bicycle}'' & ``\textit{a toy plane}'' & ``\textit{a backpack}'' & ``\textit{a penguin}''\\
    \end{tabular} 
    \vspace{-5pt}
    \caption{\textbf{Qualitative results of \ours with MCC~\cite{wu2023multiview}.} Our \ours framework yields high-fidelity rendering results even with MCC~\cite{wu2023multiview}.}
    \label{fig:qual_mcc} 
    \vspace{-5pt}
\end{figure}
\input{table/colmap_metric}
\paragraph{Quantitative evaluation.}
It is difficult to conduct a quantitative evaluation on a zero-shot text-to-3D generative model due to the absence of ground truth 3D scenes corresponding to the text prompts. Existing works provide additional user studies~\cite{lin2022magic3d} or employ CLIP R-Precision~\cite{jain2022zero,poole2022dreamfusion}. However, CLIP R-Precision only measures retrieval accuracy through projected 2D image and text input, so it is not suitable for quantifying the geometric consistency of a 3D scene.
% , the reasons for which we discuss in detail in the supplementary materials. 

Instead, we propose a new metric that utilizes COLMAP \cite{schonberger2016structure} to measure the consistency of a generated 3D scene. We based on the fact that the accuracy of COLMAP camera pose optimization is dependent upon the robustness and consistency of 3D surfaces, as stated in~\cite{schwarz2020graf}. We sample 100 uniformly spaced camera poses from a hemisphere of fixed radius, at identical azimuth, all directed towards the sphere's center, and render 100 images of the 3D scene. COLMAP predicts the camera pose of each image for reconstruction. We then measure the difference between the predicted camera poses of two adjacent images at the rendering stage. The variance of these values is used as our metric for 3D-consistency evaluation. High variance indicates inaccuracy in the predicted camera poses, which corresponds to 3D inconsistency that makes optimization for COLMAP difficult. Additional details of our metric are provided in the supplementary materials.

Using this metric, we compare the consistency of scenes generated by our \ours and SJC. In Table~\ref{tab_col}, we report the average of variance scores over 42 generated 3D scenes. Our \ours framework outperforms SJC by a large margin, quantitatively demonstrating that our framework achieves more geometrically consistent text-to-3D generation.

\paragraph{User study.}
We have conducted a user study with 102 participants, which is shown in Table.~\ref{tab_userstudy}. We have asked the participants to choose their preferred result in terms of 3D coherence, prompt adherence, and overall quality between Stable-DreamFusion~\cite{poole2022dreamfusion,stable-dreamfusion}, SJC~\cite{wang2022score}, and our \ours. The results show that \ours generates 3D scenes that the majority of people judge as having higher fidelity and better geometric consistency than previous methods. Further details are described in the supplementary material.


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig/fig_2dgen_new.pdf}\vspace{-5pt}
\caption{\textbf{Qualitative comparison of view-dependent image generation results.} (a) Results of view augmented prompting, and (b) our results with \ours framework. 
}
\label{fig:qual_2dgen}
\vspace{-5pt}
\end{figure}

\subsection{View-dependent text-to-image generation}
We conduct text-to-image generation experiments with \ours to verify our framework's capability to infuse 3D awareness into 2D diffusion models. We observe whether the images are generated in a 3D aware manner when viewpoints are given as conditions through our \ours framework. Fig.~\ref{fig:qual_2dgen} well demonstrates the effectiveness of our framework: it allows for highly precise control of the camera pose in 2D images, with even small changes in viewpoint being reflected well on the generated images. Our approach shows superior performance to previous prompt-based methods (\eg ``\textit{A front view of}'') regarding both precision and controllability of injected 3D awareness.

\vspace{-3pt}

\begin{figure}[t]
\centering
\centering
\includegraphics[width=\linewidth]{fig/imagecon/Qual_img_cond_3D_gen.pdf}\vspace{-5pt}
\vspace{-10pt}
\caption{\textbf{Qualitative result for image-conditional 3D generation.} When a reference image is directly given as input, \ours successfully captures the image's style and appearance for coherent 3D scene generation.} 
\label{fig:ablation_imcon}\vspace{-5pt}
\end{figure}

\begin{figure}[t]
\centering
\small
\setlength\tabcolsep{0.8pt}
{
\begin{tabular}{ccc}

\includegraphics[width=0.33\linewidth]{fig/ablation_geo/chair_points_d} &
\includegraphics[width=0.33\linewidth]{fig/ablation_geo/afancychair} &
\includegraphics[width=0.33\linewidth]{fig/ablation_geo/abeachchair} \\
Condition & ``\textit{a fancy chair}'' & ``\textit{a beach chair}'' \\
\includegraphics[width=0.33\linewidth]{fig/ablation_geo/mug_points_d} &
\includegraphics[width=0.33\linewidth]{fig/ablation_geo/whitemug} &
\includegraphics[width=0.33\linewidth]{fig/ablation_geo/rustymug} \\
Condition & ``\textit{a simple mug}'' & ``\textit{a rusty mug}'' \\
\end{tabular} }
\vspace{-5pt}
\caption{\textbf{3D reconstruction with different prompts given a single 3D structure.} The first column represents the given point clouds as conditions, and the second and third columns show synthesized results.  }
\label{fig:ablation_different}\vspace{-5pt}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{fig/ablation_semantic/Abl_semantic.pdf}\vspace{-5pt}
\caption{\textbf{Ablation study on semantic code sampling.} The given prompts are ``\textit{a round orange pumpkin with a stem and a textured surface}'' (top) and ``\textit{a product photo of a toy tank}'' (bottom).}
\label{fig:ablation_semantic}\vspace{-5pt}
\end{figure}

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.75\linewidth]{fig/Appendix_Robustness.pdf}
\caption{\textbf{Qualitative comparison for robustness.} When given a single prompt, \ours demonstrates robust performance even in random seed variations. We visualize the outputs generated from the fixed seed from 0 to 4.}
\label{fig:qual_robust}
\vspace{-5pt}
\end{figure*}


\subsection{Image-conditional 3D generation}
In this experiment, instead of generating the initial image from a text prompt, we directly give an input image as the initial image $\hat{x}$ of our semantic code, which effectively reconfigures our framework as an image-conditional setting. Fig.~\ref{fig:ablation_imcon} displays impressive results: it shows that our architecture successfully captures the overall structure and appearance of the input image for NeRF generation. This has interesting implications, as it demonstrates that our framework has a certain degree of 3D reconstruction capability.



\subsection{Analysis and ablation study}\vspace{-3pt}
\label{experiments:ablation}
\paragraph{Different prompts with single 3D representation.}
3D representation, \ie, point cloud obtained from off-the-shelf models~\cite{nichol2022point, wu2023multiview} is typically coarse and sparse.
To investigate how the diffusion model implicitly refines the coarse 3D structure, we conduct an ablation study using a fixed point cloud and different text prompts instead of inferring a point cloud from the initial image $\hat{x}$ of semantic code $s$.
Fig.~\ref{fig:ablation_different} shows that our \ours is flexible in responding to the error and sparsity inherent in the point cloud, robustly interpreting the depth map in accordance with the semantics of text input.

\paragraph{Semantic code sampling.}
\label{abl:semcode}
We conduct an ablation study on semantic code sampling in our \ours framework. The results in the first two columns of Fig.~\ref{fig:ablation_semantic} show significant differences in geometry (pumpkin's smooth left surface and bumpy right surface) and semantic details (the tank's absence of wheel on the left) without semantic code sampling, depending on viewpoints. In contrast, using semantic code sampling ensures both geometric and semantic consistency across viewpoints, demonstrating its prominence in preserving the semantic identity of the 3D scene.

\paragraph{Robustness.}
\label{abl:robustness}
We experiment with fixed prompts and changing random seeds to verify the robustness of our approach compared to previous works~\cite{poole2022dreamfusion,stable-dreamfusion,wang2022score}. Fig.~\ref{fig:qual_robust} demonstrates that our approach exhibits robust performance and is far less sensitive to random seeds compared to previous works.

