{
    "arxiv_id": "2303.07937",
    "paper_title": "Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation",
    "authors": [
        "Junyoung Seo",
        "Wooseok Jang",
        "Min-Seop Kwak",
        "Jaehoon Ko",
        "Hyeonsu Kim",
        "Junho Kim",
        "Jin-Hwa Kim",
        "Jiyoung Lee",
        "Seungryong Kim"
    ],
    "submission_date": "2023-03-14",
    "revised_dates": [
        "2023-03-16"
    ],
    "latest_version": 3,
    "categories": [
        "cs.CV"
    ],
    "abstract": "Text-to-3D generation has shown rapid progress in recent days with the advent\nof score distillation, a methodology of using pretrained text-to-2D diffusion\nmodels to optimize neural radiance field (NeRF) in the zero-shot setting.\nHowever, the lack of 3D awareness in the 2D diffusion models destabilizes score\ndistillation-based methods from reconstructing a plausible 3D scene. To address\nthis issue, we propose 3DFuse, a novel framework that incorporates 3D awareness\ninto pretrained 2D diffusion models, enhancing the robustness and 3D\nconsistency of score distillation-based methods. We realize this by first\nconstructing a coarse 3D structure of a given text prompt and then utilizing\nprojected, view-specific depth map as a condition for the diffusion model.\nAdditionally, we introduce a training strategy that enables the 2D diffusion\nmodel learns to handle the errors and sparsity within the coarse 3D structure\nfor robust generation, as well as a method for ensuring semantic consistency\nthroughout all viewpoints of the scene. Our framework surpasses the limitations\nof prior arts, and has significant implications for 3D consistent generation of\n2D diffusion models.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.07937v1",
        "http://arxiv.org/pdf/2303.07937v2",
        "http://arxiv.org/pdf/2303.07937v3"
    ],
    "publication_venue": "Project page https://ku-cvlab.github.io/3DFuse/"
}