\section{Probabilistic IOC}\label{sec:ioc}
We consider an agent acting in a partially observable Markov decision process as introduced in \cref{sec:pomdps}. We assume that the agent acts at time $t$ based on their belief $\belief_{t}$ about the state of the system $\state_t$, which evolves according to $\belief_{t+1} = \beliefdynamics_t(\belief_t, \action_t, \obs_t)$. While the belief of the agent is defined commonly as a distribution over the true state, here we model $\belief_t$ as a finite-dimensional summary statistics of the distribution, i.e., $\belief_{t} \in \R^b$. The function $\beliefdynamics_t: \beliefset \times \R^u \times \R^m \to \beliefset$ is called belief dynamics. We further assume that the agent follows a time-dependent policy $\policy_t: \beliefset \times \R^j \to \R^u$, i.e., $\action_t = \policy_t(\belief_t, \policynoise_t)$, which can be stochastic with  $\policynoise_{t} \sim \Normal(0, \Identity)$. Note that both the belief dynamics and the policy can be time-dependent.

In the inverse optimal control problem, the goal is to estimate parameters $\params \in \R^p$ of the agent's optimal control problem given the model and trajectory data. These parameters can include properties of the agent's cost function, the sensory and control systems of the agent, or the system's dynamics. We follow a probabilistic approach to inverse optimal control,
i.e., we consider the likelihood function 
\begin{align}
    p(\state_{1:T} \given \params) = p(\state_0 \given \params) \prod_{t=0}^{T-1} p(\state_{t+1} \given \state_{1:t}, \params), \label{eqn:likelihood}
\end{align}
describing the probability of the observed trajectory data $\state_{1:T} \defeq \{\state_t\}_{t=1, \ldots, T}$ given the parameters. For a set of trajectories we assume them to be independent given the parameters so that the likelihood factorizes into single trajectory likelihoods of the form in \cref{eqn:likelihood}. In this equation, generally, each state $\state_{t+1}$ depends on all previous states $\state_1, \dots, \state_t$, because the agent's internal noisy observations and control signals are not accessible to the researcher (\cref{fig:setup}, right). Therefore, the Markov property does not hold from the researcher's perspective, rendering computation of the likelihood function intractable. To deal with this problem, we employ two key insights: First, the joint dynamical system of the states and the agent's belief is Markovian \citep{van2011lqg}. Second, by keeping track of the distribution over the agent's belief, i.e., by performing belief tracking \citep{schultheis2021inverse}, we can iteratively compute the individual factors of the likelihood function in \cref{eqn:likelihood}.

We first introduce a general formulation of the IOC likelihood involving marginalization over the agent's internal beliefs in \cref{sec:likelihood}. Then, we show how to make the computations tractable by local linearization in \cref{sec:linearization}. In \cref{sec:fixed_linearization}, we provide details for suitable linearization points, which enables us to evaluate the approximate likelihood within a single forward pass.

\subsection{Likelihood formulation}\label{sec:likelihood}
% \subsection{Joint dynamical system}
We start by defining a joint dynamical system of states and beliefs \citep{van2011lqg} in which each depends only on the state and belief at the previous time step and the noises. For that, we insert the policy into the dynamics and
the policy and observation function into the belief dynamics, yielding the equation
\begin{align}
    \label{eqn:joint-dynamics}
    \begin{bmatrix} \state_{t+1} \\ \belief_{t+1} \end{bmatrix} 
    &= \begin{bmatrix} \dynamics(\state_t, \policy_t(\belief_t, \policynoise_t), \statenoise_t) \\
    \beliefdynamics_t(\belief_t, \policy_t(\belief_t, \policynoise_t), h(\state_t, \obsnoise_t))
    \end{bmatrix}\\ 
    &\eqdef g(\state_t, \belief_t, \statenoise_t, \obsnoise_t, \policynoise_t).
\end{align}
For given values of $\state_t$ and $\belief_t$, this equation defines the distribution $p(\state_{t+1}, \belief_{t+1} \given \state_{t}, \belief_{t})$, as $\statenoise_t, \obsnoise_t, \policynoise_t$ are independent of $\state_{t+1}$ and $\belief_{t+1}$. In \cref{sec:linearization} we will introduce an approximation via linearization, which leads to a closed-form expression for $p(\state_{t+1}, \belief_{t+1} \given \state_{t}, \belief_{t})$. 

One can use this Markovian joint dynamical system to compute the likelihood factors for each time step \citep{schultheis2021inverse}.
To this end, we first rewrite the individual likelihood terms $p(\state_{t+1} \given \state_{1:t})$ of \cref{eqn:likelihood} by marginalizing over the agent's belief at each time step, i.e., 
\begin{align}
    p(\state_{t+1} \given \state_{1:t}) =  \int p(\state_{t+1}, \belief_{t+1} \given \state_{1:t}) \, \td \belief_{t+1}. \label{eqn:marginalization}
\end{align}
As the belief is an internal quantity of the agent and thus not observable to the researcher, we keep track of its distribution, $p(\belief_t \given \state_{1:t})$.
For this, we rewrite
\begin{align}
    &p(\state_{t+1}, \belief_{t+1} \given \state_{1:t}) = \int p(\state_{t+1}, \belief_{t+1}, \belief_t \given \state_{1:t}) \, \td \belief_t \notag \\ 
    &\qquad= \int p(\state_{t+1}, \belief_{t+1} \given \state_t, \belief_t) \, p(\belief_t \given \state_{1:t}) \, \td \belief_t, \label{eqn:propagation}
\end{align}
where we have exploited the fact that the joint dynamical system of states and beliefs is Markovian. The distribution $p(\belief_t \given \state_{1:t})$ acts as a summary of the past states and can be computed by conditioning on the current state, i.e.,
\begin{align}
    p(\belief_t \given \state_{1:t}) = \frac{p(\state_t, \belief_t \given \state_{1:t-1})}{p(\state_t \given \state_{1:t-1})}. \label{eqn:summary}
\end{align}
After determining $p(\belief_t \given \state_{1:t})$, we can propagate it through the joint dynamical system to arrive at the distribution $p(\state_{t+1}, \belief_{t+1} \given \state_{1:t})$. To obtain the belief distribution of the following time step, $p(\belief_{t+1} \given \state_{1:t+1})$, we condition on the observed state $\state_{t+1}$. To obtain the likelihood contribution, on the other hand, we marginalize out the $\belief_{t+1}$.

To summarize, starting with an initialization $p(\belief_0)$, we can compute the individual terms $p(\state_{t+1} | \state_{1:t})$ of the likelihood by executing \cref{algo:likelihood}.



\begin{algorithm}
\begin{algorithmic}[1]
\ENSURE Approximate likelihood of parameters $p(\state_{1:T} \given \params)$
\REQUIRE Parameters $\params$, Data $\state_{1:T}$, Model $f, h$
    \STATE Determine the policy $\policy$ using iLQG
    \STATE Determine the belief dynamics $\beliefdynamics$ using the EKF
    \FOR{$t$ in $\{1, \ldots, T-1 \}$} 
    \STATE Compute $p(\state_{t+1}, \belief_{t+1} \given \state_{1:t})$ using \cref{eqn:propagation}
    \STATE Update $p(\belief_{t+1} \given \state_{1:t+1})$ using \cref{eqn:summary}
    \STATE Obtain $p(\state_{t+1} \given \state_{1:t})$ using \cref{eqn:marginalization}
    \ENDFOR
\end{algorithmic}

\caption{Approximate likelihood computation}\label{algo:likelihood}
\end{algorithm}

\subsection{Tractable likelihood via linearization}\label{sec:linearization}
While the marginalization and propagating operations listed in the previous section can be done in closed form for linear-Gaussian systems,
%\cite{schultheis2021inverse}, 
this is no longer feasible for non-linear systems. Therefore, we follow the approach of local linearization used in iLQG (\cref{sec:ilqg}) and the extended Kalman filter (\cref{sec:kalman}). For the belief statistics, we consider the mean of the agent's belief, i.e., $\belief_t = \E[\state_t \given \obs_1, \ldots, \obs_{t-1}]$ and
initialize the distribution for the first time step as a Gaussian, $p(\belief_1) = \Normal(\mu_1^{(b)}, \Sigma_1^{(b)})$. We then approximate the distribution $p(\state_{t+1}, \belief_{t+1} \given \state_t, \belief_t)$ as a Gaussian by applying a first-order Taylor expansion of $g$.

In order to obtain a closed-form expression for $g$, which we can linearize, we model the agent's belief dynamics using the extended Kalman filter (\cref{sec:kalman}) and its policy using iLQG (\cref{sec:ilqg}), as in the partially observable version of iLQG \citep{li2007iterative}. This choice leads to an affine control law and belief dynamics given $\belief_t$, making linearization of $p(\state_{t+1}, \belief_{t+1} \given \state_t, \belief_t)$ straight-forward. To allow for additional stochasticity in the agent's policy, we follow the common formulation of maximum causal entropy (MCE) reinforcement learning (\cref{sec:mcerl}). For the linearized dynamics, the MCE policy is -- as for the fully-observable case (\cref{sec:mcerl}) -- given by a Gaussian distribution, so that $\policy_t(\belief_t, \policynoise_t) = \controlmatrix_t (\belief_t - \bar\state_{1:T}) + \controloffset_t + \bar\action_{1:T} - \tilde{\controlmatrix}_t \policynoise_t$, with $\tilde{\controlmatrix_t}$ the Cholesky decomposition of $L_t$, and can be marginalized out in closed form.

The approximations we have introduced allow us to solve the integral in \cref{eqn:propagation} in closed form by applying standard equations for linear transformations of Gaussians, resulting in 
\begin{align}
    p(\state_{t+1}, \belief_{t+1} \given \state_{1:t}) \approx \Normal\left(\mu_t, \Sigma_t  \right)
\end{align}
with
\begin{align*}
    \mu_t &= g(\state_t, \mu_t^{(b)}, 0, 0, 0), \\
    \Sigma_t &= J_\belief \Sigma_t^{(b)} J_\belief^T + J_{\statenoise} J_{\statenoise}^T + J_{\obsnoise} J_{\obsnoise}^T + J_{\policynoise} J_{\policynoise}^T,
\end{align*}
where $J_\bullet$ denotes the Jacobian of $g$ w.r.t.\ $\bullet$, evaluated at $(\state_t, \mu_t^{(b)}, 0, 0, 0)$. Under this Gaussian approximation, both remaining operations of \cref{algo:likelihood} can also be performed in closed form. A more detailed derivation and representation of these formulas can be found in \cref{app:ioc_derivation}. If the agent has full observations of the system's state, the inverse optimal control problem is simplified significantly. The derivations for this special case are shown in \cref{app:fullyobs}. Details about the implementation are provided in \cref{app:implementation}.

\subsection{Data-based linearization}
\label{sec:fixed_linearization}
The described approach to evaluate the likelihood requires solving the optimal filtering and control problem for a given set of parameters. When iteratively maximizing the likelihood, we would have to solve both problems in every iteration, making the approach computationally expensive. We can make the method more efficient by using the insight that in the IOC problem, we are given a trajectory $\state_{1:T}$. Instead of starting with a randomly initialized nominal trajectory and iterating between computation of the locally optimal control law and linearizing again, we can simply linearize the dynamics once around the given trajectory and keep this linearization fixed. We then need to perform only one backward-pass to compute an approximately optimal control law given the current parameters, and a forward pass to compute an approximately optimal filter. This, in particular, allows efficient computation of the gradient of the likelihood function. As we assume the actions to be unobservable, but they are needed for the linearization, we compute estimates of the actions by minimizing the squared difference of the noiseless state estimates and the actual states. Note that these estimated actions are only used for the linearization, but are not used as observed actions in the IOC likelihood itself (see \cref{app:actions}).
