\setcounter{figure}{0}
\renewcommand{\thefigure}{S\arabic{figure}} 

\section*{Appendix}
\section{Notation}\label{app:notation}

\begin{table}[h]
\caption{Notation}\label{tab:notation}
    \centering
    \begin{tabular}{@{}ll@{}}
        \toprule
        $\state_t \in \mathbb{R}^n$ & state at time $t$ \\
        $\action_t \in \mathbb{R}^u$ & action at time $t$ \\
        $\obs_t \in \mathbb{R}^m$ & observation at time $t$ \\
        $T \in \N$ & number of time steps \\
        $f: \R^n \times \R^u \times \R^v \to \R^n$ & system dynamics function \\
        $h: \R^n \times \R^w \to \R^m$ & observation function \\
        $\statenoise_t \in \mathbb{R}^v$ & standard multivariate normal dynamics noise \\
        $\obsnoise_t \in \mathbb{R}^w$ & standard multivariate normal observation noise \\
        $c_t: \R^n \times \R^u \to \R$ & cost function at intermediate time steps \\
        $c_T: \R^n \to \R$ & cost function at final time step\\
        $\belief_{t} \in \beliefset$ & summary statistics of agent's belief, $p(\state_t \given \obs_{1:t-1})$ \\
        $\beliefdynamics_t: \beliefset \times \R^u \times \R^m \to \beliefset$ & belief dynamics \\
        $\policy_t: \beliefset \times \R^j \to \R^u$ & policy of the agent \\ %\midrule
        $\params \in \R^p$ & model parameters \\
        $g: \R^{n} \times \beliefset \times \R^v \times \R^w \times \R^j \to \R^n \times \beliefset$ & joint dynamics of states and beliefs \\
    \end{tabular}
    
\end{table}

\section{Inverse optimal control derivations}
\label{app:ioc_derivation}
We start with defining the joint dynamics of states and estimates, which returns the next state and estimate as a function of the previous state and estimate and the noises,
\begin{align}
    \begin{bmatrix} \state_{t+1} \\ \belief_{t+1} \end{bmatrix}
    = g(\state_t, \belief_t, \statenoise_t, \obsnoise_t, \policynoise_t).
\end{align}

To do so, we insert the policy and observation function into the system and belief dynamics, giving
\begin{align}
    \state_{t+1} &= \dynamics(\state_t, \policy_t(\belief_t, \policynoise_t), \statenoise_t),\\
    \belief_{t+1} &= \beliefdynamics_t(\belief_t, \policy_t(\belief_t, \policynoise_t), h(\state_t, \obsnoise_t)).
\end{align}

The individual factors of the likelihood function can be determined as 
\begin{align*}
    p(\state_{t+1} \given \state_{1:t}) =  \int p(\state_{t+1}, \belief_{t+1} \given \state_{1:t}) \, \td \belief_{t+1},
\end{align*}
where $p(\state_{t+1}, \belief_{t+1} \given \state_{1:t})$ is given by marginalizing over the current belief $\belief_t$ as
\begin{align*}
    p(\state_{t+1}, \belief_{t+1} \given \state_{1:t}) &= \int p(\state_{t+1}, \belief_{t+1}, \belief_t \given \state_{1:t}) \, \td \belief_t \\
    & = \int p(\state_{t+1}, \belief_{t+1} \given \state_{1:t}, \belief_t) \, p(\belief_t \given \state_{1:t}) \, \td \belief_t \\
    & = \int p(\state_{t+1}, \belief_{t+1} \given \state_t, \belief_t) \, p(\belief_t \given \state_{1:t}) \, \td \belief_t. 
\end{align*}

\subsection*{Linearization}

To derive a tractable approximation of the distribution $p(\state_{t+1}, \belief_{t+1} \given \state_{1:t})$, we model the initial belief of the agent's state estimate $p(\belief_1)$ as a Gaussian and linearize the joint dynamics function, leading to a Gaussian approximation of the desired quantity, i.e., $p(\state_{t+1}, \belief_{t+1} \given \state_{1:t}) \approx \Normal\left(\mu_t, \Sigma_t  \right)$.

First, we apply a first-order Taylor expansion of the joint dynamics $g$ around the observed state $\state_t$ and the mean of the belief $\mu_t^{(b)}$ and the noises:
\begin{align}
    g(\state_t, \belief_t, \statenoise_t, \obsnoise_t, \policynoise_t) \approx g(\state_t, \mu_t^{(b)}, 0, 0, 0) + J_\belief (\belief_t - \mu_t^{(b)}) + J_{\statenoise} \statenoise_t + J_{\obsnoise} \obsnoise_t + J_{\policynoise} \policynoise_t,
\end{align}
where $J_\bullet$ denotes the Jacobian of $g$ w.r.t. $\bullet$, evaluated at $(\state_t, \mu_t^{(b)}, 0, 0, 0)$.

To derive an explicit representation of the Jacobians, we insert the filtering and control law obtained by the Kalman filter and maximum entropy iLQG controller and find
\begin{align*}
    g(\state_t, \belief_t, \statenoise_t, \obsnoise_t, \policynoise_t)
    &= \begin{bmatrix} \dynamics(\state_t, \policy_t(\belief_t, \policynoise_t), \statenoise_t) \\
    \beliefdynamics_t(\belief_t, \policy_t(\belief_t, \policynoise_t), h(\state_t, \obsnoise_t))
    \end{bmatrix} \\
    &= \begin{bmatrix} \dynamics(\state_t, \action_t, \statenoise_t) \\
    \beliefdynamics_t(\belief_t, \action_t, \obs_t)
    \end{bmatrix},
\end{align*}
with
\begin{align*}
    \obs_t &= h(\state_t, \obsnoise_t), \\
    \action_t &= \policy_t(\belief_t, \policynoise_t) = \controlmatrix_t (\belief_t - \bar\state_{1:T}) + \controloffset_t + \bar\action_{1:T} - \tilde{\controlmatrix}_t \policynoise_t, \\
    \beliefdynamics_t(\belief_t, \action_t, \obs_t) &= \dynamics(\belief_t, \action_t, 0) + K_t (\obs_t - h(\belief_t, 0)),
\end{align*}
leading to the equations
\begin{align*}
    J_\belief 
    &= \begin{bmatrix} \nabla_{\action} \dynamics(\state_t, \action_t,0)  \nabla_{\belief} \action_t \\
    \nabla_{\belief} \beliefdynamics_t(\belief_t, \action_t, \obs_t) + \nabla_{\action} \beliefdynamics_t(\belief_t, \action_t, \obs_t) \nabla_{\belief} \action_t
    \end{bmatrix} \\
    &= \begin{bmatrix} \nabla_{\action} \dynamics(\state_t, \action_t, 0)  \controlmatrix_t \\
    \nabla_{\state} \dynamics(\belief_t, \action_t, 0) - K_t \nabla_{\state} h(\belief_t, 0) + \nabla_{\action} \dynamics(\belief_t, \action_t, 0) \controlmatrix_t
    \end{bmatrix},\\\\
    J_{\statenoise} &= \begin{bmatrix} \nabla_{\statenoise} \dynamics(\state_t, \action_t, 0) \\
    \nabla_{\statenoise} \dynamics(\belief_t, \action_t, 0)
    \end{bmatrix},\\\\
    J_{\obsnoise} &= \begin{bmatrix} 0 \\
    \nabla_{h} \beliefdynamics_t(\belief_t, \action_t, \obs_t) \nabla_{\obsnoise} h(\belief_t, 0)
    \end{bmatrix} %\\
    = \begin{bmatrix} 0 \\
    -K_t \nabla_{\obsnoise} h(\belief_t, 0)
    \end{bmatrix},\\\\
    J_{\policynoise} &= 
    \begin{bmatrix} \nabla_{\action} \dynamics(\state_t, \action_t, 0) \nabla_{\policynoise} \action_t \\
    \nabla_{\action} \dynamics(\belief_t, \action_t, 0) \nabla_{\policynoise} \action_t
    \end{bmatrix}
    = \begin{bmatrix} \nabla_{\action} \dynamics(\state_t, \action_t, 0) \controlmatrix_t \\
    \nabla_{\action} \dynamics(\belief_t, \action_t, 0) \controlmatrix_t
    \end{bmatrix}.
\end{align*}

Propagating the Gaussian belief over the agent's state estimate, $p(\belief_t \given \state_{1:t})$ through the linearized dynamics model (\cref{eqn:propagation}) can be done by applying standard identities for linear transformations of Gaussian random variables \citep{bishop2006pattern} and gives
\begin{align}
    p(\state_{t+1}, \belief_{t+1} \given \state_{1:t}) \approx \Normal\left(\mu_t, \Sigma_t  \right),
\end{align}
with $\mu_t = g(\state_t, \mu_t^{(b)}, 0, 0, 0)$ and $\Sigma_t = J_\belief \Sigma_t^{(b)} J_\belief^T + J_{\statenoise} J_{\statenoise}^T + J_{\obsnoise} J_{\obsnoise}^T + J_{\policynoise} J_{\policynoise}^T$. Marginalization over $\belief_{t+1}$ gives the desired likelihood factor, while conditioning on $\state_{t+1}$ gives the belief statistic for the following time step (see \cref{sec:likelihood}).

\newpage
\section{Special case: full observability}\label{app:fullyobs}
\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \begin{adjustbox}{width=0.45\linewidth}
        \input{tikz/mdp-agent.tikz}
    \end{adjustbox}
    \begin{adjustbox}{width=0.45\linewidth}
        \input{tikz/mdp-experimenter.tikz}
    \end{adjustbox}
    \caption{\textbf{Left:} decision network from the agent's perspective \citep[following the notation used in][]{kochenderfer2022algorithms}. The agent directly observes the state $\state_t$. At each time step, they perform an action $\action_t$ and incur a cost $c_t$. \textbf{Right: } probabilistic graphical model from the researcher's perspective, who observes a trajectory $\state_{1:T}$ from an agent. The action signals $\action_t$ are not directly observed.}
    \label{fig:fullyobs}
\end{wrapfigure}
If the state $\state_t$ is fully observable to the agent, the problem is simplified significantly. The control problem from the agent's perspective (\cref{fig:fullyobs}, left) can be solved by applying iLQG \citep{todorov2005generalized} to the observed states directly (\cref{sec:ilqg})

\begin{align}
    \action_t = \policy_t(\state_t, \policynoise_t),
\end{align}

This also simplifies the IOC problem from the researcher's perspective because 
evaluating the approximate likelihood becomes straight-forward as we do not need to marginalize over the agent's belief.

Recall that the likelihood can be decomposed as
\begin{align}
    p(\state_{1:T} \given \theta) = p(\state_0 \given \theta) \prod_{t=0}^{T-1} p(\state_{t+1} \given \state_{1:t}, \theta)
\end{align}
and the dynamics are given as
\begin{align}
    \state_{t+1} = f(\state_t, \action_t, \statenoise_t) = f(\state_t, \policy(\state_t, \policynoise_t), \statenoise_t).
\end{align}

We can approximate the likelihood contribution at each time step as
\begin{align}
    p(\state_{t+1} \given \state_{1:t}, \theta) \approx \Normal(\mu_t, \Sigma_t),
\end{align}
with
\begin{align}
    \mu_t &= f(\state_t, \policy(\state_t, 0), 0) \\
    \Sigma_t &= J_{\statenoise} J_{\statenoise}^T + J_{\policynoise} J_{\policynoise}^T,
\end{align}
where $J_\bullet$ denotes the Jacobian of $f$ w.r.t. $\bullet$, evaluated at $(\state_t, \policy(\state_t, 0), 0)$:
\begin{align*}
    J_{\statenoise} &= \nabla_{\statenoise} \dynamics(\state_t, \action_t, 0),\\
    J_{\policynoise} &= 
    \nabla_{\action} \dynamics(\state_t, \action_t, 0) \nabla_{\policynoise} \action_t
    = \nabla_{\action} \dynamics(\state_t, \action_t, 0) \controlmatrix_t.
\end{align*}


\section{Implementation}\label{app:implementation}
We provide a flexible framework for defining non-linear stochastic dynamical systems of the form introduced in \cref{sec:pomdps} by implementing the dynamics, observation function, and cost function. The optimal estimation control methods based on iterative linearization are implemented using the automatic differentiation library \texttt{jax} \citep{jax2018github}, so that Jacobians and Hessians for the linearization of the dynamics and quadratization of the cost do not have to be specified manually. Furthermore, the Jacobians needed for linear-Gaussian approximation in the IOC likelihood (\cref{sec:linearization}) are also computed using automatic differentiation. Gradient-based maximization of the log likelihood was performed using the L-BFGS-B \texttt{scipy} \citep{virtanen2020scipy} from the \texttt{jaxopt} library \citep{blondel2021efficient}.
The implementation 
will be made public upon publication.

For computations, we used an Intel Xeon Platinum 9242 Processor, using 1 core per run. The mean run times for computing maximum likelihood estimates (partially observable setting) were as follows:
\begin{itemize}
    \item Reaching: 140 s
    \item Navigation: 64 s
    \item Pendulum: 31 s
    \item CartPole: 115 s
\end{itemize}

\section{Estimating actions for linearization}\label{app:actions}
For every evaluation of the likelihood function as described in \cref{sec:likelihood}, we would naively need to solve the forward control problem once by running the iLQG algorithm given the current parameter values. This is computationally inefficient due to the iterative procedure of iLQG, which requires multiple forward and backward passes. Instead of performing the iterative procedure, which computes the locally optimal control law $\{\controlmatrix_{1:T-1}, \controloffset_{1:T-1}\}$ around a nominal trajectory $\{\bar \state_{1:T}, \bar \action_{1_T-1}\}$, then computes the new nominal trajectory given the current control law and so on, we realize that in the IOC setting, we already have observed the actual trajectory $\state_{1:T}$ performed by the agent. To make solving the forward control problem more efficient, we only compute the locally optimal control law once around this trajectory. This would require the action signals $\action_{1:T}$ to be given, but they are unobserved in our problem setting. To obtain a proxy for the actions for the purposes of linearization, we solve for the actions given the trajectory $\state_{1:T}$ in the system dynamics equation $\state_{t+1} = f(\state_t, \action_t, \statenoise_t)$ using the Gauss-Newton method for non-linear least squares as implemented in \texttt{jaxopt}.

\section{Tasks}\label{app:tasks}
\subsection{Classic control tasks}\label{app:classic-control}

We evaluate our method on two classic continuous control tasks. Specifically, we build upon the Pendulum and Cart Pole environments from the \texttt{gym} library \citep{brockman2016openai}.
Because these environments are not stochastic in their standard implementations and are typically used in an infinite-horizon reinforcement learning setting, we made the following changes to the environments. To account for the finite-horizon setting we are considering in this work, the state-dependent costs associated with the task goal are applied only to the final time step, while action-dependent costs are applied at every time step along the trajectory. To make the problems stochastic, we have added noise on the dynamics and the observation function.

\subsubsection{Pendulum}
The task is to make a pendulum, which is attached to a fixed point on one side, swing into an upright position, i.e. to reach an angle $\theta = 0$. The pendulum starts at the bottom ($\theta = \pi$) and can be controlled by applying a torque to the free end of the pendulum. We added control-dependent noise to the dynamics, where the parameter $\sigma_m$ controls, how strongly a standard normal noise is scaled by the magnitude of the torque. In addition to this motor noise parameter, the task has two other free parameters: the cost of actions $c_a$ and the cost of the final velocity $c_v$.
In the partially observable version of the task, the agent receives the observation via the non-linear stochastic observation model $\mathbf \obs_t = \begin{bmatrix} \sin \theta_t, \cos \theta_t, \dot \theta_t \end{bmatrix}^T + 0.1 \obsnoise_t$. The time horizon was $T = 50$.

\subsubsection{Cart pole}
In the Cart Pole task, a pole is attached to a cart that can be moved left or right by applying a continuous force. In our version of the task, the goal is to move the cart from the horizontal position $x = 0$ to $x = 1$ while balancing the pole in an upright position. Again, we add control-dependent noise to the force, parameterized the strength of the linear control-dependence $\sigma_m$. As above, the other two parameters are the cost of actions $c_a$ and the cost of the final velocity $c_v$. In the partially observed version of the task, the agent receives a noisy observation with $\mathbf \obs_t = \mathbf \state_t + \obsnoise_t$. The time horizon was $T = 200$.


\subsection{Reaching task with biomechanical arm model}\label{app:reaching}
We implemented the non-linear biomechanical model for arm movements from \citet{todorov2005generalized} and its partially observed version from  \citet{li2007iterative}, which are described in more detail in the PhD thesis of \citet{li2006optimal}. The dynamics describe the movement of a two-link arm, which can be controlled by applying torques to the two joints. 
The task is to move the hand to a target location, as defined in the cost function
\begin{align}
    J = \norm{\mathbf e_T - \mathbf e^*}^2 + c_v \norm{\dot{\mathbf e}_T}^2 + c_a \sum_{t=1}^{T-1} \norm{\mathbf u_t}^2,
\end{align}
where $\mathbf e_T$ is the position of the hand at the final time step in Cartesian coordinates, $\mathbf e^\star$ is the position of the target, and $\dot{\mathbf e}_T$ is the final velocity of the hand. That is, the task is to bring the hand to the target at the final time step ($T = 50$) and minimize the final velocity while making as little actions as possible along the way. The parameters $c_v$ and $c_a$ trade off, how important the velocity and action parts of the cost function are relative to the main goal of reaching the target. Additionally, we parameterize the signal-dependent variability of the motor system with the parameter $\sigma_m$, which defines how strongly the variability is scaled by the action magnitude.

\subsection{Navigation task}\label{app:navigation}
We implemented a simple navigation task in which an agent walks towards a target. The dynamics and observation model were adapted from the non-holonomic movement model and the distance-bearing observation model from \citet{kessler2022dynamic}. The state $\state_t = \begin{bmatrix}x_t, y_t, \theta_t, \omega_t\end{bmatrix}^T$ consists of the position in the horizontal plane, the heading angle, and the speed in the heading direction. The dynamics are
\begin{align}
    f(\state, \action, \statenoise) = \state + dt \begin{bmatrix}
    \cos(\theta) \cdot \omega \\ \sin(\theta) \cdot \omega \\ u_1 + \sigma_m u_1 v_1 \\ u_2 + \sigma_m u_2 v_2
    \end{bmatrix},
\end{align}
which means that the agent controls the velocity of the heading angle and the acceleration in heading direction. There is action-dependent noise, whose strength is determined by the parameter $\sigma_m$. The observation model is
\begin{align}
    g(\state, \obsnoise) = \begin{bmatrix}
    \sqrt{(x - k_1)^2 + (y - k_2)^2} \\
    \tan^{-1}(y - k_2, x - k_1) \\
    \omega
    \end{bmatrix} + \sigma_o \obsnoise,
\end{align}
where $\mathbf k  = \begin{bmatrix}k_1, k_2\end{bmatrix}^T$ is the position of the target and $\sigma_o$ determines the magnitude of the observation noise. The cost function is
\begin{align}
    J = (x_T - k_1)^2 + (y_T - k_2)^2 + c_v \omega_T + \sum_{t=1}^{T-1} c_a \action_t^T \action_t,
\end{align}
with parameters determining the cost of actions ($c_a$) and the cost of the final velocity ($c_v$). The time horizon was $T = 50$.

\section{Hyperparameters}
\label{app:hyperparams}

Throughout the experiments, we have used the following hyperparameters:

\begin{table}[h]
\caption{Hyperparameters}\label{tab:hyperparams}
    \centering
    \begin{tabular}{@{}ll@{}}
    \toprule
    Data set size (number of trajectories) & 50 \\
    Number of datasets per evaluation & 100 \\
    Number of time steps ($T$) & 50 (reaching, navigation, pendulum), 200 (cartpole) \\  
    Optimizer & L-BFGS-B (\texttt{scipy} wrapper from \texttt{jaxopt}) \\
    Maximum entropy temperature & $10^{-6}$ (reaching, navigation)), $10^{-3}$ (pendulum, cartpole) \\
    Optimizer restarts & 50 \\
    \end{tabular}
    
\end{table}

\newpage
\section{Additional results}
\label{app:add_results}
The parameter estimates and true values are provided in \cref{fig:estimates_po_pendulum}, \cref{fig:estimates_po_cartpole}, \cref{fig:estimates_po_navigation}.

\subsection{Results for all tasks in the partially observable setting}\label{app:results_partialobs}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/estimates_po_pendulum.pdf}
    % \vspace{-1cm}
    \caption{Maximum likelihood parameter estimates for the Pendulum task (partially observable cases).}
    \label{fig:estimates_po_pendulum}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/estimates_po_cartpole.pdf}
    % \vspace{-1cm}
    \caption{Maximum likelihood parameter estimates for the Cart Pole task (partially observable).}
    \label{fig:estimates_po_cartpole}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/estimates_po_navigation.pdf}
    % \vspace{-1cm}
    \caption{Maximum likelihood parameter estimates for the navigation task (partially observable).}
    \label{fig:estimates_po_navigation}
\end{figure}

\newpage
\subsection{Results for all tasks in the fully observable setting}\label{app:results_fullobs}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/estimates_reaching.pdf}
    % \vspace{-1cm}
    \caption{Maximum likelihood parameter estimates for the reaching task (fully observable).}
    \label{fig:estimates_reaching}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/estimates_pendulum.pdf}
    % \vspace{-1cm}
    \caption{Maximum likelihood parameter estimates for the pendulum task (fully observable).}
    \label{fig:estimates_pendulum}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/estimates_cartpole.pdf}
    % \vspace{-1cm}
    \caption{Maximum likelihood parameter estimates for the cart pole task (fully observable).}
    \label{fig:estimates_cartpole}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/estimates_navigation.pdf}
    % \vspace{-1cm}
    \caption{Maximum likelihood parameter estimates for the navigation task (fully observable).}
    \label{fig:estimates_navigation}
\end{figure}

\clearpage
\subsection{Results for baseline with given control signals}
\label{app:results_controls}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/estimates_baseline_givencontrols.pdf}
    % \vspace{-1cm}
    \caption{Maximum likelihood parameter estimates for the baseline with given control signals (fully observable)}
    \label{fig:estimates_baseline_givencontrols}
\end{figure}
