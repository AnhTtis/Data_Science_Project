\section{Introduction}
Inverse optimal control (IOC) is the problem of inferring an agent's cost function, and possibly other properties of their internal model, from observed behavior. 
While IOC has been a fundamental task in artificial intelligence, optimal control, and machine learning, particularly reinforcement learning and robotics, it has widespread applicability in several scientific fields including behavioral economics, psychology, and neuroscience. 
For example, in cognitive science and sensorimotor neuroscience optimal control models have been able to explain key properties of behavior, such as speed-accuracy trade-offs \citep{harris1998signal} or the minimum intervention principle \citep{todorov2002optimal}. But, while researchers usually build an optimal control model and compare its predictions to behavior, certain parameters of the agent's internal processes are typically unknown. For example, an agent might experience intrinsic costs of behavior such as effort that are different between individuals. Inferring these parameters from observed behavior can help to understand the agent's goals, internal tradeoffs, cognitive processes and predict their behavior under novel conditions.
Applying IOC in these sensorimotor control domains poses several challenges that make most previous methods not viable.

First, most IOC methods assume the agent's action signals to be known. This assumption, while convenient in simulations or robotics applications, where the control signals may be easily quantified, does not hold in many other real-world applications. In transfer learning or behavioral experiments, the action signals are internal quantities of an animal or human, e.g., neural activity or muscle activations, and are therefore not straightforwardly observable. Thus, it is worthwhile to consider the scenario where a researcher has observations of the system's state only, i.e., measurements of the animal's behavior.

Second, with few exceptions \citep[e. C.g.][]{chen2015predictive, kwon2020inverse}, IOC methods do not account for partial observability from the agent's perspective and model the variability of the agent using a maximum causal entropy formulation \citep[MCE;][]{ziebart2010modeling}. 
However, many real world control problems involve sensory uncertainty, which makes the state of the world partially observable and therefore contributes to the agent's stochasticity.
As an example, in sensorimotor neuroscience the noise or uncertainty in the sensory system can be well described quantitatively so that accurate observation models can be formulated, which are helpful to understand the variability of behavior \citep{wolpert2000computational}.

Third,
many IOC methods are based on matching feature expectations of the cost function between the model and observed data \citep[e.g.][]{ziebart2010modeling}, and are thus not easily adapted to infer parameters of other parts of the model. The cost function is often not the only quantity of interest in a behavioral experiment, where researchers might be interested in also inferring the noise characteristics of the motor system or other properties of the agent's internal model \citep[e.g.][]{golub2013learning}.

Fourth, in many real-world scenarios, the problem is not well modeled with linear dynamics and Gaussian noise, which would allow applying linear quadratic Gaussian (LQG) control \citep{anderson1990optimal}. First, 
the dynamics of the system may be non-linear. A common example comes from robotics and motor control, where
joint angles in a kinematic chain need to be controlled and the physical system's dynamics involve inertia, centripetal, and Coriolis forces, as well as friction and torque in the joints.
Secondly, the stochasticity of the system may not be well captured by normal distributions. 
A prime example is biological sensorimotor control, where the system is not only non-linear but both the sensory and action noise distributions are additionally signal dependent, i.e. the variability of sensory and control signals scale with their respective means. While iterative method for solving the optimal control problem exist \citep{todorov2005generalized}, here we consider the corresponding inverse problem.

To address these issues, we adopt a probabilistic perspective of the IOC problem. We distinguish between the control problem faced by the agent and the inference problem the researcher has to solve. From the agent's perspective, the problem consists of acting in a partially observable Markov decision process (POMDP), for which the probabilistic graphical model is shown in \cref{fig:setup}, left. 
We consider the setting of continuous states and actions, stochastic non-linear dynamics, partial observations, and finite horizon.
For this setting, there are efficient approximately optimal solutions to the estimation and control problem, for which we give an overview in \cref{sec:background}.
The researcher, on the other hand, is interested in inferring properties of the agent's model and cost function. The IOC problem from their perspective can also be formulated using a probabilistic graphical model (\cref{fig:setup}, right), in which the state of the system is observed, while quantities internal to the agent are latent variables. 

Here, we unify MCE models, which are agnostic regarding the probabilistic structure causing the observed stochasitcity of the agent's policy,
with IOC methods, which involve an explicit observation model. 
We allow for both: We employ an explicit observation model, but also allow the agent to have additional stochasticity through a MCE policy.
We provide a solution to the IOC problem in this setting by approximate filtering of the agent's state estimate via local linearization, which allows marginalizing over these latent variables and deriving an approximate likelihood function for observed trajectories given parameters (\cref{sec:ioc}). This function can be efficiently evaluated as it consists of a single forward pass. An estimate of the optimal parameters can then be determined using a gradient-based optimizer, maximizing the approximate likelihood.
We evaluate our proposed method on two classical control tasks, pendulum and cartpole, as well as a navigation task, and a manual reaching task (\cref{sec:experiments}).

\begin{figure}
    \centering
    \begin{adjustbox}{width=0.49\linewidth}
        \input{tikz/pomdp-agent.tikz}
    \end{adjustbox}
    \begin{adjustbox}{width=0.49\linewidth}
        \input{tikz/pomdp-experimenter.tikz}
    \end{adjustbox}
    \caption{\textbf{Left:} Decision network from the agent's perspective \citep[following the notational convention used in][]{kochenderfer2022algorithms}. At each time step $t$, the agent receives a partial or noisy observation $\obs_t$ of the actual state $\state_t$. The agent performs an action $\action_t$ and incurs a cost $c_t$. \textbf{Right:} Probabilistic graphical model from the researcher's perspective, who observes a trajectory $\state_{1:T}$ from an agent. Quantities that are internal to the agent, i.e., their partial observations $\obs_t$, their internal beliefs $\belief_t$ and the action signals $\action_t$ are not directly observed.}
    \label{fig:setup}
\end{figure}


\subsection*{Related work}
Inferring costs or utilities from behavior has been of interest for a long time in several scientific fields, such as behavioral economics, psychology, and neuroscience \citep{mosteller1951experimental, kahneman1979prospect, kording2004loss}. 
More specific to the problem formulation adopted here, 
estimating objective functions in the field of control was first investigated by \citet{kalman1964linear} in the context of deterministic linear systems with quadratic costs. More recent formulations were developed first for discrete state and action spaces under the term inverse reinforcement learning \citep[IRL;][]{ng2000algorithms, abbeel2004apprenticeship},
including formulations allowing for stochasticty in action selection \citep{rothkopf2011preference}.
In this line, the maximum entropy \citep[ME;][]{ziebart2008maximum} and 
MCE formulation \citep{ziebart2010modeling} gave rise to a whole string of new methods, such as addressing the IOC problem for non-linear continuous systems via linearization \citep[e.g.][]{levine2012continuous} or importance sampling \citep{boularias2011relative} for the case of deterministic dynamics and full observability.

IOC methods for stochastic systems have been developed that considered the setting of affine control dynamics \citep{aghasadeghi2011maximum, li2011inverse}.
Arbitrary non-linear stochastic dynamics in the infinite horizon setting have been approached using model-free deep MCE IRL formulations \citep{finn2016guided, garg2021iq}. The latter approaches, however, yield no interpretable representation as the reward function is represented as a neural network.
The partially observable setting for IOC has previously been addressed in the case of deterministic dynamics for the discrete state-action space \citep{choi2011inverse} and continuous state, discrete action space \citep{silva2019continuous}. 
\citet{schmitt2016exact} addressed systems with linear dynamics and continuous controls for a linear switching observation model.
Other work has considered partial observability from the researcher's perspective, e.g., through occlusions \citep{kitani2012activity, bogert2016expectation}.
There are some IOC methods which are applicable to partially observable and stochastic systems: Linear-quadratic-Gaussian systems have been regarded by \citet{schultheis2021inverse}, while the work of \citet{chen2015predictive} can be used to estimate cost functions that depend on the state only. Non-linear dynamics in the infinite-horizon setting have been approached by \citet{kwon2020inverse} by training a policy network as a function of the whole parameter space. This work, however, also assumes the action signals to be given and a stationary policy.

Applications where IOC methods have been used to estimate cost functions range 
from human locomotion \citep{mombaur2010human} over spatial navigation \citep{rothkopf2013modular}, table tennis \citep{muelling2014learning}, to attention switching \citep{schmitt2017see}, and target tracking \citep{straub2022putting}. Other work has been aimed at inferring other properties of control tasks, e.g. learning the dynamics model \citep{golub2013learning}, learning rules \citep{ashwood2020inferring}, or discount functions \citep{schultheis2022reinforcement}.
Several subfields of robotics including imitation and apprenticeship learning \citep{taylor2009transfer} as well as transfer learning \citep{osa2018algorithmic} have also employed IOC.