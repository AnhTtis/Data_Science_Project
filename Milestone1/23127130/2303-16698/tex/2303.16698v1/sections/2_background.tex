\section{Background}\label{sec:background}

Before we introduce our probabilistic approach to inverse optimal control, we give an overview of the control and filtering problems faced by the agent and algorithms that can be used to solve it. For a summary of our notation in this paper, see \cref{app:notation}.

\subsection{Partially observable Markov decision processes}
\label{sec:pomdps}
We consider a special case of partially observable Markov decision processes \cite{aastrom1965optimal,kaelbling1998planning}, a discrete-time stochastic non-linear dynamical system (\cref{fig:setup}, left) with states $\state_t \in \R^n$ following the dynamics equation $\state_{t+1} = f(\state_t, \action_t, \statenoise_t)$, where $f: \R^n \times \R^u \times \R^v \to \R^n$ is the dynamics function, $\action_t \in \R^u$ are the controls and $\statenoise_t \sim \Normal(0, \Identity)$ is $v$-dimensional Gaussian noise. We assume that the agent has only partial observations $\obs_{t} \in \R^m$ following $\obs_{t} = h(\state_t, \obsnoise_t)$, with $h: \R^n \times \R^w \to \R^m$ the stochastic observation function and $\obsnoise_{t} \sim \Normal(0, \Identity)$ $w$-dimensional Gaussian noise.
While $\statenoise_t$ and $\obsnoise_t$ are defined as standard normal random variables, the system can incorporate general control- and state-dependent noises through non-linear transformations within the dynamics function $f$ and observation function $h$.
The agent's goal is to minimize the expected cost over a time horizon $T \in \N$, defined by $J = \E [ c_T(\state_T) + \sum_{t=1}^{T-1} c_t(\state_t, \action_{t}) ]$, consisting of a final state cost $c_T(\state_T)$ and a cost at each time step $c_t(\state_t, \action_{t})$.




\subsection{Iterative linear quadratic Gaussian}
\label{sec:ilqg}


The fully-observable control problem analogous to \cref{sec:pomdps}, where we assume that the agent acts directly on the state, can be solved approximately using the method of iterative linear quadratic Gaussian \citep[iLQG; ][]{todorov2005generalized}.
This method iteratively linearizes the dynamics and employs a quadratic approximation of the costs around a nominal trajectory, 
$\{\bar \state_i, \bar \action_i \}_{i=1, \ldots, T}$, with $\bar \state_i \in \R^n,  \bar \action_i \in \R^u$,
and computes the optimal linear control law, $\action_t = \policy_t(\state_t) = \controlmatrix_t (\state_t - \bar\state_t) + \controloffset_t + \bar\action_{1:T}$ for the approximated system. The quantities $\controlmatrix_t$ and $\controloffset_t$ are the control gain and offset, respectively, and determined through a backward pass for the current reference trajectory. In the following iteration, the determined optimal control law is used to generate a new reference trajectory and the process is repeated until the controller converges.


\subsection{MCE reinforcement learning}
\label{sec:mcerl}
The MCE reinforcement learning is to minimize the expected cost as in \cref{sec:ilqg}, while maximizing the conditional entropy of the applied stochastic policy $\Pi_t(\action_t \given \state_t)$, i.e., to minimize $\E [J(\state_{1:T}, \action_{1:T}) - \sum_{t=1}^{T-1} H(\Pi_t(\action_t \given \state_t))]$. 
This formulation has been used to formulate reinforcement learning as a probabilistic inference problem \citep{kappen2012optimal, toussaint2009robot, levine2018reinforcement} and for inverse reinforcement learning (IRL) to model the stochasticity of the agent \citep[e.g.,][]{ziebart2008maximum, ziebart2010modeling}. The objective of IRL is formulated as maximizing the likelihood of given states and actions $\{\state_t, \action_t\}_{t=1, \ldots, N}$, induced by the maximum entropy policy $\Pi_t(\action_t \given \state_t)$.

It can be shown that the resulting optimal policy is given by the distribution $\Pi_t(\action_t \given \state_t) = \exp(Q_t(\state_t, \action_t) - V_t(\state_t))$, where $Q_t$ is the soft Q-function at time $t$, given by $Q_t(\state_t, \action_t) = -c_t(\state_t, \action_t) - \E [V_{t+1} (\state_{t+1})]$ and $V_t$ the normalization, i.e., $V_t(\state_t) = \log \int_{\action_t} \exp(Q_t(\state_t, \action_t)) \, \td \action_t$ \citep{gleave2022primer}. For general dynamics and reward functions, it is hard to compute the soft Q-function exactly. Approximate solutions have been derived using linearization \citep{levine2012continuous} or importance sampling \citep{boularias2011relative}. For the case of linear dynamics and quadratic reward, the optimal policy is given by a Gaussian distribution $\Pi_t(\action_t \given \state_t) = \Normal(\action_t; L_t \state_t, -L_t)$, where $L_t$ is the controller gain of the LQG controller \citep{levine2013guided}. This formulation can be extended to non-linear systems by using the control law in conjuction with the iLQG method (\cref{sec:ilqg}).


\subsection{Extended Kalman filter}
\label{sec:kalman}
Given the system defined in \cref{sec:pomdps}, the optimal filtering problem is to compute a belief distribution of the current state given past observations, i.e., $p(\lstate_t \given \lobs_{1:t-1})$. 
For linear-Gaussian systems, the solution is given in closed form and known as the Kalman filter \citep{kalman1960new}. In case of non-linear systems as in \cref{sec:pomdps}, a Gaussian approximation to the optimal belief can be computed using the extended Kalman filter via $\belief_{t+1} = f(\belief_t, \action_t, 0) + K_t (\obs_t - h(\belief_t, 0))$, where $\belief_t \in \R^n$ denotes the mean of the Gaussian belief $p(\state_t \given \obs_1, \ldots, \obs_{t-1})$. The matrix $K_t$ denotes the Kalman gain for time $t$ and is computed by applying the Kalman filter to the system locally-linearized around the nominal trajectory obtained by the approximate optimal control law of iLQG (\cref{sec:ilqg}).
