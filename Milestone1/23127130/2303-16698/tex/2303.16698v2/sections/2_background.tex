\section{Background}\label{sec:background}

Before we introduce our probabilistic approach to inverse optimal control, we give an overview of the control and filtering problems faced by the agent and algorithms that can be used to solve it. For a summary of our notation in this paper, see \cref{app:notation}.

\subsection{Partially observable Markov decision processes (POMDPs)}
\label{sec:pomdps}
We consider a special case of POMDPs \cite{aastrom1965optimal,kaelbling1998planning}, a discrete-time stochastic non-linear dynamical system (\cref{fig:setup}~A) with states $\state_t \in \R^n$ following the dynamics equation 
$\state_{t+1} = f(\state_t, \action_t, \statenoise_t)$,
where $f$
is the dynamics function, $\action_t \in \R^u$ are the controls and $\statenoise_t \sim \Normal(0, \Identity)$ is $v$-dimensional Gaussian noise.
We assume that the agent has only partial observations $\obs_{t} \in \R^m$, following
$\obs_{t} = h(\state_t, \obsnoise_t)$,
with $h$
the stochastic observation function and $\obsnoise_{t} \sim \Normal(0, \Identity)$ $w$-dimensional Gaussian noise.
While $\statenoise_t$ and $\obsnoise_t$ are standard normal random variables, the system can incorporate general control- and state-dependent noises through non-linear transformations within the dynamics function $f$ and observation function $h$.
The agent's goal is to minimize the expected cost over a time horizon $T \in \N$, defined by 
\begin{align*}
J = \E \left[ c_T(\state_T) + \sum_{t=1}^{T-1} c_t(\state_t, \action_{t}) \right],
\end{align*}
consisting of a final state cost $c_T(\state_T)$ and a cost at each time step $c_t(\state_t, \action_{t})$.



\subsection{Iterative linear-quadratic Gaussian (iLQG)}
\label{sec:ilqg}
The control problem from \cref{sec:pomdps}
can be solved approximately using iLQG \citep{todorov2005generalized, li2007iterative}.
This method iteratively linearizes the dynamics and quadratizes the costs around a nominal trajectory, 
$\{\bar \state_i, \bar \action_i \}_{i=1, \ldots, T}$, with $\bar \state_i \in \R^n,  \bar \action_i \in \R^u$,
and computes the optimal linear control law, $\action_t = \policy_t(\state_t) = \controlmatrix_t (\state_t - \bar\state_t) + \controloffset_t + \bar\action_{1:T}$ for the approximated system. The quantities $\controlmatrix_t$ and $\controloffset_t$ are the control gain and offset, respectively, and determined through a backward pass for the current reference trajectory. In the following iteration, the determined optimal control law is used to generate a new reference trajectory and the process is repeated until the controller converges.


\subsection{Maximum causal entropy (MCE) reinforcement learning}
\label{sec:mcerl}
The goal of MCE RL is to minimize the expected cost as in \cref{sec:ilqg}, while maximizing the conditional entropy of the stochastic policy $\Pi_t(\action_t \given \state_t)$, i.e., to minimize $\E [J(\state_{1:T}, \action_{1:T}) - \sum_{t=1}^{T-1} \entropy(\Pi_t(\action_t \given \state_t))]$. 
This formulation has been used to treat RL as probabilistic inference \citep{kappen2012optimal, toussaint2009robot, levine2018reinforcement} and model the stochasticity of the agent in IRL \citep[][]{ziebart2008maximum, ziebart2010modeling}. The objective of IRL is to maximize the likelihood of states and controls 
$\{\state_t, \action_t\}_{t=1, \ldots, N}$,
induced by the maximum entropy policy.
The resulting optimal policy is given by the distribution $\Pi_t(\action_t \given \state_t) = \exp(Q_t(\state_t, \action_t) - V_t(\state_t))$, where $Q_t$ is the soft Q-function
and $V_t$ the normalization
\citep{gleave2022primer}.
For general dynamics and reward functions, it is not feasible to compute the soft Q-function exactly. 
Approximate solutions have been derived using linearization \citep{levine2012continuous}, importance sampling \citep{boularias2011relative}, or deep function approximation \cite{garg2021iq}. 
For linear dynamics and quadratic costs, the optimal policy is a Gaussian distribution $\Pi_t(\action_t \given \state_t) = \Normal(\action_t; \controlmatrix_t \state_t, -\controlmatrixm_t^{-1})$, 
where $\controlmatrix_t$ and $\controlmatrixm_t$ result from the optimal LQG controller \citep{levine2013guided}.
More detailed formulas are provided in \cref{app:baseline}. 


\subsection{Extended Kalman filter (EKF)}
\label{sec:kalman}
Given the system defined in \cref{sec:pomdps}, the optimal filtering problem is to compute a belief distribution of the current state given past observations, i.e., $p(\lstate_t \given \lobs_{1:t-1})$. 
For linear-Gaussian systems, the solution is given in closed form and known as the Kalman filter \citep{kalman1960new}. In case of non-linear systems as in \cref{sec:pomdps}, a Gaussian approximation to the optimal belief can be computed using the extended Kalman filter (EKF) via $\belief_{t+1} = f(\belief_t, \action_t, 0) + K_t (\obs_t - h(\belief_t, 0))$, where $\belief_t \in \R^n$ denotes the mean of the Gaussian belief $p(\state_t \given \obs_1, \ldots, \obs_{t-1})$. The matrix $K_t$ denotes the Kalman gain for time $t$ and is computed by applying the Kalman filter to the system locally-linearized around the nominal trajectory obtained by the approximate optimal control law of iLQG (\cref{sec:ilqg}).
