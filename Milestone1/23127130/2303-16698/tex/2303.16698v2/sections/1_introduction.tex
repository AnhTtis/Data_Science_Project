\section{Introduction}
Inverse optimal control (IOC) is the problem of inferring an agent's cost function and other properties of their internal model from behavior. 
While IOC has been a fundamental task in artificial intelligence and machine learning, particularly reinforcement learning (RL) and robotics, it has widespread applicability in several scientific fields including behavioral economics, psychology, and neuroscience. 
For example, in cognitive science and sensorimotor neuroscience, optimal control models have explained key properties of behavior, such as speed-accuracy trade-offs \citep{harris1998signal} or the minimum intervention principle \citep{todorov2002optimal}. But, while researchers usually build an optimal control model and compare its predictions to behavior, certain parameters of the agent's internal processes are typically unknown. For example, an agent might have uncertainty about their perception or experience intrinsic costs of behavior. These parameters are different between individuals and inferring them from observed behavior can help to understand internal tradeoffs between behavioral goals, perceptual and cognitive processes, and predict behavior under novel conditions.
Applying IOC in these domains poses several challenges that make most previous methods not viable.

First, most IOC methods assume the agent's control signals to be known. This assumption, while convenient in simulations or robotics, where the control signals may be easily quantified, does not hold in many other real-world applications. In transfer learning or behavioral experiments, the control signals are internal quantities of an animal or human, e.g., neural activity or muscle activations, and are therefore not straightforwardly observable. Thus, we consider the scenario where a researcher has observations of the system's state only, i.e., measurements of behavior.

Second, most IOC methods model the variability of the agent using a stochastic policy in a maximum causal entropy formulation \citep[MCE;][]{ziebart2010modeling}.
Behavioral variability in biological systems, however, is known to arise from multiple distinct sources \citep{faisal2008noise}. There is noise in the sensory system, which makes the state of the world partially observable, and in the motor system.
In sensorimotor neuroscience, the uncertainty in the sensory and motor systems can be characterized quantitatively by formulating accurate models, which are helpful to understand behavioral variability \citep{wolpert2000computational}.

Third, many IOC methods are based on matching feature expectations of the cost function between the model and data \citep[][]{ziebart2010modeling}, and are thus not easily adapted to infer other model parameters. In a behavioral experiment, however, researchers are often interested in inferring the noise characteristics of the sensorimotor system or other properties of the agent's internal model besides the cost function \citep[][]{golub2013learning}.

Fourth, while the theory of linear-quadratic-Gaussian (LQG) control \citep{anderson1990optimal} is suited to deal with the issues above, many real-world systems are not captured by the LQG assumptions. First,
the dynamics may be non-linear, e.g., in robotics and motor control when controlling joint angles in a kinematic chain.
Second, the variability of the system may not be captured by normal distributions, e.g., in sensorimotor control, where the standard deviation of sensory and control signals scales with their means.
While iterative methods for solving the optimal control problem such as iterative variants of LQG \citep{todorov2005generalized} exist, here we consider the corresponding inverse problem.

\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \begin{adjustbox}{width=0.49\linewidth}
        \input{tikz/pomdp-agent.tikz}
    \end{adjustbox}
    \begin{adjustbox}{width=0.49\linewidth}
        \input{tikz/pomdp-experimenter.tikz}
    \end{adjustbox}
    \caption{\textbf{A} Decision network from the agent's perspective \citep[notation from][]{kochenderfer2022algorithms}. At each time step, the agent receives a noisy observation $\obs_t$ of the state $\state_t$, performs a control $\action_t$, and incurs a cost $c_t$. \textbf{B} Probabilistic graphical model from the researcher's perspective, who observes a trajectory $\state_{1:T}$ of an agent. Quantities internal to the agent, i.e. observations $\obs_t$, beliefs $\belief_t$ and control signals $\action_t$, are not directly observed.}
    \label{fig:setup}
    \vspace{-0.5cm}
\end{wrapfigure}

To address these issues, we adopt a probabilistic perspective. We distinguish between the control problem faced by the agent and the IOC problem faced by the researcher. From the agent's perspective, the problem consists of acting in a partially observable Markov decision process (POMDP; \cref{fig:setup}~A). 
We consider the setting of continuous states and controls, stochastic non-linear dynamics, partial observations, and finite horizon.
For this setting, there are efficient approximately optimal solutions to the estimation and control problem (see \cref{sec:background}).
The researcher, on the other hand, is interested in inferring properties of the agent's model and cost function. The IOC problem from their perspective can be formulated using a probabilistic graphical model (\cref{fig:setup}~B), in which the state of the system is observed, while variables internal to the agent are latent. 

Here, we unify MCE models, which are agnostic regarding the probabilistic structure causing the observed stochasticity of the agent's policy, with IOC methods that involve an explicit observation model.
First, we define the IOC problem where we allow for both: We employ an explicit observation model, but also allow the agent to have additional stochasticity through an MCE policy.
Second, we provide a solution to the IOC problem in this setting by approximate filtering of the agent's state estimate via local linearization, which allows marginalizing over these latent variables and deriving an approximate likelihood function for observed trajectories given parameters (\cref{sec:ioc}).
By maximizing the approximate likelihood function, an estimate of the optimal parameters can be determined.
Third, we evaluate our proposed method on two classic control tasks, pendulum and cart pole, and on two human behavioral tasks, navigation and a manual reaching (\cref{sec:reaching} - \cref{sec:other-tasks}). Fourth, we show that our approach allows disentangling the influences of perceptual uncertainty and behavioral costs on information-seeking behavior in the light-dark domain (\cref{sec:lightdark}).


\subsection*{Related work}
Inferring costs or utilities from behavior has been of interest for a long time in several scientific fields, such as behavioral economics, psychology, and neuroscience \citep{mosteller1951experimental, kahneman1979prospect, kording2004loss}. 
More specific to the problem formulation adopted here, estimating objective functions in the field of control was first investigated by \citet{kalman1964linear} in the context of deterministic linear systems with quadratic costs. More recent formulations were developed first for discrete state and control spaces under the term inverse reinforcement learning \citep[IRL;][]{ng2000algorithms, abbeel2004apprenticeship}, including formulations allowing for stochasticty in action selection \citep{rothkopf2011preference}.
In this line, the maximum entropy \citep[ME;][]{ziebart2008maximum} and MCE formulation \citep{ziebart2010modeling} gave rise to many new methods, e.g., for non-linear continuous systems via linearization \citep[][]{levine2012continuous} or importance sampling \citep{boularias2011relative} for fully observable deterministic systems.

IOC methods for stochastic systems have been developed in the setting of affine control dynamics \citep{aghasadeghi2011maximum, li2011inverse}.
Arbitrary non-linear stochastic dynamics in the infinite horizon setting have been approached using model-free deep MCE IRL \citep{finn2016guided, garg2021iq}. The latter approaches, however, do not yield interpretable representations, as the cost function is represented by a neural network. Further, past methods based on MCE are limited to estimating cost functions and cannot be used to infer other latent quantities, such as noises or subjective beliefs.
The partially observable setting for IOC has previously been addressed for discrete state-action spaces \citep{choi2011inverse} and continuous states with discrete actions \citep{silva2019continuous}. 
\citet{schmitt2016exact} addressed systems with linear dynamics and continuous controls for a linear switching observation model.
Other work has considered partial observability from the researcher's perspective, e.g., through occlusions \citep{kitani2012activity, bogert2016expectation}.
There are some IOC methods which are applicable to partially observable stochastic systems: In our previous work \citep{schultheis2021inverse} we regarded LQG systems, while the work of \citet{chen2015predictive} can be used to estimate cost functions that depend on the state only. Non-linear dynamics in the infinite-horizon setting and the joint estimation of model parameters have been approached by \citet{kwon2020inverse} by training a policy network as a function of the whole parameter space. This work, however, also assumes the control signals to be given and a stationary, deterministic policy.

Applications of IOC methods range from human locomotion \citep{mombaur2010human} over spatial navigation \citep{rothkopf2013modular}, table tennis \citep{muelling2014learning}, to attention switching \citep{schmitt2017see}, and target tracking \citep{straub2022putting}. Other work has been aimed at inferring other properties of control tasks, e.g., the dynamics model \citep{golub2013learning}, learning rules \citep{ashwood2020inferring}, or discount functions \citep{schultheis2022reinforcement}.
Several subfields of robotics including imitation and apprenticeship learning \citep{osa2018algorithmic} as well as transfer learning \citep{taylor2009transfer} have also employed IOC.
