\subsection{Exponential Growth Regime. Upper Bound}
\label{section:exp-growth-upper}

In this section and the following, we analyze the runtime of a homogeneous rumor spreading process in the regime where the number of informed nodes roughly grows by a constant factor until a linear number $fn$ of nodes is informed. Not surprisingly, this implies that the process takes a logarithmic time to inform a linear number of nodes.

The challenge in the following analysis, which was also faced by previous works, is that in most rumor spreading processes the dissemination speed reduces when more nodes are informed. So it is not true that for all $k \in [1,fn]$, a round starting with $k$ informed nodes ends with an expected number of $k + \gamma k$ nodes, where $\gamma$ is some constant, but rather that we only expect $E_k = \gamma k (1 - \Theta(k/n))$ newly informed nodes. This non-linearity also implies that a round starting with an \emph{expected} number of $k$ nodes does not end with an expected number of $k + E_k$ informed nodes, but less. So we also need to argue that the number of newly informed nodes a round ends with is strongly concentrated around its expectation, and that thus, we can assume that with sufficiently high probability we end up not too far below the expectation (which gives another small loss over the idealized multiplicative increase of the number of informed nodes).

We overcome these difficulties as follows. (i) We formulate an \emph{exponential growth condition} that is satisfied by essentially all homogeneous rumor spreading processes showing an exponential growth regime. The key observation, which allows us to treat many protocols with this single analysis is that it is not necessary that the actions of the nodes show particular independences. It suffices that a relatively mild covariance condition is satisfied. (ii) We then use (throughout the whole regime from the first informed node to a linear number of informed nodes) a simple phase-target argument. (a) We define for each number $k$ of initially informed nodes a \emph{round target} $E_0(k)$ such that a round starting with $k$ informed nodes with (sufficiently high) probability $1-q_k$ ends with $E_0(k)$ informed nodes. Hence the expected time to go from $k$ to $E_0(k)$ or more informed nodes is $t_k = 1 + \frac{q_k}{1-q_k}$. (b) From this, we define a sequence of target $k_0 = 1, k_1 = E_0(k_0), k_2 = E_0(k_1), \dots, k_J = \Theta(n)$ and argue that the time to reach $k_J$ informed nodes is just the sum of the expected times $t_{k_j}$. By defining the round targets in a suitable manner, we ensure that $J = \log_{1+\gamma}(n) + \Theta(1)$ and that the sum of the $t_{k_j}$ is $J + \Theta(1)$. We note that the phase-target argument was also used in~\cite{DoerrK14}, there however only for the push-protocol and only in the regime from $n^s$, $s$ a small constant, to $\Theta(n)$ informed nodes. Consequently, due to the large number of active nodes acting independently, the phase failure probabilities where ignorable small.

In principle, all the arguments outlined above are very elementary and use nothing more advanced than expectations and Chebyshev's inequality. Hence the main technical progress of this work is formulating an exponential growth condition (including the covariance condition) that allows these elementary arguments in a way that the deviations from the idealized ``multiply-by-$\gamma$'' world in the end all disappear in the $\Theta(1)$ term of the dissemination time. These technicalities also appear in some of the following calculations, which therefore, while all not difficult, are at times slightly lengthy. Since arguments similar to the ones in this section are used throughout this work, we give all details in this section and will be more brief in the following ones.

We start in this section with proving an upper bound for the runtime given that we have suitable lower bounds for the probability that an uninformed node becomes informed. In the following section, we prove a lower bound for the runtime given that we have suitable upper bounds on the speed of the progress. These bounds will match apart from additive constants if the growth factor $\gamma$ is identical.


\subsubsection{Exponential Growth Conditions}

Throughout this section, we assume that we regard a homogeneous epidemic protocol which satisfies the following \emph{upper exponential growth conditions} including a covariance condition.

%\merk{add the same text to other sections}

%\merk{A: should we hence split exp-growth and covariance conditions? B: not now, but a some time yes}

\begin{defn} [upper exponential growth conditions]\label{def:upper-exp-growth-conditions}
    Let $\gamma_n$ be bounded between two positive constants.
    Let $a, b, c \ge 0$ and $0 < f < 1$ with $af<1$.
    We say that a homogeneous epidemic protocol satisfies the \emph{upper exponential growth conditions} in $[1,fn[$ if for any $n \in \N$ big enough the following properties are satisfied for any $k < fn$.
	\renewcommand{\theenumi}{(\roman{enumi})}%
    \begin{enumerate}
        \item $\Pk \ge \gamma_n \tfrac{k}{n} \cdot \left(1- a\tfrac{k}{n} - \tfrac{b}{\ln n}\right)$.
        \item $\Ck \le c\tfrac{k}{n^2}$.
    \end{enumerate}
\end{defn}
%\merk{we need only to define somewhere $\Pk$ and $\Ck$}


The main result of this section is that the upper exponential growth conditions imply that the number of informed nodes multiplies by, essentially, $1+\gamma_n$ in each round, and that the expected number of rounds until $fn$ nodes are informed, is at most $\log_{1+\gamma_n}n + O(1)$.

\begin{theorem}[upper bound for the spreading time]\label{th:exp-growth-upper}
    Consider a homogeneous epidemic protocol satisfying the upper exponential growth conditions in $[1,fn[$.
    Then there exist constant $A', \alpha'$ such that
    \begin{eqnarray*}
    	&& \Expect[T(1, fn)] \le \log_{1+\gamma_n} n + O(1), \\
		&& \Pr[T(1,fn) > \log_{1+\gamma_n} n + r] \le A' e^{-\alpha' r} \, \mbox{for any $r \in N$}.
    \end{eqnarray*}
%	In addittion, there exist $\lambda, \nu > 0$ such that for any integer $r > 0$ we have
%	$$\Pr[T(1,fn) > \log_{1+\gamma_n} n + r] \le \lambda e^{-\nu r}.$$
\end{theorem}

\subsubsection{Round Targets and Failure Probabilities}\label{subsection:exp-growth-upper - round targets}
Let us introduce the random variable $X(k)$ being equal to the number of newly informed nodes in a round having $k$ informed nodes at the beginning.
Since $\Expect[X(k)] = p_k (n-k)$, the exponential growth conditions imply $\Expect[X(k)] \ge E(k)$, where
\begin{equation}
	E(k) := \gamma_n k \left(1 - (a+1)\tfrac{k}{n} - \tfrac{b}{\ln n}\right). \notag
\end{equation}

Using Chebyshev's inequality we can show that the value of $X(k)$ is concentrated around its expected value.
Lemma~\ref{lem:exp-growth-failure} hence claims that with good probability, $X(k)$ attains at least the \emph{target value}
\begin{equation}
    E_0(k) := E(k) - Ak^B, \label{eq:def-E0-upper}
\end{equation}
where $A > 0$ and $B \in ]0.5, 1[$ are some constants chosen uniformly for all values of $k$ and $n$.
There are no special conditions on $B$, so we suppose that $B$ is fixed from now on, e.g., to 3/4.
We will, in the following, choose $A$ small enough to ensure that the $-Ak^B$ term has a sufficiently small influence on the general bevahior of  $E_0(k)$.

\begin{lemma}\label{lem:exp-growth-E0-increase}
%\merk{A: we don't need here $f' < f$. B: But maybe it makes still sense...}
There exist $f' > 0$ and $A'>0$ such that for $n$ big enough, the following conditions are satisfied.
\begin{itemize}
	%\item $E(k) \ge \gamma_n k/2$,
	\item $E(\cdot)$ is increasing up to $f'n$, that is, for all $i < j \le f'n$ we have $E(i) < E(j)$;
	\item When $A$ in equation~\eqref{eq:def-E0-upper} satisfies $0 < A < A'$, then also $E_0(\cdot)$ is increasing up to $f'n$;
	\item $E_0(k) > 0$ for all $k \in [1,f'n[$.
\end{itemize}
\end{lemma}
\begin{proof}
	The first claim follows from the second, so let us regard the derivative of $E_0(k)$,
    \[
    	E_0'(k) = \gamma_n - 2\gamma_n(a+1)\tfrac{k}{n} - \gamma_n\tfrac{b}{\ln n} - ABk^{-1+B}.
    \]
    We see that, for any $f' < \tfrac1{2(a+1)}$, any $A > 0$ small enough, and any $n$ large enough, $E_0'(k)$ is positive for all $k \in [1, f'n[$. Therefore, to satisfy the first two parts of the claim, we pick any $f' \in ]0, \tfrac1{2(a+1)}[$ and then any $A' < \tfrac1B \gamma_n(1-2(a+1)f')$.

    To show that $E_0(k) > 0$ for all $k \in [1,f'n[$, it suffices to check this for $k=1$.
    By possibly lowering $A'$ further, we obtain for $n$ large enough that
    \begin{equation}
    	E_0(1) = \gamma_n \left(1 - \tfrac{a+1}{n} - \tfrac{b}{\ln n}\right) - A > 0 \notag.
    \end{equation}
\end{proof}
We assume in the following that $f$ in Definition~\ref{def:upper-exp-growth-conditions} satisfies $f < f'$ and that $A$ in~\eqref{eq:def-E0-upper} was chosen in $]0,A'[$.
%Also, we choose $A$ such that $0 < A < A'$.
%Finally, let us assume that $n$ is sufficiently large.

\begin{lemma}\label{lem:exp-growth-failure}
    For any $k < fn$,
    \begin{equation}
        \Pr[X(k) \le E_0(k)] \le \min\left\{q(k), \tfrac1{1+1/q(1)}\right\} \notag,
    \end{equation}
    where $q(k) := \tfrac{\gamma_n+c}{A^2} \cdot k^{-2B+1}$.
\end{lemma}
\begin{proof}
    By the exponential growth conditions, $\Expect[X(k)] \ge E(k)$.
    Applying Chebyshev's inequality, we compute
    \begin{align*}
        & \Pr[X(k) \le E_0(k)]
    	   = \Pr\left[X(k) \le E(k) \cdot \left(1-\tfrac{Ak^B}{E(k)}\right)\right] \\
        & \le \Pr\left[ X(k) \le \Expect[X(k)]\cdot\left(1-\tfrac{Ak^B}{E(k)}\right) \right] \\
    	& = \Pr\left[ X(k) \le \Expect[X(k)] - Ak^B \cdot \tfrac{\Expect[X(k)]}{E(k)} \right] \\
        & \le \tfrac{\Var[X]}{(Ak^B)^2} \cdot \tfrac{E(k)^2}{\Expect[X(k)]^2}.
    \end{align*}
    From the covariance condition, it follows that $\Var[X(k)] \le \Expect[X(k)] + ck$.
	Using $E(k) / \Expect[X(k)] \le 1$ once, we obtain
    \begin{align*}
        \Pr[X(k) \le E_0(k)]
        & \le \left(1 + \tfrac{ck}{\Expect[X(k)]}\right) \cdot \tfrac{E(k)}{\Expect[X(k)]}
    	   		\cdot \tfrac{E(k)}{A^2k^{2B}} \\
    	& \le \left(1 + \tfrac{ck}{E(k)}\right) \cdot \tfrac{E(k)}{A^2k^{2B}} \\
        & = \tfrac{E(k)+ck}{A^2k^{2B}} \le \tfrac{\gamma_nk + ck}{A^2k^{2B}}.
    \end{align*}
    One can see that for small values of $k$, $q(k)$ might be more than one.
	To avoid such a trivial bound for the failure probability, it suffices to replace Chebyshev's inequality in the proof by the Cantelli's inequality (see Lemma~\ref{lem:prelim:Cantelli}) and bound the probability by $\tfrac1{1+1/q(k)}$.
	To finish the proof we note that $q(k)$ is decreasing in $k$, so
	$\Pr[X(k) \le E_0(k)] \le \min\left\{q(k), \tfrac1{1+1/q(1)}\right\}$.
\end{proof}
%\begin{corollary}\label{cor:exp-growth-error-prob}
%	$\Pr[X(k) \le E_0(k)] \le \min\left\{q(k), \tfrac1{1+1/q(1)}\right\}$.
%\end{corollary}

%One can see that for small values of $k$, $q(k)$ might be more than one.
%To avoid this, it suffices to replace the Chebyshev inequality in the proof by the Cantelli's.
%Then, taking into account that $q(k)$ is decreasing in $k$, we obtain the following.

\subsubsection{The Phase Calculus}\label{subsection:exp-growth-upper - phases}

Having just defined round targets for all numbers $k$ of initially informed nodes and the probabilities that these targets are not achieved within a round, we now proceed to define the sequence $k_j$ of round targets which we aim at satisfying one after the other, ideally within one round per target.
%
%
%
%\merk{B: I'll make this text a little more crisp at some time}
%In the previous section we proved that if at the beginning of the round there were $k$ informed nodes, then at the end there will be at least $k + E_0(k)$ informed nodes, with probability at least $1-q(k)$.

We define recursively
\begin{equation}
    k_0 = 1, \quad k_{j+1} := k_j + E_0(k_j) \notag.
\end{equation}

%\merk{B: I'll write some text here}
\begin{lemma}\label{lem:exp-growth-expk-upper}
	After possibly lowering $A'$ from Lemma~\ref{lem:exp-growth-E0-increase}, there exist $\alpha > 0$ and $J = \log_{1+\gamma_n} n + O(1)$ such that
	\begin{equation}
		fn > k_j \ge \alpha(1+\gamma_n)^j \notag,
	\end{equation}
	for all $j \le J$.
	In particuar, $k_J = \Theta(n)$.
%	In addition, $fn \in [k_{J-1}, k_J[$.
\end{lemma}
%\merk{A: we do need $k_j \le fn$ to guarantee that $k_j > 0$ (see Lemma~\ref{lem:exp-growth-E0-increase}). We also need this condition for Lemma~\ref{lem:exp-growth-ETj-upper}}
%\merk{B: not yet proof-read}
\begin{proof}
	By definition of $k_j$,
	\begin{equation}
		k_j = k_{j-1} + E_0(k_{j-1})
		= k_{j-1}
			\left(1 + \gamma_n - \gamma_n(a+1)\tfrac{k_{j-1}}{n}-\gamma_n\tfrac{b}{\ln n} - Ak_{j-1}^{-1+B}\right)\notag.
	\end{equation}
	Let $\Gamma_n := 1+\gamma_n-\gamma_n\tfrac{b}{\ln n}$.
	Then,
	\begin{equation}
		k_j = \Gamma_n k_{j-1} \left(1 - \gamma_n\tfrac{a+1}{\Gamma_n}\cdot\tfrac{k_{j-1}}{n}
			- \tfrac{A}{\Gamma_n}\cdot k_{j-1}^{-1+B}\right) \notag.
	\end{equation}
	%\merk{A: we have simpler bound for $\Gamma_n$, i.e., we can avoid the $\tfrac1{1+\gamma_n}$ factor.}
	Clearly, $\Gamma_n \ge (1+\gamma_n)(1-\tfrac{b}{\ln n})$.
	By our assumption on $\gamma_n$, $\Gamma_n$ is bounded from above by a constant and is at least $1+\gamma_n/2$ for $n$ big enough.
	Let hence $\tilde{a} := \gamma_n\tfrac{a+1}{1+\gamma_n/2}$ and $\tilde{A} := \tfrac{A}{1+\gamma_n/2}$.
	Then, for any big $n$,
	\begin{equation}
		k_j \ge (1+\gamma_n) \left(1 -
			%\tfrac1{1+\gamma_n}\cdot
			\tfrac{b}{\ln n}\right)
			k_{j-1} \left(1 - \tilde{a}\tfrac{k_j-1}{n} - \tilde{A}k_{j-1}^{-1+B}\right) \notag.
	\end{equation}
    We assume that $A$ (resp. $\tilde{A}$) and $f$ are small enough such that the expression in the brackets is positive.
    Since $k_0 = 1$, by induction we obtain for all $j$ that
    %\merk{(we need here $k_i > 0$, for all $i < j$, or we must keep $f$ in lemma)}
	\begin{equation}
        k_j \ge (1+\gamma_n)^j (1-\tfrac{b}{\ln n})^j
        	\prod_{i=0}^{j-1}\left(1-\tilde{a}\tfrac{k_i}{n} - \tilde{A}k_i^{-1+B}\right) \notag.
    \end{equation}
    By choosing $f$ and $A$ small enough, we can assume that $k_i > 0$ for all $i < j$.
    \begin{equation}
        k_j \ge (1+\gamma_n)^j (1-\tfrac{b}{\ln n})^j
        	\left(1 - \tilde{a}\sum_{i=0}^{j-1} \tfrac{k_i}{n}
        	- \tilde{A}\sum_{i=0}^{j-1} k_i^{-1+B}\right) \notag.
    \end{equation}

    %\merk{A: the following modification on $J$ ensures that $k_j < fn$.}

    Let $J := \log_{1+\gamma_n}(fn) - \Delta r$ for some positive $\Delta r = O(1)$ determined later.
    For $j \le J$ we have $k_j \le (1+\gamma_n)^j$ by construction, and thus $k_j \le fn$.
    Also we have $(1-\tfrac{b}{\ln n})^j = \Theta(1)$.
    In particular this term is at least $2\alpha$ for some $\alpha > 0$ and all $n$ big enough.

    We show by induction on $j$ that $k_j \ge \alpha(1+\gamma_n)^j$ for all $j \le J$.
    The base for $j=0$ and $k_0=1$ is obvious.
    Let $1 \le j \le J$  and let $k_i \ge \alpha(1+\gamma_n)^i$ for all $i<j$.
    By construction, we have $k_i \le (1+\gamma_n)^i$.
    Therefore,
    \begin{align}
    	k_j & \ge 2\alpha (1+\gamma_n)^j
    		\left(1-\tfrac{\tilde{a}}{n}\sum_{i=0}^{j-1}(1+\gamma_n)^i
    		- \tilde{A}\alpha^{-1+B}\sum_{i=0}^{j-1} (1+\gamma_n)^{i(-1+B)}\right) \notag \\
    	& \ge 2\alpha (1+\gamma_n)^j
    		\left(1 - \tilde{a}\cdot\tfrac{(1+\gamma_n)^{-\Delta r}}{\gamma_n}
    		- \tilde{A}\alpha^{-1+B}\cdot\tfrac1{1-(1+\gamma_n)^{B-1}}\right) \notag.
    \end{align}
    By choosing $\Delta r$ large enough and $\tilde{A}$ (resp. $A$) small enough, we can bound the last two expressions by $1/4$, and obtain
    $$k_j \ge 2\alpha(1+\gamma_n)^j (1-1/4-1/4) = \alpha(1+\gamma_n)^j.$$
\end{proof}

By Lemma~\ref{lem:exp-growth-E0-increase}, the $k_j$ form a non-decreasing sequence.
We say that our homogeneous rumor spreading process is in phase $j$ for $j \in \{0,\ldots,J-1\}$, if the number of informed nodes is in $[k_j, k_{j+1}[$.

%\merk{A: we need the next condition for Lemma~\ref{lem:exp-growth-ETj-upper}.}
%Also w.l.o.g. we assume that $J$ is chosen such that $k_J < fn$.
%\merk{A: we enumerate phases from $0$, since the first phase is $[k_0, k_1[$!}

\begin{lemma}\label{lem:exp-growth-ETj-upper}
	If our process is in phase $j<J$, then the number of rounds to leave phase $j$ is stochastically dominated by $1 + \Geom(1-Q_j)$, where $Q_j := \min\left\{q(k_j), \tfrac1{1+1/q(1)}\right\}$.
%	In addition, $\sum_j Q_j = O(1)$.
%	at most
%	\begin{equation}
%		1 + \tfrac{Q_j}{1-Q_j}
%		\text{, where }
%		Q_j := \min\left\{q(k_j), \tfrac1{1+1/q(1)}\right\} \notag.
%	\end{equation}
\end{lemma}
\begin{proof}
	By Lemma~\ref{lem:exp-growth-E0-increase} we have $k + E_0(k) \ge k_j + E_0(k_j) = k_{j+1}$ for any $k_j \le k < fn$.
	By Lemma~\ref{lem:exp-growth-failure},
	\begin{equation}
		\Pr[k+X(k) \le k_{j+1}]
		< \Pr[k+X(k) \le k+E_0(k)]
		< \min\left\{q(k), \tfrac1{1+1/q(1)}\right\} \notag.
	\end{equation}
	Since $q(k)$ is decreasing,
	\begin{equation}
    	\max_{k_{j+1} > k \ge k_j} \Pr[k + X(k) < k_{j+1}] \le Q_j \notag,
	\end{equation}
	and this is an upper bound for the probability to stay in phase $j$ for one round.
	We can thus bound the number of rounds taken to leave phase $j$ by a random variable with geometric distribution $\Geom(1-Q_j)$.
%	Consequently, the expected number of rounds to leave this phase is at most $1 + \tfrac{Q_j}{1-Q_j}$.
\end{proof}

\begin{lemma}\label{lem:exp-growth-sum-Qj}
	$\sum_{j=0}^{J-1} Q_j = O(1)$.
\end{lemma}
\begin{proof}
    We apply the estimate for $q(k_j)$ from Lemma~\ref{lem:exp-growth-failure} and the bounds for $k_j$ from Lemma~\ref{lem:exp-growth-expk-upper}.
    Therefore,
    \begin{align}
        \sum_{j=0}^{J-1} Q_j
        & \le \sum_{j=0}^{J-1} q(k_j)
        	\le \tfrac{\gamma_n+c}{A^2} \cdot \sum_{j=0}^{J-1} k_j^{-2B+1} \notag \\
        & \le \tfrac{\gamma_n+c}{A^2} \cdot \alpha^{-2B+1} \cdot \sum_{j=0}^{J-1} (1+\gamma_n)^{j(-2B+1)} \notag.
    \end{align}
    The last sum is a decreasing geometric series as $B > 0.5$.
    So, $\sum_j Q_j = O(1)$.
\end{proof}

Now we can prove the main result of this section.

\begin{proof}[Proof of Theorem~\ref{th:exp-growth-upper}]
	By Lemma~\ref{lem:exp-growth-expk-upper}, there exists $J = \log_{1+\gamma_n} n + O(1)$ such that $k_J = \Theta(n)$.
	In the following we assume that $J \le \log_{1+\gamma_n} + \tau$ for some constant $\tau$.
	%So there exists some $0 < \tilde{f} < 1$ such that $k_J > \tilde{f}n$ for all $n$ big enough.
	The phase method allows us to bound the number of rounds until at least $k_J$ nodes are informed.
    We denote by the random variable $T_j$ the number of rounds spent in the $j$th phase.
    By Lemma~\ref{lem:exp-growth-ETj-upper}, $T_j$ is stochastically dominated by $1 + \Geom(1-Q_j)$.
    With Lemma~\ref{lem:exp-growth-sum-Qj}, we compute
    \begin{align}
        \Expect[T(1,k_J)]
        & \le \sum_{j=0}^{J-1} \Expect[T_j] \le \sum_{j=0}^{J-1} (1+\tfrac{Q_j}{1-Q_j}) \notag \\
        & = J + \sum_{j=0}^{J-1} \tfrac{Q_j}{1-Q_j} \le J + \tfrac1{1-Q_0}\sum_{j=0}^{J-1} Q_j \notag \\
        & = J + O(1). \notag
    \end{align}
    Since $Q_j$ is bounded by a geometric sequence, Lemma~\ref{lemma:sum geometrical-2} claims that there exist $A'_1, \alpha'_1$ such that $$\Pr[T(1,k_J) > J + r/2] \le A'_1e^{-\alpha'_1r}.$$
    If $k_J < fn$, then we observe that for all $k \in [k_J, fn[$, $p_k$ satisfies the conditions of Lemma~\ref{lem:general-connect}. Therefore, $T(k_J, fn) = O(1)$ and there exist $A'_2, \alpha'_2$ such that $\Pr[T(k_J, fn) > r/2] \le A'_2e^{-\alpha'_2r}$.
    Combining bounds for $T(1,k_J)$ and $T(k_J,fn)$ we obtain the following.
    \begin{eqnarray*}
    	&& \Expect[T(1,fn)] \le \Expect[T(1,k_J)] + \Expect[T(k_J,fn)]
    			\le \log_{1+\gamma_n} n + O(1), \\
    	&& \Pr[T(1,fn) > \log_{1+\gamma_n}n + r] \le A' e^{-\alpha' r},\,
    	\mbox{where $A' := (A'_1+A'_2)e^{\alpha' \tau}$ and $\alpha' := \min\{\alpha'_1,\alpha'_2\}$}.
    \end{eqnarray*}
\end{proof}

%\begin{corollary}
%	For any $0 < s < 1$, there exists $c > 0$ such that
%	$$\Pr\left[T(\lceil n^s \rceil, fn) > (1-s)\log_{1+\gamma_n}n + c\right] < \tfrac1n.$$
%\end{corollary}
%\begin{proof}
%	Again, there exists $J = \log_{1+\gamma_n} + O(1)$ such that $k_J = \Theta(n)$.
%	Obviously, $T(k_{j_0}, k_J)$ is stochastically dominated by $\sum_{j=j_0}^{J-1} T_j$, where $T_j$ are defined in the proof of Theorem~\ref{th:exp-growth-upper}.
%	Since by the proof of Lemma~\ref{lem:exp-growth-ETj-upper}, $T_j$ is stochastically dominated by a random variable having a distribution $1+\Geom(1-Q_j)$, $T$ is bounded by $J-j_0+\Geom(1-\sum_{j=j_0}^{J-1}Q_j)$.
%\end{proof}

%\begin{corollary}
%	There exist $A, \alpha > 0$ such that for any integer $r > 0$ we have
%	$$\Pr[T(1,fn) \le \E[T(1,fn)] + r] \le Ae^{-\alpha r}.$$
%\end{corollary}
%\begin{proof}
%	By Lemma~\ref{lemma:sum geometrical-2} and the proof of Theorem~\ref{th:exp-growth-upper}, we have for some $A', \alpha' > 0$
%	$$\Pr[T(1,k_J) > J + r] \le A'e^{-\alpha'r}.$$
%	If $k_J < fn$, then we observe that for all $k \in [k_J, fn[$, $p_k$ satisfies the conditions of Lemma~\ref{lem:general-connect}, and, thus, from~\eqref{eq:interconnection} it follows that
%	$$\Pr[T(k_J,fn) > r] \le O(1)\cdot \left(1-\max_{k \in [k_J, fn[} p_k\right)^r.$$
%	The claim of corollary directly follows from the fact that both probabilities above are exponentially small in $r$.
%\end{proof}
%\merk{the last equation is a very little inaccurate, since we don't know that we start at $k_J$}
%===========================================================================================
%================           LOWER   BOUND         ==========================================
%===========================================================================================

\subsection{Exponential Growth Regime. Lower Bound}

In this section, we prove a lower bound for an exponential growth regime. We formulate a condition matching the upper bound condition and show that this leads to a lower bound on the rumor spreading time that matches the upper bound apart from a constant number of rounds. We use again the target-phase method.

This is the first time that the target-phase argument is used to prove a lower bound. In the work closest to ours,~\cite{DoerrK14}, only the classic push protocol was regarded. Consequently, there, the simple argument that the number of nodes can at most double each round was sufficient to obtain a lower bound for the growth regime. Such an argument, e.g., is not possible for the classic pull protocol.

The main difference to the upper bound proof lies in the final argument. In the upper bound proof, the failure to reach a round target simply resulted in that we had to try again to reach this target. For the lower bound, a failure is that the process gains more than one phase in one round, resulting in that the time usually spent in these now skipped phases is spared. Arguing that the total time spared by such events is only $O(1)$ needs a slightly more complicated book-keeping of the failure events and a slightly more complicated final argument.

\subsubsection{Exponential Growth Conditions}

We formulate the lower exponential growth condition in an analoguous way as the upper one. In particular, the covariance condition is identical.

\begin{defn} [lower exponential growth conditions] \label{def:lower-exp-growth-conditions}
    Let $\gamma_n$ be bounded between two positive constants and let $a, b, c \ge 0$ and $0 < f < 1$.
    We say that a homogeneous epidemic protocol satisfies the \emph{lower exponential growth conditions} in $[1,fn[$ if for any $n \in \N$ big enough, the following properties are satisfied for any $k < fn$.
	\renewcommand{\theenumi}{(\roman{enumi})}%
    \begin{enumerate}
        \item $p_k \le \gamma_n \tfrac{k}{n} \cdot \left(1 + a\tfrac{k}{n} + \tfrac{b}{\ln n}\right)$.
        \item $c_k \le c\tfrac{k}{n^2}$.
    \end{enumerate}
\end{defn}

These conditions imply the following lower bounds on the rumor spreading time.
\begin{theorem}%[lower bound for the spreading time]
\label{th:exp-growth-lower}
	Consider a homogeneous epidemic protocol satisfying the lower exponential growth conditions in $[1,fn[$. Then there are constant $A', \alpha'>0$ such that 
	\begin{eqnarray*}
    &&\Expect[T(1, fn)] \ge \log_{1+\gamma_n} n - O(1),\\
    &&\Pr[T(1, fn) \le \log_{1+\gamma_n} n - r] \le A' \exp(-\alpha'r)\, \mbox{ for all $r \in \N$}.
  \end{eqnarray*}
  In addition there exists $f' \in ]f,1[$ such that with probability $1-O\left(\tfrac1n\right)$ there are at most $f'n$ informed nodes after $T(1,fn)$ rounds.
\end{theorem}

\subsubsection{Round Targets and Failure Probabilities}

As above, we consider a round with $k$ informed nodes initially.
We define $X(k)$ to be the number of newly informed nodes in this round.
Since  $\Expect[X(k)] = \Pk (n-k)$, the exponential growth conditions give $\Expect[X(k)] \le E(k)$ with
\begin{equation}
	E(k) := \gamma_n k \left(1 + a\tfrac{k}{n} + \tfrac{b}{\ln n}\right). \notag
\end{equation}
Note that we could replace the $a$ above by $a-1$, giving an expression closer resembling the corresponding one from the previous section. Since all these constants do not matter, we preferred the simpler version without the extra~$-1$.

Like in the previous section we introduce
\begin{equation}
	E_0(k) := E(k) + Ak^B, \label{eq:expgrowth-def-E0-lower}
\end{equation}
where $A > 0$ and $B \in ]0.5, 1[$ are some constants chosen uniformly for all values of $k$ and $n$.
Unlike in Section~\ref{section:exp-growth-upper}, it is obvious that $E(k)$ and $E_0(k)$ are increasing.

Note that we can freely replace $f$ in the definition of the lower exponential growth conditions by a smaller constant $f'$, since showing $\E[T(1,f'n)] \ge \log_{1+\gamma_n}(n) - O(1)$ in Theorem~\ref{th:exp-growth-lower} would immediately imply $\E[T(1,fn)] \ge \log_{1+\gamma_n}(n) - O(1)$. Consequently, let us assume that $f$ is small enough such that for any $n$ sufficiently large and $k<fn$,
\begin{equation}
    E(k) \le 2\gamma_n k. \label{eq:exp-growth-lower-42}
\end{equation}

%\merk{the following with need some cleaning}
%Moreover, there exist $f \in ]0, 1[$ and $A' > 0$ such that
%\begin{equation}
%    E(k) \le 2\gamma_n k, \label{eq:exp-growth-lower-42}
%\end{equation}
% or any $A < A'$ and for any $n$ large enough. To ease our notation we assume in the following that $f < f'$, $n$ is sufficiently large and we choose $A$ such that $0 < A < A'$.
%As it was made for the upper bound, we suppose $A$ to be as small as possible to provide the correct cut of the interval $[1,fn]$ into phases.
%Also we suppose that $f < f'$ defined in the following obvious lemma.
%We pick the values of constants such that $E_0(k) \ge 0$, for any $1 \le k < fn$ for some $f \in ]0, 1[$.
%\merk{todo: check the conditions.}
%\begin{lemma}\label{lem:exp-growth-increasing}
%    $k + E_0(k)$ increases on $k$.
%\end{lemma}
%\begin{proof}
%	As the probabilities of becoming informed are the same for all nodes,
%	$$\Expect[X(k)] = \sum_{i=1}^{n-k} \Pr[X_i=1] \ge (n-k) \tfrac{k}{n} \cdot \gamma_n \left(1 - a\tfrac{k}{n} - \tfrac{b}{\ln n}\right).$$
%	The claim follows from the disclosure of the parenthesis.
%\end{proof}

%Let us introduce $E_0(k) := E(k) - Ak^B$ for some chosen values of $A > 0$ and $B \in ]0.5, 1[$.
%We say that the round is successful if it informs at least $E_0(k)$ new nodes, otherwise we say about round "failure".
%We suppose that $B$ is fixed since now on e.g. 3/4, so in the following we will discuss only the choice of $A$.
%
%The motivation of $E_0(k)$ is the following.
%Lemma~\ref{lem:exp-growth-failure} uses Chebyshev's \merk{(Cantelli?)} inequality to prove that the probability of failure is of order $O(k^{1-2B})$.
%Then, introducing the sequence $k_{j+1} := k_j + E_0(k_j)$, we show in Lemma~\ref{lem:exp-growth-kj} that its elements grow exponentially.
%Lemma~\merk{lem:exp-growth-prob-next-phase} argues that the number of informed nodes overcomes the next element $k_{j+1}$ in one round with probability at least $1-O(k_j^{1-2B})$.
%The two last result yield the upper bound for the number of rounds in the exponential growth phase.
%Our goal is to prove that $X(k)$ is at least $E_0(k)$ using the Chebyshev's inequality.
%So the role of $Ak^B$ is some kind of threshold: if it is relatively big then the "failure" probability is relatively small, but we can guarantee the relatively slow progress of the epidemic process.
%On the other hand, the smaller values of $Ak^B$ will make the successful steps longer, but the probabilities of the "failures" will increase.
%\merk{reformulate, difficult to understand, because the phases are not introduced yet}

%\merk{modify this lemma - we need $E(k) \le 2\gamma_n k$}
%\begin{lemma}\label{lem:exp-growth-E0-increase}
%There exists some $0 < f' < 1$ such that if $n$ is big enough, then for any $k \in [1, f'n[$
%\begin{itemize}
%	\item $E(k) \le 2\gamma_n k$,
%	\item $E(k)$ and $E_0(k)$ are increasing on $k$.
%\end{itemize}
%\end{lemma}
%\merk{Formally $E(k)$ and $E_0(k)$ are both increasing for any values of $k$.}

%\merk{we need the probability to go further than the next phase}
%\begin{lemma}\label{lem:exp-growth-failure}
%    Let the exponential growth conditions are satisfied.
%    For any $k < fn$ and $n$ big enough, $\Pr[X(k) \ge E_0(k)] \ge q(k)$, where $$q(k) = \tfrac{2\gamma_n + c}{A^2} \cdot k^{1-2B}.$$
%\end{lemma}
%\begin{proof}
%    By the exponential growth conditions, $\Expect[X(k)] \le E(k)$.
%    Then, applying Chebyshev's inequality, we get the following.
%    \begin{align*}
%        \Pr&[X(k) \ge E_0(k)]
%    	   = \Pr\left[X(k) \ge E(k) \cdot \left(1+\tfrac{Ak^B}{E(k)}\right)\right] \\
%        & \le \Pr\left[X(k) \ge \Expect[X(k)] + Ak^B\right]
%            \le \tfrac{\Var[X]}{(Ak^B)^2} \notag
%    \end{align*}
%    From the the covariance condition of the exponential growth, it follows that $\Var[X(k)] \le \Expect[X(k)] + ck$.
%    Therefore, taking into account that $E(k) \le 2\gamma_n k$,
%    \begin{equation}
%        \Pr[X(k) \le E_0(k)]
%        \le \tfrac{\Expect[X(k)] + ck}{(Ak^B)^2}
%        \le \tfrac{2\gamma_n + c}{A^2} \cdot k^{1-2B} \notag
%    \end{equation}
%\end{proof}

The following lemma will later be used to argue that an unexpectedly fast progress is unlikely. Different from the upper bound analysis in the previous section, we now need a failure probability for different excessive progresses (quantified by the parameter $h$ below).

\begin{lemma}\label{lem:exp-growth-failure-lower}
    For any $k < fn$ and $h = 0, 1, 2, \ldots$,
    \begin{equation}
    	\Pr[X(k) \ge E(k) + Ak^B(1+\gamma_n)^h]
    	\le q_h(k) := \tfrac{2\gamma_n + c}{A^2} \cdot \tfrac{k^{-2B+1}}{(1+\gamma_n)^{2h}} \notag.
    \end{equation}
\end{lemma}
\begin{proof}
    By the exponential growth conditions, $\Expect[X(k)] \le E(k)$.
    By the covariance condition and~\eqref{eq:exp-growth-lower-42},
    \begin{equation}
        \Var[X(k)] \le E(k) + n^2 c_k \le k(2\gamma_n+c) \notag.
    \end{equation}
    Applying Chebyshev's inequality, we obtain
    \begin{align}
    	\Pr&[X(k) \ge E(k) + Ak^B(1+\gamma_n)^h] \notag \\
    	& \le \Pr[X(k) \ge \Expect[X(k)] + Ak^B(1+\gamma_n)^h] \notag \\
    	& \le \tfrac{\Var[X(k)]}{(Ak^B)^2(1+\gamma_n)^{2h}} \notag \\
    	& \le \tfrac{2\gamma_n+c}{A^2} \cdot k^{-2B+1} \cdot \tfrac1{(1+\gamma_n)^{2h}} \notag.
    \end{align}
\end{proof}
%\erk{the same trick as in Lemma~\ref{lem:exp-growth-failure} seems to be impossible, as now $\Expect[X(k)] \le E(k)$.}
%\merk{think if needed}
%Finally we remark here that $q_h(k) < 1$, for any $k > k_0$, where $k_0$ depends only on the process parameters and the choice of constants $A, B$.

%\merk{todo. Think about Cantelli's inequality. It is easier to start from $k_0$ big enough to ensure all probabilities to be less than 1}

%In the case when $q(k) \ge 1$, one can replace Chebyshev's inequality by the Cantelli's one to obtain the following.
%\begin{corollary}\label{cor:exp-growth-error-prob}
%	$\Pr[X(k) \le E_0(k)] \le min\left\{q(k), \tfrac1{1+1/q(1)}\right\}$.
%%    \begin{equation}
%%        \Pr[X(k) \le E_0(k)] \le min\left\{q(k), \tfrac1{1+1/q(1)}\right\} \notag
%%    \end{equation}
%\end{corollary}

%\begin{remark*}
%    Let us denote by $q(k) := \tfrac{\gamma_n}{A^2} \cdot k^{1-2B}$, the obtained above bounds for the failure probability.
%    For smaller values of $k$, $q(k)$ might be more than one.
%    In this case it suffices to apply the Cantelli inequality \merk{(ref?!)} instead of Chebyshev's one and to replace $q$ by $q' \leftarrow \tfrac1{1+1/q}$.
%    One can easily see that $q' < 1$ and $q' < q$.
%\end{remark*}

\subsubsection{The Phase Calculus}
Like in Section~\ref{section:exp-growth-upper}, we define the sequence $k_j$ recursively by
\begin{equation}
	k_0 = 1, \quad k_{j+1} := k_j + E_0(k_j) \notag,
\end{equation}
and obtain the following exponential growth behavior.

%In previous section we proved that if at the beginning of the round there were $k$ informed nodes, then at the end their number will be at least $k + E_0(k)$ with probability at least $1-q(k)$.
%Let us hence introduce the sequence $k_j$, defined recursively as $k_0 = 1$ and $k_{j+1} := k_j + E_0(k_j)$.
%Clearly, if we choose $A$ small enough, then $E_0(k_j)$ still be positive for any $1 \le k < fn$, and $k_j$ is an increasing sequence.
%So it splits $[1, fn]$ into consecutive intervals called phases.
%Moreover, from Lemma~\ref{lem:exp-growth-E0-increase} it follows that $k + E_0(k)$ is an increasing function, in particular $\forall k \ge k_j$, $k + E_0(k) \ge k_j + E_0(k_j) = k_{j+1}$.
%So by Lemma~\ref{lem:exp-growth-failure}, with probability at least $1-q(k)$ the process leaves the current phase in current round.
%Let us consider
%\begin{equation}
%    Q_{j+1} := \max_{k_{j+1} > k \ge k_j} \Pr[k + X(k) < k_{j+1}],
%\end{equation}
%the upper bound for the probability to leave the $j$th phase in one round.
%$q(k)$ is decreasing function, so $Q_{j+1} \le min\left\{q(k_j), \tfrac1{1+1/q(1)}\right\}$ (the second term raises from Corollary~\ref{cor:exp-growth-error-prob} to avoid problems with smaller values of $k_j$).

%So, we can bound the time spent in each phase by the geometrically distributed variable $\Geom(1-Q_{j+1})$.
%The expected number of rounds to leave this phase is at most $ 1 + Q_{j+1}/(1-Q_{j+1})$.
%Therefore, the expected exponential growth time is at most $r + \sum_{j=0}^r Q_{j+1}/(1-Q_{j+1})$, where $r$ is the number of phases in the exponential growth regime.
%To find $r$ we need to study the behavior of $k_j$ first.
%\begin{lemma}\label{lem:exp-growth-expk-lower}
%    There exists some $f \in ]0, 1[$ and $\Delta r > 0$, such that for any $n$ big enough, the sequence $k_j$ splits the interval $[1, fn]$ into at most $\log_{1+\gamma_n} n - \Delta r$ phases by the sequence $k_j$.
%    Moreover,
%    $$k_j \ge k_0 \cdot (1+\gamma_n)^j \cdot O(1), \quad \forall j \le \log_{1+\gamma_n}n - \Delta r.$$
%    %$r := \log_{1+\gamma_n} n + const$, $const$ to determine.
%    %Then for any $j \le r$, $k_j \le (1+\gamma_n)^j \cdot O(1)$.
%\end{lemma}
%\merk{can also write: there exists $const > 0$ such that $$k_j \ge k_0 \cdot (1+\gamma_n)^j \cdot const, \quad \forall j \le \log_{1+\gamma_n}n - \Delta r,$$ but it seems to be to many constants for one lemma.}
%
%\merk{the constructions look more ugly with $k_0 \ne 1$. Can we remove it and multiply just in the end?..}

\begin{lemma}\label{lem:exp-growth-expk-lower}
	By taking $A$ small enough in~\eqref{eq:expgrowth-def-E0-lower}, there exist $\alpha > 0$ and $J = \log_{1+\gamma_n}n - O(1)$ such that for all $j < J$
	\begin{equation}
		(1+\gamma_n)^j \le k_j \le \alpha(1+\gamma_n)^j \text{ and } \; \text k_j < fn \notag.
	\end{equation}
	%In particular, $k_J = \Theta(n)$.
\end{lemma}
%\merk{it is hard to understand without looking the proof that this lemma affects the choice of $A$.}
\begin{proof}
    %Indeed, we observe first that $k_j \ge (1+\gamma_n)^j$.
    Note that %$k_J = \Omega(n)$, since 
    $k_j \ge (1+\gamma_n)^j$ is immediate from the definitions and a simple induction.
    So it remains to show the upper bound on the $k_j$.
%    \merk{A: not sure that I understood correctly the sentence above from your comments...}
    Clearly, by definition of $k_j$,
    \begin{equation}
        k_j \le (1+\gamma_n) (1+\tfrac{b}{\ln n}) k_{j-1} \left(1 + a\tfrac{k_{j-1}}{n}\right)
        	\left(1 + Ak_{j-1}^{-1+B}\right) \notag.
	\end{equation}
    Since $k_0 = 1$, by induction we obtain
	\begin{equation}
        k_j \le (1+\gamma_n)^j (1+\tfrac{b}{\ln n})^j \prod_{i=0}^{j-1}\left(1 + a\tfrac{k_i}{n}\right)
        	\prod_{i=0}^{j-1}\left(1 + Ak_i^{-1+B}\right). \notag %\label{eq:expk-1}
	\end{equation}
	Let $J := \log_{1+\gamma_n}n - \Delta r$ for some $\Delta r = O(1)$ determined later.
    If $j < J$, then $(1-\tfrac{b}{\ln n})^j = \Theta(1)$.
    In particular, it is at most $\tfrac\alpha4$ for some $\alpha > 0$ and any $n$ big enough.
	By the fact that $1+x \le e^x$ for any $x > 0$, we have
	\begin{equation}
		k_j \le \tfrac\alpha4 (1+\gamma_n)^j
		\exp\left(\sum_{i=0}^{j-1}a\tfrac{k_i}{n}\right)
		\cdot \exp\left(\sum_{i=0}^{j-1}Ak_i^{-1+B}\right) \label{eq:exp-growth-upper-eq1}.
	\end{equation}
%	Since $(1+\tfrac{b'}{\ln n})^j \to c := \exp(\tfrac{b'}{\ln(1+\gamma_n)})$, when $j \to \log_{1+\gamma_n} n$, we know that if $j \le \log_{1+\gamma_n}n - 1$, then $(1+\tfrac{b'}{\ln n})^j \le c$.
	We prove the claim of lemma by induction on $j$. Assume that for some $j < J$ we have $k_i \le \alpha(1+\gamma_n)^i$ for any $i < j$.
%	We can assume $j \ge 2$.
	Since $k_i \ge (1+\gamma_n)^i$ for all $i$, both sums in~\eqref{eq:exp-growth-upper-eq1} can be bounded by geometric series.
	Therefore,
	\begin{equation}
		k_j \le \tfrac\alpha4 (1+\gamma_n)^j
		\exp\left(\sum_{i=0}^{j-1}\tfrac{a}{n} \cdot \alpha(1+\gamma_n)^i\right)
		\cdot \exp\left(\sum_{i=0}^{j-1}A(1+\gamma_n)^{i(-1+B)}\right) \notag.
	\end{equation}
	Since $j < J$, by choosing $\Delta r$ large enough and $A$ small enough, we can bound both sums by any positive constant, in particular by $\ln 2$.
	Therefore, for any $j < J$,
	\begin{equation}
		k_j \le \tfrac\alpha4(1+\gamma_n)^j \exp(\ln2) \cdot \exp(\ln 2) = \alpha(1+\gamma_n)^j\notag.
	\end{equation}
%	Observe that the second summands in both brackets form the geometric series.
%	Thus,
%	\begin{equation}
%		\tfrac{a'c}{n}\sum_{i=1}^{j-1} (1+\gamma_n)^i
%		\le \tfrac{a'c}{n}(1+\gamma_n)^j/\gamma_n
%		\le \tfrac12 \notag,
%	\end{equation}
%	if $j \le \log_{1+\gamma_n} n - \Delta r$ for the appropriate choice of $\Delta r$.
%	On the other hand,
%	\begin{equation}
%		A' \sum_{i=0}^{j-1} (1+\gamma_n)^{i(B-1)}
%		\le A' \tfrac1{1-(1+\gamma_n)^{B-1}}
%		\le \tfrac12 \notag,
%	\end{equation}
%	if $A'$ (resp. $A'$) is small enough.
%	Finally, by Lemma~\ref{lemma:Bernoulli}, both products are at most 2, so $k_j \le 4k_0c(1+\gamma_n)^j$.
\end{proof}

By definition, the $k_j$ form a non-decreasing sequence.
Like in Section~\ref{section:exp-growth-upper}, we say that the rumor spreading process is in phase $j$ for $j = 0, \ldots, J-1$, if the number of informed nodes is in $[k_j, k_{j+1}[$.

%\merk{A: Indeed, we don't need $j < J-h$ for the next lemma. But if $j+h \ge J$, the corresponding phase is not well defined, so we say "the probability that the number of informed nodes is at least $k_{j+h}$ at the end of the round".}
\begin{lemma}\label{lem:exp-growth-jump-prob}
	Let $h \ge 2$.
	If the process is in phase $j < J$ at the beginning of one round, then the probability that the number of informed nodes is at least $k_{j+h}$ at the end of the round, is at most $q_{h-2}(k_j)$.
\end{lemma}
\begin{proof}
	For $1 \le k \le k_{j+1}$, we have $$k + E(k) + Ak^B \le k_{j+1} + E(k_{j+1}) + Ak_{j+1}^B = k_{j+2}.$$
	Since $k_{j+h} \ge (1+\gamma_n)^{h-2} k_{j+2}$, we have
	\begin{equation}
		k_{j+h}
		\ge (1+\gamma_n)^{h-2} \left(E(k) + Ak^B + k\right)
		\ge k + E(k) + Ak^B(1+\gamma_n)^{h-2} \notag.
	\end{equation}

    By Lemma~\ref{lem:exp-growth-failure-lower}, the maximum probability to have at least $k_{j+h}$ informed nodes at the end of the round is
    %``jump" from phase $j$ to phase $j+h$ (or further) is
    \begin{align}
        & \max_{k\in [k_j, k_{j+1}[} \Pr[k+X(k) \ge k_{j+h}] \notag \\
    	& \le \max_{k\in [k_j,k_{j+1}[} \Pr[k+X(k)\ge k+E(k)+Ak^B(1+\gamma_n)^{h-2}] \notag \\
    	& \le \max_{k\in [k_j,k_{j+1}[} q_{h-2}(k) \le q_{h-2}(k_j) \notag.
    \end{align}
    The last inequality follows from the fact that since $B > 1/2$, $q_{h-2}(\cdot)$ decreases.
%    The last expression is increasing on $k$, so the sought probability is at most $q_h(k_j)$.
\end{proof}

%\begin{lemma}\label{lem:exp-growth-sum-Qj}
%	Let $r = \log_{1+\gamma_n} n - \Delta r$ satisfies the conditions of Lemma~\ref{lem:exp-growth-expk}.
%	Then, $\sum_{j=1}^r Q_j = O(1)$.
%\end{lemma}
%\begin{proof}
%    Indeed, $\sum Q_j \le \sum q(k_j)$.
%    Then we apply the estimate for $q(k_j)$ from Lemma~\ref{lem:exp-growth-failure} and the bounds for $k_j$ from Lemma~\ref{lem:exp-growth-expk}.
%    Therefore,
%    \begin{align}
%        \sum_{j=1}^r Q_j
%        & \le \left(1+\tfrac{2c}{\gamma_n}\right) \cdot \tfrac{\gamma_n}{A^2} \cdot \sum_{j=1}^r k_j^{1-2B} \notag \\
%        & \le O(1) \cdot \sum_{j=1}^r (1+\gamma_n)^{j(1-2B)}.
%    \end{align}
%    The last sum is the sum of decreasing geometric series, as $B > 0.5$.
%    So, $\sum_j Q_j = O(1)$.
%\end{proof}%

With Lemma~\ref{lem:exp-growth-expk-lower}~and~\ref{lem:exp-growth-jump-prob}, we can now prove Theorem~\ref{th:exp-growth-upper}.%\merk{B: I'll write some clever text here some time}

\begin{proof}[Proof of Theorem~\ref{th:exp-growth-lower}]
	Let $S$ be the set of visited phases, e.g., if the process does not jump over any phase, then $S = \{0,\ldots,J-1\}$.
	By $\tau_j$ we denote the number of rounds spent in the $j$th phase.
	So the spreading time $T(k_0, k_J) = \sum_{j\in S} \tau_j$.
	We do not know the size of $S$, so in order to bound the spreading time below, let us introduce the random variable $\Delta_j$ which is equal to the length of the jump from the $j$th phase when the process leaves it.
	Let also $d_j := \Delta_j - \tau_j$.
	Since $\sum_{j\in S} \Delta_j = J$, we have $T(k_0, k_J) = J - \sum_{j\in S} d_j$.
%	Let us study the distribution of $d_j$.
	By definition, for $j \in S$ and $h > 0$, we have $\Pr[d_j \ge h] \le \Pr[\Delta_j \ge h+1]$.
%	\begin{equation}
%		\Pr[d_j \ge h]
%		= \sum_{t \ge 1} \Pr[\Delta_j \ge h + t \text{ and } \tau_j = t]
%		\le \sum_{\Delta h \ge 1} \Pr[\Delta_j \ge h + t] \notag
%	\end{equation}
	Then, by Lemma~\ref{lem:exp-growth-failure-lower}~and~\ref{lem:exp-growth-jump-prob},
	\begin{equation*}
		\Pr[d_j \ge h]
		\le q_{h-1}(k_j)
		\le \tfrac{2\gamma_n+c}{A^2} \tfrac{k_j^{-2B+1}}{(1+\gamma_n)^{2h-2}}.
	\end{equation*}
%	\begin{align}
%		\Pr&[d_j \ge h]
%		\le \sum_{t \ge 1}
%			\tfrac{2\gamma_n+c}{A^2}\cdot\tfrac{k_{j-1}^{-2B+1}}{(1+\gamma_n)^{h+t-1}} \notag \\
%		& \le \tfrac{2\gamma_n+c}{A^2} \tfrac{k_{j-1}^{-2B+1}}{(1+\gamma_n)^h}
%			\sum_{t \ge 0} (1+\gamma_n)^t
%		\le k_{j-1}^{-2B+1} \cdot \tfrac{O(1)}{(1+\gamma_n)^h} \notag.
%	\end{align}
  The above argument shows that $T(k_0, k_J)$ stochastically dominates $J - D$, where $D$ is the sum of independent non-negative integer random variables $D = \sum_{j=0}^{J-1} D_j$ satisfying $\Pr[D_j \ge h] \le \tfrac{2\gamma_n+c}{A^2} \tfrac{k_j^{-2B+1}}{(1+\gamma_n)^{2h-2}}$ for all $h \ge 1$. Let $R_h := \{(r_0, \dots, r_{J-1}) \in \Z^J_{\ge 0} \mid \sum_{j=0}^{J-1} r_i = h\}$ for all $h \ge 1$. We compute
  \begin{align*}
  \Pr[D \ge h] & \le \sum_{r \in R_h} \prod_{j = 0}^{J-1} \Pr[D_j \ge r_j] \\
  &\le (1+\gamma_n)^{-2h} \sum_{r \in R_h} \prod_{j \in [0..J-1], r_j >0} \tfrac{2\gamma_n+c}{A^2} \tfrac{k_j^{-2B+1}}{(1+\gamma_n)^{-2}}\\
  &\le (1+\gamma_n)^{-2h} \sum_{M \subseteq [0..J-1]} \prod_{j \in M} \tfrac{2\gamma_n+c}{A^2} \tfrac{k_j^{-2B+1}}{(1+\gamma_n)^{-2}}\\
  &\le (1+\gamma_n)^{-2h} \prod_{j \in [0..J-1]} \bigg(1+\tfrac{2\gamma_n+c}{A^2} \tfrac{k_j^{-2B+1}}{(1+\gamma_n)^{-2}}\bigg)\\
  &\le (1+\gamma_n)^{-2h} \exp\bigg(\sum_{j \in [0..J-1]} \tfrac{2\gamma_n+c}{A^2} \tfrac{k_j^{-2B+1}}{(1+\gamma_n)^{-2}}\bigg)\\
  &\le (1+\gamma_n)^{-2h} O(1),
\end{align*}
	where the last estimate uses  Lemma~\ref{lem:exp-growth-expk-lower}. This proves that tail bound statement. For the claim on the expected rumor spreading time, we compute
	\begin{align*}
		\Expect[D] \le \sum_{h \ge 1} \Pr[D \ge h] \le \sum_{h \ge 1} (1+\gamma_n)^{-2h} O(1) = O(1).
	\end{align*}
  Finally, by Lemma~\ref{lem:general-connect-lower}, there exists $f' \in ]f,1[$ such that with probability $1-O\left(\tfrac1n\right)$ there are at most $f'n$ informed nodes after $T(1,fn)$ rounds.
\end{proof} 