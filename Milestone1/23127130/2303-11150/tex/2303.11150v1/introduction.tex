%
%\section{Introduction}
%
%1. Motivation and general rumor spreading.
%
%2. Our problem setting.
%
%3. State of Art.
%
%4. Our contribution.
%
%\subsection{Motivation of Rumor Spreading}
%
%By the rumor spreading protocols we understand epidemic-like processes in the network having the goal to involve all nodes in the network.
%The typical setting is following.
%Let a graph $G$ given and some nodes are \emph{informed} - they share some information called \emph{rumor}.
%Nodes can call their neighbors and communicate with them.
%During the communications the rumor goes from informed nodes to uninformed ones until each node is informed.
%
%Often, nodes choose their neighbors to call using some random law, such protocols are distributed and scalable.
%Another of important properties of such protocols is their robustness on the complete graphs.
%This means that the \emph{spreading time} (the time until all nodes are informed) of the protocol running on the complete graph doesn't change a lot even if a lot of calls or nodes are "broken" and cannot forward the rumor.
%Thus the rumor spreading are quiet useful for many problems in which we cannot guarantee the stability of the communications, such as \ldots.
%
%Also, as the idea of the rumor spreading was inspired by the human-based rumor spreading, the analysis of the gossip protocols can help understanding of the rumor spreading in social networks (cf \merk{ref?}).
%
%Finally, the rumor spreading protocols can be used as the primitive in the analysis of the randomized distributed algorithms to determine the time until all nodes are involved in the process.
%
%\subsection{Problem Setting}
%
%\begin{itemize}
%\item
%	In current work we assume that $G$ is a complete graph with $n$ vertices.
%\item
%	Therefore, the result doesn't depend on the choice of the initially informed node.
%\item
%	We consider a \emph{synchronous} rumor spreading, so all calls (and rumor transactions) take place in discrete time steps called \emph{rounds}.
%\item
%	\emph{Random call model} means that nodes choose their calls uniformly and independently at random.
%\item
%	The call emitted by informed node is \emph{push} call, otherwise it is \emph{pull} call.
%	The \emph{push, pull, or push\&pull protocols} are protocols in which only push, pull or both types of calls are allowed correspondingly.
%\item
%	By the \emph{spreading time} we mean the number of rounds after which all nodes are just informed.
%\end{itemize}
%
%\subsection{Known Results}
%
%Although we can run the rumor spreading process on any graph, in this paper we will restrict ourselves on the complete graphs only and let $n$ be the number of vertices in the graph.
%We also consider only the synchronous protocols, in which the calls take place simultaneously in some discrete stops of time called \emph{rounds}.
%
%Let us introduce some most basic protocols.
%In the simplest case each node chooses a neighbor to call uniformly at random and independently from the other nodes.
%Such setting allows us to introduce the following three protocols.
%In the \emph{push protocol}, only the informed nodes make calls and inform nodes if they get on uninformed ones.
%The \emph{pull protocol} is vice versa -- only uninformed nodes make calls and get informed if they hit informed ones.
%Finally, in the \emph{push\&pull protocol} each node makes calls.
%If within one call one node is informed and another one isn't, then the second node becomes informed.
%Hereinafter we will say that the outgoing calls of the informed nodes are \emph{push calls}, and the outgoing calls of uninformed nodes are \emph{pull calls}.
%
%All three protocols seems to be well studied and the known bounds of the spreading are in the following table.
%
%\begin{table}
%%	\caption{Spreading time of random call protocols run on complete graph of size $n$}
%	\begin{center}
%	    \begin{tabular}{ | p{2.2cm} | p{6cm} | c |}
%    		\hline
%	    	Protocol & Spreading time & Reference \\ \hline
%	    	Push && \\ \hline
%			Pull && \\ \hline
%			Push \& pull&& \\ \hline
%    	\end{tabular}
%	\end{center}
%\end{table}
%
%\subsection{Our Contribution}
%
%It is easy to note that the rumor spreading for the protocols described above can be split into two different regimes contributing two main terms in the spreading time.
%The first regime is exponential growth -- while there are relatively few informed nodes, the number of them multiplies by almost the same constant each round.
%Such process lasts until some constant fraction $fn$ of nodes is informed, implying a logarithmic term in the rumor spreading time.
%The second regime is exponential or double exponential growth.
%Such regime starts since less than some fraction $gn$ of nodes stays uninformed and lasts until the last uninformed node.
%During this time the fraction of uninformed nodes shrinks exponentially or double exponentially.
%
%Thus, both push and pull protocols have exponential growth regimes in the beginning.
%But the pull protocol is prone to the double exponential shrinking instead of the exponential shrinking regime for the push protocol.
%%
%In this paper we introduce the general notation of epidemic protocol - a round-based process in which the uninformed nodes gets informed according to some Bernoulli random variables $X_i$ which can depend on the number of informed nodes $k$ or uninformed ones $u$.
%After studying such protocol we obtained conditions that the epidemic protocol has exponential growth, exponential shrinking or double exponential shrinking regimes.
%They are presented in the table below in terms of the atomic probability $\Pr[X_i=0]$, and the covariance $\Cov[X_i,X_j]$.
%$n$ is the size of the network, $P(n), \rho, \ell, c, \alpha$ are some parameters of the concrete protocols.
%
%\begin{center}
%    \begin{tabular}{ | p{2cm} | p{2.3cm} | c | c | p{3cm} |}
%    \hline
%     & Number of nodes & $Pr[X_i=0]$ & $Cov$ & Expected spreading time up to $O(1)$ \\ \hline
%    Exponential growth & $0\ldots O(n)$ & $P(n)\tfrac{k}{n}\left(1+O(\left(\tfrac{k+\ln n}{n}\right)\right)$ & $\le c\tfrac{k}{n^2}$ & $\log_{1+P(n)} n + O(1)$ \\ \hline
%    Exponential shrinking & $O(n)\ldots n$ & $e^{-\rho}\left(1+O\left(\tfrac{u}{n}\right)\right)$ & $\le\tfrac{c}{u}$ & $\tfrac1\rho \ln n + O(1)$ \\ \hline
%    Double exponential shrinking & $O(n)\ldots{n-n^\alpha}$ & $O\left(\tfrac{u}{n}\right)^{\ell-1}$ & $\le c \cdot \tfrac{u^2}{n^3}$ & $\log_\ell \ln n + O(1)$ \\
%    \hline
%    \end{tabular}
%\end{center}
%
%Of course, it remains some rounds between two regimes, but Lemma\merk{REF?!} shows that in the most of cases their number is at most $O(1)$ in expectation.
%So these conditions can be used as the general method for the analysis of the rumor spreading rumor spreading protocol.
%To justify this, we provide some short proofs for the well studied basic protocols.
%Then we introduce new single incoming call protocols, in which we limit the number of communications, in which one node can communicate in one round (it is easy to see that for the push protocol on large graph, the corresponding number can be arbitrary big).
%We prove that the single incoming call push\&pull protocol is prone to exponential growth and shrinking.
%It is slower than standard push\&pull protocol, as the push calls from informed nodes are less effective in the end "spam" the network, taking the place of potentially effective push calls.
%So we modify the protocol to the following: the rumor is transfered together with its age (number of rounds since the first rumor transaction), while it is "hot" the protocol runs through the push\&pull mechanism, since the rumor is "cold" the push calls are turned off.
%We finally prove that \emph{fast single incoming call protocol} is prone to the double exponential shrinking regime and to almost the exponential growth regime, therefore the expected spreading time is $\log_{3-2/e} n + \log_2\ln n + O(1)$.

\section{Preliminaries}

In this section, for the sake of completeness, we collect some elementary facts which are well-known.

\subsection{Variance. Chebyshev's and Cantelli's Inequalities}
We recall that the \emph{variance} of a discrete random variable $X$ is $\Var[X] = \Expect[X^2]-\Expect[X]^2$.
By definition it is a measure of how well $X$ is concentrated around its mean.
The two following inequalities gives the bounds for the ``tail'' probabilities for any random variable $X$.
\begin{lemma}[Chebyshev's inequality]\label{lem:prelim:Chebyshev}
    For all $\lambda>0$,
    \[
        \Pr\left[|X-\Expect[X]|] \ge \lambda\sqrt{\Var[X]}\right] \le \tfrac1{\lambda^2}.
    \]
\end{lemma}
There is a one-sided version of the Chebyshev inequality attributed to Cantelli, replacing $\tfrac1{\lambda^2}$ by $\tfrac1{\lambda^2+1}$.
\begin{lemma}[Cantelli's inequality]\label{lem:prelim:Cantelli}
    For all $\lambda > 0$,
    \[
        \Pr\left[X-\Expect[X] \ge \lambda\sqrt{\Var[X]}\right] \le \tfrac1{1+\lambda^2}.
    \]
\end{lemma}
We remark that Cantelli's inequality gives the bound which is less than one for any positive $\lambda$.

In addition we provide a simple method to bound a variance of a sum of indicator random variables.
We recall that the \emph{covariance} of two discrete random variables $X$ and $Y$ is $\Cov[X,Y] = \E[(X-\E[X])(Y-\E[Y])]$.
\begin{lemma}\label{lem:prelim:variance}
    Let a random variables $X = \sum_{i=1}^n X_i$, where $X_i$ are indicator random variables.
    Suppose, for any $i \ne j$ we have $\Cov[X_i,X_j] \le c$ for some constant $c$.
    Then $\Var[X] \le \E[X] + cn^2$.
\end{lemma}
\begin{proof}
    Since $X_i$ is a binary random variable, $\Var[X_i] \le \E[X_i]$.
    Therefore,
    \[
        \Var[X] \le \sum_{i=1}^n \Var[X_i] + \sum_{i \ne j} \Cov[X_i,X_j]
        \le \E[X] + cn^2.
    \]
\end{proof}


%    First of all we recall here the \emph{Chernoff bounds} and the term of \emph{negative correlation} that we will use in the proofs.
%    \begin{recall*}[Chernoff bound]
%        Let $X = \sum_{i=1}^n X_i$, where the $X_i$ are independently distributed in $[0,1]$.
%        Then for all $0 < \delta < 1$,
%        \begin{align*}
%            \Pr[ X \ge (1+\delta) \E X ] \le \exp\left( -\frac{\delta^2}3 \text EX \right), \\
%            \Pr[ X \le (1-\delta) \E X ] \le \exp\left( -\frac{\delta^2}2 \text{E}X \right).
%        \end{align*}
%    \end{recall*}
%
%    We will apply this bound in the case of negative correlation:
%    \begin{def*}
%        Binary random variables $X_1, \ldots, X_n$ are negatively correlated if for any multi-index $I \subseteq [n]$,
%        \begin{enumerate}
%            \item $\Pr[ \forall i \in I, X_i = 1] \le \prod_{i \in I}\Pr[X_i = 1]$
%            \item $\Pr[ \forall i \in I, X_i = 0] \le \prod_{i \in I}\Pr[X_i = 0]$
%        \end{enumerate}
%    \end{def*}
%    If $X_1, \ldots, X_n$ are negatively correlated, we also can apply Chernoff bound to them.
%    The proofs of the facts above can be found in ~\cite{BookChapter}.

\subsection{Geometric Distribution and Stochastic Domination}
    \begin{def*}
        We say that a random integer variable $G$ has a \emph{geometric distribution} with success probability $p$ and write $G \sim \Geom(p)$ if $\Pr[G=k] = p(1-p)^k$ for any $k \ge 0$.
    \end{def*}
    The geometric distribution corresponds the number of failed Bernoulli trials until the first success.
    Recall that if $G \sim \Geom(p)$, then we have $\E[G] = \tfrac{1-p}{p}$ and $\Var[G] = \tfrac{1-p}{p^2}$.

    Another important concept is the stochastic domination.
    Informally, a random variable $X$ dominates a random variable $Y$ if $X$'s distribution is ``to the right'' of the $Y$'s distribution.
    \begin{def*}\label{definition:stochastic domination}
        Let a pair of random variables $X, Y$ be given.
        We say that $X$ \emph{stochastically dominates} $Y$, and write $Y \dominated X$, if
        $\Pr[X \ge x] \ge \Pr[Y \ge x]$
        for all $x$.
    \end{def*}
    The stochastic domination satisfies the following elementary properties.
    \begin{itemize}
        \item if $X \dominated Y$ and $Z \dominated T$, then $X+Y \dominated Z+T$.
        \item if $X \dominated Y$ then $\E X \le \E Y$.
    \end{itemize}
    \begin{lemma}[\cite{DoerrK14}]\label{lemma:sum geometrical}
        Let $G_1, \ldots, G_n$ be independent random variables with $G_i \sim \Geom(1-q_i)$.
        Then $\sum_{i=1}^n G_i$ is stochastically dominated by a random variable $G$ with $G \sim \Geom(1-\sum_{i=1}^n q_i)$
    \end{lemma}
%    \begin{proof}
%        Let $G_1, G_2$ be independent geometrically distributed random variables with success probabilities $1-q_1$ and $1-q_2$, respectively.
%        By law of total probability, we compute for all $t \ge 0$,
%        \begin{align*}
%            \Pr[ G_1+G_2 \ge t ]
%                = \left( \sum_{k=0}^{t-1}\Pr[G_1=k] \Pr[G_2\ge t-k] \right) + \Pr[G_1 \ge t] = \\
%            = \left( \sum_{k=0}^{t-1} (1-q_1)q_1^k q_2^{t-k} \right) + q_1^k
%                \le \sum_{k=0}^t q_1^k q_2^{t-k}
%                \le \sum_{k=0}^t \binom tk q_1^k q_2^{t-k} = (q_1+q_2)^t.
%        \end{align*}
%        Hence, $G_1 + G_2 \dominated \Geom(1-(q_1+q_2))$.
%        By successive application of this fact, we obtain
%        $\sum_{i=1}^n G_i \dominated \Geom(1-\sum_{i=1}^n q_i)$.
%    \end{proof}
    
%\begin{lemma}\label{lemms:whp sum geometrical}
%	Let $T$ stochastically dominated by $\sum_{j=0}^{J-1} T_j$, where $T_j \sim 1+\Geom(1-q_j)$.
%	Let $\sum_{j=0}^{J-1} q_j \le n^{-\alpha}$ for some $\alpha > 0$.
%	Then there exists $c > 0$ such that $\Pr[T > J+c] < \tfrac1n$.
%\end{lemma}
%\begin{proof}
%	By Lemma~\ref{lemma:sum geometrical}, $T$ is stochastically dominated by $J + \Geom\left(1-\sum_{j=0}^{J-1}q_j\right)$.
%	Therefore, $\Pr[T > J+c] \le n^{-\alpha c}$.
%	So any constant $c > \tfrac1\alpha$ satisfy the claim of lemma.
%\end{proof}

The following lemma contains a high probability bound for the sum of geometrically distributed variables in the case when $\sum_i q_i = O(1)$, but not necessarily less than 1.

%\merk{I formulated this lemma in terms of $G_1, \ldots, G_n$. The version in terms of Theorem~\ref{th:exp-growth-upper},~\ref{th:exp-shrinking-upper},~and~\ref{th:double-exp-shrinking-upper} is commented below.}
%\merk{I used $\lambda$ and $\nu$ instead of $q$ and $c$ when applied this lemma. Should we rename?}
\begin{lemma}~\label{lemma:sum geometrical-2}
	Let $\eps, \delta \in ]0,1[$ and $s > 0$.
	Let $q_j := \min\{1-\eps, s\delta^j\}$, for any $j$.
	Let $G$ be stochastically dominated by $\sum_{j=0}^{J-1} G_j$, where $G_j \sim \Geom(1-q_j)$.
	Then there exist constant $A, \alpha > 0$ such that for any integer $r>0$ we have $\Pr[G > r] \le Ae^{-\alpha r}$.
\end{lemma}
\begin{proof}
	Let $j_0$ is the smallest such that $\sum_{j\ge j_0} q_j < 1-\eps$.
	By construction, $j_0 = O(1)$.
	By Lemma~\ref{lemma:sum geometrical}, $\sum_{j=j_0}^{J-1} G_j$ is stochastically dominated by a random variable with distribution $\Geom(\eps)$.
	Therefore, for any integer $r>0$ we have
	$$\Pr\left[\sum_{j=j_0}^{J-1} G_j > \tfrac{r}{j_0+1}\right] \le (1-\eps)^{r/(j_0+1)}.$$
	Similarly, for any $j < j_0$ we have $\Pr[G_j > \tfrac{r}{j_0+1}] \le (1-\eps)^{r/(j_0+1)}$.
	We conclude,
%	Applying the union bound we obtain that
	$$\Pr[G>r] \le (j_0+1) \cdot (1-\eps)^{r/(j_0+1)}.$$
%	Clearly, there exists some $q, c>0$ such that $\Pr[T > J+k] \le qe^{-ck}$.
\end{proof}
%\begin{lemma}~\label{lemma:sum geometrical-2}
%	Let $\eps, \delta \in ]0,1[$ and $s > 0$.
%	Let $q_j := \min\{1-\eps, s\delta^j\}$, for any $j$.
%	Let $T$ is stochastically dominated by $\sum_{j=0}^{J-1} T_j$, where $T_j \sim 1 + \Geom(1-q_j)$.
%	Then, there exists $q, c > 0$ such that $\Pr[T > J+k] \le qe^{-ck}$ for any integer $k$.
%\end{lemma}
%\begin{proof}
%	Let $j_0$ is the smallest such that $\sum_{j\ge j_0} q_j < 1-\eps$.
%	By construction, $j_0 = O(1)$.
%	By Lemma~\ref{lemma:sum geometrical}, $\sum_{j=j_0}^{J-1} T_j$ is stochastically dominated by $J-j_0+\Geom(\eps)$.
%	Therefore, $$\Pr[\sum_{j=j_0}^{J-1} T_j > J - j_0 + \tfrac{k}{j_0+1}] \le (1-\eps)^{k/(j_0+1)},$$ for any integer $k$.	
%	Similarly, for any $j < j_0$ we have $\Pr[T_j > 1 + \tfrac{k}{j_0+1}] \le (1-\eps)^{k/(j_0+1)}$.
%	Therefore,
%%	Applying the union bound we obtain that
%	$$\Pr[T>J+k] \le (j_0+1) \cdot (1-\eps)^{k/(j_0+1)}.$$
%%	Clearly, there exists some $q, c>0$ such that $\Pr[T > J+k] \le qe^{-ck}$.
%\end{proof}

Finally, the following lemma will be used to argue that in the Erd\H{o}s-R\'enyi graph with $n$ vertices and edge probability $\tfrac an$, $a >0$ a constant, the maximum vertex degree at most $O(\log n)$ with high probability. This follows immediately from a simple Chernoff bound argument (as would the sharp $O(\log(n) / \log\log(n))$ bound, which we do not need).
\begin{lemma} \label{lemma: log degree}
	For any $a > 0$ there exists $c > 0$ such that
	$\Pr\left[\Bin\left(n,\tfrac an\right) \ge c \log n\right] \le \tfrac1n$.
\end{lemma}

\subsection{First Order Bounds}

%	\begin{lemma}\label{lemma:Bernoulli}
%		Let $x_i$ are $n$ non-negative numbers, such that their sum equals to some number $s < 1$.
%		Then $\prod_{i=1}^n (1+x_i) \le \tfrac1{1-s}$.
%	\end{lemma}
%	\begin{proof}
%		\begin{align*}
%			\prod_{i=1}^n & (1+x_i)
%				\le 1 + \sum_{i=1}^n x_i + \sum_{i\ne j}^n x_ix_j + \ldots + x_1\cdot\ldots\cdot x_n\\
%			& \le 1 + s + s^2 + \ldots + s^n
%				\le \tfrac1{1-s}.
%		\end{align*}
%	\end{proof}

\begin{lemma}\label{lem:prelim:(1-1/n)^n}
    For any $n > 0$ we have $\tfrac1e - \tfrac1{en} \le \left(1-\tfrac1n\right)^n \le \tfrac1e$.
\end{lemma}
\begin{lemma}\label{lem:prelim:(1-1/n)^k}
    For any $k < n$ we have
    $1-\tfrac{k}{n} \le \left(1-\tfrac1n\right)^k \le 1-\tfrac{k}{n} + \tfrac{k^2}{2n^2}$.
\end{lemma}
Replacing $n$ by $n/p$ for some $p > 0$ we get the following.
\begin{corollary}\label{cor:prelim:(1-p/n)^k}
    For any $k < n/p$ we have
    $1-p\tfrac{k}{n} \le \left(1-\tfrac{p}{n}\right)^k \le 1-p\tfrac{k}{n} + \tfrac{p^2k^2}{2n^2}$.
\end{corollary}
\begin{lemma}\label{lem:prelim:1/(1-x)}
    For any $0 \le x < 1$ we have $\tfrac1{1-x} \ge 1+x$.\\
    For any $0 \le x \le \tfrac12$ we have $\tfrac1{1-x} \le 1+2x$.
\end{lemma}
%Combining Lemma~\ref{lem:prelim:(1-1/n)^k}~with~\ref{lem:prelim:1/(1-x)} we obtain the following corollary.
%\begin{corollary}\label{cor:prelim:(1-1/n)^(-u)}
%    For any $n$ big enough and $u < n/2$ we have
%    \[
%        1+\tfrac{u}{n}-\tfrac{u^2}{2n^2}
%        \le \left(1-\tfrac1n\right)^{-u}
%        \le 1+\tfrac{u}{n}+\tfrac{2u^2}{n^2}.
%    \]
%\end{corollary}
Combining the three lemmas above we obtain the following corollary.
\begin{corollary}\label{cor:prelim:(1-1/n)^(n-u)}
    For any $u < n$ we have
    $\tfrac1e \le \left(1-\tfrac1n\right)^{n-u} \le \tfrac1e + \tfrac{2u}{en}$.
\end{corollary}

Again, we have the similar estimates for some $p > 0$.
\begin{corollary}\label{cor:prelim:(1-p/n)^(n-u)}
    For any $u < n/p$ we have
    $e^{-p} \le \left(1-\tfrac pn\right)^{n-u} \le e^{-p}\left(1 + 2p\tfrac{u}{n}\right)$.
\end{corollary}

%    Let us also recall some easy facts from analysis.
%    First 2 lemmas are close to Bernoulli's inequality.
%    \begin{lemma}\label{lemma:Bernoulli-1}
%        For any $x$, $n$ such that $xn < 1$,
%        \[
%            (1+x)^n \le 1+2xn
%        \]
%    \end{lemma}
%    \begin{proof}
%        It is equivalent to $(1+x)^n - 1 - xn \le xn$.
%        \begin{align*}
%            (1+x)^n - 1 - xn = \binom n2 x^2 + \ldots + \binom nn x^n = nx \cdot \frac{n-1}2 x + nx \cdot \frac{n-1}2 x \cdot \frac{n-2}3 \le \\
%            \le nx \left( \frac12 + \frac12 \cdot \frac13 + \frac12 \cdot \frac13 \cdot \frac14 \right) = nx(e-2) < nx
%        \end{align*}
%    \end{proof}
%
%    \begin{lemma}\label{lemma:Bernoulli-2}
%        If for any $i \in \{1, \ldots, n\}$, $x_i \ge 0$ and $\sum_i x_i < 1$, then
%        \[
%            \prod_{i=1}^n (1+x_i) \le 1 + 2 \sum_{i=1}^n x_i
%        \]
%    \end{lemma}
%    \begin{proof}
%        We will use the AM-GM inequality:
%        \[
%            \sqrt[n]{\prod_{i=1}^n (1+x_i)} \le \frac{\sum_{i=1}^n (1+x_i)}n = 1 + \frac1n \sum_{i=1}^n x_i
%        \]
%        Then, using previous Lemma~\ref{lemma:Bernoulli-1},
%        \[
%            \prod_{i=1}^n (1+x_i) \le \left( 1 + \frac1n \sum_{i=1}^n x_i \right)^n \le 1 + 2 \sum_{i=1}^n x_i.
%        \]
%    \end{proof}

%     Let us now bound a double exponential sum.
%     \begin{lemma}\label{lemma:double exponential}
%         Let $a > 1$ and $x > 0$.
%         Denote $c_n := \exp\left( -x \cdot a^n \right)$ for $n \ge 0$.
%         Then,
%         \[
%             \sum_{n\ge0} c_n \le \frac{c_0}{1-c_0^{a-1}}
%         \]
%     \end{lemma}
%     \begin{proof}
%         \begin{align*}
%             c_{n+1} &= \left( e^{-x} \right)^{a^{n+1}} = \left( e^{-x} \right)^{a^n + a^n(a-1)} = \\
%                     &= \left( e^{-x} \right)^{a^n} \cdot \left( e^{-x} \right)^{a^n \cdot (a-1)} = \\
%                     &= c_n \cdot c_n^{a-1} \le c_n \cdot c_0^{a-1} \le c_n \cdot q,
%         \end{align*}
%         where $q = c_0^{a-1}$.
%         As for each $n$, $c_{n+1} \le c_n \cdot q$, we have $c_n \le c_0 \cdot q^n$.
%         Thus the sum of $c_n$ is bounded by the some of geometrical series:
%         \[
%             \sum_{n\ge0} c_n \le c_0 \sum_{n\ge0} q^n = c_0 \cdot \frac1{1-q}= \frac{c_0}{1-c_0^{a-1}}
%         \]
%     \end{proof} 