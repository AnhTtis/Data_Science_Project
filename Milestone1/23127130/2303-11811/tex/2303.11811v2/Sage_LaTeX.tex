% sage_latex_guidelines.tex V1.20, 14 January 2017

\documentclass[Afour,sageh,times,doublespace]{sagej}

\usepackage{moreverb,url}

\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[acronym]{glossaries}
\usepackage{float}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}
\usepackage{breqn}
\usepackage[colorlinks,bookmarksopen,bookmarksnumbered,citecolor=red,urlcolor=red, hidelinks]{hyperref}
\usepackage{interval}
\usepackage{tikz}
\usetikzlibrary{patterns,shapes.arrows,calc,angles,quotes,babel,arrows,arrows.meta,shapes.geometric,svg.path}
\usepackage[capitalise]{cleveref}
\usepackage{xurl}
\usepackage{uri}
\usepackage{subfig}
\usepackage{scalerel}

\definecolor{orcidlogocol}{HTML}{A6CE39}
\tikzset{
  orcidlogo/.pic={
    \fill[orcidlogocol] svg{M256,128c0,70.7-57.3,128-128,128C57.3,256,0,198.7,0,128C0,57.3,57.3,0,128,0C198.7,0,256,57.3,256,128z};
    \fill[white] svg{M86.3,186.2H70.9V79.1h15.4v48.4V186.2z}
                 svg{M108.9,79.1h41.6c39.6,0,57,28.3,57,53.6c0,27.5-21.5,53.6-56.8,53.6h-41.8V79.1z M124.3,172.4h24.5c34.9,0,42.9-26.5,42.9-39.7c0-21.5-13.7-39.7-43.7-39.7h-23.7V172.4z}
                 svg{M88.7,56.8c0,5.5-4.5,10.1-10.1,10.1c-5.6,0-10.1-4.6-10.1-10.1c0-5.6,4.5-10.1,10.1-10.1C84.2,46.7,88.7,51.3,88.7,56.8z};
  }
}

\newcommand\orcidicon[1]{\href{https://orcid.org/#1}{\mbox{\scalerel*{
\begin{tikzpicture}[yscale=-1,transform shape]
\pic{orcidlogo};
\end{tikzpicture}
}{|}}}}

\definecolor{matplotlibBlue}{HTML}{1f77b4} % blue from matplotlib color cycle
\definecolor{matplotlibOrange}{HTML}{ff7f0e} % orange from matplotlib color cycle
\definecolor{matplotlibGreen}{HTML}{2ca02c} % green from matplotlib color cycle
\definecolor{matplotlibRed}{HTML}{d62728} % red from matplotlib color cycle
\definecolor{matplotlibPurple}{HTML}{9467bd} % purple from matplotlib color cycle
\definecolor{matplotlibBrown}{HTML}{8c564b} % brown from matplotlib color cycle
\definecolor{matplotlibPink}{HTML}{e377c2} % pink from matplotlib color cycle
\definecolor{matplotlibGrey}{HTML}{7f7f7f} % grey from matplotlib color cycle
\definecolor{matplotlibYellow}{HTML}{bcbd22} % yellow from matplotlib color cycle

\tikzstyle{kernel} = [rectangle, 
minimum width=3cm, 
minimum height=1cm, 
text centered, 
text width=3cm, 
draw=black]

\tikzstyle{syncSweep} = [rectangle, 
minimum width=3cm, 
minimum height=1cm, 
text centered, 
text width=3cm, 
draw=black, 
fill=matplotlibBlue!50]

\tikzstyle{syncKernel} = [rectangle, 
minimum width=3cm, 
minimum height=1cm, 
text centered, 
text width=3cm, 
draw=black, 
fill=matplotlibOrange!50]

\tikzstyle{hybridKernel} = [rectangle, 
minimum width=3cm, 
minimum height=1cm, 
text centered, 
text width=3cm, 
draw=black, 
fill=matplotlibGreen!50]

\tikzstyle{decision} = [diamond, 
minimum width=3cm, 
minimum height=1cm, 
text centered, 
draw=black]
\tikzstyle{arrow} = [thick,->,>=stealth]

\newcommand{\walberla}{\textsc{waLBerla}}
\newcommand{\change}[1]{{\color{black} {#1}}}

\newcommand\BibTeX{{\rmfamily B\kern-.05em \textsc{i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\def\volumeyear{2024}

\setcounter{secnumdepth}{3}

\begin{document}

\def\journalname{The International Journal of High Performance Computing Applications}

\runninghead{Kemmler et al.}

\title{Efficiency and scalability of fully-resolved fluid-particle simulations on heterogeneous CPU-GPU architectures}

\author{Samuel Kemmler\affilnum{1,2}\orcidicon{0000-0002-9631-7349}, Christoph Rettinger\affilnum{1}\orcidicon{0000-0002-0605-3731}, Ulrich Rüde\affilnum{1,3}\orcidicon{0000-0001-8796-8599},\newline Pablo Cuéllar\affilnum{2}\orcidicon{0000-0003-2446-8065} and Harald Köstler\affilnum{1}\orcidicon{0000-0002-6992-2690}}

\affiliation{\affilnum{1}Chair for System Simulation, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany\\\affilnum{2}Division 7.2 for Buildings and Structures, Federal Institute for Materials Research and Testing (BAM), Berlin, Germany\\\affilnum{3}CERFACS, Toulouse, France}

\corrauth{Samuel Kemmler, Chair for System Simulation, Friedrich-Alexander-Universität Erlangen-Nürnberg, Cauerstraße 11, 91058 Erlangen, Germany}

\email{samuel.kemmler@fau.de}

\newacronym{lbm}{LBM}{Lattice Boltzmann Method}
\newacronym{dem}{DEM}{Discrete Element Method}
\newacronym{cpu}{CPU}{Central Processing Unit}
\newacronym{gpu}{GPU}{Graphics Processing Unit}
\newacronym{psm}{PSM}{Partially Saturated Cells Method}
\newacronym{mem}{MEM}{Momentum Exchange Method}
\newacronym{pdfs}{PDFs}{Particle Distribution Functions}
\newacronym{cfd}{CFD}{Computational Fluid Dynamics}
\newacronym{simd}{SIMD}{Single Instruction, Multiple Data}
\newacronym{bc}{BC}{Boundary Condition}
\newacronym{srt}{SRT}{Single Relaxation Time}
\newacronym{pd}{PD}{Particle Dynamics}
\newacronym{sm}{SM}{Streaming Multiprocessor}

\begin{abstract}
Current supercomputers often have a heterogeneous architecture using both CPUs and GPUs. At the same time, numerical simulation tasks frequently involve multiphysics scenarios whose components run on different hardware due to multiple reasons, e.g., architectural requirements, pragmatism, etc. This leads naturally to a software design where different simulation modules are mapped to different subsystems of the heterogeneous architecture. We present a detailed performance analysis for such a hybrid four-way coupled simulation of a fully resolved particle-laden flow. The Eulerian representation of the flow utilizes GPUs, while the Lagrangian model for the particles runs on CPUs. First, a roofline model is employed to predict the node level performance and to show that the lattice-Boltzmann-based fluid simulation reaches very good performance on a single GPU. Furthermore, the GPU-GPU communication for a large-scale flow simulation results in only moderate slowdowns due to the efficiency of the CUDA-aware MPI communication, combined with communication hiding techniques. On 1024 A100 GPUs, a parallel efficiency of up to 71\% is achieved. While the flow simulation has good performance characteristics, the integration of the stiff Lagrangian particle system requires frequent CPU-CPU communications that can become a bottleneck. Additionally, special attention is paid to the CPU-GPU communication overhead since this is essential for coupling the particles to the flow simulation. However, thanks to our problem-aware co-partitioning, the CPU-GPU communication overhead is found to be negligible. As a lesson learned from this development, four criteria are postulated that a hybrid implementation must meet for the efficient use of heterogeneous supercomputers. Additionally, an a priori estimate of the speedup for hybrid implementations is suggested.
\end{abstract}

\keywords{Hybrid implementation, High-performance computing, Particulate flow, Lattice Boltzmann method, Discrete element method}
\glsresetall

\maketitle

\input{src/01-introduction}
\input{src/02-methods}
\input{src/03-implementation}
\input{src/04-performance}
\input{src/05-implications}
\input{src/06-conclusion}

\section*{Acknowledgements}
The authors gratefully acknowledge the Gauss Centre for Supercomputing e.V. (\url{www.gauss-centre.eu}) for funding this project by providing computing time on the GCS Supercomputer JUWELS at Jülich Supercomputing Centre (JSC).\newline
The authors gratefully acknowledge the scientific support and HPC resources provided by the Erlangen National High Performance Computing Center (NHR@FAU) of the Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU). The hardware is funded by the German Research Foundation (DFG).

\section*{Declaration of competing interest}
The Authors declare that there is no conflict of interest.

\section*{Funding}
The authors disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: The authors thank the Deutsche Forschungsgemeinschaft (DFG, German Research
Foundation) for funding the project 433735254. The DFG had no direct involvement in this paper; and this work has received funding from the European High Performance Computing Joint Undertaking (JU) and Sweden, Germany, Spain, Greece, and Denmark under grant agreement No 101093393.

\section*{Data availability}
Data is available on Zenodo: \doi{10.5281/zenodo.13951599}.

% chktex-file 8
\section*{Author ORCIDs}
S.\ Kemmler, \url{https://orcid.org/0000-0002-9631-7349};
C.\ Rettinger, \url{https://orcid.org/0000-0002-0605-3731};
U.\ Rüde, \url{https://orcid.org/0000-0001-8796-8599};
P.\ Cuéllar, \url{https://orcid.org/0000-0003-2446-8065};
H.\ Köstler, \url{https://orcid.org/0000-0002-6992-2690};

\section*{Author contributions}
S.\ Kemmler: Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data Curation, Writing - Original Draft, Visualization, Project administration;
C.\ Rettinger: Conceptualization, Writing - Review and Editing;
U.\ Rüde: Writing - Review and Editing;
P.\ Cuéllar: Writing - Review and Editing;
H.\ Köstler: Resources, Writing - Review and Editing, Supervision, Funding acquisition;

\bibliographystyle{SageH}
\bibliography{MyLibrary.bib}

\section*{Author biographies}
\textbf{Samuel Kemmler} is a Ph.D. student at the Chair for System Simulation at the Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU) and a research assistant at the Federal Institute for Materials Research and Testing (BAM) in Berlin. He holds a M.Sc. degree in Computational Engineering and is one of the core developers of the \walberla~HPC framework. His research interests are high-performance computing and particle-resolved sediment transport simulations.\newline

\textbf{Christoph Rettinger} studied Computational Engineering at the FAU. In 2023, he finished his Ph.D. on fully resolved simulation of particulate flows at the Chair for System Simulation.\newline

\textbf{Ulrich Rüde} heads the Chair for System Simulation at the FAU. He studied Mathematics and Computer Science at Technische Universität München (TUM) and the Florida State University. He holds a Ph.D. and Habilitation degrees from TUM. His research interest focuses on numerical simulation and high end computing, in particular computational fluid dynamics, multilevel methods, and software engineering for high performance computing. He is a Fellow of the Society of Industrial and Applied Mathematics.\newline

\textbf{Pablo Cuéllar} studied Civil Engineering at the Universidad Politécnica de Madrid and reiceived his Ph.D. in Civil Engineering from Technical University Berlin in 2011. He is a guest scientist at BAM.\newline

\textbf{Harald Köstler} got his Ph.D. in Computer Science in 2008 on variational models and parallel multigrid methods in medical image processing. 2014 he finished his habilitation on Efficient Numerical Algorithms and Software Engineering for High Performance Computing. Currently, he works at the Chair for System Simulation at the FAU. His research interests include software engineering concepts especially using code generation for simulation software on HPC clusters, multigrid methods, and programming techniques for parallel hardware, especially GPUs. The application areas are computational fluid dynamics, rigid body dynamics, and medical imaging.\newline

\end{document}
