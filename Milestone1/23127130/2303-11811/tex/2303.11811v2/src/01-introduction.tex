\section{Introduction}\label{introduction}
% Motivation: rise of heterogeneous supercomputers with CPU-GPU nodes, efficient usage is challenge for code, best solution can differ between methodologies
The Top500 list reports the most powerful supercomputers worldwide. The number of heterogeneous supercomputers, i.e., systems with additional accelerators such as \glspl{gpu}, in the Top500 list has steadily increased in the last years, accounting for roughly 39\% in June 2024\footnote{\url{www.top500.org/statistics/list/}, Top500 Release: June 2024, category: Accelerator/Co-Processor, accessed on: 18th of October 2024}. Each node of such a heterogeneous supercomputer typically consists of one or many \glspl{cpu} and \glspl{gpu}~\citep[][]{kimSnuCLOpenCLFramework2012}.
Numerical multiphysics simulations are a powerful technique for conducting in-depth investigations of complex physical phenomena by providing detailed data, which is challenging, if not impossible, to collect in experiments.
A significant challenge with these simulations is that they are computationally costly, which is why they are often run on supercomputers.
Especially supercomputers containing \glspl{gpu} have become increasingly popular for numerical simulations in recent years~\citep[][]{shimokawabeStencilFrameworkRealize2017, rohrEnergyEfficientMultiGPUSupercomputer2014, oyarzunPortableImplementationModel2017} as they offer unprecedented computing power.
A common approach in the literature for utilizing such heterogeneous supercomputers for multiphysics simulation is hybrid implementation~\citep[][]{kotsalosDigitalBloodMassively2021, feichtingerDesignPerformanceEvaluation2012, xuDiscreteParticleSimulation2012}, i.e., different simulation modules running on different hardware. Several reasons are predestinating or sometimes even forcing hybrid implementations in the context of multiphysics simulations.
First, the various combined methodologies in such multiphysics simulations may exhibit distinctly contrasting computational properties, e.g., problem sizes, parallel and sequential portions, conditionals, and branching. Therefore, the best-suited hardware architecture can differ between the simulation modules.
Second, if one simulation module dominates the overall run time of the simulation, accelerating only this part using \glspl{gpu} is a straightforward, pragmatic, and development time-saving alternative to porting the whole code base to \gls{gpu}. 
Third, practical limitations can challenge multiphysics simulations: not all coupled software frameworks and modules may support the same hardware, e.g. if parts of the simulation use commercial software.
While hybrid implementations are commonly used in practice for the aforementioned reasons, they introduce inherent challenges in performance and scalability.\newline
% Motivation: fully resolved fluid-particle simulations with challenges 1, 2, 3
A prominent example of multiphysics simulations is coupled fluid-particle simulations with fully resolved particles~\citep[i.e., multiple fluid cells per particle diameter, see][]{rettingerFullyResolvedSimulation2023a}.
Such simulations have been used in the literature, among others, to understand the formation and dynamics of dunes in river beds~\citep[][]{rettingerFullyResolvedSimulations2017, schwarzmeierParticleresolvedSimulationAntidunes2023a, kidanemariamDirectNumericalSimulation2014}, analyze the chimney fluidization in a granular medium~\citep[][]{ngomaTwodimensionalNumericalSimulation2018}, investigate the erosion kinetics of soil under an impinging jet~\citep[][]{benseghierParallelGPUbasedComputational2020} and to analyze mobile sediment beds~\citep[][]{vowinckelFluidParticleInteraction2014, rettingerRheologyMobileSediment2022}.
One encounters some of the previously mentioned reasons for hybrid implementations in the context of coupled fluid-particle simulations with fully resolved particles.
First, when coupling a particle simulation with a fluid simulation using a fully resolved approach, the number of particles is typically at least three orders of magnitude smaller than the number of fluid cells, leading to an imbalance in the workload, where the fluid simulation can overwhelmingly dominate the total run time of the simulation. Therefore, accelerating only the fluid simulation using \glspl{gpu} is a pragmatic approach for this application.
Second, while the fluid simulation employed here uses a structured, cartesian-grid-based methodology and is therefore ideally suited for \gls{gpu} parallelization~\citep[][]{kuznikLBMBasedFlow2010,holzerHighlyEfficientLattice2021}, the situation is more complex for the particle simulation. One extreme is molecular dynamics simulations using millions of particles but relatively simple and uniform particle-particle interactions, which are well suited for \glspl{gpu}~\citep[][]{machadoTinyMDMappingMolecular2021}. The other extreme is particle simulations consisting of few but large particles with complex shapes and sophisticated and diverse particle-particle interactions~\citep[][]{iglbergerMassivelyParallelGranular2010}, assumably better suited for \glspl{cpu}. The particle methodology of this paper lies somewhere in between. While spherical particles are used, the number of particles is comparably small (due to being fully resolved), and the particle-particle interactions are more sophisticated (e.g., lubrication corrections) than the typical molecular dynamics or \gls{dem} simulation on \glspl{gpu} in the literature.
This effect will increase when introducing more complex particle shapes or additional particle-particle interactions (such as cohesion) in the future.\newline
% Related work: there are already approaches a, b, c in the literature
Numerous fluid-particle simulations in the literature use hybrid parallelization in one way or another, of which we present a representative selection in the following.
One example is simulating deformable bodies (using the finite element method) in a fluid, i.e., blood cells in cellular blood flow~\citep[][]{kotsalosDigitalBloodMassively2021}. Here, the focus is more on structural mechanics, as the finite element method dominates the overall run time and is, therefore, the accelerated part of the code. Hybrid approaches have also been used in the literature to couple commercial \gls{cfd} solvers on the \gls{cpu} such as Ansys~\citep[][]{heCPUGPUCrossplatformCoupled2020, sousaniAcceleratedHeatTransfer2019}, or AVL fire~\citep[][]{jajcevicLargescaleCFDSimulations2013} with the \gls{dem} on the \gls{gpu}. Furthermore, simulations have been accelerated using hybrid parallelization to gain a deeper understanding of fluidized beds~\citep[][]{xuDiscreteParticleSimulation2012, norouziNewHybridCPUGPU2017}, or the effects of porosity and tortuosity on soil permeability~\citep[][]{sheikhNumericalInvestigationEffects2015}.
Another variant is to use the \gls{gpu} for both the fluid and the particles, but the \gls{cpu} supports the particle dynamics through collision detection~\citep[][]{juniorFluidSimulationTwoWay2010}.
Furthermore, we point the reader's attention to a related work utilizing a heterogeneous-hybrid approach, i.e., \glspl{cpu} for the particles, while the fluid simulation is distributed on both the \glspl{cpu} and \glspl{gpu}~\citep[][]{feichtingerDesignPerformanceEvaluation2012}.\newline
We chose the \gls{psm} for the coupling between the fluid and solid phase as it has been used successfully in the context of \glspl{gpu}~\citep[][]{benseghierParallelGPUbasedComputational2020, fukumoto2DCoupledFluidparticle2021} due to its \gls{simd} nature.\newline
% Contributions
In this work, we postulate several performance criteria that a hybrid implementation has to meet in our opinion for an efficient use of a heterogeneous supercomputer without misusing resources. First, the overhead introduced by the hybrid implementation (i.e., the \gls{cpu}-\gls{gpu} communication) must be negligible compared to the overall run time. Second, the performance-critical part (in our case the fluid simulation) should show good performance on the \gls{gpu}. Third, the share of the \gls{gpu} run time of the total run time must be sufficiently high to justify using a heterogeneous cluster. Fourth, it has to show a satisfactory weak scaling performance for the efficient use of multiple supercomputer nodes. One of the main contributions is introducing a multi-\gls{cpu}-multi-\gls{gpu} implementation for fully resolved fluid-particle simulations, fulfilling the four criteria stated above.
To evaluate the presented implementation, an in-depth performance analysis on a state-of-the-art heterogeneous supercomputer is provided, comparing two contrasting cases of a fluidized bed simulation, namely in dense and dilute states of the solid granular phase. The paper focuses on the experience with the performance and scalability of the presented hybrid implementation on supercomputers rather than on the exposition of physical results.
Coupled fluid-particle implementations in the literature using a comparable particle methodology still are often limited to \glspl{cpu}~\citep[][]{rettingerFullyResolvedSimulations2017, kidanemariamDirectNumericalSimulation2014, vowinckelFluidParticleInteraction2014}.
With this work, we report and study the experience with the performance and scalability of the presented hybrid implementation on a supercomputer to allow other researchers to make an informed decision if such a `partial' acceleration might be a suitable approach for their codes.\newline
% Overview
The paper starts in \cref{numerical_methods} with an introduction to the numerical methods, i.e., the \gls{lbm}, the \gls{dem}, and the corresponding coupling. This is followed in \cref{implementation} with a description of the implementation.
\cref{performance} provides a detailed performance analysis. This study includes the \gls{cpu}-\gls{gpu} communication overhead, the run times of the different simulation modules, a roofline model, weak scaling, \change{strong scaling}, and an a priori estimation of the hybrid speedup. \cref{implications} elaborates on the lessons learned and the applicability of the results beyond the present study.
The paper concludes with \cref{conclusion}.
