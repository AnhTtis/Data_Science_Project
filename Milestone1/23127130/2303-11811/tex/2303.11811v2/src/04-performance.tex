\section{Performance analysis}\label{performance}
%TODO: maybe add warmup steps
We use the Juwels Booster cluster for the performance evaluation. Each GPGPU node consists of four Nvidia A100 40 GB \glspl{gpu}, two AMD EPYC 7402 \glspl{cpu} (24 cores per chip), and eight NUMA domains. Thus, each \gls{cpu} is shared by two \glspl{gpu} and is divided into four NUMA domains, with six cores belonging to one NUMA domain. Four out of the eight NUMA domains have independent PCIe lanes to the four \glspl{gpu} on Juwels Booster, ensuring that \gls{cpu}-\gls{gpu} communications do not interfere between the \glspl{gpu}. The following will refer to a \gls{gpu} and an associated (i.e., directly connected via a PCIe lane) NUMA domain with its six cores as a \gls{cpu}-\gls{gpu} pair. All \glspl{gpu} within a node are connected via NVLinks, allowing direct \gls{gpu}-\gls{gpu} communication. For communication between \glspl{gpu} not sharing a node, PCIe lanes connect each \gls{gpu} to its own Mellanox HDR200 InfiniBand ConnectX 6 adapter. A SCALE kernel (i.e., a 1:1 read/write ratio) yields a memory bandwidth of about 1400 GB/s for the A100 40 GB~\citep[][]{ernstAnalyticalPerformanceEstimation2023}. % https://github.com/te42kyfo/gpu-benches/blob/master/gpu-stream/a100_40.txt
We use 20 cells per diameter to geometrically resolve the particles~\citep[][]{rettingerRheologyMobileSediment2022, rettingerEfficientFourwayCoupled2022, biegertCollisionModelGrainresolving2017, costaCollisionModelFully2015}.
The upcoming sections first introduce the computational properties of the simulated cases, followed by their performance results.

\subsection{Simulation setups}\label{setups}
In order to study the performance of the here introduced hybrid coupled fluid-particle implementation, we are using a fluidized bed simulation. We compare two cases: the dilute case and the dense case. They exhibit different characteristics regarding the number of particles per volume and the number of particle-particle interactions. We choose these two cases to investigate how different particle workloads on the \glspl{cpu} influence the overall performance of the hybrid \gls{cpu}-\gls{gpu} implementation.
We use ten particle sub-cycles (\cref{implementation}) per time step.
We discretize the domain using $500 \times 200 \times 800 = 80 \times 10^6$ fluid cells. The velocity (inflow) \gls{bc} on the bottom and pressure (outflow) \gls{bc} on top of the domain govern the fluid dynamics. The remaining four boundaries are no-slip conditions ensuring a zero fluid velocity, i.e., the domain is not periodic. The particle Reynolds number is $1.0$, the Galileo number is around $8.9$, the gravitational acceleration is $9.81\ \text{m}/\text{s}^2$, and the particle fluid density ratio is $1.1$.
Planes surrounding the domain prevent the particles from leaving the domain by acting as walls that form a box.
The dilute case contains 627 particles. \cref{fig:fluidized_bed_dilute} illustrates the dilute setup.
Due to the low particle concentration, the effort for computing the particle-particle interactions (collisions and lubrication corrections) is low in the dilute case. 
The dense case is generally the same setup as the dilute case, except that the particle concentration is significantly higher limiting the fluidization (\cref{fig:fluidized_bed_dense}), resulting in 8073 particles, almost 13 times more than in the dilute case.
\begin{figure}
  \centering
  \subfloat[\centering Dilute case]{\includegraphics[scale=0.5, height=10cm]{figures/dilute_case.png}\label{fig:fluidized_bed_dilute}}
  \qquad
  \subfloat[\centering Dense case]{\includegraphics[scale=0.5, height=10cm]{figures/dense_case.png}\label{fig:fluidized_bed_dense}}
  \caption{Visualization of the consolidated fluidized bed setup running on one \gls{cpu}-\gls{gpu} pair. For the fluid field, only a two-dimensional slice is visualized.}
\end{figure}

\subsection{Performance results}
In this section, we present and analyze the performance results. We first look at the individual run times of the different simulation modules to understand the bottlenecks.
Furthermore, we present a weak scaling benchmark for both cases up to 1024 \gls{cpu}-\gls{gpu} pairs.
\change{In addition, we show strong scaling results for three different problem sizes up to 1024 \gls{cpu}-\gls{gpu} pairs.}
Finally, we demonstrate the acceleration potential of hybrid implementations by comparing it to a large-scale \gls{cpu}-only simulation from the literature.
For all results, we average over 500 time steps.
In the following, we will refer to the performance criteria formulated in \cref{introduction}.

%\subsubsection{Particle scaling on the CPU}
% Description
%Using OpenMP with static scheduling, we parallelize the \gls{pd} simulation on the \gls{cpu}.\@ Per-particle computations (e.g., adding gravitational forces) are parallelized among the particles, and particle-particle interactions are parallelized among the linked cells in one dimension. We use multiple cores of a single \gls{cpu} and measure the run time of the \gls{pd} simulation. The parallel efficiency $E_n$ is defined based on the serial run time $t_{\text{s}}$ and the run time using $n$ cores in parallel $t_{\text{p}}(n)$:

%\begin{equation}
%  E_n=\frac{t_s}{t_{\text{p}}(n) \cdot n}.
%\end{equation}

%\noindent
%For a perfectly linear scaling code, i.e., $t_{\text{p}}(n)=t_s/n$, the maximum achievable parallel efficiency $E_{\max}$ is 1 $\forall n$. For a non-scaling code, i.e., $t_{\text{p}}(n)=t_{\text{s}}\ \forall n$, the minimum achievable parallel efficiency $E_{\min}$ is $1/n$.\ \cref{fig:particle_scaling} reports the parallel efficiency of the \gls{pd} simulation for both cases for up to six cores, i.e., the size of a NUMA domain. We do not use more cores than available in a NUMA domain since the \gls{pd} implementation is not NUMA-aware. % ist das so und warum wÃ¤re es ein Problem das NUMA-aware zu implementieren?

%\begin{figure}
%  \centering
%  \input{plots/particle_scaling.tex}
%  \caption{Parallel efficiency inside a NUMA domain for both cases. The perfect scaling (Best) and non-scaling (Worst) form the upper and lower bound. }\label{fig:particle_scaling}
%\end{figure}

%\noindent
% Observations
%We observe a similar pattern regarding the parallel efficiency in both the dilute and the dense case. Both show a decrease down to around 45\% for six cores.\newline
% Interpretation
%A parallel efficiency of about 45\% when using a comparatively small number of six cores is a poor scaling result. We expect this performance for several reasons. First, the \gls{pd} simulation consists of ten subsequent sub-cycles per time step, each containing several consecutive tasks (\cref{fig:implementation}). Each task consists of an OpenMP parallel loop with an implicit OpenMP barrier at the end, causing idle threads to wait for all threads to end the loop. Second, modifying the force and torque of a particle must be enclosed with an atomic region to avoid race conditions, which harms the parallel efficiency, but is a central task and thus required in several \gls{pd} routines. Third, the workload (for the \gls{dem} and lubrication routine) per particle can differ significantly depending on the particle's location inside the domain. The latter aspects hold for the dense and the dilute case, explaining the poor parallel efficiency in both cases.
%The poor parallel efficiency of the particle methodology supports our initial assumption that the particle simulation part would not benefit from a \gls{gpu} parallelization in the context of geometrically resolved coupled simulations.

\subsubsection{Run times of different simulation modules}
% Description / Reference
We investigate the run times of the different simulation modules to analyze the overhead introduced by the hybrid implementation, i.e., the \gls{cpu}-\gls{gpu} communication, assess the \gls{gpu} performance, and detect the overall bottlenecks. To evaluate the performance of the \gls{psm} kernel on the \gls{gpu}, we employ a roofline model~\citep[][]{hagerIntroductionHighPerformance2010} for the \gls{lbm} kernel.
We determine the maximal possible performance for the given kernel and hardware when exploiting the maximal memory bandwidth (i.e., the performance `lightspeed' estimation). The \gls{psm} kernel comprises the \gls{lbm} kernel plus additional memory transfers depending on the number of overlapping particles. As this number differs from cell to cell, the \gls{psm} roofline model would not be straightforward. Therefore, we analyze the \gls{lbm} model only, keeping in mind that this is a too optimistic performance estimation.
Since we use the D3Q19 lattice model, we read and write 19 \gls{pdfs} per cell and time step.
This results in 19 reads and 19 writes (double-precision), i.e., 304 bytes to update one lattice cell~\citep[][]{feichtingerPerformanceModelingAnalysis2015}. The domain consists of 8e7 fluid cells. This results in the following minimal run time per time step according to the roofline model:
\begin{equation}
  \begin{aligned}
    T_\min &= \frac{304 \ \text{B/cell} \cdot \text{8e7} \ \text{cells/time step}}{1400 \ \text{GB/s}} \\
           &= 17.4 \ \text{ms/time step}.
  \end{aligned}
\end{equation}
We divide the total run time into the following modules: the \gls{psm} kernel (PSM), the \gls{cpu}-\gls{gpu} communication (comm), the particle mapping (mapping), setting the particle velocities (setU), reducing the hydrodynamic forces $\boldsymbol{F}_{\text{p},i}^{\text{fp}}$ and torques $\boldsymbol{T}_{\text{p},i}^{\text{fp}}$ on the particles (redF), computing the \gls{pd} and finally the remaining tasks (other), e.g., the \gls{lbm} boundary handling.\ \cref{fig:runtimesa100} reports the run times per time step for these modules using a single \gls{cpu}-\gls{gpu} pair. Furthermore, a dashed horizontal line indicates the run time of the \gls{psm} kernel for a simulation without any particles in the domain. While this physically behaves precisely like an \gls{lbm} kernel, it allows the quantification of the overhead due to the different code structure of the \gls{psm} compared to the \gls{lbm} kernel without the additional effort due to particles inside the domain.
\begin{figure*}
  \centering
  \input{plots/runtimes_modules.tex}
  \caption{Individual run times of the different simulation modules on a \gls{cpu}-\gls{gpu} pair}\label{fig:runtimesa100}
\end{figure*}
% Observations
In the dilute case, the \gls{psm} kernel needs about 42\% more time per time step than the \gls{lbm} lightspeed estimation, and the dense case 83\% more time. Compared to the \gls{psm} kernel without particles, the dilute case needs about 4\% more time per time step, and the dense case 34\% more time. The \gls{cpu}-\gls{gpu} communication is negligible for both cases. All modules take longer in the dense case than in the dilute case. While in the dilute case, the \gls{psm} kernel accounts for the majority of the run time, the \gls{pd} simulation needs more time than the \gls{psm} kernel in the dense case. Still, most of the run time is spent on \gls{gpu} routines in the dense case.\newline
% Interpretation
The overhead introduced by the hybrid implementation is negligible because we only transfer a small amount of double-precision values per particle but no fluid cells, and thanks to the problem-aware co-partitioning. The overhead, therefore, shows that a hybrid parallelization with the presented technique is a viable approach, and the first criterion is met. The performance of the \gls{psm} kernel is close to utilizing the total memory bandwidth of the A100, especially considering that the given roofline model does not take the memory traffic due to the solid part of the \gls{psm} kernel into account. Therefore, the second criterion is met. There is a significant performance gain for the \gls{psm} kernel on the \gls{gpu} compared to a \gls{cpu}-only implementation since the \gls{psm} kernel is utilizing almost the entire memory bandwidth, which is significantly lower on \glspl{cpu}.
Even in the dense case, the \gls{gpu} run time accounts for most of the total time because the fluid and coupling workload is much higher than the particle workload, even though we use ten particle sub-cycles. Therefore, the third criterion is met.
The \gls{psm} kernel exhibits two remarkable effects that we analyze in detail below using NVIDIA Nsight Compute\footnote{\url{https://developer.nvidia.com/nsight-compute}}.\newline
First, the significant difference between the \gls{lbm} main memory roofline (horizontal line) and the \gls{psm} kernel without particles (dashed horizontal line) in \cref{fig:runtimesa100} indicates that the \gls{psm} kernel cannot fully utilize the main memory bandwidth as the memory transfers of the \gls{psm} kernel in the absence of particles are very similar to the ones assumed in the \gls{lbm} main memory roofline. This is noticeable because the \gls{psm} is an extension of the \gls{lbm}, and it is known that the \gls{lbm} can nearly fully utilize the main memory bandwidth on the A100 architecture~\citep[][]{lehmannAccuracyPerformanceLattice2022, holzerDevelopmentCentralmomentPhasefield2024a}.
The \gls{psm} kernel uses 196 registers per thread. This large number of registers per thread heavily limits the number of warps per \gls{sm}, leading to a maximum possible occupancy of the \gls{sm}s of only 12.50\%. This low occupancy is insufficient to issue enough load/store instructions to fully exploit the main memory bandwidth, resulting in a difference between the \gls{psm} without particles and the \gls{lbm} memory roofline.\newline
% Run times: 23.85 ms (empty), 24.78 ms (dilute), 31.90 ms (dense)
% Main memory transfers: 13.59 + 12.01 (empty), 13.76 + 12.09 (dilute), 15.60 + 13.11 (dense)
% Branch instructions: 10367616 (empty), 10581374 (dilute), 12252098 (dense) => 2.06% increase in dilute case, 18.18% in dense case
% Excessive sectors due to uncoalesced global accesses: 185823270 (18% of the total sectors) (dilute), 331668418 (21% of the total sectors) (dense) => 78.49% increase
% Numbers of instructions issued: 1.62e9 (empty), 1.69e9 (dilute), 2.26e9 (dense) => 4.32\% and 39.51\% more instructions issued (with a small number of warps per thread) => similar to run time increase => memory traffic increases less => less main memory utilization
% Main memory utilization: 69.11% (empty), 65.85% (dilute), 52.56% (dense)
\change{Second, while the \gls{psm} kernel is in the dilute case only slightly slower than without particles, this difference gets more significant in the dense case.} Several effects differentiate the dilute and dense cases.
First, more particles increase the unfavorable \gls{gpu} workload, i.e., warp divergence and coalesced access. The branch instructions increase by 2.06\% in the dilute case and by 18.18\% in the dense case compared to the \gls{psm} kernel without particles. The number of excessive sectors due to uncoalesced global accesses increases by 78.49\% from the dilute case to the dense case resulting in 21\% of the total sectors in the dense case.
Besides that, the number of instructions issued increases by 4.32\% in the dilute case and 39.51\% in the dense case, which is in a similar range than the run time increase, corresponding to the instruction boundness of the code mentioned in the first effect (i.e., the low occupancy of the \gls{sm}s limits the issued instructions).
Since the increase in main memory traffic is 0.98\% (dilute case) and 12.15\% (dense case) and therefore smaller than the run time increase, the main memory utilization drops from 65.85\% (dilute case) to 52.56\% (dense case).

\subsubsection{Weak scaling}\label{weak_scaling}
% Description
When increasing the simulation domain further to simulate physically relevant scenarios, using a single \gls{cpu}-\gls{gpu} pair is often insufficient. Instead, multiple pairs or even multiple nodes of a supercomputer must be used. Therefore, a satisfactory weak scaling is desirable. For weak scaling, the problem size is increased with an increasing number of \gls{cpu}-\gls{gpu} pairs, keeping the workload per \gls{cpu}-\gls{gpu} pair constant. When having a perfect weak scaling, the performance per \gls{cpu}-\gls{gpu} pair stays constant, independent of the number of \gls{cpu}-\gls{gpu} pairs used. In the context of \gls{lbm}, MLUPs is a standard performance metric for weak scaling~\citep[][]{holzerHighlyEfficientLattice2021}, meaning how many lattice cell updates the hardware performs per second. 
We use the total run time for computing the MLUPs, containing both the \gls{cpu} (particles) and the \gls{gpu} (fluid, coupling) time. For the weak-scaling plots, we have conducted at least three benchmarking runs and will use the best sample in the following.
We start with a single \gls{cpu}-\gls{gpu} pair and a single domain block as described in \cref{setups}. We then double the number of \gls{cpu}-\gls{gpu} pairs succesively until we reach 1024. At the same time, we double the domain blocks/size alternately in each direction: $2\times 1 \times 1$ blocks (two \glspl{gpu}), $2\times 2 \times 1$ blocks (four \glspl{gpu}), $2\times 2 \times 2$ (eight \glspl{gpu}), etc.\ \cref{fig:weak_scaling} and \cref{fig:weak_scaling_efficiency} report the weak scaling for both cases. \change{Dashed lines indicate the ideal curves.} To our best knowledge, this is the most extensive weak scaling of a hybrid fluid-particle implementation presented in the literature.
\begin{figure*}
  \centering
  \input{plots/weak_scaling.tex}
  \caption{Weak scaling performance for both cases up to 1024 \gls{cpu}-\gls{gpu} pairs}\label{fig:weak_scaling}
  \vspace{0.5cm}
\end{figure*}
\begin{figure*}
  \centering
  \input{plots/weak_scaling_efficiency.tex}
  \caption{Weak scaling parallel efficiency for both cases up to 1024 \gls{cpu}-\gls{gpu} pairs}\label{fig:weak_scaling_efficiency}
\end{figure*}
% Observations
We observe a roughly three times higher performance for the dilute case than for the dense case. Both cases show a parallel efficiency decrease particularly strong in the beginning. The parallel efficiency is 71\% in the dilute case and 53\% in the dense case when using 1024 \gls{cpu}-\gls{gpu} pairs, which corresponds to a domain size of $8000 \times 1600 \times 6400 = 8.192 \times 10^{10}$ fluid cells. A similar scaling behavior has been observed in the literature both for other hybrid~\citep[][]{ kotsalosDigitalBloodMassively2021} and \gls{cpu}-only fluid-particle implementations~\citep[][]{rettingerFullyResolvedSimulations2017}.\newline
% Description
Interpreting the overall weak scaling behavior requires more detailed analysis of the scaling of the different simulation modules. When using a single \gls{cpu}-\gls{gpu} pair, the dominating routines are the \gls{pd}, the \gls{psm} kernel, and the coupling (i.e., particle mapping, setting the particle velocities and reducing the hydrodynamic forces $\boldsymbol{F}_{\text{p},i}^{\text{fp}}$ and torques $\boldsymbol{T}_{\text{p},i}^{\text{fp}}$
on the particles). Additionally, we must now consider the communication (comm) overhead that arises from using multiple \gls{cpu}-\gls{gpu} pairs. On the one hand, this is the \gls{pd} communication (\gls{cpu}-\gls{cpu} communication), on the other hand, the PSM communication (\gls{gpu}-\gls{gpu} communication). \cref{fig:weak_scaling_routines_dilute} and \cref{fig:weak_scaling_routines_dense} show the weak scaling behavior of the dominating simulation modules for both cases. The communication numbers cover the communication steps themselves, but also load imbalances between two communication steps.
\begin{figure*}
  \centering
  \input{plots/weak_scaling_modules_dilute.tex}
  \caption{Weak scaling performance of the dominating modules for the dilute case up to 1024 \gls{cpu}-\gls{gpu} pairs}\label{fig:weak_scaling_routines_dilute}
  \vspace{0.5cm}
\end{figure*}
\begin{figure*}
  \centering
  \input{plots/weak_scaling_modules_dense.tex}
  \caption{Weak scaling performance of the dominating modules for the dense case up to 1024 \gls{cpu}-\gls{gpu} pairs}\label{fig:weak_scaling_routines_dense}
\end{figure*}
% Observations
The different modules show similar qualitative scaling behavior when comparing the two cases. The \gls{psm} kernel scales quite well in both cases. The corresponding \gls{gpu}-\gls{gpu} communication (\gls{psm} comm) is negligible. The \gls{pd} run time increases initially and then shows saturation. The \gls{cpu}-\gls{cpu} communication (\gls{pd} comm) increases drastically, surpassing the run time of the \gls{psm} kernel and the coupling in the dense case. The \gls{pd} and the corresponding communication are more relevant for the overall scaling in the dense case than in the dilute case. The coupling scales similarly to the \gls{pd}.\newline
% Interpretation
The \gls{psm} workload per \gls{gpu} stays constant in the weak scaling explaining the nearly perfect scaling.
Since we are hiding the \gls{psm} communication (\cref{implementation}), we expect it to be negligible.
We expect the \gls{pd} and the coupling run time to increase initially because the number of neighboring blocks increases. More neighboring blocks lead to more ghost particles per block, resulting in a higher workload. This effect decreases when blocks have neighbors in all directions, resulting in an almost linear scaling from this point on. This phenomenon has been reported in the literature~\citep[][]{rettingerFullyResolvedSimulations2017}.
The methodology requires ten particle sub-cycles per time step and three communication steps per sub-cycle for a physically accurate simulation. Additionally, the simulation requires two \gls{cpu}-\gls{cpu} communication steps per time step apart from the sub-cycles (\cref{fig:implementation}). We have 32 \gls{cpu}-\gls{cpu} communication steps per time step, which cannot be hidden behind other routines. This becomes the dominating factor for the decrease of the overall weak scaling performance in both cases.\newline
% Description
To gain more profound insights into the reasons for this significant increase of the \gls{pd} comm time in the weak scaling and to estimate the scaling behavior beyond 1024 \gls{cpu}-\gls{gpu} pairs, we investigate the data transfers of the \gls{pd} comm in more detail for the dense case. The \gls{pd} comm consists of two parts: the synchronization of particle quantities between processes and the reduction of particle quantities (\cref{particle_dynamics}). In the following, we focus on the dominating part, the synchronization. \cref{fig:weak_scaling_communication_dense} plots the maximum and average amount of data a process either sends to or receives from its neighboring processes per synchronization call. The two arrows indicate the steps in which the weak scaling doubles the domain in the y-direction (the out-of-plane direction normal to the cross-section in \cref{fig:fluidized_bed_dilute}) for the first two times. Furthermore, the figure illustrates the maximum number of synchronization (i.e., communication) partners per process.
\begin{figure*}
  \centering
  \input{plots/weak_scaling_communication_dense.tex}
  \caption{Weak scaling of the particle synchronization in the dense case up to 1024 \gls{cpu}-\gls{gpu} pairs. Both the maximum and average amount of data (in bytes) a process either sends to or receives from its neighboring processes per synchronization call are illustrated (left y-axis), and the maximum number of synchronization partners (right y-axis).}\label{fig:weak_scaling_communication_dense}
\end{figure*}
% Observations
The maximum number of communication partners increases initially, reaching 26 starting from $2^6$ processes and then staying constant.
The maximum amount of data a process sends to or receives from its neighboring processes increases to $2^5$ processes and then saturates.
The maximum data volume per process already saturates one process earlier than the maximum number of communication partners.
The average data volume per process increases until $2^{10}$ processes. However, the increase from $2^0$ to $2^5$ processes is larger than from $2^5$ to $2^{10}$.
For both the maximum and average data curves, the most significant spikes can be observed when going from $2^1$ to $2^2$ and from $2^4$ to $2^5$ processes, i.e., when the weak scaling doubles the domain size in the y-direction for the first two times.
The maximum and average communication data show correlation with each other, as well as with the \gls{pd} comm time in \cref{fig:weak_scaling_routines_dense}.\newline
% Interpretation
% Domain decomposition:
% 2^0=1:     1x1x1
% 2^1=2:     2x1x1
% 2^2=4:     2x2x1
% 2^3=8:     2x2x2
% 2^4=16:    4x2x2
% 2^5=32:    4x4x2
% 2^6=64:    4x4x4
% 2^7=128:   8x4x4
% 2^8=256:   8x8x4
% 2^9=512:   8x8x8
% 2^10=1024: 16x8x8
The maximum number of communication partners is bounded from above at 26 because this is the maximum number of neighboring blocks in 3D (including blocks that only share a corner) because a $3\times 3 \times 3$ setup of blocks consists of the center block and 26 neighboring blocks. The weak scaling obtains this situation for the first time for $2^6$ processes since this corresponds to $4\times 4\times 4$ blocks, and therefore, at least one block is entirely surrounded by neighboring blocks.
The initial increase in maximum data transfer in the weak scaling is expected since the maximum number of neighbors increases. Therefore, more data has to be communicated per synchronization. The large spikes correspond to the cases where the domain is increased in the y-direction, which causes the most significant increase of the boundary layer between two blocks and, therefore, the most significant communication data increase. The step from $2^4$ to $2^5$ increases the maximum number of communication partners stronger than the step from $2^1$ to $2^2$. Therefore, the maximum data volume increases stronger accordingly (see the two arrows in \cref{fig:weak_scaling_communication_dense}).
As expected for nearest-neighbor communication, the maximum communication saturates if other processes entirely surround at least one process.
However, the maximum data volume per process already saturates one process earlier than the maximum number of communication partners is reached. This is because from $2^5$ to $2^6$ processes, the domain is increased in z-direction for the second time. However, due to the arrangement of the particles, synchronization only happens in positive z-direction. Therefore, doubling the domain in z-direction requires no additional communication since still, no process has to communicate in both z-directions.
The average data size still increases after the saturation of the maximum data because the ratio of boundary blocks (with less than 26 communication partners) compared to the total number of blocks decreases, i.e., proportionally, more blocks are becoming blocks with the maximum data size.
The maximum communication serves as an upper bound for the convergence of the average communication data.
Due to the pure nearest neighbor communication and the resulting convergence of the average data size per process, we expect promising scalability on larger systems beyond 1024 \gls{cpu}-\gls{gpu} pairs.
However, the scaling of the \gls{pd} comm depends strongly on the \gls{pd} methodology, its implementation, the available hardware, and the assignment of the blocks to the hardware.
Reducing the collision history is part of \gls{pd} comm, which includes swapping old and new contact information. Since this swap is also necessary without using multiple \gls{cpu}-\gls{gpu} pairs, \gls{pd} comm is bigger zero even when using a single \gls{cpu}-\gls{gpu} pair.
Overall, we observe a weak scaling performance that justifies using multiple supercomputer nodes. Therefore, the fourth criterion is met.

\subsubsection{Strong scaling}
\change{
For strong scaling, the problem size is fixed with an increasing number of \gls{cpu}-\gls{gpu} pairs, decreasing the workload per \gls{cpu}-\gls{gpu} pair. When having a perfect strong scaling, the run time decreases to the same extent as the number of \gls{cpu}-\gls{gpu} pairs increases.
We present strong scaling results for three problem sizes: small, medium, and large.
The small problem consists of $500 \times 200 \times 800$ fluid cells, as described in \cref{setups}, which fits on a single \gls{cpu}-\gls{gpu} pair. For the medium problem, the small problem size is doubled in all three directions, resulting in $1000 \times 400 \times 1600$ fluid cells, which fits on eight \gls{cpu}-\gls{gpu} pairs. For the large problem, the medium problem size is again doubled in all three directions, resulting in $2000 \times 800 \times 3200$ fluid cells, which fits on 64 \gls{cpu}-\gls{gpu} pairs. For each problem size, we successively double the number of \gls{cpu}-\gls{gpu} pairs four times in x-, y-, z-, and again in x-direction while keeping the problem size constant.
\cref{fig:strong_scaling} and \cref{fig:strong_scaling_efficiency} illustrate the strong scaling results employing both the dilute and dense case for the three different problem sizes. Dashed lines indicate the ideal curves.
\begin{figure*}
  \centering
  \input{plots/strong_scaling.tex}
  \caption{\change{Strong scaling performance for both cases and three different problem sizes up to 1024 \gls{cpu}-\gls{gpu} pairs}}\label{fig:strong_scaling}
  \vspace{0.5cm}
\end{figure*}
\begin{figure*}
  \centering
  \input{plots/strong_scaling_efficiency.tex}
  \caption{\change{Strong scaling parallel efficiency for both cases and three different problem sizes up to 1024 \gls{cpu}-\gls{gpu} pairs}}\label{fig:strong_scaling_efficiency}
\end{figure*}
% Observations
The dilute and dense cases exhibit a similar strong scaling behavior for all three problem sizes.
In contrast to the weak scaling, the dense case does not show an overall lower parallel efficiency in the strong scaling.
In all cases, the time per time step decreases when doubling the number of \gls{cpu}-\gls{gpu} pairs. However, the decrease is less than a factor of 2, resulting in a deviation from the ideal curve.
The corresponding parallel efficiency is around 60\% in all cases when doubling the number of \gls{cpu}-\gls{gpu} pairs twice, whereas it drops to about 40\% when doubling the number of \gls{cpu}-\gls{gpu} pairs four times.
% Interpretation
The decrease in parallel efficiency is expected in strong scaling since, eventually, the problem size becomes too small to effectively utilize the computational resources while the communication overhead increases.
The most relevant driving force for the decrease in parallel efficiency in the strong scaling originates, similar to the weak scaling analysis (\cref{weak_scaling}), from the frequent synchronizations of the particle properties (\gls{pd} comm).
Similar strong scaling behavior, in particular the decrease in parallel efficiency, has been reported in the literature for other \gls{cfd} codes on different supercomputers~\citep[][]{minExascaleWindEnergy2024, karpLargeScaleDirectNumerical2023}.}

\subsubsection{Potential speedup of hybrid implementations}
We expect that the speedup of the hybrid implementation compared to a \gls{cpu}-only code $S_{\text{hyb}}$ can be estimated based on Amdahl's law as
\begin{equation}
  S_{\text{hyb}}  \approx \frac{1}{1+ \textit{frac}_{\text{acc}} \cdot (\frac{BW_{\text{CPU}}}{BW_{\text{GPU}}}-1)},
  \label{speedup}
\end{equation}
where $BW_{\text{CPU}}$ and $BW_{\text{GPU}}$ are the \gls{cpu} and \gls{gpu} memory bandwidths. $\textit{frac}_{\text{acc}}$ is the \gls{cpu}-only run time fraction of the module accelerated by the hybrid implementation. In our case, this is the \gls{psm} and the coupling. We assume $\textit{frac}_{\text{acc}}$ is memory bound.
We compare the hybrid performance results of this paper with one of the largest \gls{cpu}-only simulations of polydisperse sediment beds~\citep[][]{rettingerRheologyMobileSediment2022}.
The authors conducted the latter simulation on 320 Intel Xeon Platinum 8174 \glspl{cpu}. In total, they computed $2.25\cdot 10^{15}$ lattice cell updates in 48 hours, which leads to a performance of around 41 MLUPs per \gls{cpu} vs. 377 MLUPs per \gls{cpu}-\gls{gpu} pair in the dense case when using 1024 \glspl{gpu}. The latter numbers result in a measured speedup of around 9.2. 
For the Intel Xeon Platinum 8174, we measured a memory bandwidth $BW_{\text{CPU}}$ of 70 GB/s.
The estimated speedup based on \cref{speedup} is around 10.3 (assuming $\textit{frac}_{\text{acc}}=0.95$ and $BW_{\text{GPU}}=1400\ \text{GB/s}$) which is similar to the measured speedup. The latter computation is only a rough estimate since it ignores effects due to different \glspl{cpu}, networks, physical setups, numerical parameters, etc.
