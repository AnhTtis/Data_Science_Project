\section{Implementation}\label{implementation}
We implemented our hybrid coupled fluid-particle simulation within the massively parallel multiphysics framework \walberla~\citep[][]{bauerWaLBerlaBlockstructuredHighperformance2021a} (\url{https://www.walberla.net/}).\ \walberla~supports highly efficient and scalable \gls{lbm} simulations on both \glspl{cpu} and \glspl{gpu}~\citep[][]{holzerHighlyEfficientLattice2021}.
The MesaPD module~\citep[][]{eiblModularExtensibleSoftware2019} enables \walberla~to perform particle simulations on \glspl{cpu} using the \gls{dem}.
Large-scale simulations require the usage of numerous nodes of a supercomputer. Each node of a heterogeneous supercomputer typically consists of one or many \glspl{cpu} and \glspl{gpu}. Several \gls{cpu} cores belong to one \gls{gpu}, i.e., they have a direct connection. We divide the simulation domain into multiple blocks and exclusively assign each block to a \gls{gpu}. Figure~\ref{fig:domain_partitioning} illustrates the domain partitioning and the respective hardware assignment.
\begin{figure}
  \centering
  \input{figures/domain_partitioning.tex}
  \caption{
        Partitioning of a 2D simulation domain into four blocks. The circles with ID 0 to ID 9 indicate the particles, and the blue cells are the fluid. One \gls{gpu}x for updating the fluid cells is assigned to each block x, and the corresponding \gls{cpu}x cores are responsible for the particle dynamics. \gls{cpu}x represents the \gls{cpu} cores having a direct connection/affinity to \gls{gpu}x. MPI rank x is assigned to \gls{cpu}x, distributes the particle computations among \gls{cpu}x using OpenMP, and uses \gls{gpu}x for the fluid dynamics.
    }\label{fig:domain_partitioning}
\end{figure}
We want to highlight that the particle simulation is not a standalone framework, but for the particles and the fluid that are physically close to each other (i.e., in the same block), one MPI process is responsible for both the particle and fluid dynamics and the \gls{cpu} and \gls{gpu} have a direct connection/affinity.
The \gls{cpu} cores belonging to the respective \gls{gpu} are responsible for updating the particles whose center of mass lies inside that block (local particles). In Figure~\ref{fig:domain_partitioning}, \gls{cpu}0 is responsible for the particles with ID 2, 5, and 7.
Additionally, particles can overlap with a given block whose center of mass lies in another block (ghost particles). In Figure~\ref{fig:domain_partitioning}, the particle with ID 5 is local for block 0 and ghost for block 2. This overlapping causes the need for communication between the \glspl{cpu}. The particle computations within a block are parallelized among the \gls{cpu} cores using OpenMP.~The communication between neighboring blocks is implemented using the CUDA-Aware Message Passing Interface. On clusters with multiple \glspl{gpu} sharing a node and NVLinks between the \glspl{gpu}, NVIDIA GPUDirect is used for direct GPU-GPU MPI communications between the \glspl{gpu} on the node.
\cref{fig:implementation} illustrates the different modules of the simulation, on which hardware they are running, the workflow, and the necessary communication steps. We will explain the figure in detail in the following sections. Generally speaking, the \gls{gpu} is responsible for all operations on fluid cells (i.e., the \gls{lbm} and the coupling), whereas the \gls{cpu}  performs all computations on particles. The associated data structures are consequently located in the respective memories (fluid cells in \gls{gpu} memory, particles in \gls{cpu} memory).
\begin{figure}
  \centering
  \input{figures/implementation.tex}
  \caption{
        Flowchart of our hybrid \gls{cpu}-\gls{gpu} implementation from the perspective of a \gls{cpu} and \gls{gpu} responsible for the same block. The color coding indicates the communication types required within each step.
    }\label{fig:implementation}
\end{figure}

\subsection{Fluid dynamics and coupling on the GPU}
Performing an \gls{lbm} update step requires the communication of boundary cells between neighboring \glspl{gpu}. However, the first three kernels do not need neighboring information. Therefore, the communication is hidden behind those kernels by starting a non-blocking send before the particle mapping. A time step begins with the coupling from the particles to the fluid.

\subsubsection{Coupling from the particles to the fluid}
For the particle mapping, the \gls{gpu} has to check overlaps for all cell-particle combinations, even though there is no overlap for most cell-particle combinations. This check quickly becomes very computationally expensive. Therefore, we reduce the computational effort by dividing each block into $k$ sub-blocks in each dimension.
The \gls{cpu} inserts every particle into all sub-blocks that overlap with this particle, similar to the Linked Cell Method. Using sub-blocks allows the \gls{gpu} to consider only a tiny subset of particles when computing the overlaps for a particular grid cell, namely the particles overlapping with the sub-block the cell is located in.
Our coupled fluid-particle simulation requires the communication of various data. \cref{fig:compute_setup} gives an overview of the different types of communication from the perspective of a \gls{cpu} and \gls{gpu} responsible for the same block. We will explain the communication steps in the following.
For every particle $i$, the position $\boldsymbol{x}_{\text{p},i}$, radius $r_i$, and $f(r_i)$ (\cref{particle_mapping}) are transferred from the \gls{cpu} to the \gls{gpu} via the PCIe. In addition, the number of overlapping particles per sub-block and the corresponding particle IDs are transferred from the \gls{cpu} to the \gls{gpu}.
Then, the \gls{gpu} performs the particle mapping. For more details, see the description of the solid volume fraction computation (i.e., the particle mapping) in \cref{particle_mapping}.
In our simulation, a maximum of two particles can overlap with a given grid cell due to the geometrically resolved spherical particles and appropriate \gls{dem} parameters allowing only a small particle-particle penetration. Therefore, the grid that we use to store $B_i(\boldsymbol{x},t)$, can store two fraction values per grid cell.
Next, the linear and angular velocities $\boldsymbol{U}_{\text{p},i}$ and ${\boldsymbol{\Omega}}_{\text{p},i}$ of the particles must be synchronized between the \glspl{cpu} such that every \gls{cpu} has not only the correct velocities for its local particles but also for the ghost particles. Next, those velocities are transferred from the \gls{cpu} to the \gls{gpu} so that the \gls{gpu} can compute the velocities of the overlapping particles at the cell center for every cell (\cref{vel_at_cell_center}).
\begin{figure*}
  \centering
  \input{figures/compute_setup.tex}
  \caption{
        Overview of the different communication steps from the perspective of a \gls{cpu} and \gls{gpu} responsible for the same block
    }\label{fig:compute_setup}
\end{figure*}

\subsubsection{Fluid simulation}\label{fluid_simulation}
Next, the \gls{psm} inner kernel is performed. The term `inner' indicates that this kernel updates all cells except the outermost layer of cells. Skipping the outermost layer ensures that this routine can be called without waiting for the previously started \gls{gpu}-\gls{gpu} communication to finish.
The \gls{psm} kernel creates the highest workload of the entire simulation. It is, therefore, performance-critical. We use the code generation framework lbmpy~\citep[][]{bauerLbmpyAutomaticCode2021, hennigAdvancedAutomaticCode2023} to obtain highly efficient and scalable \gls{lbm} CUDA kernels.\ lbmpy allows the formulation of arbitrary \gls{lbm} methods (such as the \gls{psm}) as a symbolic representation and generates optimized and parallel compute kernels. We integrate those generated compute kernels within the simulation in \walberla.
Next, we wait for the non-blocking \gls{gpu}-\gls{gpu} communication started at the beginning of the time step to finish. Depending on the available hardware, this may return instantly if the previous computations completely hide the communication.
The next step is the \gls{lbm} boundary handling, which enforces boundary conditions to the fluid simulation by correctly updating the fluid cells at the domain's boundary.
Since the neighboring values are now available, we then update the outermost layer of cells in the \gls{psm} outer kernel.
The last step on the \gls{gpu} is the coupling from the fluid to the particles.

\subsubsection{Coupling from the fluid to the particles}
Finally, the \gls{gpu} reduces the forces and torques exerted by the fluid on the particles $\boldsymbol{F}_{\text{p},i}^{\text{fp}}$ and $\boldsymbol{T}_{\text{p},i}^{\text{fp}}$ (\cref{hydrodynamic_force,hydrodynamic_torque}).
Then, $\boldsymbol{F}_{\text{p},i}^{\text{fp}}$ and $\boldsymbol{T}_{\text{p},i}^{\text{fp}}$ are transfered from the \gls{gpu} to the \gls{cpu} to be available for the upcoming \gls{dem} simulation on the \gls{cpu}. 
A single particle may overlap with cells located on multiple blocks. Thus, multiple \glspl{gpu} may have computed $\boldsymbol{F}_{\text{p},i}^{\text{fp}}$ and $\boldsymbol{T}_{\text{p},i}^{\text{fp}}$ for the same particle $i$. Therefore, the corresponding \glspl{cpu} have to reduce these force and torque contributions exerted by the coupling on the particles into a single variable (\gls{cpu}-\gls{cpu} communication).
The time loop continues with the particle dynamics on the \gls{cpu}.

\subsection{Particle dynamics on the CPU}\label{particle_dynamics}
The first step of the \gls{pd} simulation on the \gls{cpu} is the pre-force integration of the velocities to update the particle positions (\cref{pre_force_integration}). The latter particle movement requires synchronization between the \glspl{cpu} to account for the position update, which potentially moves particles from one block to another, making other \glspl{cpu} responsible for the particles. Computing particle-particle interactions by iterating over all particle pairs can quickly become very expensive due to its $O(n^2)$ complexity. Therefore, we insert the particles into linked cells such that iterating over the particle pairs is limited to neighboring linked cells. The linked cell size limits the maximum distance for which correct particle-particle interactions can be ensured (collision, lubrication correction). The linked cells have a size of 1.01 times the particle diameter, which is close to the smallest size that still leads to a correct collision detection.
Next, the lubrication correction routine is applied to all particle pairs with particles close to each other but not yet in contact (\cref{lubrication_correction}).
The particle-particle interactions are modeled using the \gls{dem} collision kernel (linear spring dashpot), which exerts forces and torques on overlapping particles.
The collision kernel needs history information from the previous time step, i.e., the accumulated tangential motion between the two colliding particles (\cref{history_information}). Since different processes may have handled the previous collisions, reducing the collision histories between the \glspl{cpu} is necessary, i.e., collecting all collision histories for a given particle in one process.
Then, the hydrodynamic forces and torques exerted by the fluid on the particles and the gravitational force are added to the total force.
Since different processes may have added forces and torques to a given particle, those contributions have to be collected in one process (another \gls{cpu}-\gls{cpu} communication).
Then, the post-integration is applied to update the particle velocities (\cref{post_force_integration}). Communication can be omitted here because the velocities are unused until the subsequent communication in the upcoming sub-cycle. 
Typically, $j$ particle sub-cycles are performed per time step since the \gls{dem} requires a finer resolution in time than the \gls{lbm} for an accurate contact representation~\citep[][]{rettingerEfficientFourwayCoupled2022}.
After completing $j$ sub-cycles, the next time step starts with the fluid dynamics on the \gls{gpu}.\newline
Running the fluid dynamics and coupling on the \gls{gpu} first and the particle dynamics on the \gls{cpu} second seems to be a promising candidate for overlapping the \gls{cpu} and \gls{gpu} computations to gain some performance improvements. Therefore, we elaborate on this possibility in the remainder of this section.
The following will refer to the different simulation modules as they are named in \cref{fig:implementation}.\newline
Under the condition that the numerical error must not increase, only some parts of the \gls{cpu} and \gls{gpu} computations can overlap, others cannot due to the dependencies of the two-way coupling. 
There are two ways of overlapping the \gls{cpu} and \gls{gpu} parts: within a time step or between subsequent time steps.
Only the post-force integration step of the particle simulation on the \gls{cpu} in time step $n$ depends on the reduction of the hydrodynamic forces on the \gls{gpu} in time step $n$, and therefore, the steps from pre-force integration to the application of external forces can, in principle, be overlap with the \gls{gpu} part.
The particle mapping on the \gls{gpu} at the beginning of time step $n+1$ requires the updated particle positions of the pre-force integration in time step $n$. Therefore, the steps from inserting the particles into linked cells until the post-force integration in time step $n$ can, in principle, overlap with the particle mapping on the \gls{gpu} in time step $n+1$.
However, the particle simulation typically consists of several sub-cycles (i.e., the \gls{cpu} part in \cref{fig:implementation} is executed several times per time step), and only the respective parts of the first and last sub-cycle can be overlapped. Therefore, the maximum achievable speedup due to overlapping depends on the number of sub-cycles. In accordance with the literature~\citep[][]{rettingerEfficientFourwayCoupled2022}, we use ten sub-cycles, which could potentially decrease the run time of the particle simulation on the \gls{cpu} by 20\% if the first and last sub-cycles could be entirely overlapped. However, this is an optimistic assumption because the interruption of the consecutive particle sub-cycles by the overlapping might negatively affect the possibility of caching particle data between consecutive sub-cycles.\newline
If one disregards the requirement that numerical error must not increase, one could use results from previous time steps, allowing both the fluid simulation on the \gls{gpu} and the particle simulation on the \gls{cpu} to run entirely in parallel. This might result in acceptable minor errors for systems with negligible particle movements, but this is not generally applicable and would require a detailed error analysis.
