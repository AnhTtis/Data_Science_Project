\section{Implications and lessons learned}\label{implications}
As a lesson learned, we discovered that the relatively small data transfers between \gls{cpu} and \gls{gpu} can become negligible for coupled applications exhibiting a significant imbalance of the data exchange between the coupled modules and the computations inside the modules in favor of the computations, making hybrid coupling a feasible approach.
This is particularly true for the application used in this work, as only the several orders of magnitude smaller particle data need to be exchanged, not the entire fluid field processed by the \gls{gpu}. As a result, only 0.35\% of the total run time is devoted to \gls{cpu}-\gls{gpu} communication in the dense case.
These findings can also be applied to other applications that exhibit this desired imbalance between computation data size and data transfer between the coupled modules. Other coupled applications, such as a huge flow field around complex deformable geometries like the deformation of blood cells in cellular blood flow, which is referenced in \cref{introduction}, can also exhibit this imbalance. Since the surface data acts as a boundary condition for the deformation simulation, just this surface data needs to be exchanged between the fluid and solid phases.
\change{Approaches with a high ratio of data exchange to computations} (i.e., exchange entire fields between \gls{cpu} and \gls{gpu} in each time step) have been found in the literature to perform undesired on previous architectures, this may change in the future due to new hardware developments.
High bandwidth memory transfer between \gls{cpu} and \gls{gpu} main memory is the trend of architectures like the NVIDIA GH200 Grace Hopper, which makes hybrid implementations promising for more applications (even up to transferring the entire \gls{gpu} data in every time step) and should be further studied in the future.\newline
The advantages of code generation (\cref{fluid_simulation}) are the subject of another lesson learned. We discovered that code generation adds an extra overhead during the implementation stage, which is only beneficial in a framework with long-term support if one depends on highly optimized similar codes, portability to other architectures, and a certain form of expandable, sustainable approach. 
However, because code generation is used, the code functions flawlessly with different \gls{lbm} variations on other modern \gls{gpu} architectures, such as the AMD MI250X, which uses the HIP API, as well as on consumer \glspl{gpu}. Subsequent work will encompass a systematic performance comparison.
