\section{Introduction}
% Motivation
Numerical multiphysics simulations are a powerful technique for conducting in-depth investigations of complex physical phenomena by providing detailed data, which is challenging, if not impossible, to collect in experiments.
A prominent example is coupled fluid-particle simulations with geometrically resolved particles.
Such simulations have been used in the literature to understand the formation and dynamics of dunes in river beds~\cite{rettingerFullyResolvedSimulations2017, schwarzmeierParticleresolvedSimulationAntidunes2023, kidanemariamDirectNumericalSimulation2014}, investigate the erosion kinetics of soil under an impinging jet~\cite{benseghierParallelGPUbasedComputational2020} and to analyze mobile sediment beds~\cite{vowinckelFluidParticleInteraction2014, rettingerRheologyMobileSediment2022}.
A significant challenge with these simulations is that they are very computationally expensive, which is why they often run on supercomputers.
Especially supercomputers containing \gls{gpu}s have become increasingly popular in recent years~\cite{shimokawabeStencilFrameworkRealize2017, rohrEnergyEfficientMultiGPUSupercomputer2014, oyarzunPortableImplementationModel2017} as they offer unprecedented computing power.\newline
% Our story
In such multiphysics simulations, the various combined methodologies may exhibit distinctly contrasting computational properties, e.g., problem sizes, parallel and sequential portions, frequency of conditions, and branching. Therefore, the best-suited hardware architecture can differ between the simulation components. Furthermore, there are practical reasons that can challenge multiphysics simulations: not all coupled software frameworks and modules may support the same hardware. 
We face these challenges in coupled fluid-particle simulations with geometrically resolved particles. 
The fluid simulation using a structured, grid-based methodology is ideally suited for \gls{gpu} parallelization~\cite{holzerHighlyEfficientLattice2021}. The situation is different for the particle simulation because it operates on unstructured data structures with irregular accesses due to frequent branching. On top of that, when combining the particle simulation with a fluid simulation using a fully resolved coupling, the number of particles is typically at least three orders of magnitude smaller than the number of fluid cells, leading to an imbalance of the workload. These characteristics of the particle simulation do not predestine it for \gls{gpu}s.
Porting the entire fluid-particle simulations from the \gls{cpu} to the \gls{gpu} would be an enormous expenditure of time. Latter is hard to justify for the particle simulation as it is debatable if the computational properties of the particle simulation are suited for \gls{gpu}s at all.
This is a major reason why coupled fluid-particle implementations still are often limited to \gls{cpu}s~\cite{rettingerFullyResolvedSimulations2017, kidanemariamDirectNumericalSimulation2014, vowinckelFluidParticleInteraction2014}, even though \gls{gpu}s would be best suited for the dominating part: the fluid simulation.
These issues predestinate or even force a hybrid implementation, i.e., utilizing \gls{gpu}s for the fluid dynamics and the coupling. In contrast, the \gls{pd} simulation runs on \gls{cpu}s.\newline
% Related work
Hybrid coupled fluid-particle simulations have been used several times in the literature. On the one hand, a common approach is to use the \gls{cpu} for the fluid simulation and the \gls{gpu} for the \gls{pd}, for example, when simulating deformable bodies (using the finite element method) in a fluid, i.e., blood cells in cellular blood flow~\cite{kotsalosDigitalBloodMassively2021}. Here, the focus is more on structural mechanics, as the finite element method dominates the overall run time. The latter approach has also been taken in the literature to couple commercial \gls{cfd} solvers on the \gls{cpu} such as Ansys~\cite{heCPUGPUCrossplatformCoupled2020, sousaniAcceleratedHeatTransfer2019}, or AVL fire~\cite{jajcevicLargescaleCFDSimulations2013} with the \gls{dem} on the \gls{gpu}. This approach also appears in the context of fluidized beds~\cite{xuDiscreteParticleSimulation2012, norouziNewHybridCPUGPU2017}.
On the other hand, several publications have used the approach promising for our problem: using the \gls{cpu} for the particles and the \gls{gpu} for the fluid.
One variant is to run the \gls{lbm} on a single \gls{gpu} and use a single \gls{cpu} for the particles~\cite{valero-laraAcceleratingFluidSolid2015}, or use the \gls{gpu} for both the fluid and the particles, but the \gls{cpu} supports the particle dynamics through collision detection~\cite{juniorFluidSimulationTwoWay2010}.
Another idea is to use either multiple \gls{cpu}s for the particle simulation~\cite{roehmLatticeBoltzmannSimulations2012}, or multiple \gls{gpu}s for the \gls{lbm}~\cite{sheikhNumericalInvestigationEffects2015}.\newline
% Contributions
The Top500 list reports the most powerful supercomputer worldwide. The number of heterogeneous supercomputers, i.e., systems with additional accelerators such as \gls{gpu}s, in the Top500 list has steadily increased in the last years, accounting for roughly 28\% at the end of 2019~\cite{khanAnalysisSystemBalance2021}. Each node of such a heterogeneous supercomputer typically consists of multiple \gls{cpu}s and multiple \gls{gpu}s~\cite{kimSnuCLOpenCLFramework2012}.
On the one hand, hybrid implementation can fully utilize the capacities of a heterogeneous supercomputer by using both the \gls{cpu}s and the \gls{gpu}s.
On the other hand, for a responsible use of a heterogeneous supercomputer, the performance has to meet specific criteria. First, the penalty introduced by the hybrid implementation (i.e., the \gls{cpu}-\gls{gpu} communication) has to be negligible compared to the overall run time. Second, the performance-critical fluid simulation should show good performance, i.e., fully utilize the \gls{gpu} memory bandwidth. Third, the share of the \gls{gpu} run time in the total run time must be sufficiently high to justify using a heterogeneous cluster. Fourth, it has to show a satisfactory weak scaling performance for the responsible use of multiple supercomputer nodes. However, none of the above publications provide a framework for geometrically resolved fluid-particle simulations fulfilling all the criteria.\newline
In this paper, we want to investigate if hybrid implementations are suitable for large-scale coupled fluid-particle simulations on heterogeneous supercomputers.
Therefore, we supplement the existing literature by introducing a multi-\gls{cpu}-multi-\gls{gpu} hybrid implementation for coupled fluid-particle simulations with geometrically resolved particles. 
To evaluate our implementation based on the above criteria, we provide in-depth performance analysis on a state-of-the-art heterogeneous supercomputer comparing two cases of a fluidized bed simulation with a distinctly different number of particles per volume. We assess the hybrid penalty, investigate the run time distribution among the various simulation components and provide an in-depth analysis of the weak scaling performance for up to 1024 A100 \gls{gpu}s.
We use code generation technologies to obtain highly efficient compute kernels and combine them with handwritten codes.\newline
% Overview
The paper starts in \cref{numerical_methods} with an introduction of the numerical methods, i.e., the \gls{lbm}, the \gls{dem}, and the corresponding coupling. The numerical methods are followed by \cref{implementation} with a description of the implementation.
In \cref{performance}, we provide a detailed performance examination. This examination includes the \gls{cpu}-\gls{gpu} communication overhead, the \gls{dem} performance, the run times of the different simulation components, a weak scaling, a roofline model, and an estimation of the hybrid speedup.
The paper concludes with \cref{conclusion}.
