\section{Performance analysis}\label{performance}
We use the Juwels Booster cluster for the performance measurements. Each GPGPU node consists of four Nvidia A100 40 GB, two AMD EPYC 7402 processors (24 cores per chip), and eight NUMA domains. Thus, six cores belong to one NUMA domain. The following will refer to a \gls{gpu} and an associated NUMA domain with its six cores as a CPU-GPU pair. All \gls{gpu}s within a node are connected via NVLinks, allowing direct \gls{gpu}-\gls{gpu} communications. A scaling benchmark (i.e., a 1:1 read/write ratio) yields a memory bandwidth of about 1400 GB/s for the A100 40 GB~\cite{ernstAnalyticalPerformanceEstimation2023}. % https://github.com/te42kyfo/gpu-benches/blob/master/gpu-stream/a100_40.txt
We use 20 cells per diameter to geometrically resolve the particles~\cite{rettingerRheologyMobileSediment2022, rettingerEfficientFourwayCoupled2022, biegertCollisionModelGrainresolving2017, costaCollisionModelFully2015}.
The upcoming sections first introduce the computational properties of the simulated cases, followed by their performance results.

\subsection{Simulation setups}\label{setups}
We study the performance of our hybrid coupled fluid-particle implementation using a fluidized bed simulation. We compare two cases: the dilute case and the dense case. They exhibit different characteristics regarding the number of particles per volume and the number of particle-particle interactions. We choose these two cases to investigate how different particle workloads on the \gls{cpu}s influence the overall performance of the hybrid \gls{cpu}-\gls{gpu} implementation.
We use ten particle sub-cycles (see \cref{implementation}) per time step.
We discretize the simulated domain using $500 \times 200 \times 800 = 80 \times 10^6$ fluid cells. The inflow \gls{bc} on the bottom and pressure (outflow) \gls{bc} on top of the domain govern the fluid dynamics. The remaining four boundaries are no-slip conditions ensuring a zero fluid velocity. The particle Reynolds number is $1.0$, the Galileo number is around $8.9$, the gravitational acceleration is $9.81\ \text{m}/\text{s}^2$, and the particle fluid density ratio is $1.1$.
Planes surrounding the domain prevent the particles from leaving the domain.
The dilute case contains 627 particles. \cref{fig:fluidized_bed} (left) illustrates the dilute setup.
Due to the low particle concentration, the effort for computing the particle-particle interactions (collisions and lubrication corrections) is low in the dilute case. 
The dense case is generally the same setup as the dilute case, except that the particle concentration is significantly higher (see \cref{fig:fluidized_bed} on the right), resulting in 8073 particles, almost a 13-fold increase compared to the dilute case.

\begin{figure}
  \centering
  \includegraphics[scale=0.5, height=10cm]{figures/dilute_case.png}
  \includegraphics[scale=0.5, height=10cm, trim=0 0.65cm 0 -0.65cm]{figures/dense_case.png}
  \caption{Visualization of the fluidized bed setup running on one CPU-GPU pair. Dilute case on the left, dense case on the right. For the fluid field, only a two-dimensional slice is visualized.}\label{fig:fluidized_bed}
\end{figure}

\subsection{Performance results}
In this section, we present and analyze the performance results. We first look into the scaling of the \gls{pd} on multiple \gls{cpu} cores to assess our initial assumption that the particle simulation part of such a coupled simulation would not benefit from a GPU parallelization.
Next, we look at the individual run times of the different simulation components to understand the bottlenecks.
Furthermore, we present a weak scaling benchmark for both cases up to 1024 \gls{cpu}-\gls{gpu} pairs.
Finally, we demonstrate the acceleration potential of hybrid implementations by comparing it to a large-scale \gls{cpu}-only simulation from the literature.
For all results, we average over 500 time steps.
In the following, we will refer to the performance criteria formulated in the introduction.

\subsubsection{Particle scaling on the CPU}
% Description
Using OpenMP with static scheduling, we parallelize the \gls{pd} simulation on the \gls{cpu}.\@ Per-particle computations (e.g., adding gravitational forces) are parallelized among the particles, and particle-particle interactions are parallelized among the linked cells in one dimension. We use multiple cores of a single \gls{cpu} and measure the run time of the \gls{pd} simulation. The parallel efficiency $E_n$ is defined based on the serial run time $t_{\text{s}}$ and the run time using $n$ cores in parallel $t_{\text{p}}(n)$:

\begin{equation}
  E_n=\frac{t_s}{t_{\text{p}}(n) \cdot n}.
\end{equation}

\noindent
For a perfectly linear scaling code, i.e., $t_{\text{p}}(n)=t_s/n$, the maximum achievable parallel efficiency $E_{\max}$ is 1 $\forall n$. For a non-scaling code, i.e., $t_{\text{p}}(n)=t_{\text{s}}\ \forall n$, the minimum achievable parallel efficiency $E_{\min}$ is $1/n$.\ \cref{fig:particle_scaling} reports the parallel efficiency of the \gls{pd} simulation for both cases for up to six cores, i.e., the size of a NUMA domain. We do not use more cores than available in a NUMA domain since the \gls{pd} implementation is not NUMA-aware. % ist das so und warum w√§re es ein Problem das NUMA-aware zu implementieren?

\begin{figure}
  \centering
  \input{plots/particle_scaling.tex}
  \caption{Parallel efficiency inside a NUMA domain for both cases. The perfect scaling (Best) and non-scaling (Worst) form the upper and lower bound. }\label{fig:particle_scaling}
\end{figure}

\noindent
% Observations
We observe a similar pattern regarding the parallel efficiency in both the dilute and the dense case. Both show a decrease down to around 45\% for six cores.\newline
% Interpretation
A parallel efficiency of about 45\% when using a comparatively small number of six cores is a poor scaling result. We expect this performance for several reasons. First, the \gls{pd} simulation consists of ten subsequent sub-cycles per time step, each containing several consecutive tasks (see \cref{fig:implementation}). Each task consists of an OpenMP parallel loop with an implicit OpenMP barrier at the end, causing idle threads to wait for all threads to end the loop. Second, modifying the force and torque of a particle must be enclosed with an atomic region to avoid race conditions, which harms the parallel efficiency, but is a central task and thus required in several \gls{pd} routines. Third, the workload (for the \gls{dem} and lubrication routine) per particle can differ significantly depending on the particle's location inside the domain. The latter aspects hold for the dense and the dilute case, explaining the poor parallel efficiency in both cases.
The poor parallel efficiency of the particle methodology supports our initial assumption that the particle simulation part would not benefit from a \gls{gpu} parallelization in the context of geometrically resolved coupled simulations.

\subsubsection{Run times of different simulation components}
% Description / Reference
We investigate the run times of the different simulation components to analyze the penalty introduced by the hybrid implementation, i.e., the \gls{cpu}-\gls{gpu} communication, assess the \gls{gpu} performance, and detect the overall bottlenecks. To evaluate the performance of the \gls{psm} kernel on the \gls{gpu}, we build a roofline model~\cite{hagerIntroductionHighPerformance2010} for an \gls{lbm} kernel. As this kernel is memory bound, we determine the maximal possible performance for the given kernel and hardware when fully utilizing the memory bandwidth (i.e., the performance lightspeed estimation). The \gls{psm} kernel comprises the \gls{lbm} kernel plus additional memory transfers depending on the number of overlapping particles. As this number differs from cell to cell, the \gls{psm} roofline model is not straightforward. Therefore, we use the \gls{lbm} model, keeping in mind that this is a too-optimistic performance lightspeed estimation.
Since we use the D3Q19 lattice model, we read and write 19 \gls{pdfs} per cell and time step.
This results in 19 reads and 19 writes (double-precision), i.e., 304 bytes to update one lattice cell~\cite{feichtingerPerformanceModelingAnalysis2015}. The domain consists of $8e7$ fluid cells. This results in the following minimal run time according to the roofline model:

\begin{equation}
  T_\min = \frac{304 \ \text{B/cell} \cdot 8e7 \ \text{cells/time step}}{1400 \ \text{GB/s}} = 17.4 \ \text{ms/time step}.
\end{equation}

\noindent
We divide the total run time into the following components: the \gls{psm} kernel (PSM), the \gls{cpu}-\gls{gpu} communication (comm), the particle mapping (mapping), setting the particle velocities (setU), reducing the hydrodynamic forces $\boldsymbol{F}_{\text{p},i}^{\text{fp}}$ and torques $\boldsymbol{T}_{\text{p},i}^{\text{fp}}$ on the particles (redF), computing the \gls{pd} and finally the remaining tasks (other), e.g., the \gls{lbm} boundary handling.\ \cref{fig:runtimesa100} reports the run times per time step for these components using a single \gls{cpu}-\gls{gpu} pair.

\begin{figure}
  \centering
  \input{plots/runtimes_components.tex}
  \caption{Individual run times of the different simulation components on a \gls{cpu}-\gls{gpu} pair.}\label{fig:runtimesa100}
\end{figure}

\noindent
% Observations
In the dilute case, the \gls{psm} kernel needs about 42\% more time per time step than the \gls{lbm} lightspeed estimation, and the dense case 83\% more time. The \gls{cpu}-\gls{gpu} communication is negligible for both cases. All components take longer in the dense case than in the dilute case. While in the dilute case, the \gls{psm} kernel accounts for the majority of the run time, the \gls{pd} simulation needs more time than the \gls{psm} kernel in the dense case. Still, most of the run time is spent on \gls{gpu} routines in the dense case.\newline
% Interpretation
The penalty introduced by the hybrid implementation is negligible because we only transfer a small amount of double-precision values per particle but no fluid cells. The penalty, therefore, shows that a hybrid parallelization with the presented technique is a viable approach, and the first criterion is met. The performance of the \gls{psm} kernel is close to utilizing the total memory bandwidth of the A100, especially considering that the roofline model does not consider the memory traffic due to the solid part of the \gls{psm} kernel. Therefore, the second criterion is met. There is a significant performance gain for the \gls{psm} kernel on the \gls{gpu} compared to a \gls{cpu}-only implementation since the \gls{psm} kernel is utilizing almost the entire memory bandwidth, which is typically way lower on \gls{cpu}s.
Even in the dense case, the \gls{gpu} run time accounts for most of the total time because the fluid workload is much higher than the particle workload. Therefore, the third criterion is met.

\subsubsection{Weak scaling}
% Description
When increasing the simulation domain further to simulate physically relevant scenarios, using a single \gls{cpu}-\gls{gpu} pair is often insufficient. Instead, multiple pairs or even multiple nodes of a supercomputer have to be used. Therefore, a satisfactory weak scaling is inevitable. For weak scaling, the problem size is increased with an increasing number of \gls{cpu}-\gls{gpu} pairs, keeping the workload per \gls{cpu}-\gls{gpu} pair constant. When having a perfect weak scaling, the performance per \gls{cpu}-\gls{gpu} pair stays constant, independent of the number of \gls{cpu}-\gls{gpu} pairs used. Performance is work over time. In the context of \gls{lbm}, MLUPs is a standard performance metric for weak scaling, meaning how many mega lattice cell updates the hardware performs per second. 
We use the total run time for computing the MLUPs, containing both the \gls{cpu} (particles) and the \gls{gpu} (fluid, coupling) time. We have conducted a few benchmarking runs and will use the best sample in the following weak-scaling plots.
We start with a single \gls{cpu}-\gls{gpu} pair and a single domain block as described in \cref{setups}. We then double the number of \gls{cpu}-\gls{gpu} pairs several times until we reach 1024. At the same time, we double the domain blocks/size alternately in each direction: $2\times 1 \times 1$ blocks (two \gls{gpu}s), $2\times 2 \times 1$ blocks (four \gls{gpu}s), $2\times 2 \times 2$ (eight \gls{gpu}s), etc.\ \cref{fig:weak_scaling} reports the weak scaling for both cases. To the best of our knowledge, this is the most extensive weak scaling of a hybrid fluid-particle implementation presented in the literature.

\begin{figure}
  \centering
  \input{plots/weak_scaling.tex}
  \caption{Weak scaling performance for both cases up to 1024 \gls{cpu}-\gls{gpu} pairs.}\label{fig:weak_scaling}
\end{figure}

\noindent
% Observations
We observe a roughly three times higher performance for the dilute case than for the dense case. Both cases show a parallel efficiency decrease particularly strong in the beginning. The parallel efficiency is 71\% in the dilute case and 53\% in the dense case when using 1024 \gls{cpu}-\gls{gpu} pairs, which corresponds to a domain size of $8000 \times 1600 \times 6400 = 8.192 \times 10^{10}$ fluid cells. A similar scaling behavior has been observed in the literature both for other hybrid~\cite{ kotsalosDigitalBloodMassively2021} and \gls{cpu}-only fluid-particle implementations~\cite{rettingerFullyResolvedSimulations2017}.\newline
% Description
Interpreting the overall weak scaling behavior requires an in-depth analysis of the scaling of the different simulation components. When using a single \gls{cpu}-\gls{gpu} pair, the dominating routines are the \gls{pd}, the \gls{psm} kernel, and the coupling (i.e., particle mapping, setting the particle velocities and reducing the hydrodynamic forces $\boldsymbol{F}_{\text{p},i}^{\text{fp}}$ and torques $\boldsymbol{T}_{\text{p},i}^{\text{fp}}$
on the particles). Additionally, we must now consider the communication (comm) overhead that arises from using multiple \gls{cpu}-\gls{gpu} pairs. On the one hand, this is the \gls{pd} communication (\gls{cpu}-\gls{cpu} communication), on the other hand, the PSM communication (\gls{gpu}-\gls{gpu} communication). \cref{fig:weak_scaling_routines_dilute} and \cref{fig:weak_scaling_routines_dense} show the weak scaling behavior of the dominating simulation components for both cases. The communication numbers cover the communications themselves, but also load imbalances between two communication.

\begin{figure}
  \centering
  \input{plots/weak_scaling_components_dilute.tex}
  \caption{Weak scaling performance of the dominating components for the dilute case up to 1024 \gls{cpu}-\gls{gpu} pairs.}\label{fig:weak_scaling_routines_dilute}
\end{figure}

\begin{figure}
  \centering
  \input{plots/weak_scaling_components_dense.tex}
  \caption{Weak scaling performance of the dominating components for the dense case up to 1024 \gls{cpu}-\gls{gpu} pairs.}\label{fig:weak_scaling_routines_dense}
\end{figure}

\noindent
% Observations
The different components show similar qualitative scaling behavior when comparing the two cases. The \gls{psm} kernel scales quite well in both cases. The corresponding \gls{gpu}-\gls{gpu} communication (\gls{psm} comm) is negligible. The \gls{pd} run time increases initially and then shows saturation. The \gls{cpu}-\gls{cpu} communication (\gls{pd} comm) increases drastically, overtaking the run time of the \gls{psm} kernel and the coupling in the dense case. The \gls{pd} and the corresponding communication are more relevant for the overall scaling in the dense case than in the dilute case. The coupling scales similarly to the \gls{pd}.\newline
% Interpretation
The \gls{psm} workload per \gls{gpu} stays constant in the weak scaling explaining the nearly perfect scaling.
Since we are hiding the \gls{psm} communication (see \cref{implementation}), we expect it to be negligible.
We expect the \gls{pd} and the coupling run time to increase initially because the number of neighboring blocks increases. More neighboring blocks lead to more ghost particles per block, resulting in a higher workload. This effect fades out when blocks have neighbors in all directions resulting in an almost linear scaling from this point on. This phenomenon has been reported in the literature~\cite{rettingerFullyResolvedSimulations2017}.
The methodology requires ten particle sub-cycles per time step and three communications per sub-cycle for a physically accurate simulation. Additionally, the simulation requires two \gls{cpu}-\gls{cpu} communications per time step apart from the sub-cycles (see \cref{fig:implementation}). We have 32 \gls{cpu}-\gls{cpu} communications per time step, which cannot be hidden behind other routines. It is the dominating factor for the decrease of the overall weak scaling performance in both cases. We assume this is due to these frequent synchronizations between the processes and the high pressure on the network.
Reducing the collision history is part of \gls{pd} comm, which includes swapping old and new contact information. Since this swap is also necessary without using multiple \gls{cpu}-\gls{gpu} pairs, \gls{pd} comm is bigger zero even when using a single \gls{cpu}-\gls{gpu} pair.
Overall, we observe a weak scaling performance that justifies using multiple supercomputer nodes. Therefore, the fourth criterion is met.

\subsubsection{Potential speedup of hybrid implementations}
We expect that the speedup of the hybrid implementation compared to a \gls{cpu}-only code $S_{\text{hyb}}$ can be estimated as

\begin{equation}
  S_{\text{hyb}}  \approx \frac{1}{1+ frac_{\text{acc}} \cdot (\frac{BW_{\text{CPU}}}{BW_{\text{GPU}}}-1)},
  \label{speedup}
\end{equation}

\noindent
where $BW_{\text{CPU}}$ and $BW_{\text{GPU}}$ are the \gls{cpu} and \gls{gpu} memory bandwidths. $frac_{\text{acc}}$ is the \gls{cpu}-only run time fraction of the component accelerated by the hybrid implementation. In our case, this is the \gls{psm} and the coupling. We assume $frac_{\text{acc}}$ is memory bound.
We compare our hybrid performance results with one of the largest \gls{cpu}-only simulations of polydisperse sediment beds~\cite{rettingerRheologyMobileSediment2022}.
The authors conducted the latter simulation on 320 Intel Xeon Platinum 8174 \gls{cpu}s. In total, they computed $2.25\cdot 10^{15}$ lattice cell updates in 48 hours, which leads to a performance of around 41 MLUPs per \gls{cpu} vs. 377 MLUPs per \gls{cpu}-\gls{gpu} pair in the dense case when using 1024 \gls{gpu}s. The latter numbers result in a measured speedup of around 9.2. 
For the Intel Xeon Platinum 8174, we measured a memory bandwidth $BW_{\text{CPU}}$ of 70 GB/s.
The estimated speedup based on \cref{speedup} is around 10.3 (assuming $frac_{\text{acc}}=0.95$ and $BW_{\text{GPU}}=1400\ \text{GB/s}$) which is similar to the measured speedup. The latter computation is only a rough estimate since it ignores effects due to different \gls{cpu}s, networks, physical setups, etc.
