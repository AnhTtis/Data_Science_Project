\section{Implementation}\label{implementation}
We implemented our hybrid coupled fluid-particle simulation within the massively parallel multiphysics framework \walberla~\cite{bauerWaLBerlaBlockstructuredHighperformance2021a} (\url{https://www.walberla.net/}).\ \walberla~supports highly efficient and scalable \gls{lbm} simulations on both \gls{cpu}s and \gls{gpu}s~\cite{holzerHighlyEfficientLattice2021}.
The MesaPD module~\cite{eiblModularExtensibleSoftware2019} empowers \walberla~to perform particle simulations on \gls{cpu}s using the \gls{dem}.
Large-scale simulations require the usage of numerous nodes of a supercomputer. Each node of a heterogeneous supercomputer typically consists of multiple \gls{cpu}s and multiple \gls{gpu}s. Each \gls{cpu} core belongs to one \gls{gpu}. We divide the simulation domain into multiple blocks and exclusively assign each block to a \gls{gpu}. The \gls{cpu} cores belonging to the respective \gls{gpu} are responsible for updating the particles whose center of mass lies inside that block (local particles). Additionally, particles can overlap with a given block whose center of mass lies in another block (ghost particles). This overlapping causes the need for communication between the \gls{cpu}s. The particle computations within a block are parallelized among the \gls{cpu} cores using OpenMP.~The communication between the blocks is implemented using the CUDA-Aware Message Passing Interface, also allowing for direct \gls{gpu}-\gls{gpu} communications.
\cref{fig:implementation} illustrates the different components of the simulation, on which hardware they are running, the workflow, and the necessary communications. We will explain the figure in detail in the following sections. Generally speaking, the \gls{gpu} is responsible for all operations on fluid cells (i.e., the \gls{lbm} and the coupling), whereas the \gls{cpu}  performs all computations on particles. The associated data structures are consequently located in the respective memories (fluid cells in \gls{gpu} memory, particles in \gls{cpu} memory).

\begin{figure}
  \centering
  \input{figures/implementation.tex}
  \caption{
        Flowchart of our hybrid \gls{cpu}-\gls{gpu} implementation from the perspective of a CPU and GPU responsible for the same block. The color coding indicates the communication types required within each step.
    }\label{fig:implementation}
\end{figure}

\subsection{Fluid dynamics and coupling on the GPU}
Performing an \gls{lbm} update step requires the communication of boundary cells between neighboring \gls{gpu}s. However, the first three kernels do not need neighboring information. Therefore, the communication is hidden behind those kernels by starting a non-blocking send before the particle mapping. A time step begins with the coupling from the particles to the fluid.

\subsubsection{Coupling from the particles to the fluid}
For the particle mapping, the \gls{gpu} has to check overlaps for all cell-particle combinations. This check quickly becomes very computationally expensive, even though there is no overlap for most cell-particle combinations. Therefore, we reduce the computational effort by dividing each block into $k$ sub-blocks in each dimension.
The \gls{cpu} inserts every particle into all sub-blocks that overlap with this particle. Using sub-blocks allows the \gls{gpu} to consider only a tiny subset of particles when computing the overlaps for a particular grid cell, namely the particles overlapping with the sub-block the cell is located in.
Our coupled fluid-particle simulation requires the communication of various data. \cref{fig:compute_setup} gives an overview of the different communications from the perspective of a \gls{cpu} and \gls{gpu} responsible for the same block. We will explain the communications in the following.
For every particle $i$, the position $\boldsymbol{x}_{\text{p},i}$, radius $r_i$, and $f(r_i)$ (see \cref{particle_mapping}) are transferred from the \gls{cpu} to the \gls{gpu}. In addition, the number of overlapping particles per sub-block and the corresponding particle IDs are transferred from the \gls{cpu} to the \gls{gpu}.
Then, the \gls{gpu} performs the particle mapping. In detail, we described the solid volume fraction computation (i.e., the particle mapping) in \cref{particle_mapping}.
In our simulation, a maximum of two particles can overlap with a given grid cell due to the geometrically resolved spherical particles and appropriate \gls{dem} parameters allowing only a small particle-particle penetration. Therefore, the grid we use to store $B_i(\boldsymbol{x},t)$ can store two fraction values per grid cell.
Next, the linear and angular velocities $\boldsymbol{U}_{\text{p},i}$ and ${\boldsymbol{\Omega}}_{\text{p},i}$ of the particles have to be synchronized between the \gls{cpu}s such that every \gls{cpu} not only has the correct velocities for its local particles but also for the ghost particles. Next, those velocities are transferred from the \gls{cpu} to the \gls{gpu} so that the \gls{gpu} can compute the velocities of the overlapping particles at the cell center for every cell (see \cref{vel_at_cell_center}).

\begin{figure}
  \centering
  \input{figures/compute_setup.tex}
  \caption{
        Overview of the different communications from the perspective of a \gls{cpu} and \gls{gpu} responsible for the same block.
    }\label{fig:compute_setup}
\end{figure}

\subsubsection{Fluid simulation}
Next, the \gls{psm} inner kernel is performed. The term `inner' indicates that this kernel updates all cells except the outermost layer of cells. Skipping the outermost layer ensures this routine can be called without waiting for the previously started \gls{gpu}-\gls{gpu} communication to finish.
The \gls{psm} kernel has the highest workload of the entire simulation. It is, therefore, performance-critical. We use the code generation framework lbmpy~\cite{bauerLbmpyAutomaticCode2021} to obtain highly efficient and scalable \gls{lbm} CUDA kernels.\ lbmpy allows to formulate \gls{lbm} methods (such as the \gls{psm}) as a symbolic representation and generates optimized and parallel compute kernels. We integrate those generated compute kernels within our simulation in \walberla.
Next, we wait for the non-blocking \gls{gpu}-\gls{gpu} communication started at the beginning of the time step to finish. Depending on the available hardware, this may return instantly if the previous computations completely hide the communication.
The next step is the \gls{lbm} boundary handling, which enforces boundary conditions to the fluid simulation by correctly updating the fluid cells at the domain's boundary.
Since the neighboring values are now available, we then update the outermost layer of cells in the \gls{psm} outer kernel.
The last step on the \gls{gpu} is the coupling from the fluid to the particles.

\subsubsection{Coupling from the fluid to the particles}
Finally, the \gls{gpu} reduces the forces and torques exerted by the fluid on the particles $\boldsymbol{F}_{\text{p},i}^{\text{fp}}$ and $\boldsymbol{T}_{\text{p},i}^{\text{fp}}$ (see \cref{hydrodynamic_force,hydrodynamic_torque}).
Then, $\boldsymbol{F}_{\text{p},i}^{\text{fp}}$ and $\boldsymbol{T}_{\text{p},i}^{\text{fp}}$ are transfered from the \gls{gpu} to the \gls{cpu} to be available for the upcoming \gls{dem} simulation on the \gls{cpu}. 
A single particle may overlap with cells located on multiple blocks. Thus, multiple \gls{gpu}s may have computed $\boldsymbol{F}_{\text{p},i}^{\text{fp}}$ and $\boldsymbol{T}_{\text{p},i}^{\text{fp}}$ for the same particle $i$. Therefore, the corresponding \gls{cpu}s have to reduce these force and torque contributions exerted by the coupling on the particles into a single variable (\gls{cpu}-\gls{cpu} communication).
The time loop continues with the \gls{pd} on the \gls{cpu}.

\subsection{Particle dynamics on the CPU}
The first step of the \gls{pd} simulation on the \gls{cpu} is the pre-force integration of the velocities to update the particle positions (see \cref{pre_force_integration}). The latter particle movement requires synchronization between the \gls{cpu}s to account for the position update, which potentially moves particles from one block to another, making other \gls{cpu}s responsible for the particles. Computing particle-particle interactions by iterating over all particle pairs can quickly become very expensive due to its $O(n^2)$ complexity. Therefore, we insert the particles into linked cells such that iterating over the particle pairs is limited to neighboring linked cells. The linked cells have a size of 1.01 times the particle diameter. % Warum sollte das zu klein sein?
Next, the lubrication correction routine is applied to all particle pairs with particles close to each other but not yet in contact (see \cref{lubrication_correction}).
The particle-particle interactions are modeled using the \gls{dem} kernel, which exerts forces and torques on overlapping particles.
The \gls{dem} kernel needs history information from the previous time step, i.e., the accumulated tangential motion between the two colliding particles (see \cref{history_information}). Since a different process may have handled the previous collision, reducing the collision histories between the \gls{cpu}s is necessary.
Then, the hydrodynamic forces and torques from the coupling, and the gravitational force are added to the total force.
Since different processes may have added forces and torques to the same particle, those contributions have to be reduced into one process (another \gls{cpu}-\gls{cpu} communication).
Then, the post-integration is applied to update the particle velocities (see \cref{post_force_integration}). Here, communication can be omitted because the velocities are unused until the subsequent communication in the upcoming sub-cycle. 
Typically, $N$ particle sub-cycles are performed per time step since the \gls{dem} requires a finer resolution in time than the \gls{lbm} for an accurate contact representation~\cite{rettingerEfficientFourwayCoupled2022}.
After completing N sub-cycles, the next time step starts with the fluid dynamics on the \gls{gpu}.
