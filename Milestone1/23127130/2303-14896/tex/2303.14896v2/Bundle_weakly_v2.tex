\documentclass{article}

% \documentclass[smallextended,referee,envcountsect]{svjour3} 
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{latexsym}
 \usepackage[colorlinks]{hyperref}
\usepackage{tablefootnote}
\usepackage{algpseudocode}
\usepackage{enumerate}
\usepackage{algorithm}
\newcommand{\red}[1]{\textcolor{red}{#1}} 
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand*{\QEDA}{\hfill\hbox{\vrule width1.0ex height1.0ex}}
\newcommand*{\E}{\mathbb{E}}
\usepackage{multirow}
\include{def}


\begin{document}
\title{Proximal bundle methods
	for hybrid weakly convex composite optimization problems}
\author{
		Jiaming Liang \thanks{Goergen Institute for Data Science and Department of Computer Science, University of Rochester, Rochester, NY 14620 (email: {\tt jiaming.liang@rochester.edu}). }\qquad 
        Renato D.C. Monteiro \thanks{School of Industrial and Systems
		Engineering, Georgia Institute of
		Technology, Atlanta, GA, 30332-0205.
		(email: {\tt renato.monteiro@isye.gatech.edu} and {\tt hzhang906@gatech.edu}). This work
			was partially supported by AFORS Grant FA9550-22-1-0088.}\qquad 
	Honghao Zhang \footnotemark[2]
	}
	\date{March 26, 2023 (first revision: December 26, 2024)}
% \newcounter{savecntr}% Save footnote counter
% \newcounter{restorecntr}% Restore footnote counter
% \author{Gopal \setcounter{savecntr}{\value{footnote}}\thanks{XYZ University} ,
% Someone else \thanks{ABC University} ,
% Another \setcounter{restorecntr}{\value{footnote}}%
%   \setcounter{footnote}{\value{savecntr}}\footnotemark% Print footnotemark
%   \setcounter{footnote}{\value{restorecntr}}}

  
	\date{March 26, 2023 (first revision: December 26, 2024)}
\maketitle


\begin{abstract}
       This paper establishes the iteration-complexity of proximal bundle methods for solving
 hybrid (i.e., a blend of smooth and nonsmooth) weakly convex composite optimization (HWC-CO) problems.
 This is done in a unified manner by considering a proximal bundle framework (PBF) based on a generic bundle update framework which includes various well-known bundle update schemes.
In contrast to
hard-to-check stationary conditions 
 (e.g., the Moreau  stationarity)
 used by other methods
for solving HWC-CO, PBF uses a stationarity measure
% in the context of HWC-CO
which is easily verifiable.
% Finally, computational results comparing PBF to a well-known subgradient method  show that it can solve many HWC-CO instances in substantially less CPU time than the latter one.
   % For example, in less than 20 minutes, \ourmethod can solve a maximum stable set SDP instance with dimension pair $(n,m)\approx (10^6,10^7)$ within 
   % $10^{-5}$ relative precision.
% \red{Instead of using
%  other well-known stationary conditions 
%  (e.g., the Moreau  stationary one)
% % in the context of
% % HWC-CO,
% PBF uses a new stationary measure which is a direct extension of $\varepsilon$ subdifferentail for convex function 
%  in the context of HWC-CO. The new stationary measure is directly verifiable and,
% at the same time, implies any of the former ones.}
 
 
       
       % a proximal bundle framework (PBF) for hybrid weakly convex composite optimization (HWC-CO) problem. We obtain the first global complexity result of proximal bundle method for HWC-CO. A major advantage of our analysis is that it bases on a newly defined convergence measure which is the first checkable notion of approximate  stationarity in the literature.
       
		{\bf Key words.} hybrid weakly convex composite optimization, iteration-complexity, proximal bundle method, regularized stationary point, Moreau envelope. 
		\\
		
		% 	{\bf Mathematics Subject Classification (2010)} 
		% 	49M37 $\cdot$ 65K05 $\cdot$ 68Q25 $\cdot$ 90C25 $\cdot$ 90C30 $\cdot$ 90C60
		{\bf AMS subject classifications.} 
		49M37, 65K05, 68Q25, 90C25, 90C30, 90C60
	\end{abstract}
	 
\section{Introduction}
	Let  $h: \R^{n} \rightarrow \R\cup \{ +\infty \} $ be a proper lower semi-continuous convex function
 and $ f: \R^{n} \rightarrow \R\cup \{ +\infty \} $ be a proper lower semi-continuous $m$-weakly convex function (i.e.,
 $f+(m/2)\|\cdot\|^2$
 is convex)
 such that
	$ \dom f \supseteq \dom h$  and
	consider
	the composite optimization (CO) problem
	\begin{equation}\label{eq:opt_problem}
	\min \left\{\phi(x):=f(x)+h(x): x \in \R^n\right\}.
	\end{equation}
	It is said that \eqref{eq:opt_problem} is a hybrid weakly convex CO (HWC-CO) problem if there exist
	nonnegative scalars $M$ and $L$
	and a
	first-order oracle $f':\dom h \to \R^n$
	(i.e., $f'(x)\in \partial f(x)$
	for every $x \in \dom h$)
	satisfying the $(M,L)$-hybrid condition,
	namely:
	$\|f'(u)-f'(v)\| \le 2M + L \|u-v\|$ for every $u,v \in \dom h$.
 This problem class includes the class of weakly convex
 non-smooth (resp., smooth) CO problems, i.e., the one with $L=0$
 (resp., $M=0$).

 This problem class appears in various applications in modern data science where $f$ is usually a loss function and $h$ is either the indicator function of some set (e.g., the set of points satisfying some functional constraints) or a regularization function that imposes sparsity or some special structure on the solution being sought.
Examples of such applications are robust phase retrieval, covariance matrix estimation, and sparse dictionary learning
(see Subsection 2.1 of \cite{davis2019stochastic} and the references therein).

 The main goal of this paper is to study the complexity of
  a unified framework (referred to as PBF) of  
 proximal bundle (PB) methods for solving the HWC-CO problem \eqref{eq:opt_problem}
 based on a generic bundle update framework
 which contains different well-known
 update schemes
 as special cases.
More specifically,
like 
 other proximal bundle methods, a PBF iteration solves a prox bundle subproblem
	of the form
 \begin{equation}\label{eq:x-pre}
	    x = \underset{u\in \R^n}\argmin \left \{\Gamma (u) + \frac{1}{2\lam} \|u-x^c\|^2 \right\}
	\end{equation}
	where $\lam$ is the prox stepsize,
	$x^c$ is
 the current
	prox-center,
 and (usually simple) lower semi-continuous convex function $\Gamma$ denotes  the current bundle function.
	Moreover,
 the bundle function is updated in every iteration but the prox center $x^c$ is updated in some
 (i.e., serious)
 iterations and left the same
 in the other (i.e., null) ones.
 But instead of choosing the bundle function underneath $\phi$ as in the proximal bundle  methods for solving the convex version of \eqref{eq:opt_problem},
 PBF uses the idea of choosing $\Gamma$ underneath $\phi(\cdot)+(m/2)\|\cdot-x^c\|^2$.
 An interesting
 feature of
 PBF is that it terminates based on
 an easily verifiable stopping criterion. 
 % which is
 % novel in the context of HWC-CO.

 % One of the difficulties for weakly convex nonsmooth optimization is a continuous measure to monitor the  progress for the algorithm. In order to address this difficulty, \cite{davis2019stochastic} proposes to use the Moreau envelope as the desired measure (see Section 1 in \cite{davis2019stochastic} and Subsection \ref{sec:notion} below). However, one drawback of the Moreau envelope is that it is not checkable in general because the computation of the proximal point is very expensive. Therefore, any algorithm based on the Moreau envelope as the convergence measure can not be stopped efficiently. More details are discussed in Subsection \ref{SEC:SCS}. A natural open question is that whether there is a checkable convergence measure for weakly convex nonsmooth optimization problem?
 
 % In this paper, we answer this question with a new notion of approximate point and obtain the first global complexity result based on this new convergence measure. 

 
 {\it Literature Review:}
 % Proximal bundle methods are efficient algorithms for solving convex nonsmooth optimization problems in practice.
 This paragraph discusses 
 the development of
 proximal bundle methods
 in the context of
 convex CO problems.
They have been proposed  in  \cite{lemarechal1975extension,wolfe1975method}, and then further studied for example in \cite{frangioni2002generalized,mifflin1982modification,de2014convex}. Their convergence analyses for nonsmooth (i.e., $L=0$ and $M>0$) CO problems are broadly discussed for example in the textbooks \cite{ruszczynski2011nonlinear,urruty1996convex}. Moreover, their complexity
analyses are studied
for example in \cite{diaz2023optimal,du2017rate,kiwiel2000efficiency,liang2021proximal,liang2024unified}.

We now discuss the development of PB methods
in the context of weakly convex CO problems as mentioned at the beginning of this introduction but with $L=0$.
We start with papers
that deal only with their asymptotic convergence.
PB extensions to this context can be found in
\cite{fuduli2004minimizing,hare2009computing,hare2010redistributed,hare2016proximal,kiwiel2006methods,makela1992nonsmooth,noll2008proximity,vlvcek2001globally}.
In particular, papers
\cite{hare2009computing,hare2010redistributed,hare2016proximal}
already considered the idea of constructing 
convex bundle
(more precisely, cutting-plane) models
underneath the regularized function $\phi(\cdot)+(m/2)\|\cdot-x^c\|^2$,
which, as already mentioned above, is the one adopted in this paper. 
% ???\textbf{difference}
% a multicut cutting 
% propose 
% To extend the bundle method for weakly convex optimization problem, one needs to tackle the difficulty that the linearization of a weakly convex function $\phi$ is not necessarily underneath $\phi$ itself. \cite{kiwiel2006methods,vlvcek2001globally,makela1992nonsmooth,fuduli2004minimizing,noll2008proximity} addressed this issue by downshifting the linearization error and \cite{hare2010redistributed,hare2016proximal,hare2009computing} handle weak convexity using redistributed models which not only downshift the linearization error but also tilt the slopes. This paper follows a similar idea to the first kind but differs in the way the strategy for choosing a serious index.
% All the papers about bundle methods for weakly convex nonsmooth optimization problems mentioned above do not contain the analysis for the iteration complexity. 
The more recent paper \cite{atenas2023unified} proposes a model to analyze descent-type bundle methods and
establishes local convergence rate for the (serious) iteration sequence  as well as function value sequence under some strong stationary growth condition.
None of the aforementioned papers establish (either serious or overall) iteration complexities for their
methods.

 % However, it neglects the complexity for null indices and assumes the error bound condition. In this paper, we will take the null indices into consideration and establish the first global complexity result without the error bound condition.

Iteration-complexities for
stochastic proximal point and proximal subgradient methods
are obtained in
\cite{davis2019stochastic}
using the Moreau envelope as
an optimality measure.
Moreover, \cite{lin2020near,2019Nouiehed_Lee,ostrovskii2021efficient,rafique2022weakly,thekumparampil2019efficient} establish iteration-complexities for different types of algorithms for solving the special case where
$f(x)= \max_{y \in Y} \Phi(x,y)$,
function $x \to \Phi(x,y)$ is
weakly convex and differentiable
for every $y \in Y$,
and $y \to \Phi(x,y)$
is concave for every
$x$.
% is the maximum of a family of
% weakly convex differentiable functions
% $\{\Phi(\cdot,y)\}_y$
% such that $y \to \Phi(\cdot,y)$ is concave.


% Other methods for weakly convex nonsmooth optimization are \cite{davis2019stochastic} ??????


{\it Main contribution.}
As already mentioned above, this paper establishes the (serious and overall) iteration-complexity of PB methods for solving the HWC-CO problem described at the beginning of this introduction. This is done in a unified manner by considering a generic bundle update framework. An interesting feature of our analysis is that it is based on a stationarity measure
% in the context of HWC-CO
which, in contrast to other well-known stationary conditions
in the context of
HWC-CO (including the Moreau stationary one), is easily and directly verifiable. Moreover, by proper choice of tolerances, it is shown that the new measure
implies these
other
well-known near stationarity conditions (including the Moreau stationary one), so that any complexity result
based on the first one can be easily translated to the latter ones.

As a consequence of our analysis,
it is shown that the iteration-complexity for PBF to find a $\rho$-Moreau stationary point
of $\phi$ 
% {\bf Definition????}
is similar to that of the deterministic versions of
the stochastic proximal subgradient (PS) methods studied 
in \cite{davis2019stochastic,davis2019proximally},
i.e., ${\cal O}(\rho^{-4})$,
 but the first complexity bound has the  advantage that its constant (in its ${\cal O}(\cdot)$) is never worse and is generally better than the one which appears in the bound for the PS method
of \cite{davis2019stochastic}.
The latter feature of PBF is due to its
bundle nature which allows it to
use a considerably  larger prox stepsize that is determined by
the weakly convex parameter.
This contrasts with the nature of proximal subgradient-type methods which
use relatively small prox stepsizes
(e.g., depending on a pre-specified iteration count or decreasing towards zero as the latter grows).

Other advantages of our method are:
1) PBF assumes that $f$ satisfies the more general hybrid condition (both
\cite{davis2019stochastic,davis2019proximally} only consider the case where $L=0$);
2) as opposed to the methods of \cite{davis2019stochastic,davis2019proximally},
 PBF does not require knowledge of
the constants $M$ and $L$ (see the first paragraph above) which are usually
hard to estimate; but as the first two methods, PBF still requires knowledge of a weakly convex parameter $m$;
and 3)
% as opposed to Algorithm 2 of \cite{davis2019proximally}, 
it can obtain an estimate of the size of the gradient of the Moreau envelope whenever  the prox subproblem is solved according to an easily verifiable stopping criterion;
in contrast, 
Algorithm 2 of \cite{davis2019proximally} needs to
perform a pre-specified number of
subgradient iterations to the prox subproblem
(see Section 3 of \cite{davis2019proximally}) to estimate the
size of the gradient of the Moreau envelope.

% by simply obtaining a without performing a pre-specified number of inner number to the obtain an approximation solution of the prox subproblem.


% naturally minimized is better than the latter in terms of the
% constant
% can be found within , i.e.,
% a point $x$ such that
% $\can be obtained
% in ${\cal O}(\delta^{-4})$

% of 
% More specifically, it is shown
% that every iteration of a
% PBF variant generates a pair $(x,w)$ of vectors and a scalar $\varepsilon$ such that
% $w \in \partial_{\varepsilon}(\cdot;x^c)(x)$ and the stationary 
% It is shown in this paper that
% any instance of PBF finds a

% \begin{itemize}
%     \item We propose a new notion of stationary that is checkable and can used for other algorithms as a convergence measure for weakly convex nonsmooth optimization problem.
%     \item Based on the new convergence measure, we obtain the first global complexity result of proximal bundle method for HWC-CO which includes weakly convex nonsmooth problem as a special case under a very weak condition. We also show that our complexity for PBF is better than the  complexity for stochastic composite subgradient method proposed in \cite{davis2019stochastic}. 
% \end{itemize}


{\it Organization of the paper.}
Subsection~\ref{subsec:DefNot}  presents basic definitions and notation used throughout the paper.
Section~\ref{Sec:background} contains two subsections. Subsection~\ref{sec:cha} provides a characterization of the subdifferential for a weakly convex function. Subsection~\ref{sec:notion} introduces three different notions of approximate stationary points and discusses their relationship. 
Section~\ref{Sec:PBF} contains four subsections. Subsection~\ref{sec:assumptions} formally
describes problem \eqref{eq:opt_problem} and the assumptions made on it. Subsection~\ref{SEC:SCS}
reviews
the deterministic version of the stochastic composite subgradient method of \cite{davis2019stochastic}. Subsection~\ref{sec:BUF} presents a generic bundle update framework. Subsection~\ref{subsec:update} describes the PBF framework and states the iteration-complexity result of PBF. Section~\ref{sec:proof} contains three subsections. Subsection \ref{subsec:length} provides a preliminary bound on the length of a cycle in PBF and Subsection \ref{subsec:cycle} bounds the number of cycles generated by PBF. Subsection \ref{subsec:proof} presents the proof of the main complexity result.
  Section \ref{num} contains two subsections. Subsection \ref{pr} presents computational results comparing PBF against the PS method for the phase retrieval problem. Subsection \ref{BD} showcases  computational results comparing PBF against the PS method for the blind deconvolution problem.
Section~\ref{sec:conclusion} gives some concluding remarks and potential directions for future research.
Appendix~\ref{APP:sub} describes two useful technical results about subdifferentials.
Appendix~\ref{App:relation} provides proofs for the results in Subsection~\ref{sec:notion}. Appendix~\ref{IRRF} describes one important result  about recursive formula.
Appendix~\ref{APP:analysis} presents some useful technical results used in Section~\ref{sec:proof}.
Finally, Appendix~\ref{sec:termination} presents a complementary convergence guarantee in terms of the Moreau envelope using PBF.


% =====================================================\\
% For any function $\phi: \mathbb{R}^d \rightarrow \mathbb{R} \cup\{\infty\}$ and $\mu>0$, the Moreau envelope and the proximal map are defined as
% \begin{equation}\label{def:Mor}
%     M_\phi^{\mu} (x) = \min_u {\phi(u) + \frac{1}{2\mu}\|u-x\|^2}.
% \end{equation}
% \[
% \hat M^{\lam}(x) :=
% % M_\phi^{(\lam^{-1}+m)^{-1}} (x)
% % = 
% \min_u \left\{ \phi(u) + \frac{\lam^{-1}+ m}{2 }\|u-x\|^2 \right\}.
% \]
% \[
% p^\lam_\phi(x) = \argmin_u \left\{ \phi(u) + \frac{\lam^{-1}+ m}{2 }\|u-x\|^2 \right\}.
% \]

% \[
% \mu= \frac{\lam}{1+\lam m}
% \]
% \begin{equation}
%     \operatorname{prox}_{\mu \phi}(x):=\underset{x}
%     {\operatorname{argmin}}\left\{\phi(u)+\frac{1}{2 \mu}\|u-x\|^2\right\}
% \end{equation}
% Assume $\phi$ is $m$-weakly convex, then for $\mu < m^{-1}$,  $\phi_{2m}$ is continuously
% differentiable with gradient
% given by
% \begin{equation}\label{eq:grad_M}
% \nabla M_\phi^{\mu}(x)=\mu^{-1}\left(x-\operatorname{prox}_{\mu \phi}(x)\right).
% \end{equation}
% Denote $ \hat{x}:=\operatorname{prox}_{\mu \phi}(x)$. The following relationship is presented in \cite{davis2019stochastic} which captures the importance of Moreau envelope:
% \begin{equation}\label{eq:keyrela}
% \left\{\begin{array}{cl}
% \|\hat{x}-x\| & =\mu \left\|\nabla M_\phi^{\mu}(x)\right\|, \\
% \phi(\hat{x}) & \leq \phi(x), \\
% \operatorname{dist}(0 ; \partial \phi(\hat{x})) & \leq\left\|\nabla M_\phi^{\mu}(x)\right\| .
% \end{array}\right.
% \end{equation}
%  Davis then claim to use the $\|\nabla M_\phi^{\mu}(x)\|$ as the convergence measure for weakly convex nonsmooth problem. The next result shows how we can translate our convergence measure in the language of Moreau envelope.

%  ------------------------------------
% 	For the sake of shortness, we denote the class of functions $\Gamma^+$ satisfying
% 	the above conditions as  ${\cal C}_\phi(\Gamma,x^c,\lambda,\sigma)$.
% 	Clearly, the above update scheme does not completely determine $\Gamma^+$
% 	but rather gives minimal conditions on it which are suitable for the complexity analysis
% 	of this paper.
   

    \subsection{Basic definitions and notation} \label{subsec:DefNot}

    The sets of real numbers and positive real numbers are denoted by $\R$ and $\R_{++}$, respectively. 
    % Let $\R$ denote the set of real numbers.
    % Let $ \R_+ $ and $ \R_{++} $ denote the set of non-negative real numbers and the set of positive real numbers, respectively.
	Let $\R^n$ denote the standard $n$-dimensional Euclidean space equipped with  inner product and norm denoted by $\left\langle \cdot,\cdot\right\rangle $
	and $\|\cdot\|$, respectively. 
% 	Given a set $ S\subset \R^n $, its linear (resp., convex) hull is
% 	denoted by $\text{Lin} \, S $ (resp., ${\rm conv} \, S$).
	Let $\log(\cdot)$ denote the natural logarithm and $\log^+(\cdot)$ denote $\max\{\log(\cdot),0\}$. Let ${\cal O}$ denote the standard big-O notation and $ {\cal O}_1(\cdot)$ denote ${\cal O}(\cdot +1)$.
	
	
	For a given function $\varphi: \R^n\rightarrow (-\infty,+\infty]$, let $\dom \varphi:=\{x \in \R^n: \varphi (x) <\infty\}$ denote the effective domain of $\varphi$ 
	and $\varphi$ is proper if $\dom \varphi \ne \emptyset$.
	A proper function $\varphi: \R^n\rightarrow (-\infty,+\infty]$ is $\mu$-convex for some $\mu \ge 0$ if
	$$
	\varphi(\lam x+(1-\lam) y)\leq \lam \varphi(x)+(1-\lam)\varphi(y) - \frac{\mu}{2}\|x-y\|^2
	$$
	for every $x, y \in \dom \varphi$ and $\lam \in [0,1]$.
 Denote the set of all proper lower semicontinuous convex functions by $\bConv{n}$.
 % When $\mu=0$, we simply denote
 %    $\mConv{n}$ by $\bConv{n}$.
	For $\varepsilon \ge 0$, the \emph{$\varepsilon$-subdifferential} of $ \varphi $ at $x \in \dom \varphi$ is denoted by
	\beq \label{def:subdif}
 \partial_\varepsilon \varphi (x):=\left\{ s \in\R^n: \varphi(y)\geq \varphi(x)+\left\langle s,y-x\right\rangle -\varepsilon, \forall y\in\R^n\right\}.
        \eeq
	For simplicity, the subdifferential of $\varphi$ at $x \in \dom \varphi$, i.e., $\partial_0 \varphi(x)$, is denoted by $\partial \varphi (x)$.
         The set of proper closed convex functions $\Gamma$ such that $\Gamma \le \phi$ is denoted 
 by ${\cal B}(\phi)$ and any such $\Gamma$ is called a bundle for $\phi$.
          

    \section{Basic definitions and background}
    \label{Sec:background}

    This section contains two
    subsections.
    The first one gives the definition of the
subdifferential and the directional derivative of a general
closed function. It also 
relates these two concepts and
provides a characterization of
the first one for the case of
a closed $m$-weakly convex function.
The second subsection introduces the notion of a regularized stationary point which is sought by the main algorithm of this paper. It also introduces two different stationary conditions
% to another notion of approximate
% stationary point 
in terms of the directional derivative and the Moreau envelope, respectively, and provides the relationship between these three different notions.






\subsection{Characterization of the subdifferential  of weakly convex function}
\label{sec:cha}

% This section discusses the definition of subdifferential for weakly convex function and its various properties.

We start by giving the definitions of
directional derivative
of a closed  function.

\begin{definition}
The directional derivative $\phi'(x;d)$
of
$\phi$ at $x$ along $d$ is
\[
\phi'(x;d) := \liminf _{t \downarrow 0} \frac{\phi(x+td)-\phi(x)}{t}.
\]
\end{definition}

% We start by giving the definition of
% the
% $\varepsilon$-subdifferential
% of a proper closed function.

% In \cite{kruger2003frechet}, the Frechet $\varepsilon$-subdifferential is defined in the following way:
The next definition for $\varepsilon$-subdifferential can be found 
in  Definition 1.10 of \cite{kruger2003frechet}.

\begin{definition}
% [Frechet $\varepsilon$-subdifferential]
The Frechet subdifferential of a proper closed function
$\phi: \mathbb{R}^{n} \rightarrow \mathbb{R} \cup\{\infty\}$ is defined as 
\[
 \partial \phi(x)=\left\{v \in \R^n: \liminf _{y \rightarrow x} \frac{\phi(y)-\phi(x)-\left\langle v, y-x\right\rangle}{\|y-x\|} \geq 0 
\quad \forall x \in \R^n \right\}.
\]
\label{def:Fre_epsilon_subdifferential}
% When $\varepsilon=0$, we denote
% $\partial_{0} \phi(x)$ simply by $\partial \phi(x)$.
\end{definition}



% The above definition 
% of $\partial_\varepsilon \phi(x)$
% % of the Frechet subdifferential $\partial h(x)$
% can be found for example in Definition 1.10 in \cite{kruger2003frechet}
% and
% % In particular, the definition for  $\partial \phi(\cdot)$ is given 
% in section 2.2 of Davis \cite{davis2019stochastic}.
% , namely:
% $$
% \partial \phi(x)=\left\{v \in \mathbb{R}^{n} :  \phi(y) \geq \phi(x)+\langle v, y-x\rangle+o(\|y-x\|), \, \forall y \in \R^n \right\}.
% $$

% \begin{lemma} {\bf Move}
% \label{def: Frechet Subdifferential}
%  Let $h: \mathbb{R}^{n} \rightarrow \mathbb{R} \cup\{\infty\}$ be a proper closed function. At any point $x \in \operatorname{dom} \phi$, we have
% $$
% \partial h(x)=\left\{v \in \mathbb{R}^{n} \mid\left( \forall y \in \mathbb{R}^{n}\right) h(y) \geq \phi(x)+\langle v, y-x\rangle+o(\|y-x\|)\right\}.
% $$
% \end{lemma}

% \begin{proof}
% On one hand, if $v \in \partial \phi(x)$, then $\forall y \in \R^n$
% \[
%   h(y) \ge \phi(x) + \inner{v}{y-x} + \min\{0,\phi(y)-\phi(x)-\inner{v} { y-x}\}
% \]
% Then the fact that $\liminf_{y \rightarrow x} \frac{\phi(y)-\phi(x)-\left\langle v, y-x\right\rangle}{\|y-x\|} \geq 0$ suggests \[\min\{0,\phi(y)-\phi(x)-\inner{v} { y-x}\} = o(\|y-x\|)\]
% We then conclude
% \[
% \partial \phi(x) \subset \left\{v \in \mathbb{R}^{n} \mid\left( \forall y \in \mathbb{R}^{n}\right) \phi(y) \geq \phi(x)+\langle v, y-x\rangle+o(\|y-x\|)\right\}.
% \]
% On the other hand, if $v \in \left\{v \in \mathbb{R}^{n} \mid\left( \forall y \in \mathbb{R}^{n}\right) \phi(y) \geq \phi(x)+\langle v, y-x\rangle+o(\|y-x\|)\right\}$, then 
% \[
% \liminf _{y \rightarrow x} \frac{\phi(y)-\phi(x)-\left\langle v, y-x\right\rangle}{\|y-x\|} \geq \liminf _{y \rightarrow x}\frac{o(\|y-x\|)}{\|y-x\|} = 0
% \]
% Thus the statement follows.
% \end{proof}

% --------------------


% \[
% 0 \in \partial \phi(x)
% \]
% \[
% \inf_{\|d\| \le 1}
% \phi'(x;d) \ge 0
% \]
% \[
% \hat M^{\lam}(x) =0
% \]


% ----------------------


The following result gives a relationship between
the above two concepts.
\begin{lemma}
\label{lem:directional_deri}
 For any $v \in  \partial \phi(x)$ we have
\begin{equation}
    \operatorname{inf}_{\|d\| \le 1} \phi'(x;d) \ge -\|v\| .
    \label{eq:dire_subdif}
\end{equation}

\end{lemma}
\begin{proof}
To prove \eqref{eq:dire_subdif}, it suffices to show
if $v  \in  \partial \phi(x)$, then
$\phi'(x;d) \ge -\|v\|$ for every $d \in \R^n$ such that $\|d\|\le 1$.
Indeed, we may assume that $d \ne 0$
since the case where $d=0$ is trivial.
Then, it follows from the Definition \ref{def:Fre_epsilon_subdifferential} and the  Cauchy-Schwarz inequality that
\begin{align*}
   0 &\le 
   \liminf _{t \downarrow 0} \frac{\phi(x+td)-\phi(x)- t \left\langle v, d\right\rangle}{t\|d\|} =
   - \frac{\inner{v}{d}}{\|d\|} + \liminf _{t \downarrow 0} \frac{\phi(x+td)-\phi(x)}{t\|d\|} \\ 
   &= - \frac{\inner{v}{d}}{\|d\|} + \frac{1}{\|d\|} \phi'(x;d) \le \|v\| + \frac{1}{\|d\|} \phi'(x;d),
\end{align*}
and hence that 
$\phi'(x;d) \ge -\|v\|$ for every $d \in \R^n$ such that $\|d\|\le 1$.
\end{proof}
% \begin{lemma}
% \label{lem:directional_deri}
%  For any $v \in  \partial_\varepsilon \phi(x)$ we have
% \begin{equation}
%     \operatorname{inf}_{\|d\| \le 1} \phi'(x;d) \ge -\|v\| - \varepsilon.
%     \label{eq:dire_subdif}
% \end{equation}

% \end{lemma}
% \begin{proof}
% To prove \eqref{eq:dire_subdif}, it suffices to show
% if $v  \in  \partial_\varepsilon \phi(x)$, then
% $\phi'(x;d) \ge -\|v\|- \varepsilon$ for every $d \in \R^n$ such that $\|d\|\le 1$.
% Indeed, we may assume that $d \ne 0$
% since the case where $d=0$ is trivial.
% Then, it follows from the Definition \ref{def:Fre_epsilon_subdifferential} and the  Cauchy-Schwarz inequality that
% \begin{align*}
%    -\varepsilon &\le 
%    \liminf _{t \downarrow 0} \frac{\phi(x+td)-\phi(x)- t \left\langle v, d\right\rangle}{t\|d\|} =
%    - \frac{\inner{v}{d}}{\|d\|} + \liminf _{t \downarrow 0} \frac{\phi(x+td)-\phi(x)}{t\|d\|} \\ 
%    &= - \frac{\inner{v}{d}}{\|d\|} + \frac{1}{\|d\|} \phi'(x;d) \le \|v\| + \frac{1}{\|d\|} \phi'(x;d),
% \end{align*}
% and hence that 
% $\phi'(x;d) \ge -\|v\|- \varepsilon$ for every $d \in \R^n$ such that $\|d\|\le 1$.
% \end{proof}



Before stating the next result, we introduce the  following notation which is used not only here but also throughout the paper:
    for every function $g(\cdot)$, $m \in \R_+$, $z \in \R^n$,
    let 
    \begin{equation}
   g_m(\cdot;z) := g (\cdot) + \frac{m}{2}\|\cdot - z\|^2.
    \label{def:phim}
    \end{equation}
The following result  provides a characterization of the
Frechet subdifferential 
for a weakly convex function. 


\begin{proposition}\label{prop:Fre_sub}
\label{prop:Frechet relation}
Assume that $\phi: \mathbb{R}^{
n} \rightarrow \mathbb{R} \cup\{\infty\}$ is a closed $m$-weakly convex function. Then  we have
 \begin{align}
    \partial \phi(x) 
 &=\left\{ v \in \R^n :  \phi(y) \geq \phi(x)+\langle v, y-x\rangle-\frac{ m}{2}\|y-x\|^{2}, \ \forall y \in \R^n \right\} \label{eq:key_chara} \\
 &=
 \partial \left[\phi_{ m}(\cdot;x)
 \right](x)  \label{eq:key_chara1}.
 \end{align}
\end{proposition}
\begin{proof}
The proof of \eqref{eq:key_chara} can be found for example in Lemma 2.1 of \cite{davis2019stochastic}.
Moreover, it follows from the definition of the subdifferential in \eqref{def:subdif} with $\varepsilon = 0$ and the definition of $\phi_m(\cdot;x)$ in \eqref{def:phim} that \eqref{eq:key_chara1} is equivalent to \eqref{eq:key_chara}.
\end{proof}


\subsection{Notions of approximate stationary points}
\label{sec:notion}
% In this section, we define formally our new definition of approximate solution.

This subsection introduces three notions of stationary points, including the one adopted by the main algorithm of this paper.
It also describes how these notions are related to one another.

We start by introducing the
definition of a regularized
stationary point of a
closed $m$-weakly convex function
$\phi$ which is the one
used by the main algorithm of this paper.
% This notion of regularized
% approximate stationary point generalizes the prox gradient mapping for analyzing the proximal method in a significant way so that it can be applied to analyze any algorithm for weakly convex nonsmooth problem.

% Assume for this subsection that
% $\phi$ is a
% closed $m$-weakly convex function.

\begin{definition}[Regularized stationary point]
\label{def:appr}
For a pair $(\bar\eta,\bar \varepsilon) \in \R^2_{++} $, a point $x\in \dom \phi$ is called a $(\bar \eta,\bar\varepsilon;m)$-regularized stationary point  of $\phi$
% problem \eqref{eq:opt_problem} 
if there exists a pair $(w,\varepsilon) \in \R^n \times \R_{++}$ such that 
\begin{equation}
  w \in  \partial_\varepsilon \left[\phi_{m}(\cdot;x) \right]  (x), \quad
\|w\| \le \bar \eta , \quad \ \varepsilon \le \bar \varepsilon.
\label{eq:app_sol}
\end{equation}
\end{definition}

We now make two trivial remarks about the above definition. First,
if $(\bar \eta,\bar \varepsilon)=(0,0)$ then
it follows from 
\eqref{eq:key_chara1}
and Definition \ref{def:appr}
that
$x$ is a  $(\bar \eta,\bar \varepsilon;m)$-regularized stationary point if and only if
$x$ is an exact stationary point of \eqref{eq:opt_problem}, i.e.,
it satisfies $0 \in \partial \phi(x)$.
Second, when $m=0$ (and hence $\phi$ is convex),
the inclusion in
\eqref{eq:app_sol} reduces to $w \in \partial_{ \varepsilon} \phi(x)$, and the above notion
reduces to a familiar one which has already been used in the analysis of several
algorithms, including proximal bundle ones (e.g., see Section 6 of \cite{liang2021proximal}), for solving the convex version of \eqref{eq:opt_problem}.

The verification that $x$ is a $(\bar \eta,\bar\varepsilon;m)$-regularized stationary point
 requires exhibiting a pair of residuals
 $(w,\varepsilon)$ satisfying the inclusion in \eqref{eq:app_sol},
 which is generally not an immediate task. However, many algorithms for solving the convex version of \eqref{eq:opt_problem} and the one in this paper, are able to generate not only a sequence of iterates $\{x_k\}$ but also a sequence of corresponding residuals
 $\{(w_k,\varepsilon_k)\}$
 such that $(x,w,\varepsilon)=(x_k,w_k,\varepsilon_k)$
 satisfies the inclusion in \eqref{eq:app_sol},
 so that verification that
 $x_k$ is a $(\bar \eta,\bar\varepsilon;m)$-regularized stationary point simply amounts to checking the two inequalities in \eqref{eq:app_sol}.

The following definition uses
directional derivatives
to define a different notion
of a stationary point.

\begin{definition}[Directional stationary point] \label{def:dsp}
    For a pair $(\varepsilon_D,\delta_D) \in \R^2_{++}$, a point $x\in \dom \phi$ is called a $(\varepsilon_D,\delta_D)$-directional stationary point if there exists $\tilde x \in \dom \phi$ such that 
    \begin{equation}\label{eq:dir}
    \|x- \tilde x\|\le \delta_D, \quad \inf_{\|d\|\le 1} \phi'(\tilde x;d) \ge -\varepsilon_D.
    \end{equation}
\end{definition}

When $(\varepsilon_D,\delta_D)=(0,0)$, then \eqref{eq:dir}
reduces to the condition that $\phi'(x;d) \ge 0$ for all $d \in \R^n$, a condition which is known to be equivalent
to $0 \in \partial \phi(x)$.
% Note that the following equality holds according to Proposition 8.32 in \cite{rockafellar2009variational}:
% \begin{equation}\label{eq:disdire}
% \operatorname{dist}(0 ; \partial \phi(\tilde x))=-\inf _{\|d\| \leq 1} \phi^{\prime}(\tilde x ; d).
% \end{equation}
% Thus a  $(\varepsilon_D,\delta_D)$-directional stationary point $\tilde x$ also satisifies
% \[
% \operatorname{dist}(0 ; \partial \phi(\tilde x)) \le \varepsilon_D.
% \]

Before stating the next notion of a stationary point based on the Moreau envelope, we introduce a slightly different
notation for the Moreau envelope which is more suitable for our presentation,
namely:
for any $\lam>0$ and $x \in \R^n$, let
\begin{equation}\label{eq:Moreau}
  \hat M^{\lam}(x) :=
\min_u \left\{ \phi(u) + \frac{\lam^{-1}+ m}{2 }\|u-x\|^2 \right\}.   
\end{equation}
Note that the above definition depends on $m$
 but, for simplicity, we have omitted this dependence from
 its notation
 since the parameter $m$ is assumed constant throughout our analysis in this paper.
 
 The  gradient formula for the Moreau envelope (see Section 1 in \cite{davis2019stochastic}) is as follows:
\begin{equation}\label{eq:new_grad}
     \nabla  \hat M^{\lam}(x) = \left(\frac{1}{\lam}+m\right) \left(x - \hat x^\lam(x) \right)
\end{equation}
where 
\begin{equation}\label{eq:hatx}
    \hat x^\lam(x) := \argmin_{u \in \R^n} \left\{\phi(u)+ \frac{\lam^{-1}+m}2\|u -  x\|^2\right\}.
\end{equation}

The following definition describes another notion of a stationary point that is based on the aforementioned Moreau envelope.
% The Moreau stationary point based on the above gradient formula for Moreau envelope is stated in the following definition:
\begin{definition}[Moreau stationary point]
    For any $\varepsilon_M >0$ and $\lam>0$, a point $x\in \dom \phi$ is called a $(\varepsilon_M;\lam)$-Moreau stationary point if 
    $\|\nabla \hat M^{\lam}(x)\| \le \varepsilon_M$.
\label{def:Moreau}
\end{definition}

% We now make some remarks about the above three notions of stationary points.
%  First, 
 
 It is worth noting that among the three notions of stationary points,
 the directional stationary one is the only one which does not depend on the weak convexity parameter $m$.
 % Second, among the above three stationary notions, only the directional one
 % imposes a condition on
 % a nearby point instead of the actual point under consideration.
 
 % imposes a condition on the actual stationary point instead of a condition on nearby points which does not require the computation of a nearby point which is generally not computable. 
 % In other words, if one can find $(w,\varepsilon)$ satisfying the inclusion in \eqref{eq:app_sol}, then the regularized stationary point becomes checkable  in the sense that the norm of $w$ and $\varepsilon$ are easy to compute. Later we will show that the termination of our algorithm actually based on this observation.

The next result,
whose proof is given in Appendix \ref{App:relation}, describes the relationship between directional stationary points and Moreau stationary points.

% The relationship of these three notions of stationary point is summarized in the following two results and the  proofs of these two results are
% in Appendix \ref{App:relation}.


\begin{proposition}
Assume $\lam >0$ and $\phi$ is a $m$-weakly convex function. Then, the following statements hold:
\begin{itemize}
    \item [a)]
    if $x$ is a $(\varepsilon_D,\delta_D)$-directional stationary point then $x$ is a $\left(\varepsilon_M;\lam \right)$-Moreau stationary point where
    \[
    \varepsilon_M = \left(m+\frac{1}{\lam}\right)\left[(3+2\lam m)\delta_D + 2\lam \varepsilon_D \right];
    \]
    \item [b)]
    if $x$ is a $(\varepsilon_M;\lam)$-Moreau stationary point then $x$ is a $(\varepsilon_M,\varepsilon_M/(m+\lam^{-1}))$-directional stationary point.
\end{itemize}
\label{prop:relation}
\end{proposition}

The following result, whose  proof is given in Appendix \ref{App:relation},
provides an equivalent characterization of
a directional stationary   point
in terms of
the subdifferential of $\phi$.

\begin{proposition}\label{prop:subdir}
Assume that $\phi$ is a $m$-weakly convex function. Then,
        $x$ is a $(\varepsilon_D,\delta_D)$-directional stationary point if and only if  there exists $\tilde x \in \dom \phi$ such that 
        \[
  \|x- \tilde x\|\le \delta_D, \quad \operatorname{dist}(0 ; \partial \phi(\tilde x)) \le \varepsilon_D.
\]
\end{proposition}

The following result, whose  proof is given in Appendix \ref{App:relation},
shows that a
regularized stationary point is
both a 
directional stationary point and a Moreau stationary point.


\begin{proposition}\label{key:rela}
If $x$ is a $(\bar \eta,\bar \varepsilon;m)$-regularized stationary point, then the following statements hold:
\begin{itemize}\label{prop:ours}
    \item[a)] $x$ is a $(\bar \eta + 2 \sqrt{2 m \bar \varepsilon }, \sqrt{2\bar \varepsilon/m})$-directional stationary point;
    \item[b)]
    $x$ is a $(18\sqrt{2m\bar \varepsilon}+4\bar \eta;1/m)$-Moreau stationary point.
\end{itemize}
    % \begin{itemize}
    %     \item[a)] 
    %     $y$ is a $(\bar \eta + 2 \sqrt{2 m \bar \varepsilon }, \sqrt{2\bar \varepsilon/m})$-directional stationary point;     
    %     \item[b)] $y$ is a $(2\sqrt{4m\bar \varepsilon+2\bar \eta^2};1/2m)$-Moreau stationary point.
    % \end{itemize}
\end{proposition}




% b) Since $x$ is a $(\bar \eta, \bar \varepsilon;m)-$ stationary point of $\phi$, there exists a pair $(w,\varepsilon) \in \R^n \times \R_{++}$
% \begin{equation}\label{eq:normw}
%   w \in  \partial_\varepsilon \left[\phi_m(\cdot;y) \right]  (y), \quad
% \|w\| \le \bar \eta , \quad \ \varepsilon \le \bar \varepsilon.
% \end{equation}
%     Denote $\hat y = \operatorname{prox}_{(1/2m)\phi}(y)$ and $\tilde y = \operatorname{prox}_{(1/2m)(\phi - \inner{w}{\cdot} )}(y)$.
% Since  $\phi_{ m}$ is
% convex in view of assumption (A1),
% we easily see that
% $\phi_{2m}$ is
% $m$-strongly convex. Then we have
% \begin{equation}
%     \phi(\tilde y) + m\|\tilde y - y\|^2 \ge \phi(\hat y) + m\|\hat y - y\|^2+ \frac{m}{2}\| \hat y- \tilde y\|^2.
% \end{equation}
% Using the fact that $\phi_{2m}-\inner{w}{\cdot}$ is also
% $m$-strongly convex, we have 
% \begin{equation}
% \phi(\hat y) - \inner{w}{\hat y} + m\|\hat y - y\|^2 \ge \phi(\tilde y) - \inner{w}{\tilde y}+ m\|\tilde y - y\|^2+ \frac{m}{2}\| \hat y- \tilde y\|^2.    
% \end{equation}
% Summing the above two inequalities and using Cauchy-Schwarz inequality we can get
% \begin{equation}
%     m \|\hat y - \tilde y\|^2 \le \inner{w}{\tilde y - \hat y} \le \|w\|\|\hat y - \tilde y\|.
% \end{equation}
% The above inequality,  \eqref{eq:grad_M} with $\mu = 1/2m$, and the inequality in \eqref{eq:nearby} then imply that
% \begin{align*}
%     \|\nabla M_\phi^{1/2m}(y)\|^2 = 4m^2\|y - \hat y\|^2 &\le 8m^2\|y - \tilde y\|^2 + 8m^2\|\tilde y - \hat y\|^2 \\
%     & \le 16m \bar \varepsilon+ 8\|w\|^2
%     \le  16m\bar \varepsilon + 8 \bar \eta^2.
% \end{align*}
% where the last inequality is due to relation \eqref{eq:normw}. Thus the statement follows.


% When $(\bar \eta,\bar \varepsilon)$ is not zero,
% the following result shows that
% any $(\bar \eta,\bar \varepsilon)$-stationary point is close to an exact stationary
% point of the perturbed function
% $\phi(\cdot) -\inner{\tilde w}{\cdot}$
% % $\{ \phi (u) - \inner{\tilde w}{u} : u \in \R^n \}$
% for some sufficiently small $\tilde w$.
% \[
% \phi(\hat x) \ge \phi(x) + \inner{v}{\hat x - x} - \frac{m}2\|\hat  x -x \|^2
% \]
% \[
% m \|\hat x - x\|^2 \le \phi(x) - \phi(\hat x) \le \inner{-v}{\hat x - x} \le \varepsilon \|\hat x -x \|
% \]
% Thus
% \[

% \]

% \begin{proposition}\label{prop:sol}
% Let pair $(\bar\eta,\bar \varepsilon) \in \R^2_{++}$
% be given,
% and define ?????
% \begin{equation}\label{def:hatdelta}
% \hat \varepsilon = \bar \eta + 2 \sqrt{2 m \bar \varepsilon }, \quad \hat \delta = \sqrt{\frac{2\bar \varepsilon}{m}},  
% \end{equation}
% Assume that
% $\phi$ is a
% closed $m$-weakly convex function and that $y$ is 
% $(\bar \eta,\bar\varepsilon;m)$-stationary point of $\phi$.
% % of \eqref{eq:opt_problem}.
% Then, 
% there exists
%     $\hat y \in \R^n$ such that
%     % the following statements hold:
%     % \item[a)]
%     \begin{equation}\label{eq:dist}
%     \|y-\hat y\| \leq \hat \delta,
%     \quad
%     \mbox{\rm dist}(0, \partial \phi( \hat y) )
%     \le \hat \varepsilon,
%     \end{equation}
%    and
%    \[
% \inf_{\|d\| \le 1} \phi'(\hat y;d) \ge
% -\hat \varepsilon.
% \]
% \end{proposition}






% we  define $\varphi(\cdot) = \phi(\cdot)- \inner{w}{\cdot}$ for a given $w$ and $\hat y = \operatorname{prox}_{\lambda \varphi}(y)$. Then from \eqref{eq:keyrela} with $\phi = \varphi$ and $\lam=1/2m$ we know that
% % \[
% % \|\hat y-y\| = \frac{\|\nabla \varphi_{1/2m}(y) \|}{2m}. \quad
% % \mbox{dist}(0, \partial \varphi( \hat y) ) \le \| \nabla \varphi_{1/2m}(y)} \|
% % \]
% % From the definition of $\varphi$ and the above relation we can deduce that
% % \[
% % \|\nabla \phi_{1/2m}(y)\| \le \|\nabla \varphi_{1/2m}(y)\|+ \|w\| = 2m \|y-\hat y\| + \|w\|
% % \]



% \begin{proposition}\label{prop:tran} ({\bf Jiaming})
%     For any $((\bar \eta, \bar \varepsilon),m)$-stationary point $x$ of $\phi$, it holds that 
%     \begin{equation}\label{eq:rela}
%     \|\nabla M^{1/2m}_{\phi}(x)\| \le 2\sqrt{4m\bar \varepsilon+2\bar \eta^2}.
%     \end{equation}
% \end{proposition}



% \subsection{ Review of Davis' method}

% This subsection reviews the
% deterministic version of
% the stochastic proximal subgradient method described
% in Algorithm \ref{alg:Davis} of
% \cite{davis2019stochastic}.

% We start by describing the aforementioned method in
% Algorithm ??? below.

% \begin{algorithm}\label{alg:Davis}
% \caption{Proximal subgradient}
% \state \textbf{Input}: $x_0 \in \dom h$, a sequence of $\left\{1+m\lam_t\right\}_{t \geq 0} \subset \mathbb{R}_{+}$ and iteration count T.\\
% \state \textbf{Step} $t = 0,1,\cdots, T$:
% \begin{equation}
% \text { Set } x_{t+1}=\operatorname{prox}_{1+m\lam_t h}\left(x_t-1+m\lam_t f'(x_t)\right)
% \end{equation}
% % \state Sample $t^* \in\{0, \ldots, T\}$ according to $\mathbb{P}\left(t^*=t\right)=\frac{1+m\lam_t}{\sum_{i=0}^T 1+m\lam_i}$.\\
% % \STATE \textbf{Return} $x_{t^*} $
% \end{algorithm}

% The following result describes the
% iteration-complexity of
% Algorithm ???.


% \begin{theorem} [Proximal subgradient method]
% Assume $(f,h)$ are functions satisfying conditions (A1)-(A3)
% with $L=0$.
% Fix a real $\bar{m} \in(m, 2 m]$ and assume that  $1+m\lam_t \in(0,1 / \bar{m}]$ for every $t \ge 0$. Then,
% % the iterates $x_t$ generated by Algorithm $3.1$ satisfy
% % $$
% % \varphi_{1 / \bar{m}}\left(x_{t+1}\right) \leq \varphi_{1 / \bar{m}}\left(x_t\right)-\frac{1+m\lam_t(\bar{m}-m)}{\bar{m}} \left\|\nabla \varphi_{1 / \bar{m}}\left(x_t\right)\right\|^2+1+m\lam_t^2 \bar{m} L^2,
% % $$
% % and 
% % the point $x_{T^*}$ returned by Algorithm $3.1$ satisfies:
% % $$
% % \mathbb{E}\left\|\nabla M_\phi^{1 / \bar{m}}\left(x_{t^*}\right)\right\|^2 \leq \frac{\bar{m}}{\bar{m}-m} \cdot \frac{\left(\phi_{1 / \bar{m}}\left(x_0\right)-\min \phi\right)+2 \bar{m} M^2 \sum_{t=0}^T 1+m\lam_t^2}{\sum_{t=0}^T 1+m\lam_t} .
% % $$
% $$
% \frac{\sum_{t=0}^T 1+m\lam_t \left\|\nabla M_\phi^{1 / \bar{m}}\left(x_t\right)\right\|^2}{\sum_{t=0}^T 1+m\lam_t}  \leq \frac{\bar{m}}{\bar{m}-m} \cdot \frac{\left(\phi_{1 / \bar{m}}\left(x_0\right)- \phi_*\right)+2 \bar{m} M^2 \sum_{t=0}^T 1+m\lam_t^2}{\sum_{t=0}^T 1+m\lam_t} .
% $$
% As a consequence, Algorithm~1 with $\bar m=2m$ and $1+m\lam_t=1/\sqrt{T+1}$ for every $t \ge 0$ generates an iterate $x_t$
% satisfying
% $\left\|\nabla M_\phi^{1 / (2m)}\left(x_t\right)\right\|^2
% \le \varepsilon$ in at most
% \begin{equation}
%     \frac{\left(\phi_{1 / 2 m}\left(x_0\right)- \phi_*\right)+4 m M^2 \gamma^2}{\gamma \varepsilon^4}.
% \end{equation}
% \end{theorem}
% % In particular, if Algorithm \ref{alg:Davis} uses the constant parameter $1+m\lam_t=\frac{\gamma}{\sqrt{T+1}}$, for some real $\gamma \in\left(0, \frac{1}{2 m}\right]$, then the point $x_{t^*}$ satisfies:
% % $$
% % \frac{\sum_{t=0}^T \left\|\nabla M_\phi^{1 / 2 m}\left(x_{t}\right)\right\|^2}{
% % T+1} \leq 2 \cdot \frac{\left(\phi_{1 / 2 m}\left(x_0\right)-\min \phi\right)+4 m M^2 \gamma^2}{\gamma \sqrt{T+1}}.
% % $$
% % Thus the complexity to generate a point such that $\|\nabla M_\phi^{1/2m}\| \le \varepsilon$ is at most 
% % \begin{equation}
% %     \frac{\left(\phi_{1 / 2 m}\left(x_0\right)-\min \phi\right)+4 m M^2 \gamma^2}{\gamma \varepsilon^4}
% % \end{equation}
% One remark is that Davis only consider the case that $f$ satisfies Assumption (A2) with $L_f = 0$ which is a more restrict function class.
% \begin{corollary}
    
% \end{corollary}

% \item[b)] there holds
% \[
% \inf_{\|d\| \le 1} \phi'(\hat y;d) \ge
% -\hat \varepsilon.
% \]
% \item[c)]
% there holds

% \end{itemize}
% \end{corollary}



% -------------------------

% Let $\lam=1/(2m)$ and $\varphi(\cdot) = \phi(\cdot)- \inner{w}{\cdot}$. Then
% \[
% \|\hat y-y\| = (1/(2m)) \|\nabla \varphi_\lam(y)} \|, \quad
% \mbox{dist}(0, \partial \varphi( \hat y) ) \le \| \nabla \varphi_\lam(y)} \|

% \]
% \[
% \mbox{dist}(0, \partial \varphi( \hat y) ) \le 2m \|y-\hat y\|
% \]
% Note that
% \[
% \mbox{dist}(0, \partial \phi( \hat y) ) \le
% \mbox{dist}(0, \partial \varphi( \hat y) ) + \|w\|
% \le 2m \|y-\hat y\| + \|w\|
% \]

% ---------------------------












\section{Algorithm}
\label{Sec:PBF}

This section contains four subsections. The first one describes the main problem and the assumptions made on it.
The second one reviews the deterministic version of stochastic composite subgradient
(SCS) method of \cite{davis2019stochastic} and its main complexity result.
The third one presents a generic bundle update framework and describes three special cases of it. The fourth one describes the  proximal bundle framework (PBF) and describes its main complexity result.

\subsection{Problem description and main assumptions}
\label{sec:assumptions}

The main problem of this paper is described in \eqref{eq:opt_problem}
where the functions $f$ and $h$ are assumed to satisfy:
%\subsubsection{Main Assumptions}
\begin{itemize}
    \item[(A1)] functions $f,h:\mathbb{R}^{n} \rightarrow \mathbb{R} \cup\{\infty\}$ and
    a scalar $m>0$ such that
    $h \in \bConv{n}$,
    $f$ is $m$-weakly convex
    and $\dom h \subset \dom f$;
		% $f_m(\cdot;0)$ and $h$ are are both in
		% $\bConv{n}$ and
		% $\dom h \subset \dom f$;
% 		\item[(A2)]
% 		the set of optimal solutions $X^*$ of
% 		problem \eqref{eq:opt_problem} is nonempty;

	\item[(A2)]
% 	$f:\mathbb{R}^{n} \rightarrow \mathbb{R} \cup\{\infty\}$ is a function such that $\dom f \supset \dom h$ and
		there exist constants
		$M, L \ge 0$ and
		a subgradient oracle,
		 i.e.,
		a function $f':\dom h \to \R^n$
		satisfying $f'(u) \in \partial f(u)$ for every $u \in \dom h$, such that
		\begin{equation}
		   	\|f'(u)-f'(v)\| \le 2M + L \|u-v\| \quad\forall u,v \in \dom h;
		\label{cond:A2} 
		\end{equation}
		
		\item[(A3)]
		$\phi^*:= \inf \{ \phi(u) : u \in \R^n \}$ is finite.
  \end{itemize}

  % {\bf ?????????????  Define $\ell_{f_{m}(\cdot;z)}(y;x)$ and $\ell_{\phi_{m}(\cdot;z)}(y;x)$}

	Let ${\cal C}(M,L)$ denote the class of functions $f$ satisfying Assumption (A2).
		Even though ${\cal C}(M,L)$ depends on $\dom h$, we have omitted this dependence from its notation. For any function
  $g \in {\cal C}(M,L)$ and $x \in \dom h$, we denote the linearization of $g$ at
  $x$ by
  \begin{equation}
\label{def:l_g}
  \ell_g(\cdot;x) := g(x) + \inner{g'(x)}{\cdot-x}.
  \end{equation}

  
        % Assume $h: \R^n\rightarrow (-\infty,+\infty]$ is a nonsmooth convex function, the linearization of $\phi_m(:;z) = f_m(:;z)+ h$ at $y \in \R^n$ is given by
        % \[
        % \ell_{\phi_m(:;z)}(:;y):=\phi_m (y;z) + (f'(y)+m(y-z))(:-y).
        % \]
		
		% {\bf The ortacke in (B2) can implemented if have a oracle for convex function}
		
% 		there exists $R>0$ such that 
% 		$\dom h \subset \bar B(0;R)$.
% \end{itemize}

	\begin{lemma}
 \label{lem:Lip_ineq}
Assume that $f\in {\cal C}(M,L)$ for some $(M,L) \in \R^2_+$ and $f$ is $m$-weakly convex
on $\dom h$.
Then, for every $z \in \R^n$ and $\tz \in \dom h$, we have
\[
f'(\tz) + m(\tz-z) \in \partial [ f_m(\cdot;z) ](\tz), \qquad
f_m(\cdot;z) \in 
\Conv{n} \cap {\cal C}(M,L+m).
\]
% $f_m(\cdot;z) \in 
% \bConv{n} \cap {\cal C}(M,L+m)$ for every $z \in \R^n$.
% As a consequence, we have
% \[
%     0 \le   f_{m}(y;z) - \ell_{f_{m}(\cdot;z)}(y;x) \le     M \|y-x\| + \frac{ L + 
%     m}2 \|y-x\|^2 \quad \forall x, y\in \R^n.
% 	\]
% \\
% Let  $\phi = f + g: \mathcal{X} \mapsto(-\infty, \infty]$ be a $ m $- weakly convex function and assume that $f$ satisfies Assumption A(1) and A(3), then  we have 
\end{lemma}
\begin{proof}
Since $f\in {\cal C}(M,L)$, there exists an oracle $f'$ satisfying
the conditions in (A2), i.e.,
$f'(x) \in \partial f(x)$ for every $x \in \dom f$ and
\eqref{cond:A2} holds.
Since $f$ is $m$-weakly convex, it follows from the definition of
weak convexity that 
$f_m \in \bConv{n}$. Moreover,
it follows from Lemma \ref{lem:chara_weakly} with $(\varepsilon,x,c) = (0,\cdot,z)$ and
the fact that
$f'(x) \in \partial f(x)$ for every $x \in \dom f$
that $f_m'(\cdot;z) := f'(\cdot) + m (\cdot - z) \in \partial [f_m(\cdot;z)](\cdot)$.
The result now follows by noting that
\eqref{cond:A2} and the definition
of $f_m+1/\lam$ imply that
for every $x,y \in \dom f$
\begin{align*}
\|f_m'(x;z) - f_m'(y;z)\| &= \|f'(x) - f'(y) + m(x-y)\| \\
&\le \|f'(x) - f'(y)\| + m\|x - y\|\\
&\le 2M + (L+m)\|x-y\|
\end{align*}
which means that $f_m(\cdot;z) \in {\cal C}(M,L+m)$.
\end{proof}

In view of the first inclusion of
Lemma \ref{lem:Lip_ineq},
it follows that
for any $z \in \R^n$ and $\tz \in \dom h$, function $f_m(\cdot;z)$ admits the linearization
given by
\beq \label{def:linear}
% \ell_{f_m(\cdot;z)}(\cdot;\tz)
%\ell(\cdot;\tz,(f,z,m)):
\ell_{f_m(\cdot;z)} (\cdot;\tz) :=
 f_m(\tz;z) + \inner{f'(\tz)+m(\tz-z)\,}{\,\cdot-\tz}.
\eeq
Moreover, in view of the second inclusion of Lemma \ref{lem:Lip_ineq}, we have
\beq \label{eq:fm-linear}
    0 \le   f_{m}(\cdot;z) - 
    \ell_{f_m(\cdot;z)} (\cdot;\tz)
    %\ell(\cdot;\tz,(f,z,m)) 
    \le    2 M \|\cdot-\tz\| + \frac{ L + 
    m}2 \|\cdot-\tz\|^2 \quad \forall z \in \R^n,\, \tz \in \dom h.
	\eeq
 Note that the above observations with
 $\tz=z$ implies that for every $z \in \dom h$,
 we have
 
 \begin{equation}\label{eq:simlin}
   \ell_{f_m(\cdot;z)} (\cdot;z) = \ell_f(\cdot;z), \quad    
 \end{equation}
 and
 \beq \label{eq:fm-linear1}
   0 \le   f_{m}(\cdot;z) - 
    \ell_{f} (\cdot;z)
    %\ell(\cdot;\tz,(f,z,m)) 
    \le     2M \|\cdot-z\| + \frac{ L + 
    m}2 \|\cdot-z\|^2 \quad \forall z \in \dom h.
	\eeq
% Let $f: \R^n\rightarrow (-\infty,+\infty]$ be given. Assume f is $m$-weakly convex, the linearization of $f_m(\cdot;z)$ at $y \in \R^n$ is given by
        % \[

        % \]
        
        
        \subsection{ Review of the SCS 
         method for the weakly convex case}\label{SEC:SCS}

This subsection reviews the
deterministic version of
the SCS method of~\cite{davis2019stochastic}.

More specifically, it considers
the PS method
described below
under the assumptions stated in
Subsection~\ref{sec:assumptions} 
except that the  constant $L$
in condition (A2) is assumed to be zero.

% The aforementioned method is described in
% Algorithm \ref{alg:Davis}
% while
% its iteration-complexity is
% described in Proposition~\ref{??}
% below.

% \begin{algorithm}
% \caption{Proximal subgradient}
% \label{alg:Davis}
% \state \textbf{Input}: $\hat x_0 \in \dom h$, a sequence of $\left\{1+m\lam_t\right\}_{t \geq 0} \subset \mathbb{R}_{+}$ and iteration count $T$.\\
% \state \textbf{Step} $t = 0,1,\cdots, T$:
% \begin{equation}
% \text { Set } \hat x_{t+1}=\operatorname{prox}_{1+m\lam_t h}\left(\hat x_t-1+m\lam_t f'(\hat x_t)\right)
% \end{equation}
% % \state Sample $t^* \in\{0, \ldots, T\}$ according to $\mathbb{P}\left(t^*=t\right)=\frac{1+m\lam_t}{\sum_{i=0}^T 1+m\lam_i}$.\\
% % \STATE \textbf{Return} $x_{t^*} $
% \end{algorithm}


\noindent\rule[0.5ex]{1\columnwidth}{1pt}
	
	PS

    \noindent\rule[0.5ex]{1\columnwidth}{1pt}
    {\bf Input:} $\hat x_0 \in \dom h$, a sequence of $\left\{\alpha_t\right\}_{t \geq 0} \subset \mathbb{R}_{+}$ and iteration count $T$.\\
    {\bf Step:} For $t = 0,1,\cdots, T$, compute
\begin{equation}
 \hat x_{t+1}=\argmin_{u \in \R^n} \left\{\ell_{f}(u;\hat x_t)+ h(u)+ \frac{1}{\alpha_t}\|u -  \hat x_t\|^2\right\}.
\end{equation}
    \noindent\rule[0.5ex]{1\columnwidth}{1pt}


The following result (see Theorem 3.4 in \cite{davis2019stochastic}) describes the rate
of convergence of the PS method.

\begin{proposition}
\label{prop:Davis}
Assume $(f,h)$ are functions satisfying conditions (A1)-(A3)
with $L=0$ and
assume that  $\alpha_t \in(0,1 / \bar{m}]$ for every $t \ge 0$ for some
$\bar m \in (m,2m]$. Then,
the iterates $\hat x_t$ of
PS satisfies
\begin{equation}\label{Moreacomplexity}
\frac{\sum_{t=0}^T \alpha_t \left\|\nabla \hat M^{1 / (\bar{m}-m)}\left(\hat x_t\right)\right\|^2}{\sum_{t=0}^T \alpha_t}  \leq \frac{\bar{m}}{\bar{m}-m} \cdot \frac{\left(\hat M^{1 / (\bar{m}-m)}\left(\hat x_0\right)- \phi^*\right)+2 \bar{m} M^2 \sum_{t=0}^T \alpha_t^2}{\sum_{t=0}^T \alpha_t} .
\end{equation}
As a consequence, for any
given tolerance $\rho>0$ and constant $\gamma $ such that
$\gamma \in(0, 1/(2m)]$,
if the stepsizes $\alpha_t$ are chosen according to
$\alpha_t = \gamma/\sqrt{T+1}$
for every $t \ge 0$ and the iteration count $T$ satisfies
\begin{equation}\label{eq:MoreauT}
  T \ge \frac{\left[\left(\hat M^{1/m}\left(\hat x_0\right)- \phi^*\right)+4 m M^2 \gamma^2\right]^2}{\gamma^2 \rho^4},
\end{equation}
then
one of the iterates
$\hat x_t \in 
\{ \hat x_0, \ldots, \hat x_T\}$
% if the number of iterations $T$ 
of PS
% then one of the iterates
% $\hat x_t \in 
% \{ \hat x_0, \ldots, \hat x_T\}$
satisfies
$\left\|\nabla \hat M^{1 / m}\left(\hat x_t\right)\right\| \le \rho$.
\end{proposition}

% We now make some remarks
% about Algorithm ??? and Proposition \eqref{prop:Davis}.
% First,
% since $\nabla M_\phi^{1 / (2m)}\left(\hat x_t\right)$ is difficult to compute, a termination criterion
% based on this quantity is not
% computationally feasible.
% Instead, ??????
% Second,
% the method of
% \cite{davis2019stochastic}
% actually outputs
% an iterate $\hat x_{t^*}$ where $t^*$
% is
% sampled from $\{0, \ldots, T\}$ according to the probability mass function $\mathbb{P}\left(t^*=t\right)=1+m\lam_t/\sum_{i=0}^T 1+m\lam_i$ for every $t =0,\ldots,T$.
% It turns out that
% $\E[ \|\nabla M_\phi^{1 / (2m)}\left(\hat x_{t^*}\right)\|^2]$
% is equal to the left hand side of
% \eqref{Moreacomplexity} and hence is bounded
% by the right hand side of
% \eqref{Moreacomplexity}.
% However, a drawback of this approach is that it
% is not clear how
% to use $\hat x_{t^*}$ to
% develop an alternative stopping criterion
% that guarantees that
% $\hat x_{t^*}$ is a $(\rho,1/(2m))$-Moreau stationary point in expectation
% since neither the latter
% expected value nor
% the right hand side of \eqref{Moreacomplexity}
% is computable.

We now make some remarks
about PS and Proposition \ref{prop:Davis}.
First, PS always performs $T+1$ iterations
and the second part of Proposition \ref{prop:Davis} gives 
a sufficient condition on
$T$ which guarantees that
one of its iterates is a
$(\rho;1/m)$-Moreau stationary point.
An alternative termination condition based on the magnitude
of $\nabla \hat M^{1 /m}(\hat x_t)$
is not doable since this quantity is generally expensive to compute.
Second, a drawback of the estimate
\eqref{eq:MoreauT} on $T$
is that it is
generally not computable as it depends on $\phi^*$.
Third,
the method of
\cite{davis2019stochastic}
actually outputs
an iterate $\hat x_{t^*}$ where $t^*$
is
sampled from $\{0, \ldots, T\}$ according to the probability mass function $\mathbb{P}(t^*=t)=\alpha_t/\sum_{i=0}^T \alpha_t$ for every $t =0,\ldots,T$.
It turns out that
$\E[ \|\nabla \hat M^{1 /m}(\hat x_{t^*})\|^2]$
is equal to the left hand side of
\eqref{Moreacomplexity} and hence is bounded
by the right hand side of
\eqref{Moreacomplexity}.
The advantage of this approach
is that, without
performing
any evaluation of $\|\nabla \hat M^{1 /m}(\cdot)\| ^2$,
it returns 
an iterate
$\hat x_{t^*}$
such that the expected
value of $\|\nabla \hat M^{1 /m}(\hat x_{t^*}) \|^2$
is bounded by the right hand
side of \eqref{Moreacomplexity}. However, there is no simple way of de-randomizing this output strategy
due to the fact that
the function
$\|\nabla \hat M^{1 /m}(\cdot) \|^2$
 is generally nonconvex and hard to compute. 

 Finally, the third remark in the Concluding Remarks discusses how the alternative inexact proximal point method developed in \cite{davis2019proximally} can
 estimate the gradient of the Moreau envelope evaluated at the center of each prox subproblem after it performs
 a pre-specified number of inner iterations to solve it approximately.


 
%  This is not desirable for deterministic optimization problem because people usually seek for deterministic convergence guarantee.

% In particular, if Algorithm \ref{alg:Davis} uses the constant parameter $1+m\lam_t=\frac{\gamma}{\sqrt{T+1}}$, for some real $\gamma \in\left(0, \frac{1}{2 m}\right]$, then the point $x_{t^*}$ satisfies:
% $$
% \frac{\sum_{t=0}^T \left\|\nabla M_\phi^{1 / 2 m}\left(x_{t}\right)\right\|^2}{
% T+1} \leq 2 \cdot \frac{\left(\phi_{1 / 2 m}\left(x_0\right)-\min \phi\right)+4 m M^2 \gamma^2}{\gamma \sqrt{T+1}}.
% $$
% Thus the complexity to generate a point such that $\|\nabla M_\phi^{1/2m}\| \le \varepsilon$ is at most 
% \begin{equation}
%     \frac{\left(\phi_{1 / 2 m}\left(x_0\right)-\min \phi\right)+4 m M^2 \gamma^2}{\gamma \varepsilon^4}
% \end{equation}


% One remark is that Davis only consider the case that $f$ satisfies Assumption (A2) with $L_f = 0$ which is a more restrict function class.
% \begin{corollary}
    
% \end{corollary}

% \item[b)] there holds
% \[
% \inf_{\|d\| \le 1} \phi'(\hat y;d) \ge
% -\hat \varepsilon.
% \]
% \item[c)]
% there holds

% \end{itemize}
% \end{corollary}



% -------------------------

% Let $\lam=1/(2m)$ and $\varphi(\cdot) = \phi(\cdot)- \inner{w}{\cdot}$. Then
% \[
% \|\hat y-y\| = (1/(2m)) \|\nabla \varphi_\lam(y)} \|, \quad
% \mbox{dist}(0, \partial \varphi( \hat y) ) \le \| \nabla \varphi_\lam(y)} \|

% \]
% \[
% \mbox{dist}(0, \partial \varphi( \hat y) ) \le 2m \|y-\hat y\|
% \]
% Note that
% \[
% \mbox{dist}(0, \partial \phi( \hat y) ) \le
% \mbox{dist}(0, \partial \varphi( \hat y) ) + \|w\|
% \le 2m \|y-\hat y\| + \|w\|
% \]

% ---------------------------

 \subsection{The generic bundle update framework}
 \label{sec:BUF}

As mentioned in the Introduction, PBF uses a bundle (convex) function 
underneath $\phi(\cdot)+m\|\cdot-x^c\|^2/2$, where $x^c$ is the current prox center, to
construct subproblem \eqref{eq:x-pre} at a given iteration, and then
updates $\Gamma$ to obtain the bundle function $\Gamma^+$ for the next iteration.
This subsection describes ways of updating
the bundle.
% ???????????? One of the key step for PBF is to construct bundle functions $\Gamma$ underneath  $\phi(\cdot)+(m/2)\|\cdot-x^c\|^2$
% % in the context of weakly convex function.
	Instead of focusing
	on a specific bundle update scheme, 
	this subsection
 describes 
	a generic update scheme. It also discusses three concrete ways of implementing the generic scheme.
	% This subsection also
	% describes three concrete examples of the generic bundle update framework. 
 
    We start by describing the generic bundle update framework (BUF). 
    
%     \noindent\rule[0.5ex]{1\columnwidth}{1pt}
	
% 	BUF

%     \noindent\rule[0.5ex]{1\columnwidth}{1pt}
%     {\bf Input:}
%     $g, h \in \bConv{n}$, $\lam \in \R_{++}$, and
%     $(x^c,x,\Gamma) \in \R^n \times
%     \R^n \times \bConv{n}$ such that $\Gamma\le g+h$ and \eqref{eq:x-pre} holds;
%   %   \beq
% 	 % x = \underset{u\in \R^n}\argmin \left \{\Gamma (u) + \frac{1}{2\lam} \|u-x^c\|^2 \right\};
%   % \eeq
%     \begin{itemize}
%         % \item 
%         % compute $x$ as in \eqref{eq:x-pre};
%         \item
%         find function $\Gamma^+$ such that
% \begin{equation}\label{def:Gamma}
% 	    \Gamma^+ \in \bConv{n}, \qquad \max \{ \bar \Gamma(\cdot), \,\ell_g(\cdot;x)+h(\cdot) \}  \le \Gamma^+(\cdot) \le (g+h)(\cdot),
% 	\end{equation}
%    where $\ell_g(\cdot;\cdot)$ is as in \eqref{def:l_g} and
%    $\bar \Gamma(\cdot) $ is such that
% \begin{equation}\label{def:bar Gamma} 
% 	\bar \Gamma \in \bConv{n}, \quad
%  \bar \Gamma(x) = \Gamma(x), \quad 
% 	x = \underset{u\in \R^n}\argmin \left \{\bar \Gamma (u) + \frac{1}{2\lam} \|u-x^c\|^2 \right\}.
% 	\end{equation}
%      \end{itemize}
%      {\bf Output:} $\Gamma^+$.
     
%     \noindent\rule[0.5ex]{1\columnwidth}{1pt}

      \noindent\rule[0.5ex]{1\columnwidth}{1pt}
	
	BUF%$(\Gamma,x^c,\lam,\tau)$

    \noindent\rule[0.5ex]{1\columnwidth}{1pt}
    {\bf Input:}  $g, h \in \bConv{n}$, $\lam \in \R_{++} $ and
    $(x^c,x,\Gamma) \in \R^n \times
    \R^n \times {\cal B}(g+h)$ such that 
      \begin{equation}\label{eq:x-pre1}
    x = \underset{u\in \R^n}\argmin \left \{\Gamma (u) + \frac{1}{2\lam} \|u-x^c\|^2 \right\};
\end{equation}
    \begin{itemize}
        % \item 
        % compute $x$ as in \eqref{eq:x-pre};
        \item
        find bundle $\Gamma^+ \in   {\cal B}(g+h)$ satisfying
\begin{equation}\label{def:Gamma}
	     \Gamma^+ (\cdot) \ge  \max \{ \bar \Gamma(\cdot)  , \ell_g(\cdot;x)+h(\cdot)\},
	\end{equation}
   for some
   $\bar \Gamma(\cdot) \in {\cal B}(g+h)$ such that
\begin{equation}\label{def:bar Gamma}
 \bar \Gamma(x) = \Gamma(x), \quad 
	x = \underset{u\in \R^n}\argmin \left \{\bar \Gamma (u) + \frac{1}{2\lam} \|u-x^c\|^2 \right\}.
	\end{equation}
     \end{itemize}
     {\bf Output:} $\Gamma^+$.
     
  \noindent\rule[0.5ex]{1\columnwidth}{1pt}

 
 % BUF was ????? first proposed in \cite{liang2021unified} and
 Note that  the bundle update framework of \cite{liang2024unified} replaces \eqref{def:Gamma} by the weaker inequality $\Gamma^+ (\cdot) \ge  \tau \bar \Gamma(\cdot) +(1-\tau)(  \ell_{g}(\cdot;x)+h(\cdot))$ for some $\tau \in (0,1)$ and, as
a result, contains the one-cut bundle update scheme described in Subsection 3.1 of \cite{liang2024unified}.
Even though 
 BUF does not include the  one-cut bundle update scheme,
 it contains the two other bundle update schemes discussed in  \cite{liang2024unified} (see Subsection 3.1), which for convenience are briefly described below.
 % We will now discuss two concrete ways of implementing BUF.

	\begin{itemize}
	   
	    \item \textbf{2-cut:}
	      For this scheme, it is assumed that
        $\Gamma$
        has the form 
        \begin{equation}\label{def:Gamma-E2}
            \Gamma=\max \{A_g,\ell_g(\cdot;x^-)\}+h
        \end{equation}
        where $h \in \Conv{n}$ and
        $A_g$ is an affine function satisfying $A_g\le g$. In view of \eqref{eq:x-pre1},
        it can be shown that there exists
        $\theta \in [0,1]$ such that
        \begin{align}
            &\frac1\lam (x-x^c) + \partial h(x) 
+ \theta \nabla A_g + (1-\theta) g'(x^-) \ni 0, \label{theta1} \\
            &\theta A_g(x) + (1-\theta) \ell_g(x;x^-) = \max \{A_g(x),\ell_g(x;x^-)\}. \label{theta2}
        \end{align}
        The scheme then sets
        \begin{equation}\label{def:Af+}
         A_g^+(\cdot) :=  \theta A_g(\cdot) + (1-\theta) \ell_g(\cdot;x^-)
     \end{equation}
     and outputs the function $\Gamma^+$
      defined as
\begin{equation}\label{eq:G-agg}
		    \Gamma^+(\cdot)  := 
		    \max\{A_g^+(\cdot),\ell_g(\cdot;x)\} + h(\cdot).
	    \end{equation}
	    
	    \item  \textbf{multiple-cut (M-cut):}
	    Suppose $\Gamma=\Gamma(\cdot;C)$ where
	    $C \subset \R^n$ is a finite set (i.e., the current  bundle set) and $\Gamma(\cdot;C)$ is defined as
	    \begin{equation}\label{eq:Gamma-E2}
	        \Gamma(\cdot;C) := \max \{  \ell_g(\cdot;c) : c \in C \}+h(\cdot).
	    \end{equation}
	     This scheme 
      % computes $x$ as in \eqref{eq:x-pre}, 
      chooses the next bundle set $C^+$ so that
	    \begin{equation}\label{eq:C+}
        C(x) \cup \{x\} \subset C^+ \subset C \cup \{x\}
        \end{equation}
        where
        \begin{equation}\label{def:C+}
            C(x) := \{c \in C : \ell_g(x;c)+h(x) = \Gamma(x) \},
        \end{equation}
         and then
	    output $\Gamma^+ = \Gamma(\cdot;C^+)$.
	\end{itemize}

%     ?????????????????????????????????????????? We now make some remarks to argue that all the update schemes 
% above are special implementations of BUF.
% It can be easily seen that 
% the update $\Gamma^+$ in
% (S1), together with
%  $\bar \Gamma=\Gamma$, satisfies \eqref{def:Gamma} and \eqref{def:bar Gamma}, and hence
%  that scheme (S1) is a special way of implementing BUF.
The following facts, 
whose proofs can be found  in Appendix D of \cite{liang2024unified},
imply that
2-cut and M-cut schemes are 
 special implementations of  BUF:

\begin{itemize}\label{lem:E2}
    \item[(a)]
    if $\Gamma^+$ is obtained according to 2-cut, then
    the pair $(\Gamma^+,\bar \Gamma)$ where
   $\bar \Gamma = A_g^+ + h$ satisfies \eqref{def:Gamma} and \eqref{def:bar Gamma};
    % As a consequence,
    % $\Gamma^+$ is a special implementation of BUF.
    \item[(b)] 
    if $\Gamma^+$ is obtained according to M-cut,
     then
    the pair $(\Gamma^+,\bar \Gamma)$ where
   $\bar \Gamma =\Gamma(\cdot;C(x))$
    %  Consider the update $\Gamma^+$ of (S3) and set
    % $\bar\Gamma=\Gamma(\cdot;C(x))$ where  $\Gamma(\cdot;C(x))$ is as in \eqref{eq:Gamma-E2}. Then,
    % $(\Gamma^+,\bar \Gamma)$ 
    satisfies \eqref{def:Gamma} and \eqref{def:bar Gamma}.
    % As a consequence,
    % $\Gamma^+$ is a special implementation of BUF.
	\end{itemize}

 
% ---------FROM MOR PAPER--------------


%  [(E2)] \textbf{two-cuts scheme:}
%         For this scheme, it is assumed that
%         $\Gamma$
%         has the form 
%         \begin{equation}\label{def:Gamma-E2}
%             \Gamma=\max \{A_f,\ell_f(\cdot;x^-)\}+h
%         \end{equation}
%         where $h \in \Conv{n}$ and
%         $A_f$ is an affine function satisfying $A_f\le f$. In view of \eqref{eq:x-pre},
%         it can be shown that there exists
%         $\theta \in [0,1]$ such that
%         \begin{align}
%             &\frac1\lam (x-x^c) + \partial h(x) 
% + \theta \nabla A_f + (1-\theta) f'(x^-) \ni 0, \label{theta1} \\
%             &\theta A_f(x) + (1-\theta) \ell_f(x;x^-) = \max \{A_f(x),\ell_f(x;x^-)\}. \label{theta2}
%         \end{align}
%         The scheme then sets
%         \begin{equation}\label{def:Af+}
%          A_f^+(\cdot) :=  \theta A_f(\cdot) + (1-\theta) \ell_f(\cdot;x^-)
%      \end{equation}
%      and outputs the function $\Gamma^+$
%       defined as
% \begin{equation}\label{eq:G-agg}
% 		    \Gamma^+(\cdot)  := 
% 		    \max\{A_f^+(\cdot),\ell_f(\cdot;x)\} + h(\cdot).
% 	    \end{equation}
	

     
	    
% 	     [(E3)] \textbf{multiple-cuts scheme:} For this scheme, it is assumed that $\Gamma$ of the form
% 	    $\Gamma=\Gamma(\cdot;B)$ where
% 	    $B \subset \R^n$ is a finite set (i.e., the current  bundle set) and $\Gamma(\cdot;B)$ is defined as in \eqref{eq:Gamma-E2}.
% 	   % \begin{equation}\label{eq:Gamma-E2}
% 	   %     \Gamma(\cdot;C) := \max \{ \ell_f(\cdot;c) : c \in C \}+h ,
% 	   % \end{equation}
% 	     This scheme 
%       % computes $x$ as in \eqref{eq:x-pre}, 
%       chooses the next bundle set $B^+$ so that
% 	    \begin{equation}\label{eq:C+}
%         B(x) \cup \{x\} \subset B^+ \subset B \cup \{x\}
%         \end{equation}
%         where
%         \begin{equation}\label{def:C+}
%             B(x) := \{ b \in B : \ell_f(x;b)+h(x) = \Gamma(x) \},
%         \end{equation}
%          and then
% 	    output $\Gamma^+ = \Gamma(\cdot;B^+)$.

%      ----------------------
	

 \subsection{The proximal bundle framework} \label{subsec:update}


 This subsection describes PBF and its main complexity result.
 It also compares the complexity of PBF with that of the deterministic version of the SCS method of \cite{davis2019stochastic}
 described in Subsection~\ref{SEC:SCS}.
	
% 	We now describe three concrete
%  update schemes (E1), (E2), and (E3) which are special ways
%  of implementing BU.
%  Unless otherwise stated, it
%  is assumed that their input is
%  the same as in BU.
 
 
% 	\begin{itemize} 
%      \item [(E1)] \red{{\bf one-cut scheme}}: 
%      % Given $\Gamma\in \mConv{n}$ satisfying $\Gamma\le \phi$ and $(x^c,\lam,\sigma) \in \R^n \times \R_{++} \times (0,1)$,
%      This scheme obtains $\Gamma^+$ as in \eqref{eq:affine}.
% 	It is easy to see that if this update is used recursively
% 	then $\Gamma$ is always of the form
% 	\begin{equation}\label{eq:Gamma-form}
% 	    \Gamma(\cdot) = \sum_{x \in X} 1+m\lam_x \ell_f(\cdot;x)  + h(\cdot)
% 	\end{equation}
% 	where $X$ is a finite set in $\dom h$ and
%   $\{1+m\lam_x : x \in X\} \subset \R_{++}$ are scalars such that
% 	$\sum_{x \in X} 1+m\lam_x =1$.
	
%         \item [(E2)] \red{\textbf{two-cuts scheme:}} 
%         For this scheme, it is assumed that
%         $\Gamma$
%         has the form 
%         \begin{equation}\label{def:Gamma-E2}
%             \Gamma=\max \{A_f,\ell_f(\cdot;x^-)\}+h
%         \end{equation}
%         where $h \in \mConv{n}$ and
%         $A_f$ is an affine function satisfying $A_f\le f$. In view of \eqref{eq:x-pre},
%         it can be shown that there exists
%         $\theta \in [0,1]$ such that
%         \begin{align}
%             &\frac1\lam (x-x^c) + \partial h(x) 
% + \theta \nabla A_f + (1-\theta) f'(x^-) \ni 0, \label{theta1} \\
%             &\theta A_f(x) + (1-\theta) \ell_f(x;x^-) = \max \{A_f(x),\ell_f(x;x^-)\}. \label{theta2}
%         \end{align}
%         The scheme then sets
%         \begin{equation}\label{def:Af+}
%          A_f^+(\cdot) :=  \theta A_f(\cdot) + (1-\theta) \ell_f(\cdot;x^-)
%      \end{equation}
%      and outputs the function $\Gamma^+$
%       defined as
% \begin{equation}\label{eq:G-agg}
% 		    \Gamma^+(\cdot)  := 
% 		    \max\{A_f^+(\cdot),\ell_f(\cdot;x)\} + h(\cdot).
% 	    \end{equation}
% 	    % where 
%      % \begin{equation}\label{def:Af+}
%      %     A_f^+(\cdot) =  \theta A_f(\cdot) + (1-\theta) \ell_f(\cdot;x^-).
%      % \end{equation}
        
% 	    % \begin{equation}\label{eq:theta}
% 		   %  \theta  \left\{\begin{array}{ll}
% 		   %  =1, & \text { if } A_f(x)> %=(\Gamma-h)(x), \quad
% 		   %  \ell_f(x;x^-), \\ %\ne (\Gamma-h)(x) \\ 
% 		   %  =0, & \text { if } A_f(x) < \ell_f(x;x^-), \\ %=(\Gamma-h)(x), \quad A_f(x) \ne (\Gamma-h)(x) \\
% 		   %  \in[0,1], & \text { if } A_f(x)=\ell_f(x;x^-). %=(\Gamma-h)(x).
% 		   %  \end{array}\right.
% 	    % \end{equation}

     
% 	    % It will be shown in Lemma \ref{lem:E2} that $\Gamma^+ \in \mConv{n}$ and that, for any $\sigma \in (0,1)$, the function $\Gamma^+$ satisfies \eqref{def:Gamma} with $\bar \Gamma = A_f^+ + h$,
% 	    % and hence that $\Gamma^+$ is an output of BUS$(\Gamma,x^c,\lam,\sigma)$.

     
	    
% 	    \item [(E3)] \red{\textbf{multiple-cuts scheme:}} For this scheme, it is assumed that $\Gamma$ of the form
% 	    $\Gamma=\Gamma(\cdot;B)$ where
% 	    $B \subset \R^n$ is a finite set (i.e., the current  bundle set) and $\Gamma(\cdot;B)$ is defined as in \eqref{eq:Gamma-E2}.
% 	   % \begin{equation}\label{eq:Gamma-E2}
% 	   %     \Gamma(\cdot;C) := \max \{ \ell_f(\cdot;c) : c \in C \}+h ,
% 	   % \end{equation}
% 	     This scheme 
%       % computes $x$ as in \eqref{eq:x-pre}, 
%       chooses the next bundle set $B^+$ so that
% 	    \begin{equation}\label{eq:C+}
%         B(x) \cup \{x\} \subset B^+ \subset B \cup \{x\}
%         \end{equation}
%         where
%         \begin{equation}\label{def:C+}
%             B(x) := \{ b \in B : \ell_f(x;b)+h(x) = \Gamma(x) \},
%         \end{equation}
%          and then
% 	    output $\Gamma^+ = \Gamma(\cdot;B^+)$.

        
        
%         % It will be shown in Lemma \ref{lem:E3} that  $\Gamma^+ \in \mConv{n}$ and that, for any $\sigma \in (0,1)$, the function $\Gamma^+$ satisfies \eqref{def:Gamma} with $\bar \Gamma=\Gamma(\cdot;C(x))$,
%         % % \begin{equation}\label{eq:bar Gamma E3}
%         % %     \bar \Gamma= \max \{ \ell_f(\cdot;c) : c \in C(x)\} + h,
%         % % \end{equation}
%         % and hence that $\Gamma^+$ is an output of BUS$(\Gamma,x^c,\lam,\sigma)$.
        
        
% %         Note that if the above update 
% %         % with $C^+$ chosen
% %         % as $C^+=C \cup \{x\}$
% %         is used recursively,
% % 	    then $\Gamma$ is always of the form
% % 	$$\Gamma(\cdot) = \max \{ \ell_f(\cdot;c) : c \in C\}  + h(\cdot)$$ where
% % 	$C$ is a (finite) bundle set contained in
% % 	the set $X$ of all points
% % 	$x$ computed as in \eqref{eq:x-pre} as $\Gamma$ is recursively updated
% % 	according to \eqref{eq:Gamma E3}. Observe that $C=X$ when
% % 	$C^+$ is always chosen as $C^+=C \cup \{x\}$.
	
%         % \red{think about the $\bar C$ with Lagrange multiplier}
        
	    
% 	   % \item [(E5)] \textbf{quadratic model update:} Given $(\Gamma,x_0,\lam) \in {\cal C}_\mu(\phi) \times \R^n \times \R_{++}$, this scheme obtains $\Gamma^+$ as
% 	   % \begin{equation}\label{eq:Gamma E5}
% 	   %     \Gamma^+=\max\left \lbrace \Gamma(x) + \frac{1}{\lam}\inner{x_0-x}{\cdot-x} + \frac{\mu}2 \|\cdot-x\|^2, \ell_\phi(\cdot;x) \right \rbrace
% 	   % \end{equation}
% 	   % where $x$ is as in \eqref{eq:x-pre}.
% 	   % It will be shown in Lemma \ref{lem:unify} below that for any $\sigma \in (0,1)$, $\Gamma^+ \in {\cal C}_\mu(\phi)$ and $\Gamma^+$ satisfies \eqref{def:Gamma} with $\bar \Gamma$ as
% 	   % \begin{equation}\label{eq:bar Gamma E5}
% 	   %     \bar \Gamma=\Gamma(x) + \frac{1}{\lam}\inner{x_0-x}{\cdot-x} + \frac{\mu}2 \|\cdot-x\|^2,
% 	   % \end{equation}
% 	   % and hence $\Gamma^+ \in {\cal C}_\phi(\Gamma,x_0,\lam,\sigma)$.
%     % 	\red{Indeed, $\Gamma^+ = \max\{\bar \Gamma, \ell_\phi(\cdot;x)\}$.}
% 	\end{itemize}

%  ??? It is interesting to note that
%  \eqref{eq:G-agg}, \eqref{eq:C+} and the
%  definition of $\Gamma^+$ in (E3) imply that
%         the  updates $\Gamma^+$ output by
%         schemes (E2) and (E3) have the property that $\Gamma^+(\cdot)$ is minorized 
%         by $\ell_f(\cdot;x)+h(\cdot)$
%         where $x$ is as in \eqref{eq:x-pre}.
%         On the other hand,
%         $\Gamma^+$ output by (E1)
%         does not necessarily has this property.

% ???? We now make some remarks to argue that all the update schemes 
% above are special implementations of BU.
% It can be easily seen that 
% the update $\Gamma^+$ in
% (E1), together with
%  $\bar \Gamma=\Gamma$, satisfies
%  \eqref{def:Gamma} and \eqref{def:bar Gamma}, and hence
%  that this $\Gamma^+$ is a special way of implementing BU.
%  On the other hand, the proofs that
%  the updates $\Gamma^+$ of (E2) and (E3) are
%  special implementations of BU
%  are more involved and are given in
%  Propositions \ref{lem:E2} and \ref{lem:E3}, respectively.
 
% \begin{subsection}{generic bundle update framework} (OLD)

% In this section, we discuss some generic bundle update frameworks. Following [to do], we introduce the general Bundle update framework as follows:\\
% \begin{algorithm}
% \caption{generic bundle update framework (BUF)}
%  \begin{algorithmic}
% \State{Input:}
% $\psi, \Gamma \in \bConv{n}$
% such that $\Gamma \le \psi$
% and 
% a triple
% 	$(x^c,\lambda,\sigma) \in \R^n \times \R_{++} \times (0,1) $;
	
% \State {Output:}
% define
% \[
% 	x:= \underset{u\in \R^n}\argmin \left \{ \Gamma (u) + \frac{1}{2\lambda} \|u-x^c\|^2 \right\} \label{def:bar Gamma}
% 	\]
% and output a function
% $\Gamma^+ \in \bConv{n}$ such that 
% 	\begin{equation}\label{def:Gamma}
% 	    \sigma \bar \Gamma + (1-\sigma) [\ell_\psi(\cdot;x)+g] \le \Gamma^+ \le \psi 
% 	\end{equation}
% 	for some
% 	$\bar \Gamma \in {\cal C}(\phi)???????$ satisfying
% 	\begin{equation}
% 	\bar \Gamma(x) = \Gamma(x), \quad 
% 	x = \underset{u\in \R^n}\argmin \left \{\bar \Gamma (u) + \frac{1}{2\lambda} \|u-x^c\|^2 \right\} \label{def:bar Gamma}.
% 	\end{equation}
%  \end{algorithmic}
% \end{algorithm}
% Then we present some of the concrete bundle update rules which satisfies the above general framework.
% 	\begin{itemize}
% 	    \item [(E1)] {\bf one-cut scheme}:
% 	    This scheme obtains $\Gamma^+$ as
% 	    \begin{equation}\label{eq:affine}
%         \Gamma^+= \Gamma^+_\sigma := \sigma \Gamma + (1-\sigma) [\ell(\cdot;x)+h]
%         \end{equation}
%         where $x$ is as in \eqref{eq:x-pre} and $\sigma\in (0,1)$ depends on $(L,M,1)$. Clearly, if
%         $\Gamma$ is the sum of $h$ and
%         an affine function underneath $f$,
%         then so is $\Gamma^+$.
        
	   
% 	    \item [(E2)] \textbf{two-cuts scheme:}
% 	    Assume that
% 	    $\Gamma=\max \{A_f,\ell(\cdot;x^-)\}+h$ where $A_f$ is an affine function satisfying $A_f\le f$ and $x^-$ is the previous iterate.
% 	    This scheme sets the next bundle function $\Gamma^+$ to one similar to
% 	    $\Gamma$ but with  $(x^-,A_f)$ replaced by
% 	    $(x,A_f^+)$ where
% 	    $A_f^+ =  \theta A_f + (1-\theta) \ell(\cdot;x^-)$ for some
% 	    $\theta\in[0,1]$ which does not
% 	    depend on $(L,M,1)$.
	    
% 	    \item [(E3)] \textbf{multiple-cuts scheme:}
% 	    The current bundle function $\Gamma$ is of the form $\Gamma=\Gamma(\cdot;C)$ where
% 	    $C \subset \R^n$ is a finite set (i.e., the current  bundle set) and $\Gamma(\cdot;C)$ is defined as
% 	    \begin{equation}\label{eq:Gamma-E2}
% 	        \Gamma(\cdot;C) := \max \{ \ell(\cdot;c) : c \in C \}+h.
% 	    \end{equation}
% 	    This scheme obtains $\Gamma^+$ as $\Gamma^+ = \Gamma(\cdot;C^+)$ where
% 	    $C^+$ is the updated bundle set obtained
% 	    by possibly 
% 	    removing some points from $C$ and
% 	    then adding the most recent $x$ to
% 	    the resulting set.
% 	\end{itemize}

% \end{subsection}

% \subsubsection{Main notations}
% We let $\Gamma_j$ denote the bundle function for $j$-th iteration. We define the linearization of a convex function f by $\ell_f(\cdot;x) := f(x) + f'(x)(\cdot - x) $ where $f'(x )\in \partial f(x).$ 
% \\Throught the paper, we denote $\partial f(x)$ as the usual subdifferential for convex function f as
% \begin{equation}
%     \partial f(x)  =\left\{ v \in \R^n :  \phi(y) \geq \phi(x)+\langle v, y-x\rangle, \ \forall y \in \R^n \right\}.
%     \label{def:sub}
% \end{equation}
% \section{Main Algorithm}
% This section presents the Proximal Bundle framework. 

Before presenting  PBF, we first
provide a brief outline of the ideas behind it.
PBF consists of solving
a sequence of
subproblems of
the form
\begin{equation}
	    x = \underset{u\in \R^n}\argmin \left \{\Gamma (u) + \frac{1}{2\lam} \|u-\hat y_{k-1} \|^2 \right\}
	\end{equation}
 where  $\Gamma$ is a relatively simple convex function 
 minorizing the convexification $\phi_m(\cdot;\hat y_{k-1})$ of $\phi$ 
 defined in \eqref{def:phim}.
 As any classical proximal bundle approach, it updates the center $\hat y_{k-1}$ during some iterations (called the serious ones) and keeps the center the same in the other ones (called the null ones).

Following the description of PBF below, we also argue that it can be
viewed as
a specific implementation of
an inexact proximal point method
applied to \eqref{eq:opt_problem}.
The parameters
$m$, $ L$ and $M$  that appear on its description 
are as in Assumptions (A1) and (A2).








	\noindent\rule[0.5ex]{1\columnwidth}{1pt}
	
	PBF
	
	\noindent\rule[0.5ex]{1\columnwidth}{1pt}


     \begin{itemize}
		\item [0.] 
    Let initial point $ \hat x_0\in \dom h $, tolerance pair   $(\bar \eta, \bar \varepsilon) \in \R^2_{++}$, 
    and prox stepsize $\lambda>0$
		be given, and set  
		$y_0 = \hat y_0=\hat x_0$, $j=1$, $k=1$,  and
  \beq \label{eq:def-tildem}
  \delta= \min\left\{\frac{\bar \varepsilon}{16},\frac{\lam \bar \eta^2}{64(m\lam+2)},1\right\}, \qquad \Gamma_1(\cdot) = \ell_{ f}(\cdot;\hat x_{0}) + h(\cdot);
  % \qquad m = m+ \frac{1}\lam;
  \eeq


    
    \item[1.] {Compute 
		\begin{align}
	    x_{j} &=\underset{u\in \mathbb{R}^n} \arg\min
	    \left\lbrace  \Gamma_{j}(u)+\frac{1}{2\lambda}\|u- \hat y_{k-1} \|^2 \right\rbrace,
	    \label{def:xj} \\
	   \theta_{j}&=\Gamma_{j}(x_{j}) + \frac{1}{2\lam}
		    \|x_{j}-\hat y_{k-1}\|^2;
		    \label{def:thetaj}
	    \end{align}
	    }
		if 
		% $y_{j} \in \{ x_{j}, y_{j-1} \}$ such that
		% \begin{equation}\label{def:txj}
		%  \phi_{m}(y_{j};\hat y_{k-1})
		%  = \min \left \lbrace \phi_{m}(x_{j};\hat y_{k-1}) ,\phi_{m} (y_{j-1};\hat y_{k-1}) \right\rbrace;
		% \end{equation}
  \begin{equation}\label{def:txj}
		 % \phi_{m}(y_{j};\hat y_{k-1}) + \frac{1}{2\lam} \| y_j-\hat y_{k-1}\|^2
 \phi_{m}(x_j;\hat y_{k-1}) + \frac{1}{2\lam} \| x_j -\hat y_{k-1}\|^2 
   < \phi_{ m}(y_{j-1};\hat y_{k-1}) + \frac{1}{2\lam} \| y_{j-1} -\hat y_{k-1}\|^2	\end{equation}
   then set $y_j=x_j$; else, set $y_j = y_{j-1}$;
    \item [2.]
    Compute 
   \begin{equation}\label{def:wj}
       w_j :=\frac{1}{\lam}(\hat y_{k-1}-x_j) - m( y_j - \hat y_{k-1}), \quad \delta_j:=\delta+ \frac{\lam}{8(m \lam +1)}\| w_j\|^2;
   \end{equation}
		\item[3.] If
\begin{equation}\label{ineq:hpe1}
		     % \quad 
       t_{j} :=  \phi_{m}(y_j;\hat y_{k-1}) + \frac{1}{2\lam} \| y_j -\hat y_{k-1}\|^2     
       - \theta_{j} > \delta_j,
		\end{equation}
         then go to step 3a; else, go to step 3b;
         \begin{itemize}
         \item[3a)]
        (null update)
let 
	    $\Gamma_{j+1} \in \bConv{n}$ denote the output $\Gamma^+$ of BUF with
     input $(\lam,h)$,
	    $g=
     f_m(\cdot;\hat y_{k-1})$,
     and $(x^c,x,\Gamma) = (\hat y_{k-1}, x_j, \Gamma_{j})$,
     and go to step 4;
         \item[3b)] (serious update)
         set $\hat x_k=x_j$,
         $\hat y_k = y_j$, $\hat \Gamma_k(\cdot) = \Gamma_j(\cdot)$, $\hat w_k = w_j$, $\hat \delta_k = \delta_j$
         and
         \begin{equation}\label{def:vk}
\hat v_{k} = \frac{1}{\lambda} (\hat y_{k-1} - \hat x_{k}),
\end{equation}
   and      compute
         \begin{equation}\label{def:wk}
\hat \varepsilon_k =\phi_m (\hat y_{k};\hat y_{k-1}) -\hat \Gamma_{k}(\hat x_{k}) - \inner{\hat v_{k}}{\hat y_{k}-\hat x_{k}}
\end{equation}
 if $\|\hat w_k\| \le \bar \eta$ and
 $\hat \varepsilon_k \le \bar \varepsilon$, then stop;
 else 
 % set
 %  $$
 %        % ?????? \hat f_k(\cdot):=f_m(\cdot;\hat x_{k}) = ?????, \quad
 %    \ell_k(\cdot) := \ell(\cdot;\hat x_k,(f,\hat x_k,m))
 %    = \ell_f(\cdot;\hat x_k)$$
 %    where $\ell(\cdot;\cdot,\cdot)$ is defined in ??? and
         find Bundle $\Gamma_{j+1}(\cdot) \in {\cal B}(\phi_m(\cdot;\hat y_k))$
         such that
    
    \begin{equation}
         % \ell_{\hat f_k}(\cdot;\hat x_{k}) 
   \ell_f(\cdot;\hat y_k)+ h(\cdot) \le
          \Gamma_{j+1}(\cdot) 
\label{eq:serious}
    \end{equation}   
    where $\ell_f(\cdot;\cdot)$ is defined in \eqref{def:l_g}, set $k \leftarrow k+1$, and go to step 4;
        
\end{itemize}
% \item[3.]	compute
% \[
% w_k = ?????, \quad 
% \varepsilon_k = ???
% \]
%  if $\|w_k\| \le \bar \eta$ and
%  $\varepsilon_k \le \bar \varepsilon$, then stop;
%  else go step 4;
	    
	   % \in {\cal C}_{\phi_m(\hat y_{k+1};\hat x_k)}(\Gamma_j, x_{j}^c, \lambda, \sigma)$;

    
	\item[4.]	set $j\leftarrow j+1$, and go to step 1. 
  \end{itemize}
\rule[0.5ex]{1\columnwidth}{1pt}

% \[
% \phi_m(\hat y_k;\hat y_{k-1}) \ge \Gamma(\hat y_k)
% \]
% \[
%  \Gamma(\hat y_k) -\hat \Gamma_{k}(\hat x_{k}) - \inner{\hat v_{k}}{\hat y_{k}-\hat x_{k}} \ge 0
% \]

An iteration $j$ such that $t_j \le \delta$
     is called
	a serious iteration;
 % in which
	% case $x_j$ (resp., $y_j$) is called a serious iterate
	% (resp., auxiliary serious iterate????????);
	otherwise, $j$ is called a null iteration.
	% Moreover, we assume throughout our presentation that
	% $j=0$ is also a serious iteration.
 Let $
 % 0=j_0 \le 
 j_1 \le j_2 \le \ldots$ denote the sequence of all serious iterations and
 let $j_0:=0$.
 Define the $k$-th cycle ${\cal C}_k$ to be the iterations $j$ such that $j_{k-1}+1 \le j \le j_k$, i.e.,
 \begin{equation}\label{def:Ck}
     {\cal C}_k := \{ i_k,\ldots,j_k\}, \quad i_k := j_{k-1}+1.
 \end{equation}
 Hence, only the last 
 iteration of a cycle (which can be the first one
 if ${\cal C}_k$ contains only one iteration)  
 is serious. 
 
 
 We now make some basic remarks about PBF.
 First, PBF is referred to as a framework since it does not completely
	specify the details of
how BUF is implemented nor how
 $\Gamma_{j+1}$ in \eqref{eq:serious} is chosen.
 % and
 % to 
 % how some algorithmic quantities are generated.
	% The framework rather gives minimal  conditions on these quantities
	% which enables us to establish complexity bounds for all specific instances of it
 % in a unified manner.
    % Second, two possible ways of choosing the bundle function $\Gamma_{j+1}$ in a serious update are: 1) $\Gamma_{j+1}=\ell_f(\cdot;x_j)+h$, and 2) $\Gamma_{j+1}=\max\{\Gamma_j, \ell_f(\cdot;x_j)+h\}$.
    Second, in view of \eqref{eq:serious} or the fact that the output of BUF satisfies \eqref{def:Gamma}, it follows that
    % for every $j \ge 1$, we have
    \begin{equation}\label{phi-property}
        \Gamma_j \le \phi_m (\cdot;\hat y_{k-1}),
        %????? := f_m(\cdot;\hat y_{k-1}) + h, 
        \qquad
    \Gamma_j \in \bConv{n}, \qquad \forall j \in {\cal C}_k, \ k \ge 1.
    \end{equation}
	% Fourth, schemes (E1)-(E3)
 % in the previous subsection provide three possible concrete ways of implementing
 % the BUF blackbox in step 1. 
 Third, it is shown in
 Lemma \ref{lemma:optimality} that $\hat w_k \in \partial_{\hat \varepsilon_k} \left[\phi_m(\cdot;\hat y_k) \right]  (\hat y_k)$ for every $k \ge 1$. Hence, $\hat y_k$ is 
 $(\bar \eta,\bar \varepsilon;m)$-regularized stationary point of $\phi$ whenever
 the stopping criterion
 $\|\hat w_k\| \le \bar \eta$ and
 $\hat \varepsilon_k \le \bar \varepsilon$ is satisfied
 in step~2b.
 Fourth, in view of the definition of $ y_j $ in \eqref{def:txj}
	and the above relation, it then follows that
	\begin{equation}\label{eq:minseq}
	 	y_j \in \Argmin \left\lbrace \phi_m(x;\hat y_{k-1}) +\frac{1}{2\lam}\|x - \hat y_{k-1}\|^2:
		x \in \{ \hat y_{k-1},x_{i_k},\ldots,x_j\}	\right\rbrace. 
	\end{equation}

 
 % , 
 % although GPB does not specify a termination criterion for the sake
	% of shortness, all iteration-complexity bounds established in this paper are
	% relative to the effort of obtaining a $(\bar \eta,\bar\varepsilon;m)$-stationary solution  of \eqref{eq:opt_problem}.
 
	% ?????? Finally, although iteration-complexity bounds for GPB can also be established for other termination criteria (see for example
	% Section 6 of \cite{liang2020proximal}),
	% we have omitted the details of
	% their derivation for the sake of
	% shortness.

 % We now make some observations about possible 
 Two simple ways of choosing the bundle function $\Gamma_{j+1}$ such that \eqref{eq:serious} holds
 are to set
 % in a serious update. 
 % Specifically,
 % two simple ways are
 % either 
$\Gamma_{j+1}=\ell_{f} (\cdot;\hat y_{k}) + h$ or $\Gamma_{j+1}=\max\{\tilde \Gamma_{j+1}, \ell_{f}(\cdot;\hat y_{k}) +h\}$, where
 \[
 \tilde \Gamma_{j+1} := \Gamma_j - m \inner{\hat y_k -\hat y_{k-1}}{\cdot-\hat y_{k}} - \frac{m}2 \|\hat y_{k}-\hat y_{k-1}\|^2.
 \]
 % \[
 % \tilde \Gamma_{j} = \Gamma_j - m \inner{\hat x_k -\hat y_{k-1}}{\cdot-\hat x_{k}} - \frac{m}2 \|\hat x_{k}-\hat y_{k-1}\|^2
 % \]
 Clearly, the first choice for $\Gamma_{j+1}$
 satisfies \eqref{eq:serious}.
 Moreover, using the fact that $\Gamma_j \le \phi_m(\cdot;\hat y_{k-1})$ and the definition of
 $\phi_m(\cdot;\hat y_{k-1})$ in \eqref{def:phim}, it is easy to see that
 $\tilde \Gamma_{j+1} \le \phi_m(\cdot;\hat y_{k})$, and hence that
 the second choice
 for $\Gamma_{j+1}$
 also satisfies
 \eqref{eq:serious}.
 % $\Gamma_{j+1} \le  \hat f_{k} +h$.
 % when $\Gamma_{j+1}$ is chosen according to 2).

 % ????????? Moreover, under the assumption that every call to BUF during a null update is carried out using
 % (E2) (resp., (E3)), another
 % way to obtain $\Gamma_{j}$ during a serious update is  to also use update (E2) (resp., (E3)). In view of the observation in the second last paragraph in Subsection \ref{subsec:update}, it follows that the latter
 % way yields a bundle function $\Gamma_{j}$
 % satisfying \eqref{ineq:require}.

%  \[
%  \Gamma \le (f+h) + \frac{m}2 \|\cdot - a\|^2 
%  \]
%  \[
%   \Gamma - \left [ (f+h) + \frac{m}2 \|\cdot - b\|^2 \right ]
%   = 
%   \frac{m}2 \left( \|\cdot-a\|^2-\|\cdot - b\|^2 \right) = m \inner{a-b}{\cdot-b} + \frac{m}2 \|b-a\|^2
%  \]
%  \[
% \tilde  \Gamma := \Gamma - m \inner{a-b}{\cdot-b} - \frac{m}2 \|b-a\|^2 \le 
%  \left [ (f+h) + \frac{m}2 \|\cdot - b\|^2 \right ]
%  \]
%  \[
%  \Gamma^+ = \max \{\tilde \Gamma, \ell_{\hat f_k}+h \} \le f+\frac{m}2 \|\cdot - b\|^2 +h
%  \]
 


It is worth noting however that
PBF still requires knowledge of a parameter $m$ as in Assumption (A1),
and hence is not a completely universal method for finding a stationary point of \eqref{eq:opt_problem}.
% it contains important variants which
% do not depend on the knowledge of parameters
% $M$ and $L$ satisfying (A2).

	% (Recall the meaning of 1C-PB, 2C-PB and MC-PB given in the sentence following (E3) in Section \ref{sec:intro}.)
	% Moreover, both of these variants
	% can be viewed as special cases
	% of GPB with $\sigma$ satisfying the equality in \eqref{rel:tau1}
	% but since their implementation do not depend on $\sigma$,
	% they do
	% not need to compute or estimate such $\sigma$.
	% Third, the PBF instance
	% 1C-PB requires a scalar $\sigma$ satisfying
 % \eqref{rel:tau1} since the
	% update (E1) depends on $\sigma$ (see \eqref{eq:affine}). 
	% Finally, \eqref{rel:tau1} implies that $\sigma$ has to be sufficiently close to one
 % which, in the context of
 % (E1), means that the new bundle $\Gamma_j$ is closer to $\Gamma_{j}$
 % than the new cut
 % $\ell_{\psi_k}(\cdot;x_{j}) + h(\cdot)$
 % in view of the nature of the one-cut scheme (E1) (see relation \eqref{eq:affine}).


% The $j$-th iteration of ??? is called serious if
% 	the inequality $t_{j} \le \bar \varepsilon/2$
%     in step 1 is satisfied;
%     otherwise is called a null one.
% Let $1=i_1 < i_2 < i_3 < \ldots$ denote the serious iterations of PBF
% and, for every $k\ge 1$,
% define the $k$-th cycle of
% PBF as
% \[
% {\cal C}_k := \{ i_k,\ldots,j_k\},
% \]
% where its last iteration index $j_k$ is defined as
% \[
% j_k:=i_{k+1}-1.
% \]
% Note that a cycle may contain only a single iteration, i.e.,
% $j_k=i_k$, and, in this case,
% $j_k$ is a serious iteration.
% On the other hand, if a cycle contains more than one iteration then $j_k$, as
% well as every iteration
% in ${\cal C}_k$ other than $i_k$, is necessarily a null iteration.

{\bf PPM Interpretation:} 
We now discuss how PBF can be interpreted as an inexact proximal
point method for solving \eqref{eq:opt_problem}.
First,
the iterations within the $k$-th cycle can
be interpreted as cutting plane
iterations applied to
the prox subproblem
\begin{equation}
%\bar \theta_k
\hat M^{\lam}(\hat y_{k-1}) = \min \left \{ %\psi^k(x) := 
%\phi_m(x;\hat{x}_{k-1}) + 
\phi(x) + \frac{1}{2 } 
\left( \frac1\lam + m \right)
%(\lam^{-1}+m) 
\|x - \hat y_{k-1}\|^2 : x \in \R^n \right \}.
\label{sub_pro}
\end{equation}



Second, the pair $(y_j,\Gamma_j)$ found in the serious (i.e., the last) iteration $j$ of the $k$-th cycle  approximately solves
\eqref{sub_pro}
% where $\lam'>0$ is such that
% $(\lam')^{-1}=\lam^{-1}+m$
according to \eqref{ineq:hpe1}, in which case
the point
$x_j$ as in \eqref{def:xj} becomes the center
$\hat x_k$ for the next cycle.

We now discuss how the inexact criterion \eqref{ineq:hpe1}
can be interpreted in
terms of the prox subproblem \eqref{sub_pro}.
Since $\Gamma_{j} \le \phi_m(\cdot;\hat y_{k-1})$, it follows from the definitions of $\theta_{j}$, $\hat M^{\lam}(\hat y_{k-1})$, and $\phi_m(\cdot;\hat y_{k-1})$ in \eqref{def:thetaj},  \eqref{sub_pro}, and \eqref{def:phim}, respectively,
that $\theta_{j} \le \hat M^{\lam}(\hat y_{k-1})$, and hence that
 % \[
 %  m_j \le m_j^* \le \phi(y_j)+\frac1{2\lam}\|y_j-x_j^c\|^2
 % \]
 \begin{align}\label{ineq:tilde-tj}
     0 &\le 
     %\phi_m(y_{j};\hat y_{k-1})
     \phi(y_j) 
     +\frac12 \left(\frac1{\lam}+m\right)\|y_{j}-\hat y_{k-1}\|^2 - \hat M^{\lam}(\hat y_{k-1}) \nn \\
     &\le 
     %\phi_m(y_{j};\hat y_{k-1})
     \phi(y_j) 
     +\frac12\left(\frac1{\lam}+m\right)\|y_{j}-\hat y_{k-1}\|^2 - \theta_{j}
  = t_{j} ,
  % =: \tilde t_j
 \end{align}
 where the last identity follows
 from the definitions of
 $t_j$ in \eqref{ineq:hpe1}.
Hence, if $j$ is an iteration 
for which 
\eqref{ineq:hpe1} does not hold  (i.e., a serious one), then it follows from \eqref{ineq:tilde-tj} that
$y_j$ is a $\delta$-solution of \eqref{sub_pro}. 

We now state the main complexity result for PBF. Its complexity bound
depends on the quantity
\begin{equation}\label{def:bar tau}
 \bar \sigma :=   \frac{\left(4\lam(L+m)+1\right)\left( M^2 + \beta_2 \left\{2 \beta_1[\hat M^{\lam}(\hat x_0) - \phi^*] + 4\zeta \lam M^2 \right\} \right)}{8\lam M^2}
\end{equation}
where
\begin{align}\label{beta}
  \beta_1 := 
  \left(m + \frac{2}{\zeta \lam}\right)\left(m+\frac{1}{\lam} \right)^{-1}
%   \frac{m+2 (\zeta \lam)^{-1}}{m+1 \lam^{-1}} 
  , \quad
  \beta_2 := \left( \frac{ L+m}{2}+1\right) 
  \zeta^{-2}\left(\frac{1}{4\zeta \lam}+\frac{m}{2} \right)^{-1},
\end{align}
and
\begin{equation}\label{label:def_zeta}
      \zeta := \left\{ \begin{array}{ll}
         \frac{1}{2(L+m)\lam} \quad & \mbox{if $\lam > \frac{1}{2(L+m)}$};\\
        1 & \mbox{if $\lam \le \frac{1}{2(L+m)}$}.\end{array} \right. 
        \end{equation}

% \begin{theorem}[Main Theorem] \label{thm:main1}
% Given $(\bar \eta,\bar \varepsilon) \in \R^2_{++}$,
% PBF  with $\delta$ chosen as
% \begin{equation}\label{eq:def-tildem}
% \delta =\delta(\bar \eta,\bar \varepsilon) := \min\left\{\frac{\bar \varepsilon }{2},\frac{\lam \bar \eta^2}{8(m^2\lam^2+ 4\lam m + 4)}\right\}
% \end{equation}
% generates 
% in at most	
% \begin{equation}\label{cmplx:total}
% 		{\cal O}_1 \left( \left [\frac{\lam M^2}{\delta} + \lam (L+m) \log^+\left( \bar \sigma \right) \right] \frac{  (\hat M^\lam(\hat x_0) - \phi^*)(\lam m+1)}{\lam \bar \eta^2}\right)
% \end{equation}
% total iterations an iterate within the sequence $\{\hat y_k\}$ (as in step 3b of PBF)
% which is 
% a $(\bar \eta,\bar\varepsilon;m)$-regularized stationary point
%  of~$\phi$.
% \end{theorem}

% \begin{proposition}\label{thm:outer}
%  For a given tolerance pair $(\bar\eta,\bar \varepsilon) \in \R^2_{++}$, define
% \begin{equation}\label{def:K}
% K = K(\bar \eta,\bar\varepsilon) := \left\lceil \frac{8(m \lam +1)}{\lam \bar \eta^2} [\hat M^{\lam}(\hat x_0) - \phi^*] \right\rceil
% \end{equation}
%  where $\hat M^\lam(\cdot)$ is as in \eqref{eq:Moreau}.
% Then, PBF with $\delta$
% as in \eqref{eq:def-tildem}
% generates an iteration
% index $k \le K(\bar \eta,\bar \varepsilon)$
% such that
% \begin{equation}\label{ineq:termination}
%     \|\hat w_k\| \le \bar \eta,
% \quad \hat \varepsilon_k \le
% \bar \varepsilon.
% \end{equation}
% As a consequence,
% $\hat y_k$ is 
% a $(\bar \eta,\bar\varepsilon;m)$-regularized stationary point  of problem \eqref{eq:opt_problem}.
% \end{proposition}

\begin{theorem}[Main Theorem] \label{thm:main1}
% Given $(\bar \eta,\bar \varepsilon) \in \R^2_{++}$,
PBF 
generates 
a $(\bar \eta,\bar\varepsilon;m)$-regularized stationary point
 of~$\phi$
within the iteration sequence $\{\hat y_k\}$ (see step 3b of PBF)
in	
\begin{equation}\label{cmplx:total}
		{\cal O} \left( \left [\frac{\lam M^2}{\delta} + \lam (L+m) \log^+\left( \bar \sigma \right) + 1 \right] \left[ 2\left(\frac{1}{\bar \varepsilon} + \frac{8(m \lam +1)}{\lam \bar \eta^2} \right)(\hat M^{\lam}(\hat x_0) - \phi^*) +2 \right]\right) 
\end{equation}
% \[
% \lam = c/m
% \]
% \begin{equation}\label{cmplx:total}
% 		{\cal O} \left( \left [\frac{M^2}{\delta m} +  \frac{L}{m} \log^+\left( \bar \sigma \right) + 1 \right] \left[\frac{  m\left[ \hat M^\lam(\hat x_0) - \phi^* \right]}{\bar \eta^2}+1 \right]\right)  \quad (??? Renato????)
% \end{equation}
total iterations where $(\bar \eta,\bar \varepsilon)$ is as in step 0 of PBF and $\delta$ is as in \eqref{eq:def-tildem}.
% total iterations an iterate within the sequence $\{\hat y_k\}$ (as in step 3b of PBF)
% which is 
% a $(\bar \eta,\bar\varepsilon;m)$-regularized stationary point
%  of~$\phi$.
\end{theorem}



% We now make a remark about
% Theorem \ref{thm:main1}.
In terms of the tolerances
$\bar \eta$ and $\bar \varepsilon$ only, it follows
from Theorem \ref{thm:main1} that the
iteration complexity of
PBF to find a $(\bar \eta,\bar\varepsilon;m)$-regularized stationary point of $\phi$ is
 ${\cal O}_1(\bar \eta^{-2}\max \{ \bar \eta^{-2},\bar \varepsilon^{-1}\} )$.

For the sake of stating an iteration-complexity for
PBF to find a $(\rho;1/m)$-Moreau stationary point,
we consider the specific
case of PBF and Theorem \ref{thm:main1}
with 
parameters  $\lam$
chosen as $\lam = \Theta(m^{-1})$ and $L =0$.
In this case, it follows from \eqref{eq:def-tildem} that
% \[
% 1+m\lam=1+ m\lam = \Theta(1), \quad
% N= 8 (1+m\lam) = \Theta(1),
% \]
% and hence that
\begin{equation}\label{eq:simdelta}
\delta = {\cal O}\left( \min \left \{ \frac{\bar \eta^2}{m},  \bar \varepsilon \right \}  \right).
\end{equation}
Thus, the complexity bound
\eqref{cmplx:total} becomes
\begin{equation}\label{cmplx:total'}
   {\cal O}  \left(  \frac{M^2[\hat M^{\lam}(\hat x_0) - \phi^*]}{\delta \bar \eta^2 }       \right)
\end{equation}
which, in view of \eqref{eq:simdelta}, is
\begin{equation}\label{cmplx:total''}
   {\cal O}\left( \frac{\hat M^{\lam}(\hat x_0) - \phi^*}{\bar \eta^2} \left( M^2\max\left\{\frac{m }{\bar \eta^2 },\frac{ 1}{\bar \varepsilon}\right\} \right) \right).
\end{equation}

We are now ready to
state the iteration complexity
for PBF to find an approximate Moreau stationary point. 
\begin{corollary}
% Assume $L =0$.
For any given $\rho>0$, PBF with  $\lam = 1/m$ and 
$(\bar \eta,\bar \varepsilon)$ given by
\[
\bar \eta = \frac{\rho}8, \quad \bar \varepsilon=  \frac{\rho^2}{2592m}, 
\]
applied to an instance of \eqref{eq:opt_problem}
satisfying \eqref{cond:A2} with $L=0$
generates 
a  $(\rho;1/m)$-Moreau stationary point
 of~$\phi$
within the iteration sequence $\{\hat y_k\}$
in	
% finds in
\begin{equation}\label{new_comp}
    % \tilde {\cal O}_1 \left(  \frac{(\phi(\hat x_0) - \phi^*)M^2m}{\rho^4} + \frac{L(\phi(\hat x_0)- \phi^*)}{\rho^2} \right)
    {\cal O}_1 \left(  \frac{M^2 m [\hat M^{\lam}(\hat x_0) - \phi^*]}{\rho^4}  \right)  
\end{equation}
 total iterations.
% an iterate within the sequence $\{\hat y_k\}$
% which is a 
% $(\rho;1/m)$-Moreau stationary point of $\phi$.
% \begin{equation}\label{eq:Moresta}
% \left\|\nabla M_\phi^{1 / (2m)}\left(x\right)\right\| \le \rho
% \end{equation}
\end{corollary}

\begin{proof}
Note that the choice of $(\bar \eta, \bar \varepsilon)$ shows that PBF generates a $(\rho;1/m)$-Moreau stationary point of $\phi$ in view of Proposition \ref{key:rela}(b). The choice of $(\bar \eta, \bar \varepsilon, 1)  $ and relation \eqref{eq:simdelta} imply that 
\[
\delta = \Theta\left(\frac{\rho^2}{m}\right).
\]
The conclusion of the
corollary now follows
from
the above relation and \eqref{cmplx:total''}.
% then imply that the complexity for PBF to find a $(\rho;1/(2m))$-Moreau stationary point is at most \eqref{new_comp}.   
\end{proof}

% We now compare
% bound ??? with the iteration-complexity of Algorithm ????.
It is worth noting that the iteration-complexity of 
the PS method of Subsection \ref{SEC:SCS}
is not better than 
the one for
PBF with  $\lam =1/m$ and $L=0$, namely,
bound
\eqref{new_comp},
and the first
bound on the right hand side of \eqref{eq:MoreauT} equals the latter one only when
\[
\gamma= {\Theta}\left(\frac1M\sqrt{\frac{\hat M^{\lam}(\hat x_0)-\phi^*}m} \right).
\]
However, this choice of $\gamma$ is generally not
computable
because the quantity
 $\hat M^{\lam}(\hat x_0) - \phi^*$
and
the parameter $M$ 
may not be known. 
% It is also possible that this choice of $\gamma$ may even be outside the range constrain for $\gamma$ stated in Proposition \ref{prop:Davis}.



\section{Proof of Theorem \ref{thm:main1}}
\label{sec:proof}


This section contains three subsections.
The first one derives a preliminary bound on the length of each cycle in terms of the tolerance $\delta$.
The second one bounds the number of
cycles generated by PBF with a specific choice
of $\delta$ until it obtains
a $(\bar \eta,\bar \varepsilon;m)$-regularized stationary point of $\phi$.
Finally, the third one derives the total iteration complexity of PBF with the aforementioned choice of $\delta$
until it obtains
a $(\bar \eta,\bar \varepsilon;m)$-regularized stationary point of $\phi$.

\subsection{Bounding the cardinalities of the cycles}\label{subsec:length}
This section
establishes a preliminary  upper bound on the cardinality of
each cycle ${\cal C}_k$
defined in \eqref{def:Ck}, and hence on
the number of null iterations
between two consecutive serious ones.

Throughout this section
and the next one, we let
\beq \label{eq:hatfk}
\hat f_{k}(\cdot) = f_m(\cdot;\hat y_{k-1}).
\eeq


	
	 The first result below presents a few basic properties of the null iterations between two consecutive serious ones.
	
	

	
	\begin{lemma}\label{lem:101}
	    For every $ j \in {\cal C}_k\setminus\{i_k\}$, the following statements hold:
	 \begin{itemize}
	        \item[a)] 
         there exists $\bar \Gamma_{j-1} \in \bConv{n} $ such that
         % $\bar \Gamma_{j-1} \le\hat f_{k} + h $ and
	    \begin{align}
	            &\max \{ \bar \Gamma_{j-1} \,,\, \ell_{\hat f_{k}}(\cdot;x_{j-1})+h\} \le \Gamma_{j}, \label{eq:Gamma_j} \\
	            &\bar \Gamma_{j-1}(x_{j-1}) = \Gamma_{j-1}(x_{j-1}), \quad 
	        x_{j-1} = \underset{u\in \R^n}\argmin \left \{\bar \Gamma_{j-1} (u)  + \frac{1}{2\lambda} \|u-\hat y_{{k-1}}\|^2 \right\}; \label{eq:relation}
	    \end{align} 
	        \item[b)] for every
	    $u\in \dom h$, there holds
         \begin{equation}\label{eq:strcon}      
    \bar \Gamma_{j-1}(u)  + \frac{1}{2\lambda} \|u-\hat y_{k-1}\|^2 \ge \theta_{j-1}+ \frac{1}{2\lambda}\|u-x_{j-1}\|^2;
    \end{equation}
	    \end{itemize}
	\end{lemma}
%     \noindent\rule[0.5ex]{1\columnwidth}{1pt}
	
% 	BUF%$(\Gamma,x^c,\lam,\tau)$

%     \noindent\rule[0.5ex]{1\columnwidth}{1pt}
%     {\bf Input:}  $g, h \in \bConv{n}$, $\lam \in \R_{++} $ and
%     $(x^c,x,\Gamma) \in \R^n \times
%     \R^n \times {\cal B}(g+h)$ such that 
%       \begin{equation}\label{eq:x-pre}
%     x = \underset{u\in \R^n}\argmin \left \{\Gamma (u) + \frac{1}{2\lam} \|u-x^c\|^2 \right\};
% \end{equation}
%     \begin{itemize}
%         % \item 
%         % compute $x$ as in \eqref{eq:x-pre};
%         \item
%         find bundle $\Gamma^+ \in   {\cal B}(g+h)$ satisfying
% \begin{equation}\label{def:Gamma}
% 	     \Gamma^+ (\cdot) \ge  \max \{ \bar \Gamma(\cdot)  , \ell_g(\cdot;x)+h(\cdot)\},
% 	\end{equation}
%    for some
%    $\bar \Gamma(\cdot) \in \bConv{n}$ such that
% \begin{equation}\label{def:bar Gamma}
%  \bar \Gamma(x) = \Gamma(x), \quad 
% 	x = \underset{u\in \R^n}\argmin \left \{\bar \Gamma (u) + \frac{1}{2\lam} \|u-x^c\|^2 \right\}.
% 	\end{equation}
%      \end{itemize}
%      {\bf Output:} $\Gamma^+$.    
%   \noindent\rule[0.5ex]{1\columnwidth}{1pt}	
% 	BUF
%     \noindent\rule[0.5ex]{1\columnwidth}{1pt}
%     {\bf Input:}
%     $g, h \in \bConv{n}$, $\lam \in \R_{++}$, and
%     $(x^c,x,\Gamma) \in \R^n \times
%     \R^n \times \bConv{n}$ such that $\Gamma\le g+h$ and \eqref{eq:x-pre} holds;
%   %   \beq
% 	 % x = \underset{u\in \R^n}\argmin \left \{\Gamma (u) + \frac{1}{2\lam} \|u-x^c\|^2 \right\};
%   % \eeq
%     \begin{itemize}
%         % \item 
%         % compute $x$ as in \eqref{eq:x-pre};
%         \item
%         find function $\Gamma^+$ such that
% \begin{equation}\label{def:Gamma}
% 	    \Gamma^+ \in \bConv{n}, \qquad \max \{ \bar \Gamma(\cdot), \,\ell_g(\cdot;x)+h(\cdot) \}  \le \Gamma^+(\cdot) \le (g+h)(\cdot),
% 	\end{equation}
%    where $\ell_g(\cdot;\cdot)$ is as in \eqref{def:l_g} and
%    $\bar \Gamma(\cdot) $ is such that
% \begin{equation}\label{def:bar Gamma} 
% 	\bar \Gamma \in \bConv{n}, \quad
%  \bar \Gamma(x) = \Gamma(x), \quad 
% 	x = \underset{u\in \R^n}\argmin \left \{\bar \Gamma (u) + \frac{1}{2\lam} \|u-x^c\|^2 \right\}.
% 	\end{equation}
%      \end{itemize}
%      {\bf Output:} $\Gamma^+$.
     
%     \noindent\rule[0.5ex]{1\columnwidth}{1pt}

% \noindent\rule[0.5ex]{1\columnwidth}{1pt}
	\begin{proof}
	    a) Note that $  \Gamma_{j}$ is the output of the BUF  with input $\lam$ and $(x^c,x,\Gamma) = (\hat y_{k-1}, x_{j-1},\Gamma_{j-1})$ (see the null update in step 3 of PBF). The statement then follows from  this observation and relations \eqref{def:Gamma} and \eqref{def:bar Gamma} with $ \bar \Gamma = \bar \Gamma_{j-1} $.     
  
	   %  b) It follows from the fact that $\bar \Gamma_{j}\in \bConv{n}$  
	   % % ${\cal C}_1(\phi_m)$
	   %  that $\bar \Gamma_{j} + \frac{1}{2\lambda}\|\cdot-\hat y_{k-1}\|^2/2 $ is $\frac{1}{\lambda}$-strongly convex.
	    b) Using the second identity in \eqref{eq:relation}
     and the fact that
     $\bar \Gamma_{j-1}\in \bConv{n}$, we have for every $u\in\dom h$,
	    \[
	    \bar \Gamma_{j-1}(u)  + \frac{1}{2\lambda} \|u-\hat y_{k-1}\|^2  \ge \bar \Gamma_{j-1}(x_{j-1}) + \frac{1}{2\lambda} \|x_{j-1}- \hat y_{k-1}\|^2  + \frac{1}{2\lambda} \|u-x_{j-1}\|^2 .
	    \]
	    The statement now follows from the above inequality, the first identity in \eqref{eq:relation}, and the definition of $\theta_{j-1}$ in \eqref{ineq:hpe1}.
	\end{proof}
	
The next lemma presents some basic facts about $\{\theta_j\}$ and $\{t_j\}$.

    \begin{lemma}\label{lem:theta_t}
        For every $ j \in {\cal C}_k\setminus\{i_k\}$, the following inequalities hold: 
   \begin{align}      
 \theta_{j} &\ge \theta_{j-1}+\frac{1}{2 \lam}\|x_{j}-x_{j-1}\|^2,\label{ineq:theta_j}\\
			t_{j} &+ \frac1{2\lam} \|x_{j}-x_{j-1}\|^2 \le t_{j-1}\label{ineq:t_j}.
   \end{align}
    \end{lemma}

    \begin{proof}
         Using the definition of $\theta_j$ in \eqref{def:thetaj}, \eqref{eq:Gamma_j}, and \eqref{eq:strcon} with $u=x_j$, we have 
    \begin{align*}
		\theta_{j}&\overset{\eqref{def:thetaj}}=\Gamma_{j}(x_{j})+\frac{1}{2\lam}\|x_{j}-\hat y_{k-1}\|^2 \nn \\
		&\overset{\eqref{eq:Gamma_j}}\ge \bar \Gamma_{j-1}(x_{j})+\frac{1}{2\lam}\|x_{j}- \hat y_{k-1}\|^2
		\overset{\eqref{eq:strcon}}\ge \theta_{j-1}+\frac{1}{2 \lam}\|x_{j}-x_{j-1}\|^2 
	\end{align*}
    and thus \eqref{ineq:theta_j} holds.
 Using the definition of $ t_j $  in \eqref{ineq:hpe1}, \eqref{eq:minseq}, and \eqref{ineq:theta_j}, we have
		\begin{align*}
	 t_{j} & \stackrel{\eqref{ineq:hpe1}}=\phi_{ m}(y_{j};\hat y_{k-1}) + \frac{
  1}{2\lam} \|y_j - \hat y_{k-1}\|^2 - \theta_{j} \stackrel{\eqref{eq:minseq}}\le \phi_{ m}(y_{j-1};\hat y_{k-1})  + \frac{1}{2\lam} \|y_{j-1} - \hat y_{k-1}\|^2  - \theta_{j}  \\
            & \stackrel{\eqref{ineq:theta_j}}\le \phi_{m}(y_{j-1};\hat y_{k-1}) + \frac{1}{2\lam} \|y_{j-1} - \hat y_{k-1}\|^2  - \theta_{j-1} - \frac{1}{2\lam} \|x_{j} - x_{j-1}\|^2 \stackrel{\eqref{ineq:hpe1}} = t_{j-1} - \frac{1}{2\lam} \|x_{j} - x_{j-1}\|^2
		\end{align*}
        and thus \eqref{ineq:t_j} holds.
    \end{proof}

    
	% \textbf{Remark about one cut} \\
 %    Mono of tj Note that Lemma \ref{lem:theta_t}(a) does not apply to the one cut bundle update described in Subsection 3.1 of \cite{liang2024unified}.
    The following technical result provides an important recursive formula  for $\{t_j\}$.
	

	
% \begin{lemma}
% For every $j \in {\cal C}_k \setminus \{j_k\}$ there holds
%     \begin{equation}\label{eq:thetaj}
%     \theta_{j+1} \ge \theta_j+\frac{1}{2 \lam}\|x_{j+1}-x_j\|^2.
%     \end{equation}
% \end{lemma}

% \begin{proof}

% \end{proof}
  

 \begin{lemma}\label{lem:basic2}
 For every
  $ j \in {\cal C}_k\setminus\{i_k\}$, we have:
  % \begin{itemize}
		% 	\item[a)] 
		% 	$
		% 	t_{j} + \frac1{2\lam} \|x_{j}-x_{j-1}\|^2 \le t_{j-1};
		% 	$
		% 	\item[b)] 
			
			% \item[c)] 
			\begin{equation}	    
			t_{j}\left(1 +  \frac{t_{j}}{  4\lam(2M^2 +   (L+m) t_{j})  }\right)  \le t_{j-1}.
		% \end{itemize}
        \end{equation}
	\end{lemma}
	\begin{proof}		
         % Note that \eqref{def:Gamma} with $g = \hat f_k$ and  $(x^c,x,\Gamma) = (\hat y_{k-1}, x_{j-1}, \Gamma_{j-1})$ implies that
  % \begin{equation}\label{ineq:gammalinear}
  % \Gamma_j(\cdot) \ge \ell_{\hat f_k}(\cdot;x_{j-1}) + h(\cdot).
  % \end{equation}
  Using the definition of $t_j$ in \eqref{ineq:hpe1}, \eqref{eq:minseq}, and the definition of $\theta_j$ in \eqref{def:thetaj}, we conclude that 
		\begin{align*}
		t_j
  & \stackrel{\eqref{ineq:hpe1}}= 
  \phi_{ m}(y_j;\hat y_{k-1}) + \frac{1}{2\lam} \|y_j - \hat y_{k-1}\|^2  - \theta_j 
  \stackrel{  \eqref{eq:minseq}} \le
\phi_{ m}(x_j;\hat y_{k-1})   + \frac{1}{2\lam} \|x_j - \hat y_{k-1}\|^2  - \theta_j \\
&\stackrel{  \eqref{def:thetaj}}=
  \phi_{m} (x_j;\hat y_{k-1})    - \Gamma_j(x_j) . 
  \end{align*}
  The above inequality,
  relations \eqref{eq:Gamma_j} and \eqref{eq:fm-linear}, and
  the fact that $\phi_{ m}(x_j;\hat y_{k-1}) = f_{ m}(x_j;\hat y_{k-1})+h(x_j)$, imply that
  \begin{align*}
0  &  \le \phi_m(x_j;\hat y_{k-1}) - (\ell_{\hat f_k}(x_j;x_{j-1})+h(x_j))-t_j\\
&\stackrel{\eqref{eq:Gamma_j}} = f_m(x_j;\hat y_{k-1}) - \ell_{\hat f_k}(x_j;x_{j-1})-t_j\\
		& \stackrel{\eqref{eq:fm-linear}}\le \frac{L+m}{2} \|x_j-x_{j-1}\|^2 + 2M \|x_j-x_{j-1}\|  -t_j,
		\end{align*}
% Rewrite the above inequality  as follows
% 		\[
% 		\frac{L+m}{2}  \| x_j-x_{j-1}\|^2 + 2M \|x_j-x_{j-1}\|- t_j \ge 0,
% 		\]
	which, together with $t_j \ge 0$ due to \eqref{ineq:tilde-tj},  can be easily seen to imply that 
    % $ \| x_j-x_{j-1}\| $ and using $t_j \ge 0$ in view of \eqref{ineq:tilde-tj}, we have
		\[
		\|x_j-x_{j-1}\| \ge \frac{-2M + \sqrt{4M^2 + 2 (L+m) t_j}}{L+m}= \frac{2 t_j}{ 2M + \sqrt{4M^2 + 2 (L+m) t_j}} \ge \frac{t_j}{\sqrt{4M^2 + 2 (L+m) t_j}}.
		\]
		The statement now follows from \eqref{ineq:t_j} and the above inequality.
	\end{proof}
% We are now ready to present the inner complexity. 
 % Take
 % \[
 % a = \lam M^2, \quad b = \lam(L+m)
 % \]
 % \[
 %  \tilde t_l = t_{l+i_k}
 %  \]
  
	% \begin{lemma}\label{lem:induction}
	% 	Assume that for some $c>0$ and $ l \in {\cal C}_k $ the sequence of nonnegative scalars $\{t_j\}$
	% 	satisfies
	% 	\begin{equation}\label{ineq:recursive}
	% 	\quad t_{j+1} + \frac{t_{j+1}^2}{a
 %  + b t_{l+1}} \le t_j, \ \ \forall j \ge l+1, j\in {\cal C}_k.
	% 	\end{equation}
	% 	%Then, for every $j \ge 1$, we have
	% 	%\[
	% 	%t_j \le \frac{\max\{2c, t_1\}}{j}
	% 	%\]
	% 	Then, for every $ j\in {\cal C}_k$ such that $ j \ge l+1 $, we have
	% 	\begin{equation}\label{ineq:induction}
	% 	t_j \le \frac{2a+2b{t_{l+1}}}{j-l}
	% 	\end{equation}
	% \end{lemma}
	% \begin{proof}
	% 	We prove this lemma by induction on $ j\ge l+1 $.
	% 	It is easy to verify that the base case holds, i.e., \eqref{ineq:induction} holds with $ j=l+1 $.
	% 	Now assume for the induction argument that \eqref{ineq:induction} holds for $ j\ge l+1 $.
	% 	Using \eqref{ineq:recursive}, \eqref{ineq:induction} and the fact that $ c\le \bar c $, we have
	% 	\[
	% 	t_{j+1} + \frac{t_{j+1}^2}{\bar c} \le t_{j+1} + \frac{t_{j+1}^2}{c} \le t_j\le \frac{2\bar c}{j-l},
	% 	\]
	% 	and hence
	% 	\[
	% 	t_{j+1}^2 + \bar c t_{j+1} -\frac{2\bar c^2}{j-l} \le 0.
	% 	\]
	% 	Solving the above quadratic inequality, we obtain 
	% 	\[
	% 	t_{j+1} %\le \frac{-\bar c+\sqrt{\bar c^2+\frac{8 \bar c^2}{j-l}}}{2}
	% 	\le \frac{\bar c}{2}\left( -1+\sqrt{1+\frac{8}{j-l}}\right)
	% 	\le \frac{\bar c}{2}\left( -1+\sqrt{1 +\frac{8}{j+1-l} +\frac{16}{(j+1-l)^2}} \right)=\frac{2\bar c}{j+1 - l}
	% 	\]
	% 	where the second inequality holds for $ j\ge l+1 $. We have thus shown the conclusion of the lemma holds.
	% \end{proof}


 


%  	\begin{proof}
%  Define $\tilde \delta := \delta + a/(b+1)$ and
%  $\theta := 1/2(b+1)$.
%  % Let
%  % $\bar l := \sup \{ l \ge 1: q_l  \ge \tilde \delta\}$.
%  We first claim that
% \begin{equation}\label{ineq:j}
%  \inf \{ l \ge 1: q_l  \le \tilde \delta\} \le {\cal T}:= \log ^ + \frac{q_0}{\tilde \delta} / \log (1+\theta) \le \theta^{-1} \log ^+\frac{q_0}{\tilde \delta}.
% \end{equation}
% The claim is obvious if $q_0 < \tilde \delta$. To show the claim holds for $q_0 \ge \tilde \delta$ , assume for contradiction that there exists $j > {\cal T} $ such
% that $q_j >  \tilde \delta$.
% Using the fact
% $\{t_l\}$ is nonincreasing and the definition of $\theta$,
% we have that for every  $l=1,\ldots,j$,
%  \begin{align*}
%  \frac{q_l}{a + b q_l} \ge \frac{\tilde \delta}{a + b\tilde \delta} = 
%  \frac{\delta + a/(b+1)}{a + b\delta + ab/(b+1)} 
%  \ge 
%  \frac{\delta + a/(b+1)}{2 a + b\delta} 
%  % = \frac{\delta (b+1)+a}{(b+1)(2a +b) \delta}
%  \ge \frac{1}{2(b+1)} = \theta.
%  \end{align*}
%  Hence, 
%  in view of \eqref{ineq:recursive}, we conclude that
%  $
%  \theta^j q_{j} \le
%  % \frac{1}{1+ \frac{1}{2(b+1)}} 
%  \theta^{j-1} q_{j-1}
%   \le  \cdots \le 
%  % (\frac{2b+2}{2b+3})^j 
%  q_0 
%  % < \left( 1+ \frac{1}{2(b+1)} \right)^j \tilde \delta
%  $.
%  On the other hand, using the
%  fact that  $j > \cal T$ and the definition of $\cal T$ in \eqref{ineq:j}, we easily see that
%  $q_0 < (1+ \theta)^j \tilde \delta$. Combining the last two conclusions, it follows that $q_j < \tilde \delta$.
%  % where the strict inequality is due to the fact that  $j > \cal T$ and the definition of $\cal T$ in \eqref{ineq:j}. 
%  Since the last conclusion contradicts the fact that $q_j \ge \tilde \delta$, the claim follows.
% %  Thus we have
% %  \begin{equation}\label{ineq:j}
% %  j \le \log \frac{q_0}{\tilde \delta} / \log \frac{2b+3}{2b+2} \le 2(b+1) \log \frac{q_0}{\tilde \delta}.
% % \end{equation}

% Let $\bar l  := \sup \{ l \ge 1: q_l  \ge \tilde \delta\}$ and note that
% $\bar l < \infty$ by the above claim and that
% $q_{\bar l+1} < \tilde \delta$.
% It is easy to see from \eqref{ineq:recursive} that for any $l \ge \bar l + 1 $
%    \[
%    \frac{1}{q_{l}} - \frac{1}{q_{l-1}}  \ge \frac{1}{a
%   + (b+1) q_{l }} \ge \frac{1}{a
%   + (b+1) \tilde \delta},
%    \]
%    where the second inequality is due to the assumption that $a \ge 0$, $ b \ge 0$ and the fact that $q_{l} \le q_{\bar l+1} < \tilde \delta$ for any $l \ge \bar l + 1$.
%    Summing the above inequality from $l=\bar l +2$ to $l$ and using the fact that $q_{\bar l+1} < \tilde \delta$,
%    we conclude that
%    \[
%    \frac{1}{q_l}  \ge \frac{1}{q_{\bar l +1}} +  \frac{l-1-\bar l }{a
%   + (b+1) \tilde \delta} > \frac{1}{\tilde \delta} +  \frac{l-1-\bar l }{a
%   + (b+1) \tilde \delta}  \ge  \frac{l-\bar l }{a
%   + (b+1) \tilde \delta},
%    \]
%    and hence that
% \begin{equation}\label{ineq:side}
%        q_l \le \frac{a
%   + (b+1) \tilde \delta}{l-\bar l } = \frac{2a + (b+1)\delta }{l-\bar l } \quad \forall l \ge \bar l +2,
%    \end{equation}
%   where the equality is due to the definition of $\tilde \delta$.
%   Thus using the definition of $\tilde l$ in \eqref{def:tl} and the first claim in \eqref{ineq:j} we have
%   \[q_{\tilde l}\le \frac{2a+(b+1)  \delta }{\tilde l-\bar l } \le \frac{2a + (b+1)\delta }{(2a + (b+1) \delta)/\delta} = \delta.
%   \]
%   which implies the conslusion of the lemma.
% 	\end{proof}

	\begin{proposition}\label{prop:innercomp}
		For every cycle index $k \ge 1$ generated by PBF,
its size $|{\cal C}_k|$ is bounded by
    % $|{\cal C}_k| \le 2+ 4\lam (L+m) + \frac{16\lam M^2}{\delta} + [8\lam (L+m)+3] \log^+ \sigma_{k}$ for $l \in {\cal C}_k$ is 
		\begin{equation}\label{inner}
		N_k:=4+ 4\lam (L+m) + \frac{16\lam M^2}{\delta} + [8\lam (L+m)+3] \log^+ \sigma_{k}
		\end{equation}
  where
  \begin{equation}\label{def:tau} 
  \sigma_{k} := \frac{ t_{i_k}}{\delta +8\lam M^2/[4\lam (L+m)+1]}.
  \end{equation}
	\end{proposition}
	
	\begin{proof}	% It suffices to prove the number of iterations $l$ needed to find $t_l \le \delta$ is bounded by $N_k$ in view of the fact that $\hat \delta_k \ge \delta$. The proof of this claim is based on Lemma \ref{lem:keyrecur}. Note that  Lemma \ref{lem:basic2} implies that \eqref{ineq:recursive} is satisfied with $q_l = t_{l+i_k},a = 8\lam M^2$, and $b = 4\lam (L+m)$. Hence, Lemma \ref{lem:keyrecur} imply that there exists an  index $\hat l$ bounded by \eqref{inner} such that $t_{ \hat l+i_k} \le \delta$.
  %       ======================\\
        Suppose for contradiction that $|{\cal C}_k|> N_k$ and define $J =|{\cal C}_k|-2 $.
% This implies that
% there exists a nonnegative integer $J > 0$ such that  $J+1+i_k \in {\cal C}_k$ and $J+2> N_k$.
Since the $k$th cycle does not stop at iteration $j+i_k$ for any $j=0,\ldots,J$,
we have that
 \begin{equation}\label{vio2}
 q_j := t_{j+i_k}  > \delta  \quad \forall j = 0,\ldots,  J.
 \end{equation}
Observe that  Lemma \ref{lem:basic2} and \eqref{vio2} imply that assumptions \eqref{ineq:recursive1} and \eqref{ineq:recursive2} of Lemma \ref{lem:keyrecur} is satisfied with $a = 8\lam M^2$, $b = 4\lam (L+m)$, and $\{q_j\}_{j=0}^{J}$  . Hence, the conclusion of Lemma \ref{lem:keyrecur} and the definition of $N_k$ in \eqref{inner} imply that  $J\le N_k-2$ which contradicts with the assumption $|{\cal C}_k|=J+2 > N_k$.
	\end{proof}
% ==========================
% \[ t_{j}\left(1 +  t_{j}/[  4\lam(2M^2 +   (L+m) t_{j}) ] \right)  \le t_{j-1} 
% \]
% \[
% q_{l}\left(1 + \frac{q_{l}}{a + b q_{l}}\right) 
%   % =  q_{l} + \frac{q_{l}^2}{a
%   % + b q_{l}} 
%   \le q_{l-1},
% \]



% ===============================
%  ----------------------------
%  ---------------------------

%  {\bf Strategy for adaptively changing $(m,\lam)$}

% If \eqref{ineq:check} satisfies, then 
% \[
% m = 1.2m
% \]

%  $\lam$ is updated as follows:
%  \[
%  \lam = \lam/1.5 \quad \mbox{if $I> 300$}
%  \]
%  \[
%  \lam = 1.1 \lam \quad \mbox{if $I< 5$}
%  \]
%  otherwise $\lam$ the same

%  -----------------RENATO---------------

% \begin{lemma}\label{lem:keyrecur}
% 		Assume that for some nonnegative scalars $a$ and $b$ such that $a+b>0$ and
%   sequence of  positive scalars $\{q_l\}_{l \ge 0}$, we have that
% 		\begin{equation}\label{ineq:recursive}
% 		q_{l}\left(1 + \frac{q_{l}}{a + b q_{l}}\right) 
%   % =  q_{l} + \frac{q_{l}^2}{a
%   % + b q_{l}} 
%   \le q_{l-1}, \ \ \forall l \ge 1.
% 		\end{equation}
		% \begin{itemize}\label{lem:waiting}
		% 	\item[a)] for any $l > 0$, we have
		% 	\begin{equation}
		% 	\label{ineq:induction}	
		% 	q_l   \le 
		% 	%\frac{\max\{2c, t_l\}}{j-l}=
		% 	\frac{2a +   2b t_{0} }{l};
		% 	\end{equation}
		% 	\item[b)] 
		% 	an index $l$ such that
		% 	$q_l \le 2a/b$ can be
		% 	found in a number of iterations bounded by
		% 	\[
		% 	1+\lceil 4b \rceil \log\left( \frac{b t_0}{a}\right).
		% 	\]
		% \end{itemize}
 % Then, for any given $\delta>0$,
 % there exists an  index $\hat l \ge 1$ such that $q_{ \hat l} \le \delta$ and
 %  \begin{equation}\label{def:tl}
 %     \hat l \le (2b+3) \log ^+ \left(\frac{  q_0}{\delta + a/(b+1)}\right) + \frac{2a}{\delta} + b +2.
 %  \end{equation}
 %  \end{lemma}


\subsection{Bounding total number of cycles}\label{subsec:cycle}

% \red{Question: check the outer analysis, when $m=0$, can we recover Theorem 6.4 of \cite{liang2021proximal}}

The goal of this subsection is to
establish a bound on
 the total number of cycles generated by PBF.

% Recall that $\hat x_k$, $\hat y_k$, $\hat \Gamma_k$ denote the
% last $x_j$, $y_j$ and $\Gamma_j$
% generated within a cycle, i.e., the ones with $j=j_k$.


The following technical result
provides the main properties of
the sequences $\{\hat x_k\}$, $\{\hat y_k\}$ and $\{\hat \Gamma_k\}$
generated by PBF.
Recall that
$\hat x_k$, $\hat y_k$, and $\hat \Gamma_k$ denote the
last $x_j$, $y_j$, and $\Gamma_j$, respectively,
generated within 
the cycle ${\cal C}_k$, i.e., the ones with $j=j_k$.
% which are sufficent
% to establish a bound on
% the total number of cycles.

\begin{lemma}\label{lem:basic_prop}
The  following statements hold
for every $k \ge 1$:
\begin{itemize}
    \item[a)] $\hat x_{k}$ is
 the optimal solution 
 % and
 %    optimal value 
    of
    \begin{equation}
        \label{eq:optmality}
        \min_{u \in \R^n} \left\{ \hat \Gamma_{k}(u)+\frac{1}{2\lambda} \| u - \hat y_{k-1}  \|^2 \right\};
        \end{equation}
        hence, if $\hat \theta_k$ denotes the optimal value of \eqref{eq:optmality}, then
        \begin{equation}\label{eq:hattheta}
\hat \theta_k = \hat \Gamma_k(\hat x_k) +\frac{1}{2\lambda} \| \hat x_k - \hat y_{k-1}  \|^2;
        \end{equation}
\item[b)] there hold
\begin{equation}
\hat \Gamma_{k}(\cdot) \in \bConv{n}, \quad \hat \Gamma_{k}(\cdot) \le \phi_m (\cdot;\hat y_{k-1})
\label{eq:basic}
\end{equation} and
        \begin{equation}
        \label{eq:control}
     \phi_m (\hat y_k;\hat y_{k-1})+\frac{1}{2\lam} \|\hat y_k - \hat y_{k-1}\|^2
        - \hat \theta_{k} \le \hat \delta_k.
        \end{equation}
\end{itemize}
\end{lemma}
\begin{proof}
a) The definition of $ \hat x_k $ in step 3b of PBF  and \eqref{def:xj} imply that $\hat x_{k}$ is an  optimal solution
    of    \eqref{eq:optmality}.

b) 
 If $|C_k| = 1$, i.e., $j_k$ is also the first iteration of cycle $C_k$, then \eqref{eq:basic} follows from
 the fact that $\hat \Gamma_k(\cdot) = \Gamma_{i_k}(\cdot) \in {\cal B}(\phi_m(\cdot;\hat y_k))$. If $|C_k| > 1$, then 
$\hat \Gamma_k = \Gamma_{j_k}$ is the output of
BUF with $(g,h)=(f_m (\cdot;\hat y_{k-1}),h)$, and hence satisfies \eqref{eq:basic} in view of \eqref{def:Gamma}.
Moreover, \eqref{eq:control} follows from the logic of the prox-center update rule in step 3 of PBF (see \eqref{ineq:hpe1}).
\end{proof}


The following result presents an important inclusion involving $(\hat y_k, \hat w_k, \hat \varepsilon_k)$
which implies that $\hat y_k$ is 
a $(\|\hat w_k\|, \hat \varepsilon_k;m)$-regularized stationary point of $\phi$.


\begin{lemma}
\label{lemma:optimality}
For every $k \ge 1 $, the quantities
$\hat x_k$, $\hat y_k$, $\hat v_k$, $\hat w_k$ and $\hat \varepsilon_k$ as in step 3b of PBF
satisfy
        \begin{equation}
           \hat \varepsilon_{k} \ge 0,  \qquad
           \hat v_{k} \in \partial_{\hat \varepsilon_{k}} [\phi_m (\cdot;\hat y_{k-1})] (\hat y_{k}),
           \label{eq:key_inclusion}
        \end{equation}
        \begin{equation}
              \hat w_{k} \in  \partial_{\hat \varepsilon_{k}} [\phi_m(\cdot;\hat y_{k}) ](\hat y_{k}).
              \label{eq:goal}
        \end{equation}
\end{lemma}
\begin{proof}
Since $\hat x_k$ is an optimal solution of \eqref{eq:optmality} in view of  Lemma \ref{lem:basic_prop}(a),
using
 the optimality condition 
 for \eqref{eq:optmality}, the fact that
 $\hat \Gamma_k \in \bConv{n}$, and the definition of $\hat v_k$ in \eqref{def:vk}, we have
$\hat v_{k} \in \partial \hat \Gamma_{k}(\hat x_{k})$.
This conclusion, \eqref{eq:basic}, the definition of subdifferential in \eqref{def:subdif}, 
and the definition of $\hat \varepsilon_{k}$ in \eqref{def:wk},
% in \eqref{def:var}, 
then imply that for every $u\in \dom h$,
\[
  \phi_m (u;\hat y_{k-1}) \ge  \hat \Gamma_{k}(u) \ge \hat \Gamma_{k}(\hat x_{k}) + \inner{\hat v_{k}}{u-\hat x_{k}} 
=\phi_m (\hat y_k;\hat y_{k-1}) + \inner{\hat v_{k}}{u-\hat y_{k}} - \hat \varepsilon_{k}
\]
% ----------
% \[
%  \sum \phi_m (u;\hat x_{i-1}) \ge \sum \phi_m (\hat y_i;\hat x_{i-1}) + \inner{\hat v_{i}}{u-\hat y_{i}} - \hat \varepsilon_{ i}
%  \]
%  ?????
%  \[
%  \sum \|u- x_i\|^2 = k \|u - x\|^2
%  \]
% ---------\\
% where the equality follows the definition
% of $\hat \varepsilon_{k}$ in \eqref{def:var}.
and hence that the inclusion in \eqref{eq:key_inclusion} holds.
The inequality in \eqref{eq:key_inclusion} follows from the above
inequality with $u=\hat y_{k}$.
Moreover,
the definition of $\hat w_{k}$ (see step 3b of PBF),
the inclusion in \eqref{eq:key_inclusion},
  and Lemma \ref{lem:chara_weakly} imply  that
\[
  \hat w_{k} \in \partial_{\hat \varepsilon_{k}} [\phi_m(\cdot;\hat y_{k-1}) ] (\hat y_{k})  - m(\hat y_{k} - \hat y_{k-1}) = \partial_{\hat \varepsilon_{k}} [\phi_m(\cdot;\hat y_{k}) ](\hat y_{k}),
\]
and hence that \eqref{eq:goal} holds.
\end{proof}

It follows from \eqref{eq:goal} that $\hat y_k$ is
a $(\|\hat w_k\|, \hat \varepsilon_k;m)$-regularized stationary point of $\phi$ where
the pair $(\hat w_k,\hat \varepsilon_k)$ can be easily computed according to step 3b of PBF.
Our remaining effort from now on will be to analyze the number of iterations it takes to obtain
an index $k$ such that
$\|\hat w_k\| \le \bar \eta$ and
$ \hat \varepsilon_k \le
\bar \varepsilon$,
and hence  
%the corresponding
% $\hat y_k$ is
a $(\bar \eta, \bar \varepsilon;m)$-regularized stationary point
$\hat y_k$ of $\phi$.


% Note that \eqref{eq:goal} is our key inclusion. The remaining work of this section is to bound $\hat \varepsilon_{k}$ and $\hat w_{k}$ .

The purpose of the following three
results is to establish a recursive
formula (see Lemma \ref{lem:key_recursive} below).
Lemma \ref{lemma:key_est} and Corollary \ref{cor:wk} 
are technical results that are needed
to prove Lemma \ref{lem:key_recursive}.

\begin{lemma} \label{lemma:key_est}
 For every $k \ge 1$, the quantities $\hat x_k$, $\hat y_k$, $\hat w_k$ and $\hat \varepsilon_k$,
 as in step 3b of PBF, satisfy
    \begin{equation}
    \label{eq:key_est}
        \hat \varepsilon_{k} +
       \frac{1}{2\lambda}\|\hat y_{k} - \hat x_{k}\|^2  \le 2 \delta + \frac{1 + m\lam}{2\lam}\|
       \hat y_k - \hat y_{k-1}\|^2
    \end{equation}
    and
    \begin{equation}
    \|\hat w_{k} \|^2 
 \le \frac{8\delta}{\lam} +  4 \left( m+\frac{1}{\lambda} \right)^2\|\hat y_{k} - \hat y_{k-1}\|^2  
 \label{eq:est_w}
    \end{equation}
   where $\lam$ and $\delta$ are as in 
    step 0 of PBF.
\end{lemma}
%   \\  

% ------------------

% \[
% \frac12 w_k ^2 \le \frac 12 ( w^2_{k-1} - w_k^2) + \frac{4\delta}{\lam} +  2 \left[\left( m+\frac{1}{\lambda} \right)^2 + \frac{m\lam+1}{\lam^2} \right]\|\hat y_{k} - \hat y_{k-1}\|^2  
% \]
% Want
% \[
% \max \left \{ \frac{\|w_k\|^2}{\eta^2} , \frac{\varepsilon_k}{\bar \varepsilon} \right\} \le 1
% \]
% which is implied by
% \[
% \frac{\|w_k\|^2}{\eta^2} + \frac{\varepsilon_k}{\bar \varepsilon} \le 1
% \]

% =======================
% \[
% \|\hat w_{k} \|^2 
%  \le \frac{4\delta}{\lam} +  2 \left[\left( m+\frac{1}{\lambda} \right)^2 + \frac{m\lam+1}{\lam^2} \right]\|\hat y_{k} - \hat y_{k-1}\|^2  -\frac{4 \hat\varepsilon
%  _k}{\lam}
% \]

%   ==========================\\
  

% \begin{equation}\label{ineq:check}
%     \hat \varepsilon_k + C\left[\delta + \left[\frac{(m\lam+1)^2}{\lam} + \frac{m\lam+1}{\lam} \right]\|\hat y_{k} - \hat y_{k-1}\|^2\right] \ge 0
% \end{equation}
% % \[
% %     \hat \varepsilon_k \le - C\left[\delta - \left[\frac{(m\lam+1)^2}{\lam} + \frac{m\lam+1}{\lam} \right]\|\hat y_{k} - \hat y_{k-1}\|^2\right] 
% % \]
%  {\bf Strategy for adaptively changing $(m,\lam)$}

% If \eqref{ineq:check} is not satisfied, then 
% \[
% m = 1.2m
% \]

%  $\lam$ is updated as follows:
%  \[
%  \lam = \lam/1.5 \quad \mbox{if $I> 300$}
%  \]
%  \[
%  \lam = 1.1 \lam \quad \mbox{if $I< 5$}
%  \]
%  otherwise $\lam$ the same
    



\begin{proof}
Using both statements (a) and (b) of Lemma \ref{lem:basic_prop},
the definitions of $\hat \varepsilon_{k}$ and $\hat v_k$ in \eqref{def:wk} and \eqref{def:vk}, respectively, we
conclude that
% \begin{equation}
%         \label{eq:control}
%      \hat f_k(\hat y_{k};\hat y_{k-1}) +\frac{1}{2\lam} \|\hat y_k-\hat y_{k-1}\|^2 
%         - \hat \theta_{k} \le \delta.
%         \end{equation}
\begin{align}
    \hat \varepsilon_{k} &= \phi_m (\hat y_k;\hat y_{k-1})  - \hat \Gamma_{k}(\hat x_{k}) - \inner{\hat v_{k}}{\hat y_{k} - \hat x_{k}} \nonumber  \\
    & \stackrel{\eqref{eq:control}}\le\hat \delta_k  - \frac{1}{2\lam}\|\hat y_k-\hat y_{k-1}\|^2
    + \hat \theta_k - \hat \Gamma_{k}(\hat x_{k}) - \inner{\hat v_{k}}{\hat y_{k} - \hat x_{k}}  \nonumber \\
    &\stackrel{\eqref{eq:hattheta}}= \hat \delta_k - \frac{1}{2\lam}\|\hat y_k-\hat y_{k-1}\|^2
    + \frac{1}{2\lambda}\|\hat x_{k}-\hat y_{k-1}\|^2 - \inner{\hat v_{k}}{\hat y_{k} - \hat x_{k}}  \nonumber \\
    &\stackrel{\eqref{def:vk}}= \hat \delta_k - \frac{1 }{2\lam}\|\hat y_k-\hat y_{k-1}\|^2 
    + \frac{1}{2\lam} \|\hat x_{k}-\hat y_{k-1}\|^2 + \frac{1}{\lam} \inner{\hat x_{k}-\hat y_{k-1}}{\hat y_{k} - \hat x_{k}} \nonumber  \\
    % &= \hat \delta_k  - \frac{1 }{2\lam}\|\hat y_k-\hat y_{k-1}\|^2
    % + \frac{1}{2\lam} \left(
    % \|\hat y_{k} - \hat y_{k-1}\|^2 -
    % \| \hat y_{k} - \hat x_{k}\|^2
    %  \right) \nonumber\\
     & = \hat \delta_k  
    - \frac{1}{2\lam}  
    \| \hat y_{k} - \hat x_{k}\|^2. \label{est_var}
    \end{align}
    % and hence that \eqref{eq:key_est} holds. 
The above inequality together with  the definition of $\hat w_k$ and $\hat \delta_k$ in step 3b of PBF implies that
% \[
% \hat w_{k} = \hat v_{k} - m(\hat y_{k} - \hat y_{k-1}) = \frac{1}{\lambda}(\hat y_{k} - \hat x_{k}) - \left(m+\frac{1}{\lambda}\right)(\hat y_{k} - \hat y_{k-1})
% \]
% and hence that
\begin{align*}
    \|\hat w_{k} \|^2 &\stackrel{\eqref{def:wj}}\le \frac{2}{\lam^2}\|\hat y_{k} - \hat x_{k}\|^2 + 2 \left( m+\frac{1}{\lambda} \right)^2\|\hat y_{k} - \hat y_{k-1}\|^2 \\&
 \overset{\eqref{est_var}}\le \frac{4\hat \delta_k}{\lam}  + 2 \left( m+\frac{1}{\lambda} \right)^2\|\hat y_{k} - \hat y_{k-1}\|^2\\
 & \stackrel{\eqref{def:wj}}= \frac{4\delta}{\lam }+\frac{1}{2(m\lam+1)}\|\hat w_k\|^2+ 2 \left( m+\frac{1}{\lambda} \right)^2\|\hat y_{k} - \hat y_{k-1}\|^2
\end{align*}
 where the first inequality is due to the relation $\|a+b\|^2 \le 2 \|a\|^2 + 2 \|b\|^2$. Inequality \eqref{eq:est_w} now follows from the above inequality and simple algebraic manipulation. Using \eqref{eq:est_w}, \eqref{est_var} and the definition of $\hat \delta_k$ in step 3b of PBF, we have
 \begin{align*}
 \hat \varepsilon_{k} +
       \frac{1}{2\lambda}\|\hat y_{k} - \hat x_{k}\|^2 & \stackrel{\eqref{def:wj},\eqref{est_var}}\le  \delta + \frac{\lam}{8(m \lam+1)}\|\hat w_k\|^2\\
       &\stackrel{\eqref{eq:est_w}}\le \delta + \frac{\lam}{8(m\lam+1)}\left(\frac{8\delta}{\lam}+ 4 \left( m+\frac{1}{\lambda} \right)^2\|\hat y_{k} - \hat y_{k-1}\|^2    \right)\\
       &\le 2 \delta + \frac{1 + m\lam}{2\lam}\|
       \hat y_k - \hat y_{k-1}\|^2,
 \end{align*}
 and thus \eqref{eq:key_est} holds.
\end{proof}



  
\begin{corollary} \label{cor:wk}
    If for some $k \ge 1$, we have $\hat y_k = \hat y_{k-1}$, then
    the following statements hold:
    \begin{itemize}
        \item[a)]
        $y_j = \hat y_{k-1}$ for all $j \in {\cal C}_k$;
        \item[b)]
        $\|\hat w_k\| \le \bar \eta/4$ and $\hat \varepsilon_k \le \bar \varepsilon/8$, and hence $\hat y_k$ is 
a $(\bar \eta/4,\bar\varepsilon/8;m)$-regularized stationary point  of problem \eqref{eq:opt_problem}.
    \end{itemize}
\end{corollary}


\begin{proof}
a) Denote    
    $\Psi(x)=\phi_m(x;\hat y_{k-1}) +\frac{1}{2\lam}\|x - \hat y_{k-1}\|^2$. Assume that for contradiction that 
$j $ is the first index 
in ${\cal C}_k$ such that $y_j \ne \hat y_{k-1}$, and hence that $y_j \ne y_{j-1}=\hat y_{k-1}$. Because of the test \eqref{def:txj} in step 1 of PBF, it follows that
$y_j = x_j$ and
$\Psi(y_{j-1})> \Psi(x_j)$, and hence that  
\[
\Psi(\hat y_{k-1}) =
\Psi(y_{j-1})> \Psi(x_j) = \Psi(y_j) \ge \Psi(\hat y_k) = \Psi(\hat y_{k-1}),
\]
which yields the desired contradiction.\\
b) Since $\hat y_k =\hat y_{k-1}$, \eqref{eq:est_w} implies
\begin{equation}\label{w}
\|\hat w_k\|^2 \le \frac{8\delta}{\lam} \le \frac{\bar \eta^2}{16} 
\end{equation}
where the last inequality is due the definition of $\delta$ in step 0 of PBF. 
% \red{Can the $\delta$ in step 0 of PBF be relaxed?}. 
Observe that \eqref{eq:key_est} implies that 
\begin{equation}\label{var}
\hat \varepsilon_k \le 2\delta \le \frac{\bar \varepsilon}{8}
\end{equation}
where the last inequality is due the definition of $\delta$ in step 0 of PBF.
Finally, \eqref{eq:goal}, \eqref{w}, \eqref{var}
and Definition~\ref{def:appr} then imply
that $\hat y_k$ is a
$(\bar \eta/4,\bar\varepsilon/8;m)$-regularized stationary point  of problem \eqref{eq:opt_problem}.  
\end{proof}

The above corollary indicates that whenever PBF has a repeated cycle which yields $\hat y_k = \hat y_{k-1}$ for some $k \ge 1$, the method will terminate and return a $(\bar \eta/4,\bar\varepsilon/8;m)$-regularized stationary point  of problem \eqref{eq:opt_problem}.

\begin{lemma}\label{lem:key_recursive}
For every  $k \ge 1$, the following statements are true:
 \begin{itemize}  \item[a)]there holds \begin{equation}\label{eq:bdd_diff}
  \phi(\hat y_k) + \frac12 \left( \frac1\lam + m \right)
  \|\hat y_k-\hat y_{k-1}\|^2 
  \le \phi(\hat y_{k-1})
\end{equation}
where
$\{\hat y_{k}\}$ is as in step 3b of PBF;
\item[b)] for every $k\ge 2$, there holds
\begin{equation}\label{eq:wvar_bdd}
    \frac{1}{2}\left(\frac{1}{\lam}+m\right)\sum_{l=2}^k \| \hat y_l - \hat y_{l-1}\|^2 \le \hat M^{\lam}(\hat x_0) -  \phi(\hat y_k)+2\delta .
\end{equation}
\end{itemize}
\end{lemma}

\begin{proof}
% For the sake of this proof, 
a) It follows from \eqref{def:phim} and \eqref{eq:minseq} that 
\begin{align*}
  \phi(\hat y_k) + \frac12 \left( \frac1\lam + m \right)
  \|\hat y_k-\hat y_{k-1}\|^2 
  \le  \phi(\hat y_{k-1}) + \frac12 \left( \frac1\lam + m \right)
  \|\hat y_{k-1}-\hat y_{k-1}\|^2  = \phi(\hat y_{k-1}),
    \end{align*}
 which proves \eqref{eq:bdd_diff}.  \\
 b) Summing \eqref{eq:bdd_diff} from $k = 2$ to $k$, we have
 \begin{equation}\label{sum2}
    \phi(\hat y_k) + \frac{1}{2}\left(\frac{1}{\lam}+m\right)\sum_{l=2}^k \|\hat y_l - \hat y_{l-1}\|^2 \le \phi(\hat y_1).
 \end{equation}
% Using \eqref{eq:control} with $k=1$, the definition of $\hat M^{\lam}(\hat x_0)$ in \eqref{eq:Moreau}, and the fact that $\hat \Gamma_1 (\cdot)\le \phi_m(\cdot;\hat x_0)$, we can conclude that
% \[
%  \phi(\hat y_1) + \frac{1}{2}\left(\frac{1}{\lam}+m\right) \|\hat y_1 - \hat y_{0}\|^2 \le \hat \theta_1 + \delta_1 \le \hat M^{\lam}(\hat x_0) +\delta.
Using \eqref{eq:control} with $k=1$, the definitions of $\hat M^{\lam}(\hat x_0)$ and $\theta_j$ in \eqref{eq:Moreau} and \eqref{def:thetaj}, respectively, and the fact that $\hat \Gamma_1 (\cdot)\le \phi_m(\cdot;\hat x_0)$, we can conclude that
\begin{align*}
 \phi(\hat y_1) + \frac{1}{2}\left(\frac{1}{\lam}+m\right) \|\hat y_1 - \hat y_{0}\|^2 &\stackrel{\eqref{eq:control}}\le \hat \theta_1 + \hat \delta_1 \stackrel{\eqref{eq:Moreau}, \eqref{def:thetaj}}\le \hat M^{\lam}(\hat x_0) +\hat \delta_1 \\
 & \stackrel{\eqref{def:wj}}=  \hat M^{\lam}(\hat x_0)+ \delta+\frac{\lam}{8(m\lam+1)}\|\hat w_1\|^2, 
 \end{align*}
 where the identity is due to \eqref{def:wj}.
It thus follows from \eqref{eq:est_w} that
 \[
 \phi(\hat y_1) + \frac{1}{2}\left(\frac{1}{\lam}+m\right) \|\hat y_1 - \hat y_{0}\|^2 \overset{\eqref{eq:est_w}}\le \hat M^{\lam}(\hat x_0) +2\delta+ \frac{1}{2}\left(\frac{1}{\lam}+m\right) \|\hat y_1 - \hat y_{0}\|^2,
\]
which together with \eqref{sum2} implies that \eqref{eq:wvar_bdd}.
\end{proof}

% {\bf Comments:} Can we also use another point other than $y^-$ in the
% above result? For example, $x^-$?}

% \begin{lemma}
% \label{lem:key_recursive}
% For every $k \ge 1$, define
% \begin{equation} \label{eq:def-tauk}
% \Delta_k:=\phi_{m} (\hat y_{k};\hat y_{k-1})   + \frac{1}{2\lam} \|\hat y_k - \hat y_{k-1}\|^2 .
%  %+ \frac{1}{2\lam}\|\hat y_{k}-\hat y_{k-1}\|^2
% % \label{eq:def-tau}
% % \end{equation}
% \end{equation}
% Then,  for every $k \ge 1$,
% we have
% \begin{equation}
% \Delta_{k+1}+\frac{1+m\lam}{2\lam} \|\hat y_{k} - \hat y_{k-1}\|^2 \le \Delta_k + (2+m\lambda) \delta
% \label{eq:recursive}
% \end{equation}
% and 
% \begin{align}
%  \frac{1+m\lam}{2\lam}& \sum_{l=1}^k \|\hat y_{l}-\hat x_{l-1}\|^2  \le
%   \hat M^{\lam}(\hat x_0) - \phi(\hat y_{k+1}) - \frac{1}{2}\left(m + \frac{1}{\lam}\right)\|\hat y_{k+1}-\hat x_k\|^2    + (3+m\lambda)k\delta  .
% \label{eq:wvar_bdd}
% \end{align}
% b) Summing \eqref{eq:wvar_bdd} from $k = 1$ to $k$ we have
% \[
% \phi(\hat y_k)+ \frac{1}{2}\left(\frac{1}{\lam}+m\right)\sum{l=1}^k \|y_i - y_{i-1}\|^2 \le \phi(\hat y_0)
% \]
% \end{lemma}

% \begin{proof}
% % First note that the strict inequality in \eqref{eq:def-alpha} directly follows from the second inequality in \eqref{rel:tau1}.
%     Now, using \eqref{eq:key_est}, \eqref{eq:bdd_diff}, and \eqref{eq:def-tauk}, and the definition of $\phi_m(\cdot;\cdot)$ in \eqref{def:phim}, 
% we conclude that
% \begin{align*}
% \Delta_{k+1} 
%    & \stackrel{\eqref{eq:bdd_diff}}{\le} \phi(\hat y_k)  + \frac{1+\lam m}{2\lam}\|\hat y_k-\hat    x_k\|^2
%    + \delta \\
%     & \stackrel{\eqref{eq:key_est}}\le  \phi(\hat y_k)  +  (1+m\lambda) \delta + \delta \\
%     &\stackrel{\eqref{eq:def-tauk}}=    \Delta_k - \frac{1+m\lam}{2\lam} \|\hat y_{k} - \hat y_{k-1}\|^2+ (2+m\lambda) \delta,
% \end{align*}
% and hence that \eqref{eq:recursive} holds.
%  Now,
% summing \eqref{eq:recursive} from $k=1$ to $k$
% we conclude that
% \begin{align}
%     \frac{1+m\lam}{2\lam} \sum_{l=1}^k \|\hat y_{l}-\hat x_{l-1}\|^2  &\le \Delta_1 - \Delta_{k+1}+
% (2+m\lambda) \delta k \label{eq:tau1}
% \end{align}
% Using \eqref{eq:control} with $k=1$, the definition of $\Delta_1$ in \eqref{eq:def-tauk}, the definition of $\hat M^{\lam}(\hat x_0)$ in \eqref{eq:Moreau}, and the fact that $\hat \Gamma_1 (\cdot)\le \phi_m(\cdot;\hat x_0)$, we can conclude that
% \[
% \Delta_1 \le \hat \theta_1 + \delta \le \hat M^{\lam}(\hat x_0) +\delta.
% \]
% Inequality \eqref{eq:wvar_bdd} now follows from the above relation, \eqref{eq:tau1} and the definition of $\Delta_k$ in \eqref{eq:def-tauk}. 
% \end{proof}


We are now ready to bound the total
number of cycles generated by PBF.


\begin{proposition}\label{thm:outer}
 For a given tolerance pair $(\bar\eta,\bar \varepsilon) \in \R^2_{++}$, define
\begin{equation}\label{def:K}
K = K(\bar \eta,\bar\varepsilon) := \left\lceil 2\left(\frac{1}{\bar \varepsilon} + \frac{8(m \lam +1)}{\lam \bar \eta^2} \right)(\hat M^{\lam}(\hat x_0) - \phi^*) +1 \right\rceil
\end{equation}
 where $\hat M^\lam(\cdot)$ is as in \eqref{eq:Moreau}.
Then, PBF with $\delta$
as in \eqref{eq:def-tildem}
generates an iteration
index $k \le K(\bar \eta,\bar \varepsilon)$
such that
\begin{equation}\label{ineq:termination}
    \|\hat w_k\| \le \bar \eta,
\quad \hat \varepsilon_k \le
\bar \varepsilon.
\end{equation}
As a consequence,
$\hat y_k$ is 
a $(\bar \eta,\bar\varepsilon;m)$-regularized stationary point  of problem \eqref{eq:opt_problem}.
\end{proposition}

\begin{proof}
Assuming by contradiction that PBF does not stop at the $K$-th cycle.
Then, the stopping criterion in step 3b of PBF implies that
\[
\max \left \{ \frac{\|\hat w_k\|^2}{\bar \eta^2} , \frac{\hat \varepsilon_k}{\bar \varepsilon} \right\} > 1 \quad \forall k=1,\ldots,K
\]
and thus 
\begin{equation}\label{vioooo}
\frac{\|\hat w_k\|^2}{\bar \eta^2} + \frac{\hat \varepsilon_k}{\bar \varepsilon} >1 \quad \forall k=1,\ldots,K.
\end{equation}
% Summing the above inequality from $k=1,\ldots,K$, we have
% \[
% \sum_{k=1}^K \left(\frac{\|\hat w_k\|^2}{\bar \eta^2} + \frac{\hat \varepsilon_k}{\bar \varepsilon}\right) > K,
% \]
Observe that the definitions of $\delta$ and $K$ in \eqref{eq:def-tildem} and \eqref{def:K}, respectively, imply that \begin{equation}\label{obsss}
    \left(\frac{4}{\bar \varepsilon} + \frac{16(m\lam+2)}{\lam \bar \eta^2} \right)\delta\le \frac{1}{2}, \quad \frac{1}{K-1}\left(\frac{1}{\bar \varepsilon}+ \frac{8(m\lam+1)}{\lam \bar \eta^2}\right)(  \hat M^{\lam}(\hat x_0) -  \phi^*) \le \frac{1}{2}.
\end{equation}
% Then, \eqref{eq:goal}, \eqref{ineq:termination},
% and Definition \ref{def:appr} imply
% that $\hat y_k$ is 
% a $(\bar \eta,\bar\varepsilon;m)$-regularized stationary point  of problem \eqref{eq:opt_problem}. Now assume PBF runs for $k'\ge K $ iterations.
% Using relation \eqref{eq:wvar_bdd} with $k = K$ and the fact that
% (A3) implies that $\phi(\hat y_{K+1})\ge \phi^*$, we have
% \begin{equation}\label{eq:sum_bdd}
%  \frac{1+m\lam}{2\lam} \sum_{l=1}^K\|\hat y_l - \hat y_{l-1}\|^2 \le  \hat M^{\lam}(\hat x_0) - \phi^*  +\delta .
% \end{equation}
% Let $k \in \{1,\ldots,K\}$ be such that
% \begin{equation}
%     \|\hat y_k - \hat y_{k-1}\| = \min_{1\le l\le K} \|\hat y_l - \hat y_{l-1}\|.
% \end{equation}
%  The above relation and  \eqref{eq:sum_bdd} thus imply that 
% \begin{equation}\label{eq:minindex}
% \frac{1+m\lam}{2\lam} \|\hat y_k - \hat y_{k-1}\|^2 \le \frac{ \hat M^{\lam}(\hat x_0)-\phi^*}{K}+\delta .
% \end{equation}
% This inequality together with  \eqref{eq:minindex} and \eqref{eq:est_w} imply that
% \begin{align}
%     \|\hat w_{k} \|^2 
%  &\le \frac{8\delta
% }{\lam} +   4 \left[\left( m+\frac{1}{\lambda} \right)^2 + \frac{m\lam+1}{\lam^2} \right]\|\hat y_{k} - \hat y_{k-1}\|^2   \nonumber \\
%  &\le \frac{8\delta}{\lam} +   4 \left[\left( m+\frac{1}{\lambda} \right)^2 + \frac{m\lam+1}{\lam^2} \right] \frac{2\lam}{1+m\lam}\left[ \frac{ \hat M^{\lam}(\hat x_0)-\phi^*+\delta}{K}  \right]\nonumber\\
%   &= \left[\frac{8}{\lam} + \left( m+\frac{1}{\lambda} \right)^2 \frac{2\lam}{1+m\lam}\right]\delta + \frac{8(m\lam+2)}{\lam}\frac{ \hat M^{\lam}(\hat x_0)-\phi^*}{K}\\
%   &\le \bar \eta^2 \label{eq:w}
% \end{align}
% This inequality together with \eqref{eq:key_est} imply that 
% \begin{align}
% \hat \varepsilon_k &\le  \delta + \frac{}{1+m\lam}\left [  \frac{\hat M^{\lam}(\hat x_0))-\phi^*}{K}+ (3+m\lam)\delta \right] \nonumber\\ 
% &  = \left(1+\frac{()(3+m\lam)}{1+m\lam}\right)\delta + \frac{()(\phi(\hat x_0)-\phi^*)}{1+m\lam K} \le \bar \varepsilon \label{eq:var}
% \end{align}
% where the last inequality is due to the definition of $\delta$ and $K$ in \eqref{eq:def-tildem} and \eqref{def:K}, respectively.
% Inequality \eqref{eq:est_w} then imply that
% \begin{align}
%    \|\hat w_{l} \|^2 
%  &\le \frac{8  \delta
% }{\lam} +  \sum_{l=1}^K\left( m+\frac{1}{\lambda} \right)^2 \|\hat y_{l} - \hat y_{l-1}\|^2 
% ?????\nonumber \\
%  &\le \frac{4 K\delta}{\lam} +  2\left( m+\frac{1}{\lambda} \right)\left[ \hat M^{\lam}(\hat x_0)-\phi^*+\delta \right]\label{sumw}
% \end{align}
%  and thus we have
%  \begin{align}
%   \min_{l=1,\ldots,K} \|w_l\|^2& \le 
%   \frac{8 \delta}{\lam} +  4\left( m+\frac{1}{\lambda} \right)\frac{\left[ \hat M^{\lam}(\hat x_0)-\phi^*+\delta \right]}{K}\nonumber\\
%   &\le \left[\frac{8}{\lam} + 4\left( m+\frac{1}{\lambda} \right) \right]\delta + 4\left( m+\frac{1}{\lambda} \right) \frac{ \hat M^{\lam}(\hat x_0)-\phi^*}{K}\\
%   &\le \bar \eta^2 \label{eq:w}
% \end{align}
% ===================\\
% Denote 
% \[
% A:= \left[\left( m+\frac{1}{\lambda} \right)^2 + \right]\frac{2\lam}{1+m\lam}
% \]
% \[
% B:=1
% +\frac{\lam( + m-m)}{1 
% +m \lam}
% \]
% Then choose  
% \[
% \delta = \min \{\frac{\bar \eta^2}{2(\frac{\lam}{4}+A)},\frac{1}{2B}\bar \varepsilon\}
% \]
% \[
% K= \max\{\frac{2A( \hat M^{\lam}(\hat x_0)-\phi^*)}{\bar \eta^2},\frac{2(B-1)( \hat M^{\lam}(\hat x_0)-\phi^*)}{\bar \varepsilon}\}
% \]
% ================\\
% where the last inequality is due to the definition of $\delta$ and $K$ in \eqref{eq:def-tildem} and \eqref{def:K}, respectively. 
 Moreover, using \eqref{eq:key_est},   \eqref{eq:est_w} and \eqref{vioooo}, we have
\begin{align*}
1 &\overset{\eqref{vioooo}}< \frac{\|\hat w_k\|^2}{\bar \eta^2} + \frac{\hat \varepsilon_k}{\bar \varepsilon} \le  \frac{ 2\delta + (1 + m\lam)\|\hat y_{k} - \hat y_{k-1}\|^2/(2\lam)}{
       \bar \varepsilon
       }
       +\frac{
   8\delta +  4 (m \lam+1)^2\|\hat y_{k} - \hat y_{k-1}\|^2/\lam  }{\lam \bar \eta^2}\\
   & = \frac{2\delta}{\bar \varepsilon} + \frac{8\delta}{\lam \bar \eta^2}+  \left( \frac{  1 + m\lam}{
       2\lam \bar \varepsilon
       }
       +\frac{
    4 (m \lam+1)^2 }{\lam^2\bar \eta^2}\right) \|\hat y_{k} - \hat y_{k-1}\|^2.
    \end{align*}
    Summing the above inequality from $k=2$ to $K$, dividing the resulting inequality by $K-1$, and using \eqref{eq:wvar_bdd}, we have
    \begin{align*}
   1 &< \frac{2\delta}{\bar \varepsilon} + \frac{8\delta}{\lam \bar \eta^2}+  \left( \frac{  1 + m\lam}{
       2\lam \bar \varepsilon
       }
       +\frac{
    4 (m \lam+1)^2}{\lam^2\bar \eta^2}\right) \frac{1}{K-1}\sum_{k=2}^K \|\hat y_k - \hat y_{k-1}\|^2  \\
    &\overset{\eqref{eq:wvar_bdd}}\le \frac{2\delta}{\bar \varepsilon} + \frac{8\delta}{\lam \bar \eta^2}+  \left( \frac{  1 + m\lam}{
       2\lam \bar \varepsilon
       }
       +\frac{
    4 (m \lam+1)^2}{\lam^2\bar \eta^2}\right) \frac{2\lam}{(\lam m +1)(K-1)}(  \hat M^{\lam}(\hat x_0) -  \phi(\hat y_K) + 2\delta ) \\
    & \le \left(\frac{4}{\bar \varepsilon} + \frac{16(m\lam+2)}{\lam \bar \eta^2} \right)\delta+ \frac{1}{K-1}\left(\frac{1}{\bar \varepsilon}+ \frac{8(m\lam+1)}{\lam \bar \eta^2}\right)(  \hat M^{\lam}(\hat x_0) -  \phi^*)    \le 1
\end{align*}
where the second last inequality is due to the fact that $\phi(\hat y_K) \ge \phi^*$ and the last inequality is due  to \eqref{obsss}. Hence, the above inequality gives the desired contradiction and thus the statement is proved.
%  Finally, \eqref{eq:goal}, \eqref{eq:w}, \eqref{var2}
% and Definition \ref{def:appr} then imply
% that $\hat y_k$ is a
% $(\bar \eta,\bar\varepsilon;m)$-regularized stationary point  of problem \eqref{eq:opt_problem}.
\end{proof}

Before ending this subsection, we
observe that
the quantity 
$ \hat M^{\lam}(\hat x_0) - \phi^*$ in \eqref{def:K} can be majorized by
the more standard
initial primal gap $\phi(\hat x_0) -\phi^*$ due to
the definition
of $ \hat M^{\lam}(\cdot)$ in \eqref{eq:Moreau}.

% above 
% Then the above relation with $k = K $ ,  the definition of $K$ and $\delta$ imply
%  \begin{align*}
%     \min_{1\le k \le K} \left\lbrace \left(1+m\lam + \frac{4}{\lam}\right)\hat \varepsilon_k +  \|\hat w_k\|^2 \right\rbrace
%     &\le \frac{\phi(\hat y_0)-\phi^*}{\lam N K} + \left(\frac{4  }{\lam} + \frac{3+m\lam}{\lam N}\right) \delta\\
%     &\le \min\{(1+m\lam 
%     + \frac{4}{\lam})\bar 
% \varepsilon,  \bar \eta^2\}
%  \end{align*}
%  and the conclusion of the theorem thus follows.



% The number of serious iterations $K$ performed by GPB until it obtains for the
% 	    first time an auxiliary serious iterate $\hat y_K$ such that $ \hat y_K $ is a $\eta$-approximate solution is bounded by
% 				\begin{equation}\label{bound:outer}
% 				    \min \left\lbrace \frac{d_0^2}{\lam \bar \varepsilon}, \frac{1}{1 \lam_1} \log\left(  \frac{1 d_0^2}{\bar \varepsilon}+1 \right) \right\rbrace + 1
% 				\end{equation}



% \begin{theorem}
% Define $T = \max\{m\sqrt{\frac{2 }{m - \bar m}} + \sqrt{\frac{2}{\lambda}}+ (m+\frac{1}{\lambda}) \sqrt{  2\lambda(\phi(\hat y_0) - \phi^*)+  2\lambda(2 + m\lambda) }, \sqrt{\frac{2}{m - \bar m}\}}$. Let $\delta = {\cal O}(\frac{\eta^2}{T^2})$ and $K ={\cal O}(\frac{1}{\delta})$ , then we can get a $(\eta,\eta)$-approximate solution $\hat y$, i.e. there exists a $y^*$ such that  $\|\hat{y}-y^*\| \leq \eta $ and $y^*$ is a $\eta$- approximate solution, i.e. there exists a $w^*$ such that 
% $w^* \in \partial \phi(y^*)$ and $\|w^*\| \le \eta$.
% \end{theorem}
% \begin{proof}
% Assume $T = \argmin_{0\le l \le k} \{ \| \hat w_{l
% +1 } \| \} $. 
% % Apply Lemma \ref{lemma:Pass_Opt} with $x = \hat x_T$ $ y = \hat y_{T+1}$, $u = \hat v_{T+1}$, $\varepsilon = \hat \varepsilon_{T+1}$,  and $1 = m - \bar m$ we get there exists a $y^*$ such that 
% % \[
% % y^*:=\underset{x^{\prime}}{\operatorname{argmin}}\left\{\phi\left(x^{\prime}\right)+\frac{m}{2 }\left\|x^{\prime}-\hat x_T\right\|^{2}-\left\langle \hat v_{T+1}, x^{\prime}\right\rangle\right\}
% % \]
% % and
% % \[
% % \|\hat y_{T+1} - y^*\| \le \sqrt{\frac{2 \hat \varepsilon_{T+1}}{m - \bar m }}
% % \]
% % Thus we have
% % \[
% % \|\hat y_{T+1} - y^*\| \le \sqrt{\frac{2 \hat \varepsilon_{T+1}}{m - \bar m }} \le \sqrt{\frac{2 \delta}{m - \bar m}} \le \eta
% % \]
% % and
% % \[\hat v_{T+1} \in \partial  \left(\phi+\frac{m}{2} \left\|\cdot-\hat x_T\right\|^{2}  \right)(y^*)
% % \]
% % The above inclusion suggests together with \ref{prop:equa} lemma that 
% % \[\hat v_{T+1} - m(y^* - \hat x_{T+1}) \in \partial\phi (y^*)
% % \]
% % Define $w^* = \hat v_{T+1} - m(y^* - \hat x_{T})$, then we have
% % \begin{align}
% % \|w^* \| 
% % &\le \|w^* - \hat w_{T+1}\| + \|\hat w_{T+1}\|\\
% % &\le m\|\hat y_{T+1} - y^*\| + \|\hat w_{T+1}\|\\
% % &\le m\|\hat y_{T+1} - y^*\| + \sqrt{\frac{2  \delta}{\lambda}} + (m+\frac{1}{\lambda}) \sqrt{  \frac{2\lambda(\phi(\hat y_0) - \phi^*)}{(k+1)}+  2\lambda(2 + m\lambda) \delta }\\
% % &\le \left ( m\sqrt{\frac{2 }{m - \bar m}} + \sqrt{\frac{2}{\lambda}}+ (m+\frac{1}{\lambda}) \sqrt{  2\lambda(\phi(\hat y_0) - \phi^*)+  2\lambda(2 + m\lambda) }  \right) \sqrt \delta\\
% % &\le \eta
% % \end{align}
% % This means that $y_{T+1}$ is our desired $(\eta,\eta)$-approximate solution.
% Use Lemma \ref{lemma:Pass_Opt} with $h = \phi +  \frac{m}{2} \|\cdot - \hat y_{T+1}\|$, $x = \hat y_{T+1}$, $ y = \hat y_{T+1}$, $u = \hat w_{T+1}$, $\varepsilon = \hat \varepsilon_{T+1}$,  and $1 = \lam$ to conclude that
% \[
% \hat w_{k+1} \in  \partial_{\hat \varepsilon_{k+1}} \left(\phi  + \frac{ m+\lam^{-1}}{2} \|\cdot-\hat y_{k+1}\|^2 \right)(\hat y_{k+1})
% = \partial_{\hat \varepsilon_{k+1}} \left( \tilde\hat f_k_{k+1}  + \frac{1}{2\lam} \|\cdot-\hat y_{k+1}\|^2 \right)(\hat y_{k+1})
% \]
% Hence, by the lemma in the appendix,
% there exists $\hat z_{k+1}$ such that
% \[
% \|\hat y_{k+1}- \hat z_{k+1}\| \le \sqrt{2\lam \varepsilon}, \quad
% \]
% and
% \[
% 0 \in \partial \left( \tilde\hat f_k_{k+1}  + \frac{1}{2\lam} \|\cdot-\hat y_{k+1}\|^2 - \inner{w_{k+1}}{\cdot} \right)(\hat z_{k+1})
% \]
% or
% \begin{align*}
%     \hat w_{k+1} \in \partial \tilde\hat f_k_{k+1} (\hat z_{k+1})
% + \frac1{\lam} (\hat z_{k+1} - \hat y_{k+1}) &= \partial \left( \phi + \frac{m}2 \|\cdot- \hat y_{k+1}\|^2 \right)  (\hat z_{k+1})
% +  \frac1{\lam} (\hat z_{k+1} - \hat y_{k+1}) \\
% & = \partial \phi(\hat z_{k+1}) +  \left( m + \frac1{\lam} \right) (\hat z_{k+1} - \hat y_{k+1})
% \end{align*}
% Hence
% \[
% \tilde w = \hat w_{k+1} -
%  \left( m + \frac1{\lam} \right) (\hat z_{k+1} - \hat y_{k+1})
%  \]
%  satifies
%  \[
%  \tilde w \in \partial \phi(\hat z_{k+1})
%  \]
%  \[
%  \| \tilde w\| \le \|\hat w\| +
%  \left( m + \frac1{\lam} \right)\sqrt{2\lam \varepsilon}
%  \]
 
% \[ 
%   \partial \phi(x) = \partial \left(\phi  + \frac{ m}2 \|\cdot-c\|^2 \right)(x) -  m(x-c)
%  \]
% \end{proof}

\subsection{Proof of Theorem \ref{thm:main1}}\label{subsec:proof}

Recall that Proposition \ref{thm:outer} bounds the
total number of cycles while
Proposition \ref{prop:innercomp} provides a
bound on the cardinality of every
cycle ${\cal C}_k$ in term of
$\sigma_{k}$. The following result
refines the latter result by
providing a uniform bound on
$\sigma_{k}$.




% ???????????????????????????????????????
\begin{lemma}\label{lem:t1} 
     Let $\delta$ and $K$ be as in \eqref{eq:def-tildem} and \eqref{def:K}, respectively,
     and define
\begin{equation}\label{def:bar t}
		    \bar t:=  M^2 + 2 \beta_2 \left\{\beta_1[\hat M^{\lam}(\hat x_0) - \phi^*] + 2\zeta \lam M^2 \right\} 
		\end{equation}
		where $\zeta$ is as in \eqref{label:def_zeta} ,  and $\beta_1$ and $\beta_2$ are as in \eqref{beta}. 
     Then,  the following statements hold:
     \begin{itemize}
         \item [a)]
         $ t_{i_k}\le \bar t$ for every  $ k = 1, \ldots, K$;
         \item[b)]
         for every $ k = 1, \ldots, K$, we have
         \begin{equation}\label{def:bartau}
         \sigma_{k} \le \bar \sigma 
          \end{equation}
         where $\sigma_k$ and $\bar \sigma$ are as in \eqref{def:tau} and \eqref{def:bar tau}, respectively.
     \end{itemize}	
	\end{lemma}

	
	\begin{proof}
a) Applying \eqref{eq:fm-linear1} with  $z = \hat y_{k-1}$ , we have
	\begin{equation}
    \label{eq:subeq2}
     f_{m}(x_{i_k};\hat y_{k-1}) - \ell_{ f}(x_{i_k};\hat y_{k-1}) \le 2  M \|x_{i_k}-\hat y_{k-1}\|  + \frac{ L + 
    m}2 \|x_{i_k}-\hat y_{k-1}\|^2.
	\end{equation}
	   	 Using this inequality, \eqref{eq:serious}, the definition of $\phi_m$  in \eqref{def:phim}, and 
 relations \eqref{def:txj} and \eqref{ineq:hpe1} with $j=i_k$ we have
	    \begin{align}
	        t_{i_k} & \stackrel{\eqref{ineq:hpe1}}= \phi_{m}(y_{i_k};\hat y_{k-1})  + \frac{1}{2\lam} \|y_{i_k} - \hat y_{k-1}\|^2 
	        -\theta_{i_k} \nonumber\\        &\stackrel{\eqref{def:thetaj}}=\phi_{m}(y_{i_k};\hat y_{k-1}) + \frac{1}{2\lam} \|y_{i_k} - \hat y_{k-1}\|^2  - \Gamma_{i_k}(x_{i_k}) - \frac{1}{2\lambda}\|x_{i_k}-\hat y_{k-1}\|^2 \nonumber\\
           &\stackrel{\eqref{def:txj},\eqref{eq:def-tildem}}\le \phi_{m}(x_{i_k};\hat y_{k-1})  - \Gamma_{i_k}(x_{i_k})  \nonumber\\
	        &\stackrel{\eqref{eq:serious}} \le f_m(x_{i_k};\hat y_{k-1}) - \ell_{f}(x_{i_k};\hat y_{k-1}) \nonumber\\
	        & \stackrel{\eqref{eq:subeq2}}\le 2 M \|x_{i_k}-\hat y_{k-1}\| + \frac{ L+m }{2}\|x_{i_k}-\hat y_{k-1}\|^2 \nonumber\\
	        &\le  M^2 + \left( \frac{ L+m}{2}+1\right)\|x_{i_k}-\hat y_{k-1}\|^2,
	        \label{eq:im4}
	    \end{align}
	  where the  last inequality is due to the fact that $2ab \le a^2+b^2$ with $a = M$ and $b = \|x_{i_k}-\hat y_{k-1}\|$.
   
	  We will now  bound $\|x_{i_k}-\hat y_{k-1}\|^2$.
 It follows from  \eqref{eq:wvar_bdd}
 that
	  \[
	      \frac{1}{2}\left(m + \frac{1}{\lam}\right)\|\hat y_{k}-\hat y_{k-1}\|^2 \le \hat M^{\lam}(\hat x_0) - \phi(\hat y_{k})     + 2\delta. 
	  \]
   which, in view of the definition of $\beta_1$ in \eqref{beta}, is equivalent to
	  \begin{equation}
	      \phi(\hat y_k) - \phi^* + \frac{1}{2\beta_1}\left(m + \frac{2}{\zeta \lam}\right)\|\hat y_{k}-\hat y_{k-1}\|^2 \le \hat M^{\lam}(\hat x_0) - \phi^*      + 2\delta.
	      \label{eq:im2}
	  \end{equation}
	   This inequality,
       Lemma \ref{lem:bdd_smdist} with $(\Gamma,z_0,u) = (\Gamma_{i_k},\hat y_{k-1},\hat y_k)$,  and 
   the facts that $\phi(\hat y_k) \ge \phi^*$ and $\beta_1 \ge 1$,  imply that
	  \begin{align}
	   \zeta^2\left(\frac{1}{4\zeta \lam}+\frac{m}{2} \right)  \|x_{i_k}- \hat y_{k-1} \|^2  - 4\zeta \lam M^2
	&\stackrel{ D.3}\le
	\phi(\hat y_k) - \phi ^ * + \frac12 \left(m+\frac{2}{\zeta \lam} \right) \|\hat y_k - \hat y_{k-1}\|^2  \nonumber \\
 &\stackrel{\eqref{eq:im2}} \le \beta_1 [ \hat M^{\lam}(\hat x_0)- \phi^*      + 2\delta] \nonumber\\
  % &\stackrel{\eqref{eq:Kdelta}}\le \beta_1 \left[ \phi(\hat x_0) - \phi^* + \gamma(\phi(\hat x_0) - \phi^*) + (3+m\lam)\delta  \right]
 &\le 2\beta_1  [ \hat M^{\lam}(\hat x_0)- \phi^* ]
	  \label{eq:im3}
	  \end{align}
 where the last inequality is due to the fact that $\delta \le 1$ in view of the definition of  $\delta$ in \eqref{eq:def-tildem}.
	The statement now follows by combining \eqref{eq:im4} and the above inequality, and using the definitions of $\beta_2$ and $\bar t$ in \eqref{beta} and \eqref{def:bar t}, respectively.
 
 b) This statement follows from a) and the definitions of $\sigma_{k}$ and $\bar \sigma$ in \eqref{def:tau} and \eqref{def:bar tau}, respectively.
 \end{proof}
 
% We are now ready to present the proof of Theorem \ref{thm:main1}.

We end this section by noting that
the proof of Theorem \ref{thm:main1}
 now follows immediately from Propositions~\ref{prop:innercomp} and \ref{thm:outer}, and Lemma \ref{lem:t1}(b).
% The conclusion of
% the theorem now follows from the choice of $\sigma$ in \eqref{eq:equaltau}. 


\section{Computational Results}\label{num}
% More generally, any function of the form
% $$
% g(x)=h(c(x))
% $$
% with $h$ convex and L-Lipschitz and $c$ a smooth map with C-Lipschitz Jacobian, is LC weakly convex. \\
% Lemma 4.2 (Weak convexity of the composition).
% The function $h \circ c$ is $\rho$-weakly convex on $\mathbf{R}^d$ for some $\rho \in[0, \mu]$.\\
% Proof. To simplify notation, set $\Phi:=h \circ c$. Fix two points $x, y \in \mathbf{R}^d$ and a vector $v \in \partial \Phi(x)$. We can write $v=\nabla c(x)^* w$ for some vector $w \in \partial h(c(x))$. Taking into account convexity of $h$ and the inequality $\|c(y)-c(x)-\nabla c(x)(y-x)\| \leq \frac{\beta}{2}\|y-x\|^2$, we then deduce
% $$
% \begin{aligned}
% \Phi(y)=h(c(y)) \geq h(c(x))+\langle w, c(y)-c(x)\rangle & \geq \Phi(x)+\langle w, \nabla c(x)(y-x)\rangle-\frac{\beta\|w\|}{2}\|y-x\|^2 \\
% & \geq \Phi(x)+\langle v, y-x\rangle-\frac{\mu}{2}\|y-x\|^2 .
% \end{aligned}
% $$
% \[
% c(y) - c(x) \ge  \nabla c(x)(y-x) - \frac{\beta}{2}\
% |y-x\|^2
% \]
% The result follows.
% \subsection{Problem 0 ?????}
% \[
% A(X) := (A_i \bullet X - b_i)_{i=1}^m \le 0
% \]
% \[
% \phi(X):= \max \{ 0 , A_1\bullet X - b_1,\ldots, A_m\bullet X - b_m\}
% \]
% \[
% \min \phi(RR^T)
% \]
% Prox-approach
% \[
% \min \left| tr[R^T AR] - b \right| + \frac1{2\lam} \|R-R_0\|^2
% \]
% \[
% A= \sum_{i=1}^p a_ia_i^T
% \]
% \[
% \min \left| \sum_{i=1}^p \|a_i^TR\|^2 - b \right| + \frac1{2\lam} \|R-R_0\|^2
% \]


% \textbf{To do:}

% 1. minimizing a weakly-convex function with the max (over $y$) structure, look for problems in papers about weakly-convex concave min-max optimization.

% e.g., Weakly-Convex Concave Min-Max Optimization: Provable Algorithms and Applications in Machine Learning

% our stopping criterion
% is based on the last

% Next we describe some further observations of the paper.
% First, it is worth noting that our analysis does not assume the domain of $\phi$ is bounded and establish the point-wise convergence instead of ergodic convergence. Second, PBF is a non-descent type algorithm. 


% We finally discuss some possible directions for future investigation. 
% First, as already mentioned in the remarks following the description of  PBF in Subsection~\ref{subsec:update}, PBF requires knowledge of a weakly convex parameter $m$
% (i.e., one that satisfies Assumption ????).
% ???????  so it is interesting to see whether one can develop a variant of PBF that is totally universal in the sense that it does not depend on the knowledge of $m$.

% \subsection{Problem 1 (WCC in the weakly convex concave minmax folder ????? )}
% Distributionally robust learning.  section 1.1 and 5.2 in \cite{rafique2022weakly} 
% $$
% \min _{\mathbf{x} \in \mathcal{X}} \max _{\mathbf{y} \in \mathcal{Y}} \sum_{i=1}^n y_i f_i(\mathbf{x})-r(\mathbf{y}),
% $$
% where $f_i(\mathbf{x})$ denotes the loss of a model $\mathbf{x}$ on the $i$ th data point, $\mathcal{Y}=\{\mathbf{y} \in$ $\left.\mathbb{R}^n \mid \sum_{i=1}^n y_i=1, y_i \geq 0, i=1, \ldots, n\right\}, \mathcal{X} \subseteq \mathbb{R}^d$ is a closed convex set, and $r: \mathcal{Y} \rightarrow \mathbb{R}$ is a closed convex function, e.g., $r(\mathbf{y})=\theta D(\mathbf{y}, \mathbf{1} / n)$ for some $\theta>0$, where $\mathbf{1}$ is an all-one vector and $D(\cdot, \cdot)$ denotes some distance measure (e.g., KL divergence and Euclidean distance) between two vectors. The loss $f_i(\mathbf{x})$ can be a non-convex function such as the loss function defined by a neural network or the non-convex truncated loss [39]. It is easy to see that (2) is a special case of (1).
% \[
% \mathbf{x}:=\left(\mathbf{X}_1, \mathbf{x}_2\right)
% \]
% where
% \begin{equation}
% f_i(\mathbf{x})=f\left(\mathbf{x} ; \mathbf{a}_i, b_i\right)=\max \left(1-b_i \sigma\left(\mathbf{X}_1 \mathbf{a}_i\right)^{\top} \mathbf{x}_2, 0\right)
% \end{equation}
% and 
% \[
% \sigma(z)=\left(\frac{1}{1+\exp \left(-z_i\right)}\right)_{i=1}^p
% \]
% {\bf Jiaming will suggest different choices of $f_i$}

% This subsection discusses the performance of the methods in our computational benchmark for solving a support vector machine (SVM) problem (see (4.1) in \cite{LanUniformly}). For given data points $ \{(u_i,v_i)\}_{i=1}^p $, where $ u_i\in \R^n $ is a feature vector and $ v_i\in \{-1,1\} $ denotes the corresponding label, we formulate the SVM
% problem as
% \begin{equation}\label{eq:SVM}
% 	\min \left\lbrace f(z):= \frac1p \sum_{i=1}^{p} \ell(u_i,v_i;z)+\frac{\lam}{2}\|z\|^2: z\in B_r \right\rbrace 
% \end{equation}
% where $ \ell(u_i,v_i;\cdot)=1-\tanh(v_i\inner{\cdot}{u_i}) $ is a nonconvex sigmoid loss function, $ \lam>0 $ is a regularization parameter and $B_r :=\{z\in \R^n:\|z\|\le r\} $ is a ball with radius $ r>0 $ and centered at the origin.

% The SVM problem \eqref{eq:SVM} is an instance of \eqref{eq:PenProb2Intro} where
% $ h $ is the indicator function of the ball $B_r$. We set $ \lam = 1/p$, $ r = 50 $ and $ \Omega = B_r $, where the set $ \Omega $ is introduced in (A2). 
% It can be shown that $ f $ is differentiable everywhere and satisfies
% \[
% 	m = M = L = \frac1p \sum_{i=1}^p \frac{4\sqrt{3}}{9}\|u_i\|^2 + \lam, \quad \forall i=1,\ldots,p.
% \]
This section reports the computational results for the PBF method of Subsection~\ref{subsec:update} against the PS method of Subsection~\ref{SEC:SCS}. It contains two subsections. The first one presents the computational results for the phase retrieval problem. The second one showcases the computational results for the blind deconvolution problem.
% and solving Lagrangian cut as an interesting application integer programming.

% This section is organized into three subsections. The first subsection provides details on the
% implementation of Ad-GPB. The remaining two subsections describe the results of the computational experiments mentioned above.
% \subsection{Implementation details}
% We start by describing how we choose parameters and how we implement BUF for Ad-GPB. For Ad-GPB and Ad-GPB, First, we set $\tau = 0.95$ and $\beta_0 = 1/2$. We point out that the following two applications both have the feature that the optimal value is known. This fact and  Lemma \ref{iterate} suggest that we don't need to compute all the quantities in \eqref{def:phik}, \eqref{def:gk} and \eqref{def:nk}. This means we actually implement a simpler version because the test after \eqref{def:nk} is not performed. Second, we implement BUF by two-cut scheme (S2) and the details are stated in section \ref{Algorithm}.


% Add sparse problem 


Both phase retrieval and blind deconvolution  are special cases of \eqref{eq:opt_problem} where $h\equiv0$ and $f(\cdot)$ has the form
\begin{equation}\label{fg}
f(x)=g(c(x))
\end{equation}
for some convex and $L$-Lipschitz function $g$  and smooth map $c$ with $\beta$-Lipschitz Jacobian.
It follows from  Lemma 4.2 in \cite{drusvyatskiy2019efficiency} that such a function $f(\cdot)$  is $L\beta$-weakly convex and hence we set $m = L\beta$. Recall that the PS method terminates based on Moreau stationary point (see Definition \ref{def:Moreau}) which is hard to verify in view of the remark below Proposition \ref{prop:Davis}. On the other hand, it is still an open problem for the PS method to establish the convergence to find a  regularized stationary point (see Definition~\ref{def:appr}).
% Note that the PS method and PBF do not have a common theoretical termination condition: the PS method terminates based on the Moreau Envelope (see Definition \ref{def:Moreau}), while PBF terminates based on the regularized stationary point (see Definition~\ref{def:appr}).
Hence for the sake of our numerical experiments, we adopt the termination criterion 
\begin{equation}\label{rela-var}
\phi(x_k) - \phi_* \le \tilde \varepsilon [ \phi(x_0)-\phi_*]
\end{equation}
that requires the knowledge of $\phi_*$ where either $\tilde \varepsilon =  10^{-3}$ or $10^{-4}$. 
% , and we will demonstrate later that this is true for both the phase retrieval and blind deconvolution problems.
% (see  Lemma 4.2 in \cite{drusvyatskiy2019efficiency}).
% In the following two subsections, we will show that both phase retrieval and blind deconvolution problems fall into this form. 

 
 Now we provide details of the algorithms used in the following two subsections. 
 % First, in view of the  previous paragraph, we set  $m = L\beta$. 
We implement the constant stepsize version of PS method, i.e.,  $\alpha_t = \alpha$ for every $t \ge 0$. In our experiment, we choose four values of $\alpha$, namely,  $\alpha = 1/32m$, $1/8m$, $1/2m$, and $1/m$. Hence in our experiment: (i) the choice of $\alpha$ for the PS method does not follow  ${\cal{ O}}(1/\sqrt{T})$ recipe of Proposition \ref{prop:Davis} where $T$ is the total number of PS iterations; (ii)   $T$ is determined by \eqref{rela-var} rather than being pre-specified.
PBF sets $\lam = 1/2m$ and
       % Motivated by the termination criterion \eqref{rela-var}, PBF uses a different choice for $\delta$ compared to the theoretical one  (see step 0 of PBF), given by
       $\delta = \tilde \varepsilon [ \phi(x_0)-\phi_*]$. 
  Two variants of PBF are implemented: one based on the two-cut scheme and the other one based on the multi-cut scheme (see Subsection \ref{sec:BUF}). Due to the fact that $h=0$, the subproblem \eqref{def:xj} in the two-cut scheme has a closed-form solution . In the multi-cut scheme, we utilize  Mosek 10.2 \footnote{https://docs.mosek.com/latest/toolbox/index.html}  to solve subproblem \eqref{def:xj}.
% All experiments were performed in MATLAB 2023a and run on a PC with a 16-core Intel Core i9 processor and 32 GB of memor
% Now we describe the algorithm details used in the following two subsections. We first describe the subgradient method.
% Given $x_k$, it computes
% namely  Pol-Sub, which is a special case of subgradient method with Polyak stepsize as:
 % \[
 %     x_{k+1} = \argmin_x \left\{\ell_{\phi} (x;x_k)+\frac{1}{2\lam}\|x-x_k\|^2\right\}.
 %     \]
     % where $g(x):= A^T \operatorname{sign}(Ax-b) \in \partial f(x)$ and 
     % PBF is as in Subsection \ref{subsec:update} with 
     % \[
     % \delta = \tilde \varepsilon [ \phi(x_0)-\phi_*], \quad \lam = \frac{5}{m}
     % \]
     % where $m=$.
     
Finally, all experiments were performed in MATLAB 2023a and run on a PC with a 16-core Intel Core i9 processor and 32 GB of memory. 


% We now describe some details about all the tables that appear in this subsection.  We set the target $\bar \varepsilon$ in \eqref{rela-var} as $10^{-5}$ for dense instances and  $10^{-4}$ for sparse instances. 
% The quantities $\theta_m$, $\theta_n$ and $\theta_s$  are defined as $\theta_{m}  = m/10^3$, $\theta_{n} =  n/10^3$, and $\theta_s := \text{nnz}(A)/mn$, where $\text{nnz}(A)$ is the number of non-zero entries of $A$.  
% %  The second to tenth columns provide numbers of
% % iterations and running times for the three methods with three initial prox stepsize choices. 
% An entry in each table is given as a fraction with the numerator expressing the (rounded) number of iterations and the  denominator expressing the CPU running time in seconds. An entry marked as $*/*$ indicates that the CPU running time exceeds the allocated time limit. The
% bold numbers highlight the method that has the best performance for each instance.
\subsection{Phase Retrieval}\label{pr}
This subsection reports  computational results comparing PBF  against  PS on the phase retrieval problem.


Consider the  Phase Retrieval problem (see Subsection 5.1 of  \cite{davis2019stochastic})
\begin{equation}\label{phase}
\min _{x \in \mathbb{R}^d} \left\{f(x) := \frac{1}{n}\sum_{i=1}^nf_i(x)\right\} , \quad f_i(x):= \left|\left\langle a_i, x\right\rangle^2-b_i\right|
\end{equation}
% \[
% h = |\cdot|
% \]
% \[
% c_i(x) = x^\top (a_i a_i^\top) x - b_i
% \]
% \[
% \nabla c_i(x) = 2(a_i a_i^\top) x
% \]
% \[
% L_i = 2\|a_i a_i^\top\|
% \]
% \[
% m = \frac{1}{n} \sum L_i
% \]
% %\textbf{Agree}
% \begin{equation}
% \min _{x \in \mathbb{R}^d} g(x):=\frac{1}{n} \sum_{i=1}^n\left|\left\langle a_i, x\right\rangle^2-b_i\right| .
% \end{equation}
% \begin{equation}
% \partial g_i(x)=2\langle a
% _i, x\rangle a_i \cdot\left\{\begin{array}{lr}
% \operatorname{sign}\left(\langle a_i, x\rangle^2-b\right), & \text { if }\langle a, x\rangle^2 \neq b \\
% {[-1,1],} & \text { o.w. }
% \end{array}\right\} .
% \end{equation}
where $a_i\in \R^d$ and $b_i \in \R$. 
% Note that \eqref{phase} is a special instance of \eqref{eq:opt_problem} with $h\equiv0$. 
Observe that $f_i(\cdot)$ is a special case of \eqref{fg} with $g(\cdot)= |\cdot|$ and $c(\cdot) =\left\langle a_i, \cdot\right\rangle^2-b_i $. Thus $f_i$ is $\|a_i a_i^T\|$-weakly convex, and we set 
\[
m  =\frac{1}{n}\sum_{i=1}^n\|a_i a_i^T\|.
\]
Now we describe the data generation process. Vectors $\{a_i\}$ are i.i.d. generated according to standard Gaussian measurements $ N\left(0, I_{d \times d}\right)$. Then we generate the target signal $\bar{x}$ and initial point $x_0$ uniformly on the unit sphere; and set $b_i=\left\langle a_i, \bar{x}\right\rangle^2$ for each $i=1, \ldots, n$.  Thus the optimal value $\phi_*=0$ for \eqref{phase}.
We perform four sets of experiments corresponding to $(d, n)=(100,300),(200,600)$, $(500,1500)$,$(1000,3000)$ and record the results in Tables \ref{tabfirstpb:cpuobj1} and \ref{tabfirstpb:cpuobj2}.

 We now describe some details about  the tables that appear in this subsection.   The second to fifth columns provide numbers of
 iterations and running times for the six variants. 
 An entry in each table has two numbers where the first one expresses the (rounded) number of iterations and the  second one expresses the CPU running time in seconds. An entry marked as $*/*$ indicates that the CPU running time exceeds the two hours time limit. 
% Two cut subproblem
% \[
% \Gamma_1(x) = \max\{l(x;x_0),l(x;x_1)\}
% \]

% \[
% l(x;x_0) =  f(x_0) + \inner{g(x_0)}{x- x_0} = \inner{a_1}{x}+b_1
% \]

% \[
% a_1=g(x_0), \quad b_1 = f(x_0) - \inner{g(x_0)}{x_0}
% \]

% \[
% l(x;x_1) =  f(x_1) + \frac{m}{2}\|x_1 -x_0\|^2 + \inner{g(x_1) + m(x_1-x_0)}{x- x_1} = \inner{a_2}{x}+b_2
% \]

% \[
% a_2 = g(x_1) + m(x_1-x_0), \quad b_2 = f(x_1)+ \frac{m}{2}\|x_1-x_0\|^2 - \inner{g(x_1)}{x_1} - m\inner{x_1 - x_0}{x_1}
% \]

% {\bf Task:} Compare our code based on the $1$-cut and $2$-cut 
% with the subgradient-type algorithms of papers [3] and [4]

% Question 1 
% 1000 iterations:
% subgradient 0.5/m 1805  1/m 930
% PBF Delta:5 PERCENT (1000) 944

% Question 2
% 10000 iterations:
% subgradient 0.5/m 48 1/m 231
% PBF Delta:0.5 PERCENT (100) 42
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{-} &
\multicolumn{1}{|c|}{$L_1: (100,300)$}&
\multicolumn{1}{|c|}{$L_2: (200,600)$}&
\multicolumn{1}{|c|}{ $L_3: (500,1500)$}&
\multicolumn{1}{|c|}{ $L_4: (1000,3000)$}\\
\hline
\multirow[t]{10}{*}{PS: $\alpha = 1/32m$} 
%5  &14.6169   &0.001 & 14.6126 & 0.107 & 14.6465 & 0.0007 & 14.6904 &0.04 \\
%&10&    14.6155 & 0.0012 &14.6113  & 0.109 &14.6449  & 0.001 &14.6892& 0.05\\
&50852/1.48 & 203416/17.82 &*/*& */*\\
\hline
\multirow[t]{10}{*}{PS: $\alpha = 1/8m$} 
%5  &14.6169   &0.001 & 14.6126 & 0.107 & 14.6465 & 0.0007 & 14.6904 &0.04 \\
%&10&    14.6155 & 0.0012 &14.6113  & 0.109 &14.6449  & 0.001 &14.6892& 0.05\\
&14852/0.47 &  64294/5.58 &432176/201.12& 235700/650.21\\
\hline
\multirow[t]{10}{*}{PS: $\alpha = 1/2m$} 
%5  &14.6169   &0.001 & 14.6126 & 0.107 & 14.6465 & 0.0007 & 14.6904 &0.04 \\
%&10&    14.6155 & 0.0012 &14.6113  & 0.109 &14.6449  & 0.001 &14.6892& 0.05\\
& 4436/0.12 & 20011/1.76 &345346/167.21& 79165/216.67\\
\hline
\multirow[t]{10}{*}{PS: $\alpha = 1/m$} 
%5  &14.6169   &0.001 & 14.6126 & 0.107 & 14.6465 & 0.0007 & 14.6904 &0.04 \\
%&10&    14.6155 & 0.0012 &14.6113  & 0.109 &14.6449  & 0.001 &14.6892& 0.05\\
&*/* & */* &*/*& 67095/183.56\\
\hline
\multirow[t]{10}{*}{PBF: Twocut} 
%5 &12.8535 & 0.001 & 13.3252 & 0.0012 & 13.9843 &0.001  & 13.7750 &0.004\\
%&10&12.3639 & 0.0019 & 12.8431& 0.002& 13.9539 & 0.003 & 13.7763 &0.008\\
&5088/0.16 &24486/2.08&316782/154.21& 87514/242.62\\
\hline

\multirow[t]{10}{*}{PBF: Multicut} 
%5&13.1016&0.001  & 12.7594 & 0.002 & 13.7884 &0.001  &  13.8973&0.009\\
%&10&12.5790&0.0015  & 11.2559 & 0.003 & 13.5968 & 0.002 & 13.7777 &0.01\\
&4716/0.19 & 21345/2.31 &278234/172.32& 723475/274.38\\
\hline
\end{tabular}
\caption{PS versus two variants of PBF on \eqref{phase}. A relative tolerance of $\tilde \varepsilon  = 10^{-3}$ is set.}
\label{tabfirstpb:cpuobj1}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{-} &
\multicolumn{1}{|c|}{$L_1: (100,300)$}&
\multicolumn{1}{|c|}{$L_2: (200,600)$}&
\multicolumn{1}{|c|}{ $L_3: (500,1500)$}&
\multicolumn{1}{|c|}{ $L_4: (1000,3000)$}\\
\hline
\multirow[t]{10}{*}{PS: $\alpha = 1/32m$} 
%5  &14.6169   &0.001 & 14.6126 & 0.107 & 14.6465 & 0.0007 & 14.6904 &0.04 \\
%&10&    14.6155 & 0.0012 &14.6113  & 0.109 &14.6449  & 0.001 &14.6892& 0.05\\
&72376/2.11 & 380165/31.32 &653256/305.65& */*\\
\hline
\multirow[t]{10}{*}{PS: $\alpha = 1/8m$} 
%5  &14.6169   &0.001 & 14.6126 & 0.107 & 14.6465 & 0.0007 & 14.6904 &0.04 \\
%&10&    14.6155 & 0.0012 &14.6113  & 0.109 &14.6449  & 0.001 &14.6892& 0.05\\
&20328/0.61 & 118589/9.91 & */*
& 424433/1.03E+03\\
\hline
\multirow[t]{10}{*}{PS: $\alpha = 1/2m$} 
%5  &14.6169   &0.001 & 14.6126 & 0.107 & 14.6465 & 0.0007 & 14.6904 &0.04 \\
%&10&    14.6155 & 0.0012 &14.6113  & 0.109 &14.6449  & 0.001 &14.6892& 0.05\\
&*/* & */* &*/*& */*\\
\hline
\multirow[t]{10}{*}{PS: $\alpha = 1/m$} 
%5  &14.6169   &0.001 & 14.6126 & 0.107 & 14.6465 & 0.0007 & 14.6904 &0.04 \\
%&10&    14.6155 & 0.0012 &14.6113  & 0.109 &14.6449  & 0.001 &14.6892& 0.05\\
&*/* & */* &*/*& */*\\
\hline
\multirow[t]{10}{*}{PBF: Twocut} 
%5 &12.8535 & 0.001 & 13.3252 & 0.0012 & 13.9843 &0.001  & 13.7750 &0.004\\
%&10&12.3639 & 0.0019 & 12.8431& 0.002& 13.9539 & 0.003 & 13.7763 &0.008\\
&15228/0.42&125603/10.61&1056034/454.93& 742553/1.86E+03
\\
\hline
\multirow[t]{10}{*}{PBF: Multicut} 
%5&13.1016&0.001  & 12.7594 & 0.002 & 13.7884 &0.001  &  13.8973&0.009\\
%&10&12.5790&0.0015  & 11.2559 & 0.003 & 13.5968 & 0.002 & 13.7777 &0.01\\
&13762/0.56 & 104572/12.41&923202/511.21& 621001/2.02E+03\\
\hline
\end{tabular}
\caption{PS versus two variants of PBF on \eqref{phase}. A relative tolerance of $\tilde \varepsilon  = 10^{-4}$ is set.}
\label{tabfirstpb:cpuobj2}
\end{table}
The two tables show that the PS method is sensitive to the choice of stepsize, and the optimal stepsize in terms of CPU running time varies with different tolerances. In contrast, PBF with one stepsize demonstrates performance comparable to the best-performing stepsize of PS across all four stepsizes for both tolerance levels.
% \begin{table}[H]
%  \centering
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
% \hline
% \multicolumn{5}{|c|}{$L_1: (d,n)=(100,300)$}&
% \multicolumn{5}{|c|}{$L_2: (d,n)=(200,600)$}&
% \multicolumn{5}{|c|}{ $L_3: (d,n)=(300,900)$}\\
% \hline
% %  ALG.  &{\tt{PS}} &{\tt{PBF}} &{\tt{PS}} &{\tt{PBF}}&
% % {\tt{PS}} & {\tt{PBF}}\\
% % \hline
% % \multirow[t]{10}{*}{PS} 
% % %5  &14.6169   &0.001 & 14.6126 & 0.107 & 14.6465 & 0.0007 & 14.6904 &0.04 \\
% % %&10&    14.6155 & 0.0012 &14.6113  & 0.109 &14.6449  & 0.001 &14.6892& 0.05\\
% % &50&   82.43 & 0.0007 &3708.1&0.0028&16349& 0.0061\\
% % &100&   35.53 & 0.0014  &2607.6 &0.0055&13507&0.0129\\
% % &200&   14.38 & 0.0028 & 1544.9&0.0106&9762& 0.0257\\
% % &1000&  2.42 &0.0139 & 376.18&0.0547&2599.9& 0.1297\\
% % &$10^4$&1.75  & 0.1346 & 84.68&0.5386&413.13& 1.256\\
% -&\multicolumn{2}{|c|}{$PS$}& \multicolumn{2}{|c|}{$PBF$}& -&\multicolumn{2}{|c|}{$PS$}& \multicolumn{2}{|c|}{$PBF$}&-&\multicolumn{2}{|c|}{$PS$}& \multicolumn{2}{|c|}{$PBF$}\\
% \hline
% Obj&N&CPU&N&CPU&Obj&N&CPU&N&CPU&Obj&N&CPU&N&CPU\\
% \hline
% 1805.9&1000&0.1378&514&0.3322&7934&2000&0.79&1029&1.88&14050&5000&3.65&3070&10.29\\
% \hline
% 864&2000&0.2496&1376&0.7891&3126&5000&1.92&2599&4.70&8084&10000&7.19&8013&25\\
% \hline
% 260.2&5000&0.6704&3845&3.08&199.5&20000&7.71&????&????&387&50000&36&&\\
% \hline
% % % \multirow[t]{10}{*}{Twocut} 
% % %5 &12.8535 & 0.001 & 13.3252 & 0.0012 & 13.9843 &0.001  & 13.7750 &0.004\\
% % %&10&12.3639 & 0.0019 & 12.8431& 0.002& 13.9539 & 0.003 & 13.7763 &0.008\\
% % &50&28.74 &0.0088& 3045.9&0.0195&18621& 0.047\\
% % &100&5.44 & 0.0154& 1509.4&0.0356&12495&0.082\\
% % &200&1.48 & 0.043& 662.3&0.0674&6434& 0.170\\
% % &1000& 0.28 & 0.36 & 173.9 &0.3744&996.14& 0.710\\
% % &$10^4$&0.22  &3.403 & 23.78&5.159&118.66& 8.57\\
% % \hline
% % \multirow[t]{10}{*}{Multicut} 
% % %5&13.1016&0.001  & 12.7594 & 0.002 & 13.7884 &0.001  &  13.8973&0.009\\
% % %&10&12.5790&0.0015  & 11.2559 & 0.003 & 13.5968 & 0.002 & 13.7777 &0.01\\
% % &50& 28.74 & 0.0112 & 2936.3&0.0147&13.2& 0.7\\
% % &100& 5.44 & 0.0163 & 1469.5&0.0252&12.2& 1.5\\
% % &200& 1.62 & 0.0384 & 655.35 &0.0463&11.6& 3.4\\
% % &1000& 0.83 & 0.2980 & 184.59&0.242&11.2& 25.4\\
% % &$10^4$&0.73 & 2.8132 & 32.79&4.344&14.2& 80.3\\
% % \hline
% \end{tabular}
% \caption{PS versus two variants of FPB on the phase retrieval problem run with
% $K=10000$ inner iterations.}
% \label{tabfirstpb:cpuobj}
% \end{table}

\subsection{Blind deconvolution}\label{BD}
This subsection reports  computational results comparing PBF  against PS on the blind deconvolution problem.
Consider the blind deconvolution problem (see Subsection 5.2 of  \cite{davis2019stochastic}) 
\begin{equation}\label{blind}
\min _{x, y} \left\{f(x) :=\frac{1}{n} \sum_{i=1}^n f_i (x)\right\}, \quad f_i(x):=\left|\left\langle u_i, x\right\rangle\left\langle v_i, y\right\rangle-b_i\right|
\end{equation}
where $u_i, v_i \in \R^d$ and $b_i \in \R$. Note that \eqref{blind} is a special instance of \eqref{eq:opt_problem} with $h\equiv0$. Observe that $f_i(\cdot)$ is a special case of \eqref{fg} with $g(\cdot)= |\cdot|$ and $c(x,y) =\left\langle u_i, x\right\rangle\left\langle v_i, y\right\rangle-b_i $. Thus $f_i(\cdot)$ is $\|u_i v_i^T\|$-weakly convex, and we set 
\[
m  =\frac{1}{n}\sum_{i=1}^n\|u_i v_i^T\|.
\]
Now we describe the data generation process. Vectors $\{u_i\}$,  $\{v_i\}$ are i.i.d. generated Gaussian measurements $ N\left(0, I_{d \times d}\right)$. Then we generate the target signals $\bar{x},\bar y$ and initial points $x_0,y_0$ uniformly on the unit sphere; and set $b_i=\left\langle u_i, \bar x\right\rangle\left\langle v_i, \bar y\right\rangle$ for each $i=1, \ldots, n$. 
Thus the optimal value $\phi_*=0$ for \eqref{phase}.
We perform four sets of experiments corresponding to $(d, n)=(100,300),(200,600)$, $(500,1500)$,$(1000,3000)$ and record the results in Tables \ref{tabfirstpb:cpuobj3} and \ref{tabfirstpb:cpuobj4}. The explanations of the tables are the same as in Subsection \ref{pr}.
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{-} &
\multicolumn{1}{|c|}{$L_1: (100,300)$}&
\multicolumn{1}{|c|}{$L_2: (200,600)$}&
\multicolumn{1}{|c|}{ $L_3: (500,1500)$}&
\multicolumn{1}{|c|}{ $L_4: (1000,3000)$}\\
\hline
\multirow[t]{10}{*}{PS: $\alpha = 1/32m$} 
%5  &14.6169   &0.001 & 14.6126 & 0.107 & 14.6465 & 0.0007 & 14.6904 &0.04 \\
%&10&    14.6155 & 0.0012 &14.6113  & 0.109 &14.6449  & 0.001 &14.6892& 0.05\\
&76282/2.21 & 812578/143.48 &*/*& */*\\
\hline
\multirow[t]{10}{*}{PS: $\alpha = 1/8m$} 
%5  &14.6169   &0.001 & 14.6126 & 0.107 & 14.6465 & 0.0007 & 14.6904 &0.04 \\
%&10&    14.6155 & 0.0012 &14.6113  & 0.109 &14.6449  & 0.001 &14.6892& 0.05\\
&16852/4.41 &  224893/39.62 &121091/350.21& 343700/740.55\\
\hline
\multirow[t]{10}{*}{PS: $\alpha = 1/2m$} 
%5  &14.6169   &0.001 & 14.6126 & 0.107 & 14.6465 & 0.0007 & 14.6904 &0.04 \\
%&10&    14.6155 & 0.0012 &14.6113  & 0.109 &14.6449  & 0.001 &14.6892& 0.05\\
& 5425/0.42 & */* &92137/241.41& 83125/288.21\\
\hline
\multirow[t]{10}{*}{PS: $\alpha = 1/m$} 
%5  &14.6169   &0.001 & 14.6126 & 0.107 & 14.6465 & 0.0007 & 14.6904 &0.04 \\
%&10&    14.6155 & 0.0012 &14.6113  & 0.109 &14.6449  & 0.001 &14.6892& 0.05\\
&*/* & */* &*/*& 64035/172.12\\
\hline
\multirow[t]{10}{*}{PBF: Twocut} 
%5 &12.8535 & 0.001 & 13.3252 & 0.0012 & 13.9843 &0.001  & 13.7750 &0.004\\
%&10&12.3639 & 0.0019 & 12.8431& 0.002& 13.9539 & 0.003 & 13.7763 &0.008\\
&5021/0.39 &344221/72.68&101299/280.67& 77511/212.78\\
\hline
\multirow[t]{10}{*}{PBF: Multicut} 
%5&13.1016&0.001  & 12.7594 & 0.002 & 13.7884 &0.001  &  13.8973&0.009\\
%&10&12.5790&0.0015  & 11.2559 & 0.003 & 13.5968 & 0.002 & 13.7777 &0.01\\
&4396/0.45 & 293115/80.18&93231/310.45& 69932/232.65\\
\hline
\end{tabular}
\caption{PS versus two variants of PBF on \eqref{blind}. A relative tolerance of $\tilde \varepsilon  = 10^{-3}$ is set.}
\label{tabfirstpb:cpuobj3}
\end{table}
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{-} &
\multicolumn{1}{|c|}{$L_1: (100,300)$}&
\multicolumn{1}{|c|}{$L_2: (200,600)$}&
\multicolumn{1}{|c|}{ $L_3: (500,1500)$}&
\multicolumn{1}{|c|}{ $L_4: (1000,3000)$}\\
\hline
\multirow[t]{10}{*}{PS: $\alpha = 1/32m$} 
%5  &14.6169   &0.001 & 14.6126 & 0.107 & 14.6465 & 0.0007 & 14.6904 &0.04 \\
%&10&    14.6155 & 0.0012 &14.6113  & 0.109 &14.6449  & 0.001 &14.6892& 0.05\\
&152976/6.12 & 1701581/300.33 &*/*& */*\\
\hline
\multirow[t]{10}{*}{PS: $\alpha = 1/8m$} 
%5  &14.6169   &0.001 & 14.6126 & 0.107 & 14.6465 & 0.0007 & 14.6904 &0.04 \\
%&10&    14.6155 & 0.0012 &14.6113  & 0.109 &14.6449  & 0.001 &14.6892& 0.05\\
&41328/1.39 & 283353/50.32 & 173291/670.81
& 824121/1.93E+03\\
\hline
\multirow[t]{10}{*}{PS: $\alpha = 1/2m$} 
%5  &14.6169   &0.001 & 14.6126 & 0.107 & 14.6465 & 0.0007 & 14.6904 &0.04 \\
%&10&    14.6155 & 0.0012 &14.6113  & 0.109 &14.6449  & 0.001 &14.6892& 0.05\\
&*/* & */* &*/*& */*\\
\hline
\multirow[t]{10}{*}{PS: $\alpha = 1/m$} 
%5  &14.6169   &0.001 & 14.6126 & 0.107 & 14.6465 & 0.0007 & 14.6904 &0.04 \\
%&10&    14.6155 & 0.0012 &14.6113  & 0.109 &14.6449  & 0.001 &14.6892& 0.05\\
&*/* & */* &*/*& */*\\
\hline
\multirow[t]{10}{*}{PBF: Twocut} 
%5 &12.8535 & 0.001 & 13.3252 & 0.0012 & 13.9843 &0.001  & 13.7750 &0.004\\
%&10&12.3639 & 0.0019 & 12.8431& 0.002& 13.9539 & 0.003 & 13.7763 &0.008\\
&35227/1.21 &394212/89.32&188823/690.21& 1142572/2.65E+03
\\
\hline
\multirow[t]{10}{*}{PBF: Multicut} 
%5&13.1016&0.001  & 12.7594 & 0.002 & 13.7884 &0.001  &  13.8973&0.009\\
%&10&12.5790&0.0015  & 11.2559 & 0.003 & 13.5968 & 0.002 & 13.7777 &0.01\\
&31444/1.51 & 378932/111.34 &177012/739.32& 1023990/2.93E+03\\
\hline
\end{tabular}
\caption{PS versus two variants of PBF on \eqref{blind}. A relative tolerance of $\tilde \varepsilon  = 10^{-4}$ is set.}
\label{tabfirstpb:cpuobj4}
\end{table}

The two tables above show that the PS method is sensitive to the choice of stepsize, with the optimal stepsize for CPU running time varying depending on the tolerance. On the other hand, PBF performs comparably to the best-performing PS stepsize across all four choices, for both tolerance levels.


% \subsection{MaxQuad -- ????}

% Add constraints to avoid unboundedness.



% \section{Twocuts}
% Assume $\Gamma = \max\{A_f,\ell(:,x^-)\} = \max \{\inner{a_1}{u}+b_1,\inner{a_2}{u}+b_2\}$
% \[
% g_m(x^-) = g + m(x^- - x^c)
% \]
% \[
% b_2 = f_m(x^-) - \inner{g_m}{x^-}
% \]
% \[
% x = \argmin \{ \Gamma(u) + \frac{1}{2\lam} \|u - x^c\|^2 \}
% \]
% % Define 
% % \[
% % \phi(1+m\lam) = \min_u \{ \inner{(a_1 - a_2)1+m\lam + a_2}{u} + 1+m\lam b_1 +(1 - 1+m\lam b_2) + \frac{1}{2\lam}\|u - x^-\|^2\}
% % \]
% % Then 
% % \[
% % u^*(1+m\lam) = x^-  - \lam (1+m\lam a_1 + (1 -1+m\lam) a_2)
% % \]
% % Plug in $u^*(1+m\lam)$ we have
% % \[
% % \phi(1+m\lam)  = \inner{(a_1 - a_2)1+m\lam + a_2}{x^-} + 1+m\lam b_1 +(1 - 1+m\lam) b_2 - \frac{\lam}{2}\|(a_1 - a_2)1+m\lam + a_2 \|^2
% % \]
% % \[
% % \phi'(1+m\lam) = \inner{a_1 - a_2}{x^-} - \lam (1+m\lam \|a_1\|^2 - (1 - 1+m\lam)|a_2|^2 + (1- 2 1+m\lam)\inner{a_1}{a_2}) + b_1 -b_2
% % \]
% % Try to compute $\phi'(1+m\lam)  = 0 $
% % \[
% % 1+m\lam = 
% % \]
% \[
% \Gamma(u) = \max_{0\le \alpha \le 1} \{ \alpha(\inner{a_1}{u}+b_1)+ (1-\alpha)\inner{a_2}{u}+b_2)\}
% \]
% By strong duality 
% \[
% \min_{u} {\Gamma(u) + \frac{1}{2\lam} \|u - x^c\|^2 } = \max_{\alpha} \phi(\alpha) 
% \]
% where
% \[
% \phi(\alpha):= \min_{u} \{\inner{(a_1-a_2)\alpha+a_2}{u}+\alpha b_1+(1-\alpha)b_2 + \frac{1}{2\lam} \|u - x^c\|^2 \}
% \]
% Thus
% \[
% u^*(\alpha) = x^c - \lam (\alpha a_1 + (1-\alpha) a_2)
% \]
% Plug in we have
% \[
% \phi(\alpha) = \inner{\alpha a_1 + (1-\alpha) a_2}{x^c} +\alpha b_1+(1-\alpha)b_2 - \frac{\lam}{2} \|\alpha a_1 + (1-\alpha) a_2\|^2 
% \]
% Note that 
% \[
% \phi'(\alpha) = \inner{a_1-a_2}{x^c} - \lam(\|a_1-a_2\|^2\alpha + \inner{a_1}{a_2}- \|a_2\|^2) +b_1 -b_2 
% \]
% and 
% \[
% \phi''(\alpha) = -\lam \|a_1-a_2\|^2 \le 0
% \]
% So $\phi$ is concave
% and 
% \[
% \alpha^* = \frac{\inner{a_1-a_2}{x^c}-\lam \inner{a_1}{a_2}+\lam \|a_2\|^2 +b_1 -b_2 }{\lam\|a_1-a_2\|^2}
% \]
% (if $\alpha^* \le 0$ set it to be $0$, if $\alpha^*\ge 1$  set it to be $1$)

% =====================================\\
% \begin{equation}
% \begin{aligned}
% & \frac{1}{\lambda}\left(x-x^c\right)+\partial h(x)+\theta \nabla A_f+(1-\theta) f^{\prime}\left(x^{-}\right) \ni 0 \\
% & \theta A_f(x)+(1-\theta) \ell_f\left(x ; x^{-}\right)=\max \left\{A_f(x), \ell_f\left(x ; x^{-}\right)\right\}
% \end{aligned}
% \end{equation}

% \[
% \min \left\{ \max\{ a_1^T x+b_1 , a_2^Tx+b_2\} +  \frac1{2\lambda} \|x-x^c\|^2
% \right\}
% \]
% \[
% \min \left\{ \theta  +  \frac1{2\lambda} \|x-x^c\|^2
%  :  a_1^Tx+b_1 \le \theta, \ a_2^T x+b_2 \le \theta \right\}
% \]


% \[
% \theta (\inner{a_1}{x}+b_1) + (1-\theta )(\inner{a_2}{x}+b_2) = t
% \]
% \[
% \theta = (t - \inner{a_2}{x}-b_2)/(\inner{a_1}{x}+b_1 - \inner{a_2}{x}-b_2)
% \]


\section{Concluding remarks}
\label{sec:conclusion}

% This paper presents a proximal bundle framework for solving HWC-CO problem and establishes the first iteration-complexity for PB methods for solving ?????weakly convex problems. Instead of focusing on a specific bundle update scheme, PBF introduces BUF which contains various well-known update schemes. PBF generates a sequence of iterates $\{\hat y_k\}$ with the corresponding residuals $(\hat w_k,\hat \varepsilon_k)$ such that $(x,w,\varepsilon)=(\hat y_k,\hat w_k,\hat \varepsilon_k)$
%  satisfies the inclusion in \eqref{eq:app_sol}.
% The iteration complexity is then established based on measuring the norm of $\hat w_k$ and $\hat \varepsilon_k$ simultaneously. ??? 

In this section, we provide some further remarks and directions for future research.

First, from the point of view of the sequences of
serious iterates $\{\hat x_k\}$ and $\{\hat y_k\}$,
the complexity result for
PBF is point-wise since it
is about a single iterate
from $\{\hat y_k\}$.
It would be also interesting to
establish an ergodic complexity result about a weighted average of
such sequence. 

Second, as already observed in the fourth paragraph following PBF, PBF
requires the knowledge of a
weakly convex parameter,
i.e., a scalar $m$ as in
Assumption (A1), and hence is not a universal method.
It would be interesting to develop an adaptive method
which do not require
a scalar $m$ as above, but instead generates adaptive estimates for it which possibly violate the condition on (A1).

Third, paper
\cite{davis2019proximally} presents an
inexact proximal point method (see Algorithm 2 of  \cite{davis2019proximally}) for solving \eqref{eq:opt_problem} where the subgradient method (SM) is used to obtain an approximate solution $\hat x_k$ of \eqref{sub_pro}. More specifically, under the condition that
$j_k \ge 11 (1+\lam m)^2$, it is shown that
$j_k$ iterations of the SM initialized from $\hat y_{k-1}$ yield a point $\hat x_k$ satisfying
 \[
\phi(\hat x_k) - \phi(\hat y_{k-1}) \le \frac{72 \lam M^2}{j_k+1} - \frac1{4\lam} \|\hat y_{k-1}- x_k^*\|^2
\]
where $x_k^*$ is the minimizer of \eqref{sub_pro}.
Hence, if
$j_k$ is chosen as
\[
j_k = \bar j :=\left\lceil \max \left\{\frac{576 \lam^2 M^2}{ \varepsilon^2}, \frac{576 (1+\lam m)^2 M^2}{\varepsilon^2}, 11 (1+\lam m)^2 \right\}\right\rceil,
\]
then using \eqref{eq:new_grad}, we conclude that
\[
\frac{\lam^2}{(1+\lam m)^2}\|\nabla \hat M^{\lam}(\hat y_{k-1})\|^2 = \|\hat y_{k-1}-x_k^*\|^2 \le
     4\lam [\phi(\hat y_{k-1}) - \phi(\hat x_k)  ] + \frac{\lam^2 \varepsilon^2}{2(1+\lam m)^2}, 
\]
and hence that
\begin{equation}\label{ineq:Moreau-bound}
    \inf_{k \le K} \|\nabla \hat M^{\lam}(\hat y_{k-1})\|^2 \le \frac{4(1+\lam m)^2}{\lam K} [ \phi(x_0) - \phi(\hat x_K)] + \frac{\varepsilon^2}{2}.
\end{equation}
The above conclusion gives an easily computable bound on
the gradient of the Moreau envelope at $\hat y_{k-1}$.
A drawback of the above bound  is that it only holds if SM performs a prespecified number of iterations
$j_k \ge \bar j$.
% applied to \eqref{sub_pro}.
In contrast, it is shown 
in Appendix \ref{sec:termination}
that the iteration sequence  $\{(\hat x_k,\hat y_k)\}$ satisfies a
 bound similar to \eqref{ineq:Moreau-bound} even though it solves \eqref{sub_pro} only according to
 the stopping criterion \eqref{ineq:hpe1},
 and hence without performing a prespecified number of inner  iterations
 as SM does.\\
 
 % can be derived under the assumption that \eqref{??} is solved according to \eqref{??}. 
% \textbf{Funding}
% This work was partially supported by AFORS Grant FA9550-22-1-0088.



\bibliographystyle{plain}
\bibliography{Bundle_weakly_v2}





\appendix

\section{Technical results about subdifferentials}
\label{APP:sub}

% \begin{lemma}
% \label{def: Frechet Subdifferential}
%  Let $\phi: \mathbb{R}^{n} \rightarrow \mathbb{R} \cup\{\infty\}$ be a proper closed function. At any point $x \in \operatorname{dom} \phi$, we have
% \[
% \partial \phi(x)=\left\{v \in \mathbb{R}^{n} \mid \phi(y) \geq \phi(x)+\langle v, y-x\rangle+o(\|y-x\|), \quad \forall y \in \R^n\right\}.
% \]
% \end{lemma}
% \begin{proof}
% On one hand, if $v \in \partial \phi(x)$ \red{???}, then the fact that $\forall y \in \R^n$
% \[
%   \phi(y) \ge \phi(x) + \inner{v}{y-x} + \min\{0,\phi(y)-\phi(x)-\inner{v} { y-x}\}.
% \]
% implies that $\liminf_{y \rightarrow x} \frac{\min\{0,\phi(y)-\phi(x)-\inner{v} { y-x}\}}{\|y-x\|} \geq 0$. Thus we know  \red{???}
% \[
% \min\{0,\phi(y)-\phi(x)-\inner{v} { y-x}\} = o(\|y-x\|).
% \]
% We then conclude
% \[
% \partial \phi(x) \subset \left\{v \in \mathbb{R}^{n} \mid\left( \forall y \in \mathbb{R}^{n}\right) \phi(y) \geq \phi(x)+\langle v, y-x\rangle+o(\|y-x\|)\right\}.
% \]
% On the other hand, if $v \in \left\{v \in \mathbb{R}^{n} \mid\left( \forall y \in \mathbb{R}^{n}\right) \phi(y) \geq \phi(x)+\langle v, y-x\rangle+o(\|y-x\|)\right\}$, then 
% \[
% \liminf _{y \rightarrow x} \frac{\phi(y)-\phi(x)-\left\langle v, y-x\right\rangle}{\|y-x\|} \geq \liminf _{y \rightarrow x}\frac{o(\|y-x\|)}{\|y-x\|} = 0.
% \]
% Thus the statement follows.
% ({\bf Jiaming and Honghao})
% % In particular, let $\varepsilon = 0$ in (7) and (8) we have the second statement. So we can rewrite (7) as
% % % \[
% % % \partial_\varepsilon \phi(x) = \partial \phi(x) + \varepsilon \bar B.
% % % \]
% % % %\label{prop:Fre_sub}
% \end{proof}


This section presents two
technical results about
$\varepsilon$-subdifferentials
that will be used in our analysis.

The first result describes
a simple relationship 
involving $\varepsilon$-subdifferentials
of $\phi_m(\cdot;x)$
for different points $x$.
% The second result describes
% a simple relationship 
% involving $\varepsilon$-subdifferentials
% of $\phi_m(\cdot;x)$
% for different points $x$.
\begin{lemma}
If $\phi: \mathbb{R}^{n} \rightarrow \mathbb{R} \cup\{\infty\}$ is an $m$-weakly convex function, then
$\phi_m(\cdot;c)$ is convex for
every $c \in \R^n$.
Moreover,
for every $x \in \dom \phi$,
$c \in \R^n$, and
$\varepsilon \ge 0$, we have
\[
\partial_{\varepsilon} \left[\phi_m(\cdot;x) \right](x) = \partial_{\varepsilon} \left[\phi_m(\cdot;c) \right](x) -  m(x-c).
\]
\label{lem:chara_weakly}
\end{lemma}

\begin{proof}
% \[
% \phi_m(u;c)-\phi_m(x;c)
% =\phi_m(u) - \phi(x) + \frac{m}2 \|u-c\|^2 - \frac{m}2 \|x-c\|^2
% - \frac{m}2 \|u-x\|^2 
% = 
% =???? \phi_m(u;x)-\phi(x) - \inner{v}{u-x}
% \]
Let $x\in \dom \phi$, $c \in \R^n$, and
$\varepsilon \ge 0$ be given. Then,
using
the definition of $\phi_m(\cdot;\cdot)$
in \eqref{def:phim}, we easily see that
\[
\phi_m(u;c)-\phi_m(x;c) - \inner{v+m(x-c)}{u-x}
= \phi_m(u;x) - \phi(x)  - \inner{v}{u-x} \quad \forall  u, v \in \R^n.
\]
The result now follows from the above identity and the definition of the $\varepsilon$-subdifferential
in \eqref{def:subdif}.
\end{proof}



% ================


% Assume $v \in \partial_{\varepsilon} \left[\phi_m(\cdot;c) \right](x) -  m(x-c)$, i.e., $v+m(x-c) \in \partial_{\varepsilon} \left[\phi_m(\cdot;c) \right](x)$,
% then for any $u \in \R^n$, we have
% \[
% \phi_m(u;c)-\phi_m(x;c) \ge \inner{v+m(x-c)}{u-x} - \varepsilon
% \]
% Then rearranging the term we get
% \[
% \phi_m(u;x) \ge \phi(x) +  \inner{v}{u-x}- \varepsilon
% \]
% which implies $v \in \partial_{\varepsilon} \left(\phi_m(\cdot;x) \right)(x)$ and thus \[
% \partial_{\varepsilon} \left[\phi_m(\cdot;c) \right](x) -  m(x-c) \subset 
% \partial_{\varepsilon} \left[\phi_m(\cdot;x) \right](x).
% \]
% The conclusion then just follows from the fact that the above process can be reversed.


% \begin{corollary}
% \label{lam:validity}
% If $\phi: \mathbb{R}^{n} \rightarrow \mathbb{R} \cup\{\infty\}$ is an $m$-weakly convex function, then for any $x, c \in \R^n$, we have
% % \begin{equation}
% % \label{eq:equ_chara}
% % \partial \phi(x) = \partial \left(\phi_{  m}(\cdot;x) \right)(x).
% % \end{equation}
% \begin{equation}
% \label{eq:equ_chara}
% \partial \phi(x) = \partial \left[\phi_{  m}(\cdot;c) \right](x) - m (x-c).
% \end{equation}
% \label{coro:key}
% \end{corollary}


% \begin{proof}
%      Proposition \ref{prop:key_chara} \red{???} and  Lemma \ref{lem:chara_weakly} with $\varepsilon = 0$  suggest
%     \[
% \partial \phi(x)  =
% \partial \left[\phi_{ m}(\cdot;x) \right](x) = \partial \left[\phi_{ m}(\cdot;c) \right](x) -   m(x-c) .
% \]
% \end{proof}


% ??????? \textbf{Remark}
% From this lemma we can observe that  our new definition is invariant under the choice of $m$ and $c$ since the right hand side of \eqref{eq:equ_chara} does not depend on $m$ and $c$ explicitly. 
% \begin{proposition}
% If $\phi: \mathbb{R}^{n} \rightarrow \mathbb{R} \cup\{\infty\}$ is an $\bar m$-weakly convex function, then for every $m \ge \bar m$ it holds that:
% \[\partial_{\varepsilon} \left(\phi_{\bar m}(\cdot;x) \right)(x) + \bar B(0;2\sqrt{(m-\bar m)\varepsilon}) \supset \partial_{\varepsilon} \left(\phi_m (\cdot;x)\right)(x) \supset
% \partial_{\varepsilon} \left(\phi_{\bar m}(\cdot;x) \right)(x) 
% \]
% \end{proposition}
% \begin{proof}
% We first prove the first relation:
% \begin{align*}
% \partial_{\varepsilon} \left(\phi_m(\cdot;x)   \right)(x) &= {\rm co \,}
% \left\{ \bigcup_{\varepsilon'+ \varepsilon''\le \varepsilon}
% \left[ \partial_{\varepsilon'} \left(\phi_{\bar m} (\cdot;x)  \right)(x) +
% \partial_{\varepsilon''} \left( \frac{m-\bar m}2 \|\cdot-x\|^2 \right)(x) \right] \right\} \\ 
% &\subset \partial_{\varepsilon} \left(\phi_{\bar m}(\cdot;x)   \right)(x) + \bar B(0;2\sqrt{(m-\bar m)\varepsilon})
% \end{align*}
% Now the second relation follows from the fact that $\phi_m(\cdot;x) \ge \phi_{\bar m}(\cdot;x)$ and $\phi_{m}(x;x) = \phi_{\bar m}(x;x)$.
% \end{proof}

% \begin{corollary}
% Let $\phi(x): \mathbb{R}^{n} \rightarrow \mathbb{R} \cup\{\infty\}$ be a  convex function,  then we have 
% \begin{equation}
% \partial_\varepsilon \phi(x) = \bigcap_{t>0} \partial_{t \varepsilon} (\phi+I_{\bar B(x;t)})(x)
% \end{equation}
% where I is the indicator function.
% \end{corollary}
% \begin{proof}
% From the Remark we can have:
% \begin{align*}
%   \partial_\varepsilon \phi(x)
% &= \{ x^* : \phi(u)-\phi(x) \ge \inner{x^*}{u-x} - \varepsilon \|u-x\| \, \quad \forall u\} \\
% &= \bigcap_{t>0} \{ x^* : \phi(u)-\phi(x) \ge \inner{x^*}{u-x} - \varepsilon t , \, \|u-x\| =t \}  \\
% &= \bigcap_{t>0} \{ x^* : \phi(u)-\phi(x) \ge \inner{x^*}{u-x} - \varepsilon t , \, \|u-x\| \le t \} \\
% &= \bigcap_{t>0} \partial_{t \varepsilon} (\phi+I_{\bar B(x;t)})(x)
% \end{align*}
% \end{proof}


The second results describes the relationship between an $\varepsilon$-solution and its global minimizer for a strongly convex function.
\begin{lemma}
If $g$ is a closed $\mu$-strongly convex function and $y$ is an
$\varepsilon$-solution of $g$, i.e., $0 \in \partial_\varepsilon g(y) $, then
its global minimizer $\hat y$ satisfies
\begin{equation}\label{eq:B1}
0 \in \partial g(\hat y), \quad \|y-\hat y \| \le \sqrt{\frac{2\varepsilon}\mu}.
\end{equation}
\label{lem:mu convex}
\end{lemma}


\begin{proof}
The inclusion in \eqref{eq:B1} follows from the fact that
$\hat y$ is a global minimizer of $g$.
Since g is $\mu$-strongly convex and $\hat y$ is its global minimizer, we have 
\[
\frac{\mu}{2}\|y - \hat y\|^2 \le g(y) - g(\hat y) 
 \le \varepsilon
\]
where the second inequality is due to $0 \in \partial_\varepsilon g(y)$.
hence, the inequality in \eqref{eq:B1} follows.
\end{proof}


% \begin{lemma}
% Given two convex  functions $g$ and $h$ and a point $x\in \R^n$ such that $g \le h$ and $g(x) = h(x)$, then it holds that 
% \[
% \partial g(x) \subset \partial h(x).
% \]
% \end{lemma}


% \begin{lemma}
% if $g:\R^n \to \R^n$ is convex differentiable with $L$-Lipschitz continuous gradient and $v \in \partial_\varepsilon g(x)$ then
% \[
% \|v-\nabla g(x) \| \le 2 \sqrt{L\varepsilon}
% \]
% \end{lemma}

\section{Relationships between notions of stationary points}\label{App:relation}
This section contains three proofs for the results in Subsection~\ref{sec:notion}.

\noindent
	{\bf Proof of Proposition \ref{prop:relation}}
 For this proof only, 
we  let
\begin{equation}\label{def:psi}
    \psi(\cdot) := \phi(\cdot) + \frac{\lam^{-1}+m}{2} \|\cdot-x\|^2
\end{equation}
and denote $\hat x^\lam(x)$ as in \eqref{eq:hatx}. Observe that $\psi$ is a $(1/\lam)$-strongly convex function in view of the fact that $\phi$ is $m$-weakly convex. Thus, for any $u \in \dom \psi$, it holds that
\begin{equation}\label{eq:psimu}
  \psi(u) - \psi(\hat x) \ge \frac{1}{2\lam}\|u-\hat x\|^2.  
\end{equation}
We start with the proof of a).

   a) 
Since $x$ is a $(\varepsilon_D,\delta_D)$-directional stationary point, there exits a $\tilde x$ such that
\begin{equation}\label{eq:DtoM}
\norm{x-\tilde x} \le \delta_D \quad 
\inf_{\|d\|\le 1} \phi'(\tilde x;d) \ge -\varepsilon_D.
\end{equation}
Then for any $d \in \R^n$,
we have
\begin{align*}
   \psi'(\tilde x;d) =
\phi'(\tilde x;d) + \left(\frac{1}{\lam}+m\right)\inner{\tilde x-x}{d} \ge -\varepsilon_D \|d\| - \left(\frac{1}{\lam}+m\right)\delta_D \|d\|
= - \left[\varepsilon_D + \left(\frac{1}{\lam}+m\right)\delta_D \right] \|d\|.
\end{align*}
Using the convexity of $\psi$  and the above relation with $d = \hat x^\lam(x)- \tilde x $, we conclude  that
\begin{equation}\label{eq:sped}
\psi(\hat x) - \psi(\tilde x) \ge 
\psi'(\tilde x;\hat x-\tilde x)
\ge 
- \left[\varepsilon_D + \left(\frac{1}{\lam}+m\right)\delta_D \right] \|\hat x-\tilde x\|.
\end{equation}
Then using the above inequality and \eqref{eq:psimu} with $u = \tilde x$ we can conclude that
\[
 \varepsilon_D + \left(\frac{1}{\lam}+m\right) \delta_D  \ge \frac{1}{2\lam} \|\tilde x-\hat x\|.
\]
Thus \eqref{eq:DtoM} further implies that 
\[
\|x-\hat x\| \le \norm{x-\tilde x} + \norm{\tilde x-\hat x} 
\le \delta_D + 2\lam \left[\varepsilon_D+\left(\frac{1}{\lam}+m\right)\delta_D \right].
\] 
and hence using \eqref{eq:new_grad}  we can get
\[
\|\nabla \hat M^{\lam}(x)\| = \left(m+\frac{1}{\lam}\right)\|\hat x^\lam(x)- x\| \le \left(m+\frac{1}{\lam}\right)\left[(3+2\lam m)\delta_D + 2\lam \varepsilon_D \right].
\]
Now the statement follows from the above inequality and the definition of Moreau stationary point in Definition \ref{def:Moreau}.

b) Since $x$ is a $(\varepsilon_M;\lam)$-Moreau stationary point, \eqref{eq:new_grad}  thus imply that 
\[
\left(\frac{1}{\lam}+m\right)\|x- \hat x\| \le \varepsilon_M.
\]
Then for any $d \in \R^n$ such that $\|d\| \le 1$, we have
\begin{align*}
   0 \le \psi'(\hat x;d) =
\phi'(\hat x;d) + \left(\frac{1}{\lam}+m\right)\inner{\hat x-x}{d} \le 
\phi'(\hat x;d)
+ \left(\frac{1}{\lam}+m\right) \norm{x-\hat x} \le \phi'(\hat x;d)+ \varepsilon_M
\end{align*}
and
\[
 \|x-\hat x\| \le \frac{\varepsilon_M}{m+\frac{1}{\lam}}.
\]
Choose $\tilde x = \hat x$, $\varepsilon_D = \varepsilon_M$ and 
$\delta_D =\varepsilon_M/(m+1/\lam)$.
Then the statement follows from the above relation and the definition of $(\varepsilon_D,\delta_D)$-directional stationary point.
\QEDA

\vgap

\noindent
	{\bf Proof of Proposition \ref{prop:subdir}}
 Since $\psi$ in \eqref{def:psi} is convex, it follows from Theorem 3.26 of \cite{beck2017first} that
\begin{equation}\label{eq:directional}
    -\inf_{\|d\| \leq 1} \psi^{\prime}(x;d)
    = -\inf_{\|d\| \leq 1}\sigma_{\partial \psi(x)}(d)
    = - \inf_{\|d\| \leq 1} \sup_{s\in \partial \psi(x)}\inner{s}{d}
    = \inf_{s\in \partial \psi(x)}\|s\|
    =\operatorname{dist}(0 ; \partial \psi(x)).
\end{equation}
It follows from Proposition \ref{prop:Fre_sub} with $m=2m$ and the definition of $\psi$ in \eqref{def:psi} that
\[
\partial \phi(x) = \partial \psi(x).
\]
Using the above identity and \eqref{eq:directional}, we have
\[
\operatorname{dist}(0 ; \partial \phi(x)) = \operatorname{dist}(0 ; \partial \psi(x)) = -\inf _{\|d\| \leq 1} \psi^{\prime}(x; d) = -\inf _{\|d\| \leq 1} \phi^{\prime}(x;d)
\]
where the last identity is due to the definition of $\psi$ in \eqref{def:psi}.
Hence, the conclusion directly follows from Definition \ref{def:dsp}.
 \QEDA

\vgap
 
\noindent
	{\bf Proof of Proposition \ref{prop:ours}}
a)
Since $x$ is a
$(\bar \eta,\bar\varepsilon;m)$-regularized stationary point of $\phi$, 
there exists a pair $(w,\varepsilon) \in \R^n \times \R_{++}$ satisfying
\eqref{eq:app_sol}.
Using the fact that
$\phi_m(x;x) = \phi_{2m}(x;x)$ and
$\phi_m(\cdot;x) \le \phi_{2m}(\cdot;x)$, we easily see that
the inclusion in \eqref{eq:app_sol}
implies that
$w \in \partial_\varepsilon [\phi_{2m}(\cdot;x)](x)$.
Since  $\phi_{ m}$ is
convex in view of assumption (A1),
we easily see that
$\phi_{2m}(\cdot;x)$ is
$m$-strongly convex, and hence
the function $\phi_{2m}(\cdot;x)-\inner{w}{\cdot}$
has a global minimizer
$\tilde x$
which, in view of Lemma \ref{lem:mu convex} with $g=\phi_{2m}(\cdot;x)-\inner{w}{\cdot}$ and
$\mu=m $, satisfies
\begin{equation}
w \in \partial \left[ \phi_{2m}(\cdot;x) \right] (\tilde x), \quad \|x-\tilde x \| \le \sqrt{ \frac{2\varepsilon}{m} } \le \sqrt{ \frac{2 \bar \varepsilon}{m} }
\label{eq:nearby}
\end{equation}
where the last inequality is due to the
last inequality in \eqref{eq:app_sol}.
Now, letting
$\hat w := w - 2m (\tilde x-x)$, it follows from \eqref{eq:key_chara1} with $m = 2m$ and 
 Lemma \ref{lem:chara_weakly} with $(\varepsilon,x,c) = (0,\tilde x,x)$ that
\begin{equation}\label{eq:belong}
 \hat w \in \partial \left[ \phi_{2m}(\cdot;\tilde x) \right] (\tilde x)
= \partial \phi(\tilde x). 
\end{equation}
Moreover,
using the definition
of $\hat w$ and the triangle inequality, we
have
\[
\|\hat w\| \le \|w\| + 2m \|x-\tilde x\| \le
\bar \eta + 2 \sqrt{2m \bar \varepsilon} ,
\]
where the second inequality is due to the first inequality in
\eqref{eq:app_sol} and the inequality in \eqref{eq:nearby}. Then the above inequality and \eqref{eq:belong} imply that 
\begin{equation}\label{eq:tildedis}
\operatorname{dist}(0,\partial \phi(\tilde x)) \le \bar \eta + 2 \sqrt{2m \bar \varepsilon}.
\end{equation}
Now statement (a) follows from \eqref{eq:nearby}, \eqref{eq:tildedis}, and Proposition \ref{prop:subdir}.

b) Statement (b) immediately follows from the first statement and Proposition \ref{prop:relation}(a) with $\lam=1/m$.
\QEDA

% \begin{proof}
%     Denote $\hat x = \operatorname{prox}_{(1/2m)\phi}(x)$. Then from \eqref{eq:grad_M} with $\mu = 1/2m$ and the assumption that $x$ is a $(\varepsilon,1/(2m))$-Moreau stationary point, we know that
%    \begin{equation}\label{eq:norm_small}
%    2m\|x-\hat x\| \le \varepsilon.
%     \end{equation}
%    Since  $\phi_{ 2m}$ is
% convex in view of assumption (A1), using the optimality condition for minimizing $\phi_{2m}(\cdot;x)$ we have
% \[
% 0 \in   \partial \left[\phi_{2m}(\cdot;x) \right]  (\hat x ).
% \]
% Thus we know that
% \[
% 0 \in \partial_{\bar \varepsilon} [ \phi_{2m}(\cdot;x)](x)
% \]
% where 
% \[
%  \bar \varepsilon
% := \phi_{2m}(x;x) - \phi_{2m}(\hat x;x) =
% \phi(x) - \phi(\hat x) - m \|\hat x-x\|^2.
% \]
% Moreover, using the assumption that $\phi$ is $M$-Lipschitz, we can get
% \[
% \bar \varepsilon \le \phi(x) - \phi(\hat x)
% \le M \|\hat x-x\|.
% \]
% The statement now follows from the above inequality, \eqref{eq:norm_small} and the definition of $(0,M\varepsilon/(2m);2m)$-stationary point in Definition \ref{def:appr}.
% \end{proof}


\section{Important results about recursive formula}
\label{IRRF}
% 	\begin{lemma}\label{lem:keyrecur}
% 		Assume that for some nonnegative scalars $a$ and $b$ such that $a+b>0$ and
%   sequence of  positive scalars $\{q_l\}_{l \ge 0}$, we have that
% 		\begin{equation}\label{ineq:recursive}
% 		q_{l}\left(1 + \frac{q_{l}}{a + b q_{l}}\right) 
%   % =  q_{l} + \frac{q_{l}^2}{a
%   % + b q_{l}} 
%   \le q_{l-1}, \ \ \forall l \ge 1.
% 		\end{equation}
% 		% \begin{itemize}\label{lem:waiting}
% 		% 	\item[a)] for any $l > 0$, we have
% 		% 	\begin{equation}
% 		% 	\label{ineq:induction}	
% 		% 	q_l   \le 
% 		% 	%\frac{\max\{2c, t_l\}}{j-l}=
% 		% 	\frac{2a +   2b t_{0} }{l};
% 		% 	\end{equation}
% 		% 	\item[b)] 
% 		% 	an index $l$ such that
% 		% 	$q_l \le 2a/b$ can be
% 		% 	found in a number of iterations bounded by
% 		% 	\[
% 		% 	1+\lceil 4b \rceil \log\left( \frac{b t_0}{a}\right).
% 		% 	\]
% 		% \end{itemize}
%  Then, for any given $\delta>0$,
%  there exists an  index $\hat l \ge 1$ such that $q_{ \hat l} \le \delta$ and
%   \begin{equation}\label{def:tl}
%      \hat l \le (2b+3) \log ^+ \left(\frac{  q_0}{\delta + a/(b+1)}\right) + \frac{2a}{\delta} + b +2.
%   \end{equation}
%   \end{lemma}
%   	\begin{proof}
%  Define $\tilde \delta := \delta + a/(b+1)$ and
%  $\theta := 1/2(b+1)$.
%  % Let
%  % $\bar l := \sup \{ l \ge 1: q_l  \ge \tilde \delta\}$.
%  We first claim that
% \begin{equation}\label{ineq:j}
% \bar l :=  \max \{ l \ge 1: q_l  \ge \tilde \delta\} \le {\cal T}:= \frac{\log ^ + (q_0/\tilde \delta)}{ \log (1+\theta) }\le (1+\theta^{-1})\log ^+\left(\frac{q_0}{\tilde \delta}\right).
% \end{equation}
% % \[
% % \frac{\theta}{1+\theta} \le \log (1+\theta) \le \theta
% % \]
% % ????? The claim is obvious if $q_0 < \tilde \delta$. To show the claim holds for $q_0 \ge \tilde \delta$ , 
% Assume for contradiction that there exists an index $j > {\cal T} $ such
% that $q_j \ge  \tilde \delta$.
% Using the fact
% $\{q_l\}$ is non-increasing (see \eqref{ineq:recursive}) and the definitions of $\tilde \delta$ and $\theta$,
% we have that for every  $l=1,\ldots,j$,
%  \begin{align*}
%  \frac{q_l}{a + b q_l} \ge \frac{\tilde \delta}{a + b\tilde \delta} = 
%  \frac{\delta + a/(b+1)}{a + b\delta + ab/(b+1)} 
%  \ge 
%  \frac{\delta + a/(b+1)}{2 a + b\delta} 
%  % = \frac{\delta (b+1)+a}{(b+1)(2a +b) \delta}
%  \ge \frac{1}{2(b+1)} = \theta.
%  \end{align*}
%  Hence, 
%  in view of \eqref{ineq:recursive}, we conclude that
%  $
%  (1+\theta)^j q_{j} \le
%  (1+\theta)^{j-1} q_{j-1}
%   \le  \cdots \le  q_0 
%  $.
%  On the other hand, using the
%  fact that  $j > \cal T$ and the definition of $\cal T$ in \eqref{ineq:j}, we easily see that
%  $q_0 < (1+ \theta)^j \tilde \delta$. Combining the last two conclusions, it follows that $q_j < \tilde \delta$.
%  % where the strict inequality is due to the fact that  $j > \cal T$ and the definition of $\cal T$ in \eqref{ineq:j}. 
%  Since the last conclusion contradicts the fact that $q_j \ge \tilde \delta$, the claim follows.
% %  Thus we have
% %  \begin{equation}\label{ineq:j}
% %  j \le \log \frac{q_0}{\tilde \delta} / \log \frac{2b+3}{2b+2} \le 2(b+1) \log \frac{q_0}{\tilde \delta}.
% % \end{equation}

% Note that the definition
% of $\bar l$ in \eqref{ineq:j} implies that
% $\tilde \delta > q_{\bar l+1} \ge q_l$ for any $l \ge \bar l+1$. This fact, the assumption that $a \ge 0$ and $ b \ge 0$, and relation
%  \eqref{ineq:recursive} then imply that
%    \[
%    \frac{1}{q_{l}} - \frac{1}{q_{l-1}}  \ge \frac{1}{a
%   + (b+1) q_{l }} \ge \frac{1}{a
%   + (b+1) \tilde \delta}
%   \quad \forall l \ge \bar l+1.
%    \]
%    % where the second inequality is due to and the fact that $q_{l} \le q_{\bar l+1} < \tilde \delta$ for any $l \ge \bar l + 1$.
%    Thus, for any given $l \ge \bar l+2$, summing the above inequality from $\bar l +2$ to $l$ and using the fact that $q_{\bar l+1} < \tilde \delta$,
%    we then conclude that
%    \[
%    \frac{1}{q_l}  \ge \frac{1}{q_{\bar l +1}} +  \frac{l-1-\bar l }{a
%   + (b+1) \tilde \delta} > \frac{1}{\tilde \delta} +  \frac{l-1-\bar l }{a
%   + (b+1) \tilde \delta}  \ge  \frac{l-\bar l }{a
%   + (b+1) \tilde \delta},
%    \]
%    and hence that
% \begin{equation}\label{ineq:side}
%        q_l \le \frac{a
%   + (b+1) \tilde \delta}{l-\bar l } = \frac{2a + (b+1)\delta }{l-\bar l } \quad \forall l \ge \bar l +2,
%    \end{equation}
%   where the equality is due to the definition of $\tilde \delta$.
%   The conclusion of the lemma can now be easily seen to follow from \eqref{ineq:j} and 
%   \eqref{ineq:side}.
%   % Thus using the definition of $\tilde l$ in \eqref{def:tl} and the first claim in \eqref{ineq:j} we have
%   % \[q_{\tilde l}\le \frac{2a+(b+1)  \delta }{\tilde l-\bar l } \le \frac{2a + (b+1)\delta }{(2a + (b+1) \delta)/\delta} = \delta.
%   % \]
%   % which implies the conslusion of the lemma.
% 	\end{proof}
	
\begin{lemma}\label{lem:keyrecur}
		Assume that for some integer $J >0$, scalars $\delta>0$, $a \ge 0$ and $b \ge 0$ such that $a+b>0$, and
  scalars $\{q_0,\ldots,q_{J} \}$, we have that
		\begin{equation}\label{ineq:recursive1}
		q_{j} > \delta
  % =  q_{l} + \frac{q_{l}^2}{a
  % + b q_{l}} 
 \ \ \forall j \in \{0,\ldots,J\},
		\end{equation}
        and
        \begin{equation}\label{ineq:recursive2}
		q_{j}\left(1 + \frac{q_{l}}{a + b q_{j}}\right) 
  % =  q_{l} + \frac{q_{l}^2}{a
  % + b q_{l}} 
  \le q_{j-1}, \ \ \forall j \in \{1,\ldots,J\}.
		\end{equation}
		% \begin{itemize}\label{lem:waiting}
		% 	\item[a)] for any $l > 0$, we have
		% 	\begin{equation}
		% 	\label{ineq:induction}	
		% 	q_l   \le 
		% 	%\frac{\max\{2c, t_l\}}{j-l}=
		% 	\frac{2a +   2b t_{0} }{l};
		% 	\end{equation}
		% 	\item[b)] 
		% 	an index $l$ such that
		% 	$q_l \le 2a/b$ can be
		% 	found in a number of iterations bounded by
		% 	\[
		% 	1+\lceil 4b \rceil \log\left( \frac{b t_0}{a}\right).
		% 	\]
		% \end{itemize}
 Then, 
 % for any given $\delta>0$,
 % there exists an  index $\hat l \ge 1$ such that $q_{ \hat l} \le \delta$ and
  \begin{equation}\label{def:tl}
     J \le \bar N :=(2b+3) \log ^+ \left(\frac{  q_0}{\delta + a/(b+1)}\right) + \frac{2a}{\delta} + b +2.
  \end{equation}
  \end{lemma}
	\begin{proof}
 Define $\tilde \delta := \delta + a/(b+1)$ and
 $\theta := 1/2(b+1)$. Suppose for contradiction that $J > \bar N$.
 % Let
 % $\bar l := \sup \{ l \ge 1: q_l  \ge \tilde \delta\}$.
 We first claim that
\begin{equation}\label{ineq:j}
\bar j :=  \max \{ j \le J: q_j  \ge \tilde \delta\} \le {\cal T}:= \frac{\log ^ + (q_0/\tilde \delta)}{ \log (1+\theta) }\le (1+\theta^{-1})\log ^+\left(\frac{q_0}{\tilde \delta}\right)\le \bar N.
\end{equation}
% \[
% \frac{\theta}{1+\theta} \le \log (1+\theta) \le \theta
% \]
% ????? The claim is obvious if $q_0 < \tilde \delta$. To show the claim holds for $q_0 \ge \tilde \delta$ , 
Assume for contradiction that there exists an index $j > {\cal T} $ such
that $q_j \ge  \tilde \delta$.
Using the fact
$\{q_j\}$ is non-increasing (see \eqref{ineq:recursive2}) and the definitions of $\tilde \delta$ and $\theta$,
we have that for every  $l=1,\ldots,j$,
 \begin{align*}
 \frac{q_l}{a + b q_l} \ge \frac{\tilde \delta}{a + b\tilde \delta} = 
 \frac{\delta + a/(b+1)}{a + b\delta + ab/(b+1)} 
 \ge 
 \frac{\delta + a/(b+1)}{2 a + b\delta} 
 % = \frac{\delta (b+1)+a}{(b+1)(2a +b) \delta}
 \ge \frac{1}{2(b+1)} = \theta.
 \end{align*}
 Hence, 
 in view of  \eqref{ineq:recursive2}, we conclude that
 $
 (1+\theta)^j q_{j} \le
 (1+\theta)^{j-1} q_{j-1}
  \le  \cdots \le  q_0 
 $.
 On the other hand, using the
 fact that  $j > \cal T$ and the definition of $\cal T$ in \eqref{ineq:j}, we easily see that
 $q_0 < (1+ \theta)^j \tilde \delta$. Combining the last two conclusions, it follows that $q_j < \tilde \delta$.
 % where the strict inequality is due to the fact that  $j > \cal T$ and the definition of $\cal T$ in \eqref{ineq:j}. 
 Since the last conclusion contradicts the fact that $q_j \ge \tilde \delta$, the claim follows.
%  Thus we have
%  \begin{equation}\label{ineq:j}
%  j \le \log \frac{q_0}{\tilde \delta} / \log \frac{2b+3}{2b+2} \le 2(b+1) \log \frac{q_0}{\tilde \delta}.
% \end{equation}

Note that the definition
of $\bar j$ in \eqref{ineq:j} implies that
$\tilde \delta > q_{\bar j+1} \ge q_j$ for any $j \ge \bar j+1$. This fact, the assumption that $a \ge 0$ and $ b \ge 0$, and relation
 \eqref{ineq:recursive2} then imply that
   \[
   \frac{1}{q_{j}} - \frac{1}{q_{j-1}}  \ge \frac{1}{a
  + (b+1) q_{j }} \ge \frac{1}{a
  + (b+1) \tilde \delta}
  \quad \forall j \ge \bar j+1.
   \]
   % where the second inequality is due to and the fact that $q_{l} \le q_{\bar l+1} < \tilde \delta$ for any $l \ge \bar l + 1$.
   Thus, for any given $j \ge \bar j+2$, summing the above inequality from $\bar j +2$ to $j$ and using the fact that $q_{\bar j+1} < \tilde \delta$,
   we then conclude that
   \[
   \frac{1}{q_j}  \ge \frac{1}{q_{\bar j +1}} +  \frac{j-1-\bar j }{a
  + (b+1) \tilde \delta} > \frac{1}{\tilde \delta} +  \frac{j-1-\bar j }{a
  + (b+1) \tilde \delta}  \ge  \frac{j-\bar j }{a
  + (b+1) \tilde \delta},
   \]
   and hence that
\begin{equation}\label{ineq:side}
        q_{\bar N} \le \frac{a
  + (b+1) \tilde \delta}{\bar N-\bar j } = \frac{2a + (b+1)\delta }{\bar N-\bar j } \le \delta
   \end{equation}
  where the equality is due to the definition of $\tilde \delta$ and the last inequality is due to the definition of $\bar N$ and $\bar j$ in \eqref{def:tl} and \eqref{ineq:j}, respectively. Hence, the above inequality contradicts  \eqref{ineq:recursive1} and thus the statement follows.
  % The conclusion of the lemma can now be easily seen to follow from \eqref{ineq:j} and 
  % \eqref{ineq:side}.
  % Thus using the definition of $\tilde l$ in \eqref{def:tl} and the first claim in \eqref{ineq:j} we have
  % \[q_{\tilde l}\le \frac{2a+(b+1)  \delta }{\tilde l-\bar l } \le \frac{2a + (b+1)\delta }{(2a + (b+1) \delta)/\delta} = \delta.
  % \]
  % which implies the conslusion of the lemma.
	\end{proof}


\section{Technical results for the proof of Theorem \ref{thm:main1}}
\label{APP:analysis}
The main result of this section is Lemma \ref{lem:bdd_smdist} which was used in the proof of Lemma \ref{lem:t1}. 
	
	Before stating and proving Lemma \ref{lem:bdd_smdist},
	we first present two technical results whose proof can be found in Appendix A of \cite{liang2024unified}.
	\begin{lemma}\label{lem:prox-dist}
	Let $z_0\in \R^n$, $0<\zeta \lam < \lam$ and $\Gamma\in \bConv{n}$ be given, and
	define
	\[
	    z_\lam=\underset{u\in \R^n}\argmin \left\lbrace \Gamma(u) +\frac{1}{2 \lam}\|u- z_0 \|^2 \right\rbrace,\quad
	    z_{\zeta \lam}=\underset{u\in \R^n}\argmin \left\lbrace \Gamma(u) +\frac{1}{2 \zeta \lam}\|u-z_0\|^2 \right\rbrace.
	\]
	Then, we have $\|z_\lam-z_0\|\le (\lam/\zeta \lam) \|z_{\tilde\lam}-z_0\| $.
	\end{lemma}


	
	\begin{lemma}\label{lem:descent}
% 	{\bf Talk about $h$}
	For some $(M,L) \in \R^2_+$, assume that $(z_0,\lam)\in \R^n \times (0,1/ L)$,
	function
	 $ \tilde f  \in 
	 \bConv{n} \cap {\cal C}(M,L)$ and
	 $\Gamma, h \in \bConv{n} $ are such that
	 \[
	 \ell_{\tilde f}(\cdot;z_0)+h \le \Gamma \le \tilde f+h.
	 \]
	 Moreover,
	define
	\begin{equation}
	    z_\lam:=\underset{u\in \R^n}\argmin \left\lbrace \Gamma(u) +\frac{1}{2 \lam}\|u- z_0 \|^2 \right\rbrace.
	   \label{eq:z_lam}
	\end{equation}
	Then, for every $u\in \dom h$, we have
	\begin{equation}\label{ineq:descent}
	    \frac{1}{2\lam} \|u-z_\lam\|^2 + (\tilde f+h)(z_\lam) - (\tilde f+h)(u) \le \frac{1}{2\lam} \|u-z_0\|^2 + \frac{2\lam M^2}{1-\lam L}.
	\end{equation}
	\end{lemma}
	
 
 
 \begin{lemma}\label{lem:bdd_smdist}
	For some $(M,L) \in \R^2_+$ and $m \ge 0$, assume that $(z_0,\lam)\in \R^n \times \R_{++}$,
	function
	 $f  \in {\cal C}(M,L)$ is $m$-weakly convex, and
	 $\Gamma, h \in \bConv{n} $ are such that
	 \[
	 \ell_{ f}(\cdot;z_0)+h \le \Gamma \le f_m(\cdot;z_0)+h,
	 \]
	where $f_m(\cdot;z_0)$ is as in \eqref{def:phim}.
	Then,  for every $u \in \R^n$, we have
	\begin{equation}
	\zeta^2\left(\frac{1}{4\zeta \lam}+\frac{m}{2} \right)\|z_\lam - z_0 \|^2 
	\le
	(f+h)(u) - (f+h)(z_{\zeta\lam}) + \frac12 \left(m+\frac{2}{\zeta \lam} \right) \|u-z_0\|^2 + 4\zeta \lam M^2
% 	(\frac{1}{4 \lam}+\frac{m}{2})\|z - z_0\|^2 \le \phi_{m+\frac{2}{  \lam}}(u;z_0) - \phi^* + 4\lam M^2
\label{eq:uniform_bdd}
	\end{equation}
	where $z_\lam$ is as in \eqref{eq:z_lam} and $\zeta $ is defined as \eqref{label:def_zeta}.
\end{lemma}

 
	\begin{proof}
	We first prove the conclusion of the lemma under the assumption that
	$\lambda$ is such that
	$\lam \in (0, \frac{1}{2(L+m)}]$,
	and hence $\zeta=1$.
	Indeed, noting that
	$f_m(\cdot;z_0) \in \Conv{n} \cap {\cal C}(M,L+m)$ due to Lemma \ref{lem:Lip_ineq}, it follows from Lemma \ref{lem:descent} with
	$\tilde f = f_m(\cdot;z_0)$ and $(\Gamma,z_0,\lam) = (\Gamma, z_0, \lam)$  and the definition of $ \lam$ that
	for any $u \in \dom h$:
    \begin{align*}
	    \frac{1}{2 \lam} \|u- z_{\lam}\|^2 + (f_m(\cdot;z_0)+h)(z_{\lam}) - (f_m(\cdot;z_0)+h)(u) 
	    &\le \frac{1}{2 \lam}  \|u-z_0\|^2 + \frac{2  \lam  M^2}{1- \lam ( L +m)}\\
	    &\le \frac{1}{2  \lam}  \|u-z_0\|^2 + 4\lam M^2,
	\end{align*}
	and hence that
	\begin{align*}
	(f+h)(u) - (f+h)(z_{\lam}) &+ \frac{m}{2}\|u-z_0\|^2+4 \lam M^2
	\ge \frac{1}{2\lam} \left(\|u- z_{\lam}\|^2 - \|u-z_0\|^2 \right) + \frac{m}{2}\| z_{\lam} - z_0\|^2 \\
	&=  \frac{1}{2}(\frac{1}{ \lam} + m)\|z_{\lam}- z_0\|^2 -\frac{1}{\lam}\inner{z_{\lam} - z_0}{u - z_0} \\
	&\ge \frac{1}{2}\left(\frac{1}{ \lam} + m\right)\|z_{\lam} - z_0\|^2 -\frac{1}{2  \lam} \left(\frac{1}{2}\|z_{\lam} - z_0\|^2 + 2\|u - z_0\|^2 \right) 
	\end{align*}
	where the last inequality is due to the fact that
	$\frac{1}{2}a^2+2b^2 \ge 2ab$.
	Rearranging the above inequality, we then conclude that
	\begin{equation}
	   (f+h)(u) - (f+h)(z_{\lam}) + 4\lam M^2\ge  \frac{1}{2}\left(\frac{1}{ 2\lam} + m\right)\|z_{\lam} - z_0\|^2 - \left( \frac1{\lam} +\frac{m}2 \right)  \|u - z_0\|^2
	   \label{eq:small_lam}
	\end{equation}
	which, in view of the fact that
	$\zeta=1$, immediately implies \eqref{eq:uniform_bdd}.
	Next, we show that \eqref{eq:uniform_bdd} also holds for $\lam >1/[2(L+m)]$.
Noting that \eqref{eq:uniform_bdd} implies that $\zeta \lam = 1/[2(L+m)]$, it then follows from \eqref{eq:small_lam} with
$\lam = \zeta \lam$ that
	\begin{align*}
	    (f+h)(u) - (f+h)(z_{\zeta\lam}) + \left( \frac{m}2 +\frac{1}{\zeta \lam} \right)\|u -z_0\|^2+ 4 \zeta\lam M^2 &\ge
	    \left(\frac{1}{4 \zeta \lam}+\frac{m}{2} \right)\|z_{\zeta \lam} - z_0\|^2 \\
	    &\ge \zeta^2\left(\frac{1}{4 \zeta \lam}+\frac{m}{2} \right)\|z_{ \lam} - z_0\|^2
	\end{align*}
	where the last inequality is due to
% 		\left(\frac{1}{4 \zeta \lam}+\frac{m}{2} \right)\|z_{\zeta \lam} - z_0\|^2 \le (f+h)(u) - (f+h)(z_{\zeta\lam}) + \left( \frac{m}2 +\frac{1}{\zeta \lam} \right)\|u -z_0\|^2+ 4 \zeta\lam M^2
% 	\]
	Lemma \ref{lem:prox-dist} with $( \lam,\zeta \lam) = (\lam, \zeta \lam)$.
% 	and $\Gamma = \Gamma$ we have 
% 	\[
% 	\zeta^2(\frac{1}{4 \zeta \lam}+\frac{m}{2})\|z_{ \lam} - z_0\|^2 \le (f+h)(u) - (f+h)^* +  \frac12 \left(m+\frac{2}{\zeta \lam} \right) \|u-z_0\|^2+ 4 \zeta\lam M^2
% 	\]
	\end{proof}




 \section{Further discussion regarding \eqref{ineq:Moreau-bound}} \label{sec:termination}

% \red{change $1$ to 1}

\begin{proposition}
    % Assume $1 > \max\{2/3, 1/(1+\lam m)\}$ and 
   For every $K \ge 2$, then we have
    \[
 \inf_{k \le K} \|\nabla \hat M^{\lam}(\hat y_{k-1})\|^2 
    \le \frac{4(1+\lam m)^2}{\lam} \left(2 \delta + \frac 1K [\phi(\hat x_0) - \phi(\hat y_K)]\right).
\]
\end{proposition}

\begin{proof}
 Let $x_k^*$ be the minimizer of \eqref{sub_pro} for every $k\ge 1$. It follows from Lemma \ref{lem:basic_prop}(a) that for every $u\in \dom h$,
\begin{equation}\label{ineq:hatGamma}
    \hat \Gamma_k(\hat x_k) + \frac1{2\lam} \|\hat x_k-\hat y_{k-1}\|^2 \le
\hat \Gamma_k(u) + \frac1{2\lam} \|u-\hat y_{k-1}\|^2 - \frac1{2\lam} \|u-\hat x_k\|^2.
\end{equation}
The facts that $x_k^*$ is the minimizer of \eqref{sub_pro} and the objective function is $(1/\lam)$-strongly convex imply that
\[
\phi_{ m}(\hat y_k;\hat y_{k-1})+ \frac{1}{2\lam} \|y_{k} - \hat y_{k-1}\|^2  \ge \phi_{ m}(x_k^*;\hat y_{k-1}) + \frac{1}{2\lam} \|x_{k}^* - \hat y_{k-1}\|^2 + \frac{1}{2\lam} \|\hat y_k-x_k^*\|^2.
\]
Using \eqref{ineq:hatGamma} with $u=x_k^*$, the above inequality, \eqref{eq:hattheta}, and \eqref{eq:control}, we have
\begin{align}
    \hat \delta_k &\ge \phi_{ m}(\hat y_k;\hat y_{k-1})+ \frac{1}{2\lam} \|\hat y_{k} - \hat y_{k-1}\|^2  - \hat \Gamma_k(\hat x_k) - \frac{1}{2\lambda} \| \hat x_k - \hat y_{k-1}  \|^2 \nn \\
    &\ge 
 \phi_{ m}(x_k^*;\hat y_{k-1}) + \frac{1}{2\lam} \|x_k^* - \hat y_{k-1}\|^2 + \frac{1}{2\lam} \|\hat y_k-x_k^*\|^2 -\hat \Gamma_k(x_k^*) - \frac1{2\lam} \|x_k^*-\hat y_{k-1}\|^2 + \frac1{2\lam} \|x_k^*-\hat x_k\|^2 \nn \\
 &= \left[ \phi_m(x_k^*;\hat y_{k-1})
 - \hat \Gamma_k(x_k^*)\right] + \frac{1}{2\lam} \|\hat y_k-x_k^*\|^2
 + \frac1{2\lam} \|x_k^*-\hat x_k\|^2 \nn \\
 &\ge \frac{1}{2\lam} \|\hat y_k-x_k^*\|^2
 + \frac1{2\lam} \|x_k^*-\hat x_k\|^2 \label{ineq:second-step}
\end{align}
where the last inequality is due to the second inequality in \eqref{eq:basic}.
On the other hand, using the second inequality in \eqref{eq:basic} and \eqref{ineq:hatGamma}, we have
\begin{align*}
    &\phi_{ m}(\hat y_k;\hat y_{k-1}) + \frac{1}{2\lam} \|\hat y_{k} - \hat y_{k-1}\|^2 - \phi_m(u;\hat y_{k-1}) + \frac1{2\lam}\|u- \hat x_k\|^2 \\
			\le & \phi_{m}(\hat y_k;\hat y_{k-1})+ \frac{1}{2\lam} \|\hat y_{k} - \hat y_{k-1}\|^2  - \hat \Gamma_k(u) + \frac1{2\lam}\|u - \hat x_k\|^2\\
			\le & \phi_{ m}(\hat y_k;\hat y_{k-1}) + \frac{1}{2\lam} \|\hat y_{k} - \hat y_{k-1}\|^2 - \hat \Gamma_k(\hat x_k) - \frac1{2\lam} \|\hat x_k-\hat y_{k-1}\|^2 + \frac1{2\lam}\|u- \hat y_{k-1}\|^2
			\le \delta  + \frac{1}{2\lam}\|u- \hat y_{k-1}\|^2
\end{align*}
where the last inequality is due to \eqref{eq:hattheta} and \eqref{eq:control}.
% \[
% \phi_m(\hat y_k;\hat y_{k-1}) - \phi_m(u;\hat y_{k-1}) + \frac{1}{2\lam}\|\hat y_k-\hat y_{k-1}\|^2 \le \frac{1}{2\lam}\|u-\hat y_{k-1}\|^2 - \frac{1}{2\lam}\|u-\hat x_k\|^2
% + \delta
% \]
The above equation with $u=\hat y_{k-1}$ imply that
\[
\frac{1}{2\lam}\|\hat y_k-\hat y_{k-1}\|^2 + \frac{1}{2\lam}\|\hat x_k-\hat y_{k-1}\|^2
\le \hat \delta_k + \phi(\hat y_{k-1}) - \phi_m(\hat y_k;\hat y_{k-1}).
\]
Combining the above inequality with \eqref{ineq:second-step},
we then conclude that
\begin{align}
    &2 \hat \delta_k + \phi(\hat y_{k-1}) - \phi_m(\hat y_k;\hat y_{k-1}) \nn \\ 
    &\ge \left(\frac{1}{2\lam}\|\hat x_k-\hat y_{k-1}\|^2
+ \frac1{2\lam} \|x_k^*-\hat x_k\|^2\right) + \left(\frac{1}{2\lam}\|\hat y_k-\hat y_{k-1}\|^2 + \frac{1}{2\lam}\|\hat y_k-x_k^*\|^2\right) \nn \\
&\ge \frac1{4\lam} \|x_k^*-\hat y_{k-1}\|^2 + \frac{1}{4\lam} \|x_k^*-\hat y_{k-1}\|^2 = \frac{1}{2\lam}\|x_k^*-\hat y_{k-1}\|^2 \label{ineq:xy}
\end{align}
where the second inequality is due to the fact that $2a^2+2b^2\ge(a+b)^2$ for every $a, b \in \R$ and the triangle inequality.
The above inequality with $k=1$ implies that
\begin{equation}\label{ineq:k=1}
    \frac{1}{2\lam}\|x_1^*-\hat y_0\|^2 \le 2 \hat \delta_1 + \phi(\hat y_0) - \phi(\hat y_1) - \frac m{2}\|\hat y_1- \hat y_0\|^2.
\end{equation}
Moreover, \eqref{ineq:xy} with $k\ge 2$ implies that
\begin{align*}
     &\frac{1}{2\lam}\|x_k^*-\hat y_{k-1}\|^2 \le 2 \hat \delta_k + \phi(\hat y_{k-1}) - \phi(\hat y_k) - \frac m{2}\|\hat y_{k}- \hat y_{k-1}\|^2\\
    \le & 2 \hat \delta_k + \phi(\hat y_{k-1}) - \phi(\hat y_k).
\end{align*}
Taking minimum over the above inequality from $k=2$ to $K$ and \eqref{ineq:k=1}, using \eqref{eq:new_grad} and \eqref{eq:key_est}, we conclude that
\begin{align*}
    \frac{\lam}{2(1+\lam m)^2} \inf_{k \le K}& \|\nabla \hat M^{\lam}(\hat y_{k-1})\|^2 = \frac{1}{2\lam} \inf_{k \le K} \|x_k^*-\hat y_{k-1}\|^2 \\
    &\le  2\inf_{2 \le k \le K} \hat \delta_k + \frac 1K [\phi(x_0) - \phi(\hat y_K)]\\  &=2\delta+ 2\inf_{2 \le k \le K} \frac{\lam}{8(m\lam+1)}\|\hat w_k\|^2 + \frac 1K [\phi(x_0) - \phi(\hat y_K)]\\
    &\overset{\eqref{eq:key_est}}\le 4\delta + 2\inf_{2\le k\le K} \frac{m\lam+1}{8\lam}\|\hat y_k - \hat y_{k-1}\|^2+ \frac 1K [\phi(x_0) - \phi(\hat y_K)]\\    &\overset{\eqref{eq:wvar_bdd}}\le 4\delta +  \frac 2K [\phi(x_0) - \phi(\hat y_K)].
\end{align*}
where the last inequality is due to \eqref{eq:wvar_bdd}.
Hence, the proposition immediately follows.
\end{proof}



  
 


 

%  \subsection{Paper [2]}

% Under the assumption that $j_k \ge 11/(\gamma^2\mu^2)$, paper [2] shows that
%  \[
% \phi(x_{k+1}) - \phi(x_k) \le \frac{72 L^2}{\mu (j_k+1)} - \frac{\mu}4 \|x_k-\hat x_k\|^2 ?????
% \]
% where
% \[
% \mu = \frac{1}\gamma - m
% \]
% \beq \label{eq:2}
% \hat x_k := \underset{u\in \R^n}\argmin \left\lbrace \phi(u) +\frac{1}{2 \gamma}\|u- x_k \|^2 \right\rbrace 
% \eeq
% and $x_{k+1}$ is the output obtain by performing $j_k$
% iterations of the SM to \eqref{eq:2}.
% In particular, if 
% \[
% j_k :=\left\lceil\max \left(\frac{576 L^2}{\mu^2 \min \left\{\varepsilon, \varepsilon \gamma^2\right\}}, \frac{11}{\gamma^2 \mu^2}\right)\right\rceil.
% \]
% then it follows that
% \[
% \|x_k-\hat x_k\|^2 \le
%      \frac4 {\mu} [\phi(x_{k}) - \phi(x_{k+1})  ] + \varepsilon^2
% \]
% A sufficient condition for the above inequality is that
% \[
% \phi(x_k) - \phi(\hat x_k) \le
%      2 [\phi(x_{k}) - \phi(x_{k+1})  ] + \frac{\mu}2 \varepsilon^2
% \]
% or
% \[
% \phi(x_{k+1}) - \phi(\hat x_k) \le
%      \phi(x_{k}) - \phi(x_{k+1})   + \frac{\mu}2 \varepsilon^2
% \]
% or
% \[
% \phi(x_{k+1}) - \phi(\hat x_k)- \frac{1}{2\gamma} \|\hat x_k -x_k\|^2 \le
%      \phi(x_{k}) - \phi(x_{k+1})
%     - \frac{1}{2\gamma} \|\hat x_k -x_k\|^2
%     + \frac{\mu}2 \varepsilon^2 \quad (*)
% \]
% Have
% \[
% \phi(\hat x_k)+ \frac{1}{2\gamma} \|\hat x_k -x_k\|^2
% \ge \Gamma(\hat x_k)+ \frac{1}{2\gamma} \|\hat x_k -x_k\|^2
% \ge m_k + \frac{1}{2\gamma} \| \hat x_k- x_{k+1}\|^2 
% \]
% So a sufficient condition for $(*)$ is that
% \[
% \phi(x_{k+1}) - m_k
% \le  \phi(x_{k}) - \phi(x_{k+1})
% + \frac{1}{2\gamma} \| \hat x_k- x_{k+1}\|^2
%     - \frac{1}{2\gamma} \|\hat x_k -x_k\|^2
%     + \frac{\mu}2 \varepsilon^2
% \]


% \begin{lemma}\label{lem:descent}
% 	Let $(\Gamma,z_0,\lam)\in \mConv{n} \times \R^n \times (0,1/\bar L_f)$ be a triple such that 
%  \begin{equation}\label{ineq:local}
%      \ell_f(\cdot;z_0)+h \le \Gamma \le \phi
%  \end{equation}
%  and
% 	define
% 	\begin{equation}\label{eq:x}
% 	    z:=\underset{u\in \R^n}\argmin \left\lbrace \Gamma(u) +\frac{1}{2 \lam}\|u- z_0 \|^2 \right\rbrace.
% 	\end{equation}
% 	Then, for every $u\in \dom h$, we have
% 	\begin{equation}\label{ineq:descent}
% 	    \frac{1}{2}\left( \mu + \frac{1}{\lam}\right) \|u-z\|^2 + \phi(z) - \phi(u) \le \frac{1}{2\lam} \|u-z_0\|^2 + \frac{2\lam \bar M_f^2}{1-\lam \bar L_f}.
% 	\end{equation}
% 	\end{lemma}

%  The above lemma with $u=z_0=x_k$, $z=x_{k+1}$, and

% %  \[
% % \phi=\phi+\frac1{2\gamma}  \|\cdot-z_0\|^2
% %  \]
% %  implies that
% %  \begin{align*}
% %         \frac{1}{2}\left( \mu + \frac{1}{\lam}\right) \|z_0-z\|^2 + \phi(z) + \frac1{2\gamma}  \|z-z_0\|^2 - \phi(z_0) \le \frac{2\lam \bar M_f^2}{1-\lam \bar L_f}
% %  \end{align*}


% \[
% \frac{1}{2}\left( \mu + \frac{1}{\lam}\right) \|z_0-z\|^2 + \phi(z)  - \phi(z_0) \le \frac{2\lam \bar M_f^2}{1-\lam \bar L_f}
% \]

% \[
% \left[ \frac{\mu}{2}\|z_0-z\|^2 - \phi(z_0) \right] 
% + \phi(z)  + 
%  \frac{1}{2\lam} \|z_0-z\|^2
%  \le \frac{2\lam \bar M_f^2}{1-\lam \bar L_f}
% \]
% \[
% \left[ \frac{\mu}{2}\|z_0-z\|^2 - \phi(z_0) \right] + \phi(\hat x_k) +\frac{1}{2\lam}\|\hat x_k-z_0\|^2 + \frac{1}{2}\left(\mu+ \frac{1}{\lam}\right)\|z-\hat x_k\|^2 \le \frac{2\lam \bar M_f^2}{1-\lam \bar L_f}
% \]

% --------Convex-----------

% {\bf Our paper:}

% \[
% t= \phi(y) + \frac{1}{2\gamma} \|y-x_0\|^2  - m \le \delta
% \]
% \[
% x = \argmin \left \{ \Gamma(u) + \frac1{2\gamma} \|u-x_0\|^2 \right\}
% \]
% \[
% \hat x = \argmin \left \{ \phi(u) + \frac1{2\gamma} \|u-x_0\|^2 \right\}
% \]
% {\bf Question:} How to bound $\|x_0-\hat x\|$ ?



% \[
% \delta \ge \phi(y) + \frac{1}{2\gamma} \|y-x_0\|^2 -\Gamma(x) - \frac1{2\gamma} \|x-x_0\|^2 
% % \ge
% % \phi(y)- \Gamma(y) - \frac1{2\gamma} \|y-x_0\|^2
% \]
% \begin{align*}
%     \delta &\ge 
%     \phi(y) + \frac{1}{2\gamma} \|y-x_0\|^2 -\Gamma(x) - \frac1{2\gamma} \|x-x_0\|^2 \\
%     &\ge \phi(\hat x) +  \frac{1}{2\gamma} \|\hat x-x_0\|^2 + \frac{1}{2\gamma} \|y-\hat x\|^2 -\Gamma(x) - \frac1{2\gamma} \|x-x_0\|^2
% \end{align*}
% Have
% \[
% \Gamma(x) + \frac1{2\gamma} \|x-x_0\|^2 \le
% \Gamma(\hat x) + \frac1{2\gamma} \|\hat x-x_0\|^2 - \frac1{2\gamma} \|\hat x-x\|^2
% \]
% So, we conclude that
% \begin{align}
%     \delta &\ge 
%  \phi(\hat x) + \frac{1}{2\gamma} \|\hat x-x_0\|^2 + \frac{1}{2\gamma} \|y-\hat x\|^2 -\Gamma(\hat x) - \frac1{2\gamma} \|\hat x-x_0\|^2
%  + \frac1{2\gamma} \|\hat x-x\|^2 \nn \\
%  &= [ \phi(\hat x)
%  - \Gamma(\hat x)] +
%  \frac{1}{2\gamma} \|y-\hat x\|^2
%  + \frac1{2\gamma} \|\hat x-x\|^2 \label{ineq:first-step}
% \end{align}

% On the other hand, similar to equation  (45) of \cite{liang2021unified}, we have
% \[
% \phi( y) + \frac{1}{2\gamma}\|y-x_0\|^2 - \phi(u) \le \frac{1}{2\gamma}\|x_0-u\|^2 - \frac{1}{2\gamma}\|x-u\|^2
% + \delta
% \]
% The above equation with $u=x_0$ implies
% \[
% \frac{1}{2\gamma}\|y-x_0\|^2 + \frac{1}{2\gamma}\|x-x_0\|^2
% \le \delta + \phi(x_0) - \phi(y)
% \]

% Combining \eqref{ineq:first-step},
% we then conclude that
% \begin{align*}
%     2 \delta + \phi(x_0) - \phi(y) \ge \left(\frac{1}{2\gamma}\|x-x_0\|^2
% + \frac1{2\gamma} \|\hat x-x\|^2\right) + \left(\frac{1}{2\gamma}\|y-x_0\|^2 + \frac{1}{2\gamma}\|y-\hat x\|^2\right) \ge \frac1{2\gamma} \|\hat x-x_0\|^2
% \end{align*}

% -------------------

% \[
% \frac{1}{2\gamma}\|y-x_0\|^2 + \frac{1}{2\gamma}\|x-y_0\|^2
% \le \delta + \phi(y_0) - \phi(y)
% + \frac{1}{2\gamma}\|y_0-x_0\|^2
% \]

% \begin{align*}
%    \frac{1}{2\gamma}\|y_0-x_0\|^2 + 2 \delta + \phi(y_0) - \phi(y) 
%     &\ge \left(\frac{1}{2\gamma}\|x-y_0\|^2
% + \frac1{2\gamma} \|\hat x-x\|^2\right) + \left(\frac{1}{2\gamma}\|y-x_0\|^2 + \frac{1}{2\gamma}\|y-\hat x\|^2\right) 
% \\
% &\ge \frac1{4\gamma} \left( \|\hat x-x_0\|^2 + \|\hat x-y_0\|^2 \right)
% \end{align*}


% Have
% \[
% \Gamma(x) + \frac1{2\gamma} \|x-x_0\|^2 \le
% \Gamma(y)+ \frac1{2\gamma} \|y-x_0\|^2 - \frac1{2\gamma} \|y-x\|^2
% \]



% \begin{align*}
%     \delta &\ge 
%     \phi(y) + \frac{1}{2\gamma} \|y-x_0\|^2 -\Gamma(x) - \frac1{2\gamma} \|x-x_0\|^2 \\
%     &\ge \phi(y) + \frac{1}{2\gamma} \|y-x_0\|^2 -\Gamma(y)- \frac1{2\gamma} \|y-x_0\|^2 + \frac1{2\gamma} \|y-x\|^2\\
%     &\ge \frac1{2\gamma} \|y-x\|^2
% \end{align*}



 

% % -------------------------

% % With our tolerance
% % $(\varepsilon,\eta)$
% % \[
% % \delta_0 = \frac{\lam \min \{ \eta^2, }
% % \]
% % \[
% % \mbox{Inner} ??????= 1 + \frac{1}{\delta_0} = 1+ \frac{1}{\min\{ m \lam \bar \varepsilon,\lam N \bar \eta^2\}},
% % \quad \mbox{Outer} = \frac{1}{\bar \eta^2}
% % \ \ (\mbox{if $1=1$})
% % \]


% % ----------------- our results --------------------------

% % \[
% % Inner: \frac{1}{\delta_0} \quad Outer: \frac{1}{\bar \eta^2}
% % \]



% % Plug in
% % \[
% % \delta_0 = \min \{\bar \eta^2, \bar \varepsilon\}
% % \]
% % \[
% % \mbox{Total} = \frac{1}{\bar \eta^2} \frac{1}{\min \{\bar \eta^2, \bar \varepsilon\}}
% % \]

% % ------- their definition --------

% % 0)
% % \[
% % \partial_\delta f(\mathbf{x}):=\operatorname{conv}\left(\cup_{\mathbf{y}:\|\mathbf{y}-\mathbf{x}\| \leq \delta} \partial f(\mathbf{y})\right)
% % \]

% % They require that:

% % 1) the existence of $\hat w$ such that
% % \[
% % \hat w \in \partial_{\hat \delta} f(x), \quad \|\hat w\| \le \varepsilon
% % \]

% % 2) or equivalently,
% % the existence of $\hat w$ such that
% % \[
% % \hat w \in \partial f(y), \quad \|y-x\| \le \delta \, \quad  \|\hat w\| \le \varepsilon
% % \]

% % 3) or equivalently,
% % the existence of $\hat w$ such that
% % % \[
% % % \hat w \in \partial f(y), \quad \|y-x\| \le \delta \, \quad  \|\hat w\| \le \varepsilon
% % % \]
% % \[
% % 0 \in \partial_{\varepsilon} f(y), \quad \|y-x\| \le \delta
% % \]

% % Have
% % \[
% % (3) \Leftrightarrow (2) 
% %   \Rightarrow (1) \Leftrightarrow(0)
% % \]

% % -------------------------------

% % % So Proposition 1.9 means that x is an approximate solution of our definition implies
% % % \[
% % % \hat w \in \partial f(y) = \partial f(y)
% % % \]
% % % and 
% % % \[
% % % \|y-x\| \le \hat \delta 
% % % \]
% % % where you require
% % % \[
% % % \min \left\{\|\mathbf{g}\|: \mathbf{g} \in \partial_\delta f(\mathbf{x})\right\} \leq \epsilon
% % % \]
% % % So it means that  if 
% % % \[
% % % \hat w \le \varepsilon
% % % \]
% % % in our case then we also satisfy their definition.

% % Have
% % \[
% %  \varepsilon = \bar \eta + 2 \sqrt{2 m \bar \varepsilon }, \quad  \delta = \sqrt{\frac{2\bar \varepsilon}{m}}.
% % \]
% % If you choose

% % \[
% % \bar \eta = \frac{\varepsilon}{2}, \quad \bar \varepsilon = \frac{\min \{m \delta^2,\varepsilon ^2\} }{2}
% % \]
% % Recall that
% % \[
% % \mbox{Outer} = \frac{1}{\bar \eta^2}
% % \ \ (\mbox{if $1=1$})
% % \]
% % \[
% % \mbox{Total} = \frac{1}{\bar \eta^2} \frac{1}{\min \{\bar \eta^2, \bar \varepsilon\}}
% % \]
% % % Then 
% % % \[
% % % \hat \varepsilon \le \varepsilon \quad \hat \delta \le \delta
% % % \]
% % So our final complexity becomes
% % \[
% % \frac{1}{\varepsilon^2}\frac{1}{\min\{\varepsilon^2,\delta^2\}} 
% % \]

% % --------------

% % {\bf Conjecture:} 
% % You need to do:

% % (1)
% % \[
% % \mbox{Inner} = \frac{1}{\bar \varepsilon} 
% % \]

% % Implication:
% % \[
% % \mbox{Total} = \frac{1}{\bar \eta^2} \frac{1}{\bar \varepsilon} = \frac{1}{\varepsilon^2}
% % \frac{1}{\min\{\delta^2,\varepsilon^2\}}
% % \]



% % -----------------

% % (2)
% % \[
% % \bar \varepsilon = \delta^2 
% % \]

% % =========================\\
% % We know how show that
% % \[
% % \mbox{Inner} = \frac{1}{\min\{\varepsilon^2,\delta^2\}},
% % \quad \mbox{Outer} = \frac{1}{\varepsilon^2}
% % \]
% % \[
% % \frac{1}{\min\{\varepsilon^2,\delta^2\}} \frac{1}{\varepsilon^2}
% % \]
% % We would like to show
% % \[
% % \frac{1}{\delta \varepsilon^3}
% % \]]

% % --------------DISCUSSION------------------\\
% % For the weakly convex nonsmooth case,
% % If x is an approximate solution,
% % then it implies   there exists
% %     $y \in \R^n$ such that
% %     \[
% %     \|y - x\| \leq \hat \delta,
% %     \quad
% % \hat w \in   \partial f(\hat y) .
% % \]
% % where $\|\hat w\|$ is small. Thus
% % \[
% % \hat w \in \partial_{\hat \delta} f(x)
% % \]
% % ==============================================\\
% % \[
% % \E(X) = \E(\max( X, X) )
% % \]
% % \[
% % \E( \max( X, Y) )
% % = \sum _w p_w \max(X(w),Y(w))
% % \ge \max \left( \sum _w p_w X(w), \sum _w p_w Y(w) \right)
% % \]

% % \[
% % \tX(w_1,w_2) = (X(w_1),X(w_2))
% % \]

% % In our case
% % $X=F(u; \cdot)$
% % and
% % $Y=F(u;\cdot)$.

% % Assume that
% % \[
% % E_\xi \left [ (F(u;\xi) - f(u))^2 \right] \le \sigma^2
% % \]
% % \[
% % \ell(u,z_i) \le F(u;\xi_i)
% % \]
% % \[
% % E \left[ \max\{ F(u;\xi_i) - f(u) \} \right] 
% % \le E \left [ \left( \sum_i (F(u;\xi_i) - f(u))^2 \right)^{1/2} \right] \le \left( E \left [ \sum_i (F(u;\xi_i) - f(u))^2  \right] \right)^{1/2}
% % \le 
% % \sqrt{n}\sigma
% % \]
% % \[
% % E \left[ \max\{ F(u;\xi_i) \} \right] \le f(u) + \sqrt{n}\sigma
% % \]

% {\bf Tasks:}
% \begin{itemize}
%     \item 
%     Set $\lam = \theta/m$.
%     \item
%     Think about the case where $m=0$
%     \item
%     \red{Question: check the outer analysis, when $m=0$, can we recover Theorem 6.4 of \cite{liang2021proximal}}
%     \item 
% \end{itemize}

% -------------- Outer analysis ----- 



% --------- when $m=0$ -----

% \[
% \frac{d_0^2}{\lam^2 \bar \eta^2}
% \]


% --------- The result of the first bundle paper ---------

% \[
% 		\mathcal{O}_1\left( \max\left\lbrace \frac{\hat \varepsilon}{\lam \bar \eta^2}, \frac{d_0^2}{\lam \bar \varepsilon} \right\rbrace \right);
% 		\]
% ---------\\

% \begin{itemize}
%     \item[1)]
%     If $
% \lam \eta^2 \ge \varepsilon
% $
% then our complexity is bounded by 
% \[
% \frac{d_0^2}{\lam^2 \eta^2} + 1 \le \frac{d_0^2}{\lam \varepsilon} + 1 \le \mathcal{O}_1\left( \max\left\lbrace \frac{\hat \varepsilon}{\lam \bar \eta^2}, \frac{d_0^2}{\lam \bar \varepsilon} \right\rbrace \right);
% \]
% \item[2)]
% If $\lam \eta^2 \le \varepsilon$, then we have two subcases:
% \begin{itemize}
%     \item[2.a)] if
% $
% \varepsilon \ge \eta d_0
% $, then 
% the old complexity becomes
% \[
% \frac{\varepsilon}{\lam \eta^2}+1
% \]
% since
% \[
% \frac{\varepsilon}{\lam \eta^2} \ge \frac{\varepsilon}{\lam (\varepsilon/d_0)^2} = \frac{d_0^2}{\lam \varepsilon}
% \]
% and our complexity is bounded by
% \[
% \frac{d_0^2}{\lam^2\eta^2}+1 \le \frac{\varepsilon^2}{\lam^2 \eta^4}+1
% \]
% \item[2.b)] if
% $
% \varepsilon \le \eta d_0$, then the
% old complexity becomes
% \[
% \frac{d_0^2}{\lam \varepsilon} + 1
% \]
% since
% \[
% \frac{\varepsilon}{\lam \eta^2} \le \frac{\varepsilon}{\lam (\varepsilon/d_0)^2} = \frac{d_0^2}{\lam \varepsilon}
% \]
% and our complexity is  bounded by
% \[
% \frac{d_0^2}{\lam^2\eta^2} +1 \le \frac{d_0^4}{\lam^2 \varepsilon^2}+1
% \]
% \end{itemize}
% \end{itemize}
% ------------------------------------------------------------
% \begin{itemize}
%     \item[1)] if
% $
% \varepsilon \ge \eta d_0
% $, then 
% the old complexity becomes
% \[
% \frac{\varepsilon}{\lam \eta^2}+1
% \]
% since
% \[
% \frac{\varepsilon}{\lam \eta^2} \ge \frac{\varepsilon}{\lam (\varepsilon/d_0)^2} = \frac{d_0^2}{\lam \varepsilon}.
% \]
% % and our complexity is bounded by
% % \[
% % \frac{d_0^2}{\lam^2\eta^2} +1 \le \frac{ \varepsilon^2/\eta^2}{\lam^2\eta^2}+1 = \frac{\varepsilon^2}{\lam^2 \eta^4}+1
% % \]
% Hence, our complexity is as good as the old one under the condition that
% \[
% 1< \frac{d_0^2}{\lam^2 \eta^2} \le \frac{\varepsilon}{\lam \eta^2}
% \]
% or equivalently
% \[
% \frac{d_0}{\eta} > \lam \ge \frac{d_0^2}{\epsilon}
% \]


% % % \item[2)]


% % % If $d_0^2 \le \lam \varepsilon$, then our complexity is bounded by
% % % \[
% % % \frac{d_0^2}{\lam^2\eta^2} +1 \le \frac{\lam \varepsilon}{\lam^2\eta^2}+1 = \frac{\varepsilon}{\lam \eta^2}+1
% % % \]
%     \item[2)] if
% $
% \varepsilon \le \eta d_0
% $, then 
% the old complexity becomes
% \[
% \frac{d_0^2}{\lam \varepsilon} + 1
% \]
% since
% \[
% \frac{\varepsilon}{\lam \eta^2} \le \frac{\varepsilon}{\lam (\varepsilon/d_0)^2} = \frac{d_0^2}{\lam \varepsilon}.
% \]
% % If $\lam \eta^2 \ge \varepsilon$, then our complexity is bounded by
% % \[
% % \frac{d_0^2}{\lam^2\eta^2} +1 \le \frac{d_0^2}{\lam \varepsilon} + 1
% % \]
% Hence, our complexity is as good as the old one under the condition that
% \[
% 1< \frac{d_0^2}{\lam^2 \eta^2} \le \frac{d_0^2}{\lam \varepsilon}
% \]
% or equivalently
% \[
% \frac{d_0}{\eta} > \lam  \ge \frac{\varepsilon}{\eta^2}
% \]
% \end{itemize}

% % We only consider 
% % \[
% % \lam \eta^2 \le \varepsilon
% % \]
% % if
% % \[
% % \varepsilon \ge \eta d_0
% % \]
% % old complexity becomes
% % \[
% % \frac{\varepsilon}{\lam \eta^2}
% % \]
% % Then our complexity
% % \[
% % \frac{d_0^2}{\lam^2\eta^2} \le \frac{\varepsilon^2}{\lam^2 \eta^4}
% % \]
% % if
% % \[
% % \varepsilon \le \eta d_0
% % \]
% % old complexity becomes
% % \[
% % \frac{d_0^2}{\lam \varepsilon}
% % \]
% % Then our complexity
% % \[
% % \frac{d_0^2}{\lam^2\eta^2} \le \frac{d_0^4}{\lam^2 \varepsilon^2}
% % \]


% % ---------

% % \[ \lam = \min \{ 1/m, 1\} \]
% % so that
% % \[
% % \lam m = \min\{ 1, m\}
% % \]


% % \[
% % 	{\cal O} \left( \left [ \frac{ M^2}{m \delta} +  \frac{L}m \log^+\left( \bar \sigma \right) + 1 \right] \left[\frac{ m  \left[ \hat M^\lam(\hat x_0) - \phi^* \right]}{ \bar \eta^2}+1 \right]\right) 
% % \]
% % \[
% % 	{\cal O} \left( \left [ \frac{ M^2}{m \delta} +  \frac{L}m \log^+\left( \bar \sigma \right) + 1 \right] \left[\frac{ \max(m,1)  \left[ \hat M^\lam(\hat x_0) - \phi^* \right]}{ \bar \eta^2}+1 \right]\right) 
% % \]

% % ---------
% ---------
%  \begin{equation}\label{eq:bound-adap}
% 		 \left[ 2\left( 1 + \frac{8\lam_{\mu} T_{\bar \varepsilon}^2}{\bar \varepsilon} \right) \log\left( \frac{4\bar t}{\bar \varepsilon}\right) + 1 \right] \left[  \frac{d_0^2}{\lam \bar \varepsilon}  + 1\right].
% 		\end{equation}
% --------

% %  -----------------RENATO---------------

% % \begin{lemma}\label{lem:keyrecur}
% % 		Assume that for some nonnegative scalars $a$ and $b$ such that $a+b>0$ and
% %   sequence of  positive scalars $\{q_l\}_{l \ge 0}$, we have that
% % 		\begin{equation}\label{ineq:recursive}
% % 		q_{l}\left(1 + \frac{q_{l}}{a + b q_{l}}\right) 
% %   % =  q_{l} + \frac{q_{l}^2}{a
% %   % + b q_{l}} 
% %   \le q_{l-1}, \ \ \forall l \ge 1.
% % 		\end{equation}
% % 		% \begin{itemize}\label{lem:waiting}
% % 		% 	\item[a)] for any $l > 0$, we have
% % 		% 	\begin{equation}
% % 		% 	\label{ineq:induction}	
% % 		% 	q_l   \le 
% % 		% 	%\frac{\max\{2c, t_l\}}{j-l}=
% % 		% 	\frac{2a +   2b t_{0} }{l};
% % 		% 	\end{equation}
% % 		% 	\item[b)] 
% % 		% 	an index $l$ such that
% % 		% 	$q_l \le 2a/b$ can be
% % 		% 	found in a number of iterations bounded by
% % 		% 	\[
% % 		% 	1+\lceil 4b \rceil \log\left( \frac{b t_0}{a}\right).
% % 		% 	\]
% % 		% \end{itemize}
% %  Then, for any given $\delta>0$,
% %  there exists an  index $\hat l \ge 1$ such that $q_{ \hat l} \le \delta$ and
% %   \begin{equation}\label{def:tl}
% %      \hat l \le (2b+3) \log ^+ \left(\frac{  q_0}{\delta + a/(b+1)}\right) + \frac{2a}{\delta} + b +2.
% %   \end{equation}
% %   \end{lemma}

%   % ------------------
% -----------------
% \[
% \hat M^\lam (x_0) - \phi^* \le \phi(x_0) - \phi^* 
% \]
% and
% \[
% \hat M^\lam (x_0) - \phi^* \le \frac{1}{2 } 
% \left( \frac1\lam + m \right) 
% \|x^* - \hat x_{0}\|^2 =
% \frac{1}{2 } 
% \left( \frac1\lam + m \right) d_0^2
% \]

% --------------\\
% ---------------------


% To do list
% ----------

% 1) do a new analysis with $\phi_{m}$ in (38) and (39) and with prox term multiplied by $1$



% 2) add prox terms and test the code with a graph of inner versus outer

% 3) adaptive criterion
% on $m$


% --------------

% \[
% \delta = \varepsilon = 10^{-4} gap_0 + \frac12 gap_k
% \]
% \[
% \lam=1/m
% \]
% How to choose $m$?

% % \beq \label{eq:def-tildem}
% %   \delta= \min\left\{\bar \varepsilon ,\frac{\lam \bar \eta^2}{8(m \lam +2)}\right\}, \qquad \Gamma_1(\cdot) = \ell_{ f}(\cdot;\hat x_{0}) + h(\cdot);
% %   % \qquad m = m+ \frac{1}\lam;
% %   \eeq
% % ????????????

%  \\  =======================
% \[
% \|\hat w_{k} \|^2 
%  \le \frac{4\delta}{\lam} +  2 \left[\left( m+\frac{1}{\lambda} \right)^2 + \frac{m\lam+1}{\lam^2} \right]\|\hat y_{k} - \hat y_{k-1}\|^2  -\frac{4 \hat\varepsilon
%  _k}{\lam}
% \]

%   ==========================\\
  

% \begin{equation}\label{ineq:check}
%     \hat \varepsilon_k + C\left[\delta + \left[\frac{(m\lam+1)^2}{\lam} + \frac{m\lam+1}{\lam} \right]\|\hat y_{k} - \hat y_{k-1}\|^2\right] \ge 0
% \end{equation}
% % \[
% %     \hat \varepsilon_k \le - C\left[\delta - \left[\frac{(m\lam+1)^2}{\lam} + \frac{m\lam+1}{\lam} \right]\|\hat y_{k} - \hat y_{k-1}\|^2\right] 
% % \]
%  {\bf Strategy for adaptively changing $(m,\lam)$}

% If \eqref{ineq:check} is not satisfied, then 
% \[
% m = 1.2m
% \]

%  $\lam$ is updated as follows:
%  \[
%  \lam = \lam/1.5 \quad \mbox{if $I> 300$}
%  \]
%  \[
%  \lam = 1.1 \lam \quad \mbox{if $I< 5$}
%  \]
%  otherwise $\lam$ the same

% % {\bf Lemma}
% %   For every $k \ge 1$, the quantities $\hat x_k$, $\hat y_k$, $\hat w_k$ and $\hat \varepsilon_k$,
% %  as in step 3b of PBF, satisfy
% %     \begin{equation}
% %     \label{eq:key_est}
% %         \hat \varepsilon_{k} +
% %        \frac{1}{2\lambda}\|\hat y_{k} - \hat x_{k}\|^2  \le \delta + \frac{1 + m\lam}{2\lam}\|
% %        \hat y_k - \hat y_{k-1}\|^2
% %     \end{equation}
% %     and
% %     \begin{equation}
% %     \|\hat w_{k} \|^2 
% %  \le \frac{4\delta}{\lam} +  2 \left[\left( m+\frac{1}{\lambda} \right)^2 + \frac{m\lam+1}{\lam^2} \right]\|\hat y_{k} - \hat y_{k-1}\|^2  
% %  \label{eq:est_w}
% %     \end{equation}
% %    where $\lam$ and $\delta$ are as in 
% %     step 0 of PBF.

% \begin{proof}
% Using both statements (a) and (b) of Lemma \ref{lem:basic_prop},
% the definitions of $\hat \varepsilon_{k}$ and $\hat v_k$ in \eqref{def:wk} and \eqref{def:vk}, respectively, we
% conclude that
% % \begin{equation}
% %         \label{eq:control}
% %      \hat f_k(\hat y_{k};\hat y_{k-1}) +\frac{1}{2\lam} \|\hat y_k-\hat y_{k-1}\|^2 
% %         - \hat \theta_{k} \le \delta.
% %         \end{equation}
% \begin{align*}
%     \hat \varepsilon_{k} &= \phi_m (\hat y_k;\hat y_{k-1})  - \hat \Gamma_{k}(\hat x_{k}) - \inner{\hat v_{k}}{\hat y_{k} - \hat x_{k}}  \\
%     & \stackrel{\eqref{eq:control}}\le \delta_k  + \frac{m }{2}\|\hat y_k-\hat y_{k-1}\|^2
%     + \hat \theta_k - \hat \Gamma_{k}(\hat x_{k}) - \inner{\hat v_{k}}{\hat y_{k} - \hat x_{k}} \\
%     &\stackrel{\eqref{eq:hattheta}}= \delta_k + \frac{m }{2}\|\hat y_k-\hat y_{k-1}\|^2
%     + \frac{1}{2\lambda}\|\hat x_{k}-\hat y_{k-1}\|^2 - \inner{\hat v_{k}}{\hat y_{k} - \hat x_{k}} \\
%     &\stackrel{\eqref{def:vk}}= \delta_k + \frac{m }{2}\|\hat y_k-\hat y_{k-1}\|^2 
%     + \frac{1}{2\lam} \left( \|\hat x_{k}-\hat y_{k-1}\|^2 + 2 \inner{\hat x_{k}-\hat y_{k-1}}{\hat y_{k} - \hat x_{k}} \right) \\
%     &= \delta_k  + \frac{m }{2}\|\hat y_k-\hat y_{k-1}\|^2
%     + \frac{1}{2\lam} \left(
%     \|\hat y_{k} - \hat y_{k-1}\|^2 -
%     \| \hat y_{k} - \hat x_{k}\|^2
%      \right),
%     \end{align*}
%     and hence that \eqref{eq:key_est} holds. 
% It follows from the definition of $\hat w_k$ in \eqref{def:wk} that
% \[
% \hat w_{k} = \hat v_{k} - m(\hat y_{k} - \hat y_{k-1}) = \frac{1}{\lambda}(\hat y_{k} - \hat x_{k}) - \left(m+\frac{1}{\lambda}\right)(\hat y_{k} - \hat y_{k-1})
% \]
% and hence that
% \begin{align*}
%     \|\hat w_{k} \|^2 &\le \frac{2}{\lam^2}\|\hat y_{k} - \hat x_{k}\|^2 + 2 \left( m+\frac{1}{\lambda} \right)^2\|\hat y_{k} - \hat y_{k-1}\|^2 \\&
%  \le \frac{4\delta_k}{\lam}  + 2 \left[\left( m+\frac{1}{\lambda} \right)^2 + \frac{m\lam+1}{\lam^2} \right]\|\hat y_{k} - \hat y_{k-1}\|^2
% \end{align*}
%  where the first inequality is due to the relation $\|a+b\|^2 \le 2 \|a\|^2 + 2 \|b\|^2$ and the last inequality is due to~\eqref{eq:key_est}. 
%  Assume now that
%  \[
%  \delta_k = \delta + C_1 \|w_k\|^2
%  \]
%  Then
%  \[
%  \left( 1 - \frac{4C_1}{\lam} \right) \|w_k\|^2 \le
%  \frac{4\delta}{\lam} + 2 \left[\left( m+\frac{1}{\lambda} \right)^2 + \frac{m\lam+1}{\lam^2} \right]\|\hat y_{k} - \hat y_{k-1}\|^2
%  \]
%  Take $C_1 = \lam/8$.
% \end{proof}
\end{document}

