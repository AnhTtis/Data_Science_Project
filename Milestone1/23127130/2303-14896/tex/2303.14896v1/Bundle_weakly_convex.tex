\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{latexsym}
\usepackage{tablefootnote}
\usepackage{algpseudocode}
\usepackage{enumerate}
\usepackage{algorithm}
\newcommand{\red}[1]{\textcolor{red}{#1}} 
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand*{\QEDA}{\hfill\hbox{\vrule width1.0ex height1.0ex}}
\newcommand*{\E}{\mathbb{E}}
\include{def}

\begin{document}
\title{Proximal bundle methods
	for hybrid\\ weakly convex composite optimization problems}
\date{March 26, 2023}
\author{
		Jiaming Liang \thanks{Department of Computer Science, Yale University, New Haven, CT 06511 (email: {\tt jiaming.liang@yale.edu}). }\qquad 
        Renato D.C. Monteiro \thanks{School of Industrial and Systems
		Engineering, Georgia Institute of
		Technology, Atlanta, GA, 30332-0205.
		(email: {\tt renato.monteiro@isye.gatech.edu} and {\tt hzhang906@gatech.edu}). This work
			was partially supported by AFORS Grant FA9550-22-1-0088.}\qquad 
	Honghao Zhang \footnotemark[2]
	}
	
\maketitle


\begin{abstract}
       This paper establishes the iteration-complexity of proximal bundle methods for solving
 hybrid (i.e., a blend of smooth and nonsmooth) weakly convex composite optimization (HWC-CO) problems.
 This is done in a unified manner by considering a proximal bundle framework (PBF) based on a generic bundle update scheme which includes various well-known bundle update schemes.
 In contrast to
 other well-known stationary conditions 
 % (e.g., the Moreau  stationary one)
in the context of
HWC-CO, PBF uses a new stationarity measure
% in the context of HWC-CO
which is easily verifiable and,
at the same time, implies any of the former ones.
 
 
       
       % a proximal bundle framework (PBF) for hybrid weakly convex composite optimization (HWC-CO) problem. We obtain the first global complexity result of proximal bundle method for HWC-CO. A major advantage of our analysis is that it bases on a newly defined convergence measure which is the first checkable notion of approximate  stationarity in the literature.
       
		{\bf Key words.} hybrid weakly convex composite optimization, iteration-complexity, proximal bundle method, regularized stationary point, Moreau envelope. 
		\\
		
		% 	{\bf Mathematics Subject Classification (2010)} 
		% 	49M37 $\cdot$ 65K05 $\cdot$ 68Q25 $\cdot$ 90C25 $\cdot$ 90C30 $\cdot$ 90C60
		{\bf AMS subject classifications.} 
		49M37, 65K05, 68Q25, 90C25, 90C30, 90C60
	\end{abstract}
	 
\section{Introduction}
	Let  $h: \R^{n} \rightarrow \R\cup \{ +\infty \} $ be a proper lower semi-continuous convex function
 and $ f: \R^{n} \rightarrow \R\cup \{ +\infty \} $ be a proper lower semi-continuous $m$-weakly convex function (i.e.,
 $f+(m/2)\|\cdot\|^2$
 is convex)
 such that
	$ \dom f \supseteq \dom h$  and
	consider
	the composite optimization (CO) problem
	\begin{equation}\label{eq:opt_problem}
	\min \left\{\phi(x):=f(x)+h(x): x \in \R^n\right\}.
	\end{equation}
	It is said that \eqref{eq:opt_problem} is a hybrid weakly convex CO (HWC-CO) problem if there exist
	nonnegative scalars $M$ and $L$
	and a
	first-order oracle $f':\dom h \to \R^n$
	(i.e., $f'(x)\in \partial f(x)$
	for every $x \in \dom h$)
	satisfying the $(M,L)$-hybrid condition,
	namely:
	$\|f'(u)-f'(v)\| \le 2M + L \|u-v\|$ for every $u,v \in \dom h$.
 This problem class includes the class of weakly convex
 non-smooth (resp., smooth) CO problems, i.e., the one with $L=0$
 (resp., $M=0$).

 This problem class appears in various applications in modern data science where $f$ is usually a loss function and $h$ is either the indicator function of some set (e.g., the set of points satisfying some functional constraints) or a regularization function that imposes sparsity or some special structure on the solution being sought.
Examples of such applications are robust phase retrieval, covariance matrix estimation, and sparse dictionary learning
(see Subsection 2.1 of \cite{davis2019stochastic} and the references therein).

 The main goal of this paper is to study the complexity of
  a unified framework (referred to as PBF) of  
 proximal bundle (PB) methods for solving the HWC-CO problem \eqref{eq:opt_problem}
 based on a generic bundle update scheme
 which contains different well-known
 update schemes
 as special cases.
More specifically,
like 
 other proximal bundle methods, a PBF iteration solves a prox bundle subproblem
	of the form
 \begin{equation}\label{eq:x-pre}
	    x = \underset{u\in \R^n}\argmin \left \{\Gamma (u) + \frac{1}{2\lam} \|u-x^c\|^2 \right\}
	\end{equation}
	where $\lam$ is the prox stepsize,
	$x^c$ is
 the current
	prox-center,
 and (usually simple) lower semi-continuous convex function $\Gamma$ denotes  the current bundle function.
	Moreover,
 the bundle function is updated in every iteration but the prox center $x^c$ is updated in some
 (i.e., serious)
 iterations and left the same
 in the other (i.e., null) ones.
 But instead of choosing the bundle function underneath $\phi$ as in the proximal bundle  methods for solving the convex version of \eqref{eq:opt_problem},
 PBF uses the idea of choosing $\Gamma$ underneath $\phi(\cdot)+(m/2)\|\cdot-x^c\|^2$.
 An interesting
 feature of
 PBF is that it terminates based on
 an easily verifiable stopping criterion which is
 novel in the context of HWC-CO.

 % One of the difficulties for weakly convex nonsmooth optimization is a continuous measure to monitor the  progress for the algorithm. In order to address this difficulty, \cite{davis2019stochastic} proposes to use the Moreau envelope as the desired measure (see Section 1 in \cite{davis2019stochastic} and Subsection \ref{sec:notion} below). However, one drawback of the Moreau envelope is that it is not checkable in general because the computation of the proximal point is very expensive. Therefore, any algorithm based on the Moreau envelope as the convergence measure can not be stopped efficiently. More details are discussed in Subsection \ref{SEC:SCS}. A natural open question is that whether there is a checkable convergence measure for weakly convex nonsmooth optimization problem?
 
 % In this paper, we answer this question with a new notion of approximate point and obtain the first global complexity result based on this new convergence measure. 

 
 {\it Literature Review}
 % Proximal bundle methods are efficient algorithms for solving convex nonsmooth optimization problems in practice.
 This paragraph discusses 
 the development of
 proximal bundle methods
 in the context of
 convex CO problems.
They have been proposed  in  \cite{lemarechal1975extension,wolfe1975method}, and then further studied for example in \cite{frangioni2002generalized,mifflin1982modification,de2014convex}. Their convergence analyses for nonsmooth (i.e., $L=0$ and $M>0$) CO problems are broadly discussed for example in the textbooks \cite{ruszczynski2011nonlinear,urruty1996convex}. Moreover, their complexity
analyses are studied
for example in \cite{diaz2021optimal,du2017rate,kiwiel2000efficiency,liang2021proximal,liang2021unified}.

We now discuss the development of PB methods
in the context of weakly convex CO problems as mentioned at the beginning of this introduction but with $L=0$.
We start with papers
that deal only with their asymptotic convergence.
PB extensions to this context can be found in
\cite{fuduli2004minimizing,hare2009computing,hare2010redistributed,hare2016proximal,kiwiel2006methods,makela1992nonsmooth,noll2008proximity,vlvcek2001globally}.
In particular, papers
\cite{hare2009computing,hare2010redistributed,hare2016proximal}
already considered the idea of constructing 
convex bundle
(more precisely, cutting-plane) models
underneath the regularized function $\phi(\cdot)+(m/2)\|\cdot-x^c\|^2$,
which, as already mentioned above, is the one adopted in this paper. 
% ???\textbf{difference}
% a multicut cutting 
% propose 
% To extend the bundle method for weakly convex optimization problem, one needs to tackle the difficulty that the linearization of a weakly convex function $\phi$ is not necessarily underneath $\phi$ itself. \cite{kiwiel2006methods,vlvcek2001globally,makela1992nonsmooth,fuduli2004minimizing,noll2008proximity} addressed this issue by downshifting the linearization error and \cite{hare2010redistributed,hare2016proximal,hare2009computing} handle weak convexity using redistributed models which not only downshift the linearization error but also tilt the slopes. This paper follows a similar idea to the first kind but differs in the way the strategy for choosing a serious index.
% All the papers about bundle methods for weakly convex nonsmooth optimization problems mentioned above do not contain the analysis for the iteration complexity. 
The more recent paper \cite{atenas2023unified} proposes a model to analyze descent-type bundle methods and
establishes local convergence rate for the (serious) iteration sequence  as well as function value sequence under some strong stationary growth condition.
None of the aforementioned papers establish (either serious or overall) iteration complexities for their
methods.

 % However, it neglects the complexity for null indices and assumes the error bound condition. In this paper, we will take the null indices into consideration and establish the first global complexity result without the error bound condition.

Iteration-complexities for
stochastic proximal point and proximal subgradient methods
are obtained in
\cite{davis2019stochastic}
using the Moreau envelope as
an optimality measure.
Moreover, \cite{lin2020near,2019Nouiehed_Lee,ostrovskii2021efficient,rafique2022weakly,thekumparampil2019efficient} establish iteration-complexities for different types of algorithms for solving the special case where
$f(x)= \max_{y \in Y} \Phi(x,y)$,
function $x \to \Phi(x,y)$ is
weakly convex and differentiable
for every $y \in Y$,
and $y \to \Phi(x,y)$
is concave for every
$x$.
% is the maximum of a family of
% weakly convex differentiable functions
% $\{\Phi(\cdot,y)\}_y$
% such that $y \to \Phi(\cdot,y)$ is concave.


% Other methods for weakly convex nonsmooth optimization are \cite{davis2019stochastic} ??????


{\it Main contribution.}
As already mentioned above, this paper establishes for the first time the (serious and overall) iteration-complexity of PB methods for solving the HWC-CO problem described at the beginning of this introduction. This is done in a unified manner by considering a generic bundle update scheme.
An interesting feature of this analysis is the introduction of a new stationarity measure
% in the context of HWC-CO
which, in contrast to other well-known stationary conditions
in the context of
HWC-CO (including the Moreau stationary one), is easily verifiable. Moreover, by proper choice of tolerances, it is shown that the new measure
implies these
other
well-known near stationarity conditions (including the Moreau stationary one), so that any complexity result
based on the first one can be easily translated to the latter ones.

As a consequence of our analysis,
it is shown that the iteration-complexity for PBF to find a $\delta$-Moreau stationary point
of $\phi$ is similar to that of the deterministic version of
the stochastic proximal subgradient (PS) method studied 
in \cite{davis2019stochastic},
i.e., ${\cal O}(\delta^{-4})$,
but the first complexity bound has the major advantage that its constant (in its ${\cal O}(\cdot)$) is never worse and is generally much better than the one which appears in the bound for the PS method
of \cite{davis2019stochastic}.
The latter feature of PBF is due to its
bundle nature which allows it to
use a considerably  larger prox stepsize that is determined by
the weakly convex parameter.
This contrasts with the nature of proximal subgradient-type methods which
use relatively small prox stepsizes
(e.g., depending on a pre-specified iteration count).

% naturally minimized is better than the latter in terms of the
% constant
% can be found within , i.e.,
% a point $x$ such that
% $\can be obtained
% in ${\cal O}(\delta^{-4})$

% of 
% More specifically, it is shown
% that every iteration of a
% PBF variant generates a pair $(x,w)$ of vectors and a scalar $\varepsilon$ such that
% $w \in \partial_{\varepsilon}(\cdot;x^c)(x)$ and the stationary 
% It is shown in this paper that
% any instance of PBF finds a

% \begin{itemize}
%     \item We propose a new notion of stationary that is checkable and can used for other algorithms as a convergence measure for weakly convex nonsmooth optimization problem.
%     \item Based on the new convergence measure, we obtain the first global complexity result of proximal bundle method for HWC-CO which includes weakly convex nonsmooth problem as a special case under a very weak condition. We also show that our complexity for PBF is better than the  complexity for stochastic composite subgradient method proposed in \cite{davis2019stochastic}. 
% \end{itemize}


{\it Organization of the paper.}
Subsection~\ref{subsec:DefNot}  presents basic definitions and notation used throughout the paper.
Section~\ref{Sec:background} contains two subsections. Subsection~\ref{sec:cha} provides a characterization of the subdifferential for a weakly convex function. Subsection~\ref{sec:notion} introduces three different notions of approximate stationary points and discusses their relationship. 
Section~\ref{Sec:PBF} contains four subsections. Subsection~\ref{sec:assumptions} formally
describes the problem \eqref{eq:opt_problem} and the assumptions made on it. Subsection~\ref{SEC:SCS}
reviews
the deterministic version of the stochastic composite subgradient method of \cite{davis2019stochastic}. Subsection~\ref{sec:GBUS} presents a generic bundle update scheme. Subsection~\ref{subsec:update} describes the PBF framework and states the iteration-complexity result of PBF.
Section~\ref{sec:proof} contains three subsections. Subsection \ref{subsec:length} provides a preliminary bound on the length of a cycle in PBF and Subsection \ref{subsec:cycle} bounds the number of cycles generated by PBF. Subsection \ref{subsec:proof} presents the proof of the main complexity result.
Section~\ref{sec:conclusion} gives some concluding remarks and potential directions for future research.
Appendix~\ref{APP:sub} describes two useful technical results about subdifferentials.
Appendix~\ref{App:relation} provides proofs for the results in Subsection~\ref{sec:notion}.
Finally, Appendix~\ref{APP:analysis} presents some useful technical results used in Section~\ref{sec:proof}.


% =====================================================\\
% For any function $\phi: \mathbb{R}^d \rightarrow \mathbb{R} \cup\{\infty\}$ and $\mu>0$, the Moreau envelope and the proximal map are defined as
% \begin{equation}\label{def:Mor}
%     M_\phi^{\mu} (x) = \min_u {\phi(u) + \frac{1}{2\mu}\|u-x\|^2}.
% \end{equation}
% \[
% \hat M^{\lam}(x) :=
% % M_\phi^{(\lam^{-1}+m)^{-1}} (x)
% % = 
% \min_u \left\{ \phi(u) + \frac{\lam^{-1}+ m}{2 }\|u-x\|^2 \right\}.
% \]
% \[
% p^\lam_\phi(x) = \argmin_u \left\{ \phi(u) + \frac{\lam^{-1}+ m}{2 }\|u-x\|^2 \right\}.
% \]

% \[
% \mu= \frac{\lam}{1+\lam m}
% \]
% \begin{equation}
%     \operatorname{prox}_{\mu \phi}(x):=\underset{x}
%     {\operatorname{argmin}}\left\{\phi(u)+\frac{1}{2 \mu}\|u-x\|^2\right\}
% \end{equation}
% Assume $\phi$ is $m$-weakly convex, then for $\mu < m^{-1}$,  $\phi_{2m}$ is continuously
% differentiable with gradient
% given by
% \begin{equation}\label{eq:grad_M}
% \nabla M_\phi^{\mu}(x)=\mu^{-1}\left(x-\operatorname{prox}_{\mu \phi}(x)\right).
% \end{equation}
% Denote $ \hat{x}:=\operatorname{prox}_{\mu \phi}(x)$. The following relationship is presented in \cite{davis2019stochastic} which captures the importance of Moreau envelope:
% \begin{equation}\label{eq:keyrela}
% \left\{\begin{array}{cl}
% \|\hat{x}-x\| & =\mu \left\|\nabla M_\phi^{\mu}(x)\right\|, \\
% \phi(\hat{x}) & \leq \phi(x), \\
% \operatorname{dist}(0 ; \partial \phi(\hat{x})) & \leq\left\|\nabla M_\phi^{\mu}(x)\right\| .
% \end{array}\right.
% \end{equation}
%  Davis then claim to use the $\|\nabla M_\phi^{\mu}(x)\|$ as the convergence measure for weakly convex nonsmooth problem. The next result shows how we can translate our convergence measure in the language of Moreau envelope.

%  ------------------------------------
% 	For the sake of shortness, we denote the class of functions $\Gamma^+$ satisfying
% 	the above conditions as  ${\cal C}_\phi(\Gamma,x^c,\lambda,\tau)$.
% 	Clearly, the above update scheme does not completely determine $\Gamma^+$
% 	but rather gives minimal conditions on it which are suitable for the complexity analysis
% 	of this paper.
   

    \subsection{Basic definitions and notation} \label{subsec:DefNot}

    The sets of real numbers and positive real numbers are denoted by $\R$ and $\R_{++}$, respectively. 
    % Let $\R$ denote the set of real numbers.
    % Let $ \R_+ $ and $ \R_{++} $ denote the set of non-negative real numbers and the set of positive real numbers, respectively.
	Let $\R^n$ denote the standard $n$-dimensional Euclidean space equipped with  inner product and norm denoted by $\left\langle \cdot,\cdot\right\rangle $
	and $\|\cdot\|$, respectively. 
% 	Given a set $ S\subset \R^n $, its linear (resp., convex) hull is
% 	denoted by $\text{Lin} \, S $ (resp., ${\rm conv} \, S$).
	Let $\log(\cdot)$ denote the natural logarithm and $\log^+(\cdot)$ denote $\max\{\log(\cdot),0\}$. Let ${\cal O}$ denote the standard big-O notation and $\tilde {\cal O}_1(\cdot)$ denote ${\cal O}(\cdot +1)$ with the convention that a logarithm factor is neglected.
	
	
	For a given function $\varphi: \R^n\rightarrow (-\infty,+\infty]$, let $\dom \varphi:=\{x \in \R^n: \varphi (x) <\infty\}$ denote the effective domain of $\varphi$ 
	and $\varphi$ is proper if $\dom \varphi \ne \emptyset$.
	A proper function $\varphi: \R^n\rightarrow (-\infty,+\infty]$ is $\mu$-convex for some $\mu \ge 0$ if
	$$
	\varphi(\alpha x+(1-\alpha) y)\leq \alpha \varphi(x)+(1-\alpha)\varphi(y) - \frac{\alpha(1-\alpha) \mu}{2}\|x-y\|^2
	$$
	for every $x, y \in \dom \varphi$ and $\alpha \in [0,1]$.
 Denote the set of all proper lower semicontinuous convex functions by $\bConv{n}$.
 % When $\mu=0$, we simply denote
 %    $\mConv{n}$ by $\bConv{n}$.
	For $\varepsilon \ge 0$, the \emph{$\varepsilon$-subdifferential} of $ \varphi $ at $x \in \dom \varphi$ is denoted by
	\beq \label{def:subdif}
 \partial_\varepsilon \varphi (x):=\left\{ s \in\R^n: \varphi(y)\geq \varphi(x)+\left\langle s,y-x\right\rangle -\varepsilon, \forall y\in\R^n\right\}.
        \eeq
	For simplicity, the subdifferential of $\varphi$ at $x \in \dom \varphi$, i.e., $\partial_0 \varphi(x)$, is denoted by $\partial \varphi (x)$.
        
          

    \section{Basic definitions and background}
    \label{Sec:background}

    This section contains two
    subsections.
    The first one gives the definition of the
subdifferential and the directional derivative of a general
closed function. It also 
relates these two concepts and
provides a characterization of
the first one for the case of
a closed $m$-weakly convex function.
The second subsection introduces the notion of a regularized stationary point which is sought by the main algorithm of this paper. It also introduces two different stationary conditions
% to another notion of approximate
% stationary point 
in terms of the directional derivative and the Moreau envelope, respectively, and provides the relationship between these three different notions.






\subsection{Characterization of the subdifferential  of weakly convex function}
\label{sec:cha}

% This section discusses the definition of subdifferential for weakly convex function and its various properties.

We start by giving the definitions of
directional derivative
of a closed  function.

\begin{definition}
The directional derivative $\phi'(x;d)$
of
$\phi$ at $x$ along $d$ is
\[
\phi'(x;d) := \liminf _{t \downarrow 0} \frac{\phi(x+td)-\phi(x)}{t}.
\]
\end{definition}

% We start by giving the definition of
% the
% $\varepsilon$-subdifferential
% of a proper closed function.

% In \cite{kruger2003frechet}, the Frechet $\varepsilon$-subdifferential is defined in the following way:
The next definition for $\varepsilon$-subdifferential can be found 
in  Definition 1.10 of \cite{kruger2003frechet}.

\begin{definition}
% [Frechet $\varepsilon$-subdifferential]
The Frechet subdifferential of a proper closed function
$\phi: \mathbb{R}^{n} \rightarrow \mathbb{R} \cup\{\infty\}$ is defined as 
\[
 \partial \phi(x)=\left\{v \in \R^n: \liminf _{y \rightarrow x} \frac{\phi(y)-\phi(x)-\left\langle v, y-x\right\rangle}{\|y-x\|} \geq 0 
\quad \forall x \in \R^n \right\}.
\]
\label{def:Fre_epsilon_subdifferential}
% When $\varepsilon=0$, we denote
% $\partial_{0} \phi(x)$ simply by $\partial \phi(x)$.
\end{definition}



% The above definition 
% of $\partial_\varepsilon \phi(x)$
% % of the Frechet subdifferential $\partial h(x)$
% can be found for example in Definition 1.10 in \cite{kruger2003frechet}
% and
% % In particular, the definition for  $\partial \phi(\cdot)$ is given 
% in section 2.2 of Davis \cite{davis2019stochastic}.
% , namely:
% $$
% \partial \phi(x)=\left\{v \in \mathbb{R}^{n} :  \phi(y) \geq \phi(x)+\langle v, y-x\rangle+o(\|y-x\|), \, \forall y \in \R^n \right\}.
% $$

% \begin{lemma} {\bf Move}
% \label{def: Frechet Subdifferential}
%  Let $h: \mathbb{R}^{n} \rightarrow \mathbb{R} \cup\{\infty\}$ be a proper closed function. At any point $x \in \operatorname{dom} \phi$, we have
% $$
% \partial h(x)=\left\{v \in \mathbb{R}^{n} \mid\left( \forall y \in \mathbb{R}^{n}\right) h(y) \geq \phi(x)+\langle v, y-x\rangle+o(\|y-x\|)\right\}.
% $$
% \end{lemma}

% \begin{proof}
% On one hand, if $v \in \partial \phi(x)$, then $\forall y \in \R^n$
% \[
%   h(y) \ge \phi(x) + \inner{v}{y-x} + \min\{0,\phi(y)-\phi(x)-\inner{v} { y-x}\}
% \]
% Then the fact that $\liminf_{y \rightarrow x} \frac{\phi(y)-\phi(x)-\left\langle v, y-x\right\rangle}{\|y-x\|} \geq 0$ suggests \[\min\{0,\phi(y)-\phi(x)-\inner{v} { y-x}\} = o(\|y-x\|)\]
% We then conclude
% \[
% \partial \phi(x) \subset \left\{v \in \mathbb{R}^{n} \mid\left( \forall y \in \mathbb{R}^{n}\right) \phi(y) \geq \phi(x)+\langle v, y-x\rangle+o(\|y-x\|)\right\}.
% \]
% On the other hand, if $v \in \left\{v \in \mathbb{R}^{n} \mid\left( \forall y \in \mathbb{R}^{n}\right) \phi(y) \geq \phi(x)+\langle v, y-x\rangle+o(\|y-x\|)\right\}$, then 
% \[
% \liminf _{y \rightarrow x} \frac{\phi(y)-\phi(x)-\left\langle v, y-x\right\rangle}{\|y-x\|} \geq \liminf _{y \rightarrow x}\frac{o(\|y-x\|)}{\|y-x\|} = 0
% \]
% Thus the statement follows.
% \end{proof}

% --------------------


% \[
% 0 \in \partial \phi(x)
% \]
% \[
% \inf_{\|d\| \le 1}
% \phi'(x;d) \ge 0
% \]
% \[
% \hat M^{\lam}(x) =0
% \]


% ----------------------


The following result gives a relationship between
the above two concepts.
\begin{lemma}
\label{lem:directional_deri}
 For any $v \in  \partial \phi(x)$ we have
\begin{equation}
    \operatorname{inf}_{\|d\| \le 1} \phi'(x;d) \ge -\|v\| .
    \label{eq:dire_subdif}
\end{equation}

\end{lemma}
\begin{proof}
To prove \eqref{eq:dire_subdif}, it suffices to show
if $v  \in  \partial \phi(x)$, then
$\phi'(x;d) \ge -\|v\|$ for every $d \in \R^n$ such that $\|d\|\le 1$.
Indeed, we may assume that $d \ne 0$
since the case where $d=0$ is trivial.
Then, it follows from the Definition \ref{def:Fre_epsilon_subdifferential} and the  Cauchy-Schwarz inequality that
\begin{align*}
   0 &\le 
   \liminf _{t \downarrow 0} \frac{\phi(x+td)-\phi(x)- t \left\langle v, d\right\rangle}{t\|d\|} =
   - \frac{\inner{v}{d}}{\|d\|} + \liminf _{t \downarrow 0} \frac{\phi(x+td)-\phi(x)}{t\|d\|} \\ 
   &= - \frac{\inner{v}{d}}{\|d\|} + \frac{1}{\|d\|} \phi'(x;d) \le \|v\| + \frac{1}{\|d\|} \phi'(x;d),
\end{align*}
and hence that 
$\phi'(x;d) \ge -\|v\|$ for every $d \in \R^n$ such that $\|d\|\le 1$.
\end{proof}
% \begin{lemma}
% \label{lem:directional_deri}
%  For any $v \in  \partial_\varepsilon \phi(x)$ we have
% \begin{equation}
%     \operatorname{inf}_{\|d\| \le 1} \phi'(x;d) \ge -\|v\| - \varepsilon.
%     \label{eq:dire_subdif}
% \end{equation}

% \end{lemma}
% \begin{proof}
% To prove \eqref{eq:dire_subdif}, it suffices to show
% if $v  \in  \partial_\varepsilon \phi(x)$, then
% $\phi'(x;d) \ge -\|v\|- \varepsilon$ for every $d \in \R^n$ such that $\|d\|\le 1$.
% Indeed, we may assume that $d \ne 0$
% since the case where $d=0$ is trivial.
% Then, it follows from the Definition \ref{def:Fre_epsilon_subdifferential} and the  Cauchy-Schwarz inequality that
% \begin{align*}
%    -\varepsilon &\le 
%    \liminf _{t \downarrow 0} \frac{\phi(x+td)-\phi(x)- t \left\langle v, d\right\rangle}{t\|d\|} =
%    - \frac{\inner{v}{d}}{\|d\|} + \liminf _{t \downarrow 0} \frac{\phi(x+td)-\phi(x)}{t\|d\|} \\ 
%    &= - \frac{\inner{v}{d}}{\|d\|} + \frac{1}{\|d\|} \phi'(x;d) \le \|v\| + \frac{1}{\|d\|} \phi'(x;d),
% \end{align*}
% and hence that 
% $\phi'(x;d) \ge -\|v\|- \varepsilon$ for every $d \in \R^n$ such that $\|d\|\le 1$.
% \end{proof}



Before stating the next result, we introduce the  following notation which is used not only here but also throughout the paper:
    for every $m \in \R^+$, $z \in \R^n$,
    let 
    \begin{equation}
   \phi_m(\cdot;z) := \phi (\cdot) + \frac{m}{2}\|\cdot - z\|^2.
    \label{def:phim}
    \end{equation}
The following result  provides a characterization of the
Frechet subdifferential 
for a weakly convex function. 


\begin{proposition}\label{prop:Fre_sub}
\label{prop:Frechet relation}
Assume that $\phi: \mathbb{R}^{
n} \rightarrow \mathbb{R} \cup\{\infty\}$ is a closed $m$-weakly convex function. Then  we have
 \begin{align}
    \partial \phi(x) 
 &=\left\{ v \in \R^n :  \phi(y) \geq \phi(x)+\langle v, y-x\rangle-\frac{ m}{2}\|y-x\|^{2}, \ \forall y \in \R^n \right\} \label{eq:key_chara} \\
 &=
 \partial \left[\phi_{ m}(\cdot;x)
 \right](x)  \label{eq:key_chara1}.
 \end{align}
\end{proposition}
\begin{proof}
The proof of \eqref{eq:key_chara} can be found for example in Lemma 2.1 of \cite{davis2019stochastic}.
Moreover, it follows from the definition of the subdifferential in \eqref{def:subdif} with $\varepsilon = 0$ and the definition of $\phi_m(\cdot;x)$ in \eqref{def:phim} that \eqref{eq:key_chara1} is equivalent to \eqref{eq:key_chara}.
\end{proof}


\subsection{Notions of approximate stationary points}
\label{sec:notion}
% In this section, we define formally our new definition of approximate solution.

This subsection introduces three notions of stationary points, including the one adopted by the main algorithm of this paper.
It also describes how these notions are related to one another.

We start by introducing the
definition of a regularized
stationary point of a
closed $m$-weakly convex function
$\phi$ which is the one
used by the main algorithm of this paper.
% This notion of regularized
% approximate stationary point generalizes the prox gradient mapping for analyzing the proximal method in a significant way so that it can be applied to analyze any algorithm for weakly convex nonsmooth problem.

% Assume for this subsection that
% $\phi$ is a
% closed $m$-weakly convex function.

\begin{definition}[Regularized stationary point]
\label{def:appr}
For a pair $(\bar\eta,\bar \varepsilon) \in \R^2_{++} $, a point $x\in \dom \phi$ is called a $(\bar \eta,\bar\varepsilon;m)$-regularized stationary point  of $\phi$
% problem \eqref{eq:opt_problem} 
if there exists a pair $(w,\varepsilon) \in \R^n \times \R_{++}$ such that 
\begin{equation}
  w \in  \partial_\varepsilon \left[\phi_{m}(\cdot;x) \right]  (x), \quad
\|w\| \le \bar \eta , \quad \ \varepsilon \le \bar \varepsilon.
\label{eq:app_sol}
\end{equation}
\end{definition}

We now make two trivial remarks about the above definition. First,
if $(\bar \eta,\bar \varepsilon)=(0,0)$ then
it follows from 
\eqref{eq:key_chara1}
and Definition \ref{def:appr}
that
$x$ is a  $(\bar \eta,\bar \varepsilon)$-regularized stationary point if and only if
$x$ is an exact stationary point of \eqref{eq:opt_problem}, i.e.,
it satisfies $0 \in \partial \phi(x)$.
Second, when $m=0$ (and hence $\phi$ is convex),
the inclusion in
\eqref{eq:app_sol} reduces to $w \in \partial_{ \varepsilon} \phi(x)$, and the above notion
reduces to a familiar one which has already been used in the analysis of several
algorithms, including proximal bundle ones (e.g., see Section 6 of \cite{liang2021proximal}), for solving the convex version of \eqref{eq:opt_problem}.

The verification that $x$ is a $(\bar \eta,\bar\varepsilon;m)$-regularized stationary point
 requires exhibiting a pair of residuals
 $(w,\varepsilon)$ satisfying the inclusion in \eqref{eq:app_sol},
 which is generally not an immediate task. However, many algorithms for solving the convex version of \eqref{eq:opt_problem} and the one in this paper, are able to generate not only a sequence of iterates $\{x_k\}$ but also a sequence of corresponding residuals
 $\{(w_k,\varepsilon_k)\}$
 such that $(x,w,\varepsilon)=(x_k,w_k,\varepsilon_k)$
 satisfies the inclusion in \eqref{eq:app_sol},
 so that verification that
 $x_k$ is a $(\bar \eta,\bar\varepsilon;m)$-regularized stationary point simply amounts to checking the two inequalities in \eqref{eq:app_sol}.

The following definition uses
directional derivatives
to define a different notion
of a stationary point.

\begin{definition}[Directional stationary point] \label{def:dsp}
    For a pair $(\varepsilon_D,\delta_D) \in \R^2_{++}$, a point $x\in \dom \phi$ is called a $(\varepsilon_D,\delta_D)$-directional stationary point if there exists $\tilde x \in \dom \phi$ such that 
    \begin{equation}\label{eq:dir}
    \|x- \tilde x\|\le \delta_D, \quad \inf_{\|d\|\le 1} \phi'(\tilde x;d) \ge -\varepsilon_D.
    \end{equation}
\end{definition}

When $(\varepsilon_D,\delta_D)=(0,0)$, then \eqref{eq:dir}
reduces to the condition that $\phi'(x;d) \ge 0$ for all $d \in \R^n$, a condition which is known to be equivalent
to $0 \in \partial \phi(x)$.
% Note that the following equality holds according to Proposition 8.32 in \cite{rockafellar2009variational}:
% \begin{equation}\label{eq:disdire}
% \operatorname{dist}(0 ; \partial \phi(\tilde x))=-\inf _{\|d\| \leq 1} \phi^{\prime}(\tilde x ; d).
% \end{equation}
% Thus a  $(\varepsilon_D,\delta_D)$-directional stationary point $\tilde x$ also satisifies
% \[
% \operatorname{dist}(0 ; \partial \phi(\tilde x)) \le \varepsilon_D.
% \]

Before stating the next notion of a stationary point based on the Moreau envelope, we introduce a slightly different
notation for the Moreau envelope which is more suitable for our presentation,
namely:
for any $\lam>0$ and $x \in \R^n$, let
\begin{equation}\label{eq:Moreau}
  \hat M^{\lam}(x) :=
\min_u \left\{ \phi(u) + \frac{\lam^{-1}+ m}{2 }\|u-x\|^2 \right\}.   
\end{equation}
Note that the above definition depends on $m$
 but, for simplicity, we have omitted this dependence from
 its notation
 since the parameter $m$ is assumed constant throughout our analysis in this paper.
 
 The  gradient formula for the Moreau envelope (see Section 1 in \cite{davis2019stochastic}) is as follows:
\begin{equation}\label{eq:new_grad}
     \nabla  \hat M^{\lam}(x) = \left(\frac{1}{\lam}+m\right) \left(x - \hat x^\lam(x) \right)
\end{equation}
where 
\begin{equation}\label{eq:hatx}
    \hat x^\lam(x) := \argmin_{u \in \R^n} \left\{\phi(u)+ \frac{\lam^{-1}+m}2\|u -  x\|^2\right\}.
\end{equation}

The following definition describes another notion of a stationary point that is based on the aforementioned Moreau envelope.
% The Moreau stationary point based on the above gradient formula for Moreau envelope is stated in the following definition:
\begin{definition}[Moreau stationary point]
    For any $\varepsilon_M >0$ and $\lam>0$, a point $x\in \dom \phi$ is called a $(\varepsilon_M;\lam)$-Moreau stationary point if 
    $\|\nabla \hat M^{\lam}(x)\| \le \varepsilon_M$.
\label{def:Moreau}
\end{definition}

We now make some remarks about the above three notions of stationary points.
 First, among the three notions of stationary points,
 the directional  stationary one is the only one which does not depend on the weak convexity parameter $m$. Second, among the above three stationary notions, only the directional one
 imposes a condition on
 a nearby point instead of the actual point under consideration.
 
 % imposes a condition on the actual stationary point instead of a condition on nearby points which does not require the computation of a nearby point which is generally not computable. 
 % In other words, if one can find $(w,\varepsilon)$ satisfying the inclusion in \eqref{eq:app_sol}, then the regularized stationary point becomes checkable  in the sense that the norm of $w$ and $\varepsilon$ are easy to compute. Later we will show that the termination of our algorithm actually based on this observation.

The next result,
whose proof is given in Appendix \ref{App:relation}, describes the relationship between directional stationary points and Moreau stationary points.

% The relationship of these three notions of stationary point is summarized in the following two results and the  proofs of these two results are
% in Appendix \ref{App:relation}.


\begin{proposition}
Assume $\lam >0$ and $\phi$ is a $m$-weakly convex function. Then, the following statements hold:
\begin{itemize}
    \item [a)]
    if $x$ is a $(\varepsilon_D,\delta_D)$-directional stationary point then $x$ is a $\left(\varepsilon_M;\lam \right)$-Moreau stationary point where
    \[
    \varepsilon_M = \left(m+\frac{1}{\lam}\right)\left[(3+2\lam m)\delta_D + 2\lam \varepsilon_D \right];
    \]
    \item [b)]
    if $x$ is a $(\varepsilon_M;\lam)$-Moreau stationary point then $x$ is a $(\varepsilon_M,\varepsilon_M/(m+\lam^{-1}))$-directional stationary point.
\end{itemize}
\label{prop:relation}
\end{proposition}

The following result, whose  proof is given in Appendix \ref{App:relation},
provides an equivalent characterization of
a directional stationary   point
in terms of
the subdifferential of $\phi$.

\begin{proposition}\label{prop:subdir}
Assume that $\phi$ is a $m$-weakly convex function. Then,
        $x$ is a $(\varepsilon_D,\delta_D)$-directional stationary point if and only if  there exists $\tilde x \in \dom \phi$ such that 
        \[
  \|x- \tilde x\|\le \delta_D, \quad \operatorname{dist}(0 ; \partial \phi(\tilde x)) \le \varepsilon_D.
\]
\end{proposition}

The following result, whose  proof is given in Appendix \ref{App:relation},
shows that a
regularized stationary point is
both a 
directional stationary point and a Moreau stationary point.


\begin{proposition}\label{key:rela}
If $x$ is a $(\bar \eta,\bar \varepsilon;m)$-regularized stationary point, then the following statements hold:
\begin{itemize}\label{prop:ours}
    \item[a)] $x$ is a $(\bar \eta + 2 \sqrt{2 m \bar \varepsilon }, \sqrt{2\bar \varepsilon/m})$-directional stationary point;
    \item[b)]
    $x$ is a $(18\sqrt{2m\bar \varepsilon}+4\bar \eta;1/m)$-Moreau stationary point.
\end{itemize}
    % \begin{itemize}
    %     \item[a)] 
    %     $y$ is a $(\bar \eta + 2 \sqrt{2 m \bar \varepsilon }, \sqrt{2\bar \varepsilon/m})$-directional stationary point;     
    %     \item[b)] $y$ is a $(2\sqrt{4m\bar \varepsilon+2\bar \eta^2};1/2m)$-Moreau stationary point.
    % \end{itemize}
\end{proposition}




% b) Since $x$ is a $(\bar \eta, \bar \varepsilon;m)-$ stationary point of $\phi$, there exists a pair $(w,\varepsilon) \in \R^n \times \R_{++}$
% \begin{equation}\label{eq:normw}
%   w \in  \partial_\varepsilon \left[\phi_m(\cdot;y) \right]  (y), \quad
% \|w\| \le \bar \eta , \quad \ \varepsilon \le \bar \varepsilon.
% \end{equation}
%     Denote $\hat y = \operatorname{prox}_{(1/2m)\phi}(y)$ and $\tilde y = \operatorname{prox}_{(1/2m)(\phi - \inner{w}{\cdot} )}(y)$.
% Since  $\phi_{ m}$ is
% convex in view of assumption (A1),
% we easily see that
% $\phi_{2m}$ is
% $m$-strongly convex. Then we have
% \begin{equation}
%     \phi(\tilde y) + m\|\tilde y - y\|^2 \ge \phi(\hat y) + m\|\hat y - y\|^2+ \frac{m}{2}\| \hat y- \tilde y\|^2.
% \end{equation}
% Using the fact that $\phi_{2m}-\inner{w}{\cdot}$ is also
% $m$-strongly convex, we have 
% \begin{equation}
% \phi(\hat y) - \inner{w}{\hat y} + m\|\hat y - y\|^2 \ge \phi(\tilde y) - \inner{w}{\tilde y}+ m\|\tilde y - y\|^2+ \frac{m}{2}\| \hat y- \tilde y\|^2.    
% \end{equation}
% Summing the above two inequalities and using Cauchy-Schwarz inequality we can get
% \begin{equation}
%     m \|\hat y - \tilde y\|^2 \le \inner{w}{\tilde y - \hat y} \le \|w\|\|\hat y - \tilde y\|.
% \end{equation}
% The above inequality,  \eqref{eq:grad_M} with $\mu = 1/2m$, and the inequality in \eqref{eq:nearby} then imply that
% \begin{align*}
%     \|\nabla M_\phi^{1/2m}(y)\|^2 = 4m^2\|y - \hat y\|^2 &\le 8m^2\|y - \tilde y\|^2 + 8m^2\|\tilde y - \hat y\|^2 \\
%     & \le 16m \bar \varepsilon+ 8\|w\|^2
%     \le  16m\bar \varepsilon + 8 \bar \eta^2.
% \end{align*}
% where the last inequality is due to relation \eqref{eq:normw}. Thus the statement follows.


% When $(\bar \eta,\bar \varepsilon)$ is not zero,
% the following result shows that
% any $(\bar \eta,\bar \varepsilon)$-stationary point is close to an exact stationary
% point of the perturbed function
% $\phi(\cdot) -\inner{\tilde w}{\cdot}$
% % $\{ \phi (u) - \inner{\tilde w}{u} : u \in \R^n \}$
% for some sufficiently small $\tilde w$.
% \[
% \phi(\hat x) \ge \phi(x) + \inner{v}{\hat x - x} - \frac{m}2\|\hat  x -x \|^2
% \]
% \[
% m \|\hat x - x\|^2 \le \phi(x) - \phi(\hat x) \le \inner{-v}{\hat x - x} \le \varepsilon \|\hat x -x \|
% \]
% Thus
% \[

% \]

% \begin{proposition}\label{prop:sol}
% Let pair $(\bar\eta,\bar \varepsilon) \in \R^2_{++}$
% be given,
% and define ?????
% \begin{equation}\label{def:hatdelta}
% \hat \varepsilon = \bar \eta + 2 \sqrt{2 m \bar \varepsilon }, \quad \hat \delta = \sqrt{\frac{2\bar \varepsilon}{m}},  
% \end{equation}
% Assume that
% $\phi$ is a
% closed $m$-weakly convex function and that $y$ is 
% $(\bar \eta,\bar\varepsilon;m)$-stationary point of $\phi$.
% % of \eqref{eq:opt_problem}.
% Then, 
% there exists
%     $\hat y \in \R^n$ such that
%     % the following statements hold:
%     % \item[a)]
%     \begin{equation}\label{eq:dist}
%     \|y-\hat y\| \leq \hat \delta,
%     \quad
%     \mbox{\rm dist}(0, \partial \phi( \hat y) )
%     \le \hat \varepsilon,
%     \end{equation}
%    and
%    \[
% \inf_{\|d\| \le 1} \phi'(\hat y;d) \ge
% -\hat \varepsilon.
% \]
% \end{proposition}






% we  define $\varphi(\cdot) = \phi(\cdot)- \inner{w}{\cdot}$ for a given $w$ and $\hat y = \operatorname{prox}_{\lambda \varphi}(y)$. Then from \eqref{eq:keyrela} with $\phi = \varphi$ and $\lam=1/2m$ we know that
% % \[
% % \|\hat y-y\| = \frac{\|\nabla \varphi_{1/2m}(y) \|}{2m}. \quad
% % \mbox{dist}(0, \partial \varphi( \hat y) ) \le \| \nabla \varphi_{1/2m}(y)} \|
% % \]
% % From the definition of $\varphi$ and the above relation we can deduce that
% % \[
% % \|\nabla \phi_{1/2m}(y)\| \le \|\nabla \varphi_{1/2m}(y)\|+ \|w\| = 2m \|y-\hat y\| + \|w\|
% % \]



% \begin{proposition}\label{prop:tran} ({\bf Jiaming})
%     For any $((\bar \eta, \bar \varepsilon),m)$-stationary point $x$ of $\phi$, it holds that 
%     \begin{equation}\label{eq:rela}
%     \|\nabla M^{1/2m}_{\phi}(x)\| \le 2\sqrt{4m\bar \varepsilon+2\bar \eta^2}.
%     \end{equation}
% \end{proposition}



% \subsection{ Review of Davis' method}

% This subsection reviews the
% deterministic version of
% the stochastic proximal subgradient method described
% in Algorithm \ref{alg:Davis} of
% \cite{davis2019stochastic}.

% We start by describing the aforementioned method in
% Algorithm ??? below.

% \begin{algorithm}\label{alg:Davis}
% \caption{Proximal subgradient}
% \state \textbf{Input}: $x_0 \in \dom h$, a sequence of $\left\{\alpha_t\right\}_{t \geq 0} \subset \mathbb{R}_{+}$ and iteration count T.\\
% \state \textbf{Step} $t = 0,1,\cdots, T$:
% \begin{equation}
% \text { Set } x_{t+1}=\operatorname{prox}_{\alpha_t h}\left(x_t-\alpha_t f'(x_t)\right)
% \end{equation}
% % \state Sample $t^* \in\{0, \ldots, T\}$ according to $\mathbb{P}\left(t^*=t\right)=\frac{\alpha_t}{\sum_{i=0}^T \alpha_i}$.\\
% % \STATE \textbf{Return} $x_{t^*} $
% \end{algorithm}

% The following result describes the
% iteration-complexity of
% Algorithm ???.


% \begin{theorem} [Proximal subgradient method]
% Assume $(f,h)$ are functions satisfying conditions (A1)-(A3)
% with $L=0$.
% Fix a real $\bar{m} \in(m, 2 m]$ and assume that  $\alpha_t \in(0,1 / \bar{m}]$ for every $t \ge 0$. Then,
% % the iterates $x_t$ generated by Algorithm $3.1$ satisfy
% % $$
% % \varphi_{1 / \bar{m}}\left(x_{t+1}\right) \leq \varphi_{1 / \bar{m}}\left(x_t\right)-\frac{\alpha_t(\bar{m}-m)}{\bar{m}} \left\|\nabla \varphi_{1 / \bar{m}}\left(x_t\right)\right\|^2+\alpha_t^2 \bar{m} L^2,
% % $$
% % and 
% % the point $x_{T^*}$ returned by Algorithm $3.1$ satisfies:
% % $$
% % \mathbb{E}\left\|\nabla M_\phi^{1 / \bar{m}}\left(x_{t^*}\right)\right\|^2 \leq \frac{\bar{m}}{\bar{m}-m} \cdot \frac{\left(\phi_{1 / \bar{m}}\left(x_0\right)-\min \phi\right)+2 \bar{m} M^2 \sum_{t=0}^T \alpha_t^2}{\sum_{t=0}^T \alpha_t} .
% % $$
% $$
% \frac{\sum_{t=0}^T \alpha_t \left\|\nabla M_\phi^{1 / \bar{m}}\left(x_t\right)\right\|^2}{\sum_{t=0}^T \alpha_t}  \leq \frac{\bar{m}}{\bar{m}-m} \cdot \frac{\left(\phi_{1 / \bar{m}}\left(x_0\right)- \phi_*\right)+2 \bar{m} M^2 \sum_{t=0}^T \alpha_t^2}{\sum_{t=0}^T \alpha_t} .
% $$
% As a consequence, Algorithm~1 with $\bar m=2m$ and $\alpha_t=1/\sqrt{T+1}$ for every $t \ge 0$ generates an iterate $x_t$
% satisfying
% $\left\|\nabla M_\phi^{1 / (2m)}\left(x_t\right)\right\|^2
% \le \varepsilon$ in at most
% \begin{equation}
%     \frac{\left(\phi_{1 / 2 m}\left(x_0\right)- \phi_*\right)+4 m M^2 \gamma^2}{\gamma \varepsilon^4}.
% \end{equation}
% \end{theorem}
% % In particular, if Algorithm \ref{alg:Davis} uses the constant parameter $\alpha_t=\frac{\gamma}{\sqrt{T+1}}$, for some real $\gamma \in\left(0, \frac{1}{2 m}\right]$, then the point $x_{t^*}$ satisfies:
% % $$
% % \frac{\sum_{t=0}^T \left\|\nabla M_\phi^{1 / 2 m}\left(x_{t}\right)\right\|^2}{
% % T+1} \leq 2 \cdot \frac{\left(\phi_{1 / 2 m}\left(x_0\right)-\min \phi\right)+4 m M^2 \gamma^2}{\gamma \sqrt{T+1}}.
% % $$
% % Thus the complexity to generate a point such that $\|\nabla M_\phi^{1/2m}\| \le \varepsilon$ is at most 
% % \begin{equation}
% %     \frac{\left(\phi_{1 / 2 m}\left(x_0\right)-\min \phi\right)+4 m M^2 \gamma^2}{\gamma \varepsilon^4}
% % \end{equation}
% One remark is that Davis only consider the case that $f$ satisfies Assumption (A2) with $L_f = 0$ which is a more restrict function class.
% \begin{corollary}
    
% \end{corollary}

% \item[b)] there holds
% \[
% \inf_{\|d\| \le 1} \phi'(\hat y;d) \ge
% -\hat \varepsilon.
% \]
% \item[c)]
% there holds

% \end{itemize}
% \end{corollary}



% -------------------------

% Let $\lam=1/(2m)$ and $\varphi(\cdot) = \phi(\cdot)- \inner{w}{\cdot}$. Then
% \[
% \|\hat y-y\| = (1/(2m)) \|\nabla \varphi_\lam(y)} \|, \quad
% \mbox{dist}(0, \partial \varphi( \hat y) ) \le \| \nabla \varphi_\lam(y)} \|

% \]
% \[
% \mbox{dist}(0, \partial \varphi( \hat y) ) \le 2m \|y-\hat y\|
% \]
% Note that
% \[
% \mbox{dist}(0, \partial \phi( \hat y) ) \le
% \mbox{dist}(0, \partial \varphi( \hat y) ) + \|w\|
% \le 2m \|y-\hat y\| + \|w\|
% \]

% ---------------------------












\section{Algorithm}
\label{Sec:PBF}

This section contains four subsections. The first one describes the main problem and the assumptions made on it.
The second one reviews the deterministic version of stochastic composite subgradient
(SCS) method of \cite{davis2019stochastic} and its main complexity result.
The third one presents a generic bundle update scheme and describes three special cases of it. The fourth one describes the  proximal bundle framework (PBF) and describes its main complexity result.

\subsection{Problem description and main assumptions}
\label{sec:assumptions}

The main problem of this paper is described in \eqref{eq:opt_problem}
where the functions $f$ and $h$ are assumed to satisfy:
%\subsubsection{Main Assumptions}
\begin{itemize}
    \item[(A1)] functions $f,h:\mathbb{R}^{n} \rightarrow \mathbb{R} \cup\{\infty\}$ and
    a scalar $m>0$ such that
    $h \in \bConv{n}$,
    $f$ is $m$-weakly convex
    and $\dom h \subset \dom f$;
		% $f_m(\cdot;0)$ and $h$ are are both in
		% $\bConv{n}$ and
		% $\dom h \subset \dom f$;
% 		\item[(A2)]
% 		the set of optimal solutions $X^*$ of
% 		problem \eqref{eq:opt_problem} is nonempty;

	\item[(A2)]
% 	$f:\mathbb{R}^{n} \rightarrow \mathbb{R} \cup\{\infty\}$ is a function such that $\dom f \supset \dom h$ and
		there exist constants
		$M, L \ge 0$ and
		a subgradient oracle,
		 i.e.,
		a function $f':\dom h \to \R^n$
		satisfying $f'(u) \in \partial f(u)$ for every $u \in \dom h$, such that
		\begin{equation}
		   	\|f'(u)-f'(v)\| \le 2M + L \|u-v\| \quad\forall u,v \in \dom h;
		\label{cond:A2} 
		\end{equation}
		
		\item[(A3)]
		$\phi^*:= \inf \{ \phi(u) : u \in \R^n \}$ is finite.
  \end{itemize}

  % {\bf ?????????????  Define $\ell_{f_{m}(\cdot;z)}(y;x)$ and $\ell_{\phi_{m}(\cdot;z)}(y;x)$}

	Let ${\cal C}(M,L)$ denote the class of functions $f$ satisfying Assumption (A2).
		Even though ${\cal C}(M,L)$ depends on $\dom h$, we have omitted this dependence from its notation. For any function
  $g \in {\cal C}(M,L)$ and $x \in \dom h$, we denote the linearization of $g$ at
  $x$ by
  \begin{equation}
\label{def:l_g}
  \ell_g(\cdot;x) := g(x) + \inner{g'(x)}{\cdot-x}.
  \end{equation}

  
        % Assume $h: \R^n\rightarrow (-\infty,+\infty]$ is a nonsmooth convex function, the linearization of $\phi_m(:;z) = f_m(:;z)+ h$ at $y \in \R^n$ is given by
        % \[
        % \ell_{\phi_m(:;z)}(:;y):=\phi_m (y;z) + (f'(y)+m(y-z))(:-y).
        % \]
		
		% {\bf The ortacke in (B2) can implemented if have a oracle for convex function}
		
% 		there exists $R>0$ such that 
% 		$\dom h \subset \bar B(0;R)$.
% \end{itemize}

	\begin{lemma}
 \label{lem:Lip_ineq}
Assume that $f\in {\cal C}(M,L)$ for some $(M,L) \in \R^2_+$ and $f$ is $m$-weakly convex
on $\dom h$.
Then, for every $z \in \R^n$ and $\tz \in \dom h$, we have
\[
f'(\tz) + m(\tz-z) \in \partial [ f_m(\cdot;z) ](\tz), \qquad
f_m(\cdot;z) \in 
\Conv{n} \cap {\cal C}(M,L+m).
\]
% $f_m(\cdot;z) \in 
% \bConv{n} \cap {\cal C}(M,L+m)$ for every $z \in \R^n$.
% As a consequence, we have
% \[
%     0 \le   f_{m}(y;z) - \ell_{f_{m}(\cdot;z)}(y;x) \le     M \|y-x\| + \frac{ L + 
%     m}2 \|y-x\|^2 \quad \forall x, y\in \R^n.
% 	\]
% \\
% Let  $\phi = f + g: \mathcal{X} \mapsto(-\infty, \infty]$ be a $ m $- weakly convex function and assume that $f$ satisfies Assumption A(1) and A(3), then  we have 
\end{lemma}
\begin{proof}
Since $f\in {\cal C}(M,L)$, there exists an oracle $f'$ satisfying
the conditions in (A2), i.e.,
$f'(x) \in \partial f(x)$ for every $x \in \dom f$ and
\eqref{cond:A2} holds.
Since $f$ is $m$-weakly convex, it follows from the definition of
weak convexity that 
$f_m \in \bConv{n}$. Moreover,
it follows from Lemma \ref{lem:chara_weakly} with $(\varepsilon,x,c) = (0,\cdot,z)$ and
the fact that
$f'(x) \in \partial f(x)$ for every $x \in \dom f$
that $f_m'(\cdot;z) := f'(\cdot) + m (\cdot - z) \in \partial [f_m(\cdot;z)](\cdot)$.
The result now follows by noting that
\eqref{cond:A2} and the definition
of $f_m+\chi/\lam$ imply that
for every $x,y \in \dom f$
\begin{align*}
\|f_m'(x;z) - f_m'(y;z)\| &= \|f'(x) - f'(y) + m(x-y)\| \\
&\le \|f'(x) - f'(y)\| + m\|x - y\|\\
&\le 2M + (L+m)\|x-y\|
\end{align*}
which means that $f_m(\cdot;z) \in {\cal C}(M,L+m)$.
\end{proof}

In view of the first inclusion of
Lemma \ref{lem:Lip_ineq},
it follows that
for any $z \in \R^n$ and $\tz \in \dom h$, function $f_m(\cdot;z)$ admits the linearization
given by
\beq \label{def:linear}
% \ell_{f_m(\cdot;z)}(\cdot;\tz)
%\ell(\cdot;\tz,(f,z,m)):
\ell_{f_m(\cdot;z)} (\cdot;\tz) :=
 f_m(\tz;z) + \inner{f'(\tz)+m(\tz-z)\,}{\,\cdot-\tz}.
\eeq
Moreover, in view of the second inclusion of Lemma \ref{lem:Lip_ineq}, we have
\beq \label{eq:fm-linear}
    0 \le   f_{m}(\cdot;z) - 
    \ell_{f_m(\cdot;z)} (\cdot;\tz)
    %\ell(\cdot;\tz,(f,z,m)) 
    \le    2 M \|\cdot-\tz\| + \frac{ L + 
    m}2 \|\cdot-\tz\|^2 \quad \forall z \in \R^n,\, \tz \in \dom h.
	\eeq
 Note that the above observations with
 $\tz=z$ implies that for every $z \in \dom h$,
 we have
 
 \begin{equation}\label{eq:simlin}
   \ell_{f_m(\cdot;z)} (\cdot;z) = \ell_f(\cdot;z), \quad    
 \end{equation}
 and
 \beq \label{eq:fm-linear1}
   0 \le   f_{m}(\cdot;z) - 
    \ell_{f} (\cdot;z)
    %\ell(\cdot;\tz,(f,z,m)) 
    \le     2M \|\cdot-z\| + \frac{ L + 
    m}2 \|\cdot-z\|^2 \quad \forall z \in \dom h.
	\eeq
% Let $f: \R^n\rightarrow (-\infty,+\infty]$ be given. Assume f is $m$-weakly convex, the linearization of $f_m(\cdot;z)$ at $y \in \R^n$ is given by
        % \[

        % \]
        
        
        \subsection{ Review of the SCS 
         method for the weakly convex case}\label{SEC:SCS}

This subsection reviews the
deterministic version of
the SCS method of~\cite{davis2019stochastic}.

More specifically, it considers
the PS method
described below
under the assumptions stated in
Subsection \ref{sec:assumptions} 
except that the  constant $L$
in condition (A2) is assumed to be zero.

% The aforementioned method is described in
% Algorithm \ref{alg:Davis}
% while
% its iteration-complexity is
% described in Proposition~\ref{??}
% below.

% \begin{algorithm}
% \caption{Proximal subgradient}
% \label{alg:Davis}
% \state \textbf{Input}: $\hat x_0 \in \dom h$, a sequence of $\left\{\alpha_t\right\}_{t \geq 0} \subset \mathbb{R}_{+}$ and iteration count $T$.\\
% \state \textbf{Step} $t = 0,1,\cdots, T$:
% \begin{equation}
% \text { Set } \hat x_{t+1}=\operatorname{prox}_{\alpha_t h}\left(\hat x_t-\alpha_t f'(\hat x_t)\right)
% \end{equation}
% % \state Sample $t^* \in\{0, \ldots, T\}$ according to $\mathbb{P}\left(t^*=t\right)=\frac{\alpha_t}{\sum_{i=0}^T \alpha_i}$.\\
% % \STATE \textbf{Return} $x_{t^*} $
% \end{algorithm}


\noindent\rule[0.5ex]{1\columnwidth}{1pt}
	
	PS

    \noindent\rule[0.5ex]{1\columnwidth}{1pt}
    {\bf Input:} $\hat x_0 \in \dom h$, a sequence of $\left\{\alpha_t\right\}_{t \geq 0} \subset \mathbb{R}_{+}$ and iteration count $T$.\\
    {\bf Step:} $t = 0,1,\cdots, T$:
\begin{equation}
\text { Set } \hat x_{t+1}=\argmin_{u \in \R^n} \left\{\ell_{f}(u;\hat x_t)+ h(u)+ \frac{1}{2\alpha_t}\|u -  \hat x_t\|^2\right\}.
\end{equation}
    \noindent\rule[0.5ex]{1\columnwidth}{1pt}


The following result (see Theorem 3.4 in \cite{davis2019stochastic}) describes the rate
of convergence of the PS method.

\begin{proposition}
\label{prop:Davis}
Assume $(f,h)$ are functions satisfying conditions (A1)-(A3)
with $L=0$ and
assume that  $\alpha_t \in(0,1 / \bar{m}]$ for every $t \ge 0$ for some
$\bar m \in (m,2m]$. Then,
the iterates $\hat x_t$ of
PS satisfies
\begin{equation}\label{Moreacomplexity}
\frac{\sum_{t=0}^T \alpha_t \left\|\nabla \hat M^{1 / (\bar{m}-m)}\left(\hat x_t\right)\right\|^2}{\sum_{t=0}^T \alpha_t}  \leq \frac{\bar{m}}{\bar{m}-m} \cdot \frac{\left(\hat M^{1 / (\bar{m}-m)}\left(\hat x_0\right)- \phi^*\right)+2 \bar{m} M^2 \sum_{t=0}^T \alpha_t^2}{\sum_{t=0}^T \alpha_t} .
\end{equation}
As a consequence, for any
given tolerance $\rho>0$ and constant $\gamma $ such that
$\gamma \in(0, 1/(2m)]$,
if the stepsizes $\alpha_t$ are chosen according to
$\alpha_t = \gamma/\sqrt{T+1}$
for every $t \ge 0$ and the iteration count $T$ satisfies
\begin{equation}\label{eq:MoreauT}
  T \ge \frac{\left[\left(\hat M^{1/m}\left(\hat x_0\right)- \phi^*\right)+4 m M^2 \gamma^2\right]^2}{\gamma^2 \rho^4},
\end{equation}
then
one of the iterates
$\hat x_t \in 
\{ \hat x_0, \ldots, \hat x_T\}$
% if the number of iterations $T$ 
of PS
% then one of the iterates
% $\hat x_t \in 
% \{ \hat x_0, \ldots, \hat x_T\}$
satisfies
$\left\|\nabla \hat M^{1 / m}\left(\hat x_t\right)\right\| \le \rho$.
\end{proposition}

% We now make some remarks
% about Algorithm ??? and Proposition \eqref{prop:Davis}.
% First,
% since $\nabla M_\phi^{1 / (2m)}\left(\hat x_t\right)$ is difficult to compute, a termination criterion
% based on this quantity is not
% computationally feasible.
% Instead, ??????
% Second,
% the method of
% \cite{davis2019stochastic}
% actually outputs
% an iterate $\hat x_{t^*}$ where $t^*$
% is
% sampled from $\{0, \ldots, T\}$ according to the probability mass function $\mathbb{P}\left(t^*=t\right)=\alpha_t/\sum_{i=0}^T \alpha_i$ for every $t =0,\ldots,T$.
% It turns out that
% $\E[ \|\nabla M_\phi^{1 / (2m)}\left(\hat x_{t^*}\right)\|^2]$
% is equal to the left hand side of
% \eqref{Moreacomplexity} and hence is bounded
% by the right hand side of
% \eqref{Moreacomplexity}.
% However, a drawback of this approach is that it
% is not clear how
% to use $\hat x_{t^*}$ to
% develop an alternative stopping criterion
% that guarantees that
% $\hat x_{t^*}$ is a $(\rho,1/(2m))$-Moreau stationary point in expectation
% since neither the latter
% expected value nor
% the right hand side of \eqref{Moreacomplexity}
% is computable.

We now make some remarks
about PS and Proposition \ref{prop:Davis}.
First, PS always performs $T+1$ iterations
and the second part of Proposition \ref{prop:Davis} gives 
a sufficient condition on
$T$ which guarantees that
one of its iterates is a
$(\rho;1/m)$-Moreau stationary point.
An alternative termination condition based on the magnitude
of $\nabla \hat M^{1 /m}(\hat x_t)$
is not doable since this quantity is generally expensive to compute.
Second, a drawback of PS
is that the right hand side of \eqref{eq:MoreauT} is
generally not computable,
and hence there is no clear
way of choosing $T$ satisfying \eqref{eq:MoreauT}.
Third,
the method of
\cite{davis2019stochastic}
actually outputs
an iterate $\hat x_{t^*}$ where $t^*$
is
sampled from $\{0, \ldots, T\}$ according to the probability mass function $\mathbb{P}(t^*=t)=\alpha_t/\sum_{i=0}^T \alpha_i$ for every $t =0,\ldots,T$.
It turns out that
$\E[ \|\nabla \hat M^{1 /m}(\hat x_{t^*})\|^2]$
is equal to the left hand side of
\eqref{Moreacomplexity} and hence is bounded
by the right hand side of
\eqref{Moreacomplexity}.
The advantage of this approach
is that, without
performing
any evaluation of $\|\nabla \hat M^{1 /m}(\cdot)\| ^2$,
it returns 
an iterate
$\hat x_{t^*}$
such that the expected
value of $\|\nabla \hat M^{1 /m}(\hat x_{t^*}) \|^2$
is bounded by the right hand
side of \eqref{Moreacomplexity}. However, there is no simple way of de-randomizing this output strategy
due to the fact that
the function
$\|\nabla \hat M^{1 /m}(\cdot) \|^2$
 is generally nonconvex and hard to compute. 


 
%  This is not desirable for deterministic optimization problem because people usually seek for deterministic convergence guarantee.

% In particular, if Algorithm \ref{alg:Davis} uses the constant parameter $\alpha_t=\frac{\gamma}{\sqrt{T+1}}$, for some real $\gamma \in\left(0, \frac{1}{2 m}\right]$, then the point $x_{t^*}$ satisfies:
% $$
% \frac{\sum_{t=0}^T \left\|\nabla M_\phi^{1 / 2 m}\left(x_{t}\right)\right\|^2}{
% T+1} \leq 2 \cdot \frac{\left(\phi_{1 / 2 m}\left(x_0\right)-\min \phi\right)+4 m M^2 \gamma^2}{\gamma \sqrt{T+1}}.
% $$
% Thus the complexity to generate a point such that $\|\nabla M_\phi^{1/2m}\| \le \varepsilon$ is at most 
% \begin{equation}
%     \frac{\left(\phi_{1 / 2 m}\left(x_0\right)-\min \phi\right)+4 m M^2 \gamma^2}{\gamma \varepsilon^4}
% \end{equation}


% One remark is that Davis only consider the case that $f$ satisfies Assumption (A2) with $L_f = 0$ which is a more restrict function class.
% \begin{corollary}
    
% \end{corollary}

% \item[b)] there holds
% \[
% \inf_{\|d\| \le 1} \phi'(\hat y;d) \ge
% -\hat \varepsilon.
% \]
% \item[c)]
% there holds

% \end{itemize}
% \end{corollary}



% -------------------------

% Let $\lam=1/(2m)$ and $\varphi(\cdot) = \phi(\cdot)- \inner{w}{\cdot}$. Then
% \[
% \|\hat y-y\| = (1/(2m)) \|\nabla \varphi_\lam(y)} \|, \quad
% \mbox{dist}(0, \partial \varphi( \hat y) ) \le \| \nabla \varphi_\lam(y)} \|

% \]
% \[
% \mbox{dist}(0, \partial \varphi( \hat y) ) \le 2m \|y-\hat y\|
% \]
% Note that
% \[
% \mbox{dist}(0, \partial \phi( \hat y) ) \le
% \mbox{dist}(0, \partial \varphi( \hat y) ) + \|w\|
% \le 2m \|y-\hat y\| + \|w\|
% \]

% ---------------------------

 \subsection{The generic bundle update scheme}
 \label{sec:GBUS}

As mentioned in the Introduction, PBF uses a bundle (convex) function 
underneath $\phi(\cdot)+m\|\cdot-x^c\|^2/2$, where $x^c$ is the current prox center, to
construct subproblem \eqref{eq:x-pre} at a given iteration, and then
updates $\Gamma$ to obtain the bundle function $\Gamma^+$ for the next iteration.
This subsection describes ways of updating
the bundle.
% ???????????? One of the key step for PBF is to construct bundle functions $\Gamma$ underneath  $\phi(\cdot)+(m/2)\|\cdot-x^c\|^2$
% % in the context of weakly convex function.
	Instead of focusing
	on a specific bundle update scheme, 
	this subsection
 describes 
	a generic update scheme. It also discusses three concrete ways of implementing the generic scheme.
	% This subsection also
	% describes three concrete examples of the generic bundle update scheme. 
 
    We start by describing the generic bundle update scheme (GBUS) . 

    \newpage
    
    \noindent\rule[0.5ex]{1\columnwidth}{1pt}
	
	GBUS

    \noindent\rule[0.5ex]{1\columnwidth}{1pt}
    {\bf Input:} Pair $(g,h)$ where
    $g, h \in \bConv{n}$, $(\lam,\tau) \in \R_{++} \times (0,1)$, and
    $(x^c,x,\Gamma) \in \R^n \times
    \R^n \times \bConv{n}$ such that $\Gamma\le g+h$ and \eqref{eq:x-pre} holds;
  %   \beq
	 % x = \underset{u\in \R^n}\argmin \left \{\Gamma (u) + \frac{1}{2\lam} \|u-x^c\|^2 \right\};
  % \eeq
    \begin{itemize}
        % \item 
        % compute $x$ as in \eqref{eq:x-pre};
        \item
        find function $\Gamma^+$ such that
\begin{equation}\label{def:Gamma}
	    \Gamma^+ \in \bConv{n}, \qquad \tau \bar \Gamma(\cdot)  + (1-\tau) [\ell_g(\cdot;x)+h(\cdot)] \le \Gamma^+(\cdot) \le (g+h)(\cdot),
	\end{equation}
   where $\ell_g(\cdot;\cdot)$ is as in \eqref{def:l_g} and
   $\bar \Gamma(\cdot) $ is such that
\begin{equation}\label{def:bar Gamma} 
	\bar \Gamma \in \bConv{n}, \quad
 \bar \Gamma(x) = \Gamma(x), \quad 
	x = \underset{u\in \R^n}\argmin \left \{\bar \Gamma (u) + \frac{1}{2\lam} \|u-x^c\|^2 \right\}.
	\end{equation}
     \end{itemize}
     {\bf Output:} $\Gamma^+$.
     
    \noindent\rule[0.5ex]{1\columnwidth}{1pt}

 
 % GBUS was ????? first proposed in \cite{liang2021unified} and
 We will now discuss three concrete ways of implementing GBUS.

	\begin{itemize}
	    \item [(S1)] {\bf one-cut scheme}:
	    This scheme sets    
        $\Gamma^+= \Gamma^+_\tau := \tau \Gamma + (1-\tau) [\ell_g(\cdot;x)+h]$
        where $x$ is as in \eqref{def:bar Gamma}.
	   
	    \item [(S2)] \textbf{two-cut scheme:}
	    Suppose $\Gamma=\max \{A_g,\ell_g(\cdot;x^-)\}+h$ where $A_g\le g$ is an affine function and $x^-$ is the previous iterate.
	    This scheme sets $\Gamma^+=\max \{A_g^+,\ell_g(\cdot;x)\}+h$ where
	    $A_g^+ =  \theta A_g + (1-\theta) \ell_g(\cdot;x^-)$ for some
	    $\theta\in[0,1]$.
	    
	    \item [(S3)] \textbf{multiple-cut scheme:}
	    Suppose $\Gamma=\Gamma(\cdot;C)$ where
	    $C \subset \R^n$ is a finite set (i.e., the current  bundle set) and $\Gamma(\cdot;C)$ is defined as
	    \begin{equation}\label{eq:Gamma-E2}
	        \Gamma(\cdot;C) := \max \{ \ell_g(\cdot;c) : c \in C \}+h(\cdot).
	    \end{equation}
	     This scheme 
      % computes $x$ as in \eqref{eq:x-pre}, 
      chooses the next bundle set $C^+$ so that
	    \begin{equation}\label{eq:C+}
        C(x) \cup \{x\} \subset C^+ \subset C \cup \{x\}
        \end{equation}
        where
        \begin{equation}\label{def:C+}
            C(x) := \{c \in C : \ell_g(x;c)+h(x) = \Gamma(x) \},
        \end{equation}
         and then
	    output $\Gamma^+ = \Gamma(\cdot;C^+)$.
	\end{itemize}

     We now make some remarks to argue that all the update schemes 
above are special implementations of GBUS.
It can be easily seen that 
the update $\Gamma^+$ in
(S1), together with
 $\bar \Gamma=\Gamma$, satisfies \eqref{def:Gamma} and \eqref{def:bar Gamma}, and hence
 that scheme (S1) is a special way of implementing GBUS.
 On the other hand, the results below, whose proof can be found in Appendix D in \cite{liang2021unified},
 show that
 (S2) and (S3) are also
 special implementations of GBUS.


\begin{itemize}\label{lem:E2}
    \item[(a)]
    If $\Gamma^+$ is obtained according to (S2), then
    the pair $(\Gamma^+,\bar \Gamma)$ where
   $\bar \Gamma = A_g^+ + h$ satisfies \eqref{def:Gamma} and \eqref{def:bar Gamma}.
    % As a consequence,
    % $\Gamma^+$ is a special implementation of GBUS.
    \item[(b)] 
    If $\Gamma^+$ is obtained according to (S3),
     then
    the pair $(\Gamma^+,\bar \Gamma)$ where
   $\bar \Gamma =\Gamma(\cdot;C(x))$
    %  Consider the update $\Gamma^+$ of (S3) and set
    % $\bar\Gamma=\Gamma(\cdot;C(x))$ where  $\Gamma(\cdot;C(x))$ is as in \eqref{eq:Gamma-E2}. Then,
    % $(\Gamma^+,\bar \Gamma)$ 
    satisfies \eqref{def:Gamma} and \eqref{def:bar Gamma}.
    % As a consequence,
    % $\Gamma^+$ is a special implementation of GBUS.
	\end{itemize}
	

 \subsection{The proximal bundle framework} \label{subsec:update}


 This subsection describes PBF and its main complexity result.
 It also compares the complexity of PBF with that of the deterministic version of the SCS method of \cite{davis2019stochastic}
 described in Subsection~\ref{SEC:SCS}.
	
% 	We now describe three concrete
%  update schemes (E1), (E2), and (E3) which are special ways
%  of implementing BU.
%  Unless otherwise stated, it
%  is assumed that their input is
%  the same as in BU.
 
 
% 	\begin{itemize} 
%      \item [(E1)] \red{{\bf one-cut scheme}}: 
%      % Given $\Gamma\in \mConv{n}$ satisfying $\Gamma\le \phi$ and $(x^c,\lam,\tau) \in \R^n \times \R_{++} \times (0,1)$,
%      This scheme obtains $\Gamma^+$ as in \eqref{eq:affine}.
% 	It is easy to see that if this update is used recursively
% 	then $\Gamma$ is always of the form
% 	\begin{equation}\label{eq:Gamma-form}
% 	    \Gamma(\cdot) = \sum_{x \in X} \alpha_x \ell_f(\cdot;x)  + h(\cdot)
% 	\end{equation}
% 	where $X$ is a finite set in $\dom h$ and
%   $\{\alpha_x : x \in X\} \subset \R_{++}$ are scalars such that
% 	$\sum_{x \in X} \alpha_x =1$.
	
%         \item [(E2)] \red{\textbf{two-cuts scheme:}} 
%         For this scheme, it is assumed that
%         $\Gamma$
%         has the form 
%         \begin{equation}\label{def:Gamma-E2}
%             \Gamma=\max \{A_f,\ell_f(\cdot;x^-)\}+h
%         \end{equation}
%         where $h \in \mConv{n}$ and
%         $A_f$ is an affine function satisfying $A_f\le f$. In view of \eqref{eq:x-pre},
%         it can be shown that there exists
%         $\theta \in [0,1]$ such that
%         \begin{align}
%             &\frac1\lam (x-x^c) + \partial h(x) 
% + \theta \nabla A_f + (1-\theta) f'(x^-) \ni 0, \label{theta1} \\
%             &\theta A_f(x) + (1-\theta) \ell_f(x;x^-) = \max \{A_f(x),\ell_f(x;x^-)\}. \label{theta2}
%         \end{align}
%         The scheme then sets
%         \begin{equation}\label{def:Af+}
%          A_f^+(\cdot) :=  \theta A_f(\cdot) + (1-\theta) \ell_f(\cdot;x^-)
%      \end{equation}
%      and outputs the function $\Gamma^+$
%       defined as
% \begin{equation}\label{eq:G-agg}
% 		    \Gamma^+(\cdot)  := 
% 		    \max\{A_f^+(\cdot),\ell_f(\cdot;x)\} + h(\cdot).
% 	    \end{equation}
% 	    % where 
%      % \begin{equation}\label{def:Af+}
%      %     A_f^+(\cdot) =  \theta A_f(\cdot) + (1-\theta) \ell_f(\cdot;x^-).
%      % \end{equation}
        
% 	    % \begin{equation}\label{eq:theta}
% 		   %  \theta  \left\{\begin{array}{ll}
% 		   %  =1, & \text { if } A_f(x)> %=(\Gamma-h)(x), \quad
% 		   %  \ell_f(x;x^-), \\ %\ne (\Gamma-h)(x) \\ 
% 		   %  =0, & \text { if } A_f(x) < \ell_f(x;x^-), \\ %=(\Gamma-h)(x), \quad A_f(x) \ne (\Gamma-h)(x) \\
% 		   %  \in[0,1], & \text { if } A_f(x)=\ell_f(x;x^-). %=(\Gamma-h)(x).
% 		   %  \end{array}\right.
% 	    % \end{equation}

     
% 	    % It will be shown in Lemma \ref{lem:E2} that $\Gamma^+ \in \mConv{n}$ and that, for any $\tau \in (0,1)$, the function $\Gamma^+$ satisfies \eqref{def:Gamma} with $\bar \Gamma = A_f^+ + h$,
% 	    % and hence that $\Gamma^+$ is an output of BUS$(\Gamma,x^c,\lam,\tau)$.

     
	    
% 	    \item [(E3)] \red{\textbf{multiple-cuts scheme:}} For this scheme, it is assumed that $\Gamma$ of the form
% 	    $\Gamma=\Gamma(\cdot;B)$ where
% 	    $B \subset \R^n$ is a finite set (i.e., the current  bundle set) and $\Gamma(\cdot;B)$ is defined as in \eqref{eq:Gamma-E2}.
% 	   % \begin{equation}\label{eq:Gamma-E2}
% 	   %     \Gamma(\cdot;C) := \max \{ \ell_f(\cdot;c) : c \in C \}+h ,
% 	   % \end{equation}
% 	     This scheme 
%       % computes $x$ as in \eqref{eq:x-pre}, 
%       chooses the next bundle set $B^+$ so that
% 	    \begin{equation}\label{eq:C+}
%         B(x) \cup \{x\} \subset B^+ \subset B \cup \{x\}
%         \end{equation}
%         where
%         \begin{equation}\label{def:C+}
%             B(x) := \{ b \in B : \ell_f(x;b)+h(x) = \Gamma(x) \},
%         \end{equation}
%          and then
% 	    output $\Gamma^+ = \Gamma(\cdot;B^+)$.

        
        
%         % It will be shown in Lemma \ref{lem:E3} that  $\Gamma^+ \in \mConv{n}$ and that, for any $\tau \in (0,1)$, the function $\Gamma^+$ satisfies \eqref{def:Gamma} with $\bar \Gamma=\Gamma(\cdot;C(x))$,
%         % % \begin{equation}\label{eq:bar Gamma E3}
%         % %     \bar \Gamma= \max \{ \ell_f(\cdot;c) : c \in C(x)\} + h,
%         % % \end{equation}
%         % and hence that $\Gamma^+$ is an output of BUS$(\Gamma,x^c,\lam,\tau)$.
        
        
% %         Note that if the above update 
% %         % with $C^+$ chosen
% %         % as $C^+=C \cup \{x\}$
% %         is used recursively,
% % 	    then $\Gamma$ is always of the form
% % 	$$\Gamma(\cdot) = \max \{ \ell_f(\cdot;c) : c \in C\}  + h(\cdot)$$ where
% % 	$C$ is a (finite) bundle set contained in
% % 	the set $X$ of all points
% % 	$x$ computed as in \eqref{eq:x-pre} as $\Gamma$ is recursively updated
% % 	according to \eqref{eq:Gamma E3}. Observe that $C=X$ when
% % 	$C^+$ is always chosen as $C^+=C \cup \{x\}$.
	
%         % \red{think about the $\bar C$ with Lagrange multiplier}
        
	    
% 	   % \item [(E5)] \textbf{quadratic model update:} Given $(\Gamma,x_0,\lam) \in {\cal C}_\mu(\phi) \times \R^n \times \R_{++}$, this scheme obtains $\Gamma^+$ as
% 	   % \begin{equation}\label{eq:Gamma E5}
% 	   %     \Gamma^+=\max\left \lbrace \Gamma(x) + \frac{1}{\lam}\inner{x_0-x}{\cdot-x} + \frac{\mu}2 \|\cdot-x\|^2, \ell_\phi(\cdot;x) \right \rbrace
% 	   % \end{equation}
% 	   % where $x$ is as in \eqref{eq:x-pre}.
% 	   % It will be shown in Lemma \ref{lem:unify} below that for any $\tau \in (0,1)$, $\Gamma^+ \in {\cal C}_\mu(\phi)$ and $\Gamma^+$ satisfies \eqref{def:Gamma} with $\bar \Gamma$ as
% 	   % \begin{equation}\label{eq:bar Gamma E5}
% 	   %     \bar \Gamma=\Gamma(x) + \frac{1}{\lam}\inner{x_0-x}{\cdot-x} + \frac{\mu}2 \|\cdot-x\|^2,
% 	   % \end{equation}
% 	   % and hence $\Gamma^+ \in {\cal C}_\phi(\Gamma,x_0,\lam,\tau)$.
%     % 	\red{Indeed, $\Gamma^+ = \max\{\bar \Gamma, \ell_\phi(\cdot;x)\}$.}
% 	\end{itemize}

%  ??? It is interesting to note that
%  \eqref{eq:G-agg}, \eqref{eq:C+} and the
%  definition of $\Gamma^+$ in (E3) imply that
%         the  updates $\Gamma^+$ output by
%         schemes (E2) and (E3) have the property that $\Gamma^+(\cdot)$ is minorized 
%         by $\ell_f(\cdot;x)+h(\cdot)$
%         where $x$ is as in \eqref{eq:x-pre}.
%         On the other hand,
%         $\Gamma^+$ output by (E1)
%         does not necessarily has this property.

% ???? We now make some remarks to argue that all the update schemes 
% above are special implementations of BU.
% It can be easily seen that 
% the update $\Gamma^+$ in
% (E1), together with
%  $\bar \Gamma=\Gamma$, satisfies
%  \eqref{def:Gamma} and \eqref{def:bar Gamma}, and hence
%  that this $\Gamma^+$ is a special way of implementing BU.
%  On the other hand, the proofs that
%  the updates $\Gamma^+$ of (E2) and (E3) are
%  special implementations of BU
%  are more involved and are given in
%  Propositions \ref{lem:E2} and \ref{lem:E3}, respectively.
 
% \begin{subsection}{Generic Bundle Update Scheme} (OLD)

% In this section, we discuss some generic bundle update Schemes. Following [to do], we introduce the general Bundle update framework as follows:\\
% \begin{algorithm}
% \caption{Generic Bundle Update Scheme (GBUS)}
%  \begin{algorithmic}
% \State{Input:}
% $\psi, \Gamma \in \bConv{n}$
% such that $\Gamma \le \psi$
% and 
% a triple
% 	$(x^c,\lambda,\tau) \in \R^n \times \R_{++} \times (0,1) $;
	
% \State {Output:}
% define
% \[
% 	x:= \underset{u\in \R^n}\argmin \left \{ \Gamma (u) + \frac{1}{2\lambda} \|u-x^c\|^2 \right\} \label{def:bar Gamma}
% 	\]
% and output a function
% $\Gamma^+ \in \bConv{n}$ such that 
% 	\begin{equation}\label{def:Gamma}
% 	    \tau \bar \Gamma + (1-\tau) [\ell_\psi(\cdot;x)+g] \le \Gamma^+ \le \psi 
% 	\end{equation}
% 	for some
% 	$\bar \Gamma \in {\cal C}(\phi)???????$ satisfying
% 	\begin{equation}
% 	\bar \Gamma(x) = \Gamma(x), \quad 
% 	x = \underset{u\in \R^n}\argmin \left \{\bar \Gamma (u) + \frac{1}{2\lambda} \|u-x^c\|^2 \right\} \label{def:bar Gamma}.
% 	\end{equation}
%  \end{algorithmic}
% \end{algorithm}
% Then we present some of the concrete bundle update rules which satisfies the above general framework.
% 	\begin{itemize}
% 	    \item [(E1)] {\bf one-cut scheme}:
% 	    This scheme obtains $\Gamma^+$ as
% 	    \begin{equation}\label{eq:affine}
%         \Gamma^+= \Gamma^+_\tau := \tau \Gamma + (1-\tau) [\ell(\cdot;x)+h]
%         \end{equation}
%         where $x$ is as in \eqref{eq:x-pre} and $\tau\in (0,1)$ depends on $(L,M,\chi)$. Clearly, if
%         $\Gamma$ is the sum of $h$ and
%         an affine function underneath $f$,
%         then so is $\Gamma^+$.
        
	   
% 	    \item [(E2)] \textbf{two-cuts scheme:}
% 	    Assume that
% 	    $\Gamma=\max \{A_f,\ell(\cdot;x^-)\}+h$ where $A_f$ is an affine function satisfying $A_f\le f$ and $x^-$ is the previous iterate.
% 	    This scheme sets the next bundle function $\Gamma^+$ to one similar to
% 	    $\Gamma$ but with  $(x^-,A_f)$ replaced by
% 	    $(x,A_f^+)$ where
% 	    $A_f^+ =  \theta A_f + (1-\theta) \ell(\cdot;x^-)$ for some
% 	    $\theta\in[0,1]$ which does not
% 	    depend on $(L,M,\chi)$.
	    
% 	    \item [(E3)] \textbf{multiple-cuts scheme:}
% 	    The current bundle function $\Gamma$ is of the form $\Gamma=\Gamma(\cdot;C)$ where
% 	    $C \subset \R^n$ is a finite set (i.e., the current  bundle set) and $\Gamma(\cdot;C)$ is defined as
% 	    \begin{equation}\label{eq:Gamma-E2}
% 	        \Gamma(\cdot;C) := \max \{ \ell(\cdot;c) : c \in C \}+h.
% 	    \end{equation}
% 	    This scheme obtains $\Gamma^+$ as $\Gamma^+ = \Gamma(\cdot;C^+)$ where
% 	    $C^+$ is the updated bundle set obtained
% 	    by possibly 
% 	    removing some points from $C$ and
% 	    then adding the most recent $x$ to
% 	    the resulting set.
% 	\end{itemize}

% \end{subsection}

% \subsubsection{Main notations}
% We let $\Gamma_j$ denote the bundle function for $j$-th iteration. We define the linearization of a convex function f by $\ell_f(\cdot;x) := f(x) + f'(x)(\cdot - x) $ where $f'(x )\in \partial f(x).$ 
% \\Throught the paper, we denote $\partial f(x)$ as the usual subdifferential for convex function f as
% \begin{equation}
%     \partial f(x)  =\left\{ v \in \R^n :  \phi(y) \geq \phi(x)+\langle v, y-x\rangle, \ \forall y \in \R^n \right\}.
%     \label{def:sub}
% \end{equation}
% \section{Main Algorithm}
% This section presents the Proximal Bundle framework. 

Before presenting  PBF, we first
provide a brief outline of the ideas behind it.
PBF consists of solving
a sequence of
subproblems of
the form
\begin{equation}
	    x = \underset{u\in \R^n}\argmin \left \{\Gamma (u) + \frac{1}{2\lam} \|u-\hat x_{k-1} \|^2 \right\}
	\end{equation}
 where  $\Gamma$ is a relatively simple convex function 
 minorizing the convexification $\phi_m(\cdot;\hat x_{k-1})$ of $\phi$ 
 defined in \eqref{def:phim}.
 As any classical proximal bundle approach, it updates the center $\hat x_{k-1}$ during some iterations (called the serious ones) and keeps the center the same in the other ones (called the null ones).

Following the description of PBF below, we also argue that it can be
viewed as
a specific implementation of
an inexact proximal point method
applied to \eqref{eq:opt_problem}.
The parameters
$m$, $ L$ and $M$  that appear on its description 
are as in Assumptions (A1) and (A2).








	\noindent\rule[0.5ex]{1\columnwidth}{1pt}
	
	PBF
	
	\noindent\rule[0.5ex]{1\columnwidth}{1pt}


     \begin{itemize}
		\item [0.] 
    Let initial point $ \hat x_0\in \dom h $, tolerance pair   $(\bar \eta, \bar \varepsilon) \in \R^2_{++}$, 
    cycle tolerance $\delta>0$,
    prox stepsize $\lambda>0$,
    and parameters $\chi \in (0,1]$  and $\tau \in (0,1)$ 
		such that
\begin{equation}\label{rel:tau1} 
	        \frac{\tau}{1-\tau} \ge \lambda  \left(\frac{4M^2}{ \delta} + L + m \right ), \qquad
    \chi(2+m\lam ) -1 >0,
	\end{equation}
		be given, and set  
		$y_0=\hat x_0$, $j=1$, $k=1$,  and
  \beq \label{eq:def-tildem}
  \Gamma_1(\cdot) = \ell_{ f}(\cdot;\hat x_{0}) + h(\cdot), \qquad \tm = m+ \frac{\chi}\lam;
  \eeq
  

	    
    
    \item[1.] {Compute 
		\begin{align}
	    x_{j} &=\underset{u\in \mathbb{R}^n} \arg\min
	    \left\lbrace  \Gamma_{j}(u)+\frac{1}{2\lambda}\|u- \hat x_{k-1} \|^2 \right\rbrace,
	    \label{def:xj} \\
	   \theta_{j}&=\Gamma_{j}(x_{j}) + \frac{1}{2\lam}
		    \|x_{j}-\hat x_{k-1}\|^2,
		    \label{def:thetaj}
	    \end{align}
	    }
		choose
		$y_{j} \in \{ x_{j}, y_{j-1} \}$ such that
		\begin{equation}\label{def:txj}
		 \phi_{\tm}(y_{j};\hat x_{k-1})
		 = \min \left \lbrace \phi_{\tm}(x_{j};\hat x_{k-1}) ,\phi_{\tm} (y_{j-1};\hat x_{k-1}) \right\rbrace;
		\end{equation}
		\item[2.] If
\begin{equation}\label{ineq:hpe1}
		     % \quad 
       t_{j} := \phi_{\tm}(y_{j};\hat x_{k-1}) 
       % +\frac{\chi}{2\lam} \|y_j -x_j^c\|^2 
       - \theta_{j} > \delta,
		\end{equation}
         then go to step 2a; else, go to step 2b;
         \begin{itemize}
         \item[2a)]
        (null update)
let 
	    $\Gamma_{j+1} \in \bConv{n}$ denote the output $\Gamma^+$ of GBUS with
     input $(\lam,\tau,h)$,
	    $g=
     f_m(\cdot;\hat x_{k-1})$,
     and $(x^c,x,\Gamma) = (\hat x_{k-1}, x_j, \Gamma_{j})$,
     and go to step 3;
         \item[2b)] (serious update)
         set $\hat x_k=x_j$,
         $\hat y_k = y_j$, $\hat \Gamma_k(\cdot) = \Gamma_j(\cdot)$,
         and
         \begin{equation}\label{def:vk}
\hat v_{k} = \frac{1}{\lambda} (\hat x_{k-1} - \hat x_{k}),
\end{equation}
   and      compute
         \begin{equation}\label{def:wk}
\hat w_k = \hat v_{k} - m(\hat y_{k} - \hat x_{k-1}), \quad 
\hat \varepsilon_k =\phi_m (\hat y_{k};\hat x_{k-1}) -\hat \Gamma_{k}(\hat x_{k}) - \inner{\hat v_{k}}{\hat y_{k}-\hat x_{k}}
\end{equation}
 if $\|\hat w_k\| \le \bar \eta$ and
 $\hat \varepsilon_k \le \bar \varepsilon$, then stop;
 else 
 % set
 %  $$
 %        % ?????? \hat f_k(\cdot):=f_m(\cdot;\hat x_{k}) = ?????, \quad
 %    \ell_k(\cdot) := \ell(\cdot;\hat x_k,(f,\hat x_k,m))
 %    = \ell_f(\cdot;\hat x_k)$$
 %    where $\ell(\cdot;\cdot,\cdot)$ is defined in ??? and
         find function $\Gamma_{j+1}(\cdot)$
         such that
    
    \begin{equation}
         % \ell_{\hat f_k}(\cdot;\hat x_{k}) 
    \Gamma_{j+1} \in \bConv{n}, \qquad \ell_f(\cdot;\hat x_k)+ h(\cdot) \le
          \Gamma_{j+1}(\cdot) \le
          % (\hat f_k
          \phi_m(\cdot;\hat x_k)
\label{eq:serious}
    \end{equation}   
    where $\ell_f(\cdot;\cdot)$ is defined in \eqref{def:l_g}, set $k \leftarrow k+1$, and go to step 3;
        
\end{itemize}
% \item[3.]	compute
% \[
% w_k = ?????, \quad 
% \varepsilon_k = ???
% \]
%  if $\|w_k\| \le \bar \eta$ and
%  $\varepsilon_k \le \bar \varepsilon$, then stop;
%  else go step 4;
	    
	   % \in {\cal C}_{\phi_m(\hat y_{k+1};\hat x_k)}(\Gamma_j, x_{j}^c, \lambda, \tau)$;

    
	\item[3.]	set $j\leftarrow j+1$, and go to step 1. 
  \end{itemize}
\rule[0.5ex]{1\columnwidth}{1pt}

An iteration $j$ such that $t_j \le \delta$
     is called
	a serious iteration;
 % in which
	% case $x_j$ (resp., $y_j$) is called a serious iterate
	% (resp., auxiliary serious iterate????????);
	otherwise, $j$ is called a null iteration.
	% Moreover, we assume throughout our presentation that
	% $j=0$ is also a serious iteration.
 Let $
 % 0=j_0 \le 
 j_1 \le j_2 \le \ldots$ denote the sequence of all serious iterations and
 let $j_0:=0$.
 Define the $k$-th cycle ${\cal C}_k$ to be the iterations $j$ such that $j_{k-1}+1 \le j \le j_k$, i.e.,
 \begin{equation}\label{def:Ck}
     {\cal C}_k := \{ i_k,\ldots,j_k\}, \quad i_k := j_{k-1}+1.
 \end{equation}
 Hence, only the last 
 iteration of a cycle (which can be the first one
 if ${\cal C}_k$ contains only one iteration)  
 is serious. 
 
 
 We now make some basic remarks about PBF.
 First, PBF is referred to as a framework since it does not completely
	specify the details of
how GBUS is implemented nor how
 $\Gamma_{j+1}$ in \eqref{eq:serious} is chosen.
 % and
 % to 
 % how some algorithmic quantities are generated.
	% The framework rather gives minimal  conditions on these quantities
	% which enables us to establish complexity bounds for all specific instances of it
 % in a unified manner.
    % Second, two possible ways of choosing the bundle function $\Gamma_{j+1}$ in a serious update are: 1) $\Gamma_{j+1}=\ell_f(\cdot;x_j)+h$, and 2) $\Gamma_{j+1}=\max\{\Gamma_j, \ell_f(\cdot;x_j)+h\}$.
    Second, in view of \eqref{eq:serious} or the fact that the output of GBUS satisfies \eqref{def:Gamma}, it follows that
    % for every $j \ge 1$, we have
    \begin{equation}\label{phi-property}
        \Gamma_j \le \phi_m (\cdot;\hat x_{k-1}),
        %????? := f_m(\cdot;\hat x_{k-1}) + h, 
        \qquad
    \Gamma_j \in \bConv{n}, \qquad \forall j \in {\cal C}_k, \ k \ge 1.
    \end{equation}
	% Fourth, schemes (E1)-(E3)
 % in the previous subsection provide three possible concrete ways of implementing
 % the GBUS blackbox in step 1. 
 Third, it is shown in
 Lemma \ref{lemma:optimality} that $\hat w_k \in \partial_{\hat \varepsilon_k} \left[\phi_m(\cdot;\hat y_k) \right]  (\hat y_k)$ for every $k \ge 1$. Hence, $\hat y_k$ is 
 $(\bar \eta,\bar \varepsilon;m)$-regularized stationary point of $\phi$ whenever
 the stopping criterion
 $\|\hat w_k\| \le \bar \eta$ and
 $\hat \varepsilon_k \le \bar \varepsilon$ is satisfied
 in step~2b.
 Fourth, in view of the definition of $ y_j $ in \eqref{def:txj}
	and the above relation, it then follows that
	\[
		y_j \in \Argmin \left\lbrace \phi_m(x;\hat x_{k-1}) :
		x \in \{ \hat y_{k-1},x_{i_k},\ldots,x_j\}
		\right\rbrace. 
	\]

 
 % , 
 % although GPB does not specify a termination criterion for the sake
	% of shortness, all iteration-complexity bounds established in this paper are
	% relative to the effort of obtaining a $(\bar \eta,\bar\varepsilon;m)$-stationary solution  of \eqref{eq:opt_problem}.
 
	% ?????? Finally, although iteration-complexity bounds for GPB can also be established for other termination criteria (see for example
	% Section 6 of \cite{liang2020proximal}),
	% we have omitted the details of
	% their derivation for the sake of
	% shortness.

 % We now make some observations about possible 
 Two simple ways of choosing the bundle function $\Gamma_{j+1}$ such that \eqref{eq:serious} holds
 are to set
 % in a serious update. 
 % Specifically,
 % two simple ways are
 % either 
$\Gamma_{j+1}=\ell_{f} (\cdot;\hat x_{k}) + h$ or $\Gamma_{j+1}=\max\{\tilde \Gamma_{j+1}, \ell_{f}(\cdot;\hat x_{k}) +h\}$, where
 \[
 \tilde \Gamma_{j+1} := \Gamma_j - m \inner{\hat x_k -\hat x_{k-1}}{\cdot-\hat x_{k}} - \frac{m}2 \|\hat x_{k}-\hat x_{k-1}\|^2.
 \]
 % \[
 % \tilde \Gamma_{j} = \Gamma_j - m \inner{\hat x_k -\hat x_{k-1}}{\cdot-\hat x_{k}} - \frac{m}2 \|\hat x_{k}-\hat x_{k-1}\|^2
 % \]
 Clearly, the first choice for $\Gamma_{j+1}$
 satisfies \eqref{eq:serious}.
 Moreover, using the fact that $\Gamma_j \le \phi_m(\cdot;\hat x_{k-1})$ and the definition of
 $\phi_m(\cdot;\hat x_{k-1})$ in \eqref{def:phim}, it is easy to see that
 $\tilde \Gamma_{j+1} \le \phi_m(\cdot;\hat x_{k})$, and hence that
 the second choice
 for $\Gamma_{j+1}$
 also satisfies
 \eqref{eq:serious}.
 % $\Gamma_{j+1} \le  \hat f_{k} +h$.
 % when $\Gamma_{j+1}$ is chosen according to 2).

 % ????????? Moreover, under the assumption that every call to GBUS during a null update is carried out using
 % (E2) (resp., (E3)), another
 % way to obtain $\Gamma_{j}$ during a serious update is  to also use update (E2) (resp., (E3)). In view of the observation in the second last paragraph in Subsection \ref{subsec:update}, it follows that the latter
 % way yields a bundle function $\Gamma_{j}$
 % satisfying \eqref{ineq:require}.

%  \[
%  \Gamma \le (f+h) + \frac{m}2 \|\cdot - a\|^2 
%  \]
%  \[
%   \Gamma - \left [ (f+h) + \frac{m}2 \|\cdot - b\|^2 \right ]
%   = 
%   \frac{m}2 \left( \|\cdot-a\|^2-\|\cdot - b\|^2 \right) = m \inner{a-b}{\cdot-b} + \frac{m}2 \|b-a\|^2
%  \]
%  \[
% \tilde  \Gamma := \Gamma - m \inner{a-b}{\cdot-b} - \frac{m}2 \|b-a\|^2 \le 
%  \left [ (f+h) + \frac{m}2 \|\cdot - b\|^2 \right ]
%  \]
%  \[
%  \Gamma^+ = \max \{\tilde \Gamma, \ell_{\hat f_k}+h \} \le f+\frac{m}2 \|\cdot - b\|^2 +h
%  \]
 

We now discuss the role played by the parameter $\tau$ of PBF.
	First, the scalar $\tau$ satisfying the first inequality in \eqref{rel:tau1} is only used in step~2a as input to GBUS to obtain $\Gamma_{j+1}$.
 Second,
even though the analysis of PBF
requires a parameter $\tau$ satisfying the first inequality in \eqref{rel:tau1}
(whose right-hand side depends on $M$ and $L$), the implementations of some specific instances of PBF do not require knowledge of such
$\tau$. 
For instance,
since the updates (E2) and (E3) in 
Subsection~\ref{sec:GBUS} do not depend on $\tau$,
the two PBF instances
based on these updates
do not depend
on $\tau$ either.
In summary, even though the analysis of PBF requires a $\tau$ satisfying \eqref{rel:tau1},
it contains important variants which do not
require knowledge of
such $\tau$, and hence of
parameters
$M$ and $L$ satisfying (A2).

It is worth noting however that
PBF still requires knowledge of a parameter $m$ as in Assumption (A1),
and hence is not a completely universal method for finding a stationary point of \eqref{eq:opt_problem}.
% it contains important variants which
% do not depend on the knowledge of parameters
% $M$ and $L$ satisfying (A2).

	% (Recall the meaning of 1C-PB, 2C-PB and MC-PB given in the sentence following (E3) in Section \ref{sec:intro}.)
	% Moreover, both of these variants
	% can be viewed as special cases
	% of GPB with $\tau$ satisfying the equality in \eqref{rel:tau1}
	% but since their implementation do not depend on $\tau$,
	% they do
	% not need to compute or estimate such $\tau$.
	% Third, the PBF instance
	% 1C-PB requires a scalar $\tau$ satisfying
 % \eqref{rel:tau1} since the
	% update (E1) depends on $\tau$ (see \eqref{eq:affine}). 
	% Finally, \eqref{rel:tau1} implies that $\tau$ has to be sufficiently close to one
 % which, in the context of
 % (E1), means that the new bundle $\Gamma_j$ is closer to $\Gamma_{j}$
 % than the new cut
 % $\ell_{\psi_k}(\cdot;x_{j}) + h(\cdot)$
 % in view of the nature of the one-cut scheme (E1) (see relation \eqref{eq:affine}).


% The $j$-th iteration of ??? is called serious if
% 	the inequality $t_{j} \le \bar \varepsilon/2$
%     in step 1 is satisfied;
%     otherwise is called a null one.
% Let $1=i_1 < i_2 < i_3 < \ldots$ denote the serious iterations of PBF
% and, for every $k\ge 1$,
% define the $k$-th cycle of
% PBF as
% \[
% {\cal C}_k := \{ i_k,\ldots,j_k\},
% \]
% where its last iteration index $j_k$ is defined as
% \[
% j_k:=i_{k+1}-1.
% \]
% Note that a cycle may contain only a single iteration, i.e.,
% $j_k=i_k$, and, in this case,
% $j_k$ is a serious iteration.
% On the other hand, if a cycle contains more than one iteration then $j_k$, as
% well as every iteration
% in ${\cal C}_k$ other than $i_k$, is necessarily a null iteration.

{\bf PPM Interpretation:} 
We now discuss how PBF can be interpreted as an inexact proximal
point method for solving \eqref{eq:opt_problem}.
First,
the iterations within the $k$-th cycle can
be interpreted as cutting plane
iterations applied to
the prox subproblem
\begin{equation}
%\bar \theta_k
\hat M^{\lam}(\hat x_{k-1}) = \min \left \{ %\psi^k(x) := 
%\phi_m(x;\hat{x}_{k-1}) + 
\phi(x) + \frac{1}{2 } 
\left( \frac1\lam + m \right)
%(\lam^{-1}+m) 
\|x - \hat x_{k-1}\|^2 : x \in \R^n \right \}.
\label{sub_pro}
\end{equation}
Second, the pair $(y_j,\Gamma_j)$ found in the serious (i.e., the last) iteration $j$ of the $k$-th cycle  approximately solves
\eqref{sub_pro}
% where $\lam'>0$ is such that
% $(\lam')^{-1}=\lam^{-1}+m$
according to \eqref{ineq:hpe1}, in which case
the point
$x_j$ as in \eqref{def:xj} becomes the center
$\hat x_k$ for the next cycle.

We now discuss how the inexact criterion \eqref{ineq:hpe1}
can be interpreted in
terms of the prox subproblem \eqref{sub_pro}.
Since $\Gamma_{j} \le \phi_m(\cdot;\hat x_{k-1})$, it follows from the definitions of $\theta_{j}$, $\hat M^{\lam}(\hat x_{k-1})$, and $\phi_m(\cdot;\hat x_{k-1})$ in \eqref{def:thetaj},  \eqref{sub_pro}, and \eqref{def:phim}, respectively,
that $\theta_{j} \le \hat M^{\lam}(\hat x_{k-1})$, and hence that
 % \[
 %  m_j \le m_j^* \le \phi(y_j)+\frac1{2\lam}\|y_j-x_j^c\|^2
 % \]
 \begin{align}\label{ineq:tilde-tj}
     0 &\le 
     %\phi_m(y_{j};\hat x_{k-1})
     \phi(y_j) 
     +\frac12 \left(\frac1{\lam}+m\right)\|y_{j}-\hat x_{k-1}\|^2 - \hat M^{\lam}(\hat x_{k-1}) \nn \\
     &\le 
     %\phi_m(y_{j};\hat x_{k-1})
     \phi(y_j) 
     +\frac12\left(\frac1{\lam}+m\right)\|y_{j}-\hat x_{k-1}\|^2 - \theta_{j}
  = t_{j} + \frac{1-\chi}{2\lam} \|y_{j}-\hat x_{k-1}\|^2,
  % =: \tilde t_j
 \end{align}
 where the last identity follows
 from the definitions of
 $t_j$ in \eqref{ineq:hpe1}.
Hence, if $j$ is an iteration 
for which 
\eqref{ineq:hpe1} does not hold  (i.e., a serious one), then it follows from \eqref{ineq:tilde-tj} that
$y_j$ is a $\delta_j$-solution of \eqref{sub_pro} where
\[
\delta_j := \delta + \frac{1-\chi}{2\lam} \|y_{j}-\hat x_{k-1}\|^2.
\]
Clearly, the smaller $\chi \in (0,1]$ is,
the looser \eqref{ineq:tilde-tj} becomes. However,
our analysis only holds
under the condition that
the second (strict) inequality in \eqref{rel:tau1}
holds. On the other hand, our analysis also requires
the left hand side of this strict inequality to be bounded away from zero. Hence a convenient way of choosing 
$\chi$ is as
\[
\chi= \frac{1+ \gamma}{2+m\lam}
\]
for some $\gamma \in (0,1+m\lam]$
such that $\gamma^{-1} = {\cal O}(1)$.\\

We now state the main complexity result for PBF.

\begin{theorem}[Main Theorem] \label{thm:main1}
Given $(\bar \eta,\bar \varepsilon) \in \R^2_{++}$, define 
\begin{equation}\label{def:delta}
\delta =\delta(\bar \eta,\bar \varepsilon) := \min\left\{\frac{\bar \varepsilon \alpha}{2\left(\alpha+(1-\chi)(3+m\lam)\right)},\frac{\lam \bar \eta^2}{8+N(3+m\lam)}\right\},
\end{equation}
\begin{equation}
N := \frac{8 \left[1 - \chi  +  ( m\lam+1)^2\right]  }{\alpha}, \quad
\alpha := \chi(2+m\lam ) -1 >0,
\label{def:N}
\end{equation}
and let $\tau>0$
be such that
\begin{equation}\label{eq:equaltau}
     \frac{\tau}{1-\tau} = \lambda  \left(\frac{4M^2}{ \delta} + L + m \right ).
\end{equation}
Then,  PBF  with the above choice of $\delta$ and $\tau$
generates  in at most	
\begin{equation}\label{cmplx:total}
    \left[ \left\{1+ \lambda  \left(\frac{4 M^2}{ \delta} + L + m \right)\right\} \log^+\left( \frac{2\bar t}{\delta}\right) + 2 \right]\left[ (\hat M^{\lam}(\hat x_{0})- \phi^*) \max\left\{\frac{2(1-\chi)}{\alpha \bar \varepsilon},\frac{N}{\lam \bar \eta^2}\right\}+1 \right]
\end{equation}
total iterations an iterate within the sequence $\{\hat y_k\}$ (as in step 2b of PBF)
which is 
a $(\bar \eta,\bar\varepsilon;m)$-regularized stationary point
 of~$\phi$.
\end{theorem}

We now make two remarks about
Theorem \ref{thm:main1}.
First, the denominator of
\eqref{def:N} is positive in view of
the inequality in \eqref{rel:tau1}.
Second,
in terms of the tolerances
$\bar \eta$ and $\bar \varepsilon$ only, it follows
from Theorem \ref{thm:main1} that the
iteration complexity of
PBF to find a $(\bar \eta,\bar\varepsilon;m)$-regularized stationary point of $\phi$ is
$\tilde {\cal O}_1(\max \{ \bar \eta^{-4},\bar \varepsilon^{-2}\} )$.
Third, if $\chi=1$ then
\eqref{cmplx:total}
is $\tilde {\cal O}_1(\max\{ \bar \eta^{-4}, \bar \eta^{-2}  \bar \varepsilon^{-1}\} )$,
which is better than the 
previous estimate
when $\bar \varepsilon \ll \bar \eta^2$.

For the sake of stating an iteration-complexity for
PBF to find a $(\rho;1/m)$-Moreau stationary point,
we now consider the specific
case of PBF and Theorem \ref{thm:main1}
with 
parameters $\chi$ and $\lam$
chosen as
$\chi=1$ and $\lam = \Theta(m^{-1})$.
In this case, it follows that
\[
\alpha=1+ m\lam = \Theta(1), \quad
N= 8 (1+m\lam) = \Theta(1),
\]
and hence that
\begin{equation}\label{eq:simdelta}
\delta = {\cal O}\left( \min \left \{ \frac{\bar \eta^2}{m},  \bar \varepsilon \right \}  \right).
\end{equation}
Thus, the complexity bound
\eqref{cmplx:total} becomes
\begin{equation}\label{cmplx:total'}
   {\cal O} \left(  \left[ \left(\frac{M^2}{m\delta } + \frac{L}m + 1 \right) \log^+_1\left( \frac{\bar t}{\delta}\right)  + 1\right]\left[ \frac{\hat M^{\lam}(\hat x_0) - \phi^*}{\lam \bar \eta^2}      + 1\right] \right)
\end{equation}
which, upon disregarding %the logarithmic term and
all the ${\cal O}(1)$ and ${\cal O}(1/\delta)$ terms, becomes
\begin{equation}\label{cmplx:total''}
   \tilde {\cal O}_1 \left( \frac{\hat M^{\lam}(\hat x_0) - \phi^*}{\bar \eta^2} \left( M^2\max\left\{\frac{m }{\bar \eta^2 },\frac{ 1}{\bar \varepsilon}\right\} + L \right) \right).
\end{equation}

We are now ready to
state the iteration complexity
for PBF to find an approximate Moreau stationary point. 
\begin{corollary}
For any given $\rho>0$, PBF with $\chi = 1$, $\lam = 1/m$, and 
$(\bar \eta,\bar \varepsilon)$ given by
\[
\bar \eta = \frac{\rho}8, \quad \bar \varepsilon=  \frac{\rho^2}{2592m}, 
\]
finds in
\begin{equation}\label{new_comp}
    % \tilde {\cal O}_1 \left(  \frac{(\phi(\hat x_0) - \phi^*)M^2m}{\rho^4} + \frac{L(\phi(\hat x_0)- \phi^*)}{\rho^2} \right)
   \tilde {\cal O}_1 \left(  \frac{(\hat M^{1/m}(\hat x_0) - \phi^*)}{\rho^2} \left( \frac{M^2m}{\rho^2} + L \right)  \right) 
\end{equation}
total iterations an iterate within the sequence $\{\hat y_k\}$
which is a 
$(\rho;1/m)$-Moreau stationary point of $\phi$.
% \begin{equation}\label{eq:Moresta}
% \left\|\nabla M_\phi^{1 / (2m)}\left(x\right)\right\| \le \rho
% \end{equation}
\end{corollary}

\begin{proof}
Note that the choice of $(\bar \eta, \bar \varepsilon)$ shows that PBF generates a $(\rho;1/m)$-Moreau stationary point of $\phi$ in view of Proposition \ref{key:rela}(b). The choice of $(\bar \eta, \bar \varepsilon, \chi)  $ and relation \eqref{eq:simdelta} imply that 
\[
\delta = \Theta\left(\frac{\rho^2}{m}\right).
\]
The conclusion of the
corollary now follows
from
the above relation and \eqref{cmplx:total''}.
% then imply that the complexity for PBF to find a $(\rho;1/(2m))$-Moreau stationary point is at most \eqref{new_comp}.   
\end{proof}

% We now compare
% bound ??? with the iteration-complexity of Algorithm ????.
It is worth noting that the iteration-complexity of 
the PS method of Subsection \ref{SEC:SCS}
is not better than 
the one for
PBF with $\chi=1$, $\lam =1/m$ and $L=0$, namely,
bound
\eqref{new_comp}
with $L=0$,
and the first
bound in the right hand side of \eqref{eq:MoreauT} equals the latter one only when
\[
\gamma= {\Theta}\left(\frac1M\sqrt{\frac{\hat M^{1/m}(\hat x_0)-\phi^*}m} \right).
\]
However, this choice of $\gamma$ is generally not
computable
due to the fact that
 $\hat M^{1/m}(\hat x_0) - \phi^*$
and, most likely,
the parameter $M$ is not known. 
% It is also possible that this choice of $\gamma$ may even be outside the range constrain for $\gamma$ stated in Proposition \ref{prop:Davis}.







\vgap
\vgap



\section{Proof of Theorem \ref{thm:main1}}
\label{sec:proof}


This section contains three subsections.
The first one derives a preliminary bound on the length of each cycle in terms of the tolerance $\delta$.
The second one bounds the number of
cycles generated by PBF with a specific choice
of $\delta$ until it obtains
a $(\bar \eta,\bar \varepsilon;m)$-regularized stationary point of $\phi$.
Finally, the third one derives the total iteration complexity of PBF with the aforementioned choice of $\delta$
until it obtains
a $(\bar \eta,\bar \varepsilon;m)$-regularized stationary point of $\phi$.

\subsection{Bounding the cardinalities of the cycles}\label{subsec:length}

This section establishes a preliminary  upper bound on the cardinality of
each cycle ${\cal C}_k$
defined in \eqref{def:Ck}, and hence on
the number of null iterations
between two consecutive serious ones.

Throughout this section
and the next one, we let
\beq \label{eq:hatfk}
\hat f_{k}(\cdot) = f_m(\cdot;\hat x_{k-1}).
\eeq


	
	 The first result below presents a few basic properties of the null iterations between two consecutive serious ones.
	
	

	
	\begin{lemma}\label{lem:101}
	    For every $j \in {\cal C}_k \setminus \{j_k\}$ and
	    $u\in \dom h$ the following statements hold:
	 \begin{itemize}
	        \item[a)] 
         there exists $\bar \Gamma_{j} \in \bConv{n} $ such that $\bar \Gamma_{j} \le\hat f_{k} + h $ and
	    \begin{align}
	            &\tau \bar \Gamma_{j} + (1-\tau) [\ell_{\hat f_{k}}(\cdot;x_{j})+h] \le \Gamma_{j+1}, \label{eq:Gamma_j} \\
	            &\bar \Gamma_{j}(x_{j}) = \Gamma_{j}(x_{j}), \quad 
	        x_{j} = \underset{u\in \R^n}\argmin \left \{\bar \Gamma_{j} (u)  + \frac{1}{2\lambda} \|u-\hat x_{{k-1}}\|^2 \right\}; \label{eq:relation}
	    \end{align} 
	        \item[b)] there holds
         $$\bar \Gamma_{j}(u)  + \frac{1}{2\lambda} \|u-\hat x_{k-1}\|^2 \ge \theta_{j}+ \frac{1}{2\lambda}\|u-x_{j}\|^2.$$
	    \end{itemize}
	\end{lemma}
	\begin{proof}
	    a) This statement immediately follows from \eqref{def:Gamma}, \eqref{def:bar Gamma}, and the facts that iteration $j$ is a null one and $\Gamma_{j+1}$ is the output of the GBUS  with input $(\lam,\tau)$ and $(x^c,x,\Gamma) = (\hat x_{k-1}, x_j,\Gamma_j)$ (see the null update in step 2 of GPB) .  
	    
	   %  b) It follows from the fact that $\bar \Gamma_{j}\in \bConv{n}$  
	   % % ${\cal C}_\chi(\phi_m)$
	   %  that $\bar \Gamma_{j} + \frac{1}{2\lambda}\|\cdot-\hat x_{k-1}\|^2/2 $ is $\frac{1}{\lambda}$-strongly convex.
	    b) Using \eqref{eq:relation}
     and the fact that
     $\bar \Gamma_{j}\in \bConv{n}$, we have for every $u\in\dom h$,
	    \[
	    \bar \Gamma_{j}(u)  + \frac{1}{2\lambda} \|u-\hat x_{k-1}\|^2  \ge \bar \Gamma_{j}(x_{j}) + \frac{1}{2\lambda} \|x_{j}- \hat x_{k-1}\|^2  + \frac{1}{2\lambda} \|u-x_{j}\|^2 .
	    \]
	    The statement now follows from the above inequality, \eqref{eq:relation} and the definition of $\theta_j$ in \eqref{ineq:hpe1}.
	\end{proof}
	
	The following technical result provides an important recursive formula for $\{\theta_j\}$
	which is used in Lemma \ref{lem:tj} to give
	a recursive formula for $\{t_j\}$.
	It is worth observing that its proof
	uses for the first time the condition
	that
	$\tau$ is chosen to satisfy \eqref{rel:tau1}.

	
	\begin{lemma}\label{lem:recur}
		For every 
  % $\tau > 0$ satisfying \eqref{rel:tau1} and 
  $j\in {\cal C}_k\setminus\{j_k\}$, we have 
		\begin{equation}
	    \label{eq:recur}
		\theta_{j+1} \ge \tau \theta_{j} + (1-\tau) \left[ \ell_{\hat f_{k}}(x_{j+1};x_{j}) + h(x_{j+1}) +\frac{1}{2\lambda} \|x_{j+1}-\hat x_{k-1}\|^2 + \left( \frac{2 M^2}{\delta} + \frac{L + m}2 \right) \|x_{j+1}-x_{j}\|^2 \right]
		\end{equation}
  where $\hat f_k$ is as in \eqref{eq:hatfk}.
	\end{lemma}
    
	\begin{proof}
		Using \eqref{eq:Gamma_j} and Lemma \ref{lem:101}(b) with $u=x_{j+1}$, and the definitions of $\theta_{j+1}$ and $\Gamma_{j+1}$  in \eqref{ineq:hpe1} and \eqref{def:xj}, respectively, we have
		\begin{align*}
		& \theta_{j+1} 
		= \Gamma_{j+1}(x_{j+1}) +  \frac{1}{2\lambda} \|x_{j+1}-\hat x_{k-1}\|^2 \\
		&\ge (1-\tau) \left[ \ell_{\hat f_{k}}(x_{j+1};x_{j}) + h(x_{j+1})+\frac{1}{2\lambda} \|x_{j+1}- \hat x_{k-1}\|^2 \right]  + \tau \left( \bar \Gamma_{j}(x_{j+1}) + \frac{1}{2\lambda} \|x_{j+1}-\hat x_{k-1}\|^2 \right) \\
		& \ge (1-\tau) \left[ \ell_{\hat f_{k}}(x_{j+1};x_{j}) + h(x_{j+1})+\frac{1}{2\lambda} \|x_{j+1}- \hat x_{k-1}\|^2 \right] + \tau \left( \theta_{j} + \frac{1}{2 \lambda } \|x_{j+1} -x_{j}\|^2 \right)
		\end{align*}
		which, together with the definition of $\tau$ in \eqref{rel:tau1}, implies the conclusion of the lemma.
	\end{proof}
	
	The next result establishes
	a key recursive formula for $\{t_j\}$ which
	plays an important role in the analysis
	of the null iterates.
	
	\begin{lemma}\label{lem:tj}
	    For every  $j\in {\cal C}_k\setminus\{j_k\}$, we have
	    \[
	    t_{j+1}-\frac\delta2 \le \tau \left(t_{j} - \frac\delta2\right) 
	    \]
	    where $t_{j}$ is as in \eqref{ineq:hpe1}.
	\end{lemma}
	
	\begin{proof}
	Applying \eqref{eq:fm-linear} with  $z = \hat x_{k-1}$ and $\tz = x_j$,
 % and $\cdot = x_{j+1}$, 
 together with the definition of $\hat f_k$ in \eqref{eq:hatfk}, we have
	\begin{equation}
    \label{eq:subeq}
    \hat f_{k}(x_{j+1}) - 2  M \|x_{j+1}-x_{j}\| \le \ell_{\hat f_{k}}(x_{j+1};x_{j}) + \frac{ L + 
    m}2 \|x_{j+1}-x_{j}\|^2
	\end{equation}
	This inequality,
relations \eqref{def:phim}, \eqref{eq:def-tildem} and \eqref{eq:hatfk}, Lemma \ref{lem:recur},  and the fact that   $\chi \le 1$, then imply
		\begin{align}
		 \theta_{j+1}
  -\tau \theta_j &\stackrel{\eqref{eq:recur}}\ge 
%  \tau \theta_{j} + 
  (1-\tau) \left[ \ell_{\hat f_k}(x_{j+1};x_{j})+h(x_{j+1}) + \frac{1}{2\lambda} \|x_{j+1}-\hat x_{k-1}\|^2 + \left( \frac{2 M^2}{\delta} + \frac{L + m}2 \right) \|x_{j+1}-x_{j}\|^2 \right] \nn \\
		 &\stackrel{\eqref{eq:subeq} ,  1 \ge \chi  } \ge %\tau \theta_{j}+ 
   (1-\tau) \left [
		 (\hat f_{k}+h)(x_{j+1}) - 2  M \|x_{j+1}-x_{j}\| + \frac{2 M^2}{\delta}  \|x_{j+1}-x_{j}\|^2
		  + \frac{\chi}{2\lambda} \|x_{j+1}-\hat x_{k-1}\|^2 \right ] \nn \\
		  &\ge 
 %   \tau \theta_{j} + 
    (1-\tau) \left [
		 (\hat f_{k}+h)(x_{j+1}) +  \frac{\chi}{2\lambda} \|x_{j+1}-\hat x_{k-1}\|^2 - \frac{\delta}2 
		  \right ] \nn \\
    & \stackrel{ \eqref{def:phim} , \eqref{eq:def-tildem},  \eqref{eq:hatfk}} = (1-\tau) \left [
		 \phi_{\tm}(x_{j+1};\hat x_{k-1})
   - \frac{\delta}2
		  \right ] \label{eq:long-ineq}
    \end{align}
where 
the third inequality 
is due to 
the inequality
		$ a^2-2ab \ge - b^2$ with $a=\sqrt{2} M\|x_{j+1}-x_j\|$ and $b=\delta/\sqrt{2}$.
  Hence, it follows from \eqref{ineq:hpe1}, \eqref{eq:long-ineq} and \eqref{def:txj} that
  \begin{align*}
     t_{j+1}-\tau t_j
    &\stackrel{ \eqref{ineq:hpe1}}= [ \phi_{\tm} (y_{j+1};\hat x_{k-1}) - \theta_{j+1}] -
     \tau [\phi_{\tm} (y_{j};\hat x_{k-1}) - \theta_{j}] \\
     &= [ \phi_{\tm} (y_{j+1};\hat x_{k-1}) - \tau \phi_{\tm} (y_{j};\hat x_{k-1})] - [ \theta_{j+1}-\tau \theta_j] \\
     &\stackrel{ \eqref{eq:long-ineq}}\le [ \phi_{\tm} (y_{j+1};\hat x_{k-1}) - \tau \phi_{\tm} (y_{j};\hat x_{k-1})] - (1-\tau) \left [
		 \phi_{\tm}(x_{j+1};\hat x_{k-1})
   - \frac{\delta}2 
		  \right ]
    \stackrel{\eqref{def:txj}} \le \frac{(1-\tau)\delta}2,
  \end{align*}
  and hence that the conclusion of the lemma holds.
	\end{proof}


	
    



  
	

    
    We are now ready to present the main result of this subsection where a bound on $|{\cal C}_k|$ is obtained in terms of $\tau$, $t_{i_k}$ and $\delta$.
    
    
	\begin{proposition}\label{prop:null-strong} 

		The set ${\cal C}_k$ is finite and 
	    \[
	         |{\cal C}_k| \le \frac{1}{1-\tau} \log^+\left( \frac{2  t_{i_k}}{\delta }\right) + 2
	    \]
	where  $\tau$ is as in step 0 of PBF.
	\end{proposition}
	\begin{proof}
	If $t_{i_k} \le \delta$, then $|{\cal C}_k| =1$ and the conclusion of the lemma trivially holds.
 Assume now that $t_{i_k}> \delta$
 and let $j \in {\cal C}_k$ be such that $t_j > \delta$. Then,
 using the inequality $\tau \le e^{\tau-1}$
	    and  Lemma \ref{lem:tj}, we conclude that 	
		\[
		\frac{\delta}2 < t_j - \frac{\delta}{2}\le
\tau^{j-i_k} \left( t_{i_k} - \frac{\delta}2 \right)  \le \tau^{j-i_k}t_{i_k} \le e^{(\tau-1)(j - i_k)}t_{i_k} . 
		\]
and hence that
  \[
  j-i_k \le \frac{1}{1-\tau} \log^+ 
  \left(\frac{2 t_{i_k}}{\delta} \right).
  \]
 This inequality together with 
		the definition of ${\cal C}_k$ 
immediately implies the conclusion of the proposition.
\end{proof}




\subsection{Bounding total number of cycles}\label{subsec:cycle}


Recall that $\hat x_k$, $\hat y_k$, $\hat \Gamma_k$ denote the
last $x_j$, $y_j$ and $\Gamma_j$
generated within a cycle, i.e., the ones with $j=j_k$.


The following technical result
provides the main properties of
the sequences $\{\hat x_k\}$, $\{\hat y_k\}$ and $\{\hat \Gamma_k\}$, which are sufficent
to establish a bound on
the total number of cycles.

\begin{lemma}\label{lem:basic_prop}
The  following statements hold
for every $k \ge 1$:
\begin{itemize}
    \item[a)] $\hat x_{k}$ is
 the optimal solution 
 % and
 %    optimal value 
    of
    \begin{equation}
        \label{eq:optmality}
        \min_{u \in \R^n}  \hat \Gamma_{k}(u)+\frac{1}{2\lambda} \| u - \hat x_{k-1}  \|^2;
        \end{equation}
        hence, if $\hat \theta_k$ denotes the optimal value of \eqref{eq:optmality}, then
        \begin{equation}\label{eq:hattheta}
\hat \theta_k = \hat \Gamma_k(\hat x_k) +\frac{1}{2\lambda} \| \hat x_k - \hat x_{k-1}  \|^2;
        \end{equation}
\item[b)] there hold
\begin{equation}
\hat \Gamma_{k}(\cdot) \in \bConv{n}, \quad \hat \Gamma_{k}(\cdot) \le \phi_m (\cdot;\hat x_{k-1})
\label{eq:basic}
\end{equation} and
        \begin{equation}
        \label{eq:control}
     \phi_{\tm} (\hat y_k;\hat x_{k-1}) 
        - \hat \theta_{k} \le \delta
        \end{equation}
        where $\tm$ is as in \eqref{eq:def-tildem}.
\end{itemize}
\end{lemma}
\begin{proof}
a) The definition of $ \hat x_k $ in step 2b of PBF  and \eqref{def:xj} imply that $\hat x_{k}$ is an  optimal solution
    of    \eqref{eq:optmality}.

b) 
 If $|C_k| = 1$, i.e., $j_k$ is also the first iteration of cycle $C_k$, then \eqref{eq:basic} follows from
\eqref{eq:serious}. If $|C_k| > 1$, then 
$\hat \Gamma_k = \Gamma_{j_k}$ is the output of
GBUS with $(g,h)=(f_m (\cdot;\hat x_{k-1}),h)$, and hence satisfies \eqref{eq:basic} in view of \eqref{def:Gamma}.
Moreover, \eqref{eq:control} follows from the logic of the prox-center update rule in step 2 of PBF (see \eqref{ineq:hpe1}).
\end{proof}


The following result presents an important inclusion involving $(\hat y_k, \hat w_k, \hat \varepsilon_k)$
which implies that $\hat y_k$ is 
a $(\|\hat w_k\|, \hat \varepsilon_k;m)$-regularized stationary point of $\phi$.


\begin{lemma}
\label{lemma:optimality}
For every $k \ge 1 $, the quantities
$\hat x_k$, $\hat y_k$, $\hat v_k$, $\hat w_k$ and $\hat \varepsilon_k$ as in step 2b of PBF
satisfy
        \begin{equation}
           \hat \varepsilon_{k} \ge 0, \quad
           \hat v_{k} \in \partial_{\hat \varepsilon_{k}} [\phi_m (\cdot;\hat x_{k-1})] (\hat y_{k}),
           \label{eq:key_inclusion}
        \end{equation}
        \begin{equation}
              \hat w_{k} \in  \partial_{\hat \varepsilon_{k}} [\phi_m(\cdot;\hat y_{k}) ](\hat y_{k}).
              \label{eq:goal}
        \end{equation}
\end{lemma}
\begin{proof}
Since $\hat x_k$ is an optimal solution of \eqref{eq:optmality} in view of  Lemma \ref{lem:basic_prop}(a),
using
 the optimality condition 
 for \eqref{eq:optmality}, the fact that
 $\hat \Gamma_k \in \bConv{n}$, and the definition of $\hat v_k$ in \eqref{def:vk}, we have
$\hat v_{k} \in \partial \hat \Gamma_{k}(\hat x_{k})$.
This conclusion, \eqref{eq:control}, the definition of subdifferential in \eqref{def:subdif}, 
and the definition of $\hat \varepsilon_{k}$ in \eqref{def:wk},
% in \eqref{def:var}, 
then imply that for every $u\in \dom h$,
\[
   \phi_m (u;\hat x_{k-1}) \ge  \hat \Gamma_{k}(u) \ge \hat \Gamma_{k}(\hat x_{k}) + \inner{\hat v_{k}}{u-\hat x_{k}} 
=\phi_m (\hat y_k;\hat x_{k-1}) + \inner{\hat v_{k}}{u-\hat y_{k}} - \hat \varepsilon_{k}
\]
% where the equality follows the definition
% of $\hat \varepsilon_{k}$ in \eqref{def:var}.
and hence that the inclusion in \eqref{eq:key_inclusion} holds.
The inequality in \eqref{eq:key_inclusion} follows from the above
inequality with $u=\hat y_{k}$.
Moreover,
the definition of $\hat w_{k}$ in \eqref{def:wk},
the inclusion in \eqref{eq:key_inclusion},
  and Lemma \ref{lem:chara_weakly} imply  that
\[
  \hat w_{k} \in \partial_{\hat \varepsilon_{k}} [\phi_m(\cdot;\hat x_{k-1}) ] (\hat y_{k})  - m(\hat y_{k} - \hat x_{k-1}) = \partial_{\hat \varepsilon_{k}} [\phi_m(\cdot;\hat y_{k}) ](\hat y_{k}),
\]
and hence that \eqref{eq:goal} holds.
\end{proof}

It follows from \eqref{eq:goal} that $\hat y_k$ is
a $(\|\hat w_k\|, \hat \varepsilon_k;m)$-regularized stationary point of $\phi$ where
the pair $(\hat w_k,\hat \varepsilon_k)$ can be easily computed according to step 2b of PBF.
Our remaining effort from now on will be to analyze the number of iterations it takes to obtain
an index $k$ such that
$\|\hat w_k\| \le \bar \eta$ and
$ \hat \varepsilon_k \le
\bar \varepsilon$,
and hence  
%the corresponding
% $\hat y_k$ is
a $(\bar \eta, \bar \varepsilon;m)$-regularized stationary point
$\hat y_k$ of $\phi$.


% ????????Note that \eqref{eq:goal} is our key inclusion. The remaining work of this section is to bound $\hat \varepsilon_{k}$ and $\hat w_{k}$ .

The purpose of the following three
results is to establish a recursive
formula (see Lemma \ref{lem:key_recursive} below) involving 
$\phi_{\tm}(\hat y_k;\hat x_{k-1})$,
a quantity which will be used as a potential in
the analysis of this subsection.
The two results preceding Lemma \ref{lem:key_recursive}
are technical ones that are needed
to prove the aforementioned lemma.

\begin{lemma}
 For every $k \ge 1$, the quantities $\hat x_k$, $\hat y_k$, $\hat w_k$ and $\hat \varepsilon_k$,
 as in step 2b of PBF, satisfy
    \begin{equation}
    \label{eq:key_est}
        \hat \varepsilon_{k} +
       \frac{1}{2\lambda}\|\hat y_{k} - \hat x_{k}\|^2  \le \delta +
     \frac{1 - \chi}{2\lam} \|\hat y_{k} - \hat x_{k-1}\|^2     
    \end{equation}
    and
    \begin{equation}
    \|\hat w_{k} \|^2 
 \le \frac{4\delta}{\lam} +  \frac{\alpha N }{4 \lam^2 }\|\hat y_{k} - \hat x_{k-1}\|^2  
 \label{eq:est_w}
    \end{equation}
    where $\alpha$ and $N$ are as in \eqref{def:N},
    and $\lam$, $\chi$ and $\delta$ are as in 
    step 0 of PBF.
    
\label{lemma:key_est}
\end{lemma}


\begin{proof}
Using both statements (a) and (b) of Lemma \ref{lem:basic_prop},
the definitions of $\hat \varepsilon_{k}$, $\tm$ and $\hat v_k$ in \eqref{def:wk}, \eqref{eq:def-tildem} and \eqref{def:vk}, respectively, we
conclude that
% \begin{equation}
%         \label{eq:control}
%      \hat f_k(\hat y_{k};\hat x_{k-1}) +\frac{\chi}{2\lam} \|\hat y_k-\hat x_{k-1}\|^2 
%         - \hat \theta_{k} \le \delta.
%         \end{equation}
\begin{align*}
    \hat \varepsilon_{k} &= \phi_m (\hat y_k;\hat x_{k-1})  - \hat \Gamma_{k}(\hat x_{k}) - \inner{\hat v_{k}}{\hat y_{k} - \hat x_{k}}  \\
    &\stackrel{\eqref{eq:control},\eqref{eq:def-tildem}}\le \delta  - \frac{\chi}{2\lam} \|\hat y_k-\hat x_{k-1}\|^2
    + \hat \theta_k - \hat \Gamma_{k}(\hat x_{k}) - \inner{\hat v_{k}}{\hat y_{k} - \hat x_{k}} \\
    &\stackrel{\eqref{eq:hattheta}}= \delta  - \frac{\chi}{2\lam} \|\hat y_k-\hat x_{k-1}\|^2
    + \frac{1}{2\lambda}\|\hat x_{k}-\hat x_{k-1}\|^2 - \inner{\hat v_{k}}{\hat y_{k} - \hat x_{k}} \\
    &= \delta  - \frac{\chi}{2\lam} \|\hat y_k-\hat x_{k-1}\|^2
    + \frac{1}{2\lam} \left( \|\hat x_{k}-\hat x_{k-1}\|^2 + 2 \inner{\hat x_{k}-\hat x_{k-1}}{\hat y_{k} - \hat x_{k}} \right) \\
    &= \delta  - \frac{\chi}{2\lam} \|\hat y_k-\hat x_{k-1}\|^2
    + \frac{1}{2\lam} \left(
    \|\hat y_{k} - \hat x_{k-1}\|^2 -
    \| \hat y_{k} - \hat x_{k}\|^2
     \right),
    \end{align*}
    and hence that \eqref{eq:key_est} holds. 
It follows from the definition of $\hat w_k$ in \eqref{def:wk} that
\[
\hat w_{k} = \hat v_{k} - m(\hat y_{k} - \hat x_{k-1}) = \frac{1}{\lambda}(\hat y_{k} - \hat x_{k}) - \left(m+\frac{1}{\lambda}\right)(\hat y_{k} - \hat x_{k-1})
\]
and hence that
\begin{align*}
    \|\hat w_{k} \|^2 &\le \frac{2}{\lam^2}\|\hat y_{k} - \hat x_{k}\|^2 + 2 \left( m+\frac{1}{\lambda} \right)^2\|\hat y_{k} - \hat x_{k-1}\|^2 \\&
 \le \frac{4\delta}{\lam} +  \frac{2(1 - \chi)}{\lam^2} \|\hat y_{k} - \hat x_{k-1}\|^2  + 2 \left( m+\frac{1}{\lambda} \right)^2\|\hat y_{k} - \hat x_{k-1}\|^2
\end{align*}
 where the first inequality is due to the relation $\|a+b\|^2 \le 2 \|a\|^2 + 2 \|b\|^2$ and the last inequality is due to~\eqref{eq:key_est}.
    The above inequality and the definitions of $\alpha$ and $N$ in \eqref{def:N} imply \eqref{eq:est_w}.
\end{proof}



\begin{lemma}\label{lem:bdd_dif}
Define $\hat y_0 = \hat x_0$.
Then,
for every  $k \ge 1$, 
we have
 \begin{equation}\label{eq:bdd_diff}
\phi_{\tm} (\hat y_{k};\hat x_{k-1})
    \le \delta+\phi_m (\hat y_{k-1} ;\hat x_{k-1})  + \frac1{2\lambda} \|\hat y_{k-1}-\hat    x_{k-1}\|^2 
\end{equation}
where $\tm$ is as in \eqref{eq:def-tildem}, and
$\{\hat y_{k}\}$ and $\{\hat x_{k}\}$ are as in step 2b of PBF.
\end{lemma}

\begin{proof}
% For the sake of this proof, 
Using the definitions
of $\phi_{\tm}(\cdot;\cdot)$ and
$\tm$ in \eqref{def:phim} and \eqref{eq:def-tildem}, respectively,
and statements (a) and (b) of Lemma \ref{lem:basic_prop},
we conclude that 
\begin{align*}
   \phi_{\tm} (\hat y_k;\hat x_{k-1}) - \delta &\le \hat \theta_k
    = \hat \Gamma_{k}(\hat x_k) + \frac{1}{2\lambda} \|\hat x_k-\hat x_{k-1}\|^2  \\
    & \le \hat \Gamma_{k}(\hat y_{k-1}) + \frac{1}{2\lambda} \|\hat y_{k-1}-\hat x_{k-1}\|^2  \\
    &\le \phi_m (\hat y_{k-1};\hat x_{k-1})  + \frac{1}{2\lambda} \|\hat y_{k-1}-\hat    x_{k-1}\|^2,
    \end{align*}
 which shows \eqref{eq:bdd_diff}.  
\end{proof}

\begin{lemma}
\label{lem:key_recursive}
For every $k \ge 1$, define
\begin{equation} \label{eq:def-tauk}
\Delta_k:=\phi_{\tm} (\hat y_{k};\hat x_{k-1}).
 %+ \frac{\chi}{2\lam}\|\hat y_{k}-\hat x_{k-1}\|^2
% \label{eq:def-tau}
% \end{equation}
\end{equation}
Then,  for every $k \ge 1$,
we have
\begin{equation}
\Delta_{k+1} + \frac{\alpha}{2\lam} \|\hat y_{k} - \hat x_{k-1}\|^2 \le \Delta_k + (2+m\lambda) \delta.
\label{eq:recursive}
\end{equation}
and 
\begin{align}
 \frac{\alpha}{2\lam}& \sum_{l=1}^k \|\hat y_{l}-\hat x_{l-1}\|^2  \le
  \hat M^{\lam}(\hat x_0) - \phi(\hat y_{k+1}) - \frac{1}{2}\left(m + \frac{\chi}{\lam}\right)\|\hat y_{k+1}-\hat x_k\|^2    + (3+m\lambda)k\delta  .
\label{eq:wvar_bdd}
\end{align}
\end{lemma}

\begin{proof}
% First note that the strict inequality in \eqref{eq:def-alpha} directly follows from the second inequality in \eqref{rel:tau1}.
    Now, using \eqref{def:N}, \eqref{eq:key_est}, \eqref{eq:bdd_diff}, and \eqref{eq:def-tauk}, and the definition of $\phi_m(\cdot;\cdot)$ in \eqref{def:phim}, 
we conclude that
\begin{align*}
\Delta_{k+1} 
   & \stackrel{\eqref{eq:bdd_diff}}{\le} \phi(\hat y_k)  + \frac{1+\lam m}{2\lam}\|\hat y_k-\hat    x_k\|^2
   + \delta \\
    & \stackrel{\eqref{eq:key_est}}\le  \phi(\hat y_k)  +  (1+m\lambda) \left [ \delta + \frac{1-\chi}{2\lam}\|\hat y_k-\hat x_{k-1}\|^2 \right] + \delta \\
    &\stackrel{\eqref{eq:def-tauk}}= \left[ \Delta_k - \frac{\lam m+\chi}{2\lam} \|\hat y_{k}
    - \hat x_{k-1}\|^2  \right]
    +\frac{(1+m\lambda)(1-\chi)}{2\lam}
 \|\hat y_{k}
    - \hat x_{k-1}\|^2 
    + (2+m\lambda) \delta
    \\
    &\stackrel{\eqref{def:N}}= \Delta_k - \frac{\alpha}{2\lam} \|\hat y_{k} - \hat x_{k-1}\|^2+ (2+m\lambda) \delta,
\end{align*}
and hence that \eqref{eq:recursive} holds.
 Now,
adding \eqref{eq:recursive} from $k=1$ to $k=k$
we conclude that
\begin{align}
    \frac{\alpha}{2\lam} \sum_{l=1}^k \|\hat y_{l}-\hat x_{l-1}\|^2  &\le \Delta_1 - \Delta_{k+1}+
(2+m\lambda) \delta k \label{eq:tau1}
\end{align}
Using \eqref{eq:control} with $k=1$, the definition of $\Delta_1$ in \eqref{eq:def-tauk}, the definition of $\hat M^{\lam}(\hat x_0)$ in \eqref{eq:Moreau}, and the fact that $\hat \Gamma_1 (\cdot)\le \phi_m(\cdot;\hat x_0)$, we can conclude that
\[
\Delta_1 \le \hat \theta_1 + \delta \le \hat M^{\lam}(\hat x_0) +\delta.
\]
Inequality \eqref{eq:wvar_bdd} now follows from the above relation, \eqref{eq:tau1} and the definition of $\Delta_k$ in \eqref{eq:def-tauk}. 
\end{proof}


We are now ready to bound the total
number of cycles generated by PBF.


\begin{theorem}
For a given tolerance pair $(\bar\eta,\bar \varepsilon) \in \R^2_{++}$, define
\begin{equation}\label{def:K}
K = K(\bar \eta,\bar\varepsilon) := \left\lceil (\hat M^\lam(\hat x_0) - \phi^*) \max\left\{\frac{2(1-\chi)}{\alpha \bar \varepsilon},\frac{N}{\lam \bar \eta^2}\right\} \right\rceil
\end{equation}
where $\hat M^\lam(\cdot)$ is as in \eqref{eq:Moreau}, and
$\alpha$ and $N$ are as in \eqref{def:N}.
Then, PBF with $\delta$
as in \eqref{def:delta}
generates an iteration
index $k \le K(\bar \eta,\bar \varepsilon)$
such that
\begin{equation}\label{ineq:termination}
    \|\hat w_k\| \le \bar \eta,
\quad \hat \varepsilon_k \le
\bar \varepsilon.
\end{equation}
As a consequence,
$\hat y_k$ is 
a $(\bar \eta,\bar\varepsilon;m)$-regularized stationary point  of problem \eqref{eq:opt_problem}.

\label{thm:outer}
\end{theorem}
\begin{proof}
If PBF stops for some $k' \le K$ iterations, then the stopping criterion in step 2b of PBF ensures that \eqref{ineq:termination} holds. Then, \eqref{eq:goal}, \eqref{ineq:termination},
and Definition \ref{def:appr} imply
that $\hat y_k$ is 
a $(\bar \eta,\bar\varepsilon;m)$-regularized stationary point  of problem \eqref{eq:opt_problem}. Now assume PBF runs for $k'\ge K $ iterations.
Using relation \eqref{eq:wvar_bdd} with $k = K$ and the fact that
(A3) implies that $\phi(\hat y_{K+1})\ge \phi^*$, we have
\begin{equation}\label{eq:sum_bdd}
 \frac{\alpha}{2\lam} \sum_{l=1}^K\|\hat y_l - \hat x_{l-1}\|^2 \le \hat M^{\lam}(\hat x_0) - \phi^*   + (3+m\lambda)K \delta.
\end{equation}
Let $k \in \{1,\ldots,K\}$ be such that
\begin{equation}
    \|\hat y_k - \hat x_{k-1}\| = \min_{1\le l\le K} \|\hat y_l - \hat x_{l-1}\|.
\end{equation}
 The above relation and  \eqref{eq:sum_bdd} thus imply that 
\begin{equation}\label{eq:minindex}
\frac{\alpha}{2\lam} \|\hat y_k - \hat x_{k-1}\|^2 \le \frac{ \hat M^{\lam}(\hat x_0)-\phi^*}{K}+ (3+m\lam)\delta.
\end{equation}
This inequality together with \eqref{eq:key_est} imply that 
\begin{align}
\hat \varepsilon_k &\le  \delta + \frac{1-\chi}{\alpha}\left [  \frac{\hat M^{\lam}(\hat x_0))-\phi^*}{K}+ (3+m\lam)\delta \right] \nonumber\\ 
&  = \left(1+\frac{(1-\chi)(3+m\lam)}{\alpha}\right)\delta + \frac{(1-\chi)(\phi(\hat x_0)-\phi^*)}{\alpha K} \le \bar \varepsilon \label{eq:var}
\end{align}
where the last inequality is due to the definition of $\delta$ and $K$ in \eqref{def:delta} and \eqref{def:K}, respectively.
Following the similar reason, inequalities \eqref{eq:minindex} and \eqref{eq:est_w} imply that
\begin{align}
    \|\hat w_{k} \|^2 
 &\le \frac{4\delta}{\lam} +  \frac{\alpha N }{4 \lam^2 }\|\hat y_{k} - \hat x_{k-1}\|^2 \nonumber \\
 &\le \frac{4\delta}{\lam} + \frac{N}{2\lam}\left[ \frac{ \hat M^{\lam}(\hat x_0)-\phi^*}{K}+ (3+m\lam)\delta \right] \nonumber\\
 & = \frac{8+N(3+m\lam)}{2\lam}\delta + \frac{N( \hat M^{\lam}(\hat x_0)-\phi^*)}{2\lam K} \le \bar \eta^2 \label{eq:w}
\end{align}
where the last inequality is due to the definition of $\delta$ and $K$ in \eqref{def:delta} and \eqref{def:K}, respectively.
 Finally, \eqref{eq:goal}, \eqref{eq:var}, \eqref{eq:w},
and Definition \ref{def:appr} then imply
that $\hat y_k$ is a
$(\bar \eta,\bar\varepsilon;m)$-regularized stationary point  of problem \eqref{eq:opt_problem}.
\end{proof}

Before ending this subsection, we
observe that
the quantity 
$ \hat M^{\lam}(\hat x_0) - \phi^*$ in \eqref{def:K} can be majorized by
the more standard
initial primal gap $\phi(\hat x_0) -\phi^*$ due to
the definition
of $ \hat M^{\lam}(\cdot)$ in \eqref{eq:Moreau}.

% above 
% Then the above relation with $k = K $ ,  the definition of $K$ and $\delta$ imply
%  \begin{align*}
%     \min_{1\le k \le K} \left\lbrace \left(1+m\lam + \frac{4}{\lam}\right)\hat \varepsilon_k +  \|\hat w_k\|^2 \right\rbrace
%     &\le \frac{\phi(\hat y_0)-\phi^*}{\lam N K} + \left(\frac{4  }{\lam} + \frac{3+m\lam}{\lam N}\right) \delta\\
%     &\le \min\{(1+m\lam 
%     + \frac{4}{\lam})\bar 
% \varepsilon,  \bar \eta^2\}
%  \end{align*}
%  and the conclusion of the theorem thus follows.



% The number of serious iterations $K$ performed by GPB until it obtains for the
% 	    first time an auxiliary serious iterate $\hat y_K$ such that $ \hat y_K $ is a $\eta$-approximate solution is bounded by
% 				\begin{equation}\label{bound:outer}
% 				    \min \left\lbrace \frac{d_0^2}{\lam \bar \varepsilon}, \frac{1}{\chi \lam_\chi} \log\left(  \frac{\chi d_0^2}{\bar \varepsilon}+1 \right) \right\rbrace + 1
% 				\end{equation}



% \begin{theorem}
% Define $T = \max\{m\sqrt{\frac{2 }{m - \bar m}} + \sqrt{\frac{2}{\lambda}}+ (m+\frac{1}{\lambda}) \sqrt{  2\lambda(\phi(\hat y_0) - \phi^*)+  2\lambda(2 + m\lambda) }, \sqrt{\frac{2}{m - \bar m}\}}$. Let $\delta = {\cal O}(\frac{\eta^2}{T^2})$ and $K ={\cal O}(\frac{1}{\delta})$ , then we can get a $(\eta,\eta)$-approximate solution $\hat y$, i.e. there exists a $y^*$ such that  $\|\hat{y}-y^*\| \leq \eta $ and $y^*$ is a $\eta$- approximate solution, i.e. there exists a $w^*$ such that 
% $w^* \in \partial \phi(y^*)$ and $\|w^*\| \le \eta$.
% \end{theorem}
% \begin{proof}
% Assume $T = \argmin_{0\le l \le k} \{ \| \hat w_{l
% +1 } \| \} $. 
% % Apply Lemma \ref{lemma:Pass_Opt} with $x = \hat x_T$ $ y = \hat y_{T+1}$, $u = \hat v_{T+1}$, $\varepsilon = \hat \varepsilon_{T+1}$,  and $\chi = m - \bar m$ we get there exists a $y^*$ such that 
% % \[
% % y^*:=\underset{x^{\prime}}{\operatorname{argmin}}\left\{\phi\left(x^{\prime}\right)+\frac{m}{2 }\left\|x^{\prime}-\hat x_T\right\|^{2}-\left\langle \hat v_{T+1}, x^{\prime}\right\rangle\right\}
% % \]
% % and
% % \[
% % \|\hat y_{T+1} - y^*\| \le \sqrt{\frac{2 \hat \varepsilon_{T+1}}{m - \bar m }}
% % \]
% % Thus we have
% % \[
% % \|\hat y_{T+1} - y^*\| \le \sqrt{\frac{2 \hat \varepsilon_{T+1}}{m - \bar m }} \le \sqrt{\frac{2 \delta}{m - \bar m}} \le \eta
% % \]
% % and
% % \[\hat v_{T+1} \in \partial  \left(\phi+\frac{m}{2} \left\|\cdot-\hat x_T\right\|^{2}  \right)(y^*)
% % \]
% % The above inclusion suggests together with \ref{prop:equa} lemma that 
% % \[\hat v_{T+1} - m(y^* - \hat x_{T+1}) \in \partial\phi (y^*)
% % \]
% % Define $w^* = \hat v_{T+1} - m(y^* - \hat x_{T})$, then we have
% % \begin{align}
% % \|w^* \| 
% % &\le \|w^* - \hat w_{T+1}\| + \|\hat w_{T+1}\|\\
% % &\le m\|\hat y_{T+1} - y^*\| + \|\hat w_{T+1}\|\\
% % &\le m\|\hat y_{T+1} - y^*\| + \sqrt{\frac{2  \delta}{\lambda}} + (m+\frac{1}{\lambda}) \sqrt{  \frac{2\lambda(\phi(\hat y_0) - \phi^*)}{(k+1)}+  2\lambda(2 + m\lambda) \delta }\\
% % &\le \left ( m\sqrt{\frac{2 }{m - \bar m}} + \sqrt{\frac{2}{\lambda}}+ (m+\frac{1}{\lambda}) \sqrt{  2\lambda(\phi(\hat y_0) - \phi^*)+  2\lambda(2 + m\lambda) }  \right) \sqrt \delta\\
% % &\le \eta
% % \end{align}
% % This means that $y_{T+1}$ is our desired $(\eta,\eta)$-approximate solution.
% Use Lemma \ref{lemma:Pass_Opt} with $h = \phi +  \frac{m}{2} \|\cdot - \hat y_{T+1}\|$, $x = \hat y_{T+1}$, $ y = \hat y_{T+1}$, $u = \hat w_{T+1}$, $\varepsilon = \hat \varepsilon_{T+1}$,  and $\chi = \lam$ to conclude that
% \[
% \hat w_{k+1} \in  \partial_{\hat \varepsilon_{k+1}} \left(\phi  + \frac{ m+\lam^{-1}}{2} \|\cdot-\hat y_{k+1}\|^2 \right)(\hat y_{k+1})
% = \partial_{\hat \varepsilon_{k+1}} \left( \tilde\hat f_k_{k+1}  + \frac{1}{2\lam} \|\cdot-\hat y_{k+1}\|^2 \right)(\hat y_{k+1})
% \]
% Hence, by the lemma in the appendix,
% there exists $\hat z_{k+1}$ such that
% \[
% \|\hat y_{k+1}- \hat z_{k+1}\| \le \sqrt{2\lam \varepsilon}, \quad
% \]
% and
% \[
% 0 \in \partial \left( \tilde\hat f_k_{k+1}  + \frac{1}{2\lam} \|\cdot-\hat y_{k+1}\|^2 - \inner{w_{k+1}}{\cdot} \right)(\hat z_{k+1})
% \]
% or
% \begin{align*}
%     \hat w_{k+1} \in \partial \tilde\hat f_k_{k+1} (\hat z_{k+1})
% + \frac1{\lam} (\hat z_{k+1} - \hat y_{k+1}) &= \partial \left( \phi + \frac{m}2 \|\cdot- \hat y_{k+1}\|^2 \right)  (\hat z_{k+1})
% +  \frac1{\lam} (\hat z_{k+1} - \hat y_{k+1}) \\
% & = \partial \phi(\hat z_{k+1}) +  \left( m + \frac1{\lam} \right) (\hat z_{k+1} - \hat y_{k+1})
% \end{align*}
% Hence
% \[
% \tilde w = \hat w_{k+1} -
%  \left( m + \frac1{\lam} \right) (\hat z_{k+1} - \hat y_{k+1})
%  \]
%  satifies
%  \[
%  \tilde w \in \partial \phi(\hat z_{k+1})
%  \]
%  \[
%  \| \tilde w\| \le \|\hat w\| +
%  \left( m + \frac1{\lam} \right)\sqrt{2\lam \varepsilon}
%  \]
 
% \[ 
%   \partial \phi(x) = \partial \left(\phi  + \frac{ m}2 \|\cdot-c\|^2 \right)(x) -  m(x-c)
%  \]
% \end{proof}

\subsection{Proof of Theorem \ref{thm:main1}}\label{subsec:proof}

Recall that Theorem \ref{thm:outer} bounds the
total number of cycles while
Proposition \ref{prop:null-strong} provides a
bound on the cardinality of every
cycle ${\cal C}_k$ in term of
$t_{i_k}$. The following result
refines the latter result by
providing a uniform bound on
$t_{i_k}$.

To simplify the statement
of the next result,
we introduce the
following constants:
\begin{equation}\label{label:def_zeta}
      \zeta := \left\{ \begin{array}{ll}
         \frac{1}{2(L+m)\lam} \quad & \mbox{if $\lam > \frac{1}{2(L+m)}$};\\
        1 & \mbox{if $\lam \le \frac{1}{2(L+m)}$},\end{array} \right. 
        \end{equation}
and
\begin{align}\label{beta}
  \beta_1 := 
  \left(m + \frac{2}{\zeta \lam}\right)\left(m+\frac{\chi}{\lam} \right)^{-1}
%   \frac{m+2 (\zeta \lam)^{-1}}{m+\chi \lam^{-1}} 
  > 1, \quad
  \beta_2 := \left( \frac{ L+m}{2}+1\right) 
  \zeta^{-2}\left(\frac{1}{4\zeta \lam}+\frac{m}{2} \right)^{-1}.
\end{align}




\begin{lemma}\label{lem:t1} 
     Let $\delta$ and $K$ be as in \eqref{def:delta} and \eqref{def:K}, respectively,
     and define
\begin{equation}\label{def:bar t}
		    \bar t:=  M^2 + \beta_2 \left\{ \beta_1[ \hat M^{\lam}(\hat x_0) - \phi^*] + \beta_1(3+m\lam)K\delta + 4\zeta \lam M^2 \right\} 
		\end{equation}
		where $\zeta$ and $N$ are as in \eqref{label:def_zeta} and \eqref{def:N}, respectively, and $\beta_1$ and $\gamma$ are as in \eqref{beta}. 
     Then,  $ t_{i_k}\le \bar t$ for every $1\le k \le K$.	
	\end{lemma}
	
	\begin{proof}
Applying \eqref{eq:fm-linear1} with  $z = \hat x_{k-1}$ , we have
	\begin{equation}
    \label{eq:subeq2}
     f_{m}(x_{i_k};\hat x_{k-1}) - \ell_{ f}(x_{i_k};\hat x_{k-1}) \le 2  M \|x_{i_k}-\hat x_{k-1}\|  + \frac{ L + 
    m}2 \|x_{i_k}-\hat x_{k-1}\|^2.
	\end{equation}
	   	 Using this inequality, \eqref{eq:serious}, the definition of $\phi_m$ and $\tm$ in \eqref{def:phim} and \eqref{eq:def-tildem} respectively, \eqref{def:txj} and \eqref{ineq:hpe1} with $j=i_k$, and the fact that $1 \ge \chi$,  we have
	    \begin{align}
	        t_{i_k} & \stackrel{\eqref{ineq:hpe1}}= \phi_{\tm}(y_{i_k};\hat x_{k-1}) 
	        -\theta_{i_k} \nonumber\\        &\stackrel{\eqref{def:thetaj}}=\phi_{\tm}(y_{i_k};\hat x_{k-1})  - \Gamma_{i_k}(x_{i_k}) - \frac{1}{2\lambda}\|x_{i_k}-\hat x_{k-1}\|^2 \nonumber\\
           &\stackrel{\eqref{def:txj},\eqref{eq:def-tildem}}\le \phi_{m}(x_{i_k};\hat x_{k-1})  - \Gamma_{i_k}(x_{i_k}) - \frac{1-\chi}{2\lambda}\|x_{i_k}-\hat x_{k-1}\|^2 \nonumber\\
	        &\stackrel{\eqref{eq:serious}} \le f_m(x_{i_k};\hat x_{k-1}) - \ell_{f}(x_{i_k};\hat x_{k-1}) \nonumber\\
	        & \stackrel{\eqref{eq:subeq2}}\le 2 M \|x_{i_k}-\hat x_{k-1}\| + \frac{ L+m }{2}\|x_{i_k}-\hat x_{k-1}\|^2 \nonumber\\
	        &\le  M^2 + \left( \frac{ L+m}{2}+1\right)\|x_{i_k}-\hat x_{k-1}\|^2,
	        \label{eq:im4}
	    \end{align}
	  where the  last inequality is due to the fact that $2ab \le a^2+b^2$ with $a = M$ and $b = \|x_{i_k}-\hat x_{k-1}\|$.
   
	  We will now  bound $\|x_{i_k}-\hat x_{k-1}\|^2$.
 It follows from the inequality in \eqref{eq:key_inclusion} and \eqref{eq:wvar_bdd}
 that
	  \[
	     0 \le \hat M^{\lam}(\hat x_0) - \phi(\hat y_{k}) - \frac{1}{2}\left(m + \frac{\chi}{\lam}\right)\|\hat y_{k}-\hat x_{k-1}\|^2    + (3+m\lambda)(k-1)\delta. 
	  \]
   Thus, using the definition of $\beta_1$ in \eqref{beta}
   and the fact that $k \le K$, we have
	  \begin{equation}
	      \phi(\hat y_k) - \phi^* + \frac{1}{2\beta_1}\left(m + \frac{2}{\zeta \lam}\right)\|\hat y_{k}-\hat x_{k-1}\|^2 \le \hat M^{\lam}(\hat x_0) - \phi^*      + (3+m\lambda)K\delta.
	      \label{eq:im2}
	  \end{equation}
	   This inequality, Lemma \ref{lem:bdd_smdist} with $(\Gamma,z_0,u) = (\Gamma_{i_k},\hat x_{k-1},\hat y_k)$,  and 
   the fact that $\phi \ge \phi^*$,  imply that
	  \begin{align}
	   \zeta^2\left(\frac{1}{4\zeta \lam}+\frac{m}{2} \right)  \|x_{i_k}- \hat x_{k-1} \|^2  - 4\zeta \lam M^2
	&\stackrel{\eqref{eq:uniform_bdd}}\le
	\phi(\hat y_k) - \phi ^ * + \frac12 \left(m+\frac{2}{\zeta \lam} \right) \|\hat y_k - \hat x_{k-1}\|^2  \nonumber \\
 &\stackrel{\eqref{eq:im2}} \le \beta_1 [ \hat M^{\lam}(\hat x_0)- \phi^*      + (3+m\lambda)K\delta] \nonumber.
 % &\stackrel{\eqref{eq:Kdelta}}\le \beta_1 \left[ \phi(\hat x_0) - \phi^* + \gamma(\phi(\hat x_0) - \phi^*) + (3+m\lam)\delta  \right].
	  \label{eq:im3}
	  \end{align}
	The statement now follows by combining \eqref{eq:im4} and the above inequality and using the definitions of $\beta_2$ and $\bar t$ in \eqref{beta} and \eqref{def:bar t}, respectively.	
 \end{proof}
 
Now we are ready to present the proof of Theorem \ref{thm:main1}.

\noindent
	{\bf Proof of Theorem \ref{thm:main1}}
Using Theorem \ref{thm:outer}, Lemma \ref{lem:t1}, and Proposition \ref{prop:null-strong}, we know that the total complexity for PBF 
to obtain a $(\bar \eta,\bar\varepsilon;m)$-regularized stationary point  is 
\[
 \left[ \frac{1}{1-\tau} \log^+\left( \frac{2  \bar t}{\delta }\right) + 2\right]\left[ (\hat M^{\lam}(\hat x_0)- \phi^*) \max\left\{\frac{2(1-\chi)}{\alpha \bar \varepsilon},\frac{N}{\lam \bar \eta^2}\right\}+1 \right].
\]
The conclusion of
the theorem now follows from the choice of $\tau$ in \eqref{eq:equaltau}. 
\QEDA

\section{Concluding remarks}
\label{sec:conclusion}

% This paper presents a proximal bundle framework for solving HWC-CO problem and establishes the first iteration-complexity for PB methods for solving ?????weakly convex problems. Instead of focusing on a specific bundle update scheme, PBF introduces GBUS which contains various well-known update schemes. PBF generates a sequence of iterates $\{\hat y_k\}$ with the corresponding residuals $(\hat w_k,\hat \varepsilon_k)$ such that $(x,w,\varepsilon)=(\hat y_k,\hat w_k,\hat \varepsilon_k)$
%  satisfies the inclusion in \eqref{eq:app_sol}.
% The iteration complexity is then established based on measuring the norm of $\hat w_k$ and $\hat \varepsilon_k$ simultaneously. ??? 

In this section, we provide some further remarks and directions for future research.

First, from the point of view of the sequences of
serious iterates $\{\hat x_k\}$ and $\{\hat y_k\}$,
the complexity result for
PBF is point-wise since it
is about a single iterate
from $\{\hat y_k\}$.
It would be also interesting to
establish an ergodic complexity result about a weighted average of
such sequence. 

Second, as already observed in the fourth paragraph following PBF, PBF
requires the knowledge of a
weakly convex parameter,
i.e., a scalar $m$ as in
Assumption (A1), and hence is not a universal method.
It would be interesting to develop an adaptive method
which do not require
a scalar $m$ as above, but instead generates adaptive estimates for it which possibly violate the condition on (A1).


% our stopping criterion
% is based on the last

% Next we describe some further observations of the paper.
% First, it is worth noting that our analysis does not assume the domain of $\phi$ is bounded and establish the point-wise convergence instead of ergodic convergence. Second, PBF is a non-descent type algorithm. 


% We finally discuss some possible directions for future investigation. 
% First, as already mentioned in the remarks following the description of  PBF in Subsection~\ref{subsec:update}, PBF requires knowledge of a weakly convex parameter $m$
% (i.e., one that satisfies Assumption ????).
% ???????  so it is interesting to see whether one can develop a variant of PBF that is totally universal in the sense that it does not depend on the knowledge of $m$.



\bibliographystyle{plain}
\bibliography{new_ref}



\appendix
\section{Technical results about subdifferentials}
\label{APP:sub}

% \begin{lemma}
% \label{def: Frechet Subdifferential}
%  Let $\phi: \mathbb{R}^{n} \rightarrow \mathbb{R} \cup\{\infty\}$ be a proper closed function. At any point $x \in \operatorname{dom} \phi$, we have
% \[
% \partial \phi(x)=\left\{v \in \mathbb{R}^{n} \mid \phi(y) \geq \phi(x)+\langle v, y-x\rangle+o(\|y-x\|), \quad \forall y \in \R^n\right\}.
% \]
% \end{lemma}
% \begin{proof}
% On one hand, if $v \in \partial \phi(x)$ \red{???}, then the fact that $\forall y \in \R^n$
% \[
%   \phi(y) \ge \phi(x) + \inner{v}{y-x} + \min\{0,\phi(y)-\phi(x)-\inner{v} { y-x}\}.
% \]
% implies that $\liminf_{y \rightarrow x} \frac{\min\{0,\phi(y)-\phi(x)-\inner{v} { y-x}\}}{\|y-x\|} \geq 0$. Thus we know  \red{???}
% \[
% \min\{0,\phi(y)-\phi(x)-\inner{v} { y-x}\} = o(\|y-x\|).
% \]
% We then conclude
% \[
% \partial \phi(x) \subset \left\{v \in \mathbb{R}^{n} \mid\left( \forall y \in \mathbb{R}^{n}\right) \phi(y) \geq \phi(x)+\langle v, y-x\rangle+o(\|y-x\|)\right\}.
% \]
% On the other hand, if $v \in \left\{v \in \mathbb{R}^{n} \mid\left( \forall y \in \mathbb{R}^{n}\right) \phi(y) \geq \phi(x)+\langle v, y-x\rangle+o(\|y-x\|)\right\}$, then 
% \[
% \liminf _{y \rightarrow x} \frac{\phi(y)-\phi(x)-\left\langle v, y-x\right\rangle}{\|y-x\|} \geq \liminf _{y \rightarrow x}\frac{o(\|y-x\|)}{\|y-x\|} = 0.
% \]
% Thus the statement follows.
% ({\bf Jiaming and Honghao})
% % In particular, let $\varepsilon = 0$ in (7) and (8) we have the second statement. So we can rewrite (7) as
% % % \[
% % % \partial_\varepsilon \phi(x) = \partial \phi(x) + \varepsilon \bar B.
% % % \]
% % % %\label{prop:Fre_sub}
% \end{proof}


This section presents two
technical results about
$\varepsilon$-subdifferentials
that will be used in our analysis.

The first result describes
a simple relationship 
involving $\varepsilon$-subdifferentials
of $\phi_m(\cdot;x)$
for different points $x$.
% The second result describes
% a simple relationship 
% involving $\varepsilon$-subdifferentials
% of $\phi_m(\cdot;x)$
% for different points $x$.
\begin{lemma}
If $\phi: \mathbb{R}^{n} \rightarrow \mathbb{R} \cup\{\infty\}$ is an $m$-weakly convex function, then
$\phi_m(\cdot;c)$ is convex for
every $c \in \R^n$.
Moreover,
for every $x \in \dom \phi$,
$c \in \R^n$, and
$\varepsilon \ge 0$, we have
\[
\partial_{\varepsilon} \left[\phi_m(\cdot;x) \right](x) = \partial_{\varepsilon} \left[\phi_m(\cdot;c) \right](x) -  m(x-c).
\]
\label{lem:chara_weakly}
\end{lemma}

\begin{proof}
% \[
% \phi_m(u;c)-\phi_m(x;c)
% =\phi_m(u) - \phi(x) + \frac{m}2 \|u-c\|^2 - \frac{m}2 \|x-c\|^2
% - \frac{m}2 \|u-x\|^2 
% = 
% =???? \phi_m(u;x)-\phi(x) - \inner{v}{u-x}
% \]
Let $x\in \dom \phi$, $c \in \R^n$, and
$\varepsilon \ge 0$ be given. Then,
using
the definition of $\phi_m(\cdot;\cdot)$
in \eqref{def:phim}, we easily see that
\[
\phi_m(u;c)-\phi_m(x;c) - \inner{v+m(x-c)}{u-x}
= \phi_m(u;x) - \phi(x)  - \inner{v}{u-x} \quad \forall  u, v \in \R^n.
\]
The result now follows from the above identity and the definition of the $\varepsilon$-subdifferential
in \eqref{def:subdif}.
\end{proof}

% ================


% Assume $v \in \partial_{\varepsilon} \left[\phi_m(\cdot;c) \right](x) -  m(x-c)$, i.e., $v+m(x-c) \in \partial_{\varepsilon} \left[\phi_m(\cdot;c) \right](x)$,
% then for any $u \in \R^n$, we have
% \[
% \phi_m(u;c)-\phi_m(x;c) \ge \inner{v+m(x-c)}{u-x} - \varepsilon
% \]
% Then rearranging the term we get
% \[
% \phi_m(u;x) \ge \phi(x) +  \inner{v}{u-x}- \varepsilon
% \]
% which implies $v \in \partial_{\varepsilon} \left(\phi_m(\cdot;x) \right)(x)$ and thus \[
% \partial_{\varepsilon} \left[\phi_m(\cdot;c) \right](x) -  m(x-c) \subset 
% \partial_{\varepsilon} \left[\phi_m(\cdot;x) \right](x).
% \]
% The conclusion then just follows from the fact that the above process can be reversed.


% \begin{corollary}
% \label{lam:validity}
% If $\phi: \mathbb{R}^{n} \rightarrow \mathbb{R} \cup\{\infty\}$ is an $m$-weakly convex function, then for any $x, c \in \R^n$, we have
% % \begin{equation}
% % \label{eq:equ_chara}
% % \partial \phi(x) = \partial \left(\phi_{  m}(\cdot;x) \right)(x).
% % \end{equation}
% \begin{equation}
% \label{eq:equ_chara}
% \partial \phi(x) = \partial \left[\phi_{  m}(\cdot;c) \right](x) - m (x-c).
% \end{equation}
% \label{coro:key}
% \end{corollary}


% \begin{proof}
%      Proposition \ref{prop:key_chara} \red{???} and  Lemma \ref{lem:chara_weakly} with $\varepsilon = 0$  suggest
%     \[
% \partial \phi(x)  =
% \partial \left[\phi_{ m}(\cdot;x) \right](x) = \partial \left[\phi_{ m}(\cdot;c) \right](x) -   m(x-c) .
% \]
% \end{proof}


% ??????? \textbf{Remark}
% From this lemma we can observe that  our new definition is invariant under the choice of $m$ and $c$ since the right hand side of \eqref{eq:equ_chara} does not depend on $m$ and $c$ explicitly. 
% \begin{proposition}
% If $\phi: \mathbb{R}^{n} \rightarrow \mathbb{R} \cup\{\infty\}$ is an $\bar m$-weakly convex function, then for every $m \ge \bar m$ it holds that:
% \[\partial_{\varepsilon} \left(\phi_{\bar m}(\cdot;x) \right)(x) + \bar B(0;2\sqrt{(m-\bar m)\varepsilon}) \supset \partial_{\varepsilon} \left(\phi_m (\cdot;x)\right)(x) \supset
% \partial_{\varepsilon} \left(\phi_{\bar m}(\cdot;x) \right)(x) 
% \]
% \end{proposition}
% \begin{proof}
% We first prove the first relation:
% \begin{align*}
% \partial_{\varepsilon} \left(\phi_m(\cdot;x)   \right)(x) &= {\rm co \,}
% \left\{ \bigcup_{\varepsilon'+ \varepsilon''\le \varepsilon}
% \left[ \partial_{\varepsilon'} \left(\phi_{\bar m} (\cdot;x)  \right)(x) +
% \partial_{\varepsilon''} \left( \frac{m-\bar m}2 \|\cdot-x\|^2 \right)(x) \right] \right\} \\ 
% &\subset \partial_{\varepsilon} \left(\phi_{\bar m}(\cdot;x)   \right)(x) + \bar B(0;2\sqrt{(m-\bar m)\varepsilon})
% \end{align*}
% Now the second relation follows from the fact that $\phi_m(\cdot;x) \ge \phi_{\bar m}(\cdot;x)$ and $\phi_{m}(x;x) = \phi_{\bar m}(x;x)$.
% \end{proof}

% \begin{corollary}
% Let $\phi(x): \mathbb{R}^{n} \rightarrow \mathbb{R} \cup\{\infty\}$ be a  convex function,  then we have 
% \begin{equation}
% \partial_\varepsilon \phi(x) = \bigcap_{t>0} \partial_{t \varepsilon} (\phi+I_{\bar B(x;t)})(x)
% \end{equation}
% where I is the indicator function.
% \end{corollary}
% \begin{proof}
% From the Remark we can have:
% \begin{align*}
%   \partial_\varepsilon \phi(x)
% &= \{ x^* : \phi(u)-\phi(x) \ge \inner{x^*}{u-x} - \varepsilon \|u-x\| \, \quad \forall u\} \\
% &= \bigcap_{t>0} \{ x^* : \phi(u)-\phi(x) \ge \inner{x^*}{u-x} - \varepsilon t , \, \|u-x\| =t \}  \\
% &= \bigcap_{t>0} \{ x^* : \phi(u)-\phi(x) \ge \inner{x^*}{u-x} - \varepsilon t , \, \|u-x\| \le t \} \\
% &= \bigcap_{t>0} \partial_{t \varepsilon} (\phi+I_{\bar B(x;t)})(x)
% \end{align*}
% \end{proof}


The second results describes the relationship between an $\varepsilon$-solution and its global minimizer for a strongly convex function.
\begin{lemma}
If $g$ is a closed $\mu$-strongly convex function and $y$ is an
$\varepsilon$-solution of $g$, i.e., $0 \in \partial_\varepsilon g(y) $, then
its global minimizer $\hat y$ satisfies
\begin{equation}\label{eq:B1}
0 \in \partial g(\hat y), \quad \|y-\hat y \| \le \sqrt{\frac{2\varepsilon}\mu}.
\end{equation}
\label{lem:mu convex}
\end{lemma}


\begin{proof}
The inclusion in \eqref{eq:B1} follows from the fact that
$\hat y$ is a global minimizer of $g$.
Since g is $\mu$-strongly convex and $\hat y$ is its global minimizer, we have 
\[
\frac{\mu}{2}\|y - \hat y\|^2 \le g(y) - g(\hat y) 
 \le \varepsilon
\]
where the second inequality is due to $0 \in \partial_\varepsilon g(y)$.
hence, the inequality in \eqref{eq:B1} follows.
\end{proof}


% \begin{lemma}
% Given two convex  functions $g$ and $h$ and a point $x\in \R^n$ such that $g \le h$ and $g(x) = h(x)$, then it holds that 
% \[
% \partial g(x) \subset \partial h(x).
% \]
% \end{lemma}


% \begin{lemma}
% if $g:\R^n \to \R^n$ is convex differentiable with $L$-Lipschitz continuous gradient and $v \in \partial_\varepsilon g(x)$ then
% \[
% \|v-\nabla g(x) \| \le 2 \sqrt{L\varepsilon}
% \]
% \end{lemma}

\section{Relationships between notions of stationary points}\label{App:relation}
This section contains three proofs for the results in Subsection~\ref{sec:notion}.

\noindent
	{\bf Proof of Proposition \ref{prop:relation}}
 For this proof only, 
we  let
\begin{equation}\label{def:psi}
    \psi(\cdot) := \phi(\cdot) + \frac{\lam^{-1}+m}{2} \|\cdot-x\|^2
\end{equation}
and denote $\hat x^\lam(x)$ as in \eqref{eq:hatx}. Observe that $\psi$ is a $(1/\lam)$-strongly convex function in view of the fact that $\phi$ is $m$-weakly convex. Thus, for any $u \in \dom \psi$, it holds that
\begin{equation}\label{eq:psimu}
  \psi(u) - \psi(\hat x) \ge \frac{1}{2\lam}\|u-\hat x\|^2.  
\end{equation}
We start with the proof of a).

   a) 
Since $x$ is a $(\varepsilon_D,\delta_D)$-directional stationary point, there exits a $\tilde x$ such that
\begin{equation}\label{eq:DtoM}
\norm{x-\tilde x} \le \delta_D \quad 
\inf_{\|d\|\le 1} \phi'(\tilde x;d) \ge -\varepsilon_D.
\end{equation}
Then for any $d \in \R^n$,
we have
\begin{align*}
   \psi'(\tilde x;d) =
\phi'(\tilde x;d) + \left(\frac{1}{\lam}+m\right)\inner{\tilde x-x}{d} \ge -\varepsilon_D \|d\| - \left(\frac{1}{\lam}+m\right)\delta_D \|d\|
= - \left[\varepsilon_D + \left(\frac{1}{\lam}+m\right)\delta_D \right] \|d\|.
\end{align*}
Using the convexity of $\psi$  and the above relation with $d = \hat x^\lam(x)- \tilde x $, we conclude  that
\begin{equation}\label{eq:sped}
\psi(\hat x) - \psi(\tilde x) \ge 
\psi'(\tilde x;\hat x-\tilde x)
\ge 
- \left[\varepsilon_D + \left(\frac{1}{\lam}+m\right)\delta_D \right] \|\hat x-\tilde x\|.
\end{equation}
Then using the above inequality and \eqref{eq:psimu} with $u = \tilde x$ we can conclude that
\[
 \varepsilon_D + \left(\frac{1}{\lam}+m\right) \delta_D  \ge \frac{1}{2\lam} \|\tilde x-\hat x\|.
\]
Thus \eqref{eq:DtoM} further implies that 
\[
\|x-\hat x\| \le \norm{x-\tilde x} + \norm{\tilde x-\hat x} 
\le \delta_D + 2\lam \left[\varepsilon_D+\left(\frac{1}{\lam}+m\right)\delta_D \right].
\] 
and hence using \eqref{eq:new_grad}  we can get
\[
\|\nabla \hat M^{\lam}(x)\| = \left(m+\frac{1}{\lam}\right)\|\hat x^\lam(x)- x\| \le \left(m+\frac{1}{\lam}\right)\left[(3+2\lam m)\delta_D + 2\lam \varepsilon_D \right].
\]
Now the statement follows from the above inequality and the definition of Moreau stationary point in Definition \ref{def:Moreau}.

b) Since $x$ is a $(\varepsilon_M;\lam)$-Moreau stationary point, \eqref{eq:new_grad}  thus imply that 
\[
\left(\frac{1}{\lam}+m\right)\|x- \hat x\| \le \varepsilon_M.
\]
Then for any $d \in \R^n$ such that $\|d\| \le 1$, we have
\begin{align*}
   0 \le \psi'(\hat x;d) =
\phi'(\hat x;d) + \left(\frac{1}{\lam}+m\right)\inner{\hat x-x}{d} \le 
\phi'(\hat x;d)
+ \left(\frac{1}{\lam}+m\right) \norm{x-\hat x} \le \phi'(\hat x;d)+ \varepsilon_M
\end{align*}
and
\[
 \|x-\hat x\| \le \frac{\varepsilon_M}{m+\frac{1}{\lam}}.
\]
Choose $\tilde x = \hat x$, $\varepsilon_D = \varepsilon_M$ and 
$\delta_D =\varepsilon_M/(m+1/\lam)$.
Then the statement follows from the above relation and the definition of $(\varepsilon_D,\delta_D)$-directional stationary point.
\QEDA

\vgap

\noindent
	{\bf Proof of Proposition \ref{prop:subdir}}
 Since $\psi$ in \eqref{def:psi} is convex, it follows from Theorem 3.26 of \cite{beck2017first} that
\begin{equation}\label{eq:directional}
    -\inf_{\|d\| \leq 1} \psi^{\prime}(x;d)
    = -\inf_{\|d\| \leq 1}\sigma_{\partial \psi(x)}(d)
    = - \inf_{\|d\| \leq 1} \sup_{s\in \partial \psi(x)}\inner{s}{d}
    = \inf_{s\in \partial \psi(x)}\|s\|
    =\operatorname{dist}(0 ; \partial \psi(x)).
\end{equation}
It follows from Proposition \ref{prop:Fre_sub} with $m=2m$ and the definition of $\psi$ in \eqref{def:psi} that
\[
\partial \phi(x) = \partial \psi(x).
\]
Using the above identity and \eqref{eq:directional}, we have
\[
\operatorname{dist}(0 ; \partial \phi(x)) = \operatorname{dist}(0 ; \partial \psi(x)) = -\inf _{\|d\| \leq 1} \psi^{\prime}(x; d) = -\inf _{\|d\| \leq 1} \phi^{\prime}(x;d)
\]
where the last identity is due to the definition of $\psi$ in \eqref{def:psi}.
Hence, the conclusion directly follows from Definition \ref{def:dsp}.
 \QEDA

\vgap
 
\noindent
	{\bf Proof of Proposition \ref{prop:ours}}
a)
Since $x$ is a
$(\bar \eta,\bar\varepsilon;m)$-regularized stationary point of $\phi$, 
there exists a pair $(w,\varepsilon) \in \R^n \times \R_{++}$ satisfying
\eqref{eq:app_sol}.
Using the fact that
$\phi_m(x;x) = \phi_{2m}(x;x)$ and
$\phi_m(\cdot;x) \le \phi_{2m}(\cdot;x)$, we easily see that
the inclusion in \eqref{eq:app_sol}
implies that
$w \in \partial_\varepsilon [\phi_{2m}(\cdot;x)](x)$.
Since  $\phi_{ m}$ is
convex in view of assumption (A1),
we easily see that
$\phi_{2m}(\cdot;x)$ is
$m$-strongly convex, and hence
the function $\phi_{2m}(\cdot;x)-\inner{w}{\cdot}$
has a global minimizer
$\tilde x$
which, in view of Lemma \ref{lem:mu convex} with $g=\phi_{2m}(\cdot;x)-\inner{w}{\cdot}$ and
$\mu=m $, satisfies
\begin{equation}
w \in \partial \left[ \phi_{2m}(\cdot;x) \right] (\tilde x), \quad \|x-\tilde x \| \le \sqrt{ \frac{2\varepsilon}{m} } \le \sqrt{ \frac{2 \bar \varepsilon}{m} }
\label{eq:nearby}
\end{equation}
where the last inequality is due to the
last inequality in \eqref{eq:app_sol}.
Now, letting
$\hat w := w - 2m (\tilde x-x)$, it follows from \eqref{eq:key_chara1} with $m = 2m$ and 
 Lemma \ref{lem:chara_weakly} with $(\varepsilon,x,c) = (0,\tilde x,x)$ that
\begin{equation}\label{eq:belong}
 \hat w \in \partial \left[ \phi_{2m}(\cdot;\tilde x) \right] (\tilde x)
= \partial \phi(\tilde x). 
\end{equation}
Moreover,
using the definition
of $\hat w$ and the triangle inequality, we
have
\[
\|\hat w\| \le \|w\| + 2m \|x-\tilde x\| \le
\bar \eta + 2 \sqrt{2m \bar \varepsilon} ,
\]
where the second inequality is due to the first inequality in
\eqref{eq:app_sol} and the inequality in \eqref{eq:nearby}. Then the above inequality and \eqref{eq:belong} imply that 
\begin{equation}\label{eq:tildedis}
\operatorname{dist}(0,\partial \phi(\tilde x)) \le \bar \eta + 2 \sqrt{2m \bar \varepsilon}.
\end{equation}
Now statement (a) follows from \eqref{eq:nearby}, \eqref{eq:tildedis}, and Proposition \ref{prop:subdir}.

b) Statement (b) immediately follows from the first statement and Proposition \ref{prop:relation}(a) with $\lam=1/m$.
\QEDA

% \begin{proof}
%     Denote $\hat x = \operatorname{prox}_{(1/2m)\phi}(x)$. Then from \eqref{eq:grad_M} with $\mu = 1/2m$ and the assumption that $x$ is a $(\varepsilon,1/(2m))$-Moreau stationary point, we know that
%    \begin{equation}\label{eq:norm_small}
%    2m\|x-\hat x\| \le \varepsilon.
%     \end{equation}
%    Since  $\phi_{ 2m}$ is
% convex in view of assumption (A1), using the optimality condition for minimizing $\phi_{2m}(\cdot;x)$ we have
% \[
% 0 \in   \partial \left[\phi_{2m}(\cdot;x) \right]  (\hat x ).
% \]
% Thus we know that
% \[
% 0 \in \partial_{\bar \varepsilon} [ \phi_{2m}(\cdot;x)](x)
% \]
% where 
% \[
%  \bar \varepsilon
% := \phi_{2m}(x;x) - \phi_{2m}(\hat x;x) =
% \phi(x) - \phi(\hat x) - m \|\hat x-x\|^2.
% \]
% Moreover, using the assumption that $\phi$ is $M$-Lipschitz, we can get
% \[
% \bar \varepsilon \le \phi(x) - \phi(\hat x)
% \le M \|\hat x-x\|.
% \]
% The statement now follows from the above inequality, \eqref{eq:norm_small} and the definition of $(0,M\varepsilon/(2m);2m)$-stationary point in Definition \ref{def:appr}.
% \end{proof}




	
	
	




\section{Technical results for the proof of theorem \ref{thm:main1}}
\label{APP:analysis}
The main result of this section is Lemma \ref{lem:bdd_smdist} which was used in the proof of Lemma \ref{lem:t1}. 
	
	Before stating and proving Lemma \ref{lem:bdd_smdist},
	we first present two technical results whose proof can be found in Appendix A of \cite{liang2021unified}.
	\begin{lemma}\label{lem:prox-dist}
	Let $z_0\in \R^n$, $0<\zeta \lam < \lam$ and $\Gamma\in \bConv{n}$ be given, and
	define
	\[
	    z_\lam=\underset{u\in \R^n}\argmin \left\lbrace \Gamma(u) +\frac{1}{2 \lam}\|u- z_0 \|^2 \right\rbrace,\quad
	    z_{\zeta \lam}=\underset{u\in \R^n}\argmin \left\lbrace \Gamma(u) +\frac{1}{2 \zeta \lam}\|u-z_0\|^2 \right\rbrace.
	\]
	Then, we have $\|z_\lam-z_0\|\le (\lam/\zeta \lam) \|z_{\tilde\lam}-z_0\| $.
	\end{lemma}


	
	\begin{lemma}\label{lem:descent}
% 	{\bf Talk about $h$}
	For some $(M,L) \in \R^2_+$, assume that $(z_0,\lam)\in \R^n \times (0,1/ L)$,
	function
	 $ \tilde f  \in 
	 \bConv{n} \cap {\cal C}(M,L)$ and
	 $\Gamma, h \in \bConv{n} $ are such that
	 \[
	 \ell_{\tilde f}(\cdot;z_0)+h \le \Gamma \le \tilde f+h.
	 \]
	 Moreover,
	define
	\begin{equation}
	    z_\lam:=\underset{u\in \R^n}\argmin \left\lbrace \Gamma(u) +\frac{1}{2 \lam}\|u- z_0 \|^2 \right\rbrace.
	   \label{eq:z_lam}
	\end{equation}
	Then, for every $u\in \dom h$, we have
	\begin{equation}\label{ineq:descent}
	    \frac{1}{2\lam} \|u-z_\lam\|^2 + (\tilde f+h)(z_\lam) - (\tilde f+h)(u) \le \frac{1}{2\lam} \|u-z_0\|^2 + \frac{2\lam M^2}{1-\lam L}.
	\end{equation}
	\end{lemma}
	
 
 
 \begin{lemma}\label{lem:bdd_smdist}
	For some $(M,L) \in \R^2_+$ and $m \ge 0$, assume that $(z_0,\lam)\in \R^n \times \R_{++}$,
	function
	 $f  \in {\cal C}(M,L)$ is $m$-weakly convex, and
	 $\Gamma, h \in \bConv{n} $ are such that
	 \[
	 \ell_{ f}(\cdot;z_0)+h \le \Gamma \le f_m(\cdot;z_0)+h,
	 \]
	where $f_m(\cdot;z_0)$ is as in \eqref{eq:z_lam}.
	Then,  for every $u \in \R^n$, we have
	\begin{equation}
	\zeta^2\left(\frac{1}{4\zeta \lam}+\frac{m}{2} \right)\|z_\lam - z_0 \|^2 
	\le
	(f+h)(u) - (f+h)(z_{\zeta\lam}) + \frac12 \left(m+\frac{2}{\zeta \lam} \right) \|u-z_0\|^2 + 4\zeta \lam M^2
% 	(\frac{1}{4 \lam}+\frac{m}{2})\|z - z_0\|^2 \le \phi_{m+\frac{2}{  \lam}}(u;z_0) - \phi^* + 4\lam M^2
\label{eq:uniform_bdd}
	\end{equation}
	where $z_\lam$ is as in \eqref{eq:z_lam} and $\zeta $ is defined as \eqref{label:def_zeta}.
\end{lemma}

 
	\begin{proof}
	We first prove the conclusion of the lemma under the assumption that
	$\lambda$ is such that
	$\lam \in (0, \frac{1}{2(L+m)}]$,
	and hence $\zeta=1$.
	Indeed, noting that
	$f_m(\cdot;z_0) \in \Conv{n} \cap {\cal C}(M,L+m)$ due to Lemma \ref{lem:Lip_ineq}, it follows from Lemma \ref{lem:descent} with
	$\tilde f = f_m(\cdot;z_0)$ and $(\Gamma,z_0,\lam) = (\Gamma, z_0, \lam)$  and the definition of $ \lam$ that
	for any $u \in \dom h$:
    \begin{align*}
	    \frac{1}{2 \lam} \|u- z_{\lam}\|^2 + (f_m(\cdot;z_0)+h)(z_{\lam}) - (f_m(\cdot;z_0)+h)(u) 
	    &\le \frac{1}{2 \lam}  \|u-z_0\|^2 + \frac{2  \lam  M^2}{1- \lam ( L +m)}\\
	    &\le \frac{1}{2  \lam}  \|u-z_0\|^2 + 4\lam M^2,
	\end{align*}
	and hence that
	\begin{align*}
	(f+h)(u) - (f+h)(z_{\lam}) &+ \frac{m}{2}\|u-z_0\|^2+4 \lam M^2
	\ge \frac{1}{2\lam} \left(\|u- z_{\lam}\|^2 - \|u-z_0\|^2 \right) + \frac{m}{2}\| z_{\lam} - z_0\|^2 \\
	&=  \frac{1}{2}(\frac{1}{ \lam} + m)\|z_{\lam}- z_0\|^2 -\frac{1}{\lam}\inner{z_{\lam} - z_0}{u - z_0} \\
	&\ge \frac{1}{2}\left(\frac{1}{ \lam} + m\right)\|z_{\lam} - z_0\|^2 -\frac{1}{2  \lam} \left(\frac{1}{2}\|z_{\lam} - z_0\|^2 + 2\|u - z_0\|^2 \right) 
	\end{align*}
	where the last inequality is due to the fact that
	$\frac{1}{2}a^2+2b^2 \ge 2ab$.
	Rearranging the above inequality, we then conclude that
	\begin{equation}
	   (f+h)(u) - (f+h)(z_{\lam}) + 4\lam M^2\ge  \frac{1}{2}\left(\frac{1}{ 2\lam} + m\right)\|z_{\lam} - z_0\|^2 - \left( \frac1{\lam} +\frac{m}2 \right)  \|u - z_0\|^2
	   \label{eq:small_lam}
	\end{equation}
	which, in view of the fact that
	$\zeta=1$, immediately implies \eqref{eq:uniform_bdd}.
	Next, we show that \eqref{eq:uniform_bdd} also holds for $\lam >1/[2(L+m)]$.
Noting that \eqref{eq:uniform_bdd} implies that $\zeta \lam = 1/[2(L+m)]$, it then follows from \eqref{eq:small_lam} with
$\lam = \zeta \lam$ that
	\begin{align*}
	    (f+h)(u) - (f+h)(z_{\zeta\lam}) + \left( \frac{m}2 +\frac{1}{\zeta \lam} \right)\|u -z_0\|^2+ 4 \zeta\lam M^2 &\ge
	    \left(\frac{1}{4 \zeta \lam}+\frac{m}{2} \right)\|z_{\zeta \lam} - z_0\|^2 \\
	    &\ge \zeta^2\left(\frac{1}{4 \zeta \lam}+\frac{m}{2} \right)\|z_{ \lam} - z_0\|^2
	\end{align*}
	where the last inequality is due to
% 		\left(\frac{1}{4 \zeta \lam}+\frac{m}{2} \right)\|z_{\zeta \lam} - z_0\|^2 \le (f+h)(u) - (f+h)(z_{\zeta\lam}) + \left( \frac{m}2 +\frac{1}{\zeta \lam} \right)\|u -z_0\|^2+ 4 \zeta\lam M^2
% 	\]
	Lemma \ref{lem:prox-dist} with $( \lam,\zeta \lam) = (\lam, \zeta \lam)$.
% 	and $\Gamma = \Gamma$ we have 
% 	\[
% 	\zeta^2(\frac{1}{4 \zeta \lam}+\frac{m}{2})\|z_{ \lam} - z_0\|^2 \le (f+h)(u) - (f+h)^* +  \frac12 \left(m+\frac{2}{\zeta \lam} \right) \|u-z_0\|^2+ 4 \zeta\lam M^2
% 	\]
	\end{proof}


% -------------------------

% With our tolerance
% $(\varepsilon,\eta)$
% \[
% \delta_0 = \frac{\lam \min \{ \eta^2, }
% \]
% \[
% \mbox{Inner} ??????= 1 + \frac{1}{\delta_0} = 1+ \frac{1}{\min\{ m \lam \bar \varepsilon,\lam N \bar \eta^2\}},
% \quad \mbox{Outer} = \frac{1}{\bar \eta^2}
% \ \ (\mbox{if $\chi=1$})
% \]


% ----------------- our results --------------------------

% \[
% Inner: \frac{1}{\delta_0} \quad Outer: \frac{1}{\bar \eta^2}
% \]



% Plug in
% \[
% \delta_0 = \min \{\bar \eta^2, \bar \varepsilon\}
% \]
% \[
% \mbox{Total} = \frac{1}{\bar \eta^2} \frac{1}{\min \{\bar \eta^2, \bar \varepsilon\}}
% \]

% ------- their definition --------

% 0)
% \[
% \partial_\delta f(\mathbf{x}):=\operatorname{conv}\left(\cup_{\mathbf{y}:\|\mathbf{y}-\mathbf{x}\| \leq \delta} \partial f(\mathbf{y})\right)
% \]

% They require that:

% 1) the existence of $\hat w$ such that
% \[
% \hat w \in \partial_{\hat \delta} f(x), \quad \|\hat w\| \le \varepsilon
% \]

% 2) or equivalently,
% the existence of $\hat w$ such that
% \[
% \hat w \in \partial f(y), \quad \|y-x\| \le \delta \, \quad  \|\hat w\| \le \varepsilon
% \]

% 3) or equivalently,
% the existence of $\hat w$ such that
% % \[
% % \hat w \in \partial f(y), \quad \|y-x\| \le \delta \, \quad  \|\hat w\| \le \varepsilon
% % \]
% \[
% 0 \in \partial_{\varepsilon} f(y), \quad \|y-x\| \le \delta
% \]

% Have
% \[
% (3) \Leftrightarrow (2) 
%   \Rightarrow (1) \Leftrightarrow(0)
% \]

% -------------------------------

% % So Proposition 1.9 means that x is an approximate solution of our definition implies
% % \[
% % \hat w \in \partial f(y) = \partial f(y)
% % \]
% % and 
% % \[
% % \|y-x\| \le \hat \delta 
% % \]
% % where you require
% % \[
% % \min \left\{\|\mathbf{g}\|: \mathbf{g} \in \partial_\delta f(\mathbf{x})\right\} \leq \epsilon
% % \]
% % So it means that  if 
% % \[
% % \hat w \le \varepsilon
% % \]
% % in our case then we also satisfy their definition.

% Have
% \[
%  \varepsilon = \bar \eta + 2 \sqrt{2 m \bar \varepsilon }, \quad  \delta = \sqrt{\frac{2\bar \varepsilon}{m}}.
% \]
% If you choose

% \[
% \bar \eta = \frac{\varepsilon}{2}, \quad \bar \varepsilon = \frac{\min \{m \delta^2,\varepsilon ^2\} }{2}
% \]
% Recall that
% \[
% \mbox{Outer} = \frac{1}{\bar \eta^2}
% \ \ (\mbox{if $\chi=1$})
% \]
% \[
% \mbox{Total} = \frac{1}{\bar \eta^2} \frac{1}{\min \{\bar \eta^2, \bar \varepsilon\}}
% \]
% % Then 
% % \[
% % \hat \varepsilon \le \varepsilon \quad \hat \delta \le \delta
% % \]
% So our final complexity becomes
% \[
% \frac{1}{\varepsilon^2}\frac{1}{\min\{\varepsilon^2,\delta^2\}} 
% \]

% --------------

% {\bf Conjecture:} 
% You need to do:

% (1)
% \[
% \mbox{Inner} = \frac{1}{\bar \varepsilon} 
% \]

% Implication:
% \[
% \mbox{Total} = \frac{1}{\bar \eta^2} \frac{1}{\bar \varepsilon} = \frac{1}{\varepsilon^2}
% \frac{1}{\min\{\delta^2,\varepsilon^2\}}
% \]



% -----------------

% (2)
% \[
% \bar \varepsilon = \delta^2 
% \]

% =========================\\
% We know how show that
% \[
% \mbox{Inner} = \frac{1}{\min\{\varepsilon^2,\delta^2\}},
% \quad \mbox{Outer} = \frac{1}{\varepsilon^2}
% \]
% \[
% \frac{1}{\min\{\varepsilon^2,\delta^2\}} \frac{1}{\varepsilon^2}
% \]
% We would like to show
% \[
% \frac{1}{\delta \varepsilon^3}
% \]]

% --------------DISCUSSION------------------\\
% For the weakly convex nonsmooth case,
% If x is an approximate solution,
% then it implies   there exists
%     $y \in \R^n$ such that
%     \[
%     \|y - x\| \leq \hat \delta,
%     \quad
% \hat w \in   \partial f(\hat y) .
% \]
% where $\|\hat w\|$ is small. Thus
% \[
% \hat w \in \partial_{\hat \delta} f(x)
% \]
% ==============================================\\
% \[
% \E(X) = \E(\max( X, X) )
% \]
% \[
% \E( \max( X, Y) )
% = \sum _w p_w \max(X(w),Y(w))
% \ge \max \left( \sum _w p_w X(w), \sum _w p_w Y(w) \right)
% \]

% \[
% \tX(w_1,w_2) = (X(w_1),X(w_2))
% \]

% In our case
% $X=F(u; \cdot)$
% and
% $Y=F(u;\cdot)$.

% Assume that
% \[
% E_\xi \left [ (F(u;\xi) - f(u))^2 \right] \le \sigma^2
% \]
% \[
% \ell(u,z_i) \le F(u;\xi_i)
% \]
% \[
% E \left[ \max\{ F(u;\xi_i) - f(u) \} \right] 
% \le E \left [ \left( \sum_i (F(u;\xi_i) - f(u))^2 \right)^{1/2} \right] \le \left( E \left [ \sum_i (F(u;\xi_i) - f(u))^2  \right] \right)^{1/2}
% \le 
% \sqrt{n}\sigma
% \]
% \[
% E \left[ \max\{ F(u;\xi_i) \} \right] \le f(u) + \sqrt{n}\sigma
% \]


\end{document}

