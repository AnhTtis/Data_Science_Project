\documentclass[10pt]{article}
\renewcommand{\baselinestretch}{1.0}
\usepackage{amsmath,amsthm,verbatim,amssymb,amsfonts,amscd, graphicx}
\usepackage{graphics}
\usepackage{centernot}
\usepackage{authblk}
\topmargin0.0cm
\headheight0.0cm
\headsep0.0cm
\oddsidemargin0.0cm
\textheight23.0cm
\textwidth16.5cm
\footskip1.0cm
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem*{remark}{Remark}
\newtheorem{proposition}{Proposition}
\newtheorem*{surfacecor}{Corollary 1}
\newtheorem{conjecture}{Conjecture}
\newtheorem{question}{Question} 
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}


\newcommand{\p}{\mathbb{P}}


%\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}

\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\weight}{\theta}

\usepackage{paralist}



\usepackage[colorlinks,linkcolor=blue,citecolor=blue]{hyperref}


\newcommand{\fsl}[1]{{\centernot{#1}}}
\setlength{\fboxsep}{0pt}
\setlength{\fboxrule}{.1pt}

\usepackage[bottom]{footmisc}
\usepackage{caption}
\usepackage{subcaption}
%\usepackage{subfloat}
%\usepackage{subfig}
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{multirow}
\usepackage{amsthm}
\usepackage{color}
\usepackage{MnSymbol}
\usepackage{makecell}
\usepackage{arydshln}
\usepackage{amsmath}
\usepackage[dvipsnames]{xcolor}
\usepackage{caption} 
\usepackage{natbib}
%\usepackage{epsfig}
%\usepackage{subcaption}
%\usepackage{float}
\usepackage{textcomp}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{csquotes}
\newcommand{\re}{\mathrm{Re}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{{\rm Var}}
\def\fr#1#2{{\textstyle\frac{#1}{#2}}}

\begin{document}
\title{Probabilistic Stability of Stochastic Gradient Descent}
\author{Liu Ziyin$^1$, Botao Li$^2$, Tomer Galanti$^3$, Masahito Ueda$^{1,4}$\\
{\small
\textit{$^1$Department of Physics, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan}\\
\textit{$^2$Laboratoire de Probabilit\'es, Statistique et Mod\'elisation/Universit\'e Paris Cit\'e, Paris, France}\\
\textit{$^3$ Center of Brains, Minds and Machines (CBMM), Massachusetts Institute of Technology, Cambridge, MA, USA}\\
\textit{$^4$RIKEN Center for Emergent Matter Science (CEMS), Wako, Saitama 351-0198, Japan}\\
}
}
\date{}
\maketitle
%\vspace{-3em}
\begin{abstract}
%\vspace{-0.5em}
Characterizing and understanding the stability of Stochastic Gradient Descent (SGD) remains an open problem in deep learning. A common method is to utilize the convergence of statistical moments, esp. the variance, of the parameters to quantify the stability. We revisit the definition of stability for SGD and propose using the \textit{convergence in probability} condition to define the \textit{probabilistic stability} of SGD. The probabilistic stability sheds light on a fundamental question in deep learning theory: how SGD selects a meaningful solution for a neural network from an enormous number of possible solutions that may severely overfit. We show that only through the lens of probabilistic stability does SGD exhibit rich and practically relevant phases of learning, such as the phases of the complete loss of stability, incorrect learning where the model captures incorrect data correlation, convergence to low-rank saddles, and correct learning where the model captures the correct correlation. These phase boundaries are precisely quantified by the Lyapunov exponents of the dynamics. The obtained phase diagrams imply that SGD prefers low-rank saddles in a neural network when the underlying gradient is noisy, thereby influencing the learning performance.
\end{abstract}

%\vspace{-3mm}
\section{Introduction}
%\vspace{-1mm}
Stochastic gradient descent (SGD) is the primary workhorse for optimizing neural networks. As such, an important problem in deep learning theory is to characterize and understand how SGD selects the solution of a deep learning model, which often exhibits remarkable generalization capability. At the heart of this problem lies the \textit{stability} of SGD because the models trained with SGD stay close to the attractive solutions where the dynamics is stable and moves away from unstable ones. Solving this problem thus hinges on having a good definition of the stability of SGD. The stability of SGD is often defined as a function of the variance of the model's parameters or gradients during training. The hidden assumption behind this mainstream idea is that when the variance diverges, the training becomes unstable~\citep{wu2018sgd, zhu2018anisotropic, liu2020improved, liu2021noise, ziyin2022strength}. In some sense, the idea that the variance of the parameters matters the most is also an underlying assumption in the deep learning optimization literature, where the utmost important quantity is how fast the variance and the expected distance of the parameters decay to zero \citep{vaswani2019fast, gower2019sgd}. We revisit this perspective and show that a variance-based notion of stability is insufficient to understand the empirically observed stability of training of SGD. In fact, we demonstrate a lot of natural learning settings where the variance of SGD diverges, yet the model still converges with high probability. 

In this work, we study the \textit{convergence in probability} condition to understand the stability of SGD. We then show that this stability condition can be quantified with the Lyapunov exponent \citep{lyapunov1992general} of the optimization dynamics of SGD, a quantity deeply rooted in the study of dynamical systems and has been well understood in physics and control theory \citep{eckmann1985ergodic}. The main contribution of this work is to propose a new notion of stability that sheds light on how SGD selects solutions and to discover multiple deep-learning phenomena that can only be understood in terms of this notion. Perhaps the most important implication of our theory is the characterization of the highly nontrivial and practically important phase diagram of SGD in a neural network loss landscape. %(as illustrated in Figure~\ref{fig: phase diagram}).

%\vspace{-1mm}
\section{Probabilistic Stability}
%\vspace{-1mm}
In this section, we introduce probabilistic stability, a rather general concept that appears in many scenarios in control theory \citep{khas1967necessary, eckmann1985ergodic, teel2014converse}. The definition of probabilistic stability relies on the notion of convergence in probability. We use $\|\cdot \|_n$ to denote the $n-$norm of a matrix or vector and $\|\cdot \|$ to denote the case where $n=2$. The notation $\to_p$ indicates convergence in probability.


\begin{definition}
    A sequence of random variables $\{
    \theta_t\}_{t}^{\infty}$ is probabilistically stable at $\theta^*$ if $\theta_t \to_p \theta^*$.
\end{definition}
Here, the notation $\to_p$ denotes convergence in probability. A sequence $z_t$ converges in probability to $\theta^*$ if $\lim_{t\to\infty} \p(\|\theta_t-\theta^*\| < \epsilon) = 0$ for any $\epsilon>0$.

We will see that this notion of stability is especially suitable for studying the stability of the stationary points in SGD, be it a local minimum or a saddle point. In contrast, the popular type of stability is based on the convergence of statistic moments, which we also define below.

\begin{definition}
    A sequence of random variables $\{\theta_t\}_{t}^{\infty}$ is $L_p$-norm stable at $\theta^*$ if $\lim_{t\to \infty} \E\|\theta_t - \theta^*\|_p^p \to 0$.
\end{definition}

%As will be demonstrated below, a lot of behavior of SGD can only be understood with probabilistic stability and not with norm stability.

For deep learning, the sequence of $\theta_t$ is the model parameters obtained by the iterations of the SGD algorithm for a neural network. For dynamical systems in general and deep learning specifically, it is impossible to analyze the convergence behavior of the dynamics starting from an arbitrary initial condition. Therefore, we have to restrict ourselves to the neighborhood of a given stationary point and consider the linearized dynamics around it.

The type of dynamics we consider in this work are all of the following form:
\begin{equation}\label{eq: sgd dynamics linearized}
    \theta_{t+1} = \theta_t - \lambda \hat{H}(x) (\theta_t - \theta^*) + O(||\theta_t - \theta^*||^2),
\end{equation}
where $\lambda$ is the learning rate, $\theta^*$ is the critical point under consideration, $\hat{H}(x)$ is a random symmetric matrix that is a function of the random variables $x$, which can stand for both a single data point or a minibatch of data points. In this work, $\theta^*$ is said to be a local minimum if $H:=\E_x[\hat{H}(x)]$ is positive semi-definite (PSD) and is a saddle point otherwise.

Note that $\hat{H}(x)$ does not have to be tied to a single data point but can also be the Hessian of a minibatch of data points. Essentially, it is the data distribution of $\hat{H}$ that matters. When we have the same data distribution but a different batch size $S$, the distribution of $\hat{H}$ is different. This linearized dynamics is especially suitable for studying two types of stationary points that appear in modern deep learning: (1) interpolation minima and (2) symmetry-induced saddle points. Let us first consider the stability of the interpolation minimum.

A minimum is said to be an interpolation minimum if the loss function reaches zero for all data points. This means that close to such a minimum, the per-sample loss functions all have vanishing first-order derivatives and positive-semidefinite Hessians \citep{wu2018sgd}:
\begin{equation}\label{eq: interpolation minimum expansion}
    \ell(\theta,x)=  (\theta -\theta^*)^T\hat{H}(x)(\theta- \theta^*) + O(||\theta||^3).
\end{equation}
The dynamics of $w$ thus obeys Eq.~\eqref{eq: sgd dynamics linearized}.
This type of minimum is of great importance in deep learning because modern neural networks are often ``overparametrized," and overparametrized networks under gradient flow are observed to reach these interpolation minima easily. 

The dynamics we consider is more general than that around an interpolation minimum because the Hessians $\hat{H}$ in Eq.~\eqref{eq: sgd dynamics linearized} are allowed to have eigenvalues of both positive and negative signs, whereas Eq.~\eqref{eq: interpolation minimum expansion} only allows for positive semidefinite $\hat{H}$. Therefore, the general solution of Eq.~\eqref{eq: sgd dynamics linearized} also helps us understand Eq.~\eqref{eq: interpolation minimum expansion} once we restrict the study to PSD Hessians. The types of saddle points that obey Eq.\eqref{eq: sgd dynamics linearized} have been found to widely exist when there is any type of loss function symmetry, such as permutation symmetry \citep{fukumizu2000local, simsek2021geometry, entezari2021role, hou2019minimal}, rescaling symmetry \citep{Dinh_SharpMinima, neyshabur2014search}, and rotation symmetry \citep{ziyin2023what}. Recently, it has been shown that every symmetry in the loss function leads to a critical point of this type, and, more importantly, the subset of parameters relevant to the symmetry, up to the leading order, has a dynamics completely independent of the rest of the parameters \citep{ziyin2023symmetry}. This type of critical point is mostly likely a saddle point, but can easily become local minima or global minima when weight decay is used. This means that Eq.~\eqref{eq: sgd dynamics linearized} can be seen as an effective description for only a small subset of all parameters in the model under consideration and is not as restrictive as it naively seems to be.

%\vspace{-2mm}
\section{The Probabilistic Stability of SGD}
%\vspace{-1mm}
This section presents the main theoretical results. We first show that the dynamics is exactly solvable for a rank-$1$ dynamics. We then prove a result showing that no moment-based stability can understand the stability of SGD around the saddle. Lastly, we prove that the probabilistic stability of SGD in high-dimension is equivalent to a condition on the sign of the Lyapunov exponent of the SGD dynamics.

%\vspace{-1mm}
\subsection{Rank-1 Dynamics}
%\vspace{-1mm}
Let us first consider the case in which $\hat{H}(x)=h(x) nn^T$ is rank-$1$ for a random scalar function $h(x)$, and a fixed unit vector $n$ for all data points $x$. Thus, the dynamics simplifies to a one-dimensional dynamics, where $h(x) \in \mathbb{R}$ is the corresponding eigenvalue of $\hat{H}(x)$:
\begin{equation}\label{eq: 1d symmetry dynamics}
    \theta_{t+1} = \theta_t - \lambda h(x) (\theta_t - \theta^*).
\end{equation}


\begin{theorem}\label{theo: 1d symmetry lyapunov condition}
    Let $\theta_t$ follow Eq.~\eqref{eq: 1d symmetry dynamics}. Then, for any distribution of $h(x )$,
    \begin{equation}
        n^T(\theta_t - \theta^*) \to_p 0
    \end{equation}
    if and only if\footnote{This condition generalizes to the case when the batch size $S$ is larger than $1$, where $h(x)$ becomes the per-batch Hessian, and the expectation is taken over all possible batches.}
    \begin{equation}\label{eq: 1d prob stability condition}
        \E_x[\log |1-\lambda h(x)|] < 0.
    \end{equation}
\end{theorem}

\begin{figure}
%\vspace{-1em}
    \centering
    \includegraphics[width=0.4\linewidth]{plots/phase_diagram_thesis.png}
    \includegraphics[width=0.44\linewidth]{plots/lyapunov.png}
    %\vspace{-1em}
    \caption{SGD exhibits a complex phase diagram through the lens of probabilistic stability. \textbf{Left}: For a matrix factorization saddle point, the dynamics of SGD can be categorized into at least five different phases. Phase \textbf{I}, \textbf{II}, and \textbf{IV} correspond to a successful escape from the saddle. Phase \textbf{III} is where the model converges to a low-rank saddle point. Phase \textbf{I} corresponds to the case $w_t \to_p u_t$, which signals correct learning. In phase \textbf{Ib}, the model also converges in variance. Phase \textbf{II} corresponds to stable but incorrect learning, where $w_t \to_p -u_t$. Phase \textbf{IV} corresponds to complete instability. \textbf{Right}: the phases of SGD can quantified by the sign of the Lyapunov exponent $\Lambda$. Where $\Lambda<0$, SGD collapses to a saddle point; when $\Lambda >0$, SGD escapes the saddle and enters a escaping phase. The two escaping phases are qualitatively different. For a small learning rate, the model is in a learning phase due to the repulsiveness of the saddle point at a small learning rate, and the model is likely to converge to local minima close to the saddle. For a very large learning rate, SGD escapes the saddle due to the dynamical instability of SGD, and the model will move far away from the saddle. Besides, the magnitude of the Lyapunov exponent can also quantity the speed of the learning dynamics. See Appendix~\ref{app sec: exp detail} for numerical details of this example.}
    %\vspace{-1em}
    \label{fig:first phase diagram}
\end{figure}

The condition \eqref{eq: 1d prob stability condition} is a sharp characterization of when a stationary condition or an interpolation minimum becomes attractive. It also works with weight decay. When weight decay is present, the diagonal terms of $\hat{H}$ are shifted by $\gamma$, and so $h = h' + \gamma$. 

At what learning rate is the condition violated? To leading orders in $\lambda$, this can be identified by expanding the logarithmic term up to the second order in $\lambda$:
\begin{equation}
    \E_x[\log |1-\lambda h(x)|] =  - \lambda \E[h(x)] - \frac{1}{2}\lambda^2\E_x[h(x)^2] + (\lambda^3).
\end{equation}
Ignoring the second-order term, we see that the dynamics always follow the sign of $\E[h(x)]$, in agreement with the GD algorithm. If $\E[h]>0$, the condition is always violated. When the second-order term is taken into consideration, the fluctuation of $h(x)$ now decides the stability of the stationary condition. The stationary condition is attractive if 
\begin{equation}
    \lambda > 2\frac{-\E[h(x)]}{\E[h(x)^2]}.
    \label{eq: lr condition}
\end{equation}
This result implies that the stationary condition can be attractive even if $h<0$. Comparing this critical learning rate with the continuous-time analysis in \cite{ziyin2023law}, we see that the two results agree. The r.h.s. of the condition also has a natural interpretation as a signal-to-noise ratio in the gradient. The denominator is the Hessian of the original loss function, which determines the signal in the gradient. The denominator is the strength of the gradient noise in the minibatch, and has been termed as ``gradient non-uniformity" in previous literature \cite{wu2018sgd}. An illustration of this solution is given in Figure~\ref{fig:first phase diagram}. We show the probabilistic stability conditions for a rank-$1$ saddle point with a rescaling symmetry (see Section~\ref{sec: phases of learning sgd}). The loss function is $\ell(u,w)= -xy uw + o(u^2 + w^2)$. Here, the data points $xy=1$, and $xy=a$ are sampled with equal probability. These saddles appear naturally in matrix factorization problems and also in recent sparse learning algorithms \citep{poon2021smooth, poon2022smooth, ziyin2023sparsity, kolb2023smoothing}. 

When the dynamics is high-dimensional, the problem becomes harder to solve because for each realization of $x$, $\hat{H}(x)$ do not commute with each other. While an analytical solution to the stability condition in high-dimension is unlikely to exist, we can say something quite general about them.

%\vspace{-2mm}
\subsection{Insufficiency of Norm-Stability}
%\vspace{-1mm}
Probabilistic stability offers a new tool for understanding the stability of the SGD algorithm around a given solution. Theorem~\ref{theo: 1d symmetry lyapunov condition} provides a perfect example to compare the probabilistic stability with the norm-stability, which is a conventional way to study the stability of SGD \citep{wu2018sgd, zhu2018anisotropic, liu2020improved, liu2021noise, ziyin2022strength, vaswani2019fast, gower2019sgd}. The following rather trivial proposition shows that if SGD converges to a point in $L_p$-norm, it must converge in probability.
\begin{proposition}
    If $\theta_t$ is stable at $\theta^*$ in $L_p$ norm, then it is stable at $\theta^*$ in probability.
\end{proposition}
The proof follows from the well-known fact that convergence in $L_p$ norm implies the convergence in probability. This implies that norm stability is a more restricted notion than probabilistic stability. In many cases, the two types of stabilities agree. However, we will see that for SGD, the two types of stability conditions can offer dramatically different predictions, which is constructively established by the following proposition.

\begin{proposition}\label{prop: moment insufficiency}
    Let $\theta_t$ follow Eq.~\eqref{eq: sgd dynamics linearized} around a critical point $\theta^*$. Then, 
    \begin{enumerate}
        \item for any fixed $\lambda$, there exists a data distribution such that $\theta_t$ is probabilistically stable but $L_p$-stable;
        \item if $\theta^*$ is a saddle point and $p \geq 1$, the set of $\theta_0$ that is $L_p$-stable has Lebesgue measure zero.
    \end{enumerate}
\end{proposition}
Therefore, this means that the $L_p$-stability is not useful in understanding the stability of SGD close to saddle points. One reason is that the outliers strongly influence the $L_p$ norm in the data, whereas the probabilistic stability is robust against such outliers.

%\vspace{-1mm}
\subsection{Lyapunov Exponent and Probabilistic Stability}
%\vspace{-1mm}

Extending the probabilistic stability to high-rank dynamics is nontrivial because the stability of SGD is generally initialization dependent, unlike the rank-$1$ case, where the condition is found to take an exact form and is initialization independent. It is thus useful to consider the worst-case initialization. Here, the crucial quantity is the \textit{Lyapunov exponent} of a point $\theta^*$:\footnote{More appropriately, this should be called the maximum Lyapunov exponent, which is initialization-independent. One can also consider the initialization-dependent Lyapunov exponent. If $\bar{H}$ is a $d$-by-$d$ matrix, a well-known fact is that the initialization-dependent Lyapunov exponent takes at most $d$ distinctive values. Conceptually, this means that there can be, at most, $d$ collapses at different critical learning rates.}
\begin{equation}
    \Lambda = \max_{\theta_0} \lim_{t\to \infty} \E \left[\log \frac{\|\theta_t - \theta^*\|} {\|\theta_0\|} \right].
\end{equation}
Here, the expectation is taken over the random samplings of the SGD algorithm. In general, $\Lambda$ does not vanish. The following theorem shows that SGD is probabilistically stable at a point if and only if its Lyapunov exponent is negative. 

\begin{theorem}\label{theo: main theorem}
    Assuming that $\Lambda \neq 0$, the linearized dynamics of SGD is probabilistically stable at $\theta^*$ for any $\theta_0$ if and only if $\Lambda < 0$.
\end{theorem}


The proof of this theorem invokes the Furstenberg-Kesten theorem~\citep{furstenberg1960products}. It is straightforward to show that the Lyapunov exponent exists and to find an upper and lower bound. For a finite-size dataset, we can define $r_{\max}$ to be larger than the absolute value of the eigenvalues of $I-\lambda \hat{H}(x)$ for all $x$. Similarly, we can define $r_{\min}>0$ to be smaller than all the absolute values of all the eigenvalues of $I-\lambda \hat{H}(x)$ for all $x$. Therefore, it is easy to check that 
\begin{equation}
    \log r_{\min}<  \Lambda < \log r_{\max}.
\end{equation}
However, it is difficult to give a better estimation of the exponent. In fact, it is a well-known open problem in the field of dynamical systems to find an analytical expression of the Lyapunov exponent \citep{crisanti2012products, pollicott2010maximal, jurga2019effective}. 

Now, we give two quantitative estimates about when the Lyapunov exponent will be negative. This discussion also implies a sufficient but weak condition for a general type of multidimensional dynamics to converge in probability. Let $h^*(x)$ be the largest eigenvalue of $\hat{H}(x)$ and assume that $1-h^*(x)>0$ for all $x$. Then, the following condition implies that $w\to_p 0$:
\begin{equation}
    \E_{x}[\log|1-\lambda h^*(x)|]<0
\end{equation}
which mimics the condition we found for rank-$1$ systems. An alternative estimate can be made by assuming that $\hat{H}(x)$ commute with $\hat{H}(x')$ for all $x$ and $x'$. If $\hat{H}$ has rank $d$, This reduces the problem to $d$ separated rank-1 dynamics, and Theorem~\ref{theo: 1d symmetry lyapunov condition} gives the exact solutions in each subspace. Numerical evidence shows that the commutation approximation quite accurately predicts the onset of low-rank behavior the actual rank (see Section~\ref{sec: phases of learning sgd} and Appendix~\ref{app: critical lr}). 


Another relevant question is whether this theorem is trivial for SGD at a high dimension in the sense that it could be the case that $\Lambda$ could be identically zero independent of the dataset. One can show that the Lyapunov exponent is generally nonzero for all datasets that satisfy a mild condition. Let $\E[\hat{H}]$ be full rank. By definition, 
\begin{equation}
    \Lambda = \lim_{t\to\infty}\frac{1}{t}\E\left[\log \weight_0^T \left(\prod_j^t (I - \lambda\hat{H}_{i_j})\right)\left(\prod_j^t (I - \lambda\hat{H}_{i_j})\right)^T \weight_0 \right] = -\frac{2\lambda \weight_0^T  H \weight_0}{\|\weight_0\|^2} + O (\lambda^2).
\end{equation}
Therefore, as long as $\lambda$ is sufficiently small, the sign of the Lyapunov exponent is opposite to the sign of the eigenvalues of $H$. This proves something quite general for SGD at an interpolation minimum: with a small learning rate, the model converges to the minimum exponentially fast, in agreement with common analysis in the optimization literature. See Figure~\ref{fig:first phase diagram}-right for numerical computation of Lyapunov exponents of a matrix factorization problem and the corresponding phases.

%\vspace{-2mm}
\section{Phases of Learning}\label{sec: phases of learning sgd}
%\vspace{-1mm}
With this notation of stability, we can study the actual effect of minibatch noise on a neural network-like landscape. The theory has interesting implications for the stability of interpolation minima in deep learning, which we explore in Appendix~\ref{app sec: interpolation minimum}. In the main text, we focus on the stability of SGD on saddle points. 

A commonly studied minimal model of the landscape of neural networks is a deep linear net (or deep matrix factorization) \citep{kawaguchi2016deep, lu2017depth, ziyin2022exact, wang2022posterior}. For these problems, we understand that all local minima are identical copies of each other, and so all local minima have the same generalization capability \citep{kawaguchi2016deep, ge2016matrix}. A deep linear net's special and interesting solutions are the saddle points, which are low-rank solutions and often achieve similar training loss with dramatically different generalization performances.\footnote{The influence of these singularities on generalization has been established by the singular learning theory \citep{watanabe2010asymptotic}.} More importantly, these saddles points also appear in nonlinear models with similar geometric properties, and they could be a rather general feature of the deep learning landscape \citep{brea2019weight}.  It is thus important to understand how the noise of SGD affects the stability of a low-rank saddle here. Let the loss function be $\E_x[(\sum_i u_i \sigma(w_i x) - y)^2/2]$, where $\sigma(x) = c_0 x + O(x^2)$ is any nonlinearity that is locally linear at $x=0$. We let $c_0=1$ and focus on cases where both $x$ and $y$ are one-dimensional. Locally around $u,\ w\approx 0$, the model $u^Tw$ is either rank-$1$ or rank-$0$. The rank-$0$ point where $u_i=w_i=0$ for all $i$ is a saddle point as long as $\E[xy] \neq 0$. In this section, we show that the stability of this saddle point features complex and dramatic phase transition-like behaviors as we change the learning rate of SGD.

Consider the linearized dynamics around the saddle at $w_i=u_i=0$. The expanded loss function takes the following form:
\begin{equation}
    \ell(u,w) = -{xy} \sum^d_i u_iw_i + const.
\end{equation}
For learning to happen, SGD needs to escape from the saddle point. Let us consider a simple data distribution where $xy=1$ and $xy=a$ with equal probability. When $a>-1$, \textit{correct} learning happens when ${\rm sign}(w)={\rm sign}(u)$. We thus focus on the case of $a>-1$. The case of $a<-1$ is symmetric to this case up to a rescaling. This example is already presented in Figure~\ref{fig:first phase diagram}. The two most important observations are: (1) SGD can indeed converge to low-rank saddle points; however, this happens only when the gradient noise is sufficiently strong and when the learning rate is large (but not too large); (2) the region for convergence to saddles (region III) is exclusive with the region for convergence in mean square (Ia), and thus one can only understand the saddle-seeking behavior of SGD within the proposed probabilistic framework. We prove the following proposition. It becomes evident that the low-rank solution is reached when both $w_t+u_t \to_p 0$ and $w_t-u_t \to_p 0$.

\begin{proposition}\label{prop: phase diagram}
    For any $w_0, u_0 \in \mathbb{R}/\{0\}$. $w_t - u_t \to_p 0$ if and only if $\E_x[\log|1-\lambda xy|] <0$. $w_t + u_t$ converges to $0$ in probability if and only if $\E_x[\log|1+\lambda xy|] <0$.
\end{proposition}


\begin{figure}
%\vspace{-1em}
    \centering
    \includegraphics[width=0.32\linewidth]{plots/phase_diagram_300000_edited.png}
    \includegraphics[trim={0 2mm 0 0}, clip, width=0.32\linewidth]{plots/phase_diagram_S_edited.png}
    %\includegraphics[width=0.31\linewidth]{plots/saddle_linear_ppt.png}
    %\includegraphics[width=0.31\linewidth]{plots/saddle_tanh_ppt.png}
    %%\vspace{-1em}
    %\vspace{-1em}
    \caption{\textbf{Phase diagrams of SGD stability}. The definitions of the phases are the same as those in Figure~\ref{fig:first phase diagram}. We sample a dataset of size $N$ such that $x \sim \mathcal{N}(0, 1)$ and noise $\epsilon \sim \mathcal{N}(0, 4)$, and generate a noisy label $y=\mu x + (1-\mu) \epsilon$. Left: the $\lambda-\mu$ phase diagram for $S=1$ and $N=\infty$. Right: The $\lambda-S$ phase diagram for $\mu=0.06$ and $N=\infty$.}
    \label{fig: more phase diagrams}
    %\vspace{-1em}
\end{figure}


\begin{figure}[t!]
%    \vspace{-4em}
    \centering
    \includegraphics[width=0.4\linewidth]{plots/saddle_linear.png}
    \includegraphics[width=0.4\linewidth]{plots/saddle_tanh.png}
%    \vspace{-1em}
    \caption{Convergence to low-rank solutions in nonlinear neural networks. At every training step, we sample input $x \sim \mathcal{N}(0, I_{200})$ and noise $\epsilon \sim \mathcal{N}(0, \sqrt{2}I_{200})$, and generate a noisy label $y=\mu x + (1-\mu) \epsilon$, where $1-\mu$ controls the level of the noise. \textbf{Left}: Linear network. \textbf{Right}: tanh network. The white dashed line shows the theoretical prediction of the appearance of low-rank structure computed by numerically integrating the condition in Proposition~\ref{prop: phase diagram}.}
    \label{fig: nn low rank}
\end{figure}

\begin{figure}[t!]
 %\vspace{0.5em}
    \centering
    \includegraphics[width=0.4\linewidth]{plots/Sparsity/module.layer2.1.conv2.weight.png}
    \includegraphics[width=0.4\linewidth]{plots/Sparsity/module.layer2.1.conv2.weight_static.png}
    \includegraphics[width=0.4\linewidth]{plots/Sparsity/module.layer3.1.conv2.weight_static.png}
    \includegraphics[width=0.4\linewidth]{plots/Sparsity/module.layer4.1.conv2.weight_static.png}
%    \vspace{-1em}
    \caption{Density ($1-sparsity$) of the convolutional layers in a ResNet18, when there is static noise (mislabeling) in the training data. Here, we show the number of sparse parameters in the two of the largest convolutional layers, each containing roughly one million parameters in total. The figures respectively show layer1.1.conv2 (upper left), layer2.1.conv2 (upper right), layer3.1.conv2 (lower left), and layer4.1.conv2 (lower right).}
    \label{app fig: resnet sparsity static}

    
    \centering
     %\vspace{0.5em}
    \includegraphics[width=0.4\linewidth]{plots/cifar10_static_noise.png}
    \includegraphics[width=0.4\linewidth]{plots/cifar10_static_noise_acc.png}
    %\vspace{-1em}
    \caption{Rank (\textbf{left}) and test accuracy (\textbf{right}) of the ResNet18 trained in a data set with static noise. The transition of rank has a clear boundary. The model has a full rank but random-guess level performance for large noise and small learning rates. Here, \textit{noise} refers to the probability that data point is mislabeled.}
    \label{app fig: cifar static}
    %\vspace{-1em}
\end{figure}

The theory shows that the phase diagram of SGD strongly depends on the data distribution, and it is interesting to explore and compare a few different settings. Now, we consider a size-$N$ Gaussian dataset. Let $x_i \sim \mathcal{N}(0, 1)$ and noise $\epsilon_i \sim \mathcal{N}(0, 4)$, and generate a noisy label $y_i=\mu x_i + (1-\mu) \epsilon_i$. See the phase diagram for this dataset in Figure~\ref{fig: more phase diagrams} for an infinite $N$. The phase diagrams in Figure~\ref{fig: finite size dataset} show the phase diagram for a finite $N$. We see that the phase diagram has a very rich structure at a finite size. We make three rather surprising observations about the phase diagrams: (1) as $N\to \infty$, the phase diagram becomes smoother and smoother and each phase takes a connected region (cf. finite size experiments in Appendix~\ref{app sec: finite size}); (2) phase II seems to disappear as $N$ becomes large; (3) the lower part of the phase diagram seems universal, taking the same shape for all samplings of the datasets and across different sizes of the dataset. This suggests that the convergence to low-rank structures can be a universal aspect of SGD dynamics, which corroborates the widely observed phenomenon of collapse in deep learning \citep{papyan2020prevalence, wang2022posterior, tian2022deep}. The theory also shows that if we fix the learning rate and noise level, increasing the batch size makes it more and more difficult to converge to the low-rank solution (see Figure~\ref{fig:resnet18 different batchsize}, for example). This is expected because the larger the batch size, the smaller the effective noise in the gradient.



Many recent works have suggested how neural networks could be biased toward low-rank solutions. Theoretically, \cite{galanti2022sgd} showed that with weak weight decay, SGD is biased towards low-rank solutions. \cite{ziyin2022exact} showed that GD converges to a low-rank solution with weight decay. Therefore, weight decay already induces a low-rank bias in learning, and it is unknown if SGD alone has any bias toward low-rank solutions. \cite{andriushchenko2022sgd} showed empirical hints of a preference for low-rank solutions when training without SGD. However, it remains to be clarified when or why SGD has such a preference on its own. To the best of our knowledge, our theory is the first to precisely characterize the low-rank bias of SGD in a deep learning setting. Compared with the stability diagram of linear regression, this result implies that a large learning rate can both help and hinder optimization.









\paragraph{Phase Diagram for Neural Networks}

There is a strong sense of universality in the lower-left part of the phase diagram in Figure \ref{fig:first phase diagram} since they all take a similar shape independent of the size or sampling of the data points. We now verify its existence in actual neural networks.

We start with a controlled experiment where, at every training step, we sample input $x \sim \mathcal{N}(0, I_{200})$ and noise $\epsilon \sim \mathcal{N}(0, 4I_{200})$, and generate a noisy label $y=\mu x + (1-\mu) \epsilon$. Note that $1-\mu$ controls the level of the noise. Training proceeds with SGD on the MSE loss. We train a two-layer model with the architecture: $200 \to 200 \to 200$. See Figure~\ref{fig: nn low rank} for the theoretical phase diagram, which is estimated under the commutation approximation. Under SGD, the model escapes from the saddle with a finite variance to the right of the dashed line and has an infinite variance to its left. In the region $\lambda \in (0, 0.2)$, this loss of the $L_2$ stability condition coincides with the condition for the convergence to the saddle. The experiment shows that the theoretical boundary agrees well with the numerical results.\footnote{Commonly used variants of SGD such as the Adam optimizer \citep{journals/corr/KingmaB14_adam} also have a qualitatively similar phase diagram. See Appendix~\ref{app sec: exp} This suggests that the effects we studied are rather universal, not just a special feature of SGD.}


Lastly, we train independently initialized ResNets on CIFAR-10 with SGD. The training proceeds with SGD without momentum at a fixed learning rate and batch size $S=32$ (unless specified otherwise) for $10^5$ iterations. Our implementation of Resnet18 contains $11$M parameters and achieves $94\%$ test accuracy under the standard training protocol, consistent with the established values. To probe the effect of noise, we artificially inject a dynamical label noise during every training step, where, at every step, a correct label is flipped to a random label with probability $noise$, and we note that the phase diagrams are similar regardless of whether the noise is dynamical or static (where the mislabelling is fixed). See Figure~\ref{app fig: cifar static} for the phase diagram of static label noise. Interestingly, the best generalization performance is achieved close to the phase boundary when the noise is strong. This is direct evidence that SGD noise has a strong regularization effect on the trained model. We see that the results agree with the theoretical expectation and the analytical model's phase diagram. We also study the sparsity of the ResNets in different layers in Figure~\ref{app fig: resnet sparsity static}, and we observe that the phase diagrams are all qualitatively similar. Also, see Appendix~\ref{app sec: exp} for the experiment with a varying batch size.



\begin{figure}
    \centering
    %\vspace{-1em}
    \includegraphics[width=\linewidth]{plots/swish_example.png}
    %\vspace{-1.5em}
    \caption{\textbf{How SGD selects a solution}. \textbf{Left}: The landscape of a two-layer network with the swish activation function \citep{ramachandran2017searching}. \textbf{Middle, Right}: the generalization performance of the model for different learning rates. \textbf{Middle}: Initialized at solution B, SGD first jumps to C and then diverges. \textbf{Right}: Initialized at A, SGD also jumps to C and diverges. In both cases, the behavior of SGD agrees with the prediction of the probabilistic stability instead of the $L_2$ stability. Instead of jumping between local minima, SGD, at a large learning rate, transitions from minima to saddles.}
    \label{fig:select minimum}
    %\vspace{-1em}
\end{figure}

%\vspace{-2mm}
\section{Which Solution Does SGD Prefer?}
%\vspace{-1mm}
We now investigate one of the most fundamental problems in deep learning through the lens of probabilistic stability: how SGD selects a solution for a neural network. In this section, we study a two-layer network with a single hidden neuron with the swish activation function: $f(w,u,x) = u \times {\rm swish}(wx)$, where ${\rm swish}(x) = x \times {\rm sigmoid}(x)$. Swish is a differentiable ReLU variant discovered by meta-learning techniques and consistently outperforms ReLU in various tasks. We generate $100$ data points $(x,y)$ as $y = 0.1 {\rm swish}(x) + 0.9\epsilon$, where both $x$ and $\epsilon$ are sampled from normal distributions. See Figure~\ref{fig:select minimum} for an illustration of the training loss landscape. There are two local minima: solution A at roughly $(-0.7, -0.2)$ and solution B at $(1.1, -0.3)$. Here, the solution with better generalization is A because it captures the correct correlation between $x$ and $y$ when $x$ is small. Solution A is also the sharper one; its largest Hessian eigenvalue is roughly $h_a = 7.7$. Solution B is the worse solution; it is also the flatter one, with the largest Hessian value being $h_b=3.0$. There is also a saddle point C at $(0,0)$, which performs significantly better than B and slightly worse than A in generalization.

If we initialize the model at A, $L_2$ stability theory would predict that as we increase the learning rate, the model moves from the sharper solution A to the flatter minimum B when SGD loses $L_2$ stability in A; the model would then lose total stability once SGD becomes $L_2$-unstable at B. As shown by the red arrows in Figure~\ref{fig:select minimum}. In contrast, probabilistic stability predicts that SGD will move from A to C as C becomes attractive and then lose stability, as the black arrows indicate. See the right panel of the figure for the comparison with the experiment for the model's generalization performance. The dashed lines show the predictions of the $L_2$ stability and probabilistic theories, respectively. We see that the probabilistic theory predicts both the error and the place of transition right, whereas $L_2$ stability neither predicts the right transition nor the correct level of performance. 

If we initialize at B, the flatter minimum, $L_2$ stability theory would predict that the model will only have one jump from B to divergence as we increase the learning rate. Thus, from $L_2$ stability, SGD would have roughly the performance of B until it diverges, and having a large learning rate will not help increase the performance. In sharp contrast, the probabilistic stability predicts that the model will have two jumps: it stays at B for a small $\lambda$ and jumps to C as it becomes attractive at an intermediate learning rate. The model will ultimately diverge if C loses stability. Thus, our theory predicts that the model will first have a bad performance, then show a better performance at an intermediate learning rate, and finally diverge. See the middle panel of Figure~\ref{fig:select minimum}. We see that the prediction of the probabilistic stability agrees with the experiment and correctly explains why SGD leads to better performance.

%\vspace{-2mm}
\section{Discussion}
%\vspace{-1mm}

In this work, we have demonstrated that the convergence in probability condition serves as an essential notion for understanding the stability of SGD, leading to highly nontrivial and practically relevant phase diagrams at a finite learning rate. Crucially, these effects are only present for SGD and not for GD, demonstrating that the algorithmic regularization due to SGD is qualitatively different from that of GD. We also clarified its intimate connection to Lyapunov exponents, which are fundamental metrics of stability in the study of dynamical systems. The proposed stability agrees with the norm-based notion of stability at a small learning rate and large batch size. At a large learning rate and a small batch size, we have shown that the proposed notion of stability captures the actual behavior of SGD much better and successfully explains a series of experiment phenomena that have been quite puzzling. Among the many implications that we discussed, perhaps the most fundamental one is a novel understanding of the implicit bias of SGD. When viewed from a dynamical stability point of view, the implicit bias of stochastic gradient descent is thus fundamentally different from the implicit bias of gradient descent. The new perspective we provide is also different from the popular perspective that SGD influences the learning outcome directly by making the model converge to flat minima. In fact, in our construction, the flatter minimum does not have better generalization properties, nor does SGD favor it over the sharper one. Instead, SGD performs a selection between the case of converging to saddles and local minima and that of directly and significantly affecting its performance. Thus, we believe that probabilistic stability and Lyapunov exponents are important future directions to investigate the stability of SGD in a deep-learning scenario. 


\bibliography{ref}
\bibliographystyle{iclr2024_conference}


\clearpage
\appendix


\section{Additional Numerical Results}\label{app sec: exp}

\subsection{Experimental Detail for Figure~\ref{fig:first phase diagram}-right}\label{app sec: exp detail}
The experiment is performed for a two-dimensional system whose dynamics is specified in \eqref{eq: sgd dynamics linearized}. The expectation of the Hessian $\E[\hat{H}]$ is chosen to be $\text{diag}(0.1, -0.1)$, while the noise is generated via a normal $2\times 2$ random matrix $M_\text{noise}$. The noisy Hessian is obtained as
\begin{equation}
    \hat{H} = \E[\hat{H}] + M_\text{noise}+ M_\text{noise}^T,
\end{equation}
and one can verify that such $\hat{H}$ is symmetric and consistent with our choice of $\E[\hat{H}]$. The initial state is sampled from a unit circle. The dynamics stops at time step $t$, and the Lyapunov exponent is calculated as $\frac{1}{t}\log||\weight_t||$, if one of the three following conditions is satisfied: $||\weight||$ reaches the upper cutoff of $10^{100}$; $||\weight||$ reaches the lower cutoff of $10^{-140}$; the preset maximal number of steps of $5000$ is reached. For each learning rate, the Lyapunov is obtained as the average of the results collected in $800$ independent runs.

\clearpage
\subsection{Phases of Finite-Size Datasets}\label{app sec: finite size}

See Figure~\ref{fig: finite size dataset}. 

\begin{figure}[b!]
    \centering
    \includegraphics[width=0.33\linewidth]{plots/phase_diagram_3.png}
    \includegraphics[width=0.33\linewidth]{plots/phase_diagram_4.png}
    
    \includegraphics[width=0.33\linewidth]{plots/phase_diagram_8.png}
    \includegraphics[width=0.33\linewidth]{plots/phase_diagram_10.png}

    \includegraphics[width=0.33\linewidth]{plots/phase_diagram_24.png}
    \includegraphics[width=0.33\linewidth]{plots/phase_diagram_100.png}

    %%\vspace{-1em}
    \caption{\textbf{Phase diagrams of SGD stability for finite-size dataset}. The data sampling is the same as in Figure~\ref{fig: more phase diagrams}. From upper left to lower right: $N=3,\ 4,\ 8,\ 10,\ 24,\ 100$. As the dataset size tends to infinity, the phase diagram converges to that in Figure~\ref{fig: more phase diagrams}. The lower parts of all the phase diagrams look similar, suggesting a universal structure.}
    \label{fig: finite size dataset}
\end{figure}


\clearpage
\subsection{Effect of Changing Batchsize on Resnet18}
See Figure~\ref{fig:resnet18 different batchsize}.
\begin{figure}[t!]
    \centering
    %\includegraphics[width=0.3\linewidth]{plots/cifar10_dynamic_noise.png}
    %\includegraphics[width=0.26\linewidth]{plots/cifar10_dynamic_noise_phase_boundary.png}
    \includegraphics[width=0.3\linewidth]{plots/resnet_bs.png}
    \caption{The rank of the penultimate-layer representation of Resnet18 trained with different levels of batch sizes. In agreement with the phase diagram, the model escapes from the low-rank saddle as one increases the batch size.}
    \label{fig:resnet18 different batchsize}
\end{figure}


%\clearpage
\subsection{Critical learning rate}
\label{app: critical lr}
Here, we compare the empirical rank of the solution with the commutation approximated critical learning rates obtained in \eqref{eq: lr condition}. See Figure~\ref{fig: crit lr}. The experiment is run on a two-layer fully connected linear network: $50 \to 50\to 50$, which is equivalent to a matrix factorization problem. The model is initialized with the standard Kaiming init. The dataset we consider is one with a sparse but full-rank signal. 

Let $\odot$ denote the Hadamard product. The input data is generated as $x= m \odot X$, where $X\sim \mathcal{N}(0, I_{50})$ and $m$ is a random mask where a random element is set to be $1$, and the rest is zero. The labels $Y$ is generated as $Y = \mu x + (1-\mu) (m \odot \epsilon)$, where $\epsilon \sim \mathcal{N}(0, 2 \text{diag}(0.01, 0.05, ...., 2.01))$ is the noise. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.35\linewidth]{plots/RankLR/synopsis5.png}
    \includegraphics[width=0.35\linewidth]{plots/RankLR/synopsis15.png}
    \includegraphics[width=0.35\linewidth]{plots/RankLR/synopsis25.png}
    \includegraphics[width=0.35\linewidth]{plots/RankLR/synopsis35.png}
    \caption{Rank vs learning rate in a vanilla matrix factorization problem for, from upper left to lower right, $\mu = 0.05,\ 0.15,\ 0.25,\ 0.35$. The theoretical curve is from the commutation approximation where each subspace of the model collapses at the critical learning rate $\lambda = -2\frac{\E[h(x)]}{\E[h^2(x)]}$.}
    \label{fig: crit lr}
\end{figure}




\clearpage
\section{Experiment with Adam}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.33\linewidth]{plots/Adam/Adam_linear.png}
    \includegraphics[width=0.33\linewidth]{plots/Adam/Adam_tanh.png}
    \includegraphics[width=0.33\linewidth]{plots/Adam/Adam_relu.png}
    \includegraphics[width=0.33\linewidth]{plots/Adam/Adam_swish.png}
    \caption{Rank of the converged solution for two-layer linear (upper left), tanh (upper right), relu (lower left) and swish (lower right) models.}
    \label{fig: nn adam}
\end{figure}


\subsection{Experiment with Adam}
We note that the phenomena we studied is not just a special feature of the SGD, but, empirically, seems to be a universal feature of first-order optimization methods that rely on minibatch sampling. Here, we repeat the experiment in Figure~\ref{fig: more phase diagrams}. We train with the same data and training procedure, except that we replace SGD with Adam \citep{journals/corr/KingmaB14_adam}, the most popular first-order optimization method in deep learning. Figure~\ref{fig: nn adam} shows that similarly to SGD, Adam also converges to the low-rank saddles in similar regions of learning rate and $\mu$.



\clearpage
\section{Additional Theoretical Concerns}

\subsection{A Simple example of Interpolation Minimum}\label{app sec: interpolation minimum}
To illustrate different concepts of stability of the SGD algorithm, we examine a simple one-dimensional linear regression problem. The training loss function for this problem is defined as $L(w)=\frac{1}{N}\sum_i^{N}(wx_i - y_i)^2$. 

For GD, the dynamics diverges when the learning rate is larger than twice the inverse of the largest Hessian eigenvalue. To see this, let $H=\E_x[\hat{H}(w^*,x)]$ denote the Hessian of $L$ and $h$ its largest eigenvalue. Using GD leads to $||w_{t+1}|| = ||w_0(I - \lambda H)^t|| \propto |1-\lambda h|^t$. Divergence happens when $|1 -\lambda h| > 1$. The range of viable learning rates is thus:
\begin{equation}\label{eq: lambda gd}
\lambda_{\rm GD} \leq 2/h = 2/\E_x[x^2].
\end{equation}
Naively, one would expect that a similar condition approximates the stability condition for the case when mini-batch sampling is used to estimate the gradient. 

For SGD, the variance stability condition is the same as the condition that the second moment of SGD decreases after every time step, starting from an arbitrary initialization (see Section \ref{app sec: dynamical stability}):
\begin{equation}\label{eq: lambda DS}
    \lambda_{\rm DS} \leq  \frac{2S^2 \E[x^2]}{\E[x^4] + (S-1)^2 \E[x^2]^2}.%\frac{2}{h}\frac{S^2}{3S + (S-1)^2}.
\end{equation} 
Also related is the stability condition proposed by \cite{ziyin2022strength}, who showed that starting from a stationary distribution, $w$ stays stationary under the condition $\lambda_{\rm SS} < \frac{2}{h} \frac{1}{1+1/S}$, which we call the stationary linear stability condition (SS). When we have batch size $1$, the stability condition halves: $\lambda < 1/h$. For all stability conditions, we denote the maximum stable learning rate with an asterisk as $\lambda^*$. %The most important prediction made by the variance stability theories is that SGD prefers flatters minima to sharper ones because the linear stability is lost as one increases the learning rate above $2/h$, which is believed to lead to better performance \citep{zhu2018anisotropic, wu2018sgd, xie2020diffusion, wu2022does}. %On the contrary, we will show that this mechanism of minimum selection is not what probabilistic stability implies. %Throughout this work, we use the term ``linear stability theory" to refer to any theory of stability that is based on the statistical moments in order to emphasize their difference from probabilistic stability, which is our main proposal.

For the probabilistic stability, let us consider the interpolation regime, where all data points $(x,y) \in \mathbb{R}^{2}$ lie on a straight line. In this situation, the loss function has a unique global minimum of $w^*=y_i/x_i$ for any $i$. Applying Theorem~\ref{theo: 1d symmetry lyapunov condition}, one can immediately prove the following proposition.
\begin{proposition}\label{prop: learning rates are not isolated}
    Let $\lambda$ be such that $\E_x[\log |1-\lambda x^2|]\neq 0$. Then, for any $w_0$, $w_t\to_p w^*$ if and only if $\E_x[\log |1-\lambda x^2|]< 0$.
\end{proposition}
It is worth remarking that this condition is distinctively different from the case in which the gradient noise is a parameter-independent random vector. For example, \cite{liu2021noise} showed that if the gradient noise is a parameter-independent Gaussian, SGD diverges in distribution if $\lambda >2/h$. This suggests that the fact that the noise of SGD is $w$-dependent is crucial for its probabilistic stability.


One of the implications of the probabilistic stability is that for $\lambda = 1/x_i^2$, the SGD dynamics is always stable. Therefore, the largest stable learning rate is roughly given by:
\begin{equation}
\lambda_{\max} = 1/x^2_{\min}.
\end{equation}
However, for these special choices of learning rates, the moment stability is not always guaranteed. As mentioned earlier, convergence in mean occurs when $\lambda \leq \lambda_{DS}^*$. However, this condition does not hold when $\lambda = 1/x^2_{\min}$ and $x_{\min} < \E[x_i]/2$, which is often the case for standard datasets. This result shows that the maximal learning rate that ensures stable training can be much larger than the maximal learning rate required for convergence in mean (cf. \eqref{eq: lambda DS}). For a fixed value of $\E[x^2]$, $x_{\min}^2$ can be arbitrarily small, which means that the maximal stable learning rate can be arbitrarily large. Another consequence of this result is that the stability of SGD depends strongly on individual data points and not just on summary statistics of the whole dataset.


This result highlights the importance of the Lyapunov exponent $\Lambda = \E_x[\log|1-\lambda x^2|]$ and its sign in understanding the convergence of $w_t$ to the global minimum. When $\Lambda$ is negative, the convergence to the global minimum occurs. If $\Lambda$ is positive, SGD becomes unstable. We can determine when $\Lambda$ is negative for a training set of finite size by examining the following equation:
\begin{equation}\label{eq: convergent condition}
\Lambda = \frac{1}{N} \sum_i \log |1-\lambda x_i^2|,
\end{equation}
which is negative when $\lambda$ is close to $1/x_i^2$ for some $i \in {1,\dots,N}$. What is the range of $\lambda$ values that satisfy this condition? Suppose that $\lambda$ is in the vicinity of some $1/x_i^2$: $\lambda = \delta\lambda + 1/x_i^2$, and the instability is caused by a single outlier data point $x_{\rm out} \gg 1$. Then, $\Lambda$ is determined by the competing contributions from the outlier, which destabilizes training, and $x_i^2$, which stabilizes training, and the resulting condition is approximately $|1 - \lambda x_i^2| < {1}/{|\lambda x_{\rm out}^2|}$. Because $\lambda \approx 1/x_i^2$, this condition leads to:
\begin{equation}
|\delta \lambda | < {x_i^2}/{x_{\rm out}^2}.
\end{equation}
This is a small quantity. However, if we change the learning rate to the stability region associated with another data point $x_j$ as soon as we exit the stability region of $x_i$, we still maintain stability. Therefore, the global stability region depends on the density of data points near $x_i$. Assuming that there are $N$ data points near $x_i$ with a variance of $\sigma^2$, the average distance between $x_i$ and its neighbors is approximately $\sigma^2/N$. As long as $\sigma^2/N < {x_i^2}/{x_{\rm out}^2}$, SGD will remain stable in a large neighborhood. In practical terms, this means that when the number of data points is large, SGD is highly resilient to outliers in the data as shown in Figure~\ref{fig:robustness}. We see that the region of convergence in probability is dramatic, featuring stripes of convergent regions that correspond to $1/x_i^2$ for each data point and divergent regions where $\Lambda>0$.  While simple, this example has a fundamental implication: there are problems that cannot be learned by SGD at a small learning rate but can be learned by SGD at a finite learning rate. This implies the insufficiency of the commonly used stochastic differential equation theories of SGD \citep{li2021validity}. 

\begin{figure}[t!]
    \centering
    %\begin{subfigure}{0.2\linewidth}
    %\includegraphics[width=\linewidth]{plots/temp/25_toy_extreme.png}
    %    %\vspace{-2em}
    %\caption{$N=25$}
    %\end{subfigure}
    % \begin{subfigure}{0.22\linewidth}
    %\includegraphics[width=\linewidth]{plots/temp/25_toy_extreme2.png}
    %    %\vspace{-2em}
    %\caption{$N=25$}
    %\end{subfigure}   
    \begin{subfigure}{0.23\linewidth}
    \includegraphics[width=\linewidth]{plots/temp/1_toy.png}
        %\vspace{-1em}
    \caption{\small $N=2$}
    \end{subfigure}
    \begin{subfigure}{0.23\linewidth}
    \includegraphics[width=\linewidth]{plots/temp/5_toy.png}
        %\vspace{-1em}
    \caption{\small $N=6$}
    \end{subfigure}
    \begin{subfigure}{0.23\linewidth}
    \includegraphics[width=\linewidth]{plots/temp/500_toy_extreme.png}
        %\vspace{-1em}
    \caption{\small $N=500$}
    \end{subfigure}
     \begin{subfigure}{0.255\linewidth}
    \includegraphics[width=\linewidth]{plots/temp/500_toy_extreme2.png}
    %\vspace{-1em}
    \caption{\small $N=500$}
    \end{subfigure}  

    %%\vspace{-1em}
    \caption{{\bf  Stability of SGD against a single outlier data} in a dataset of size $N$. Yellow denotes where SGD converges in probability, and dark blue denotes divergence. We control the norm of the first data point ($x_1^2$) while sampling the rest data from a standard normal distribution. (a-c) Stability of SGD for different sizes of the dataset; (d) zoom-in of (c) at a small learning rate. The grey dashed curves show $\lambda_{GD}^*$, and the green dashed curve shows $\lambda_{GD}^*/N$. The intermediate finite learning rates are robust against outliers in the data, whereas the smallest learning rates are strongly sensitive to outliers in the data.} 
    \label{fig:robustness}
\end{figure}


An important implication is the robustness of SGD to outliers in comparison to gradient descent. As Figure~\ref{fig:robustness} shows, the bulk region of probabilistic stability stays roughly unchanged as the outlier data point becomes larger and larger; in contrast, both $\lambda_{GD}^*$ and $\lambda_{DS}^*$ decreases quickly to zero. In the bulk region of the learning rates, SGD is thus probabilistically stable but not stable in the moments. Meanwhile, in sharp contrast to this bulk robustness is the sensitivity of the smallest branch of learning rates of SGD to the outliers. Assuming that there is an outlier data point with a very large norm $c \gg N$, the largest $\lambda_{\rm GD}$ scales as $\lambda_{\max} \sim Nc^{-1}$. In contrast, for SGD, the smallest branch of the probabilistically stable learning rate scales as $c^{-1}$, independent of the dataset size. This means that if we only consider the smallest learning rate, SGD is much less stable than GD, and one needs to use a much smaller learning rate to ensure convergence. For $\lambda_{\rm DS}$ a detailed analysis in Section~\ref{app sec: dynamical stability} shows that $\lambda_{\rm DS}^* = (Nc)^{-1}$. Thus, the threshold of convergence in mean square is yet one order of magnitude smaller than that of probabilistic convergence. In the limit $N\to \infty$, SGD cannot converge in variance but can still converge in probability.
\clearpage

\subsubsection{$L_2$ Stability Conditions and the Derivation of Eq.~\eqref{eq: lambda DS}}\label{app sec: dynamical stability}
For a general batch size $S$, the dynamics of SGD reads 
\begin{align}
    w_{t+1} &= w_t - \lambda w_t \frac{1}{S}\sum_{i=1}^S x_i^2\\
    &= w_t \left(1-\lambda \frac{1}{S}\sum_{i=1}^S x_i^2\right).
\end{align}
The second moment of $w_{t+1}$ is
\begin{align}
    \E_x[w_{t+1}^2|w_t] &= w_t^2\E_x\left(1 - \lambda \frac{1}{S}\sum_i^S x_i^2\right)^2 \\
    &= w_t^2 \left(1 - 2\lambda \E[x^2] + \frac{\lambda^2}{S^2} \sum_{i,j}^S \E[x_i^2 x_j^2] \right)\\
    &= w_t^2 \left(1 - 2\lambda \E[x^2] + \frac{\lambda^2}{S^2}  \E[x^4] + \frac{\lambda^2(S-1)^2}{S^2} \E[x^2]^2\right).
\end{align}
Note that this equation applies to any $w_t\in \mathbb{R}$. Therefore, the second moment of $w_t$ is convergent if
\begin{equation}
    1 - 2\lambda \E[x^2] + \frac{\lambda^2}{S^2}  \E[x^4] + \frac{\lambda^2(S-1)^2}{S^2} \E[x^2]^2<1,
\end{equation}
which solves to
\begin{equation}\label{app eq: general ds condition}
    \lambda < \frac{2S^2 \E[x^2]}{\E[x^4] + (S-1)^2 \E[x^2]^2}.
\end{equation}
This condition applies to any data distribution. One immediate observation is that it only depends on the second and fourth moments of the data distribution and that both moments need to be finite for convergence at a non-zero learning rate. It is quite instructive to solve this condition under a few special conditions.


\paragraph{Gaussian Data Distribution}
The condition~\eqref{app eq: general ds condition} takes a precise form when the data is Gaussian. Using the fact that for a Gaussian variable $x$ with variance $\sigma^2$, $\E[x^4]=3\sigma^4$, the condition simplifies to
\begin{equation}
    \lambda < \frac{2}{\E[x^2]} \frac{S^2}{3S + (S-1)^2}.
\end{equation}
This is the same as Eq.~\eqref{eq: lambda DS}.

\paragraph{Bernoulli Dataset}
Another instructive case to consider is the case when there are only two data points in the data: $x_1$ and $x_2$. The moments are
\begin{equation}
    \begin{cases}
        \E[x^2] = \frac{1}{2}(x_1^2 + x_2^2),\\
        \E[x^4] = \frac{1}{2}(x_1^4 + x_2^4).
    \end{cases}
\end{equation}
When one of the data points, say $x_1$, is large, the condition becomes
\begin{equation}
    \lambda < \frac{2S^2 x_1^2}{2x_1^4 + (S-1)^2 x_1^4} = \frac{2S^2}{x_1^2} \frac{1}{2 + (S-1)^2}.
\end{equation}

\paragraph{Extreme Outlier}
We can also consider the general case of a finite dataset with a large outlier, $x_{\rm max}$. The condition is similar to the Bernoulli case. We have
\begin{equation}
    \begin{cases}
        \E[x^2] \approx \frac{1}{N}x_{\rm max}^2,\\
        \E[x^4] \approx \frac{1}{N}x_{\rm max}^4.
    \end{cases}
\end{equation}
The condition reduces to
\begin{equation}
    \lambda < \frac{2S^2}{N x_{\rm max}^2} \frac{1}{1 + \frac{(S-1)^2}{N}}.
\end{equation}
This can be seen as the generalization of the Bernoulli condition. When $S=1$, this condition becomes
\begin{equation}
    \lambda < \frac{2}{N x_{\rm max}^2}.
\end{equation}

There are many other interesting limits of this condition we can consider from the perspective of extreme value theory. However, this is beyond the scope of this work and we leave it as an interesting future work.


\subsection{Proofs}
\subsubsection{Proof of Theorem~\ref{theo: 1d symmetry lyapunov condition}}

\begin{proof}
Consider Eq.~\eqref{eq: sgd dynamics linearized}:
\begin{equation}
    \theta_{t+1} = \theta_t - \lambda \hat{H}(x) (\theta_t - \theta^*).
\end{equation}
Defining $w_t=\theta_t - \theta^*$, this equation can be written as
\begin{equation}\label{eq: proof linear dynamics}
    w_{t+1} = w_t - \lambda \hat{H}(x) w_t,
\end{equation}
which is mathematically equivalent to the case when $\theta^*=0$. Therefore, without loss of generality, we write the dynamics in the form of Eq.~\eqref{eq: proof linear dynamics} in this proof and the rest of the proofs.

Now, when $\hat{H} \propto nn^T$ is rank-$1$, we can multiply $n^T$ from the left on both sides of the dynamics to obtain
\begin{equation}
     n^Tw_{t+1} = n^Tw_t - \lambda h(x) n^Tw_t.
\end{equation}
The dynamics thus becomes one-dimensional in the direction of $n^T$. 

Let $h_t$ denote the eigenvalue of the Hessian of the randomly sampled batch at time step $t$. The dynamics in Eq.~\eqref{eq: 1d symmetry dynamics} implies the following dynamics
\begin{equation}
    \|n^Tw_{t+1}\| / \|n^Tw_t\| = |1 - \lambda h_t|,
\end{equation}
which implies
\begin{equation}
    \|n^Tw_{t+1}\| / \|n^T w_0\| = \prod_{\tau=1}^t |1 - \lambda h_\tau|.
\end{equation}
We can define auxiliary variables $z_t:= \log (\|n^T w_{t+1}\| / \|n^T w_0\|) - m$ and $m := \E [\log (\|n^T w_{t+1}\| / \|n^T w_0\|)] = t\E_x [\log |1 - \lambda h_t|]$. Let $\epsilon>0$. We have that 
\begin{align}
    \mathbb{P}(\|n^T w_t\| < \epsilon)  &= \mathbb{P}(\|n^T w_0\|e^{z_t + m} < \epsilon)\\
    &= \mathbb{P}\left(\frac{1}{{t}} z_t < \frac{1}{{t}} ( \log \epsilon / \|n^T w_0\| - m)\right)\\
    &= \mathbb{P}\left( \frac{z_t}{t} <  - \E_x \log |1 - \lambda h_t| + o(1)\right).
\end{align}
By the law of large numbers, the left-hand side of the inequality converges to $0$, whereas the right-hand side converges to a constant. Thus, we have, for all $\epsilon>0$,
\begin{equation}
    \lim_{t\to \infty} \mathbb{P}(\|n^T w_t\| < \epsilon) = \begin{cases} 1 &\text{if $m<0$};\\
    0 &\text{if $m>1$}.
    \end{cases}
\end{equation}
This completes the proof.
\end{proof}


\subsubsection{Proof of Proposition~\ref{prop: moment insufficiency}}

\begin{proof}
Part 2 of the proposition follows immediately from Proposition~\ref{prop: no convergence}, which we prove below. Here, we prove part 1. 

It suffices to consider a dataset with two data points for which $h(x_1)=1/\lambda$ and $h(x_2)=c_0$, where each data point is sampled with equal probability. Let $c_0$ be such that 
\begin{equation}
    |1-\lambda c_0|^p > \frac{1}{2}.
\end{equation}
Now, we claim that this dynamics converges to zero in probability. To see this, note that
\begin{equation}
    \|z_{t+1}\| = \begin{cases}
        \|z_{t}\||1 - \lambda/\lambda| =0 &\text{with probability $0.5$};\\
        \|z_{t}\||1 - \lambda c_0| &\text{with probability $0.5$}.
    \end{cases}
\end{equation}
Therefore, at time step $t$, $\p(z_t=0) \geq 1- 2^{-t}$, which converges to $0$. This means that $z_t$ converges in probability to $0$. 

Meanwhile, the $p$-norm is
\begin{align}
    \E[\|z_{t+1}\|^p] &= \frac{1}{2} \E[\|z_{t}\|^p] |1 - \lambda c_0|^p\\
    &\propto \frac{1}{2^t}|1 - \lambda c_0|^{pt} \to 0.
\end{align}
The convergence to zero follows from the construction that $|1-\lambda c_0|^p > \frac{1}{2}$. This completes the proof.
\end{proof}


\begin{proposition}\label{prop: no convergence}
    (No convergence in $L_p$.) For every strict saddle point $\theta^*$, there exists an initialization $\theta_0$ such that for any $\lambda\in \mathbb{R}_+$ and distance function $f(\cdot, \theta^*)$, $\theta^*$ is unstable in $f$.
\end{proposition}
\begin{proof}
    This problem is easy to prove when $\theta$ is one-dimensional. For a high-dimensional $\theta$, the dynamics of SGD is
    \begin{equation}
        \theta_{t+1} = (I-\lambda \hat{H}_t)\theta_t.
    \end{equation}
Note that the expected value of $\theta_t$ is the same as the gradient descent iterations:
\begin{equation}
    \E[\theta_{t+1}] = (I-\lambda \E[\hat{H}])\E[\theta_t] = (I-\lambda \E[\hat{H}])^t \theta_0,
\end{equation}
which diverges if $\theta_0$ is in one of the escape directions of $\E[\hat{H}]$, which exist by the definition of strict saddle points. 

Taking the $f-$distance of both sides and taking expectation, we obtain
\begin{align}
    \E[f(\theta_{t}, \theta^*)] & \geq f(\E[\theta_t], \theta^*)\\
    &= f\left((I-\lambda \E[\hat{H}])^t\theta_0, \theta^*\right)\nto 0.
\end{align}
The first line follows from the fact that the distance function is convex by definition, and so one can apply Jensen's inequality.

Therefore, as long as $\theta_0$ overlaps with the concave directions of $\E[\hat{H}]$, the argument of $f$ diverges, which implies that the distance function converges to a nonzero value. The expected value of $\theta_t$ is just the gradient descent trajectory, which diverges for any strict saddle point. 

By definition, $\E[\hat{H}]$ contains at least one negative eigenvalue, and so the directions that do not overlap with this direction are strict linear subspaces with dimension lower than the the total available dimensions. This is a space with Lesbegue measure zero. The proof is complete.
\end{proof}




\subsubsection{Proof of Theorem~\ref{theo: main theorem}}
Let us first state the Furstenberg-Kesten theorem.
\begin{theorem} (Furstenberg-Kesten theorem)
    Let $X_1,\ X_2,\ X_3,...$ be independent random square matrices drawn from a metrically transitive time-independent stochastic process and $\E[\log_+\|X^1\| < \infty]$, then\footnote{$\log_+(x) =\max(\log x,0)$.}
    \begin{equation}
        \lim_{n\to \infty} \frac{1}{n}\log\|X_1 X_2...X_n\|= \lim_{n\to \infty}\E \left[\frac{1}{n}\log\|X_1 X_2...X_n\|\right]
    \end{equation}
    with probability $1$, where $\|\cdot\|$ denotes any matrix norm.
\end{theorem}
Namely, the Lyapunov exponent of every trajectory converges to the expected value almost surely. Essentially, this is a law of large numbers for the Lyapunov exponent. 

Now, we present the proof of Theorem~\ref{theo: main theorem}.

\begin{proof}
First of all, we define $m_t= \log \|\theta_t - \theta^*\|$ and $z_t = m_t - \E[m_t]$. By definition, we have
\begin{align}
    \mathbb{P}( g_t < \epsilon)  &= \mathbb{P}(e^{z_t + m_t} < \epsilon)\\
    &= \mathbb{P}\left(\frac{1}{{t}} (z_t + \E[m_t]) < \frac{1}{{t}} \log \epsilon \right)\\
    &= \mathbb{P}\left( \frac{1}{{t}} (z_t + \E[m_t])  <  o(1)\right).\label{eq: proof prob 1}
\end{align}

We can lower bound this probability by
\begin{equation}
    \mathbb{P}\left( \frac{1}{{t}} (z_t + \E[m_t])  < o(1)\right) \geq \mathbb{P}\left( \frac{1}{{t}} \max_{\theta_0}(z_t + \E[m_t])  < o(1)\right).
\end{equation}

By the definition of SGD, we have 
\begin{align}
    \frac{1}{t} \max_{\theta_0}\left(z_t(\theta_0) + \E[m_t]\right) &= \frac{1}{t} \max_{\theta_0}\log \left\| \prod_i^t(I - \lambda \hat{H}_i) (\theta_t -\theta_0) \right\|.
\end{align}
By the Furstenberg-Kesten theorem~\citep{furstenberg1960products}, this quantity converges to the constant $\Lambda = \lim_{t\to\infty}\E[m_t]/t \in \mathbb{R}$ almost surely. Namely, $z_t/t$ converges to $0$ for almost every SGD trajectory.

Thus, for every $\epsilon$, if $\Lambda <0$, Eq.~\eqref{eq: proof prob 1} can be bounded as
%\begin{equation}
%    \lim_{t\to \infty}\mathbb{P}( g_t < \epsilon) < \mathbb{P}(\Lambda<0).
%\end{equation}
\begin{equation}
    \lim_{t\to \infty}\mathbb{P}( g_t < \epsilon) = \mathbb{P}(\Lambda<0) = 1.
\end{equation}
Because $\Lambda$ is a constant, we have that if $\Lambda <0$, all trajectories from all initialization converge to $0$. This finishes the first part of the proof. For the second part, simply let $z_t$ be the trajectory starting from the trajectory that achieves the maximum Lyapunov exponent. Again, this dynamics escapes with probability $1$ by the Furstenberg-Kesten theorem. The proof is complete.
\end{proof}


\subsubsection{Proof of Proposition~\ref{prop: phase diagram}}
\begin{proof}
    We consider the dynamics of SGD around a saddle:
\begin{equation}
    \ell = -\chi \sum_i u_i w_i,
\end{equation}
where we have combined $xy$ into a single variable $\chi$. The dynamics of SGD
is 
\begin{equation}
\begin{cases}
    w_{i,t+1} = w_{i,t} +\lambda \chi u_{i,t};\\
    u_{i,t+1} = u_{i,t} +\lambda \chi w_{i,t}.
\end{cases}
\end{equation}
Namely, we obtain a set of coupled stochastic difference equations. Since the dynamics is the same for all values of the index $i$, we omit $i$ from now on. This dynamics can be decoupled if we consider two transformed parameters: $h_t = w_t + u_t$ and $m_t = w_t -u_t$. The dynamics for these two variables is given by
\begin{equation}
\begin{cases}
    h_{t+1} = h_{t} +\lambda \chi h_{t};\\
    m_{t+1} = m_{t} -\lambda  \chi m_{t}.
\end{cases}
\end{equation}
We have thus obtained two decoupled linear dynamics that take the same form as that in Theorem~\ref{theo: 1d symmetry lyapunov condition}. Therefore, as immediate corollaries, we know that $h$ converges to $0$ if and only if $\E[\log|1+\lambda \chi|] <0$, and $m$ converges to $0$ if and only if $\E[\log |1- \lambda \chi|] <0$.

When both $h$ and $m$ converge to zero in probability, we have that both $w$ and $u$ converge to zero in probability. For the data distribution under consideration, we have
\begin{equation}
    \E[\log|1+\lambda \chi|] = \frac{1}{2} \log\left|(1+\lambda)(1+\lambda a)\right|
\end{equation}
and 
\begin{equation}
    \E[\log|1-\lambda \chi|] = \frac{1}{2} \log\left|({1-\lambda})({1-\lambda a})\right|.
\end{equation}
There are four cases: (1) both conditions are satisfied; (2) one of the two is satisfied; (3) neither is satisfied. These correspond to four different phases of SGD around this saddle.
\end{proof}


\end{document}