\documentclass[10pt]{article}
\renewcommand{\baselinestretch}{1.0}
\usepackage{amsmath,amsthm,verbatim,amssymb,amsfonts,amscd, graphicx}
\usepackage{graphics}
\usepackage{centernot}
\usepackage{authblk}
\topmargin0.0cm
\headheight0.0cm
\headsep0.0cm
\oddsidemargin0.0cm
\textheight23.0cm
\textwidth16.5cm
\footskip1.0cm
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem*{remark}{Remark}
\newtheorem{proposition}{Proposition}
\newtheorem*{surfacecor}{Corollary 1}
\newtheorem{conjecture}{Conjecture}
\newtheorem{question}{Question} 
\newtheorem{definition}{Definition}

%\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}

\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}

\usepackage{paralist}



\usepackage[colorlinks,linkcolor=blue,citecolor=blue]{hyperref}


\newcommand{\fsl}[1]{{\centernot{#1}}}
\setlength{\fboxsep}{0pt}
\setlength{\fboxrule}{.1pt}

\usepackage[bottom]{footmisc}
\usepackage{caption}
\usepackage{subcaption}
%\usepackage{subfloat}
%\usepackage{subfig}
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{multirow}
\usepackage{amsthm}
\usepackage{color}
\usepackage{MnSymbol}
\usepackage{makecell}
\usepackage{arydshln}
\usepackage{amsmath}
\usepackage[dvipsnames]{xcolor}
\usepackage{caption} 
\usepackage{natbib}
%\usepackage{epsfig}
%\usepackage{subcaption}
%\usepackage{float}
\usepackage{textcomp}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{csquotes}
\newcommand{\re}{\mathrm{Re}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{{\rm Var}}
\def\fr#1#2{{\textstyle\frac{#1}{#2}}}

\begin{document}
%\[ \fbox{$\Box$} \fbox{$\hat\Box$} \fbox{$\tilde\Box$} \]

\title{The Probabilistic Stability of Stochastic Gradient Descent}
\author{Liu Ziyin$^1$, Botao Li$^2$, Tomer Galanti$^3$, Masahito Ueda$^{1,4}$\\
{\small
\textit{$^1$Department of Physics, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan}\\
\textit{$^2$Laboratoire de Physique de lâ€™Ecole normale sup\'erieure, ENS,
Universit\'e PSL,}\\
\textit{CNRS, Sorbonne Universit\'e, Universit\'e de Paris Cit\'e, Paris, France}\\
\textit{$^3$ Center of Brains, Minds and Machines (CBMM), Massachusetts Institute of Technology, Cambridge, MA, USA}\\
\textit{$^4$RIKEN Center for Emergent Matter Science (CEMS), Wako, Saitama 351-0198, Japan}\\
}
}
\maketitle
\vspace{-1em}
\begin{abstract}
A fundamental open problem in deep learning theory is how to define and understand the stability of stochastic gradient descent (SGD) close to a fixed point. Conventional literature relies on the convergence of statistical moments, esp., the variance, of the parameters to quantify the stability. We revisit the definition of stability for SGD and use the \textit{convergence in probability} condition to define the \textit{probabilistic stability} of SGD. The proposed stability directly answers a fundamental question in deep learning theory: how SGD selects a meaningful solution for a neural network from an enormous number of solutions that may overfit badly. To achieve this, we show that only under the lens of probabilistic stability does SGD exhibit rich and practically relevant phases of learning, such as the phases of the complete loss of stability, incorrect learning, convergence to low-rank saddles, and correct learning. When applied to a neural network, these phase diagrams imply that SGD prefers low-rank saddles when the underlying gradient is noisy, thereby improving the learning performance. This result is in sharp contrast to the conventional wisdom that SGD prefers flatter minima to sharp ones, which we find insufficient to explain the experimental data. We also prove that the probabilistic stability of SGD can be quantified by the Lyapunov exponents of the SGD dynamics, which can easily be measured in practice. Our work potentially opens a new venue for addressing the fundamental question of how the learning algorithm affects the learning outcome in deep learning.

    

\end{abstract}
%Among the enormous number of solutions to fit the data with a neural network, stochastic gradient descent (SGD), the training algorithm of deep learning, selects one that generalizes surprisingly well. 




\section{Introduction}

Stochastic gradient descent (SGD) is the main workhorse for optimizing deep learning models. A fundamental problem in deep learning theory is to characterize how SGD selects the solution of a deep learning model, which often exhibits remarkable generalization capability. At the heart of this problem is the \textit{stability} of SGD because the models trained with SGD stay close to the solution where the dynamics is stable and moves away from unstable solutions. Solving this problem thus hinges on having a good definition of the stability of SGD. 
The established literature often defines the stability of SGD as a function of the variance of the model's parameters or gradients during training. The hidden assumption behind the mainstream thought is that if the variance diverges, then the training becomes unstable~\citep{wu2018sgd, zhu2018anisotropic, liu2020improved, liu2021noise, ziyin2022strength}. In some sense, the thought that the variance of the parameters matters the most is also an underlying assumption of the deep learning optimization literature, where the utmost important quantity is how fast the variance and the expected distance of the parameters decay to zero \citep{vaswani2019fast, gower2019sgd}. We revisit this perspective and show that a variance-based notion of stability is insufficient to understand the empirically observed stability of training of SGD. For example, we demonstrate natural learning settings where the variance of SGD diverges, yet the model still converges with high probability. %Our work revisits this perspective and shows that a variance-based notion of stability is insufficient to understand the empirically observed stability of training of SGD. For example, we demonstrate natural learning settings where the variance of SGD diverges even though the model still converges with high probability.


% As an alternative, we promote a \textit{probabilistic} perspective on the stability of SGD that is based on a stochastic generalization of the Lyapunov exponent \citep{lyapunov1992general}, which takes a fundamental role in the study of dynamical systems \citep{eckmann1985ergodic}. 

In this work, we study the \textit{convergence in probability} condition to understand the stability of SGD. We then show that this stability condition can be quantified with a stochastic extension of the Lyapunov exponent \citep{lyapunov1992general}, a quantity rooted in the study of dynamical systems and has been wellunderstood in physics and control theory \citep{eckmann1985ergodic}.

The main contribution of this work is to propose a new notion of stability that sheds light on how SGD selects solutions and multiple deep-learning phenomena that can only be understood by this notion. Perhaps the most important implication of our theory is the characterization of the highly nontrivial and practically important phase diagram of SGD in a neural-network loss landscape (as illustrated in Figure~\ref{fig: phase diagram}).


%Technically, 
%\begin{enumerate}
%    \item we show that linear stability theory is not sufficient to describe the stability of SGD;
%    \item we employ a quantity called stochastic Lyapunov exponent and a new framework for understanding the stability of SGD, which correctly captures a series of optimization and learning phenomena of SGD;
%    \item we demonstrate that with this new notion of stability, our theory captures and explains a series of training phenomena that previous theories fail to explain. For example, we show SGD is much more stable than previously thought and that the use of minibatch sampling can make SGD even more stable than the deterministic GD.
%\end{enumerate}

%\paragraph{Organization.} In Section~\ref{sec: background} we describe the conventional notion of stability and establish the necessary background for understanding our result and the prior literature. Our primary theoretical findings are presented in Section~\ref{sec: theory}. We explore the implications of our theory in Section~\ref{sec: implications} and address critical unresolved issues and future directions in Section~\ref{sec: discussion}.

% We introduce the concept of conventional stability theory in Section~\ref{sec: background}, which establishes the necessary background for understanding our result and discusses the most pertinent prior literature. The main theoretical results are introduced in Section~\ref{sec: theory}. We discuss the major implications of our theory in Section~\ref{sec: implications} and the important open problems and future steps in Section~\ref{sec: discussion}.



\section{Problem Setting}\label{sec: background}

In this section, we introduce the problem setting, describe the standard linear stability theory, and discuss its implications for understanding the stability of minibatch SGD.

\begin{figure}[t!]
    \centering
    
    \includegraphics[width=1.02\linewidth]{plots/nc_figure1_revised.png}
    \vspace{-2em}
    \caption{\small \textbf{SGD exhibits a complex phase diagram through the lens of probabilistic stability.} (a1) Landscape of a simple two-layer tanh neural network: $f(x)=u\tanh (wx)$. Triangles show the location of the global minima. The star shows the origin, which is a saddle point. (a2) With a large learning rate, SGD converges to the saddle in probability, even though it escapes in expectation. Here, $m$ is the Lyapunov exponent times $t$, which agrees well with a typical learning trajectory. The inset shows the distribution of the parameters before and after training. (b) A phase diagram of SGD. For a matrix factorization saddle point, the dynamics of SGD can be categorized into at least five different phases. Phase \textbf{I}, \textbf{II}, and \textbf{IV} correspond to a successful escape from the saddle. Phase \textbf{III} is the case where the model converges to a low-rank saddle point. Phase \textbf{I} corresponds to the case $w_t \to_p u_t$, which signals correct learning. In phase \textbf{Ib}, the model also converges in variance. Phase \textbf{II} corresponds to stable but incorrect learning, where $w_t \to_p -u_t$. Phase \textbf{IV} corresponds to complete instability. See Section~\ref{sec: phase diagram} for a detailed discussion. (c1) Numerical results on training with ResNet18 for an image classification task. The color shows the penultimate-layer representation of Resnet18 trained with different levels of label noise. The inset shows the estimated boundary of rank $511$ and $1$. We observe that the phase boundary agrees qualitatively with simple two-layer networks without nonlinearity (c2) and with the tanh activation (c3), for which phase boundaries can be theoretically computed. (c4) The phase diagram is not limited to SGD but also applies to models trained with Adam, suggesting a universal effect that may be attributable to all first-order learning algorithms with minibatch sampling.}
    \label{fig: phase diagram}
\end{figure}

%\begin{wrapfigure}{r!}{0.35\textwidth}
%\vspace{-3em}
%\centering
%\includegraphics[width=\linewidth]{plots/phase_diagram_edited.png}
%\vspace{-2em}
%\caption{\textbf{Phase diagram of SGD for training objective}: $\ell(w,x)=[\sum_i u_i w_i x -y(x)]^2$. For a matrix factorization saddle point, the dynamics of SGD can be categorized into at least five different phases. Phase \textbf{I}, \textbf{II}, and \textbf{IV} correspond to a successful escape from the saddle. Phase \textbf{III} is the case where the model converges to a low-rank saddle point. Phase \textbf{I} corresponds to the case $w_t \to_p u_t$, which signals correct learning. In phase \textbf{Ib}, the model also converges in variance. Phase \textbf{II} corresponds to stable but incorrect learning, where $w_t \to_p -u_t$. Phase \textbf{IV} corresponds to complete instability. See Section~\ref{sec: phase diagram} for a detailed discussion and Section~\ref{sec: nn experiment} for its empirical validation.}\label{fig: phase diagram}
%\vspace{-2em}
%\end{wrapfigure}

\paragraph{Definitions.} In a standard supervised learning setting, we are given a data distribution $p(x,y)$ from which independently sampled input-label pairs $(x,y)$ are drawn. For notational concision, we let $y=y(x)$ be a function of $x$, so that $p(x,y) = p(x)$. We allow $p(x)$ to be a uniform distribution over samples from a given dataset of size $N$ or a distribution over a continuous space. The training loss is defined as the empirical expectation $L(w) = \E_x [\ell(w, x)]$ of a sample-wise loss function $\ell(w,x)$, where $w$ is the vectorized model parameters. 

Model training proceeds with the stochastic gradient descent (SGD) algorithm with a batch size $S$ and learning rate $\lambda$. At each iteration $t$, SGD computes the gradient by using a randomly drawn mini-batch of $S$ samples ${(x_{j})}^{S}_{j=1}$ from $p(x)$. Then, SGD updates the parameters $w$ according to the following rule:
\begin{equation}
w_{t+1} = w_t - \frac{\lambda}{S}\sum^{S}_{j=1} \nabla_{w}\ell(w_t,x_j).
\end{equation}
In this definition, we can handily define gradient descent (GD) as the infinite $S$ limit of SGD. To study the stability of SGD, we will focus on the notion of convergence in probability, denoted by $\to_p$. The weight parameters $w_t$ is said to converge to $c$ in probability if for any $\epsilon>0$ $\lim\limits_{t \to \infty} \mathbb{P}(|w_t - c| > \epsilon) = 0$. 

A choice of the learning rate $\lambda$ and that of the batch size constitutes an important practical problem that involves complicated tradeoffs. On the one hand, one wants to use a large learning rate and a small batch size so that the model trains faster and generalizes better \citep{Keskar2017, hoffer2017train, he2019control, li2019towards, galanti2022sgd}. On the other hand, one wants to use a small learning rate and a large batch size to keep the training stable and convergent. At the core of this tradeoff thus lies the notion of stability. To understand the stability of an interpolation minimum, we follow the standard practice and consider the linearized dynamics of SGD around a local minimum $w^*$:
\begin{equation}\label{eq: sgd dynamics linearized}
    w_{t+1}-w^* = w_t - \frac{\lambda}{S}\sum^{S}_{j=1} \hat{H}(w^*,x_j)(w_t-w^*),
\end{equation}
where $\hat{H}(w,x) := \nabla_w^2 \ell(w,x)$ is the sample-wise Hessian.

The previous literature focuses on variants of the following notion of stability. GD is said to be stable at $w^*$ if $w_t-w^*$ converges to zero. For SGD, the learning is considered stable if $\E[||w_t -w^*||^2] \to 0$. In probability theory, this condition is equivalent to the condition that $w_t$ converges $w^*$ in mean square. It is elementary to show that convergence in mean square implies the convergence in probability but not vice versa. We will show that these two convergence conditions are dramatically different for SGD and that convergence in mean square is too strong a condition to understand the actual learning behavior of SGD in deep learning.

%This work mainly focuses on analyzing the case where the dynamics does not fluctuate asymptotically. Sometimes, the dynamics is allowed to fluctuate, and the condition can be weakened to that $\V[w_t]$ is upper bounded by a constant. However, a crucial perspective shared by the mainstream literature is that the variance of SGD remaining finite is a \textit{prerequisite} for SGD stability \citep{wu2018sgd}. 

%This work primarily focuses on analyzing cases where there is no asymptotic fluctuation. In some instances, it may be possible to allow for fluctuation in the dynamics, such that the variance $\V[w_t]$ remains upper-bounded by a constant. It is widely acknowledged in the literature that the stability of stochastic gradient descent (SGD) requires that its variance remain finite, as reported in~\cite{wu2018sgd}. %Notably, \cite{wang2021convergence} studied the convergence of SGD in $L^p$ norm under an infinite gradient variance and a polynomially decreasing learning rate. However, they do not consider the case of a constant learning rate.


\paragraph{Stability of a minimal model.} To investigate the stability of the SGD algorithm, we examine a simple one-dimensional linear regression problem. The training loss function for this problem is defined as $L(w)=\frac{1}{N}\sum_i^{N}(wx_i - y_i)^2$. 

For GD, the dynamics diverge when the learning rate is larger than twice the inverse of the largest eigenvalue of the Hessian. To see this, let $H=\E_x[\hat{H}(w^*,x)]$ denote the Hessian of $L$ and $h$ its largest eigenvalue. The dynamics of GD leads to $||w_{t+1}|| = ||w_0(I - \lambda H)^t|| \propto |1-\lambda h|^t$. Divergence happens if and only if $|1 -\lambda h| > 1$. The range of viable learning rates is thus given by:
\begin{equation}\label{eq: lambda gd}
\lambda_{\rm GD} \leq 2/h = 2/\E_x[x^2].
\end{equation}
Naively, one would expect that a similar condition approximates the stability condition for the case when mini-batch sampling is used to estimate the gradient. This has indeed been argued to be the case in the recent works \citep{wu2018sgd, liu2021noise, ziyin2022strength}. 



For SGD, the stability condition is the same as the condition that the second moment of SGD decreases after every time step, starting from an arbitrary initialization (see Appendix~\ref{app sec: theory}):
\begin{equation}\label{eq: lambda DS}
    \lambda_{\rm DS} \leq  \frac{2S^2 \E[x^2]}{\E[x^4] + (S-1)^2 \E[x^2]^2}.%\frac{2}{h}\frac{S^2}{3S + (S-1)^2}.
\end{equation} 
Also related is the stability condition proposed by \cite{ziyin2022strength}, who showed that starting from a stationary distribution, $w$ stays stationary under the condition $\lambda_{\rm SS} < \frac{2}{h} \frac{1}{1+1/S}$, which we call the stationary linear stability condition (SS). Namely, when minibatch sampling is used, one expects the dynamics to be less stable by a factor of $1 + 1/S$. When we have batch size $1$, the stability condition halves: $\lambda < 1/h$. For all stability conditions, we denote the maximum stable learning rate with an asterisk: $\lambda^*$. The most important prediction made by the linear stability theories is that SGD prefers flatters minima to sharper ones because the linear stability is lost as one increases the learning rate above $2/h$, which is believed to lead to  better performance \citep{zhu2018anisotropic, wu2018sgd, xie2020diffusion, wu2022does}. On the contrary, we will show that this mechanism of minimum selection is not what probabilistic stability implies. Throughout this work, we use the term ``linear stability theory" to refer to any theory of stability that is based on the statistical moments in order to emphasize their difference from probabilistic stability, which is our main proposal.

%Both conditions contain two essential features that have influenced our present understanding of SGD: (1) SGD is less stable than GD, which can be seen from the relation $\lambda_{\rm DS}^* < \lambda_{\rm SS}^* < \lambda^*_{\rm DS}$; (2) stability decreases with decreasing batch size. In this work, we show that, rather surprisingly, neither statement is true when we look at the convergence in probability condition instead.


%such analyses are not sufficient to understand the commonly observed stability-related phenomenon of SGD and that SGD is much more stable can has been conventionally understood. %We will see that our theory, in sharp contrast to the linear stability theory, suggests that both conventional understandings of SGD stability are not accurate, if not incorrect.

%\paragraph{The inadequacy of the linear stability theory.} Also, the prediction of the linear stability theory seems quite incompatible with the prevalence of the edge-of-stability (EOS) phenomenon in deep learning \citep{cohen2021gradient}. Standard linear stability theory predicts that the fluctuation of the model parameter scales inversely to the stability condition: $\V[w_\infty] \propto 1/(\lambda^* -\lambda)$ \citep{liu2021noise}. Meanwhile, \cite{cohen2021gradient} found that SGD on neural networks very likely stabilizes around a local minimum whose $\lambda^*$ is approximately equal to the actual learning rate $\lambda$: $\lambda \approx \lambda^*$. This implies that SGD has a very large, if not infinite, fluctuation. Close to a local minimum, the loss is also proportional to the fluctuation: $L(w_t) \propto \V[w_t]$. The linear stability theory thus predicts a very high loss value, which contradicts the empirical observation that the training loss often stabilizes at a rather low value even at the EOS. Therefore, the EOS phenomenon is also a piece of evidence that a variance-based stability theory cannot completely characterize the practical problems and empirical phenomena in deep learning. A different notion that allows for stability even at a diverging variance is needed.  %T the assumption of the linear stability is thus 


\section{Convergence in Probability Condition for SGD}\label{sec: theory}

We now present the main theoretical results of this work. We first show that there exist critical learning rates that are far larger than $2/h$ for which SGD converges in probability to the global minimum. We then show that such learning rates are not special or isolated points but span a rather large space with a nonzero measure. Lastly, we prove the connection of the notion of convergence in probability to a stochastic generalization of the Lyapunov exponent, which could serve as a foundation of future study of the probabilistic stability in the context of deep learning. 

\subsection{Critical Learning Rates}\label{sec: main result 1}
We begin by demonstrating a specific case to illustrate our analysis. Let us consider a simple interpolation regime, where all data points $(x,y) \in \mathbb{R}^{2}$ lie on a straight line. In this situation, the loss function has a unique global minimum of $w=y_i/x_i$ for any $x_i$. Our initial question is: what is the maximum learning rate that can be used for SGD without causing divergence? We prove that for all $i$, SGD is convergent in probability if $\lambda = 1/x_i^2$. Therefore, the largest stable learning rate is roughly given by:
\begin{equation}
\lambda_{\max} = 1/x^2_{\min}.
\end{equation}



%Namely, the model converges to the global minimum almost surely at an exponential rate. However, for $\lambda=1/x^2_i$, the convergence in mean is not always possible. As mentioned earlier, convergence in mean occurs when $\lambda \leq \lambda_{DS}^*$ which does not hold when $\lambda = 1/x^2_{\min}$ and $x_{\min} < \E[x_i]/2$ that happens for standard datasets. 

%, especially when $x_{\min}$ is an outlier. 
%which can easily happen for standard datasets, not to mention the pathological case where $1/x_{\min}^2$ is an outlier and arbitrarily small.

%This result implies that the maximum convergent learning rate for SGD is much larger than the maximal learning rate necessary for convergence in mean (cf. \eqref{eq: lambda DS}). In particular, for a fixed $\E[x^2]$, $x_{\min}^2$ can be arbitrarily small, and a maximum stable learning rate can be arbitrarily large. Another implication is that for SGD, the stability strongly depends on individual data points, not just a summary statistics of the whole dataset.
However, for these special choices of learning rates, linear stability is not always guaranteed. As mentioned earlier, convergence in mean occurs when $\lambda \leq \lambda_{DS}^*$, but this condition does not hold when $\lambda = 1/x^2_{\min}$ and $x_{\min} < \E[x_i]/2$, which is often the case for standard datasets. This result shows that the maximal learning rate that ensures stable training can be much larger than the maximal learning rate required for convergence in mean (cf. \eqref{eq: lambda DS}). For a fixed value of $\E[x^2]$, $x_{\min}^2$ can be arbitrarily small, which means that the maximal stable learning rate can be arbitrarily large. Another consequence of this result is that the stability of SGD depends strongly on individual data points and not just on summary statistics of the whole dataset. % This example illustrates the ``super-stability'' of SGD close to some fixed points: even with a learning rate $\lambda > 2/\E[x^2]$, where the variance of the gradient and parameter diverge, SGD still converges to the fixed point with high probability. %Therefore, traditional linear stability theories are unable to explain the stability of mini-batch SGD.

% To summarize, this example demonstrates the ``super stability'' of SGD: \textit{with a learning rate $\lambda > 2/\E[x^2]$, the variance of the gradient and parameter are divergent, yet, SGD converges to the global minimum with high probability}. Linear stability theories thus cannot explain the stability of mini-batch SGD.
 

\subsection{Are Such Learning Rates Isolated?}\label{sec: main result 2}
% The next important question is whether such learning rates are very special or not and whether such convergence is robust or not. In different words, we are interested in whether one can still converge if we perturb the learning by a small amount. The answer is yes. We show that there is a set of non-zero measure in the neighborhood of the critical learning rates identified in Proposition~\ref{prop: special learning rates} such that the model still converges. A technical difference is that the convergence is no longer with probability $1$ but in probability. We note that convergence in probability is sufficient to imply that SGD is stable at the corresponding learning rate for all practical considerations because the probability of observing an unstable trajectory decreases to zero in the large training step limit.

The important question is whether the critical learning rates in Proposition~\ref{prop: special learning rates} are special or not, and whether the convergence is robust against perturbations in the learning rate. The answer to both questions is yes. We show that there exists a wide neighborhood close to the critical learning rates such that the model still converges in probability. We note that convergence in probability is sufficient to ensure that SGD is stable at the corresponding learning rate for all practical purposes, as the probability of an unstable trajectory being observed decreases to zero as the number of training steps increases.

Let $v_t := w_t - y_t/x_t = w_t - w^*$. The SGD dynamics in terms of $v_t$ can be written as
\begin{equation}\label{eq: zero label dynamics}
     v_{t+1} = v_t - \lambda x_t^2 v_t.
\end{equation}
Apparently, the unique global minimum is $v^*=0$, i.e., $w^* = y_t/x_t$. 
Furthermore, because the dynamics of Eq. \eqref{eq: zero label dynamics} is independent of $w^*$, we can assume $w^* = y_t/x_t =0$. The following proposition gives the convergence condition.

\begin{proposition}\label{prop: learning rates are not isolated}
    Let $\lambda$ be such that $\E_x[\log |1-\lambda x^2|]\neq 0$. Then, for any $w_0$, $w_t\to_p w^*$ if and only if $\E_x[\log |1-\lambda x^2|]< 0$.
\end{proposition}


It is worth remarking that this condition is distinctively different from the case when the gradient noise is a parameter-independent random vector. For example, \cite{liu2021noise} showed that if the gradient noise is a parameter-independent gaussian, SGD diverges in distribution if $\lambda >2/h$. This suggests that the fact that the noise of SGD is $w$-dependent is crucial for its probabilistic stability.

This result highlights the importance of the quantity $m := \E_x[\log|1-\lambda x^2|]$ and its sign in understanding the convergence of $w_t$ to the global minimum. When $m$ is negative, the convergence to the global minimum occurs. If $m$ is positive, SGD becomes unstable. We can determine when $m$ is negative for a training set of finite size by examining the following equation:
\begin{equation}\label{eq: convergent condition}
m = \frac{1}{N} \sum_i \log |1-\lambda x_i^2|,
\end{equation}
which is negative when $\lambda$ is close to $1/x_i^2$ for some $i \in {1,\dots,N}$. What is the range of $\lambda$ values that satisfy this condition? Suppose that $\lambda$ is in the vicinity of some $1/x_i^2$: $\lambda = \delta\lambda + 1/x_i^2$, and the instability is caused by a single outlier data point $x_{\rm out} \gg 1$. Then, $m$ decided by the competitive contributions from the outlier, which destablizes training, and $x_i^2$, which stablizes training, and the condition is approximately $|1 - \lambda x_i^2| < {1}/{|\lambda x_{\rm out}^2|}$. Because $\lambda \approx 1/x_i^2$, this condition leads to:
\begin{equation}
|\delta \lambda | < {x_i^2}/{x_{\rm out}^2}.
\end{equation}
This is a small quantity. However, if we change the learning rate to the stability region associated with another data point $x_j$ as soon as we exit the stability region of $x_i$, we still maintain stability. Therefore, the global stability region depends on the density of data points near $x_i$. Assuming there are $N$ data points near $x_i$ with a variance of $\sigma^2$, the average distance between $x_i$ and its neighbors is approximately $\sigma^2/N$. As long as $\sigma^2/N < {x_i^2}/{x_{\rm out}^2}$, SGD will remain stable in a large neighborhood. In practical terms, this means that when the number of data points is large, SGD is highly resilient to outliers in the data as shown in Figure~\ref{fig:robustness}. We see that the region of convergence in probability is very dramatic, featuring stripes of convergent regions that correspond to $1/x_i^2$ for each data point and divergent regions where $m>0$.

\begin{figure}
    \centering
    %\begin{subfigure}{0.2\linewidth}
    %\includegraphics[width=\linewidth]{plots/temp/25_toy_extreme.png}
    %    \vspace{-2em}
    %\caption{$N=25$}
    %\end{subfigure}
    % \begin{subfigure}{0.22\linewidth}
    %\includegraphics[width=\linewidth]{plots/temp/25_toy_extreme2.png}
    %    \vspace{-2em}
    %\caption{$N=25$}
    %\end{subfigure}   
    \begin{subfigure}{0.23\linewidth}
    \includegraphics[width=\linewidth]{plots/temp/1_toy.png}
        \vspace{-2em}
    \caption{\small $N=2$}
    \end{subfigure}
    \begin{subfigure}{0.23\linewidth}
    \includegraphics[width=\linewidth]{plots/temp/5_toy.png}
        \vspace{-2em}
    \caption{\small $N=6$}
    \end{subfigure}
    \begin{subfigure}{0.23\linewidth}
    \includegraphics[width=\linewidth]{plots/temp/500_toy_extreme.png}
        \vspace{-2em}
    \caption{\small $N=500$}
    \end{subfigure}
     \begin{subfigure}{0.255\linewidth}
    \includegraphics[width=\linewidth]{plots/temp/500_toy_extreme2.png}
    \vspace{-2em}
    \caption{\small $N=500$}
    \end{subfigure}  

    \vspace{-1em}
    \caption{\small {\bf  Stability of SGD against a single outlier data} in a dataset of size $N$. Yellow denotes where SGD converges in probability, and dark blue denotes divergence. We control the norm of the first data point ($x_1^2$) while sampling the rest data from a standard normal distribution. (a-c) stability of SGD for different sizes of the dataset; (d) zoom-in of (c) at a small learning rate. The grey dashed curves show $\lambda_{GD}^*$, and the green dashed curve shows $\lambda_{GD}^*/N$. The intermediate finite learning rates are robust against outliers in the data, whereas the smallest learning rates are strongly sensitive to outliers in the data.} 
    \label{fig:robustness}
\end{figure}



%The satisfaction range of this condition is easy to check when there are just two samples: $x_1$ and $x_2$. Let $x_2$ be the data with a smaller norm. Here, the condition for convergence is $|1-\lambda x_1||1-\lambda x_2|< 1$. The largest possible learning rate can be found to be 
%\begin{equation}
%    \lambda < \frac{x_1^2 + x_2^2}{x_1^2x_2^2} = \frac{1}{x_1^2} + \frac{1}{x_2^2}.
%\end{equation}
%If the two data points have a similar norm, then $\lambda \approx 2/\E[x^2]$, in agreement with the standard theory. When one of the data points has a very small or very large norm, the maximum allowable learning rate becomes $1/x_2^2$, which can be arbitrarily large for a sufficiently small $x_2^2$. This is the first surprising aspect of minibatch SGD that our theory implies: SGD can be stable even if the learning rate is arbitrarily far away from what the standard stability theory requires.

%Another important consequence is that SGD can also diverge in the range where the standard theory predicts stability. To see this, we consider the case when we draw $N-1$ data points from a standard Gaussian distribution with variance $1$, manually create an outlier data point $x_1$, and vary its magnitude from small to large. We plot the stability and convergence of the model according to Eq.~\eqref{eq: convergent condition} across a wide range of $\lambda$. 

%we analyze the case when the two data points are normalized such that $x_1^2 + x_2^2 = 1$. For this case, the standard theory predicts a maximally stable learning rate of $4$, independent of the data distribution. However, our theory predicts something quite dramatic. Solving for the condition of convergence, one can find the analytical boundaries as shown in Figure~\ref{fig:toy example}.\footnote{We do not show the explicit forms here because they are quite complicated to write down, and one does not gain much from inspecting those equations. Curious readers can investigate the following link: \url{https://www.wolframalpha.com/input?i=1+1+>+|1-x(1-b^2)||1-+xb^2|}.} 

%\begin{figure}
%    \centering
%    \includegraphics[width=0.194\linewidth]{plots/temp/1_toy.png}
    %\includegraphics[width=0.194\linewidth]{plots/temp/2_toy.png}
%    \includegraphics[width=0.194\linewidth]{plots/temp/5_toy.png}
    %\includegraphics[width=0.194\linewidth]{plots/temp/25_toy.png}
%    \includegraphics[width=0.194\linewidth]{plots/temp/500_toy.png}
%    \vspace{-1em}
%    \caption{\small Convergence of SGD at a large learning rate against an outlier data in a size-$N$ training set. SGD converges to the global minimum in the yellow region and diverges in the blue region. The grey dashed line shows $\lambda_{\rm GD}^*$. Left to right: $N=2,\ 6,\ 500$.}
%    \label{fig:toy example}
%\end{figure}




Lastly, we comment that the same analysis carries over to the case of large batch size. The difference is that for $S=1$, the distribution of the gradient only depended on the distribution of $x^2$, whereas for $S>1$, the gradient depends on a sum of squares: $\mathbb{P}(\frac{1}{S}\sum_i^S x_i^2)$. When $x$ is Gaussian, this is a rescaled chi-squared distribution with the degree of freedom $S$. Let $g$ denote the gradient on a single data. When $S\to \infty$, the distribution tends to a Gaussian with variance $v:=\V[g]/S$ and mean $\E[g]$. In the case of infinite batch size, the dynamics becomes the same as that of GD, and the condition for convergence reduces to the prediction of the linear stability theory. When the batch size is large but not too large (namely, when the distribution is effectively a Gaussian, but its variance is not vanishingly small), the stability condition is nothing but the expectation of the quantity $\log |1-\lambda x^2|$ against a Gaussian distribution, which exists in general and can be both negative and positive depending on its mean, variance, and $\lambda$. Also, the case when interpolation is no longer possible is more involved. Here, since there is no single parameter that fits all the batches, the location of the local minimum necessarily oscillates across batches. We analyze this case briefly in Appendix Section~\ref{app sec: non interpolating}. We now show that the above discussion carries naturally to the general linearized SGD dynamics for a multidimensional parameter space in Eq.~\eqref{eq: sgd dynamics linearized}.

%This means that instead of converging to a constant, SGD converges to a stationary distribution with a nonvanishing variance. We analyze this case in Appendix Section~\ref{app sec: non interpolating}. While it is difficult to reach precise results in this case, we show that the main message holds: convergence of SGD can be achieved for learning rates far larger than the condition of keeping the variance finite. The difference is that the convergence is no longer in probability but in distribution.




\subsection{The Stochastic Lyapunov Exponent and Probabilistic Stability}
In practice, it is difficult to check the condition of convergence in probability. A remaining question is thus whether there is an easy-to-measure quantity that captures the probabilistic stability of SGD. Our theory implies that the following quantity is an important metric:
\begin{equation}
    m_w(t) := \E_{w_t} [ \log |w_t - w^*|^2],
\end{equation}
where the expectation is taken over the trajectories of $w_t$ with different samplings of the minibatch.
One is interested in its time dependence and whether it is positive or negative. In fact, $m/t$ is a stochastic generalization of the Lyapunov exponent, a quantity of fundamental importance in the study of dynamical systems. $m > 0$ corresponds to a chaotic system that loses stability at the rate $m/t$, and $m<0$ corresponds to a stabilizing system that converges to a fixed point. When $w^*$ is not accessible, one can instead study the following quantity:
\begin{equation}
    m_L:= \E_{w_t} [ \log L(w_t)],
\end{equation}
which expands to $m_w$ when $w_t$ is close to an interpolating minimum. From a dynamical system point of view, this quantity can be used to investigate the stability of stationary points of $L$. Alternatively, $e^{m_L}$ can be seen as a robust estimate of the training loss, and our theory suggests that the practitioners may use this quantity as an alternative monitoring quantity to gauge the progress of training because $m_L$ is much less sensitive to outliers due to the use of a small batch size or a large learning rate. In fact, for the example in the previous section, one can show that the condition $\E[\log|1-\lambda x^2|]<0$ is identical to the condition $m_L<0$.

The formal connection is established in following theorem between the stochastic Lyapunov exponent and convergence in probability. 

\begin{theorem}\label{theo: main theorem}
        Let $g(w)$ be a function of the parameters, $\Delta g(t):=||g(w_t) - g^*||$, $\hat{m}_g(w_t):= \log \Delta g(w_t)$, and $m_g(t) = \E_w[\hat{m}_g(t)]$. Let $\V [\log ||\Delta g(t)||] = o(t^{2})$. Then, if and only if $\lim_{t\to \infty} m_g(t) < 0$, $\log \Delta g(t) \to_p 0$.
        %\end{equation}
\end{theorem}


A detailed analysis proves that the SGD dynamics in Eq.~\eqref{eq: sgd dynamics linearized} indeed obeys this theorem (Section~\ref{app sec: theory}). Note that a key assumption here is that the variance of $\log \Delta g(w_t)$ does not grow too fast, which is a mild condition that applies to SGD in general. For example, it is much weaker than the assumption that $g(t)$ has a well-controlled variance as in the linear stability theory. Generally speaking, if $g(w_t)$ follows a multiplicative process, one expects $\V [\log ||\Delta g(t)||] \propto t$, like a Brownian motion. %When $g$ converges to a stationary distribution, one would expect $\V [\log ||\Delta g(t)||]$ to converge to a constant as in standard ergodic processes.\footnote{The ergodicity of SGD under various convex and nonconvex settings has recently been established. For example, see ~\cite{yu2020analysis}.} 


%\begin{theorem}
%        Let $g(w)$ be a function of the parameters, $\Delta g(t):=||g(w_t) - g^*||$, and $m_g(t):= \E[\log \Delta g(t)]$. Let $\log \Delta g(t)$ be ergodic and $\frac{1}{t}m_g(t) \in \Omega(1)$. Then, if and only if $\lim_{t\to \infty} m_g(t) < 0$,
%        \begin{equation}
%            \log \Delta g(t) \to_p 0
%        \end{equation}

%\end{theorem}



\section{Implications}\label{sec: implications}
%Before we discuss a series of theoretical and practical implications of our result. We first summarize a list of rather surprising phenomena that we have already discovered:
%\begin{compactitem}
%     \item the stability of SGD is a highly nonlinear function of the data distribution;
%    \item stable learning rates of SGD form convergent ``stripes" that are robust to data outliers, especially in an intermediate learning rate regime;
    %\item SGD is robust against outlier data and gradient in an intermediate learning rate;
%    \item SGD is not robust against outlier at a small learning rate;
%    \item SGD can converge even when the gradient variance diverges (or when the weight variance diverge);
%    \item there are regimes where decreasing learning rate can cause divergence.
%\end{compactitem}
%We now discuss some interesting and relevant applications of the probabilistic stability.




%\subsection{Sufficiently small learning rate leads to convergence}








\subsection{Abnormal Sensitivity and Robustness to outliers}

%Let us start with the least surprising implication. The condition in Eq.~\eqref{eq: convergent condition} expands as $-\sum\lambda x_i^2$ for $\lambda < 2/x_{\max}^2$. %However, a crucial difference exists between this condition and $\lambda_{\rm GD}$ is that this convergence condition depends much more strongly on the outliers of data.

One important implication of our result is the robustness of SGD to outliers in comparison to gradient descent. As Figure~\ref{fig:robustness} shows, the bulk region of probabilistic stability stays roughly unchanged as the outlier data point becomes larger and larger; in contrast, both $\lambda_{GD}^*$ and $\lambda_{DS}^*$ decreases quickly to zero. In the bulk region of the learning rates, SGD is thus probabilistically stable but not stable in the moments.

Meanwhile, in sharp contrast to this bulk robustness is the sensitivity of the smallest branch of learning rates of SGD to the outliers. Assuming that there is an outlier data point with a very large norm $c \gg N$, the largest $\lambda_{\rm GD}$ scales as $\lambda_{\max} \sim Nc^{-1}$. In contrast, for SGD, the smallest branch of probabilistically stable learning rate scales as $c^{-1}$, independent of the dataset size. This means that if we only consider the smallest learning rate, SGD is much less stable than GD, and one needs to use a much smaller learning rate to ensure convergence. For $\lambda_{\rm DS}$ A detailed analysis in Section~\ref{app sec: dynamical stability} shows that $\lambda_{\rm DS}^* = (Nc)^{-1}$. Thus, the threshold of convergence in mean square is yet one order of magnitude smaller than that of probabilistic convergence. In the limit $N\to \infty$, SGD cannot converge in variance but can still converge in probability.


%\iffalse

%\begin{figure}[t!]
%    \centering
    %\begin{subfigure}{0.22\linewidth}
    %\includegraphics[width=\linewidth]{plots/data_filter_1.png}
    %\vspace{-1.5em}
    %\caption{}
    %\end{subfigure}
    %\begin{subfigure}{0.21\linewidth}
    %\includegraphics[width=\linewidth]{plots/regression_example_0.png}
    %\vspace{-1.5em}
    %\caption{}
    %\end{subfigure}
    %\begin{subfigure}{0.21\linewidth}
    %\includegraphics[width=\linewidth]{plots/regression_example_8.png}
    %\vspace{-1.5em}
    %\caption{}
    %\end{subfigure}
    %\begin{subfigure}{0.21\linewidth}
    %\includegraphics[width=\linewidth]{plots/regression_example_18.png}
    %\vspace{-1.5em}
    %\caption{}
    %\end{subfigure}
%    \includegraphics[width=0.24\linewidth]{plots/data_filtering.png}
%    \includegraphics[width=0.23\linewidth]{plots/regression_example_18.png}
%    \includegraphics[width=0.24\linewidth]{plots/data_filter_1.png}
    %\includegraphics[width=0.21\linewidth]{plots/regression_example_0.png}
    %\includegraphics[width=0.21\linewidth]{plots/regression_example_8.png}
%    \includegraphics[width=0.24\linewidth]{plots/data_filter_2.png}
    %\includegraphics[width=0.21\linewidth]{plots/regression_example_part2_0.png}
    %\includegraphics[width=0.21\linewidth]{plots/regression_example_part2_6.png}
    %\includegraphics[width=0.21\linewidth]{plots/regression_example_part2_14.png}
%    \caption{\small \textbf{SGD at a large learning rate selects the data it learns}. The figures show the median of 5000 independent runs of a 1d linear regression problem across different learning rates $\lambda$. \textbf{Left}: the data distribution (a $\chi^2$ distribution)  vs. the stability condition. To the right of the grey line, the data points are unstable. When the blue area is smaller than the orange, learning is probabilistically unstable. When the blue area is larger, learning is stable but the data points in the orange region will not be learned.\textbf{Mid Left}: the learned parameter $w$ for $y=x^2$; here, SGD with a large learning rate prefers a small-norm solution. \textbf{Mid Right}: a systematic experiment for the learned $w$ vs. $\lambda$. \textbf{Right}: $y=\sqrt{x}$; here, SGD with a large learning rate prefers a large norm solution. Comparing the two cases demonstrates that SGD does not generically prefer a small or large norm solution but directly filters the data.}
%    \label{fig:sgd selects data}
%\end{figure}
%\subsection{How SGD selects data}

%This section discusses a very suggestive interpretation of our result: \textit{with a large learning rate, SGD selects the data points that it learns}. For a fixed learning rate $\lambda$, one can invert the stability condition to obtain an upper bound for the ``maximum learnable data point":
%\begin{equation}
%    \log |1 - \lambda x^2| \leq  0
%\end{equation}
%Any data point violating this condition will not be learned. As a result, SGD \textit{filters} the data points with a large learning rate. Close to a local minimum, this condition implies that data points with a large gradient are filtered. In a non-interpolating regime, these data points usually consist of difficult data, outliers, or mislabelers; filtering out these data points is thus expected to directly affect the performance of the model.

%In the previous literature, one predominant explanation of the effect of using a large learning rate is that it forbids SGD to converge to local minima that are linearly unstable \citep{wu2018sgd, xie2020diffusion, mori2022powerlaw}. One problematic hidden assumption behind this perspective is that the use of SGD does not change the landscape it traverses. Our analysis suggests that the dominant factor of SGD at a large learning rate is to directly affect its landscape. Instead of learning on the loss landscape spanned by the whole dataset, SGD effectively learns on the landscape spanned only by the learnable data points. We demonstrate this effect in Figure~\ref{fig:sgd selects data}. We compare a learned 1d linear regressor across different learning rates against the optimal regressor on the filtered dataset. We see that the SGD solution agrees very well with the optimal solution on the filtered dataset and deviates from the population-optimal model.
%\fi


%In the limit, $N\to \infty$, this branch of feasible learning rate effectively disappears. This leads to a rather surprising prediction about SGD: using a small learning rate can lead to divergence when larger learning rates do not. See Figure~\ref{fig:robustness}.% A particularly interesting case to solve is the case when a finite training set is drawn from a heavy-tail distribution. For example, suppose we draw $N$ data points from a Cauchy distribution. The second order statistics is determined by the outliers of the data. For the Cauchy distribution, the largest $x$ of $N$ samples scales linearly in $N$


%\begin{wrapfigure}{r}{0.32\linewidth}
%\vspace{-5em}
%\centering
%\includegraphics[width=0.85\linewidth]{plots/stability_comparison.png}
%\vspace{-1.5em}
%\caption{\small Largest stable learning rate in different theories. In comparison with the conventional theories, SGD becomes more probabilistically stable as one decreases the batch size.}\label{fig: stability comparison}
%\vspace{-0em}
%\end{wrapfigure}






%\subsection{Small batch size leads to better stability}
%Condition~\eqref{eq: convergent condition} implies a different picture for the stability of SGD as one varies the batch size. See Figure~\ref{fig: stability comparison} for a comparison of this condition with the linear stability theories. We show the maximum stable learning rate of the same problem when the dataset is drawn from a standard Gaussian distribution in an online fashion. We note that it suggests a completely opposite understanding of the linear stability theories: (1) SGD is always more stable than GD because $\lambda_{\rm max}$ is always larger than $\lambda_{\rm GD}^*$; (2) using a smaller batch size leads to better stability, in sharp contrast to the prediction of the linear stability theory. Both implications are rather surprising from a theoretical point of view, even if they agree quite well with what practitioners actually observe in training with SGD. 



\subsection{Phase Diagram of SGD}\label{sec: phase diagram}
With this notation of stability, we can study the actual effect of mini-batch noise on a neural network-like landscape. A commonly-studied minimal model of the landscape of neural networks is a deep linear net (or deep matrix factorization) \citep{kawaguchi2016deep, lu2017depth, ziyin2022exact, wang2022posterior}. For these problems, we understand that all local minima are identical copies of each other, and so all local minima have the same generalization capability \citep{kawaguchi2016deep, ge2016matrix}. The special and interesting solutions of a deep linear net are the saddle points, which are low-rank solutions and often achieving similar training loss with dramatically different generalization performances. More importantly, these saddles points also appear in nonlinear models with similar geometric properties and they could be a rather general feature of the deep learning landscape \citep{brea2019weight}.  It is thus important to understand how the noise of SGD affects the stability of a low-rank saddle here. Let the loss function be $\E_x[(\sum_i u_i \sigma(w_i x) - y)^2/2]$, where $\sigma(x) = c_0 x + O(x^2)$ is any nonlinearity that is locally linear at $x=0$. We let $c_0=1$ and focus on the case where both $x$ and $y$ are one-dimensional. Locally around $w\approx 0$, the model $uw$ is either rank-$1$ or rank-$0$. The rank-$0$ point where $u_i=w_i=0$ for all $i$ is a saddle point as long as $\E[xy] \neq 0$. In this section, we show that the stability of this saddle point features complex and dramatic phase transition-like behaviors as we change the learning rate of SGD.

Consider the linearized dynamics around the saddle at $w=u=0$. The expanded loss function takes the form:
\begin{equation}
    \ell(u,w) = -{xy} \sum^d_i u_iw_i + const.
\end{equation}
For learning to happen, SGD needs to escape from the saddle point. For analytical tractability, we let $xy=1$ with probability $0.5$ and $xy=a$; otherwise, for a controllable parameter $a$. When $a>-1$, \textit{correct} learning happens when ${\rm sign}(w)={\rm sign}(u)$. We thus focus on the case when $a>-1$. The case for $a<-1$ is symmetric to this case up to a rescaling. We solve the probabilistic stability regimes of this saddle in Section~\ref{app sec: phase diagram}. See Figure~\ref{fig: phase diagram} (b). The two most important observations are: (1) SGD can indeed converge to low-rank saddle points; however, this happens only when the gradient noise is sufficiently strong and when the learning rate is large (but not too large); (2) the region for convergence to saddles (region III) is exclusive with the region for convergence in mean square (Ia), and thus one can only understand the saddle-seeking behavior of SGD within the proposed probabilistic framework. Rigorously, we prove the following proposition. It becomes evident that the low-rank solution is reached when both $w_t+u_t \to_p 0$ and $w_t-u_t \to_p 0$.

\begin{proposition}\label{prop: phase diagram}
    For any $w_0, u_0 \in \mathbb{R}/\{0\}$. $w_t - u_t \to_p 0$ if and only if $\E_x[\log|1-\lambda xy|] <0$. $w_t + u_t$ converges to $0$ in probability if and only if $\E_x[\log|1+\lambda xy|] <0$.
\end{proposition}

We present empirical demonstrations of this effect in Section~\ref{sec: nn experiment}. Many recent works have suggested how neural networks could be biased toward low-rank solutions. Theoretically, \cite{galanti2022sgd} showed that with a weak weight decay, SGD is biased towards low-rank solutions. \cite{ziyin2022exact} showed that with weight decay, GD converges to a low-rank solution. Therefore, weight decay already induces a low-rank bias in learning, and it is not known if SGD on its own has any bias toward low-rank solutions. \cite{andriushchenko2022sgd} showed empirical hints of a preference of low-rank solutions when training without SGD. However, it remains to be clarified when or why SGD has such a preference on its own. To the best of our knowledge, our theory is the first to precisely characterize the low-rank bias of SGD in a deep learning setting. Compared with the stability diagram of linear regression, this result implies that a large learning rate can both help and hinder optimization.

Our theory shows that the phase diagram of SGD is a function of the data distribution, and it is interesting to explore and compare a few different settings. We consider a size-$N$ Gaussian data. Let $x_i \sim \mathcal{N}(0, 1)$ and noise $\epsilon_i \sim \mathcal{N}(0, 4)$, and generate a noisy label $y_i=\mu x_i + (1-\mu) \epsilon_i$. See the phase diagram for this dataset in Figure~\ref{fig: more phase diagrams} for an infinite $N$. The phase diagrams in Figure~\ref{fig: finite size dataset} show the phase diagram for a finite $N$. We see that the phase diagram has a very rich structure at a finite size. We make three rather surprising observations about the phase diagrams: (1) as $N\to \infty$, the phase diagram tends to be something smooth and quite universal; (2) phase II seems to disappear as $N$ becomes large; (3) the lower part of the phase diagram seems universal, taking the same shape for all samplings of the datasets and across different sizes of the dataset. This suggests that the convergence to low-rank structures can be a universal aspect of SGD dynamics, which corroborates the widely observed phenomenon of collapse in deep learning \citep{papyan2020prevalence, wang2022posterior, tian2022deep}. The theory also shows that if we fix the learning rate and noise level, increasing the batch size makes it more and more difficult to converge to the low-rank solution (see section~\ref{app sec: experiments}). This is expected because the larger the batch size, the smaller the effective noise in the gradient.


\begin{figure}
    \centering
    \includegraphics[width=0.3\linewidth]{plots/phase_diagram_300000_edited.png}
    \includegraphics[trim={0 2mm 0 0}, clip, width=0.3\linewidth]{plots/phase_diagram_S_edited.png}
    %\includegraphics[width=0.31\linewidth]{plots/saddle_linear_ppt.png}
    %\includegraphics[width=0.31\linewidth]{plots/saddle_tanh_ppt.png}
    \vspace{-1em}
    \caption{\small \textbf{Phase diagrams of SGD stability}. The definitions of the phases are the same as Figure~\ref{fig: phase diagram}. We sample a dataset of size N such that $x \sim \mathcal{N}(0, 1)$ and noise $\epsilon \sim \mathcal{N}(0, 4)$, and generate a noisy label $y=\mu x + (1-\mu) \epsilon$. Left: the $\lambda-\mu$ phase diagram for $S=1$ and $N=\infty$. Right: The $\lambda-S$ phase diagram for $\mu=0.06$ and $N=\infty$.}
    \label{fig: more phase diagrams}

    \includegraphics[width=0.3\linewidth]{plots/phase_diagram_3.png}
    \includegraphics[width=0.3\linewidth]{plots/phase_diagram_4.png}
    
    \includegraphics[width=0.3\linewidth]{plots/phase_diagram_8.png}
    \includegraphics[width=0.3\linewidth]{plots/phase_diagram_10.png}

    \includegraphics[width=0.3\linewidth]{plots/phase_diagram_24.png}
    \includegraphics[width=0.3\linewidth]{plots/phase_diagram_100.png}

    \vspace{-1em}
    \caption{\small \textbf{Phase diagrams of SGD stability for finite-size dataset}. The sampling of the data is the same as in Figure~\ref{fig: more phase diagrams}. From upper left to lower right: $N=3,\ 4,\ 8,\ 10,\ 24,\ 100$. As the dataset size tends to infinity, the phase diagram converges to that in Figure~\ref{fig: more phase diagrams}. The lower parts of all the phase diagrams look very similar, suggesting a universal structure.}
    \label{fig: finite size dataset}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{plots/swish_example.png}
    \caption{\small \textbf{How SGD selects a solution}. \textbf{Left}: The landscape of a two layer network with the swish activation function \citep{ramachandran2017searching}. \textbf{Middle, Right}: the generalization performance of the model as one increases the learning rate. \textbf{Middle}: Initialized at solution B, SGD first jumps to C and then diverge. \textbf{Right}: Initialized at A, SGD also jumps to C and diverge. In both cases, the behavior of SGD agrees with the prediction of the probabilistic stability, instead of the linear stablity. Instead of jumping between local minima, SGD at a large learning rate transitions from minima to saddles.}
    \label{fig:select minimum}
\end{figure}

\subsection{How SGD Selects a Solution}
We now investigate one of the most fundamental problems in deep learning: how SGD selects a solution for a neural network. In this section, we study a two-layer network with a single hidden neuron with the swish activation function: $f(w,u,x) = u \times {\rm swish}(wx)$, where ${\rm swish}(x) = x \times {\rm sigmoid}(x)$. Swish is a differentiable variant of ReLU that is discovered by meta-learning techniques and consistently outperforms ReLU in various tasks. We generate $100$ data points $(x,y)$ as $y = 0.1 {\rm swish}(x) + 0.9\epsilon$, where both $x$ and $\epsilon$ are sampled from normal distributions. See Figure~\ref{fig:select minimum} for an illustration of the training loss landscape. Here, there are two local minima: solution A at roughly $(-0.7, -0.2)$ and solution B at $(1.1, -0.3)$. Here, the solution with better generalization is A because it captures the correct correlation between $x$ and $y$ when $x$ is small. Solution A is also sharper; its largest Hessian eigenvalue is roughly $h_a = 7.7$. Solution B is the worse solution; it is also flatter, with the largest Hessian value being $h_b=3.0$. There is also a saddle point C at $(0,0)$, which performs significantly better than B and slightly worse than A in generalization.

If we initialize the model at A, linear stability theory would predict that as we increase the learning rate, the model moves from the sharper solution A to the flatter minimum B when SGD loses linear stability in A; the model would then lose total stability once SGD becomes linearly unstable at (b). As shown by the red arrows in Figure~\ref{fig:select minimum}. In contrast, probabilistic stability predicts that SGD will move from A to C as C becomes attractive and then lose stability, as indicated by the black arrows. See the right panel of the figure for the comparison with the experiment for the model's generalization performance. The dashed lines show the predictions of the linear stability theory and the probabilistic theory, respectively. We see the probabilistic theory predicts both the error and the place of transition right, whereas linear stability neither predicts the right transition nor the correct level of performance. 

If we initialize at B, the flatter minimum, linear stability theory would predict that as we increase the learning rate, the model will only have one jump from B to divergence. Thus, from linear stability, SGD would have roughly the performance of B until it diverges, and having a large learning rate will not help with performance. In sharp contrast, the probabilistic stability predicts that the model will have two jumps: it stays at B for a small $\lambda$ and jumps to C as it becomes attractive at an intermediate learning rate. The model will ultimately diverge if $C$ loses stability. Thus, our theory predicts that the model will first have a bad performance, then a better performance at an intermediate learning rate, and finally diverge. See the middle panel of Figure~\ref{fig:select minimum}. We see that the prediction of the probabilistic stability agrees with the experiment and correctly explains why SGD leads to a better performance of the neural network.




%\subsection{A More Informative Metric of Optimization}

%Thus, if we take $m$ as a faithful metric of the typical convergence rate of SGD, it would be interesting to study the convergence rate of SGD with respect to this measure, which we leave as an interesting future step.
%how $m$ changes as we change the learning rate. It turns out that there are also unexpected phenomena here. Naively, one would expect that there exists a single optimal learning rate $\lambda_*$ at which the convergence speed is maximized. The convergence speed should decrease monotonically for smaller learning rates because the dynamics is slow and for larger learning rates because of the loss of stability. However, our result implies that the convergence speed is a highly nonlinear function of $\lambda$, featuring multiple maxima interlaced with regions of divergences. See the right panel of Figure~\ref{fig:toy dynamics}.


%\textbf{Relationship to the conventional convergence rate}. A common metric for convergence rate is 
%\begin{equation}
%    r:= \E [(w_t -w^*)^2].
%\end{equation}
%Alternatively, the expected objective (as of function of iteration) is also used as a metric of convergence: $\E[L(w_t)]$, which is proportional to $r$ close to a local minimum. Thus, both metrics suffer from the problem discussed in this work, they strongly depend on the outliers of data, which in turn leads to pathological gradients. A comparison of this quantity with $m$ has been made in Figure~\ref{fig:toy dynamics}. 

%It is important to note that by a simple application of Jensen's inequality, we have $r \geq e^{2m}$, thus $r$ upper bounds $e^{2m}$, and always underestimate the convergence rate. In some sense, this is expected because the conventional convergence rate is related to a convergence in mean square. In contrast, $m$ is related to a convergence in probability, and convergence in mean square always implies convergence in probability. 






%In particular, it cannot learn the data points whose gradient is larger than 


%In our analysis, we have shown that 


%\subsection{The Problem of Measurement}

%[todo]


%\end{figure}
\iffalse
\begin{figure}[t!]
    \centering
    %\includegraphics[width=0.3\linewidth]{plots/cifar10_dynamic_noise.png}
    %\includegraphics[width=0.26\linewidth]{plots/cifar10_dynamic_noise_phase_boundary.png}
    \includegraphics[width=0.26\linewidth]{plots/resnet_bs.png}
    \caption{\small The rank of the penultimate-layer representation of Resnet18 trained with different levels of batch sizes. IN agreement with the phase diagram, the model escapes from the low-rank saddle as one increases the batch size.}
    \label{fig:resnet18}
\end{figure}
\fi


\subsection{Neural Network Phase Diagrams}\label{sec: nn experiment}
We start with a controlled experiment where, at every training step, we sample input $x \sim \mathcal{N}(0, I_{200})$ and noise $\epsilon \sim \mathcal{N}(0, 4I_{200})$, and generate a noisy label $y=\mu x + (1-\mu) \epsilon$. Note that $1-\mu$ controls the level of the noise. Training proceeds with SGD on the MSE loss. We train a two-layer model with the architecture: $200 \to 200 \to 200$. See Figure~\ref{fig: nn low rank} for the theoretical phase diagram. SGD escapes from the saddle with a finite variance to the right of the dashed line and has an infinite variance to its left. In the region $\lambda \in (0, 0.2)$, this loss of linear stability condition coincides with the condition for the convergence to the saddle. The experiment shows that the theoretical boundary agrees well with the numerical results. For completeness, we show that the Adam optimizer \citep{journals/corr/KingmaB14_adam} also has a qualitatively similar phase diagram in Appendix~\ref{app sec: experiments}. This suggests that the effects we studied in this work are rather universal, not just a special feature of SGD.


Lastly, we train independently initialized ResNets on CIFAR-10 with SGD. The training proceeds with SGD without momentum at a fixed learning rate and batch size $S=32$ (unless specified otherwise) for $10^5$ iterations. Our implementation of Resnet18 contains $11$M parameters in total and achieves $94\%$ test accuracy under the standard training protocol, consistent with the established values. To probe the effect of noise, we artificially inject a dynamical label noise during every training step, where, at every step, a correct label is flipped to a random label with probability $noise$. See Figure~\ref{fig: phase diagram}. We see that the results agree with the theoretical expectation and with the phase diagram of simpler models. We also study the sparsity of the ResNets in different layers in Appendix~\ref{app sec: experiments}, and we observe that the phase diagrams are all qualitatively similar. We also note that the effect is not due to having a dynamical label noise. Our experiments with a static label noise also show the same results with almost the same regime boundaries.

%We see that 






\section{Discussion}\label{sec: discussion}
In this work, we have demonstrated that the convergence in probability condition serves as an essential notion for understanding the stability of SGD, leading to highly nontrivial and practically relevant phase diagrams at a finite learning rate. We also clarified its connection to Lyapunov exponents, which are conventional and easy-to-measure metrics of stability in the study of dynamical systems. At a small learning rate and large batch size, the proposed stability agrees with the conventional notion of stability. At a large learning rate and a small batch size, we have shown that the proposed notion of stability captures the actual behavior of SGD much better and successfully explained a series of experiment phenomena that had been quite puzzling. Among the many implications that we discussed, perhaps the most fundamental one is a novel understanding of the implicit bias of SGD.
When viewed from a dynamical stability point of view, the implicit bias of stochastic gradient descent is thus fundamentally different from the implicit bias of gradient descent.
The new perspective we provide is also different from the popular perspective that SGD influences the learning outcome directly by making the model converge to flat minima. In fact, in our construction, the flatter minimum does not have better generalization properties, nor does SGD favor it over the sharper one. Instead, SGD performs a selection between converging to saddles and local minima, which directly and significantly affects its performance. Thus, we believe that the probabilistic stability is an important future direction to investigate the stability of SGD in a deep learning scenario through this new angle we suggested.


In the current work, our analysis centers around studying when and why the quantity $m:= \E [\log |w_t - w^*|]$ becomes positive and does not discuss too much what the magnitude of $m$ means. In fact, the quantity $m$ tells us that the quantity $|w_t - w^*|$ is typically evolving like 
\begin{equation}
    |w_t - w^*| \propto e^{m},
\end{equation}
and, therefore, $m$ can be seen as a robust metric of the convergence rate of the system. Thus, $m/t$ can be seen as a metric of the time scale of the relevant dynamics in the neighborhood of a stationary point. Moreover, it is much more informative than the common metric of convergence in the theoretical literature, such as the expected regret or the training loss. As we have argued, these quantities are not good metrics of convergence because they are dominated by rare outliers of trajectories and can diverge even if the system is probabilistically stable. What makes the problem worse is the fact that at a large learning rate, such outlier trajectories lead to divergence of fluctuation. In this sense, $m$ reflects the \textit{typical} behavior of training much better. See Section~\ref{app sec: typical trajectory}, where we compare $e^{2m}$ with sample trajectories of learning empirically.

Practitioners often plot the training loss in a logarithmic scale vs. training iteration to monitor the progress, where the training loss is estimated by a minibatch sampling. For example, see Figure 2 and 3 of \citep{journals/corr/KingmaB14_adam}. Looking at the logarithmic scale often gives us a much better grasp of how the training is progressing than looking at the raw training loss. This is essentially monitoring the quantity $m_L$. Our theory, thus, offers a first step towards understanding a more practically relevant metric of training progress.

%\clearpage
%\bibliographystyle{apalike}
%\bibliography{ref}

\begin{thebibliography}{}

\bibitem[Andriushchenko et~al., 2022]{andriushchenko2022sgd}
Andriushchenko, M., Varre, A., Pillaud-Vivien, L., and Flammarion, N. (2022).
\newblock Sgd with large step sizes learns sparse features.
\newblock {\em arXiv preprint arXiv:2210.05337}.

\bibitem[Brea et~al., 2019]{brea2019weight}
Brea, J., Simsek, B., Illing, B., and Gerstner, W. (2019).
\newblock Weight-space symmetry in deep networks gives rise to permutation
  saddles, connected by equal-loss valleys across the loss landscape.
\newblock {\em arXiv preprint arXiv:1907.02911}.

\bibitem[Eckmann and Ruelle, 1985]{eckmann1985ergodic}
Eckmann, J.-P. and Ruelle, D. (1985).
\newblock Ergodic theory of chaos and strange attractors.
\newblock {\em The theory of chaotic attractors}, pages 273--312.

\bibitem[Galanti and Poggio, 2022]{galanti2022sgd}
Galanti, T. and Poggio, T. (2022).
\newblock Sgd noise and implicit low-rank bias in deep neural networks.
\newblock {\em arXiv preprint arXiv:2206.05794}.

\bibitem[Ge et~al., 2016]{ge2016matrix}
Ge, R., Lee, J.~D., and Ma, T. (2016).
\newblock Matrix completion has no spurious local minimum.
\newblock {\em Advances in neural information processing systems}, 29.

\bibitem[Gower et~al., 2019]{gower2019sgd}
Gower, R.~M., Loizou, N., Qian, X., Sailanbayev, A., Shulgin, E., and
  Richt{\'a}rik, P. (2019).
\newblock Sgd: General analysis and improved rates.
\newblock In {\em International conference on machine learning}, pages
  5200--5209. PMLR.

\bibitem[He et~al., 2019]{he2019control}
He, F., Liu, T., and Tao, D. (2019).
\newblock Control batch size and learning rate to generalize well: Theoretical
  and empirical evidence.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Hoffer et~al., 2017]{hoffer2017train}
Hoffer, E., Hubara, I., and Soudry, D. (2017).
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1731--1741.

\bibitem[Kawaguchi, 2016]{kawaguchi2016deep}
Kawaguchi, K. (2016).
\newblock Deep learning without poor local minima.
\newblock {\em Advances in Neural Information Processing Systems}, 29:586--594.

\bibitem[Kingma and Ba, 2014]{journals/corr/KingmaB14_adam}
Kingma, D.~P. and Ba, J. (2014).
\newblock Adam: A method for stochastic optimization.
\newblock {\em CoRR}, abs/1412.6980.

\bibitem[Li et~al., 2019]{li2019towards}
Li, Y., Wei, C., and Ma, T. (2019).
\newblock Towards explaining the regularization effect of initial large
  learning rate in training neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Liu et~al., 2021]{liu2021noise}
Liu, K., Ziyin, L., and Ueda, M. (2021).
\newblock Noise and fluctuation of finite learning rate stochastic gradient
  descent.

\bibitem[Liu et~al., 2020]{liu2020improved}
Liu, Y., Gao, Y., and Yin, W. (2020).
\newblock An improved analysis of stochastic gradient descent with momentum.
\newblock {\em Advances in Neural Information Processing Systems},
  33:18261--18271.

\bibitem[Lu and Kawaguchi, 2017]{lu2017depth}
Lu, H. and Kawaguchi, K. (2017).
\newblock Depth creates no bad local minima.
\newblock {\em arXiv preprint arXiv:1702.08580}.

\bibitem[Lyapunov, 1992]{lyapunov1992general}
Lyapunov, A.~M. (1992).
\newblock The general problem of the stability of motion.
\newblock {\em International journal of control}, 55(3):531--534.

\bibitem[Mori et~al., 2022]{mori2022powerlaw}
Mori, T., Ziyin, L., Liu, K., and Ueda, M. (2022).
\newblock Power-law escape rate of sgd.

\bibitem[Papyan et~al., 2020]{papyan2020prevalence}
Papyan, V., Han, X., and Donoho, D.~L. (2020).
\newblock Prevalence of neural collapse during the terminal phase of deep
  learning training.
\newblock {\em Proceedings of the National Academy of Sciences},
  117(40):24652--24663.

\bibitem[Ramachandran et~al., 2017]{ramachandran2017searching}
Ramachandran, P., Zoph, B., and Le, Q.~V. (2017).
\newblock Searching for activation functions.

\bibitem[{Shirish Keskar} et~al., 2016]{Keskar2017}
{Shirish Keskar}, N., {Mudigere}, D., {Nocedal}, J., {Smelyanskiy}, M., and
  {Tang}, P.~T.~P. (2016).
\newblock {On Large-Batch Training for Deep Learning: Generalization Gap and
  Sharp Minima}.
\newblock {\em ArXiv e-prints}.

\bibitem[Tian, 2022]{tian2022deep}
Tian, Y. (2022).
\newblock Deep contrastive learning is provably (almost) principal component
  analysis.
\newblock {\em arXiv preprint arXiv:2201.12680}.

\bibitem[Vaswani et~al., 2019]{vaswani2019fast}
Vaswani, S., Bach, F., and Schmidt, M. (2019).
\newblock Fast and faster convergence of sgd for over-parameterized models and
  an accelerated perceptron.
\newblock In {\em The 22nd international conference on artificial intelligence
  and statistics}, pages 1195--1204. PMLR.

\bibitem[Wang and Ziyin, 2022]{wang2022posterior}
Wang, Z. and Ziyin, L. (2022).
\newblock Posterior collapse of a linear latent variable model.
\newblock {\em arXiv preprint arXiv:2205.04009}.

\bibitem[Wu et~al., 2018]{wu2018sgd}
Wu, L., Ma, C., et~al. (2018).
\newblock How sgd selects the global minima in over-parameterized learning: A
  dynamical stability perspective.
\newblock {\em Advances in Neural Information Processing Systems}, 31.

\bibitem[Wu et~al., 2022]{wu2022does}
Wu, L., Wang, M., and Su, W. (2022).
\newblock When does sgd favor flat minima? a quantitative characterization via
  linear stability.
\newblock {\em arXiv preprint arXiv:2207.02628}.

\bibitem[Xie et~al., 2020]{xie2020diffusion}
Xie, Z., Sato, I., and Sugiyama, M. (2020).
\newblock A diffusion theory for deep learning dynamics: Stochastic gradient
  descent exponentially favors flat minima.
\newblock {\em arXiv preprint arXiv:2002.03495}.

\bibitem[Zhu et~al., 2018]{zhu2018anisotropic}
Zhu, Z., Wu, J., Yu, B., Wu, L., and Ma, J. (2018).
\newblock The anisotropic noise in stochastic gradient descent: Its behavior of
  escaping from sharp minima and regularization effects.
\newblock {\em arXiv preprint arXiv:1803.00195}.

\bibitem[Ziyin et~al., 2022a]{ziyin2022exact}
Ziyin, L., Li, B., and Meng, X. (2022a).
\newblock Exact solutions of a deep linear network.
\newblock {\em arXiv preprint arXiv:2202.04777}.

\bibitem[Ziyin et~al., 2022b]{ziyin2022strength}
Ziyin, L., Liu, K., Mori, T., and Ueda, M. (2022b).
\newblock Strength of minibatch noise in {SGD}.
\newblock In {\em International Conference on Learning Representations}.

\end{thebibliography}


\clearpage
\appendix

\section{Additional Theoretical Concerns}\label{app sec: theory}


\subsection{Convergence at a special learning rate}

\begin{proposition}\label{prop: special learning rates}
Consider a dataset where all the data points $(x_i, y_i) \in \mathbb{R}^{2}$ lie on a straight line. For any initialization, $w_0$ and learning rate $\lambda = 1/x_i^2$, stochastic gradient descent (SGD) converges to the global minimum $w^*=x_i/y_i$ with probability 1.
%Let $i \in \{1,\dots,N\}$ and  $\lambda=1/x_i^2$. Then, for any initialization $w_0$, $w_t$ converges to the global minimum $w^*=x_i/y_i$ with probability $1$ under SGD.
\end{proposition} 
\begin{proof}
Let us examine a single step of SGD dynamics. With probability $1/N$, the algorithm selects data point $x_i$ for training, and the update rule is as follows:
\begin{equation}
w_{t+1}= w_t - \lambda x_i(x_i w_t - y_i) = y_i/x_i.
\end{equation}
Thus, we can update $w_t$ to $w_{t+1}=y_i/x_i$ with probability $\geq 1/N$. Once $w_t$ reaches the global minimum $y_i/x_i$, it stays there with zero probability of leaving because it is the unique minimum. Hence, for any initialization $w_0$, we have
\begin{equation}
\mathbb{P}(w_t \neq y_i/x_i) \leq (1-\fr{1}{N})^t \to 0.
\end{equation}
This concludes the proof. 
\end{proof}


\subsection{Linear Stability Conditions and the Derivation of Eq.~\eqref{eq: lambda DS}}\label{app sec: dynamical stability}

\subsubsection{General Condition for Dynamical Stability}
For a general batch size $S$, the dynamics of SGD reads 
\begin{align}
    w_{t+1} &= w_t - \lambda w_t \frac{1}{S}\sum_{i=1}^S x_i^2\\
    &= w_t \left(1-\lambda \frac{1}{S}\sum_{i=1}^S x_i^2\right).
\end{align}
The second moment of $w_{t+1}$ is
\begin{align}
    \E_x[w_{t+1}^2|w_t] &= w_t^2\E_x\left(1 - \lambda \frac{1}{S}\sum_i^S x_i^2\right)^2 \\
    &= w_t^2 \left(1 - 2\lambda \E[x^2] + \frac{\lambda^2}{S^2} \sum_{i,j}^S \E[x_i^2 x_j^2] \right)\\
    &= w_t^2 \left(1 - 2\lambda \E[x^2] + \frac{\lambda^2}{S^2}  \E[x^4] + \frac{\lambda^2(S-1)^2}{S^2} \E[x^2]^2\right).
\end{align}
Note that this equation applies to any $w_t\in \mathbb{R}$. Therefore, the second moment of $w_t$ is convergent if
\begin{equation}
    \left(1 - 2\lambda \E[x^2] + \frac{\lambda^2}{S^2}  \E[x^4] + \frac{\lambda^2(S-1)^2}{S^2} \E[x^2]^2\right)<1,
\end{equation}
which solves to
\begin{equation}\label{app eq: general ds condition}
    \lambda < \frac{2S^2 \E[x^2]}{\E[x^4] + (S-1)^2 \E[x^2]^2}.
\end{equation}
This condition applies to any data distribution. One immediate observation is that it only depends on the second and fourth moments of the data distribution and that both moments need to be finite for convergence at a non-zero learning rate. It is quite instructive to solve this condition under a few special conditions.


\subsubsection{Gaussian Data Distribution}
The condition~\eqref{app eq: general ds condition} takes a precise form when the data is Gaussian. Using the fact that for a Gaussian variable $x$ with variance $\sigma^2$, $\E[x^4]=3\sigma^4$, the condition simplifies to
\begin{equation}
    \lambda < \frac{2}{\E[x^2]} \frac{S^2}{3S + (S-1)^2}.
\end{equation}
This is the same as Eq.~\eqref{eq: lambda DS}.

\subsubsection{Bernoulli Dataset}
Another instructive case to consider is the case when there are only two data points in the data: $x_1$ and $x_2$. The moments are
\begin{equation}
    \begin{cases}
        \E[x^2] = \frac{1}{2}(x_1^2 + x_2^2),\\
        \E[x^4] = \frac{1}{2}(x_1^4 + x_2^4).
    \end{cases}
\end{equation}
When one of the data points, say $x_1$, is very large, the condition becomes
\begin{equation}
    \lambda < \frac{2S^2 x_1^2}{2x_1^4 + (S-1)^2 x_1^4} = \frac{2S^2}{x_1^2} \frac{1}{2 + (S-1)^2}.
\end{equation}

\subsubsection{Extreme Outlier}
We can also consider the general case of a finite dataset with a large outlier, $x_{\rm max}$. The condition is similar to the Bernoulli case. We have
\begin{equation}
    \begin{cases}
        \E[x^2] \approx \frac{1}{N}x_{\rm max}^2,\\
        \E[x^4] \approx \frac{1}{N}x_{\rm max}^4.
    \end{cases}
\end{equation}
The condition reduces to
\begin{equation}
    \lambda < \frac{2S^2}{N x_{\rm max}^2} \frac{1}{1 + \frac{(S-1)^2}{N}}.
\end{equation}
This can be seen as the generalization of the Bernoulli condition. When $S=1$, this condition becomes
\begin{equation}
    \lambda < \frac{2}{N x_{\rm max}^2}.
\end{equation}

There are many other interesting limits of this condition we can consider from the perspective of extreme value theory. However, this is beyond the scope of this work and we leave it as an interesting future work.

%\subsection{Analysis for the non-interpolating case}
\subsection{Non-interpolating case}\label{app sec: non interpolating}
%The next interesting question is what will happen if the batch size is not $1$.
 In this section, we show that consistent with our previous theory, the convergence to a stationary distribution is possible for learning rates far larger than a linear stability theory would expect. We only discuss the main steps and conclusions in the main text. The formal analysis is left to the appendix. To keep the computation concise, we focus on the case when the learning rates are ``critical". Namely, we let $\lambda = 1/x_i^2$ for some $i$. Here, the SGD dynamics implies
\begin{equation}
    |w_{t+1}| = \begin{cases}
        |y_i/x_i| & \text{with probability $1/N$};\\
        |a_t w_t + b_t| & \text{otherwise},
    \end{cases}
\end{equation}
where $a$ and $b$ are constants depending on the batch at step $t$. Such a dynamics is upper bounded by an alternative dynamics that is independent of the other data points:
\begin{equation}
    |z_{t+1}| = \begin{cases}
        |y_i/x_i| & \text{with probability $1/N$};\\
        |a| z_t + |b| & \text{otherwise},
    \end{cases}
\end{equation}
where $a = \max_t |a_t|$, $b = \max_t |b_t|$, and $z_0 = w_0$. This dynamics is easy to study -- its possible achievable values are discrete in space:
\begin{equation}
    |y_i/x_i| a^j + b a^j \frac{1 - (1/a)^j}{1 - 1/a} \approx a^j\left ( |y_i/x_i|  + \frac{ab}{a-1}\right)
\end{equation}
for all $j \in \mathbb{Z}^+$. The number $j$ is thus a natural index for indexing these states. One can show that in the infinite time limit, any bounded starting distribution converges to a unique stationary distribution:
\begin{equation}
    \mathbb{P}\left(z_j = |y_i/x_i| a^j + b a^j \frac{1 - (1/a)^j}{1 - 1/a} \right) = \frac{1}{N}(1- 1/N)^j,
\end{equation}
which upper bounds the distribution of $w$. Therefore, $w$ does not diverge as long as $z$ does not diverge. One interesting question is how likely is $w$ to be away from $y_i/x_i$. Let $\epsilon_j =  a^j\left ( |y_i/x_i|  + \frac{ab}{a-1}\right) $ We have
\begin{align}
    \mathbb{P}(z \geq  \epsilon_j) &= \mathbb{P}(z = \epsilon_j) + \mathbb{P}(z = \epsilon_{j+1}) + ...\\
    &= \sum_{k=j}^\infty \frac{1}{N}(1- 1/N)^k \\
    &= (1-1/N)^j \\
    &\propto \epsilon_j^{-  \frac{1}{N \log a}}
\end{align}
This result means that the distribution of the parameter $w$ has a power-law tail. The exponent decides the thickness of this tail. The closer the exponent to zero, the less likely the parameter will be around the minimum. Here, we see that the scaling exponent is proportional to the growth factor $\log a$, where
\begin{equation}
    \log a = \log |1-\lambda h|.
\end{equation}
In the worst case scenario, $h=x_{\max}^2$. However, in the average case, one expects $h$ to be close to the average data norm: $h\approx \E_x[x^2]$. Thus, when the learning rate is not too large, we have that the exponent is $\propto \frac{1}{\lambda \E[x^2]}$. When $\lambda$ is very large, the scaling exponent becomes logarithmic: $1/\log (\lambda \E[x^2])$. In both cases, the scaling is clear: the large the learning rate, the heavier the tail of the distribution. This is consistent with the previous results \cite{mori2022powerlaw}. While one often interprets this power-law behaviors as that SGD encourages exploration, we interpret this conversely -- the model parameter has a significant nonzero probability of staying around the given local minimum. In practice, it is very likely such local minimum are reflected in a robust estimator of the parameter. %In the experiment section, we demonstrate this effect by measuring the median of the parameters, instead of the mean.





\subsection{Proofs}
\subsubsection{Proof of Proposition~\ref{prop: learning rates are not isolated}}
\begin{proof} First of all, when $\lambda = 1/x_i^2$, the convergence is almost sure, which implies convergence in probability. Thus, we focus on the case when $\lambda = 1/x_i^2 \neq 0$ for all $i$. Note that this condition is equivalent to that $\E_x[1-\lambda x^2]$ is not infinite.


Now, the dynamics Eq.~\eqref{eq: zero label dynamics} implies the following dynamics
\begin{equation}
    w_{t+1} / w_t = 1 - \lambda x_t^2,
\end{equation}
which implies
\begin{equation}
    |w_{t+1} / w_0| = \prod_{\tau=1}^t |1 - \lambda x_\tau^2|.
\end{equation}
We can define auxiliary variables $z_t:= \log |w_{t+1} / w_0| - m$ and $m := \E \log |w_{t+1} / w_0| = t\E_x \log |1 - \lambda x^2|$. Also define $s:= \V[\log |1 - \lambda x^2|]$. Let $\epsilon>0$. We have that 
\begin{align}
    \mathbb{P}(|w_t| < \epsilon)  &= \mathbb{P}(|w_0|e^{z_t + m} < \epsilon)\\
    &= \mathbb{P}\left(\frac{1}{{t}} z_t < \frac{1}{{t}} ( \log \epsilon / |w_0| - m)\right)\\
    &= \mathbb{P}\left( z_t/t <  - \E_x \log |1 - \lambda x^2| + o(1)\right)
   % &\to P(h < \log \epsilon / |w_0| - m))
\end{align}
By the law of large numbers, the left-hand side of the inequality converges to $0$, whereas the right-hand side converges to a constant. Thus, we have, for all $\epsilon>0$,
\begin{equation}
    \lim_{t\to \infty} \mathbb{P}(|w_t| < \epsilon) = \begin{cases} 1 &\text{if $m<0$}\\
    0 &\text{if $m>1$}.
    \end{cases}
\end{equation}
This completes the proof.
\end{proof}

\subsubsection{Proof of Theorem~\ref{theo: main theorem}}
\begin{proof} 
First of all, we define $z_t = \hat{m}_g(w_t) - m_g(t)$. By definition, we have
\begin{align}
    \mathbb{P}(\|\Delta g_t\| < \epsilon)  &= \mathbb{P}(e^{z_t + m_g(t)} < \epsilon)\\
    &= \mathbb{P}\left(\frac{1}{{t}} z_t < \frac{1}{{t}} ( \log \epsilon / |w_0| - m_w(t))\right)\\
    &= \mathbb{P}\left( \frac{1}{t} z_t <  \frac{1}{t} m_w(t) + o(1)\right).
   % &\to P(h < \log \epsilon / |w_0| - m))
\end{align}
Now, because $ \V[z_t] = \V [\log \|\Delta g(t)\|] = o(t^{2})$, we have that $ \V[z_t/t] = o(1)$. Applying Chebychev's inequality, one obtains that for any $\eta>0$
\begin{equation}
    \mathbb{P}(z_t/t > \eta) \leq \frac{\V[z_t/t]}{\eta^2} \to 0.
\end{equation}
This means that $z_t/t$ converges to $0$ in mean square. In turn, this implies that 
\begin{equation}
    \lim_{t\to \infty} \mathbb{P}(|\Delta g_t| < \epsilon) =\lim_{t\to\infty}\mathbb{P}\left( \frac{1}{t} z_t <  \frac{1}{t} m_w(t)\right) = \begin{cases} 1 &\text{if $m<0$}\\
    0 &\text{if $m>0$}.
    \end{cases}
\end{equation}
This finishes the proof.
\end{proof}


\subsection{Convergence of Generic SGD Dynamics}
In this section, we show that a generic SGD dynamics in multi-dimension obeys Proposition~\ref{theo: main theorem}. Recall that the dynamics of SGD close to a \textit{non-fluctuating }stationary point is
\begin{equation}\label{app eq: sgd dynamics}
    w_{t+1}-w^* = w_t - \frac{2\lambda}{S}\sum^{S}_{j=1} \hat{H}(w^*,x_j)(w_t-w^*),
\end{equation}
where $\hat{H}(w,x) := \nabla_w^2 \ell(w,x)$ is the sample-wise Hessian. Note that we allow $H$ to have negative eigenvalues so that our result includes the case where $w^*$ is a saddle point. It suffices to prove for the case $S=1$ and $N$ is finite.
\begin{theorem}
    Let $w_t$ obey Eq.~\eqref{app eq: sgd dynamics}, and $\hat{m}_t = \log ||w_t -w^*||$. Let $I -\lambda\hat{H}_i$ be full rank for all $i$. Then,
    \begin{equation}
        \lim_{t\to 0} \V[\hat{m}_t/t] = 0.
    \end{equation}
\end{theorem}
\begin{proof}
Without loss of generality, we let $w^* =0$. By assumption, $\lim_{t\to \infty}\hat{m}_t/t  = c_0 \neq 0$. By definition, we have that
\begin{equation}
    \V[\hat{m}_t/t] = \frac{1}{t^2}(\E[\hat{m}_t^2] - m_t^2).
\end{equation}
We thus need to compute $\E[\hat{m}_t^2]$. 

By assumption, $I-\lambda \hat{H}_i$ is full rank, we can thus define $r_{\max}$ to be larger than the absolute value of all the eigenvalues of $I-\lambda \hat{H}_i$ for all $i$. Similarly, we can define $r_{\min}>0$ to be smaller than all the absolute values of all the eigenvalues of $I-\lambda \hat{H}_i$ for all $i$. Note that by definition, $r_{\min}<e^{c_0}<r_{\max}$, and so we can choose $r_{\min}$ and $r_{\max}$
such that
\begin{equation}
    \frac{1}{2}(\log r_{\min} + \log r_{\max}) = m.
\end{equation}

Now, we use $r_{\min}$ and $r_{\max}$ to construct an auxiliary process that upper bounds the variance of $\hat{m}_t$. Let 
\begin{equation}
    \mu_{t+1}  = \mu_t  +  \log r,
\end{equation}
where $r = r_{\min}$ with probability $0.5$, and $r=r_{\max}$ with probability $0.5$. Note that by definition,
\begin{equation}
    \E[\mu_t/t] = c_0,
\end{equation}
and
\begin{equation}
    \V[\mu_t /t] = \frac{1}{2t} \left[(\log r_{\max} - c_0)^2 + (\log r_{\min} - c_0)^2 \right] = O(t^{-1}).
\end{equation}
However, for any fixed $w_t$, we have that 
\begin{equation}
    ||w_{t+1}|| = ||(I-\lambda \hat{H}_{i_t})w_t||,
\end{equation}
which implies that 
\begin{equation}
    r_{\min} ||w_{t}|| \leq ||w_{t+1}|| \leq r_{\max} ||w_{t}||.
\end{equation}
which implies that
\begin{equation}
    \V[\hat{m}_{t+1}| \hat{m}_t]    \leq \V[\mu_{t+1}| \mu_t].
\end{equation}
In turn, this implies that if $\mu_0 = \hat{m}_0$
\begin{equation}
    \V[\hat{m}_{t}/t]    \leq \V[\mu_{t}/t]
\end{equation}
for all $t$. Taking the infinite time limit on both sides, we have that
\begin{equation}
    \V[\hat{m}_{t}/t] \to 0.
\end{equation}
The proof is complete.
\end{proof}


Therefore, SGD dynamics satisfies the assumption of Proposition~\ref{theo: main theorem}. Another important question is whether this theorem is trivial for SGD at a high dimension in the sense that it could be the case that $m_t$ could be identically zero independent of the dataset. One can show that for all datasets that satisfy a very mild condition, the Lyapunov exponent is, in general, nonzero.

Let $\E[\hat{H}]$ be full rank. Let $\hat{m}_t$ be defined as in the previous proof. By definition, 
\begin{equation}
    \E[\hat{m}_t] = \E[\log w_0^T \left(\prod_j^t (I - \lambda\hat{H}_{i_j})\right)\left(\prod_j^t (I - \lambda\hat{H}_{i_j})\right)^T w_0 ].
\end{equation}

Again, let $w^* = 0$. When $\lambda$ is small, this can be expanded as
\begin{align}
    \E[m_t] &=  \E \log \left(||w_0||^2 + 2\lambda w_0^T \sum_j^t \hat{H}_j w_0 + O(\lambda^2) \right)\\
    &= \log||w_0||^2 + \frac{ \E[ 2\lambda w_0^T \sum_j^t \hat{H}_j w_0]}{||w_0||^2} + O(\lambda^2)\\
    &= \log||w_0||^2 + \frac{2\lambda t w_0^T  H w_0}{||w_0||^2} + O(\lambda^2).
\end{align}
Therefore,
\begin{equation}
    \lim_{t\to\infty} \E[\hat{m}_t/t] = -\frac{2\lambda w_0^T  H w_0}{||w_0||^2} + O (\lambda^2).
\end{equation}
Therefore, as long as $\lambda$ is sufficiently small, the Lyapunov exponent is always negative. This proves something quite general for SGD at an interpolation minimum: with a small learning rate, the model converges to the minimum exponentially.

%It is also instructive to compute the second order term of $m$ in $\lambda$, which is given by
%\begin{equation}
%    \lim_{t\to\infty} \E[\hat{m}_t/t] = -\frac{2\lambda w_0^T  H w_0}{||w_0||^2} + \frac{2\lambda w_0^T  H^2 w_0}{||w_0||^2}
%\end{equation}

\subsection{Proof of Proposition~\ref{prop: phase diagram}}\label{app sec: phase diagram}
\begin{proof}
    To repeat, we consider the dynamics of SGD around a saddle:
\begin{equation}
    \ell = -\chi \sum_i u_i w_i,
\end{equation}
where we have combined $xy$ into a single variable $\chi$. The dynamics of SGD
is 
\begin{equation}
\begin{cases}
    w_{i,t+1} = w_{i,t} +\lambda \chi u_{i,t},\\
    u_{i,t+1} = u_{i,t} +\lambda \chi w_{i,t}.
\end{cases}
\end{equation}
Namely, we obtain a coupled set of stochastic difference equations. Since the dynamics is the same for all index $i$, we omit $i$ from now on. This dynamics can be decoupled if we consider two transformed parameters: $h_t = w_t + u_t$, and $m_t = w_t -u_t$. The dynamics for these two variables is
\begin{equation}
\begin{cases}
    h_{t+1} = h_{t} +\lambda \chi h_{t},\\
    m_{t+1} = m_{t} -\lambda  \chi m_{t}.
\end{cases}
\end{equation}
We have thus obtained two decoupled linear dynamics that take the same form as the linear regression problem we studied and our results on linear regression directly carry over. For example, as immediate corollaries, we know that $h$ converges to $0$ if and only if $\E[\log|1+\lambda \chi|] <0$, and $m$ converges to $0$ if and only if $\E[\log |1- \lambda \chi|] <0$.

When both $h$ and $m$ converge to zero in probability, we have that both $w$ and $u$ converge to zero in probability. For the data distribution under consideration, we have
\begin{equation}
    \E[\log|1+\lambda \chi|] = \frac{1}{2} \log\left|(1+\lambda)(1+\lambda a)\right|
\end{equation}
and 
\begin{equation}
    \E[\log|1-\lambda \chi|] = \frac{1}{2} \log\left|({1-\lambda})({1-\lambda a})\right|.
\end{equation}
There are four cases: (1) both conditions are satisfied; (2) one of the two is satisfied; (3) none is satisfied. These correspond to four different phases of SGD around this saddle.
\end{proof}




\clearpage

\section{Additional Experiments}\label{app sec: experiments}


\subsection{Typical training trajectories}\label{app sec: typical trajectory}

See Figure~\ref{fig:toy dynamics}. We see that $e^{2m}$ better reflects the convergence rate of the typical trajectories and is easy to estimate. In contrast, the variance of SGD is very difficult to estimate and is dominated by outlier trajectories.

\begin{figure}
    \centering
    \includegraphics[width=0.3\linewidth]{plots/toy_trajectory_small.png}
    \includegraphics[width=0.3\linewidth]{plots/toy_trajectory.png}
    %\includegraphics[width=0.285\linewidth]{plots/convergence_speed.png}
    \vspace{-1em}
    \caption{\small Dynamics and convergence rate of SGD as measured by the logarithmic rate ($e^{2m}$), and the squared error ($\langle |w_t -w^*|^2\rangle$). The green lines show $200$ independent samples of training. For a small learning rate (\textbf{left}), the two trajectories are similar, both agreeing quite well with the majority trajectories. For a large learning rate (\textbf{middle}), the proposed quantity $e^{2m}$ agrees with the majority of the trajectories, whereas $\langle |w_t -w^*|^2 \rangle$ is dominated by rare trajectories that have lost stability, and is thus far larger than the typical trajectories. $e^{2m}$ thus better reflects the convergence rate of the typical trajectories and is easy to estimate.} %The \textbf{right} figure shows the convergence speed of the dynamics as a function of the learning rate, compared with the theoretical prediction of our theory.}
    \label{fig:toy dynamics}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.26\linewidth]{plots/phase_diagram_300000_edited.png}
    \includegraphics[width=0.31\linewidth]{plots/saddle_linear_ppt.png}
    \includegraphics[width=0.31\linewidth]{plots/saddle_tanh_ppt.png}

    \caption{Convergence to low-rank solutions in nonlinear neural networks. At every training step, we sample input $x \sim \mathcal{N}(0, I_{200})$ and noise $\epsilon \sim \mathcal{N}(0, \sqrt{2}I_{200})$, and generate a noisy label $y=\mu x + (1-\mu) \epsilon$. $1-\mu$ controls the level of the noise. Left: the stability of the saddle low-rank saddle for a corresponding $1d$ input given by Proposition~\ref{fig: phase diagram}. We compare the rank of the last layer of a trained two-layer model for $\lambda \in (0, 0.25]$ with the theoretical prediction. Yellow region denotes full rank. The darker the color, the lower the rank of the trained 
 model. \textbf{Middle}: Linear network. \textbf{Right}: tanh network. The red dashed line shows the theoretical prediction of the appearance of low-rank structure computed by numerically integrating the condition in Proposition~\ref{fig: phase diagram}.}
    \label{fig: nn low rank}
\end{figure}
\subsection{Neural Network Experiments}
See Figure~\ref{fig: nn low rank} presents the enlarged numerical results described in Section~\ref{sec: nn experiment}.


%\subsection{Phase Diagram at Different Batch Sizes}
%See Figure~\ref{app fig:batch size phase diagram}. We see that as the batch size increases, it becomes more and more difficult to converge to a low-rank solution.




\clearpage
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.33\linewidth]{plots/Adam/Adam_linear.png}
    \includegraphics[width=0.33\linewidth]{plots/Adam/Adam_tanh.png}
    \includegraphics[width=0.33\linewidth]{plots/Adam/Adam_relu.png}
    \includegraphics[width=0.33\linewidth]{plots/Adam/Adam_swish.png}
    \caption{Rank of the converged solution for two-layer linear (upper left), tanh (upper right), relu (lower left) and swish (lower right) models.}
    \label{fig: nn adam}
\end{figure}


\subsection{Experiment with Adam}
We note that the phenomena we studied is not just a special feature of the SGD, but, empirically, seems to be a universal feature of first-order optimization methods that rely on minibatch sampling. Here, we repeat the experiment in Figure~\ref{fig: phase diagram}. We train with the same data and training procedure, except that we replace SGD with Adam \citep{journals/corr/KingmaB14_adam}, the most popular first-order optimization method in deep learning. Figure~\ref{fig: nn adam} shows that similar to SGD, Adam also converges to the low-rank saddles in similar regions of learning rate and $\mu$.



\clearpage
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.33\linewidth]{plots/Sparsity/module.layer1.1.conv2.weight.png}
    \includegraphics[width=0.33\linewidth]{plots/Sparsity/module.layer2.1.conv2.weight.png}
    \includegraphics[width=0.33\linewidth]{plots/Sparsity/module.layer3.1.conv2.weight.png}
    \includegraphics[width=0.33\linewidth]{plots/Sparsity/module.layer4.1.conv2.weight.png}
    \caption{Sparsity of the convolutional layers in a ResNet18. Here, we show the number of parameters are approximately zero in the two of the largest convolutional layers, each containing roughly 1M parameters in total. The figure respectively show layer1.1.conv2 (upper left), layer2.1.conv2 (upper right), layer3.1.conv2 (lower left), layer4.1.conv2 (lower right).}
    \label{app fig: resnet sparsity}
\end{figure}
\subsection{ResNet18 Experiments}
The task we study is the classification in CIFAR-10 using the ResNet18 model. The model is trained by SGD with a constant learning rate using a batch size of $32$ for $10^5$ iterations. The input images are augmented by random crops and random horizontal flips to reduce overfitting. Two hyperparameters, that is, learning rate and noise rate, are controlled during the experiments. The learning rate is defined conventionally, while the noise rate is the probability of reselecting the label from a uniform distribution for every input image at every iteration of training. The noise for each image is independent and is injected during training, so it adds uncertainty to the training and does not introduce bias to the data.

The trained model is evaluated by its accuracy on the test data set, sparsity of parameters, and the rank of the feature space. The sparsity of parameters is defined as the fraction of zero parameters in a specific layer. Before the last linear layer, our implementation of ResNet18 has $512$ output nodes, which are also perceived as the $512$ features encoded from the image. We randomly select $1000$ images from the test data set of CIFAR-10 and calculate the $512$ features for each of them, then find the dimensionality of the linear space of the $1000$ $512$-dimensional vectors. See Figure~\ref{app fig: resnet sparsity}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.33\linewidth]{plots/Sparsity/module.layer1.1.conv2.weight_static.png}
    \includegraphics[width=0.33\linewidth]{plots/Sparsity/module.layer2.1.conv2.weight_static.png}
    \includegraphics[width=0.33\linewidth]{plots/Sparsity/module.layer3.1.conv2.weight_static.png}
    \includegraphics[width=0.33\linewidth]{plots/Sparsity/module.layer4.1.conv2.weight_static.png}
    \caption{Sparsity of the convolutional layers in a ResNet18, when there is static noise in the training data. Here, we show the number of sparse parameters in the two of the largest convolutional layers, each containing roughly 1M parameters in total. The figures respectively show layer1.1.conv2 (upper left), layer2.1.conv2 (upper right), layer3.1.conv2 (lower left), and layer4.1.conv2 (lower right).}
    \label{app fig: resnet sparsity static}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.33\linewidth]{plots/cifar10_static_noise.png}
    \includegraphics[width=0.33\linewidth]{plots/cifar10_static_noise_acc.png}
    \caption{Rank (left) and test accuracy of the ResNet18 trained in a data set with static noise. The transition of the rank has a clear boundary. The model has full rank but random-guess level performance for large noise and small learning rates.}
    \label{app fig: cifar static}
\end{figure}

Also, see Figure~\ref{fig:resnet18 different batchsize}. The rank of the learned representation decreases as we decrease the batch size of SGD.


\begin{figure}[t!]
    \centering
    %\includegraphics[width=0.3\linewidth]{plots/cifar10_dynamic_noise.png}
    %\includegraphics[width=0.26\linewidth]{plots/cifar10_dynamic_noise_phase_boundary.png}
    \includegraphics[width=0.26\linewidth]{plots/resnet_bs.png}
    \caption{\small The rank of the penultimate-layer representation of Resnet18 trained with different levels of batch sizes. In agreement with the phase diagram, the model escapes from the low-rank saddle as one increases the batch size.}
    \label{fig:resnet18 different batchsize}
\end{figure}

\end{document}