\section{Experiments}
We now turn to a set of experiments that validate and highlight the generation capabilities of Set-the-Scene.

\vspace{-0.3cm}
\paragraph{Implementation Details}
Our method uses the Stable-Diffusion 2.0 model~\cite{rombach2021highresolution} implemented in \textit{Diffusers}~\cite{von-platen-etal-2022-diffusers}. For score-distillation, we use~\cite{metzer2022latent}. During training we iteratively pass over the objects one after the other and apply 10 training steps for each object, global iterations are interleaved between the objects. We train for about 15K iterations, with about 5K of them being global iterations.


\input{figures/more_scenes/more_scenes}
\input{figures/no_sketch_shape/no_sketch_shape}



\subsection{Scene Generation}
\paragraph{Qualitative Results} Figure~\ref{fig:different styles} shows results of generated scenes guided by different text prompts. 
One can see that our method closely follows the given proxies in terms of location and coarse shape. 
For example, observe how in the first row the wardrobe and bed are consistently placed in the scene even when the guiding text prompt changes.
Nevertheless, our method is still able to expressively alter the style of the generated shapes according to the guiding text prompt given the geometric constraints, thus offering both control and expressiveness.
In Figure~\ref{fig:more_scenes}, the objects are used multiple times within a single scene.
This is done by defining a set of object proxies that share the same object NeRF, explicitly enforcing similarity between a set of duplicated objects in a scene, which is difficult to achieve when generating a scene directly without object proxies.
This design choice also enables aggregating information about how an object is viewed from different locations and viewing angles into a single NeRF model, instead of training each model separately. 
Figure~\ref{fig:no_sketch_shape} shows results where an object in the scene is generated without any specific geometry constraint, but rather only with its respective location in the scene and a guiding text prompt, showing the flexibility of our controls.
Finally, Figure~\ref{fig:convergence} presents the exact text prompts used for a specific scene along with the convergence of each object and the scene as a whole during the optimization process.


\vspace{-0.3cm}
\paragraph{Comparisons} Recent text-to-3D methods generate the scene as a whole and do not utilize a composable representation. This makes it harder to control and can cause it to fail on complex scenes. 
To highlight this issue, Figure~\ref{fig:comparison} shows a result of Latent-NeRF~\cite{metzer2022latent} on our scene text prompt. Observe how Latent-NeRF struggles with generating the complex scene.
We note that although methods like Dreamfusion~\cite{poole2022dreamfusion} might be able to generate better scenes due to their larger diffusion model~\cite{imagen}, they still would not provide explicit control.
Furthermore, as Latent-NeRF is the basis of our local optimization process, comparison to it highlights the improvements gained using our scheme.

\input{figures/convergence/convergence}
\input{figures/comparison/fig}

\vspace{-0.3cm}
\paragraph{Ablation Study} Next, we perform an ablation study over our proposed Global-Local training scheme.
Figure~\ref{fig:ablation} shows results for the same proxy configuration with and without the global optimization phase, which makes several important aspects of joint optimization noticeable.
First, observe how color schemes between objects do not match well without joint optimization steps, for example, the sofa set which is generated in two different colors.
This supports the claim that without global optimization steps the model will not be able to match the objects well.
Moreover, the results generated without applying global optimization steps do not blend well within the scene and tend to look like they have been pasted over, which is expected for objects that were trained separately.
By contrast, our method generates globally consistent scenes with realistic shadows generated on the background object, resulting in more natural renderings.
\input{figures/ablation/ablation}

\vspace{-0.3cm}
\paragraph{User Study} We conducted a user study to quantitatively assess the effectiveness of our training scheme. We chose 10 scenes and applied both our Global-Local and Local-Only training schemes. We then gave 40 participants two sets of tasks. In the ranking task, participants are presented with two results side-by-side (in random order) and are asked to choose their preferred result with respect to two aspects: (a) realism; and (b) compatibility between the objects in the scene. In the second set, participants are presented with only a single result and are asked to rank it on a scale of 1-5 with respect to (a) realism; (b) object compatibility; and (c) text fidelity.
All results are shown as short videos showing the moving scene to allow participants to better evaluate the generated results. Table~\ref{tb:user_study_rank} shows the outcome of our study. One can see that using our Global-Local method is superior to a Local-Only solution and results in generally higher scores for the generated images.
\input{figures/user_study/table}



\input{figures/editing/edit_placment}

\subsection{Scene Editing}
Having shown our scene generation capabilities, we now turn to evaluate our ability to edit already-generated shapes.
\paragraph{Placement Editing} Figure~\ref{fig:edit_placment} shows some examples of placement editing.
Due to our composable representation, Set-the-Scene inherently allows for editing of object placement after training by editing the object proxies, adding new proxies, or even removing existing ones. Note that this can be done without any additional fine-tuning steps. One can additionally apply some fine-tuning steps to further improve the result with respect to the new placement.

\paragraph{Geometry Editing} Figure~\ref{fig:edit_geometry} shows how we can edit the geometry of a specific NeRF within the scene by changing its corresponding object, which allows for easy and intuitive control over the implicit NeRF.
As the proxy geometry is based on a user-defined mesh, the corresponding mesh can be easily edited with existing tools such as MeshLab~\cite{meshlab} or Blender~\cite{blender}. One can see that after fine-tuning the edited result remains faithful to the original scene.

\paragraph{Appearance Editing} Finally, Figure~\ref{fig:edit_color} demonstrates that our method can easily change the appearance of specified objects in the scene independently of other objects or the geometry of the edited object.
Our method is able to change the color scheme of both sofas while keeping them well-matched with one another. This is not the case when optimizing without global iterations, as also shown in the same Figure. Observe for example how the exact shade of red does not much well when using only local iterations.
\input{figures/editing/edit_geometry}
\input{figures/color/fig}
\input{figures/limitations/fig}


\paragraph{Limitations.} While our experiments show the capabilities of our approach, it is worthwhile to note that there are still some limitations. 
First of all, the quality of our results is governed by the local generation process which is based on score distillation with a latent diffusion model. This approach still generally lacks in terms of quality and resolution, and in turn, limits our generation's capabilities. 
Another limitation is that the objects in the scene may be generated in the background NeRF as textures, without a corresponding geometry, as shown in Figure~\ref{fig:limitation}.
We believe that this occurs due to the limited viewing angles available when optimizing an indoor scene.
Finally, our convergence time is also governed by the number of unique NeRFs in the scene, where adding more objects requires more optimization time, as each object requires its own local iterations.



