\section{Related Work}

\input{figures/pipeline/pipeline}

\paragraph{Text-Driven Shape Generation}
3D shape generation from text has been a well-researched topic for the past couple of years. 
Text2Mesh~\cite{michel2022text2mesh}, ClipMesh~\cite{khalid2022clipmesh} and Tango~\cite{chen2022tango} use Clip~\cite{radford2021learning} to optimize a triangular mesh to match an input text prompt.
ClipForge~\cite{sanghi2021clipforge} trains a point cloud generator conditioned on Clip embeddings, such that novel shapes can be generated solely by a single matching text prompt.
DreamFields~\cite{jain2021dreamfields} uses Clip supervision to guide a 3D NeRF scene to match a target text. 

There has been an extraordinary development in NeRF papers in recent years. 
Originally used for capturing scenes from a collection of images~\cite{mildenhall2021nerf, barron2021mip, mueller2022instant}, NeRFs have recently also been adopted for shape generation~\cite{jain2021dreamfields, poole2022dreamfusion, wang2022score, metzer2022latent, lin2022magic3d, liao2023text}.
DreamFusion~\cite{poole2022dreamfusion} first introduced \textit{Score-Distillation} for generating novel objects conditioned on an input text prompt by leveraging a pretrained 2D diffusion model. 
Latent-NeRF~\cite{metzer2022latent} extended DreamFusion to the latent domain in order to leverage the publicly available Stable-Diffusion~\cite{rombach2021highresolution}. Additionally, Latent-NeRF introduced \textit{Sketch-Shapes} as proxies for specifying a desired approximate target geometry. Yet, their approach is designed to generate only a single object and not multiple ones.


 \paragraph{Scene Generation}
Scene generation has also been tackled before using different approaches. 
Koh \etal \cite{koh2022simple} uses point cloud rendering as a 3D consistent representation and a 2D GAN architecture to synthesize novel realistic and spatially consistent viewpoints of indoor scenes. 
SceneDreamer~\cite{chen2023scenedreamer} is able to generate large-scale 3D landscapes from a collection of images, leveraging a 3D consistent \textit{bird-eye-view} that is trained using a 2D GAN objective.
SceneScape~\cite{fridman2023scenescape} generates novel parts of a scene conditioned on an input text description of the sense and an initial image, by iteratively warping and out-painting the previously generated image according to specified camera movements. 
These methods generate the scene as a whole and do not offer composable control over the generated scenes.



\paragraph{Composable Neural Radiance Fields}
Decomposing neural scenes enables controlling different objects in the scene in a disentangled manner, allowing for explicit editing, removal, and addition of objects.
Compressible-composable NeRF~\cite{tang2022compressible} utilizes a tensor representation of the scene for compression, which also enables rendering multiple objects together as a unified scene.
GIRAFFE~\cite{niemeyer2021giraffe} uses an adversarial loss to train a decomposable generator of feature fields, which when rendered together, form a single scene. The disentangled control over each individual feature field allows controlling the location and appearance of each individual object in the scene.


Object-Centric NeRF~\cite{guo2020object} is able to render multiple NeRF implicit models altogether, where each model was independently optimized to capture a single object.
Similarly, Panoptic Neural Fields~\cite{kundu2022panoptic} also trains separate implicit models for each object, plus a single model for the background. Additionally, each object is associated with a 3D bounding box, which is considered in the merging policy between implicit models at the volumetric rendering stage of the entire scene. 
Nerflets~\cite{Zhang2023Nerflets} propose a set of local NeRFs that represent a scene as a collection of decomposed objects. Each of them maintains its own spatial position, orientation, and extent, allowing for efficient structure-aware 3D scene representation from images. 
Object-NeRF~\cite{yang2021learning} is able to learn individual per-object NeRF models of a cluttered scene, through object-level supervision that is achieved by leveraging a rough segmentation mask of each training image. We adopt the same ray aggregation strategy as Object-NeRF for rendering multiple implicit models in a single image. 

Note that all the above methods are not generative nor text-guided and are limited in their editing capabilities.
Similarly to our global-local paradigm, DisCoScene~\cite{xu2022discoscene} also learns to generate controllable scenes at inference time, 
yet they require a 3D object-level training dataset, while our method does not use any 3D supervision.







