
\section{Method}
We turn to describe Set-the-Scene. We start by defining object proxies and composable NeRF rendering and then formulate our Global-Local training. Finally, we show how one may easily apply post-training edits using our method.

\subsection{Composable NeRF Rendering}
In order to allow object-level optimization and generation, we represent our scenes as a set of composable NeRFs. 
For explaining our proposed strategy, we first describe general NeRF rendering and the concept of \textit{object proxies} for improved control over the scene.

\vspace{-0.3cm}
\paragraph{NeRF Rendering} A single NeRF~\cite{mildenhall2021nerf} represents a 3D volumetric scene with a 5D function $f_\Theta$ that maps a 3D coordinate $\mathbf{x} = (x,y,z)$ and a 2D viewing direction $\mathbf{d} = (\theta, \phi)$ into a volume density $\sigma$ and an emittance color $\mathbf{c} = (r, g, b)$.
The rendering in Equation~\eqref{eq:rendering} is utilized to render the scene from a defined camera location $o$, by shooting rays from $o$ through each pixel and integrating them for color.
More specifically, given a ray $\mathbf{r}$ originating at $\mathbf{o}$ with direction $\mathbf{d}$, we query $f_\Theta$ at points $\mathbf{x}_i = \mathbf{o} + t_i \mathbf{d}$ that are sequentially sampled along the ray to get densities $\{\sigma_i\}$ and colors $\{\mathbf{c}_i\}$.
Finally, the pixel color is taken to be:
\begin{equation}
    \label{eq:rendering}
    \begin{split}
    \mathbf{\hat C}(\mathbf{r}) = \sum_i  T_i \alpha_i \mathbf{c}_i, 
    T_i = \prod_{j < i} (1 - \alpha_i), \\
    \alpha_i = 1 - \exp(-\sigma_i \delta_i), \delta_i = t_{i+1} - t_i
    \end{split}
\end{equation}
where $\delta_i$ is the step size, $\alpha_i$ is the opacity, and $T_i$ is the transmittance.


\input{figures/different_styles/fig}

 \paragraph{Object Proxies} 
When rendering multiple NeRFs in the same scene, one first has to define their respective locations in the scene.
We do so through \textit{object proxies}, where each proxy is associated with a NeRF model, offset position, orientation, and size, which define its settings in the scene.
We designed our method so that a single NeRF can be associated with multiple proxies. This allows rendering scenes with numerous instances of the same object, which in turn can be optimized together, as discussed in Section~\ref{subsec:training}.
\paragraph{Multi-Proxy NeRF Rendering}
When rendering multiple objects together, we divide the sampled points from each ray between the different objects, such that each object gets a fraction of the points. 
In order to integrate a single ray over different proxies, each set of points has to undergo a rigid transformation from the scene coordinate system to the proxy coordinate system, such that the computed colors and opacity values are calculated at the object level.
This allows sharing information between different proxies associated with a single NeRF model, and between the Local-Global training phases described in Section~\ref{subsec:training}.
 
 
We generalize the treatment in Equation~\ref{eq:rendering} by  summing over different NeRFs and transforming their input points according to their proxy parameters, i.e.,
\begin{equation}
    \label{eq:rendering2}
    \begin{split}
    \mathbf{\hat C}(\mathbf{r}) &= \sum_i  T_i \alpha_i^{(k)} \mathbf{c}^{(k)}_i, \\
    x_i^{(k)} & = R^{(k)}(x_i + {\rm loc}^{(k)}),
    \end{split}
\end{equation}
where $\alpha_i^{(k)}$  is the output of the $k$th NeRF, $R^{k}$  is a rigid matrix defined by the proxy's scale and orientation, and ${\rm loc^{k}}$ is a 3D coordinate defined by the proxy's location. The index $k$ is selected to be $k = i~{\rm mod}~N_{\rm obj}$, where $N_{\rm obj}$ is the total number of objects in the scene, i.e., we sample alternatively between each of the objects equitably.

\subsection{Global-Local Training}\label{subsec:training}
Given our composing mechanism, we now turn to describe our training losses and supervision technique.
\vspace{-0.3cm}
\paragraph{Score Distillation}
To optimize a NeRF using a text-prompt we follow the score distillation loss proposed in~\cite{poole2022dreamfusion}.
Score distillation turns a pretrained diffusion model $\mathcal{M}$ to a critique that is able to provide per-pixel gradients that measure the similarity of a given image to a target text.

\begin{equation}
    \nabla_x L_{sds} \sim \epsilon_{\theta} (x, \mathcal{T}) - \epsilon,
\end{equation}
where $\epsilon$ is a random noise purposely added to the image $x$, $\epsilon_{\theta}$ is the predicted noise by $\mathcal{M}$, and $\mathcal{T}$ is the target text prompt. This allows us to optimize a NeRF to gradually match a given text prompt. In practice, we apply the score-distillation directly to the latent representation and only later decode the results as proposed in~\cite{metzer2022latent}.
\vspace{-0.3cm}
\paragraph{Interleaved Training}

Our method involves an iterative process where we alternate between optimizing each object individually and optimizing the entire scene as a whole. This allows us to take advantage of our composable representation and create objects that harmonize well together but can still be rendered independently.
The object-level iterations are especially important for optimizing occluded areas, which might not be visible at all on the scene level.

In practice, for object-level iterations we choose one of the scene objects and render it in its canonical coordinate system, $\mathcal{O}_i$, such that it is located at the origin and we optimize it with a user-provided text prompt that describes it (\textit{e.g.} ``a wardrobe''). For scene-level iterations, we render all the objects together based on their proxies in the scene coordinate system $\mathcal{S}$ and use a text prompt describing the scene as a whole (\textit{e.g.} ``a baroque bedroom'').

\vspace{-0.3cm}
\paragraph{Defining Proxy Geometry}
A proxy object is always used to define a NeRF placement in the scene. For even higher levels of control, we 
adopt the shape loss from~\cite{metzer2022latent} and allow user-defined shape proxies for each individual object in the scene.
The shape constraints allow users to define proxy geometries in the form of 3D-like sketches and control the dimensions and structure of the generated object, resulting in a much more controllable process.

In practice, the geometry proxy constraint is imposed through an auxiliary loss function, applied both on the scene scale $\mathcal{S}$ and on each individual object scale $\mathcal{O}_i$ alongside the score distillation loss function $L_{sds}$:
\begin{equation*}
    L_{shape} = CE(\alpha_{NeRF}(p), \alpha_{GT}(p)) \cdot (1- e^{-\frac{d^2}{2\sigma_S}}),  
\end{equation*}
where $\alpha_{NeRF}$ is the NeRF's occupancy, $\alpha_{GT}$ is the occupancy of the specified proxy, $d$ is the distance to the proxy's surface, and $\sigma_S$ is a hyperparameter that controls the leniency of the constraint.


\subsection{Post-Training Editing}
Given a generated scene, one might wish to modify some aspects of it. We propose several different tools for refining and editing a generated scene. 

\vspace{-0.4cm}
\paragraph{Placement Editing} The composable formulation of Set-the-Scene inherently allows editing objects' placement in the scene. This is done by changing the proxy location and updating the rays accordingly during rendering. The same technique can also easily duplicate or remove objects.

\vspace{-0.4cm}
\paragraph{Shape Editing} To modify an object's geometry, we simply edit the proxy's geometry and then fine-tune the scene for more iterations. This allows defining shape edits without having to extract a mesh from the implicit NeRF representation. Only the weights of the relevant NeRF are updated in the fine-tuning, and we alternate between rendering it in its canonical coordinate system $\mathcal{O}_i$ and with the rest of the scene in $\mathcal{S}_i$. Scene-level iterations are key to ensuring that the object remains consistent with the scene when edited.

\vspace{-0.4cm}
\paragraph{Color Editing} Finally, we show that one may also edit the color scheme of a generated object. This is done by using an architecture where the density and albedo predictions are separable, and the albedo is predicted using a set of additional fully-connected layers. During fine-tuning we can then optimize the albedo layers to guarantee that the generated shape will not change while modifying its color. 



