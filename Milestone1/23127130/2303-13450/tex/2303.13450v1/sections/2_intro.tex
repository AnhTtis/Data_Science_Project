\section{Introduction}
Creating high-quality 3D content has traditionally been a time-consuming process, requiring specialized skills and knowledge. However, recent advances in text-to-3D synthesis~\cite{poole2022dreamfusion,lin2022magic3d,metzer2022latent, wang2022score} are revolutionizing the generation of 3D scenes. These methods use pretrained text-to-image diffusion models to optimize a Neural Radiance Field (NeRF) and generate 3D objects that match a given text prompt.

In spite of these exciting advancements, current solutions still lack the capability to create a specific envisioned scene. That is because controlling the generation of text prompts alone is extremely challenging, especially if one wants to describe specific objects with defined dimensions and geometry, and locate them at specific positions. 
Moreover, once generated, modifying specific aspects of the scene, while leaving the others untouched, can prove to be challenging. Various aspects of a scene may need to be modified after generation, such as the position or orientation of certain objects, or the texture or geometry of individual components. 
One may also want to export a single object to be used in another scene. 
However, current methods represent the scene as a whole, and objects are interdependent in their representation, making it impossible to edit a specific scene component or use objects in other scenes.


\input{figures/ablation_teaser/fig}


 Given that the generation process of scenes using current text-to-3D methods takes a considerable amount of time, the need for interactive editing capabilities has become even more apparent.
One may potentially save time and gain control over individual objects by generating them separately and then just rendering them together at inference time. However, such an approach has several limitations: it cannot generate objects that interact with each other; it cannot ensure consistency in style between objects; and it cannot model the interaction among objects, like shadows and other global shading effects, see Figure~\ref{fig:ablation_teaser} (A). 

In this paper, we introduce a novel framework for synthesizing a controllable scene using text and object proxies, using a \textbf{Global-Local} approach. The key idea is to represent the scene as a composition of multiple object NeRFs, each built around an object proxy. The models are jointly optimized to ``locally'' represent the required object and ``globally'' to be part of the larger scene. Both the local and the global optimization propagate gradients into the same models, creating a harmonious scene composed of disentangled objects, see Figure~\ref{fig:ablation_teaser} (B).
For optimizing our objects and scenes, we follow~\cite{poole2022dreamfusion} and use the score distillation loss. Our method leverages the composability of our representation and iteratively alternates between localized training of individual objects and optimizing the scene as a whole, where objects are dependent on their representation. When optimizing a single NeRF, we simply render it on its own from a random viewpoint and apply score distillation based on
a text prompt describing the object.  For scene-level optimization, we shift the rays using a rigid transformation to match the desired placement defined by each object proxy and apply score distillation with a ``scene text prompt''. 


In many scenarios it is desirable to not only define the placement of an object, but also its dimensions and coarse geometry. Therefore, we also optionally apply a shape loss~\cite{metzer2022latent} on each object proxy to guide it towards a specific shape. 
In addition, we demonstrate that our approach enables the definition of multiple object proxies that can be linked to a single object NeRF. This permits the specification of replicated objects that are intended to be located in multiple positions throughout the scene (e.g., chairs around a table), while aggregating the score distillation from the different placements to optimize a single NeRF

Our proposed Global-Local approach not only provides more control during the training process, but also allows for better editing and fine-tuning of generated scenes. Specifically, using object proxies, we can easily control the placement of an object without the need for further refinement and even remove or duplicate the object as desired. Additionally, we can selectively fine-tune only parts of the scene by defining the set of proxies that are trained and fine-tuning the respective NeRF with modified %
text prompts. Furthermore, we demonstrate that the object proxy can be used to define geometry edits on the coarse shape, which are then applied during the fine-tuning process.

The contributions of our paper are threefold: (i) we propose to represent each object in the scene as a separate NeRF around a proxy, which allows getting a disentangled model for each object, (ii) we introduce a new optimization strategy that interleaves between single-object optimization and scene optimization, resulting in self-contained objects that can be combined to create a plausible scene, and (iii) our strategy provides control over the generated scene, both before and after its creation.

