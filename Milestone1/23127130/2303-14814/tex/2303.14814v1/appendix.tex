\onecolumn
\FloatBarrier

%\begin{minipage}{\textwidth}
\begin{center}
    {\bf {\LARGE Supplementary Material}}
\end{center}
\begin{center}
{\bf {\Large WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation}}
\end{center}
%\end{minipage}

\setcounter{figure}{5}
\setcounter{table}{7}

\section{Experimental details}

% \paragraph{Metrics. }

\noindent\textbf{Compositional prompt ensemble. }
Figure~\ref{fig:comp_prompt} provides a detailed list of prompts we adopt to perform compositional prompt ensemble proposed in Section~4.1 of the main text. %\ref{sec:zeroshot}. 
Recall that we consider two levels of prompts: \ie (a) state-level, and (b) template level. A complete prompt can be composed by replacing the token \verb|[c]| in a template-level prompt with one of state-level prompt, either from the normal or anomaly states. Each of the state-level prompt takes an object-level label \verb|[o]|. In our experiments, we use the object name words available for both MVTec-AD and VisA per dataset to replace \verb|[o]|. 


\begin{figure*}
\noindent\begin{minipage}[t]{0.32\linewidth}
(a) \emph{State}-level (normal)

{\tt \small
\begin{itemize}
    \item c := "[o]"
    \item c := "flawless [o]"
    \item c := "perfect [o]"
    \item c := "unblemished [o]"
    \item c := "[o] without flaw"
    \item c := "[o] without defect"
    \item c := "[o] without damage"
\end{itemize}
}

(b) \emph{State}-level (anomaly)

{\tt \small
\begin{itemize}
    \item c := "damaged [o]"
    \item c := "[o] with flaw"
    \item c := "[o] with defect"
    \item c := "[o] with damage"
\end{itemize}
}
\end{minipage}
\hfill
\begin{minipage}[t]{0.33\linewidth}
(c) \emph{Template}-level

{\tt \small
\begin{itemize}
\item "a cropped photo of the [c]."
\item "a cropped photo of a [c]."
\item "a close-up photo of a [c]."
\item "a close-up photo of the [c]."
\item "a bright photo of a [c]." 
\item "a bright photo of the [c]."
\item "a dark photo of the [c]."
\item "a dark photo of a [c]."
\item "a jpeg corrupted photo of a [c]."
\item "a jpeg corrupted photo of the [c]."
% \item "a blurry photo of the [c]."
% \item "a blurry photo of a [c]."
% \item "a photo of a [c]."
% \item "a photo of the [c]."
% \item "a photo of a small [c]."
% \item "a photo of the small [c]."
% \item "a photo of a large [c]."
% \item "a photo of the large [c]."
% \item "a photo of the [c] for visual inspection."
% \item "a photo of a [c] for visual inspection."
% \item "a photo of the [c] for anomaly detection."
% \item "a photo of a [c] for anomaly detection."
\end{itemize}
}
\end{minipage}
\begin{minipage}[t]{0.33\linewidth}
{\tt \small
\begin{itemize}
% \item "a cropped photo of the [c]."
% \item "a cropped photo of a [c]."
% \item "a close-up photo of a [c]."
% \item "a close-up photo of the [c]."
% \item "a bright photo of a [c]." 
% \item "a bright photo of the [c]."
% \item "a dark photo of the [c]."
% \item "a dark photo of a [c]."
% \item "a jpeg corrupted photo of a [c]."
% \item "a jpeg corrupted photo of the [c]."
\item {\rm (cont'd)} "a blurry photo of the [c]."
\item "a blurry photo of a [c]."
\item "a photo of a [c]."
\item "a photo of the [c]."
\item "a photo of a small [c]."
\item "a photo of the small [c]."
\item "a photo of a large [c]."
\item "a photo of the large [c]."
\item "a photo of the [c] for visual inspection."
\item "a photo of a [c] for visual inspection."
\item "a photo of the [c] for anomaly detection."
\item "a photo of a [c] for anomaly detection."
\end{itemize}
}
\end{minipage}
\caption{Lists of multi-level prompts considered in this paper to construct compositional prompt ensemble.}
\label{fig:comp_prompt}
\end{figure*}

\vspace{0.05in}
\noindent\textbf{Data pre-processing. }
For CLIP-based models, including our proposed WinCLIP and WinCLIP+, we apply the data pre-processing pipeline given in OpenCLIP \cite{ilharco_gabriel_2021_5143773} for both MVTec-AD and VisA datasets to minimize potential train-test discrepancy. Specifically, it performs a channel-wise standardization with the pre-computed mean \texttt{[0.48145466, 0.4578275, 0.40821073]} and standard deviation \texttt{[0.26862954, 0.26130258, 0.27577711]} after normalizing each RGB image into $[0, 1]$, followed by a bicubic re-sizing based on the \verb|Pillow| implementation. By default, we make the input resolution to be $240$ for the shorter edge from the re-sizing, to be compatible with ViT-B/16+ in our experiments. This re-sizing policy also applies to other baseline models for fairer comparisons, although we keep the remaining parts of their original data pre-processing pipelines. In addition, similar policy can also be used in other backbones with input of different resolutions.

\vspace{0.05in}
\noindent\textbf{Evaluation metrics. }
% For anomaly classification, we report (a) \emph{Area Under the Receiver Operating Characteristic} (AUROC) following the literature \cite{yi2020patch,defard2021padim,roth2022towards}, as well as (b) \emph{Area Under the Precision-Recall curve} (AUPR) and (c) \emph{$F_1$-score at optimal threshold} ($F_1$-max) for a clearer view against potential data imbalance (as discussed by \citet{zou2022spot}). For anomaly segmentation, we report (a) \emph{pixel-wise AUROC} (pAUROC) and (b) \emph{Per-Region Overlap} (PRO) \cite{bergmann2020uninformed} scores as per the previous works \cite{defard2021padim,li2021cutpaste}, and (c) \emph{(pixel-wise) $F_1$-max} as well in a similar manner to the anomaly classification evaluation. 
% Will mention the AUPR upon acceptance.
Although the AUROC is a good metric for balanced dataset, it provides an inflated view of model performance in imbalanced dataset, especially in anomaly segmentation where the normal pixels dominate anomalies. This is also discussed by Zou et al.~\cite{zou2022spot}. $F_1$-max is computed from the precision and recall for the anomalous samples at the optimal threshold, which is a more straightforward metric to measure the upper bound of anomaly prediction performance across thresholds. Thus we acknowledge that the low-shot anomaly segmentation is still not solved since our best model only achieves $<60\%$ $F_1$-max for both MVTec-AD and VisA, even though WinCLIP+ achieves $>95\%$ pixel-AUROC. In addition, our WinCLIP and WinCLIP+ outperform all the compared methods in terms of all these metrics on the setups, demonstrating the effectiveness of the proposed methods.

\vspace{0.05in}
\noindent\textbf{Other implementation details.}
(\textit{i}) The ViT-B/16+ architecture \cite{ilharco_gabriel_2021_5143773}, that we mainly adopt in our experiments, is a modification of ViT-B/16 \cite{touvron2021training} with (a) an increased dimension in both image ($768 \rightarrow 896$) and text ($512 \rightarrow 640$) embeddings, as well as in (b) the input resolution ($224^2 \rightarrow 240^2$; $196 \rightarrow 225$ tokens);
(\textit{ii}) We note that CLIP models require the square-shaped resolution, \eg $240^2$ for ViT-B/16+, to be compatible with the attention layers inside. 
Although the MVTec-AD benchmark already consists of square images, most of images in the VisA benchmark are non-squared (\eg $1500\times1000$) and simply taking a crop can affect the anomaly status of the given images. 
In this respect, to enable CLIP-based models properly handle non-squared images in our experiments, we perform a simple ``image tiling'' scheme. Specifically, for such non-squared images, we first extract multiple overlapping (squared) ``tiles'' of size the shorter edge $L_s$, by taking a sliding window across the longer edge. Then we average the predictions from the tiles to get the final (either in image- and pixel-level) prediction. The stride for the sliding is set to $0.8\cdot L_s$ at most, \ie the tiles have overlaps with its neighbors at least in $0.2\cdot L_s$; (\textit{iii}) In addition, for the baseline results, we use our re-implementation of SPADE \cite{cohen2020sub} and PaDiM \cite{defard2021padim}, and adopt the official implementation of PatchCore\footnote{\url{https://github.com/amazon-science/patchcore-inspection}} in our experiments. 

% \vspace{0.05in}
% \noindent\textbf{Discussion on evaluation metrics.} % Will mention the AUPR upon acceptance.
% As a remark, although the AUROC is a good metric for balanced dataset, it provides an inflated view of model performance in imbalanced dataset, especially in anomaly segmentation where the normal pixels dominate anomalies. This is also discussed by Zou et al.~\cite{zou2022spot}. $F_1$-max is computed from the precision and recall for the anomalous samples at the optimal threshold, which is a more straightforward metric to measure the upper bound of anomaly prediction performance across thresholds. Thus we acknowledge that the low-shot anomaly segmentation is still not solved since our best model only achieves $<60\%$ $F_1$-max for both MVTec-AD and VisA, even though WinCLIP+ achieves $>95\%$ pixel-AUROC. In addition, our WinCLIP and WinCLIP+ outperform all the compared methods in terms of all these metrics on the setups, demonstrating the effectiveness of the proposed methods.

\section{Additional results on ablation study}

% \begin{table*}[h]
%     \centering
%     \begin{adjustbox}{width=0.75\linewidth}
%     \input{tables/ablation_patchcore}
%     \end{adjustbox}
%     \caption{Caption}
%     \label{tab:ab_patchcore}
% \end{table*}
% \begin{table}[h]
%     \centering
%     \begin{adjustbox}{width=0.3\linewidth}
%     \input{tables/ablation_arch}
%     \end{adjustbox}
%     \caption{Caption}
%     \label{tab:ab_arch}
% \end{table}

\begin{table*}[t]
    \centering
    \hfill
    \begin{minipage}[t]{0.71\linewidth}
        \centering
        \begin{adjustbox}{width=\linewidth}
        \input{tables/ablation_patchcore}
        \end{adjustbox}
        \caption{Comparison of few-shot performances on MVTec-AD. We report the mean and standard deviation over 5 random seeds for each measurement. Bold indicates the best performance.}
        \label{tab:ab_patchcore}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.24\linewidth}
        \centering
        \small
        \begin{adjustbox}{width=\linewidth}
        \input{tables/ablation_arch}
        \end{adjustbox}
        \caption{Comparison of WinCLIP performance in AUROC (for AC) and pAUROC (for AS) on zero-shot MVTec-AD, across different CLIP backbone architectures. }
        \label{tab:ab_arch}
    \end{minipage}
    \hfill
    \vspace{-0.05in}
\end{table*}


\noindent\textbf{Comparison with CLIP-based PatchCore: } 
PatchCore \cite{roth2022towards}, a current state-of-the-art considered in our experiments, is originally based on the internal features of convolutional network: \eg WideResNet-50-2 (WRN-50-2)  \cite{zagoruyko2016wideresnet} pre-trained on ImageNet.   
In Table~\ref{tab:ab_patchcore}, we test whether PatchCore can further benefit from the CLIP-based backbone that our WinCLIP+ is based on. Specifically, we additionally consider two variants of PatchCore that take the patch-token features of CLIP-based ViT-B/16+ backbone, one from (a) the 6$^\text{th}$- and 9$^\text{th}$-layer of ViT (which corresponds to \texttt{block2} and \texttt{block3} in ResNet-like models as considered by \cite{roth2022towards}; ``\emph{hidden}''), and the other one from (b) the last layer of ViT (``\emph{last}''). Overall, we have the following observations. First, in case of the ViT-B/16+ backbone, PatchCore performs better with the last layer, which is in contrast to the cases of convolutional backbones. Second, compared to the original PatchCore, the CLIP-based variants achieve no better performances. Third, WinCLIP+ significantly outperforms ``PatchCore (last)'' where our WinCLIP+ also utilizes the last patch-token features, namely as referred as $\mathbf{F}^{\tt P}$ (Section~4.3 of the main text). % (Section~\ref{sec:winclip+})
The results confirm the effectiveness of (a) our simple association-based module over a more sophisticated PatchCore\footnote{Technically, PatchCore incorporates several techniques upon a patch-level memory scheme, \eg local patch aggregation, clustering and score re-weighting.} in ViT, and (b) the WinCLIP features $\mathbf{F}^{\tt W}$.


\vspace{0.05in}
\noindent\textbf{Effect of different CLIP backbones: }
Table~\ref{tab:ab_arch}, on the other hand, explores the effect of different CLIP architectures to the WinCLIP zero-shot performance. Specifically, on zero-shot setups, we compare AUROC (and pixel-AUROC) from WinCLIP in AC (and AS) testing over the CLIP pre-trained models available at OpenCLIP,\footnote{\url{https://github.com/mlfoundations/open_clip}} including our default choice of ViT-B/16+. To apply WinCLIP for ResNet-based backbones, we notice that the CLIP implementation of ResNet architectures incorporates an attention layer to perform the feature pooling, namely as \emph{attention pooling}, similar to ViT-based architectures. In this respect, for the CLIP-ResNet models, we apply our window-based inference to perform zero-shot AS from the convolutional feature map before the attention pooling, in the same way of applying WinCLIP for ViTs. Here, we remark that the effective patch size of each pixel on the last feature map (before the pooling) of ResNet-based models is designed to be $32$ (the downsampling rate), which is larger than those of ViTs we test, \eg of 16. Overall, we observe that ViT-based models generally show better performance compared to ResNets, in both AC and AS. The particular gap in AS is possibly due to the bigger patch sizes in ResNets, which can result in more blurry outputs. Still, we observe the performance benefits from larger models or resolutions in both types of architecture.

% \clearpage
\section{Additional qualitative results}

In Figure~\ref{fig:good_0shot_mvtec}-\ref{fig:good_4shot_visa}, we provide further qualitative results obtained from our (zero-shot) WinCLIP and (few-shot) WinCLIP+ for anomaly segmentation, both in MVTec-AD and VisA considered in our experiments. Specifically, we report MVTec-AD results in Figure~\ref{fig:good_0shot_mvtec} and \ref{fig:good_4shot_mvtec}, and VisA results in Figure~\ref{fig:good_0shot_visa} and \ref{fig:good_4shot_visa}.

\begin{figure*}[ht]
  \includegraphics[width=\linewidth]{figures/mvtec_0shot_good.png}
  \caption{Additional qualitative results from WinCLIP (0-shot), tested on MVTec-AD.}
  \label{fig:good_0shot_mvtec}
\end{figure*}

\begin{figure*}[ht]
  \includegraphics[width=\linewidth]{figures/mvtec_4shot_good.png}
  \caption{Additional qualitative results from few-shot WinCLIP+ (4-shot), tested on MVTec-AD.}
  \label{fig:good_4shot_mvtec}
\end{figure*}

\begin{figure*}[ht]
  \includegraphics[width=\linewidth]{figures/visa_0shot_good.png}
  \caption{Additional qualitative results from WinCLIP (0-shot), tested on VisA.}
  \label{fig:good_0shot_visa}
  \vspace{0.1in}
  \includegraphics[width=\linewidth]{figures/visa_4shot_good.png}
  \caption{Additional qualitative results from few-shot WinCLIP+ (4-shot), tested on VisA.}
  \label{fig:good_4shot_visa}
\end{figure*}

% \begin{figure*}[ht]
%   \includegraphics[width=\linewidth]{figures/mvtec_0shot_bad.png}
%   \caption{Curated illustrations of failure cases from zero-shot WinCLIP, tested on MVTec-AD.}
%   \label{fig:fail_0shot_mvtec}
% \end{figure*}


% \begin{figure*}[!t]
%   \includegraphics[width=\linewidth]{figures/mvtec_4shot_bad.png}
%   \caption{Curated illustrations of failure cases from few-shot WinCLIP (4-shot), tested on MVTec-AD.}
%   \label{fig:fail_4shot_mvtec}
% \end{figure*}

\clearpage
\noindent\textbf{Failure cases. } 
% \subsection{Failure cases}
We present some failure examples from both MVTec-AD and VisA for language driven zero-shot WinCLIP in Figure \ref{fig:fail_0shot}. Note that the normal images are shown just for better illustration and are not used in model prediction. The first major factor causing the failure is the logical anomaly \cite{bergmann2022beyond} illustrated in Figure \ref{fig:fail_0shot}\textcolor{red}{(a)}, \eg misplaced axis in cable, missing text on capsule, missing capacitor in PCB1 and bent component in PCB3. Such type of anomalies need to be clarified by normal reference images while language might be not sufficient. The issues are alleviated by our few-normal-shot WinCLIP+. The second major factor refers to tiny defect illustrated in Figure \ref{fig:fail_0shot}\textcolor{red}{(b)}, such as the ones in carpet, wood, capsule, macaroni1. We conjecture that spatial features with more local details might improve these cases, which is left for future exploration. The third major factor is the irrelevant deviation from normality that are not defects of interests illustrated in Figure \ref{fig:fail_0shot}\textcolor{red}{(c)}, \eg the tiny red/white dots in pill/hazelnut, extra ingredient on cashew, designed holes and acceptable scratches in PCB2. We hypothesize that more clarification on these deviation and a pre-trained model with better understanding on these states might alleviate the problem. Lastly, although WinCLIP can roughly localize anomalies such as the cases in bottle, tile, PCB4 and fryum, it makes some errors around the true positives, illustrated in Figure \ref{fig:fail_0shot}\textcolor{red}{(d)}. However, we argue this is minor as the rough anomaly localization is sufficient to explain where the defects are for visual inspection. 

\begin{figure*}[ht]
  \includegraphics[width=\linewidth]{figures/0shot_bad.png}
  \caption{Curated illustrations of failure cases from zero-shot WinCLIP.}
  \label{fig:fail_0shot}
\end{figure*}

\clearpage
\FloatBarrier
\section{Detailed quantitative results}

In this section, we report the detailed, subset-level performance values for the evaluation metrics provided in Table~1 and 4 of the main text. % Table~\ref{tab:ad} and \ref{tab:al} 
Specifically, we report MVTec-AD results in Table~\ref{tab:mvtec/ac/roc}-\ref{tab:mvtec/as/pf1} and VisA results in Table~\ref{tab:visa/ac/roc}-\ref{tab:visa/as/pf1}.

% \begin{table*}[!ht]
%   \centering
%   \small
%   \begin{adjustbox}{width=\linewidth}
%   \input{tables/mvtec_subset}
%   \end{adjustbox}
%   \caption{Comparison for anomaly classification on each subset of MVTec-AD.}
%   \label{tab:mvtec_subset}
% \end{table*}

\begin{table*}[ht]
  \centering
  \begin{adjustbox}{width=\linewidth}
  \input{tables/classwise/mvtec/ac/roc}
  \end{adjustbox}
  \caption{Comparison of anomaly classification (AC) performance in terms of class-wise AUROC on MVTec-AD. We report the mean and standard deviation over 5 random seeds for each measurement.}
  \label{tab:mvtec/ac/roc}
  \vspace{0.1in}
  \begin{adjustbox}{width=\linewidth}
  \input{tables/classwise/mvtec/ac/aupr}
  \end{adjustbox}
  \caption{Comparison of anomaly classification (AC) performance in terms of class-wise AUPR on MVTec-AD. We report the mean and standard deviation over 5 random seeds for each measurement.}
  \label{tab:mvtec/ac/aupr}
  \vspace{0.1in}
  \begin{adjustbox}{width=\linewidth}
  \input{tables/classwise/mvtec/ac/f1}
  \end{adjustbox}
  \caption{Comparison of anomaly classification (AC) performance in terms of class-wise $F_1$-max on MVTec-AD. We report the mean and standard deviation over 5 random seeds for each measurement.}
  \label{tab:mvtec/ac/f1}
\end{table*}

\begin{table*}[!ht]
  \centering
  \begin{adjustbox}{width=\linewidth}
  \input{tables/classwise/mvtec/as/proc}
  \end{adjustbox}
  \caption{Comparison of anomaly segmentation (AS) performance in terms of class-wise pixel-AUROC on MVTec-AD. We report the mean and standard deviation over 5 random seeds for each measurement.}
  \label{tab:mvtec/as/proc}
  \vspace{0.1in}
  \begin{adjustbox}{width=\linewidth}
  \input{tables/classwise/mvtec/as/pro}
  \end{adjustbox}
  \caption{Comparison of anomaly segmentation (AS) performance in terms of class-wise PRO on MVTec-AD. We report the mean and standard deviation over 5 random seeds for each measurement.}
  \label{tab:mvtec/as/pro}
  \vspace{0.1in}
  \begin{adjustbox}{width=\linewidth}
  \input{tables/classwise/mvtec/as/pf1}
  \end{adjustbox}
  \caption{Comparison of anomaly segmentation (AS) performance in terms of class-wise $F_1$-max on MVTec-AD. We report the mean and standard deviation over 5 random seeds for each measurement.}
  \label{tab:mvtec/as/pf1}
\end{table*}

\begin{table*}[!ht]
  \centering
  \begin{adjustbox}{width=\linewidth}
  \input{tables/classwise/visa/ac/roc}
  \end{adjustbox}
  \caption{Comparison of anomaly classification (AC) performance in terms of class-wise AUROC on VisA. We report the mean and standard deviation over 5 random seeds for each measurement.}
  \label{tab:visa/ac/roc}
  \vspace{0.1in}
  \begin{adjustbox}{width=\linewidth}
  \input{tables/classwise/visa/ac/aupr}
  \end{adjustbox}
  \caption{Comparison of anomaly classification (AC) performance in terms of class-wise AUPR on VisA. We report the mean and standard deviation over 5 random seeds for each measurement.}
  \label{tab:visa/ac/aupr}
  \vspace{0.1in}
  \begin{adjustbox}{width=\linewidth}
  \input{tables/classwise/visa/ac/f1}
  \end{adjustbox}
  \caption{Comparison of anomaly classification (AC) performance in terms of class-wise $F_1$-max on VisA. We report the mean and standard deviation over 5 random seeds for each measurement.}
  \label{tab:visa/ac/f1}
\end{table*}

\begin{table*}[!ht]
  \centering
  \begin{adjustbox}{width=\linewidth}
  \input{tables/classwise/visa/as/proc}
  \end{adjustbox}
  \caption{Comparison of anomaly segmentation (AS) performance in terms of class-wise pixel-AUROC on VisA. We report the mean and standard deviation over 5 random seeds for each measurement.}
  \label{tab:visa/as/proc}
  \vspace{0.1in}
  \begin{adjustbox}{width=\linewidth}
  \input{tables/classwise/visa/as/pro}
  \end{adjustbox}
  \caption{Comparison of anomaly segmentation (AS) performance in terms of class-wise PRO on VisA. We report the mean and standard deviation over 5 random seeds for each measurement.}
  \label{tab:visa/as/pro}
  \vspace{0.1in}
  \begin{adjustbox}{width=\linewidth}
  \input{tables/classwise/visa/as/pf1}
  \end{adjustbox}
  \caption{Comparison of anomaly segmentation (AS) performance in terms of class-wise $F_1$-max on VisA. We report the mean and standard deviation over 5 random seeds for each measurement.}
  \label{tab:visa/as/pf1}
\end{table*}