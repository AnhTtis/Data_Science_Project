% CVPR 2023 Paper Template
\pdfoutput=1
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper,pagenumbers]{article}
% \documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[]{cvpr} % To force page numbers, e.g. for an arXiv version
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{bm}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, \eg.
\usepackage{microtype}      % microtypography
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{wrapfig}
\usepackage{placeins}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\renewcommand{\ie}{\textit{i}.\textit{e}., }
\renewcommand{\eg}{\textit{e}.\textit{g}., }
\newcommand{\viz}{\textit{viz}., }
\newcommand{\dev}[1]{\footnotesize{$\pm$#1}}
\newcommand{\ghdev}[1]{\phantom{\dev{#1}}}

\newcommand{\note}[1]{\textcolor{red}{#1}}

\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}
\def\rvs{{\mathbf{s}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}
\def\rveps{{\boldsymbol{\epsilon}}}

\usepackage{color}
\definecolor{Red7}{rgb}{0.941, 0.243, 0.243}
\definecolor{Green7}{RGB}{55, 178, 77}
\definecolor{Tdgreen}{rgb}{0,0.4,0.7}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\ck}{\color{Green7}{\cmark}}
\newcommand{\xk}{\color{Red7}{\xmark}}

\newcommand{\jh}[1]{\textcolor{blue}{#1}}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\hypersetup{citecolor=Tdgreen}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\newcommand{\citet}{\cite}
\newcommand{\Yang}[1]{\textcolor{magenta}{\textbf{[Yang: #1]}}}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{3976} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

    \makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or \dagger\or *\or \ddagger\or
   \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
   \or \ddagger\ddagger \else\@ctrerr\fi}}
    \makeatother

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation}

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }
% \author{Yang Zou\inst{1}, Jongheon Jeong\inst{2}\thanks{~~work done during an Amazon internship}, Latha Pemula\inst{1} \\Dongqing Zhang\inst{1}, Onkar Dabeer\inst{1}}

\author{
Jongheon Jeong$^{2*}$\thanks{Work done during an Amazon internship.}~~~~
Yang Zou$^{1}$\thanks{The authors contributed equally.}~~~~
Taewan Kim$^{1}$\\
Dongqing Zhang$^{1}$~~~~
% Avinash Ravichandran$^{1}$\thanks{work done work done as part of AI labs.}~~~~
Avinash Ravichandran$^{1}$\thanks{Work done as part of AWS AI Labs.}~~~~
Onkar Dabeer$^{1}$\\
$^{1}$~AWS AI Labs~~~~
$^{2}$~KAIST\\
% {\small\Letter~~\tt
% \MYhref[black]{mailto:yanzo@amazon.com}{yanzo@amazon.com}}
}

\maketitle
% \twocolumn[{
% \maketitle
% \begin{center}
%     \captionsetup{type=figure}
%     \includegraphics[width=\textwidth]{figures/teaser.png}
%     \captionof{figure}{Test caption}
% \end{center}
% }]

%%%%%%%%% ABSTRACT
\begin{abstract}
% 4rd version
Visual anomaly classification and segmentation are vital for automating industrial quality inspection. The focus of prior research in the field has been on training custom models for each quality inspection task, which requires task-specific images and annotation. In this paper we move away from this regime, addressing zero-shot and few-normal-shot anomaly classification and segmentation. Recently CLIP, a vision-language model, has shown revolutionary generality with competitive zero-/few-shot performance in comparison to full-supervision. But CLIP falls short on anomaly classification and segmentation tasks. Hence, we propose window-based CLIP (WinCLIP) with (1) a compositional ensemble on state words and prompt templates and (2) efficient extraction and aggregation of window/patch/image-level features aligned with text. We also propose its few-normal-shot extension WinCLIP+, which uses complementary information from normal images. In MVTec-AD (and VisA), without further tuning, WinCLIP achieves \mbox{$91.8\%/85.1\%$ $(78.1\%/79.6\%)$} AUROC in zero-shot anomaly classification and segmentation while WinCLIP+ does \mbox{$93.1\%/95.2\%$ $(83.8\%/96.4\%)$} in 1-normal-shot, surpassing state-of-the-art by large margins.
% In MVTec-AD and VisA benchmarks, without further tuning, WinCLIP achieves $91.8\%, 78.1\%/85.1\%, 79.6\%$ AUROC in zero-shot anomaly classification/segmentation while WinCLIP+ achieves $93.1\%, 83.1\%/95.2\%, 95.2\%$ AUROC in 1-normal-shot setup, surpassing state-of-the-art with large margins.

%Previous works mainly focus on training a dedicated model using numerous or a few normal images for each object. However, with limited generality in these works, it is not scalable to deploy a different model for each category considering the wide use cases. In this paper, we address efficient zero-shot and few-normal-shot anomaly classification and segmentation guided by language. Recently, large-scale vision-language pre-training such as CLIP has shown revolutionary generality: language guides CLIP to present competitive zero-/few-shot performances compared to fully-supervised counterparts on various downstream tasks including object classification and segmentation. Based on CLIP, we propose window-based CLIP (WinCLIP) with (1) a compositional ensemble on state words and prompt templates and (2) efficient extraction and aggregation of window/patch/image-level features aligned with text. We also propose its few-normal-shot extension WinCLIP+, using the complementary information from normal images to help normality modeling. In MVTec-AD and VisA benchmarks, without further tuning, WinCLIP achieves $91.8\%, 78.1\%/85.1\%, 79.6\%$ AUROC in zero-shot anomaly classification/segmentation while WinCLIP+ achieves $93.1\%, 83.1\%/95.2\%, 95.2\%$ AUROC 1-shot setups, surpassing state-of-the-art with large margins.
% 3rd version
% Visual anomaly classification and segmentation are commonly used in industrial quality inspection. Previous works mainly focus on training a dedicated model using numerous or a few normal images for each object. However, with limited generality in these works, it is not scalable to deploy a different model for each category considering the wide use cases. Recently, large-scale vision-language pre-training such as CLIP has shown revolutionary generality: without any model tuning, language guides CLIP to present competitive zero-/few-shot performances compared to fully-supervised counterparts on various downstream tasks including object classification and segmentation. We make a first attempt at language guided zero-shot anomaly classification and segmentation using CLIP. We propose window-based CLIP (WinCLIP) with (1) a compositional ensemble on state words and prompt templates and (2) efficient extraction and aggregation of window/patch/image-level features aligned with text. We also propose its few-normal-shot extension WinCLIP+, using the complementary information from normal images to help normality modeling. In MVTec-AD and VisA benchmarks, without further tuning, WinCLIP achieves $91.8\%, 77.6\%/85.1\%, 74.9\%$ AUROC in zero-shot anomaly classification/segmentation while WinCLIP+ achieves $93.1\%, 83.1\%/95.2\%, 95.2\%$ AUROC 1-shot setups, surpassing state-of-the-art with large margins.

% 1st version
% Visual anomaly classification and segmentation are commonly used in industrial quality inspection. The majority of works focus on training a dedicated model for each object given a large amount of normal images as reference. Such learning paradigm limits model generality and usability since additional data is required to specify normality of any other objects. Recent advance in vision-language modeling, such as Contrastive Language-Image Pre-training (CLIP), presents that visual models learned from natural language supervision have promising ability to be transferred to new categories without or with only a handful example. With pre-trained CLIP, we demonstrate that natural language can be used to reference learned visual normal and defective states (or describe new ones) and enable zero-shot anomaly classification on new objects. Then by leveraging window and patch levels visual features extracted from CLIP, we propose \jh{Window-based CLIP (WinCLIP)} which localizes defects in a zero-shot manner guided by language. Given a few normal images as reference, we propose WinCLIP+ for few-shot anomaly classification and segmentation by utilizing the complementary information from both language guided and visual based anomaly classification. In MVTec-AD and VisA benchmarks, WinCLIP achieves $91.8\%, 77.6\%/85.1\%, 74.9\%$ \jh{AUROC} in zero-shot anomaly classification/segmentation while WinCLIP+ achieves $93.1\%, 83.1\%/95.2\%, 95.2\%$ \jh{AUROC} 1-shot classification/segmentation, surpassing state-of-the-art with large margins.

% 2nd version
% \begin{abstract}
% Visual anomaly classification and segmentation are commonly used in industrial quality inspection. The focus of prior research in the field has been on training custom models for each quality inspection task, which requires task-specific images and annotation. In this paper, we address zero-shot and few-normal-shot anomaly classification and segmentation. Large-scale vision-language pre-trained models such as CLIP have shown promising performances in zero-shot object classification tasks. However, they do not perform satisfactorily on anomaly classification and are not designed for localization. In this paper, we propose WinCLIP, which performs zero-shot anomaly classification and segmentation by a compositional ensemble of language prompts and window/patch features from CLIP. We also propose its few-normal-shot extension WinCLIP+, which leverages the availability of few normal images. In MVTec-AD and VisA benchmarks, WinCLIP achieves $91.8\%, 77.6\%/85.1\%, 74.9\%$ AUROC in zero-shot anomaly classification/segmentation while WinCLIP+ achieves $93.1\%, 83.1\%/95.2\%, 95.2\%$ AUROC 1-shot classification/segmentation, surpassing state-of-the-art with large margins.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\linewidth]{figures/vis_intro.png}
%   \caption{Language guided zero-/one-shot anomaly segmentation from WinCLIP/WinCLIP+. Best viewed in color and zoom in.}
\caption[Caption for LOF]{Language guided zero-/one-shot\protect\footnotemark anomaly segmentation from WinCLIP/WinCLIP+. Best viewed in color and zoom in.}
  \label{fig:vis_intro}
  \vspace{-0.2in}
\end{figure}

\footnotetext{\textit{few-shot} and \textit{few-normal-shot} are used interchangeably in our case.}
\begin{figure*}[!t]
  \centering
  \includegraphics[width=\linewidth]{figures/teaser.png}
  \caption{Motivation of language guided visual inspection. (a) Language helps describe and clarify normality and anomaly; (b) Aggregating multi-scale features helps identify local defects; (c) Normal images provide rich referencing content to visually define normality}
  \label{fig:principals}
  \vspace{-0.15in}
\end{figure*}
% We need a single model while lots of efforts are conducted in 1-class method
%Visual surface anomaly classification (AC) and segmentation (AS) classify and localize defects in industrial manufacturing, predicting an image or a pixel as normal or anomalous. The challenges of the tasks arise from the lack of defective samples, due to their (a) open-ended nature, and (b) rare occurrence in practice, although defect-free images are easier to acquire.
%Thus a lot of effort has been devoted to one-class or unsupervised anomaly detection \cite{zou2022spot,li2021cutpaste,roth2022towards,defard2021padim,cohen2020sub,Ristea-CVPR-2022,zavrtanik2021draem,bergmann2022beyond}. Generally speaking, these methods model the normality distribution with normal images only, regarding the samples out of the modeled distribution to be anomalous. 

Visual anomaly classification (AC) and segmentation (AS) classify and localize defects in industrial manufacturing, respectively, predicting an image or a pixel as normal or anomalous. Visual inspection is a long-tail problem. The objects and their defects vary widely in color, texture, and size across a wide range of industrial domains, including aerospace, automobile, pharmaceutical, and electronics. These result in two main challenges in the field. 

First, defects are rare with wide range of variations, leading to a lack of representative anomaly samples in the training data. Consequently, existing works have mainly focused on one-class or unsupervised anomaly detection \cite{zou2022spot,li2021cutpaste,roth2022towards,defard2021padim,cohen2020sub,Ristea-CVPR-2022,zavrtanik2021draem,bergmann2022beyond}, which only requires normal images.  These methods typically fit a model to the normal images and treat any deviations from it as anomalous. When hundreds or thousands of normal images are available, many methods achieve high-accuracy on public benchmarks \cite{bergmann2019mvtec,defard2021padim,roth2022towards}. But in the few-normal-shot regime, there is still room to improve performance \cite{huang2022registration,zou2022spot,rudolph2021same,sheynin2021hierarchical}, particularly in comparison with the fully-supervised upper bound.

Second, prior work has focused on training a bespoke model for each visual inspection task, which is not scalable across the long-tail of tasks. This motivates our interest in zero-shot anomaly classification and segmentation. But many defects are defined with respect to a normal image. For example, a missing component on a circuit board is most easily defined with respect to a normal circuit board with all components present. For such cases, at least a few normal images are needed. So in addition to the zero-shot case, we also consider the case of few-normal-shot anomaly classification and segmentation. Since only few normal images are available, there is no segmentation supervision for localizing anomalies, making this a challenging problem across the long-tail of tasks.

%Visual inspection is a long-tail problem. The objects and their defects vary widely in color, texture, and size across a wide range of industrial domains, including aerospace, automobile, pharmaceutical, and electronics. Prior work has focused on training a bespoke model for each task. But collecting hundreds to thousands of normal images and training models for each visual inspection task is not scalable. This motivates our interest in zero-shot anomaly classification and segmentation. But many defects are defined with respect to a normal image. For example, a missing component on a circuit board is most easily defined with respect to a normal circuit board with all components present. For such cases, at least a few normal images are needed, and in addition to the zero-shot case, we also consider the case of few-normal-shot anomaly classification and segmentation.

%a large train set and deploy different models for each object, not to mention many cases of in-field inspection (\eg roof and car damage inspection) requiring cold-start models with no in-distribution training samples \cite{hezaveh2017roof,patil2017deep}. Moreover, these works have limited generality and re-usability where additional data is needed to re-train for new instances, even if the new one is from the same super-class, \eg from a green to blue circuit board. 
% (a green circuit board \textit{vs}.~a blue circuit board). % without any in-distribution samples for training \cite{cao2020review}. 

% can be rapidly adapted
%To meet the demands of the wide use cases, several works have explored training anomaly detection models with only a handful of normal images (\eg 2-10) \cite{rudolph2021same, sheynin2021hierarchical,huang2022registration}. Recent works \cite{rudolph2021same,sheynin2021hierarchical} leverage data augmentation to expand the set of normal images for better normal distribution estimation. However, due to the limited access of in-task samples, it is difficult to tune hyper-parameters and training policies for the realistic few-shot cases \cite{perez2021true}. Moreover, these models cannot generalize on new objects and still need to be retrained. RegAD \cite{huang2022registration} conducts a further study on the model reuse for unseen objects. It pre-trains an object-agnostic registration network by numerous normal images of different categories, which is used to model normality for unseen instances give a few in-task normal samples. However, the generality of these models is still limited if the domain gap between training and test data is large. Also, as far as we know, no prior works have made attempts at zero-shot anomaly recognition.

% And \cite{zou2022spot} studied the large-scale pre-training on ImageNet \cite{russakovsky2015imagenet} with contrastive loss to promote feature sensitivity to local variation, facilitating few-shot adaptation to downstream tasks.

% Building a model that can rapidly adapt to new categories with zero or only a handful of normal referencing images (\eg 1-5) is an ideal solution for the wide use cases. It is an open problem with several challenges. First, normal and anomalous categorization depends on a wide range of attributes (\eg color, texture, pose) and states \cite{isola2015discovering} (\eg smooth, fresh, scratched, cracked). Even though some defects (\eg damage, break) are shared across objects, there are also domain specific terms, \eg ``tombstoning'' referring to electronic components lifted up from the circuit board. Second, although anomaly prediction is a binary-class problem, definitions of normality and abnormality vary for each object. For example, light scratches on rail road metals or cracks in background are tolerable deviation from normality while the defects of interests are holes on surface. However, scratches on a wedding ring are non-acceptable and the flawless surface is the expected normality. Third, the contextual diversity of states is huge due to the innumerable objects. 

Vision-language models \cite{radford2021learning,alayrac2022flamingo,jia2021scaling,schuhmann2022laionb} have shown promise in zero-shot classification tasks. Large-scale training with vision-language annotated pairs learns expressive representations that capture broad concepts. Without additional fine-tuning, text prompts can then be used to extract knowledge from such models for zero-/few-shot transfer to downstream tasks including image classification \cite{radford2021learning}, object detection \cite{gu2021open} and segmentation \cite{xu2022simple}. Since CLIP is one of the few open-source vision-language models, these works build on top of CLIP, benefiting from its generalization ability, and showing competitive low-shot performances in both seen and unseen objects compared to full supervision.

%Meanwhile, recent large-scale vision-language pre-training \cite{alayrac2022flamingo,jia2021scaling,schuhmann2022laionb}, such as CLIP \cite{radford2021learning}, are shown to learn powerful representations with good vision-text alignment, capturing broad concepts. Without any model tuning, text prompts are used as interface to extract knowledge from the pre-trained models and enable zero-/few-shot transfer to downstream tasks including image classification \cite{radford2021learning}, object detection \cite{gu2021open} and segmentation \cite{xu2022simple}. Benefiting from the unprecedented generality of CLIP, these methods present competitive low-shot performances in both seen and unseen objects compared to the fully-supervised counterparts. 

In this paper, we focus on zero-shot and few-normal-shot (1 to 4) regime, which has received limited attention \cite{rudolph2021same, sheynin2021hierarchical,huang2022registration}.
Our hypothesis is that language is perhaps even more important for zero-shot/few-normal-shot anomaly classification and segmentation. This hypothesis stems from multiple observations.
 First, ``normal'' and ``anomalous'' are states \cite{isola2015discovering} of an object that are context-dependent, and language helps clarify these states. For example, ``a hole in a cloth" may be a desirable or undesirable  depending upon whether distressed fashion or regular fashion clothes are being manufactured. Language can bring such context and specificity to the broad ``normal'' and ``anomalous'' states. Second, language can provide additional information to distinguish defects from acceptable deviations from normality. For example, in Figure~\ref{fig:principals}\textcolor{red}{(a)}, language provides information on the soldering defect, while minor scratches/stains on background are acceptable. In spite of these advantages, we are not aware of prior work leveraging vision-language models for anomaly classification and segmentation. In this work, with the pre-trained CLIP as a base model, we show and verify our hypothesis that language aids zero-/few-shot anomaly classification/segmentation.
 
 %language defines the abstract terms of ``normal'' and ``anomalous'' in terms of states \cite{isola2015discovering}, which vary for each object: \eg ``rotten'' and ``fresh'' are anomalous and normal patterns for bananas while ``scratch'' and ``flawless'' are relevant to wedding rings. Second, illustrated in Figure~\ref{fig:principals}\textcolor{red}{(a)}, language clarifies defects among the irrelevant deviation from normality, such as ``crack in background'', ``acceptable tiny scratch'', ``lighting change'', \textit{etc.} Third, the anomaly distribution is open-ended due to the wide range of attributes (\eg texture, pose) \cite{nagarajan2018attributes} and states (\eg smooth, dented). Language is a low-effort way to define the anomaly in concrete words compared to images. 

% light scratches on rail road metals are tolerable deviations while the defects of interests are holes on surface \cite{zhang2021rail}. However, scratches on a wedding ring are non-acceptable and the flawless surface is the expected normality. Language easily describes the normality and defects.
% For fast adaptation with zero-/few-normal-shot, there are several challenges: (1) definitions of normality and anomaly vary case-by-case; (2) huge diversity in states and objects. Due to these challenges, images based methods are inefficient to define anomaly, considering the open-ended nature of the problem.

% Previous efforts in building a single model
% Previous works explored two options to solve the problem. The first one is to learn a representation capturing all the states and objects. Although learning and image-processing methods were proposed for some specific defects such as scratch and cracks \cite{cao2020review,kong2017accurate}, these models are not able to capture the diversity in states and objects due to the lack of the large-scale dataset with sufficient variety \cite{maguire2018sdnet2018,zou2022spot,bergmann2019mvtec,mishra21-vt-adl}. Actually, considering the large variety plus defective samples' rarity, it is infeasible to collect images covering all the cases. The second option is to design efficient algorithms to extract knowledge from some large-scale pre-trained models for anomaly recognition. \cite{sheynin2021hierarchical,rudolph2021same,huang2022registration} leverages ImageNet pre-trained models \cite{he2016deep,hinton2012imagenet} to extract visual features of a few normal samples to model normality. However, it is inefficient to use images only to define the normality and defects among the deviation of normality.

% Specifically, the deviation from normality has a wide variety and some of them are accepted and not defects, \eg ``crack in background'', ``tiny and acceptable scratch'', ``lighting change'', \textit{etc.} It is hard to just use images to clarify the deviation and normality.

% Advance in VLM makes a single model promising
% To solve the problem, learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. 


% And we believe the pre-training web-scale dataset \cite{schuhmann2022laionb} covers the states relevant to anomaly classification in both images and captions, which helps the tasks. 

% In this work, we leverage CLIP and show that language enables zero-shot anomaly classification while improving few-shot setups given the normal images as reference. 
 Since CLIP is one of the few open-source vision-language models, we build on top of it. Previously, CLIP-based methods have been applied for zero-shot classification \cite{radford2021learning}. CLIP can be applied in the same way to anomaly classification, using text prompts for ``normal'' and ``anomalous'' as classes. However, we find na\"ive prompts are not effective (see Table \ref{tab:ad_0shot}). So we improve the na\"ive baseline with a state-level word ensemble to better describe normal and anomalous states. Another challenge is that CLIP is trained to enforce cross-modal alignment only on the global embeddings of image and text. However, for anomaly segmentation we seek pixel-level classification and it is non-trivial to extract dense visual features aligned with language for zero-shot anomaly segmentation. Therefore, we propose a new \emph{Window-based CLIP} (WinCLIP), which extracts and aggregates the multi-scale features while ensuring vision-language alignment. The multiple scales used are illustrated in Figure~\ref{fig:principals}\textcolor{red}{(b)}. To leverage normal images available in the few-normal-shot setting, we introduce WinCLIP+, which aggregates complementary information from the language driven WinCLIP and visual cues from the normal reference images, such as the one shown in Figure~\ref{fig:principals}\textcolor{red}{(c)}. We emphasize that our zero-shot models do not require any tuning for individual cases, and the few-normal-only setup does not use any segmentation annotation, facilitating applicability across a broad range of visual inspection tasks. As a sample, Figure~\ref{fig:vis_intro} illustrates WinCLIP and WinCLIP+ qualitative results for a few cases. 

To summarize, our main contributions are:
\begin{itemize}
    \item We introduce a compositional prompt ensemble, which improves zero-shot anomaly classification over the na\"ive CLIP based zero-shot classification. 
    \item Using the pre-trained CLIP model, we propose WinCLIP, that  efficiently extract and aggregate multi-scale spatial features aligned with language for zero-shot anomaly segmentation. As far as we know, we are the first to explore language-guided zero-shot anomaly classification and segmentation. 
    \item We propose a simple reference association method, which is applied to multi-scale feature maps for image based few-shot anomaly segmentation. WinCLIP+ combines the language-guided and vision-only methods for few-normal-shot anomaly recognition.
    \item We show via extensive experiments on MVTec-AD and VisA benchmarks that our proposed methods WinCLIP/WinCLIP+ outperform the state-of-the-art methods in zero-/few-shot anomaly classification and segmentation with large margins.
\end{itemize}

% The challenging of building a single model 
% Building a model that can be rapidly adapted to new categories with zero or a handful ($1-5$) of normal reference images is an ideal solution for the wide applications of quality inspection. It is an open problem with several challenges. First, normality and abnormality refer to a wide range of states, such as smooth, fresh, scratch, crack, \textit{etc.} Even though some defects (damage/break) are shared across objects, there are also domain specific terms, \eg ``tombstoning'' which refers to an electronic component lifted up from the circuit board. Second, although anomaly prediction is a binary classification task, definitions of normality and anomaly may vary for each object. For example, light scratches on rail road metals are tolerable while the defects of interests are holes on surface. However, scratches on a wedding ring are non-acceptable and the flawless surface is the expected normality. Third, the contextual diversity of states is huge due to the innumerable objects. 

% % Previous efforts in building a single model
% There are two options to solve the problem. The first one is to learn a representation capturing all the states and objects. A naive method is to use a fixed set of predetermined state categories across diverse objects and then train a standard supervised model with image-level labels. Although learning and image-processing methods were proposed for some specific defects such as scratch and cracks \cite{cao2020review,kong2017accurate}, these models are not able to capture the diversity in states and objects due to the lack of the large-scale dataset with sufficient variety \cite{maguire2018sdnet2018,zou2022spot,bergmann2019mvtec,mishra21-vt-adl}. Actually, considering the large variety in objects and states plus the rarity of defective data, it is infeasible to collect images covering all the cases. In addition, the limited form of class-type supervision in learning based methods restricts model generality as new data is needed for any other visual states. Second option is to design efficient algorithms to extract knowledge from some pre-trained models for anomaly recognition. \cite{sheynin2021hierarchical,rudolph2021same,huang2022registration} regard ImageNet pre-trained visual models \cite{he2016deep,hinton2012imagenet} as the knowledge base and proposed visual based methods with a few normal samples as context. Although normal images provides rich context to define normality, these visual based methods have its own limitation. Specifically, the deviation from normality has a wide variety and some of them are accepted and not defects, \eg ``crack in background'', ``tiny and acceptable scratch'', ``lighting change'', \textit{etc.} It is hard to just use normal images to rule out all these deviations. 


% Towards an unified model which can fast adapted to downstream applications, we need to use language as definitions. Given fixed categories it's hard. Language helps to define normality and anomaly. Also given pure language might be not enough, for example, human might tolerate some kinds of defects. It's hard to use language to describe all the contexts. But people can provide some reference images as visual references, which helps define normality. The visual normality and textual normality and anomality can be complementary to each other, then help a more comprehensive anomaly classification.

% Towards anomaly segmentation, we need to get the local features. Also multi-scale is important as defects can be small. CLIP is not designed for anomaly classification. With minimal modification, WinCLIP and WinCLIP+ can be used for anomaly segmentation. 

% \textbf{Contributions}: WinCLIP; WinCLIP+. 

% Visual surface anomaly classification and segmentation identify and localize defects in industrial manufacturing. While anomaly classification and segmentation are instances of image classification and semantic segmentation problems, respectively, they have unique challenges. First, defects are rare, and it is hard to obtain a large number of anomalous images. Second, common types of anomalies, such as surface scratches and damages, are often small. Fig. 1 (a) gives an example. Third, manufacturing is a performance sensitive domain and usually
% requires highly accurate models. Fourth, inspection in manufacturing spans a wide range of domains and tasks, from detecting leakages in capsules to finding damaged millimeter-sized components on a complex circuit board.

% Visual inspection has wide applications in many industry, including car damage assessment for insurance, manufacturing quality monitor, package delivery, etc. Current method trains a separate model for each application, which makes it hard to scale up to the innumerable products. This paper conducts the first attempt towards an unified model which can adapt to various objects with zero or a few anomalous samples without further paramater tuning.

% There are several previous works which works on training defect detection across various objects for a certain type of defect, such as crack detector, scratch detector . However, considering innumerable types of defects, such as bad soldering, missing transistor, etc. "This restricted form of supervision limits their generality and usability since
% additional labeled data is needed to specify any
% other visual concept. CLIP demonstrates that Learning directly from raw text about images is a promising alternative which
% leverages a much broader source of supervision."

% he majority of industrial \textbf{visual anomaly detection} methods \cite{Roth2022,Ristea-CVPR-2022} focus on training a specific model for each category given a large amount of normal images as reference. However, in real-world scenarios, there are millions of industrial products and it is not cost-effective to collect a large training set for each object and deploy different models for different categories, not to mention many cases requiring cold-start models.
% \\
% \\
% Building a \textbf{single model that can be rapidly adapted} to numerous categories without or with only a handful of normal reference images is an ideal solution and an open challenge to the community. Unfortunately, \textbf{no previous benchmarks} were designed for such a setting, and therefore, it is difficult to evaluate current methods for this problem. 

% \begin{figure}[!h]
%   \includegraphics[width=0.48\textwidth]{figures/teaser.png}
%   \caption{This is a tiger.}
% \end{figure}

%-------------------------------------------------------------------------

\section{Related work}
\label{sec:related_work}

% \vspace{0.05in}
\noindent\textbf{Vision-language modeling. } 
Among recent successes of large pre-trained vision-language models (VLM) \cite{radford2021learning,jia2021scaling,alayrac2022flamingo}, CLIP \cite{radford2021learning} is the first to perform pre-training on web-scale image-text data, showing unprecedented generality: \eg its language-driven zero-shot inference, improved both effective robustness \cite{taori2020measuring} and perceptual alignment \cite{goh2021multimodal}.
Many following VLM works explored large-scale pre-training in different aspects, \eg scaling up data \cite{jia2021scaling}, efficient designs \cite{li2021align, alayrac2022flamingo, yang2022unified}, multi-tasks \cite{wang2022ofa,lu2022unified}, \textit{etc.} To democratize large-scale VLM for the usages in different domains, a billion-scale data LAION-5B \cite{schuhmann2022laionb}, a code base of OpenCLIP with pre-trained models \cite{ilharco_gabriel_2021_5143773} are open-sourced. Other works presented CLIP's promise in zero-/few-shot transfer to downstream tasks beyond classification \cite{tewel2022zerocap,gu2021open,xu2022simple,rombach2021highresolution}. % with promising performances.
Good prompt engineering and tuning can non-trivially benefit generalization performances \cite{radford2021learning,zhou2022learning}. Moreover, some other works \cite{zhou2022extract,rao2022denseclip,zhong2022regionclip} leverage the pre-trained CLIP for language guided detection and segmentation with promising performances. 
% In addition, Vision Transformer \cite{dosovitskiy2021an} based image encoders generally outperforms convolutional networks \cite{he2016deep,krizhevsky2017imagenet} for various vision tasks \cite{radford2021learning} while transformers are mainly used for text encoder \cite{vaswani2017attention,devlin2018bert,2020t5}.

\vspace{0.05in}
\noindent\textbf{Anomaly classification and segmentation. } 
Due to the scarcity of anomalies, the major focus has been on one-class methods with many normal images \cite{li2021cutpaste,defard2021padim,cohen2020sub,yi2020patch,zavrtanik2021draem,yu2021fastflow}. While the MVTec-AD benchmark \cite{bergmann2019mvtec} is saturated by several works \cite{roth2022towards,yu2021fastflow,yang2022memseg}, their specific application is hindered due to their unscalable full-normal-shot setup. Recent works \cite{rudolph2021same,sheynin2021hierarchical} explored few-shot setups by leveraging augmentation to expand the small support set for better normality modeling. RegAD \cite{huang2022registration} further proposed a model-reusing by pre-training an object-agnostic registration network with diverse images to model normality for unseen object, given a few normal samples. Meanwhile, to close the gap between academical and industrial data, Visual Anomaly (VisA) \cite{zou2022spot} is introduced for a challenging benchmark over MVTec-AD. 
Additionally, Vision Transformer (ViT) have recently shown its potential in visual inspection \cite{mishra21-vt-adl,fort2021exploring}. 
% Additionally, while most works use CNN backbones\cite{he2016deep}, Vision Transformer (ViT) shows its potential in visual inspection \cite{mishra21-vt-adl,fort2021exploring}. 
% SpotDiff \cite{zou2022spot} investigated contrastive self-supervised representation learning on external large-scale dataset \cite{russakovsky2015imagenet} for fast few-shot adaptation.

\vspace{0.05in}
\noindent\textbf{State classification. } 
In some sense, anomaly classification is related to state classification \cite{isola2015discovering} that predicts if an object is normal or anomalous. While the major works in computer vision focus on object, scene, or material recognition \cite{russakovsky2015imagenet,hinton2012imagenet,xiao2010sun,sharan2013recognizing}, state classification aims to differentiate the fine-grained sub-object physical properties or attributes. Several datasets covering generic states/attributes (e.g. tall, crack, red, smooth) over diverse objects and scenes are introduced \cite{isola2015discovering,yu2014fine,hudson2019gqa,mancini2021open}. Some works \cite{welling2016semi,mancini2022learning,naeem2021learning} built graphs consisting of attributes and objects, of which relationship is learnt by graph neural networks \cite{zhang2019graph}. 
% Recently, a prompt tuning for CLIP with some downstream data is shown to be effective in zero-shot state classification on unseen objects \cite{nayak2022learning}.

% \section{Preliminary}
\section{Background}
\label{sec:preliminary}

% detection -> classification
% detector -> classifier
% localization -> segmentation
% AD -> AC
% AL -> AS
% ADL -> ACS


\noindent\textbf{Anomaly classification and segmentation. }
% definition of AC and AS
% For a given input image $\rvx \in \mathcal{X}$, \emph{anomaly classification and segmentation} (ACS) aim to model ``anomality'' of $\rvx$ in two different granularities,
Given an image $\rvx \in \mathcal{X}$, both anomaly classification and segmentation (ACS) aim to predict ``abnormality'' in $\rvx$.
Specifically, we consider anomaly classification (AC) as a binary classification $\mathcal{X} \rightarrow \{-, +\}$ where ``$+$'' indicates the presence of anomaly in image-level. And anomaly segmentation (AS) is its pixel-level extension to output the location of anomalies via $\mathcal{X} \rightarrow \{-, +\}^{h\times w}$ for a certain image with size $h \times w$. In practice, the tasks are often cast into problems of predicting anomaly scores. For example, anomaly classification typically models a mapping $\mathrm{ascore}: \mathcal{X}\rightarrow [0, 1]$
%$\mathrm{f_{AC}}: \mathcal{X}\rightarrow [0, 1]$
so that a binary classification can be performed by thresholding $\mathrm{ascore}(\rvx)$. 

% For a given image $\rvx \in \mathcal{X}$, both \emph{anomaly classification and segmentation} (ACS) aim to predict ``abnormality'' in $\rvx$.
% Specifically, we consider \emph{anomaly classification} (AC) as a binary classification $\mathcal{X} \rightarrow \{-, +\}$ where ``$+$'' indicates the presence of anomaly in image-level, and \emph{anomaly segmentation} (AS) as its pixel-level extension to output where the anomalies are located, \eg in a form of $\mathcal{X} \rightarrow \{-, +\}^{h\times w}$ for a certain image resolution of $h \times w$. 
% In practice, the tasks are often cast into problems of predicting \emph{anomaly scores}: \eg for anomaly classifiation, one is expected to define a mapping $\mathrm{AC}: \mathcal{X}\rightarrow [0, 1]$ so that a binary classification can be performed by thresholding $\mathrm{AC}(\rvx)$. 


% formulation
% The challenges of anomaly recognition tasks arise mainly from the lack of anomaly samples, due to their (a) open-ended nature, and (b) rare occurrence in practice. In this respect, 
Due to the lack of anomalous (or positive) samples in practice, the one-class scenario, where the training data $\mathcal{D}:=\{(x_i, -)\}_{i=1}^{K}$ consists of only normal (or negative) samples, has been widely used. 
In this paper, we follow the one-class protocol, particularly focusing on extreme cases of few-shot ($K=1$ to $4$) and the unexplored zero-shot setups for both AC and AS. And we assume an available list of task-specific texts tags, \eg for objects and relevant defects. 
% but further explore more extreme scenarios of \emph{few-shot} and \emph{zero-shot} setups: \ie we assume $\mathcal{D}$ to have low samples such as $K=1$, or even no sample at all, which have been relatively unexplored in the literature.

% \subsection{Contrastive Language-Image Pre-training}

\vspace{0.05in}
\noindent\textbf{Zero-shot classification with CLIP. }
% CLIP and naive application to AC
%Among the recent successes of large pre-trained vision-language models \cite{radford2021learning,jia2021scaling,alayrac2022flamingo}, \emph{CLIP} \cite{radford2021learning} is considered as the first attempt of this kind to perform a pre-training on web-scale image-text data, showing unprecedented capability to transfer: \eg its language-driven zero-shot inference, improved effective robustness \cite{taori2020measuring}, as well as showing a better perceptual alignment \cite{goh2021multimodal}. %Specifically,
% For a given stream
\emph{Contrastive Language Image Pre-training} (CLIP) \cite{radford2021learning} is a large-scale pre-training method offering a joint vision-language representation.
Given million-scale image-text pairs $\{(x_t, s_t)\}_{t=1}^T$ from the web, CLIP trains an image encoder $f$ and a text encoder $g$ via {contrastive learning} \cite{zhang2020contrastive,chen2020simple} to maximize 
% the (cosine) similarity $\langle f(\rvx), g(\rvs) \rangle$ between the joint output representation. 
the correlation between $f(x_t)$ and $g(s_t)$ across $t$ in terms of cosine similarity $\langle f(\rvx), g(\rvs) \rangle$. 
Given an input $\rvx$ and a closed set of free-form texts $S=\{ s_1, \cdots, s_k \}$, CLIP can perform zero-shot classification via a $k$-way categorical distribution:
% based on the model of $\log p(\rvx, \rvs)\propto\langle f(\rvx), g(\rvs) \rangle$ it offers.
\begin{equation}\label{eq:clip_0shot}
    p(\rvs = s_i|\rvx; \rvs \in S) := \frac{\exp(\langle f(\rvx), g(s_i) \rangle / \tau) }{\sum_{s \in S} \exp(\langle f(\rvx), g(s) \rangle / \tau) },
\end{equation}
where $\tau > 0$ is the temperature hyperparameter. 

For a set of class words $C = \{c_1, \cdots, c_k\}$, it has shown that accompanying each label word $c\in C$ with a \emph{prompt template}, \eg ``\texttt{a photo of a [c]}'', improves accuracy over the case without templates. Moreover, an ensemble of prompt embeddings that aggregates multiple (80) templates \eg ``\texttt{a cropped photo of a [c]}'', can further boost the performance \cite{radford2021learning}. 
Overall, we are essentially ``retrieving'' the visual knowledge of CLIP through the language interface in appropriate manners. In this paper, we further explore how to extract the knowledge of CLIP in a way more suitable for anomaly recognition. 


% \section{Method}
\section{WinCLIP and WinCLIP+}
\label{sec:method}

% At a high level, this work explores two questions in the context of visual anomaly classification and segmentation: 
% \begin{enumerate}
%     \item Could CLIP recognize ``abnormality'' without seeing any normal reference, \eg similarly to humans?
%     \item Is the current protocol of retrieving CLIP knowledge suitable for anomaly recognition tasks?
% \end{enumerate}
% To address (1), we first explore and establish a practice to define a CLIP-based zero-shot classifier for anomaly classification (Section~\ref{sec:zeroshot}). For (2), on the other hand, we propose \emph{Window-based CLIP} (WinCLIP) as a simple-yet-effective method to better extract CLIP knowledge on local details (Section~\ref{sec:winclip}). Lastly, we introduce an extension of WinCLIP, namely WinCLIP+, to also benefit from few normal samples for performance, while maintaining the complementary benefits of (zero-shot) \mbox{WinCLIP} features (Section~\ref{sec:winclip+}).  

In this section, we first establish a novel binary zero-shot anomaly classification framework with a Compositional Prompt Ensemble to improve CLIP for anomaly classification (Section~\ref{sec:zeroshot}). Next, we propose a simple-yet-effective \emph{Window-based CLIP} (WinCLIP) for efficient zero-shot anomaly segmentation (Section~\ref{sec:winclip}). Lastly, we propose an extension \emph{WinCLIP+} to benefit from few normal reference images, while maintaining the complementary benefits of language-guided predictions (Section~\ref{sec:winclip+}).  

% Based on the pre-trained CLIP, we propose a novel formulation for binary zero-shot anomaly classification by leveraging texts about anomaly as class prototype 

% In this section, we elaborate our methods. 
% % Firstly, we explore and establish a practice to define a CLIP-based
% Firstly, we explore and establish a baseline improvement over CLIP zero-shot classifier for anomaly classification (Section~\ref{sec:zeroshot}) based on CLIP CLIP zero-shot classifier. Next, we propose \emph{Window-based CLIP} (WinCLIP) as a simple-yet-effective method to better extract CLIP knowledge on local details (Section~\ref{sec:winclip}). Lastly, we introduce an extension of WinCLIP, namely \emph{WinCLIP+}, to also benefit from few normal samples, while maintaining the complementary benefits of language-driven predictions (Section~\ref{sec:winclip+}).  


\subsection{Language-driven zero-shot AC}
\label{sec:zeroshot}

% Recall that one of key technical challenges in anomaly classification stems from the \emph{open-ended} nature of abnormality.
% : \ie the distribution of anomalous samples is usually infeasible to model. 
% This has forced previous approaches to rely on the one-class modeling, which necessitates an access to normal samples. 
% In this section, we make an encouraging observation that the language interface of a pre-trained CLIP model can bring the open concept of abnormality into concrete words, offering a strong anomaly model as is: this enables us to construct a \emph{zero-shot} model for anomaly classification.

% We report a component-wise analysis of the proposed method in Table~\ref{tab:ad_0shot} of our ablation study (in Section~\ref{sec:ablation}).

% it is generally harder to model a concrete distribution of anomalies,   
% In other words, one is generally hard to consider an anomaly as a concrete distribution.  
% have necessitated
% \vspace{-0.05in}
\noindent\textbf{Two-class design. }
We introduce a binary zero-shot anomaly classification framework \emph{CLIP-AC} by adapting CLIP with two class prompts \verb|[c]| - ``\verb|normal [o]|'' \textit{vs.}~``\verb|anomalous [o]|''. \verb|[o]| is an object-level label, \eg ``\verb|bottle|'' when available, or simply ``\verb|object|''. In addition, we also test a one-class design by only using the normal prompt $s_{-}:=\text{``\texttt{normal [o]}''}$ to define anomaly score as ``$-\langle f(\rvx), g(s_{-}) \rangle$''. We observe the simple two-class design from CLIP already yields a non-trial performance and outperforms one-class design significantly in experiments (Table~\ref{tab:ad_0shot}). This demonstrates (a) CLIP pre-trained by large web dataset provides a powerful representation with good alignment between text and images for anomaly tasks (b) specific definition about anomaly is necessary for good performance. 
% which suggests that the knowledge in abnormality of CLIP is the key source of the performance. 
% \paragraph{State-level ensemble.}
% Next, we observe that CLIP-AC can significantly benefit from diversifying the ways to describe the abnormality. Unlike standard (object-level) classifiers, CLIP-AC performs classification between two \emph{states} of a given object, \ie either ``normal'' or ``anomalous'', which are by nature subjective and thus have more diversity to explain themselves.
% % : \eg the prompt with ``\verb|anomalous [c]|'' we use for CLIP-AC can also be with ``\verb|damaged [c]|'' or ``\verb|[c] with a damage|''. 
% Motivated by this, we consider two ``bags'' of states (see the supplementary material for the details), each for ``normal'' or ``anomalous'', and compute the average of the (textual) embeddings to represent each of the states.
% As an application, one could also include some \emph{user-specific} contexts into the bags, \eg in cases the user has prior knowledge on the defect types such as ``scratches''. 

% , and the more details, \eg specification of the prompts adopted, are given in the supplementary material.
% averaging the embeddings. 
% , and (b) we could significantly boost the performance by composing multiple levels of ensemble. 

% \paragraph{Prompt-level ensemble.}

% We also apply an ensemble over a variety of prompt templates in a similar manner to \citet{radford2021learning}, \ie besides ``\verb|a photo of [c]|'': here, a practical consideration is that the templates proposed by \citet{radford2021learning} often affect the states of target object, which could be undesirable in our application. We have constructed a different set of templates by filtering out such cases, (see the supplementary material for the details). 
% Combined with the state-level ensemble in a \emph{compositional} manner, we could dramatically increase the diversity of prompts and this could further boosts the performance without increasing the computational cost.
% \vspace{0.01in}
\noindent\textbf{Compositional prompt ensemble (CPE). }
%We next observe that CLIP-AC can significantly benefit from diversifying the ways to describe the abnormality. 
Unlike object-level classifiers, CLIP-AC performs classification between two \emph{states} of a given object, \ie either ``normal'' or ``anomalous'', which are subjective with various definitions depending on tasks. For example, ``missing transistor'' is ``anomalous'' for a circuit board while ``cracked'' is ``anomalous'' for wood. To better define the two abstract states of objects, we propose a Compositional Prompt Ensemble to generate all combinations of pre-defined lists of (a) \emph{state words} per label and (b) \emph{text templates}, rather than freely writing definitions. The state words include common states shared by most objects, \eg ``flawless'' for normality/``damaged'' for anomaly. Also we can optionally add task-specific state words given prior knowledge of defects, \eg ``bad soldering'' on PCB. Moreover, we curate a template list specifically for anomaly tasks \eg ``\texttt{a photo of a [c] for visual inspection}''. Check details on prompt engineering in supplementary. As in top-left of Figure ~\ref{fig:overall}, after getting all the combinations of states and templates, we compute the average of text embeddings per label to represent the normal and anomalous classes. Note that CPE is different from CLIP prompt ensemble that does not explain object labels (\eg ``cat'') and only augments templates selected by trial-and-error for object classification, including the ones unsuitable for anomaly tasks, \eg ``\verb|a cartoon [c]|''. Thus, the texts from CPE are more aligned with images in CLIP's joint embedding space for anomaly tasks. We denote the zero-shot scoring model with CPE as $\mathrm{ascore_0}: \mathbb{R}^d \rightarrow [0, 1]$ for an image embedding $f(\rvx)$.

% \vspace{0.01in}
\noindent\textbf{Remark. }
Our two-class design with CPE is a novel approach to define anomaly compared to standard one-class methods \cite{pmlr-v80-ruff18a,roth2022towards}. Anomaly detection is an ill-posed problem due to the open-ended nature. Previous methods model normality only by normal images regarding any deviation from normality as anomaly. Such solution is by nature hard to distinguish true anomalies from acceptable deviations from normality, \eg "scratch on circuit" vs. "tiny yet acceptable scratch". But language can define states in concrete words.

% To better define the two abstract states of objects, we propose a Compositional Prompt Ensemble to generate all combinations of pre-defined lists of (a) \emph{state words} and (b) \emph{text templates}, rather than freely writing definitions. The state words includes common states shared by most objects, \eg ``flawless'' for normality/``damaged'' for anomaly. Also we can optionally use task-specific state words given prior knowledge of defects, \eg ``bad soldering'' on PCB. Moreover, we curate a template list specifically for anomaly tasks. Note that compositional prompt ensemble is different from CLIP prompt ensemble that does not explain object labels (\eg ``cat'') and only augments templates selected by trial-and-error for object classification, including the ones unsuitable for anomaly tasks, \eg ``a cartoon \{\}''. Thus, the texts from our method are more aligned with images in CLIP's joint embedding space for anomaly tasks. For references, we denote our final zero-shot AC method as $\mathrm{ascore}_0: \mathbb{R}^d \rightarrow [0, 1]$ for a given image embedding, \eg of $f(\rvx)\in\mathbb{R}^d$.


% This is different from CLIP prompt ensemble that does not explain object labels (\eg ``cat'') and only augments templates selected by trial-and-error for object classification, including the ones unsuitable for anomaly tasks, \eg ``a cartoon \{\}''.
% The texts from the compositional prompt ensemble are more aligned with images in CLIP's joint embedding space for anomaly tasks than texts of CLIP prompt ensemble.
% The texts from CPE are more aligned with images in CLIP's joint embedding space for anomaly tasks than texts of CLIP prompt ensemble. This is achieved by (1) common state words shared by most objects, \eg ``flawless'' for normality/``damaged'' for anomaly; (2) (optionally) task-specific state words given prior knowledge of defects, \eg ``bad soldering'' on PCB (elaborated in \textbf{Generalization of CPE}); (3) curated template list for anomaly tasks. This is different from CLIP prompt ensemble that does not explain object labels (\eg ``cat'') and only augments templates selected by trial-and-error for object classification, including the ones unsuitable for anomaly tasks, \eg ``a cartoon \{\}''.
% to combine two levels of ensemble, namely (a) \emph{state} and (b) \emph{template} levels. 

% To provide fine-grained definitions of ``normal'' or ``anomalous'' states, we propose a Compositional Prompt Ensemble. 
% For example, the prompt with ``\verb|anomalous [c]|'' can also be with ``\verb|damaged [c]|'' or ``\verb|[c] with a damage|''. Motivated by this, we propose to combine two levels of ensemble, namely (a) \emph{state} and (b) \emph{template} levels. 
% Specifically, we consider a ``bag'' of various text templates (\eg ``\verb|a photo of a [state] [c]|'', ``\verb|a cropped photo of a [state] [c]|''), as well as two bags of \emph{pre-defined} \emph{state} words for ``normal'' (\eg ``\verb|flawless|'', ``\verb|perfect|'') or ``anomalous'' (\eg ``\verb|damaged|'', ``\verb|broken|'') respectively. Also, one could optionally include some \emph{user-specific} contexts into the state bags, \eg in cases the user has prior knowledge on the defect types such as ``metal melted'', ``bad soldering''. We generate the prompts by compositing the two levels of bags, and compute the average of the (textual) embeddings to represent each state.
% In this way, we can dramatically increase the diversity of text prompts, and this further boosts AC without increasing the computational cost.\footnote{The details on prompt design are given in the supplementary material.} 
% For references, we denote our final zero-shot AC method as $\mathrm{AC}_0: \mathbb{R}^d \rightarrow [0, 1]$ for a given image embedding, \eg of $f(\rvx)\in\mathbb{R}^d$.
% As mentioned earlier, language can describe the abstract and subjective terms ``normal'' or ``anomalous'' in concrete words. 
% So for more effective prompting, we follow the prompt ensemble in CLIP but do it in a compositional way in both \emph{state} and \emph{template} level. 
% First, we diversify the label words for both classes by two bags of \emph{pre-defined} generic state words for ``normal'' (\eg ``\texttt{flawless}'', ``\texttt{perfect}'') and ``anomalous'' (\eg ``\texttt{damaged}'', ``\texttt{broken}'') respectively. Also, one could optionally include some \emph{user-specific} states into the bags given the prior knowledge on the defect types such as ``\texttt{metal melted}'', ``\texttt{bad soldering}''. Second, among the prompt templates used in CLIP, we filter out some irrelevant ones \eg ``\texttt{a doodle of the [c]}'', and add more appropriate templates for visual inspection \eg ``\texttt{a photo of a [c] for visual inspection}''. As illustrated in the top-left of Figure ~\ref{fig:overall}, we then generate the prompts by compositing the two levels of bags, and compute the average of the text embeddings to represent the normal and anomalous classes. In this way, we provide richer and more diverse contexts in text prompts, further boosting AC without extra computational cost.\footnote{The details on prompt engineering are given in the supplementary.} 
% CPE easily generates textual definitions of the abstract ``normality'' and ``anomaly'' via combinations of state word and text template in given lists, rather than freely writing definitions. The texts from CPE are more aligned with images in CLIP's joint embedding space for anomaly tasks than texts of CLIP prompt ensemble (evidence in 0-shot results in Table 1 and \ref{ablate:context_words}). This is achieved by (1) common state words shared by most objects, \eg ``flawless'' for normality/``damaged'' for anomaly; (2) (optionally) task-specific state words given prior knowledge of defects, \eg ``bad soldering'' on PCB (elaborated in \textbf{Generalization of CPE}); (3) curated template list for anomaly tasks. This is different from CLIP prompt ensemble that does not explain object labels (\eg ``cat'') and only augments templates selected by trial-and-error for object classification, including the ones unsuitable for anomaly tasks, \eg ``a cartoon \{\}''.

% We observe that CLIP-AC can significantly benefit from diversifying the ways to describe the abnormality. Unlike standard object-level classifiers, CLIP-AC performs classification between two \emph{states} of a given object, \ie either ``normal'' or ``anomalous''. They are subjective and thus have more diversity to explain themselves: \eg the prompt with ``\verb|anomalous [c]|'' we use for CLIP-AC can also be with ``\verb|damaged [c]|'' or ``\verb|[c] with a damage|''.
% Motivated by this, we propose to combine two levels of ensemble, namely (a) \emph{state-level} and (b) \emph{template-level} (see upper part of Figure~\ref{fig:overall}), Specifically, we consider a ``bag'' of various text templates (\eg ``\verb|a photo of a [state] [c]|'', ``\verb|a cropped photo of a [state] [c]|''), as well as two bags of \emph{pre-defined} \emph{state} words for ``normal'' (\eg ``\verb|flawless|'', ``\verb|perfect|'') or ``anomalous'' (\eg ``\verb|damaged|'', ``\verb|broken|'') respectively.  We generate the prompts by compositing the two levels of bags, and compute the average of the (textual) embeddings to represent each state.


% consider two ``bags'' of states (see the supplementary material for the details), each for ``normal'' or ``anomalous'', and compute the average of the (textual) embeddings to represent each of the states.

\begin{figure}[t]
  \includegraphics[width=0.48\textwidth]{figures/featExtract.png}
%   \caption{Feature extraction in image, mid-/small-scale windows through the pre-trained CLIP visual encoder, which is a ViT taking a sequence of patches as input. Each window is produced by image with masked patches, where the corresponding window embedding encodes the global information within the window.}
  \caption{WinCLIP feature extraction in multiple scales of windows through CLIP image encoder, \eg ViT taking a sequence of (non-masked) patches as input. Window embeddings encode the global information (\eg from the class token) within each window.}
  \label{fig:featExtract}
  \vspace{-0.15in}
\end{figure}

% \begin{figure}[t]
%   \includegraphics[width=0.48\textwidth]{figures/refAssociate.png}
%   \caption{Reference association. For each local feature, the nearest local feature of the normal images is retrieved for association and the paired distance is regarded as the anomaly score}
% \end{figure}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.98\linewidth]{figures/winCLIP_w_plus.png}
  \vspace{-0.05in}
%   \caption{Workflow of WinCLIP (upper part) and WinCLIP+ (entire section). Several text prompts are used to define normality and anomaly. With CLIP text encoder, they are converted and group-averaged to two text embeddings as class prototypes. The two class prototypes are correlated with the multi-scale features extracted by CLIP image encoder for zero-shot anomaly classification/segmentation in WinCLIP. WinCLIP+ applies the reference association on patch, small-/mid-window (Patch/WindowAssociation) for vision-based anomaly score map in each scale, which are aggregated for few-shot anomaly segmentation and classification with the text-dependent scores from WinCLIP.}
  \caption{Workflows of WinCLIP/WinCLIP+ (upper/entire pane). Various states and templates are composited and converted to two text embeddings as class prototypes via CLIP text encoder (Section \ref{sec:zeroshot}). The class prototypes are correlated with the multi-scale features from CLIP image encoder (Figure \ref{fig:featExtract}) for zero-shot AC/AS in WinCLIP. WinCLIP+ applies the reference association on patch, small-/mid-window (Patch/WindowAssociation) for vision-based anomaly score maps, which are aggregated for few-shot AS/AC with language-guided scores.}
  \label{fig:overall}
  \vspace{-0.15in}
\end{figure*}

\subsection{WinCLIP for zero-shot AS}
\label{sec:winclip}

% Given the zero-shot AC model $\mathrm{ascore}_0$ (Section~\ref{sec:zeroshot}), we extend the model to zero-shot anomaly segmentation. In particular, we aim to extract a dense feature map from $\rvx$ aligned with language where the $\mathrm{ascore}_0$ can be re-used pixel-wise to obtain anomaly score map for zero-shot segmentation. This is challenging as CLIP is originally not trained to return dense image features suitable for localization. 
Given the language guided anomaly scoring model from CPE, we propose Window-based CLIP (WinCLIP) for zero-shot anomaly segmentation to predict pixel-level anomalies. WinCLIP extracts dense visual features with good language alignment and local details for $\rvx$, followed by applying $\mathrm{ascore}_0$ spatially to obtain the anomaly segmentation map. Specifically, given an image $\rvx$ of resolution $h\times w$ and an image encoder $f$, \mbox{WinCLIP} obtains a map of $d$-dimensional feature map $\mathbf{F}^{\tt W} \in \mathbb{R}^{h\times w\times d}$ as follows:
\begin{enumerate}
    \item Generate a set of sliding windows $\{\mathbf{w}_{ij}\}_{ij}$, where each window $\mathbf{w}_{ij} \in \{0,1\}^{h \times w}$ is a binary mask that is active locally for a $k \times k$ kernel around $(i, j)$.
    \item Collect each output embedding $\mathbf{F}^{\tt W}_{ij}$, computed from the active area of $\rvx$ after applying each $\mathbf{w}_{ij}$, defined by:
    % \footnote{Here, we assume the stride of 1 for simplicity, but the notation can be easily generalized for other appropriate strides.}
    \begin{equation}\label{eq:winclip}
        \mathbf{F}^{\tt W}_{ij} := f(\rvx \odot \mathbf{w}_{ij}),
    \end{equation}
    where $\odot$ is the element-wise product (see Figure~\ref{fig:featExtract}).
\end{enumerate}
Figure~\ref{fig:featExtract} illustrates the dense feature extraction of WinCLIP with ViT while it is also applicable to CNN.
% This is challenging as CLIP is originally not trained to return dense image features suitable for localization tasks. 

% Remark that as CLIP enforces cross-modality alignment only on the global embeddings of image and text, it is not designed for localization tasks such as semantic segmentation.

In addition, we also explore a natural dense representation candidate, \emph{penultimate feature map}, the last feature map before pooling. Specifically, for patch embedding map $\mathbf{F}^{\tt P}$ (other than the \emph{class token} \verb|[CLS]|) of ViT-based CLIP, top of Figure~\ref{fig:featExtract}, we apply $\mathrm{ascore}_0$ patch-wisely for segmentation. However, we observe that such patch-level features are not aligned with the language space, leading to a poor dense predictions (Table~\ref{tab:al_0shot}). We conjecture this is caused by those features have not been directly supervised with language signal in CLIP. Also these patch features have already aggregated the global context due to self-attention, hindering capturing local details for segmentation.

% However, we observe that 
% The key challenge in this attempt is 
% Challenge: patch-language (dense) is not available ; patch representation may already aggregate global context (large receptive field) 
Compared to the penultimate features $\mathbf{F}^{\tt P}$, we remark dense features from WinCLIP is more aligned with language: \eg for ViT-based CLIP, all the features in $\mathbf{F}^{\tt W}$ are now from class tokens which are directly aligned to texts in CLIP pre-training. Also the features focus more on local details via sliding windows. Lastly, WinCLIP can be efficiently computed, especially with ViT architecture. Concretely, the computation of \eqref{eq:winclip} can directly benefit from just dropping all the masked patches before forwarding them, in a similar manner to masked autoencoder \cite{he2022masked}.

\vspace{0.05in}
\noindent\textbf{Harmonic aggregation of windows. }
For each local window, the zero-shot anomaly score $\mathbf{M}^{\tt W}_{0,ij}$ is similarity between the window feature $\mathbf{F}^{\tt W}_{ij}$ and text embeddings from compositional prompt ensemble. This score is distributed to every pixel of the local window. Then at each pixel, we aggregate multiple scores from all overlapping windows to improve segmentation by \emph{harmonic averaging} \eqref{eq:harmonic}, weighting more on scores towards normality prediction (zero value).
% to weight more on scores implement ``normal-first'' policy: if there is at least one observation confident as normal (\ie $\mathbf{M}^{\tt W}_{0,ij} \approx 0$), we follow that score:
\begin{equation}\label{eq:harmonic}
    \bar{\mathbf{M}}^{\tt W}_{0,ij} := \left(\frac{1}{\sum_{u,v} {(\mathbf{w}_{uv})_{ij}}} \sum_{u,v} \frac{(\mathbf{w}_{uv})_{ij}}{\mathbf{M}^{\tt W}_{0,uv}} \right)^{-1}.
\end{equation}

% Having a WinCLIP based feature map $\mathbf{F}^{\tt W}$, it becomes straightforward to obtain pixel-level anomaly scores $\mathbf{M}_0^{\tt W} \in [0, 1]^{h\times w}$ for zero-shot AS. We apply our $\mathrm{ascore}_0(\cdot)$ (Section~\ref{sec:zeroshot}) on the CLIP embedding pixel-wise, \ie $\mathbf{M}^{\tt W}_{0,ij} := \mathrm{ascore}_0(\mathbf{F}^{\tt W}_{ij})$.
% % \begin{equation}
% %     \mathbf{M}^{\tt W}_{0,ij} := \mathrm{AC}_0(\mathbf{F}^{\tt W}_{ij}).
% % \end{equation}
% To incorporate certain effects from having overlapping pixels across $\mathbf{w}_{ij}$'s, we propose to re-distribute the scores in $\mathbf{M}^{\tt W}_0$ 
% back to the corresponding pixels in each window $\mathbf{w}_{ij}$, and aggregate them via \emph{harmonic averaging}. Specifically, we define the aggregated AS scores $\bar{\mathbf{M}}^{\tt W}_0$ by: % based on $\mathbf{w}$, 
% % \begin{equation}\label{eq:harmonic}
% %     \bar{\mathbf{M}}^{\tt W}_{0,ij} := \left(\frac{1}{\sum_{w\in \mathbf{w}} w_{ij}} \sum_{(w, M) \in (\mathbf{w}, \mathbf{M}^{\tt W}_0)} \frac{w_{ij}}{M_{ij}} \right)^{-1}.
% % \end{equation}
% % \begin{equation}\label{eq:harmonic}
% %     \bar{\mathbf{M}}^{\tt W}_{0,ij} := \left(\frac{1}{\sum_{w \in \mathbf{w}} w_{ij}} \sum_{u,v} \frac{(\mathbf{w}_{uv})_{ij}}{\mathbf{M}^{\tt W}_{0,uv}} \right)^{-1}.
% % \end{equation}
% % \begin{equation}\label{eq:harmonic}
% %     \bar{\mathbf{M}}^{\tt W}_{0,ij} := \left(\frac{1}{\sum_{u',v'} {(\mathbf{w}_{u'v'})_{ij}}} \sum_{u,v} \frac{(\mathbf{w}_{uv})_{ij}}{\mathbf{M}^{\tt W}_{0,uv}} \right)^{-1}.
% % \end{equation}
% \begin{equation}\label{eq:harmonic}
%     \bar{\mathbf{M}}^{\tt W}_{0,ij} := \left(\frac{1}{\sum_{u,v} {(\mathbf{w}_{uv})_{ij}}} \sum_{u,v} \frac{(\mathbf{w}_{uv})_{ij}}{\mathbf{M}^{\tt W}_{0,uv}} \right)^{-1}.
% \end{equation}
% By taking an harmonic average, we aim to implement the ``normal-first'' policy for aggregating multiple (overlapping) scores: \viz if there is at least one observation confident as normal (\ie $\mathbf{M}^{\tt W}_{0,ij} \approx 0$), we follow that score.
% % : this makes sense under a reasonable assumption of $p(-) > p(+)$.

\vspace{0.05in}
\noindent\textbf{Multi-scale aggregation. } 
The kernel size $k$ corresponds to the amount of surrounding context for each location in computing WinCLIP features \eqref{eq:winclip}. It controls the balance between local details and global information in segmentation. To capture defects of sizes ranging from small to large scale, we aggregate predictions from multi-scale features: \eg (a) small-scale ($2\times 2$ in patch scales of ViT; corresponds to $32\times 32$ in pixels), (b) mid-scale ($3\times 3$ in ViT; $48\times 48$), and (c) image-scale feature (ViT class token capturing image context due to self-attention). We also adopt harmonic averaging for aggregation. Figure~\ref{fig:featExtract} illustrates the features on each scale.


\subsection{WinCLIP+ with few-normal-shots}
\label{sec:winclip+}
% Although language guided zero-shot anomaly prediction 
For a comprehensive anomaly classification and segmentation, language guided zero-shot approach is not enough as certain defects can only be defined via visual reference rather than only text. For example, ``Metal-nut'' in MVTec-AD \cite{bergmann2019mvtec} has an anomaly type labeled as ``flipped upside-down'', which can only be identified relatively from a normal image. To define and recognize the anomalies more precisely, we propose an extension of WinCLIP, \emph{WinCLIP+}, by incorporating $K$ normal reference images $\mathcal{D}:=\{(x_i, -)\}_{i=1}^{K}$. WinCLIP+ combines the complementary prediction from both language-guided and visual based approachs for better anomaly classification and segmentation.
% Although WinCLIP offers strong models to perform zero-shot ACS, it can be still useful to have an additional reference (normal) image to compare with. Actually it is even necessary in certain cases: \eg in ``Metal-nut'' of the MVTec-AD dataset \cite{bergmann2019mvtec}, there is an anomaly type labeled as ``flipped'', which can only be recognized relatively from a normal reference. 
% In this respect, we now assume few-normal-shot scenarios, \ie where one has an access to $K$ normal samples $\mathcal{D}:=\{(x_i, -)\}_{i=1}^{K}$, and question on how to incorporate this information effectively into WinCLIP-based ACS. Especially, we are interested in whether the (zero-shot) language guided information extracted from WinCLIP collaborates with those from the (few-shot) reference images.

% In this section, therefore, we now assume a \emph{few-shot} setup, \ie where  $\mathcal{D}:=\{(x_i, -)\}_{i=1}^{K}$ we question (a) , and (b) ... 

We first propose a \emph{reference association} as the key module to incorporate given reference images, which can simply store and retrieve the memory features $\mathbf{R}$ of $\mathcal{D}$ based on the cosine similarity. Given such module and the corresponding (\eg patch-level\footnote{Nevertheless, the module is generally applicable for other scales.}) features $\mathbf{F} \in \mathbb{R}^{h\times w\times d}$ extracted from a query image, a prediction $\mathbf{M} \in [0, 1]^{h\times w}$ for anomaly segmentation can be made by: 
% To this end, we first consider a \emph{reference association} $\mathbf{R}$ as the key component to incorporate the given reference images, which is a simple memory-based module that can store and retrieve some features from $\mathcal{D}$ based on the cosine similarity. Given such a module $\mathbf{R}$ and the corresponding (\eg patch-level\footnote{Nevertheless, the module is generally applicable for other scales.}) features $\mathbf{F} \in \mathbb{R}^{h\times w\times d}$ extracted from a query image, a prediction $\mathbf{M} \in [0, 1]^{h\times w}$ for (few-shot) anomaly segmentation can be made by: 
\begin{equation}\label{eq:memory_al}
    % \mathbf{M}_{ij} := \min_{r \in \mathbf{R}} \| \mathbf{F}_{ij} - r \|_2.
    \mathbf{M}_{ij} := \min_{r \in \mathbf{R}} \tfrac{1}{2} (1 - \langle \mathbf{F}_{ij}, r \rangle).
\end{equation}

Then we apply this association module at multiple scales of feature maps that are obtained from WinCLIP (see Figure~\ref{fig:overall} for the overall illustration). Specifically, given few-shot samples, we construct separate reference memories from three different features: (a) WinCLIP features at small-scale $\mathbf{F}^{\tt W}_{\tt s}$, (b) those at mid-scale $\mathbf{F}^{\tt W}_{\tt m}$, and also (c) from penultimate features $\mathbf{F}^{\tt P}$ with global context (\eg the patch tokens in ViT capturing image context due to self-attention). Even though $\mathbf{F}^{\tt P}$ is not aligned with language, it still useful to define normality and anomaly. 

As a result, WinCLIP+ gets three reference memories: $\mathbf{R}^{\tt W}_{\tt s}$, $\mathbf{R}^{\tt W}_{\tt m}$, and $\mathbf{R}^{\tt P}$. 
Then, we average their multi-scale predictions (\ref{eq:memory_al}) for anomaly segmentation for a given query,
\begin{equation}\label{eq:winplus_al}
    \mathbf{M}^{\tt W} := \tfrac{1}{3}(\mathbf{M}^{\tt P} + \mathbf{M}^{\tt W}_{\tt s} + \mathbf{M}^{\tt W}_{\tt m}),
\end{equation}
and then fusing with our language-guided prediction $\bar{\mathbf{M}}^{\tt W}_0$.

To perform anomaly classification, we combine the maximum value of $\mathbf{M}^{\tt W}$ and the WinCLIP zero-shot classification score.
The two scores have complementary information to collaborative with, specifically (a) one from the spatial features of few-shot references, and (b) the other one from the CLIP knowledge retrieved via language:
% interestingly, we found the (language-driven) zero-shot AC score can still be collaborative with the few-shot based score, reflecting its complementary information based on the CLIP knowledge: 
\begin{equation}\label{eq:winplus_ad}
    \mathrm{ascore}_{\tt W}(\rvx) := \tfrac{1}{2}\left(\mathrm{ascore}_0(f(\rvx)) + \max_{ij} \mathbf{M}^{\tt W}_{ij} \right).
\end{equation}
% igure~\ref{fig:overall} illustrates the overall workflow of our method.

% \subsubsection{Rreference matching}
% kNN distance is point-to-set distance
% \paragraph{Reference matching score map.} We get the reference matching score maps in all patch and window levels.
% \subsubsection{WinCLIP with reference}

\section{Experiments}
\label{sec:experiment}

% We verify the effectiveness of our proposed components 
We perform an array of experiments to evaluate the performance of WinCLIP-based ACS under low-shot regimes, covering recent challenging benchmarks on industrial anomaly classification and segmentation that we are focusing on. 
% From the experiments, we show that (a) ..., and (b) ... 
We also conduct an extensive ablation study to validate the individual effectiveness of our proposed components. The detailed setups, \eg pre-processing, metrics, and other implementation details, are given in the supplementary. 
% Note that we use $F_1$-max, $F_1$-score at optimal threshold, for a clearer view against potential data imbalance \citet{zou2022spot}.

% \begin{table*}[t]
%   \centering
%   \small
%   \begin{adjustbox}{width=0.74\linewidth}
%   \input{tables/ad_main}
%   \end{adjustbox}
%   \caption{Comparison of anomaly classification (AC) performance on MVTec-AD and VisA benchmarks. We report the mean and standard deviation over 5 random seeds for each measurement. Bold indicates the best performance.}\label{tab:ad}
%   \vspace{-0.1in}
% \end{table*}

\begin{table*}[t]
    \centering
    \small
    \hfill
    \begin{minipage}{0.69\linewidth}
        \centering
        \begin{adjustbox}{width=\linewidth}
        \input{tables/ad_main}
        \end{adjustbox}
        \vspace{-0.05in}
        \caption{Comparison of anomaly classification (AC) performance on MVTec-AD and VisA benchmarks. We report the mean and standard deviation over 5 random seeds for each measurement. Bold indicates the best performance.}\label{tab:ad}
    \end{minipage}
    \hfill
    \begin{minipage}{0.27\linewidth}
        \centering
        \begin{adjustbox}{width=\linewidth}
        \input{tables/mvtec_fullshot}
        \end{adjustbox}
        \vspace{-0.1in}
        \caption{Comparison with existing many-shot ACS methods in AUROC (or pixel-) on MVTec-AD.}
        \label{tab:mvtec_fs}
        \vspace{0.05in}
        \begin{adjustbox}{width=\linewidth}
        \input{tables/ad_0shot}
        \end{adjustbox}
        \vspace{-0.1in}
        \caption{Comparison of AC performance on MVTec-AD across WinCLIP ablations in AC (Section~\ref{sec:zeroshot}).}
        \label{tab:ad_0shot}
    \end{minipage}
    \hfill
    \vspace{-0.2in}
\end{table*}


\vspace{0.05in}
\noindent\textbf{Datasets. }
Our experiments are based on MVTec-AD \cite{bergmann2019mvtec} and VisA \cite{zou2022spot} datasets. Both benchmarks have diverse subsets of different objects, \eg capsules, circuit boards. They contain high-resolution images (\eg $700^2$-$1024^2$ for MVTec-AD, and roughly $1.5\text{K} \times 1\text{K}$ for VisA) of common objects with the full pixel-level annotations.
% \Yang{For the texts for the defects, we leverage the defect types provided in both MVTec-AD and VisA.}

\vspace{0.05in}
\noindent\textbf{Evaluation metrics. }
For classification, we report (a) \emph{Area Under the Receiver Operating Characteristic} (AUROC) following the literature \cite{yi2020patch,defard2021padim,roth2022towards}, as well as (b) \emph{Area Under the Precision-Recall curve} (AUPR) and (c) \emph{$F_1$-score at optimal threshold} ($F_1$-max) for a clearer view against potential data imbalance \citet{zou2022spot}). For segmentation, we report (a) \emph{pixel-wise AUROC} (pAUROC) and (b) \emph{Per-Region Overlap} (PRO) \cite{bergmann2020uninformed} scores \cite{defard2021padim,li2021cutpaste}, and (c) \emph{(pixel-wise) $F_1$-max} in a similar manner to the anomaly classification evaluation. 

\vspace{0.05in}
\noindent\textbf{Implementation details. }
We adopt the CLIP implementation of OpenCLIP\footnote{\url{https://github.com/mlfoundations/open_clip}} and its public pre-trained models in our experiments: namely, we use the LAION-400M \cite{schuhmann2021laion} based CLIP with {ViT-B/16+} \cite{ilharco_gabriel_2021_5143773} unless otherwise noted. 
We apply WinCLIP with stride 1 on ViT patch embeddings, which is equivalent to stride 16 in pixel-level in case of ViT-B/16+. 

% , where ViT-B/16+ is a modification of ViT-B/16 \cite{touvron2021training} with (a) an increased dimension in both image ($768 \rightarrow 896$) and text ($512 \rightarrow 640$) embeddings, as well as in (b) the input resolution ($224^2 \rightarrow 240^2$; $196 \rightarrow 225$ tokens). When evaluating CLIP-based models, we follow the data pre-processing pipeline given in OpenCLIP for both MVTec-AD and VisA datasets to minimize potential train-test discrepancy. For the other baseline models, we keep their own pre-processing steps except for that we set fixed the input resolution to have $240^2$ for fairer comparisons.

% \subsection{Quantitative results}

% Baseline: 
% 0-shot: Naive CLIP, 
% Few-shot: PaDiM, SPADE, PatchCORE, (optionally RegAD)
% Compare with other strong full-shot methods



\subsection{Zero-/few-shot anomaly classification}
\label{sec:exp_ad}

In Table~\ref{tab:ad} we compare zero-shot and few-normal-shot anomaly classification results with prior works.

For zero-shot setup, we compare WinCLIP with two prior models: CLIP-AC (first row of Table~\ref{tab:ad}), which is the original 
CLIP zero-shot classification \cite{radford2021learning} with labels of the form $\{\text{``\texttt{normal [c]}''}, \text{``\texttt{anomalous [c]}''}\}$, and CLIP-AC with the prompt ensemble (second row in Table~\ref{tab:ad}) from \cite{radford2021learning} engineered for ImageNet \cite{krizhevsky2017imagenet}. We see that WinCLIP significantly improves over using these na\"ive adaptations of CLIP on both MVTec-AD and VisA. Section~\ref{sec:ablation} presents ablation study on a break-down of this gain.

For the few-normal-shot setup, we see the same trend: WinCLIP+ outperforms prior works by a wide margin across all metrics on both benchmarks. In particular, we improve upon the state-of-the-art PatchCore \cite{roth2022towards} by $9.7\%$ on 1-shot MVTec-AD and by $5.3\%$ on 1-shot VisA. On MVTec-AD, we note that zero-shot WinCLIP outperforms the few-shot versions of prior works. Furthermore, WinCLIP+ 1/2/4-shot performance is better than WinCLIP 0-shot performance, highlighting the additional value of reference normal images. 

%For the zero-shot setup, where our language-driven $\mathrm{ascore}_0$ is applicable (Section~\ref{sec:zeroshot}), we compare our results with a direct adaptation of the original CLIP zero-shot classification \cite{radford2021learning} with labels of the form $\{\text{``\texttt{normal [c]}''}, \text{``\texttt{anomalous [c]}''}\}$ (``CLIP-AC''). We also test the standard prompt ensemble practice \cite{radford2021learning} upon CLIP-AC, that was engineered for ImageNet \cite{krizhevsky2017imagenet} (``+ Prompt ens.''): overall, we observe that such a generic approach may not be enough for anomaly recognition tasks, and confirm that our AC-specific design can significantly improve the results. 
%For the non-zero-shot cases, on the other hand, we could consistently yet significantly improve the previous best attempts with WinCLIP+, \eg we improve upon the state-of-the-art of PatchCore \cite{roth2022towards} by $9.7\%$ on 1-shot MVTec-AD and by $5.3\%$ on 1-shot VisA: in fact, even our \emph{zero-shot} model on MVTec-AD already outperforms all the baselines considered here. 

\subsection{Zero-/few-shot anomaly segmentation}
\label{sec:exp_al}

In Table~\ref{tab:al} we compare zero-shot and few-normal-shot anomaly segmentation results with prior works. While there are no prior works on zero-shot anomaly segmentation, we adapt two methods developed for other problems to our setup. First, Trans-MM \cite{chefer2021generic} is a recent model interpretation method applicable to Transformers that provides a pixel-level mask. Second, MaskCLIP \cite{zhou2022extract} is a  general semantic segmentation model based on CLIP. We see that WinCLIP outperforms both methods by a wide margin on both MVTec-AD and VisA, highlighting that generic adaptations of CLIP do not perform as well as WinCLIP. 

For the few-normal-shot setup, we compare with three prior works, which are designed specifically for anomaly localization. We see that WinCLIP+ again outperforms these prior methods across all metrics on both benchmarks, showing the additional value provided by language prompts. In Figure~\ref{fig:qualitative}, we show qualitative results for a number of objects and defects. We see that in all cases, 1-shot WinCLIP+ provides a mask that is more concentrated on the ground truth compared to prior works. We also see that 1/2/4-normal-shot WinCLIP+ is better than 0-shot WinCLIP, demonstrating the complementary benefits of language driven prediction and visual only based model based on reference normal images. 

%Table~\ref{tab:al}, on the other hand, evaluates the anomaly segmentation performance of WinCLIP (and WinCLIP+), assuming the same low-shot scenarios with the AC evaluation (Section~\ref{sec:exp_ad}). For the zero-shot AS, we consider two recent methods as baselines that offer a pixel-level attribution map with different purposes: specifically, we test (a) \emph{Trans-MM} \cite{chefer2021generic}: a recent model interpretation method generally applicable for Transformers, and (b) \emph{MaskCLIP} \cite{zhou2022extract} which targets general semantic segmentation from CLIP, showing that the \emph{value features} in the last attention layer (of CLIP image encoder) is better aligned with the language space. Overall, we observe that our approach of WinCLIP can significantly outperform these (generic) methods, highlighting the importance of focusing on local information for zero-shot AS.
%For the few-shot cases, our WinCLIP+ achieves consistently better performances compared to the best of prior arts: this is remarkable given the simplicity of our approach, \ie by combining simple memory-based predictions but with our WinCLIP features. Figure~\ref{fig:qualitative} compares the methods qualitatively on 1-shot MVTec-AD and VisA, and more comparisons can be found in the supplementary material.  

\subsection{Comparison with many-shot methods}
In Table~\ref{tab:mvtec_fs} we compare our zero-/few-shot results with full-shot results of several prior works on MVTec-AD. Our 4-shot WinCLIP+ is competitive with CutPaste \cite{li2021cutpaste}, a recent method that utilizes the \textit{full-shot} samples for model tuning. 
Also, our 0-shot WinCLIP outperforms recent few-shot methods in AC, such as DifferNet \cite{rudolph2021same} and TDG \cite{sheynin2021hierarchical}, even compared to their results with more than 10-shots.
Recently, a new setup of aggregated few-shot is proposed \cite{huang2022registration}, where one is free to use all the training samples but for the target class which is restricted to $k$-shot. Our 4-shot WinCLIP+ outperforms RegAD's aggregated 4-shot \cite{huang2022registration} performance. 

%In Table~\ref{tab:mvtec_fs}, we compare our zero-shot and few-shot results along the line of \emph{many-shot} benchmarks on MVTec-AD, \eg covering the typical \emph{full-shot} setup, as well as a recently-proposed \emph{aggregated few-shot} \cite{huang2022registration} where one is free to use all the training data except for the target class with $k$-shot. From the results, we make the following highlights: (a) our result on zero-shot AC outperforms recent 2-shot and aggregated 4-shot results from RegAD \cite{huang2022registration}, and also in AS with only using 4-shot of samples; (b) compared to the full-shot models, our \textit{4-shot} WinCLIP+ is competitive with CutPaste \cite{li2021cutpaste}, a recent method that utilizes the \textit{full-shot} samples for model pre-training (or fine-tuning).

% \begin{table*}[t]
%     \centering
%     \small
%     \hfill
%     \begin{minipage}{0.67\linewidth}
%         \centering
%         \begin{adjustbox}{width=\linewidth}
%         \input{tables/al_main}
%         \end{adjustbox}
%         \vspace{-0.02in}
%         \caption{Comparison of anomaly segmentation (AS) performance on MVTec-AD and VisA benchmarks. We report the mean and standard deviation over 5 random seeds for each measurement. Bold indicates the best performance.}\label{tab:al}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.27\linewidth}
%         \centering
%         \begin{adjustbox}{width=\linewidth}
%         \input{tables/mvtec_fullshot}
%         \end{adjustbox}
%         \vspace{-0.1in}
%         \caption{Comparison with existing many-shot ACS methods in AUROC (or pixel-) on MVTec-AD.}
%         \label{tab:mvtec_fs}
%         \vspace{0.05in}
%         \begin{adjustbox}{width=\linewidth}
%         \input{tables/ad_0shot}
%         \end{adjustbox}
%         \vspace{-0.1in}
%         \caption{Comparison of AC performance on MVTec-AD across WinCLIP ablations in AC (Section~\ref{sec:zeroshot}).}
%         \label{tab:ad_0shot}
%     \end{minipage}
%     \hfill
% \end{table*}

\begin{figure*}[t]
    \centering
    \hspace*{\fill}
    \begin{subfigure}{0.40\linewidth}
        \includegraphics[width=\linewidth]{figures/mvtec_1shot.png}
        % \vspace{0.01in}
        \caption{MVTec-AD (1-shot)}\label{fig:qual_mvtec}
    \end{subfigure}
    \hspace*{\fill}
    \begin{subfigure}{0.56\linewidth}
        \includegraphics[width=\linewidth]{figures/visa_1shot.png}
        % \vspace{0.01in}
        \caption{VisA (1-shot)}\label{fig:qual_visa}
    \end{subfigure}
    \hspace*{\fill}
    \vspace{-0.07in}
    \caption{Qualitative comparison of 1-shot anomaly segmentation results on MVTec-AD and VisA benchmarks.}
    \label{fig:qualitative}
    \vspace{-0.07in}
\end{figure*}

\begin{table*}[t]
    \centering
    \small
    \hfill
    \begin{minipage}{0.64\linewidth}
        \centering
        \begin{adjustbox}{width=\linewidth}
        \input{tables/al_main}
        \end{adjustbox}
        \vspace{-0.06in}
        \caption{Comparison of anomaly segmentation (AS) performance on MVTec-AD and VisA benchmarks. We report the mean and standard deviation over 5 random seeds for each measurement. Bold indicates the best performance.}\label{tab:al}
    \end{minipage}
    \hfill
    \begin{minipage}{0.31\linewidth}
        \centering
        \begin{adjustbox}{width=0.98\linewidth}
        \input{tables/winclipp_ad}
        \end{adjustbox}
        \vspace{-0.05in}
        \caption{$k$-shot AC ablations: MVTec-AD. Bold/underline indicate the best/runner-up.}\label{tab:winclipp_ad}
        \vspace{0.1in}
        \begin{adjustbox}{width=0.98\linewidth}
        \input{tables/winclipp_al}
        \end{adjustbox}
        \vspace{-0.05in}
        \caption{$k$-shot AS ablations: MVTec-AD. Bold/underline indicate the best/runner-up.}\label{tab:winclipp_al}
        \vspace{0.1in}
        \begin{adjustbox}{width=0.98\linewidth}
        \input{tables/ablation_specific_states}
        \end{adjustbox}
        \vspace{-0.05in}
        \caption{Ablation on specific states: VisA.}\label{ablate:context_words}
    \end{minipage}
    \hfill
    \vspace{-0.18in}
\end{table*}
\begin{table}[t]
    \centering
    \begin{adjustbox}{width=0.83\linewidth}
    \input{tables/al_0shot}
    \end{adjustbox}
    \vspace{-0.05in}
    \caption{Comparison of AS performance on MVTec-AD and its per-image inference time, measured at Amazon EC2 G4dn instances. }\label{tab:al_0shot}
    \vspace{-0.2in}
\end{table}

% \begin{table*}[t]
%     \hfill
%     \begin{minipage}[t]{0.37\linewidth}
%         \centering
%         \begin{adjustbox}{width=\linewidth}
%         \input{tables/al_0shot}
%         \end{adjustbox}
%         \caption{Comparison of AS performance on MVTec-AD and its per-image inference time, measured at Amazon EC2 G4dn instances. Bold and underline indicates the best and runner-up, respectively.}\label{tab:al_0shot}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[t]{0.29\linewidth}
%         \centering
%         \begin{adjustbox}{width=\linewidth}
%         \input{tables/winclipp_ad}
%         \end{adjustbox}
%         \caption{Comparison of AUROC on MVTec-AD with WinCLIP+ ablations in AC \eqref{eq:winplus_ad}. Bold and underline indicates the best and runner-up, respectively.}\label{tab:winclipp_ad}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[t]{0.28\linewidth}
%         \centering
%         \begin{adjustbox}{width=\linewidth}
%         \input{tables/winclipp_al}
%         \end{adjustbox}
%         \caption{Comparison of pAUROC on MVTec-AD with WinCLIP+ ablations in AS \eqref{eq:winplus_al}. Bold and underline indicates the best and runner-up, respectively.}\label{tab:winclipp_al}
%     \end{minipage}
%     \hfill
%     \vspace{-0.1in}
% \end{table*}


% \begin{table*}[t]
%   \centering
%   \small
%   \input{tables/al_main}
%   \caption{Comparison of anomaly segmentation (AS) performance on MVTec-AD and VisA benchmarks. We report the mean and standard deviation over 5 random seeds for each measurement. Bold indicates the best performance.}
%   \label{tab:al}
% \end{table*}

% \begin{table}[t]
%   \centering
%   \small
%   %\begin{adjustbox}{width=\linewidth}
%   \input{tables/al_0shot}
%   % \end{adjustbox}
%   \caption{Comparison of AS performance and per-sample inference time (in millisecond) on MVTec-AD. Bold and underline indicates the best performance and its runner-up, respectively.}
%   \label{tab:al_0shot}
% \end{table}

% \begin{table}[t]
%   \centering
%   \small
%   %\begin{adjustbox}{width=\linewidth}
%   \input{tables/winclipp_ad}
%   %\end{adjustbox}
%   \caption{Comparison of AUROC on MVTec-AD with WinCLIP+ ablations in AC \eqref{eq:winplus_ad}. Bold and underline indicates the best performance and its runner-up, respectively.}
%   \label{tab:winclipp_ad}
% \end{table}

% \begin{table}[t]
%   \centering
%   \small
%   %\begin{adjustbox}{width=\linewidth}
%   \input{tables/winclipp_al}
%   %\end{adjustbox}
%   \caption{Comparison of pAUROC on MVTec-AD with WinCLIP+ ablations in AS \eqref{eq:winplus_al}. Bold and underline indicates the best performance and its runner-up, respectively.}
%   \label{tab:winclipp_al}
% \end{table}

% \begin{table}[t]
%   \centering
%   \small
%   %\begin{adjustbox}{width=\linewidth}
%   \input{tables/mvtec_fullshot}
%   %\end{adjustbox}
%   \caption{Comparison with existing many-shot ACS methods in AUROC (``AC'') and pAUROC (``AS'') on MVTec-AD.}
%   \label{tab:mvtec_fs}
% \end{table}

% \begin{table}[t]
%   \centering
%   \small
%   %\begin{adjustbox}{width=\linewidth}
%   \input{tables/ad_0shot}
%   %\end{adjustbox}
%   \caption{Comparison of AC performance on MVTec-AD across WinCLIP ablations in AC (Section~\ref{sec:zeroshot}).}
%   \label{tab:ad_0shot}
% \end{table}


\subsection{Ablation study}
\label{sec:ablation}

We perform component-wise analysis on MVTec-AD \cite{bergmann2019mvtec}. 
A further study, \eg comparison with CLIP-based PatchCore, effect of different backbones, discussion on failure cases, \textit{etc.}, can be found in the supplementary material.

\vspace{0.05in}
\noindent\textbf{WinCLIP for AC: } 
In Table~\ref{tab:ad_0shot}, we report the individual effect of components that constitute our zero-shot AC model. Firstly, we observe (a) the textual supervision for the word ``\verb|anomalous|'' is crucial to achieve a reasonable performance (``One-class''; Section~\ref{sec:zeroshot}), suggesting the effectiveness of CLIP knowledge about ``abnormality''. Next, we confirm that having a diversity in both (b) state-level and (c) prompt-level texts are the key source of gains. And we remark the proposed state ensemble as a more significant component. 
Finally, we observe (d) applying multi-crop prediction \cite{hinton2012imagenet} could also yield a minor improvement. 


\vspace{0.05in}
\noindent\textbf{WinCLIP for AS: }
Table~\ref{tab:al_0shot} validates not only the efficiency of WinCLIP to extract local features for zero-shot AS, but also the effectiveness of multi-scale and harmonic averaging to boost the results. To this end, we consider the following additional baselines that also extract patch-level features: (\textit{i}) \emph{Patch-token} (Section \ref{sec:winclip}): it takes the patch features at the last layer, and (\textit{ii}) \emph{Image tiling}: it first performs dense ``tiling'' on an image and then obtains ``tile'' embeddings for segmentation by forwarding each tile with resizing. Overall, the comparison shows that patch-tokens are not aligned with language despite its fast inference time, while ``Image tiling'' makes a significant computational overhead although it does benefit from their local features. WinCLIP achieves accelerated inference due to its window-based computation of local features, with even better performance. Also based on the multi-scale study, we observe that segmentation benefits from both features with image-level, and middle/local context. Note that the scores from last patch embeddings of ViT encodes global context thanks to self-attention, which contributes to a comprehensive localization in WinCLIP.

\vspace{0.05in}
\noindent\textbf{WinCLIP+ for AC and AS: } 
% Recall from \eqref{eq:winplus_al} and \eqref{eq:winplus_ad} that WinCLIP+ (Section~\ref{sec:winclip+}) combines different sources of information to define its anomaly scores, both in AC and AS. 
We ablate on different factors to define WinCLIP+ scores for AC \eqref{eq:winplus_ad} and AS \eqref{eq:winplus_al} respectively. For AC, from Table~\ref{tab:winclipp_ad}, we clearly remark the effectiveness of $\mathrm{ascore}_0$ upon $\max \mathbf{M}^{\tt W}$. Interestingly, we observe $\mathrm{ascore}_0$ is beneficial even in higher-shot regimes where $\max \mathbf{M}^{\tt W}$ can be better, confirming their complementary effects. For AS, in Table~\ref{tab:winclipp_al}, we notice the effect of adding $\mathbf{M}_{\tt m}^{\tt W}$ (or $\mathbf{M}_{\tt s}^{\tt W}$) upon $\mathbf{M}^{\tt P}$, \ie the prediction from WinCLIP features: apart from the good performance of $\mathbf{M}^{\tt P}$, $\mathbf{M}^{\tt W}$ could still provide useful information from its local-awareness.

\vspace{0.05in}
\noindent\textbf{WinCLIP with task-specific defects: } 
As mentioned in Section~\ref{sec:zeroshot}, besides using the generic state words and templates (Fig.~6 of supplementary) to cover common cases, our compositional prompt ensemble also supports task-specific state words, \eg ``missing part'' on PCB/``burnt'' pipe fryum; both VisA and MVTec-AD release specific defect types. Ablation study in Table \ref{ablate:context_words} shows that specific state words further improve zero-shot classification in VisA by $0.8\%$ average AUROC with $5.3\%$ gain on the challenging PCB2. 

% \vspace{0.05in}
% \noindent\textbf{WinCLIP+ for AS. }


% 1. Different CLIP.ViT backbones;
% % 2. Compositional Prompt ens v.s. prompt ens v.s. label word ens.
% % 3. Multi-Scale WinCLIP
% % 4. WindowMemories v.s. WindowMemories + PatchMemories
% 5. PatchMemories with ImageNet pre-trained ViT models;
% 6. Window size.;
% % 7. Inference time v.s. other methods
% 8. Training(adaptation) time;
% \eg.;

% \vspace{0.05in}
% \noindent\textbf{Effect of architecture. }

% \subsection{Qualitative results}


\section{Conclusion}

% Modeling the open-ended concept of abnormality 
We propose a novel framework to define normality and anomaly via both fine-grained textual definitions and normal reference images for comprehensive anomaly classification and segmentation. First, we show that the CLIP pre-trained on large-scale web data provides a powerful representation with good alignment between texts and images for anomaly recognition tasks. The compositional prompt ensemble defines the normality and anomaly in text and helps to distill knowledge from the pre-trained CLIP for better zero-shot anomaly recognition. WinCLIP efficiently aggregates multi-scale features with image-text alignment from window and image-level to perform zero-shot segmentation. Moreover, given a few normal samples, vision based reference association provides complementary information about the two states to language definitions, leading to few-shot WinCLIP+. In recent benchmarks, WinCLIP and WinCLIP+ outperform state-of-the-arts in zero-/few-shot setups with considerable margins. We believe our work will bring values complementary to standard one-class methods. For further improvement, vision-language pre-training with industrial domain data is a promising direction that is left as a future work. 
% \newpage
% We introduce a compositional prompt ensemble that ensembles both the label words for states and text templates, which improves zero-shot anomaly classification over the naive CLIP based zero-shot classification. By aggregating multi-scale spatial features from the pre-trained CLIP visual encoder in window and image levels, WinCLIP is proposed for zero-shot anomaly segmentation. As far as we know, we are the first one to explore language guided zero-shot anomaly classification and segmentation. 
% We propose a reference association method which is applied to multi-scale spatial feature maps for visual based few-shot anomaly segmentation. WinCLIP+ combines the language guided and visual based method for holistic anomaly classification.
%  In MVTec-AD and VisA benchmarks, WinCLIP achieves $91.8\%, 77.6\%/85.1\%, 74.9\%$ ROC-AUC in zero-shot anomaly classification/segmentation while WinCLIP+ achieves $93.1\%, 83.1\%/95.2\%, 95.2\%$ ROC-AUC 1-shot classification/segmentation, surpassing SOTA few-shot methods with large margins.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\appendix
% \onecolumn
\input{appendix}

\end{document}
