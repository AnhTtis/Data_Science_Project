\section{Proofs}
\subsection{Derivation of GSS ELBO}
We provide the proof of Eq.~\eqref{equ:elbo_sem} in the main paper here.
We rewrite the log-likelihood of semantic segmentation $\log{p(c|x)}$ by introducing  a discrete L-dimension latent distribution $q(z|c)$ (with $z\in \mathbb{Z}^L$).
\begin{align*}
    \notag\log{p(c|x)} &= \log{\int p(c,z|x)} \text{\ d}z\\
    \notag &= \log{\int p(c,z|x)} \frac{q(z|c)}{q(z|c)}\text{\ d}z\\
    \notag &=\log\mathbb{E}_{q(z|c)}\left[\frac{p(c,z|x)}{q(z|c)}\right]\\
    \notag &\geq \mathbb{E}_{q(z|c)}\left[\log \frac{p(c,z|x)}{q(z|c)} \right]\\
    \notag \left( \text{as} \right.-log(\cdot)& \text{ is convex, by Jensen's Inequality: }\\
    \notag f(\sum_i\lambda_i x_i)& \leq \sum_i \lambda_if(x_i)\text{, where } \lambda_i\geq 0, \sum_i\lambda_i=1 \left. \right)\\
    \notag  &= \mathbb{E}_{q(z|c)}\left[\log \frac{p(c|z)p(z|x)}{q(z|c)} \right]\\
    \notag  &= \mathbb{E}_{q(z|c)}\left[\log p(c|z) \right] + \mathbb{E}_{q(z|c)}\left[\log \frac{p(z|x)}{q(z|c)} \right]\\
    \notag  = \mathbb{E}_{q(z|c)}&\left[ \log p(c|z) \right] -D_{KL}(q(z|c), p(z|x)) \\
    \notag  = \mathbb{E}_{q_\phi(z|c)}&\left[ \log p_\theta(c|z) \right] -D_{KL}(q_\phi(z|c), p_\psi(z|x)).
\end{align*}

Different from ELBO~\cite{kingma2013auto} in VAE, the latent variable we introduce here is $q(z|c)$, rather than $q(z)$ to solve the conditioned mask generation problem.

\subsection{Derivation of latent posterior learning}
We provide the proof of Eq.~\eqref{eq:posterior} in the main paper here.
As stated in main paper, the first stage latent posterior training is conducted by a MSE loss
\begin{equation*}
    \min_{\theta, \phi}\sum_c \mathbb{E}_{q_\phi(z|c)}\|p_\theta(c|z)-c\|.
\end{equation*}

Let us denote $\hat{c}=p_\theta(c|z)$ is the reconstructed mask.
Then, we define a linear transform $x^{(c)}=\mathcal{X}_\beta(c)=c\beta$, where $\beta\in\mathbb{R}^{K\times 3}$ and an arbitrary inverse transform $\hat{c}=\mathcal{X}^{-1}_\gamma(\hat{x}^{(c)})$.
Noted that the parameter $\gamma$ can be non-linear.
$x^{(c)}$ is called \texttt{maskige} and $\hat{x}^{(c)}$ is the reconstructed \texttt{maskige} produced by the {\tt maskige} decoder $\hat{x}^{(c)}=\mathcal{D}_{{\theta}}(\hat{z})$. 
The transformed latent parameter $\hat{z}$ preserves the probability for the linear transformation,
\input{figures/sup_obj}
\begin{equation}
\label{equ:qhatz}
    q_{\hat{\phi}}(\hat{z}|x^{(c)}) =q_{\hat{\phi}}(\hat{z}|c\beta)= q_\phi(z|c).
\end{equation}
Then, we have
\begin{align}
    \notag &\min_{\theta, \phi}\sum_c \mathbb{E}_{q_\phi(z|c)}2\|\hat{c}-c\|\\
    \notag =&\min_{\theta, \phi, \gamma}\sum_c \mathbb{E}_{q_\phi(z|c)}\left[\|\mathcal{X}^{-1}(\hat{x}^{(c)})-c\| + \|\hat{c}-c\|\right].
\end{align}
For the first term, since $\mathcal{X}^{-1}(\hat{x}^{(c)})=\mathcal{X}^{-1}(\mathcal{D}_{{\theta}}(\hat{z}))$ which is not related to $\theta$ and $\phi$.
Therefore, we have
\begin{align}
    \notag &\min_{\theta, \phi, \gamma}\sum_c \mathbb{E}_{q_\phi(z|c)}\|\mathcal{X}^{-1}(\hat{x}^{(c)})-c\|\\
     \notag=&\min_{\gamma}\sum_c \mathbb{E}_{q_\phi(z|c)}\|\mathcal{X}^{-1}(\mathcal{D}_{\theta}(\hat{z}))-c\|\\
    \label{equ:t1}
    =&\min_{\gamma,\beta}\sum_c \mathbb{E}_{q_{\hat{\phi}}(\hat{z}|x^{(c)})}\|\mathcal{X}^{-1}(\mathcal{D}_{{\theta}}(\hat{z}))-c\|\ (\text{Eq.~\eqref{equ:qhatz}}).
\end{align}
\input{figures/sup_architecture}

For the second term, $\hat{c}=\mathcal{D}_\theta(z)$ is not related to $\gamma$, which can be rewritten as
\begin{align}
    \notag &\min_{\theta, \phi}\sum_c \mathbb{E}_{q_\phi(z|c)}\|\hat{c}-c\|\\
    \notag &=\min_{\theta, \phi, s.t. \|\beta\|=1}\sum_c \mathbb{E}_{q_\phi(z|c)}\|\hat{c}-c\|\|\beta\|\\
    \notag &=\min_{\theta, \phi, s.t. \|\beta\|=1}\sum_c \mathbb{E}_{q_\phi(z|c)}\|\hat{c}\beta-c\beta\|\\
    % \notag &=\min_{\theta, \phi, s.t. \|\beta\|=1}\sum_c \mathbb{E}_{q_\phi(z|c)}\|\hat{c}\beta-\hat{x}^{(c)} + \hat{x}^{(c)} - x^{(c)} + x^{(c)} -c\beta\|\\
    \notag &= \min_{\theta, \phi, s.t. \|\beta\|=1} \sum_c
    \mathbb{E}_{q_\phi(z|c)}\|(\hat{c}\beta-\hat{x}^{(c)}) \ (\text{equal to 0 by def.})\\
    \notag &\quad\quad\quad\quad\quad\quad\quad\quad+ (\hat{x}^{(c)} - x^{(c)})\\
    \notag&\quad\quad\quad\quad\quad\quad\quad\quad+ (x^{(c)} -c\beta)\|\ (\text{equal to 0 by def.})\\
    \notag &=\min_{\theta, \phi, s.t. \|\beta\|=1}\sum_{x^{(c)}} \mathbb{E}_{q_\phi(z|c)} \|\hat{x}^{(c)} - x^{(c)}\|\ (\text{not related to }\theta)\\
    \notag &=\min_{{\theta}, \phi, s.t. \|\beta\|=1}\sum_{x^{(c)}} \mathbb{E}_{q_\phi(z|c)} \|\mathcal{D}_{{\theta}}(\hat{z}) - x^{(c)}\|\\
    \notag &=\min_{{\theta}, \hat{\phi}, \beta, s.t. \|\beta\|=1}\sum_{x^{(c)}} \mathbb{E}_{q_{\hat{\phi}}(\hat{z}|x^{(c)})}
    \label{equ:t2}\|\mathcal{D}_{{\theta}}(\hat{z}) - x^{(c)}\|\ (\text{Eq.~\eqref{equ:qhatz}}).
\end{align}
%
Combining Eq.~\eqref{equ:t1} and Eq.~\eqref{equ:t2}, our final objective is 
\begin{align}
    \notag& \min_{\hat{\phi},{\theta}, \beta, s.t. \|\beta\|=1}\sum_{x^{(c)}} \mathbb{E}_{q_{\hat{\phi}}(\hat{z}|x^{(c)})}\|\mathcal{D}_{{\theta}}(\hat{z})-x^{(c)}\|\\
    + &\quad \min_{\gamma, \beta}\sum_c\mathbb{E}_{q_{\hat{\phi}}(\hat{z}|x^{(c)})}\|\mathcal{X}^{-1}(\mathcal{D}_{{\theta}}(\hat{z}))-c\|.
\end{align}
For the first term, it is a VQVAE~\cite{van2017neural} reconstruction objective for \texttt{maskige}.
Therefore, a VQVAE pretrained by DALL$\cdot$E~\cite{ramesh2021zero} with a large-scale OpenImage dataset can 
readily offer a good 
lower bound for the first term.

As such, only the second term is left for optimization.
We can optimize the $\gamma$ with gradient descent,
corresponding to {\model{}-\gssc{}\&\gsse{}}. 
Besides, we can solve this problem more efficiently by a {linear assumption}, \ie{} $\mathcal{X}^{-1}(\hat{x}^{(c)}) = \hat{x}^{(c)}\gamma$ where $\gamma\in\mathbb{R}^{3\times K}$.
We denote the $\hat{X}^{(c)}$ is a matrix with each row an reconstructed \texttt{maskige} and $C$ is a matrix with each row an input mask.
We solve the optimization with least square error
\begin{align}
    \notag&\|\mathcal{X}^{-1}(\hat{X}^{(c)})-C\|^2\\
    \notag=&\|\hat{X}^{(c)}\gamma-C\|^2\\
    \notag=&\|(\hat{X}^{(c)}-X^{(c)}+X^{(c)})\gamma-C\|^2\\
    \label{equ:opt_gamma_beta_mid}
    \leq&\left(\|\hat{X}^{(c)}-C\beta\|\|\gamma\|+\|X^{(c)}\gamma-C\|\right)^2.
\end{align}
The optimization over both $\beta$ and $\gamma$ is non-convex (as shown by the poor performance with {\model{}-\gsse{}}), so we optimize them sequentially in {\model{}-\gssb{}}\&{\gssc{}\&\gssd{}}.
For {\model{}-\gssb{}}, we use a hand-crafted optimized $\beta$.
\begin{align}
    \notag&\left(\|\hat{X}^{(c)}-C\beta\|\|\gamma\|+\|X^{(c)}\gamma-C\|\right)^2\\
    \notag\leq&\left(\tau\|\gamma\|+\|X^{(c)}\gamma-C\|\right)^2\\
    \notag=&\left(\tau\|\gamma\|+\|C\beta\gamma-C\|\right)^2\\
    \label{equ:opt_gamma_beta_mid2}
    \leq&\left(\tau\|\gamma\|+\|C\|\|\beta\gamma-\mathbb{1}\|\right)^2.
\end{align}
where $\tau=\|\hat{X}^{(c)}-C\beta\|$ is bounded and unrelated to $\gamma$ by provided VQVAE and $\beta$.
Our objective then changes to minimize the upper bound.
\begin{align}
    \notag \min_{\gamma, s.t. \|\gamma\|=1} \text{RSS}(\gamma)&=\min_{\gamma, s.t. \|\gamma\|=1} \|\beta\gamma-\mathbb{1}\|^2\\
    \label{equ:rss}
    &=\min_{\gamma, s.t. \|\gamma\|=1}(\beta\gamma-\mathbb{1})^\top(\beta\gamma-\mathbb{1}).
\end{align}
We take the derivative of Eq.~\eqref{equ:rss}, then
\begin{equation}
\label{equ:drss}
    \frac{\partial \text{RSS}}{\partial \gamma}=2\beta^\top(\beta\gamma-\mathbb{1})=0.
\end{equation}
The unique solution of Eq.~\eqref{equ:drss} is 
\begin{align}
    \notag\beta^\top\beta\gamma&=\beta^\top\\
    \notag(\Rightarrow)\quad (\beta^\top\beta)^{-1}(\beta^\top\beta)\gamma &= (\beta^\top\beta)^{-1}\beta^\top\\
    \label{equ:gamma_mse}
    (\Rightarrow)\quad\quad\quad\quad\quad\quad\quad\ \ \gamma &= (\beta^\top\beta)^{-1}\beta^\top.
\end{align}

For the special design {\model{}-\gssd{}}, we use a cascaded optimization to automatically optimize $\beta$ and $\gamma$.
\begin{align}
    \notag\beta_{t+1} &= \mathop{\arg\min}\limits_{\beta}\left(\|\hat{X}^{(c)}-C\beta\|\|\gamma_t\|+\|C\beta\gamma_t-C\|\right)^2\\
    \notag\gamma_{t+1} &= (\beta_{t+1}^\top\beta_{t+1})^{-1}\beta_{t+1}^\top.
\end{align}
The $\notag\beta_{t+1}$ is optimized by one mini-batch step of gradient descent.

\input{tables/supp_mseg.tex}

\section{{\tt Maskige} optimization designs}

As illustrated above, $\beta$ is a linear projection applied on the ground-truth mask, \ie{} $x^{(c)}=c\beta$.
%
As is shown in Eq.~\eqref{equ:opt_gamma_beta_mid} and Eq.~\eqref{equ:opt_gamma_beta_mid2}, the quality of $\|\hat{X}^{(c)}-C\beta\|$ directly affects the quality of the following $\gamma$ optimization, so the optimization of $\beta$ will affect the difficulty degree of our learning of $\mathcal{X}_\gamma^{-1}$.
%
The cascaded optimization in {\model{}-\gssd{}} (84.37\% reconstruction mIoU) provides an upper-bound for $\beta$ optimization. 
However, the gradient descent optimization costs another $5$ GPU hours according to Table~\ref{tab:vqvae} of main paper.
%
Therefore, we propose a hand-crafted optimization of $\beta$ in {\model-\gssb{}\&\gssc{}\&\gssc{}-W} that achieves satisfying performance without requiring extra training time, based on the \emph{maximal distance assumption}.

To understand this assumption, we can consider the linear projection parameter $\beta \in \mathbb{R}^{K \times 3}$ as a colorization process, where each category is assigned an \texttt{rgb} color. The idea behind the {\em maximal distance assumption} is to maximize the color difference between the encoding of each category. For instance, if two different categories are assigned similar colors, the model may struggle to differentiate between them. Therefore, by maximizing the distance between color embeddings, we can improve the model's ability to distinguish between categories.
%
We interpret the parameter $\beta$ as R, G, B color sequences $\mathcal{A}^r, \mathcal{A}^g, \mathcal{A}^b$ assigned to each category. To better satisfy the \emph{maximal distance assumption}, we will try different ways to construct these sequences, \ie., assigning colors to each category.

\emph{\textbf{(i)} Arithmetic sequence on R/G/B channels}: 
Designing three arithmetic sequences $\mathcal{A}^r, \mathcal{A}^g, \mathcal{A}^b$ for R/G/B channels respectively. Then we have
\begin{equation}
\mathcal{A}^m = \left \{a_1^m, a_2^m, \dots, a_i^m, \dots, a_n^m \right \}, m \in \left \{r, g, b\right \}.
\end{equation}
For the $i$-th color value,
\begin{equation}
      a_i^m = a_1^m + (i-1)\cdot k^m,k^m \in N^+, 
\end{equation}
where color channel $m \in \left \{r, g, b\right \}$, the interval of arithmetic sequence $k^m$ can be difference between channels, $a_1^m$ default is $0$.
The set of colors is the Cartesian product of these three series, 
\begin{equation}
    \mathcal{C} =  \mathcal{A}^r \times \mathcal{A}^g \times \mathcal{A}^b.
\end{equation}
\Eg, if the interval of R, G, B channel $k=45$, the color set $\mathcal{C}$ will be $\left \{(0, 0, 0)\right .$, $(0, 0, 45), \dots,$ $\left .(225, 225, 225) \right \}$.

\emph{\textbf{(ii)} Misalignment start points}: The original starting point of the arithmetic sequence is 0, 0, 0 for R/G/B respectively. In order to avoid duplication of values, we let R/G/B have different starting points, 
\begin{equation}
    a_1^{r}\ne a_1^{g} \ne a_1^{b}.
\end{equation}
In practice, we simply set to $a_1^r=0, a_1^g=1, a_1^b=2$.

\emph{\textbf{(iii)} Random additive factors}: Adding three independent random factors $t \in [0, T]$ on the R/G/B arithmetic sequence respectively, to avoid repetition of several same values,
\begin{equation}
    a_i^m = a_1^m + (i-1)\cdot k^m + t_i^m.
\end{equation}
\Eg, a color sequence with random additive factors: $\left \{(1, 7, 3)\right .$, $(4, 2, 45)$, $\dots$, $\left .(235, 215, 232)\right \}$. In practical terms, $T$ is set to $15$.

\emph{\textbf{(iv)} Category-specific refinement}: We equip the lower IoU categories with values where the R/G/B values vary at large degrees (\eg, we replace $(128, 128, 128)$ with $(0, 128, 255)$). In addition, we keep the color away from gray as possible, because gray is located in the center of the color space, thus being close to many categories and giving rise to a harder learning problem.
Such an category-specific refinement allows each category to
be possibly furthest from the others as possible.

\input{tables/ablation_on_colorization.tex}

\paragraph{Results}

As shown in Table~\ref{tab:supp_colorization},
it is evident that the colorization design for \texttt{maskige} generation
presents a good amount of impact on the reconstruction performance.
In particular, the last design {category-specific refinement}
yields the best results, conforming our intuition and design consideration.

\paragraph{Visualization}
For visual understanding,
in Figure~\ref{fig:supp_color_ade_mda} and Figure~\ref{fig:supp_color_ade_opt} we visualize the 150 colors corresponding to all the categories of ADE20K~\cite{zhou2019semantic} generated by 
the \emph{maximal distance assumption} (hand-designed) and gradient descent optimization (learned), respectively.
We observe that the hand-designed method
produces the colors with enhanced contrast and greater vibrancy.
Instead, the colors learned 
are vibrant for the more frequent categories and relatively dark for the less frequent categories.

\section{Overall architecture}
Following \cite{esser2021taming, ramesh2021zero}, the modeling of latent prior learning is formulated by an encoder-decoder architecture (See Figure~\ref{fig:sup_arch}).
%
For the image encoder $\mathcal{I}_\psi$, we take the advantage of hierarchical shifted window transformer~\cite{liu2021swin}
for extracting the multi-scale information~\cite{wang2021pyramid} and memory efficiency~\cite{lu2021soft}.
This is different from UViM \cite{kolesnikov2022uvim}, which uses a single-scale and full-range Transformer as the encoder.
To implement the image encoder, we use the Swin-Large architecture \cite{liu2021swin}, pre-trained on ImageNet-22K \cite{deng2009imagenet}, as the backbone.
As shown in Figure~\ref{fig:sup_arch}, we use four-scale feature maps ($1/4$, $1/8$, $1/16$, $1/32$) and upsample all the lower-resolution features to $1/4$ scale, then concatenate four features across the channel \cite{xie2021segformer}.
The multi-level aggregation consists of an MLP and $D$ layers of hierarchical shifted-window Transformer \cite{liu2021swin}, with the swin window size set to 7, the number of attention heads to 16, the embedding dimension to 512, and the FFN dimension to 1024. 
For the implementation version with resnet as the backbone, $D=6$. However, for models with strong Swin Transformer backbones, fewer MLA layers are needed, and thus $D=2$.
We implement the {\tt maskige} decoder $\mathcal{D}_\theta$ as a fixed VQVAE decoder \cite{van2017neural}.

\section{More training details}
\noindent \emph{\textbf{(i)} Latent posterior learning}: 
As illustrated before, the latent posterior learning is simplified as:
\begin{equation}
    \min_{\fxpi}\mathbb{E}_{q_{\hat{\phi}}(\hat{z}|\mathcal{X}(c))}\|\mathcal{X}^{-1}(\hat{x}^{(c)})-c\|.
\end{equation} 
The target can be interpreted as minimizing the distance between a ground-truth segmentation mask and the predicted mask.
Following \cite{kolesnikov2022uvim, chen2022generalist}, we use cross-entropy loss instead of euclidean distance for a better minimization between segmentation masks.

\noindent \emph{\textbf{(ii)} Latent posterior learning for $\fx$}: For GSS variants whose $\mathcal{X}$ dose not require training, such as {\model{}-\gssb{}}\&{\gssc{}}\&{\gssc{}-W}, we assign a 3-channel encoding to each category directly based on the {\em maximum distance assumption}. 
For the GSS variants that require training, including {\model{}-\gssd{}}\&{\gssd{}}\&, we freeze the parameters of the VQVAE of the DALL-E pretrain and train $\fx$ for 4,000 iterations using \texttt{SGD} optimizer with a batch size of 16. 
By the way, the training process of \model{}-\gsse{} also optimizes the $\fxpi$ function.

\noindent \emph{\textbf{(iii)}~Latent posterior learning for $\fxpi$}: 
We propose a method for training an $\fxpi$ that is more robust to noise (used in {\model{}-\gssc{}}\&{\gssc{}-W}). We found that training $\fxpi$ with \textit{noisy \texttt{maskige}} helps it learn to be more robust. To provide noisy {\tt maskige}, we can use the trained $\mathcal{I}_\psi$, DALL$\cdot$E pre-trained $\mathcal{D}_\theta$, and $\fxpi$ to directly predict noisy {\tt maskige} predictions. In practice, we use $\mathcal{I}_\psi$ that trained up to the middle checkpoint (\eg, 32,000 iterations) or final checkpoint in latent prior learning. $\fxpi$ is trained using cross-entropy loss and optimized with \texttt{AdamW}, with a batch size of 16. We trained \model{}-{\gssc{}-W} for 40,000 iterations, while \model{}-\gssc{} was trained for only 3,000 iterations due to its fast convergence.
The non-linear function $\fxpi$ is implemented using either a convolutional or Swin block structure. 
Specifically, for \model{}-\gssc{}, the structure comprises two conv $1 \times 1$ layers enclosing a conv $3 \times 3$ layer. 
However, this approach is superseded by \model{}-\gssc{}-W, the final model, which employs a group of Swin blocks with a number of heads of 4, a Swin window size of 7, and an embedding channel of 128 to realize $\fxpi$. 
Regardless of the specific implementation, $\fxpi$ relies on local RGB information in the predicted mask to deduce the category of each pixel.

\noindent \emph{\textbf{(iv)}~Latent prior learning}:
For the optimization of~~$\mathcal{I}_\psi$, we use the AdamW optimizer and implement a polynomial learning rate decay schedule\cite{zhao2017pyramid} with a minimum learning rate of $0.0$. We set the initial learning rate to $1.5 \times 10^{-3}$ for Cityscapes and $1.2 \times 10^{-4}$ for ADE20K and Mseg.

\section{Domain generic \texttt{maskige} and image encoder}
We did two tests by deriving a {\bf \em general maskige} on MSeg~\cite{lambert2020mseg}.

\noindent \text{\textbf{(i)}}
As shown in Table~\ref{tab:share_maskiage_between_cityscapes_and_mseg}, we applied our general \texttt{maskige} to the Cityscapes dataset and achieved a mIoU score of 79.5, which is only slightly lower than the mIoU score of 80.5 obtained using the Cityspaces specific \texttt{maskige}. This result demonstrates the versatility of our \texttt{maskige} across different datasets.

\noindent \text{\textbf{(ii)}} To further evaluate the effectiveness of our domain-generic approach, we shared the image encoder $\mathcal{I}_\psi$ between MSeg and Cityscapes and trained our model on the training split of MSeg. We then evaluated the model on the {\bf \em zero-shot} test split consisting of 6 unseen datasets. As shown in Table~\ref{tab:mseg}, our GSS outperforms other state-of-the-art methods on the MSeg dataset. These experiments demonstrate that our \texttt{maskige} is domain-generic and has the potential for open-world settings.

\section{Additional quantitative results}
We additionally compare the improved versions of MSeg~\cite{lambert2020mseg} with 1,500k longer training on the cross-domain benchmark.
As shown in Table~\ref{tab:supp_mseg},
despite using short training, our model still achieves 
better performance.
This verifies the advantage of our method
in terms of training efficiency, in addition to the accuracy.

\section{Additional qualitative results}
For further qualitative evaluation,
we visualize the prediction results of our GSS on both single-domain segmentation datasets~\cite{cordts2016cityscapes,zhou2019semantic} and cross-domain segmentation dataset~\cite{lambert2020mseg}.

As shown in Figure~\ref{fig:supp_visualization_stage_2_cityscapes}, our \model{} has an accurate perception of buses, trucks and pedestrians in distance, whilst also splitting the dense and slim poles. 
In Figure~\ref{fig:supp_visualization_stage_2_ade20k}, we see that \model{} correctly recognises a wide range of furniture such as curtains, cabinets, murals, doors and toilets;
This suggests that our {\texttt maskige} generative approach can accurately represent a wide range of semantic entities. 
Figure~\ref{fig:supp_visualization_stage_2_mseg_main} and Figure~\ref{fig:supp_visualization_stage_2_meg_kitti} show the cross-domain segmentation performance
on images from previously unseen domains (Mseg {\tt test} datasets). 
It can be seen that \model{} performs well in all five datasets in the MSeg {\tt test} split~\cite{lambert2020mseg}, further validating that our generative algorithm has strong cross-domain generalization capabilities.

\section{Reproduced semantic segmentation version of UViM~\cite{kolesnikov2022uvim}}
We reproduce UViM with {\em mmsegmentation} and follow the hyperparameter and structure in the paper~\cite{kolesnikov2022uvim}.
To achieve a fair comparison with our approach, we have made some modifications: 
\textbf{(i)} we implement Swin-Large~\cite{liu2021swin} pretrained on ImageNet 22K~\cite{deng2009imagenet} as the Language model $LM$ as ours; 
\textbf{(ii)} we generate the Guiding code straightforwardly in a non-autoregressive manner; 
\textbf{(iii)} we trained 80k iterations in the first stage of UViM~\cite{kolesnikov2022uvim} and 160k iterations in the second stage. 
These modifications are necessary to ensure \textbf{a fair comparison}.

\section{Societal impact}
Given that the strong cross-domain generalization capability,  we consider our model has the potential to be used in a wide range of visual scenarios. This is desired in practical applications
due to the benefits of reducing the demands of per-domain model training and easier deployment and system management.
This is meaningful and advantageous in both economics and environment. 
%
On the other hand, there exist the potential to spawn abuse of our algorithm and unexpected undesirable uses. 
%
Therefore, it is necessary to strengthen the regulation and supervision of algorithm applications, in order to guarantee that new algorithms including ours can be used safely and responsibly for the good of the humanity and society.

\section{Limitations and Future Work}
While our study represents a significant step forward for generative segmentation, our models still fall short of the performance achieved by top discriminative models. 
One contributing factor is that decision boundaries for generative models are often less precise than those of discriminative models, resulting in less accurate object edges in segmentation. 
Another drawback is that generative models require larger amounts of data to achieve good performance, because discriminative models only learn decision boundaries, while generative models need to learn the distribution of the entire sample space. In our experiments, the performance of MSeg is better compared to Cityscapes and ADE20K, which roughly indicates this point. 

Additionally, since we convert all categories to colors, the color space is limited, and as the number of categories increases, the colors become more crowded. This can lead to confusion when using $\mathcal{X}^{-1}$ to query and predict the closest pre-defined color for each category from {\tt maskige}, especially near object edges. Therefore, it is worth trying to expand this space to higher dimensions.

Looking ahead, there are several avenues for future research in generative semantic segmentation. One promising direction is instance-level segmentation, which would enable more precise identification and separation of individual objects within an image. Additionally, we believe that it would be valuable to explore a unified model that can perform multiple vision tasks, such as segmentation, 2D object detection, depth prediction, 3D detection, and more.

Given that the second stage training of \model{} focuses on latent prior learning, new vision tasks could be inclusively added by incorporating a new posterior distribution of latent variables, without requiring any changes to the model architecture. By pursuing these directions, we believe that significant advances can be made in the field of generative semantic segmentation.

\input{figures/supp_visualization_stage_2_cityscapes.tex}
\input{figures/supp_visualization_stage_2_ade20k.tex}
\input{figures/supp_visualization_stage_2_mseg.tex}
\input{figures/supp_visualization_stage_2_mseg_kitti.tex}
\input{figures/color_ade_mda.tex}
\input{figures/color_ade_opt.tex}
