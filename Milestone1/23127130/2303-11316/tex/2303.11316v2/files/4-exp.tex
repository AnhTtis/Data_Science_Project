\section{Experiment}
\subsection{Experimental setup}
\label{sec:set_up}
\paragraph{Cityscapes~\cite{cordts2016cityscapes}}
provides pixel-level annotations for 19 object categories in urban scene images at a high resolution of $2048 \times 1024$.
It contains 5000 finely annotated images, split into 2975, 500 and 1525 images for training, validation and testing respectively.
\paragraph{ADE20K~\cite{zhou2019semantic}}
is a challenging benchmark for scene parsing with 150 fine-grained semantic categories.
It has 20210, 2000 and 3352 images for training, validation and testing.

\paragraph{MSeg~\cite{lambert2020mseg}} is a composite dataset that unifies multiple semantic segmentation datasets from different domains. In particular, the taxonomy and pixel-level annotations are aligned by relabeling more than 220,000 object masks in over 80,000 images. We follow the standard setting: the train split~\cite{lin2014microsoft, caesar2018coco, chen2014semantic, cordts2016cityscapes, neuhold2017mapillary, varma2019idd, yu2020bdd100k, song2015sun} for training a unified semantic segmentation model,
the test split (unseen to model training)~\cite{everingham2010pascal, mottaghi2014role,brostow2009semantic, geiger2013vision, dai2017scannet, zendel2018wilddash} for cross-domain validation.

\paragraph*{Evaluation metrics}
The mean Intersection over Union (mIoU) and pixel-level accuracy (mAcc) are reported for all categories, following the standard evaluation protocol~\cite{cordts2016cityscapes}.


\paragraph{Implementation details}
We operate all experiments on {\em mmsegmentation} \cite{contributors2020openmmlab} with 8 NVIDIA A6000 cores. \emph{(i)~Data augmentation}: 
Images are resized to $1024 \times 2048$ on Cityscapes, $512 \times 2048$ on ADE20K and MSeg, and random cropped ($768 \times 768$ on cityscapes and $512\times512$ on ADE20K and MSeg) and random horizontal flipped during training.
\textbf{No test time augmentation} is applied. \emph{(ii)~Training schedule for latent prior learning}: 
The batch size is 16 on Cityscapes and MSeg and 32 on ADE20K. The total number of iterations is 80,000 on Cityscapes and 160,000 on ADE20K and MSeg.

\input{figures/unlabel_pixel_handling.tex}
\input{tables/ablation_vqvae}
\input{tables/ablation_vqvae_strcture}
\input{tables/ablation_transformer}
\input{tables/cityscapes}
\input{tables/ade20k}
\input{tables/mseg}
\input{tables/share_maskiage_between_cityscapes_and_mseg}
\subsection{Ablation studies}
\label{sec:ablation}
{\noindent\bf Latent posterior learning}
We evaluate the variants of latent posterior learning as described in Section \ref{sec:maskige}.
We observe from Table~\ref{tab:vqvae} that: 
\textbf{(i)} \model{}-\gssb{} comes with no extra training cost. 
Whilst UViM consumes nearly 2K GPU hours for training a VQVAE with similar performance achieved~\cite{kolesnikov2022uvim}.
\textbf{(ii)} The randomly initialized $\beta$ (\ie\ \model{}-\gssa{}) leads to considerable degradation on the least square optimization.
\textbf{(iii)} With our {\em maximal distance assumption}, the regularized $\beta$ of \model{}-\gssb{} brings clear improvement,
suggesting the significance of initialization and its efficacy of our strategy.
\textbf{(iv)} With three-layer conv network with activation function for $\fxpi$, \model{}-\gssc{} achieves good reconstruction.
Further equipping with a two-layer Shifted Window Transformer block~\cite{liu2021swin} for $\mathcal{X}^{-1}$ (\ie\ {\model{}-\gssc{}-W}) leads to the best result at a cost of extra 329.5 GPU hours.
%
This is due to more accurate translation from predicted {\tt maskige} to segmentation mask.
%
\textbf{(v)} Interestingly, with automatic $\beta$ optimization,
\model{}-\gssd{} brings no benefit over \model{}-\gssb{}.
\textbf{(vi)} 
Further, joint optimization of both $\fx$ and $\fxpi$ (\ie\ \model{}-\gsse{}) fails to achieve the best performance.
\textbf{(vii)} In conclusion, \model{}-\gssb{} is most efficient with reasonable accuracy,
whilst {\model{}-\gssc{}-W} is strongest with good efficiency.

{\noindent\bf VQVAE design}
We examine the effect of VQVAE in the context of \texttt{maskige}.
%
We compare three designs:
{\bf(1)} {\em UViM-style} \cite{kolesnikov2022uvim}: Using images as auxiliary input to reconstruct a segmentation mask in form of $K$-channels ($K$ is the class number) in a ViT architecture.
In this no {\tt maskige} case, the size of segmentation mask 
may vary across different datasets, leading to a need for dataset-specific training.
%
This scheme is thus more expensive in compute.
%
{\bf (2)} {\em VQGAN-style} \cite{esser2021taming}: 
Using a CNN model for reconstructing natural images ({\tt maskige} needed for segmentation mask reconstruction) or $K$-channel segmentation masks (no {\tt maskige} case) separately,
both optimized in 
generative adversarial training manner with a smaller codebook.
%
{\bf (3)} {\em DALL$\cdot$E-style} \cite{ramesh2021zero}:
The one we adopt, as discussed earlier.
%
We observe from Table~\ref{tab:vqvae_struct} that:
\textbf{(i)} 
Due to the need for dataset specific training,
UViM-style is indeed more costly than the others.
%
This issue can be well mitigated by our \texttt{maskige} with 
the first stage training cost compressed dramatically,
as evidenced by DALL$\cdot$E-style and VQGAN-style.
%
Further, the inferiority of UViM over DALL$\cdot$E
suggests that our \texttt{maskige} is a favored strategy 
than feeding image as auxiliary input.
%
\textbf{(ii)} 
In conclusion, using our \texttt{maskige} and DALL$\cdot$E pretrained VQVAE yields the best performance in terms of both accuracy and efficiency.

{\noindent\bf Latent prior learning}
We ablate the second training stage for learning latent joint prior.
The baseline is 
% Our baseline is chosen as 
{\model{}-\gssb{}} without the unlabeled area auxiliary and Multi-Level Aggregation (MLA, including a 2-layer Swin block~\cite{liu2021swin}), under 1/8 downsample ratio.
We observe from Table~\ref{tab:transformer} that: 
\textbf{(i)} 
Our unlabeled area auxiliary boosts the accuracy by $3.1\%$,
 suggesting the importance of complete labeling which however is extremely costly
 in semantic segmentation.
%
\textbf{(ii)} 
Increasing the discrete mask representation resolution is slightly useful. 
\textbf{(iii)} 
The MLA plays another important role,
\eg\ giving a gain of $2.3\%$.

\input{figures/visualization_stage_1.tex}
\input{figures/visualization_stage_2.tex}
% \vspace{-0.2em}
\subsection{Single-domain semantic segmentation}
% \vspace{-0.1em}
We compare our \model{} with prior art discriminative methods and the latest generative model (UViM~\cite{kolesnikov2022uvim}, a replicated version for semantic segmentation task).
We report the results in Table~\ref{tab:cityscapes_val}
for Cityscapes~\cite{cordts2016cityscapes} and
Table~\ref{tab:ade20k_val} for ADE20K~\cite{zhou2019semantic}. 
\emph{\textbf{(i)} In comparison to discriminative methods}:
Our \model{} yields competitive performance with either Transformers (Swin) or CNNs (\eg\ ResNet-101).
For example, under the same setting, \model{} matches the result of Maskformer~\cite{cheng2021per}.
Also, {\model{}-\gssc{}-W} is competitive to the Transformer-based SETR~\cite{zheng2021rethinking} on both datasets.
\emph{\textbf{(ii)} In comparison to generative methods}: 
% Under fair comparison, 
\model{}-\gssb{} surpasses UViM~\cite{kolesnikov2022uvim} by a large margin
whilst enjoying higher training efficiency.
Specifically, UViM takes 1,900 TPU-v3 hours for the first training stage and 900 TPU-v3 hours for the second stage.
While the first stage takes only 329.5 GPU hours with \model{}-\gssc{}-W, and zero time with \model{}-\gssb{}. The second stage of \model{}-\gssb{} requires approximately 680 GPU hours. 
%
This achievement is 
due to our \texttt{maskige} mechanism for enabling the use of pretrained data representation and a series of novel designs for joint probability distribution modeling.

\subsection{Cross-domain semantic segmentation}
We evaluate cross-domain zero-shot benchmark~\cite{lambert2020mseg}. 
We compare the proposed \model{} with MSeg~\cite{lambert2020mseg}, a domain generalization algorithm (CCSA)~\cite{motiian2017unified} and a multi-task learning algorithm (MGDA)~\cite{sener2018multi}.
We test both HRNet-W48~\cite{sun2019high} and Swin-Large~\cite{liu2021swin} as backbone. 
As shown in Table~\ref{tab:mseg},
our \model{} is superior to all competitors using either backbone.
%
This suggests that generative learning could achieve more domain-generic representation than conventional discriminative learning counterparts.

\paragraph{Domain generic \texttt{maskige}}
Being independent to the visual appearance of images, \texttt{maskige} is intrinsically domain generic. 
To evaluate this, we transfer the {\tt maskige} from MSeg to Cityscapes.
As shown in Table~\ref{tab:share_maskiage_between_cityscapes_and_mseg}, \model{} can still achieve 79.5 mIoU (1\% drop).
%
In comparison, image representation transfer would double the performance decrease.
\subsection{Qualitative evaluation}
We evaluate the first-stage reconstruction quality of our \model{}, UViM~\cite{kolesnikov2022uvim} and VQGAN~\cite{esser2021taming}. 
As shown in Figure~\ref{fig:visualization_stage_1}, \model{} produces almost error-free reconstruction with clear and precise edges, and UViM fails to recognize some small objects while yielding distorted segmentation. VQGAN~\cite{esser2021taming} achieves better classification accuracy but produces more ambiguous edge segmentation.
As shown in Figure~\ref{fig:visualization_stage_2}, \model{} produces fine edge segmentation for interior furniture divisions on ADE20K~\cite{zhou2019semantic} and accurately segments distant pedestrians and slender poles on Cityscapes~\cite{cordts2016cityscapes}.
