\section{Related work}
{\noindent \bf Semantic segmentation}
Since the inception of FCN~\cite{long2015fully}, semantic segmentation have flourished by various deep neural networks with ability to classify each pixel. 
The follow-up efforts then shift to improve the limited receptive field of these models.
%
For example, PSPNet~\cite{zhao2017pyramid} and DeepLabV2~\cite{chen2014semantic} aggregate multi-scale context between convolution layers.
Sequentially, %attention-embedded CNN models like 
Nonlocal~\cite{wang2018nonlocal}, CCNet~\cite{huang2019ccnet}, and DGMN~\cite{zhang2020dynamic} integrate the attention mechanism in the convolution structure.
Later on, Transformer-based methods (\eg\ SETR~\cite{zheng2021rethinking} and Segformer~\cite{xie2021segformer}) are proposed following the introduction of Vision Transformers.
More recently, MaskFormer~\cite{cheng2021per} and Mask2Former~\cite{cheng2022masked} realize semantic segmentation with bipartite matching. 
Commonly, all the methods adopt the discriminative pixel-wise classification learning paradigm.
This is in contrast to our generative semantic segmentation.

{\noindent \bf Image generation}
In parallel, generative models \cite{esser2021taming, ramesh2021zero} also excel. They are often optimized in a two-stage training process: 
(1) Learning data representation in the first stage and 
(2) building a probabilistic model of the encoding in the second stage.
%
For learning data representation, VAE~\cite{kingma2013auto} reformulates the autoencoder by variational inference.
GAN~\cite{goodfellow2020generative} plays a zero-sum game.
% to learn a data representation.
VQVAE~\cite{van2017neural} extends the image representation learning to discrete spaces, making it possible for language-image cross-model generation.
\cite{larsen2016autoencoding} replaces element-wise errors of VAE with feature-wise errors to capture data distribution.
%
%
For probabilistic model learning, some works \cite{rombach2020making, esser2020disentangling, xiao2019generative} use flow for joint probability learning.
Leveraging the Transformers to model the composition between condition and images, Esser et al.~\cite{esser2021taming} demonstrate the significance of
data representation (\ie\ the first stage result) for the challenging high-resolution image synthesis, obtained at high computational cost.
%
This result is inspiring to this work in the sense that the diverse and rich knowledge about data representation achieved in the first stage could be transferable across more tasks such as semantic segmentation.
%

{\noindent \bf Generative models for visual perception}
Image-to-image translation made
one of the earliest attempts in generative segmentation, with far less success in performance~\cite{isola2017image}.
Some good results were achieved
in limited scenarios such as face parts segmentation and Chest X-ray segmentation~\cite{li2021semantic}.
%
Replacing the discriminative classifier with a generative Gaussian Mixture model, GMMSeg~\cite{liang2022gmmseg} is claimed as generative segmentation,
but the most is still of discriminative modeling.
%
The promising performance of Pix2Seq~\cite{chen2021pix2seq} on several vision tasks leads to the prevalence of sequence-to-sequence task-agnostic vision frameworks.
For example,
Unified-I/O~\cite{lu2022unified} supports a variety of vision tasks within a single model by seqentializing each task to sentences.
Pix2Seq-D~\cite{chen2022generalist} deploys a hierarchical VAE (\ie{} diffusion model) to generate panoptic segmentation masks.
This method is inefficient due to the need for iterative denoising.
%
UViM~\cite{kolesnikov2022uvim} realizes its generative panoptic segmentation by introducing latent variable conditioned on input images.
%
It is also computationally heavy due to the need for model training 
from scratch.
%
To address these issues,
we introduce a notion of {\tt maskige} for expressing segmentation masks
in the form of RGB images,
enabling the adopt of off-the-shelf data representation models (\eg\ VGVAE)
already pretrained on vast diverse imagery.
%
This finally allows for generative segmentation model training as efficiently as conventional discriminative counterparts.
