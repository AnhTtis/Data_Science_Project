\begin{figure}[tb]
	\begin{center}
\includegraphics[width=1.0\linewidth]{figures/stage2_drawio.pdf}
	\end{center}
	\vspace{-1em}
	\caption{
\textbf{Illustration of latent prior learning} {(\bf\em top)} and \textbf{generative inference pipeline} {(\bf\em bottom)}.
During training, for latent prior learning, we optimize the image encoder $\mathcal{I}_\psi$ while freezing the {\tt maskige} encoder $\mathcal{E}_\phi$.
The objective is to minimize the divergence (\eg\ cross entropy loss) between the prior distribution and the posterior distribution of latent tokens.
During generative inference, we use the prior $z \sim p(z|x)$ inferred by $\mathcal{I}_\psi$ to generate the \texttt{maskige} with {\tt maskige} decoder $\mathcal{D}_\theta$.
``Pred.'': Predicted.
}
\vspace{-1em}
	\label{fig:stage2}
\end{figure}