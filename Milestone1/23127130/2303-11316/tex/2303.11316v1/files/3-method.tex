\section{Methodology}
\subsection{GSS formulation}

\input{figures/stage1}
\input{figures/stage2}
Traditionally, semantic segmentation is formulated as a discriminative learning problem as
\begin{equation}
\label{equ:dis_ss}
    \max_\pi \log p_\pi(c|x)
\end{equation}
where $x\in \mathbb{R}^{H\times W\times3}$ is an input image, 
$c\in \{0,1\}^{ H\times W\times K}$ is a {\em segmentation mask} in $K$ semantic categories, and $p_\pi$ is a discriminative pixel classifier.
Focusing on learning the classification boundary of input pixels,
this approach enjoys high data and training efficiency~\cite{murphy2012machine}.


In this work, we introduce {\bf\em Generative Semantic Segmentation} (\model) by introducing a discrete $L$-dimension latent distribution $q_\phi(z|c)$ (with $z\in \mathbb{Z}^L$) to the above log-likelihood as:
\begin{align}
\label{eq:elbo2}
    \notag \log p(c|x) \geq \mathbb{E}_{q_\phi(z|c)}\left[
    \log \frac{p(z,c|x)}{q_{\phi}(z|c)}
    \right],
\end{align}
which is known as the Evidence Lower Bound (ELBO)~\cite{kingma2013auto} (details are given in the supplementary material).
Expanding the ELBO gives us
\begin{equation}
\label{equ:elbo_sem}
\mathbb{E}_{ q_{\phi}(z|c)} \left[
    \log p_\theta(c|z)\right] - D_{KL}\Big(q_{\phi}(z|c), p_{\psi}(z|x)\Big),
\end{equation}
where we have three components in our formulation:
% to implement the ELBO:

\noindent \textbullet~$p_\psi$: An \textbf{image encoder} (denoted as $\mathcal{I}_\psi$) that models {\em the prior distribution} of latent tokens $z$ conditioned on the input image $x$.

\noindent \textbullet~$q_\phi$: A function that encodes the semantic segmentation mask $c$ into discrete latent tokens $z$, which includes a \textbf{\texttt{maskige} encoder} (denoted as $\mathcal{E}_\phi$, implemented by a VQVAE encoder~\cite{van2017neural}) and a linear projection (denoted as $\fx$, which will be detailed in Section~\ref{sec:maskige}).

\noindent \textbullet~$p_\theta$: A function that decodes the semantic segmentation mask $c$ from the discrete latent tokens $z$, which includes a 
\textbf{\texttt{maskige} decoder} (denoted $\mathcal{D}_\theta$, implemented by a VQVAE decoder~\cite{van2017neural}) and $\fxpi$ (the inverse process of $\fx$).



\paragraph{Architecture} 
The architecture of \model{} comprises three components: $\mathcal{I}_\psi$, $\mathcal{E}_\phi$ and $\mathcal{D}_\theta$. 
$\mathcal{E}\phi$ and $\mathcal{D}_\theta$ are implemented as VQVAE encoder and decoder~\cite{van2017neural}, respectively. 
Meanwhile, $\mathcal{I}_\psi$ is composed of an image backbone R(\eg\ esNet~\cite{he2016deep} or Swin Transformer~\cite{liu2021swin}) and a \emph{{M}ulti-{L}evel {A}ggregation} (MLA).
As an essential part, MLA is constructed using $D$ shifted window Transformer layers~\cite{liu2021swin} and a linear projection layer. The resulting output is a discrete code $z\in\mathbb{Z}^{H/d\times W/d}$, where $d$ denotes the downsample ratio. 

\paragraph{Optimization} 
Compared to the log-likelihood in discriminative models, optimizing the ELBO of a general model is more challenging \cite{murphy2012machine}.
End-to-end training cannot reach a global optimization.
For Eq. \eqref{equ:elbo_sem}, often we name the first term $\mathbb{E}_{ q_{\phi}(z|c)} \left[\log p_\theta(c|z)\right]$ as a reconstruction term and the second KL-divergence as the prior term.
In the next section we will introduce the optimization of this ELBO.

\subsection{ELBO optimization for semantic segmentation}
The ELBO optimization process for semantic segmentation involves two main steps, as described in \cite{ramesh2021zero}.
%
The first step is {\bf \em latent posterior learning}, also known as reconstruction (see Figure~\ref{fig:stage1}). Here, the ELBO is optimized with respect to $\theta$ and $\phi$ through training a VQVAE~\cite{van2017neural} to reconstruct the desired segmentation masks.
The second step is {\bf \em latent prior learning} (see Figure~\ref{fig:stage2}). Once $\theta$ and $\phi$ are fixed, an image encoder $\psi$ is optimized to learn the prior distribution of latent tokens given an input image.

Typically, the first stage of ELBO optimization is
both most important and most expensive (much more than many discriminative learning counterparts) \cite{esser2021taming}.
%
To address this challenge, we propose an efficient latent posterior learning process.


\subsection{Stage I: Efficient latent posterior learning}
\label{sec:maskige}
During the first stage (shown in Figure~\ref{fig:stage1}), the initial prior $p_\psi(z|x)$ is set as the uniform distribution.
Conventionally, the first stage latent posterior training is conducted by
\begin{equation}
\label{equ:opt_stage1}
    \min_{\theta, \phi} \mathbb{E}_{q_\phi(z|c)}\|p_\theta(c|z)-c\|.
\end{equation}
To optimize Eq.~\eqref{equ:opt_stage1} more efficiently, we introduce a random variable transformation $\fx: \mathbb{R}^{K} \to \mathbb{R}^{3}$, its pseudo-inverse function $\fxpi : \mathbb{R}^{3} \to \mathbb{R}^{K}$, and the \texttt{maskige} decoder $\mathcal{D}_\theta$.
Applying expectation with transformed random variable, we have
\begin{align}
    \notag&\quad \min_{\hat{\phi},\hat{\theta}} \mathbb{E}_{q_{\hat{\phi}}(\hat{z}|\mathcal{X}(c))}\|\mathcal{D}_{\hat{\theta}}(\hat{z})-\mathcal{X}(c)\|\\
    &+\min_{\fxpi}\mathbb{E}_{q_{\hat{\phi}}(\hat{z}|\mathcal{X}(c))}\|\mathcal{X}^{-1}(\mathcal{D}_{\hat{\theta}}(\hat{z}))-c\|.
    \label{eq:posterior}
\end{align}
Please refer to supplementary material for more details.
Then we find that $\mathcal{X}(c) = x^{(c)}\in \mathbb{R}^{ H\times W\times 3}$ can be regarded as a kind of RGB image, where each category is represented by a specific color.
For convenience, we term it \texttt{maskige}. Therefore, the optimization can be rewritten as
\begin{align}
    \notag&\quad \min_{\hat{\phi},\hat{\theta}} \mathbb{E}_{q_{\hat{\phi}}(\hat{z}|x^{c})}\|\mathcal{D}_{\hat{\theta}}(\hat{z})-x^{(c)}\|\\
    \label{equ:opt_stage1_1}&+\min_{\fxpi}\mathbb{E}_{q_{\hat{\phi}}(\hat{z}|\mathcal{X}(c))}\|\mathcal{X}^{-1}(\hat{x}^{(c)})-c\|,
\end{align}
where $\hat{x}^{(c)}=\mathcal{D}_{\hat{\theta}}(\hat{z})$.
Now, the first term of Eq.~\eqref{equ:opt_stage1_1} can be regarded as an image reconstruction task (see Figure~\ref{fig:stage1}).
In practice, this has been already well optimized by \cite{esser2021taming, ramesh2021zero} using million-scale datasets.
%
This allows us to directly utilize 
the off-the-shelf pretrained models.
As such, our optimization problem can be simplified as:
\begin{equation}
\label{equ:opt_stage1_2}
    \min_{\fxpi}\mathbb{E}_{q_{\hat{\phi}}(\hat{z}|\mathcal{X}(c))}\|\mathcal{X}^{-1}(\hat{x}^{(c)})-c\|.
\end{equation}
Note that the parameters (0.9K$\sim$466.7K in our designs) of $\mathcal{X}$ and $\mathcal{X}^{-1}$ are far less than $\theta, \phi$ (totally 29.1M parameters with the VQVAE from~\cite{ramesh2021zero}), thus more efficient and cheaper to train.
%
Concretely, we only optimize the small $\mathcal{X}$ while
freezing $\theta,\phi$. 
Following \cite{kolesnikov2022uvim, chen2022generalist}, we use cross-entropy loss instead of MSE loss in Eq.~\eqref{equ:opt_stage1_2} for a better minimization between segmentation masks.


\paragraph{Linear maskige designs}
The optimization problem for $\fx$ and $\fxpi$ is non-convex, making their joint optimization challenging.
To overcome this issue, we optimize $\fx$ and $\fxpi$ separately.
%
For simplicity, we model both $\fx$ and $\fxpi$ as linear functions ({\em linear assumption}).
%
Specifically, we set $x^{(c)} = c\beta$ where $\beta\in \mathbb{R}^{K\times 3}$, and $\hat{c} = \hat{x}^{(c)}\beta^\dag$ where $\beta^\dag\in \mathbb{R}^{3\times K}$.
%
Under the {linear assumption}, we can transfer Eq.~\eqref{equ:opt_stage1_2} into a least squares problem with an explicit solution $\beta^\dag = \beta^\top(\beta\beta^\top)^{-1}$, so that $\mathcal{X}^{-1}$ is free of training.

To enable zero-cost training of $\fx$, we can also manually set the value of $\beta$ properly.
We suggest a {\bf \em maximal distance assumption} for selecting the value of $\beta$ to encourage the encoding of $K$ categories to be as widely dispersed as possible in the three-dimensional Euclidean space $\mathbb{R}^{3}$. More details are provided in the supplementary material.

\paragraph{Non-linear maskige designs}
For more generic design, 
non-linear models (\eg\ CNNs or Transformers) can be also used 
to express $\fxpi$. ({\em non-linear assumption})

\paragraph{Concrete maskige designs}
We implement four optimization settings for $\fx$ and/or $\fxpi$
with varying training budgets.
We define the naming convention of ``\model{}-[F/T]~[F/T]~(-$O$)'' in the following rules. 
{\em \textbf{(i)} Basic settings} ``-[F/T]~[F/T]'' on whether $\fx$ and/or $\fxpi$ require training: ``F" stands for {\em{}F}{ree of training}, and ``T'' for {\em{}T}{raining required}.
{\em \textbf{(ii)} Optional settings} ``-O'' (\eg\ ``R" or ``W") which will be explained later on. All \model{} variants are described below.

\noindent \textbullet~\textbf{\model{}-\gssb{}} (training free): Modeling both $\fx$ and $\fxpi$ using linear functions, with $\beta$ initialized under {\em maximal distance assumption}, and $\beta^\dag$ optimized using least squares. 
For comparison, we will experiment with \textbf{\model-\gssa{}}, where $\beta$ is \textbf{R}andomly initialized. %(Free of training)

\noindent \textbullet~\textbf{\model{}-\gssc{}} (training required): Modeling $\fx$ using a linear function but modeling $\fxpi$ using a non-linear function (\eg\ a three-layer convolutional neural network). We initialize $\beta$ with {\em maximal distance assumption} and optimize $\fxpi$ with gradient descent. %To push the performance, we test 
A stronger design \textbf{\model{}-\gssc{}-W} utilizes a single-layer Shifted \textbf{W}indow Transformer block~\cite{liu2021swin} as the non-linear $\fxpi$. Notably, we train $\fx$ and $\fxpi$ separately, as described in Section~\ref{sec:set_up}. %(Training required)

\noindent \textbullet~\textbf{\model{}-\gssd{}} (training required): Modeling both $\fx$ and $\fxpi$ using linear functions. We train $\beta$ with gradient descent and optimize $\beta^\dag$ with least squares according to $\beta$. %(Training required)

\noindent \textbullet~\textbf{\model{}-\gsse{}} (training required): Modeling $\fx$ using a linear function but modeling $\fxpi$ using a non-linear function (\eg\ a three-layer CNN). We jointly train both functions using gradient descent. %(Training required)

To perform end-to-end optimization of the $\fx$ function using gradient descent for both \model{}-\gssd{}\&\gsse{}, %rely on 
a hard Gumbel-softmax relaxation technique~\cite{maddison2016concrete} is used.
% for VQVAE. 
This involves computing the \texttt{argmax} operation during the forward step, while broadcasting the gradients during the backward step.
%
Our {\em linear} designs (\ie\ \model{}-\gssb{}\&\gssa{})
is training free with zero cost. 
%
Our {non-linear assumption} based designs (\eg\ \model{}-\gssc{}\&\gssc{}-W) has high performance potential at acceptable training cost (see Section~\ref{sec:ablation}).




\subsection{Stage II: Latent prior learning}
We show in Figure~\ref{fig:stage2} {(\bf \em Top)} the latent prior learning.
In this stage, we learn the prior joint distribution between mask latent representation $z$ and images $x$, with $\phi, \theta$ both fixed. 


\paragraph{Objective} The optimization target of this stage is the second term of Eq.~\eqref{equ:elbo_sem}:
\begin{equation*}
    \min_\psi D_{KL}\Big(q_{\phi}(z|c), p_{\psi}(z|x)\Big),
\end{equation*}
where $z$ is in a discrete space of codebook-sized (\eg\ 8192 in \cite{ramesh2021zero}) integers.
% in our implementation.
The objective is to minimize the distance between the discrete distribution of $z$ predicted by latent prior encoder $p_\psi$ and the $z$ given by VQVAE.
Since the entropy of $q_\phi$ is fixed (\ie the ground truth), %and will not be optimized, 
we can use the cross-entropy function to measure their alignment.


\paragraph{Unlabeled area auxiliary}
Due to high labeling cost and challenge, it is often the case that a fraction of areas per image are unlabeled (\ie\ unknown/missing labels).
Modeling per-pixel conditional probability $p(c|x)$ in existing discriminative models,
this issue can be simply tackled by ignoring all unlabeled pixels during training.


In contrast, generative models (\eg\ UViM~\cite{kolesnikov2022uvim} and our \model{}) are trained at the latent token level, without flexible access to individual pixels. 
%
As a result, unlabeled pixels bring about extra challenges,
as they can be of objects/stuff of any categories heterogeneously.
%
Without proper handling, a generative model may learn to classify difficult pixels as the unlabelled and hurting the final performance (see Figure~\ref{fig:unlabel_area_auxiliary}).

To address this problem, we exploit a pseudo labeling strategy. 
The idea is to predict a label for each unlabeled pixel.
%
Specifically, we further introduce an auxiliary head $p_\xi(\bar{c}|z)$ during latent prior learning (\ie\ state II)
to label all unlabeled areas.
Formally, we form an enhanced ground-truth mask
by $\tilde{c}=M_u\cdot \bar{c} + (1-M_u)\cdot c$ where $M_u$ masks out labeled pixels, $\bar{c}$ denotes the pseudo labels, and $\tilde{c}$ denotes the labels after composition.
The training objective of this stage cane be then revised as:
\begin{equation}
\label{equ:opt_unlabel}
    \min_\psi D_{KL}\left(q_{\phi}(z|c), p_{\psi}(z|x)\right) + p_\xi(\bar{c}|z).
\end{equation}

\subsection{Generative inference}
As illustrated in Figure~\ref{fig:stage2} {\bf\em(bottom)}, we first take the latent tokens $z$ that are predicted by the image encoder $\mathcal{I}_\psi$, and feed them into the {\tt maskige} decoder $\mathcal{D}_\theta$ to generate the predicted \texttt{maskige} $\hat{x}^{(c)}$. Next, we apply the inverse transformation $\mathcal{X}^{-1}$ (Section~\ref{sec:maskige}) to the predicted \texttt{maskige} to obtain the final segmentation mask $\hat{c}$.
