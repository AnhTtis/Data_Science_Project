\section{Related Work}
\label{sec:relate}
\noindent \textbf{Graph-based Collaborative Filtering}
Inspired by the success of GNNs, a lot of research works have designed various graph neural architectures to build collaborative recommender systems~\cite{wu2020graph, gao2021graphrec}. For example, to model user-item interactions graph, many efforts have been devoted to developing powerful GNN models for message passing, \eg~NGCF~\cite{wang2019neural}, {STGCN}~\cite{zhang2019star} and {GCMC}~\cite{berg2017graph}. GCCF~\cite{chen2020revisiting} and LightGCN~\cite{he2020lightgcn} enrich GNNs in CF by simplifying the GCN architecture.
% ii) \textit{Heterogeneous CF}. This research line incorporates supplementary relational data into CF, to enhance user and item embedding learning in variaous scenarios, including knowledge graph-enhanced CF (\eg~KGAT~\cite{wang2019kgat} and KGNN-LS~\cite{huang2021knowledge}), multi-behavior recommendation (\eg~MBGCN~\cite{jin2020multi} and KHGT~\cite{xia2021knowledge}), social recommendation (\eg~GraphRec~\cite{fan2019graph} and KGCN~\cite{huang2021knowledge}).
% iii) \textit{Sequential CF}. GNNs have also been employed to model the dynamics in users' interaction sequences, \eg~MAGNN~\cite{ma2020memory}, HyperRec~\cite{wang2020next} and MTD~\cite{huang2021graph}.
To increase model scalability and prevent over-smoothing in making recommendations, our \model\ abandons graph encoders in the inference model, and conducts soft embedding smoothing by distilling useful knowledge from the GNN-based teacher model. \\\vspace{-0.12in}

\noindent\textbf{Self-Supervised Learning for Recommendation}.
To address the noise and sparsity issue in recommendation, recent works propose to employ various types of SSL techniques for data augmentation~\cite{yu2021self,wei2022contrastive,xia2022self,chen2023heterogeneous}. For example, SGL~\cite{wu2021self} and SimGCL~\cite{yu2022graph} conduct random perturbation to augment additional views for CL. HCCF~\cite{xia2022hypergraph} and NCL~\cite{lin2022improving} introduce global views to generate semantically related pairs for CL.
% MHCN~\cite{yu2021self} and S3-Rec~\cite{zhou2020s3} conduct mutual information maximization to supplement the recommendation task. 
% SHT~\cite{xia2022self} proposes a denoising SSL task by learning global relations. 
Although the approaches mentioned above have helped to mitigate the issues caused by noisy and sparse data in recommendation, they still heavily rely on graph neural networks (GNNs) to generate effective embeddings. This often leads to an over-smoothing effect, which can limit the representation ability of current recommendation framework. \\\vspace{-0.12in}

% Though the above approaches greatly relieve the deficiency of the noisy and sparse data in CF, these methods still greatly rely on GNNs to extract effective embeddings. This inevitably causes over-smoothing effect and limit the representation ability of current CF frameworks. Different from the existing works, our \model\ employs CL to effectively filter over-smoothing signals from GNN teachers in our contrastive KD schema.\\\vspace{-0.12in}

\noindent\textbf{Knowledge Distillation for Recommendation}.
Knowledge distillation aims to transfer knowledge from a complex and well-trained teacher model to a simpler student model~\cite{zhanggraph}. It utilizes the predictions of the teacher model to generate informative soft targets for the student model to learn from. In the context of recommender systems, knowledge distillation has been used to develop simpler yet effective models~\cite{kang2020rrd, lee2019collaborative, tang2018ranking, xia2022device}. As examples, Tang~\etal~\cite{tang2018ranking} proposes a method to leverage knowledge distillation for ranking tasks in recommender systems. Xia~\etal~\cite{xia2022device} develop highly-efficient models for on-device recommendations with effective knowledge transferring. Unlike the works that primarily focus on model reduction, our proposed approach, \model, aims to address the over-smoothing issue in state-of-the-art GNN-based CF models. By distilling unbiased signals from GNNs to simple multilayer perceptrons (MLPs), we can reduce the over-smoothing effect and enhance the model representation ability.

% Knowledge distillation aims at transferring knowledge from a complex well-trained teacher model to a simple student model. 
% It utilizes the predictions of the teacher to generate informative soft targets for the student to learn. KD has been applied to produce recommender models that are simple and effective~\cite{kang2020rrd, lee2019collaborative, tang2018ranking, xia2022device}. For example, Tang~\etal~\cite{tang2018ranking} proposes to learn from teachers' top-K list with KD technique.
% % applies KD to top-K recommendation. 
% Xia~\etal~\cite{xia2022device} proposes to learn highly-efficient models for on-devices recommendations. Different from these works that are mainly for model reduction, our \model\ targets the over-smoothing problem in state-of-the-art GNN-based CF frameworks, by distilling unbiased signals from GNNs to simple MLPs. Additionally, our \model\ combines knowledge distillation with contrastive learning, to transfer knowledge in a finer embedding degree.