\appendix \section{Appendix}
\balance
\label{sec:appendix}

\subsection{Learning Algorithm of \model}
\label{sec:learn_alg}
The parameter learning for our \model\ is elaborated in Algorithm~\ref{alg:learn_alg}
\begin{algorithm}[h]
	\caption{Learning Process of \model}
	\label{alg:learn_alg}
	\LinesNumbered
	\KwIn{User-item interaction matrix $\textbf{A}$, loss weights and temperature factors $\lambda_1, \lambda_2, \lambda_3, \lambda_4, \lambda^{(t)}, \tau_1, \tau_2, \tau_3$, learning rate $\eta$, maximum training epochs $E$, number of graph iterations $L$, number of MLP layers $L'$.}
	\KwOut{Trained embeddings $\bar{\textbf{H}}^{(s)}$ and MLP parameters $\textbf{W}$.}
	Initialize model parameters $\bar{\textbf{H}}^{(s)}, \bar{\textbf{H}}^{(t)}, \textbf{W}$\\
	Train the GCN teacher model for well-trained $\bar{\textbf{H}}^{(t)}$ (Eq~\ref{eq:teacher_bpr})\\
	\For{$e=1$ to $E$}{
	    \For{mini-batch $\mathcal{T}_2$ drawn from $\mathcal{E}$} {
    	    Sample a batch of triplet $\mathcal{T}_1$\\
    	    Calculate preference difference $z_{i,j,k}^{(s)}, z_{i,j,k}^{(t)}$ for samples in $\mathcal{T}_1$ (Eq~\ref{eq:difference})\\
    	    Compute loss $\mathcal{L}_1$ for prediction-level KD (Eq~\ref{eq:l1})\\
    	    Calculate loss $\mathcal{L}_2$ for embedding-level KD (Eq~\ref{eq:l2})\\
    	    Calculate the adjustment factor $\omega_i, \omega_j$ for users and items in $\mathcal{T}_2$ (Eq~\ref{eq:omega})\\
    	    Compute loss $\mathcal{L}_3$ for contrastive regularization (Eq~\ref{eq:l3})\\
    	    Calculate $\mathcal{L}_\text{rec}$ for recommendation task\\
    	    Calculate $\mathcal{L}_4$ for weight-decay regularization\\
    	    Calculate overall loss $\mathcal{L}^{(s)}$ for the student (Eq~\ref{eq:overall_loss})\\
    	    \For{each parameter ${\theta}$ in $\{\bar{\textbf{H}}^{(s)},\textbf{W}\}$}{
                ${\theta} = {\theta} - \eta\cdot {\partial \mathcal{L}^{(s)}}/{\partial{\theta}}$;\\
            }
	    }
	}
    \Return all parameters $\bar{\textbf{H}}^{(s)}, \textbf{W}$
    \Description{The algorithm for the learning process of \model.}
\end{algorithm}
\vspace{-0.1in}
\subsection{Implementation Details}
\label{sec:implement}
For fair comparison, we present the hyperparameter settings for implementing the proposed \model\ framework and the baseline methods.
Specifically, our \model\ is implemented with PyTorch, using Adam optimizer and Xavier initializer with default parameters. Training batch size is set as $|\mathcal{T}_1|=100000, |\mathcal{T}_2|=4096$. The dimensionality of embedding vectors is set as $32$. The number of MLP layers is selected from $\{1, 2,3\}$. The number of graph iterations for the teacher model is selected from $\{2, 4, 6\}$. The loss weights $\lambda_1, \lambda_2, \lambda_3$ are tuned from $\{10, 3, 1, 0.3, 0.1, 0.03, 0.01\}$, and the weights $\lambda_4, \lambda^{(t)}$ for weight-decay regularization are tuned from $\{1e^{-3},$ $1e^{-4},$ $1e^{-5},$ $1e^{-6},$ $1e^{-7},$ $1e^{-8}, 0\}$. The temperatures $\tau_1, \tau_2, \tau_3$ are chosen from $\{10, 3, 1, 0.3, 0.1, 0.03, 0.01\}$. Parameter $\varepsilon$ for contrastive regularization adjustment is set as $0.2$.

% For compared methods that are enhanced by self-supervised learning tasks, the weights and the temperature parameters for the supplementary loss terms are also carefully tuned in a wide range.

For the baseline methods, we apply the same Adam optimization algorithm, Xavier parameter initializer, and batch size of 4096 as our \model. The hidden dimensionality for all baselines is also set as 32. Hyperparameters that are shared by baseline methods and our \model, are tuned in the same range as above. Such hyperparameters include the number of GNN layers, the weight for weight-decay regularizer. Specifically, for NCL, HCCF, SGL, SimGCL, SLRec, the weight for supplementary tasks are tuned from $\{1e^{-k}, 3e^{-k}|-1\leq k\leq 6\}$. The temperature hyperparameters are tuned from $\{1e^{-k}, 3e^{-k}|2\leq k\leq -1\}$. For SimGCL, which generates random noise for feature augmentation, we tune the standard deviation for noise generation from $[0.001, 5]$. For NCL, which conducts K-Means clustering every $n$ epochs, we tune $n$ from $\{1, 2, 3, 4, 5\}$. For baseline methods that employs random message dropout (\eg, LightGCN, SGL), the dropout rate is tuned from $\{0.1, 0.2, 0.3, 0.5, 0.8, 0.9\}$. For models that were trained for rating predictions in the original paper (\eg, AutoR, ST-GCN), we train these methods using pair-wise BPR loss for implicit feedback.
For NCF, we adopt the NeuMF version which combines MLPs with Generalized MF.


\subsection{Ablation Study}
We show more results of ablation study in Figure~\ref{fig:more_ablation_lines}, including the Recall@20 and NDCG@20 results on Yelp data, and NDCG@20 results on Gowalla and Amazon data. We can observe that the dual-level knowledge distillation schema and the adaptive contrastive regularization in our \model\ framework significantly improves the performance of the simple MLP model, to even surpass the performance of the GCN-based teacher model. From the results on Yelp data, it can be observed that removing the prediction-level KD causes severe over-fitting. This strongly validates the importance of distilling from the predictions made by the teacher model. Removing the embedding-level distillation, also causes significant performance drop and prominently lower learning efficiency on Yelp data. In comparison, the CL regularization contributes less to the performance of \model\ on Yelp data, which is due to its smaller interaction set that makes it less likely to over-smooth embeddings.

% is also indicated by results in Table~\ref{tab:module_ablation}. We ascribe to this to Yelp data having less user-item interaction data which makes it less likely to over-smooth node embeddings.

\subsection{Visualization for Embeddings Distribution}
We show more visualization results for the embedding distribution \wrt~NCL and SimGCL in Figure~\ref{fig:more_embeds_dist}. The visualization is done by first compressing the learned embeddings into a 2-d space using t-SNE dimension reduction. Then the scatter plot is smoothed using Gaussian kernel density estimation (KDE) to estimate the distribution of the embeddings. As shown by Figure~\ref{fig:intro_dist} and Figure~\ref{fig:more_embeds_dist}, our \model\ learns to allocates users into a bigger sub-space. In contrast, the baseline methods rely on iterative graph information propagation, which over-smooths the node embeddings to be too similar. From the visualization for the baselines, we can observe that the GNN frameworks over-smooth the user embeddings too much, such that users are split into several prominent subspaces disconnected to each other. This greatly hinders the CF models from learning relations between users from different subspaces.

\subsection{Hyperparameter Study}
We further investigate the influence of hidden dimensionality in our \model\ for the model performance. Specifically, we first train GCN-based teacher models with different hidden dimensionality (8, 16, 32, 64), and then distill the teacher model to a MLP-based student model with the same embedding size. As shown by results in Figure~\ref{fig:hyperparam_embed}, the performance shows a typical under-fitting to over-fitting curve \wrt~the hyperparameter $d$ on different datasets. After $d$ reaches the default embedding size 32, the performance increases slightly on Yelp dataset. Instead, the performance still prominently grows when $d$ increases from 32 to 64 on Gowalla data. This could be attributed to the larger scale of interaction records and the lower sparsity degree of Gowalla data.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.43\columnwidth]{figs/ablation_converge_Yelp_Recall.pdf}\quad
    \includegraphics[width=0.43\columnwidth]{figs/ablation_converge_Yelp_NDCG.pdf}
    \includegraphics[width=0.43\columnwidth]{figs/ablation_converge_Gowalla_NDCG.pdf}\quad
    \includegraphics[width=0.43\columnwidth]{figs/ablation_converge_Amazon_NDCG.pdf}
    \vspace{-0.15in}
    \caption{Test performance in each epoch for ablated models on three experimental datasets in terms of Recall and NDCG.}
    \vspace{-0.15in}
    \label{fig:more_ablation_lines}
    \Description{A line figure showing the performance with respect to epochs for \model\ and some representative baselines. The figure shows that \model\ converges faster while training.}
\end{figure}

\begin{figure}[t]
    \centering
    \subfigure[NCL]{
        \includegraphics[width=0.3\columnwidth]{figs/kde_yelp_ncl.pdf}
    }
    \subfigure[SimGCL]{
        \includegraphics[width=0.3\columnwidth]{figs/kde_yelp_simgcl.pdf}
    }
    \subfigure[\model]{
        \includegraphics[width=0.3\columnwidth]{figs/kde_yelp_oursmlp.pdf}
    }
    \vspace{-0.17in}
    \caption{KDE visualization for distribution of embeddings learned by NCL, SimGCL and the proposed \model.}
    \label{fig:more_embeds_dist}
    \vspace{-0.08in}
    \Description{A figure showing the visualized embedding distribution learned by HCCF, SimGCL, NCL and the proposed \model, where the embedding given by \model\ spreads in a wider range.}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{adjustbox}{width=\columnwidth}
    \input{./figs/hyper.tex}
    \end{adjustbox}
    \vspace{-0.25in}
    \caption{Hyperparameter study for hidden dimensionality of \model\ in terms of Recall and NDCG on Yelp and Gowalla.}
    \vspace{-0.1in}
    \label{fig:hyperparam_embed}
    \Description{A line figure presenting the performance change of \model\ with respect to the hidden dimensionality.}
\end{figure}


\subsection{Theoretical Analysis}

\subsubsection{\bf Detailed Complexity Analysis}
\label{sec:complexity_analysis}
The complexity analysis is to answer the following two questions: i) How do GCNs compare to MLPs in efficiency? ii) What is the overhead of our KD paradigm?
In each training step, GNN-based CF methods must conduct whole-graph information propagation for the embedding process. This takes $\mathcal{O}(|\mathcal{E}|\times L\times d)$ complexity for our lightweight GCN. The prediction phase of our GCN takes $\mathcal{O}(|\mathcal{T}_\text{bpr}|\times d)$ for computing dot-product. In comparison, the embedding process of MLP is not in graph-level but focus on one embedding vector at once. It costs $\mathcal{O}(|\mathcal{T}_2|\times L'\times d^2)$ where $|\mathcal{T}_2|=|\mathcal{T}_\text{bpr}|\ll|\mathcal{E}|/d$. It also requires $\mathcal{O}( |\mathcal{T}_\text{bpr}| \times d)$ computational cost to predict in each training batch. 

% In conclusion, graph encoders lower the efficiency of GNNs in comparison to MLPs.

Our prediction-level KD $\mathcal{L}_1$ requires $\mathcal{O}(|\mathcal{T}_1|\times d)$ cost. The $\mathcal{L}_2$ KD takes $\mathcal{O}(|\mathcal{T}_2|\times d)$ for the numerators, and $\mathcal{O}(|\mathcal{T}_2|\times J\times d)$ for the denominators. Similar to the second term for $\mathcal{L}_2$, the contrastive regularization $\mathcal{L}_3$ costs $\mathcal{O}(|\mathcal{T}_2|\times (I+J)\times d)$ computations. In conclusion, the KD of our \model\ has the total time complexity of $\mathcal{O}(|\mathcal{T}_2|\times (I+J)\times d)$, which is comparable to the state-of-the-art CF methods (\eg, self-supervised methods SGL~\cite{wu2021self}, SimGCL~\cite{yu2022graph}). Note that although the training process has the same complexity, our \model\ conducts inference with simple MLPs which is much more efficient as discussed above.


\subsubsection{\bf Derivations for High-Order Smoothing}
\label{sec:embed_analysis}
In this section, we present details for the derivations related to Section~\ref{sec:highorder_smoothing}. To begin with, we show the high-order smoothing effect of the GCN teacher in the perspective of gradients, which yield the results in Eq~\ref{eq:gcn_gradient}. Specifically, the gradients that maximize the similarity between  $\bar{\textbf{h}}_i^{(t)}$ and $\bar{\textbf{h}}_j^{(t)}$, given by the loss $\mathcal{L}^{(t)}$ is as follows:
\begin{align}
    \frac{\partial \mathcal{L}_{i,j}}{\partial \bar{\textbf{h}}_i}
    &=  -\sum_{u_i,v_j,v_k} \frac{\partial\log \text{sigm}(z_{i,j,k})}{\partial \bar{\textbf{h}}_i}
    = -\sum_{u_i,v_j,v_k} \sigma\frac{\partial z_{i,j,k}}{\partial \bar{\textbf{h}}_i}\nonumber\\
    &= -\sum_{u_i,v_j,v_k}\sigma\frac{\partial\textbf{h}_i^\top\textbf{h}_j}{\partial\bar{\textbf{h}}_i}
    % =-\sum_{v_k} \sigma\sum_{n_{i'}\in\mathcal{N}_i^L}\sum_{n_{j'}\in\mathcal{N}_j^L} a_{i',j'} \frac{\partial \textbf{h}_{i'}^\top\textbf{h}_{j'}}{\partial\bar{\textbf{h}}_i}\nonumber\\
    =-\sum_{v_k}\sigma \sum_{n_{i'},n_{j'}}\sum_{\mathcal{P}_{i,i'}^L, \mathcal{P}_{j,j'}^L} \frac{\partial \textbf{h}_{i'}^\top\textbf{h}_{j'}}{\partial\bar{\textbf{h}}_i}\nonumber\\
    &=-\sum_{v_k}\sigma \sum_{n_{i'},n_{j'}}\sum_{\mathcal{P}_{i,i'}^L, \mathcal{P}_{j,j'}^L} \prod_{(n_{a},n_{b})\in\mathcal{P}_{i,i'}^L}\frac{1}{\sqrt{d_a d_b}}\nonumber\\
    % \frac{1}{\sqrt{d_ad_b}}
    &\prod_{(n_a,n_b)\in\mathcal{P}_{j,j'}^L}\frac{1}{\sqrt{d_ad_b}}
    \frac{\partial\bar{\textbf{h}}_i^\top\bar{\textbf{h}}_j}{\partial\bar{\textbf{h}}_i}\nonumber\\
    &=\sum_{v_k}- \sigma \cdot
    \Big(\sum_{\mathcal{P}_{i,j}^{2L}} \prod_{(n_a,n_b)\in\mathcal{P}_{i,j}^{2L}} \frac{1}{\sqrt{d_a d_b}}\Big)\cdot
    \frac{\partial\bar{\textbf{h}}_i^{\top}\bar{\textbf{h}}_j^{}}{\partial \bar{\textbf{h}}_i}
\end{align}
\noindent where $\sigma$ denotes $1-\text{sigm}(z_{i,j,k})$. For simplicity, we omit the $(t)$ superscript. As $\mathcal{L}_{i,j}$ refers to the pull-close terms, $-\textbf{h}_i^\top\textbf{h}_k$ is omitted. Next, we show the details of derivations that obtain Eq~\ref{eq:pd_gradient} as follows:
\begin{align}
    {\mathcal{L}_1}
    &= \sum_{u_i,v_j,v_k}-\bar{z}_{i,j,k}^{(t)} \cdot \log \bar{\textbf{z}}_{i,j,k}^{(s)} + (\bar{z}_{i,j,k}^{(t)} - 1) \cdot \log(1 - \bar{\textbf{z}}_{i,j,k}^{(s)})\nonumber\\
    &= \sum_{u_i,v_j,v_k} \bar{z}_{i,j,k}^{(t)} \cdot \log\frac{1-\bar{z}_{i,j,k}^{(s)} }{\bar{z}_{i,j,k}^{(s)}} - \log(1 - \bar{z}_{i,j,k}^{(s)})\nonumber\\
    &=\sum_{u_i,v_j,v_k} -\bar{z}_{i,j,k}^{(t)} {z}_{i,j,k}^{(s)} /\tau_1+\log(1+\exp(z_{i,j,k}^{(s)}/\tau_1))\nonumber\\
    \frac{\partial\mathcal{L}_1}{\partial\textbf{h}_i^{(s)}}
    &= \sum_{u_i,v_j,v_k} \frac{\text{sigm}(z_{i,j,k}^{(s)}/\tau_1)}{\tau_1} \frac{\partial\textbf{h}_i^{(s)\top}\textbf{h}_j^{(s)}}{\partial\textbf{h}_i^{(s)}} - \frac{1}{\tau_1}\bar{z}_{i,j,k}^{(t)} \frac{\partial\textbf{h}_i^{(s)\top}\textbf{h}_j^{(s)}}{\partial\textbf{h}_i^{(s)}} \nonumber\\
    &=\sum_{u_i,v_j,v_k} -\frac{1}{\tau_1} \cdot (\bar{z}_{i,j,k}^{(t)} - \bar{z}_{i,j,k}^{(s)}) \cdot \frac{\partial\textbf{h}_i^{(s)\top} \textbf{h}_j^{(s)}}{\partial\textbf{h}_i^{(s)}}
\end{align}
From the derivation above, we can observe that GCN conduct high-order embedding smoothing using the cumulative product of node degrees as weights. This manner is restricted by the graph structures and may be affected by noisy edges. Instead, our developed \model\ uses knowledge distillation to perform adaptive high-order smoothing for any user-item pair $u_i,v_j$, using the teacher model's predictions as guidance during the model compression process. This allows the lightweight student model to effectively learn from the teacher's knowledge and make accurate predictions.

% the prediction-level knowledge distillation in our developed \model\ is able to conduct adaptive high-order smoothing for any user-item pair $u_i,v_j$ using predictions given by the teacher and itself as guidance.
