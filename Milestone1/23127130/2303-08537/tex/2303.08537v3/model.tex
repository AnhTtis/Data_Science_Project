\section{Collaborative Filtering}
\label{sec:model}
In this section, we introduce important notations in collaborative filtering, and recap MLP-based Neural CF and GNN-based CF architectures. In a typical recommendation scenario, there are $I$ users $\{u_1,u_2,...,u_I\}$ and $J$ items $\{v_1, v_2, ..., v_J\}$, indexed by $u_i$ and $v_j$, respectively. An interaction matrix $\textbf{A}\in\mathbb{R}^{I\times J}$ represents the observed interactions between users and items, in which an element $a_{i,j}=1$ if user $u_i$ has adopted item $v_j$, otherwise $a_{i,j}=0$.% Note that $\textbf{A}$ encodes users' implicit feedback, so $a_{i,j}=0$ represents no observed interaction between user $u_i$ and item $v_j$, instead of negative user feedback. 

Based on the above definitions, a CF-based recommender can be formalized as an inference model that i) $\textbf{Inputs}$ the user-item interaction data $\textbf{A}\in\mathbb{R}^{I\times J}$, models users' interactive patterns based on the input; ii) $\textbf{Outputs}$ interaction prediction results $y_{i,j}$ between the unobserved user-item pair $(u_i, v_j)$. In general, a CF model can be summarized as the following two-stage schema:
\begin{align}
    \label{eq:cf}
    y_{i,j} \leftarrow \textbf{Predict}(\textbf{h}_i, \textbf{h}_j), ~~~~ \textbf{h}_i, \textbf{h}_j \leftarrow \textbf{Embed}(u_i, v_j; \textbf{A})
\end{align}
\noindent The first stage $\textbf{Embed}(\cdot)$ denotes the embedding process which projects user $u_i$ and item $v_j$ into a $d$-dimensional hidden space based on the observed historical interactions $\textbf{A}$. The results of $\textbf{Embed}(\cdot)$ is vectorized representations $\textbf{h}_i, \textbf{h}_j\in\mathbb{R}^d$ for each user $u_i$ and item $v_j$, to preserve user-item interactive patterns. The second stage $\textbf{Predict}(\cdot)$ aims to forecast user-item relations with the prediction score $y_{i,j}\in\mathbb{R}$ using the learned embeddings $\textbf{h}_i, \textbf{h}_j$. 
Based on the above two-stage schema, our proposed method \model\ aims to conduct knowledge distillation from both the embedding and prediction levels for effectively knowledge transferring. \\\vspace{-0.12in}

% aligning both the prediction process $\textbf{Predict}(\cdot)$ and the embedding process $\textbf{Embed}(\cdot)$, to combine the strength of GNN-based CF and MLP-based CF.

\noindent\textbf{MLP-based Collaborative Filtering}.
MLP-based neural CF methods~\cite{he2017neural, xue2017deep} are proposed to endow CF with non-linear relation modeling. Due to the simplicity in model architectures, MLP-based CF is highly-efficient and unlikely to learn over-smoothed emebddings like GNNs~\cite{chen2020measuring}. Inspired by the advantages, we adopt MLP as the student model in our contrastive KD framework. In brief, the MLP in \model\ adheres to the two-stage paradigm as follows:
\begin{align}
    y_{i,j} = \textbf{h}_i^\top \textbf{h}_j, ~~~ \textbf{h}_i = \textbf{M-Embed}(\bar{\textbf{h}}_i), ~~~ \textbf{h}_j = \textbf{M-Embed}(\bar{\textbf{h}}_j)
\end{align}
\noindent where $\bar{\textbf{h}}_i, \bar{\textbf{h}}_j\in\mathbb{R}^d$ denote the initial embedding vectors for user $u_i$ and item $v_j$, respectively. $\textbf{M-Embed}(\cdot)$ denotes the MLP-based embedding function. We adopt dot-product for $\textbf{Predict}(\cdot)$, which has been shown to be efficient and effective~\cite{rendle2020neural}. \\\vspace{-0.12in}

\noindent\textbf{GNN-enhanced Collaborative Filtering}.
Most recent CF models apply graph neural information propagation on a bipartite interaction graph $\mathcal{G}=\{\mathcal{U}, \mathcal{V}, \mathcal{E}\}$, to encode users' high-order interactive relations into node embeddings.
Here, $\mathcal{U}=\{u_i\}, \mathcal{V}=\{v_j\}$ denote user and item node sets, respectively. An edge $e_{i,j}\in\mathcal{E}$ exists if and only if $a_{i,j}=1$. Typically, GNN-based CF can be abstracted as:
\begin{align}
    y_{i,j}=\textbf{h}_i^\top\textbf{h}_j,~
    \textbf{H} = \textbf{G-Embed}(\mathcal{G}, \bar{\textbf{H}})=\left(\textbf{Agg}(\textbf{Prop}(\mathcal{G}, \bar{\textbf{H}}))\right)^L
\end{align}
\noindent where $\textbf{H}, \bar{\textbf{H}}\in\mathbb{R}^{(I+J)\times d}$ denote the embedding matrices whose rows are node embedding vectors. $\textbf{G-Embed}(\cdot)$ denotes the GNN-based embedding function which iteratively propagates ($\textbf{Prop}(\cdot)$) and aggregates ($\textbf{Agg}(\cdot)$) the embeddings $\bar{\textbf{H}}$ along the interaction graph $\mathcal{G}$ for $L$ times. Note that though it injects informative structural information, GNNs based on holistic graph modeling and high-order iterations also damage the model scalability and bring the risk of over-smoothing in the collaborative filtering task.
