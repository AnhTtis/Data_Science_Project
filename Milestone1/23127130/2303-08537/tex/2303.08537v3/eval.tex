\section{Evaluation}
\label{sec:eval}

We conduct experiments from different aspects to validate the efficacy of the propose \model\ framework. The implementation details for our \model\ and the baseline methods are presented in~\ref{sec:implement}. Our experiments aim to answer the following research questions:
\begin{itemize}[leftmargin=*]
    \item \textbf{RQ1}: How does the proposed \model\ perform on different experimental datasets in comparison to state-of-the-art baselines?
    \item \textbf{RQ2}: How does different sub-modules of the proposed \model\ framework contribute to the overall performance?
    \item \textbf{RQ3}: How scalabile is \model\ in handling large-scale data?
    \item \textbf{RQ4}: How does the model performance vary when tuning important hyperparameters of the proposed \model\ model?
    \item \textbf{RQ5}: How can our \model\ model address the over-smoothing issue compared with GNN-based recommendation methods?
\end{itemize}

\subsection{Experimental Settings}
\subsubsection{\bf Experimental Datasets}
\begin{table}[t]
    \centering
    \caption{Statistics of the experimental datasets.}
    \label{tab:datasets}
    \small
    \vspace{-0.18in}
    \begin{tabular}{ccccc}
        \toprule
        Dataset & \# Users & \# Items & \# Interactions & Interaction Density \\
        \midrule
        Gowalla & 25,557 & 19,747 & 294,983 & $5.85\times 10^{-4}$\\
        Yelp & 42,712 & 26,822 & 182,357 & $1.59\times 10^{-4}$\\
        Amazon & 76,469 & 83,761 & 966,680 & $1.51\times 10^{-4}$\\
        % Tmall & 805,506 & 584,050 & 39,183,700 & $8.33\times 10^{-5}$\\
        \bottomrule
    \end{tabular}
    \vspace{-0.15in}
    \Description{A table showing the statistics of the Gowalla data (25557 users, 19747 items, 294983 interactions), the Yelp data (42712 users, 26822 items, 182357 interactions), and the Amazon data (76469 users, 83761 items, 966680 interactions).}
\end{table}


\begin{table*}[h]
\vspace{-0.1in}
\caption{Performance comparison on Gowalla, Yelp, and Amazon datasets in terms of \textit{Recall} and \textit{NDCG}.}
\vspace{-0.15in}
\centering
%\ssmall
% \scriptsize
\footnotesize
%\small
\setlength{\tabcolsep}{1.2mm}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|l|}
\hline
Data & Metric & BiasMF & NCF & AutoR & PinSage & STGCN & GCMC & NGCF & GCCF & LightGCN & DGCF & SLRec & NCL & SGL & HCCF & \emph{\model} & p-val.\\
\hline
\multirow{4}{*}{Gowalla}
&Recall@20 & 0.0867 & 0.1019 & 0.1477 & 0.1235 & 0.1574 & 0.1863 & 0.1757 & 0.2012 & 0.2230 & 0.2055 & 0.2001 & 0.2283 & 0.2332 & 0.2293 & \textbf{0.2434} & $2.1e^{-8}$\\
&NDCG@20 & 0.0579 & 0.0674 & 0.0690 & 0.0809 & 0.1042 & 0.1151 & 0.1135 & 0.1282 & 0.1433 & 0.1312 & 0.1298 & 0.1478 & 0.1509 & 0.1482 & \textbf{0.1592} & $1.2e^{-9}$\\
\cline{2-18}
&Recall@40 & 0.1269 & 0.1563 & 0.2511 & 0.1882 & 0.2318 & 0.2627 & 0.2586 & 0.2903 & 0.3181 & 0.2929 & 0.2863 & 0.3232 & 0.3251 & 0.3258 & \textbf{0.3399} & $2.4e^{-8}$\\
&NDCG@40 & 0.0695 & 0.0833 & 0.0985 & 0.0994 & 0.1252 & 0.1390 & 0.1367 & 0.1532 & 0.1670 & 0.1555 & 0.1540 & 0.1745 & 0.1780 & 0.1751 & \textbf{0.1865} & $1.7e^{-9}$\\
\hline

\multirow{4}{*}{Yelp}
&Recall@20 & 0.0198 & 0.0304 & 0.0491 & 0.0510 & 0.0562 & 0.0584 & 0.0681 & 0.0742 & 0.0761 & 0.0700 & 0.0665 & 0.0806 & 0.0803 & 0.0789 & \textbf{0.0823} & $3.7e^{-4}$\\
&NDCG@20 & 0.0094 & 0.0143 & 0.0222 & 0.0245 & 0.0282 & 0.0280 & 0.0336 & 0.0365 & 0.0373 & 0.0347 & 0.0327 & 0.0402 & 0.0398 & 0.0391 & \textbf{0.0414} & $3.8e^{-5}$\\
\cline{2-18}
&Recall@40 & 0.0307 & 0.0487 & 0.0692 & 0.0743 & 0.0856 & 0.0891 & 0.1019 & 0.1151 & 0.1175 & 0.1072 & 0.1032 & 0.1230 & 0.1226 & 0.1210 & \textbf{0.1251} & $4.8e^{-3}$\\
&NDCG@40 & 0.0120 & 0.0187 & 0.0268 & 0.0315 & 0.0355 & 0.0360 & 0.0419 & 0.0466 & 0.0474 & 0.0437 & 0.0418 & 0.0505 & 0.0502 & 0.0492 & \textbf{0.0519} & $2.4e^{-4}$\\
\hline

\multirow{4}{*}{Amazon}
&Recall@20 & 0.0324 & 0.0367 & 0.0525 & 0.0486 & 0.0583 & 0.0837 & 0.0551 & 0.0772 & 0.0868 & 0.0617 & 0.0742 & 0.0955 & 0.0874 & 0.0885 & \textbf{0.1067} & $1.1e^{-10}$\\
&NDCG@20 & 0.0211 & 0.0234 & 0.0318 & 0.0317 & 0.0377 & 0.0579 & 0.0353 & 0.0501 & 0.0571 & 0.0372 & 0.0480 & 0.0623 & 0.5690 & 0.0578 & \textbf{0.0734} & $7.0e^{-12}$\\
\cline{2-18}
&Recall@40 & 0.0578 & 0.0600 & 0.0826 & 0.0773 & 0.0908 & 0.1196 & 0.0876 & 0.1175 & 0.1285 &0.0912 & 0.1123 & 0.1409 & 0.1312 & 0.1335 & \textbf{0.1535} & $6.6e^{-10}$\\
&NDCG@40 & 0.0293 & 0.0306 & 0.0415 & 0.0402 & 0.0478 & 0.0692 & 0.0454 & 0.0625 & 0.0697 & 0.0468 & 0.0598 & 0.0764 & 0.0704 & 0.0716 & \textbf{0.0879} & $2.0e^{-12}$\\
\hline
\end{tabular}
\vspace{-0.1in}
\label{tab:overall_performance}
\Description{A table presenting the evaluated performance of the proposed \model\ model and the baselines, in which \model\ significantly outperforms the baseline methods.}
\end{table*}

% atings are transformed into binary implicit feedback following~\cite{he2020lightgcn}. We filter users and items with less than 3 interactions, 

Three benchmark datasets collected from real-world online services are used to evaluate the performance of \model. Data statistics are shown in Table~\ref{tab:datasets}. We split the interaction data into training set, validation set and test set with 70\%:5\%:25\%. Details of the experimental datasets are:
\begin{itemize}[leftmargin=*]
    \item \textbf{Gowalla}: This dataset is collected from Gowalla, including user check-in records at geographical locations, from Jan to Jun, 2010.
    \item \textbf{Yelp}: This dataset contains users' ratings on venues, collected from Yelp platform. The time range is from Jan to Jun, 2018.
    \item \textbf{Amazon}: This dataset is composed of users' rating behaviors over books collected from Amazon platform, during 2013.
\end{itemize}

\vspace{-0.1in}
\subsubsection{\bf Evaluation Protocols}
Following previous works on CF recommenders~\cite{wang2019neural, xia2022self}, we conduct all-rank evaluation, in which positive items from test set are ranked with all un-interacted items for each user. The widely-used \emph{Recall@N} and \emph{NDCG@N} metrics~\cite{wu2021self,2021knowledge} are used adopted for evaluation, where $N=20$ by default.

\vspace{-0.05in}
\subsubsection{\bf Baseline Models}
We compare \model\ with the following 14 baselines from 4 research lines for comprehensive validation.
\\\noindent\textbf{Traditional Collaborative Filtering Technique:}
\begin{itemize}[leftmargin=*]
    \item \textbf{BiasMF}~\cite{koren2009matrix}: It is a classic matrix factorization approach that combines user/item biases with learnable embedding vectors.
\end{itemize}
\textbf{Non-GNN Neural Collaborative Filtering}:
\begin{itemize}[leftmargin=*]
    \item \textbf{NCF}~\cite{he2017neural}: It is an early study of deep learning CF model that enhances the user-item interaction modeling with MLP networks.
    \item \textbf{AutoR}~\cite{sedhain2015autorec}: This method applies a three-layer autoencoder with fully-connected layers to encode user interaction vectors.
\end{itemize}
\textbf{Graph Neural Architectures for Collaborative Filtering}:
\begin{itemize}[leftmargin=*]
    \item \textbf{PinSage}~\cite{ying2018graph}: This method combines random walk with graph convolutions for web-scale graph in recommendation.
    \item \textbf{STGCN}~\cite{zhang2019star}: This method augments GCN with autoencoding sub-networks on hidden features for better inductive inference.
    \item \textbf{GCMC}~\cite{berg2017graph}: This is a representative work to introduce graph convolutional operations into the matrix completion task.
    \item \textbf{NGCF}~\cite{wang2019neural}: It is a GNN-based CF method which conducts graph convolutions on the user-item interaction graph for embeddings.
    \item \textbf{GCCF}~\cite{chen2020revisiting} and \textbf{LightGCN}\cite{he2020lightgcn}: These two methods propose to simplify conventional GCN structures by removing transformations and activations for improving performance.
\end{itemize}
\textbf{Disentangled GNN-based Collaborative Filtering}:
\begin{itemize}[leftmargin=*]
    \item \textbf{DGCF}\cite{wang2020disentangled}: This method disentangles user-item interactions into multiple hidden factors in the graph message passing process.
\end{itemize}
\textbf{Self-Supervised Learning Approaches for Recommendation}:
\begin{itemize}[leftmargin=*]
    \item \textbf{SLRec}~\cite{yao2021self}: This method applies contrastive learning to recommendation models with feature-level data augmentations.
    \item \textbf{NCL}~\cite{lin2022improving}: This approach enhances self-supervised graph CF models with enriched neighbor-wise contrastive learning.
    \item \textbf{SGL}~\cite{wu2021self}: It conducts various types of graph augmentations and feature augmentations with graph contrastive learning for CF.
    \item \textbf{HCCF}~\cite{xia2022hypergraph}: This method augments GNN-based CF with a global hypergraph GNN and conducts cross-view contrastive learning.
\end{itemize}

\subsection{Overall Performance Comparison (RQ1)}


The overall performance of \model\ and the baselines are shown in Table~\ref{tab:overall_performance}. From the results we have the following observations: \vspace{-0.05in}
\begin{itemize}[leftmargin=*]
    \item Our \model\ consistently achieves best performance compared to baselines methods. Also, we re-train \model\ and the best-performed baselines (\ie, SGL and NCL) for 5 times to calculate $p$-values. The experimental results validate the significance of the improvement by \model. Compared to the state-of-the-art GNN methods, the MLP-based inference model of our graph-less \model\ generates more accurate recommendation results, due to its adaptive contrastive knowledge distillation. Specifically, the dual-level KD in \model\ enables enriched and adaptive high-order smoothing, which not only distills the accurate dark knowledge in the well-trained GNN teacher, but also avoids being affected by the over-smoothing signals. Furthermore, the adaptive contrastive regularization automatically alleviates the over-smoothing effects, which further boosts the performance. \\\vspace{-0.12in}
    
    \item While the self-supervised learning schema greatly improves the performance of GNN-based CF, our graph-less \model\ model still significantly outperforms the SSL-enhanced graph models. We attribute the performance deficiency to the inherent incapability of existing SSL frameworks in filtering over-smoothing signals. For example, SGL augments model training by introducing random noises, which may even aggravate the inaccuracy in node embeddings when the noises are magnified through high-order graph propagation. As for NCL and HCCF, they seek to connect nodes based on global semantic relatedness, which may even over-smooth nodes distant from each other in the original graph. In comparison, our graph-less \model\ model abandons GNN architectures in the inference model, which fundamentally minimizes the possibility of over-smoothed node embeddings. Furthermore, our KD paradigm avoids distilling over-smoothed embeddings via the adaptive contrastive regularization. \\\vspace{-0.12in}
    
    \item We observe that non-GNN CF models (\ie, NCF and AutoR) present very bad performance, event though they have similar MLP-based network architectures as the inference model in \model. This sheds light on the deficiency of MLPs in modeling high-order graph connectivity into user/item embeddings. While sharing similar MLP structures, our \model\ is additionally supervised by knowledge distilled from advanced GNN models. This not only improves the optimization for MLP networks, but also makes it possible to adaptively filter the over-smoothing signals in parameter learning. The huge performance gap between NCF/AutoR and our \model\ strongly shows the effectiveness of our contrastive knowledge distillation.
\end{itemize}


\begin{table}[t]
    %\vspace{-0.05in}
    \caption{Ablation study on key components of \model.}
    \vspace{-0.15in}
    \centering
    %\small
    %\scriptsize
    %\ssmall
    \footnotesize
    %\small
    % \setlength{\tabcolsep}{1.2mm}
    \begin{tabular}{c|c|cc|cc|cc}
        \hline
        % \multirow{2}{*}{Category} & \multirow{2}{*}{Variant} 
        \multicolumn{2}{c|}{Data}& \multicolumn{2}{c|}{Gowalla} & \multicolumn{2}{c|}{Yelp} & \multicolumn{2}{c}{Amazon}\\
        % \cline{3-8}
        \hline
        \multicolumn{2}{c|}{Variant} & Recall & NDCG & Recall & NDCG & Recall & NDCG\\
        \hline
        % \hline
        % \multicolumn{8}{c}{Top-20}\\
        \hline
        \multicolumn{2}{c|}{-$\mathcal{L}_1$} & 0.2180 & 0.1415 & 0.0756 & 0.0377 & 0.1012 & 0.0692\\
        \hline
        \multirow{3}{*}{-$\mathcal{L}_2$} & User & 0.2292 & 0.1493 & 0.0806 & 0.0405 & 0.0998 & 0.0667 \\
        & Item & 0.2266 & 0.1477 & 0.0808 & 0.0406 & 0.0974 & 0.0649 \\
        & Both & 0.2222 & 0.1451 & 0.0787 & 0.0399 & 0.0938 & 0.0626 \\
        \hline
        \multirow{4}{*}{-$\mathcal{L}_3$} & U-I & 0.2330 & 0.1496 & 0.0814 & 0.0410 & 0.0939 & 0.0607 \\
        & U-U & 0.2349 & 0.1512 & 0.0811 & 0.0407 & 0.0965 & 0.0634 \\
        & I-I & 0.2331 & 0.1514 & 0.0813 & 0.0409 & 0.1009 & 0.0674\\
        & All & 0.2282 & 0.1480 & 0.0810 & 0.0407 & 0.0933 & 0.0605\\
        \hline
        \hline
        \multicolumn{2}{c|}{\emph{\model}} & \textbf{0.2434} & \textbf{0.1592} & \textbf{0.0823} & \textbf{0.0414} & \textbf{0.1067} & \textbf{0.0734}\\
        \hline
    \end{tabular}
    \vspace{-0.1in}
    \label{tab:module_ablation}
    \Description{A table presenting the results of module ablation study. The results are divided into three parts: loss $\mathcal{L}_1$ for the prediction-level distillation, loss $\mathcal{L}_2$ for the embedding level distillation, and loss $\mathcal{L}_3$ for the contrastive regularization. All ablated variants performs worse than the proposed \model.}
\end{table}

\vspace{-0.1in}
\subsection{Model Ablation Study (RQ2)}
We validate the effectiveness of the applied sub-modules in \model\ by ablating each module separately. The evaluated performance is shown in Table~\ref{tab:module_ablation}. We also show the performance change \wrt, training epochs in Figure~\ref{fig:ablation_lines}. We have the following observations:
\begin{itemize}[leftmargin=*]
    \item \textbf{Effect of Prediction-Level Distillation}: Our prediction-level distillation (\ie, $\mathcal{L}_1$) excavates deep dark knowledge in the teacher using the pair-wise ranking task with enriched KD samples. The variant -$\mathcal{L}_1$ removes this module, which leads to performance degradation on Gowalla and Yelp data. The results validate the effectiveness of learning from the predictive outputs of teacher model using our distillation loss $\mathcal{L}_1$.\\\vspace{-0.12in}
    % The prediction-level KD is removed to produce variant \textbf{-$\mathcal{L}_1$}. From the results we can observe that, removing $\mathcal{L}_1$ causes the most significant performance degradation compared to other variants on Gowalla data and Yelp data. This evidently reflects the importance of learning from the predictive outputs of teacher model. And it validates the effectiveness of excavating deep dark knowledge in the teacher using the pair-wise ranking task with enriched KD samples.
    \item \textbf{Effect of Embedding-Level Distillation}: We then test the effect of embedding-level KD with the variant -$\mathcal{L}_2$ by removing $\mathcal{L}_2$ on user/item embeddings. In some cases the alignment between users and the alignment between items have different effect on the performance. What's more, the results reveal not only the contribution of $\mathcal{L}_2$ to the final performance, but also its prominent accelerating effect in model training shown in Fig~\ref{fig:ablation_lines}. \\\vspace{-0.12in}
    \item \textbf{Effect of Contrastive Regularization}: We ablate \model\ without the contrastive regularization in variant -$\mathcal{L}_3$. The regularization for user-item, user-user, and item-item relatedness are individually ablated. We observe the importance of $\mathcal{L}_3$ for the superior performance, especially on Amazon data. We ascribe this to the larger scale of Amazon data which makes it more likely to over-smooth with irrelevant high-order neighbors. The incorporation of $\mathcal{L}_3$ can cancel out over-smoothing signals.\\\vspace{-0.12in}
    \item \textbf{Comparison to Student and Teacher Models}: From the learning curves in Fig~\ref{fig:ablation_lines}, we can observe the great performance gap between simple MLP student and advanced GNN teacher. The three augmented tasks greatly minimizes this gap by effectively distilling useful knowledge. Additionally, the distillation tasks accelerate the training to surpass the original teacher model.
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.43\columnwidth]{figs/ablation_converge_Gowalla_Recall.pdf}\quad
    \includegraphics[width=0.43\columnwidth]{figs/ablation_converge_Amazon_Recall.pdf}
    \vspace{-0.12in}
    \caption{Test performance in each epoch for ablated models.}
    \vspace{-0.1in}
    \label{fig:ablation_lines}
    \Description{A line figure showing the performance with respect to epochs for \model\ and some representative baselines. The figure shows that \model\ converges faster while training.}
\end{figure}

\begin{table}[t]
    \centering
    %\small
    \footnotesize
    \setlength{\tabcolsep}{1.4mm}
    % \caption{Model efficiency study on per-epoch training time and inference time on Gowalla, Yelp, and Amazon data.}
    \caption{Model performance and per-epoch model inference time of representative methods on large-scale Tmall dataset.}
    \label{tab:scalability}
    \vspace{-0.1in}
    \begin{tabular}{ccccccc}
        \hline
        Metric & \# Edges & DGCF & SGL & HCCF & NCL & \emph{\model}\\
        \hline
        \hline
        \multirow{2}{*}{R@20} & 1.6M & 0.0221 & 0.0258 & 0.0272 & 0.0286 & \multirow{2}{*}{\textbf{0.0308}}\\
        & 2.9M & 0.0253 & 0.0278 & 0.0283 & 0.0294 & \\
        \hline
        \multirow{2}{*}{N@20} & 1.6M & 0.0258 & 0.0296 & 0.0309 & 0.0337 & \multirow{2}{*}{\textbf{0.0366}}\\
        & 2.9M & 0.0279 & 0.0311 & 0.0319 & 0.0334 & \\
        \hline
        \multirow{2}{*}{Time} & 1.6M & 7190.2s & 1331.8s & 1342.5s & 1392.2s & \multirow{2}{*}{\textbf{785.1s}}\\
        & 2.9M & 11431.8s & 1456.3s & 1530.8s & 1693.8s & \\
        \hline
    \end{tabular}
    \vspace{-0.12in}
    \Description{A table showing the performance and the inference time of \model\ and baselines on the large-scale Tmall dataset. \model\ outperforms the baselines and consumes the least time for inference.}
\end{table}

\vspace{-0.1in}
\subsection{Model Scalability Study (RQ3)}
To validate the efficiency of our \model\ in handling large-scale real-world data, we compare \model\ with the best performed baselines on a e-commerce data collected from Tmall platform. The dataset contains around 40 million records of user clicks. To successfully run on this dataset, GNN-based methods have to sample subgraphs for information propagation. In contrast, graph sampling is not required by the MLP-based inference model of our \model. The performance and the inference time are shown in Table~\ref{tab:scalability}, where we run the baselines using graph sampling strategy~\cite{hu2020heterogeneous} with two scales (\ie, subgraphs contain 1.6M edges and 2.9M edges, respectively). We have mainly two key observations shown as follows:
\begin{itemize}[leftmargin=*]
    \item \textbf{More Accurate Recommendations}: \model\ achieves better recommendation performance in terms of Recall and NDCG. This reflects the higher probability of over-smoothing on the large but sparse interaction graph. Our \model\ avoids this problem without explicit graph message passing. Instead, informative knowledge is distilled from GNNs for model compression.
    \item \textbf{Much Higher Efficiency}: \model\ greatly reduces the inference time on the large Tmall data. \textit{Firstly}, the embedding process of our MLP predictor is agnostic to the holistic interaction graph, thus the large-scale graph does not increase much overhead for embedding processing. No graph sampling is required in comparison to GNNs. \textit{Secondly}, \model\ infers user-item relations based on simple MLPs. The computational costs of fully-connected layers in MLPs are much lower than the cost of GNNs.
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.3\columnwidth]{figs/hyper_gowalla_soft_Recall_20.pdf}\quad
    \includegraphics[width=0.3\columnwidth]{figs/hyper_gowalla_cd_Recall_20.pdf}\quad
    \includegraphics[width=0.3\columnwidth]{figs/hyper_gowalla_sc_Recall_20.pdf}\\
    \includegraphics[width=0.3\columnwidth]{figs/hyper_gowalla_soft_NDCG_20.pdf}\quad
    \includegraphics[width=0.3\columnwidth]{figs/hyper_gowalla_cd_NDCG_20.pdf}\quad
    \includegraphics[width=0.3\columnwidth]{figs/hyper_gowalla_sc_NDCG_20.pdf}\\
    \vspace{-0.12in}
    \caption{Hyperparameter study for our \model\ model on Gowalla dataset, in terms of \emph{Recall@20} and \emph{NDCG@20}.}
    \vspace{-0.1in}
    \label{fig:hyper2d}
    \Description{A line figure showing the performance change with respect to the weight of the prediction-level distillation, the embedding-level distillation, and the contrastive regularization.}
\end{figure}

\begin{figure}[t]
    \centering
    \subfigure[Pred. Distillation]{
        \includegraphics[width=0.3\columnwidth]{figs/hyper_soft_Recall.pdf}
        \label{fig:hyper3d_pred}
    }
    \subfigure[Embed. Distillation]{
        \includegraphics[width=0.3\columnwidth]{figs/hyper_cd_Recall.pdf}
        \label{fig:hyper3d_embed}
    }
    \subfigure[Contrastive Reg.]{
        \includegraphics[width=0.3\columnwidth]{figs/hyper_sc_Recall.pdf}
    }
    \vspace{-0.17in}
    \caption{Impact of weights and temperature in different learning objectives on Yelp, in terms of \emph{Recall@20}.}
    \vspace{-0.2in}
    \label{fig:hyper3d}
    \Description{A three-D figure showing the composite effect of the weight and the temperature coefficient on the performance, for the prediction-level distillation, the embedding-level distillation, and the contrastive regularization.}
\end{figure}
\subsection{Hyperparameter Study (RQ4)}
In this section, we examine the influence of different hyperparameters on the performance of \model. The effect of loss weights $\lambda_1, \lambda_2, \lambda_3$ are shown in Figure~\ref{fig:hyper2d}. The composite effect of loss weights and corresponding temperatures $\tau_1, \tau_2, \tau_3$ are shown in Figure~\ref{fig:hyper3d}. The effect of the size $|\mathcal{T}_1|$ for the prediction-level distillation is shown in Table~\ref{tab:batch_hyper}. Our observations are as follows:
\begin{itemize}[leftmargin=*]
    \item \textbf{Strength of Prediction-Level Distillation}. $\lambda_1, \tau_1$: This weight $\lambda_1$ and temperature $\tau_1$ jointly control the strength of the prediction-level KD $\lambda_1$. We first study the influence of $\lambda_1$ in Figure~\ref{fig:hyper2d} with $\tau_1$ fixed. When $\lambda_1$ is small, not enough knowledge is distilled to the student model which results in deficient performance. When $\lambda_1$ is too large, $\mathcal{L}_1$ cover up the optimization of main loss and yield degraded performance. Additionally, Figure~\ref{fig:hyper3d_pred} shows the positive effect of applying smaller $\tau_1$ to produce larger gradients.
    
    \item \textbf{Strength of Embedding-Level Distillation}. $\lambda_2, \tau_2$: The parameters control the strength of \model\ in restricting the embeddings in MLP to be close to embeddings in GNN. From Figure~\ref{fig:hyper3d_embed} it can be observed that $\lambda_2$ and $\tau_2$ jointly adjust the strength of embedding KD to have modest influence on optimization, to prevent from insufficient knowledge distillation and too-strict embedding regularization. Either large weight with low temperature or small weight with high temperature causes performance decay.
    
    \item \textbf{Strength of Contrastive Regularization} $\lambda_3, \tau_3$: These parameters determine the strength of push-away regularization for preventing over-smoothing. The results show that either too small weight $\lambda_3$ or too high temperature $\tau_3$ causes insufficient regularization and produces over-smoothed embeddings. Meanwhile, strong regularization may damage the modeling of node-wise affinity, and also yields worse performance.
    
    \item \textbf{Per-Batch Number of Samples to Distill} $|\mathcal{T}_1|$: This hyperparameter determines how many instances are sampled to conduct the prediction-level distillation in each training step. According to the results in Table~\ref{tab:batch_hyper}, increasing batch size brings better KD performance until the performance saturates. We ascribe this to the effect that larger batch size filters low-frequency noise in predictions made by the teacher model in \model.
\end{itemize}


\begin{table}[t]
    %\vspace{-0.05in}
    \caption{Investigation on the impact of batch size in the prediction-oriented distillation of the proposed \model.}
    \vspace{-0.15in}
    \centering
    %\small
    %\scriptsize
    %\ssmall
    \footnotesize
    %\small
    % \setlength{\tabcolsep}{1.2mm}
    \begin{tabular}{c|c|cccccc}
        \hline
        \multirow{2}{*}{Data} & \multirow{2}{*}{Metric} & \multicolumn{6}{c}{Batch Size $|\mathcal{T}_1|$ in Prediction-Level Distillation}\\
        \cline{3-8}
        & & $1e3$ & $5e3$ & $1e4$ & $5e4$ & $1e5$ & $5e5$\\
        \hline
        \hline
        \multirow{2}{*}{Gowalla} & Recall & 0.2208 & 0.2361 & 0.2399 & 0.2420 & 0.2434 & 0.2448\\
        & NDCG & 0.1441 & 0.1530 & 0.1554 & 0.1577 & 0.1592 & 0.1597\\
        \hline
        \multirow{2}{*}{Yelp} & Recall & 0.0443 & 0.0730 & 0.0773 & 0.0802 & 0.0823 & 0.0822\\
        & NDCG & 0.0210 & 0.0372 & 0.0392 & 0.0407 & 0.0414 & 0.0414\\
        \hline
    \end{tabular}
    \vspace{-0.2in}
    \label{tab:batch_hyper}
    \Description{A table recording the performance change of \model\ with respect to the }
\end{table}

\vspace{-0.1in}
\subsection{Over-Smoothing Investigation (RQ5)}
To investigate whether our graph-less \model\ framework is able to mitigate the over-smoothing effect in graph-structured relation learning for CF, we compare representative baselines and our \model\ model on the Mean Average Distance (MAD) values~\cite{chen2020measuring} over embeddings for the most popular users and items. The evaluation results are shown in Table~\ref{tab:mad}. Our \model\ has higher MAD values on both user and item embeddings for Gowalla and Yelp data, in comparison to not only GCN model GCCF, but also state-of-the-art SSL frameworks. It can be concluded that our \model\ framework better addresses the over-smoothing issue, by learning more uniform-distributed embeddings for users and items, to better characterize their unique interaction patterns. This should be attributed to the MLP-based inference framework, and the contrastive regularization that adaptively alleviates over-smoothing signals.


\begin{table}[t]
    %\vspace{-0.05in}
    \caption{Investigation on the ability to address the over-smoothing effect on Gowalla and Yelp data in terms of MAD.}
    \vspace{-0.15in}
    \centering
    % \small
    %\scriptsize
    %\ssmall
    \footnotesize
    %\small
    % \setlength{\tabcolsep}{1.2mm}
    \begin{tabular}{c|c|cccccc}
        \hline
        \multicolumn{2}{c|}{Data} & GCCF & LightGCN & SGL & NCL & HCCF & \emph{\model}\\
        \hline
        \hline
        \multirow{2}{*}{Gowalla} & User & 0.8276 & 0.8203 & 0.8412 & 0.8088 & 0.8394 & \textbf{0.8576}\\
        & Item & 0.7579 & 0.7614 & 0.7702 & 0.8169 & 0.7905 & \textbf{0.8335}\\
        \hline
        \multirow{2}{*}{Yelp} & User & 0.9226 & 0.9610 & 0.9755 & 0.9640 & 0.9749 & \textbf{0.9819}\\
        & Item & 0.6288 & 0.7095 & 0.7191 & 0.6953 & 0.6246 & \textbf{0.7662}\\
        \hline
    \end{tabular}
    \vspace{-0.1in}
    \label{tab:mad}
    \Description{A table presenting the evaluated MAD value of \model\ and baselines. The MAD value of \model\ is higher.}
\end{table}