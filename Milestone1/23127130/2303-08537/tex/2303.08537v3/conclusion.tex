\section{Conclusion}
\label{sec:conclusoin}

In this paper, we propose a contrastive knowledge distillation model which adaptively transfers knowledge from the GNN-based teacher model to a small feed-forward network, significantly improving the efficiency and robustness of recommender models. Our designed adaptive contrastive regularization generate unbiased self-supervision signals to alleviate the over-smoothing and noise effects commonly exist in recommender systems. Our comprehensive experiments demonstrate the effectiveness of our method in improving recommendation accuracy and achieving better efficiency when compared to state-of-the-art learning techniques. 

% One future work can explore the interpretability of our model for explaining the distilled knowledge.