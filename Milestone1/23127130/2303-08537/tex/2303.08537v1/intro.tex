\section{Introduction}
\label{sec:intro}

% background: recommender system, collaborative filtering
% GNN-based methods and self-supervised Graph models
% limitations of GNNs (over-smoothing and scalability), which cannot be solved by recent SSL methods
% simple models have the same representation capacity to work as well as sophisticated GNN methods
% motivated by the limitations of GNN-based CF and the potentials of simple CF, this paper proposes to build a principled approach that effectively distills the learned knowledge from sophisticated GNNs to simple MLPs.
% To do so, we faces challenges: 
% 1. how to effectively and efficiently distill the learned knowledge from GNN-based CF to Simple MLPs. Existing knowledge distillation methods
% 2. how to prevent the over-smoothed embeddings from affecting the embeddings of simple models.



% GNN has achieved superior performance in collaborative recommender systems. Self-supervised learning techniques further improved the model robustness against noise and sparsity issue.
% However, the state-of-the-art graph neural approaches require conducting iterative message passing for representation refinement, which exhibit two inherent limitations. \emph{Firstly}, stacking more and more graph neural layers smooth the centric node embeddings using far-away nodes, which inevitably introduce irrelevant information into the node representation. \emph{Secondly}, the time-consuming graph message passing makes it less-competitive in efficiency compared to simple MLP-based models in applications. What's more, graph neural networks require global message passing on whole graph, which makes it intractable to work on web-scale interaction data. Previous works seek to sub-graph sampling methods which causes even more overhead and further limit its scalability.

% Essentially, simple collaborative models such as MLP have the same representation capacity as sophisticated models such as graph neural networks, as these methods all solve the collaborative filtering task via learning semantic embedding vectors for users and items. 

% Motivated by the limitations of GNN-based CF and the 

% To address the above limitations, we propose to predict user behavior with a simple MLP-based model that abandons the heavy graph neural encoders. The MLP network is additionally supervised through two distillation schemes, one of which guide the MLP to learn from Graph

Recent years have witnessed the great success of graph neural network (GNN) in learning latent representations for graph structured data~\cite{zhu2020simple,wu2019simplifying,velivckovic2017graph}. Inspired by such development, many efforts have introduced GNN into Collaborative Filtering (CF) and shown its power in modeling high-order user-item relationships, such as NGCF~\cite{wang2019neural}, LightGCN~\cite{he2020lightgcn}, and GCCF~\cite{chen2020revisiting}. At the core of GNN-based CF models is to utilize a neighborhood aggregation scheme to encode user (item) embeddings via recursively message passing. 

Despite their achieved remarkable performance, we argue that two important limitations exist in current GNN-based CF methods.\\\vspace{-0.12in}

\noindent (i) \textbf{Over-Smoothing and Noise Issues}. The inherent design of GNN may lead to \emph{over-smoothing} issue as the increase of stacked graph layers for embedding propagation~\cite{zhou2020towards,liu2020towards}. The neighborhood aggregator built upon low-pass Laplacian smoothing operation with graph-structured user-item connections, will unavoidably generate indistinguishable user and item representations. Hence, most of current GNN-based recommender systems suffer from the over-smoothing problem as the number of layers increases, which fall short in learning uniformity of diverse user preference and thus obtain suboptimal performance. Additionally, the recursively aggregating schema in GNN-based recommenders easily fuses \emph{noisy} signals, which hinders representation learning of the genuine interaction patterns. Various biases of user behavior data widely exist in recommender systems~\cite{chen2021autodebias,zhang2021causal}, such as misclick behaviors, popularity bias. Even worse, the recursive message passing will amplify the noise effects across multiple propagation layers, which seriously affects the representation performance for recommendation. \\\vspace{-0.12in}

\noindent (ii) \textbf{Scalability Limitation with Recursive Expansion}. Although GNN-based recommenders can capture high-order connectivity by stacking multiple propagation layers, the recursive neighbor information aggregation incurs expensive computation in model inference~\cite{yan2020tinygnn,zheng2022bytegnn,gallicchio2020fast}. Therefore, GNN-based CF models with deeper graph neural layers require repeatedly propagate representations among many neighboring nodes, which show poor scalability in practical scenarios, especially for large-scale recommender systems. In light of this, the scalability limitation of current GNN-based methods brings an urge for designing an efficient and effective high-order relation learning paradigm in recommendation, which remains unexplored in existing CF recommendation models.

\begin{figure*}
    \vspace{-0.15in}
    \centering
    \subfigure[Performance v.s. inference time]{
        \includegraphics[height=0.2\textwidth]{figs/intro_time_performance.pdf}
        \label{fig:intro_time_performance}
    }
    \subfigure[Advantage of our adaptive embedding recalibration]{
        \includegraphics[height=0.2\textwidth]{figs/intro_casestudy.pdf}
        \label{fig:intro_case}
    }
    \subfigure[Distribution of learned user embeddings]{
        \includegraphics[height=0.2\textwidth]{figs/intro_dist_new.pdf}
        \label{fig:intro_dist}
    }
    \vspace{-0.15in}
    \caption{Illustration of motivation and advantages of our \model\ model from different perspectives.}
    \vspace{-0.15in}
    \label{fig:intro}
    \Description{Motivated examples showing the strength of \model\ from three aspects. Firstly, the proposed \model\ achieves best performance with less inference time. Secondly, our \model\ discovers the difference between a noisy user pair. Thirdly, \model\ learns embeddings preserving better user preference uniformly in comparison to baselines.}
\end{figure*}

Having realized the importance of addressing the above challenges, however, it is non-trivial considering the following factors:\vspace{-0.08in}
\begin{itemize}[leftmargin=*]

\item How to well preserve global collaborative signals in an efficient manner for user-item interaction modeling, remains a challenge. \\\vspace{-0.12in}

\item How to encode informative representations which are robust to over-smoothing and noise issues while preserving high-order interaction patterns, requiring adaptive knowledge transfer.

\end{itemize}

As shown in Figure~\ref{fig:intro}, we illustrate motivation examples for the model design in our \model\ recommender. To be specific, by comparing our \model\ with various state-of-the-art GNN-based CF methods (\eg, GCCF~\cite{chen2020revisiting}, SGL~\cite{wu2021self}, HCCF~\cite{xia2022hypergraph}, SimGCL~\cite{yu2022graph}) in Figure~\ref{fig:intro_time_performance}, \model\ significantly improves model efficiency, meanwhile maintaining superior recommendation performance. In Figure~\ref{fig:intro_case} of two user with dissimilar interests, we show the advantage of our adaptive contrastive knowledge distillation to recalibrate the similar representations encoded by GNN teacher model into distinguishable embedding space. To reflect the better uniformity preserved in learned embeddings of \model, we visualize the distributions of projected embeddings learned by different methods in Figure~\ref{fig:intro_dist}.\\\vspace{-0.12in}

Inspired by the effectiveness of knowledge distillation (KD) for transferring knowledge in various domains (\eg, computer vision~\cite{zhang2021data} and text mining~\cite{chen2020distilling}), KD has become an effective solution to transfer knowledge from a large model to a smaller one. In general, KD aims to reach the agreement between the prediction results of a well-trained teacher model and a student model by minimizing their distribution difference. However, collaborative filtering task usually involves highly sparse interaction data, which undermines the capability of knowledge distillation. Specifically, direct distillation from noisy and sparse graph structures, is difficult to advance the performance of original GNN model after being compressed. Fortunately, recent developments of contrastive learning bring new insights in alleviating data sparsity with auxiliary self-supervision signals, this paper explores the possibility of marrying the power of knowledge distillation and contrastive learning to pursue adaptive knowledge transfer with a robust and efficient CF model.

In this work, we propose a novel graph-less collaborative filtering framework, named \model, to improve both the effectiveness and efficiency of recommender without the sophisticated GNN structures. In particular, we propose a bi-level alignment framework to distill knowledge with both prediction-level and embedding-level signals. With such design, the distilled knowledge comes from not only the teacher model's predictions but also the latent high-order collaborative semantics preserved in embeddings. Furthermore, we propose to enhance our knowledge distillation paradigm against the perturbation of over-smoothing and noise effects in GNN teacher model. Towards this end, an adaptive knowledge transfer module is designed with contrastive regularization to capture the diversity of user preference, based on the derived consistency between the supervised CF objective and the augmented SSL task. In our proposed \model\ model, the latent knowledge of GNN-based teacher model will be distilled into a lightweight yet empowered feed-forward network that can jointly capture user-specific preference uniformity and cross-user global collaborative dependencies.

To summarize, our contributions are presented as follows:
\begin{itemize}[leftmargin=*]

\item We propose contrastive knowledge distillation to compress GNN-based CF model into a simple recommender to improve both effectiveness and efficiency. In our adaptive distillation paradigm, an embedding calibration module is designed to enhance KD to preserve useful knowledge and discard the noisy information.

% \item We propose a contrastive knowledge distillation model that improves GNN-based CF model in terms of both effectiveness and efficiency. In our adaptive distillation paradigm, an embedding calibration module is designed to enhance KD to preserve useful knowledge and discard the noisy information.

\item Theoretical analysis is provided from two perspectives: i) the benefits of our distillation model in alleviating over-smoothing issue; ii) effectiveness of our distilled self-supervision signals for data augmentation in an adaptive manner.

\item Extensive experiments on public datasets demonstrate that \model\ significantly improves the performance of CF tasks. Additionally, the empirical results show that \model\ gains more efficient embedding encoding over LightGCN on different datasets. 

\end{itemize}

% To make our results more reproducible, we release our implementation at the link: \url{https://github.com/HKUDS/SimRec}.%https://anonymous.4open.science/r/GLCF-F60F/.