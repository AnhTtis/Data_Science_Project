\documentclass[sigconf]{acmart}
\usepackage{booktabs} % To thicken table lines

\usepackage[english]{babel}
\usepackage{moresize}
\usepackage{amsmath}
\usepackage{algorithmic}
\usepackage{balance}
\usepackage{comment}
\usepackage{paralist}
\usepackage{bm}
\usepackage{pgfplots}
\usetikzlibrary{pgfplots.dateplot}

\usepackage{flushend}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{mathrsfs}
\usepackage{graphicx}
\let\Bbbk\relax
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{longtable}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{mathrsfs}
\usepackage{subfigure}
\usepackage{enumitem}
\usepackage[linesnumbered,algoruled,boxed,lined]{algorithm2e}
\usepackage{adjustbox}
\usepackage{hyperref}
\usepackage{pgfplots}
\usetikzlibrary{pgfplots.dateplot}
\usepackage{filecontents}
% Tableau colors
\definecolor{tblue}{RGB}{31,119,180}
\definecolor{torange}{RGB}{255,127,14}
\definecolor{tgreen}{RGB}{44,160,44}
\definecolor{tred}{RGB}{214,39,40}
\definecolor{tpurple}{RGB}{148,103,189}

\newcommand{\hide}[1]{} %hide
\newcommand{\vpara}[1]{\vspace{0.05in}\noindent\textbf{#1 }}
\newcommand{\noind}[1]{\hspace{-0.05in}\noindent{#1}}
\newcommand{\etal}{\textit{et al}.}
\newcommand{\beq}[1]{\vspace{-0.03in}\begin{equation}#1\end{equation}\vspace{-0.02in}}
\newcommand{\beqn}[1]{\vspace{-0.03in}\begin{eqnarray}#1\end{eqnarray}\vspace{-0.03in}}
\newcommand{\besp}[1]{\begin{split}#1\end{split}}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\ie}{\textit{i}.\textit{e}.}
\newcommand{\eg}{\textit{e}.\textit{g}.} 
\newcommand{\wrt}{\textit{w}.\textit{r}.\textit{t}} 
\newtheorem{Dfn}{Definition}
\newtheorem{Exa}{Example}

\def\model{SimRec}
% \def\full{Graph Masked Autoencoding Transformer}

\setcopyright{none}
%\setcopyright{acmcopyright}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}

% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation emai}{June 03--05,
%   2018}{Woodstock, NY}
% \acmPrice{15.00}
% \acmISBN{978-1-4503-XXXX-X/18/06}


\begin{document}
\fancyhead{}

% \title{Self-Supervised Graph Transformer via Adaptive Masked Autoencoding for Recommendation}

% \title{Adaptive Masked Graph Transformer for Recommendation}

% \title{Graph-less Collaborative Filtering with \\ Contrastive Knowledge Distillation}

\keywords{Collaborative Filtering, Graph Neural Network, Contrastive Learning, Knowledge Distillation, Recommender Systems}

\copyrightyear{2023}
\acmYear{2023} 
\setcopyright{acmlicensed}\acmConference[WWW'23]{Proceedings of the ACM Web Conference 2023}{April 30-May 4, 2023}{Austin, TX, USA}
\acmBooktitle{Proceedings of the ACM Web Conference 2023 (WWW'23), April 30-May 4, 2023, Austin, TX, USA}
\acmPrice{15.00}
\acmDOI{10.1145/3543507.3583196}
\acmISBN{978-1-4503-9416-1/23/04}

\title{Graph-less Collaborative Filtering}

% \author{Anonymous Author(s)}

\author{Lianghao Xia}
\affiliation{The University of Hong Kong}
\email{aka\_xia@foxmail.com}

\author{Chao Huang}
\authornote{Chao Huang is the corresponding author.}
\affiliation{The University of Hong Kong}
\email{chaohuang75@gmail.com}

\author{Jiao Shi}
\affiliation{South China University of Technology}
\email{yjjiaoshi@scut.edu.cn}

\author{Yong Xu}
\affiliation{South China University of Technology}
\email{yxu@scut.edu.cn}


%%
%% The abstract is a short summary of the work to be presented in the
%% article.

\begin{abstract}
Graph neural networks (GNNs) have shown the power in representation learning over graph-structured user-item interaction data for collaborative filtering (CF) task. However, with their inherently recursive message propagation among neighboring nodes, existing GNN-based CF models may generate indistinguishable and inaccurate user (item) representations due to the over-smoothing and noise effect with low-pass Laplacian smoothing operators. In addition, the recursive information propagation with the stacked aggregators in the entire graph structures may result in poor scalability in practical applications. Motivated by these limitations, we propose a simple and effective collaborative filtering model (\model) that marries the power of knowledge distillation and contrastive learning. In \model, adaptive transferring knowledge is enabled between the teacher GNN model and a lightweight student network, to not only preserve the global collaborative signals, but also address the over-smoothing issue with representation recalibration. Empirical results on public datasets show that \model\ archives better efficiency while maintaining superior recommendation performance compared with various strong baselines. Our implementations are publicly available at: \url{https://github.com/HKUDS/SimRec}.
\end{abstract}


% \begin{abstract}
% Graph neural networks (GNNs) have emerged as the state-of-the-art paradigm for collaborative filtering (CF). To further improve the representation quality, contrastive learning over graph neural networks has attracted attention in recommendation, because of their effectiveness in encoding embeddings from sparse interaction data. However, the success of most existing contrastive learning-enhanced graph CF models largely relies on mannuly generating the effective contrastive views in the auxiliary self-supervised learning task. This results in the learned user/item representations suboptimal for the downsteam recommendation task, and difficult to be adaptive for data augmentation and robust to noise perturbation. In this work, we propose a \underline{A}daptive \underline{M}asked \underline{G}raph \underline{T}ransformer (\model) to tackle this challenge for adaptive self-supervised graph augmentation. Specifically, we propose to focus on the self-supervised reconstruction with a learnable masking paradigm that benefits the automated distillation of important and task-relevant self-supervised signals. To enhance the representation discrimination ability, our masked graph autoencoder is built over the graph transformer architecture to aggregate global information for reconstructing the masked subgraph structures. Extensive experiments and ablation studies on several public datasets demontrate the superiority of the proposed \model\ model against various state-of-the-art recommendation solutions across different settings.
% \end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>10010520.10010553.10010562</concept_id>
%   <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010575.10010755</concept_id>
%   <concept_desc>Computer systems organization~Redundancy</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010553.10010554</concept_id>
%   <concept_desc>Computer systems organization~Robotics</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10003033.10003083.10003095</concept_id>
%   <concept_desc>Networks~Network reliability</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Computer systems organization~Embedded systems}
% \ccsdesc[300]{Computer systems organization~Redundancy}
% \ccsdesc{Computer systems organization~Robotics}
% \ccsdesc[100]{Networks~Network reliability}


% \keywords{datasets, neural networks, gaze detection, text tagging}

\maketitle

\input{intro}
%\clearpage
\input{model}
\input{solution}
\input{eval}
\input{relate}
\input{conclusion}

\clearpage
\bibliographystyle{ACM-Reference-Format}
\balance
\bibliography{full_refs}

\clearpage
\input{appendix}



\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
