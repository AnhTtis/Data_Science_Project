\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{color,xcolor}
\usepackage{amssymb}
\usepackage[ruled, vlined]{algorithm2e}

%\newcommand*{\red}{\textcolor{red}}
\newcommand*{\red}{\textcolor{black}}

\begin{document}

\title{Cross-Modal Causal Intervention for Medical Report Generation}

\author{Weixing~Chen, Yang~Liu,~\IEEEmembership{Member,~IEEE}, Ce~Wang, Jiarui~Zhu, Shen~Zhao, Guanbin~Li,~\IEEEmembership{Member,~IEEE}, Cheng-Lin Liu,~\IEEEmembership{Fellow,~IEEE}, and~Liang~Lin,~\IEEEmembership{Fellow,~IEEE}
\thanks{This work is supported in part by the National Key R\&D Program of China under Grant No.2021ZD0111601, in part by the National Natural Science Foundation of China under Grant No.62002395 and No.61976250, in part by the Guangdong Basic and Applied Basic Research Foundation under Grant No.2023A1515011530, No.2021A1515012311 and No.2020B1515020048, and in part by the Guangzhou Science and Technology Planning Project under Grant No. 2023A04J2030. (\emph{Corresponding author: Yang Liu.})} 
\thanks{Weixing Chen, Yang Liu, Guanbin Li and Liang Lin are with the School
of Computer Science and Engineering, Sun Yat-sen University, China. \protect
(E-mail: chen867820261@gmail.com, liuy856@mail.sysu.edu.cn, liguanbin@mail.sysu.edu.cn, linliang@ieee.org)}
\thanks{Ce Wang is with the Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China. (E-mail: fogever@icloud.com)}
\thanks{Jiarui Zhu is with the Hong Kong Polytechnic University. (E-mail: jiarui.zhu@connect.polyu.hk)}
\thanks{Shen Zhao is with the School of Intelligent Systems Engineering, Sun Yat-sen University, China. (E-mail: z-s-06@163.com)}
\thanks{Cheng-Lin Liu is with the State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China, and also with the School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China. (liucl@nlpr.ia.ac.cn)}
}

%\markboth{IEEE Transactions on Image Processing}%
%{How to Use the IEEEtran \LaTeX \ Templates}

\maketitle

\begin{abstract}
 Medical report generation (MRG) is essential for computer-aided diagnosis and medication guidance, which can relieve the heavy burden of radiologists by automatically generating the corresponding medical reports according to the given radiology image. However, due to the spurious correlations within image-text data induced by visual and linguistic biases, it is challenging to generate accurate reports reliably describing lesion areas. Moreover, the cross-modal confounders are usually unobservable and challenging to be eliminated \red{explicitly}. In this paper, we aim to mitigate the cross-modal data bias for MRG from a new perspective, i.e., cross-modal causal intervention, and propose a novel Visual-Linguistic Causal Intervention (VLCI) framework for MRG, which consists of a visual deconfounding module (VDM) and a linguistic deconfounding module (LDM), to implicitly mitigate the visual-linguistic confounders by causal front-door intervention.
 % 缺少极具泛化性的语义提取方法？ —> VDM -> 不需要更细粒度的昂贵的注释
 Specifically, due to the absence of a generalized semantic extractor, the VDM explores and disentangles the visual confounders from the patch-based local and global features without expensive fine-grained annotations. 
 % 难以涵盖整个医学领域的知识图谱? -> LDM -> 无需构建指定的医学术语库
 Simultaneously, due to the lack of knowledge encompassing the entire \red{field of} medicine, the LDM eliminates the linguistic confounders caused by salient visual features and high-frequency context without constructing a terminology database. Extensive experiments on IU-Xray and MIMIC-CXR datasets show that our VLCI significantly outperforms the state-of-the-art MRG methods. The code and models are available at \url{https://github.com/WissingChen/VLCI}. 
\end{abstract}

\begin{IEEEkeywords}
Medical report generation, causality, visual-language pre-training model.
\end{IEEEkeywords}


\section{Introduction}

%\begin{figure}[t]
% \centering
%    \includegraphics[width=1\linewidth]{figure/intro_bias_info.pdf}
%    \vspace{-25pt}
%    \caption{The example of visual-linguistic spurious correlation on the IU-Xray dataset, where the colored texts in images are from the medical reports and also describe as the colored box in images. The red box and tag mean the abnormal region, while the blues mean the normal region.(a-b) illustrate the ground truth of two distinct MRG samples from the training set, while (c) displays a sample from the testing set. Spurious correlations arise from the visual and linguistic confounders in (a-b). Specifically, the visual feature of the heart and the description of heart size, lead to an incorrect description of (c). Additionally, (d) shows the distribution difference between the training and testing sets and (e) demonstrates that the causal relations between cardiomegaly (i.e., enlarged heart) and pleural effusion are various in different situations. Specifically, the occurrences of ``cardiomegaly $\to$ pleural effusion," ``pleural effusion $\to$ cardiomegaly," and ``potential variables $\to$ occurrence of both" are separately presented in (i.-iii.), which are different causal relations considering the individual differences.
%    }
%        \vspace{-10pt}
%    \label{fig:intro_bias_info}
%\end{figure}

\IEEEPARstart{M}{edical} images (e.g., X-Ray, MRI) are widely used in clinical procedures ~\cite{yu2022crosslink%,tang2022matr
}, providing significant evidence for disease analysis and medical diagnosis~\cite{zhou2021review}.
Nevertheless, observing suspicious lesions and writing a coherent diagnosis report is time-consuming, even for experienced radiologists. Furthermore, inexperienced radiologists often fail to capture tiny abnormalities due to the high requirement for clinical knowledge. 
To relieve this issue,
Medical Report Generation (MRG) has emerged and attracted growing interest in recent years~\cite {tanida2023interactive}. 
MRG extracts features from medical images and generates the corresponding reports, which is similar to image captioning~\cite{nguyen2022effective}. However, the current MRG faces three challenges that are significantly different from the image captioning task:
1) Longer sentence generation (60-100 tokens) tends to accumulate a larger bias, whereas image captions typically consist of fewer than 20 tokens~\cite{chen2020generating}. 
2) The necessity to capture all key regions in the medical image (i.e., abnormalities and diagnostic evidence) and a low tolerance for factually incorrect information~\cite{tanida2023interactive}. 
3) More sophisticated linguistic and visual semantic patterns require a proficient understanding of complex medical information, whereas entities in natural images are diverse and easily distinguishable~\cite{zhang2020radiology, zhou2021review}. 
Therefore, these challenges impose significant limitations on modeling visual-linguistic interactions and learning informative cross-modal representations for accurate MRG~\cite{chen2020generating}.

\begin{figure}[t]
 \centering
    \includegraphics[width=1\linewidth]{figure/intro_bias_info_v2.pdf}
    \vspace{-25pt}
    \caption{The example of visual-linguistic spurious correlation of MRG, where the tags (e.g., \red{``No Effusion", ``Normal Heart"}) represent different \red{findings}, and the boxes \red{represent} their regions. Blue color means normal findings while \red{orange} means abnormal ones. \red{(a) illustrates the combinations of cardiac enlargement and chest effusion findings, along with their distribution in the training dataset, with the majority representing normal cases. This data bias also results in (b) showcasing spurious correlation, leading to a reliance on spurious correlation for inference in (c). In (d), these spurious correlations encompass three scenarios, leading to wrong judgments. However, we encourage the model to focus on chest features (true causal relation) when assessing the presence of chest effusion.}
    % namely visual confounders, linguistic confounders, and cross-modal confounders. 
    %In this context, visual confounders can be regarded as visual features related to the cardiac region, linguistic confounders pertain to descriptions of a normal heart, and cross-modal confounders arise due to the emphasis on visual features of the cardiac region, leading to wrong judgments. Actually, we encourage the model to focus on chest features (true causal relation) when assessing the presence of chest effusion.}
    %(a) illustrates the ground truth of two distinct MRG samples from the training set, while (c) displays a sample from the testing set, and the box in a dashed line means the prediction from the non-causal model. 
    %Spurious correlations arise from the visual and linguistic confounders in (a-b). In (d), it demonstrates the various causal relations between ``abnormal {A}" and ``abnormal {B}" in different situations. 
    %Specifically, the causal graphs should consider individual differences and may indicate that ``abnormal {A} $\to$ abnormal {B}" or ``abnormal {A} $\gets$ abnormal {B}". Moreover,  the abnormal organ ``C" is a common cause that leads to ``abnormal {A}" and ``abnormal {B}". 
    %are separately presented in (i.-iii.), which are different causal relations considering the individual differences.
    }\vspace{-15pt}
    \label{fig:intro_bias_info}
\end{figure}


To tackle the aforementioned challenges, current MRG methods have made significant efforts, such as the memory-driven module for longer sentence generation~\cite{chen2020generating}, additional knowledge for more accurate description~\cite{liu2021exploring}, and contrast with normal samples (i.e., images without lesion areas) for the capture of abnormalities (i.e., lesion areas within images)~\cite{liu2021contrastive}. 
Actually, most of the previous MRG methods aim to capture the latent subtle differences in images (visual biases) and learn a concise set of key descriptions in the text (linguistic biases) for accurate long-sequence generation. Moreover, these methods usually focus on training computationally expensive models based on a large number of region-level annotations~\cite{
%sun2022lesion, 
tanida2023interactive} and task-specific knowledge\footnote{These methods build the template or knowledge database laboriously, making it hard to transfer those approaches directly to other datasets~\cite{yang2022knowledge}.}, rather than mitigating cross-modal biases. Nevertheless, significant visual and linguistic biases exist in numerous image-text data, as shown in Fig. \ref{fig:intro_bias_info}. Therefore, lightweight models that can mitigate the cross-modal data bias are essential for MRG to accurately discover abnormalities and generate reliable reports.


%\begin{figure}[t]
%\centering
%    \includegraphics[width=1\linewidth]{figure/intro_deconfounding.pdf}
%        \vspace{-25pt}
%    \caption{The mechanism of front-door causal intervention via the structural causal model (SCM). The example from Fig.~\ref{fig:intro_bias_info} shows the patient suffering from pleural effusion with a normal heart. According to the SCM, the link between Z and F can be cut off by the mediator, enabling the intervention and adjustment of the relation between F and R, thereby revealing the true causal relation between the two variables. Specifically, the non-causal model tends to capture the spurious correlations and probability, while our VLCI estimates the mediator by accumulating the probability of each sub-distribution and calculating the deconfounded probability of the correct word.
%    }
%        \vspace{-15pt}
%    \label{fig:intro_deconfounding}
%\end{figure}

%%%%%%%%% abnormal -> causal (unbias and help long sequence generation)
% spurious correlations -> front door 更笼统一点
Moreover, the most significant difficulty in MRG is the existence of visual and linguistic biases that lead to entangled cross-modal features, i.e., spurious correlations, causing the incorrect report in the model prediction with confounders.
This means that the complexity of the medical diagnosis requires the discovery of true causal relations rather than spurious correlations, especially for medical diagnosis when considering \red{rare cases.}
% individual differences. 
Actually, the majority of samples in medical reports consist of normal individuals, while there are a limited number of abnormal samples that require comprehensively considering various organs to determine pathological changes in other regions. 
Additionally, there exist significant visual-linguistic biases in the training sets, which makes the MRG model predict wrong reports in the testing set when relying on the spurious correlations in the training set, 
\red{as shown in Fig.~\ref{fig:intro_bias_info}(a-b). This indicates the necessity to uncover true causal relations. Otherwise, spurious correlations may be employed, leading to an inaccurate estimation of ``No Effusion" instead of ``Pleural Effusion", as shown in Fig.~\ref{fig:intro_bias_info}(c-d). Specifically, visual confounders can be regarded as visual features related to the cardiac region; linguistic confounders pertain to descriptions of a normal heart; and cross-modal confounders arise due to the emphasis on visual features of the cardiac region, leading to wrong judgments.}
    
%as shown in Fig.~\ref{fig:intro_bias_info}(a-b). This indicates the existence of complex causal relations among abnormalities, making it necessary to comprehensively consider the conditions of each variable for an accurate diagnosis, as shown in Fig.~\ref{fig:intro_bias_info}(d). Otherwise, false correlations may be adopted, resulting in erroneous estimations of ``normal {B}" rather than ``abnormal {B}" (Fig.~\ref{fig:intro_bias_info}(c)).
%Since the potential visual and linguistic correlations are complicated in MRG, there exist significant differences in visual and linguistic biases between the training and testing sets. 

%,niu2021counterfactual
To mitigate the visual-linguistic bias, causal inference \cite{pearl2016causal,liu2022causal} has shown promising performance in image classification \cite{yue2020interventional}, image question answering \cite{wang2020visual}, and image captioning \cite{liu2022show}. However, directly applying existing causal methods to the MRG task may yield unsatisfactory results due to the unobservable confounders in the visual and linguistic domain and the complex visual-linguistic interaction in medical images and textual reports.
%%%%%%%%%%%%%%%%%
Although back-door intervention can cut off the shortcut path, it requires approximating the observable confounders using a well-trained visual object detector or a well-constructed linguistic dictionary.
Fortunately, causal front-door intervention gives a feasible way to calculate unobservable confounders and mitigate visual-linguistic spurious correlations. 
%%%%%%%%%%%%%%%%%%
With causal front-door intervention, we can eliminate the spurious cross-modal correlations effectively by introducing an additional mediator~\cite{liu2023cross, yang2021causal} and generate an accurate description of \red{``Pleural Effusion"}. 
The mediator can be assumed as the \red{feature of ``Pleural" in different findings (e.g., ``Normal Heart")} to estimate the sub-distribution of \red{``Pleural Effusion"}~\cite{yang2021deconfounded}. 
However, the accurate and reasonable acquisition of the mediator is challenging, especially without the support of additional medical knowledge.

% motivation
Motivated by the effectiveness of causal inference in deconfounding the cross-modal bias, we propose a lightweight cross-modal causal intervention framework for MRG without the observable confounders assumption, named Visual-Linguistic Causal Intervention (\textbf{VLCI}), to mitigate the visual and linguistic data biases. 
Specifically, we propose Prefix Language Modeling (PLM) and degradation-aware Masked Image Modeling (MIM) for cross-modal pre-training. To mitigate the visual and linguistic biases, we propose the visual deconfounding module (\textbf{VDM}) and linguistic deconfounding module (\textbf{LDM}) based on the causal front-door intervention paradigm.
% detail
In VDM, the visual mediator is constructed by local detail information (e.g., lung texture) and global contour (e.g., lung contour) from medical images, to disentangle the visual features. The linguistic confounders can be eliminated by the LDM, which estimates the change in the probability of word embedding caused by visual details and linguistic context. 
In summary, our main contributions are listed as follows:

\begin{itemize}
    % 适用多种数据情况
    % causal 主要贡献， VDM和LDM分别针对的问题是什么
    \item  To implicitly mitigate cross-modal biases, we propose visual-linguistic causal front-door intervention modules VDM and LDM based on the Structural Causal Model. The VDM aims to disentangle the region-based features from images, and the LDM aims to eliminate the spurious correlations caused by the visual-linguistic embedding. 
    % vlp 预训练你是有改进现有方法？有的话要体现出来
    \item To alleviate the problem of unpaired data and capture detailed features when pre-training cross-modal data, we propose PLM and degradation-aware MIM for pre-training in various data situations (e.g., unpaired, single modality), which is efficient and easy to implement.
    % 为什么不需要额外的标注数据
    \item We propose a lightweight Visual-Linguistic Causal Intervention (VLCI) framework for MRG, which introduces mediators without additional knowledge, to implicitly deconfound the visual-linguistic features by causal front-door intervention. To the best of our knowledge, we are the first to conduct cross-modal causal intervention for MRG. Experiments show that we achieve state-of-the-art performance on IU-Xray and MIMIC-CXR datasets.
\end{itemize}

\section{Related Work}
%In this section, we begin with a brief review of image captioning and causal inference. Subsequently, we provide a comprehensive review of the related works on MRG, covering four aspects. Additionally, we summarize the differences between our methods and the prior related works.

\subsection{Image Captioning}
Image captioning aims to understand image information and describe it in text, which mainly adopts the encoder-decoder framework~\cite{devlin2018bert}.
Recent work has achieved significant success in this task, generally improving model performance from three aspects: visual representation learning, linguistic comprehension and generation, and training strategy~\cite{stefanini2022show}.
The global features extractor from CNNs leads to excessive compression of information and \red{lacks} granularity~\cite{karpathy2015deep}, thus,
%which are then used as inputs to the language model. However, this approach often resulted in excessive compression of information and lacked granularity, leading to the generation of generic and less detailed descriptions~\cite{karpathy2015deep}. Motivated by this drawback, % visual attention is interpreted as the relative importance of each grid feature for generating the next word~\cite{xu2015show}.
% object detector, graph
%Additionally, 
visual saliency % estimated by object detectors 
can further improve performance via the \red{spatial relation} of regional features and rely on the integration of visual and semantic data to improve performance~\cite{
%anderson2018bottom, li2019entangled, 
yu2018topic,jiang2022visual}.
% self-attention
% MRG中可以利用标签提取器做14类的分类（基于属性的），并以此继续生成报告
%Yu et al. propose to generate captions under a given topic~\cite{yu2018topic}. The topic candidates are extracted from the caption corpus, and it can provide general content for captions.
% 
% Zhou et al. aim to distill visual saliency, sample saliency, and semantic saliency without a saliency predictor for performance boosting~\cite{zhou2019re}.
% linguistic， LSTM，级联式LSTM， transformer
As for the linguistics features, generating the captioning from coarse to fine~\cite{wang2017skeleton} and integrating multi-modal information via multiple layers of LSTM~\cite{%anderson2018bottom, 
xian2019self} achieve a promising performance, but are limited by the training efficiency and expression ability. With the rise of transformer-based models~\cite{
%, radford2018improving
devlin2018bert}, intrinsic information of each modality is learned through self-attention, while cross-attention enables the learning of multi-modal information, thereby effectively enhancing the model's performance~\cite{nguyen2022grit, liu2022show}.
% The key to enhancing the model lies in the ability of language understanding and generation once sufficient visual information is obtained. Wang et al. proposed the use of LSTM to decompose the generation process into two stages: generating the main sentence first and then enriching the attribute description of the sentence from coarse to fine~\cite{wang2017skeleton}. Additionally, multiple layers of LSTM can be utilized to simultaneously integrate multi-modal information and generate accurate descriptions~\cite{anderson2018bottom, xian2019self}. With the rise of transformer-based models~\cite{devlin2018bert, radford2018improving}, coupled with image models like ViT, we can model image captioning as a sequence-to-sequence task. Within the transformer framework, intrinsic information of each modality is learned through self-attention, while cross-attention enables the learning of multimodal information, thereby effectively enhancing the model's performance~\cite{}.
% rl, comparison, Reinforcement Learning, Pre-Training{zhao2020cross}
Furthermore, the performance improvements can be achieved by the formulation of training strategies, including the learning order of the samples~\cite{zhou2019re}, reinforcement learning~\cite{%{liu2021vocabulary}, 
nguyen2022effective}, and visual-linguistic pre-training~\cite{yu2022coca, wang2021simvlm}. Compared with the image captioning approaches, the MRG has similar structures~\cite{stefanini2022show}. Nevertheless, image captioning usually generates \red{a} single sentence to describe the main entities, while the MRG focuses on potential subtle lesion areas in medical images and \red{generates} longer \red{sentences} from more sophisticated visual-linguistic semantics. %Especially, the focus is often on a tiny region within the medical image, and the generated report typically consists of multiple sentences describing normal conditions and one key sentence describing the abnormality.

%%%%%%%%%%%%%%%% 晚点再加 %%%%%%%%%%%%%%%%%
\subsection{Causal Inference}
As the visual-linguistic bias caused by the heterogeneity of multi-modal data, causality provides a new methodology to design robust models via the elimination of spurious correlation~\cite{pearl2016causal, pearl2018book,liu2022causal}. 
Causal inference estimates the hidden causal effects in the distribution while significantly improving the model's generalization. It mitigates confounders through back-door, front-door intervention, or counterfactual intervention. This approach substantially advances the performance of tasks, including image classification~\cite{yue2020interventional}, image semantic segmentation\red{~\cite{miao2023caussl}}, visual features representation~\cite{liu2022contextual, wang2021causal}, image captioning~\cite{yang2021deconfounded, liu2022show}, and VQA~\cite{liu2023cross}. Specifically, Wang et al.~\cite{wang2020visual} improved Faster R-CNN by causal back-door intervention to obtain a more robust object detection model, which improves the performance of VQA \red{\cite{zang2023discovering}} and image captioning\red{~\cite{liu2022show}}. 
% It employs the average image of masked region images for each category in the object detector as confounders and performs back-door intervention before the classifier.
% Furthermore, Liu et al.~\cite{liu2022show} utilized this Causal Faster R-CNN and incorporated the confounders from the former as visual confounders, and utilizing the label set as linguistic confounders in the transformer model, reducing the visual-linguistic bias.
% 
Furthermore, \red{it can also achieve deconfounding by simulating causal interventions based on human priors in modeling %. Yang et al. demonstrated superior performance in the re-identification task by separately modeling clothing and identity with a dual-branch network, allowing them to influence each other, thus addressing the inherent entanglement between clothing and identity
\cite{yang2023good}.}

However, the confounders are usually unobservable and elusive, thus front-door intervention and counterfactual intervention can be applied. Specifically, the implicit capture of potential confounders can be achieved by integrating the causal intervention mechanism into the attention module through cross-sample attention or self-annotation\red{~\cite{hu2021causal, yang2021causal, wang2021causal, xue2023variational}}.
%Yang et al. propose Causal Attention, which consisted of In-Sample Attention (IS-ATT) and Cross-Sample Attention (CS-ATT)~\cite{yang2021causal}. The CS-ATT can forcibly bring other samples into every IS-ATT, mimicking the causal intervention. 
Additionally, Liu et al. proposed event-level causal visual question answering utilizing visual front-door intervention, which uses attention to aggregate local and global causality-aware visual-linguistic representations~\cite{liu2023cross}.
Therefore, causal inference has achieved remarkable performance in cross-modal tasks~\cite{liu2022causal}. Compared with the previous works that address VQA or image captioning, we focus on a more challenging task MRG, which requires modeling complex visual-linguistic interaction in medical images and textual reports\red{~\cite{zhao2018causaltriad, miao2023caussl, li2023causally}}. And we propose a visual-linguistic front-door causal intervention that simultaneously eliminates spurious correlations from visual and linguistic confounders.

\begin{figure*}[h!]
   \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
\includegraphics[width=0.85\linewidth]{figure/method_vlci.pdf}
    \vspace{-15pt}
   \caption{
   \red{The pipeline of VLCI. The mechanism of front-door causal intervention via the structural causal model (SCM) (a) is implemented in the example from Fig.~\ref{fig:intro_bias_info}, in contrast to the non-causal model.
   % The example from Fig.~\ref{fig:intro_bias_info} shows the patient suffering from ``abnormal B" with a normal organ ``A". According to the SCM, the link between Z and F can be cut off by the mediator, enabling the intervention and adjustment of the relation between F and R, thereby revealing the true causal relation between the two variables. Specifically, the non-causal model tends to capture the spurious correlations and probability, while our VLCI estimates the mediator by accumulating the probability of each sub-distribution and calculating the deconfounded probability of the correct word.
   Our approach follows a two-stage pipeline: 1) Visual Linguistic Pre-training (VLP) (c) utilizes the Multiway Transformer (b) to learn multi-modal context and concepts, and achieve effective cross-modal alignment. 2) Visual-Linguistic Causal Intervention (VLCI) (d) consists of a pre-trained transformer, a Visual Deconfounding Module (VDM), and a Linguistic Deconfounding Module (LDM), 
   %In stage 1, VLP (c) efficiently integrates the benefits of Masked Image Modeling (MIM) and Prefix Language Modeling (PLM), enabling the learning of multi-modal context and concepts, and achieving effective cross-modal alignment.  
   %The structure of the Multiway Transformer allows the execution of various modules based on different modalities, facilitating alignment between visual and linguistic information, as demonstrated in (b).
   %VLP can be effectively employed in diverse data scenarios, as demonstrated in (c) and facilitated by the Multiway Transformer.
   %Subsequently, VLCI (d) 
   which is implemented after VLP to mitigate the cross-modal bias.
   The image embedding module employs the initial three blocks of ResNet101.
   Specifically, the VDM explores visual bias using local sampling and global sampling. The LDM estimates linguistic bias using a vocabulary dictionary and visual features.}
   %We implement the weight transfer method as demonstrated in (e), in which the transformer model is executed similarly to PLM.
   }
       \vspace{-10pt}
   \label{fig:method_vlci}
\end{figure*}

\subsection{Medical Report Generation}
Recently, MRG methods have followed the works of image captioning and have shown remarkable performance. % For the issues above, the knowledge-aware module~\cite{liu2021exploring, yang2022knowledge, wang2022medical}, template retrieval module~\cite{liu2021contrastive}, and memory-driven module~\cite{chen2020generating, nooralahzadeh2021progressive} are used to generate useful reports. However, it still has some limitations. 
% knowledge，不能泛化
However, the abnormal descriptions and lesion regions in patient samples only constitute a small portion, leading to the visual-linguistic bias in the MRG task. To address this issue, knowledge-aware methods \red{prevail} because they can explore and distill the knowledge to accurately identify abnormalities and describe them using appropriate terminology. 
%The knowledge employed in such approaches mainly falls into two categories: a general graph constructed by Zhang et al.~\cite{zhang2020radiology}
% , comprising a global node, 7 organs/tissues, and 20 findings (representing normal or disease-related keywords); 
%and RadGraph, a large-scale knowledge graph developed by Jain et al.~\cite{jain2021radgraph} based on the MIMIC-CXR dataset~\cite{johnson2019mimic}. 
Specifically, the bias from language can be mitigated by utilizing the general graph~\cite{zhang2020radiology} as prior knowledge~\cite{liu2021exploring, huang2023kiut}
%, and task-aware modeling can be achieved by extracting entities from the general graph and decomposing the task~\cite{wang2022inclusive}. 
However, fixed and limited knowledge is insufficient to address complex medical problems. Therefore, Yang et al.~\cite{yang2022knowledge} and Li et al.~\cite{li2023dynamic} employed RadGraph~\cite{jain2021radgraph} for knowledge retrieval and dynamic knowledge construction. Additionally, utilizing a well-trained label extractor or manually annotated region labels for classification tasks, and further incorporating them to assist in report generation, can also be considered as approaches leveraging external knowledge~\cite{you2021aligntransformer, wang2022cross, tanida2023interactive}.

% template，能泛化
Although the knowledge-based model is a promising direction, acquiring knowledge is expensive and challenging to transfer to other tasks. 
Therefore, %some approaches directly explore the intrinsic subtle differences and patterned information within the samples themselves. 
CA~\cite{liu2021contrastive} and CMCL~\cite{liu2022competence} explored the potential abnormal regions by comparing them with normal samples.
% extracts normal samples for comparison by constructing a template pool during training to uncover the minor differences between abnormal samples and the templates. Similarly,  devises a training strategy based on the complexity of the samples, initially learning from simple samples and gradually increasing the difficulty level. 
Moreover, knowledge can be retrieved or constructed using templates corresponding to similar images~\cite{yang2022knowledge, li2023dynamic}, which effectively mitigates the issue of representation distortion caused by excessive focus on knowledge.
%  CA~\cite{liu2021contrastive} and CMCL~\cite{liu2022competence} utilized the comparison of data differences to enact the training strategy, which needs a data pool to estimate distribution.
% memory，能泛化
Additionally, the template-based approach requires \red{several} samples for comparison, %which undoubtedly increases the deployment complexity of the model. Therefore, % , employing a memory module to implicitly capture the patterns of the samples is a viable solution.
memory-driven transformers can implicitly store the visual~\cite{nooralahzadeh2021progressive%, wang2022medical
} and linguistic~\cite{chen2020generating} context without templates.
% Chen et al.~\cite{chen2020generating} proposed to capture the context of long sentences via a memory-driven transformer without templates. Moreover, M2TR~\cite{nooralahzadeh2021progressive} and MSAT~\cite{wang2022medical} integrated mesh memory into attention for visual representation, which can implicitly store and retrieve information of the sample. 
However, considering only the context would lead to cross-modal bias. Thus, the models align the cross-modal information in the embedding space and incorporate it as memory, which can be retrieved by visual-linguistic features~\cite{chen2022cross, wang2022cross}. The visual-linguistic bias hinders the robustness and reliability of MRG. To mitigate the cross-modal bias, we introduce a lightweight VLCI approach that implicitly mitigates cross-modal confounders, uncovers the true cross-modal causality through causal front-door intervention, and reduces the need for additional annotation when discovering abnormalities.
% However, only considering the context and disregarding alignment would lead to cross-modal bias, thus, Chen et al.~\cite{chen2022cross} and Wang et al.~\cite{wang2022cross} models aligned the cross-modal information in the embedding space and incorporate them as memory, which can be retrieved by visual-linguistic features~\cite{chen2022cross, wang2022cross}.
% Unfortunately, these memory-based models tend to require more computational resources to model and tend to infer slowly. 



\section{Method} % (3.5)
% 1. Overview
% 定义问题，给定各种符号和缩写的定义。因为缺乏精心设计的数据集也没有通用的医学目标检测器，所以我们前门干预进行去混淆。而为了获得足够的先验也就是视觉-语言模态中的各种概念特征，我们在干预前进行模型的预训练。
% 2. VLP
% 为了获得视觉的概念，我们采用MAE的方式进行像素级语义的学习。另外，为了学习更为细致的特征，我们对图像输入进行了退化同时在cnn提取后的特征进行mask。
% 为了获得语言的概念，我们采用PLM的方式进行自回归式的预训练。PLM的perfix采用的是双向注意力，这是为了在学习获得概念的同时与视觉模态进行对齐。
% 3. Causal (重点是，mediator具体是什么)
% 在得到多模态的特征概念后，我们需要对其进行因果干预的微调。
% 开始推导公式同时表明与级联式特征提取的不同，我们是对原有特征的分布进行调整，后者是从原有特征中进一步提取特征。
% VDM采用全局与局部的特征融合作为mediator，全局信息相当于病灶的轮廓与位置等信息，而局部信息是当下最关键的几处位置的细节，如肺部阴影下的血管。
% LDM采用的mediator是通过全token与当前的视觉局部特征进行

In this section, we first introduce the visual-linguistic pre-training (VLP), followed by the two essential cross-modal causal intervention modules, i.e., the Visual Deconfounding Module (VDM) and the Linguistic Deconfounding Module (LDM). Then, we introduce how to integrate VDM and LDM into the VLCI for cross-modal causal intervention.

\subsection{Overview}
% 一个典型的MRG模型可以在给定一个medical image后生成一系列医学发现以及见解。如图所示，VLCI获得CXR图像I后，先利用视觉特征提取器得到hv并以此引导前缀文本的词向量hw生成下一个单词。但是，由于视觉模态与语言模态都存在混淆因子，我们在多模态信息输入到decoder前进行因果干预并通过去混淆的特征生成报告。然而因为缺乏精心设计的数据集以及通用的医学目标检测器，我们需要通过mediator进行前门干预。同时，mediator与混淆因子的估计都需要充足的数据先验，也就是视觉-语言模态中的各种概念特征。因此，我们在干预前需要对模型进行预训练。

%The example from Fig.~\ref{fig:intro_bias_info} shows the patient suffering from ``abnormal B" with a normal organ ``A". According to the SCM, the link between Z and F can be cut off by the mediator, enabling the intervention and adjustment of the relation between F and R, thereby revealing the true causal relation between the two variables. Specifically, the non-causal model tends to capture the spurious correlations and probability, while our VLCI estimates the mediator by accumulating the probability of each sub-distribution and calculating the deconfounded probability of the correct word.

A typical MRG model can generate a series of medical findings and insights given a medical image. As shown in Fig.~\ref{fig:method_vlci}, upon receiving the Chest X-ray image $I\in\mathbb{R}^{C\times H\times W}$, the VLCI first utilizes a visual feature extractor to obtain $h_v$, which guides the generation of the next word $w_i \in R$ from the prefix text embedding $h_w$. 
However, due to the presence of confounders $Z$ in both the visual and linguistic modalities, the non-causal model may capture the spurious correlations between \red{``Normal Heart"} and \red{``No Effusion"}, which causes the neglect of \red{``Pleural Effusion"} accompanied by \red{``Normal Heart"}, as shown in Fig.~\ref{fig:method_vlci}(a). Fortunately, the SCM of VLCI in Fig.~\ref{fig:method_vlci}(a), which shows the link between $Z$ and $F$ can be cut off by the mediators. The existence of mediators enables the intervention and adjustment of the relation between $F$ and $R$, thereby revealing the true causal relation between the two variables. 
Thus, we perform a causal intervention on the multi-modal feature prior to the decoder to estimate the deconfounded probability of the correct word. This estimation is achieved by aggregating the probabilities of each sub-distribution of mediators, as illustrated in Fig.~\ref{fig:method_vlci}(a).
Nevertheless, due to the absence of elaborate datasets and a well-trained feature extractor, we conduct a causal front-door intervention to eliminate the spurious correlations via mediator $M$ implicitly.
However, the estimation of both confounders and the mediator requires sufficient prior information, i.e., various visual-linguistic concepts. Therefore, we leverage the Visual-Language Pre-training (VLP) model to construct the correlation between the visual contexts and linguistic concepts before conducting the causal intervention.

%A typical MRG model takes a medical image $I\in\mathbb{R}^{C\times H\times W}$ as input and generates the corresponding report $R=\{w_1,w_2,…,w_n\}$ that contains critical information. 
%As illustrated in Fig.~\ref{fig:method_vlci}, the VLCI employs the transformer structure to model $P(R|I)=\sum_{i=1}^{n}P(w_{i}|h_v, h_w)$, where $h_v$ is the visual feature extracted by an encoder and guides the prefix word embedding $h_w$ to generate the next word $w_i$ with visual-linguistic deconfounding. 
%To ensure that the estimation of confounders $Z$ is caused by the prior $P(I)$ and $P(R)$, we leverage the Visual-Language Pre-training (VLP) model to construct the correlation between the visual contexts and linguistic concepts. 
%Meanwhile, due to the absence of a knowledge graph and a well-trained feature extractor. We innovatively leverage causal front-door intervention to eliminate implicitly the spurious correlations from visual and linguistic modalities, and it is integrated into VDM and LDM, respectively.

\subsection{Visual-Linguistic Pre-training (VLP)}
%%%% motivation
% procedure, overall
% 难点是非配对和各异的区域特征（病灶形态多样，很难区分）
% 在多模态医学图像的预训练框架中，存在两个难点：（1）非配对数据，即只有一种模态的数据集很难被利用到有监督学习中（2）由于相同病变的形态差异很大，因此数据的异质性使得区分区域特征变得困难。由于跨模态预训练可以不依赖区域标签的前提下提供了细粒度的区域特征，我们采取了跨模态的预训练方式，学习并对齐多模态信息。
% 为了获得视觉的概念，我们采用MIM的方式进行像素级语义的学习。另外，为了学习更为细致的特征，我们对图像输入进行了退化同时在cnn提取后的特征进行mask。为了获得语言的概念，我们采用PLM的方式进行自回归式的预训练。PLM的perfix采用的是双向注意力，这是为了在学习获得概念的同时与视觉模态进行对齐。


In the medical pre-training framework, there exist two difficulties: (1) the unpaired data that only has a single modality is hard to be utilized in supervised learning, and (2) heterogeneous data that makes it difficult to distinguish the region features because the morphology of the same lesion varies greatly~\cite{zhou2021review}. 
To address these challenges and provide fine-grained region features without region labels, we employ a cross-modal pre-training approach to learn and align multi-modal information~\cite{xvlm}, as shown in Fig.~\ref{fig:method_vlci}(c). 
For language concepts, we employ Prefix Language Modeling (PLM) to align with the visual modality utilizing bidirectional attention in image and prefix text.
Meanwhile, to acquire visual context, we utilize Masked Image Modeling (MIM) for pixel-level semantic learning. Additionally, to capture more detailed features, we utilize degradation of the image and masking the features extracted by CNN. 
% In the medical pre-training framework, there exist two difficulties: (1) The unpaired data that only has a single modality is hard to be utilized in supervised learning, (2) heterogeneous data that makes it difficult to distinguish the region feature because the morphology of the same lesion varies greatly~\cite{zhou2021review}.
% Since the cross-modal pre-training provides fine-grained regional features without regional label~\cite{xvlm}, we utilize PLM and MIM in linguistic and visual modeling to deal with unpaired data.
% 所以结合多模态预训练，实现V-T, T-V, V-V, T-T这几种情况
Therefore, we use a multiway transformer to extract multi-modal features and 
two linear layers to solve PLM and MIM tasks, respectively~\cite{wang2021simvlm, he2022masked}. In each block of the multiway encoder, the attention layer is weight-shared while the two feed-forward layers handle the corresponding modal features respectively~\cite{wang2022image}. Similarly, each block of the multiway decoder consists of a weight-shared self-attention layer, a pool of feed-forward networks used for different modalities, as shown in Fig.~\ref{fig:method_vlci}(b).

% SimVLM -> Prefix Language Model
\subsubsection{PLM}Motivated by the work of SimVLM~\cite{wang2021simvlm}, we extract image features from the first three blocks of ResNet101~\cite{he2016deep} as prefix tokens in the PLM. Simultaneously, the text is randomly divided into two parts, with one part generated by another under the guidance of the obtained image tokens. In cases where the corresponding image is missing, the PLM can still be trained using only the text, similar to SimVLM. Let $h_v \in \mathbb{R}^{\frac{HW}{P^2}\times d}$ denote the image token extracted from the raw image $I$, where $P$ represents the patch size and $d$ is the embedding size. Then, $\{{w}_{{np}}, \ldots, {w}_{n}\}$ represents the postfix sequence following the textual description $h_w$ of length $n_p\ge 0$. Thus, the formulation is as follows:
\begin{equation}
    \mathcal{L}_{\textrm{PLM}}(\theta) = -\sum^{n}_{i=n_p}logP_\theta(w_{i}|h_v, h_{w_{<n_p}}),
\end{equation}
Here, $\theta$ denotes the trainable parameters of the model, $h_v$ represents the visual embedding with a trainable 2D positional encoding, $h_w$ is learned based on a fixed vocabulary and serves as the prefix received by the encoder, and $n$ denotes the length of the report.

%Motivated by SimVLM~\cite{wang2021simvlm}, we extract image features from the first three blocks of ResNet101~\cite{he2016deep} as prefix tokens in PLM. Simultaneously, the text is divided randomly into two parts, of which one is generated by another under the guidance of the obtained image tokens. When the corresponding image is absent, the PLM can also be trained with only text modality, which is the same as SimVLM. 
%Assume that $h_v\in\mathbb{R}^{\frac{HW}{P^2}\times d}$ is denoted as the image token extracted by the raw image $I$, where $P$ is the patch size, and $d$ is the embedding size. 
% Assume that $h_v\in\mathbb{R}^{k\times d}$ is denoted the image token extracted by the raw image $I$, where $k=\frac{HW}{P^2}$ is the length of the image tokens, $P$ is the patch size, and $d$ is the embedding size. 
%Then% $\{w_{i},\}_{i={L}_{p}}^{L}$ 
%$ \{{w}_{{n}_{p}}, \ldots, {w}_{n}  \}$
%is the postfix sequence after the textual description $h_w$ of length $n_p\ge 0$. Thus, the formulation is as follows:
% In this proxy task, we considered images as prefixes and the training objective becomes:
%\begin{equation}
%    \mathcal{L}_{\textrm{PLM}}(\theta) = -\sum^{n}_{i=n_p}logP_\theta(w_{i}|h_v, h_{w_{<n_p}}),
%\end{equation}
%where $\theta$ is the trainable parameters of the model, $h_v$ is the visual embedding with a trainable 2D positional encoding, $h_w$ is learned for a fixed vocabulary and received by the encoder as the prefix, and $n$ is the report length. 

% Mask Image Model
\subsubsection{MIM}To handle unpaired images, we leverage the Masked Image Modeling (MIM) paradigm~\cite{he2022masked}. 
Furthermore, since MIM can be trained using pairwise data~\cite{geng2022multimodal}, the missing semantics of masked images can be complemented by text, thereby enhancing cross-modal association. Additionally, we learn medical image representations by reconstructing high-resolution patches from low-resolution inputs, which can encode more local information into latent embeddings~\cite{zhou2023advancing}.
Consequently, we sub-sample the images for degradation before the visual embedding and reconstruct the masked visual token extracted from CNNs by incorporating the semantics of both the unmasked visual token and the linguistic token.
This approach enables us to capture subtle differences in the dataset~\cite{seo2022masked}. The objective of MIM can be formulated as:
\begin{equation}
    \mathcal{L}_{\textrm{MIM}}(\theta) = P_\theta(h_{vm}|h_{vv}, h_w),
\end{equation}
where ${h}_{vm}$ represents the masked visual tokens extracted by the ResNet backbone, ${h}_{vv}$ refers to the unmasked tokens, and $h_w$ corresponds to the word tokens of the entire report. %Subsequently, the ResNet and the multi-modal transformer encoder are employed as the Visual-Textual Representation Learning Model (VRLM) in downstream tasks.

% To further deal with unpaired images, we take advantage of \textbf{MIM}
%To deal with unpaired images like MAE~\cite{he2022masked}, we take advantage of the MIM paradigm. Additionally, since the MIM is trained with pairwise data, the missing semantics of masked images can be provided by text to enhance the cross-modal association~\cite{geng2022multimodal}. 
%Thus, we reconstruct the masked visual token via the semantics of the unmasking visual token and linguistic token, which can learn the tiny difference in the dataset~\cite{seo2022masked}. 
%The target of the MIM can be formulated as follows:
%\begin{equation}
%    \mathcal{L}_{\textrm{MIM}}(\theta) = P_\theta(h_{vm}|h_{vv}, h_w),
%\end{equation}
%where ${h}_{vm}$ denotes the masked visual tokens extracted by the ResNet backbone, ${h}_{vv}$ is the unmasked tokens, and $h_w$ denotes the word tokens of the whole report. Then the ResNet and the multi-modal transformer encoder are utilized as VRLM in the downstream tasks.

\subsection{Visual-Linguistic Causal Intervention}
% Causal (重点是，mediator具体是什么)
% 在得到多模态的特征概念后，我们需要对其进行因果干预的微调。
% 开始推导公式同时表明与级联式特征提取的不同，我们是对原有特征的分布进行调整，后者是从原有特征中进一步提取特征。
% VDM采用全局与局部的特征融合作为mediator，全局信息相当于病灶的轮廓与位置等信息，而局部信息是当下最关键的几处位置的细节，如肺部阴影下的血管。
% LDM采用的mediator是通过全token与当前的视觉局部特征进行
After the Visual-Linguistic pre-training (VLP), the learned visual-linguistic feature encoders still contain visual and linguistic biases from cross-modal confounders~\cite{yang2021deconfounded}. 
Therefore, we further employ front-door causal intervention to discover the causal effect between visual and linguistic modalities during MRG, as shown in Fig.~\ref{fig:method_vlci}(d).

\subsubsection{Preliminaries}
To clarify the mechanism of causal intervention, we introduce Pearl’s Structural Causal Model (SCM)~\cite{pearl2016causal}, as shown in Fig.~\ref{fig:method_scm}. The SCM is a mathematical framework used in causal inference to model the relations among variables and determine cause and effect relations. The SCM uses directed acyclic graphs (DAGs) to represent causal systems, where each variable is a node and the arrows represent causal relations between the variables.

\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{figure/method_scm.pdf}
      \vspace{-20pt}
  \caption{The structural causal model (SCM) depicted in (a) shows that $Y$ is caused by $X$. However, when two or more variables or events are influenced by a common cause, it leads to a spurious correlation, involving confounders $Z$ as shown in (b).
  % The MRG utilizes the structural causal model (SCM) depicted in (a). However, the presence of multi-modal confounders impacts the characterization of multi-modal features and influences the generation of reports as shown in (b).
  In (c), the back-door causal intervention is demonstrated to calculate the probabilities of observable confounders $Z$, effectively blocking the back-door path $Z \to X$, and obtaining the true probability of $Y$.
  Furthermore, when dealing with unobservable confounders, the front-door causal intervention can estimate the mediator $M$ and subsequently block the path $Z \to X \to M$.
  In our proposed approach (e), we decompose the \red{cross-modal} confounders into visual ($Z_v$) and linguistic ($Z_l$) components. The front-door causal intervention is implemented by the mediators $M_v$ and $M_l$, effectively blocking the paths $Z_v \to h_v$ and $Z_l \to h_w$, thereby preventing the back-door paths $h_v\gets Z_v \to R$ and $h_v \gets h_w\gets Z_l \to R$. Finally, the structural causal model (SCM) of VLCI is obtained, as depicted in (f).}
      \vspace{-10pt}
  \label{fig:method_scm}
\end{figure}

In Fig.~\ref{fig:method_scm}(a), the chain structure $X \to Y$ represents the output $Y$ is affected by the input $X$, formulated as $P(Y|X)$. But the confounders $Z$ caused by data bias would lead to a spurious correlation, as shown in Fig.~\ref{fig:method_scm}(b). 
% Assume that organ ``A" is ``heart" and ``B" is ``pleural". 
This graph indicates that two causes (both X and Z) lead to the common effect or outcome (Y), such as the \red{cross-modal} confounders of \red{``Normal Heart"} and \red{cross-modal} feature of \red{``Pleural"} leading to a confounded estimation of \red{``No Effusion"} (the spurious correlation is \red{``Normal Heart"} and \red{``No Effusion"}). In this case, we formulate $P(Y|X)$ as:
\begin{equation}
\begin{aligned}
    P(Y|X) = \sum_{z} P(Y|X, Z=z)P(Z=z|X),
\end{aligned}
\end{equation}
where the confounders $Z$ generally brings about the observational bias via $P(z|X)$~\cite{liu2022show}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Assume that $F\triangleq\{h_v, h_w\}$, in Fig.~\ref{fig:method_scm}(a), the chain structure $F \to R$ represents the multi-modal feature affecting the report generation. In contrast, the multi-modal feature can be structured by the attended word feature and visual representation, which introduces the visual-linguistic confounders, as shown in Fig.~\ref{fig:method_scm}(b) ($F \gets Z\triangleq\{Z_v, Z_l\} \to R$). This fork structure indicates that two variables share a common effect or outcome, such as the word "normal" and the visual feature of the heart introducing the multi-modal confounders of ``normal heart" and leading to a confounded estimation of "pleural effusion". In this case, we formulate $P(R|I)$ as:
% 后门干预 -> 前门干预
To alleviate this issue, back-door causal intervention can be implemented by introducing the do calculus $do(\cdot)$~\cite{liu2022show, liu2023cross, lopez2017discovering, qi2020two}. We can calculate the probabilities of observable confounders and block the back-door path $Z \to F$, the interventional probability is as follows:
\begin{equation}
\begin{aligned}
    P(Y|do(X))=\sum_{z} P(Y|X, Z=z)P(Z=z),
    \label{eq:back_door_do}
\end{aligned}
\end{equation}
where $Z$ can be the learned RoI features of the heart and the attended word feature of \red{``Normal Heart"}. However, the back-door causal intervention is limited by the observability of confounders, and the front-door causal intervention provides an implicit method of deconfounding in Fig.~\ref{fig:method_scm} (d). To eliminate the unobservable confounder, we introduce the mediator $M$ to cut off the link $X \gets Z \to Y$.
The total probability $P(Y|do(X))$
% of Eq.~(\ref{eq:do__1}) 
can be represented as the following summation:
\begin{equation}
\begin{aligned}
    P(Y|do(X)=\sum_{m}P(Y|do(X),M=m)P(M=m|do(X)),
    \label{eq:front_door_do_1}
\end{aligned}
\end{equation}
where $M$ is introduced by $X$ without the back-door path. Thus, the intervention probability is equal to the conditional probability in the path $X \to M$~\cite{liu2023cross}. Besides, there is no direct causal path between $X$ and $Y$. In this way, the introduced summation in Eq.~(\ref{eq:front_door_do_1}) can be reformulated as:
\begin{equation}
\begin{aligned}
    &P(Y|do(X)\\
    &=\sum_{m}P(Y|do(X), do(M=m))P(M=m|X=x)\\
    &=\sum_{m}P(Y|do(M=m))P(M=m|X=x).
    \label{eq:front_door_do_2}
\end{aligned}
\end{equation}
To estimate $P(Y|do(M=m))$, we can apply the back-door intervention to cut off the link $M \gets X \gets Z \to Y$~\cite{liu2022show}. Therefore, we have the intervention probability formulation:
\begin{equation}
\begin{aligned}
    &P(Y|do(M=m))\\
    &=\sum_{\hat{x}}P(Y|do(M=m),X=\hat{x})P(X=\hat{x}|do(M=m))\\
    &=\sum_{\hat{x}}P(X=\hat{x})P(Y|X=\hat{x},M=m),
    \label{eq:front_door_do_3}
\end{aligned}
\end{equation}
where $\hat{x}$ is the selected features from $X$ not caused by $M$. Finally, we can calculate Eq.~(\ref{eq:front_door_do_2}) by applying Eq.~({\ref{eq:front_door_do_3}}):
\begin{equation}
\begin{aligned}
    &P(Y|do(M=m))=\\
    &\sum_{m}P(M=m|X=x)\sum_{\hat{x}}P(X=\hat{x})P(Y|X=\hat{x},M=m).
    \label{eq:front_door_do}
\end{aligned}
\end{equation}
However, existing front-door causal intervention methods employ diverse approaches for estimating mediators and conducting interventions. 
% 说明e
\red{In MRG, we construct the structural causal model (SCM) similar to Image Caption~\cite{liu2022show}, assuming that $h_v$ is the visual feature, $h_w$ is the linguistic feature of the attended word, and $h$ is the fusion feature from $h_v$ and $h_w$ ($h_v \to h \gets h_w$), leading to the generation of report $R$. 
Moreover, $h_v$ would be influenced by $h_w$ through cross-attention and makes $h_v \gets h_w$, as shown in Fig.~\ref{fig:method_scm} (e).
Following that, we divide the mediator into visual and linguistic modalities and introduce a Front-door Intervention Module (FIM) based on Equation~(\ref{eq:front_door_do}) to deconfound them.
}

% by dividing it into visual and linguistic modalities, as illustrated in Fig.~\ref{fig:method_scm} (e). Consequently, we introduce a Front-door Intervention Module (FIM) based on Equation~(\ref{eq:front_door_do}) to deconfound visual and linguistic modalities.

\subsubsection{Front-door Intervention Module (FIM)}

\begin{figure}[t]
 \centering
    \includegraphics[width=0.8\linewidth]{figure/method_fim.pdf}
        \vspace{-15pt}
    \caption{Illustration of the Front-door Intervention Module (FIM), which consists of two Attention Fusion layers with non-parameters.
    }
        \vspace{-15pt}
    \label{fig:method_fim}
\end{figure}

In Fig.~\ref{fig:method_scm} (e), the causal effects $h_v\to R$ and $h_w\to R$ are affected by the confounders $Z=\{Z_v,Z_l\}$ from back-door paths $h_v \gets Z_v\to R$ and $h_w \gets Z_l\to R$  ~\cite{liu2022show}, respectively. In our SCM, the non-interventional prediction can be expressed as:
\begin{equation}
\begin{aligned}
    &P(R|I) = P(R|h_v, h_w)\\
    &=\sum_{i=1}^{n}\sum_{z} P(w_{i}|h_v, h_w, Z=z)P(Z=z|h_v, h_w),
    \label{eq:mrg_non_do}
\end{aligned}
\end{equation}
% The confounders and mediator
where $Z$ brings the spurious correlation via $P(Z=z|h_v, h_w)$, leading to incorrect reports.
$h_v$ is the visual token from visual embedding, and $h_w$ is the linguistic token of the attended word from linguistic embedding.
\red{
Taking Fig.~\ref{fig:intro_bias_info} as an example, when $P(Z=\textrm{``Normal~Heart''}|h_v=\textrm{``Heart"}, h_w=\textrm{``Normal''})$ is large while $P(Z=\textrm{``Cardiomegaly"}|h_v=\textrm{``Heart''}, h_w=\textrm{``Normal"})$ is small in the training set, it tends to enlarge $P(R=\textrm{``No~Effusion"}|h_v, h_w, Z=\textrm{``Normal~Heart"})$ in the testing set. 
}
%shown in Fig.~\ref{fig:intro_bias_info}.
To mitigate visual-linguistic confounders and uncover the true causal structure, we apply causal front-door intervention by introducing mediator $M_v$ and $M_l$, respectively, as shown in Fig.~\ref{fig:method_scm} (f).
Generally, $Z_v$ is unobservable without a well-trained object detector, and the back-door path $h_v\gets Z_v \to R$ can be blocked by $M_v$ via learning the true causal effect $h_v \to M_v \to h\to R$. Similarly, the intervention on the back-door path $h_v\gets h_w\gets Z_l\to R$ can be implemented by calculating the $M_l$ without well-constructed confounders dictionaries. Thus, we can formulate Eq.~(\ref{eq:mrg_non_do}) as:
\begin{equation}
\begin{aligned}
    P(R|do(I))=P(R|do(h_v), do(h_w))
    \label{eq:mrg_do}
\end{aligned}
\end{equation}
% The confounders and mediator can be shown intuitively in Figure\ref{fig:method_vlci} (f).

% Since the front-door intervention can eliminate unobservable confounders, we integrate the Front-door Intervention Module (FIM) into VDM and LDM, as shown in Fig.~\ref{fig:method_vlci}. 

% \begin{algorithm}[t]
% \caption{Front-door Intervention.}\label{alg:alg1}
% \begin{algorithmic}
% \STATE 
% \STATE {\textsc{Attention Fusion}}$(\mathbf{Q}, \mathbf{K}, \mathbf{V})$
% \STATE \hspace{0.5cm}$ qk \gets \mathbf{Q}\mathbf{K}^T  $
% \STATE \hspace{0.5cm}$ score \gets \textbf{softmax}(qk)  $
% \STATE \hspace{0.5cm}$ out \gets score\mathbf{V}  $
% \STATE \hspace{0.5cm}\textbf{return}  $out $
% \STATE 
% \STATE {\textsc{Front-door Intervention Module}}$(\mathbf{F}, \mathbf{M})$
% \STATE \hspace{0.5cm}$ \mathbf{\hat{M}} \gets \textbf{Attention Fusion}(\mathbf{M}, \mathbf{F}, \mathbf{F})  $
% \STATE \hspace{0.5cm}$ \mathbf{F_{de}} \gets \textbf{Attention Fusion}(\mathbf{F}, \mathbf{M}, \mathbf{\hat{M}})  $
% \STATE \hspace{0.5cm}\textbf{return}  $\mathbf{F_{de}} $
% \end{algorithmic}
% \label{alg1}
% \end{algorithm}

% why
% \noindent \textbf{Front-door Intervention Module.}
%To cut off back-door paths $h_v \gets Z_v \to R$ and $h_w \gets Z_l \to R$ via $M_v$ and $M_l$ (SCM in Fig.~\ref{fig:method_scm}(f)), the front-door causal intervention can be formulated as:
%\begin{equation}
%\begin{aligned}
%    &P(R|do(h_v), do(h_w)) = \\
%    &\sum_{m}P(R|do(M=m))P(M=m|do(h_v), do(h_w)),
%    \label{eq:do}
%\end{aligned}
%\end{equation}
%where $M$ is the mediator containing $M_v$ and $M_l$. Since the intervention probability is equal to the conditional probability in the path $\{h_v, h_w\} \to M$, the back-door path between $M$ and $R$ can be blocked, and enlarge $P(R=\textrm{``small~pleural~effusion"}|h_v=\textrm{``heart"}, h_w=\textrm{``normal"}, M=\textrm{``pleural"})$, as the previously mentioned example, where the ``pleural" is the multi-modal feature of pleural in each condition of heart size.
%Consequently, we can assume that $\{h_v, h_w\}$ is feature $F$, and formulate Eq.~%(\ref{eq:do}) as:
%\begin{equation}
%\begin{aligned}
%    &P(R|do(h_v), do(h_w)) = P(R|do(F))=\\
%    &\sum_{m}P(M=m|F)\sum_{\hat{F}}P(F=\hat{F})P(R|F=\hat{F},M=m).
%    \label{eq:do_1}
%\end{aligned}
%\end{equation}
%To further estimate , we implement the front-door causal intervention Eq.~(\ref{eq:front_door_do}) with the deep learning framework.

To further estimate Eq.~(\ref{eq:mrg_do}) with the deep learning framework using Eq.~(\ref{eq:front_door_do}), we adopt Normalized Weighted Geometric Mean (NWGM)~\cite{xu2015show}
as:
\begin{equation}
\begin{aligned}
    P(R|do(h_v), do(h_w)) \approx \textrm{Softmax}(g(h_w, h_v, \hat{M_v}, \hat{M_l})).
    \label{eq:done}
\end{aligned}
\end{equation}
where $g(\cdot)$ denote the network mapping functions, $\hat{M_v}$ and $\hat{M_l}$ denote the estimations of $M_v$ and $M_l$ via VDM and LDM. 
% TODO 为什么这么设计FIM？
In Fig.~\ref{fig:method_fim}, the FIM consists of two Attention Fusion layers and it is integrated into VDM and LDM. Different from the cascaded transformer~\cite{nguyen2022grit}, FIM does not have optimized parameters. It relies on extracted features and intervenes using estimated mediators of the corresponding modalities. This approach ensures that the estimation of mediators is entirely dependent on the sampling in VDM and LDM rather than FIM, achieving implicit learning of mediators.

\begin{figure}[t]
 \centering
    \includegraphics[width=0.8\linewidth]{figure/method_vdm_ldm.pdf}
        \vspace{-15pt}
    \caption{Illustration of the Visual Deconfound Module (VDM) and linguistic Deconfound Module (LDM).
    }
        \vspace{-15pt}
    \label{fig:method_vdm_ldm}
\end{figure}

\subsubsection{Visual Deconfounding Module (VDM)}
% 为什么要用local和gobal
% local 的high attn token 代表了局部的细节特征，global是类似心脏轮廓和位置的全局性的特征
% 心脏轮廓影响了胸积液的判断，同时肺部的纹理细节也可以有助于判断
% In Fig.~\ref{fig:method_vlci}(d), we calculate the visual mediator $M_v$ via local feature $h_{vl}$ from local sampling and global feature $h_{vg}$ from global sampling. The $h_{vl}$ is denoted as the local detail information acquired from Local Sampling, while the $h_{vg}$ is the contours and position feature acquired from Global Sampling~\cite{sun2022lesion}. 
% For instance, the contour of the heart affects the determination of pleural effusion, and the texture of the lungs can also be the basis of detection.
In Fig.~\ref{fig:method_vdm_ldm}, the visual mediator $M_v$ is calculated using local features $h_{vl}$ obtained from local sampling and global features $h_{vg}$ obtained from global sampling.
The features $h_{vl}$ represent local details acquired from Local Sampling, while $h_{vg}$ represents contour and position features obtained from Global Sampling~\cite{sun2022lesion}.
For example, the contour of the heart influences the determination of pleural effusion, and the texture of the lungs can also serve as a basis for detection.

\noindent\textbf{Local Sampling.} 
Inspired by TransFG \cite{he2022transfg}, we leverage the attention accumulated from the encoder to select the top $k$ tokens. These selected visual tokens with high attention correspond to the report's keywords as $h_{vl}\in\mathbb{R}^{k\times d}$, where $k=6$ for each attention head, and $d$ is the dimension of the transformer. Subsequently, $h_{vl}$ is further enhanced using CaaM \cite{wang2021causal}, which excavates the local internal relations. Specifically, in CaaM, these highly attended tokens are computed not only with self-attention but also with negative attention scores, aiming to achieve more robust features. The purpose of $h_{vl}$ is to capture crucial local details in the image, which serve as the key basis for further processing.

% Inspire by TransFG \cite{he2022transfg}, we use the attention accumulated from the encoder to select top $k$ tokens. These selected visual tokens with high attention can correspond to the keywords of the report as $h_{vl}\in\mathbb{R}^{k\times d}$, where $k=6$ for each head of attention, and $d$ is the dimension of the transformer. Then, $h_{vl}$ is enhanced via CaaM \cite{wang2021causal}, which further excavates the local internal relations. Specifically, these highly attended tokens in CaaM are computed not only with self-attention but also with negative attention scores, aiming to achieve more robust features. The $h_{vl}$ aims to obtain local critical details in the image, which can be used as the key basis.

\noindent\textbf{Global Sampling.} The global sampling is implemented by Down Sampling Transformer block, in which the $14\times 14$ visual tokens are down-sampled to $7\times 7$ as $h_{vg}\in\mathbb{R}^{49\times d}$. Max pooling in this block can better retain the global structure information in the image as the general features of the data itself. We formulate the operation as follows:
\begin{equation}
    h_{vg} = W[\textrm{P}(h_v) + \textrm{Attn}(\textrm{P}(\textrm{LN}(h_v))],
    \label{eq:dst}
\end{equation}
where P is the 2d max pooling layer, LN is layer normalization, Attn is the 2d relative attention \cite{dai2021coatnet}, and $W$ denotes the weights of the linear layer.

\noindent\textbf{Local-Global Fuse Module.} 
Finally, the $h_{vl}$ is integrated with $h_{vg}$ to enhance local details with global structural information via Local-Global Fuse Module formulated as Eq.~(\ref{eq:LGFM}), namely mediator $M_v$. 
\begin{equation}
    M_v=\textrm{FFN}([\textrm{MHA}(h_{vl}, h_{vl}, h_{vl}), \textrm{MHA}(h_{vl}, h_{vg}, h_{vg})])
    \label{eq:LGFM}
\end{equation}
where MHA and FFN are the Multi-Head Attention layer and Feed-Forward Network layer, respectively. $[\cdot,\cdot]$ denotes concatenation.

\subsubsection{Linguistic Deconfounding Module (LDM)}
% Textual Confounder
For linguistic deconfounding, we have some observations from $h_v \gets h_w\gets Z_l\to R$ (Fig.~\ref{fig:method_scm} (e)): (1) the linguistic contexts can affect the generation of the next word, and (2) the attended word features affect the attended visual features via cross-attention~\cite{liu2022show}.
% why, 因为文本的上下文和来自视觉的高注意力特征都会影响文本的生成
% 为什么在embedding space做，词频的差异也会带来很大的距离偏差，因此词向量的距离就不能很好地代表语义相关性
Additionally, the difference in word frequency brings a large distance deviation in embedding space, so the distance of word vectors cannot represent semantic relevance well~\cite{li2020sentence}. 

\noindent\textbf{Visual-Linguistic Fuse Module.} 
Thus, we calculate the linguistic mediator $M_l$ in embedding space via all vocabularies from the tokenizer as the global features and use $h_{vl}$ obtained from the VDM, which estimates the current word frequency to adjust the distribution of $h_w$, see in Fig.~\ref{fig:method_vdm_ldm}. 
% formulation
\begin{equation}
\begin{aligned}
   &h^{'}_{vl} = \textrm{FFN}(\textrm{MHA}(h_{vl}, \hat{w}, \hat{w})); \\
   &M_t = \textrm{FFN}(\textrm{MHA}(h^{'}_{vl}, h_{vl}, h_{vl}))
\end{aligned}
\end{equation}
where $\hat{w}$ denotes all word tokens from the tokenizer. Then, we build the causal effect $ h_w \to M_l \to h_v \to M_v \to h \to R$ to cut off the back-door path $h_v \gets h_w\gets Z_l \to R$ via $M_l$.

In Fig.~\ref{fig:method_vlci}, the deconfounded visual and linguistic features are fed to the decoder to learn fused cross-modal features. The output layer is a linear projection with softmax operation, mapping probability into the dimensional equal to the vocabulary size. Finally, the training target is to minimize the negative log-likelihood loss according to Eq.~(\ref{eq:mrg_do}) and Eq.~(\ref{eq:done}):
\begin{equation}
\begin{aligned}
    &\mathcal{L}_{\textrm{nll}}(\theta) = -\sum_{i=1}^{n}log (\textrm{Softmax}(g(h_{w_{<i}}, h_v, \hat{M_v}, \hat{M_l})))
\end{aligned}
\end{equation}
where $n$ is the length of the report and $h_{w_{<i}}$ is the prefix text when estimating the word $w_i$.
\section{Experiment} %(7)
% 1. 实验设置，参数说明，CE Metric的说明（重点）
% 2. Main Result，这里只对比非知识图谱的方法，同时强调我们方法的轻量与快速。
% 3. 医学上的分析需要围绕CE Metric上进行，目前也是同规模最优的，同时超越了部分基于知识的方法。
% 4. ablation，同时在开源的方法中，我们即插即用的因果模块可以进一步提升原有模型 （MAE和PLM的ablation不是重点，舍去）
% 5. 在image caption的任务中同样有效
% 6. 进一步分析可视化的结果，以及生成的例子

\subsection{Experimental settings}


% model scale
\begin{table}[]\renewcommand\arraystretch{0.9}
  \centering
  \setlength{\tabcolsep}{0.7mm}{
    \caption{The details of VLCI and several compared RRG models, the \#Enc and \#Dec denote the number of transformer layers in the encoder and decoder, respectively. 
    The marker $\clubsuit$ means 2 Contrastive Attentions, and $\spadesuit$ means Hierarchical LSTM. 
    The backbone of VLCI is the first three blocks of Resnet101. 
    Besides, we show the employed model boosting modules, including the knowledge-aware module $\mathcal{K}$, template retrieval module $\mathcal{T}$, and memory-drive module $\mathcal{M}$.
    Additionally, we adopt two scales of the VLCI, $\textbf{VLCI}_3$ for the IU-Xray dataset and $\textbf{VLCI}_6$ for the MIMIC-CXR dataset.
    }
        \vspace{-10pt}
  \label{tab:model_scale}
  \begin{tabular}{@{}lllccccc@{}}
    \toprule
    &Method              & Visual Embedding &  \#Enc  &   \#Dec & $\mathcal{K}$ & $\mathcal{T}$ & $\mathcal{M}$ \\
    \midrule
    \multirow{5}{*}{\rotatebox{90}{Light}}
    &R2Gen\cite{chen2020generating}               & Resnet101         & 3 & 3 &  &  & $\surd$\\
    % chen2022cross
    &\red{R2GenCMN\cite{chen2022cross}}               & \red{Resnet101}         & \red{3} & \red{3} &  &  & \red{$\surd$}\\
    &CMCL\cite{liu2022competence}                & Resnet50          & - & $\spadesuit$ &  &  &  \\
    &PPKED\cite{liu2021exploring}               & Resnet152         & 2 & 1 & $\surd$ & $\surd$ &   \\
    &CA\cite{liu2021contrastive}                  & Resnet50          & $\clubsuit$ & $\spadesuit$ &   & $\surd$ &  \\
    &AlignTransformer\cite{you2021aligntransformer}    & Resnet50          & 3 & 3 & $\surd$  &   & $\surd$  \\
    &MMTN\cite{cao2023mmtn}                & DenseNet121              & 3 & 3 & $\surd$ &   & $\surd$ \\
    \midrule
    \multirow{3}{*}{\rotatebox{90}{Heavy}}
    &M2TR\cite{nooralahzadeh2021progressive}                & Densenet151       & 6 & 12 &   &  & $\surd$\\
    %Self-boost          & Resnet101         & 12 & $\spadesuit$ &   &   &    \\
    &MGSK\cite{yang2022knowledge}             & Resnet101         & 12 & 3 & $\surd$ & $\surd$ &  \\
    %&MSAT\cite{wang2022medical}                & CLIP              & 6 & 6 & $\surd$ &   & $\surd$ \\
    &RAMT\cite{zhang2023semi}                & DenseNet121              & 6 & 6 & $\surd$ &   &  \\
    &DCL\cite{li2023dynamic}                & ViT Linear Projection           & 25 & 1 & $\surd$ & $\surd$  &  \\
    \midrule
    &$\textbf{VLCI}_3$                & Resnet101*        & 3 & 3 &  &  &  \\
    &$\textbf{VLCI}_6$                & Resnet101*        & 6 & 6 &  &  &  \\
    \bottomrule
  \end{tabular}}
      \vspace{-10pt}
\end{table}


\begin{table}\renewcommand\arraystretch{0.9}
  \centering
  \setlength{\tabcolsep}{1mm}{
    \caption{\red{Comparison of computational cost between VLCI with R2Gen and R2GenCMN. The $1^{st}$ and $2^{nd}$ best results are bolded and underlined, respectively.}}
          \vspace{-9pt}
  \label{tab:model_cost}
  \begin{tabular}{@{}lllll@{}}
    \toprule
    \red{Method}          & \red{Inference Time (s)}  & \red{Params (M)} & \red{FLOPs (TFLOPs)} & \red{BLEU-4}\\
    \midrule
    \red{R2Gen}         & \red{97.14} & \red{78.07} & \textbf{\red{3.66}} & \red{0.103}\\ 
    \red{R2GenCMN}  & \textbf{\red{40.25}} & \textbf{\red{58.65}}& \red{12.98} & \red{\underline{0.105}}\\ 
    \red{VLCI}       & \red{\underline{97.10}} & \red{\underline{69.41}} & \red{\underline{10.50}} & \textbf{\red{0.119}}\\ 
    \bottomrule
  \end{tabular}}
        \vspace{-15pt}
\end{table}

%\subsection{Datasets, Metrics and Settings}
\subsubsection{Dataset}
%\noindent 
\noindent \textbf{IU-Xray}~\cite{jamiaocv080}, also known as the Indiana University Chest X-ray Collection, is a publicly available medical dataset widely used to evaluate the performance of MRG methods. It comprises 7,470 chest images and 3,955 corresponding reports, with an average report length of 30~\cite{huang2023kiut}. To maintain consistency, we follow the setting used in R2Gen~\cite{chen2020generating} and partition the dataset into training, validation, and testing sets at a ratio of 7:1:2, ensuring that there is no overlap in patients. We tokenize the words with more than 3 occurrences and set the max length as 60. Note that we adopt two images (frontal and lateral views) as input for each sample.
% with 2,955 image-text pairs

\noindent \textbf{MIMIC-CXR}~\cite{johnson2019mimic} is a large-scale chest medical dataset, with 377,110 images and 227,835 corresponding reports, of which the average length of reports is 48 in the training/val set and 61 in testing set~\cite{huang2023kiut}. We use the official paradigm, the dataset is divided into the training set with 368,960 images and 222,758 reports, the validation set with 2,991 images and 1,808 reports, and the testing set with 5,159 images and 3,269 reports. Different from the IU-Xray dataset, MIMIC-CXR has instances of a single modality (only image or report) as well as multiple images corresponding to one report. Therefore, we use a single image as the input and tokenize the words with more than 10 occurrences, and set the max length as 80. 
% with 279,733 image-report pairs

% main
\begin{table*}\renewcommand\arraystretch{0.9}
  \centering
  \setlength{\tabcolsep}{3mm}{
    \caption{The performances of VLCI and other methods on IU-Xray and MIMIC-CXR datasets. The ${1}^{st}$ and ${2}^{nd}$ best results are bolded and underlined, respectively.
    For some methods, the results are missing and denoted by a ``-". 
  }
      \vspace{-5pt}
  \label{tab:main_result}
  \begin{tabular}{@{}lcccccccccc@{}}
    \toprule
    Method             & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4  & CIDEr & Rouge-L & METEOR & Precision & Recall & F1\\
    \midrule
    \multicolumn{11}{c}{\textbf{IU-Xray Dataset}} \\
    \midrule
    %\multirow{5}{*}{\rotatebox{0}{\scriptsize Lightweight}}
                    R2Gen\cite{chen2020generating}              & 0.470 & 0.304 & 0.219 & 0.165 &    -   & 0.371 & 0.187 & - & - & - \\ 
                    CMCL\cite{liu2022competence}               & 0.473 & 0.305 & 0.217 & 0.162 &    -   & 0.378 & 0.186 & - & - & - \\ 
                    PPKED\cite{liu2021exploring}              & 0.483 & 0.315 & 0.224 & 0.168 & 0.351  & 0.376 & 0.190 & - & - & - \\
                    CA\cite{liu2021contrastive}                & 0.492 & 0.314 & 0.222 & 0.169 &    -   & 0.381 & 0.193 & - & - & - \\
                    AlignTransformer\cite{you2021aligntransformer}   & 0.484 & 0.313 & 0.225 & 0.173 &    -   & 0.379 & \underline{\red{0.204}} & - & - & - \\
    %\cmidrule{1-11}
    %\multirow{2}{*}{\rotatebox{0}{\scriptsize Heavyweight}}
                    M2TR\cite{nooralahzadeh2021progressive}               & 0.486 & 0.317 & 0.232 & 0.173 &    -   & \underline{0.390} & 0.192 & - & - & - \\
                    MGSK\cite{yang2022knowledge}             & \underline{0.496} & \underline{0.327} & \underline{0.238} & \underline{0.178} &  0.382 & 0.381 & - & - & - & - \\

                    RAMT\cite{zhang2023semi}               & 0.482 & 0.310 & 0.221 & 0.165 &    -    & 0.377 & \red{0.195}& - & - & - \\
                    MMTN\cite{cao2023mmtn}               & 0.486 & 0.321 & 0.232 & 0.175 &    0.361   & 0.375 & - & - & - & - \\
                    %&Self-boost\#         & 0.487 & \textbf{0.346} & \textbf{0.270} & \textbf{0.208} &  \textbf{0.452} & 0.359 & /\\
    %\cmidrule{1-12}
                    % XPRONET & \textbf{0.525} & \underline{0.357} & \textbf{0.262} & \textbf{0.199} & - & \textbf{0.411} & \underline{0.220} & - & - & /\\
                    DCL\cite{li2023dynamic} & - & - & - & 0.163 & \underline{\red{0.586}} & 0.383 & 0.193 & - & - & - \\
                    % KiUT & \textbf{0.525} & \textbf{0.360} & \textbf{0.251} & 0.185 & - & \textbf{0.409} & \textbf{0.242}\\
                    $\textbf{VLCI}_3 \textbf{(ours)}$               & \textbf{0.505} & \textbf{0.334} &  \textbf{0.245} & \textbf{\red{0.190}} &  \textbf{\red{0.592}} & \textbf{\red{0.394}} & \textbf{\red{0.210}} & - & - & - \\\hline
    \midrule
    \multicolumn{11}{c}{\textbf{MIMIC-CXR Dataset}} \\
    \midrule
    %\multirow{5}{*}{\rotatebox{0}{\scriptsize Lightweight}}
                    R2Gen\cite{chen2020generating}             & 0.353 & 0.218 & 0.145 & 0.103 &    \red{0.149}   & 0.277  & 0.142 & 0.333 & 0.273 & 0.276\\ 
                    \red{R2GenCMN\cite{chen2022cross}}             & \red{0.356} & \red{0.219} & \red{0.147} & \red{0.105} &    \red{0.152}   & \red{0.278}  & \red{0.141} & \red{0.334} & \red{0.275} & \red{0.278}\\ 
                    CMCL\cite{liu2022competence}              & 0.334 & 0.217 & 0.140 & 0.097 &    -   & 0.281 & 0.133 & - & - & - \\ 
                    PPKED\cite{liu2021exploring}             & 0.360 & 0.224 & 0.149 & 0.106 &  \underline{0.237} & \textbf{0.284} & 0.149 & - & - & - \\
                    CA\cite{liu2021contrastive}                & 0.350 & 0.219 & 0.152 & 0.109 &    -   & \underline{0.283} & 0.151 & 0.352 & 0.298 & 0.303\\
                    AlignTransformer\cite{you2021aligntransformer}   & 0.378 & 0.235 & 0.156 & 0.112 &    -   &\underline{0.283} & \underline{0.158} & - & - & - \\
    %\cmidrule{1-11}
    %\multirow{3}{*}{\rotatebox{0}{\scriptsize Heavyweight}}
                    M2TR\cite{nooralahzadeh2021progressive}               & 0.378 & 0.232 & 0.154 & 0.107 &    -   & 0.272 & 0.145 & 0.240 & \textbf{0.428} & 0.308\\
                    %&Self-boost*         & 0.359 & 0.224 & 0.150 & 0.109 &  \underline{0.270} & 0.277& 0.141 \\
                    MGSK\cite{yang2022knowledge}             & 0.363 & 0.228 & 0.156 & 0.115 &  0.203 & \textbf{0.284} & - & 0.458 & 0.348 & 0.371\\
                    RAMT\cite{zhang2023semi}               & 0.362 & 0.229 & 0.157 & 0.113 &    -   & \textbf{0.284}& 0.153  & 0.380 & 0.342 & 0.335\\
                    MMTN\cite{cao2023mmtn}               & \underline{0.379} & \underline{0.238} & \underline{0.159} & \underline{0.116} &    -   & \underline{0.283} & \textbf{0.161} & - & - & - \\
                    % XPRONET & 0.344 & 0.215 & 0.146 & 0.105 & - & 0.279 & 0.138 & - & - & /\\
                    DCL\cite{li2023dynamic}  & - & - & - & 0.109 & \textbf{0.281} & \textbf{0.284} & 0.150 & \underline{0.471} & \underline{0.352} & \underline{0.373}\\
                    % KiUT & \underline{0.393} & \underline{0.243} & \underline{0.159} & 0.113 & - & \textbf{0.285} & \textbf{0.160}& 0.371 & 0.318 & 0.321\\
    %\cmidrule{1-11}
                    $\textbf{VLCI}_6 \textbf{(ours)}$              & \textbf{0.400} & \textbf{0.245} & \textbf{0.165} & \textbf{0.119} &  0.190 & 0.280 & 0.150 & \textbf{0.489} & 0.340 & \textbf{0.401}\\
    \bottomrule
  \end{tabular}}
    \vspace{-10pt}
\end{table*}

\subsubsection{Baseline Models}
We compare the proposed VLCI model with several state-of-the-art MRG models, including R2Gen \cite{chen2020generating}, R2GenCMN \cite{chen2022cross}, CMCL \cite{liu2022competence}, PPKED \cite{liu2021exploring}, CA \cite{liu2021contrastive}, AlignTransformer \cite{you2021aligntransformer}, M2TR \cite{nooralahzadeh2021progressive}, RAMT \cite{zhang2023semi}, MMTN \cite{cao2023mmtn}, MGSK \cite{yang2022knowledge}, and DCL \cite{li2023dynamic}. These models are categorized into lightweight and heavyweight models. The lightweight models comprise no more than 3-layer modules for both the encoder and the decoder, as detailed in Table~\ref{tab:model_scale}. Most of these models incorporate various modules to enhance their performance, such as the knowledge-aware module, template retrieval module, and memory-drive module, which can be computationally expensive.  \red{It is important to note that the total parameters of our VLCI are less than the R2Gen, while VLCI is more efficient by eliminating the recursive memory calculation dependency, as shown in Table~\ref{tab:model_cost}}.

\subsubsection{Evaluation Metrics} We adopt the widely used natural language generation (NLG) metrics, including BLEU~\cite{papineni2002bleu}, ROUGE-L~\cite{rouge2004package}, METEOR~\cite{banerjee2005meteor} and CIDEr~\cite{vedantam2015cider}. Since the MRG specifically focuses on the abnormality detection accuracy rather than the text fluency and similarity with the real report, we further adopt clinical efficacy (CE) metrics~\cite{chen2020generating, liu2021contrastive, nooralahzadeh2021progressive, yang2022knowledge}. It is calculated by the labels extracted from CheXpert~\cite{irvin2019chexpert}. 
Specifically, the extracted positive labels are considered as positives, while the non-positive labels (negative, not mentioned, and uncertain) are treated as negatives. Using this approach, we calculate the micro-precision, micro-recall, and micro-F1 scores between the labels from reference reports and generated reports.

\subsubsection{Implementation Settings} We use the first three blocks of ResNet101~\cite{he2016deep} to extract 1,024 feature maps, which are projected into 512 maps of size $14 \times 14$. 
The dimension of the transformer layers and the number of attention heads are fixed to 512 and 8, respectively. 
In the IU-Xray dataset (referred to as VLCI$_3$), both the encoder and decoder consist of 3 layers, whereas in the MIMIC-CXR dataset (referred to as VLCI$_6$), both the encoder and decoder comprise 6 layers, as illustrated in Table~\ref{tab:model_scale}. The variation in the number of layers can be attributed to the significantly larger size of the MIMIC-CXR dataset in comparison to the IU-Xray dataset.
% A. framework of VLP, B. Mask rate C. Setting of Baseline
During pre-training, we utilize a dataset that combines IU-Xray and MIMIC-CXR datasets, resulting in 4,347 unique words. We perform fine-tuning on these two datasets separately \red{with the same tokenizer as VLP}. The batch size is set to 64 during pre-training and 16 during fine-tuning.
In the pre-training stage, we adopt an image mask rate of 85\% for MIM.  
The VLP is trained using the AdamW optimizer with a warm-up step of 10\% of the total training steps, and the peak learning rate is set to 5e-4.  The weight decay of the optimizer is set to 1e-2, and the total epochs are set to 30 in pre-training.
%, and set to 100 and 30 for the IU-Xray and MIMIC-CXR datasets, vely. 
In the fine-tuning stage, the model is fine-tuned using the Adam optimizer with an initial learning rate of 1e-5 and a weight decay of 5e-5 for 10 epochs on both the IU-Xray and MIMIC-CXR datasets.


\subsection{Quantitative Analysis}
% 总体性能 解释CIDEr差一点
% NLG
\subsubsection{NLG Metric}
As shown in Table~\ref{tab:main_result}, our VLCI outperforms nearly all the MRG methods. Specifically, compared with the lightweight model MMTN~\cite{cao2023mmtn}, VLCI$_3$ significantly improves the BLEU-1 metric by 1.9\% on the IU-Xray dataset. 
Similarly, when compared with the heavyweight model RAMT~\cite{zhang2023semi}, VLCI$_6$ achieves a substantial 3.8\% boost in the BLEU-1 metric on the MIMIC-CXR dataset. This significant improvement in the BLEU-1 metric highlights our method's ability to select words more precisely.  Notably, on the IU-Xray dataset, our tokenizer incorporates words from the MIMIC-CXR dataset, leading to even greater estimation challenges.
To assess text fluency more effectively, we also consider the precise match of four consecutive words using the BLEU-4 metric. Our VLCI demonstrates a \red{1.5}\% improvement over MMTN on the IU-Xray dataset and a 0.6\% improvement over RAMT on the MIMIC-CXR dataset.  Notably, the longer descriptions in the MIMIC-CXR dataset present visual-linguistic data bias and significant spurious correlations among multiple words. Our VLCI effectively leverages cross-modal causal intervention to achieve performance improvement.

In contrast to BLEU, Rouge-L focuses more on the structural and sequential similarity of sentences. Our method demonstrates significant superiority on the IU-Xray dataset while exhibiting slightly lower performance compared to DCL~\cite{li2023dynamic} on the MIMIC-CXR dataset. However, Table~\ref{tab:main_result} reveals that all methods achieve Rouge-L scores around 0.280 on the MIMIC-CXR dataset. This observation suggests that, when generating long sentences, the differences in structure and sequence with the reference reports are similar across various methods. As a result, this metric may not be a sensitive evaluation criterion for this dataset. 

Similarly, the METEOR metric considers synonyms and phrase reordering, emphasizing diverse matches of phrases and vocabulary. As indicated in Table~\ref{tab:main_result}, VLCI achieves the best performance in terms of the METEOR metric on the IU-Xray dataset, but it falls short compared to MMTN on the MIMIC-CXR dataset. This suggests that there are some limitations in matching phrase and vocabulary diversity in our approach. By referring to Table~\ref{tab:model_scale}, we observe that both the best-performing MMTN and AlignTransformer benefit from knowledge supplementation and the assistance of a memory module, potentially enhancing the model's ability to estimate synonyms more effectively. \red{While our approach relies on learning semantics solely from the linear projection space of tokens, which may have limitations.}

Moreover, \red{CIDEr metrics emphasizes the importance of more significant and information-rich words in the generated text, rather than common words, posing a challenge for long sequence reports containing medical terminology. on the MIMIC-CXR dataset, VLCI outperforms two open-source non-knowledge-based methods (R2Gen and R2GenCMN) but falls behind knowledge-based methods (PPKED, MGSK, DCL).  We speculate that domain-specific knowledge with specialized concepts can assist the model in generating more accurate disease descriptions and achieving superior performance, even if these descriptions may be less common.  Our VLCI focuses on causal intervention within cross-modal data without relying on external knowledge, making it unable to accurately capture domain-specific terminology.}
% the performance of VLCI is lower than that of DCL on MIMIC-CXR. This metric assesses the similarity of the entire report, posing challenges for long-sequence reports with medical terminology. Thus, the knowledge-based approach (PPKED, MGSK, DCL) with professional concepts can generate more precise disease descriptions and achieve superior performance, while VLCI focuses on causal intervention within cross-modal data without relying on external knowledge. 



\subsubsection{CE Metric}
The purpose of MRG is to alleviate the burden on medical professionals and provide precise diagnostic evidence. Therefore,  the CE metric is more appropriate for evaluating the clinical significance of MRG methods compared to NLG metrics, as it specifically assesses the accuracy of abnormality classification. 
The CE metric is only applied to the MIMIC-CXR dataset because the label extractor (CheXpert)~\cite{irvin2019chexpert} is specially designed for MIMIC-CXR to obtain class labels. Compared with the lightweight R2Gen in Table~\ref{tab:main_result}, VLCI demonstrates a remarkable improvement of 15.6\% in Precision, 6.7\% in Recall, and 12.5\% in F1-Score. This validates that VLCI can provide a more accurate clinic diagnosis rather than merely generating fluent reports. Additionally, our VLCI outperforms the state-of-the-art method DCL in terms of Precision score and F1 score. Notably, when compared to several knowledge-based methods, our approach achieves a more efficient clinical evidence generation by relying solely on causal intervention without requiring additional knowledge assistance. Nevertheless, the Recall score is relatively lower compared to M2TR in Table~\ref{tab:main_result}, indicating that the dataset may contain extreme categories. M2TR employs staged generation, which leverages the preliminary concepts generated in the first stage, enabling effective anomaly detection. However, this process can also lead to an increase in false positives, consequently reducing the Precision score.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.85\linewidth]{figure/experiment_abnormality.pdf}
      \vspace{-15pt}
  \caption{Evaluation of abnormality classification results (accuracy) on MIMIC-CXR. The baseline model is the transformer without causal intervention.}
      \vspace{-15pt}
  \label{fig:experiment_abnormality}
\end{figure}


\subsection{Qualitative Analysis}
% abnormalities detection
To further validate the assistance of causal intervention (i.e., alleviate the
burden on medical professionals and provide precise diagnostic evidence) in our method, we extract 14 categories of labels from the reports generated by the baseline and VLCI, and evaluated their accuracy, as shown in Fig.~\ref{fig:experiment_abnormality}. Our approach achieves significant performance improvements in all categories, particularly in ``Edema" ($0.509 \to 0.840$) and ``Enlarge Cardiomediastinum" ($0.473 \to 0.842$). This is because our VLCI explores sufficient visual information and further produces more accurate and less biased descriptions by cross-modal causal intervention than the Transformer baseline. 
However, the estimation of some categories \red{remains} ambiguous, e.g., ``Lung Opacity". It reveals that VLCI can provide a comprehensive consideration of various radiologic signs to detect the abnormality but give less improvement for the single source abnormality. For example, whether ``Edema" is caused by the heart has different radiologic signs, while the increase in lung density can be considered as ``Lung Opacity". Thus, VLCI can capture the abnormality with complex causes more effectively, where exists more spurious correlations. Besides, Fig.~\ref{fig:experiment_abnormality} shows the unavailability of causal intervention in independent abnormalities, e.g. ``Support Devices".


\begin{figure*}[h]
 \centering
    \includegraphics[width=0.81\linewidth]{figure/experiment_result.pdf}
        \vspace{-15pt}
    \caption{\red{The results of the Non-causal Model (Baseline) and VLCI models on the MIMIC-CXR dataset are presented in (a-c), and (d) shows the false sample in VLCI. Thirteen kinds of abnormalities are marked with different markers and colors. Note that keywords in the reports are also marked with different markers and colors. Correctly identified abnormalities are marked in the corresponding color, while other descriptions in bold, italics, and underscores are incorrect. Descriptions marked only with underscores indicate repeated words. In each sample, the green regions represent causal graphs constructed by ChatGPT based on Ground Truth, which can further elucidate the inherent \red{causal relation} within each example.}
    }
        \vspace{-10pt}
    \label{fig:experiment_result}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We further conduct the qualitative analysis on the MIMIC-CXR dataset via three intuitive generated examples of the baseline and the VLCI in Fig.~\ref{fig:experiment_result}.
% visual
In Fig.~\ref{fig:experiment_result} (a), the reference report contains three abnormalities. However, the baseline model neglects "pleural effusion" and "consolidation," while VLCI accurately identifies all abnormalities. This indicates that our VDM comprehensively captures all essential visual features, crucial for MRG.
% linguistic
Fig.~\ref{fig:experiment_result} (b) shows an example where the same visual region is simultaneously discovered by the baseline and the VLCI but leads to different descriptions. Our VLCI can accurately describe the heart, while the baseline is uncertain and even has a miscalculation of pneumonia. 
It shows that LDM can alleviate the semantic bias caused by word frequency in word embedding space.
%
Fig.~\ref{fig:experiment_result} (c) illustrates a complex causal graph, where ``atelectasis" and ``edema" could also be causes of ``cardiomegaly." However, the baseline fails to correctly consider the causes of ``cardiomegaly" and erroneously captures these pieces of evidence. In contrast, VLCI leverages the causal intervention module to disentangle these confounders, enabling careful consideration of various pieces of evidence for an accurate judgment.
% 对报告的可视化做了删改，所以看不出流畅程度
% Compared to the Baseline, our VLCI can generate a more fluent report and indicates the normality. 
%Both the Baseline and VLCI can generate a fluent report and indicates the normality. But VLCI fails to capture the peribronchial opacities, which are the radiologic signs between ``Clear Lung" and ``Consolidation". This is because the ``Lung Opacity" only changes the pulmonary density and it is difficult to be discovered determinedly.

In Fig.~\ref{fig:experiment_result} (d), the ground truth indicates the presence of hydropneumothorax, a condition characterized by the simultaneous presence of gas and fluid in the chest, whereas pleural effusion contains only fluid. Although VLCI correctly identifies the presence of gas and fluid in the chest, leading to the diagnosis of pneumothorax, it erroneously estimates pleural effusion due to insufficient relevant knowledge. In this example, our method produces inaccurate text and fails to identify pneumonia and lung consolidation. Moreover, while VDM and LDM excel at recognizing visual and language concepts, detecting highly specialized concepts with latent relations not explicitly present in the data presents challenges.

\subsection{Ablation Studies}
\subsubsection{\red{effectiveness of VLCI}}
\red{In Table~\ref{tab:ablation_densenet}, we adopt the same setting as ResNet101 to conduct the ablation experiments using DenseNet121 as the backbone. We perform VLP and fine-tuning with causal intervention on IU-Xray and MIMIC-CXR datasets. Due to the small scale of the IU-Xray dataset and the simplicity of report content, the persuasiveness of the CIDEr metric on this dataset is limited. We evaluated BLEU-4, Rouge-L, and METEOR on the IU-Xray dataset, and the results indicate that our method still achieves a significant improvement with DenseNet121. However, we observed that the performance of models using DenseNet121 as the backbone is notably inferior to those using ResNet101. We speculate that while DenseNet121 is efficient in feature extraction due to its dense connections, it may not always provide the best performance for tasks involving specific types of medical images or requiring deeper feature extraction capabilities. Additionally, we conducted experiments on the larger MIMIC-CXR dataset, using BLEU-4, CIDEr, and METEOR as evaluation metrics. The experimental results further confirm our speculations and validate the effectiveness of our approach.
}

\begin{table}\renewcommand\arraystretch{0.9}
  \centering
  \setlength{\tabcolsep}{1mm}{
    \caption{\red{Ablation Result of DenseNet121 backbone.}}
          \vspace{-9pt}
  \label{tab:ablation_densenet}
  \begin{tabular}{@{}lllll@{}}
    \toprule
    \red{Method on IU-Xray}          & \red{BLEU-4}  & \red{Rouge-L} & \red{METEOR}\\
    \midrule
    \red{ResNet101}         & \red{0.148} & \red{0.345} & \red{0.180}\\ 
    \red{ResNet101 + VLCI}  & \red{\textbf{0.190} (+0.042)} & \red{\textbf{0.394} (+0.049)}& \red{\textbf{0.210} (+0.03)}\\ 
    \red{DenseNet121}       & \red{0.164} & \red{0.361} & \red{0.183}\\ 
    \red{DenseNet121 + VLCI}& \red{\textbf{0.176} (+0.012)} & \red{\textbf{0.394} (+0.033)} & \red{\textbf{0.195} (+0.012)}\\ 
    \midrule
    \red{Method on MIMIC-CXR}          & \red{BLEU-4}  & \red{CIDEr} & \red{METEOR}\\
    \midrule
    \red{ResNet101}& \red{0.101} & \red{0.130} & \red{0.135}\\ 
    \red{ResNet101 + VLCI}& \red{\textbf{0.119} (+0.018)} & \red{\textbf{0.190} (+0.06)}& \red{\textbf{0.150} (+0.015)}\\ 
    \red{DenseNet121}& \red{0.091} & \red{0.111} & \red{0.127}\\ 
    \red{DenseNet121 + VLCI}& \red{\textbf{0.113} (+0.022)} & \red{\textbf{0.134} (+0.023)} & \red{\textbf{0.149} (+0.022)}\\ 
    \bottomrule
  \end{tabular}}
        \vspace{-15pt}
\end{table}



\subsubsection{Effectiveness of VLP}

In Table~\ref{tab:experiment_vlp_mask}, we conduct ablation experiments to assess the impact of masking ratios on model performance, and the results are presented. Our VLP model achieved the best performance with a higher masking ratio of 85\%, which is in contrast to the optimal masking ratio of 75\% reported by MAE~\cite{he2022masked}. We attribute this difference to the cross-modal information correlations, where the masked information can be reconstructed by visible features from both language and images. Furthermore, VLP tends to learn general features from the masked modality at higher masking ratios, while distinguishable features can be extracted by the complete information from another modality. To explore whether increasing the masking ratio further would further improve the performance, we experimented with a higher masking ratio of 95\%. However, the decreased results in Table~\ref{tab:experiment_vlp_mask} indicate that this approach leads to excessive information loss.

% VLP mask
\begin{table}[t]\renewcommand\arraystretch{0.9}
  \centering
  \setlength{\tabcolsep}{1mm}{
  \caption{We evaluated the performance of various masking ratios for MIM on the IU-Xray dataset. We pre-trained the VLP model for 100 epochs and then fine-tuned it in the baseline (non-causal model) for an additional 5 epochs.}
        \vspace{-5pt}
  \label{tab:experiment_vlp_mask}
  \begin{tabular}{@{}lcccc@{}}
    \toprule
    Masking Ratio           & BLEU-1  & BLEU-4  & CIDEr & Rouge-L \\
    \midrule
    Baseline            & 0.433 & 0.148 & 0.501 & 0.345 \\ 
    w/ 75\%             & 0.450 & 0.160 & 0.486 & \textbf{0.360} \\ 
    w/ 85\%             & \textbf{0.452} & \textbf{0.161} & \textbf{0.522} & 0.351 \\
    w/ 95\%             & 0.432 & 0.153 & 0.460 & 0.346 \\ 
    \bottomrule
  \end{tabular}}
        \vspace{-15pt}
\end{table}

% VLP ablation
\begin{table}[!t]\renewcommand\arraystretch{0.9}
  \centering
  \setlength{\tabcolsep}{1mm}{
    \caption{The performance of different pre-training methods on IU-Xray, the result marked by * means fine-tuning with 10 epochs, while the rest only use the encoder with 100 epochs. The result marked by $\dagger$ is from \cite{Chen_2022_CVPR}.}
          \vspace{-9pt}
  \label{tab:ablation_vlp}
  \begin{tabular}{@{}lccc@{}}
    \toprule
    Method           & BLEU-4  & CIDEr & Rouge-L \\
    \midrule
    Baseline         & 0.148 & 0.501 & 0.345 \\ 
    w/ MAE           & 0.154 & 0.486 & 0.360 \\ 
    w/ VisualGPT$\dagger$     & 0.159 & 0.497 & \textbf{0.374} \\ 
    w/ VLP (MIM)     & 0.162 & \underline{0.602} & 0.362 \\
    w/ VLP (PLM)     & \underline{0.165} & 0.538 & \underline{0.365} \\ 
    w/ VLP (PLM+MIM)  & 0.160 & 0.431 & 0.364 \\ 
    \midrule
    w/ VLP (PLM)*    & 0.151 & 0.399 & 0.349 \\ 
    w/ VLP (PLM+MIM)*   & 0.161 & 0.522 & 0.351 \\ 
    \textbf{w/ VLP (PLM+MIM+Degradation)}* (Ours)   & \textbf{0.170} & \textbf{0.631} & 0.363 \\ 
    \bottomrule
  \end{tabular}}
        \vspace{-15pt}
\end{table}

In Table~\ref{tab:ablation_vlp}, we make a comparison with different pre-training methods. It shows that the cross-modal pre-training method has a more robust representation ability than the MIM with single-modality. However, the Rouge-L metric in VisualGPT surpasses ours, possibly due to its exclusive pre-training of the text decoder, enabling more concentrated learning of the intricate structure in medical reports.
Additionally, our cross-modal pre-training achieves comparable performance to the PLM model that only fine-tunes the encoder, while ours fine-tunes the whole model with fewer epochs. 
Moreover, our VLP adopts the degradation images as input, which facilitates the extraction of visual details in the downstream task.

% main ablation
\begin{table}\renewcommand\arraystretch{0.9}
  \centering
  \setlength{\tabcolsep}{0.7mm}{
    \caption{Ablation analysis of our VLCI. The Baseline is implemented by the transformer. The marker at the Baseline (non-causal model) and R2Gen~\cite{chen2020generating} means the operation in the brackets.}
          \vspace{-10pt}
  \label{tab:ablation_baseline}
  \begin{tabular}{@{}llccc@{}}
    \toprule
    Dataset     & Method        & BLEU-4  & CIDEr & Rouge-L \\
    \midrule
                & Baseline      & 0.148 & 0.501 & 0.345 \\ 
                & Baseline$^{w\blacklozenge}$ (w/ VDM) &  0.160 & 0.521 & 0.364\\
                & Baseline$^{w\bullet}$ (w/ LDM)  &  0.155 & 0.509 & 0.361\\
                & Baseline$^{w\blacklozenge \bullet} $ (w/ VDM\&LDM)   & 0.163 & {0.544} & 0.361 \\ 
                \cmidrule{2-5}
                & R2Gen                                                 & 0.165   & 0.493 & 0.360 \\
                & R2Gen$^{w\blacklozenge}$ (w/ VDM)                     & 0.171 & 0.553 & 0.370 \\
    IU-Xray     & R2Gen$^{w\bullet}$ (w/ LDM)                           & 0.166 & 0.546 & 0.360 \\
                & R2Gen$^{w\blacklozenge\bullet}$ (w/ VDM\&LDM)         & 0.173 & \underline{0.628} & 0.368 \\
                \cmidrule{2-5}
                & Baseline$^{w\bigstar}$ (w/ VLP)                       & 0.170 & \textbf{0.631} & 0.363 \\ 
                & Baseline$^{w\bigstar\blacklozenge}$ (w/ VLP\&VDM)     & 0.174  &  0.523 & 0.374  \\
                & Baseline$^{w\bigstar \bullet}$ (w/ VLP\&LDM)          &  \underline{0.178} &  0.573 & \underline{0.378}  \\
                & VLCI          & \red{\textbf{0.190}} & \red{0.592} & \red{\textbf{0.394}} \\
    \midrule
                & Baseline      & 0.101 & 0.130 & 0.270 \\ 
                & Baseline$^{w\blacklozenge}$ (w/ VDM) & 0.103  & 0.144 & 0.272\\
                & Baseline$^{w\bullet}$ (w/ LDM)  &  0.069 & 0.071 & 0.224\\
                & Baseline$^{w\blacklozenge \bullet} $ (w/ VDM\&LDM)   & 0.070 & 0.074 & 0.230 \\
                \cmidrule{2-5}
                & R2Gen    & 0.103  &  0.168   &  \underline{0.278}   \\
                & R2Gen$^{w\blacklozenge}$ (w/ VDM) &  0.106 & 0.171  & 0.277  \\
    MIMIC-CXR   & R2Gen$^{w\bullet}$ (w/ LDM)  &  0.091 & 0.136  & 0.256  \\
                & R2Gen$^{w\blacklozenge\bullet}$ (w/ VDM\&LDM) &  0.100 & 0.143  & 0.264\\
                \cmidrule{2-5}
                & Baseline$^{w\bigstar}$ (w/ VLP)      & 0.106 & 0.151 & \underline{0.278} \\ 
                & Baseline$^{w\bigstar\blacklozenge}$ (w/ VLP\&VDM)   & 0.110 & \underline{0.177} & \textbf{0.280} \\
                & Baseline$^{w\bigstar \bullet}$ (w/ VLP\&LDM)   & \underline{0.115} & 0.157 & 0.277 \\
                & VLCI          & \textbf{0.119} & \textbf{0.190} & \textbf{0.280} \\
    \bottomrule
  \end{tabular}}
      \vspace{-10pt}
\end{table}
 
% Slight improvements from VDM and LDM
Furthermore, in Table~\ref{tab:ablation_baseline}, Baseline$^{w\blacklozenge \bullet}$ is significantly worse than baseline on the MIMIC-CXR dataset, e.g., 0.101 $\to$ 0.070 for BLEU-4, while still keeping performance improvement on IU-Xray dataset. % compared with the baseline. 
This validates the significant feature complexity from the large-scale MIMIC-CXR dataset leads to unstable probability distribution estimation with causal intervention. 
% 为了关联，得到先验分布. mimic的数据集非常大且复杂以至于很难直接在intervention中得到先验分布。而相对于iu-xray这种小数据集，由于data bias反而容易得到关联。
Meanwhile, we find that the VLP can substantially boost the performance of the baseline, e.g., 0.148 $\to$ 0.170, 0.101 $\to$ 0.106 for BLEU-4 on IU-Xray and MIMIC-CXR datasets, respectively. The improvement is caused by the learned comprehensive concepts and context in the pre-training and the cross-modal features alignment stage, which shows the importance of VLP. Similarly, The Rough-L is also barely improved due to the features' complexity and long sequence from the MIMIC-CXR dataset. For example, although AlignTransformer achieves the same score of the Rough-L as CA on the MIMIC-CXR, it outperforms CA on all other metrics.


\subsubsection{Effectiveness of Causal Intervention}
% 与VLP相比，VLP + VDM 小数据集提升明显，但大的少；大数据集文本复杂；有视觉信息，但是不能准确描述
% 与VLP相比，VLP + LDM 大数据集提升明显，但小的少；大数据集文本复杂；更好的描述，但是缺少准确的视觉信息
% VLCI 小数据集的CIDEr明显不如 VDM + LDM；因为小数据集的文本相对单一且短，虽然报告整体显示但是用词准确度不如
\noindent \textbf{VDM}. %To disentangle visual features, we implement the visual deconfounding via the mediator that fuses local detail information and global contour position. 
In Table~\ref{tab:ablation_baseline}, Baseline$^{w\blacklozenge}$ and R2Gen$^{w\blacklozenge}$ can boost the performance compared to Baseline and R2Gen, which demonstrates the validity of the VDM. 
% CIDEr metric?
However, the improvement of BLEU-4 between Baseline$^{w\bigstar \blacklozenge}$ and Baseline$^{w\bigstar}$ on the IU-Xray dataset is more significant than that on the MIMIC-CXR dataset. This is because the VDM can discover essential visual information, but the report of the MIMIC-CXR is more complex and the model fails to generate accurate descriptions. The performance degradation of CIDEr can further illustrate it.

In Fig.~\ref{fig:experiment_local_vis}, the encoder of the non-causal model exhibits limited attention to all potential abnormal regions.  Instead, it excessively focuses on the base of the lung, possibly due to the dataset's high prevalence of lung-related diseases. In contrast, the attention map from the VLCI encoder can truly focus on the dominant regions that may indicate abnormalities, including the entire lung lobe, carina, and pleura, rather than false correlations with biased visual concepts. This confirms the semantic sensitivity of the VDM, which captures dominant visual content by performing visual causal interventions.
%In Fig.~\ref{fig:experiment_local_vis}, the attention map from the encoder of our VLCI can truly focus on the dominated area of possible abnormalities rather than spurious correlations with biased visual concepts. This validates that the VDM is semantics-sensitive to capture dominant visual content by conducting visual causal intervention.

\begin{figure}[t]
 \centering
    %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
    \includegraphics[width=0.9\linewidth]{figure/experiment_local_vis.pdf}
          \vspace{-15pt}
    \caption{The visualization of the attention map. (a) is an example from the MIMIC-CXR dataset that the colored text should be discovered in the marked region of the image. The images in (b) are the attention maps of the non-causal model and our VLCI, respectively. The tag ``Enc" means the accumulated attention maps from the encoder for the selected local features, and ``Dec" is the response to the ``pleural" (decoder output).}
          \vspace{-15pt}
    \label{fig:experiment_local_vis}
\end{figure}

% 句子描述更准确
\noindent \textbf{LDM}. %To generate accurate reports after perceiving all essential visual features, the LDM can mitigate spurious correlations caused by cross-modal bias and adjust the semantic embedding space. 
Compared to the VDM, the LDM plays a more significant role in MRG because the sophisticated linguistic semantic patterns within reports are entangled and biased that require elaborate linguistic deconfounding. 
In Table~\ref{tab:ablation_baseline}, the performance drops without LDM, e.g., 0.119 $\to$ 0.110 for the BLEU-4 metric on the MIMIC-CXR dataset. This shows the importance of adjusting semantic relevance in word embedding space. 
Compared with the baseline, the performance improvement of Baseline$^{w\bigstar \bullet}$ on the MIMIC-CXR dataset demonstrates that the LDM can generate more accurate reports even with biased visual information. 
% CIDEr 的问题
However, the CIDEr metric on the IU-Xray dataset shows the effectiveness of the combination of VDM and LDM, while ILVD obtains a lower score. This is due to the worse diversity on the IU-Xray dataset, where Baseline$^{w\blacklozenge \bullet}$ and R2Gen$^{w\blacklozenge \bullet}$ can get higher CIDEr but lower BLEU-4 with inadequate multi-modal feature correlation.
In Fig.~\ref{fig:experiment_local_vis} (b), the attention map of the decoder in the non-causal model exhibits evident redundant responses, with high attention widely distributed in the upper part of the lung, especially the carina. 
The VLCI, in contrast, can capture dominant semantic information in a coarse-to-fine manner, refining it from the potential abnormal regions that receive extensive attention in the encoder to the bilateral thorax. The high attention on the carina may be attributed to the presence of a support device that could increase cardiac load, cause vascular occlusion or congestion, leading to changes in intrathoracic pressure and eventually resulting in pleural effusion.
These findings indicate that the LDM can capture more discriminative semantic information from the linguistic modality through linguistic front-door interventions.

%In Fig.~\ref{fig:experiment_local_vis} (b), the attention map of the baseline decoder shows an obvious redundancy response, while VLCI can capture dominated semantic information in a coarse-to-fine manner, which is more related to the abnormalities. These results show that LDM can capture more discriminative semantic information from linguistic modality by linguistic front-door intervention. 



%-------------------------------------------------------------------------
\section{Conclusion}

In this paper, we propose Visual-Linguistic Causal Intervention (VLCI) framework for MRG, to implicitly deconfound the visual-linguistic confounders by causal front-door intervention. To alleviate the problem of unpaired visual-linguistic data when pre-training, we combine the PLM and MIM for cross-modal pre-training. To implicitly mitigate cross-modal confounders and discover the true cross-modal causality, we propose visual-linguistic causal front-door intervention modules VDM and LDM. Experiments on IU-Xray and MIMIC-CXR datasets show that our VLCI can effectively mitigate visual-linguistic bias and outperforms the state-of-the-art methods. The lower computational cost and faster inference speed of VLCI promote its clinical application. In the future work, we will distill medical knowledge from large language models (LLMs) for more medical imaging modalities such as CT, MRI, and pathological slides. We believe our work could inspire more causal reasoning methods in MRG. 


{\small
\bibliography{ref}
\bibliographystyle{IEEEtran}
}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/WeixingChen}}]{Weixing Chen} has received the B.S. degree from the college of Medicine and Biological Information Engineering, Northeastern University, in 2020 and M.S. degree from Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences in 2023. He is currently a research assistant  at the School of Computer Science and Engineering, Sun Yat-sen University. His main interests include medical image analysis, multi-modal learning, and causal relation discovery. He has been serving as a reviewer for numerous academic journals and conferences.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/YangLiu}}]{Yang Liu}(M'21) is currently a research associate professor working at the School of Computer Science and Engineering, Sun Yat-sen University. He received his Ph.D. degree from Xidian University in 2019. His current research interests include multi-modal cognitive reasoning and causal relation discovery. He is the recipient of the First Prize of the Third Guangdong Province Young Computer Science Academic Show. He has authorized and co-authorized more than 20 papers in top-tier academic journals and conferences such as TPAMI, TIP, TCSVT, TII, CVPR, ICCV, IJCAI, and ACM MM. More information can be found on his personal website https://yangliu9208.github.io.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/CeWang}}]{Ce Wang} has received the B.S. degree from the Department of Mathematics, Jilin University, in 2015 and Ph.D. degree from the Center for Combinatorics, Nankai University in 2020. He is currently a postdoctoral researcher in the Institute of Computing Technology, Chinese Academy of Sciences. His main interests include generative modeling, image and video processing, computational imaging, medical image analysis, and explainable healthcare AI. He has been serving as a reviewer for numerous academic journals and conferences such as TMI, MIA, TCSVT, NIPS, ICLR, MICCAI, and ACM MM.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/JiaruiZhu}}]{Jiarui Zhu} is currently a Ph.D. student in Department of Health Technology and Informatics, the Hong Kong Polytechnic University. He received his master of science degree from the Hong Kong Polytechnic University in 2023. His current research interests include medical imaging processing and deep learning. 
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/zhaoshen}}]{Shen Zhao} received his Ph.D. degree in Tsinghua University in 2015. He is now researching on deep learning methods in medical image analysis. His research interests are mainly on automatic disease diagnosis on multi-model spinal images using novel methods on object detection/segmentation, metric learning, metric learning, deep active contours, and the combination of visual and linguistic information in medical image diagnosis.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/GuanbinLi}}]{Guanbin Li}(M'15) is currently an associate professor in School of Computer Science and Engineering, Sun Yat-Sen University. He received his PhD degree from the University of Hong Kong in 2016. His current research interests include computer vision, image processing, and deep learning. He is a recipient of ICCV 2019 Best Paper Nomination Award. He has authorized and co-authorized on more than 100 papers in top-tier academic journals and conferences. He serves as an area chair for the conference of VISAPP. He has been serving as a reviewer for numerous academic journals and conferences such as TPAMI, IJCV, TIP, TMM, TCyb, CVPR, ICCV, ECCV and NeurIPS.
\end{IEEEbiography}

\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/Cheng-LinLiu.pdf}}]{Cheng-Lin Liu}(Fellow, IEEE) received the B.S. degree in electronic engineering from Wuhan University, Wuhan, China, in 1989, the M.E. degree in electronic engineering from the Beijing University of Technology, Beijing, China, in 1992, and the Ph.D. degree in pattern recognition and intelligent control from the Institute of Automation of the Chinese Academy of Sciences, Beijing, in 1995. From 1999 to 2004, he was a Research Staff Member and later a Senior Researcher with the Central Research Laboratory, Hitachi Ltd., Tokyo, Japan. Since 2005, he has been a Professor with the National Laboratory of Pattern Recognition (NLPR), Institute of Automation of the Chinese Academy of Sciences, where he is currently the Director. He has published over 300 technical papers in prestigious journals and conferences. His research interests include pattern recognition, image processing, neural networks, machine learning, and especially the applications to character recognition and document analysis. He is a fellow of the CAA and IAPR. He is an Associate Editor-in-Chief of Pattern Recognition.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/LiangLin}}]{Liang Lin}(M'09, SM'15, F'23) is a Full Professor of computer science at Sun Yat-sen University. He served as the Executive Director and Distinguished Scientist of SenseTime Group from 2016 to 2018, leading the R\&D teams for cutting-edge technology transferring. He has authored or co-authored more than 200 papers in leading academic journals and conferences, and his papers have been cited by more than 29,000 times. He is an associate editor of IEEE Trans.Neural Networks and Learning Systems and IEEE Trans. Multimedia, and served as Area Chairs for numerous conferences such as CVPR, ICCV, SIGKDD and AAAI. He is the recipient of numerous awards and honors including Wu Wen-Jun Artificial Intelligence Award, the First Prize of China Society of Image and Graphics, ICCV Best Paper Nomination in 2019, Annual Best Paper Award by Pattern Recognition (Elsevier) in 2018, Best Paper Dimond Award in IEEE ICME 2017, Google Faculty Award in 2012. His supervised PhD students received ACM China Doctoral Dissertation Award, CCF Best Doctoral Dissertation and CAAI Best Doctoral Dissertation. He is a Fellow of IET/IAPR.
\end{IEEEbiography}

%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{}}]{IEEE Publications Technology Team}
%In this paragraph you can place your educational, professional background and research and other interests.\end{IEEEbiography}


\end{document}


